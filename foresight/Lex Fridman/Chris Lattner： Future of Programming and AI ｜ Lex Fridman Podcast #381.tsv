start	end	text
0	4080	On one axis, you have more hardware coming in. On the other hand, you have an explosion of
4080	8800	innovation in AI. And so what happened with both TensorFlow and PyTorch is that the explosion
8800	13680	of innovation in AI has led to, it's not just about matrix multiplication and convolution,
13680	18160	these things have now like 2,000 different operators. And on the other hand, you have,
18160	21120	I don't know how many pieces of hardware there are out there, it's a lot.
21120	25600	Part of my thesis, part of my belief of where computing goes, if you look at 10 years from now,
26160	30160	is it's not going to get simpler. Physics isn't going back to where we came from.
30720	35920	It's only going to get weirder from here on out. And so to me, the exciting part about what we're
35920	42320	building is it's about building that universal platform, which the world can continue to get
42320	47600	weird, because again, I don't think it's avoidable, it's physics. But we can help lift people scale,
47600	50480	do things with it, and they don't have to rewrite their code every time a new device comes out.
51040	52160	And I think that's pretty cool.
52320	59440	The following is a conversation with Chris Latner, his third time on this podcast.
59440	64720	As I've said many times before, he's one of the most brilliant engineers in modern computing,
64720	70160	having created LLM Compiler Infrastructure Project, the Klang Compiler, the Swift programming
70160	74720	language, a lot of key contributions to TensorFlow and TPUs as part of Google.
74720	80800	He served as vice president of autopilot software at Tesla, was a software innovator
80800	87680	and leader at Apple, and now he co-created a new full stack AI infrastructure for distributed
87680	93920	training, inference, and deployment on all kinds of hardware called Modular, and a new
93920	99920	programming language called Mojo, that is a superset of Python, giving you all the usability of
99920	107120	Python, but with the performance of C++. In many cases, Mojo code has demonstrated over
107760	114480	30,000 x speed up over Python. If you love machine learning, if you love Python,
114480	120160	you should definitely give Mojo a try. This programming language, this new AI framework,
120160	126400	and infrastructure, and this conversation with Chris is mind blowing. I love it.
127280	132400	It gets pretty technical at times, so I hope you hang on for the ride. This is the Lex
132400	136640	Friedman podcast. To support it, please check out our sponsors in the description,
136640	142800	and now, dear friends, here's Chris Lattner. It's been, I think, two years since we last
142800	149040	talked, and in that time, you somehow went and co-created a new programming language called Mojo.
149840	155360	So it's optimized for AI. It's a superset of Python. Let's look at the big picture. What is
155360	162000	the vision for Mojo? For Mojo? Well, I think you have to zoom out. So I've been working on a lot
162000	166800	of related technologies for many, many years. So I've worked on LVM, and a lot of things, and
166800	172160	mobile, and servers, and things like this. But the world's changing. And what's happened with
172160	177920	AI is we have new GPUs, and new machine learning accelerators, and other ASICs, and things like
177920	183280	that that make AI go real fast. At Google, I worked on TPUs. That's one of the biggest,
183280	189280	largest scale deployed systems that exist for AI. And really what you see is if you look across
189280	192000	all of the things that are happening in the industry, there's this new compute platform
192000	198880	coming. And it's not just about CPUs, or GPUs, or TPUs, or NPUs, or IPUs, or whatever,
198880	205920	all the PUs. It's about how do we program these things. And so for software folks like us,
206960	210240	it doesn't do us any good if there's this amazing hardware that we can't use.
211040	215760	And one of the things you find out really quick is that having the theoretical capability of
215760	221040	programming something, and then having the world's power and the innovation of all the
221040	225680	smart people in the world get unleashed on something can be quite different. And so really
225680	230880	where Mojo came from was starting from a problem of we need to be able to take machine learning,
230880	234720	take the infrastructure underneath it, and make it way more accessible, way more usable,
234720	239520	way more understandable by normal people and researchers and other folks that are not
239520	244240	themselves like experts in GPUs and things like this. And then through that journey,
244240	246880	we realize, hey, we need syntax for this, we need to do a program language.
247440	254480	So one of the main features of the language, I say so fully in jest, is that it allows you to have
254480	265280	the file extension to be an emoji, or the fire emoji, which is one of the first emojis used
265280	269680	as a file extension I've ever seen in my life. And then you ask yourself the question, why in
269680	276400	the 21st century, are we not using Unicode for file extensions? This I mean, it's an epic decision.
276400	281040	I think clearly the most important decision you made the most, but you could also just use Mojo
281040	284800	as the file extension. Well, so okay, so take a step back. I mean, come on, Lex, do you think that
284800	288720	the world's ready for this? This is a big moment in the world, right? This is we're releasing this
288720	295200	onto the world. This is innovation. I mean, it really is kind of brilliant. Emojis are such a
295200	301600	big part of our daily lives. Why isn't it not in programming? Well, and like you take a step back
301600	306880	and look at what file extensions are, right? They're basically metadata, right? And so why are
306880	311120	we spending all the screen space on them and all this stuff? Also, you know, you have them stacked
311120	315120	up next to text files and PDF files and whatever else, like, if you're gonna do something cool,
315120	319840	you want to stand out, right? Emojis are colorful, they're visual, they're beautiful, right?
319840	325360	What's been the response so far from, is there support on like Windows on operating systems
325360	330160	in displaying like File Explorer? Yeah, the one problem I've seen is that Git doesn't escape it
330160	334560	right? And so it thinks that the Fire Emoji is unprintable. And so it like prints out weird
334560	339120	hex things if you use the command line Git tool. But everything else as far as I'm aware works
339120	345600	fine. And I have faith that Git can be improved. So GitHub is fine. GitHub is fine. Yep. GitHub is
345600	350160	fine. Visual Studio Code, Windows, like all this stuff totally ready, because people have
350160	355520	internationalization in their normal part of their paths. So this is just like taking the next step,
355520	362480	right? Somewhere between, oh, wow, that makes sense. Cool. I like new things too. Oh my god,
362480	366160	you're killing my baby. Like, what are you talking about? This can never be like, I can never hand
366160	371120	it and list how am I going to type this like all these things. And so this is something where I
371120	374880	think that the world will get there. We don't have to bet the whole farm on this. I think we can
375440	381440	provide both paths. But I think it'll be great. When can we have emojis as part of the code? I
381440	386400	wonder. Yeah, so I mean, lots of languages provide that. So I think that we have partial support
386400	390800	for that. It's probably not fully done yet. But but yeah, you can you can do that. For example,
390800	397280	in Swift, you can do that for sure. So an example we give gave it Apple was the dog cow. Yeah,
397280	401520	so that's a classical Mac heritage thing. And so you use the dog and the cow emoji together.
401520	404800	And that could be your variable name. But of course, the internet went and made
404800	409200	pile of poop for everything. Yeah. So you know, if you want to name your function pile of poop,
409200	412080	then you can totally go to town and see how that gets through code review.
414240	421120	Okay, so let me just ask a bunch of random questions. So is Mojo primarily designed for
421120	426880	AIs? Or is it a general purpose program? Good question. So it's AI first. And so AI is driving
426960	432960	a lot of the requirements. And so modular is building and designing and driving Mojo forward.
432960	438080	And it's not because it's an interesting project theoretically to build, it's because we need it.
439200	443840	And so modular, we're really tackling the AI infrastructure landscape and the big problems
443840	449280	in AI, and the reasons that it is so difficult to use and scale and adopt and deploy and like all
449280	454800	these big problems in AI. And so we're coming out from that perspective. Now, when you do that,
454800	460240	when you start tackling these problems, you realize that the solution to these problems
460240	464640	isn't actually an AI specific solution. And so while we're doing this, we're building Mojo to
464640	471040	be a fully general programming language. And that means that you can obviously tackle GPUs and CPUs
471040	476160	and like these AI things, but it's also a really great way to build NumPy and other things like
476160	481040	that or you know, just if you look at what many Python libraries are today, often they're a layer
481040	487200	of Python for the API, and they end up being C and C++ code underneath them. That's very true in AI,
487200	490480	that's true in lots of other demands as well. And so anytime you see this pattern,
490480	494800	that's an opportunity for Mojo to help simplify the world and help people have one thing.
496000	502240	To optimize through simplification, by having one thing. So you mentioned modular. Mojo is
502240	507120	the programming language, modular is the whole software stack. So just over a year ago, we started
507200	512400	this company called modular. What modular is about is it's about taking AI and up leveling it
512960	519360	into the next generation. And so if you take a step back, what's gone on in the last five, six,
519360	524240	seven, eight years is that we've had things like TensorFlow and PyTorch and these other systems
524240	529520	come in, you've used them, you know this. And what's happened is these things have grown like crazy,
529520	534400	and they get tons of users, it's in production deployment scenarios, it's being used to power
534400	540240	so many systems. AI is all around us now. It used to be controversial years ago, but now it's a
540240	547040	thing. But the challenge with these systems is that they haven't always been thought out with
547040	553920	current demands in mind. So you think about it, where were LLMs eight years ago? Well, they didn't
553920	558320	exist, right? AI has changed so much. And a lot of what people are doing today are very different
558320	562720	than when these systems were built. And meanwhile, the hardware side of this has gotten into a huge
562720	567200	mess. There's tons of new chips and accelerators and every big company's announcing a new chip every
567200	574240	day it feels like. And so between that, you have like this moving system on one side, moving system
574240	577920	on the other side, and it just turns into this gigantic mess, which makes it very difficult for
577920	582800	people to actually use AI, particularly in production deployment scenarios. And it's what
582800	586880	modular students were helping build out that software stack to help solve some of those problems.
586880	590240	So then people can be more productive and get more AI research into production.
590880	595840	Now, what Mojo does is it's a really, really, really important piece of that. And so that is,
596400	600560	you know, part of that engine and part of the technology that allows us to solve these problems.
600560	606960	So Mojo is a programming language that allows you to do a high level program and the low level
606960	612880	programming, they do all kinds of programming in that spectrum that gets you closer and closer
612880	616320	to the hardware. So take a step back. So Lex, what do you love about Python?
616960	623600	Oh boy. Where do I begin? What is love? What do I love about Python?
623600	628400	You're a guy who knows love. I know this. Yes. How intuitive it is.
630960	633520	How it feels like I'm writing natural language, English.
636320	642160	How when I can not just write but read other people's code somehow I can understand it faster.
642160	648880	It's more condensed than other languages like ones I'm really familiar with like C++
649520	656480	and C. There's a bunch of sexy little features. Yeah. We'll probably talk about some of them,
656480	662560	but list comprehensions and stuff like this. Also, and don't forget the entire ecosystem
662560	666160	of all the packages. Oh yeah, that's probably huge. Because there's always something. If you want to do
666160	672160	anything, there's always a package. Yeah. So it's not just the ecosystem of the packages
672160	677520	and the ecosystem of the humans that do it. That's a really, that's an interesting dynamic.
677520	684000	That's good. Because I think something about the usability and the ecosystem makes the thing viral.
684000	688480	It grows and then it's a virtuous cycle, I think. Well, there's many things that went into that.
688480	692960	Like, so I think that ML was very good for Python. And so I think that TensorFlow and
693040	699360	PyTorch and these systems embracing Python really took and helped Python grow. But I think that the
699360	704640	major thing underlying it is that Python is like the universal connector. It really helps bring
704640	708400	together lots of different systems so you can compose them and build out larger systems without
708400	713920	having to understand how it works. But then what is the problem with Python? Well, I guess you
713920	718560	could say several things, but probably that it's slow. I think that's usually what people complain
718640	724800	about. And so other people complain about tabs in spaces versus curly braces or whatever. But
725680	729760	those people are just wrong because it is actually just better to use an indentation.
731200	736240	Wow, strong words. So actually, I'm a small tangent. Let's actually take that. Let's take
736240	739200	all kinds of tangents. Oh, come on, Lex. You can push me on it. I could take it.
739200	746720	Design. Listen, I've recently left Emacs for VS Code. The kind of hate mail I had to receive.
746720	752960	Because on the way to doing that, I also said I've considered Vim and chose not to and went with
752960	759280	VS Code. You're touching on deep religions, right? Anyway, tabs is an interesting design decision.
759280	765680	And so you've really written a new programming language here. Yes, it is a superset of Python,
765680	769840	but you can make a bunch of different interesting decisions here. Totally. And you chose actually
769840	777040	to stick with Python in terms of some of the syntax. So let me explain why. So
778480	784480	I mean, you can explain this in many rational ways. I think that the indentation is beautiful,
784480	788800	but that's not a rational explanation. But I can defend it rationally. So first of all,
789440	795200	Python 1 has millions of programmers. It's huge. It's everywhere. It owns machine learning. So
796000	801840	factually, it is the thing, right? Second of all, if you look at it, C codes, C++ code, Java,
801840	807920	whatever, Swift, curly brace languages also run through formatting tools and get indented.
809040	813040	And so if they're not indented correctly, first of all, it will twist your brain around.
814000	818720	It can lead to bugs. There's notorious bugs that have happened across time where the indentation
818720	823440	was wrong or misleading and it wasn't formatted, right? And so it turned into an issue, right?
823440	828320	And so what ends up happening in modern large-scale code bases is people run automatic formatters.
829040	834000	So now what you end up with is indentation and curly braces. Well, if you're going to have
836320	841360	the notion of grouping, why not have one thing, right? And get rid of all the clutter and have
841360	844480	a more beautiful thing, right? Also, you look at many of these languages, it's like, okay, well,
844480	848880	you can have curly braces or you can omit them if there's one statement or you just enter this
848880	853840	entire world of complicated design space that objectively you don't need if you have Python
853840	858320	style indentation. So yeah, I would love to actually see statistics on errors made because
858320	864880	of indentation. Like how many errors are made in Python versus in C++ that have to do with basic
864880	868880	formatting, all that kind of stuff. I would love to see. I think it's probably pretty minor because
868880	874080	once you get like, you use VS code, I do too. So if you get VS code set up, it does the indentation
874080	877920	for you generally. Right. And so you don't, you know, it's actually really nice to not have to
878560	883680	fight it. And then what you can see is the editor is telling you how your code will work by indenting
883680	891280	it, which I think is pretty cool. I honestly don't think I've ever, I don't remember having an error
891280	895600	in Python because I indented stuff wrong. So I mean, I think that there's, again, this is a religious
895600	901200	thing. And so I can joke about it. And I love, I love to kind of, you know, I realize that this is
901200	905520	such a polarizing thing and everybody wants to argue about it. And so I like poking at the bear
905520	910320	a little bit, right? But, but frankly, right, come back to the first point, Python one,
910320	915120	like it's huge, it's an AI, it's the right thing. For us, like we see mojo as being an incredible
915120	920320	part of the Python ecosystem, we're not looking to break Python or change it or quote unquote,
920320	925200	fix it. We love Python for what it is. Our view is that Python is just not done yet.
926240	929360	And so if you look at, you know, you mentioned Python being slow, well, there's a couple of
929360	932960	different things that go into that, which we can talk about if you want. But one of them is it just
932960	938480	doesn't have those features that you would use to do C like programming. And so if you say, okay,
938480	943760	well, I'm forced out of Python into C for certain use cases. Well, then what we're doing is we're
943760	948640	saying, okay, well, why, why is that? Can we just add those features that are missing from Python back
948640	952880	up to mojo? And then you can have everything that's great about Python, all the things you're talking
952880	959040	about the love, plus not be forced out of it when you do something a little bit more computationally
959120	962320	intense or weird or hardwarey or whatever it is that you're doing.
962880	967600	Well, a million questions. I want to ask what high level again, is it compiled or is it an
967600	972240	interpretive language? So Python is just in time compilation. What's, what's mojo?
973840	978320	So mojo, the complicated answer does all the things. So it's interpreted, it's just compiled
978320	986000	and it's statically compiled. And so this is for a variety of reasons. So one of the things that
986000	991760	makes Python beautiful is that it's very dynamic. And because it's dynamic, one of the things they
991760	996320	added is that it has this powerful metaprogramming feature. And so if you look at something like
996320	1003040	PyTorch or TensorFlow, or, or, I mean, even a simple, simple use case, like you'd find a class
1003040	1008720	that has the plus method, right, you can overload the Dunder methods like Dunder add, for example,
1008720	1012880	and then the plus method works on your class. And so it has very nice and very expressive,
1013680	1018800	dynamic metaprogramming features. In mojo, we want all those features come in. Like,
1018800	1022800	we don't want to break Python, we want it all to work. But the problem is, is you can't run those
1022800	1030160	super dynamic features on an embedded processor, or on a GPU, right? Or if you could, you probably
1030160	1034560	don't want to just because of the performance. And so we entered this question of saying,
1034560	1040880	okay, how do you get the power of this dynamic metaprogramming into a language that has to be
1040880	1045760	super efficient in specific cases? And so what we did was we said, okay, we'll take that interpreter,
1045760	1050080	Python has an interpreter in it, right, take that interpreter and allow it to run it compile time.
1051360	1055360	And so now what you get is you get compile time metaprogramming. And so this is super
1055360	1060880	interesting, super powerful, because one of the big advantages you get is you get Python style
1060880	1065840	expressive APIs, you get the ability to have overloaded operators. And if you look at what
1065840	1069840	happens inside of like PyTorch, for example, with automatic differentiation and eager mode,
1069840	1074000	and like all these things, they're using these really dynamic and powerful features at runtime.
1074560	1077600	But we can take those features and lift them so that they run at compile time.
1078160	1085280	So you're, because C++ has metaprogramming with templates, but it's really messy.
1085280	1090560	It's super messy. It's always, it was accidentally, I mean, different people have different
1090560	1096080	interpretations. My interpretation is that it was made accidentally powerful. It was not designed
1096080	1100480	to be terrain complete, for example, but that was discovered kind of along the way accidentally.
1101360	1106000	And so there have been a number of languages in the space. And so they usually have templates
1106560	1109440	or code instantiation, code copying features of various sorts.
1110880	1115360	Some more modern languages, or some more newer newer languages, let's say, like, you know,
1115360	1122640	they're fairly unknown, like zig, for example, says, okay, well, let's take all of those types
1122640	1127120	so you can run it, all those things you can do at runtime and allow them to happen at compile time.
1128160	1133440	And so one of the problems with C++, I mean, which is one of one of the problems with C++
1134000	1137040	Here we go. Wrong words. We're going to offend everybody today.
1137040	1140480	Oh, it's okay. I mean, everybody hates me for a variety of reasons. Anyways, I'm sure, right?
1141520	1146000	I've written enough the way they show love. I've written enough C++ code to earn a little bit
1146000	1151600	of grumpyness with C++. But, but one of the problems with it is that the metaprogramming
1151600	1157440	system templates is just a completely different universe from the normal runtime programming
1157440	1161280	world. And so if you do metaprogramming and programming, it's just like a different universe,
1161280	1166320	different syntax, different concepts, different stuff going on. And so again, one of our goals
1166320	1171200	with Mojo is to make things really easy to use, easy to learn. And so there's a natural stepping
1171200	1176320	stone. And so as you do this, you say, okay, well, I have to do programming at runtime,
1176400	1180400	after you're programming at compile time. Why are these different things?
1181200	1185920	How hard is that to pull it out? Because that sounds to me as a fan of metaprogramming in C++
1185920	1191040	even. How hard is it to pull that off? That sounds really, really exciting, because you can do the
1191040	1195360	same style programming at compile time and at runtime. That's really, really exciting.
1195360	1201280	Yep. And so I mean, in terms of the compiler implementation details, it's hard. I won't be
1201280	1205840	shy about that. It's super hard. It requires, I mean, what Mojo has underneath the covers is a
1205840	1210640	completely new approach to the design of the compiler itself. And so this builds on these
1210640	1215600	technologies like MLIR that you mentioned, that also includes other like caching and other
1216400	1219120	interpreters and jit compilers and other stuff like that.
1219120	1220560	Do you have like an interpreter inside the compiler?
1220560	1228800	Within the compiler, yes. And so it really takes the standard model of programming languages and
1228800	1234080	kind of twists it and unifies it with the runtime model, right, which I think is really cool. And
1234080	1238080	to me, the value of that is that, again, many of these languages have metaprogramming features,
1238080	1240800	like they grow macros or something, right? You list, right?
1241760	1242240	Yes.
1242240	1248320	I know your roots, right? And this is a powerful thing, right? And so if you go back to list,
1248320	1252560	one of the most powerful things about it is that it said that the metaprogramming and the programming
1252560	1257040	are the same, right? And so that made it way simpler, way more consistent, way easier to
1257040	1260880	understand reason about, and it made it more composable. So if you build a library, you can use
1260960	1264240	it both at runtime and compile time, which is pretty cool.
1264240	1270560	Yeah. And for machine learning, I think metaprogramming, I think we could generally say is extremely
1270560	1277200	useful. And so you get features, I mean, I'll jump around, but there's the feature of auto-tuning
1277200	1280320	and adaptive compilation just blows my mind.
1280320	1281920	Yeah. Well, so, okay, so let's come back to that.
1281920	1282640	Okay, all right.
1282640	1286480	So what is machine learning? Like, what is a machine learning model? Like,
1286480	1290800	you take a PyTorch model off there, right? It's really interesting to me because what
1291360	1295520	PyTorch and what TensorFlow and all these frameworks are kind of pushing compute into,
1295520	1300800	is they're pushing into like this abstract specification of a compute problem,
1300800	1303280	which then gets mapped in a whole bunch of different ways, right?
1303280	1306800	And so this is why it became a metaprogramming problem, is that you want to be able to say,
1306800	1314400	cool, I have this neural net, now run with batch size 1000, right? Do a mapping across batch,
1314400	1319520	or okay, I want to take this problem now running across 1000 CPUs or GPUs, right?
1319520	1325280	And so like, this problem of like, describe the compute and then map it and do things and
1325280	1329200	transform it are like, actually, it's very profound. And that's one of the things that
1329200	1331200	makes machine learning systems really special.
1332080	1336720	Maybe can you describe auto-tuning and how do you pull off? I mean, I guess adaptive
1336720	1339440	compilation is what we're talking about as metaprogramming.
1339440	1339600	Yeah.
1339600	1343840	How do you pull off auto-tuning? I mean, is that as profound as I think it is? It just seems like
1343840	1350080	I really like, you know, we'll mention list comprehensions to me, from a quick glance at
1350080	1357440	Mojo, which by the way, I have to absolutely like dive in. As I realize how amazing this is,
1357440	1363120	I absolutely must dive in. That looks like just an incredible feature for machine learning people.
1363120	1367680	Yeah. Well, so what is auto-tuning? So take a step back. Auto-tuning is a feature in Mojo.
1367680	1372320	It's not, so very little of what we're doing is actually research. Like many of these ideas
1372880	1376240	have existed in other systems and other places. And so what we're doing is we're pulling together
1376240	1380800	good ideas, remixing them, and making them into hopefully a beautiful system, right?
1381440	1388080	And so auto-tuning, the observation is that it turns out hardware systems, algorithms are really
1388080	1392080	complicated. It turns out maybe you don't actually want to know how the hardware works,
1393120	1397440	right? A lot of people don't, right? And so there are lots of really smart hardware people.
1397440	1402960	I know a lot of them, where they know everything about, okay, that the cache size is this and
1402960	1406480	the number of registers is that. And if you use this, what length of vector, it's going to be
1406480	1409920	super efficient because it maps directly onto what it can do. And like all this kind of stuff,
1409920	1414400	or the GPU has SMs and it has a warp size of whatever, right? All the stuff that goes into
1414400	1421920	these things or the tile size of a TPU is 128, like these factoids, right? My belief is that
1421920	1425920	most normal people, and I love hardware people also, I'm not trying to offend literally everybody
1425920	1432000	on the internet, but most programmers actually don't want to know this stuff, right? And so if
1432000	1436480	you come at it from the perspective of how do we allow people to build both more abstracted,
1436480	1441200	but also more portable code, because, you know, could be that the vector length changes or the
1441200	1444960	cache size changes, or it could be that the tile size of your matrix changes or the number,
1444960	1449920	you know, an A100 versus an H100 versus a Volta versus a whatever GPU have different
1449920	1455040	characteristics, right? A lot of the algorithms that you run are actually the same, but the
1455040	1459440	parameters, these magic numbers you have to fill in end up being really fiddly numbers that an
1459440	1464560	expert has to go figure out. And so what autotuning does, it says, okay, well, guess what? There's
1464560	1469600	a lot of compute out there, right? So instead of having humans go randomly try all the things or
1469600	1474560	do a grid search or go search some complicated multi-dimensional space, how about we have
1474560	1479040	computers do that, right? And so what autotuning does is you can say, hey, here's my algorithm.
1479920	1484240	If it's a matrix operation or something like that, you can say, okay, I'm going to
1484320	1489840	carve it up into blocks. I'm going to do those blocks in parallel. And I want this with 128 things
1489840	1494000	that I'm running on, I want to cut it this way or that way or whatever. And you can say, hey, go see
1494000	1499360	which one's actually empirically better on the system. And then the result of that, you cache for
1499360	1506000	that system. Yep. You save it. And so come back to twisting your compiler brain, right? So not only
1506000	1510800	does the compiler have an interpreter that's used to do metaprogramming, that compiler that
1510800	1515360	interpreter that metaprogramming now has to actually take your code and go run it on a target
1515360	1521200	machine. See which one likes the best and then stitch it in and then keep going, right? So part
1521200	1525680	of the compilation is machine specific. Yeah. Well, so I mean, this is an optional feature, right?
1525680	1530400	So you don't have to use it for everything. But yeah, if you're, so one of, one of the things
1530400	1535440	that we're in the quest of is ultimate performance. Yes. Right. And ultimate performance is important
1535440	1539280	for a couple of reasons, right? So if you're an enterprise, you're looking to save cost and compute
1539280	1544080	and things like this, ultimate performance translates to, you know, fewer servers. Like,
1544080	1549280	if you care about the environment, hey, better performance leads to more efficiency. Right?
1549280	1552800	I mean, you could joke and say like, you know, Python's bad for the environment.
1553760	1557760	Right. And so if you move to Mojo, it's like at least 10x better just out of the box and
1557760	1563120	keep going, right? But, but performance is also interesting because it leads to better products.
1563760	1567840	And so in the space of machine learning, right, if you reduce the latency of a model,
1568480	1572640	so it runs faster. So every time you query the server running the model, it takes less time.
1572640	1576880	Well, then the product team can go and make the model bigger. Well, that's actually makes it,
1576880	1581360	so you have a better experience as a customer. And so a lot of people care about that.
1581360	1586320	So for auto tuning, for like tile size, you mentioned 128 for TPU, you would specify like a
1586320	1592160	bunch of options to try. Just in the code. It's just a simple statement. And then you just set
1592160	1596720	and forget and know, depending on where it compiles, it'll actually be the fastest.
1596960	1600080	And yeah, exactly. And the beauty of this is that it helps you in a whole bunch of different
1600080	1604240	ways, right? So if you're building, so often what'll happen is that, you know, you've written a
1604240	1607840	bunch of software yourself, right? You, you wake up one day, you say, I have an idea,
1607840	1613760	I'm going to go code up some code. I get to work. I forget about it. I move on with life. I come
1613760	1617360	back six months or a year or two years or three years later, you dust it off and you go use it
1617360	1622320	again in a new environment. And maybe your GPU is different. Maybe you're running on a server
1622320	1627280	instead of a laptop, maybe whatever, right? And so the problem now is you say, okay, well,
1627280	1630400	I mean, again, not everybody cares about performance. But if you do, you say, okay,
1630400	1634320	well, I want to take advantage of all these new features. I don't want to break the old thing,
1634320	1640400	though. Right. And so the typical way of handling this kind of stuff before is, you know, if you're
1640400	1645360	talking about C++ templates, or you're talking about C with macros, you end up with if deaths,
1645360	1649440	you get like all these weird things get layered in, make the code super complicated. And then how
1649520	1654400	do you test it? Right? Because this crazy complexity, multi-dimensional space you have to
1654400	1660160	worry about. And, you know, that just doesn't scale very well. Actually, let me just jump around
1660160	1665280	before I go to some specific features. Like the increase in performance here that we're talking
1665280	1675280	about can be just insane. You write that Moja can provide a 35,000 x speed up over Python.
1675920	1683840	How does it do that? Yeah, so it can even do more. But we'll get to that. So first of all,
1683840	1688240	when we say that we're talking about what's called C Python, it's the default Python that
1688240	1693200	everybody uses when you type Python three, that's like typically the one you use, right? C Python is
1693200	1699280	an interpreter. And so interpreters, they have an extra layer of like byte codes and things like this
1699280	1703360	that they have to go read, parse, interpret and make some kind of slow from that perspective.
1703440	1708560	And so one of the first things we do is we move to a compiler. And so I'm just moving to a compiler
1708560	1713360	getting the interpreter out of the loop is two to five to 10 x speed up depending on the code.
1713360	1720480	So just out of the gate, just using more modern techniques, right? Now, if you do that, one of
1720480	1726320	the things you can do is you can start to look at how C Python started to lay out data. And so one
1726320	1731760	of the things that C Python did, and this isn't part of the Python spec necessarily, but this is
1732160	1739120	sets of decisions, is that if you take an integer, for example, it'll put it in an object. In Python,
1739120	1745040	everything's an object. And so they do the very logical thing of keeping the memory representation
1745040	1750720	of all objects the same. So all objects have a header, they have like payload data. And what this
1750720	1754320	means is that every time you pass around an object, you're passing around a pointer to the data.
1755360	1760160	Well, this has overhead. It turns out that modern computers don't like chasing pointers very much
1760160	1764320	in things like this. It means that you have to allocate the data. It means you have to reference
1764320	1768720	count it, which is another way that Python uses to keep track of memory. And so this has a lot
1768720	1776000	of overhead. And so if you say, okay, let's try to get that out of the heap, out of a box, out of
1776000	1783520	an indirection, and into the registers. That's that's another 10 x. So it adds up if you if
1783520	1788320	you're reference counting every single every single thing you create that adds up. Yep. And
1788320	1791920	if you look at, you know, people complain about the Python Gil, this is one of the things that
1791920	1797440	hurts parallelism. That's because the reference counting. Right. And so the Gil and reference
1797440	1801040	counting are very tightly intertwined in Python. It's not the only thing, but it's very tightly
1801040	1805600	intertwined. And so then you lean into this and you say, okay, cool, well, modern computers, they
1805600	1809760	can do more than one operation at a time. And so they have vectors. What is a vector? Well, a vector
1809760	1813600	allows you to take one, instead of taking one piece of data doing an ad or a multiply and then
1814400	1819200	pick up the next one, you can now do four or eight or 16 or 32 at a time. Right. Well, Python
1819200	1823200	doesn't expose that because of reasons. And so now you can say, okay, well, you can adopt that.
1823920	1827360	Now you have threads. Now you have like additional things like you can control memory
1827360	1831120	hierarchy. And so what mojo allows you to do is it allows you to start taking advantage of
1831120	1834800	all these powerful things have been built into the hardware over time. And it gives
1835440	1841520	the library gives very nice features. So you can say, just parallelize, let's do this in parallel.
1841920	1848160	So it's very, very powerful weapons against slowness, which is why people have been I think
1848160	1852400	having fun like just taking code and making go fast because it's just kind of an adrenaline rush
1852400	1855920	to see like how fast you can get things. Before I talk about some of the interesting
1855920	1860240	stuff with parallelization, all that, let's first talk about like the basics. We talked
1860240	1865920	about indentation, right? So this thing looks like Python. It's sexy and beautiful like Python,
1865920	1870720	as I mentioned. Is it a typed language? So what's the role of types?
1870720	1877280	Yeah, good question. So Python has types. It has strings as integers, it has dictionaries and
1877280	1882560	like all that stuff. But they all live at runtime. Right. And so because all those types live at
1882560	1888000	runtime and Python, you never, you don't have to spell them. Python also has like this whole
1888000	1891680	typing thing going on now. And a lot of people use it. Yeah, I'm not talking about that. That's
1891680	1895200	kind of a different thing. We can go back to that if you want. But, but typically the,
1895840	1900800	you know, you just say I take, I have a def and my def takes two parameters. I'm going to call them
1900800	1907120	A and B and I don't have to write a type. Okay. So that is great. But what that does is that forces
1907120	1911680	what's called a consistent representation. So these things have to be a pointer to an object
1911680	1916320	with the object header, and they all have to look the same. And then when you dispatch a method,
1916320	1920400	you go through all the same different paths, no matter what the receiver or whatever that type
1920400	1925760	is. So what Mojo does is it allows you to have more than one kind of type. And so what it does
1925760	1930080	is allows you to say, okay, cool, I have, I have an object and objects behave like Python does. And
1930080	1934480	so it's fully dynamic and that's all great. And for many things classes, like that's all very
1934480	1939760	powerful and very important. But if you want to say, Hey, it's an integer, and it's 32 bits or 64
1939760	1945200	bits or whatever it is, or it's a floating point value, it's 64 bits. Well, then the compiler can
1945200	1949680	take that and it can use that to do way better optimization. And it turns out again, getting
1949680	1954160	rid of the interactions, that's huge, means you can get better code completion because you have,
1954800	1959440	because compiler knows what the type is. And so it knows what operations work on it. And so that's
1959440	1966400	actually pretty huge. And so what Mojo does allows you to progressively adopt types into your program.
1966400	1970800	And so you can start again, it's compatible with Python. And so then you can add however many types
1970800	1974000	you want, wherever you want them. And if you don't want to deal with it, you don't have to deal with
1974000	1980880	it. Right. And so one of one of, you know, our opinions on this is it's not that types are the
1980880	1984240	right thing or the wrong thing. It's that they're a useful thing.
1985360	1989200	Which was kind of optional. It's not strict typing, you don't have to specify type.
1989200	1994400	Exactly. Okay, so starting from the thing that Python is kind of reaching towards right now with
1995040	1999680	trying to inject types into it. Yeah, with a very different approach. But yes.
2000080	2002800	What's the different approach? I'm actually one of the people
2004320	2006720	that have not been using types very much in Python.
2006720	2008480	That's okay. Why did you say?
2009520	2016800	It just, well, because I know the importance it's like adults use strict typing. And so I refuse
2016800	2023600	to grow up in that sense. It's a kind of rebellion. But I just know that it probably reduces the
2023600	2027680	amount of errors even just for forget about performance improvements, it probably reduces
2027680	2031040	errors when you do strict typing. Yeah, so I mean, I think it's interesting if you look at that,
2031040	2037600	right? And the reason I'm giving a hard time is that there's this cultural norm, this pressure,
2037600	2041600	this like, there has to be a right way to do things. Like, you know, grown-ups only do it one
2041600	2045520	way. And if you don't do that, you should feel bad. Right. Like some people feel like Python's a
2045520	2049120	guilty pleasure or something. And it's like, when it gets serious, I need to go rewrite it.
2051120	2054400	I mean, cool. I understand history and I understand kind of where this comes from. But
2055040	2059120	I don't think it has to be guilty pleasure. Yeah. So if you look at that, you say,
2059120	2062880	why do you have to rewrite it? Well, you have to rewrite it to deploy. Well, why do you want to
2062880	2067680	deploy? Well, you care about performance, you care about productivity, or you want, you know,
2067680	2071120	a tiny thing on the server that has no dependencies, or, you know, you have
2071120	2076720	objectives that you're trying to attain. So what if Python can achieve those objectives?
2077520	2080320	So if you want types, well, maybe you want types because you want to make sure you're
2080320	2085680	passing the right thing. Sure, you can add a type. If you don't care, you're prototyping some stuff,
2085680	2088880	you're hacking some things out, you're like pulling some MAM code off the internet,
2088880	2094240	it should just work. Right. And you shouldn't be like pressured. You shouldn't feel bad
2094240	2098720	about doing the right thing or the thing that feels good. Now, if you're in a team, right,
2098720	2103680	you're working at some massive internet company and you have 400 million lines of Python code,
2103680	2107760	well, they may have a house rule that you use types. Yeah. Right. Because it makes it easier
2107760	2111920	for different humans to talk to each other and understand what's going on and bugs at scale.
2111920	2116880	Right. And so there are lots of good reasons why you might want to use types. But that doesn't
2116880	2120480	mean that everybody should use them all the time. Right. So what Mojo does is it says, cool, well,
2121200	2125600	allow people to use types. And if you use types, you get nice things out of it. Right. You get
2125600	2132160	better performance and things like this. Right. But Mojo is a full compatible superset of Python.
2132960	2138400	Right. And so that means it has to work without types. It has to support all the dynamic things,
2138400	2143200	has to support all the packages, has to support for comprehension, list comprehensions and things
2143200	2147680	like this. Right. And so that starting point, I think, is really important. And I think that,
2148800	2152560	again, you can look at why it cares so much about this. And there's many different aspects of that,
2152560	2157520	one of which is the world went through a very challenging migration from Python 2 to Python 3.
2158400	2163920	Right. And this migration took many years. And it was very painful for many teams. Right.
2163920	2169600	And there's a lot of things that went on in that. I'm not an expert in all the details. I honestly
2169600	2174480	don't want to be. I don't want the world to have to go through that. Right. And people can ignore
2174480	2178400	Mojo. And if it's not their thing, that's cool. But if they want to use Mojo, I don't want them
2178400	2182160	to have to rewrite all their code. Yeah. I mean, this, okay, the superset part is,
2183120	2187600	it's just, I mean, there's so much brilliant stuff here. That definitely is incredible.
2188880	2192240	We'll talk about that. Yeah. First of all, how's the typing implemented differently
2192960	2199440	in Python versus Mojo? Yeah. So this heterogeneous flexibility,
2199440	2203760	you said, is definitely implemented. Yeah. So I'm not a full expert in the whole
2203760	2207600	backstory on types in Python. So I'll give you that. I can give you my understanding.
2207760	2214400	My understanding is basically like many dynamic languages, the ecosystem went through a phase
2214400	2221200	where people went from ranked scripts to ranked large scale, huge code bases in Python. And at
2221200	2225600	scale, it kind of helps have types. Yeah. People want to be able to reason about interfaces,
2225600	2230080	what do you expect to string or an int or like, what are these basic things, right?
2230080	2234720	And so what the Python community started doing is it started saying, okay, let's have tools on the
2234720	2241280	side, checker tools, right? The go and like enforce and variance, check for bugs, try to
2241280	2245840	identify things. These are called static analysis tools generally. And so these tools run over
2245840	2249680	your code and try to look for bugs. What ended up happening is there's so many of these things,
2249680	2253520	so many different weird patterns and different approaches on specifying the types and different
2253520	2257840	things going on that the Python community realized and recognized, hey, hey, hey, there's a thing here.
2258720	2262720	And so what they started to do is they started to standardize the syntax for adding types to Python.
2263360	2266960	Now, one of the challenges that they had is that they're coming from kind of this fragmented
2266960	2270640	world where there's lots of different tools, they have different trade-offs and interpretations
2270640	2274880	and the types mean different things. And so if you look at types in Python, according to the
2274880	2281360	Python spec, the types are ignored, right? So according to the Python spec, you can write
2281360	2289280	pretty much anything in a type position, okay? And technically, you can write any expression,
2289280	2294480	okay? Now, that's beautiful because you can extend it, you can do cool things, you can
2294480	2298720	write, build your own tools, you can build your own house, linter or something like that, right?
2298720	2304160	But it's also a problem because any existing Python program may be using different tools
2304160	2308720	and they have different interpretations. And so if you adopt somebody's package into your ecosystem,
2308720	2312880	try around the tool you prefer, it may throw out tons of weird errors and warnings and problems
2312880	2317520	just because it's incompatible with how these things work. Also because they're added late
2317520	2321120	and they're not checked by the Python interpreter, it's always kind of more of a hint than it is a
2321120	2327280	requirement. Also, the C Python implementation can't use them for performance. And so it's really
2327280	2331360	that's a big one, right? So you can't utilize the for the compilation for the just entire
2331360	2335200	compilation. Okay, exactly. And this all comes back to the design principle of it's,
2335760	2339200	it's kind of, they're kind of hints, they're kind of the definition is a little bit murky,
2339200	2343280	it's unclear exactly the interpretation in a bunch of cases. And so because of that, you can't
2343280	2348240	actually, even if you want to, it's really difficult to use them to say like it is going
2348240	2352240	to be an int. And if it's not, it's a problem, right? A lot of code would break if you did that.
2352960	2357040	So, so in mojo, right, so you can still use those kind of type annotations, it's fine.
2357040	2362960	But in mojo, if you declare a type and you use it, then it means it is going to be that type.
2362960	2368080	And the compiler helps you check that and force it and it's safe. And it's not it's not a like
2369040	2374240	best effort hint kind of a thing. So if you try to shove a string type thing into a integer,
2374240	2381840	you get an error from the compiler, compile time. Nice. Okay, what kind of basic types are there?
2381840	2389200	Yeah, so mojo is pretty hardcore in terms of what it tries to do in the language,
2389760	2396320	which is the philosophy there is that we, again, if you, if you look at Python, right,
2396320	2400880	Python's beautiful language because it's so extensible, right? And so all of the different
2400880	2404480	things in Python like for loops and plus and like all these things can be
2405040	2410240	accessed through these underbar and bar methods. Okay, so you have to say, okay,
2410240	2412960	if I make something that is super fast, I can go all the way down to the metal.
2413600	2415760	Why do I need to have integers built into the language?
2416960	2420720	Right. And so what mojo does is it says, okay, well, we can have this notion of structs. So
2420800	2426160	we have classes in Python, now you can have structs. Classes are dynamic, structs are static.
2427040	2431440	Cool, we can get high performance, we can write C++ kind of code with structs if you want.
2431440	2435680	These things mix and work beautifully together. But what that means is that you can go and
2435680	2440560	implement strings and ints and floats and arrays and all that kind of stuff in the language.
2441360	2449280	Right. And so that's really cool because, you know, to me as a ideal, idealizing compiler language
2449280	2453360	type of person, what I want to do is I want to get magic out of the compiler and put in the
2453360	2457760	libraries. Because if somebody can, you know, if we can build an integer that's beautiful and it
2457760	2462400	has an amazing API and does all the things you'd expect an integer to do, but you don't like it,
2463040	2466880	maybe you want a big integer, maybe you want like sideways integer, I don't know, like what
2466880	2473280	all the space of integers are, then you can do that and it's not a second class citizen.
2473520	2479680	And so if you look at certain other languages like C++, one I also love and use a lot,
2481040	2488640	int is hard code in the language, but complex is not. And so it's kind of weird that, you know,
2488640	2494960	you have this std complex class, but you have int and complex tries to look like a natural
2494960	2500240	numeric type and things like this. But integers and floating point have these like special promotion
2500240	2503680	rules and other things like that that are magic and they're hacked into the compiler.
2503680	2506560	And because of that, you can't actually make something that works like the built-in types.
2507520	2512960	Is there something provided as a standard because, you know, because it's AI first,
2513760	2518560	you know, numerical types are so important here. So is there something like a nice
2519200	2523280	standard implementation of integer and float? Yeah, so we're still building all this stuff out. So we
2523280	2527200	provide integers and floats and all that kind of stuff. We also provide like buffers and tensors
2527200	2532560	and things like that that you'd expect in an ML context. Honestly, we need to keep designing and
2532560	2535280	redesigning and working with the community to build that out and make that better. That's not
2535280	2539520	our strength right now. Give us six months or a year and I think it'll be way better. But
2540720	2544800	the power of putting in the library means that we can have teams of experts that aren't compiler
2544800	2550160	engineers that can help us design and refine and drive us forward. So one of the exciting things
2550160	2558640	we should mention here is that this is new and fresh. This cake is unbaked. It's almost baked.
2558640	2564240	You can tell it's delicious, but it's not fully ready to be consumed. Yep, that's very fair. It is
2564240	2568080	very useful, but it's very useful if you're a super low-level programmer right now. And what
2568080	2571680	we're doing is we're working our way up the stack. And so the way I would look at Mojo
2572160	2580240	today in May and 2023 is that it's like a 0.1. So I think that a year from now,
2580240	2585360	it's going to be way more interesting to a variety of people. But what we're doing is we
2585360	2589040	decided to release it early so that people can get access to it and play with it. We can build
2589040	2595520	it with the community. We have a big roadmap fully published, being transparent about this,
2595520	2599040	and a lot of people are involved in this stuff. And so what we're doing is we're really optimizing
2599040	2603920	for building this thing the right way. And building it the right way is kind of interesting
2603920	2610880	working with the community because everybody wants it yesterday. And so sometimes there's
2610880	2615440	some dynamics there, but I think it's the right thing. So there's a discord also,
2615440	2620080	so the dynamics is pretty interesting. Sometimes the community probably can be very chaotic
2621600	2627760	and introduce a lot of stress. Guido famously quit over the stress of the Walrus operator.
2629040	2634720	Broke, straw the brook with camels there. Exactly. And so it could be very stressful to
2634720	2642800	develop. But can you just add tangent upon a tangent? Is it stressful to work through the
2642800	2647120	design of various features here given that the community is so racially involved?
2647120	2652720	Well, so I've been doing open development and community stuff for decades now. Somehow this
2652720	2657680	has happened to me. So I've learned some tricks. But the thing that always gets me is I want to
2657680	2664160	make people happy. And so this is maybe not all people all happy all the time. But generally,
2664160	2669760	I want people to be happy. And so the challenge is that, again, we're tapping into some long,
2670800	2676480	some deep-seated long tensions and pressures both in the Python world, but also in the AI world,
2676480	2680000	in the hardware world, and things like this. And so people just want us to move faster.
2680960	2687440	And so, again, our decision was let's release this early. Let's get people used to it or access to
2687440	2694800	and play with it. And let's build in the open, which we could have had the language monk sitting
2694800	2700400	in the cloister up on the hilltop bevaring away trying to build something. But in my experience,
2700400	2702640	you get something that's way better if you work with the community.
2704160	2707440	And so, yes, it can be frustrating. It can be challenging for lots of people involved. And
2708800	2713200	if you mention our Discord, we have over 10,000 people on the Discord, 11,000 people or something.
2713200	2721120	Keep in mind, we released Mojo like two weeks ago. So it's very cool. But what that means is that
2722640	2728320	10, 11,000 people all will want something different. And so what we've done is we've tried to say,
2728320	2734320	okay, cool, here's our roadmap. And the roadmap isn't completely arbitrary. It's based on,
2734320	2738160	here's the logical order in which to build these features or add these capabilities and
2738160	2742960	things like that. And what we've done is we've spun really fast on bug fixes. And so we actually
2742960	2748400	have very few bugs, which is cool. I mean, actually for projects in the state. But then
2748400	2751120	what we're doing is we're dropping in features very deliberately.
2751120	2756080	I mean, this is fun to watch because you got the two gigantic communities of like hardware,
2756080	2762000	like systems engineers. And then you have the machine learning Python people that are like
2762000	2769760	higher level. And it's just too like army, like they've been at war. Yeah, they've been at war.
2770480	2774480	Right. And so here's a Tolkien novel or something. Okay, so here's the test. Again,
2774480	2778720	like it's super funny for something that's only been out for two weeks, right? People are so
2778720	2784640	impatient, right? But okay, cool. Let's fast forward a year. Like any year's time, Mojo will be
2784640	2790240	actually quite amazing and solve tons of problems and be very good. People still have these problems.
2791040	2795680	Right. And so you look at this and you say, and the way I look at this at least is to say,
2795680	2802640	okay, well, we're solving big longstanding problems. To me, I, again, working on many
2802640	2806400	different problems, I want to make sure we do it right. Right? There's like a responsibility you
2806400	2811600	feel because if you mess it up, right? There's very few opportunities to do projects like this
2811600	2816000	and have them really have impact on the world. If we do it right, then maybe we can take those
2816000	2820880	feuding armies and actually heal some of those wounds. Yeah, this is like, this feels, this
2820880	2825760	feels like a speech by George Washington or Abraham Lincoln or something. And you look at
2825760	2829760	this and it's like, okay, well, how different are we? Yeah. We all want beautiful things. We all
2829760	2832640	want something that's nice. We all want to be able to work together. We all want our stuff to be
2832640	2838000	used, right? And so if we can help heal that, now I'm not optimistic that all people will use Mojo
2838000	2842720	and they'll stop using C++. Like that's not my goal, right? But if we can heal some of that,
2842720	2847600	I think that'd be pretty cool. Yeah. And we start by putting the people who like braces into the
2847600	2855360	gulag. No. So there are proposals for adding braces to Mojo. We just tell them no. Okay.
2857440	2861920	Politely. Yeah. Anyway, so there's a lot of amazing features on the roadmap and those ready
2861920	2868080	to implement it. It'd be awesome. I can just ask you a few things. So the other performance
2868080	2873920	improvement comes from immutability. So what's the what's this var and this let thing that we got
2874000	2881200	going on? What's immutability? Yeah. So one of the things that is useful and it's not always
2881200	2884800	required, but it's useful is knowing whether something can change out from underneath you.
2885440	2890640	Right. And so in Python, you have a pointer to an array, right? And so you pass that pointer
2890640	2896480	to an array around to things. If you pass into a function, maybe take that and scroll away in
2896480	2901040	some other data structure. And so you get your array back and you go to use it. Now somebody else
2901040	2904880	is like putting stuff in your array. How do you reason about that? Because to be very
2905760	2910640	complicated, at least a lot of bugs, right? And so one of the things that, you know, again,
2910640	2914720	this is not something Mojo forces on you, but something that Mojo enables is a thing called
2914720	2922080	value semantics. And what value semantics do is they take collections like arrays, like dictionaries,
2922880	2928240	also tensors and strings and things like this that are much higher level and make them behave
2928320	2933600	like proper values. And so it makes it look like if you pass these things around, you get a logical
2933600	2938560	copy of all the data. And so if I pass you an array, sure array, you can go do what you want
2938560	2943680	to it. You're not going to hurt my array. Now, that is an interesting and very powerful design
2943680	2947840	principle. It defines the way a ton of bugs. You have to be careful to implement it in an efficient
2947840	2954480	way. Yeah, is there a performance hit that's significant? Generally not, if you implement it
2954480	2959840	the right way. But it requires a lot of very low level, getting the language right bits.
2960560	2964480	I assume there'll be a huge performance hit, because it's a really nice, the benefit is really
2964480	2968560	nice, because you don't get into the complex. Absolutely. Well, the trick is, is you can't do
2968560	2976240	copies. So you have to provide the behavior of copying without doing the copy. Yeah. How do you
2976240	2982240	do that? How do you do that? It's not magic. It's just, it's actually pretty cool. Well,
2982240	2986000	so first, before we talk about how that works, let's talk about how it works in Python, right?
2986000	2991200	So in Python, you define a person class, or maybe a person class is a bad idea. You define a
2991200	2995360	database class, right? And database class has an array of records, something like that, right?
2995360	3000720	And so the problem is that if you pass in a record or a class instance into the database,
3000720	3006720	it'll take a hold of that object. And then it assumes it has it. And if you're passing an object in,
3006720	3010720	you have to know that that database is going to take, take it. And therefore you shouldn't change
3010800	3013920	it after you put in the database, right? This is, this is kind of have to know that you just
3013920	3018400	have to kind of know that, right? And so you roll out version one of the database, you just kind
3018400	3022720	of have to know that. Of course, Lex uses his own database, right? Yeah, right? Because you built
3022720	3026960	it, you understand this works, right? Somebody else joins the team, they don't know this. Yes.
3026960	3031680	Right. And so now they suddenly get bugs, you're having to maintain the database, you shake your
3031680	3036560	fist, you argue the 10th time this happens, you're like, okay, we have to do something different.
3036560	3040000	Right. And so what you do is you go change your Python code, and you change your database
3040000	3045440	class to copy the record every time you add it. And so what ends up happening is you say, okay,
3045440	3050320	I will do what's called a defensive copy inside the database. And then that way,
3050320	3055440	if somebody passes something in, I will have my own copy of it. And they can go do whatever,
3055440	3060480	and they're not going to break my thing. Okay, this is usually the, the two design patterns.
3060480	3064880	If you look in PyTorch, for example, this is cloning a tensor, like there's a specific thing,
3064880	3066800	and you have to know where to call it. And if you don't call in the right place,
3066880	3072240	you get these bugs, and this is state of the art. Right. So a different approach. So it's used in
3072240	3077520	many languages. So I worked with it in Swift, as you say, okay, well, let's provide value semantics.
3077520	3083440	And so we want to provide the view that you get a logically independent copy. But we want to do
3083440	3089360	that lazily. And so what we do is you say, okay, if you pass something into a function, it doesn't
3089360	3093440	actually make a copy. What it actually does is it just increments a reference to it. And if you
3093440	3098240	pass it around, you stick in your database, you can go into the database, you own it. And then
3098240	3102320	you come back out of the stack, nobody's copied anything, you come back out of the stack, and
3102320	3107440	then the caller, let's go of it. Well, then you've just handed it off to the database,
3107440	3112880	you've transferred it, and there's no copies made. Now, on the other hand, if, you know,
3112880	3117360	your coworker goes and hands you a record, and you pass it in, you stick it in the database,
3117360	3122160	and then you go to town, and you start modifying it, what happens is you get a copy lazily
3122800	3127120	on demand. And so what this does, this gives you copies only when you need them.
3127680	3131760	And it also, so it defines the way the bugs, but it also generally reduces the number of copies
3131760	3137440	in practice. And so the implementation details are tricky here. Yes. So this is, yes, something
3137440	3144080	with reference counting, but to make it performant across the number of different kinds of
3145120	3150000	objects. Yeah, so you need a couple of things. And so there's many, so this concept has existed
3150000	3156160	in many different worlds. And so, again, it's not novel research at all, right? The magic is
3156160	3160000	getting the design right so that you can do this in a reasonable way, right? And so there's a number
3160000	3165040	of components that go into this. One is when you're passing around, so we're talking about
3165040	3169200	Python and reference counting at the expense of doing that, when you're passing values around,
3169200	3172720	you don't want to do extra reference counting for no good reason. And so you have to make
3172720	3177200	sure that you're efficient and you transfer ownership instead of duplicating references
3177200	3182400	and things like that, which is a very low level problem. You also have to adopt this and you have
3182400	3187680	to build these data structures. And so if you say, you know, Mojo has to be compatible with Python.
3187680	3193200	So of course, the default list is a reference semantic list that works the way you'd expect
3193200	3197680	in Python. But then you have to design a value semantic list. And so you just have to implement
3197680	3202480	that and then you implement the logic within. And so the role of the language here is to provide
3202480	3208080	all the low level hooks that allow the author of the type to be able to get and express this
3208080	3212800	behavior without forcing it into all cases or hard coding this into the language itself.
3212800	3217040	But there's ownership. So you're constantly transferring, you're tracking who owns the thing.
3217040	3221760	Yes. And so there's a whole system called ownership. And so this is related to work done
3221760	3225600	in the Rust community. Also the Swift community has done a bunch of work. And there's a bunch of
3225600	3230720	different other languages that evolve kind of C++ actually has copy constructors and destructors
3230720	3235520	and things like that. And so I mean, C++ has everything. So it has move constructors and
3235520	3241760	has like this whole world of things. And so this is a body of work that's kind of been developing
3241760	3247360	for many, many years now. And so Mojo takes some of the best ideas out of all these systems and
3247360	3252960	remixes it in a nice way so that you get the power of something like the Rust programming language.
3252960	3257200	But you don't have to deal with it when you don't want to, which is a major thing in terms of
3257200	3259760	teaching and learning and being able to use and scale these systems.
3260880	3265440	How does that play with argument conventions? What are they? Why are they important?
3265440	3269840	How does the value semantics? How does the transfer ownership work with the arguments
3269840	3274160	when they're passed into functions? So if you go deep into systems programming land,
3274160	3277840	so this isn't again, this is not something for everybody, but if you go deep into systems
3277840	3282640	programming land, what you encounter is you encounter these types that get weird.
3283600	3287200	So if you're used to Python, you think about everything, I can just copy it around,
3287200	3290560	I can go change it and mutate it and do these things. And it's all cool.
3292080	3296560	If you get into systems programming land, you get into these things like I have an atomic number,
3296560	3303680	or I have a mutex, or I have a uniquely owned database handle, things like this, right?
3303680	3307760	So these types, you can't necessarily copy. Sometimes you can't necessarily even move them
3307760	3312720	to a different address. And so what Mojo allows you to do is it allows you to express,
3312720	3317440	hey, I don't want to get a copy of this thing. I want to actually just get a reference to it.
3318080	3322000	And by doing that, well, you can say, as you can say, okay, if I'm defining something weird,
3322000	3329440	like a atomic number or something, it has to be, so an atomic number is an area in memory
3329440	3337200	that multiple threads can access at a time without locks. And so the definition of atomic
3337200	3340720	numbers, multiple different things have to be poking it. Therefore, they have to agree on where
3340720	3345680	it is. And so you can't just move it out from underneath one because it kind of breaks what
3345680	3351360	it means. And so that's an example of a type that you can't copy, you can't move. Once you create,
3351360	3357520	it has to be where it was. Now, if you look at many other examples, like a database handle,
3357520	3362880	so okay, well, what happens, how do you copy a database handle? Do you copy the whole database?
3362960	3368400	That's not something you necessarily want to do. There's a lot of types like that
3368400	3374000	where you want to be able to say that they are uniquely owned. So there's always one of this
3374000	3381200	thing. And or if I create a thing, I don't copy it. And so what Mojo allows you to do is it allows
3381200	3385440	you to say, hey, I want to pass around a reference to this thing without copying it. And so it has
3386000	3391760	borrowed conventions. So you can say, you can use it, but you don't get to change it. You can pass
3391840	3396320	it by mutable reference. And so if you do that, then you can, you get a reference to it, but
3396320	3402080	you can change it. And so it manages all that kind of stuff. So it's just a really nice implementation
3402080	3410080	of like C++ has, you know, the different kinds of pointers, different kinds of applications,
3410080	3414880	smart pointers that you can explicitly define this logic. But you're saying that's more like
3415600	3420000	the weird case versus the common case. Well, it depends on where, I mean, I mean,
3420480	3423760	I don't think I'm a normal person. So I mean, I'm not one to call other people weird.
3425280	3430640	But the, but, you know, if you talk to a normal Python, a typical Python programmer,
3430640	3434000	you're typically not thinking about this, right? This is a lower level of abstraction. Now,
3434000	3438240	if you talk to a C++ programmer, certainly if you talk to a Rust programmer, again, they're
3438240	3443200	not weird, they're delightful, like these are all good people, right? Those folks will think about
3443200	3448640	all the time. Right. And so I look at this as there's a spectrum between very deep low level
3448640	3452400	systems. I'm going to go poke the bits and care about how they're laid out in memory all the
3452400	3457120	way up to application and scripting and other things like this. And so it's not that anybody's
3457120	3463680	right or wrong. It's about how do we build one system that scales? By the way, the idea of an
3463680	3471680	atomic number has been something that always brought me deep happiness. Because the flip side
3471680	3481840	of that, the idea that threads can just modify stuff asynchronously, just the whole idea of
3481840	3486640	concurrent programming is a source of infinite stress for me. Well, so this is where you jump into,
3488640	3492160	you know, again, you zoom out and get out of programming languages or compilers and you just
3492160	3497600	look at what the industry has done. My mind is constantly blown by this, right? And you look
3497600	3502800	at what, you know, Moore's law, Moore's law has this idea that like computers for a long time,
3503360	3506400	single thread performance has got faster and faster and faster and faster for free.
3507040	3512400	But then physics and other things intervened and power consumption, like other things started
3512400	3517280	to matter. And so what ended up happening is we went from single core computers to multi core,
3517280	3521680	then we went to accelerators, right? And this trend towards specialization of hardware
3521680	3528480	is only going to continue. And so for years, us programming language nerds and compiler people
3528480	3532640	have been saying, okay, well, how do we tackle multi core, right? For a while, it was like,
3532640	3536720	multi core is the future, we have to get on top of this thing. And then it was multi cores to default.
3536720	3540720	What are we doing with this thing? And then it's like, there's chips with hundreds of cores in them.
3541520	3547680	What happened, right? And so I'm super inspired by the fact that, you know, in the face of this,
3548160	3553600	you know, those machine learning people invented this idea of a tensor, right? And what is a tensor?
3553600	3560160	A tensor isn't like an arithmetic and algebraic concept, it's like an abstraction around a
3560160	3565440	gigantic parallelizable data set, right? And because of that, and because of things like
3565440	3570800	TensorFlow and PyTorch, we're able to say, okay, we'll express the math of the system.
3571360	3575120	This enables you to do automatic differentiations, enables you to do like all these cool things.
3575120	3581360	And it's an abstract representation. Because you have that abstract representation,
3581360	3586960	you can now map it onto these parallel machines without having to control, okay, put that right
3586960	3591680	here, put that right there, put that right there. And this has enabled an explosion in terms of AI,
3591680	3596240	compute, accelerators, like all the stuff. And so that's super, super excited.
3596240	3603040	What about the deployment, the execution across multiple machines? So you write that
3603040	3608240	the modular compute platform dynamically partitions models with billions of parameters
3608240	3614400	and distributes their execution across multiple machines, enabling unparalleled efficiency.
3615360	3620240	By the way, the use of unparalleled in that sentence, anyway, enabling unparalleled efficiency
3620240	3628880	scale and reliability for the largest workloads. So how do you do this abstraction of distributed
3628880	3633040	deployment of large models? Yeah, so one of the really interesting
3633920	3637920	tensions. So there's a whole bunch of stuff that goes into that. I'll pick a random walkthrough.
3638800	3643120	If you go back and replay the history of machine learning, right? I mean, the brief,
3643120	3646480	the most recent history of machine learning, because this is, as you know, very deep.
3648160	3657520	I knew Lex when he had an AI podcast. So if you look at just TensorFlow and PyTorch,
3657520	3662320	which is pretty recent history in the big picture, right? But TensorFlow is all about graphs.
3662960	3668080	PyTorch, I think, pretty unarguably ended up winning. And why did it win? Mostly because
3668080	3672400	of usability, right? And the usability of PyTorch is, I think, huge. And I think, again,
3672400	3678560	that's a huge testament to the power of taking abstract theoretical technical concepts and bring
3678560	3684400	it to the masses, right? Now, the challenge with what the TensorFlow versus the PyTorch
3684560	3690400	design points was that TensorFlow is kind of difficult to use for researchers, but it was
3690400	3694320	actually pretty good for deployment. PyTorch is really good for researchers. It kind of
3694320	3700080	not super great for deployment, right? And so I think that we as an industry have been struggling.
3700080	3704560	And if you look at what deploying a machine learning model today means is that you'll have
3704560	3710160	researchers who are, I mean, wicked smart, of course, but they're wicked smart at model architecture
3710160	3716400	and data and calculus. They're wicked smart in various domains. They don't want to know anything
3716400	3720800	about the hardware or deployment or C++ or things like this, right? And so what's happened is you
3720800	3725200	get people who train the model, they throw it over the fence, and then you have people that
3725200	3732640	try to deploy the model. Well, every time you have a team A does X, they throw it over the fence,
3732640	3739840	and team B does Y, you have a problem. Because, of course, it never works the first time.
3740000	3744000	And so you throw it over the fence, they figure out, okay, it's too slow, won't fit,
3744560	3750800	doesn't use the right operator, the tool crashes, whatever the problem is, then they have to throw
3750800	3755520	it back over the fence. And every time you throw a thing over a fence, it takes three weeks of
3755520	3760400	project managers and meetings and things like this. And so what we've seen today is that getting
3760400	3765280	models in production can take weeks or months. Like it's not atypical, right? I talked to lots
3765280	3770480	of people and you talk about like VP of software at some internet company trying to deploy a model,
3770480	3776480	and they're like, why do I need a team of 45 people? It's so easy to train a model, why can't I
3776480	3783120	deploy it, right? And if you dig into this, every layer is problematic. So if you look at the language
3783120	3787600	piece, I mean, this is tip of the iceberg. It's a very exciting tip of the iceberg for folks, but
3788160	3793520	you've got Python on one side and C++ on the other side. Python doesn't really deploy. I mean,
3793600	3797680	can theoretically, technically in some cases, but often a lot of production teams will want to get
3797680	3801680	things out of Python because they get their performance and control and whatever else. So
3801680	3808080	Mojo can help with that. If you look at serving, so you talk about gigantic models, well, a gigantic
3808080	3814640	model won't fit on one machine, right? And so now you have this model, it's written Python, it has
3814640	3819120	to be rewritten in C++. Now it also has to be carved up so that half of it runs on one machine,
3819120	3825280	half of it runs on another machine, or maybe it runs on 10 machines. So now suddenly, the complexity
3825280	3831360	is exploding, right? And the reason for this is that if you look into TensorFlow, PyTorps, these
3831360	3836320	systems, they weren't really designed for this world, right? They were designed for, you know,
3836320	3841360	back in the day when we were starting and doing things where it was a different, much simpler
3841360	3846560	world, like you want to run ResNet 50 or some ancient model architecture like this. It was just a,
3846560	3848000	it was a completely different world.
3848000	3850000	Trained on 1GPU. Exactly.
3850000	3851360	Doing some 1GPU.
3851360	3858080	Yeah, AlexNet, right? The major breakthrough. And the world has changed, right? And so now the
3858080	3861920	challenge is that TensorFlow, PyTorps, these systems, they weren't actually designed for
3861920	3868400	LLMs. That was not a thing. And so where TensorFlow actually has amazing power in terms of scale and
3868400	3873040	deployment and things like that. And I think Google is, I mean, maybe not unmatched, but they're
3873280	3880160	incredible in terms of their capabilities and gigantic scale. Many researchers using PyTorps,
3880160	3883440	right? And so PyTorps doesn't have those same capabilities. And so what Modular can do is
3883440	3888000	it can help with that. Now, if you take a step back and say like, what is Modular doing, right?
3888000	3894400	So Modular has like a bitter enemy that we're fighting against in the industry. And it's one
3894400	3900160	of these things where everybody knows it, but nobody is usually willing to talk about it.
3900800	3902240	The bitter enemy.
3902240	3906960	The bitter thing that we have to destroy, that we're all struggling with, and it's like fish
3906960	3912960	can't see water, is complexity. Sure. Yes. It's complexity. That was very close.
3914880	3918640	And so if you look at it, yes, it is on the hardware side. Yes.
3918640	3922160	All these accelerators, all these software stacks that go with the accelerator,
3922160	3927840	all these massive complexity over there. You look at what's happening on the modeling side,
3928480	3931680	massive amount of complexity. Like things are changing all the time. People are inventing,
3931680	3935840	turns out the research is not done, right? And so people want to be able to move fast.
3935840	3940560	Transformers are amazing, but there's a ton of diversity even within transformers. And what's
3940560	3946800	the next transformer, right? And you look into serving also huge amounts of complexity. It turns
3946800	3952080	out that all the cloud providers, right, have all their very weird, but very cool hardware for
3952080	3956160	networking and all this kind of stuff. And it's all very complicated. People aren't using that.
3956240	3961360	You look at classical serving, right? There's this whole world of people who know how to write
3961360	3965280	high-performance servers with zero-copy networking and like all this fancy,
3966160	3970480	asynchronous I.O. and like all these fancy things in the serving community,
3971280	3975680	very little that has pervaded into the machine learning world, right? And why is that? Well,
3975680	3980640	it's because, again, these systems have been built up over many years. They haven't been
3980640	3985040	rethought. There hasn't been a first principles approach to this. And so what Modular's doing is
3985040	3990720	we're saying, okay, we've built many of these things. So I've worked on TensorFlow and TPUs
3990720	3995440	and things like that. Other folks on our team have worked on PyTorch core. We've worked on
3995440	4001040	Onyx one time. We've worked on many of these other systems. And so systems like the Apple
4001040	4006480	accelerators and all that kind of stuff, our team is quite amazing. And so one of the things that
4007120	4011600	roughly everybody at Modular's grumpy about is that when you're working on one of these projects,
4011680	4017440	you have a first order goal, get the hardware to work, get the system to enable one more model,
4017440	4022880	get this product out the door, enable this specific workload or solve this problem for
4022880	4027840	this product team, right? And nobody's been given a chance to actually do that step back.
4027840	4032320	And so we as an industry, we didn't take two steps forward. We took like 18 steps forward
4032320	4036480	in terms of all this really cool technology across compilers and systems and runtimes and
4036480	4039840	heterogeneous computing, like all this kind of stuff. And like all this technology has been,
4040720	4045760	you know, I wouldn't say beautifully designed, but it's been proven in different quadrants.
4045760	4051920	Like, you know, you look at Google with TPUs, massive, huge exoflops of compute strapped
4051920	4056240	together into machines that researchers are programming in Python in a notebook.
4056880	4060960	That's huge. That's amazing. That's incredible, right? It's incredible. And so you look at the
4060960	4066400	technology that goes into that. And the algorithms are actually quite general. And so
4067120	4071040	lots of other hardware out there and lots of other teams out there don't have the sophistication
4071040	4076960	or maybe the years working on it or the budget or whatever that Google does, right? And so
4076960	4080240	they should be getting access to the same algorithms, but they just don't have that, right?
4080240	4085920	And so what Modular's doing is we're saying, cool, this is not research anymore. Like, we've built
4085920	4090720	auto tuning in many systems. We've built programming languages, right? And so like have,
4090720	4094960	have, you know, implemented C++ have implemented Swift have implemented many of these things.
4094960	4101680	And so, you know, it's hard, but it's not research. And you look at accelerators. Well,
4101680	4106400	we know there's a bunch of different weird kind of accelerators, but they actually cluster together,
4107040	4111360	right? And you look at GPUs. Well, there's a couple of major vendors of GPUs, and they maybe
4111360	4116080	don't always get along, but their architectures are very similar. You look at CPUs. CPUs are
4116080	4120720	still super important for the deployment side of things. And you see new, new architectures coming
4120720	4124160	out from all the cloud providers and things like this, and they're all super important
4124160	4129040	to the world, right? But they don't have the 30 years of development that the entrenched people
4129040	4134880	do, right? And so what Modular can do is we're saying, okay, all this complexity, like, it's not,
4134880	4141280	it's not bad complexity. It's actually innovation, right? And so it's innovation that's happening.
4141280	4146640	And it's for good reasons. But I have sympathy for the poor software people, right? I mean, again,
4146640	4151120	I'm generally a software person too, I love hardware. But software people want to build
4151120	4156560	applications and products and solutions that scale over many years. They don't want to build a
4156560	4161360	solution for one generation of hardware with one vendor's tools. Right? And because of this,
4161360	4166480	they need something that scales with them, they need something that works on cloud and mobile,
4167600	4171600	right? Because, you know, their product manager said, Hey, I want to be at have lower latency,
4172160	4176800	it's better for personalization or whatever they decide, right? Products evolve. And so
4177440	4180880	the challenge with the machine learning technology and the infrastructure we have today,
4180880	4185680	in the industry, is that it's all these point solutions. And because they're all these point
4185680	4189280	solutions, it means that as your product evolves, you have to like switch different technology
4189280	4193280	stacks or switch to different vendor. And what that does is that slows down progress.
4194080	4201760	So basically, a lot of the things we've developed in those little silos for machine learning tasks,
4201760	4206000	you want to make that the first class citizen of a general purpose programming language,
4206000	4208560	they can then be compiled across all these kinds of hardware.
4208560	4211920	Well, so it's not really about a programming language. I mean, the program language is a
4211920	4217200	component of the mission, right? And the mission is are not literal, but our joking mission is to
4217200	4223840	save the world from terrible AI software. Excellent. Okay. So, so, you know, if you look at this
4223840	4230240	mission, you need a syntax. So that's the so yeah, she need a program language, right? And and like,
4230240	4234480	we wouldn't have to build the programming language if one existed. Right? So if Python was already
4234480	4238720	good enough, then cool, we would just used it, right? We're not just doing very large scale
4238720	4243680	expensive engineering projects for the sake of it, like, it's to solve a problem, right? It's also
4243680	4251280	about accelerators. It's also about exotic numerics and b-float 16 and matrix multiplications and
4251280	4256400	convolutions and like this, this kind of stuff. Within the stack, there are things like kernel
4256400	4262000	fusion. It's a esoteric but really important thing that leads to much better performance
4262000	4268960	and much more general research hackability together, right? And that's enabled by the ASICS,
4268960	4272480	that's enabled by certain hardware. So it's like, where's the dance between
4274480	4277760	I mean, there's several questions here, like, how do you add a piece of hardware to this stack?
4278320	4284800	Yeah. If you need pieces, like if I have this genius invention of a specialized accelerator,
4284800	4289920	how do I add that to the modular framework? And also, how does modular as a standard
4290640	4295040	start to define the kind of hardware that should be developed?
4295040	4301120	Yeah. So let me take a step back and talk about status quo. Okay. And so if you go back to TensorFlow
4301120	4307360	1, PyTorch 1, this kind of timeframe, and these have all evolved and gotten way more
4307360	4312720	complicated. So let's go back to the glorious simple days, right? These things basically were CPUs and
4312720	4319040	CUDA. And so what you do is you say, go do a dense layer and a dense layer has a matrix
4319040	4323360	multiplication, right? And so when you say that, you say, go do this big operation,
4323360	4329200	a matrix multiplication. And if it's on a GPU, kick off CUDA kernel, if it's on a CPU, go do
4330640	4333760	like an Intel algorithm or something like that with the Intel MKL. Okay.
4334560	4341120	Now, that's really cool. If you're either video or Intel, right? But then more hardware comes in.
4342160	4346640	Right. And on one axis, you have more hardware coming in. On the other hand, you have
4346640	4351840	an explosion of innovation in AI. And so what happened with both TensorFlow and PyTorch is that
4351840	4356640	the explosion of innovation in AI has led to, it's not just about matrix multiplication and
4356640	4361360	convolution. These things have now like 2,000 different operators. And on the other hand,
4361360	4364400	you have, I don't know how many pieces of hardware there are there, it's a lot.
4365440	4371760	It's not even hundreds, it's probably thousands. Okay. And across all the edge and across all the
4371760	4377520	different things that are used at scale. Yeah, exactly. I mean, it's not just like everywhere.
4377520	4383040	Yeah. It's not a handful of TPU alternatives. Correct. It's every phone, often with many
4383040	4390000	different chips inside of it from different vendors. Right. Like it's, AI is everywhere.
4390000	4393760	It's a thing, right? Why are they all making their own chips? Like, why is everybody making
4393760	4398880	their own thing? Well, so because, was that a good thing? First of all, so Chris's philosophy on
4398880	4405440	hardware. Yeah. Right. So my philosophy is that there isn't one right solution. Right. And so I
4405440	4410400	think that, again, we're at the end of Moore's Law, specialization happens. Yeah. If you're
4410400	4417280	building, if you're training GPT-5, he wants some crazy supercomputer data center thingy.
4417920	4422800	If you're making a smart camera that runs on batteries, you want something that looks very
4422800	4426240	different. If you're building a phone, you want something that looks very different. If you have
4426320	4431040	something like a laptop, you want something that looks maybe similar, but a different scale. Right.
4431040	4436560	And so AI ends up touching all of our lives, robotics. Right. And like, lots of different
4436560	4441440	things. And so as you look into this, these have different power envelopes. There's different
4441440	4445600	trade-offs in terms of the algorithms. There's new innovations in sparsity and other data formats
4445600	4450880	and things like that. And so hardware innovation, I think is a really good thing. Right. And what
4450880	4454640	I'm interested in is unlocking that innovation. There's also like analog and quantum and like,
4454640	4460720	although the really weird stuff. Right. And so if somebody can come up with a chip that uses
4460720	4465440	analog computing and it's 100x more power efficient, I think what that would mean in terms of the
4465440	4472080	daily impact on the products we use, that'd be huge. Now, if you're building an analog computer,
4472080	4477120	you may not be a compiler specialist. Right. These are different skill sets. Right. And so
4477120	4481200	you can hire some compiler people if you're running a big company, maybe. But it turns out
4481920	4487920	these are really like exotic new generation of compilers. Like this is a different thing. Right.
4487920	4492640	And so if you take a step back out and come back to it, what is the status quo? The status quo is
4492640	4498000	that if you're Intel or you're Nvidia, you continue to keep up with the industry and you chase and,
4498000	4502960	okay, there's 1900 now. There's 2000 now. There's 2100. And you have a huge team of people that
4502960	4507840	are like trying to keep up and tune and optimize. And even when one of the big guys comes out with
4507840	4512560	a new generation of their chip, they have to go back and rewrite all these things. Right. So really,
4512560	4517120	it's only powered by having hundreds of people that are all like frantically trying to keep up.
4517120	4521120	And what that does is that keeps out the little guys. And sometimes they're not so little guys,
4521120	4528000	the big guys that are also just not in those dominant positions. And so what has been happening,
4528000	4532560	and so a lot of you talk about the rise of new exotic crazy accelerators is people have been
4532560	4537520	trying to turn this from a let's go write lots of special kernels problem into a compiler problem.
4538560	4543760	And so we and I contributed to this as well. We as an industry went into it like let's go make
4543760	4549200	this compiler problem phase, let's call it. And much of the industry is still in this phase,
4549200	4553920	by the way. So I wouldn't say this phase is over. And so the idea is to say, look, okay,
4554640	4558160	what a compiler does is it provides a much more general, extensible,
4560000	4567360	hackable interface for dealing with the general case. Right. And so within machine
4567360	4571600	learning algorithms, for example, people figured out that, hey, if I do a matrix multiplication,
4571600	4579440	I do a value, right, the classic activation function, it is way faster to do one pass over
4579440	4584400	the data and then do the value on the output, where I'm writing out the data, because really,
4584400	4590800	it's just a maximum operation, right, max is zero. And so it's an amazing optimization, take
4590800	4596400	map more value, squish together one operation, now I have map more value. Well, wait a second,
4596400	4600160	if I do that, now I just went from having, you know, two operators to three.
4600800	4604000	But now I figure out, okay, well, there's a lot of activation functions. What about
4605680	4610720	leaky value? What about like a million things that are out there, right. And so as I start fusing
4610720	4615120	these in, now I get permutations of all these algorithms, right. And so what the compiler
4615120	4619280	people said is they said, hey, cool, I will go enumerate all the algorithms and I will enumerate
4619280	4624000	all the pairs and I will actually generate a kernel for you. And I think that this has been very,
4624000	4627280	very useful for the industry. This is one of the things that powers Google TPUs,
4628000	4632800	PyTorch 2s, like rolling out really cool compiler stuff with Triton, this other technology and
4632800	4637440	things like this. And so the compiler people are kind of coming into their fore and saying like,
4637440	4642720	awesome, this is a compiler problem, we'll compiler it. Here's the problem. Not everybody's
4642720	4647040	a compiler person. I love compiler people, trust me, right, but not everybody can or should be a
4647040	4651760	compiler person. It turns out that there are people that know analog computers really well,
4651840	4657600	or they know some GPU internal architecture thing really well, or they know some crazy, sparse,
4657600	4663520	numeric, interesting algorithm that is the cusp of research, but they're not compiler people.
4663520	4667600	And so one of the challenges with this new wave of technology trying to turn everything into a
4667600	4673760	compiler is again, it is excluded a ton of people. And so you look at what does Mojo do, what does
4673760	4679440	the modular stack do is brings programmability back into this world, like it enables, I wouldn't
4679440	4684880	say normal people, but a different kind of delightful nerd that cares about numerics or
4684880	4688880	cares about hardware or cares about things like this to be able to express that in the stack and
4688880	4694800	extend the stack without having to actually go hack the compiler itself. So extend the stack on the
4694800	4700560	algorithm side, and then on the hardware side. Yeah, so again, go back to the simplest example
4700560	4706560	of int. And so both Swift and Mojo and other things like this did is we said, okay, pull magic out of
4706560	4710960	the compiler and put it in the standard library. And so what modular is doing with the engine that
4710960	4716080	we're providing and like this, this very deep technology stack, right, which goes into heterogeneous
4716080	4722560	runtimes and like a whole bunch of really cool, really cool things. This whole stack allows that
4722560	4728160	stack to be extended and hacked and changed by researchers and by hardware innovators and by
4728160	4733280	people who know things that we don't know, because you know, modular has some smart people, but we
4733280	4740720	don't have all the smart people, it turns out. What are heterogeneous runtimes? Yeah, so what is
4740720	4745520	heterogeneous, right? So heterogeneous means many different kinds of things together. And so
4745520	4751360	the simplest example you might come up with is a CPU and a GPU. And so it's a simple heterogeneous
4751360	4756720	computer to say, I will run my data loading and preprocessing and other algorithms on the CPU.
4756720	4760480	And then once I get it into the right shape, I shove it into the GPU, I do a lot of matrix
4760480	4766400	multiplications and convolutions and things like this. And I get it back out and I do some reductions
4766400	4771200	and summaries and they shove it across the wire to across the network to another machine, right?
4771200	4778320	And so you've got now what are effectively two computers, a CPU and a GPU talking to each other,
4778320	4786160	working together in a heterogeneous system. But that was 10 years ago. Okay, look at a modern
4786160	4791920	cell phone. Modern cell phone, you've got CPUs. And they're not just CPUs, there's like big dot
4791920	4796240	little CPUs. And so there's multiple different kinds of CPUs are working together. They're
4796240	4802880	multi core. You've got GPUs, you've got neural network accelerators, you got dedicated hardware
4802880	4807760	blocks for for media. So for video decode and JPEG decode and things like this. And so you've
4807760	4811920	got this massively complicated system. And this isn't just cell phones, every laptop these days is
4811920	4819280	doing the same thing. And all these blocks can run at the same time and need to be choreographed,
4819280	4823360	right? And so again, one of the cool things about machine learning is it's moving things to like
4823360	4828720	data flow graphs and higher level of abstractions and tensors and these things that it doesn't specify.
4828720	4833440	Here's how to do the algorithm. It gives the system a lot more flexibility in terms of how to
4833440	4838320	translate or map or compile it onto the system that you have. And so what you need, you know,
4838320	4843360	at the bottomest part of the layer there is a way for all these devices to talk to each other.
4843360	4847760	And so this is one thing that, you know, I'm very passionate about. I mean, you know, I'm a nerd,
4847760	4853600	but, but all these, all these machines and all these systems are effectively parallel computers
4853600	4858160	running at the same time, sending messages to each other. And so they're all fully asynchronous.
4859040	4862560	Well, this is actually a small version of the same problem you have in a data center,
4863280	4867520	right? In a data center, you now have multiple different machines, sometimes very specialized,
4867520	4872720	sometimes with GPUs or TPs and one node and sometimes with disks and other nodes. And so you
4872720	4877520	get a much larger scale, heterogeneous computer. And so what ends up happening is you have this
4877520	4883840	like multi-layer abstraction of hierarchical parallelism, hierarchical asynchronous communication
4884400	4890560	and making that again, the enemy, my enemy is complexity by getting that away from being
4890560	4894640	different specialized systems at every different part of the stack and having more consistency
4894640	4899600	and uniformity. I think we can help lift the world and make it much simpler and actually get used.
4899600	4903520	But how do you leverage like the strengths of the different specialized systems? So we're looking
4903520	4908560	inside the smartphone. Yeah. Like there's just what I don't know, five, six computers essentially
4908560	4918080	inside a smartphone. How do you, without trying to minimize the explicit, making it explicit,
4918080	4920480	which, which computer is supposed to be used for which operation?
4920480	4924400	Yeah. So there's, there's a pretty well known algorithm. And what you're doing is you're looking
4924480	4928960	at two, two factors. You're looking at the factor of sending data from one thing to another.
4928960	4932000	Right. So it takes time to get it from that side of the chip to that side of the chip
4932000	4935840	and things like this. And then you're looking at what is the time it takes to do
4935840	4942880	an operation on a particular block. So take CPUs. CPUs are fully general. They can do anything.
4942880	4947280	Right. But then you have a neural net accelerator that's really good at matrix multiplications.
4947280	4951120	Okay. And so you say, okay, well, if my workload is all matrix multiplications,
4951680	4956080	I start up, I send the data over the neural net thing, it goes and does matrix multiplications,
4956080	4960560	when it's done, it sends me back the result, all is good. Right. And so the simplest thing is just
4960560	4966400	saying, do matrix, do matrix operations over there. Right. But then you realize you get a little bit
4966400	4970160	more complicated because you can do matrix multiplications on a GPU, you can do it on
4971520	4976080	a neural net accelerator, you can do it on CPU, and they'll have different tradeoffs and costs.
4976080	4980320	And it's not just matrix multiplication. And so what you actually look at is you look at,
4980400	4986240	I have generally a graph of compute. I want to do a partitioning. I want to look at the communication,
4986960	4990960	the bisection bandwidth and like the overhead and the sending of all these different things and
4991680	4995840	build a model for this and then decide, okay, it's an optimization problem. Where do I want to
4995840	5002720	place this compute? So the old school theoretical computer science problem of scheduling. And then
5002720	5009120	how does presumably it's possible to somehow magically include auto tune into this?
5010400	5016320	Absolutely. So I mean, in my opinion, this is an opinion, this is not, not everybody would agree
5016320	5021920	with this, but in my opinion, the world benefits from simple and predictable systems at the bottom
5021920	5027600	that you can control. But then once you have a predictable execution layer, you can build
5027600	5031440	lots of different policies on top of it. Right. And so one policy can be that
5032480	5037600	the human programmer says, do that here, do that here, do that here, do that here and like
5037600	5043280	fully manually controls everything. And the systems just do it. Right. Then you quickly
5043280	5047440	get in the mode of like, I don't want to have to tell it to do it. Yeah. And so the next logical
5047440	5052160	step that people typically take, because they write some terrible heuristic, if it's a major
5052160	5055360	small location, do it over there. Or if it's floating point, do it on the GPU, if it's integer,
5055360	5060720	do it on the CPU, like something like that. Right. And, and then you then get into this
5060720	5063280	mode of like people care more and more and more and you say, okay, well, let's actually
5064160	5070560	like make the heuristic better. Let's get into auto tint. Let's actually do a search of the space
5071120	5076880	to decide, well, what is actually better? Right. Well, then you get into this problem where you
5076880	5082880	realize this is not a small space. This is a many dimensional, hyper dimensional space that you
5082880	5087840	cannot exhaustively search. So do you know of any algorithms that are good at searching very
5087840	5093120	complicated spaces for? Don't tell me you're going to turn this into a machine learning problem.
5093360	5097440	So then you turn into a machine learning problem. And then you have a space of generic algorithms
5097440	5102000	and reinforcement learning and like all these, all these, can you include that into the stack,
5102000	5106320	into the, into the module stack? Yeah. Yeah. And where does it sit? Where does it live?
5106320	5110320	Is it a separate thing or is it part of the compilation? So you start from simple and
5110320	5115600	predictable models. And so you can have full control and you can have coarse grain knobs
5115600	5120080	that like nudge, nudge systems, you don't have to do this. But if you really care about getting
5120160	5125200	the best, you know, the last ounce out of a problem, then you can use additional tools.
5125200	5128960	And they're the cool thing is you don't want to do this every time you run a model. You want to
5128960	5133120	figure out the right answer and then cash it. And once you do that, you can get,
5133120	5138880	you can say, okay, cool, I can get up and running very quickly. I can get good execution out of my
5138880	5143360	system. I can decide if something's important. And if it's important, I can go throw a bunch of
5143360	5147680	machines at it and do a big expensive search over the space using whatever technique I feel like
5147680	5152080	it's probably up to the problem. And then when I get the right answer, cool, I can just start using
5152080	5158000	it. And so you can get out of this, this trade off between, okay, am I going to like spend forever
5158000	5162240	doing a thing or do I get up and running quickly? And as a quality result, like these, these are
5162240	5168720	actually not in contention with each other if the system's designed to scale. You started and did
5168720	5176960	a little bit of a whirlwind overview of how you get the 35,000 X speed up or more over Python.
5177840	5182320	Jeremy Howard did a really great presentation about sort of the basic, like looking at the code,
5182320	5186800	here's how you get the speed up. Like you said, that's something we could probably developers
5186800	5192400	can do for their own code to see how you can get these gigantic speed up. But can you maybe speak
5192400	5196000	to the machine learning tasks in general? How do you, how do you make some of this code fast,
5196000	5205440	some specifics like what would you say is the main bottleneck for machine learning tasks? So are we
5205440	5210240	talking about metmall, matrix multiplication, how do you make that fast?
5210240	5214960	So I mean, if you just look at the Python problem, right, you can say, how do I make Python faster?
5215840	5219840	There's been a lot of people that have been working on the, okay, how do I make Python 2x faster,
5219840	5223360	10x faster or something like that, right? And there've been a ton of projects in that vein,
5223360	5229680	right? Mojo started from the, what can the hardware do? Like what is the limit of physics?
5229680	5233440	Yeah, what is the speed of light? What is it? Like how fast can this thing go? And then
5234080	5238880	how do I express that? Yeah, right. And so it wasn't, it wasn't anchored relatively on
5238880	5242720	make Python a little bit faster. It's saying, cool, I know what the hardware can do. Let's
5242720	5249120	unlock that, right? Now, when you, wait, and just say how, how gutsy that is to be in the meeting,
5249120	5253040	and as opposed to trying to see how do we get the improvement? It's like, what can the physics do?
5254000	5258560	I mean, maybe I'm a special kind of nerd, but you look at that, what is the limit of physics,
5258560	5263280	how fast can these things go, right? When you start looking at that, typically,
5263280	5268560	it ends up being a memory problem, right? And so today, particularly with these specialized
5268560	5274160	accelerators, the problem is that you can do a lot of math within them, but yet you get bottleneck
5274160	5280240	sending data back and forth to memory, whether it be local memory or distant memory or disk or
5280240	5285840	whatever it is. And that bottleneck, particularly as the training sizes get large, as you start
5285840	5290560	doing tons of inferences all over the place, like that becomes a huge bottleneck for people,
5290560	5296000	right? So again, what happened is we went through a phase of many years where people
5296000	5300080	took the special case and hand tuned it and tweaked it and tricked it out and they knew
5300080	5302960	exactly how the hardware worked and they knew the model and they made it, they made it fast.
5303920	5309440	Didn't generalize. And so you can make, you know, ResNet 50 or some, or AlexNet or something,
5309440	5314000	Inception V1, like you can, you can do that, right? Because the models are small, they fit in your
5314000	5319280	head, right? But as the models get bigger, more complicated, as the machines get more complicated,
5319280	5324800	it stops working, right? And so this is where things like kernel fusion come in. So what is
5324800	5329760	kernel fusion? This is this idea of saying, let's avoid going to memory. And let's do that by building
5329760	5338160	a new hybrid kernel and a numerical algorithm that actually keeps things in the accelerator
5338160	5342800	instead of having to write it all the way out to memory. What's happened with these accelerators
5342800	5346880	now is you get multiple levels of memory. Like in a GPU, for example, you'll have global memory
5346880	5353600	and local memory and like all these things. If you zoom way into how hardware works,
5353600	5359600	the register file is actually a memory. So the registers are like an L0 cache. And so
5359600	5365760	a lot of taking advantage of the hardware ends up being fully utilizing the full power
5366720	5371680	in all of its capability. And this has a number of problems, right? One of which is,
5371680	5376240	again, the complexity disaster, right? There's too much hardware. Even if you just say, let's look at
5377040	5380960	the chips from one line of vendor like Apple or Intel or whatever it is,
5381760	5386160	each version of the chip comes out with new features. And they change things so that it
5386160	5389920	takes more time or less time to do different things. And you can't rewrite all the software
5389920	5394400	whenever a new chip comes in, right? And so this is where you need a much more scalable approach.
5394400	5399520	And this is what Mojo and what the modular stack provides is it provides this infrastructure and
5399520	5404080	the system for factoring all this complexity and then allowing people to express algorithms.
5404080	5409040	You talk about auto tuning, for example, express algorithms in a more portable way
5409040	5412400	so that when a new chip comes out, you don't have to rewrite it all.
5413440	5418400	So to me, like, you know, I kind of joke like, what is a compiler? Well, there's many ways to
5418400	5423520	explain that. You convert thing A into thing B, and you convert source code to machine code.
5423520	5430000	Like you can talk about many, many things that compilers do. But to me, it's about a bag of tricks.
5430640	5436320	It's about a system and a framework that you can hang complexity. It's a system that can then
5436320	5439840	generalize and it can work on problems that are bigger than fit in one human's head,
5440960	5445680	right? And so what that means, what a good stack and what the modular stack provides is
5446240	5450080	the ability to walk up to it with a new problem and it'll generally work quite well.
5450880	5453760	And that's something that a lot of machine learning infrastructure and tools and
5453760	5458560	technologies don't have. Typical state of the art today is you walk up, particularly if you're
5458560	5462560	deploying, if you walk up with a new model, you try to push it through the converter and the converter
5462560	5471280	crashes. That's crazy. The state of ML tooling today is not anything that a C programmer would
5471280	5476560	ever accept, right? And it's always been this kind of flaky set of tooling that's never been
5477200	5482880	integrated well and it's been never worked together because it's not designed together.
5482880	5486240	It's built by different teams. It's built by different hardware vendors. It's built by different
5486240	5489760	systems. It's built by different internet companies that are trying to solve their
5489760	5494800	problems, right? And so that means that we get this fragmented, terrible mess of complexity.
5495840	5500640	So, I mean, the specifics of, I mean, Jeremy showed this. There's the vectorized function,
5500720	5509360	which I guess is built into Mojo. Vectorized as he showed is built into the library.
5509360	5515760	Into the library, instead of the library. Vectorized, parallelized, which vectorizes more
5515760	5520800	low level, parallelizes higher level. There's the tiling thing, which is how he demonstrated the
5522400	5529520	autotune, I think. So, think about this in like levels, hierarchical levels of abstraction, right?
5529520	5534160	And so, at the very, if you zoom all the way into a compute problem, you have one floating
5534160	5538720	point number, right? And so, then you say, okay, I want to be, I can do things one at a time
5538720	5544800	in an interpreter. It's pretty slow, right? So, I can get to doing one at a time in a compiler,
5544800	5551120	I can see. Then I can get to doing four or eight or 16 at a time with vectors. That's called
5551120	5556400	vectorization. Then you can say, hey, I have a whole bunch of different, you know, what a
5556400	5561920	multi-core computer is, it's basically a bunch of computers, right? So, they're all independent
5561920	5566560	computers that can talk to each other and they share memory. And so, now what parallelized does,
5566560	5571520	it says, okay, run multiple instances on different computers. And now they can all work together
5571520	5575680	on a problem, right? And so, what you're doing is you're saying, keep going out to the next level
5575680	5582240	out. And as you do that, how do I take advantage of this? So, tiling is a memory optimization,
5582240	5586720	right? It says, okay, let's make sure that we're keeping the data close to the compute
5586720	5592080	part of the problem, instead of sending it all back and forth through memory every, every time I
5592080	5597120	load a block. And the size of the block, size is all, that's how you get to the autotune to make
5597120	5600720	sure it's optimized. Right. Yeah. Well, so all of these, the details matter so much to get good
5600720	5605840	performance. This is another funny thing about machine learning and high performance computing
5605840	5611680	that is very different than C compilers we all grew up with, where, you know, if you get a new
5611680	5616240	version of GCC or a new version of Clang or something like that, you know, maybe something
5616240	5622880	will go 1% faster, right? And so compiler insurers will work really, really, really hard to get half
5622880	5628960	a percent out of your C code, something like that. But when you're talking about an accelerator or
5628960	5634240	an AI application, or you're talking about these kinds of algorithms, and these are things people
5634240	5640080	used to write in Fortran, for example, right? If you get it wrong, it's not 5% or 1%. It could
5640080	5647440	be 2x or 10x. Right. If you think about it, you really want to make use of the full memory you
5647440	5651920	have the cache, for example. But if you use too much space, it doesn't fit in the cache. Now you're
5651920	5657440	going to be thrashing all the way back out to main memory. And these can be 2x, 10x, major
5657440	5661840	performance differences. And so this is where getting these magic numbers and these things right
5661840	5667280	is really actually quite important. So you mentioned that Moji is a superset of Python.
5667360	5682160	Can you run Python code as if it's Mojo code? Yes. Yes. And this has two sides of it. So Mojo's not
5682160	5686320	done yet. So I'll give you a disclaimer. Mojo's not done yet. But already we see people that take
5686880	5692640	small pieces of Python code, move it over. They don't change it. And you can get 12x speedups.
5692720	5696800	Somebody was just tweeting about that yesterday, which is pretty cool. And again,
5696800	5703760	interpreters, compilers. And so without changing any code, also this is not JIT compiling or do
5703760	5710000	anything fancy. This is just basic stuff. Move it straight over. Now Mojo will continue to grow
5710000	5714640	out. And as it grows out, it will have more and more and more features. And our North Star is to be
5714640	5719200	a full superset of Python. And so you can bring over basically arbitrary Python code and have it
5719200	5725760	just work. And it may not always be 12x faster, but it should be at least as fast and way faster
5725760	5732000	in many cases. This is cool. Right. Now, it will take time to do that. And Python is a complicated
5732000	5737120	language. There's not just the obvious things, but there's also non-obvious things that are
5737120	5741760	complicated. Like we have to be able to talk to CPython packages that talk to the C API.
5741760	5747440	And there's a bunch of pieces to this. So you have to, I mean, just to make explicit,
5747440	5753440	the obvious may not be so obvious until you think about it. So to run Python code, that means you
5753440	5762480	have to run all the Python packages and libraries. So that means what? What's the relationship between
5762480	5769360	Mojo and CPython, the interpreter that presumably would be tasked with getting those packages to
5769360	5775440	work? So in the fullness of time, Mojo will solve for all the problems and you'll be able to move
5775440	5780720	Python packages over and run them in Mojo. Without the CPython. Without CPython. Someday.
5781280	5785760	Yeah. Right. It's not today, but someday. And that'll be a beautiful day because then you'll get
5785760	5788960	a whole bunch of advantages and you'll get massive speedups and things like this.
5788960	5791520	But you can do that one at a time, right? You can move packages one at a time.
5791520	5796800	Exactly. But we're not willing to wait for that. Python is too important. The ecosystem is too
5796800	5802640	broad. We want to both be able to build Mojo out. We also want to do it the right way without time,
5803200	5808480	without intense time pressure. We're obviously moving fast. And so what we do is we say, okay,
5808480	5814640	well, let's make it so you can import an arbitrary existing package, arbitrary,
5815680	5819760	including you write your own on your local disk or whatever. It's not like a standard
5819760	5825680	like an arbitrary package. And import that using CPython. Because CPython already runs all the
5825680	5831360	packages. And so what we do is we built an integration layer where we can actually use
5831360	5837760	CPython. Again, I'm practical to actually just load and use all the existing packages as they are.
5838400	5841280	The downside of that is you don't get the benefits of Mojo for those packages.
5841760	5845360	Right. And so they run as fast as they do in the traditional CPython way.
5846480	5850480	But what that does is that gives you an incremental migration path. And so if you say,
5850480	5855840	hey, cool, well, here's a, you know, the Python ecosystem is vast. I want all of it to just work.
5855840	5859680	But there's certain things that are really important. And so if I, if I'm doing weather
5859680	5864320	forecasting or something, well, I want to be able to load all the data. I want to be able to work
5864320	5868560	with it. And then I have my own crazy algorithm inside of it. Well, normally I'd write that in
5868560	5874880	C++. If I can write in Mojo and have one system that scales, well, that's way easier to work with.
5874880	5880880	Is it hard to do that to have that layer that's running CPython? Because is there some
5880880	5886160	communication back and forth? Yes, it's complicated. I mean, this is what we do. So I mean, we make it
5886160	5893280	look easy, but it is, it is complicated. But what we do is we use the CPython existing interpreter.
5893280	5896480	So it's running its own byte codes, and that's how it provides full compatibility.
5897120	5903920	And then it gives us CPython objects. And we use those objects as is. And so that way,
5903920	5909440	we're fully compatible with all the CPython objects and all the, you know, it's not just the
5909440	5913600	Python part, it's also the C packages, the C libraries underneath them, because they're often
5913600	5918000	hybrid. And so we can fully run and we're fully compatible with all that. And the way we do that
5918000	5923040	is that we have to play by the rules, right? And so we keep objects in that representation
5923040	5927280	when they're coming from that world. What's the representation that's being used in memory?
5927280	5933200	We'd have to know a lot about how the CPython interpreter works. It has, for example, reference
5933200	5937840	counting, but also different rules on how to pass pointers around and things like this. Super
5937840	5943440	low level fiddly. And it's not like Python, it's like how the interpreter works. Okay. And so that
5943440	5948000	gets all exposed out. And then you have to define wrappers around the low level C code.
5948560	5955360	Right. And so what this means is you have to know not only C, which is a different world from Python,
5955360	5960560	obviously, not only Python, but the wrappers, but the interpreter and the wrappers and the
5960560	5964880	implementation details and the conventions. And it's just this really complicated mess.
5964880	5968080	And when you do that, now suddenly you have a debugger that debugs Python,
5968640	5975520	they can't step into C code. So you have this two world problem, right? And so by pulling this all
5975520	5981200	into Mojo, what you get is you get one world. You get the ability to say, cool, I have untyped,
5981200	5986560	very dynamic, beautiful, simple code. Okay, I care about performance for whatever reason, right?
5986560	5991760	There's lots of reasons you could, you might care. And so then you add types, you can parallelize
5991760	5996080	things, you can vectorize things, you can use these techniques, which are general techniques to
5996080	6002000	solve a problem. And then you can do that by staying in the system. And if you're, you have
6002000	6005920	that one Python package is really important to you, you can move it to Mojo, you get massive
6005920	6010880	performance benefits on that. And other advantages, you know, if you like SAC types, it's nice if
6010880	6015600	they're enforced. Some people like that, right, rather than being hints. So there's other advantages
6015600	6020240	too. And then, and then you can do that incrementally as you go.
6022240	6030880	So one different perspective on this will be why Mojo instead of making C Python faster,
6030880	6036080	redesigning C Python. Yeah, well, I mean, you can argue Mojo is redesigning C Python,
6036080	6041440	but, but, but why not make C Python faster and better and other things like that. There's lots
6041440	6046640	of people working on that. So actually, there's a team at Microsoft that is really improving,
6046640	6052320	I think C Python 3.11 came out in October or something like that. And it was, you know,
6052320	6059680	15% faster, 20% faster across the board, which is pretty huge, given how mature Python is and
6059680	6067840	things like this. And so that's awesome. I love it. Doesn't run on GPU. It doesn't do AI stuff,
6067840	6075200	like it doesn't do vectors, doesn't do things. I'm 20% good, 35,000 times is better. Right. So
6075200	6079600	like they're, they're, they're, they're definitely, I'm a huge fan of that work, by the way, and it
6079600	6083520	composes well with what we're doing. And so it's not, it's not like we're fighting or anything
6083520	6087520	like that. It's actually just general, it's goodness for the world. But it's just a different path.
6087520	6092320	Right. And again, we're not working forwards from making Python a little bit better, we're working
6092880	6098880	backwards from what is the limit of physics. What's the process of porting Python code to Mojo? Is
6098880	6106880	there, what's involved in that, in the process? Is there tooling for that? Not yet. So we're missing
6106880	6110720	some basic features right now. And so we're continuing to drop out new features, like on a
6110720	6117920	weekly basis. But, you know, at the fullness of time, give us a year and a half, maybe two years.
6117920	6123440	Is it an automatable process? So when we're ready, it will be very automatable. Yes.
6123440	6126400	Is it automatable? Like is it possible to automate
6127680	6132080	in the general case, the Python to Mojo conversion? Yeah. Well, you're saying it's possible.
6132080	6135760	Well, so, and this is why, I mean, among other reasons why we use tabs.
6136560	6139760	Yes. Right. So first of all, by being a superset,
6139760	6144320	yeah, you can, it's like C versus C plus plus. Can you move C code to C plus plus?
6144480	6149840	Yes. Yeah. Right. And you move, you can move C code to C plus plus. And
6151280	6155520	then you can adopt classes, you can add, adopt templates, you can adopt other references or
6155520	6159920	whatever C plus plus features you want. After you move C to C code to C plus plus,
6159920	6164800	like you can't use templates in C. Right. And so if you leave it to C, fine, you can't use the
6164800	6168800	cool features, but it still works. Right. And C and C plus plus could work together.
6168880	6178080	And so that's the analogy. Right. Now, here, right, you, you, there's not a Python is bad and
6178080	6183040	then Mojo is good. Right. Mojo just gives you superpowers. Right. And so if you want to stay
6183040	6188960	with Python, that's cool. But the tooling should be actually very beautiful and simple
6188960	6194560	because we're doing the hard work of defining a superset. Right. So you're right. So there's
6194640	6198880	several things to say there, but also the conversion tooling should probably give you hints
6198880	6202480	as to like how you can improve the code. And then exactly once you're in the new world,
6202480	6206000	then you can build all kinds of cool tools to say like, Hey, should you adopt this feature?
6206000	6209840	Or like, and we haven't built those tools yet, but I fully expect those tools will exist. And
6209840	6213760	then you can like, you know, quote unquote modernize your code or however you want to look at it.
6213760	6217680	Right. So I mean, one of the things that I think is really interesting about Mojo is that
6218320	6221680	there have been a lot of projects to improve Python over the years.
6222640	6226480	Everything from, you know, getting Python run on the Java virtual machine,
6227520	6231360	PyPy, which is a JIT compiler, there's tons of these projects out there that have been working
6231360	6237440	on improving Python in various ways. They've fallen to one of two camps. So PyPy is a great
6237440	6242960	example of a camp that is trying to be compatible with Python. Even there, not really doesn't work
6242960	6248160	with all the C packages and stuff like that. But but they're trying to be compatible with Python.
6248160	6252240	There's also another category of these things where they're saying, well, Python is too complicated.
6253280	6259040	And, you know, I'm going to cheat on the edges. And, you know, like integers in Python can be
6259040	6265200	an arbitrary size integer. If you care about it fitting in a going fast on a register and a computer,
6265200	6270480	that's really annoying. Right. And so you can, you can choose to pass on that. Right. You can say,
6270480	6274400	well, people don't really use big integers that often. Therefore, I'm going to just not do it.
6274400	6280800	And it will be fine. Not not a Python superset. Or you can do the hard thing and say, okay,
6280800	6287600	this is Python. You can't be a superset of Python without being a superset of Python. And that's
6287600	6293360	a really hard technical problem. But it's, in my opinion, worth it. Right. And it's worth it because
6294080	6298960	it's not about anyone packages about this ecosystem. It's about what Python means for the world. And
6298960	6303600	it also means we don't want to repeat the Python 2 to Python 3 transition. Like we want,
6303680	6308800	we want people to be able to adopt this stuff quickly. And so by doing that work, we can help
6308800	6313360	lift people. Yeah, the challenge, it's really interesting technical philosophical challenge of
6314800	6317920	really making a language a superset of another language.
6319680	6324240	That's breaking my brain a little bit. Well, it paints you into corners. So again,
6324240	6329040	I'm very happy with Python. So joking, all joking aside, I think that the indentation thing is not
6329760	6335280	the actual important part of the problem. Right. But the fact that Python has amazing
6335280	6339680	dynamic metaprogramming features, and they translate to beautiful static metaprogramming
6339680	6345280	features, I think is profound. I think that's huge. Right. And so Python, I've talked with Guido
6345280	6350800	about this. It's like, it was not designed to do what we're doing. That was not the reason they
6350800	6354400	built it this way. But because they really cared and they were very thoughtful about how they designed
6354400	6359360	the language, it scales very elegantly in the space. But if you look at other languages,
6359360	6365920	for example, C and C++, right, if you're building a superset, you get stuck with the
6365920	6373760	design decisions of the subset. Right. And so, you know, C++ is way more complicated because
6373760	6378400	of C in the legacy than it would have been if they would have theoretically designed a from
6378400	6384160	scratch thing. And there's lots of people right now that are trying to make C++ better and
6384160	6388560	re-syntax C++. It's going to be great. We'll just change all the syntax. But if you do that,
6388560	6394080	now suddenly you have zero packages. You don't have compatibility. So what are the, if you could
6394080	6399920	just linger on that, what are the biggest challenges of keeping that superset status?
6401120	6404720	What are the things just struggling with? Is it all boiled down to having a big integer?
6405600	6410480	No, I mean, what are the other things like? Usually it's the, it's a long tail of weird
6410480	6417120	things. So let me, let me give you a war story. So war story in the space is you go way back in
6417120	6424960	time. Project I worked on is called Clang. Clang, what it is, is a C C++ parser, right. And when
6424960	6431200	I started working on Clang, it must have been like 2006 or something was when I, 2007, 2006,
6431200	6438560	when I first started working on it, right? It's funny how time flies. I started that project
6438560	6445440	and I'm like, okay, well, I want to build a C parser, C++ parser for LLVM. It's going to be
6446080	6452880	the word GCC is yucky. You know, this is me in earlier times. It's yucky. It's unprincipled.
6452880	6458320	It has all these weird features, like all these bugs, like it's yucky. So I'm going to build
6458320	6463920	a standard compliant C and C++ parser. It's going to be beautiful. It'll be amazing. Well,
6463920	6468000	engineered, all the cool things an engineer wants to do. And so I start implementing building it
6468320	6472160	out, building it out, building it out. And then I got to include standard IO.h.
6474000	6477120	And all of the headers in the world use all the GCC stuff.
6479920	6488080	So again, come back away from theory back to reality, right? I was at a fork on the road.
6488080	6492080	I could have built an amazingly beautiful academic thing that nobody would ever use.
6492400	6500800	Or I could say, well, it's yucky in various ways. All these design mistakes, accents of history,
6500800	6506880	the legacy at that point, GCC was like over 20 years old, which, by the way, now LLVM is over
6506880	6513040	20 years old. So it's funny how time catches up to you, right? And so you say, okay, well,
6514320	6519200	what is easier, right? I mean, as an engineer, it's actually much easier for me to go implement
6519840	6524640	long tail compatibility weird features, even if they're distasteful, and just do the hard work
6524640	6528640	and like figure it out, reverse engineer, understand what it is, write a bunch of test
6528640	6533680	cases, like try to understand behavior. It's way easier to do all that work as an engineer
6533680	6537360	than it is to go talk to all C programmers and get, argue with them and try to get them to
6537360	6544320	rewrite their code. Yeah. Right. And because that breaks a lot more things. Yeah. And you have
6544320	6548480	realities like nobody actually even understands how the code works, because it was written by
6548480	6556320	the person who quit 10 years ago, right? And so this software is kind of frustrating that way,
6556320	6560480	but it's, that's how the world works. Yeah. Unfortunately, it can never be this
6561360	6566080	perfect, beautiful thing. Well, there are, there are occasions in which you get to build,
6566080	6570240	like, you know, you invent a new data structure or something like that, or there's this beautiful
6570240	6575040	algorithm that just like makes you super happy. I love that moment. But when you're working with
6575040	6579520	people, you're working with code and dusty that code bases and things like this, right?
6580560	6584480	It's not about what's theoretically beautiful, it's about what's practical, what's real, what
6584480	6589520	people will actually use. And I don't meet a lot of people that say, I want to rewrite all my code
6590560	6594720	just for the sake of it. By the way, there could be interesting possibilities and we'll probably
6594720	6600320	talk about it where AI can help rewrite some code that might be farther out future, but it's a
6600320	6608000	really interesting one, how that could create more, be a tool in the battle against this monster
6608000	6617280	of complexity that you mentioned. You mentioned Guido, the benevolent dictator for life of Python.
6617280	6620320	What does he think about Mojo? Have you talked to him much about it?
6621120	6625200	I have talked with him about it. He found it very interesting. We actually talked with Guido
6625200	6629440	before it launched. And so he was aware of it before it went public. I have a ton of respect
6629440	6635600	for Guido for a bunch of different reasons. You talk about Waller's operator and Guido is pretty
6635600	6644880	amazing in terms of steering such a huge and diverse community and driving it forward. And
6644880	6651360	I think Python is what it is thanks to him. And so to me, it was really important starting to work
6651360	6658080	on Mojo to get his feedback and get his input and get his eyes on this. Now, a lot of what Guido
6658960	6664320	was and is, I think, and thought about is, have we not fragment the community? We don't want to
6664320	6669760	Python 2 to Python 3 thing. That was really painful for everybody involved. And so we spent
6669760	6673440	quite a bit of time talking about that and some of the tricks I learned from Swift, for example.
6673440	6680400	So in the migration from Swift, we managed to not just convert Objective-C into a slightly
6680400	6686400	prettier Objective-C, which we did. We then converted not entirely, but almost an entire
6686480	6691920	community to a completely different language. And so there's a bunch of tricks that you learn
6691920	6696480	along the way that are directly relevant to what we do. And so this is where, for example,
6697600	6703840	you leverage C Python while bringing up the new thing. That approach is, I think, proven and
6703840	6708960	comes from experience. And so Guido is very interested in, like, okay, cool. I think that
6708960	6713440	Python is really his legacy. It's his baby. I have tons of respect for that. Incidentally,
6713520	6717600	I see Mojo as a member of the Python family. We're not trying to take Python away from Guido
6717600	6724560	and from the Python community. And so to me, it's really important that we're a good member
6724560	6729120	of that community. And so I think that, again, you would have to ask Guido this, but I think
6729120	6734000	that he was very interested in this notion of, like, cool, Python gets beaten up for being slow.
6734560	6742800	So maybe there's a path out of that, right? And that, you know, if the future is Python,
6742800	6749920	right? I mean, look at the far outside case on this, right? And I'm not saying this is Guido's
6749920	6754560	perspective, but, you know, there's this path of saying, like, okay, well, suddenly Python can
6754560	6759360	suddenly go all the places it's never been able to go before. Right. And that means that Python
6759360	6763360	can go even further and can have even more impact on the world. So in some sense,
6764800	6770640	Mojo could be seen as Python 4.0. I would not say that. I think that would drive a lot of people
6770640	6776240	really crazy. Because of the PTSD of the 3.02. I'm willing to annoy people about Emax versus VIM
6776240	6779840	or about text versus spaces. That's that one. I don't know. That might be a little bit far
6779840	6785040	even for me. Like my skin may not be that thick. But the point is the step to being a super set
6785040	6790800	and allowing all of these capabilities, I think, is the evolution of a language. It feels like
6790800	6796800	an evolution of a language. So he he's interested by the ideas that you're playing with, but also
6796800	6801520	concerned about the fragmentation. So how, what are the ideas you've learned? What are you thinking
6801520	6810480	about? How do we avoid fragmenting the community? Where the the Pythonistas and the, I don't know
6810480	6818800	what to call the Mojo people. Magicians. I like it. Can coexist happily and share code and basically
6818800	6826080	just have these big code bases that are using C Python and more and more moving towards Mojo.
6826080	6830880	Well, so again, these are lessons I learned from Swift. And here we face very similar problems,
6830880	6839280	right? In Swift, you have Objective C, Super Dynamic. They're very different syntax, right?
6839840	6845280	But you're talking to people who have large scale code bases. I mean, Apple's got the biggest,
6845280	6850080	largest scale code base of Objective C code, right? And so, you know, none of the companies,
6850080	6853360	none of the iOS developers, none of the other developers want to rewrite everything all at
6853360	6857760	once. And so you want to be able to adopt things piece at a time. And so a thing that I found that
6857760	6862640	worked very well in the Swift community was saying, okay, cool. And this is when Swift was very
6862640	6869280	young. As you say, okay, you have a million line of code Objective C app. Don't rewrite it all.
6869280	6874160	But when you implement a new feature, go implement that new class using Swift.
6875040	6879520	Right. And so now this turns out is a very wonderful thing for an app developer.
6880320	6884960	But it's a huge challenge for this compiler team and the systems people that are implementing.
6884960	6889280	That's right. And this comes back to what is this trade off between doing the hard thing that
6890080	6895600	enables scale versus doing the theoretically pure and ideal thing, right? And so Swift had
6895600	6900160	adopted and built a lot of different machinery to deeply integrate with the Objective C runtime.
6900160	6904800	And we're doing the same thing with Python, right? Now, what happened in the case of Swift is that
6905680	6911520	Swift's language got more and more mature over time, right? And incidentally, Mojo is a much
6911520	6915840	simpler language than Swift in many ways. And so I think that Mojo will develop way faster than Swift
6915840	6920560	for a variety of reasons. But as the language gets more mature and parallel with that, you have
6920560	6926000	new people starting new projects, right? And so when the language is mature and somebody
6926000	6929520	is starting a new project, that's when they say, okay, cool, I'm not dealing with a million lines
6929520	6934560	of code. I'll just start and use the new thing for my whole stack. Now, the problem is, again,
6934560	6940320	you come back to where communities and where people that work together, you build new subsystem or
6940320	6946640	new feature or new thing in Swift or you build new thing in Mojo, then you want to be end up being
6946640	6951920	used on the other side, right? And so then you need to work on integration back the other way.
6952720	6956640	And so it's not just Mojo talking to Python, it's also Python talking to Mojo,
6957680	6961360	right? And so what I would love to see, and I don't want to see this next month, right? But
6961360	6965760	what I want to see over the course of time is I would love to see people that are building these
6965760	6974080	packages, like NumPy or TensorFlow or these packages that are half Python, half C++.
6975040	6982000	And if you say, okay, cool, I want to get out of this Python C++ world into a unified world,
6982000	6989040	and so I can move to Mojo, but I can't give up all my Python clients because these libraries
6989040	6995120	get used by everybody and they're not all going to switch all once and maybe never, right? Well,
6995120	6999760	so the way we should do that is we should vend Python interfaces to the Mojo types.
7000880	7004640	And that's what we did in Swift and worked great. I mean, it was a huge implementation challenge
7004640	7009680	for the compiler people, right? But there's only a dozen of those compiler people and there are
7009680	7016800	millions of users. And so it's a very expensive, capital intensive, like skill set intensive
7016800	7021200	problem. But once you solve that problem, it really helps adoption and really helps the community
7021200	7025760	progressively adopt technologies. And so I think that this approach will work quite well with the
7025760	7031600	Python and the Mojo world. So for a package ported to Mojo and then create a Python interface?
7031600	7039520	Yep. So how do you just to link on these packages, NumPy, PyTorch, and TensorFlow?
7039520	7045040	Yeah. How do they play nicely together? So is Mojo supposed to be, let's talk about the machine
7045040	7053360	learning ones. Is Mojo kind of vision to replace PyTorch and TensorFlow to incorporate it? What's
7053360	7059200	the relationship in this? All right. So let's dance. So take a step back. So I wear many hats.
7060320	7066240	So you're angling it on the Mojo side. Mojo is a programming language. And so it can help solve
7066880	7072560	the C++ Python feud that's happening. The fire Mojo got me. I'm sorry. We should be talking
7073040	7080160	modular. Yes. Yes. Okay. So the fire emoji is amazing. I love it. It's a big deal. The other
7080160	7086080	side of this is the fire emoji is in service of solving some big AI problems. And so the big AI
7086080	7092800	problems are again, this fragmentation, this hardware nightmare, this explosion of new potential,
7092800	7098480	but that's not getting felt by the industry. And so when you look at how does the modular engine
7098480	7104400	help TensorFlow and PyTorch, it's not replacing them. In fact, when I talk to people, again,
7104400	7108480	they don't like to rewrite all their code, you have people that are using a bunch of PyTorch,
7108480	7113200	a bunch of TensorFlow. They have models that they've been building over the course of many years.
7113200	7117840	And when I talk to them, there's a few exceptions, but generally they don't want to rewrite all their
7117840	7122880	code. And so what we're doing is we're saying, okay, well, you don't have to rewrite all your code.
7122880	7127200	What happens is the modular engine goes in there and goes underneath TensorFlow and PyTorch.
7127280	7131200	It's fully compatible. And it just provides better performance, better predictability,
7131200	7136160	better tooling. It's a better experience that helps lift TensorFlow and PyTorch and make them even
7136160	7141280	better. I love Python. I love TensorFlow. I love PyTorch, right? This is about making the world
7141280	7147520	better, because we need AI to go further. But if I have a process that trains a model and I have a
7147520	7153920	process that performs inference on that model, and I have the model itself, what should I do with that
7153920	7161600	in the long arc of history in terms of if I use PyTorch to train it? Should I rewrite stuff in
7161600	7167200	Mojo? Would that, if I care about performance? Oh, so I mean, again, it depends. So if you care
7167200	7171120	about performance, then writing in Mojo is going to be way better than writing in Python. But if
7171120	7177200	you look at LLM companies, for example, if you look at OpenAI, rumored, and you look at many of
7177200	7183760	the other folks that are working on many of these LLMs and other innovative machine learning models,
7184320	7188560	on the one hand, they're innovating in the data collection and the model billions of parameters
7188560	7194960	and the model architecture and the RLE or HF and the like all the all the cool things that people
7194960	7201040	are talking about. But on the other hand, they're spending a lot of time writing CUDA curls, right?
7201840	7206240	And so you say, wait a second, how much faster could all this progress go if they were not having
7206240	7210320	to handwrite all these CUDA curls? Right. And so there are a few technologies that are out there
7210320	7215120	and people have been working on this problem for a while. And they're trying to solve subsets to
7215120	7219600	the problem again, kind of fragmenting the space. And so what Mojo provides for these kinds of companies
7219600	7225200	is the ability to say, cool, I can have a unifying theory. Right. And again, the better
7225200	7229360	together the unifying theory, the the two world problem or the three world problem or the n world
7229360	7234000	problem, like this is the thing that is slowing people down. And so as we help solve this problem,
7234000	7239120	I think it'll be very helpful for making this whole cycle go faster. So obviously, we talked
7239120	7245840	about the transition from Objective C to Swift, if design this programming language. And you've
7245840	7254080	also talked quite a bit about the use of Swift for machine learning context. Why have you decided
7254080	7261280	to move away from maybe an intense focus on Swift for the machine learning context versus sort of
7262000	7266160	designing a new programming language that happens to be a super surprise.
7266160	7268640	You're saying this is an irrational set of life choices I make?
7270480	7277280	Did you go to the desert? And did you meditate on it? Okay. All right. No, it was bold and needed.
7277280	7282880	And I think, I mean, it's just bold and sometimes to take those leaps is a difficult leap to take.
7282880	7285680	Yeah. Well, so okay, I mean, I think there's a couple of different things. So
7286720	7293840	actually, I left Apple back in 2017, like January 2017. So it's been a number of years that I left
7293840	7301920	Apple. And the reason I left Apple was to do AI. Okay. So and again, I won't come on Apple and AI,
7301920	7308400	but at the time, right, I wanted to get into and understand and understand the technology,
7308400	7312560	understand the applications, the workloads. And so, okay, I'm going to go dive deep into
7312560	7318640	applied and AI and then the technology underneath it. Right. I found myself at Google.
7319280	7322560	And that was like when TPUs were waking up.
7322560	7329120	Exactly. And so I found myself at Google and Jeff Dean, who's a rock star, as you know, right?
7329120	7335760	And in 2017, TensorFlow is like really taking off and doing incredible things. And I was
7335760	7339760	attracted to Google to help them with the TPUs, right? And TPUs are an innovative hardware
7339760	7345360	accelerator platform have now, I mean, I think proven massive scale and like done incredible
7345360	7350960	things, right? And so one of the things that this led into is a bunch of different projects,
7350960	7356240	which I'll skip over, right? One of which was this Swift for TensorFlow project, right? And so
7356240	7361600	that project was a research project. And so the idea of that is say, okay, well, let's look at
7361600	7366320	innovative new programming models, where we can get a fast programming language, we can get
7367120	7371360	automatic differentiation into language, let's push the boundaries of these things in a research
7371360	7378160	setting, right? Now, that project, I think lasted two, three years. There's some really cool outcomes
7378160	7384480	of that. So one of the things that's really interesting is I published a talk at an LLVM
7384480	7390480	conference in 2018. And this seems like so long ago about graph program abstraction, which is
7390480	7395200	basically the thing that's in PyTorch 2. And so PyTorch 2 with all this Dynamo real thing,
7395200	7399760	it's all about this graph program abstraction thing from Python bytecodes. And so a lot of the
7399840	7405680	research that was done ended up pursuing and going out through the industry and influencing
7405680	7410000	things. And I think it's super exciting and awesome to see that. But the Swift for TensorFlow project
7410000	7414080	itself did not work out super well. And so there's a couple of different problems with that,
7414080	7420400	one of which is that you may have noticed Swift is not Python. There's a few people
7420400	7426400	that write Python code. Yes. And so it turns out that all of ML is pretty happy with Python.
7426400	7430080	It's actually a problem that other programming languages have as well,
7430080	7434400	that they're not Python. We'll probably maybe briefly talk about Julia,
7434400	7438400	who's a very interesting, beautiful programming language, but it's not Python.
7438400	7443600	Exactly. Well, and so like if you're saying, I'm going to solve a machine learning problem
7443600	7448160	where all the programmers are Python programmers. And you say the first thing you have to do is
7448160	7453200	switch to a different language. Well, your new thing may be good or bad or whatever,
7453200	7456800	but if it's a new thing, the adoption barrier is massive.
7457440	7458400	It's still possible.
7458400	7462080	Still possible. Yeah, absolutely. The world changes and evolves. And there's definitely
7462080	7468240	room for new and good ideas, but it just makes it so much harder. And so lesson learned,
7468240	7472480	Swift is not Python. And people are not always in search of learning a new thing for the sake
7472480	7476000	of learning a new thing. And if you want to be compatible with all the world's code, it turns
7476000	7483200	out, meet the world where it is. Right. Second thing is that, you know, a lesson learned is that
7484080	7489920	Swift as a very fast and efficient language, kind of like Mojo, but a different take on it still
7491920	7497200	really worked well with eager mode. And so eager mode is something that PyTorch does,
7497200	7503440	and it proved out really well, and it enables really expressive and dynamic and easy to debug
7503520	7507200	programming. TensorFlow at the time was not set up for that.
7508320	7511600	Let's say that was not the timing is also important in this world.
7511600	7516240	Yeah. Yeah. And TensorFlow is a good thing and it has many, many strengths, but
7518320	7521760	you could say Swift for TensorFlow is a good idea, except for the Swift and except for the
7521760	7528080	TensorFlow part. So because it's not Python and TensorFlow because it's not wasn't set up for
7528400	7534720	eager mode at the time. Yeah. That is 1.0. Exactly. And so one of the things about that is in the
7534720	7539600	context of it being a research project, I'm very happy with the fact that we built a lot of really
7539600	7543440	cool technology. We learned a lot of things. I think the ideas went on to have influence
7543440	7547360	and other systems like PyTorch. A few people use that right here. Right. And so I think that's
7547360	7552320	super cool. And for me personally, I learned so much from it. Right. And I think a lot of the
7552320	7556560	engineers that worked on it also learned a tremendous amount. And so, you know, I think that
7557440	7561600	that's just really exciting to see. And, you know, I'm sorry that the project didn't work out. I
7561600	7568640	wish it did, of course. Right. But, you know, it's a research project and so you're there to learn
7568640	7576720	from it. Well, it's interesting to think about the evolution of programming as we come up with these
7577440	7582880	whole new set of algorithms in machine learning and artificial intelligence and what's going to
7582880	7589280	win out. Because it could be a new programming language. Yeah. It could be, I mean, I just
7589280	7598000	mentioned Julia. I think there's a lot of ideas behind Julia that Mojo shares. What are your
7598000	7605360	thoughts about Julia in general? So, I will have to say that when we launched Mojo, one of the
7605360	7611120	biggest things I didn't predict was the response from the Julia community. And so, I was not,
7611120	7616000	I mean, I've, okay, let me take a step back. I've known the Julia folks for a really long time.
7616000	7621040	They were an adopter of LLVM a long time ago. They've been pushing state of the art in a bunch
7621040	7626640	of different ways. Julia is a really cool system. I had always thought of Julia as being mostly a
7626640	7634480	scientific computing focused environment. And I thought that was its focus. I neglected to
7634480	7639200	understand that one of their missions is to like help make Python work end to end.
7640160	7644000	And so, I think that was my error for not understanding that. And so, I could have been
7644000	7648880	maybe more sensitive to that. But there's major differences between what Mojo is doing
7648880	7654240	and what Julia is doing. So, as you say, Julia is not Python. And so, one of the things that
7654800	7660000	a lot of the Julia people came out and said is like, okay, well, if we put a ton of more energy
7660000	7666080	and ton more money or engineering or whatever into Julia, maybe that would be better than
7666160	7671360	starting Mojo, right? Well, I mean, maybe that's true, but it still wouldn't make Julia into Python.
7672400	7677280	So, if you've worked backwards from the goal of let's build something for Python programmers
7677280	7684480	without requiring them to relearn syntax, then Julia just isn't there, right? I mean,
7684480	7689920	that's a different thing, right? And so, if you anchor on, I love Julia and I want Julia to go
7689920	7694800	further, then you can look at it from a different lens. But the lens we were coming at was, hey,
7694800	7700160	everybody is using Python. Python isn't, syntax isn't broken. Let's take what's great about Python
7700160	7704000	and make it even better. And so, it's just a different starting point. So, I think Julia is
7704000	7707440	a great language. The community is a lovely community. They're doing really cool stuff,
7707440	7713840	but it's just a slightly different angle. But it does seem that Python is quite sticky. Is there
7713840	7720560	some philosophical almost thing you could say about why Python, by many measures, seems to be
7720560	7724800	the most popular programming language in the world? Well, I can tell you things I love about it.
7724800	7730480	Maybe that's one way to answer the question, right? So, huge package ecosystem. Super lightweight and
7730480	7736720	easy to integrate. It has very low startup time. So, what startup time? You mean like learning curve
7736720	7742080	or what? Yeah, so, if you look at certain other languages, you say like, go. And it just takes
7742080	7747840	a, like Java, for example, takes a long time to compile all the things. And then the VM starts
7747840	7751440	up and the garbage clusters kicks in and then it revs its engines and then it can plow through a
7751440	7757840	lot of internet stuff or whatever, right? Python is like scripting. It just goes, right? Python
7757840	7762240	has very low compile time. So, you're not sitting there waiting. Python integrates into notebooks
7762240	7768480	in a very elegant way that makes exploration super interactive and it's awesome, right? Python is also
7769120	7775360	it's like almost the glue of computing because it has such a simple object representation,
7775360	7779520	a lot of things plug into it. That dynamic metaprogramming thing we were talking about also
7779520	7783760	enables really expressive and beautiful APIs, right? So, there's lots of reasons that you can
7784400	7789520	look at technical things that Python has done and say like, okay, wow, this is actually a pretty
7789520	7796320	amazing thing and any one of those you can neglect. People all just talk about indentation and ignore
7796320	7800880	like the fundamental things. But then you also look at the community side, right? So, Python
7800880	7805760	owns machine learning. Machine learning is pretty big. Yeah, and it's growing. It's growing, right?
7805760	7809760	And it's growing in importance, right? And so, and there's a reputation of prestige to machine
7809760	7813680	learning to where like, if you're a new programmer, you're thinking about like,
7814480	7818960	which programming language do I use? Well, I should probably care about machine learning. Therefore,
7818960	7824160	let me try Python and kind of builds and builds and builds. And you can go back before that,
7824160	7829600	like my kids learn Python, right? Not because I'm telling them to learn Python, but because
7829680	7833760	what they were buying against you or what? Well, no, right? Well, they also learned Scratch,
7833760	7837760	right? And things like this too. But it's because Python is taught everywhere, right? Because it's
7837760	7843680	easy to learn, right? And because it's pervasive, right? Back to my day, we learned Java and C++.
7845600	7850800	I'll pale both directions. But yes, I guess Python is the main language of teaching software
7850800	7855920	engineering in schools now. Yeah. Well, and if you look at this, there's these growth cycles,
7856160	7860800	right? If you look at what causes things to become popular, and then gain in popularity,
7860800	7864880	there's reinforcing feedback loops and things like this. And I think Python has done,
7864880	7868720	again, the whole community has done a really good job of building those growth loops and help
7868720	7872720	propel the ecosystem. And I think that again, you look at what you can get done with just a few
7872720	7880720	lines of code, it's amazing. So this kind of self building loop, it's interesting to understand
7880720	7887440	because when you look at Mojo, what it stands for some of the features, it seems sort of clear that
7888080	7892880	this is a good direction for programming languages to evolve in the machine learning community.
7893440	7899280	But it's still not obvious that it will, because of this, whatever the engine of popularity,
7899280	7905520	of virality, is there something you could speak to like how, how do you get people to switch?
7905520	7910080	Yeah, well, I mean, I think that the viral growth loop is to switch people to Unicode.
7911120	7914400	I think the Unicode file extensions are what I'm betting on. I think that's going to be the thing.
7915920	7919040	Tell the kids that you could use the fire emoji and they'd be like, what?
7919040	7925120	Exactly. Well, in all seriousness, I mean, I think there's really, I'll give you two
7925680	7932080	opposite answers. One is, I hope if it's useful, if it solves problems and people care about those
7932080	7938560	problems being solved, they'll adopt the tech. That's kind of the simple answer. And when you're
7938560	7944000	looking to get tech adopted, the question is, is it solving an important problem people need solved?
7944000	7949200	And is the adoption cost low enough that they're willing to make the switch and
7949840	7955200	cut over and do the pain up front so that they can actually do it? And so hopefully,
7955200	7961120	Mojo will be that for a bunch of people and people building these hybrid packages are suffering.
7961120	7965040	It's really painful. And so I think that we have a good shot of helping people.
7965040	7970000	But the other side is, it's okay if people don't use Mojo. It's not my job to say,
7970000	7973520	everybody should do this. I'm not saying Python is bad. I hope Python,
7973520	7977200	see Python, like all these implementations, because Python ecosystem is not just see Python.
7977200	7981360	It's also a bunch of different implementations with different tradeoffs. And this ecosystem is
7981360	7986800	really powerful and exciting, as are other programming languages. It's not like TypeScript
7986800	7992560	or something is going to go away. And so there's not a winner take all thing. And so I hope that
7992560	7996000	Mojo is exciting and useful to people. But if it's not, that's also fine.
7996000	8004800	But I also wonder what the use case for why you should try Mojo would be. So practically speaking.
8005840	8011840	It seems like, so there's entertainment. There's a dopamine hit of saying, holy
8011840	8017840	shit, this is 10 times faster. This little piece of code is 10 times faster in Mojo.
8018160	8022800	Box before he gets to 35,000. Exactly. I mean, just even that, I mean,
8022800	8030560	that's the dopamine hit that every programmer sort of dreams of is the optimization. It's also the
8030560	8037680	drug that can pull you in and have you waste way too much of your life optimizing and over
8037680	8044320	optimizing, right? But so what would you see it would be like comedy. It's very hard to predict,
8044320	8051120	of course. But if you look 10 years from now, Mojo is super successful. What do you think
8051120	8057840	would be the thing where people try it and then use it regularly and it kind of grows and grows
8057840	8063600	and grows and grows? So you talk about dopamine hit. And so again, humans are not one thing.
8064240	8068480	And some people love rewriting their code and learning new things and throwing themselves
8068480	8073760	in the deep end and trying out a new thing. In my experience, most people don't. Like,
8073760	8079600	they're too busy. They have other things going on. By number, most people don't like this. I want
8079600	8087600	to rewrite all my code. But even those people, the two busy people, the people that don't actually
8087600	8092000	care about the language that just care about getting stuff done, those people do like learning new
8092000	8097280	things, right? And so you talk about the dopamine rush of 10x faster. Wow, that's cool. I want to
8097280	8101600	do that again. Well, it's also like, here's a thing I've heard about in a different domain.
8101600	8105840	And I don't have to rewrite all my code. I can learn a new trick, right? Well, that's called
8105840	8112400	growth. And so one thing that I think is cool about Mojo, and again, those will take a little bit
8112400	8117040	of time for, for example, the blog posts and the books and like all that kind of stuff to develop
8117040	8121200	and the language needs to get further along. But what we're doing, you talk about types,
8121200	8125680	like you can say, look, you can start with the world you already know, and you can progressively
8125680	8131200	learn new things and adopt them where it makes sense. If you never do that, that's cool. You're
8131200	8135920	not a bad person. If you, if you get really excited about it, want to go all the way in the deep end
8135920	8140480	and want to rewrite everything and like whatever, that's cool, right? But I think the middle path
8140480	8146400	is actually the more likely one where it's, you know, you come out with a new, a new idea and
8146400	8150960	you discover, wow, that makes my code way simpler, way more beautiful, way faster, way whatever.
8150960	8156160	And I think that's what people like. Now, if you fast forward and you said like 10 years out,
8156720	8160400	right? I can give you a very different answer on that, which is, I mean,
8161360	8166480	if you go back and look at what computers looked like 20 years ago, every 18 months, they got
8166480	8172480	faster for free, right? 2x faster every 18 months, it was like clockwork, it was, it was free,
8172480	8178000	right? You go back 10 years ago, and we entered in this world where suddenly we had multicore CPUs
8178000	8184480	and we had GPUs. And if you squint and turn your head, what are GPUs? It's just a many core,
8184480	8191360	very simple CPU thing kind of, right? And so, and 10 years ago, it was CPUs and GPUs and graphics.
8194000	8201360	Today, we have CPUs, GPUs, graphics, and AI, because it's so important because the compute
8201360	8205680	is so demanding because of the smart cameras and the watches and all the different places
8205760	8211120	that AI needs to work in our lives, it's caused this explosion of hardware. And so,
8211120	8215520	part of my thesis, part of my belief of where computing goes, if you look at 10 years from now,
8216160	8220080	is it's not going to get simpler. Physics isn't going back to where we came from.
8220720	8225440	It's only going to get weirder from here on out, right? And so, to me, the exciting part about
8225440	8231280	what we're building is it's about building that universal platform, which the world can
8231280	8234720	continue to get weird, because again, I don't think it's avoidable, it's physics,
8235360	8239280	but we can help lift people scale, do things with it, and they don't have to rewrite their code
8239280	8243600	every time a new device comes out. And I think that's pretty cool. And so, if Mojo can help with
8243600	8247760	that problem, then I think that it will be hopefully quite interesting and quite useful to
8247760	8252800	a wide range of people because there's so much potential and like there's so, you know, maybe
8252800	8257040	analog computers will become a thing or something, right? And we need to be able to get into a mode
8257040	8261840	where we can move this programming model forward, but do so in a way where we're lifting people
8261840	8266640	and growing them instead of forcing them to rewrite all their code and exploding them.
8266640	8270720	Do you think there'll be a few major libraries that go Mojo first?
8273680	8278560	Well, so, I mean, the modular engine is all Mojo. So, again, come back to like, we're not
8278560	8282640	building Mojo because it's fun, we're building Mojo because we had to dissolve these accelerators.
8282640	8285680	That's the origin story. But I mean, ones that are currently in Python.
8285680	8289200	Yeah. So, I think that a number of these projects will. And so, one of the things, again, this is
8289200	8293680	just my best guess. Like, each of the package maintainers also has, I'm sure, plenty of other
8293680	8297440	things going on. People don't like really don't like rewriting code just for the sake of rewriting
8297440	8303840	code. But sometimes, like, people are excited about like adopting a new idea.
8303840	8309360	Yeah. And it turns out that while rewriting code is generally not people's first thing,
8309360	8314560	turns out that redesigning something while you rewrite it and using a rewrite as an excuse to
8314560	8322480	redesign can lead to the 2.0 of your thing that's way better than the 1.0. And so, I have no idea.
8322480	8327040	I can't predict that. But there's a lot of these places where, again, if you have a package that
8327040	8332640	is half C and half Python, right, you just solve the pain, make it easier to move things faster,
8332640	8337760	make it easier to debug and evolve your tech. Adopting Mojo kind of makes sense to start with.
8337760	8340080	And then it gives you this opportunity to rethink these things.
8340080	8349120	So the two big gains are that there's a performance gain and then there's the portability to all kinds
8349120	8354960	of different devices. And there's safety, right? So you talk about real types. I mean, not saying
8354960	8360160	this is for everybody, but that's actually a pretty big thing, right? And so there's a bunch of
8360160	8364960	different aspects of what value Mojo provides. And so, I mean, it's funny for me. I've been working
8364960	8372000	on these kinds of technologies and tools for too many years now. But you look at Swift, right?
8372000	8375280	And we talked about Swift for TensorFlow, but Swift as a programming language, right?
8376400	8384400	Swift's now 13 years old from when I started it. So, because I started in 2010, if I remember.
8384400	8391200	And so, that project, and I was involved with it for 12 years or something, right? That project
8391200	8395360	has gone through its own really interesting story arc, right? And it's a mature, successful,
8395360	8400720	used by millions of people's system, right? Certainly not dead yet, right? But also,
8400720	8404160	going through that story arc, I learned a tremendous amount about building languages,
8404160	8408320	about building compilers, about working with community, and things like this. And so,
8408320	8413440	that experience, like I'm helping channel and bring directly in Mojo. And other systems,
8413440	8417840	same thing. Apparently, I like building and iterating and evolving things. And so, you look
8417840	8423360	at this LLVM thing I worked on 20 years ago, and you look at MLIR, right? And so, a lot of the
8423360	8428640	lessons learned in LLVM got fed into MLIR. And I think that MLIR is a way better system than
8428640	8434960	LLVM was. And Swift is a really good system. And it's amazing. But I hope that Mojo will take
8434960	8443440	the next step forward in terms of design. In terms of running Mojo, people can play with it.
8443440	8449200	What's Mojo Playground? Yeah. And from the interface perspective,
8449200	8453920	and from the hardware perspective, what's this incredible thing running on?
8454480	8460080	Yeah. So, right now, so here we are two weeks after launch. We decided that, okay, we have this
8460080	8464640	incredible set of technology that we think might be good, but we have not given it to
8465360	8469840	lots of people yet. And so, we were very conservative and said, let's put it in a workbook
8469840	8473280	so that if it crashes, we can do something about it. We can monitor and track that.
8473280	8479360	Right. And so, again, things are still super early, but we're having one person a minute
8480080	8486000	sign up with over 70,000 people two weeks in. It's kind of crazy.
8486000	8490720	So, you can sign up to Mojo Playground and you can use it in the cloud.
8490720	8493760	Yeah. In your browser. And so, what that's running on a workbook?
8493760	8499520	Yeah. What that's running on is, that's running on cloud VMs. And so, you share a machine with a
8499520	8503760	bunch of other people, but it turns out there's a bunch of them now because there's a lot of people.
8503760	8506720	And so, what you're doing is you're getting free compute and you're getting to play with
8506720	8511120	this thing in kind of a limited controlled way so that we can make sure that it doesn't
8512160	8515360	totally crash and be embarrassing. Right? Yeah.
8515360	8518720	So, now a lot of the feedback we've gotten is people want to download it around locally.
8518720	8522880	So, we're working on that right now. So, that's the goal to be able to download locally?
8522880	8525840	Yeah. Yeah. That's what everybody expects. And so, we're working on that right now.
8525840	8527680	And so, we just want to make sure that we do it right.
8527680	8531680	And I think this is one of the lessons I learned from Swift also, by the way.
8532480	8536800	Is it when we launched Swift? Gosh, it feels like forever ago. It was 2014.
8537360	8543760	And we, I mean, it was super exciting. I and we, the team had worked on Swift for a number of years
8543760	8551040	in secrecy. Okay. And we, four years into this development, roughly, of working on this thing,
8551920	8556480	at that point, about 250 people at Apple knew about it. Okay. So, it was secret.
8556480	8561440	Apple's good at secrecy. And it was a secret project. And so, we launched this at WWDC,
8561440	8565520	a bunch of hoopla and excitement, and said, developers, you're going to be able to develop
8565520	8571360	and submit apps to the App Store in three months. Okay. Well, several interesting things happened,
8571360	8577360	right? So, first of all, we learned that, A, it had a lot of bugs. It was not actually production
8577360	8583120	quality. And it was extremely stressful in terms of like, trying to get it working for a bunch of
8583120	8587840	people. And so, what happened was we went from zero to, you know, I don't know how many developers,
8587840	8592800	Apple had at the time, but a lot of developers overnight, and they ran into a lot of bugs,
8592800	8596800	and it was really embarrassing. And it was very stressful for everybody involved, right? It was
8596800	8601040	also very exciting because everybody was excited about that. The other thing I learned is that,
8601040	8605600	when that happened, roughly every software engineer who did not know about the project at Apple,
8605600	8610080	their head exploded when it was launched, because they didn't know it was coming. And so, they're
8610080	8613840	like, wait, what is this? I signed up to work for Apple because I love Objective C. Why is there a
8613840	8622240	new thing, right? And so, now, what that meant practically is that the push from launch to,
8622240	8628880	first of all, the fall, but then to 2.0 and 3.0 and like, all the way forward was super painful
8628880	8634560	for the engineering team and myself. It was very stressful. The developer community was very grumpy
8634560	8638000	about it because they're like, okay, well, wait a second, you're changing and breaking my code and
8638480	8643600	we have to fix the bugs. And it was just a lot of tension and friction on all sides.
8644960	8649440	There's a lot of technical debt in the compiler because we have to run really fast. You have to
8649440	8653040	go implement the thing and unblock the use case and do the thing. And you know it's not right,
8653040	8657600	but you never have time to go back and do it, right? And I'm very proud of the Swift team because
8657600	8665600	they've come, I mean, we, but they came so far and made so much progress over this time since
8665600	8670160	launch. It's pretty incredible and Swift is a very, very good thing, but I just don't want to do
8670160	8676480	that again, right? And so, iterate more through the development process. And so, what we're doing is
8676480	8681600	we're not launching it when it's hopefully 0.9 with no testers. We're launching it and saying it's
8681600	8686400	0.1, right? And so, we're saying expectations of saying like, okay, well, don't use this for
8686400	8691920	production, right? If you're interested in what we're doing, we'll do it in an open way and we
8691920	8696640	can do it together, but don't use it in production yet. Like, we'll get there, but let's do it the
8696640	8702880	right way. And I'm also saying we're not in a race. The thing that I want to do is build the
8702880	8708640	world's best thing, right? Because if you do it right and it lifts the industry, it doesn't matter
8708640	8714080	if it takes an extra two months. Like, two months is worth waiting. And so, doing it right and not
8714080	8720320	being overwhelmed with technical debt and things like this is like, again, war wounds, lessons
8720320	8724480	learned, whatever you want to say, I think is absolutely the right thing to do. Even though,
8724480	8729040	right now, people are very frustrated that you can't download it or it doesn't have feature X
8729040	8736880	or something like this. What have you learned in a little bit of time since it's been released
8736880	8742480	into the wild that people have been complaining about feature X or Y or Z? What have they been
8742480	8748720	complaining about? What have they been excited about? Like, almost like detailed things versus
8748720	8752800	a big vision. I think everyone would be very excited about the big vision.
8752800	8756480	Yeah. Yeah. Well, so, I mean, I've been very pleased. In fact, I mean, we've been massively
8756480	8761680	overwhelmed with response, which is a good problem to have. It's kind of like a success
8761680	8769040	disaster in a sense, right? And so, I mean, if you go back in time, when we started Modular,
8769040	8774480	which is just not yet a year and a half ago, so it's still a pretty new company, new team,
8775040	8781040	small but very good team of people, like we started with extreme conviction that there's a set of
8781040	8784800	problems that we need to solve. And if we solve it, then people will be interested in what we're
8784800	8790720	doing, right? But again, you're building in basically secret, right? You're trying to figure
8790720	8795840	it out. Creation is a messy process. You're having to go through different paths and understand what
8795840	8799840	you want to do and how to explain it. Often, when you're doing disruptive and new kinds of things,
8800800	8806160	just knowing how to explain it is super difficult, right? And so, when we launched,
8806160	8811680	we hoped people would be excited. But, you know, I'm an optimist, but I'm also like,
8811680	8815520	don't want to get ahead of myself. And so, when people found out about Mojo,
8815520	8821120	I think their heads exploded a little bit, right? And, you know, here's, I think, a pretty credible
8821120	8825280	team that has built some languages and some tools before. And so, they have some lessons learned
8826000	8830480	and are tackling some of the deep problems in the Python ecosystem and giving it the love
8830480	8834240	and attention that it should be getting. And I think people got very excited about that. And
8834240	8838320	so, if you look at that, I mean, I think people are excited about ownership and taking a step
8838320	8841280	beyond Rust, right? And there's people that are very excited about that. And there's people that
8841280	8848320	are excited about, you know, just like, I made Game of Life go 400 times faster, right? And things
8848320	8851680	like that. And that's really cool. There are people that are really excited about the, okay,
8851680	8854480	I really hate writing stuff in C++. Save me.
8854480	8857760	Well, like systems in your, they're like stepping up like, oh, yes.
8857760	8864400	So, that's me, by the way. Also, I really want to stop writing C++. But the,
8865120	8871120	I get third-person excitement when people tweet, hey, I made this code, Game of Life,
8871120	8874000	or whatever, faster. And you're like, yeah.
8874000	8881520	Yeah. And also, like, I would also say that, let me cast blame out to people who deserve
8881600	8882480	it. Sure.
8882480	8885760	These terrible people who convinced me to do some of this.
8885760	8886560	Yes.
8886560	8887440	Jeremy Howard.
8887440	8888000	Yes.
8888000	8888560	That guy.
8889520	8891280	Well, he's been pushing for this kind of thing.
8891280	8893040	He's been pushing for more years.
8893040	8894400	Yeah, he's wanted this for a long, long time.
8894400	8895920	He's wanted this for years.
8895920	8899520	For people who don't know Jeremy Howard, he's like one of the most legit people in the machine
8899520	8905840	learning community. He's a grassroots. He really teaches, he's an incredible educator,
8905840	8910800	he's an incredible teacher, but also legit in terms of a machine learning engineer himself.
8911040	8916720	He's been running the fast.ai and looking, I think, for exactly what you've done.
8916720	8917280	Exactly.
8917280	8924640	And so, I mean, the first time, so I met Jeremy pretty early on, but the first time I sat up
8924640	8930640	and I'm like, this guy is ridiculous is when I was at Google and we were bringing up TPUs and
8930640	8937120	we had a whole team of people and where there was this competition called Don Bench of who can
8937120	8940720	train ImageNet fastest.
8941600	8948800	And Jeremy and one of his researchers crushed Google by not through sheer force of the amazing
8948800	8953600	amount of compute and the number of TPUs and stuff like that, that he just decided that progressive
8953600	8957760	imagery sizing was the right way to train the model and if you were epochs faster and
8958320	8960720	make the whole thing go go vroom, right?
8960720	8963440	And I'm like, this guy is incredible.
8964000	8968000	So you can say, anyways, come back to where's Mojo coming from.
8968640	8970320	Chris finally listened to Jeremy.
8972080	8973040	It's all his fault.
8973040	8980800	But there's a kind of very refreshing, pragmatic view that he has about machine learning that
8982400	8989760	I don't know if it's this mix of a desire for efficiency, but ultimately grounded and
8989840	8993680	desired to make machine learning more accessible to a lot of people.
8993680	8994560	I don't know what that is.
8994560	9000240	I guess that's coupled with efficiency and performance, but it's not just obsessed about
9000240	9001200	performance.
9001200	9006480	So a lot of AI and AI research ends up being that it has to go fast enough to get scale.
9007200	9010800	So a lot of people don't actually care about performance, particularly on the research side,
9010800	9014000	until it allows them to have a bigger data set, right?
9014000	9018560	And so suddenly now you care about distributed compute and like all these exotic HPC,
9018560	9020160	like you don't actually want to know about that.
9020160	9024960	You just want to be able to do more experiments faster and do so with bigger data sets, right?
9024960	9027840	And so Jeremy has been really pushing the limits.
9027840	9031520	And one of the things I'll say about Jeremy, and there's many things I could say about Jeremy,
9031520	9035680	because I'm a fanboy of his, but he fits in his head.
9036720	9041360	And Jeremy actually takes the time where many people don't to really dive deep into
9042080	9046320	why is the beta parameter of the atom optimizer equal to this, right?
9046320	9051200	And he'll go survey and understand what are all the activation functions in the trade-offs
9051200	9055680	and why is it that everybody that does this model pick that thing.
9055680	9060640	So the why, not just trying different values, like really what is going on here.
9060640	9061120	Right.
9061120	9066800	And so as a consequence of that, he's always, again, he makes time, but he spends time
9066800	9069600	to understand things at a depth that a lot of people don't.
9069600	9072320	And as you say, he then brings it and teaches people.
9072880	9078640	And his mission is to help lift, his website says, making AI uncool again.
9078640	9082480	Like, it's about, like, forget about the hype, it's actually practical and useful.
9082480	9084080	Let's teach people how to do this, right?
9084720	9088480	Now, the problem Jeremy struggled with is that he's pushing the envelope, right?
9088480	9093360	Research isn't about doing the thing that is staying on the happy path or the well-paid road,
9093360	9094080	right?
9094080	9098640	And so a lot of the systems today have been these really fragile, fragmented things or
9098640	9100160	special case in this happy path.
9100160	9103440	And if you fall off the happy path, you get eaten by an alligator.
9105280	9112000	So what about, so Python has this giant ecosystem of packages
9112880	9114640	and is a package repository.
9114640	9117520	Do you have ideas of how to do that well for Mojo?
9118320	9118560	Yeah.
9118560	9120640	How to do a repository of packages?
9120640	9124000	Well, so that's another really interesting problem that I knew about,
9124000	9126960	but I didn't understand how big of a problem it was.
9126960	9131040	Python packaging, a lot of people have very big pain points
9131040	9132800	and a lot of scars with Python packaging.
9132800	9134880	Oh, you mean, so there's several things to say.
9134880	9139840	Building and distributing and managing dependencies and versioning and all this stuff.
9139840	9143040	So from the perspective of if you want to create your own package.
9143040	9143600	Yes.
9143600	9146800	And then, or you want to build on top of a bunch of other people's packages
9146800	9148480	and then they get updated and things like this.
9149040	9153440	Now, I'm not an expert in this, so I don't know the answer.
9153440	9156640	I think this is one of the reasons why it's great that we work as a team and
9156960	9159040	there's other really good and smart people involved.
9160800	9165440	But one of the things I've heard from smart people who've done a lot of this
9165440	9170080	is that the packaging becomes a huge disaster when you get the Python and C together.
9171040	9175840	And so if you have this problem where you have code split between Python and C,
9175840	9179200	now not only do you have to package the C code, you have to build the C code.
9180160	9182480	C doesn't have a package manager, right?
9182480	9185920	C doesn't have a dependency versioning management system, right?
9185920	9189040	And so I'm not experienced in the state of the art
9189040	9192480	and all the different Python package managers,
9192480	9195840	but my understanding is that's a massive part of the problem.
9195840	9198720	And I think Mojo solves that part of the problem directly heads on.
9199280	9201680	Now, one of the things I think we'll do with the community,
9201680	9205120	and this isn't, again, we're not solving all the world's problems at once,
9205120	9207360	we have to be kind of focused to start with,
9207360	9212080	is that I think that we will have an opportunity to reevaluate packaging, right?
9212080	9213680	And so I think that we can come back and say,
9213680	9216160	okay, well, given the new tools and technologies
9216160	9218000	and the cool things we have that we've built up,
9218000	9219760	because we have not just syntax,
9219760	9222720	we have an entirely new compiler stack that works in a new way,
9222720	9224960	maybe there's other innovations we can bring together
9224960	9226960	and maybe we can help solve that problem.
9226960	9230480	So almost that tangent to that question from the user perspective of packages,
9231840	9238160	it was always surprising to me that it was not easier to sort of explore and find packages.
9239120	9246000	With PIP install, it feels, it's an incredible ecosystem.
9246000	9249280	It's just interesting that it wasn't made,
9249840	9253120	it's still, I think, not made easier to discover packages to do,
9253840	9259760	like search and discovery, as YouTube calls it.
9259760	9263120	Well, I mean, it's kind of funny because this is one of the challenges of these
9264160	9267200	intentionally decentralized communities.
9267280	9269360	And so I don't know what the right answer is for Python.
9269360	9272320	I mean, there are many people that were,
9272320	9273680	I don't even know the right answer for Mojo.
9275120	9278480	So there are many people that would have much more informed opinions than I do,
9278480	9280480	but it's interesting if you look at this, right?
9280480	9283680	Open source communities, you know, there's Git,
9284320	9286080	Git is a fully decentralized,
9286080	9287360	and they can do it any way they want,
9287360	9289360	but then there's GitHub, right?
9289360	9293120	And GitHub centralized, commercial in that case, right?
9293120	9296880	Thing, really help pull together and help solve some of the discovery problems
9296880	9300240	and help build a more consistent community.
9300240	9303040	And so maybe there's opportunities for something like a GitHub.
9304160	9306480	Although even GitHub, I might be wrong on this,
9306480	9310480	but the search and discovery for GitHub is not that great.
9310480	9312160	Like I still use Google search.
9313040	9316800	Yeah, well, I mean, maybe that's because GitHub doesn't want to replace Google search,
9317760	9318000	right?
9318000	9322480	And I think there is room for specialized solutions to specific problems.
9323360	9325280	I don't know, I don't know the right answer for GitHub either.
9325280	9327680	That's, they can go figure that out.
9328720	9330960	But the point is to have an interface that's usable,
9330960	9333760	that's accessible to people of all different skill levels and so on.
9333760	9336160	Well, and again, like what are the benefit of standards, right?
9336160	9339280	Standards allow you to build these next level up ecosystem,
9339280	9342000	the next level up infrastructure, the next level up things.
9342000	9345600	And so again, come back to, I hate complexity.
9347120	9348640	C plus Python is complicated.
9349200	9351280	It makes everything more difficult to deal with.
9351280	9354640	It makes it difficult to port, move code around, work with.
9354640	9356000	All these things get more complicated.
9356000	9359680	And so, I mean, I'm not an expert, but maybe Mojo can help a little bit
9359680	9362640	by helping reduce the amount of C in this ecosystem
9362640	9363760	and make it therefore scale better.
9363760	9366880	So when you kind of package this, the hybrid in nature
9366880	9369440	would be a natural fit to move to Mojo.
9369440	9370880	Which is a lot of them, by the way.
9372560	9375600	A lot of them, especially they're doing some interesting stuff computation-wise.
9376960	9378320	Let me ask you about some features.
9378800	9379600	Yeah.
9379600	9383840	So we talked about, obviously, the indentation that it's the type language
9383840	9385120	or optionally typed.
9385920	9387120	Is that the right way to say it?
9387120	9388720	It's either optionally or progressively.
9388720	9389600	Progressively.
9389600	9393280	I think, so people have very strong opinions on the right word to use.
9394320	9394960	I don't know.
9394960	9396400	I look forward to your letters.
9397760	9399920	So there's the var versus let.
9399920	9401200	But let is for constants.
9402560	9403520	Var is an optional.
9404320	9407040	Yeah, var makes it mutable, so you can reassign.
9407120	9412480	Okay, then there's function overloading.
9412480	9413360	Oh, okay, yeah.
9414320	9416320	I mean, there's a lot of source of happiness for me,
9416320	9423280	but function overloading that's, I guess, is that for performance?
9423280	9425920	Or is that, why does Python not have function overloading?
9427040	9428080	So I can speculate.
9428080	9430400	So Python is a dynamic language.
9430400	9435200	The way it works is that Python and Objective-C
9435200	9439840	are actually very similar worlds if you ignore syntax.
9440880	9445760	And so Objective-C is straight line derived from Smalltalk,
9446880	9451760	a really venerable, interesting language that much of the world has forgotten about,
9451760	9454240	but the people that remember it, love it, generally.
9455040	9458400	And the way that Smalltalk works is that every object has a dictionary in it,
9458960	9463280	and the dictionary maps from the name of a function or the name of a value within an object
9463840	9464880	to its implementation.
9465600	9468400	And so the way you call a method in Objective-C is you say,
9469040	9472000	go look up, the way I call foo is I go look up foo,
9472000	9473840	I get a pointer to the function back, and then I call it.
9474400	9476000	Okay, that's how Python works.
9476640	9478480	Right, and so now the problem with that is that
9479360	9482480	the dictionary within a Python object, all the keys are strings,
9483520	9486480	and it's a dictionary, so you can only have one entry per name.
9486480	9487920	You think it's as simple as that?
9487920	9489280	I think it's as simple as that.
9489280	9492880	And so now, why do they never fix this?
9492880	9494720	Like, why do they not change it to not be a dictionary?
9495680	9496960	Like, do other things.
9497920	9501520	Well, you don't really have to in Python because it's dynamic.
9501520	9503440	And so you can say, I get into the function,
9504160	9507120	now if I got past an integer, do some dynamic test for it,
9507120	9509120	if it's a string, go do another thing.
9510080	9511840	There's another additional challenge, which is,
9511840	9513840	even if you did support overloading, you're saying,
9513840	9517360	okay, well, here's a version of a function for integers and a function for strings.
9518000	9520240	Well, you'd have, even if you could put it in that dictionary,
9520240	9522320	you'd have to have the caller do the dispatch.
9522960	9524800	And so every time you call the function, you'd have to say,
9524800	9526400	like, is an integer a string?
9526400	9528160	And so you'd have to figure out where to do that test.
9528720	9534080	And so in a dynamic language, overloading is something you don't have to have.
9535920	9537920	But now you get into a typed language.
9537920	9541760	And in Python, if you subscript with an integer,
9542640	9546080	then you get typically one element out of a collection.
9546080	9549440	If you subscript with a range, you get a different thing out.
9550160	9554160	Right? And so often in typed languages, you'll want to be able to express the fact that,
9554160	9559040	cool, I have different behavior depending on what I actually pass into this thing.
9559040	9563040	If you can model that, it can make it safer and more predictable and faster and like all these things.
9563680	9568000	It somehow feels safe for yes, but also feels empowering.
9568000	9572400	Make it in terms of clarity, like you don't have to design whole different functions.
9572400	9578560	Yeah. Well, this is also one of the challenges with the existing Python typing systems,
9578560	9583280	is that in practice, like you take subscript, in practice, a lot of these functions,
9583280	9585760	they don't have one signature.
9585760	9587680	They actually have different behavior in different cases.
9587680	9592400	And so this is why it's difficult to retrofit this into existing Python code and make it
9593760	9597440	play well with typing. You kind of have to design for that.
9597440	9602640	Okay. So there's an interesting distinction that people that program Python might be interested in
9602640	9607680	is def versus fn. So it's two different ways to define a function.
9608960	9615600	And fn is a stricter version of def. What's the coolness that comes from the strictness?
9616160	9619120	So here you get into what is the trade off with the superset?
9619760	9620000	Yes.
9621120	9624960	So superset, you have to, or you really want to be compatible.
9625680	9630000	If you're doing a superset, you've decided compatibility with existing code
9630000	9633760	is the important thing, even if some of the decisions they made were maybe not what you'd
9633760	9638320	choose. Okay. So that means you put a lot of time in compatibility,
9638320	9640800	and it means that you get locked into decisions of the past,
9641840	9647360	even if they may not have been a good thing. Now, systems programmers typically like to control
9647360	9654000	things. And they want to make sure that, not in all cases, of course, and even systems programmers
9654000	9659520	are not one thing, but often you want predictability. And so one of the things that Python has,
9659520	9663360	for example, as you know, is that if you define a variable, you just say x equals four.
9664000	9670480	I have a variable name to x. Now I say some long, some long name equals 17.
9671280	9677280	Print out some long name. Oops, I typoed it. Right. Well, the compiler, the Python compiler
9677280	9682560	doesn't know, in all cases, what you're defining and what you're using. And did you typo the use
9682560	9689360	of it or the definition? Right. And so for people coming from type languages, again, I'm not saying
9689360	9693280	the right or wrong, but that drives them crazy because they want the compiler to tell them you
9693280	9698160	typoed the name of this thing. Right. And so what fn does is it turns on, as you say, it's a strict
9698160	9702080	mode. And so it says, okay, well, you have to actually declare, intentionally declare your
9702080	9706640	variables before you use them. That gives you more predictability, more error checking and
9706640	9714960	things like this. But you don't have to use it. And this is a way that Mojo is both compatible
9714960	9718800	because devs work the same way that devs have already always worked. But it provides a new
9718800	9722640	alternative that gives you more control and allows certain kinds of people to have a different
9722640	9728240	philosophy to be able to express that and get that. But usually, if you're writing Mojo code
9728240	9734160	from scratch, you'll be using fn. It depends. Again, it depends on your mentality. It's not
9734160	9740400	that dev is Python and fn is Mojo. Mojo has both and it loves both. It really depends on...
9740400	9745040	Time is just strict. Yeah, exactly. Are you playing around and scripting something out?
9745040	9750960	Is it a one off throwaway script? Cool. Python is great at that. I will still be using fn, but yeah.
9751680	9757360	I love strictness. Control, power. You also like suffering, right?
9758000	9765600	Yes. You go hand in hand. How many pull-ups? I've lost count at this point.
9766320	9770240	And that's cool. I love you for that. And I love other people who like strict things,
9770240	9775120	right? But I don't want to say that that's the right thing because Python is also very beautiful
9775120	9779520	for hacking around and doing stuff and research and these other cases where you may not want that.
9779520	9783920	You see, I just feel like... Maybe I'm wrong with that, but it feels like strictness
9783920	9789920	leads to faster debugging. So in terms of going from... Even on a small project from zero to
9789920	9794320	completion, it just... I guess it depends how many bugs you generate usually.
9794880	9798400	Well, so I mean, it's again lessons learned and looking at the ecosystem. It's really...
9799280	9804160	I mean, I think it's... If you study some of these languages over time, like the Ruby community,
9804160	9808480	for example. Now, Ruby is a pretty well-developed, pretty established community,
9808480	9813920	but along their path, they really invested in unit testing. So I think that the Ruby community is
9813920	9818480	really pushed forward the state-of-the-art of testing because they didn't have a type system
9818480	9823120	that caught a lot of bugs at compile time. And so you can have the best of both worlds. You
9823120	9827360	can have good testing and good types and things like this. But I thought that it was really
9827360	9831120	interesting to see how certain challenges get solved. And in Python, for example,
9831920	9835440	the interactive notebook kind of experiences and stuff like this are really amazing.
9835440	9839680	And if you typo something, it doesn't matter. It just tells you. That's fine, right? And so
9839680	9844640	I think that the tryouts are very different if you're building a large-scale production system
9844640	9849200	versus you're building and exploring in a notebook. And speaking of control, the hilarious thing, if
9849200	9854240	you look at code, I write just for myself for fun. It's like littered with asserts everywhere.
9856240	9858240	It's a kind of... Yeah, you'd like that.
9858480	9865920	Yes. It's basically saying in a dictatorial way, this should be true now. Otherwise,
9865920	9873520	everything stops. And that is the sign. I can't... I love you, man. But that is the sign of somebody
9873520	9878800	who likes control. And so, yes, I think that you'll like... And I think you'll like mojo.
9878800	9886080	Therapy session. Yes, I definitely will. Speaking of asserts, exceptions are called errors.
9886160	9892320	Why is it called errors? So, I mean, we use the same... We're the same as Python, right? But we
9892320	9897600	implement it a very different way. And so, if you look at other languages, like we'll pick on C++,
9897600	9904400	our favorite, right? C++ has a thing called zero-cost exception handling. Okay. And this is,
9905680	9911520	in my opinion, something to learn lessons from. It's a nice polite way of saying it.
9911520	9918560	And so, zero-cost exception handling, the way it works is that it's called zero-cost because
9919440	9925440	if you don't throw an exception, there's supposed to be no overhead for the non-error code. And so,
9925440	9933760	it takes the error path out of the common path. It does this by making throwing an error extremely
9933760	9939120	expensive. And so, if you actually throw an error with a C++ compiler using exceptions,
9939200	9942960	let's go look up in tables on the side and do all this stuff. And so, throwing an error could be
9942960	9949200	like 10,000 times more expensive than returning from a function, right? Also, it's called zero-cost
9949200	9953840	exceptions, but it's not zero-cost. By any stretch of the imagination, because it massively blows
9953840	9959600	out your code, your binary, it also adds a whole bunch of different paths because of
9959600	9963760	destructors and other things like that that exist in C++. And it reduces the number of
9963760	9968240	optimizations. It adds like all these effects. And so, this thing that was called zero-cost
9968240	9977280	exceptions, it really ain't. Okay. Now, if you fast forward to newer languages, and this includes
9977280	9984400	Swift and Rust and Go and now Mojo, well, and Python's a little bit different because it's
9984400	9987920	interpreted. And so, it's got a little bit of a different thing going on. But if you look at
9987920	9995680	compiled languages, many newer languages say, okay, well, let's not do that zero-cost exception
9995680	10003840	handling thing. Let's actually treat throwing an error the same as returning a variant, returning
10003840	10010720	either the normal result or an error. Now, programmers generally don't want to deal with
10010720	10016240	all the typing machinery and like pushing around a variant. And so, you use all the syntax that
10016240	10021440	Python gives us, for example, try and catch, you know, functions that raise and things like this,
10021440	10026640	you can put a raises, decorator on your functions, stuff like this. And if you want to control that,
10026640	10032480	and then the language can provide syntax for it. But under the hood, the way the computer executes it,
10032480	10035520	throwing an error is basically as fast as returning something.
10035520	10039040	I think so it's exactly the same way from a compiled perspective.
10039040	10044960	And so, this is actually, I mean, it's a fairly nerdy thing, right? Which is why I love it. But
10045920	10053280	this has a huge impact on the way you design your APIs, right? So in C++, huge communities turn
10053280	10060320	off exceptions, because the cost is just so high, right? And so the zero cost cost is so high, right?
10060320	10067520	And so that means you can't actually use exceptions in many libraries, right? And even for the people
10067520	10073440	that do use it, well, okay, how and when do you want to pay the cost? If I try to open a file,
10073440	10078400	should I throw an error? Well, what if I'm probing around looking for something, right?
10078400	10082400	I'm looking it up in many different paths. Well, if it's really slow to do that, maybe I'll add
10082400	10087600	another function that doesn't throw an error returns an error code instead. And I have two
10087600	10093040	different versions the same thing. And so it causes you to fork your APIs. And so, you know,
10093040	10097520	one of the things I learned from Apple and I so love is the art of API design is actually
10097520	10101680	really profound. I think this is something that Python's also done a pretty good job at in terms
10101680	10106640	of building out this large scale package ecosystem, it's about having standards and things like this.
10106640	10111920	And so, you know, we wouldn't want to enter a mode where, you know, there's this theoretical feature
10111920	10116720	that exists in language, but people don't use it in practice. Now, I'll also say one of the other
10116720	10120800	really cool things about this implementation approach is that it can run on GPUs and it can run
10120800	10126080	accelerators and things like this. And that standard zero cost exception thing would never
10126080	10130800	work on an accelerator. And so this is also part of how Mojo can scale all the way down to like
10130800	10134080	little embedded systems and to running on GPUs and things like that.
10134640	10141920	Can you actually say about the maybe is there some high level way to describe the challenge of
10142800	10149120	exceptions and how they work in code during compilation? So just this idea of percolating
10149120	10155680	up a thing, an error. Yeah. Yeah. So the way the way to think about it is think about a function
10155680	10162000	that doesn't return anything. Just as a simple case, right? And so you have function one calls
10162000	10167040	function two calls function three calls function four, along that call stack that are tri blocks.
10167760	10171680	Right. And so if you have function one calls function two function two has a tri block,
10171680	10176160	and then within it, it calls function three, right? Well, what happens if function three throws?
10177840	10181760	Well, actually start simpler. What happens if it returns? Well, if it returns, it's supposed to
10181760	10185600	go back out and continue executing and then fall off the bottom of the tri block and keep going
10185600	10190480	and it all's good. If the function throws, you're supposed to exit the current function
10191200	10195760	and then get into the accept clause, right? And then do whatever codes there and then keep
10195760	10201760	following on and going on. And so the way that a compiler like Mojo works is that the call to
10201760	10206720	that function, which happens in the accept block calls a function and then instead of returning
10206720	10214160	nothing, it actually returns, you know, a variant between nothing and an error. And so if you return
10214160	10220080	normally off the bottom or do return, you return nothing. And if you throw through an error, you
10221360	10226400	return the variant that is I'm an error, right? So when you get to the call, you say, okay, cool,
10226400	10232240	I called a function. Hey, I know locally I'm in a tri block. Right. And so I, I call the function
10232240	10236560	and then I check to see what it returns. A half is that error thing jump to the accept block.
10237200	10241360	And that's all done for you behind the scenes. Exactly. And so the compiler does all this for
10241360	10245440	you. And I mean, one of the things if you dig into how this stuff works in Python,
10245440	10250160	it gets a little bit more complicated because you have finally blocks, which now need, you need to
10250160	10257120	go into do some stuff. And then those can also throw and return. Wait, what? Like the stuff matters
10257200	10262960	compatibility. Like there's, there's nest them. There's with clauses. And so with clauses are
10262960	10267040	kind of like finally blocks of some special stuff going on. And so there's nesting in general,
10267040	10274080	nesting of anything nesting of functions should be illegal. It just feels like it adds a level
10274080	10281520	of complexity. I'm merely an implementer. And so this is again, one of the trade offs you get
10281520	10286400	when you decide to build a super set is you get to implement a full fidelity implementation of the
10286400	10294000	thing that you decided is good. And so, yeah, I mean, we can, we can complain about the reality
10294000	10299200	of the world and shake our fists, but it always feels like you shouldn't be a lot to do that,
10299200	10305360	like to declare functions and sudden functions inside functions. Wait, wait, wait, what happened
10305360	10311440	to Lex the Lisp guy? No, I understand that. But Lisp is what I used to do in college.
10312400	10313360	So now you've grown up.
10314720	10320080	You know, we've all done things in college. We're not part of, no, I love Lisp. I love Lisp.
10320080	10323200	Okay. Yeah, I was going to say, you're afraid of me. You're taking the whole internet.
10325200	10330640	It's, it's, uh, it worked. It worked as a joke in my head. So nested functions are
10330640	10334720	joking aside, actually really great. And for certain things, right? And so these are also
10334720	10339440	called closures. Closures are pretty cool. And you can pass callbacks. There's a lot of good
10339440	10346320	patterns. And so, uh, so speaking of which, I don't think you have, uh, nested functions
10346320	10353360	implemented yet in Mojo. Uh, we don't have Lambda syntax, but we do have, uh, there's a few things
10353360	10358400	on the roadmap that you have that it'd be cool to sort of just fly through. Cause it's interesting
10358400	10364800	to see, you know, how many features there are in a language, small and big, they have to implement.
10364800	10369120	Yeah. So first of all, there's tuple support and that has to do with some very specific
10369120	10373440	aspect of it. Like the parentheses are not parentheses that. Yeah. This is just a totally
10373440	10377760	a syntactic thing. A syntactic thing. Okay. There's, but it's cool. It's still, uh,
10379040	10383120	so keyword arguments and functions. Yeah. So this is where in Python, you can say
10383680	10388560	call a function x equals four. Yeah. And x is the name of the argument. That's a nice sort of
10388560	10393040	documenting self-documenting feature. Yeah. I mean, and again, this isn't rocket science to
10393040	10398000	implement. That's just the laundry. It's just on the list. Uh, the bigger features are things
10398000	10404640	like traits. So traits are when you want to define abstract. So when you get into typed languages,
10405200	10409360	you need the ability to write generics. And so you want to say, I want to write this function.
10409360	10414240	And now I want to work on all things that are arithmetic like. Well, what does arithmetic
10414240	10419920	like mean? Well, arithmetic like is a categorization of a bunch of types. And so it's,
10420480	10423600	again, you can define many different ways and I'm not going to go into ring theory or something.
10424640	10428160	But the, uh, you know, you can say it's arithmetic like if you can add the track
10428160	10431600	multiply, divide it, for example. Right. And so what you're saying is you're saying
10432240	10439280	there's a set of traits that apply to a broad variety of types. And so there, all these types
10439280	10443840	are arithmetic like all these tensors and floating point integer. And like there's this category
10443840	10449520	of types. And then I can define on an orthogonal access algorithms that then work against
10449520	10455920	types that have those properties. And so this is a, again, it's a widely known thing. It's been
10455920	10462480	implemented in Swift and Rust and many languages. So it's not Haskell, which is where everybody
10462480	10468480	learns, learns their tricks from. But the, but we need to implement that and that'll enable a new
10468480	10475200	level of expressivity. So classes. Yeah, classes are a big deal. It's a big deal still to be
10475200	10482320	implemented. Um, like you said, a Lambda syntax, and there's like detail stuff like whole module
10482320	10489920	import, um, support for top level code at file scope. So, and then global variables also.
10491600	10495600	So being able to have variables outside of a top level. Well, and so this comes back to the
10495600	10501280	where module came from and the fact that this is your point one, right? And so we're building,
10501280	10506000	so modular is building an AI stack, right? And an AI stack has a bunch of problems working with
10506000	10510560	hardware and writing high performance kernels and doing this kernel fusion thing I was talking about
10510560	10515680	and getting the most out of the hardware. And so we've really prioritized and built Mojo to solve
10515680	10521760	modules problem, right? Now our North Star is build out and support all the things. And so we're
10521760	10526880	making incredible progress. By the way, Mojo is only like seven months old. So that's another
10526880	10530480	interesting thing. I mean, part of the reason I wanted to mention some of these things is like,
10530560	10537120	there's a lot to do and it's pretty cool how you just kind of, sometimes you take for granted how
10537120	10540640	much there is in a programming language, how many cool features you kind of rely on. And this is
10540640	10546320	kind of a nice reminder when you lay it as a to do list. Yeah. And so I mean, but also you look into,
10547040	10554080	it's amazing how much is also there. And you take it for granted that a value, if you define it,
10554080	10559680	it will get destroyed automatically. Like that little feature itself is actually really complicated,
10559760	10564480	given the way the ownership system has to work. And the way that works within Mojo is a huge step
10564480	10568720	forward from what Rust and Swift have done. But can you say that again, when a value, when you
10568720	10572400	define it gets destroyed on the map? Yeah. So like say you have a string, right? So you just find a
10572400	10578160	string on the stack or whatever that means, like in your local function, right? And so you say,
10579040	10584640	like, whether it be in a def, and so you just say x equals hello world, right? Well, if your
10584640	10590480	string type requires you to allocate memory, then when it's destroyed, you have to deallocate it.
10590480	10595600	So in Python and Mojo, you define that with the Dell method, right? Where does that get run?
10598800	10606160	Well, it gets run sometime between the last use of the value and the end of the program.
10607120	10610960	Like in this, you now get into garbage collection, you get into like all these
10611040	10617120	long debated, you talk about religions and tradeoffs and things like this. This is a hugely
10617120	10624000	hotly contested world. If you look at C++, the way this works is that if you define a variable,
10624000	10630640	or a set of variables within a function, they get destroyed in a last in first out order.
10630640	10636640	So it's like nesting. This has a huge problem because if you define, you have a big scope,
10636640	10640640	and you define a whole bunch of values at the top, and then you use them, and then you do a whole
10640640	10644640	bunch of code that doesn't use them, they don't get destroyed until the very end of that scope.
10645680	10650880	And so this also destroys tail calls, so good functional programming, right? This has a bunch
10650880	10655600	of different impacts on, you talk about reference counting optimizations and things like this,
10655600	10660880	a bunch of very low level things. And so what Mojo does is it has a different approach on that
10660880	10666720	from any language I'm familiar with, where it destroys them as soon as possible. And by doing
10666720	10671120	that, you get better memory use, you get better predictability, you get tail calls that work,
10671120	10674720	like you get a bunch of other things, you get better ownership tracking, there's a bunch of
10674720	10681280	these very simple things that are very fundamental, that are already built in there in Mojo today,
10681280	10685200	that are the things that nobody talks about generally, but when they don't work right,
10685200	10688960	you find out and you have to complain about. Is it trivial to know
10690640	10693920	what's the soonest possible to delete a thing that's not going to be used again?
10693920	10698400	Yeah, well, I mean, it's generally trivial, it's after the last use of it. So if you just find x
10698400	10701760	as a string, and then you have some use of x somewhere in your code.
10701760	10705120	Within that scope? You mean within the scope that is accessible?
10705120	10709120	It's, yeah, exactly. So you can only use something within its scope. And so then
10709120	10714160	it doesn't wait until the end of the scope to delete it. It destroys it after the last use.
10714160	10718240	So there's kind of some very ego machine that's just sitting there and deleting.
10718240	10721120	Yeah, and it's all in the compiler, so it's not at runtime, which is also cool.
10721840	10727440	And so, yeah, and so what, and this is actually non-trivial because you have control flow.
10728480	10731440	And so it gets complicated pretty quickly. And so like getting this right was not,
10731440	10734240	Oh, so you have to insert delete like in a lot of places?
10734240	10738240	Potentially, yeah, exactly. So the compiler has to reason about this. And this is where,
10738240	10741840	again, it's experience building languages and not getting this right. So again,
10741840	10745600	you get another chance to do it and you get basic things like this, right?
10745600	10749840	But it's extremely powerful when you do that, right? And so there's a bunch of things like that
10749840	10755280	that kind of combine together. And this comes back to the, you get a chance to do it the right way,
10755280	10758800	do it the right way and make sure that every brick you put down is really good,
10758800	10762560	so that when you put more bricks on top of it, they stack up to something that's beautiful.
10762560	10770000	Well, there's also like, how many design discussions do there have to be about particular
10770000	10775920	details like implementation of particular small features? Because the features that seem small,
10776400	10782560	I bet some of them might be like really require really big design decisions.
10782560	10786720	Yeah. Well, so, I mean, let me give you another example of this. Python has a feature called
10786720	10794080	async await. So it's a new feature, I mean, in the long arc of history, it's a relatively new
10794080	10800640	feature, right? That allows way more expressive asynchronous programming. Okay. Again, this is
10800640	10805360	a Python's a beautiful thing and they did things that are great for Mojo for completely different
10805440	10811440	reasons. The reason the async await got added to Python, as far as I know, is because Python doesn't
10811440	10817520	support threads. Okay. And so Python doesn't support threads, but you want to work with
10818080	10821840	networking and other things like that that can block. I mean, Python does support threads,
10821840	10828240	it's just not its strength. And so they added this feature called async await. It's also seen
10828240	10834400	in other languages like Swift and JavaScript and many other places as well. Async await in Mojo
10834400	10838480	is amazing. Because we have a high-performance heterogeneous compute runtime underneath the
10838480	10846560	covers that then allows non-blocking IO, so you get full use of your accelerator. That's huge,
10846560	10851120	turns out. It's actually really an important part of fully utilizing the machine. You talk about
10851120	10856000	design discussions. That took a lot of discussions, right? And it probably will require more
10856000	10860720	iteration. And so my philosophy with Mojo is that, you know, we have a small team of really
10860800	10865760	good people that are pushing forward, and they're very good at the extremely deep knowing how the
10865760	10871680	compiler and runtime and all the low-level stuff works together. But they're not perfect. Same
10871680	10876800	thing as the Swift team, right? And this is where one of the reasons we released Mojo much earlier
10876800	10879920	is so we can get feedback. And we've already renamed a keyword
10880720	10887120	due to a community feedback. We use an ampersand, and now it's named in and out. We're not
10887120	10891680	renaming existing Python keywords because that breaks compatibility. We're naming things we're
10891680	10897280	adding and making sure that they are designed well. We get usage experience. We iterate and work
10897280	10901040	with the community because, again, if you scale something really fast and everybody writes all
10901040	10904960	their code and they start using it in production, then it's impossible to change. And so you want
10904960	10909280	to learn from people. You want to iterate and work on that early on. And this is where design
10909280	10915040	discussions, it's actually quite important. Could you incorporate an emoji into the language,
10915040	10918880	into the main language? Do you have a favorite one?
10919520	10925440	Why really inters the humor, like rawful, whatever, rolling on the floor laughing?
10926240	10931760	So that could be like, what would that be, the use case for that? Like throw an exception
10931760	10938400	of some sort? You should totally file a feature request. Or maybe a hard one. It has to be a
10938400	10943360	hard one. People have told me that I'm insane. I'm liking this.
10944320	10949440	I'm going to use the viral nature of the internet to actually get this past.
10950160	10953120	I mean, it's funny you come back to the flame emoji, file extension, right?
10955440	10961040	We have the option to use the flame emoji, which just even that concept cause, for example,
10961040	10963280	the people at GitHub to say, now I've seen everything.
10965600	10972720	Yeah, there's something, it's reinvigorating. It's like, oh, that's possible. That's really
10972720	10976400	cool that for some reason that makes everything else seem really exciting.
10976400	10979920	I think the world is ready for this stuff, right? And so, you know, when we have a package manager,
10979920	10984080	we'll clearly have to innovate by having the compiled package saying be the little
10984080	10988640	box with the bow on it, right? I mean, it has to be done.
10988640	10992240	It has to be done. Is there some stuff on the roadmap that you're particularly
10992880	10995920	stressed about or excited about that you're thinking about a lot?
10995920	11001120	I mean, as a today snapshot, which will be obsolete tomorrow, the lifetime stuff is really
11001120	11007040	exciting. And so lifetimes give you safe references to memory without dangling pointers.
11007680	11010720	And so this has been done in languages like Rust before. And so we have a new approach,
11010720	11014800	which is really cool. I'm very excited about that. That'll be out to the community very soon.
11015520	11021040	The traits feature is really a big deal. And so that's blocking a lot of API design.
11021040	11023280	And so there's that. I think that's really exciting.
11025200	11027760	A lot of it is these kind of table stakes features.
11028720	11032160	One of the things that is, again, also lessons learned with Swift
11033840	11037680	is that programmers in general like to add syntactic sugar.
11038880	11043200	And so it's like, oh, well, this annoying thing, like in Python, you have to spell
11043200	11049280	unbar unbar add. Why can't I just use plus? Def plus, come on. Why can't I just do that, right?
11049280	11053920	And so trivial bit of syntactic sugar, it makes sense. It's beautiful. It's obvious.
11053920	11060560	We're trying not to do that. And so for two different reasons, one of which is that, again,
11060560	11066720	lesson learned with Swift. Swift has a lot of syntactic sugar, which may be a good thing,
11066720	11071760	maybe not. I don't know. But because it's such an easy and addictive thing to do,
11071760	11077680	sugar, like make sure blood get crazy, right? Like the community will really dig into that and
11077680	11081360	want to do a lot of that. And I think it's very distracting from building the core abstractions.
11081920	11084240	The second is we want to be a good member of the Python community,
11086240	11091920	right? And so we want to work with the broader Python community. And yeah, we're pushing forward
11091920	11095280	a bunch of systems programming features, and we need to build them out to understand them.
11095280	11099600	But once we get a long ways forward, I want to make sure that we go back to the Python community
11099600	11102720	and say, okay, let's do some design reviews. Let's actually talk about this stuff. Let's figure
11102720	11107280	out how we want this stuff all to work together. And syntactic sugar just makes all that more
11107280	11113760	complicated. And yeah, list comprehension is like yet to be implemented. And my favorite,
11113760	11121360	I mean, dictionaries. Yeah, there's some basic zero point one, zero point one. But nonetheless,
11121360	11125680	it's actually still quite interesting and useful. As you mentioned, modular is very new.
11127120	11134240	Mojo is very new. It's a relatively small team. Yeah, that's building up this gigantic stack.
11134960	11138960	This incredible stack that's going to perhaps define the future of
11140000	11145040	development of our AI overlords. We just hope it will be useful.
11146720	11154400	As do all of us. So what, what have you learned from this process of building up a team? Maybe
11154400	11161440	one question is, how do you hire? Yeah, great programmers, great people that operate in this
11162400	11171520	compiler, hardware, machine learning, software, interface design space. And maybe you're a
11171520	11177440	little bit fluid in what they can do. So okay, so language design too. So building a company is
11177440	11183120	just as interesting in different ways as building a language, like different skill sets, different
11183120	11186720	things, but super interesting. And I've built a lot of teams in a lot of different places.
11187680	11190400	If you zoom in from the big problem into recruiting,
11191520	11196400	well, so here's our problem. Okay, I'll just, I'll be very straightforward about this.
11196400	11200880	We started modular with a lot of conviction about we understand the problems, we understand the
11200880	11205920	customer pain points, we need to work backwards from the suffering in the industry. And if we
11205920	11210640	solve those problems, we think it'll be useful for people. But the problem is, is that the people
11210640	11215920	we need to hire, as you say, are all these super specialized people that have jobs at big tech
11217280	11223040	big tech worlds, right? And, you know, we, I don't think we have product market fit in the way that
11223040	11228480	a normal startup does, we don't have product market fit challenges, because right now,
11228480	11232720	everybody's using AI and so many of them are suffering and they want help. And so again,
11232720	11237520	we started with strong conviction. Now, again, you have to hire and recruit the best and the
11237520	11242080	best all have jobs. And so what we've done is we said, okay, well, let's build an amazing culture.
11243040	11246960	Start with that. That's usually not something a company starts with. Usually you hire a bunch
11246960	11251680	of people and then people start fighting and it turns into a gigantic mess. And then you try to
11251680	11255760	figure out how to improve your culture later. My co-founder, Tim, in particular, is super
11255760	11260240	passionate about making sure that that's right. And we've spent a lot of time early on to make
11260240	11264640	sure that we can scale. Can you comment, sorry, before we get to the second, what makes for a
11264640	11269920	good culture? So I mean, there's many different cultures. And I have learned many things from
11270880	11276560	several very unique, almost famously unique cultures. And some of them I learned what to do
11276560	11284800	and some of them I learned what not to do. And so we want an inclusive culture. I believe in
11286560	11291360	amazing people working together. And so I've seen cultures where people, you have amazing people
11291360	11296640	and they're fighting each other. I see amazing people and they're told what to do. Like,
11296640	11300400	doubt, shout, line up and do what I say. It doesn't matter if it's the right thing. Do it.
11301520	11305760	And neither of these, and I've seen people that have no direction. They're just kind of floating
11305760	11310480	in different places. And they want to be amazing. They just don't know how. And so a lot of it starts
11310480	11317040	with have a clear vision. And so we have a clear vision of what we're doing. And so I kind of grew
11317040	11324000	up at Apple in my engineering life. And so a lot of the Apple DNA rubbed off on me. My co-founder,
11324080	11329120	Tim, also is like a strong product guy. And so what we learned is, you know, I decided Apple that
11329120	11334320	you don't work from building cool technology. You don't work from, like, come up with a cool
11334320	11337600	product and think about the features you'll have in the big checkboxes and stuff like this.
11338240	11340960	Because if you go talk to customers, they don't actually care about your product.
11341520	11345040	They don't care about your technology. What they care about is their problems.
11346000	11350480	Right? And if your product can help solve their problems, well, hey, they might be interested
11350560	11354000	in that. And so if you speak to them about their problems, if you understand and you
11354000	11358000	have compassion, you understand what people are working with, then you can work backwards to
11358000	11361680	building an amazing product. So the vision starts by defining the problem.
11361680	11365920	And then you can work backwards in solving technology. And at Apple, like it's, I think,
11365920	11371680	pretty famously said that, you know, for every, you know, there's 100 no's for every yes.
11372880	11378320	I would refine that to say that there's 100 not yet for every yes. But famously, if you go back
11378320	11382800	to the iPhone, for example, right, the iPhone one, I read, I mean, many people laughed at it
11382800	11385200	because it didn't have 3G. It didn't have copy and paste.
11386560	11391840	Right. And then a year later, okay, finally, it has 3G, but it still doesn't have copy and paste.
11391840	11394880	It's a joke. Nobody will ever use this product, blah, blah, blah, blah, blah, blah, blah,
11394880	11399600	blah, right? Well, your three had copy and paste and people stopped talking about it.
11399600	11404960	Right. And so, and so being laser focused and having conviction and understanding
11404960	11408880	what the core problems are and giving the team the space to be able to build the right tech
11409440	11414560	is really important. Also, I mean, you come back to recruiting, you have to pay well.
11415600	11418560	Right. So we have to pay industry leading salaries and have good benefits and things
11418560	11423600	like this. That's a big piece. We're a remote first company. And so we have to,
11426320	11432960	so remote first has a very strong set of pros and cons. On the one hand, you can hire people
11433040	11436800	from wherever they are and you can attract amazing talent, even if they live in
11436800	11440880	strange places or unusual places. On the other hand, you have time zones.
11442000	11447040	On the other hand, you have like everybody on the internet will fight if they don't understand
11447040	11451440	each other. And so we've had to learn how to like have a system where we actually fly people in
11451440	11455600	and we get the whole company together periodically and then we get work groups together and we plan
11455600	11459840	and execute together. And there's like an intimacy to the in-person brainstorming.
11460560	11464560	Yeah, I guess you lose, but maybe you don't. Maybe if you get to know each other well and
11464560	11468240	you trust each other, maybe you can do that. Yeah. Well, so when the pandemic first hit,
11468240	11472640	I mean, I'm curious about your experience too. The first thing I missed was having whiteboards.
11472640	11479040	Yeah. Right. Those design discussions are like, I can high intensity work through things, get
11479040	11482880	things done, work through the problem of the day, understand where you're on, figure out and solve
11482880	11489360	the problem and move forward. But we figured out ways to work around that now with
11490080	11494480	all these screen sharing and other things like that that we do. The thing I miss now
11494480	11501600	is sitting down at a lunch table with the team, the spontaneous things like the coffee bar
11501600	11506400	things and the bumping into each other and getting to know people outside of the transactional
11506960	11513360	solve a problem over Zoom thing. And I think there's just a lot of stuff that I'm not an expert
11513360	11517680	at this. I don't know who is, hopefully there's some people, but there's stuff that somehow is
11517680	11524240	missing on Zoom. Even with the whiteboard, if you look at that, if you have a room with one
11524240	11531120	person at the whiteboard and there's like three other people at a table, there's a, first of all,
11531120	11535680	there's a social aspect of that where you're just shooting the shit a little bit, almost like.
11535680	11542320	Yeah. As people just kind of coming in and yeah, that, but also while like it's a breakout
11542320	11548240	discussion that happens for like seconds at a time, maybe an inside joke or it's like this
11548240	11552240	interesting dynamic that happens that Zoom. And you're bonding. Yeah. You're bonding. You're
11552240	11556960	bonding, but through that bonding, you get the excitement. There's certain ideas that are like
11557600	11563200	complete bullshit and you'll see that in the faces of others that you won't see necessarily on Zoom.
11564160	11570640	And like something, it feels like that should be possible to do without being in person.
11570640	11576320	Well, I mean, being in person is a very different thing. It's worth it, but you can't always do it.
11576320	11582240	And so again, we're still learning and we're also learning as like humanity with this new reality,
11582240	11587200	right? But what we found is that getting people together, whether it be a team or the whole
11587200	11591760	company or whatever, is worth the expense because people work together and are happier
11592640	11597920	after that. Like there's a massive period of time where you go out and things start getting
11597920	11602560	frayed, pull people together, and then you realize that we're all working together. We see things the
11602560	11606240	same way. We work through the disagreement or the misunderstanding. We're talking across each other
11606240	11610640	and then you work much better together. And so things like that, I think are really quite important.
11610640	11615840	What about people that are kind of specialized in very different aspects of the stack working
11615840	11619600	together? What are some interesting challenges there? Yeah. Well, so I mean, I mean, there's
11619600	11622720	lots of interesting people, as you can tell, I'm, you know, hard to deal with too.
11623440	11630080	You're one of the most lovable people. So one of the, so there's different
11630080	11635920	philosophies in building teams. For me, and so some people say, hire 10x programmers,
11635920	11640480	and that's the only thing that whatever that means, right? What I believe in is building
11640480	11646640	well balanced teams. Teams that have people that are different in them. Like if you have all generals
11646640	11652240	and no troops, or all troops and no generals, or you have all people that think in one way,
11652240	11656400	and not the other way, what you get is you get a very biased and skewed and weird situation where
11656400	11660720	people end up being unhappy. And so what I like to do is I like to build teams of people where
11660720	11666160	they're not all the same. You know, we do have teams that are focused on like runtime or compiler,
11666160	11671760	GPU or whatever the specialty is, but people bring a different take and have a different
11671760	11676320	perspective. And I look for people that complement each other. And particularly if you look at
11676320	11680960	leadership teams and things like this, you don't want everybody thinking the same way. You want
11680960	11685360	people bringing different perspectives and experiences. And so I think that's really important.
11685360	11690960	That's team, but what about building a company as ambitious as modular? So what are some
11690960	11697440	interesting questions there? Oh, I mean, so many. Like, so one of the things I love about, okay, so
11697440	11705520	modular is the first company I built from scratch. One of the first things that was profound was I'm
11705520	11710880	not cleaning up somebody else's mess. Right. And so if you look at that's liberating to some degree.
11710880	11717520	It's super liberating. And also, many of the projects I've built in the past have not been
11717520	11726160	core to the product of the company. Swift is not Apple's product, right? MLIR is not Google's
11726160	11731280	revenue machine or whatever, right? It's not, it's important. But it's like working on the
11731280	11737760	accounting software for, you know, the retail giant or something, right? It's like enabling
11737760	11744400	infrastructure and technology. And so at modular, the tech we're building is here to solve people's
11744400	11749120	problems. Like it is directly the thing we're giving to people. And so this is a really big
11749120	11753680	difference. And what it means for me as a leader, but also for many of our engineers is they're
11753680	11758560	working on the thing that matters. And that's actually pretty, I mean, again, for compiler
11758560	11763440	people and things like that, that's usually not the case, right? And so that's also pretty exciting
11763440	11770400	and quite nice. But one of the ways that this manifests is it makes it easier to make decisions.
11771200	11774720	And so one of the challenges I've had in other worlds is it's like, okay, well,
11775440	11781040	community matters somehow for the goodness of the world, like, or open-source matters
11781040	11787040	theoretically, but I don't want to pay for a t-shirt, right? Or some swag. Like, well,
11787040	11792800	t-shirts cost $10 each. You can have 100 t-shirts for $1,000 to a mega-corp. $1,000 is
11793680	11799120	uncountably, can't count that low, right? But justifying it and getting a t-shirt,
11799120	11801040	by the way, if you'd like a t-shirt, I can give you a t-shirt.
11801040	11805120	Well, I would 100% like a t-shirt. Are you joking?
11805120	11810720	You can have a fire emoji t-shirt. I will treasure this.
11810720	11813120	Is that a good thing? I will pass it down to my grandchildren.
11813120	11817200	And so it's very liberating to be able to decide, I think that Lex should have a t-shirt,
11818800	11822080	right? And it becomes very simple because I like Lex.
11824800	11831840	This is awesome. So I have to ask you about the...
11833840	11836720	One of the interesting developments with large language models
11836960	11843760	is that they're able to generate code recently really well.
11845040	11845600	Yes.
11845600	11851760	To a degree that maybe I don't know if you understand, but I have... I struggle to
11851760	11856720	understand because it forces me to ask questions about the nature of programming,
11856720	11863360	of the nature of thought, because the language models are able to predict the kind of code
11863360	11868960	I was about to write so well that it makes me wonder how unique my brain is and where the
11868960	11876560	valuable ideas actually come from. How much do I contribute in terms of ingenuity, innovation,
11877120	11883120	to code, I write, or design, and that kind of stuff. When you stand on the shoulders of giants,
11883120	11888400	are you really doing anything? And what LLMs are helping you do is they help you stand on
11888400	11892560	the shoulders of giants in your program. There's mistakes. They're interesting that you learned
11892560	11899280	from, but I would love to get your opinion first high-level of what you think about this
11899280	11904080	impact of large language models when they do program synthesis, when they generate code.
11907040	11913280	I don't know where it all goes. I'm an optimist and I'm a human optimist. I think that
11914000	11918000	things I've seen are that a lot of the LLMs are really good at crushing leak code projects,
11918640	11922880	and they can reverse the link list like crazy. Well, it turns out there's a lot of
11923920	11927920	instances of that on the internet, and it's a pretty stock thing. And so if you want to see
11928960	11932800	standard questions answered, LLMs can memorize all the answers, and that can be amazing.
11932800	11938480	And also, they do generalize out from that, and so there's good work on that. But I think that
11939280	11943680	in my experience, building things, building something like you talk about mojo or you talk
11943760	11948640	about these things or you talk about building an applied solution to a problem, it's also about
11948640	11952400	working with people. It's about understanding the problem. What is the product that you want to
11952400	11956240	build? What are the use case? What are the customers? You can't just go survey all the
11956240	11960720	customers because they'll tell you that they want a faster horse. Maybe they need a car.
11961440	11966800	And so a lot of it comes into, I don't feel like we have to compete with LLMs. I think they'll
11966800	11972080	help automate a ton of the mechanical stuff out of the way. And just like, I think we all try
11972080	11977040	to scale through delegation and things like this. Delegating wrote things to an LLM, I think,
11977040	11982240	because it's extremely valuable and approach that will help us all scale and be more productive.
11982240	11986480	But I think it's a fascinating companion. But I'd say I don't think that that means that we're
11986480	11994000	going to be done with coding. Sure. But there's power in it as a companion. And from there,
11994000	12000480	I would love to zoom in on to mojo a little bit. Do you think about that? Do you think about
12000480	12008000	LLMs generating mojo code and helping sort of like, we design new programming language, it almost
12008000	12015360	seems like, man, it would be nice to sort of, almost as a way to learn how I'm supposed to use
12015360	12022160	this thing for them to be trained on some of the mojo code. So I do lead an AI company. So maybe
12022160	12029280	there'll be a mojo LLM at some point. But if your question is like, how do we make a language to be
12029280	12038240	suitable for LLMs? I think the cool thing about LLMs is you don't have to. And so if you look at
12038240	12042400	what is English or any of these other terrible languages that we as humans deal with on a
12042400	12047440	continuous basis, they're never designed for machines. And yet, they're the intermediate
12047440	12052640	representation, they're the exchange format that we humans use to get stuff done. And so these
12052640	12057440	programming languages, they're an intermediate representation between the human and the computer
12057440	12063520	or the human and the compiler, roughly, right? And so I think the LLMs will have no problem learning
12064240	12068560	whatever keyword we pick. Maybe the phi emoji is going to... Maybe that's going to break it,
12068560	12073760	it doesn't tokenize. No, the reverse of that, it will actually enable it because one of the issues
12073760	12079600	I could see with being a superset of Python is there would be confusion by the gray area. So it
12079600	12085920	would be mixing stuff. But... Well, I'm a human optimist, I'm also an LLM optimist. I think that
12085920	12094320	we'll solve that problem. But you look at that and you say, okay, well, reducing the rote thing,
12094320	12098480	right? Turns out compilers are very particular and they really want things, they really want the
12098480	12102000	indentation to be right. They really want the colon to be there on your else or else that will
12102000	12108240	complain, right? I mean, compilers can do better at this, but LLMs can totally help solve that
12108240	12113040	problem. And so I'm very happy about the new predictive coding and co-pilot type features
12113040	12115840	and things like this because I think it will all just make us more productive.
12115840	12121440	It's still messy and fuzzy and uncertain, unpredictable, so... But is there a future you
12121440	12128880	see given how big of a leap GPT-4 was, where you start to see something like LLMs inside
12129520	12134400	a compiler or no? I mean, you could do that. Yeah, absolutely. I mean, I think that would be
12134400	12141200	interesting. Is that wise? Well, I mean, it would be very expensive. So compilers run fast and they're
12141200	12145360	very efficient and LLMs are currently very expensive. There's on-device LLMs and there's
12145360	12150240	other things going on and so maybe there's an answer there. I think that one of the things that I
12150240	12157600	haven't seen enough of is that... So LLMs to me are amazing when you tap into the creative potential
12157600	12163920	of the hallucinations, right? And so if you're doing creative brainstorming or creative writing
12163920	12169600	or things like that, the hallucinations work in your favor. If you're writing code that has to be
12169600	12172720	correct because you're going to ship it in production, then maybe that's not actually a feature.
12173840	12177920	And so I think that there has been research and there has been work on building
12178640	12184240	algebraic reasoning systems and figuring out more things that feel like proofs.
12184960	12188000	And so I think that there could be interesting work in terms of building more
12188640	12193040	reliable at scale systems and that could be interesting. But if you chase that rabbit hole
12193040	12197440	down, the question then becomes how do you express your intent to the machine? And so maybe you want
12197440	12203840	LLMs to provide the spec, but you have a different kind of net that then actually implements the code.
12203840	12210400	Right. So it's used as documentation and inspiration versus the actual implementation.
12210400	12219040	Yeah, potentially. Since a successful modular will be the thing that runs, I say so jokingly,
12219040	12226560	are AI overlords. But AI systems that are used across... I know it's a cliche term, but in
12226640	12232080	a lot of things. So across... So I'll joke and say like AGI should be written in Mojo.
12232080	12236960	Yeah, AGI should be written in Mojo. You're joking, but it's also possible that it's not a joke.
12238640	12245440	That a lot of the ideas behind Mojo is seems like the natural set of ideas that would enable
12245440	12253120	at scale training and inference of AI systems. So I just have to ask you about the big philosophical
12253120	12259040	question about human civilization. So folks like Eliezer Yatkowski are really concerned about the
12259040	12269600	threat of AI. Do you think about the good and the bad that can happen at scale deployment of AI systems?
12269600	12273920	Well, so I've thought a lot about it and there's a lot of different parts to this problem,
12273920	12279520	everything from job displacement to sky nut, things like this. And so you can zoom in to
12279520	12287360	sub parts of this problem. I'm not super optimistic about AGI being solved next year.
12288320	12294000	I don't think that's going to happen personally. So you have a kind of zen like calm about. Is
12294000	12301360	there's a nervousness because the leap of GBT4 seemed so big. Sure. It's like we're almost...
12302000	12306400	There's some kind of transition era period. You're thinking... Well, so I mean,
12306400	12311760	there's a couple of things going on there. One is I'm sure GPT5 and 7 and 19 will be
12311760	12317200	also huge leaps. They're also getting much more expensive to run. And so there may be a limiting
12317200	12322560	function in terms of just expense on one hand and train. That could be a limiter that slows things
12322560	12329360	down. But I think the bigger limiter outside of sky nut takes over and I don't spend any time thinking
12329360	12332720	about that because if sky nut takes over and kills us all, then I'll be dead. So I don't worry about
12332720	12340080	that. Other things worry about, I'll just focus on. I'll focus and not worry about that one.
12341280	12347760	But I think that the other thing I'd say is that AI moves quickly, but humans move slowly and we
12347760	12354800	adapt slowly. And so what I expect to happen is just like any technology diffusion, the promise
12354800	12361120	and then the application takes time to roll out. And so I think that I'm not even too worried about
12361760	12365920	autonomous cars defining away all the taxi drivers. Remember, autonomy is supposed to be
12365920	12373600	solved by 2020? Boy, do I remember. And so I think that on the one hand we can see amazing
12373600	12379040	progress, but on the other hand we can see that the reality is a little bit more complicated
12379040	12383840	and it may take longer to roll out than you might expect. Well, that's in the physical space. I do
12383840	12389520	think in the digital space is the stuff that's built on top of LLMs that runs
12391120	12396160	millions of apps that could be built on top of them. And that could be run on millions of devices,
12396160	12404880	millions of types of devices. I just think that the rapid effect it has on human civilization could
12404880	12413760	be truly transformative to it. And there I think it depends on are you an optimist or a pessimist
12413760	12422080	or a masochist? Just to clarify, optimist about human civilization. Me too. And so I look at that
12422080	12426880	as saying, okay, cool, what will AI do? And so some people say, oh my god, is it going to destroy
12426880	12431760	us all? How do we prevent that? I kind of look at it from a, is it going to unlock us all?
12432640	12435520	You talk about coding, is it going to make so I don't have to do all the repetitive stuff?
12436560	12441600	Well, suddenly that's a very optimistic way to look at it. And you look at what a lot of these
12441600	12445360	technologies have done to improve our lives. And I want that to go faster.
12447120	12451120	What do you think the future of programming looks like in the next 10, 20, 30, 50 years
12452160	12460720	with LLMs and with Mojo with modular, like your vision for devices, the hardware to the
12460720	12465520	compilers to this, to the different stacks of software. Yeah. Well, so what I want, I mean,
12465920	12473120	coming back to my arch nemesis, it's complexity. So again, me being the optimist, if we drive down
12473120	12478000	complexity, we can make these tools, these technologies, these cool hardware widgets
12478000	12482560	accessible to way more people. And so what I'd love to see is more personalized experiences,
12482560	12487280	more things, the research getting into production instead of being lost at NeurIPS.
12488240	12495760	Right. And like these things that impact people's lives by entering products. And so one of the
12495760	12501360	things that I'm a little bit concerned about is right now, the big companies are investing huge
12501360	12506720	amounts of money and are driving the top line of AI capability forward really quickly. But if it
12506720	12513120	means that you have to have $100 million to train a model or more $100 billion, right? Well, that's
12513120	12517840	going to make it very concentrated with very few people in the world that can actually do this
12517840	12524160	stuff. I would much rather see lots of people across the industry be able to participate and use
12524160	12528080	this, right? And you look at this, you know, I mean, a lot of great research has been done
12528640	12535040	in the health world and looking at like detecting pathologies and doing radiology with AI and like
12535040	12539680	doing all these things. Well, the problem today is that to deploy and build these systems, you have
12539680	12546880	to be an expert in radiology and an expert in AI. And if we can break down the barriers so that more
12546880	12553440	people can use AI techniques, it's more like programming Python, which roughly everybody
12553440	12557600	can do if they want to, right? Then I think that we'll get a lot more practical application of
12557600	12563040	these techniques and a lot more niche year, cool, but narrower demands. And I think that's
12563040	12567520	that's going to be really cool. Do you think we'll have more or less programmers in the world than
12567520	12572720	now? Well, so I think we'll have more more programmers, but they may not consider themselves
12572720	12576080	to be programmers. That'd be a different name for you, right? I mean, do you consider somebody
12576080	12581760	that uses, you know, I think that arguably the most popular programming language is Excel?
12582640	12588400	Yeah. Right? Yep. And so do they consider themselves to be programmers? Maybe not. I mean,
12588400	12595440	some of them make crazy macros and stuff like that. But but but what what you mentioned, Steve
12595440	12602160	Jobs, is it's the bicycle for the mind that allows you to go faster, right? And so I think that as
12602160	12607280	we look forward, right? What is AI? I look at it as hopefully a new programming paradigm. It's like
12607280	12611520	object-oriented programming, right? If you want to write a cat to texture, you don't use for loops.
12612320	12616560	Turns out that's not the right tool for the job, right? And so right now, unfortunately,
12616560	12620640	because I mean, it's not unfortunate, but it's just kind of where where things are. AI is this
12620640	12625840	weird, different thing that's not integrated into programming languages and normal tool chains and
12625840	12630720	all the technology is really weird and doesn't work right. And you have to babysit it. And
12630720	12635040	every time you switch hardware, it's different. It shouldn't be that way. When you change that,
12635040	12639360	when you fix that, suddenly, again, the tools technologies can be way easier to use. You
12639360	12642960	can start using them for many more things. And so that's that's why I would be excited about.
12643680	12648160	What kind of advice could you give to somebody in high school right now or maybe early college
12648160	12655440	who's curious about programming and feeling like the world is changing really quickly here?
12655440	12660960	Yeah. Well, what kind of stuff to learn? What kind of stuff to work on? Should they finish college?
12660960	12665440	Should they go work at a company? Should they build a thing? What do you think?
12665440	12670000	Well, so I mean, one of the things I'd say is that you'll be most successful if you work on
12670000	12676640	something you're excited by. And so don't get the book and read the book cover to cover and study
12676640	12681520	and memorize and recite and flashcard and go build something. Like go solve a problem. Go
12681520	12688000	build the thing that you want to exist. Go build an app. Go train a model. Like go build something
12688000	12692400	and actually use it and set a goal for yourself. And if you do that, then you'll, you know,
12692400	12696720	there's a success. There's the adrenaline rush. There's the achievement. There's the unlock that
12696720	12700720	I think is where, you know, if you keep setting goals and you keep doing things and building
12700720	12705760	things, learning by building is really powerful. In terms of career advice, I mean,
12705760	12711680	everybody's different. It's very hard to give generalized advice. I'll speak as a compiler
12711680	12718880	nerd. If everybody's going left, sometimes it's pretty cool to go right. And so just because
12718880	12723840	everybody's doing a thing, it doesn't mean you have to do the same thing and follow the herd.
12723840	12730560	In fact, I think that sometimes the most exciting paths through life lead to being curious about
12730560	12736480	things that nobody else actually focuses on, right? And turns out that understanding deeply
12736480	12741280	parts of the problem that people want to take for granted makes you extremely valuable and
12741280	12746720	specialized in ways that the herd is not. And so again, I mean, there's lots of rooms for
12746720	12751040	specialization, lots of rooms for generalists. There's lots of room for different kinds and
12751040	12756080	parts of the problem. But I think that it's, you know, just because everybody's doing one thing
12756160	12761440	doesn't mean you should necessarily do it. And now the herd is using Python. So if you want to
12761440	12769200	be a rebel, go check out Mojo and help Chris and the rest of the world fight the arch nemesis
12769200	12774080	of complexity, because simple is beautiful. There you go. Because you're an incredible person.
12774080	12779040	You've been so kind to me ever since we met. You've been extremely supportive. I'm forever
12779040	12784320	grateful for that. Thank you for being who you are, for being legit, for being kind, for fighting
12784320	12792400	this really interesting problem of how to make AI accessible to a huge number of people, a huge
12792400	12797680	number of devices. Yeah. Well, so Lex, you're a pretty special person too, right? And so I think
12797680	12802160	that, you know, one of the funny things about you is that besides being curious and pretty damn
12802160	12807600	smart, you're actually willing to push on things. And I think that you've got an agenda to like
12807600	12813200	make the world think, which I think is a pretty good agenda. It's a pretty good one. Thank you so
12813200	12818000	much for talking, Chris. Yeah, thanks, Lex. Thanks for listening to this conversation with Chris
12818000	12823200	Ladner. To support this podcast, please check out our sponsors in the description. And now,
12823200	12830160	let me leave you some words from Isaac Asimov. I do not fear computers. I fear the lack of them.
12831600	12834480	Thank you for listening and hope to see you next time.
