1
00:00:00,000 --> 00:00:04,720
of radical political movement, of which there will always be a lot in the country,

2
00:00:05,680 --> 00:00:10,000
has managed to do something that a radical movement is not supposed to be able to do in the U.S.,

3
00:00:10,640 --> 00:00:16,880
which is they've managed to hijack institutions all across the country and hijack medical journals

4
00:00:16,880 --> 00:00:25,280
and universities and the ACLU, all the activist organizations and nonprofits and many tech companies.

5
00:00:25,840 --> 00:00:31,280
And the way I view a liberal democracy is it is a bunch of these institutions that were trial

6
00:00:31,280 --> 00:00:38,240
and error crafted over hundreds of years. And they all rely on trust, public trust,

7
00:00:38,240 --> 00:00:43,360
and there's certain kind of feeling of unity that actually is critical to a liberal democracy's

8
00:00:43,360 --> 00:00:50,720
functioning. And what I see this thing is as a parasite on that, that whose goal is, and I'm not

9
00:00:50,720 --> 00:00:54,880
saying each, by the way, each individual in this is, I don't think they're bad people. I think that

10
00:00:55,120 --> 00:01:01,360
it's the ideology itself has the property of its goal is to tear apart the pretty delicate

11
00:01:01,360 --> 00:01:04,960
workings of the liberal democracy and shred the critical lines of trust.

12
00:01:07,440 --> 00:01:11,680
The following is a conversation with Tim Urban, his second time in the podcast.

13
00:01:11,680 --> 00:01:16,000
He's the author and illustrator of the amazing blog called Wait But Why,

14
00:01:16,560 --> 00:01:22,240
and is the author of a new book coming out tomorrow called What's Our Problem, a self-help

15
00:01:22,240 --> 00:01:28,560
book for societies. We talk a lot about this book in this podcast, but you really do need to get it

16
00:01:28,560 --> 00:01:34,480
and experience it for yourself. It is a fearless, insightful, hilarious, and I think important

17
00:01:34,480 --> 00:01:40,240
book in this divisive time that we live in. The Kindle version, the audiobook, and the web version

18
00:01:40,240 --> 00:01:46,400
should be all available on date of publication. I should also mention that my face might be a bit

19
00:01:46,400 --> 00:01:54,320
more beat up than usual. I got hit in the chin pretty good since I've been getting back into

20
00:01:54,320 --> 00:02:00,560
training jujitsu, a sport I love very much, after recovering from an injury. So if you see marks on

21
00:02:00,560 --> 00:02:06,480
my face during these intros or conversations, you know that my life is in a pretty good place.

22
00:02:07,280 --> 00:02:12,400
This is the Lex Friedman podcast. The supported, please check out our sponsors in the description,

23
00:02:12,400 --> 00:02:20,080
and now, dear friends, here's Tim Urban. You wrote an incredible book called What's Our Problem,

24
00:02:20,080 --> 00:02:29,920
a self-help book for societies. In the beginning, you present this view of human history as a

25
00:02:29,920 --> 00:02:38,160
thousand-page book where each page is 250 years, and it's a brilliant visualization because almost

26
00:02:38,160 --> 00:02:44,400
nothing happens for most of it. So what blows your mind most about that visualization when you

27
00:02:44,400 --> 00:02:50,960
just sit back and think about it? It's a boring book. So 950 pages, 95% of the book, hunter-gatherer

28
00:02:50,960 --> 00:02:54,480
is kind of doing their thing. I'm sure there's, you know, there's obviously some major cognitive

29
00:02:54,480 --> 00:02:59,040
and advancements along the way in language, and I'm sure, you know, the bow and arrow comes around

30
00:02:59,040 --> 00:03:03,760
at some point, you know, so tiny things, but it's like, oh, now 400 pages till the next thing.

31
00:03:03,760 --> 00:03:06,560
But then you get to page 950 and things start moving.

32
00:03:06,560 --> 00:03:09,840
Recorded history starts at 976. Right, right.

33
00:03:09,840 --> 00:03:14,480
So basically, the bottom row is when anything interesting happens. There's a bunch of agriculture

34
00:03:14,480 --> 00:03:19,280
for a while before we know anything about it, and then the recorded history starts.

35
00:03:19,280 --> 00:03:26,000
Yeah, 25 pages of actual, like, recorded history. So when we think of prehistoric,

36
00:03:26,000 --> 00:03:34,800
we're talking about pages one through 975 of the book. And then history is page, you know, 976 to

37
00:03:34,800 --> 00:03:40,320
1000. If you were reading the book, it would be like epilogue AD, you know, the last little 10 pages

38
00:03:40,320 --> 00:03:45,680
of the book. And we think of AD is super long, right? 2000 years, the Roman Empire 2000 years

39
00:03:45,680 --> 00:03:52,160
ago, like that's so long. Yeah. Human history has been going on for over 2000 centuries.

40
00:03:53,120 --> 00:03:58,160
Like, that is, it's just, it's hard to wrap your head around. And this is, I mean, even that's just

41
00:03:58,160 --> 00:04:04,320
the end of a very long road. Like, you know, the 100,000 years before that, it's not like, you know,

42
00:04:04,320 --> 00:04:10,400
it's not like that was that different. So it's just, there's been people like us that have emotions

43
00:04:10,400 --> 00:04:20,160
like us, that have physical sensations like us for so, so long. And who are they all? And what was

44
00:04:20,240 --> 00:04:25,520
their life like? And it's, you know, I think we have no idea what it was like to be them. The thing

45
00:04:25,520 --> 00:04:30,640
that's craziest about the people of the far past is not just that they had different lives, they had

46
00:04:30,640 --> 00:04:34,000
different fears, they had different dangers and different responsibilities, and they lived in

47
00:04:34,000 --> 00:04:38,320
tribes and everything, but they didn't know anything. Like, we just take it for granted that

48
00:04:38,320 --> 00:04:44,480
we're born on top of this tower of knowledge. And from the very beginning, we, we know that the

49
00:04:44,480 --> 00:04:49,440
earth is a ball floating in space. And we know that we're going to die one day. And we know that

50
00:04:50,720 --> 00:04:56,000
you know, we evolved from animals and all the, those were all like incredible, you know,

51
00:04:56,000 --> 00:05:01,280
epiphanies quite recently. And the people a long time ago, they just had no idea what was going on.

52
00:05:01,280 --> 00:05:04,560
And like, I'm kind of jealous, because I feel like it, you know, might have been scary to not

53
00:05:04,560 --> 00:05:08,320
know what's going on, but it also I feel like would be, you'd have a sense of awe and wonder all

54
00:05:08,320 --> 00:05:11,440
the time. And, and you don't know what's going to happen next. And it's once you learn, you're

55
00:05:11,440 --> 00:05:16,240
kind of like, oh, that's like, it's a little grim. But they probably had the same capacity for

56
00:05:16,320 --> 00:05:22,480
consciousness to experience the world, to wander about the world, maybe to construct

57
00:05:22,480 --> 00:05:29,520
narratives about the world and myths and so on. They just had less grounded systematic facts to

58
00:05:29,520 --> 00:05:35,120
play with. They've still probably felt the narratives, the myths that constructed as

59
00:05:35,120 --> 00:05:41,760
intensely as we do. Oh yeah, they also fell in love. They also had friends and they had

60
00:05:41,760 --> 00:05:46,480
falling outs with friends. They didn't shower much though. They didn't not smell nice.

61
00:05:47,520 --> 00:05:52,640
Maybe they did. Maybe beauty's in the high of the beholden. Maybe it's all like relative.

62
00:05:53,520 --> 00:05:58,800
How many people in history have experienced a hot shower? Like almost none. That's like,

63
00:05:58,800 --> 00:06:05,840
one more hot shower was invented 100 years ago, like less. So like George Washington never had

64
00:06:05,840 --> 00:06:10,880
a hot shower. It's like, it's just kind of weird. Like he took cold showers all the time or like,

65
00:06:12,480 --> 00:06:16,960
and again, we just take this for granted. But that's like an unbelievable life experience

66
00:06:16,960 --> 00:06:23,760
to have a rain, a controlled little booth where it rains hot water on your head. And then you get

67
00:06:23,760 --> 00:06:29,040
out and it's not everywhere. It's like contained. That was like, you know, a lot of people probably

68
00:06:29,040 --> 00:06:33,680
lived and died with never experiencing hot water. Maybe they had a way to heat water over a fire.

69
00:06:33,680 --> 00:06:38,960
But like then it's, I don't know, it's just like, there's so many things about our lives now

70
00:06:38,960 --> 00:06:43,680
that are completely, just total anomaly. It makes me wonder, like, what is the thing

71
00:06:43,680 --> 00:06:49,120
they would notice the most? I mean, the sewer system, like it doesn't smell in cities.

72
00:06:50,000 --> 00:06:54,080
Incredible. What does the sewer system do? I mean, it gets rid of waste efficiently,

73
00:06:54,080 --> 00:06:58,640
etc. We don't have to confront it, both with our, with any of our senses. And that's probably

74
00:06:58,640 --> 00:07:02,480
wasn't there. I mean, what else? Plus all the medical stuff associated with sewage.

75
00:07:02,480 --> 00:07:06,720
Yeah, I mean, how about the disease? Yeah. How about the cockroaches and the rats and the,

76
00:07:06,720 --> 00:07:11,680
and the disease and the plagues and, you know, and then when they got, so they caught more

77
00:07:11,680 --> 00:07:16,160
diseases, but then when they caught the disease, they also didn't have treatment for it. So they

78
00:07:16,160 --> 00:07:21,840
often would die or they would just be in a huge amount of pain. They also didn't know what the

79
00:07:21,840 --> 00:07:25,680
disease was. They didn't know about microbes. That was this new thing, the idea that these tiny

80
00:07:25,680 --> 00:07:30,000
little animals that are causing these diseases. So what did they think, you know, in the, the

81
00:07:30,000 --> 00:07:35,680
bubonic plague, you know, in the Black Death, the 1300s, people thought that it was an active

82
00:07:35,680 --> 00:07:40,080
God because, you know, God's angry at us because why would, you know, why would you not think that

83
00:07:40,080 --> 00:07:45,120
if you didn't know what it was? And so the crazy thing is that these were the same primates. So

84
00:07:45,120 --> 00:07:50,320
I do know something about them. I know in some sense what it's like to be them because I am a

85
00:07:50,320 --> 00:07:55,280
human as well. And to know that this particular primate that I know what it's like to be

86
00:07:56,000 --> 00:08:01,760
experienced such different things. It's, and like this isn't, our life is not the life that

87
00:08:01,760 --> 00:08:06,240
this primate has experienced almost ever. So it's just, it's just a bit strange.

88
00:08:06,800 --> 00:08:12,880
I don't know. I have a sense that we would get acclimated very quickly. Like if we threw ourselves

89
00:08:12,880 --> 00:08:18,000
back a few thousand years ago, it would be very uncomfortable at first, but the whole hot shower

90
00:08:18,000 --> 00:08:24,880
thing, you'll get used to it after a year. You would not even like miss it. Because there's a few,

91
00:08:24,880 --> 00:08:28,080
trying to remember which book that talks about hiking the Appalachian Trail,

92
00:08:28,080 --> 00:08:33,040
but you kind of miss those hot showers. But I have a sense like after a few months,

93
00:08:33,040 --> 00:08:38,800
after a few years. Well, your skill recalibrates. Yeah. Yeah. I was saying the other day to a

94
00:08:38,800 --> 00:08:42,640
friend that whatever you used to, you start to think that, oh, that the people that have more

95
00:08:42,640 --> 00:08:47,440
than me are more fortunate. Like, it just sounds incredible. I would be so happy. But you know,

96
00:08:47,440 --> 00:08:51,360
that's not true. Because experience, what would happen is you would, you would, you would get

97
00:08:51,360 --> 00:08:55,120
these new things or you would, you would get these new opportunities and then you would get

98
00:08:55,120 --> 00:08:58,720
used to it. And then you would this, the hedonic treadmill, you'd come back to where you are.

99
00:08:58,720 --> 00:09:02,640
And likewise though, because you think, oh my God, what if I had to, you know,

100
00:09:02,640 --> 00:09:07,280
have this kind of job that I never would want? Or I had this kind of marriage that I never would

101
00:09:07,280 --> 00:09:11,920
want. You know what, if you did, you would adjust and get used to it and you might not be that much

102
00:09:11,920 --> 00:09:16,880
less happy than you are now. So on the other side of the, you being okay going back, you know,

103
00:09:16,880 --> 00:09:21,040
we would survive if we had to go back. You know, we'd have to learn some skills and,

104
00:09:21,040 --> 00:09:24,640
but, but we would buck up and, you know, people have gone to war before they were in the, you

105
00:09:24,640 --> 00:09:27,520
know, shopkeepers a year before that, they were in the trenches the next year.

106
00:09:28,480 --> 00:09:32,400
But on the other hand, if you brought them here, you know, I always think it would be so fun to

107
00:09:32,400 --> 00:09:36,960
just bring, forget the hunter-gatherers, bring a 1700s person here and tour them around, take them

108
00:09:36,960 --> 00:09:40,880
on an airplane, show them your phone and all the things that can do, show them the internet,

109
00:09:40,880 --> 00:09:45,920
show them the grocery store, imagine taking them to a Whole Foods. Likewise, I think they would

110
00:09:45,920 --> 00:09:51,600
be completely awestruck and on their knees crying tears of joy and then they'd get used to it and

111
00:09:51,600 --> 00:09:55,600
they'd be complaining about like, you know, you don't have the oranges in stock is like, you know,

112
00:09:55,600 --> 00:10:00,480
and that's the grocery store is a tough one to get used to. Like when I first came to this country,

113
00:10:01,040 --> 00:10:07,840
the, the abundance of bananas was the thing that struck me the most or like fruits in general,

114
00:10:07,840 --> 00:10:13,360
but food in general, but bananas somehow struck me the most that you could just eat them as much

115
00:10:13,360 --> 00:10:18,240
as you want. And that took a long time for me, probably took several years to really

116
00:10:20,400 --> 00:10:23,840
get acclimated to that. Is that- Why didn't you have bananas?

117
00:10:24,880 --> 00:10:32,320
The number of bananas, fresh bananas, I don't, that wasn't available. Bread, yes, bananas, no.

118
00:10:33,680 --> 00:10:37,920
Yeah, it's like, we don't even know what to, like, we don't even know the proper levels of

119
00:10:37,920 --> 00:10:41,840
gratitude. You know, walking around the grocery store, I don't know to be like, the bread's nice,

120
00:10:41,840 --> 00:10:45,520
but the bananas are like, we're so lucky. I don't know. I'm like, oh, I could have been the other

121
00:10:45,520 --> 00:10:52,720
way. I have no idea. Well, it's interesting then where we point our gratitude in the West,

122
00:10:52,720 --> 00:11:02,240
in the United States. Probably, do we point it away from materialist possessions towards,

123
00:11:02,800 --> 00:11:08,720
or do we just aspire that to do that towards other human beings that we love? Because in the

124
00:11:08,720 --> 00:11:17,040
East and the Soviet Union, growing up poor is having food is the gratitude. Having transportation

125
00:11:17,040 --> 00:11:24,880
is gratitude. Having warmth and shelter is gratitude. And now, but see within that, the deep

126
00:11:24,880 --> 00:11:30,000
gratitude is for other human beings. It's the penguins huddling together for warmth in the cold.

127
00:11:30,000 --> 00:11:33,920
I think it's a person by person basis. I mean, I'm sure, yes, of course, in the West,

128
00:11:34,240 --> 00:11:38,640
on average, feel gratitude towards different things, or maybe a different level of gratitude.

129
00:11:38,640 --> 00:11:43,920
Maybe we feel less gratitude than countries that, you know, obviously, I think the easiest,

130
00:11:43,920 --> 00:11:48,960
the person that's most likely to feel gratitude is going to be someone whose life happens to be one

131
00:11:48,960 --> 00:11:53,360
where they just move up, up, up throughout their life. A lot of people in the greatest generation,

132
00:11:53,360 --> 00:11:57,280
you know, people who were born in the 20s or whatever. And a lot of the boomers too.

133
00:11:57,280 --> 00:12:01,040
This story is the greatest generation grew up dirt poor, and they often ended up middle class.

134
00:12:01,760 --> 00:12:06,720
And the boomers, some of them started off middle class, and many of them ended up quite wealthy.

135
00:12:06,720 --> 00:12:14,240
And I feel like that life trajectory is naturally going to foster gratitude, right?

136
00:12:15,040 --> 00:12:17,600
Because you're not going to take for granted these things because you didn't have them.

137
00:12:18,160 --> 00:12:22,320
You know, I didn't go out of the country really in my childhood very much, you know, like, you know,

138
00:12:23,040 --> 00:12:28,160
we traveled, but it was to Virginia to see my grandparents or Wisconsin to see other relatives

139
00:12:28,160 --> 00:12:31,840
or, you know, maybe Florida after going out to the beach. And then I started going out of the

140
00:12:31,840 --> 00:12:37,360
country like crazy in my 20s because I really, you know, became my favorite thing. And I feel like

141
00:12:37,360 --> 00:12:40,880
because I, if I had grown up always doing that, it would have been another thing. I'm like, yeah,

142
00:12:40,880 --> 00:12:45,760
it's just something I do. But I still, every time I go to a new country, I'm like, oh my god,

143
00:12:45,760 --> 00:12:48,960
this is so cool. And in another country, this thing I've only seen on the map, I'm like, I'm

144
00:12:48,960 --> 00:12:54,640
there now. And so I feel like it's a lot of times it's a product of what you didn't have,

145
00:12:54,640 --> 00:13:01,040
and then you suddenly had. But I still think it's case by case in that there's like a meter

146
00:13:01,680 --> 00:13:10,400
in everyone's head, you know, that I think on, at a 10, you're experiencing just immense

147
00:13:10,400 --> 00:13:17,200
gratitude, right, which is a euphoric feeling. It's a great feeling. And it's, it makes you happy.

148
00:13:17,920 --> 00:13:22,560
To savor what you have, to look down at the mountain of stuff that you have that you're

149
00:13:22,560 --> 00:13:27,680
standing on, right, to look, to look down at it and say, oh my god, I'm so lucky. And I'm so

150
00:13:27,680 --> 00:13:31,760
grateful for this and this and this. And, you know, obviously, that's a happy exercise. Now,

151
00:13:31,760 --> 00:13:36,000
when you move the meter down to six or seven, maybe you think that sometimes, but you're,

152
00:13:36,000 --> 00:13:42,160
you're not always thinking that because you're sometimes looking up at this cloud of things

153
00:13:42,160 --> 00:13:45,840
that you don't have and the things that they have, but you don't or the things you wished you had,

154
00:13:45,840 --> 00:13:50,000
or you thought you were going to have or whatever. And that's the opposite direction to look,

155
00:13:50,000 --> 00:13:56,160
right? And that's the, either that's, that's envy, that's yearning, or often it's, if you think

156
00:13:56,160 --> 00:14:02,080
about your past, it's grievance, right? And so then you go into a one and you have someone who

157
00:14:02,080 --> 00:14:07,520
feels like a complete victim. They are just a victim of the society, of their siblings and

158
00:14:07,520 --> 00:14:15,200
their parents and their loved one. And they are, they're wallowing in everything that's happened

159
00:14:15,200 --> 00:14:18,960
wrong to me, everything I should have that I don't, everything that has gone wrong for me.

160
00:14:19,040 --> 00:14:24,240
And so that's a very unhealthy, mentally unhealthy place to be. Anyone can go there. You know,

161
00:14:24,240 --> 00:14:28,480
there's an endless list of stuff you can, it can be aggrieved about and an endless list of stuff

162
00:14:28,480 --> 00:14:32,960
you can have gratitude for. And so it's, it's, in some ways, it's a choice and it's a habit.

163
00:14:32,960 --> 00:14:36,320
And maybe it's part of how you're raised or a natural demeanor, but it's such a good exercise.

164
00:14:36,320 --> 00:14:39,280
You are really good at this, by the way. Your Twitter is like-

165
00:14:39,280 --> 00:14:39,840
Go on.

166
00:14:40,800 --> 00:14:48,240
Well, like, you're constantly just saying, man, I'm lucky. Or like, I'm so grateful for this.

167
00:14:48,240 --> 00:14:52,160
And that's, it's, it's a good thing to do because you're reminding yourself, but you're also

168
00:14:52,160 --> 00:14:56,800
reminding other people to think that way. And it's like, we are lucky, you know, and,

169
00:14:57,840 --> 00:15:00,880
and so anyway, I think that scale can go from one to 10. And I think it's hard to be a 10.

170
00:15:00,880 --> 00:15:05,440
I think you'd be very happy if you could be. But I think trying to be above a five and looking down

171
00:15:05,440 --> 00:15:10,640
at the things you have more often than you are looking up at the things you don't or being,

172
00:15:10,640 --> 00:15:14,320
you know, resentful about the things that people have wronged you and-

173
00:15:14,320 --> 00:15:19,760
Well, the interesting thing, I think, was an open question, but I suspect that you can control

174
00:15:19,760 --> 00:15:24,640
that knob for, for the individual. Like you yourself can choose, it's like the Stoic philosophy,

175
00:15:24,640 --> 00:15:29,200
you could choose where you are as a matter of habit, like you said. But you can also probably

176
00:15:29,200 --> 00:15:36,240
control that on a scale of a family, of a tribe, of a nation, of a society. I mean, a lot, you

177
00:15:36,240 --> 00:15:40,640
couldn't describe a lot of the things that happened in Nazi Germany and different other parts of

178
00:15:40,640 --> 00:15:46,880
history through a sort of societal envy and resentment that builds up. Maybe certain narratives

179
00:15:47,840 --> 00:15:53,200
pick up and then they infiltrate your mind. And then now your knob goes to, from the gratitude

180
00:15:53,200 --> 00:15:57,840
for everything, it goes to resentment and envy and all the- Germany between the two world wars,

181
00:15:57,840 --> 00:16:05,760
you know, like you said, the Soviet kind of mentality. So yeah. And then when you're soaking

182
00:16:05,760 --> 00:16:12,560
in a culture, so there's this kind of two factors, right? It's, it's what's going on in your own head

183
00:16:12,560 --> 00:16:15,760
and then what's surrounding you. And what's surrounding you kind of has concentric circles.

184
00:16:15,760 --> 00:16:20,640
There's your immediate group of people, because that group of people, if they're a certain way,

185
00:16:20,640 --> 00:16:23,840
if they feel a lot of gratitude and they talk about it a lot, that kind of insulates you from

186
00:16:23,840 --> 00:16:28,160
the broader culture because, you know, the people are going to have the most impact on you or the

187
00:16:28,160 --> 00:16:32,880
ones closest. But often, they're all the, all the concentric circles are saying the same thing,

188
00:16:32,880 --> 00:16:36,080
the people around you or they're feeling the same way that the broader community, which is

189
00:16:36,080 --> 00:16:41,760
feeling the same way as the broader country. And, you know, I think this is why I think American

190
00:16:41,760 --> 00:16:47,200
patriotism, you know, nationalism, you know, can be tribal, can be very not, not a good thing.

191
00:16:48,000 --> 00:16:55,280
Patriotism, I think is, is a great thing because really, what is patriotism? I mean, it's, if you

192
00:16:55,280 --> 00:16:58,320
love your country, you should love your fellow countrymen, you know, to patriot, you know,

193
00:16:58,320 --> 00:17:04,480
that's a rigged quote. It's like patriotism is like, I think a feeling of like unity

194
00:17:06,000 --> 00:17:10,400
and, but it also comes along with an implicit kind of concept of gratitude because it's like,

195
00:17:10,400 --> 00:17:13,600
we are so lucky to live in, you know, people, you know, think it's chauvinist to say,

196
00:17:13,600 --> 00:17:17,120
we live in the best country in the world, right? And, you know, yes, when Americans say that,

197
00:17:17,120 --> 00:17:21,520
no one likes it, right? But actually, it's not a bad thing to think. It's a nice thing to think.

198
00:17:21,520 --> 00:17:26,400
It's a way of saying, I'm so grateful for all the great things this country gives to me and this

199
00:17:26,400 --> 00:17:29,680
country has done. And, and I think, you know, if you heard the Filipino, you know, a Filipino

200
00:17:29,680 --> 00:17:32,640
person say, you know what, the Philippines is the best country in the world, no one in America

201
00:17:32,640 --> 00:17:36,720
would say that's chauvinist. They'd say, awesome, right? Because when it's coming from someone,

202
00:17:36,720 --> 00:17:41,280
you know, who's not American, it sounds totally fine. But I think, I think, you know, national

203
00:17:41,280 --> 00:17:45,360
pride is actually good. Now, again, that can quickly translate into xenophobia and nationalism.

204
00:17:45,360 --> 00:17:47,360
And so, you know, you have to make sure it doesn't go off that cliff. But

205
00:17:47,760 --> 00:17:51,840
yeah, there's good ways to formulate that. Like you talk about, we'll talk about like

206
00:17:51,840 --> 00:17:58,000
high rung progressivism, high rung conservatism. Those are two different ways of,

207
00:17:59,120 --> 00:18:04,640
of embodying patriotism. So you could talk about maybe loving the tradition that this

208
00:18:04,640 --> 00:18:11,040
country stands for, or you could talk about loving the people that ultimately push progress. And

209
00:18:11,040 --> 00:18:16,800
those are, from an intellectual perspective, a good way to represent patriotism. We gotta zoom

210
00:18:16,800 --> 00:18:22,160
out because this, this graphic is epic. A lot of images in your book are just epic on their own.

211
00:18:22,720 --> 00:18:27,600
It's brilliantly done. But this one has famous people for each of the cards,

212
00:18:29,440 --> 00:18:35,520
like the best of. Yeah. For you. And by the way, good for them to be the person. Yeah. That,

213
00:18:35,520 --> 00:18:39,280
it's not that I could have chosen lots of people for each card. But I think most people would

214
00:18:39,280 --> 00:18:44,000
agree, you know, that's a pretty fair choice for each, each page. And good for them to be,

215
00:18:44,080 --> 00:18:48,480
you know, you, you crushed it if you can be the person for your whole 250 year page. So.

216
00:18:48,480 --> 00:18:51,040
Well, I noticed he put Gandhi, didn't put Hitler. I mean,

217
00:18:51,680 --> 00:18:55,200
there's a lot of people going to argue with you about that particular last page.

218
00:18:55,200 --> 00:18:59,040
True. Yes, you're right. I could have, I could have put him, I actually, I was thinking about

219
00:18:59,040 --> 00:19:03,840
Darwin there too. Darwin. Einstein. Yeah, exactly. You really could have put anyone.

220
00:19:03,840 --> 00:19:06,960
You think about putting yourself first. Yeah, I should have. I should have. That would have

221
00:19:06,960 --> 00:19:10,400
been awesome. I'm sure that would have endeared the readers to me from right from the beginning

222
00:19:10,640 --> 00:19:15,200
of the first page of the book. A little bit of a messianic complex going on. But yeah,

223
00:19:15,200 --> 00:19:20,720
so the list of people just, you know, so these are 250 year chunks, the last one being from

224
00:19:20,720 --> 00:19:29,280
1770 to 2020. And so it goes Gandhi, Shakespeare, Joan of Arc, Jengus Khan, Charlemagne, Muhammad,

225
00:19:29,280 --> 00:19:36,320
Constantine, Jesus, Cleopatra, Aristotle, Buddha. That's so interesting to think about this very

226
00:19:36,320 --> 00:19:42,000
recent human history. That's 11 pages. So it would be 2750, almost 3000 years.

227
00:19:42,640 --> 00:19:48,080
Just that there's these figures that stand out and then define the course of human history.

228
00:19:48,880 --> 00:19:55,440
It's like the crazy, the craziest thing to me is that like Buddha was a dude. He was a guy with like

229
00:19:56,320 --> 00:20:02,000
arms and legs and fingernails that he may be bit and like, he liked certain foods and maybe he got

230
00:20:02,000 --> 00:20:09,040
like, you know, he had like digestive issues sometimes and like, he got cuts and they stung

231
00:20:09,040 --> 00:20:14,640
and like he was a guy and he had hopes and dreams and he probably had a big ego for a while before

232
00:20:14,640 --> 00:20:19,440
he I guess Buddha totally overcame that one. But like, and it's like who knows, you know,

233
00:20:20,080 --> 00:20:24,000
the mythical figure of Buddha who knows how similar he was. But the fact same with Jesus,

234
00:20:24,000 --> 00:20:30,720
like this was a good guy. Like to me, it's he's a primate. What impact? He was a cell first and

235
00:20:30,720 --> 00:20:35,520
then a baby. Yeah. And he was a fetus at some point. A dumb baby trying to learn how to walk.

236
00:20:35,520 --> 00:20:40,080
Yeah. Like having tantrum. Yeah. Because he's frustrated because he's in the terrible twos.

237
00:20:40,080 --> 00:20:43,760
Jesus was in the terrible twos. Buddha never had a tantrum. Let's be honest. The myth.

238
00:20:44,480 --> 00:20:50,560
The mother was like, this baby is great. Wow. Figure something out. It just but I mean,

239
00:20:50,560 --> 00:20:55,600
this, I mean, listen, learning about Genghis Khan, it's incredible to me because it's just like,

240
00:20:55,600 --> 00:21:04,560
this was some Mongolian, you know, herder guy who was taken as a slave and he was like dirt poor,

241
00:21:05,280 --> 00:21:11,200
you know, catching rats is a, you know, young teen with, you know, to feed him and his mom and his,

242
00:21:11,200 --> 00:21:20,000
I think his brother. And it's just like the odds on when he was born, he was just one of,

243
00:21:20,960 --> 00:21:27,600
you know, probably tens of thousands of random teen boys living in Mongolia in the 1200s. The

244
00:21:27,600 --> 00:21:32,240
odds of that person, any one of them being a household name today that we're talking about,

245
00:21:33,200 --> 00:21:39,280
it's just crazy, like what had to happen. And it's for that guy to, for that poor, dirt poor

246
00:21:39,280 --> 00:21:44,640
herder to take over the world. I don't know. So history just like continually blows my mind.

247
00:21:45,600 --> 00:21:48,560
And he's the reason you and I are related, probably.

248
00:21:48,560 --> 00:21:54,960
Yeah, no, I mean, it's also, that's the other thing is that some of these dudes by becoming king,

249
00:21:54,960 --> 00:21:59,120
by being, having a better army at the right time, you know, William the Conqueror, whatever has,

250
00:21:59,120 --> 00:22:02,720
is in the right place at the right time with the right army, you know, and there's a weakness

251
00:22:02,720 --> 00:22:07,200
at the right moment and he comes over and he exploits it and ends up probably having, you

252
00:22:07,200 --> 00:22:12,480
know, I don't know, thousand children. And those children are high up people who maybe have a ton

253
00:22:12,480 --> 00:22:18,560
of, the species is different now because of him. Like if that, forget England's different or, you

254
00:22:18,560 --> 00:22:25,520
know, European borders look different. Like, like we are, like we look different because of a small

255
00:22:25,520 --> 00:22:30,160
handful of people, you know, when I sometimes I think, I'm like, Oh, you know, this part of the

256
00:22:30,160 --> 00:22:34,560
world, I can recognize someone's Greek, you know, someone's Persian, someone's wherever, because,

257
00:22:34,560 --> 00:22:38,160
you know, they kind of have certain facial features. And I'm like, it may have happened. I

258
00:22:38,160 --> 00:22:43,040
mean, obviously it's that that's a population, but it may be that like someone 600 years ago that

259
00:22:43,040 --> 00:22:49,280
looked like that really spread their seed. And that's why the ethnicity looks kind of like that now.

260
00:22:49,280 --> 00:22:55,920
Sorry. Anyway. Yeah. Yeah. Do you think individuals like that can turn the direction of history?

261
00:22:56,960 --> 00:23:00,160
Or is that an illusion that narrative would tell ourselves?

262
00:23:00,880 --> 00:23:04,240
Well, it's both. I mean, so I said William the Conqueror, right? Or Hitler, right?

263
00:23:04,400 --> 00:23:10,400
Right. It's not that Hitler was born and destined to be great at all, right? I think in a lot of cases,

264
00:23:10,400 --> 00:23:15,120
he's some frustrated artist with a temper who's turning over the table in his studio and hitting

265
00:23:15,120 --> 00:23:22,240
his wife and being kind of a dick and a total nobody, right? I think almost all the times you

266
00:23:22,240 --> 00:23:27,120
could have put Hitler baby on earth. He's a, he's a rando, right? You know, and maybe he's a, you

267
00:23:27,120 --> 00:23:31,440
know, maybe sometimes he becomes, you know, some kind of, you know, he uses the speaking ability

268
00:23:31,440 --> 00:23:34,480
because that ability was going to be there either way, but maybe he uses it for something else.

269
00:23:35,360 --> 00:23:41,680
But, but that said, I don't also, I think you, but it's not that World War II was going to happen

270
00:23:41,680 --> 00:23:47,360
either way, right? So it's both. It's that like these circumstances were one way and this person

271
00:23:48,240 --> 00:23:52,240
came along at the right time and those two made a match made in this case hell.

272
00:23:52,240 --> 00:23:58,320
But he makes you wonder, yes, it's a match in hell, but are there other people that could have taken

273
00:23:58,320 --> 00:24:05,200
this place or do these people that stand out? They're the rarest spark of that genius, whether

274
00:24:05,200 --> 00:24:12,080
it take us towards evil, towards good, whether those figures singularly define the trajectory

275
00:24:12,080 --> 00:24:17,520
of humanity. You know, what defines the trajectory of humanity in the 21st century, for example,

276
00:24:17,520 --> 00:24:22,480
might be the influence of AI, might be the influence of nuclear war, negative or positive,

277
00:24:23,440 --> 00:24:33,920
not in the case of nuclear war, but the bioengineering, nanotech, virology,

278
00:24:35,120 --> 00:24:40,000
what else is there? Maybe the structure of governments and so on. Maybe the structure

279
00:24:40,000 --> 00:24:45,200
of universities. I don't know. There could be singular figures that stand up and lead the way

280
00:24:45,200 --> 00:24:51,920
for human, but I wonder if the society is the thing that manifests that person or that person

281
00:24:52,880 --> 00:24:58,080
really does have a huge impact. I think it's probably a spectrum where there are some cases

282
00:24:58,640 --> 00:25:05,200
when a circumstance was such that something like what happened was going to happen. If you pluck

283
00:25:05,200 --> 00:25:09,920
that person from the earth, I don't know whether the Mongols is a good example or not, but maybe

284
00:25:09,920 --> 00:25:16,080
it could be that if you plucked Genghis Khan as a baby, there was because of the specific way

285
00:25:16,160 --> 00:25:22,640
Chinese civilization was at that time and the specific climate that was causing a certain

286
00:25:22,640 --> 00:25:26,880
kind of pressure on the Mongols and the way they still had their great archers and they had their

287
00:25:26,880 --> 00:25:31,600
horses and they had a lot of the same advantages. Maybe it was waiting to happen. It was going to

288
00:25:31,600 --> 00:25:36,560
happen either way and it might not have happened to sit to the extent or whatever. Maybe or you

289
00:25:36,560 --> 00:25:39,920
could go the full other direction and say, actually, this was probably not going to happen.

290
00:25:40,000 --> 00:25:46,560
I think World War II is an example where I kind of think World War II really was kind of the

291
00:25:47,280 --> 00:25:51,200
work of, of course, it relied on all these other circumstances. You had to have the resentment

292
00:25:51,200 --> 00:25:56,160
in Germany. You have to have the Great Depression, but I think if you take Hitler out, I'm pretty

293
00:25:56,160 --> 00:26:01,760
sure World War I. World War II doesn't happen. Well, then it seems like easier to answer these

294
00:26:01,760 --> 00:26:05,600
questions when you look at history, even recent history, but let's look at now. Let's look at,

295
00:26:05,600 --> 00:26:09,520
I'm sure we'll talk about social media. So who are the key players in social media?

296
00:26:10,160 --> 00:26:13,760
Mark Zuckerberg. What's the name of the Myspace guy, Tom?

297
00:26:13,760 --> 00:26:15,600
Tom, it's just Tom. Yeah.

298
00:26:17,360 --> 00:26:22,320
There's a meme going around where Myspace is the perfect social media because no algorithmic

299
00:26:22,320 --> 00:26:27,040
involvement. Everybody's happy and positive. Also, Tom did it right. At the time, we were like,

300
00:26:27,040 --> 00:26:33,680
oh man, Tom only made a few million dollars. It sucks to not be Zuck. Tom might be living a

301
00:26:33,680 --> 00:26:38,400
nice life right now where he doesn't have this nightmare that these other people have.

302
00:26:38,400 --> 00:26:40,240
Yeah. And he's always smiling in his profile pic.

303
00:26:41,920 --> 00:26:46,240
And so there's that clear page. So with Google, that's kind of intermingled into that whole

304
00:26:46,240 --> 00:26:52,240
thing into the development of the internet. Jack Dorsey now, Elon. Who else? I mean, there's people

305
00:26:52,800 --> 00:26:56,800
playing with the evolution of social media. And to me, that seems to be

306
00:26:57,520 --> 00:27:03,760
connected to the development of AI. And it seems like those singular figures will define the

307
00:27:03,760 --> 00:27:08,320
direction of AI development and social media development with social media seeming to have

308
00:27:08,320 --> 00:27:14,320
such a huge impact on our collective intelligence. It does feel in one way like

309
00:27:15,360 --> 00:27:19,760
individuals have an especially big impact right now in that a small number of people

310
00:27:19,760 --> 00:27:25,920
are pulling some big levers. And there can be a little meeting of three people at

311
00:27:26,640 --> 00:27:29,760
Facebook and they come out and they come in, they come out of that meeting and make a decision

312
00:27:29,760 --> 00:27:37,120
that totally changes the world. On the other hand, you see a lot of conformity. You see a

313
00:27:37,120 --> 00:27:43,520
lot of, they all pulled the plug on Trump the same day. So that suggests that there's some bigger

314
00:27:43,520 --> 00:27:48,480
force that is also kind of driving them, in which case it's less about the individuals.

315
00:27:49,040 --> 00:27:55,680
I think, what is leadership? I mean, to me, leadership is the ability to move things

316
00:27:55,760 --> 00:28:01,280
in a direction that the cultural forces are not already taking things. A lot of times people

317
00:28:01,280 --> 00:28:06,080
seem like a leader because they're just kind of hopping on the cultural wave and they happen

318
00:28:06,080 --> 00:28:10,240
to be the person who gets to the top of it. Now it seems like they're, but actually the wave was

319
00:28:10,240 --> 00:28:18,560
already going. Real leadership is when someone actually changes the wave, changes the shape

320
00:28:18,560 --> 00:28:27,920
of the wave. I think Elon with SpaceX and with Tesla, genuinely shaped a wave. Maybe you could

321
00:28:27,920 --> 00:28:33,120
say that EVs were actually, they were going to happen anyway, but there's not much evidence

322
00:28:33,120 --> 00:28:38,480
about at least happening when it did. If we end up on Mars, you can say that Elon was a

323
00:28:38,480 --> 00:28:43,360
genuine leader there. And so there are examples now. Zuckerberg definitely has done a lot of

324
00:28:43,360 --> 00:28:52,800
leadership along the way. He's also potentially caught in a storm that is happening and he's

325
00:28:52,800 --> 00:28:58,240
one of the figures in it. So I don't know. And it's possible that he is a big shaper if the

326
00:28:58,240 --> 00:29:03,680
metaverse becomes a reality. If in 30 years we're all living in a virtual world, to many people

327
00:29:03,680 --> 00:29:09,520
that seems ridiculous now, that that was a poor investment. Well, he talked about getting 10,

328
00:29:09,520 --> 00:29:14,320
I think it was something like a billion people with a VR headset in their pocket in by, I think,

329
00:29:14,320 --> 00:29:20,880
10 years from now back in 2015. So we're behind that. But when he was talking about that, and

330
00:29:20,880 --> 00:29:27,760
honestly, this is something I've been wrong about because I went to one of the Facebook conferences

331
00:29:27,760 --> 00:29:33,520
and tried out all the new Oculus stuff. And I was pretty early talking to some of the major

332
00:29:33,520 --> 00:29:37,040
players there because I was going to write a big post about it that then got swallowed by this book.

333
00:29:37,040 --> 00:29:42,480
But I would have been wrong in the post because what I would have said was that this thing is,

334
00:29:43,120 --> 00:29:48,640
when I tried it, I was like, some of them suck, some of them make you nauseous and they're just

335
00:29:48,640 --> 00:29:54,480
not that your headsets were big. But I was like, the times when this is good, it is,

336
00:29:54,480 --> 00:29:58,960
I have this feeling, it reminds me of the feeling I had when I first was five and I went to a

337
00:29:58,960 --> 00:30:03,520
friend's house and he had Nintendo. And he gave me the controller and I was looking at the screen

338
00:30:03,520 --> 00:30:10,480
and I pressed a button and Mario jumped. And I said, I can make something on the screen move.

339
00:30:10,480 --> 00:30:13,280
And the same feeling I had the first time someone showed me how to send an email,

340
00:30:13,280 --> 00:30:15,760
it was like really early and he's like, you can send this. And I was like, it goes,

341
00:30:15,760 --> 00:30:18,880
I can press enter on my computer and something happens on your computer.

342
00:30:18,880 --> 00:30:22,960
Those were obviously, when you have that feeling, it often means you're witnessing a

343
00:30:22,960 --> 00:30:27,920
paradigm shift. And I thought, this is one of those things. And I still kind of think it is,

344
00:30:27,920 --> 00:30:32,800
but it's kind of weird that it hasn't, where's the VR revolution?

345
00:30:32,800 --> 00:30:37,440
Yeah, I'm surprised because I'm with you. My first and still instinct is,

346
00:30:37,440 --> 00:30:41,520
this feels like it changes everything. VR feels like it changes everything,

347
00:30:41,520 --> 00:30:46,640
but it's not changing anything. A dumb part of my brain is genuinely convinced that this is real.

348
00:30:46,640 --> 00:30:49,120
And then the smart part knows it's not. But that's why the dumb part was like,

349
00:30:49,120 --> 00:30:53,200
we're not walking off that cliff. The smart part is like, you're on your rug. It's fine.

350
00:30:53,200 --> 00:30:57,040
The dumb part of my brain is like, I'm not walking off the cliff. So I was like, it's crazy.

351
00:30:57,040 --> 00:31:02,320
I feel like it's waiting for like that revolutionary person who comes in and says,

352
00:31:02,320 --> 00:31:06,480
I'm going to create a headset. Like honestly, it's Steve Jobs iPhone of honestly,

353
00:31:06,480 --> 00:31:10,240
a little bit of a comic type guy, which is why it was really interesting for him to be involved

354
00:31:10,240 --> 00:31:15,280
with Facebook is basically how do we create a simple dumb thing that's a hundred bucks,

355
00:31:15,280 --> 00:31:20,080
but actually creates that experience. And then there's going to be some viral killer app on it.

356
00:31:20,080 --> 00:31:24,000
And that's going to be the gateway into a thing that's going to change everything.

357
00:31:24,000 --> 00:31:27,840
I mean, I don't know what exactly was the thing that changed everything with the personal computer.

358
00:31:29,440 --> 00:31:36,320
Does that understood why that maybe graphics? What was the use case? I mean, exactly.

359
00:31:36,320 --> 00:31:43,520
It wasn't the 84 Macintosh like a moment when it was like, this is actually something that

360
00:31:43,520 --> 00:31:47,520
normal people can and want to use. Because it was less than $5,000, I think.

361
00:31:47,520 --> 00:31:52,560
And I just think it had some like Steve Jobs user friendliness already to it that other ones

362
00:31:52,640 --> 00:31:58,480
hadn't had. I think Windows 95 was a really big deal. Because I'm old enough to remember

363
00:31:58,480 --> 00:32:02,640
the MS-DOS when I was like kind of remembered the command. And then suddenly this concept of

364
00:32:02,640 --> 00:32:07,280
like a window you drag something into or you double click an icon, which now seems like so

365
00:32:07,280 --> 00:32:14,160
obvious to us was like revolutionary because it made it intuitive. So I don't know.

366
00:32:14,160 --> 00:32:19,520
Windows 95 was good. It was crazy. I forget what the big leaps was because Windows 2000

367
00:32:19,520 --> 00:32:25,920
was sucked. And then Windows XP was good. I moved to Mac around 2004. So I stopped.

368
00:32:25,920 --> 00:32:33,280
I sold your soul to the devil. I see. Well, us, the people still use Windows and Android.

369
00:32:34,320 --> 00:32:39,200
The device in the operating system of the people. Not you elitist folk with your books

370
00:32:40,160 --> 00:32:43,600
and your what else and success. Okay.

371
00:32:44,240 --> 00:32:52,320
You write more technology means better good times, but it also means bad or bad times.

372
00:32:52,320 --> 00:32:57,280
And the scary thing is if the good and bad keep exponentially growing, it doesn't matter how great

373
00:32:57,280 --> 00:33:02,640
the good times become. If the bad gets to a certain level of bad, it's all over for us.

374
00:33:02,640 --> 00:33:08,320
Can you elaborate on this? Why is there, why does the bad have that property?

375
00:33:09,280 --> 00:33:13,600
That if it's all exponentially getting more powerful, then the bad is going to win in the end.

376
00:33:14,160 --> 00:33:19,520
Is my misinterpreting that? No. So the first thing is I noticed a trend, which was like

377
00:33:21,120 --> 00:33:28,000
the good is getting better every century. Like the 20th century was the best century yet in terms

378
00:33:28,000 --> 00:33:33,280
of prosperity, in terms of GDP per capita, in terms of life expectancy, in terms of poverty

379
00:33:33,280 --> 00:33:38,480
and disease and every metric that matters. The 20th century was incredible. It also had the

380
00:33:38,480 --> 00:33:42,400
biggest wars in history, the biggest genocide in history, the biggest existential threat yet

381
00:33:42,400 --> 00:33:48,880
with nuclear weapons. The depression was probably as big and economic. So it's this

382
00:33:48,880 --> 00:33:54,080
interesting thing where the stakes are getting higher in both directions. And so the question is,

383
00:33:54,080 --> 00:34:00,720
like, if you get enough good, does that protect you against the bad? The dream, and I do think

384
00:34:00,720 --> 00:34:04,960
this is possible too, is the good gets so good. You know, have you ever read the culture series

385
00:34:04,960 --> 00:34:10,560
that Ian Banks books? Not yet, but I get criticized on a daily basis by some of the mutual folks we

386
00:34:10,560 --> 00:34:15,920
know for not having done so. And I feel like a lesser man for it, yes. So that's how I got

387
00:34:15,920 --> 00:34:20,960
onto it. And I read six of the 10 books, and they're great. But the thing I love about them is,

388
00:34:20,960 --> 00:34:28,160
like, it just paints one of these futuristic societies where the good has gotten so good

389
00:34:28,160 --> 00:34:33,680
that the bad is no longer even an issue. Like, basically, and the way that this works is the AI,

390
00:34:34,400 --> 00:34:40,080
you know, the AIs are benevolent and they control everything. And so, like, there's one

391
00:34:41,120 --> 00:34:45,440
random anecdote where they're like, you know, what happens if you murder someone in, because

392
00:34:45,440 --> 00:34:48,400
you're still, you know, there's still people with rage and jealousy or whatever. So someone

393
00:34:48,400 --> 00:34:52,320
murders someone. First of all, that person's backed up. So it's like, they have to get a new

394
00:34:52,320 --> 00:34:57,120
body and it's annoying, but it's like, it's not death. And secondly, that person, what are they

395
00:34:57,120 --> 00:34:59,920
going to do? Put them in jail? No, no, no. They're just going to send a slap drone around, which is

396
00:34:59,920 --> 00:35:04,240
this little, like, tiny, you know, random drone that just will float around next to them forever.

397
00:35:04,240 --> 00:35:06,880
And by the way, kind of be their servant. Like, it's kind of fun to have a slap drone, but just

398
00:35:06,880 --> 00:35:11,040
making sure that they never do anything. And it's like, I was like, oh, man, it could just be,

399
00:35:11,040 --> 00:35:15,360
everyone could be so safe. And everything could be so, like, you know, you want a house, you know,

400
00:35:15,360 --> 00:35:19,520
the AI's will build your house. There's endless space. There's endless resources. So I do think

401
00:35:19,520 --> 00:35:23,120
that that could be part of our future. That's part of what excites me is, like, there is,

402
00:35:23,120 --> 00:35:27,040
like, today would seem like a utopia to Thomas Jefferson, right? Thomas Jefferson is a world

403
00:35:27,040 --> 00:35:32,080
that would seem like a utopia to a caveman. There is a future. And by the way, these are

404
00:35:32,080 --> 00:35:36,400
happening faster, these jumps, right? So the thing that would seem like a utopia to us,

405
00:35:36,400 --> 00:35:41,040
we could experience in our own lifetimes, right? Like, it's especially a, you know,

406
00:35:41,040 --> 00:35:46,560
life extension you get combines with exponential progress. I want to get there. And I think in

407
00:35:46,560 --> 00:35:51,120
that part of what makes it utopia is you don't have to be scared of the worst bad guy in the

408
00:35:51,120 --> 00:35:56,960
world trying to do the worst damage because we have protection. But that said, I'm not sure

409
00:35:57,040 --> 00:36:03,280
how that happens. Like, it's either easier said than done. Nick Bostrom uses the example of

410
00:36:03,280 --> 00:36:10,160
if nuclear weapons could be manufactured by microwaving sand, for example, we probably would

411
00:36:10,160 --> 00:36:16,240
be in the Stone Age right now because 0.001% of people would love to destroy all of humanity,

412
00:36:16,240 --> 00:36:21,360
right? Some 16-year-old with huge mental health problems who right now goes and shoots up a school

413
00:36:21,360 --> 00:36:26,640
would say, oh, even better, I'm going to blow up a city. And now suddenly there's copycats, right?

414
00:36:26,640 --> 00:36:35,840
And so that's like, as our technology grows, it's going to be easier for the worst bad guys

415
00:36:36,800 --> 00:36:42,640
to do tremendous damage. And it's easier to destroy than to build. So it takes a tiny,

416
00:36:42,640 --> 00:36:48,560
tiny number of these people with enough power to do that. So that, to me, I'm like, the stakes

417
00:36:48,560 --> 00:36:52,960
are going up because what we have to lose is this incredible utopia. But also, like,

418
00:36:52,960 --> 00:36:57,680
dystopia is real. It happens. The Romans ended up in a dystopia they probably earlier thought

419
00:36:57,680 --> 00:37:03,360
that was never possible. Like, we should not get cocky. And so to me, that trend

420
00:37:04,400 --> 00:37:10,000
is the exponential tech is a double-edged sword. It's so exciting. I'm happy to be alive now

421
00:37:10,000 --> 00:37:16,480
overall because I'm an optimist and I find it exciting. But it's really scary. And the dumbest

422
00:37:16,480 --> 00:37:19,680
thing we can do is not be scared. The dumbest thing we can do is get cocky and think, well,

423
00:37:19,680 --> 00:37:23,920
my life is always the last couple generations, everything's been fine. Stop that.

424
00:37:24,880 --> 00:37:30,800
What's your gut? What percentage of trajectories take us towards the, as you put unimaginably

425
00:37:30,800 --> 00:37:36,720
good future versus unimaginably bad future? Is it like, as an optimist?

426
00:37:36,720 --> 00:37:41,040
It's really hard to know. I mean, all like, you know, one of the things we can do is look at history.

427
00:37:41,680 --> 00:37:47,760
And on one hand, there's a lot of stories, I actually listening to a great podcast right now

428
00:37:47,760 --> 00:37:52,160
called The Fall of Civilizations. And it's literally every episode is like, you know,

429
00:37:52,160 --> 00:37:56,720
a little like two hour deep dive into some civilization. Some are really famous like

430
00:37:56,720 --> 00:38:04,640
the Roman Empire. Some are more obscure, like the Norse and Greenland. But each one is so

431
00:38:04,640 --> 00:38:11,280
interesting. But what's, I mean, there's a lot of civilizations that had their peak. There's always

432
00:38:11,280 --> 00:38:16,880
the peak, right, when they're thriving and they're at their max size and they have their waterways

433
00:38:17,840 --> 00:38:24,160
it's civilized and it's representative and it's fair and whatever. Not always, but the peak is

434
00:38:24,160 --> 00:38:27,200
a great, you know, if I could go back in time, you know, it's not that you don't, you know,

435
00:38:27,200 --> 00:38:30,800
the farther you go back, the worse it gets. No, no, no, you want to go back to a civilization

436
00:38:30,800 --> 00:38:34,800
during, I would go into the Roman Empire in the year a hundred. Sounds great, right? You don't

437
00:38:34,800 --> 00:38:38,560
want to go to the Roman Empire in the year 400. We might be in the peak right now here, whatever

438
00:38:38,560 --> 00:38:44,640
this empire is. So honestly, I think about like the 80s, you know, the 70s, the 80s. Oh, here we go.

439
00:38:44,720 --> 00:38:48,320
The music. No, no, so much better. No, the 80s culture is so annoying.

440
00:38:48,320 --> 00:38:52,480
It's just like, when I read, when I listen to these things, I'm thinking, you know, the 80s,

441
00:38:52,480 --> 00:38:59,440
the 90s, America, the 90s was popular. People forget that now. Like Clinton was a superstar

442
00:38:59,440 --> 00:39:03,680
around the world. Michael Jordan was exported internationally, then basketball was everywhere

443
00:39:03,680 --> 00:39:09,040
suddenly. You had like music, the sports, whatever. It was a little probably like the 50s, you know,

444
00:39:09,040 --> 00:39:12,480
you're coming out of the world war and the depression before it, it was like this kind of

445
00:39:12,960 --> 00:39:16,960
everyone was in a good mood kind of time, you know, it's like a finish a big project and it's

446
00:39:16,960 --> 00:39:21,520
Saturday. It was like, I feel like the 50s was kind of like everyone was having, you know, the

447
00:39:22,400 --> 00:39:27,200
20s, I feel like everyone was in good mood randomly. Then the 30s, everyone was in a bad mood.

448
00:39:28,080 --> 00:39:32,240
But the 90s, I think we'll look back on it as a time when everyone was in a good mood. And it was

449
00:39:32,240 --> 00:39:36,240
like, you know, again, of course, at the time, it doesn't feel that way necessarily. But I look at

450
00:39:36,240 --> 00:39:41,600
that, I'm like, maybe that was kind of America's peak. And like, no, maybe not, but like, it hasn't

451
00:39:41,600 --> 00:39:46,160
been popular since really worldwide. It's gone in and out depending on the country, but like,

452
00:39:46,160 --> 00:39:52,080
it hasn't reached that level of like, America's awesome around the world. And the political,

453
00:39:52,080 --> 00:39:57,280
you know, situations gotten, you know, really ugly. And, you know, maybe it's social media,

454
00:39:57,280 --> 00:40:04,000
maybe who knows, but I wonder if it'll ever be as simple and positive as it was then. Like,

455
00:40:04,000 --> 00:40:09,760
maybe we are in the, you know, it feels a little like maybe we're in the beginning of the downfall

456
00:40:09,760 --> 00:40:13,680
or not because these things don't just, it's not a perfect smooth hill. It goes up and down,

457
00:40:13,680 --> 00:40:15,680
up and down. So maybe we're, there's another big upcoming.

458
00:40:15,680 --> 00:40:19,440
And it's unclear whether public opinion, which is kind of what you're talking to,

459
00:40:20,160 --> 00:40:25,280
is correlated strongly with influence. Because you could say that even though America's been

460
00:40:25,280 --> 00:40:31,520
on a decline in terms of public opinion, the exporting of technology that America has still,

461
00:40:32,160 --> 00:40:36,800
with all the talk of China, has still been leading the way in terms of AI, in terms of

462
00:40:36,800 --> 00:40:40,160
social media, in terms of just basically any software related product.

463
00:40:40,160 --> 00:40:41,280
Like chips.

464
00:40:41,280 --> 00:40:46,000
Yeah, chips. So hardware and software, I mean, America leads the way. You could argue that

465
00:40:46,000 --> 00:40:51,360
Google and Microsoft and Facebook are no longer American companies, they're international companies,

466
00:40:51,360 --> 00:40:57,040
but they really are still at the, you know, headquartered in Silicon Valley, broadly speaking.

467
00:40:57,040 --> 00:41:02,960
So in Tesla, of course, and just all of its, all of the technological innovation still seems to be

468
00:41:02,960 --> 00:41:10,960
happening in the United States. Although culturally and politically, this is not,

469
00:41:10,960 --> 00:41:17,600
it's not, it's not good. Well, maybe that could shift at any moment when all the technological

470
00:41:17,600 --> 00:41:23,200
development can actually be, create some positive impact in the world that could shift it,

471
00:41:23,200 --> 00:41:25,440
put the right leadership and so on, with the right messaging.

472
00:41:26,400 --> 00:41:32,960
Yeah, I think, I don't feel confident at all about whether, no, no, I don't mean that. I don't

473
00:41:32,960 --> 00:41:37,680
mean, I don't feel confident in my opinion that we may be on the downswing or that we may be on,

474
00:41:37,680 --> 00:41:43,280
because I truly don't know. It's like, I think the people, these are really big macro stories

475
00:41:43,280 --> 00:41:47,840
that are really hard to see when you're inside of them. It's like, it's like being on a beach

476
00:41:47,840 --> 00:41:52,080
and running around, you know, a few miles this way and trying to suss out the shape of the coastline.

477
00:41:52,720 --> 00:41:58,720
It's just really hard to see the big picture. You get caught up in the micro stories, the

478
00:41:58,720 --> 00:42:04,560
little tiny ups and downs that are part of some bigger trend. And also giant paradigm shifts

479
00:42:04,560 --> 00:42:07,920
happen quickly nowadays. The internet came out of nowhere and suddenly was like,

480
00:42:09,120 --> 00:42:12,160
changed everything. So there could be a change everything thing on the way. It seems like there's

481
00:42:12,160 --> 00:42:17,440
a few candidates for it. And like, but, I mean, it feels like the stakes are just high, higher

482
00:42:17,440 --> 00:42:23,840
than it even was for the Romans, higher than it was, were because that we were more powerful as

483
00:42:23,840 --> 00:42:29,040
a species. We have God like powers with technology that other civilizations at their peak didn't

484
00:42:29,040 --> 00:42:36,240
have. And so I wonder if those high stakes and powers will feel laughable to people that live

485
00:42:36,240 --> 00:42:42,880
humans, aliens, cyborgs, whatever lives quality years from now, that maybe, maybe are a little

486
00:42:43,440 --> 00:42:47,120
this feeling of political and technological turmoil is nothing.

487
00:42:47,120 --> 00:42:53,520
Well, that's the big question. So right now, you know, the 1890s was like a super politically

488
00:42:53,520 --> 00:43:00,160
contentious decade in the US. It was like immense tribalism and newspapers were all lying and

489
00:43:00,160 --> 00:43:04,320
telling, you know, there was a lot of what we would associate with today's media, the worst of it.

490
00:43:04,960 --> 00:43:08,960
And it was over gold or silver being this, I don't know, it's something that I don't understand.

491
00:43:08,960 --> 00:43:13,440
But the point is, it was a little bit of a blip, right? It happened, it felt, it must have felt

492
00:43:13,440 --> 00:43:17,520
like the end of days at the time. And then now we look, and most people don't even know about that.

493
00:43:18,480 --> 00:43:23,040
Versus, you know, again, the Roman Empire actually collapsed. And so the question is just like,

494
00:43:23,040 --> 00:43:28,800
is yeah, you know, will in 50 years, will this be like, or like McCarthyism? Oh, they had like,

495
00:43:28,800 --> 00:43:33,520
oh, that was like a crazy few years in America. And then it was fine. Or is this the beginning

496
00:43:33,520 --> 00:43:40,400
of something really big? And that's what? Well, I wonder if we can predict what the big thing is

497
00:43:40,400 --> 00:43:44,880
at the beginning. It feels like we're not, we're just here along for the ride. And at the local

498
00:43:44,880 --> 00:43:50,480
level, and at every level, I try to do our best. Well, how do we do our best? And what's the,

499
00:43:50,480 --> 00:43:55,520
that's the one thing I know for sure, is that we need to have our wits about us and do our best.

500
00:43:55,520 --> 00:44:00,400
And the way that we can do that, you know, we have to be as wise as possible, right,

501
00:44:00,400 --> 00:44:06,080
to proceed forward. And wisdom is an emergent property of discourse.

502
00:44:06,080 --> 00:44:09,200
So you're a proponent of wisdom versus stupidity? Because you can make an,

503
00:44:10,080 --> 00:44:16,240
I can steal man in the case for stupidity. Do it. I probably can't. But there's some,

504
00:44:16,960 --> 00:44:22,720
I think wisdom, and you talk about this, can come with a false confidence, arrogance. I mean,

505
00:44:22,720 --> 00:44:26,880
you talk about this in the book, that's too easy. That's not wisdom, then. If you're being arrogant,

506
00:44:26,880 --> 00:44:32,080
you're being unwise. Unwise. Yeah, I think wisdom is doing what people a hundred years from now with

507
00:44:32,080 --> 00:44:36,240
the hindsight that we don't have would do if they could come back in time and they knew everything.

508
00:44:36,240 --> 00:44:40,080
It's like, how do we figure out how to have hindsight when we actually are not? What if

509
00:44:40,080 --> 00:44:44,480
stupidity is the thing that people from a hundred years from now will see as wise?

510
00:44:45,440 --> 00:44:50,080
I mean. The idiot by the Sieski being naive and trusting everybody, maybe that's the one.

511
00:44:50,080 --> 00:44:55,440
Well, then you get lucky. Then maybe you get to a good future by stumbling upon it.

512
00:44:56,880 --> 00:45:03,440
But ideally, you can get there. I think a lot of the America, the great things about it are a

513
00:45:03,440 --> 00:45:12,640
product of the wisdom of previous Americans. The Constitution was a pretty wise system to set up.

514
00:45:12,640 --> 00:45:16,560
There's not much stupid stumbling around. Well, there is. I mean, with the Sieski's,

515
00:45:17,520 --> 00:45:25,440
the idiot, Prince Mishkin and brothers Karamazov, there's Alyosha Karamazov. You are on the side

516
00:45:26,400 --> 00:45:33,680
of love and almost like a naive trust in other human beings. That turns out to be,

517
00:45:33,680 --> 00:45:39,440
at least in my perspective, in the long term, for the success of the species is actually wisdom.

518
00:45:39,440 --> 00:45:42,880
It's a compass. We don't know. It's a compass when you're in the fog.

519
00:45:42,880 --> 00:45:46,960
In the fog. It's a compass. Love is a compass.

520
00:45:47,600 --> 00:45:50,080
Here's the thing. I think we should have a compass as nice, but you know what else is

521
00:45:50,080 --> 00:45:53,440
nice? It's a flashlight in the fog that can help. You can't see that far, but you can see,

522
00:45:53,440 --> 00:45:58,720
oh, you can see four feet ahead instead of one foot. That, to me, is discourse. That is open,

523
00:45:58,720 --> 00:46:07,360
vigorous discussion in a culture that fosters that is how the species, the American citizens as a

524
00:46:07,360 --> 00:46:12,960
unit, can be as wise as possible, can maybe see four feet ahead instead of one foot ahead.

525
00:46:12,960 --> 00:46:18,320
That said, Charles Bukowski said that love is a fog that fades with the first light of reality.

526
00:46:18,320 --> 00:46:22,880
So I don't know how that works out, but I feel like there's intermixing of metaphors that works.

527
00:46:23,680 --> 00:46:28,800
Okay. You also write that quote as the authors of the story of us, which is this thousand page

528
00:46:28,800 --> 00:46:36,400
book. We have no mentors, no editors, no one to make sure it all turns out okay. It's all in our

529
00:46:36,400 --> 00:46:43,120
hands. This scares me, but it also gives me hope. If we can all get just a little wiser together,

530
00:46:43,120 --> 00:46:49,120
it may be enough to nudge the story onto a trajectory that points towards an unimaginably

531
00:46:49,120 --> 00:46:55,120
good future. Do you think we can possibly define what a good future looks like?

532
00:46:56,080 --> 00:47:04,400
I mean, this is the problem that we ran into with communism of thinking of utopia,

533
00:47:05,840 --> 00:47:10,880
of having a deep confidence about what a utopian world looks like.

534
00:47:11,680 --> 00:47:14,880
Well, it's a deep confidence. That was a deep confidence about the instrumental

535
00:47:15,680 --> 00:47:22,400
way to get there. I think a lot of us can agree that if everyone had everything they needed and

536
00:47:22,400 --> 00:47:26,720
we didn't have disease or poverty and people could live as long as they wanted to and choose one to

537
00:47:26,720 --> 00:47:32,960
die, and there was no major existential threat because we had control. I think almost everyone

538
00:47:32,960 --> 00:47:40,320
can agree that would be great. That communism is a, they said, this is the way to get there,

539
00:47:40,320 --> 00:47:47,440
and that's a different question. So the unimaginably good future I'm picturing,

540
00:47:47,440 --> 00:47:50,880
I think a lot of people would picture, and I think most people would agree. No, not everyone.

541
00:47:50,880 --> 00:47:53,920
There's a lot of people out there who would say humans are the scourge on the earth and we should

542
00:47:53,920 --> 00:47:58,480
de-growth or something, but I think a lot of people would agree that, you know, just again,

543
00:47:58,480 --> 00:48:01,920
take Thomas Jefferson, bring him here. He would see it as utopia for obvious reasons,

544
00:48:01,920 --> 00:48:10,880
for the medicine, the food, the transportation, just how the quality of life and the safety and

545
00:48:10,880 --> 00:48:15,920
all of that. So extrapolate that forward for us. Now, we were Thomas Jefferson, you know, what's

546
00:48:15,920 --> 00:48:22,160
the equivalent? That's what I'm talking about, and the big question is, I don't try to say,

547
00:48:22,160 --> 00:48:26,640
here's the way to get there. Here's the actual specific way to get there. I try to say,

548
00:48:27,520 --> 00:48:31,520
how do we have a flashlight so that we can together figure it out? How do we give ourselves

549
00:48:31,520 --> 00:48:35,520
the best chance of figuring out the way to get there? And I think part of the problem with

550
00:48:35,520 --> 00:48:42,880
communists and people, ideologues, is that they're way too overconfident that they know the way to

551
00:48:42,880 --> 00:48:48,720
get there. And it becomes a religion to them, this solution. And then you can't update once you

552
00:48:48,720 --> 00:48:53,520
have a solution as a religion. And so... I felt a little violated when you said communists and

553
00:48:53,520 --> 00:49:01,120
stared deeply into myself. In this book, you've developed a framework for how to fix everything.

554
00:49:02,320 --> 00:49:06,320
It's called the latter. Can you explain it? Okay, it's not a framework for how to fix everything.

555
00:49:07,360 --> 00:49:10,160
Humor. I'll explain it to Tim Urban at some point. Okay.

556
00:49:10,160 --> 00:49:16,560
How this humor thing works. Yeah, I know. A framework of how to think about collaboration

557
00:49:16,560 --> 00:49:25,840
between humans such that we could fix things. I think it's a compass. It's like a ruler that we

558
00:49:25,840 --> 00:49:29,600
can, once we look at it together and see what it is, we can all say, oh, we want to go to that side

559
00:49:29,600 --> 00:49:35,520
of the ruler, not this side. And so it gives us a direction to go. So what are the parts of the

560
00:49:35,520 --> 00:49:41,440
latter? So I have these two characters. This orange guy, the primitive mind, is this is our

561
00:49:41,440 --> 00:49:49,200
software. That is the software that was in a 50,000 BC person's head that was specifically

562
00:49:49,200 --> 00:49:53,840
optimized to help that person survive in that world. And not even not just not really survive,

563
00:49:53,840 --> 00:50:02,640
but help them pass their genes on in that world. And civilization happened quickly and brains

564
00:50:02,640 --> 00:50:10,800
changed slowly. And so that unchanged dude is still running the show in our head. And I used

565
00:50:10,800 --> 00:50:17,920
the example of like Skittles. Like, why do we eat Skittles? It's trash. It's obviously bad for you.

566
00:50:18,480 --> 00:50:25,520
And it's because the primitive mind in the world that it was programmed for, there was no Skittles

567
00:50:25,520 --> 00:50:30,080
and it was just fruit. And, you know, and if there was a dense, chewy, sweet fruit like that,

568
00:50:30,080 --> 00:50:35,840
it meant you just found like a calorie goldmine energy, energy, take it, take it, eat as much as

569
00:50:35,840 --> 00:50:41,040
you can. Gorge on it. Hopefully you get a little fat. That would be the dream. And now we're so

570
00:50:41,040 --> 00:50:46,080
good with energy for a while. We don't have to stress about it anymore. So today, Mars, Inc.

571
00:50:47,280 --> 00:50:52,480
is clever and says, let's not sell things to people's higher minds, who's the other character.

572
00:50:52,480 --> 00:50:56,400
Let's sell the people's primitive minds. Primitive minds are dumb. And let's trick them into thinking

573
00:50:56,800 --> 00:51:01,360
this is this thing you should eat. And then they'll eat it. Now Mars, Inc. is a huge company.

574
00:51:01,360 --> 00:51:05,120
Actually, just to link real quick. You said primitive mind and higher minds. So those are

575
00:51:05,120 --> 00:51:09,840
the two things that make up this bigger mind that is the modern human being.

576
00:51:09,840 --> 00:51:13,760
Yeah. It's like, you know, it's not perfect. Obviously, there's a lot of crossover. There's

577
00:51:13,760 --> 00:51:17,440
people who will yell at me for saying there's two minds. And, you know, but to me, it's just

578
00:51:17,440 --> 00:51:23,040
still a useful framework where you have this software that has making decisions based on a

579
00:51:23,040 --> 00:51:27,200
world that you're not in anymore. And then you've got this other character, I call it the higher

580
00:51:27,200 --> 00:51:31,280
mind. And it's the part of you that knows that skills are not good and can override the instinct.

581
00:51:31,280 --> 00:51:34,400
And the reason you don't always eat skittles is because the higher mind says, no, no, no,

582
00:51:34,400 --> 00:51:38,960
we're not doing that, because that's bad. And I know that, right? Now you can apply that to a

583
00:51:38,960 --> 00:51:41,840
lot of things. The higher mind is the one that knows I shouldn't procrastinate. The primitive

584
00:51:41,840 --> 00:51:45,360
mind is the one that wants to conserve energy and not do anything icky and, you know, can't

585
00:51:45,360 --> 00:51:49,680
see the future. So he procrastinates that, you know, you can apply this. No, I in this book apply it

586
00:51:49,680 --> 00:51:56,320
to how we form our beliefs is one of the ways. And then eventually to politics and political

587
00:51:56,320 --> 00:52:02,000
movements. But like, if you think about, well, what's the equivalent of the skittles tug of war

588
00:52:02,000 --> 00:52:10,400
in your head for how do you form your beliefs? And it's that the primitive mind in the world

589
00:52:10,400 --> 00:52:21,200
that it was optimized for, it wanted to feel conviction about its beliefs. It wanted to

590
00:52:21,760 --> 00:52:28,480
be sure that it was wanted to feel conviction and it wanted to agree with the people around

591
00:52:28,480 --> 00:52:32,560
there didn't want to stand out. It wanted to to perfectly agree with the tribe about the tribe's

592
00:52:32,560 --> 00:52:36,480
sacred beliefs. Right. And so there's a big part of us that wants to do that that doesn't like

593
00:52:36,480 --> 00:52:40,080
changing our mind. It feels like it's part of our the primitive mind identifies with beliefs.

594
00:52:40,480 --> 00:52:45,200
Feels like it's a threat, a physical threat to you, to your primitive mind when you change your

595
00:52:45,200 --> 00:52:50,160
mind or when someone disagrees with you in a smart way. So there's that huge force in us,

596
00:52:50,160 --> 00:52:55,280
which is confirmation bias. That's where that comes from. It's this desire to keep believing

597
00:52:55,280 --> 00:53:00,480
what we believe and this desire to also fit in with our beliefs, to believe what the people around

598
00:53:00,480 --> 00:53:05,680
us believe. And that can be fun in some ways. We all like the same sports team and we're all

599
00:53:05,680 --> 00:53:09,600
super into it and we're all going to be biased about that call together. I mean, it's not always

600
00:53:09,600 --> 00:53:15,360
bad, but it's not a very smart way to be. And you're actually you're working kind of for those

601
00:53:15,360 --> 00:53:20,000
ideas. Those ideas are like your boss and you're working so hard to keep believing those. Those

602
00:53:20,000 --> 00:53:24,640
ideas are, you know, a really good paper comes in that you read that conflicts with those ideas

603
00:53:24,640 --> 00:53:31,360
and you will do all this work to say that paper is bullshit because you're a faithful employee

604
00:53:31,360 --> 00:53:35,840
of those ideas. Now the higher mind to me, the same party that can override the Skittles can

605
00:53:35,920 --> 00:53:40,000
override this and can search for something that makes a lot more sense, which is truth,

606
00:53:40,960 --> 00:53:44,800
because what rational being wouldn't want to know the truth, who wants to be delusional.

607
00:53:45,440 --> 00:53:51,520
And so there's this tug of war because the higher mind doesn't identify with ideas. Why would you?

608
00:53:51,520 --> 00:53:54,960
It's an experiment you're doing and it's a mental model. And if someone can come over and say,

609
00:53:54,960 --> 00:53:59,280
you're wrong, you'd say, where show me, show me. And if they point out something that is wrong,

610
00:53:59,280 --> 00:54:02,960
you say, oh, thanks. Oh, good. I just got a little smarter, right? You're not going to identify

611
00:54:02,960 --> 00:54:05,840
with the thinking. Oh, yeah, kick it. See if you can break it. If you can break it, it's not that

612
00:54:05,840 --> 00:54:10,400
good, right? So there's both of these in our heads. And there's this tug of war between them.

613
00:54:11,040 --> 00:54:14,400
And sometimes, you know, if you're telling me about something with AI, I'm probably going to

614
00:54:14,400 --> 00:54:18,160
think with my higher minds. I'm not identified with it. But if you go and you criticize the ideas

615
00:54:18,160 --> 00:54:22,000
in this book, or you criticize my religious beliefs, you criticize, I might have a harder time

616
00:54:22,000 --> 00:54:26,800
because the primitive mind says, no, no, no, those are our special ideas. And so, yeah, so that's,

617
00:54:26,800 --> 00:54:30,880
that's one way to use this ladder is like, it's a spectrum, you know, at the top, the higher

618
00:54:30,880 --> 00:54:34,400
mind is doing all the thinking. And then as you go down, it becomes more of a tug of war. And at

619
00:54:34,400 --> 00:54:39,760
the bottom, the primitive mind is in total control. And this is distinct as you show from the spectrum

620
00:54:39,760 --> 00:54:44,480
of ideas. So this is how you think versus what you think. And those are distinct, those are

621
00:54:44,480 --> 00:54:50,400
different dimensions. We need, we need a vertical axis. We have all these horizontal axes, left,

622
00:54:50,400 --> 00:54:53,680
right, center, or, you know, this opinion all the way to this opinion. But it's like,

623
00:54:54,400 --> 00:54:59,360
what's much more important than where you stand is how you got there, right? And how you think.

624
00:54:59,840 --> 00:55:05,120
So this helps if, if I can say this person's kind of on the left or on the right, but they're up

625
00:55:05,120 --> 00:55:09,440
high, I think, I think in other words, I think they got there using evidence and reason, and they

626
00:55:09,440 --> 00:55:13,120
were willing to change their mind. Now that means a lot to me, what they have to say. If I think

627
00:55:13,120 --> 00:55:16,800
they're just a tribal person, and I can predict all their beliefs from hearing one, because it's

628
00:55:16,800 --> 00:55:21,040
so obvious what political beliefs, that person's views are irrelevant to me, because they're not

629
00:55:21,040 --> 00:55:28,000
real, they didn't come from information, they came from a tribe's kind of, you know, sacred

630
00:55:28,000 --> 00:55:33,280
10 commandments. I really like the comic you have in here with the boxer. This is the best

631
00:55:33,280 --> 00:55:40,320
boxer in the world. Wow, cool. Who has he beaten? No one. He's never fought anyone. Then how do you

632
00:55:40,320 --> 00:55:44,960
know he's the best boxer in the world? I can just tell. I mean, this connects with me, and I think

633
00:55:44,960 --> 00:55:49,120
with a lot of people, just because in martial arts, this is especially kind of true. There's this,

634
00:55:49,120 --> 00:55:55,040
this whole legend about different martial artists, and that kind of, we construct, like,

635
00:55:55,040 --> 00:56:00,560
action figures, like, you know, thinking that Steven Seagal is the best fighter in the world,

636
00:56:00,560 --> 00:56:04,720
or Chuck Norris. But Chuck Norris is actually backed up. He's done really well in competition,

637
00:56:04,720 --> 00:56:10,480
but still, the ultimate test for particular martial arts is what we now know as mixed martial

638
00:56:10,480 --> 00:56:15,600
arts, UFC, and so on. That's the actual scientific testing ground. It's a meritocracy. Yeah,

639
00:56:15,600 --> 00:56:19,440
exactly. I mean, there's within certain rules, and you can criticize those rules like this doesn't

640
00:56:19,440 --> 00:56:23,520
actually represent the broader combat that you would think of when you're thinking about martial

641
00:56:23,520 --> 00:56:28,640
arts, but reality is you're actually testing things, and that's when you realize that Aikido and

642
00:56:28,640 --> 00:56:34,880
some of these kind of woo woo martial arts in their certain implementations don't work in the way

643
00:56:34,880 --> 00:56:40,160
you think they would in the context of fighting. I think this is one of the places where everyone

644
00:56:40,160 --> 00:56:44,160
can agree, which is why it's a really nice comic, because then you start to talk about,

645
00:56:46,240 --> 00:56:52,160
map this onto ideas that people take personally. It starts becoming a lot more difficult to

646
00:56:54,240 --> 00:56:59,600
basically highlight that we're thinking with not with our higher mind, but with our primitive mind.

647
00:57:00,320 --> 00:57:05,200
Yeah, I mean, if I'm thinking with my higher mind, and now here, you can use different things for an

648
00:57:05,200 --> 00:57:11,280
idea as a metaphor. So here, the metaphor is a boxer for one of your conclusions, one of your

649
00:57:11,280 --> 00:57:20,080
beliefs. And if all I care about is truth, in other words, that means all I care about is having

650
00:57:20,080 --> 00:57:26,880
a good boxer. I would say, go, yeah, try, see if this person's good. In other words, I would get

651
00:57:26,880 --> 00:57:31,840
into arguments, which is throwing my boxer out there to fight against other ones. And if I think

652
00:57:31,840 --> 00:57:36,880
my argument is good, by the way, I love boxing, right? If I think my guy is amazing, you know,

653
00:57:36,880 --> 00:57:42,080
Mike Tyson, I'm thinking, oh yeah, bring it on. Who wants to come see? I bet no one can beat my

654
00:57:42,080 --> 00:57:49,040
boxer. I love a good debate, right? In that case. Now, what would you think about my boxer if not

655
00:57:49,040 --> 00:57:53,280
only was I telling you he was great, but he's never boxed anyone. But then you said, okay, well,

656
00:57:53,280 --> 00:57:58,000
your idea came over to try to punch him. And I screamed and I said, what are you doing? That's

657
00:57:58,000 --> 00:58:02,800
violence. And you're an awful person. And I don't want to be friends with you anymore.

658
00:58:03,520 --> 00:58:07,680
You would think this boxer obviously sucks, or at least I think it sucks deep down,

659
00:58:08,320 --> 00:58:16,320
because why would I be so anti? Anyone, no boxing allowed. So I think if you're in,

660
00:58:16,960 --> 00:58:21,760
so this, I call this a ladder, right? If you're in low rung land, you know, whether it's a culture

661
00:58:21,760 --> 00:58:26,800
or whatever, a debate and argument when someone says, no, that's totally wrong. What you're saying

662
00:58:26,800 --> 00:58:32,160
about that. And here's why you're actually being totally biased. It sounds like a fight. People

663
00:58:32,160 --> 00:58:36,160
are going to say, Oh, wow, we got in like a fight. It was really awkward. Like, are we still friends

664
00:58:36,160 --> 00:58:40,400
with that person? Because that's not a culture of boxing. It's a culture where you don't touch

665
00:58:40,400 --> 00:58:49,520
each other's ideas. That's insensitive. Versus in a high rung culture, it's sport. I mean,

666
00:58:49,520 --> 00:58:54,560
like everyone in your podcast, whether you're agreeing or disagreeing, the tone is the same.

667
00:58:54,560 --> 00:58:58,320
It's not like, Oh, this got awkward. It's like, the tone is identical because you're just playing

668
00:58:58,320 --> 00:59:02,720
intellectually either way, because it's a good high rung space. At his best, at his best. But

669
00:59:02,720 --> 00:59:07,200
people do take stuff personally. And then that's actually one of the skills of conversation,

670
00:59:07,200 --> 00:59:12,240
just as a fan of podcasts is when you sense that people take a thing personally, you have to like,

671
00:59:13,120 --> 00:59:18,160
there's sort of methodologies and little paths you can take to like calm things down,

672
00:59:18,160 --> 00:59:22,320
like go around. Don't take it as a violation of like that. You're trying to suss out which

673
00:59:22,320 --> 00:59:27,440
of their ideas are sacred to them and which ones are, bring it on. And sometimes it's actually,

674
00:59:27,440 --> 00:59:31,040
I mean, that's the skill of it, I suppose, that sometimes it's the certain wardings

675
00:59:32,400 --> 00:59:36,960
in the way you challenge those ideas are important. You can challenge them indirectly

676
00:59:36,960 --> 00:59:44,160
and then together walk together in that way. Because what I've learned is people are used

677
00:59:45,040 --> 00:59:50,480
to their ideas being attacked in a certain way, in a certain tribal way. And if you just avoid

678
00:59:50,480 --> 00:59:54,800
those, like for example, if you have political discussions and just never mentioned left or

679
00:59:54,800 --> 01:00:02,080
right, or Republican and Democrat, none of that, just talk about different ideas and avoid certain

680
01:00:02,080 --> 01:00:06,880
kind of triggering words, you can actually talk about ideas versus falling into this

681
01:00:07,840 --> 01:00:12,480
path that's well established through battles that people have previously fought.

682
01:00:12,480 --> 01:00:15,680
When you say triggering, I mean, who's getting triggered? The primitive mind. So what you're

683
01:00:15,680 --> 01:00:20,240
trying to do, what you're saying in this language is how do you have conversations with other people's

684
01:00:20,240 --> 01:00:24,560
higher minds almost like whispering without waking up the primitive mind. The primitive

685
01:00:24,560 --> 01:00:28,640
mind is there sleeping, right? And as soon as you say something, the left primitive mind gets up

686
01:00:28,640 --> 01:00:33,040
and says, what? What are you saying about the left? And now, now that everything goes off the rails.

687
01:00:33,680 --> 01:00:37,600
What do you make of conspiracy theories under this framework of the latter?

688
01:00:37,600 --> 01:00:43,600
So here's the thing about conspiracy theories is that once in a while, they're true, right?

689
01:00:43,600 --> 01:00:47,040
Because sometimes there's a natural conspiracy. Actually, humans are pretty good at real

690
01:00:47,040 --> 01:00:52,720
conspiracies, secret things. And then, you know, I just watched the made off doc,

691
01:00:53,680 --> 01:01:03,200
great new Netflix doc, by the way. And so the question is, how do you create a system that is

692
01:01:03,200 --> 01:01:10,880
good at you put the conspiracy theory in? And it either goes, eh, or it says, it's interesting,

693
01:01:10,880 --> 01:01:13,440
let's keep exploring it. Like, how do you put, how do you do something that it can,

694
01:01:13,440 --> 01:01:18,960
how do you assess? And so again, I think the high rung culture is really good at it because

695
01:01:19,680 --> 01:01:24,240
a real conspiracy, what's going to happen is you put it, it's like a little machine you put

696
01:01:24,240 --> 01:01:27,600
in the middle of the table and everyone starts firing darts at it or bow and arrow or whatever

697
01:01:27,600 --> 01:01:32,400
and everyone starts kicking it and trying to, and almost all conspiracy theories, they quickly

698
01:01:32,400 --> 01:01:37,040
crumble, right? Because they actually, you know, you know, Trump's election one is, you know,

699
01:01:37,040 --> 01:01:41,280
I actually dug in and I looked at like every claim that he or his team made. And I was like,

700
01:01:42,160 --> 01:01:46,160
all of these, none of these hold up to scrutiny, none of them. I was open minded, but none of them

701
01:01:46,640 --> 01:01:50,080
did. So that was one that as soon as it's open to scrutiny, it crumbles.

702
01:01:51,120 --> 01:01:58,000
The only way that conspiracy can stick around in a community is if it is a culture where

703
01:01:58,000 --> 01:02:02,320
that's being treated as a sacred idea that no one should kick or throw a dart at because if

704
01:02:02,320 --> 01:02:07,840
you throw a dart, it's going to break. So it's being, it's, and so what you want is a culture

705
01:02:07,840 --> 01:02:12,800
where no idea is sacred. Anything can get thrown at. And so I think that then what you'll find is

706
01:02:12,800 --> 01:02:19,040
that 94 out of 100 conspiracy theories come in and they fall down. The other, maybe four of the

707
01:02:19,040 --> 01:02:22,480
others come in and there's something there, but it's not as extreme as people say. And then maybe

708
01:02:22,480 --> 01:02:27,440
one is a huge deal and it's actually a real conspiracy. Well, isn't there a lot of gray area

709
01:02:27,440 --> 01:02:34,400
and there's a lot of mystery? Isn't that where the conspiracy theories seep in? So it's great to

710
01:02:34,400 --> 01:02:40,960
hear that you've really looked into the Trump election fraud claims, but aren't they resting

711
01:02:40,960 --> 01:02:48,240
on a lot of kind of gray area like fog, basically saying that there is dark forces in the shadows

712
01:02:48,240 --> 01:02:52,400
that are actually controlling everything? I mean, the same thing with maybe you can, there's like

713
01:02:53,120 --> 01:02:58,080
safer conspiracy theories, more less controversial ones. Like, have we landed on the moon?

714
01:02:58,640 --> 01:03:05,120
Right? The United States ever landed on the moon. There's, you know, you can, like the reason those

715
01:03:05,120 --> 01:03:10,560
conspiracy theories work is you could construct, there's incentives and motivation for faking

716
01:03:10,560 --> 01:03:18,960
the moon landing. There's a lot of, there's very little data supporting the moon landing.

717
01:03:19,760 --> 01:03:22,800
Like that's very public and kind of looks fake, space kind of looks fake.

718
01:03:22,800 --> 01:03:25,760
That would be a big story if it turned out to be fake.

719
01:03:25,760 --> 01:03:30,800
That's the, that would be the argument against it. Like, are people really as a collective

720
01:03:30,800 --> 01:03:37,520
going to hold on to a story that big? Yeah. So that, but there's a lot, that the reason they

721
01:03:37,520 --> 01:03:41,600
work is there's mystery. Yeah. There's a great documentary called Behind the Curve about

722
01:03:41,600 --> 01:03:45,840
Flat Earthers. And one of the things that you learn about Flat Earthers is they believe

723
01:03:46,800 --> 01:03:50,400
all the conspiracies, not just the Flat Earth. They're, they're, they're convinced the moon

724
01:03:50,400 --> 01:03:56,080
landing is fake. They're convinced 9-11 was an American con job. They're convinced, you know,

725
01:03:56,800 --> 01:04:03,040
that name a conspiracy and they believe it. And so it's so interesting is that I think of it as a,

726
01:04:03,280 --> 01:04:10,480
as a skepticism spectrum. Yeah. So on one side, you, it's like a filter in your head,

727
01:04:11,280 --> 01:04:17,200
filter in your, in the beliefs section of your brain. On one end of the spectrum, you are gullible,

728
01:04:17,200 --> 01:04:19,840
perfectly gullible. You believe anything someone says, right? On the other side,

729
01:04:19,840 --> 01:04:23,360
you're paranoid. You think everyone's lying to you, right? Everything's, everything is false.

730
01:04:23,360 --> 01:04:26,720
Nothing that anyone says is true, right? So obviously those aren't good places to be.

731
01:04:27,600 --> 01:04:32,320
Now the healthy place, I think that the, so, so, so I think the healthy place is to be somewhere

732
01:04:32,320 --> 01:04:35,840
in the middle. And, but also you can learn to trust certain sources and then, you know,

733
01:04:35,840 --> 01:04:40,160
you don't have to do as much, apply as much skepticism to them. And so here's what,

734
01:04:41,680 --> 01:04:47,280
like, when you start having a bias, just say you have a political bias, when your side says

735
01:04:47,280 --> 01:04:50,880
something, you, you will find yourself moving towards the gullible side of the spectrum. You

736
01:04:50,880 --> 01:04:54,560
read an article written that supports your views. You move to the gullible side of the

737
01:04:54,560 --> 01:04:57,840
spectrum and you just believe it and you don't have any, where's that skepticism that you normally

738
01:04:57,840 --> 01:05:00,960
have, right? And then you move, and then you soon, soon as it's the other person talking,

739
01:05:00,960 --> 01:05:05,280
the other team talking, you move to the skeptical, the closer to the, to the, you know,

740
01:05:05,280 --> 01:05:11,920
in denial paranoid side. Now, flat earthers are the extreme. They are either 10 or one.

741
01:05:12,880 --> 01:05:15,760
So it's like, it's so interesting because they're the people who are saying, ah, no,

742
01:05:15,760 --> 01:05:20,000
I won't believe you. I'm not gullible. No, everyone else is gullible about the moon landing. I won't.

743
01:05:20,000 --> 01:05:23,920
And then yet, when there's this evidence like, oh, because you can't see Seattle,

744
01:05:23,920 --> 01:05:28,400
you can't see the buildings over that horizon and you should, which isn't true. You should be,

745
01:05:28,400 --> 01:05:32,720
if it were, if the earth around you wouldn't be able to see them, therefore it's, so suddenly

746
01:05:32,720 --> 01:05:35,920
they become the most gullible person to hear any theory about the earth flat. They believe it.

747
01:05:35,920 --> 01:05:39,600
It goes right into their beliefs. So they're actually jumping back and forth between

748
01:05:39,600 --> 01:05:43,920
refuses to believe anything and believe anything. And so they're the extreme example,

749
01:05:43,920 --> 01:05:47,360
but I think when it comes to conspiracy theories, the people that get themselves into trouble

750
01:05:48,000 --> 01:05:51,920
are the ones who they, they, they become really gullible when they hear a conspiracy theory that

751
01:05:51,920 --> 01:05:55,680
kind of fits with their worldview. And they likewise, when there's something that's kind

752
01:05:55,680 --> 01:06:01,520
of obviously true and it's not a big lie, they will actually, they'll think it is. They, they,

753
01:06:01,520 --> 01:06:06,720
they just tighten up their kind of skepticism filter. And so, yeah. So I think the healthy

754
01:06:06,720 --> 01:06:10,320
places to be is where you are not, because you also don't want to be the person who says every

755
01:06:10,320 --> 01:06:16,080
conspiracy, you hear the word conspiracy theory and it sounds like a synonym for like quack,

756
01:06:16,080 --> 01:06:21,120
job, crazy theory, right? So, yeah. So I think, yeah, I think it's, it'd be somewhere in the

757
01:06:21,120 --> 01:06:25,040
middle of that spectrum and to learn to fine tune it, which is a tricky place to operate.

758
01:06:25,680 --> 01:06:29,680
You kind of have to, every time you hear a new conspiracy theory, you should approach it with

759
01:06:29,680 --> 01:06:35,680
an open mind. And, you know, and also if you don't have enough time to investigate, which most

760
01:06:35,680 --> 01:06:41,280
people don't kind of still have a humility, not to make a conclusive statement that that's nonsense.

761
01:06:41,280 --> 01:06:48,480
There's a lot of social pressure actually, yeah, to immediately laugh off any conspiracy theory

762
01:06:48,480 --> 01:06:53,280
if it's done by the bad guys, right? You will quickly get mocked and laughed at and not taken

763
01:06:53,280 --> 01:06:57,680
seriously. If you give any credence, you know, back the lab leak was a good one where it's like

764
01:06:58,720 --> 01:07:06,320
turned out that that was at least very credible, if not true. And that was a perfect example of

765
01:07:06,320 --> 01:07:11,600
one where when it first came out, and not only so, so, so Brett Weinstein talked about it.

766
01:07:11,600 --> 01:07:16,560
And then I, in a totally different conversation, said something complimentary about him on a

767
01:07:16,560 --> 01:07:21,440
totally different subject. And people were saying, Tim, you might have gone a little

768
01:07:21,520 --> 01:07:26,000
off the deep end, you're like quoting someone who is like a lab leak person. So I was getting my

769
01:07:26,000 --> 01:07:32,640
reputation dinged for complimenting on a different topic, someone whose reputation was totally

770
01:07:32,640 --> 01:07:38,640
sullied, because they had, you know, they questioned an orthodoxy, right? So you see,

771
01:07:38,640 --> 01:07:43,040
so what does that make me want to do? Distance myself from Brett Weinstein, that's the, at least

772
01:07:43,040 --> 01:07:46,320
they see incentive that's, and what does that make other people want to do? Don't become the next

773
01:07:46,320 --> 01:07:50,080
Brett Weinstein, don't say it out loud, because you don't want to become someone that no one wants

774
01:07:50,080 --> 01:07:54,240
to compliment anymore, right? You can see the social pressure, and that's, and of course,

775
01:07:54,240 --> 01:08:01,360
when there is a conspiracy, that social pressure is its best friend. Because then they see the

776
01:08:01,360 --> 01:08:07,200
people from outside are seeing that social pressure enact, like Tim Urban becoming more

777
01:08:07,200 --> 01:08:10,640
and more and more extreme to the other side, and so they're going to take the more and more and more

778
01:08:10,640 --> 01:08:19,520
extreme. I mean, this, what do you see that the pandemic did, like COVID did to our civilization

779
01:08:19,520 --> 01:08:26,000
in that regard, in the forces? Why was it so divisive? Do you understand that?

780
01:08:26,880 --> 01:08:31,520
Yeah, so COVID, you know, I thought might be, you know, we always, you know, the ultimate example

781
01:08:31,520 --> 01:08:35,280
of a topic that will unite us all is the alien attack. Yeah. Although honestly, I don't even

782
01:08:35,280 --> 01:08:39,680
have that much faith then, I think there'd be like, some people are super like, you know,

783
01:08:39,680 --> 01:08:43,520
pro-alien and some people are anti-alien. But anyway. I was actually excited to interrupt,

784
01:08:43,520 --> 01:08:51,600
because I was talking to a few astronomers, and they're the first folks that maybe kind of said

785
01:08:52,400 --> 01:08:57,360
in that, if we could discover life on Mars, for example, that there's going to be a

786
01:08:57,360 --> 01:09:01,200
potentially a division over that too, where half the people will not believe that's real.

787
01:09:01,840 --> 01:09:12,160
Well, because we live in a current society where the political divide has subsumed everything,

788
01:09:12,160 --> 01:09:17,920
and that's not always like that. It goes into stages like that. We're in a really bad one,

789
01:09:17,920 --> 01:09:23,760
where it's actually in the book, I call it like a vortex, like almost like a whirlpool,

790
01:09:23,760 --> 01:09:29,440
that pulls everything into it. It pulls. And so normally you'd say, okay, you know,

791
01:09:29,440 --> 01:09:34,160
immigration naturally going to be contentious. That's always political, right? But like,

792
01:09:35,440 --> 01:09:39,520
COVID seemed like, oh, that's one of those that will unite us all. Let's fight this

793
01:09:40,480 --> 01:09:45,920
not human virus thing. Like, obvious is no one's sensitive. No one's like getting hurt when we

794
01:09:45,920 --> 01:09:49,760
insult the virus. Like, let's all be, we have this threat, this common threat that's a threat to

795
01:09:49,760 --> 01:09:56,720
everyone of every nationality in every country, every ethnicity, and it didn't do that at all.

796
01:09:56,720 --> 01:10:02,800
The whirlpool was too powerful. So it pulled COVID in and suddenly masks. If you're on the left,

797
01:10:02,800 --> 01:10:06,880
you like them. If you're on the right, you hate them. And suddenly lockdowns. If you're on the

798
01:10:06,880 --> 01:10:11,520
left, you like them. And on the right, you hate them. And vaccines, this is people forget this

799
01:10:11,520 --> 01:10:18,960
when Trump first started talking about the vaccine, Biden, Harris, Cuomo, they're all saying,

800
01:10:18,960 --> 01:10:23,440
I'm not taking that vaccine, not from this CDC. Because it was too rushed or something like that.

801
01:10:23,440 --> 01:10:27,840
But because I'm not trusting anything that Trump says. Trump wants me to take it. I'm not taking

802
01:10:27,840 --> 01:10:33,120
it. I'm not taking it from this CDC. So this was if Trump, this Trump was almost out of office.

803
01:10:33,120 --> 01:10:37,120
But at the time, if Trump had been, it would have been, I'm pretty sure it would have stayed.

804
01:10:37,120 --> 01:10:42,000
Right likes vaccines. The left doesn't like vaccines. Instead, the president switched.

805
01:10:42,000 --> 01:10:46,720
And all those people are suddenly saying, they were actually specifically saying that if you,

806
01:10:47,440 --> 01:10:51,760
that like, if you're saying the CDC is not trustworthy, that's misinformation, which is

807
01:10:51,760 --> 01:10:55,600
exactly what they were saying about the other CDC. And they were saying it because they genuinely

808
01:10:55,600 --> 01:11:01,040
didn't trust Trump, which is fair. But now when other people don't trust the Biden CDC,

809
01:11:01,120 --> 01:11:04,960
suddenly it's this kind of misinformation that needs to be censored. So it was a sad moment

810
01:11:04,960 --> 01:11:09,360
because it was a couple of months, even a week or so, a month or so at the very beginning when

811
01:11:10,480 --> 01:11:14,480
it felt like a lot of our other squabbles were kind of like, oh, I feel like they're kind of irrelevant

812
01:11:14,480 --> 01:11:20,320
right now. And then very quickly, the whirlpool sucked it in. And in a way where I think it

813
01:11:20,320 --> 01:11:24,640
damaged the reputation of these, a lot of the trust in a lot of these institutions for the long

814
01:11:24,640 --> 01:11:29,920
run. But there's also an individual psychological impact. It's like a vicious negative feedback

815
01:11:29,920 --> 01:11:34,880
cycle where they were deeply affected on an emotional level and people just were not their best

816
01:11:34,880 --> 01:11:41,280
selves. That's definitely true. Yeah. I mean, talk about the primitive mind. I mean, one thing

817
01:11:41,280 --> 01:11:46,960
that we've been dealing with for our whole human history is pathogens. And it's emotional, right?

818
01:11:46,960 --> 01:11:54,560
It brings out, you know, there's really interesting studies where like, if they study the

819
01:11:54,880 --> 01:12:01,840
phenomenon of disgust, which is one of these like, you know, smiling is universal. You don't

820
01:12:01,840 --> 01:12:07,200
have to ever translate a smile, right? Certain, you know, throwing your hands up when your

821
01:12:07,200 --> 01:12:11,920
sports team wins is universal because it's part of our coding. And so is disgust to kind of make

822
01:12:11,920 --> 01:12:15,600
this like, you know, face where you wrinkle up your nose and you kind of put out your tongue and

823
01:12:15,600 --> 01:12:21,040
maybe even gag. That's to expel, expel whatever it's because it's the reaction when something is

824
01:12:21,040 --> 01:12:27,040
potentially a pathogen that might harm us, right? Feces, vomit, whatever. But they did this

825
01:12:27,040 --> 01:12:35,200
interesting study where people who in two groups, the control group, you know, was shown images of

826
01:12:36,400 --> 01:12:38,800
and it might be getting two studies mixed up, but they were showing, they were showing images of

827
01:12:38,800 --> 01:12:42,400
like car crashes and like disturbing, but not disgusting. And the other one was shown like,

828
01:12:42,400 --> 01:12:46,400
you know, like, you know, rotting things and just things that were disgusting. And then they were

829
01:12:46,400 --> 01:12:51,200
asked about immigration. These were Canadians. And the group that was had the disgust feeling

830
01:12:51,200 --> 01:12:56,800
going through pulsing through their body was way more likely to prefer like immigrants from white

831
01:12:56,800 --> 01:13:03,040
countries. And the group that was had shown car accidents, they were, they still prefer the groups

832
01:13:03,040 --> 01:13:08,400
from white countries, but much less so. And so what does that mean? It's because the disgust impulse

833
01:13:09,040 --> 01:13:13,840
makes us scared of, you know, sexual practices that are foreign of ethnicities that are not

834
01:13:14,160 --> 01:13:18,720
that don't look like us. It's still xenophobia. So it's ugly. It's really ugly stuff. This is,

835
01:13:18,720 --> 01:13:26,080
of course, also how, you know, the Nazi propaganda with cockroaches and it was, Rwandan was cockroaches,

836
01:13:26,080 --> 01:13:31,760
you know, the Nazis was rats. And, you know, it's specifically, it's a dehumanizing emotion. So

837
01:13:32,640 --> 01:13:39,760
anyway, we were talking about COVID, but I think it does, it taps deep into like the human psyche

838
01:13:39,760 --> 01:13:43,040
and it's, I don't think it brings out our, I think, like you said, I think it brings out an

839
01:13:43,040 --> 01:13:50,640
ugly side in us. You describe an ideal lab as being opposite of echo chambers. So we know what

840
01:13:50,640 --> 01:13:56,400
echo chambers are. And you said like, just basically no good term for the opposite of an echo chamber.

841
01:13:56,400 --> 01:14:00,960
So what's an ideal lab? Yeah, well, first of all, both of these, we think of an echo chamber as

842
01:14:00,960 --> 01:14:05,520
like a group maybe or even a place, but it's, it's a culture. It's an intellectual culture.

843
01:14:05,520 --> 01:14:10,880
True. And this goes along with the high rung, so high rung and low rung thinking is individual,

844
01:14:10,880 --> 01:14:13,760
so I was talking about what's going on in your head, but this is very connected to

845
01:14:14,480 --> 01:14:22,080
the social scene around us. And so groups will do high rung and low rung thinking together.

846
01:14:23,680 --> 01:14:28,480
Basically, it's, so an echo chamber to me is a collaborative low rung thinking. It is,

847
01:14:28,480 --> 01:14:35,200
it's a culture where the cool, it's based around a sacred set of ideas. And it's the

848
01:14:35,200 --> 01:14:40,160
coolest thing you can do in an echo chamber culture is talk about how great the sacred ideas

849
01:14:40,160 --> 01:14:48,000
are and how bad and evil and stupid and wrong the people are who have the other views. And this,

850
01:14:48,000 --> 01:14:54,160
and, and, and it's, it's quite boring, you know, it's quite boring, you know, it's very hard to learn

851
01:14:54,160 --> 01:14:59,040
and changing your mind is not cool in an echo chamber culture. It makes you seem wishy-washy.

852
01:14:59,600 --> 01:15:05,840
It makes you seem like, you know, like you're waffling and you're flip-flopping or whatever.

853
01:15:06,480 --> 01:15:10,560
Showing conviction about the sacred ideas in echo chamber culture is awesome. If you're just like,

854
01:15:10,560 --> 01:15:14,320
you know, obviously this is what makes you seem smart while being, you know, humble,

855
01:15:14,320 --> 01:15:18,320
makes you seem dumb. So now flip all of those things on their heads and you have an, you have

856
01:15:18,320 --> 01:15:21,920
the opposite, which is idea lab culture, which is collaborative high rung thinking. It's

857
01:15:21,920 --> 01:15:26,400
collaborative truth finding, but it's also just, it's just a totally different vibe. It's,

858
01:15:27,280 --> 01:15:33,200
it's a place where arguing is a fun thing. It's not, no one's getting offended and, and

859
01:15:33,200 --> 01:15:37,760
criticizing like the thing everyone believes is actually, it makes you seem like interesting,

860
01:15:37,760 --> 01:15:42,880
like, oh, really? Like, why, why do you think we're all wrong? And expressing too much conviction

861
01:15:42,880 --> 01:15:46,160
makes people lose trust in you. It doesn't make you seem smart. It makes you seem stupid if you

862
01:15:46,160 --> 01:15:49,600
don't really know what you're talking about, but you're acting like you do. I really like this

863
01:15:49,600 --> 01:15:55,760
diagram of where on the x-axis agreement, the y-axis is decency. That's in an idea lab

864
01:15:55,760 --> 01:16:01,760
and echo chamber, there's only one axis. It's asshole to non-asshole. Right. This is a really

865
01:16:01,760 --> 01:16:07,680
important thing to understand about the difference between, you call it decency here, about

866
01:16:07,680 --> 01:16:13,280
asshole-ishness and disagreement. So my college friends, we love to argue, right? And, and no

867
01:16:13,280 --> 01:16:17,360
one thought anyone was an asshole for, it was just for sport. Sometimes we'd realize we're not

868
01:16:17,360 --> 01:16:20,880
even disagreeing on something and that would be disappointing and be like, oh, I think we agree.

869
01:16:20,880 --> 01:16:25,520
And it was kind of like sad. It was like, oh, well, there goes the fun. And one of the,

870
01:16:26,000 --> 01:16:31,920
members of this group has this, she brought her new boyfriend to one of our like hangouts.

871
01:16:32,480 --> 01:16:37,200
And there was like a heated, heated debate, you know, just, just, just one of our typical

872
01:16:37,200 --> 01:16:41,280
things. And afterwards, you know, the next day he said like, is everything okay? And she was like,

873
01:16:41,280 --> 01:16:44,640
what do you mean? And he said like after, you know, the fight and she was like, what fight?

874
01:16:44,640 --> 01:16:47,440
And he was like, you know, the fight last night and she was like, and she had to,

875
01:16:47,440 --> 01:16:50,960
and then she was like, you mean like the arguing and she, and he was like, yeah. And she,

876
01:16:50,960 --> 01:16:54,960
and so that's someone who is not used to idea lab culture coming into it

877
01:16:55,040 --> 01:16:59,600
and seeing it is like, that was like, this is like, are they still friends, right? And idea

878
01:16:59,600 --> 01:17:04,480
lab is nice for the people in them because you're, it, individuals thrive. You don't want to just

879
01:17:04,480 --> 01:17:07,920
conform. That does, it makes you seem boring in an idea, but you want to be yourself. You want to

880
01:17:07,920 --> 01:17:12,080
challenge things. You want to have a unique brain. So that's great. And, and, and you also have people

881
01:17:12,080 --> 01:17:16,080
criticizing your ideas, which makes you smarter. It doesn't always feel good, but you, you become

882
01:17:16,080 --> 01:17:20,960
more correct and smarter. And echo chamber is, is the opposite where it's not good for the people

883
01:17:20,960 --> 01:17:27,360
in it. It doesn't, you're learning skills atrophy. And, and I think it's boring, but the thing is

884
01:17:27,360 --> 01:17:33,360
they also have emergent properties. So the emergent property of an idea lab is like super

885
01:17:33,360 --> 01:17:39,520
intelligence, just you and me alone, just the two of us, if we're working together on something,

886
01:17:39,520 --> 01:17:43,600
but we're being really grown up about it, we're, we're disagreeing, we're not, you know,

887
01:17:43,600 --> 01:17:47,440
no one's set sensitive about anything. We're going to each find flaws in the other one's

888
01:17:47,440 --> 01:17:51,760
arguments that, that you wouldn't have found on your own. And we're going to have epiphany,

889
01:17:51,760 --> 01:17:56,240
double the epiphanies, right? So it's almost like the two of us together is like as smart as 1.5,

890
01:17:56,240 --> 01:18:00,960
is like 50% smarter than either of us alone, right? So you have this 1.5 intelligent kind of

891
01:18:01,600 --> 01:18:05,440
joint being now we've made. Now, bringing a third person in, fourth person in, right? You see,

892
01:18:05,440 --> 01:18:11,840
it starts to scale up. This is why science institutions can discover the relativity and

893
01:18:11,840 --> 01:18:15,200
quantum mechanics and these things that no individual human, you know, was going to come

894
01:18:15,200 --> 01:18:20,240
up with without a ton of collaboration because it's this giant idea lab. So it has an emergent

895
01:18:20,240 --> 01:18:26,400
property of super intelligence and echo chamber is the opposite where it has the emergent property of

896
01:18:28,400 --> 01:18:32,480
stupidity. I mean, has the emergent property of a bunch of people all, you know, you know,

897
01:18:32,480 --> 01:18:39,120
paying field, you know, fealty to this set of sacred ideas. And, and so you lose this magical

898
01:18:39,120 --> 01:18:43,440
thing about language and humans, which is collaborative intelligence, you lose it,

899
01:18:43,440 --> 01:18:49,840
it disappears. But there is that access of decency, which is really interesting because

900
01:18:49,840 --> 01:18:55,360
you kind of painted this picture of you and your friends arguing really harshly. But underlying

901
01:18:55,360 --> 01:19:04,640
that is a basic camaraderie, respect. There's, there's all kinds of mechanisms we humans have

902
01:19:04,640 --> 01:19:10,080
constructed to communicate like mutual respect or maybe communicate that you're here for the idea

903
01:19:10,080 --> 01:19:15,600
lab version of this. Totally. You don't take it, you don't get personal, right? You're not getting

904
01:19:15,600 --> 01:19:22,560
personal. You're not, you're not taking things personally. People are respected in an idea lab

905
01:19:22,560 --> 01:19:28,320
and ideas are disrespected. And there's a way ways to signal that. So like for with friends,

906
01:19:28,320 --> 01:19:32,000
you've already done the signaling, you've already established a relationship. The interesting thing

907
01:19:32,000 --> 01:19:38,880
is online. I think you have to do some of that work. To me, the sort of steel manning the other

908
01:19:38,880 --> 01:19:44,800
side or no, uh, having empathy and hearing out, being able to basically repeat the argument the

909
01:19:44,800 --> 01:19:49,520
other person is making before you and show them like respect to that argument. I could see how

910
01:19:49,520 --> 01:19:55,040
you could think that before you make a counter argument. There's just a bunch of ways to communicate

911
01:19:55,040 --> 01:20:03,920
that you're here not to, uh, do kind of what is it? Low rung, you know, shit talking, mockery,

912
01:20:03,920 --> 01:20:08,960
derision, but are actually here ultimately to discover the truth and the space of ideas and

913
01:20:08,960 --> 01:20:16,400
the tension of those ideas. And I think it's, uh, I think that's a skill that we're all learning as

914
01:20:16,400 --> 01:20:21,360
a civilization of how to do that kind of communication effectively. Cause I think disagreement

915
01:20:21,360 --> 01:20:26,720
is I'm learning on the internet. It's actually a really tricky skill, like high effort, high

916
01:20:26,720 --> 01:20:32,080
decency disagreement. Like I listened to, uh, there's a really good debate podcast, uh,

917
01:20:32,800 --> 01:20:37,200
Intelligent Squared and like they, they can go pretty hard in the paint.

918
01:20:37,200 --> 01:20:43,040
The classic idea lab. It's exactly. But like, how do we map that to the social media? When people

919
01:20:43,040 --> 01:20:49,920
like, we'll say, um, we'll say, well, like Lex or anybody, you're not, you hate disagreement. You,

920
01:20:49,920 --> 01:20:55,360
you want a sense of disagreement. No, um, I love Intelligent Squared type of disagreement. That's

921
01:20:55,360 --> 01:21:01,520
fun. You want to reduce asshole. And for me personally, I don't want to reduce asshole if,

922
01:21:01,520 --> 01:21:06,000
you know, I kind of like asshole or it's like fun in many ways. But the problem is when the

923
01:21:06,000 --> 01:21:12,240
asshole shows up to the party, they make it less fun for the, for the party that's there for the

924
01:21:12,240 --> 01:21:17,120
idea lab and the other people, especially the quiet voices at the back of the room, they leave.

925
01:21:17,120 --> 01:21:22,320
And so all you're left is, was, was with assholes. Well, that Twitter, political Twitter to me is

926
01:21:22,320 --> 01:21:29,840
one of those parties. It's a big party where a few assholes have really sent a lot of the quiet

927
01:21:30,720 --> 01:21:36,720
thinkers away. Yeah. Um, and, and so, so if you think about this graph again,

928
01:21:37,840 --> 01:21:45,520
what, what, what some place like Twitter, um, a great way to get followers is to be

929
01:21:45,520 --> 01:21:50,080
an asshole with a certain, you know, pumping a certain ideology. You'll get a huge amount of

930
01:21:50,080 --> 01:21:53,440
followers. And for those followers and the followers you're going to get, the people who

931
01:21:53,440 --> 01:21:59,360
would let, you know, not the people who like you are probably going to be people who are really

932
01:21:59,360 --> 01:22:03,520
thinking with their primitive mind because they're seeing your, you're being, you're being an asshole,

933
01:22:03,520 --> 01:22:07,440
but because you agree with them, they love you and they think they don't see any problem with

934
01:22:07,440 --> 01:22:10,800
how you're being. Yeah, they don't see the asshole. This is the fascinating thing. Because look,

935
01:22:10,800 --> 01:22:15,360
because look at the thing on the right, agreement and decency are the same. So if you're in that

936
01:22:15,360 --> 01:22:19,440
mindset, bigger the asshole, the better. If you're agreeing with me, you're my man. I love what you're

937
01:22:19,440 --> 01:22:24,480
saying. Yes, show them. Right. And the algorithm helps those people does, those people do great

938
01:22:24,480 --> 01:22:30,320
on the algorithm. There's a fascinating dynamic that happens because I have, I've currently hired

939
01:22:30,320 --> 01:22:35,440
somebody that looks at my social media and they block people because the assholes will roll in,

940
01:22:35,440 --> 01:22:40,960
they're not actually there to have an interesting disagreement, which I love. They're there to do

941
01:22:40,960 --> 01:22:47,760
kind of mockery. And then when they get blocked, they then celebrate that to their echo chamber,

942
01:22:47,760 --> 01:22:53,600
like, look at this, I got them or whatever they or they'll say some annoying thing like, oh, so it's,

943
01:22:53,600 --> 01:22:57,600
so he talks about, he likes, you know, if I've done this, they'll say, oh, he says he likes

944
01:22:57,600 --> 01:23:02,560
idea labs, but he actually wants to create an echo chamber. But I'm like, nope, you're an asshole.

945
01:23:03,120 --> 01:23:08,480
I'm not, look at the other 50 people on this thread that disagreed with me respectfully. They're

946
01:23:08,480 --> 01:23:12,480
not blocked. Yep, exactly. You know, and so they see it as some kind of hypocrisy, because again,

947
01:23:12,480 --> 01:23:17,440
they only see the thing on the right. And they're not understanding that there's two axes or that

948
01:23:17,440 --> 01:23:21,760
I see it as two axes. And so you seem petty in that moment, but it's like, no, no, no,

949
01:23:21,920 --> 01:23:24,800
this is very specific what I'm doing. You're actually killing the conversation.

950
01:23:26,080 --> 01:23:32,240
In generally, I give all those folks a pass and just send them love telepathically. But yes,

951
01:23:33,520 --> 01:23:38,240
getting rid of assholes in the conversation is the way you allow for the disagreement.

952
01:23:38,240 --> 01:23:44,240
You do a lot of like, when I think when primitive mindedness comes at you, at least on Twitter,

953
01:23:44,240 --> 01:23:47,600
I don't know what you're feeling internally in that moment, but you do a lot of like,

954
01:23:48,560 --> 01:23:53,840
I'm going to meet that with my higher mind. And you come out and you'll be like, thanks for all

955
01:23:53,840 --> 01:24:03,200
the criticism. I love you. And that's actually an amazing response, because it just, what it does

956
01:24:04,000 --> 01:24:09,920
is it, it unriles up that person's primitive mind and actually wakes up their higher mind who says,

957
01:24:09,920 --> 01:24:14,560
oh, okay, you know, this guy's not so bad. And suddenly like, civility comes back. So it's a

958
01:24:14,640 --> 01:24:19,760
very powerful, hopefully long term. But the thing is, they do seem to drive away

959
01:24:20,960 --> 01:24:27,440
high quality disagreement, because like, because it takes so much effort to disagree in a high

960
01:24:27,440 --> 01:24:32,320
quality way. I've noticed this on my blog, like my, one of the things I pride myself on is like,

961
01:24:32,320 --> 01:24:37,360
my comment section is awesome. Like there's, there's, there's every, everyone's being respectful.

962
01:24:38,720 --> 01:24:42,800
No one's afraid to disagree with me and tell them and say, say, you know, tear my post apart,

963
01:24:42,800 --> 01:24:45,680
but in a totally respectful way where the underlying thing is like, I'm here because

964
01:24:45,680 --> 01:24:50,000
I like this guy and his writing. And people disagree with each other and they get in these

965
01:24:50,000 --> 01:24:53,520
long intro and it's interesting and I read it and I'm learning. And then I, you know,

966
01:24:53,520 --> 01:24:56,880
I have a couple posts, especially the ones I've written about politics. It's not like

967
01:24:57,520 --> 01:25:00,880
it seems like any other comment section, people are being nasty to me, they're being nasty to

968
01:25:00,880 --> 01:25:06,320
each other. And then I looked down one of them and I realized like, almost all of this is the

969
01:25:06,320 --> 01:25:10,640
work of like three people. That's who you need to block. Those people need to be blocked. You're

970
01:25:10,640 --> 01:25:16,880
not being thin-skinned. You're not being petty doing it. You're actually protecting an idea lab

971
01:25:16,880 --> 01:25:20,720
because what would really aggressive people like that do is they'll turn it into their own echo

972
01:25:20,720 --> 01:25:25,040
chamber because now everyone is scared to kind of disagree with them. It's unpleasant. And so

973
01:25:25,040 --> 01:25:28,720
people who will chime in or the people who agree with them and suddenly like they've taken over

974
01:25:28,720 --> 01:25:33,600
the space. And I kind of believe that those people on a different day could actually do high

975
01:25:33,600 --> 01:25:38,160
effort disagreement. It's just that they're in a, in a, in a certain kind of mood. And a lot of us,

976
01:25:38,160 --> 01:25:43,680
just like you said, with the primitive mind could get into that mood. And it's, I believe it's

977
01:25:43,680 --> 01:25:49,600
actually the job of the technology, the platform to incentivize those folks to be like, are you

978
01:25:49,600 --> 01:25:55,360
sure this is the best you can do? Like if you really want to talk shit about this idea, like do

979
01:25:55,360 --> 01:26:02,480
better. Like, and then we need to create incentives where you get likes for high effort disagreement.

980
01:26:02,480 --> 01:26:08,560
Because currently you get likes for like a something that's slightly funny and is a little bit

981
01:26:08,560 --> 01:26:16,400
like mockery. Like, yeah, basically signals to some kind of echo chamber that this person is

982
01:26:16,960 --> 01:26:22,320
a horrible person is a hypocrite is evil, whatever, that feels like it's solvable with

983
01:26:22,320 --> 01:26:25,520
technology. Because I think in our private lives, none of us want that.

984
01:26:26,080 --> 01:26:30,960
I wonder if it's making me think that I want to like, because a much easier way for me to do it

985
01:26:30,960 --> 01:26:37,360
just for my, my world would be to say something like, you know, here's this axis, this high, this

986
01:26:37,360 --> 01:26:41,280
is, this is part of what I, part of what I like about the latter is it's a language that we can

987
01:26:41,280 --> 01:26:47,200
use. It's like specifically what we're talking about is high rung disagreement, good, low rung

988
01:26:47,200 --> 01:26:50,960
disagreement, bad, right. And so, so it gives us like a language for that. And so what I would say

989
01:26:50,960 --> 01:26:55,200
is I would, you know, my, you know, I would have my, you know, readers, you know, understand this

990
01:26:55,200 --> 01:27:02,560
axis. And then I would specifically say something like, please do the, do the, do it, but why a

991
01:27:02,560 --> 01:27:07,440
favor and upvote regardless of what they're saying horizontally, right? Regardless of what their

992
01:27:07,440 --> 01:27:13,440
actual view is, upvote high rungness, they can be tearing me apart, they can be saying great,

993
01:27:13,440 --> 01:27:18,720
they can be praising me, whatever, upvote high rungness and downvote low rungness.

994
01:27:18,720 --> 01:27:22,160
And if enough people are doing that, suddenly there's all this incentive to try to say, no,

995
01:27:22,160 --> 01:27:26,560
I need to calm my emotion down here and not be yet personal because I'm going to get voted into

996
01:27:26,560 --> 01:27:32,960
oblivion by these people. I think a lot of people would be very good at that. They, they, and they

997
01:27:32,960 --> 01:27:38,320
not are only, would they be good at that? They would want that, that task of saying, I know I

998
01:27:38,320 --> 01:27:44,480
completely disagree with this person, but this was a high effort, high rung disagreement.

999
01:27:44,480 --> 01:27:47,440
It gets everyone thinking about that other axis too. You're not just looking at where do you

1000
01:27:47,440 --> 01:27:50,320
stand horizontally. You're saying, well, how did you get there? And how are you, you know,

1001
01:27:51,280 --> 01:27:55,840
are you treating ideas like machines or are you treating them like little, you know, babies?

1002
01:27:55,840 --> 01:28:00,800
And then there should be some kind of labeling on personal attacks versus idea disagreement.

1003
01:28:00,800 --> 01:28:03,440
Sometimes people like throwing both a little bit. Right.

1004
01:28:03,440 --> 01:28:07,520
That's like, all right. No, like there should be a disincentive at personal attacks versus idea

1005
01:28:07,520 --> 01:28:13,520
attacks. Well, you can also, one metric is a respectful disagreement. If I see, just to say

1006
01:28:13,520 --> 01:28:18,160
someone else's Twitter and I see, you know, you put out a thought and I see someone say, you know,

1007
01:28:19,120 --> 01:28:24,080
someone say, you know, I don't see it that way. Here's where I think you went wrong and they're

1008
01:28:24,080 --> 01:28:29,520
just explaining. I'm thinking that if Lex reads that, he's going to be interested. He's going to

1009
01:28:29,520 --> 01:28:32,800
want to post more stuff, right? Because he's going to like that. If I see someone being like,

1010
01:28:35,120 --> 01:28:39,040
wow, this really shows the kind of person that you become or shows them. I'm thinking that person

1011
01:28:39,040 --> 01:28:43,200
is making Lex want to be on Twitter less. It's making him, and so what's that doing? What that

1012
01:28:43,200 --> 01:28:47,040
person's actually doing is they're putting, is they're actually, they're chilling discussion

1013
01:28:47,040 --> 01:28:50,640
because they're making it unpleasant to, they're making it scary to say what you think.

1014
01:28:50,640 --> 01:28:54,080
And the first person isn't at all. The first person is making you want to say more stuff.

1015
01:28:54,080 --> 01:28:57,120
So, and those are both disagreed. Those are people who are both disagree with you.

1016
01:28:57,120 --> 01:29:06,160
Exactly. Exactly. I want to, great disagreements with friends in meat space is like you're,

1017
01:29:06,160 --> 01:29:12,000
they disagree with you. They could be even yelling at you. Honestly, they could even have

1018
01:29:12,000 --> 01:29:15,360
some shit talk where it's like personal attacks. It still feels good.

1019
01:29:15,360 --> 01:29:18,960
Because you know them well and you know that that shit talk, because yeah,

1020
01:29:18,960 --> 01:29:22,880
friends shit talk all the time playing a, playing a sport or a game. And again, it's,

1021
01:29:22,880 --> 01:29:27,600
it's, it's because they know each other well enough to know that this is fun. We're having fun.

1022
01:29:27,600 --> 01:29:31,920
And obviously I love you. Like, exactly. You know, and, and that's, that's important online.

1023
01:29:31,920 --> 01:29:37,360
It's a lot harder. Yeah. That obviously I love you that underlies a lot of human interaction.

1024
01:29:37,360 --> 01:29:42,480
Right. Seems to be easily lost online. I've seen some people on Twitter and elsewhere just behave

1025
01:29:42,480 --> 01:29:47,280
they're worst. Yeah. And it's like, I know that's not who you are. Totally. Like, why are you?

1026
01:29:47,280 --> 01:29:52,480
I actually, you know, I know who is this human. I know someone personally who is one of the best

1027
01:29:52,480 --> 01:29:59,360
people. Yeah. I'm just, I love this guy. Like, one of the best, like fun, funny, like nicest dudes.

1028
01:30:00,640 --> 01:30:05,200
And he, if you would, if you looked at this Twitter only, you would think he's a culture

1029
01:30:05,200 --> 01:30:12,400
worrier, an awful culture worrier. And, you know, you know, biased and just stoking anger.

1030
01:30:13,360 --> 01:30:16,800
And, and, and it comes out of a good place. And I'm not going to give any other info about,

1031
01:30:16,800 --> 01:30:19,840
you know, specific, but like, I think you're describing a lot of people. It comes out of

1032
01:30:19,840 --> 01:30:23,120
a good place because he really cares about what he, you know, it comes out, but it's just,

1033
01:30:23,120 --> 01:30:27,040
I can't square the two. It's so, and that's it. You have to, once you know someone like that,

1034
01:30:27,040 --> 01:30:30,640
you can realize, okay, apply that to everyone. Cause a lot of these people are lovely people.

1035
01:30:31,200 --> 01:30:34,880
And it's just bring, even just, you know, back in the before social media, did you ever had a

1036
01:30:34,880 --> 01:30:40,880
friend who like was just like, they had this like dickishness on text or email that they

1037
01:30:40,880 --> 01:30:44,880
didn't have in person. You're like, wow, like email you is like kind of a dick. And it's like,

1038
01:30:44,880 --> 01:30:47,600
it just, certain people have a different persona behind the screen.

1039
01:30:48,240 --> 01:30:53,040
It has for me personally, it's become a bit of a meme that Lex blocks with love.

1040
01:30:53,760 --> 01:30:58,800
But there is a degree to that where this is, I don't see people on social media as representing

1041
01:30:58,800 --> 01:31:03,040
who they really are. I really do have love for them. I really do think positive thoughts of them

1042
01:31:03,040 --> 01:31:08,080
throughout the entirety of the experience. I see this as some weird side effect of

1043
01:31:08,080 --> 01:31:14,560
online communication. And so it's like, to me, blocking is not some kind of a

1044
01:31:14,560 --> 01:31:18,080
derisive act towards that individual. It's just like saying, well, a lot of times what's

1045
01:31:18,080 --> 01:31:25,920
happened is they have slipped into a very common delusion that dehumanizes others.

1046
01:31:25,920 --> 01:31:30,160
So that doesn't mean they're a bad person. We all can do it. But they're dehumanizing you

1047
01:31:30,160 --> 01:31:34,560
or whoever they're being nasty to, because in a way they would never do in person,

1048
01:31:34,560 --> 01:31:38,320
because in a person, they're reminded that's a person. Remember, I said the dumb part of

1049
01:31:38,320 --> 01:31:42,320
my brain when I'm doing VR, like won't step off the cliff, but the smart part of my brain knows

1050
01:31:42,320 --> 01:31:48,080
I'm just on the rug. That dumb part of our brain is really dumb in a lot of ways. It's the part

1051
01:31:48,080 --> 01:31:53,040
of your brain where you can set the clock five minutes fast to help you not be late. The smart

1052
01:31:53,040 --> 01:31:57,120
part of your brain knows that you did that, but the dumb part will fall for it. That same dumb

1053
01:31:57,120 --> 01:32:04,320
part of your brain can forget that the person behind that handle is a human that has feelings.

1054
01:32:04,320 --> 01:32:08,400
And that doesn't mean they're a bad person for forgetting that because it's possible.

1055
01:32:08,400 --> 01:32:11,600
Well, this really interesting idea, and I wonder if it's true that you write,

1056
01:32:12,720 --> 01:32:17,600
is that both primitive mindedness and high-mindedness tend to be contagious.

1057
01:32:18,560 --> 01:32:24,640
I hope you're right that it's possible to make both contagious, because our

1058
01:32:24,640 --> 01:32:31,440
sort of popular intuition is only one of them. The primitive mindedness is contagious as exhibited

1059
01:32:31,440 --> 01:32:36,240
by social media. To compliment you again, don't you think that your Twitter to me is like,

1060
01:32:36,960 --> 01:32:42,640
I was just looking down, and I mean, it's just high-mindedness. It's just high-mindedness,

1061
01:32:42,720 --> 01:32:48,000
down, down, down, down, down. It's gratitude. It's optimism. It's love. It's forgiveness.

1062
01:32:48,000 --> 01:32:52,480
It's all these things that are the opposite of grievance and victimhood and resentment and

1063
01:32:52,480 --> 01:32:59,040
pessimism, right? And there's, I think, a reason that a lot of people follow you because it

1064
01:32:59,680 --> 01:33:02,160
is contagious. It makes other people feel those feelings.

1065
01:33:02,160 --> 01:33:09,520
I don't know. I've been recently, over the past few months, attacked quite a lot. It is fascinating

1066
01:33:09,520 --> 01:33:14,560
to watch because there's over things that I think I probably have done stupid things,

1067
01:33:14,560 --> 01:33:20,720
but I'm being attacked for things that are totally not worthy of attack. I got attacked for a book

1068
01:33:20,720 --> 01:33:27,920
list. I saw that, by the way. I thought it was great. But you can always kind of find ways to,

1069
01:33:29,280 --> 01:33:35,680
I guess the assumption is this person surely is a fraud or some other explanation. He sure

1070
01:33:35,760 --> 01:33:39,760
has dead bodies in the basement. He's hiding or something like this. And then I'm going to

1071
01:33:39,760 --> 01:33:43,600
construct a narrative around that and mock and attack that. I mean, I don't know how that works,

1072
01:33:43,600 --> 01:33:48,640
but there is a, there does, and I think you write this in the book, there seems to be a gravity

1073
01:33:49,760 --> 01:33:52,000
pulling people towards the primitive mind and it's like-

1074
01:33:52,000 --> 01:33:58,640
Well, when it comes to anything political, right? Religious, certain things are bottom heavy,

1075
01:33:58,640 --> 01:34:04,880
you know, for our psyche. They have a magnet that pulls our psyches downwards on the ladder.

1076
01:34:04,880 --> 01:34:12,720
And why? Why does politics pull our psyches down on the ladder? Because it, for the tens of

1077
01:34:12,720 --> 01:34:21,040
thousand years that we were evolving during human history, it was life or death. Politics was life

1078
01:34:21,040 --> 01:34:30,560
or death. And so there's actually an amazing study where it's like they challenged 20 different

1079
01:34:30,560 --> 01:34:36,480
beliefs of a person and different parts of the person's brain, and then you had an MRI going,

1080
01:34:36,480 --> 01:34:40,320
different parts of the person's brain lit up when non-political beliefs were challenged versus

1081
01:34:40,320 --> 01:34:44,960
political beliefs were challenged. When political beliefs were challenged, when non-political beliefs

1082
01:34:44,960 --> 01:34:52,560
were challenged, the rational, like the prefrontal cortex type areas were lit up, when the political

1083
01:34:52,560 --> 01:34:56,400
beliefs were challenged, and then I'm getting over my head here, but it's like the parts of your

1084
01:34:56,400 --> 01:35:01,520
brain, the default mode network, the parts of your brain associated with introspection and your own

1085
01:35:01,520 --> 01:35:08,080
identity were lit up. And they were much more likely to change their mind on all the beliefs,

1086
01:35:08,080 --> 01:35:14,000
the non-political beliefs. When that default mode network part of your brain lit up, you were going

1087
01:35:14,000 --> 01:35:20,160
to, if anything, get more firm in those beliefs when you had them challenged. So politics is one

1088
01:35:20,160 --> 01:35:26,000
of those topics that just literally lights up different part of our brain. And again,

1089
01:35:26,000 --> 01:35:31,760
I think we come back to primitive mind, higher mind here. This is one of the things our primitive

1090
01:35:31,760 --> 01:35:37,360
mind comes programmed to care a ton about, and so it's going to be very hard for us to stay

1091
01:35:37,360 --> 01:35:41,840
rational and calm and looking for truth because we have all this gravity.

1092
01:35:41,840 --> 01:35:46,080
Well, it's weird because politics, what is politics? Talk about it's a bunch of different

1093
01:35:46,080 --> 01:35:49,840
issues, and each individual issue, if we really talk about it, we don't-

1094
01:35:49,840 --> 01:35:52,640
Tax policy. Why are we being emotional about this?

1095
01:35:52,640 --> 01:35:57,760
I don't think we're actually that, I mean, yeah, we're emotional about something else.

1096
01:35:57,760 --> 01:36:04,160
Yeah, I think what we're emotional about is my side, the side I've identified with, is in power

1097
01:36:04,160 --> 01:36:09,360
and making the decisions, and your side is out of power. And if your side's in power,

1098
01:36:09,360 --> 01:36:15,680
that's really scary for me because that goes back to the idea of who's pulling the strings in this

1099
01:36:15,680 --> 01:36:22,240
tribe, right? Who's the chief? Is it your family's patriarch or is it mine? You might not have food

1100
01:36:22,240 --> 01:36:28,960
if we don't win this kind of whatever chief election. So I think that it's not about the

1101
01:36:28,960 --> 01:36:33,680
tax policy or anything like that, and then it gets tied to this broader. I think a lot of our

1102
01:36:33,680 --> 01:36:38,640
tribalism has really coalesced around this. We don't have that much religious tribalism in the

1103
01:36:38,640 --> 01:36:43,040
US, right? Not the Protestants and the Catholics hate each other. We don't have that really.

1104
01:36:43,920 --> 01:36:48,080
And honestly, people like to say we have racial tribalism and everything, but

1105
01:36:48,640 --> 01:36:57,040
a white, even kind of a racist white conservative guy, I think takes the black conservative over

1106
01:36:57,040 --> 01:37:02,560
the woke white person any day of the week right now. So that's the strongest source of the division.

1107
01:37:02,560 --> 01:37:06,560
It tells me that the college is way stronger tribalism right now. I think that that white

1108
01:37:06,560 --> 01:37:13,360
racist guy loves the black conservative guy compared to the white woke guy, right? So

1109
01:37:14,320 --> 01:37:17,200
again, not that racial tribalism isn't a thing. Of course, it's always a thing,

1110
01:37:17,200 --> 01:37:24,240
but political tribalism is the number one right now. So race is almost a topic for the political

1111
01:37:24,240 --> 01:37:30,560
division versus the actual element of the tribe. It's a political football. It's yeah. So there's

1112
01:37:31,440 --> 01:37:38,560
this is dark because so this is a book about human civilization. This is a book about human nature,

1113
01:37:38,560 --> 01:37:47,280
but it's also a book about politics. It is just the way you listed out in the book.

1114
01:37:48,480 --> 01:37:55,440
It's kind of dark how we just fall into these left and right checklists. So if you're on the left,

1115
01:37:56,000 --> 01:38:02,720
it's maintaining where we weighed universal healthcare, good mainstream media, fine guns,

1116
01:38:02,720 --> 01:38:08,160
kill people, US as a racist country, protect immigrants, tax cuts, bad climate change, awful

1117
01:38:08,160 --> 01:38:13,520
raise minimum wage. And on the right is a flip of that reverse where we weighed universal healthcare,

1118
01:38:13,520 --> 01:38:19,360
bad mainstream media, bad people, kill people, not guns, kill people. US was a racist country,

1119
01:38:19,360 --> 01:38:25,200
protect borders, tax cuts, good climate change, overblown, don't raise minimum wage. I mean,

1120
01:38:25,200 --> 01:38:29,840
it has you almost don't have to think about any of this. Well, it's like literally. So when you

1121
01:38:29,840 --> 01:38:34,240
say it's a book about politics, it's interesting because it's a book about the vertical axis,

1122
01:38:34,720 --> 01:38:38,080
it's specifically not a book about the horizontal axis and that I'm not talking,

1123
01:38:38,080 --> 01:38:42,000
I don't actually talk about any of these issues. I don't put out an opinion on them.

1124
01:38:42,880 --> 01:38:49,680
Those are all horizontal, right? So rather than having another book about those issues,

1125
01:38:49,680 --> 01:38:55,600
about right versus left, I wanted to do a book about this other axis. And so on this axis,

1126
01:38:57,280 --> 01:39:02,560
the reason I had this checklist is that this is a low part of the low rung politics world.

1127
01:39:03,520 --> 01:39:09,360
Low rung politics is a checklist and that checklist evolves. Russia suddenly is popular

1128
01:39:09,360 --> 01:39:14,720
with the right as opposed to, used to be in the 60s, the left was the one defending Stalin.

1129
01:39:14,720 --> 01:39:18,240
So they'll switch, it doesn't even matter, the substance doesn't matter, it's that this is the

1130
01:39:18,240 --> 01:39:22,960
approved checklist of the capital P party and this is what everyone believes. That's a low rung

1131
01:39:22,960 --> 01:39:29,280
thing. The high rungs, this is not what it's like. High rung politics, you tell me your one view on

1132
01:39:29,280 --> 01:39:34,320
this, I have no idea what you think about anything else, right? And you're going to say I don't know

1133
01:39:34,320 --> 01:39:37,680
about a lot of stuff because inherently you're not going to have that strong an opinion because

1134
01:39:37,680 --> 01:39:42,000
you don't have that much info. These are complex things. So there's a lot of I don't know and

1135
01:39:42,000 --> 01:39:48,400
people are all over the place. You know you're talking to someone who has been subsumed with

1136
01:39:48,400 --> 01:39:54,240
low rung politics when if they tell you their opinion on any one of these issues, you know

1137
01:39:54,240 --> 01:39:58,400
you could just rattle off their opinion on every single other one. And if in three years it becomes

1138
01:39:58,400 --> 01:40:03,040
fashionable to to have this new view, they're going to have it. That's you're not thinking,

1139
01:40:03,040 --> 01:40:09,680
that's echo chamber culture. And I've been using kind of a shorthand of centrists to describe this

1140
01:40:09,680 --> 01:40:15,920
kind of a high rung thinking. But people tend to, I mean it seems to be difficult to be a

1141
01:40:15,920 --> 01:40:22,320
centrist or whatever a high rung thinker. It's like people want to label you as a person who's

1142
01:40:22,320 --> 01:40:28,880
too cowardly to take stands somehow as opposed to saying I don't know is a first statement.

1143
01:40:28,880 --> 01:40:34,640
Well the problem with centrist is that would mean that in each of these tax cuts bag, tax cuts good.

1144
01:40:34,640 --> 01:40:38,960
It means that you are saying I am in that I think we should have some tax cuts but not that many.

1145
01:40:38,960 --> 01:40:42,960
You might not think that. You might actually come do some research and say actually I think tax cuts

1146
01:40:42,960 --> 01:40:48,640
are really important. That doesn't mean oh I'm not a centrist anymore. I guess I'm a far you know

1147
01:40:48,640 --> 01:40:52,560
no no no that's why we need the second axis. So what you're trying to be when you say centrist

1148
01:40:52,560 --> 01:40:56,320
is high rung which means you might be all over the place horizontally. You might agree with the far

1149
01:40:56,320 --> 01:41:00,400
left on this thing, the far right on this thing, you might agree with the centrists on this thing.

1150
01:41:00,400 --> 01:41:06,240
But calling yourself a centrist actually like is putting yourself in a prison on the horizontal

1151
01:41:06,240 --> 01:41:12,160
axis and saying that you know whatever on the different topics I'm right in between the two

1152
01:41:12,160 --> 01:41:17,200
policy wise. That's not where you are. So yeah that's why we're badly missing this other axis.

1153
01:41:17,200 --> 01:41:24,720
Yeah I mean I still do think it's like for me I am a centrist when you project it down to the

1154
01:41:24,720 --> 01:41:30,640
horizontal but the point is you're missing so much data by not considering the vertical

1155
01:41:30,640 --> 01:41:35,920
because like on average maybe it falls somewhere in the middle but in reality there's just a lot

1156
01:41:35,920 --> 01:41:41,200
of nuance issue to issue that involves just thinking and uncertainty and changing in the

1157
01:41:41,920 --> 01:41:47,520
given the context of the current geopolitics and economics and just always considering,

1158
01:41:47,520 --> 01:41:52,640
always questioning, always evolving your views, all of that. Not just about like oh I think we

1159
01:41:52,640 --> 01:41:56,880
should be in the center on this but another way to be in the center is if there's some phenomenon

1160
01:41:56,880 --> 01:42:02,320
happening you know there's a terrorist attack you know and one side wants to say this has

1161
01:42:02,320 --> 01:42:06,080
nothing to do with Islam and the other one the other side wants to say this is radical Islam.

1162
01:42:06,640 --> 01:42:12,240
Right what's in between those is saying this is complicated and nuanced and we have to learn

1163
01:42:12,240 --> 01:42:16,400
more and it probably has something to do with Islam and something to do with the economic

1164
01:42:16,400 --> 01:42:22,400
circumstances and something to do with you know geopolitics. So in a case like that you actually

1165
01:42:22,400 --> 01:42:26,080
do get really un-nuanced when you go to the extremes and all of that nuance which is where

1166
01:42:26,080 --> 01:42:30,800
all the truth usually is is going to be in the middle so yeah. But there is a truth to the

1167
01:42:30,800 --> 01:42:36,640
fact that if you take that nuance on those issues like war in Ukraine, COVID you're going to be

1168
01:42:36,640 --> 01:42:42,080
attacked by both sides. Yes people who have who are really strongly on one side or the other hate

1169
01:42:42,880 --> 01:42:48,160
centrist people. I've gotten this myself and you know the slur that I've had thrown at me is I'm

1170
01:42:48,160 --> 01:42:52,720
an enlightened centrist in a very mocking way. So what are they actually saying? What does

1171
01:42:52,720 --> 01:42:57,040
enlightened centrist mean? It means someone who is you know Stephen Pinker or Jonathan Hyde gets

1172
01:42:57,040 --> 01:43:03,280
accused of is you know that they're highfalutin you know intellectual world and they don't actually

1173
01:43:03,280 --> 01:43:08,640
have any they don't actually take a side they don't actually get their hands dirty and they can be

1174
01:43:08,640 --> 01:43:13,520
superior to both sides without actually taking a stand right. So I see the argument and I disagree

1175
01:43:13,520 --> 01:43:20,800
with it because I firmly believe that the hardcore tribes they think they're taking a stand and they're

1176
01:43:20,800 --> 01:43:23,440
out in the streets and they're pushing for something I think what they're doing is they're

1177
01:43:23,440 --> 01:43:27,200
just driving the whole country downwards and I think they're they're hurting all the causes they

1178
01:43:27,200 --> 01:43:31,280
care about and so it's not that it's not that you know it's not that we need everyone to be sitting

1179
01:43:31,280 --> 01:43:35,920
there you know refusing to take a side it's that you can be far left and far right but be upper

1180
01:43:35,920 --> 01:43:41,760
left and upper right. If we talk about the you use the word liberal a lot in the book to mean something

1181
01:43:42,480 --> 01:43:48,160
that we don't in modern political discourse mean so it's this higher philosophical view and then

1182
01:43:48,160 --> 01:43:54,480
you use the words progressive to mean the left and conservative to mean the right can you describe

1183
01:43:54,480 --> 01:44:02,640
the concept of liberal games and power games? So the power games is is what I call the like

1184
01:44:02,640 --> 01:44:09,600
basically just the laws of nature as the when laws of nature are the laws of the land that's the

1185
01:44:09,600 --> 01:44:15,600
power game so animals watch any David Attenborough special and when the little lizard is running away

1186
01:44:15,600 --> 01:44:21,600
from the you know the bigger animal or whatever and I use an example of a bunny and a bear I don't

1187
01:44:21,600 --> 01:44:25,200
even know if bears eat bunnies they probably don't but pretend bears eat bunnies right so it's like

1188
01:44:25,200 --> 01:44:30,640
in the power games the bear is chasing the bunny there's no fairness there's no okay well what's

1189
01:44:30,640 --> 01:44:36,000
right but you know what what what what's legal no no no if the bear is fast enough it can eat the

1190
01:44:36,000 --> 01:44:41,120
bunny if the bunny is can get away it can stay living in so that's it that's the only rule now

1191
01:44:41,200 --> 01:44:46,800
humans have spent a lot of time in essentially that environment so when you have a totalitarian

1192
01:44:46,800 --> 01:44:51,520
dictatorship it's and so the what's the rule of the power games it's everyone can do whatever

1193
01:44:51,520 --> 01:44:55,760
they want if they have the power to do so it's just a game of power so that if the bunny gets away

1194
01:44:55,760 --> 01:45:00,480
the bunny actually has more power than the bear in that situation right and likewise the totalitarian

1195
01:45:00,480 --> 01:45:05,360
dictatorship there's no rules the dictator can do whatever they want they can they can they can

1196
01:45:05,360 --> 01:45:10,240
torture they can you know flatten a rebellion with a lot of murder because they have the power

1197
01:45:10,240 --> 01:45:14,320
to do so what are you gonna do right and that's that's kind of the state of nature that's our

1198
01:45:14,320 --> 01:45:19,600
natural way you know that when you look at the mafia watch a mafia movie you know there's what

1199
01:45:19,600 --> 01:45:25,520
we do a lot of we have we have it in us we all have we all can snap into power games mode when

1200
01:45:26,400 --> 01:45:33,840
it becomes all about you know just just actual raw power now the liberal games is is you know

1201
01:45:33,840 --> 01:45:38,240
something that civilizations for thousands of years have been working on it's not invented by

1202
01:45:38,240 --> 01:45:44,000
america or modern times but america's kind of was like the latest crack at it yet which is this idea

1203
01:45:44,560 --> 01:45:48,640
instead of everyone can do what they want if they have the power to do so it's everyone can do what

1204
01:45:48,640 --> 01:45:52,240
they want as long as it doesn't harm anyone else now that's really complicated how do you

1205
01:45:52,240 --> 01:45:57,600
define harm and and the idea is that everyone has their a list of rights which are protected by the

1206
01:45:57,600 --> 01:46:03,040
government and then they have their inalienable rights and they're they're protected you know those

1207
01:46:03,040 --> 01:46:09,200
are protected again by you know from from an invasion by other people and so you have this

1208
01:46:09,200 --> 01:46:15,040
kind of fragile balance and so the idea with the liberal games is you that there are laws but it's

1209
01:46:15,040 --> 01:46:21,360
not totalitarian they will build very clear strict laws kind of around the edges of what you can and

1210
01:46:21,360 --> 01:46:26,960
can't do and then everything else freedom so unlike a totalitarian dictatorship actually it's it's

1211
01:46:26,960 --> 01:46:31,280
very loose you can there's a lot of things can happen and it's kind of up to the people but

1212
01:46:31,280 --> 01:46:34,880
there are still laws that protect the very basic and alienable rights and stuff like that so it's

1213
01:46:34,880 --> 01:46:43,680
this much looser thing now the vulnerability there is that it so so so the the benefits of it are

1214
01:46:43,680 --> 01:46:47,920
obvious right freedom is great it seems like it's the most fair they you know that that equality

1215
01:46:47,920 --> 01:46:55,760
of opportunity seemed like the most fair thing and um and you know equality before the law you

1216
01:46:55,760 --> 01:47:00,560
know due process and all of this stuff so it seems fair to the founders of the us and other

1217
01:47:00,640 --> 01:47:05,440
enlightenment thinkers and it also is a great way to manifest productivity right you know you have

1218
01:47:06,240 --> 01:47:10,160
you have adam smith saying it's not from the benevolence of the butcher or the baker that

1219
01:47:10,160 --> 01:47:13,840
we get our dinner but from the from their own self-interest so you have you can harness kind of

1220
01:47:13,840 --> 01:47:20,000
selfishness for for progress but um it has a vulnerability which is that because the laws it's

1221
01:47:20,000 --> 01:47:26,160
like the totalitarian laws they don't have an excessive laws for no reason they want to control

1222
01:47:26,160 --> 01:47:29,280
everything and the us you know in the us we say we're they're not going to do that and so

1223
01:47:30,000 --> 01:47:35,280
the the second it's almost two puzzle pieces you have the laws and then you've got a liberal culture

1224
01:47:35,280 --> 01:47:40,640
liberal laws have to be married to liberal culture kind of a defense of liberal spirit

1225
01:47:40,640 --> 01:47:46,640
in order to truly have the liberal games going on and so that's vulnerable because free speech you

1226
01:47:46,640 --> 01:47:52,400
can have the first amendment that's the the laws part but if if you're in a culture where anyone

1227
01:47:52,400 --> 01:47:59,280
who you know speaks out against orthodoxy is going to be shunned from the community well you're

1228
01:47:59,280 --> 01:48:04,400
lacking the second piece of the puzzle there you're lacking liberal culture and so therefore you um

1229
01:48:04,400 --> 01:48:08,240
you might as well be in it you might as well not even have the first amendment and there's a

1230
01:48:08,240 --> 01:48:12,800
lot of examples like that where the culture has to do its part for the true liberal games to be

1231
01:48:12,800 --> 01:48:18,560
enjoyed so it's just much more complicated much more nuanced than the power games it's kind of

1232
01:48:18,800 --> 01:48:26,080
it's kind of a set of basic laws that then are coupled with a basic spirit to create this

1233
01:48:27,760 --> 01:48:33,440
very awesome and human environment that's also very vulnerable so what do you mean the culture

1234
01:48:33,440 --> 01:48:40,080
has to play along so for something like a freedom of speech to work there has to be a basic what

1235
01:48:40,080 --> 01:48:46,400
decency that if all people are perfectly good then perfect freedom without any restrictions

1236
01:48:46,400 --> 01:48:51,520
is great it's where the human nature starts getting a little iffy we start being cruel to

1237
01:48:51,520 --> 01:48:58,960
each other we start being greedy and desiring of harm and also the narcissist and sociopath

1238
01:48:58,960 --> 01:49:04,000
and psychopath in society all of that that's when you start to have to inject some limitations on

1239
01:49:04,000 --> 01:49:11,280
that freedom yeah i mean if um so that what the government basically says is we're going to let

1240
01:49:11,280 --> 01:49:18,320
everyone be mostly free um but no one no one is going to be free to physically harm other people

1241
01:49:18,320 --> 01:49:24,560
or to steal their property right um and so we're we're also we're all agreeing to sacrifice that

1242
01:49:24,560 --> 01:49:30,800
you know that that 20 percent of our freedom and then in return all of us in theory can be 80

1243
01:49:30,800 --> 01:49:36,720
percent free and that's kind of the the bargain um but now that's a lot of freedom to leave people

1244
01:49:37,040 --> 01:49:41,760
with and a lot of people choose it's like you're so free in the u.s you're actually free to be

1245
01:49:41,760 --> 01:49:47,680
unfree if you choose that's kind of what an echo chamber is to me it's you know um you can you

1246
01:49:47,680 --> 01:49:56,480
can choose to kind of be friends with people who uh essentially make it make it so uncomfortable to

1247
01:49:57,600 --> 01:50:04,160
speak your mind that it's no actual effective difference for you than if you live in a country

1248
01:50:04,160 --> 01:50:10,880
if you can't you know criticize christianity in a certain community that you have a first

1249
01:50:10,880 --> 01:50:14,480
amendment so you're not going to get arrested by the government for criticizing christianity

1250
01:50:16,080 --> 01:50:22,080
but if you but if you have this if the social penalties are so extreme that it's just never

1251
01:50:22,080 --> 01:50:28,480
worth it you might as well be in a country that it in prisons people for criticizing christianity

1252
01:50:29,040 --> 01:50:33,600
and so that same thing goes for for wokeness right this is what people get you know you know

1253
01:50:33,680 --> 01:50:37,280
cancel culture and stuff so when the reason these things are bad is because they're actually

1254
01:50:38,400 --> 01:50:46,560
they're depriving americans of the beauty of the freedom of the liberal games by you know imposing

1255
01:50:46,560 --> 01:50:51,680
a social culture that is very power games ask it's basically a power games culture comes in

1256
01:50:51,680 --> 01:50:56,560
and you might as well be in the power games now and so liberal if you live in a liberal democracy

1257
01:50:57,360 --> 01:51:02,000
it's it's you there will be always be challenges to a liberal culture

1258
01:51:04,400 --> 01:51:08,800
lowercase l liberal there'll always be challenges to a liberal culture

1259
01:51:08,800 --> 01:51:12,960
from people who are much more interested in playing the power games and and and and there has

1260
01:51:12,960 --> 01:51:17,040
to be kind of an immune system that stands up to that culture and says that's not how we do things

1261
01:51:17,040 --> 01:51:22,640
here in america actually we don't excommunicate people for not having the right religious beliefs

1262
01:51:22,640 --> 01:51:27,360
or not rent you know we don't disinvite a speaker from campus for having the wrong political beliefs

1263
01:51:27,920 --> 01:51:33,360
and if it doesn't stand up for itself it's it's it's like the immune system of the country failing

1264
01:51:33,360 --> 01:51:39,920
and power games rushes in so before chapter four in your book

1265
01:51:42,640 --> 01:51:48,480
and the chapters that will surely result in you being burned at the stake you right quote will start

1266
01:51:48,480 --> 01:51:53,440
our pitchfork tour in this chapter by taking a brief trip through the history of the republican

1267
01:51:53,440 --> 01:52:00,240
party then in the following chapters we'll take a tim's career tanking deep dive into america's

1268
01:52:00,240 --> 01:52:06,560
social justice movement as you started to talk about okay so let's go uh what's the history of

1269
01:52:06,560 --> 01:52:12,240
the republican party i'm looking at this through my vertical ladder and what is this this familiar

1270
01:52:12,240 --> 01:52:19,200
story of the republicans from the 60s to today what does it look like through the vertical lens

1271
01:52:19,200 --> 01:52:23,440
right does it look different and and is there is there an interesting story here that's been

1272
01:52:23,440 --> 01:52:27,360
kind of hidden because we're always looking at the horizontal now the horizontal story you'll

1273
01:52:27,360 --> 01:52:30,960
hear people talk about it and it's they'll say something like they're the republicans have moved

1274
01:52:30,960 --> 01:52:40,320
farther and farther to the right and um and to me that that's not really true like it was trump

1275
01:52:40,320 --> 01:52:45,600
more right wing than reagan i don't think so i think he's left left actual policy yeah yeah so it's

1276
01:52:45,600 --> 01:52:48,880
so we're using this again it's just like you're calling yourself centrist when it's not exactly

1277
01:52:48,880 --> 01:52:54,400
what you mean even though it also is yeah so i again this i was like okay look this vertical

1278
01:52:54,400 --> 01:52:58,000
lens helps with other things let's let's apply it to the republicans and here's what i saw is

1279
01:52:59,120 --> 01:53:04,720
i looked at the 60s and i saw an interesting story which is i don't think that you know

1280
01:53:05,680 --> 01:53:08,880
not everyone's familiar with like what happened in the early 60s but

1281
01:53:09,520 --> 01:53:16,240
in 1960 the republican party was very it was a plurality you had progressives like genuine

1282
01:53:16,240 --> 01:53:21,920
you know Rockefeller you know pretty progressive people um all the way to you know then you had the

1283
01:53:21,920 --> 01:53:27,520
you know moderates like eisenhower and dewey and then you go all the way to the you know farther

1284
01:53:27,520 --> 01:53:32,880
right you had goldwater and you know what you might call i call them the fundamentalists um

1285
01:53:34,480 --> 01:53:40,960
and so it's this interesting plurality right something we don't have today and what happened

1286
01:53:40,960 --> 01:53:47,920
was the the goldwater contingent which was the underdog they were small right the eisenhower

1287
01:53:47,920 --> 01:53:53,440
was the president uh had just been the president and was it seemed like the moderates were you

1288
01:53:53,440 --> 01:53:56,480
know that was the that's the he said you have to be close to the center of the chessboard that's

1289
01:53:56,880 --> 01:54:00,400
that's that's how you maintain power these people were very far from the center of the

1290
01:54:00,400 --> 01:54:06,080
chessboard but they ended up basically had like a hostile takeover they conquered their own party

1291
01:54:06,080 --> 01:54:12,880
and they did it by breaking all of the kind of unwritten rules and norms so they did things

1292
01:54:12,880 --> 01:54:16,400
like they first started with like the college republicans which was like this feeder group

1293
01:54:16,400 --> 01:54:21,280
that turned in you know a lot of the politicians started there and they they went to the election

1294
01:54:21,280 --> 01:54:26,560
and they wouldn't let the the the current president the incumbent speak and they were

1295
01:54:26,560 --> 01:54:30,000
throwing chairs and they were fist fights and eventually people gave up and they just sat there

1296
01:54:30,000 --> 01:54:33,440
and they sat in the chair talking for you know they're got their candidate until everyone

1297
01:54:33,440 --> 01:54:38,880
eventually left and then they declared victory so basically they they they they came in there

1298
01:54:38,880 --> 01:54:42,960
was there was a there was a certain set of rules agreed upon rules and they came in playing the

1299
01:54:42,960 --> 01:54:48,880
power games saying well actually if we do this you won't have the power you know we have the power

1300
01:54:48,960 --> 01:54:53,600
to to take it if we just break all the rules right and so they did and they won and that

1301
01:54:53,600 --> 01:54:57,600
became a hugely influential thing which then they then they conquered california through again

1302
01:54:57,600 --> 01:55:02,000
these these people were taken aback these you know these these these proper republican candidates

1303
01:55:02,000 --> 01:55:05,600
were appalled by the kind of like you know the insults that were being hurled at them and the

1304
01:55:05,600 --> 01:55:10,240
intimidation and the bullying and eventually they ended up in the national convention which was called

1305
01:55:10,240 --> 01:55:15,200
like the right wing wood stock it was like you know the republican national convention in 64 was

1306
01:55:15,200 --> 01:55:19,280
just they again there was jeering and they wouldn't let their moderates so their their progressives

1307
01:55:19,280 --> 01:55:23,840
even speak and there was racism you know you know jockey robinson was there and he was a proud

1308
01:55:23,840 --> 01:55:27,440
republican and he said that like he feels like he was a jew in hitler's germany with the way

1309
01:55:27,440 --> 01:55:31,840
the blacks were being treated there and it was nasty and but what do they do they they had they had

1310
01:55:31,840 --> 01:55:37,680
fiery you know plurality enough to win um and they won they ended up getting crushed in the

1311
01:55:37,680 --> 01:55:41,840
general election and they kind of faded away but to me i was like what that was an interesting

1312
01:55:41,840 --> 01:55:46,560
story i see it as um i have this character in the book called the golem which is a big kind of a

1313
01:55:46,560 --> 01:55:51,600
big dumb powerful monster that's the you know the emergent property of like a political echo chamber

1314
01:55:51,600 --> 01:55:57,600
it's like this big giant it's stupid but it's powerful and scary um and to me i was like a golem

1315
01:55:57,600 --> 01:56:05,520
rose up conquered the party for a second knocked it on his ass and then and then faded away and to

1316
01:56:05,520 --> 01:56:10,560
me when i looked at the trump revolution and a lot and not just trump the last 20 years i see that

1317
01:56:10,560 --> 01:56:19,200
same lower right that lower right monster kind of um making another charge for it but this time

1318
01:56:19,200 --> 01:56:24,640
succeeding and really taking over the party for a long period of time i see the same story which

1319
01:56:24,640 --> 01:56:29,920
is the power games are being played um in a situation when it had always been the government

1320
01:56:29,920 --> 01:56:35,360
relies on all these unwritten rules and norms to function but for example you have in 2016

1321
01:56:35,360 --> 01:56:43,360
merit garland gets it gets nominated by obama and the unwritten norm says that when the president

1322
01:56:43,360 --> 01:56:47,040
nominates a justice then you pass them through unless there's some egregious thing that's what

1323
01:56:47,040 --> 01:56:50,800
has happened but they said actually this is the last year of his presidency and the people should

1324
01:56:50,800 --> 01:56:56,400
choose i don't think we should set a new precedent where the president can't nominate people nominate

1325
01:56:56,400 --> 01:57:00,640
a supreme court justice in the last year so they they passed the through and it ends up being

1326
01:57:00,640 --> 01:57:06,240
gorsuch um and so they they they lose that seat now three years later it's trump's last year

1327
01:57:06,240 --> 01:57:12,480
and it's another election year and ginsburg dies and what did they say they say oh let's keep our

1328
01:57:12,480 --> 01:57:16,320
precedent they said no oh actually we changed our mind we're going to nominate a meek parrot

1329
01:57:17,280 --> 01:57:21,200
so to me that is classic power games right that there there's there's no actual rule and what

1330
01:57:21,200 --> 01:57:24,800
you're doing is they had the they did technically have the power to block the nomination then and

1331
01:57:24,800 --> 01:57:27,280
then they technically had the power to put someone in they and they're pretending there's

1332
01:57:27,280 --> 01:57:32,880
some principle to it but they're just extra they're going for the a short-term edge at the expense of

1333
01:57:32,880 --> 01:57:37,680
what is like the workings of the system in the long run and then one of the democrats have to do

1334
01:57:37,680 --> 01:57:42,080
in that situation because both parties have been doing this is they either can lose now all the

1335
01:57:42,080 --> 01:57:45,760
time or they start playing the power games too and now you have a prison prisoner's dilemma where

1336
01:57:45,760 --> 01:57:50,960
it's like both are end up doing this thing because and they and everyone ends up worse off the debt

1337
01:57:50,960 --> 01:57:55,280
ceiling all these power plays that are being made with these these holding the country hostage this

1338
01:57:55,280 --> 01:57:59,600
is power games and to me that's what goldwater was doing in the 60s but it was a healthier time in a

1339
01:57:59,600 --> 01:58:05,520
way because there was this plurality within the parties reduced some of the national tribalism

1340
01:58:05,520 --> 01:58:09,200
and that there wasn't as much of an appeal to that but today it's just like do whatever you

1341
01:58:09,200 --> 01:58:14,800
have to do to beat the enemies and so i'm seeing a rise in power games and i talk about the republicans

1342
01:58:14,800 --> 01:58:17,520
because they did a lot of these things first they have been a little bit more egregious but

1343
01:58:17,520 --> 01:58:22,800
both parties have been doing it over the last 20 30 years can you place a blame or maybe there's a

1344
01:58:22,800 --> 01:58:31,120
different term for it at the subsystems of this so is it the media is it the politicians like in

1345
01:58:31,120 --> 01:58:40,800
the senate and in congress is it trump so the leadership is it or maybe it's us human beings

1346
01:58:41,680 --> 01:58:47,360
maybe social media versus mainstream media is there a sense of where what is the cause of what

1347
01:58:47,360 --> 01:58:51,120
is the symptom it's very complex or as we're clients a great book why we're polarized where he

1348
01:58:51,120 --> 01:58:54,960
talks about a lot of this and there's there's there's some of these aren't you know it's really

1349
01:58:54,960 --> 01:58:59,920
no one's fault first of all it's the environment has changed in a bunch of ways you just mentioned

1350
01:58:59,920 --> 01:59:03,360
and what happens when you take human nature which is a constant and you put it into an environment

1351
01:59:04,160 --> 01:59:08,320
behavior comes out the environment's the independent variable when that changes

1352
01:59:08,320 --> 01:59:13,040
the dependent variable the behavior changes with it right and so the environment has changed in a

1353
01:59:13,040 --> 01:59:21,760
lot of ways so one major one is it used to for a long time actually the the first it was the

1354
01:59:21,760 --> 01:59:26,800
republicans and then the democrats just had a stranglehold on senate on congress there was no

1355
01:59:26,800 --> 01:59:33,280
it was not even competitive the democrats for 40 years had the majority and so therefore it actually

1356
01:59:33,280 --> 01:59:39,680
is a decent environment to compromise it because now we can both you know what you want is congress

1357
01:59:39,680 --> 01:59:44,400
people thinking about their home district and and you know voting yes on a national policy

1358
01:59:44,400 --> 01:59:48,240
because we're going to get a good deal on it back at home that's actually healthy as opposed to

1359
01:59:48,240 --> 01:59:54,480
voting in lockstep together because this is what the red party is doing regardless of what's good

1360
01:59:54,480 --> 01:59:58,480
for my home district you know an example is obama care you know there were certain republican

1361
01:59:58,480 --> 02:00:03,600
districts that would have actually officially been benefited by obama care but every republican

1362
02:00:03,600 --> 02:00:08,640
voted against it so and part of the reason is because there's no longer this obvious majority

1363
02:00:08,640 --> 02:00:13,760
every few years it switches it's a 50-50 thing and that's you know partially because

1364
02:00:14,400 --> 02:00:18,800
it's become so we've been so subsumed with this one national divide of left versus right

1365
02:00:18,800 --> 02:00:24,240
that um that that that that people are not people are whoever you know they're voting for the same

1366
02:00:24,240 --> 02:00:28,960
party for president all the way down the ticket now and so you have this just kind of 50-50 color

1367
02:00:28,960 --> 02:00:33,600
war and that's awful for compromise so there's like 10 of these things you know that have

1368
02:00:33,600 --> 02:00:39,360
redistricting but also it is social media it is you know I call it hypercharged tribalism we've

1369
02:00:39,360 --> 02:00:42,800
you know in the 60s you had kind of distributed tribalism you had some people that are worked up

1370
02:00:42,800 --> 02:00:49,040
about the USSR right they're national that's what they care about US versus foreign you had some

1371
02:00:49,040 --> 02:00:52,320
people that were saying left versus right like they had today and then other people that were saying

1372
02:00:52,320 --> 02:00:57,680
that they were fighting within the party but today you don't have that it's it's the you have

1373
02:00:57,680 --> 02:01:01,440
ideological realignment so you kind of got rid of a lot of the in-party fighting and then there's

1374
02:01:01,440 --> 02:01:06,080
not hasn't been that big of a foreign threat not nothing like the USSR for a long time so you

1375
02:01:06,080 --> 02:01:10,160
kind of lost that and what's left is just this left versus right thing and so that's kind of this

1376
02:01:10,160 --> 02:01:17,040
hypercharged whirlpool that subsumes everything and um and so yeah yeah it's I mean people point

1377
02:01:17,040 --> 02:01:21,840
to Newt Gingrich you know and people like there's certain characters that enacted policies that

1378
02:01:21,840 --> 02:01:26,160
stoked this kind of thing but I think this is a much bigger kind of environmental shift

1379
02:01:26,160 --> 02:01:29,680
well that's going back to our questions about the role of individuals in human history

1380
02:01:30,240 --> 02:01:34,400
so the the interesting one of the many interesting questions here is about Trump

1381
02:01:34,400 --> 02:01:40,400
is he a symptom or a cause because he seems to be from the public narrative such a significant

1382
02:01:41,440 --> 02:01:45,680
catalyst for some of the things we're seeing this goes back to what we were talking about earlier

1383
02:01:45,680 --> 02:01:50,640
right like is it the person or is the times I think he's a perfect example of it's a both

1384
02:01:50,640 --> 02:01:54,480
situation I don't think that I don't think if you plucked Trump out of this situation I don't

1385
02:01:54,480 --> 02:02:00,400
think it Trump was inevitable but I think we were very vulnerable to a demagogue and if you

1386
02:02:00,400 --> 02:02:05,760
hadn't been Trump would have had no chance and so what and why were we vulnerable to a demagogue

1387
02:02:05,760 --> 02:02:13,440
is because you have these well I mean I think it's specifically on the right if you actually look

1388
02:02:13,440 --> 02:02:17,760
at the stats it's pretty bad like the people who because it's not just who voted for Trump a lot

1389
02:02:17,760 --> 02:02:22,160
of people just vote for the red right what's interesting is who voted for Obama against

1390
02:02:22,160 --> 02:02:28,000
Romney and then voted for Trump who you know these are not racists right these are not

1391
02:02:28,000 --> 02:02:33,040
hardcore republicans they voted for Obama and where did the switch come from

1392
02:02:33,040 --> 02:02:38,080
places that had economic despair where bridges were were not working well that's a signifier

1393
02:02:38,080 --> 02:02:42,720
you know we're we're paints chipping in the schools you know these little things like this so I

1394
02:02:42,720 --> 02:02:47,920
think that you know you had this a lot of these kind of rural towns you have true despair and then

1395
02:02:47,920 --> 02:02:55,120
you also have a the number one indicator of voting for Trump was distrust in media and the media has

1396
02:02:55,120 --> 02:03:02,160
become much less trustworthy you know and so you you have the all these ingredients that actually

1397
02:03:02,160 --> 02:03:06,720
make us very vulnerable to a demagogue and a demagogue is someone who takes advantage right

1398
02:03:06,720 --> 02:03:11,760
there's someone who comes in and says I can pull all the right strings and pull it and and and push

1399
02:03:11,760 --> 02:03:16,080
all the right emotional buttons right now and get myself power by taking advantage of the

1400
02:03:16,080 --> 02:03:22,800
circumstances and that is what Trump totally did it makes me wonder how easy it is for somebody

1401
02:03:22,800 --> 02:03:29,840
who is a charismatic leader to capitalize on cultural resentment when when there's economic

1402
02:03:29,840 --> 02:03:35,200
hardship to channel that so john height wrote a great article about like the truth we basically

1403
02:03:35,200 --> 02:03:40,160
we literally like truth is in an all-time low right now like it's the media is not penalized

1404
02:03:40,160 --> 02:03:46,400
for lying yeah right msnbc fox news these are not penalized for being inaccurate or penalized if

1405
02:03:46,400 --> 02:03:53,760
they stray from the orthodoxy on social media it's not the truest tweets that go viral right and so

1406
02:03:53,760 --> 02:04:00,320
trump understood that better than anyone right he he took advantage of it he he was living in the

1407
02:04:00,320 --> 02:04:06,800
current world when everyone else was stuck in the past and he saw that and he just lied he everything

1408
02:04:06,880 --> 02:04:11,680
he said you know it doesn't the truth was not relevant at all right it's just it's

1409
02:04:11,680 --> 02:04:15,760
truly it's not relevant to him when what he's talking about he doesn't care and and and he

1410
02:04:15,760 --> 02:04:20,880
knew that neither do us a subset of the country i was thinking about this just reading articles by

1411
02:04:20,880 --> 02:04:28,000
journalists especially when you're not a um a famous journalist in yourself but you're more like in

1412
02:04:28,000 --> 02:04:34,240
your times journalists so the big famous thing is the institution you're part of that they're like

1413
02:04:34,240 --> 02:04:39,440
you can just lie yeah because you're not going to get punished for it you're going to be rewarded for

1414
02:04:39,440 --> 02:04:46,080
of the popularity of an article so if you write 10 articles it's there's a huge incentive to just

1415
02:04:46,080 --> 02:04:50,960
make stuff up you gotta get clicks to get clicks that's the first and foremost and like culturally

1416
02:04:51,840 --> 02:04:55,600
people will attack that article to say it's not like one half the country will attack that

1417
02:04:55,600 --> 02:05:02,480
article for saying it's dishonest but they'll kind of forget the you will not have a reputational hit

1418
02:05:02,880 --> 02:05:06,640
there won't be a memory like this person made up a lot of stuff in the past no they'll take

1419
02:05:06,640 --> 02:05:12,320
one article at a time and they'll attach the reputation hits will be to new york times the

1420
02:05:12,320 --> 02:05:16,160
institution yeah and so for the individual journalist there's a huge incentive to make

1421
02:05:16,160 --> 02:05:22,880
stuff up totally it's it's it's and it's scary because it's almost you can't survive if you're

1422
02:05:22,880 --> 02:05:26,720
just an old school honest journalist who really works hard and tries to get it right and does it

1423
02:05:26,720 --> 02:05:32,000
with nuance like well you can be as you can be a big time substacker or big time podcaster a lot

1424
02:05:32,000 --> 02:05:37,920
of people do have a reputation for accuracy and rigor and they have huge audiences but

1425
02:05:39,040 --> 02:05:45,680
if you're working in a big company right now it's I mean especially I mean I like I think that

1426
02:05:46,880 --> 02:05:51,200
many of the big media brands are very much controlled by the left and but I will say that

1427
02:05:51,200 --> 02:05:55,840
that the ones that are controlled by the right are even more egregious not just in terms of accuracy

1428
02:05:55,920 --> 02:05:59,440
but also in terms of you know the new york times for all of its criticisms

1429
02:06:00,720 --> 02:06:06,320
they have a handful of uh they they every they here and there they put out a pretty

1430
02:06:07,280 --> 02:06:12,480
you know an article that strays from the you know Barry Weiss wrote there for a long time and then

1431
02:06:12,480 --> 02:06:17,600
you've got um they wrote an article criticizing free speech on campus stuff you know recently

1432
02:06:17,600 --> 02:06:23,680
um and they have you know they have a couple very you know left uh progressive friendly

1433
02:06:23,680 --> 02:06:28,160
conservatives but they have conservatives that are writing the op-eds fox news you know you're not

1434
02:06:28,160 --> 02:06:34,560
seeing thoughtful uh bright bar you're not seeing thoughtful progressives writing there right there's

1435
02:06:34,560 --> 02:06:42,080
some degree to which the new york times I think still incentivizing the values the vertical the

1436
02:06:42,080 --> 02:06:48,880
high effort so the you're allowed to have a conservative opinion if you do a really damn

1437
02:06:48,880 --> 02:06:55,440
good job yeah like if it's a very thorough in-depth art kind of and if you kind of pander to the

1438
02:06:55,440 --> 02:07:00,720
progressive senses in all the right ways you know I always joke that you know Ted they always have a

1439
02:07:00,720 --> 02:07:05,520
couple you know token conservatives but they get on stage and they're basically like so totally

1440
02:07:05,520 --> 02:07:10,560
you're all you know where the the progressives are it's right about all of this but maybe maybe

1441
02:07:10,560 --> 02:07:14,560
you know libertarianism isn't all about you know it's this so there is an element but you know what

1442
02:07:14,560 --> 02:07:20,000
it's it's something it's better than than being a total tribal I think you can you can see the

1443
02:07:20,000 --> 02:07:23,600
New York Times tug-of-war the internal tug-of-war you can see it because then they also have these

1444
02:07:23,600 --> 02:07:27,680
awful instances you know or like you know the firing of James Bennett which is this whole

1445
02:07:27,680 --> 02:07:34,080
other story but like they have you yeah you can see it going both ways but in the 60s what did

1446
02:07:34,080 --> 02:07:39,520
you have you had ABC NBC CBS you know the 70s you know you had these three news channels and

1447
02:07:39,520 --> 02:07:42,960
they weren't always right and they definitely sometimes spun a narrative together maybe about the

1448
02:07:42,960 --> 02:07:48,800
Vietnam or whatever but they if one of them was just lying they'd be embarrassed for it they would

1449
02:07:48,800 --> 02:07:53,040
be penalized they'd be dinged and they'd be known as this is the trash one and that would be terrible

1450
02:07:53,040 --> 02:07:55,680
for their ratings because they weren't just catering to half the country they're getting

1451
02:07:55,680 --> 02:08:00,400
they all are catering to the whole country so both on the axis of accuracy and on the axis of

1452
02:08:00,400 --> 02:08:07,600
neutrality they had to you know try to stay somewhere in the the reasonable range and that's

1453
02:08:07,600 --> 02:08:14,960
just gone one of the things i'm really curious about is i think your book is incredible i'm very

1454
02:08:14,960 --> 02:08:20,640
curious to see how it's written about by the press because i could see click i could myself

1455
02:08:20,640 --> 02:08:26,400
write with the help of chat jpt of course clickbait articles in either direction yeah it's easy to

1456
02:08:26,400 --> 02:08:31,680
imagine your whole book is beautifully written for clickbait articles yeah if any journalist

1457
02:08:31,680 --> 02:08:38,400
out there need help i can i can help yeah i can write the most atrocious criticisms yeah uh i'm

1458
02:08:38,400 --> 02:08:47,680
i'm i'm i'm i'm ready i'm braced um yeah so speaking of which uh you write about social

1459
02:08:47,680 --> 02:08:55,360
justice you write about two kinds of social justice liberal social justice and uh sjf social

1460
02:08:55,360 --> 02:09:01,360
justice fundamentalism what are those yeah so like the term wokeness is so loaded with baggage it's

1461
02:09:01,360 --> 02:09:06,960
kind of like mocking and derogatory and that's i was trying not to do that in this book um if it's

1462
02:09:06,960 --> 02:09:10,720
the terms loaded with baggage you're already kind of you're out you're out you're from from the first

1463
02:09:10,720 --> 02:09:21,920
minute you're already behind um so to me uh it also that when people say wokeness is bad social

1464
02:09:21,920 --> 02:09:27,760
justice is bad they're throwing the baby out with the bathwater um because the the you know the proudest

1465
02:09:27,840 --> 02:09:32,480
tradition in the us is liberal social justice and what i mean by that again liberal meaning with

1466
02:09:32,480 --> 02:09:39,520
lower case l it is it is intertwined with liberalism so martin luther king classic example his i

1467
02:09:39,520 --> 02:09:47,600
have a dream speech he says stuff like this country you know is wrote you know has made a promise

1468
02:09:48,720 --> 02:09:54,320
to all of its citizens and it has broken that promise to to its black citizens right in other

1469
02:09:54,320 --> 02:10:00,800
words liberalism the constitution the core ideals those are great we are not living up to them we're

1470
02:10:00,800 --> 02:10:07,280
failing on some of them so civil disobedience the goal if it wasn't to to hurt liberalism is to

1471
02:10:07,280 --> 02:10:12,160
specifically break the laws that were already violating liberal that were the laws that were

1472
02:10:12,160 --> 02:10:17,600
a violation of liberalism to expose that this is illiberal that the constitution should not have

1473
02:10:17,600 --> 02:10:23,360
people of different skin color sitting in different parts of the bus and so it was it was kind of a

1474
02:10:23,360 --> 02:10:27,040
it was really patriotic you know the civil rights movement who was saying this is a beautiful you

1475
02:10:27,040 --> 02:10:31,280
know we have a we have a liberalism is this beautiful thing and we need to do better at it

1476
02:10:31,280 --> 02:10:37,600
so i call it liberal social justice and it used the tools of liberalism to try to uh

1477
02:10:38,720 --> 02:10:44,800
to try to uh improve the flaws and that were going on so free speech you know mario savio in the

1478
02:10:44,800 --> 02:10:50,080
sixties was the you know he's a leftist and what what would the leftist doing in the sixties on

1479
02:10:50,080 --> 02:10:55,760
berkeley campus you know they were saying we need more free speech um because that's what social

1480
02:10:55,760 --> 02:10:59,120
liberal social justice was fighting for that but you can also go back to the 20s women's suffrage i

1481
02:10:59,120 --> 02:11:04,720
mean so the you know the emancipation the thing that america obviously has all of it's these these

1482
02:11:04,720 --> 02:11:09,440
are these are all ugly things that it had to get out of but it got out of them you know one by one

1483
02:11:09,440 --> 02:11:13,440
and it's still getting out of them that's what's cool about america and liberal social justice

1484
02:11:13,440 --> 02:11:19,920
basically is the practice of saying where are we not being perfect liberals and now let's fix that

1485
02:11:20,720 --> 02:11:25,040
so that that's the idea of liberalism that permeates the history in the united states but

1486
02:11:25,040 --> 02:11:30,480
then there's interplay heaven there's so many good images in this book but one of them is

1487
02:11:31,280 --> 02:11:36,960
highlighting the interplay of different ideas over the past uh let's say a hundred years so

1488
02:11:36,960 --> 02:11:42,800
liberalism is on one side there's that thread there's marxism on the other and then there's

1489
02:11:42,800 --> 02:11:51,760
postmodernism how do those interplay together so it's interesting because marxism is and all of

1490
02:11:51,760 --> 02:11:55,920
its various you know descendants obviously there's a lot of things that are rooted in marxism that

1491
02:11:55,920 --> 02:12:01,360
aren't you know the same thing as what carl marx preached but what do they all have in common

1492
02:12:02,480 --> 02:12:12,480
they think liberalism is bad right they actually think that um that the opposite of

1493
02:12:12,480 --> 02:12:17,680
what martin leuther king and other people in the civil rights and other movements they think the

1494
02:12:17,680 --> 02:12:20,960
opposite they they think he thinks you know liberalism is good we need to preserve it they

1495
02:12:20,960 --> 02:12:27,600
said liberalism is the problem these other problems with racism and inequality that we're seeing those

1496
02:12:27,600 --> 02:12:33,520
are inevitable results of liberalism liberalism is a rigged game and it's just the power games in

1497
02:12:33,520 --> 02:12:38,080
disguise there is no liberal games it's just the power games in disguise and there's the upper

1498
02:12:38,080 --> 02:12:42,480
people that oppress the lower people and they convince the lower people there's all about

1499
02:12:42,480 --> 02:12:46,720
false consciousness they convince the lower people that everything is fair and now the lower

1500
02:12:46,720 --> 02:12:50,240
people vote against their own interests and they and they work to preserve the system that's

1501
02:12:50,240 --> 02:12:54,720
oppressing them and what do we need to do we need to actually there's much more revolutionary

1502
02:12:54,720 --> 02:13:01,440
we need to overthrow liberalism right so people think is oh you know like what we call a wokeness

1503
02:13:01,440 --> 02:13:06,480
is just you know a normal social justice activist activism but it's like more extreme right it's this

1504
02:13:06,560 --> 02:13:12,480
no no it's the polar opposite polar opposite and so um now that's that's that's the marxist

1505
02:13:12,480 --> 02:13:17,440
threat now postmodernism is kind of you know this term that is super controversial and I don't

1506
02:13:17,440 --> 02:13:20,400
think anyone calls themselves a postmodernist or take all of this with a grain of salt in

1507
02:13:20,400 --> 02:13:24,720
terms of the term but what's the definition of radical the definition of radical to me is

1508
02:13:25,360 --> 02:13:34,160
how deep you want change to happen at so so um a liberal progressive and and a conservative

1509
02:13:34,160 --> 02:13:38,800
progressive will disagree about policies the liberal progressive wants to you know change a

1510
02:13:38,800 --> 02:13:43,520
lot of policies and change change change right and the conservative is more wants to keep things the

1511
02:13:43,520 --> 02:13:49,600
way they are but they're both conservative when it comes to liberalism um beneath it the liberal

1512
02:13:49,600 --> 02:13:54,480
kind of foundation of the country they both want to they both become conservatives about that

1513
02:13:55,200 --> 02:14:00,480
the marxist is more radical because they want to go one notch deeper and actually overthrow that

1514
02:14:00,480 --> 02:14:08,400
foundation now what's below what's below liberalism is kind of the core tenets of modernity um this

1515
02:14:08,400 --> 02:14:16,720
idea of reason and um the notion that there is an objective truth and um uh science as the scientific

1516
02:14:16,720 --> 02:14:20,320
method right these things are actually beneath and even the marxist if you look at the frankford

1517
02:14:20,320 --> 02:14:24,880
school you know these these these post marxist thinkers and and marx himself they were not

1518
02:14:24,960 --> 02:14:30,000
anti science they believed in that bottom bottom foundation they were they were they were they

1519
02:14:30,000 --> 02:14:34,000
actually wanted to preserve modernity but they wanted to get rid of liberalism on top of it

1520
02:14:34,000 --> 02:14:37,920
the post modernist is even more radical because they want to actually go down to the bottom level

1521
02:14:37,920 --> 02:14:43,840
and overthrow they think science itself is a tool of oppression they think it's a tool uh where

1522
02:14:43,840 --> 02:14:48,640
oppression kind of flows through you know they think that the white western world has invented

1523
02:14:48,640 --> 02:14:53,120
these concepts like you know they they claim that there's an objective truth and that there's

1524
02:14:53,120 --> 02:14:57,360
you know reason and science and they think all of that is just one meta narrative and right and

1525
02:14:57,360 --> 02:15:03,360
it and it goes a long way to serve the interests of the powerful so in the sense that it's almost

1526
02:15:03,360 --> 02:15:08,400
caricatured but that that is to the core their belief that math math could be racist for example

1527
02:15:08,400 --> 02:15:15,040
oh yeah not the not the educational math but literally math the math the notion in math that

1528
02:15:15,040 --> 02:15:19,680
there's a right answer and a wrong answer that they believe is a meta narrative that serves

1529
02:15:19,680 --> 02:15:24,080
white supremacy or in in the post modernist might have said it serves just the powerful

1530
02:15:25,120 --> 02:15:30,880
or the wealthy but to so what social justice fundamentalism is is you take the the Marxist

1531
02:15:30,880 --> 02:15:37,600
thread that has been going on in lots of countries and has and the whoever the upper and lower is

1532
02:15:37,600 --> 02:15:41,760
that's what they all have in common but the upper and lower you know and for for Marx was

1533
02:15:41,760 --> 02:15:48,320
the ruling class and the oppressed class it was economic um and then uh but you come here and and

1534
02:15:49,280 --> 02:15:53,200
the economic class doesn't you know doesn't resonate as much here as it did maybe in some

1535
02:15:53,200 --> 02:15:58,960
of those other places but what does resonate here in the 60s and 70s is race and gender and these

1536
02:15:58,960 --> 02:16:04,960
kind of social justice disagreements and so what social justice fundamentalism is is basically this

1537
02:16:04,960 --> 02:16:13,120
this tried and true framework of of Marxist you know this Marxist framework kind of with a

1538
02:16:13,120 --> 02:16:18,640
new skin on it which is American social justice and then made even more radical with the infusion

1539
02:16:18,640 --> 02:16:23,680
of post modernism where you know not just as liberalism bad but actually the you know that

1540
02:16:23,680 --> 02:16:28,880
like you said math can be racist so it's this kind of like philosophical Frankenstein this

1541
02:16:28,880 --> 02:16:34,080
like stitched together of these it and has and and so again it's called you you know they they

1542
02:16:34,080 --> 02:16:37,680
wear the same uniform as the liberal social justice they say social justice right you know

1543
02:16:37,680 --> 02:16:43,440
racial equality but it has nothing to do with liberal social justice it is directly opposed

1544
02:16:43,440 --> 02:16:49,600
to liberal social justice this is fascinating the evolution of ideas if if we ignore the harm done by

1545
02:16:49,600 --> 02:16:56,240
it it's fascinating how humans get together and evolve these ideas so as you show Marxism is the

1546
02:16:56,240 --> 02:17:01,360
idea that societies is zero some i mean i guess zero some is a really important thing here uh

1547
02:17:01,360 --> 02:17:06,160
zero some struggle between the ruling class and the working class with power being exerted

1548
02:17:06,240 --> 02:17:13,840
to politics and economics then you add critical theory Marxism 2.0 on top of that and you add to

1549
02:17:13,840 --> 02:17:18,560
politics and economics you add culture and institutions and then on top of that for post

1550
02:17:18,560 --> 02:17:22,880
modernism you add science you have morality basically anything else you can think of to stitch

1551
02:17:22,880 --> 02:17:27,200
together Frankenstein and if you notice and which is not necessarily bad but in this case I think

1552
02:17:27,200 --> 02:17:32,160
it's actually violating the Marxist tradition by being anti-science and and and you know and it's

1553
02:17:32,160 --> 02:17:37,200
violating the post modernism because what post modernists were they were radical skeptics not

1554
02:17:37,200 --> 02:17:40,880
just of they were radical skeptics not just of the way things were but of their own beliefs

1555
02:17:41,440 --> 02:17:45,360
that they and what and social justice fundamentalism is suddenly is not at all

1556
02:17:46,800 --> 02:17:50,960
self self critical it says that we have the answers which is the opposite of what post

1557
02:17:50,960 --> 02:17:54,480
modernists would ever say this and no you just have another meta narrative so and it's also

1558
02:17:54,480 --> 02:17:57,840
violating of course the tradition of like liberal social justice in a million ways because it's

1559
02:17:57,840 --> 02:18:03,360
it's anti-liberal and so this Frankenstein comes together meanwhile liberal social justice doesn't

1560
02:18:03,360 --> 02:18:09,840
have a Frankenstein it's very clear it's very it's a crisp ideology that says we need we they're

1561
02:18:09,840 --> 02:18:15,520
trying to make we trying to get to a more perfect union they're trying to to to keep the promises

1562
02:18:15,520 --> 02:18:20,960
made in the constitution and that's what it's trying to do and so it's it's much simpler in a lot of

1563
02:18:20,960 --> 02:18:26,800
ways so you write that my big problem with social justice fundamentalism isn't the ideology itself

1564
02:18:26,800 --> 02:18:33,120
it's what its scholars and activists started to do sometimes around 2013 when they began to wield a

1565
02:18:33,120 --> 02:18:38,960
kajo that's not supposed to have any place in the country like the u.s so it's the actions not the

1566
02:18:38,960 --> 02:18:45,920
ideas well i'm clear i don't like the ideology i think it's a low rung ideology i think it's

1567
02:18:45,920 --> 02:18:51,520
morally inconsistent based on you know it's it's flip-flops on on its morals depending on the the

1568
02:18:51,520 --> 02:18:58,960
group i think it's echo chambery i think it's um i i think it's it's it's full of inaccuracies and

1569
02:18:58,960 --> 02:19:03,840
kind of can't stand up to debate so i think it's a low but but there's a ton of lowering ideologies

1570
02:19:03,840 --> 02:19:07,840
i don't like i don't like a lot of religious doctrines i don't like a lot of political doctrines right

1571
02:19:08,720 --> 02:19:14,560
the u.s is a place inherently that is a mishmash of a ton of ideologies and i'm not going to like

1572
02:19:14,560 --> 02:19:18,320
two-thirds of them at any given time so my problem but the reason i'm writing about this is not

1573
02:19:18,400 --> 02:19:21,920
because i'm like by the way this ideology is not something i like that's not interesting

1574
02:19:23,280 --> 02:19:28,160
the reason that it must be written about right now this particular ideology is because

1575
02:19:28,960 --> 02:19:34,080
it's not playing nicely with others what what what i if you if you want to be a hardcore you

1576
02:19:34,080 --> 02:19:39,920
know evangelical christian go in the u.s says live and let live not only are you allowed to

1577
02:19:39,920 --> 02:19:44,160
to have an echo chamber of some kind you're it's actively protected here live and let live

1578
02:19:44,160 --> 02:19:48,160
they can do what they want you do what you want now if the evangelical christians started saying

1579
02:19:48,400 --> 02:19:53,600
by the way anyone who says anything that conflicts with even evangelical christianity

1580
02:19:53,600 --> 02:19:58,880
is going to be severely socially punished and they have the cultural power to do so which they

1581
02:19:58,880 --> 02:20:03,200
don't in this case they might like to but they don't have the power but they're able to get

1582
02:20:03,200 --> 02:20:07,440
anyone fired who they want and they're able to actually change the curriculum in all of these

1583
02:20:07,440 --> 02:20:12,480
schools and to to suddenly not conflict with no more evolution in the textbooks because they don't

1584
02:20:12,480 --> 02:20:18,560
want it now i would write a book about why i about evangelical christianity because that's

1585
02:20:18,560 --> 02:20:24,160
what every liberal regardless of what you think of the actual horizontal beliefs doesn't matter

1586
02:20:24,160 --> 02:20:31,280
what they believe when when they start violating live and let live and shutting down other areas

1587
02:20:31,280 --> 02:20:36,080
other segments of society and in kind of it's almost like a you know not to you know it's not

1588
02:20:36,080 --> 02:20:40,800
the best analogy but like it's like a an echo chamber is like a benign tumor and what you

1589
02:20:40,800 --> 02:20:44,560
but you have to watch out for is a tumor that starts to metastasize starts to forcefully spread

1590
02:20:45,200 --> 02:20:51,680
and damage the tissue around it and that's what this particular ideology has been doing

1591
02:20:51,680 --> 02:21:01,200
do you worry about it you know as an existential threat to to liberalism in in the west in the

1592
02:21:01,200 --> 02:21:10,160
united states is it a problem or is it the biggest problem that's threatening all of human

1593
02:21:10,240 --> 02:21:16,240
civilization it's i would never i would not say it's the biggest problem it might be i wouldn't

1594
02:21:16,240 --> 02:21:20,960
if someone if it turns out in 50 years someone says actually it was i wouldn't be shocked but i

1595
02:21:20,960 --> 02:21:26,000
also i would i wouldn't bet on that because there's a lot of problems i'm a little sorry to interrupt

1596
02:21:26,000 --> 02:21:32,240
it it is popular to say that kind of thing though and it's less popular to say the same

1597
02:21:32,240 --> 02:21:39,440
thing about ai or nuclear weapons which worries me that i'm more worried about nuclear weapons

1598
02:21:39,440 --> 02:21:44,960
even still than i am about wokeism so i've gotten i've had probably a thousand arguments

1599
02:21:44,960 --> 02:21:49,520
about this that's one nice thing about spending six years procrastinating on getting a book done

1600
02:21:49,520 --> 02:21:53,200
is you end up test battle testing your ideas a million times so i've heard this one a lot right

1601
02:21:53,200 --> 02:22:01,120
which is there's kind of three groups of former obama voters one is super woke now another one

1602
02:22:01,120 --> 02:22:06,960
is super anti woke now and the third is what you just said which is sure wokeness is over the top

1603
02:22:06,960 --> 02:22:13,040
right they're not you're not woke but i think that the anti woke people are totally lost their

1604
02:22:13,040 --> 02:22:19,840
mind and it's just not that big a deal right now here's why i disagree with that because it's not

1605
02:22:20,560 --> 02:22:28,800
it's not wokeness itself it's that a radical um political movement of which there will always

1606
02:22:28,800 --> 02:22:34,320
be a lot in the country has managed to do something that a radical movement is not supposed to be able

1607
02:22:34,320 --> 02:22:44,240
to do in the us which is they've managed to hijack institutions all across the country and hijack

1608
02:22:46,080 --> 02:22:51,680
medical journals and universities and you know the aclu you know so you know all the you know

1609
02:22:51,680 --> 02:22:59,760
activist organizations and nonprofits and and certain NGOs yeah and many and many tech companies

1610
02:22:59,760 --> 02:23:04,560
and what so it's not that i think this thing is so bad it's a little like we said with trump

1611
02:23:04,560 --> 02:23:09,200
it's that what i'm what the reason trump scares me is not because trump's so bad it's that because

1612
02:23:09,200 --> 02:23:15,520
it shows it's it reveals that we were vulnerable to a demagogue to candidate and what wokeness

1613
02:23:15,520 --> 02:23:20,560
reveals to me is that we are currently and until something changes will continue to be vulnerable

1614
02:23:21,120 --> 02:23:30,400
to a um a bully a bully movement and a forcefully expansionist movement that wants to actually

1615
02:23:33,280 --> 02:23:39,600
destroy the workings inner their liberal gears and tear them apart and so here's the way I view

1616
02:23:39,600 --> 02:23:43,920
a liberal democracy is it is it it is a bunch of these institutions that were that were trial and

1617
02:23:43,920 --> 02:23:51,120
error crafted over you know hundreds of years and they all rely on trust public trust and

1618
02:23:51,120 --> 02:23:56,000
there's certain kind of feeling of unity that that actually is critical to a liberal democracies

1619
02:23:56,000 --> 02:24:03,360
functioning and what i see this thing is is as a parasite on that that whose goal is and i'm not

1620
02:24:03,360 --> 02:24:07,520
saying each by the way each individual in this is i don't think they're bad people i think that

1621
02:24:07,600 --> 02:24:14,000
it's it's the ideology itself has the property of its goal is to tear apart the pretty delicate

1622
02:24:14,000 --> 02:24:20,240
workings of the liberal democracy and shred the critical lines of trust and so you talk about

1623
02:24:20,240 --> 02:24:24,960
ai and you talk about all these other big problem nuclear right the reason i i like writing about

1624
02:24:24,960 --> 02:24:29,600
that stuff a lot more than i like writing about politics this wasn't a fun topic for me is because

1625
02:24:29,600 --> 02:24:34,080
i realized that like all of those things if they were going to have a good future with those things

1626
02:24:34,080 --> 02:24:38,400
and they're actually threats like i said we need to have our wits about us and we need the the liberal

1627
02:24:39,120 --> 02:24:44,080
you know gears and and and levers working we need the liberal machine working and so with

1628
02:24:44,080 --> 02:24:49,200
something's threatening to undermine that it affects everything else we need to have our

1629
02:24:49,200 --> 02:24:56,080
scientific mind about us about these foundational ideas but i i i guess my sense of hope comes from

1630
02:24:56,720 --> 02:25:04,720
observing the immune system respond to wokeism there seems to be a pro liberalism immune system

1631
02:25:05,600 --> 02:25:08,800
and not only that so like there's intellectuals there's people that are

1632
02:25:09,440 --> 02:25:16,320
willing to do the fight you talk about courage are being courageous and there is a hunger for that

1633
02:25:16,320 --> 02:25:22,560
such that those ideas can become viral and they take over so i just don't see a mechanism by which

1634
02:25:22,560 --> 02:25:30,960
wokeism accelerates like exponentially and takes over like it's expand it feels like as it expands

1635
02:25:30,960 --> 02:25:38,080
the immune system responds uh the the the immune system of the of liberalism of of basically a

1636
02:25:38,080 --> 02:25:43,760
country in at least in the united states that still ultimately at the core of the individual values

1637
02:25:43,760 --> 02:25:48,880
the freedom of speech just freedoms in general the freedom of an individual but that's the battle

1638
02:25:49,360 --> 02:25:56,000
which is stronger so to me it is like a virus and an immune system yeah and i totally agree i see

1639
02:25:56,000 --> 02:26:00,000
the same story happening and i you know i'm sitting here rooting for the immune system

1640
02:26:00,000 --> 02:26:06,480
are you still worried well here's the thing so a liberal democracy is always going to be vulnerable

1641
02:26:06,480 --> 02:26:13,040
to a movement like this right and there will be more because it's not a totalitarian dictatorship

1642
02:26:13,040 --> 02:26:17,600
because if you can socially pressure people to not say what they're thinking you can suddenly

1643
02:26:17,600 --> 02:26:22,800
start to just take over right you can break the liberalism of the liberal democracy quite easily

1644
02:26:22,800 --> 02:26:29,680
and suddenly a lot of things are illiberal on the other hand the same vulnerability the same

1645
02:26:29,680 --> 02:26:36,480
system that's vulnerable to that also is hard to truly conquer because now the Maoists right

1646
02:26:36,480 --> 02:26:41,120
similar kind of vibe they were saying that science is evil and is a you know and that and that they're

1647
02:26:41,120 --> 02:26:47,920
the intellectuals are you know it's all the it's all this big conspiracy but they could murder you

1648
02:26:49,040 --> 02:26:56,880
and they had the hard cudgel in their hand right and the hard cudgel is scary and

1649
02:26:57,920 --> 02:27:02,960
um and and you can conquer a country with the hard cudgel but you can't use that in the us

1650
02:27:02,960 --> 02:27:08,960
so what they have is a soft cudgel which can have the same effect initially you can scare people

1651
02:27:08,960 --> 02:27:12,960
into shutting up you can't maybe imprison them and murder them but if you can socially ostracize

1652
02:27:12,960 --> 02:27:17,440
them and get them fired that basically is going to have the same effect so the soft cudgel can

1653
02:27:17,440 --> 02:27:23,040
have the same effect for a while but the thing is it's it's a little bit of a house of cards

1654
02:27:23,040 --> 02:27:31,280
because it relies on fear and as soon as that fear goes away the whole thing falls apart right the

1655
02:27:31,280 --> 02:27:37,760
soft cudgel requires people to be so scared of getting canceled or getting whatever and as soon

1656
02:27:37,760 --> 02:27:42,400
as some people start you know Toby Lutka of Shopify I always like think about you know he just

1657
02:27:42,400 --> 02:27:46,320
said you know what I'm not scared of this soft cudgel and spoke up and said we're not political

1658
02:27:46,320 --> 02:27:49,680
at this company and we're not a family we're a team and we're gonna do this and you know what like

1659
02:27:50,400 --> 02:27:55,280
they're thriving he will be on this podcast he seems like a fascinating he's amazing he spoke up

1660
02:27:55,280 --> 02:28:02,480
he's one of the the smartest and like kindest dudes but he's also um he has courage at a time

1661
02:28:02,480 --> 02:28:06,800
when it's hard but here's the thing is that it's different than that you need so much less courage

1662
02:28:06,800 --> 02:28:12,560
against a soft cudgel then you do the the Iranians throwing their hijabs into the fire those people's

1663
02:28:12,560 --> 02:28:19,120
courage just just blows away any courage we have here because they might get executed that's the

1664
02:28:19,120 --> 02:28:26,480
thing is that you can actually have courage right now and it's so don't worry about it

1665
02:28:28,960 --> 02:28:34,320
oh man the irony of that and you talk about so two things to fight this there's two things

1666
02:28:34,320 --> 02:28:44,320
awareness and courage what's the awareness piece the awareness piece is um is is under first just

1667
02:28:44,320 --> 02:28:49,040
no understanding the stakes like getting our heads out of the sand and being like technology is

1668
02:28:49,040 --> 02:28:54,240
blowing up exponentially we're our society's trust is devolving like we're kind of falling

1669
02:28:54,240 --> 02:28:59,920
apart in some important ways we're losing our grip on some stability at the worst time that's the

1670
02:29:00,000 --> 02:29:04,480
first point just a big picture and then also awareness of i think this vertical axis or whatever

1671
02:29:04,480 --> 02:29:10,240
your version of it is this concept of how do i really form my beliefs where do they actually come

1672
02:29:10,240 --> 02:29:14,640
from where do you know did they are they someone else's beliefs am i following a checklist

1673
02:29:17,040 --> 02:29:22,400
how about my values it you know i used to identify with the blue party or the red party but now

1674
02:29:22,400 --> 02:29:26,480
they've changed and and i suddenly am okay with that is that because my values changed with it or

1675
02:29:26,480 --> 02:29:32,320
am i actually anchored to the party not to any principle asking yourself these questions um

1676
02:29:33,280 --> 02:29:37,840
asking you know looking for where do i feel disgusted by fellow human beings you know that

1677
02:29:37,840 --> 02:29:42,080
maybe i'm being a crazy tribal person without realizing it how about the people around me am

1678
02:29:42,080 --> 02:29:48,000
i being bullied by some echo chamber without realizing it um am i the bully somewhere right

1679
02:29:48,000 --> 02:29:55,120
so that's the first just just just i think just to kind of do a self audit and and and um and i

1680
02:29:55,120 --> 02:29:59,760
think that like just just some awareness like that it's just a self audit about these things

1681
02:29:59,760 --> 02:30:04,320
can can go a long way but if you don't if you if you keep it to yourself it's almost useless

1682
02:30:05,520 --> 02:30:10,000
because if it doesn't if you don't have without you know awareness without courage does very little

1683
02:30:10,640 --> 02:30:15,760
so courage is when you take that awareness and you actually export it out into the world and it

1684
02:30:15,760 --> 02:30:19,680
starts affecting other people and so courage can happen on multiple levels it can happen

1685
02:30:20,400 --> 02:30:24,400
by first of all just stop saying stuff you don't believe if you're being pressured by a

1686
02:30:24,400 --> 02:30:29,360
kind of a ideology or a movement to say stuff that you don't actually believe

1687
02:30:29,360 --> 02:30:32,880
just stop just just just stand your ground and don't say anything that's that's courage that's

1688
02:30:32,880 --> 02:30:40,000
one first step start speaking out in small groups starts you know actually speaking your mind see

1689
02:30:40,000 --> 02:30:44,240
what happens the sky doesn't usually fall actually people usually respect you for it like you know

1690
02:30:44,240 --> 02:30:48,000
and it's not not every group but like you'd be surprised and then eventually you know maybe

1691
02:30:48,640 --> 02:30:52,720
start speaking out in bigger groups start going public you know go go public with it but

1692
02:30:52,720 --> 02:30:56,160
and you don't need everyone doing this look some people will lose their jobs for it I'm not talking

1693
02:30:56,160 --> 02:31:01,920
to those people most people won't lose their jobs but they have the same fear as if they would right

1694
02:31:01,920 --> 02:31:06,400
and it's like what are you gonna get criticized or you can get a bunch of people you know angry

1695
02:31:06,400 --> 02:31:11,520
twitter people will will criticize you like it yeah it's not pleasant but actually that's a little

1696
02:31:11,520 --> 02:31:18,240
bit like our primitive minds fear that really back when it was programmed that kind of ostracism

1697
02:31:18,240 --> 02:31:22,480
or or criticism will get leave you out of the tribe and you'll die today it's kind of a delusional

1698
02:31:22,480 --> 02:31:26,960
fear it's not actually that scary and the people who realize that it can can exercise incredible

1699
02:31:26,960 --> 02:31:34,240
leadership right now so you have a really interesting description of censorship of self

1700
02:31:34,240 --> 02:31:42,000
censorship also as you've been talking about uh who's king mustachion uh this gap I think I hope

1701
02:31:42,000 --> 02:31:46,560
you write even more even more than you've written in the book about these ideas because it's so strong

1702
02:31:46,880 --> 02:31:53,920
this censorship gaps that are created between the dormant thought pile and uh the kind of

1703
02:31:54,480 --> 02:32:00,960
thing under the speech curve yeah so first of all so I like to think of I think it's a useful tool

1704
02:32:00,960 --> 02:32:06,640
is this thing called a thought pile which is if you have a on any given issue you have a horizontal

1705
02:32:06,640 --> 02:32:11,440
spectrum and just say I could take your brain out of your head and I put it on the thought pile

1706
02:32:11,440 --> 02:32:15,920
right where you happen to believe about that issue now I did that for everyone in the in the

1707
02:32:15,920 --> 02:32:19,840
community or in a society and you're going to end up with a big mushy pile that I think will

1708
02:32:19,840 --> 02:32:24,720
often form a bell curve if it's really politicized might form like a camel with two humps because

1709
02:32:24,720 --> 02:32:28,400
there's like concentrated here but for a typical issue it'll just form you know a fear of AI you're

1710
02:32:28,400 --> 02:32:33,200
going to have a bell curve right you know things like this that's the thought pile now the second

1711
02:32:33,200 --> 02:32:38,240
thing is a line that's I call the speech curve which is what people are saying so the speech curve

1712
02:32:38,240 --> 02:32:42,320
is high when not just a lot of people are saying it but it's being said from the biggest platforms

1713
02:32:42,320 --> 02:32:46,080
being set in the you know and then you know in the New York Times and it's being said

1714
02:32:46,800 --> 02:32:50,320
by the president on you know in the state of the you know those things are the top of the speech

1715
02:32:50,320 --> 02:32:54,880
curve now and then as you know and then when the speech curve is lower it means it's being said

1716
02:32:54,880 --> 02:32:59,120
either whispered in small groups or it's just not very many people are talking about it now a healthy

1717
02:32:59,840 --> 02:33:05,280
when a when a free speech democracy is healthy on a certain topic you've got the speech curve

1718
02:33:05,280 --> 02:33:09,040
sitting right on top of the thought pile they mirror each other which is naturally what would

1719
02:33:09,040 --> 02:33:12,880
happen more more people think something it's going to be said more often and from higher platforms

1720
02:33:14,480 --> 02:33:18,880
what censorship does and that's the censorship can be from the government I so I use the tail

1721
02:33:18,880 --> 02:33:25,360
of king mustache and king mustache he's a little tiny tyrant and he's very sensitive and people are

1722
02:33:25,360 --> 02:33:28,800
making fun of his mustache and they're saying he's not a good king and he does not like that so what

1723
02:33:28,800 --> 02:33:33,920
does he do he enacts a policy and he says anyone who has heard criticize anyone who has heard

1724
02:33:33,920 --> 02:33:40,560
criticizing me or my mustache or my rule will be put to death and immediately at the town

1725
02:33:40,560 --> 02:33:46,480
is because his father was a very liberal there was always free speech in in his kingdom but now

1726
02:33:46,480 --> 02:33:50,080
king must have just taken over and he's saying this is a new new rules now and so a few people

1727
02:33:50,080 --> 02:33:54,240
yell out and they say that's not how we do things here and that moment that's what I call a moment

1728
02:33:54,240 --> 02:33:59,920
of truth did the king's guards stand with the principles of the kingdom and say yeah king

1729
02:33:59,920 --> 02:34:03,600
mustache it's not what we do in which case he would kind of have to he's nothing he can do

1730
02:34:04,320 --> 02:34:08,800
or are they going to execute so in this case it's as if he laid down an electric fence over a

1731
02:34:08,800 --> 02:34:12,400
part of this thought pile and said no one's allowed to speak over here the speech curve

1732
02:34:12,960 --> 02:34:15,840
maybe people will think these things but the speech curve cannot go over here

1733
02:34:16,800 --> 02:34:20,080
but the electric fence wasn't actually electrified until the king's guards

1734
02:34:20,720 --> 02:34:24,480
in a moment of truth get scared and say okay and they hang the five people who spoke out

1735
02:34:24,960 --> 02:34:30,400
so in that moment that fence just became electric and now no one criticizes king

1736
02:34:30,400 --> 02:34:34,800
mustache anymore so I use this as an allegory now of course he has a hard cudgel because he can

1737
02:34:34,800 --> 02:34:39,840
execute people but now when we look at the US what you're seeing right now is a lot of pressure

1738
02:34:40,560 --> 02:34:44,640
which is very similar an electric fence is being laid down saying no one can criticize these ideas

1739
02:34:45,280 --> 02:34:49,360
and if you do you won't be executed you'll be cancelled you'll be you'll be you'll be fired

1740
02:34:50,160 --> 02:34:53,600
now what is that fence electrified from there no they don't they can't actually

1741
02:34:53,600 --> 02:34:57,920
they're not working the company they can't fire you but they can start a twitter mob when someone

1742
02:34:57,920 --> 02:35:04,240
violates that speech curve when someone violates that speech rule and then the leadership of the

1743
02:35:04,240 --> 02:35:10,960
company has the moment of truth and what the leaders should do is stand up for their company's

1744
02:35:10,960 --> 02:35:14,960
values which is almost always in favor of the employee and say look you know even if they

1745
02:35:14,960 --> 02:35:17,920
made a mistake they make people make mistakes we're not going to fire them or maybe that person

1746
02:35:17,920 --> 02:35:21,600
actually said something that's reasonable and we should discuss it but either way we're not going

1747
02:35:21,600 --> 02:35:25,840
to fire them and if they said no what happens is the the twitter mob actually doesn't have

1748
02:35:25,840 --> 02:35:29,520
they can't execute you they they go away and the fence has proven to have no electricity

1749
02:35:29,520 --> 02:35:33,040
what's been the problem with the past few years is what's happened again and again

1750
02:35:33,040 --> 02:35:36,880
is the leader gets scared and they don't they get scared in the front when they fire them

1751
02:35:36,880 --> 02:35:44,160
boom that fence has electricity and now actually if you cross that it's not just you know a threat

1752
02:35:44,160 --> 02:35:48,960
like you will have you'll be out of a job like it's really bad like you'll have a huge penalty

1753
02:35:48,960 --> 02:35:53,040
you might not be able to feed your kids so that's an electric fence that goes up now what happens

1754
02:35:53,040 --> 02:35:57,520
when an electric fence goes up and it's proven to actually be electrified the speech curve morphs

1755
02:35:57,520 --> 02:36:02,480
into a totally different position and now these new people say instead of having the kind of marketplace

1756
02:36:02,480 --> 02:36:07,280
of ideas you know that turns into a kind of a natural bell curve they say no no no these ideas

1757
02:36:07,280 --> 02:36:11,360
are okay to say not just okay you'll be socially rewarded and these ones don't that's the rules

1758
02:36:11,360 --> 02:36:15,760
of their own echo chamber that they're now applying to everyone and it's working and so the speech

1759
02:36:15,760 --> 02:36:20,720
curve distorts and so you end up with now instead of one region which is a region of kind of active

1760
02:36:20,720 --> 02:36:25,760
communal thinking what people are thinking and saying you now have three regions you have a little

1761
02:36:25,760 --> 02:36:29,600
active communal thinking but mostly you now have this dormant thought pile which is all these

1762
02:36:30,160 --> 02:36:33,360
these opinions that suddenly everyone's scared to say out loud everyone's thinking but they're scared

1763
02:36:33,360 --> 02:36:37,200
to say everyone's thinking but no one's saying and then you have this other region which is this

1764
02:36:37,840 --> 02:36:44,480
the the approved ideas of this now cultural kind of dictator and those are being spoken

1765
02:36:44,480 --> 02:36:47,840
from the largest platforms and they're being repeated by the president and they're being

1766
02:36:47,840 --> 02:36:53,600
repeated all over the place you know even though people don't believe it and that's this distortion

1767
02:36:53,600 --> 02:36:58,480
and what happens is the society becomes really stupid because active communal thinking is the

1768
02:36:58,480 --> 02:37:03,280
region where we can actually think together and now no one can think together and it gets it gets

1769
02:37:03,360 --> 02:37:07,920
siloed into small private conversations it's really powerful what you said about institutions

1770
02:37:07,920 --> 02:37:13,840
and so on it's not trivial to from a leadership position to be like no we we defend the employee

1771
02:37:13,840 --> 02:37:19,600
or we defend the um the yeah the employee the person with us on our like because we don't

1772
02:37:20,160 --> 02:37:26,560
there's because there's no actual uh ground to the any kind of violation we're hearing about

1773
02:37:26,560 --> 02:37:32,080
so the mob they resist the mob it's ultimately to the leader I guess of a particular institution

1774
02:37:32,080 --> 02:37:36,720
of a particular company and it's difficult oh yeah no no it's not I don't if it were easy

1775
02:37:36,720 --> 02:37:41,840
it wouldn't there wouldn't be all of these failings and by the way this is that's the immune system

1776
02:37:41,840 --> 02:37:46,880
failing that's the liberal immune system of that company failing but also then it's an example

1777
02:37:46,880 --> 02:37:51,440
which means that a lot of other you know it's failing to kind of to the country it's not easy

1778
02:37:51,440 --> 02:37:55,280
of course it's not because why because we have primitive minds that are wired to care so much

1779
02:37:55,280 --> 02:37:59,280
about what people think of us and even if we're not gonna you know maybe first of all we're scared

1780
02:37:59,280 --> 02:38:02,320
that it's going to start a because there's you know what you know what what do mobs do

1781
02:38:03,280 --> 02:38:07,440
they don't just say I'm going to criticize you I'm going to criticize anyone who still buys

1782
02:38:07,440 --> 02:38:12,400
your product I'm going to criticize anyone who goes on your podcast so it's not just you it's now

1783
02:38:12,400 --> 02:38:17,600
suddenly if if if if Lex becomes tarnished enough now I go on the podcast and people are saying oh

1784
02:38:17,600 --> 02:38:21,840
I'm not buying his book you went on Lex Friedman no no thanks right and now I get by there it's a

1785
02:38:21,840 --> 02:38:25,600
call I call it a smear web like you've been smeared and it's so we're in such a you know bad time

1786
02:38:25,600 --> 02:38:29,360
that it smeared travels to me and now meanwhile someone who buys my book and tries to share it

1787
02:38:29,360 --> 02:38:33,040
someone said you're buying that guy's book you know he goes on Lex Friedman you know you see how

1788
02:38:33,040 --> 02:38:37,760
this happens right so that hasn't happened in this case but that so we are so wired not and

1789
02:38:37,760 --> 02:38:42,960
hey that is kind of bad right like that is actually like bad for you but but we're wired to care about

1790
02:38:42,960 --> 02:38:49,440
it so much because it meant life or death back in the day yeah yeah and luckily in this case we're

1791
02:38:49,440 --> 02:38:54,640
both uh probably can smear each other in this conversation this is wonderful I smear you all

1792
02:38:54,640 --> 02:39:01,600
the time given given the nature of your book um what what do what do you think about freedom of

1793
02:39:01,600 --> 02:39:07,600
speech as a term and as an idea as a way to resist the mechanism this mechanism of dormant

1794
02:39:07,600 --> 02:39:12,880
thought pile artificially generated speech this ideal of the freedom of speech and protecting

1795
02:39:12,880 --> 02:39:19,200
speech and celebrating yeah well so this is this is kind of the point I was talking about earlier

1796
02:39:19,200 --> 02:39:28,400
about King Westash made a rule against for he's created officially guys just I just love the one

1797
02:39:28,400 --> 02:39:32,800
of the amazing things about your book as you get later and later in the book you cover more and more

1798
02:39:32,800 --> 02:39:37,600
difficult issues as a way to illustrate the importance of the vertical perspective but

1799
02:39:37,600 --> 02:39:46,080
there's something about using hilarious drawings throughout that make it much more fun and it

1800
02:39:46,160 --> 02:39:51,520
takes you away from the personal somehow and you start thinking in the space of ideas versus like

1801
02:39:51,520 --> 02:39:56,480
outside of the tribal type of thinking so it's a really brilliant I mean I would advise for

1802
02:39:56,480 --> 02:40:01,200
any way to do when they write controversial books to have hilarious drawings it's true like put the

1803
02:40:01,200 --> 02:40:05,040
silly stick figure in your thing and it it lightens it does it lightens the mood it gets people's

1804
02:40:05,040 --> 02:40:09,920
guard down a little yeah you know and it works it reminds people that like we're all friends here

1805
02:40:09,920 --> 02:40:14,560
right like we're you know let's like laugh you know laugh at ourselves laugh at the laugh at

1806
02:40:14,560 --> 02:40:18,160
the fact that we're like in a culture war a little bit and now we can talk about it right as opposed

1807
02:40:18,160 --> 02:40:24,000
to like getting like religious about it but but basically like King Westash had no first amendment

1808
02:40:24,000 --> 02:40:29,040
he said we the government is censoring right which is very common around the world right government

1809
02:40:29,040 --> 02:40:33,600
censor all them the US you know again there's some you can argue there's some controversial

1810
02:40:33,600 --> 02:40:39,600
things recently but basically the US the first amendment isn't the problem right no one is

1811
02:40:39,600 --> 02:40:46,800
being arrested for saying the wrong thing but this graph is still happening and so so freedom of

1812
02:40:46,800 --> 02:40:53,040
speech when people what if what people like to say is if someone's camp complaining about a

1813
02:40:53,040 --> 02:40:57,600
cancel culture and saying you know this is this is you know an anti-free speech people like to

1814
02:40:57,600 --> 02:41:01,680
point out no it's not the government's not arresting you for anything this is called like

1815
02:41:01,680 --> 02:41:06,800
you know the free market buddy like this is called you know you're putting your ideas out and you're

1816
02:41:06,800 --> 02:41:10,640
getting criticized and your precious marketplace of ideas there it is right you know I've gotten

1817
02:41:10,640 --> 02:41:16,240
this a lot and this is not making a critical distinction between cancel culture and criticism

1818
02:41:16,240 --> 02:41:24,400
culture um criticism culture is a little bit of this kind of high rung idea lab stuff we talked

1819
02:41:24,400 --> 02:41:34,400
about criticism culture attacks the idea and and and and encourages further discussion right it

1820
02:41:34,480 --> 02:41:41,360
enlivens discussion it makes everyone smarter cancel culture attacks the person very different

1821
02:41:41,360 --> 02:41:45,520
can't criticism culture says here's why this idea is so bad let me tell you cancel culture says

1822
02:41:45,520 --> 02:41:50,400
here's why this person is bad and no one should talk to them and they should be fired and what

1823
02:41:50,400 --> 02:41:54,640
does that do it doesn't enliven the discussion it makes everyone scared to talk and it's the

1824
02:41:54,640 --> 02:41:58,720
opposite it shuts down discussion so you still have your first amendment but first amendment

1825
02:41:58,720 --> 02:42:02,640
plus cancel culture equals you might as well be in king must you might as well have government

1826
02:42:02,640 --> 02:42:08,400
censorship right first amendment plus criticism culture great now you have this vibrant marketplace

1827
02:42:08,400 --> 02:42:16,240
of ideas so there's a very clear difference um and so when when people criticize the cancel culture

1828
02:42:16,240 --> 02:42:20,240
and then someone says oh see you're so sensitive now you look you're doing the cancel culture yourself

1829
02:42:20,240 --> 02:42:25,440
you're trying to punish this person for critics like no no no no no no every good liberal and I

1830
02:42:25,440 --> 02:42:29,600
and I mean that in the lower case which is that anyone who believes in liberal democracies

1831
02:42:29,600 --> 02:42:34,320
regardless of what they believe should stand up and say no to cancel culture and say this is not

1832
02:42:34,320 --> 02:42:39,760
okay regardless of what the actual topic is and that makes them a good liberal versus if they're

1833
02:42:39,760 --> 02:42:43,840
trying to cancel someone who's just criticizing they're doing the opposite now they're shutting

1834
02:42:43,840 --> 02:42:48,880
so it's the opposite things but it's very easy to get confused you can see people take advantage of

1835
02:42:48,880 --> 02:42:52,560
the and sometimes they just don't know it themselves the the the lines here can be very

1836
02:42:52,560 --> 02:42:56,880
confusing the wording can be very confusing and without that wording someone suddenly it looks

1837
02:42:56,880 --> 02:43:03,760
like someone who's criticizing cancel culture is cancelling but they're not you apply this

1838
02:43:03,760 --> 02:43:11,280
thinking to universities in particular um I've that there's a great yet another great image

1839
02:43:11,280 --> 02:43:18,080
on the trade off between knowledge and conviction and it's what's commonly actually can maybe explain

1840
02:43:18,080 --> 02:43:23,760
to me the difference but you it's often referred to as the dunning kruger effect where you uh when

1841
02:43:23,760 --> 02:43:30,160
you first learn of a thing you have an extremely um high confidence about self estimation of how

1842
02:43:30,160 --> 02:43:34,080
well you understand that thing you actually say that dunning kruger means something else

1843
02:43:34,080 --> 02:43:38,480
so yeah it's everyone I post this everyone's like dunning kruger and it's and it's what everyone

1844
02:43:38,480 --> 02:43:43,200
thinks dunning kruger isn't dunning kruger is a little different it's it's you have a diagonal line

1845
02:43:43,200 --> 02:43:48,240
like this one right which is the place you are it's the I call it like the humility tight row but

1846
02:43:48,240 --> 02:43:52,400
the humility sweet spot it's exactly the right level of humility based on what you know if you're

1847
02:43:52,400 --> 02:43:55,760
below it you're insecure you actually have too much humility you don't have enough confidence

1848
02:43:55,760 --> 02:43:59,200
because you know more than you're giving yourself credit for and when you're above the line you're

1849
02:43:59,200 --> 02:44:03,120
in the arrogant zone right you're you need a you need a dose of humility right you think you know

1850
02:44:03,120 --> 02:44:06,720
more than you do so y'all want to stay on that tight rope and dunning kruger is basically a

1851
02:44:06,720 --> 02:44:12,240
straight line that's just a has a lower slope so you start off you still are you still are getting

1852
02:44:12,240 --> 02:44:20,960
more confident as you go along but you start off above that line and as you learn more you end

1853
02:44:20,960 --> 02:44:26,320
up below the line later so but anyway so this wavy thing this wavy thing is is is in different

1854
02:44:26,320 --> 02:44:31,360
phenomenon and it's just it's just related but so this idea so for people just listening

1855
02:44:32,880 --> 02:44:38,080
there's a child's hill pretty damn sure you know a whole lot and feeling great about it that's in

1856
02:44:38,080 --> 02:44:43,840
the beginning and then there's an insecure canyon you crash down acknowledging that you don't know

1857
02:44:43,840 --> 02:44:51,280
that much and then there's a growth mountain grown up mountain grown up mountain where after you

1858
02:44:51,280 --> 02:44:56,480
feel ashamed and embarrassed about not knowing that much you begin to realize that knowing how

1859
02:44:56,480 --> 02:45:01,440
little you know is the first step in becoming someone who actually knows stuff and that's the

1860
02:45:01,440 --> 02:45:07,440
the grown-up mountain and you climb and climb and climb you're saying that in universities

1861
02:45:07,440 --> 02:45:14,320
were pinning people at the top of the child's hill so for me this is a very you know I think of

1862
02:45:14,320 --> 02:45:20,560
myself with this because I went to college like a lot of 18 year olds and I was very cocky I just

1863
02:45:20,560 --> 02:45:25,680
thought I knew a lot you know and when it came to politics I was like bright blue just because I

1864
02:45:25,680 --> 02:45:29,120
grew up in a bright blue suburb and I wasn't thinking that hard about it and I thought that

1865
02:45:29,120 --> 02:45:33,760
you know and what I did when I went to college is met a lot of smart conservatives and a lot of

1866
02:45:33,760 --> 02:45:38,320
smart progressives but I've met a lot of people who weren't just going down a checklist and they

1867
02:45:38,320 --> 02:45:44,080
knew stuff and when I and it suddenly I realized that like a lot of these views I have are not

1868
02:45:44,080 --> 02:45:49,840
based on knowledge they're based on other people's conviction everyone else thinks that's true so now

1869
02:45:49,840 --> 02:45:56,080
I think it's whoa you know I'm I'm actually like I'm I'm transferring someone else's conviction

1870
02:45:56,080 --> 02:45:58,960
to me and who knows why they have conviction they might have conviction because they're transferring

1871
02:45:58,960 --> 02:46:06,880
from someone else and I'm a smart dude I thought why why am I why am I like giving away my own

1872
02:46:06,880 --> 02:46:13,600
independent you know learning abilities here and just adopting other views so anyway it was this

1873
02:46:13,600 --> 02:46:17,360
humbling experience and it wasn't just about politics by the way it was that I had strong

1874
02:46:17,360 --> 02:46:23,520
views about a lot of stuff and I just I got lucky not or not lucky I sought out you know the kind

1875
02:46:23,520 --> 02:46:27,760
of people I sought out were the type that loved to disagree and they were man they knew stuff

1876
02:46:28,960 --> 02:46:33,520
and so you're quickly in you know in in again idea lab culture it was an idea lab and also I

1877
02:46:33,520 --> 02:46:37,280
also went to I started getting in the habit I started loving listening to people who disagreed

1878
02:46:37,280 --> 02:46:41,760
with me because it was so exhilarating listening to a smart when I thought there was no no credence

1879
02:46:41,760 --> 02:46:47,440
to this other argument right the this side of this debate is obviously wrong I wanted to see an

1880
02:46:47,440 --> 02:46:50,880
intelligence squared on that debate in particular I wanted to go see I actually got into intelligence

1881
02:46:50,880 --> 02:46:56,720
squared in college I wanted to see a smart person who disagrees with me talk it became so

1882
02:46:56,720 --> 02:47:00,480
fascinating to me right it was the most interesting thing that was a new thing I didn't think I like

1883
02:47:00,480 --> 02:47:06,560
that and so what did that do that that shoved me down the humble tumble here number three it shoved

1884
02:47:06,560 --> 02:47:09,920
me down where I started to and I and then I and then I went the other way where I realized that I

1885
02:47:09,920 --> 02:47:15,600
had been a lot of my identity had been based on this faux feeling of knowledge this idea that I

1886
02:47:15,600 --> 02:47:20,160
thought I knew everything now that I don't have that I was like I felt really like dumb and I felt

1887
02:47:20,160 --> 02:47:24,320
really almost like embarrassed of what I knew and so that's where I call this insecure canyon I

1888
02:47:24,320 --> 02:47:27,440
think it's sometimes when you're so used to thinking you know everything and then you realize

1889
02:47:27,440 --> 02:47:32,160
you don't it's like it's and then you start to realize that actually really awesome thinkers

1890
02:47:32,160 --> 02:47:36,880
they were they they don't judge me for this they totally respect if I say I don't know anything

1891
02:47:36,880 --> 02:47:39,440
about this to say oh cool you should read this and this and this they don't say you don't know

1892
02:47:39,440 --> 02:47:44,720
anything they don't say that right and so and not that I'm by the way this is not to say I'm now

1893
02:47:44,720 --> 02:47:48,880
on grown-up mountain and you should all join me I am often find myself drifting up with like a

1894
02:47:48,880 --> 02:47:54,000
helium balloon oh I think I read about the new thing and suddenly I think I have all I think I

1895
02:47:54,000 --> 02:47:58,640
you know I read three things about you know a new AI thing and I'm like I'll go do a talk on this

1896
02:47:58,640 --> 02:48:03,600
and like no I won't I don't I just I'm going to just be spouting out the opinion of the person

1897
02:48:03,600 --> 02:48:08,160
I just read so I have to remind myself but it's useful now what the reason my problem with colleges

1898
02:48:08,160 --> 02:48:17,600
today is that it's I was graduated in 2004 this is a recent change is that all of those speakers

1899
02:48:17,600 --> 02:48:22,080
I went who disagreed with me a lot of them were conservative so many of those speakers would not

1900
02:48:22,080 --> 02:48:27,680
be allowed on campuses today and so many of the discussions I had were in big groups or classrooms

1901
02:48:27,680 --> 02:48:32,560
and this is still you know this was a liberal campus so many of those disagreements

1902
02:48:34,560 --> 02:48:38,480
they're not happening today and you I've interviewed a ton of college students it's chilly

1903
02:48:38,480 --> 02:48:43,920
it is you know people keep to themselves so what's happening is not only are people losing that

1904
02:48:43,920 --> 02:48:49,200
push off the child's hill which was so valuable to me so valuable to me as a thinker it kind of

1905
02:48:49,200 --> 02:48:53,520
started my life as a better thinker they're losing that but actually would college a lot of the

1906
02:48:53,520 --> 02:48:58,480
college classes and the vibe in colleges a lot of what is now saying that there is one right set

1907
02:48:58,480 --> 02:49:04,240
of views and it's this kind of you know woke ideology and it's right and anyone who disagrees

1908
02:49:04,240 --> 02:49:09,600
with it is bad and anyone and and don't speak up you know unless you're going to agree with it

1909
02:49:09,600 --> 02:49:14,800
it's teaching people that child's hills that you know it's it's nailing people's feet to child's hill

1910
02:49:14,800 --> 02:49:19,520
it's teaching people that these are right this user right and like you don't have any you nothing

1911
02:49:19,520 --> 02:49:27,600
to you should feel a complete conviction about them yeah how do we fix it is a part of the

1912
02:49:27,600 --> 02:49:33,520
administration is a part of the culture is a part of the uh is a part like actually instilling in

1913
02:49:33,520 --> 02:49:39,600
the individual like 18 year olds the idea that this is the beautiful way to live is to embrace

1914
02:49:39,600 --> 02:49:44,240
the disagreement and the growth from that it's awareness and courage it's the same thing so

1915
02:49:44,240 --> 02:49:50,480
first of all just get when that awareness is people need to see what's happening here that

1916
02:49:50,480 --> 02:49:55,680
kids are getting losing the they're not going to college and becoming better tougher more robust

1917
02:49:55,680 --> 02:50:00,240
thinkers yeah they're actually going to college and becoming zealots they're getting taught to be

1918
02:50:00,240 --> 02:50:05,760
zealots and the at in the website still advertises you know wide variety of i've you know the website

1919
02:50:05,760 --> 02:50:09,120
is a bait and switch you listen to all the universities yeah it's a bait and switch it's

1920
02:50:09,120 --> 02:50:13,360
it's still saying here you're coming here for a wide intellectual basically they're advertising

1921
02:50:13,360 --> 02:50:16,240
this is an idea lab and you get there and it's like actually it's an echo chamber that you're

1922
02:50:16,240 --> 02:50:23,120
paying money for so if people realize that they start to get mad hopefully and then courage i mean

1923
02:50:23,120 --> 02:50:28,160
starts you know yes brave students there's been some very brave students who have started you know

1924
02:50:28,160 --> 02:50:32,960
big think clubs and stuff like that where it's like we're gonna have you know present both sides

1925
02:50:32,960 --> 02:50:40,160
of a debate here and that that takes courage but also um courage and leadership um like the it's

1926
02:50:40,160 --> 02:50:45,440
it's like if you look at these colleges it's specifically the leaders who show strength

1927
02:50:46,400 --> 02:50:51,760
who get who get the best results remember the the cudgel is soft so if a leader of one of these

1928
02:50:51,760 --> 02:50:58,160
places says you know the the college presidents who have shown some strength um they actually

1929
02:50:58,160 --> 02:51:04,560
don't get as much trouble it's the ones who pander the ones who um uh in that you know

1930
02:51:04,560 --> 02:51:10,560
in that moment of truth they they they shrink away then they get a lot more trouble the mob

1931
02:51:10,560 --> 02:51:17,840
smells blood for the listener uh the the podcast favorite live burry just entered and your friend

1932
02:51:17,840 --> 02:51:23,680
just entered the room uh do you mind if she joins us please i think there's a story she has about you

1933
02:51:24,640 --> 02:51:29,680
so live you mentioned something that there's a funny story about we haven't talked at all about

1934
02:51:29,680 --> 02:51:35,120
the actual process of writing the book is is there you guys made a bet of some kind

1935
02:51:36,080 --> 02:51:43,680
yeah is this a true story is this a completely false fabric no no it's it's true live is she's

1936
02:51:43,680 --> 02:51:49,280
mean when you i didn't i did not know mean live she's like she's like a bully she's like scary i

1937
02:51:49,280 --> 02:51:54,000
have to have that have that screenshot so live was FaceTiming me and she was like she was like

1938
02:51:54,000 --> 02:51:57,760
being intimidating i took a screenshot and i made it my phone background so every time i opened it

1939
02:51:57,760 --> 02:52:02,880
i was like ah so to give the background of this it's because if you hadn't noticed tim started

1940
02:52:02,880 --> 02:52:09,600
writing this book how many years ago six 2016 mid 2016 right as a sort of a response to like the

1941
02:52:09,600 --> 02:52:13,760
trump stuff and not not even yeah it was just supposed to be a mini post i was like oh i'm so

1942
02:52:13,760 --> 02:52:18,320
like i was like i'm looking at all these like future tech things and i feel this like uneasiness

1943
02:52:18,320 --> 02:52:22,480
like ah we're gonna like mess up all these things why there's like some cloud over our society let

1944
02:52:22,480 --> 02:52:26,640
me just write a mini post and i opened it up to wordpress to write a one day little essay

1945
02:52:27,840 --> 02:52:34,240
and things went on politics it was going to be on like this feeling i had that like this feeling i

1946
02:52:34,240 --> 02:52:41,040
had that um we were our tech was was just growing and growing and we were becoming less wise what's

1947
02:52:41,040 --> 02:52:43,840
up what's up with that and i just wanted to write like just like a little like a little

1948
02:52:43,840 --> 02:52:47,040
thousand word essay on like something i think we should pay attention to and that was the

1949
02:52:47,040 --> 02:52:52,480
beginning of this six year nightmare did you anticipate also the blog post would take a long

1950
02:52:52,480 --> 02:52:58,560
while um i don't remember the process fully in terms of i remember you saying oh i'm actually

1951
02:52:58,560 --> 02:53:02,560
writing this is it's turning into a bigger thing and i was like mmm you know and because the more

1952
02:53:02,560 --> 02:53:05,600
we talked about remember we were talking about it i was like oh this goes deep because i didn't

1953
02:53:05,600 --> 02:53:10,880
really understand the full scope of the situation like nowhere near and and you sort of explained

1954
02:53:10,880 --> 02:53:14,640
it i was like okay yeah i see that and then the more we dug into it the sort of the deeper and

1955
02:53:14,640 --> 02:53:19,280
deeper and deeper it went but no i did not anticipate it would be six years let's put it that way

1956
02:53:19,280 --> 02:53:25,600
and when was your ted talk on uh procrastination so that was that was march of 2016 and i started

1957
02:53:25,600 --> 02:53:31,200
this book three months later and fell like to the biggest procrastination hole that i've ever

1958
02:53:31,200 --> 02:53:36,960
fallen into oh wow the irony isn't lost on me i mean it's like it's i i just like i like how much

1959
02:53:36,960 --> 02:53:42,480
cred i have as as for that ted talk i'm like i am legit procrastinator that is no i'm not just

1960
02:53:42,480 --> 02:53:47,360
saying it like it wasn't just that because i mean you did you know you did intend it to start out

1961
02:53:47,360 --> 02:53:51,360
as a blog post but then you're like actually this needs to be multiple actually let's make it into

1962
02:53:51,360 --> 02:53:55,920
a full series you know what i'll turn it into a book and then that's what and and and what

1963
02:53:55,920 --> 02:54:00,640
but also what live witnessed a few times and my wife has witnessed like 30 of these is like

1964
02:54:00,640 --> 02:54:07,920
these these 180 epiphanies where i'll be like i'll like i'll have a moment when i'm and i don't

1965
02:54:07,920 --> 02:54:11,440
know what you know sometimes it's that there's a really good idea sometimes it's like i'm just

1966
02:54:11,440 --> 02:54:15,360
dreading having to finish this the way it is and so there's epiphanies where it's like you know what

1967
02:54:15,360 --> 02:54:20,800
i need to start over from the beginning and just make this like a short like 20 little blog post

1968
02:54:20,800 --> 02:54:24,800
list and then i'll do that and then i was like no no i have like a new epiphany i have to and it's

1969
02:54:24,800 --> 02:54:29,600
these and and yeah it's kind of like the crazy person a little bit but anyway can i tell the

1970
02:54:29,600 --> 02:54:36,560
story of the the bed all right so things came to a head when we were in we were all in vacation

1971
02:54:36,560 --> 02:54:43,840
on dominican republic uh tim and his wife me and ego and we were in the ocean and i remember you'd

1972
02:54:43,840 --> 02:54:49,200
been in the ocean for like an hour just bobbing in there becoming it and we got talking and and

1973
02:54:49,200 --> 02:54:56,240
we were talking about the book and you know you were expressing just like this you know just the

1974
02:54:56,240 --> 02:55:01,200
the horror of the of the situation basically you're like look i just i'm so close but there's still

1975
02:55:01,200 --> 02:55:07,120
this and then there's this and um an idea popped into my head which is the you know poker players

1976
02:55:07,120 --> 02:55:14,240
often uh we we will set ourselves like negative bets you know like uh essentially if we don't

1977
02:55:14,240 --> 02:55:18,800
get a job done then we have to do something we really don't want to do so instead of having

1978
02:55:18,800 --> 02:55:23,280
a carrot like a really really big stick um so i had the idea to ask tim okay

1979
02:55:26,000 --> 02:55:33,040
what is the worst either organization or individual uh that you if you had to you know

1980
02:55:33,120 --> 02:55:37,760
that you would loathe to give a large sum of money to and he thought about it for a little while

1981
02:55:37,760 --> 02:55:44,240
and he gave his answer and i was like all right what's your net worth he said his net worth all

1982
02:55:44,240 --> 02:55:49,280
right ten percent of your net worth to that thing if you don't get the draft because oh sorry but

1983
02:55:49,280 --> 02:55:54,160
just before that i'd asked him how long like if you had a gun to your head or to your wife's head

1984
02:55:54,960 --> 02:55:58,960
and you had to get the booked into a state where you could like send off an edit to the

1985
02:55:58,960 --> 02:56:04,080
or to a draft to your editor how long he's like oh i guess like i could get it like 95%

1986
02:56:04,080 --> 02:56:09,920
good in a month i was like okay great in one month's time if you do not have that edit handed

1987
02:56:09,920 --> 02:56:14,400
in you have scared by the draft hand it's really scary really scary ten percent of your net worth

1988
02:56:14,400 --> 02:56:18,720
is going to this thing that you really really think is terrible but you're forgetting the kicker

1989
02:56:20,080 --> 02:56:26,400
the kicker was that because you know procrastinators they self-defeat that's what they do and then

1990
02:56:26,400 --> 02:56:36,800
Liv says i'm gonna sweeten the deal and i am going to basically match you and i'm going to put in

1991
02:56:36,800 --> 02:56:42,480
i'm gonna send this like a huge amount of my own money there if you don't do it so and i can't

1992
02:56:42,480 --> 02:56:46,560
that's that would be really bad so not only you screwing yourself you're screwing a friend

1993
02:56:46,560 --> 02:56:52,640
and she and she was like and and as your friend because i'm your friend i will send it i will

1994
02:56:52,640 --> 02:57:03,520
send the money i mean like that you know like tyranny um and um i got the draft in yeah i got

1995
02:57:03,520 --> 02:57:08,560
the draft in just i know i well i was ego could attest to this like actually it was it was funny

1996
02:57:08,560 --> 02:57:11,520
because it was it was like supposed to be by the summer solstice or whatever it was it was like a

1997
02:57:11,520 --> 02:57:18,480
certain date and it was like four i got it at four i got no i got it at four a.m like the next morning

1998
02:57:18,480 --> 02:57:22,960
but then and and and and they were both like that doesn't count i'm like it does it's still for me

1999
02:57:22,960 --> 02:57:27,120
it's the same day still it's okay do you imagine how fucked in the head you have to be yeah so like

2000
02:57:27,120 --> 02:57:33,120
literally technically pass the deadline by four hours i for an obscene amount of money to a thing

2001
02:57:33,120 --> 02:57:38,880
you loathe that's how bad the his his sickness because i knew the hard hard deadline i knew that

2002
02:57:38,880 --> 02:57:42,160
there was no way she was going to actually send that money at because it was four a.m so i knew

2003
02:57:42,160 --> 02:57:46,160
i actually had the whole night so yeah i should actually punish you and just i should send i should

2004
02:57:46,160 --> 02:57:55,040
send like a nominal amount to that thing no thanks no but is there some micro like lessons from that

2005
02:57:55,040 --> 02:57:59,280
from how to avoid procrastination writing a book that you've learned yes well i've learned a lot of

2006
02:57:59,280 --> 02:58:03,600
things i mean like first don't take don't write like a dissertation about like proving some grand

2007
02:58:03,600 --> 02:58:10,080
theory of society because that's really procrastinating like i would have been an awful phd

2008
02:58:10,080 --> 02:58:13,520
student for that reason some and so like i'm gonna do another book and it's going to be like a bunch

2009
02:58:13,520 --> 02:58:18,080
of short chapters that are one-offs because that's like it just doesn't feed into your book is like

2010
02:58:18,080 --> 02:58:22,240
a giant like framework there is grand theories all through your book i know and i learned not to do

2011
02:58:22,240 --> 02:58:26,960
that again i did it once i don't want to do it again oh with the book yeah yeah i i learned the book

2012
02:58:26,960 --> 02:58:31,920
is a giant mistake yes don't do another one of this looks look some people should it's just not for me

2013
02:58:31,920 --> 02:58:37,120
i i you just did it i know and it and it almost killed me okay so that's the first one but secondly

2014
02:58:37,120 --> 02:58:42,320
yeah like basically there's two ways to fix procrastination one is you fix it's like a picture

2015
02:58:42,320 --> 02:58:46,160
you have a boat that's leaking and it's not working very well you can fix it in two ways you can get

2016
02:58:46,160 --> 02:58:52,000
your hammer and nails out and your boards and actually fix the boat or you can duct tape it

2017
02:58:52,000 --> 02:58:58,480
for now to get yourself across the river but it's not actually fixed so ideally down the road i have

2018
02:58:58,480 --> 02:59:03,920
repaired whatever kind of bizarre mental illness that i have that makes me procrastinate in a very

2019
02:59:03,920 --> 02:59:10,000
like i just don't self-defeat in this way anymore but in the meantime i can duct tape the boat by

2020
02:59:10,000 --> 02:59:15,120
bringing what i call the panic monster into the situation via things like this and this scary person

2021
02:59:15,120 --> 02:59:21,600
and having external pressure to have external pressure of some kind is critical for me uh it's

2022
02:59:21,600 --> 02:59:26,160
it's yes i don't have the muscle to do the work i need to do without external pressure by the way

2023
02:59:26,160 --> 02:59:32,960
i live is there a possible future where you write a book uh and meanwhile by the way huge

2024
02:59:32,960 --> 02:59:37,200
procrastinator that's the funny thing about this yeah no i'm yeah i mean i'm how long did you last

2025
02:59:37,280 --> 02:59:42,400
video take oh my god is there advice do you give to live how to get the videos done faster

2026
02:59:42,400 --> 02:59:45,760
well it would be the same exact thing i mean actually i can give good procrastination advice

2027
02:59:45,760 --> 02:59:50,320
panic monster um yeah well we should do it together it should be like we have this date but

2028
02:59:50,320 --> 02:59:54,960
right you know it's it's um we should actually just do another bet i have to have my script

2029
02:59:54,960 --> 02:59:59,200
done by this time yes if i go to get the third part out because then you'll actually do it um

2030
02:59:59,200 --> 03:00:04,880
and um and and it's not the thing is the time in but it's like if you if you could take three weeks

2031
03:00:04,880 --> 03:00:10,320
on a video and instead you take 10 weeks it's not like oh well i also i'm having more fun in

2032
03:00:10,320 --> 03:00:14,720
those 10 weeks you're the whole 10 weeks are bad yeah it's torture bad so you're just you're just

2033
03:00:14,720 --> 03:00:18,080
having a bad time and you're getting less work done and less work out and it's not like you're

2034
03:00:18,080 --> 03:00:22,480
enjoying your personal life it's bad for you for your relationships it's bad for your your own but

2035
03:00:22,480 --> 03:00:28,560
you keep doing it anyway yeah well a lot of people a lot of people uh have troubles keeping a diet

2036
03:00:28,560 --> 03:00:34,880
right yeah primitive mind why'd you point at me that was offensive what's your procrastination

2037
03:00:34,880 --> 03:00:40,560
weakness do you have everything everything everything everything it's everything preparing

2038
03:00:40,560 --> 03:00:45,600
for a conversation i had your book amazing book i really enjoyed it i started reading it

2039
03:00:46,480 --> 03:00:52,320
i was like this is awesome it's so awesome that i'm going to save it when i'm behind a computer

2040
03:00:52,320 --> 03:00:57,520
and can take notes like good notes of course that resulted in like last minute everything

2041
03:00:57,520 --> 03:01:01,920
everything everything i'm doing in my life not everyone's like that you know people

2042
03:01:01,920 --> 03:01:05,600
self-defeat in different ways some people don't have this particular problem adam grant is a he

2043
03:01:05,600 --> 03:01:11,360
calls himself a pre-crastinator where he gets an assignment he will go home and do it until it's

2044
03:01:11,360 --> 03:01:14,960
done and handed it which is also not necessarily good you know it's like you you're rushing it

2045
03:01:14,960 --> 03:01:21,200
either way but it's better but some people have the opposite thing um where they will um that the

2046
03:01:21,200 --> 03:01:26,080
the the the the looming deadline makes them so anxious that they go and fix it right and the

2047
03:01:26,080 --> 03:01:30,160
the procrastinator i think has a similar anxiety but it they they solve it in a totally

2048
03:01:30,160 --> 03:01:33,520
different way or they don't solve it they just live with the anxiety right right they just live

2049
03:01:33,520 --> 03:01:38,480
with the exact now i think there's a even bigger group of people so there's these people that adam

2050
03:01:38,480 --> 03:01:42,320
grants there's people like me and then there's people who have a healthy relationship with deadlines

2051
03:01:42,320 --> 03:01:50,960
but they're still part of a bigger group of people that actually they they um they need a deadline

2052
03:01:51,680 --> 03:01:57,200
there to do something so they actually they they still are motivated by a deadline and as

2053
03:01:57,200 --> 03:02:00,800
soon as you have all the things in life that don't have a deadline like working out and like

2054
03:02:00,800 --> 03:02:04,400
working on that album you want it to write they don't do anything either so there's actually like

2055
03:02:04,400 --> 03:02:07,680
that's why procrastination is a much bigger problem than people realize because it's not

2056
03:02:07,680 --> 03:02:12,880
just the funny last second people it's anyone who um actually can't get things done that don't have a

2057
03:02:12,880 --> 03:02:20,160
deadline you dedicate your book quote to tannis who never planned on being married to someone who

2058
03:02:20,160 --> 03:02:25,440
would spend six years talking about his book on politics but here we are uh what's the secret

2059
03:02:25,440 --> 03:02:31,360
to a successful relationship with a procrastinator that's maybe for both of you um well i think

2060
03:02:32,000 --> 03:02:36,560
the the first most important thing you already started with a political answer i can tell okay

2061
03:02:36,560 --> 03:02:41,760
go ahead no the first and most important thing is because people who don't procrastinate if you

2062
03:02:41,760 --> 03:02:48,080
know it's like you will they people and the instinct is to judge it as like uh that's either

2063
03:02:48,080 --> 03:02:51,600
either just think think they're just being like a loser they're taking it they'll take it personally

2064
03:02:51,600 --> 03:02:58,960
you know uh and instead to see this is like this is this is a uh of some form of addiction or some

2065
03:02:58,960 --> 03:03:05,200
form of um ailment you know they're not just being a dick right like they have a problem and so so

2066
03:03:05,200 --> 03:03:11,200
some compassion but then also maybe finding that line where you can you know maybe apply some tough

2067
03:03:11,200 --> 03:03:14,960
love some middle ground on the other hand you might say that you know you don't want the the

2068
03:03:14,960 --> 03:03:19,040
significant other relationship where it's like they're into one nagging you maybe that's you don't

2069
03:03:19,040 --> 03:03:22,880
want them even being part of that and i think it maybe it's you know better to have a live do it

2070
03:03:22,880 --> 03:03:27,920
instead right having someone who can like create the infrastructure where they aren't the direct

2071
03:03:27,920 --> 03:03:31,840
stick you need a bit of carrot and stick right maybe they can be the person who keeps reminding

2072
03:03:31,840 --> 03:03:36,800
in them of the carrot and then they set up the friend group to be the stick and then that keeps

2073
03:03:36,800 --> 03:03:42,880
your relationship yeah in a good place stick like looming in the background that's your friend group

2074
03:03:43,440 --> 03:03:47,520
okay at the beginning of the conversation we talked about how all of human history

2075
03:03:47,520 --> 03:03:55,440
can be presented as a a thousand page book uh what are you excited about for the one thousandth

2076
03:03:56,960 --> 03:04:03,280
probably say that first page uh so the next 250 years what are you what are you what are you

2077
03:04:03,280 --> 03:04:11,840
most excited about i'm most excited about um have you read the fable of the dragon okay well

2078
03:04:11,840 --> 03:04:17,680
it's an allegory for death and it's you know nick bostrom and he talks about the he compares

2079
03:04:17,680 --> 03:04:23,280
death to a dragon that eats 60 million people or whatever the number is every year and you just

2080
03:04:23,280 --> 03:04:27,280
every year we shepherd those people up and they feed them to the dragon and that there's a stock

2081
03:04:27,280 --> 03:04:31,600
home syndrome when we say that's just a lot of man and that's what we have to do and anyone who

2082
03:04:31,600 --> 03:04:36,800
says maybe we should try to beat the dragon they get called vain and narcissistic um but someone who

2083
03:04:37,680 --> 03:04:41,440
tries to someone who goes does chemo no one calls them vain or narcissistic they say they're

2084
03:04:41,440 --> 03:04:44,880
they're you know good good for you right you're a hero you're you're you're fighting fighting the

2085
03:04:44,880 --> 03:04:49,760
good fight so i think there's some disconnect here and i think that if we can get out of that

2086
03:04:49,760 --> 03:04:55,040
stock home syndrome and realize that death is just the machine the human physical machine

2087
03:04:55,680 --> 03:05:01,840
failing and that there's no law of nature that says you can't with enough technology

2088
03:05:04,160 --> 03:05:07,840
repair the machine and keep it going until no one i don't think anyone wants to live forever

2089
03:05:07,840 --> 03:05:13,680
people think they do no one does but until people are ready and i think when we hit a world where we

2090
03:05:13,680 --> 03:05:19,040
can we we have enough tech that we can continue to keep the human machine alive until the person

2091
03:05:19,040 --> 03:05:23,600
says i'm i'm done i'm ready i think we will look back and we will think that anything before that

2092
03:05:23,600 --> 03:05:29,200
time that'll be the real ad bc you know we'll look back at bc before the big advancement and it'll

2093
03:05:29,200 --> 03:05:35,120
seem so sad and so heartbreaking barbaric and people say i can't believe that humans like us

2094
03:05:35,120 --> 03:05:39,200
had to live with that when they lost loved ones and they they died before they were ready

2095
03:05:39,200 --> 03:05:45,680
i think that's the ultimate achievement but we need to stop criticizing and smearing people

2096
03:05:46,400 --> 03:05:50,800
who you talk about it so you think where that's actually doable in the next 250 years

2097
03:05:52,880 --> 03:05:57,280
a lot happens in 250 years especially when technology is really exponentially yeah

2098
03:05:58,160 --> 03:06:03,920
and you think humans would be around versus ai completely takes over where i mean i mean

2099
03:06:03,920 --> 03:06:10,000
look the optimist in me and maybe the stupid kind of 2023 person in me says yeah of course we'll

2100
03:06:10,000 --> 03:06:16,560
make it we'll we'll figure it out but you know i mean we are going into a great you know i have

2101
03:06:16,560 --> 03:06:21,280
a friend who knows as much about the future as anyone i know i mean he's really he's a big investor

2102
03:06:21,280 --> 03:06:25,360
and you know future tech and he um he's really on the pulse of things and he just says the future

2103
03:06:25,360 --> 03:06:28,480
is going to be weird that's what he says future is going to be weird and it's it's going to be

2104
03:06:28,480 --> 03:06:32,240
weird don't look at your the last few decades of your life and apply that forward and say that's

2105
03:06:32,240 --> 03:06:36,400
just what life is like no no no it's going to be weird and different well some of my favorite

2106
03:06:36,400 --> 03:06:42,160
things in this world are weird and speaking of which it's good to have this conversation it's good

2107
03:06:42,160 --> 03:06:46,960
to have you as friends this was an incredible one thanks for coming back and live thanks for talking

2108
03:06:46,960 --> 03:06:51,920
with me a bunch more times this was awesome thank you lex thank you thanks for listening to this

2109
03:06:51,920 --> 03:06:56,160
conversation with tim urban to support this podcast please check out our sponsors in the

2110
03:06:56,160 --> 03:07:01,920
description and now let me leave you with some words from Winston Churchill when there's no enemy

2111
03:07:01,920 --> 03:07:17,840
within the enemies outside cannot hurt you thank you for listening and hope to see you next time

