1
00:00:00,000 --> 00:00:02,720
The following is a conversation with Gary Marcus.

2
00:00:02,720 --> 00:00:08,160
He's a professor emeritus at NYU, founder of robust AI and geometric intelligence.

3
00:00:08,160 --> 00:00:12,800
The latter is a machine learning company that was acquired by Uber in 2016.

4
00:00:13,440 --> 00:00:18,160
He's the author of several books on natural and artificial intelligence,

5
00:00:18,160 --> 00:00:22,560
including his new book, Rebooting AI, Building Machines We Can Trust.

6
00:00:23,440 --> 00:00:28,800
Gary has been a critical voice highlighting the limits of deep learning and AI in general

7
00:00:28,800 --> 00:00:35,680
and discussing the challenges before our AI community that must be solved in order to achieve

8
00:00:35,680 --> 00:00:41,360
artificial general intelligence. As I'm having these conversations, I try to find paths toward

9
00:00:41,360 --> 00:00:46,640
insight towards new ideas. I try to have no ego in the process. It gets in the way.

10
00:00:47,520 --> 00:00:53,760
I'll often continuously try on several hats, several roles. One, for example, is the role of

11
00:00:53,760 --> 00:00:59,520
a three-year-old who understands very little about anything and asks big what and why questions.

12
00:01:00,240 --> 00:01:04,800
The other might be a role of a devil's advocate who presents counter-ideas with the goal of

13
00:01:04,800 --> 00:01:11,120
arriving at greater understanding through debate. Hopefully, both are useful, interesting,

14
00:01:11,120 --> 00:01:16,880
and even entertaining at times. I ask for your patience as I learn to have better conversations.

15
00:01:17,760 --> 00:01:22,960
This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,

16
00:01:22,960 --> 00:01:28,480
give it five stars on iTunes, support it on Patreon, or simply connect with me on Twitter

17
00:01:28,480 --> 00:01:36,000
at Lex Freedman, spelled F-R-I-D-M-A-N. And now, here's my conversation with Gary Marcus.

18
00:01:37,200 --> 00:01:42,960
Do you think human civilization will one day have to face an AI-driven technological singularity

19
00:01:42,960 --> 00:01:49,040
that will, in a societal way, modify our place in the food chain of intelligent living beings

20
00:01:49,040 --> 00:01:53,760
on this planet? I think our place in the food chain has already changed.

21
00:01:54,720 --> 00:01:58,560
So there are lots of things people used to do by hand that they do with machine.

22
00:01:59,120 --> 00:02:03,120
If you think of a singularity as one single moment, which is, I guess, what it suggests,

23
00:02:03,120 --> 00:02:07,280
I don't know if it'll be like that. But I think that there's a lot of gradual change,

24
00:02:07,280 --> 00:02:11,600
and AI is getting better and better. I'm here to tell you why I think it's not nearly as good

25
00:02:11,600 --> 00:02:17,280
as people think. But the overall trend is clear. Maybe Ray Kurzweil thinks it's an exponential,

26
00:02:17,280 --> 00:02:21,360
and I think it's linear. In some cases, it's close to zero right now, but it's all going to

27
00:02:21,360 --> 00:02:27,760
happen. I mean, we are going to get to human-level intelligence or whatever you want. Artificial

28
00:02:27,760 --> 00:02:32,160
general intelligence at some point. And that's certainly going to change our place in the food

29
00:02:32,160 --> 00:02:36,160
chain because a lot of the tedious things that we do now, we're going to have machines do,

30
00:02:36,160 --> 00:02:40,640
and a lot of the dangerous things that we do now, we're going to have machines do. I think our whole

31
00:02:40,640 --> 00:02:45,760
lives are going to change from people finding their meaning through their work, through people

32
00:02:45,760 --> 00:02:53,200
finding their meaning through creative expression. So the singularity will be a very gradual,

33
00:02:53,920 --> 00:02:59,200
in fact, removing the meaning of the word singularity. It'll be a very gradual transformation

34
00:02:59,200 --> 00:03:04,720
in your view. I think that it'll be somewhere in between, and I guess it depends what you mean

35
00:03:04,720 --> 00:03:08,720
by gradual and sudden. I don't think it's going to be one day. I think it's important to realize

36
00:03:08,720 --> 00:03:14,720
that intelligence is a multi-dimensional variable. So people write this stuff as if

37
00:03:15,760 --> 00:03:22,880
IQ was one number, and the day that you hit 262 or whatever, you displace the human beings.

38
00:03:22,880 --> 00:03:26,640
And really, there's lots of facets to intelligence. So there's verbal intelligence,

39
00:03:26,640 --> 00:03:31,440
and there's motor intelligence, and there's mathematical intelligence, and so forth.

40
00:03:32,080 --> 00:03:37,440
Machines, in their mathematical intelligence, far exceed most people already in their ability

41
00:03:37,440 --> 00:03:41,680
to play games. They far exceed most people already. In their ability to understand language,

42
00:03:41,680 --> 00:03:46,240
they lag behind my five-year-old, far behind my five-year-old. So there are some facets of

43
00:03:46,240 --> 00:03:51,360
intelligence, the machines of graphs, and some that they haven't. And we have a lot of work left

44
00:03:51,360 --> 00:03:57,680
to do to get them to, say, understand natural language or to understand how to flexibly approach

45
00:03:57,680 --> 00:04:04,320
some kind of novel MacGyver problem-solving kind of situation. And I don't know that all of these

46
00:04:04,320 --> 00:04:09,280
things will come once. I think there are certain vital prerequisites that we're missing now.

47
00:04:09,840 --> 00:04:13,360
For example, machines don't really have common sense now. So they don't

48
00:04:13,360 --> 00:04:18,000
understand that bottles contain water, and that people drink water to quench their thirst,

49
00:04:18,000 --> 00:04:22,080
and that they don't want to dehydrate. They don't know these basic facts about human beings. And

50
00:04:22,080 --> 00:04:27,040
I think that that's a great limiting step for many things. It's a great limiting step for reading,

51
00:04:27,040 --> 00:04:31,440
for example, because stories depend on things like, oh my god, that person's running out of water.

52
00:04:31,440 --> 00:04:36,400
That's why they did this thing. Or if they only had water, they could put out the fire.

53
00:04:36,960 --> 00:04:43,360
So you watch a movie, and you're knowledge about how things work matter. And so a computer can't

54
00:04:43,360 --> 00:04:46,560
understand that movie if it doesn't have that background knowledge. Same thing if you read a

55
00:04:46,560 --> 00:04:53,200
book. And so there are lots of places where if we had a good machine interpretable set of common

56
00:04:53,200 --> 00:04:59,280
sense, many things would accelerate relatively quickly. But I don't think even that is a single

57
00:04:59,280 --> 00:05:05,120
point. There's many different aspects of knowledge. And we might, for example, find that we make a lot

58
00:05:05,120 --> 00:05:10,400
of progress on physical reasoning, getting machines to understand, for example, how keys fit into

59
00:05:10,400 --> 00:05:18,320
locks or that kind of stuff, or how this gadget here works, and so forth. And so machines might do

60
00:05:18,320 --> 00:05:23,920
that long before they do really good psychological reasoning, because it's easier to get labeled

61
00:05:23,920 --> 00:05:30,160
data or to do direct experimentation on a microphone stand than it is to do direct

62
00:05:30,160 --> 00:05:34,480
experimentation on human beings to understand the levers that guide them.

63
00:05:34,720 --> 00:05:39,600
That's a really interesting point, actually, whether it's easier to gain common sense knowledge

64
00:05:39,600 --> 00:05:44,240
or psychological knowledge. I would say that common sense knowledge includes both physical

65
00:05:44,240 --> 00:05:48,080
knowledge and psychological knowledge. And the argument I was making.

66
00:05:48,080 --> 00:05:51,520
Physical versus psychological. Yeah, physical versus psychological. And the argument I was

67
00:05:51,520 --> 00:05:55,200
making is physical knowledge might be more accessible, because you could have a robot,

68
00:05:55,200 --> 00:06:00,240
for example, lift a bottle, try putting a bottle cap on it, see that it falls off if it does this,

69
00:06:00,320 --> 00:06:03,760
and see that it could turn it upside down. And so the robot could do some experimentation.

70
00:06:04,640 --> 00:06:10,640
We do some of our psychological reasoning by looking at our own minds. So I can sort of guess

71
00:06:10,640 --> 00:06:14,640
how you might react to something based on how I think I would react to it. And robots don't have

72
00:06:14,640 --> 00:06:19,680
that intuition. And they also can't do experiments on people in the same way, or we'll probably

73
00:06:19,680 --> 00:06:26,880
shut them down. So if we wanted to have robots figure out how I respond to pain by pinching

74
00:06:26,880 --> 00:06:30,880
me in different ways, that's probably not going to make it past the human subjects board,

75
00:06:30,880 --> 00:06:35,200
and companies are going to get sued or whatever. So there's certain kinds of practical

76
00:06:35,200 --> 00:06:41,040
experience that are limited or off limits to robots. That's a really interesting point.

77
00:06:41,040 --> 00:06:50,480
What is more difficult to gain a grounding in? Because to play devil's advocate, I would say

78
00:06:50,480 --> 00:06:58,320
that human behavior is easier expressed in data and digital form. And so when you look at Facebook

79
00:06:58,320 --> 00:07:03,920
algorithms, they get to observe human behavior. So you get to study and manipulate even a human

80
00:07:03,920 --> 00:07:10,480
behavior in a way that you perhaps cannot study or manipulate the physical world. So it's true

81
00:07:10,480 --> 00:07:17,120
why you said pain is like physical pain. But that's again the physical world. Emotional pain

82
00:07:17,120 --> 00:07:23,360
might be much easier to experiment with, perhaps unethical, but nevertheless, some would argue

83
00:07:23,360 --> 00:07:30,800
it's already going on. I think that you're right, for example, that Facebook does a lot of experimentation

84
00:07:30,800 --> 00:07:38,320
in psychological reasoning. In fact, Zuckerberg talked about AI at a talk that he gave NIPPS.

85
00:07:38,320 --> 00:07:42,160
I wasn't there, but the conference has been renamed NeurOps, but he used to be called NIPPS when he

86
00:07:42,160 --> 00:07:46,720
gave the talk. And he talked about Facebook basically having a gigantic theory of mind.

87
00:07:47,840 --> 00:07:52,240
I think it is certainly possible. Facebook does some of that. I think they have a really good

88
00:07:52,240 --> 00:07:56,320
idea of how to addict people to things. They understand what draws people back to things.

89
00:07:56,320 --> 00:07:59,840
And I think they exploit it in ways that I'm not very comfortable with. But even so,

90
00:08:00,800 --> 00:08:06,160
I think that there are only some slices of human experience that they can access through

91
00:08:06,160 --> 00:08:09,120
the kind of interface they have. And of course, they're doing all kinds of VR stuff. And maybe

92
00:08:09,120 --> 00:08:14,000
that'll change and they'll expand their data. And I'm sure that that's part of their goal.

93
00:08:14,720 --> 00:08:23,200
So, it is an interesting question. I think love, fear, insecurity, all of the things that

94
00:08:24,160 --> 00:08:28,800
I would say some of the deepest things about human nature and the human mind could be

95
00:08:28,800 --> 00:08:33,520
exported to digital form. You're actually the first person just now that brought up.

96
00:08:33,520 --> 00:08:41,120
I wonder what is more difficult because I think folks who are the slow and we'll talk a lot about

97
00:08:41,120 --> 00:08:45,920
deep learning, but the people who are thinking beyond deep learning are thinking about the physical

98
00:08:45,920 --> 00:08:51,680
world. You're starting to think about robotics in the home robotics. How do we make robots manipulate

99
00:08:51,680 --> 00:08:55,840
objects which requires an understanding of the physical world and then requires common sense

100
00:08:55,840 --> 00:09:00,800
reasoning? And that has felt to be like the next step for common sense reasoning. But

101
00:09:00,800 --> 00:09:04,640
you've not brought up the idea that there's also the emotional part. And it's interesting

102
00:09:04,640 --> 00:09:10,000
whether that's hard or easy. I think some parts of it are and some aren't. So, my company that I

103
00:09:10,000 --> 00:09:17,040
recently founded with Brod Brooks from MIT for many years and so forth, we're interested in both.

104
00:09:17,040 --> 00:09:21,120
We're interested in physical reasoning and psychological reasoning, among many other things.

105
00:09:23,360 --> 00:09:27,680
There are pieces of each of these that are accessible. So, if you want a robot to figure

106
00:09:27,680 --> 00:09:32,320
out whether it can fit under a table, that's a relatively accessible piece of physical reasoning.

107
00:09:33,440 --> 00:09:36,880
If you know the height of the table and you know the height of the robot, it's not that hard.

108
00:09:36,880 --> 00:09:41,360
If you wanted to do physical reasoning about Jenga, it gets a little bit more complicated.

109
00:09:41,360 --> 00:09:46,800
And you have to have higher resolution data in order to do it. With psychological reasoning,

110
00:09:46,800 --> 00:09:51,280
it's not that hard to know, for example, that people have goals and they like to act on those

111
00:09:51,280 --> 00:09:56,960
goals. But it's really hard to know exactly what those goals are. But ideas of frustration. I mean,

112
00:09:56,960 --> 00:10:01,360
you could argue it's extremely difficult to understand the sources of human frustration

113
00:10:01,360 --> 00:10:07,200
as they're playing Jenga with you or not. Yeah, I mean, that is very accessible.

114
00:10:07,920 --> 00:10:12,960
There's some things that are going to be obvious and some not. So, I don't think anybody really

115
00:10:12,960 --> 00:10:18,960
can do this well yet, but I think it's not inconceivable to imagine machines in the not so

116
00:10:18,960 --> 00:10:24,960
distant future being able to understand that if people lose in a game that they don't like that.

117
00:10:25,600 --> 00:10:29,920
Right. You know, that's not such a hard thing to program and it's pretty consistent across people.

118
00:10:29,920 --> 00:10:34,480
Most people don't enjoy losing and so, you know, that makes it relatively easy to code.

119
00:10:34,480 --> 00:10:38,320
On the other hand, if you wanted to capture everything about frustration, well, people

120
00:10:38,320 --> 00:10:42,160
get frustrated for a lot of different reasons. They might get sexually frustrated,

121
00:10:42,160 --> 00:10:46,320
they might get frustrated, they can get their promotion at work, all kinds of different things.

122
00:10:47,040 --> 00:10:51,520
And the more you expand the scope, the harder it is for anything like the existing techniques to

123
00:10:51,520 --> 00:10:57,280
really do that. So, I'm talking to Gary Kasparov next week and he seemed pretty frustrated with

124
00:10:57,280 --> 00:11:01,200
this game against Deep Blue. So, yeah, well, I'm frustrated with my game against him last year

125
00:11:01,200 --> 00:11:05,920
because I played him. I had two excuses. I'll give you my excuses up front. It won't mitigate the

126
00:11:05,920 --> 00:11:12,240
outcome. I was jet lagged and I hadn't played in 25 or 30 years, but the outcome is he completely

127
00:11:12,240 --> 00:11:18,960
destroyed me and it wasn't even close. Have you ever been beaten in any board game by a machine?

128
00:11:19,600 --> 00:11:27,120
I have. I actually played the predecessor to Deep Blue. Deep thought, I believe it was

129
00:11:27,120 --> 00:11:35,200
called and that too crushed me. And after that, you realized it's over for us.

130
00:11:35,200 --> 00:11:38,000
Well, there's no point in my playing Deep Blue. I mean, it's a waste of Deep Blue's

131
00:11:39,120 --> 00:11:43,040
computation. I mean, I played Kasparov because we both gave lectures this

132
00:11:44,080 --> 00:11:48,080
same event and he was playing 30 people. I forgot to mention that not only did he crush me, but he

133
00:11:48,080 --> 00:11:54,800
crushed 29 other people at the same time. I mean, but the actual philosophical and emotional

134
00:11:54,800 --> 00:12:00,880
experience of being beaten by a machine, I imagine, is to you who thinks about these things

135
00:12:01,440 --> 00:12:07,600
may be a profound experience or no, it was a simple mathematical experience.

136
00:12:07,600 --> 00:12:12,560
Yeah, I think a game like chess, particularly where it's, you know, you have perfect information,

137
00:12:12,560 --> 00:12:17,600
it's two player closed end and there's more computation for the computer. It's no surprise

138
00:12:17,600 --> 00:12:24,720
the machine. I mean, I'm not sad when a computer calculates a cube root faster

139
00:12:24,800 --> 00:12:28,720
than me. Like, I know I can't win that game. I'm not going to try.

140
00:12:28,720 --> 00:12:34,400
Well, with a system like AlphaGo or AlphaZero, do you see a little bit more magic in a system

141
00:12:34,400 --> 00:12:39,040
like that, even though it's simply playing a board game, but because there's a strong learning

142
00:12:39,040 --> 00:12:42,880
component? You know, I find you should mention that in the context of this conversation because

143
00:12:42,880 --> 00:12:47,040
Kasparov and I are working on an article that's going to be called AI is not magic.

144
00:12:48,160 --> 00:12:51,440
And you know, neither one of us thinks that it's magic. And part of the point of this

145
00:12:51,440 --> 00:12:56,000
article is that AI is actually a grab bag of different techniques and some of them have,

146
00:12:56,000 --> 00:13:02,720
or they each have their own unique strengths and weaknesses. So, you know, you read media accounts

147
00:13:02,720 --> 00:13:08,400
and it's like, ooh, AI, it must be magical or can solve any problem. Well, no, some problems

148
00:13:08,400 --> 00:13:13,280
are really accessible like chess and go and other problems like reading are completely outside the

149
00:13:14,000 --> 00:13:18,400
current technology. And it's not like you can take the technology that drives AlphaGo

150
00:13:18,400 --> 00:13:23,360
and apply it to reading and get anywhere. You know, DeepMind has tried that a bit. They have

151
00:13:23,360 --> 00:13:28,080
all kinds of resources. You know, they built AlphaGo and they have, you know, they, I wrote a piece

152
00:13:28,080 --> 00:13:33,280
recently that they lost, and you can argue about the word lost, but they spent $530 million more

153
00:13:33,280 --> 00:13:37,840
than they made last year. So, you know, they're making huge investments. They have a large budget

154
00:13:37,840 --> 00:13:44,800
and they have applied the same kinds of techniques to reading or to language. It's just much less

155
00:13:44,800 --> 00:13:49,200
productive there because it's a fundamentally different kind of problem. Chess and go and so

156
00:13:49,200 --> 00:13:53,840
forth are closed-in problems. The rules haven't changed in 2,500 years. There's only so many

157
00:13:53,840 --> 00:13:57,840
moves you can make. You can talk about the ex-financial as you look at the combinations of

158
00:13:57,840 --> 00:14:02,400
moves, but fundamentally, you know, the Go board has 361 squares. That's it. That's the only, you

159
00:14:02,400 --> 00:14:08,000
know, those intersections are the only places that you can place your stone. Whereas when you're

160
00:14:08,000 --> 00:14:13,360
reading, the next sentence could be anything. You know, it's completely up to the writer what

161
00:14:13,360 --> 00:14:17,600
they're going to do next. That's fascinating that you think this way. You're clearly a brilliant

162
00:14:17,600 --> 00:14:22,000
mind who points out the emperor has no clothes, but so I'll play the role of a person who says-

163
00:14:22,000 --> 00:14:24,160
You're gonna put clothes on the emperor? Good luck with it.

164
00:14:24,160 --> 00:14:30,080
Romanticizes the notion of the emperor, period, suggesting that clothes don't even matter.

165
00:14:30,080 --> 00:14:37,280
Okay, so that's really interesting that you're talking about language. So there's the physical

166
00:14:37,280 --> 00:14:42,240
world of being able to move about the world, making an omelet and coffee and so on. There's

167
00:14:42,240 --> 00:14:48,880
language where you first understand what's being written and then maybe even more complicated than

168
00:14:48,880 --> 00:14:54,480
that having a natural dialogue. And then there's the game of Go and chess. I would argue that

169
00:14:55,040 --> 00:15:01,360
language is much closer to Go than it is to the physical world. Like it is still very constrained.

170
00:15:01,360 --> 00:15:06,640
When you say the possibility of the number of sentences that could come, it is huge, but it

171
00:15:06,640 --> 00:15:12,800
nevertheless is much more constrained. It feels maybe I'm wrong than the possibilities that the

172
00:15:12,800 --> 00:15:17,760
physical world brings us. There's something to what you say in some ways in which I disagree. So

173
00:15:19,120 --> 00:15:24,720
one interesting thing about language is that it abstracts away. This bottle, I don't know if it'll

174
00:15:24,720 --> 00:15:29,680
be in the field of view, is on this table. And I use the word on here. And I can use the word

175
00:15:29,760 --> 00:15:37,600
on here, maybe not here. But that one word encompasses in analog space a sort of infinite

176
00:15:37,600 --> 00:15:44,560
number of possibilities. So there is a way in which language filters down the variation of the world.

177
00:15:45,680 --> 00:15:51,200
And there's other ways. So we have a grammar and more or less, you have to follow the rules of

178
00:15:51,200 --> 00:15:55,360
that grammar. You can break them a little bit, but by and large, we follow the rules of grammar.

179
00:15:55,360 --> 00:15:59,360
And so that's a constraint on language. So there are ways in which language is a constrained system.

180
00:15:59,440 --> 00:16:04,880
On the other hand, there are many arguments that say there's an infinite number of possible sentences,

181
00:16:04,880 --> 00:16:09,600
and you can establish that by just stacking them up. So I think there's water on the table. You

182
00:16:09,600 --> 00:16:13,280
think that I think there's water on the table. Your mother thinks that you think that I think

183
00:16:13,280 --> 00:16:17,520
the water is on the table. Your brother thinks that maybe your mom is wrong to think that you

184
00:16:17,520 --> 00:16:23,520
think that I think. We can make sentences of infinite length or we can stack up adjectives.

185
00:16:23,520 --> 00:16:27,840
This is a very silly example, a very, very silly example, a very, very, very, very, very, very,

186
00:16:28,480 --> 00:16:32,480
example and so forth. There are good arguments that there's an infinite range of sentences. In

187
00:16:32,480 --> 00:16:37,680
any case, it's vast by any reasonable measure. And for example, almost anything in the physical

188
00:16:37,680 --> 00:16:43,200
world we can talk about in the language world. And interestingly, many of the sentences that we

189
00:16:43,200 --> 00:16:48,240
understand, we can only understand if we have a very rich model of the physical world. So I don't

190
00:16:48,240 --> 00:16:52,960
ultimately want to adjudicate the debate that I think you just set up, but I find it interesting.

191
00:16:53,920 --> 00:16:59,040
You know, maybe the physical world is even more complicated than language. I think that's fair,

192
00:16:59,040 --> 00:17:03,040
but you think that language is really, really complicated. It's hard.

193
00:17:03,040 --> 00:17:06,000
It's really, really hard. Well, it's really, really hard for machines,

194
00:17:06,000 --> 00:17:10,400
for linguists, people trying to understand it. It's not that hard for children and that's part of

195
00:17:10,400 --> 00:17:15,280
what's driven my whole career. I was a student of Stephen Pinkers and we were trying to figure out

196
00:17:15,280 --> 00:17:20,400
why kids could learn language when machines couldn't. I think we're going to get into language,

197
00:17:20,480 --> 00:17:25,040
we're going to get into communication intelligence and neural networks and so on. But let me

198
00:17:26,640 --> 00:17:35,200
return to the high level of the futuristic for a brief moment. So you've written in your book,

199
00:17:35,840 --> 00:17:41,280
in your new book, it will be arrogant to suppose that we could forecast where AI will be,

200
00:17:41,280 --> 00:17:46,720
where the impact it will have in a thousand years or even 500 years. So let me ask you to be arrogant.

201
00:17:47,440 --> 00:17:52,960
What do AI systems with or without physical bodies look like a hundred years from now?

202
00:17:55,440 --> 00:18:00,000
You can't predict, but if you were to philosophize and imagine, do.

203
00:18:00,000 --> 00:18:03,920
Can I first justify the arrogance before you try to push me beyond it?

204
00:18:03,920 --> 00:18:04,240
Sure.

205
00:18:05,760 --> 00:18:09,600
I mean, there are examples. Like, you know, people figured out how electricity worked.

206
00:18:09,600 --> 00:18:13,600
They had no idea that that was going to lead to cell phones, right? I mean,

207
00:18:13,600 --> 00:18:19,360
things can move awfully fast once new technologies are perfected. Even when they made transistors,

208
00:18:19,360 --> 00:18:22,480
they weren't really thinking that cell phones would lead to social networking.

209
00:18:23,360 --> 00:18:28,640
There are nevertheless predictions of the future, which are statistically unlikely to come to be,

210
00:18:28,640 --> 00:18:30,720
but nevertheless, is the best. You're asking me to be wrong.

211
00:18:31,360 --> 00:18:33,920
Asking you to be... In which way would I like to be wrong?

212
00:18:33,920 --> 00:18:39,680
Pick the least unlikely to be wrong thing, even though it's most very likely to be wrong.

213
00:18:39,680 --> 00:18:42,080
I mean, here's some things that we can safely predict, I suppose.

214
00:18:42,880 --> 00:18:45,920
We can predict that AI will be faster than it is now.

215
00:18:47,120 --> 00:18:52,800
It will be cheaper than it is now. It will be better in the sense of being more general

216
00:18:52,800 --> 00:19:01,600
and applicable in more places. It will be pervasive. I mean, these are easy predictions.

217
00:19:01,600 --> 00:19:06,000
I'm sort of modeling them in my head on Jeff Bezos' famous predictions. He says,

218
00:19:06,000 --> 00:19:08,880
I can't predict the future, not in every way. I'm paraphrasing,

219
00:19:09,680 --> 00:19:13,120
but I can predict that people will never want to pay more money for their stuff.

220
00:19:13,120 --> 00:19:15,120
They're never going to want it to take longer to get there.

221
00:19:16,480 --> 00:19:19,440
You can't predict everything, but you can predict something. Sure, of course,

222
00:19:19,440 --> 00:19:25,680
it's going to be faster and better. We can't really predict is the full scope of

223
00:19:26,720 --> 00:19:31,760
where AI will be in a certain period. I mean, I think it's safe to say that,

224
00:19:31,760 --> 00:19:37,200
although I'm very skeptical about current AI, that it's possible to do much better.

225
00:19:37,760 --> 00:19:42,000
There's no in-principled argument that says AI is an insolvable problem,

226
00:19:42,000 --> 00:19:45,040
that there's magic inside our brains that will never be captured. I mean,

227
00:19:45,040 --> 00:19:47,600
I've heard people make those kind of arguments. I don't think they're very good.

228
00:19:48,960 --> 00:19:56,400
So AI is going to come and probably 500 years is plenty to get there. And then once it's here,

229
00:19:56,400 --> 00:20:00,640
it really will change everything. So when you say AI is going to come,

230
00:20:00,640 --> 00:20:05,920
are you talking about human level intelligence? I like the term general

231
00:20:05,920 --> 00:20:10,640
intelligence. So I don't think that the ultimate AI, if there is such a thing,

232
00:20:10,640 --> 00:20:14,240
is going to look just like humans. I think it's going to do some things that humans do

233
00:20:14,800 --> 00:20:21,040
better than current machines like reason flexibly and understand language and so forth.

234
00:20:21,040 --> 00:20:25,200
But this doesn't mean they have to be identical to humans. So for example, humans have terrible

235
00:20:25,200 --> 00:20:31,440
memory and they suffer from what some people call motivated reasoning. So they like arguments that

236
00:20:31,440 --> 00:20:36,160
seem to support them and they dismiss arguments that they don't like. There's no reason that a

237
00:20:36,160 --> 00:20:43,840
machine should ever do that. So you see that those limitations of memory as a bug, not a feature?

238
00:20:43,840 --> 00:20:48,320
Absolutely. I'll say two things about that. One is I was on a panel with Danny Kahneman,

239
00:20:48,320 --> 00:20:52,720
the Nobel Prize winner last night, and we were talking about this stuff. And I think what we

240
00:20:52,720 --> 00:20:58,800
converged on is that humans are a low bar to exceed. They may be outside of our skill right now,

241
00:20:59,760 --> 00:21:05,920
as AI programmers, but eventually AI will exceed it. So we're not talking about human level AI.

242
00:21:05,920 --> 00:21:09,680
We're talking about general intelligence that can do all kinds of different things and do it

243
00:21:09,680 --> 00:21:13,600
without some of the flaws that human beings have. The other thing I'll say is I wrote a whole book

244
00:21:13,600 --> 00:21:18,880
actually about the flaws of humans. It's actually a nice book end to the counterpoint to the current

245
00:21:18,880 --> 00:21:24,160
book. So I wrote a book called Cluj, which was about the limits of the human mind. The current

246
00:21:24,160 --> 00:21:27,840
book is kind of about those few things that humans do a lot better than machines.

247
00:21:28,640 --> 00:21:34,080
Do you think it's possible that the flaws of the human mind, the limits of memory, our mortality,

248
00:21:34,960 --> 00:21:44,720
our bias is a strength, not a weakness. That is the thing that enables from which motivation

249
00:21:45,520 --> 00:21:50,080
springs and meaning springs. I've heard a lot of arguments like this. I've never found them

250
00:21:50,080 --> 00:21:56,640
that convincing. I think that there's a lot of making lemonade out of lemons. So we, for example,

251
00:21:56,640 --> 00:22:01,680
do a lot of free association, where one idea just leads to the next and they're not really that well

252
00:22:01,680 --> 00:22:07,040
connected. And we enjoy that and we make poetry out of it and we make movies with free associations

253
00:22:07,040 --> 00:22:12,720
and it's fun and whatever. I don't think that's really a virtue of the system. I think that

254
00:22:13,600 --> 00:22:17,520
the limitations in human reasoning actually get us in a lot of trouble. For example,

255
00:22:17,600 --> 00:22:21,760
politically, we can't see eye to eye because we have the motivational reasoning I was talking

256
00:22:21,760 --> 00:22:26,560
about and something related called confirmation bias. So we have all of these problems that

257
00:22:26,560 --> 00:22:31,040
actually make for a rougher society because we can't get along because we can't interpret the

258
00:22:31,040 --> 00:22:38,080
data in shared ways. And then we do some nice stuff with that. So my free associations are

259
00:22:38,080 --> 00:22:42,720
different from yours and you're kind of amused by them and that's great and hence poetry. So

260
00:22:43,600 --> 00:22:48,400
there are lots of ways in which we take a lousy situation and make it good. Another example

261
00:22:48,400 --> 00:22:53,280
would be our memories are terrible. So we play games like Concentration where you flip over

262
00:22:53,280 --> 00:22:56,960
two cards, try to find a pair. Can you imagine a computer playing that? A computer is like,

263
00:22:56,960 --> 00:23:00,080
this is the dullest game in the world. I know where all the cards are. I see it once. I know

264
00:23:00,080 --> 00:23:05,920
where it is. What are you even talking about? So we make a fun game out of having this terrible

265
00:23:05,920 --> 00:23:13,440
memory. So we are imperfect in discovering and optimizing some kind of utility function,

266
00:23:13,440 --> 00:23:17,600
but you think in general, there is a utility function. There's an objective function that's

267
00:23:17,600 --> 00:23:23,200
better than others. I didn't say that. But see the presumption, when you say...

268
00:23:24,720 --> 00:23:30,000
I think you could design a better memory system. You could argue about utility functions and

269
00:23:31,040 --> 00:23:34,960
how you want to think about that. But objectively, it would be really nice to do

270
00:23:34,960 --> 00:23:39,680
some of the following things. To get rid of memories that are no longer useful.

271
00:23:41,120 --> 00:23:45,280
Objectively, that would just be good. And we're not that good at it. So when you park in the same

272
00:23:45,280 --> 00:23:49,120
lot every day, you confuse where you parked today with where you parked yesterday with where you

273
00:23:49,120 --> 00:23:53,120
parked the day before and so forth. So you blur together a series of memories. There's just no

274
00:23:53,120 --> 00:23:57,920
way that that's optimal. I mean, I've heard all kinds of wacky arguments, people trying to defend

275
00:23:57,920 --> 00:24:01,600
that. But in the end of the day, I don't think any of them hold water. Or trauma memories of

276
00:24:01,600 --> 00:24:06,640
traumatic events would be possibly a very nice feature to have to get rid of those.

277
00:24:06,640 --> 00:24:11,280
It'd be great if you could just be like, I'm going to wipe this sector. I'm done with that.

278
00:24:11,920 --> 00:24:15,680
I didn't have fun last night. I don't want to think about it anymore. Bye-bye.

279
00:24:15,680 --> 00:24:17,040
I'm gone. But we can't.

280
00:24:17,760 --> 00:24:22,560
Do you think it's possible to build a system? So you said human-level intelligence is a weird

281
00:24:22,560 --> 00:24:28,080
concept. I'm saying I prefer general intelligence. I mean, human-level intelligence is a real thing.

282
00:24:28,160 --> 00:24:31,920
And you could try to make a machine that matches people or something like that.

283
00:24:31,920 --> 00:24:37,280
I'm saying that per se shouldn't be the objective, but rather that we should learn from humans the

284
00:24:37,280 --> 00:24:41,280
things they do well and incorporate that into our AI, just as we incorporate the things that

285
00:24:41,280 --> 00:24:46,240
machines do well that people do terribly. So I mean, it's great that AI systems can do all

286
00:24:46,240 --> 00:24:50,800
this brute force computation that people can't. And one of the reasons I work on this stuff

287
00:24:50,800 --> 00:24:54,400
is because I would like to see machines solve problems that people can't,

288
00:24:54,400 --> 00:25:02,080
that in order to be solved would combine the strengths of machines to do all this computation

289
00:25:02,080 --> 00:25:06,560
with the ability, let's say, of people to read. So I'd like machines that can read the entire

290
00:25:06,560 --> 00:25:11,600
medical literature in a day, 7,000 new papers or whatever the numbers comes out every day.

291
00:25:11,600 --> 00:25:17,360
There's no way for any doctor or whatever to read them all. The machine that could read would be a

292
00:25:17,360 --> 00:25:22,640
brilliant thing. And that would be strengths of brute force computation combined with kind of

293
00:25:22,720 --> 00:25:26,720
subtlety and understanding medicine that, you know, a good doctor or scientist has.

294
00:25:26,720 --> 00:25:29,600
So if we can linger a little bit on the idea of general intelligence.

295
00:25:29,600 --> 00:25:34,400
So Jan Lacoon believes that human intelligence isn't general at all. It's very narrow.

296
00:25:35,520 --> 00:25:40,960
How do you think? I don't think that makes sense. We have lots of narrow intelligences for specific

297
00:25:40,960 --> 00:25:48,160
problems. But the fact is like anybody can walk into, let's say a Hollywood movie and reason

298
00:25:48,160 --> 00:25:54,080
about the content of almost anything that goes on there. So you can reason about what happens

299
00:25:54,080 --> 00:26:02,160
in a bank robbery or what happens when someone is infertile and wants to go to IVF to try to have

300
00:26:02,160 --> 00:26:09,280
a child. The list is essentially endless. And not everybody understands every scene in the

301
00:26:09,280 --> 00:26:14,960
movie, but there's a huge range of things that pretty much any ordinary adult can understand.

302
00:26:14,960 --> 00:26:22,080
His argument is that actually the set of things seems large to us humans because we're very limited

303
00:26:22,800 --> 00:26:27,200
in considering the kind of possibilities of experience as they're possible.

304
00:26:27,200 --> 00:26:31,920
But in fact, the amount of experience that are possible is infinitely larger.

305
00:26:32,480 --> 00:26:37,440
Well, I mean, if you want to make an argument that humans are constrained in what they can

306
00:26:37,440 --> 00:26:44,400
understand, I've no issue with that. I think that's right. But it's still not the same thing at all

307
00:26:44,400 --> 00:26:49,760
as saying, here's a system that can play go. It's been trained on five million games.

308
00:26:49,760 --> 00:26:54,080
And then I say, can it play on a rectangular board rather than a square board? And you say,

309
00:26:54,080 --> 00:26:58,960
well, if I retrain it from scratch on another five million games, I can't. That's really,

310
00:26:58,960 --> 00:27:04,480
really narrow. And that's where we are. We don't have even a system that could play go.

311
00:27:05,040 --> 00:27:11,120
And then without further retraining play on a rectangular board, which any human could do

312
00:27:11,120 --> 00:27:16,800
with very little problem. So that's what I mean by narrow. And so it's just wordplay to say

313
00:27:16,800 --> 00:27:22,480
semantics. Then it's just words. Then yeah, you mean general in a sense that you can do all kinds

314
00:27:22,480 --> 00:27:28,960
of go board shapes flexibly? Well, I mean, that would be like a first step in the right direction.

315
00:27:28,960 --> 00:27:34,800
Obviously, that's not what it really meaning. You're kidding. What I mean by general is that you

316
00:27:34,880 --> 00:27:41,200
could transfer the knowledge you learn in one domain to another. So if you learn about bank

317
00:27:41,200 --> 00:27:46,720
robberies in movies and there's chase scenes, then you can understand that amazing scene in

318
00:27:46,720 --> 00:27:52,560
Breaking Bad when Walter White has a car chase scene with only one person. He's the only one in it.

319
00:27:52,560 --> 00:27:57,360
And you can reflect on how that car chase scene is like all the other car chase scenes you've

320
00:27:57,360 --> 00:28:03,040
ever seen and totally different and why that's cool. And the fact that the number of domains you

321
00:28:03,120 --> 00:28:06,960
can do that with is finite doesn't make it less general. So the idea of general is you can just

322
00:28:06,960 --> 00:28:11,280
do it on a lot of transfer across a lot of domains. Yeah, I mean, I'm not saying humans are infinitely

323
00:28:11,280 --> 00:28:16,240
general or that humans are perfect. I just said a minute ago, it's a low bar, but it's just it's

324
00:28:16,240 --> 00:28:21,360
a low bar. But you know, right now, like the bar is here and we're there and eventually we'll get

325
00:28:21,360 --> 00:28:27,760
way past it. So speaking of low bars, you've highlighted in your new book as well, but a couple

326
00:28:27,840 --> 00:28:32,960
years ago wrote a paper titled Deep Learning a Critical Appraisal that lists 10 challenges

327
00:28:32,960 --> 00:28:40,480
faced by current deep learning systems. So let me summarize them as data efficiency, transfer

328
00:28:40,480 --> 00:28:47,040
learning, hierarchical knowledge, open-ended inference, explainability, integrating prior

329
00:28:47,040 --> 00:28:53,440
knowledge, causal reasoning, modeling on a stable world, robustness, adversarial examples, and so

330
00:28:53,520 --> 00:28:59,280
on. And then my favorite problem is reliability and engineering of real world systems. So

331
00:29:00,000 --> 00:29:03,280
whatever people can read the paper, they should definitely read the paper, should definitely

332
00:29:03,280 --> 00:29:09,600
read your book. But which of these challenges is solved in your view has the biggest impact on the

333
00:29:09,600 --> 00:29:16,240
AI community? It's a very good question. And I'm going to be evasive because I think that

334
00:29:16,240 --> 00:29:22,000
they go together a lot. So some of them might be solved independently of others. But I think a

335
00:29:22,000 --> 00:29:28,320
good solution to AI starts by having real what I would call cognitive models of what's going on.

336
00:29:28,320 --> 00:29:33,600
So right now we have an approach that's dominant where you take statistical approximations of

337
00:29:33,600 --> 00:29:39,040
things, but you don't really understand them. So you know that bottles are correlated in your data

338
00:29:39,040 --> 00:29:44,080
with bottle caps, but you don't understand that there's a thread on the bottle cap that fits with

339
00:29:44,080 --> 00:29:48,560
the thread on the bottle and that that's tightens in if I tighten enough that there's a seal and

340
00:29:48,560 --> 00:29:53,360
the water can come out. Like there's no machine that understands that. And having a good cognitive

341
00:29:53,360 --> 00:29:57,680
model of that kind of everyday phenomena is what we call common sense. And if you had that,

342
00:29:57,680 --> 00:30:03,600
then a lot of these other things start to fall into at least a little bit better place. Right now

343
00:30:03,600 --> 00:30:07,600
you're like learning correlations between pixels when you play a video game or something like that.

344
00:30:07,600 --> 00:30:11,200
And it doesn't work very well. It works when the video game is just the way that you

345
00:30:11,200 --> 00:30:14,800
studied it. And then you alter the video game in small ways, like you move the paddle and break

346
00:30:14,800 --> 00:30:19,360
out a few pixels and the system falls apart because it doesn't understand, it doesn't have

347
00:30:19,360 --> 00:30:24,080
a representation of a paddle, a ball, a wall, a set of bricks and so forth. And so it's reasoning

348
00:30:24,080 --> 00:30:31,440
at the wrong level. So the idea of common sense is full of mystery. You've worked on it, but it's

349
00:30:31,440 --> 00:30:37,360
nevertheless full of mystery, full of promise. What does common sense mean? What does knowledge

350
00:30:37,360 --> 00:30:41,920
mean? So the way you've been discussing it now is very intuitive. It makes a lot of sense that

351
00:30:41,920 --> 00:30:44,960
that is something we should have and that's something deep learning systems don't have.

352
00:30:45,520 --> 00:30:52,560
But the argument could be that we're oversimplifying it because we're oversimplifying the notion of

353
00:30:52,560 --> 00:30:58,480
common sense because that's how it feels like we as humans at the cognitive level approach

354
00:30:58,480 --> 00:31:05,120
problems. So a lot of people aren't actually going to read my book. But if they did read the book,

355
00:31:05,120 --> 00:31:09,920
one of the things that might come as a surprise to them is that we actually say a common sense

356
00:31:09,920 --> 00:31:15,280
is really hard and really complicated. So my critics know that I like common sense, but

357
00:31:16,320 --> 00:31:20,880
that chapter actually starts by us beating up not on deep learning, but kind of on our own

358
00:31:20,880 --> 00:31:26,640
home team as it will. So Ernie and I are first and foremost people that believe in at least some

359
00:31:26,640 --> 00:31:31,360
of what good old fashioned AI tried to do. So we believe in symbols and logic and programming.

360
00:31:32,400 --> 00:31:38,720
Things like that are important. And we go through why even those tools that we hold fairly dear

361
00:31:38,720 --> 00:31:44,160
aren't really enough. So we talk about why common sense is actually many things. And some of them

362
00:31:44,160 --> 00:31:50,320
fit really well with those classical sets of tools. So things like taxonomy. So I know that a bottle

363
00:31:50,320 --> 00:31:55,760
is an object or it's a vessel, let's say, and I know a vessel is an object and objects are

364
00:31:55,760 --> 00:32:03,280
material things in the physical world. So I can make some inferences. If I know that vessels need to

365
00:32:03,280 --> 00:32:10,000
not have holes in them, then I can infer that in order to carry their contents, then I can infer

366
00:32:10,000 --> 00:32:14,560
that a bottle shouldn't have a hole in order to carry its contents. So you can do hierarchical

367
00:32:14,560 --> 00:32:19,200
inference and so forth. And we say that's great, but it's only a tiny piece of what you need

368
00:32:20,160 --> 00:32:25,120
for common sense. We give lots of examples that don't fit into that. So another one that we talk

369
00:32:25,120 --> 00:32:29,600
about is a cheese grater. You've got holes in a cheese grater. You've got a handle on top. You

370
00:32:29,600 --> 00:32:35,040
can build a model in the game engine sense of a model so that you could have a little cartoon

371
00:32:35,040 --> 00:32:40,400
character flying around through the holes of the grater. But we don't have a system yet. Taxonomy

372
00:32:40,400 --> 00:32:44,480
doesn't help us that much. It really understands why the handle is on top and what you do with the

373
00:32:44,480 --> 00:32:50,000
handle or why all of those circles are sharp or how you'd hold the cheese with respect to the

374
00:32:50,000 --> 00:32:55,360
grater in order to make it actually work. Do you think these ideas are just abstractions that could

375
00:32:55,360 --> 00:33:01,760
emerge on a system like a very large deep neural network? I'm a skeptic that that kind of emergence

376
00:33:01,760 --> 00:33:07,360
per se can work. So I think that deep learning might play a role in the systems that do what I

377
00:33:07,360 --> 00:33:13,840
want systems to do, but it won't do it by itself. I've never seen a deep learning system really

378
00:33:13,840 --> 00:33:19,440
extract an abstract concept. What they do, principal reasons for that stemming from how

379
00:33:19,440 --> 00:33:25,040
back propagation works, how the architectures are set up. One example is deep learning people

380
00:33:25,120 --> 00:33:31,360
actually all build in something called convolution, which Jan Lacoon is famous for,

381
00:33:31,360 --> 00:33:36,320
which is an abstraction. They don't have their systems learn this. So the abstraction is an

382
00:33:36,320 --> 00:33:40,880
object that looks the same if it appears in different places. And what Lacoon figured out and

383
00:33:40,880 --> 00:33:45,360
why, you know, essentially why he was a co-winner of the Thuring Ward was that if you programmed

384
00:33:45,360 --> 00:33:51,680
this in innately, then your system would be a whole lot more efficient. In principle, this

385
00:33:51,680 --> 00:33:57,040
should be learnable, but people don't have systems that kind of reify things that make them

386
00:33:57,040 --> 00:34:02,560
more abstract. And so what you really wind up with if you don't program that in advance is a system

387
00:34:02,560 --> 00:34:06,880
that kind of realizes that this is the same thing as this, but then I take your little clock there

388
00:34:06,880 --> 00:34:10,400
and I move it over and it doesn't realize that the same thing applies to the clock.

389
00:34:10,400 --> 00:34:15,280
So the really nice thing you're right that convolution is just one of the things that's

390
00:34:15,280 --> 00:34:20,320
like it's an innate feature that's programmed by the human expert, but we need more of those

391
00:34:20,320 --> 00:34:26,560
not less. So the, but the nice feature is it feels like that requires coming up with that brilliant

392
00:34:27,200 --> 00:34:35,360
idea can get your Thuring Award, but it requires less effort than encoding and something we'll

393
00:34:35,360 --> 00:34:41,920
talk about the expert system. So encoding a lot of knowledge by hand. So it feels like one,

394
00:34:41,920 --> 00:34:46,320
there's a huge amount of limitations, which you clearly outline with deep learning,

395
00:34:46,320 --> 00:34:50,160
but the nice feature deep learning, whatever it is able to accomplish, it does it,

396
00:34:51,680 --> 00:34:54,720
it does a lot of stuff automatically without human intervention.

397
00:34:54,720 --> 00:34:59,040
Well, and that's part of why people love it, right? But I always think of this quote from

398
00:34:59,040 --> 00:35:04,720
Bertrand Russell, which is it has all the advantages of theft over honest toil. It's

399
00:35:04,720 --> 00:35:10,880
really hard to program into a machine a notion of causality or, you know, even how a bottle

400
00:35:10,880 --> 00:35:16,560
works or what containers are. Ernie Davis and I wrote a, I don't know, 45 page academic paper

401
00:35:16,560 --> 00:35:20,960
trying just to understand what a container is, which I don't think anybody ever read the paper,

402
00:35:20,960 --> 00:35:26,560
but it's a very detailed analysis of all the things. Well, not even all the some of the things

403
00:35:26,560 --> 00:35:31,040
you need to do in order to understand a container. It would be a whole lot nice. And, you know,

404
00:35:31,040 --> 00:35:34,720
I'm a co-author on the paper, I made it a little bit better, but Ernie did the hard work for that

405
00:35:34,720 --> 00:35:40,480
particular paper. And he took him like three months to get the logical statements correct.

406
00:35:40,560 --> 00:35:45,520
And maybe that's not the right way to do it. It's a way to do it. But on that way of doing it,

407
00:35:46,320 --> 00:35:50,400
it's really hard work to do something as simple as understanding containers.

408
00:35:50,400 --> 00:35:54,800
And nobody wants to do that hard work. Even Ernie didn't want to do that hard work.

409
00:35:55,520 --> 00:35:59,520
Everybody would rather just like feed their system in with a bunch of videos with a bunch of

410
00:35:59,520 --> 00:36:04,880
containers and have the systems infer how containers work. It would be like so much

411
00:36:04,880 --> 00:36:08,880
less effort. Let the machine do the work. And so I understand the impulse. I understand why

412
00:36:08,880 --> 00:36:14,480
people want to do that. I just don't think that it works. I've never seen anybody build a system

413
00:36:14,480 --> 00:36:20,960
that in a robust way can actually watch videos and predict exactly, you know, which containers would

414
00:36:20,960 --> 00:36:24,960
leak and which ones wouldn't or something like. And I know someone's going to go out and do that

415
00:36:24,960 --> 00:36:31,120
since I said it and I look forward to seeing it. But getting these things to work robustly is really,

416
00:36:31,120 --> 00:36:36,800
really hard. So Yann LeCun, who was my colleague at NYU for many years,

417
00:36:37,600 --> 00:36:43,040
thinks that the hard work should go into defining an unsupervised learning algorithm

418
00:36:43,040 --> 00:36:48,400
that will watch videos, use the next frame basically in order to tell it what's going on.

419
00:36:48,400 --> 00:36:51,680
And he thinks that's the royal road and he's willing to put in the work in

420
00:36:51,680 --> 00:36:57,840
devising that algorithm. Then he wants the machine to do the rest. And again, I understand the impulse.

421
00:36:57,840 --> 00:37:03,280
My intuition based on years of watching this stuff and making predictions 20 years ago that

422
00:37:03,280 --> 00:37:07,040
still hold even though there's a lot more computation and so forth, is that we actually

423
00:37:07,040 --> 00:37:11,200
have to do a different kind of hard work, which is more like building a design specification

424
00:37:11,200 --> 00:37:15,680
for what we want the system to do, doing hard engineering work to figure out how we do things

425
00:37:15,680 --> 00:37:21,600
like what Yann did for convolution in order to figure out how to encode complex knowledge

426
00:37:21,600 --> 00:37:26,880
into the systems. The current systems don't have that much knowledge other than convolution,

427
00:37:26,880 --> 00:37:32,480
which is again this, you know, objects being in different places and having the same perception,

428
00:37:32,480 --> 00:37:39,600
I guess I'll say, same appearance. People don't want to do that work. They don't see how to naturally

429
00:37:39,600 --> 00:37:45,440
fit one with the other. I think that's, yes, absolutely. But also on the expert system side,

430
00:37:45,440 --> 00:37:49,760
there's a temptation to go too far the other way. So we're just having an expert sort of sit down

431
00:37:49,760 --> 00:37:55,280
and encode the description, the framework for what a container is, and then having the system

432
00:37:55,280 --> 00:38:00,400
reason the rest. From my view, like one really exciting possibility is of active learning where

433
00:38:00,400 --> 00:38:06,320
it's continuous interaction between a human and machine. As the machine, there's kind of deep

434
00:38:06,320 --> 00:38:12,720
learning type extraction of information from data patterns and so on. But humans also guiding

435
00:38:12,720 --> 00:38:20,960
the learning procedures, guiding both the process and the framework of how the machine

436
00:38:20,960 --> 00:38:24,720
learns whatever the task is. I was with you with almost everything you said except the phrase

437
00:38:24,720 --> 00:38:30,880
deep learning. What I think you really want there is a new form of machine learning. So

438
00:38:30,880 --> 00:38:35,360
let's remember deep learning is a particular way of doing machine learning. Most often it's done

439
00:38:35,360 --> 00:38:40,560
with supervised data for perceptual categories. There are other things you can do with deep learning.

440
00:38:41,680 --> 00:38:46,480
Some of them quite technical, but the standard use of deep learning is I have a lot of examples

441
00:38:46,480 --> 00:38:51,280
and I have labels for them. So here are pictures. This one's the Eiffel Tower. This one's the Sears

442
00:38:51,280 --> 00:38:55,120
Tower. This one's the Empire State Building. This one's a cat. This one's a pig and so forth.

443
00:38:55,120 --> 00:39:00,400
You just get millions of examples, millions of labels. And deep learning is extremely good at

444
00:39:00,400 --> 00:39:06,080
that. It's better than any other solution that anybody has devised. But it is not good at representing

445
00:39:06,080 --> 00:39:13,600
abstract knowledge. It's not good at representing things like bottles contain liquid and have tops

446
00:39:13,600 --> 00:39:17,760
to them and so forth. It's not very good at learning or representing that kind of knowledge.

447
00:39:17,760 --> 00:39:22,880
It is an example of having a machine learn something, but it's a machine that learns a

448
00:39:22,880 --> 00:39:27,200
particular kind of thing, which is object classification. It's not a particularly good

449
00:39:27,200 --> 00:39:32,080
algorithm for learning about the abstractions that govern our world. There may be such a thing.

450
00:39:32,960 --> 00:39:36,800
Part of what we counsel in the book is maybe people should be working on devising such things.

451
00:39:36,800 --> 00:39:43,600
So one possibility, I wonder what you think about it, is deep neural networks do form

452
00:39:43,600 --> 00:39:49,280
abstractions, but they're not accessible to us humans in terms of we can't.

453
00:39:49,280 --> 00:39:54,720
There's some truth in that. So is it possible that either current or future neural networks

454
00:39:54,720 --> 00:40:02,080
form very high level abstractions, which are as powerful as our human abstractions of common

455
00:40:02,080 --> 00:40:07,600
sense, we just can't get a hold of them. And so the problem is essentially what we need to make

456
00:40:07,600 --> 00:40:12,480
them explainable. This is an astute question, but I think the answer is at least partly no.

457
00:40:13,120 --> 00:40:17,520
One of the kinds of classical neural network architecture is what we call an auto-associator.

458
00:40:17,520 --> 00:40:22,960
It just tries to take an input, goes through a set of hidden layers, and comes out with an output.

459
00:40:22,960 --> 00:40:26,880
And it's supposed to learn essentially the identity function, that your input is the same as your

460
00:40:26,880 --> 00:40:30,560
output. So you think of those binary numbers, you've got like the 1, the 2, the 4, the 8,

461
00:40:30,560 --> 00:40:35,920
the 16, and so forth. And so if you want to input 24, you turn on the 16, you turn on the 8. It's

462
00:40:36,000 --> 00:40:44,640
like binary 1, 1, and bunch of zeros. So I did some experiments in 1998 with the precursors of

463
00:40:45,280 --> 00:40:51,600
contemporary deep learning. And what I showed was you could train these networks on all the even

464
00:40:51,600 --> 00:40:56,240
numbers and they would never generalize to the odd number. A lot of people thought that I was,

465
00:40:56,240 --> 00:41:01,600
I don't know, an idiot or faking the experiment or wasn't true or whatever, but it is true that

466
00:41:01,600 --> 00:41:06,320
with this class of networks that we had in that day, that they would never ever make this

467
00:41:06,320 --> 00:41:11,360
generalization. And it's not that the networks were stupid, it's that they see the world in a

468
00:41:11,360 --> 00:41:16,880
different way than we do. They were basically concerned, what is the probability that the

469
00:41:16,880 --> 00:41:22,000
rightmost output node is going to be 1? And as far as they were concerned, in everything they'd

470
00:41:22,000 --> 00:41:27,760
ever been trained on, it was a 0. That node had never been turned on. And so they figured,

471
00:41:27,760 --> 00:41:31,120
why turn it on now? Whereas a person would look at the same problem and say, well,

472
00:41:31,120 --> 00:41:35,520
it's obvious, we're just doing the thing that corresponds. The Latin for it is mutatus mutatus,

473
00:41:35,520 --> 00:41:42,720
will change what needs to be changed. And we do this, this is what algebra is. So I can do f of x

474
00:41:42,720 --> 00:41:47,360
equals y plus two. And I can do it for a couple of values, I can tell you if y is three, then x is

475
00:41:47,360 --> 00:41:50,960
five, and if y is four, x is six. And now I can do it with some totally different number,

476
00:41:50,960 --> 00:41:53,920
like a million, then you can say, well, obviously it's a million and two, because you have

477
00:41:53,920 --> 00:41:59,520
an algebraic operation that you're applying to a variable. And deep learning systems kind of

478
00:41:59,520 --> 00:42:04,800
emulate that, but they don't actually do it. The particular example, you could

479
00:42:06,560 --> 00:42:10,400
fudge a solution to that particular problem. The general form of that problem remains

480
00:42:10,400 --> 00:42:14,240
that what they learn is really correlations between different input and output nodes.

481
00:42:14,240 --> 00:42:19,200
They're complex correlations with multiple nodes involved and so forth. But ultimately,

482
00:42:19,200 --> 00:42:24,000
they're correlative. They're not structured over these operations over variables. Now, someday,

483
00:42:24,000 --> 00:42:27,840
people may do a new form of deep learning that incorporates that stuff. And I think it will

484
00:42:27,840 --> 00:42:32,160
help a lot. And there's some tentative work on things like differentiable programming right now

485
00:42:32,160 --> 00:42:36,800
that fall into that category. But the sort of classic stuff like people use for ImageNet

486
00:42:37,920 --> 00:42:41,120
doesn't have it. And you have people like Hinton going around saying,

487
00:42:41,120 --> 00:42:45,680
symbol manipulation like what Marcus, what I advocate is like the gasoline engine,

488
00:42:45,680 --> 00:42:49,680
it's obsolete, we should just use this cool electric power that we've got with the deep

489
00:42:49,680 --> 00:42:55,840
learning. And that's really destructive, because we really do need to have the gasoline engine stuff

490
00:42:55,840 --> 00:43:02,560
that represents, I don't think it's a good analogy, but we really do need to have the stuff that

491
00:43:02,560 --> 00:43:08,160
represents symbols. And Hinton as well would say that we do need to throw out everything

492
00:43:08,160 --> 00:43:15,520
and start over. Hinton said that to Axios. And I had a friend who interviewed him and

493
00:43:15,520 --> 00:43:19,200
tried to pin him down on what exactly we need to throw. And he was very evasive.

494
00:43:19,760 --> 00:43:24,400
Well, of course, because we can't, if he knew that he'd throw it out himself. But I mean,

495
00:43:24,400 --> 00:43:27,520
you can't have it both ways. You can't be like, I don't know what to throw out,

496
00:43:27,520 --> 00:43:32,800
but I am going to throw out the symbols. And not just the symbols, but the variables and

497
00:43:32,800 --> 00:43:36,720
the operations over variables. Don't forget the operations over variables, the stuff that I'm

498
00:43:36,720 --> 00:43:43,120
endorsing, and which John McCarthy did when he founded AI, that stuff is the stuff that we build

499
00:43:43,120 --> 00:43:47,680
most computers out of. There are people now who say, we don't need computer programmers anymore.

500
00:43:48,720 --> 00:43:52,320
Not quite looking at the statistics of how much computer programmers actually get paid right now.

501
00:43:52,880 --> 00:43:57,680
We need lots of computer programs. And most of them, they do a little bit of machine learning,

502
00:43:57,680 --> 00:44:02,480
but they still do a lot of code, right? Code where it's like, if the value of x is greater

503
00:44:02,480 --> 00:44:06,720
than the value of y, then do this kind of thing, like conditionals and comparing operations over

504
00:44:06,720 --> 00:44:10,880
variables. There's this fantasy you can machine learn anything. There's some things you would

505
00:44:10,880 --> 00:44:15,920
never want to machine learn. I would not use a phone operating system that was machine learned.

506
00:44:15,920 --> 00:44:19,680
Like you made a bunch of phone calls and you recorded which packets were transmitted and

507
00:44:19,680 --> 00:44:26,640
you just machine learned it. It would be insane. Or to build a web browser by taking logs of key

508
00:44:26,640 --> 00:44:32,000
strokes and images, screenshots, and then trying to learn the relation between them. Nobody would

509
00:44:32,000 --> 00:44:36,800
ever, no rational person would ever try to build a browser that way. They would use symbol

510
00:44:36,800 --> 00:44:41,280
manipulation, the stuff that I think AI needs to avail itself of in addition to deep learning.

511
00:44:42,000 --> 00:44:48,000
Can you describe what your view of symbol manipulation in its early days? Can you

512
00:44:48,000 --> 00:44:53,520
describe expert systems and where do you think they hit a wall or a set of challenges?

513
00:44:55,520 --> 00:44:59,600
First, I just want to clarify. I'm not endorsing expert systems per se. You've been

514
00:44:59,600 --> 00:45:03,040
kind of contrasting them. There is a contrast, but that's not the thing that I'm endorsing.

515
00:45:04,080 --> 00:45:09,360
So expert systems try to capture things like medical knowledge with a large set of rules.

516
00:45:09,360 --> 00:45:14,240
So if the patient has this symptom and this other symptom, then it is likely that they have this

517
00:45:14,240 --> 00:45:18,800
disease. So there are logical rules and they were symbol manipulating rules of just the sort

518
00:45:18,800 --> 00:45:25,040
that I'm talking about. They encode a set of knowledge that the experts have put in.

519
00:45:25,040 --> 00:45:31,040
Very explicitly so. So you'd have somebody interview an expert and then try to turn that stuff into

520
00:45:31,040 --> 00:45:37,520
rules. At some level, I'm arguing for rules, but the difference is those guys did in the 80s

521
00:45:37,520 --> 00:45:43,120
who was almost entirely rules, almost entirely handwritten with no machine learning. What a lot

522
00:45:43,120 --> 00:45:48,160
of people are doing now is almost entirely one species of machine learning with no rules.

523
00:45:48,160 --> 00:45:52,240
And what I'm counseling is actually a hybrid. I'm saying that both of these things have their

524
00:45:52,240 --> 00:45:56,960
advantage. So if you're talking about perceptual classification, how do I recognize a bottle?

525
00:45:56,960 --> 00:46:00,320
Deep learning is the best tool we've got right now. If you're talking about making

526
00:46:00,320 --> 00:46:04,640
inferences about what a bottle does, something closer to the expert systems is probably still

527
00:46:05,680 --> 00:46:10,480
the best available alternative. And probably we want something that is better able to handle

528
00:46:10,560 --> 00:46:14,880
quantitative and statistical information than those classical systems typically were.

529
00:46:14,880 --> 00:46:19,360
So we need new technologies that are going to draw some of the strengths of both the expert

530
00:46:19,360 --> 00:46:23,280
systems and the deep learning, but are going to find new ways to synthesize them.

531
00:46:23,280 --> 00:46:30,400
How hard do you think it is to add knowledge at the low level? So mind human intellects

532
00:46:30,400 --> 00:46:35,680
to add extra information to symbol manipulating systems.

533
00:46:36,320 --> 00:46:41,520
In some domains, it's not that hard, but it's often really hard, partly because a lot of the

534
00:46:42,960 --> 00:46:48,320
things that are important, people wouldn't bother to tell you. So if you pay someone on

535
00:46:48,320 --> 00:46:54,240
Amazon Mechanical Turk to tell you stuff about bottles, they probably won't even bother to tell

536
00:46:54,240 --> 00:47:00,960
you some of the basic level stuff that's just so obvious to a human being and yet so hard to capture

537
00:47:01,040 --> 00:47:08,800
in machines. They're going to tell you more exotic things, and they're all well and good,

538
00:47:08,800 --> 00:47:16,400
but they're not getting to the root of the problem. So untutored humans aren't very good at knowing,

539
00:47:16,400 --> 00:47:23,440
and why should they be, what kind of knowledge the computer system developers actually need.

540
00:47:23,440 --> 00:47:28,800
I don't think that that's an irremediable problem. I think it's historically been a problem. People

541
00:47:29,040 --> 00:47:33,120
had crowd sourcing efforts, and they don't work that well. There's one at MIT. We're

542
00:47:33,120 --> 00:47:38,640
recording this at MIT called Virtual Home, and we talk about this in the book.

543
00:47:39,440 --> 00:47:43,920
Find the exact example there, but people were asked to do things like describe an exercise

544
00:47:43,920 --> 00:47:49,440
routine. And the things that the people describe it are very low level and don't really capture

545
00:47:49,440 --> 00:47:54,640
what's going on. So they're like, go to the room with the television and the weights,

546
00:47:54,640 --> 00:48:00,880
turn on the television, press the remote to turn on the television, lift weight, put weight down.

547
00:48:01,360 --> 00:48:06,720
It's like very micro level, and it's not telling you what an exercise routine is really about,

548
00:48:06,720 --> 00:48:11,120
which is like, I want to fit a certain number of exercises in a certain time period. I want to

549
00:48:11,120 --> 00:48:16,000
emphasize these muscles. You want some kind of abstract description. The fact that you happen

550
00:48:16,000 --> 00:48:21,360
to press the remote control in this room when you watch this television isn't really the essence of

551
00:48:22,080 --> 00:48:26,080
the exercise routine. But if you just ask people what did they do, then they give you

552
00:48:26,080 --> 00:48:32,480
this fine grain. And so it takes a little level of expertise about how the AI works in order to

553
00:48:33,120 --> 00:48:37,520
craft the right kind of knowledge. So there's this ocean of knowledge that we all operate on.

554
00:48:37,520 --> 00:48:43,120
Some of it may not even be conscious, or at least we're not able to communicate it effectively.

555
00:48:43,120 --> 00:48:47,360
Yeah, most of it we would recognize if somebody said it if it was true or not,

556
00:48:47,360 --> 00:48:50,560
but we wouldn't think to say that it's true or not. It's a really interesting

557
00:48:51,280 --> 00:48:56,640
mathematical property. This ocean has the property that every piece of knowledge in it,

558
00:48:56,640 --> 00:49:04,080
we will recognize it as true if we're told, but we're unlikely to retrieve it in the reverse.

559
00:49:04,080 --> 00:49:09,760
So that interesting property, I would say there's a huge ocean of that knowledge.

560
00:49:10,480 --> 00:49:15,120
What's your intuition? Is it accessible to AI systems somehow? Can we?

561
00:49:16,080 --> 00:49:21,360
Yeah, I mean, most of it is not, I'll give you an asterisk on this in a second, but most of it is

562
00:49:21,360 --> 00:49:27,200
not ever been encoded in machine interpretable form. And so, I mean, if you say accessible,

563
00:49:27,200 --> 00:49:32,240
there's two meanings of that. One is like, could you build it into a machine? Yes.

564
00:49:32,240 --> 00:49:38,240
The other is like, is there some database that we could go, you know, download and stick into our

565
00:49:38,240 --> 00:49:43,120
machine? But the first thing, no. Could we? What's your intuition? I think we could. I think it

566
00:49:43,120 --> 00:49:50,720
hasn't been done right. The closest, and this is the asterisk, is the CYC psych system. Try to do

567
00:49:50,720 --> 00:49:56,640
this. A lot of logicians worked for Doug Lennon for 30 years on this project. I think they stuck

568
00:49:56,640 --> 00:50:01,120
too closely to logic, didn't represent enough about probabilities, tried to hand code it.

569
00:50:01,120 --> 00:50:05,840
There are various issues, and it hasn't been that successful. That is the closest

570
00:50:06,800 --> 00:50:13,360
existing system to trying to encode this. Why do you think there's not more excitement

571
00:50:13,360 --> 00:50:19,520
slash money behind this idea currently? There was. People view that project as a failure. I think that

572
00:50:19,520 --> 00:50:25,440
they confused the failure of a specific instance that was conceived 30 years ago for the failure

573
00:50:25,440 --> 00:50:31,360
of an approach, which they don't do for deep learning. So, in 2010, people had the same

574
00:50:31,440 --> 00:50:36,880
attitude towards deep learning. They're like, this stuff doesn't really work. And, you know,

575
00:50:36,880 --> 00:50:41,440
all these other algorithms work better and so forth. And then certain key technical advances

576
00:50:41,440 --> 00:50:46,320
were made, but mostly it was the advent of graphics processing units that changed that.

577
00:50:46,320 --> 00:50:51,360
It wasn't even anything foundational in the techniques. And there were some new tricks, but

578
00:50:52,880 --> 00:50:57,120
mostly it was just more compute and more data, things like ImageNet that didn't exist before,

579
00:50:57,760 --> 00:51:02,160
that allowed deep learning. And it could be to work. It could be that, you know,

580
00:51:02,160 --> 00:51:07,600
psych just needs a few more things or something like psych. But the widespread view is that

581
00:51:07,600 --> 00:51:12,800
that just doesn't work. And people are reasoning from a single example. They don't do that with

582
00:51:12,800 --> 00:51:18,240
deep learning. They don't say nothing that existed in 2010. And there were many, many efforts in

583
00:51:18,240 --> 00:51:23,760
deep learning was really worth anything. Right? I mean, really, there's no model from 2010

584
00:51:23,840 --> 00:51:27,760
in deep learning. So, this is the deep learning that has any commercial value

585
00:51:27,760 --> 00:51:32,880
whatsoever at this point, right? They're all failures. But that doesn't mean that there wasn't

586
00:51:32,880 --> 00:51:38,720
anything there. I have a friend, I was getting to know him, and he said, I had a company too,

587
00:51:38,720 --> 00:51:43,440
I was talking about, I had a new company. He said, I had a company too, and it failed. And I said,

588
00:51:43,440 --> 00:51:48,240
well, what did you do? And he said, deep learning. And the problem was he did it in 1986 or something

589
00:51:48,240 --> 00:51:53,120
like that. And we didn't have the tools then or 1990. We didn't have the tools then, not the

590
00:51:53,120 --> 00:51:57,600
algorithms. His algorithms weren't that different from other algorithms, but he didn't have the GPUs

591
00:51:57,600 --> 00:52:02,800
to run it fast enough. He didn't have the data. And so it failed. It could be that

592
00:52:04,240 --> 00:52:11,440
simple manipulation per se with modern amounts of data and compute and maybe some advance in

593
00:52:11,440 --> 00:52:18,640
compute for that kind of compute might be great. My perspective on it is not that we want to

594
00:52:18,640 --> 00:52:22,160
resuscitate that stuff per se, but we want to borrow lessons from it, bring together with

595
00:52:22,160 --> 00:52:27,120
other things that we've learned. And it might have an ImageNet moment where it will spark the

596
00:52:27,120 --> 00:52:31,920
world's imagination, and it'll be an explosion of simple manipulation efforts. Yeah, I think that

597
00:52:31,920 --> 00:52:39,520
people at AI2, Paul Allen's AI Institute, are trying to build datasets that, well, they're not

598
00:52:39,520 --> 00:52:43,520
doing it for quite the reason that you say, but they're trying to build datasets that at least

599
00:52:43,520 --> 00:52:48,080
spark interest in common sense reasoning. To create benchmarks. Benchmarks for common sense.

600
00:52:48,080 --> 00:52:51,920
That's a large part of what the AI2.org is working on right now.

601
00:52:51,920 --> 00:52:56,800
So speaking of compute, Rich Sutton wrote a blog post titled Bitter Lesson. I don't know if you've

602
00:52:56,800 --> 00:53:01,440
read it, but he said that the biggest lesson that can be read from 70 years of AI research

603
00:53:01,440 --> 00:53:05,520
is that general methods that leverage computation are ultimately the most effective.

604
00:53:06,320 --> 00:53:13,280
The most effective of what? So they have been most effective for perceptual classification

605
00:53:13,360 --> 00:53:19,360
problems, and for some reinforcement learning problems, he works on reinforcement learning.

606
00:53:19,360 --> 00:53:24,000
Well, no, let me push back on that. You're actually absolutely right. But I would also

607
00:53:24,000 --> 00:53:31,440
say they've been most effective generally, because everything we've done up to the world.

608
00:53:31,440 --> 00:53:37,040
Would you argue against that? To me, deep learning is the first thing that has been

609
00:53:37,520 --> 00:53:47,120
successful at anything in AI. And you're pointing out that this success is very limited, folks,

610
00:53:47,120 --> 00:53:51,600
but has there been something truly successful before deep learning?

611
00:53:51,600 --> 00:53:58,240
Sure. I want to make a larger point, but on the narrower point, classical AI

612
00:53:59,520 --> 00:54:06,400
is used, for example, in doing navigation instructions. It's very successful. Everybody

613
00:54:06,400 --> 00:54:10,640
on the planet uses it now, multiple times a day. That's a measure of success.

614
00:54:13,360 --> 00:54:18,640
I don't think classical AI was wildly successful, but there are cases like that that is used all

615
00:54:18,640 --> 00:54:25,680
the time nobody even notices them, because they're so pervasive. So there are some successes for

616
00:54:25,680 --> 00:54:30,000
classical AI. I think deep learning has been more successful, but my usual line about this,

617
00:54:30,000 --> 00:54:34,640
and I didn't invent it, but I like it a lot, is just because you can build a better ladder,

618
00:54:34,720 --> 00:54:39,600
it doesn't mean you can build a ladder to the moon. So the bitter lesson is if you have a

619
00:54:39,600 --> 00:54:45,120
perceptual classification problem, throwing a lot of data at it is better than anything else.

620
00:54:45,760 --> 00:54:51,840
But that has not given us any material progress in natural language understanding,

621
00:54:51,840 --> 00:54:57,120
common sense reasoning like a robot would need to navigate a home. Problems like that,

622
00:54:57,120 --> 00:55:02,160
there's no actual progress there. So flip side of that, if we remove data from the picture,

623
00:55:02,160 --> 00:55:11,440
another bitter lesson is that you just have a very simple algorithm and you wait for compute

624
00:55:11,440 --> 00:55:14,880
to scale. This doesn't have to be learning, it doesn't have to be deep learning, it doesn't

625
00:55:14,880 --> 00:55:19,840
have to be data driven, but just wait for the compute. So my question for you, do you think

626
00:55:19,840 --> 00:55:25,600
compute can unlock some of the things with either deep learning or simple manipulation that?

627
00:55:25,680 --> 00:55:32,800
Sure, but I'll put a proviso on that. The more compute's always better. Nobody's going to argue

628
00:55:32,800 --> 00:55:37,440
with more compute, it's like having more money. There's diminishing returns on more money.

629
00:55:37,440 --> 00:55:41,280
Exactly, there's diminishing returns on more money, but nobody's going to argue if you want

630
00:55:41,280 --> 00:55:45,120
to give them more money, right? Except maybe the people who signed the giving pledge and some of

631
00:55:45,120 --> 00:55:50,160
them have a problem, they've promised to give away more money than they're able to. But the rest of

632
00:55:50,160 --> 00:55:54,480
us, if you want to give me more money, fine. Say more money, more problems, but okay.

633
00:55:54,480 --> 00:56:01,200
That's true too. What I would say to you is your brain uses 20 watts and it does a lot of things

634
00:56:01,200 --> 00:56:06,080
that deep learning doesn't do or that simple manipulation doesn't do, that AI just hasn't

635
00:56:06,080 --> 00:56:12,560
figured out how to do. So it's an existence proof that you don't need server resources that are

636
00:56:12,560 --> 00:56:18,880
Google scale in order to have an intelligence. I built, with a lot of help from my wife,

637
00:56:18,960 --> 00:56:23,920
two intelligences that are 20 watts each and far exceed anything that anybody else

638
00:56:25,040 --> 00:56:31,760
has built at a silicon. Speaking of those two robots, what have you learned about AI from

639
00:56:31,760 --> 00:56:36,560
having- Well, they're not robots, but- Sorry, intelligent agents.

640
00:56:36,560 --> 00:56:40,960
There's two intelligent agents. I've learned a lot by watching my two intelligent agents.

641
00:56:42,720 --> 00:56:47,600
I think that what's fundamentally interesting, well, one of the many things that's fundamentally

642
00:56:47,600 --> 00:56:51,280
interesting about them is the way that they set their own problems to solve.

643
00:56:51,920 --> 00:56:56,320
So my two kids are a year and a half apart. They're both five and six and a half.

644
00:56:56,320 --> 00:57:01,280
They play together all the time and they're constantly creating new challenges. That's what

645
00:57:01,280 --> 00:57:06,720
they do is they make up games and they're like, well, what if this or what if that or what if I

646
00:57:06,720 --> 00:57:11,920
had this superpower or what if you could walk through this wall. They're doing these what if

647
00:57:11,920 --> 00:57:19,200
scenarios all the time and that's how they learn something about the world and grow their minds

648
00:57:19,200 --> 00:57:24,480
and machines don't really do that. So that's interesting. You've talked about this. You've

649
00:57:24,480 --> 00:57:31,840
written about it. You've thought about it. Nature versus nurture. So what innate knowledge do you

650
00:57:31,840 --> 00:57:38,160
think we're born with and what do we learn along the way in those early months and years?

651
00:57:38,160 --> 00:57:44,720
Can I just say how much I like that question? You phrased it just right and almost nobody ever does,

652
00:57:45,680 --> 00:57:50,240
which is what is the innate knowledge and what's learned along the way. So many people

653
00:57:50,240 --> 00:57:56,080
dichotomize it and they think it's nature versus nurture. When it is obviously, it has to be nature

654
00:57:56,080 --> 00:58:01,520
and nurture. They have to work together. You can't learn the stuff along the way unless you have some

655
00:58:01,520 --> 00:58:05,280
innate stuff. But just because you have the innate stuff doesn't mean you don't learn anything.

656
00:58:05,680 --> 00:58:11,360
And so many people get that wrong, including in the field. People think if I work in machine

657
00:58:11,360 --> 00:58:16,080
learning, the learning side, I must not be allowed to work on the innate side where

658
00:58:16,080 --> 00:58:22,080
that will be cheating. Exactly. People have said that to me and it's just absurd. So thank you.

659
00:58:23,280 --> 00:58:27,600
But you could break that apart more. I've talked to folks who studied the development of the brain

660
00:58:28,240 --> 00:58:35,680
and the growth of the brain in the first few days, in the first few months, in the womb,

661
00:58:35,680 --> 00:58:42,720
all of that, you know, is that innate? So that process of development from a stem cell to the

662
00:58:42,720 --> 00:58:50,400
growth of the central nervous system and so on to the information that's encoded through the long

663
00:58:50,400 --> 00:58:56,320
arc of evolution. So all of that comes into play and it's unclear. It's not just whether it's a

664
00:58:56,320 --> 00:59:03,200
dichotomy or not. It's where most or where the knowledge is encoded. So what's your intuition

665
00:59:03,840 --> 00:59:10,560
about the innate knowledge, the power of it, what's contained in it, what can we learn from it?

666
00:59:11,200 --> 00:59:14,560
One of my earlier books was actually trying to understand the biology of this. The book was

667
00:59:14,560 --> 00:59:20,160
called The Birth of the Mind. Like how is it the genes even build innate knowledge? And from the

668
00:59:20,160 --> 00:59:24,480
perspective of the conversation we're having today, there's actually two questions. One is what

669
00:59:24,480 --> 00:59:30,800
innate knowledge or mechanisms or what have you? People or other animals might be endowed with.

670
00:59:30,800 --> 00:59:35,680
I always like showing this video of a baby ibex climbing down a mountain. That baby ibex, you

671
00:59:35,680 --> 00:59:39,760
know, a few hours after his birth, knows how to climb down a mountain. That means that it knows,

672
00:59:39,760 --> 00:59:45,920
not consciously, something about its own body and physics and 3D geometry and all of this kind of

673
00:59:45,920 --> 00:59:51,200
stuff. So there's one question about it. Like what does biology give its creatures? You know,

674
00:59:51,280 --> 00:59:55,440
what has evolved in our brains? How is that represented in our brains? The question I

675
00:59:55,440 --> 00:59:59,200
thought about in the book The Birth of the Mind. And then there's a question of what AI should have.

676
00:59:59,200 --> 01:00:06,960
And they don't have to be the same. But I would say that it's a pretty interesting

677
01:00:06,960 --> 01:00:11,040
set of things that we are equipped with that allows us to do a lot of interesting things. So

678
01:00:11,040 --> 01:00:15,120
I would argue or guess based on my reading of the developmental psychology literature,

679
01:00:15,120 --> 01:00:21,840
which I've also participated in, that children are born with a notion of space,

680
01:00:21,840 --> 01:00:29,200
time, other agents, places, and also this kind of mental algebra that I was describing before.

681
01:00:30,160 --> 01:00:35,760
No certain of causation, if I didn't just say that. So at least those kinds of things. They're

682
01:00:35,760 --> 01:00:41,120
like frameworks for learning the other things. So are they disjoint in your view or is it just

683
01:00:41,200 --> 01:00:47,520
somehow all connected? You've talked a lot about language. Is it all kind of connected in some

684
01:00:47,520 --> 01:00:54,000
mesh that's language like? If understanding concepts all together? I don't think we know

685
01:00:54,000 --> 01:00:58,960
for people how they're represented in machines just don't really do this yet. So I think it's an

686
01:00:58,960 --> 01:01:04,880
interesting open question both for science and for engineering. Some of it has to be at least

687
01:01:05,520 --> 01:01:10,880
interrelated in the way that like the interfaces of a software package have to be able to talk to

688
01:01:10,880 --> 01:01:19,200
one another. So the systems that represent space and time can't be totally disjoint because a lot

689
01:01:19,200 --> 01:01:23,120
of the things that we reason about are the relations between space and time and cause. So

690
01:01:24,240 --> 01:01:28,640
I put this on and I have expectations about what's going to happen with the bottle cap on top of the

691
01:01:28,640 --> 01:01:36,000
bottle and those span space and time. If the cap is over here, I get a different outcome. If

692
01:01:36,800 --> 01:01:41,120
the timing is different, if I put this here after I move that, then I get a different outcome

693
01:01:41,760 --> 01:01:46,640
that relates to causality. So obviously these mechanisms, whatever they are,

694
01:01:47,760 --> 01:01:53,440
can certainly communicate with each other. So I think evolution had a significant role to play

695
01:01:53,440 --> 01:01:59,360
in the development of this whole collage. How efficient do you think is evolution? Oh, it's

696
01:01:59,360 --> 01:02:09,920
terribly inefficient except that it's inefficient except that once it gets a good idea, it runs with

697
01:02:09,920 --> 01:02:21,120
it. So it took I guess a billion years, roughly a billion years to evolve to a vertebrate

698
01:02:21,440 --> 01:02:30,160
brain plan. Once that vertebrate plan evolved, it spread everywhere. So fish have it and dogs

699
01:02:30,160 --> 01:02:35,360
have it and we have it. We have adaptations of it and specializations of it and the same thing

700
01:02:35,360 --> 01:02:41,520
with a primate brain plan. So monkeys have it and apes have it and we have it. So there are

701
01:02:41,520 --> 01:02:47,280
additional innovations like color vision and those spread really rapidly. So it takes evolution a

702
01:02:47,280 --> 01:02:54,480
long time to get a good idea and being anthropomorphic and not literal here. But once it has that

703
01:02:54,480 --> 01:02:59,440
idea, so to speak, which caches out into one set of genes or in the genome, those genes spread

704
01:02:59,440 --> 01:03:04,000
very rapidly and they're like subroutines or libraries, I guess the word people might use

705
01:03:04,000 --> 01:03:08,720
nowadays or be more familiar with. They're libraries that can get used over and over again.

706
01:03:08,720 --> 01:03:13,680
So once you have the library for building something with multiple digits, you can use it for a hand

707
01:03:13,680 --> 01:03:18,000
but you can also use it for a foot. You just kind of reuse the library with slightly different

708
01:03:18,000 --> 01:03:23,520
parameters. Evolution does a lot of that, which means that the speed over time picks up. So

709
01:03:23,520 --> 01:03:29,200
evolution can happen faster because you have bigger and bigger libraries. And what I think has

710
01:03:29,920 --> 01:03:37,200
happened in attempts at evolutionary computation is that people start with libraries that are

711
01:03:38,080 --> 01:03:45,520
very, very minimal, like almost nothing and then progress is slow and it's hard for someone to get

712
01:03:45,520 --> 01:03:50,560
a good PhD thesis out of it and then give up. If we had richer libraries to begin with, if you were

713
01:03:50,560 --> 01:03:56,640
evolving from systems that hadn't originate structure to begin with, then things might speed up.

714
01:03:56,640 --> 01:04:01,600
Or more PhD students, if the evolution process is indeed in a meta way,

715
01:04:01,840 --> 01:04:06,080
runs away with good ideas, you need to have a lot of ideas,

716
01:04:06,640 --> 01:04:10,320
pool of ideas in order for it to discover one that you can run away with. And

717
01:04:10,320 --> 01:04:13,120
PhD students representing individual ideas as well.

718
01:04:13,120 --> 01:04:16,160
Yeah. I mean, you could throw a billion PhD students at it.

719
01:04:16,160 --> 01:04:18,720
Yeah. The monkeys are typewriters with Shakespeare. Yep.

720
01:04:20,000 --> 01:04:24,480
Well, I mean, those aren't cumulative, right? That's just random. And part of the point that

721
01:04:24,480 --> 01:04:31,040
I'm making is that evolution is cumulative. So if you have a billion monkeys independently,

722
01:04:31,040 --> 01:04:34,720
you don't really get anywhere. But if you have a billion monkeys, I think Dawkins made this

723
01:04:34,720 --> 01:04:38,640
point originally, or probably other people. Dawkins made it very nice in either selfish

724
01:04:38,640 --> 01:04:44,800
gene or blind watchmaker. If there is some sort of fitness function that can drive you toward

725
01:04:44,800 --> 01:04:49,440
something, I guess that's Dawkins' point. And my point, which is a little variation on that,

726
01:04:49,440 --> 01:04:55,520
is that if the evolution is cumulative of the related points, then you can start going faster.

727
01:04:55,520 --> 01:04:59,360
Do you think something like the process of evolution is required to build the intelligence

728
01:04:59,360 --> 01:05:05,760
systems? Not logically. So all the stuff that evolution did, a good engineer might be able to

729
01:05:05,760 --> 01:05:13,680
do. So for example, evolution made quadrupeds, which distribute the load across a horizontal

730
01:05:13,680 --> 01:05:18,240
surface. A good engineer could come up with that idea. I mean, sometimes good engineers come up

731
01:05:18,240 --> 01:05:23,600
with ideas by looking at biology. There's lots of ways to get your ideas. Part of what I'm suggesting

732
01:05:23,600 --> 01:05:29,440
is we should look at biology a lot more. We should look at the biology of thought and the

733
01:05:29,440 --> 01:05:35,840
understanding and the biology by which creatures intuitively reason about physics or other agents

734
01:05:35,840 --> 01:05:40,400
or how do dogs reason about people? They're actually pretty good at it. If we could understand,

735
01:05:41,760 --> 01:05:47,520
at my college we joked, dognition. If we could understand dognition well and how it was implemented,

736
01:05:47,600 --> 01:05:55,680
that might help us with our AI. Do you think it's possible that the kind of timescale that

737
01:05:55,680 --> 01:06:00,400
evolution took is the kind of timescale that will be needed to build intelligence systems,

738
01:06:00,400 --> 01:06:03,760
or can we significantly accelerate that process inside a computer?

739
01:06:05,520 --> 01:06:09,440
I think the way that we accelerate that process is we borrow from biology.

740
01:06:10,560 --> 01:06:15,600
Not slavishly, but I think we look at how biology has solved problems and we say,

741
01:06:15,600 --> 01:06:21,040
does that inspire any engineering solutions here? Try to mimic biological systems and then

742
01:06:21,040 --> 01:06:26,960
therefore have a shortcut. There's a field called biomimicry. People do that for material

743
01:06:26,960 --> 01:06:34,400
science all the time. We should be doing the analog of that for AI. The analog for that for AI

744
01:06:34,400 --> 01:06:38,640
is to look at cognitive science or the cognitive sciences, which is psychology,

745
01:06:38,640 --> 01:06:42,720
maybe neuroscience, linguistics, and so forth. Look to those for insight.

746
01:06:43,280 --> 01:06:45,680
What do you think is a good test of intelligence in your view?

747
01:06:46,480 --> 01:06:52,080
I don't think there's one good test. In fact, I try to organize a movement towards something

748
01:06:52,080 --> 01:06:57,120
called a Turing Olympics. My hope is that Francois is actually going to take, Francois Chalet,

749
01:06:57,120 --> 01:07:01,840
is going to take over this. I think he's interested and I just don't have place in my

750
01:07:01,840 --> 01:07:07,840
busy life at this moment. The notion is that there'll be many tests and not just one because

751
01:07:07,920 --> 01:07:14,000
intelligence is multifaceted. There can't really be a single measure of it because it isn't a single

752
01:07:14,000 --> 01:07:19,840
thing. Just at the crudest level, the SAT is a verbal component and a math component because

753
01:07:19,840 --> 01:07:23,920
they're not identical. Howard Gardner has talked about multiple intelligence like

754
01:07:23,920 --> 01:07:28,880
kinesthetic intelligence and verbal intelligence and so forth. There are a lot of things that go

755
01:07:28,880 --> 01:07:34,640
into intelligence and people can get good at one or the other. In some sense, every expert has

756
01:07:34,720 --> 01:07:39,120
developed a very specific kind of intelligence and then there are people that are generalists.

757
01:07:39,920 --> 01:07:44,080
I think of myself as a generalist with respect to cognitive science, which doesn't mean I know

758
01:07:44,080 --> 01:07:48,240
anything about quantum mechanics, but I know a lot about the different facets of the mind.

759
01:07:49,920 --> 01:07:53,760
There's a kind of intelligence to thinking about intelligence. I like to think that I have some

760
01:07:53,760 --> 01:07:59,120
of that, but social intelligence, I'm just okay. There are people that are much better at that than

761
01:07:59,120 --> 01:08:06,160
I am. Sure, but what would be really impressive to you? I think the idea of a touring Olympics is

762
01:08:06,160 --> 01:08:11,920
really interesting, especially if somebody like Francois is running it, but to you in general,

763
01:08:13,360 --> 01:08:17,600
not as a benchmark, but if you saw an AI system being able to accomplish something

764
01:08:18,400 --> 01:08:23,680
that would impress the heck out of you, what would that thing be? Would it be natural language

765
01:08:23,680 --> 01:08:29,840
conversation? For me personally, I would like to see a kind of comprehension that relates

766
01:08:29,840 --> 01:08:36,080
to what you just said. I wrote a piece in The New Yorker in I think 2015, right after Eugene

767
01:08:36,080 --> 01:08:46,000
Gustman, which was a software package, won a version of the touring test. The way you win

768
01:08:46,000 --> 01:08:51,040
the touring test, it's called win it, is the touring test is you fool a person into thinking

769
01:08:52,000 --> 01:08:58,320
that a machine is a person. You're evasive, you pretend to have limitations, so you don't have

770
01:08:58,320 --> 01:09:04,000
to answer certain questions and so forth. This particular system pretended to be a 13-year-old

771
01:09:04,000 --> 01:09:08,640
boy from Odessa who didn't understand English and was kind of sarcastic and wouldn't answer your

772
01:09:08,640 --> 01:09:13,440
questions and so forth. Judges got fooled into thinking briefly with a very little exposure

773
01:09:13,440 --> 01:09:17,360
as a 13-year-old boy, and it docked to all the questions the touring was actually interested

774
01:09:17,360 --> 01:09:21,600
in, which is like, how do you make the machine actually intelligent? That test itself is not

775
01:09:21,600 --> 01:09:27,040
that good. In The New Yorker, I proposed an alternative, I guess. The one that I proposed

776
01:09:27,040 --> 01:09:31,680
there was a comprehension test. I must like Breaking Bad because I've already given you

777
01:09:31,680 --> 01:09:36,640
one Breaking Bad example. In that article, I have one as well, which was something like,

778
01:09:36,640 --> 01:09:40,880
if Walter White, you should be able to watch an episode of Breaking Bad or maybe you have to

779
01:09:40,880 --> 01:09:44,800
watch the whole series to be able to answer the question and say, if Walter White took a hit

780
01:09:44,800 --> 01:09:49,920
out on Jesse, why did he do that? If you could answer kind of arbitrary questions about characters

781
01:09:49,920 --> 01:09:54,320
motivations, I would be really impressed with that. I mean, you build software to do that.

782
01:09:55,200 --> 01:10:00,640
They could watch a film or they're different versions. Ultimately, I wrote this up with

783
01:10:00,640 --> 01:10:05,280
Praveen Paratosh in a special issue of AI magazine that basically was about the Touring

784
01:10:05,280 --> 01:10:09,600
Olympics. There were like 14 tests proposed. The one that I was pushing was a comprehension

785
01:10:09,600 --> 01:10:14,080
challenge and Praveen, who's at Google, was trying to figure out how we would actually run it. We

786
01:10:14,080 --> 01:10:18,960
wrote a paper together. You could have a text version too. You could have an auditory podcast

787
01:10:18,960 --> 01:10:24,640
version. You could have a written version. But the point is that you win at this test if you can do,

788
01:10:24,640 --> 01:10:30,160
let's say, human level or better than humans at answering kind of arbitrary questions. Why did this

789
01:10:30,160 --> 01:10:34,640
person pick up the stone? What were they thinking when they picked up the stone? Were they trying to

790
01:10:34,640 --> 01:10:39,360
knock down glass? I mean, ideally, these wouldn't be multiple choice either because multiple choice

791
01:10:39,360 --> 01:10:45,760
is pretty easily gamed. If you could have relatively open-ended questions and you can answer why people

792
01:10:45,760 --> 01:10:50,880
are doing this stuff, I would be very impressed. Of course, humans can do this. If you watch a

793
01:10:50,880 --> 01:10:57,600
well-constructed movie and somebody picks up a rock, everybody watching the movie knows why they

794
01:10:57,600 --> 01:11:03,520
picked up the rock. They all know, oh my gosh, he's going to hit this character or whatever.

795
01:11:03,520 --> 01:11:08,720
We have an example in the book about when a whole bunch of people say, I am Spartacus.

796
01:11:08,720 --> 01:11:18,240
You know this famous scene? The viewers understand, first of all, everybody or everybody minus one

797
01:11:18,240 --> 01:11:22,000
has to be lying. They can't all be Spartacus. We have enough common sense knowledge to know

798
01:11:22,000 --> 01:11:26,560
they couldn't all have the same name. We know that they're lying and we can infer why they're

799
01:11:26,560 --> 01:11:31,040
lying. They're lying to protect someone and to protect things they believe in. You get a machine

800
01:11:31,040 --> 01:11:36,240
that can do that. They can say, this is why these guys all got up and said, I am Spartacus.

801
01:11:36,880 --> 01:11:41,360
I will sit down and say AI has really achieved a lot. Thank you.

802
01:11:41,360 --> 01:11:46,320
Without cheating any part of the system. Yeah. If you do it, there are lots of ways you can

803
01:11:46,320 --> 01:11:51,120
cheat. You could build a Spartacus machine that works on that film. That's not what I'm talking

804
01:11:51,120 --> 01:11:55,680
about. You can do this with essentially arbitrary films from a large set.

805
01:11:55,680 --> 01:12:00,560
Even beyond films because it's possible such a system would discover that the number of narrative

806
01:12:00,560 --> 01:12:06,400
arcs in film is limited to like 93. Well, there's a famous thing about the classic seven plots or

807
01:12:06,400 --> 01:12:10,720
whatever. I don't care if you want to build in the system, boy meets girl, boy loses girl,

808
01:12:10,720 --> 01:12:14,400
boy finds girl. That's fine. I don't mind having some headstone. Any knowledge?

809
01:12:16,240 --> 01:12:20,400
I mean, you could build it in an Ailey or you could have your system watch a lot of films again.

810
01:12:20,400 --> 01:12:26,000
If you can do this at all, but with a wide range of films, not just one film in one genre,

811
01:12:26,960 --> 01:12:30,080
but even if you could do it for all Westerns, I'd be reasonably impressed.

812
01:12:31,920 --> 01:12:37,520
So in terms of being impressed just for the fun of it, because you've put so many interesting

813
01:12:37,520 --> 01:12:44,480
ideas out there in your book, challenging the community for further steps, is it possible

814
01:12:44,480 --> 01:12:51,360
on the deep learning front that you're wrong about its limitations? That deep learning will

815
01:12:51,360 --> 01:12:56,160
unlock. Yanlacoon next year will publish a paper that achieves this comprehension.

816
01:12:56,800 --> 01:13:03,360
So do you think that way often as a scientist, do you consider that your intuition that deep

817
01:13:03,360 --> 01:13:10,880
learning could actually run away with it? I'm worried about rebranding as a kind of political

818
01:13:10,880 --> 01:13:16,320
thing. So I mean, what's going to happen, I think, is that deep learning is going to start to encompass

819
01:13:16,320 --> 01:13:20,880
simple manipulation. So I think Hinton's just wrong. Hinton says we don't want hybrids. I think

820
01:13:20,880 --> 01:13:24,800
people will work towards hybrids and they will relabel their hybrids as deep learning. We've

821
01:13:24,800 --> 01:13:30,000
already seen some of that. So AlphaGo is often described as a deep learning system, but it's

822
01:13:30,000 --> 01:13:33,760
more correctly described as a system that has deep learning, but also Monte Carlo tree search,

823
01:13:33,760 --> 01:13:38,960
which is a classical AI technique. And people will start to blur the lines in the way that IBM

824
01:13:38,960 --> 01:13:42,000
blurred Watson. First, Watson meant this particular system, and then it was just

825
01:13:42,000 --> 01:13:45,680
anything that IBM built in their cognitive division. But purely, let me ask for sure,

826
01:13:45,920 --> 01:13:51,920
that's a branding question, and that's a giant mess. I mean, purely a single neural network

827
01:13:51,920 --> 01:13:55,440
being able to accomplish reasonable comprehension. I don't step at night

828
01:13:55,440 --> 01:13:59,920
worrying that that's going to happen. And I'll just give you two examples. One is

829
01:14:00,560 --> 01:14:06,720
a guy at DeepMind thought he had finally outfoxed me at Xergy Lord, I think is his Twitter handle.

830
01:14:07,920 --> 01:14:13,840
And he specifically made an example. Marcus said that such and such, he fed it into GP2,

831
01:14:14,800 --> 01:14:19,280
which is the AI system that is so smart that OpenAI couldn't release it because it would

832
01:14:19,280 --> 01:14:25,440
destroy the world, right? You remember that a few months ago. So he feeds it into GPT2.

833
01:14:26,000 --> 01:14:29,600
And my example was something like a rose is a rose, a tulip is a tulip,

834
01:14:30,160 --> 01:14:34,080
a lily is a blank. And he got it to actually do that, which was a little bit impressive. And I

835
01:14:34,080 --> 01:14:39,280
wrote back and I said, that's impressive. But can I ask you a few questions? I said, was that just

836
01:14:39,280 --> 01:14:43,840
one example? Can it do it generally? And can it do it with novel words, which is part of what I

837
01:14:43,840 --> 01:14:51,120
was talking about in 1998, when I first raised the example. So a DAX is a DAX, right? And he

838
01:14:51,120 --> 01:14:54,720
sheepishly wrote back about 20 minutes later and the answer was, well, it had some problems with

839
01:14:54,720 --> 01:15:01,840
those. So I made some predictions 21 years ago that still hold in the world of computer science.

840
01:15:01,840 --> 01:15:08,320
That's amazing, right? Because there's a thousand or a million times more memory and computations,

841
01:15:08,400 --> 01:15:14,320
million times, do million times more operations per second, spread across a cluster,

842
01:15:15,360 --> 01:15:24,080
and there's been advances in replacing sigmoids with other functions and so forth. There's all

843
01:15:24,080 --> 01:15:28,240
kinds of advances, but the fundamental architecture hasn't changed and the fundamental limit hasn't

844
01:15:28,240 --> 01:15:33,040
changed. And what I said then is kind of still true. And then here's a second example. I recently

845
01:15:33,040 --> 01:15:40,080
had a piece in WIRED that's adapted from the book. And the book was when to press before GP2 came out.

846
01:15:40,080 --> 01:15:46,480
But we describe this children's story and all the inferences that you make in this story about a boy

847
01:15:46,480 --> 01:15:53,920
finding a lost wallet. And for fun, in the WIRED piece, we ran it through GP2. GPT2 at something

848
01:15:53,920 --> 01:15:58,720
called TalkToTransformer.com, and your viewers can try this experiment themselves, go to the WIRED

849
01:15:58,800 --> 01:16:05,280
piece that has the link and has the story. And the system made perfectly fluent text that was

850
01:16:05,280 --> 01:16:11,840
totally inconsistent with the conceptual underpinnings of the story. And this is what, again,

851
01:16:11,840 --> 01:16:16,880
I predicted in 1998. And for that matter, Chomsky Miller made the same prediction in 1963. I was

852
01:16:16,880 --> 01:16:23,040
just updating their claim for a slightly new text. So those particular architectures that

853
01:16:23,040 --> 01:16:28,400
don't have any bills to acknowledge, they're basically just a bunch of layers doing correlational

854
01:16:28,480 --> 01:16:33,440
stuff. They're not going to solve these problems. So 20 years ago, you said the emperor has no

855
01:16:33,440 --> 01:16:38,720
clothes. Today, the emperor still has no clothes. The lighting's better, though. The lighting is

856
01:16:38,720 --> 01:16:43,920
better. And I think you yourself are also, I mean, and we found out some things to do with naked

857
01:16:43,920 --> 01:16:48,800
emperors. I mean, it's not like stuff is worthless. I mean, they're not really naked. It's more like

858
01:16:48,800 --> 01:16:53,280
they're in their briefs and everybody thinks that. And so like, I mean, they are great at speech

859
01:16:53,280 --> 01:16:57,680
recognition. But the problems that I said were hard, because I didn't literally say the emperor

860
01:16:57,680 --> 01:17:02,240
has no clothes. I said, this is a set of problems that humans are really good at. And it wasn't

861
01:17:02,240 --> 01:17:07,760
couched as AI. It was couched as cognitive science. But I said, if you want to build a neural model

862
01:17:07,760 --> 01:17:12,080
of how humans do certain class of things, you're going to have to change the architecture. And

863
01:17:12,080 --> 01:17:18,400
I stand by those claims. And I think people should understand you're quite entertaining in your

864
01:17:18,400 --> 01:17:24,320
cynicism, but you're also very optimistic and a dreamer about the future of AI, too. So you're

865
01:17:24,320 --> 01:17:30,640
both is just there's a famous saying about being people overselling technology in the short run

866
01:17:30,640 --> 01:17:38,560
and underselling it in the long run. And so I actually end the book or Ernie Davis and I end

867
01:17:38,560 --> 01:17:43,520
our book with an optimistic chapter, which kind of killed Ernie because he's even more pessimistic

868
01:17:43,520 --> 01:17:49,280
than I am. He describes me as a contrarian and him as a pessimist. But I persuaded that we should

869
01:17:49,280 --> 01:17:55,280
end the book with a look at what would happen if AI really did incorporate, for example,

870
01:17:55,280 --> 01:17:59,680
the common sense reasoning and the nativism and so forth, the things that we counseled for. And

871
01:17:59,680 --> 01:18:05,680
we wrote it and it's an optimistic chapter that AI suitably reconstructed so that we could trust

872
01:18:05,680 --> 01:18:11,760
it, which we can't now could really be world changing. So on that point, if you look at the

873
01:18:11,760 --> 01:18:17,600
future trajectories of AI, people have worries about negative effects of AI, whether it's

874
01:18:18,160 --> 01:18:25,280
at the large existential scale or smaller short term scale of negative impact on society. So

875
01:18:25,280 --> 01:18:31,760
you write about trustworthy AI. How can we build AI systems that align with our values that make

876
01:18:31,760 --> 01:18:35,840
for a better world that we can interact with that we can trust? The first thing we have to do is to

877
01:18:35,840 --> 01:18:42,800
replace deep learning with deep understanding. So you can't have alignment with a system that

878
01:18:42,800 --> 01:18:47,200
traffics only in correlations and doesn't understand concepts like bottles or harm.

879
01:18:48,880 --> 01:18:55,360
Asimov talked about these famous laws. The first one was first do no harm. And you can quibble

880
01:18:55,360 --> 01:18:58,800
about the details of Asimov's laws, but we have to if we're going to build real robots in the

881
01:18:58,800 --> 01:19:02,880
real world have something like that. That means we have to program in a notion that's at least

882
01:19:02,880 --> 01:19:07,120
something like harm. That means we have to have these more abstract ideas that deep learning is

883
01:19:07,120 --> 01:19:12,320
not particularly good at. They have to be in the mix somewhere. You could do statistical analysis

884
01:19:12,320 --> 01:19:16,080
about probabilities of given harms or whatever, but you have to know what a harm is in the same

885
01:19:16,080 --> 01:19:19,360
way that you have to understand that a bottle isn't just a collection of pixels.

886
01:19:23,280 --> 01:19:28,320
You're implying that you need to also be able to communicate that to humans. So the AI systems

887
01:19:28,320 --> 01:19:35,360
would be able to prove to humans that they understand that they know what harm means.

888
01:19:35,360 --> 01:19:38,800
I might run it in the reverse direction, but roughly speaking, I agree with you. So

889
01:19:39,520 --> 01:19:44,880
we probably need to have committees of wise people, ethicists, and so forth,

890
01:19:45,600 --> 01:19:49,840
think about what these rules ought to be, and we should just leave it to software engineers.

891
01:19:49,840 --> 01:19:54,000
It shouldn't just be software engineers, and it shouldn't just be people who

892
01:19:54,720 --> 01:19:59,840
own large mega corporations that are good at technology. Ethicists and so forth should be

893
01:19:59,840 --> 01:20:05,280
involved. But there should be some assembly of wise people, as I was putting it,

894
01:20:06,080 --> 01:20:11,280
that tries to figure out what the rules ought to be, and those have to get translated into code.

895
01:20:12,400 --> 01:20:15,920
You can argue code or neural networks or something. They have to be

896
01:20:17,360 --> 01:20:22,240
translated into something that machines can work with, and that means there has to be a way of

897
01:20:22,240 --> 01:20:26,560
working the translation, and right now we don't. We don't have a way. So let's say you and I were

898
01:20:26,560 --> 01:20:30,720
the committee, and we decide that Asimov's first law is actually right, and let's say it's not just

899
01:20:30,720 --> 01:20:35,920
two white guys, which would be unfortunate, and then we have a broad representative sample of the

900
01:20:35,920 --> 01:20:41,360
world or however we want to do this, and the committee decides eventually, okay, Asimov's

901
01:20:41,360 --> 01:20:44,800
first law is actually pretty good. There are these exceptions to it. We want to program in

902
01:20:44,800 --> 01:20:48,800
these exceptions, but let's start with just the first one, and then we'll get to the exceptions.

903
01:20:48,800 --> 01:20:54,080
First one is first do no harm. Well, somebody has to now actually turn that into a computer

904
01:20:54,080 --> 01:20:58,960
program or a neural network or something, and one way of taking the whole book, the whole

905
01:20:59,040 --> 01:21:03,520
argument that I'm making is that we just don't have to do that yet, and we're fooling ourselves

906
01:21:03,520 --> 01:21:09,200
if we think that we can build trustworthy AI. If we can't even specify in any kind of,

907
01:21:09,200 --> 01:21:14,000
you know, we can't do it in Python and we can't do it in TensorFlow, we're fooling ourselves

908
01:21:14,000 --> 01:21:18,960
and thinking that we can make trustworthy AI if we can't translate harm into something that we

909
01:21:18,960 --> 01:21:24,320
can execute, and if we can't, then we should be thinking really hard, how could we ever do such

910
01:21:24,320 --> 01:21:29,200
a thing? Because if we're going to use AI in the ways that we want to use it to make job interviews

911
01:21:29,200 --> 01:21:32,880
or to do surveillance, not that I personally want to do that, or whatever, I mean, if we're going to

912
01:21:32,880 --> 01:21:38,960
use AI in ways that have practical impact on people's lives or medicine, it's got to be able

913
01:21:38,960 --> 01:21:44,160
to understand stuff like that. So one of the things your book highlights is that, you know,

914
01:21:44,880 --> 01:21:50,560
a lot of people in the deep learning community, but also the general public, politicians, just

915
01:21:50,560 --> 01:21:57,280
people in all general groups and walks of life have different levels of misunderstanding of AI.

916
01:21:57,280 --> 01:22:06,800
So when you talk about committees, what's your advice to our society? How do we grow? How do we

917
01:22:06,800 --> 01:22:12,640
learn about AI such that such committees could emerge, where large groups of people could have

918
01:22:13,440 --> 01:22:17,760
a productive discourse about how to build successful AI systems?

919
01:22:17,760 --> 01:22:22,880
Part of the reason we wrote the book was to try to inform those committees. So part of the reason

920
01:22:22,880 --> 01:22:27,040
we wrote the book was to inspire a future generation of students to solve what we think are the

921
01:22:27,040 --> 01:22:31,120
important problems. So a lot of the book is trying to pinpoint what we think are the hard problems

922
01:22:31,120 --> 01:22:38,160
where we think effort would most be rewarded. And part of it is to try to train people who

923
01:22:38,160 --> 01:22:43,360
talk about AI, but aren't experts in the field to understand what's realistic and what's not.

924
01:22:43,360 --> 01:22:46,080
One of my favorite parts in the book is the six questions you should ask

925
01:22:46,880 --> 01:22:51,040
anytime you read a media account. So number one is if somebody talks about something,

926
01:22:51,040 --> 01:22:55,440
look for the demo. If there's no demo, don't believe it. The demo that you can try. If you

927
01:22:55,440 --> 01:23:00,240
can't try it at home, maybe it doesn't really work that well yet. So we don't have this example

928
01:23:00,240 --> 01:23:06,560
in the book. But if Sundar Pinchai says, we have this thing that allows it to sound like human

929
01:23:06,560 --> 01:23:12,000
beings in conversation, you should ask, can I try it? And you should ask how general it is. And it

930
01:23:12,000 --> 01:23:16,560
turns out at that time, I'm alluding to Google Duplex, when it was announced, it only worked on

931
01:23:16,560 --> 01:23:21,200
calling hairdressers, restaurants, and finding opening hours. That's not very general. That's

932
01:23:21,200 --> 01:23:26,960
narrow AI. And I'm not going to ask your thoughts about Sophia. But yeah, I understand that's a

933
01:23:26,960 --> 01:23:32,160
really good question to ask of any kind of high-top idea. So Sophia has very good material written

934
01:23:32,160 --> 01:23:37,360
for her, but she doesn't understand the things that she's saying. So a while ago, you've written a

935
01:23:37,360 --> 01:23:42,400
book on the science of learning, which I think is fascinating. But the learning case studies of

936
01:23:42,400 --> 01:23:48,640
playing guitar, guitar zero, I love guitar myself, I've been playing my whole life. So let me ask

937
01:23:48,640 --> 01:23:56,080
a very important question. What is your favorite song, rock song to listen to or try to play?

938
01:23:56,080 --> 01:23:59,920
Well, those would be different. But I'll say that my favorite rock song to listen to is probably

939
01:23:59,920 --> 01:24:04,000
all along the Watchtower, the Jimi Hendrix version. Jimi Hendrix version feels magic to me.

940
01:24:04,720 --> 01:24:08,560
I've actually recently learned that I love that song. I've been trying to put it on YouTube,

941
01:24:08,560 --> 01:24:13,280
myself singing. Singing is the scary part. If you could party with a rock star for a weekend,

942
01:24:13,280 --> 01:24:20,080
living or dead, who would you choose? And pick their mind, it's not necessarily about the partying.

943
01:24:21,040 --> 01:24:28,240
Thanks for the clarification. I guess John Lennon is such an intriguing person. I mean,

944
01:24:28,480 --> 01:24:34,640
a troubled person, but an intriguing one. So beautiful. Well, Imagine is one of my favorite

945
01:24:34,640 --> 01:24:38,480
songs. So also one of my favorite songs. That's a beautiful way to end it. Gary,

946
01:24:38,480 --> 01:24:40,560
thank you so much for talking to me. Thanks so much for having me.

