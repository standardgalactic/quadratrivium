start	end	text
0	5920	The video you're watching now is in 360. Resolution is not great but we wanted to
5920	10280	try something different. So if you're on a desktop or laptop you can pan around
10280	13800	with your mouse or if you're on a phone or tablet you should be able to just
13800	18960	move your device to look around. Of course it's best viewed with a VR headset.
18960	23320	The video that follows is a guest lecture on machine learning that I gave an
23320	28560	MIT Sloan course on the business of artificial intelligence. The lecture is
28560	33540	non-technical and intended to build intuition about these ideas amongst the
33540	38400	business students in the audience. The room was a half circle so we thought why
38400	43880	not film the lecture in 360. We recorded a screencast of the slides and pasted it
43880	48480	into the video so that the slides are more crisp. Let me know what you think and
48480	56120	remember it's an experiment. So this course is talking about the broad context
56120	59880	the impact of artificial intelligence. The global. There's global which is the
59880	63240	global impact of artificial intelligence. There's the business which is when you
63240	67480	have to take these fun research ideas that I'll talk about today. A lot of them
67480	72580	are cool on toy examples. When you bring them to reality you face real challenges
72580	78040	which is what I would like to really highlight today. That's the business part.
78040	81440	When you want to make real impact. When you're going to make these technologies
81440	86920	a reality. So I'll talk about how amazing the technology is for a nerd like me but
86920	92160	also talk about how when you take that into the real world what are the
92160	96960	challenges you face. So machine learning which is the technology at the core of
96960	103560	artificial intelligence. We'll talk about the promise. The excitement that I feel
103560	108000	about it. The limitations. We'll bring it down a little bit. What are the real
108040	115800	capabilities of technology. Where for the first time really as a civilization
115800	122360	exploring the meaning of intelligence. It is if you pause for a second and just
122360	125200	think you know maybe you have many of you want to make money out of this
125200	130920	technology. Many of you want to save lives help people but also on the
130920	136200	philosophical level we get to explore what makes us human. So while I'll talk
136280	141320	about the low-level technologies also think about the incredible opportunity
141320	146960	here. We get to almost psychoanalyze ourselves by trying to build versions of
146960	153640	ourselves in the machine. All right so here's the open question. How powerful is
153640	158480	artificial intelligence. How powerful is machine learning that lies at the core
158480	163120	of artificial intelligence. Is it simply a helpful tool a special purpose tool to
163120	169000	help you solve simple problems. Which is what it currently is. Currently
169000	174760	machine learning artificial intelligence is a way if you can formally define the
174760	178240	problem. You can formally define the tools you're working with. You can formally
178240	181800	define the utility function where you want to achieve with those tools. As long
181800	185720	as you can define those things we can come up with algorithms that can solve
185720	189600	them. As long as you have the right kind of data which is what I'll talk about.
189960	200200	Data is key. And the question is into the future can we break past this very
200200	202920	narrow definition of what machine learning can give us which is solve
202920	209440	specific problems. To something bigger. To where we approach the general
209440	213000	intelligence that we exhibit as human beings. When we're born we know
213080	221240	nothing and we learn quickly. From very little data. The right answer is we
221240	225920	don't know. We don't know what are the limitations of the technology. What kind
225920	231120	of machine learning are there. There are several flavors. The first two is what's
231120	235720	really the first is what's achieved success today. Supervised learning. What
235720	241760	I'm showing here on the left of the slide is the teachers. Is the data that is
241840	245440	fed to the system. And on the right is the students which is the system itself
245440	249280	from machine learning. So there's supervised learning. Whenever everybody
249280	253960	talks about machine learning today. For the most part they're referring to
253960	257600	supervised learning. Which means every single piece of data that is used to
257600	263720	train the model is seen by human eyes. And those human eyes with an
263720	269800	accompanying brain label that data in a way that makes it useful to the machine.
270240	275080	This is critical. Because that's one. The blue box. The human. It's really
275080	280760	costly. So whenever every single piece of data that's used to train the machine
280760	284800	needs to be seen by a human. You need to pay for that human. And second you're
284800	291120	limited to just the time. There's the amount of data necessary to label what
291120	297760	it means to exist in this world is humongous. Augmented supervised learning
297800	301240	is when you get machine to really to help you a little bit. There's a few
301240	304520	tricks there. But still it's still only tricks. It's still the human is at the
304520	311480	core of it. And the promise of future research that we're pursuing that I'm
311480	314880	pursuing and perhaps in the applications if we get to discuss or some of the
314880	320200	speakers here get to discuss. They're pursuing in semi-supervised and
320200	324080	reinforcement learning. Where the human starts to play a smaller and smaller
324080	328240	role in how much they get to annotate. They have to annotate the data. And the
328240	334880	dream of the sort of wizards of the dark arts of deep learning are all excited
334880	340640	about unsupervised learning. That has very few actual successes in application
340640	348880	in the real world today. But it is the idea that you can build a machine that
348880	357120	doesn't require a human teacher. A human being to teach you anything. It fills us
357120	365320	artificial intelligence researchers with excitement. There's a theme here.
365320	372600	Machine learning is really simple. The learning system in the middle. There's a
372600	377920	training stage where you teach it something. All you need is some data.
377960	386640	Input data. And you need to teach it the correct output for that input data. So
386640	392080	you have to have a lot of pairs of input data and correct output. There'll be a
392080	395280	theme of cats throughout this presentation. So if you want to teach a
395280	401400	system difference between a cat and a dog, you need a lot of images of cats. And
401400	405120	you need to tell it that this is a cat. This bounding box here in the image is a
405160	410800	cat. You have to give it a lot of images of dogs and tell it, okay, in these
410800	416520	pictures there are dogs. And then there's a spelling mistake on the second
416520	420800	stage is the testing stage. When you actually give it new input data, it's
420800	426160	never seen before. And you hope that it has given for cat versus dog enough data
426160	434920	to guess is this new image that I've never seen before, a cat or a dog. Now
435680	444160	one of the open questions you want to keep in mind is what in this world can we
444160	455680	not model in this way? What activity? What task? What goal? I offer to you that
455680	468360	there's nothing you can't model in this way. So let's think about what in terms
468360	474360	of machine learning can be, so let's start small. What can be modeled in this
474360	479280	way? First on the bottom of the slide left is one to one mapping where the
479280	484320	input is an image of a cat and the output is a label that says cat or dog.
485080	490760	You can also do one to many where the image, the input is an image of a cat and
490760	498160	the output is a story about that cat, a captioning of the image. You can first of
498160	503760	all, you can do the other way, many to one mapping where you give it a story
503760	508320	about a cat and it generates an image. There's many to many. This is Google
508320	513400	translate. We translate a sentence from one language to another and there's
513640	518880	various flavors of that. Again, same theme here. Input data provided with
518880	527440	correct output and then let it go into the wild where it runs on input data
527440	533320	that hasn't seen before to provide guesses. And it's as simple as this.
533320	538960	Whatever you can convert into one of the following four things. Numbers, vector of
539000	543480	numbers, so a bunch of numbers, a sequence of numbers where the temporal
543480	549640	dynamics matters, so like audio, video, where the sequence, the ordering matters,
549640	553480	or a sequence of vector numbers, just a bunch of numbers. If you can convert it
553480	557440	to numbers and I propose to you that there's nothing you can't convert it
557440	564600	to numbers. If you can convert it to numbers, you can have a system learn to
564600	568560	do it. And the same thing with the output. Generate numbers, vectors, and numbers
569080	571840	sequence of numbers or sequence of vectors and numbers.
575560	577400	First, is there any questions at this point?
579960	584080	Well, we have a lot of fun slides to get through, but I'll pause every once in a
584080	588520	while to make sure we're on the same page here. So what kind of input are we
588520	593480	talking about? Just to fly through it. Images, so faces, or medical applications
593480	599640	for looking at scans of different parts of the body to determine if, to
599640	605200	diagnose any kind of medical conditions. Texts, so conversations, your texts,
605200	610600	article, blog posts, for sentiment analysis, question and answering, so you
610600	614760	ask it a question where the output you hope answers. Sounds of voice
614760	620840	recognition, any kind of, anything you can tell from audio. Time series data, so
620840	626240	financial data, stock market, you can use it to predict anything you want about
626240	630040	the stock market, including whether to buy or sell. If you're curious, doesn't
630040	637800	work quite well as a machine learning application. Physical world, so cars or
637800	642800	any kind of object, any kind of robot that exists in this world. So location of
642800	647520	where I am, location of where other things are, the actions of others, that
647560	651600	could be all the input. All of it can be converted to numbers. And the correct
651600	656120	output, same thing. Classification, a bunch of numbers. Classification is
656120	660600	saying it's a cat or a dog. Regression is saying to what degree I turn the
660600	665120	steering wheel. Sequence is generating audio, generating video, generating
665120	670240	stories, captioning, text, images, generating anything you could think of as
670240	677040	numbers. And at the core of it is a bunch of data agnostic machine learning
677040	680840	algorithms. There's traditional ones, nearest neighbors, Naive Bay support
680840	690480	machine, support vector machines. A lot of them are limited, and I'll describe
690480	696200	how. And then there's neural networks. There's nothing special and new about
696200	703200	neural networks. And I'll describe exactly the very subtle thing that is
703880	709480	powerful. That's always been there all along. And certain things have now been
709480	713640	able to unlock that power about neural networks. But it's still just the flavor
713640	717960	of a machine learning algorithm. And the inspiration for neural networks, as
717960	723600	Jonathan showed last time, is our human brain. It's perhaps why the media, perhaps
723600	729560	why the hype is captivated by the idea of neural networks, is because you
729600	734840	immediately jump to this feeling like, because there's this mysterious structure
734840	738440	to them that scientists don't understand. Artificial neural networks, I'm
738440	743200	referring to. And the biological ones. We don't understand them. And the
743200	749640	similarity captivates our minds and we think, well, this approach is perhaps as
749640	756640	limited, as limitless as our own human mind. But the comparison ends there. In
756640	763400	fact, artificial neuron, their artificial neural networks are much simpler
763400	769720	computational units. At the core of everything is this neuron. This is a
769720	778760	computational unit that does two very simple operations. On the left side, it
778760	786880	takes a set of numbers as inputs. It applies weights to those inputs, sums
786880	795120	them together, applies a little bias, and provides an output somewhere between zero
795120	803120	and one. So you can think of it as a computational entity that gets excited
803280	811480	when it sees certain inputs and gets totally turned off when it gets other
811480	819320	kinds of inputs. So maybe this neuron with a 0, with a 0.7, 0.6, 1.4 weights, it
819320	825080	gets really excited when it sees pictures of cats and totally doesn't care about
825080	832880	dogs. Some of us are like that. So that's the job of this neuron, is to detect
832880	841000	cats. Now what, the way you build an artificial neural network, the way you
841000	846040	release the power that I'll talk about in the following slides about the
846040	850200	applications, what could be achieved, is just stacking a bunch of these together.
852000	858160	Think about it. This is, this is a extremely simple computational unit.
859120	864840	So you need to sort of pause whenever we talk about the following slides and
864840	871920	think that there's a few slides that I'll show that say neural networks are
871920	877920	amazing. I want you to think back to this slide that everything is built on top
877920	883560	of these really simple addition operations with a simple nonlinear
883560	890160	function applied at the end. Just a tiny math operation. We stack them together
890160	894320	with it in a feedforward way so there's a bunch of layers and when people talk
894320	901160	about deep neural networks it means there is a bunch of those layers and then
901160	905320	there's recurrent neural networks that are also a special flavor that's able to
905320	912000	have memory. So as opposed to just pushing input into output directly, it's
912040	916280	also able to do stuff on the inside in a loop where it remembers things. This is
916280	919680	useful for natural language processing, for audio processing, whenever the
919680	928440	sequence is not, the length of the sequence is not defined. Okay, slide number
928440	935200	one in terms of neural networks are amazing. This is, this is perhaps for the
935240	942920	math nerds. But also I want you to use your imagination. There's a universality
942920	947680	to neural networks. It means that the simple computational unit on the left is
947680	952680	an input and the right is the output of this network with just a single hidden
952680	957000	layer. It's called a hidden layer because it sits there in the middle of the
957000	963640	input and the output layers. A single hidden layer with some number of nodes
964560	975640	can represent any function. Any function. That means anything you want to build in
975640	982880	this world. Everyone in this room can be represented with a neural network with a
982880	989520	single hidden layer. So the power, and this is just one hidden layer, the power
989560	995880	of these things is limitless. The problem, of course, is how do you find the
995880	1003560	network? So how do you build a network that is as clever as many of the people
1003560	1010600	in this room? But the fact that you can build such a network is incredible. It's
1010600	1019000	amazing. I want you to think about that. And the way you train a network, so it's
1019040	1025160	born as a blank slate. Some random weights assigned to the edges. Again, a network
1025160	1028080	is represented. The numbers at the core, the parameters at the core of this
1028080	1033760	network are the numbers on each of those arrows, each of those edges. And you
1033760	1038800	start knowing nothing. This is a baby network. And the way you teach it
1038800	1044600	something, unfortunately, currently, as I said, in a supervised learning
1044960	1048680	mechanism, you have to give it pairs of input and output. You have to give it
1049720	1057560	pictures of cats and labels on those pictures saying that they're cats. And the
1057560	1067760	basic fundamental operation of learning is when you compute the measure of an
1067760	1075800	error and you back propagate it to the network. What I mean, everything's easier
1075800	1085160	with cats. I apologize. I apologize. Too many cats. And so the input here is a cat.
1085160	1090280	And the neural network we trained, it's just guessing. It doesn't know. So I don't
1090280	1096920	know. It's guessing cat. Well, it happens to be right. So we have to, this is the
1096960	1101520	measure of error. Yes. You got it right. And you have to back propagate that error.
1102640	1107600	You have to reward the network for doing a good job. And all you do, what I mean
1107600	1113400	by reward, there's weights on each of those edges. And so the node, the individual
1113400	1117800	neurons that were responsible that back to that cat neuron, that cat neuron needs
1117800	1122680	to be rewarded for seeing the cat. So you just increase the weights on the neurons
1122680	1126800	that were associated with producing the correct answer. Now you give it a picture
1126840	1134120	of a dog and the neural network says cat. Well, that's an incorrect answer. So no,
1134120	1137840	there's a high error. It needs to be back propagated to the network. So the weights
1137840	1143800	that were responsible with classifying this picture as a cat need to be punished.
1143800	1149960	They need to be decreased. Simple. And you just repeat this process over and over.
1149960	1156640	This is what we do as kids when we're first learning. For the most part,
1156680	1161800	we're also supervised learning machines in the sense that we have our parents and
1161800	1168440	we have the environment, the world, that teaches about what's correct and what's
1168440	1173440	incorrect. And we back propagate this error and reward through our brain to learn.
1174440	1179440	The problem is, as human beings, we don't need too many examples. And I'll talk
1179440	1183200	about some of the drawbacks of these approaches. We don't need too many
1183240	1187240	examples. You fall off your bike once or twice and you learn how to ride the bike.
1187240	1193240	Unfortunately, neural networks need tens of thousands of times when they fall off
1193240	1198240	the bike in order to learn how to not do it. That's one of the limitations.
1198240	1206240	And one key thing I didn't mention here is when we refer to input data,
1206280	1214280	we usually refer to sensory data, raw data. We have to represent that data in
1214280	1224280	some clever way, in some deeply clever way, where we can reason about it,
1224280	1230280	whether it's in our brains or in the neural network. And a very simple example
1230320	1237320	here to illustrate why representation of data matters. So the way you represent
1237320	1243320	the data can make the discrimination of one class from another, a cat versus dog,
1243320	1249320	either incredibly difficult or incredibly simple. Here is a visualization of the
1249320	1254320	same kind of data in Cartesian coordinates and polar coordinates. On the right,
1254360	1261360	you can just draw a simple line to separate the two. What you want is a system that's
1261360	1267360	able to learn the polar coordinate representation versus the Cartesian
1267360	1275360	representation automatically. And this is where deep learning has stepped in and
1275360	1281360	revealed the incredible power of this approach, which deep learning is the
1281400	1287400	smallest circle there. It's a type of representational learning. Machine learning
1287400	1291400	is the bigger second to the biggest. So this class is about the biggest circle,
1291400	1296400	AI, includes robotics, includes all the fun things that are built on learning.
1296400	1300400	And I'll discuss while machine learning I think will close this entire circle into
1300400	1306400	one. But for now, AI is the biggest circle, then a subset of that is machine
1306400	1310400	learning and a smaller subset of that is representation learning. So deep
1310440	1315440	learning is not only able to say, given a few examples of cats and dogs that
1315440	1321440	discriminate between a cat and a dog, it's able to represent what it means to be a
1321440	1329440	cat. So it's able to automatically determine what are the fundamental
1329440	1335440	units at the low level and the high level. We're talking about this very Plato.
1335480	1342480	What it means to represent a cat from the whiskers to the high level shape of
1342480	1348480	the head to the fuzziness and the deformable aspects of the cat. Not a cat
1348480	1352480	expert, but I hear these are the features of a cat versus that are essential to
1352480	1357480	discriminate between a cat and a dog. Learning those features as opposed to
1357480	1362480	having to have experts, this is the drawback of systems that Jonathan talked
1362520	1366520	about from the eighties and nineties where you have to bring in experts for any
1366520	1370520	specific domain that you try to solve. You have to have them encode that
1370520	1378520	information. Deep learning, this is simply the only big difference between deep
1378520	1382520	learning and other methods is that it learns the representation for you. It
1382520	1386520	learns what it means to be a cat. Nobody has to step in and help it figure out
1386560	1393560	that cats have whiskers and dogs don't. What does this mean? The fact that it
1393560	1399560	can learn these features, these whisker features, is as opposed to having five
1399560	1404560	or ten or a hundred or five hundred features that are encoded by brilliant
1404560	1410560	engineers with PhDs, it can find hundreds of thousands, millions of features
1410600	1418600	automatically. Hundreds of millions of features. Stuff that can't be put into
1418600	1422600	words or described, in fact it's one of the limitations in neural networks is they
1422600	1426600	find so many fundamental things about what it means to be a cat that you can't
1426600	1432600	visualize what it really knows. It just seems to know stuff. It finds that
1432600	1438600	stuff automatically. What does this mean? The critical thing here is because it's
1438640	1443640	able to automatically learn those hundreds of millions of features, it's able to
1443640	1452640	utilize data. It doesn't start, the diminishing returns don't hit until well we
1452640	1455640	don't know when they hit. The point is with the classical machine learning
1455640	1461640	algorithms, you start hitting a wall when you have tens of thousands of images of
1461680	1470680	cats. With deep learning, you get better and better with more data.
1470680	1476680	Neural networks are amazing slide two. Here's a game, a simple arcade game where
1476680	1481680	there's two paddles, they're bouncing a ball back and forth. Okay, great. You can
1481680	1485680	figure out an artificial intelligence agent that can play this game. It can,
1485720	1491720	not even that well, just kind of, it kind of learns to do alright and eventually
1491720	1501720	win. Here's the fascinating thing. With deep learning as opposed to encoding the
1501720	1507720	position of the paddles, the position of the ball, having an expert in this game,
1507720	1514720	there's many, come in and encode the physics of this game. The input to the
1514760	1523760	neural network is the raw pixels of the game. So, it's learning in the following
1523760	1529760	way. You give it an evolution of the game. You give it a bunch of pixels.
1529760	1535760	Pixels are, images are built up of pixels. They're just numbers from 0 to 256. So
1535760	1540760	there's this array of numbers that represent each image and then you give it
1540800	1545800	several tens of thousands of images that represent a game. So you have the stack
1545800	1551800	of pixels and stack of images that represent a game. And the only thing you
1551800	1556800	know, this giant stack of numbers, the only thing you know is at the end you
1556800	1564800	won or lost. That's it. So based on that, you have to figure out how to play the
1564800	1569760	game. You know nothing about games. You know nothing about colors or balls or
1569800	1577800	paddles or winning or anything. That's it. So this is, why is this amazing? That
1577800	1582800	it even works and it works, it wins. It's amazing because that's exactly what we do
1582800	1587800	as human beings. This is general intelligence. So I need you to pause and
1587800	1591800	think about this. We'll talk about special intelligence and the usefulness and
1591800	1597800	okay, there's cool tricks here and there that we can do to get you an edge on your
1597840	1603840	high frequency trading system. But this is general intelligence. General
1603840	1608840	intelligence is the same intelligence we use as babies when we're born. What we
1608840	1613840	get is an input, sensory input of image sensory input. Right now, all of us,
1613840	1620840	most of us are seeing, hearing, feeling with touch and that's the only input we
1620880	1627880	get. We know nothing and with that input, we have to learn something. Nobody is
1627880	1631880	pre-teaching us stuff and this is an example of that. A trivial example, but
1631880	1636880	one of the first examples where this is truly working. I'm sorry to linger on
1636880	1642880	this, but it's a fundamental fact. The fact that we have systems that, and now
1642880	1648880	outperform human beings in these simple arcade games is incredible. This is the
1648920	1656920	research side of things. But let me step back, these again, the takeaways. That
1656920	1664920	previous slide is why I think machine learning is limitless in the future.
1664920	1673920	Currently, it's limited. Again, the representation of the data matters and if
1673960	1680960	you want to have impact, we currently can only tackle the small problems. What are
1680960	1685960	those problems? Image recognition. We can classify, given the entire image of a
1685960	1695960	leopard, of a boat, of a mite, with pretty good accuracy of what's in that image.
1695960	1700960	That's image classification. What else? We can find exactly where in that image
1701000	1707000	each individual object is. That's called image segmentation. Again, the process
1707000	1716000	is the same. The learning system in the middle, a neural network, as long as you
1716000	1722000	give it a set of numbers as input and the correct set of labels as output, it
1722000	1727000	learns to do that for data it hasn't seen in the past. Let me pause a second and
1727040	1732040	maybe if you have any questions, does anyone have any questions about the
1732040	1735040	techniques of neural networks? Yes.
1735080	1744080	How do you represent data, as you mentioned, to us anyways, as well as
1744080	1752080	coordinate the recent coordinates? How do you represent these two different
1752080	1755080	kinds of data?
1755080	1763080	That's a great question. In a couple of slides, I'll get to it exactly. The
1763120	1769120	data representation, I'll elaborate in a little bit, but loosely, the data
1769120	1777120	representation is for a neural network is in the weights of each of those arrows
1777120	1783120	that connect the neurons. That's where the representation is. I'll show to
1783120	1791120	really clarify that example of what that means. The Cartesian versus polar
1791160	1799160	coordinates is just a very simple visualization of the concept. You want
1799160	1804160	to be able to represent the data in an arbitrary way where there's no limits
1804160	1809160	to the representation. It could be highly nonlinear, highly complex. Any other
1809160	1811160	questions?
1811200	1816200	We touched on it earlier. Generally speaking, in our current state, when we talk
1816200	1820200	about machine learning or AI, it's simply statistical models that are able to
1820200	1825200	recognize the trends or things of that nature where they're not necessarily
1825200	1832200	thinking, but simply recognizing. I'm a little confused about how the current
1832200	1839200	system differs from deep learning. Whether you think that there is a
1839240	1843240	possibility of transition from recognizing to actually thinking.
1843240	1849240	I have a couple of slides almost asking this question, because there's no
1849240	1854240	good answers. One could argue, and I think somebody in the last class brought up
1854240	1863240	that is machine learning just pattern recognition. It's possible that reasoning,
1863280	1874280	thinking, is just pattern recognition. I'll describe sort of an intuition
1874280	1885280	behind that. We tend to respect thinking a lot, because we've recently as human
1885280	1890280	beings learned to do it. In our evolutionary time, we think that it's somehow
1890320	1895320	special from, for example, perception. We've had visual perception for several
1895320	1901320	orders of magnitude longer in our evolution as a living species. We've
1901320	1907320	started to learn to reason, I think, about 100,000 years ago. We think it's
1907320	1912320	somehow special from the same kind of mechanism we use for seeing things.
1912320	1917320	Perhaps it's exactly the same thing. Perception is pattern recognition.
1917360	1923360	Perhaps reasoning is just a few more layers of that. That's the hope.
1923360	1928360	That's an open question.
1928400	1937400	The concept of neural network itself is not very new. Is there any
1937400	1944400	technical innovation breakthrough to expand the use of neural network?
1944400	1952400	I think more it's just the increase of the result of computational
1952440	1958440	innovation. Yes, that's a great question. There's been very few
1958440	1963440	breakthroughs in neural networks, since through the AI winters that we've discussed,
1963440	1970440	through a lot of excitement, in spurts, and even recently, there's been
1970440	1976440	very few algorithmic innovations. The big gains came from compute,
1976480	1982480	so improvements in GPU and better, faster computers. You can't underestimate
1982480	1988480	the power of community, so the ability to share code and the internet,
1988480	1992480	the ability to communicate together through the internet and work on code
1992480	1996480	together, and then digitization of data, so the ability to have large data sets
1996480	2001480	easily accessible and downloadable. All of those little things.
2001520	2005520	I think in terms of the future of deep learning and machine learning,
2005520	2013520	it all rides on compute, meaning continued, bigger, and faster computers.
2013520	2018520	That doesn't necessarily mean Moore's law in making small and small chips.
2018520	2023520	It means getting clever in different directions, massive parallelization,
2023520	2028520	coming up with ways to do super-efficient, power-efficient
2028560	2033560	implementations in neural networks, and so on.
2033560	2038560	Let me just fly through a few examples of what we can do with machine learning
2038560	2042560	just to give you a flavor, I think, in future lectures as possible.
2042560	2047560	We'll discuss different speakers, different specific applications,
2047560	2052560	really dig into those. As opposed to working with just images,
2052560	2057560	you can work with videos and segment those, I mentioned, image segmentation.
2057600	2061600	We can do video segmentation through video segment, the different parts
2061600	2065600	of a scene that's useful to a particular application. Here in driving,
2065600	2069600	you can segment the road from cars and vegetation
2069600	2073600	and lane markings.
2073600	2077600	You can also, this is a subtle but important point.
2077600	2082600	Just go back to that one slide. How do they see the light?
2082640	2086640	It's such a critical piece. The more I listen to you
2086640	2090640	and read your stuff, it seems like this critical, these very small
2090640	2094640	piece of information that we know are important, like there is a red light.
2094640	2098640	I have to stop, I have to slow down. How does it filter that out
2098640	2102640	and pick out that?
2102640	2106640	It's got to be 100% reliable on that.
2106680	2110680	Hello.
2110680	2114680	Oof, hard question.
2114680	2118680	The question was how do you detect the traffic light
2118680	2122680	and lights.
2122680	2126680	How do we do it as human beings? First of all, let's start there.
2126680	2130680	The way we do it is by
2130680	2134680	the knowledge we bring to the table. We know what it means
2134720	2138720	to be on the road. There's a lot of the huge network of knowledge
2138720	2142720	that you come with. And so that makes the perception problem much easier.
2142720	2146720	This is pure perception. You take an image and you separate different parts
2146720	2150720	based purely on tiny patterns of pixels.
2150720	2154720	So first it finds all the edges and it learns that
2154720	2158720	traffic lights have certain kinds of edges around them and then
2158720	2162720	zoom out a little bit. They have
2162760	2166760	certain collection of edges that make up this black
2166760	2170760	rectangle type shape. So it's all about shapes. It kind of build up
2170760	2174760	knowing this shape structure of things. But it's a purely
2174760	2178760	perception problem and one of the things I argue is that if it's
2178760	2182760	purely a perception approach and you bring no knowledge to the
2182760	2186760	table about the physics of the world, the three-dimensional physics and the temporal dynamics,
2186760	2190760	that you are not going to be able to successfully achieve
2190800	2194800	near 100% accuracy on some of these systems.
2194800	2198800	So that's exactly the right question is
2198800	2202800	for all of these things, think about how you as a human being would solve
2202800	2206800	these problems and what is lacking in the machine learning approach.
2206800	2210800	What data is lacking in the machine learning approach in order to achieve
2210800	2214800	the same kind of results, the same kind of reasoning required to
2214800	2218800	that you would use as a human. So there is also image
2218840	2222840	detection. Image detection, which means
2222840	2226840	the subtle but important point. The stuff I mentioned before, image classification
2226840	2230840	is given an image of a cat, you find the cat,
2230840	2234840	sorry you don't find the cat, you say this image is of a cat or not, and then
2234840	2238840	detection or localization is when you actually find where in the image that is.
2238840	2242840	That problem is much harder but also doable
2242840	2246840	with machine learning, with deep neural networks.
2246880	2250880	Now as I said, inputs-outputs can be anything. The input can be
2250880	2254880	video, the output can be video, and you can do anything you want with these videos,
2254880	2258880	you can colorize the video. You can add, take an old black and white
2258880	2262880	film and produce color images.
2262880	2266880	Again, in terms of
2266880	2270880	having an impact in the world using these applications,
2270880	2274880	you have to think, this is a cool demonstration, but how well does it actually
2274920	2278920	work in the real world? Translation,
2278920	2282920	whether that's from text to text or image to image, you can translate
2282920	2286920	here dark chocolate from one language to another.
2286920	2290920	Class, global business of
2290920	2294920	artificial intelligence, there's a reference below there, you can go and generate
2294920	2298920	your own text. You can generate the writing of
2298920	2302920	the act of generating handwriting. You can type in some text
2302960	2306960	and given different styles that it learns from other handwriting
2306960	2310960	samples, it can generate any kind of text using
2310960	2314960	handwriting. Again, the input is language, the
2314960	2318960	output is a sequence of writing
2318960	2322960	of pen movements on the screen. You can complete sentences.
2322960	2326960	This is kind of a fun one where if you start
2326960	2330960	actually you can generate language, and you can generate
2331000	2335000	language where you feed the system some input first. So in black
2335000	2339000	there it says life is, and then have the neural network complete
2339000	2343000	those sentences. Life is about kids.
2343000	2347000	Life is about the weather, there's a lot of knowledge here
2347000	2351000	I think being conveyed, and you can start the sentence with the meaning of life is.
2351000	2355000	The meaning of life is literary recognition,
2355000	2359000	true for us academics, or the meaning of life is the tradition of
2359040	2363040	ancient human production. Also true.
2363040	2367040	But these are all generated by a computer. You can also
2367040	2371040	caption, this has become very popular recently, is
2371040	2375040	caption generation. Given input as an image, the output is
2375040	2379040	a set of text. The caption is the content of the image.
2379040	2383040	You find the different objects in the image, that's a perception
2383040	2387040	problem, and once you find the different objects, you stitch them together in a sentence
2387080	2391080	that makes sense. You generate a bunch of sentences, and classify which sentences the most
2391080	2395080	likely to fit this image.
2395080	2399080	And you can, so certainly in the
2399080	2403080	I try to avoid mentioning driving too much, because it is my field
2403080	2407080	that is what I'm excited about, but then
2407080	2411080	the moment I start talking about driving it'll all be about driving.
2411080	2415080	But I should mention of course that deep learning is critical to driving
2415120	2419120	applications, for both the perception and what is really exciting to us now is
2419120	2423120	the end-to-end, the end-to-end approach.
2423120	2427120	So whenever you say end-to-end in any application, what that means
2427120	2431120	is you start from the very raw inputs that the
2431120	2435120	system gets, and you produce the very final
2435120	2439120	output that's expected of the system. So as opposed to in the cell driving car case
2439120	2443120	as opposed to breaking a car down into each individual components of
2443160	2447160	perception, localization, mapping, control, planning
2447160	2451160	and just taking the whole stack and just ignoring
2451160	2455160	all the super complex problems in the middle and just taking the external
2455160	2459160	scene as input and as output produced steering and
2459160	2463160	acceleration and braking commands. And so in this way, taking this input
2463160	2467160	is the image of the external world, in this case in a Tesla
2467160	2471160	we can generate steering commands for the car.
2471200	2475200	And then input a bunch of numbers that's just images,
2475200	2479200	output a single number that gives you the steering of
2479200	2483200	the car.
2483200	2487200	Okay, so let's step back
2487200	2491200	for a second and think about what can't we do
2491200	2495200	with machine learning. We talked about you can map numbers to numbers. Let's think about
2495200	2499200	what we can't do. At the core of artificial intelligence in terms of
2499240	2503240	how we can impact on this world is robotics. So what
2503240	2507240	can't we solve in robotics and artificial intelligence with the machine learning approach?
2507240	2511240	And let's break down what artificial intelligence means.
2511240	2515240	Here's a stack starting at the very top is the environment, the world
2515240	2519240	that you operate in. There's sensors that sense that world. There is feature
2519240	2523240	extraction and learning from that data and there's some reasoning,
2523240	2527240	planning and affectors are the ways you manipulate
2527280	2531280	the world. What can't we learn in this way?
2531280	2535280	So we've had a lot of success as Jonathan talked about
2535280	2539280	in the history of AI with formal tasks, playing games,
2539280	2543280	solving puzzles. Recently we're having a lot of breakthroughs with medical
2543280	2547280	diagnosis. We're still
2547280	2551280	struggling
2551280	2555280	but are very excited about in the robotics space
2555320	2559320	with more mundane tasks of walking, of
2559320	2563320	basic perception, of natural language written and spoken.
2563320	2567320	And then there is the human tasks which are
2567320	2571320	perhaps completely out of reach of this pipeline
2571320	2575320	at the moment is cognition,
2575320	2579320	imagination, subjective
2579320	2583320	experience. So high level reasoning, not just common sense,
2583360	2587360	but high level human level reasoning.
2587360	2591360	So let's fly through this pipeline. There's sensors,
2591360	2595360	cameras, LiDAR, audio,
2595360	2599360	there's communication that flies through the air
2599360	2603360	or wired or wireless or wired. IMU measuring the movement
2603360	2607360	of things. So that's the way you think about it.
2607360	2611360	That's the way as human beings and as any kind of system that you design, you measure the world.
2611400	2615400	You don't just get an API to the world.
2615400	2619400	You need to somehow measure aspects of this world.
2619400	2623400	So that's how you get the data. So that's how you convert the
2623400	2627400	world into data you can play with. And once you have the data,
2627400	2631400	this is the representation side. You have to convert that raw data, raw
2631400	2635400	pixels, raw audio, raw LiDAR data. You have to convert that
2635400	2639400	into data that's useful for the intelligence system,
2639440	2643440	for the learning system to use to discriminate
2643440	2647440	between one thing and another.
2647440	2651440	For vision, that's finding edges, corners,
2651440	2655440	object parts and entire objects. And there's the machine learning
2655440	2659440	that I've talked about. There's different kinds of mapping
2659440	2663440	of the representation that you've learned to an actual outputs.
2663440	2667440	There is, once you have this, so you have this idea
2667480	2671480	of, and this goes to maybe a little bit of Simon's question,
2671480	2675480	is reasoning. This is something that's
2675480	2679480	out of reach in machine learning at the moment. This is going to your question.
2679480	2683480	Then we can build
2683480	2687480	a world class machine learning system for taking an image
2687480	2691480	and classifying that it's a duck. I wonder if this will work.
2697480	2701480	Wake you up.
2701480	2705480	So we could take, this is well studied,
2705480	2709480	exceptionally well studied problem. We could take audio sample
2709480	2713480	of a duck and tell that it's a duck. In fact, what species
2713480	2717480	of bird? It's incredible how much research there is in bird species
2717480	2721480	classification. And we can look at video and we can tell
2721480	2725480	that we can do actual recognition that it's swimming. But we
2725520	2729520	can't do with learning now, is reason. That if it
2729520	2733520	looks like a duck, it swims like a duck and quacks like a duck,
2733520	2737520	it's very likely to be a duck. This is the reasoning problem.
2737520	2741520	This is the task that I personally am obsessed with
2741520	2745520	and that I hope that machine learning can close.
2745520	2749520	And then there is the planning action and the effectors.
2755520	2759520	So this is another place
2759520	2763520	where machine learning
2763520	2767520	has not had many strides. There's mechanical issues here that are incredibly
2767520	2771520	difficult. The degrees of freedom with all the actuators involved,
2771520	2775520	with all the just the ability to
2775520	2779520	localize every part of yourself in this dynamic
2779520	2783520	space. Where things
2783560	2787560	are constantly changing when there's degrees of uncertainty, when there's noise. Just that
2787560	2791560	basic problem is exceptionally difficult.
2795560	2799560	Let me just pose this question. We talked
2799560	2803560	about what machine learning can do with the cats and the duck.
2803560	2807560	We could do that. Given representation, it could predict what's in the image.
2807560	2811560	But one of the open questions is, and deep
2811600	2815600	learning has been able to do the feature extraction, the representation
2815600	2819600	learning. This is the big breakthrough that everybody's excited about.
2819600	2823600	But can it also reason? These are the open questions.
2823600	2827600	Can it reason? Can it do the planning and action?
2827600	2831600	And as human beings do, can it close the loop entirely
2831600	2835600	from sensors to effectors? So learn not only
2835600	2839600	the brain, but the way you sense the
2839640	2843640	world, and the way you affect the world.
2861640	2865640	So the question was about the pond game. Thank you.
2865680	2869680	Let's get to talk to it for a little longer. It doesn't
2869680	2873680	get punished when it doesn't detect the ball. This is the beautiful thing.
2873680	2877680	It gets punished only at the very end of the game for losing the game, and gets
2877680	2881680	rewarded for winning the game. So it knows nothing about that ball.
2881680	2885680	And it learns about that ball. That's something you need to really sit and think
2885680	2889680	about. Because like, as human
2889680	2893680	beings, imagine if you're playing with a physical ball, how do you
2893720	2897720	learn what a ball is? You get hurt by it,
2897720	2901720	you like squeeze it, you throw it, you feel the dynamics of it,
2901720	2905720	the physics of it, and
2905720	2909720	nobody tells you about what a ball is. You're just using the raw
2909720	2913720	sensory input. We take it for granted, and maybe this is what I can
2913720	2917760	end on, is this is what something Jonathan
2917760	2921760	brought up, is we take the simplicity of this task for granted.
2921840	2925840	Because we've been, we've had eyes
2925840	2929840	we broadly speaking as living
2929840	2933840	species on planet Earth. These eyes have been evolved
2933840	2937840	for 540 million years. So we have 540 million
2937840	2941840	years of data. We've been walking for close to that
2941840	2945840	by petal mammals. We have been
2945840	2949840	thinking only very recently. So 100,000 years
2949840	2953840	is 100 million years. And
2953840	2957840	that's why we can't, some of these problems that we're trying to
2957840	2961840	solve, you can't take for granted how actually difficult they are. So for
2961840	2965840	example, this is the Marvex Paradox that Jonathan brought up,
2965840	2969840	is that the easy problems are hard. The things we think are easy, actually really
2969840	2973840	hard. This is the state-of-the-art robot on the right playing soccer,
2973840	2977840	and that was the state-of-the-art human
2977840	2981840	on the left playing soccer. And
2981840	2985840	I'll give it a second.
2993840	2997840	The question was, you know, there's a
2997840	3001840	fundamental difference between the way we train neural networks and the way we've trained
3001840	3005840	biological neural networks through evolution by discarding through natural selection a bunch of
3005840	3009840	the neural networks that didn't work
3009840	3013840	so well. So first of all, the process of
3013840	3017840	evolution is, I think, not well understood.
3017840	3021840	Meaning, sorry, the role,
3021840	3025840	careful here. The role of
3025840	3029840	evolution in the evolution of our cognition, of
3029840	3033840	our intelligence. I don't know if that's, so this
3033840	3037840	is an open question. So maybe clarify this point. Is neural networks
3037840	3041840	artificial neural networks are fixed for the most part in size?
3041840	3045840	This is exactly right. It's like a single human being that gets to
3045840	3049840	learn. We don't have mechanisms of
3049840	3053840	modifying or revolving those neural networks yet.
3053840	3057840	Although you could think of researchers
3057840	3061840	as doing exactly that. You have grad students working on different
3061840	3065840	neural networks and the ones that don't do a good job don't get
3065840	3069840	promoted and get a good job. There is a natural selection there, but other than that
3069840	3073840	it's an open question. It's a fascinating one.
3085840	3089840	Stay tuned and keep your head up because the future I believe
3089840	3093840	is really promising. And the slides will be
3093840	3097840	made available for sure.
3097840	3101840	I think a lot of the explorations of
3101840	3105840	what it means to build an intelligent machine has been in sci-fi movies. We're now beginning
3105840	3109840	to actually make it a reality. This is Space Odyssey to keep with that theme
3109840	3113840	in the previous lecture that we had. This is
3113840	3117840	as opposed to the dream like monolith view
3117840	3121840	when the astronaut is gazing out
3121840	3125840	into the open sky at the stars. We're going to look at the practice of
3125840	3129840	AI today and how we go, if you're familiar with the
3129840	3133840	movie, when this new technology appeared before our eyes
3133840	3137840	and we're full of excitement, how we transfer that into
3137840	3141840	actual practical impact
3141840	3145840	on our lives. To quickly review what we
3145840	3149840	talked about last time, I presented the technology
3149840	3153840	and asked the question of whether this technology merely serves a special purpose
3153840	3157840	to answer specific tasks that can be formalized or whether it can
3157840	3161840	be through the process of transferring
3161840	3165840	the knowledge learned on one domain be generalizable
3165840	3169840	to where an intelligent system that's trained in a small domain can be
3169840	3173840	used to achieve general intelligent tasks like we do
3173840	3177840	as human beings. This is kind of the stack of artificial intelligence
3177840	3181840	going from all the way up to the top, the environment, the world
3181840	3185840	the sensors, the sensor data, the intelligence system, the way
3185840	3189840	it perceives this world. Then once you have this, you convert
3189840	3193840	the world into some numbers, you're able to extract some representation of that
3193840	3197840	world and this is where machine learning starts to come into play. And then there's
3197840	3201840	the part where I will raise it again today is can machine learning
3201840	3205840	be doing the following steps too that we can do very well as human beings is the
3205840	3209840	reasoning step. You can tell the difference in a cat and a dog, but can you
3209840	3213840	now start to reason about what it means to be alive, what it
3213840	3217840	means to be a cat, a living creature, what it means to be this kind of physical object
3217840	3221840	or this kind of physical object and take what's called common sense, things we take
3221840	3225840	for granted, start to construct models of the world through
3225840	3229840	reasoning. Descartes, I think therefore I am.
3229840	3233840	We want our neural networks to come up with that on their own.
3233840	3237840	And once you do that, action.
3237840	3241840	You'll go right back into the world and you start acting in that world. So the question is
3241840	3245840	can machine learning, can this be learned from data or do experts need to encode
3245840	3249840	the knowledge of reasoning, the knowledge of actions, the set of actions.
3249840	3253840	That's kind of the question, the open questions I raise. It continues throughout the talk
3253840	3257840	today. And so as we start to think
3257840	3261840	about how artificial intelligence, especially machine learning
3261840	3265840	as it relies itself through robotics, gets to impact the world, we start thinking
3265840	3269840	about what are the easy problems and what are the hard problems. And it seems
3269840	3273840	to us that vision
3273840	3277840	and movement, walking is easy because we've been doing it for millions
3277840	3281840	of years, hundreds of millions of years, and thinking is hard.
3281840	3285840	Reasoning is hard. I propose to you that it's perhaps because we've only
3285840	3289840	been doing it for a short time and so think we're quite special
3289840	3293840	because we're able to think. So we have to kind of question of what is easy
3293840	3297840	and what is hard. Because when we start to develop some of these systems
3297840	3301840	and you start to realize that all of these
3301840	3305840	problems are equally hard. So the problem of walking that we take for granted,
3305840	3309840	the actuation and the physical, the ability to recognize
3309840	3313840	where you are in the physical space to sense the world
3313840	3317840	around you, to deal with the
3317840	3321840	uncertainty of the perception problem. And then
3321840	3325840	so all of these robots, by the way, this is for the most recent DARPA Challenge
3325840	3329840	which MIT was also part of.
3329840	3333840	And so what are these robots doing? They
3333840	3337840	don't have any, they only have sparse communication with human beings
3337840	3341840	on the periphery. So most of the stuff they have to do autonomously
3341840	3345840	like get inside a car, this is an MIT robot unfortunately,
3345840	3349840	that they have to get in the car in the hardest tasks, they have to get out of the
3349840	3353840	car. That's walking. So this kind of
3353840	3357840	raises to you a very real
3357840	3361840	aspect here. You want to build applications that actually work in the real world.
3361840	3365840	And that's the first challenge and opportunity here.
3365840	3369840	The many of the technologies we talked about currently crumble
3369840	3373840	under the reality of
3373840	3377840	our world. When we transfer them from a small
3377840	3381840	data set in the lab to the real world. For the computer vision
3381840	3385840	it's perhaps one of the best illustrations of this. Computer vision is the task
3385840	3389840	as we talked about of interpreting images. And so when
3389840	3393840	you, there's been a lot of great accomplishments on interpreting images.
3393840	3397840	Cats versus dogs. Now when you try to create
3397840	3401840	a system like the Tesla vehicle
3401840	3405840	that I've often, that we work with and
3405840	3409840	I always talk about is, it's a vision
3409840	3413840	based robot, right? It has radar for basic obstacle avoidance
3413840	3417840	but most of the understanding of the world comes from a single monocular camera.
3417840	3421840	Now they've expanded the number of cameras but for the most time there's been 100,000
3421840	3425840	vehicles driving on the roads today with a single, essentially
3425840	3429840	a single webcam. So when you start to do
3429840	3433840	that you have to perform all of these extraction of texture, color,
3433840	3437840	optical flow. So the movement through time, temporal dynamics of the images
3437840	3441840	you have to construct these patterns, construct the understanding of
3441840	3445840	objects and entities and how they interact. And from that you have to act
3445840	3449840	in this world. And that's all based on this computer vision system. So it's no longer
3449840	3453840	cats versus dogs, it's
3453840	3457840	detection of pedestrians. Where the wrong
3457840	3461840	classification, the wrong detection
3461840	3465840	is the difference between life and death. So let's look at
3465840	3469840	cats, where things are a little more comfortable. So computer vision
3469840	3473840	and I would like to illustrate to you why this is such a hard
3473840	3477840	task. We talked about, we've been doing it for 500 million years
3477840	3481840	so we think it's easy. Computer vision is actually incredible
3481840	3485840	so all you're getting with your human eyes is you're getting essentially pixels
3485840	3489840	in. There's light coming into your eyes and all you're getting is the reflection
3489840	3493840	from the different surfaces in here of light and there's
3493840	3497840	perception, there's sensors inside your eyes
3497840	3501840	converting that into numbers. It's really very similar to this.
3501840	3505840	Numbers in the case of what we use with computers, RGB images
3505840	3509840	or the individual pixels that are numbers from
3509840	3513840	255 to 256 possible numbers and there's just a bunch of them.
3513840	3517840	And that's all we get. We get a collection of numbers where they're spatially
3517840	3521840	connected. The ones that are close together are part of the same object so
3521840	3525840	cat pixels are all connected together. That's the only thing we have to help
3525840	3529840	us but the rest of it is just numbers, intensity numbers. And we have to use those numbers
3529840	3533840	to classify what's in the image.
3533840	3537840	And if you really think about it, this is a really difficult task. All you get
3537840	3541840	is these numbers. How the heck are you supposed to form a model
3541840	3545840	of the world with which you can detect
3545840	3549840	pedestrians with really 99.99999% accuracy.
3549840	3553840	Because these pedestrians or these cars
3553840	3557840	are cyclists in the car context or any kind of applications that you're looking at.
3557840	3561840	Even if your job is on the factory floor to detect
3561840	3565840	the defective gummy bears that are flying past like 100 miles an hour
3565840	3569840	your task is you don't want that bad gummy bear to get by
3569840	3573840	that your product and the brand will be damaged. However
3573840	3577840	serious or not serious your application is, what you have to be
3577840	3581840	you have to have a computer vision system that deals with
3581840	3585840	all of these aspects. Viewpoint variation, scale
3585840	3589840	variation, no matter the size of the object is still the same object.
3589840	3593840	No matter the viewpoint from which area you
3593840	3597840	look at that object is still the same object. The lighting that moves
3597840	3601840	we have lighting consistently here because we're indoors. But when you're outdoors
3601840	3605840	or you're moving, the scene is moving, the lighting, the complexity
3605840	3609840	of the lighting variations is incredible. From the illumination to just
3609840	3613840	the movement of the different objects in the scene.
3613840	3617840	Now that we've had these conversations, I think about this every time
3617840	3621840	I drive. I think about you at this point and how hard it is to see these things.
3621840	3625840	Particularly when I'm driving at night and particularly when it's twilight and the light is changing
3625840	3629840	I think almost every time I drive
3629840	3633840	there's one or two things that I see that I'm drawing
3633840	3637840	like 200 million years in order to be able to figure out. It's a guy who's
3637840	3641840	open his car door and I can't see him but I can just see the light doesn't look quite
3641840	3645840	right on that side of the road and somehow I know in my
3645840	3649840	mind it's a person. But it seems like an almost
3649840	3653840	impossible problem for the machines to get right with sufficient accuracy.
3653840	3657840	I will argue that the pure perception task is too hard.
3657840	3661840	That you come to the table as human beings with all this
3661840	3665840	huge amount of knowledge. That you're not actually
3665840	3669840	interpreting all the complex lighting variations that you're seeing.
3669840	3673840	You actually know enough about the world, enough about your commute home,
3673840	3677840	enough about the way, the kinds of things you would see in this
3677840	3681840	world, about Boston, about the way pedestrians move, the
3681840	3685840	certain light of day. You bring all that to the table that makes the perception task
3685840	3689840	doable. And that's one of the big missing pieces in the technology.
3689840	3693840	As I'll talk about, that's the open problem of machine learning.
3693840	3697840	It's how to bring all that knowledge, first of all build that knowledge and then bring that
3697840	3701840	knowledge to the table as opposed to starting from scratch every time.
3701840	3705840	And so, cats, I promise cats.
3705840	3709840	Okay, so to me, occlusion, for most of the
3709840	3713840	computer vision community, this is one of the biggest challenges. And it really highlights
3713840	3717840	how far we are from
3717840	3721840	being able to reason about this world. Occlusions are
3721840	3725840	what an occlusion is, is when the objects you're trying to
3725840	3729840	detect, something about, classify the object, detect the object,
3729840	3733840	the object is blocked
3733840	3737840	by another object in front of them. This is something
3737840	3741840	that you think is trivial, perhaps. You don't even really think about it, because
3741840	3745840	we reason in a three-dimensional way. But the occlusion
3745840	3749840	aspect is, makes, makes perception
3749840	3753840	incredibly difficult. So we have to design, think about this.
3753840	3757840	So this image is converted into numbers. And we, for the task of
3757840	3761840	detecting, is there a cat in this image? Yes or no? You have to be able to
3761840	3765840	reason about this image with that object in the scene. Most
3765840	3769840	of us are able to very easily detect that there's a cat in this image.
3769840	3773840	We're able to detect that there's a cat in this image. Now think about this.
3773840	3777840	There's a single eye and there's an ear.
3777840	3781840	So you have to think about what is it part of our brain that allows
3781840	3785840	us to understand, to, to suppose that with some
3785840	3789840	high degree of accuracy that there's a cat here in this picture.
3789840	3793840	I mean the degree of occlusion here is immense.
3793840	3797840	So this is for most of you.
3797840	3801840	Some of you will think this is in fact a monk
3801840	3805840	eating a banana. But I would venture to say that most of us are
3805840	3809840	able to tell it's never the last of cat.
3809840	3813840	You'll watch this for hours.
3813840	3817840	And so let me give you another, this is kind of a paper that's often cited
3817840	3821840	or a set of papers
3821840	3825840	to illustrate how difficult computer vision is.
3825840	3829840	How thin the line that we're walking with
3829840	3833840	all of these impressive results that we've been able to show recently in the machine
3833840	3837840	learning community. In this case
3837840	3841840	for deep neural networks are easily fooled paper.
3841840	3845840	The seminal paper at this point shows that when you apply
3845840	3849840	a network trained on ImageNet, so
3849840	3853840	basically on detecting cats versus dogs or different categories inside images
3853840	3857840	if you're, you can find
3857840	3861840	an arbitrary number of images that look like noise up in the top row.
3861840	3865840	Where the algorithm used
3865840	3869840	to classify those images in ImageNet of cat versus dog
3869840	3873840	is able to confidently say with 99.6% accuracy
3873840	3877840	above that it's seeing a robin or a cheetah
3877840	3881840	or an armadillo or a panda in that noise.
3881840	3885840	So it's confidently saying given this noise that that's obviously a robin.
3885840	3889840	So you have to realize that the kind
3889840	3893840	of, this is patterns, the kind
3893840	3897840	of processes it's using to understand what's contained in the image
3897840	3901840	is purely a collection of patterns that it has been able to
3901840	3905840	track from other images that has been human, annotated by humans.
3905840	3909840	And that perhaps is very limiting
3909840	3913840	to trying to create a system that's able to operate in the real world.
3913840	3917840	This is a very, so this is a very clean illustration
3917840	3921840	of that concept. In the same you can confidently predict
3921840	3925840	in those images below where there's strong patterns, it's not even noise.
3925840	3929840	Strong patterns that have nothing to do with the entities being detected.
3929840	3933840	Again, confidently that same algorithm is able to see a penguin,
3933840	3937840	a starfish, a baseball, and a guitar in that noise.
3937840	3941840	And more serious for people
3941840	3945840	designing robots like myself on the
3945840	3949840	sensor side, you can flip that and say
3949840	3953840	I can take a image
3953840	3957840	and I can distort it with some very little amount of noise.
3957840	3961840	And if that noise is applied to the image
3961840	3965840	I can completely change the confident prediction about what's in that image.
3965840	3969840	So to explain what's being shown. So on the left, the column in the left,
3969840	3973840	and again here, what's
3973840	3977840	the same kind of neural network is able to predict
3977840	3981840	accurately, confidently, that there is a dog in that image.
3981840	3985840	But if we apply just a little bit of noise to that image, to produce
3985840	3989840	that image, imperceptible to our human eyes, the difference between those two,
3989840	3993840	the same algorithm is saying that there's confidently an ostrich
3993840	3997840	in that image. So another thing to really think about
3997840	4001840	that noise can have such a significant impact on the
4001840	4005840	prediction of these algorithms. This is really, really
4005840	4009840	quite honestly out of all the things I'll say today and I'm aware of
4009840	4013840	one of the biggest challenges of
4013840	4017840	machine learning being applied in the real world is robustness.
4017840	4021840	How much noise can you add into the system before
4021840	4025840	everything falls apart? So how do you validate
4025840	4029840	sensors? So say a car company has to produce a vehicle
4029840	4033840	and it has sensors in that vehicle. How do you know that those sensors
4033840	4037840	will not start generating slight noise due to interference of various
4037840	4041840	kinds? And because of that noise, instead of seeing a pedestrian
4041840	4045840	it will see nothing. Or the opposite, it will see pedestrians everywhere.
4045840	4049840	So of course the most dangerous is when it will not see an object
4049840	4053840	and collide with it, in the case of cars. There's also spoofing, which a lot of
4053840	4057840	people as always with security, people are really concerned about.
4057840	4061840	And perhaps people here are really concerned about this issue. I think this
4061840	4065840	is a really important issue, but because you can apply noise and convince the system
4065840	4069840	that you're seeing an ostrich when there's in fact no ostrich,
4069840	4073840	you can do the same thing in an
4073840	4077840	attacking way. So you can attack the sensors of a car and make
4077840	4081840	it believe like with lidar spoofing. So spoof lidar, radar, ultrasonic
4081840	4085840	sensors to believe that you're seeing pedestrians when they're not there
4085840	4089840	and the opposite. To hide pedestrians, make pedestrians invisible
4089840	4093840	to the sensor when they're in fact there.
4093840	4097840	So whenever you have indulgent systems operating in this world
4097840	4101840	they become susceptible to
4101840	4105840	the fact that everything, so much of the work is done in software and based on
4105840	4109840	sensors. So at any point in the chain if there's a failure
4109840	4113840	you have to be able to detect that failure and right now we have no mechanisms for
4113840	4117840	automatically detecting that failure. So on the data side
4117840	4121840	so one challenge is that we're constantly
4121840	4125840	dealing with is
4125840	4129840	we, the algorithms in machine learning algorithms
4129840	4133840	that we're using are need labeled
4133840	4137840	data and we have very little labeled data.
4137840	4141840	Labeled data again is when you have pairs of
4141840	4145840	input data and the ground truth, the
4145840	4149840	true label annotation class that
4149840	4153840	that image belongs to or concept. And it doesn't have to
4153840	4157840	be an image, it can be any source of data. It's a really costly process to
4157840	4161840	do. So because it's so costly
4161840	4165840	we rely
4165840	4169840	every breakthrough we've had so far relies on that
4169840	4173840	labeled data and because of its cost we don't
4173840	4177840	have much of it. So all the problems that come from data can either
4177840	4181840	be solved by having a lot more of this data which I believe is
4181840	4185840	and most people believe is too challenging. It's too challenging to have
4185840	4189840	human beings annotate huge amounts of data or we have to develop
4189840	4193840	algorithms that are able to do something with the unlabeled data.
4193840	4197840	It's the unsupervised, semi-supervised, sparsely supervised
4197840	4201840	reinforcement learning. As we talked about last time I'll mention again
4201840	4205840	here. So one way you understand
4205840	4209840	something about data when you don't have labels is you reason
4209840	4213840	about it. All you're given is a few facts. When you're a baby
4213840	4217840	your parents give you a few facts and you go into this world with those facts
4217840	4221840	and you grow your knowledge graph, your knowledge base, your understanding of the world from those
4221840	4225840	few facts. We don't have a good method of doing that in an automated, unrestricted
4225840	4229840	way. The inefficiency of our
4229840	4233840	learners. The machine learning algorithms I've talked about in neural networks need
4233840	4237840	a lot of examples of every single concept that they're given in order to learn anything
4237840	4241840	about them. Thousands, tens of thousands of cats are needed to
4241840	4245840	understand what the spatial patterns at every level
4245840	4249840	the representation of a cat, the visual representation of a cat.
4249840	4253840	We don't, we can't do anything with a single example. There's a few approaches
4253840	4257840	but nothing quite robust
4257840	4261840	yet. And we haven't come up with
4261840	4265840	a way, this is also possible, to make
4265840	4269840	annotation, this labeling process, somehow be
4269840	4273840	very cheap. So leveraging, this is something been called
4273840	4277840	human computation. That term is falling out of favor a little bit.
4277840	4281840	One of my big passions is human computation is using
4281840	4285840	something about our behavior, something about what we do in this world online
4285840	4289840	or in the real world to annotate data automatically.
4289840	4293840	So for example
4293840	4297840	as you drive, which is what we do, everybody has to
4297840	4301840	drive and we can collect data about you driving in order to train self-driving vehicles
4301840	4305840	to drive. And that's a
4305840	4309840	free annotation. So here are the annotated data sets
4309840	4313840	we have. The supervised learning data sets.
4313840	4317840	There's many, but these are some of the more famous ones.
4317840	4321840	From the toy data sets of MNIST to the large, broad
4321840	4325840	arbitrary categories of images data sets
4325840	4329840	which is what ImageNet is. And there's
4329840	4333840	in healthcare, there's in audio, there's in video, there's
4333840	4337840	a huge number of data sets now, but each one of them is usually
4337840	4341840	on the scale of hundreds of thousands, millions, tens of millions
4341840	4345840	not billions or trillions, which is what we need to create
4345840	4349840	systems that operate in the real world. And again
4349840	4353840	these are the kinds of machine learning algorithms we have. There's five listed
4353840	4357840	here. The teachers on the left
4357840	4361840	is what is the input
4361840	4365840	to the system that requires to train it. From the supervised
4365840	4369840	learning at the very top is what we have all of our successes. And everything else is where the
4369840	4373840	promise lies. The semi-supervised, the reinforcement
4373840	4377840	or the fully unsupervised learning, where the input from the human is very
4377840	4381840	minimal. And another way to think about this, so
4381840	4385840	whenever you think about machine learning today, whenever somebody talks about machine
4385840	4389840	learning, what they're talking about is systems that memorize.
4389840	4393840	That memorize patterns. And so this is one of the big
4393840	4397840	criticisms of the current machine learning approaches, where all they're doing is
4397840	4401840	you're providing, they're only as good as the human annotated
4401840	4405840	data that they're provided. We don't have mechanisms for actually
4405840	4409840	understanding. You can pause and think about this. In order to
4409840	4413840	create an intelligent system it shouldn't just memorize. It should understand
4413840	4417840	the representations inside that data in order to operate in that
4417840	4421840	world. And that's the open question.
4421840	4425840	One of them. And one of the challenges and opportunities for machine learning
4425840	4429840	researchers today is to extend machine learning
4429840	4433840	from memorization to understanding. This is that
4433840	4437840	duck. The reasoning
4437840	4441840	if you get information from the perception systems that it
4441840	4445840	looks like a duck, from the audio processing that it quacks like a duck,
4445840	4449840	and then from video classification that the activity recognition that it
4449840	4453840	swims like a duck, the reasoning step is how to connect those
4453840	4457840	facts to then say that it is in fact a duck.
4457840	4461840	Okay, so that's on the algorithm side and the
4461840	4465840	data side. Now this is one of the reasons
4465840	4469840	computational power, computational hardware that is at the core
4469840	4473840	of the success of machine learning.
4473840	4477840	So our algorithms have been the same since the 60's, since the
4477840	4481840	80's, 90's depending on how you're counting. The big
4481840	4485840	breakthroughs came in compute. So there's Moore's law.
4485840	4489840	Most of you know the way the CPU
4489840	4493840	side of our computers works for a single CPU is that it's
4493840	4497840	for the most part executing a single action at a time
4497840	4501840	in a sequence. So sequential. Very different from
4501840	4505840	our brain which is a massively parallelized system.
4505840	4509840	So because it's sequential the clock speed matters because that's how fast
4509840	4513840	essentially those instructions are able to be executed. And so
4513840	4517840	where we're leveling off, physics
4517840	4521840	is stopping us from continuing Moore's law.
4521840	4525840	Intel, AMD are aggressively pushing this Moore's law
4525840	4529840	forward. But, and there's
4529840	4533840	some promise that it will actually continue for another 10 or 15 years.
4533840	4537840	Then there's another
4537840	4541840	form of parallelism, massive parallelism. It's the GPU.
4541840	4545840	This is essential for neural networks.
4545840	4549840	This is essential to the success, recent success of neural networks is the
4549840	4553840	ability to utilize these inherently parallel
4553840	4557840	architectures of graphics processing
4557840	4561840	units, GPUs. The same thing used for video games. This is the
4561840	4565840	reason NVIDIA stock is doing
4565840	4569840	extremely well, is GPUs. So it's
4569840	4573840	parallelism of basic computational processes that make
4573840	4577840	machine learning work on the GPU. One of the
4577840	4581840	limitations of GPUs, one of the challenges is
4581840	4585840	in bringing them to, in scaling and bringing them into real world applications is
4585840	4589840	power usage, is power consumption. And so there is
4589840	4593840	a lot of specialized chips specialized just
4593840	4597840	from the neural network architectures coming out from Google
4597840	4601840	with their tensor processing unit from IBM, Intel and so on.
4601840	4605840	It's unclear how far this goes. So this is sort of the direction
4605840	4609840	of trying to design an electronic brain, so it has the efficiency.
4609840	4613840	Our human brain is exceptionally efficient at running the neural networks
4613840	4617840	in our heads. Or does the magnitude more efficient
4617840	4621840	than our computers are? And this is trying to design systems that are able to
4621840	4625840	go towards that efficiency. Why do you care about
4625840	4629840	efficiency? For several reasons. One, of course,
4629840	4633840	I'm sure we'll talk about throughout this class is about the thing in our
4633840	4637840	smartphones, battery usage. And this
4637840	4641840	is the big one, community. I think
4641840	4645840	it could be attributed to the big
4645840	4649840	breakthroughs in machine learning recently in the last decade is
4649840	4653840	the, you know, compute is important, algorithm development
4653840	4657840	is important, but it's the community
4657840	4661840	of nerds, global. This is global artificial intelligence
4661840	4665840	and I will show in several ways why global is
4665840	4669840	essential here is tens of
4669840	4673840	hundreds of thousands, millions of programmers, mechanical
4673840	4677840	engineers, building robots, building intelligence systems,
4677840	4681840	building machine learning algorithms. The exciting nature
4681840	4685840	of the growth of the community perhaps is the key
4685840	4689840	to the future to unlocking the power of machine learning. So this is just
4689840	4693840	one example of GitHub as a repository for code and this is showing on
4693840	4697840	the y-axis at the bottom is 2008 when GitHub first opened and this is
4697840	4701840	going up to 2012. Quick, near exponential
4701840	4705840	growth of the number of users participating and the number of repositories. So these are
4705840	4709840	standalone unique projects that are being hosted on GitHub.
4709840	4713840	So this is one example I'll show you about this competition
4713840	4717840	that we're recently running and then I'll challenge people here to participate in this competition
4717840	4721840	if you dare. So this is a
4721840	4725840	chance for you to build a neural network in your browser
4725840	4729840	so you can do this on your phone later tonight of course.
4729840	4733840	On your phone you can specify various
4733840	4737840	parameters of the neural network, specify different numbers of layers and the depth
4737840	4741840	of the network, the number of neurons in the network, the type of layers and it's pretty
4741840	4745840	self-explanatory, super easy in terms of just
4745840	4749840	tweaking little things and remember machine learning to a large
4749840	4753840	part is an art at this point. It's
4753840	4757840	more perhaps than even, you know, more than a well understood
4757840	4761840	theoretically bounded science which is one of the challenges but it's also an opportunity.
4761840	4765840	Deep traffic is a chance, so we've all been stuck
4765840	4769840	in traffic. There you go, Americans spend 8 billion hours stuck in traffic every year.
4769840	4773840	That's our pitch for this competition. So deep neural network can help
4773840	4777840	and so you have a neural network that drives that little car with an
4777840	4781840	MIT logo, red one, on this highway and tries to weave in and out of traffic to get
4781840	4785840	to his destination and trying to achieve a speed of
4785840	4789840	80 miles an hour which is the speed limit which is the physical
4789840	4793840	speed limit of the car. Of course the actual speed limit of the road is 65 miles an hour
4793840	4797840	but we don't care about that. We just want to get to work as quickly as possible at home.
4797840	4801840	So what the basic
4801840	4805840	structure of this game is and I want to explain this game a little bit and then tell
4805840	4809840	you how incredibly popular it's gotten and how incredibly
4809840	4813840	powerful the
4813840	4817840	networks that people have built from all over the world. The community has built
4817840	4821840	of this over a single month is incredible and this happens for
4821840	4825840	thousands of projects out there. Now another challenging
4825840	4829840	opportunity. Okay so you may have seen this, this is kind of ethics.
4829840	4833840	Most engineers, I personally don't like, I
4833840	4837840	love philosophy but this kind of construction of
4837840	4841840	ethics that's often presented here is one that is not usually
4841840	4845840	concerned to engineering. So what is this question? You know when you have a car
4845840	4849840	and you have a bunch of pedestrians, do you hit the larger group of pedestrians
4849840	4853840	or the smaller group of pedestrians? Do you avoid the group
4853840	4857840	of pedestrians but put yourself into danger? These kinds of ethical
4857840	4861840	questions of an intelligent system. It's a very interesting question.
4861840	4865840	It's one that we can debate and there's really no good answer quite honestly
4865840	4869840	but it's a problem that both humans and machines struggle with and so it's
4869840	4873840	not interesting on the engineering side. We're interested with problems that we can
4873840	4877840	solve on the engineering side. So the kind of problem that I'm obsessed with
4877840	4881840	and very interested in is the real world problem of controlling a vehicle through this space.
4881840	4885840	So it happens in a few seconds
4885840	4889840	here. So this is a Manhattan-New York intersection, right?
4889840	4893840	This is pedestrians walking perfectly
4893840	4897840	legally. I think they have a green light. Of course there's a lot of jaywalking too as well.
4897840	4901840	Well this car just, it's not part of the
4901840	4905840	point but yes, exactly, there's an ambulance. And so there's another car that starts
4905840	4909840	making a left turn in a little bit. Let me admit that hopefully not.
4909840	4913840	But yeah, and then there's another car after that too that just illustrates
4913840	4917840	when you design an algorithm that's supposed to move through this space
4917840	4921840	like watch this car. The aggression it shows. Now this isn't
4921840	4925840	a trivial example for those that try to build robots. This is the real question
4925840	4929840	is how do you design a system
4929840	4933840	that's able, so you have to think. You have to put
4933840	4937840	reward functions, objective functions, utility functions under which
4937840	4941840	it performs the planning. So a car like that has
4941840	4945840	several thousand candidate trajectories you can take through that
4945840	4949840	intersection. You can take a trajectory where it speeds up to 60 miles an hour and doesn't stop
4949840	4953840	and just swerves and hits everything. Okay, that's a bad trajectory, right? Then
4953840	4957840	there's a trajectory which most companies take, most
4957840	4961840	Google self-driving car and every company that's concerned about PR is
4961840	4965840	whenever there's any kind of obstacle, any kind of risk that's
4965840	4969840	at all reasonable that you can maybe even touch an obstacle then you're not going
4969840	4973840	to take that trajectory. So what that means is you're going to navigate through this intersection
4973840	4977840	at 10 miles an hour and let people abuse you by walking in front of you
4977840	4981840	because they know you're not going to stop. And so in the middle there is
4981840	4985840	hundreds, thousands of trajectories that are ethically questionable
4985840	4989840	in the sense that you're putting other human beings at risk in order to
4989840	4993840	safely and successfully navigate through the intersection. And the design of those
4993840	4997840	objective functions is the kind of question you have to ask
4997840	5001840	for intelligence systems for cars. There's no
5001840	5005840	grandma and a few children you have to choose who gets to
5005840	5009840	die. Very difficult problems, of course. But
5009840	5013840	the problem of what I'm very interested in in streets of Boston
5013840	5017840	and streets of New York is how to gently nudge yourself
5017840	5021840	through a crowd of pedestrians in the way we all actually do
5021840	5025840	when we drive in New York in order to be able to safely
5025840	5029840	navigate these environments. And these questions come up in healthcare, these questions come up
5029840	5033840	in factory, in robots, in armed and humanoid robots
5033840	5037840	that operate with other human beings.
5037840	5041840	And that's one of the big challenges. Another sort of
5041840	5045840	fun illustration that folks at OpenAI use often
5045840	5049840	to illustrate, well let me just pause for a second, the gamified version
5049840	5053840	of this. There's a game called Coast Runners and you're racing against other boats
5053840	5057840	along this track. And your job is, there's your score here
5057840	5061840	at the bottom left, number of laps, your time, and you're
5061840	5065840	trying to get to the destination as quickly as possible while also collecting
5065840	5069840	funky little things like these green
5069840	5073840	these green little things along the way. Okay, so
5073840	5077840	what they've done is a build intent system, the one, the general purpose one that we
5077840	5081840	talked about last time that learns, oops, that learns
5081840	5085840	how to navigate successfully through the space. So
5085840	5089840	you're trying to maximize the reward. And what this boat
5089840	5093840	learns to do is instead of finishing the race
5093840	5097840	it learns to find a loop where
5097840	5101840	it can keep going around and around, collecting those green dots
5101840	5105840	and it learns the fact that they regenerate with time.
5105840	5109840	So it learns to maximize this score
5109840	5113840	by going around and around. Now these are the kinds of things
5113840	5117840	this is the big challenge of reward functions, of designing systems
5117840	5121840	of designing what you want your system to achieve is
5121840	5125840	not only is it difficult to, the ethical questions are difficult
5125840	5129840	but just avoiding the pitfalls of local optima
5129840	5133840	of figuring out something really good that happens
5133840	5137840	in the short term, the greedy, what are those, those psychology experiments of the kid
5137840	5141840	eats the marshmallow and can't wait for, you know, can't
5141840	5145840	delay gratification. This kind of, the idea of delay gratification
5145840	5149840	in the case of designing intelligence systems is a huge actual serious problem
5149840	5153840	and this is a good illustration of that.
5153840	5157840	So we flew through a few concepts here
5157840	5161840	is there any questions about
5161840	5165840	the compute and the algorithm side we talked about today?
5165840	5169840	So the question was, yeah you highlighted some of the limitations of
5169840	5173840	machine, computer vision algorithms, machine learning algorithms, but
5173840	5177840	you haven't highlighted some of the limitations of human beings and if you put those in a column
5177840	5181840	and you compare those, are machines doing better
5181840	5185840	overall or is there any kind of way to compare those? I mean there is actually
5185840	5189840	interesting work on ImageNet, so ImageNet is this categorization
5189840	5193840	task of where you have to classify images and you can ask the question
5193840	5197840	when I present you images of cats and dogs, where are machines better than humans
5197840	5201840	and when are they not? So you can compare when machines do better,
5201840	5205840	what are the fail points and what are the fail points for humans and there's a lot of interesting
5205840	5209840	visual perception questions there. But I think overall it's certainly
5209840	5213840	true that machines fail differently than human beings, but
5213840	5217840	in order to make an artificial intelligence system
5217840	5221840	usable and could make you a lot of money
5221840	5225840	and people would want to use, it has to be better for that particular task
5225840	5229840	in every single way. In order
5229840	5233840	for you to want to use a system, it has to be superior
5233840	5237840	to human performance and usually far superior to human performance.
5237840	5241840	So on the philosophical level it's an interesting thing to compare
5241840	5245840	what are we good at, what are not, but if you're using
5245840	5249840	Amazon Echo, your voice recognition
5249840	5253840	or any kind of natural language, chatbots, or a car,
5253840	5257840	you're not going to be, well this car is not so good with pedestrians, but I appreciate the fact
5257840	5261840	that you can stay in the lane. Fortunately you have a very high standard
5261840	5265840	for every single thing that you're good at and it has to be superior to that. I think
5265840	5269840	maybe that's unfair to the robots.
5269840	5273840	I'm more of the nerd that makes the technology happen
5273840	5277840	but it's certainly on the self-driving car aspect
5277840	5281840	policy is probably the biggest challenge and I don't think there's good answers
5281840	5285840	there. Some of those ethical questions that
5285840	5289840	come up, it feels like, so we work a lot with Tesla
5289840	5293840	so I'm driving a Tesla around every day and we're playing around with it
5293840	5297840	and studying human behavior inside Tesla and it seems like there's so much
5297840	5301840	hunger amongst the media to jump on something and it feels like
5301840	5305840	a very shaky PR terrain, a very shaky
5305840	5309840	policy terrain we're all walking because we have no idea how
5309840	5313840	we coexist with intelligent systems and then of course
5313840	5317840	the government is nervous because how do we regulate this shaky terrain
5317840	5321840	and everybody's nervous and excited.
5321840	5325840	That's a perfect transition point if that's okay.
5325840	5329840	That same kind of question to Jason in a moment. Thanks a lot Lex for another great session.
5329840	5333840	Thank you.
