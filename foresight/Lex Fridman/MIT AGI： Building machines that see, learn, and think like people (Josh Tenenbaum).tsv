start	end	text
0	6000	Today we have Josh Tenenbaum. He's a professor here at MIT, leading the Computational Cognitive
6000	12000	Science Group. Among many other topics in cognition and intelligence, he is fascinated
12000	18000	with the question of how human beings learn so much from so little, and how these insights
18000	23480	can lead to build AI systems that are much more efficient learning from data. So please
23480	25480	give Josh a warm welcome.
31480	37480	Alright, thank you very much. Thanks for having me. Excited to be part of what looks like
37480	43480	really quite a very impressive lineup, especially starting after today. And it's, I think, quite a
43480	47480	great opportunity to get to see perspectives on artificial intelligence from many of the
47480	54480	leaders in industry and other entities working on this great quest. So I'm going to talk to you
54480	58480	about some of the work that we do in our group, but also I'm going to try to give a broader
58480	62480	perspective, reflective of a number of MIT faculty, especially those who are affiliated with the
62480	67480	Center for Brains, Minds, and Machines. So you can see up there on my affiliation, academically
67480	72480	I'm part of Brain and Cognitive Science, or Course 9. I'm also part of CSAIL, but I'm also part of
72480	77480	the Center for Brains, Minds, and Machines, which is an NSF-funded center, Science and Technology
77480	82480	Center, which really stands for the bridge between the science and the engineering of intelligence. It
82480	86480	literally straddles Vassar Street in that we have CSAIL and BCS members. We also have partners at
86480	91480	Harvard and other academic institutions. And again, what we stand for, I want to try to convey
91480	95480	some of the specific things we're doing in the center and where we want to go with a vision that
95480	101480	really is about jointly pursuing the science, the basic science of how intelligence arises in the
101480	107480	human mind and brain, and also the engineering enterprise of how to build something increasingly
107480	111480	like human intelligence in machines. And we deeply believe that these two projects have something to do
111480	117480	with each other and are best pursued jointly. Now it's a really exciting time to be doing anything
117480	122480	related to intelligence or certainly to AI for all the reasons that, you know, brought you all here.
122480	127480	I don't have to tell you this. We have all these ways in which AI is kind of finally here. We finally
127480	133480	live in the era of something like real practical AI, or for those who've been around for a while and
133480	138480	have seen some of the rises and falls, you know, AI is back in a big way. But from my perspective,
138480	144480	and I think maybe this reflects, you know, why we distinguish what we might call AGI from AI, we don't
144480	149480	really have any real AI, basically. We have what I like to call AI technologies, which are systems
149480	154480	that do things we used to think that only humans could do. And now we have machines that do them,
154480	160480	often quite well, maybe even better than any human who's ever lived, right? Like a machine that plays go.
160480	165480	But none of these systems, I would say, are truly intelligent. None of them have anything like common sense.
165480	171480	None of them have anything like the flexible general purpose intelligence that each of you might use to learn
171480	177480	every one of these skills or tasks, right? Each of these systems had to be built by large teams of engineers,
177480	182480	working together often for a number of years at often at great cost to somebody who's willing to pay for it.
182480	189480	And each of them just does one thing. So AlphaGo might beat the world's best, but it can't drive to the match,
189480	196480	or even tell you what go is. It can't even tell you that go is a game, because it doesn't even know what a game is, right?
196480	203480	So what's missing? What is it that makes every one of your brains, maybe you can't beat, you know, the world's best in go,
203480	209480	but any one of you can get behind the wheel of a car. I think of this because my daughter is going to turn 16 tomorrow.
209480	216480	If she lived in California, she'd have a driver's license. It's a little bit down the line for us here in Massachusetts.
216480	221480	But, you know, she didn't have to be specially engineered by billion-dollar startups.
221480	226480	And, you know, she got really into chess recently, and now she's taught herself chess by playing just, you know,
226480	231480	and a handful of games, basically. And she can do any one of these activities, and any one of us can.
231480	236480	So what is it? What makes up the difference? Well, there's many things, right?
236480	244480	I'll talk about the focus for us in our research, and a lot of us, again, in CBMM, is summarized here.
244480	253480	What drives the successes right now in AI, especially in industry, okay, and all these AI technologies, is many, many things, many things.
253480	259480	But what's where the progress has been made most recently, and what's getting most of the attention is, of course, deep learning,
259480	267480	but other kinds of machine learning technologies, which essentially represent the maturation of a decades-long effort to solve the problem of pattern recognition.
267480	277480	That means taking data and finding patterns in the data that tell you something you care about, like how to label a class or how to predict some other signal, okay?
277480	285480	And pattern recognition is great. It's an important part of intelligence, and it's reasonable to say that deep learning as a technology
285480	292480	has really made great strides on pattern recognition, and maybe even, you know, has come in close to solving the problems of pattern recognition.
292480	299480	But intelligence is about many other things. Intelligence is about a lot more. In particular, it's about modeling the world,
299480	306480	and think about all the activities that a human does to model the world that go beyond, just say, recognizing patterns in data,
306480	310480	but actually trying to explain and understand what we see, for instance, okay?
310480	317480	Or to be able to imagine things that we've never seen, that never seen, maybe even very different from anything we've ever seen,
317480	324480	but what I want to see, and then to set those as goals, to make plans and solve problems needed to make those things real,
324480	329480	or thinking about learning, again, that, you know, some kinds of learning can be thought of as pattern recognition
329480	333480	if you're learning sufficient statistics or weights in a neural net that are used for those purposes.
333480	337480	But many activities of learning are about building out new models, right?
337480	344480	Either refining, reusing, improving old models, or actually building fundamentally new models as you've experienced more of the world.
344480	349480	And then think about sharing our models, communicating our models to others, modeling their models, learning from them.
349480	357480	All these activities of modeling, these are at the heart of human intelligence, and it requires a much broader set of tools.
358480	363480	So I want to talk about the ways we're studying these activities of modeling the world and something in a pretty non-technical way
363480	367480	about what are the kind of tools that allow us to capture these abilities.
367480	372480	Now, I think it's, I want to be very honest up front and to say this is just the beginning of a story, right?
372480	377480	When you look at deep learning successes, that itself is a story that goes back decades.
377480	379480	I'll say a little bit about that history in a minute.
379480	384480	But where we are now is just looking forward to a future when we might be able to capture these abilities,
385480	387480	at a really mature engineering scale.
387480	394480	And I would say we are far from being able to capture all the ways in which humans richly, flexibly, quickly build models of the world
394480	396480	at the kind of scale that, say, Silicon Valley wants.
396480	402480	Either big tech companies like Google, or Microsoft, or IBM, or Facebook, or small startups, right?
402480	404480	We can get there.
404480	408480	And I think what I want to talk to you about here is one route for trying to get there.
408480	411480	And this is the route that CBMM stands for.
411480	415480	The idea that by reverse engineering how intelligence works in the human mind and brain,
415480	419480	that will give us a route to engineering these abilities in machines.
419480	423480	When we say reverse engineering, we're talking about science, but doing science like engineers.
423480	425480	This is our fundamental principle.
425480	428480	That if we approach cognitive science and neuroscience like an engineer,
428480	432480	where, say, the output of our science isn't just a description of the brain or the mind in words,
432480	436480	but in the same terms that an engineer would use to build an intelligent system,
436480	440480	then that will be both the basis for a much more rigorous and deeply insightful science,
440480	445480	but also direct translation of those insights into engineering applications.
445480	450480	Now, before I talk a little bit about history, what I mean by that is this.
450480	453480	Again, if part of what brought you here is deep learning, and I know,
453480	456480	even if you've never heard of deep learning before, which I'm sure is unlikely,
456480	462480	you saw a good spectrum of that in the overview session last night.
462480	468480	It's really interesting and important to look back on the history of where did techniques for deep learning come from,
468480	469480	or reinforcement learning.
469480	474480	Those are the two tools in the current machine learning arsenal that are getting the most attention.
474480	477480	Things like back propagation or end-to-end stochastic gradient descent
477480	479480	or temporal difference learning or Q-learning.
479480	481480	Here's a few papers from the literature.
481480	483480	Maybe some of you have read these original papers.
483480	487480	Here's the original paper by Rumelhardt, Hinton, and colleagues
487480	491480	in which they introduced the back propagation algorithm for training multi-layer perceptrons,
491480	492480	multi-layer neural networks.
492480	494480	Here's the original perceptron paper by Rosenblatt,
494480	497480	which introduced the one-layer version of that architecture
497480	499480	and the basic perceptron learning algorithm.
499480	503480	Here's the first paper on the temporal difference learning method
503480	506480	for reinforcement learning from Sutton and Bartow.
506480	510480	Here's the original Boltzmann machine paper also by Hinton and colleagues,
510480	518480	those of you who don't know that architecture think of a probabilistic undirected multi-layer perceptron.
518480	523480	For example, before there were LSTMs, if you know about current-recurrent neural network architecture,
523480	527480	earlier much simpler versions of the same idea were proposed by Jeff Elman
527480	529480	and his simple-recurrent networks.
529480	531480	The reason I want to put up the original papers here
531480	535480	is for you to look at both when they were published and where they were published.
535480	540480	If you look at the dates, you'll see papers going back to the 80s,
540480	543480	but even the 60s or even the 1950s.
543480	545480	Look at where they were published.
545480	547480	Most of them were published in psychology journals.
547480	549480	The journal Psychological Review, if you don't know,
549480	553480	it is like the leading journal of theoretical psychology and mathematical psychology,
553480	557480	or Cognitive Science, the journal of the Cognitive Science Society,
557480	561480	or the back prop paper was published in Nature, which is a general interest science journal,
561480	565480	but by people who are mostly affiliated with the Institute for Cognitive Science in San Diego.
565480	570480	What you see here is already a long history of scientists thinking like engineers.
570480	573480	These are people who are in psychology or cognitive science departments
573480	575480	and publishing in those places,
575480	580480	but by formalizing even very basic insights about how humans might learn
580480	584480	or how brains might learn in the right kind of math,
584480	587480	that led to, of course, progress on the science side,
587480	589480	but it led to all the engineering that we see now.
589480	591480	It wasn't sufficient.
591480	594480	We needed, of course, lots of innovations and advances
594480	597480	in computing, hardware, and software systems.
597480	600480	But this is where the basic math came from
600480	603480	and it came from doing science like an engineer.
603480	606480	What we want to talk about in our vision is what does the future of this look like?
606480	608480	If we were to look 50 years into the future,
608480	611480	what would we be looking back on now over this time scale?
611480	615480	Well, here's a long-term research roadmap that reflects some of my ambitions
615480	618480	and some of our center's goals and many others, too.
618480	622480	We'd like to be able to address basic questions, fundamental questions
622480	624480	of what it is to be and to think like a human,
624480	629480	questions, for example, of consciousness or meaning in language or real learning,
629480	633480	questions like, you know, even beyond the individual, like questions of culture
633480	635480	or creativity.
635480	637480	Those are our big ideas up there.
637480	639480	And for each of these, there are basic scientific questions, right?
639480	642480	How do we become aware of the world and ourselves in it?
642480	645480	It starts with perception, but it really turns into awareness,
645480	649480	awareness of yourself and of the world and what we might call consciousness, right?
649480	651480	Or how does a word start to have a meaning?
651480	654480	What really is a meaning and how does a child grasp it?
654480	655480	Or how do children actually learn?
655480	657480	What do babies' brains actually start with?
657480	660480	Are they blank slates or do they start with some kind of cognitive structure?
660480	662480	And then what does real learning look like?
662480	665480	These are just some of the questions that we're interested in working on.
665480	667480	Or when we talk about culture, we mean,
667480	671480	how do you learn all the things you didn't directly experience, right?
671480	675480	But that's somehow you got from the accumulation of knowledge in society over many generations.
675480	678480	Or how do you ever think of new ideas or answers to new questions?
678480	680480	How do you think of the new questions themselves?
680480	681480	How do you decide what to think about?
681480	684480	These are all key activities of human intelligence
684480	687480	when we talk about how we model the world, where our models come from,
687480	689480	what we do with our models, this is what we're talking about.
689480	691480	And if we could get machines that could do these things,
691480	696480	well, again, on the bottom row, think of all the actual real engineering payoffs.
696480	700480	Now, in our center, in both my own activities and a lot of what my group does these days
700480	704480	and what a number of other colleagues in the center for brains, minds, and machines do,
704480	707480	as well as very broadly people in BCS and CSAIL,
707480	710480	one place where we work on the beginnings of these problems in the near term.
710480	713480	This is the long term, like, think 50 years, okay?
713480	719480	Maybe shorter, maybe longer, I don't know, but think well beyond 10 years, okay?
719480	723480	But in the short term, 5 to 10 years, a lot of our focus is around visual intelligence.
723480	724480	And there's many reasons for that.
724480	727480	Again, we can build on the successes of deep networks
727480	729480	and a lot of pattern recognition and machine vision.
729480	731480	It's a good way to put these ideas into practice.
731480	735480	When we look at the actual brain, the visual system in the brain,
735480	738480	in the human and other mammalian brains, for example,
738480	741480	is really very clearly the best understood part of the brain.
741480	746480	And at a circuit level, it's the part of the brain that's most inspired current deep learning
746480	747480	and neural network systems.
747480	752480	But even there, there's things which we still don't really understand like engineers.
752480	755480	So here's an example of a basic problem in visual intelligence
755480	759480	that we and others in the center are trying to solve.
759480	764480	Look around you and you feel like there's a whole world around you.
764480	765480	And there is a whole world around you.
765480	767480	You feel like your brain captures it.
767480	770480	But what the actual sense data that's coming in through your eyes
770480	773480	looks more like this photograph here where you can see there's a crowd scene
773480	777480	but it's mostly blurry except for a small region of high resolution in the center.
777480	781480	So that corresponds biologically to what part of the image is in your fovea.
781480	783480	That's the central region of cells in the retina
783480	786480	where you have really high resolution visual data.
786480	790480	The size of your fovea is roughly like if you hold out your thumb at arm's length.
790480	792480	It's a little bit bigger than that but not much bigger, right?
792480	796480	Most of the image, in terms of the actual information coming in
796480	799480	in a bottom-up sense to your brain is really quite blurry.
799480	803480	But somehow by looking at just one part and then by saccading around
803480	806480	or making a few eye movements, you get a few glimpses,
806480	809480	each not much bigger than the size of your thumb at arm's length.
809480	812480	Somehow you stitch that information together into what feels like
812480	815480	and really is a rich representation of the whole world around you.
815480	818480	And when I say around you, I mean literally around you.
818480	820480	So here's another kind of demonstration.
820480	824480	Without turning around, nobody's allowed to turn around.
824480	826480	Ask yourself what's behind you.
826480	828480	Now the answer is going to be different for different people
828480	830480	depending on where you're sitting, right?
830480	833480	For most of you, you might think, well, I think there's a person
833480	835480	pretty close behind me, right?
835480	837480	You know you're in a crowded auditorium, although you haven't seen that person.
837480	840480	You know that they're there, right?
840480	843480	For people in the very back row, you know there isn't a person behind you
843480	845480	and you're conscious of being in the back row, right?
845480	847480	You might be conscious that there's a wall right behind you.
847480	851480	But now for the people who are in the room, not in the very back,
851480	853480	think about how far behind you is the back,
853480	855480	like where's the nearest wall behind you?
855480	858480	Maybe we can call out, try a little demonstration.
858480	860480	So I don't know. I'm pointing to someone there.
860480	863480	Can you say something if you think I'm pointing at you?
863480	865480	Well, I could have been pointing at you,
865480	867480	but I'm pointing someone behind you, okay?
867480	869480	I'll point to you. Yeah, I'm pointing to you.
869480	871480	All right, so how far is the nearest wall...
871480	873480	No, you can't turn around. You've blown your chance.
873480	876480	Without turning around. Okay, so you were...
876480	878480	Okay, do you see I'm pointing to you there with the tie?
878480	882480	Okay, so without turning around, how far is the nearest wall behind you?
886480	888480	Sorry, how far?
888480	891480	Five meters. Okay, well, I mean, that might be about right.
891480	894480	Other people can turn around.
894480	897480	Now, how about you? How far is the nearest wall behind you?
897480	899480	Ten meters.
899480	901480	Ten meters, okay.
901480	903480	That might be right, yeah.
903480	906480	How about here? What do you think?
906480	908480	20, okay.
908480	910480	So yeah, since I didn't grow up in the metric system, I barely know.
910480	912480	But yeah, I mean...
912480	914480	The point is that like, you're...
914480	917480	Each of you is surely not exactly right,
917480	919480	but you're certainly within an order of magnitude,
919480	922480	and I guess if we actually tried to measure, you know, you're probably...
922480	925480	My guess is you're probably right within, you know, 50% or less,
925480	928480	often, you know, maybe just 20% error.
928480	930480	Okay, so how do you know this?
930480	932480	I mean, even if it's not... What did you say, 20 meters?
932480	935480	Even if it's not 20 meters, it's probably closer to 20 meters
935480	938480	than it is to 5 or 10 meters, and then it is to 50 meters.
938480	941480	So how did you know this? You haven't turned around in a while, right?
941480	945480	So how do you know if your brain is tracking the whole world around you, right?
945480	948480	And how many people are behind you?
948480	950480	Yeah, like a few hundred, right?
950480	952480	I mean, I don't know if it's 200 or 300 or...
952480	954480	But it's not a thousand.
954480	956480	I don't think so.
956480	958480	And it's certainly not 10 or 20 or 50, right?
958480	962480	So you track these things, and you use them to plan your actions.
962480	965480	Okay, so again, think about how instantly, effortlessly,
965480	967480	and very reliably, okay?
967480	969480	Your brain computes all these things.
969480	971480	There's a lot of people and objects around you,
971480	973480	and it's not just approximations.
973480	976480	Certainly, when we're talking about what's behind you in space,
976480	978480	there's a lot of imprecision.
978480	980480	But when it comes to reaching for things right in front of you,
980480	983480	very precise shape and physical property estimates
983480	985480	needed to pick up and manipulate objects.
985480	988480	And then when it comes to people, it's not just the existence of the people,
988480	990480	but something about what's in their head, right?
990480	992480	You track whether someone's paying attention to you
992480	994480	and you're talking to them, what they might want from you,
994480	996480	what they might be thinking about you,
996480	998480	what they might be thinking about other people, okay?
998480	1000480	So when we talk about visual intelligence,
1000480	1002480	this is the whole stuff we're talking about.
1002480	1005480	And you can start to see how it turns into basic questions, I think,
1005480	1009480	of what we might call the beginnings of consciousness,
1009480	1012480	or at least our awareness of our self in the world,
1012480	1015480	and of ourselves as a self in the world,
1015480	1018480	but also other aspects of higher level intelligence and cognition
1018480	1020480	that are not just about perception, like symbols, right?
1020480	1024480	To describe even to ourselves what's around us and where we are
1024480	1026480	and what we can do with it.
1026480	1030480	We can go beyond just what we would normally call the stuff of perception
1030480	1034480	to say the thoughts in somebody's head and your own thoughts about that, okay?
1034480	1036480	So what we've been doing in CBMM
1036480	1039480	is trying to develop an architecture for visual intelligence.
1039480	1042480	And I'm not going to go into any of the details of how this works,
1042480	1044480	and this is just notional, this is just a picture,
1044480	1047480	it's like a sketch from a grand proposal of what we say we want to do.
1047480	1050480	But it's based on a lot of scientific understanding
1050480	1052480	of how the brain works.
1052480	1055480	There are different parts of the brain that correspond to these different modules
1055480	1058480	of architecture, as well as some kind of emerging engineering way
1058480	1061480	to try to capture at the software and maybe even hardware levels
1061480	1063480	how these modules might work.
1063480	1066480	So we talk about sort of an early module of visual or perceptual stream,
1066480	1070480	which is like bottom-up visual or other perceptual input.
1070480	1073480	That's the kind of thing that is pretty close to what we currently have
1073480	1075480	in say deep convolutional neural networks.
1075480	1080480	But then we talk about some kind of, the output of that isn't just pattern class labels,
1080480	1083480	but what we call the cognitive core or core cognition.
1083480	1086480	And an understanding of space and objects, their physics,
1086480	1089480	other people, their minds, that's the real stuff of cognition
1089480	1092480	that has to be the output of perception.
1092480	1096480	But somehow we have to have, this is what we call the brain OS in this picture,
1096480	1099480	we have to get there by stitching together the bottom-up inputs
1099480	1102480	from a glimpse here, a glimpse here, a little bit here and there,
1102480	1105480	and accessing prior knowledge that comes from our memory systems
1105480	1108480	to tell us how to stitch these things together
1108480	1112480	into the really core cognitive representations of what's out there in the world.
1112480	1115480	And then if we're going to start to talk about it in language
1115480	1120480	or to build plans on top of what we have seen and understood,
1120480	1124480	that's where we talk about symbols coming into the picture.
1124480	1128480	The building blocks of language and plans and so on.
1128480	1131480	So now we might say, well, okay, this is an architecture
1131480	1134480	that is brain-inspired and cognitively inspired
1134480	1137480	and we're planning to turn into real engineering.
1137480	1139480	And you can say, well, do we need that?
1139480	1142480	Again, I know this is a question you considered in the first lecture.
1142480	1146480	Maybe the engineering toolkit that's currently been making a lot of progress
1146480	1148480	in, let's say, industry, maybe that's good enough.
1148480	1150480	Maybe, you know, let's take deep learning
1150480	1154480	but to stand for a broader set of modern pattern recognition-based
1154480	1156480	and reinforcement learning-based tools
1156480	1160480	and say, okay, well, maybe that can scale up to this.
1160480	1162480	And you might, you know, maybe that's possible.
1162480	1164480	I'm happy in the question period if people want to debate this.
1164480	1166480	My sense is, no.
1166480	1169480	I think that it's not, when I say no,
1169480	1172480	I don't mean like it can't happen or it won't happen.
1172480	1176480	What I mean is the highest value, the highest expected route right now
1176480	1179480	is to take this more science-based reverse engineering approach.
1179480	1182480	And that at least if you follow the current trajectory
1182480	1185480	that industry incentives especially optimize for,
1185480	1188480	it's not even really trying to take us to these things.
1188480	1191480	So think about, for example, a case study of visual intelligence
1191480	1194480	that is in some ways, as pattern recognition, very much of a success.
1194480	1196480	It's again been mostly driven by industry.
1196480	1199480	It's something that if you read in the news
1199480	1202480	or even play around with in certain publicly available data sets
1202480	1204480	feels like we've made great progress.
1204480	1206480	And this is an aspect of visual intelligence
1206480	1209480	which is sometimes called image captioning.
1209480	1212480	It's mapping images to text.
1212480	1214480	You know, basically there's been a bunch of systems.
1214480	1216480	Here's a couple of press releases.
1216480	1218480	I guess this one's about Google.
1218480	1221480	Google's AI can now capture images almost as well as humans.
1221480	1223480	Here's one's about Microsoft.
1223480	1226480	A couple of years ago, I think there were something like
1226480	1229480	eight papers all released on to archive around the same time
1229480	1232480	from basically all the major industry computer vision groups
1232480	1235480	as well as a couple of academic partners, okay,
1235480	1238480	which all driven by basically the same data set
1238480	1241480	produced by some Microsoft researchers and other collaborators,
1241480	1244480	trained a combination of deep convolutional neural networks,
1244480	1247480	you know, state-of-the-art visual pattern recognition
1247480	1250480	with recurrent neural networks which had recently been developed
1250480	1253480	for, you know, basically kinds of neural statistical language modeling,
1253480	1255480	glued them together and produced a system
1255480	1259480	which made very impressive results in a big training set
1259480	1262480	and a held out test set where the goal was to take an image
1262480	1265480	and write a sentence like a short sentence caption
1265480	1269480	that would seem like the kind of way a human would describe that image.
1269480	1272480	And these systems, you know, surpassed human level accuracy
1272480	1275480	on the held out test set from a big training set.
1275480	1277480	But what you can see when you really dig into these things
1277480	1280480	is there's often a lot of what I would call data set overfitting.
1280480	1282480	It's not overfitting to the training set,
1282480	1285480	but it's overfitting to whatever are the particular characteristics
1285480	1288480	of this data set, you know, wherever it came from,
1288480	1291480	certain set of photographs and certain ways of captioning them, okay,
1291480	1294480	which even a big data set, it's not about quantity,
1294480	1298480	it's more about the quality, the nature of what people are doing, all right.
1298480	1301480	So one way to test this system is to apply it
1301480	1304480	to what seems like basically the same problem
1304480	1308480	but not within a certain curated or built data set.
1308480	1311480	And there's a convenient Twitter bot that lets you do this.
1311480	1313480	So there's something called the PIC desk bot,
1313480	1317480	which takes one of the state of the art industry AI captioning systems,
1317480	1319480	a very good one. Again, this is not meant to,
1319480	1322480	I'm not trying to critique these systems for what they're trying to do,
1322480	1324480	I'm just trying to point out what they don't really even try to do.
1324480	1327480	So this takes the Microsoft caption bot
1327480	1330480	and just every couple of hours takes a random image from the web,
1330480	1333480	captions it and uploads the results to Twitter.
1333480	1336480	And a couple of months ago when I prepared the first version of this talk,
1336480	1339480	I just took a few days in the life of this Twitter bot.
1339480	1342480	I didn't take every single image, but I took, you know,
1342480	1345480	most of the images in a way that was meant to be representative of the successes
1345480	1348480	and the kinds of failures that such a system will make.
1348480	1350480	So we can go through this and it's a little bit entertaining
1350480	1352480	and I think quite informative.
1352480	1357480	So here's just a somewhat random sample of a few days in the life
1357480	1359480	of one of these caption bots.
1359480	1362480	So here we have a picture of a person holding,
1362480	1364480	fortunately my screen is very small here and I can't read up there,
1364480	1366480	so maybe you'll have to tell me what it is.
1366480	1368480	But a person holding a cell phone, I guess I'll just read along with you.
1368480	1370480	So you have a person holding a cell phone.
1370480	1372480	Well, it's not a person holding a cell phone, but it's kind of close.
1372480	1374480	It's a person holding some kind of machine.
1374480	1378480	I don't even know what that is, but it's some kind of musical instrument, right?
1378480	1381480	So that's a mixed success or failure.
1381480	1385480	Here's a pretty good one, a group of people on a field playing football.
1385480	1389480	I would call that an A result, maybe even A plus.
1389480	1392480	Here's a group of people standing on top of a mountain.
1392480	1395480	So less good, there's a mountain, but as far as I can tell there's no people.
1395480	1398480	But these systems like to see people because of both the combination,
1398480	1400480	because in the data set they were trained on there's a lot of people
1400480	1402480	and people often talk about people.
1402480	1407480	And the fact that you can appreciate both what I said and why it's funny,
1407480	1412480	there you did some of my cognitive activities that this system is not even trying to do.
1412480	1414480	Here we've got a building with a cake.
1414480	1415480	I'll go through these fast.
1415480	1418480	Building with a cake, a large stone building with a clock tower.
1418480	1419480	I think that's pretty good.
1419480	1420480	I'd give that like a B plus.
1420480	1422480	There's no clock, but it's plausibly right.
1422480	1424480	There might be a clock in there.
1424480	1425480	There's definitely something like that.
1425480	1427480	Here's a truck parked on the side of a building.
1427480	1429480	I don't know, maybe a B minus.
1429480	1432480	There is a car on the side of a building, but it's not a truck
1432480	1435480	and it doesn't seem like the main thing in the image.
1435480	1437480	Here's a necklace made of bananas.
1437480	1441480	Here's a large ship in the water.
1441480	1442480	This is pretty good.
1442480	1445480	I give this like an A minus or B plus because there is a ship in the water,
1445480	1446480	but it's not very large.
1446480	1448480	It's really more of like a tugboat or something.
1448480	1450480	Here's a sign sitting on the grass.
1450480	1451480	You know, in some sense, that's great.
1451480	1454480	No, but in another sense, it's really missing what's actually
1454480	1457480	interesting and important and meaningful to humans.
1457480	1462480	Here's a garden is in the dirt.
1462480	1464480	A pizza sitting on top of a building.
1464480	1466480	A small house with a red brick building.
1466480	1468480	That's pretty good, although a kind of weird way of saying it.
1468480	1470480	A vintage photo of a pond.
1470480	1471480	That's good.
1471480	1472480	They like vintage photos.
1472480	1474480	A group of people that are standing in the grass near a bridge.
1474480	1476480	Again, there's two people and there's some grass and there's a bridge,
1476480	1479480	but it's really not what's going on.
1479480	1482480	A person in a yard, kind of.
1482480	1484480	A group of people standing on top of a boat.
1484480	1485480	There's a boat.
1485480	1486480	There's a group of people.
1486480	1487480	They're standing.
1487480	1491480	But again, the sentence that you see is more based on a bias of what people
1491480	1495480	have said in the past about images that are only vaguely like this.
1495480	1496480	A clock tower is a little bit night.
1496480	1498480	That's really, I think, pretty impressive.
1498480	1500480	A large clock mounted to the side of a building.
1500480	1501480	A little bit less so.
1501480	1503480	A snow-covered feel, very good.
1503480	1505480	A building with snow on the ground.
1505480	1506480	A little bit less good.
1506480	1507480	There's no snow.
1507480	1508480	It's white.
1508480	1511480	Some people who I don't know them, but I bet that's probably right
1511480	1514480	because identifying faces and recognizing people who are famous
1514480	1516480	because they won medals in the Olympics,
1516480	1520480	probably I would trust current pattern recognition systems to get that.
1520480	1522480	A painting of a vase in front of a mirror.
1522480	1523480	Less good.
1523480	1526480	Also a famous person there, but we didn't get him.
1526480	1528480	A person walking in the rain.
1528480	1531480	Again, there is sort of a person and there's some puddles.
1531480	1535480	A group of stuffed animals.
1535480	1537480	A car parked in a parking lot.
1537480	1538480	That's good.
1538480	1540480	A car parked in front of a building.
1540480	1541480	Less good.
1541480	1543480	A plate with a fork and knife.
1543480	1544480	A clear blue sky.
1544480	1545480	Okay.
1545480	1546480	So you get the idea.
1546480	1549480	Again, if you actually go and play with this system,
1549480	1553480	partly because I think my friends at Microsoft told me they've improved it some.
1553480	1556480	This is partly for entertainment values.
1556480	1558480	I chose what also would be the funnier examples.
1558480	1560480	So I want to be quite honest about this.
1560480	1564480	I'm not trying to take away what are impressive AI technologies,
1564480	1569480	but I think it's clear that there's a sense of understanding any one of these images,
1569480	1572480	that it's important to see that even when it seems to be correct,
1572480	1575480	if it can make the kind of errors that it makes,
1575480	1577480	that even when it seems to be correct,
1577480	1579480	it's probably not doing what you're doing
1579480	1582480	and it's probably not even trying to scale towards the dimensions of intelligence
1582480	1585480	that we think about when we're talking about human intelligence.
1585480	1587480	Another way to put this,
1587480	1590480	I want to show you a really insightful blog post from one of your other speakers.
1590480	1592480	So in a couple of days,
1592480	1594480	I'm not sure you're going to have Andre Carpathi,
1594480	1597480	who's one of the leading people in deep learning.
1597480	1601480	This is a really great blog post he wrote a couple of years ago,
1601480	1603480	when he was, I think, still at Stanford.
1603480	1605480	He got his PhD from Stanford.
1605480	1610480	He worked at Google a little bit on some early big neural net AI projects there.
1610480	1612480	He was at OpenAI.
1612480	1614480	He was one of the founders of OpenAI,
1614480	1618480	and recently he joined Tesla as their director of AI research.
1618480	1620480	But about five years ago,
1620480	1624480	he was looking at the state of computer vision from a human intelligence point of view
1624480	1626480	and lamenting how far away we were.
1626480	1628480	So this is the title of his blog post,
1628480	1630480	the state of computer vision and AI.
1630480	1632480	We are really, really far away.
1632480	1636480	And he took this image, which was a sort of a famous image in its own right.
1636480	1639480	It was a popular image of Obama back when he was president,
1639480	1642480	kind of playing around as he liked to do when he was on tour.
1642480	1644480	If you take a look at this, you can see,
1644480	1647480	you probably all can recognize the previous president of the United States,
1647480	1650480	but you can also get the sense of where he is and what's going on,
1650480	1652480	and you might see people smiling,
1652480	1654480	and you might get the sense that he's playing a joke on someone.
1654480	1656480	Can you see that?
1656480	1659480	So how do you know that he's playing a joke and what that joke is?
1659480	1662480	Well, as Andre goes on to talk about in his blog post,
1662480	1667480	if you think about all the things that you have to really deploy in your mind to understand that,
1667480	1669480	it's a huge list.
1669480	1673480	Of course, it starts with seeing people and objects and maybe doing some face recognition,
1673480	1675480	but you have to do things like, for example,
1675480	1679480	notice his foot on the scale and understand enough about how scales work,
1679480	1681480	that when a foot presses down, it exerts force,
1681480	1684480	that the scale is sensitive, doesn't just magically measure people's weight,
1684480	1686480	but it does that somehow through force.
1686480	1689480	You have to see who can see that he's doing that and who can't,
1689480	1691480	who cannot see that he's doing that, right,
1691480	1693480	in particular the person on the scale,
1693480	1695480	and why some people can see that he's doing that
1695480	1697480	and can see that some other people can't see it,
1697480	1699480	and that makes it funny to them.
1699480	1703480	And someday, we should have machines that can understand this,
1703480	1709480	but hopefully you can see why the kind of architecture that I'm talking about
1709480	1714480	would be the building blocks of the ingredients to be able to get them to do that.
1714480	1717480	Now, again, I prepared a version of this talk a few months ago
1717480	1720480	and I wrote to Andre and I said, I was going to use this
1720480	1724480	and I was curious if he had any reflections on this
1724480	1727480	and where he thought we were relative to five years ago,
1727480	1729480	because certainly a lot of progress has been made.
1729480	1733480	But he said, here's his email, I hope he doesn't mind me sharing it,
1733480	1735480	but I mean, again, he's a very honest person
1735480	1738480	and that's one of the many reasons why he's such an important person right now in AI.
1738480	1741480	He's both very technically strong and honest about what we can do what we can't do,
1741480	1743480	and as he says, what does he say?
1743480	1746480	It's nice to hear from you, it's fun you should bring this up.
1746480	1749480	I was also thinking about writing a return to this
1749480	1752480	and in short, basically I don't believe we've made very much progress, right?
1752480	1756480	He points out that in his long list of things that you need to understand the image,
1756480	1758480	we have made progress on some, the ability to, again,
1758480	1762480	detect people and do face recognition for well-known individuals, okay?
1762480	1764480	But that's kind of about it, all right?
1764480	1768480	And he wasn't particularly optimistic that the current route that's being pursued in industry
1768480	1774480	is anywhere close to solving or even really trying to solve these larger questions.
1774480	1778480	If we give this image to that caption bot,
1778480	1781480	what we see is, again, represents the same point.
1781480	1783480	Here's the caption bot, it says,
1783480	1786480	I think it's a group of people standing next to a man in a suit and tie, right?
1786480	1788480	So that's right, right?
1788480	1791480	As far as it goes, it just doesn't go far enough,
1791480	1794480	and the current ideas of build a data set,
1794480	1798480	train a deep learning algorithm on it, and then repeat,
1798480	1803480	aren't really even, I would venture, trying to get to what we're talking about.
1803480	1806480	Or here's another, I'll just give you one other example of a couple of photographs
1806480	1810480	from my recent vacation in a nice warm tropical locale,
1810480	1814480	which I think illustrate ways in which, again, the gap where we have machines
1814480	1818480	that can, say, beat the world's best at go,
1818480	1821480	but can't even beat a child at tic-tac-toe.
1821480	1823480	Now, what do I mean by that?
1823480	1826480	Well, of course, we don't even need reinforcement learning or deep learning
1826480	1830480	to build a machine that can win or tie, do optimally in tic-tac-toe.
1830480	1833480	But think about this, this is a real tic-tac-toe game,
1833480	1836480	which I saw on the grass outside my hotel, right?
1836480	1839480	What do you have to do to look at this and recognize that it's a tic-tac-toe game?
1839480	1842480	You have to see the objects, you have to see what's, you know, in some sense,
1842480	1846480	there's a three-by-three grid, but it's only abstract, right?
1846480	1850480	It's only delimited by these ropes or strings, okay?
1850480	1854480	It's not actually a grid in any simple geometric sense, all right?
1854480	1858480	But yet a child can look at that, and indeed, here's an actual child who was looking at it
1858480	1861480	and recognize, oh, it's a game of tic-tac-toe, and even know what they need to do to win,
1861480	1864480	namely put the X and complete it, and now they've got three in a row, right?
1864480	1867480	That's literally child's play, okay?
1867480	1871480	You show this sort of thing, though, to one of these, you know, image-understanding caption bots,
1871480	1874480	and I think it's a close-up of a sign, okay?
1874480	1882480	Again, saying that this is a close-up of a sign is not the same thing, I would venture,
1882480	1887480	as a cognitive or computational activity that's going to give us what we need to say,
1887480	1890480	recognize the objects, recognize it as a game to understand the goal
1890480	1892480	and how to plan to achieve those goals.
1892480	1896480	Whereas this kind of architecture is designed to try to do all of these things, ultimately, right?
1896480	1904480	And I bring in these examples of games or jokes to really show where perception goes to cognition,
1904480	1907480	you know, all the way up to symbols, right?
1907480	1912480	So to get objects and forces and mental states, that's the cognitive core,
1912480	1918480	but to be able to get goals and plans and what do I do or how do I talk about it, that symbols, okay?
1918480	1924480	Here's another way into this, and it's one that also motivates, I think, a lot of really good work on the engineering side
1924480	1928480	and a lot of our interest in the science side is think about robotics
1928480	1934480	and think about what do you have to do to, you know, what does the brain have to be like to control the body?
1934480	1938480	So again, you're going to hear from, shortly, I think maybe it's next week, from Mark Rayburt,
1938480	1944480	who's one of the founders of Boston Dynamics, which is one of my favorite companies anywhere.
1944480	1951480	They're, without doubt, the leading maker of humanoid robots, legged, locomoting robots in industry.
1951480	1957480	They have all sorts of other really cool robots, robots like dogs, robots that have all, you know,
1957480	1960480	I think you'll even get to see a live demonstration of one of these robots.
1960480	1963480	It's really awesome, impressive stuff, okay?
1963480	1966480	But what about the minds and brains of these robots?
1966480	1971480	Well, again, if you ask Mark, ask them how much of human-like cognition do they have in their robots,
1971480	1974480	and I think he would say very little.
1974480	1976480	In fact, we have asked him that, and he would say very little.
1976480	1978480	He has said very little.
1978480	1983480	He's actually one of the advisors of our center, and I think in many ways we're very much on the same page.
1983480	1990480	We both want to know how do you build the kind of intelligence that can control these bodies like the way a human does?
1990480	1993480	All right, here's another example of an industry robotics effort.
1993480	1996480	This is Google's arm farm, where, you know, they've got lots of robot arms,
1996480	2001480	and they're trying to train them to pick up objects using various kinds of deep learning and reinforcement learning techniques.
2001480	2003480	And I think it's one approach.
2003480	2009480	I just think it's very, very different from the way humans learn to, say, control their body and manipulate objects.
2009480	2013480	And you can see that in terms of things that go back to what you were saying when you were introducing me, right?
2013480	2015480	Think about how quickly we learn things, right?
2015480	2020480	Here you have these, the arm farm is trying to generate, you know, effectively, maybe if not infinite,
2020480	2026480	but hundreds of thousands, millions of examples of reaches and pickups of objects, even with just a single gripper.
2026480	2033480	And yet a child who in some ways can't control their body nearly as well as robots can be controlled at the low level
2033480	2035480	is able to do so much more.
2035480	2041480	So I'll show you two of my favorite videos from YouTube here, which motivate some of the research that we're doing.
2041480	2044480	The one on the left is a one-and-a-half-year-old and the other one's a one-year-old.
2044480	2049480	So just watch this one-and-a-half-year-old here doing a popular activity for many kids.
2049480	2051480	Is it playing?
2051480	2053480	Hmm.
2055480	2057480	You see video up there?
2058480	2059480	Okay, there we go.
2059480	2063480	Okay, so he's doing this stacking cup activity.
2063480	2067480	All right, he's stacking up cups to make a tall tower.
2067480	2068480	He's got a stack of three.
2068480	2073480	And what you can see for the first part of this video is it looks like he's trying to make a second stack
2073480	2075480	that he's trying to pick up at once.
2075480	2079480	Basically, he's trying to make a stack of two that'll go on the stack of three.
2079480	2084480	And, you know, he's trying to debug his plan because it got a little bit stuck here.
2084480	2089480	But, and think about, I mean, again, if you know anything about robots manipulating objects,
2089480	2093480	even just what he just did, no robot can decide to do that and actually do it, right?
2093480	2095480	At some point, he's almost got it.
2095480	2099480	It's a little bit tricky, but at some point he's going to get that stack of two.
2099480	2102480	He realizes he has to move that object out of the way.
2102480	2103480	Look at what he just did.
2103480	2105480	Move it out of the way, use two hands to pick it up.
2105480	2109480	And now he's got a stack of two on a stack of three, and suddenly, you know, sub-goal completed.
2109480	2110480	He's now got a stack of five.
2110480	2116480	And he gives himself a hand because he knows he accomplished a key waypoint along the way to his final goal.
2116480	2119480	That's a kind of early symbolic cognition, right?
2119480	2124480	To understand that I'm trying to build a tall tower, but a tower is made up of little towers.
2124480	2128480	And you can take a tower and put it on top of another tower or stack a stack on a stack,
2128480	2130480	and you have a bigger stack, right?
2130480	2134480	So think about how he goes from bottom-up perception to the objects that the physics needed
2134480	2139480	to manipulate the objects to the ability to make even those early kinds of symbolic plans.
2139480	2140480	At some point, he keeps doing this.
2140480	2143480	He puts another stack on there.
2143480	2144480	I'll just jump to the end.
2144480	2145480	Oops, sorry.
2145480	2146480	You missed.
2146480	2147480	Sorry.
2147480	2151480	He gets really excited and he gives himself another big hand, but falls over.
2151480	2152480	Okay.
2152480	2157480	Again, Boston Dynamics now has robots that could pick themselves up after that.
2157480	2159480	That's really impressive, again.
2159480	2163480	But all the other stuff to get to that point, we don't really know how to do in a robotic setting.
2163480	2164480	Or think about this baby here.
2164480	2165480	This is a younger baby.
2165480	2172480	This is one of the Internet's very most popular videos because it features a baby and a cat.
2172480	2174480	But the baby's doing something interesting.
2174480	2179480	He's got the same cups, but he's decided, again, decided to try a new thing.
2179480	2180480	So think about creativity.
2180480	2185480	He's decided that his goal is to stack up cups on the back of a cat, I guess.
2185480	2187480	He's asking, how many cups can I fit on the back of a cat?
2187480	2188480	Well, three.
2188480	2191480	Let's see, can I fit more?
2191480	2193480	Let's try another one.
2193480	2194480	Okay.
2194480	2195480	Well, he can't fit more than three.
2195480	2196480	It turns out.
2196480	2197480	And then he, then it's not working.
2197480	2199480	So he changes his goal.
2199480	2202480	Now his goal appears to be to get the cups on the other side of the cat.
2202480	2205480	Now watch that part when he reaches back behind him there.
2205480	2207480	I'll just pause it there for a moment.
2207480	2211480	So when he just reached back there, that's a particularly striking moment in the video.
2211480	2216480	It shows a very strong form of what we call in cognitive science, object permanence.
2216480	2217480	Okay.
2217480	2221480	That's the idea that you represent objects as these permanent, enduring entities in the
2221480	2223480	world, even when you can't see them.
2223480	2227480	In this case, he hadn't seen or touched that object behind him for like at least a minute,
2227480	2228480	right?
2228480	2229480	Maybe much longer, I don't know.
2229480	2232480	And yet he still knew it was there and he was able to incorporate it in his plan, right?
2232480	2235480	There's a moment before that when he's about to reach for it, but then he sees this other
2235480	2236480	one, right?
2236480	2239480	And it's only when he's now exhausted all the other objects here that he can see.
2239480	2242480	He's like, okay, now time to get this object and bring it into play, right?
2242480	2247480	So think about what has to be going on in his brain for him to be able to do that, right?
2247480	2251480	That's like the analog of you understanding what's behind you, okay?
2251480	2254480	It's not that these things are impossible to capture machines far from it.
2254480	2257480	It's just that like training a deep neural network or any kind of pattern recognition
2257480	2259480	system we don't think is going to do it.
2259480	2263480	But we think by reverse engineering how it works in the brain, we might be able to do
2263480	2264480	it.
2264480	2265480	I think we can do it, okay?
2265480	2267480	It's not just humans that do this kind of activity.
2267480	2269480	Here's a couple of, again, rather famous videos.
2269480	2271480	You can watch all of these on YouTube.
2271480	2278480	Crows are famous object manipulators and tool users, but also orangutans, other primates,
2278480	2279480	rodents.
2279480	2282480	We can watch, if we just, here, let me pause this one for a second.
2282480	2285480	If we watch this orangutan here, he's got a bunch of big Legos.
2285480	2290480	And over the course of this video, he's building up a stack of Legos.
2290480	2293480	It's really quite impressive.
2293480	2296480	He's just jumping to the end.
2296480	2301480	There's actually some controversy out there of whether this video is a fake.
2301480	2305480	But the controversy isn't about, you know, it's not like whether it was, I don't know,
2305480	2307480	done with computer animation.
2307480	2311480	Some people think the video was actually filmed backwards, that a human built up the stack
2311480	2314480	and the orangutan just slowly disassembled it piece by piece.
2314480	2317480	And it turns out it's remarkably hard to tell whether it's played forward or backwards in
2317480	2318480	time.
2318480	2320480	And people have argued over little details because, you know, it would be quite impressive
2320480	2325480	if an orangutan actually was able to build up this really impressive stack of Legos.
2325480	2328480	And I would submit that it would be almost as impressive if he disassembled it.
2328480	2330480	Think about the activity.
2330480	2333480	I mean, if I wanted to disassemble that, the easiest thing to do would just be to knock it over.
2333480	2335480	That's really all most robots could do.
2335480	2339480	But to piece by piece disassemble it, even if it's played backwards like this,
2339480	2343480	that's still a really impressive act of symbolic planning on physical objects.
2343480	2348480	Or here you've got this famous mouse, this you can find on the internet under the
2348480	2350480	Mouse vs. Cracker video.
2350480	2355480	And what you'll see here over the course of this video is a mouse valiantly and mostly
2355480	2360480	hopelessly struggling with a cracker that they're hoping to bring back to their nest.
2360480	2363480	I guess it's a very appealing big meal.
2363480	2368480	And at some point after just trying to get it over the wall, at some point the mouse
2368480	2370480	just gives up because it's just never going to happen.
2370480	2371480	And he just goes away.
2371480	2377480	Except that because even mouses can dream or mice can dream, at some point he decides,
2377480	2379480	he's not going to come back for one more try.
2379480	2382480	And he tries one more time and this time valiantly gets it over.
2382480	2384480	Isn't that very impressive?
2384480	2385480	Congratulations.
2385480	2386480	You don't have to clap.
2386480	2389480	You can clap for me at the end or clap for whoever later.
2389480	2392480	But I want to applaud the mouse there every time I see that.
2392480	2396480	But again, think what had to be going on in his brain to be able to do that.
2396480	2398480	It's a crazy thing.
2398480	2401480	And yet he formulated the goal and was able to achieve it.
2401480	2404480	I'll just show one more video that is really more about science.
2404480	2407480	These other ones are, you know, some of them actually were from scientific experiments.
2407480	2410480	But this is one that motivates a lot of the science that I do.
2410480	2415480	And to me it sets up kind of a grand cognitive science challenge for AI and robotics.
2415480	2420480	It's from an experiment with humans, again 18-month-olds or one and a half-year-olds.
2420480	2423480	So the kids in this experiment were the same age as the first baby I showed you,
2423480	2424480	the one who did the stacking.
2424480	2429480	And 18 months is really a very, very good age to study if you're interested in intelligence
2429480	2432480	for reasons we can talk about later if you're interested.
2432480	2435480	This is from a very famous experiment done by two psychologists,
2435480	2438480	Felix Wernicke and Michael Tomasello.
2438480	2442480	And it was studying the spontaneous helping behavior of young children.
2442480	2444480	It also contrasted humans and chimps.
2444480	2448480	And the punchline is that chimps sometimes do things that are kind of like what this human did,
2448480	2451480	but not nearly as reliably or as flexibly.
2451480	2456480	Okay, so not nearly in his, and I'll show you a particular kind of unusual situation
2456480	2460480	where human kids had relatively little trouble figuring out kind of what to do
2460480	2462480	or even whether they should do it.
2462480	2466480	Whereas basically no chimp did what you're going to see humans sometimes doing here.
2466480	2471480	So the experimenter in this movie, and I'll turn on the sound here if you can hear it.
2471480	2477480	The experimenter is the tall guy and the participant is the little kid in the corner.
2477480	2481480	There's sound but no words, right?
2481480	2485480	And at some point he stops and then the kid just does whatever they want to do.
2485480	2486480	So watch what he does.
2486480	2491480	He goes over, he opens the cabinet, looks inside, then he steps back
2491480	2496480	and he looks up at Felix and then looks down, okay, and then the action is completed.
2496480	2501480	Now, I want you to watch it one more time and think about what's got to be going inside the kid's head
2501480	2505480	to understand this, to understand, like, so it seems like what it looks like to us
2505480	2508480	is the kid figured out that this guy needed help and helped him.
2508480	2510480	And the paper is full of many other situations like this.
2510480	2512480	This is just one, okay?
2512480	2514480	But the key idea is that the situation is somewhat novel.
2514480	2517480	People have seen people holding books and opening cabinets,
2517480	2521480	but probably it's very rare to see this kind of situation exactly, right?
2521480	2524480	It's different in some important details from what you might have seen before.
2524480	2526480	And there's other ones in there that are really truly novel
2526480	2529480	because they just made up a machine right there, okay?
2529480	2534480	But somehow he has to understand causally from the way the guy is banging the books against the thing.
2534480	2538480	It's sort of both a symbol, but it's also somehow he's got to understand
2538480	2542480	what he can do and what he can't do and then what the kid can do to help.
2542480	2545480	And I'll show this again, but really just watch,
2545480	2552480	the main part I want you to see is, I'll just sort of skip ahead.
2552480	2555480	So watch this part here.
2555480	2559480	Let's say I'll just jump right when he, watch, right now he's about to look up.
2559480	2563480	He looks up and makes eye contact and then his eyes look down.
2563480	2568480	So again, he looks up, he looks up and then a saccade,
2568480	2572480	a sudden rapid eye movement down, down to his hands, up, down, okay?
2572480	2575480	So that's, again, that's this brain OS in action, right?
2575480	2581480	He's making one glance, small glance, at the big guy's eyes to make eye contact,
2581480	2585480	to see, to get a signal, did I understand what you wanted
2585480	2588480	and did you register that joint attention?
2588480	2591480	And then he makes a prediction about what the guy's going to do,
2591480	2593480	so he looks right down, he doesn't just like look around randomly,
2593480	2598480	he looks right down to the guy's hands to track the action that he expects to see happening
2598480	2602480	if I did the right thing to help you, then I expect you're going to put the books there, okay?
2602480	2606480	So you can see these things happening and we want to know what's going on inside the mind
2606480	2608480	that guides all of that, all right?
2608480	2612480	So that's this sort of big scientific agenda that we're working on over the next few years
2612480	2618480	where we think some kind of human, understanding of human intelligence and scientific terms
2618480	2620480	could lead to all sorts of AI payoffs.
2620480	2623480	In particular, suppose we could build a robot that could do what this kid
2623480	2627480	and many other kids in these experiments do to say help you out around the house
2627480	2631480	without having to be programmed or even really instructed just to kind of get a sense,
2631480	2634480	oh yeah, you need to hand with that, sure, let me help you out, okay?
2634480	2638480	Even 18-month-olds will do that, sometimes not very reliably or effectively,
2638480	2641480	sometimes they'll try to help and really do the opposite, right?
2641480	2647480	But imagine if you could take the flexible understanding of humans, actions, goals and so on
2647480	2651480	and make those reliable engineering technology, that would be very useful.
2651480	2655480	And it would also be related to, say, machines that you could actually start to talk to
2655480	2658480	and trust in some ways, right, that shared understanding.
2658480	2660480	So how are we going to do this?
2660480	2663480	Well, let me spend the rest of the time talking about how we try to do this, right?
2663480	2668480	Some of the technology that we're building both in our group and more broadly
2668480	2671480	to try to make these kinds of architectures real.
2671480	2676480	And I'll talk about two or three technical ideas, again, not in any detail, all right?
2676480	2679480	One is the idea of a probabilistic program.
2679480	2685480	So this is a kind of, think of it as a computational abstraction
2685480	2689480	that we can use to capture the common sense knowledge of this core cognition.
2689480	2693480	So when I say we have an intuitive understanding of physical objects and people's goals,
2693480	2696480	how do I build a model of that model you have in the head?
2696480	2699480	Probabilistic programs, a little bit more technically,
2699480	2703480	one way to understand them is as a generalization of Bayesian networks
2703480	2707480	or other kinds of directed graphical models, if you know those, okay?
2707480	2711480	But where instead of defining a probability model on a graph,
2711480	2713480	you define it on a program,
2713480	2719480	and thereby have access to a much more expressive toolkit of knowledge representation.
2719480	2724480	So data structures, other kinds of algorithmic tools for representing knowledge, okay?
2724480	2727480	But you still have access to the ability to do probabilistic inference,
2727480	2732480	like in a graphical model, but also causal inference in a directed graphical model.
2732480	2735480	So for those of you who know about graphical models, that might make some sense to you.
2735480	2738480	But just more broadly, what this is, think of this as a toolkit
2738480	2741480	that allows us to combine several of the best ideas,
2741480	2743480	not just of the recent deep learning era,
2743480	2747480	but if you look back over the whole scope of AI and as well as cognitive science,
2747480	2751480	I think there's three or four ideas and more,
2751480	2753480	but definitely like three ideas we can really put up there
2753480	2757480	that have proven their worth and have risen and fallen in terms of,
2757480	2760480	each of these had ideas when the mainstream of the field thought,
2760480	2764480	this was totally the way to go and every other idea was obviously a waste of time,
2764480	2768480	and also had its time when many people thought it was a waste of time, okay?
2768480	2773480	And these three big ideas, I would say, are first of all the idea of symbolic representation,
2773480	2776480	or symbolic languages for knowledge representation,
2776480	2780480	probabilistic inference in generative models to capture uncertainty, ambiguity,
2780480	2785480	learning from sparse data, and in their hierarchical setting, learning to learn, right?
2785480	2792480	And then, of course, the recent developments with neural-inspired architectures for pattern recognition, okay?
2792480	2796480	Each of these things, each of these ideas, symbolic languages, probabilistic inference,
2796480	2801480	and neural networks have some distinctive strengths that are real weak points of the other approaches, right?
2801480	2804480	So to take one example that I haven't really talked about here,
2804480	2809480	people in the, but you mentioned as an outstanding challenge for neural networks,
2809480	2814480	transfer learning, or learning to take knowledge across a number of previous tasks to transfer to others,
2814480	2818480	is a real challenge and has always been a challenge in a neural net, okay?
2818480	2823480	But it's something that's addressed very naturally and very scalably in, for example, a hierarchical Bayesian model.
2823480	2827480	And if you look at some of the recent attempts, really interesting attempts within the deep learning world
2827480	2831480	to try to get kinds of transfer learning and learning to learn, they're really cool, okay?
2831480	2836480	But many of them are in some ways kind of reinventing within a neural network paradigm
2836480	2841480	ideas that people, you know, maybe just 10 or 15 years ago developed in very sophisticated ways
2841480	2844480	in, let's say, hierarchical Bayesian models, okay?
2844480	2849480	And a lot of attempts to get sort of symbolic algorithm-like behavior in neural networks, again,
2849480	2854480	are really, you know, they're very small steps towards something which is a very mature technology
2854480	2857480	in computer systems and programming languages.
2857480	2861480	Probabilistic programs, I'll just sort of advertise mostly,
2861480	2864480	are a way to combine the strengths of all of these approaches,
2864480	2869480	to have knowledge representations which are as expressive as anything that anybody ever did in the symbolic paradigm,
2869480	2874480	that are as flexible at dealing with uncertainty and sparse data as anything in the probabilistic paradigm,
2874480	2879480	but that also can support pattern recognition tools to be able to, for example,
2879480	2883480	do very fast efficient inference in very complex scenarios.
2883480	2886480	And there's a number of, that's the kind of conceptual framework.
2886480	2889480	There's a number of actually implemented tools.
2889480	2894480	I point to here on the slide a number of probabilistic programming languages which you can go explore.
2894480	2897480	For example, there's one that was developed in our group a few years ago,
2897480	2901480	almost 10 years ago, now called Church, which was the antecedent of some of these other languages
2901480	2903480	built on a functional programming core.
2903480	2906480	So Church is a probabilistic programming language built on the Lambda Calculus,
2906480	2908480	or really in LISP, basically.
2908480	2914480	But there are many other more modern tools, especially if you are interested in neural networks.
2914480	2918480	There are tools like, for example, Pyro, or ProbTorch, or BayesFlow,
2918480	2922480	that try to combine all these ideas in a, or for example, Gen here,
2922480	2926480	which is a project of Vakash Mansingh's probabilistic computing group.
2926480	2931480	These are all things which are just in the very beginning stages, very, very alpha.
2931480	2935480	But you can find out more about them online or by writing to their creators.
2935480	2942480	And I think this is a very exciting place where the convergence of a number of different AI tools are happening.
2942480	2948480	And this will be absolutely necessary for making the kind of architecture that I'm talking about work.
2948480	2951480	Another key idea, which we've been building on in our lab,
2951480	2955480	and I think, again, many people are using some version of this idea,
2955480	2958480	but maybe a little bit different from the way we're doing it,
2958480	2963480	is what the version of this idea that I like to talk about is what I call the game engine in the head.
2963480	2967480	So this is the idea that it's really what the programs are about.
2967480	2971480	When I talk about probabilistic programs, I haven't said anything about what kind of programs we're using.
2971480	2975480	We're just basically, these probabilistic programming languages at their best,
2975480	2979480	and church, the language that was developed by Noah Goodman and Vakash and others,
2979480	2981480	and Dan Roy and our group some 10 years ago,
2981480	2985480	was intended to be a turing complete probabilistic programming language.
2985480	2991480	So any probability model that was computable, or for whose conditional inferences are computable,
2991480	2993480	you could represent in these languages.
2993480	2999480	But that leaves completely open what kind of program I'm going to write to model the world.
2999480	3006480	And I've been very inspired in the last few years by thinking about the kinds of programs that are in modern video game engines.
3006480	3009480	So again, probably most of you are familiar with these,
3009480	3012480	and increasingly they're playing a role in all sorts of ways in AI,
3012480	3018480	but these are tools that were developed by the video game industry to allow a game designer to make a new game
3018480	3023480	without having to do most of, in some sense, many, most of the hard technical work from scratch,
3023480	3027480	but rather to focus on the characters, the world, the story,
3027480	3031480	the things that are more interesting for designing a novel game.
3031480	3036480	In particular, if we want a player to explore some new three-dimensional world,
3036480	3039480	but to have them be able to interact with the world in real time
3039480	3046480	and to render nice-looking graphics in real time in an interactive way as the player moves around and explores the world,
3046480	3051480	or if you want to populate the world with non-player characters that will behave in an even vaguely intelligent way.
3051480	3056480	Game engines give you tools for doing all of this without having to write all of graphics from scratch
3056480	3060480	or all of physics, the rules of physics from scratch,
3060480	3065480	so what are called game physics engines, and in some sense are a set of principles,
3065480	3068480	but also hacks from Newtonian mechanics and other areas of physics
3068480	3073480	that allow you to simulate plausible-looking physical interactions in very complex world,
3073480	3076480	very approximately, but very fast.
3076480	3080480	There's also what's called game AI, which are basically very simple planning models.
3080480	3086480	So let's say I want to have an AI in the game that is like a guard that guards a base and a player is going to attack the space.
3086480	3089480	So back in the old Atari days, like when I was a kid,
3089480	3094480	the guards would just be random things that would fire missiles kind of randomly in random directions at random times,
3094480	3097480	but let's say you want a guard to be a little intelligent,
3097480	3102480	so to actually look around and, oh, and I see the player, and then to actually start shooting at you and to even maybe pursue you.
3102480	3105480	So that requires putting a little AI in the game,
3105480	3109480	and you do that by having basically simple agent models in the game.
3109480	3114480	So what we think, and some of you might think this is crazy and some of you might think this is a very natural idea,
3114480	3116480	I get both kinds of reactions.
3116480	3121480	What we think is that these tools of fast-approximate renderers, physics engines,
3121480	3126480	and sort of very simple kinds of AI planning are an interesting first approximation
3126480	3131480	to the kinds of common-sense knowledge representations that evolution has built into our brains.
3131480	3136480	So when we talk about the cognitive core or how do babies start,
3136480	3139480	ways in which a baby's brain isn't a blank slate,
3139480	3144480	one interesting idea is that it starts with something like these tools
3144480	3147480	and then wrapped inside a framework for probabilistic inference,
3147480	3149480	that's what we mean by probabilistic programs,
3149480	3153480	that can support many activities of common-sense perception and thinking.
3153480	3158480	So I'll just give you one example of what we call this intuitive physics engine.
3158480	3165480	So this is work that we did in our groups that Pete Battaglia and Jess Hamrick started this work about five years ago now,
3165480	3168480	where we showed people, in some sense,
3168480	3172480	and this is also an illustration of a kind of experiment that you might do,
3172480	3175480	when I keep talking about science, I'll show you now a couple of experiments.
3175480	3179480	So we would show people simple physical scenes like these blocks world scenes
3179480	3181480	and ask them to make a number of judgments.
3181480	3185480	And the model we built does basically a little bit of probabilistic inference
3185480	3188480	in a game-style physics engine, it perceives the physical state
3188480	3193480	and imagines a few different possible ways the world could go over the next one or two seconds
3193480	3196480	to answer questions like, will the stack of blocks fall?
3196480	3198480	Or if they fall, how far will they fall?
3198480	3199480	Or which way will they fall?
3199480	3205480	Or what would happen if, say, one color of blocks or one material, like the green stuff,
3205480	3207480	is ten times heavier than the gray stuff?
3207480	3209480	Or vice versa, how will that change the direction of fall?
3209480	3214480	Or look at those red and yellow blocks, some of which look like they should be falling but aren't.
3214480	3215480	So why?
3215480	3221480	Can you infer from the fact that they're not falling that one color block is much heavier than the other?
3221480	3224480	Or let me show you a sort of a slightly weird task.
3224480	3226480	It's like other behavioral experiments.
3226480	3231480	Sometimes we do weird things so that we can test ways in which you use your knowledge
3231480	3234480	that you didn't just learn from pattern recognition,
3234480	3237480	but use it to do new kinds of tasks that you'd never seen before.
3237480	3241480	So here's a task which many of you have maybe seen me talk about these things.
3241480	3245480	So you might have seen this task, but probably only if you saw me give a talk around here before.
3245480	3249480	We call this the red-yellow task, and again, we'll make this one interactive.
3249480	3254480	So imagine that the blocks on the table are knocked hard enough to bump,
3254480	3257480	the tables bumped hard enough to knock some of the blocks onto the floor.
3257480	3260480	So you tell me, is it more likely to be red blocks or yellow blocks?
3260480	3261480	What do you say?
3261480	3262480	Red.
3262480	3263480	Okay, good.
3263480	3265480	How about here?
3265480	3266480	Yellow.
3266480	3267480	Yellow.
3267480	3268480	Good.
3268480	3269480	How about here?
3269480	3270480	Uh-huh.
3270480	3271480	Here?
3271480	3273480	Here?
3273480	3275480	Okay.
3275480	3278480	Here?
3278480	3279480	Here?
3279480	3280480	Here?
3280480	3281480	Okay.
3281480	3286480	So you just experience for yourself what it's like to be a subject in one of these experiments.
3286480	3287480	We just did the experiment here.
3287480	3290480	The data is all captured on video, sort of, right?
3290480	3291480	Okay.
3291480	3295480	You could see that sometimes people were very quick, other times people were slower.
3295480	3299480	Sometimes there was a lot of consensus, sometimes there was a little bit less consensus, right?
3299480	3301480	That reflects uncertainty.
3301480	3304480	So again, there's a long history of studying this scientifically.
3305480	3309480	That, you know, you could, but you can see some, you can see the probabilistic inference at work.
3309480	3311480	Probabilistic inference over what?
3311480	3318480	Well, I would say one way to describe it is over one or a few short low precision simulations of the physics of these scenes.
3318480	3320480	So here is what I mean by this.
3320480	3326480	I'm going to show you a video of a game engine reconstruction of one of these scenes that simulates a small bump.
3326480	3327480	So here's a small bump.
3327480	3329480	Here's the same scene with the big bump.
3329480	3330480	Okay.
3330480	3333480	Now notice that at the micro level, different things happen.
3333480	3338480	But at the cognitive or macro level that matters for common sense reasoning, the same thing happened.
3338480	3343480	Namely, all the yellow blocks went over onto one side of the table and few or none of the red blocks did.
3343480	3345480	So it didn't matter which of those simulations you ran in your head.
3345480	3347480	You'd get the same answer in this case, right?
3347480	3349480	This is one that's very easy and high confidence and quick.
3349480	3352480	Also, you didn't have to run the simulation for very long.
3352480	3356480	You only have to run it for a few time steps like that to see what's going to happen or similarly here.
3356480	3358480	You only have to run it for a few time steps, okay?
3358480	3360480	And it doesn't have to be even very accurate.
3360480	3366480	Even a fair amount of imprecision will give you basically the same answer at the level that matters for common sense.
3366480	3368480	So that's the kind of thing our model does.
3368480	3371480	It runs a few low precision simulations for a few time steps.
3371480	3375480	But if you take the average of what happens there and you compare that with people's judgments,
3375480	3377480	you get results like what I show you here.
3377480	3381480	This scatter plot shows on the y-axis the average judgments of people.
3381480	3383480	On the x-axis, the average judgments of this model.
3383480	3384480	And it does a pretty good job.
3384480	3392480	It's not perfect, but the model basically captures people's graded sense of what's going on in this scene and many of these others, okay?
3392480	3395480	And it doesn't do it with any learning, but I'll come back to that in a second.
3395480	3399480	It just does it by probabilistic reasoning over a game physics simulation.
3399480	3405480	Now we can use and we have used the same kind of technology to capture in very simple forms.
3405480	3407480	Really just proofs of concept at this point.
3407480	3412480	The kind of common sense physical scene understanding in a child playing with blocks or other objects.
3412480	3416480	Or in what might go on in a young child's understanding of other people's actions.
3416480	3418480	What we call the intuitive psychology engine.
3418480	3424480	Where now the probabilistic programs are defined over these kind of very simple planning and perception programs.
3424480	3425480	And I won't go into any details.
3425480	3429480	I'll just point to a couple of papers that my group played a very small role in.
3429480	3432480	But we provided some models, which together with some infant researchers,
3432480	3438480	people working on both of these are experiments that were done with 10 or 12 month infants.
3438480	3440480	So younger than even some of the babies I showed you before.
3440480	3443480	But basically like that youngest baby, the one with the cat.
3443480	3446480	Here's an example of showing simple physical scenes.
3446480	3453480	These are moving objects to 12 month olds where they saw a few objects bouncing around inside a gumball machine.
3453480	3456480	And after some point in time, the scene gets occluded.
3456480	3457480	You'll see the scene is occluded.
3457480	3461480	And then after another period of time, one of the objects will appear at the bottom.
3461480	3464480	And the question is, is that the object you expected to see or not?
3464480	3466480	Is it expected or surprising?
3466480	3470480	The standard way you study what infants know is by what's called looking time methods.
3470480	3471480	Just like an adult.
3471480	3474480	If I show you something that's surprising, you might look longer.
3474480	3477480	If you're bored, you'll look away.
3477480	3480480	So you can do that same kind of thing with infants.
3480480	3482480	And by measuring how long they look at a scene,
3482480	3486480	you can measure whether you've shown them something surprising or not.
3486480	3489480	There are literally hundreds of studies, if not more,
3489480	3492480	using looking time measures to study what infants know.
3492480	3496480	But only with this paper that we published a few years ago,
3496480	3499480	did we have a quantitative model where we were able to show a relation
3499480	3501480	between inverse probability in this case and surprise.
3501480	3506480	So things which were objectively lower probability under one of these probabilistic physics simulations
3506480	3509480	across a number of different manipulations of how fast the objects were,
3509480	3511480	where they were, when the scene was occluded,
3511480	3514480	how long the delay was, various physically relevant variables,
3514480	3516480	how many objects there were of one type or another.
3516480	3519480	Infants' expectations connected with this model.
3519480	3522480	Or another paper that we published, that one was done,
3522480	3526480	the experiments there were done by Erno Teglas and Luca Benotti's lab.
3526480	3531480	Here is a study that was done just recently by Sherry Liu in Liz Spelke's lab
3531480	3534480	there at Harvard, but they're partners with us in CBMM,
3534480	3536480	which was about infants' understanding of goals.
3536480	3539480	So this is more like, again, understanding of agents and intuitive psychology,
3539480	3542480	where, again, in very simple cartoon scenes,
3542480	3545480	you show an infant an agent that seems to be doing something,
3545480	3547480	like an animated cartoon character.
3547480	3551480	But it jumps over a wall, or it rolls up a hill, or it jumps over a gap.
3551480	3555480	And the question is, basically, how much does the agent want the goal
3555480	3557480	that it seems to be trying to achieve?
3557480	3561480	And what this study showed, and the models here were done by Tomer Omen,
3561480	3566480	was that infants appeared to be sensitive to the physical work done by the agent.
3566480	3571480	The more work the agent did, in the sense of the integral of force applied over a path,
3571480	3576480	the more the infants thought the agent wanted the goal.
3576480	3580480	We think of this as representing what we've sometimes called the naive utility calculus.
3580480	3584480	So the idea that there's a basic calculus of cost and benefit,
3584480	3588480	you know, we take actions which are a little bit costly to achieve goal states
3588480	3591480	which give us some reward, that's the most basic way,
3591480	3594480	the oldest way to think about rational and intentional action.
3594480	3597480	And it seems that even 10-month-olds understand some version of that,
3597480	3600480	where the cost can be measured in physical terms.
3600480	3603480	I see I'm running a little bit behind on time,
3603480	3605480	and I wanted to leave some time for discussion.
3605480	3608480	So I'll just go very quickly through a couple of other things,
3608480	3613480	and happy to stay around at the end for discussion.
3613480	3617480	What I showed you here was the science, where does the engineering go?
3617480	3620480	So one thing you can do with this is say,
3620480	3623480	build a machine system that can look not a little animated cartoon
3623480	3626480	like these baby experiments, but a real person doing something,
3626480	3629480	and again, combine physical costs and constraints of actions
3629480	3632480	with some understanding of the agent's utilities,
3632480	3636480	that's the math of planning, to figure out what they wanted.
3636480	3640480	So look in this scene here, and see if you can judge which object
3640480	3642480	the woman is reaching for.
3642480	3645480	So you can see there's a grid of four by four objects,
3645480	3648480	there's 16 objects here, and she's going to be reaching for one of them.
3648480	3650480	It's going to play in slow motion,
3650480	3653480	but raise your hand when you know which one she's reaching for.
3653480	3660480	So just watch and raise your hand when you know which one she wants.
3660480	3662480	So most of the hands are up by now.
3662480	3664480	And notice, I was looking at your hands not here,
3664480	3666480	but what happened is most of the hands were up
3666480	3670480	at about the time when that gray, or the one that dashed line,
3670480	3671480	shot up.
3671480	3675480	That's not human data, you provided the data, this is our model.
3675480	3677480	So our model is predicting, more or less,
3677480	3679480	when you're able to say what her goal was.
3679480	3681480	It's well before she actually touched the object.
3681480	3682480	How does the model work?
3682480	3686480	Again, I'll skip the details, but it does the same kind of thing
3686480	3688480	that our models of those infants did.
3688480	3692480	Namely, but in this case, it does it with a full body model from robotics.
3692480	3694480	We use what's called the Mujoko physics engine,
3694480	3697480	which is a standard tool in robotics for planning
3697480	3700480	physically efficient reaches of, say, a humanoid robot.
3700480	3705480	And we say, we can give this planner program a goal object as input.
3705480	3707480	We can give it each of the possible goal objects as input
3707480	3709480	and say, plan the most physically efficient action,
3709480	3712480	so the one that uses the least energy to get to that object.
3712480	3714480	And then we can do a Bayesian inference.
3714480	3716480	This is the probabilistic inference part.
3716480	3719480	This program is the Mujoko planner.
3719480	3722480	But then we can say, I want to do Bayesian inference
3722480	3724480	to work backwards from what I observed,
3724480	3726480	which was the action, to the input to that program.
3726480	3728480	What goal was provided as input to the planner?
3728480	3732480	And here you can see the full array of four by four possible inputs
3732480	3734480	and those bars that are moving up and down,
3734480	3736480	that's the Bayesian posterior probability
3736480	3739480	of how likely each of those was to be the goal.
3739480	3741480	And what you can see is it converges on the right answer,
3741480	3744480	at least, well, it turns out to be the ground truth right answer,
3744480	3746480	but also the right answer according to what people think,
3746480	3749480	with about the same kind of data that people took.
3749480	3751480	Now, you might say, well, okay,
3751480	3753480	sure, if I just wanted to build a system
3753480	3755480	that could detect what somebody was reaching for,
3755480	3758480	I could generate a training data set of this sort of scene
3758480	3761480	and train something up to analyze patterns of motion.
3761480	3763480	But again, because the engine in your head
3763480	3765480	actually does something we think more like this,
3765480	3768480	it does what we call inverse planning over a physics model.
3768480	3770480	It can apply to much more interesting scenes
3770480	3772480	that you haven't really seen much of before.
3772480	3774480	So take the scene on the left, right,
3774480	3776480	where again, you see somebody reaching for one
3776480	3778480	of a four by four array of objects,
3778480	3780480	but what you see is a strange kind of reach.
3780480	3782480	Can you see why he's doing a strange reach?
3782480	3784480	Up there, it's a little small,
3784480	3787480	but you can see that he's reaching over something, right?
3787480	3789480	It's actually a pane of glass, right?
3789480	3790480	You see that?
3790480	3793480	And then there's this other guy who's helping him,
3793480	3796480	who sees what he wants and hands him the thing he wants.
3796480	3800480	So how does the guy on the foreground see the other guy's goal?
3800480	3803480	How does he infer his goal and know how to help him?
3803480	3805480	And then how do we look at the two of them
3805480	3807480	and figure out who's trying to help who?
3807480	3809480	Or that in a scene like this one here,
3809480	3811480	that it's not somebody trying to help somebody,
3811480	3813480	but rather the opposite, okay?
3813480	3815480	So here's a model on the left of how that might work, right?
3815480	3817480	And we think this is the kind of model
3817480	3819480	needed to tackle this sort of challenge here, right?
3819480	3821480	Basically, it's a model,
3821480	3823480	we take this model of planning,
3823480	3825480	sort of maximal expected utility planning,
3825480	3827480	which you can run backwards,
3827480	3829480	but then we recursively nest these models inside each other.
3829480	3830480	Okay?
3830480	3832480	An agent is helping another agent.
3832480	3834480	If this agent is acting apparently to us,
3834480	3837480	seems to be maximizing an expected utility,
3837480	3840480	that's a positive function of that agent's expectation
3840480	3842480	about another agent's expected utility,
3842480	3844480	and that's what it means to be a helper.
3844480	3846480	Hindering is sort of the opposite
3846480	3849480	if one seems to be trying to lower somebody else's utility, okay?
3849480	3851480	And we've used these same kind of models
3851480	3853480	to also describe infant's understanding
3853480	3855480	of helping and hindering in a range of scenes.
3855480	3858480	I'll just say one last word about learning
3858480	3860480	that everybody wants to know about learning,
3860480	3862480	and the key thing here,
3862480	3864480	and it's definitely part of any picture of AGI,
3864480	3866480	but the thought I want to leave you on
3866480	3868480	is really about what learning is about, okay?
3868480	3871480	It'll be just a few more slides, and then I'll stop, I promise.
3871480	3873480	None of the models I showed you so far
3873480	3874480	really did any learning.
3874480	3877480	They certainly didn't do any task-specific learning, okay?
3877480	3879480	We set up a probabilistic program, and then we let it do inference.
3879480	3881480	Now, that's not to say that we don't think
3881480	3883480	people learn to do these things, we do.
3883480	3886480	But the real learning goes on when you're much younger, right?
3886480	3888480	The other thing I showed you in basic form,
3888480	3890480	even a one-year-old baby can do, okay?
3890480	3893480	The basic learning goes on to support these kinds of abilities,
3893480	3895480	not that there isn't learning beyond one year,
3895480	3897480	but the basic way you learn to, say,
3897480	3899480	solve these physics problems is what goes on
3899480	3903480	in the brain of a child between zero and 12 months.
3903480	3905480	So this is just an example of some phenomena
3905480	3908480	that come from the literature on infant cognitive development.
3908480	3910480	These are very rough timelines.
3910480	3912480	You can take pictures of this if you like.
3912480	3915480	This is always a popular slide because it really is quite inspiring, I think,
3915480	3917480	and I can give you lots of literature pointers,
3917480	3920480	but I'm summarizing in very broad strokes with big error bars
3920480	3924480	what we've learned in the field of infant cognitive development
3924480	3927480	about when and how kids seem to at least come
3927480	3930480	to a certain understanding of basic aspects of physics.
3930480	3934480	So if you really want to study how people learn to be intelligent,
3934480	3936480	a lot of what you have to study are kids at this age.
3936480	3939480	You have to study what's already in their brain at zero months
3939480	3941480	and what they learn and how they learn
3941480	3943480	between four, six, eight, ten, twelve, and so on,
3943480	3946480	and all that beyond that, okay?
3946480	3949480	Now, effectively what that amounts to, we think,
3949480	3952480	is if what you're learning is something like, let's say,
3952480	3956480	an intuitive game physics engine to capture these basic abilities,
3956480	3959480	then what we need, if we're going to try to reverse engineer that,
3959480	3961480	is what we might think of as a program learning program.
3961480	3963480	If your knowledge is in the form of a program,
3963480	3966480	then you have to have programs that build other programs, right?
3966480	3968480	This is what I was talking about at the beginning,
3968480	3970480	about learning as building models of the world.
3970480	3972480	Or ultimately, if you think what we start off with
3972480	3975480	is something like a game engine that can play any game,
3975480	3977480	then what you have to learn is the program of the game
3977480	3978480	that you're actually playing,
3978480	3981480	or the many different games that you might be playing over your life.
3981480	3984480	So think of learning as programming the game engine in your head
3984480	3987480	to fit with your experience and to fit with the possible actions
3987480	3989480	that you seem like you can take.
3989480	3992480	Now, this is what you could call the hard problem of learning
3992480	3994480	if you come to learning from, say, neural networks
3994480	3996480	or other tools in machine learning, right?
3996480	3999480	So what makes most of machine learning go right now,
3999480	4001480	and certainly what makes neural networks so appealing,
4001480	4004480	is that you can set up basically a big function approximator
4004480	4007480	that can approximate many of the functions you might want to do
4007480	4009480	in a certain application or task,
4009480	4011480	but in a way that's end-to-end differentiable
4011480	4013480	and with a meaningful cost function.
4013480	4015480	So you can have one of these nice optimization landscapes.
4015480	4018480	You can compute the gradients and basically just roll downhill
4018480	4020480	until you get to an optimal solution.
4020480	4023480	But if you're talking about learning as something like search
4023480	4025480	in the space of programs,
4025480	4027480	we don't know how to do anything like that yet.
4027480	4030480	We don't know how to set this up as any kind of a nice optimization problem
4030480	4033480	with any notion of smoothness or gradients, okay?
4033480	4036480	Rather, what we need is instead of learning as, like,
4036480	4038480	rolling downhill effectively, right,
4038480	4040480	a process which just, if you're willing to wait long enough,
4040480	4044480	you know, some, you know, simple algorithm we'll take care of,
4044480	4047480	think of what we call the idea of learning as programming.
4047480	4050480	There's a popular metaphor in cognitive development
4050480	4052480	called the child as scientist,
4052480	4055480	which emphasizes children as active theory builders
4055480	4059480	and children's play as a kind of casual experimentation.
4059480	4061480	But this is the algorithmic complement to that,
4061480	4063480	what we call the child as coder,
4063480	4065480	or around MIT we'll say the child as hacker,
4065480	4067480	but the rest of the world, if you say child as hacker,
4067480	4069480	they think of someone who breaks into your email
4069480	4071480	and steals your credit card numbers,
4071480	4075480	we all know that hacking is, you know, making your code more awesome, right?
4075480	4077480	If your knowledge is some kind of code,
4077480	4079480	or, you know, library of programs,
4079480	4082480	then learning is all the ways that a child hacks on their code
4082480	4084480	to make it more awesome.
4084480	4086480	Awesome can mean more accurate,
4086480	4088480	but it can also mean faster, more elegant,
4088480	4091480	more transportable to other applications or their tasks,
4091480	4094480	more explainable to others, maybe just more entertaining, okay?
4094480	4097480	Children have all of those goals in learning,
4097480	4100480	and the activities by which they make their code more awesome
4100480	4104480	also correspond to many of the activities of coding, right?
4104480	4107480	So think about all the ways on a day-to-day basis
4107480	4109480	you might make your code more awesome, right?
4109480	4113480	You might have a big library of existing functions
4113480	4115480	with some parameters that you can tune on a data set.
4115480	4118480	That's basically what you do with backprop or stochastic gradient descent
4118480	4120480	in training a deep learning system.
4120480	4123480	But think about all the ways in which you might actually modify the underlying function.
4123480	4126480	So write new code, or take old code from some other thing
4126480	4129480	and map it over here, or make a whole new library of code,
4129480	4131480	or refactor your code to some other, you know,
4131480	4134480	some other basis for it that will work more robustly
4134480	4138480	and be more extensible, or transpiling, or compiling, right?
4138480	4140480	Or even just commenting your code,
4140480	4142480	or asking someone else for their code, okay?
4142480	4145480	Again, these are all ways that we make our code more awesome
4145480	4148480	and children's learning has analogues to all of these
4148480	4150480	that we would want to understand as an engineer
4150480	4152480	from an algorithmic point of view.
4152480	4155480	So in our group, we've been working on various early steps towards this.
4155480	4159480	And again, we don't have anything like program writing programs
4159480	4161480	at the level of children's learning algorithms.
4161480	4163480	But one example of something that we did in our group,
4163480	4165480	which you might not have thought of being about this,
4165480	4167480	but it's definitely the AI work we did
4167480	4171480	that got the most attention in the last couple of years from our group.
4171480	4173480	We had this paper that was in science,
4173480	4175480	it was actually on the cover of science,
4175480	4178480	sort of just hit the market at the right time, if you like,
4178480	4180480	and it got about 100 times more publicity
4180480	4182480	than anything else I've ever done,
4182480	4184480	which is partly a testament to the really great work
4184480	4187480	that Brendan Lake, who was the first author, did for his PhD here,
4187480	4190480	but much more so just about the hunger for AI systems at the time
4190480	4192480	when we published this in 2015.
4192480	4195480	And we built a machine system that, the way we described it,
4195480	4198480	was doing human-level concept learning for simple concepts,
4198480	4200480	very simple visual concepts.
4200480	4203480	These handwritten characters in many of the world's alphabets.
4203480	4205480	For those of you who know the famous MNIST dataset,
4205480	4208480	the dataset of handwritten digits 0 through 9,
4208480	4211480	sorry, that drove so much good research in deep learning
4211480	4213480	and pattern recognition,
4213480	4216480	it did that not because Jan Lacoon, who put that together,
4216480	4219480	or Jeff Hinton, who did a lot of work on deep learning with MNIST,
4219480	4221480	they were interested fundamentally in character recognition,
4221480	4224480	that they saw that as a very simple test bed
4224480	4226480	for developing more general ideas.
4226480	4228480	And similarly, we did this work on getting machines
4228480	4232480	to do a kind of one-shot learning of generative models,
4232480	4234480	also to develop more general ideas.
4234480	4238480	We saw this as learning very simple, little mini probabilistic programs.
4238480	4240480	In this case, what are those programs?
4240480	4242480	They're the programs you use to draw a character.
4242480	4245480	So ask yourself, how can you look at any one of these characters
4245480	4247480	and see, in a sense, how somebody might draw it?
4247480	4251480	The way we tested this in our system was this little visual-turing test
4251480	4254480	where we showed people one character in a novel alphabet
4254480	4256480	and we said, draw another one.
4256480	4258480	We paired nine people, like say on the left,
4258480	4261480	and nine samples from our machine, say on the right,
4261480	4263480	and we asked other people,
4263480	4265480	could you tell which was the human drawing another example,
4265480	4268480	or imagining another example, and which was the machine?
4268480	4269480	And people couldn't tell.
4269480	4271480	When I said ones on the left and ones on the right,
4271480	4272480	I don't actually remember,
4272480	4274480	and on different ones you can see if you can tell.
4274480	4275480	It's very hard to tell.
4275480	4277480	Can you tell for each one of these characters
4277480	4281480	which new set of examples were drawn by a human versus a machine?
4281480	4284480	Here's the right answer, and probably you couldn't tell.
4284480	4288480	The way we did this was by assembling a simple kind of program learning program.
4288480	4290480	So we basically said, when you draw a character,
4290480	4293480	you're assembling strokes and substrokes with goals and sub-goals
4293480	4295480	that produce ink on the page,
4295480	4298480	and when you see a character, you're working backwards to figure out
4298480	4301480	what was the program, the most efficient program that did that.
4301480	4304480	So you're basically inverting a probabilistic program,
4304480	4306480	doing Bayesian inference to the program,
4306480	4308480	most likely to have generated what you saw.
4308480	4310480	This is one small step, we think,
4310480	4312480	towards being able to learn programs,
4312480	4315480	to being able to learn something ultimately like a whole game engine program.
4315480	4319480	The last thing I'll leave you with is just a pointer to sort of work in action.
4319480	4322480	So this is some work being done by a current PhD student
4322480	4323480	who works partly with me,
4323480	4325480	but also with Armando, Solar Lasama, and CSAIL.
4325480	4326480	This is Kevin Ellis.
4326480	4329480	It's an example of what's now, I think, again,
4329480	4331480	an emerging, exciting area in AI
4331480	4333480	well beyond anything that we're doing,
4333480	4336480	is combining techniques from where Armando comes from,
4336480	4338480	which is the world of programming languages,
4338480	4340480	not machine learning or AI,
4340480	4342480	but tools from programming languages
4342480	4345480	which can be used to automatically synthesize code.
4345480	4347480	With the machine learning toolkit,
4347480	4350480	in this case a kind of Bayesian minimum description-length idea,
4350480	4352480	to be able to make, again,
4352480	4355480	what is really one small step towards machines that can learn programs
4355480	4359480	by basically trying to efficiently find the shortest, simplest program
4359480	4361480	which can capture some data set.
4361480	4363480	So we think by combining these kinds of tools,
4363480	4366480	in this case, let's say, from Bayesian inference over programs
4366480	4368480	with a number of tools that have been developed
4368480	4370480	in other areas of computer science
4370480	4372480	that don't look anything or haven't been considered
4372480	4375480	to be machine learning or AI, like programming languages.
4375480	4378480	It's one of the many ways that, going forward,
4378480	4381480	we're going to be able to build smarter, more human-like machines.
4381480	4385480	So just to end then, what I've tried to tell you here is talk,
4385480	4388480	first of all, identify the ways in which human intelligence
4388480	4390480	goes beyond pattern recognition
4390480	4392480	to really all these activities of modeling the world.
4392480	4394480	To give you a sense of some of the domains
4394480	4396480	where we can start to study this,
4396480	4399480	like common sense scene understanding, for example,
4399480	4403480	or something like one-shot learning, for example,
4403480	4405480	like what we were just doing there,
4405480	4408480	or learning as programming the engine in your head.
4408480	4411480	And to give you a sense of some of the technical tools,
4411480	4414480	probabilistic programs, program synthesis, game engines,
4414480	4417480	for example, as well as a little bit of deep learning,
4417480	4421480	that bringing together, we're starting to be able to make these things real.
4421480	4424480	Now, that's the science agenda and the reverse engineering agenda,
4424480	4427480	but think about, for those of you who are interested in technology,
4427480	4430480	what are the many big AI frontiers that this opens up?
4430480	4433480	So the one I'm most excited about is this idea
4433480	4436480	which I've highlighted here in our big research agenda.
4436480	4438480	This is the one I'm most excited about to work on
4438480	4440480	for the rest of my career, honestly.
4440480	4444480	But it's really what is the oldest and maybe the best dream
4444480	4448480	of AI researchers of how to build a human-like intelligence system,
4448480	4450480	a real AGI system.
4450480	4453480	It's the idea that Turing proposed when he proposed the Turing test,
4453480	4455480	and Minsky proposed this at different times in his life.
4455480	4457480	Or many people have proposed this, right?
4457480	4459480	Which is to build a system that grows into intelligence
4459480	4462480	the way a human does, that starts like a baby and learns like a child.
4462480	4466480	And I've tried to show you how we're starting to be able to understand those things.
4466480	4470480	What a baby's mind starts with, how children actually learn.
4470480	4472480	And looking forward, we might imagine
4472480	4474480	that someday we'll be able to build machines that can do this.
4474480	4477480	I think we can actually start working on this right now.
4477480	4480480	And that's something that we're doing in our group.
4480480	4483480	So if that kind of thing excites you, then I encourage you to work on it,
4483480	4484480	maybe even with us.
4484480	4487480	Or if any one of these other activities of human intelligence excite you,
4487480	4491480	I think taking the kind of science-based reverse engineering approach
4491480	4495480	that we're doing and then trying to put that into engineering practice,
4495480	4498480	this is not just a possible route,
4498480	4501480	but I think it's quite possibly the most valuable route
4501480	4504480	that you could work on right now to try to actually achieve
4504480	4507480	at least some kind of artificial general intelligence,
4507480	4510480	especially the kind of intelligence, AI system,
4510480	4513480	that's going to live in a human world and interact with humans.
4513480	4516480	There's many kinds of AI systems that could live in worlds of data
4516480	4518480	that none of us can understand or will ever live in ourselves.
4518480	4521480	But if you want to build machines that can live in our world
4521480	4524480	and interact with us the way we are used to interacting with other people,
4524480	4527480	then I think this is a route that you should consider.
4527480	4528480	Okay, thank you.
4528480	4539480	Hi there.
4539480	4543480	So, early in the talk you expressed some skepticism about whether or not
4543480	4546480	industry would get us to understanding human level intelligence.
4546480	4549480	It seems that there's a couple of trends that favour industry.
4549480	4552480	One is that industry is better than that academia at accumulating resources
4552480	4554480	and plowing them back into the topic.
4554480	4557480	And it seems at the moment we've got a bit of brain drain going on
4557480	4561480	from academia into industry, and that seems like an ongoing trend.
4561480	4565480	If you look at something like learning to fly or learning to fly into space,
4565480	4570480	then it looks like the story is one of industry kind of taking over the field
4570480	4573480	and going off on its own a little bit.
4573480	4577480	Academics still have a role, but industry kind of dominates,
4577480	4579480	so is industry going to overtake the field, do you think?
4579480	4581480	Well, that's a really good question,
4581480	4583480	and it's got several good questions packed into one there, right?
4583480	4586480	I didn't mean to say, this wasn't meant to say,
4586480	4589480	go academia, bad industry, right?
4589480	4594480	What I tried to say was the approaches that are currently getting
4594480	4596480	the most attention in industry, and they're really,
4596480	4599480	because they're really the most valuable ones right now for the short term,
4599480	4602480	you know, any industry is really focused on what it can do,
4602480	4605480	what are the value propositions on basically a two-year timescale at most?
4605480	4608480	I mean, if you ask, say, Google researchers to take the most prominent example,
4608480	4611480	that's pretty much what they'll all tell you, okay?
4611480	4616480	Maybe things that might pay off initially in two years,
4616480	4618480	but maybe take five years or more to really develop,
4618480	4621480	but if you can't show that it's going to do something practical for us
4621480	4623480	in two years in a way that matters for our bottom line,
4623480	4626480	then it's not really worth doing, okay?
4626480	4630480	So what I'm talking about is the technologies which right now
4630480	4633480	industry sees as meeting that specification,
4633480	4638480	and what I'm saying is right now, I think that's not where the root is
4638480	4642480	to something like human-like, but not the most valuable promising root
4642480	4644480	to human-like kinds of AI systems, all right?
4644480	4647480	But I hope that like in the case as you said, you know,
4647480	4650480	the basic research that we're doing now will be successful enough
4650480	4654480	that it will get the attention of industry when the time is right.
4654480	4657480	So, you know, I hope at some point, you know,
4657480	4661480	it won't, at least the engineering side will have to be done in industry,
4661480	4666480	not just in academia, but you're also pointing to issues of like brain drain
4666480	4669480	and other things like that, that I think these are real issues confronting our community.
4669480	4673480	I think everybody knows this, and I'm sure this will come up multiple times here,
4673480	4677480	which is, you know, I think we have to find ways to, even now,
4677480	4682480	to combine the best of the ideas, the energy, and the resources of academia and industry
4682480	4686480	if we want to keep doing basically something interesting, right?
4686480	4690480	If we just want to redefine AI to be, well, whatever people currently call AI
4690480	4694480	but scaled up, well, then fine, forget about it.
4694480	4697480	Or if we just want to say, let me and people like me do what we're doing
4697480	4701480	at what industry we consider a snail's pace on toy problems, okay, fine.
4701480	4706480	But if we want to, you know, if I want to take what I'm doing to the level
4706480	4710480	that will really be, you know, paying off the level that industry can appreciate
4710480	4714480	or just that really has technological impact on a broad scale, right?
4714480	4718480	Or I think if industry wants to take what it's doing and really build machines
4718480	4722480	that are actually intelligent, right, or machine learning that actually learns like a person,
4722480	4726480	then I think we need each other now and not just in some point in the future.
4726480	4730480	So this is a general challenge for MIT and for everywhere and for Google.
4730480	4734480	I mean, we just spent a few days talking to Google about exactly this issue.
4734480	4737480	In fact, this was a talk I prepared partly for that purpose.
4737480	4741480	So we wanted to raise those issues and it's just, I mean, really, I don't know what,
4741480	4745480	I mean, rather I can think of some solutions to that problem
4745480	4748480	of what you could call brain drain from the academic point of view
4748480	4752480	or what you could call just narrowing in into certain local minima in the industry point of view.
4752480	4757480	But they will require the leadership of both academic institutions like MIT
4757480	4760480	and companies like Google being creative about how they might work together
4760480	4762480	in ways that are a little bit outside of their comfort zone.
4762480	4767480	I hope that will start to happen, including at MIT and at many other universities
4767480	4769480	and at companies like Google and many others.
4769480	4773480	And I think we need it to happen for the health of all parties concerned.
4773480	4774480	Okay, thank you very much.
4774480	4776480	Thanks.
4776480	4782480	I'm curious about sort of the premise that you gave that one of the big gaps missing
4782480	4789480	at determining intelligence is the fact that we need to teach machines how to recognize models.
4789480	4797480	And I'm curious as to what you think sort of non-goal-oriented cognitive activity comes into play.
4797480	4803480	There are things like feelings and emotions and why you don't think
4803480	4809480	that might not necessarily be like the most important question.
4809480	4814480	The only reason emotions didn't appear on my slide is because there's a few reasons,
4814480	4816480	but the slide is only so big.
4816480	4819480	I wanted the font to be big, readable for such an important slide.
4819480	4824480	I have versions of my slide in which I do talk about that.
4824480	4827480	It's not that I think feelings or emotions aren't important.
4827480	4828480	I think they are important.
4828480	4832480	And I used to not have many insights about what to do about them,
4832480	4836480	but actually partly based on some of my colleagues here at MIT, BCS,
4836480	4841480	Laura Shultz and Rebecca Sacks, two of my cognitive colleagues who I work closely with,
4841480	4845480	they've been starting to do research on how people understand emotions,
4845480	4849480	both their own and others, and we've been starting to work with them on computational models.
4849480	4852480	So that's actually something I'm actively interested in and even working on.
4852480	4855480	But I would say, and again, for those of you who study emotion or know about this,
4855480	4857480	actually you're going to have Lisa coming in, right?
4857480	4860480	So she's going to basically say a version of the same thing, I think.
4860480	4863480	The deepest way to understand, she's one of the world's experts on this,
4863480	4867480	the deepest way to understand emotion is very much based on our mental models
4867480	4870480	of ourselves, of the situation we're in and of other people, right?
4870480	4876480	Think about, for example, all of the different, I mean, if you think about,
4876480	4879480	I mean, again, Lisa will talk all about this, but if you think about emotion,
4879480	4882480	it's just a very small set of what are sometimes called basic emotions,
4882480	4889480	like being happy or angry or sad or, you know, those are small number of them, right?
4889480	4892480	There's usually only a few, right?
4892480	4896480	You might not say, you might see that as somehow like very basic things
4896480	4898480	that are opposed to some kind of cognitive activity.
4898480	4902480	But think about all the different words we have for emotion, right?
4902480	4907480	For example, think about a famous cognitive emotion like regret.
4907480	4910480	What does it mean to feel regret or frustration, right?
4910480	4915480	To know both for yourself when you're not just feeling kind of down or negative,
4915480	4917480	but you're feeling regret.
4917480	4921480	That means something like I have to feel like there's a situation that came out
4921480	4926480	differently from how I hoped and I realized I could have done something differently, right?
4926480	4929480	So that means you have to be able to understand, you have to have a model.
4929480	4932480	You have to be able to do a kind of counterfactual reasoning and to think,
4932480	4935480	oh, if only I had acted a different way, then I can predict that the world would have
4935480	4937480	come out differently and that's the situation I wanted,
4937480	4940480	but instead it came out this other way, right?
4940480	4944480	Or think about frustration, again, that requires something like understanding,
4944480	4947480	okay, I've tried a bunch of times, I thought this would work,
4947480	4950480	but it doesn't seem to be working, maybe I'm ready to give up.
4950480	4953480	Those are all, those are very important human emotions.
4953480	4956480	We have to understand ourselves, we need that,
4956480	4958480	to understand other people, to understand communication,
4958480	4962480	but those are all filtered through the kinds of models of action
4962480	4964480	that I was, just the ones I was talking about here
4964480	4966480	with these say cost-benefit analyses of action.
4966480	4969480	So what I'm, so I'm just trying to say I think this is very basic stuff
4969480	4972480	that will be the basis for building, I think,
4972480	4976480	better engineering-style models of the full spectrum of human emotion
4976480	4979480	beyond just like, well, I'm feeling good or bad or scared, okay?
4979480	4982480	And I think when you see Lisa, she will, in her own way,
4982480	4984480	say something very similar.
4984480	4986480	Interesting, thanks.
4986480	4987480	Yeah.
4987480	4989480	Thanks, Josh, for your nice talk.
4989480	4993480	So all is about human cognition and try to build a model to mimic those cognition,
4993480	4996480	but you don't, how much could help you to understand
4996480	4998480	how the circuit implement those things?
4998480	5000480	I mean, like the circuit's in the brain.
5000480	5001480	Yeah, and what's the...
5001480	5003480	Is that what you work on by any chance?
5003480	5004480	Sorry, what?
5004480	5005480	Is that what you work on by any chance?
5005480	5006480	Yeah, I know.
5006480	5008480	I'm kidding, yeah, yeah.
5008480	5010480	So in the center for brains, minds, and machines,
5010480	5012480	as well as in brain and cognitive science, yeah,
5012480	5016480	I have a number of colleagues who study the actual hardware basis
5016480	5018480	of this stuff in the brain, and that includes
5018480	5020480	like the large-scale architecture of the brain,
5020480	5022480	say like what Nancy Camuscher, Rebecca Sacks,
5022480	5025480	study with functional brain imaging, or the more detailed circuitry,
5025480	5028480	usually requires recording from, say, non-human brains, right,
5028480	5031480	at the level of individual neurons and connections between neurons.
5031480	5033480	All right, so I'm very interested in those things,
5033480	5036480	although it's not mostly what I work on, right?
5036480	5039480	But I would say, you know, again, like in many other areas of science,
5039480	5042480	certainly in neuroscience, the kind of work I'm talking about here
5042480	5044480	in a sort of classic reductionist program
5044480	5047480	sets the target for what we might look for.
5047480	5049480	Like if I just want to go...
5049480	5050480	I mean, I would...
5050480	5054480	What I would assert, right, in my working conjecture
5054480	5057480	is that if you do the kind of work that I'm talking about here,
5057480	5059480	it gives you the right targets
5059480	5061480	or gives you a candidate set of targets to look for,
5061480	5064480	what are the neural circuits computing, right?
5064480	5069480	Whereas if you just go in and just, say, start poking around in the brain
5069480	5071480	or have some idea that what you're going to try to do
5071480	5073480	is find the neural circuits which underlie behavior,
5073480	5078480	without a sense of the computations needed to produce those behaviors,
5078480	5082480	I think it's going to be very difficult to know what to look for
5082480	5085480	and to know when you've found even viable answers.
5085480	5090480	So I think that's the standard kind of reductionist program.
5090480	5092480	But it's not...
5092480	5097480	I also think it's not one that is divorced from the study of neural circuits.
5097480	5100480	It's also one, if you look at the broad picture of reverse engineering,
5100480	5105480	it's one where neural circuits and understanding the circuits in the brain
5105480	5108480	play an absolutely critical role, okay?
5108480	5113480	I would say when you look at the brain at the hardware level as an engineer,
5113480	5115480	I'm mostly looking at the software level, right?
5115480	5118480	But when you look at the hardware level, there are some remarkable properties.
5118480	5121480	One remarkable property, again, is how much parallelism there is
5121480	5124480	and in many ways how fast the computations are, okay?
5124480	5128480	Neurons are slow, but the computations of intelligence are very fast.
5128480	5132480	So how do we get elements that are, in some sense, quite slow in their time constant
5132480	5134480	to produce such intelligent behavior so quickly?
5134480	5137480	That's a great mystery and I think if we understood that,
5137480	5140480	we would have payoff for building all sorts of, you know,
5140480	5143480	basically application embedded circuits, okay?
5143480	5146480	But also, maybe most important is the power consumption
5146480	5149480	and again, many people have noted this, right?
5149480	5152480	If you look at the power consumption, the power that the brain consumes,
5152480	5154480	like, what did I eat today, okay?
5154480	5156480	Almost nothing.
5156480	5159480	My daughter, who's, again, she's doing an internship here,
5159480	5161480	literally yesterday, all she ate was a burrito.
5161480	5165480	And yet, she wrote 300 lines of code for her internship project
5165480	5168480	on a really cool computational linguistics project.
5168480	5173480	So somehow she turned a burrito into, you know, a model of child language acquisition, okay?
5173480	5176480	But how did she do that or how do any of us do this, right?
5176480	5178480	Where if you look at the power that we consume, when we simulate
5178480	5182480	even a very, very small chunk of cortex on our conventional hardware,
5182480	5184480	or we do any kind of machine learning thing,
5184480	5188480	we have systems which are very, very, very, very far from the power
5188480	5190480	of the human brain computationally,
5190480	5193480	but in terms of physical energy consumed,
5193480	5196480	way past what any individual brain is doing.
5196480	5201480	So how do we get circuitry of any sort, biological or just any physical circuits
5201480	5204480	to be as smart as we are with as little energy as we are?
5204480	5209480	This is a huge problem for basically every area of engineering, right?
5209480	5214480	If you want to have any kind of robot, the power consumption is a key bottleneck.
5214480	5216480	Same for self-driving cars.
5216480	5221480	If we want to build AI without contributing to global warming and climate change,
5221480	5225480	let alone use AI to solve climate change, we really need to address these issues,
5225480	5229480	and the brain is a huge guide there, right?
5229480	5231480	I think there are some people who are really starting to think about this.
5231480	5235480	How can we say, for example, build somehow brain-inspired computers
5235480	5238480	which are very, very low power but maybe only approximate?
5238480	5240480	So I'm thinking here of Joe Bates.
5240480	5242480	I don't know if you know Joe.
5242480	5245480	He's been around MIT and other places for quite a while.
5245480	5247480	Can I tell them about your company?
5247480	5251480	So Joe has a startup in Kendall Square called Singular Computing,
5251480	5255480	and they have some very interesting ideas including some actual implemented technology
5255480	5259480	for low power approximate computing in a sort of a brain-like way
5259480	5264480	that might lead to possibly even like the ability to build something, this is Joe's dream,
5264480	5267480	to build something that's about the size of this table but that has a billion cores,
5267480	5271480	a billion cores and runs on a reasonable kind of power consumption.
5271480	5273480	I would love to have such a machine.
5273480	5277480	If somebody wants to help Joe build it, I think he'd love to talk to you.
5277480	5280480	But it's one of a number of ideas.
5280480	5282480	I mean Google X, people are working on similar things.
5282480	5286480	Probably most of the major chip companies are also inspired by this idea.
5286480	5289480	And I think even if you didn't think you were interested in the brain,
5289480	5292480	if you want to build the kind of AI we're talking about
5292480	5294480	and run it on physical hardware of any sort
5294480	5298480	and understanding how the brain's circuits compute what they do,
5298480	5302480	what I'm talking about with as little power as they do,
5302480	5304480	I don't know any better place to look.
5304480	5308480	It seems like a lot of the improvements in AI have been driven by increasing
5308480	5310480	like computational power.
5310480	5312480	How far would you say...
5312480	5314480	I mean like GPUs or CPUs.
5314480	5320480	How far would you say we are from hardware that could run a general artificial intelligence?
5320480	5322480	Of the kind that I'm talking about.
5322480	5325480	Yeah I don't know, I'll start with a billion cores and then we'll see.
5325480	5328480	I mean I think we're...
5328480	5331480	I think there's no way to answer that question in a way that's software independent.
5331480	5333480	I don't know how to do that, right?
5333480	5337480	But I think that it's...
5337480	5340480	And I don't know, like when you say how far are we,
5340480	5343480	you mean how far am I with the resources I have right now?
5343480	5347480	How far am I if Google decides to put all of its resources at my disposal,
5347480	5350480	like they might if I were working at DeepMind?
5350480	5352480	I don't know the answer to that question.
5352480	5356480	But I think what we can say is this.
5356480	5360480	Individual neurons, I mean again this goes back to another reason to study neural circuits.
5360480	5364480	If you look at what we currently call neural networks in the AI side,
5364480	5368480	the model of a neuron is this very very simple thing, right?
5368480	5370480	Individual neurons are not only much more complex,
5370480	5372480	but have a lot more computational power.
5372480	5375480	It's not clear how they use it or whether they use it.
5375480	5379480	But I think it's just as likely that a neuron is something like a real you, right?
5379480	5381480	Is that a neuron is something like a computer.
5381480	5386480	Like one neuron in your brain is more like a CPU node, okay?
5386480	5387480	Maybe.
5387480	5390480	And thus the ten billion or trillion, you know,
5390480	5393480	the large number of neurons in your brain,
5393480	5395480	I think it's like ten billion cortical,
5395480	5397480	pyramidal neurons or something,
5397480	5399480	might be like ten billion cores, okay?
5399480	5402480	For example, that's at least as plausible I think to me as any other estimate.
5402480	5408480	So I think we're definitely on the underside with very big error bars.
5408480	5410480	So I completely agree that,
5410480	5412480	or if this is what you might be suggesting,
5412480	5414480	and going back to my answer to your question,
5414480	5416480	I don't think we're going to get to what I'm talking about,
5416480	5418480	anything like a real brain scale
5418480	5421480	without major innovations on the hardware side.
5421480	5424480	And you know, it's interesting that what drove those innovations
5424480	5427480	that support current AI was mostly not AI.
5427480	5429480	It was the video game industry.
5429480	5432480	When I point to the video game engine in your head,
5432480	5436480	that's a similar thing that was driven by the video game industry on the software side.
5436480	5440480	I think we should all play as many video games as we can
5440480	5442480	and contribute to the growth of the video game industry.
5443480	5446480	Because, no, because I mean, you can see this in very,
5446480	5448480	there are companies out there, for example,
5448480	5450480	there's a company called Improbable,
5450480	5452480	which is a London company,
5452480	5455480	London-based startup, a pretty sizable startup at this point,
5455480	5458480	which is building something that they call Spatial OS,
5458480	5460480	which it's not a hardware idea,
5460480	5462480	but it's a kind of software idea
5462480	5464480	for very, very big distributed computing environments
5464480	5466480	to run much, much more complex,
5466480	5468480	realistic simulations of the world
5468480	5471480	for much more interesting, immersive, permanent video games.
5471480	5473480	I think that's one thing that might, hopefully,
5473480	5476480	that will lead to more fun new kinds of games,
5476480	5479480	but that's one example of where we might look to that industry
5479480	5482480	to drive some of the, you know, just computer systems,
5482480	5484480	really hardware and software systems
5484480	5488480	that will take our game to the next level.
5489480	5493480	Josh, understanding on the algorithmic level or cognitive level
5493480	5496480	is just to understanding the meaning of learning
5496480	5498480	would be how to predict,
5498480	5500480	but on the circuit level it's different.
5500480	5502480	On the circuit level?
5502480	5504480	Well, of course it's different, right?
5504480	5507480	But already, I think you made a mistake there, honestly.
5507480	5510480	Like, you said the cognitive level is learning how to predict,
5510480	5512480	but I'm not sure what you mean by that.
5512480	5514480	There's many things you could mean,
5514480	5516480	and what our cognitive science is about
5516480	5518480	is learning which of those versions,
5518480	5520480	like, I don't think it's learning how to predict.
5520480	5523480	I think it's learning what you need to know to plan actions
5523480	5525480	and to, you know, all those things.
5525480	5527480	Like, it's not just about predicting.
5527480	5529480	Because there are things we can imagine
5529480	5531480	that make the world different.
5531480	5533480	So generalization, sorry, you're not predicting.
5533480	5535480	When your model could generalize.
5535480	5537480	But especially in the transfer learning that you are interested in.
5537480	5539480	A few hundred of neurons in prefrontal cortex
5539480	5541480	they could generalize a lot.
5541480	5545480	But not kind of a Bayesian model could do that.
5545480	5548480	You said, but a Bayesian model won't do that?
5548480	5550480	Or they don't do it the way a Bayesian model does?
5550480	5553480	For sure, because that's in the abstract level.
5553480	5555480	Well, I mean, how do you really know?
5555480	5558480	Like, and what does it mean to say that some neurons do it?
5558480	5560480	So maybe another way to put this is to say,
5560480	5562480	look, we have a certain math that we use
5562480	5564480	to capture these, you could call it abstract
5564480	5566480	or I call it software level abstractions.
5566480	5569480	I mean, all engineering is based on some kind of abstraction.
5569480	5571480	But you might have a circuit level abstraction,
5571480	5573480	a certain kind of hardware level
5573480	5575480	that you're interested in describing the brain at.
5575480	5577480	And I'm mostly working out or starting from
5577480	5579480	a more software level of abstraction.
5579480	5581480	They're all abstractions. We're not talking about molecules here.
5581480	5583480	We're talking about some abstract notion
5583480	5586480	of maybe a circuit or of a program.
5586480	5588480	Now, it's a really interesting question.
5588480	5590480	If I look at some circuits, how do I know
5590480	5592480	what program they're implementing?
5592480	5594480	If I look at the circuits in this machine,
5594480	5596480	could I tell what program they're implementing?
5596480	5598480	Well, maybe, but certainly it would be a lot easier
5598480	5600480	if I knew something about what programs they might be implementing
5600480	5602480	before I start to look at the circuitry.
5602480	5604480	If I just looked at the circuitry without
5604480	5606480	knowing what a program was or what programs
5606480	5608480	the thing might be doing or what kind of
5608480	5610480	programming components would be
5610480	5612480	mappable to circuits in different ways,
5612480	5614480	I don't even know how I'd begin to answer that question.
5614480	5616480	So I think we've made some progress
5616480	5618480	at understanding what neurons
5618480	5620480	are doing in certain low-level parts
5620480	5622480	of sensory system and certain parts
5622480	5624480	of the motor system, like primary motor cortex,
5624480	5626480	like basically the parts of the neurons
5626480	5628480	that are closest to the inputs and outputs of the brain,
5628480	5630480	where we don't...
5630480	5632480	Well, you could say we don't need
5632480	5634480	the kind of software abstractions
5634480	5636480	that I'm talking about or where
5636480	5638480	we sort of agree on what those things already are
5638480	5640480	so we can make enough progress
5640480	5642480	on knowing what to look for and how to know
5642480	5644480	when we found it.
5644480	5646480	But if you want to talk about flexible planning,
5646480	5648480	things that are more like cognition that
5648480	5650480	go on in prefrontal cortex,
5650480	5652480	at this point I don't think
5652480	5654480	that just by recording from those neurons
5654480	5656480	we're going to be able to answer those questions
5656480	5658480	in a meaningful engineering way.
5658480	5660480	A way that any engineer, software, hardware, whatever,
5660480	5662480	could really say, yeah, okay, I get it.
5662480	5664480	I get those insights in a way that I can engineer with.
5664480	5666480	And that's what my goal is.
5666480	5668480	So that's my goal to do at the software level,
5668480	5670480	the hardware level, or the entire systems level
5670480	5672480	connecting them.
5672480	5674480	And I think that we can do that
5674480	5676480	by taking what we're doing and bringing into contact
5676480	5678480	with people studying neural circuits.
5678480	5680480	But I don't think you can leave this level out
5680480	5682480	and just go straight to the neural circuits.
5682480	5684480	And I think the more progress we make,
5684480	5686480	the more we can help people who are
5686480	5688480	studying at the neural circuit level.
5688480	5690480	And they can help us address these other engineering questions
5690480	5692480	that we don't really have access to,
5692480	5694480	like the power issue or the speed issue.
5694480	5696480	Okay, thanks.
5696480	5698480	That was great.
5698480	5700480	Thank you.
5700480	5702480	Thank you.
5702480	5704480	Thank you.
