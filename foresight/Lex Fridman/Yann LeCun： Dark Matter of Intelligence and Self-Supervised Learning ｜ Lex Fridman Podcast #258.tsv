start	end	text
0	4400	The following is a conversation with Yann LeCun, his second time in the podcast.
4720	11280	He is the chief AI scientist at Meta, formerly Facebook, professor at NYU,
11680	16120	touring award winner, one of the seminal figures in the history of machine
16120	21640	learning and artificial intelligence, and someone who is brilliant and opinionated
22000	23240	in the best kind of way.
23600	25200	And so it was always fun to talk to him.
26000	28640	This is the Lex Friedman podcast to support it.
28840	30800	Please check out our sponsors in the description.
31240	34880	And now here's my conversation with Yann LeCun.
36120	40840	You co-wrote the article, Self-Supervised Learning, the Dark Matter of Intelligence.
40880	43200	Great title, by the way, with Yann Mizra.
43680	49360	So let me ask, what is self-supervised learning and why is it the dark matter of intelligence?
49880	51560	I'll start by the dark matter part.
52200	61120	There is obviously a kind of learning that humans and animals are doing that we currently
61120	64640	are not reproducing properly with machines or with AI, right?
64640	69640	So the most popular approaches to machine learning today are, or pydimes, I should say,
69640	71680	are supervised learning and reinforcement learning.
72680	74560	And they are extremely inefficient.
75080	78920	Supervised learning requires many samples for learning anything.
79760	84880	And reinforcement learning requires a ridiculously large number of trial and errors
84880	87160	to, for, you know, a system to run anything.
89320	90960	And that's why we don't have self-driving cars.
92960	94560	That's a big leap from one to the other.
94800	95240	Okay.
95320	102560	So that to solve difficult problems, you have to have a lot of human annotation for
102560	106360	supervised learning to work and to solve those difficult problems with reinforcement
106360	110920	learning. You have to have some way to maybe simulate that problem such that you can do
110920	114240	that large scale kind of learning that reinforcement learning requires.
114400	114760	Right.
114760	120720	So how is it that, you know, most teenagers can learn to drive a car in about 20 hours of
120840	128520	practice, whereas even with millions of hours of simulated practice, a self-driving car can't
128640	130600	actually learn to drive itself properly.
132120	133920	And so obviously we're missing something, right?
133920	138480	And it's quite obvious for a lot of people that, you know, the immediate response you get from
138920	144240	many people is, well, you know, humans use their background knowledge to learn faster.
144680	145320	And they're right.
145840	148080	Now, how was that background knowledge acquired?
148280	149360	And that's the big question.
150080	156000	So now you have to ask, you know, how do babies in the first few months of life learn how the
156000	160040	world works, mostly by observation, because they can hardly act in the world.
160920	165960	And they learn an enormous amount of background knowledge about the world that may be the basis
165960	167240	of what we call common sense.
168080	171200	This type of learning is not learning a task.
171240	173640	It's not being reinforced for anything.
173680	177080	It's just observing the world and figuring out how it works.
178320	180560	Building world models, learning world models.
181160	181880	How do we do this?
182040	184480	And how do we reproduce this in machine?
184480	191400	So self-supervisioning is, you know, one instance or one attempt at trying to reproduce
191400	192040	this kind of learning.
193040	193320	Okay.
193320	196320	So you're looking at just observation.
196320	198440	So not even the interacting part of a child.
198640	203360	It's just sitting there watching mom and dad walk around, pick up stuff, all of that.
203400	205280	That's what we mean by background knowledge.
205440	209320	Perhaps not even watching mom and dad, just, you know, watching the world go by.
209920	213920	Just having eyes open or having eyes closed or the very act of opening and closing eyes.
214480	217680	That the world appears and disappears, all of that basic information.
219120	224960	And you're saying in order to learn to drive, like the reason humans are able to learn to
224960	228680	drive quickly, some faster than others, is because of the background knowledge.
228680	233720	They were able to watch cars operate in the world in the many years leading up to it, the
233720	235800	physics of basics, objects, all that kind of stuff.
235800	236120	That's right.
236160	239680	I mean, the basic physics of objects, you don't even know, you don't even need to know, you
239680	240880	know, how a car works, right?
240880	242400	Because that you can learn fairly quickly.
242480	247640	I mean, the example I use very often is you're driving next to a cliff and you know in
247640	253000	advance because of your, you know, understanding of intuitive physics that if you turn the
253000	257240	wheel to the right, the car will veer to the right, we'll run off the cliff, fall off the
257240	259200	cliff and nothing good will come out of this, right?
260400	265520	But if you are a sort of, you know, tabular rice reinforcement learning system that doesn't
265520	271280	have a model of the world, you have to repeat folding off this cliff thousands of times
271280	272800	before you figure out it's a bad idea.
272800	276560	And then a few more thousand times before you figure out how to not do it.
277120	280680	And then a few more million times before you figure out how to not do it in every situation
280680	281520	you ever encounter.
282520	289280	So self-supervised learning still has to have some source of truth being told to it by
289280	289880	somebody.
290120	295480	And so you have to figure out a way without human assistance or without significant
295480	298720	amount of human assistance to get that truth from the world.
299080	303960	So the mystery there is how much signal is there?
303960	308440	How much truth is there that the world gives you, whether it's the human world, like you
308440	312360	watch YouTube or something like that, or it's the more natural world.
312960	314360	So how much signal is there?
314880	316000	So here's the trick.
316280	322480	There is way more signal in sort of a self-supervised setting than there is in either a supervised
322480	323680	or reinforcement setting.
324520	328200	And this is going to my, you know, analogy of the cake.
329520	334560	The, you know, low cake has someone that's called it, where when you try to figure out
334560	339080	how much information you ask the machine to predict and how much feedback you give the
339080	340280	machine at every trial.
340960	343280	In reinforcement learning, you give the machine a single scalar.
343280	345000	You tell the machine you did good, you did bad.
345360	348960	And you only tell this to the machine once in a while.
349560	352320	When I say you, it could be the universe telling the machine, right?
354040	355360	But it's just one scalar.
355840	360120	So as a consequence of this, you cannot possibly learn something very complicated without many,
360120	364200	many, many trials where you get many, many feedbacks of this type.
364720	370040	Supervised learning, you, you give a few bits to the machine at every, every sample.
371240	376320	Let's say your training image system on, you know, recognizing images on the image net.
376320	380720	There is 1000 categories, that's a little less than 10 bits of information per sample.
380960	382560	But self-supervised learning, here is the setting.
382560	388320	You ideally, we don't know how to do this yet, but ideally you would show a machine a segment
388320	394080	of a video and then stop the video and ask, ask the machine to predict what's going to happen next.
396320	402160	So you let the machine predict and then you let time go by and show the machine what actually
402160	407440	happened and hope the machine will, you know, learn to do a better job at predicting next
407440	408040	time around.
408200	413400	There's a huge amount of information you give the machine because it's an entire video clip
414600	420040	of, you know, of the future after the video clip you fed it in the first place.
420040	426760	So both for language and for vision, there's a subtle, seemingly trivial construction,
426760	430920	but maybe that's representative of what is required to create intelligence, which is
431720	432520	filling the gap.
433640	437480	So it sounds dumb, but can you
438520	443000	it's, it is possible that you can solve all of intelligence in this way.
443000	450120	Just for both language, just give a sentence and continue it or give a sentence and there's
450120	455000	a gap in it, some words blanked out and you fill in what words go there.
455640	461080	For vision, you give a sequence of images and predict what's going to happen next or
461080	462680	you fill in what happened in between.
463640	466600	Do you think it's possible that formulation alone
468520	473560	as a signal for self-supervised learning can solve intelligence for vision and language?
473560	475240	I think that's our best shot at the moment.
476280	481400	So whether this will take us all the way to, you know, human level intelligence or
481400	487240	something or just cat level intelligence is not clear, but among all the possible approaches
487240	489480	that people have proposed, I think it's our best shot.
489480	496440	So I think this idea of an intelligent system filling in the blanks, either,
497160	501800	predicting the future, inferring the past, filling in missing information.
503720	508120	I'm currently filling the blank of what is behind your head and what your head looks like
508120	513160	and from the back because I have a basic knowledge about how humans are made.
513720	517160	And I don't know if you're going to say at which point you're going to speak,
517160	520120	whether you're going to move your head this way or that way, which way you're going to look,
520120	524680	but I know you're not going to just dematerialize and reappear three meters down the hall
525640	530760	because I know what's possible and what's impossible according to the physics.
530760	534360	But you have a model of what's possible, what's impossible and then you'd be very surprised
534360	537160	if it happens and then you'll have to reconstruct your model.
537720	542120	Right. So that's the model of the world. It's what tells you, you know, what fills in the blanks.
542120	546760	So given your partial information about the state of the world, given by your perception,
547960	552680	your model of the world fills in the missing information and that includes predicting the
552680	558280	future, rich predicting the past, you know, filling in things you don't immediately perceive.
558280	564200	And that doesn't have to be purely generic vision or visual information or generic language.
564200	571000	You can go to specifics like predicting what control decision you make when you're driving
571000	578280	in a lane. You have a sequence of images from a vehicle and then you could, you have information
578360	584120	if you recorded on video where the car ended up going. So you can go back in time and predict
584120	588520	where the car went based on the visual information. That's very specific, domain specific.
589400	592680	Right. But the question is whether we can come up with sort of a generic
594120	599800	method for, you know, training machines to do this kind of prediction or filling in the blanks.
599800	606200	So right now, this type of approach has been unbelievably successful in the context of
606200	610440	natural language processing. Every modern natural language processing is pre-trained in
610440	615720	self-supervised manner to fill in the blanks. You show it a sequence of words, you remove 10%
615720	619080	of them, and then you train some gigantic neural net to predict the words that are missing.
620280	625800	And once you've pre-trained that network, you can use the internal representation
625800	631160	learned by it as input to, you know, something that you train supervised or whatever.
632040	636680	That's been incredibly successful, not so successful in images, although it's making progress.
637480	644040	And it's based on sort of manual data augmentation. We can go into this later. But
644040	648360	what has not been successful yet is training from video. So getting a machine to learn,
648360	653480	to represent the visual world, for example, by just watching video. Nobody has really
653480	658760	succeeded in doing this. Okay. Well, let's kind of give a high-level overview. What's the difference
658840	666360	in kind and in difficulty between vision and language? So you said people haven't been able to
667720	671880	really kind of crack the problem of vision open in terms of self-supervised learning,
671880	676920	but that may not be necessarily because it's fundamentally more difficult. Maybe like when
676920	683240	we're talking about achieving, like passing the Turing test in the full spirit of the Turing test
683240	688680	in language might be harder than vision. That's not obvious. So in your view, which is harder
689320	694680	or perhaps are they just the same problem? When the farther we get to solving each,
694680	698040	the more we realize it's all the same thing. It's all the same cake. I think
698840	703480	what I'm looking for are methods that make them look essentially like the same cake,
703480	709080	but currently they're not. And the main issue with learning world models or learning predictive
709080	718360	models is that the prediction is never a single thing because the world is not entirely predictable.
719160	722920	It may be deterministic or stochastic. We can get into the philosophical discussion about it,
722920	729480	but even if it's deterministic, it's not entirely predictable. And so if I play
730680	734040	a short video clip and then I ask you to predict what's going to happen next,
734040	739880	there's many, many plausible continuations for that video clip. And the number of continuation
739880	745480	grows with the interval of time that you're asking the system to make a prediction for.
746360	752520	And so one big question with self-provisioning is how you represent this uncertainty, how you
752520	758120	represent multiple discrete outcomes, how you represent a continuum of possible outcomes,
759320	765560	et cetera. And if you are a classical machine learning person, you say, oh,
765560	772440	you just represent a distribution. And that we know how to do when we're predicting words,
772520	777880	missing words in the text, because you can have a neural net give a score for every word in the
777880	783720	dictionary. It's a big list of numbers, maybe 100,000 or so. And you can turn them into a
783720	791480	probability distribution that tells you when I say a sentence, the cat is chasing the blank
791480	797240	in the kitchen. There are only a few words that make sense there. It could be a mouse or it could
797240	805720	be a lizard spot or something like that. And if I say the blank is chasing the blank in the savannah,
805720	812680	you also have a bunch of plausible options for those two words. Because you have kind of a
812680	821400	underlying reality that you can refer to to fill in those blanks. So you cannot say for sure in
821400	826920	the savannah, if it's a lion or a cheetah or whatever, you cannot know if it's a zebra or
827560	836680	a goo or whatever. We're the beast, the same thing. But you can represent the uncertainty
836680	842920	by just a long list of numbers. Now, if I do the same thing with video and I ask you to predict
842920	847960	a video clip, it's not a discrete set of potential frames. You have to have
848760	853400	somewhere representing a sort of infinite number of plausible continuations
853400	858360	of multiple frames in a high-dimensional, continuous space. And we just have no idea
858360	865320	how to do this properly. Finite high-dimensional. It's finite high-dimensional, yes.
865320	873320	Just like the words. They try to get it down to a small finite set of like under a million,
873320	878920	something like that. I mean, it's kind of ridiculous that we're doing a distribution
878920	884280	over every single possible word for language, and it works. It feels like that's a really dumb
884280	893080	way to do it. It seems to be like there should be some more compressed representation of the
893080	898840	distribution of the words. You're right about that. I agree. Do you have any interesting ideas
898840	903320	about how to represent all the reality in a compressed way such that you can form a distribution
903320	909320	over it? That's one of the big questions. How do you do that? I mean, another thing that really is
910600	915560	stupid about, I shouldn't say stupid, but simplistic about current approaches to
915560	922840	self-supervisioning in NLP in text is that not only do you represent a giant distribution over
922840	927560	words, but for multiple words that are missing, those distributions are essentially independent
927560	938200	of each other. You don't pay too much of a price for this. The system, in the sentence that I gave
938200	944120	earlier, if it gives a certain probability for a lion and cheetah, and then a certain
944120	954360	probability for gazelle, wildebeest, and zebra, those two probabilities are independent of each
954360	959960	other. It's not the case that those things are independent lions actually attack bigger animals
959960	967640	than cheetahs. There's a huge independent hypothesis in this process which is not actually true.
967640	973080	The reason for this is that we don't know how to represent properly distributions over
973080	979080	combinatorial sequences of symbols, essentially, because the number grows exponentially with the
979080	987160	length of the symbols. We have to use tricks for this, but those techniques don't even deal with
987160	994840	it. The big question is, would there be some sort of abstract latent representation of text
995480	1004440	that would say that when I switch lion for cheetah, I also have to switch zebra for gazelle.
1005160	1011000	Yeah, so this independence assumption, let me throw some criticism at you that I often hear
1011000	1016520	and see how you respond. This kind of feeling in the blanks is just statistics. You're not
1016520	1023720	learning anything, like the deep underlying concepts. You're just mimicking stuff from
1025000	1030440	the past. You're not learning anything new such that you can use it to generalize about the world.
1031400	1037160	Okay, let me just say the crude version, which is just statistics. It's not intelligence.
1038200	1042520	What do you have to say to that? What do you usually say to that if you hear this kind of thing?
1042520	1045640	I don't get into those discussions because they are kind of pointless.
1047240	1050760	First of all, it's quite possible that intelligence is just statistics. It's just
1050760	1058440	statistics of a particular kind. This is the philosophical question. Is it possible that
1058440	1065800	intelligence is just statistics? Yeah, but what kind of statistics? So if you're asking the question,
1067080	1072520	are the models of the world that we learn, do they have some notion of causality? Yes.
1073320	1078680	So if the criticism comes from people who say a current machine non-existent don't care about
1078680	1085640	causality, which by the way is wrong, I agree with that. Your model of the world should have
1085720	1091320	your actions as one of the inputs and that will drive you to learn causal models of the world
1091320	1097400	where you know what intervention in the world will cause, what results, or you can do this by
1097400	1103400	observation of other agents acting in the world and observing the effect of other humans, for
1103400	1112120	example. So I think at some level of description, intelligence is just statistics, but that doesn't
1112120	1119000	mean you won't have models that have deep mechanistic explanation for what goes on.
1119880	1126200	The question is how do you learn them? That's the question I'm interested in. Because a lot of people
1126200	1131880	who actually voice their criticism say that those mechanistic models have to come from
1131880	1135400	someplace else. They have to come from human designers. They have to come from, I don't know
1135400	1142120	what, and obviously we learn them. Or if we don't learn them as an individual, nature
1143080	1148040	learned them for us using evolution. So regardless of what you think, those processes have been
1148040	1154760	learned somehow. So if you look at the human brain, just like when we humans introspect about
1154760	1161480	how the brain works, it seems like when we think about what is intelligence, we think about the
1161480	1166280	high level stuff, like the models we've constructed, concepts like cognitive science, like concepts of
1166280	1173880	memory and reasoning module, almost like these high level modules. Is this serve as a good analogy?
1175320	1184520	Like, are we ignoring the dark matter, the basic low level mechanisms, just like we ignore the way
1184520	1190440	the operating system works, we're just using the high level software. We're ignoring that
1191160	1196920	at the low level, the neural network might be doing something like statistics. Like,
1196920	1201640	meaning, sorry to use this word, probably incorrectly and crudely, but doing this kind
1201640	1206280	of fill in the gap kind of learning and just kind of updating the model constantly in order to be
1206280	1211480	able to support the raw sensory information, to predict it and then adjust to the prediction when
1211480	1217160	it's wrong. But like when we look at our brain at the high level, it feels like we're doing,
1217160	1222440	like we're playing chess, like we're, we're like playing with high level concepts and we're
1222440	1227160	stitching them together and we're putting them into long term memory. But really what's going
1227160	1233640	underneath is something we're not able to introspect, which is this kind of simple large
1233640	1238120	neural network that's just filling in the gaps. Right. Well, okay, so there's a lot of questions
1238120	1242600	that are answers there. Okay, so first of all, there's a whole school of thought in neuroscience,
1242600	1247720	competition on neuroscience in particular, that likes the idea of predictive coding,
1247720	1252200	which is really related to the idea I was talking about in self supervised running. So
1252200	1255640	everything is about prediction. The essence of intelligence is the ability to predict.
1256280	1261240	And everything the brain does is trying to predict, predict everything from everything
1261240	1265400	else. Okay, and that's really sort of the underlying principle if you want that
1266920	1271080	self supervised learning is trying to kind of reproduce this idea of prediction as kind of an
1271080	1278760	essential mechanism of task independent learning if you want. The next step is what kind of
1278760	1283400	intelligence are you interested in reproducing? And of course, you know, we all think about,
1283400	1288280	you know, trying to reproduce sort of, you know, high level cognitive processes in humans.
1288280	1292520	But like with machines, we're not even at the level of even reproducing the
1293240	1299000	running processes in a cat brain. You know, the most intelligent or intelligent systems
1299320	1305640	don't have as much common sense as a house cat. So how is it that cats learn? And, you know,
1305640	1310280	cats don't do a whole lot of reasoning. They certainly have causal models. They certainly have,
1311480	1315400	because, you know, many cats can figure out like how they can act on the world to get what they want.
1316520	1324120	They certainly have a fantastic model of intuitive physics, certainly the dynamics of their own
1324120	1330520	bodies, but also praise and things like that. Right. So they're pretty smart. They only do
1330520	1337880	this with about 800 million neurons. We are not anywhere close to reproducing this kind of thing.
1337880	1344600	So to some extent, I could say, let's not even worry about like the high level cognition
1346200	1349400	and kind of, you know, long term planning and reasoning that humans can do until we figure
1349400	1354760	out like, you know, can we even reproduce what cats are doing? Now that said, this ability to
1355640	1362040	learn world models, I think is the key to the possibility of running machines that can also
1362040	1366680	reason. So whenever I give a talk, I say there are three challenges in the three main challenges
1366680	1370760	in machine learning. The first one is, you know, getting machines to learn to represent the world
1371720	1378280	and proposing self supervised learning. The second is getting machines to reason in ways
1378280	1382760	that are compatible with essentially gradient based learning because this is what deep learning is
1382760	1388200	all about really. And the third one is something we have no idea how to solve. At least I have no
1388200	1395640	idea how to solve is can we get machines to learn hierarchical representations of action plans?
1396920	1400280	You know, like, you know, we know how to train them to learn hierarchical representations of
1400280	1405160	perception, you know, with convolutional nets and things like that and transformers. But what
1405160	1409960	about action plans? Can we get them to spontaneously learn good hierarchical representations of
1409960	1415800	actions also gradient based? Yeah, all of that, you know, needs to be somewhat differentiable
1415800	1419880	so that you can apply sort of gradient based learning, which is really what deep learning is
1419880	1430680	about. So it's background knowledge, ability to reason in a way that's differentiable that
1431640	1436200	is somehow connected deeply integrated with that background knowledge or builds on top of
1436200	1440360	that background knowledge. And then giving that background knowledge be able to make hierarchical
1440360	1445800	plans right in the world. So if you take classical optimal control, there's something
1445800	1451560	classical optimal control called model predictive control. And it's, you know, it's been around
1451560	1457560	since the early 60s. NASA uses that to compute trajectories of rockets. And the basic idea is
1457640	1463800	that you have a predictive model of the rocket, let's say, or whatever system you are, you intend
1463800	1470840	to control, which given the state of the system at time t and given an action that you're taking
1470840	1476840	the system. So for a rocket to be thrust and, you know, all the controls you can have, it gives
1476840	1480840	you the state of the system at time t plus delta t, right? So basically differential equation,
1480920	1488360	something like that. And if you have this model, and you have this model in the form of some sort
1488360	1493480	of neural net, or some sort of set of formula that you can back propagate gradient through,
1493480	1498280	you can do what's called model predictive control, or gradient based model predictive control.
1498280	1509880	So you have, you can enroll that model in time, you feel it a hypothesized sequence of actions.
1511000	1515960	And then you have some objective function that measures how well at the end of the trajectory
1515960	1520920	of the system has succeeded or matched what you wanted to do. You know, is it a robot harm,
1520920	1525960	as if you grasp the object you want to grasp, if it's a rocket, you know, are you at the right
1525960	1531080	place near the space station, things like that. And by back propagation through time, and again,
1531080	1537960	this was invented in the 1960s by optimal control theorists, you can figure out what is the optimal
1537960	1546920	sequence of actions that will, you know, get my system to the best final state. So that's a form
1546920	1551560	of reasoning. It's basically planning. And a lot of planning systems in robotics are actually based
1551560	1558520	on this. And, and you can think of this as a form of reasoning. So, you know, to take the example
1558520	1563080	of the teenager driving a car again, you have a pretty good dynamical model of the car, it doesn't
1563080	1567560	need to be very accurate. But you know, again, that if you turn the wheel to the right, and there
1567560	1570520	is a cliff, you're going to run off the cliff, right, you don't need to have a very accurate
1570520	1575400	model to predict that. And you can run this in your mind, and decide not to do it for that reason.
1575960	1579240	Because you can predict in advance that the result is going to be bad. So you can sort of
1579240	1585160	imagine different scenarios, and, and then, you know, employ, or take the first step in the scenario
1585160	1589160	that is most favorable, and then repeat the process of planning that's called receding horizon
1589160	1593640	model predictive control. So even, you know, all those things have names, you know, going back,
1593640	1599880	you know, decades. And so, if you're not not the, you know, in classical optimal control,
1599880	1604840	the model of the world is not generally learned. This, you know, sometimes a few parameters you
1604840	1610120	have to identify that's called systems identification. But, but generally, the model is
1610920	1616760	mostly deterministic and mostly built by hand. So the big question of AI, I think the big challenge
1616760	1621800	of AI for the next decade is how do we get machines to learn predictive models of the world
1621800	1626280	that deal with uncertainty, and deal with the real world in all this complexity. So it's not
1626280	1631160	just trajectory of a rocket, which you can reduce to first principles, it's not, it's not even just
1631160	1636120	a trajectory of a robot arm, which again, you can model by, you know, careful mathematics.
1636120	1639640	But it's everything else, everything we observe in the world, you know, people, behavior,
1641640	1647720	you know, physical systems that involve collective phenomena, like water or, or, you know,
1649000	1654760	trees and, you know, branches in a tree or something, or, or like complex things that,
1654760	1659640	you know, humans have no trouble developing abstract representations and predictive model for,
1659640	1661560	but we still don't know how to deal with machines.
1661560	1665720	Where do you put in, in these three, maybe in the, in the planning stages,
1667240	1673960	the, the game theoretic nature of this world, where your actions not only respond to the dynamic
1673960	1679720	nature of the world, the environment, but also affect it. So if there's other humans involved,
1679720	1684440	is this, is this point number four, or is it somehow integrated into the hierarchical
1684440	1689400	representation of action in your view? I think it's integrated. It's just, it's just that now
1689400	1693000	your model of the world has to deal with, you know, it just makes it more complicated, right?
1693000	1698120	The fact that humans are complicated and not easily predictable, that makes your model of
1698120	1700840	the world much more complicated, that much more complicated.
1700840	1707240	Well, there's a chess, I mean, I suppose chess is an analogy. So multi-carvel tree search.
1707960	1714840	I mean, it, there's a, I go, you go, I go, you go, like, under Kapatha recently gave a talk
1714840	1720840	at MIT about car doors. I think there's some machine learning too, but mostly car doors.
1720840	1725000	And there's a dynamic nature to the car, like the person opening the door, checking,
1725560	1728760	I mean, he wasn't talking about that. He was talking about the perception problem of what
1728760	1733320	the, the ontology of what defines a car door, this big philosophical question. But to me,
1733320	1737400	it was interesting because like, it's obvious that the person opening the car doors, they're
1737400	1742680	trying to get out like here in New York, trying to get out of the car, you slowing down is going
1742680	1746840	to signal something, you speeding up is going to signal something. And that's a dance. It's a
1747640	1757080	asynchronous chess game. I don't know. So it feels like it's not just, I mean,
1757080	1761480	I guess you can integrate all of them to one giant model, like the entirety of the,
1762680	1766440	these little interactions, because it's not as complicated as chess, it's just like a little
1766440	1769880	dance. We do like a little dance together. And then we figure it out.
1769880	1774920	Well, in some ways, it's way more complicated than chess because, because it's continuous,
1774920	1779720	it's uncertain in a continuous manner. It doesn't feel more complicated,
1779720	1783560	but it doesn't feel more complicated because that's what we're, we've evolved to solve.
1783560	1787080	This is the kind of problem we've evolved to solve. And so we're good at it because, you know,
1787640	1793880	nature has made us good at it. Nature has not made us good at chess. We completely suck at chess.
1794440	1801400	In fact, that's why we designed it as a game is to be challenging. And if there is something that,
1801400	1806520	you know, recent progress in chess and Go has made us realize is that humans are
1806520	1811400	really terrible at those things, like really bad. You know, there was a story, right, before Alpha Go
1811400	1817240	that, you know, the best Go player thought there were maybe two or three stones behind,
1817240	1822120	you know, an ideal player that they would call God. In fact, no, there are like nine or 10
1822680	1829000	stones behind. I mean, we're just bad. So we're not good at, and it's because we have limited
1829000	1833000	working memory. We know we're not very good at like doing this tree exploration that,
1833000	1838360	you know, computers are much better at doing than we are, but we are much better at learning
1838360	1843640	differentiable models to the world. I mean, I said differentiable in the kind of, you know,
1843640	1847320	I should say not differentiable in the sense that, you know, we run back from through it,
1847400	1853160	but in the sense that our brain has some mechanism for estimating gradients of some kind.
1853960	1860040	And that's what, you know, makes us efficient. So if you have an agent that consists of
1861240	1865720	a model of the world, which, you know, in the human brain is basically the entire front half of
1865720	1875160	your brain, an objective function, which in humans is a combination of two things. There is
1875240	1879000	your sort of intrinsic motivation module, which is in the basal ganglia, you know,
1879000	1883080	the base of your brain. That's the thing that measures pain and hunger and things like that,
1883080	1891080	like immediate feelings and emotions. And then there is, you know, the equivalent of what people
1891080	1896840	in Reference Metronomy call a critic, which is a sort of module that predicts ahead what the outcome
1897800	1904920	of a situation will be. And so it's not a cost function, but it's sort of not an objective
1904920	1910760	function, but it's sort of a, you know, trained predictor of the ultimate objective function.
1910760	1915640	And that also is differentiable. And so if all of this is differentiable, your cost function,
1915640	1923320	your critic, your, you know, your world model, then you can use gradient based type methods to do
1923320	1928200	planning, to do reasoning, to do learning, you know, to do all the things that would like an
1928200	1935560	intelligent agent to do. And gradient based learning, like what's your intuition, that's
1935560	1943640	probably at the core of what can solve intelligence. So you don't need like a logic based reasoning
1944520	1947960	in your view. I don't know how to make logic based reasoning compatible with
1948760	1953720	efficient learning. And okay, I mean, there is a big question, perhaps a philosophical question.
1953720	1958680	I mean, it's not that philosophical, but that we can ask is, is that, you know, all the learning
1959320	1965080	algorithms we know from engineering and computer science, proceed by optimizing some objective
1965080	1973000	function? Yeah, right. So one question we may ask is, is those learning in the brain minimize
1973000	1978360	an objective function? I mean, it could be, you know, a composite of multiple objective functions,
1978360	1984760	but it's still an objective function. Second, if it does optimize an objective function,
1984760	1990360	does it do, does it do it by some sort of gradient estimation? You know, it doesn't need to be
1990360	1995560	backdrop, but you know, some way of estimating the gradient in efficient manner, whose complexity
1995560	2000520	is on the same order of magnitude as, you know, actually running the inference.
2001480	2006520	Because you can't afford to do things like, you know, perturbing a weight in your brain to
2006520	2011480	figure out what the effect is, and then sort of, you know, you can do sort of estimating gradient
2011480	2017240	by perturbation. It's, to me, it seems very implausible that the brain uses some sort of,
2018840	2024600	you know, zero-thorough black box gradient free optimization, because it's so much less
2024600	2028120	efficient than gradient optimization. So it has to have a way of estimating gradient.
2029080	2035320	Is it possible that some kind of logic-based reasoning emerges in pockets as a useful,
2035320	2039560	like you said, if the brain is an objective function? Maybe it's a mechanism for creating
2039560	2045640	objective functions. It's a mechanism for creating knowledge bases, for example,
2046360	2051000	that can then be quarried. Like maybe it's an efficient representation of knowledge that's
2051000	2053560	learned in a gradient-based way or something like that.
2053560	2057880	Well, so I think there is a lot of different types of intelligence. So first of all,
2057960	2063720	I think the type of logical reasoning that we think about that we are, you know, maybe stemming
2063720	2071400	from, you know, sort of classical AI of the 1970s and 80s, I think humans use that relatively
2071400	2074520	rarely and are not particularly good at it.
2074520	2081560	But we judge each other based on our ability to solve those rare problems called IQ tests.
2081560	2084280	I think so. Like, I'm not very good at chess.
2085160	2089640	Yes, I'm judging you this whole time because, well, we actually...
2089640	2092680	With your, you know, heritage, I'm sure you're good at chess.
2093400	2096440	No, stereotypes. Not all stereotypes are true.
2097880	2104600	Well, I'm terrible at chess. So, you know, but I think perhaps another type of intelligence
2104600	2109720	that I have is this, you know, ability of sort of building models to the world from,
2110680	2114920	you know, reasoning, obviously, but also data.
2115800	2121480	And those models generally are more kind of analogical, right? So it's reasoning by simulation
2122200	2126760	and by analogy, where you use one model to apply to a new situation.
2126760	2131640	Even though you've never seen that situation, you can sort of connect it to a situation you've
2131640	2138360	encountered before. And your reasoning is more akin to some sort of internal simulation.
2138360	2142120	So you're kind of simulating what's happening when you're building, I don't know,
2142120	2145720	a box out of wood or something, right? You're going to imagine in advance,
2146280	2149480	like, will we be the result of, you know, cutting the wood in this particular way?
2149480	2151480	Are you going to use, you know, screws on nails or whatever?
2152760	2156920	When you are interacting with someone, you also have a model of that person and sort of
2156920	2163560	interact with that person. You know, having this model in mind to kind of tell the person
2163720	2170200	what you think is useful to them. So I think this ability to construct models to the world
2170200	2176600	is basically the essence of intelligence. And the ability to use it then to plan
2177400	2185320	actions that will fulfill a particular criterion, of course, is necessary as well.
2185320	2190120	So I'm going to ask you a series of impossible questions as we keep asking, as I've been doing.
2190120	2195160	So if that's the fundamental sort of dark matter of intelligence, this ability to form
2195160	2200280	a background model, what's your intuition about how much knowledge is required?
2201320	2206040	You know, I think dark matter, you could put a percentage on it of
2208680	2212600	the composition of the universe and how much of it is dark matter, how much of it is dark energy,
2212600	2221080	how much information do you think is required to be a house cat? So you have to be able to,
2221080	2227000	when you see a box going, when you see a human compute the most evil action, if there's a thing
2227000	2232680	that's near an edge, you knock it off, all of that, plus the extra stuff you mentioned,
2232680	2237800	which is a great self-awareness of the physics of your own body and the world.
2238600	2240840	How much knowledge is required, do you think, to solve it?
2241800	2245480	I don't even know how to measure an answer to that question.
2245480	2250760	I'm not sure how to measure it, but whatever it is, it fits in about 800,000 neurons,
2252280	2253640	800 million neurons.
2253640	2255000	The representation does.
2256280	2258120	Everything, all knowledge, everything, right?
2259880	2264120	You know, it's less than a billion, a dog is 2 billion, but a cat is less than 1 billion.
2265320	2269240	And so multiply that by a thousand and you get the number of synapses.
2270200	2275880	And I think almost all of it is learned through this, you know, a sort of self-supervised running,
2275880	2279800	although, you know, I think a tiny sliver is learned through reinforcement running,
2279800	2283560	and certainly very little through, you know, classical supervised running, although it's
2283560	2287880	not even clear how supervised running actually works in the biological world.
2289160	2296680	So I think almost all of it is self-supervised running, but it's driven by the sort of
2296680	2301320	ingrained objective functions that a cat or human have at the base of their brain,
2301320	2304840	which kind of drives their behavior.
2304840	2308280	So, you know, nature tells us you're hungry.
2309320	2311800	It doesn't tell us how to feed ourselves.
2311800	2314520	That's something that the rest of our brain has to figure out, right?
2315640	2319720	What's interesting, because there might be more like deeper objective functions than
2319720	2320680	allowing the whole thing.
2321240	2327160	So hunger may be some kind of, now you go to like neurobiology, it might be just the brain
2329960	2331560	trying to maintain homeostasis.
2332360	2339480	So hunger is just one of the human perceivable symptoms of the brain being unhappy with the
2339480	2340520	way things are currently.
2341320	2344760	It could be just like one really dumb objective function at the core.
2344760	2347560	But that's how behavior is driven.
2348440	2351080	The fact that, you know, the, or basal ganglia
2352360	2357240	drivers to do things that are different from say an orangutan or certainly a cat
2358120	2362280	is what makes, you know, human nature versus orangutan nature versus cat nature.
2363240	2371320	So for example, you know, our basal ganglia drives us to seek the company of other humans.
2372120	2377080	And that's because nature has figured out that we need to be social animals for our species to
2377080	2383080	survive. And it's true of many primates. It's not true of orangutans, orangutans are
2383080	2387880	solitary animals. They don't seek the company of others. In fact, they avoid them.
2389240	2391960	In fact, they scream at them when they come too close because they're territorial.
2392600	2397400	Because for their survival, you know, evolution has figured out that's the best thing.
2398120	2400760	I mean, they're occasionally social, of course, for, you know,
2402120	2405880	reproduction and stuff like that. But they're mostly solitary.
2406840	2410360	So all of those behaviors are not part of intelligence. You know, people say, oh,
2410360	2413800	you're never going to have intelligent machines because, you know, human intelligence is social.
2413800	2417960	But then you look at orangutans, you look at octopus. Octopus never know their parents.
2418680	2423800	They barely interact with any other and they get to be really smart in less than a year,
2423800	2428760	in like half a year. You know, in a year or they're adults, in two years they're dead.
2428760	2435720	So there are things that we think, as humans, are intimately linked with intelligence,
2435720	2442280	like social interaction, like language. We think, I think we give way too much
2442280	2445880	importance to language as a substrate of intelligence as humans,
2446680	2449800	because we think our reasoning is so linked with language.
2449800	2455080	So for, to solve the house cat intelligence problem, you think you could do it on a desert
2455080	2463080	island. You could have a cat sitting there, looking at the waves, at the ocean waves,
2463080	2468600	and figure a lot of it out. It needs to have sort of, you know, the right set of drives
2470040	2474440	to kind of, you know, get it to do the thing and learn the appropriate things, right? But
2476120	2479560	like, for example, you know, baby humans are driven to
2480520	2486360	learn to stand up and walk. Okay, that's kind of, this desire is hardwired. How to do it
2486360	2491480	precisely is not, that's learned. But the desire to, to walk, move around and stand up,
2492760	2497400	that's sort of hardwired. It's very simple to hardwire this kind of stuff.
2498840	2503960	Oh, like the desire to, well, that's interesting. You're hardwired to want to walk.
2504360	2511960	That's not a, there's got to be a deeper need for walking. I think it was probably socially
2511960	2517080	imposed by society that you need to walk all the other bipedal. No, no, like a lot of simple
2517080	2522840	animals that, you know, would probably walk without ever watching any other members of the
2522840	2529160	species. It seems like a scary thing to have to do because you suck at bipedal walking at first.
2529160	2534840	It seems crawling is much safer, much more like, why are you in a hurry?
2535560	2538840	Well, because, because you have this thing that drives you to do it, you know,
2540200	2544920	which is sort of part of the sort of human development.
2544920	2546600	Is that understood actually what?
2546600	2547640	Not entirely, no.
2547640	2551800	What is, what's the reason to get on two feet? It's really hard. Like most animals don't get on
2551800	2552760	two feet. Why not?
2552760	2555720	Well, they get on four feet. You know, many mammals get on four feet.
2555720	2556040	Yeah, they do.
2556040	2557800	Very quickly. Some of them extremely quickly.
2558440	2562520	But I don't, you know, like from the last time I've interacted with a table,
2562520	2566360	that's much more stable than a thing on two legs. It's just a really hard problem.
2566360	2568360	Yeah, I mean birds have figured it out with two feet.
2569240	2574280	Well, technically, we can go into ontology. They have four, I guess they have two feet.
2574280	2575000	They have two feet.
2575000	2575480	Chickens.
2576280	2578760	You know, dinosaurs have two feet, many of them.
2578760	2579400	Allegedly.
2581400	2585320	I'm just now learning that T-Rex was eating grass, not other animals.
2585320	2591080	T-Rex might have been a friendly pet. What do you think about, I don't know if you looked at
2592440	2596200	the test for general intelligence that Francois Chalet put together?
2596200	2598600	I don't know if you got a chance to look at that kind of thing.
2598600	2603640	Like what's your intuition about how to solve like an IQ type of test?
2603640	2607320	I don't know. I think it's so outside of my radar screen that it's not really
2608120	2609880	relevant, I think in the short term.
2610600	2615720	Well, I guess one way to ask another way, perhaps more closer to what
2616600	2622600	to your work is like, how do you solve MNIST with very little example data?
2622600	2625800	That's right. And that's the answer to this probably is self-supervised running.
2625800	2631240	Just learn to represent images and then learning, you know, to recognize handwritten digits on top
2631240	2637160	of this will only require a few samples. And we observe this in humans, right? You show a
2637160	2641080	young child a picture book with a couple of pictures of an elephant and that's it.
2641800	2647000	The child knows what an elephant is. And we see this today with practical systems that we
2647960	2655640	train image recognition systems with enormous amounts of images either completely self-supervised
2655640	2661960	or very weakly supervised. For example, you can train a neural net to predict whatever
2661960	2665720	hashtag people type on Instagram, right? Then you can do this with billions of images because
2665800	2671080	it's billions per day that are showing up. So the amount of training data there is essentially
2671080	2676760	unlimited. And then you take the output representation, you know, a couple layers down from the outputs
2677560	2683080	of what the system learned and feed this as input to a classifier for any object in the
2683080	2687880	world that you want and it works pretty well. So that's transfer learning, okay? Or weekly
2687880	2694280	supervised transfer learning. People are making very, very fast progress using self-supervised
2694280	2702360	learning with this kind of scenario as well. And my guess is that that's going to be the future.
2702360	2708280	For self-supervised learning, how much cleaning do you think is needed for filtering
2710600	2715480	malicious signal or with a better term? But a lot of people use hashtags on Instagram
2716680	2722200	to get good SEO that doesn't fully represent the contents of the image.
2722920	2728440	Like they'll put a picture of a cat and hashtag it with like science, awesome, fun, I don't know,
2728440	2732200	all kind of... Why would you put science? That's not very good SEO.
2732200	2738280	The way my colleagues who worked on this project at Facebook now, META, META AI,
2738840	2743640	a few years ago, they helped with this is that they only selected something like 17,000 tags
2743640	2749640	that correspond to kind of physical things or situations. Like, you know, that has some visual
2749640	2756440	content. So, you wouldn't have like hash TBT or anything like that.
2757000	2761160	Also, they keep a very select set of hashtags. Is that what you're saying?
2761160	2762040	Yeah. Okay.
2762040	2767800	But it's still on the order of 10 to 20,000. So it's fairly large.
2767800	2773960	Okay. Can you tell me about data augmentation? What the heck is data augmentation? And how is it
2773960	2780120	used maybe contrast of learning for video? What are some cool ideas here?
2780760	2784680	Right. So data augmentation, I mean, first, data augmentation, you know, is the idea of
2784680	2790280	artificially increasing the size of your training set by distorting the images that you have in ways
2790280	2794360	that don't change the nature of the image. Right? So you take... You do MNIST, you can do data
2794360	2799560	augmentation on MNIST, and people have done this since the 1990s, right? You take a MNIST digit
2799640	2806840	and you shift it a little bit or you change the size or rotate it, skew it, you know, etc.
2806840	2807560	Add noise.
2808200	2813400	Add noise, etc. And it works better. If you train a supervised classifier with augmented data,
2813400	2819960	you're going to get better results. Now, it's become really interesting over the last couple
2819960	2826440	years because a lot of self-supervised learning techniques to pre-train vision systems are based
2826440	2834840	on data augmentation. And the basic techniques is originally inspired by techniques that I worked
2834840	2838600	on in the early 90s and Jeff Hinton worked on also in the early 90s. They were sort of parallel
2839400	2845000	work. I used to call this Siamese network. So basically, you take two identical copies of
2845000	2851720	the same network, they share the same weights, and you show two different views of the same object.
2851720	2855320	Either those two different views may have been obtained by data augmentation,
2855320	2859240	or maybe it's two different views of the same scene from a camera that you moved
2859240	2863160	or at different times or something like that, right? Or two pictures of the same person,
2863160	2868360	things like that. And then you train this neural net, those two identical copies of this neural net,
2868360	2874440	to produce an output representation, a vector, in such a way that the representation for those two
2875400	2880840	images are as close to each other as possible, as identical to each other as possible, right?
2880840	2886040	Because you want the system to basically learn a function that will be invariant,
2886040	2890120	that will not change, whose output will not change when you transform those inputs
2890840	2897720	in those particular ways, right? So that's easy to do. What's complicated is how do you make sure
2897720	2902520	that when you show two images that are different, the system will produce different things. Because
2902520	2907640	if you don't have a specific provision for this, the system will just ignore the inputs
2908360	2911880	when you train it, it will end up ignoring the input and just produce a constant vector that
2911880	2916600	is the same for every input, right? That's called a collapse. Now, how do you avoid collapse?
2916600	2923000	So there's two ideas. One idea that I proposed in the early 90s with my colleagues at Bell Labs,
2923000	2928200	Gene Bromley and a couple other people, which we now call contrastive learning,
2928200	2933000	which is to have negative examples, right? So you have pairs of images that you know are different,
2933960	2939000	and you show them to the network and those two copies, and then you push the two output vectors
2939000	2943720	away from each other, and they will eventually guarantee that things that are symmetrically
2943720	2947400	similar produce similar representations and things that are different produce different
2947400	2953240	representations. We actually came up with this idea for a project of doing signature
2953240	2959560	verification. So we would collect signatures from multiple signatures on the same person
2960120	2963560	and then train a neural net to produce the same representation, and then
2965720	2969400	force the system to produce different representation for different signatures.
2970840	2976280	This was actually the problem was proposed by people from what was a subsidiary of AT&T at
2976280	2982680	the time called NCR, and they were interested in storing representation of the signature on the 80
2982680	2988520	bytes of the magnetic strip of a credit card. So we came up with this idea of having a neural
2988520	2993800	net with 80 outputs that we would quantize on bytes so that we could encode the...
2993800	2997000	And that encoding was then used to compare whether the signature matches or not?
2997000	3001000	That's right. So then you would sign, it would run through the neural net, and then you would
3001000	3003400	compare the output vector to whatever is stored on your card.
3003400	3004040	Did it actually work?
3004600	3010520	It worked, but they ended up not using it because nobody cares actually. I mean, the
3011480	3017480	American financial payment system is incredibly lax in that respect compared to Europe.
3017560	3020520	Over the signatures? What's the purpose of the signatures anyway?
3020520	3022520	Nobody looks at them, nobody cares.
3025960	3029400	So that's contrastive learning, right? So you need positive and negative pairs,
3029400	3034040	and the problem with that is that even though I had the original paper on this,
3034680	3038600	I'm actually not very positive about it because it doesn't work in high dimension.
3038600	3043000	If your representation is high dimensional, there's just too many ways for two things to be
3043000	3047320	different. And so you would need lots and lots and lots of negative pairs.
3048120	3053480	So there is a particular implementation of this, which is relatively recent, from actually the
3053480	3059640	Google Toronto group, where Jeff Hinton is the senior member there, and it's called Simclear,
3059640	3065400	S-I-M-C-L-R. And it's basically a particular way of implementing this idea of
3065400	3067960	contrastive learning, the particular objective function.
3068600	3074840	Now, what I'm much more enthusiastic about these days is non-contrastive methods. So
3074840	3083000	other ways to guarantee that the representations would be different for different inputs.
3084280	3089880	And it's actually based on an idea that Jeff Hinton proposed in the early 90s with his
3089880	3093720	student at the time, Sue Becker. And it's based on the idea of maximizing the mutual
3093720	3097320	information between the outputs of the two systems. You only show positive pairs,
3097320	3102200	you only show pairs of images that you know are somewhat similar. And you're trying the two
3102200	3110120	networks to be informative, but also to be as informative of each other as possible. So basically,
3110120	3117080	one representation has to be predictable from the other, essentially. And he proposed that idea
3117080	3122920	had a couple of papers in the early 90s, and then nothing was done about it for decades.
3123000	3129000	And I kind of revived this idea together with my postdocs at FAIR, particularly a postdoc called
3129000	3135960	Steph Anthony, who's now a junior professor in Finland at University of Alto. We came up with
3135960	3141560	something called, that we call Balu twins. And it's a particular way of maximizing the
3141560	3150040	information content of a vector, you know, using some hypothesis. And we have kind of
3151000	3155640	another version of it that's more recent now called Vicreg, V-I-C-A-R-E-G, that means
3155640	3159880	variance invariance covariance regularization. And it's the thing I'm the most excited about
3159880	3164360	in machine learning in the last 15 years. I mean, I'm not, I'm really, really excited about this.
3164360	3169240	What kind of data augmentation is useful for that non-contrasting learning method?
3170120	3175160	Are we talking about, does that not matter that much? Or it seems like a very important part of
3175160	3179480	the step. Yeah. How you generate the images that are similar, but sufficiently different.
3179480	3183000	Yeah, that's right. It's an important step. And it's also an annoying step because you need to
3183000	3189320	have that knowledge of what the documentation you can do that do not change the nature of the
3189320	3196360	object. And so the standard scenario, which a lot of people working in this area are using, is you
3196360	3203320	use the type of distortion. So basically you do geometric distortion. So one basically just shifts
3203320	3207640	the image a little bit, it's called cropping. Another one kind of changes the scale a little bit.
3207720	3212280	Another one kind of rotates it. Another one changes the colors. You know, you can do a shift in
3212280	3217720	color balance or something like that. Saturation. Another one sort of blurs it. Another one adds
3217720	3222360	noise. So you have like a catalog of kind of standard things and people try to use the same
3222360	3227800	ones for different algorithms so that they can compare. But some algorithms, some self-supervised
3227800	3233160	algorithm actually can deal with much bigger, like more aggressive data augmentation and some
3233160	3237880	don't. So that kind of makes the whole thing difficult. But that's the kind of distortions
3237880	3247240	we're talking about. And so you train with those distortions. And then you chop off the last layer
3247240	3253640	or couple layers of the network. And you use the representation as input to a classifier. You
3253640	3260760	train the classifier on ImageNet, let's say, or whatever, and measure the performance. And
3261640	3265960	interestingly enough, the methods that are really good at eliminating the information that is
3265960	3271320	irrelevant, which is the distortions between those images, do a good job at eliminating it.
3272360	3277800	And as a consequence, you cannot use the representations in those systems for things
3277800	3283400	like object detection and localization because that information is gone. So the type of
3283400	3288600	data augmentation you need to do depends on the tasks you want eventually the system to solve.
3288600	3293480	And the type of standard data augmentation that we use today are only appropriate for
3293480	3297640	object recognition or image classification. They're not appropriate for things like...
3297640	3302040	Can you help me out understand why the localization... So you're saying it's just not good at the
3302040	3307800	negative, like at classifying the negative. So that's why it can't be used for the localization?
3307800	3313560	No, it's just that you train the system, you give it an image and then you give it the same image
3313560	3319000	shifted and scaled, and you tell it that's the same image. So the system basically is trained
3319000	3324200	to eliminate the information about position and size. So now, and now you want to use that
3325000	3329320	to figure out where an object is and what size it is. Like a bounding box, like they'd be able to
3329320	3335880	actually... Okay, it can still find the object in the image. It's just not very good at finding
3335880	3342280	the exact boundaries of that object. Interesting. Which, you know, that's an interesting sort of
3342360	3348520	philosophical question. How important is object localization anyway? We're like obsessed by
3348520	3354120	measuring like image segmentation, obsessed by measuring perfectly knowing the boundaries of
3354120	3363720	objects when arguably that's not that essential to understanding what are the contents of the scene.
3363720	3368680	On the other hand, I think evolutionarily, the first vision systems in animals were basically
3368680	3374360	all about localization, very little about recognition. And in the human brain, you have two
3374360	3382840	separate pathways for recognizing the nature of a scene or an object and localizing objects. So
3382840	3387880	you use the first pathway called the ventral pathway for telling what you're looking at.
3389000	3394040	The other pathway, the dorsal pathway is used for navigation, for grasping, for everything else.
3394040	3399480	And basically, a lot of the things you need for survival are localization and detection.
3401800	3407000	Is similarity learning or contrastive learning or these non-contrastive methods the same as
3407000	3412680	understanding something? Just because you know a distorted cat is the same as a non-distorted cat,
3412680	3415800	does that mean you understand what it means to be a cat?
3416600	3419960	To some extent. I mean, it's a superficial understanding, obviously.
3419960	3423160	But what is the ceiling of this method, do you think? Is this just one
3423880	3429960	trick on the path to doing self-supervised learning or can we go really, really far?
3429960	3436600	I think we can go really far. So if we figure out how to use techniques of that type, perhaps
3436600	3443320	very different, but of the same nature, to train a system from video to do video prediction,
3443320	3452840	essentially. I think we'll have a path towards, I wouldn't say unlimited, but a path towards
3452840	3464520	some level of physical common sense in machines. And I also think that that ability to learn how
3464600	3472360	the world works from a high throughput channel like vision is a necessary step towards
3474040	3478520	real artificial intelligence. In other words, I believe in grounded intelligence. I don't think
3478520	3484280	we can train a machine to be intelligent purely from text. Because I think the amount of information
3484280	3489640	about the world that's contained in text is tiny compared to what we need to know.
3490280	3497720	So, for example, people have attempted to do this for 30 years, the SAG project and things
3497720	3503480	like that, of basically writing down all the facts that are known and hoping that some sort
3503480	3508200	of common sense would emerge. I think it's basically hopeless. But let me take an example.
3508200	3513400	You take an object. I describe a situation to you. I take an object, I put it on the table,
3513400	3518520	and I push the table. It's completely obvious to you that the object will be pushed with the table,
3519080	3524280	because it's sitting on it. There's no text in the world, I believe, that explains this.
3524920	3533800	And so, if you train a machine as powerful as it could be, your GPT 5000 or whatever it is,
3533800	3540200	it's never going to learn about this. That information is just not present in any text.
3540920	3544600	Well, the question with the SAG project, the dream I think is to have like
3545560	3554680	10 million, say, facts like that, that give you a head start, like a parent guiding you.
3555400	3558440	Now, we humans don't need a parent to tell us that the table will move,
3559080	3565800	sorry, the smartphone will move with the table. But we get a lot of guidance in other ways,
3565800	3568200	so it's possible that we can give it a quick shortcut.
3568200	3570040	And what about cat? The cat knows that.
3570920	3574360	No, but they evolved. No, they learned like us.
3575960	3585400	Sorry, the physics of stuff. Well, yeah, so you're putting a lot of intelligence onto the
3585400	3593560	nurture side, not the nature. There's a very inefficient, arguably, process of evolution
3593560	3599800	that got us from bacteria to who we are today. Started at the bottom, now we're here.
3599800	3607640	So the question is how fundamental is that the nature of the whole hardware?
3608520	3614200	And then is there any way to shortcut it if it's fundamental? If it's not, if it's most of intelligence,
3614200	3619080	most of the cool stuff we've been talking about is mostly nurture, mostly trained. We figured
3619080	3625000	out by observing the world. We can form that big, beautiful, sexy background model that you're
3625000	3632120	talking about just by sitting there. Then, okay, then you need to then like maybe
3634760	3638760	it is all supervised learning all the way down. It's all supervised learning.
3638760	3644920	Whatever it is that makes human intelligence different from other animals, which a lot of
3644920	3649800	people think is language and logical reasoning and this kind of stuff. It cannot be that complicated
3649800	3659160	because it only popped up in the last million years. It only involves less than 1% of a genome,
3659160	3663560	right, which is the difference between human genome and chimps or whatever. So
3665480	3670760	it can be that complicated. It can be that fundamental. Most of the so complicated stuff
3670760	3675560	already exist in cats and dogs and certainly primates, non-human primates.
3675880	3682440	Yeah, that little thing with humans might be just something about social interaction and
3682440	3690200	ability to maintain ideas across like a collective of people. It sounds very dramatic and very
3690200	3693240	impressive, but it probably isn't mechanistically speaking.
3693240	3700920	It is, but we're not there yet. We have, I mean, this is number 634 in the list of
3700920	3707240	problems we have to solve. So basic physics of the world is number one. What are you,
3708200	3716760	just a quick tangent on data augmentation. So a lot of it is hard coded versus learned.
3718200	3722840	Do you have any intuition that maybe there could be some weird data augmentation,
3723560	3728280	like generative type of data augmentation, like doing something weird to images, which
3728280	3736120	then improves the similarity learning process. So not just kind of dumb, simple distortions,
3736120	3740760	but by you shaking your head, just saying that even simple distortions are enough.
3740760	3744840	I think no, I think data augmentation is a temporary necessary evil.
3746360	3751800	So what people are working on now is two things. One is the type of self-supervisioning,
3753080	3756920	like trying to translate the type of self-supervisioning people using language,
3757000	3761080	translating these two images, which is basically a denosing autoencoder method.
3761720	3769320	So you take an image, you block, you mask some parts of it, and then you train some giant neural
3769320	3777640	net to reconstruct the parts that are missing. And until very recently, there was no working
3777640	3782360	methods for that. All the autoencoder type methods for images weren't producing very
3782360	3787240	good representation. But there's a paper now coming out of the Fair Group at Immelo Park
3787240	3793080	that actually works very well. So that doesn't require the documentation, that requires only
3793080	3800440	masking. Okay. Only masking for images. Okay. Right. So you mask a part of the image and you
3800440	3808520	train a system, which in this case is a transformer because the transformer represents the image as
3809240	3813160	non-overlapping patches. So it's easy to mask patches and things like that.
3813160	3818760	Okay. Then my question transfers to that problem, then masking. Why should the mask be a square
3818760	3826520	rectangle? So it doesn't matter. I think we're gonna come up probably in the future with ways
3826520	3832840	to mask that are kind of random, essentially. I mean, they are random already, but...
3832840	3841480	No, no. But something that's challenging, optimally challenging. So maybe it's a metaphor
3841480	3848120	that doesn't apply, but it seems like there's a data augmentation or masking. There's an
3848120	3853880	interactive element with it. You're almost playing with an image, and it's the way we
3853880	3857960	play with an image in our minds. No, but it's like dropout. It's like Boston Machine Training.
3858760	3868920	You know, every time you see a percept, you can perturb it in some way. And then the principle
3868920	3875000	of the training procedure is to minimize the difference of the output or the representation
3875000	3881400	between the clean version and the corrupted version, essentially. And you can do this in
3881400	3887720	real time. So Boston Machine worked like this. You show a percept, you tell the machine that's
3887720	3896760	a good combination of activities or your input neurons. And then you either let them go their
3896760	3902680	merry way without clamping them to values, or you only do this with a subset. And what you're
3902680	3908440	doing is you're training the system so that the stable state of the entire network is the same,
3908440	3912120	regardless of whether it sees the entire input or whether it sees only part of it.
3913080	3917880	You know, the nosing autoencoder method is basically the same thing, right? You're training
3917880	3922920	a system to reproduce the input, the complete input and filling the blanks, regardless of which
3922920	3926920	parts are missing. And that's really the underlying principle. And you could imagine,
3926920	3932440	sort of even in the brain, some sort of neural principle where, you know, neurons can oscillate,
3932440	3937960	right? So they take their activity and then temporarily they kind of shut off to, you know,
3937960	3943800	force the rest of the system to basically reconstruct the input without their help,
3943800	3950520	you know? And I mean, you can imagine, you know, more or less biologically possible
3950520	3956920	processes. Something like that. And I guess with this denoising autoencoder and masking and
3956920	3962760	data augmentation, you don't have to worry about being super efficient. You can just do as much
3962760	3968680	as you want and get better over time. Because I was thinking like you might want to be clever
3968680	3975000	about the way you do all these procedures, you know, but that's only if it's somehow costly
3975000	3981400	to do every iteration, but it's not really. Not really. And then there is, you know,
3981400	3985480	data augmentation without explicit data augmentation is data augmentation by weighting,
3985480	3991400	which is, you know, the sort of video prediction. You're observing a video clip,
3991480	3998040	observing the, you know, the continuation of that video clip. You try to learn a representation
3998040	4002280	using the joint embedding architectures in such a way that the representation of the
4002280	4007160	future clip is easily predictable from the representation of the observed clip.
4008520	4015400	Do you think YouTube has enough raw data from which to learn how to be a cat?
4016280	4016840	I think so.
4017720	4021160	So the amount of data is not the constraint?
4021160	4023400	No, it would require some selection, I think.
4024040	4028360	Some selection of, you know, maybe the right type of data.
4028360	4033800	Don't go down the rabbit hole of just cat videos. You might need to watch some lectures or something.
4035720	4042120	How meta would that be if it like watches lectures about intelligence and then learns,
4042200	4046040	watches your lectures in NYU and learns from that how to be intelligent?
4046040	4047080	I don't think that would be enough.
4050280	4054360	What's your, do you find multimodal learning interesting? We've been talking about visual
4054360	4058040	language, like combining those together, maybe audio, all those kinds of things.
4058040	4061960	There's a lot of things that I find interesting in the short term, but are not
4062520	4067160	addressing the important problem that I think are really kind of the big challenges. So I think,
4067160	4071240	you know, things like multitask learning, continual learning, you know,
4073320	4077640	adversarial issues. I mean, those have, you know, great practical interests in the
4077640	4082520	relatively short term, possibly, but I don't think they're fundamental, you know, active learning,
4082520	4088120	even to some extent reinforcement learning. I think those things will become either obsolete or
4089000	4096120	useless or easy once we figure out how to do self-supervised representation learning or
4097560	4103000	predictive world models. And so I think that's what, you know, the entire community should be
4103000	4107160	focusing on. At least people are interested in sort of fundamental questions or, you know,
4107160	4111720	really kind of pushing the envelope of AI towards the next stage. But of course,
4111720	4115160	there is like a huge amount of, you know, very interesting work to do in sort of practical
4115160	4121400	questions that have, you know, short term impact. Well, you know, it's difficult to talk about the
4121400	4126360	temporal scale because all of human civilization will eventually be destroyed because the
4127480	4133240	the sun will die out. And even if Elon Musk is successful, multi-planetary colonization across
4133240	4139240	the galaxy, eventually the entirety of it will just become giant black holes. And
4140280	4145560	that's gonna take a while though. So, but what I'm saying is then that logic can be used to say
4145560	4154280	it's all meaningless. I'm saying all that to say that multitask learning might be your song,
4154280	4158920	you're calling it practical or pragmatic or whatever. That might be the thing that achieves
4158920	4167080	something very akin to intelligence while we're trying to solve the more general problem of
4167080	4172200	self-supervised learning of background knowledge. So the reason I bring that up may be one way to
4172200	4176600	ask that question. I've been very impressed by what Tesla autopilot team is doing. I don't know
4176600	4181960	if you've gotten a chance to glance at this particular one example of multitask learning
4182040	4188840	where they're literally taking the problem like, I don't know, Charles Darwin start studying animals.
4188840	4194120	They're studying the problem of driving and asking, okay, what are all the things you have to perceive?
4195000	4199960	And the way they're solving it is one, there's an ontology where you're bringing that to the
4199960	4203800	table. So you're formulating a bunch of different tasks. It's like over a hundred tasks or something
4203800	4208520	like that that they're involved in driving. And then they're deploying it and then getting data
4208520	4213320	back from people that run to trouble. And they're trying to figure out, do we add tasks? Do we,
4213320	4218840	like we focus on each individual task separately? In fact, half. So the, I would say, I'll classify
4218840	4223640	Andre Carpathi's talk in two ways. So one was about doors. And the other one about how much
4223640	4230840	ImageNet sucks. He kept going back and forth on those two topics, which ImageNet sucks, meaning
4230840	4237320	you can't just use a single benchmark. There's so like, you have to have like a giant suite of
4237320	4239880	benchmarks to understand how well your system actually works.
4239880	4247240	Oh, I agree with him. I mean, he's a very sensible guy. Now, okay, it's very clear that if you're
4247240	4251880	faced with an engineering problem that you need to solve in a relatively short time,
4251880	4256520	particularly if you have Elon Musk breathing down your neck, you're going to have to take
4256520	4263000	shortcuts, right? You might think about the fact that the right thing to do and the long-term
4263000	4269240	solution involves some fancy self-improvement running, but you have Elon Musk breathing down
4269240	4278760	your neck. And this involves human lives. And so you have to basically just do the systematic
4278760	4286680	engineering and fine-tuning and refinements and try and error and all that stuff. There's nothing
4286680	4294840	wrong with that. That's called engineering. That's called putting technology out in the world.
4295800	4305960	And you have to kind of ironclad it before you do this so much for grand ideas and principles.
4308200	4315640	But I'm placing myself sort of some upstream of this, quite a bit upstream of this.
4315720	4317800	You're a play-doh think about platonic forms.
4319000	4326120	It's not platonic because eventually, I want the stuff to get used, but it's okay if it takes
4326120	4330280	five or 10 years for the community to realize this is the right thing to do. I've done this
4330280	4336840	before. It's been the case before that I've made that case. I mean, if you look back in the mid-2000,
4336840	4341800	for example, and you ask yourself the question, okay, I want to recognize cars or faces or whatever.
4342200	4346920	You know, I can use convolutional nets, so I can use more conventional
4348200	4352040	kind of computer vision techniques using interest point detectors or sift
4352600	4358040	density features and sticking an SVM on top. At that time, the datasets were so small that
4358760	4363960	those methods that use more hand-engineering worked better than comnets. There was just
4363960	4368920	not enough data for comnets. And comnets were a little slow with the kind of hardware that
4368920	4376120	was available at the time. And there was a sea change when, basically, when datasets became
4376120	4384280	bigger and GPUs became available. That's what two of the main factors that basically made people
4384280	4394520	change their mind. And you can look at the history of all sub-branches of AI or pattern
4394520	4401560	recognition. And there's a similar trajectory followed by techniques where people start by,
4401560	4409160	you know, engineering the hell out of it. You know, be it optical character recognition,
4409160	4414840	speech recognition, computer vision like image recognition in general, natural language
4414840	4419000	understanding like, you know, translation, things like that, right? You start to engineer the hell
4419000	4424680	out of it. You start to acquire all the knowledge, the prior knowledge you know about image formation,
4424680	4429480	about, you know, the shape of characters, about, you know, morphological operations,
4429480	4434200	about like feature extraction, Fourier transforms, you know, vernike moments, you know, whatever,
4434200	4438920	right? People have come up with thousands of ways of representing images so that they could be easily
4439800	4444920	classified afterwards. Same for speech recognition, right? There is, you know, two decades for people
4444920	4451080	to figure out a good front-end to prepossess a speech signal so that, you know, the information
4451080	4456360	about what is being said is preserved, but most of the information about the identity of the speaker
4456360	4463560	is gone. You know, casserole coefficients or whatever, right? And same for text, right?
4464440	4473320	You do name identity recognition and you parse and you do tagging of the parts of speech and,
4473320	4478360	you know, you do this sort of tree representation of clauses and all that stuff right before you
4478360	4487240	can do anything. So that's how it starts, right? Just engineer the hell out of it. And then you
4487240	4492680	start having data and maybe you have more powerful computers, maybe you know something about statistical
4492680	4496680	learning. So you start using machine learning and it's usually a small sliver on top of your
4496680	4501960	kind of handcrafted system where, you know, you extract features by hand, okay? And now, you know,
4501960	4505720	nowadays the standard way of doing this is that you train the entire thing end-to-end with a deep
4505720	4511000	learning system and it learns its own features and, you know, speech recognition systems nowadays,
4511880	4517160	OCR systems are completely end-to-end. It's, you know, it's some giant neural net that takes
4517160	4522760	raw waveforms and produces a sequence of characters coming out. And it's just a huge neural net,
4522760	4528120	right? There's no, you know, Markov model, there's no language model that is explicit other than,
4528120	4531640	you know, something that's ingrained in the, in the sort of neural language model if you want.
4531960	4537800	Same for translation, same for all kinds of stuff. So you see this continuous evolution from,
4539240	4549000	you know, less and less hand-crafting and more and more learning. And I think it's true in biology
4549000	4556760	as well. So, I mean, we might disagree about this, maybe not in this one little piece at the end,
4556760	4562840	you mentioned active learning. It feels like active learning, which is the selection of data,
4562840	4567640	and also the interactivity needs to be part of this giant neural network. You cannot just be
4567640	4573800	an observer to do self-supervised learning. You have to, well, self-supervised learning is just
4573800	4579560	a word, but I would, whatever this giant stack of a neural network that's automatically learning,
4579560	4588200	it feels, my intuition is that you have to have a system, whether it's a physical robot
4588200	4594680	or a digital robot that's interacting with the world and doing so in a flawed way and
4594680	4603000	improving over time in order to form the self-supervised learning. Well, you can't just give it a
4603080	4608520	giant sea of data. Okay, I agree and I disagree. I agree in the sense that I think,
4610120	4616280	I agree in two ways. The first way I agree is that if you want and you certainly need
4616280	4619720	a causal model of the world that allows you to predict the consequences of your actions,
4620360	4624920	to train that model, you need to take actions. You need to be able to act in a world and see the
4624920	4630360	effect for you to learn causal models of the world. So that's not obvious because you can
4630440	4635880	observe others and you can infer that they're similar to you and then you can learn from that.
4635880	4640280	Yeah, but then you have to kind of hardware that part and mirror neurons and all that stuff.
4642120	4649320	And it's not clear to me how you would do this in a machine. So I think the action part would be
4649320	4656920	necessary for having causal models of the world. The second reason it may be necessary or at least
4657000	4663960	more efficient is that active learning basically goes for the juggler of what you don't know.
4663960	4671800	Right? There's obvious areas of uncertainty about your world and about how the world behaves.
4672920	4680200	And you can resolve this uncertainty by systematic exploration of that part that you don't know.
4680200	4684120	And if you know that you don't know, then it makes you curious. You kind of look into
4684120	4692520	situations that and across the animal world, different species at different levels of curiosity.
4693720	4699560	Depending on how they're built. So cats and rats are incredibly curious. Dogs know so much,
4699560	4703800	I mean less. So it could be useful to have that kind of curiosity.
4703800	4708440	So it'd be useful, but curiosity just makes the process faster. It doesn't make the process exist.
4709400	4717640	So what process, what learning process is it that active learning makes more efficient?
4718600	4724680	And I'm asking that first question. We haven't answered that question yet,
4724680	4727960	so I'll worry about active learning once this question is...
4727960	4735240	So it's the more fundamental question to ask. And if active learning or interaction increases
4735320	4741560	the efficiency of the learning. See, sometimes it becomes very different if the increase is
4741560	4748040	several orders of magnitude. Right? That's true. But fundamentally, it's still the same thing.
4748040	4752840	And building up the intuition about how to in a self-supervised way to construct
4752840	4759240	background models, efficient or inefficient, is the core problem. What do you think about
4759240	4764520	Yosha Ben-Jos talking about consciousness and all of these kinds of concepts?
4764520	4768680	Okay. I don't know what consciousness is, but...
4770120	4771000	It's a good opener.
4771880	4776120	And to some extent, a lot of the things that are said about consciousness remind me of
4777000	4781560	the questions people were asking themselves in the 18th century or 17th century when they
4781560	4788680	discovered that how the eye works and the fact that the image at the back of the eye was upside
4788680	4794440	down because you have a lens. And so on your retina, the image that forms is an image of the
4794440	4799720	world, but it's upside down. How is it that you see right side up? And with what we know today
4799720	4805880	in science, we realize this question doesn't make any sense or is kind of ridiculous in some way.
4805880	4809560	Right? So I think a lot of what is said about consciousness is of that nature. Now that said,
4809560	4814520	there's a lot of really smart people for whom I have a lot of respect who are talking about this
4814520	4821880	topic, people like David Chalmers, who is a colleague of mine at NYU. I have kind of an
4821880	4829960	orthodox folk speculative hypothesis about consciousness. So we're talking about this
4829960	4838360	idea of a world model. And I think our entire prefrontal context basically is the engine for
4838440	4845320	a world model. But when we are attending at a particular situation, we're focused on that
4845320	4853240	situation, we basically cannot attend to anything else. And that seems to suggest that we basically
4853240	4861320	have only one world model engine in our prefrontal context. That engine is configurable to the
4861320	4868280	situation at hand. So we are building a box out of wood or we are driving down the highway
4868280	4874200	playing chess. We basically have a single model of the world that we configure into the situation
4874200	4880440	at hand, which is why we can only attend to one task at a time. Now, if there is a task that we
4880440	4887320	do repeatedly, it goes from the sort of deliberate reasoning using model of the world and prediction
4887320	4890360	and perhaps something like model predictive control, which I was talking about earlier,
4891240	4895480	to something that is more subconscious that becomes automatic. So I don't know if you've
4895480	4905400	ever played against a chess grandmaster. I get wiped out in 10 plays. And I have to think about
4905400	4913080	my move for like 15 minutes. And the person in front of me, the grandmaster, would just like
4913720	4919880	react within seconds. He doesn't need to think about it. That's become part of the subconscious
4919880	4927560	because it's basically just pattern recognition at this point. The first few hours you drive a car,
4927560	4931720	you are really attentive, you can't do anything else. And then after 20, 30 hours of practice,
4931720	4936120	50 hours, it's subconscious. You can talk to the person next to you, things like that.
4936920	4940040	Unless the situation becomes unpredictable, and then you have to stop talking.
4941000	4947240	So that suggests you only have one model in your head. And it might suggest the idea that
4947240	4952440	consciousness basically is the module that configures this world model of yours. You need to have
4952440	4959160	some sort of executive kind of overseer that configures your world model for the situation
4959160	4965400	at hand. And that needs to kind of the really curious concept that consciousness is not a
4965400	4970840	consequence of the power of our mind, but of the limitation of our brains. But because we have only
4970840	4977720	one world model, we have to be conscious. If we had as many world models as there are situations we
4977720	4981960	encounter, then we could do all of them simultaneously. And we wouldn't need this sort of
4981960	4986840	executive control that we call consciousness. Yeah, interesting. And somehow maybe that
4986840	4992120	executive controller, I mean, the hard problem of consciousness, there's some kind of chemicals
4992120	4997480	and biology that's creating a feeling like it feels to experience some of these things.
4998440	5005080	That's kind of like the hard question is, what the heck is that? And why is that useful? Maybe the
5005080	5011560	more pragmatic question? Why is it useful to feel like this is really you experiencing this versus
5011560	5020920	just like information being processed? It could be just a very nice side effect of the way we
5020920	5029800	evolved. That's just very useful to feel a sense of ownership to the decisions you make,
5029800	5034040	to the perceptions you make, to the model you're trying to maintain, like you own this thing.
5034760	5039080	And this is the only one you got. And if you lose it, it's going to really suck. And so you should
5039080	5047800	really send the brain some signals about it. What ideas do you believe might be true that most or at
5047800	5052840	least many people disagree with you with? Let's say in the space of machine learning.
5053640	5061000	Well, depends who you talk about. So certainly there is a bunch of people who are nativists,
5061000	5065000	who think that a lot of the basic things about the world are kind of hardwired in our minds.
5066360	5069560	Things like the world is three dimensional, for example, is that hardwired.
5070360	5077400	Things like object permanence is something that we learn before the age of three months or so,
5077800	5084280	or are we born with it? And there are very wide disagreements among the cognitive scientists
5085560	5088280	for this. I think those things are actually very simple to learn.
5090520	5096360	Is it the case that the oriented edge detectors in V1 are learned or are they hardwired? I think
5096360	5100520	they are learned. They might be learned before both because it's really easy to generate signals
5100520	5106360	from the retina that actually will train edge detectors. And again, those are things that can
5106360	5114120	be learned within minutes of opening your eyes. Since the 1990s, we have algorithms that can
5114120	5118760	learn oriented edge detectors completely unsupervised with the equivalent of a few minutes of real
5118760	5124840	time. So those things have to be learned. And there's also those MIT experiments where you
5124840	5131240	kind of plug the optical nerve on the auditory cortex of a baby ferret and that auditory cortex
5131240	5137000	become a visual cortex essentially. So clearly, there's running taking place there.
5137880	5142440	So I think a lot of what people think are so basic that they need to be hardwired,
5143080	5145320	I think a lot of those things are learned because they're easy to learn.
5146200	5151480	So you put a lot of value in the power of learning. What kind of things do you suspect
5151480	5155320	might not be learned? Is there something that could not be learned?
5155960	5163320	So your intrinsic drives are not learned. There are the things that make humans human
5163320	5170200	or make cats different from dogs. It's the basic drives that are kind of hardwired in our
5170840	5175240	Bezo Ganglia. I mean, there are people who are working on this kind of stuff that's called
5175240	5179480	intrinsic motivation in the context of reinforcement learning. So these are objective
5179480	5184440	functions where the reward doesn't come from the external world. It's computed by your own brain.
5184440	5189160	Your own brain computes whether you're happy or not. It measures your degree of
5190840	5197000	comfort or in comfort. And because it's your brain computing this, presumably it knows also
5197000	5205240	how to estimate gradients of this. So it's easier to learn when your objective is intrinsic.
5206920	5213240	So that has to be hardwired. The critic that makes long-term prediction of the outcome,
5213320	5219560	which is the eventual result of this, that's learned. And perception is learned and your
5219560	5224360	model of the world is learned. But let me take an example of why the critic... I mean,
5224360	5231480	an example of how the critic may be learned. If I come to you, I reach across the table and
5231480	5236040	I pinch your arm, complete surprise for you. You would not have expected this from me.
5236040	5239800	I was expecting that the whole time, but yes. Let's say for the sake of the story.
5240520	5246520	Okay. Your bezelganglia is going to light up because it's going to hurt, right?
5248280	5252840	And now your model of the world includes the fact that I may pinch you if I approach my...
5254680	5255800	Don't trust humans.
5256920	5261320	My hand to your arm. So if I try again, you're going to recoil. And that's your critic,
5262280	5271960	your predictor of your ultimate pain system that predicts that something bad is going to
5271960	5275080	happen and you recoil to avoid it. So even that can be learned.
5275080	5280760	That is learned, definitely. This is what allows you also to define some goals, right? So
5282280	5286760	the fact that you're a school child who wake up in the morning and you go to school and
5287000	5292680	it's not because you necessarily like waking up early and going to school,
5292680	5295800	but you know that there is a long-term objective you're trying to optimize.
5295800	5299800	So Ernest Becker, I'm not sure if you're familiar with the philosopher who wrote the
5299800	5304680	book Denial of Death. And his idea is that one of the core motivations of human beings is our
5304680	5310440	terror of death, our fear of death. That's what makes us unique from cats. Cats are just surviving.
5310440	5321000	They do not have a deep, like cognizance introspection that over the horizon is the end.
5321720	5325640	And he says that, I mean, there's a terror management theory that just all these psychological
5325640	5334280	experiments that show basically this idea that all of human civilization, everything we create,
5334280	5339640	is kind of trying to forget if even for a brief moment that we're going to die.
5340840	5347160	When do you think humans understand that they're going to die? Is it learned early on also?
5348920	5354040	I don't know at what point. I mean, it's a question like, at what point do you realize
5354040	5358760	that what death really is? And I think most people don't actually realize what death is,
5358760	5361800	right? I mean, most people believe that you go to heaven or something, right?
5361880	5368600	So to push back on that, what Ernest Becker says and Sheldon Solomon, all of those folks,
5369240	5374040	and I find those ideas a little bit compelling is that there is moments in life, early in life.
5374040	5382120	A lot of this fun happens early in life when you are, when you do deeply experience the terror
5382120	5387080	of this realization and all the things you think about about religion, all those kinds of things
5387080	5391960	that we kind of think about more like teenage years and later. We're talking about way earlier.
5391960	5394120	No, there's like seven or eight years or something like that. Yeah.
5394120	5402760	You realize, holy crap, this is like the mystery, the terror, like it's almost like you're a little
5402760	5406440	prey, a little baby deer sitting in the darkness of the jungle of the woods,
5407000	5411480	looking all around you, there's darkness full of terror. I mean, that's, that realization says,
5411480	5416280	okay, I'm going to go back in the comfort of my mind where there is a, where there is a deep
5416280	5423800	meaning where there is a, maybe like pretend I'm immortal and however way, however kind of idea I
5423800	5429080	can construct to help me understand that I'm immortal. Religion helps with that. You can,
5429080	5434120	you can delude yourself in all kinds of ways, like lose yourself in the busyness of each day,
5434120	5438120	have little goals in mind, all those kinds of things to think that it's going to go on forever.
5438120	5443080	And you kind of know you're going to die. Yeah. And it's going to be sad, but you don't really
5443160	5447800	understand that you're going to die. And so that's, that's their idea. And I find that compelling
5448520	5454520	because it does seem to be a core unique aspect of human nature that we were able to think that
5454520	5460520	we're going, we're able to really understand that this life is finite. That seems important.
5460520	5463560	There's, there's a bunch of different things there. So first of all, I don't think there is
5463560	5468600	a qualitative difference between, between us and cats in the term. I think the difference is that
5468600	5475480	we just have a better long term ability to predict, you know, in the long term. And so
5475480	5478680	we have a better understanding of other world works. So we have better understanding of,
5478680	5483480	you know, funniness of life and things like that. Do we have a better planning engine than cats?
5483480	5489720	Yeah. Okay. But what's the motivation for planning that far? Well, I think it's just a side
5489720	5494360	effect to the fact that we have just a better planning engine because it makes us, as I said,
5494360	5499240	you know, the essence of intelligence is the ability to predict. And so the, because we're
5499240	5503880	smarter, as a side effect, we also have this ability to kind of make predictions about our own
5505080	5511080	future existence or lack the rough. You say religion helps with that. I think religion
5511080	5515800	hurts actually. It makes people worry about like, you know, what's going to happen after
5515800	5521080	their death, etc. If you believe that, you know, you just don't exist after death, like, you know,
5521080	5524840	it solves completely the problem at least. You're saying if you don't believe in God,
5524840	5529960	you don't worry about what happens after death? Yeah. I don't know. You only worry about the,
5529960	5533000	about, you know, this life, because that's the only one you have.
5534120	5537640	I think it's, well, I don't, I don't know, if I were to say what Ernest Becker says,
5537640	5547080	and I'll say I agree with him more than not is you do deeply worry. If you, if you believe
5547080	5552600	there's no God, there's still a deep worry like of the mystery of it all. Like, how does that make
5552600	5559880	any sense that it just ends? I don't think we can truly understand that this right. I mean,
5559880	5566520	so much of our life, the consciousness, the ego is invested in this, in this being. And then
5567560	5573080	science keeps bringing humanity down from its pedestal. And that's, that's just another,
5573800	5578520	example of it. That's wonderful. But for us individual humans, we don't like to be
5578520	5584040	brought down from a pedestal. You're saying like, but see, you're fine with it because, well,
5584040	5587720	so what Ernest Becker would say is you're fine with it because that's just a more peaceful
5587720	5591880	existence for you, but you're not really fine. You're hiding from, in fact, some of the people
5591880	5599000	that experienced the deepest trauma that earlier in life, they often, before they seek extensive
5599080	5603400	therapy will say that I'm fine. It's like, when you talk to people who are truly angry,
5603400	5607080	how are you doing? I'm fine. The question is, what's going on?
5607640	5613800	Now I had a near death experience. I had a very bad motorbike accident when I was 17. So,
5614920	5620360	but that didn't have any impact on my reflection on that topic.
5620360	5625000	So I'm basically just playing a bit of devil's advocate and pushing back and wondering,
5625720	5629320	is it truly possible to accept death? And the flip side that's more interesting,
5629320	5637080	I think, for AI and robotics is how important is it to have this as one of the suite of motivations
5637080	5649000	is to not just avoid falling off the roof or something like that, but ponder the end of the ride.
5649720	5656760	Well, if you listen to the Stoics, it's a great motivator. It adds a sense of urgency.
5656760	5662200	So it might be to truly fear death or be cognizant of it might give
5663640	5667960	a deeper meaning and urgency to the moment to live fully.
5670440	5674440	Maybe I don't disagree with that. I mean, I think what motivates me here is
5674680	5681960	knowing more about human nature. I mean, I think human nature and human intelligence is a big
5681960	5690120	mystery. It's a scientific mystery in addition to philosophical and etc. But I'm a true believer
5690120	5698840	in science. And I do have kind of a belief that for complex systems like the brain and the mind,
5699800	5707320	the way to understand it is to try to reproduce it with artifacts that you built. Because you
5707320	5712280	know what's essential to it when you try to build it. The same way, I've used this analogy before
5712280	5718760	with you, I believe, the same way we only started to understand aerodynamics when we started building
5718760	5725400	airplanes. And that helped us understand how birds fly. So I think there's a similar process here
5725400	5731640	where we don't have a theory of a full theory of intelligence. But building intelligent artifacts
5731640	5738680	will help us perhaps develop some underlying theory that encompasses not just artificial
5738680	5743720	implements, but also human and biological intelligence in general.
5743720	5748760	So you're an interesting person to ask this question about sort of all kinds of different other
5749400	5756280	intelligent entities or intelligences. What are your thoughts about kind of like the touring or
5756280	5764120	the Chinese room question? If we create an AI system that exhibits a lot of properties of
5764120	5771160	intelligence and consciousness, how comfortable are you thinking of that entity as intelligent or
5771160	5776520	conscious? So you're trying to build now systems that have intelligence and there's metrics about
5776520	5786280	their performance. But that metric is external. So are you okay calling a thing intelligent?
5786280	5792840	Or are you going to be like most humans and be once again unhappy to be brought down from a
5792840	5802200	pedestal of consciousness slash intelligence? No, I'll be very happy to understand more about
5802200	5807400	human nature, human mind and human intelligence through the construction of machines that
5808840	5815480	have similar abilities. And if a consequence of this is to bring down humanity one notch down from
5816200	5821960	its already low pedestal, I'm just fine with it. That's just a reality of life. So I'm fine with
5821960	5826920	that. Now you were asking me about things that opinions I have that a lot of people may disagree
5826920	5835160	with. I think if we think about the design of an autonomous intelligence system, so assuming that
5835160	5840280	we are somewhat successful at some level of getting machines to learn models of the world,
5840280	5846440	predictive models of the world, we build intrinsic motivation, objective functions to drive the
5846440	5851480	behavior of that system. The system also has perception modules that allows it to estimate
5851480	5856200	the state of the world and then have some way of figuring out a sequence of actions that to
5856280	5862600	optimize a particular objective. If it has a critic of the type that was describing before,
5862600	5865800	the thing that makes recoil your arm the second time I try to pinch you,
5868440	5874200	intelligent autonomous machine will have emotions. I think emotions are an integral part of
5874200	5881560	autonomous intelligence. If you have an intelligence system that is driven by intrinsic
5881560	5887960	motivation, by objectives, if it has a critic that allows it to predict in advance whether the
5887960	5892600	outcome of a situation is going to be good or bad, it's going to have emotions. It's going to have
5892600	5900120	fear when it predicts that the outcome is going to be bad and something to avoid is going to have
5900120	5908840	elation when it predicts it's going to be good. If it has drives to relate with humans in some
5908920	5916680	ways, the way humans have, it's going to be social. It's going to have emotions about
5916680	5926840	attachment and things of that type. I think the sci-fi thing where you see commander data
5926840	5931080	like having an emotion chip that you can turn off, I think that's ridiculous.
5931480	5939000	So, here's the difficult philosophical social question. Do you think there will be a time
5939880	5946360	like a civil rights movement for robots where, okay, forget the movement, but a discussion
5946360	5954520	like the Supreme Court that particular kinds of robots, particular kinds of systems,
5955480	5961320	deserve the same rights as humans because they can suffer just as humans can,
5962840	5970040	all those kinds of things? Well, perhaps not. Imagine that humans were that you could
5972200	5979400	die and be restored. You could be 3D reprinted and your brain could be reconstructed in its
5979400	5986760	finest details. Our ideas of rights will change in that case. If there's always a backup,
5986760	5991880	you could always restore. Maybe the importance of murder will go down one notch.
5991880	6000520	That's right. But also, your desire to do dangerous things like skydiving or
6000680	6011160	race car racing or that kind of stuff would probably increase or airplane aerobatics or
6011160	6016760	that kind of stuff. It would be fine to do a lot of those things or explore dangerous areas and
6016760	6021960	things like that. It would change your relationship. Now, it's very likely that robots would be like
6021960	6027880	that because they'll be based on perhaps technology that is somewhat similar to
6028760	6035320	this technology and you can always have a backup. So, it's possible. I don't know if you like video
6035320	6044520	games, but there's a game called Diablo. My sons are huge fans of this. Yes. In fact,
6044520	6051080	they made a game that's inspired by it. Awesome. Like built a game? My three sons have a game design
6051080	6055080	studio between them. That's awesome. They came out with a game. They just came out of the game.
6055080	6058040	Last year? No, this was last year, about a year ago.
6058040	6063320	That's awesome. But in Diablo, there's something called hardcore mode, which if you die,
6063320	6069240	there's no, you're gone. That's it. And so, it's possible with AI systems
6070600	6075800	for them to be able to operate successfully and for us to treat them in a certain way because
6075800	6081960	they have to be integrated in human society, they have to be able to die no copies allowed.
6081960	6086920	In fact, copying is illegal. It's possible with humans as well, like cloning will be illegal,
6086920	6090040	even when it's possible. But cloning is not copying, right? I mean,
6090040	6095400	you don't reproduce the mind of the person and experience. It's just a delayed twin.
6096280	6101400	But then, we were talking about with computers that you'll be able to copy. You'll be able to
6101400	6110040	perfectly save, pickle the mind state. And it's possible that that would be illegal because
6110840	6118840	that will destroy the motivation of the system. Okay, so let's say you have a domestic robot,
6120200	6127240	sometime in the future. And the domestic robot comes to you somewhat pre-trained,
6127240	6131400	can do a bunch of things. But it has a particular personality that makes it slightly different
6131400	6136360	from the other robots because that makes them more interesting. And then because it's lived
6136360	6142840	with you for five years, you've grown some attachment to it and vice versa. And it's learned
6142840	6148920	a lot about you. Or maybe it's not a household robot, maybe it's a virtual assistant that lives in
6148920	6154760	your augmented reality glasses or whatever, right? The HER movie type thing, right?
6156600	6164280	And that system to some extent, the intelligence in that system is a bit like your child or maybe
6164280	6170520	your PhD student in the sense that there's a lot of you in that machine now, right? And so,
6171080	6177240	if it were a living thing, you would do this for free if you want, right? If it's your child,
6177240	6183880	your child can then live his or her own life. And the fact that they learn stuff from you
6183880	6189240	doesn't mean that you have any ownership of it, right? But if it's a robot that you've trained,
6189240	6193800	perhaps you have some intellectual property claim about...
6193800	6199080	Going to intellectual property. Oh, I thought you meant like permanent value in the sense that's part
6199080	6203720	of you is in... Well, there is permanent value, right? So you would lose a lot if that robot
6203720	6207800	were to be destroyed and you had no backup, you would lose a lot. You would lose a lot of investment,
6207800	6215640	you know, kind of like a person dying, you know, that a friend of yours dying or a co-worker or
6215640	6225000	something like that. But also you have intellectual property rights in the sense that that system
6225000	6230600	is fine-tuned to your particular existence. So that's now a very unique instantiation of that
6230600	6235400	original background model, whatever it was that arrived. And then there are issues of privacy,
6235400	6241480	right? Because now imagine that that robot has its own kind of volition and decides to work
6241480	6247640	for someone else or kind of thinks life with you is sort of untenable or whatever.
6249560	6252280	Now, all the things that that system learned from you,
6254520	6258520	you know, how can you like, you know, delete all the personal information that that system
6258520	6262440	knows about you? Yeah. I mean, that would be kind of an ethical question. Like, you know,
6262440	6270520	can you erase the mind of an intelligent robot to protect your privacy? You can't do this with
6270600	6275560	humans. You can ask them to shut up, but that you don't have complete power over them.
6275560	6280040	Can't erase humans. Yeah, it's the problem with the relationships, you know, that you break up,
6280040	6284840	you can't you can't erase the other human with robots. I think it will have to be the same thing
6284840	6294440	with robots that that risk that there has to be some risk to our interactions to truly experience
6294440	6300520	them deeply, it feels like. So you have to be able to lose your robot friend. And that robot
6300520	6306120	friend to go tweeting about how much of an asshole you are. But then are you allowed to, you know,
6306120	6310680	murder the robot to protect your private information? Yeah, probably not. I have this
6310680	6317560	intuition that for robots with with certain, like it's almost like a regulation, if you declare
6317560	6322600	your robot to be, let's call it sentient or something like that, like this, this robot is
6322600	6326680	designed for human interaction, then you're not allowed to murder these robots, it's the same as
6326680	6331400	murdering other humans. Well, but what about you do a backup of the robot, you do preserve on the
6331400	6336200	on a high drive or the equivalent in the future, that might be illegal, just like it's a piracy,
6336200	6341480	piracy is illegal. But it's your own, it's your own robot, right? But you can't, you don't,
6341480	6346840	but then but then you can wipe out his brain. So the this robot doesn't know anything about you
6346840	6351560	anymore, but you still have technically is still in existence because you backed it up.
6351560	6356600	And then there'll be these great speeches at the Supreme Court by saying, Oh, sure, you can erase
6356600	6361080	the mind of the robot, just like you can erase the mind of a human, we both can suffer. There'll
6361080	6367080	be some epic like Obama type character with a speech that we we like the robots and the humans
6367080	6374200	are the same. We can both suffer, we can both hope, we can both all those all those kinds of
6374200	6379720	things, raise families, all that kind of stuff. It's it's interesting for these just like you
6379720	6386360	said, emotion seems to be a fascinatingly powerful aspect of human human interaction, human robot
6386360	6392280	interaction. And if they're able to exhibit emotions at the end of the day, that's probably
6392280	6399080	going to have us deeply consider human rights, like what we value in humans, what we value in
6399080	6404840	other animals. That's why robots and AI is great. It makes us ask as the hard questions.
6404840	6409240	Yeah. But you, I mean, you asked about you asked about the Chinese room type argument,
6409240	6413160	you know, is it real? If it looks real? I think the Chinese room argument is the
6413160	6419880	ridiculous one. So, so for people who don't know Chinese room is you can, I don't even know how
6419880	6426200	to formulate it well, but basically, you can mimic the behavior of an intelligent system by just
6426200	6432760	following a giant algorithm code book that tells you exactly how to respond in exactly each case.
6432760	6437720	But is that really intelligent? It's like a giant lookup table. When this person says this,
6437720	6443880	you answer this, when this person says this, you answer this. And if you understand how that
6443880	6448760	works, you have this giant nearly infinite lookup table. Is that really intelligence? Because
6448760	6453960	intelligence seems to be a mechanism that's much more interesting and complex than this lookup
6453960	6459400	table. I don't think so. So the, I mean, the real question comes down to, do you think,
6460280	6466680	you know, you can, you can mechanize intelligence in some way, even if that involves learning?
6467480	6472040	And the answer is, of course, yes, there's no question. There's a second question then,
6472040	6478360	which is assuming you can reproduce intelligence in sort of different hardware than biological
6478360	6486680	hardware, you know, like computers. Can you, you know, match human intelligence in
6487160	6494840	all the domains in which humans are intelligent? Is it possible, right? So this is quite the
6494840	6501080	hypothesis of strong AI. The answer to this, in my opinion, is an unqualified yes. This would
6501080	6505640	as well happen at some point. There's no question that machines at some point will become more
6505640	6510040	intelligent than humans in all domains where humans are intelligent. This is not for tomorrow,
6510040	6516360	it's going to take a long time, regardless of what, you know, Elon and others have claimed
6516440	6521800	or believed. This is a lot, a lot harder than many of, many of those guys think it is.
6523240	6527400	And many of those guys who thought it was simpler than that years, you know, five years ago,
6527400	6532360	now think it's hard because it's been five years and they realize it's going to take a lot longer
6533240	6535320	than includes a bunch of people at DeepMind, for example. But
6535960	6540200	Oh, interesting. I haven't actually touched base with the DeepMind folks, but some of it,
6540200	6548840	Elon or Dennis Sousa, I mean, sometimes in your role, you have to kind of create deadlines that
6548840	6553880	are nearer than farther away to kind of create an urgency, because, you know, you have to believe
6553880	6558120	the impossible is possible in order to accomplish it. And there's, of course, a flip side to that
6558120	6562360	coin, but it's a weird, you can't be too cynical if you want to get something done.
6562360	6568120	Absolutely. I agree with that. But I mean, you have to inspire people to work on sort of ambitious
6568120	6576280	things. So, you know, it's certainly a lot harder than we believe, but there's no question in my
6576280	6580360	mind that this will, this will happen. And now, you know, people are kind of worried about what
6580360	6585640	does that mean for humans, they are going to be brought down from their pedestal, you know,
6585640	6591800	a bunch of notches with that. And, you know, is that going to be good or bad? I mean,
6591800	6595560	it's just going to give more power, right? It's an amplifier for human intelligence really.
6596120	6598840	So speaking of doing cool, ambitious things,
6599800	6604840	FAIR, the Facebook AI Research Group, has recently celebrated its eighth birthday.
6605400	6612280	Or maybe you can correct me on that. Looking back, what has been the successes, the failures,
6612280	6616440	the lessons learned from the eight years of FAIR? And maybe you can also give context of
6616440	6622520	where does the newly minted meta AI fit into how does it relate to FAIR?
6622520	6624920	Right. So let me tell you a little bit about the organization of all this.
6626600	6631080	Yeah, FAIR was created almost exactly eight years ago. It wasn't called FAIR yet.
6631080	6638440	It took that name a few months later. And at the time, I joined Facebook. There was a group
6638440	6644680	called the AI Group that had about 12 engineers and a few scientists, like, you know, 10 engineers
6644680	6649640	and two scientists or something like that. I ran it for three and a half years as a director,
6650520	6654920	you know, hired the first few scientists and kind of set up the culture and organized it,
6654920	6660680	you know, explained to the Facebook leadership what fundamental research was about and how it
6660680	6670360	can work within industry and how it needs to be open and everything. And I think it's been an
6670360	6677880	unqualified success in the sense that FAIR has simultaneously produced, you know,
6677960	6683320	top level research and advanced the science and the technology provided tools, open source tools
6683320	6691320	like PyTorch and many others. But at the same time as had a direct or mostly indirect impact
6691960	6700360	on Facebook at the time, now meta, in the sense that a lot of systems that are that meta is built
6700360	6709320	around now are based on research projects that started at FAIR. And so if you were to take out,
6709320	6716360	you know, deep running out of Facebook services now and meta more generally, I mean, the company
6716360	6721720	would literally crumble. I mean, it's completely built around AI these days. And it's really
6721720	6728600	essential to the operations. So what happened after three and a half years is that I changed
6728600	6735560	role, I became chief scientist. So I'm not doing day to day management of FAIR anymore. I'm more of a
6735560	6741320	kind of, you know, think about strategy and things like that. And I carry my, I conduct my own research,
6741320	6744600	I've, you know, my own kind of research group working on self supervised learning and things
6744600	6751560	like this, which I didn't have time to do when I was director. So now FAIR is run by Joel Pinot
6751560	6757160	and Antoine Baud together, because FAIR is kind of split into now there's something called FAIR
6757160	6762920	Labs, which is sort of bottom up census driven research and FAIR Excel, which is slightly more
6762920	6768360	organized for bigger projects that require a little more kind of focus and more engineering
6768360	6772760	support and things like that. So Joel needs FAIR Lab and Antoine Baud needs FAIR Excel.
6772760	6780200	Where are they located? It's delocalized all over. So there's no question that the leadership
6781160	6787560	of the company believes that this was a very worthwhile investment. And what that means is that
6790120	6797080	it's there for the long run, right? So there is, if you want to talk in these terms, which I don't
6797080	6802920	like, there's a business model, if you want, where FAIR, despite being a very fundamental
6802920	6807560	research lab brings a lot of value to the company, either mostly indirectly through other groups.
6807560	6813800	Now, what happened three and a half years ago when I stepped down was also the creation of
6813800	6820600	Facebook AI, which was basically a larger organization that covers FAIR. So FAIR is
6820600	6828440	included in it, but also has other organizations that are focused on applied research or advanced
6828440	6834520	development of AI technology that is more focused on the products of the company.
6834520	6836520	So less emphasis on fundamental research?
6836600	6839720	Less fundamental, but it's still a research. I mean, there's a lot of papers coming out of
6839720	6848120	those organizations and people are awesome and wonderful to interact with. But it serves as
6850280	6859320	a way to scale up, if you want, AI technology, which may be very experimental and lab prototypes
6859320	6864760	in two things that are usable. So FAIR is a subset of meta AI. If FAIR becomes like KFC,
6865080	6868280	it'll just keep the F. Nobody cares what the F stands for.
6869400	6875400	Will knows soon enough by probably by the end of 2021.
6875400	6878200	This is not a giant change, FAIR.
6878200	6883400	Well, FAIR doesn't sound too good. But the brand people are kind of deciding on this,
6883400	6887880	and they've been hesitating for a while now, and they tell us they're going to come up with
6887880	6891560	an answer as to whether FAIR is going to change name or whether we're going to change just the
6891560	6896040	meaning of the F. Oh, that's a good call. I will keep FAIR and change the meaning of the F.
6896040	6899800	That would be my preference. I would turn the F into fundamental.
6900840	6902200	Oh, that's good. Fundamental AI research.
6902200	6903000	Oh, that's really good. Yeah.
6903000	6908200	Within meta AI. So this would be meta FAIR, but people will call it FAIR, right?
6908200	6909720	Yeah, exactly. I like it.
6909720	6920920	And now meta AI is part of the reality lab. So meta now, the new Facebook
6920920	6928360	where it's called meta, and it's kind of divided into Facebook, Instagram, WhatsApp,
6930360	6939800	and reality lab. And reality lab is about AR, VR, telepresence, communication technology,
6939800	6945000	and stuff like that. It's kind of the, you can think of it as the sort of a combination of
6945720	6951880	sort of new products and technology part of meta.
6951880	6956040	Is that where the touch sensing for robots? I saw that you were posting about that.
6956040	6958840	Touch sensing for robots is part of FAIR, actually. That's a fact.
6958840	6959880	Oh, it is. Okay, cool.
6959880	6965560	Yeah. There's also the, no, but there is the other way, the haptic glove, right?
6965560	6967560	Yes. That's more reality lab.
6967560	6969880	That's reality lab research.
6969880	6973640	Reality lab research. But by the way, the touch sense is super interesting,
6974280	6980040	like integrating that modality into the whole sensing suite is very interesting.
6980040	6984600	So what do you think about the metaverse? What do you think about this whole,
6985960	6990680	this whole kind of expansion of the view of the role of Facebook and meta in the world?
6990680	6995240	Well, metaverse really should be thought of as the next step in the internet, right?
6995240	7006520	Sort of trying to kind of make the experience more compelling of being connected either with
7006520	7015640	other people or with content. And we are evolved and trained to evolve in 3D environments where
7017160	7020920	we can see other people, we can talk to them when you're near them,
7021000	7025000	or, you know, and other people are far away, can hear us, you know, things like that, right?
7025000	7029960	So it, it, there's a lot of social conventions that exist in the real world that we can try to
7029960	7036200	transpose. Now, what is going to be eventually the, the, how compelling is it going to be?
7036200	7039800	Like our, you know, is it going to be the case that people are going to be willing to
7040760	7045160	do this if they have to wear, you know, a huge pair of goggles all day? Maybe not.
7046280	7050200	But then again, if the experience is sufficiently compelling, maybe so.
7050200	7054440	Or if the device that you have to wear is just basically a pair of glasses, you know,
7054440	7061480	technology makes sufficient progress for that. You know, AR is a much easier concept to grasp
7061480	7067000	that you're going to have, you know, augmented reality glasses that basically contain some sort
7067000	7070120	of, you know, virtual assistant that can help you in your daily lives.
7070120	7074120	But at the same time with the AR, you have to contend with reality. With VR, you can
7074120	7078440	completely detach yourself from reality. So it gives you freedom. It might be easier to design
7078440	7085960	worlds in VR. Yeah, but you, you can imagine how, you know, the metaverse being a mix, a mix,
7085960	7089800	right? Or, or like you can have objects that exist in a metaverse that, you know,
7089800	7093720	pop up on top of the real world or only exist in virtual reality.
7094280	7099160	Okay, let me ask the hard question. Because all of this was easy. This was easy.
7100600	7107400	The Facebook now meta, the social network has been painted by the media as net negative for
7107400	7113480	society, even destructive and evil at times. You've pushed back against this defending Facebook.
7114040	7119960	Can you explain your defense? Yeah, so the, the description, the company that is being described
7119960	7128360	in the, in some media is not the company we know when we work inside. And, you know,
7129320	7134520	it could be claimed that a lot of employees are uninformed about what really goes on in the company.
7134520	7138680	But, you know, I'm a vice president. I mean, I have a pretty good vision of what goes on. You
7138680	7142920	know, I don't know everything, obviously, I'm not involved in, in, in everything, but certainly
7142920	7147000	not in decision about like, you know, content moderation or anything like this. But, but I
7147000	7152840	have some decent vision of what goes on. And this evil that is being described, I just don't see it.
7153560	7160520	And then, you know, I think there is an easy story to buy, which is that, you know, all the bad
7160520	7164760	things in the, in the world and, you know, the, the reason your friend believe crazy stuff,
7166600	7172680	you know, there's an easy scapegoat, right, in the, in, in, in social media, in general,
7172680	7178840	Facebook in particular, we have to look at the data, like, is it the case that Facebook, for
7178840	7185880	example, polarizes people politically? Are there academic studies that show this? Is it the case
7185880	7193080	that, you know, teenagers think of themselves less if they use Instagram more? Is it the case that,
7194520	7201320	you know, people get more riled up against, you know, opposite sides in a, in a debate or
7201320	7207400	political opinion, if they, if they are more on Facebook, or if they are less. And study after
7207400	7212600	study show that none of this is true. This is independent studies by academic, they're not
7212600	7218200	funded by Facebook or Meta, you know, studied by Stanford, by some of my colleagues at NYU,
7218200	7223320	actually, with whom I have no connection. You know, there's a study recently, they, they,
7223320	7230760	they paid people, I think it was in, in, in, in the former Yugoslavia, I'm not exactly sure in
7230760	7239400	what, what part, but they paid people to not use Facebook for a while in the period before the
7239400	7245480	anniversary of the cybernature massacres, right? So, you know, people get riled up like, you know,
7245480	7251400	should we have a celebration? I mean, a memorial kind of celebration for it or not. So they paid
7251400	7259320	a bunch of people to not use Facebook for a few weeks. And it turns out that those people ended
7259320	7263720	up being more polarized than they were at the beginning. And the people who were more on Facebook
7263720	7270600	were less polarized. There's a study, you know, from Stanford of economics at Stanford that
7271240	7276920	tried to identify the causes of increasing polarization in the US. And it's been going
7276920	7285080	on for 40 years before, you know, Mark Zuckerberg was born continuously. And, and so if there is
7285080	7289480	a cause, it's not Facebook or social media. So you could say if social media just accelerated,
7289480	7294920	but no, I mean, it's basically a continuous evolution by some measure of polarization in the
7294920	7302040	US. And then you compare this with other countries like the West half of Germany, because you can
7302040	7309320	go 40 years in the East side, or Denmark or other countries. And they use Facebook just as much.
7309320	7313320	And they're not getting more polarized, they're getting less polarized. So if you want to look for,
7313400	7319880	you know, a causal relationship there, you can find a scapegoat, but you can't find a cause. Now,
7319880	7325080	if you want to fix the problem, you have to find the right cause. And what rise me up is that people
7325080	7330440	now are accusing Facebook of bad deeds that are done by others. And those others are, we're not
7330440	7335560	doing anything about them. And by the way, those others include the owner of the Wall Street Journal
7335560	7340040	in which all of those papers were published. So I should mention that I'm talking to Shrep,
7340040	7344520	Mike Shrep on this podcast, and also Mark Zuckerberg, and probably these are the conversations
7344520	7349880	you can have with them. Because it's very interesting to me, even if Facebook has some
7349880	7354680	measurable negative effect, you can't just consider that in isolation, you have to consider about
7354680	7360600	all the positive ways it connects us. So like every technology, you can't just say like,
7361720	7366840	there's an increase in division. Yes, probably Google search engine has created
7366920	7371080	increase in division, we have to consider about how much information are brought to the world.
7371080	7375160	Like, I'm sure Wikipedia created more division, if you just look at the division,
7375160	7379000	we have to look at the full context of the world and it didn't make a better world.
7379000	7382040	I mean, the printing press has created more difference, right? Exactly.
7383000	7389720	So, you know, when the printing press was invented, the first books that were printed were
7389720	7393720	things like the Bible, and that allowed people to read the Bible by themselves,
7393720	7400360	not get the message uniquely from priests in Europe, and that created the Protestant movement
7400360	7405640	and 200 years of religious persecution and wars. So, that's a bad side effect of the printing
7405640	7409800	press. Social networks aren't being nearly as bad as the printing press, but nobody would
7409800	7416040	say the printing press was a bad idea. Yeah, a lot of this perception, and there's a lot of
7416040	7421560	different incentives operating here. Maybe a quick comment, since you're one of the top
7421560	7428440	leaders at Facebook and at Meta, sorry, that's in the tech space, I'm sure Facebook involves
7428440	7433640	a lot of incredible technological challenges that need to be solved. A lot of it probably
7433640	7439480	is in the computer infrastructure, the hardware, I mean, it's just a huge amount. Maybe can you
7439480	7446120	give me context about how much of Shrep's life is AI and how much of it is low-level compute,
7446120	7451960	how much of it is flying all around doing business stuff, and the same with Mark Zuckerberg?
7451960	7461400	They really focus on AI. I mean, certainly in the run-up of the Creation Affair, and for at
7461400	7467400	least a year after that, if not more, Mark was very, very much focused on AI and was spending
7467400	7471960	quite a lot of effort on it, and that's his style. When he gets interested in something,
7471960	7476200	he reads everything about it. He read some of my papers, for example, before I joined.
7479560	7482360	And so he learns a lot about it.
7486360	7492200	And Shrep was really to it also. I mean, Shrep is really kind of,
7494680	7500040	has something I've tried to preserve also, despite my not so young age,
7500040	7505000	which is a sense of wonder about science and technology, and he certainly has that.
7506120	7511400	He's also a wonderful person. I mean, in terms of, as a manager, like dealing with people and
7511400	7519400	everything, Mark also actually. I mean, they're very human people. In the case of Mark, it's
7519400	7527240	shockingly human, given his trajectory. I mean, the personality of him that he's
7527240	7531320	spending in the press, it's just completely wrong. Yeah. But you have to know how to play
7531320	7536760	the press. So that's, I put some of that responsibility on him, too. You have to,
7538760	7544920	it's like, you know, like the director, the conductor of an orchestra, you have to play
7544920	7549720	the press and the public in a certain kind of way, where you convey your true self to them,
7549720	7554440	if there's a depth of kindness. And it's probably not the best at it. So, yeah.
7555160	7562440	You have to learn. And it's sad to see, I'll talk to him about it, but the Shrep is slowly
7562440	7568680	stepping down. It's always sad to see folks sort of be there for a long time and slowly,
7569400	7577160	I guess time is sad. I think he's done the thing he said had to do and, you know, he's got, you know,
7577320	7585960	you know, family priorities and stuff like that. And I understand, you know, after 13 years or
7585960	7592760	something. It's been a good run. Which in Silicon Valley is basically a lifetime. Yeah. You know,
7592760	7597240	because, you know, it's dog years. So, in Europe, the conference just wrapped up.
7598520	7603320	Let me just go back to something else. You posted the paper you co-authored was rejected from
7603400	7614520	Europe. As you said, proudly in quotes rejected. Can you describe this paper and like, what was
7614520	7620440	the idea in it? And also, maybe this is a good opportunity to ask what are the pros and cons,
7620440	7624840	what works and what doesn't about the review process. Yeah. Let me talk about the paper first.
7624840	7631720	I'll talk about the review process afterwards. The paper is called Vicreg. So this is, I mentioned
7631720	7636520	that before, variance in variance, covariance, regularization. And it's a technique, a non-
7636520	7642040	contrastive learning technique for what I call joint embedding architecture. So,
7642040	7646200	sami's nets are an example of joint embedding architecture. So joint embedding architecture is
7649240	7652200	let me back up a little bit, right? So if you want to do self-supervised learning,
7653240	7658680	you can do it by prediction. So let's say you want to train a system to predict video, right?
7658680	7664360	You show it a video clip and you train the system to predict the next, the continuation of that
7664360	7668920	video clip. Now, because you need to handle uncertainty, because there are many, you know,
7668920	7673960	many continuations that are plausible, you need to have, you need to handle this in some way.
7673960	7681480	You need to have a way for the system to be able to produce multiple predictions. And the way,
7681480	7686360	the only way I know to do this is through what's called a latent variable. So you have some sort of
7687320	7692920	hidden vector of a variable that you can vary over a set or draw from a distribution. And as you
7692920	7697560	vary this vector over a set, the output, the prediction varies over a set of plausible predictions.
7698280	7702840	Okay. So that's called, I call this a generative latent variable model.
7703960	7710120	Got it. Okay. Now, there is an alternative to this to handle uncertainty. And instead of directly
7710200	7719400	predicting the next frames of the clip, you also run those through another neural net.
7720920	7727880	So you now have two neural nets, one that looks at the, you know, the initial segment of the video
7727880	7734440	clip. And another one that looks at the continuation during training, right? And what you're trying
7734440	7741400	to do is learn a representation of those two video clips that is maximally informative about
7741400	7747880	the video clips themselves. But it's such that you can predict the representation of the second
7747880	7753320	video clip from the representation of the first one easily. Okay. And you can sort of formalize
7753320	7756680	this in terms of maximizing mutual information and some stuff like that, but it doesn't matter.
7757400	7764360	What you want is informative, representative, you know, informative representations
7764360	7769800	of the two video clips that are mutually predictable. What that means is that there's a
7769800	7777800	lot of details in the second video clips that are irrelevant. You know, I, let's say a video clip
7777800	7783000	consists in, you know, a camera panning the scene, there's going to be a piece of that
7783000	7787320	room that is going to be revealed. And I can somewhat predict what the, what that room is going
7787320	7792520	to look like, but I may not be able to predict the details of the texture of the ground and where
7792520	7797240	the tiles are ending and stuff like that. Right. So those are irrelevant details that perhaps
7797240	7803960	my representation will eliminate. And so what I need is to train this second neural net in such a
7803960	7811880	way that whenever the continuation video clip varies over all the plausible continuations,
7813480	7819240	the representation doesn't change. Got it. So it's the, yeah, yeah. Got it. Over the space of
7819240	7824440	representations, doing the same kind of thing as you do with similarity learning. Right.
7825640	7830680	So, so these are two ways to handle multimodality in a prediction, right? In the first way,
7830680	7835160	you prioritize the prediction with a latent variable, but you predict pixels essentially,
7835160	7839240	right? In the second one, you don't, you don't predict pixels. You predict an abstract
7839240	7844040	representation of pixels. And you guarantee that this abstract representation has as much
7844040	7848440	information as possible about the input, but sort of, you know, drops all the stuff that you really
7848440	7854600	can't predict essentially. I used to be a big fan of the first approach. And in fact, in this paper
7854600	7859160	with the Chen Mishra, this blog post, the dark matter intelligence, I was kind of advocating
7859160	7863480	for this. And in the last year and a half, I've completely changed my mind. I'm now a big fan
7863560	7870760	of the second one. And it's because of a small collection of algorithms that have been proposed
7870760	7878520	over the last year and a half or so, two years to do this, including V Craig. It's predecessor
7878520	7884200	called Barlow Twins, which I mentioned, a method from our friends at DeepMind could be YOL.
7885960	7890360	And, and, and there's a bunch of others now that kind of work similarly. So they're all based on
7890440	7895480	this idea of joint embedding. Some of them have an explicit criterion that is an approximation of
7895480	7899960	mutual information. Some others are BOL work, but we don't really know why. And there's been like
7899960	7904040	lots of theoretical papers about why BOL works. No, it's not that because we take it out and it
7904040	7909320	still works. And, you know, blah, blah, blah. I mean, so there's like a big debate. But, but
7909320	7913160	the important point is that we now have a collection of non-contrastive joint embedding
7913160	7918200	methods, which I think is the best thing since sliced bread. So I'm super excited about this,
7918280	7925160	because I think it's our best shot for techniques that would allow us to kind of build predictive
7925160	7929800	work models. And at the same time, learn hierarchical representations of the world,
7929800	7933400	where what matters about the world is preserved and what is irrelevant is eliminated.
7934520	7939800	By the way, the representations that before and after is across in the space in a sequence of
7939800	7944600	images, or is it for single images? It would be either for a single image for a sequence. It
7944600	7948120	doesn't have to be images. This could be applied to text. This could be applied to just about any
7948120	7952840	signal. I'm looking at, you know, I'm looking for methods that are generally applicable,
7952840	7957480	that are not specific to, you know, one particular modality, you know, it could be audio or whatever.
7957480	7962760	Got it. So what's the story behind this paper? This paper is what is describing one of the one
7962760	7968200	such method? This is this Vicreg method. So this is co-authored. The first author is a student
7968200	7974840	called Adrien Bard, who is a resident PhD student at Fer Paris. He's co-advised by me and Jean Ponce,
7975640	7980440	who's a professor at Economa Supérieure, also a research director at INRIA.
7981480	7986120	So this is a wonderful program in France where PhD students can basically do their PhD in
7986120	7994360	industry. And that's kind of what's happening here. And this paper is a follow-up on this
7994360	8001960	Balotuin paper by my former postdoc, now Stéphane Denis, with Lijing and Yorish Bontar and a bunch
8001960	8009000	of other people from Fer. And one of the main criticism from reviewers is that Vicreg is not
8009000	8016040	different enough from Balotuin's. But, you know, my impression is that it's, you know,
8016760	8022520	Balotuin's with a few bugs fixed, essentially. And in the end, this is what people will use.
8023080	8028840	Right. So, but, you know, I'm used to stuff that I submit being rejected forward.
8028840	8032120	So it might be rejected and actually exceptionally well cited because people use it.
8032120	8034360	Well, it's already cited like a bunch of times.
8034360	8040120	So, I mean, the question is then to the deeper question about peer review and conferences.
8040120	8044200	I mean, computer science is a field that's kind of unique that the conference is highly prized.
8044840	8046040	That's one. Right.
8046040	8051080	And it's interesting because the peer review process there is similar, I suppose, to journals,
8051080	8055640	but it's accelerated significantly. Well, not significantly, but it goes fast.
8056360	8061800	And it's a nice way to get stuff out quickly, to peer review quickly, go to present it quickly
8061800	8068200	to the community. So, not quickly, but quicker. But nevertheless, it has many of the same flaws of
8068200	8072680	peer review because it's a limited number of people look at it. There's bias and following,
8072680	8076600	like that if you want to do new ideas, you're going to get pushed back.
8077960	8085240	There's self-interested people that kind of can infer who submitted it and kind of, you know,
8085240	8089080	be cranky about it, all that kind of stuff. Yeah. I mean, there's a lot of, you know,
8089080	8094120	social phenomena there. There's one social phenomenon, which is that because the field
8094120	8099560	has been growing exponentially, the vast majority of people in the field are extremely junior.
8100520	8105480	So, as a consequence, and that's just a consequence of the field growing, right? So,
8105480	8110360	as the number of, as the size of the field kind of starts saturating, you will have less of that
8110360	8117240	problem of reviewers being very inexperienced. A consequence of this is that, you know,
8118280	8124200	young reviewers, I mean, there's a phenomenon which is that reviewers try to make their life
8124200	8128600	easy. And to make their life easy when reviewing a paper is very simple. You just have to find
8128600	8134840	a flaw in the paper, right? So, basically, they see their task as finding flaws in papers. And
8134840	8143560	most papers have flaws, even the good ones. So, it's easy to do that. Your job is easier as a
8143560	8151000	reviewer if you just focus on this. But what's important is, like, is there a new idea in that
8151000	8156120	paper that is likely to influence? It doesn't matter if the experiments are not that great,
8156120	8164280	if the protocol is, you know, so things like that. As long as there is a worthy idea in it,
8165000	8170600	that will influence the way people think about the problem. Even if they make it better, you know,
8170600	8177640	eventually, I think that's really what makes a paper useful. And so, this combination of
8178200	8185080	social phenomena creates a disease that has plagued, you know, other fields in the past,
8185080	8190600	like speech recognition, where basically, you know, people chase numbers on benchmarks.
8191560	8197320	And it's much easier to get a paper accepted if it brings an incremental improvement on a
8198120	8205880	sort of mainstream, well-accepted method or problem. And those are, to me, boring papers.
8205880	8211560	I mean, they're not useless, right? Because industry, you know, strives on those kind of progress.
8212200	8215720	But they're not the ones that I'm interested in, in terms of, like, new concepts and new ideas. So,
8216360	8222680	papers that are really trying to strike kind of new advances generally don't make it. Now,
8222680	8228040	thankfully, we have archive. Archive, exactly. And then there's open review type of situations
8228040	8233400	we use. And then, I mean, Twitter is a kind of open review. I'm a huge believer that review should
8233400	8239080	be done by thousands of people, not two people. I agree. And so, archive, like, do you see a
8239080	8243880	future where a lot of really strong papers, it's already the present, but a growing future where
8243880	8252040	it'll just be archive. And you're presenting an ongoing continuous conference called Twitter
8252040	8259560	slash the internet slash archive sanity. Andre just released a new version. So, just not, you know,
8259560	8265000	not being so elitist about this particular gating. It's not a question of being elitist or not. It's
8265000	8273560	a question of being basically recommendation and zero approvals for people who don't see themselves
8273560	8278600	having the ability to do so by themselves, right? And so, it saves time, right? If you rely on other
8278600	8287000	people's opinion, and you trust those people or those groups to evaluate a paper for you,
8288760	8293240	that saves you time because, you know, you don't have to, like, scrutinize the paper as much,
8293240	8296520	you know, it is brought to your attention. I mean, it's the whole idea of sort of, you know,
8296520	8302520	collective recommender system. So, I actually thought about this a lot, you know, about 10,
8302520	8309480	15 years ago, because there were discussions at NIPPS and, you know, and we're about to create
8309480	8316120	iClear with Yosha Benjo. And so, I wrote a document kind of describing a reviewing system,
8316120	8320520	which basically was, you know, you post your paper on some repository, let's say archive,
8320520	8327320	or now could be open review. And then you can form a reviewing entity, which is equivalent to a
8327320	8334200	reviewing board, you know, of a journal or program committee of a conference. You have to
8334200	8341720	list the members. And then that group reviewing entity can choose to review a particular paper
8342440	8347000	spontaneously or not. There is no exclusive relationship anymore between a paper and a
8347000	8353800	venue or reviewing entity. Any reviewing entity can review any paper or may choose not to.
8354760	8358200	And then, you know, give an evaluation. It's not published, not published, it's just an
8358200	8363800	evaluation and a comment, which would be public, signed by the reviewing entity. And
8364520	8367880	if it's signed by a reviewing entity, you know, it's one of the members of reviewing entity. So,
8367880	8373880	if the reviewing entity is, you know, Lex Friedman's, you know, preferred papers, right, you know,
8373880	8378680	it's Lex Friedman writing a review. Yes. What, so for me, one, that's a beautiful
8379000	8385720	system, I think. But what's in addition to that, it feels like there should be a reputation system
8385720	8390120	for the reviewers. Absolutely. For the reviewing entities. Not the reviewers individually.
8390120	8394120	The reviewing entities, sure. But even within that, the reviewers too. Because
8395720	8401000	there's another thing here. It's not just the reputation. It's an incentive for an individual
8401000	8406120	person to do great. Right now, in the academic setting, the incentive is kind of
8407080	8411160	internal, just wanting to do a good job. But honestly, that's not a strong enough incentive
8411160	8415880	to do a really good job at reading a paper and finding the beautiful amidst the mistakes and
8415880	8420680	the flaws and all that kind of stuff. Right. Like, if you're the person that first discovered
8420680	8427240	a powerful paper, and you get to be proud of that discovery, then that gives a huge incentive to
8427240	8431560	you. That's, that's a big part of my proposal. Actually, I described that as, you know, if,
8431640	8439000	if your evaluation of papers is predictive of future success, then your reputation should
8439000	8446680	go up as a reviewing entity. So yeah, exactly. I mean, I even had a master's student who was a
8447240	8451720	master's student in library science and computer science, actually kind of work out exactly
8452280	8456680	how that should work with formulas and everything. But so in terms of implementation,
8456680	8460120	do you think that's something that's doable? I mean, I've been sort of, you know, talking about
8460200	8465880	this to sort of various people like, you know, Andrew McCallum, who started Open Review. And
8465880	8470440	the reason why we picked Open Review for iClear initially, even though it was very early for them,
8471320	8475720	is because my hope was that iClear, it was eventually going to kind of
8476600	8483320	inaugurate this type of system. So iClear kept the idea of Open Reviews. So whether reviews are,
8483320	8489160	you know, published with a paper, which I think is very useful. But in many ways, that's kind of
8489160	8495560	reverted to kind of more of a conventional type conferences for everything else. And that, I mean,
8496760	8504040	I don't run iClear, I'm just the president of the foundation. But, you know, people who run it
8504040	8507800	should make decisions about how to run it. And I'm not going to tell them, because they are
8507800	8513880	volunteers. And I'm really thankful that they do that. So, but I'm saddened by the fact that we're
8513880	8521240	not being innovative enough. Yeah, me too. I hope that changes. Yeah. Because the communication
8521240	8527560	science broadly, but communication, computer science ideas is how you make those ideas have
8527560	8534040	impact, I think. Yeah. And I think, you know, a lot of this is because people have in their minds
8534040	8541640	kind of an objective, which is, you know, fairness for authors, and the ability to count points,
8541640	8548040	basically, and give credits accurately. But that comes at the expense of the progress of science.
8548680	8551320	So to some extent, we're slowing down the progress of science.
8552040	8554280	And are we actually achieving fairness?
8554280	8558520	And we're not achieving fairness, you know, we still have biases, you know, we're doing,
8558520	8565000	you know, a double blind review, but, you know, the biases are still there, there are different
8565000	8570840	kinds of biases. You write that the phenomenon of emergence, collective behavior exhibited by
8570920	8576440	large collection of simple elements in interaction is one of the things that got you into neural
8576440	8582840	nets in the first place. I love cellular automata. I love simple interacting elements and the things
8582840	8588680	that emerge from them. Do you think we understand how complex systems can emerge from such simple
8588680	8593800	components that interact simply? No, we don't. It's a big mystery. Also, it's a mystery for
8593800	8602200	physicists, it's a mystery for biologists. You know, how is it that the universe around us seems
8602200	8608280	to be increasing in complexity and not decreasing? I mean, that is a kind of curious property of
8609160	8615880	physics that despite the second law of thermodynamics, we seem to be, you know, evolution and learning
8615880	8624040	and et cetera seems to be kind of at least locally to increase complexity and not decrease it. So,
8624040	8628360	perhaps the ultimate purpose of the universe is to just get more complex.
8629000	8637080	Have these, I mean, small pockets of beautiful complexity. Does that, to sell your time under
8637080	8644040	these kinds of emergence and complex systems give you some intuition or guide your understanding
8644120	8648760	of machine learning systems and neural networks and so on? Are these for you right now disparate
8648760	8655480	concepts? Well, it got me into it. You know, I discovered the existence of the perceptron
8655480	8661640	when I was a college student by reading a good book. It was a debate between Chomsky and Piaget
8661640	8667160	and Seymour Papert from MIT who was kind of singing the praise of the perceptron in that
8667160	8671320	book. And the first time I heard about the running machine, right? So, I started digging the literature
8671320	8677800	and I found those books which were basically transcription of, you know, workshops or conferences
8678600	8684520	from the 50s and 60s about self-organizing systems. So, there was a series of conferences
8684520	8690360	on self-organizing systems and these books on this. Some of them are, you can actually get them at the
8690360	8697640	Internet Archive, you know, the digital version. And there are like fascinating articles in there
8697640	8703960	by, there's a guy whose name has been largely forgotten, Heinz von Förster. He's a German
8703960	8711560	physicist who emigrated to the US and worked on self-organizing systems in the 50s. And in the
8711560	8716680	60s, he created at the University of Illinois, Japan, Japan, he created the biological computer
8716680	8722520	laboratory, VCL, which was, you know, all about neural nets. Unfortunately, that was kind of
8722520	8727640	towards the end of the popularity of neural nets. So, that lab never kind of strived very much.
8727640	8733320	But he wrote a bunch of papers about self-organization and about the mystery of self-organization.
8733320	8738760	An example he has is, you take, imagine you are in space, there's no gravity. You have a big box
8738760	8745400	with magnets in it, okay? You know, kind of rectangular magnets with north pole on one end,
8745400	8749640	south pole on the other end. You shake the box gently and the magnets will kind of stick to
8749640	8755320	themselves and probably form like complex structure, you know, spontaneously. You know,
8755320	8758520	that could be an example of self-organization. But, you know, you have lots of example, neural
8758520	8764520	nets are an example of self-organization to, you know, in many respects. And it's a bit of a mystery,
8765560	8771080	you know, how, like, what is possible with this? You know, pattern formation in physical systems,
8771800	8776440	in chaotic system and things like that, you know, the emergence of life, you know, things
8776440	8782440	like that. So, you know, how does that happen? So, it's a big puzzle for physicists as well.
8782440	8789880	It feels like understanding this, the mathematics of emergence in some constrained situations might
8789880	8797640	help us create intelligence. Like, help us add a little spice to the systems because you seem to
8797640	8804840	be able to, in complex systems with emergence, to be able to get a lot from little. And so,
8804920	8811960	that seems like a shortcut to get big leaps in performance. But there's a missing
8811960	8820600	concept that we don't have. And it's something also I've been fascinated by since my undergrad days.
8820600	8827080	And it's how you measure complexity, right? So, we don't actually have good ways of measuring or at
8827080	8831960	least we don't have good ways of interpreting the measures that we have at our disposal. Like,
8831960	8835640	how do you measure the complexity of something, right? So, there's all those things, you know,
8835640	8839960	like, you know, Karmogorov, Chaitin, Solomonov complexity of, you know, the length of the
8839960	8844600	shortest program that would generate a bit string can be thought of as the complexity of that bit
8844600	8852440	string. I've been fascinated by that concept. The problem with that is that that complexity is
8852440	8858760	defined up to a constant, which can be very large. There are similar concepts that are derived from,
8858840	8866760	you know, Bayesian probability theory, where, you know, the complexity of something is the
8866760	8870680	negative log of its probability essentially, right? And you have a complete equivalence between
8870680	8875240	the two things. And there you would think, you know, the probability is something that's well
8875240	8879480	defined mathematically, which means complexity is well defined. But it's not true. You need to have
8879480	8885160	a model of the distribution. You may need to have a prior, if you're doing Bayesian inference. And
8885160	8889320	the prior plays the same role as the choice of the computer with which you measure your Karmogorov
8889320	8896600	complexity. And so, every measure of complexity we have has some arbitraryness in it, you know,
8896600	8903480	an additive constant, which is, can be arbitrarily large. And so, you know, how can we come up with
8903480	8906840	a good theory of how things become more complex if we don't have a good measure of complexity?
8906840	8912840	Yeah, which we need for is one way that people study this in the space of biology,
8912840	8917160	the people that study the origin of life or try to recreate life in the laboratory.
8917720	8921160	And the more interesting one is the alien one is when we go to other planets,
8921960	8927480	how do we recognize this life? Because, you know, complexity, we associate complexity,
8927480	8933480	maybe some level of mobility with life, you know, we have to be able to, like, have concrete
8934680	8941320	algorithms for, like, measuring the level of complexity we see in order to know the
8941320	8944840	difference between life and non-life. And the problem is that complexity is in the
8944840	8953480	IODB holder. So, let me give you an example. If I give you an image of the MNIST digits,
8953480	8959000	right, and I flip through MNIST digits, there is some, obviously some structure to it because
8959720	8965160	local structure, you know, neighboring pixels are correlated across the entire dataset.
8965960	8971640	Now, imagine that I apply a random permutation to all the pixels,
8972600	8977960	a fixed random permutation. Now, I show you those images, they will look, you know,
8977960	8983480	really disorganized to you, more complex. In fact, they're not more complex in absolute terms,
8983480	8987480	they're exactly the same as originally, right? And if you knew what the permutation was,
8987480	8993240	you know, you could undo the permutation. Now, imagine I give you special glasses that undo
8993240	8998680	that permutation. Now, all of a sudden, what looked complicated becomes simple. Right. So,
8998680	9003880	if you have two, if you have, you know, humans on one end, and then another race of aliens that
9003880	9007320	sees the universe with permutation glasses. Yeah, with the permutation glasses.
9008600	9012280	What we perceive as simple to them is hardly complicated, it's probably heat.
9012280	9019000	Yeah, heat, yeah. Okay. And what they perceive as simple to us is random fluctuation, it's heat.
9019000	9024280	Yeah. So, truly in the eye of the beholder, depends what kind of glasses you're wearing.
9024840	9028280	Right. Depends what kind of algorithm you're running in your perception system.
9028280	9033240	So, I don't think we'll have a theory of intelligence, self-organization, evolution,
9033240	9039320	things like that, until we have a good handle on a notion of complexity, which we know is in the
9039320	9045080	high, the eye of the beholder. Yeah, it's sad to think that we might not be able to detect or
9045080	9049160	interact with alien species because we're wearing different glasses.
9050200	9052760	Because their notion of locality might be different from ours. Yeah.
9052760	9056920	This actually connects with fascinating questions in physics at the moment, like modern physics,
9058040	9062440	quantum physics, like, you know, questions about like, you know, can we recover the information
9062440	9067640	that's lost in a black hole and things like this, right? And that relies on notions of complexity,
9069000	9071560	which, you know, I find it's fascinating.
9071560	9079080	Can you describe your personal quest to build an expressive electronic wind instrument EWI?
9079720	9083960	What is it? What does it take to build it?
9083960	9088360	Well, I'm a thinker. I like building things. I like building things with combinations of
9088360	9094280	electronics and, you know, mechanical stuff. You know, I have a bunch of different hobbies, but
9095480	9099560	you know, probably my first one was little was building model airplanes and stuff like
9099560	9104040	that. And I still do that to some extent. But also electronics, I taught myself electronics before
9104040	9110440	I studied it. And the reason I taught myself electronics is because of music. My cousin
9111400	9115560	was an aspiring electronic musician, and then he had an analog synthesizer. And I was, you know,
9115560	9120120	basically modifying it for him and building sequencers and stuff like that, right, for him.
9120120	9121800	I was in high school when I was doing this.
9122520	9127880	How's the interest in like progressive rock like 80s? Like, what's the greatest band of all time,
9127880	9132600	according to Yonah Kuhn? There's too many of them. But, you know, it's a combination of
9135800	9142760	you know, my vision orchestra, weather report, yes, Genesis, you know,
9142760	9148520	yes, Genesis, Peter Gabriel, gentle giant, you know, things like that.
9149080	9154200	Great. Okay. So this, this love of electronics and this love of music combined together.
9154200	9160280	Right. So I was actually trained to play Baroque and Renaissance music. And I played in
9161480	9166840	orchestra when I was in high school and first years of college. And I played the recorder,
9166840	9172520	Cromhorn, a little bit of oboe, you know, things like that. So I'm a wind instrument player. But
9172520	9175240	I always wanted to play improvised music, even though I don't know anything about it.
9176280	9181400	And the only way I figured, you know, short of like learning to play saxophone was to
9182120	9187000	play electronic wind instruments. So they behave on the fingering is similar to a saxophone, but,
9187000	9191000	you know, you have a wide variety of sound because you control the synthesizer with it.
9191000	9198120	So I had a bunch of those, you know, going back to the late 80s from either Yamaha or
9198120	9203640	Akai, they're both kind of the main manufacturers of those that they were classically, you know,
9203640	9208200	going back several decades. But I've never been completely satisfied with them because of lack
9208200	9213480	of expressivity. And, you know, those things, you know, are somewhat expressive. I mean,
9213480	9217000	they measure the breath pressure, they measure the lip pressure, and, you know,
9218360	9224040	you have various parameters, you can vary with fingers, but they're not really
9224040	9229320	as expressive as an acoustic instrument, right? You hear John Coltrane play two notes,
9229320	9231960	and you know it's John Coltrane, you know, it's got a unique sound.
9232920	9239560	Or Miles Davis, right? You can hear it's Miles Davis playing the trumpet because the sound
9240440	9246120	reflects their, you know, physiognomy, basically the shape of the vocal track
9248040	9253960	kind of shapes the sound. So how do you do this with an electronic instrument? And I was, many
9253960	9259400	years ago, I met a guy called David Wessel. He was a professor at Berkeley and created the
9259640	9264680	center for like, you know, music technology there. And he was interested in that question.
9265960	9270120	And so I kept kind of thinking about this for many years. And finally, because of COVID, you
9270120	9275960	know, I was at home, I was in my workshop, my workshop serves also as my kind of Zoom room
9275960	9283240	and home office. And this is in New Jersey? In New Jersey. And I started really being serious about,
9283240	9288200	you know, building my own EWI instrument. What else is going on in the New Jersey workshop? Is
9289160	9295320	some crazy stuff you built or like left on the workshop floor left behind?
9295320	9301000	A lot of crazy stuff is, you know, electronics built with microcontrollers of various kinds.
9301560	9307960	And, you know, weird flying contraptions. So you still love flying?
9308520	9315000	It's a family disease. My dad got me into it when I was a kid. And he was building
9315080	9320760	model airplanes when he was a kid. And he was a mechanical engineer. He taught himself electronics
9320760	9328840	also. So he built his early radio control systems in the late 60s, early 70s. And so that's what
9328840	9332920	got me into, I mean, he got me into kind of, you know, engineering and science and technology.
9332920	9337400	Do you also have an interest in appreciation of flight in other forms, like with drones,
9337400	9344040	quadroptors? Or do you, is it model airplane? You know, before drones were,
9345000	9351080	you know, kind of consumer products, you know, I built my own, you know, with also building
9351080	9356520	a microcontroller with JavaScripts and accelerometers for stabilization, writing the
9356520	9359480	firmware for it, you know, and then when it became kind of a standard thing you could buy,
9359480	9362120	it was boring, you know, I stopped doing it. It was in front anymore.
9363400	9369960	Yeah, you were doing it before it was cool. What advice would you give to a young person today
9369960	9375800	in high school and college that dreams of doing something big, like young like Coon,
9375800	9381560	like let's talk in the space of intelligence, dreams of having a chance to solve some fundamental
9381560	9387560	problem in space of intelligence, both for their career and just in life, being somebody who was
9387560	9395320	a part of creating something special. So try to get interested by big questions,
9395320	9401560	things like, you know, what is intelligence? What is the universe made of? What's life all about?
9401560	9410120	Things like that. Like even like crazy big questions like what's time, like nobody knows what time is.
9413080	9421880	And then learn basic things like basic methods, either from math, from physics, or from engineering.
9422840	9427640	Things that have a long shelf life. Like if you have a choice between like, you know,
9428680	9434280	learning, you know, mobile programming on iPhone, or quantum mechanics, take quantum mechanics.
9437000	9443400	Because you're going to learn things that you have no idea exist. You may not, you may never be a
9443400	9447880	quantum physicist, but you'll learn about path integrals. And path integrals are used
9448600	9452520	everywhere. It's the same formula that you use for, you know, vision integration and stuff like
9452520	9459160	that. So the ideas, the little ideas within quantum mechanics within some of these kind
9459160	9466280	of more solidified fields will have a longer shelf life, they will use somehow use indirectly in your
9466280	9472440	work. Learn classical mechanics, like you learn about Lagrangians, for example, which is like a
9472520	9477480	huge, hugely useful concept, you know, for all kinds of different things. Learn
9478360	9484120	statistical physics, because all the math that comes out of, you know, for machine learning,
9485400	9490200	basically comes out of what we got out by statistical physicists in the, you know, late 19, early 20th
9490200	9496040	century. Right. So, and for some of them, actually, more recently, by people like George O'Parisi,
9496040	9501640	who just got the Nobel Prize for the replica method, among other things, it's used for a lot
9501640	9507400	of different things, you know, variational inference, that math comes from statistical physics.
9508520	9515320	So, so a lot of those kind of, you know, basic courses, you know, you'll, if you do it
9515320	9518920	like you're engineering, you take signal processing, you'll learn about Fourier transforms.
9519800	9525800	Again, something super useful is at the basis of things like graph neural nets, which is an
9525800	9531640	entirely new subarea of, you know, AI machine learning, deep learning, which I think is super
9531640	9536040	promising for all kinds of applications. Something very promising, if you're more interested in
9536040	9541880	applications is the applications of AI machine learning and deep learning to science, or to
9543240	9548520	science that can help solve big problems in the world. I have colleagues at Meta, at fair,
9549080	9554680	who started this project called Open Catalyst, and it's an open project collaborative. And the
9554680	9562360	idea is to use deep learning to help design new chemical compounds or materials that would
9562360	9568920	facilitate the separation of hydrogen from oxygen. If you can efficiently separate oxygen from hydrogen
9568920	9577240	with electricity, you solve climate change. It's as simple as that, because you cover,
9577240	9583320	you know, some random desert with solar panels, and you have them work all day, produce hydrogen,
9583320	9586200	and then you see the hydrogen wherever it's needed. You don't need anything else.
9588440	9599000	You know, you have controllable power that can be transported anywhere. So if we have a large-scale,
9599000	9606920	efficient energy storage technology like producing hydrogen, we solve climate change. Here's another
9606920	9611480	way to solve climate change is figuring out how to make fusion work. Now, the problem with fusion
9611480	9616120	is that you make a super hot plasma, and the plasma is unstable, and you can control it.
9616120	9619720	Maybe with deep learning, you can find controllers that will stabilize plasma and make, you know,
9619720	9624440	practical fusion reactors. I mean, that's very speculative, but, you know, it's worth trying,
9624440	9631080	because, you know, the payoff is huge. There's a group at Google working on this led by John Platt.
9631080	9636760	So control, convert as many problems in science and physics and biology and chemistry
9636760	9640920	into a, into a learnable problem and see if a machine can learn it.
9641480	9646360	Right. I mean, there's properties of, you know, complex materials that we don't understand from
9646360	9653720	first principle, for example. Right. So, you know, if we could design new, you know, new materials,
9654600	9658920	we could make more efficient batteries, you know, we could make maybe faster electronics. We could,
9658920	9664680	I mean, there's a lot of things we can imagine doing, or, you know, lighter materials for,
9664680	9668360	for cars or airplanes or things like that, maybe better fuel cells. I mean, there's all kinds of
9668360	9673080	stuff we can imagine. If we had good fuel cells, hydrogen fuel cells, we could use them to power
9673080	9679400	airplanes, and, you know, transportation wouldn't be, or cars, and we wouldn't have a emission
9679400	9685880	problem, CO2 emission problems for, for air transportation anymore. So there's a lot of
9685880	9692040	those things, I think, where AI, you know, can be used. And this is not even talking about all the
9692040	9697720	sort of medicine biology and everything like that, right? You know, like protein folding,
9697720	9701800	you know, figuring out, like, how could you design your proteins, that it sticks to another protein
9701800	9707000	that a particular site, because that's how you design drugs in the end. So, you know, deep learning
9707000	9711080	would be useful, all of this. And those are kind of, you know, would be sort of enormous progress
9711080	9718200	if we could use it for that. Here's an example. If you take, this is like from recent material physics,
9718200	9725560	you take a monoatomic layer of graphene, right? So it's just carbon on an hexagonal mesh, and you
9725560	9731640	make this single, single atom thick. You put another one on top, you twist them by some magic
9732280	9737960	number of degrees, three degrees or something. It becomes superconductor. Nobody has any idea why.
9741080	9743800	I want to know how that was discovered, but that's the kind of thing that machine learning
9743800	9749320	can actually discover, these kinds of things. Well, maybe not, but there is a hint, perhaps, that
9749320	9755480	with machine learning, we would train a system to basically be a phenomenological model of some
9755480	9760120	complex emergent phenomenon, which, you know, superconductivity is one of those,
9762360	9766840	where, you know, this collective phenomenon is too difficult to describe from first principles
9766840	9772680	with the current, you know, the usual sort of reductionist type method. But we could have
9774040	9779480	deep learning systems that predict the properties of a system from a description of it after being
9779480	9788760	trained with sufficiently many samples. This guy, Pascal Foua at DPFL, he has a startup company that
9789640	9795960	where he basically trained a convolutional net essentially to predict the aerodynamic
9795960	9800520	properties of solids. And you can generate as much data as you want by just running
9800520	9808600	a computational free dynamics, right? So you give, like, a wing, a foil or something shape
9808600	9814440	of some kind, and you run computational free dynamics, you get, as a result, the drag and,
9814440	9820600	you know, lift and all that stuff, right? And you can generate lots of data, train a neural
9820600	9825240	net to make those predictions. And now what you have is a differentiable model of, let's say,
9825240	9829560	drag and lift as a function of the shape of that solid. And so you can do background and
9829560	9832680	design, you can optimize the shape, so you get the properties you want.
9834680	9840200	Yeah, that's incredible. That's incredible. And on top of all that, probably, you should read a
9840200	9846760	little bit of literature and a little bit of history for inspiration and for wisdom, because
9846760	9851640	after all, all of these technologies will have to work in a human world. And the human world is
9851640	9859240	complicated. Yeah, and this is an amazing conversation. I really honored that you
9859320	9863160	talked with me today. Thank you for all the amazing work you're doing at FAIR at Metta.
9863720	9868680	And thank you for being so passionate after all these years about everything that's going on.
9868680	9872600	You're a beacon of hope for the machine learning community. And thank you so much
9872600	9875080	for spending your valuable time with me today. That was awesome.
9875080	9877480	Thanks for having me on. That was a pleasure.
9878680	9882680	Thanks for listening to this conversation with Yann LeCun. To support this podcast,
9882680	9888680	please check out our sponsors in the description. And now let me leave you some words from Isaac
9888680	9895880	Asimov. Your assumptions are your windows on the world. Scrub them off every once in a while,
9895880	9904600	or the light won't come in. Thank you for listening and hope to see you next time.
