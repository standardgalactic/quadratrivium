1
00:00:00,000 --> 00:00:04,040
Welcome back to 6SES 099, Artificial General Intelligence.

2
00:00:04,040 --> 00:00:12,340
Today we have Ilya Setskever, co-founder and research director

3
00:00:12,340 --> 00:00:13,440
of OpenAI.

4
00:00:13,440 --> 00:00:17,320
He started in the ML group in Toronto with Geoffrey Hinton,

5
00:00:17,320 --> 00:00:19,040
then at Stanford with Andrew Ang,

6
00:00:19,040 --> 00:00:21,480
co-founded DNN Research for three years

7
00:00:21,480 --> 00:00:23,520
as a research scientist at Google Brain,

8
00:00:23,520 --> 00:00:26,280
and finally co-founded OpenAI.

9
00:00:26,280 --> 00:00:29,520
Citations aren't everything, but they

10
00:00:29,520 --> 00:00:31,160
do indicate impact.

11
00:00:31,160 --> 00:00:35,040
And his work, recent work, in the past five years,

12
00:00:35,040 --> 00:00:39,360
has been cited over 46,000 times.

13
00:00:39,360 --> 00:00:43,320
He has been the key creative intellect and driver

14
00:00:43,320 --> 00:00:45,400
behind some of the biggest breakthrough ideas

15
00:00:45,400 --> 00:00:49,480
in deep learning and artificial intelligence ever.

16
00:00:49,480 --> 00:00:51,880
So please welcome Ilya.

17
00:00:51,880 --> 00:01:02,280
All right, thanks for the introduction, Lex.

18
00:01:02,280 --> 00:01:04,560
All right, thanks for coming to my talk.

19
00:01:04,560 --> 00:01:06,160
I will tell you about some work we've

20
00:01:06,160 --> 00:01:10,280
done over the past year on meta-learning and self-play

21
00:01:10,280 --> 00:01:11,920
at OpenAI.

22
00:01:11,920 --> 00:01:15,960
And before I dive into some of the more technical details

23
00:01:15,960 --> 00:01:19,520
of the work, I want to spend a little bit of time

24
00:01:19,520 --> 00:01:24,440
talking about deep learning and why it works at all

25
00:01:24,440 --> 00:01:26,440
in the first place, which I think

26
00:01:26,440 --> 00:01:29,080
it's actually not a self-evident thing

27
00:01:29,080 --> 00:01:30,640
that they should work.

28
00:01:30,640 --> 00:01:36,720
One fact, it's actually a fact, some mathematical theorem

29
00:01:36,720 --> 00:01:42,240
that you can prove, is that if you could find the shortest

30
00:01:42,240 --> 00:01:47,320
program that does very well on your data,

31
00:01:47,320 --> 00:01:50,560
then you will achieve the best generalization possible.

32
00:01:50,560 --> 00:01:51,960
With a little bit of modification,

33
00:01:51,960 --> 00:01:54,440
you can turn it into a precise theorem.

34
00:01:54,440 --> 00:01:58,320
And on a very intuitive level, it's

35
00:01:58,320 --> 00:02:00,920
easy to see why it should be the case.

36
00:02:00,920 --> 00:02:05,040
If you have some data and you're able to find a shorter

37
00:02:05,040 --> 00:02:08,000
program which generates this data,

38
00:02:08,000 --> 00:02:10,920
then you've essentially extracted all conceivable

39
00:02:10,920 --> 00:02:13,800
regularity from this data into your program.

40
00:02:13,800 --> 00:02:15,640
And then you can use this object to make the best

41
00:02:15,640 --> 00:02:18,080
predictions possible.

42
00:02:18,080 --> 00:02:21,880
If you have data which is so complex

43
00:02:21,880 --> 00:02:26,160
that there is no way to express it as a shorter program,

44
00:02:26,160 --> 00:02:28,200
then it means that your data is totally random.

45
00:02:28,200 --> 00:02:32,400
There is no way to extract any regularity from it whatsoever.

46
00:02:32,400 --> 00:02:35,640
Now, there is little known mathematical theory behind this.

47
00:02:35,640 --> 00:02:37,200
And the proofs of these statements

48
00:02:37,200 --> 00:02:39,160
are actually not even that hard.

49
00:02:39,160 --> 00:02:42,640
But the one minor slight disappointment

50
00:02:42,640 --> 00:02:44,880
is that it's actually not possible, at least given

51
00:02:44,880 --> 00:02:47,000
today's tools and understanding,

52
00:02:47,000 --> 00:02:52,480
to find the best short program that explains or generates

53
00:02:52,480 --> 00:02:55,800
or solves your problem given your data.

54
00:02:55,800 --> 00:02:59,480
This problem is computationally intractable.

55
00:02:59,480 --> 00:03:04,040
The space of all programs is a very nasty space.

56
00:03:04,040 --> 00:03:06,760
Small changes to your program result in massive changes

57
00:03:06,760 --> 00:03:08,840
to the behavior of the program, as it should be.

58
00:03:08,840 --> 00:03:10,120
It makes sense.

59
00:03:10,120 --> 00:03:11,320
You have a loop.

60
00:03:11,320 --> 00:03:13,440
It changed the inside of the loop.

61
00:03:13,440 --> 00:03:15,840
Of course, you get something totally different.

62
00:03:15,840 --> 00:03:17,960
So the space of programs is so hard,

63
00:03:17,960 --> 00:03:19,360
at least given what we know today,

64
00:03:19,360 --> 00:03:24,480
search there seems to be completely off the table.

65
00:03:24,480 --> 00:03:28,920
Well, if we give up on short programs,

66
00:03:28,920 --> 00:03:31,960
what about small circuits?

67
00:03:31,960 --> 00:03:34,840
Well, it turns out that we are lucky.

68
00:03:34,840 --> 00:03:37,560
It turns out that when it comes to small circuits,

69
00:03:37,560 --> 00:03:40,360
you can just find the best small circuit that

70
00:03:40,360 --> 00:03:43,080
solves your problem using back propagation.

71
00:03:43,120 --> 00:03:49,600
And this is the miraculous fact on which the rest of AI stands.

72
00:03:49,600 --> 00:03:52,360
It is the fact that when you have a circuit

73
00:03:52,360 --> 00:03:54,320
and you impose constraints on your circuits,

74
00:03:54,320 --> 00:03:58,480
on your circuit using data, you can find a way

75
00:03:58,480 --> 00:04:01,120
to satisfy these constraints, these constraints

76
00:04:01,120 --> 00:04:06,000
using backprop, by iteratively making small changes

77
00:04:06,000 --> 00:04:08,240
to the weights of your neural network

78
00:04:08,240 --> 00:04:13,120
until its predictions satisfy the data.

79
00:04:13,120 --> 00:04:16,880
What this means is that the computational problem that's

80
00:04:16,880 --> 00:04:19,680
solved by back propagation is extremely profound.

81
00:04:19,680 --> 00:04:21,400
It is circuit search.

82
00:04:21,400 --> 00:04:24,800
Now, we know that you can solve it always,

83
00:04:24,800 --> 00:04:26,680
but you can solve it sometimes.

84
00:04:26,680 --> 00:04:30,480
And you can solve it at those times

85
00:04:30,480 --> 00:04:32,240
where we have a practical data set.

86
00:04:32,240 --> 00:04:35,120
It is easy to design artificial data sets for which you cannot

87
00:04:35,120 --> 00:04:36,720
find the best neural network.

88
00:04:36,720 --> 00:04:40,040
But in practice, that seems to be not a problem.

89
00:04:40,040 --> 00:04:42,240
You can think of training a neural network

90
00:04:42,240 --> 00:04:45,840
as solving a neural equation in many cases

91
00:04:45,840 --> 00:04:49,880
where you have a large number of equation terms

92
00:04:49,880 --> 00:04:52,760
like this, f of x i theta equals y i.

93
00:04:52,760 --> 00:04:54,760
So you've got your parameters, and they represent

94
00:04:54,760 --> 00:04:57,240
all your degrees of freedom.

95
00:04:57,240 --> 00:05:01,160
And you use gradient descent to push the information

96
00:05:01,160 --> 00:05:05,680
from these equations into the parameters to satisfy them all.

97
00:05:05,680 --> 00:05:08,680
And you can see that a neural network, let's say one

98
00:05:08,680 --> 00:05:14,080
with 50 layers, is basically a parallel computer that

99
00:05:14,080 --> 00:05:17,440
is given 50 time steps to run.

100
00:05:17,440 --> 00:05:19,800
And you can do quite a lot with 50 time steps

101
00:05:19,800 --> 00:05:23,960
of a very, very powerful massively parallel computer.

102
00:05:23,960 --> 00:05:29,400
So for example, I think it is not widely known

103
00:05:29,440 --> 00:05:36,520
that you can learn to sort n n bit numbers using

104
00:05:36,520 --> 00:05:40,600
a modestly sized neural network with just two hidden layers,

105
00:05:40,600 --> 00:05:42,720
which is not bad.

106
00:05:42,720 --> 00:05:45,680
It's not self-evident, especially since we've

107
00:05:45,680 --> 00:05:49,680
been taught that sorting requires log n parallel steps.

108
00:05:49,680 --> 00:05:52,720
With a neural network, you can sort successfully

109
00:05:52,720 --> 00:05:54,840
using only two parallel steps.

110
00:05:54,840 --> 00:05:58,440
So there's something slightly obvious going on.

111
00:05:58,440 --> 00:06:01,200
Now these are parallel steps of threshold neurons.

112
00:06:01,200 --> 00:06:03,160
So they're doing a little bit more work.

113
00:06:03,160 --> 00:06:04,480
That's the answer to the mystery.

114
00:06:04,480 --> 00:06:05,920
But if you've got 50 such layers,

115
00:06:05,920 --> 00:06:08,520
you can do quite a bit of logic, quite a bit of reasoning,

116
00:06:08,520 --> 00:06:10,320
all inside the neural network.

117
00:06:10,320 --> 00:06:12,520
And that's why it works.

118
00:06:12,520 --> 00:06:17,000
Given the data, we are able to find the best neural network.

119
00:06:17,000 --> 00:06:18,760
And because the neural network is deep,

120
00:06:18,760 --> 00:06:22,960
because it can run computation inside of its layers,

121
00:06:22,960 --> 00:06:25,840
the best neural network is worth finding.

122
00:06:25,840 --> 00:06:27,240
Because that's really what you need.

123
00:06:27,240 --> 00:06:29,360
You need something.

124
00:06:29,360 --> 00:06:33,480
You need a model class, which is worth optimizing.

125
00:06:33,480 --> 00:06:35,720
But it also needs to be optimizable.

126
00:06:35,720 --> 00:06:39,400
And deep neural networks satisfy both of these constraints.

127
00:06:39,400 --> 00:06:40,800
And this is why everything works.

128
00:06:40,800 --> 00:06:45,440
This is the basis on which everything else resides.

129
00:06:45,440 --> 00:06:48,160
Now I want to talk a little bit about reinforcement learning.

130
00:06:48,160 --> 00:06:50,840
So reinforcement learning is a framework.

131
00:06:50,840 --> 00:06:55,520
It's a framework of evaluating agents in their ability

132
00:06:55,520 --> 00:06:59,360
to achieve goals in complicated stochastic environments.

133
00:06:59,360 --> 00:07:01,520
You've got an agent, which is plugged into an environment,

134
00:07:01,520 --> 00:07:04,120
as shown in the figure right here.

135
00:07:04,120 --> 00:07:09,760
And for any given agent, you can simply run it many times

136
00:07:09,760 --> 00:07:12,800
and compute its average reward.

137
00:07:12,800 --> 00:07:15,160
Now, the thing that's interesting about the reinforcement

138
00:07:15,160 --> 00:07:19,960
learning framework is that there exist interesting,

139
00:07:19,960 --> 00:07:22,960
useful reinforcement learning algorithms.

140
00:07:22,960 --> 00:07:25,400
The framework existed for a long time.

141
00:07:25,400 --> 00:07:27,480
It became interesting once we realized

142
00:07:27,480 --> 00:07:29,040
that good algorithms exist.

143
00:07:29,040 --> 00:07:31,000
Now, these are not perfect algorithms,

144
00:07:31,000 --> 00:07:35,080
but they are good enough to do interesting things.

145
00:07:35,080 --> 00:07:38,280
And all you want, the mathematical problem,

146
00:07:38,280 --> 00:07:43,320
is one where you need to maximize the expected reward.

147
00:07:43,320 --> 00:07:46,960
Now, one important way in which the reinforcement learning

148
00:07:46,960 --> 00:07:49,160
framework is not quite complete is

149
00:07:49,160 --> 00:07:52,440
that it assumes that the reward is given by the environment.

150
00:07:52,440 --> 00:07:54,280
You see this picture.

151
00:07:54,280 --> 00:07:57,400
The agent sends an action, while the reward sends it

152
00:07:57,400 --> 00:08:00,000
an observation, and both the observation

153
00:08:00,000 --> 00:08:01,520
and the reward backwards.

154
00:08:01,520 --> 00:08:04,400
That's what the environment communicates back.

155
00:08:04,400 --> 00:08:07,480
The way in which this is not the case in the real world

156
00:08:07,480 --> 00:08:14,200
is that we figure out what the reward is from the observation.

157
00:08:14,200 --> 00:08:16,200
We reward ourselves.

158
00:08:16,200 --> 00:08:16,840
We are not told.

159
00:08:16,840 --> 00:08:20,240
The environment doesn't say, hey, here's some negative reward.

160
00:08:20,240 --> 00:08:22,720
It's our interpretation of our senses

161
00:08:22,720 --> 00:08:25,960
that lets us determine what the reward is.

162
00:08:25,960 --> 00:08:28,280
And there is only one real true reward in life,

163
00:08:28,280 --> 00:08:31,120
and this is existence or nonexistence.

164
00:08:31,120 --> 00:08:34,040
And everything else is a corollary of that.

165
00:08:34,040 --> 00:08:36,760
So, well, what should the agent be?

166
00:08:36,760 --> 00:08:38,000
You already know the answer.

167
00:08:38,000 --> 00:08:40,200
It should be a neural network.

168
00:08:40,200 --> 00:08:42,200
Because whenever you want to do something,

169
00:08:42,200 --> 00:08:44,080
the answer is going to be a neural network,

170
00:08:44,080 --> 00:08:47,320
and you want the agent to map observations to actions.

171
00:08:47,320 --> 00:08:49,800
So you let it be parametrized with a neural net,

172
00:08:49,800 --> 00:08:51,640
and you apply a learning algorithm.

173
00:08:51,680 --> 00:08:54,680
So I want to explain to you how reinforcement learning works.

174
00:08:54,680 --> 00:08:56,440
This is model-free reinforcement learning.

175
00:08:56,440 --> 00:08:57,760
The reinforcement learning is actually

176
00:08:57,760 --> 00:08:59,040
being used in practice everywhere.

177
00:09:01,920 --> 00:09:04,320
But it's also deeply, it's very robust.

178
00:09:04,320 --> 00:09:05,760
It's very simple.

179
00:09:05,760 --> 00:09:07,680
It's also not very efficient.

180
00:09:07,680 --> 00:09:09,040
So the way it works is the following.

181
00:09:09,040 --> 00:09:11,520
This is literally the one-sentence description

182
00:09:11,520 --> 00:09:13,760
of what happens.

183
00:09:13,760 --> 00:09:18,040
In short, try something new.

184
00:09:18,040 --> 00:09:21,160
Add randomness to your actions.

185
00:09:21,160 --> 00:09:25,040
And compare the result to your expectation.

186
00:09:25,040 --> 00:09:28,240
If the result surprises you, if you

187
00:09:28,240 --> 00:09:31,560
find that the result exceeded your expectation,

188
00:09:31,560 --> 00:09:33,880
then change your parameters to take those actions

189
00:09:33,880 --> 00:09:35,680
in the future.

190
00:09:35,680 --> 00:09:36,640
That's it.

191
00:09:36,640 --> 00:09:38,960
This is the full idea of reinforcement learning.

192
00:09:38,960 --> 00:09:40,160
Try it out.

193
00:09:40,160 --> 00:09:41,280
See if you like it.

194
00:09:41,280 --> 00:09:45,080
And if you do, do more of that in the future.

195
00:09:45,080 --> 00:09:45,960
And that's it.

196
00:09:45,960 --> 00:09:47,240
That's literally it.

197
00:09:47,240 --> 00:09:48,880
This is the core idea.

198
00:09:48,880 --> 00:09:50,680
Now, it turns out it's not difficult to formalize

199
00:09:50,680 --> 00:09:51,720
mathematically.

200
00:09:51,720 --> 00:09:53,160
But this is really what's going on.

201
00:09:53,160 --> 00:09:57,120
If in a neural network, in a regular neural network, like

202
00:09:57,120 --> 00:09:59,520
this, you might say, OK, what's the goal?

203
00:09:59,520 --> 00:10:01,040
You run the neural network.

204
00:10:01,040 --> 00:10:02,160
You get an answer.

205
00:10:02,160 --> 00:10:05,040
You compare it to the desired answer.

206
00:10:05,040 --> 00:10:06,720
And whatever difference you have between those two, you

207
00:10:06,720 --> 00:10:10,560
send it back to change the neural network.

208
00:10:10,560 --> 00:10:11,800
That's supervised learning.

209
00:10:11,800 --> 00:10:14,920
In reinforcement learning, you run a neural network.

210
00:10:14,920 --> 00:10:17,800
You add a bit of randomness to your action.

211
00:10:17,800 --> 00:10:20,400
And then if you like the result, your randomness turns

212
00:10:20,400 --> 00:10:23,360
into the desired target in effect.

213
00:10:23,360 --> 00:10:25,320
So that's it.

214
00:10:25,320 --> 00:10:28,320
Trivial.

215
00:10:28,320 --> 00:10:30,200
Now, math exists.

216
00:10:33,520 --> 00:10:36,400
Without explaining what these equations mean, the point is

217
00:10:36,400 --> 00:10:39,800
not really to derive them, but just to show that they exist.

218
00:10:39,800 --> 00:10:41,480
There are two classes of reinforcement learning

219
00:10:41,480 --> 00:10:42,600
algorithms.

220
00:10:42,600 --> 00:10:46,080
One of them is the policy gradient, where basically

221
00:10:46,080 --> 00:10:49,080
what you do is that you take this expression right there,

222
00:10:49,080 --> 00:10:53,160
the sum of rewards, and you just crunch through the

223
00:10:53,160 --> 00:10:57,960
derivatives, you expand the terms, you do some algebra,

224
00:10:57,960 --> 00:11:00,560
and you get a derivative.

225
00:11:00,560 --> 00:11:03,880
And miraculously, the derivative has exactly the

226
00:11:03,880 --> 00:11:09,360
form that I told you, which is try some actions.

227
00:11:09,360 --> 00:11:11,080
And if you like them, increase the

228
00:11:11,080 --> 00:11:12,600
low probability of the actions.

229
00:11:12,600 --> 00:11:14,040
That literally follows from the math.

230
00:11:14,040 --> 00:11:17,680
It's very nice when the intuitive explanation has a

231
00:11:17,680 --> 00:11:20,200
one-to-one correspondence to what you get in the equation.

232
00:11:20,200 --> 00:11:22,600
Even though you'll have to take my word for it, if you're not

233
00:11:22,600 --> 00:11:26,240
familiar with it, that's the equation at the top.

234
00:11:26,240 --> 00:11:27,920
Then there is a different class of reinforcement learning

235
00:11:27,920 --> 00:11:30,480
algorithms, which is a little bit more difficult to explain.

236
00:11:30,480 --> 00:11:32,720
It's called the Q learning-based algorithms.

237
00:11:32,720 --> 00:11:38,840
They are a bit less stable, a bit more sample efficient.

238
00:11:38,840 --> 00:11:44,360
And it has the property that it can learn not only from the

239
00:11:44,360 --> 00:11:47,640
data generated by the actor, but from any other data as

240
00:11:47,640 --> 00:11:48,140
well.

241
00:11:48,140 --> 00:11:52,080
So it has a different robustness profile, which will be a

242
00:11:52,080 --> 00:11:53,360
little bit important.

243
00:11:53,360 --> 00:11:56,480
But it's only going to be a technicality.

244
00:11:56,480 --> 00:11:59,240
So yeah, this is the on policy, off policy distinction.

245
00:11:59,240 --> 00:12:00,360
But it's a little bit technical.

246
00:12:00,360 --> 00:12:03,520
So if you find this hard to understand, don't worry

247
00:12:03,520 --> 00:12:04,200
about it.

248
00:12:04,200 --> 00:12:08,400
If you already know it, then you already know it.

249
00:12:08,400 --> 00:12:10,000
So now what's the potential for reinforcement learning?

250
00:12:10,000 --> 00:12:11,760
What's the promise?

251
00:12:11,760 --> 00:12:13,440
What is it actually?

252
00:12:13,440 --> 00:12:16,320
Why should we be excited about it?

253
00:12:16,320 --> 00:12:17,800
Now, there are two reasons.

254
00:12:17,800 --> 00:12:21,640
The reinforcement learning algorithms of today are already

255
00:12:21,640 --> 00:12:22,760
useful and interesting.

256
00:12:22,760 --> 00:12:25,360
And especially if you have a really good simulation of

257
00:12:25,360 --> 00:12:27,720
your world, you could train agents to do lots of

258
00:12:27,720 --> 00:12:28,960
interesting things.

259
00:12:31,120 --> 00:12:34,120
But what's really exciting is if you can build a super

260
00:12:34,120 --> 00:12:36,800
amazing sample efficient reinforcement learning

261
00:12:36,800 --> 00:12:40,040
algorithm, which is give it a tiny amount of data, and the

262
00:12:40,040 --> 00:12:42,200
algorithm just crunches through it and extracts every

263
00:12:42,200 --> 00:12:44,920
bit of entropy out of it in order to learn in

264
00:12:44,920 --> 00:12:47,160
the fastest way possible.

265
00:12:47,160 --> 00:12:49,680
Now, today our algorithms are not particularly

266
00:12:49,680 --> 00:12:50,360
data efficient.

267
00:12:50,360 --> 00:12:52,800
They are data inefficient.

268
00:12:52,800 --> 00:12:57,480
But as our field keeps making progress, this will change.

269
00:12:57,480 --> 00:13:02,760
Next, I want to dive into the topic of meta learning.

270
00:13:02,760 --> 00:13:06,000
The goal of meta learning, so meta learning is a beautiful

271
00:13:06,000 --> 00:13:10,640
idea that doesn't really work, but it kind of works.

272
00:13:10,640 --> 00:13:11,760
And it's really promising too.

273
00:13:11,760 --> 00:13:14,600
It's another promising idea.

274
00:13:14,640 --> 00:13:16,600
So what's the dream?

275
00:13:16,600 --> 00:13:19,200
We have some learning algorithms.

276
00:13:19,200 --> 00:13:21,480
Perhaps we could use those learning algorithms in order

277
00:13:21,480 --> 00:13:23,720
to learn to learn.

278
00:13:23,720 --> 00:13:26,760
It would be nice if we could learn to learn.

279
00:13:26,760 --> 00:13:28,840
So how would you do that?

280
00:13:28,840 --> 00:13:34,680
You would take a system which you train it, not on one task,

281
00:13:34,680 --> 00:13:37,640
but on many tasks, and you ask it that it learns to solve

282
00:13:37,640 --> 00:13:40,360
these tasks quickly.

283
00:13:40,360 --> 00:13:42,480
And that may actually be enough.

284
00:13:42,480 --> 00:13:43,320
So here's how it looks like.

285
00:13:43,320 --> 00:13:46,080
Here's how most traditional meta learning

286
00:13:46,080 --> 00:13:47,920
looks like.

287
00:13:47,920 --> 00:13:50,960
You have a model which is a big neural network.

288
00:13:50,960 --> 00:13:56,360
But what you do is that you treat every, instead of

289
00:13:56,360 --> 00:13:58,960
training cases, you have training tasks.

290
00:13:58,960 --> 00:14:01,240
And instead of test cases, you have test tasks.

291
00:14:01,240 --> 00:14:04,960
So your input may be, instead of just your current test

292
00:14:04,960 --> 00:14:08,520
case, it would be all the information about the test

293
00:14:08,520 --> 00:14:10,960
tasks plus the test case.

294
00:14:10,960 --> 00:14:14,000
And you'll try to output the prediction or action for that

295
00:14:14,000 --> 00:14:14,840
test case.

296
00:14:14,840 --> 00:14:17,920
So basically you say, yeah, I'm going to give you your 10

297
00:14:17,920 --> 00:14:21,640
examples as part of your input to your model, figure out how

298
00:14:21,640 --> 00:14:24,280
to make the best use of them.

299
00:14:24,280 --> 00:14:27,520
It's a really straightforward idea.

300
00:14:27,520 --> 00:14:30,880
You turn the neural network into the learning algorithm by

301
00:14:30,880 --> 00:14:34,760
turning a training task into a training case.

302
00:14:34,760 --> 00:14:37,520
So training task equals training case.

303
00:14:37,520 --> 00:14:39,120
This is meta learning.

304
00:14:39,120 --> 00:14:40,120
This one sentence.

305
00:14:43,160 --> 00:14:45,680
And so there have been several success stories which I

306
00:14:45,680 --> 00:14:48,440
think are very interesting.

307
00:14:48,440 --> 00:14:51,280
One of the success stories of meta learning is learning to

308
00:14:51,280 --> 00:14:53,400
recognize characters quickly.

309
00:14:53,400 --> 00:14:58,960
So there have been a data set produced by MIT by Lake et al.

310
00:14:58,960 --> 00:15:02,720
And this is a data set.

311
00:15:02,720 --> 00:15:04,480
We have a large number of different handwritten

312
00:15:04,480 --> 00:15:06,160
characters.

313
00:15:06,160 --> 00:15:08,860
And people have been able to train extremely strong

314
00:15:08,860 --> 00:15:11,940
meta learning system for this task.

315
00:15:11,940 --> 00:15:15,460
Another very successful example of meta learning is that of

316
00:15:15,460 --> 00:15:21,620
neural architecture search by Zopen Lee from Google, where

317
00:15:21,620 --> 00:15:24,100
they found a neural architecture that solved one

318
00:15:24,100 --> 00:15:26,180
problem well, a small problem.

319
00:15:26,180 --> 00:15:27,900
And then it could generalize, and then it would successfully

320
00:15:27,900 --> 00:15:29,260
solve large problems as well.

321
00:15:29,260 --> 00:15:34,620
So this is kind of the small number of bits meta learning.

322
00:15:34,620 --> 00:15:36,580
It's like when you learn the architecture, or maybe even

323
00:15:36,580 --> 00:15:38,420
learn a program, a small program, or a learning

324
00:15:38,420 --> 00:15:40,660
algorithm, it should apply to new tasks.

325
00:15:40,660 --> 00:15:43,500
So this is the other way of doing meta learning.

326
00:15:43,500 --> 00:15:47,060
So anyway, but the point is what's really happening in

327
00:15:47,060 --> 00:15:50,900
meta learning in most cases is that you turn a training

328
00:15:50,900 --> 00:15:54,500
task into a training case and pretend that this is totally

329
00:15:54,500 --> 00:15:56,460
normal, normal deep learning.

330
00:15:56,460 --> 00:15:57,420
That's it.

331
00:15:57,420 --> 00:15:59,380
This is the entirety of meta learning.

332
00:15:59,380 --> 00:16:03,660
Everything else suggests minor details.

333
00:16:03,660 --> 00:16:05,140
Next, I want to dive in.

334
00:16:05,140 --> 00:16:07,820
So now that I've finished the introduction section, I want to

335
00:16:07,820 --> 00:16:11,820
start discussing different work by different people from

336
00:16:11,820 --> 00:16:12,860
OpenAI.

337
00:16:12,860 --> 00:16:14,980
And I want to start by talking about hindsight

338
00:16:14,980 --> 00:16:16,620
experience replay.

339
00:16:16,620 --> 00:16:20,380
There's been a large effort by Andriy Khopich et al. to

340
00:16:20,380 --> 00:16:25,420
develop a learning algorithm for reinforcement learning that

341
00:16:25,420 --> 00:16:29,940
doesn't solve just one task, but it solves many tasks.

342
00:16:29,940 --> 00:16:33,660
And it learns to make use of its experience in a much

343
00:16:33,660 --> 00:16:35,980
more efficient way.

344
00:16:35,980 --> 00:16:37,860
And I want to discuss one problem in reinforcement

345
00:16:37,860 --> 00:16:38,620
learning.

346
00:16:38,620 --> 00:16:42,140
It's actually, I guess, a set of problems which are related to

347
00:16:42,140 --> 00:16:43,140
each other.

348
00:16:47,140 --> 00:16:49,300
But one really important thing you need to learn to do is to

349
00:16:49,300 --> 00:16:50,940
explore.

350
00:16:50,940 --> 00:16:55,100
You start out in an environment you don't know what to do.

351
00:16:55,100 --> 00:16:56,460
What do you do?

352
00:16:56,460 --> 00:16:58,660
So one very important thing that has to happen is that you

353
00:16:58,660 --> 00:17:01,140
must get rewards from time to time.

354
00:17:01,180 --> 00:17:06,260
If you try something and you don't get rewards, then how

355
00:17:06,260 --> 00:17:09,060
can you learn?

356
00:17:09,060 --> 00:17:11,380
So that's the kind of the crux of the problem.

357
00:17:11,380 --> 00:17:12,420
How do you learn?

358
00:17:12,420 --> 00:17:19,460
And relatedly, is there any way to meaningfully benefit from

359
00:17:19,460 --> 00:17:23,340
the experience, from your attempts, from your failures?

360
00:17:23,340 --> 00:17:25,780
If you try to achieve a goal and you fail, can you still

361
00:17:25,780 --> 00:17:27,220
learn from it?

362
00:17:27,220 --> 00:17:29,980
Instead of asking your algorithm to achieve a single

363
00:17:29,980 --> 00:17:32,780
goal, you want to learn a policy that can achieve a very

364
00:17:32,780 --> 00:17:34,380
large family of goals.

365
00:17:34,380 --> 00:17:37,220
For example, instead of reaching one state, you want to

366
00:17:37,220 --> 00:17:40,820
learn a policy that reaches every state of your system.

367
00:17:40,820 --> 00:17:42,740
Now, what's the implication?

368
00:17:42,740 --> 00:17:46,860
Anytime you do something, you achieve some state.

369
00:17:46,860 --> 00:17:51,340
So let's suppose you say, I want to achieve state A. I

370
00:17:51,340 --> 00:17:56,220
try my best and I end up achieving state B. I can

371
00:17:56,220 --> 00:17:58,780
either conclude, while I was disappointing, I haven't learned

372
00:17:58,780 --> 00:18:00,500
almost anything.

373
00:18:00,500 --> 00:18:04,260
I still have no idea how to achieve state A. But

374
00:18:04,260 --> 00:18:07,180
alternatively, I can say, well, wait a second, I've just

375
00:18:07,180 --> 00:18:10,660
reached a perfectly good state, which is B. Can I learn

376
00:18:10,660 --> 00:18:14,820
how to achieve state B from my attempt to achieve state A?

377
00:18:14,820 --> 00:18:16,460
And the answer is yes, you can.

378
00:18:16,460 --> 00:18:17,860
And it just works.

379
00:18:17,860 --> 00:18:20,460
And I just want to point out, this is the one case.

380
00:18:20,460 --> 00:18:25,780
There's a small subtlety here, which may be interesting to

381
00:18:25,780 --> 00:18:28,020
those of you who are very familiar with on-pot, be the

382
00:18:28,020 --> 00:18:31,260
distinction between on-policy and off-policy.

383
00:18:31,260 --> 00:18:35,380
When you try to achieve A, you're doing on-policy learning

384
00:18:35,380 --> 00:18:38,980
for reaching the state A. But you're doing off-policy

385
00:18:38,980 --> 00:18:41,700
learning for reaching the state B. Because you would take

386
00:18:41,700 --> 00:18:44,660
different actions if you would actually try to reach state B.

387
00:18:44,660 --> 00:18:46,780
So that's why it's very important that the algorithm

388
00:18:46,780 --> 00:18:49,780
you use here can support off-policy learning.

389
00:18:49,780 --> 00:18:52,020
But that's a minor technicality.

390
00:18:52,020 --> 00:18:57,660
At the crux of the idea is you make the problem easier by

391
00:18:57,660 --> 00:19:00,540
ostensibly making it harder.

392
00:19:00,540 --> 00:19:04,260
By training a system which aspires to reach, to learn to

393
00:19:04,260 --> 00:19:07,860
reach every state, to learn to achieve every goal, to learn

394
00:19:07,860 --> 00:19:12,300
to master its environment in general, you build a system

395
00:19:12,300 --> 00:19:15,100
which always learns something.

396
00:19:15,100 --> 00:19:17,620
It learns from success as well as from failure.

397
00:19:17,620 --> 00:19:20,500
Because if it tries to do one thing and it does something

398
00:19:20,500 --> 00:19:23,180
else, it now has training data for how to achieve that

399
00:19:23,180 --> 00:19:24,820
something else.

400
00:19:24,820 --> 00:19:26,340
I want to show you a video of how this thing

401
00:19:26,340 --> 00:19:27,900
works in practice.

402
00:19:27,900 --> 00:19:32,260
So one challenge in reinforcement learning systems is

403
00:19:32,260 --> 00:19:34,260
the need to shape the reward.

404
00:19:34,260 --> 00:19:36,260
So what does it mean?

405
00:19:36,260 --> 00:19:38,980
It means that at the beginning of the system, at the start

406
00:19:38,980 --> 00:19:42,340
of learning when the system doesn't know much, it will

407
00:19:42,340 --> 00:19:43,860
probably not achieve your goal.

408
00:19:43,860 --> 00:19:46,180
And so it's important that you design your reward function

409
00:19:46,180 --> 00:19:48,140
to give it gradual increments, to make it

410
00:19:48,140 --> 00:19:49,980
smooth and continuous so that even when the system is not

411
00:19:49,980 --> 00:19:52,260
very good, it achieves the goal.

412
00:19:52,260 --> 00:19:55,780
Now, if you give your system a very sparse reward, where the

413
00:19:55,780 --> 00:20:00,500
reward is achieved only when you reach a final state, then it

414
00:20:00,500 --> 00:20:03,300
becomes very hard for normal reinforcement learning

415
00:20:03,300 --> 00:20:04,540
algorithms to solve a problem.

416
00:20:04,540 --> 00:20:07,180
Because naturally, you never get the reward, so you never

417
00:20:07,180 --> 00:20:07,980
learn.

418
00:20:07,980 --> 00:20:10,100
No reward means no learning.

419
00:20:10,100 --> 00:20:13,500
But here, because you learn from failure as well as from

420
00:20:13,500 --> 00:20:17,540
success, this problem simply doesn't occur.

421
00:20:17,540 --> 00:20:19,140
And so this is nice.

422
00:20:19,140 --> 00:20:22,740
I think let's look at the videos a little bit more.

423
00:20:22,740 --> 00:20:26,300
It's nice how it confidently and energetically moves the

424
00:20:26,300 --> 00:20:29,140
little green pack to its target.

425
00:20:29,140 --> 00:20:30,380
And here's another one.

426
00:20:50,140 --> 00:20:51,740
OK, so we can skip the physics.

427
00:20:51,740 --> 00:20:54,540
It works if you do it on a physical robot as well, but we

428
00:20:54,540 --> 00:20:56,460
can skip it.

429
00:20:56,460 --> 00:20:59,940
So I think the point is that the hindsight experience replay

430
00:20:59,940 --> 00:21:05,540
algorithm is directionally correct because you want to

431
00:21:05,540 --> 00:21:08,820
make use of all your data and not only a small fraction of

432
00:21:08,820 --> 00:21:10,020
it.

433
00:21:10,020 --> 00:21:14,140
Now, one huge question is, where do you get the high level

434
00:21:14,140 --> 00:21:15,700
states?

435
00:21:15,740 --> 00:21:19,140
Where do the high level states come from?

436
00:21:19,140 --> 00:21:23,140
Because in the work that I've shown you so far, the system

437
00:21:23,140 --> 00:21:25,100
is asked to achieve low level states.

438
00:21:25,100 --> 00:21:28,060
So I think one thing that will become very important for

439
00:21:28,060 --> 00:21:31,220
these kind of approaches is representation learning and

440
00:21:31,220 --> 00:21:32,660
unsupervised learning.

441
00:21:32,660 --> 00:21:36,340
Figure out what are the right states, what's the state

442
00:21:36,340 --> 00:21:41,340
space of goals that's worth achieving?

443
00:21:41,340 --> 00:21:47,260
Now I want to go through some real meta-learning results.

444
00:21:47,260 --> 00:21:52,140
And I'll show you a very simple way of doing seem-to-real

445
00:21:52,140 --> 00:21:56,900
from simulation to the physical robot with meta-learning.

446
00:21:56,900 --> 00:22:00,860
And this is work by Pangadal, was a really nice intern

447
00:22:00,860 --> 00:22:04,060
project in 2017.

448
00:22:04,060 --> 00:22:09,180
So I think we can agree that in the domain of robotics, it

449
00:22:09,180 --> 00:22:13,820
would be nice if you could train your policy in simulation

450
00:22:13,820 --> 00:22:18,580
and then somehow this knowledge would carry over to the

451
00:22:18,580 --> 00:22:21,060
physical robot.

452
00:22:21,060 --> 00:22:27,340
Now, we can build simulators that are OK, but they can never

453
00:22:27,340 --> 00:22:30,620
perfectly match the real world unless you want to have an

454
00:22:30,620 --> 00:22:32,620
insanely slow simulator.

455
00:22:32,620 --> 00:22:38,020
And the reason for that is that it turns out that simulating

456
00:22:38,020 --> 00:22:40,260
contacts is super hard.

457
00:22:40,260 --> 00:22:43,260
And I heard somewhere, correct me if I'm wrong, that

458
00:22:43,260 --> 00:22:45,620
simulating friction is NP-complete.

459
00:22:45,620 --> 00:22:46,660
I'm not sure.

460
00:22:46,660 --> 00:22:49,780
But it's like stuff like that.

461
00:22:49,780 --> 00:22:53,900
So your simulation is just not going to match reality.

462
00:22:53,900 --> 00:22:56,260
There'll be some resemblance, but that's it.

463
00:22:56,260 --> 00:22:59,300
How can we address this problem?

464
00:22:59,300 --> 00:23:00,820
And I want to show you one simple idea.

465
00:23:01,140 --> 00:23:06,620
So let's say one thing that would be nice is that if you

466
00:23:06,620 --> 00:23:12,980
could learn a policy that would quickly adapt itself to the

467
00:23:12,980 --> 00:23:16,620
real world, well, if you want to learn a policy that can

468
00:23:16,620 --> 00:23:19,740
quickly adapt, we need to make sure that it has opportunities

469
00:23:19,740 --> 00:23:21,460
to adapt during training time.

470
00:23:21,460 --> 00:23:22,900
So what do we do?

471
00:23:22,900 --> 00:23:28,380
Instead of solving a problem in just one simulator, we

472
00:23:28,500 --> 00:23:30,820
add a huge amount of variability to the simulator.

473
00:23:30,820 --> 00:23:33,660
We say we will randomize the friction.

474
00:23:33,660 --> 00:23:37,300
So we'll randomize the masses, the length of the different

475
00:23:37,300 --> 00:23:41,700
objects and their dimensions.

476
00:23:41,700 --> 00:23:45,660
So you try to randomize physics, the simulator, in

477
00:23:45,660 --> 00:23:46,900
lots of different ways.

478
00:23:46,900 --> 00:23:50,020
And then importantly, you don't tell the policy how you

479
00:23:50,020 --> 00:23:51,740
randomized it.

480
00:23:51,740 --> 00:23:53,020
So what is it going to do then?

481
00:23:53,020 --> 00:23:55,500
You take your policy and you put it in an environment and

482
00:23:55,500 --> 00:23:57,420
it says, well, this is really tough.

483
00:23:57,420 --> 00:23:59,780
I don't know what the masses are and I don't know what the

484
00:23:59,780 --> 00:24:00,900
frictions are.

485
00:24:00,900 --> 00:24:03,700
I need to try things out and figure out what the friction

486
00:24:03,700 --> 00:24:07,300
is as I get responses from the environment.

487
00:24:07,300 --> 00:24:12,980
So you learn a certain degree of adaptability into the policy.

488
00:24:12,980 --> 00:24:14,820
And it actually works.

489
00:24:14,820 --> 00:24:16,140
I just want to show you.

490
00:24:16,140 --> 00:24:18,860
This is what happens when you just train a policy in

491
00:24:18,860 --> 00:24:22,020
simulation and deploy it on the physical robot.

492
00:24:22,020 --> 00:24:25,260
And here the guy who's going to do that, he's going to do

493
00:24:25,260 --> 00:24:29,540
the goal is to bring the hockey puck towards the red dot.

494
00:24:29,540 --> 00:24:31,860
And you will see that it will struggle.

495
00:24:37,780 --> 00:24:40,220
And the reason it struggles is because of the systematic

496
00:24:40,220 --> 00:24:44,700
differences between the simulator and the real

497
00:24:44,700 --> 00:24:47,300
physical robot.

498
00:24:47,300 --> 00:24:51,020
So even the basic movement is difficult for the policy

499
00:24:51,020 --> 00:24:53,300
because the assumptions are violated so much.

500
00:24:53,340 --> 00:24:56,020
So if you do the training as I discussed, we train a

501
00:24:56,020 --> 00:24:59,540
recurrent neural network policy which learns to quickly

502
00:24:59,540 --> 00:25:03,220
infer properties of the simulator in order to

503
00:25:03,220 --> 00:25:04,500
accomplish the task.

504
00:25:04,500 --> 00:25:07,620
You can then give it the real thing, the real physics, and

505
00:25:07,620 --> 00:25:10,060
it will do much better.

506
00:25:10,060 --> 00:25:11,780
So now this is not a perfect technique, but it's

507
00:25:11,780 --> 00:25:12,820
definitely very promising.

508
00:25:12,820 --> 00:25:15,900
It's promising whenever you are able to sufficiently

509
00:25:15,900 --> 00:25:19,020
randomize the simulator.

510
00:25:19,020 --> 00:25:22,100
So it's definitely very nice to see the closed loop nature

511
00:25:22,140 --> 00:25:22,820
of the policy.

512
00:25:22,820 --> 00:25:25,540
You can see that it would push the hockey puck, and it would

513
00:25:25,540 --> 00:25:29,180
correct it very, very gently to bring it to the goal.

514
00:25:29,180 --> 00:25:30,500
Yeah, you saw that?

515
00:25:30,500 --> 00:25:31,500
That was cool.

516
00:25:33,860 --> 00:25:39,380
So that was a cool application of meta-learning.

517
00:25:39,380 --> 00:25:41,980
I want to discuss one more application of meta-learning,

518
00:25:41,980 --> 00:25:46,740
which is learning a hierarchy of actions.

519
00:25:46,740 --> 00:25:49,260
And this was work done by France Aral.

520
00:25:49,300 --> 00:25:52,700
Actually, Kevin France, the ancient who did it, was in

521
00:25:52,700 --> 00:25:57,500
high school when he wrote this paper.

522
00:25:57,500 --> 00:26:07,100
So one thing that would be nice is if reinforcement

523
00:26:07,100 --> 00:26:09,300
learning was hierarchical.

524
00:26:09,300 --> 00:26:12,340
If instead of simply taking microactions, you had some

525
00:26:12,340 --> 00:26:16,460
kind of little subroutines that you could deploy.

526
00:26:16,460 --> 00:26:19,100
Maybe the term subroutine is a little bit too crude, but if

527
00:26:19,140 --> 00:26:23,300
you had some idea of which action primitives are

528
00:26:23,300 --> 00:26:25,420
worth starting with.

529
00:26:25,420 --> 00:26:31,860
Now, no one has been able to get actually real value add

530
00:26:31,860 --> 00:26:33,860
from hierarchical reinforcement learning yet.

531
00:26:33,860 --> 00:26:36,180
So far, all the really cool results, all the really

532
00:26:36,180 --> 00:26:37,900
convincing results of reinforcement learning, do not

533
00:26:37,900 --> 00:26:39,900
use it.

534
00:26:39,900 --> 00:26:43,420
That's because we haven't quite figured out what's the

535
00:26:43,420 --> 00:26:48,180
right way for hierarchical reinforcement learning.

536
00:26:48,180 --> 00:26:51,100
And I just want to show you one very simple approach where

537
00:26:51,100 --> 00:26:57,540
you use meta-learning to learn a hierarchy of actions.

538
00:26:57,540 --> 00:26:59,740
So here's what you do.

539
00:26:59,740 --> 00:27:07,660
You have, in this specific work, you have a certain number

540
00:27:07,660 --> 00:27:08,780
of low-level primitives.

541
00:27:08,780 --> 00:27:11,060
Let's say you have 10 of them.

542
00:27:11,060 --> 00:27:14,140
And you have a distribution of tasks.

543
00:27:14,140 --> 00:27:21,100
And your goal is to learn low-level primitives such that

544
00:27:21,100 --> 00:27:25,700
when they're used inside a very brief run of some reinforcement

545
00:27:25,700 --> 00:27:27,980
learning algorithm, you will make as much progress as

546
00:27:27,980 --> 00:27:30,060
possible.

547
00:27:30,060 --> 00:27:33,180
So the idea is you want to get the greatest amount of

548
00:27:33,180 --> 00:27:33,580
progress.

549
00:27:33,580 --> 00:27:41,140
You want to learn primitives that result in the greatest

550
00:27:41,140 --> 00:27:44,540
amount of progress possible when used inside learning.

551
00:27:44,540 --> 00:27:46,380
So this is a meta-learning setup because you need

552
00:27:46,380 --> 00:27:47,780
distribution of tasks.

553
00:27:47,780 --> 00:27:52,580
And here, we've had a little maze.

554
00:27:52,580 --> 00:27:53,780
You have a distribution of amazes.

555
00:27:53,780 --> 00:27:56,820
And in this case, the little bug learned three policies which

556
00:27:56,820 --> 00:28:00,460
move it in a fixed direction.

557
00:28:00,460 --> 00:28:02,780
And as a result of having this hierarchy, you're able to solve

558
00:28:02,780 --> 00:28:03,900
problems really fast.

559
00:28:03,900 --> 00:28:06,460
But only when the hierarchy is correct.

560
00:28:06,460 --> 00:28:08,060
So hierarchical reinforcement learning is still

561
00:28:08,060 --> 00:28:08,980
working progress.

562
00:28:08,980 --> 00:28:18,620
And this work is an interesting proof point of how

563
00:28:18,620 --> 00:28:24,020
hierarchical reinforcement learning could be like if it

564
00:28:24,020 --> 00:28:26,660
worked.

565
00:28:26,660 --> 00:28:31,540
Now, I want to just spend one slide addressing the

566
00:28:31,540 --> 00:28:35,220
limitations of high-capacity meta-learning.

567
00:28:35,220 --> 00:28:41,340
The specific limitation is that the training task

568
00:28:41,340 --> 00:28:44,100
distribution has to be equal to the test

569
00:28:44,100 --> 00:28:46,580
task distribution.

570
00:28:46,580 --> 00:28:50,700
And I think this is a real limitation because in reality,

571
00:28:50,700 --> 00:28:54,020
the new task that you want to learn will in some ways be

572
00:28:54,020 --> 00:28:58,260
fundamentally different from anything you've seen so far.

573
00:28:58,260 --> 00:29:00,540
So for example, if you go to school, you learn lots of

574
00:29:00,540 --> 00:29:02,740
useful things.

575
00:29:02,740 --> 00:29:07,340
But then when you go to work, only a fraction of the things

576
00:29:07,340 --> 00:29:09,340
that you've learned carries over.

577
00:29:09,340 --> 00:29:12,940
You need to learn quite a few more things from scratch.

578
00:29:12,940 --> 00:29:16,380
So meta-learning would struggle with that because it

579
00:29:16,380 --> 00:29:20,340
really assumes that the training data, the distribution

580
00:29:20,340 --> 00:29:22,500
over the training tasks has to be equal to the distribution

581
00:29:22,500 --> 00:29:24,100
over the test tasks.

582
00:29:24,100 --> 00:29:25,020
So that's a limitation.

583
00:29:25,020 --> 00:29:29,300
I think that as we develop better algorithms for being

584
00:29:29,300 --> 00:29:34,860
robust when the test tasks are outside of the distribution

585
00:29:34,860 --> 00:29:37,900
of the training task, then meta-learning

586
00:29:37,900 --> 00:29:40,340
would work much better.

587
00:29:40,340 --> 00:29:44,220
Now, I want to talk about self-play.

588
00:29:44,220 --> 00:29:49,100
I think self-play is a very cool topic that's starting to

589
00:29:49,100 --> 00:29:51,260
get attention only now.

590
00:29:51,260 --> 00:29:55,540
And I want to start by reviewing very old work

591
00:29:55,540 --> 00:29:57,460
called TD Gammon.

592
00:29:57,460 --> 00:30:01,380
It's back from all the way from 1992, so it's 26 years old

593
00:30:01,380 --> 00:30:02,420
now.

594
00:30:02,420 --> 00:30:04,140
It was done by Jared Sauron.

595
00:30:04,140 --> 00:30:13,500
So this work is really incredible because it has so much

596
00:30:13,500 --> 00:30:15,060
relevance today.

597
00:30:15,060 --> 00:30:20,260
What they did, basically, they said, OK, let's take two

598
00:30:20,260 --> 00:30:25,900
neural networks and let them play against each other, let

599
00:30:25,900 --> 00:30:29,300
them play backgammon against each other, and let them be

600
00:30:29,300 --> 00:30:31,260
trained with Q-Learning.

601
00:30:31,260 --> 00:30:33,940
So it's a super modern approach.

602
00:30:33,940 --> 00:30:39,140
And you would think this was a paper from 2017, except that

603
00:30:39,140 --> 00:30:41,540
when you look at this plot, it shows that you only have 10

604
00:30:41,540 --> 00:30:44,300
hidden units, 20 hidden units, 40 and 80 for the

605
00:30:44,300 --> 00:30:48,100
different colors, where you notice that the largest

606
00:30:48,100 --> 00:30:49,580
neural network works best.

607
00:30:49,580 --> 00:30:52,300
So in some ways, not much has changed, and this is the

608
00:30:52,300 --> 00:30:53,300
evidence.

609
00:30:54,300 --> 00:30:56,700
And in fact, they were able to beat the world champion in

610
00:30:56,700 --> 00:30:59,100
backgammon, and they were able to discover new strategies

611
00:30:59,100 --> 00:31:05,100
that the best human backgammon players have not noticed.

612
00:31:05,100 --> 00:31:07,100
And they've determined that the strategy is covered by TD

613
00:31:07,100 --> 00:31:08,900
Gammon are actually better.

614
00:31:08,900 --> 00:31:14,700
So that's pure self-play with Q-Learning, which remained

615
00:31:14,700 --> 00:31:19,100
dormant until the DQN worked with Atari, but in mind.

616
00:31:19,300 --> 00:31:27,300
So now other examples of self-play include AlphaGo Zero,

617
00:31:27,300 --> 00:31:29,900
which was able to learn to beat the world champion in Go

618
00:31:29,900 --> 00:31:33,700
without using any external data whatsoever.

619
00:31:33,700 --> 00:31:36,700
And now the result of this way is Biopen AI, which is our

620
00:31:36,700 --> 00:31:40,700
Dota 2 bot, which was able to build the world champion on

621
00:31:40,700 --> 00:31:41,900
the 1v1 version of the game.

622
00:31:42,100 --> 00:31:45,100
And so I want to spend a little bit of time talking about the

623
00:31:45,100 --> 00:31:49,100
allure of self-play and why I think it's exciting.

624
00:31:51,100 --> 00:32:00,100
So one important problem that we must face as we try to build

625
00:32:00,100 --> 00:32:05,100
truly intelligent systems is what is the task?

626
00:32:05,100 --> 00:32:07,100
What are we actually doing?

627
00:32:07,300 --> 00:32:12,300
And one very attractive attribute of self-play is that

628
00:32:12,300 --> 00:32:15,300
the agents create the environment.

629
00:32:15,300 --> 00:32:19,300
By virtue of the agent acting in the environment,

630
00:32:19,300 --> 00:32:22,300
the environment becomes difficult for the other agents.

631
00:32:22,300 --> 00:32:26,300
And you can see here an example of an iguana interacting

632
00:32:26,300 --> 00:32:28,300
with snakes that try to eat it.

633
00:32:28,300 --> 00:32:34,300
And that's why we're trying to build the world champion

634
00:32:34,300 --> 00:32:37,500
of the iguana interacting with snakes that try to eat it

635
00:32:37,500 --> 00:32:39,500
unsuccessfully this time.

636
00:32:39,500 --> 00:32:42,500
So we can see what will happen in a moment.

637
00:32:42,500 --> 00:32:44,500
The iguana is trying its best.

638
00:32:44,500 --> 00:32:48,500
And so the fact that you have this arms race between the

639
00:32:48,500 --> 00:32:52,500
snakes and the iguana motivates their development,

640
00:32:52,500 --> 00:32:55,500
potentially without bound.

641
00:32:55,500 --> 00:33:00,500
And this is what happened in effect in biological evolution.

642
00:33:00,700 --> 00:33:04,700
Now, interesting work in this direction was done in 1994

643
00:33:04,700 --> 00:33:05,700
by Carl Simms.

644
00:33:05,700 --> 00:33:09,700
There is a really cool video on YouTube by Carl Simms.

645
00:33:09,700 --> 00:33:12,200
You should check it out, which really kind of shows all the

646
00:33:12,200 --> 00:33:14,200
work that he's done.

647
00:33:14,200 --> 00:33:17,200
And here you have a little competition between agents

648
00:33:17,200 --> 00:33:20,200
where you evolve both the behavior and the morphology

649
00:33:20,200 --> 00:33:25,200
when the agent is trying to gain possession of a green cube.

650
00:33:27,200 --> 00:33:30,200
And so you can see that the agents create the challenge

651
00:33:30,200 --> 00:33:31,200
for each other.

652
00:33:31,200 --> 00:33:33,200
And that's why they need to develop.

653
00:33:34,900 --> 00:33:40,900
So one thing that we did, and this is work by Ansel et al

654
00:33:40,900 --> 00:33:43,900
from OpenAI, is we said, okay, well,

655
00:33:43,900 --> 00:33:48,600
can we demonstrate some unusual results in self play

656
00:33:48,600 --> 00:33:52,400
that would really convince us that there is something there?

657
00:33:52,400 --> 00:33:56,900
So what we did here is that we created a small ring

658
00:33:56,900 --> 00:33:58,800
and you have these two humanoid figures.

659
00:33:58,800 --> 00:34:02,700
And their goal is just to push each other outside the ring.

660
00:34:02,700 --> 00:34:05,000
And they don't know anything about wrestling.

661
00:34:05,000 --> 00:34:07,300
They don't know anything about standing your balance

662
00:34:07,300 --> 00:34:08,000
in each other.

663
00:34:08,000 --> 00:34:10,100
They don't know anything about centers of gravity.

664
00:34:10,100 --> 00:34:13,100
All they know is that if you don't do a good job,

665
00:34:13,100 --> 00:34:16,700
then your competition is going to do a better job.

666
00:34:16,700 --> 00:34:20,400
Now, one of the really attractive things about self play

667
00:34:20,400 --> 00:34:25,900
is that you always have an opponent that's roughly

668
00:34:25,900 --> 00:34:28,400
as good as you are.

669
00:34:28,400 --> 00:34:31,400
In order to learn, you need to sometimes win

670
00:34:31,400 --> 00:34:32,700
and sometimes lose.

671
00:34:32,700 --> 00:34:34,700
You can't always win.

672
00:34:34,700 --> 00:34:36,100
Sometimes you must fail.

673
00:34:36,100 --> 00:34:39,600
Sometimes you must succeed.

674
00:34:39,600 --> 00:34:42,000
So let's see what will happen here.

675
00:34:42,000 --> 00:34:43,700
Yeah, so it was able to be.

676
00:34:43,700 --> 00:34:48,400
So the green humanoid was able to block the ball.

677
00:34:48,400 --> 00:34:52,900
In a well-balanced self play environment,

678
00:34:52,900 --> 00:34:55,500
the competition is always level.

679
00:34:55,500 --> 00:34:58,400
No matter how good you are or how bad you are,

680
00:34:58,400 --> 00:35:01,600
you have a competition that makes it exactly

681
00:35:01,600 --> 00:35:03,100
of exactly the right challenge for you.

682
00:35:03,100 --> 00:35:04,100
Oh, and one thing here.

683
00:35:04,100 --> 00:35:06,300
So this video shows transfer learning.

684
00:35:06,300 --> 00:35:09,100
You take the little wrestling humanoid

685
00:35:09,100 --> 00:35:11,300
and you take its friend away

686
00:35:11,300 --> 00:35:14,500
and you start applying big, large, random forces on it.

687
00:35:14,500 --> 00:35:17,000
And you see if it can maintain its balance.

688
00:35:17,000 --> 00:35:19,900
And the answer turns out to be that yes, it can

689
00:35:19,900 --> 00:35:23,000
because it's been trained against an opponent

690
00:35:23,000 --> 00:35:24,500
and it pushes it.

691
00:35:24,500 --> 00:35:27,300
And so that's why even if it doesn't understand

692
00:35:27,300 --> 00:35:29,500
where the pressure force is being applied on it,

693
00:35:29,500 --> 00:35:31,500
it's still able to balance itself.

694
00:35:31,500 --> 00:35:35,000
So this is one potentially attractive feature

695
00:35:35,000 --> 00:35:36,600
of self play environments that you could learn

696
00:35:36,600 --> 00:35:40,200
a certain broad set of skills.

697
00:35:40,200 --> 00:35:42,300
Although it's a little hard to control those

698
00:35:42,300 --> 00:35:43,800
what the skills will be.

699
00:35:43,800 --> 00:35:45,900
And so the biggest open question with this research

700
00:35:45,900 --> 00:35:52,300
is how do you learn agents in a self play environment

701
00:35:52,300 --> 00:35:54,600
such that they do whatever they do,

702
00:35:54,600 --> 00:35:57,000
but then they are able to solve a battery of tasks

703
00:35:57,000 --> 00:35:58,200
that is useful for us,

704
00:35:58,200 --> 00:36:01,800
that is explicitly specified externally.

705
00:36:01,800 --> 00:36:05,000
Yeah.

706
00:36:05,000 --> 00:36:08,400
I also want to highlight one attribute

707
00:36:08,400 --> 00:36:09,500
of self play environments

708
00:36:09,500 --> 00:36:12,000
that we've observed in our Dota bot.

709
00:36:12,000 --> 00:36:14,400
And that is that we've seen a very rapid increase

710
00:36:14,400 --> 00:36:16,100
in the competence of the bot.

711
00:36:16,100 --> 00:36:19,000
So over the period, over the course of maybe five months,

712
00:36:19,000 --> 00:36:23,900
we've seen the bot go from playing totally randomly

713
00:36:23,900 --> 00:36:28,100
all the way to the world champion.

714
00:36:28,100 --> 00:36:29,400
And the reason for that

715
00:36:29,400 --> 00:36:32,200
is that once you have a self play environment,

716
00:36:32,200 --> 00:36:36,400
if you put compute into it, you turn it into data.

717
00:36:36,400 --> 00:36:40,300
Self play allows you to turn compute into data.

718
00:36:40,300 --> 00:36:42,300
And I think we will see a lot more of that

719
00:36:42,300 --> 00:36:44,900
as being an extremely important thing

720
00:36:44,900 --> 00:36:45,900
to be able to turn compute

721
00:36:45,900 --> 00:36:48,700
into essentially data or generalization.

722
00:36:48,700 --> 00:36:51,700
Simply because the speed of neural net processors

723
00:36:51,700 --> 00:36:54,900
will increase very dramatically over the next few years.

724
00:36:54,900 --> 00:36:56,600
So neural net cycles will be cheap

725
00:36:56,600 --> 00:36:58,800
and it will be important to make use of these

726
00:36:58,800 --> 00:37:03,300
newly found over abundance of cycles.

727
00:37:03,300 --> 00:37:04,400
I also want to talk a little bit

728
00:37:04,400 --> 00:37:08,400
about the end game of the self play approach.

729
00:37:08,400 --> 00:37:12,500
So one thing that we know about the human brain

730
00:37:12,500 --> 00:37:15,100
is that it has increased in size fairly rapidly

731
00:37:15,100 --> 00:37:18,700
over the past two million years.

732
00:37:18,700 --> 00:37:21,900
My theory, the reason I think it happened

733
00:37:21,900 --> 00:37:26,600
is because our ancestors got to a point

734
00:37:26,600 --> 00:37:29,700
where the thing that's most important for your survival

735
00:37:29,700 --> 00:37:32,200
is your standing in the tribe

736
00:37:32,200 --> 00:37:34,800
and less the tiger in the lion.

737
00:37:34,800 --> 00:37:36,900
Once the most important thing

738
00:37:36,900 --> 00:37:39,200
is how you deal with those other things

739
00:37:39,200 --> 00:37:40,600
which have a large brain,

740
00:37:40,600 --> 00:37:43,200
then it really helps to have a slightly larger brain.

741
00:37:43,200 --> 00:37:44,500
And I think that's what happened.

742
00:37:44,500 --> 00:37:47,700
And there exists at least one paper from science

743
00:37:47,700 --> 00:37:50,200
which supports this point of view.

744
00:37:50,200 --> 00:37:52,400
So apparently there has been convergent evolution

745
00:37:52,400 --> 00:37:56,700
between social apes and social birds

746
00:37:56,700 --> 00:38:00,800
even though in terms of various behaviors.

747
00:38:00,800 --> 00:38:04,300
Even though the divergence in evolution

748
00:38:04,300 --> 00:38:06,500
and timescale between humans and birds

749
00:38:06,500 --> 00:38:08,600
has occurred a very long time ago

750
00:38:08,600 --> 00:38:11,500
and humans and sorry humans apes and humans apes

751
00:38:11,500 --> 00:38:16,500
and birds have very different brain structure.

752
00:38:16,500 --> 00:38:19,900
So I think what should happen if we succeed,

753
00:38:19,900 --> 00:38:23,200
if we successfully follow the path of this approach

754
00:38:23,200 --> 00:38:25,000
is that we should create a society of agents

755
00:38:25,000 --> 00:38:28,700
which will have language and theory of mind,

756
00:38:28,700 --> 00:38:33,600
negotiation, social skills, trade, economy, politics,

757
00:38:33,600 --> 00:38:35,100
justice system.

758
00:38:35,100 --> 00:38:37,000
All these things should happen

759
00:38:37,000 --> 00:38:39,000
inside the multi-agent environment.

760
00:38:39,000 --> 00:38:40,600
And there will also be some alignment issue

761
00:38:40,600 --> 00:38:43,100
of how do you make sure that the agents we learn

762
00:38:43,100 --> 00:38:45,200
behave in a way that we want.

763
00:38:45,200 --> 00:38:49,800
Now I want to make a speculative digression here

764
00:38:49,800 --> 00:38:57,100
which is, I want to make the following observation.

765
00:38:57,100 --> 00:39:02,000
If you believe that this kind of society of agents

766
00:39:02,000 --> 00:39:08,500
is a plausible place where truly,

767
00:39:08,500 --> 00:39:12,200
where fully general intelligence will emerge.

768
00:39:12,200 --> 00:39:16,700
And if you accept that our experience with the dotabot

769
00:39:16,700 --> 00:39:18,700
where we've seen a very rapid increase in competence

770
00:39:18,700 --> 00:39:22,000
will carry over once all the details are right.

771
00:39:22,000 --> 00:39:24,600
If you assume both of these conditions

772
00:39:24,600 --> 00:39:28,000
then it should follow that we should see a very rapid increase

773
00:39:28,000 --> 00:39:30,700
in the competence of our agents

774
00:39:30,700 --> 00:39:34,500
as they leave in the society of agents.

775
00:39:34,500 --> 00:39:40,300
So now that we've talked about a potentially interesting way

776
00:39:40,300 --> 00:39:43,100
of increasing the competence and teaching agents

777
00:39:43,100 --> 00:39:46,600
social skills and language and a lot of things

778
00:39:46,600 --> 00:39:49,200
that actually exist in humans as well.

779
00:39:49,200 --> 00:39:53,700
We want to talk a little bit about how you convey goals

780
00:39:53,700 --> 00:39:55,700
to agents.

781
00:39:55,700 --> 00:39:59,500
And the question of conveying goals to agents

782
00:39:59,500 --> 00:40:01,400
is just a technical problem.

783
00:40:01,400 --> 00:40:04,700
But it will be important

784
00:40:04,700 --> 00:40:08,100
because it is more likely than not

785
00:40:08,100 --> 00:40:12,100
that the agents that we will train will eventually

786
00:40:12,100 --> 00:40:14,300
be dramatically smarter than us.

787
00:40:14,300 --> 00:40:17,500
And this is work by the OpenAI safety team

788
00:40:17,500 --> 00:40:21,700
by Paul Cruciano and others.

789
00:40:21,700 --> 00:40:23,500
So I'm just going to show you this video

790
00:40:23,500 --> 00:40:27,600
which basically explains how the whole thing works.

791
00:40:27,600 --> 00:40:30,600
There is some behavior you are looking for

792
00:40:30,600 --> 00:40:34,700
and you the human gets to see pairs of behaviors.

793
00:40:34,700 --> 00:40:39,500
And you simply click on the one that looks better.

794
00:40:39,500 --> 00:40:44,200
And after a very modest number of clicks

795
00:40:44,200 --> 00:40:57,500
you can get this little simulated leg to do backflips.

796
00:40:57,500 --> 00:40:58,800
There you go.

797
00:40:58,800 --> 00:41:00,000
You can now do backflips.

798
00:41:00,000 --> 00:41:02,300
And to get this specific behavior

799
00:41:02,300 --> 00:41:08,000
it took about 500 clicks by human annotators.

800
00:41:08,000 --> 00:41:10,500
The way it works is that you take all the...

801
00:41:10,500 --> 00:41:13,300
So this is a very data efficient reinforcement learning

802
00:41:13,300 --> 00:41:17,100
algorithm but it is efficient in terms of rewards

803
00:41:17,100 --> 00:41:20,400
and not in terms of the environment interactions.

804
00:41:20,400 --> 00:41:22,900
So what you do here is that you take all the clicks

805
00:41:22,900 --> 00:41:24,200
so you've got your...

806
00:41:24,200 --> 00:41:27,700
Here is one behavior which is better than the other.

807
00:41:27,700 --> 00:41:32,400
You fit a reward function, a numerical reward function

808
00:41:32,400 --> 00:41:33,500
to those clicks.

809
00:41:33,500 --> 00:41:34,800
So you want to fit a reward function

810
00:41:34,800 --> 00:41:36,100
which satisfies those clicks

811
00:41:36,100 --> 00:41:37,600
and then you optimize this reward function

812
00:41:37,600 --> 00:41:39,500
with reinforcement learning.

813
00:41:39,500 --> 00:41:41,200
And it actually works.

814
00:41:41,200 --> 00:41:44,800
So this requires 500 bits of information.

815
00:41:44,800 --> 00:41:48,000
You've also been able to train lots of Atari games

816
00:41:48,000 --> 00:41:49,500
using several thousand bits of information.

817
00:41:49,500 --> 00:41:53,000
So in all these cases you had human annotators

818
00:41:53,000 --> 00:41:56,600
or human judges just like in the previous slide

819
00:41:56,700 --> 00:42:00,500
looking at pairs of trajectories

820
00:42:00,500 --> 00:42:03,900
and clicking on the one that they thought was better.

821
00:42:03,900 --> 00:42:08,000
And here's an example of an unusual goal

822
00:42:08,000 --> 00:42:10,000
where this is a car racing game

823
00:42:10,000 --> 00:42:16,100
but the goal was to ask the agent to train the white car

824
00:42:16,100 --> 00:42:18,800
drive right behind the orange car.

825
00:42:18,800 --> 00:42:19,800
So it's a different goal

826
00:42:19,800 --> 00:42:23,400
and it was very straightforward to communicate this goal

827
00:42:23,400 --> 00:42:26,500
using this approach.

828
00:42:27,000 --> 00:42:31,300
So then to finish off, alignment is a technical problem.

829
00:42:31,300 --> 00:42:33,000
It has to be solved.

830
00:42:33,000 --> 00:42:36,300
But of course the determination of the correct goals

831
00:42:36,300 --> 00:42:38,600
we want RA systems to have

832
00:42:38,600 --> 00:42:42,100
will be a very challenging political problem.

833
00:42:42,100 --> 00:42:43,600
And on this note,

834
00:42:43,600 --> 00:42:45,600
I want to thank you so much for your attention

835
00:42:45,600 --> 00:42:48,200
and I just want to say that it will be a happy hour

836
00:42:48,200 --> 00:42:50,400
at Cambridge Brewing Company at 8.45.

837
00:42:50,400 --> 00:42:53,400
If you want to chat more about AI and other topics,

838
00:42:53,400 --> 00:42:54,800
please come by.

839
00:42:54,800 --> 00:42:56,500
I think that deserves an applause.

840
00:42:56,500 --> 00:42:57,600
Thank you very much.

841
00:43:03,600 --> 00:43:07,400
So back propagation is a...

842
00:43:07,400 --> 00:43:09,100
or neural networks are inspired

843
00:43:09,100 --> 00:43:10,700
but back propagation doesn't look as though

844
00:43:10,700 --> 00:43:12,500
it's what's going on in the brain

845
00:43:12,500 --> 00:43:15,800
because signals in the brain go one direction down the axons

846
00:43:15,800 --> 00:43:17,700
whereas back propagation requires the errors

847
00:43:17,700 --> 00:43:20,700
to be propagated back up the wires.

848
00:43:20,700 --> 00:43:24,200
So can you just talk a little bit

849
00:43:24,200 --> 00:43:26,200
about that whole situation

850
00:43:26,200 --> 00:43:28,600
where it looks as though the brain is doing something a bit different

851
00:43:28,600 --> 00:43:31,400
than our highly successful algorithms?

852
00:43:31,400 --> 00:43:33,500
Are algorithms going to be improved

853
00:43:33,500 --> 00:43:35,300
once we figure out what the brain is doing

854
00:43:35,300 --> 00:43:37,400
or is the brain really sending signals back

855
00:43:37,400 --> 00:43:40,200
even though it's got no obvious way of doing that?

856
00:43:40,200 --> 00:43:42,300
What's happening in that area?

857
00:43:42,300 --> 00:43:44,400
So that's a great question.

858
00:43:44,400 --> 00:43:46,500
So first of all, I'll say that the true answer

859
00:43:46,500 --> 00:43:48,900
is that the honest answer is that I don't know

860
00:43:48,900 --> 00:43:50,600
but I have opinions.

861
00:43:50,700 --> 00:43:55,000
And so I'll say two things.

862
00:43:55,000 --> 00:43:58,400
Like first of all, given that...

863
00:43:58,400 --> 00:44:01,200
like if you agree, if we agree like so rather

864
00:44:01,200 --> 00:44:04,000
it is a true fact that back propagation

865
00:44:04,000 --> 00:44:08,000
solves the problem of circuit search.

866
00:44:08,000 --> 00:44:12,200
This problem feels like an extremely fundamental problem

867
00:44:12,200 --> 00:44:15,100
and for this reason I think that it's unlikely to go away.

868
00:44:15,100 --> 00:44:18,600
Now you also write that the brain doesn't obviously

869
00:44:18,600 --> 00:44:21,000
do back propagation although there have been multiple proposals

870
00:44:21,000 --> 00:44:22,900
of how it could be doing them.

871
00:44:22,900 --> 00:44:29,000
For example, there's been a work by Tim Lillicrap and others

872
00:44:29,000 --> 00:44:31,100
where they've shown that if you use...

873
00:44:31,100 --> 00:44:34,400
that it's possible to learn a different set of connections

874
00:44:34,400 --> 00:44:36,700
that can be used for the backward pass

875
00:44:36,700 --> 00:44:38,700
and that can result in successful learning.

876
00:44:38,700 --> 00:44:42,500
Now, the reason this hasn't been like really pushed to the limit

877
00:44:42,500 --> 00:44:44,000
by practitioners is because they say,

878
00:44:44,000 --> 00:44:46,500
well, I got tf.gradients,

879
00:44:46,500 --> 00:44:48,600
I'm just not going to worry about it.

880
00:44:48,600 --> 00:44:50,600
But you are right that this is an important issue

881
00:44:50,600 --> 00:44:53,400
and one of two things is going to happen.

882
00:44:53,400 --> 00:44:56,000
So my personal opinion is that back propagation

883
00:44:56,000 --> 00:44:58,200
is just going to stay with us till the very end

884
00:44:58,200 --> 00:45:01,800
and we'll actually build fully human level and beyond systems

885
00:45:01,800 --> 00:45:06,200
before we understand how the brain does what it does.

886
00:45:06,200 --> 00:45:09,600
So that's what I believe but of course

887
00:45:09,600 --> 00:45:12,600
it is a difference that has to be acknowledged.

888
00:45:12,600 --> 00:45:14,200
Okay, thank you.

889
00:45:14,300 --> 00:45:18,200
Do you think it was a fair matchup for the Dota bot

890
00:45:18,200 --> 00:45:21,700
and that person given the constraints of the system?

891
00:45:21,700 --> 00:45:27,100
So I'd say that the biggest advantage computers have in games like this,

892
00:45:27,100 --> 00:45:32,000
like one of the big advantages is that they obviously have a better reaction time.

893
00:45:32,000 --> 00:45:33,900
Although in Dota in particular,

894
00:45:33,900 --> 00:45:39,100
the number of clicks per second over the top layer is fairly small,

895
00:45:39,100 --> 00:45:40,600
which is different from StarCraft.

896
00:45:40,600 --> 00:45:44,700
So in StarCraft, StarCraft is a very mechanically heavy game

897
00:45:44,700 --> 00:45:46,700
because you have a large number of units

898
00:45:46,700 --> 00:45:50,600
and so the top layers they just click all the time.

899
00:45:50,600 --> 00:45:53,400
In Dota, every player control is just one hero

900
00:45:53,400 --> 00:45:56,900
and so that greatly reduces the total number of actions they need to make.

901
00:45:56,900 --> 00:45:58,500
Now still precision matters.

902
00:45:58,500 --> 00:46:02,200
I think that we'll discover that,

903
00:46:02,200 --> 00:46:05,500
but what I think will really happen is that we'll discover that computers have

904
00:46:05,600 --> 00:46:12,600
the advantage in any domain or rather every domain.

905
00:46:14,600 --> 00:46:15,100
Not yet.

906
00:46:15,100 --> 00:46:18,100
So do you think that the emergent behaviors from the agent

907
00:46:18,100 --> 00:46:20,500
were actually kind of directed

908
00:46:20,500 --> 00:46:22,700
because the constraints were already kind of in place?

909
00:46:22,700 --> 00:46:24,100
Like so it was kind of forced to discover those

910
00:46:24,100 --> 00:46:28,000
or do you think that like that was actually something quite novel

911
00:46:28,000 --> 00:46:31,000
that like, wow, it actually discovered these on its own?

912
00:46:31,000 --> 00:46:33,600
Like you didn't actually bias towards constraining it?

913
00:46:33,600 --> 00:46:35,600
So it's definitely de-discovering new strategies

914
00:46:35,600 --> 00:46:39,300
and I can share an anecdote where our tester,

915
00:46:39,300 --> 00:46:42,100
we have a pro which would test the bot

916
00:46:42,100 --> 00:46:44,600
and he played against it for a long time

917
00:46:44,600 --> 00:46:48,000
and the bot would do all kinds of things against the player,

918
00:46:48,000 --> 00:46:50,100
the human player, which were effective.

919
00:46:50,100 --> 00:46:55,000
Then at some point that pro decided to play against the better pro

920
00:46:55,000 --> 00:46:58,200
and he decided to imitate one of the things that the bot was doing

921
00:46:58,200 --> 00:47:03,400
and by imitating it he was able to defeat a better pro.

922
00:47:03,400 --> 00:47:06,200
So I think the strategies that he discovers are real

923
00:47:06,200 --> 00:47:11,500
and so like it means that there's very real transfer between, you know,

924
00:47:11,500 --> 00:47:15,800
I would say I think what that means is that the,

925
00:47:15,800 --> 00:47:18,500
because the strategies discovered by the bot help the humans,

926
00:47:18,500 --> 00:47:23,000
it means that the fundamental gameplay is deeply related.

927
00:47:23,000 --> 00:47:27,400
For a long time now I've heard that the objective of reinforcement learning

928
00:47:27,400 --> 00:47:31,300
is to determine a policy that chooses an action

929
00:47:31,300 --> 00:47:36,200
to maximize the expected reward, which is what you said earlier.

930
00:47:36,200 --> 00:47:41,500
Would you ever want to look at the standard deviation of possible rewards?

931
00:47:41,500 --> 00:47:42,700
Does that even make sense?

932
00:47:42,700 --> 00:47:47,500
Yeah, I mean I think for sure, I think it's really application dependent.

933
00:47:47,500 --> 00:47:50,900
One of the reasons to maximize the expected reward

934
00:47:50,900 --> 00:47:54,700
is because it's easier to design algorithms for it.

935
00:47:54,700 --> 00:47:59,200
So you write down this equation, the formula,

936
00:47:59,200 --> 00:48:00,800
you do a little bit of derivation,

937
00:48:00,800 --> 00:48:03,900
you get something which amounts to a nice-looking algorithm.

938
00:48:03,900 --> 00:48:08,700
Now, I think there exists like really,

939
00:48:08,700 --> 00:48:11,000
there exist applications where you never want to make mistakes

940
00:48:11,000 --> 00:48:13,900
and you want to work on the standard deviation as well.

941
00:48:13,900 --> 00:48:17,800
But in practice it seems that the, just looking at the expected reward

942
00:48:17,800 --> 00:48:24,900
covers a large fraction of the situation as you'd like to apply this to.

943
00:48:24,900 --> 00:48:25,600
Okay, thanks.

944
00:48:25,600 --> 00:48:26,100
Thank you.

945
00:48:29,500 --> 00:48:37,100
We talked last week about motivations and that has a lot to do with the reinforcement.

946
00:48:37,100 --> 00:48:43,800
And some of the ideas is that the, our motivations are actually connection

947
00:48:43,800 --> 00:48:46,300
with others and cooperation.

948
00:48:46,300 --> 00:48:50,300
And I'm wondering if, Thurnoff, and I understand it's very popular

949
00:48:50,300 --> 00:48:54,800
to have the computers play these competitive games.

950
00:48:54,800 --> 00:49:03,700
But is there any use in like having an agent self-play collaboratively, collaborative games?

951
00:49:03,700 --> 00:49:07,600
Yeah, I think that's an extremely good question.

952
00:49:07,600 --> 00:49:10,800
I think one place from which we can get some inspiration

953
00:49:10,800 --> 00:49:13,500
is from the evolution of cooperation.

954
00:49:13,500 --> 00:49:19,700
Like, I think cooperation, we cooperate ultimately

955
00:49:19,700 --> 00:49:24,800
because it's much better for you, the person to be cooperative than not.

956
00:49:24,800 --> 00:49:33,500
And so, I think what should happen, if you have a sufficiently open-ended game,

957
00:49:33,500 --> 00:49:36,700
then cooperation will be the winning strategy.

958
00:49:36,700 --> 00:49:43,800
And so I think we will get cooperation, whether we like it or not.

959
00:49:43,900 --> 00:49:50,100
Hey, you mentioned the complexity of the simulation of friction.

960
00:49:50,100 --> 00:49:53,600
I was wondering if you feel that there exists open complexity,

961
00:49:53,600 --> 00:49:56,800
theoretic problems relevant to, relevant to AI,

962
00:49:56,800 --> 00:49:59,700
or whether it's just a matter of finding good approximations

963
00:49:59,700 --> 00:50:04,100
that humans, of the types of problems that humans tend to solve.

964
00:50:04,100 --> 00:50:07,100
Yeah, so complexity theory.

965
00:50:07,100 --> 00:50:10,800
Well, like at a very basic level,

966
00:50:10,900 --> 00:50:13,900
we know that whatever algorithm we're going to run

967
00:50:13,900 --> 00:50:16,900
is going to run fairly efficiently on some hardware.

968
00:50:16,900 --> 00:50:23,400
So that puts a pretty strict upper bound on the true complexity of the problems we are solving.

969
00:50:23,400 --> 00:50:28,700
Like, by definition, we are solving problems which aren't too hard in a complexly theoretic sense.

970
00:50:28,700 --> 00:50:33,400
Now, it is also the case that many of the problems,

971
00:50:33,400 --> 00:50:38,400
so while the overall thing that we do is not hard from a complexity-theoretic sense,

972
00:50:38,400 --> 00:50:43,400
and indeed, humans cannot solve NP-complete problems in general.

973
00:50:43,400 --> 00:50:48,400
It is true that many of the optimization problems that we pose to our algorithms

974
00:50:48,400 --> 00:50:53,400
are intractable in the general case, starting from neural net optimization itself.

975
00:50:53,400 --> 00:50:58,400
It is easy to create a family of datasets for a neural network with a very small number of neurons,

976
00:50:58,400 --> 00:51:01,400
such that finding the global optimum is NP-complete.

977
00:51:01,400 --> 00:51:04,400
And so, how do we avoid it?

978
00:51:04,400 --> 00:51:08,400
Well, we just try gradient descent anyway, and somehow it works.

979
00:51:08,400 --> 00:51:17,400
But without question, we do not solve problems which are truly intractable.

980
00:51:17,400 --> 00:51:20,400
So, I mean, I hope this answers the question.

981
00:51:20,400 --> 00:51:28,400
Hello. It seems like an important sub-problem on the path towards AGI will be understanding language,

982
00:51:28,400 --> 00:51:32,400
and the state of generative language modeling right now is pretty abysmal.

983
00:51:32,400 --> 00:51:37,400
What do you think are the most productive research trajectories towards generative language models?

984
00:51:37,400 --> 00:51:44,400
So, I'll first say that you are completely correct that the situation with language is still far from great,

985
00:51:44,400 --> 00:51:52,400
although progress has been made, even without any particular innovations beyond models that exist today.

986
00:51:52,400 --> 00:51:58,400
Simply scaling up models that exist today on larger datasets is going to go surprisingly far,

987
00:51:58,400 --> 00:52:01,400
not even larger datasets, but larger and deeper models.

988
00:52:01,400 --> 00:52:06,400
For example, if you trained a language model with a thousand layers, and it's the same layer,

989
00:52:06,400 --> 00:52:10,400
I think it's going to be a pretty amazing language model.

990
00:52:10,400 --> 00:52:15,400
Like, we don't have the cycles for it yet, but I think it will change very soon.

991
00:52:15,400 --> 00:52:24,400
Now, I also agree with you that there are some fundamental things missing in our current understanding of deep learning,

992
00:52:24,400 --> 00:52:28,400
which prevent us from really solving the problem that we want.

993
00:52:28,400 --> 00:52:33,400
So, I think one of these problems, one of the things that's missing is that, or that seems like patently wrong,

994
00:52:33,400 --> 00:52:41,400
is the fact that we train a model, then we stop training the model, and we freeze it,

995
00:52:41,400 --> 00:52:47,400
even though it's the training process where the magic really happens.

996
00:52:47,400 --> 00:52:56,400
Like, the magic is like, if you think about it, like, the training process is the true general part of the whole story,

997
00:52:56,400 --> 00:52:59,400
because your TensorFlow code doesn't care which dataset to optimize.

998
00:52:59,400 --> 00:53:03,400
It just says, whatever, just give me the dataset, I don't care which one to solve, I'll solve them all.

999
00:53:03,400 --> 00:53:11,400
So, like, the ability to do that feels really special, and I think we are not using it at test time.

1000
00:53:11,400 --> 00:53:15,400
Like, it's hard to speculate about, like, things of which we don't know the answer,

1001
00:53:15,400 --> 00:53:22,400
but all I'll say is that simply train bigger, deeper language models will go surprisingly far scaling up,

1002
00:53:22,400 --> 00:53:25,400
but also doing things like training at test time and inference at test time,

1003
00:53:25,400 --> 00:53:29,400
I think would be another important boost to performance.

1004
00:53:29,400 --> 00:53:32,400
Hi, thank you for the talk.

1005
00:53:32,400 --> 00:53:36,400
So, it seems like right now another interesting approach to solving reinforcement learning problems

1006
00:53:36,400 --> 00:53:41,400
could be to go for the evolutionary routes, using evolutionary strategies.

1007
00:53:41,400 --> 00:53:45,400
And although they have their caveats, I wanted to know if, at OpenAI in particular,

1008
00:53:45,400 --> 00:53:50,400
you're working on something related, and what is your general opinion on them?

1009
00:53:50,400 --> 00:53:58,400
So, like, at present, I believe that something like evolutionary strategies is not great for reinforcement learning.

1010
00:53:58,400 --> 00:54:03,400
I think that normal reinforcement learning algorithms, especially with big policies, are better.

1011
00:54:03,400 --> 00:54:09,400
But I think if you want to evolve a small compact object, like a piece of code, for example,

1012
00:54:09,400 --> 00:54:14,400
I think that would be a place where this would be seriously worth considering.

1013
00:54:14,400 --> 00:54:20,400
But this, you know, evolving a useful piece of code is a cool idea.

1014
00:54:20,400 --> 00:54:24,400
It hasn't been done yet, so still a lot of work to be done before we get there.

1015
00:54:24,400 --> 00:54:26,400
Hi, thank you so much for coming.

1016
00:54:26,400 --> 00:54:31,400
My question is, you mentioned what is the right goal is a political problem.

1017
00:54:31,400 --> 00:54:34,400
So, I'm wondering if you can elaborate a bit on that,

1018
00:54:34,400 --> 00:54:39,400
and also what do you think would be their approach for us to maybe get there?

1019
00:54:40,400 --> 00:54:45,400
Well, I can't really comment too much, because all the thoughts that, you know,

1020
00:54:45,400 --> 00:54:50,400
we now have a few people who are thinking about this full time at OpenAI,

1021
00:54:50,400 --> 00:54:57,400
I don't have enough of a super strong opinion to say anything too definitive.

1022
00:54:57,400 --> 00:55:02,400
All I can say at a very high level is given the size, like, if you go into the future,

1023
00:55:02,400 --> 00:55:06,400
whenever soon or later, you know, whenever it's going to happen when you build a computer

1024
00:55:06,400 --> 00:55:10,400
which can do anything better than a human.

1025
00:55:10,400 --> 00:55:13,400
It will happen, because the brain is physical.

1026
00:55:13,400 --> 00:55:18,400
The impact on society is going to be completely massive and overwhelming.

1027
00:55:18,400 --> 00:55:23,400
It's very difficult to imagine, even if you try really hard.

1028
00:55:23,400 --> 00:55:27,400
And I think what it means is that people will care a lot.

1029
00:55:27,400 --> 00:55:31,400
And that's what I was alluding to, the fact that this will be something

1030
00:55:31,400 --> 00:55:34,400
that many people will care about strongly.

1031
00:55:34,400 --> 00:55:39,400
And, like, as the impact increases, gradually we sell driving cars, more automation,

1032
00:55:39,400 --> 00:55:42,400
I think we will see a lot more people care.

1033
00:55:42,400 --> 00:55:46,400
Do we need to have a very accurate model of the physical world,

1034
00:55:46,400 --> 00:55:53,400
and then simulate that in order to have these agents that can eventually come out into the real world

1035
00:55:53,400 --> 00:55:58,400
and do something approaching, you know, human level intelligence tasks?

1036
00:55:58,400 --> 00:56:00,400
That's a very good question.

1037
00:56:00,400 --> 00:56:04,400
So I think if that were the case, we'd be in trouble.

1038
00:56:04,400 --> 00:56:10,400
And I am very certain that it could be avoided.

1039
00:56:10,400 --> 00:56:16,400
So specifically, the real answer has to be that, look, you learn to problem solve,

1040
00:56:16,400 --> 00:56:18,400
you learn to negotiate, you learn to persist,

1041
00:56:18,400 --> 00:56:21,400
you learn lots of different useful life lessons in the simulation.

1042
00:56:21,400 --> 00:56:23,400
And yes, you learn some physics, too.

1043
00:56:23,400 --> 00:56:26,400
But then you go outside of the real world, and you have to start over to some extent,

1044
00:56:26,400 --> 00:56:30,400
because many of your deeply held assumptions will be false.

1045
00:56:30,400 --> 00:56:37,400
And one of the goals, so that's one reason I care so much about never stopping training.

1046
00:56:37,400 --> 00:56:41,400
You've accumulated your knowledge, now you go into an environment where some of your assumptions are violated,

1047
00:56:41,400 --> 00:56:44,400
you continue training, you try to connect the new data to your old data.

1048
00:56:44,400 --> 00:56:46,400
And this is an important requirement from our algorithms,

1049
00:56:46,400 --> 00:56:50,400
which is already met to some extent, but it will have to be met a lot more,

1050
00:56:50,400 --> 00:56:54,400
so that you can take the partial knowledge that you've acquired

1051
00:56:54,400 --> 00:56:57,400
and go in a new situation, learn some more.

1052
00:56:57,400 --> 00:57:02,400
Literally the example of you go to school, you learn useful things, then you go to work.

1053
00:57:02,400 --> 00:57:06,400
It's not a perfect, it's not, you know, for your four years of CS and undergrad,

1054
00:57:06,400 --> 00:57:10,400
it's not going to fully prepare you for whatever it is you need to know at work.

1055
00:57:10,400 --> 00:57:12,400
It will help somewhat, you'll be able to get off the ground,

1056
00:57:12,400 --> 00:57:14,400
but there will be lots of new things you need to learn.

1057
00:57:14,400 --> 00:57:16,400
So that's the spirit of it.

1058
00:57:16,400 --> 00:57:18,400
Think of it as a school.

1059
00:57:18,400 --> 00:57:22,400
One of the things you mentioned pretty early on in your talk is that one of the limitations

1060
00:57:22,400 --> 00:57:26,400
of this sort of style of reinforcement learning is there's no self-organization.

1061
00:57:26,400 --> 00:57:29,400
So you have to tell it, went into a good thing or did a bad thing.

1062
00:57:29,400 --> 00:57:31,400
And that's actually a problem in neuroscience,

1063
00:57:31,400 --> 00:57:34,400
is when you're trying to teach a rat to, you know, navigate a maze,

1064
00:57:34,400 --> 00:57:36,400
you have to artificially tell it what to do.

1065
00:57:36,400 --> 00:57:39,400
So where do you see moving forward when we already have this problem with teaching,

1066
00:57:39,400 --> 00:57:41,400
you know, not necessarily learning, but also teaching.

1067
00:57:41,400 --> 00:57:44,400
So where do you see the research moving forward in that respect?

1068
00:57:44,400 --> 00:57:47,400
How do you sort of introduce this notion of self-organization?

1069
00:57:47,400 --> 00:57:50,400
So I think without question, one really important thing you need to do

1070
00:57:50,400 --> 00:57:57,400
is to be able to infer the goals and strategies of other agents by observing them.

1071
00:57:57,400 --> 00:58:02,400
That's a fundamental skill you need to be able to learn, to embed into the agents.

1072
00:58:02,400 --> 00:58:05,400
So that, for example, you have two agents, one of them is doing something,

1073
00:58:05,400 --> 00:58:08,400
and the other agent says, well, that's really cool, I want to be able to do that too.

1074
00:58:08,400 --> 00:58:10,400
And then you go on and do that.

1075
00:58:10,400 --> 00:58:14,400
And so I'd say that this is a very important component in terms of setting the reward.

1076
00:58:14,400 --> 00:58:18,400
You see what they do, you infer the reward,

1077
00:58:18,400 --> 00:58:21,400
and now we have a knob which says, you see what they're doing?

1078
00:58:21,400 --> 00:58:23,400
Now go and try to do the same thing.

1079
00:58:23,400 --> 00:58:28,400
So I'd say this is, as far as I know, this was one of the important ways

1080
00:58:28,400 --> 00:58:33,400
in which humans are quite different from other animals

1081
00:58:33,400 --> 00:58:42,400
in the way in the scale and scope in which we copy the behavior of other humans.

1082
00:58:42,400 --> 00:58:44,400
You might have asked a quick follow-up.

1083
00:58:44,400 --> 00:58:45,400
Go for it.

1084
00:58:45,400 --> 00:58:48,400
So that's kind of obvious how that works in the scope of competition,

1085
00:58:48,400 --> 00:58:50,400
but what about just sort of arbitrary tasks?

1086
00:58:50,400 --> 00:58:54,400
Like, I'm in a math class with someone and I see someone doing a problem a particular way

1087
00:58:54,400 --> 00:58:57,400
and I'm like, oh, that's a good strategy, maybe I should try that out.

1088
00:58:57,400 --> 00:59:00,400
How does that work in a sort of non-competitive environment?

1089
00:59:00,400 --> 00:59:04,400
So I think that this will be, I think that's going to be a little bit separate

1090
00:59:04,400 --> 00:59:10,400
from the competitive environment, but it will have to be somehow either,

1091
00:59:11,400 --> 00:59:16,400
probably baked in, maybe evolved into the system where like,

1092
00:59:16,400 --> 00:59:21,400
if you have other agents doing things, they're generating data which you observe

1093
00:59:21,400 --> 00:59:24,400
and the only way to truly make sense of the data that you see

1094
00:59:24,400 --> 00:59:29,400
is to infer the goal of the agent, the strategy, their belief state.

1095
00:59:29,400 --> 00:59:32,400
That's important also for communicating with them.

1096
00:59:32,400 --> 00:59:34,400
If you want to successfully communicate with someone,

1097
00:59:34,400 --> 00:59:37,400
you have to keep track both of their goal and of their belief state instead of knowledge.

1098
00:59:37,400 --> 00:59:41,400
So I think you will find that there are many, I guess, connections

1099
00:59:41,400 --> 00:59:45,400
between understanding what other agents are doing, inferring their goals,

1100
00:59:45,400 --> 00:59:48,400
imitating them and successfully communicating them.

1101
00:59:48,400 --> 00:59:52,400
Alright, let's give Ilya and the happy hour a big hand.

1102
01:00:07,400 --> 01:00:09,400
Thank you.

