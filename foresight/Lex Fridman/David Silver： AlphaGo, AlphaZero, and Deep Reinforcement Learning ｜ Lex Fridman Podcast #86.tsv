start	end	text
0	4100	The following is a conversation with David Silver, who leads the Reinforcement Learning
4100	11140	Research Group at DeepMind, and was the lead researcher on AlphaGo, AlphaZero, and co-led
11140	15680	the AlphaStar and MuZero efforts, and a lot of important work in reinforcement learning
15680	17180	in general.
17180	23100	I believe AlphaZero is one of the most important accomplishments in the history of artificial
23100	28900	intelligence, and David is one of the key humans who brought AlphaZero to life together
28900	32060	with a lot of other great researchers at DeepMind.
32060	34500	He's humble, kind, and brilliant.
34500	38660	We were both jet lagged, but didn't care and made it happen.
38660	43460	It was a pleasure and truly an honor to talk with David.
43460	47100	This conversation was recorded before the outbreak of the pandemic.
47100	51900	For everyone feeling the medical, psychological, and financial burden of this crisis, I'm
51900	53540	sending love your way.
53540	57860	Stay strong, or in this together, we'll beat this thing.
57860	60100	This is the Artificial Intelligence Podcast.
60100	65180	If you enjoy it, subscribe on YouTube, review it with 5 stars on Apple Podcasts, support
65180	72220	on Patreon, or simply connect with me on Twitter at Lex Freedman, spelled F-R-I-D-M-A-N.
72220	76540	As usual, I'll do a few minutes of ads now and never any ads in the middle that can break
76540	78420	the flow of the conversation.
78420	82780	I hope that works for you and doesn't hurt the listening experience.
82780	84060	Quick summary of the ads.
84060	85060	Two sponsors.
85060	87580	Masterclass and Cash App.
87580	92700	Please consider supporting the podcast by signing up to masterclass and masterclass.com
92700	98940	slash lex and downloading Cash App and using code Lex Podcast.
98940	103340	This show is presented by Cash App, the number one finance app in the App Store.
103340	107220	When you get it, use code Lex Podcast.
107220	111540	Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with
111540	113980	as little as $1.
113980	118020	Since Cash App allows you to buy Bitcoin, let me mention that cryptocurrency in the
118020	121420	context of the history of money is fascinating.
121420	125380	I recommend Ascent of Money as a great book on this history.
125380	129740	Debits and credits on ledgers started around 30,000 years ago.
129740	135940	The US Dollar created over 200 years ago and Bitcoin, the first decentralized cryptocurrency,
135940	138740	released just over 10 years ago.
138740	144020	So given that history, cryptocurrency is still very much in its early days of development,
144020	149180	but it's still aiming to and just might redefine the nature of money.
149180	153460	So again, if you get Cash App from the App Store or Google Play and use the code Lex
153460	159700	Podcast, you get $10 and Cash App will also donate $10 the first, an organization that
159700	165020	is helping to advance robotics and STEM education for young people around the world.
165020	167180	This show is sponsored by Masterclass.
167180	171660	Sign up at masterclass.com slash Lex to get a discount and to support this podcast.
171660	176780	In fact, for a limited time now, if you sign up for an All Access Pass for a year, you
176780	181300	get to get another All Access Pass to share with a friend.
181300	182740	Buy one, get one free.
182740	186420	When I first heard about Masterclass, I thought it was too good to be true.
186420	193020	For $180 a year, you get an All Access Pass to watch courses from to list some of my favorites.
193020	198220	Chris Hatfield on space exploration, Neil deGrasse Tyson on scientific thinking communication,
198220	204780	Will Wright, the creator of SimCity and Sims, on game design, Jane Goodall on conservation,
204780	211020	Carl Santana on guitar, his song Europa could be the most beautiful guitar song ever written.
211020	215740	Gary Kasparov on chess, Daniel Nagrano on poker, and many, many more.
215740	219460	Chris Hatfield explaining how rockets work and the experience of being launched into
219460	221740	space alone is worth the money.
221740	226940	For me, the key is to not be overwhelmed by the abundance of choice, pick three courses
226940	229740	you want to complete, watch each of them all the way through.
229740	233660	It's not that long, but it's an experience that will stick with you for a long time.
233660	234660	I promise.
234660	236860	It's easily worth the money.
236860	239340	You can watch it on basically any device.
239340	243940	Once again, sign up on masterclass.com slash Lex to get a discount and to support this
243940	245740	podcast.
245740	249900	And now here's my conversation with David Silver.
249900	254020	What was the first program you ever written and what programming language?
254020	255020	Do you remember?
255020	262140	I remember very clearly, yeah, my parents brought home this BBC model B microcomputer.
262140	264220	It was just this fascinating thing to me.
264220	270100	I was about seven years old and couldn't resist just playing around with it.
270100	277220	So I think first program ever was writing my name out in different colors and getting
277220	279820	it to loop and repeat that.
280380	284580	There was something magical about that, which just led to more and more.
284580	286820	How did you think about computers back then?
286820	291700	The magical aspect of it, that you can write a program and there's this thing that you
291700	297660	just gave birth to that's able to create visual elements and live in its own.
297660	300100	Or did you not think of it in those romantic notions?
300100	302460	Was it more like, oh, that's cool.
302460	305380	I can solve some puzzles.
305380	306980	It was always more than solving puzzles.
306980	314340	It was something where there was this limitless possibilities once you have a computer in
314340	315340	front of you.
315340	316340	You can do anything with it.
316340	318100	I used to play with Lego with the same feeling.
318100	321540	You can make anything you want out of Lego, but even more so with a computer.
321540	324660	You're not constrained by the amount of kit you've got.
324660	329020	And so I was fascinated by it and started pulling out the user guide and the advanced
329020	330820	user guide and then learning.
330820	334700	So I started in basic and then later 6502.
334740	340180	My father also became interested in this machine and gave up his career to go back to school
340180	347100	and study for a master's degree in artificial intelligence, funnily enough, at Essex University
347100	348740	when I was seven.
348740	352100	So I was exposed to those things at an early age.
352100	357780	He showed me how to program in Prologue and do things like querying your family tree.
357780	364300	And those are some of my earliest memories of trying to figure things out on a computer.
364340	369020	Those are the early steps in computer science programming, but when did you first fall in
369020	374900	love with artificial intelligence or with the ideas, the dreams of AI?
374900	379140	I think it was really when I went to study at university.
379140	384260	So I was an undergrad at Cambridge and studying computer science.
384260	389660	And I really started to question, you know, what really are the goals?
389660	390660	What's the goal?
390660	393060	Where do we want to go with computer science?
393060	402300	And it seemed to me that the only step of major significance to take was to try and recreate
402300	404300	something akin to human intelligence.
404300	407860	If we could do that, that would be a major leap forward.
407860	412500	And that idea certainly wasn't the first to have it, but it, you know, nestled within
412500	418700	me somewhere and became like a bug, you know, I really wanted to crack that problem.
418940	422980	So you thought it was, like, you had a notion that this is something that human beings can do,
422980	427300	that it is possible to create an intelligent machine?
427300	433420	Well, I mean, unless you believe in something metaphysical, then what are our brains doing?
433420	441580	Well, at some level, their information processing systems, which are able to take whatever
441580	445620	information is in there, transform it through some form of program and produce some kind
445620	449660	of output, which enables that human being to do all the amazing things that they can
449660	452300	do in this incredible world.
452300	458540	So then, do you remember the first time you've written a program that, because you also had
458540	462380	an interest in games, do you remember the first time you were in a program that beat
462380	471740	you in a game, that more beat you at anything, sort of achieved super David Silver level
471740	474460	performance?
474460	476580	So I used to work in the games industry.
476580	481420	So for five years, I programmed games for my first job.
481420	485980	So it was an amazing opportunity to get involved in a startup company.
485980	492260	And so I was involved in building AI at that time.
492260	499580	And so for sure, there was a sense of building, handcrafted, what people used to call AI in
499580	503820	the games industry, which I think is not really what we might think of as AI in its fullest
504180	511460	sense, but something which is able to take actions in a way which makes things interesting
511460	515220	and challenging for the human player.
515220	520380	And at that time, I was able to build these handcrafted agents, which in certain limited
520380	526740	cases could do things which were able to do better than me, but mostly in these kind of
526740	531260	twitch-like scenarios where they were able to do things faster or because they had some
531260	535420	pattern which was able to exploit repeatedly.
535420	541780	I think if we're talking about real AI, the first experience for me came after that when
541780	548380	I realized that this path I was on wasn't taking me towards, it wasn't dealing with
548380	554740	that bug which I still had inside me to really understand intelligence and try and solve it.
554740	560860	Everything people were doing in games was short-term fixes rather than long-term vision.
560980	566660	So I went back to study for my PhD, which was, finally enough, trying to apply reinforcement
566660	568540	learning to the game of Go.
568540	573860	And I built my first Go program using reinforcement learning, a system which would, by trial and
573860	580540	error, play against itself and was able to learn which patterns were actually helpful
580540	584740	to predict whether it was going to win or lose the game and then choose the moves that
584740	588460	led to the combination of patterns that would mean that you're more likely to win.
588500	590540	That system, that system beat me.
591540	592860	And how did that make you feel?
593380	594380	It made me feel good.
597860	603780	It's a mix of a sort of excitement and was there a tinge of sort of almost like a fearful
603780	604780	awe?
604780	617780	It's like in 2001 Space Odyssey kind of realizing that you've created something that's achieved
617820	621220	human-level intelligence in this one particular little task.
621220	624460	And in that case, I suppose neural networks weren't involved.
624460	626940	There were no neural networks in those days.
626940	633260	This was pre-deep learning revolution, but it was a principled self-learning system based
633260	640340	on a lot of the principles which people still use in deep reinforcement learning.
640340	641340	How did I feel?
641340	650100	I think I found it immensely satisfying that a system which was able to learn from first
650100	656380	principles for itself was able to reach the point that it was understanding this domain
656380	660100	better than I could and able to outwit me.
660100	661580	I don't think it was a sense of awe.
661580	669020	It was a sense that satisfaction, that something I felt should work, had worked.
669060	674580	To me AlphaGo, and I don't know how else to put it, but to me AlphaGo and AlphaGo Zero
674580	680660	mastering the game of Go is, again, to me the most profound and inspiring moment in the
680660	683620	history of artificial intelligence.
683620	686740	So you're one of the key people behind this achievement.
686740	692700	And I'm Russian, so I really felt the first sort of seminal achievement when Deep Blue
692700	696940	be Gare Kasparov in 1997.
696940	702740	So as far as I know, the AI community at that point largely saw the game of Go as unbeatable
702740	709100	in AI using the state-of-the-art to brute-force search methods.
709100	714940	Even if you consider, at least the way I saw it, even if you consider arbitrary exponential
714940	722660	scaling of compute, Go would still not be solvable, hence why it was thought to be impossible.
722660	729580	So given that the game of Go was impossible to master, when was the dream for you, you
729580	734580	just mentioned your PhD thesis of building the system that plays Go, when was the dream
734580	740300	for you that you could actually build a computer program that achieves the world class, not
740300	745180	necessarily beats the world champion, but achieves that kind of level of playing Go?
745180	746180	First of all, thank you.
746180	748700	That was very kind words.
748700	754660	Funnily enough, I just came from a panel where I was actually in a conversation with
754660	759540	Gare Kasparov and Murray Campbell, who was the author of Deep Blue, and it was their
759540	765180	first meeting together since the match, so I'm just acquired yesterday, so I'm literally
765180	767500	fresh from that experience.
767500	771980	So these are amazing moments when they happen, but where did it all start?
771980	776260	Well, for me, it started when I became fascinated in the game of Go.
776260	781860	So Go, for me, I've grown up playing games, I've always had a fascination in board games.
781860	786220	I played chess as a kid, I played Scrabble as a kid.
786220	790620	When I was at university, I discovered the game of Go, and to me, it just blew all of
790620	791620	those other games out of the water.
791620	797980	It was just so deep and profound in its complexity with endless levels to it.
797980	807380	What I discovered was that I could devote endless hours to this game, and I knew in
807380	810740	my heart of hearts that no matter how many hours I would devote to it, I would never
810740	818180	become a grandmaster, or there was another path, and the other path was to try and understand
818180	822660	how you could get some other intelligence to play this game better than I would be able
822660	823660	to.
823700	829300	So even in those days, I had this idea that, what if it was possible to build a program
829300	831220	that could crack this?
831220	837380	And as I started to explore the domain, I discovered that this was really the domain
837380	844580	where people felt deeply that if progress could be made in Go, it would really mean
844580	846420	a giant leap forward for AI.
846420	851060	It was the challenge where all other approaches had failed.
851060	856820	This is coming out of the era you mentioned, which was in some sense the golden era for
856820	860020	the classical methods of AI, like heuristic search.
860020	866780	In the 90s, they all fell one after another, not just chess with deep blue, but checkers,
866780	868980	batgammon, Othello.
868980	876620	There were numerous cases where systems built on top of heuristic search methods with these
876620	880700	high-performance systems had been able to defeat the human world champion in each of
880700	882100	those domains.
882100	889380	And yet, in that same time period, there was a million-dollar prize available for the
889380	892980	game of Go, for the first system to be a human professional player.
892980	898500	And at the end of that time period, at year 2000, when the prize expired, the strongest
898500	902820	Go program in the world was defeated by a nine-year-old child.
902820	906940	When that nine-year-old child was giving nine free moves to the computer at the start of
906940	909980	the game to try and even things up.
909980	918260	And computer Go expert beat that same strongest program with 29 handicap stones, 29 free moves.
918260	923940	So that's what the state of affairs was when I became interested in this problem in around
923940	929620	2003 when I started working on computer Go.
929620	930620	There was nothing.
930620	936660	There was just very, very little in the way of progress towards meaningful performance
936660	939300	again at anything approaching human level.
939300	945060	And so it wasn't through lack of effort people have tried many, many things.
945060	950860	And so there was a strong sense that something different would be required for Go than had
950860	954340	been needed for all of these other domains where AI had been successful.
954340	960900	And maybe the single clearest example is that Go, unlike those other domains, had this kind
960900	967100	of intuitive property that a Go player would look at a position and say, hey, here's this
967100	969780	mess of black and white stones.
969780	975980	But from this mess, oh, I can predict that this part of the board has become my territory,
975980	979940	this part of the board has become your territory, and I've got this overall sense that I'm going
979940	982500	to win and that this is about the right move to play.
982500	988300	And that intuitive sense of judgment of being able to evaluate what's going on in a position,
988300	992900	it was pivotal to humans being able to play this game and something that people had no
992900	995180	idea how to put into computers.
995180	1000020	So this question of how to evaluate a position, how to come up with these intuitive judgments
1000020	1008380	was the key reason why Go was so hard in addition to its enormous search space and the reason
1008380	1013100	why methods which had succeeded so well elsewhere failed in Go.
1013100	1019060	And so people really felt deep down that in order to crack Go, we would need to get something
1019060	1020580	akin to human intuition.
1020580	1026060	And if we got something akin to human intuition, we'd be able to solve many, many more problems
1026060	1027060	in AI.
1027060	1031260	So for me, that was the moment where it's like, okay, this is not just about playing
1031260	1032260	the game of Go.
1032260	1033740	This is about something profound.
1033740	1037820	And it was back to that bug which had been itching me all those years.
1037820	1042900	This is the opportunity to do something meaningful and transformative and I guess a dream was
1042900	1043900	born.
1043900	1045420	That's a really interesting way to put it.
1045420	1050900	So almost this realization that you need to find formulate Go as a kind of a prediction
1050900	1054780	problem versus a search problem was the intuition.
1054780	1063580	I mean, maybe that's the wrong crude term, but to give it the ability to kind of intuit
1063580	1067140	things about positional structure of the board.
1067140	1071060	Now, okay, but what about the learning part of it?
1071900	1077540	Did you have a sense that learning has to be part of the system?
1077540	1083380	Again, something that hasn't really, as far as I think, except with TD Gammon and the
1083380	1088820	90s with RL a little bit, hasn't been part of those day-to-day art game-playing systems?
1088820	1095180	So I strongly felt that learning would be necessary and that's why my PhD topic back
1095180	1100220	then was trying to apply reinforcement learning to the game of Go.
1100380	1106060	I'm not just learning of any type, but I felt that the only way to really have a system
1106060	1111140	to progress beyond human levels of performance wouldn't just be to mimic how humans do it,
1111140	1114020	but to understand for themselves.
1114020	1119140	How else can a machine hope to understand what's going on except through learning?
1119140	1120540	If you're not learning, what else are you doing?
1120540	1125540	Well, you're putting all the knowledge into the system and that just feels like something
1125540	1132300	which decades of AI have told us is maybe not a dead end, but certainly has a ceiling
1132300	1133300	to the capabilities.
1133300	1136580	It's known as the knowledge acquisition bottleneck.
1136580	1141060	The more you try to put into something, the more brittle the system becomes.
1141060	1142860	So you just have to have learning.
1142860	1143860	You have to have learning.
1143860	1148980	That's the only way you're going to be able to get a system which has sufficient knowledge
1148980	1154180	in it, millions and millions of pieces of knowledge, billions, trillions, of a form
1154220	1157620	that it can actually apply for itself and understand how those billions and trillions
1157620	1162380	of pieces of knowledge can be leveraged in a way which will actually lead it towards
1162380	1166340	its goal without conflict or other issues.
1166340	1167340	Yeah.
1167340	1173580	I mean, if I put myself back in that time, I just wouldn't think like that without a
1173580	1174900	good demonstration of RL.
1174900	1182780	I would think more in the symbolic AI, like the not learning, but sort of a simulation
1182780	1190500	of knowledge base, like a growing knowledge base, but it would still be sort of pattern-based,
1190500	1195740	like basically have little rules that you kind of assemble together into a large knowledge
1195740	1196740	base.
1196740	1199900	Well, in a sense, that was the state of the art back then.
1199900	1205460	So if you look at the Go programs which had been competing for this prize I mentioned,
1205460	1211300	they were an assembly of different specialized systems, some of which used huge amounts of
1211340	1216300	human knowledge to describe how you should play the opening, how you should all the different
1216300	1223700	patterns that were required to play well in the game of Go, end game theory, combinatorial
1223700	1229260	game theory, and combined with more principled search-based methods which were trying to
1229260	1236940	solve for particular sub-parts of the game, like life and death, connecting groups together,
1236940	1242460	all these amazing sub-problems that just emerged in the game of Go, there were different pieces
1242460	1249380	all put together into this collage which together would try and play against a human.
1249380	1256300	And although not all of the pieces were handcrafted, the overall effect was nevertheless still
1256300	1260340	brittle and it was hard to make all these pieces work well together.
1260340	1265500	And so really what I was pressing for and the main innovation of the approach I took
1265580	1271900	was to go back to first principles and say, well, let's back off that and try and find a
1271900	1278540	principled approach where the system can learn for itself just from the outcome, like, you know,
1278540	1282620	learn for itself if you try something, did that help or did it not help?
1282620	1287820	And only through that procedure can you arrive at knowledge which is verified,
1287820	1292060	the system has to verify it for itself, not relying on any other third party to say this
1292060	1299020	is right or this is wrong. And so that principle was already very important in those days,
1299020	1302220	that unfortunately we were missing some important pieces back then.
1303180	1309020	So before we dive into maybe discussing the beauty of reinforcement learning,
1309020	1315340	let's take a step back, we kind of skipped it a bit, but the rules of the game of Go,
1315900	1326300	what the elements of it perhaps contrasting to chess that sort of you really enjoy as a human
1326300	1332300	being and also that make it really difficult as a AI machine learning problem.
1332940	1339020	So the game of Go has remarkably simple rules. In fact, so simple that people have speculated
1339020	1343660	that if we were to meet alien life at some point that we wouldn't be able to communicate with them,
1343660	1347660	but we would be able to play Go with them, they probably have discovered the same ruleset.
1348860	1354140	So the game is played on a 19 by 19 grid, and you play on the intersections of the grid and
1354140	1360300	the players take turns. And the aim of the game is very simple, it's to surround as much territory
1360300	1365340	as you can as many of these intersections with your stones and to surround more than your opponent
1365340	1370380	does. And the only nuance to the game is that if you fully surround your opponent's piece,
1370380	1373500	then you get to capture it and remove it from the board and it counts as your own territory.
1374300	1379020	Now, from those very simple rules, immense complexity arises. There's kind of profound
1379020	1385900	strategies in how to surround territory, how to kind of trade off between making solid territory
1385900	1390780	yourself now, compared to building up influence that will help you acquire territory later in
1390780	1394060	the game, how to connect groups together, how to keep your own groups alive,
1394220	1400620	which patterns of stones are most useful compared to others.
1401340	1407020	There's just immense knowledge and human Go players have played this game for,
1407020	1411260	it was discovered thousands of years ago, and human Go players have built up this immense
1411260	1417180	knowledge base over the years. It's studied very deeply and played by something like 50 million
1417180	1422700	players across the world, mostly in China, Japan and Korea, where it's an important part of the
1422700	1428380	culture, so much so that it's considered one of the four ancient arts that was required by
1428380	1433180	Chinese scholars. So, there's a deep history there. But there's interesting quality. So,
1433740	1439260	if I were to compare to chess, chess is in the same way as it is in Chinese culture for Go,
1439260	1445980	and chess in Russia is also considered one of the sacred arts. So, if we contrast Go with
1445980	1451260	chess, there's interesting qualities about Go. Maybe you can correct me if I'm wrong, but the
1452220	1461340	evaluation of a particular static board is not as reliable. In chess, you can kind of assign
1461340	1467500	points to the different units, and it's kind of a pretty good measure of who's winning, who's
1467500	1473260	losing. It's not so clear. So, in the game of Go, you find yourself in a situation where
1473260	1478300	both players have played the same number of stones, actually captures a strong level of play
1478300	1481740	happen very rarely, which means that at any moment in the game, you've got the same number
1481740	1486220	of white stones and black stones. And the only thing which differentiates how well you're doing
1486220	1491740	is this intuitive sense of where are the territories ultimately going to form on this board?
1492380	1500060	And if you look at the complexity of a real Go position, it's mind-boggling that kind of
1500060	1505100	question of what will happen in 300 moves from now when you see just a scattering of 20
1505100	1513180	white and black stones intermingled. And so, that challenge is the reason why
1513180	1518300	position evaluation is so hard in Go compared to other games. In addition to that, it has an
1518300	1524620	enormous search space. So, there's around 10 to 170 positions in the game of Go. That's an
1524620	1530380	astronomical number. And that search space is so great that traditional heuristic search methods
1530380	1535660	that were so successful and things like Deep Blue and chess programs just kind of fall over in Go.
1537420	1543820	Which point did reinforcement learning enter your life, your research life, your way of thinking?
1543820	1548620	We just talked about learning, but reinforcement learning is a very particular kind of learning,
1549580	1555260	one that's both philosophically sort of profound, but also one that's pretty difficult to get to
1555260	1560700	work as if you look back in the early days. So, when did that enter your life and how did
1560700	1566940	that work progress? So, I had just finished working in the games industry at this startup
1566940	1574780	company. And I took a year out to discover for myself exactly which path I wanted to take. I
1574780	1579740	knew I wanted to study intelligence, but I wasn't sure what that meant at that stage. I really
1579740	1583500	didn't feel I had the tools to decide on exactly which path I wanted to follow.
1583580	1591100	So, during that year, I read a lot. And one of the things I read was Saturn and Bartow,
1591100	1598220	the sort of seminal textbook on an introduction to reinforcement learning. And when I read that
1598220	1606700	textbook, I just had this resonating feeling that this is what I understood intelligence to be.
1607660	1614540	And this was the path that I felt would be necessary to go down to make progress in AI.
1615740	1623740	So, I got in touch with Rich Saturn and asked him if he would be interested in supervising me
1623740	1635420	on a PhD thesis in computer go. And he basically said that if he's still alive, he'd be happy to.
1636860	1642300	But unfortunately, he'd been struggling with very serious cancer for some years. And he really
1642300	1647420	wasn't confident at that stage that he'd even be around to see the end of it. But fortunately,
1647420	1652940	that part of the story worked out very happily. And I found myself out there in Alberta. They've
1652940	1659020	got a great games group out there with a history of fantastic work in board games as well as Rich
1659020	1664380	Saturn, the father of RL. So, it was the natural place for me to go in some sense to study this
1664380	1675020	question. And the more I looked into it, the more strongly I felt that this wasn't just the path to
1675020	1679740	progress in computer go. But really, this was the thing I'd been looking for. This was
1683740	1692780	really an opportunity to frame what intelligence means. What are the goals of AI in a single
1693100	1697180	problem definition such that if we're able to solve that clear single problem definition,
1698700	1704780	in some sense, we've cracked the problem of AI? So, to you, reinforcement learning ideas,
1704780	1711260	at least sort of echoes of it, would be at the core of intelligence. It is at the core of intelligence.
1711260	1716540	And if we ever create a human level intelligence system, it would be at the core of that kind of
1716540	1721660	system. Let me say it this way that I think it's helpful to separate out the problem from the solution.
1722300	1729580	So, I see the problem of intelligence, I would say it can be formalized as the reinforcement
1729580	1736060	learning problem. And that that formalization is enough to capture most, if not all of the things
1736060	1741660	that we mean by intelligence, that they can all be brought within this framework and gives us
1741660	1748540	a way to access them in a meaningful way that allows us as scientists to understand intelligence
1748540	1756220	and us as computer scientists to build them. And so, in that sense, I feel that it gives us a path,
1756220	1764860	maybe not the only path, but a path towards AI. And so, do I think that any system in the future
1764860	1770620	that's, you know, solved AI would have to have RL within it? Well, I think if you ask that,
1770620	1775500	you're asking about the solution methods. I would say that if we have such a thing,
1775500	1781100	it would be a solution to the RL problem. Now, what particular methods have been used to get there?
1781100	1784540	Well, we should keep an open mind about the best approaches to actually solve any problem.
1785660	1790700	And, you know, the things we have right now for reinforcement learning, maybe they, maybe
1791820	1795260	I believe they've got a lot of legs, but maybe we're missing some things. Maybe there's going
1795260	1801340	to be better ideas. I think we should keep, you know, let's remain modest and we're at the early
1801340	1806300	days of this field and there are many amazing discoveries ahead of us. For sure. The specifics,
1806300	1811180	especially of the different kinds of RL approaches currently, there could be other things that fall
1811180	1817660	into the very large umbrella of RL. But if it's, if it's okay, can we take a step back and kind of
1817660	1824140	ask the basic question of what is, do you, reinforcement learning? So, reinforcement learning
1824140	1834140	is the study and the science and the problem of intelligence in the form of an agent that
1834140	1837580	interacts with an environment. So, the problem you're trying to solve is represented by some
1837580	1842940	environment like the world in which that agent is situated. And the goal of RL is clear that the
1842940	1848060	agent gets to take actions. Those actions have some effect on the environment and the environment
1848060	1853180	gives back an observation to the agent saying, you know, this is what you see or sense. And one
1853180	1857980	special thing which it gives back is called the reward signal, how well it's doing in the environment.
1857980	1865180	And the reinforcement learning problem is to simply take actions over time so as to maximize
1865180	1874540	that reward signal. So, a couple of basic questions. What types of RL approaches are there? So,
1874540	1881500	I don't know if there's a nice brief inwards way to paint the picture of sort of value-based,
1881500	1888460	model-based, policy-based reinforcement learning. Yeah. So, now if we think about, okay, so there's
1888460	1893420	this ambitious problem definition of RL. It's really, you know, it's truly ambitious. It's
1893420	1897740	trying to capture and encircle all of the things in which an agent interacts with an environment and
1897740	1902700	say, well, how can we formalize and understand what it means to crack that? Now, let's think about
1902700	1907500	the solution method. Well, how do you solve a really hard problem like that? Well, one approach
1907500	1914140	you can take is to decompose that very hard problem into pieces that work together to solve
1914140	1919580	that hard problem. And so, you can kind of look at the decomposition that's inside the agent's
1919580	1924540	head, if you like, and ask, well, what form does that decomposition take? And some of the most
1924540	1929500	common pieces that people use when they're kind of putting the solution method together,
1929500	1934300	some of the most common pieces that people use are whether or not that solution has a value
1934380	1938540	function. That means, is it trying to predict, explicitly trying to predict how much reward
1938540	1943740	it will get in the future? Does it have a representation of a policy? That means something
1943740	1948380	which is deciding how to pick actions. Is that decision-making process explicitly represented?
1949100	1954380	And is there a model in the system? Is there something which is explicitly trying to predict
1954380	1961020	what will happen in the environment? And so, those three pieces are, to me, some of the most
1961020	1968540	common building blocks. And I understand the different choices in RL as choices of whether
1968540	1971900	or not to use those building blocks when you're trying to decompose the solution.
1972540	1977100	Should I have a value function represented? Should I have a policy represented? Should I have a
1977100	1981100	model represented? And there are combinations of those pieces and, of course, other things that
1981100	1985820	you could add into the picture as well. But those three fundamental choices give rise to some of
1985820	1990060	the branches of RL with which we are very familiar. And so, those, as you mentioned,
1990940	1998940	there is the choice of what's specified or modeled explicitly. And the idea is that
1999740	2005020	all of these are somehow implicitly learned within the system. So, it's almost a choice of
2006620	2012460	how you approach a problem. Do you see those as fundamental differences or these almost like
2013420	2017980	small specifics, like the details of how you solve the problem, but they're not fundamentally
2017980	2025980	different from each other? I think the fundamental idea is maybe at the higher level, the fundamental
2025980	2032540	idea is the first step of the decomposition is really to say, well, how are we really going to
2032540	2037820	solve any kind of problem where you're trying to figure out how to take actions and just from this
2038380	2042220	stream of observations, you know, you've got some agent situated in its sensory motor stream and
2043100	2046220	getting all these observations in, getting to take these actions, and what should it do? How
2046220	2049900	can you even broach that problem? You know, maybe the complexity of the world is so great
2050700	2055100	that you can't even imagine how to build a system that would understand how to deal with that.
2055660	2060060	And so, the first step of this decomposition is to say, well, you have to learn. The system has to
2060060	2065980	learn for itself. And so, note that the reinforcement learning problem doesn't actually stipulate
2065980	2069660	that you have to learn, like you could maximize your rewards without learning, it would just
2070220	2076140	wouldn't do a very good job of it. So, learning is required because it's the only way to achieve
2076140	2082140	good performance in any sufficiently large and complex environment. So, that's the first step.
2082140	2086940	And so, that step gives commonality to all of the other pieces, because now you might ask, well,
2086940	2091180	what should you be learning? What does learning even mean? You know, in this sense,
2091340	2097500	learning might mean, well, you're trying to update the parameters of some system, which
2098300	2102460	is then the thing that actually picks the actions. And those parameters could be
2102460	2107420	representing anything. They could be parameterizing a value function or a model or a policy.
2108460	2112380	And so, in that sense, there's a lot of commonality in that whatever is being represented there is
2112380	2116780	the thing which is being learned, and it's being learned with the ultimate goal of maximizing rewards.
2117420	2122700	But the way in which you decompose the problem is really what gives the semantics to the whole
2122700	2128460	system. Like, are you trying to learn something to predict well, like a value function or a model?
2128460	2133660	Are you learning something to perform well, like a policy? And the form of that objective,
2133660	2138700	like, is kind of giving the semantics to the system. And so, it really is, at the next level
2138700	2143820	down, a fundamental choice. And we have to make those fundamental choices as system designers,
2143820	2148700	or enable our algorithms to be able to learn how to make those choices for themselves.
2149260	2155900	So, then the next step you mentioned, the very first thing you have to deal with is,
2155900	2161660	can you even take in this huge stream of observations and do anything with it? So,
2161660	2168060	the natural next basic question is, what is the, what is deeper enforcement learning?
2168060	2173820	And what is this idea of using neural networks to deal with this huge incoming stream?
2174460	2179980	So, amongst all the approaches for reinforcement learning, deep reinforcement learning is one
2180940	2190220	family of solution methods that tries to utilize powerful representations that are offered by
2190220	2197820	neural networks to represent any of these different components of the solution, of the agent.
2197820	2202620	Like, whether it's the value function, or the model, or the policy, the idea of deep learning
2202620	2208220	is to say, well, here's a powerful toolkit that's so powerful that it's universal in the sense that
2208220	2213580	it can represent any function, and it can learn any function. And so, if we can leverage that
2213580	2219100	universality, that means that whatever we need to represent for our policy, or for our value
2219100	2224700	function for a model, deep learning can do it. So, that deep learning is one approach
2224780	2231260	that offers us a toolkit that is, has no ceiling to its performance, that as we start to put more
2231260	2238220	resources into the system, more memory and more computation, and more data, more experience of
2238220	2242540	more interactions with the environment, that these are systems that can just get better and better
2242540	2246300	and better at doing whatever the job is they've asked them to do, whatever we've asked that
2246300	2252780	function to represent, it can learn a function that does a better and better job of representing
2252780	2257420	that knowledge, whether that knowledge be estimating how well you're going to do in the
2257420	2262460	world, the value function, whether it's going to be choosing what to do in the world, the policy,
2262460	2266300	or whether it's understanding the world itself, what's going to happen next, the model.
2266860	2274780	Nevertheless, the fact that neural networks are able to learn incredibly complex representations
2274780	2281660	that allow you to do the policy, the model, or the value function is, at least to my mind,
2281660	2289740	exceptionally beautiful and surprising. Was it surprising to you? Can you still
2289740	2294700	believe it works as well as it does? Do you have good intuition about why it works at all
2294700	2303260	and works as well as it does? I think let me take two parts to that question. I think
2306060	2309740	it's not surprising to me that the idea of reinforcement learning
2310380	2317340	works because in some sense, I feel it's the only thing which can, ultimately,
2317340	2323020	and so I feel we have to address it and there must be success as possible because we have
2323020	2330380	examples of intelligence and it must at some level be able to possible to acquire experience and use
2330380	2336380	that experience to do better in a way which is meaningful to environments of the complexity
2336380	2341500	that humans can deal with. It must be. Am I surprised that our current systems can do as well
2341500	2347100	as they can do? I think one of the big surprises for me and a lot of the community
2349500	2360940	is really the fact that deep learning can continue to perform so well despite the
2360940	2365500	facts that these neural networks that they're representing have these incredibly nonlinear
2365820	2372940	bumpy surfaces which to our low-dimensional intuitions make it feel like surely you're
2372940	2377260	just going to get stuck and learning will get stuck because you won't be able to make any
2377260	2385660	further progress and yet the big surprise is that learning continues and these what appear to be
2385660	2390300	local optima turn out not to be because in high dimensions when we make really big neural nets
2390300	2393900	there's always a way out and there's a way to go even lower and then
2394620	2398460	you're still not a local optima because there's some other pathway that will take you out and
2398460	2403660	take you lower still and so no matter where you are learning can proceed and do better and better
2403660	2412700	and better without bound and so that is a surprising and beautiful property of neural nets
2413580	2420220	which I find elegant and beautiful and and somewhat shocking that it turns out to be the case
2420780	2427580	as you said which I really like to our low-dimensional intuitions that's surprising
2428140	2434940	yeah yeah we're very we're very tuned to working within a three-dimensional environment and so
2434940	2442140	to start to visualize what a billion dimensional neural network surface that you're trying to
2442220	2448300	optimize over what that even looks like is very hard for us and so I think that really if you try
2448300	2456700	to account for for the essentially the AI winter where where people gave up on neural networks
2456700	2462380	I think it's really down to that that lack of ability to generalize from from low dimensions
2462380	2466380	to high dimensions because back then we were in the low-dimensional case people could only
2466380	2473660	build neural nets with you know 50 nodes in them or something and to to imagine that it might be
2473660	2477420	possible to build a billion dimensional neural net and that it might have a completely different
2477420	2482940	qualitatively different property was very hard to anticipate and I think even now we're starting
2482940	2488860	to build the the theory to support that and and it's incomplete at the moment but all of the
2488860	2494140	theory seems to be pointing in the direction that indeed this is an approach which which truly is
2494220	2498460	universal both in its representational capacity which was known but also in its learning ability
2498460	2505420	which is which is surprising and it makes one wonder what else we're missing due to our low
2505420	2512460	dimensional intuitions that that will seem obvious once it's discovered I often wonder
2513500	2522220	you know when we one day do have AI's which are superhuman in their abilities to to understand
2522220	2529660	the world what will they think of of the algorithms that we developed back now will it be you know
2529660	2537020	looking back at these these days and you know and and thinking that well will we look back and feel
2537020	2541580	that these algorithms were were naive first steps or will they still be the fundamental ideas which
2541580	2549340	are used even in a hundred thousand ten thousand years yeah I know they'll they'll watch back to
2549340	2556460	this conversation and and uh with a smile maybe a little bit of a laugh I mean my my sense is um
2557580	2565100	I think it's just like when we used to think that the the sun revolved around the earth
2565900	2572460	they'll see our systems of today reinforcement learning as too complicated that the answer was
2572460	2579340	simple all along there's something just just like you said in the game of go I mean I love the
2579340	2584940	systems of like cellular automata that there's simple rules from which incredible complexity
2584940	2591740	emerges so it feels like there might be some very least simple approaches just like where Sutton
2591740	2600620	says right these simple methods with compute over time seem to prove to be the most effective
2600620	2610940	I 100% agree I think that if we try to anticipate what will generalize well into the future I think
2610940	2616780	it's likely to be the case that it's the simple clear ideas which will have the longest legs and
2616780	2621100	which will carry us furthest into the future nevertheless we're in a situation where we need
2621100	2625900	to make things work right and today and sometimes that requires putting together more complex systems
2626620	2631500	where we don't have the the full answers yet as to what those minimal ingredients might be
2631500	2639340	so speaking of which if we could take a step back to go uh what was mogo and what was the key idea
2639340	2646780	behind the system so back during my um phd on computer go around about that time there was a
2646780	2653100	a major new development in in which actually happened in the context of computer go and
2654060	2660140	and it was really a revolution in the way that heuristic search was was done and and the idea was
2660860	2667260	essentially that um a position could be evaluated or a state in general could be evaluated
2668700	2675260	not by humans saying whether that um position is good or not or even humans providing rules as to
2675260	2683580	how you might evaluate it but instead by allowing the system to randomly play out the game until the
2683580	2690460	end multiple times and taking the average of those outcomes as the prediction of what will happen
2690460	2696140	so for example if you're in the game of go the intuition is that you take a position and you
2696140	2700140	get the system to kind of play random moves against itself all the way to the end of the game and you
2700140	2705740	see who wins and if black ends up winning more of those random games than white well you say hey
2705740	2709660	this is a position that favors white and if white ends up winning more of those random games than
2709660	2721180	black then it favors white um so that idea um was known as Monte Carlo um um search and a particular
2721180	2725980	form of Monte Carlo search that became very effective and was developed in computer go
2725980	2731820	first by Remy Coulomb in 2006 and then taken further by others was something called Monte
2731820	2739740	Carlo tree search which basically takes that same idea and uses that that insight to evaluate every
2739740	2745100	node of a search tree is evaluated by the average of the random playouts from that from that node
2745100	2751980	onwards um and this idea was very powerful and suddenly led to huge leaps forward in the strength
2752060	2758460	of computer go playing programs um and among those the the strongest of the go playing programs in
2758460	2764860	those days was a program called mogo which was the first program to actually reach human master
2764860	2771020	level on small boards nine by nine boards and so this was a program by someone called sylvan jelly
2771660	2777420	who's a good colleague of mine but i worked with him a little bit um in those days part of my phd
2778300	2784460	and mogo was a a first step towards the latest successes we saw in computer go
2785260	2792860	but it was still missing a key ingredient mogo was evaluating purely by random rollouts against
2792860	2798940	itself and in a way it's it's truly remarkable that random play should give you anything at all
2798940	2805020	yeah like why why in this perfectly deterministic game that's very precise and involves these very
2805020	2812860	exact sequences why is it that that random randomization is is is helpful and so the intuition
2812860	2818940	is that randomization captures something about the the nature of the of the the search tree that
2818940	2824140	from a position that you're you're understanding the nature of the search tree um from that node
2824140	2830540	onwards by by by using randomization and this was a very powerful idea and i've seen this in
2830540	2836700	other spaces uh i'm going to talk to Richard karp and so on randomized algorithms somehow
2836700	2844060	magically are able to do exceptionally well and and simplifying the problem somehow makes you wonder
2844060	2850460	about the fundamental nature of randomness in our universe it seems to be a useful thing but so from
2850460	2858300	that moment can you maybe tell the origin story in the journey of alpha go yeah so programs based on
2858300	2864620	Monte Carlo tree search were a first revolution in the sense that they led to um suddenly programs
2864620	2870940	that could play the game to any reasonable level but they they plateaued it seemed that no matter
2870940	2876300	how much effort people put into these techniques they couldn't exceed the level of um amateur
2876300	2882460	dan level go players so strong players but not not anywhere near the level of of professionals
2882460	2889100	never mind the world champion and so that brings us to the birth of alpha go which happened in the
2889100	2899180	context of a startup company known as deep mind i heard them where a a project was born and the
2899180	2909180	project was really a scientific investigation um where um myself and adjo huang and an intern
2909180	2915180	chris madison were exploring a scientific question and that scientific question was really
2917180	2922780	is there another fundamentally different approach to to this key question of of go the key challenge
2922780	2928140	of of how can you build that intuition and how can you just have a system that could look at a
2928140	2933420	position and understand um what moved to play or or how well you're doing in that position who's
2933420	2941900	going to win and so the deep learning revolution had just begun that systems like image net had
2942540	2948540	suddenly been won by deep learning techniques back in 2012 and following that it was natural to ask
2948540	2954060	well you know if if deep learning is able to scale up so effectively with images to to understand
2954060	2961420	them enough to to classify them well why not go why why why not take a um uh the black and white
2961500	2966140	stones of the go board and build some a system which can understand for itself what that means in
2966140	2971660	terms of what moved to pick or who's going to win the game black or white and so that was our
2971660	2977580	scientific question which we we were probing and trying to understand and as we started to look at
2977580	2983740	it we discovered that we could build a a system so in fact our very first paper on alpha go was
2983740	2990060	actually a pure deep learning system which was trying to answer this question and we showed
2990060	2996060	that actually a pure deep learning system with no search at all was actually able to reach human
2996620	3003980	band level master level at the full game of go 19 by 19 boards um and so without any search at all
3003980	3007180	suddenly we had systems which were playing at the level of the best
3008700	3012620	Monte Carlo tree set systems the ones with randomized rollouts so first I was sorry to
3012620	3019660	interrupt but uh that's kind of a groundbreaking notion that's a that's like basically a definitive
3019660	3026700	step away from the a couple of decades of essentially search dominating AI yeah so what how
3026700	3031580	did that make you feel would you think it was a surprising from a scientific perspective
3032300	3037900	in general how to make you feel I I found this to be profoundly surprising um in fact it was so
3037900	3044140	surprising that um that we had a bet back then and like many good projects you know bets are quite
3044140	3050940	motivating and and the bet was you know whether it was possible for a a a system based purely on
3050940	3058460	on deep learning no search at all to beat a a down level human player um and so we had um someone
3058460	3063740	who joined our team um who was a down level player he came in and um and we had this first
3063740	3069740	match um against him and which side of the bet were you on by the way did you hit the losing
3069740	3076460	on the winning side I tend to be an optimist um with the with the power of of of deep learning
3076460	3083100	and and reinforcement learning so the the system won and we were able to beat this um human down
3083100	3088300	level player and for me that was the moment where where it was like okay something something special
3088380	3095660	is afoot here we have a system which um without search is able to to already just look at this
3095660	3101340	position and understand things as well as a strong human player and from that point onwards
3101340	3109420	I really felt that um reaching that reaching the top levels of human play you know professional
3109420	3117820	level well champion level I felt it was actually an inevitability um and and if it was inevitable
3118620	3125820	outcome I was rather keen that it would be us that achieved it so we scaled up this was something
3125820	3133340	where you know so had lots of conversations back then with um Demisus Arbus that um um the um head
3133340	3141100	of of DeepMind who was extremely excited um and we we made the decision to to scale up the project
3141100	3149740	brought more people on board and and so AlphaGo became something where where we we had a clear
3149740	3156140	goal which was to try and um crack this outstanding challenge of AI to see if we could beat the world's
3156140	3163900	best players and this led within the space of um not so many months to playing against the
3163900	3169100	European champion Fan Hui in a match which became you know memorable in history as the
3169100	3175420	first time a go program had ever beaten a professional player and at that time we had to
3175420	3180620	make a judgment as to whether when and and whether we should go and challenge the world
3180620	3186700	champion and and this was a difficult decision to make again we were basing our predictions on
3186700	3192780	on our own progress and had to estimate based on the rapidity of our own progress when we thought we
3192780	3199340	would um exceed the level of the human world champion and and we tried to make an estimate
3199340	3206140	and set up a match and that became the the AlphaGo versus LisaDoll match in um 2016
3207100	3215020	and we should say spoiler alert that AlphaGo was able to defeat LisaDoll that's right yeah
3215020	3226140	so maybe uh we could take even a broader view AlphaGo involves both learning from expert games and
3227980	3232940	as far as I remember a self-played component to where it learns by playing against itself
3234220	3240220	but in your sense what was the role of learning from expert games there and in terms of your
3240300	3246060	self-evaluation whether you can take on the world champion what was the thing that they're trying
3246060	3253500	to do more of sort of train more on expert games or was there's now another I'm asking so many
3254300	3259900	poorly phrased questions but uh did you have a hope or dream that self-play would be the
3259900	3268460	key component at that moment yet so in the early days of AlphaGo we we used human data
3268460	3273020	to explore the science of what deep learning can achieve and so when we had our first paper
3273020	3278620	that showed um that it was possible to predict um the winner of the game that it was possible to
3278620	3283980	suggest moves that was done using human data or solely human data yeah and and and and so the
3283980	3288220	reason that we did it that way was at that time we were exploring separately the deep learning
3288220	3292940	aspect from the reinforcement learning aspect that was the part which was which was new and
3292940	3299900	unknown to to to me at that time was how far could that be stretched once we had that it then
3299900	3305100	became natural to try and use that same representation and see if we could learn for ourselves using
3305100	3311340	that same representation and so right from the beginning actually our goal had been to build
3311340	3318700	a system using self-play and to us the human data right from the beginning was an expedient step
3318700	3323500	to help us for pragmatic reasons to go faster towards the goals of the project
3324460	3329420	than we might be able to starting solely from self-play and so in those days we were very
3329420	3334140	aware that we were choosing to to use human data and that might not be the long-term
3335740	3341660	holy grail of AI but that it was something which was extremely useful to us it helped us to understand
3341660	3346700	the system it helped us to build deep learning representations which were clear and simple and
3346700	3353740	easy to use and so really I would say it's it served a purpose not just as part of the algorithm but
3353740	3359420	something which I continue to use in our research today which is trying to break down a very hard
3359420	3364940	challenge into pieces which are easier to understand for us as researchers and develop so if you if you
3364940	3370940	use a component based on human data it can help you to understand the system such that then you
3370940	3377740	can build the more principled version later that that does it for itself so as I said the Alpha
3377740	3384300	Go victory and I don't think I'm being sort of uh romanticizing this notion I think it's one of the
3384300	3390700	greatest moments in the history of AI so were you cognizant of this magnitude of the accomplishment
3391260	3398140	at the time I mean are you cognizant of it even now because to me I feel like it's something that
3398140	3403660	would we mentioned what the AGI systems of the future will look back I think they'll look back at
3403660	3411580	the Alpha Go victory as like holy crap they figured it out this is where this is where it started
3411580	3416700	well thank you again I mean it's funny because I guess I've been working on I've been working on
3416700	3421180	computer go for a long time so I've been working at the time of the Alpha Go match on computer go
3421180	3427020	for more more than a decade and throughout that decade I'd had this dream of what would it be like
3427020	3432940	to what would it be like really to to actually be able to build a system that could play against
3432940	3438060	the world champion and and I imagined that that would be an interesting moment that maybe you
3438060	3442940	know some people might care about that and that this might be you know a nice achievement
3443980	3451820	but I think when I arrived in in Seoul and discovered the legions of journalists that were
3451820	3458060	following us around and 100 million people that were watching the match online live I realized
3458060	3463260	that I'd been off in my estimation of how significant this moment was by several orders of magnitude
3464300	3473260	and so there was definitely an adjustment process to to realize that this this was something which
3474220	3478700	the world really cared about and which was a watershed moment and I think there was that
3478700	3485180	moment of realization which was also a little bit scary because you know if you go into something
3485180	3490780	thinking it's going to be maybe of interest and then discover that 100 million people are watching
3490780	3494380	it suddenly makes you worry about whether some of the decisions you've made were really the
3494380	3499260	best ones or the wisest or were going to lead to the best outcome and we knew for sure that there
3499260	3503660	were still imperfections in Alpha Go which were going to be exposed to the whole world watching
3504300	3510860	and so yeah it was a it was I think a great experience and I feel privileged to have been
3510860	3518220	part of it, privileged to have led that amazing team, I feel privileged to have been in a moment
3518220	3525580	of history like you say but also lucky that you know in a sense I was insulated from from the
3525580	3530380	knowledge of I think it would have been harder to focus on the research if the full kind of reality
3530460	3536380	of of what was going to come to pass had been known to me and the team I think it was you know
3536380	3539900	we were in our bubble and we were working on research and we were trying to answer the scientific
3539900	3546460	questions and then bam you know the public sees it and I think it was it was it was better that
3546460	3552380	way in retrospect. Were you confident that I guess what were the chances that you could get the win
3553500	3560140	so just like you said I'm a little bit more familiar with another accomplishment
3560140	3564540	than we may not even get a chance to talk to I talked to Oriel Venialis about Alpha Star which
3564540	3571020	is another incredible accomplishment but here you know with Alpha Star and beating the Starcraft
3571020	3576780	there was like already a track record with Alpha Go this is like the really first time you get to
3576780	3583340	see reinforcement learning face the best human in the world so what was your confidence like what
3583340	3591180	was the odds? Well we actually um was there a bet? Funnily enough there was so so just before the
3591180	3597420	match we weren't betting on anything concrete but we all held out a hand everyone in the team held
3597420	3601420	out a hand at the beginning of the match and the number of fingers that they had out on that hand
3601420	3606540	was supposed to represent how many games they thought we would win against Lisa Dahl and there
3606540	3612300	was an amazing spread in the team's predictions but I have to say I predicted 4-1
3615020	3621740	and the reason was based purely on data so I'm a scientist first and foremost and one of the things
3621740	3628460	which we had established was that Alpha Go in around one in five games would develop something
3628460	3632540	which we called a delusion which was a kind of you know hole in its knowledge where it wasn't
3632540	3637580	able to fully understand everything about the position and that hole in its knowledge would
3637580	3643740	persist for tens of moves throughout the game and we knew two things we knew that if there were no
3643740	3649260	delusions that Alpha Go seemed to be playing at a level that was far beyond any human capabilities
3649260	3657260	but we also knew that if there were delusions the opposite was true and in fact you know that's
3657420	3663500	what came to pass we saw all of those outcomes and Lisa Dahl in one of the games played a really
3663500	3671100	beautiful sequence that Alpha Go just hadn't predicted and after that it led it into this
3671100	3676620	situation where it was unable to really understand the position fully and found itself in one of
3676620	3682780	these delusions so indeed 4-1 was the outcome. So yeah and can you maybe speak to it a little bit
3682780	3689820	more what were the five games like what happened is there interesting things that come to memory
3689820	3696140	in terms of the play of the human machine? So I remember all of these games vividly of course
3696780	3702620	you know moments like these don't come too often in the lifetime of a scientist and
3702700	3712940	the first game was magical because it was the first time that a computer program had
3712940	3718540	defeated a world champion in this grand challenge of Go and there was a moment where
3722460	3726380	Alpha Go invaded Lisa Dahl's territory towards the end of the game
3727820	3731660	and that's quite an audacious thing to do it's like saying hey you thought this was going to be
3731660	3735020	your territory in the game but I'm going to stick a stone right in the middle of it and
3735020	3741100	and prove to you that I can break it up and Lisa Dahl's face just dropped he wasn't expecting a
3741100	3749820	computer to do something that audacious. The second game became famous for a move known as
3749820	3758540	Move 37 this was a move that was played by Alpha Go that broke all of the conventions of Go that
3758780	3764300	Go players were so shocked by this they thought that maybe the operator had made a mistake
3765260	3770460	they thought there was something crazy going on and it just broke every rule that Go players
3770460	3775020	are taught from a very young age they're just taught you know this kind of move called a shoulder
3775020	3779980	hit you can only play it on the third line or the fourth line and Alpha Go played it on the fifth
3779980	3785180	line and it turned out to be a brilliant move and made this beautiful pattern in the middle
3785180	3792780	of the board that ended up winning the game and so this really was a clear instance where we could
3792780	3799340	say computers exhibited creativity that this was really a move that was something humans hadn't
3800060	3806140	known about hadn't anticipated and computers discovered this idea they were the ones to say
3806140	3811820	actually you know here's a new idea something new not not in the domains of of human knowledge of the
3811820	3819420	game and and now the humans think this is a reasonable thing to do and and it's part of
3819420	3825500	Go knowledge now. The third game something special happens when you play against a human
3825500	3831660	world champion which again I hadn't anticipated before going there which is you know these these
3831660	3838300	players are amazing Lisa Doll was a true champion 18 time world champion and had this amazing ability
3838300	3847020	to to probe Alpha Go for weaknesses of any kind and in the third game he was losing and we felt
3847020	3854860	we were sailing comfortably to victory but he managed to from nothing stir up this fight and
3854860	3862140	build what's called a double co these kind of repetitive positions and he knew that historically
3862140	3866620	no no computer Go program had ever been able to deal correctly with double code positions
3866620	3872220	and he managed to summon one out of out of nothing and so for us you know this was this
3872220	3875900	was a real challenge like would Alpha Go be able to to to deal with this or would it just kind of
3875900	3881420	crumble in the face of of this situation and fortunately it dealt with it perfectly. The
3881420	3888860	fourth game was was amazing in that Lisa doll appeared to be losing this game Alpha Go thought
3888860	3894940	it was winning and then Lisa doll did something which I think only a true world champion can do
3894940	3899980	which is he found a brilliant sequence in the middle of the game a brilliant sequence that
3901340	3909900	led him to really just transform the position it kind of it it he found just a piece of genius
3909900	3917100	really and after that Alpha Go its its evaluation just tumbled it thought it was winning this game
3917100	3922380	and all of a sudden it tumbled and said oh now I've got no chance and it starts to behave rather
3922380	3928860	oddly at that point in the final game for some reason we as a team were convinced having seen
3928860	3933820	Alpha Go in the previous game suffer from delusions we as a team were convinced
3933820	3937660	that it was suffering from another delusion we were convinced that it was mis-evaluating the
3937660	3942780	position and that that something was going terribly wrong and it was only in the last
3942780	3947420	few moves of the game that we realized that actually although it had been predicting it
3947420	3953500	was going to win all the way through it really was and um and so somehow you know it just taught us
3953500	3957820	yet again that you have to have faith in in your systems when they when they exceed your own level
3957820	3963740	of ability in your own judgment you have to trust in them to to know better than than you the designer
3963740	3970940	once um you've you've bestowed in them the ability to to judge better than you can then trust the
3970940	3981180	system to do so. So just like in the case of Deep Blue beating Gary Kasparov so Gary is I think
3981180	3987180	the first time he's ever lost actually to anybody and I mean there's a similar situation at least
3987180	3997500	at all it's a it's a tragic it's a tragic loss for humans but a beautiful one I think that's kind of
3998460	4007500	from the tragedy sort of emerges over time emerges the kind of inspiring story but
4008940	4016860	Lisa Dahl recently analyses her time and I don't know if we can look too deeply into it but he did
4016860	4023820	say that even if I become number one there's an entity that cannot be defeated so what do you think
4023820	4028540	about these words what do you think about his retirement from the game ago? Well let me take
4028540	4032940	you back first of all to the first part of your comment about Gary Kasparov because actually
4032940	4040940	at the panel yesterday um he specifically said that when he first lost to Deep Blue he he viewed
4040940	4046940	it as a failure he viewed that this this had been a failure of his but later on in his career he
4046940	4052060	said he'd come to realize that actually it was a success it was a success for everyone because
4052060	4059660	this marked a transformational moment for for AI and so even for Gary Kasparov he came to to
4059660	4066220	realize at that moment was was was pivotal and actually meant something much more than than you
4066220	4073980	know his personal loss in that moment. Lisa Dahl I think was a much more cognizant of that even
4073980	4081580	at the time so in his closing remarks to the match he really felt very strongly that what
4081580	4086380	had happened in the AlphaGo match was not only meaningful for AI but but for humans as well
4086380	4092140	and he felt as a go player that it had opened his horizons and meant that he could start exploring
4092140	4097980	new things it brought his joy back for the game of go because it had broken all of the the conventions
4097980	4104460	and barriers and meant that you know suddenly suddenly anything was possible again and so
4104460	4109660	you know I was sad to hear that he'd retired but you know he's been a great a great world champion
4109660	4115260	over many many years and I think you know that he'll be he'll be remembered for that ever more
4116060	4121820	he'll be remembered as the last person to to beat AlphaGo I mean after after that we we
4121820	4129260	increased the power of the system and and the next version of AlphaGo beats the other strong
4129260	4136140	human players 60 games to nil so you know what a great moment for him and something to be remembered
4136140	4143820	for it's interesting that you spent time at AAAI on a panel with Gary Kasparov
4145260	4148540	what I mean it's almost I'm just curious to learn
4150380	4156460	the conversations you've had with Gary and the because he's also now he's written a book about
4156460	4162700	artificial intelligence he's thinking about AI he has kind of a view of it and he talks about AlphaGo
4162700	4170540	a lot what what's your sense I arguably I'm not just being Russian but I think Gary is the greatest
4170540	4178620	chess player of all time the probably one of the greatest game players of all time and you sort of
4180060	4185500	at the center of creating a system that beats one of the greatest players of all time so what
4185500	4191100	is that conversation like is there anything yeah any interesting digs any bets any come
4191100	4198460	any funny things any profound things so Gary Kasparov has an incredible respect for
4200060	4206460	what we did with AlphaGo and you know it's it's an amazing tribute coming from from him of all people
4207420	4213900	that he really appreciates and respects what what we've done and I think he feels that the progress
4213900	4221340	which has happened in in computer chess which later after AlphaGo we we built the AlphaZero system
4222380	4228780	which defeated the the world's strongest chess programs and to Gary Kasparov that moment in
4228780	4234780	computer chess was more profound than than than deep blue and the reason he believes it mattered more
4235580	4239980	was because it was done with with learning and a system which was able to discover for itself
4239980	4246540	new principles new ideas which were able to play the game in a in a in a way which he hadn't always
4247740	4253900	known about or anyone and in fact one of the things I discovered at this panel was that
4253900	4260540	the current world champion Magnus Carlsen apparently recently commented on his improvement
4260540	4265820	in performance and he attributes it to AlphaZero that he's been studying the games of AlphaZero
4265820	4272620	he's changed his style to play more like AlphaZero and it's led to him actually increasing his his
4272620	4280620	his rating to a new peak yeah I guess to me just like to Gary the inspiring thing is that and just
4280620	4286460	like you said with reinforcement learning reinforcement learning and deep learning machine
4286460	4292780	learning feels like what intelligence is yeah and you know you could attribute it to sort of
4293100	4300140	a bitter viewpoint from Gary's perspective from us humans perspective saying that
4300140	4306780	cert pure search that IBM deep blue was doing is not really intelligence but somehow it didn't feel
4306780	4312460	like it and so that's the magical I'm not sure what it is about learning that feels like intelligence
4312460	4319260	but but it does so I think we should not demean the achievements of what was done in previous
4319260	4326540	areas of AI I think that deep blue was an amazing achievement in itself and that heuristic search
4326540	4331820	of the kind that was used by deep blue had some powerful ideas that were in there but it also
4331820	4337180	missed some things so so the fact that the that the evaluation function the way that the chess
4337180	4345100	position was understood was created by humans and not by the machine is a limitation which means that
4346060	4351580	there's a ceiling on how well it can do but maybe more importantly it means that the same idea
4351580	4358060	cannot be applied in other domains where we don't have access to the kind of human grandmasters
4358060	4362860	and that ability to kind of encode exactly their knowledge into an evaluation function and the
4362860	4368140	reality is that the story of AI is that you know most domains turn out to be of the second type
4368140	4373980	where when knowledge is messy it's hard to extract from experts or it isn't even available and so
4374060	4382380	and so so we need to solve problems in a different way and I think alpha goes a step towards solving
4382380	4390060	things in a way which which puts learning as a first class citizen and says systems need to
4390060	4397580	understand for themselves how to understand the world how to judge their the value of of of
4398940	4403180	any action that they might take within that world and any state they might find themselves in and
4403180	4410940	in order to do that we we make progress towards AI yeah so one of the nice things about this
4411820	4418460	about taking a learning approach to the game of go or game playing is that the things you learn the
4418460	4423180	things you figure out are actually going to be applicable to other problems that are real world
4423180	4428780	problems that's sort of that's ultimately I mean there's two really interesting things about alpha
4428780	4435020	go one is the science of it just the science of learning the science of intelligence and then the
4435020	4440780	other is well you're actually learning to figuring out how to build systems that would be potentially
4440780	4447020	applicable in in other applications medical autonomous vehicles robotics all I mean it's
4447020	4455420	just open the door to all kinds of applications so the next incredible step right really the
4455420	4462060	profound step is probably alpha go zero I mean it's arguable I kind of see them all as the same
4462060	4467180	place but really and perhaps you were already thinking that alpha go zero is the natural it was
4467180	4472540	always going to be the next step but it's removing the reliance on human expert games
4473500	4482140	for pre-training as you mentioned so how big of an intellectual leap was this that that self-play
4482140	4488540	could achieve superhuman level performance in its own and maybe could you also say what is self-play
4488540	4497260	I kind of mentioned it a few times but so let me start with self-play so the idea of self-play
4497260	4503020	is something which is really about systems learning for themselves but in the situation
4503020	4509420	where there's more than one agent and so if you're in a game and a game is a played between
4509420	4516700	two players then self-play is really about understanding that game just by playing games
4516700	4521180	against yourself rather than against any actual real opponent and so it's a way to kind of
4522540	4528220	discover strategies without having to actually need to go out and play against
4530220	4532540	any particular human player for example
4533500	4544940	um the main idea of alpha zero was really to you know try and step back from any of the
4544940	4549580	knowledge that we put into the system and ask the question is it possible to come up with a
4549580	4557580	a single elegant principle by which a system can learn for itself all of the knowledge which it
4557580	4564700	requires to play to play a game such as go importantly by taking knowledge out you not only
4565500	4570860	make the system less brittle in the sense that perhaps the knowledge you were putting in was
4570860	4576060	was just getting in the way and maybe stopping the system learning for itself but also you make it
4576060	4582860	more general the more knowledge you put in the harder it is for a system to actually be placed
4583420	4588620	taken out of the system in which it's kind of been designed and placed in some other system
4588620	4592060	that maybe would need a completely different knowledge base to to understand and perform well
4592700	4598540	and so the real goal here is to strip out all of the knowledge that we put in to the point that we
4598540	4604220	can just plug it into something totally different um and that to me is really you know the the
4604220	4608940	promise of AI is that we can have systems such as that which you know no matter what the goal is
4609900	4616060	um no matter what goal we set to the system we can come up with we have an algorithm which
4616060	4622060	can be placed into that world into that environment and can succeed in achieving that goal and then
4622060	4628460	that that's to me is almost the the essence of intelligence if we can achieve that and so alpha
4628460	4635260	zero is a step towards that um and it's a step that was taken in the context of of two player
4635260	4640700	perfect information games like go and chess um we also applied it to Japanese chess
4641420	4648140	so just to clarify the first step was alpha go zero the first step was to try and take all of
4648140	4657420	the knowledge out of alpha go in such a way that it it could play in a in a fully um self-discovered
4657420	4662060	way purely from self-play and to me the the motivation for that was always that we could
4662060	4669980	then plug it into other domains um but we saved that that until later well and in fact I mean
4671500	4676780	just for fun I could tell you exactly the moment where where the idea for alpha zero occurred to
4676780	4681580	me um because I think there's maybe a lesson there for for researchers who are kind of too deeply
4681580	4686300	embedded in there in their research and you know working 24 sevens try and come up with the next
4686380	4694860	idea um which is it actually occurred to me um on honeymoon um and um and I was like at my most
4694860	4704060	fully relaxed state really enjoying myself um and and just bing this like the algorithm for alpha
4704060	4711180	zero just appeared like um and like in in its full form and this was actually before we played
4711180	4719660	against um Lisa doll but we we just didn't I think we were so busy trying to make sure we could beat
4719660	4726300	the um the the world champion that it was only later that we had the the opportunity to step
4726300	4731100	back and and start examining that that sort of deeper scientific question of of whether this
4731100	4739260	could really work so nevertheless so self-play is probably one of the most sort of profound ideas
4739820	4748060	that it represents to me at least artificial intelligence but the fact that you could use
4748060	4755740	that kind of mechanism to uh again beat world-class players that's very surprising so we kind of
4757020	4762540	to me it feels like you have to train in a large number of expert games so was it surprising to
4762540	4767420	you what was the intuition can you sort of think not necessarily at that time even now what's your
4767420	4772700	intuition why this thing works so well why it's able to learn from scratch well let me first say
4772700	4778460	why we tried it so we tried it both because I I feel that it was the deeper scientific question
4778460	4785260	to to be asking to make progress towards AI and also because in general in my research I don't
4785260	4791740	like to do research on questions for which we already know the likely outcome I don't see much
4791740	4797980	value in running an experiment where you're 95 confident that that you will succeed and so we
4797980	4803660	could have tried you know maybe to to take out the go and do something which we we knew for sure
4803660	4808380	it would succeed on but much more interesting to me was to try try on the things which we weren't
4808380	4814620	sure about and one of the big questions on our minds back then was you know could you really
4814620	4820220	do this with self-play alone how far could that go would it be as strong and honestly
4821100	4827260	we weren't sure yeah it was 50-50 I think you know we I really if you'd asked me I wasn't confident
4827260	4832700	that it could reach the same level as these systems but it felt like the right question to ask
4833740	4839660	and even if even if it had not achieved the same level I felt that that was an important
4839980	4843260	direction to be studying and so
4845980	4851820	then low and behold it actually ended up outperforming the the previous version of of
4851820	4858140	AlphaGo and indeed was able to beat it by a hundred games to zero so what's the intuition as to as
4858140	4867340	to why I think the intuition to me is clear that whenever you have errors in a in a system
4868300	4871100	as we did in AlphaGo AlphaGo suffered from these delusions
4871980	4875260	occasionally it would misunderstand what was going on in a position and misevaluate it
4876060	4882060	how can how can you remove all of these these errors errors arise from many sources for us
4882060	4886780	they were arising both from you know starting from the human data but also from the from the
4886780	4891340	nature of the search and the nature of the algorithm itself but the only way to address them
4891340	4898780	in any complex system is to give the system the ability to correct its own errors it must be able
4898780	4903740	to correct them it must be able to learn for itself when it's doing something wrong and correct
4903740	4910540	for it and so it seemed to me that the way to correct delusions was indeed to have more iterations
4910540	4914780	of reinforcement learning that you know no matter where you start you should be able to correct for
4914780	4920380	those errors until it gets to play that out and understand oh well I thought that I was going
4920380	4925020	to win in this situation but then I ended up losing that suggests that I was misevaluating
4925020	4928860	something there's a hole in my knowledge and now now the system can correct for itself and
4928860	4935100	and understand how to do better now if you take that same idea and trace it back all the way to
4935100	4941180	the beginning it should be able to take you from no knowledge from completely random starting point
4941740	4947340	all the way to the highest levels of knowledge that you can achieve in a in a domain and the
4947340	4952540	principle is the same that if you give if you bestow a system with the ability to correct its own
4952540	4958860	errors then it can take you from random to something slightly better than random because it sees the
4958860	4962540	stupid things that the random is doing and it can correct them and then it can take you from that
4962540	4967020	slightly better system and understand well what's that doing wrong and it takes you on to the next
4967020	4974540	level and the next level and and this progress it can go on indefinitely and indeed you know
4974540	4978140	what would have happened if we'd carried on training AlphaGo Zero for longer
4979340	4984220	we saw no sign of it slowing down its improvements or at least it was certainly
4984220	4991420	carrying on to improve and presumably if you had the computational resources this
4991420	4995660	this could lead to better and better systems that discover more and more.
4995660	5000940	So your intuition is fundamentally there's not a ceiling to this process
5001740	5006700	one of the surprising things just like you said is the process of patching errors
5007260	5012620	that's intuitively makes sense that this is a reinforcement learning should be part of that
5012620	5019580	process but what is surprising is in the process of patching your own lack of knowledge you don't
5020140	5027500	open up other patches you keep sort of like there's a monotonic decrease of your weaknesses.
5028380	5031580	Well let me let me back this up you know I think science always should make
5031580	5036940	falsifiable hypotheses yes so let me let me back up this claim with a falsifiable hypothesis
5036940	5043900	which is that if someone was to in the future take Alpha Zero as an algorithm and run it on
5045340	5052060	with greater computational resources that we had available today then I would predict that they
5052060	5056540	would be able to beat the previous system 100 games to zero and that if they were then to do
5056540	5061980	the same thing a couple of years later that that would beat that previous system 100 games to zero
5061980	5068060	and that that process would continue indefinitely throughout at least my human lifetime presumably
5068060	5073500	the game of go would set the the ceiling I mean the game of go would set the ceiling but the game
5073500	5079980	of go has 10 to the 170 states in it so so the ceiling is is unreachable by any computational
5079980	5085100	device that can be built out of the you know 10 to the 80 atoms in the universe.
5086620	5091180	You asked a really good question which is you know do you not open up other errors
5091180	5096540	when you when you correct your previous ones and the answer is is yes you do and so
5097420	5103020	so it's a remarkable fact about about this class of of two player game and also true of
5103020	5113660	single agent games that essentially progress will always lead you to if you have sufficient
5113660	5118620	representational resource like imagine you had could represent every state in a big table
5118620	5125420	of the game then we we know for sure that a progress of self-improvement will lead all the way
5126140	5130460	in the single agent case to the optimal possible behavior and in the two player case to the
5130460	5135980	minimax optimal behavior that is the the best way that I can play knowing that you're playing
5135980	5144540	perfectly against me and so so for those cases we know that even if you do open up some new error
5144540	5148940	that in some sense you've made progress you've you've you're progressing towards the the best
5148940	5157500	that can be done so alpha go was initially trained on expert games with some self-play alpha go zero
5157500	5164460	remove the need to be trained on expert games and then another incredible step for me because I just
5164460	5171260	love chess is to generalize that further to be in alpha zero to be able to play the game of go
5172300	5177580	beating alpha go zero and alpha go and then also being able to play the game of chess
5178140	5185020	and others so what was that step like what's the interesting aspects there that required to make
5185020	5192780	that happen I think the remarkable observation which we saw with alpha zero was that actually
5192780	5199420	without modifying the algorithm at all it was able to play and crack some of ai's greatest
5199420	5206060	previous challenges in particular we dropped it into the game of chess and unlike the previous
5206060	5212620	systems like deep blue which had been worked on for you know years and years and we were able to beat
5212620	5220380	the world's strongest computer chess program convincingly using a system that was fully
5220460	5226700	discovered by its own from from scratch with its own principles and in fact one of the nice things
5226700	5233100	that that we found was that in fact we also achieved the same result in in Japanese chess a variant
5233100	5237340	of chess where where you get to capture pieces and then place them back down on your on your own
5237340	5243020	side as an extra piece so a much more complicated variant of chess and we also beat the world's
5243020	5250060	strongest programs and reach superhuman performance in that game too and it was the very first time
5250060	5256140	that we'd ever run the system on that particular game was the version that we published in the
5256140	5262140	paper on on alpha zero it just worked out of the box literally no no no touching it we didn't have
5262140	5268460	to do anything and and there it was superhuman performance no tweaking no no twiddling and so
5268460	5273100	I think there's something beautiful about that principle that you can take an algorithm and
5273100	5283020	without twiddling anything it just it just works now to go beyond alpha zero what's required alpha
5283020	5289260	zero is is just a step and there's a long way to go beyond that to really crack the deep problems of
5289260	5296220	AI but one of the important steps is to acknowledge that the world is a really messy place you know
5296220	5302700	it's this rich complex beautiful but messy environment that we live in and no one gives us
5302700	5308460	the rules like no one knows the rules of the world at least maybe we understand that it operates
5308460	5314140	according to Newtonian or or quantum mechanics at the micro level or according to relativity at the
5314140	5320620	macro level but that's not a model that's useful useful for us as people to to operate in it somehow
5320620	5325900	the agent needs to understand the world for itself in a way where no one tells it the rules of the
5325900	5332460	game and yet it can still figure out what to do in that world deal with this stream of
5332540	5337820	observations coming in rich sensory input coming in actions going out in a way that allows it to
5337820	5343260	reason in the way that alpha go or alpha zero can reason in the way that these go and chess playing
5343260	5349580	programs can reason but in a way that allows it to take actions in that messy world to to achieve
5349580	5357820	its goals and so this led us to the most recent step in the story of of of alpha go which was a
5357900	5364220	system called mu zero and mu zero is a system which learns for itself even when the rules
5364220	5369820	are not given to it it actually can be dropped into a system with messy perceptual inputs we
5369820	5377020	actually tried it in the in some Atari games the canonical domains of Atari that have been used
5377020	5384300	for reinforcement learning and and this system learned to build a model of these Atari games
5384300	5391820	that was sufficiently rich and useful enough for it to be able to plan successfully and in fact
5391820	5397820	that system not only went on to to beat the state of the art in Atari but the same system without
5397820	5404620	modification was able to reach the same level of superhuman performance in go chess and shogi
5404620	5410140	that we'd seen in alpha zero showing that even without the rules the system can learn for itself
5410140	5415100	just by trial and error just by playing this game of go and no one tells you what the rules are but
5415100	5421020	you just get to the end and and someone says you know win or loss you play this game of chess and
5421020	5426460	someone says win or loss or you you play a game of breakout in Atari and someone just tells you
5426460	5431500	you know your score at the end and the system for itself figures out essentially the rules of the
5431500	5438540	system the dynamics of the world how the world works and that not in any explicit way but just
5438620	5444380	implicitly enough understanding for it to be able to plan in that in that system in order to
5444380	5448860	achieve its goals and that's the you know that's the fundamental process they have to go through when
5448860	5453900	you're facing in any uncertain kind of environment that you would in the real world is figuring out
5453900	5459340	the sort of the rules the basic rules of the game that's right so there's a lot I mean yeah that
5459340	5466540	that allows it to be applicable to basically any domain that could be digitized in the way that it
5466540	5472220	needs to in order to be consumable sort of in order for the reinforcement learning framework to
5472220	5476460	be able to sense the environment to be able to act in the environment and so on the full reinforcement
5476460	5482700	learning problem needs to deal with with worlds that are unknown and and complex and and the agent
5482700	5487900	needs to learn for itself how to deal with that and so Musero is there's a step a further step in
5487900	5493740	that direction one of the things that inspire the general public in just in conversations I have like
5493740	5500220	with my parents or something with my mom that just loves what was done is kind of at least a notion
5500220	5506060	that there was some display of creativity some new strategies new behaviors that were created that
5506060	5511500	that again has echoes of intelligence so is there something that stands up do you see it the same
5511500	5517740	way that there's creativity and there's some behaviors patterns that you saw that alpha
5517740	5526060	zero was able to display that are truly creative so let me start by I think saying that I think
5526060	5533100	we should ask what creativity really means so to me creativity means discovering something
5533660	5541020	which wasn't known before something unexpected something outside of our norms and so in that sense
5541980	5548780	the process of reinforcement learning or the self-play approach that was used by alpha zero
5549740	5555420	is it's the essence of creativity it's really saying at every stage you're playing according to
5555420	5563180	your current norms and you try something and if it works out you say hey here's something great
5563180	5568060	I'm going to start using that and then that process it's like a micro discovery that happens
5568060	5573660	millions and millions of times over the course of the algorithm's life where it just discovers some
5573660	5577900	new idea oh this pattern this pattern's working really well for me I'm gonna I'm gonna start using
5577900	5583180	that oh now oh here's this other thing I can do I can start to to connect these stones together in
5583180	5590300	this way or I can start to you know sacrifice stones or give up on on pieces or play shoulder
5590300	5594220	hits on the fifth line or whatever it is the system's discovering things like this for itself
5594300	5600460	continually repeatedly all the time and so it should come as no surprise to us then when
5600460	5605660	if you leave these systems going that they discover things that are not known to humans
5605660	5613580	that the to the human norms are considered creative and we've seen this several times in fact
5613580	5622780	in alpha go zero we saw this beautiful timeline of discovery where what we saw was that there
5622780	5626940	were these opening patterns that humans play called joseki these are like the patterns that
5626940	5630540	that humans learn to play in the corners and they've been developed and refined over
5630540	5634940	over literally thousands of years in the game of go and what we saw was in the course of the
5635580	5640940	training alpha go zero over the course of the the 40 days that we trained this system
5641820	5648700	it starts to discover exactly these patterns that human players play and over time we found
5648700	5653980	that all of the joseki that humans played were discovered by the system through this process
5653980	5661100	of of self play and this sort of essential notion of creativity but what was really interesting
5661100	5666940	was that over time it then starts to discard some of these in favor of its own joseki that humans
5666940	5672940	didn't know about and it starts to say oh well you thought that the knights move pincer joseki
5672940	5678620	was a great idea but here's something different you can do there which makes some new variations
5678780	5683740	that humans didn't know about and actually now the human go players study the joseki that alpha
5683740	5690540	go played and they become the new norms that are used in in today's top level go competitions
5691180	5698300	that never gets old even just the first to me maybe just makes me feel good as a human being
5698300	5703740	that a self playing mechanism that knows nothing about us humans discovers patterns that we humans
5703740	5709500	do it's this is like an affirmation that we're all doing we're doing okay as humans yeah we've
5710460	5715740	in this domain in other domains we figured out it's like the church will quote about democracy
5715740	5722940	it's the you know it's the but it sucks but it's the best one we've tried so um in general
5722940	5728380	taking a step outside of go and you have like a million accomplishments that have no time to talk
5728380	5736060	about the with alpha star and so on and and the current work but in general this self playing
5736060	5742780	mechanism that you've inspired the world with by beating the world champion go player do you see
5742780	5750620	that as um do you see being applied in other domains do you have sort of dreams and hopes that
5750620	5757260	is applied in both the simulated environments in the constrained environments of games constrained
5757260	5761580	i mean alpha star really demonstrates that you can remove a lot of the constraints but nevertheless
5761580	5767100	it's in a digital simulated environment do you have a hope a dream that it starts being applied
5767100	5773500	in the robotics environment and maybe even in domains that are safety critical and so on and
5774380	5778300	have you know have a real impact in the real world like autonomous vehicles for example which
5778300	5785740	seems like a very far out dream at this point so i absolutely do um hope and and imagine that we
5785740	5790060	will we will get to the point where ideas just like these are used in all kinds of different
5790060	5795020	domains in fact one of the most satisfying things as a researcher is when you start to see other
5795020	5800940	people use your your algorithms in unexpected ways so in the last couple of years there have been
5800940	5809100	you know a couple of nature papers where different teams unbeknownst to to us took alpha zero and
5809100	5816940	applied exactly those same algorithms and ideas to real world problems of huge meaning to to
5816940	5822140	society so one of them was the problem of chemical synthesis and they were able to beat the state of
5822140	5830860	the art in finding pathways of how to actually synthesize chemicals retro retro chemical synthesis
5831820	5835900	and the second paper actually actually just came out a couple of weeks ago in nature
5836620	5842220	showed that in quantum computation you know one of the big questions is how to how to
5842220	5849020	understand the nature of the the the function in quantum computation and a system based on alpha
5849020	5853980	zero beat the state of the art by quite some distance there again so so these are just examples
5853980	5858700	and i think you know that the lesson which we've seen elsewhere in in in machine learning time
5858700	5864140	and time again is that if you make something general it will be used in all kinds of ways you
5864140	5869660	know you provide a really powerful tools to society and and those tools can be used in in
5869660	5876060	amazing ways and so i think we're just at the beginning and and for sure i hope that we we
5876060	5883260	see all kinds of outcomes so the the and the the other side of the question of a reinforcement
5883260	5888860	learning framework is you know usually want to specify reward function and an objective function
5888940	5896300	what do you think about sort of ideas of intrinsic rewards of and when we're not really sure about
5896940	5904700	you know of if we take you know human beings as existence proof that we don't seem to be
5904700	5911500	operating according to a single reward do you think that there's interesting ideas
5912220	5918060	for when you don't know how to truly specify the reward you know that there's some flexibility
5918060	5922620	for discovering it intrinsically or so on in the context of reinforcement learning
5922620	5926700	so i think you know when we think about intelligence it's really important to be clear
5926700	5931260	about the problem of intelligence and i think it's clearest to understand that problem in
5931260	5936220	terms of some ultimate goal that we want the system to to try and solve for and after all if
5936220	5941660	if we don't understand the ultimate purpose of the system do we really even have a clearly defined
5941980	5948220	problem that we're solving at all now within that as with your example for humans
5950300	5956940	the system may choose to create its own motivations and sub-goals that help the system to achieve its
5956940	5963340	ultimate goal and that may indeed be a hugely important mechanism to achieve those ultimate
5963340	5967740	goals but there is still some ultimate goal i think the system needs to be measurable and and
5967740	5973180	evaluated against and even for humans i mean humans we're incredibly flexible we feel that we
5973180	5978780	we can you know any goal that we're given we feel we can we can master to some some degree
5980060	5984380	but if we think of those goals really you know like the the goal of being able to pick up an
5984380	5990620	object or the goal of of being able to communicate or influence people to do things in a in a particular
5990620	5998380	way or whatever those goals are really they are their sub-goals really that we set ourselves
5998380	6004540	you know we choose to pick up the the object we choose to communicate we choose to to influence
6004540	6009820	someone else and we choose those because it we think it will lead us to something you know in
6009820	6016140	later on we think that's helpful to us to achieve some ultimate goal now i don't want to speculate
6016140	6022300	whether or not humans as a system necessarily have a singular overall goal of survival or whatever
6022300	6028220	it is but i think the principle for understanding and implementing intelligence is has to be that if
6028220	6032460	we're trying to understand intelligence or implement our own there has to be a well-defined
6032460	6039900	problem otherwise if it's not i think it's it's like an admission of defeat that for there to be
6039900	6043900	hope for for understanding or implementing intelligence we have to know what we're doing
6043900	6048060	we have to know what we're asking the system to do otherwise if you if you don't have a clearly
6048060	6054460	defined purpose you're not going to get a clearly defined answer the the ridiculous big question that
6054460	6061580	has to naturally follow because i have to pin you down on this on this thing that nevertheless one
6061580	6068780	of the big silly or big real questions before humans is the meaning of life is us trying to
6068780	6073180	figure out our own reward function yeah and you just kind of mentioned that if you want to build
6073260	6078300	intelligent systems and you know what you're doing you should be at least cognizant to some degree
6078300	6085020	of what the reward function is so the natural question is what do you think is the reward
6085020	6090380	function of human life the meaning of life for us humans the meaning of our existence
6092860	6098380	i think you know i'd be speculating beyond my own expertise but but just for fun let me do that
6098380	6102940	yes please and say i think that there are many levels at which you can understand the system
6102940	6109180	and and you can understand something as as optimizing for for a goal at many levels and so
6110220	6115260	so you can understand the you know let's start with the universe like does the universe have a
6115260	6122220	purpose well it feels like it's just at one level just following certain mechanical laws of physics
6122220	6126700	and that that's led to the development of the universe but at another level you can view it as
6127660	6131180	actually there's the second law of thermodynamics that says that this is
6131180	6135500	increasing in entropy over time forever and now there's a view that's been developed by
6136220	6140540	certain people at MIT that this you can think of this as as almost like a goal of the universe
6140540	6146140	that the purpose of the universe is to maximize entropy so there are multiple levels at which
6146140	6153580	you can understand a system the next level down you might say well if the goal is to is to maximize
6153580	6161180	entropy well how do how does how can that be done by a particular system and maybe evolution
6161180	6165900	is something that the universe discovered in order in order to kind of dissipate energy as
6165900	6170780	efficiently as possible and by the way i'm borrowing from max tegmark for some of these
6170780	6178140	metaphors the physicist but if you can think of evolution as a mechanism for for dispersing energy
6179100	6186140	then then evolution you might say is is then becomes a goal which is if if evolution disperses
6186140	6192700	energy by reproducing as efficiently as possible what's evolution then well it's now got its own
6192700	6200220	goal within that which is to actually reproduce as effectively as possible and now how does
6200220	6207340	reproduction how is that made as effective as possible well you need entities within that
6207420	6211260	that can survive and reproduce as effectively as possible and so it's natural that in order to
6211260	6217820	achieve that high level goal those individual organisms discover brains intelligences which
6217820	6226300	enable them to support the the goals of evolution and those brains what do they do well perhaps the
6226300	6232540	early brains maybe they were controlling things at some direct level you know maybe they were the
6232540	6236380	equivalent of pre-programmed systems which were directly controlling what was going on
6237500	6241980	and setting certain you know things in order to achieve these particular particular goals
6242940	6247500	but that led to a another level of discovery which was learning systems you know parts of
6247500	6252380	the brain which are able to to learn for themselves and learn how to to program themselves to achieve
6252380	6258700	any goal and presumably there are parts of the game of the brain where goals are set to parts of
6258700	6263900	that that system and provides this very flexible notion of intelligence that that we as humans
6263900	6268620	presumably have which is the ability to kind of why the reason we feel that we can we can we can
6268620	6273580	achieve any goal so so it's a very long-winded answer to say that you know I think there are many
6273580	6280380	perspectives and many levels at which intelligence can be understood and and each of those levels
6280380	6284300	you can take multiple perspectives like you know you can view the system as as something which is
6284300	6289500	optimizing for a goal which is understanding it at a level by which we can maybe implement it and
6289500	6294460	understand it as AI researchers or computer scientists or you can understand it at the
6294460	6297980	level of the mechanistic thing which is going on that there are these you know atoms bouncing
6297980	6302300	around in the brain and they lead to the the outcome of that system is not in contradiction
6302300	6308780	with the fact that it's it's also a decision-making system that's optimizing for some goal and and
6308780	6315900	purpose I've never heard the the description of the meaning of life structured so beautifully in
6315900	6322620	layers but you did miss one layer which is the next step which you're responsible for which is
6322620	6329740	creating the the artificial intelligence indeed layer on top of that and indeed I can't wait to
6329740	6336860	see well I may not be around but the can't wait to see what the next layer beyond that well well
6336860	6341740	let's just take that that argument you know and pursue it to its natural conclusion so so the
6341740	6348140	next level indeed is for for how can our how can our learning brain achieve its goals most
6348140	6358380	effectively well maybe it does so by by us as learning beings building a system which is able
6358380	6364140	to solve for those goals more effectively than we can and so when we build a system to play the game
6364140	6368860	of go you know when I said that I wanted to build a system that can play go better than I can I've
6368860	6374460	enabled myself to achieve that goal of playing go better than I could by by directly playing it
6374460	6380300	and learning it myself and so now a new layer has been created which is systems which are able to
6380300	6386220	achieve goals for themselves and ultimately there may be layers beyond that where they set sub-goals
6386220	6394460	to parts of their own system in all in order to to achieve those and so forth so incredible so the
6394540	6398780	story of intelligence I think I think is is a multi-layered one and a multi-perspective one
6399900	6405740	we live in an incredible universe David thank you so much first of all for dreaming of using
6405740	6412300	learning to solve go and building intelligence systems and for actually making it happen and
6412300	6417980	for inspiring millions of people in the process it's truly an honor thank you so much for talking
6417980	6423020	today okay thank you thanks for listening to this conversation with David Silver and thank you to
6423020	6428700	our sponsors masterclass and cash app please consider supporting the podcast by signing up
6428700	6435580	to masterclass at masterclass.com slash lex and downloading cash app and using code lex podcast
6435580	6440140	if you enjoy this podcast subscribe on youtube review it with five stars an apple podcast
6440140	6446140	support on patreon or simply connect with me on twitter at lex freedman and now let me leave you
6446140	6452540	some words from david silver my personal belief is that we've seen something of a turning point
6452620	6458220	where we're starting to understand that many abilities like intuition and creativity that
6458220	6463660	we've previously thought were in the domain only of the human mind are actually accessible to machine
6463660	6470220	intelligence as well and I think that's a really exciting moment in history thank you for listening
6470220	6475180	and hope to see you next time
