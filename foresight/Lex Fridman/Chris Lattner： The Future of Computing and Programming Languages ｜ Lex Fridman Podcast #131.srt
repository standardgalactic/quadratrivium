1
00:00:00,000 --> 00:00:04,160
The following is a conversation with Chris Lattner, his second time in the podcast.

2
00:00:04,720 --> 00:00:08,720
He's one of the most brilliant engineers in modern computing, having created

3
00:00:08,720 --> 00:00:14,640
LLVM compiler infrastructure project, the Clang Compiler, the Swift programming language,

4
00:00:14,640 --> 00:00:19,040
a lot of key contributions to TensorFlow and TPUs as part of Google.

5
00:00:19,040 --> 00:00:25,440
He served as vice president of autopilot software at Tesla, was a software innovator and leader at

6
00:00:26,160 --> 00:00:32,480
and now is at SyFive as senior vice president of platform engineering looking to revolutionize

7
00:00:32,480 --> 00:00:38,160
chip design to make it faster, better, and cheaper. Quick mention of each sponsor,

8
00:00:38,160 --> 00:00:42,320
followed by some thoughts related to the episode. First sponsor is Blinkist,

9
00:00:42,320 --> 00:00:47,280
an app that summarizes key ideas from thousands of books. I use it almost every day to learn new

10
00:00:47,280 --> 00:00:53,840
things or to pick which books I want to read or listen to next. Second is Nero,

11
00:00:53,840 --> 00:00:58,400
the maker of functional sugar-free gum and mints that I use to supercharge my mind

12
00:00:58,400 --> 00:01:05,840
with caffeine, Althianine, and B vitamins. Third is Masterclass, online courses from the best people

13
00:01:05,840 --> 00:01:12,480
in the world on each of the topics covered from rockets to game design to poker to writing and

14
00:01:12,480 --> 00:01:18,480
to guitar. And finally, Cash App, the app I use to send money to friends for food,

15
00:01:18,480 --> 00:01:23,600
drinks, and unfortunately lost bets. Please check out the sponsors in the description

16
00:01:23,600 --> 00:01:29,840
to get a discount and to support this podcast. As a side note, let me say that Chris has been an

17
00:01:29,840 --> 00:01:35,760
inspiration to me on a human level because he is so damn good as an engineer and leader of

18
00:01:35,760 --> 00:01:40,960
engineers and yet he's able to stay humble, especially humble enough to hear the voices

19
00:01:40,960 --> 00:01:46,560
of disagreement and to learn from them. He was supportive of me and this podcast from the early

20
00:01:46,560 --> 00:01:52,160
days and for that, I'm forever grateful. To be honest, most of my life, no one really believed

21
00:01:52,160 --> 00:01:57,600
that I would amount to much. So when another human being looks at me and makes me feel like I might

22
00:01:57,600 --> 00:02:03,760
be someone special, it can be truly inspiring. That's a lesson for educators. The weird kid in

23
00:02:03,760 --> 00:02:09,280
the corner with a dream is someone who might need your love and support in order for that dream to

24
00:02:09,280 --> 00:02:14,560
flourish. If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple

25
00:02:14,560 --> 00:02:20,560
Podcasts, follow on Spotify, support on Patreon, or connect with me on Twitter at Lex Freedman.

26
00:02:21,280 --> 00:02:24,560
And now here's my conversation with Chris Latner.

27
00:02:25,760 --> 00:02:32,160
What are the strongest qualities of Steve Jobs, Elon Musk, and the great and powerful

28
00:02:32,160 --> 00:02:35,040
Jeff Dean since you've gotten the chance to work with each?

29
00:02:35,920 --> 00:02:41,120
You're starting with an easy question there. These are three very different people. I guess

30
00:02:41,120 --> 00:02:45,680
you could do maybe a pairwise comparison between them instead of a group comparison.

31
00:02:45,680 --> 00:02:50,080
So if you look at Steve Jobs and Elon, I worked a lot more with Elon than I did with Steve.

32
00:02:50,880 --> 00:02:56,000
They have a lot of commonality. They're both visionary in their own way. They're both very

33
00:02:56,000 --> 00:03:03,120
demanding in their own way. My sense is Steve is much more human factor focused, where Elon is

34
00:03:03,120 --> 00:03:08,000
more technology focused. What does human factor mean? Steve's trying to build things that feel

35
00:03:08,000 --> 00:03:14,240
good, that people love, that affect people's lives, how they live. He's looking into the future a

36
00:03:14,240 --> 00:03:20,640
little bit in terms of what people want, where I think that Elon focuses more on learning how

37
00:03:20,640 --> 00:03:26,400
exponentials work and predicting the development of those. Steve worked a lot of engineers. That was

38
00:03:26,400 --> 00:03:33,200
one of the things that reading the biography and how can a designer essentially talk to engineers

39
00:03:33,200 --> 00:03:38,560
and get their respect? I did not work very closely with Steve. I'm not an expert at all.

40
00:03:38,560 --> 00:03:43,760
My sense is that he pushed people really hard, but then when he got an explanation that made sense to

41
00:03:43,760 --> 00:03:50,720
him, then he would let go. He did actually have a lot of respect for engineering, but he also knew

42
00:03:50,720 --> 00:03:57,200
when to push. When you can read people well, you can know when they're holding back and when you

43
00:03:57,200 --> 00:04:02,800
can get a little bit more out of them. I think he was very good at that. If you compare the other

44
00:04:02,800 --> 00:04:11,840
folks, Jeff Dean is an amazing guy. He's super smart, as are the other guys. Jeff is a really,

45
00:04:11,840 --> 00:04:18,880
really nice guy, well-meaning. He's a classic Googler. He wants people to be happy. He combines

46
00:04:18,880 --> 00:04:23,760
it with brilliance so he can pull people together in a really great way. He's definitely not a CEO

47
00:04:23,760 --> 00:04:30,480
type. I don't think he would even want to be that. If he still programs. He definitely programs.

48
00:04:30,480 --> 00:04:36,160
Jeff is an amazing engineer today, and that has never changed. It's really hard to compare Jeff

49
00:04:36,160 --> 00:04:44,880
to either of those two. I think that Jeff leads through technology and building it himself and

50
00:04:44,880 --> 00:04:50,560
then pulling people in and inspiring them. I think that that's one of the amazing things about Jeff.

51
00:04:50,560 --> 00:04:55,520
But each of these people, with their pros and cons, all are really inspirational and have achieved

52
00:04:55,520 --> 00:05:00,640
amazing things. I've been very fortunate to get to work with these guys.

53
00:05:00,640 --> 00:05:07,040
For yourself, you've led large teams. You've done so many incredible, difficult technical

54
00:05:07,040 --> 00:05:12,480
challenges. Is there something you've picked up from them about how to lead?

55
00:05:12,480 --> 00:05:16,080
Yeah. I think leadership is really hard. It really depends on what you're looking for there.

56
00:05:17,040 --> 00:05:22,080
I think you really need to know what you're talking about. Being grounded on the product,

57
00:05:22,160 --> 00:05:26,000
on the technology, on the business, on the mission is really important.

58
00:05:27,040 --> 00:05:31,520
Being, understanding what people are looking for, why they're there. One of the most amazing

59
00:05:31,520 --> 00:05:36,480
things about Tesla is the unifying vision. People are there because they believe in

60
00:05:36,480 --> 00:05:39,360
clean energy and electrification and all these kinds of things.

61
00:05:41,840 --> 00:05:45,760
The other is to understand what really motivates people, how to get the best people,

62
00:05:45,760 --> 00:05:49,600
how to build a plan that actually can be executed. There's so many different

63
00:05:49,600 --> 00:05:53,360
aspects of leadership and it really depends on the time, the place, the problems.

64
00:05:54,880 --> 00:05:58,560
There's a lot of issues that don't need to be solved. If you focus on the right things and

65
00:05:58,560 --> 00:06:01,440
prioritize well, that can really help move things.

66
00:06:01,440 --> 00:06:05,360
Two interesting things you mentioned. One is you really have to know what you're talking about.

67
00:06:09,200 --> 00:06:11,600
You've worked on a lot of very challenging technical things.

68
00:06:12,160 --> 00:06:19,920
I assume you were born technically savvy, but I assume that's not the case.

69
00:06:21,200 --> 00:06:27,760
How did you develop technical expertise? Even at Google, you worked on, I don't know,

70
00:06:27,760 --> 00:06:31,280
how many projects, but really challenging, very varied.

71
00:06:32,160 --> 00:06:35,440
Compilers, TPUs, hardware, cloud stuff, a bunch of different things.

72
00:06:36,320 --> 00:06:44,240
The thing that I've become more comfortable with as I've gained experience is being okay

73
00:06:44,240 --> 00:06:50,720
with not knowing. A major part of leadership is actually it's not about having the right answer,

74
00:06:50,720 --> 00:06:55,120
it's about getting the right answer. If you're working in a team of amazing people,

75
00:06:56,240 --> 00:07:00,160
and many of these places, many of these companies all have amazing people,

76
00:07:00,160 --> 00:07:04,000
it's the question of how do you get people together? How do you build trust?

77
00:07:04,080 --> 00:07:10,480
How do you get people to open up? How do you get people to be vulnerable sometimes with an idea

78
00:07:10,480 --> 00:07:14,240
that maybe isn't good enough, but it's the start of something beautiful? How do you

79
00:07:16,400 --> 00:07:20,400
provide an environment where you're not just top-down, don't do the thing that I tell you to

80
00:07:20,400 --> 00:07:26,640
do, but you're encouraging people to be part of the solution, and providing a safe space where

81
00:07:26,640 --> 00:07:28,960
if you're not doing the right thing, they're willing to tell you about it?

82
00:07:29,520 --> 00:07:31,200
So you're asking dumb questions?

83
00:07:31,200 --> 00:07:35,760
Oh yeah, dumb questions are my specialty. So I've been in the hardware realm recently,

84
00:07:35,760 --> 00:07:39,920
and I don't know much at all about how chips are designed. I know a lot about using them.

85
00:07:39,920 --> 00:07:45,600
I know some of the principles and the arse technical level of this, but it turns out that

86
00:07:45,600 --> 00:07:50,000
if you ask a lot of dumb questions, you get smarter really quick. And when you're surrounded by

87
00:07:50,000 --> 00:07:53,840
people that want to teach and learn themselves, it can be a beautiful thing.

88
00:07:54,800 --> 00:08:00,800
So let's talk about programming languages, if it's okay, at the highest absurd philosophical

89
00:08:00,800 --> 00:08:08,400
level. Don't get romantic on me, Lex. I will forever get romantic and torture you. I apologize.

90
00:08:10,320 --> 00:08:13,120
Why do programming languages even matter?

91
00:08:14,080 --> 00:08:17,840
Okay, well, thank you very much. So you're saying why should you care about any one

92
00:08:17,840 --> 00:08:20,960
programming language, or why do we care about programming computers?

93
00:08:20,960 --> 00:08:27,520
No, why do we care about programming language design, creating effective programming languages,

94
00:08:30,080 --> 00:08:33,920
choosing a, you know, one program languages versus another programming language,

95
00:08:34,560 --> 00:08:39,680
why we keep struggling and improving through the evolution of these programming languages?

96
00:08:39,680 --> 00:08:43,360
Sure, sure. Okay, so I mean, I think you have to come back to what are we trying to do here,

97
00:08:43,360 --> 00:08:48,160
right? So we have these these beasts called computers that are very good at specific

98
00:08:48,160 --> 00:08:53,200
kinds of things, and we think it's useful to have them do it for us, right? Now, you have this

99
00:08:53,200 --> 00:08:58,000
question of how best to express that, because you have a human brain still that has an idea in its

100
00:08:58,000 --> 00:09:03,840
head, and you want to achieve something. So, well, there's lots of ways of doing this. You can go

101
00:09:03,840 --> 00:09:07,760
directly to the machine and speak assembly language, and then you can express directly what the

102
00:09:07,760 --> 00:09:13,440
computer understands. That's fine. You can then have higher and higher and higher levels of abstraction

103
00:09:13,440 --> 00:09:16,960
up until machine learning, and you're designing a neural net to do the work for you.

104
00:09:18,000 --> 00:09:22,800
The question is, where, where along this way do you want to stop? And what benefits do you get out

105
00:09:22,800 --> 00:09:28,080
of doing so? And so programming languages in general, you have C, you have Fortran, Java, and

106
00:09:29,120 --> 00:09:34,320
Ada, Pascal, Swift, you have lots of different things. They'll have different tradeoffs, and

107
00:09:34,320 --> 00:09:39,040
they're tackling different parts of the problems. Now, one of the things that most programming

108
00:09:39,040 --> 00:09:43,520
languages do is they're trying to make it so that you have pretty basic things like portability

109
00:09:43,520 --> 00:09:48,000
across different hardware. So you've got, I'm going to run on an Intel PC, I'm going to run on a

110
00:09:48,000 --> 00:09:54,080
RISC-5 PC, I'm going to run on an ARM phone or something like that. Fine. I want to write one

111
00:09:54,080 --> 00:09:58,960
program and have it portable, and this is something the assembly doesn't do. Now, when you start looking

112
00:09:58,960 --> 00:10:04,960
at the space of programming languages, this is where I think it's fun, because programming languages

113
00:10:04,960 --> 00:10:10,480
all have tradeoffs, and most people will walk up to them and they look at the surface level of syntax

114
00:10:10,480 --> 00:10:16,320
and say, oh, I like curly braces, or I like tabs, or I like, you know, semicolons or not,

115
00:10:16,320 --> 00:10:22,320
or whatever, right? Subject, fairly subjective, very shallow things. But programming languages,

116
00:10:22,320 --> 00:10:29,120
when done right, can actually be very powerful. And the, the benefit they bring is expression.

117
00:10:30,080 --> 00:10:33,920
Okay. And if you look at programming languages, there's really kind of two different levels

118
00:10:33,920 --> 00:10:39,200
to them. One is the down-in-the-dirt nuts and bolts of how do you get the computer to be efficient,

119
00:10:39,200 --> 00:10:44,080
stuff like that, how they work, type systems, compiler stuff, things like that. The other is

120
00:10:44,080 --> 00:10:49,440
the UI. And the UI for programming language is really a design problem, and a lot of people don't

121
00:10:49,440 --> 00:10:54,160
think about it that way. And the UI, you mean all that stuff with the braces and... Yeah, all that

122
00:10:54,160 --> 00:11:00,240
stuff's the UI, and what it is, and UI means user interface. And so what, what's really going on is

123
00:11:00,320 --> 00:11:07,040
it's the interface between the guts and the human. And humans are hard, right? Humans have

124
00:11:07,920 --> 00:11:11,600
feelings, they have things they like, they have things they don't like. And a lot of people

125
00:11:11,600 --> 00:11:16,640
treat programming languages as though humans are just kind of abstract creatures that cannot be

126
00:11:16,640 --> 00:11:22,560
predicted. But it turns out that actually there are, there is better and worse. Like, people can

127
00:11:22,560 --> 00:11:28,400
tell when a program language is good or when it was an accident, right? And one of the things

128
00:11:28,400 --> 00:11:33,120
with Swift in particular is that a tremendous amount of time by a tremendous number of people

129
00:11:33,120 --> 00:11:37,920
has been put into really polishing and making it feel good. But it also has really good nuts and

130
00:11:37,920 --> 00:11:44,000
bolts underneath it. You said that Swift makes a lot of people feel good. How do you get to that

131
00:11:44,000 --> 00:11:52,720
point? So how do you predict that, you know, tens of thousands, hundreds of thousands of people

132
00:11:52,720 --> 00:11:57,600
are going to enjoy using this, the user experience of this programming language? Well, you can,

133
00:11:57,600 --> 00:12:00,960
you can look at it in terms of better and worse, right? So if you have to write lots of boiler

134
00:12:00,960 --> 00:12:05,200
plate or something like that, you will feel unproductive. And so that's a bad thing. You can

135
00:12:05,200 --> 00:12:09,920
look at it in terms of safety. If like C, for example, is what's called a memory unsafe language.

136
00:12:09,920 --> 00:12:13,840
And so you get dangling pointers, and you get all these kinds of bugs that then you have spent

137
00:12:13,840 --> 00:12:18,160
tons of time debugging, it's a real pain in the butt. And you feel unproductive. And so by

138
00:12:18,160 --> 00:12:24,000
subtracting these things from the experience, you get, you know, happier people. But again,

139
00:12:24,080 --> 00:12:27,440
keep interrupting. I'm sorry. It's so hard to deal with.

140
00:12:29,120 --> 00:12:32,800
If you look at the people, people that are most productive on Stack Overflow,

141
00:12:34,080 --> 00:12:40,080
they are, they have a set of priorities that may not always correlate perfectly with the

142
00:12:40,080 --> 00:12:46,800
experience of the majority of users. You know, if you look at the most upvoted, quote unquote,

143
00:12:46,800 --> 00:12:57,040
correct answer on Stack Overflow is usually really sort of prioritizes like safe code,

144
00:12:57,040 --> 00:13:03,600
proper code, stable code, you know, that kind of stuff, as opposed to like, if I want to use

145
00:13:03,600 --> 00:13:11,760
go to statements in my basic, right, I want to use go to state, like what if 99% of people want

146
00:13:11,760 --> 00:13:15,920
to use go to statements are used completely improper, you know, unsafe syntax.

147
00:13:16,480 --> 00:13:20,080
I don't think that people actually like if you boil it down, you get below the surface level,

148
00:13:20,080 --> 00:13:24,720
people don't actually care about go tos or if statements or things like this, they care about

149
00:13:24,720 --> 00:13:29,840
achieving a goal. Yeah. Right. So the real question is, I want to set up a web server,

150
00:13:29,840 --> 00:13:34,880
and I want to do a thing, whatever, like how, how quickly can I achieve that? Right. And so the

151
00:13:34,880 --> 00:13:39,360
from programming language perspective, there's really two things that matter there. One is

152
00:13:40,000 --> 00:13:45,440
what libraries exist? And then how quickly can you put it together? And what are the tools around

153
00:13:45,440 --> 00:13:50,320
that look like? Right. And, and when you want to build a library that's missing, what do you do?

154
00:13:50,320 --> 00:13:56,240
Okay. Now, this is where you see huge divergence in the force between worlds. Okay. And so you

155
00:13:56,240 --> 00:14:00,400
look at Python, for example, Python is really good at assembling things, but it's not so great

156
00:14:00,400 --> 00:14:04,800
at building all the libraries. And so you get because of performance reasons, other things like

157
00:14:04,800 --> 00:14:10,560
this, as you get Python layered on top of C, for example, and that means that doing certain

158
00:14:10,560 --> 00:14:14,400
kinds of things, well, it doesn't really make sense to do in Python. Instead, you do it in C,

159
00:14:14,400 --> 00:14:18,560
and then you wrap it and then you have, you're living in two worlds and two worlds never is

160
00:14:18,560 --> 00:14:23,040
really great because tooling and the debugger doesn't work right and like all these kinds of things.

161
00:14:23,680 --> 00:14:28,480
Can you clarify a little bit what, what do you mean by Python is not good at building libraries,

162
00:14:28,480 --> 00:14:33,520
meaning it doesn't make it conducive? Certain kinds of libraries. No, but it's just the actual

163
00:14:33,520 --> 00:14:39,280
meaning of the sentence. Yeah. Meaning like it's not conducive to developers to come in and add

164
00:14:39,280 --> 00:14:45,680
libraries, or it's, it's, or the language, or is it the, the duality of the, it's a dance between

165
00:14:46,560 --> 00:14:51,040
Python and C and... Well, so Python's amazing. Python's a great language. I do not mean to say

166
00:14:51,040 --> 00:14:56,800
that Python is bad for libraries. What I meant to say is, they're Python, they're libraries that

167
00:14:56,800 --> 00:15:01,440
Python's really good at, that you can write in Python. But there are other things like if you

168
00:15:01,440 --> 00:15:04,640
want to build a machine learning framework, you're not going to build a machine learning

169
00:15:04,640 --> 00:15:09,440
framework in Python because of performance, for example, or you want GPU acceleration or

170
00:15:09,440 --> 00:15:14,320
things like this. Instead, what you do is you write a bunch of C or C++ code or something

171
00:15:14,320 --> 00:15:21,120
like that, and then you talk to it from Python. Right. And so this is because of decisions that

172
00:15:21,120 --> 00:15:27,120
were made in the Python design and, and those decisions have other counterbalancing forces.

173
00:15:27,120 --> 00:15:31,280
But, but the trick when you start looking at this from a programming language perspective,

174
00:15:31,280 --> 00:15:37,040
you sort of say, okay, cool, how do I build this catalog of libraries that are really powerful?

175
00:15:37,680 --> 00:15:42,480
And how do I make it so that then they can be assembled into ways that feel good and they

176
00:15:42,480 --> 00:15:47,120
generally work the first time? Because when you're talking about building a thing, you have to

177
00:15:47,120 --> 00:15:52,400
include the debugging, the fixing, the turnaround cycle, the development cycle, all that kind of

178
00:15:52,400 --> 00:15:58,320
stuff in, in, into the process of building the thing. It's not just about pounding out the code.

179
00:15:58,320 --> 00:16:02,400
And so this is where things like, you know, catching bugs at compile time is valuable,

180
00:16:02,400 --> 00:16:09,360
for example. But if you dive into the details in this, Swift, for example, has certain things

181
00:16:09,360 --> 00:16:15,440
like value semantics, which is this fancy way of saying that when you treat a treat a variable

182
00:16:15,440 --> 00:16:24,400
like a value, it acts like a mathematical object would. Okay, so in you have used PyTorch a little

183
00:16:24,400 --> 00:16:31,840
bit. In PyTorch, you have tensors, tensors are any n dimensional grid of numbers. Very simple,

184
00:16:31,840 --> 00:16:36,800
you can do plus and other operators on them. It's all totally fine. But why do you need to clone

185
00:16:36,800 --> 00:16:42,960
a tensor sometimes? Have you ever run into that? Yeah. Okay. And so why is that? Why do you need

186
00:16:42,960 --> 00:16:48,880
to clone a tensor? It's the usual object thing that's in Python. So in Python, and just like with

187
00:16:48,880 --> 00:16:52,720
Java and many other languages, this isn't unique to Python. In Python, it has a thing called

188
00:16:52,720 --> 00:16:56,960
reference semantics, which is the nerdy way of explaining this. And what that means is you

189
00:16:56,960 --> 00:17:03,680
actually have a pointer do a thing instead of the thing. Okay. Now, this is due to a bunch of

190
00:17:03,680 --> 00:17:08,560
implementation details that you don't want to go into. But in Swift, you have this thing called

191
00:17:08,560 --> 00:17:13,280
value semantics. And so when you have a tensor in Swift, it is a value. If you copy it, it looks

192
00:17:13,280 --> 00:17:18,880
like you have a unique copy. And if you go change one of those copies, then it doesn't update the

193
00:17:18,880 --> 00:17:23,680
other one, because you just made a copy of this thing. Right. So that's like highly error prone.

194
00:17:24,320 --> 00:17:31,120
In at least computer science math centric disciplines about Python, that like the

195
00:17:32,000 --> 00:17:38,400
the thing you would expect to behave like math, like math, it doesn't behave like math. And in

196
00:17:38,400 --> 00:17:43,840
fact, quietly, it doesn't behave like math and then can ruin the entirety of your math. Exactly.

197
00:17:43,840 --> 00:17:47,920
Well, and then it puts you in debugging land again. Yeah. Right. Now, now you just want to get

198
00:17:47,920 --> 00:17:51,440
something done. And you're like, wait a second, where do I need, where do I need to put clone?

199
00:17:51,440 --> 00:17:56,160
And what level of the stack, which is very complicated, which I thought I was using somebody's

200
00:17:56,160 --> 00:18:00,560
library. And now I need to understand it to know where to clone a thing. Right. And hard to debug,

201
00:18:00,560 --> 00:18:04,480
by the way. Exactly. Right. And so this is where programming languages really matter. Right. And

202
00:18:04,480 --> 00:18:10,960
so in Swift, having value semantics, so that both you get the benefit of math, working like math,

203
00:18:11,680 --> 00:18:15,680
right. But also the efficiency that comes with certain advantages there,

204
00:18:15,680 --> 00:18:19,040
certain implementation details that really benefit you as a programmer. Right. So

205
00:18:21,040 --> 00:18:27,200
how do you know that a thing should be treated like value? Yeah. So Swift has a pretty strong

206
00:18:27,200 --> 00:18:32,560
culture and good language support for defining values. And so if you have an array, so tensors

207
00:18:32,560 --> 00:18:37,680
are one example that the machine learning folks are very used to. Just think about arrays, same

208
00:18:37,680 --> 00:18:42,960
thing, where you have an array, you put, you create an array, you put two or three or four

209
00:18:42,960 --> 00:18:49,680
things into it, and then you pass it off to another function. What happens if that function

210
00:18:49,680 --> 00:18:54,560
adds some more things to it? Well, you'll see it on the side that you pass it in. Right. This is

211
00:18:54,560 --> 00:19:02,080
called reference semantics. Now, what if you pass an array off to a function? It scrolls it away in

212
00:19:02,080 --> 00:19:06,160
some dictionary or some other data structure somewhere. Right. Well, it thought that you just

213
00:19:06,160 --> 00:19:11,600
handed it that array. Then you return back and that reference to that array still exists in the

214
00:19:11,600 --> 00:19:18,160
caller. And they go and put more stuff in it. Right. The person you handed it off to may have

215
00:19:18,160 --> 00:19:22,720
thought they had the only reference to that. And so they didn't know that this was going to change

216
00:19:22,800 --> 00:19:27,440
underneath the covers. And so this is where you end up having to do clone. So like, I was past a

217
00:19:27,440 --> 00:19:32,720
thing, I'm not sure if I have the only version of it. So now I have to clone it. So what value

218
00:19:32,720 --> 00:19:38,320
semantics does is it allows you to say, hey, I have a, so in Swift, it defaults to value semantics.

219
00:19:38,320 --> 00:19:44,960
Also defaults to value semantics. And then, because most things should be changed, then it makes sense

220
00:19:44,960 --> 00:19:48,000
for that to be the default. And one of the important things about that is that arrays and

221
00:19:48,000 --> 00:19:51,680
dictionaries and all these other collections that are aggregations of other things also have

222
00:19:51,680 --> 00:19:56,960
value semantics. And so when you pass this around to different parts of your program, you don't have

223
00:19:56,960 --> 00:20:01,600
to do these defensive copies. And so this is, this is great for two sides, right? It's great

224
00:20:01,600 --> 00:20:06,880
because you define away the bug, which is a big deal for productivity than the number one thing

225
00:20:06,880 --> 00:20:11,440
most people care about. But it's also good for performance. Because when you're doing a clone,

226
00:20:11,440 --> 00:20:15,280
so you pass the array down to the thing, it was like, I don't know if anybody else has it.

227
00:20:15,280 --> 00:20:19,040
I have to clone it. Well, you just did a copy of a bunch of data. It could be big.

228
00:20:19,920 --> 00:20:24,080
And then it could be that the thing that called you is not keeping track of the old thing. So

229
00:20:24,080 --> 00:20:29,280
you just made a copy of it. And you may not have had to. And so the way the value semantics work

230
00:20:29,280 --> 00:20:33,840
is in Swift is it uses a thing called copy on write, which means that you get, you get the

231
00:20:33,840 --> 00:20:40,000
benefit of safety and performance. And it has another special trick. Because if you think

232
00:20:40,000 --> 00:20:44,640
certain languages like Java, for example, they have immutable strings. And so what they're trying

233
00:20:44,640 --> 00:20:50,160
to do is they provide value semantics by having pure immutability. Functional languages have pure

234
00:20:50,160 --> 00:20:54,320
immutability in lots of different places. And this provides a much safer model and provides

235
00:20:54,320 --> 00:20:59,440
value semantics. The problem with this is if you have immutability, everything is expensive.

236
00:20:59,440 --> 00:21:06,160
Everything requires a copy. For example, in Java, if you have a string x and a string y,

237
00:21:06,880 --> 00:21:10,720
you append them together, we have to allocate a new string to hold x, y.

238
00:21:11,200 --> 00:21:13,280
Oh, if they're immutable.

239
00:21:15,120 --> 00:21:19,280
Strings in Java are immutable. And if there's there's optimizations for short ones,

240
00:21:19,280 --> 00:21:24,960
and it's complicated, but but generally think about them as a separate allocation. And so when

241
00:21:24,960 --> 00:21:29,280
you append them together, you have to go allocate a third thing, because somebody might have a

242
00:21:29,280 --> 00:21:32,400
point or two, either of the other ones, right? And you can't go change them. So you have to go

243
00:21:32,400 --> 00:21:37,600
allocate a third thing. Because of the beauty of how the Swift value semantics system works out,

244
00:21:37,680 --> 00:21:43,600
if you have a string and Swift, you say, hey, put in x, right? And they say, append on y, z, w,

245
00:21:44,880 --> 00:21:48,800
it knows that there's only one reference to that. And so can do an in place update.

246
00:21:50,080 --> 00:21:54,160
And so you're not allocating tons of stuff on the side, you're not, you don't have all these

247
00:21:54,160 --> 00:21:58,240
problems. When you pass it off, you can know you have the only reference. If you pass it off to

248
00:21:58,240 --> 00:22:02,880
multiple different people, but nobody changes it, they can all share the same thing. So you get a

249
00:22:02,880 --> 00:22:07,680
lot of the benefit of purely immutable design. And so you get a really nice sweet spot that I

250
00:22:07,680 --> 00:22:11,600
haven't seen in other languages. Yeah, that's interesting. Like, I thought I thought there

251
00:22:11,600 --> 00:22:18,480
was going to be a philosophical like narrative here that you're going to have to pay a cost for it.

252
00:22:19,280 --> 00:22:27,520
Because it sounds like I think value semantics is beneficial for easing of debugging, or

253
00:22:28,160 --> 00:22:33,680
minimizing the risk of errors, like bringing the errors closer to the source,

254
00:22:35,760 --> 00:22:40,080
bringing the symptom of the error closer to the source of the error, however you say that.

255
00:22:40,720 --> 00:22:46,160
But you're saying there's not a performance cost either, if you implement correctly.

256
00:22:46,160 --> 00:22:51,680
Well, so there's tradeoffs with everything. And so if you are doing very low level stuff,

257
00:22:51,680 --> 00:22:55,840
then sometimes you can notice cost. But then what you're doing is you're saying, what is the right

258
00:22:55,920 --> 00:23:00,640
default. So coming back to user interface, when you talk about programming languages,

259
00:23:00,640 --> 00:23:07,200
one of the major things that Swift does that makes people love it, that is not obvious when it comes

260
00:23:07,200 --> 00:23:14,080
to designing a language is this UI principle of progressive disclosure of complexity. So Swift,

261
00:23:14,080 --> 00:23:19,280
like many languages, is very powerful. The question is, when do you have to learn the power as a user?

262
00:23:20,640 --> 00:23:23,680
So Swift like Python allows you to start with like print hello world,

263
00:23:24,560 --> 00:23:28,640
right, certain other languages, start with like public static void main class,

264
00:23:30,480 --> 00:23:35,200
like all the ceremony, right. And so you go to teach, you teach a new person, hey, welcome,

265
00:23:35,200 --> 00:23:41,120
welcome to this new thing. Let's talk about public access control classes, wait, what's that,

266
00:23:41,120 --> 00:23:48,320
string system dot out, print land like packages, like, right. And so instead, if you take this,

267
00:23:48,320 --> 00:23:52,800
and you say, hey, we need, you need, we need packages, you know, modules, we need, we need

268
00:23:52,800 --> 00:23:57,200
powerful things like classes, we need data structures, we need like all these things.

269
00:23:57,200 --> 00:24:00,880
The question is, how do you factor the complexity? And how do you make it so that

270
00:24:00,880 --> 00:24:06,240
the normal case scenario is you're dealing with things that work the right way in the right way

271
00:24:06,240 --> 00:24:11,680
and give you good performance right by default. But then as a power user, if you want to dive down

272
00:24:11,680 --> 00:24:16,880
to it, you have full cc performance, full control over low level pointers, you can call malloc if

273
00:24:16,880 --> 00:24:21,200
you want to call malloc. This is not recommended on the first page of every tutorial, but it's

274
00:24:21,200 --> 00:24:26,080
actually really important when you want to get work done, right. And so being able to have that

275
00:24:26,080 --> 00:24:31,360
is really the design in programming language design. And design is really, really hard. It's

276
00:24:31,360 --> 00:24:37,840
something that I think a lot of people kind of outside of UI, again, a lot of people just think

277
00:24:37,840 --> 00:24:43,760
is subjective, like there's nothing, you know, it's just like curly braces or whatever, it's just

278
00:24:43,760 --> 00:24:47,760
like somebody's preference. But actually, good design is something that you can feel.

279
00:24:48,640 --> 00:24:54,800
And how many people are involved with good design? So if we look at Swift, but look at historically,

280
00:24:54,800 --> 00:24:59,920
I mean, this might touch like, there's almost like a Steve Jobs question too, like,

281
00:25:00,800 --> 00:25:09,440
how much dictatorial decision making is required versus collaborative? And we'll talk about how

282
00:25:09,520 --> 00:25:14,800
that can go wrong or right. But yeah, we'll Swift. So I can't speak to in general, all design

283
00:25:14,800 --> 00:25:21,120
everywhere. So the way it works with Swift is that there's a core team. And so core team is

284
00:25:21,120 --> 00:25:25,680
six or seven people ish something like that that is people that have been working with Swift since

285
00:25:25,680 --> 00:25:32,720
very early days. And so by early days is not that long ago. Okay, yeah. So it's, it became public

286
00:25:32,720 --> 00:25:38,240
in 2014. So it's been six years public now. But but so that's enough time that there's a story

287
00:25:38,240 --> 00:25:43,600
arc there. Okay. And there's mistakes have been made that then get fixed and you learn something

288
00:25:43,600 --> 00:25:48,960
and then you, you know, and so what the core team does is it provides continuity. And so you want

289
00:25:48,960 --> 00:25:55,360
to have a, okay, well, there's a big hole that we want to fill. We know we want to fill it. So

290
00:25:55,360 --> 00:26:00,400
don't do other things that invade that space until we fill the hole, right? There's a boulder

291
00:26:00,400 --> 00:26:03,840
that's missing here. We want to, we will do that boulder, even though it's not today.

292
00:26:04,800 --> 00:26:10,240
Keep, keep out of that space. And the whole team remembers of the remembers the myth of the boulder

293
00:26:10,240 --> 00:26:14,320
that's there. Yeah, yeah, there's a general sense of what the future looks like in broad strokes

294
00:26:14,320 --> 00:26:18,880
and a shared understanding of that, combined with a shared understanding of what has happened in the

295
00:26:18,880 --> 00:26:24,320
past that worked out well and didn't work out well. The next level out is you have the what's

296
00:26:24,320 --> 00:26:28,400
called the Swift Evolution community. And you've got in that case, hundreds of people that really

297
00:26:28,400 --> 00:26:32,880
care passionately about the ways Swift evolves. And that's like an amazing thing to again,

298
00:26:33,840 --> 00:26:37,440
the core team doesn't necessarily need to come up with all the good ideas. You got hundreds of

299
00:26:37,440 --> 00:26:40,960
people out there that care about something, and they come up with really good ideas too.

300
00:26:40,960 --> 00:26:48,400
And that provides this like tumbling rock tumbler for ideas. And so the, the evolution process is,

301
00:26:48,400 --> 00:26:51,600
you know, a lot of people in a discourse forum, they're like hashing it out and trying to like

302
00:26:51,600 --> 00:26:55,600
talk about, okay, well, should we go left or right? Or if we did this, what would be good?

303
00:26:55,600 --> 00:26:58,960
And, you know, here you're talking about hundreds of people. So you're not going to get consensus,

304
00:26:59,600 --> 00:27:06,160
necessarily. You're not obvious consensus. And so there's a proposal process that then allows

305
00:27:06,160 --> 00:27:11,440
the core team and the community to work this out. And what the core team does is it aims to get

306
00:27:11,440 --> 00:27:16,320
consensus out of the community and provide guardrails, but also provide long term,

307
00:27:17,440 --> 00:27:20,240
make sure we're going the right direction kind of things.

308
00:27:20,240 --> 00:27:27,280
So does that group represent like the how much people will love the user interface?

309
00:27:27,280 --> 00:27:29,360
Like, do you think they're able to capture that?

310
00:27:29,360 --> 00:27:32,240
Well, I mean, it's something we talk about a lot. It's something we care about.

311
00:27:32,240 --> 00:27:36,720
How will we, how will we do that for debate? But I think that we've done pretty well so far.

312
00:27:36,720 --> 00:27:40,560
Is the beginner in mind, like you said, the progressive disclosure?

313
00:27:40,560 --> 00:27:46,560
Yeah, so we care a lot about a lot about that, a lot about power, a lot about efficiency, a lot

314
00:27:46,560 --> 00:27:50,960
about there are many factors to good design. And you have to figure out a way to kind of

315
00:27:51,760 --> 00:27:54,480
work your way through that. And so if you like think about

316
00:27:55,200 --> 00:28:00,560
like a language I love is Lisp, probably still because I use Emacs, but I haven't done anything,

317
00:28:00,560 --> 00:28:04,800
any serious working list, but it has a ridiculous amount of parentheses.

318
00:28:06,480 --> 00:28:12,320
I've also, you know, with Java and C++, the braces,

319
00:28:14,240 --> 00:28:21,680
you know, I, I like, I enjoyed the comfort of being between braces, you know, and then Python

320
00:28:21,680 --> 00:28:26,560
is really excited to interrupt just like, and last thing to me as a design, if I was a language

321
00:28:26,560 --> 00:28:35,120
designer, God forbid, is I would be very surprised that Python with no braces would nevertheless

322
00:28:35,120 --> 00:28:40,480
somehow be comforting also. So like, I can see arguments for all of these.

323
00:28:40,480 --> 00:28:44,240
But look at this, this is evidence that it's not about braces versus tabs.

324
00:28:44,240 --> 00:28:46,960
Right. Exactly. You're good. It's a good point.

325
00:28:46,960 --> 00:28:49,920
Right. So like, you know, there's, there's evidence that

326
00:28:49,920 --> 00:28:52,240
but see, like it's one of the most argued about things.

327
00:28:52,240 --> 00:28:54,880
Oh yeah, of course, just like tabs and spaces, which it doesn't, I mean,

328
00:28:55,600 --> 00:28:59,040
there's one obvious right answer, but it doesn't, it doesn't actually matter.

329
00:28:59,040 --> 00:28:59,440
What's that?

330
00:29:00,640 --> 00:29:03,360
Come on for friends. Like, come on, what are you trying to do to me here?

331
00:29:03,360 --> 00:29:05,680
People are going to, yeah, half the people are going to tune out. Yeah.

332
00:29:06,880 --> 00:29:11,840
So, so these two, you're able to identify things that don't really matter for the experience.

333
00:29:12,480 --> 00:29:16,560
Well, no, no, no, it's always a really hard, so the easy decisions are easy.

334
00:29:16,560 --> 00:29:19,440
Right. I mean, you can, fine. Those are not the interesting ones.

335
00:29:19,440 --> 00:29:21,680
The hard ones are the ones that are most interesting. Right.

336
00:29:21,680 --> 00:29:24,800
The hard ones are the places where, hey, we want to do a thing.

337
00:29:25,360 --> 00:29:28,720
Everybody agrees we should do it. There's one proposal on the table,

338
00:29:28,720 --> 00:29:31,760
but it has all these bad things associated with it. Well,

339
00:29:32,320 --> 00:29:34,880
okay, what are we going to do about that? Do we just take it?

340
00:29:34,880 --> 00:29:38,400
Do we delay it? Do we say, hey, well, maybe there's this other feature

341
00:29:38,400 --> 00:29:40,480
that if we do that first, this will work out better.

342
00:29:41,440 --> 00:29:45,840
How does this, if we do this, are we paying ourselves into a corner?

343
00:29:46,080 --> 00:29:48,720
Right. And so this is where, again, you're having that core team of people that

344
00:29:49,520 --> 00:29:53,520
has some continuity and has perspective, has some of the historical understanding

345
00:29:53,520 --> 00:29:57,040
is really valuable because you get, it's not just like one brain,

346
00:29:57,040 --> 00:30:00,000
you get the power of multiple people coming together to make good decisions,

347
00:30:00,000 --> 00:30:06,160
and then you get the best out of all these people, and you also can harness the community around it.

348
00:30:06,160 --> 00:30:10,160
What about like the decision of whether like in Python having one type

349
00:30:10,800 --> 00:30:14,000
or having, you know, strict typing?

350
00:30:14,000 --> 00:30:18,880
Yeah, okay. Yeah, let's talk about this. So I like how you put that, by the way.

351
00:30:18,880 --> 00:30:21,760
Like, so many people would say that Python doesn't have types.

352
00:30:21,760 --> 00:30:23,120
Doesn't have types, yeah. But you're right.

353
00:30:23,120 --> 00:30:27,680
Well, I've listened to you enough to where I'm a fan of yours,

354
00:30:27,680 --> 00:30:32,320
and I've listened to way too many podcasts and videos of you talking about this.

355
00:30:32,320 --> 00:30:35,120
Oh, yeah. So I would argue that Python has one type. And so,

356
00:30:36,240 --> 00:30:39,680
so like when you import Python and Swift, which by the way works really well,

357
00:30:39,680 --> 00:30:41,760
you have everything comes in as a Python object.

358
00:30:41,760 --> 00:30:47,360
Now, here they're trade-offs because, you know, it depends on what you're optimizing for.

359
00:30:47,360 --> 00:30:50,080
And Python is a super successful language for a really good reason,

360
00:30:50,960 --> 00:30:55,200
because it has one type, you get duck typing for free and things like this.

361
00:30:55,200 --> 00:31:00,400
But also, you're pushing, you're making it very easy to pound out code in one hand,

362
00:31:00,400 --> 00:31:05,200
but you're also making it very easy to introduce complicated bugs, the F2D bug,

363
00:31:05,200 --> 00:31:10,080
and you pass the string into something that expects an integer, and it doesn't immediately die,

364
00:31:10,080 --> 00:31:13,280
it goes all the way down the stack trace, and you find yourself in the middle of some code

365
00:31:13,280 --> 00:31:16,240
that you really didn't want to know anything about, and it blows up, and you're just saying,

366
00:31:16,240 --> 00:31:21,600
well, what did I do wrong, right? And so, types are good and bad, and they have trade-offs,

367
00:31:21,600 --> 00:31:24,560
are good for performance, and certain other things, depending on where you're coming from.

368
00:31:24,560 --> 00:31:28,480
But it's all about trade-offs. And so this is what design is, right?

369
00:31:28,480 --> 00:31:33,440
Design is about weighing trade-offs and trying to understand the ramifications of the things

370
00:31:33,440 --> 00:31:37,040
that you're weighing, like types or not, or one type or many types.

371
00:31:37,360 --> 00:31:43,360
But also, within many types, how powerful do you make that type system is another very complicated

372
00:31:43,360 --> 00:31:50,560
question with lots of trade-offs. It's very interesting, by the way. But that's one dimension,

373
00:31:51,680 --> 00:31:55,200
and there's a bunch of other dimensions. JIT-compiled versus static-compiled,

374
00:31:55,200 --> 00:32:00,400
garbage-collected versus reference-counted versus manual memory management versus,

375
00:32:01,600 --> 00:32:05,440
all these different trade-offs and how you balance them are what make the program language good.

376
00:32:05,520 --> 00:32:11,200
Concurrency, so in all those things, I guess, when you're designing the language,

377
00:32:11,200 --> 00:32:14,080
you also have to think of how that's going to get all compiled down to...

378
00:32:15,120 --> 00:32:19,360
If you care about performance, yeah. Well, and go back to Lisp, right? So Lisp,

379
00:32:19,360 --> 00:32:25,120
also I would say JavaScript is another example of a very simple language, right? And so one of the...

380
00:32:25,840 --> 00:32:29,680
So I also love Lisp. I don't use it as much as maybe you do, or you did.

381
00:32:29,760 --> 00:32:35,040
No, I think we're both everyone who loves Lisp. It's like, I don't know,

382
00:32:35,040 --> 00:32:39,120
I love Frank Sinatra, but how often do I seriously listen to Frank Sinatra?

383
00:32:39,120 --> 00:32:44,000
Sure. But you look at that, or you look at JavaScript, which is another very different,

384
00:32:44,000 --> 00:32:48,240
but relatively simple language. And there are certain things that don't exist in the language,

385
00:32:49,040 --> 00:32:52,960
but there is inherent complexity to the problems that we're trying to model.

386
00:32:52,960 --> 00:32:57,040
And so what happens to the complexity? In the case of both of them, for example,

387
00:32:57,120 --> 00:33:00,960
you say, well, what about large-scale software development? Okay. Well, you need something

388
00:33:00,960 --> 00:33:06,320
like packages. Neither language has a language affordance for packages. And so what you get

389
00:33:06,320 --> 00:33:12,000
is patterns. You get things like NPN. You get things like these ecosystems that get built around.

390
00:33:12,000 --> 00:33:17,680
And I'm a believer that if you don't model at least the most important inherent complexity

391
00:33:17,680 --> 00:33:21,440
in the language, then what ends up happening is that complexity gets pushed elsewhere.

392
00:33:22,400 --> 00:33:26,560
And when it gets pushed elsewhere, sometimes that's great because often building things as

393
00:33:26,560 --> 00:33:30,480
libraries is very flexible and very powerful and allows you to evolve and things like that.

394
00:33:30,480 --> 00:33:35,440
But often it leads to a lot of unnecessary divergence in the force and fragmentation.

395
00:33:36,240 --> 00:33:41,840
And when that happens, you just get kind of a mess. And so the question is, how do you balance that?

396
00:33:42,720 --> 00:33:45,600
Don't put too much stuff in the language because that's really expensive and it makes things

397
00:33:45,600 --> 00:33:50,240
complicated. But how do you model enough of the inherent complexity of the problem that

398
00:33:51,120 --> 00:33:54,080
you provide the framework and the structure for people to think about?

399
00:33:55,120 --> 00:34:00,240
So the key thing to think about with programming languages, and you think about what a programming

400
00:34:00,240 --> 00:34:05,200
language is there for, is it's about making a human more productive. And so there's an old,

401
00:34:05,200 --> 00:34:12,560
I think it's Steve Jobs quote about, it's a bicycle for the mind. You can definitely walk,

402
00:34:13,680 --> 00:34:16,800
but you'll get there a lot faster if you can bicycle on your way.

403
00:34:17,440 --> 00:34:22,000
And a programming language is a bicycle for the mind is basically, wow,

404
00:34:22,000 --> 00:34:23,680
that's a really interesting way to think about it.

405
00:34:23,680 --> 00:34:27,280
But by raising the level of abstraction now, you can fit more things in your head.

406
00:34:27,280 --> 00:34:30,000
By being able to just directly leverage somebody's library,

407
00:34:30,000 --> 00:34:36,080
you can now get something done quickly. In the case of Swift, SwiftUI is this new framework

408
00:34:36,080 --> 00:34:42,000
that Apple has released recently for doing UI programming. And it has this declarative programming

409
00:34:42,000 --> 00:34:47,680
model, which defines away entire classes of bugs. It builds on value semantics and many other nice

410
00:34:47,680 --> 00:34:52,640
Swift things. And what this does is allows you just get way more done with way less code.

411
00:34:53,200 --> 00:34:58,640
And now your productivity as a developer is much higher. And so that's really what

412
00:34:58,640 --> 00:35:02,400
programming languages should be about is it's not about tabs versus spaces or curly braces or

413
00:35:02,400 --> 00:35:07,680
whatever. It's about how productive do you make the person. And you can only see that when you

414
00:35:07,760 --> 00:35:13,040
have libraries that were built with the right intention that the language was designed for.

415
00:35:13,600 --> 00:35:18,000
And with Swift, I think we're still a little bit early. But SwiftUI and many other things

416
00:35:18,000 --> 00:35:22,400
that are coming out now are really showing that. And I think that they're opening people's eyes.

417
00:35:22,400 --> 00:35:28,960
It's kind of interesting to think about like how that, you know, the knowledge of something,

418
00:35:29,520 --> 00:35:36,080
of how good the bicycle is, how people learn about that. You know, so I've used C++. Now,

419
00:35:36,080 --> 00:35:41,680
this is not going to be a trash talking session about C++, but you use C++ for a really long

420
00:35:41,680 --> 00:35:49,600
I can go there if you want. I have the scars. I feel like I spent many years without realizing

421
00:35:49,600 --> 00:35:56,000
like there's languages that could, for my particular life style, brain style, thinking

422
00:35:56,000 --> 00:36:01,760
style, there's languages that could make me a lot more productive in the debugging stage,

423
00:36:02,320 --> 00:36:06,560
in just the development stage, and thinking like the bicycle for the mind that could fit

424
00:36:06,560 --> 00:36:10,480
more stuff into my... Python's a great example of that, right? I mean, a machine learning framework

425
00:36:10,480 --> 00:36:15,040
in Python is a great example of that. It's just very high abstraction level. And so you can be

426
00:36:15,040 --> 00:36:19,920
thinking about things on a like very high level algorithmic level, instead of thinking about,

427
00:36:19,920 --> 00:36:25,040
okay, well, am I copying this tensor to a GPU or not? Right? It's not what you want to be thinking

428
00:36:25,040 --> 00:36:30,400
about. And as I was telling you, I mean, I guess the question I had is, you know, how does a person

429
00:36:30,400 --> 00:36:39,040
like me or general people discover more productive, you know, languages? Like how I was, as I've been

430
00:36:39,040 --> 00:36:44,320
telling you offline, I've been looking for like a project to work on in Swift, so I can really

431
00:36:44,320 --> 00:36:49,680
try it out. I mean, my intuition was like doing a Hello World is not going to get me there,

432
00:36:51,200 --> 00:36:55,920
to get me to experience the power of language. You need a few weeks to change your metabolism.

433
00:36:55,920 --> 00:37:01,440
Exactly. I think it's beautifully put. That's one of the problems with people with diets.

434
00:37:01,440 --> 00:37:07,760
Like I'm actually currently to go in parallel, but in a small tangent is I've been recently

435
00:37:07,760 --> 00:37:16,400
eating only meat. Okay. Okay. And okay. So most people are like, the thing that's horribly unhealthy

436
00:37:16,400 --> 00:37:21,760
or whatever, you have like a million, whatever the science is, it just doesn't sound right.

437
00:37:22,320 --> 00:37:26,400
Well, so back when I was in college, we did the Atkins diet. That was, that was a thing and

438
00:37:26,400 --> 00:37:31,920
similar. And, but if you, you have to always give these things a chance. I mean, with dieting,

439
00:37:31,920 --> 00:37:38,000
always not dieting, but just the things that you like. If I eat personally, if I eat meat,

440
00:37:38,000 --> 00:37:44,480
just everything, I could be super focused or more focused than usual. I just feel great. I've been,

441
00:37:44,480 --> 00:37:48,560
I've been running a lot of, you know, doing push-ups and posts and so on. And your Python

442
00:37:48,560 --> 00:37:55,120
is similar in that sense for me. Where are you going with this? I mean, literally, I just,

443
00:37:55,120 --> 00:38:01,440
I felt ahead like a stupid smile on my face when I first started using Python. I could

444
00:38:01,440 --> 00:38:07,680
code up really quick things. Like I, like I would see the world. I'll be empowered to write a script

445
00:38:07,680 --> 00:38:13,440
to, to, um, you know, to do some basic data processing, to rename files on my computer.

446
00:38:13,440 --> 00:38:20,480
Yeah. Right. And like Perl didn't do that for me, uh, a little bit. Well, and, and again, like

447
00:38:20,480 --> 00:38:24,080
none of these are about which, which is best or something like that, but there's definitely

448
00:38:24,080 --> 00:38:28,720
better and worse here. But it clicks, right? Well, yeah. And if you, if you look at Perl,

449
00:38:28,720 --> 00:38:34,480
for example, you get bogged down in, uh, scalers versus arrays versus hashes versus type globs and

450
00:38:34,480 --> 00:38:39,200
like all that kind of stuff. And, and Python's like, yeah, let's not do this. Right. And some of

451
00:38:39,200 --> 00:38:44,240
it's debugging. Like everyone has different priorities, but for me it's, can I create systems

452
00:38:44,240 --> 00:38:51,200
for myself that empower me to debug quickly? Like I've always been a big fan, even just crewed

453
00:38:51,200 --> 00:38:58,800
like asserts, like always, uh, stating things that should be true, uh, which in Python I found

454
00:38:58,800 --> 00:39:03,840
in myself do more because of type, all these kinds of stuff. Well, you could think of types in a

455
00:39:03,840 --> 00:39:07,760
program language as being kind of assert. Yeah. They could check to compile time. Right.

456
00:39:08,880 --> 00:39:13,600
So how do you learn a new thing? Well, so this, or how do, how do people learn new things? Right.

457
00:39:13,600 --> 00:39:18,560
This, this is hard. Uh, people don't like to change. People generally don't like change around

458
00:39:18,560 --> 00:39:24,560
them either. And so, uh, we're all very slow to adapt and change. And usually there's a catalyst

459
00:39:24,560 --> 00:39:29,600
that's required to, to force yourself over the, over, over this. So for learning a programming

460
00:39:29,680 --> 00:39:35,200
language, it just really comes down to finding an excuse, like build a thing that's, that the

461
00:39:35,200 --> 00:39:41,680
language is actually good for that the ecosystem's ready for. Um, and so, um, and so if you were to

462
00:39:41,680 --> 00:39:45,600
write an iOS app, for example, that'd be the easy case. Obviously you would, you would use Swift

463
00:39:45,600 --> 00:39:52,320
for that. Right. There are Android. So Swift runs on Android. Oh, does it? Oh yeah. Yeah. Swift runs

464
00:39:52,320 --> 00:39:59,440
in lots of places. So, uh, okay. So Swift, Swift, Swift is built on top of LLVM. LLVM runs

465
00:39:59,440 --> 00:40:06,320
everywhere. LLVM, for example, builds the Android kernel. Oh, okay. So yeah. Um, so they realize

466
00:40:06,320 --> 00:40:11,600
this. Yeah. So Swift, Swift is very portable, runs on Windows. There's, it runs on lots of

467
00:40:11,600 --> 00:40:16,560
different things. And, uh, Swift, sorry to interrupt that. Swift UI. And then there's a thing called

468
00:40:16,640 --> 00:40:23,120
UIKit. Can I build an app with Swift? Uh, well, so that, that's the thing is the ecosystem is what

469
00:40:23,120 --> 00:40:28,640
matters there. So Swift UI and UIKit are Apple technologies. Okay. Got it. And so they happen

470
00:40:28,640 --> 00:40:33,520
to, like Swift UI happens to be written in Swift, but it's an Apple proprietary framework that, um,

471
00:40:33,520 --> 00:40:37,920
Apple loves and wants to keep on its platform, which makes total sense. You go, go to Android and

472
00:40:37,920 --> 00:40:43,040
you don't have that library. Yeah. Right. And so Android has a different ecosystem of things that

473
00:40:43,120 --> 00:40:47,280
hasn't been built out and doesn't work as well with Swift. And so you can totally use Swift to do,

474
00:40:48,000 --> 00:40:52,480
like arithmetic and things like this, but building a UI with Swift on Android is not,

475
00:40:52,480 --> 00:40:57,760
not, not a, not a great example right now. So, so if I wanted to, uh, to learn Swift, what's the,

476
00:40:58,720 --> 00:41:05,520
I mean, the one practical different version of that is, um, Swift for TensorFlow, for example.

477
00:41:05,520 --> 00:41:11,760
And one of the inspiring things for me with both TensorFlow and PyTorch is how quickly the community

478
00:41:11,760 --> 00:41:17,920
can like switch from different libraries. Yep. Like you could see some of the communities switching

479
00:41:17,920 --> 00:41:23,600
to PyTorch now, but it could, it's very easy to see and then TensorFlow is really stepping up its

480
00:41:23,600 --> 00:41:28,960
game. And then there's no reason why, I think the way it works is basically it has to be one GitHub

481
00:41:28,960 --> 00:41:33,600
repo, like one paper steps up. It gets people excited. It gets people excited and they're like,

482
00:41:33,600 --> 00:41:40,720
ah, I have to learn this at Swift for what, what's Swift again? And then they learn and they fall

483
00:41:40,720 --> 00:41:44,320
along with it. I mean, that's what happened with PyTorch. There has to be a reason, a catalyst.

484
00:41:44,320 --> 00:41:50,080
Yeah. And so, and, and there, I mean, people don't like change, but it turns out that once you've

485
00:41:50,080 --> 00:41:54,800
worked with one or two programming languages, the, the basics are pretty similar. And so

486
00:41:54,800 --> 00:41:58,320
one of the fun things about learning programming languages, even, even maybe less, but I don't

487
00:41:58,320 --> 00:42:02,560
know if you agree with this, is that when you start doing that, you start learning new things

488
00:42:03,840 --> 00:42:08,160
because you have a new way to do things and you're forced to do them and that forces you to

489
00:42:08,240 --> 00:42:11,920
explore. And it puts you in learning mode. And when you get in learning mode, your mind kind of

490
00:42:11,920 --> 00:42:16,320
opens a little bit and you can, you can see things in a new way, even when you go back to the old

491
00:42:16,320 --> 00:42:23,520
place. Right. Yeah. So it would list with functional stuff. But I wish there was a kind of window.

492
00:42:23,520 --> 00:42:29,200
Maybe you can tell me if there is, there you go. This is a question to ask what is the most beautiful

493
00:42:29,200 --> 00:42:34,800
feature in a programming language. Before I ask, let me say like with Python, I remember I saw

494
00:42:34,800 --> 00:42:43,040
list comprehensions. It was like, when I like really took it in. Yeah. I don't know. I just

495
00:42:43,040 --> 00:42:47,120
loved it. It was like fun to do like, it was fun to do that kind of,

496
00:42:49,280 --> 00:42:54,240
it was something about it to be able to filter through a list and to create a new list on a

497
00:42:54,240 --> 00:43:01,440
single line was elegant. I could all get into my head and it just made me fall in love with the

498
00:43:01,440 --> 00:43:07,520
language. So it's there. Let me ask you a question. Is there what to use the most beautiful feature

499
00:43:07,520 --> 00:43:15,040
in a program languages that you've ever encountered in Swift, maybe, and then outside of Swift?

500
00:43:15,040 --> 00:43:20,000
I think the thing that I like the most from a programming language. So I think the thing you

501
00:43:20,000 --> 00:43:24,400
have to think about with the programming language, again, what is the goal? You're trying to get

502
00:43:24,400 --> 00:43:30,400
people to get things done quickly. And so you need libraries, you need high quality libraries.

503
00:43:30,400 --> 00:43:34,320
And then you need a user base around them that can assemble them and do cool things with them.

504
00:43:34,320 --> 00:43:38,000
Right. And so to me, the question is what enables high quality libraries?

505
00:43:39,680 --> 00:43:46,720
Yeah. Yeah. And there's a huge divide in the world between libraries who enable high quality

506
00:43:46,720 --> 00:43:54,240
libraries versus the ones that put special stuff in the language. So programming languages that

507
00:43:54,240 --> 00:44:01,920
enable high quality libraries. Got it. So what I mean by that is expressive libraries that then

508
00:44:01,920 --> 00:44:08,640
feel like a natural integrated part of the language itself. So an example of this in Swift is that

509
00:44:08,640 --> 00:44:13,600
int and float and also array and string, things like this, these are all part of the library.

510
00:44:13,600 --> 00:44:21,040
Like int is not hard coded into Swift. And so what that means is that because int is just a library

511
00:44:21,040 --> 00:44:24,640
thing defined in the standard library, along with strings and arrays and all the other things that

512
00:44:24,640 --> 00:44:31,520
come with the standard library. Well, hopefully you do like int. But anything that any language

513
00:44:31,520 --> 00:44:36,720
features that you needed to define int, you can also use in your own types. So if you wanted to

514
00:44:36,720 --> 00:44:42,880
find a quaternion or something like this, right? Well, it doesn't come in the standard library.

515
00:44:43,520 --> 00:44:49,280
There's a very special set of people that care a lot about this. But those people are also important.

516
00:44:49,280 --> 00:44:53,680
It's not about classism, right? It's not about the people who care about instant floats are more

517
00:44:53,680 --> 00:44:57,440
important than the people who care about quaternions. And so to me, the beautiful things about programming

518
00:44:57,440 --> 00:45:02,880
languages is when you allow those communities to build high quality libraries that feel native,

519
00:45:02,880 --> 00:45:06,560
that feel like they're built into the compiler without having to be.

520
00:45:07,920 --> 00:45:17,840
What does it mean for the int to be part of not hard coded in? So what is an int?

521
00:45:19,360 --> 00:45:24,480
Int is just an integer. In this case, it's like a 64-bit integer or something like this.

522
00:45:24,480 --> 00:45:28,080
But so the 64-bit is hard coded or no?

523
00:45:28,080 --> 00:45:32,080
No, none of that's hard coded. So int, if you go look at how it's implemented,

524
00:45:32,080 --> 00:45:37,520
it's just a struct in Swift. And so it's a struct. And then how do you add two structs? Well,

525
00:45:37,520 --> 00:45:43,520
you define plus. And so you can define plus on int. Well, you can define plus on your thing, too.

526
00:45:43,520 --> 00:45:49,120
You can define int as an odd method or something like that on it. And so, yeah, you can add methods

527
00:45:49,120 --> 00:45:57,120
on a thing. So you can define operators like how it behaves. That's used beautiful when there's

528
00:45:57,120 --> 00:46:04,720
something about the language which enables others to create libraries which are not hacky.

529
00:46:05,280 --> 00:46:09,280
Yeah, they feel native. And so one of the best examples of this is Lisp.

530
00:46:10,800 --> 00:46:15,760
Because in Lisp, all the libraries are basically part of the language. You write

531
00:46:15,760 --> 00:46:20,880
term rewrite systems and things like this. And so can you, as a counter example, provide

532
00:46:20,880 --> 00:46:25,360
what makes it difficult to write a library that's native? Is it the Python C?

533
00:46:25,360 --> 00:46:31,280
Well, so one example, I'll give you two examples. Java and C++, there's Java and C.

534
00:46:32,960 --> 00:46:37,360
They both allow you to define your own types. But int is hard coded in the language.

535
00:46:38,240 --> 00:46:42,720
Okay. Well, why? Well, in Java, for example, coming back to this whole reference semantic

536
00:46:42,800 --> 00:46:53,120
value semantic thing, int gets passed around by value. But if you make a pair or something like

537
00:46:53,120 --> 00:46:58,640
that, a complex number, it's a class in Java. And now it gets passed around by reference,

538
00:46:58,640 --> 00:47:06,800
by pointer. And so now you lose value semantics. You lost math. Okay. Well, that's not great.

539
00:47:07,040 --> 00:47:09,120
If you can do something with int, why can't I do it with my type?

540
00:47:10,160 --> 00:47:17,280
Right. So that's the negative side of the thing I find beautiful is when you can solve that,

541
00:47:17,280 --> 00:47:23,440
when you can have full expressivity where you as a user of the language have as much or almost

542
00:47:23,440 --> 00:47:27,840
as much power as the people who implemented all the standard built-in stuff. Because what that

543
00:47:27,840 --> 00:47:33,840
enables is that enables truly beautiful libraries. You know, it's kind of weird because I've gotten

544
00:47:33,840 --> 00:47:39,600
used to that. That's one, I guess, other aspect of program language design. You have to think,

545
00:47:40,480 --> 00:47:46,160
you know, the old first principles thinking, like, why are we doing it this way? By the way,

546
00:47:46,160 --> 00:47:52,400
I mean, I remember because I was thinking about the Waller's operator and I'll ask you about it

547
00:47:52,400 --> 00:48:00,480
later. But it hit me that like the equal sign for assignment, like why are we using the equal

548
00:48:00,480 --> 00:48:05,360
sign? It's wrong. And that's not the only solution, right? So if you look at Pascal,

549
00:48:05,360 --> 00:48:12,720
they use colon equals for assignment and equals for equality. And they use like less and greater

550
00:48:12,720 --> 00:48:18,720
than instead of the not equal. Like there are other answers here. So but like, and yeah, like I

551
00:48:18,720 --> 00:48:25,760
ask you all, but how do you then decide to break convention to say, you know what?

552
00:48:26,720 --> 00:48:33,600
That everybody's doing it wrong. We're going to do it right. Yeah. So it's like an ROI,

553
00:48:33,600 --> 00:48:38,160
like return on investment trade off, right? So if you do something weird, let's just say like

554
00:48:38,160 --> 00:48:43,520
not like colon equal instead of equal for assignment, that would be weird with today's

555
00:48:43,520 --> 00:48:48,160
aesthetic, right? And so you'd say, cool, this is theoretically better. But is it better in which

556
00:48:48,160 --> 00:48:53,200
ways? Like, what do I get out of that? Do I define away class of bugs? Well, one of the class of

557
00:48:53,200 --> 00:49:00,480
bugs that C has is that you can use like, you know, if x equals without equals equals fx equals y.

558
00:49:00,480 --> 00:49:06,320
Yeah. Right. Well, turns out you can solve that problem in lots of ways. Clang, for example,

559
00:49:06,320 --> 00:49:10,640
GCC, all these compilers will detect that as a as a likely bug, produce a warning.

560
00:49:10,640 --> 00:49:16,800
Do they? Yeah, I feel like they didn't or clang doesn't GCC didn't. And it's like,

561
00:49:16,800 --> 00:49:20,640
one of the important things about programming language design is like, you're literally

562
00:49:20,640 --> 00:49:28,000
creating suffering in the world. Like, I feel like, I mean, one way to see it is the

563
00:49:28,000 --> 00:49:32,000
bicycle for the mind, but the other way is to like, minimizing suffering.

564
00:49:32,000 --> 00:49:35,360
Well, you have to decide if it's worth it, right? And so let's come back to that.

565
00:49:35,360 --> 00:49:39,920
Okay. But, but if you if you look at this, and again, this is where there's a lot of detail

566
00:49:39,920 --> 00:49:44,800
that goes into each of these things. Equal and C returns a value.

567
00:49:45,520 --> 00:49:52,000
Yep. That's messed up. That allows you to say x equals y equals z, like that works in C.

568
00:49:52,000 --> 00:49:58,160
Yeah. Is it messed up? You know, well, so that most people think it's messed up, I think it is

569
00:49:58,160 --> 00:50:05,360
very by messed up. What I mean is it is very rarely used for good. And it's often used for bugs.

570
00:50:05,360 --> 00:50:11,920
Yeah. Right. And so that's a good definition. You could use, you know, it's a in hindsight,

571
00:50:12,000 --> 00:50:16,000
this was not such a great idea, right? Now, one of the things with Swift that is really powerful

572
00:50:16,000 --> 00:50:20,640
and one of the reasons it's actually good versus it being full of good ideas is that

573
00:50:22,000 --> 00:50:27,040
when we launched Swift 1, we announced that it was public, people could use it, people could

574
00:50:27,040 --> 00:50:32,160
build apps, but it was going to change and break. Okay. When Swift 2 came out, we said,

575
00:50:32,160 --> 00:50:37,200
hey, it's open source, and there's this open process which people can help evolve and direct

576
00:50:37,200 --> 00:50:43,040
the language. So the community at large like Swift users can now help shape the language as it is.

577
00:50:43,040 --> 00:50:48,240
And what happened is that part as part of that process is a lot of really bad mistakes got taken

578
00:50:48,240 --> 00:50:54,240
out. So for example, Swift used to have the C style plus plus and minus minus operators.

579
00:50:54,880 --> 00:51:00,960
Like what does it mean when you put it before versus after? Right? Well, that got cargo-culted

580
00:51:00,960 --> 00:51:05,840
from C into Swift early on. What's cargo-culted? Cargo-culted means brought forward without really

581
00:51:06,800 --> 00:51:10,800
considering it. Okay. This is maybe not the most PC term.

582
00:51:11,360 --> 00:51:16,640
But I have to look it up in Urban Dictionary. Yeah. So it got pulled into C without,

583
00:51:17,360 --> 00:51:22,080
or it got pulled into Swift without very good consideration. And we went through this process

584
00:51:22,080 --> 00:51:26,800
and one of the first things got ripped out was plus plus and minus minus because they lead to

585
00:51:26,800 --> 00:51:32,880
confusion. They have very little value over saying X plus equals one. And X plus equals one is way

586
00:51:32,880 --> 00:51:38,160
more clear. And so when you're optimizing for teachability and clarity and bugs and this

587
00:51:38,160 --> 00:51:43,280
multidimensional space that you're looking at, things like that really matter. And so being

588
00:51:43,280 --> 00:51:47,200
first principles on where you're coming from and what you're trying to achieve and being anchored

589
00:51:47,200 --> 00:51:56,880
on, the objective is really important. Well, let me ask you about the most sort of this podcast

590
00:51:56,880 --> 00:52:02,480
isn't about information, it's about drama. So let me talk to you about some drama. So you mentioned

591
00:52:02,560 --> 00:52:10,880
Pascal and colon equals, there's something that's called the walrus operator. Okay. And Python

592
00:52:12,320 --> 00:52:17,280
Python 3.8 added the walrus operator. And the reason I think it's interesting,

593
00:52:18,960 --> 00:52:23,280
it's not just because of the feature, it does, it's, it has the same kind of expression feature

594
00:52:23,280 --> 00:52:28,560
you can mention to see that it returns the value of the assignment. And maybe you can comment on

595
00:52:28,560 --> 00:52:35,920
that in general. But on the other side of it, it's also the thing that toppled the dictator.

596
00:52:37,840 --> 00:52:43,120
It finally drove Guido to step down from the DFL, the toxicity of the community. So maybe

597
00:52:43,920 --> 00:52:47,920
what do you think about the walrus operator in Python? Is there an equivalent

598
00:52:48,560 --> 00:52:56,480
thing in Swift that really stress tested the community? And then on the flip side,

599
00:52:56,480 --> 00:53:01,040
what do you think about Guido stepping down over it? Yeah, well, like if I look past the details

600
00:53:01,040 --> 00:53:05,120
of the walrus operator, one of the things that makes it most polarizing is that it's syntactic

601
00:53:05,120 --> 00:53:11,200
sugar. Okay, what do you mean by syntactic sugar? It means you can take something that already exists

602
00:53:11,200 --> 00:53:15,520
in language and you can express it in a more concise way. So okay, I'm going to play devil's

603
00:53:15,520 --> 00:53:23,040
advocate. So this is great. Is that objective or subjective statement? Like, can you can you argue

604
00:53:23,040 --> 00:53:30,560
that basically anything is syntactic sugar or not? No, not everything is syntactic sugar. So for

605
00:53:30,560 --> 00:53:39,600
example, the type system, like can you have classes versus versus, like, do you have types or not?

606
00:53:39,600 --> 00:53:45,360
Right. So one type versus many types is not something that affects syntactic sugar. And so

607
00:53:45,360 --> 00:53:49,360
if you say I want to have the ability to define types, I have to have all this like language

608
00:53:49,360 --> 00:53:54,480
mechanics to define classes. And oh, now I have to have inheritance. And I have like, I have all

609
00:53:54,480 --> 00:53:58,800
this stuff that's just making my language more complicated. That's not that's not about sugaring

610
00:53:58,800 --> 00:54:06,400
it. Swift has the sugar. So like Swift has this thing called if let and it has various operators

611
00:54:06,400 --> 00:54:13,280
are used to concisify specific use cases. So the problem with syntactic sugar, when you're talking

612
00:54:13,280 --> 00:54:17,840
about, Hey, I have a thing that takes a lot to write and I have a new way to write it. You have

613
00:54:17,920 --> 00:54:24,080
this like horrible trade off, which becomes almost completely subjective, which is how often does

614
00:54:24,080 --> 00:54:28,400
this happen? And does it matter? And one of the things that is true about human psychology,

615
00:54:28,400 --> 00:54:34,480
particularly when you're talking about introducing a new thing is that people over over estimate the

616
00:54:34,480 --> 00:54:38,720
burden of learning something. And so it looks foreign when you haven't gotten used to it.

617
00:54:38,720 --> 00:54:43,040
But if it was there from the beginning, of course, just part of Python, like unquestionably,

618
00:54:43,040 --> 00:54:47,360
like this is this is just the thing I know. And it's not a new thing that you're worried about

619
00:54:47,360 --> 00:54:53,920
learning is just part of part of the deal. Now with Guido, I don't know Guido well.

620
00:54:55,360 --> 00:54:59,360
Yeah, have you passed cost much? Yeah, I've met him a couple of times, but I don't know Guido

621
00:54:59,360 --> 00:55:05,680
well. But the sense that I got out of that whole dynamic was that he had put the not just the

622
00:55:06,400 --> 00:55:12,240
decision maker weight on his shoulders, but it was so tied to his personal identity that

623
00:55:13,040 --> 00:55:16,960
he took it personally. And he felt the need and he kind of put himself in the situation of being

624
00:55:16,960 --> 00:55:22,720
the person instead of building a base of support around him. I mean, this is probably not quite

625
00:55:22,720 --> 00:55:29,440
literally true. But by too much so there's too much too much concentrated on him, right? And so

626
00:55:29,440 --> 00:55:34,240
and that can wear you down. Well, yeah, particularly because people then say Guido,

627
00:55:34,240 --> 00:55:37,520
you're a horrible person, I hate this thing, blah, blah, blah, blah, blah, blah, blah,

628
00:55:37,520 --> 00:55:42,320
and sure, it's like, you know, maybe 1% of the community that's doing that. But Python's got a

629
00:55:42,320 --> 00:55:48,480
big community in 1% of millions of people is a lot of hate mail. And that just from human factor

630
00:55:48,480 --> 00:55:53,840
will just wear on you. Well, to clarify, it looked from just what I saw in the messaging for the

631
00:55:53,840 --> 00:55:59,200
let's not look at the million Python users, but at the Python core developers, it feels like the

632
00:55:59,200 --> 00:56:06,720
majority, the big majority on a vote were opposed to it. Okay, I'm not that close to it. So this

633
00:56:06,720 --> 00:56:13,040
okay, so the situation is like literally, yeah, I mean, the majority of the core developers are

634
00:56:13,040 --> 00:56:22,080
against it. So I and they weren't, they weren't even like against it. It was there was a few

635
00:56:22,080 --> 00:56:28,160
while they were against it, but the against it wasn't like, this is a bad idea. They were more

636
00:56:28,160 --> 00:56:35,040
like, we don't see why this is a good idea. And what that results in is there's a stalling feeling

637
00:56:35,120 --> 00:56:41,760
like you just slow things down. Now, from my perspective, that you could argue this. And I

638
00:56:41,760 --> 00:56:47,520
think it's a very, it's very interesting if we look at politics today, and the way Congress works,

639
00:56:47,520 --> 00:56:52,960
it's slowed down everything. It's a dampener. Yeah, it's a dampener. But like, that's a dangerous

640
00:56:52,960 --> 00:56:58,880
thing too, because if it dampens things like, you know, well, dampening results, what are you

641
00:56:58,880 --> 00:57:02,880
talking about? Like, it's a low pass filter. But if you need billions of dollars injected into the

642
00:57:02,880 --> 00:57:09,680
economy or trillions of dollars, then suddenly stuff happens. Right. And so for sure. So you're

643
00:57:09,680 --> 00:57:12,560
talking about I'm not I'm not defending our political situation just to be clear.

644
00:57:13,200 --> 00:57:19,760
But you're talking about like a global pandemic. Well, I was hoping we could fix like the healthcare

645
00:57:19,760 --> 00:57:25,280
system and the education. So like, you know, I'm not I'm not a politics person. I don't I don't

646
00:57:25,280 --> 00:57:30,880
I don't know. When it comes to languages, the community is kind of right in terms of it's a

647
00:57:30,880 --> 00:57:34,480
very high burden to add something to a language. So as soon as you add something, you have a

648
00:57:34,480 --> 00:57:39,200
community of people building on it, and you can't remove it. Okay. And if there's a community of

649
00:57:39,200 --> 00:57:44,800
people that feel really uncomfortable with it, then taking it slow, I think is is is an important

650
00:57:44,800 --> 00:57:49,600
thing to do. And there's no rush, particularly with something that's 25 years old and is very

651
00:57:49,600 --> 00:57:55,200
established. And, you know, it's not like coming coming into its own. What about features?

652
00:57:55,760 --> 00:58:00,560
Well, so I think that the issue with with Guido is that maybe this is a case where he

653
00:58:00,560 --> 00:58:06,880
realized it had outgrown him. And it went from being or the language, the language. So Python,

654
00:58:07,760 --> 00:58:13,680
I mean, Guido is amazing. But but Python isn't about Guido anymore. It's about the users. And to

655
00:58:13,680 --> 00:58:19,600
a certain extent, the users own it. And, you know, Python Guido spent years of his life,

656
00:58:19,600 --> 00:58:24,560
a significant fraction of his career on Python. And from his perspective, I imagine he's like,

657
00:58:24,560 --> 00:58:29,120
well, this is my thing, I should be able to do the thing I think is right. But you can also

658
00:58:29,120 --> 00:58:34,480
understand the users where they feel like, you know, this is my thing, I use this like, and

659
00:58:35,920 --> 00:58:41,200
and I don't know, it's a hard, it's a hard thing. But what if we could talk about leadership in this

660
00:58:41,200 --> 00:58:44,640
because it's so interesting to me, I'm going to, I'm going to make, I'm going to work, hopefully

661
00:58:44,640 --> 00:58:50,240
somebody makes it, if not, I'll make it a water supply shirt, because I think it represents to me,

662
00:58:50,240 --> 00:58:56,240
maybe it's my Russian roots or something. You know, it's the burden of leadership, like,

663
00:58:56,960 --> 00:59:05,520
I feel like to push back, I feel like progress can own like most difficult decisions, just like

664
00:59:05,520 --> 00:59:10,960
you said, there'll be a lot of divisiveness over, especially in the passionate community.

665
00:59:12,080 --> 00:59:19,360
It just feels like leaders need to take those risky decisions that that if you like,

666
00:59:19,360 --> 00:59:25,120
listen, that with some non-zero probability, maybe even a high probability will be the wrong

667
00:59:25,120 --> 00:59:28,640
decision. But they have to use their gut and make that decision.

668
00:59:29,680 --> 00:59:35,600
This is like one of the things where you see amazing founders. The founders understand exactly

669
00:59:35,600 --> 00:59:40,080
what's happened and how the company got there and are willing to say, we have been doing

670
00:59:40,080 --> 00:59:46,560
Thing X the last 20 years. But today, we're going to do Thing Y. And they make a major pivot for

671
00:59:46,560 --> 00:59:50,560
the whole company, the company lines up behind them, they move, and it's the right thing. But

672
00:59:50,560 --> 00:59:57,760
then when the founder dies, the successor doesn't always feel that agency to be able to make those

673
00:59:57,760 --> 01:00:02,640
kinds of decisions. Even though they're a CEO, they could theoretically do whatever. There's two

674
01:00:02,640 --> 01:00:09,280
reasons for that, in my opinion, or in many cases, it's always different. But one of which is they

675
01:00:09,280 --> 01:00:13,280
weren't there for all the decisions that were made. And so they don't know the principles

676
01:00:13,280 --> 01:00:19,360
in which those decisions were made. And once the principles change, you should be obligated

677
01:00:19,360 --> 01:00:25,360
to change what you're doing and change direction. And so if you don't know how you got to where

678
01:00:25,360 --> 01:00:30,640
you are, it just seems like gospel. And you're not going to question it. You may not understand

679
01:00:30,640 --> 01:00:33,360
that it really is the right thing to do. So you just may not see it.

680
01:00:33,360 --> 01:00:39,280
That's so brilliant. I never thought of it that way. It's so much higher burden when as a leader

681
01:00:39,280 --> 01:00:41,680
you step into a thing that's already worked for a long time.

682
01:00:41,680 --> 01:00:45,520
Yeah. And if you change it and it doesn't work out, now you're the person who screwed it up.

683
01:00:46,160 --> 01:00:50,960
People always second guess that. And the second thing is that even if you decide to make a change,

684
01:00:50,960 --> 01:00:57,360
even if you're theoretically in charge, you're just a person that thinks they're in charge.

685
01:00:57,360 --> 01:01:00,000
Meanwhile, you have to motivate the troops. You have to explain it to them in terms of

686
01:01:00,000 --> 01:01:02,960
understanding. You have to get them to buy into and believe in it because if they don't,

687
01:01:03,520 --> 01:01:07,680
then they're not going to be able to make the turn even if you tell them their bonuses are

688
01:01:07,680 --> 01:01:12,160
going to be curtailed. They're just not going to buy into it. And so there's only so much power you

689
01:01:12,160 --> 01:01:19,600
have as a leader and you have to understand what those limitations are. You've been a BDFL of some

690
01:01:19,600 --> 01:01:30,640
stuff. You're very heavy on the B, the benevolent dictated for life. I guess LVM. So I still lead

691
01:01:30,720 --> 01:01:38,160
the LVM world. So then on Swift, you said that there's a group of people.

692
01:01:39,040 --> 01:01:44,960
So if you contrast Python with Swift, one of the reasons... So everybody on the core team takes

693
01:01:44,960 --> 01:01:50,160
the role really seriously. And I think we all really care about where Swift goes. But you're

694
01:01:50,160 --> 01:01:55,920
almost delegating the final decision-making to the wisdom of the group. And so it doesn't become

695
01:01:55,920 --> 01:02:01,920
personal. And also when you're talking with the community, so yeah, some people are very annoyed

696
01:02:01,920 --> 01:02:06,960
at certain decisions that get made. There's a certain faith in the process because it's a very

697
01:02:06,960 --> 01:02:12,080
transparent process. And when a decision gets made, a full rationale is provided, things like this.

698
01:02:12,080 --> 01:02:17,440
These are almost defense mechanisms to help both guide future discussions and provide case lock.

699
01:02:17,440 --> 01:02:21,840
And like Supreme Court does about this decision was made for this reason. And here's the rationale

700
01:02:21,840 --> 01:02:27,520
and what we want to see more of or less of. But it's also a way to provide a defense mechanism

701
01:02:27,520 --> 01:02:31,920
so that when somebody's griping about it, they're not saying that person did the wrong thing.

702
01:02:31,920 --> 01:02:38,480
They're saying, well, this thing sucks. And later they move on and they get over it.

703
01:02:38,480 --> 01:02:44,320
Yeah, the analogy is Supreme Court, I think, is really good. But then, okay, not to get

704
01:02:44,320 --> 01:02:51,360
personal on the Swift team, but is there... It just seems like it's impossible for division

705
01:02:51,440 --> 01:02:56,880
not to emerge. Well, each of the humans on the Swift core team, for example, are different

706
01:02:56,880 --> 01:03:01,840
and the membership of the Swift core team changes slowly over time, which is, I think, a healthy

707
01:03:01,840 --> 01:03:06,400
thing. And so each of these different humans have different opinions. Trust me, it's not a

708
01:03:07,360 --> 01:03:12,800
singular consciousness by any stretch of the imagination. You've got three major organizations,

709
01:03:12,800 --> 01:03:18,800
including Apple, Google and Sci-Fi, all working together. And it's a small group of people,

710
01:03:18,800 --> 01:03:23,280
but you need high trust. Again, it comes back to the principles of what you're trying to achieve

711
01:03:23,280 --> 01:03:30,400
and understanding what you're optimizing for. And I think that starting with strong principles

712
01:03:30,400 --> 01:03:36,160
and working towards decisions is always a good way to both make wise decisions in general,

713
01:03:36,160 --> 01:03:41,200
but then be able to communicate them to people so that they can buy into them. And that is hard.

714
01:03:41,200 --> 01:03:48,560
And so you mentioned LVM. LVM is going to be 20 years old this December. So it's showing its own

715
01:03:48,560 --> 01:03:57,200
age. Do you have a dragon cake plan? No, I should definitely do that. If we can have a pandemic

716
01:03:57,200 --> 01:04:07,520
cake, everybody gets a slice of cake and it gets sent through email. But LVM has had tons of its

717
01:04:07,520 --> 01:04:12,720
own challenges over time too. And one of the challenges that the LVM community has, in my

718
01:04:12,720 --> 01:04:18,240
opinion, is that it has a whole bunch of people that have been working on LVM for 10 years.

719
01:04:18,960 --> 01:04:23,680
Because this happens somehow. And LVM has always been one way, but it needs to be a different way.

720
01:04:24,960 --> 01:04:28,720
And they've worked on it for like 10 years. It's a long time to work on something. And

721
01:04:30,080 --> 01:04:34,720
you suddenly can't see the faults in the thing that you're working on. And LVM has lots of problems

722
01:04:34,720 --> 01:04:38,080
and we need to address them and we need to make it better. And if we don't make it better, then

723
01:04:38,080 --> 01:04:42,640
somebody else will come up with a better idea. And so it's just kind of of that age where the

724
01:04:42,720 --> 01:04:50,320
community is in danger of getting too calcified. And so I'm happy to see new projects joining

725
01:04:50,320 --> 01:04:55,280
and new things mixing it up. Fortran is now a new thing in the LVM community, which is hilarious

726
01:04:55,280 --> 01:05:01,360
and good. I've been trying to find, on a little tangent, find people who program in Cobalt or

727
01:05:01,360 --> 01:05:09,680
Fortran, Fortran especially, to talk to. They're hard to find. Yeah, look to the scientific community.

728
01:05:09,680 --> 01:05:13,200
They still use Fortran quite a bit. Well, interesting thing you kind of mentioned

729
01:05:13,200 --> 01:05:19,040
with LVM or just in general, that if something evolves, you're not able to see the faults.

730
01:05:19,600 --> 01:05:25,120
So do you fall in love with the thing over time or do you start hating everything about the thing

731
01:05:25,120 --> 01:05:32,640
over time? Well, so my personal folly is that I see maybe not all, but many of the faults

732
01:05:33,360 --> 01:05:37,440
and they grate on me and I don't have time to go fix them. Yeah. And they get magnified over time.

733
01:05:37,440 --> 01:05:41,200
Well, and they may not get magnified, but they never get fixed. And it's like sand underneath,

734
01:05:41,200 --> 01:05:45,360
you know, it's just like grating against you and it's like sand underneath your fingernails

735
01:05:45,360 --> 01:05:51,440
or something. It's just like, you know, it's there, you can't get rid of it. And so the problem is

736
01:05:51,440 --> 01:05:56,160
that if other people don't see it, right, nobody ever get like, I can't go, I don't have time to

737
01:05:56,160 --> 01:06:01,760
go write the code and fix it anymore. But then people are resistant to change. And so you say,

738
01:06:01,760 --> 01:06:06,160
hey, we should go fix this thing. Like, oh, yeah, that sounds risky. Well, is it the right thing

739
01:06:06,160 --> 01:06:12,480
or not? Are the challenges the group dynamics or is it also just technical? I mean, some of these

740
01:06:12,480 --> 01:06:20,320
features like, yeah, I think as an observer is almost like a fan in the, you know, as a spectator

741
01:06:20,320 --> 01:06:25,600
of the whole thing. I don't often think about, you know, some things might actually be technically

742
01:06:25,600 --> 01:06:30,720
difficult to implement. An example of this is we built this new compiler framework called MLR.

743
01:06:31,200 --> 01:06:36,560
Yes, MLR is this a whole new framework. It's not many people think it's about machine learning.

744
01:06:37,120 --> 01:06:41,600
The ML stands for multi level because compiler people can't name things very well, I guess.

745
01:06:41,600 --> 01:06:49,040
Can we dig into what MLIR is? Yeah, so when you look at compilers compilers have historically

746
01:06:49,040 --> 01:06:56,400
been solutions for a given space. So LLVM is a it's really good for dealing with CPUs,

747
01:06:56,400 --> 01:07:02,880
let's just say, at a high level, you look at Java. Java has a JVM. The JVM is very good for

748
01:07:02,880 --> 01:07:07,360
garbage collected languages that need dynamic compilation, and it's very optimized for specific

749
01:07:07,360 --> 01:07:11,760
space. And so hotspot is one of the compilers that gets used in that space. And that compiler's really

750
01:07:11,760 --> 01:07:16,640
good at that kind of stuff. Usually, when you build these domain specific compilers,

751
01:07:16,640 --> 01:07:20,240
you end up building whole thing from scratch for each domain.

752
01:07:21,040 --> 01:07:26,480
What's a domain? So what's the scope of a domain?

753
01:07:26,480 --> 01:07:30,640
Well, so here I would say like, if you look at Swift, there's several different parts to the

754
01:07:30,640 --> 01:07:36,960
Swift compiler, one of which is covered by the LLVM part of it. There's also a high level

755
01:07:36,960 --> 01:07:42,800
piece that's specific to Swift. And there's a huge amount of redundancy between those two

756
01:07:42,800 --> 01:07:48,320
different infrastructures and a lot of re implemented stuff that is similar but different.

757
01:07:48,320 --> 01:07:54,480
What is LLVM defined? LLVM is effectively an infrastructure. So you can mix and match it in

758
01:07:54,480 --> 01:07:57,760
different ways. It's built out of libraries. You can use it for different things. But it's

759
01:07:57,760 --> 01:08:03,120
really good at CPUs and GPUs. CPUs and like the tip of the iceberg on GPUs. It's not really great

760
01:08:03,120 --> 01:08:11,920
at GPUs. Okay. But it turns out languages that then use it to talk to CPUs. Got it. And so it turns

761
01:08:11,920 --> 01:08:16,080
out there's a lot of hardware out there that is custom accelerators. So machine learning, for example,

762
01:08:16,080 --> 01:08:21,280
there are a lot of matrix multiply accelerators and things like this. There's a whole world of

763
01:08:21,280 --> 01:08:28,400
hardware synthesis. So we're using MLIR to build circuits. Okay. And so you're compiling for a

764
01:08:28,400 --> 01:08:33,840
domain of transistors. And so what MLIR does is it provides a tremendous amount of compiler

765
01:08:33,840 --> 01:08:38,880
infrastructure that allows you to build these domain specific compilers in a much faster way

766
01:08:38,880 --> 01:08:45,200
and have the result be good. If we're thinking about the future, now we're talking about like

767
01:08:45,200 --> 01:08:52,960
ASICs. So anything. Yeah, yeah. So if we project into the future, it's very possible that the number

768
01:08:52,960 --> 01:09:00,880
of these kinds of ASICs, very specific infrastructure thing, the architecture things,

769
01:09:03,120 --> 01:09:10,880
like multiplies exponentially. I hope so. So that's MLIR. So what MLIR does is it allows you to build

770
01:09:10,880 --> 01:09:16,640
these compilers very efficiently. Now, one of the things that coming back to the LVM thing,

771
01:09:16,640 --> 01:09:22,800
and then we'll go to hardware, is LVM is a specific compiler for specific domain.

772
01:09:23,840 --> 01:09:28,320
MLIR is now this very general, very flexible thing that can solve lots of different kinds of

773
01:09:28,320 --> 01:09:35,200
problems. So LVM is a subset of what MLIR does. So MLIR is, I mean, it's an ambitious project then.

774
01:09:35,200 --> 01:09:40,720
Yeah, it's a very ambitious project. Yeah. And so to make it even more confusing, MLIR has joined

775
01:09:40,720 --> 01:09:47,200
the LVM umbrella project. So it's part of the LVM family. But where this comes full circle is now

776
01:09:47,200 --> 01:09:53,440
folks that work on the LVM part, the classic part that's 20 years old, aren't aware of all the cool

777
01:09:53,440 --> 01:09:59,440
new things that have been done and the new thing that MLIR was built by me and many other people

778
01:09:59,520 --> 01:10:04,240
that knew a lot about LVM. And so we fixed a lot of the mistakes that lived in LVM.

779
01:10:04,880 --> 01:10:08,960
I mean, we have this community dynamic where it's like, well, there's this new thing, but it's not

780
01:10:08,960 --> 01:10:13,440
familiar. Nobody knows it. It feels like it's new. And so let's not trust it. And so it's just really

781
01:10:13,440 --> 01:10:18,480
interesting to see the cultural social dynamic that comes out of that. And, you know, I think it's

782
01:10:18,480 --> 01:10:23,120
super healthy because we're seeing the ideas percolate and we're seeing the technology diffusion

783
01:10:23,120 --> 01:10:27,120
happen as people get more comfortable with it. They start to understand things in their own terms.

784
01:10:27,120 --> 01:10:33,520
And this just gets to the it takes a while for ideas to propagate, even though they may be very

785
01:10:33,520 --> 01:10:37,520
different than what people are used to. Maybe let's talk about that a little bit, the world of

786
01:10:37,520 --> 01:10:46,800
Asics. And yeah, well, actually, you're, you're, you have a new role at SciFive. What's that place

787
01:10:46,800 --> 01:10:52,880
about? What is the vision for their vision for, I would say, the future of computer?

788
01:10:52,960 --> 01:10:58,240
Yeah. So I lead the engineering and product teams at SciFive. SciFive is a company who's

789
01:10:58,800 --> 01:11:04,320
was founded with this architecture called Risk Five. Risk Five is a new instruction set.

790
01:11:04,320 --> 01:11:07,440
Instruction sets are the things inside of your computer that tell it how to run things.

791
01:11:08,320 --> 01:11:13,840
X86 from Intel and ARM from the ARM company and things like this or other instruction sets.

792
01:11:13,840 --> 01:11:16,960
I've talked to, sorry to interrupt, I talked to Dave Patterson, who's super excited about

793
01:11:17,040 --> 01:11:22,800
Risk Five. Dave is awesome. He's brilliant. Yeah. The Risk Five is distinguished by not being

794
01:11:22,800 --> 01:11:30,800
proprietary. And so X86 can only be made by Intel and AMD. ARM can only be made by ARM. They

795
01:11:30,800 --> 01:11:34,880
sell licenses to build ARM chips to other companies, things like this. MIPS is another

796
01:11:34,880 --> 01:11:39,520
instruction set that is owned by the MIPS company, now Wave, and then it gets licensed out, things

797
01:11:39,520 --> 01:11:46,000
like that. And so Risk Five is an open standard that anybody can build chips for. And so SciFive

798
01:11:46,000 --> 01:11:51,440
was founded by three of the founders of Risk Five that designed and built it in Berkeley,

799
01:11:51,440 --> 01:11:57,760
working with Dave. And so that was the genesis of the company. SciFive today has some of the

800
01:11:57,760 --> 01:12:01,680
world's best Risk Five cores and we're selling them and that's really great. They're going to

801
01:12:01,680 --> 01:12:06,400
tons of products. It's very exciting. So they're taking this thing that's open source and just

802
01:12:06,400 --> 01:12:11,600
being trying to be or are the best in the world at building these things. Yeah. So here it's

803
01:12:11,600 --> 01:12:17,920
the specifications open source. It's like saying TCPIP is an open standard or C is an open standard.

804
01:12:17,920 --> 01:12:22,800
But then you have to build an implementation of the standard. And so SciFive, on the one hand,

805
01:12:22,800 --> 01:12:28,000
pushes forward and defined and pushes forward the standard. On the other hand, we have implementations

806
01:12:28,000 --> 01:12:33,120
that are best in class for different points in the space, depending on if you want a really tiny

807
01:12:33,120 --> 01:12:38,480
CPU or if you want a really big beefy one that is faster, but it uses more area and things like

808
01:12:38,480 --> 01:12:44,560
this. What about the actual manufacturer? So where does that all fit? I'm going to ask a bunch

809
01:12:44,560 --> 01:12:51,520
of dumb questions. That's okay. This is how we learn, right? And so the way this works is that

810
01:12:51,520 --> 01:12:55,680
there's generally a separation of the people who design the circuits and then the people who

811
01:12:55,680 --> 01:13:02,000
manufacture them. And so you'll hear about fabs like TSMC and Samsung and things like this that

812
01:13:02,000 --> 01:13:07,280
actually produce the chips, but they take a design coming in and that design specifies how

813
01:13:08,800 --> 01:13:18,480
the, you know, you turn code for the chip into little rectangles that then use photo lithography

814
01:13:18,480 --> 01:13:26,160
to make mask sets and then burn transistors onto a chip or onto silicon. And we're talking about

815
01:13:26,160 --> 01:13:30,480
mass manufacturing. Yeah, they're talking about making hundreds of millions of parts and things

816
01:13:30,480 --> 01:13:35,440
like that. And so the fab handles the volume production, things like that. But when you look

817
01:13:35,440 --> 01:13:42,000
at this problem, the interesting thing about the space when you look at it is that these,

818
01:13:42,000 --> 01:13:46,480
the steps that you go from designing a chip and writing the quote unquote code for it and things

819
01:13:46,480 --> 01:13:52,720
like barrel log and languages like that down to what you hand off to the fab is a really well

820
01:13:52,720 --> 01:13:58,720
studied really old problem. Okay. Tons of people have worked on it. Lots of smart people have built

821
01:13:58,720 --> 01:14:04,400
systems and tools. These tools then have generally gone through acquisitions. And so they've ended

822
01:14:04,400 --> 01:14:08,880
up at three different major companies that build and sell these tools. They're called EDA tools

823
01:14:08,880 --> 01:14:13,200
like for electronic design automation. The problem with this is you have huge amounts of

824
01:14:13,200 --> 01:14:20,240
fragmentation. You have loose standards. And the tools don't really work together. So you have

825
01:14:20,240 --> 01:14:25,760
tons of duct tape, and you have tons of lost productivity. Now, these are, these are tools

826
01:14:25,760 --> 01:14:33,040
for design. So the risk five is a instruction. Like what is risk five? Like how deep does it go?

827
01:14:34,000 --> 01:14:38,320
How much does it touch the hardware? How much does it define how much of the hardware is?

828
01:14:38,320 --> 01:14:44,880
Yeah. So risk five is all about given a CPU. So the processor and your computer,

829
01:14:44,880 --> 01:14:49,600
how does the compiler like Swift compiler, the C compiler, things like this, how does it make it

830
01:14:49,600 --> 01:14:56,000
work? So it's what is the assembly code? And so you write risk five assembly instead of XA6 assembly,

831
01:14:56,000 --> 01:15:01,360
for example. But it's a set of instructions as opposed to instructions. What do you say it tells

832
01:15:01,360 --> 01:15:08,080
you how the compiler works? Sorry, it's what the compiler talks to. Okay. And then the tooling you

833
01:15:08,080 --> 01:15:13,760
mentioned that the disparate tools are for what? For when you're building a specific chip. So risk

834
01:15:13,760 --> 01:15:19,760
five in hardware, in hardware. Yeah. So risk five, you can buy risk five core from sci five and say,

835
01:15:19,760 --> 01:15:24,160
Hey, I want to have a certain number of run a certain number of gigahertz. I want it to be this

836
01:15:24,160 --> 01:15:30,400
big. I want to be have these features. I want have like I want floating point or not, for example.

837
01:15:31,760 --> 01:15:35,680
And then what you get is you get a description of a CPU with those characteristics.

838
01:15:36,480 --> 01:15:40,880
Now, if you want to make a chip, you want to build like an iPhone chip or something like that,

839
01:15:40,880 --> 01:15:44,880
right? You have to take both the CPU, but then you have to talk to memory. You have to have

840
01:15:45,440 --> 01:15:52,160
timers, IOs, a GPU, other components. And so you need to pull all those things together into

841
01:15:52,160 --> 01:15:57,520
what's called an ASIC, an application specific in a grade circuit. So custom chip. And then

842
01:15:57,600 --> 01:16:01,920
you take that design, and then you have to transform it into something that the fabs,

843
01:16:01,920 --> 01:16:09,200
like TSMC, for example, know how to take to production. Got it. So but yeah. And so that

844
01:16:09,200 --> 01:16:18,240
process, I will, I can't help but see it as is a big compiler. It's a whole bunch of compilers

845
01:16:18,240 --> 01:16:23,520
written without thinking about it through that lens. Isn't the universe a compiler?

846
01:16:24,240 --> 01:16:28,400
Yeah. Compilers do two things. They represent things and transform them.

847
01:16:28,400 --> 01:16:33,120
Yeah. And so there's a lot of things that end up being compilers. But this is a space where

848
01:16:33,760 --> 01:16:38,160
we're talking about design and usability. And the way you think about things, the way

849
01:16:38,160 --> 01:16:43,440
things compose correctly, it matters a lot. And so Sci-Fi is investing a lot into that space.

850
01:16:43,440 --> 01:16:47,760
And we think that there's a lot of benefit that can be made by allowing people to design

851
01:16:47,760 --> 01:16:55,360
chips faster, get them to market quicker and scale out. Because at the alleged end of Moore's

852
01:16:55,360 --> 01:17:01,920
Law, you've got this problem of you're not getting free performance just by waiting another year

853
01:17:01,920 --> 01:17:06,960
for a faster CPU. And so you have to find performance in other ways. And one of the

854
01:17:06,960 --> 01:17:10,480
ways to do that is with custom accelerators and other things in hardware.

855
01:17:11,360 --> 01:17:21,920
And so we'll talk a little more about ASICs. But do you see that a lot of people,

856
01:17:21,920 --> 01:17:27,760
a lot of companies will try to have a different sets of requirements that this whole process

857
01:17:27,760 --> 01:17:35,120
to go for. So almost different car companies might use different and different PC manufacturers.

858
01:17:35,680 --> 01:17:44,080
Is risk five in this whole process, is it potentially the future of all computing devices?

859
01:17:44,720 --> 01:17:49,520
Yeah, I think that, so if you look at risk five and step back from the silicon side of things,

860
01:17:49,520 --> 01:17:55,360
risk five is an open standard. And one of the things that has happened over the course of decades,

861
01:17:55,360 --> 01:17:59,120
if you look over the long arc of computing, somehow became decades old.

862
01:17:59,120 --> 01:17:59,760
Yeah.

863
01:17:59,760 --> 01:18:04,160
Because you have companies that come and go and you have instruction sets that come and go.

864
01:18:04,720 --> 01:18:09,120
Like one example of this out of many is Sun with Spark.

865
01:18:09,760 --> 01:18:10,240
Yeah.

866
01:18:10,240 --> 01:18:16,320
Sun one way. Spark still lives on it if you just do. But we have HP had this instruction set called

867
01:18:16,320 --> 01:18:23,680
PA risk. So PA risk was its big server business and had tons of customers. They decided to move

868
01:18:23,680 --> 01:18:28,960
to this architecture called itanium from Intel. Yeah, it didn't work out so well.

869
01:18:29,520 --> 01:18:30,000
Yeah.

870
01:18:30,000 --> 01:18:35,520
Right. And so you have this issue of you're making many billion dollar investments on

871
01:18:35,520 --> 01:18:40,800
instruction sets that are owned by a company. And even companies as big as Intel don't always

872
01:18:40,800 --> 01:18:43,520
execute as well as they could. They have their own issues.

873
01:18:44,640 --> 01:18:48,480
HP for example, decided that it wasn't in their best interest to continue investing in the space

874
01:18:48,480 --> 01:18:53,040
because it was very expensive. And so they make technology decisions or they make their

875
01:18:53,040 --> 01:18:56,720
own business decisions. And this means that as a customer, what do you do?

876
01:18:57,680 --> 01:19:01,200
You've sunk all this time, all this engineering, all this software work, all these,

877
01:19:01,200 --> 01:19:03,200
you've built other products around them and now you're stuck.

878
01:19:04,320 --> 01:19:08,960
Right. What risk five does is provide you more optionality in the space because if you buy

879
01:19:09,840 --> 01:19:14,160
an implementation of risk five from sci-fi, and you should, they're the best ones.

880
01:19:16,160 --> 01:19:21,040
But if something that happens to sci-fi in 20 years, right, well, great, you can turn around

881
01:19:21,040 --> 01:19:25,440
and buy risk five core from somebody else. And there's an ecosystem of people that are all making

882
01:19:25,440 --> 01:19:29,920
different risk five cores with different trade-offs, which means that if you have more than one

883
01:19:29,920 --> 01:19:33,840
requirement, if you have a family of products, you can probably find something in the risk five

884
01:19:33,840 --> 01:19:38,960
space that fits your needs. Whereas with, if you're talking about XA6, for example,

885
01:19:40,240 --> 01:19:43,200
Intel's only going to bother to make certain classes of devices.

886
01:19:44,640 --> 01:19:54,640
Right. I see. So maybe a weird question, but like if sci-fi is infinitely successful

887
01:19:54,720 --> 01:20:00,720
in the next 20, 30 years, what does the world look like? So like, how does the world of computing

888
01:20:00,720 --> 01:20:07,040
change? So too much diversity in hardware instruction sets, I think is bad. We have a

889
01:20:07,040 --> 01:20:12,160
lot of people that are using lots of different instruction sets, particularly in the embedded,

890
01:20:12,160 --> 01:20:19,920
the very tiny microcontroller space, the thing in your toaster, that are just weird and different

891
01:20:19,920 --> 01:20:24,320
for historical reasons. And so the compilers and the tool chains and the languages on top of them,

892
01:20:25,120 --> 01:20:31,680
aren't there. And so the developers for that software have to use really weird tools because

893
01:20:31,680 --> 01:20:36,560
the ecosystem that supports is not big enough. So I expect that will change. People will have

894
01:20:36,560 --> 01:20:40,560
better tools and better languages, better features everywhere that then can serve as many

895
01:20:40,560 --> 01:20:47,680
different points in the space. And I think risk five will progressively eat more of the ecosystem

896
01:20:47,680 --> 01:20:52,800
because it can scale up, it can scale down sideways, left, right. It's very flexible and

897
01:20:52,800 --> 01:20:58,080
very well-considered and well-designed instruction set. I think when you look at sci-fi of tackling

898
01:20:58,080 --> 01:21:04,960
silicon and how people build chips, which is a very different space, that's where you say,

899
01:21:04,960 --> 01:21:09,680
I think we'll see a lot more custom chips. And that means that you get much more battery life,

900
01:21:09,680 --> 01:21:18,160
you get better, better tuned solutions for your IoT thingy. You get people that move faster,

901
01:21:18,160 --> 01:21:21,280
you get the ability to have faster time to market, for example.

902
01:21:21,280 --> 01:21:27,200
So how many custom... So first of all, on the IoT side of things, do you see the number of smart

903
01:21:27,920 --> 01:21:38,800
toasters increasing exponentially? And if you do, how much customization per toaster is there?

904
01:21:38,800 --> 01:21:45,120
Do all toasters in the world run the same silicon, the same design? Or is it different companies

905
01:21:45,120 --> 01:21:48,800
have different designs? How much customization is possible here?

906
01:21:49,520 --> 01:21:56,240
Well, a lot of it comes down to cost. And so the way that chips work is you end up paying by the...

907
01:21:56,240 --> 01:22:01,680
One of the factors is the size of the chip. And so what ends up happening just from an

908
01:22:01,680 --> 01:22:07,280
economic perspective is there's only so many chips that get made in any year of a given design.

909
01:22:07,280 --> 01:22:12,080
And so often what customers end up having to do is they end up having to pick up a chip that exists

910
01:22:12,080 --> 01:22:17,280
that was built for somebody else so that they can then ship their product. And the reason for that

911
01:22:17,280 --> 01:22:22,240
is they don't have the volume of the iPhone. They can't afford to build a custom chip. However,

912
01:22:22,240 --> 01:22:28,160
what that means is they're now buying an off-the-shelf chip that isn't a perfect fit for their needs.

913
01:22:28,160 --> 01:22:32,400
And so they're paying a lot of money for it because they're buying silicon that they're not using.

914
01:22:33,360 --> 01:22:38,160
Well, if you now reduce the cost of designing the chip, now you get a lot more chips. And

915
01:22:38,160 --> 01:22:44,160
the more you reduce it, the easier it is to design chips. The more the world keeps evolving,

916
01:22:44,160 --> 01:22:48,720
and we get more AI accelerators, we get more other things, we get more standards to talk to,

917
01:22:48,720 --> 01:22:54,480
we get 6G, right? You get changes in the world that you want to be able to talk to these different

918
01:22:54,480 --> 01:22:59,760
things. There's more diversity in the cross product of features that people want. And that drives

919
01:23:00,640 --> 01:23:05,520
differentiated chips in another direction. And so nobody really knows what the future looks like.

920
01:23:05,520 --> 01:23:08,800
But I think that there's a lot of silicon in the future.

921
01:23:09,600 --> 01:23:15,520
Speaking of the future, you said Moore's Law allegedly is dead. So do you agree with

922
01:23:17,520 --> 01:23:23,840
Dave Patterson and many folks that Moore's Law is dead? Or do you agree with Jim Keller,

923
01:23:26,080 --> 01:23:30,640
who's standing at the helm of the pirate ship, saying it's still alive?

924
01:23:30,640 --> 01:23:37,680
It's still alive. Well, so I agree with what they're saying, and different people are interpreting

925
01:23:37,680 --> 01:23:44,080
the animal's law in different ways. So Jim would say, there's another 1000X left in physics,

926
01:23:44,080 --> 01:23:50,000
and we can continue to squeeze the stone and make it faster and smaller and smaller geometries and

927
01:23:50,000 --> 01:23:57,040
all that kind of stuff. He's right. So Jim is absolutely right that there's a ton of progress

928
01:23:57,040 --> 01:24:03,760
left. And we're not at the limit of physics yet. That's not really what Moore's Law is, though.

929
01:24:04,880 --> 01:24:11,360
If you look at what Moore's Law is, is that it's a very simple evaluation of, okay, well,

930
01:24:11,360 --> 01:24:16,880
you look at the cost per, I think it was cost per area and the most economic point in that space.

931
01:24:16,880 --> 01:24:23,040
And if you go look at the now quite old paper that describes this, Moore's Law has a specific

932
01:24:23,040 --> 01:24:28,160
economic aspect to it. And I think this is something that Dave and others often point out.

933
01:24:28,160 --> 01:24:34,320
And so on a technicality, that's right. I look at it from, so I can acknowledge both of those

934
01:24:34,320 --> 01:24:39,040
viewpoints. They're both right. They're both right. I'll give you a third wrong viewpoint

935
01:24:39,040 --> 01:24:45,040
that may be right in its own way, which is single threaded performance doesn't improve

936
01:24:45,040 --> 01:24:51,120
like it used to. And it used to be back when you got a, you know, a Pentium 66 or something. And

937
01:24:51,200 --> 01:24:57,600
the year before, you had a Pentium 33. And now it's twice as fast. Right? Well, it was twice as

938
01:24:57,600 --> 01:25:03,680
fast at doing exactly the same thing. Okay. Like literally the same program ran twice as fast.

939
01:25:03,680 --> 01:25:08,720
You just wrote a check and waited a year, year and a half. Well, so that's what a lot of people

940
01:25:08,720 --> 01:25:14,160
think about Moore's Law. And I think that is dead. And so what we're seeing instead is we're pushing,

941
01:25:15,120 --> 01:25:18,080
we're pushing people to write software in different ways. And so we're pushing people to

942
01:25:18,080 --> 01:25:24,480
write CUDA so they can get GPU compute and the thousands of cores on GPU. We're talking about

943
01:25:24,480 --> 01:25:29,840
C programmers having to use P threads because they now have, you know, 100 threads or 50 cores

944
01:25:29,840 --> 01:25:33,680
in a machine or something like that. You're now talking about machine learning accelerators that

945
01:25:33,680 --> 01:25:39,040
are now domain specific. And when you look at these kinds of use cases, you can still get

946
01:25:39,040 --> 01:25:45,600
performance. And Jim will come up with cool things that utilize the silicon in new ways for sure.

947
01:25:45,600 --> 01:25:49,600
But you're also going to change the programming model. Right. And now when you start talking

948
01:25:49,600 --> 01:25:53,680
about changing the programming model, that's when you come back to languages and things like this

949
01:25:53,680 --> 01:26:00,240
too. Because often what you see is like you take the C programming language, right? The C programming

950
01:26:00,240 --> 01:26:06,720
language is designed for CPUs. And so if you want to talk to GPU, now you're talking to its cousin,

951
01:26:06,720 --> 01:26:12,720
CUDA. Okay. CUDA is a different thing with a different set of tools, a different world,

952
01:26:12,800 --> 01:26:18,080
a different way of thinking. And we don't have one world that scales. And I think that we can

953
01:26:18,080 --> 01:26:22,800
get there. We can have one world that scales in a much better way. On a small tangent, then I think

954
01:26:22,800 --> 01:26:28,800
most programming languages are designed for CPUs, for a single core, even just in their spirit,

955
01:26:28,800 --> 01:26:33,360
even if they allow for parallelization. So what does it look like for programming language

956
01:26:34,080 --> 01:26:40,640
to have parallelization or massive parallelization as it's like first principle?

957
01:26:41,200 --> 01:26:48,480
So the canonical example of this is the hardware design world. So Verilog, VHDL,

958
01:26:48,480 --> 01:26:54,880
these kinds of languages, they're what's called a high-level synthesis language. This is the thing

959
01:26:54,880 --> 01:27:00,720
people design chips in. And when you're designing a chip, it's kind of like a brain where you have

960
01:27:00,720 --> 01:27:06,480
infinite parallelism. Like you've got, you're like laying down transistors. Transistors are always

961
01:27:06,560 --> 01:27:11,360
running. And so you're not saying run this transistor, then this transistor, then this

962
01:27:11,360 --> 01:27:15,360
transistor. It's like your brain, like your neurons are always just doing something. They're

963
01:27:15,360 --> 01:27:23,440
not clocked. They're just doing their thing. And so when you design a chip or when you design a CPU,

964
01:27:23,440 --> 01:27:26,560
when you design a GPU, when you design when you're laying down the transistors,

965
01:27:27,200 --> 01:27:30,320
similarly, you're talking about, well, okay, well, how do these things communicate?

966
01:27:31,200 --> 01:27:36,000
And so these languages exist. Verilog is a kind of mixed example of that.

967
01:27:36,000 --> 01:27:39,440
Now, these languages are really great. Yeah, very low level. Yeah.

968
01:27:39,440 --> 01:27:44,080
Yeah, they're very low level. And abstraction is necessary here. And there's different approaches

969
01:27:44,080 --> 01:27:50,960
with that. And it's itself a very complicated world. But it's implicitly parallel. And so

970
01:27:51,680 --> 01:27:58,000
having that as the domain that you program towards makes it so that by default, you get

971
01:27:58,000 --> 01:28:03,600
parallel systems. If you look at CUDA, CUDA is a point halfway in the space where in CUDA,

972
01:28:03,600 --> 01:28:08,160
when you write a CUDA kernel for your GPU, it feels like you're writing a scalar program. So

973
01:28:08,160 --> 01:28:11,360
you're like, you have ifs, you have for loops, stuff like this, you're just writing normal,

974
01:28:11,360 --> 01:28:16,320
normal code. But what happens outside of that in your driver is that it actually is running you on

975
01:28:16,320 --> 01:28:21,840
like 1000 things at once. Right. And so it's, it's parallel, but it has pulled it out of the

976
01:28:21,840 --> 01:28:28,800
programming model. And so now you as a programmer are working in a, in a simpler world, and it's

977
01:28:28,800 --> 01:28:33,360
solved that for you. How do you take the language like Swift?

978
01:28:35,360 --> 01:28:40,800
You know, if we think about GPUs, but also ASICs, maybe if we can dance back and forth

979
01:28:40,800 --> 01:28:47,600
between hardware and software, is, you know, how do you design for these features to be able to

980
01:28:47,600 --> 01:28:52,960
program, make it a first class citizen to be able to do like Swift for TensorFlow,

981
01:28:52,960 --> 01:28:58,240
to be able to do machine learning on current hardware, but also future hardware like

982
01:28:58,880 --> 01:29:02,080
TPUs and all kinds of ASICs that I'm sure will be popping up more and more.

983
01:29:02,080 --> 01:29:06,400
Yeah. Well, so, so a lot of this comes down to this whole idea of having the nuts and bolts

984
01:29:06,400 --> 01:29:11,120
underneath the covers that work really well. So you need, if you're talking to TPUs, you need,

985
01:29:11,120 --> 01:29:17,280
you know, MLIR or XLA or one of these compilers that talks to TPUs to build on top of. Okay.

986
01:29:17,280 --> 01:29:21,440
And if you're talking to circuits, you need to figure out how to lay down the transistors and

987
01:29:21,440 --> 01:29:25,440
how to organize it and how to set up clocking and like all the domain problems that you get with

988
01:29:25,520 --> 01:29:30,400
circuits. Then you have to decide how to explain it to a human. What is EY?

989
01:29:31,520 --> 01:29:35,280
Right. And if, if you do it right, that's a library problem, not a language problem.

990
01:29:36,320 --> 01:29:40,160
And that works if you have a library or a language which allows your library

991
01:29:41,200 --> 01:29:45,120
to write things that feel native in the language by implementing libraries,

992
01:29:45,760 --> 01:29:51,040
because then you can innovate in programming models without having to change your syntax again.

993
01:29:51,040 --> 01:29:56,560
And like you have to invent new code formatting tools and like all the other things that languages

994
01:29:56,560 --> 01:30:03,040
come with. And this, this gets really interesting. And so if you look at this space, the interesting

995
01:30:03,040 --> 01:30:08,960
thing once you separate out syntax becomes what is that programming model? And so do you want the

996
01:30:08,960 --> 01:30:16,000
CUDA style? I write one program and it runs many places. The, do you want the implicitly parallel

997
01:30:16,000 --> 01:30:21,440
model? How do you reason about that? How do you give developers, you know, chip architects the,

998
01:30:21,440 --> 01:30:26,720
the ability to express their intent? And that comes into this whole design question of,

999
01:30:26,720 --> 01:30:31,280
how do you detect bugs quickly? So you don't have to tape out a chip to find out it's wrong,

1000
01:30:31,280 --> 01:30:37,040
ideally, right? How do you, and, and, you know, this is a spectrum. How do you make it so that

1001
01:30:37,040 --> 01:30:41,440
people feel productive? So their turnaround time is very quick. All these things are really hard

1002
01:30:41,440 --> 01:30:47,600
problems. And, and this world, I think that not a lot of effort has been put into that design

1003
01:30:47,600 --> 01:30:53,360
problem and thinking about the layering and other pieces. Well, you've, on the topic of concurrency,

1004
01:30:53,360 --> 01:30:58,000
you've written the Swift concurrency manifesto. I think it's, it's kind of interesting. Anything

1005
01:30:58,000 --> 01:31:05,520
that has the word manifesto in is very interesting. Can you summarize the key ideas of each of the

1006
01:31:05,520 --> 01:31:12,240
five parts you've written about? So what is a manifesto? Yes. How about we start there? So in

1007
01:31:12,240 --> 01:31:16,720
the Swift community, we have this problem, which is on the one hand, you want to have

1008
01:31:17,520 --> 01:31:22,400
relatively small proposals that you can kind of fit in your head. You can understand the details

1009
01:31:22,400 --> 01:31:27,440
at a very fine grain level that move the world forward. But then you also have these big arcs.

1010
01:31:28,000 --> 01:31:33,120
Okay. And often when you're working on something that is a big arc, but you're tackling it in

1011
01:31:33,120 --> 01:31:37,920
small pieces, you have this question of, how do I know I'm not doing a random walk? Where are we

1012
01:31:37,920 --> 01:31:43,440
going? Like, how does this add up? Furthermore, when you start that first, the first small step,

1013
01:31:43,440 --> 01:31:47,760
what terminology do you use? How do we think about it? What is better and worse in the space?

1014
01:31:47,760 --> 01:31:51,280
What are the principles? What are we trying to achieve? And so what a manifesto in the Swift

1015
01:31:51,280 --> 01:31:56,480
community does is it starts to say, Hey, well, let's step back from the details of everything.

1016
01:31:56,480 --> 01:32:01,520
And let's paint a broad picture to talk about how, what we're trying to achieve. Let's give

1017
01:32:01,520 --> 01:32:06,560
an example design point. Let's try to paint the big picture so that then we can zero in on the

1018
01:32:06,560 --> 01:32:10,640
individual steps and make sure that we're making good progress. And so the Swift concurrency

1019
01:32:10,640 --> 01:32:16,400
manifesto is something I wrote three years ago, it's been a while, maybe, maybe more, trying to

1020
01:32:16,400 --> 01:32:22,960
do that for, for Swift in concurrency. It starts with some fairly simple things like making the

1021
01:32:22,960 --> 01:32:27,680
observation that when you have multiple different computers or multiple different threads that are

1022
01:32:27,760 --> 01:32:33,840
communicating, it's best for them to be asynchronous. And so you need things to be able to run

1023
01:32:33,840 --> 01:32:38,480
separately and then communicate with each other. And this means a synchrony. And this means that

1024
01:32:38,480 --> 01:32:42,960
you need a way to modeling asynchronous communication. Many languages have features like

1025
01:32:42,960 --> 01:32:48,000
this. Async await is a popular one. And so that's what I think is very likely in Swift.

1026
01:32:49,280 --> 01:32:53,600
But as you start building this tower of abstractions, it's not just about how do you write this,

1027
01:32:53,600 --> 01:32:58,640
you then reach into the, how do you get memory safety? Because you want correctness, you want

1028
01:32:59,200 --> 01:33:04,400
debuggability and sanity for developers. And how do you get that memory safety into,

1029
01:33:05,200 --> 01:33:10,800
into the language? So if you take a language like Go or C or any of these languages, you get what's

1030
01:33:10,800 --> 01:33:15,520
called a race condition when two different threads or Go routines or whatever touch the same point

1031
01:33:15,520 --> 01:33:23,680
in memory, right? This is a huge like maddening problem to debug because it's not reproducible

1032
01:33:23,680 --> 01:33:28,240
generally. And so there's tools, there's a whole ecosystem of solutions that built up around this,

1033
01:33:28,240 --> 01:33:33,280
but it's, it's a huge problem when you're writing concurrent code. And so with Swift, this whole

1034
01:33:33,280 --> 01:33:38,720
value semantics thing is really powerful there because it turns out that math and copies actually

1035
01:33:38,720 --> 01:33:43,520
work even in concurrent worlds. And so you get a lot of safety just out of the box, but there are

1036
01:33:43,520 --> 01:33:48,320
also some hard problems and it talks about some of that. When you start building up to the next

1037
01:33:48,320 --> 01:33:51,840
level up and you start talking beyond memory safety, you have to talk about what is the programmer

1038
01:33:51,840 --> 01:33:56,720
model? How does a human think about this? So a developer that's trying to build a program,

1039
01:33:56,720 --> 01:34:02,640
think about this, and it proposes a really old model with a new spin called actors. Actors are

1040
01:34:02,640 --> 01:34:09,120
about saying, we have islands of single threadedness, logically. So you write something that feels

1041
01:34:09,120 --> 01:34:15,040
like it's one programming, one program running in a unit, and then it communicates asynchronously

1042
01:34:15,040 --> 01:34:21,120
with other other things. And so making that expressive and natural feel good be the first

1043
01:34:21,120 --> 01:34:25,840
thing you reach for and being safe by default is a big part of the design of that proposal.

1044
01:34:26,400 --> 01:34:30,320
When you start going beyond that, now you start to say, cool, well, these things that communicate

1045
01:34:30,320 --> 01:34:34,240
asynchronously, they don't have to share memory. Well, if they don't have to share memory and

1046
01:34:34,240 --> 01:34:37,680
they're sending messages to each other, why do they have to be in the same process?

1047
01:34:39,120 --> 01:34:43,920
These things should be able to be in different processes on your machine. And why just processes?

1048
01:34:43,920 --> 01:34:49,920
Well, why not different machines? And so now you have a very nice gradual transition towards

1049
01:34:49,920 --> 01:34:54,720
distributed programming. And of course, when you start talking about the big future,

1050
01:34:54,720 --> 01:35:02,400
the manifesto doesn't go into it, but accelerators are things you talk to asynchronously by sending

1051
01:35:02,400 --> 01:35:08,400
messages to them. And how do you program those? Well, that gets very interesting. That's not in

1052
01:35:08,400 --> 01:35:16,480
the proposal. And how much do you want to make that explicit, like the control of that whole

1053
01:35:16,480 --> 01:35:21,680
process explicit to the programmer? Yeah, good question. So when you're designing any of these

1054
01:35:21,680 --> 01:35:26,960
kinds of features or language features or even libraries, you have this really hard tradeoff

1055
01:35:26,960 --> 01:35:32,320
you have to make, which is how much is it magic or how much is it in the human's control? How much

1056
01:35:32,400 --> 01:35:38,320
can they predict and control it? What do you do when the default case is the wrong case?

1057
01:35:40,160 --> 01:35:46,720
And so when you're designing a system, I won't name names, but there are systems where

1058
01:35:48,640 --> 01:35:55,520
it's really easy to get started. And then you jump it. So let's pick like logo. So something like this.

1059
01:35:55,520 --> 01:36:00,480
So it's really easy to get started. It's really designed for teaching kids. But as you get into

1060
01:36:00,560 --> 01:36:04,320
it, you hit a ceiling. And then you can't go any higher. And then what do you do? Well, you have

1061
01:36:04,320 --> 01:36:09,040
to go switch to a different world and rewrite all your code. And this logo is a silly example here.

1062
01:36:09,040 --> 01:36:15,840
This exists in many other languages. With Python, you would say like concurrency, right? So Python

1063
01:36:15,840 --> 01:36:21,520
has the global interpreter lock. So threading is challenging in Python. And so if you start writing

1064
01:36:21,520 --> 01:36:26,080
a large scale application in Python, and then suddenly you need concurrency, you're kind of

1065
01:36:26,080 --> 01:36:32,160
stuck with the series of bad trade-offs, right? There's other ways to go where you say like

1066
01:36:32,160 --> 01:36:38,720
foist all the complexity on the user all at once, right? And that's also bad in a different way.

1067
01:36:38,720 --> 01:36:46,000
And so what I prefer is building a simple model that you can explain that then has an escape

1068
01:36:46,000 --> 01:36:52,320
hatch. So you get in, you have guardrails, you memory safety works like this in Swift where

1069
01:36:52,320 --> 01:36:57,040
you can start with, like by default, if you use all the standard things, it's memory safe,

1070
01:36:57,040 --> 01:37:02,240
you're not going to shoot your foot off. But if you want to get a C level pointer to something,

1071
01:37:02,240 --> 01:37:07,440
you can explicitly do that. But by default, there's guardrails.

1072
01:37:07,440 --> 01:37:10,800
There's guardrails. Okay. So, but like, you know,

1073
01:37:12,720 --> 01:37:16,000
whose job is it to figure out which part of the code is paralyzable?

1074
01:37:16,960 --> 01:37:20,000
So in the case of the proposal, it is the human's job.

1075
01:37:20,960 --> 01:37:27,360
So they decide how to architect their application. And then the runtime in the compiler is very

1076
01:37:27,360 --> 01:37:32,880
predictable. And so this, this is in contrast to, like, there's a long body of work,

1077
01:37:32,880 --> 01:37:40,080
including on Fortran, for auto paralyzing compilers. And this is an example of a bad thing in my,

1078
01:37:40,080 --> 01:37:44,880
so as a compiler person, I can rag on compiler people. Often compiler people will say,

1079
01:37:45,680 --> 01:37:48,960
cool, since I can't change the code, I'm going to write my compiler that then takes

1080
01:37:48,960 --> 01:37:54,320
this unmodified code and makes it go way faster on this machine. Okay. Application development.

1081
01:37:54,320 --> 01:37:59,120
And so it does pattern matching, it does like really deep analysis, compiler people are really

1082
01:37:59,120 --> 01:38:03,680
smart. And so they like want to like do something really clever and tricky. And you get like 10x

1083
01:38:03,680 --> 01:38:08,000
speed up by taking like an array of structures and turn it into a structure of arrays or something,

1084
01:38:08,000 --> 01:38:11,520
because it's so much better for memory. Like there's bodies, like tons of tricks.

1085
01:38:12,480 --> 01:38:15,680
They love optimization. Yeah, you love optimization. Everyone loves optimization.

1086
01:38:15,680 --> 01:38:19,920
Everyone loves it. Well, and it's this promise of build with my compiler and your thing goes fast.

1087
01:38:19,920 --> 01:38:23,920
Yeah. Right. But here's the problem. Lex, you write, you write a program,

1088
01:38:24,560 --> 01:38:28,240
you run it with my compiler, it goes fast, you're very happy. Wow, it's so much faster than the

1089
01:38:28,240 --> 01:38:32,640
other compiler. Then you go and you got a feature to your program or you refactor some code. And

1090
01:38:32,640 --> 01:38:37,920
suddenly you got a 10x loss in performance. Well, why? What just happened there? What just happened

1091
01:38:38,000 --> 01:38:42,640
there is you, the heuristic, the pattern matching the compiler or whatever analysis

1092
01:38:42,640 --> 01:38:48,080
it was doing just got defeated because you didn't inline a function or something, right?

1093
01:38:48,080 --> 01:38:50,720
As a user, you don't know, you don't want to know, that was the whole point,

1094
01:38:50,720 --> 01:38:54,240
you don't want to know how the compiler works. You don't want to know how the memory hierarchy

1095
01:38:54,240 --> 01:38:58,000
works. You don't want to know how it got paralyzed across all these things. You wanted

1096
01:38:58,000 --> 01:39:03,440
that abstract away from you. But then the magic is lost as soon as you did something and you fall

1097
01:39:03,440 --> 01:39:08,400
off a performance cliff. And now you're in this funny position where what do I do? I don't change

1098
01:39:08,400 --> 01:39:14,240
my code. I don't fix that bug. It costs 10x performance. Now what do I do? Well, this is the

1099
01:39:14,240 --> 01:39:18,240
problem with unpredictable performance. If you care about performance, predictability is a very

1100
01:39:18,240 --> 01:39:25,440
important thing. And so what the proposal does is it provides architectural patterns for being able

1101
01:39:25,440 --> 01:39:29,920
to lay out your code, gives you full control over that, makes it really simple so you can explain

1102
01:39:30,080 --> 01:39:35,760
it. And then if you want to scale out in different ways, you have full control over that.

1103
01:39:36,400 --> 01:39:41,680
So in your sense, the intuition is for compilers too hard to do automated parallelization.

1104
01:39:43,280 --> 01:39:49,680
Because the compilers do stuff automatically that's incredibly impressive for other things.

1105
01:39:49,680 --> 01:39:50,320
Right.

1106
01:39:50,320 --> 01:39:54,480
But for parallelization, we're not even close to there.

1107
01:39:54,480 --> 01:39:58,240
Well, it depends on the programming model. So there's many different kinds of compilers.

1108
01:39:58,240 --> 01:40:02,560
And so if you talk about like a C compiler or a Swift compiler or something like that where

1109
01:40:02,560 --> 01:40:07,120
you're writing imperative code, parallelizing that and reasoning about all the pointers and

1110
01:40:07,120 --> 01:40:12,560
stuff like that is a very difficult problem. Now, if you switch domains, so there's this

1111
01:40:12,560 --> 01:40:18,560
cool thing called machine learning, right? So machine learning nerds among other endearing

1112
01:40:18,560 --> 01:40:25,280
things like solving cat detectors and other things like that have done this amazing breakthrough

1113
01:40:25,280 --> 01:40:31,040
of producing a programming model, operations that you compose together that has raised the

1114
01:40:31,040 --> 01:40:36,640
levels of abstraction high enough that suddenly you can have auto parallelizing compilers.

1115
01:40:36,640 --> 01:40:42,400
You can write a model using TensorFlow and have it run on 1024 nodes of a TPU.

1116
01:40:43,280 --> 01:40:48,080
Yeah, that's true. I didn't even think about like, you know, because there's so much flexibility

1117
01:40:48,080 --> 01:40:52,960
in the design of architectures that ultimately boil down to a graph that's parallelizable for you,

1118
01:40:52,960 --> 01:40:56,400
parallelized for you. And if you think about it, that's pretty cool.

1119
01:40:57,520 --> 01:41:01,920
And you think about batching, for example, as a way of being able to exploit more parallelism.

1120
01:41:02,480 --> 01:41:06,480
That's a very simple thing that now is very powerful. That didn't come out of the programming

1121
01:41:06,480 --> 01:41:11,360
language, nerds, those people. That came out of people that are just looking to solve a problem

1122
01:41:11,360 --> 01:41:15,920
and use a few GPUs and organically developed by the community of people focusing on machine

1123
01:41:15,920 --> 01:41:22,000
learning as an incredibly powerful abstraction layer that enables the compiler people to go

1124
01:41:22,000 --> 01:41:27,360
and exploit that. And now you can drive supercomputers from Python. That's pretty cool.

1125
01:41:27,360 --> 01:41:32,160
That's amazing. So just to pause on that, because I'm not sufficiently low level,

1126
01:41:32,160 --> 01:41:38,880
I forget to admire the beauty and power of that. But maybe just to linger on it, like what

1127
01:41:39,600 --> 01:41:43,920
does it take to run in your network fast? Like how hard is that compilation?

1128
01:41:43,920 --> 01:41:48,800
It's really hard. So we just skipped, you said like, it's amazing that that's a thing.

1129
01:41:49,520 --> 01:41:51,440
But how hard is that of a thing?

1130
01:41:51,440 --> 01:41:56,080
It's hard. And I would say that not all of the systems are really great,

1131
01:41:57,040 --> 01:42:00,560
including the ones I helped build. So there's a lot of work left to be done there.

1132
01:42:00,560 --> 01:42:04,480
Is it the compiler nerds working on that? Or is it a whole new group of people?

1133
01:42:04,480 --> 01:42:10,320
Well, it's a full stack problem, including compiler people, including APIs, like Keras and

1134
01:42:10,320 --> 01:42:16,560
the module API and PyTorch and JAX. And there's a bunch of people pushing on all the different

1135
01:42:16,560 --> 01:42:21,120
parts of these things. Because when you look at it, it's both, how do I express the computation?

1136
01:42:21,120 --> 01:42:26,160
Do I stack up layers? Well, cool. Setting up a linear sequence of layers is great for the

1137
01:42:26,160 --> 01:42:29,520
simple case. But how do I do the hard case? How do I do reinforcement learning? Well,

1138
01:42:29,520 --> 01:42:34,560
now I need to integrate my application logic in this. Then it's the next level down of,

1139
01:42:34,560 --> 01:42:37,920
how do you represent that for the runtime? How do you get hardware abstraction?

1140
01:42:38,960 --> 01:42:41,760
And then you get to the next level down of saying, forget about abstraction,

1141
01:42:41,760 --> 01:42:47,360
how do I get the peak performance out of my TPU or my iPhone accelerator or whatever,

1142
01:42:47,360 --> 01:42:50,800
right? And all these different things. And so this is a layered problem with a lot of really

1143
01:42:50,800 --> 01:42:56,800
interesting design and work going on in the space. And a lot of really smart people working on it.

1144
01:42:56,800 --> 01:43:01,520
Machine learning is a very well-funded area of investment right now. And so

1145
01:43:01,520 --> 01:43:05,120
there's a lot of progress being made. So how much innovation is there on the

1146
01:43:05,120 --> 01:43:10,960
lower level, so closer to the ASIC? So redesigning the hardware or redesigning

1147
01:43:11,040 --> 01:43:15,600
concurrently compilers with that hardware? If you were to predict the biggest,

1148
01:43:17,600 --> 01:43:23,120
you know, the equivalent of Moore's law improvements in the inference and the training

1149
01:43:23,120 --> 01:43:26,880
of neural networks and just all of that, where is that going to come from, you think?

1150
01:43:26,880 --> 01:43:32,160
Sure. You get scalability, you have different things. And so you get Jim Keller shrinking

1151
01:43:32,160 --> 01:43:37,600
process technology, you get three nanometers instead of five or seven or 10 or 28 or whatever.

1152
01:43:38,320 --> 01:43:43,440
And so that marches forward and that provides improvements. You get architectural level

1153
01:43:43,440 --> 01:43:48,720
performance. And so the, you know, a TPU with a matrix multiply unit in a systolic array is

1154
01:43:48,720 --> 01:43:54,240
much more efficient than having a scaler core doing multiplies and ads and things like that.

1155
01:43:54,240 --> 01:44:01,280
You then get system level improvements. So how you talk to memory, how you talk across a cluster

1156
01:44:01,280 --> 01:44:07,040
of machines, how you scale out, how you have fast interconnects between machines, you then get system

1157
01:44:07,040 --> 01:44:11,440
level programming models. So now that you have all this hardware, how do you utilize it? You then

1158
01:44:11,440 --> 01:44:16,160
have algorithmic breakthroughs where you say, Hey, wow, cool. Instead of training in, you know,

1159
01:44:16,160 --> 01:44:24,720
resident 50 and a week, I'm now training it in, you know, 25 seconds. And it's a combination of,

1160
01:44:24,720 --> 01:44:30,960
you know, new, new optimizers and new, new, new just training regimens and different,

1161
01:44:30,960 --> 01:44:34,880
different approaches to train. And, and all of these things come together to, to push the world

1162
01:44:34,880 --> 01:44:43,280
forward. That was a beautiful exposition. But if you were to force to bet all your money on one of

1163
01:44:43,280 --> 01:44:51,040
these, would you? Why do we have to? Unfortunately, we have people working on all this. It's an

1164
01:44:51,040 --> 01:44:57,040
exciting time, right? So I mean, you know, open the eye, did this little paper showing the algorithmic

1165
01:44:57,040 --> 01:45:02,480
improvement you can get has been, you know, improving exponentially. I haven't quite seen

1166
01:45:02,480 --> 01:45:09,200
the same kind of analysis on other layers of the stack. I'm sure it's also improving significantly.

1167
01:45:09,200 --> 01:45:15,360
I just, it's a, it's a nice intuition builder. I mean, there's a reason why Moore's law,

1168
01:45:16,000 --> 01:45:21,120
that's the beauty of Moore's law is somebody writes a paper that makes a ridiculous prediction.

1169
01:45:22,560 --> 01:45:28,800
And it, you know, becomes reality in a sense. There's, there's something about these narratives

1170
01:45:28,880 --> 01:45:36,560
when you, when Chris Latner on a silly little podcast makes bets, all his money on a particular

1171
01:45:36,560 --> 01:45:41,600
thing, somehow it can have a ripple effect of actually becoming real. That's an interesting

1172
01:45:42,240 --> 01:45:48,560
aspect of it. Cause like, it might have been, you know, we focused with Moore's law, most of the

1173
01:45:48,560 --> 01:45:55,200
computing industry really, really focused on the hardware. I mean, software innovation, I don't

1174
01:45:55,280 --> 01:45:57,200
know how much software innovation there was in terms of

1175
01:45:57,200 --> 01:45:58,800
what Intel give a bill takes away.

1176
01:46:00,160 --> 01:46:03,360
Yeah. I mean, compilers improved significantly also.

1177
01:46:04,000 --> 01:46:08,880
Well, not, not really. So actually, I mean, some, I'm joking about how software's gotten slower

1178
01:46:08,880 --> 01:46:14,240
pretty much as fast as Harvard got better, at least through the nineties. There's another joke,

1179
01:46:14,240 --> 01:46:18,480
another law in compilers, which is called, I think it's called probestine's law, which is

1180
01:46:19,680 --> 01:46:23,520
compilers double the performance of any given code every 18 years.

1181
01:46:26,160 --> 01:46:27,120
So they move slowly.

1182
01:46:27,920 --> 01:46:30,880
Yeah. Well, so well, yeah, it's exponential also.

1183
01:46:30,880 --> 01:46:36,960
Yeah. You're making progress, but there again, it's not about the power of compilers is not

1184
01:46:36,960 --> 01:46:40,640
just about how do you make the same thing go faster? It's how do you unlock the new hardware?

1185
01:46:41,760 --> 01:46:45,600
A new chip came out. How do you utilize it? You say, oh, the programming model, how do we make

1186
01:46:45,600 --> 01:46:53,520
people more productive? How do we, how do we like have better error messages? Even such mundane

1187
01:46:53,520 --> 01:46:59,200
things like how do I generate a very specific error message about your code actually makes people

1188
01:46:59,200 --> 01:47:03,760
happy because then they know how to fix it, right? And it comes back to how do you help people get

1189
01:47:03,760 --> 01:47:10,320
their job done? Yeah. And yeah. And then in this world of exponentially increasing smart toasters,

1190
01:47:10,320 --> 01:47:18,080
how do you expand computing to all these kinds of devices? Do you see this world where just

1191
01:47:18,080 --> 01:47:24,000
everything's a computing surface? You see that possibility? Just everything's a computer?

1192
01:47:24,000 --> 01:47:27,680
Yeah, I don't see any reason that that couldn't be achieved. It turns out that

1193
01:47:28,480 --> 01:47:35,200
sand goes into glass and glass is pretty useful too. And, you know, like, why not?

1194
01:47:35,200 --> 01:47:45,680
Why not? So a very important question then if, if we're living in a simulation and the simulation

1195
01:47:45,680 --> 01:47:50,320
is running a computer, like, what was the architecture of that computer, do you think?

1196
01:47:51,760 --> 01:47:56,080
So you're saying is it a quantum system? Is it? Yeah, like this whole quantum

1197
01:47:56,080 --> 01:48:02,400
discussion, is it needed? Or can we run it on a, you know, with a risk five architecture,

1198
01:48:03,600 --> 01:48:08,400
a bunch of CPUs? I think it comes down to the right tool for the job. Okay. And so.

1199
01:48:08,400 --> 01:48:13,600
And what's the compiler? Yeah, exactly. That's, that's my question. Did I get that job?

1200
01:48:13,680 --> 01:48:20,720
Feed the universe compiler. And so there, as far as we know, quantum, quantum,

1201
01:48:20,720 --> 01:48:27,440
quantum systems are the bottom of the pile of turtles so far. Yeah. And so we don't know

1202
01:48:27,440 --> 01:48:30,880
efficient ways to implement quantum systems without using quantum computers.

1203
01:48:32,080 --> 01:48:35,040
Yeah. And that's totally outside of everything we've talked about quantum.

1204
01:48:35,040 --> 01:48:40,400
But who runs that quantum computer? Yeah. Right. So if we really are living in a simulation,

1205
01:48:41,360 --> 01:48:46,480
then is it bigger quantum computers? Is it different ones? Like, how does that work out?

1206
01:48:46,480 --> 01:48:51,040
How does that scale? Well, it's the same size. It's the same size. But then,

1207
01:48:51,040 --> 01:48:54,720
but then the thought of the simulation is that you don't have to run the whole thing that,

1208
01:48:54,720 --> 01:48:57,440
you know, we humans are cognitively very limited. Do you do check points?

1209
01:48:57,440 --> 01:49:03,920
Do you do check points? Yeah. And, and if we, the point at which we human, so you basically do

1210
01:49:03,920 --> 01:49:12,960
minimal amount of, what is it, the SWIFT does on right, copy on right. So you only,

1211
01:49:12,960 --> 01:49:17,440
yeah, you only adjust the simulation and parallel universe theories. Right. And so,

1212
01:49:17,440 --> 01:49:23,680
and so every time a decision is made, somebody opens the shorting in your box, then there's a fork.

1213
01:49:23,680 --> 01:49:30,640
And then this could happen. And then thank you for considering the possibility. But yeah,

1214
01:49:30,640 --> 01:49:34,880
so it may not require, you know, the entirety of the universe to simulate it. But it's

1215
01:49:36,000 --> 01:49:43,760
interesting to think about as we create this, this higher and higher fidelity systems. But

1216
01:49:43,760 --> 01:49:48,240
I do want to ask on the, on the quantum computer side, because everything we've talked about

1217
01:49:48,240 --> 01:49:52,080
with us, with your work with sci-fi, with everything with compilers,

1218
01:49:52,080 --> 01:49:56,160
none of that includes quantum computers. Right. That's true. So

1219
01:49:57,120 --> 01:50:05,040
if you ever thought about what a, you know, this whole serious engineering work,

1220
01:50:05,040 --> 01:50:10,480
how quantum computers looks like of compilers, of architectures, all of that kind of stuff.

1221
01:50:10,480 --> 01:50:14,080
So I've looked at it a little bit. I'd know almost nothing about it,

1222
01:50:14,080 --> 01:50:18,000
which means that at some point I will have to find an excuse to get involved, because that's

1223
01:50:18,000 --> 01:50:22,400
how it works. But do you think, do you think that's the thing to be, like, is, with your little

1224
01:50:22,480 --> 01:50:26,720
tingly senses of the timing of when to be involved? Is it not yet?

1225
01:50:26,720 --> 01:50:32,800
Well, so the thing I do really well is I jump into messy systems and figure out how to make them

1226
01:50:33,520 --> 01:50:39,040
figure out what the truth in the situation is, try to figure out what the unifying theory is,

1227
01:50:39,040 --> 01:50:42,960
how to, like, factor the complexity, how to find a beautiful answer to a problem that

1228
01:50:43,760 --> 01:50:47,440
has been well studied and lots of people have bashed their heads against it. I don't know that

1229
01:50:47,440 --> 01:50:53,840
quantum computers are mature enough and accessible enough to be figured out yet, right? And

1230
01:50:56,000 --> 01:51:00,160
the, I think the open question with quantum computers is, is there a useful problem that

1231
01:51:00,160 --> 01:51:05,520
gets solved with the quantum computer that makes it worth the economic cost of, like,

1232
01:51:05,520 --> 01:51:12,400
having one of these things and having legions of people that set it up? You go back to the 50s,

1233
01:51:12,480 --> 01:51:16,880
right? And there's the projections of the world will only need seven computers,

1234
01:51:17,520 --> 01:51:21,840
right? Well, and part of that was that people hadn't figured out what they're useful for.

1235
01:51:21,840 --> 01:51:24,880
What are the algorithms we want to run? What are the problems that get solved? And this comes back

1236
01:51:24,880 --> 01:51:29,840
to how do we make the world better, either economically or making somebody's life better,

1237
01:51:29,840 --> 01:51:35,040
or, like, solving a problem that wasn't solved before, things like this. And I think that just

1238
01:51:35,040 --> 01:51:38,320
we're a little bit too early in that development cycle because it's still, like, literally a science

1239
01:51:38,320 --> 01:51:44,400
project, not a negative connotation, right? It's literally a science project. And the progress

1240
01:51:44,400 --> 01:51:50,560
there's amazing. And so I don't know if it's 10 years away, if it's two years away, exactly where

1241
01:51:50,560 --> 01:51:57,360
that breakthrough happens. But you look at machine learning, it, we went through a few winters

1242
01:51:58,320 --> 01:52:04,160
before the AlexNet transition, and then suddenly had its breakout moment. And that was the catalyst

1243
01:52:04,240 --> 01:52:09,840
that then drove the talent flocking into it. That's what drove the economic applications

1244
01:52:09,840 --> 01:52:14,880
of it. That's what drove the technology to go faster because you now have more minds thrown

1245
01:52:14,880 --> 01:52:21,200
at the problem. This is what caused, like, a serious knee and deep learning and the algorithms

1246
01:52:21,200 --> 01:52:26,080
that we're using. And so I think that's what quantum needs to go through. And so right now,

1247
01:52:26,080 --> 01:52:32,400
it's in that formidable finding itself, getting the, like, literally the physics figured out.

1248
01:52:32,960 --> 01:52:37,200
And then it has to figure out the application that makes that useful.

1249
01:52:38,480 --> 01:52:42,720
I'm not skeptical that I think that will happen. I think it's just, you know, 10 years away,

1250
01:52:42,720 --> 01:52:46,960
something like that. I forgot to ask, what programming language do you think the simulation

1251
01:52:46,960 --> 01:52:58,000
is written in? Probably Lisp. So not Swift. Like, if you were to bet, I'll just leave it at that.

1252
01:52:58,000 --> 01:53:02,320
So, I mean, we've mentioned that you worked with all these companies, we've talked about all these

1253
01:53:02,320 --> 01:53:09,920
projects. It's kind of, if we just step back and zoom out about the way you did that work,

1254
01:53:09,920 --> 01:53:16,000
and we look at COVID times, this pandemic we're living through, that may, if I look at the way

1255
01:53:16,000 --> 01:53:21,440
Silicon Valley folks are talking about it, the way MIT is talking about it, this might last for a

1256
01:53:21,440 --> 01:53:31,280
long time, not just the virus, but the remote nature. The economic impact. Yeah, it's going to

1257
01:53:31,280 --> 01:53:41,520
be a mess. Do you think, what's your prediction? I mean, from sci-fi to Google, to just all the

1258
01:53:41,520 --> 01:53:44,880
places you worked in, just Silicon Valley, you're in the middle of it. What do you think is,

1259
01:53:44,880 --> 01:53:49,680
how is this whole place going to change? Yeah, so, I mean, I really can only speak to the tech

1260
01:53:49,680 --> 01:53:56,720
perspective. I am in that bubble. I think it's going to be really interesting because the, you

1261
01:53:56,720 --> 01:54:01,040
know, the zoom culture of being remote and on video chat all the time has really interesting

1262
01:54:01,040 --> 01:54:07,120
effects on people. So on the one hand, it's a great normalizer. It's a normalizer that I think

1263
01:54:07,120 --> 01:54:13,280
will help communities of people that have traditionally been underrepresented. Because

1264
01:54:13,280 --> 01:54:17,360
now you're taking, in some cases, a face off, because you don't have to have a camera going,

1265
01:54:18,320 --> 01:54:22,560
right? And so you can have conversations without physical appearance being part of the dynamic,

1266
01:54:22,560 --> 01:54:27,040
which is pretty powerful. You're taking remote employees that have already been remote and

1267
01:54:27,040 --> 01:54:32,320
you're saying you're now on the same level and footing as everybody else. Nobody gets whiteboards.

1268
01:54:33,280 --> 01:54:36,400
You're not going to be the one person that doesn't get to be participating in the whiteboard

1269
01:54:36,400 --> 01:54:42,160
conversation. And that's pretty powerful. You've got, you're forcing people to think

1270
01:54:43,120 --> 01:54:48,240
asynchronously in some cases, because it's hard to just, just get people physically together and

1271
01:54:48,240 --> 01:54:53,120
the bumping into each other forces people to find new ways to solve those problems. And I think

1272
01:54:53,120 --> 01:54:58,160
that that leads to more inclusive behavior, which is good. On the other hand, it's also, it just

1273
01:54:58,160 --> 01:55:06,400
sucks, right? And so the, the nature, the, the actual communication or it just sucks being

1274
01:55:06,400 --> 01:55:10,720
not in, with people like on a daily basis and collaborating with them.

1275
01:55:11,600 --> 01:55:16,640
Yeah, all of that. I mean, everything, this whole situation is terrible. What I meant primarily

1276
01:55:16,640 --> 01:55:23,600
was the, I think that most humans like working physically with humans. I think this is something

1277
01:55:23,600 --> 01:55:28,800
that not everybody, but many people are programmed to do. And I think that we get something out of

1278
01:55:28,800 --> 01:55:33,280
that that is very hard to express, at least for me. And so maybe this isn't true of everybody. But

1279
01:55:33,520 --> 01:55:38,720
and so the question to me is, you know, when you get through that time of adaptation,

1280
01:55:39,680 --> 01:55:44,560
right, you get out of March and April and you get into December and you get into next March,

1281
01:55:44,560 --> 01:55:49,120
if it's not changed, right? It's already terrifying. Well, you think about that and you

1282
01:55:49,120 --> 01:55:54,160
think about what is the nature of work? And how do, how do we adapt? And humans are very adaptable

1283
01:55:54,160 --> 01:55:59,200
species, right? We can, we can learn things. And when we're forced to, and there's a catalyst to

1284
01:55:59,200 --> 01:56:03,680
make that happen. And so what is it that comes out of this? And are we better or worse off?

1285
01:56:04,240 --> 01:56:08,720
Right? I think that, you know, you look at the Bay Area, housing prices are insane.

1286
01:56:08,720 --> 01:56:13,120
Well, why? Well, there's a high incentive to be physically located, because if you don't have

1287
01:56:13,120 --> 01:56:20,240
proximity, you end up paying for it and commute, right? And there's, there has been huge social

1288
01:56:20,240 --> 01:56:26,080
social pressure in terms of like, you will be there for the meeting, right? Or whatever scenario

1289
01:56:26,080 --> 01:56:30,640
it is. And I think that's gonna be way better. I think it's gonna be much more the norm to have

1290
01:56:30,640 --> 01:56:35,120
remote employees. And I think this is gonna be really great. Do you, do you have friends or do

1291
01:56:35,120 --> 01:56:42,080
you hear of people moving? Yeah, I know one family friend that moved. They moved back to Michigan and

1292
01:56:43,280 --> 01:56:48,640
you know, they were a family with three kids living in a small apartment and like, we're going insane.

1293
01:56:50,400 --> 01:56:55,200
Right? And they're in tech, husband works for Google. So first of all,

1294
01:56:55,200 --> 01:57:00,960
friends of mine have, are in the process of or are have already lost the business. The thing that

1295
01:57:00,960 --> 01:57:06,240
represents their passion, their dream, it could be small entrepreneur projects, but it could be large

1296
01:57:06,240 --> 01:57:10,720
businesses like people that run gyms, like restaurants, like tons of things. Yeah. So,

1297
01:57:10,720 --> 01:57:15,200
but also people like look at themselves in the mirror and ask the question of like,

1298
01:57:16,000 --> 01:57:20,160
what do I want to do in life? For some reason, they don't, they haven't done it until COVID.

1299
01:57:20,160 --> 01:57:26,240
Like, they really ask that question and that results often in moving or leaving the company

1300
01:57:26,240 --> 01:57:31,360
here with starting your business or transitioning to a different company. Do you think we're gonna

1301
01:57:31,360 --> 01:57:38,000
see that a lot? Well, I can't speak to that. I mean, we're definitely gonna see it at a higher

1302
01:57:38,000 --> 01:57:43,600
frequency than we did before. Just because I think what you're trying to say is there are

1303
01:57:43,600 --> 01:57:48,000
decisions that you make yourself and big life decisions that you make yourself. And like,

1304
01:57:48,000 --> 01:57:52,800
I'm gonna like quit my job and start a new thing. There's also decisions that get made for you.

1305
01:57:52,800 --> 01:57:57,600
Like, I got fired from my job. What am I going to do? Right? And that's not a decision that you

1306
01:57:57,600 --> 01:58:03,520
think about, but you're forced to act. Okay. And so I think that those you're forced to act kind

1307
01:58:03,520 --> 01:58:08,320
of moments where like, you know, global pandemic comes and wipes out the economy. And now your

1308
01:58:08,320 --> 01:58:13,440
business doesn't exist. I think that does lead to more reflection, right? Because you're less

1309
01:58:13,440 --> 01:58:19,040
anchored on what you have. And it's not a, what do I have to lose versus what do I have to gain,

1310
01:58:19,040 --> 01:58:25,280
AB comparison? It's more of a fresh slate. Cool. I could do anything now. Do I want to do the same

1311
01:58:25,280 --> 01:58:31,360
thing I was doing? Did that make me happy? Is this now time to go back to college and take a class

1312
01:58:31,360 --> 01:58:37,680
and learn new skill? Is this a time to spend time with family? If you can afford to do that,

1313
01:58:37,680 --> 01:58:42,400
is this time to like, you know, literally move into the parents, right? I mean, all these things

1314
01:58:42,400 --> 01:58:49,120
that were not normative before suddenly become, I think, very, the value system has changed. And

1315
01:58:49,120 --> 01:58:55,600
I think that's actually a good thing in the short term, at least, because it leads to, you know,

1316
01:58:56,480 --> 01:59:01,680
there's kind of been an over optimization along one, one set of priorities for the world. And

1317
01:59:01,680 --> 01:59:06,400
now maybe we'll get to a more balanced and more interesting world where people are doing different

1318
01:59:06,400 --> 01:59:09,520
things. I think it could be good. I think there could be more innovation that comes out of it,

1319
01:59:09,520 --> 01:59:14,240
for example. What do you think about all the social chaos in the middle of? It sucks.

1320
01:59:17,360 --> 01:59:19,840
Let me ask you, I hope, do you think it's all going to be okay?

1321
01:59:20,960 --> 01:59:26,480
Well, I think humanity will survive. From the next extension, we're not all going to kill,

1322
01:59:26,480 --> 01:59:30,720
yeah, well. Yeah, I don't think the virus is going to kill all the humans. I don't think all the

1323
01:59:30,720 --> 01:59:35,120
humans are going to kill all the humans. I think that's unlikely. But I look at it as

1324
01:59:35,360 --> 01:59:45,840
progress requires a catalyst, right? So you need a reason for people to be willing to do

1325
01:59:45,840 --> 01:59:51,680
things that are uncomfortable. I think that the US, at least, but I think the world in general,

1326
01:59:51,680 --> 01:59:58,160
is a pretty unoptimal place to live in for a lot of people. And I think that what we're

1327
01:59:58,160 --> 02:00:03,440
seeing right now is we're seeing a lot of unhappiness. And because of all the pressure,

1328
02:00:03,520 --> 02:00:07,040
because of all the badness in the world that's coming together, it's really kind of igniting

1329
02:00:07,040 --> 02:00:10,560
some of that debate that should have happened a long time ago, right? I mean, I think that

1330
02:00:10,560 --> 02:00:14,400
we'll see more progress. You're asking about offline, you're asking about politics and wouldn't

1331
02:00:14,400 --> 02:00:18,080
be great if politics moved faster because there's all these problems in the world and we can move it.

1332
02:00:18,080 --> 02:00:24,880
Well, people are intentionally conservative. And so if you're talking about conservative people,

1333
02:00:24,880 --> 02:00:28,560
particularly if they have heavy burdens on their shoulders because they represent

1334
02:00:28,560 --> 02:00:33,840
literally thousands of people, it makes sense to be conservative. But on the other hand,

1335
02:00:33,840 --> 02:00:39,440
when you need change, how do you get it? The global pandemic will probably lead to some change.

1336
02:00:40,480 --> 02:00:46,880
And it's not a directed plan, but I think that it leads to people asking really interesting

1337
02:00:46,880 --> 02:00:49,440
questions. And some of those questions should have been asked a long time ago.

1338
02:00:49,440 --> 02:00:54,720
Well, let me know if you've observed this as well. Something has bothered me

1339
02:00:54,720 --> 02:00:59,040
in the machine learning community. I'm guessing it might be prevalent in other places

1340
02:00:59,600 --> 02:01:06,240
is something that feels like in 2020, increased level of toxicity. Like people are just

1341
02:01:07,600 --> 02:01:18,320
quicker to pile on, they're just harsh on each other to mob, pick a person that screwed up

1342
02:01:19,200 --> 02:01:24,400
and make it a big thing. And is there something that we can...

1343
02:01:26,320 --> 02:01:30,080
Have you observed that in other places? Is there some way out of this?

1344
02:01:30,080 --> 02:01:33,760
I think there's an inherent thing in humanity that's kind of an us versus them thing,

1345
02:01:34,400 --> 02:01:38,480
which is that you want to succeed. And how do you succeed? Well, it's relative to somebody else.

1346
02:01:39,520 --> 02:01:46,960
And so what's happening, at least in some part, is that with the internet and with online communication,

1347
02:01:46,960 --> 02:01:55,200
the world's getting smaller. And so we're having some of the social ties of my town versus your

1348
02:01:55,200 --> 02:02:04,800
town's football team turn into much larger and yet shallower problems. And people don't have time,

1349
02:02:04,800 --> 02:02:10,880
the incentives, clickbait and all these things really feed into this machine. And I don't know

1350
02:02:10,960 --> 02:02:17,040
where that goes. Yeah, I mean, the reason I think about that, I mentioned to you this offline a

1351
02:02:17,040 --> 02:02:25,040
little bit, but I have a few difficult conversations scheduled, some of them political-related,

1352
02:02:25,040 --> 02:02:30,480
some of them within the community, difficult personalities that went through some stuff.

1353
02:02:30,480 --> 02:02:35,760
I mean, one of them I've talked before, I will talk again, is Yann LeCun. He got a little crap on

1354
02:02:35,760 --> 02:02:43,840
Twitter for talking about a particular paper and the bias within a data set. And then there's been

1355
02:02:43,840 --> 02:02:52,720
a huge in my view, and I'm only comfortable saying it, irrational, over exaggerated pile on

1356
02:02:53,360 --> 02:02:59,280
on his comments, because he made pretty basic comments about the fact that if there's bias in

1357
02:02:59,280 --> 02:03:05,280
the data, there's going to be bias in the results. So we should not have bias in the data. But people

1358
02:03:05,360 --> 02:03:11,760
piled on to him because he said he trivialized the problem of bias. It's a lot more than just

1359
02:03:11,760 --> 02:03:19,920
bias in the data. But yes, that's a very good point. That's not what he was saying. And the

1360
02:03:19,920 --> 02:03:29,520
response, the implied response that he's basically sexist and racist is something that completely

1361
02:03:29,600 --> 02:03:35,600
drives away the possibility of a nuanced discussion. One nice thing about a pocket-long form

1362
02:03:37,200 --> 02:03:44,480
conversation is you can talk it out, you can lay your reasoning out, and even if you're wrong,

1363
02:03:44,480 --> 02:03:47,840
you can still show that you're a good human being underneath it.

1364
02:03:48,400 --> 02:03:52,400
Your point about you can't have a productive discussion, well, how do you get to that point

1365
02:03:52,400 --> 02:03:57,360
where people can turn, they can learn, they can listen, they can think, they can engage versus

1366
02:03:57,440 --> 02:04:03,200
just being a shallow like, like, and then keep moving, right? And I don't think that

1367
02:04:04,640 --> 02:04:10,080
progress really comes from that, right? And I don't think that one should expect that. I think

1368
02:04:10,080 --> 02:04:15,280
that you'd see that as reinforcing individual circles and the us versus them thing. And I

1369
02:04:15,280 --> 02:04:22,880
think that's fairly divisive. Yeah, I think there's a big role in, like, the people that bother me

1370
02:04:22,880 --> 02:04:28,880
most on Twitter when I observe things. It's not the people who get very emotional, angry,

1371
02:04:28,880 --> 02:04:36,640
like, over the top. It's the people who, like, prop them up. It's all the, it's that I think what

1372
02:04:36,640 --> 02:04:43,760
should be the, we should teach each other is to be sort of empathetic. The thing that it's really

1373
02:04:43,760 --> 02:04:48,320
easy to forget, particularly on, like, Twitter or the internet or the email, is that sometimes

1374
02:04:48,320 --> 02:04:54,160
people just have a bad day, right? You have a bad day, or you're like, I've been in this situation

1375
02:04:54,160 --> 02:04:58,160
where it's like, between meetings, like, fire off a quick response in emails, because I want to, like,

1376
02:04:58,160 --> 02:05:05,600
help get something unblocked, phrase it really objectively wrong. I screwed up. And suddenly,

1377
02:05:05,600 --> 02:05:10,880
this is now something that sticks with people. And it's not because they're bad. It's not because

1378
02:05:10,880 --> 02:05:16,480
you're bad, just psychology of like, you said a thing, it sticks with you, you didn't mean it

1379
02:05:16,480 --> 02:05:21,520
that way. But it really impacted somebody because the way they interpreted it, and this is just an

1380
02:05:21,520 --> 02:05:26,720
aspect of working together as humans. And I have a lot of optimism in the long term, the very long

1381
02:05:26,720 --> 02:05:30,720
term, about what we as humanity can do. But I think that's going to be, it's just always a rough

1382
02:05:30,720 --> 02:05:36,240
ride. And you came into this by saying, like, what is COVID and all the social strife that's

1383
02:05:36,240 --> 02:05:41,200
happening right now mean? And I think that it's really bad in the short term, but I think it

1384
02:05:41,200 --> 02:05:47,360
will lead to progress. And for that, I'm very thankful. Yeah, it's painful in the short term,

1385
02:05:47,360 --> 02:05:51,280
though. Well, yeah, I mean, people are out of jobs, like, some people can't eat, like, it's

1386
02:05:51,280 --> 02:05:58,800
horrible. And, but, but, you know, it's progress. So we'll see, we'll see what happens. I mean,

1387
02:05:58,800 --> 02:06:04,320
the real question is when you look back 10 years, 20 years, 100 years from now, how do we evaluate

1388
02:06:04,320 --> 02:06:09,680
the decisions that are being made right now? I think that's really the way you can frame that

1389
02:06:09,680 --> 02:06:14,160
and look at it. And you say, you know, you integrate across all the short term horribleness

1390
02:06:14,160 --> 02:06:19,520
that's happening. And you look at what that means. And is the, you know, improvement across the world

1391
02:06:19,520 --> 02:06:24,800
or the regression across the world significant enough to make it a good or bad thing? I think

1392
02:06:24,800 --> 02:06:31,280
that's the question. Yeah. And for that's good to study history. I mean, one of the big problems

1393
02:06:31,280 --> 02:06:37,680
for me right now is I'm reading the rise and fall of the third Reich. Light reading. So it's

1394
02:06:37,680 --> 02:06:43,440
everything is just, I just see parallels and it means it's, it's, you have to be really careful

1395
02:06:43,440 --> 02:06:50,160
not to overstep it, but just the, the thing that worries me the most is the pain that people feel

1396
02:06:50,720 --> 02:06:56,880
when, when a few things combined, which is like economic depression, which is quite possible

1397
02:06:56,880 --> 02:07:03,840
in this country. And then just being disrespected by in some kind of way, which the German people

1398
02:07:03,920 --> 02:07:10,880
were really disrespected by most of the world, like in a way that's over the top, that something

1399
02:07:10,880 --> 02:07:17,760
can, it can build up. And then all you need is a charismatic leader just to go either positive

1400
02:07:17,760 --> 02:07:23,280
or negative in both work, as long as they're charismatic. And it's taking advantage of,

1401
02:07:23,280 --> 02:07:28,400
again, that, that inflection point that the world's in and what they do with it could be good or bad.

1402
02:07:28,960 --> 02:07:35,680
And so it's a good way to think about times now, like on an individual level, what we decide to do

1403
02:07:35,680 --> 02:07:40,880
is when, when history is written, you know, 30 years from now, what happened in 2020,

1404
02:07:40,880 --> 02:07:43,840
probably history is going to remember 2020. Yeah, I think so.

1405
02:07:45,440 --> 02:07:49,440
Either for good or bad. And it's like up to us to write it. So it's good.

1406
02:07:49,440 --> 02:07:54,960
Well, one of the things I've observed that I find fascinating is most people act as though the world

1407
02:07:54,960 --> 02:08:02,080
doesn't change. You make decision knowingly, right, you make a decision where you're predicting

1408
02:08:02,080 --> 02:08:05,840
the future based on what you've seen in the recent past. And so if something's always been

1409
02:08:05,840 --> 02:08:09,360
had, it's rained every single day, then of course, you expect it to rain today too, right?

1410
02:08:09,920 --> 02:08:16,640
On the other hand, the world changes all the time. Yeah, constantly, like for better and for worse.

1411
02:08:16,640 --> 02:08:20,160
And so the question is, if you're interested in something that's not right,

1412
02:08:20,800 --> 02:08:24,320
what is the inflection point that led to a change? And you can look to history for this,

1413
02:08:24,320 --> 02:08:29,280
like what is, what is the catalyst that led to that, that explosion that led to that bill that

1414
02:08:29,280 --> 02:08:35,120
led to the, like you can kind of work your way backwards from that. And maybe if you pull together

1415
02:08:35,120 --> 02:08:38,240
the right people and you get the right ideas together, you can actually start driving that

1416
02:08:38,240 --> 02:08:43,040
change and doing it in a way that's productive and hurts fewer people. Yeah, like a single person,

1417
02:08:43,040 --> 02:08:46,960
single event can turn all. Yeah, absolutely. Everything starts somewhere. And often it's

1418
02:08:46,960 --> 02:08:52,480
a combination of multiple factors. But, but yeah, this is these, these things can be engineered.

1419
02:08:52,480 --> 02:08:57,840
That's actually the optimistic view that I'm a long-term optimist on pretty much everything and

1420
02:08:57,840 --> 02:09:02,160
human nature, you know, we can look to all the negative things that the humanity has,

1421
02:09:02,160 --> 02:09:08,640
all the pettiness and all the like self self-servingness and the just the the cruelty,

1422
02:09:09,360 --> 02:09:14,400
right? The biases, the just humans can be very horrible. But on the other hand,

1423
02:09:14,400 --> 02:09:22,240
we're capable of amazing things. And, and the progress across, you know, 100 year chunks is

1424
02:09:22,320 --> 02:09:27,680
striking. And even across decades, it's, we've come a long ways and there's so long ways to go,

1425
02:09:27,680 --> 02:09:32,960
but that doesn't mean that we've stopped. Yeah, the kind of stuff we've done in the last 100 years

1426
02:09:32,960 --> 02:09:37,760
is, is unbelievable. It's kind of scary to think what's going to happen this 100 years. It's scary,

1427
02:09:37,760 --> 02:09:43,280
like exciting, like scary in a sense that it's kind of sad that the kind of technology is going to

1428
02:09:43,280 --> 02:09:48,400
come out in 10, 20, 30 years will probably too old to really appreciate because you don't grow up

1429
02:09:48,400 --> 02:09:54,160
with it. It'll be like kids these days with their virtual reality and their talks and stuff like

1430
02:09:54,160 --> 02:09:58,800
this. Like, how does this thing and like, come on, give me my, you know, static photo.

1431
02:10:00,800 --> 02:10:07,120
My Commodore 64. Yeah, exactly. Okay. Sorry, we kind of skipped over. Let me ask on,

1432
02:10:08,960 --> 02:10:15,680
you know, the machine learning world has been kind of inspired, their imagination captivated

1433
02:10:15,680 --> 02:10:22,320
with GPT-3 and these language models. I thought it'd be cool to get your opinion on it. What's

1434
02:10:22,320 --> 02:10:31,040
your thoughts on this exciting world of, it connects to computation actually is of language

1435
02:10:31,040 --> 02:10:38,080
models that are huge and take multiple, many, many computers, not just to train, but to also

1436
02:10:38,080 --> 02:10:43,680
do inference on. Sure. Well, I mean, it depends on what you're speaking to there. But I mean,

1437
02:10:43,680 --> 02:10:49,280
I think that there's been a pretty well understood maximum deep learning that if you make the model

1438
02:10:49,280 --> 02:10:53,040
bigger and you shove more data into it, assuming you train it right and you have a good model

1439
02:10:53,040 --> 02:10:59,040
architecture, that you'll get a better model out. And so on one hand, GPT-3 was not that surprising.

1440
02:10:59,680 --> 02:11:03,200
On the other hand, a tremendous amount of engineering went into making it possible.

1441
02:11:04,800 --> 02:11:09,600
The implications of it are pretty huge. I think that when GPT-2 came out, there was a very

1442
02:11:09,600 --> 02:11:14,080
provocative blog post from OpenAI talking about, we're not going to release it because of the

1443
02:11:14,080 --> 02:11:20,480
social damage it could cause if it's misused. I think that's still a concern. I think that we

1444
02:11:20,480 --> 02:11:26,400
need to look at how technology is applied and well-meaning tools can be applied in very horrible

1445
02:11:26,400 --> 02:11:32,960
ways and they can have very profound impact on that. I think that GPT-3 is a huge technical

1446
02:11:32,960 --> 02:11:37,520
achievement. And what will GPT-4 be? Will it probably be bigger, more expensive to train?

1447
02:11:38,560 --> 02:11:41,120
Really cool architectural tricks.

1448
02:11:42,240 --> 02:11:47,680
What do you think? I don't know how much thought you've done on distributed computing.

1449
02:11:48,640 --> 02:11:54,560
Is there some technical challenges that are interesting that you're hopeful about exploring

1450
02:11:54,560 --> 02:12:08,240
in terms of a system like a piece of code that GPT-4 might have hundreds of trillions

1451
02:12:08,240 --> 02:12:13,840
of parameters which have to run on thousands of computers? Is there some hope that we can

1452
02:12:13,840 --> 02:12:22,320
make that happen? Today, you can write a check and get access to a thousand TPU cores and do

1453
02:12:22,320 --> 02:12:26,400
really interesting large-scale training and inference and things like that in Google Cloud,

1454
02:12:26,400 --> 02:12:32,320
for example. I don't think it's a question about scales, it's a question about utility.

1455
02:12:33,120 --> 02:12:38,640
And when I look at the Transformer series of architectures that the GPT series is based on,

1456
02:12:38,640 --> 02:12:42,960
it's really interesting to look at that because they're actually very simple designs. They're

1457
02:12:42,960 --> 02:12:48,720
not recurrent. The training regimens are pretty simple and so they don't really reflect like

1458
02:12:48,720 --> 02:12:55,760
human brains. But they're really good at learning language models and they're unrolled enough that

1459
02:12:56,400 --> 02:13:03,440
you can simulate some recurrence. And so the question I think about is, where does this take us?

1460
02:13:03,440 --> 02:13:08,400
We can just keep scaling it, have more parameters, more data, more things, we'll get a better result

1461
02:13:08,400 --> 02:13:13,840
for sure. But are there architectural techniques that can lead to progressive faster pace?

1462
02:13:14,800 --> 02:13:20,000
Right, this is when, you know, how do you get, instead of just like making it a constant time

1463
02:13:20,000 --> 02:13:24,400
bigger, how do you get like an algorithmic improvement out of this, right? And whether it

1464
02:13:24,400 --> 02:13:31,280
be a new training regimen, if it becomes sparse networks, for example, human brain sparse,

1465
02:13:31,280 --> 02:13:36,720
all these networks are dense. The connectivity patterns can be very different. I think this

1466
02:13:36,720 --> 02:13:41,360
is where I get very interested and I'm way out of my league on the deep learning side of this.

1467
02:13:41,440 --> 02:13:46,320
But I think that could lead to big breakthroughs. When we talk about large scale networks, one of

1468
02:13:46,320 --> 02:13:52,160
the things that Jeff Dean likes to talk about and he's given a few talks on is this idea of having

1469
02:13:52,160 --> 02:13:58,560
a sparsely gated mixture of experts kind of a model where you have, you know, different nets

1470
02:13:58,560 --> 02:14:03,840
that are trained and are really good at certain kinds of tasks. And so you have this distributor

1471
02:14:03,840 --> 02:14:07,520
across a cluster. And so you have a lot of different computers that end up being kind of

1472
02:14:07,520 --> 02:14:12,720
locally specialized in different demands. And then when a query comes in, you gate it and use

1473
02:14:12,720 --> 02:14:17,440
learn techniques to route to different parts of the network. And then you utilize the compute

1474
02:14:17,440 --> 02:14:21,840
resources of the entire cluster by having specialization within it. And I don't know

1475
02:14:22,640 --> 02:14:26,880
where that goes or if it starts when it starts to work. But I think things like that could be

1476
02:14:26,880 --> 02:14:34,160
really interesting as well. And on the data side too, if you can think of data selection as a

1477
02:14:34,160 --> 02:14:39,840
kind of programming. Yeah, I mean, at the center, if you look at like Karpathi talked about software

1478
02:14:39,840 --> 02:14:46,880
2.0, I mean, in a sense, data is the programming. Yeah, yeah. So I just, so let me try to summarize

1479
02:14:46,880 --> 02:14:53,520
Andre's position really quick before I disagree with it. Yeah. So Andre Karpathi is amazing. So

1480
02:14:53,520 --> 02:14:59,200
this is nothing personal with him. He's an amazing engineer and also a good blog post writer.

1481
02:14:59,200 --> 02:15:03,200
Yeah, well, he's a great communicator. I mean, he's just an amazing person. He's also really

1482
02:15:03,280 --> 02:15:10,640
sweet. So his basic premise is that software is suboptimal. I think we can all agree to that.

1483
02:15:11,840 --> 02:15:16,480
He also points out that deep learning and other learning based techniques are really great because

1484
02:15:16,480 --> 02:15:22,720
you can solve problems in more structured ways with less like ad hoc code that people write

1485
02:15:22,720 --> 02:15:26,240
out and don't write test cases for in some cases. And so they don't even know if it works in the

1486
02:15:26,240 --> 02:15:34,240
first place. And so if you start replacing systems of imperative code with deep learning models,

1487
02:15:34,240 --> 02:15:41,280
then you get better, a better result. Okay. And I think that he argues that software 2.0 is a

1488
02:15:41,280 --> 02:15:47,120
pervasively learned set of models. And you get away from writing code. And he's given talks where

1489
02:15:47,120 --> 02:15:51,680
he talks about, you know, swapping over more and more and more parts of the code to being learned

1490
02:15:51,680 --> 02:15:58,720
and driven that way. I think that works. And if you're pretty predisposed to liking machine

1491
02:15:58,720 --> 02:16:03,280
learning, then I think that that's definitely a good thing. I think this is also good for

1492
02:16:03,280 --> 02:16:07,600
accessibility in many ways because certain people are not going to write C code or something.

1493
02:16:07,600 --> 02:16:12,560
And so having a data driven approach to do this kind of stuff, I think can be very valuable.

1494
02:16:12,560 --> 02:16:16,480
On the other hand, they're huge trade offs. And it's not clear to me that software 2.0 is

1495
02:16:17,040 --> 02:16:22,320
the answer. And probably Andre wouldn't argue that it's the answer for every problem either.

1496
02:16:22,960 --> 02:16:29,440
But I look at machine learning as not a replacement for software 1.0. I look at it as a new programming

1497
02:16:29,440 --> 02:16:36,560
paradigm. And so programming paradigms, when you look across, across demands is the structured

1498
02:16:36,560 --> 02:16:42,320
programming where you go from go tos to if then else, or functional programming from Lisp. And

1499
02:16:42,320 --> 02:16:46,320
you start talking about higher order functions and values and things like this, or you talk about

1500
02:16:46,320 --> 02:16:50,320
object oriented programming, you're talking about encapsulation, subclassing inheritance,

1501
02:16:50,320 --> 02:16:54,640
you start talking about generic programming, where you start talking about code reuse through

1502
02:16:56,560 --> 02:17:01,120
specialization and different type instantiations. When you start talking about differentiable

1503
02:17:01,120 --> 02:17:05,840
programming, something that I am very excited about in the context of machine learning,

1504
02:17:05,840 --> 02:17:11,040
talking about taking functions and generating variants like the derivative of another function.

1505
02:17:11,040 --> 02:17:16,080
Like that's a programming paradigm that's very useful for solving certain classes of problems.

1506
02:17:16,080 --> 02:17:19,680
Machine learning is amazing at solving certain classes of problems. Like you're not going to

1507
02:17:19,680 --> 02:17:27,280
write a cat detector, or even a language translation system by writing C code. That's not a very

1508
02:17:27,280 --> 02:17:31,920
productive way to do things anymore. And so machine learning is absolutely the right way to do that.

1509
02:17:31,920 --> 02:17:36,640
In fact, I would say that learned models are really one of the best ways to work with the human

1510
02:17:36,640 --> 02:17:41,280
world in general. And so anytime you're talking about sensory input of different modalities,

1511
02:17:41,280 --> 02:17:45,680
anytime that you're talking about generating things in a way that makes sense to a human,

1512
02:17:45,680 --> 02:17:50,560
I think that learned models are really, really useful. And that's because humans are very difficult

1513
02:17:50,560 --> 02:17:56,960
to characterize. And so this is a very powerful paradigm for solving classes of problems.

1514
02:17:56,960 --> 02:18:01,680
But on the other hand, imperative code is too, you're not going to write a bootloader for your

1515
02:18:01,680 --> 02:18:06,960
computer with a deep learning model. Deep learning models are very hardware intensive,

1516
02:18:06,960 --> 02:18:13,280
they're very energy intensive, because you have a lot of parameters. And you can provably

1517
02:18:13,360 --> 02:18:18,320
implement any function with a learned model, like this has been shown. But that doesn't make it

1518
02:18:18,320 --> 02:18:23,600
efficient. And so if you're talking about caring about a few orders and magnitudes worth of energy

1519
02:18:23,600 --> 02:18:26,800
usage, then it's useful to have other tools in the toolbox.

1520
02:18:26,800 --> 02:18:32,400
There's also robustness too. I mean, exactly, all the problems of dealing with data and bias in data,

1521
02:18:32,400 --> 02:18:37,040
all the problems of, you know, software 2.0. And one of the great things that Andre is

1522
02:18:37,920 --> 02:18:43,680
arguing towards, which I completely agree with him, is that when you start implementing things

1523
02:18:43,680 --> 02:18:48,000
with deep learning, you need to learn from software 1.0 in terms of testing, continuous

1524
02:18:48,000 --> 02:18:53,840
integration, how you deploy, how do you validate all these things and building systems around that,

1525
02:18:53,840 --> 02:18:58,480
so that you're not just saying, like, ooh, it seems like it's good, ship it, right? Well,

1526
02:18:58,480 --> 02:19:02,000
what happens when I regress something? What happens when I make a classification that's

1527
02:19:02,000 --> 02:19:07,280
wrong and now I heard somebody, right? All these things you have to reason about.

1528
02:19:07,280 --> 02:19:14,240
Yeah, but at the same time, the bootloader that works for us humans looks awfully a lot like a

1529
02:19:14,240 --> 02:19:20,320
neural network, right? It's messy and you can cut out different parts of the brain. There's a lot of

1530
02:19:20,320 --> 02:19:26,880
this neuroplasticity work that shows that it's going to adjust. It's a really interesting question,

1531
02:19:26,880 --> 02:19:31,440
how much of the world's programming could be replaced by software 2.0?

1532
02:19:32,720 --> 02:19:36,320
Well, I mean, it's provably true that you could replace all of it.

1533
02:19:37,440 --> 02:19:39,040
Right, so then it's a question of the trade-offs.

1534
02:19:39,040 --> 02:19:43,840
So anything that's a function, you can. So it's not a question about if I think it's an

1535
02:19:43,840 --> 02:19:48,960
economic question. It's a, what kind of talent can you get? What kind of trade-offs in terms of

1536
02:19:48,960 --> 02:19:53,680
maintenance? Those kinds of questions, I think, what kind of data can you collect? I think one

1537
02:19:53,760 --> 02:19:59,040
of the reasons that I'm most interested in machine learning is a programming paradigm is that one

1538
02:19:59,040 --> 02:20:04,080
of the things that we've seen across computing in general is that being laser focused on one paradigm

1539
02:20:04,640 --> 02:20:10,320
often puts you in a box. It's not super great. And so you look at object-oriented programming,

1540
02:20:10,320 --> 02:20:14,320
like it was all the rage in the early 80s and like everything has to be objects and people

1541
02:20:14,320 --> 02:20:19,760
forgot about functional programming even though came first. And then people rediscovered that,

1542
02:20:19,760 --> 02:20:24,160
hey, if you mix functional and object-oriented and structure, like you mix these things together,

1543
02:20:24,160 --> 02:20:28,320
you can provide very interesting tools that are good at solving different problems.

1544
02:20:28,320 --> 02:20:33,360
And so the question there is, how do you get the best way to solve the problems? It's not about

1545
02:20:33,360 --> 02:20:39,120
whose tribe should win, right? It's not about, that shouldn't be the question. The question is,

1546
02:20:39,120 --> 02:20:43,440
how do you make it so that people can solve those problems the fastest and they have the right

1547
02:20:43,440 --> 02:20:47,760
tools in their box to build good libraries and they can solve these problems? And when you look

1548
02:20:47,760 --> 02:20:52,160
at that, that's like, you look at reinforcement learning as one really interesting subdomain

1549
02:20:52,160 --> 02:20:56,960
of this. Reinforcement learning, often you have to have the integration of a learned model

1550
02:20:57,520 --> 02:21:02,880
combined with your Atari or whatever the other scenario it is that you're working in. You have

1551
02:21:02,880 --> 02:21:09,440
to combine that thing with the robot control for the arm, right? And so now it's not just about

1552
02:21:09,440 --> 02:21:15,840
that one paradigm, it's about integrating that with all the other systems that you have, including

1553
02:21:15,840 --> 02:21:21,040
often legacy systems and things like this, right? And so to me, I think that the interesting thing

1554
02:21:21,040 --> 02:21:25,120
to say is like, how do you get the best out of this domain? And how do you enable people to

1555
02:21:25,120 --> 02:21:30,000
achieve things that they otherwise couldn't do without excluding all the good things we already

1556
02:21:30,000 --> 02:21:38,800
know how to do? Right. But okay, this is a crazy question. But we talked a little about GPT-3,

1557
02:21:38,800 --> 02:21:47,360
but do you think it's possible that these language models that in essence, in the language domain,

1558
02:21:47,360 --> 02:21:54,160
software 2.0 could replace some aspect of compilation, for example, or do program synthesis

1559
02:21:54,160 --> 02:21:59,680
replace some aspect of programming? Yeah, absolutely. So I think that the learned models

1560
02:21:59,680 --> 02:22:02,960
in general are extremely powerful. And I think the people underestimate them.

1561
02:22:03,920 --> 02:22:10,640
Maybe you can suggest what I should do. So of access to the GPT-3 API,

1562
02:22:11,280 --> 02:22:15,520
would I be able to generate Swift code, for example? Do you think that could do something

1563
02:22:15,520 --> 02:22:22,720
interesting? So GPT-3 is probably not trained on the right corpus. So it probably has the ability

1564
02:22:22,720 --> 02:22:26,880
to generate some Swift, I bet it does. It's probably not going to generate a large enough

1565
02:22:26,880 --> 02:22:32,080
body of Swift to be useful. But take it a next step further. If you had the goal of training

1566
02:22:32,080 --> 02:22:39,120
something like GPT-3, and you wanted to try to generate source code, it could definitely do that.

1567
02:22:39,680 --> 02:22:44,480
Now, the question is, how do you express the intent of what you want filled in? You can

1568
02:22:44,480 --> 02:22:50,480
definitely write scaffolding of code and say fill in the hole, and put in some for loops or put in

1569
02:22:50,480 --> 02:22:54,880
some classes or whatever. And the power of these models is impressive. But there's an unsolved

1570
02:22:54,880 --> 02:22:59,280
question, at least unsolved to me, which is, how do I express the intent of what to fill in?

1571
02:22:59,840 --> 02:23:05,840
Right. And kind of what you'd really want to have, and I don't know that these models are up to

1572
02:23:05,840 --> 02:23:11,040
the task, is you want to be able to say, here's the scaffolding, and here are the assertions at the end.

1573
02:23:12,320 --> 02:23:16,320
And the assertions always pass. And so you want a generative model, on the one hand, yes.

1574
02:23:16,320 --> 02:23:17,520
Oh, that's fascinating. Yeah.

1575
02:23:17,520 --> 02:23:23,040
Right. But you also want some loopback, some reinforcement learning system or something

1576
02:23:23,040 --> 02:23:27,680
where you're actually saying, like, I need to hill climb towards something that is more correct.

1577
02:23:28,400 --> 02:23:34,080
And I don't know that we have that. So it would generate not only a bunch of the code, but like

1578
02:23:34,080 --> 02:23:38,080
the checks that do the testing. It would generate the tests. I think the humans would generate

1579
02:23:38,080 --> 02:23:43,040
the tests, right? Oh, okay. But it would be fascinating if... Well, the tests are the requirements.

1580
02:23:43,040 --> 02:23:46,960
Yes, but the... Okay, so... Because you have to express to the model what you want to...

1581
02:23:46,960 --> 02:23:50,480
You don't just want gibberish code. Look at how compelling this code looks.

1582
02:23:51,200 --> 02:23:54,720
You want a story about four horned unicorns or something.

1583
02:23:54,720 --> 02:24:00,560
Well, okay, so exactly. But that's human requirements. But then I thought it's a compelling idea that

1584
02:24:00,560 --> 02:24:12,640
the GPT-4 model could generate checks that are more high fidelity, that check for correctness.

1585
02:24:12,640 --> 02:24:20,720
Because the code it generates, say I ask it to generate a function that gives me the Fibonacci

1586
02:24:20,720 --> 02:24:27,760
sequence. Sure. I don't like... So decompose the problem, right? So you have two things. You have...

1587
02:24:27,760 --> 02:24:32,320
You need the ability to generate syntactically corrects with code that's interesting, right?

1588
02:24:32,960 --> 02:24:38,720
I think GPT series of model architectures can do that. But then you need the ability to

1589
02:24:39,360 --> 02:24:46,240
add the requirements. So generate Fibonacci. The human needs to express that goal. We don't

1590
02:24:46,240 --> 02:24:51,200
have that language that I know of. No, I mean, it can generate stuff. Have you seen

1591
02:24:51,200 --> 02:24:58,240
what GPT-3 can generate? You can say, I mean, there's interface stuff. It can generate HTML.

1592
02:24:58,240 --> 02:25:04,880
It can generate basic for loops that give you... Right, but pick HTML. How do I say I want google.com?

1593
02:25:05,920 --> 02:25:11,040
Well, no, you can say... Or not literally google.com. How do I say I want a web page that's got a

1594
02:25:11,040 --> 02:25:16,880
shopping cart and this and that? It does that. Okay, so just... I don't know if you've seen these

1595
02:25:16,880 --> 02:25:22,640
demonstrations, but you type in, I want a red button with the text that says hello, and you

1596
02:25:22,640 --> 02:25:27,920
type that natural language, and it generates the correct HTML. I've done this demo. It's kind

1597
02:25:27,920 --> 02:25:35,040
of compelling. So you have to prompt it with similar kinds of mappings. Of course, it's probably

1598
02:25:35,040 --> 02:25:40,000
handpicked. I have to experiment that probably... But the fact that you can do that once, even out of

1599
02:25:40,000 --> 02:25:47,680
like 20 is quite impressive. Again, that's very basic. Like the HTML is kind of messy and bad.

1600
02:25:48,320 --> 02:25:52,480
But yes, the intent is... The idea is the intent is specified in natural language.

1601
02:25:53,440 --> 02:25:59,760
So I have not seen that. That's really cool. Yeah, but the question is the correctness of that.

1602
02:25:59,760 --> 02:26:09,520
Like visually, you can check, oh, the button is red. But for more complicated functions,

1603
02:26:10,080 --> 02:26:15,920
where the intent is harder to check, this goes into like NP completeness kind of things, like

1604
02:26:15,920 --> 02:26:22,400
I want to know that this code is correct and generates a giant thing that just some kind

1605
02:26:22,400 --> 02:26:29,520
of calculation. It seems to be working. It's interesting to think like should the system also

1606
02:26:29,520 --> 02:26:34,560
try to generate checks for itself for correctness? Yeah, I don't know. And this is way beyond my

1607
02:26:34,560 --> 02:26:40,400
experience. The thing that I think about is that there doesn't seem to be a lot of

1608
02:26:41,040 --> 02:26:45,920
equational reasoning going on. Right. There's a lot of pattern matching and filling in and

1609
02:26:45,920 --> 02:26:49,840
kind of propagating patterns that have been seen before into the future and into the

1610
02:26:49,840 --> 02:26:54,320
generated result. And so if you want to get correctness, you kind of need their improving

1611
02:26:54,320 --> 02:26:59,360
kind of things and like higher level logic. And I don't know that... You could talk to Yon about

1612
02:26:59,360 --> 02:27:06,160
that and see what the bright minds are thinking about right now. But I don't think the GPT

1613
02:27:06,160 --> 02:27:12,800
is in that vein. It's still really cool. Yeah. And who knows? Maybe reasoning is...

1614
02:27:13,840 --> 02:27:19,440
Is overrated. Yeah, it's overrated. I mean, do we reason? How do you tell? Are we just pattern

1615
02:27:19,440 --> 02:27:22,960
matching based on what we have and then reverse justifying it to ourselves?

1616
02:27:22,960 --> 02:27:26,240
Exactly. So like I think what the neural networks are missing

1617
02:27:26,880 --> 02:27:33,600
and I think GPT form might have is to be able to tell stories to itself about what it did.

1618
02:27:33,600 --> 02:27:38,160
Well, that's what humans do, right? I mean, you talk about like network explainability, right?

1619
02:27:38,160 --> 02:27:42,320
And we give... No, no, that's a hard time about this. But humans don't know why we make decisions.

1620
02:27:42,320 --> 02:27:46,160
We have this thing called intuition. And then we try to like say this feels like the right thing,

1621
02:27:46,160 --> 02:27:50,400
but why? Right? And you wrestle with that when you're making hard decisions. And

1622
02:27:51,360 --> 02:27:57,280
is that science? Not really. Let me ask you about a few high level questions, I guess, is

1623
02:27:59,840 --> 02:28:05,760
you've done a million things in your life and been very successful. A bunch of young folks

1624
02:28:05,760 --> 02:28:13,200
listen to this, ask for advice from successful people like you. If you were to give advice to

1625
02:28:13,520 --> 02:28:21,440
somebody, you know, another graduate student or some high school student about pursuing a

1626
02:28:21,440 --> 02:28:27,840
career in computing or just advice about life in general, is there some words of wisdom you can

1627
02:28:27,840 --> 02:28:34,320
give them? So I think you come back to change and, you know, profound leaps happen because

1628
02:28:34,320 --> 02:28:39,440
people are willing to believe that change is possible and that the world does change and

1629
02:28:39,440 --> 02:28:44,640
are willing to do the hard thing that it takes to make change happen. And whether it be implementing

1630
02:28:44,640 --> 02:28:49,120
a new programming language or implementing a new system or implementing a new research paper,

1631
02:28:49,120 --> 02:28:53,360
designing a new thing, moving the world forward in science and philosophy, whatever,

1632
02:28:53,360 --> 02:28:57,600
it really comes down to somebody who's willing to put in the work, right? And you have...

1633
02:28:59,120 --> 02:29:03,520
The work is hard for a whole bunch of different reasons. One of which is you...

1634
02:29:04,080 --> 02:29:09,680
It's work, right? And so you have to have the space in your life in which you can do that work,

1635
02:29:09,680 --> 02:29:12,960
which is why going to grad school can be a beautiful thing for certain people.

1636
02:29:14,640 --> 02:29:18,240
But also there's a self-doubt that happens. Like you're two years into a project,

1637
02:29:18,240 --> 02:29:23,120
is it going anywhere? Right? Well, what do you do? Do you just give up because it's hard?

1638
02:29:23,120 --> 02:29:30,160
Well, no. I mean, some people like suffering. And so you plow through it. The secret to me is

1639
02:29:30,160 --> 02:29:36,320
that you have to love what you're doing. And follow that passion because when you get to

1640
02:29:36,320 --> 02:29:41,520
the hard times, that's when... If you love what you're doing, you're willing to push through.

1641
02:29:43,200 --> 02:29:49,280
This is really hard because it's hard to know what you will love doing until you start doing

1642
02:29:49,280 --> 02:29:53,120
a lot of things. And so that's why I think that particularly early in your career,

1643
02:29:53,120 --> 02:29:59,360
it's good to experiment. Do a little bit of everything. Go take the survey class on the

1644
02:29:59,360 --> 02:30:05,760
first half of every class in your upper division lessons and just get exposure to things because

1645
02:30:05,760 --> 02:30:09,200
certain things will resonate with you and you'll find out, wow, I'm really good at this. I'm really

1646
02:30:09,200 --> 02:30:13,920
smart at this. Well, it's just because it works with the way your brain. And when something jumps

1647
02:30:13,920 --> 02:30:19,840
out, I mean, that's one of the things that people often ask about is like, well, I think there's

1648
02:30:19,840 --> 02:30:27,440
a bunch of cool stuff out there. Like, how do I pick the thing? Like, how do you hook in your life?

1649
02:30:27,440 --> 02:30:31,920
How did you just hook yourself in and stuck with it? Well, I got lucky, right? I mean,

1650
02:30:31,920 --> 02:30:38,880
I think that many people forget that a huge amount of it or most of it is luck, right? So

1651
02:30:39,600 --> 02:30:46,960
let's not forget that. So for me, I fell in love with computers early on because they spoke to me,

1652
02:30:46,960 --> 02:30:57,200
I guess. What language did they speak? Basic. But then it was just kind of following a set of

1653
02:30:57,200 --> 02:31:02,960
logical progressions, but also deciding that something that was hard was worth doing and a

1654
02:31:02,960 --> 02:31:07,600
lot of fun, right? And so I think that that is also something that's true for many other

1655
02:31:07,600 --> 02:31:12,800
domains, which is if you find something that you love doing that's also hard, if you invest

1656
02:31:12,800 --> 02:31:17,760
yourself in it and add value to the world, then it will mean something generally, right? And again,

1657
02:31:17,760 --> 02:31:21,920
that can be a research paper, that can be a software system, that can be a new robot,

1658
02:31:21,920 --> 02:31:27,440
that can be, there's many things that can be, but a lot of it is like real value comes from

1659
02:31:27,440 --> 02:31:34,560
doing things that are hard. And that doesn't mean you have to suffer. But it's hard. I mean,

1660
02:31:34,560 --> 02:31:39,600
you don't often hear that message. We talked about it last time a little bit, but it's one of my

1661
02:31:40,240 --> 02:31:47,280
not enough people talk about this. It's beautiful to hear a successful person.

1662
02:31:47,280 --> 02:31:52,560
Well, in self-doubt and imposter syndrome, and these are all things that successful people suffer

1663
02:31:52,560 --> 02:31:58,240
with as well, particularly when they put themselves in a point of being uncomfortable, which I like

1664
02:31:58,240 --> 02:32:03,120
to do now and then just because it puts you in learning mode. Like if you want to, if you want

1665
02:32:03,120 --> 02:32:08,400
to grow as a person, put yourself in a room with a bunch of people that know way more about whatever

1666
02:32:08,400 --> 02:32:13,840
you're talking about than you do and ask dumb questions. And guess what? Smart people love to

1667
02:32:13,920 --> 02:32:18,480
teach, often, not always, but often. And if you listen, if you're prepared to listen, if you're

1668
02:32:18,480 --> 02:32:21,840
prepared to grow, if you're prepared to make connections, you can do some really interesting

1669
02:32:21,840 --> 02:32:27,280
things. And I think a lot of progress is made by people who kind of hop between domains now and

1670
02:32:27,280 --> 02:32:35,360
then because they bring a perspective into a field that nobody else has, if people have only

1671
02:32:35,360 --> 02:32:41,040
been working in that field themselves. We mentioned that the universe is kind of like a compiler,

1672
02:32:41,840 --> 02:32:46,320
you know, the entirety of it, the whole evolution is kind of a kind of a compilation.

1673
02:32:47,520 --> 02:32:53,680
Maybe us human beings are kind of compilers. Let me ask the old sort of question that I

1674
02:32:53,680 --> 02:32:58,880
didn't ask you last time, which is, what's the meaning of it all? Is there a meaning? Like if

1675
02:32:58,880 --> 02:33:04,400
you asked a compiler why, what would a compiler say? What's the meaning of life?

1676
02:33:04,400 --> 02:33:08,240
What's the meaning of life? You know, I'm prepared for not to mean anything.

1677
02:33:08,640 --> 02:33:19,520
Here we are, all biological things programmed to survive and propagate our DNA. And maybe the

1678
02:33:19,520 --> 02:33:24,640
universe is just a computer and you just go until entropy takes over the world or takes

1679
02:33:24,640 --> 02:33:30,160
over the universe and then you're done. I don't think that's a very productive way to live your

1680
02:33:30,160 --> 02:33:37,120
life, if so. And so I prefer to bias towards the other way, which is saying the universe has a

1681
02:33:37,120 --> 02:33:43,360
lot of value. And I take happiness out of other people. And a lot of times part of that's having

1682
02:33:43,360 --> 02:33:49,040
kids, but also the relationships you build with other people. And so the way I try to live my

1683
02:33:49,040 --> 02:33:53,360
life is like, what can I do that has value? How can I move the world forward? How can I

1684
02:33:53,360 --> 02:33:59,360
take what I'm good at and like bring it into the world? And how can I, I'm one of these people

1685
02:33:59,360 --> 02:34:04,400
that likes to work really hard and be very focused on the things that I do. And so if I'm going to

1686
02:34:04,400 --> 02:34:09,920
do that, how can it be in a domain that actually will matter? Because a lot of things that we do,

1687
02:34:09,920 --> 02:34:13,600
we find ourselves in the cycle of like, okay, I'm doing a thing, I'm very familiar with it,

1688
02:34:13,600 --> 02:34:18,640
I've done it for a long time, I've never done anything else, but I'm not really learning.

1689
02:34:19,680 --> 02:34:23,680
I'm not really, I'm keeping things going, but there's a, there's a younger generation that

1690
02:34:23,680 --> 02:34:28,000
can do the same thing, maybe even better than me, right? Maybe if I actually step out of this and

1691
02:34:28,000 --> 02:34:33,840
jump into something I'm less comfortable with, it's scary. But on the other hand, it gives somebody

1692
02:34:33,840 --> 02:34:38,320
else a new opportunity. It also then puts you back in learning mode. And that can be really

1693
02:34:38,320 --> 02:34:43,280
interesting. And one of the things I've learned is that when you go through that, that first you're

1694
02:34:43,280 --> 02:34:47,760
deep into imposter syndrome. But when you start working your way out, you start to realize, hey,

1695
02:34:47,760 --> 02:34:53,600
well, there's actually a method to this. And, and now I'm able to add new things because I bring

1696
02:34:53,600 --> 02:34:58,240
different perspective. And this is one of the, the good things about bringing different kinds of

1697
02:34:58,240 --> 02:35:04,320
people together. Diversity of thought is really important. And if you can pull together people

1698
02:35:04,320 --> 02:35:09,600
that are coming at things from different directions, you often get innovation. And I love to see that,

1699
02:35:09,600 --> 02:35:13,920
that aha moment where you're like, we've like really cracked this, this is something nobody's

1700
02:35:13,920 --> 02:35:18,240
ever done before. And then if you can do it in the context where it adds value, other people can

1701
02:35:18,240 --> 02:35:22,560
build on it, it helps move the world, then that's what, that's what really excites me.

1702
02:35:22,560 --> 02:35:27,440
So that kind of description of the magic of the human experience, do you think we'll ever create

1703
02:35:27,440 --> 02:35:37,360
that in like an AGI system, you think we'll be able to create, give, give AI systems a sense of

1704
02:35:37,360 --> 02:35:42,160
meaning, where they operate in this kind of world exactly in the way you've described, which is they

1705
02:35:42,160 --> 02:35:47,360
interact with each other, they interact with us humans. Sure. Sure. Well, so I mean, I, why, why are

1706
02:35:47,360 --> 02:35:59,440
you being so a species, right? All right. So, so AGI versus bionets, you know, what are we about

1707
02:35:59,440 --> 02:36:04,080
machines, right? We're just programmed to run our, we have our objective function that we're

1708
02:36:04,080 --> 02:36:09,200
optimized for, right? And so we're doing our thing, we think we have purpose, but do we really?

1709
02:36:09,200 --> 02:36:15,440
Yeah. Right. I'm not prepared to say that those newfangled AGI's have no soul just because we

1710
02:36:15,440 --> 02:36:20,400
don't understand them, right? And I think that would be when they, when they exist, that would be

1711
02:36:20,400 --> 02:36:26,320
very premature to look at a new thing through your own lens without fully understanding it.

1712
02:36:28,080 --> 02:36:32,960
You might be just saying that because AI systems in the future will be listening to this and then,

1713
02:36:32,960 --> 02:36:36,960
Oh yeah, exactly. You don't want to say anything. Please be nice to me, you know, when Skynet,

1714
02:36:36,960 --> 02:36:42,560
Skynet kills everybody, please spare me. So why is, why is a look ahead thinking?

1715
02:36:42,560 --> 02:36:46,320
Yeah, but I mean, I think that people will spend a lot of time worrying about this kind of stuff.

1716
02:36:46,320 --> 02:36:49,840
And I think that what we should be worrying about is how do we make the world better?

1717
02:36:49,840 --> 02:36:57,360
And the thing that I'm most scared about with AGI's is not that, that necessarily the Skynet

1718
02:36:57,360 --> 02:37:01,840
will start shooting everybody with lasers and stuff like that to, to use us for our calories.

1719
02:37:02,960 --> 02:37:08,960
The thing that I'm worried about is that humanity, I think needs a challenge. And if we get into a

1720
02:37:08,960 --> 02:37:14,240
mode of not having a personal challenge, not having a personal contribution, whether that be

1721
02:37:14,240 --> 02:37:19,120
like, you know, your kids and seeing what they grow into and helping, helping guide them, whether it

1722
02:37:19,120 --> 02:37:24,160
be your community that you're engaged in, you're driving forward, whether it be your work and the

1723
02:37:24,160 --> 02:37:27,360
things that you're doing and the people you're working with in the product you're building and

1724
02:37:27,360 --> 02:37:33,600
the contribution there, if people don't have a objective, I'm afraid what that means. And

1725
02:37:34,240 --> 02:37:40,400
and I think that this would lead to a rise of the worst part of people, right? Instead of

1726
02:37:40,400 --> 02:37:47,760
people striving together and trying to make the world better, it could degrade into a very

1727
02:37:48,560 --> 02:37:53,920
unpleasant world. But, but I don't know. I mean, we hopefully have a long ways to go before we

1728
02:37:53,920 --> 02:37:58,400
discover that. And unfortunately, we have pretty on the ground problems with the pandemic right

1729
02:37:58,400 --> 02:38:03,680
now. And so I think we should be focused on that as well. Yeah, ultimately, just as you said,

1730
02:38:03,680 --> 02:38:09,440
you're optimistic. I think it helps for us to be optimistic. That's faking until you make it.

1731
02:38:10,240 --> 02:38:13,200
Yeah, well, and why not? I mean, what's what's the other side? Right? So I mean,

1732
02:38:15,200 --> 02:38:19,760
I'm not personally a very religious person. But I've heard people say like, Oh, yeah, of course,

1733
02:38:19,760 --> 02:38:25,360
I believe in God. Of course, I go to church because if God's real, I want to be on the right

1734
02:38:25,360 --> 02:38:28,640
side of that. And if it's not real, it doesn't matter. Yeah, it doesn't matter. And so, you know,

1735
02:38:29,200 --> 02:38:35,520
that's that's a fair way to do it. Yeah, I mean, the same thing with with nuclear deterrence,

1736
02:38:35,520 --> 02:38:40,640
all, you know, global warming, all these things, all these threats, natural engineer pandemics,

1737
02:38:41,280 --> 02:38:50,640
all these threats we face, I think it's, it's paralyzing to be terrified of all the possible

1738
02:38:50,640 --> 02:38:57,440
ways we could destroy ourselves. I think it's much better, or at least productive to be hopeful

1739
02:38:57,440 --> 02:39:04,800
and to engineer defenses against these things to engineer a future where like, you know,

1740
02:39:04,800 --> 02:39:09,200
see like a positive future and engineer that future. Yeah, well, and I think that's other

1741
02:39:09,200 --> 02:39:14,080
another thing to think about as, you know, a human, particularly if you're young and trying

1742
02:39:14,080 --> 02:39:19,120
to figure out what it is that you want to be when you grow up, like I am. I'm always looking for that.

1743
02:39:19,840 --> 02:39:25,600
The question then is, how do you want to spend your time? And right now, there seems to be a

1744
02:39:25,600 --> 02:39:31,760
norm of being a consumption culture, like I'm going to watch the news and, and revel in how

1745
02:39:31,760 --> 02:39:36,880
horrible everything is right now, I'm going to go find out about the latest atrocity and find out

1746
02:39:36,880 --> 02:39:42,400
all the details of like the terrible thing that happened and be outraged by it. You can spend

1747
02:39:42,400 --> 02:39:47,440
a lot of time watching TV and watching the news sitcom or whatever people watch these days, I

1748
02:39:47,440 --> 02:39:53,760
don't know. But that's a lot of hours, right? And those are hours that if you're turning to

1749
02:39:53,760 --> 02:40:00,400
being productive, learning, growing, experiencing, you know, when the pandemic's over, going exploring,

1750
02:40:01,680 --> 02:40:06,640
right, it leads to more growth. And I think it leads to more optimism and happiness because

1751
02:40:06,640 --> 02:40:10,880
you're, you're, you're building, right, you're building yourself, you're building your capabilities,

1752
02:40:10,880 --> 02:40:15,760
you're building your viewpoints, you're building your perspective. And I think that a lot of the

1753
02:40:16,720 --> 02:40:22,800
the consuming of other people's messages leads to kind of a negative viewpoint, which you need to

1754
02:40:22,800 --> 02:40:27,600
be aware of what's happening, because that's also important. But there's a balance that I think

1755
02:40:27,600 --> 02:40:33,360
focusing on creation is, is a very valuable thing to do. Yeah. So what you're saying is people should

1756
02:40:33,360 --> 02:40:40,000
focus on working on the sexiest field of all, which is compiler design. Exactly. Hey, you can

1757
02:40:40,000 --> 02:40:44,240
go work on machine learning and be crowded out by the thousands of graduates popping out of school

1758
02:40:44,240 --> 02:40:48,560
that all want to do the same thing, or you could work in the place that people overpay you,

1759
02:40:48,560 --> 02:40:53,360
because there's not enough smart people working in it. And here at the end of Moore's Law,

1760
02:40:53,360 --> 02:40:56,880
according to some people, actually the software is the hard part too.

1761
02:40:58,400 --> 02:41:04,800
I mean, optimization is truly, truly beautiful. And also on the YouTube side or education side,

1762
02:41:06,080 --> 02:41:11,120
you know, it's, there's a, it'd be nice to have some material that shows the beauty of

1763
02:41:11,120 --> 02:41:17,440
compilers. Yeah, yeah. That's, that's something. So that's a call for, for people to create that

1764
02:41:17,440 --> 02:41:23,680
kind of content as well. Chris, you're one of my favorite people to talk to. It's such a huge honor

1765
02:41:23,680 --> 02:41:28,400
that you would waste your time talking to me. I've always appreciated. Thank you so much for

1766
02:41:28,400 --> 02:41:33,200
talking to me. The truth of it is you spent a lot of time talking to me just on, you know,

1767
02:41:33,200 --> 02:41:36,000
walks and other things like that. So it's great to catch up with. Thanks, man.

1768
02:41:36,000 --> 02:41:42,240
Thanks for listening to this conversation with Chris Latner. And thank you to our sponsors,

1769
02:41:42,240 --> 02:41:48,240
Blinkist, an app that summarizes key ideas from thousands of books, Neuro, which is a maker of

1770
02:41:48,240 --> 02:41:54,240
functional gum and mints that supercharged my mind, Masterclass, which are online courses from

1771
02:41:54,240 --> 02:42:00,560
world experts, and finally, Cash App, which is an app for sending money to friends. Please check

1772
02:42:00,560 --> 02:42:06,240
out these sponsors in the description to get a discount and to support this podcast. If you

1773
02:42:06,240 --> 02:42:10,960
enjoyed this thing, subscribe on YouTube, review it with five stars on Apple podcast, follow on

1774
02:42:10,960 --> 02:42:17,120
Spotify, support on Patreon, connect with me on Twitter, Alex Friedman. And now let me leave you

1775
02:42:17,120 --> 02:42:22,000
with some words from Chris Latner. So much of language design is about trade offs. And you

1776
02:42:22,000 --> 02:42:26,880
can't see those trade offs unless you have a community of people that really represent those

1777
02:42:26,880 --> 02:42:32,160
different points. Thank you for listening and hope to see you next time.

