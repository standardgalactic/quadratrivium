1
00:00:00,000 --> 00:00:02,880
The following is a conversation with Greg Brockman.

2
00:00:02,880 --> 00:00:05,360
He's the co-founder and CTO of OpenAI,

3
00:00:05,360 --> 00:00:09,040
a world-class research organization developing ideas in AI

4
00:00:09,040 --> 00:00:12,880
with the goal of eventually creating a safe and friendly

5
00:00:12,880 --> 00:00:15,360
artificial general intelligence.

6
00:00:15,360 --> 00:00:18,800
One that benefits and empowers humanity.

7
00:00:18,800 --> 00:00:24,400
OpenAI is not only a source of publications, algorithms, tools, and data sets.

8
00:00:24,400 --> 00:00:28,080
Their mission is a catalyst for an important public discourse

9
00:00:28,080 --> 00:00:33,200
about our future with both narrow and general intelligence systems.

10
00:00:33,920 --> 00:00:38,800
This conversation is part of the Artificial Intelligence Podcast at MIT and BEYOND.

11
00:00:39,440 --> 00:00:44,400
If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter

12
00:00:44,400 --> 00:00:48,000
at Lex Friedman, spelled F-R-I-D.

13
00:00:48,000 --> 00:00:51,600
And now here's my conversation with Greg Brockman.

14
00:00:52,640 --> 00:00:56,080
So in high school and right after you wrote a draft of a chemistry textbook,

15
00:00:56,640 --> 00:01:00,720
I saw that that covers everything from basic structure of the atom to quantum mechanics.

16
00:01:01,440 --> 00:01:08,240
So it's clear you have an intuition and a passion for both the physical world with chemistry

17
00:01:08,240 --> 00:01:14,400
and non-robotics to the digital world with AI, deep learning, reinforcement learning,

18
00:01:14,400 --> 00:01:18,480
so on. Do you see the physical world and the digital world as different?

19
00:01:18,480 --> 00:01:19,760
And what do you think is the gap?

20
00:01:20,400 --> 00:01:23,200
A lot of it actually boils down to iteration speed, right?

21
00:01:23,200 --> 00:01:26,480
That I think that a lot of what really motivates me is building things, right?

22
00:01:27,680 --> 00:01:30,800
Think about mathematics, for example, where you think really hard about a problem,

23
00:01:30,800 --> 00:01:33,920
you understand it. You're right down in this very obscure form that we call proof.

24
00:01:34,480 --> 00:01:38,320
But then this is in humanity's library, right? It's there forever.

25
00:01:38,320 --> 00:01:39,760
This is some truth that we've discovered.

26
00:01:40,480 --> 00:01:42,880
And maybe only five people in your field will ever read it,

27
00:01:42,880 --> 00:01:44,720
but somehow you've kind of moved humanity forward.

28
00:01:45,520 --> 00:01:48,640
And so I actually used to really think that I was going to be a mathematician.

29
00:01:48,640 --> 00:01:51,520
And then I actually started writing this chemistry textbook.

30
00:01:51,520 --> 00:01:54,880
One of my friends told me, you'll never publish it because you don't have a PhD.

31
00:01:54,880 --> 00:01:59,840
So instead, I decided to build a website and try to promote my ideas that way.

32
00:01:59,840 --> 00:02:05,200
And then I discovered programming. And in programming, you think hard about a problem,

33
00:02:05,200 --> 00:02:09,040
you understand it, you're right down in a very obscure form that we call a program.

34
00:02:09,840 --> 00:02:12,160
But then once again, it's in humanity's library, right?

35
00:02:12,160 --> 00:02:15,600
And anyone can get the benefit from it and the scalability is massive.

36
00:02:15,600 --> 00:02:19,040
And so I think that the thing that really appeals to me about the digital world

37
00:02:19,040 --> 00:02:21,920
is that you can have this insane leverage, right?

38
00:02:21,920 --> 00:02:26,240
A single individual with an idea is able to affect the entire planet.

39
00:02:26,240 --> 00:02:30,240
And that's something I think is really hard to do if you're moving around physical atoms.

40
00:02:30,240 --> 00:02:36,320
But you said mathematics. So if you look at the wet thing over here, our mind,

41
00:02:37,120 --> 00:02:41,680
you ultimately see it as just math, as just information processing,

42
00:02:41,680 --> 00:02:47,040
or is there some other magic if you've seen through biology and chemistry and so on?

43
00:02:47,040 --> 00:02:50,960
I think it's really interesting to think about humans as just information processing systems,

44
00:02:50,960 --> 00:02:54,960
and that it seems like it's actually a pretty good way of describing a lot of

45
00:02:56,240 --> 00:03:00,560
how the world works, a lot of what we're capable of, to think that, again,

46
00:03:00,560 --> 00:03:04,480
if you just look at technological innovations over time, that in some ways,

47
00:03:04,480 --> 00:03:07,680
the most transformative innovation that we've had has been the computer, right?

48
00:03:07,680 --> 00:03:10,400
In some ways, the internet, what has the internet done?

49
00:03:10,400 --> 00:03:12,640
The internet is not about these physical cables.

50
00:03:12,640 --> 00:03:16,480
It's about the fact that I am suddenly able to instantly communicate with any other human

51
00:03:16,480 --> 00:03:20,720
on the planet. I'm able to retrieve any piece of knowledge that, in some ways,

52
00:03:20,720 --> 00:03:26,000
the human race has ever had, and that those are these insane transformations.

53
00:03:26,000 --> 00:03:31,360
Do you see our society as a whole the collective as another extension of the intelligence of the

54
00:03:31,360 --> 00:03:34,960
human being? So if you look at the human being as an information processing system,

55
00:03:34,960 --> 00:03:39,280
you mentioned the internet, the networking. Do you see us all together as a civilization

56
00:03:39,280 --> 00:03:41,600
as a kind of intelligent system?

57
00:03:41,600 --> 00:03:45,760
Yeah, I think this is actually a really interesting perspective to take and to think about

58
00:03:45,760 --> 00:03:49,360
that you sort of have this collective intelligence of all of society.

59
00:03:49,360 --> 00:03:54,320
The economy itself is this superhuman machine that is optimizing something, right?

60
00:03:54,320 --> 00:03:57,840
And in some ways, a company has a will of its own, right?

61
00:03:57,840 --> 00:04:01,040
That you have all these individuals who are all pursuing their own individual goals and thinking

62
00:04:01,040 --> 00:04:05,280
really hard and thinking about the right things to do, but somehow the company does something

63
00:04:05,280 --> 00:04:10,480
that is this emergent thing and that it's a really useful abstraction.

64
00:04:10,480 --> 00:04:15,040
And so I think that in some ways, we think of ourselves as the most intelligent things on the

65
00:04:15,040 --> 00:04:18,800
planet and the most powerful things on the planet, but there are things that are bigger

66
00:04:18,800 --> 00:04:22,400
than us that are these systems that we all contribute to. And so I think actually,

67
00:04:22,400 --> 00:04:27,040
you know, it's interesting to think about, if you've read Asa Geismov's Foundation,

68
00:04:27,040 --> 00:04:30,880
right, that there's this concept of psycho history in there, which is effectively this,

69
00:04:30,880 --> 00:04:35,040
that if you have trillions or quadrillions of beings, then maybe you could actually

70
00:04:35,040 --> 00:04:40,880
predict what that being, that huge macro being will do and almost independent of what the

71
00:04:40,880 --> 00:04:44,880
individuals want. And I actually have a second angle on this that I think is interesting,

72
00:04:44,880 --> 00:04:50,560
which is thinking about technological determinism. One thing that I actually think a lot about with

73
00:04:50,560 --> 00:04:56,320
OpenAI, right, is that we're kind of coming on onto this insanely transformational technology of

74
00:04:56,320 --> 00:04:59,920
general intelligence, right, that will happen at some point. And there's a question of,

75
00:04:59,920 --> 00:05:04,000
how can you take actions that will actually steer it to go better rather than worse?

76
00:05:04,720 --> 00:05:09,280
And that I think one question you need to ask is, as a scientist, as an event as a creator,

77
00:05:09,280 --> 00:05:13,280
what impact can you have in general, right? You look at things like the telephone invented by

78
00:05:13,280 --> 00:05:17,040
two people on the same day. Like, what does that mean? Like, what does that mean about the shape

79
00:05:17,040 --> 00:05:21,040
of innovation? And I think that what's going on is everyone's building on the shoulders of the same

80
00:05:21,040 --> 00:05:25,600
giants. And so you can kind of, you can't really hope to create something no one else ever would.

81
00:05:25,600 --> 00:05:29,040
You know, if Einstein wasn't born, someone else would have come up with relativity.

82
00:05:29,040 --> 00:05:32,880
You know, you change the timeline a bit, right, that maybe it would have taken another 20 years,

83
00:05:32,880 --> 00:05:37,120
but it wouldn't be that fundamentally humanity would never discover these fundamental truths.

84
00:05:37,520 --> 00:05:44,320
There's some kind of invisible momentum that some people like Einstein or OpenAI is plugging into

85
00:05:45,280 --> 00:05:50,240
that anybody else can also plug into. And ultimately, that wave takes us into a certain

86
00:05:50,240 --> 00:05:53,120
direction. That's what you mean by digitalism. That's right. That's right. And, you know,

87
00:05:53,120 --> 00:05:56,800
this kind of seems to play out in a bunch of different ways, that there's some exponential

88
00:05:56,800 --> 00:06:00,880
that is being written, and that the exponential itself, which one it is, changes, think about

89
00:06:00,880 --> 00:06:06,080
Moore's law, an entire industry set, it's clocked to it for 50 years. Like, how can that be, right?

90
00:06:06,080 --> 00:06:11,760
How is that possible? And yet somehow it happened. And so I think you can't hope to ever invent

91
00:06:11,760 --> 00:06:16,080
something that no one else will. Maybe you can change the timeline a little bit. But if you

92
00:06:16,080 --> 00:06:19,280
really want to make a difference, I think that the thing that you really have to do,

93
00:06:19,280 --> 00:06:23,840
the only real degree of freedom you have is to set the initial conditions under which a technology

94
00:06:23,840 --> 00:06:27,760
is born. And so you think about the internet, right, that there are lots of other competitors

95
00:06:27,760 --> 00:06:33,440
trying to build similar things. And the internet one, and that the initial conditions were that

96
00:06:33,440 --> 00:06:38,480
it was created by this group that really valued people being able to be, you know, anyone being

97
00:06:38,480 --> 00:06:43,040
able to plug in this very academic mindset of being open and connected. And I think that the

98
00:06:43,040 --> 00:06:47,920
internet for the next 40 years really played out that way. You know, maybe today, things are

99
00:06:47,920 --> 00:06:51,440
starting to shift in a different direction. But I think that those initial conditions were really

100
00:06:51,440 --> 00:06:56,560
important to determine the next 40 years worth of progress. That's really beautifully put. So

101
00:06:56,560 --> 00:07:01,680
another example of that I think about, you know, I recently looked at it, I looked at Wikipedia,

102
00:07:01,680 --> 00:07:06,880
the formation of Wikipedia. And I wonder what the internet would be like if Wikipedia had ads.

103
00:07:07,600 --> 00:07:13,200
You know, there's a interesting argument that why they chose not to make it put advertisement

104
00:07:13,200 --> 00:07:18,800
on Wikipedia. I think it's, I think Wikipedia is one of the greatest resources we have on the internet.

105
00:07:18,800 --> 00:07:23,920
It's extremely surprising how well it works and how well it was able to aggregate all this kind of

106
00:07:23,920 --> 00:07:28,640
good information. And essentially the creator of Wikipedia, I don't know, there's probably some

107
00:07:28,640 --> 00:07:33,440
debates there, but set the initial conditions. And now it carried itself forward. That's really

108
00:07:33,440 --> 00:07:37,920
interesting. So you're the way you're thinking about AGI or artificial intelligence is you're

109
00:07:37,920 --> 00:07:43,360
focused on setting the initial conditions for the progress. That's right. That's powerful. Okay, so

110
00:07:43,920 --> 00:07:50,080
look into the future. If you create an AGI system, like one that can ace the Turing test,

111
00:07:50,080 --> 00:07:56,080
natural language, what do you think would be the interactions you would have with it? What do you

112
00:07:56,080 --> 00:07:59,280
think are the questions you would ask? Like what would be the first question you would ask?

113
00:08:00,480 --> 00:08:05,840
It, her, him. That's right. I think that at that point, if you've really built a powerful system

114
00:08:05,840 --> 00:08:10,240
that is capable of shaping the future of humanity, the first question that you really should ask is

115
00:08:10,240 --> 00:08:14,560
how do we make sure that this plays out well? And so that's actually the first question that I would

116
00:08:14,560 --> 00:08:20,640
ask a powerful AGI system is. So you wouldn't ask your colleague, you wouldn't ask like Ilya,

117
00:08:20,640 --> 00:08:24,320
you would ask the AGI system. Oh, we've already had the conversation with Ilya,

118
00:08:24,320 --> 00:08:27,600
right? And everyone here. And so you want as many perspectives and

119
00:08:28,400 --> 00:08:32,400
piece of wisdom as you can for answering this question. So I don't think you necessarily defer

120
00:08:32,400 --> 00:08:37,680
to whatever your powerful system tells you, but you use it as one input to try to figure out what

121
00:08:37,680 --> 00:08:42,960
to do. But I guess fundamentally, what it really comes down to is if you built something really

122
00:08:42,960 --> 00:08:47,600
powerful, and you think about, think about, for example, the creation of, of shortly after the

123
00:08:47,600 --> 00:08:51,280
creation of nuclear weapons, right? The most important question in the world was, what's the

124
00:08:51,280 --> 00:08:55,760
world we're going to be like? How do we set ourselves up in a place where we're going to be

125
00:08:55,760 --> 00:09:00,640
able to survive as a species? With AGI, I think the question is slightly different, right? That

126
00:09:00,640 --> 00:09:04,720
there is a question of how do we make sure that we don't get the negative effects? But there's

127
00:09:04,720 --> 00:09:09,600
also the positive side, right? You imagine that, you know, like, like, what will AGI be like?

128
00:09:09,600 --> 00:09:14,400
Like, what will it be capable of? And I think that one of the core reasons that an AGI can be

129
00:09:14,400 --> 00:09:19,120
powerful and transformative is actually due to technological development, right? If you have

130
00:09:19,120 --> 00:09:25,280
something that's capable, as capable as a human, and that it's much more scalable, that you absolutely

131
00:09:25,280 --> 00:09:28,800
want that thing to go read the whole scientific literature and think about how to create cures

132
00:09:28,800 --> 00:09:33,040
for all the diseases, right? You want it to think about how to go and build technologies to help

133
00:09:33,040 --> 00:09:37,920
us create material abundance and to figure out societal problems that we have trouble with,

134
00:09:37,920 --> 00:09:41,200
like how are we supposed to clean up the environment? And, you know, maybe you want

135
00:09:41,200 --> 00:09:46,480
this to go and invent a bunch of little robots that will go out and be biodegradable and turn

136
00:09:46,480 --> 00:09:54,720
ocean debris into harmless molecules. And I think that that positive side is something that I think

137
00:09:54,720 --> 00:09:59,680
people miss sometimes when thinking about what an AGI will be like. And so I think that if you

138
00:09:59,680 --> 00:10:03,840
have a system that's capable of all of that, you absolutely want its advice about how do I make sure

139
00:10:03,840 --> 00:10:09,040
that we're using your capabilities in a positive way for humanity.

140
00:10:09,040 --> 00:10:14,080
So what do you think about that psychology that looks at all the different possible

141
00:10:14,080 --> 00:10:19,840
trajectories of an AGI system, many of which perhaps the majority of which are positive,

142
00:10:19,840 --> 00:10:24,560
and nevertheless focuses on the negative trajectories? I mean, you get to interact with folks,

143
00:10:24,560 --> 00:10:30,480
you get to think about this, maybe within yourself as well. You look at Sam Harris and so on.

144
00:10:30,480 --> 00:10:34,960
It seems to be, sorry to put it this way, but almost more fun to think about the negative

145
00:10:35,600 --> 00:10:40,720
possibilities. Whatever that's deep in our psychology, what do you think about that?

146
00:10:40,800 --> 00:10:44,400
And how do we deal with it? Because we want AI to help us.

147
00:10:44,400 --> 00:10:47,200
So I think there's kind of two problems

148
00:10:48,160 --> 00:10:53,840
entailed in that question. The first is more of the question of how can you even picture

149
00:10:53,840 --> 00:10:58,320
what a world with a new technology will be like? Now imagine we're in 1950 and I'm trying to

150
00:10:58,320 --> 00:11:08,240
describe Uber to someone. Apps and the internet. Yeah, I mean, that's going to be extremely

151
00:11:08,240 --> 00:11:14,560
complicated, but it's imaginable. It's imaginable, right? And now imagine being in 1950 and predicting

152
00:11:14,560 --> 00:11:19,760
Uber, right? And you need to describe the internet, you need to describe GPS, you need to describe

153
00:11:19,760 --> 00:11:25,680
the fact that everyone's going to have this phone in their pocket. And so I think that just the first

154
00:11:25,680 --> 00:11:30,240
truth is that it is hard to picture how a transformative technology will play out in the world.

155
00:11:31,040 --> 00:11:35,440
We've seen that before with technologies that are far less transformative than AGI will be.

156
00:11:35,440 --> 00:11:41,120
And so I think that one piece is that it's just even hard to imagine and to really put yourself

157
00:11:41,120 --> 00:11:47,760
in a world where you can predict what that positive vision would be like. And I think the

158
00:11:47,760 --> 00:11:54,720
second thing is that I think it is always easier to support the negative side than the positive

159
00:11:54,720 --> 00:12:01,600
side. It's always easier to destroy than create. And less in a physical sense and more just in an

160
00:12:01,600 --> 00:12:06,160
intellectual sense, right? Because I think that with creating something, you need to just get a

161
00:12:06,160 --> 00:12:11,360
bunch of things right and to destroy, you just need to get one thing wrong. And so I think that

162
00:12:11,360 --> 00:12:15,360
what that means is that I think a lot of people's thinking dead ends as soon as they see the negative

163
00:12:15,360 --> 00:12:23,040
story. But that being said, I actually have some hope, right? I think that the positive vision

164
00:12:23,040 --> 00:12:28,640
is something that I think can be, is something that we can talk about. And I think that just

165
00:12:28,720 --> 00:12:32,400
simply saying this fact of, yeah, like there's positive, there's negatives, everyone likes to

166
00:12:32,400 --> 00:12:36,160
dwell on the negative, people actually respond well to that message and say, huh, you're right,

167
00:12:36,160 --> 00:12:40,080
there's a part of this that we're not talking about, not thinking about. And that's actually

168
00:12:40,080 --> 00:12:46,640
something that's I think really been a key part of how we think about AGI at OpenAI, right? You

169
00:12:46,640 --> 00:12:50,880
can kind of look at it as like, okay, like OpenAI talks about the fact that there are risks,

170
00:12:50,880 --> 00:12:55,360
and yet they're trying to build this system. Like, how do you square those two facts?

171
00:12:55,920 --> 00:13:01,440
So do you share the intuition that some people have, I mean, from Sam Harris to even Elon Musk

172
00:13:01,440 --> 00:13:10,400
himself, that it's tricky as you develop AGI to keep it from slipping into the existential threats,

173
00:13:10,400 --> 00:13:18,240
into the negative? What's your intuition about how hard is it to keep AI development on the

174
00:13:18,240 --> 00:13:22,240
positive track? What's your intuition there? To answer that question, you can really look at

175
00:13:22,240 --> 00:13:27,120
how we structure OpenAI. So we really have three main arms. So we have capabilities, which is

176
00:13:27,120 --> 00:13:32,400
actually doing the technical work and pushing forward what these systems can do. There's safety,

177
00:13:32,400 --> 00:13:37,520
which is working on technical mechanisms to ensure that the systems we build are aligned with human

178
00:13:37,520 --> 00:13:42,240
values. And then there's policy, which is making sure that we have governance mechanisms, answering

179
00:13:42,240 --> 00:13:47,760
that question of, well, whose values. And so I think that the technical safety one is the one

180
00:13:47,760 --> 00:13:52,160
that people kind of talk about the most, right? You talk about, like, think about, you know,

181
00:13:52,160 --> 00:13:57,040
all of the dystopic AI movies, a lot of that is about not having good technical safety in place.

182
00:13:57,760 --> 00:14:01,280
And what we've been finding is that, you know, I think that actually a lot of people

183
00:14:01,280 --> 00:14:03,760
look at the technical safety problem and think it's just intractable.

184
00:14:05,040 --> 00:14:09,040
Right, this question of, what do humans want? How am I supposed to write that down?

185
00:14:09,040 --> 00:14:15,280
Can I even write down what I want? No way. And then they stop there. But the thing is,

186
00:14:15,280 --> 00:14:21,280
we've already built systems that are able to learn things that humans can't specify, you know,

187
00:14:21,280 --> 00:14:24,880
even the rules for how to recognize if there's a cat or a dog in an image.

188
00:14:24,880 --> 00:14:27,760
Turns out it's intractable to write that down. And yet we're able to learn it.

189
00:14:28,320 --> 00:14:31,680
And that what we're seeing with systems we build at OpenAI, and they're still

190
00:14:31,680 --> 00:14:36,480
in early proof of concept stage, is that you are able to learn human preferences. You're able

191
00:14:36,480 --> 00:14:40,960
to learn what humans want from data. And so that's kind of the core focus for our technical

192
00:14:40,960 --> 00:14:45,680
safety team. And I think that they're actually, we've had some pretty encouraging updates in

193
00:14:45,680 --> 00:14:50,880
terms of what we've been able to make work. So you have an intuition and a hope that from data,

194
00:14:51,600 --> 00:14:56,160
you know, looking at the value alignment problem, from data, we can build systems that align

195
00:14:56,960 --> 00:15:04,080
with the collective better angels of our nature. So align with the ethics and the morals of human

196
00:15:04,080 --> 00:15:08,240
beings. To even say this in a different way, I mean, think about how do we align humans,

197
00:15:08,240 --> 00:15:13,120
right? Think about like a human baby can grow up to be an evil person or a great person. And a lot

198
00:15:13,120 --> 00:15:17,520
of that is from learning from data, right? That you have some feedback as a child is growing up,

199
00:15:17,520 --> 00:15:23,680
they get to see positive examples. And so I think that just like the only example we have of a

200
00:15:23,680 --> 00:15:30,160
general intelligence that is able to learn from data, to align with human values and to learn

201
00:15:30,160 --> 00:15:36,160
values, I think we shouldn't be surprised that we can do the same sorts of techniques or whether

202
00:15:36,160 --> 00:15:40,320
the same sort of techniques end up being how we, we, we saw value alignment for AGI's.

203
00:15:40,960 --> 00:15:46,800
So let's go even higher. I don't know if you've read the book sapiens, but there's an idea that,

204
00:15:46,800 --> 00:15:51,680
you know, that as a collective as us human beings, we kind of develop together

205
00:15:52,480 --> 00:15:58,640
and ideas that we hold. There's no, in that context, objective truth, we just kind of all

206
00:15:58,640 --> 00:16:02,720
agree to certain ideas and hold them as a collective. Did you have a sense that there is

207
00:16:03,360 --> 00:16:07,440
in the world of good and evil, do you have a sense that to the first approximation,

208
00:16:07,440 --> 00:16:13,520
there are some things that are good and that you could teach systems to behave to be good?

209
00:16:14,400 --> 00:16:19,840
So I think that this actually blends into our third team, right, which is the policy team.

210
00:16:19,840 --> 00:16:24,960
And this is the one, the aspect that I think people really talk about way less than they should,

211
00:16:24,960 --> 00:16:28,960
right? Because imagine that we build super powerful systems that we've managed to figure

212
00:16:28,960 --> 00:16:33,280
out all the mechanisms for these things to do whatever the operator wants. The most important

213
00:16:33,280 --> 00:16:38,400
question becomes, who's the operator? What do they want? And how is that going to affect everyone

214
00:16:38,400 --> 00:16:44,640
else? Right? And, and I think that this question of what is good, what are those values? I mean,

215
00:16:44,640 --> 00:16:48,320
I think you don't even have to go to those, those, those very grand existential places

216
00:16:48,320 --> 00:16:53,280
to start to realize how hard this problem is. You just look at different countries and cultures

217
00:16:53,280 --> 00:16:58,800
across the world and that there's, there's a very different conception of how the world works and

218
00:16:58,960 --> 00:17:04,800
you know, what, what, what kinds of ways that society wants to operate. And so I think that,

219
00:17:04,800 --> 00:17:10,880
that the really core question is, is actually very concrete. And I think it's not a question

220
00:17:10,880 --> 00:17:16,480
that we have ready answers to, right? Is how do you have a world where all the different countries

221
00:17:16,480 --> 00:17:22,720
that we have, United States, China, Russia, and you know, the hundreds of other countries out there

222
00:17:22,720 --> 00:17:29,440
are able to continue to not just operate in the way that they, that they see fit, but in, in a,

223
00:17:29,440 --> 00:17:34,320
that the world that emerges in these, where you have these very powerful systems,

224
00:17:36,000 --> 00:17:40,560
operating alongside humans, ends up being something that empowers humans more, that makes

225
00:17:40,560 --> 00:17:46,480
like, human existence be a more meaningful thing and that people are happier and wealthier and

226
00:17:47,040 --> 00:17:50,960
able to live more fulfilling lives. It's not an obvious thing for how to design that world

227
00:17:51,520 --> 00:17:56,480
once you have that very powerful system. So if we take a little step back and we're having

228
00:17:56,480 --> 00:18:01,920
like a fascinating conversation and Open AI is in many ways a tech leader in the world.

229
00:18:01,920 --> 00:18:06,240
And yet we're thinking about these big existential questions, which is fascinating,

230
00:18:06,240 --> 00:18:10,720
really important. I think you're a leader in that space and that's a really important space

231
00:18:10,720 --> 00:18:15,760
of just thinking how AI affects society in a big picture view. So Oscar Wilde said,

232
00:18:16,320 --> 00:18:20,800
we're all in the gutter, but some of us are looking at the stars. And I think Open AI has a

233
00:18:21,360 --> 00:18:26,240
charter that looks to the stars, I would say, to create intelligence, to create general

234
00:18:26,240 --> 00:18:30,080
intelligence, make it beneficial, safe, and collaborative. So can you tell me

235
00:18:32,160 --> 00:18:37,760
how that came about? How a mission like that and the path to creating a mission like that at Open

236
00:18:37,760 --> 00:18:44,000
AI was founded? Yeah. So I think that in some ways it really boils down to taking a look at the

237
00:18:44,000 --> 00:18:49,840
landscape. So if you think about the history of AI that basically for the past 60 or 70 years,

238
00:18:49,840 --> 00:18:54,960
people have thought about this goal of what could happen if you could automate human intellectual

239
00:18:54,960 --> 00:19:00,560
labor. Imagine you can build a computer system that could do that. What becomes possible? We have

240
00:19:00,560 --> 00:19:04,960
a lot of sci-fi that tells stories of various dystopias and increasingly you have movies like

241
00:19:04,960 --> 00:19:10,000
Her that tell you a little bit about maybe more of a little bit utopic vision. You think about

242
00:19:10,560 --> 00:19:17,040
the impacts that we've seen from being able to have bicycles for our minds in computers,

243
00:19:17,760 --> 00:19:24,320
and that I think that the impact of computers on the internet has just far outstripped what anyone

244
00:19:24,320 --> 00:19:29,280
really could have predicted. And so I think that it's very clear that if you can build an AGI,

245
00:19:29,280 --> 00:19:32,640
it will be the most transformative technology that humans will ever create.

246
00:19:34,480 --> 00:19:39,680
And so what it boils down to then is a question of, well, is there a path? Is there hope? Is there

247
00:19:39,680 --> 00:19:45,040
a way to build such a system? And I think that for 60 or 70 years that people got excited and

248
00:19:46,080 --> 00:19:51,920
that ended up not being able to deliver on the hopes that people had pinned on them. And I think

249
00:19:51,920 --> 00:19:59,520
that then that after two winters of AI development, that people I think kind of almost stopped daring

250
00:19:59,520 --> 00:20:05,360
to dream that really talking about AGI or thinking about AGI became almost this taboo in the community.

251
00:20:06,320 --> 00:20:10,880
But I actually think that people took the wrong lesson from AI history. And if you look back,

252
00:20:10,880 --> 00:20:15,760
starting in 1959 is when the Perceptron was released. And this is basically one of the

253
00:20:15,760 --> 00:20:20,640
earliest neural networks. It was released to what was perceived as this massive over-hype.

254
00:20:20,640 --> 00:20:27,200
So in the New York Times in 1959, you have this article saying that the Perceptron will one day

255
00:20:27,200 --> 00:20:31,360
recognize people, call out their names, instantly translate speech between languages,

256
00:20:31,360 --> 00:20:36,080
and people at the time looked at this and said, this is your system can't do any of that. And

257
00:20:36,080 --> 00:20:40,560
basically spent 10 years trying to discredit the whole Perceptron direction and succeeded.

258
00:20:40,560 --> 00:20:45,840
And all the funding dried up and people kind of went in other directions. And in the 80s,

259
00:20:45,840 --> 00:20:50,160
there was this resurgence. And I'd always heard that the resurgence in the 80s was due to the

260
00:20:50,160 --> 00:20:54,960
invention of back propagation and these algorithms that got people excited. But actually, the causality

261
00:20:54,960 --> 00:20:59,040
was due to people building larger computers. That you can find these articles from the 80s

262
00:20:59,040 --> 00:21:02,800
saying that the democratization of computing power suddenly meant that you could run these

263
00:21:02,800 --> 00:21:06,960
larger neural networks. And then people started to do all these amazing things back propagation

264
00:21:06,960 --> 00:21:10,880
algorithm was invented. And the neural nets people were running were these tiny little

265
00:21:10,880 --> 00:21:16,080
like 20 neuron neural nets. What are you supposed to learn with 20 neurons? And so, of course,

266
00:21:16,720 --> 00:21:21,840
they weren't able to get great results. And it really wasn't until 2012 that this approach

267
00:21:21,840 --> 00:21:26,800
that's almost the most simple, natural approach that people had come up with in the 50s.

268
00:21:27,360 --> 00:21:30,640
Right. And in some ways, even in the 40s, before there were computers with the pits,

269
00:21:30,640 --> 00:21:36,480
McCullin there in Iran, suddenly this became the best way of solving problems.

270
00:21:37,120 --> 00:21:40,640
Right. And I think there are three core properties that deep learning has

271
00:21:41,360 --> 00:21:46,560
that I think are very worth paying attention to. The first is generality. We have a very small

272
00:21:46,560 --> 00:21:52,000
number of deep learning tools, SGD, deep neural net, maybe some, some, you know, RL,

273
00:21:52,960 --> 00:21:57,200
and it solves this huge variety of problems, speech recognition, machine translation,

274
00:21:57,200 --> 00:22:01,920
game playing, all of these problems, small set of tools. So there's the generality.

275
00:22:02,800 --> 00:22:06,160
There's a second piece, which is the competence. You want to solve any of those problems

276
00:22:06,880 --> 00:22:11,360
throughout 40 years worth of normal computer vision research, replace it with a deep neural

277
00:22:11,360 --> 00:22:16,160
net. It's kind of worked better. And there's a third piece, which is the scalability, right?

278
00:22:16,160 --> 00:22:20,400
That one thing that has been shown time and time again is that you, if you have a larger neural

279
00:22:20,400 --> 00:22:26,880
network, throw more compute, more data at it, it will work better. Those three properties together

280
00:22:26,880 --> 00:22:31,920
feel like essential parts of building a general intelligence. Now, it doesn't just mean that

281
00:22:31,920 --> 00:22:36,640
if we scale up what we have, that we will have an AGI, right? There are clearly missing pieces.

282
00:22:36,640 --> 00:22:42,320
There are missing ideas. We need to have answers for reasoning. But I think that the core here

283
00:22:42,320 --> 00:22:47,840
is that for the first time, it feels that we have a paradigm that gives us hope

284
00:22:47,840 --> 00:22:52,080
that general intelligence can be achievable. And so as soon as you believe that,

285
00:22:52,080 --> 00:22:56,880
everything else becomes, comes into focus, right? If you imagine that you may be able to and,

286
00:22:56,880 --> 00:23:01,040
you know, that the timeline, I think, remains uncertain. But I think that, you know,

287
00:23:01,040 --> 00:23:04,880
certainly within our lifetimes and possibly within a much shorter period of time than,

288
00:23:04,880 --> 00:23:09,440
than people would expect, if you can really build the most transformative technology that will

289
00:23:09,440 --> 00:23:13,520
ever exist, you stop thinking about yourself so much, right? And you start thinking about just

290
00:23:13,520 --> 00:23:17,280
like, how do you have a world where this goes well, and that you need to think about the

291
00:23:17,280 --> 00:23:21,600
practicalities of how do you build an organization and get together a bunch of people and resources,

292
00:23:22,240 --> 00:23:29,760
and to make sure that people feel motivated and ready to do it. But I think that then

293
00:23:29,760 --> 00:23:33,520
you start thinking about, well, what if we succeed? And how do we make sure that when we

294
00:23:33,520 --> 00:23:38,640
succeed, that the world is actually the place that we want ourselves to exist in? And, you know,

295
00:23:38,640 --> 00:23:43,840
almost in the Rawlsian Bale sense of the word. And so that's kind of the broader landscape.

296
00:23:43,840 --> 00:23:51,440
And OpenAI was really formed in 2015 with that high level picture of AGI might be possible

297
00:23:51,440 --> 00:23:57,440
sooner than people think, and that we need to try to do our best to make sure it's going to go well.

298
00:23:57,440 --> 00:24:01,040
And then we spent the next couple of years really trying to figure out, what does that mean? How do

299
00:24:01,040 --> 00:24:07,440
we do it? And, you know, I think that typically with a company, you start out very small. So you

300
00:24:07,440 --> 00:24:10,960
want to co-founder and you build a product, you get some users, you get a product market fit,

301
00:24:11,280 --> 00:24:16,560
then at some point you raise some money, you hire people, you scale, and then down the road,

302
00:24:16,560 --> 00:24:21,120
then the big companies realize you exist and try to kill you. And for OpenAI, it was basically

303
00:24:21,120 --> 00:24:27,440
everything in exactly the opposite order. Let me just pause for a second. You said a lot of things.

304
00:24:27,440 --> 00:24:35,040
And let me just admire the jarring aspect of what OpenAI stands for, which is daring to dream.

305
00:24:35,040 --> 00:24:39,200
I mean, you said it's pretty powerful. You caught me off guard because I think that's very true.

306
00:24:40,160 --> 00:24:46,640
The step of just daring to dream about the possibilities of creating intelligence

307
00:24:46,640 --> 00:24:53,520
in a positive and a safe way, but just even creating intelligence is a much needed, refreshing

308
00:24:55,600 --> 00:25:00,320
catalyst for the AI community. So that's the starting point. Okay. So then the formation of

309
00:25:00,320 --> 00:25:06,880
OpenAI. I would just say that, you know, when we're starting OpenAI, that kind of the first

310
00:25:06,880 --> 00:25:11,280
question that we had is, is it too late to start a lab with a bunch of the best people?

311
00:25:11,920 --> 00:25:15,200
Right? Was that even possible? That was an actual question. That was really,

312
00:25:15,200 --> 00:25:20,400
that was the core question of, you know, we had this dinner in July of 2015, and that was

313
00:25:20,400 --> 00:25:25,120
really what we spent the whole time talking about. And, you know, because it's, you think

314
00:25:25,120 --> 00:25:31,440
about kind of where AI was, is that it transitioned from being an academic pursuit to an industrial

315
00:25:31,440 --> 00:25:36,000
pursuit. And so a lot of the best people were in these big research labs, and that we wanted to

316
00:25:36,000 --> 00:25:41,200
start our own one that, you know, no matter how much resources we could accumulate would be,

317
00:25:41,200 --> 00:25:45,280
you know, pale in comparison to the big tech companies. And we knew that. And there's a

318
00:25:45,280 --> 00:25:49,280
question of, are we going to be actually able to get this thing off the ground? You need critical

319
00:25:49,280 --> 00:25:53,440
mass. You can't just do you and a co-founder build a product, right? You really need to have a group

320
00:25:53,440 --> 00:25:59,600
of, you know, five to 10 people. And we kind of concluded it wasn't obviously impossible. So it

321
00:25:59,600 --> 00:26:05,760
seemed worth trying. Well, you're also a dreamer. So who knows, right? That's right. Okay. So

322
00:26:06,880 --> 00:26:14,400
speaking of that, competing with the big players, let's talk about some of the tricky things as you

323
00:26:14,400 --> 00:26:21,440
think through this process of growing, of seeing how you can develop these systems at scale that

324
00:26:21,440 --> 00:26:30,160
competes. So you recently formed OpenAI LP, a new cap profit company that now carries the name

325
00:26:30,160 --> 00:26:36,560
OpenAI. So OpenAI is now this official company. The original nonprofit company still exists and

326
00:26:36,560 --> 00:26:42,480
carries the OpenAI nonprofit name. So can you explain what this company is, what the purpose

327
00:26:42,480 --> 00:26:50,720
of its creation is, and how did you arrive at the decision to create it? OpenAI, the whole entity

328
00:26:50,720 --> 00:26:56,960
and OpenAI LP as a vehicle is trying to accomplish the mission of ensuring that artificial general

329
00:26:56,960 --> 00:27:00,800
intelligence benefits everyone. And the main way that we're trying to do that is by actually

330
00:27:00,800 --> 00:27:04,800
trying to build general intelligence to ourselves and make sure the benefits are distributed to the

331
00:27:04,800 --> 00:27:09,840
world. That's the primary way. We're also fine if someone else does this, right? It doesn't have

332
00:27:09,840 --> 00:27:14,560
to be us. If someone else is going to build an AGI and make sure that the benefits don't get locked

333
00:27:14,560 --> 00:27:20,320
up in one company or, you know, with one set of people, like, we're actually fine with that.

334
00:27:21,040 --> 00:27:28,240
And so those ideas are baked into our charter, which is kind of the foundational document

335
00:27:28,240 --> 00:27:34,320
that describes kind of our values and how we operate. But it's also really baked into the

336
00:27:34,320 --> 00:27:40,640
structure of OpenAI LP. And so the way that we've set up OpenAI LP is that in the case where we

337
00:27:40,640 --> 00:27:46,480
succeed, right, if we actually build what we're trying to build, then investors are able to get

338
00:27:46,560 --> 00:27:52,560
a return. But that return is something that is capped. And so if you think of AGI in terms of the

339
00:27:52,560 --> 00:27:56,480
value that you could really create, you're talking about the most transformative technology ever

340
00:27:56,480 --> 00:28:01,120
created, it's going to create, or does the magnitude more value than any existing company?

341
00:28:01,760 --> 00:28:07,760
And that all of that value will be owned by the world, like legally titled to the nonprofit

342
00:28:07,760 --> 00:28:11,680
to fulfill that mission. And so that's the structure.

343
00:28:12,640 --> 00:28:18,880
So the mission is a powerful one. And it's one that I think most people would agree with.

344
00:28:18,880 --> 00:28:25,600
It's how we would hope AI progresses. And so how do you tie yourself to that mission? How do you

345
00:28:25,600 --> 00:28:33,360
make sure you do not deviate from that mission? That, you know, other incentives that are profit

346
00:28:33,360 --> 00:28:38,960
driven wouldn't don't interfere with the mission. So this was actually a really core question for

347
00:28:38,960 --> 00:28:43,200
us for the past couple of years, because, you know, I'd say that like the way that our history

348
00:28:43,200 --> 00:28:46,960
went was that for the first year, we were getting off the ground, right? We had this high level

349
00:28:46,960 --> 00:28:53,360
picture, but we didn't know exactly how we wanted to accomplish it. And really two years ago is

350
00:28:53,360 --> 00:28:57,840
when we first started realizing in order to build AGI, we're just going to need to raise way more

351
00:28:57,840 --> 00:29:03,120
money than we can as a nonprofit. And you know, we're talking many billions of dollars. And so

352
00:29:04,320 --> 00:29:07,920
the first question is, how are you supposed to do that and stay true to this mission?

353
00:29:08,560 --> 00:29:12,000
And we looked at every legal structure out there and included none of them are quite right for

354
00:29:12,000 --> 00:29:15,200
what we wanted to do. And I guess it shouldn't be too surprising if you're going to do some like

355
00:29:15,200 --> 00:29:18,800
crazy unprecedented technology that you're going to have to come with some crazy unprecedented

356
00:29:18,800 --> 00:29:26,000
structure to do it in. And a lot of our conversation was with people at OpenAI, right,

357
00:29:26,000 --> 00:29:30,400
the people who really joined because they believe so much in this mission, and thinking about how

358
00:29:30,400 --> 00:29:36,160
do we actually raise the resources to do it and also stay true to what we stand for. And the place

359
00:29:36,160 --> 00:29:40,000
you've got to start is to really align on what is it that we stand for, right? What are those

360
00:29:40,000 --> 00:29:44,880
values? What's really important to us? And so I'd say that we spent about a year really compiling

361
00:29:44,880 --> 00:29:49,680
the OpenAI charter and that determines, and if you even look at the first the first line item in

362
00:29:49,680 --> 00:29:53,600
there, it says that look, we expect we're going to have to marshal huge amounts of resources,

363
00:29:53,600 --> 00:29:58,560
but we're going to make sure that we minimize conflict of interest with the mission. And that

364
00:29:58,560 --> 00:30:03,280
kind of aligning on all of those pieces was the most important step towards figuring out

365
00:30:04,160 --> 00:30:09,600
how do we structure a company that can actually raise the resources to do what we need to do.

366
00:30:10,240 --> 00:30:17,040
I imagine OpenAI, the decision to create OpenAI LP was a really difficult one. And there was a

367
00:30:17,040 --> 00:30:23,760
lot of discussions as you mentioned for a year. And there's different ideas, perhaps detractors

368
00:30:23,760 --> 00:30:30,080
within OpenAI, sort of different paths that you could have taken. What were those concerns?

369
00:30:30,080 --> 00:30:33,920
What were the different paths considered? What was that process of making that decision like?

370
00:30:34,800 --> 00:30:40,240
But so if you look actually at the OpenAI charter, that there's almost two paths embedded within it.

371
00:30:40,800 --> 00:30:46,160
There is, we are primarily trying to build AGI ourselves, but we're also okay if someone else

372
00:30:46,160 --> 00:30:51,440
does it. And this is a weird thing for a company. It's really interesting, actually. Yeah. There

373
00:30:51,440 --> 00:30:57,360
is an element of competition that you do want to be the one that does it. But at the same time,

374
00:30:57,360 --> 00:31:00,880
you're okay if somebody else doesn't. We'll talk about that a little bit, that trade-off,

375
00:31:00,880 --> 00:31:04,960
that dance that's really interesting. And I think this was the core tension as we were

376
00:31:04,960 --> 00:31:09,920
designing OpenAI LP and really the OpenAI strategy, is how do you make sure that both,

377
00:31:09,920 --> 00:31:15,120
you have a shot at being a primary actor, which really requires building an organization,

378
00:31:15,760 --> 00:31:20,480
raising massive resources, and really having the will to go and execute on some really,

379
00:31:20,480 --> 00:31:25,120
really hard vision. You need to really sign up for a long period to go and take on a lot of

380
00:31:25,120 --> 00:31:31,600
pain and a lot of risk. And to do that, normally you just import the startup mindset, right?

381
00:31:31,600 --> 00:31:35,360
And that you think about, okay, how do we out-execute everyone? You have this very competitive

382
00:31:35,360 --> 00:31:39,760
angle. But you also have the second angle of saying that, well, the true mission isn't for

383
00:31:39,760 --> 00:31:46,560
OpenAI to build AGI. The true mission is for AGI to go well for humanity. And so how do you take

384
00:31:46,560 --> 00:31:52,000
all of those first actions and make sure you don't close the door on outcomes that would actually

385
00:31:52,000 --> 00:31:56,560
be positive and fulfill the mission? And so I think it's a very delicate balance, right?

386
00:31:56,560 --> 00:32:01,200
And I think that going 100% one direction or the other is clearly not the correct answer.

387
00:32:01,200 --> 00:32:05,280
And so I think that even in terms of just how we talk about OpenAI and think about it,

388
00:32:05,280 --> 00:32:09,600
there's just like one thing that's always in the back of my mind is to make sure

389
00:32:09,600 --> 00:32:14,640
that we're not just saying OpenAI's goal is to build AGI, right? That it's actually much

390
00:32:14,640 --> 00:32:20,160
broader than that, right? That, first of all, it's not just AGI. It's safe AGI that's very important.

391
00:32:20,160 --> 00:32:23,920
But secondly, our goal isn't to be the ones to build it. Our goal is to make sure it goes well

392
00:32:23,920 --> 00:32:28,720
for the world. And so I think that figuring out how do you balance all of those and to get people

393
00:32:28,720 --> 00:32:35,760
to really come to the table and compile the like a single document that encompasses all of that,

394
00:32:36,480 --> 00:32:43,440
wasn't trivial. So part of the challenge here is your mission is, I would say, beautiful,

395
00:32:43,440 --> 00:32:48,160
empowering and a beacon of hope for people in the research community and just people thinking

396
00:32:48,160 --> 00:32:55,760
about AI. So your decisions are scrutinized more than I think a regular profit driven company.

397
00:32:55,760 --> 00:33:00,080
Do you feel the burden of this in the creation of the charter and just in the way you operate?

398
00:33:00,080 --> 00:33:00,320
Yes.

399
00:33:02,880 --> 00:33:10,320
So why do you lean into the burden by creating such a charter? Why not keep it quiet?

400
00:33:10,320 --> 00:33:15,360
I mean, it just boils down to the mission, right? Like, I'm here and everyone else is here because

401
00:33:15,360 --> 00:33:17,840
we think this is the most important mission, right?

402
00:33:17,840 --> 00:33:24,640
Dare to dream. All right. So do you think you can be good for the world or create an AGI system

403
00:33:24,640 --> 00:33:31,520
that's good when you're a for profit company? From my perspective, I don't understand why profit

404
00:33:32,880 --> 00:33:41,280
interferes with positive impact on society. I don't understand why Google that makes most

405
00:33:41,280 --> 00:33:47,360
of its money from ads can't also do good for the world or other companies, Facebook, anything.

406
00:33:47,360 --> 00:33:55,920
I don't understand why those have to interfere. Profit isn't the thing in my view that affects

407
00:33:55,920 --> 00:34:01,280
the impact of a company. What affects the impact of the company is the charter, is the culture,

408
00:34:01,280 --> 00:34:08,640
is the people inside and profit is the thing that just fuels those people. What are your views there?

409
00:34:08,640 --> 00:34:14,080
Yeah. So I think that's a really good question and there's some real long-standing debates

410
00:34:14,080 --> 00:34:18,560
in human society that are wrapped up in it. The way that I think about it is just think about

411
00:34:18,560 --> 00:34:25,840
what are the most impactful nonprofits in the world? What are the most impactful for profits in

412
00:34:25,840 --> 00:34:30,800
the world? Right. It's much easier to list the for profits. That's right. And I think that there's

413
00:34:30,800 --> 00:34:37,120
some real truth here that the system that we set up, the system for kind of how today's world is

414
00:34:37,120 --> 00:34:44,320
organized is one that really allows for huge impact and that kind of part of that is that you

415
00:34:44,320 --> 00:34:51,280
need to be, that for profits are self-sustaining and able to kind of build on their own momentum.

416
00:34:51,280 --> 00:34:55,920
And I think that's a really powerful thing. It's something that when it turns out that we

417
00:34:55,920 --> 00:35:00,000
haven't set the guardrails correctly causes problems. Think about logging companies that go

418
00:35:00,000 --> 00:35:05,840
into the rainforest. That's really bad. We don't want that. And it's actually really interesting

419
00:35:05,840 --> 00:35:11,200
to me that kind of this question of how do you get positive benefits out of a for-profit company?

420
00:35:11,200 --> 00:35:14,560
It's actually very similar to how do you get positive benefits out of an AGI,

421
00:35:15,600 --> 00:35:20,240
that you have this very powerful system. It's more powerful than any human and

422
00:35:20,240 --> 00:35:24,400
it's kind of autonomous in some ways. It's super human in a lot of axes and somehow you have to

423
00:35:24,400 --> 00:35:30,000
set the guardrails to get good things to happen. But when you do, the benefits are massive. And so

424
00:35:30,000 --> 00:35:35,600
I think that when I think about nonprofit versus for-profit, I think it's just not enough

425
00:35:35,680 --> 00:35:39,920
happens in nonprofits. They're very pure, but it's just kind of, you know, it's just hard to do things

426
00:35:39,920 --> 00:35:46,320
there. And for-profits in some ways, like too much happens. But if kind of shaped in the right way,

427
00:35:46,320 --> 00:35:52,320
it can actually be very positive. And so with OpenILP, we're picking a road in between. Now,

428
00:35:52,320 --> 00:35:56,000
the thing that I think is really important to recognize is that the way that we think about

429
00:35:56,000 --> 00:36:01,520
OpenILP is that in the world where AGI actually happens, right? In a world where we are successful,

430
00:36:01,520 --> 00:36:04,960
we build the most transformative technology ever, the amount of value we're going to create will

431
00:36:04,960 --> 00:36:14,160
be astronomical. And so then in that case, the cap that we have will be a small fraction of the

432
00:36:14,160 --> 00:36:18,880
value we create. And the amount of value that goes back to investors and employees looks pretty

433
00:36:18,880 --> 00:36:25,200
similar to what would happen in a pretty successful startup. And that's really the case that we're

434
00:36:25,200 --> 00:36:30,560
optimizing for, right? That we're thinking about in the success case, making sure that the value we

435
00:36:30,560 --> 00:36:35,200
create doesn't get locked up. And I expect that in other, you know, for-profit companies that it's

436
00:36:35,200 --> 00:36:40,320
possible to do something like that, I think it's not obvious how to do it, right? And I think that

437
00:36:40,320 --> 00:36:44,400
as a for-profit company, you have a lot of fiduciary duty to your shareholders and that

438
00:36:44,400 --> 00:36:49,360
there are certain decisions that you just cannot make. In our structure, we've set it up so that

439
00:36:50,080 --> 00:36:55,200
we have a fiduciary duty to the charter that we always get to make the decision that is right

440
00:36:55,200 --> 00:37:00,080
for the charter rather than even if it comes at the expense of our own stakeholders.

441
00:37:00,640 --> 00:37:04,880
And so I think that when I think about what's really important, it's not really about

442
00:37:04,880 --> 00:37:10,320
non-profit versus for-profit. It's really a question of if you build a GI and you kind of,

443
00:37:10,320 --> 00:37:16,240
you know, humanity is now at this new age, who benefits? Whose lives are better? And I think

444
00:37:16,240 --> 00:37:22,080
that what's really important is to have an answer that is everyone. Yeah, which is one of the core

445
00:37:22,080 --> 00:37:27,040
aspects of the charter. So one concern people have, not just with OpenAI, but with Google,

446
00:37:27,040 --> 00:37:36,400
Facebook, Amazon, anybody, really, that's creating impact at scale is how do we avoid,

447
00:37:36,400 --> 00:37:42,480
as your charter says, avoid enabling the use of AI or AGI to unduly concentrate power?

448
00:37:43,520 --> 00:37:48,480
Why would not a company like OpenAI keep all the power of an AGI system to itself?

449
00:37:48,480 --> 00:37:49,360
The charter.

450
00:37:49,360 --> 00:37:51,520
The charter. So, you know, how does the charter

451
00:37:51,920 --> 00:37:56,640
actualize itself in day to day?

452
00:37:57,200 --> 00:38:02,000
So I think that the first to zoom out, right, that the way that we structure the company is so

453
00:38:02,000 --> 00:38:07,280
that the power for sort of, you know, dictating the actions that OpenAI takes ultimately rests

454
00:38:07,280 --> 00:38:12,240
with the board, right? The board of the non-profit and the board is set up in certain ways with

455
00:38:12,240 --> 00:38:16,640
certain restrictions that you can read about in the OpenAI LP blog post. But effectively,

456
00:38:16,720 --> 00:38:24,320
the board is the governing body for OpenAI LP. And the board has a duty to fulfill the mission

457
00:38:24,320 --> 00:38:30,080
of the non-profit. And so that's kind of how we tie, how we thread all these things together.

458
00:38:30,800 --> 00:38:35,520
Now, there's a question of so day to day, how do people, the individuals who in some ways are

459
00:38:35,520 --> 00:38:39,760
the most empowered ones, right? You know, the board sort of gets to call the shots at the high level,

460
00:38:39,760 --> 00:38:43,840
but the people who are actually executing are the employees, right? The people here

461
00:38:43,840 --> 00:38:47,360
on a day to day basis who have the, you know, the keys to the technical whole kingdom.

462
00:38:48,880 --> 00:38:53,840
And there, I think that the answer looks a lot like, well, how does any company's values

463
00:38:53,840 --> 00:38:58,080
get actualized, right? And I think that a lot of that comes down to the unique people who are here

464
00:38:58,080 --> 00:39:03,440
because they really believe in that mission and they believe in the charter and that they

465
00:39:03,440 --> 00:39:08,720
are willing to take actions that maybe are worse for them, but are better for the charter.

466
00:39:08,720 --> 00:39:13,120
And that's something that's really baked into the culture. And honestly, I think it's, you know,

467
00:39:13,120 --> 00:39:17,600
I think that that's one of the things that we really have to work to preserve as time goes on.

468
00:39:18,240 --> 00:39:22,080
And that's a really important part of how we think about hiring people and bringing people

469
00:39:22,080 --> 00:39:27,440
into open AI. So there's people here, there's people here who could speak up and say,

470
00:39:28,960 --> 00:39:34,480
like, hold on a second, this is totally against what we stand for, culture wise.

471
00:39:34,480 --> 00:39:38,000
Yeah, yeah, for sure. I mean, I think that we actually have, I think that's like a pretty

472
00:39:38,000 --> 00:39:44,320
important part of how we operate and how we have, even again, with designing the charter and

473
00:39:44,320 --> 00:39:49,440
designing open ALP in the first place, that there has been a lot of conversation with employees

474
00:39:49,440 --> 00:39:53,200
here and a lot of times where employees said, wait a second, this seems like it's going in

475
00:39:53,200 --> 00:39:57,280
the wrong direction. And let's talk about it. And so I think one thing that's, I think a really,

476
00:39:57,280 --> 00:40:02,080
and you know, here's actually one thing that I think is very unique about us as a small company

477
00:40:02,080 --> 00:40:05,760
is that if you're at a massive tech giant, that's a little bit hard for someone who's

478
00:40:05,760 --> 00:40:10,000
aligned employee to go and talk to the CEO and say, I think that we're doing this wrong.

479
00:40:10,720 --> 00:40:15,600
And, you know, you'll get companies like Google that have had some collective action from employees

480
00:40:15,600 --> 00:40:20,080
to, you know, make ethical changes around things like Maven. And so maybe there are

481
00:40:20,080 --> 00:40:24,320
mechanisms that other companies that work. But here, super easy for anyone to pull me aside,

482
00:40:24,320 --> 00:40:27,040
to pull Sam aside, to pull Eli aside, and people do it all the time.

483
00:40:27,680 --> 00:40:32,080
One of the interesting things in the charter is this idea that it'd be great if you could try to

484
00:40:32,080 --> 00:40:38,720
describe or untangle switching from competition to collaboration and lead stage AGI development.

485
00:40:38,720 --> 00:40:42,400
It's really interesting this dance between competition and collaboration. How do you

486
00:40:42,400 --> 00:40:46,880
think about that? Yeah, assuming that you can actually do the technical side of AGI development,

487
00:40:46,880 --> 00:40:50,320
I think there's going to be two key problems with figuring out how do you actually deploy it,

488
00:40:50,320 --> 00:40:55,280
make it go well. The first one of these is the run up to building the first AGI.

489
00:40:56,240 --> 00:40:59,840
You look at how self-driving cars are being developed, and it's a competitive race.

490
00:41:00,800 --> 00:41:04,080
The thing that always happens in competitive race is that you have huge amounts of pressure

491
00:41:04,080 --> 00:41:09,920
to get rid of safety. And so that's one thing we're very concerned about, right, is that people,

492
00:41:09,920 --> 00:41:16,640
multiple teams figuring out, we can actually get there. But, you know, if we took the slower path

493
00:41:16,640 --> 00:41:22,240
that is more guaranteed to be safe, we will lose. And so we're going to take the fast path.

494
00:41:22,240 --> 00:41:27,360
And so the more that we can, both ourselves, be in a position where we don't generate that

495
00:41:27,360 --> 00:41:32,400
competitive race, where we say, if the race is being run and that someone else is further

496
00:41:32,400 --> 00:41:36,800
ahead than we are, we're not going to try to leapfrog. We're going to actually work with them,

497
00:41:36,800 --> 00:41:41,360
right? We will help them succeed. As long as what they're trying to do is to fulfill our mission,

498
00:41:42,000 --> 00:41:45,760
then we're good. We don't have to build AGI ourselves. And I think that's a really important

499
00:41:45,760 --> 00:41:49,600
commitment from us. But it can't just be unilateral, right? I think that it's really

500
00:41:49,600 --> 00:41:54,320
important that other players who are serious about building AGI make similar commitments,

501
00:41:54,320 --> 00:41:58,240
right? And I think that that, you know, again, to the extent that everyone believes that AGI

502
00:41:58,240 --> 00:42:01,600
should be something to benefit everyone, then it actually really shouldn't matter which company

503
00:42:01,600 --> 00:42:05,920
builds it. And we should all be concerned about the case where we just race so hard to get there

504
00:42:05,920 --> 00:42:12,000
that something goes wrong. So what role do you think government, our favorite entity,

505
00:42:12,000 --> 00:42:18,400
has in setting policy and rules about this domain, from research to the development to

506
00:42:19,360 --> 00:42:22,800
early stage to late stage AI and AGI development?

507
00:42:22,800 --> 00:42:27,120
So I think that, first of all, it's really important that government's in there,

508
00:42:27,680 --> 00:42:30,800
right? In some way, shape or form, you know, at the end of the day, we're talking about

509
00:42:30,800 --> 00:42:37,120
building technology that will shape how the world operates and that there needs to be government

510
00:42:37,120 --> 00:42:43,520
as part of that answer. And so that's why we've done a number of different congressional testimonies.

511
00:42:43,520 --> 00:42:49,200
We interact with a number of different lawmakers and that, you know, right now, a lot of our message

512
00:42:49,200 --> 00:42:56,800
to them is that it's not the time for regulation, it is the time for measurement, right? That our

513
00:42:56,800 --> 00:43:00,560
main policy recommendation is that people, and, you know, the government does this all the time

514
00:43:00,560 --> 00:43:06,480
with bodies like NIST, spend time trying to figure out just where the technology is,

515
00:43:06,480 --> 00:43:12,160
how fast it's moving and can really become literate and up to speed with respect to what to

516
00:43:12,160 --> 00:43:18,080
expect. So I think that today, the answer really is about measurement. And I think that there will

517
00:43:18,080 --> 00:43:24,160
be a time and place where that will change. And I think it's a little bit hard to predict exactly

518
00:43:25,440 --> 00:43:27,040
what exactly that trajectory should look like.

519
00:43:27,040 --> 00:43:33,440
So there will be a point at which regulation, federal and the United States, the government

520
00:43:33,440 --> 00:43:40,720
steps in and helps be the, I don't want to say the adult in the room, to make sure that there is

521
00:43:40,800 --> 00:43:44,480
strict rules, maybe conservative rules that nobody can cross.

522
00:43:45,040 --> 00:43:49,760
Well, I think there's kind of maybe two angles to it. So today with narrow AI applications,

523
00:43:49,760 --> 00:43:53,840
that I think there are already existing bodies that are responsible and should be responsible

524
00:43:53,840 --> 00:43:58,320
for regulation. You think about, for example, with self-driving cars, that you want the, you know,

525
00:43:58,320 --> 00:44:03,920
the national highway, NETSA, exactly to be regulated in that, that makes sense, right?

526
00:44:03,920 --> 00:44:08,160
That basically what we're saying is that we're going to have these technological systems that

527
00:44:08,160 --> 00:44:13,760
are going to be performing applications that humans already do. Great. We already have ways

528
00:44:13,760 --> 00:44:17,920
of thinking about standards and safety for those. So I think actually empowering those regulators

529
00:44:17,920 --> 00:44:24,080
today is also pretty important. And then I think for, for a GI, you know, that there's going to

530
00:44:24,080 --> 00:44:27,520
be a point where we'll have better answers. And I think that maybe a similar approach

531
00:44:27,520 --> 00:44:31,440
of first measurement and, you know, start thinking about what the rules should be.

532
00:44:31,440 --> 00:44:36,240
I think it's really important that we don't prematurely squash, you know, progress. Like,

533
00:44:36,240 --> 00:44:41,120
I think it's very easy to kind of smother the budding field. And I think that's something to

534
00:44:41,120 --> 00:44:46,240
really avoid. But I don't think that the right way of doing it is to say, let's just try to blaze

535
00:44:46,240 --> 00:44:56,240
ahead and not involve all these other stakeholders. So you've recently released a paper on GPT2

536
00:44:56,240 --> 00:45:03,680
language modeling, but did not release the full model because you had concerns about

537
00:45:03,680 --> 00:45:10,640
the possible negative effects of the availability of such model. It's outside of just that decision.

538
00:45:10,640 --> 00:45:16,960
It's super interesting because of the discussion at a societal level, the discourse it creates. So

539
00:45:16,960 --> 00:45:23,440
it's fascinating in that aspect. But if you think it's the specifics here at first, what are some

540
00:45:23,440 --> 00:45:28,480
negative effects that you envisioned? And of course, what are some of the positive effects?

541
00:45:28,480 --> 00:45:34,960
Yeah. So again, I think to zoom out, like the way that we thought about GPT2 is that with language

542
00:45:34,960 --> 00:45:41,440
modeling, we are clearly on a trajectory right now where we scale up our models and we get

543
00:45:42,400 --> 00:45:47,680
qualitatively better performance. Right. GPT2 itself was actually just a scale up of a model

544
00:45:47,680 --> 00:45:52,720
that we've released in the previous June. Right. And we just ran it at, you know, much larger scale

545
00:45:52,720 --> 00:45:57,760
and we got these results where suddenly starting to write coherent pros, which was not something

546
00:45:57,760 --> 00:46:04,480
we'd seen previously. And what are we doing now? Well, we're going to scale up GPT2 by 10x, by 100x,

547
00:46:04,480 --> 00:46:10,560
by 1000x, and we don't know what we're going to get. And so it's very clear that the model that

548
00:46:10,560 --> 00:46:16,720
we released last June, you know, I think it's kind of like, it's a good academic toy. It's not

549
00:46:16,720 --> 00:46:20,720
something that we think is something that can really have negative applications or, you know,

550
00:46:20,720 --> 00:46:25,360
to the extent that it can, that the positive of people being able to play with it is, you know,

551
00:46:25,600 --> 00:46:31,600
far outweighs the possible harms. You fast forward to not GPT2, but GPT220,

552
00:46:32,400 --> 00:46:37,120
and you think about what that's going to be like. And I think that the capabilities are going to be

553
00:46:37,120 --> 00:46:43,360
substantive. And so there needs to be a point in between the two where you say, this is something

554
00:46:43,360 --> 00:46:47,920
where we are drawing the line, and that we need to start thinking about the safety aspects.

555
00:46:47,920 --> 00:46:51,600
And I think for GPT2, we could have gone either way. And in fact, when we had conversations

556
00:46:51,600 --> 00:46:56,640
internally, that we had a bunch of pros and cons, and it wasn't clear which one, which one

557
00:46:56,640 --> 00:47:01,040
outweighed the other. And I think that when we announced that, hey, we decide not to release

558
00:47:01,040 --> 00:47:05,120
this model, then there was a bunch of conversation where various people said, it's so obvious that

559
00:47:05,120 --> 00:47:07,920
you should have just released it. There are other people said, it's so obvious you should not have

560
00:47:07,920 --> 00:47:12,640
released it. And I think that that almost definitionally means that holding it back was the correct

561
00:47:12,640 --> 00:47:17,760
decision, right? If it's not obvious whether something is beneficial or not, you should

562
00:47:17,760 --> 00:47:23,680
probably default to caution. And so I think that the overall landscape for how we think about it

563
00:47:23,680 --> 00:47:27,840
is that this decision could have gone either way. There are great arguments in both directions.

564
00:47:27,840 --> 00:47:32,640
But for future models down the road, and possibly sooner than you'd expect, because

565
00:47:32,640 --> 00:47:37,360
scaling these things up doesn't actually take that long, those ones, you're definitely not

566
00:47:37,360 --> 00:47:42,480
going to want to release into the wild. And so I think that we almost view this as a test case,

567
00:47:42,480 --> 00:47:48,160
and to see, can we even design, how do you have a society, or how do you have a system that goes

568
00:47:48,160 --> 00:47:53,280
from having no concept of responsible disclosure, where the mere idea of not releasing something

569
00:47:53,280 --> 00:47:58,560
for safety reasons is unfamiliar to a world where you say, okay, we have a powerful model,

570
00:47:58,560 --> 00:48:02,080
let's at least think about it, let's go through some process. And you think about the security

571
00:48:02,080 --> 00:48:06,160
community, it took them a long time to design responsible disclosure, right? You think about

572
00:48:06,160 --> 00:48:10,560
this question of, well, I have a security exploit, I send it to the company, the company is like,

573
00:48:10,560 --> 00:48:17,360
tries to prosecute me, or just ignores it. What do I do? And so the alternatives of, oh,

574
00:48:17,360 --> 00:48:21,040
I just always publish your exploits, that doesn't seem good either. And so it really took a long

575
00:48:21,040 --> 00:48:27,120
time, and it was bigger than any individual. It's really about building a whole community that

576
00:48:27,120 --> 00:48:30,880
believe that, okay, we'll have this process where you send it to the company, if they don't act in

577
00:48:30,880 --> 00:48:35,280
a certain time, then you can go public, and you're not a bad person, you've done the right thing.

578
00:48:36,080 --> 00:48:42,560
And I think that in AI, part of the response to GPT-2 just proves that we don't have any concept

579
00:48:42,560 --> 00:48:50,400
of this. So that's the high level picture. And so I think that this was a really important move

580
00:48:50,400 --> 00:48:55,920
to make, and we could have maybe delayed it for GPT-3, but I'm really glad we did it for GPT-2.

581
00:48:55,920 --> 00:48:59,280
And so now you look at GPT-2 itself, and you think about the substance of, okay,

582
00:48:59,280 --> 00:49:03,600
what are potential negative applications? So you have this model that's been trained on the

583
00:49:03,600 --> 00:49:08,240
internet, which is also going to be a bunch of very biased data, a bunch of very offensive

584
00:49:08,240 --> 00:49:14,640
content in there, and you can ask it to generate content for you on basically any topic. You just

585
00:49:14,640 --> 00:49:19,120
give it a prompt, and it'll just start writing, and it writes content like you see on the internet,

586
00:49:19,120 --> 00:49:25,360
even down to saying advertisement in the middle of some of its generations. And you think about

587
00:49:25,360 --> 00:49:30,160
the possibilities for generating fake news or abusive content. And it's interesting seeing

588
00:49:30,240 --> 00:49:35,600
what people have done with... We released a smaller version of GPT-2, and the people have

589
00:49:35,600 --> 00:49:41,280
done things like try to generate, take my own Facebook message history and generate more

590
00:49:41,280 --> 00:49:48,960
Facebook messages like me, and people generating fake politician content or there's a bunch of

591
00:49:48,960 --> 00:49:53,280
things there where you at least have to think, is this going to be good for the world?

592
00:49:54,560 --> 00:49:57,760
There's the flip side, which is I think that there's a lot of awesome applications that

593
00:49:57,760 --> 00:50:04,400
we really want to see, like creative applications in terms of if you have sci-fi authors that can

594
00:50:04,400 --> 00:50:09,600
work with this tool and come with cool ideas, that seems awesome if we can write better sci-fi

595
00:50:09,600 --> 00:50:13,360
through the use of these tools. And we've actually had a bunch of people write into us asking,

596
00:50:13,360 --> 00:50:17,680
hey, can we use it for a variety of different creative applications?

597
00:50:18,240 --> 00:50:27,040
So the positive are actually pretty easy to imagine. The usual NLP applications are

598
00:50:27,680 --> 00:50:33,120
really interesting, but let's go there. It's kind of interesting to think about a world where,

599
00:50:34,160 --> 00:50:41,040
look at Twitter, where not just fake news, but smarter and smarter bots being able to

600
00:50:42,480 --> 00:50:49,440
spread in an interesting complex networking way information that just floods out us regular

601
00:50:49,440 --> 00:50:57,920
human beings with our original thoughts. So what are your views of this world with GPT-20,

602
00:50:58,640 --> 00:51:04,000
right? How do we think about it? Again, it's like one of those things about in the 50s trying to

603
00:51:04,000 --> 00:51:10,320
describe the internet or the smartphone. What do you think about that world, the nature of

604
00:51:10,320 --> 00:51:18,640
information? One possibility is that we'll always try to design systems that identify robot versus

605
00:51:18,640 --> 00:51:24,800
human and we'll do so successfully. And so we'll authenticate that we're still human. And the

606
00:51:24,800 --> 00:51:30,720
other world is that we just accept the fact that we're swimming in a sea of fake news and just

607
00:51:30,720 --> 00:51:41,440
learn to swim there. Have you ever seen the popular meme of a robot with a physical arm and pen

608
00:51:41,520 --> 00:51:48,880
clicking the I'm not a robot button? I think the truth is that really trying to distinguish between

609
00:51:49,520 --> 00:51:54,080
robot and human is a losing battle. Ultimately, you think it's a losing battle? I think it's a

610
00:51:54,080 --> 00:51:58,480
losing battle ultimately, right? I think that in terms of the content, in terms of the actions

611
00:51:58,480 --> 00:52:02,320
that you can take, think about how captures have gone. The captures used to be a very nice,

612
00:52:02,320 --> 00:52:08,320
simple. You just have this image, all of our OCR is terrible. You put a couple of artifacts in it,

613
00:52:08,800 --> 00:52:13,840
humans are going to be able to tell what it is an AI system wouldn't be able to. Today,

614
00:52:13,840 --> 00:52:18,480
I could barely do captures. And I think that this is just kind of where we're going. I think

615
00:52:18,480 --> 00:52:23,600
captures were a moment in time thing. And as AI systems become more powerful, that there being

616
00:52:23,600 --> 00:52:29,280
human capabilities that can be measured in a very easy automated way that the AIs will not be

617
00:52:29,280 --> 00:52:32,960
capable of. I think that's just like, it's just an increasingly hard technical battle.

618
00:52:33,920 --> 00:52:38,800
But it's not that all hope is lost, right? You think about how do we already authenticate

619
00:52:39,600 --> 00:52:44,000
ourselves, right? We have systems, we have social security numbers, if you're in the U.S.

620
00:52:44,000 --> 00:52:51,360
or you have ways of identifying individual people and having real world identity tied to digital

621
00:52:51,360 --> 00:52:56,800
identity seems like a step towards authenticating the source of content rather than the content

622
00:52:56,800 --> 00:53:02,800
itself. Now, there are problems with that. How can you have privacy and anonymity in a world

623
00:53:02,880 --> 00:53:07,120
where the only content you can really trust is, or the only way you can trust content is by looking

624
00:53:07,120 --> 00:53:12,240
at where it comes from? And so I think that building out good reputation networks may be

625
00:53:12,240 --> 00:53:18,000
one possible solution. But yeah, I think that this question is not an obvious one. And I think that

626
00:53:19,200 --> 00:53:24,400
maybe sooner than we think we'll be in a world where today, I often will read a tweet and be like,

627
00:53:24,400 --> 00:53:27,920
do I feel like a real human wrote this? Or do I feel like this is genuine? I feel like I can

628
00:53:27,920 --> 00:53:32,480
kind of judge the content a little bit. And I think in the future, it just won't be the case.

629
00:53:32,480 --> 00:53:38,000
You look at, for example, the FCC comments on net neutrality. It came out later that millions of

630
00:53:38,000 --> 00:53:42,880
those were auto-generated and that the researchers were able to do various statistical techniques

631
00:53:42,880 --> 00:53:47,920
to do that. What do you do in a world where those statistical techniques don't exist? It's just

632
00:53:47,920 --> 00:53:53,840
impossible to tell the difference between humans and AIs. And in fact, the most persuasive arguments

633
00:53:53,840 --> 00:54:00,400
are written by AI. All that stuff, it's not sci-fi anymore. You look at GPT2 making a great argument

634
00:54:00,400 --> 00:54:04,240
for why recycling is bad for the world. You got to read that and be like, huh, you're right.

635
00:54:04,240 --> 00:54:08,240
We are addressing just the symptoms. Yeah, that's, that's quite interesting. I mean,

636
00:54:08,240 --> 00:54:13,520
ultimately, it boils down to the physical world being the last frontier of proving,

637
00:54:13,520 --> 00:54:19,280
so you said like basically networks of people, humans vouching for humans in the physical world

638
00:54:19,280 --> 00:54:26,880
and somehow the authentication ends there. I mean, if I had to ask you, I mean, you're way too

639
00:54:26,880 --> 00:54:32,400
eloquent for a human. So if I had to ask you to authenticate, like prove, how do I know you're

640
00:54:32,400 --> 00:54:40,560
not a robot and how do you know I'm not a robot? I think that's so far we're in this space, this

641
00:54:40,560 --> 00:54:46,960
conversation we just had, the physical movements we did, is the biggest gap between us and AI systems,

642
00:54:46,960 --> 00:54:52,240
is the physical manipulation. So maybe that's the last frontier. Well, here's another question,

643
00:54:52,960 --> 00:54:59,520
why is solving this problem important? Like what aspects are really important to us? I think that

644
00:54:59,520 --> 00:55:04,800
probably where we'll end up is we'll hone in on what do we really want out of knowing if we're

645
00:55:04,800 --> 00:55:10,800
talking to a human. And I think that again, this comes down to identity. And so I think that the

646
00:55:10,800 --> 00:55:14,960
internet of the future, I expect to be one that will have lots of agents out there that will

647
00:55:14,960 --> 00:55:21,920
interact with you. But I think that the question of is this real flesh and blood human or is this

648
00:55:22,240 --> 00:55:28,880
an automated system may actually just be less important. Let's actually go there. It's GPT2

649
00:55:29,600 --> 00:55:40,160
is impressive. And let's look at GPT20. Why is it so bad that all my friends are GPT20? Why is it

650
00:55:40,160 --> 00:55:47,680
so important on the internet? Do you think to interact with only human beings? Why can't we

651
00:55:47,760 --> 00:55:52,560
live in a world where ideas can come from models trained on human data?

652
00:55:54,080 --> 00:55:56,880
I think this is actually a really interesting question. This comes back to the how do you even

653
00:55:56,880 --> 00:56:02,000
picture a world with some new technology? And I think that one thing that I think is important

654
00:56:02,000 --> 00:56:10,960
is, let's say honesty. And I think that if you have almost the Turing test style sense of technology,

655
00:56:10,960 --> 00:56:16,800
you have AIs that are pretending to be humans and deceiving you. I think that feels like a bad

656
00:56:16,800 --> 00:56:21,120
thing. I think that it's really important that we feel like we're in control of our environment,

657
00:56:21,120 --> 00:56:24,880
that we understand who we're interacting with. And if it's an AI or a human,

658
00:56:26,560 --> 00:56:30,480
that's not something that we're being deceived about. But I think that the flip side of can I

659
00:56:30,480 --> 00:56:35,440
have as meaningful of an interaction with an AI as I can with a human? Well, I actually think here

660
00:56:35,440 --> 00:56:41,280
you can turn to sci-fi and her I think is a great example of asking this very question. One thing

661
00:56:41,280 --> 00:56:46,080
I really love about her is it really starts out almost by asking how meaningful our human virtual

662
00:56:46,080 --> 00:56:52,880
relationships and then you have a human who has a relationship with an AI and that you really

663
00:56:52,880 --> 00:56:57,360
start to be drawn into that and that all of your emotional buttons get triggered in the same way

664
00:56:57,360 --> 00:57:02,560
as if there was a real human that was on the other side of that phone. And so I think that this is

665
00:57:02,560 --> 00:57:07,680
one way of thinking about it is that I think that we can have meaningful interactions and that if

666
00:57:07,680 --> 00:57:12,800
there's a funny joke, some sense it doesn't really matter if it was written by a human or an AI,

667
00:57:12,800 --> 00:57:16,320
but what you don't want in a way where I think we should really draw hard lines

668
00:57:16,320 --> 00:57:21,760
is deception. And I think that as long as we're in a world where why do we build AI systems at

669
00:57:21,760 --> 00:57:26,000
all? The reason we want to build them is to enhance human lives, to make humans be able to do more

670
00:57:26,000 --> 00:57:31,040
things, to have humans feel more fulfilled. And if we can build AI systems that do that,

671
00:57:32,160 --> 00:57:39,200
sign me up. So the process of language modeling, how far do you think it take us? Let's look at

672
00:57:39,200 --> 00:57:46,080
Movie Her. Do you think a dialogue, natural language conversation is formulated by the

673
00:57:46,080 --> 00:57:51,040
Turing test, for example? Do you think that process could be achieved through this kind

674
00:57:51,040 --> 00:57:57,760
of unsupervised language modeling? So I think the Turing test in its real form isn't just about

675
00:57:57,760 --> 00:58:02,240
language. It's really about reasoning too. To really pass the Turing test, I should be able

676
00:58:02,240 --> 00:58:07,760
to teach calculus to whoever's on the other side and have it really understand calculus and be able

677
00:58:07,760 --> 00:58:13,360
to, you know, go and solve new calculus problems. And so I think that to really solve the Turing

678
00:58:13,360 --> 00:58:17,600
test, we need more than what we're seeing with language models. We need some way of plugging

679
00:58:17,600 --> 00:58:23,520
in reasoning. Now, how different will that be from what we already do? That's an open question,

680
00:58:23,520 --> 00:58:28,000
right? It might be that we need some sequence of totally radical new ideas, or it might be that we

681
00:58:28,000 --> 00:58:33,680
just need to kind of shape our existing systems in a slightly different way. But I think that

682
00:58:33,680 --> 00:58:38,240
in terms of how far language modeling will go, it's already gone way further than many people

683
00:58:38,240 --> 00:58:42,240
would have expected, right? I think that things like, and I think there's a lot of really interesting

684
00:58:42,240 --> 00:58:48,720
angles to poke in terms of how much does GPT2 understand physical world? Like, you know, you

685
00:58:48,720 --> 00:58:54,000
read a little bit about fire underwater in GPT2. So it's like, okay, maybe it doesn't quite understand

686
00:58:54,000 --> 00:58:59,680
what these things are. But at the same time, I think that you also see various things like smoke

687
00:58:59,680 --> 00:59:04,320
coming from flame and, you know, a bunch of these things that GPT2, it has no body, it has no physical

688
00:59:04,320 --> 00:59:11,520
experience, it's just statically read data. And I think that I think that if the answer is like,

689
00:59:11,520 --> 00:59:16,080
we don't know yet, then these questions, though, we're starting to be able to actually ask them

690
00:59:16,080 --> 00:59:20,480
to physical systems, the real systems that exist. And that's very exciting. Do you think, what's

691
00:59:20,480 --> 00:59:27,600
your intuition? Do you think if you just scale language modeling, like significantly scale,

692
00:59:27,600 --> 00:59:31,200
that reasoning can emerge from the same exact mechanisms?

693
00:59:31,200 --> 00:59:37,840
I think it's unlikely that if we just scale GPT2 that we'll have reasoning in the full fledged

694
00:59:37,840 --> 00:59:41,360
way. And I think that there's like, you know, the type signature is a little bit wrong, right?

695
00:59:41,360 --> 00:59:46,720
That like, there's something we do with, that we call thinking, right? Where we spend a lot of

696
00:59:46,720 --> 00:59:51,120
compute, like a variable amount of compute to get to better answers, right? I think a little bit

697
00:59:51,120 --> 00:59:57,920
harder, I get a better answer. And that that kind of type signature isn't quite encoded in a GPT,

698
00:59:58,560 --> 01:00:03,920
right? GPT will kind of like, it's spent a long time in it's like evolutionary history, baking

699
01:00:03,920 --> 01:00:08,400
and all this information getting very, very good at this predictive process. And then at runtime,

700
01:00:08,400 --> 01:00:14,320
I just kind of do one forward pass and, and I'm able to generate stuff. And so, you know, there

701
01:00:14,320 --> 01:00:19,120
might be small tweaks to what we do in order to get the type signature, right? For example, well,

702
01:00:19,120 --> 01:00:22,480
you know, it's not really one forward pass, right? You know, you generate symbol by symbol.

703
01:00:22,480 --> 01:00:26,800
And so maybe you generate like a whole sequence of thoughts and you only keep like the last bit

704
01:00:26,800 --> 01:00:30,960
or something. Right. But I think that at the very least, I would expect you have to make changes

705
01:00:30,960 --> 01:00:38,000
like that. Yeah, just exactly how we, you said think is the process of generating thought by

706
01:00:38,000 --> 01:00:42,960
thought in the same kind of way, like you said, keep the last bit, the thing that we converge

707
01:00:42,960 --> 01:00:47,200
towards. Yep. And I think there's, there's another piece, which is, which is interesting,

708
01:00:47,200 --> 01:00:52,240
which is this out of distribution generalization, right? That like thinking somehow lets us do that,

709
01:00:52,240 --> 01:00:56,000
right? That we have an experience of thing. And yet somehow we just kind of keep refining

710
01:00:56,000 --> 01:01:02,240
our mental model of it. This is again, something that feels tied to whatever reasoning is.

711
01:01:03,360 --> 01:01:08,000
And maybe it's a small tweak to what we do. Maybe it's many ideas and we'll take as many decades.

712
01:01:08,000 --> 01:01:14,560
Yeah. So the assumption there, generalization out of distribution is that it's possible to create

713
01:01:15,440 --> 01:01:20,880
new, new ideas. The pot, you know, it's possible that nobody's ever created any new ideas. And

714
01:01:20,880 --> 01:01:29,760
then we're scaling GPT-2 to GPT-20, you would, you would essentially generalize to all possible

715
01:01:29,760 --> 01:01:35,440
thoughts that us humans can have. Just to play devil's advocate. Right. Right. I mean, how many,

716
01:01:35,440 --> 01:01:39,920
how many new, new story ideas have we come up with since Shakespeare, right? Yeah, exactly.

717
01:01:40,320 --> 01:01:44,880
It's just all different forms of love and drama and so on. Okay.

718
01:01:45,680 --> 01:01:49,200
Not sure if you read Biddle Lesson, a recent blog post by Ray Sutton.

719
01:01:50,800 --> 01:01:55,680
He basically says something that echoes some of the ideas that you've been talking about, which is

720
01:01:56,720 --> 01:02:01,120
he says the biggest lesson that can be read from 70 years of AI research is that

721
01:02:01,120 --> 01:02:06,880
general methods that leverage computation are ultimately going to ultimately win out.

722
01:02:07,760 --> 01:02:14,160
Do you agree with this? So basically open AI in general about the ideas you're exploring,

723
01:02:14,160 --> 01:02:20,000
about coming up with methods, whether it's GPT-2 modeling, or whether it's open AI-5

724
01:02:20,000 --> 01:02:29,120
playing Dota, where a general method is better than a more fine-tuned, expert-tuned method.

725
01:02:29,680 --> 01:02:33,680
Yeah. So I think that, well, one thing that I think was really interesting about the reaction

726
01:02:33,680 --> 01:02:38,400
to that blog post was that a lot of people have read this as saying that compute is all that

727
01:02:38,400 --> 01:02:43,360
matters. And that's a very threatening idea, right? And I don't think it's a true idea either,

728
01:02:43,360 --> 01:02:46,880
right? It's very clear that we have algorithmic ideas that have been very important

729
01:02:46,880 --> 01:02:50,720
for making progress. And to really build AGI, you want to push as far as you can on the

730
01:02:50,720 --> 01:02:56,000
computational scale, and you want to push as far as you can on human ingenuity. And so I think

731
01:02:56,000 --> 01:02:59,280
you need both. But I think the way that you phrase the question is actually very good,

732
01:02:59,280 --> 01:03:04,640
right? That it's really about what kind of ideas should we be striving for? And absolutely,

733
01:03:04,640 --> 01:03:09,680
if you can find a scalable idea, you pour more compute into it, you pour more data into it,

734
01:03:09,680 --> 01:03:16,480
it gets better. Like that's the real Holy Grail. And so I think that the answer to the question,

735
01:03:16,480 --> 01:03:21,760
I think, is yes. That's really how we think about it, and that part of why we're excited

736
01:03:21,760 --> 01:03:26,800
about the power of deep learning and the potential for building AGI is because we look at the system

737
01:03:26,800 --> 01:03:32,720
that exists in the most successful AI systems, and we realize that you scale those up, they're

738
01:03:32,720 --> 01:03:36,800
going to work better. And I think that that scalability is something that really gives us

739
01:03:36,800 --> 01:03:41,360
hope for being able to build transformative systems. So I'll tell you, this is partially an

740
01:03:41,360 --> 01:03:46,880
emotional, you know, a thing that a response that people often have, if compute is so important

741
01:03:46,880 --> 01:03:51,680
for state of the art performance, you know, individual developers, maybe a 13 year old

742
01:03:51,680 --> 01:03:55,600
sitting somewhere in Kansas or something like that, you know, they're sitting, they might not

743
01:03:55,600 --> 01:04:01,600
even have a GPU and or maybe have a single GPU or 1080 or something like that. And there's this

744
01:04:01,600 --> 01:04:08,560
feeling like, well, how can I possibly compete or contribute to this world of AI if scale is so

745
01:04:08,560 --> 01:04:14,400
important? So if you can comment on that, and in general, do you think we need to also in the

746
01:04:14,400 --> 01:04:22,480
future focus on democratizing compute resources more more or as much as we democratize the algorithms?

747
01:04:22,480 --> 01:04:28,720
Well, so the way that I think about it is that there's this space of possible progress, right?

748
01:04:28,720 --> 01:04:33,520
There's a space of ideas and sort of systems that will work that will move us forward. And there's a

749
01:04:33,520 --> 01:04:37,760
portion of that space, and to some extent, an increasingly significant portion of that space

750
01:04:37,760 --> 01:04:44,080
that does just require massive compute resources. And for that, that I think that the answer is

751
01:04:44,080 --> 01:04:48,640
kind of clear, and that part of why we have the structure that we do is because we think it's

752
01:04:48,640 --> 01:04:52,720
really important to be pushing the scale and to be, you know, building these large clusters and

753
01:04:52,720 --> 01:04:58,000
systems. But there's another portion of the space that isn't about the large scale compute that are

754
01:04:58,000 --> 01:05:03,200
these ideas that, and again, I think that for the ideas to really be impactful and really shine,

755
01:05:03,200 --> 01:05:07,280
that they should be ideas that if you scale them up, would work way better than they do at small

756
01:05:07,280 --> 01:05:13,120
scale. But you can discover them without massive computational resources. And if you look at the

757
01:05:13,120 --> 01:05:18,080
history of recent developments, you think about things like the GAN or the VAE, that these are

758
01:05:18,080 --> 01:05:22,560
ones that I think you could come up with them without having, and, you know, in practice,

759
01:05:22,560 --> 01:05:26,320
people did come up with them without having massive, massive computational resources.

760
01:05:26,320 --> 01:05:32,560
Right. I just talked to Ian Goodfellow, but the thing is, the initial GAN produced pretty terrible

761
01:05:32,560 --> 01:05:39,840
results, right? So only because they're smart enough to know that this is quite surprising,

762
01:05:39,840 --> 01:05:45,840
it can generate anything that they know. Do you see a world, or is that too optimistic and dream

763
01:05:45,840 --> 01:05:52,320
or like, to imagine that the compute resources are something that's owned by governments and

764
01:05:52,320 --> 01:05:58,960
provided as utility? Actually, to some extent, this question reminds me of a blog post from

765
01:05:58,960 --> 01:06:03,600
one of my former professors at Harvard, this guy, Matt Welsh, who was a systems professor.

766
01:06:03,600 --> 01:06:08,080
I remember sitting in his tenure talk, right? And, you know, that he had literally just gotten

767
01:06:08,080 --> 01:06:15,600
tenure. He went to Google for the summer and then decided he wasn't going back to academia, right?

768
01:06:15,600 --> 01:06:20,240
And that kind of in his blog post, he makes this point that, look, as a systems researcher,

769
01:06:20,880 --> 01:06:25,040
that I come up with these cool system ideas, right? And I kind of build a little proof of concept.

770
01:06:25,040 --> 01:06:30,880
And the best thing I could hope for is that the people at Google or Yahoo, which was around at

771
01:06:30,880 --> 01:06:35,840
the time, will implement it and like, actually make it work at scale, right? That's like the

772
01:06:35,840 --> 01:06:38,480
dream for me, right? I built the little thing and they've turned it into the big thing that's

773
01:06:38,480 --> 01:06:44,800
actually working. And for him, he said, I'm done with that. I want to be the person who's actually

774
01:06:44,800 --> 01:06:49,680
doing building and deploying. And I think that there's a similar dichotomy here, right? I think

775
01:06:49,680 --> 01:06:55,040
that there are people who've really actually find value. And I think it is a valuable thing to do,

776
01:06:55,040 --> 01:06:59,120
to be the person who produces those ideas, right? Who builds the proof of concept. And yeah, you

777
01:06:59,120 --> 01:07:05,040
don't get to generate the coolest possible GAN images, but you invented the GAN, right? And so

778
01:07:05,040 --> 01:07:08,880
that there's a real trade-off there. And I think that that's a very personal choice,

779
01:07:08,880 --> 01:07:10,480
but I think there's value in both sides.

780
01:07:10,480 --> 01:07:16,240
So do you think creating AGI, something or some new models,

781
01:07:17,840 --> 01:07:23,440
we would see echoes of the brilliance even at the prototype level. So you would be able to develop

782
01:07:23,440 --> 01:07:30,000
those ideas without scale, the initial seeds. So take a look at, I always like to look at

783
01:07:30,000 --> 01:07:36,240
examples that exist, right? Look at real precedent. And so take a look at the June 2018 model that

784
01:07:36,240 --> 01:07:41,920
we released that we scaled up to turn to GPT2. And you can see that at small scale, it set some

785
01:07:41,920 --> 01:07:47,040
records, right? This was the original GPT. We actually had some cool generations that weren't

786
01:07:47,040 --> 01:07:52,960
nearly as amazing and really stunning as the GPT2 ones, but it was promising. It was interesting.

787
01:07:52,960 --> 01:07:57,440
And so I think it is the case that with a lot of these ideas that you see promise at small scale.

788
01:07:58,160 --> 01:08:02,000
But there is an asterisk here, a very big asterisk, which is sometimes we see

789
01:08:02,480 --> 01:08:07,600
behaviors that emerge that are qualitatively different from anything we saw at small scale.

790
01:08:07,600 --> 01:08:12,720
And that the original inventor of whatever algorithm looks at and says, I didn't think

791
01:08:12,720 --> 01:08:18,000
it could do that. This is what we saw in Dota, right? So PPO was created by John Shulman,

792
01:08:18,000 --> 01:08:24,640
who's a researcher here. And with Dota, we basically just ran PPO at massive, massive scale.

793
01:08:24,640 --> 01:08:29,520
And there's some tweaks in order to make it work, but fundamentally it's PPO at the core.

794
01:08:30,320 --> 01:08:36,880
And we were able to get this long-term planning, these behaviors to really play out

795
01:08:37,840 --> 01:08:42,560
on a time scale that we just thought was not possible. And John looked at that and was like,

796
01:08:42,560 --> 01:08:45,760
I didn't think it could do that. That's what happens when you're at three orders of magnitude,

797
01:08:45,760 --> 01:08:50,080
more scale than you tested at. Yeah, but it still has the same flavors of,

798
01:08:51,120 --> 01:08:57,840
you know, at least echoes of the expected billions. Although I suspect with GPT,

799
01:08:57,920 --> 01:09:04,160
scaled more and more, you might get surprising things. So yeah, you're right. It's interesting.

800
01:09:04,800 --> 01:09:10,080
It's difficult to see how far an idea will go when it's scaled. It's an open question.

801
01:09:10,960 --> 01:09:15,040
Well, so to that point with Dota and PPO, like, I mean, here's a very concrete one, right? It's

802
01:09:16,000 --> 01:09:19,120
actually one thing that's very surprising about Dota that I think people don't really pay that

803
01:09:19,120 --> 01:09:24,560
much attention to is the decree of generalization out of distribution that happens, right? That

804
01:09:24,560 --> 01:09:30,240
you have this AI that's trained against other bots for its entirety, the entirety of its existence.

805
01:09:30,240 --> 01:09:38,640
Sorry to take a step back. Can you talk through, you know, a story of Dota, a story of leading

806
01:09:38,640 --> 01:09:44,560
up to opening I-5 and that past, and what was the process of self-play and so on of training?

807
01:09:45,280 --> 01:09:47,280
Yeah, yeah, yeah. So with Dota. What is Dota?

808
01:09:48,480 --> 01:09:52,640
Dota is a complex video game. And we started training, we started trying to solve Dota

809
01:09:52,640 --> 01:09:57,680
because we felt like this was a step towards the real world relative to other games like Chess or Go,

810
01:09:57,680 --> 01:10:01,760
right? Those very cerebral games where you just kind of have this board of very discrete moves.

811
01:10:01,760 --> 01:10:05,520
Dota starts to be much more continuous time. So you have this huge variety of different

812
01:10:05,520 --> 01:10:11,040
actions that you have a 45-minute game with all these different units and it's got a lot of messiness

813
01:10:11,040 --> 01:10:16,640
to it that really hasn't been captured by previous games. And famously, all of the hard-coded bots

814
01:10:16,640 --> 01:10:20,400
for Dota were terrible, right? Just impossible to write anything good for it because it's so

815
01:10:20,400 --> 01:10:25,520
complex. And so this seemed like a really good place to push what's the state of the art in

816
01:10:25,520 --> 01:10:29,920
reinforcement learning. And so we started by focusing on the one versus one version of the game

817
01:10:29,920 --> 01:10:35,600
and we're able to solve that. We're able to beat the world champions and the learning,

818
01:10:35,600 --> 01:10:39,840
you know, the skill curve was this crazy exponential, right? And it was like constantly

819
01:10:39,840 --> 01:10:43,920
we were just scaling up, that we were fixing bugs and, you know, that you look at the skill

820
01:10:43,920 --> 01:10:47,760
curve and it was really a very, very smooth one. So it's actually really interesting to see how

821
01:10:47,760 --> 01:10:52,560
that like human iteration loop yielded very steady exponential progress.

822
01:10:52,560 --> 01:10:57,360
And to one side note, first of all, it's an exceptionally popular video game. The side

823
01:10:57,360 --> 01:11:03,120
effect is that there's a lot of incredible human experts at that video game. So the benchmark

824
01:11:03,120 --> 01:11:07,680
that you're trying to reach is very high. And the other, can you talk about the approach

825
01:11:07,680 --> 01:11:11,920
that was used initially and throughout training these agents to play this game?

826
01:11:11,920 --> 01:11:16,480
Yep. And so the person we used is self-play. And so you have two agents that don't know

827
01:11:16,480 --> 01:11:21,360
anything. They battle each other. They discover something a little bit good. And now they both

828
01:11:21,360 --> 01:11:24,960
know it. And they just get better and better and better without bound. And that's a really

829
01:11:24,960 --> 01:11:30,960
powerful idea, right? That we then went from the one versus one version of the game and scaled up

830
01:11:30,960 --> 01:11:34,720
to five versus five, right? So you think about kind of like with basketball, where you have this like

831
01:11:34,720 --> 01:11:41,280
team sport and you need to do all this coordination. And we were able to push the same idea, the same

832
01:11:41,280 --> 01:11:48,320
self-play to really get to the professional level at the full five versus five version of the game.

833
01:11:49,040 --> 01:11:55,040
And the things I think are really interesting here is that these agents, in some ways, they're

834
01:11:55,040 --> 01:11:59,600
almost like an insect-like intelligence, right? Where they have a lot in common with how an insect

835
01:11:59,600 --> 01:12:03,760
is trained, right? An insect kind of lives in this environment for a very long time, or the

836
01:12:03,760 --> 01:12:07,440
ancestors of this insect have been around for a long time and had a lot of experience that gets

837
01:12:07,440 --> 01:12:12,960
baked into this agent. And, you know, it's not really smart in the sense of a human, right? It's

838
01:12:12,960 --> 01:12:17,200
not able to go and learn calculus, but it's able to navigate its environment extremely well. It's

839
01:12:17,200 --> 01:12:22,480
able to handle unexpected things in the environment that's never seen before pretty well. And we see

840
01:12:22,480 --> 01:12:26,960
the same sort of thing with our Dota bots, right? That they're able to, within this game, they're

841
01:12:26,960 --> 01:12:31,520
able to play against humans, which is something that never existed in its evolutionary environment.

842
01:12:31,520 --> 01:12:36,480
Totally different play styles from humans versus the bots. And yet it's able to handle it extremely

843
01:12:36,480 --> 01:12:42,960
well. And that's something that I think was very surprising to us, was something that doesn't really

844
01:12:42,960 --> 01:12:48,080
emerge from what we've seen with PPO at smaller scale, right? And the kind of scale we're running

845
01:12:48,080 --> 01:12:54,160
this stuff at was, you know, I could take 100,000 CPU cores running with like hundreds of GPUs.

846
01:12:54,160 --> 01:13:00,320
It's probably about, you know, like, you know, something like hundreds of years of experience

847
01:13:00,320 --> 01:13:07,200
going into this bot every single real day. And so that scale is massive. And we start to see

848
01:13:07,200 --> 01:13:10,160
very different kinds of behaviors out of the algorithms that we all know and love.

849
01:13:10,800 --> 01:13:20,560
Dota, you mentioned beat the world expert 1v1. And then you weren't able to win 5v5 this year

850
01:13:21,200 --> 01:13:26,960
at the best players in the world. So what's the comeback story? What's, first of all,

851
01:13:27,040 --> 01:13:32,480
talk through that. That's an exceptionally exciting event. And what's the following months in this

852
01:13:32,480 --> 01:13:37,200
year look like? Yeah. Yeah. So one thing that's interesting is that, you know, we lose all the

853
01:13:37,200 --> 01:13:44,240
time because we play, so the Dota team at OpenAI, we play the bot against better players

854
01:13:44,240 --> 01:13:49,360
than our system all the time. Or at least we used to, right? Like, you know, the first time we lost

855
01:13:49,360 --> 01:13:54,000
publicly was we went up on stage at the international and we played against some of the best teams in

856
01:13:54,000 --> 01:13:58,480
the world. And we ended up losing both games, but we gave them a run for their money, right?

857
01:13:58,480 --> 01:14:02,880
That both games were kind of 30 minutes, 25 minutes, and they went back and forth, back and

858
01:14:02,880 --> 01:14:08,400
forth, back and forth. And so I think that really shows that we're at the professional level. And

859
01:14:08,400 --> 01:14:12,240
that kind of looking at those games, we think that the coin could have gone a different direction

860
01:14:12,240 --> 01:14:16,720
and it could have had some wins. That was actually very encouraging for us. And, you know, it's

861
01:14:16,720 --> 01:14:21,680
interesting because the international was at a fixed time, right? So we knew exactly what day

862
01:14:21,680 --> 01:14:24,560
we were going to be playing and we pushed as far as we could, as fast as we could.

863
01:14:25,520 --> 01:14:30,320
Two weeks later, we had a bot that had an 80% win rate versus the one that played at TI. So the

864
01:14:30,320 --> 01:14:34,320
march of progress, you know, you should think of as a snapshot rather than as an end state.

865
01:14:34,960 --> 01:14:40,960
And so in fact, we'll be announcing our finals pretty soon. I actually think that we'll announce

866
01:14:40,960 --> 01:14:47,360
our final match prior to this podcast being released. So there should be, we'll be playing,

867
01:14:48,160 --> 01:14:53,200
against the world champions. And, you know, for us, it's really less about like the way that we

868
01:14:53,200 --> 01:15:00,080
think about what's upcoming is the final milestone, the final competitive milestone for the project,

869
01:15:00,080 --> 01:15:06,160
right? That our goal in all of this isn't really about beating humans at Dota. Our goal is to push

870
01:15:06,160 --> 01:15:09,440
the state of the art and reinforcement learning. And we've done that, right? And we've actually

871
01:15:09,440 --> 01:15:13,840
learned a lot from our system. And that we have, you know, I think a lot of exciting next steps

872
01:15:13,840 --> 01:15:17,680
that we want to take. And so, you know, kind of as a final showcase of what we built, we're going

873
01:15:17,680 --> 01:15:23,280
to do this match. But for us, it's not really the successor failure to see, you know, do we have

874
01:15:23,280 --> 01:15:28,720
the coin flip go in our direction or against? Where do you see the field of deep learning

875
01:15:28,720 --> 01:15:36,720
heading in the next few years? Where do you see the work and reinforcement learning perhaps heading?

876
01:15:36,720 --> 01:15:43,520
And more specifically with OpenAI, all the exciting projects that you're working on,

877
01:15:44,400 --> 01:15:49,680
what does 2019 hold for you? Massive scale. Scale. I will put an actress on that and just say, you

878
01:15:49,680 --> 01:15:55,200
know, I think that it's about ideas plus scale, you need both. So that's a really good point. So

879
01:15:55,200 --> 01:16:03,360
the question, in terms of ideas, you have a lot of projects that are exploring different areas of

880
01:16:03,360 --> 01:16:08,240
intelligence. And the question is, when you, when you think of scale, do you think about

881
01:16:08,800 --> 01:16:13,760
growing the scale of those individual projects? Or do you think about adding new projects? And

882
01:16:13,760 --> 01:16:18,880
sorry to do that. And if you're thinking about adding new projects, or if you look at the past,

883
01:16:18,880 --> 01:16:23,360
what's the process of coming up with new projects and new ideas? So we really have a

884
01:16:23,360 --> 01:16:28,480
life cycle of project here. So we start with a few people just working on a small scale idea.

885
01:16:28,480 --> 01:16:32,080
And language is actually a very good example of this, that it was really, you know, one person

886
01:16:32,080 --> 01:16:37,040
here who was pushing on language for a long time. And then you get signs of life, right? And so this

887
01:16:37,040 --> 01:16:42,640
is like, let's say, you know, with, with the original GPT, we had something that was interesting.

888
01:16:42,640 --> 01:16:46,000
And we said, okay, it's time to scale this, right? It's time to put more people on it,

889
01:16:46,000 --> 01:16:51,040
put more computational resources behind it. And, and then we just kind of keep pushing and keep

890
01:16:51,040 --> 01:16:54,720
pushing. And the end state is something that looks like Dota or robotics, where you have a

891
01:16:54,720 --> 01:16:59,520
large team of, you know, 10 or 15 people that are running things at very large scale, and that

892
01:16:59,520 --> 01:17:04,880
you're able to really have material engineering, and, and, and, and, you know, sort of machine

893
01:17:04,880 --> 01:17:10,560
learning science coming together to make systems that work and get material results that just

894
01:17:10,560 --> 01:17:14,480
would have been impossible otherwise. So we do that whole life cycle, we've done it a number of

895
01:17:14,480 --> 01:17:20,400
times, you know, typically end to end, it's probably two, two years or so to do it, you know,

896
01:17:20,400 --> 01:17:23,360
the organization's been around for three years. So maybe we'll find that we also have longer

897
01:17:23,360 --> 01:17:30,800
life cycle projects. But, you know, we will work up to those. We have, so one team that we were

898
01:17:30,800 --> 01:17:34,640
actually just starting, Illy and I are kicking off a new team called the reasoning team. And

899
01:17:34,640 --> 01:17:40,080
that this is to really try to tackle, how do you get neural networks to reason? And we think that

900
01:17:40,080 --> 01:17:43,680
this will be a long term project. It's one that we're very excited about.

901
01:17:44,640 --> 01:17:51,840
In terms of reasoning, super exciting topic. What do you, what kind of benchmarks? What kind of

902
01:17:51,920 --> 01:17:56,560
tests of reasoning do you envision? What, what would, if you set back

903
01:17:57,840 --> 01:18:02,720
whatever drink and you would be impressed that this system is able to do something,

904
01:18:02,720 --> 01:18:03,600
what would that look like?

905
01:18:03,600 --> 01:18:04,800
Not theorem proving.

906
01:18:04,800 --> 01:18:10,480
Theorem proving. So some kind of logic and especially mathematical logic.

907
01:18:10,480 --> 01:18:13,760
I think so, right? And I think that there's, there's, there's kind of other problems that are

908
01:18:13,760 --> 01:18:18,640
dual to theorem proving in particular. You know, you think about programming, I think about even

909
01:18:18,640 --> 01:18:25,280
like security analysis of, of code, that these all kind of capture the same sorts of core reasoning

910
01:18:25,280 --> 01:18:28,000
and being able to do some out of distribution generalization.

911
01:18:29,280 --> 01:18:34,720
It would be quite exciting if open AI reasoning team was able to prove that P equals NP, that

912
01:18:34,720 --> 01:18:35,440
would be very nice.

913
01:18:36,000 --> 01:18:39,680
It would be very, very, very exciting, especially if it turns out that P equals NP,

914
01:18:39,680 --> 01:18:40,640
that'll be interesting too.

915
01:18:41,600 --> 01:18:45,760
It just, it would be ironic and humorous.

916
01:18:47,760 --> 01:18:54,160
So what problem stands out to you as the most exciting and challenging and impactful to the

917
01:18:54,160 --> 01:18:59,520
work for us as a community in general and for open AI this year? You mentioned reasoning,

918
01:18:59,520 --> 01:19:01,280
I think that's, that's a heck of a problem.

919
01:19:01,280 --> 01:19:04,160
Yeah. So I think reasoning is an important one. I think it's going to be hard to get good results

920
01:19:04,160 --> 01:19:09,680
in 2019. You know, again, just like we think about the life cycle takes time. I think for 2019,

921
01:19:09,680 --> 01:19:13,520
language modeling seems to be kind of on that ramp, right? It's at the point that we have a

922
01:19:13,520 --> 01:19:16,960
technique that works. We want to scale 100x, 1000x see what happens.

923
01:19:18,000 --> 01:19:20,560
Awesome. Do you think we're living in a simulation?

924
01:19:21,760 --> 01:19:25,920
I think it's, I think it's hard to have a real opinion about it. You know, it's actually

925
01:19:25,920 --> 01:19:30,000
interesting. I separate out things that I think can have like, you know, yield

926
01:19:30,000 --> 01:19:34,080
materially different predictions about the world from ones that are just kind of,

927
01:19:34,080 --> 01:19:37,840
you know, fun, fun to speculate about. And I kind of view simulation as more like,

928
01:19:37,840 --> 01:19:40,480
is there a flying teapot between Mars and Jupiter? Like,

929
01:19:41,760 --> 01:19:45,040
maybe, but it's a little bit hard to know what that would mean for my life.

930
01:19:45,040 --> 01:19:51,360
So there is something actionable. So some of the best work opening has done is in the field of

931
01:19:51,360 --> 01:19:57,680
reinforcement learning. And some of the success of reinforcement learning come from being able

932
01:19:57,680 --> 01:20:03,760
to simulate the problem you're trying to solve. So do you have a hope for reinforcement for the

933
01:20:03,760 --> 01:20:07,680
future of reinforcement learning and for the future of simulation? Like, whether it's we're

934
01:20:07,680 --> 01:20:13,440
talking about autonomous vehicles or any kind of system, do you see that scaling? So we'll be able

935
01:20:13,440 --> 01:20:20,720
to simulate systems and hence be able to create a simulator that echoes our real world and proving

936
01:20:20,720 --> 01:20:23,840
once and for all, even though you're denying it that we're living in a simulation?

937
01:20:24,880 --> 01:20:28,240
I feel like I've used that for questions, right? So, you know, kind of at the core there of like,

938
01:20:28,240 --> 01:20:33,520
can we use simulation for self-driving cars? Take a look at our robotic system, Dackel,

939
01:20:33,520 --> 01:20:39,200
right? That was trained in simulation using the Dota system, in fact, and it transfers to a physical

940
01:20:39,200 --> 01:20:43,600
robot. And I think everyone looks at our Dota system, they're like, okay, it's just a game. How

941
01:20:43,600 --> 01:20:47,040
are you ever going to escape to the real world? And the answer is, well, we did it with the physical

942
01:20:47,040 --> 01:20:51,120
robot, the no-wink program. And so I think the answer is simulation goes a lot further than you

943
01:20:51,120 --> 01:20:55,680
think if you apply the right techniques to it. Now, there's a question of, you know, are the

944
01:20:55,680 --> 01:21:01,360
beings in that simulation going to wake up and have consciousness? I think that one seems a lot

945
01:21:01,360 --> 01:21:05,280
harder to again reason about. I think that, you know, you really should think about like,

946
01:21:05,280 --> 01:21:09,680
where exactly does human consciousness come from in our own self-awareness? And, you know,

947
01:21:09,680 --> 01:21:12,640
is it just that like, once you have like a complicated enough neural net, do you have to

948
01:21:12,640 --> 01:21:18,640
worry about the agent's feeling pain? And, you know, I think there's like interesting speculation

949
01:21:18,640 --> 01:21:22,960
to do there. But, you know, again, I think it's a little bit hard to know for sure.

950
01:21:22,960 --> 01:21:27,600
Well, let me just keep with the speculation. Do you think to create intelligence, general

951
01:21:27,600 --> 01:21:34,560
intelligence, you need one consciousness and two a body? Do you think any of those elements are

952
01:21:34,560 --> 01:21:38,400
needed or is intelligence something that's orthogonal to those?

953
01:21:38,400 --> 01:21:42,800
I'll stick to the kind of like the non-grand answer first, right? So the non-grand answer

954
01:21:42,800 --> 01:21:46,800
is just to look at, you know, what are we already making work? You look at GPT2,

955
01:21:46,800 --> 01:21:50,560
a lot of people would have said that to even get these kinds of results, you need real world

956
01:21:50,560 --> 01:21:54,480
experience. You need a body, you need grounding. How are you supposed to reason about any of these

957
01:21:54,480 --> 01:21:58,000
things? How are you supposed to like even kind of know about smoke and fire and those things

958
01:21:58,000 --> 01:22:03,040
if you've never experienced them? And GPT2 shows that you can actually go way further

959
01:22:03,040 --> 01:22:10,480
than that kind of reasoning would predict. So I think that in terms of do we need consciousness,

960
01:22:10,480 --> 01:22:13,920
do we need a body? It seems the answer is probably not, right? That we probably just

961
01:22:13,920 --> 01:22:19,520
continue to push kind of the systems we have. They already feel general. They're not as competent

962
01:22:19,520 --> 01:22:24,240
or as general or able to learn as quickly as an AGI would. But, you know, they're at least like

963
01:22:24,560 --> 01:22:31,440
proto-AGI in some way, and they don't need any of those things. Now, let's move to the grand

964
01:22:31,440 --> 01:22:37,680
answer, which is, you know, if our neural nets conscious already, would we ever know? How can

965
01:22:37,680 --> 01:22:44,880
we tell, right? Here's where the speculation starts to become, you know, at least interesting or fun

966
01:22:44,880 --> 01:22:49,680
and maybe a little bit disturbing, depending on where you take it. But it certainly seems that

967
01:22:49,680 --> 01:22:53,920
when we think about animals, that there's some continuum of consciousness. You know, my cat,

968
01:22:53,920 --> 01:22:58,960
I think, is conscious in some way, right? You know, not as conscious as a human. And you could

969
01:22:58,960 --> 01:23:02,160
imagine that you could build a little consciousness meter, right? You pointed a cat, it gives you

970
01:23:02,160 --> 01:23:06,960
a little reading, pointed a human, it gives you much bigger reading. What would happen if you

971
01:23:06,960 --> 01:23:12,000
pointed one of those at a Dota neural net? And if you're training in this massive simulation,

972
01:23:12,000 --> 01:23:19,120
do the neural nets feel pain? You know, it becomes pretty hard to know that the answer is no. And

973
01:23:19,120 --> 01:23:23,920
it becomes pretty hard to really think about what that would mean if the answer were yes.

974
01:23:25,360 --> 01:23:30,160
And it's very possible, you know, for example, you could imagine that maybe the reason these humans

975
01:23:30,160 --> 01:23:35,520
are have consciousness is because it's a convenient computational shortcut, right? If you think about

976
01:23:35,520 --> 01:23:40,080
it, if you have a being that wants to avoid pain, which seems pretty important to survive in this

977
01:23:40,080 --> 01:23:45,680
environment and wants to like, you know, eat food, then that maybe the best way of doing it is to

978
01:23:45,680 --> 01:23:49,520
have a being that's conscious, right? That, you know, in order to succeed in the environment,

979
01:23:49,520 --> 01:23:53,360
you need to have those properties and how are you supposed to implement them? And maybe this

980
01:23:53,360 --> 01:23:57,920
consciousness is a way of doing that. If that's true, then actually, maybe we should expect that

981
01:23:57,920 --> 01:24:02,480
really competent reinforcement learning agents will also have consciousness. But, you know,

982
01:24:02,480 --> 01:24:05,920
that's a big if and I think there are a lot of other arguments that you can make in other directions.

983
01:24:06,640 --> 01:24:11,440
I think that's a really interesting idea that even GPT2 has some degree of consciousness,

984
01:24:11,440 --> 01:24:16,000
that something is actually not as crazy to think about. It's useful to think about

985
01:24:17,040 --> 01:24:20,880
as we think about what it means to create intelligence of a dog, intelligence of a cat,

986
01:24:22,560 --> 01:24:31,120
and the intelligence of a human. So last question, do you think we will ever fall in love like in

987
01:24:31,120 --> 01:24:36,240
the movie, her with an artificial intelligence system, or an artificial intelligence system

988
01:24:36,320 --> 01:24:43,680
falling out in love with a human? I hope so. If there's any better way to end it is on love.

989
01:24:43,680 --> 01:24:46,480
So, Greg, thanks so much for talking today. Thank you for having me.

