WEBVTT

00:00.000 --> 00:02.320
The following is a conversation with Jeff Hawkins.

00:02.320 --> 00:07.040
He's the founder of the Redwood Center for Theoretical Neuroscience in 2002 and New

00:07.040 --> 00:13.280
Menta in 2005. In his 2004 book titled On Intelligence and in the research before and

00:13.280 --> 00:18.640
after, he and his team have worked to reverse engineer the New York Cortex and propose artificial

00:18.640 --> 00:22.960
intelligence architectures approaches and ideas that are inspired by the human brain.

00:23.600 --> 00:28.880
These ideas include hierarchical temporal memory, HTM from 2004, and new work,

00:28.880 --> 00:36.080
the 1000s brain theory of intelligence from 2017, 18, and 19. Jeff's ideas have been an inspiration

00:36.080 --> 00:40.400
to many who have looked for progress beyond the current machine learning approaches,

00:40.400 --> 00:46.160
but they have also received criticism for lacking a body of empirical evidence supporting the models.

00:46.160 --> 00:50.720
This is always a challenge when seeking more than small incremental steps forward in AI.

00:51.360 --> 00:56.480
Jeff is a brilliant mind and many of the ideas he has developed and aggregated from neuroscience

00:56.480 --> 01:01.360
are worth understanding and thinking about. There are limits to deep learning as it is

01:01.360 --> 01:07.040
currently defined. Forward progress in AI is shrouded in mystery. My hope is that conversations

01:07.040 --> 01:13.280
like this can help provide an inspiring spark for new ideas. This is the Artificial Intelligence

01:13.280 --> 01:18.560
Podcast. If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter

01:18.560 --> 01:25.040
at Lex Friedman spelled F-R-I-D. And now here's my conversation with Jeff Hawkins.

01:49.520 --> 01:55.360
I also firmly believe that we will not be able to create fully intelligent machines until we

01:55.360 --> 02:00.560
understand how the human brain works. So I don't see those as separate problems. I think there's

02:00.560 --> 02:04.240
limits to what can be done with machine intelligence if you don't understand the principles by which

02:04.240 --> 02:10.000
the brain works. And so I actually believe that studying the brain is actually the fastest way

02:10.000 --> 02:15.520
to get to machine intelligence. And within that, let me ask the impossible question. How do you

02:15.520 --> 02:18.720
not define but at least think about what it means to be intelligent?

02:19.360 --> 02:24.400
So I didn't try to answer that question first. We said let's just talk about how the brain works

02:24.400 --> 02:29.120
and let's figure out how certain parts of the brain, mostly the neocortex, but some other parts too,

02:29.680 --> 02:34.480
the parts of the brain most associated with intelligence, and let's discover the principles

02:34.480 --> 02:39.920
by how they work. Because intelligence isn't just like some mechanism and it's not just some

02:39.920 --> 02:46.880
capabilities. It's like, okay, we don't even know where to begin on this stuff. And so now that we've

02:46.880 --> 02:51.520
made a lot of progress on this, after we've made a lot of progress on how the neocortex works,

02:51.520 --> 02:56.080
and we can talk about that, I now have a very good idea what's going to be required to make

02:56.080 --> 03:02.000
intelligent machines. I can tell you today, some of the things are going to be necessary, I believe,

03:02.000 --> 03:06.400
to create intelligent machines. Well, so we'll get there. We'll get to the neocortex and some of

03:06.400 --> 03:11.360
the theories of how the whole thing works. And you're saying, as we understand more and more

03:12.560 --> 03:17.280
about the neocortex, about our own human mind, we'll be able to start to more specifically

03:17.280 --> 03:21.760
define what it means to be intelligent. It's not useful to really talk about that until...

03:21.760 --> 03:26.880
I don't know if it's not useful. Look, there's a long history of AI, as you know. And there's

03:26.880 --> 03:34.480
been different approaches taken to it. And who knows, maybe they're all useful. So the good

03:34.480 --> 03:39.600
old fashioned AI, the expert systems, the current convolutional neural networks, they all have their

03:39.600 --> 03:45.520
utility. They all have a value in the world. But I would think almost everyone agree that none of

03:45.520 --> 03:52.960
them are really intelligent in a sort of a deep way that humans are. And so it's just the question

03:52.960 --> 03:57.760
is how do you get from where those systems were or are today to where a lot of people think we're

03:57.760 --> 04:04.880
going to go? And there's a big, big gap there, a huge gap. And I think the quickest way of bridging

04:04.880 --> 04:10.320
that gap is to figure out how the brain does that. And then we can sit back and look and say, oh,

04:10.320 --> 04:15.360
what are these principles that the brain works on are necessary and which ones are not? Clearly,

04:15.360 --> 04:18.640
we don't have to build this in... and intelligent machines aren't going to be built out of

04:20.480 --> 04:25.760
organic living cells. But there's a lot of stuff that goes on the brain that's going to be necessary.

04:25.760 --> 04:31.840
So let me ask maybe, before we get into the fun details, let me ask maybe a depressing or

04:31.840 --> 04:37.360
a difficult question. Do you think it's possible that we will never be able to understand how our

04:37.360 --> 04:44.960
brain works? That maybe there's aspects to the human mind, like we ourselves cannot introspectively

04:44.960 --> 04:49.840
get to the core, that there's a wall you eventually hit? Yeah, I don't believe that's the case.

04:50.720 --> 04:54.400
I have never believed that's the case. There's not been a single thing we've ever,

04:54.400 --> 04:58.640
human have ever put their minds to. We've said, oh, we reached the wall. We can't go any further.

04:58.640 --> 05:03.440
People keep saying that. People used to believe that about life, you know, Alain Vaital, right?

05:03.440 --> 05:06.400
There's like, what's the difference between living matter and non-living matter? Something

05:06.400 --> 05:12.720
special you never understand. We no longer think that. So there's no historical evidence that

05:12.720 --> 05:17.440
suggests this is the case. And I just never even consider that's a possibility. I would also say

05:17.520 --> 05:23.760
I would also say today we understand so much about the near cortex. We've made tremendous

05:23.760 --> 05:30.880
progress in the last few years that I no longer think of as an open question. The answers are

05:30.880 --> 05:36.000
very clear to me. The pieces we know we don't know are clear to me, but the framework is all there.

05:36.000 --> 05:39.840
And it's like, oh, okay, we're going to be able to do this. This is not a problem anymore.

05:39.840 --> 05:43.440
It just takes time and effort. But there's no mystery, big mystery anymore.

05:44.000 --> 05:52.240
So then let's get into it for people like myself who are not very well versed in the human brain,

05:52.800 --> 05:58.160
except my own. Can you describe to me at the highest level, what are the different parts

05:58.160 --> 06:04.000
of the human brain and then zooming in on the near cortex, the parts of the near cortex and so on,

06:04.000 --> 06:09.680
a quick overview? Yeah, sure. The human brain, we can divide it roughly into two parts.

06:10.640 --> 06:16.400
There's the old parts, lots of pieces, and then there's the new part. The new part is the near

06:16.400 --> 06:22.080
cortex. It's new because it didn't exist before mammals. The only mammals have a near cortex

06:22.080 --> 06:27.200
and in humans, in primates, it's very large. In the human brain, the near cortex occupies about

06:27.200 --> 06:33.520
70 to 75% of the volume of the brain. It's huge. And the old parts of the brain are,

06:34.720 --> 06:38.640
there's lots of pieces there. There's a spinal cord, and there's the brain stem,

06:38.640 --> 06:41.360
and the cerebellum, and the different parts of the basal ganglion and so on.

06:41.920 --> 06:46.240
In the old parts of the brain, you have the autonomic regulation like breathing and heart rate.

06:46.240 --> 06:50.560
You have basic behaviors. So like walking and running are controlled by the old parts of the

06:50.560 --> 06:54.160
brain. All the emotional centers of the brain are in the old part of the brain. So when you

06:54.160 --> 06:57.200
feel anger or hungry, lust or things like that, those are all in the old parts of the brain.

06:59.040 --> 07:04.480
And we associate with the near cortex all the things we think about as sort of high level

07:04.480 --> 07:12.880
perception and cognitive functions, anything from seeing and hearing and touching things to

07:12.880 --> 07:18.160
language to mathematics and engineering and science and so on. Those are all associated with the

07:18.160 --> 07:24.080
near cortex. And they're certainly correlated. Our abilities in those regards are correlated with

07:24.080 --> 07:29.120
the relative size of our near cortex compared to other mammals. So that's like the rough

07:29.200 --> 07:35.440
division. And you obviously can't understand the near cortex completely isolated, but you

07:35.440 --> 07:39.600
can understand a lot of it with just a few interfaces to the old parts of the brain.

07:40.320 --> 07:47.120
And so it gives you a system to study. The other remarkable thing about the near cortex

07:47.920 --> 07:54.720
compared to the old parts of the brain is the near cortex is extremely uniform. It's not visibly or

07:54.720 --> 08:01.200
anatomically or it's very it's like a I always like to say it's like the size of a dinner napkin

08:01.200 --> 08:06.240
about two and a half millimeters thick. And it looks remarkably the same everywhere. Everywhere

08:06.240 --> 08:10.400
you look in that two and a half millimeters is this detailed architecture. And it looks

08:10.400 --> 08:14.640
remarkably the same everywhere. And that's across species, a mouse versus a cat and a dog and a

08:14.640 --> 08:18.720
human. Where if you look at the old parts of the brain, there's lots of little pieces do specific

08:18.720 --> 08:23.200
things. So it's like the old parts of a brain involved like this is the part that controls

08:23.200 --> 08:26.000
heart rate. And this is the part that controls this and this is this kind of thing. And that's

08:26.000 --> 08:30.960
this kind of thing. And these evolve for eons of a long, long time. And they have those specific

08:30.960 --> 08:34.320
functions. And all of a sudden mammals come along and they got this thing called the near

08:34.320 --> 08:39.280
cortex. And it got large by just replicating the same thing over and over and over again.

08:39.280 --> 08:47.760
This is like, wow, this is incredible. So all the evidence we have. And this is an idea that was

08:47.760 --> 08:54.880
first articulated in a very cogent and beautiful argument by a guy named Vernon Malcastle in 1978,

08:54.880 --> 09:03.760
I think it was, that the neocortex all works on the same principle. So language, hearing,

09:03.760 --> 09:08.320
touch, vision, engineering, all these things are basically underlying or all built in the same

09:08.320 --> 09:12.960
computational substrate. They're really all the same problem. So the low level of the building

09:12.960 --> 09:17.440
blocks all look similar. Yeah. And they're not even that low level. We're not talking about like

09:17.440 --> 09:21.520
neurons. We're talking about this very complex circuit that exists throughout the neocortex is

09:22.160 --> 09:27.200
remarkably similar. It is, it's like, yes, you see variations of it here and there, more of the cell

09:27.200 --> 09:34.080
left and left and so on. But what Malcastle argued was it says, you know, if you take a section on

09:34.080 --> 09:40.160
neocortex, why is one a visual area and one is a auditory area? Or why is, and his answer was

09:41.040 --> 09:44.320
it's because one is connected to eyes and one is connected to ears.

09:45.280 --> 09:50.320
Literally, you mean just it's most closest in terms of number of connections to the sensor?

09:50.320 --> 09:55.200
Literally, if you took the optic nerve and attached it to a different part of the neocortex,

09:55.200 --> 10:00.320
that part would become a visual region. This actually, this experiment was actually done by

10:00.320 --> 10:06.640
McGonkiss Sir in developing, I think it was lemurs, I can't remember what it was, some animal.

10:06.640 --> 10:10.480
And, and there's a lot of evidence to this. You know, if you take a blind person, a person is

10:10.480 --> 10:16.000
born blind at birth. They, they're born with a visual neocortex. It doesn't

10:16.000 --> 10:21.440
may not get any input from the eyes, because of some congenital defect or something. And

10:22.480 --> 10:29.200
that region becomes, does something else. It picks up another task. So, and it's, it's,

10:29.200 --> 10:33.760
so it's this, it's this very complex thing. It's not like, oh, they're all built on neurons. No,

10:33.760 --> 10:39.520
they're all built in this very complex circuit. And, and somehow that circuit underlies everything.

10:40.160 --> 10:47.040
And so this is the, it's called the common cortical algorithm, if you will. Some scientists just find

10:47.040 --> 10:51.440
it hard to believe. And they just say, I can't believe that's true. But the evidence is overwhelming

10:51.440 --> 10:56.320
in this case. And so a large part of what it means to figure out how the brain creates intelligence

10:56.320 --> 11:02.240
and what is intelligence in the brain is to understand what that circuit does. If you can

11:02.240 --> 11:08.000
figure out what that circuit does, as amazing as it is, then you can, then you, then you understand

11:08.000 --> 11:13.760
what all these other cognitive functions are. So if you were to sort of put neural cortex outside

11:13.760 --> 11:18.400
of your book on intelligence, if you look, if you wrote a giant tome, a textbook on the neural

11:18.400 --> 11:25.760
cortex, and you look maybe a couple centuries from now, how much of what we know now would still

11:25.760 --> 11:30.720
be accurate two centuries from now. So how close are we in terms of understanding? So I'm going to,

11:30.720 --> 11:35.840
I have to speak from my own particular experience here. So I run a small research lab here. It's

11:36.560 --> 11:39.680
it's like any other research lab on the sort of the principal investigator, there's actually

11:39.680 --> 11:44.320
two of us and there's a bunch of other people. And this is what we do. We started the neural

11:44.320 --> 11:50.160
cortex, and we published our results and so on. So about three years ago, we had a real

11:50.160 --> 11:53.920
breakthrough in this, in this field. It's a tremendous breakthrough. We started, we've now

11:53.920 --> 12:00.320
published, I think, three papers on it. And so I have, I have a pretty good understanding of all

12:00.320 --> 12:06.800
the pieces and what we're missing. I would say that almost all the empirical data we've collected

12:06.800 --> 12:10.960
about the brain, which is enormous, if you don't know the neuroscience literature, it's just

12:10.960 --> 12:20.960
incredibly big. And it's, for the most part, all correct. It's facts and experimental results and

12:20.960 --> 12:26.720
measurements and all kinds of stuff. But none of that has been really assimilated into a theoretical

12:26.720 --> 12:33.440
framework. It's, it's data without, it's in the, in the language of Thomas Coon, the historian,

12:33.440 --> 12:38.080
it would be a sort of a pre-paradigm science, lots of data, but no way to fit it in together.

12:38.080 --> 12:40.800
I think almost all of that's correct. There's going to be some mistakes in there.

12:42.080 --> 12:47.200
And for the most part, there aren't really good cogent theories about how to put it together.

12:47.200 --> 12:51.120
It's not like we have two or three competing good theories, which ones are right and which ones

12:51.120 --> 12:54.640
are wrong. It's like, yeah, people just like scratching their heads, throwing things, you know,

12:54.640 --> 12:57.520
some people have given up on trying to like figure out what the whole thing does.

12:57.520 --> 13:04.160
In fact, there's very, very few labs that we do that focus really on theory and all this

13:04.160 --> 13:08.880
unassimilated data and trying to explain it. So it's not like we haven't, we've got it wrong.

13:08.880 --> 13:14.160
It's just that we haven't got it at all. So it's really, I would say, pretty early days

13:15.040 --> 13:20.160
in terms of understanding the fundamental theories, forces of the way our mind works.

13:20.160 --> 13:23.520
I don't think so. I would have said that's true five years ago.

13:25.280 --> 13:30.480
So as I said, we had some really big breakthroughs on this recently, and we started publishing papers

13:30.480 --> 13:36.720
on this. So we'll get to that. So I don't think it's, you know, I'm an optimist,

13:36.720 --> 13:40.320
and from where I sit today, most people would disagree with this, but from where I sit today,

13:40.320 --> 13:47.200
from what I know, it's not super early days anymore. The way these things go is it's not a

13:47.200 --> 13:50.720
linear path, right? You don't just start accumulating and get better and better and better.

13:50.720 --> 13:54.320
No, you got all the stuff you've collected. None of it makes sense. All these different

13:54.320 --> 13:57.520
things we just started around. And then you're going to have some breaking points all of a sudden,

13:57.520 --> 14:03.520
oh my God, now we got it right. That's how it goes in science. And I personally feel like we

14:03.520 --> 14:07.440
passed that little thing about a couple of years ago, all that big thing a couple of years ago.

14:07.440 --> 14:12.480
So we can talk about that. Time will tell if I'm right. But I feel very confident about it.

14:12.560 --> 14:18.560
That's the moment to say it on tape like this. At least very optimistic. So let's,

14:18.560 --> 14:25.840
before those few years ago, let's take a step back to HTM, the hierarchical temporal memory theory,

14:25.840 --> 14:29.360
which you first proposed on intelligence and went through a few different generations. Can

14:29.360 --> 14:34.480
you describe what it is, how it evolved through the three generations, since you first put it

14:34.480 --> 14:42.240
on paper? Yeah. So one of the things that neuroscientists just sort of missed for many,

14:42.240 --> 14:47.600
many years, and especially people who are thinking about theory, was the nature of time in the brain.

14:49.040 --> 14:53.520
Brains process information through time. The information coming into the brain is constantly

14:53.520 --> 14:59.280
changing. The patterns from my speech right now, if you're listening to it at normal speed,

15:00.000 --> 15:04.000
would be changing on your ears about every 10 milliseconds or so you'd have it change.

15:04.000 --> 15:08.160
This constant flow, when you look at the world, your eyes are moving constantly,

15:08.160 --> 15:12.480
three to five times a second, and the inputs completely. If I were to touch something like

15:12.480 --> 15:17.600
a coffee cup, as I move my fingers, the inputs change. So this idea that the brain works on

15:17.600 --> 15:22.720
time changing patterns is almost completely or was almost completely missing from a lot of

15:22.720 --> 15:26.480
the basic theories like fears of vision and so on. It's like, oh, no, we're going to put this

15:26.480 --> 15:30.880
image in front of you and flash it and say, what is it? convolutional neural networks work that

15:30.880 --> 15:36.560
way today, right? Classified this picture. But that's not what vision is like. Vision is this

15:36.560 --> 15:41.680
sort of crazy time-based pattern that's going all over the place and so is touch and so is hearing.

15:41.680 --> 15:46.000
So the first part of a hierarchical temporal memory was the temporal part. It's to say,

15:46.800 --> 15:50.320
you won't understand the brain, nor will you understand intelligent machines unless you're

15:50.320 --> 15:55.920
dealing with time-based patterns. The second thing was the memory component of it was, is to say

15:56.640 --> 16:04.160
that we aren't just processing input. We learn a model of the world. And the memory stands for

16:04.160 --> 16:08.080
that model. We have to, the point of the brain, the part of the neocortex, it learns a model of

16:08.080 --> 16:13.760
the world. We have to store things that are experiences in a form that leads to a model of

16:13.760 --> 16:17.280
the world. So we can move around the world. We can pick things up and do things and navigate and

16:17.280 --> 16:20.960
know how it's going on. So that's, that's what the memory referred to. And many people just,

16:20.960 --> 16:26.160
they were thinking about like certain processes without memory at all. They're just like processing

16:26.160 --> 16:32.160
things. And then finally, the hierarchical component was a reflection to that the neocortex,

16:32.160 --> 16:37.520
although it's just a uniform sheet of cells, different parts of it project to other parts,

16:37.520 --> 16:43.200
which project to other parts. And there is a sort of rough hierarchy in terms of that. So

16:43.840 --> 16:47.760
the hierarchical temporal memory is just saying, look, we should be thinking about the brain

16:47.760 --> 16:56.160
as time-based, you know, model memory-based and hierarchical processing. And, and that was a

16:56.160 --> 17:01.680
placeholder for a bunch of components that we would then plug into that. We still believe all

17:01.680 --> 17:07.120
those things I just said, but we now know so much more that I'm stopping to use the word

17:07.120 --> 17:11.440
hierarchical temporal memory yet because it's insufficient to capture the stuff we know. So

17:11.440 --> 17:16.800
again, it's not incorrect, but it's, I now know more and I would rather describe it more accurately.

17:16.800 --> 17:22.880
Yeah. So you're basically, we could think of HTM as emphasizing that there's three aspects

17:23.520 --> 17:27.680
of intelligence that are important to think about whatever the, whatever the eventual theory

17:27.680 --> 17:33.760
converges to. So in terms of time, how do you think of nature of time across different timescales?

17:33.760 --> 17:39.600
So you mentioned things changing, sensory inputs changing every 10, 20 minutes. What about

17:39.600 --> 17:43.840
every few minutes, every few months and years? Well, if you think about a neuroscience

17:44.320 --> 17:51.440
problem, the brain problem, neurons themselves can stay active for certain parts of time.

17:51.440 --> 17:54.160
They can, they're parts of the brain where they stay active for minutes, you know,

17:54.160 --> 18:02.320
so you could hold a certain perception or activity for a certain part of time, but not,

18:02.320 --> 18:08.960
most of them don't last that long. And so if you think about your thoughts or the activity neurons,

18:08.960 --> 18:12.880
if you're going to want to involve something that happened a long time ago, even just this morning,

18:12.880 --> 18:17.600
for example, the neurons haven't been active throughout that time. So you have to store that.

18:17.600 --> 18:22.320
So if I ask you, what did you have for breakfast today? That is memory. That is,

18:22.320 --> 18:25.840
you've built that into your model of the world now, you remember that. And that memory is in the

18:27.120 --> 18:34.640
synapses, it's basically in the formation of synapses. And so it's, you're sliding into what,

18:34.640 --> 18:38.560
you know, used to different timescales. There's timescales of which we are like

18:38.560 --> 18:42.080
understanding my language and moving about and seeing things rapidly and over time. That's the

18:42.080 --> 18:46.640
timescales of activities of neurons. But if you want to get in longer timescales, then it's more

18:46.640 --> 18:50.960
memory. And we have to invoke those memories to say, Oh, yes, well, now I can remember what

18:50.960 --> 18:57.440
I had for breakfast because I stored that someplace. I may forget it tomorrow, but I'd store it for

18:57.440 --> 19:07.040
now. So this memory also need to have the hierarchical aspect of reality is not just about

19:07.040 --> 19:09.600
concepts, it's also about time. Do you think of it that way?

19:09.600 --> 19:14.560
Uh, yeah, time is infused in everything. It's like, you really can't separate it out.

19:15.440 --> 19:20.800
If I ask you, what is the, what is your, you know, how's the brain learn a model of this coffee cup

19:20.800 --> 19:25.440
here? I have a coffee cup and then I met the coffee cup. I said, well, time is not an inherent

19:25.440 --> 19:29.520
property of this, of this, of the model I have with this cup, whether it's a visual model or

19:29.520 --> 19:34.800
tactile model. I can sense it through time, but the model itself doesn't really have much time.

19:34.800 --> 19:38.000
If I asked you, if I said, well, what is the model of my cell phone?

19:38.880 --> 19:43.360
My brain has learned a model of the cell phones. If you have a smartphone like this.

19:43.360 --> 19:47.920
And I said, well, this has time aspects to it. I have expectations when I turn it on,

19:47.920 --> 19:51.200
what's going to happen, what water, how long it's going to take to do certain things.

19:51.760 --> 19:56.400
If I bring up an app, what sequences. And so I have instant, it's like melodies in the world,

19:56.400 --> 20:01.440
you know, melody has a sense of time. So many things in the world move and act, and there's

20:01.440 --> 20:10.160
a sense of time related to them. Some don't, but most things do actually. So it's sort of infused

20:10.160 --> 20:14.560
throughout the models of the world. You build a model of the world, you're learning the structure

20:14.560 --> 20:19.440
of the objects in the world, and you're also learning how those things change through time.

20:20.640 --> 20:26.080
Okay, so it's, it really is just a fourth dimension that's infused deeply. And you have

20:26.080 --> 20:31.120
to make sure that your models of an intelligence incorporated. So

20:33.040 --> 20:36.960
like you mentioned, the state of neuroscience is deeply empirical, a lot of data collection.

20:37.680 --> 20:42.960
It's, you know, that's, that's where it is. You mentioned Thomas Kuhn, right?

20:42.960 --> 20:50.400
Yeah. And then you're proposing a theory of intelligence, and which is really the next step,

20:50.400 --> 20:59.520
the really important step to take. But why, why is HTM or what we'll talk about soon,

21:00.880 --> 21:08.480
the right theory? So is it more in this? Is it backed by intuition? Is it backed by

21:09.360 --> 21:14.960
evidence? Is it backed by a mixture of both? Is it kind of closer to where string theory is in physics,

21:15.520 --> 21:22.240
where there's mathematical components which show that, you know what, it seems that this,

21:22.880 --> 21:28.480
it fits together too well for it not to be true, which is what where string theory is. Is that

21:28.480 --> 21:33.040
where it's a mix of all those things, although definitely where we are right now, it's definitely

21:33.040 --> 21:38.320
much more on the empirical side than let's say string theory. The way this goes about, we're

21:38.320 --> 21:42.320
theorists, right? So we look at all this data and we're trying to come up with some sort of model

21:42.400 --> 21:48.480
that explains it, basically. And there's a, unlike string theory, there's this vast more

21:48.480 --> 21:56.560
amounts of empirical data here that I think that most physicists deal with. And so our challenge

21:56.560 --> 22:03.360
is to sort through that and figure out what kind of constructs would explain this. And when we have

22:03.360 --> 22:09.200
an idea, you come up with a theory of some sort, you have lots of ways of testing it. First of all,

22:09.920 --> 22:16.560
I am, you know, there are 100 years of assimilated, undesimulated empirical data from neuroscience.

22:16.560 --> 22:21.840
So we go back and read papers and we say, oh, did someone find this already? We can predict X,

22:21.840 --> 22:27.680
Y, and Z. And maybe no one's even talked about it since 1972 or something, but we go back and

22:27.680 --> 22:33.440
find that. And we say, oh, either it can support the theory or it can invalidate the theory. And

22:33.440 --> 22:36.880
we say, okay, we have to start over again. Oh, no, it's support. Let's keep going with that one.

22:37.840 --> 22:45.280
So the way I kind of view it, when we do our work, we come up, we look at all this empirical data,

22:45.280 --> 22:48.640
and it's what I call it as a set of constraints. We're not interested in something that's

22:48.640 --> 22:52.720
biologically inspired. We're trying to figure out how the actual brain works. So every piece

22:52.720 --> 22:56.880
of empirical data is a constraint on a theory. In theory, if you have the correct theory,

22:56.880 --> 23:02.960
it needs to explain every pin, right? So we have this huge number of constraints on the problem,

23:03.040 --> 23:07.120
which initially makes it very, very difficult. If you don't have many constraints,

23:07.120 --> 23:10.080
you can make up stuff all the day. You can say, oh, here's an answer. How you can do this,

23:10.080 --> 23:13.680
you can do that, you can do this. But if you consider all biology as a set of constraints,

23:13.680 --> 23:17.200
all neuroscience, a set of constraints, and even if you're working in one little part of

23:17.200 --> 23:21.280
the Neocortex, for example, there are hundreds and hundreds of constraints, these are empirical

23:21.280 --> 23:26.880
constraints, that it's very, very difficult initially to come up with a theoretical framework

23:26.880 --> 23:32.880
for that. But when you do, and it solves all those constraints at once, you have a high confidence

23:32.880 --> 23:38.560
that you got something close to correct. It's just mathematically almost impossible not to be.

23:39.120 --> 23:46.800
So that's the curse and the advantage of what we have. The curse is we have to meet all these

23:46.800 --> 23:53.120
constraints, which is really hard. But when you do meet them, then you have a great confidence

23:53.120 --> 23:58.640
that you've discovered something. In addition, then we work with scientific labs. So we'll say,

23:58.640 --> 24:03.200
oh, there's something we can't find, we can predict something, but we can't find it anywhere in the

24:03.200 --> 24:08.640
literature. So we will then, we have people we collaborated with, we'll say, sometimes they'll

24:08.640 --> 24:12.480
say, you know what, I have some collected data, which I didn't publish. But we can go back and

24:12.480 --> 24:16.880
look at it and see if we can find that, which is much easier than designing a new experiment,

24:16.880 --> 24:21.920
you know, new neuroscience experiments take a long time, years. So although some people

24:21.920 --> 24:28.720
are doing that now too. So, but between all of these things, I think it's a reasonable,

24:30.080 --> 24:34.400
actually a very, very good approach. We are blessed with the fact that we can test our theories

24:34.960 --> 24:39.280
out to Yang Yang here, because there's so much on a similar data. And we can also falsify our

24:39.280 --> 24:44.080
theories very easily, which we do often. So it's kind of reminiscent to whenever, whenever that

24:44.080 --> 24:50.720
was with Copernicus, you know, when you figure out that the sun's at the center of the solar

24:50.720 --> 24:56.240
system as opposed to Earth, the pieces just fall into place. Yeah, I think that's the general

24:57.200 --> 25:03.840
nature of the Ha moments is in Copernicus, it could be, you could say the same thing about Darwin.

25:05.120 --> 25:11.360
You could say the same thing about, you know, about the double helix, that people have been

25:11.360 --> 25:14.480
working on a problem for so long and have all this data and they can't make sense of it,

25:14.480 --> 25:19.280
they can't make sense of it. But when the answer comes to you and everything falls into place,

25:19.280 --> 25:26.400
it's like, oh, my gosh, that's it. That's got to be right. I asked both Jim Watson and Francis

25:26.400 --> 25:33.440
Crick about this. I asked them, you know, when you were working on trying to discover the structure

25:33.440 --> 25:40.720
of the double helix, and when you came up with the sort of the structure that ended up being correct,

25:42.400 --> 25:46.320
but it was sort of a guess, you know, it wasn't really verified yet. I said,

25:46.320 --> 25:51.360
did you know that it was right? And they both said, absolutely. So we absolutely knew it was

25:51.360 --> 25:55.520
right. And it doesn't matter if other people didn't believe it or not, we knew it was right,

25:55.520 --> 26:00.000
they'd get around to thinking it and agree with it eventually anyway. And that's the kind of thing

26:00.000 --> 26:05.760
you hear a lot with scientists who really are studying a difficult problem. And I feel that way

26:05.760 --> 26:11.520
too about our work. Have you talked to Crick or Watson about the problem you're trying to solve,

26:12.000 --> 26:19.840
the, of finding the DNA of the brain? Yeah. In fact, Francis Crick was very interested in this,

26:19.840 --> 26:25.280
in the latter part of his life. And in fact, I got interested in brains by reading an essay he wrote

26:25.280 --> 26:32.000
in 1979 called Thinking About the Brain. And that was when I decided I'm going to leave my

26:32.000 --> 26:36.640
profession of computers and engineering and become a neuroscientist, just reading that one essay from

26:36.640 --> 26:43.600
Francis Crick. I got to meet him later in life. I got to, I spoke at the Salk Institute and he

26:43.600 --> 26:49.600
was in the audience and then I had a tea with him afterwards. You know, he was interested in a

26:49.600 --> 26:57.520
different problem. He was focused on consciousness. Oh, the easy problem, right? Well, I think it's

26:57.520 --> 27:04.240
the red herring. And so we weren't really overlapping a lot there. Jim Watson, who's still alive,

27:05.200 --> 27:10.080
is also interested in this problem. And he was, when he was director of the Coltsman Harbor

27:10.080 --> 27:15.840
laboratories, he was really sort of behind moving in the direction of neuroscience there.

27:16.480 --> 27:21.920
And so he had a personal interest in this field. And I have met with him numerous times.

27:23.520 --> 27:29.200
And in fact, the last time was about a little bit over a year ago, I gave a talk at Coltsman

27:29.280 --> 27:37.920
Harbor Labs about the progress we were making in our work. And it was a lot of fun because

27:39.360 --> 27:42.240
he said, well, you wouldn't be coming here unless you had something important to say,

27:42.240 --> 27:48.960
so I'm going to go attend your talk. So he sat in the very front row. Next to him was the director

27:48.960 --> 27:52.480
of the lab, Bruce Stillman. So these guys are in the front row of this auditorium, right? So

27:52.480 --> 27:55.600
nobody else in the auditorium wants to sit in the front row because there's Jim Watson and there's

27:55.600 --> 28:05.040
the director. And I gave a talk and then I had dinner with Jim afterwards. But there's a great

28:05.040 --> 28:09.840
picture of my colleague, Subitai Amantik, where I'm up there sort of explaining the basics of

28:09.840 --> 28:14.560
this new framework we have. And Jim Watson is on the edge of his chair. He's literally on the edge

28:14.560 --> 28:20.480
of his chair, like intently staring up at the screen. And when he discovered the structure of

28:20.480 --> 28:26.560
DNA, the first public talk he gave was at Coltsman Harbor Labs. And there's a picture,

28:26.560 --> 28:30.720
there's a famous picture of Jim Watson standing at the whiteboard with a overrated thing pointing

28:30.720 --> 28:34.400
at something, holding at the double helix with his pointer. And it actually looks a lot like the

28:34.400 --> 28:37.760
picture of me. So there was a sort of funny, there's an area talking about the brain and there's Jim

28:37.760 --> 28:41.520
Watson staring up intently at it. And of course, there was, you know, whatever, 60 years earlier,

28:41.520 --> 28:46.000
he was standing, you know, pointing at the double helix. And it's one of the great discoveries in

28:46.000 --> 28:52.320
all of, you know, whatever, by all the science, all science DNA. So it's just the funny that

28:52.320 --> 28:57.280
there's echoes of that in your presentation. Do you think in terms of evolutionary timeline and

28:57.280 --> 29:07.360
history, the development of the neocortex was a big leap? Or is it just a small step? So like,

29:07.360 --> 29:12.720
if we ran the whole thing over again, from the from the birth of human of life on earth, how

29:12.720 --> 29:16.320
likely would we develop the mechanism of the neocortex? Okay, well, those are two separate

29:16.320 --> 29:22.240
questions. One is, was it a big leap? And one was how likely it is. Okay, they're not necessarily

29:22.240 --> 29:27.200
related. Maybe correlated. And we don't really have enough data to make a judgment about that.

29:28.000 --> 29:32.560
I would say definitely was a big leap. And I can tell you why I think I don't think it was just

29:32.560 --> 29:38.320
another incremental step. I'll get that moment. I don't really have any idea how likely it is.

29:38.320 --> 29:44.080
If we look at evolution, we have one data point, which is earth, right? Life formed on earth billions

29:44.080 --> 29:48.880
of years ago, whether it was introduced here, or it created here, or someone introduced it,

29:48.880 --> 29:54.160
we don't really know, but it was here early. It took a long, long time to get to multicellular life.

29:55.040 --> 30:02.640
And then from multicellular life, it took a long, long time to get the neocortex. And we've only

30:02.640 --> 30:09.040
had the neocortex for a few hundred thousand years. So that's like nothing. Okay, so is it

30:09.040 --> 30:13.760
likely? Well, certainly isn't something that happened right away on earth. And there were

30:13.760 --> 30:17.520
multiple steps to get there. So I would say it's probably not going to something that would happen

30:17.520 --> 30:22.400
instantaneously on other planets that might have life. It might take several billion years on average.

30:23.040 --> 30:26.880
Is it likely? I don't know, but you'd have to survive for several billion years to find out.

30:27.840 --> 30:36.560
Probably. Is it a big leap? Yeah, I think it is a qualitative difference in all other evolutionary

30:36.560 --> 30:42.800
steps. I can try to describe that if you'd like. Sure. In which way? Yeah, I can tell you how.

30:43.760 --> 30:49.600
Pretty much, let's start with a little preface. Many of the things that humans are able to do

30:50.320 --> 31:00.400
do not have obvious survival advantages precedent. We could create music. Is there

31:00.400 --> 31:05.280
a really survival advantage to that? Maybe, maybe not. What about mathematics? Is there a real

31:05.280 --> 31:10.480
survival advantage to mathematics? You can stretch it. You can try to figure these things out.

31:12.080 --> 31:17.920
But mostly evolutionary history, everything had immediate survival advantages to write.

31:18.640 --> 31:26.240
So I'll tell you a story, which I like. It may not be true. But the story goes as follows.

31:29.040 --> 31:32.320
Organisms have been evolving since the beginning of life here on Earth,

31:33.680 --> 31:37.680
adding this sort of complexity onto that and this sort of complexity onto that. And the brain itself

31:38.240 --> 31:43.360
is evolved this way. In fact, there's an old part, an older part, an older, older part to the

31:43.360 --> 31:47.360
brain that kind of just keeps calming on new things and we keep adding capabilities. And we

31:47.360 --> 31:53.840
got to the neocortex. Initially, it had a very clear survival advantage in that it produced better

31:53.840 --> 31:59.200
vision and better hearing and better touch and maybe, you know, say, so on. But what I think

31:59.200 --> 32:05.440
happens is that evolution took a mechanism, and this is in our recent theory, but it took a

32:05.440 --> 32:10.320
mechanism that evolved a long time ago for navigating in the world, for knowing where you are.

32:10.320 --> 32:14.320
These are the so-called grid cells and place cells of an old part of the brain.

32:15.120 --> 32:22.160
And it took that mechanism for building maps of the world and knowing where you are on those

32:22.160 --> 32:27.920
maps and how to navigate those maps and turns it into a sort of a slimmed down, idealized version

32:27.920 --> 32:33.120
of it. And that idealized version could now apply to building maps of other things, maps of

32:33.680 --> 32:39.200
coffee cups and maps of phones, maps of, you know, concepts, concepts, yes, and not just almost,

32:39.200 --> 32:44.400
exactly. And so you, and it just started replicating this stuff, right? You just think

32:44.400 --> 32:50.480
more and more and more. So we went from being sort of dedicated purpose neural hardware to solve

32:50.480 --> 32:56.000
certain problems that are important to survival to a general purpose neural hardware that could

32:56.000 --> 33:03.360
be applied to all problems. And now it's escaped the orbit of survival. It's, we are now able to

33:03.360 --> 33:12.240
apply it to things which we find enjoyment, you know, but aren't really clearly survival

33:12.240 --> 33:18.000
characteristics. And that it seems to only have happened in humans to the large extent.

33:19.120 --> 33:24.960
And so that's what's going on where we sort of have, we've sort of escaped the gravity of

33:24.960 --> 33:30.960
evolutionary pressure in some sense in the New York cortex. And it now does things which

33:31.520 --> 33:36.000
that are really interesting, discovering models of the universe, which may not really help us,

33:36.000 --> 33:40.800
doesn't matter. How does it help us surviving knowing that there might be multiverses or

33:40.800 --> 33:45.440
that there might be, you know, the age of the universe or how do, you know, various stellar

33:45.440 --> 33:50.320
things occur? It doesn't really help us survive at all. But we enjoy it. And that's what happened.

33:50.320 --> 33:57.200
Or at least not in the obvious way, perhaps it is required. If you look at the entire

33:57.200 --> 34:01.280
universe in an evolutionary way, it's required for us to do interplanetary travel and therefore

34:01.280 --> 34:04.320
survive past our own fun. But you know, let's not get too quick.

34:04.320 --> 34:10.000
Yeah, but you know, evolution works at one time frame and survival, if you think of survival

34:10.000 --> 34:15.600
of the phenotype, survival of the individual, what you're talking about there is spans well

34:15.600 --> 34:22.400
beyond that. So there's no genetic, I'm not transferring any genetic traits to my children

34:23.280 --> 34:25.760
that are going to help them survive better on Mars.

34:25.760 --> 34:31.200
Right. Totally different mechanism. So let's get into the new, as you've mentioned,

34:31.200 --> 34:35.280
this idea, I don't know if you have a nice name, 1000.

34:35.280 --> 34:37.200
I would call it the thousand brain theory of intelligence.

34:37.200 --> 34:44.000
I like it. So can you talk about the this idea of spatial view of concepts and so on?

34:44.000 --> 34:48.480
Yeah. So can I just describe sort of the there's an underlying core discovery,

34:49.200 --> 34:52.320
which then everything comes from that that's a very simple.

34:53.120 --> 34:59.520
This is really what happened. We were deep into problems about understanding how we build models

34:59.520 --> 35:05.280
of stuff in the world and how we make predictions about things. And I was holding a coffee cup just

35:05.280 --> 35:10.960
like this in my hand. And I had my finger was touching the side, my index finger. And then I

35:10.960 --> 35:16.560
moved it to the top. And I was going to feel the rim at the top of the cup. And I asked myself

35:16.560 --> 35:21.280
a very simple question. I said, well, first of all, let's say I know that my brain predicts

35:21.280 --> 35:25.040
what it's going to feel before it touches it. You can just think about it and imagine it.

35:25.920 --> 35:29.200
And so we know that the brain's making predictions all the time. So the question is,

35:29.200 --> 35:32.880
what does it take to predict that? Right. And there's a very interesting answer.

35:33.440 --> 35:36.720
First of all, it says the brain has to know it's touching a coffee cup. It has to have

35:36.720 --> 35:42.320
a model of a coffee cup and needs to know where the finger currently is on the cup relative to

35:42.320 --> 35:46.320
the cup. Because when I make a movement, it needs to know where it's going to be on the cup

35:46.320 --> 35:52.000
after the movement is completed relative to the cup. And then it can make a prediction about

35:52.000 --> 35:56.320
what it's going to sense. So this told me that the neocortex, which is making this prediction,

35:56.320 --> 36:00.960
needs to know that it's sensing it's touching a cup. And it needs to know the location of

36:00.960 --> 36:05.040
my finger relative to that cup in a reference frame of the cup. It doesn't matter where the

36:05.040 --> 36:09.200
cup is relative to my body. It doesn't matter its orientation. None of that matters. It's

36:09.200 --> 36:13.040
where my finger is relative to the cup, which tells me then that the neocortex

36:13.600 --> 36:18.400
has a reference frame that's anchored to the cup. Because otherwise, I wouldn't be able to

36:18.400 --> 36:22.480
say the location and I wouldn't be able to predict my new location. And then we quickly,

36:22.480 --> 36:26.560
very instantly, you can say, well, every part of my skin could touch this cup. And therefore,

36:26.560 --> 36:29.760
every part of my skin is making predictions and every part of my skin must have a reference frame

36:30.800 --> 36:38.240
that it's using to make predictions. So the big idea is that throughout the neocortex,

36:38.320 --> 36:47.520
everything is being stored and referenced in reference frames. You can think of them like

36:47.520 --> 36:51.760
XYZ reference frames, but they're not like that. We know a lot about the neural mechanisms for

36:51.760 --> 36:56.640
this. But the brain thinks in reference frames. And as an engineer, if you're an engineer,

36:56.640 --> 37:01.200
this is not surprising. You'd say, if I were to build a CAD model of the coffee cup, well,

37:01.200 --> 37:04.800
I would bring it up in some CAD software and I would assign some reference frame and say,

37:04.880 --> 37:09.920
this features at this location and so on. But the idea that this is occurring throughout

37:09.920 --> 37:18.640
the neocortex everywhere, it was a novel idea. And then a zillion things fell into place after

37:18.640 --> 37:22.960
that. A zillion. So now we think about the neocortex as processing information quite

37:22.960 --> 37:26.480
differently than we used to do it. We used to think about the neocortex as processing

37:26.480 --> 37:30.880
sensory data and extracting features from that sensory data and then extracting features from

37:30.880 --> 37:35.680
the features, very much like a deep learning network does today. But that's not how the brain

37:35.680 --> 37:41.360
works at all. The brain works by assigning everything, every input, everything to reference

37:41.360 --> 37:46.160
frames. And there are thousands, hundreds of thousands of them active at once in your neocortex.

37:47.520 --> 37:51.600
It's a surprising thing to think about. But once you sort of internalize this, you understand

37:51.600 --> 37:59.680
that it explains almost all the mysteries we've had about the structure. So one of the consequences

38:00.240 --> 38:05.600
of that is that every small part of the neocortex, say a millimeter square and there's 150,000 of

38:05.600 --> 38:10.240
those. So it's about 150,000 square millimeters. If you take every little square millimeter of the

38:10.240 --> 38:15.120
cortex, it's got some input coming into it and it's going to have reference frames where it's

38:15.120 --> 38:21.440
assigning that input to and each square millimeter can learn complete models of objects. So what do

38:21.440 --> 38:25.840
I mean by that? If I'm touching the coffee cup, well, if I just touch it in one place, I can't

38:25.840 --> 38:30.320
learn what this coffee cup is because I'm just feeling one part. But if I move it around the cup

38:30.880 --> 38:34.800
and touch it in different areas, I can build up a complete model of the cup because I'm now

38:34.800 --> 38:38.560
filling in that three dimensional map, which is the coffee cup, I can say, oh, what am I feeling

38:38.560 --> 38:41.760
at all these different locations? That's the basic idea. It's more complicated than that.

38:42.880 --> 38:48.000
But so through time, and we talked about time earlier, through time, even a single column,

38:48.000 --> 38:51.520
which is only looking at or a single part of the cortex, it's only looking at a small part of the

38:51.520 --> 38:56.960
world can build up a complete model of an object. And so if you think about the part of the brain,

38:56.960 --> 39:01.440
which is getting input from all my fingers, so there's spread across the top of your head here,

39:01.440 --> 39:06.160
this is the somatosensory cortex, there's columns associated with all the different areas of my

39:06.160 --> 39:12.720
skin. And what we believe is happening is that all of them are building models of this cup,

39:12.720 --> 39:17.680
every one of them, or things, not all building all, not every column or every part of the

39:17.680 --> 39:23.760
cortex builds models of everything. But they're all building models of something. And so you have,

39:23.760 --> 39:29.040
so when I touch this cup with my hand, there are multiple models of the cup being invoked.

39:29.040 --> 39:33.280
If I look at it with my eyes, there are again many models of the cup being invoked because each part

39:33.280 --> 39:39.360
of the visual system, the brain doesn't process an image, that's a misleading idea. It's just like

39:39.360 --> 39:42.320
your fingers touching the cup, so different parts of my retina are looking at different parts of the

39:42.320 --> 39:48.160
cup. And thousands and thousands of models of the cup are being invoked at once. And they're all

39:48.160 --> 39:50.960
voting with each other trying to figure out what's going on. So that's why we call it the

39:50.960 --> 39:55.520
Thousand Brains Theory of Intelligence because there isn't one model of a cup. There are thousands

39:55.520 --> 39:59.200
of models of this cup. There are thousands of models of your cell phone and about cameras and

39:59.200 --> 40:03.680
microphones and so on. It's a distributed modeling system, which is very different than

40:03.680 --> 40:08.160
what people have thought about it. So that's a really compelling and interesting idea. I have two

40:08.160 --> 40:12.560
first questions. So one, on the ensemble part of everything coming together, you have these

40:12.560 --> 40:18.800
Thousand Brains, how do you know which one has done the best job of forming the cup?

40:18.800 --> 40:23.840
Great question. Let me try to explain. There's a problem that's known in neuroscience called the

40:23.840 --> 40:29.280
sensor fusion problem. And so the idea is something like, oh, the image comes from the eye. There's

40:29.280 --> 40:34.480
a picture on the retina and it gets projected to the neocortex. Oh, by now it's all sped out all

40:34.480 --> 40:38.960
over the place and it's kind of squirrely and distorted and pieces are all over the, you know,

40:38.960 --> 40:43.600
it doesn't look like a picture anymore. When does it all come back together again? Right?

40:43.600 --> 40:48.560
Or you might say, well, yes, but I also, I also have sounds or touches associated with the cup.

40:48.560 --> 40:52.560
So I'm seeing the cup and touching the cup. How do they get combined together again?

40:52.560 --> 40:56.480
So this is called the sensor fusion problem as if all these disparate parts have to be brought

40:56.480 --> 41:02.400
together into one model someplace. That's the wrong idea. The right idea is that you get all

41:02.400 --> 41:06.560
these guys voting. There's auditory models of the cup, there's visual models of the cup,

41:06.560 --> 41:11.840
there's tactile models of the cup. In the vision system, there might be ones that are more focused

41:11.840 --> 41:15.360
on black and white, one's version on color. It doesn't really matter. There's just thousands and

41:15.360 --> 41:20.560
thousands of models of this cup and they vote. They don't actually come together in one spot.

41:20.560 --> 41:25.360
Just literally think of it this way. Imagine you have each column about the size of a little

41:25.360 --> 41:29.840
piece of spaghetti, okay? Like a two and a half millimeters tall and about a millimeter in white.

41:29.840 --> 41:34.480
They're not physical like, but you can think of them that way. And each one's trying to guess

41:34.480 --> 41:38.400
what this thing is we're touching. Now they can, they can do a pretty good job if they're allowed

41:38.400 --> 41:43.360
to move over time. So I can reach my hand into a black box and move my finger around an object

41:43.360 --> 41:48.160
and if I touch enough space, it's like, okay, I know what it is. But often we don't do that.

41:48.160 --> 41:51.520
Often I can just reach and grab something with my hand all at once and I get it. Or

41:51.520 --> 41:55.760
if I had to look through the world through a straw, so I'm only invoking one little column,

41:55.760 --> 41:58.880
I can only see part of something because I have to move the straw around. But if I open my eyes

41:58.880 --> 42:03.120
to see the whole thing at once. So what we think is going on is all these little pieces of spaghetti

42:03.120 --> 42:07.200
if you have all these little columns in the cortex or all trying to guess what it is that they're

42:07.200 --> 42:12.320
sensing. They'll do a better guess if they have time and can move over time. So if I move my eyes

42:12.320 --> 42:17.760
or move my fingers, but if they don't, they have a, they have a poor guess. It's a, it's a probabilistic

42:17.760 --> 42:22.000
guess of what they might be touching. Now imagine they can post their probability

42:22.880 --> 42:26.400
at the top of little piece of spaghetti, each one of them says, I think, and it's not really a

42:26.400 --> 42:30.800
probability distribution. It's more like a set of possibilities in the brain. It doesn't work

42:30.800 --> 42:35.360
as a probability distribution. It works as more like what we call a union. You could say, and one

42:35.360 --> 42:40.800
column says, I think it could be a coffee cup soda can or a water bottle. And another column says,

42:40.800 --> 42:47.840
I think it could be a coffee cup or, you know, telephone or camera or whatever. Right. And all

42:47.840 --> 42:51.680
these guys are saying what they think it might be. And there's these long range connections in

42:51.760 --> 42:57.600
certain layers in the cortex. So there's some layers in the, some cells types in each column

42:57.600 --> 43:02.720
send the projections across the brain. And that's the voting occurs. And so there's a simple

43:02.720 --> 43:07.040
associative memory mechanism. We've, we've described this in a recent paper and we've modeled this

43:08.560 --> 43:14.160
that says they can all quickly settle on the only or the one best answer for all of them.

43:14.800 --> 43:19.040
If there is a single best answer, they all vote and say, yep, it's got to be the coffee cup.

43:19.040 --> 43:22.800
And at that point, they all know it's a coffee cup. And at that point, everyone acts as if it's

43:22.800 --> 43:25.920
the coffee cup. They know it's a coffee, even though I've only seen one little piece of this

43:25.920 --> 43:30.080
world. I know it's a coffee cup I'm touching or I'm seeing or whatever. And so you can think of

43:30.080 --> 43:33.920
all these columns are looking at different parts and different places, different sensory input,

43:33.920 --> 43:37.840
different locations. They're all different. But this layer that's doing the voting,

43:39.040 --> 43:43.280
that's, it solidifies. It's just like it crystallizes and says, oh, we all know what we're

43:43.280 --> 43:47.520
doing. And so you don't bring these models together in one model, you just vote and there's a

43:47.520 --> 43:52.480
crystallization of the vote. Great. That's at least a compelling way to think about

43:54.240 --> 44:02.000
about the way you form a model of the world. Now, you talk about a coffee cup. Do you see this

44:02.000 --> 44:06.160
as far as I understand that you were proposing this as well, that this extends to much more than

44:06.160 --> 44:14.080
coffee cups? Yeah, it does. Or at least the physical world that expands to the world of concepts.

44:14.080 --> 44:19.040
Yeah, it does. And well, the first, the primary phase of evidence for that is that

44:19.040 --> 44:23.760
the regions of the neocortex that are associated with language or high-level thought or mathematics

44:23.760 --> 44:27.520
or things like that, they look like the regions of the neocortex that process vision and hearing

44:27.520 --> 44:34.080
and touch. They don't look any different or they look only marginally different. And so one would

44:34.080 --> 44:39.280
say, well, if Vernon Mountcastle, who proposed that all the parts of the neocortex do the same

44:39.280 --> 44:43.840
thing, if he's right, then the parts that are doing language or mathematics or physics

44:44.480 --> 44:47.600
are working on the same principle. They must be working on the principle of reference frames.

44:48.480 --> 44:55.040
So that's a little odd thought. But of course, we had no prior idea how these things happen,

44:55.040 --> 45:01.840
so let's go with that. And in our recent paper, we talked a little bit about that. I've been

45:01.840 --> 45:06.960
working on it more since. I have better ideas about it now. I'm sitting here very confident

45:07.040 --> 45:10.240
that that's what's happening. And I can give you some examples to help you think about that.

45:11.120 --> 45:14.240
It's not that we understand it completely, but I understand it better than I've described it

45:14.240 --> 45:22.800
in any paper so far. But we did put that idea out there. It's a good place to start. And the

45:22.800 --> 45:26.720
evidence would suggest it's how it's happening. And then we can start tackling that problem one

45:26.720 --> 45:30.080
piece at a time. What does it mean to do high-level thought? What does it mean to do language? How

45:30.080 --> 45:37.040
would that fit into a reference framework? I don't know if you could tell me if there's a

45:37.040 --> 45:42.320
connection, but there's an app called Anki that helps you remember different concepts.

45:42.320 --> 45:47.920
And they talk about a memory palace that helps you remember completely random concepts by

45:48.880 --> 45:52.320
trying to put them in a physical space in your mind and putting them next to each other.

45:52.320 --> 45:56.560
It's called the method of loci. For some reason, that seems to work really well.

45:57.520 --> 46:00.480
Now that's a very narrow kind of application of just remembering some facts.

46:00.480 --> 46:03.040
But that's a very, very telling one.

46:03.040 --> 46:09.040
Yes, exactly. So this seems like you're describing a mechanism why this seems to work.

46:09.040 --> 46:13.680
Yeah. So basically the way what we think is going on is all things you know,

46:13.680 --> 46:19.600
all concepts, all ideas, words, everything, you know, are stored in reference frames.

46:20.400 --> 46:26.480
And so if you want to remember something, you have to basically navigate through a reference

46:26.480 --> 46:30.320
frame the same way a rat navigates to a maven, the same way my finger rat navigates to this

46:30.320 --> 46:36.000
coffee cup. You are moving through some space. And so if you have a random list of things you

46:36.000 --> 46:40.480
would ask to remember, by assigning them to a reference frame, you've already know very well

46:40.480 --> 46:45.040
to see your house, right? And the idea of the method of loci is you can say, okay, in my lobby,

46:45.040 --> 46:48.160
I'm going to put this thing. And then the bedroom, I put this one. I go down the hall,

46:48.160 --> 46:52.240
I put this thing. And then you want to recall those facts or recall those things. You just walk

46:52.240 --> 46:56.560
mentally, you walk through your house. You're mentally moving through a reference frame

46:56.560 --> 47:00.640
that you already had. And that tells you there's two things that are really important about that.

47:00.640 --> 47:05.520
It tells us the brain prefers to store things in reference frames. And that the method of

47:05.520 --> 47:11.520
recalling things or thinking, if you will, is to move mentally through those reference frames.

47:11.520 --> 47:14.720
You could move physically through some reference frames, like I could physically move through

47:14.720 --> 47:17.840
the reference frame of this coffee cup. I can also mentally move through the reference frame

47:17.840 --> 47:23.920
of the coffee cup, imagining me touching it. But I can also mentally move my house. And so now we

47:23.920 --> 47:30.000
can ask yourself, are all concepts stored this way? There was some recent research using human

47:30.000 --> 47:34.960
subjects in fMRI. And I'm going to apologize for not knowing the name of the scientists who did

47:34.960 --> 47:41.840
this. But what they did is they put humans in this fMRI machine, which is one of these imaging

47:41.840 --> 47:47.680
machines. And they gave the humans tasks to think about birds. So they had different types of birds

47:47.680 --> 47:52.560
and birds that looked big and small and long necks and long legs, things like that. And what they

47:52.560 --> 47:59.040
could tell from the fMRI was a very clever experiment. Get to tell when humans were thinking

47:59.040 --> 48:05.520
about the birds, that the birds, the knowledge of birds was arranged in a reference frame,

48:05.520 --> 48:10.240
similar to the ones that are used when you navigate in a room. These are called grid cells.

48:10.240 --> 48:14.320
And there are grid cell-like patterns of activity in the neocortex when they do this.

48:15.280 --> 48:20.880
So it's a very clever experiment. And what it basically says is that even when you're thinking

48:20.880 --> 48:24.560
about something abstract, and you're not really thinking about it as a reference frame,

48:24.560 --> 48:28.160
it tells us the brain is actually using a reference frame. And it's using the same neural

48:28.160 --> 48:32.800
mechanisms. These grid cells are the basic same neural mechanisms that we propose that grid cells,

48:32.800 --> 48:37.280
which exist in the old part of the brain, the entirionic cortex, that that mechanism

48:37.280 --> 48:42.560
is now similar mechanism is used throughout the neocortex. It's the same nature of preserve this

48:42.560 --> 48:47.680
interesting way of creating reference frames. And so now they have empirical evidence that

48:47.680 --> 48:52.160
when you think about concepts like birds, that you're using reference frames that are built on

48:52.160 --> 48:56.880
grid cells. So that's similar to the method of loci, but in this case, the birds are related so

48:56.880 --> 49:00.320
that makes they create their own reference frame, which is consistent with bird space.

49:01.040 --> 49:04.720
And when you think about something, you go through that, you can make the same example.

49:04.720 --> 49:09.920
Let's take a math mathematics. Let's say you want to prove a conjecture. What is a conjecture?

49:09.920 --> 49:16.080
Conjecture is a statement you believe to be true, but you haven't proven it. And so it might be an

49:16.080 --> 49:21.120
equation. I want to show that this is equal to that. And you have some places you start with,

49:21.120 --> 49:25.040
you say, well, I know this is true, and I know this is true. And I think that maybe to get to

49:25.040 --> 49:29.840
the final proof, I need to go through some intermediate results. But I believe it's happening

49:30.640 --> 49:36.960
is literally these equations or these points are assigned to a reference frame, a mathematical

49:36.960 --> 49:41.200
reference frame. And when you do mathematical operations, a simple one might be multiply or

49:41.200 --> 49:45.280
divide, but you might be able to transform or something else, that is like a movement in the

49:45.280 --> 49:51.680
reference frame of the math. And so you're literally trying to discover a path from one location to

49:51.680 --> 49:58.160
another location in a space of mathematics. And if you can get to these intermediate results,

49:58.160 --> 50:02.000
then you know your map is pretty good. And you know you're using the right operations.

50:02.960 --> 50:08.000
Much of what we think about is solving hard problems, is designing the correct reference

50:08.000 --> 50:12.400
frame for that problem, figuring out how to organize the information, and what behaviors

50:12.400 --> 50:18.880
I want to use in that space to get me there. Yeah, so if you dig in an idea of this reference

50:18.880 --> 50:23.920
frame, whether it's the math, you start a set of axioms to try to get to proving the conjecture.

50:24.640 --> 50:29.840
Can you try to describe, maybe take a step back, how you think of the reference frame in that

50:29.840 --> 50:37.120
context? Is it the reference frame that the axioms are happy in? Is it the reference frame

50:37.120 --> 50:43.040
that might contain everything? Is it a changing thing? So you have many, many reference frames.

50:43.040 --> 50:46.480
I mean, in fact, the way the theory, the 1000 brain theory of intelligence says that every

50:46.480 --> 50:50.480
single thing in the world has its own reference frame. So every word has its own reference

50:50.480 --> 50:55.680
frames. And we can talk about this, the mathematics work out, this is no problem for neurons to do

50:55.680 --> 50:59.920
this. But how many reference frames does the coffee cup have? Well, it's on a table. Remember,

51:00.720 --> 51:06.880
let's say you ask how many reference frames could the column in my finger that's touching the coffee

51:06.880 --> 51:10.800
cup have? Because there are many, many copy, there are many, many models of coffee cups. So

51:10.800 --> 51:14.320
the coffee, there is no one model of coffee cup, there are many models of coffee cup. And you

51:14.320 --> 51:18.560
could say, well, how many different things can my finger learn? Is this is the question you want

51:18.560 --> 51:23.520
to ask? Imagine, I say every concept, every idea, everything you've ever know about that you can

51:23.520 --> 51:29.200
say, I know that thing, it has a reference frame associated with it. And what we do when we build

51:29.200 --> 51:34.320
composite objects, we can we assign reference frames to points, another reference frame. So

51:34.320 --> 51:39.440
my coffee cup has multiple components to it. It's got a limb, it's got a cylinder, it's got a handle.

51:40.640 --> 51:44.720
And those things that have their own reference frames, and they're assigned to a master reference

51:44.720 --> 51:48.640
frame, which is called this cup. And now I have this mental logo on it. Well, that's something

51:48.640 --> 51:52.400
that exists elsewhere in the world. It's its own thing. So it has its own reference frame. So we

51:52.400 --> 51:58.640
now have to say, how can I assign the mental logo reference frame onto the cylinder or onto the coffee

51:58.640 --> 52:05.520
cup? So it's all, we talked about this in the paper that came out in December of this last year.

52:06.800 --> 52:10.480
The idea of how you can assign reference frames to reference frames, how neurons could do this.

52:10.480 --> 52:15.600
So my question is, even though you mentioned reference frames a lot, I almost feel it's really

52:15.600 --> 52:21.440
useful to dig into how you think of what a reference frame is. I mean, it was already helpful for me

52:21.440 --> 52:27.760
to understand that you think of reference frames as something there is a lot of. Okay, so let's just

52:27.760 --> 52:32.320
say that we're going to have some neurons in the brain, not many actually, 10,000, 20,000 are going

52:32.320 --> 52:36.160
to create a whole bunch of reference frames. What does it mean? What is a reference frame?

52:37.120 --> 52:41.120
First of all, these reference frames are different than the ones you might have

52:41.120 --> 52:45.520
be used to. We know lots of reference things. For example, we know the Cartesian coordinates,

52:45.520 --> 52:50.800
x, y, z, that's a type of reference frame. We know longitude and latitude, that's a different type

52:50.800 --> 52:59.040
of reference frame. If I look at a printed map, you might have columns a through m and rows,

52:59.040 --> 53:02.640
you know, one through 20, that's a different type of reference frame. It's kind of a Cartesian

53:02.720 --> 53:07.840
reference frame. The interesting thing about the reference frames in the brain, we know this

53:07.840 --> 53:12.160
because these have been established through neuroscience, studying the entorhonic cortex.

53:12.160 --> 53:16.800
So I'm not speculating here. Okay, this is known neuroscience in an old part of the brain. The

53:16.800 --> 53:23.760
way these cells create reference frames, they have no origin. So what is more like you have a point,

53:24.320 --> 53:30.720
a point in some space, and you given a particular movement, you can then tell what the next point

53:30.720 --> 53:36.800
should be. And you can then tell what the next point would be and so on. You can use this to

53:38.160 --> 53:43.360
calculate how to get from one point to another. So how do I get from my house to my home, or how

53:43.360 --> 53:48.400
do I get my finger from the side of my cup to the top of the cup? How do I get from the axioms to

53:49.600 --> 53:56.080
the conjecture? So it's a different type of reference frame. And if you want, I can describe

53:56.080 --> 53:59.600
in more detail, I can paint a picture how you might want to think about that.

53:59.600 --> 54:07.040
It's really helpful to think it's something you can move through. Yeah. But is it helpful to think

54:07.040 --> 54:12.160
of it as spatial in some sense, or is there something? No, it's definitely spatial. It's spatial

54:12.160 --> 54:16.400
in a mathematical sense. How many dimensions? Can it be a crazy number of dimensions? Well,

54:16.400 --> 54:19.520
that's an interesting question. In the old part of the brain, the entorhonic cortex,

54:20.160 --> 54:24.640
they studied rats. And initially, it looks like, oh, this is just two dimensional. It's like the

54:24.640 --> 54:29.120
rat is in some box and a maze or whatever. And they know whether the rat is using these two

54:29.120 --> 54:34.800
dimensional reference frames and know where it is in the maze. We said, okay, what about bats?

54:35.360 --> 54:40.000
That's a mammal. And they fly in three dimensional space. How do they do that? They seem to know

54:40.000 --> 54:45.840
where they are. So this is a current area of active research. And it seems like somehow the

54:45.840 --> 54:52.240
neurons in the entorhonic cortex can learn three dimensional space. We just, two members of our

54:52.240 --> 54:58.880
team, along with Ilef Fett from MIT, just released a paper this literally last week,

54:59.440 --> 55:05.840
it's on bioarchive, where they show that you can, if you, the way these things work, and I won't get

55:05.840 --> 55:11.920
unless you want to, I won't get into the detail, but grid cells can represent any n dimensional

55:11.920 --> 55:18.000
space. It's not inherently limited. You can think of it this way. If you had two dimensional,

55:18.000 --> 55:21.600
the way it works is you had a bunch of two dimensional slices. That's the way these things

55:21.600 --> 55:26.160
work. There's a whole bunch of two dimensional models. And you can just, you can slice up any

55:26.160 --> 55:30.880
n dimensional space and with two dimensional projections. So, and you could have one dimensional

55:30.880 --> 55:34.640
models. It does. So there's, there's nothing inherent about the mathematics about the way

55:34.640 --> 55:40.000
the neurons do this, which, which constrained the dimensionality of the space, which I think was

55:40.000 --> 55:44.880
important. So obviously, I have a three dimensional map of this cup, maybe it's even more than that.

55:44.880 --> 55:49.520
I don't know. But it's clearly three dimensional map of the cup. I don't just have a projection of

55:49.520 --> 55:53.920
the cup. But when I think about birds or when I think about mathematics, perhaps it's more than

55:53.920 --> 56:02.000
three dimensions. Who knows? So in terms of each individual column building up more and more

56:02.000 --> 56:08.320
information over time, do you think that mechanism is well understood in your mind? You've proposed

56:08.320 --> 56:14.640
a lot of architectures there. Is that a key piece or is it, is the big piece, the thousand brain

56:15.360 --> 56:18.880
theory of intelligence, the ensemble of it all? Well, I think they're both big. I mean,

56:18.960 --> 56:22.960
clearly the concept as a theorist, the concept that's most exciting, right?

56:22.960 --> 56:26.240
A high level concept. A high level concept. This is a totally new way of thinking about

56:26.240 --> 56:30.640
how the near characteristics work. So that is appealing. It has all these ramifications.

56:30.640 --> 56:34.880
And with that, as a framework for how the brain works, you can make all kinds of predictions

56:34.880 --> 56:38.400
and solve all kinds of problems. Now we're trying to work through many of these details right now.

56:38.400 --> 56:42.560
Okay. How do the neurons actually do this? Well, it turns out, if you think about grid cells and

56:42.560 --> 56:46.240
place cells in the old parts of the brain, there's a lot that's known about them, but there's still

56:46.240 --> 56:50.000
some mysteries. There's a lot of debate about exactly the details, how these work and what are

56:50.000 --> 56:54.000
the signs. And we have that still, that same level of detail, that same level of concern.

56:54.000 --> 57:00.480
What we spend here, most of our time doing is trying to make a very good list of the things

57:00.480 --> 57:05.520
we don't understand yet. That's the key part here. What are the constraints? It's not like,

57:05.520 --> 57:09.760
oh, this seems to work. We're done. Now it's like, okay, it kind of works, but these are other things

57:09.760 --> 57:14.880
we know what has to do and it's not doing those yet. I would say we're well on the way here.

57:14.960 --> 57:22.240
We're not done yet. There's a lot of trickiness to this system, but the basic principles about how

57:22.240 --> 57:26.080
different layers in the neocortex are doing much of this, we understand,

57:27.120 --> 57:29.840
but there's some fundamental parts that we don't understand as well.

57:29.840 --> 57:35.680
So what would you say is one of the harder open problems or one of the ones that have been bothering

57:35.680 --> 57:40.800
you, keeping you up at night the most? Oh, well, right now, this is a detailed thing that wouldn't

57:40.800 --> 57:44.960
apply to most people, okay. But you want me to answer that question? Yeah, please.

57:46.080 --> 57:50.880
We've talked about, as if, oh, to predict what you're going to sense on this coffee cup, I need

57:50.880 --> 57:54.960
to know where my finger is going to be on the coffee cup. That is true, but it's insufficient.

57:56.240 --> 58:00.240
Think about my finger touches the edge of the coffee cup. My finger can touch it at different

58:00.240 --> 58:07.520
orientations. I can rotate my finger around here, and that doesn't change. I can make that prediction

58:08.160 --> 58:12.160
and somehow, so it's not just the location. There's an orientation component of this as well.

58:13.200 --> 58:16.880
This is known in the old part of the brain too. There's things called head direction cells, which

58:16.880 --> 58:21.520
way the rat is facing. It's the same kind of basic idea. So if my finger were a rat,

58:22.160 --> 58:25.040
you know, in three dimensions, I have a three-dimensional orientation,

58:25.600 --> 58:28.640
and I have a three-dimensional location. If I was a rat, I would have a,

58:28.640 --> 58:31.920
I think of it as a two-dimensional location, a two-dimensional orientation, a one-dimensional

58:32.000 --> 58:38.720
orientation, like just which way is it facing. So how the two components work together, how it is

58:38.720 --> 58:48.480
that I combine orientation, the orientation of my sensor, as well as the location, is a tricky

58:48.480 --> 58:55.680
problem, and I think I've made progress on it. So at a bigger version of that, the perspective is

58:55.680 --> 59:01.360
super interesting, but super specific. Yeah, I warned you. No, no, that's really good, but

59:02.080 --> 59:07.840
there's a more general version of that. Do you think context matters? The fact that we are

59:08.400 --> 59:16.160
in a building in North America, that we, in the day and age where we have mugs, I mean,

59:17.120 --> 59:20.640
there's all this extra information that you bring to the table about

59:21.360 --> 59:24.560
everything else in the room that's outside of just the coffee cup. Of course it is.

59:24.560 --> 59:30.240
How does it get connected, do you think? Yeah, and that is another really interesting question.

59:30.240 --> 59:35.280
I'm going to throw that under the rubric or the name of attentional problems. First of all,

59:35.280 --> 59:39.920
we have this model. I have many, many models. And also the question, does it matter because...

59:39.920 --> 59:44.640
Well, it matters for certain things. Of course it does. Maybe what we think of that as a coffee

59:44.640 --> 59:48.320
cup in another part of the world is viewed as something completely different, or maybe our

59:48.320 --> 59:52.480
logo, which is very benign in this part of the world, it means something very different in

59:52.480 --> 59:59.200
another part of the world. So those things do matter. I think the way to think about this

59:59.200 --> 01:00:03.120
the following, or one way to think about it, is we have all these models of the world.

01:00:04.640 --> 01:00:08.800
And we model everything. And as I said earlier, I kind of snuck it in there.

01:00:08.800 --> 01:00:14.880
Our models are actually, we build composite structure. So every object is composed of other

01:00:14.880 --> 01:00:18.960
objects, which are composed of other objects, and they become members of other objects. So this

01:00:18.960 --> 01:00:23.840
room has chairs and a table and a room and walls and so on. Now we can just arrange these things

01:00:23.840 --> 01:00:30.560
in a certain way. You go, oh, that's in the romantic conference room. And what we do is,

01:00:30.560 --> 01:00:36.240
when we go around the world and we experience the world, by walking into a room, for example,

01:00:36.240 --> 01:00:39.200
the first thing I do is say, oh, I'm in this room. Do I recognize the room?

01:00:39.200 --> 01:00:44.000
Then I can say, oh, look, there's a table here. And by attending to the table,

01:00:44.000 --> 01:00:47.120
I'm then assigning this table in the context of the room. Then I say, oh, on the table,

01:00:47.120 --> 01:00:50.960
there's a coffee cup. Oh, and on the table, there's a logo. And in the logo,

01:00:50.960 --> 01:00:54.320
there's the word Nemento. On the look in the logo, there's the letter E. On the look,

01:00:54.320 --> 01:00:59.600
it has an unusual surf. And it doesn't actually, but pretend that there's a surf.

01:00:59.600 --> 01:01:06.240
So the point is your attention is kind of drilling deep in and out of these nested structures.

01:01:07.360 --> 01:01:11.040
And I can pop back up and I can pop back down. I can pop back up and I can pop back down. So

01:01:11.600 --> 01:01:15.520
when I attend to the coffee cup, I haven't lost the context of everything else,

01:01:16.080 --> 01:01:18.720
but it's sort of, there's this sort of nested structure.

01:01:18.720 --> 01:01:21.840
So the attention filters the reference frame formation

01:01:22.880 --> 01:01:24.240
for that particular period of time?

01:01:24.240 --> 01:01:28.240
Yes. It basically, moment to moment, you attend the subcomponents,

01:01:28.240 --> 01:01:30.240
and then you can attend the subcomponents to subcomponents.

01:01:30.240 --> 01:01:31.360
You can move up and down that.

01:01:31.360 --> 01:01:33.360
You can move up and down that. We do that all the time. You're not even,

01:01:34.080 --> 01:01:37.120
now that I'm aware of it, I'm very conscious of it. But until,

01:01:38.160 --> 01:01:41.120
but most people don't even think about this, you know, you just walk in the room and you

01:01:41.120 --> 01:01:44.480
don't say, oh, I looked at the chair and I looked at the board and looked at that word on the board

01:01:44.480 --> 01:01:46.960
and I looked over here. What's going on? Right?

01:01:46.960 --> 01:01:49.920
So what percentage of your day are you deeply aware of this?

01:01:49.920 --> 01:01:52.720
And what part can you actually relax and just be Jeff?

01:01:52.720 --> 01:01:54.320
Me personally, like my personal day.

01:01:54.320 --> 01:01:54.480
Yeah.

01:01:55.360 --> 01:01:57.680
Unfortunately, I'm afflicted with too much of the former.

01:02:01.200 --> 01:02:02.640
Unfortunately, they are unfortunate.

01:02:02.640 --> 01:02:04.400
Yeah. So you don't think it's useful?

01:02:04.400 --> 01:02:05.760
Oh, it is useful. Totally useful.

01:02:06.640 --> 01:02:09.120
I think about this stuff almost all the time.

01:02:09.120 --> 01:02:13.760
And one of my primary ways of thinking is when I'm asleep at night,

01:02:13.760 --> 01:02:18.080
I always wake up in the middle of the night and then I stay awake for at least an hour with my

01:02:18.080 --> 01:02:22.480
eyes shut in sort of a half-sleep state thinking about these things. I come up with answers to

01:02:22.480 --> 01:02:26.720
problems very often in that sort of half-sleeping state. I think about on my bike ride, I think

01:02:26.720 --> 01:02:31.520
about on walks. I'm just constantly thinking about this. I have to almost schedule time

01:02:32.400 --> 01:02:37.120
to not think about this stuff because it's very, it's mentally taxing.

01:02:38.320 --> 01:02:42.000
When you're thinking about this stuff, are you thinking introspectively, like almost

01:02:42.080 --> 01:02:45.520
taking a step outside of yourself and trying to figure out what is your mind doing right now?

01:02:45.520 --> 01:02:50.640
I do that all the time, but that's not all I do. I'm constantly observing myself.

01:02:50.640 --> 01:02:55.120
So as soon as I started thinking about grid cells, for example, and getting into that,

01:02:55.120 --> 01:02:58.240
I started saying, oh, well, grid cells can have my place of sense in the world.

01:02:58.240 --> 01:03:01.440
That's where you know where you are. And it's interesting, we always have a sense of where

01:03:01.440 --> 01:03:05.840
we are unless we're lost. And so I started at night when I got up to go to the bathroom,

01:03:05.840 --> 01:03:09.200
I would start trying to do it completely with my eyes closed all the time and I would test my

01:03:09.200 --> 01:03:14.320
sense of grid cells. I would walk five feet and say, okay, I think I'm here. Am I really there?

01:03:14.320 --> 01:03:17.840
What's my error? And then I would calculate my error again and see how the errors can accumulate.

01:03:17.840 --> 01:03:20.320
So even something as simple as getting up in the middle of the night to go to the bathroom,

01:03:20.320 --> 01:03:25.040
I'm testing these theories out. It's kind of fun. I mean, the coffee cup is an example of that too.

01:03:25.600 --> 01:03:31.600
So I think I find that these sort of everyday introspections are actually quite helpful.

01:03:32.800 --> 01:03:38.080
It doesn't mean you can ignore the science. I mean, I spend hours every day reading ridiculously

01:03:38.080 --> 01:03:43.840
complex papers. That's not nearly as much fun, but you have to sort of build up those constraints

01:03:44.480 --> 01:03:48.880
and the knowledge about the field and who's doing what and what exactly they think is happening here.

01:03:48.880 --> 01:03:52.160
And then you can sit back and say, okay, let's try to have pieces all together.

01:03:53.280 --> 01:03:58.320
Let's come up with some, you know, I'm very, in this group here, people, they know they do this,

01:03:58.320 --> 01:04:01.280
I do this all the time. I come in with these introspective ideas and say, well,

01:04:01.280 --> 01:04:03.920
did we ever thought about this? Now watch, well, let's all do this together.

01:04:04.880 --> 01:04:10.240
And it's helpful. It's not, as long as you don't, if all you did was that,

01:04:10.240 --> 01:04:14.880
then you're just making up stuff, right? But if you're constraining it by the reality of

01:04:14.880 --> 01:04:20.800
the neuroscience, then it's really helpful. So let's talk a little bit about deep learning and

01:04:20.800 --> 01:04:28.880
the successes in the applied space of neural networks and ideas of training model on data

01:04:28.960 --> 01:04:33.120
and these simple computational units and you're on artificial neurons

01:04:33.920 --> 01:04:41.280
that would back propagation of statistical ways of being able to generalize from the training

01:04:41.280 --> 01:04:47.440
set onto data that's similar to that training set. So where do you think are the limitations of

01:04:47.440 --> 01:04:52.240
those approaches? What do you think are its strengths relative to your major efforts of

01:04:52.800 --> 01:04:57.680
constructing a theory of human intelligence? Yeah. Well, I'm not an expert in this field.

01:04:57.760 --> 01:05:02.000
I'm somewhat knowledgeable. So some of it is in just your intuition. What are your...

01:05:02.000 --> 01:05:07.200
Well, I have a little bit more than intuition, but I just want to say one of the things that

01:05:07.200 --> 01:05:10.560
you asked me, do I spend all my time thinking about neuroscience? I do. That's to the exclusion

01:05:10.560 --> 01:05:14.560
of thinking about things like convolutional neural networks. But I try to stay current.

01:05:15.200 --> 01:05:19.760
So look, I think it's great the progress they've made. It's fantastic. And as I mentioned earlier,

01:05:19.760 --> 01:05:26.160
it's very highly useful for many things. The models that we have today are actually derived

01:05:26.160 --> 01:05:30.480
from a lot of neuroscience principles. They are distributed processing systems and distributed

01:05:30.480 --> 01:05:35.760
memory systems. And that's how the brain works. They use things that we might call them neurons,

01:05:35.760 --> 01:05:39.680
but they're really not neurons at all. So we can just... They're not really neurons. So they're

01:05:39.680 --> 01:05:47.040
distributed processing systems. And nature of hierarchy that came also from neuroscience.

01:05:47.040 --> 01:05:51.440
And so there's a lot of things, the learning rules basically, not backprop, but other heavy

01:05:51.440 --> 01:05:55.920
and tight learning. I'd be curious to say they're not neurons at all. Can you describe in which

01:05:55.920 --> 01:06:00.880
way? I mean, some of it is obvious, but I'd be curious if you have specific ways in which

01:06:00.880 --> 01:06:05.440
you think are the biggest differences. Yeah, we had a paper in 2016 called Why Neurons of

01:06:05.440 --> 01:06:11.280
Thousands of Synapses. And if you read that paper, you'll know what I'm talking about here.

01:06:11.280 --> 01:06:17.440
A real neuron in the brain is a complex thing. Let's just start with the synapses on it, which is

01:06:17.440 --> 01:06:23.280
a connection between neurons. Real neurons can everywhere from five to 30,000 synapses on them.

01:06:24.240 --> 01:06:29.680
The ones near the cell body, the ones that are close to the soma or the cell body,

01:06:30.400 --> 01:06:34.560
those are like the ones that people model in artificial neurons. There's a few hundred of

01:06:34.560 --> 01:06:41.520
those, maybe they can affect the cell, they can make the cell become active. 95% of the synapses

01:06:42.160 --> 01:06:45.920
can't do that. They're too far away. So if you activate one of those synapses,

01:06:45.920 --> 01:06:48.800
it just doesn't affect the cell body enough to make any difference.

01:06:48.800 --> 01:06:50.000
Any one of them individually.

01:06:50.000 --> 01:06:52.320
Any one of them individually, or even if you do a mass of them.

01:06:54.000 --> 01:07:02.160
What real neurons do is the following. If you activate or you get 10 to 20 of them

01:07:03.440 --> 01:07:06.640
active at the same time, meaning they're all receiving an input at the same time,

01:07:06.640 --> 01:07:11.200
and those 10 to 20 synapses or 40 synapses within a very short distance on the dendro,

01:07:11.200 --> 01:07:15.280
like 40 microns, a very small area. So if you activate a bunch of these right next to each

01:07:15.280 --> 01:07:20.480
other at some distant place, what happens is it creates what's called the dendritic spike.

01:07:21.120 --> 01:07:26.640
And then dendritic spike travels through the dendroids and can reach the soma or the cell body.

01:07:27.680 --> 01:07:33.440
Now, when it gets there, it changes the voltage, which is sort of like going to make the cell fire,

01:07:33.440 --> 01:07:38.320
but never enough to make the cell fire. It's sort of what we call it, we depolarize the cell,

01:07:38.320 --> 01:07:41.920
you raise the voltage a little bit, but not enough to do anything. It's like, well,

01:07:41.920 --> 01:07:48.800
good as that. And then it goes back down again. So we proposed a theory, which I'm very confident

01:07:49.280 --> 01:07:56.240
basics are, is that what's happening there is those 95% of the synapses are recognizing dozens

01:07:56.240 --> 01:08:01.120
to hundreds of unique patterns. They can write, you know, about the 10 and 20 synapses at a time,

01:08:02.000 --> 01:08:06.880
and they're acting like predictions. So the neuron actually is a predictive engine on its own.

01:08:07.600 --> 01:08:11.600
It can fire when it gets enough what they call proximal input from those ones near the cell

01:08:11.600 --> 01:08:16.560
fire, but it can get ready to fire from dozens to hundreds of patterns that it recognizes from

01:08:16.560 --> 01:08:23.360
the other guys. And the advantage of this to the neuron is that when it actually does produce a spike

01:08:23.360 --> 01:08:28.480
in action potential, it does so slightly sooner than it would have otherwise. And so what could

01:08:28.480 --> 01:08:33.120
it slightly sooner? Well, the slightly sooner part is it, there's it all the neurons in the,

01:08:33.120 --> 01:08:36.560
the excitatory neurons in the brain are surrounded by these inhibitory neurons,

01:08:36.560 --> 01:08:42.480
and they're very fast, the inhibitory neurons, these basket cells. And if I get my spike out

01:08:42.480 --> 01:08:47.280
a little bit sooner than someone else, I inhibit all my neighbors around me. Right. And what you

01:08:47.280 --> 01:08:51.440
end up with is a different representation. You end up with a representation that matches your

01:08:51.440 --> 01:08:55.920
prediction. It's a, it's a sparser representation, meaning the few are non interactive, but it's

01:08:55.920 --> 01:09:02.560
much more specific. And so we showed how networks of these neurons can do very sophisticated temporal

01:09:02.560 --> 01:09:09.840
prediction, basically. So, so this summarizes real neurons in the brain are time based prediction

01:09:09.920 --> 01:09:17.280
engines. And, and they, and there's no concept of this at all, in artificial, what we call point

01:09:17.280 --> 01:09:21.040
neurons. I don't think you can mail the brain without them. I don't think you can build intelligence

01:09:21.040 --> 01:09:25.920
about it, because it's the, it's the, it's where large part of the time comes from. It's, it's,

01:09:25.920 --> 01:09:31.120
these are predictive models. And the time is, is there's a prior and a, you know, a prediction

01:09:31.120 --> 01:09:36.560
and an action. And it's inherent through every neuron in the neocortex. So, so I would say that

01:09:36.560 --> 01:09:41.520
point neurons sort of model a piece of that and not very well at that either. But, you know, like,

01:09:41.520 --> 01:09:49.120
like for example, synapses are very unreliable. And you cannot assign any precision to them.

01:09:49.840 --> 01:09:55.200
So even one digit of precision is not possible. So the way real neurons work is they don't add

01:09:55.200 --> 01:09:59.920
these, they don't change these weights accurately, like artificial neural networks do. They basically

01:09:59.920 --> 01:10:05.760
form new synapses. And so what you're trying to always do is, is detect the presence of some 10

01:10:05.760 --> 01:10:11.680
to 20 active synapses at the same time, as opposed, and there's, they're almost binary. It's like,

01:10:11.680 --> 01:10:15.520
because you can't really represent anything much finer than that. So these are the kind of

01:10:16.160 --> 01:10:19.840
and I think that's actually another essential component, because the brain works on sparse

01:10:19.840 --> 01:10:24.880
patterns. And all that, all that mechanism is based on sparse patterns. And I don't actually

01:10:24.880 --> 01:10:30.000
think you could build our real brains or machine intelligence without incorporating some of those

01:10:30.000 --> 01:10:35.280
ideas. It's hard to even think about the complexity that emerges from the fact that the timing of the

01:10:35.280 --> 01:10:43.120
firing matters in the brain, the fact that you form new, new synapses. And I mean, everything

01:10:43.120 --> 01:10:47.120
you just mentioned in the past few minutes, trust me, if you spend time on it, you can get your mind

01:10:47.120 --> 01:10:52.320
around it. It's not like it's no longer a mystery to me. No, but, but sorry, as a function in a

01:10:52.320 --> 01:10:58.000
mathematical way, it's, can you get it start getting an intuition about what gets it excited,

01:10:58.000 --> 01:11:03.760
what not, and what kind of representation it's not as easy as there's many other types of neural

01:11:03.760 --> 01:11:10.560
networks that are more amenable to pure analysis. You know, especially very simple networks, you

01:11:10.560 --> 01:11:14.400
know, oh, I have four neurons and they're doing this. Can we, you know, describe them mathematically

01:11:14.400 --> 01:11:19.280
what they're doing type of thing. Even the complexity of convolutional neural networks today,

01:11:19.280 --> 01:11:25.360
it's sort of a mystery. They can't really describe the whole system. And so it's different. My colleague,

01:11:25.360 --> 01:11:32.880
Subitain Ahmad, he did a nice paper on this. You can get all the stuff on our website if you're

01:11:32.880 --> 01:11:37.920
interested in talking about some of the mathematical properties of sparse representations. And so we

01:11:37.920 --> 01:11:44.560
can't, what we can do is we can show mathematically, for example, why 10 to 20 synapses to recognize a

01:11:44.560 --> 01:11:48.800
pattern is the correct number is the right number you'd want to use. And by the way, that matches

01:11:48.800 --> 01:11:57.520
biology, we can show mathematically some of these concepts about the show why the brain is so robust

01:11:58.480 --> 01:12:02.960
to noise and error and fall out and so on. We can show that mathematically as well as empirically

01:12:02.960 --> 01:12:09.760
in simulations. But the system can't be analyzed completely. Any complex system can't. And so

01:12:09.760 --> 01:12:17.760
that's out of the realm. But there is there is mathematical benefits and intuitions that can

01:12:17.760 --> 01:12:22.080
be derived from mathematics. And we try to do that as well. Most most of our papers have a section

01:12:22.080 --> 01:12:27.440
about that. So I think it's refreshing and useful for me to be talking to you about deep

01:12:27.440 --> 01:12:34.480
neural networks, because your intuition basically says that we can't achieve anything like intelligence

01:12:34.480 --> 01:12:37.760
with artificial neural networks. Well, not in the current form. Not in the current form. I'm sure

01:12:37.760 --> 01:12:42.800
we can do it in the ultimate form, sure. So let me dig into it and see what your thoughts are there

01:12:42.800 --> 01:12:47.440
a little bit. So I'm not sure if you read this little blog post called Bitter Lesson by Rich

01:12:47.520 --> 01:12:53.280
Sutton. Recently, he's a reinforcement learning pioneer. I'm not sure if you're familiar with him.

01:12:53.280 --> 01:12:59.440
His basic idea is that all the stuff we've done in AI in the past 70 years, he's one of the old

01:12:59.440 --> 01:13:08.960
school guys. The biggest lesson learned is that all the tricky things we've done don't, you know,

01:13:08.960 --> 01:13:13.600
they benefit in the short term. But in the long term, what wins out is a simple general method

01:13:14.400 --> 01:13:19.760
that just relies on Moore's law on computation getting faster and faster.

01:13:19.760 --> 01:13:22.720
This is what he's saying. This is what has worked up to now.

01:13:23.600 --> 01:13:29.920
What has worked up to now, that if you're trying to build a system, if we're talking about,

01:13:29.920 --> 01:13:33.360
he's not concerned about intelligence. He's concerned about a system that works

01:13:34.400 --> 01:13:40.160
in terms of making predictions on applied, narrow AI problems. That's what the discussion is about.

01:13:41.120 --> 01:13:48.480
That you just try to go as general as possible and wait years or decades for the computation

01:13:48.480 --> 01:13:53.200
to make it actually. Is he saying that as a criticism or is he saying this is a prescription

01:13:53.200 --> 01:13:57.920
of what we ought to be doing? Well, it's very difficult. He's saying this is what has worked

01:13:57.920 --> 01:14:00.880
and yes, a prescription, but it's a difficult prescription because it says

01:14:01.520 --> 01:14:07.280
all the fun things you guys are trying to do, we are trying to do, he's part of the community,

01:14:07.280 --> 01:14:13.200
is saying it's only going to be short-term gains. This all leads up to a question, I guess,

01:14:13.840 --> 01:14:19.200
on artificial neural networks and maybe our own biological neural networks is,

01:14:20.320 --> 01:14:25.280
do you think if we just scale things up significantly, so take these dumb artificial

01:14:25.840 --> 01:14:33.120
neurons, the point neurons, I like that term, if we just have a lot more of them,

01:14:33.120 --> 01:14:38.080
do you think some of the elements that we see in the brain may start emerging?

01:14:38.080 --> 01:14:44.960
No, I don't think so. We can do bigger problems of the same type. It's been pointed out by many

01:14:44.960 --> 01:14:48.720
people that today's convolutional neural networks aren't really much different than the ones we had

01:14:48.720 --> 01:14:53.680
quite a while ago. They're bigger and train more and we have more labeled data and so on,

01:14:56.320 --> 01:15:01.120
but I don't think you can get to the kind of things I know the brain can do and that we think

01:15:01.120 --> 01:15:07.120
about as intelligence by just scaling it up. It's a good description of what's happened in

01:15:07.120 --> 01:15:11.680
the past, what's happened recently with the reemergence of artificial neural networks.

01:15:12.480 --> 01:15:16.160
It may be a good prescription for what's going to happen in the short term,

01:15:17.520 --> 01:15:21.920
but I don't think that's the path. I've said that earlier, there's an alternate path. I should

01:15:21.920 --> 01:15:28.000
mention to you, by the way, that we've made sufficient progress on the whole cortical theory

01:15:28.000 --> 01:15:36.880
in the last few years that last year, we decided to start actively pursuing how we get these ideas

01:15:36.880 --> 01:15:42.480
embedded into machine learning. That's again being led by my colleague, Subhathayamad,

01:15:43.040 --> 01:15:46.080
and because he's more of a machine learning guy, I'm more of an neuroscience guy.

01:15:49.200 --> 01:15:56.320
Now, I wouldn't say our focus, but it is now an equal focus here because we need to

01:15:56.320 --> 01:16:03.280
proselytize what we've learned, and we need to show how it's beneficial to the machine

01:16:03.280 --> 01:16:07.600
learning. We have a plan in place right now. In fact, we just did our first paper on this.

01:16:07.600 --> 01:16:12.480
I can tell you about that, but one of the reasons I want to talk to you is because I'm trying to

01:16:13.200 --> 01:16:17.120
get more people in the machine learning community to say, I need to learn about this stuff,

01:16:17.120 --> 01:16:20.880
and maybe we should just think about this a bit more about what we've learned about the brain,

01:16:20.880 --> 01:16:24.640
and what are those team members meant to have? What have they done? Is that useful for us?

01:16:25.120 --> 01:16:29.760
Yeah, so is there elements of all the cortical theory that things we've been talking about

01:16:29.760 --> 01:16:33.360
that may be useful in the short term? Yes, in the short term, yes.

01:16:33.360 --> 01:16:39.200
This is the, sorry to interrupt, but the open question is it certainly feels from my perspective

01:16:39.200 --> 01:16:44.160
that in the long term, some of the ideas we've been talking about will be extremely useful.

01:16:44.160 --> 01:16:48.480
The question is whether in the short term. Well, this is always what I would call the

01:16:48.480 --> 01:16:54.720
entrepreneur's dilemma. So you have this long term vision. Oh, we're going to all be driving

01:16:54.720 --> 01:16:58.160
electric cars or we're all going to have computers or we're all going to whatever.

01:16:58.960 --> 01:17:03.120
And, and you're at some point in time and you say, I can see that long term vision. I'm sure

01:17:03.120 --> 01:17:06.320
it's going to happen. How do I get there without killing myself, you know, without going out of

01:17:06.320 --> 01:17:11.760
business? That's the challenge. That's the dilemma. That's the really difficult thing to do. So we're

01:17:11.760 --> 01:17:16.240
facing that right now. So ideally what you'd want to do is find some steps along the way that you

01:17:16.240 --> 01:17:19.840
can get there incrementally. You don't have to like throw it all out and start over again.

01:17:20.400 --> 01:17:27.520
The first thing that we've done is we focus on the sparse representations. So just, just in case

01:17:27.520 --> 01:17:31.520
you don't know what that means or some of the listeners don't know what that means. In the

01:17:31.520 --> 01:17:36.880
brain, if I have like 10,000 neurons, what you would see is maybe 2% of them active at a time.

01:17:36.880 --> 01:17:42.560
You don't see 50%, you don't think 30%, you might see 2%. And it's always like that.

01:17:42.560 --> 01:17:46.480
For any set of sensory inputs. It doesn't matter if anything, it doesn't matter with any part of the

01:17:46.480 --> 01:17:54.080
brain. But which neurons differs? Which neurons are active? Yeah, so let me put this, let's say I

01:17:54.080 --> 01:17:58.000
take 10,000 neurons that are representing something. They're sitting there in a block together. It's a

01:17:58.000 --> 01:18:01.600
teeny little block in a neuron, 10,000 neurons. And they're representing a location, they're

01:18:01.600 --> 01:18:04.560
representing a cop, they're representing the input from my sensors. I don't know, it doesn't

01:18:04.560 --> 01:18:10.000
matter. It's representing something. The way the representations occur, it's always a sparse

01:18:10.000 --> 01:18:14.320
representation, meaning it's a population code. So which 200 cells are active tells me what's

01:18:14.320 --> 01:18:19.280
going on. It's not individual cells aren't that important at all. It's the population code that

01:18:19.280 --> 01:18:25.680
matters. And when you have sparse population codes, then all kinds of beautiful properties come out

01:18:25.680 --> 01:18:30.160
of them. So the brain uses sparse population codes that we've written and described these

01:18:30.160 --> 01:18:37.360
benefits in some of our papers. So they give this tremendous robustness to the systems.

01:18:37.360 --> 01:18:41.920
Your brains are incredibly robust. Neurons are dying all the time and spasming and synapses

01:18:41.920 --> 01:18:49.120
falling apart and, you know, all the time and it keeps working. So what Subitai and Louise,

01:18:49.120 --> 01:18:55.760
one of our other engineers here have done, I've shown that they're introducing sparseness into

01:18:55.760 --> 01:18:58.480
convolutional neural networks. Now other people are thinking along these lines, but we're going

01:18:58.480 --> 01:19:04.000
about it in a more principled way, I think. And we're showing that if you enforce sparseness

01:19:04.000 --> 01:19:11.920
throughout these convolutional neural networks, in both which neurons are active and the connections

01:19:11.920 --> 01:19:17.120
between them, that you get some very desirable properties. So one of the current hot topics in

01:19:17.920 --> 01:19:22.640
deep learning right now are these adversarial examples. So, you know, I can give me any deep

01:19:22.640 --> 01:19:26.800
learning network and I can give you a picture that looks perfect and you're going to call it,

01:19:26.800 --> 01:19:32.400
you know, you're going to say the monkey is, you know, an airplane. So that's a problem.

01:19:32.400 --> 01:19:36.480
And DARPA just announced some big thing. They're trying to, you know, have some contests for this.

01:19:36.480 --> 01:19:42.000
But if you enforce sparse representations here, many of these problems go away. They're much more

01:19:42.000 --> 01:19:47.440
robust and they're not easy to fool. So we've already shown some of those results,

01:19:48.240 --> 01:19:56.160
just literally in January or February, just like last month we did that. And you can, I think it's

01:19:56.160 --> 01:20:02.320
on bioarchive right now or on iCry, you can read about it. But so that's like a baby step.

01:20:02.320 --> 01:20:05.920
Okay. That's a take something from the brain. We know, we know about sparseness. We know why

01:20:05.920 --> 01:20:09.440
it's important. We know what it gives the brain. So let's try to enforce that onto this.

01:20:09.440 --> 01:20:13.360
What's your intuition why sparsity leads to robustness? Because it feels like it would be

01:20:13.360 --> 01:20:22.880
less robust. Why would you feel the rest robust to you? So it just feels like if the fewer neurons

01:20:22.960 --> 01:20:27.840
are involved, the more fragile the representation. Yeah, but I didn't say there was lots of

01:20:27.840 --> 01:20:34.400
funerals. I said, let's say 200. That's a lot. There's still a lot. So here's an intuition for it.

01:20:35.120 --> 01:20:40.640
This is a bit technical. So for, you know, for engineers, machine learning people,

01:20:40.640 --> 01:20:45.600
this would be easy, but all the listeners, maybe not. If you're trying to classify something,

01:20:45.600 --> 01:20:49.600
you're trying to divide some very high dimensional space into different pieces,

01:20:49.600 --> 01:20:53.520
A and B, and you're trying to create some point where you say all these points in this

01:20:53.520 --> 01:20:56.320
high dimensional space are A and all these points inside dimensional space are B.

01:20:57.520 --> 01:21:03.280
And if you have points that are close to that line, it's not very robust. It works for all

01:21:03.280 --> 01:21:08.240
the points you know about, but it's, it's not very robust because you just move a little bit and

01:21:08.240 --> 01:21:14.160
you've crossed over the line. When you have sparse representations, imagine I pick, I have,

01:21:14.160 --> 01:21:20.240
I'm going to pick 200 cells active out of, out of 10,000. Okay. So I have 200 cells active.

01:21:20.240 --> 01:21:24.880
Now let's say I pick randomly another, a different representation, 200. The overlap

01:21:24.880 --> 01:21:30.800
between those is going to be very small, just a few. I can pick millions of samples randomly

01:21:31.440 --> 01:21:38.880
of 200 neurons and not one of them will overlap more than just a few. So one way to think about

01:21:38.880 --> 01:21:43.360
is if I want to fool one of these representations to look like one of those other representations,

01:21:43.360 --> 01:21:48.160
I can't move just one cell or two cells or three cells or four cells. I have to move a hundred cells

01:21:49.120 --> 01:21:56.080
and that makes them robust. In terms of further, so you mentioned sparsity.

01:21:56.080 --> 01:22:01.040
Won't it be the next thing? Yeah. Okay. So we have, we picked one. We don't know if it's going

01:22:01.040 --> 01:22:05.040
to work well yet. So again, we're trying to come up incremental ways of moving from

01:22:05.760 --> 01:22:11.360
brain theory to add pieces to machine learning, current machine learning world and one step at

01:22:11.360 --> 01:22:16.000
a time. So the next thing we're going to try to do is sort of incorporate some of the ideas of

01:22:17.520 --> 01:22:23.360
the 1000 brains theory that you have many, many models and that are voting. Now that idea is not

01:22:23.360 --> 01:22:28.960
new. There's a mixture of models has been around for a long time, but the way the brain does it is

01:22:28.960 --> 01:22:35.680
a little different and the way it votes is different and the kind of way it represents

01:22:35.680 --> 01:22:40.960
uncertainty is different. So we're just starting this work, but we're going to try to see if we

01:22:41.360 --> 01:22:45.280
sort of incorporate some of the principles of voting or principles of 1000 brain theory,

01:22:46.000 --> 01:22:52.640
like lots of simple models that talk to each other in a very certain way.

01:22:53.840 --> 01:23:00.480
And can we build more machines and systems that learn faster and also, well, mostly

01:23:01.920 --> 01:23:09.600
are multimodal and robust to multimodal type of issues. So one of the challenges there

01:23:09.600 --> 01:23:15.520
is the machine learning computer vision community has certain sets of benchmarks.

01:23:15.520 --> 01:23:20.960
So it's a test based on which they compete. And I would argue, especially from your perspective,

01:23:21.920 --> 01:23:28.720
that those benchmarks aren't that useful for testing the aspects that the brain is good at

01:23:28.720 --> 01:23:33.360
or intelligent. They're not really testing intelligence. They're very fine. And it's been

01:23:33.360 --> 01:23:39.760
extremely useful for developing specific mathematical models, but it's not useful in the

01:23:39.760 --> 01:23:44.960
long term for creating intelligence. So you think you also have a role in proposing better

01:23:46.080 --> 01:23:50.080
tests? Yeah, this is a very, you've identified a very serious problem.

01:23:51.360 --> 01:23:55.520
First of all, the test that they have or the test that they want, not the test of the other

01:23:55.520 --> 01:24:02.720
things that we're trying to do, right? You know, what are the so on? The second thing is,

01:24:02.720 --> 01:24:09.280
sometimes these to be competitive in these tests, you have to have huge data sets and huge computing

01:24:09.280 --> 01:24:15.440
power. And so, you know, and we don't have that here. We don't have it as well as other big teams

01:24:15.440 --> 01:24:22.320
that big companies do. So there's numerous issues there. You know, we come out of, you know,

01:24:22.320 --> 01:24:26.000
we're our approach to this is all based on in some sense, you might argue elegance,

01:24:26.000 --> 01:24:28.800
we're coming at it from like a theoretical base that we think, Oh, my God, this is so,

01:24:28.800 --> 01:24:31.680
this is so clearly elegant. This is how brains work. This is what intelligence is.

01:24:31.680 --> 01:24:34.880
But the machine learning world has gotten in this phase where they think it doesn't matter.

01:24:35.440 --> 01:24:39.360
Doesn't matter what you think, as long as you do, you know, 0.1% better on this benchmark,

01:24:39.360 --> 01:24:45.280
that's what that's all that matters. And that's a problem. You know, we have to figure out how

01:24:45.280 --> 01:24:48.640
to get around that. That's that's a challenge for us. That's that's one of the challenges that

01:24:48.640 --> 01:24:55.120
we have to deal with. So I agree, you've identified a big issue. It's difficult for those reasons.

01:24:55.840 --> 01:25:00.720
But, you know, part of the reasons I'm talking to you here today is I hope I'm going to get some

01:25:00.720 --> 01:25:05.040
machine learning people to say, read those papers. Those might be some interesting ideas. I'm tired.

01:25:05.040 --> 01:25:07.600
I'm tired of doing this 0.1% improvement stuff, you know,

01:25:08.400 --> 01:25:12.960
well, that's what that's why I'm here as well, because I think machine learning now as a community

01:25:12.960 --> 01:25:20.800
is a place where the next step is needs to be orthogonal to what has received success in the

01:25:20.800 --> 01:25:26.080
past. You see other leaders saying this, machine learning leaders, you know, Jeff Hinton, with

01:25:26.080 --> 01:25:29.760
his capsules idea. Many people have gotten up saying, you know, we're going to hit road,

01:25:30.880 --> 01:25:36.080
maybe we should look at the brain, you know, things like that. So hopefully that thinking

01:25:36.080 --> 01:25:41.120
will occur organically. And then then we're in a nice position for people to come and look at our

01:25:41.120 --> 01:25:44.960
work and say, well, what can we learn from these guys? Yeah, MIT is just launching a

01:25:44.960 --> 01:25:50.160
billion dollar computing college that's centered around this idea. So on this idea of what?

01:25:50.880 --> 01:25:55.600
Well, the idea that, you know, the humanities, psychology and neuroscience have to work all

01:25:55.600 --> 01:26:01.360
together to get to build the S. Yeah. I mean, Stanford just did this human center today. I

01:26:01.360 --> 01:26:07.920
said, yeah, I'm a little disappointed in these initiatives because, you know, they're, they're

01:26:07.920 --> 01:26:13.440
focusing on sort of the human side of it. And it could very easily slip into how humans interact

01:26:13.440 --> 01:26:19.360
with intelligent machines, which is nothing wrong with that. But that's not, that is orthogonal

01:26:19.360 --> 01:26:23.120
to what we're trying to do. We're trying to say, like, what is the essence of intelligence? I don't

01:26:23.120 --> 01:26:27.040
care. In fact, I want to build intelligent machines that aren't emotional, that don't

01:26:27.040 --> 01:26:31.760
smile at you, that, you know, that aren't trying to tuck you in at night.

01:26:31.760 --> 01:26:36.880
Yeah, there is that pattern that you, when you talk about understanding humans is important

01:26:36.880 --> 01:26:41.440
for understanding intelligence, that you start slipping into topics of ethics or,

01:26:42.720 --> 01:26:46.880
yeah, like you said, the interactive elements as opposed to, no, no, no, we have to zoom in on

01:26:46.880 --> 01:26:52.720
the brain, study, study what the human brain, the baby, the, let's study what a brain does.

01:26:52.720 --> 01:26:57.920
Does. And then we can decide which parts of that we want to recreate in some system. But

01:26:57.920 --> 01:27:01.120
until you have that theory about what the brain does, what's the point? You know, it's just,

01:27:01.120 --> 01:27:04.560
you're going to be wasting time, I think. Right. Just to break it down on the artificial

01:27:04.560 --> 01:27:09.040
neural networks side, maybe you can speak to this on the, on the biologic neural networks side,

01:27:09.040 --> 01:27:15.040
the process of learning versus the process of inference. Maybe you can explain to me,

01:27:16.400 --> 01:27:20.000
what is there a difference between, you know, in artificial neural networks, there's a

01:27:20.000 --> 01:27:24.240
difference between the learning stage and the inference stage. Do you see the brain as something

01:27:24.240 --> 01:27:29.840
different? One of the, one of the big distinctions that people often say, I don't know how correct

01:27:29.840 --> 01:27:34.240
it is, is artificial neural networks need a lot of data, they're very inefficient learning.

01:27:34.800 --> 01:27:40.160
Do you see that as a correct distinction from the, the biology of the human brain,

01:27:40.160 --> 01:27:44.160
that the human brain is very efficient? Or is that just something we deceive ourselves with?

01:27:44.160 --> 01:27:47.440
No, it is efficient, obviously. We can learn new things almost instantly.

01:27:47.440 --> 01:27:52.240
And so what elements do you think? Yeah, I can talk about that. You brought up two issues there.

01:27:52.240 --> 01:27:57.200
So remember I talked early about the constraints we, we always feel, well, one of those constraints

01:27:57.200 --> 01:28:02.720
is the fact that brains are continually learning. That's not something we said, oh, we can add that

01:28:02.720 --> 01:28:09.520
later. That's something that was upfront, had to be there from the start, made our problems

01:28:10.080 --> 01:28:16.160
harder. But we showed, going back to the 2016 paper on sequence memory, we showed how that

01:28:16.160 --> 01:28:22.560
happens, how the brains infer and learn at the same time. And our models do that. They're not

01:28:22.560 --> 01:28:29.680
two separate phases or two separate sets of time. I think that's a big, big problem in AI,

01:28:29.760 --> 01:28:36.480
at least for many applications, not for all. So I can talk about that. There are some that gets

01:28:36.480 --> 01:28:41.600
detailed. There are some parts of the neocortex in the brain where actually what's going on,

01:28:41.600 --> 01:28:46.160
there's these, there's these, with these cycles, they're like cycles of activity in the brain.

01:28:46.800 --> 01:28:52.320
And there's very strong evidence that you're doing more of inference on one part of the phase and

01:28:52.320 --> 01:28:55.680
more of learning on the other part of the phase. So the brain can actually sort of separate different

01:28:55.680 --> 01:29:00.720
populations of cells are going back and forth like this. But in general, I would say that's an

01:29:00.720 --> 01:29:06.880
important problem. We have all of our networks that we've come up with do both. They're learning,

01:29:06.880 --> 01:29:11.840
continuous learning networks. And you mentioned benchmarks earlier. Well, there are no benchmarks

01:29:11.840 --> 01:29:17.920
about that. Exactly. So we have to like, we get in our little soapbox and say, hey, by the way,

01:29:17.920 --> 01:29:23.840
this is important and here's the mechanism for doing that. But until you can prove it to someone

01:29:23.840 --> 01:29:27.600
in some, you know, commercial system or something, it's a little harder. So yeah, one of the things

01:29:27.600 --> 01:29:35.200
I had to linger on that is in some ways to learn the concept of a coffee cup. You only need this one

01:29:35.200 --> 01:29:40.080
coffee cup and maybe some time alone in a room with it. Well, the first thing is I, when I imagine

01:29:40.080 --> 01:29:44.320
I reach my hand into a black box and I'm reaching, I'm trying to touch something. I don't know up

01:29:44.320 --> 01:29:49.920
front if it's something I already know, or if it's a new thing. And I have to, I'm doing both at the

01:29:49.920 --> 01:29:54.720
same time. I don't say, oh, let's see if it's a new thing. Oh, let's see if it's an old thing. I

01:29:54.720 --> 01:30:00.880
don't do that. As I go, my brain says, oh, it's new or it's not new. And if it's new, I start learning

01:30:01.600 --> 01:30:05.840
what it is. So and by the way, it starts learning from the get go, even if we're going to recognize

01:30:05.840 --> 01:30:10.960
it. So they're not separate problems. And so that's the thing. The other thing you mentioned

01:30:10.960 --> 01:30:16.320
was the fast learning. So I was just talking about continuous learning, but there's also fast

01:30:16.320 --> 01:30:20.080
learning. Literally, I can show you this coffee cup. And I say, here's a new coffee cup. It's

01:30:20.080 --> 01:30:25.040
got the logo on it. Take a look at it. Done. You're done. You can predict what it's going to look

01:30:25.040 --> 01:30:32.720
like, you know, in different positions. So I can talk about that too. In the brain, the way learning

01:30:32.720 --> 01:30:37.440
occurs. I mentioned this earlier, but I mentioned again, the way learning occurs, I imagine I have

01:30:37.440 --> 01:30:43.520
a section of a dendrite of a neuron. And I want to learn, I'm going to learn something new. I'm

01:30:43.520 --> 01:30:47.600
just doesn't matter what it is, I'm just going to learn something new. I need to recognize a new

01:30:47.600 --> 01:30:53.600
pattern. So what I'm going to do is I'm going to form new synapses. New synapses, we're going to

01:30:53.600 --> 01:31:00.480
rewire the brain onto that section of the dendrite. Once I've done that, everything else that neuron

01:31:00.480 --> 01:31:06.320
has learned is not affected by it. That's because it's isolated to that small section of the dendrite.

01:31:06.320 --> 01:31:11.200
They're not all being added together, like a point neuron. So if I learned something new on this

01:31:11.200 --> 01:31:14.800
segment here, it doesn't change any of the learning that occur anywhere else in that neuron.

01:31:14.800 --> 01:31:19.440
So I can add something without affecting previous learning. And I can do it quickly.

01:31:20.880 --> 01:31:24.400
Now let's talk, we can talk about the quickness, how it's done in real neurons. You might say,

01:31:24.400 --> 01:31:29.840
well, doesn't it take time to form synapses? Yes, it can take maybe an hour to form a new synapse.

01:31:30.800 --> 01:31:36.000
We can form memories quicker than that. And I can explain that happens too, if you want. But

01:31:36.720 --> 01:31:41.280
it's getting a bit neuroscience-y. That's great. But is there an understanding

01:31:41.280 --> 01:31:46.160
of these mechanisms at every level? So from the short-term memories and the forming

01:31:47.520 --> 01:31:52.400
many connections. So this idea of synaptogenesis, the growth of new synapses, that's well

01:31:52.400 --> 01:31:56.720
described, as well understood. And that's an essential part of learning. That is learning.

01:31:56.720 --> 01:32:04.560
That is learning. Going back many, many years, people was

01:32:05.520 --> 01:32:11.120
what's his name, the psychologist proposed, Heb, Donald Heb. He proposed that learning was the

01:32:11.120 --> 01:32:17.200
modification of the strength of a connection between two neurons. People interpreted that as

01:32:17.200 --> 01:32:21.680
the modification of the strength of a synapse. He didn't say that. He just said there's a

01:32:21.680 --> 01:32:26.400
modification between the effect of one neuron and another. So synaptogenesis is totally consistent

01:32:26.400 --> 01:32:30.800
with Donald Heb said. But anyway, there's these mechanisms, the growth of new synapse,

01:32:30.800 --> 01:32:33.840
you can go online, you can watch a video of a synapse growing in real time.

01:32:33.840 --> 01:32:38.960
It's literally, you can see this little thing going. It's pretty impressive. So those

01:32:38.960 --> 01:32:43.520
mechanisms are known. Now, there's another thing that we've speculated and we've written about,

01:32:43.520 --> 01:32:49.120
which is consistent with no neuroscience, but it's less proven. And this is the idea,

01:32:49.120 --> 01:32:53.920
how do I form a memory really, really quickly? Like instantaneous. If it takes an hour to

01:32:53.920 --> 01:33:00.080
grow a synapse, like that's not instantaneous. So there are types of synapses called silent

01:33:00.080 --> 01:33:05.040
synapses. They look like a synapse, but they don't do anything. They're just sitting there. It's

01:33:05.040 --> 01:33:10.560
like if an action potential comes in, it doesn't release any neurotransmitter. Some parts of the

01:33:10.560 --> 01:33:15.200
brain have more of these than others. For example, the hippocampus has a lot of them, which is where

01:33:15.200 --> 01:33:22.000
we associate most short-term memory with. So what we speculated, again, in that 2016 paper,

01:33:22.000 --> 01:33:28.160
we proposed that the way we form very quick memories, very short-term memories, or quick

01:33:28.160 --> 01:33:35.200
memories, is that we convert silent synapses into active synapses. It's like saying a synapse has

01:33:35.200 --> 01:33:41.520
a zero weight and a one weight. But the long-term memory has to be formed by synaptogenesis. So

01:33:41.520 --> 01:33:45.600
you can remember something really quickly by just flipping a bunch of these guys from silent to active.

01:33:46.080 --> 01:33:52.080
It's not from 0.1 to 0.15. It doesn't do anything until it releases transmitter.

01:33:52.080 --> 01:33:55.520
And if I do that over a bunch of these, I've got a very quick short-term memory.

01:33:56.240 --> 01:34:00.880
So I guess the lesson behind this is that most neural networks today are fully connected.

01:34:01.760 --> 01:34:05.920
Every neuron connects every other neuron from layer to layer. That's not correct in the brain.

01:34:05.920 --> 01:34:10.880
We don't want that. We actually don't want that. It's bad. You want a very sparse connectivity so

01:34:10.880 --> 01:34:16.640
that any neuron connects to some subset of the neurons in the other layer. And it does so on a

01:34:16.640 --> 01:34:23.360
dendrite by dendrite segment basis. So it's a very parcelated out type of thing. And that then

01:34:23.360 --> 01:34:27.360
learning is not adjusting all these weights, but learning is just saying, okay, connect to these

01:34:27.360 --> 01:34:33.360
10 cells here right now. In that process, you know, with artificial neural networks, it's a very

01:34:33.360 --> 01:34:39.840
simple process of back propagation that adjusts the weights. The process of synaptogenesis.

01:34:39.840 --> 01:34:44.160
Synaptogenesis. Synaptogenesis. It's even easier. It's even easier. It's even easier.

01:34:44.160 --> 01:34:49.840
Back propagation requires something that really can't happen in brains. This back propagation

01:34:49.840 --> 01:34:53.360
of this error signal. They really can't happen. People are trying to make it happen in brains,

01:34:53.360 --> 01:34:58.080
but it doesn't happen in brain. This is pure Hebbian learning. Well, synaptogenesis is pure

01:34:58.080 --> 01:35:02.080
Hebbian learning. It's basically saying there's a population of cells over here that are active

01:35:02.080 --> 01:35:06.320
right now. And there's a population of cells over here active right now. How do I form connections

01:35:06.320 --> 01:35:12.480
between those active cells? And it's literally saying this guy became active. These 100 neurons

01:35:12.480 --> 01:35:17.440
here became active before this neuron became active. So form connections to those ones. That's it.

01:35:17.440 --> 01:35:22.800
There's no propagation of error. Nothing. All the networks we do, all the models we have work on

01:35:23.520 --> 01:35:31.520
almost completely on Hebbian learning, but in on dendritic segments and multiple synaptes at the

01:35:31.520 --> 01:35:37.360
same time. So now let's turn the question that you already answered and maybe you can answer it again.

01:35:38.640 --> 01:35:43.760
If you look at the history of artificial intelligence, where do you think we stand? How

01:35:43.840 --> 01:35:48.800
far are we from solving intelligence? You said you were very optimistic. Can you elaborate on that?

01:35:48.800 --> 01:35:54.960
Yeah. It's always the crazy question to ask because no one can predict the future.

01:35:55.520 --> 01:36:01.440
So I'll tell you a story. I used to run a different neuroscience institute called the

01:36:01.440 --> 01:36:06.240
Redburn Neuroscience Institute. And we would hold these symposiums and we'd get like 35 scientists

01:36:06.240 --> 01:36:10.640
from around the world to come together. And I used to ask them all the same question. I would say,

01:36:10.640 --> 01:36:14.320
well, how long do you think it'll be before we understand how the New York Cortex works?

01:36:14.320 --> 01:36:17.040
And everyone went around the room and they introduced the name and they have to answer

01:36:17.040 --> 01:36:24.640
that question. So I got, the typical answer was 50 to 100 years. Some people would say 500 years.

01:36:24.640 --> 01:36:30.160
Some people said never. I said, why are you, why are you a neuroscience? It's a good pay.

01:36:32.560 --> 01:36:37.840
It's interesting. So, you know, but it doesn't work like that. As I mentioned earlier, these are

01:36:38.720 --> 01:36:42.560
step functions. Things happen and then bingo, they happen. You can't predict that.

01:36:43.520 --> 01:36:48.720
I feel I've already passed a step function. So if I can do my job correctly over the next five years,

01:36:50.720 --> 01:36:54.960
then meaning I can proselytize these ideas, I can convince other people they're right,

01:36:56.000 --> 01:37:00.400
we can show that other people or other machine learning people should pay attention to these

01:37:00.400 --> 01:37:05.840
ideas, then we're definitely in an under 20 year time frame. If I can do those things,

01:37:05.920 --> 01:37:09.680
if I, if I'm not successful in that, and this is the last time anyone talks to me

01:37:09.680 --> 01:37:14.640
and no one reads our papers and, you know, I'm wrong or something like that, then,

01:37:14.640 --> 01:37:21.520
then I don't know. But it's, it's not 50 years. It's, it, you know, it'll, it'll, you know,

01:37:21.520 --> 01:37:25.280
the same thing about electric cars, how quickly are they going to populate the world? It probably

01:37:25.280 --> 01:37:30.240
takes about a 20 year span. It'll be something like that. But I think if I can do what I said,

01:37:30.240 --> 01:37:35.760
we're starting it. And of course, there could be other use of step functions. It could be

01:37:37.440 --> 01:37:41.760
everybody gives up on your ideas for 20 years, and then all of a sudden somebody picks it up

01:37:41.760 --> 01:37:45.520
again. Wait, that guy was onto something. Yeah. So that would be a, that would be a failure on

01:37:45.520 --> 01:37:50.560
my part, right? You know, think about Charles Babbage, you know, Charles Babbage, he's the guy

01:37:50.560 --> 01:37:58.320
who invented the computer back in the 18 something 1800s. And everyone forgot about it until, you

01:37:58.320 --> 01:38:02.080
know, 100 years later and say, Hey, this guy figured this stuff out a long time ago. Yeah.

01:38:02.080 --> 01:38:05.680
You know, but he was ahead of his time. Yeah. I don't think, you know, like, as I said,

01:38:06.320 --> 01:38:11.040
I recognize this is part of any entrepreneur's challenge. I use entrepreneur broadly in this

01:38:11.040 --> 01:38:13.920
case. I'm not meaning like I'm building a business trying to sell something. I mean,

01:38:13.920 --> 01:38:20.000
like I'm trying to sell ideas. And this is the challenge as to how you get people to pay attention

01:38:20.000 --> 01:38:25.360
to you. How do you get them to give you positive or negative feedback? How do you get to people

01:38:25.360 --> 01:38:29.520
act differently based on your ideas? So, you know, we'll see how well we do on that.

01:38:30.080 --> 01:38:34.800
So, you know, that there's a lot of hype behind artificial intelligence currently. Do you,

01:38:36.240 --> 01:38:42.400
as, as you look to spread the ideas that are of New York cortical theory of the things you're

01:38:42.400 --> 01:38:47.120
working on, do you think there's some possibility we'll hit an AI winter once again?

01:38:47.120 --> 01:38:49.440
Yeah, it's certainly a possibility. No question about it.

01:38:50.160 --> 01:38:56.400
Yeah. Well, I guess, do I worry about it? I haven't decided yet if that's good or bad for

01:38:56.400 --> 01:39:03.680
my mission. That's true. That's very true because it's almost like you need the winter to refresh

01:39:03.680 --> 01:39:08.880
the pallet. Yeah. So, it's like, I want, here's what you want to have it is you want like,

01:39:09.520 --> 01:39:16.000
to the extent that everyone is so thrilled about the current state of machine learning and AI and

01:39:16.080 --> 01:39:21.040
they don't imagine they need anything else. It makes my job harder. Right. If, if everything

01:39:21.040 --> 01:39:25.600
crashed completely and every student left the field and there was no money for anybody to do

01:39:25.600 --> 01:39:29.200
anything and it became an embarrassment to talk about machine intelligence and AI, that wouldn't

01:39:29.200 --> 01:39:33.840
be good for us either. You want, you want sort of the soft landing approach, right? You want enough

01:39:33.840 --> 01:39:38.880
people, the senior people in AI and machine learning and say, you know, we need other approaches. We

01:39:38.880 --> 01:39:42.960
really need other approaches. Damn, we need other approaches. Maybe we should look to the brain.

01:39:42.960 --> 01:39:46.800
Okay, let's look to the brain. Who's got some brain ideas? Okay, let's, let's start a little

01:39:46.800 --> 01:39:51.200
project on the side here, trying to do a brain idea related stuff. That's the ideal outcome we

01:39:51.200 --> 01:39:56.320
would want. So, I don't want a total winter and yet I don't want it to be sunny all the time either.

01:39:57.520 --> 01:40:01.760
So, what do you think it takes to build a system with human level intelligence

01:40:02.960 --> 01:40:08.560
where once demonstrated, you would be very impressed? So, does it have to have a body?

01:40:08.560 --> 01:40:13.360
Does it have to have the, the, the C word we used before consciousness

01:40:15.600 --> 01:40:21.280
as, as, as an entirety as a holistic sense? First of all, I don't think the goal is to create a

01:40:21.280 --> 01:40:25.840
machine that is human level intelligence. I think it's a false goal. Back to Turing, I think it was

01:40:25.840 --> 01:40:30.160
a false statement. We want to understand what intelligence is and then we can build intelligent

01:40:30.160 --> 01:40:35.120
machines of all different scales, all different capabilities. You know, a dog is intelligent.

01:40:35.120 --> 01:40:39.040
I don't need, you know, that'd be pretty good to have a dog, you know, but what about something

01:40:39.040 --> 01:40:44.320
that doesn't look like an animal at all in different spaces. So, my thinking about this is that we

01:40:44.320 --> 01:40:49.600
want to define what intelligence is, agree upon what makes an intelligent system. We can then say,

01:40:49.600 --> 01:40:53.680
okay, we're now going to build systems that work on those principles or some subset of them,

01:40:54.240 --> 01:41:00.160
and we can apply them to all different types of problems. And the, the kind, the idea, it's

01:41:00.160 --> 01:41:05.680
like computing. We don't ask, if I take a little, you know, little one chip computer, I don't say,

01:41:05.680 --> 01:41:09.600
well, that's not a computer because it's not as powerful as this, you know, big server over here.

01:41:09.600 --> 01:41:12.800
No, no, because we know that what the principles are computing on, and I can apply those principles

01:41:12.800 --> 01:41:16.800
to a small problem or into a big problem. And same intelligence needs to get there. We have to say,

01:41:16.800 --> 01:41:20.000
these are the principles. I can make a small one, a big one, I can make them distributed, I can

01:41:20.000 --> 01:41:24.000
put them on different sensors. They don't have to be human like at all. Now you did bring up a very

01:41:24.000 --> 01:41:29.120
interesting question about embodiment. Does it have to have a body? It has to have some concept

01:41:29.200 --> 01:41:34.080
of movement. It has to be able to move through these reference frames I talked about earlier.

01:41:34.080 --> 01:41:37.680
I, whether it's physically moving, like I need, if I'm going to have an AI that

01:41:37.680 --> 01:41:41.360
understands coffee cups, it's going to have to pick up the coffee cup and touch it and look at it

01:41:41.360 --> 01:41:47.360
with its, with its eyes and hands or something equivalent to that. If I have a mathematical AI,

01:41:48.160 --> 01:41:53.440
maybe it needs to move through mathematical spaces. I could have a virtual AI that lives

01:41:54.320 --> 01:42:00.880
in the internet and its movements are traversing links and digging into files, but it's got a

01:42:00.880 --> 01:42:07.600
location that it's traveling through some space. You can't have an AI that just takes some flash

01:42:07.600 --> 01:42:14.720
thing input, you know, we call it flash inference. Here's a pattern, done. No, it's movement time,

01:42:14.720 --> 01:42:18.080
movement pattern, movement pattern, movement pattern, attention, digging, building, building

01:42:18.080 --> 01:42:23.040
structure, just figuring out the model of the world. So some sort of embodiment, whether it's

01:42:23.040 --> 01:42:27.920
physical or not, has to be part of it. So self-awareness in the way to be able to answer

01:42:27.920 --> 01:42:31.440
where am I? You're bringing up self-awareness, it's a different topic, self-awareness.

01:42:31.440 --> 01:42:39.040
No, the very narrow definition, meaning knowing a sense of self enough to know where am I in this

01:42:39.040 --> 01:42:44.640
space? Yeah, basically the system, the system needs to know its location or each component of the

01:42:44.640 --> 01:42:50.720
system needs to know where it is in the world at that point in time. So self-awareness and

01:42:50.720 --> 01:42:56.320
consciousness, do you think, one, from the perspective of neuroscience and neocortex,

01:42:56.320 --> 01:43:02.240
these are interesting topics, solvable topics, do you have any ideas of why the heck it is that

01:43:02.240 --> 01:43:05.520
we have a subjective experience at all? Yeah, I have a lot of questions.

01:43:05.520 --> 01:43:08.320
And is it useful or is it just a side effect of us?

01:43:08.320 --> 01:43:13.760
It's interesting to think about. I don't think it's useful as a means to figure out how to

01:43:13.760 --> 01:43:21.280
build intelligent machines. It's something that systems do, and we can talk about what it is,

01:43:21.840 --> 01:43:25.440
that are like, well, if I build a system like this, then it would be self-aware. Or

01:43:26.400 --> 01:43:29.920
if I build it like this, it wouldn't be self-aware. So that's a choice I can have.

01:43:29.920 --> 01:43:36.000
It's not like, oh my god, it's self-aware. I heard an interview recently with this

01:43:36.000 --> 01:43:39.600
philosopher from Yale, I can't remember his name, I apologize for that. But he was talking about,

01:43:39.600 --> 01:43:43.120
well, if these computers were self-aware, then it would be a crime done, plug them. And I'm like,

01:43:43.120 --> 01:43:50.160
oh, come on. I unplug myself every night, I go to sleep. Is that a crime? I plug myself in again

01:43:50.160 --> 01:43:55.920
in the morning, and there I am. So people get kind of bent out of shape about this.

01:43:55.920 --> 01:44:02.160
I have very definite, very detailed understanding or opinions about what it means to be conscious

01:44:02.160 --> 01:44:07.120
and what it means to be self-aware. I don't think it's that interesting a problem. You talk to

01:44:07.120 --> 01:44:12.160
Kristoff Koch, he thinks that's the only problem. I didn't actually listen to your interview with

01:44:12.240 --> 01:44:15.680
him, but I know him, and I know that's the thing he cares about.

01:44:15.680 --> 01:44:19.200
He also thinks intelligence and consciousness have disjoint. So I mean, it's not,

01:44:19.200 --> 01:44:23.600
you don't have to have one or the other. I disagree with that. I just totally disagree with that.

01:44:24.400 --> 01:44:28.160
So where's your thoughts and consciousness? Where does it emerge from? Because it is...

01:44:28.160 --> 01:44:32.000
So then we have to break it down to the two parts, okay? Because consciousness isn't one thing,

01:44:32.000 --> 01:44:35.440
that's part of the problem with that term. It means different things to different people,

01:44:35.440 --> 01:44:40.000
and there's different components of it. There is a concept of self-awareness, okay?

01:44:40.800 --> 01:44:47.360
That can be very easily explained. You have a model of your own body, the neocortex models

01:44:47.360 --> 01:44:54.000
things in the world, and it also models your own body. And then it has a memory. It can remember

01:44:54.000 --> 01:44:58.000
what you've done, okay? So it can remember what you did this morning, can remember what you had

01:44:58.000 --> 01:45:04.480
for breakfast, and so on. And so I can say to you, okay, Lex, were you conscious this morning when

01:45:04.560 --> 01:45:10.160
you had your bagel? And you'd say, yes, I was conscious. Now what if I could take your brain

01:45:10.160 --> 01:45:14.960
and revert all the synapses back to the state they were this morning? And then I said to you,

01:45:14.960 --> 01:45:18.640
Lex, were you conscious when you ate the bagel? And he said, no, I wasn't conscious. I said,

01:45:18.640 --> 01:45:23.200
here's a video of eating the bagel. And he said, I wasn't there. I have no... That's not possible

01:45:23.200 --> 01:45:27.360
because I must have been unconscious at that time. So we can just make this one-to-one correlation

01:45:27.360 --> 01:45:31.920
between memory of your body's trajectory through the world over some period of time,

01:45:31.920 --> 01:45:36.080
a memory of it. And the ability to recall that memory is what you would call conscious. I was

01:45:36.080 --> 01:45:42.160
conscious of that. It's a self-awareness. And any system that can recall, memorize what it's

01:45:42.160 --> 01:45:48.560
done recently, and bring that back and invoke it again would say, yeah, I'm aware. I remember what

01:45:48.560 --> 01:45:53.680
I did. All right, I got it. That's an easy one, although some people think that's a hard one.

01:45:54.640 --> 01:45:59.520
The more challenging part of consciousness is this is one that's sometimes used by the word

01:45:59.520 --> 01:46:08.240
qualia, which is, why does an object seem red? Or what is pain? And why does pain feel like

01:46:08.240 --> 01:46:13.200
something? Why do I feel redness? Or why do I feel a little painless in a way? And then I could say,

01:46:13.200 --> 01:46:16.800
well, why does sight seems different than hearing? That's the same problem. It's really,

01:46:17.360 --> 01:46:22.080
these are all just neurons. And so how is it that why does looking at you feel different than

01:46:22.960 --> 01:46:26.720
hearing you? It feels different, but there's just neurons in my head. They're all doing the same

01:46:26.800 --> 01:46:32.160
thing. So that's an interesting question. The best treatise I've read about this is by a guy named

01:46:32.160 --> 01:46:38.800
O'Regan. O'Regan, he wrote a book called Why Red Doesn't Sound Like a Bell. It's a little,

01:46:40.400 --> 01:46:46.640
it's not a trade book, easy to read, but it, and it's an interesting question. Take something like

01:46:46.640 --> 01:46:51.520
color. Color really doesn't exist in the world. It's not a property of the world. Property of the

01:46:51.520 --> 01:46:57.920
world that exists is light frequency. And that gets turned into we have certain cells in the retina

01:46:57.920 --> 01:47:01.280
that respond to different frequencies, different than others. And so when they enter the brain,

01:47:01.280 --> 01:47:06.720
you just have a bunch of axons that are firing at different rates. And from that, we perceive color.

01:47:06.720 --> 01:47:10.800
But there is no color in the brain. I mean, there's, there's no color coming in on those synapses.

01:47:10.800 --> 01:47:16.160
It's just a correlation between some, some, some axons and some property of frequency.

01:47:16.720 --> 01:47:20.560
And that isn't even color itself. Frequency doesn't have a color. It's just a,

01:47:21.280 --> 01:47:26.400
it's just what it is. So then the question is, well, why does it even appear to have a color at all?

01:47:27.840 --> 01:47:31.440
Just as you're describing it, there seems to be a connection to these, those ideas of reference

01:47:31.440 --> 01:47:40.080
frames. I mean, it just feels like consciousness having the subject, assigning the feeling of red

01:47:40.880 --> 01:47:47.920
to the actual color or to the wavelength is useful for intelligence.

01:47:47.920 --> 01:47:51.920
Yeah, I think that's a good way of putting it. It's useful as a predictive mechanism or useful

01:47:51.920 --> 01:47:56.720
as a generalization idea. It's a way of grouping things together to say it's useful to have a model

01:47:56.720 --> 01:48:04.800
like this. Think about the, the, the well-known syndrome that people who've lost a limb experience

01:48:04.800 --> 01:48:12.720
called phantom limbs. And what they claim is they can have their arms removed, but they feel

01:48:12.720 --> 01:48:17.680
the arm that not only feel it, they know it's there. They, it's there. I can, I know it's there.

01:48:17.680 --> 01:48:21.040
They'll swear to you that it's there and then they can feel pain in their arm and they'll

01:48:21.040 --> 01:48:24.880
feel it in their finger and if they move their, they move their non-existent arm behind their

01:48:24.880 --> 01:48:30.720
back, then they feel the pain behind their back. So this whole idea that your arm exists is a model

01:48:30.720 --> 01:48:37.520
of your brain. It may or may not really exist. And just like, but it's useful to have a model

01:48:37.520 --> 01:48:41.520
of something that sort of correlates to things in the world so you can make predictions about what

01:48:41.520 --> 01:48:45.200
would happen when those things occur. It's a little bit of a fuzzy, but I think you're getting

01:48:45.200 --> 01:48:50.960
quite towards the answer there. It's, it's useful for the model of to, to express things certain

01:48:50.960 --> 01:48:54.880
ways that we can then map them into these reference frames and make predictions about them.

01:48:55.680 --> 01:48:58.560
I need to spend more time on this topic. It doesn't bother me.

01:48:58.800 --> 01:49:04.640
Do you really need to spend more time? It does feel special that we have subjective experience,

01:49:04.640 --> 01:49:09.040
but I'm yet to know why. I'm just, I'm just personally curious. It's not,

01:49:09.040 --> 01:49:13.200
it's not necessary for the work we're doing here. I don't think I need to solve that problem to

01:49:13.200 --> 01:49:18.000
build intelligent machines at all, not at all. But there is sort of the silly notion that you

01:49:18.000 --> 01:49:24.000
described briefly that doesn't seem so silly to us humans is, you know, if you're successful

01:49:24.000 --> 01:49:31.120
building intelligent machines, it feels wrong to then turn them off. Because if you're able to

01:49:31.120 --> 01:49:37.920
build a lot of them, it feels wrong to then be able to, you know, to turn off the,

01:49:38.560 --> 01:49:42.960
Well, why, but just let's, let's break that down a bit. As humans, why do we fear death?

01:49:43.760 --> 01:49:47.920
There's, there's two reasons we fear death. Well, first of all, I'll say when you're

01:49:47.920 --> 01:49:53.040
dead, it doesn't matter. Oh, okay. You're dead. So why do we fear death? We fear death for two

01:49:53.040 --> 01:49:58.960
reasons. One is because we are our program genetically to fear death. That's a, that's a

01:49:58.960 --> 01:50:05.360
survival and prop beginning of the genes thing. And we also are programmed to feel sad when people

01:50:05.360 --> 01:50:09.360
we know die. We don't feel sad for someone we don't know dies. There's people dying right now,

01:50:09.360 --> 01:50:12.080
they're only scared to say, I'm feel bad about them because I don't know them. But I knew them,

01:50:12.080 --> 01:50:18.960
I'd feel really bad. So again, this, these are old brain genetically embedded things that we

01:50:19.040 --> 01:50:25.200
fear death. There's outside of those, those uncomfortable feelings. There's nothing else

01:50:25.200 --> 01:50:30.080
to worry about. Wait, wait, hold on a second. Do you know the denial of death by Becker?

01:50:31.040 --> 01:50:33.760
You know, there's a thought that death is,

01:50:36.320 --> 01:50:44.560
you know, our whole conception of our world model kind of assumes immortality. And then death is

01:50:44.560 --> 01:50:49.600
this terror that underlies it all. So like, well, some people's world model, not mine.

01:50:50.320 --> 01:50:54.400
But okay, so what, what Becker would say is that you're just living in an illusion,

01:50:54.400 --> 01:50:59.520
you've constructed illusion for yourself, because it's such a terrible terror. The fact that

01:51:00.080 --> 01:51:04.240
what's the illusion, the illusion that death doesn't matter, you're still not coming to grips

01:51:04.240 --> 01:51:09.760
with the illusion of what that death is going to happen. Oh, like it's not going to happen.

01:51:10.640 --> 01:51:14.160
You're actually operating. You haven't, even though you said you've accepted it,

01:51:14.160 --> 01:51:17.680
you haven't really accepted the notion of death is what he was saying. So it sounds like,

01:51:19.520 --> 01:51:21.600
it sounds like you disagree with that notion. I mean,

01:51:21.600 --> 01:51:24.240
Yeah, yeah, totally. I, like, I,

01:51:24.240 --> 01:51:27.120
So death is not that such an important. Every night, every night I go to bed,

01:51:27.120 --> 01:51:30.640
it's like dying. With little deaths. It's full of death. And if I didn't wake up,

01:51:31.600 --> 01:51:35.280
it wouldn't matter to me. Only if I knew that was going to happen would it be bothersome. But I

01:51:35.280 --> 01:51:39.360
didn't know it was going to happen. How would I know? Then I would worry about my wife.

01:51:39.440 --> 01:51:44.000
So imagine, imagine I was a loner and I lived in Alaska and I lived them

01:51:44.000 --> 01:51:47.760
out there and there was no animals. Nobody knew I existed. I was just eating these roots all the

01:51:47.760 --> 01:51:55.920
time and nobody knew I was there. And one day I didn't wake up. What pain in the world would there

01:51:55.920 --> 01:52:01.920
exist? Well, so most people that think about this problem would say that you're just deeply enlightened

01:52:01.920 --> 01:52:09.840
or are completely delusional. But I would say, I would say that's a very enlightened

01:52:10.400 --> 01:52:15.760
way to see the world is that that's the rational one. Well, I think it's rational. That's right.

01:52:15.760 --> 01:52:23.440
But the fact is we don't, I mean, we really don't have an understanding of why the heck it is we're

01:52:23.440 --> 01:52:27.840
born and why we die and what happens after we die. Well, maybe there isn't a reason, maybe there is.

01:52:27.840 --> 01:52:32.240
So I'm interested in those big problems too, right? You know, you interviewed Max Tagmark,

01:52:32.240 --> 01:52:35.280
you know, and there's people like that, right? I'm interested in those big problems as well.

01:52:35.280 --> 01:52:41.600
And in fact, when I was young, I made a list of the biggest problems I could think of. First,

01:52:41.600 --> 01:52:47.040
why does anything exist? Second, why did we have the laws of physics that we have? Third,

01:52:47.680 --> 01:52:53.040
is life inevitable? And why is it here? Fourth, is intelligence inevitable? And why is it here?

01:52:53.040 --> 01:52:57.120
I stopped there because I figured if you can make a truly intelligent system,

01:52:57.840 --> 01:53:00.560
we'll be, that'll be the quickest way to answer the first three questions.

01:53:03.200 --> 01:53:08.640
I'm serious. And so I said, my mission, you know, you asked me earlier, my first mission is

01:53:08.640 --> 01:53:12.160
to understand the brain, but I felt that is the shortest way to get to true machine intelligence.

01:53:12.160 --> 01:53:15.920
And I want to get to true machine intelligence because even if it doesn't occur in my lifetime,

01:53:15.920 --> 01:53:19.600
other people will benefit from it because I think it'll occur in my lifetime, but you know,

01:53:19.600 --> 01:53:26.720
20 years, you never know. And but that will be the quickest way for us to, you know, we can make

01:53:26.720 --> 01:53:32.880
super mathematicians, we can make super space explorers, we can make super physicists brains

01:53:32.880 --> 01:53:38.640
that do these things, and that can run experiments that we can't run, we don't have the abilities

01:53:38.640 --> 01:53:42.560
to manipulate things and so on. But we can build and tell the machines to do all those things.

01:53:42.560 --> 01:53:47.360
And with the ultimate goal of finding out the answers to the other questions.

01:53:47.600 --> 01:53:54.800
Let me ask you another depressing and difficult question, which is, once we achieve that goal,

01:53:56.080 --> 01:54:02.960
do you, of creating, no, of understanding intelligence, do you think we would be happier

01:54:02.960 --> 01:54:06.720
and more fulfilled as a species? Understanding intelligence or understanding the answers to

01:54:06.720 --> 01:54:12.880
the big questions? Understanding intelligence. Totally. Totally. It would be a far more fun

01:54:12.880 --> 01:54:18.080
place to live. You think so? Oh, yeah, why not? I mean, you know, just put aside this, you know,

01:54:18.080 --> 01:54:24.880
terminator nonsense and, and, and, and just think about, you can think about the, we can talk about

01:54:24.880 --> 01:54:30.160
the risk of AI if you want. I'd love to. So let's talk about. But I think the world is far better

01:54:30.160 --> 01:54:34.080
knowing things. We're always better than no things. Do you think it's better? Is it a better place to

01:54:34.080 --> 01:54:38.880
live in that I know that our planet is one of many in the solar system and the solar system is one

01:54:38.880 --> 01:54:43.280
of many of the galaxies? I think it's a more, I, I dread, I used to, I sometimes think like,

01:54:43.280 --> 01:54:47.360
God, what would it be like 300 years ago? I'd be looking up the sky. I can't understand anything.

01:54:47.360 --> 01:54:51.120
Oh my God, I'd be like going to bed every night going, what's going on here? Well, I mean, in

01:54:51.120 --> 01:54:56.720
some sense, I agree with you, but I'm not exactly sure. So I'm also a scientist. So I have, I share

01:54:56.720 --> 01:55:03.360
your views, but I'm not, we're, we're like rolling down the hill together. What's down the hill?

01:55:03.360 --> 01:55:07.520
I feel like we're climbing a hill. Whatever. We're getting, we're getting closer to enlightenment.

01:55:07.760 --> 01:55:13.200
Whatever. We're climbing, we're getting pulled up a hill by our curiosity.

01:55:13.200 --> 01:55:16.960
We are putting, our polarity is pulling, we're pulling ourselves up the hill by our curiosity.

01:55:16.960 --> 01:55:23.120
Yeah. Sisyphus are doing the same thing with the rock. Yeah. But okay, our happiness aside,

01:55:23.120 --> 01:55:30.480
do you have concerns about, you know, you talk about Sam Harris, Elon Musk, of existential threats

01:55:30.480 --> 01:55:34.240
of intelligence systems? No, I'm not worried about existential threats at all. There are,

01:55:34.240 --> 01:55:37.680
there are some things we really do need to worry about. Even today's AI, we have things we have

01:55:37.680 --> 01:55:42.640
to worry about. We have to worry about privacy and about how impacts false beliefs in the world.

01:55:42.640 --> 01:55:47.680
And, and we have real problems that, and things to worry about with today's AI.

01:55:48.480 --> 01:55:52.000
And that will continue as we create more intelligent systems. There's no question, you

01:55:52.000 --> 01:55:57.440
know, the whole issue about, you know, making intelligent armaments and weapons is something

01:55:57.440 --> 01:56:01.760
that really we have to think about carefully. I don't think of those as existential threats.

01:56:01.760 --> 01:56:06.400
I think those are the kind of threats we always face, and we'll have to face them here and, and,

01:56:07.280 --> 01:56:12.320
and we'll have to deal with them. The, we can, we could talk about what people think are the

01:56:12.320 --> 01:56:17.360
existential threats. But when I hear people talking about them, they all sound hollow to me.

01:56:17.360 --> 01:56:21.360
They're, they're based on ideas, they're based on people who really have no idea what intelligence

01:56:21.360 --> 01:56:27.440
is. And, and if they knew what intelligence was, they wouldn't say those things. So those are not

01:56:27.440 --> 01:56:32.640
experts in the field, you know. So yeah, so there's two, right? There's, so one is like

01:56:32.640 --> 01:56:43.040
super intelligent. So a system that becomes far, far superior in reasoning ability than us humans.

01:56:43.040 --> 01:56:49.840
How is that an existential threat? Then, so there's a lot of ways in which it could be. One way is

01:56:50.640 --> 01:56:56.000
us humans are actually irrational, inefficient, and get in the way of,

01:56:57.520 --> 01:57:03.600
of not happiness, but whatever the objective function is of maximizing that objective function.

01:57:03.600 --> 01:57:04.880
Yeah. Yeah. Super intelligent.

01:57:04.880 --> 01:57:06.480
There's a paperclip problem and things like that.

01:57:06.480 --> 01:57:09.360
But so the paperclip problem, but with a super intelligent.

01:57:09.360 --> 01:57:13.840
Yeah. Yeah. Yeah. So we already faced this threat in some sense.

01:57:14.480 --> 01:57:20.400
They're called bacteria. These are organisms in the world that would like to turn everything into

01:57:20.400 --> 01:57:25.680
bacteria. And they're constantly morphing. They're constantly changing to evade our protections.

01:57:26.320 --> 01:57:33.200
And in the past, they have killed huge swaths of populations of humans on this planet.

01:57:33.200 --> 01:57:37.440
So if you want to worry about something that's going to multiply endlessly, we have it.

01:57:38.320 --> 01:57:43.200
And I'm far more worried in that regard, I'm far more worried that some scientists in a laboratory

01:57:43.200 --> 01:57:48.400
will create a super virus or a super bacteria that we cannot control. That is a more existential

01:57:48.400 --> 01:57:53.840
threat. Putting an intelligence thing on top of it actually seems to make it less existential

01:57:53.840 --> 01:57:58.480
to me. It's like, it limits its power. It limits where it can go. It limits the number of things

01:57:58.480 --> 01:58:03.920
it can do in many ways. A bacteria is something you can't even see. So that's only one of those

01:58:03.920 --> 01:58:09.680
problems. Yes, exactly. So the other one, just in your intuition about intelligence, when you

01:58:09.680 --> 01:58:15.600
think about the intelligence of us humans, do you think of that as something, if you look at

01:58:15.600 --> 01:58:22.000
intelligence on a spectrum from zero to us humans, do you think you can scale that to something far

01:58:22.000 --> 01:58:25.280
superior? Yeah, all the mechanisms we've been talking about. Let me, I want to make another

01:58:25.280 --> 01:58:31.440
point here, Alex, before I get there. Sure. Intelligence is the neocortex. It is not the

01:58:31.440 --> 01:58:38.240
entire brain. If I, the goal is not to make a human. The goal is not to make an emotional system.

01:58:38.240 --> 01:58:42.640
The goal is not to make a system that wants to have sex and reproduce. Why would I build that?

01:58:42.640 --> 01:58:45.840
If I want to have a system that wants to reproduce and have sex, make bacteria,

01:58:45.840 --> 01:58:50.960
make computer viruses. Those are bad things. Don't do that. Those are really bad. Don't do

01:58:50.960 --> 01:58:56.880
those things. Regulate those. But if I just say I want an intelligent system, why doesn't have to

01:58:56.880 --> 01:59:01.520
have any of the human-like emotions? Why does it even care if it lives? Why does it even care

01:59:01.520 --> 01:59:06.240
if it has food? It doesn't care about those things. It's just, you know, it's just in a trance

01:59:06.240 --> 01:59:11.600
thinking about mathematics or it's out there just trying to build the space, you know, for it on

01:59:11.600 --> 01:59:18.080
Mars. It's a, we, that's a choice we make. Don't make human-like things. Don't make replicating

01:59:18.080 --> 01:59:21.600
things. Don't make things that have emotions. Just stick to the neocortex. So that's, that's a

01:59:21.600 --> 01:59:26.480
view actually that I share, but not everybody shares in the sense that you have faith and

01:59:26.480 --> 01:59:33.520
optimism about us as engineers of systems, humans as builders of systems to, to, to not put in

01:59:34.080 --> 01:59:38.960
stupid, not stupid. So this is why, this is why I mentioned the bacteria one. Because you might

01:59:38.960 --> 01:59:43.280
say, well, some person's going to do that. Well, some person today could create a bacteria that's

01:59:43.280 --> 01:59:49.840
resistant to all the non-antibacterial agents. So we already have that threat. We already know

01:59:49.840 --> 01:59:55.920
this is going on. It's not a new threat. So just accept that and then we have to deal with it,

01:59:55.920 --> 02:00:01.120
right? Yeah. So my point is nothing to do with intelligence. It, intelligence is a separate

02:00:01.200 --> 02:00:05.200
component that you might apply to a system that wants to reproduce and do stupid things.

02:00:05.840 --> 02:00:09.600
Let's not do that. Yeah. In fact, it is a mystery why people haven't done that yet.

02:00:10.400 --> 02:00:16.480
My, my dad as a physicist believes that the reason, for example, nuclear weapons haven't

02:00:17.120 --> 02:00:23.280
proliferated amongst evil people. So one, one belief that I share is that there's not that many

02:00:23.280 --> 02:00:30.400
evil people in the world that would, that, that would use back to whether it's bacteria,

02:00:30.400 --> 02:00:36.720
nuclear weapons, or maybe the future AI systems to do bad. So the fraction is small. And the second

02:00:36.720 --> 02:00:42.560
is that it's actually really hard, technically. So the, the intersection between evil and

02:00:42.560 --> 02:00:47.600
competent is small in terms. And by the way, to really annihilate humanity, you'd have to have,

02:00:48.240 --> 02:00:52.160
you know, sort of the, the nuclear winter phenomenon, which is not one person shooting,

02:00:52.160 --> 02:00:57.600
you know, or even 10 bombs, you'd have to have some automated system that, you know, detonates

02:00:57.600 --> 02:01:02.400
a million bombs or 10, whatever many thousands we have. So it's extreme evil combined with extreme

02:01:02.400 --> 02:01:07.200
competence and just building some stupid system that would automatically, you know, Dr. Strangelup

02:01:07.200 --> 02:01:13.760
type of thing, you know, I mean, look, we could have some nuclear bomb go off in some major city

02:01:13.760 --> 02:01:17.520
in the world. Like, I think that's actually quite likely even in my lifetime. I don't think that's

02:01:17.680 --> 02:01:23.840
unlike the thing. And it'll be a tragedy. But it won't be an existential threat. And it's the same

02:01:23.840 --> 02:01:31.600
as, you know, the virus of 1917, whenever it was, you know, the influenza, these bad things can happen

02:01:31.600 --> 02:01:36.960
and the plague and so on. We can't always prevent it. We always, to always try, but we can't. But

02:01:36.960 --> 02:01:40.320
they're not existential threats until we combine all those crazy things together.

02:01:41.040 --> 02:01:46.240
So on the, on the spectrum of intelligence from zero to human, do you have a sense of

02:01:46.960 --> 02:01:52.880
whether it's possible to create several orders of magnitude or at least double that

02:01:53.520 --> 02:01:55.840
of human intelligence, talking about New York context?

02:01:55.840 --> 02:02:00.080
I think it's the wrong thing to say double the intelligence. Break it down into different

02:02:00.080 --> 02:02:04.880
components. Can I make something that's a million times faster than a human brain? Yes,

02:02:04.880 --> 02:02:11.200
I can do that. Could I make something that is, has a lot more storage than human brain? Yes,

02:02:11.200 --> 02:02:14.720
I could do that. More common, more copies come. Can I make something that attaches to

02:02:14.800 --> 02:02:19.280
different sensors than human brain? Yes, I can do that. Could I make something that's distributed?

02:02:19.280 --> 02:02:22.960
So these people, yeah, we talked earlier about the important New York Cortex voting,

02:02:22.960 --> 02:02:26.160
they don't have to be co-located. Like, you know, they can be all around the places. I could do that,

02:02:26.160 --> 02:02:33.360
too. Those are the levers I have, but is it more intelligent? What depends what I train

02:02:33.360 --> 02:02:39.360
in on? What is it doing? Well, so here's the thing. So let's say larger in New York Cortex

02:02:39.360 --> 02:02:48.480
and or whatever size that allows for higher and higher hierarchies to form. We're talking about

02:02:48.480 --> 02:02:51.920
reference frames and concepts. So I could, could I have something that's a super physicist or

02:02:51.920 --> 02:02:57.120
a super mathematician? Yes. And the question is, once you have a super physicist, will they be

02:02:57.120 --> 02:03:03.040
able to understand something? Do you have a sense that it will be orders like us compared to ants?

02:03:03.040 --> 02:03:11.200
Could we ever understand it? Yeah. Most people cannot understand general relativity.

02:03:11.840 --> 02:03:15.760
It's a really hard thing to get. I mean, you can paint it in a fuzzy picture,

02:03:15.760 --> 02:03:21.280
stretchy space, you know? Yeah. But the field equations to do that in the deep intuitions

02:03:21.280 --> 02:03:27.520
are really, really hard. And I've tried, I'm unable to do it. Like, easy to get, you know,

02:03:27.520 --> 02:03:30.240
it's easy to get special relative, but general relative, man, that's too much.

02:03:32.320 --> 02:03:36.800
And so we already live with this to some extent. The vast majority of people can't understand

02:03:36.800 --> 02:03:41.200
actually what the vast majority of other people actually know. We're just either we don't have

02:03:41.200 --> 02:03:45.600
the effort to or we can't or we don't have time or just not smart enough, whatever. So,

02:03:46.800 --> 02:03:50.800
but we have ways of communicating. Einstein has spoken in a way that I can understand.

02:03:51.520 --> 02:03:57.120
He's given me analogies that are useful. I can use those analogies from my own work and think

02:03:57.120 --> 02:04:03.440
about, you know, concepts that are similar. It's not stupid. It's not like he's exist in some

02:04:03.440 --> 02:04:08.320
of the plane. There's no connection to my plane in the world here. So that will occur. It already

02:04:08.320 --> 02:04:12.800
has occurred. That's my point that this story is it already has occurred. We live it every day.

02:04:14.400 --> 02:04:18.320
One could argue that with we create machine intelligence that think a million times faster

02:04:18.320 --> 02:04:22.240
than us that it'll be so far, we can't make the connections. But, you know, at the moment,

02:04:23.200 --> 02:04:27.280
everything that seems really, really hard to figure out in the world when you actually figure

02:04:27.280 --> 02:04:32.320
it out is not that hard. You know, we can almost everyone can understand the multiverses. Almost

02:04:32.320 --> 02:04:35.920
everyone can understand quantum physics. Almost everyone can understand these basic things,

02:04:35.920 --> 02:04:40.960
even though hardly any people could figure those things out. Yeah, but really understand. So,

02:04:42.160 --> 02:04:45.360
only a few people really don't understand. You need to only understand the

02:04:46.320 --> 02:04:50.800
the projections, the sprinkles of the useful insights from that. That was my example of

02:04:50.800 --> 02:04:55.200
Einstein, right? His general theory of relativity is one thing that very, very, very few people

02:04:55.200 --> 02:04:59.680
can get. And what if we just said those other few people are also artificial intelligences?

02:05:00.480 --> 02:05:05.120
How bad is that? In some sense, they are, right? Yeah, they say already. I mean, Einstein wasn't

02:05:05.120 --> 02:05:09.360
a really normal person. He had a lot of weird quirks. And so the other people who work with him.

02:05:09.360 --> 02:05:13.440
So, you know, maybe they already were sort of this astral plane of intelligence that

02:05:14.160 --> 02:05:19.120
we live with it already. It's not a problem. It's still useful and, you know.

02:05:20.080 --> 02:05:24.000
So, do you think we are the only intelligent life out there in the universe?

02:05:24.800 --> 02:05:30.240
I would say that intelligent life has and will exist elsewhere in the universe. I'll say that.

02:05:31.360 --> 02:05:35.440
There is a question about contemporaneous intelligence life, which is hard to even answer

02:05:35.440 --> 02:05:40.720
when we think about relativity and the nature of space time. We can't say what exactly is this

02:05:40.720 --> 02:05:48.000
time someplace else in the world. But I think it's, you know, I do worry a lot about the filter

02:05:48.000 --> 02:05:55.120
idea, which is that perhaps intelligent species don't last very long. And so we haven't been around

02:05:55.120 --> 02:05:59.760
very long. And as a technological species, we've been around for almost nothing, right? You know,

02:05:59.760 --> 02:06:05.360
what 200 years or something like that. And we don't have any data, a good data point on whether

02:06:05.360 --> 02:06:10.640
it's likely that we'll survive or not. So, do I think that there have been intelligent

02:06:10.640 --> 02:06:14.880
life elsewhere in the universe? Almost certainly, of course. In the past and the future, yes.

02:06:16.320 --> 02:06:21.040
Does it survive for a long time? I don't know. This is another reason I'm excited about our work,

02:06:21.040 --> 02:06:27.120
is our work meaning the general world of AI. I think we can build intelligent machines

02:06:28.640 --> 02:06:35.040
that outlast us. And, you know, they don't have to be tied to earth. They don't have to,

02:06:35.040 --> 02:06:39.600
you know, I'm not saying they're recreating, you know, you know, aliens. I'm just saying

02:06:40.800 --> 02:06:44.960
if I asked myself, and this might be a good point to end on here. If I asked myself, you know,

02:06:44.960 --> 02:06:49.760
what's special about our species? We're not particularly interesting physically. We're not,

02:06:49.760 --> 02:06:53.840
we don't fly. We're not good swimmers. We're not very fast. We're not very strong, you know.

02:06:53.840 --> 02:06:57.760
It's our brain. That's the only thing. And we are the only species on this planet that's built

02:06:57.760 --> 02:07:02.320
the model of the world that extends beyond what we can actually sense. We're the only people who

02:07:02.320 --> 02:07:07.280
know about the far side of the moon and the other universes and other galaxies and other stars and

02:07:08.400 --> 02:07:13.200
what happens in the atom. That knowledge doesn't exist anywhere else. It's only in our heads.

02:07:13.760 --> 02:07:18.240
Cats don't do it. Dogs don't do it. Monkeys don't do it. That is what we've created that's unique.

02:07:18.240 --> 02:07:24.400
Not our genes. It's knowledge. And if I ask me, what is the legacy of humanity? What should our

02:07:24.400 --> 02:07:28.880
legacy be? It should be knowledge. We should preserve our knowledge in a way that it can exist

02:07:28.880 --> 02:07:33.680
beyond us. And I think the best way of doing that, in fact, you have to do it, is that it has to go

02:07:33.680 --> 02:07:39.200
along with intelligent machines to understand that knowledge. That's a very broad idea,

02:07:39.840 --> 02:07:44.320
but we should be thinking, I call it a state planning for humanity. We should be thinking

02:07:44.320 --> 02:07:49.760
about what we want to leave behind when as a species we're no longer here. And that'll happen

02:07:49.760 --> 02:07:54.400
sometime. Sooner or later, it's going to happen. And understanding intelligence and creating

02:07:54.400 --> 02:07:59.840
intelligence gives us a better chance to prolong. It does give us a better chance to prolong life,

02:08:00.400 --> 02:08:05.360
yes. It gives us a chance to live on other planets. But even beyond that, I mean, our

02:08:05.360 --> 02:08:10.560
solar system will disappear one day. It's given enough time. So I don't know. I doubt we will

02:08:10.560 --> 02:08:16.320
ever be able to travel to other things, but we could tell the stars, but we could send intelligent

02:08:16.320 --> 02:08:24.320
machines to do that. So you have an optimistic, a hopeful view of our knowledge of the

02:08:24.320 --> 02:08:29.120
echoes of human civilization living through the intelligent systems we create.

02:08:29.120 --> 02:08:32.160
Oh, totally. Well, I think the intelligent systems are greater in some sense, the

02:08:32.720 --> 02:08:38.480
vessel for bringing them beyond Earth or making them last beyond humans themselves.

02:08:39.040 --> 02:08:43.600
So... And how do you feel about that? That they won't be human, quote unquote.

02:08:43.600 --> 02:08:47.920
Okay, it's not. But human, what is human? Our species are changing all the time.

02:08:48.560 --> 02:08:53.840
Human today is not the same as human just 50 years ago. It's, what is human? Do we care about

02:08:53.840 --> 02:08:58.320
our genetics? Why is that important? As I point out, our genetics are no more interesting than

02:08:58.320 --> 02:09:02.880
a bacterium's genetics. It's no more interesting than a monkey's genetics. What we have, what's

02:09:02.880 --> 02:09:08.640
unique and what's valuable is our knowledge, what we've learned about the world. And that

02:09:08.640 --> 02:09:13.280
is the rare thing. That's the thing we want to preserve. We care about our genes.

02:09:15.360 --> 02:09:19.520
It's the knowledge. It's the knowledge. That's a really good place to end. Thank you so much

02:09:19.520 --> 02:09:21.520
for talking to me. Oh, it was fun.

