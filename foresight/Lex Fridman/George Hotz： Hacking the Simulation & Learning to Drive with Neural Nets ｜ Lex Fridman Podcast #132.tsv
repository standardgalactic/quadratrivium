start	end	text
0	6560	The following is a conversation with George Hotz, a.k.a. Geohot, his second time on the podcast.
6560	12880	He's the founder of Kamma AI, an autonomous and semi-autonomous vehicle technology company
12880	21360	that seeks to be, to Tesla autopilot, what Android is, to the iOS. They sell the Kamma 2 device for
21360	27040	$1,000 that, when installed in many of their supported cars, can keep the vehicle centered
27040	33520	in the lane even when there are no lane markings. It includes driver sensing that ensures that the
33520	39440	driver's eyes are on the road. As you may know, I'm a big fan of driver sensing. I do believe Tesla
39440	45520	Autopilot and others should definitely include it in their sensor suite. Also, I'm a fan of Android
45520	52400	and a big fan of George, for many reasons, including his non-linear out-of-the-box brilliance and the
52400	58480	fact that he's a superstar programmer of a very different style than myself. Styles make fights
59040	64480	and styles make conversations, so I really enjoyed this chat and I'm sure we'll talk many more times
64480	70160	on this podcast. Quick mention of a sponsor followed by some thoughts related to the episode.
70160	77520	First is ForSigmatic, the maker of delicious mushroom coffee. Second is the coding digital,
77520	84480	a podcast on tech and entrepreneurship that I listen to and enjoy. And finally, ExpressVPN,
84480	90000	the VPN I've used for many years to protect my privacy on the internet. Please check out the
90000	95760	sponsors in the description to get a discount and to support this podcast. As a side note,
95760	101840	let me say that my work at MIT on autonomous and semi-autonomous vehicles led me to study the human
101840	107680	side of autonomy enough to understand that it's a beautifully complicated and interesting problem
107680	114080	space, much richer than what can be studied in the lab. In that sense, the data that comma AI,
114080	119600	Tesla Autopilot, and perhaps others like Cadillac Supercruiser Collecting, gives us a chance to
119600	126320	understand how we can design safe semi-autonomous vehicles for real human beings in real-world
126320	132960	conditions. I think this requires bold innovation and a serious exploration of the first principles
132960	138400	of the driving task itself. If you enjoyed this thing, subscribe on YouTube, review it with
138400	144320	five stars and up a podcast, follow on Spotify, support on Patreon, or connect with me on Twitter
144320	152800	at Lex Freedman. And now here's my conversation with George Hotz. So last time we started talking
152800	157520	about the simulation, this time let me ask you, do you think there's intelligent life out there in
157520	164000	the universe? I always maintained my answer to the Fermi paradox. I think there has been intelligent
164000	168800	life elsewhere in the universe. So intelligent civilizations existed, but they've blown themselves
168800	175760	up. So your general intuition is that intelligent civilizations quickly, like there's that parameter
175760	181360	in the Drake equation, your sense is they don't last very long. Yeah. How are we doing on that?
181840	187760	Have we lasted pretty good? Oh, no. How do we do? Oh, yeah. I mean, not quite yet.
189040	195360	Well, it's only as Yukowski, IQ required to destroy the world falls by one point every year.
195360	200160	Okay. So technology democratizes the destruction of the world.
200960	202320	When can a meme destroy the world?
205200	207200	It kind of is already, right?
207200	213040	Somewhat. I don't think we've seen anywhere near the worst of it yet. World's going to get weird.
213840	219360	Well, maybe a meme can save the world. We thought about that, the meme Lord Elon Musk fighting on
219360	226880	the side of good versus the meme Lord of the darkness, which is not saying anything bad about
226880	232960	Donald Trump, but he is the Lord of the meme on the dark side. He's a Darth Vader of memes.
233680	239760	I think in every fairy tale, they always end it with, and they lived happily ever after,
239760	244320	and I'm like, please tell me more about this happily ever after. I've heard 50% of marriages
244320	249360	end in divorce. Why doesn't your marriage end up there? You can't just say happily ever after. So
250880	256320	the thing about destruction is it's over after the destruction. We have to do everything right
256320	262000	in order to avoid it. And one thing wrong, I mean, actually, this is what I really like about
262000	265680	cryptography. Cryptography, it seems like we live in a world where the defense wins.
267760	273280	Versus nuclear weapons, the opposite is true. It is much easier to build a warhead that splits
273280	279440	into 100 little warheads than to build something that can take out 100 little warheads. The offense
279440	286960	has the advantage there. So maybe our future is in crypto. So cryptography, right? The Goliath
286960	295920	is the defense. And then all the different hackers are the Davids, and that equation is flipped for
295920	302000	nuclear war. Because there's so many, like one nuclear weapon destroys everything, essentially.
302000	308400	Yeah, and it is much easier to attack with a nuclear weapon than it is to like, the technology
308400	313120	required to intercept and destroy a rocket is much more complicated than the technology required to
313120	321040	just orbital trajectory send a rocket to somebody. Okay, your intuition that there were intelligent
321040	326880	civilizations out there, but it's very possible that they're no longer there. It's kind of a sad
326880	334240	picture. They enter some steady state. They all wirehead themselves. What's wirehead? Stimulate
334240	342480	their platter centers and just live forever in this kind of stasis. They become, well, I mean,
342480	349120	I think the reason I believe this is because where are they? If there's some reason they stopped
349120	353280	expanding, because otherwise they would have taken over the universe. The universe isn't that big.
353280	357440	Or at least, you know, let's just talk about the galaxy, right? 70,000 light years across.
358480	364560	I took that number from Star Trek Voyager. I don't know how true it is. But yeah, that's not big.
364560	370080	Right? 70,000 light years is nothing. For some possible technology that you can imagine that
370240	373920	leverage like wormholes or something like that. You don't even need wormholes. Just a von Neumann
373920	379120	probe is enough. A von Neumann probe and a million years of sub light travel, and you'd have taken
379120	385040	over the whole universe. That clearly didn't happen. So something stopped it. So you mean a few,
385040	390640	right, for like a few million years, if you sent out probes that travel close, what's sub light?
390640	395600	You mean close to the speed of light? Let's say 0.1c. And it just spreads. Interesting. Actually,
395600	401840	that's an interesting calculation. So what makes you think that we'd be able to communicate with
401840	408880	them? Like, yeah, what's why do you think we would be able to be able to comprehend
408880	415440	intelligent lies that are out there? Like, even if they were among us kind of thing? Like,
415440	422880	or even just flying around? Well, I mean, that's possible. It's possible that there is some sort
422880	427840	of prime directive that'd be a really cool universe to live in. And there's some reason
427840	434000	they're not making themselves visible to us. But it makes sense that they would use the same,
435040	438960	well, at least the same entropy. Well, you're implying the same laws of physics. I don't know
438960	443440	what you mean by entropy in this case. Oh, yeah. I mean, if entropy is the scarce resource in the
443440	448800	universe. So what do you think about like Steven Wolfram and everything is a computation? And
448880	453680	then what if they are traveling through this world of computation? So if you think of the
453680	461440	universe as just information processing, then what you're referring to with entropy, and then
461440	466240	these pockets of interesting complex computation swimming around, how do we know they're not
466240	473440	already here? How do we know that this, like all the different amazing things that are full of
473440	480560	mystery on earth are just like little footprints of intelligence from light years away?
481520	486880	Maybe. I mean, I tend to think that as civilizations expand, they use more and more energy.
487680	491680	And you can never overcome the problem of waste heat. So where is their waste heat?
491680	495520	So we'd be able to, with our crude methods, be able to see like, there's a whole lot of
497200	502080	energy here. But it could be something we're not, I mean, we don't understand dark energy,
502400	507360	dark matter. It could be just stuff we don't understand at all. Or they can have a fundamentally
507360	513360	different physics, you know, like that we just don't even comprehend. Well, I think, okay,
513360	516960	I mean, it depends how far out you want to go. I don't think physics is very different on the
516960	523520	other side of the galaxy. I would suspect that they have, I mean, if they're in our universe,
523520	528800	they have the same physics. Well, yeah, that's the assumption we have. But there could be like
528800	537840	super trippy things like, like our cognition only gets to a slice, and all the possible
537840	541760	instruments that we can design only get to a particular slice of the universe. And there's
541760	549280	something much like weirder. Maybe we can try a thought experiment. Would people from the past
549920	556400	be able to detect the remnants of our, we'll be able to detect our modern civilization?
556400	560560	And I think the answer is obviously yes. You mean past from a hundred years ago?
560560	563200	Well, let's even go back further. Let's go to a million years ago.
564320	568400	The humans who were lying around in the desert probably didn't even have, maybe they just barely
568400	581840	had fire. They would understand if a 747 flew overhead. In this vicinity, but not if a 747 flew
581840	586320	on Mars. Like, because they wouldn't be able to see far, because we're not actually communicating
586320	592640	that well with the rest of the universe. We're doing okay, just sending out random like 50s tracks
592640	599120	of music. True. And yeah, I mean, they'd have to, you know, the, we've only been broadcasting radio
599120	607920	waves for 150 years. And well, there's your light cone. So yeah, okay. What do you make about all
608000	615760	the, I recently came across this, having talked to David's favor. I don't know if you caught
615760	622800	what the videos that Pentagon released and the New York Times reporting of the UFO sightings.
623440	631600	So I kind of looked into it, quote unquote. And there's actually been like hundreds of thousands
631600	637040	of UFO sightings, right? And a lot of it, you can explain it way in different kinds of ways.
637040	643120	So one is it could be interesting physical phenomena. Two, it could be people wanting to
643120	647680	believe. And therefore they conjure up a lot of different things that just, you know, when you
647680	653600	see different kinds of lights, some basic physics phenomena, and then you just conjure up ideas
653600	660000	of possible out there mysterious worlds. But, you know, it's also possible like you have a case of
660960	668880	David Fravor, who is a Navy pilot, who's, you know, as legit as it gets in terms of humans who
668880	676160	are able to perceive things in the environment and make conclusions, whether those things are
676160	683280	a threat or not. And he and several other pilots saw a thing, I don't know if you followed this,
683280	688320	but they saw a thing that they've since then called TikTok that moved in all kinds of weird ways.
689280	696480	They don't know what it is. It could be technology developed by the United States,
696480	700560	and they're just not aware of it and the surface level from the Navy, right? It could be
700560	705120	different kind of lighting technology or drone technology, all that kind of stuff. It could
705120	711280	be the Russians and the Chinese, all that kind of stuff. And of course their mind, our mind,
711280	717280	can also venture into the possibility that it's from another world. Have you looked into this at
717280	724960	all? What do you think about it? I think all the news is a scythe. I think that the most plausible...
724960	733120	Nothing is real. Yeah, I listened to the, I think it was Bob Lazar on Joe Rogan. And like,
733120	738000	I believe everything this guy is saying. And then I think that it's probably just some like MK
738000	744640	Ultra kind of thing, you know? What do you mean? Like, they, you know, they made some weird thing
744640	748320	and they called it an alien spaceship. You know, maybe it was just to like stimulate young
748320	752080	physicists' minds and tell them it's alien technology and we'll see what they come up with,
752080	757840	right? Do you find any conspiracy theories compelling? Like, have you pulled at the
757840	762960	string of the, of the rich complex world of conspiracy theories that's out there?
763760	768960	I think that I've heard a conspiracy theory that conspiracy theories were invented by the CIA in
768960	778480	the 60s to discredit true things. Yeah. So, you know, you can go to ridiculous conspiracy theories
778480	786880	like Flat Earth and Pizza Gate and, you know, these things are almost to hide like conspiracy
786880	790480	theories that like, you know, remember when the Chinese like locked up the doctors who discovered
790480	794560	coronavirus? Like I tell people this and I'm like, no, no, no, that's not a conspiracy theory. That
794560	798720	actually happened. Do you remember the time that the money used to be backed by gold and now it's
798720	804480	backed by nothing? This is not a conspiracy theory. This actually happened. Well, that's one of my
804480	815680	worries today with the idea of fake news is that when nothing is real, then like you dilute the
815680	821200	possibility of anything being true by conjuring up all kinds of conspiracy theories. And then you
821200	827680	don't know what to believe. And then like the idea of truth of objectivity is lost completely.
827760	833440	Everybody has their own truth. So you used to control information by censoring it.
833440	838240	Then the internet happened and governments are like, oh, shit, we can't censor things anymore.
838240	843120	I know what we'll do. You know, it's the old story of the story of like,
844080	848320	tying a flag with a leprechaun tells you as gold is buried and you tie one flag and you make the
848320	851680	leprechaun swear to not remove the flag and you come back to the field later with a shovel and
851680	860160	this flag's everywhere. That's one way to maintain privacy, right? In order to protect the contents
860160	865920	of this conversation, for example, we could just generate like millions of deep fake conversations
865920	871200	where you and I talk and say random things. So this is just one of them and nobody knows which
871200	875920	one was the real one. This could be fake right now. Classic steganography technique.
876000	883840	Okay, another absurd question about intelligent life because you're an incredible programmer
883840	892480	outside of everything else we'll talk about just as a programmer. Do you think intelligent beings
892480	898960	out there, the civilizations that were out there had computers and programming? Did they
898960	905680	do naturally have to develop something where we engineer machines and are able to encode both
906880	912240	knowledge into those machines and instructions that process that knowledge, process that
912240	917680	information to make decisions and actions and so on? And would those programming languages,
918240	922480	if you think they exist, be at all similar to anything we've developed?
924080	929840	So I don't see that much of a difference between quote unquote natural languages and programming
929840	940960	languages. I think there's so many similarities. So when asked the question what do alien languages
940960	948720	look like, I imagine they're not all that dissimilar from ours. And I think translating in and out of
948720	958160	them wouldn't be that crazy. It was difficult to compile like DNA to Python and then to see.
959120	962560	There is a little bit of a gap in the kind of languages we use for
964720	970880	touring machines and the kind of languages nature seems to use a little bit. Maybe that's just,
971520	976240	we just haven't understood the kind of language that nature uses well yet.
976240	985200	DNA is a CAD model. It's not quite a programming language. It has no sort of serial execution.
985200	992400	It's not quite a CAD model. So I think in that sense, we actually completely understand it.
992400	999360	The problem is simulating on these CAD models. I played with it a bit this year. It's super
999360	1004800	computationally intensive. If you want to go down to the molecular level where you need to go to see
1004800	1012080	a lot of these phenomenon like protein folding. So yeah, it's not that we don't understand it.
1012080	1014400	It just requires a whole lot of compute to kind of compile it.
1015040	1020400	For human minds, it's inefficient both for the data representation and for the programming.
1020400	1024960	Yeah, it runs well on raw nature. It runs well on raw nature. And when we try to build
1024960	1029360	emulators or simulators for that, well, they're mad slow and I've tried it.
1030480	1035840	It runs in that, yeah, you've commented elsewhere. I don't remember where that
1036160	1043920	one of the problems is simulating nature is tough. And if you want to sort of deploy a prototype,
1045440	1050240	I forgot how you put it, but it made me laugh. But animals or humans would need to be involved
1051680	1060480	in order to try to run some prototype code. If we're talking about COVID and viruses and so on,
1061120	1065760	if you were to try to engineer some kind of defense mechanisms like a vaccine
1067280	1072480	against COVID or all that kind of stuff, that doing any kind of experimentation like you can
1072480	1079520	with autonomous vehicles would be very technically and ethically costly.
1079520	1085440	I'm not sure about that. I think you can do tons of crazy biology and test tubes. I think
1085440	1089680	my bigger complaint is more, all the tools are so bad.
1091360	1096480	Like literally, you mean like like libraries and I'm not pipetting shit. Like your hand
1096480	1105200	of me, I gotta, no, no, no, no, there has to be some like automating stuff. And like the
1105920	1111520	yeah, but human biology is messy. Like it seems like, look at those Duranos videos. They were
1111520	1115760	joke. It's like a little gantry. It's like a little XY gantry high school science project
1115760	1120240	with the pipet. I'm like, really? Gotta be something better. You can't build like nice
1120240	1126400	microfluidics and I can program the computation to biointerface. I mean, this is going to happen.
1127040	1133360	But like right now, if you are asking me to pipet 50 milliliters of solution, I'm out.
1134240	1141200	This is so crude. Yeah. Okay. Let's get all the crazy out of the way. So a bunch of people
1141200	1146800	asked me, since we talked about the simulation last time, we talked about hacking the simulation.
1146800	1151520	Do you have any updates, any insights about how we might be able to go about
1152400	1155920	hacking simulation if we indeed do live in a simulation?
1157040	1162800	I think a lot of people misinterpreted the point of that South by talk. The point of the
1162800	1167360	South by talk was not literally to hack the simulation. I think that this
1168240	1175440	is an idea is literally just I think theoretical physics. I think that's the whole
1178880	1183120	goal. You want your grand unified theory, but then, okay, build a grand unified theory,
1183120	1189280	search for exploits. I think we're nowhere near actually there yet. My hope with that was just
1189280	1195200	more to like, are your people kidding me with the things you spend time thinking about? Do you
1195200	1201760	understand kind of how small you are? You are bites and God's computer, really?
1202400	1210800	And the things that people get worked up about. So basically, it was more a message of we should
1210800	1222320	humble ourselves. What are we humans in this byte code? Yeah. And not just humble ourselves,
1222320	1227200	but I'm not trying to make people feel guilty or anything like that. I'm trying to say literally,
1227200	1231600	look at what you are spending time on, right? What are you referring to? You're referring to
1231600	1235920	the Kardashians? What are we talking about? I'm referring to, no, the Kardashians,
1235920	1243840	everyone knows that's kind of fun. I'm referring more to the economy, this idea that
1244400	1254720	we got to up our stock price. Or what is the goal function of humanity?
1255280	1259360	You don't like the game of capitalism? You don't like the games we've constructed for
1259360	1264720	ourselves as humans? I'm a big fan of capitalism. I don't think that's really the game we're playing
1264720	1268320	right now. I think we're playing a different game where the rules are rigged.
1269040	1274240	Okay, which games are interesting to you that we humans have constructed and which
1274240	1281840	aren't? Which are productive and which are not? Actually, maybe that's the real point of the talk.
1281840	1287680	It's like, stop playing these fake human games. There's a real game here. We can play the real
1287680	1294400	game. The real game is nature wrote the rules. This is a real game. There still is a game to play.
1295040	1298320	But if you look at, sorry to interrupt, I don't know if you've seen the Instagram account,
1298320	1306240	Nature is Metal. The game that nature seems to be playing is a lot more cruel than we humans
1306240	1312240	want to put up with. Or at least we see it as cruel. It's like the bigger thing eats the smaller
1312240	1319680	thing and does it to impress another big thing so it can mate with that thing.
1320320	1323200	And that's it. That seems to be the entirety of it.
1324000	1330720	Well, there's no art. There's no music. There's no comma AI. There's no comma one,
1330720	1336160	no comma two, no George Hott's with his brilliant talks at South by Southwest.
1336880	1341520	I disagree though. I disagree that this is what nature is. I think nature just provided
1342320	1350800	basically a open world MMORPG. And here it's open world. I mean, if that's the game you want to
1350800	1356160	play, you can play that game. Isn't that beautiful? I know if you play Diablo, they used to have,
1356160	1363760	I think, cow level where it's, so everybody will go just, they figured out this,
1364320	1370720	like the best way to gain like experience points is to just slaughter cows over and over and over.
1372160	1377600	And so they figured out this little sub game within the bigger game that this is the most
1377600	1382560	efficient way to get experience points. And everybody somehow agreed that getting experience
1382560	1388000	points in RPG context where you always want to be getting more stuff, more skills, more levels,
1388000	1394960	keep advancing. That seems to be good. So might as well spend sacrifice, actual enjoyment of
1395040	1402240	playing a game, exploring a world and spending like hundreds of hours of your time in cow level.
1402240	1408320	I mean, the number of hours I spent in cow level, I'm not like the most impressive person because
1408320	1413920	people have probably thousands of hours there, but it's ridiculous. So that's a little absurd game
1413920	1419920	that brought me joints and weird dopamine drug kind of way. So you don't like those games.
1420640	1427200	You don't think that's us humans failing the nature. I think so.
1427200	1431280	And that was the point of the talk. Yeah. So how do we hack it then?
1431280	1435680	Well, I want to live forever. And wait, I want to live forever.
1435680	1437840	And this is the goal. Well, that's a game against nature.
1439120	1442320	Yeah. Immortality is the good objective function to you.
1443360	1446000	I mean, start there and then you can do whatever else you want because you got a long time.
1446960	1451280	What if immortality makes the game just totally not fun? I mean,
1451280	1458080	like why do you assume immortality is somehow a good objective function?
1458080	1462480	It's not immortality that I want. A true immortality where I could not die.
1462480	1466560	I would prefer what we have right now. But I want to choose my own death, of course.
1467920	1471440	I don't want nature to decide when I die. I'm going to win. I'm going to be you.
1471840	1480080	And then at some point, if you choose commit suicide, how long do you think you'd live?
1481440	1482240	Until I get bored?
1482960	1489200	See, I don't think people, brilliant people like you that really ponder
1490240	1497680	living a long time are really considering how meaningless life becomes.
1498400	1500320	Well, I want to know everything and then I'm ready to die.
1502320	1507360	But why do you want, isn't it possible that you want to know everything because
1508080	1513200	it's finite? Like the reason you want to know quote unquote everything is because you
1513200	1519440	don't have enough time to know everything. And once you have unlimited time, then you realize
1519440	1523760	like why do anything? Like why learn anything?
1524880	1526960	I don't want to know everything and then I'm ready to die.
1527120	1533760	It's a terminal value. It's not in service of anything else.
1534640	1539440	I'm conscious of the possibility. This is not a certainty. But the possibility of
1539440	1549680	that engine of curiosity that you're speaking to is actually a symptom of the finiteness of life.
1549680	1556640	Like without that finiteness, your curiosity would vanish like a morning fog.
1557680	1559200	Okosky talked about love like that.
1560080	1563840	Let me solve immortality. Let me change the thing in my brain that reminds me of the fact
1563840	1567600	that I'm immortal tells me that life is finite shit. Maybe I'll have it tell me that life ends
1567600	1574320	next week. I'm okay with some self manipulation like that. I'm okay with deceiving myself.
1574320	1577120	Oh, Rika, changing the code.
1577120	1581680	If that's the problem, right? If the problem is that I will no longer have that curiosity,
1581680	1588000	I'd like to have backup copies of myself which I check in with occasionally to make sure they're
1588000	1592320	okay with the trajectory and they can kind of override it. Maybe a nice like I think of like
1592320	1595040	those wave nets, those like logarithmic go back to the copies.
1595040	1600880	But sometimes it's not reversible. I've done this with video games. Once you figure out the
1600880	1606640	cheat code or like you look up how to cheat old school like single player, it ruins the game for
1606720	1611760	you. Absolutely. I know that feeling. But again, that just means our brain manipulation
1611760	1614560	technology is not good enough yet. Remove that cheat code from your brain.
1615600	1622720	So it's also possible that if we figure out immortality that all of us will kill ourselves
1623360	1628640	before we advance far enough to be able to revert the change.
1628640	1630240	I'm not killing myself till I know everything.
1631760	1634720	That's what you say now because your life is finite.
1635680	1641360	You know, I think self modifying systems comes up with all these hairy complexities and
1641360	1645440	can I promise that I'll do it perfectly? No, but I think I can put good safety structures in place.
1647120	1650720	So that talk in your thinking here is not literally
1653120	1661360	referring to a simulation in that our universe is a kind of computer program running in a computer.
1662080	1669040	That's more of a thought experiment. Do you also think of the potential of the sort of
1670560	1679520	Bostrom, Elon Musk, and others that talk about an actual program that simulates our universe?
1679520	1685120	Oh, I don't doubt that we're in a simulation. I just think that it's not quite that important.
1685120	1688880	I mean, I'm interested only in simulation theory as far as like it gives me power over nature.
1689680	1692960	If it's totally unfalsifiable, then who cares?
1692960	1697040	I mean, what do you think that experiment would look like? Like somebody on Twitter asks
1698160	1702800	George what signs we would look for to know whether or not we're in the simulation,
1702800	1709680	which is exactly what you're asking is like the step that precedes the step of knowing how to
1709680	1715120	get more power from this knowledge is to get an indication that there's some power to be gained.
1715840	1722000	Get an indication that you can discover and exploit cracks in the simulation,
1722000	1724960	or it doesn't have to be in the physics of the universe.
1726640	1730240	Show me. I mean, like a memory leak would be cool.
1731520	1735200	Like some scrying technology. What kind of technology?
1735200	1741840	Scrying. What's that? That's a weird. Scrying is the paranormal ability to
1742800	1746720	like remote viewing, like being able to see somewhere where you're not.
1748160	1754000	So, I don't think you can do it by chanting in a room, but if we could find, it's a memory leak,
1754000	1759920	basically. It's a memory leak. Yeah, you're able to access parts you're not supposed to.
1759920	1762000	Yeah, yeah, yeah. And thereby discover shortcut.
1762000	1765280	Yeah, memory leak means the other thing as well. But I mean like, yeah,
1765280	1769760	like an ability to read arbitrary memory. And that one's not that horrifying.
1769760	1774720	The right ones start to be horrifying. Read it right. So, the reading is not the problem.
1774720	1779680	Yeah, it's like Heartbleed for the universe. Oh boy, the writing is a big, big problem.
1780560	1786080	It's a big problem. It's the moment you can write anything, even if it's just random noise.
1787600	1792480	That's terrifying. I mean, even without, even without that, like even some of the, you know,
1792480	1798800	the nanotech stuff that's coming, I think. I don't know if you're paying attention, but
1798880	1803840	actually Eric Weinstein came out with the theory of everything. I mean, that came out. He's been
1803840	1808800	working on a theory of everything in the physics world called geometric unity. And then for me,
1808800	1814480	from computer science person, like you, Stephen Wolfram's theory of everything of like,
1814480	1819760	hypergraphs is super interesting and beautiful. But not from a physics perspective, but from a
1819760	1822720	computational perspective. I don't know, have you paid attention to any of that?
1823040	1828640	So, again, like what would make me pay attention and like why like a hate string theory is,
1829360	1834640	okay, make a testable prediction, right? I'm only interested in, I'm not interested in theories
1834640	1838160	for their intrinsic beauty. I'm interested in theories that give me power over the universe.
1839760	1842080	So, if these theories do, I'm very interested.
1842960	1848080	Can I just say how beautiful that is? Because a lot of physicists say, I'm interested in
1848160	1854320	experimental validation. And they skip out the part where they say, to give me more power in
1854320	1862800	the universe. I just love the clarity of that. I want 100 gigahertz processors. I want transistors
1862800	1872240	that are smaller than atoms. I want like power. That's true. And that's where people from aliens
1872320	1878160	to this kind of technology where people are worried that governments, like who owns that power?
1879200	1885760	Is it George Haatz? Is it thousands of distributed hackers across the world? Is it governments?
1886560	1893840	You know, is it Mark Zuckerberg? There's a lot of people that, I don't know if anyone trusts
1893840	1896560	anyone individual with power. So, they're always worried.
1897120	1898400	It's the beauty of blockchains.
1899200	1903840	That's the beauty of blockchains, which we'll talk about on Twitter.
1903840	1909200	Somebody pointed me to a story, a bunch of people pointed me to a story a few months ago,
1909200	1913040	where you went into a restaurant in New York, and you can correct me if I'm saying this is wrong,
1913680	1920320	and ran into a bunch of folks from a company in a crypto company who are trying to scale up Ethereum.
1921760	1926560	And they had a technical deadline related to a solidity to OVM compiler.
1927280	1933200	So, these are all Ethereum technologies. So, you stepped in, they recognized you,
1934640	1938320	pulled you aside, explained their problem, and you stepped in and helped them solve the problem,
1939520	1949520	thereby creating legend status story. So, can you tell me the story a little more detail? It seems
1949520	1954320	kind of incredible. Did this happen? Yeah, yeah, it's a true story. It's a true story. I mean,
1954400	1963680	they wrote a very flattering account of it. So, the company is called Optimism,
1963680	1968400	spin-off of Plasma. They're trying to build L2 solutions on Ethereum. So, right now,
1970800	1975680	every Ethereum node has to run every transaction on the Ethereum network.
1976560	1980640	And this kind of doesn't scale, right? Because if you have N computers, well,
1980640	1984160	if that becomes two N computers, you actually still get the same amount of compute.
1984960	1991440	Right? This is like O of one scaling because they all have to run it. Okay, fine. You get more
1991440	1996320	blockchain security, but like, blockchain is already so secure. Can we trade some of that off
1996320	2002080	for speed? So, that's kind of what these L2 solutions are. They built this thing, which kind of
2003200	2009040	sandbox for Ethereum contracts. So, they can run it in this L2 world, and it can't do certain
2009040	2015760	things in L1. Can I ask you for some definitions? What's L2? Oh, L2 is Layer 2. So, L1 is like the
2015760	2021600	base Ethereum chain, and then Layer 2 is like a computational layer that runs
2023360	2029600	elsewhere, but still is kind of secured by Layer 1. And I'm sure a lot of people know,
2029600	2034080	but Ethereum is a cryptocurrency, probably one of the most popular cryptocurrency, second to
2034080	2041200	Bitcoin, and a lot of interesting technological innovations there. Maybe you can also slip in
2041840	2046240	whenever you talk about this, any things that are exciting to you in the Ethereum space.
2046240	2053200	And why Ethereum? Well, I mean, Bitcoin is not turned complete. Ethereum is not technically
2053200	2058000	turned complete with the gas limit, but close enough. With the gas limit? What's the gas limit?
2058960	2060800	Yeah, I mean, no computer is actually turned complete.
2062960	2065520	They're just fine at RAM. I can actually solve the whole problem.
2065520	2069280	What's the word gas limit? You just have so many brilliant words. I'm not even gonna ask.
2069280	2071600	Well, that's not my word. That's Ethereum's word.
2072560	2077040	Ethereum, you have to spend gas per instruction. So, different op codes use different amounts of
2077040	2081920	gas, and you buy gas with Ether to prevent people from basically de-dossing the network.
2082640	2086960	So, Bitcoin is proof of work, and then what's Ethereum?
2087040	2090960	It's also proof of work. They're working on some proof of stake Ethereum 2.0 stuff,
2090960	2094640	but right now, it's proof of work. It uses a different hash function from Bitcoin
2094640	2096560	that's more ASIC resistance because you need RAM.
2097280	2102880	So, we're all talking about Ethereum 1.0. So, what were they trying to do to scale
2102880	2106960	this whole process? So, they were like, well, if we could run contracts elsewhere,
2107920	2113680	and then only save the results of that computation, well, we don't actually have
2113680	2116800	to do the compute on the chain. We can do the compute off chain and just post what the results
2116800	2121120	are. Now, the problem with that is, well, somebody could lie about what the results are.
2121120	2125520	So, you need a resolution mechanism, and the resolution mechanism can be really expensive
2126400	2132320	because you just have to make sure that the person who is saying, look, I swear that this
2132320	2139120	is the real computation. I'm staking $10,000 on that fact, and if you prove it wrong, yeah,
2139120	2144640	it might cost you $3,000 in gas fees to prove wrong, but you'll get the $10,000 bounty.
2144640	2150480	So, you can secure using those kind of systems. So, it's effectively a sandbox,
2150480	2156720	which runs contracts, and like just like any kind of normal sandbox, you have to like replace
2156720	2160320	syscalls with, you know, calls into the hypervisor.
2162880	2165920	Sandbox, syscalls, hypervisor, what do these things mean?
2167520	2169040	As long as it's interesting to talk about.
2169040	2172640	Yeah, I mean, you can take like the Chrome sandbox is maybe the one to think about, right?
2172720	2177680	So, the Chrome process is doing a rendering. I can't, for example, read a file from the file
2177680	2183920	system. If it tries to make an open syscall in Linux, you can't make an open syscall. No, no,
2183920	2191040	no. You have to request from the kind of hypervisor process or like, I don't know what's
2191040	2197440	called in Chrome, but the, hey, could you open this file for me? And then it does all these checks,
2197440	2201600	and then it passes the file, handle back in if it's approved. So, that's yeah.
2203040	2207600	What's the, in the context of Ethereum, what are the boundaries of the sandbox that we're talking
2207600	2214160	about? Well, like one of the calls that you actually reading and writing any state to the
2214160	2221200	Ethereum contract or to the Ethereum blockchain. Writing state is one of those calls that you're
2221200	2228240	going to have to sandbox in layer two, because if you let layer two just arbitrarily write to the
2228240	2235520	Ethereum blockchain. So, layer two is really sitting on top of layer one. So, you're going to
2235520	2240320	have a lot of different kinds of ideas that you can play with. And they're all, they're not fundamentally
2240320	2248800	changing the source code level of Ethereum. Well, you have to replace a bunch of calls
2248800	2255360	with calls into the hypervisor. So, instead of doing the syscall directly, you replace it with a
2255360	2262400	call to the hypervisor. So, originally they were doing this by first running the, so solidity is
2262400	2267760	the language that most Ethereum contracts are written in, it compiles to a bytecode. And then
2267760	2272800	they wrote this thing they called the transpiler. And the transpiler took the bytecode and it
2272800	2278160	transpiled it into OVM safe bytecode, basically bytecode that didn't make any of those restricted
2278160	2284240	syscalls and added the calls to the hypervisor. This transpiler was a 3000 line mess.
2285520	2289440	And it's hard to do. It's hard to do if you're trying to do it like that because you have to
2289440	2295760	kind of like deconstruct the bytecode, change things about it, and then reconstruct it. And
2295760	2298960	I mean, as soon as I hear this, I'm like, well, why don't you just change the compiler?
2299760	2303280	Right? Why not the first place you build the bytecode, just do it in the compiler?
2305520	2310960	So, yeah, you know, I asked them how much they wanted it. Of course, we're measured in dollars
2310960	2317520	and I'm like, well, okay. And yeah, you wrote the compiler. Yeah, I modified, I wrote a 300
2317520	2322400	line diff to the compiler. It's open source, you can look at it. Yeah, I looked at the code last
2322400	2335920	night. Yeah, exactly. Qt is a good word for it. And it's C++. C++, yeah. So when asked how you
2335920	2343760	were able to do it, you said, you just got to think and then do it right. So can you break that
2343760	2348800	apart a little bit? What's your process of one thinking and two doing it right?
2349760	2354080	You know, the people who I was working for are amused that I said that it doesn't really mean
2354080	2362560	anything. Okay. I mean, is there some deep profound insights to draw from like how you problem solve
2362560	2366640	from that? This is always what I say. I'm like, do you want to be a good programmer? Do it for 20
2366640	2376240	years? Yeah, there's no shortcuts. What are your thoughts on crypto in general? So what parts
2376240	2381600	technically or philosophically defined, especially beautiful, maybe? Oh, I'm extremely bullish on
2381600	2390560	crypto long term, not any specific crypto project, but this idea of, well, two ideas. One,
2391360	2398480	the Nakamoto consensus algorithm is I think one of the greatest innovations of the 21st century.
2398480	2404480	This idea that people can reach consensus, you can reach a group consensus using a relatively
2404480	2414880	straightforward algorithm is wild. And like Satoshi Nakamoto, people always ask me who
2414880	2421360	I look up to. It's like, whoever that is. Who do you think it is? Elon Musk? Is it you?
2422160	2430240	It is definitely not me. And I do not think it's Elon Musk. But yeah, this idea of groups
2430240	2437440	reaching consensus in a decentralized yet formulaic way is one extremely powerful idea from crypto.
2438080	2448400	Maybe the second idea is this idea of smart contracts. When you write a contract between
2448400	2455120	two parties, any contract, this contract, if there are disputes, it's interpreted by lawyers.
2456080	2462560	Lawyers are just really shitty overpaid interpreters. Let's talk about them in terms of like,
2462560	2469920	let's compare a lawyer to Python. Well, okay. That's brilliant. I never thought of it that way.
2469920	2477040	It's hilarious. So Python, I'm paying even 10 cents an hour. I'll use the nice Azure machine.
2477040	2484880	I can run Python for 10 cents an hour. Lawyers cost $1,000 an hour. So Python is 10,000x better
2484880	2492080	on that axis. Lawyers don't always return the same answer. Python almost always does.
2496640	2502720	Cost. Yeah. I mean, just cost, reliability, everything about Python is so much better than
2502720	2512400	lawyers. So if you can make smart contracts, this whole concept of code is law. I would love to
2512400	2520000	live in a world where everybody accepted that fact. So maybe you can talk about what smart contracts
2520000	2533040	are. So let's say we have even something as simple as a safety deposit box. Safety deposit box that
2533040	2538640	holds a million dollars. I have a contract with the bank that says two out of these three parties
2539600	2545840	must be present to open the safety deposit box and get the money out. So that's a contract with
2545840	2552640	the bank. And it's only as good as the bank and the lawyers. Let's say somebody dies and now,
2552640	2556400	oh, we're going to go through a big legal dispute about whether, oh, was it in the will? Was it not
2556400	2565120	in the will? It's just so messy. And the cost to determine truth is so expensive versus a smart
2565200	2568880	contract, which just uses cryptography to check if two out of three keys are present.
2570000	2575760	Well, I can look at that and I can have certainty in the answer that it's going to return.
2575760	2579520	And that's what all businesses want is certainty. You know, they say businesses don't care,
2579520	2584720	Viacom YouTube. YouTube's like, look, we don't care which way this lawsuit goes. Just please
2584720	2590320	tell us so we can have certainty. I wonder how many agreements in this, because we're talking about
2590400	2596160	financial transactions only in this case, correct? The smart contracts. Oh, you can go to,
2596160	2599360	you can go to anything. You can go, you can put a prenup in the theorem blockchain.
2601840	2606640	Married smart contract. Sorry, divorce lawyer. Sorry, you're going to be replaced by Python.
2609520	2617120	Okay, so that's, so that's another beautiful idea. Do you think there's something that's
2617120	2623760	appealing to you about any one specific implementation? So if you look 10, 20, 50 years
2623760	2630080	down the line, do you see any like Bitcoin, Ethereum, any of the other hundreds of cryptocurrencies
2630080	2634320	winning out? Is there like, what's your intuition about the space? Are you just sitting back and
2634320	2639040	watching the chaos and look who cares what emerges? Oh, I don't, I don't speculate. I don't
2639040	2643760	really care. I don't really care which one of these projects wins. I'm kind of in the Bitcoin
2643760	2649680	as a meme coin camp. I mean, why does Bitcoin have value? It's technically kind of, you know,
2651600	2656080	not great. Like the block size debate, or when I found out what the block size debate was, I'm
2656080	2662640	like, are you guys kidding? What's the block size debate? You know what, it's really, it's too stupid
2662640	2668960	to even talk. People can look it up, but I'm like, wow, you know, Ethereum seems, the governance of
2668960	2675680	Ethereum seems much better. I've come around a bit on proof of stake ideas. You know, very
2675680	2681200	smart people thinking about some things. Yeah, you know, governance is interesting. It does feel
2681200	2689280	like Vitalik, it could just feel like an open, even in these distributed systems, leaders are
2689280	2696960	helpful because they kind of help you drive the mission and the vision. And they put a face to
2696960	2703600	a project. It's a weird thing about us humans. Geniuses are helpful, like Vitalik. Yeah, brilliant.
2706480	2713440	Leaders are not necessarily, yeah. So you think the reason he's the face
2714560	2719280	of Ethereum is because he's a genius. That's interesting. I mean, that was,
2720240	2728800	it's interesting to think about that we need to create systems in which the quote unquote leaders
2728800	2735200	that emerge are the geniuses in the system. I mean, that's arguably why the current state of
2735200	2740240	democracy is broken is the people who are emerging as the leaders are not the most competent, are
2740240	2746160	not the superstars of the system. And it seems like at least for now in the crypto world, oftentimes
2747120	2750880	the leaders are the superstars. Imagine at the debate, they asked,
2751600	2755440	what's the sixth amendment? What are the four fundamental forces in the universe?
2756080	2761920	What's the integral of two to the X? Yeah, I'd love to see those questions asked. And that's
2761920	2769680	what I want as our leader. It's a little bit. What's Bayes rule? Yeah, I mean, even, oh, wow,
2769760	2776480	you're hurting my brain. My standard was even lower, but I would have loved to see
2777600	2783040	just this basic brilliance, like I've talked to historians. There's just these,
2783040	2787120	they're not even like, they don't have a PhD or even education history. They just
2787760	2794320	like a Dan Carlin type character who just like, holy shit, how did all this information get
2794320	2800240	into your head? They were able to just connect Genghis Khan to the entirety of the history of
2800240	2806800	the 20th century. They know everything about every single battle that happened. And they know the
2808480	2815040	like the game of thrones of the different power plays and all that happened there.
2815040	2820960	And they know like the individuals and all the documents involved. And they integrate that into
2821040	2826240	their regular life. It's not like they're ultra history nerds. They're just, they know this information.
2826240	2830720	That's what competence looks like. Yeah. Because I've seen that with programmers too, right? That's
2830720	2836640	what great programmers do. But yeah, it'll be, it's really unfortunate that those kinds of people
2836640	2842480	aren't emerging as our leaders. But for now, at least in the crypto world, that seems to be the
2842480	2848320	case. I don't know if that always, you could imagine that in 100 years, it's not the case,
2848800	2852960	crypto world has one very powerful idea going for it. And that's the idea of forks.
2854880	2864000	I mean, imagine we'll use a less controversial example. This was actually in my joke
2866080	2870720	app in 2012. I was like, Barack Obama, Mitt Romney, let's let them both be president.
2870720	2874720	All right. Like imagine we could fork America and just let them both be president. And then
2874720	2879360	the Americas could compete and people could invest in one, pull their liquidity out of one,
2879360	2884160	put it in the other. You have this in the crypto world. Ethereum forks into Ethereum and Ethereum
2884160	2889760	Classic. And you can pull your liquidity out of one and put it in another. And people vote with
2889760	2897520	their dollars, which forks, companies should be able to fork. I'd love to fork Nvidia.
2898000	2905520	Yeah. Like different business strategies. And then try them out and see what works.
2906080	2916560	Like even take, yeah, take common AI that closes its source and then take one that's open source
2916560	2923440	and see what works. Take one that's purchased by GM and one that remains Android Renegade
2923440	2927360	in all these different versions and see the beauty of common AI. Someone can actually do that.
2927840	2932160	Please take common AI and fork it. That's right. That's the beauty of open source.
2932960	2939440	So you're, I mean, we'll talk about autonomous vehicle space, but it does seem that you're
2941120	2945440	really knowledgeable about a lot of different topics. So the natural question a bunch of people
2945440	2951600	ask this, which is how do you keep learning new things? Do you have like practical advice
2952320	2959200	if you were to introspect like taking notes, allocate time, or do you just mess around and
2959200	2964400	just allow your curiosity to drive? I'll write these people a self-help book and I'll charge $67
2964400	2971680	for it. And I will write on the cover of the self-help book. All of this advice is completely
2971680	2976800	meaningless. You're going to be a sucker and buy this book anyway. And the one lesson that I hope
2976800	2981520	they take away from the book is that I can't give you a meaningful answer to that.
2982400	2990080	That's interesting. Let me translate that as you haven't really thought about what it is you do
2991520	2995840	systematically because you could reduce it. And then some people, I mean, I've met brilliant
2995840	3004160	people that this is really clear with athletes. Some are just the best in the world at something
3005040	3011440	and they have zero interest in writing like a self-help book or how to master this game.
3011440	3017200	And then there are some athletes who become great coaches and they love the analysis,
3017200	3021680	perhaps the over-analysis. And you right now, at least at your age, which is an interesting,
3021680	3025920	you're in the middle of the battle. You're like the warriors that have zero interest in writing
3025920	3032320	books. So you're in the middle of the battle. So yeah. This is a fair point. I do think I have
3032320	3039520	a certain aversion to this kind of deliberate, intentional way of living life.
3040640	3045120	You eventually, the hilarity of this, especially since this is recorded,
3047120	3050640	it will reveal beautifully the absurdity when you finally do publish this book.
3051200	3058720	That guarantee you will. The story of comma AI, maybe it'll be a biography written about you.
3059200	3062880	That'll be better, I guess. And you might be able to learn some cute lessons if you're starting
3062880	3067520	a company like comma AI from that book. But if you're asking generic questions like,
3067520	3074160	how do I be good at things? Dude, I don't know. Well, I mean, the interesting- Do them a lot.
3074160	3081760	Do them a lot. But the interesting thing here is learning things outside of your current trajectory,
3081760	3088560	which is what it feels like from an outsider's perspective. I don't know if there's
3089200	3094640	advice on that, but it is an interesting curiosity. When you become really busy,
3094640	3103920	you're running a company. Hard time. Yeah. But there's a natural inclination and
3105040	3110960	trend. Just the momentum of life carries you into a particular direction of wanting to focus.
3110960	3118000	And this kind of dispersion that curiosity can lead to gets harder and harder with time.
3118560	3123680	You get really good at certain things, and it sucks trying things that you're not good at,
3123680	3128880	like trying to figure them out. I mean, you do this with your live streams. You're on the fly
3128880	3135920	figuring stuff out. You don't mind looking dumb. No. You just figured out pretty quickly.
3136560	3140880	Sometimes I try things and I don't figure them out quickly. My chest rating is like a 1400,
3140880	3145840	despite putting like a couple hundred hours in. It's pathetic. I mean, to be fair, I know that
3145840	3150960	I could do it better. If I did it better, don't play five minute games, play 15 minute games at
3150960	3156080	least. I know these things, but it just doesn't. It doesn't stick nicely in my knowledge stream.
3157120	3163360	All right. Let's talk about Kama AI. What's the mission of the company? Let's look at the biggest
3163360	3170640	picture. Oh, I have an exact statement. Solve self-driving cars while delivering shipable intermediaries.
3171440	3177520	So long-term vision is have fully autonomous vehicles and make sure you're making money along
3177520	3182560	the way. I think it doesn't really speak to money, but I can talk about what solve self-driving cars
3182560	3188400	means. Solve self-driving cars, of course, means you're not building a new car. You're building
3188400	3193840	a person replacement. That person can sit in the driver's seat and drive you anywhere a person can
3193840	3199200	drive with a human or better level of safety, speed, quality, comfort.
3201280	3203040	And what's the second part of that?
3203040	3207920	Delivering shipable intermediaries is, well, it's a way to fund the company. That's true,
3207920	3215040	but it's also a way to keep us honest. If you don't have that, it is very easy with
3215840	3221280	this technology to think you're making progress when you're not. I've heard it best described on
3221280	3228960	Hacker News as you can set any arbitrary milestone, meet that milestone, and still be infinitely
3228960	3234080	far away from solving self-driving cars. So it's hard to have real deadlines when you're
3235120	3246880	cruise or Waymo when you don't have revenue. Is revenue essentially the thing we're talking
3246880	3252640	about here? Capitalism is based around consent. Capitalism, the way that you get
3252640	3257200	revenue is real capitalism. Common is in the real capitalism camp. There's definitely scams out there,
3257200	3261200	but real capitalism is based around consent. It's based around this idea that if we're getting
3261200	3265600	revenue, it's because we're providing at least that much value to another person. When someone buys
3265600	3270080	$1,000 comma two from us, we're providing them at least $1,000 of value, but they wouldn't buy it.
3270080	3274560	Brilliant. So can you give a world-wind overview of the products that Kamii provides?
3274880	3280720	Like throughout its history and today? I mean, yeah, the past ones aren't really that interesting.
3280720	3287440	It's kind of just been refinement of the same idea. The real only product we sell today is the comma
3287440	3294560	two, which is a piece of hardware with cameras. So the comma two, I mean, you can think about it
3294560	3299360	kind of like a person. You know, in future hardware will probably be even more and more person-like.
3300160	3308320	So it has eyes, ears, a mouth, a brain, and a way to interface with the car.
3309280	3312560	Does it have consciousness? Just kidding. That was a trick question.
3313280	3316240	I don't have consciousness either. Me and the comma two are the same.
3316240	3316960	They are the same.
3316960	3320880	I have a little more compute than it. It only has like the same compute as a B.
3323120	3326320	You're more efficient energy-wise for the compute you're doing.
3326400	3330480	Far more efficient energy-wise. 20 pay-to-flops, 20 wants, crazy.
3330480	3332480	Do you lack consciousness? Sure.
3332480	3335280	Do you fear death? You do. You want immortality.
3335280	3335840	Of course I fear death.
3335840	3338640	Does Kamii fear death? I don't think so.
3339360	3343280	Of course it does. It very much fears, well, it fears negative loss. Oh, yeah.
3345600	3350800	Okay. So comma two, when did that come out? That was a year ago? No, two.
3351760	3352640	Early this year.
3353440	3360560	Wow, time, it feels, yeah. 2020 feels like it's taken 10 years to get to the end.
3360560	3361680	It's a long year.
3361680	3369040	It's a long year. So what's the sexiest thing about comma two, feature-wise?
3369840	3375600	So, I mean, maybe you can also link on like, what is it? Like, what's its purpose?
3375600	3380560	Because there's a hardware, there's a software component. You've mentioned the sensors, but
3380640	3382960	also like, what is it, its features and capabilities?
3382960	3386960	I think our slogan summarizes it well. A comma slogan is make driving chill.
3388800	3395280	I love it. Okay. Yeah, I mean, it is, you know, if you like cruise control, imagine cruise control,
3395280	3402000	but much, much more. So it can do adaptive cruise control things, which is like, slow down for
3402000	3407120	cars in front of it, maintain a certain speed. And it can also do lane keeping, so stay in the
3407120	3411680	lane and do it better and better and better over time. It's very much machine learning based.
3413040	3416320	So there's cameras, there's a driver facing camera too.
3421040	3426560	What else is there? What am I thinking? So the hardware versus software, so open pilot versus
3426560	3431440	the actual hardware, the device. What's, can you draw that distinction? What's one? What's the other?
3431440	3435840	I mean, the hardware is pretty much a cell phone with a few additions, a cell phone with a cooling
3435840	3444240	system and with a car interface connected to it. And by cell phone, you mean like Qualcomm Snapdragon?
3445520	3451760	Yeah, the current hardware is a Snapdragon 821. It has Wi-Fi radio, it has an LTE radio, it has a screen.
3453840	3458400	We use every part of the cell phone. And then the interface of the car is specific to the car,
3458400	3462800	so you keep supporting more and more cars? Yeah, so the interface to the car, I mean,
3462800	3467200	the device itself just has four CAN buses, has four CAN interfaces on it, they're connected
3467200	3474400	through the USB port to the phone. And then, yeah, on those four CAN buses, you connect it to the car
3474400	3479280	and there's a little part to do this. Cars are actually surprisingly similar. So CAN is the
3479280	3484480	protocol about which cars communicate and then you're able to read stuff and write stuff to be
3484480	3489120	able to control the car depending on the car. So what's the software side? What's open pilot?
3489920	3494480	So, I mean, open pilot is, the hardware is pretty simple compared to open pilot, open pilot is...
3498000	3504960	Well, so you have a machine learning model, which it's an open pilot, it's a blob, it's just a blob
3504960	3508720	of weights. It's not like people are like, oh, it's closed source, I'm like, it's a blob of weights,
3508720	3512800	what do you expect? So it's primarily neural network based?
3513680	3517920	Well, open pilot is all the software kind of around that neural network, that if you have
3517920	3522240	a neural network that says, here's where you want to send the car, open pilot actually goes and
3522240	3528000	executes all of that. It cleans up the input to the neural network, it cleans up the output and
3528000	3533200	executes on it. So it's the glue that connects everything together. Runs the sensors, does a
3533200	3538480	bunch of calibration for the neural network, does, you know, deals with like, you know, if the car
3538480	3543360	is on a banked road, you have to counter steer against that. And the neural network can't necessarily
3543360	3549120	know that by looking at the picture. So you can do that with other sensors, infusion and localizer.
3549760	3555840	Open pilot also is responsible for sending the data up to our servers, so we can learn from it,
3556640	3560640	logging it, recording it, running the cameras, thermally managing the device,
3561360	3564640	managing the disk space on the device, managing all the resources on the device.
3564640	3569280	So what, since we last spoke, I don't remember when, maybe a year ago, maybe a little bit longer,
3570080	3572960	how has open pilot improved?
3572960	3576560	We did exactly what I promised you. I promised you that by the end of the year,
3576560	3586000	we would be able to remove the lanes. The lateral policy is now almost completely end to end. You
3586000	3590880	can turn the lanes off and it will drive slightly worse on the highway if you turn the lanes off,
3590880	3596720	but you can turn the lanes off and it will drive well trained completely end to end on user data.
3597280	3599920	And this year we hope to do the same for the longitudinal policy.
3599920	3604400	So that's the interesting thing is you're not doing, you don't appear to be,
3604400	3610000	maybe you can correct me, you don't appear to be doing lane detection or lane marking
3610000	3615040	detection or kind of the segmentation task or any kind of object detection task,
3615040	3618560	you're doing what's traditionally more called like end to end learning.
3619360	3625840	So entrained on actual behavior of drivers when they're driving the car manually.
3627600	3631120	And this is hard to do. It's not supervised learning.
3632080	3635840	Yeah, but so the nice thing is there's a lot of data, so it's hard and easy.
3637680	3639920	We have a lot of high quality data, yeah.
3639920	3641600	Like more than you need in the sun.
3641600	3644800	Well, we've way more than we do. We've way more data than we need.
3644800	3649600	I mean, it's an interesting question, actually, because in terms of amount, you have more than
3649680	3654160	you need. But the, you know, driving is full of edge cases.
3654160	3656880	So how do you select the data you train on?
3658160	3660480	I think this is an interesting open question.
3660480	3664080	Like what's the cleverest way to select data?
3664080	3666240	That's the question Tesla is probably working on.
3667600	3669840	That's, I mean, the entirety of machine learning can be,
3669840	3672160	they don't seem to really care. They just kind of select data.
3672160	3676160	But I feel like that if you want to solve, if you want to create intelligence systems,
3676160	3678800	you have to pick data well, right?
3678800	3682800	And so do you have any hints, ideas of how to do it well?
3682800	3688400	So in some ways, that is the definition I like of reinforcement learning versus supervised learning.
3689200	3692880	In supervised learning, the weights depend on the data, right?
3694400	3699360	And this is obviously true, but the, in reinforcement learning, the data depends on the weights.
3700320	3700800	Yeah.
3700800	3702400	Right. And actually both ways.
3702400	3703920	That's poetry.
3703920	3704800	That's brilliant.
3704800	3706160	How does it know what data to train on?
3706160	3707360	Well, let it pick.
3707360	3709440	We're not there yet, but that's the eventual.
3709440	3712560	So you're thinking this almost like a reinforcement learning framework.
3713120	3714320	We're going to do RL on the world.
3715280	3718000	Every time a car makes a mistake, user disengages.
3718000	3720000	We train on that and do RL on the world.
3720000	3721760	Ship out a new model. That's an epoch, right?
3723200	3729600	And for now, you're not doing the Elon style promising that it's going to be fully autonomous.
3729600	3734400	You really are sticking to level two and like it's supposed to be supervised.
3735440	3738240	It is definitely supposed to be supervised and we enforce the fact that it's supervised.
3739680	3743520	We look at our rate of improvement in disengagements.
3743520	3746640	OpenPilot now has an unplanned disengagement about every 100 miles.
3747280	3756400	This is up from 10 miles, like maybe, maybe a, maybe a year ago.
3756400	3761600	Yeah. So maybe we've seen 10x improvement in a year, but 100 miles is still a far cry
3761600	3763200	from the 100,000 you're going to need.
3763760	3768240	So you're going to somehow need to get three more 10x's in there.
3769760	3771280	And you're, what's your intuition?
3772240	3775440	You're basically hoping that there's exponential improvement built into the,
3775440	3776720	baked into the cake somewhere.
3776720	3780400	Well, that's even, I mean, 10x improvement, that's already assuming exponential, right?
3780400	3782480	There's definitely exponential improvement.
3782480	3785200	And I think when Elon talks about exponential, like these things,
3785200	3787760	these systems are going to exponentially improve.
3787760	3792800	Just exponential doesn't mean you're getting 100 gigahertz processors tomorrow, right?
3793200	3796080	Like it's going to still take a while because the gap
3796080	3799520	between even our best system and humans is still large.
3800160	3802240	So that's an interesting distinction to draw.
3802240	3804800	So if you look at the way Tesla is approaching the problem,
3806000	3810640	and the way you're approaching the problem, which is very different than the rest of the
3810640	3812560	self-driving car world.
3812560	3813760	So let's put them aside.
3813760	3817440	Is you're treating most, the driving task as a machine learning problem.
3817440	3820960	And the way Tesla is approaching it is with the multitask learning,
3820960	3825280	where you break the task of driving into hundreds of different tasks.
3825280	3831520	And you have this multi-headed neural network that's very good at performing each task.
3831520	3836240	And there's presumably something on top that's stitching stuff together
3836240	3842080	in order to make control decisions, policy decisions about how you move the car.
3842080	3845520	But what that allows you, there's a brilliance to this because it allows you to
3845760	3854560	master each task, like lane detection, stop sign detection, the traffic light detection,
3855440	3861280	drivable area segmentation, vehicle bicycle pedestrian detection.
3862880	3864800	There's some localization tasks in there.
3865440	3874000	Also predicting how the entities in the scene are going to move.
3874240	3877920	Everything is basically a machine learning task, where there's a classification,
3877920	3884320	segmentation, prediction, and it's nice because you can have this entire engine,
3884320	3889360	data engine that's mining for edge cases for each one of these tasks.
3889360	3893600	And you can have people, like engineers that are basically masters of that task,
3893600	3899600	they become the best person in the world at, as you talk about, the cone guy for Waymo.
3899600	3901040	Yeah, we're a good old cone guy.
3901600	3906000	Become the best person in the world at cone detection.
3907200	3910640	So that's a compelling notion from a supervised learning perspective,
3912160	3917360	automating much of the process of edge case discovery and retraining neural network for
3917360	3922000	each of the individual perception tasks. And then you're looking at the machine learning
3922000	3928960	in a more holistic way, basically doing end-to-end learning on the driving tasks, supervised,
3929760	3936240	trained on the data of the actual driving of people they use comma AI,
3936240	3941760	like actual human drivers, their manual control, plus the moments of disengagement
3942560	3947120	that maybe with some labeling could indicate the failure of the system.
3948800	3953920	You have a huge amount of data for positive control of the vehicle, like successful control
3954000	3959920	of the vehicle, both maintaining the lane as I think you're also working on
3959920	3964640	longitude, no control of the vehicle, and then failure cases where the vehicle does
3964640	3972320	something wrong that needs disengagement. So why do you think you're right and Tesla is wrong
3973040	3979840	on this? Do you think you'll come around the Tesla way? Do you think Tesla will come around to your way?
3980160	3985120	If you were to start a chess engine company, would you hire a bishop guy?
3986000	3993360	See, we have, this is Monday morning quarterbacking, is yes, probably.
3995840	4000800	Oh, our Rook guy. Oh, we stole the Rook guy from that company. Oh, we're gonna have real good Rooks.
4000800	4008640	Well, there's not many pieces, right? There's not many guys and gals to hire.
4008720	4012560	You just have a few that work on the bishop, a few that work on the Rook.
4012560	4016640	But is that not ludicrous today to think about in a world of AlphaZero?
4017440	4023600	But AlphaZero is a chess game. So the fundamental question is how hard is driving compared to chess?
4024320	4032080	Because so long term, end to end will be the right solution. The question is how many years
4032080	4035680	away is that? End to end is going to be the only solution for level five.
4035680	4039200	For the only way we get there. Of course. And of course, Tesla is going to come around to my
4039200	4045360	way. And if you're a Rook guy out there, I'm sorry. The cone guy. I don't know.
4045360	4049360	We're going to specialize each task. We're going to really understand Rook placement. Yeah.
4050400	4057040	I understand the intuition you have. I mean, that is a very compelling notion that we can
4057040	4061200	learn the task end to end, like the same compelling notion you might have for natural
4061200	4069840	language conversation. But I'm not sure. Because one thing you sneaked in there is assertion that
4070640	4076400	it's impossible to get to level five without this kind of approach. I don't know if that's
4076400	4082400	obvious. I don't know if that's obvious either. I don't actually mean that. I think that it is much
4082400	4086880	easier to get to level five with an end to end approach. I think that the other approach is
4087760	4092880	doable, but the magnitude of the engineering challenge may exceed what humanity is capable of.
4093680	4100720	But what do you think of the Tesla data engine approach, which to me is an active learning
4100720	4107360	task is kind of fascinating, is breaking it down into these multiple tasks and mining their data
4107360	4112080	constantly for like edge cases for these different tasks. But the tasks themselves are not being
4112080	4121120	learned. This is feature engineering. I mean, it's a higher abstraction level of feature
4121120	4125920	engineering for the different tasks. It's task engineering in a sense. It's slightly better
4125920	4130560	feature engineering, but it's still fundamentally as feature engineering. And if anything about the
4130560	4135680	history of AI has taught us anything, it's that feature engineering approaches will always be
4135680	4142000	replaced and lose to end to end. Now, to be fair, I cannot really make promises on timelines,
4142000	4146960	but I can say that when you look at the code for stock fish and the code for alpha zero,
4146960	4151440	one is a lot shorter than the other. A lot more elegant required a lot less programmer hours to
4151440	4165840	write. Yeah, but there was a lot more murder of bad agents on the alpha zero side. By murder, I mean
4167840	4174240	agents that played a game and failed miserably. Yeah. Oh, in simulation, that failure is less
4174240	4179680	costly. Yeah. In real world. Wait, do you mean in practice, like alpha zero has lost games
4179760	4186960	miserably? No, I haven't seen that. No, but I know the requirement for alpha zero is
4188000	4193440	to be able to like evolution, human evolution, not human evolution, biological evolution of life
4193440	4201280	on earth from the origin of life has murdered trillions upon trillions of organisms on the path
4201280	4207680	thus humans. So the question is, can we stitch together a human like object without having to
4207680	4211840	go through the entirety process of evolution? Well, no, but do the evolution in simulation?
4211840	4215680	Yeah, that's the question. Can we simulate? So do you ever sense that it's possible to simulate
4215680	4222320	some aspect of it? Mu zero is exactly this. Mu zero is the solution to this. Mu zero, I think,
4222320	4226720	is going to be looked back as the canonical paper. And I don't think deep learning is everything.
4226720	4230320	I think that there's still a bunch of things missing to get there. But Mu zero, I think,
4230320	4236960	is going to be looked back as the kind of cornerstone paper of this whole deep learning era.
4236960	4241280	And Mu zero is the solution to self-driving cars. You have to make a few tweaks to it. But
4241280	4247040	Mu zero does effectively that. It does those rollouts and those murdering in a learned
4247040	4251440	simulator and a learned dynamics model. It's interesting. It doesn't get enough love.
4251440	4256320	I was blown away when I was blown away when I read that paper. I'm like, okay, I've always
4256320	4259360	said a comma. I'm going to sit and I'm going to wait for the solution to self-driving cars to come
4259360	4268160	along. This year, I saw it. It's Mu zero. So sit back and let the winning roll in.
4269040	4275040	So your sense just to elaborate a little bit to link on the topic. Your sense is neural networks
4275040	4278160	will solve driving. Yes. Like we don't need anything else.
4278720	4283200	I think the same way chess was maybe the chess and maybe Google are the pinnacle of like
4284000	4291440	search algorithms and things that look kind of like a star. The pinnacle of this error is going
4291440	4299440	to be self-driving cars. But on the path that you have to deliver products and it's possible that
4299440	4306720	the path to full self-driving cars will take decades. I doubt it. How long would you put on it?
4307680	4314720	Like what are we? You're chasing it. Tesla's chasing it. What are we talking about? 5 years,
4314720	4320960	10 years, 50 years? Let's say in the 2020s. In the 2020s. The later part of the 2020s.
4323520	4328240	With the neural network, that would be nice to see. And on the path to that, you're delivering
4328240	4332960	products, which is a nice L2 system. That's what Tesla is doing, a nice L2 system.
4332960	4336560	Just gets better every time. The only difference between L2 and the other levels
4336560	4340240	is who takes liability. And I'm not a liability guy. I don't want to take liability. I'm going to
4340240	4348800	level 2 forever. Now on that little transition, I mean, how do you make the transition work?
4349520	4355840	Is this where driver sensing comes in? Like how do you make the, because you said 100 miles,
4355840	4362080	like is there some sort of human factor psychology thing where people start to
4362080	4366080	over-trust the system, all those kinds of effects. Once it gets better and better and
4366080	4370880	better and better, they get lazier and lazier and lazier. Is that, like how do you get that
4370880	4375520	transition right? First off, our monitoring is already adaptive. Our monitoring is already seen
4375520	4380800	adaptive. Driver monitoring is just the camera that's looking at the driver. You have an infrared
4380800	4388560	camera. Our policy for how we enforce the driver monitoring is seen adaptive. What's that mean?
4389120	4396800	For example, in one of the extreme cases, if the car is not moving, we do not actively enforce
4396800	4404480	driver monitoring. If you are going through like a 45-mile-an-hour road with lights
4405680	4410720	and stop signs and potentially pedestrians, we enforce a very tight driver monitoring policy.
4410720	4415520	If you are alone on a perfectly straight highway, and it's all machine learning,
4415520	4418960	none of that is hand-coded. Actually, the stop is hand-coded, but...
4418960	4422080	So there's some kind of machine learning estimation of risk.
4423520	4429120	Yeah. I mean, I've always been a huge fan of that. That's a, it's difficult to do
4430640	4436240	every step into that direction is a worthwhile stop to take. It might be difficult to do really
4436240	4441360	well. Like us humans are able to estimate risk pretty damn well. Whatever the hell that is,
4441440	4448880	that feels like one of the nice features of us humans. Because we humans are really good drivers
4448880	4454000	when we're really tuned in. And we're good at estimating risk, like when are we supposed to
4454000	4460400	be tuned in? Yeah. And people are like, oh, well, why would you ever make the driver monitoring policy
4460400	4464560	less aggressive? Why would you always not keep it at its most aggressive? Because then people
4464560	4467680	are just going to get fatigued from it. Yeah. Well, they get annoyed. You want them,
4468240	4472400	you want the experience to be pleasant. Obviously, I want the experience to be pleasant,
4472400	4478240	but even just from a straight up safety perspective, if you alert people when they look
4478240	4482400	around and they're like, why is this thing alerting me? There's nothing I could possibly hit right
4482400	4487040	now. People will just learn to tune it out. People will just learn to tune it out, to put
4487040	4492160	weights on the steering wheel, to do whatever, to overcome it. And remember that you're always
4492160	4496880	part of this adaptive system. So all I can really say about how this scales going forward is,
4496880	4501680	yeah, something we have to monitor for. We don't know. This is a great psychology experiment at
4501680	4505920	scale. We'll see. Yeah, it's fascinating. Track it. And making sure you have a good
4506560	4511360	understanding of attention is a very key part of that psychology problem.
4511360	4515600	Yeah. I think you and I probably have a different come to it differently, but to me,
4516560	4522160	it's a fascinating psychology problem to explore something much deeper than just driving. It's
4523040	4530080	it's such a nice way to explore human attention and human behavior, which is why, again,
4530080	4537280	we've probably both criticized Mr. Elon Musk on this one topic from different avenues.
4538000	4541280	So both offline and online, I had little chats with Elon and
4544000	4550160	like, I love human beings as a computer vision problem, as an AI problem. It's fascinating.
4551040	4552960	He wasn't so much interested in that problem.
4554400	4559040	It's like, in order to solve driving, the whole point is you want to remove the human from the
4559040	4566640	picture. And it seems like you can't do that quite yet. Eventually, yes, but you can't quite do that
4566640	4577600	yet. So this is the moment where you can't yet say, I told you so to Tesla, but it's getting there
4577600	4581760	because I don't know if you've seen this, there's some reporting that they're in fact starting to do
4582400	4584320	driver monitoring. Yeah, they ship the model in shadow mode.
4586160	4593120	We though, I believe only a visible light camera. It might even be fisheye. It's like a low resolution.
4593120	4597040	Low resolution visible light. I mean, to be fair, that's what we have in the Eon as well,
4597040	4602000	our last generation product. This is the one area where I can say our hardware is ahead of Tesla.
4602000	4605120	The rest of our hardware way way behind, but our driver monitoring camera.
4605840	4612320	So you think, I think on the third row Tesla podcast or somewhere else, I've heard you say that
4613680	4616240	obviously eventually they're going to have driver monitoring.
4617040	4621200	I think what I've said is Elon will definitely ship driver monitoring before he ships level
4621200	4625840	five. And I'm willing to bet 10 grand on that. And you bet 10 grand on that.
4626880	4629600	I mean, now I don't want to take the bet, but before, maybe someone would have thought I should
4629600	4638480	have got my money. It's an interesting bet. I think you're right. I'm actually on a human level
4639040	4646880	because he's made the decision. He said that driver monitoring is the wrong way to go.
4647520	4654400	But you have to think of as a human as a CEO. I think that's the right thing to say when
4654400	4661680	like sometimes you have to say things publicly that different than when you actually believe
4661680	4666800	because when you're producing a large number of vehicles and the decision was made not to
4666800	4671920	include the camera, like what are you supposed to say? Like our cars don't have the thing that I
4671920	4678160	think is right to have. It's an interesting thing. But like on the other side as a CEO,
4678160	4683520	I mean, something you could probably speak to as a leader, I think about me as a human
4684800	4689440	to publicly change your mind on something. How hard is that? Well, especially when assholes
4689440	4695760	like George Haas say, I told you so. All I will say is I am not a leader and I am happy to change
4695760	4705280	my mind. You think Elon will? Yeah, I do. I think he'll come up with a good way to make it
4705280	4710480	psychologically okay for him. Well, it's such an important thing, man, especially for a first
4710480	4715680	principles thinker because he made a decision that driver monitoring is not the right way to go.
4715680	4720320	And I could see that decision and I could even make that decision. Like I was on the fence
4721280	4728640	too. Like I'm not a driver monitoring is such an obvious simple solution to the problem of
4728640	4734320	attention. It's not obvious to me that just by putting a camera there, you solve things. You
4734320	4740960	have to create an incredible compelling experience just like you're talking about.
4740960	4746160	I don't know if it's easy to do that. It's not at all easy to do that, in fact, I think. So
4747120	4752800	as a creator of a car that's trying to create a product that people love, which is what Tesla
4752800	4759840	tries to do, right? It's not obvious to me that as a design decision, whether adding a camera
4759920	4765280	is a good idea. From a safety perspective either, like in the human factors community,
4765280	4771120	everybody says that you should obviously have driver sensing, driver monitoring. But like
4772480	4778880	that's like saying it's obvious as parents you shouldn't let your kids go out at night.
4779920	4785440	But okay. But like they're still going to find ways to do drugs.
4786000	4792000	Yeah. You have to also be good parents. So it's much more complicated than just
4792720	4798800	you need to have driver monitoring. I totally disagree on, okay, if you have a camera there
4798800	4802880	and the camera is watching the person but never throws an alert, they'll never think about it.
4803600	4809840	Right? The driver monitoring policy that you choose to, how you choose to communicate with
4809840	4820320	the user is entirely separate from the data collection perspective. Right? So there's one
4820320	4826640	thing to say, tell your teenager they can't do something. There's another thing to gather the
4826640	4830240	data. So you can make informed decisions. That's really interesting. But you have to make that,
4831040	4836960	that's the interesting thing about cars. But even true with Comm AI, you don't have to
4836960	4841120	manufacture the thing into the car is you have to make a decision that anticipates
4841920	4847360	the right strategy long term. So like you have to start collecting the data and start making
4847360	4852080	decisions. Started it, started it three years ago. I believe that we have the best driver
4852080	4857680	monitoring solution in the world. I think that when you compare it to Supercruise is the only
4857680	4864880	other one that I really know that shipped and ours is better. What do you like and not like about
4864880	4871920	Supercruise? I mean, I had a few Supercruise, the sun would be shining through the window,
4871920	4875440	would blind the camera, and it would say I wasn't paying attention when I was looking completely
4875440	4879760	straight. I couldn't reset the attention with a steering wheel touch and Supercruise would
4879760	4884320	disengage. Like I was communicating to the car, I'm like, look, I am here, I am paying attention.
4884320	4890400	Why are you really going to force me to disengage? And it did. So it's a constant
4890400	4894640	conversation with the user. And yeah, there's no way to ship a system like this if you can OTA.
4895680	4900960	We're shipping a new one every month. Sometimes we balance it with our users on Discord. Sometimes
4900960	4904320	we make the driver monitoring a little more aggressive and people complain. Sometimes they
4904320	4908560	don't. We want it to be as aggressive as possible where people don't complain and it doesn't feel
4908560	4912640	intrusive. So being able to update the system over the air is an essential component. I mean,
4912640	4922880	that's probably, to me, that is the biggest innovation of Tesla, that it made it. People
4922880	4930000	realize that over the air updates is essential. Yeah. Was that not obvious from the iPhone?
4930000	4934560	The iPhone was the first real product that OTAed, I think. Was it? Actually, that's brilliant.
4934560	4937840	You're right. I mean, the game consoles used to not, right? The game consoles were maybe the
4937840	4942160	second thing that did. Well, I didn't really think about it. One of the amazing features
4942160	4949520	of a smartphone isn't just like the touchscreen isn't the thing. It's the ability to constantly
4949520	4960720	update. Yeah. It gets better. It gets better. I love my iOS 14. Yeah. One thing that I probably
4960720	4967760	disagree with you on driver monitoring is you said that it's easy. I mean, you tend to say
4967760	4975520	stuff is easy. I guess you said it's easy relative to the external perception problem there.
4975760	4983680	Can you elaborate why you think it's easy? Feature engineering works for driver monitoring. Feature
4983680	4990640	engineering does not work for the external. So human faces are not, human faces and the movement
4990640	4996080	of human faces and head and body is not as variable as the external environment? Yeah.
4996080	4999360	Yeah. Because you're intuition. Yes. And there's another big difference as well.
5000080	5004240	Your reliability of a driver monitoring system doesn't actually need to be that high.
5004320	5008320	The uncertainty, if you have something that's detecting whether the human's paying attention
5008320	5012640	and it only works 92% of the time, you're still getting almost all the benefit of that
5012640	5017520	because the human, you're training the human. You're dealing with a system that's
5017520	5023360	really helping you out. It's a conversation. It's not the external thing where guess what?
5023360	5028240	If you swerve into a tree, you swerve into a tree. You get no margin for error there.
5028240	5034160	Yeah. I think that's really well put. I think that's the right, exactly the place where
5036720	5041440	comparing to the external perception and the control problem, driver monitoring is easier
5041440	5047840	because the bar for success is much lower. Yeah. But I still think the human face
5049120	5053280	is more complicated actually than the external environment. But for driving, you don't give a
5053280	5062000	damn. I don't need something that complicated to have to communicate the idea to the human
5062000	5066480	that I want to communicate, which is, yo, system might mess up here. You got to pay attention.
5067680	5073840	Yeah. See, that's my love and fascination is the human face. And it feels like this is
5074800	5082240	a nice place to create products that create an experience in the car. It feels like there should
5082240	5092080	be more richer experiences in the car. That's an opportunity for something like AMAI or just any
5092080	5097040	kind of system like a Tesla or any of the autonomous vehicle companies is because software,
5097600	5101920	there's much more sensors and so much is running on software and you're doing machine learning anyway,
5102720	5108080	there's an opportunity to create totally new experiences that we're not even anticipating.
5108160	5114080	You don't think so? No. You think it's a box that gets you from A to B and you want to do it chill?
5114960	5119200	Yeah. I mean, I think as soon as we get to level three on highways, okay, enjoy your candy crush,
5119200	5124720	enjoy your Hulu, enjoy your, you know, whatever, whatever. Sure, you get this, you can look at
5124720	5129360	screens basically versus right now, what do you have? Music and audiobooks. So level three is
5129360	5137280	where you can kind of disengage and stretch this of time. Well, you think level three is possible?
5137280	5144560	Like on the highway going 400 miles and you can just go to sleep? Oh yeah, sleep. So again,
5144560	5150480	I think it's really all on a spectrum. I think that being able to use your phone while you're on
5150480	5155600	the highway and like this all being okay and being aware that the car might alert you and you have
5155600	5159120	five seconds to basically. So the five second thing is that you think it's possible? Yeah,
5159120	5164320	I think it is. Oh yeah. Not in all scenarios. Right. Some scenarios it's not. It's the whole
5164320	5170480	risk thing that you mentioned is nice is to be able to estimate like how risk is this situation.
5170480	5177280	That's really important to understand. One other thing you mentioned comparing comma and autopilot
5177280	5185760	is that something about the haptic feel of the way comma controls the car when things are uncertain,
5185760	5190400	like it behaves a little bit more uncertain when things are uncertain. That's kind of an
5190400	5195440	interesting point. And then autopilot is much more confident always, even when it's uncertain,
5195440	5202160	until it runs into trouble. That's a funny thing. I actually mentioned that to Elon,
5202160	5207920	I think, and then the first time we talked he was inviting is like communicating uncertainty.
5208720	5213280	I guess comma doesn't really communicate uncertainty explicitly. It communicates it
5213280	5218000	through haptic feel. Like what's the role of communicating uncertainty, do you think?
5218080	5221680	We do some stuff explicitly. We do detect the lanes when you're on the highway,
5221680	5225280	and we'll show you how many lanes we're using to drive with. You can look at where it thinks
5225280	5231440	the lanes are. You can look at the path. We want to be better about this. We're actually hiring.
5231440	5236080	We want to hire some new UI people. UI people. You mentioned this because it's such a UI problem
5236080	5241120	too. We have a great designer now, but we need people who are just going to build this and
5241120	5246320	debug these UIs, QT people. QT. Is that what the UI has done with this QT?
5246400	5250240	Moving. The new UI is in QT. C++, QT?
5251840	5255760	Tesla uses it too. We had some React stuff in there.
5257760	5261040	React.js or just React. React has its own language, right?
5261040	5269680	React Native. React is a JavaScript framework. It's all based on JavaScript, but I like C++.
5270160	5278240	QT. What do you think about Dojo with Tesla and their foray into what appears to be
5280160	5287280	specialized hardware for training on that? I guess it's something, maybe you can correct me,
5287280	5292000	for my shallow looking at it. It seems like something that Google did with TPUs,
5292000	5297120	but specialized for driving data. I don't think it's specialized for driving data.
5297120	5302560	QT. It's just legit, just TPU. They want to go the Apple way. Basically,
5302560	5305200	everything required in the chain is done in-house.
5305200	5312720	QT. You have a problem right now. This is one of my concerns. I really would like to see
5312720	5316800	somebody deal with this. If anyone out there is doing it, I'd like to help them if I can.
5317840	5323520	You basically have two options right now to train. Your options are Nvidia or Google.
5324480	5332240	QT. Google is not even an option. Their TPUs are only available in Google Cloud.
5333040	5339120	Google has absolutely onerous terms of service restrictions. They may have changed it,
5339120	5342400	but back in Google's terms of service, it said explicitly you are not allowed to use
5342400	5347200	Google Cloud ML for training autonomous vehicles or for doing anything that competes with Google
5347200	5352080	without Google's prior written permission. Google is not a platform company.
5354480	5358320	I wouldn't touch TPUs with a 10-foot pole. That leaves you with the monopoly.
5358320	5359600	QT. Nvidia?
5359600	5361440	QT. Nvidia.
5361440	5363120	QT. You're not a fan of?
5363120	5372640	QT. Well, look, I was a huge fan of 2016 Nvidia. Jensen came sat in the car. Cool guy
5372640	5379760	when the stock was $30 a share. Nvidia stock has skyrocketed. I witnessed a real change
5379840	5387280	in who was in management over there in 2018. Now they are, let's exploit, let's take every
5387280	5392080	dollar we possibly can out of this ecosystem. Let's charge $10,000 for A100s because we know
5392080	5398800	we got the best share in the game. Let's charge $10,000 for an A100 when it's really not that
5398800	5406080	different from $3080, which is $699. The margins that they are making off of those high-end chips
5406080	5411120	are so high that I think they're shooting themselves in the foot just from a business
5411120	5415760	perspective because there's a lot of people talking like me now who are like, somebody's
5415760	5422000	got to take Nvidia down. Where they could dominate, Nvidia could be the new Intel.
5422000	5431120	QT. Yeah, to be inside everything essentially. Yet the winners in certain spaces like in
5431200	5436800	autonomous driving, the winners, only the people who are like desperately falling back and trying
5436800	5441520	to catch up and have a ton of money like the big automakers are the ones interested in partnering
5441520	5446240	with Nvidia. Jensen. Oh, and I think a lot of those things are going to fall through. If I were
5446240	5453440	in Nvidia, sell chips. Sell chips at a reasonable markup. QT. To everybody. Jensen. To everybody.
5453440	5458080	QT. Without any restrictions. Jensen. Without any restrictions. Intel did this. Look at Intel.
5458160	5462960	They had a great long run. Nvidia is trying to turn their, they're like trying to productize
5462960	5469360	their chips way too much. They're trying to extract way more value than they can sustainably.
5469360	5472960	Sure. You can do it tomorrow. Is it going to up your share price? Sure. If you're one of those
5472960	5477760	CEOs, it's like, how much can I strip mine in this company? And that's what's weird about it too.
5477760	5482160	Like the CEO is the founder. It's the same guy. I mean, I still think Jensen's a great guy.
5482240	5488560	It is great. Why do this? You have a choice. You have a choice right now. Are you trying to cash
5488560	5494880	out? Are you trying to buy a yacht? If you are, fine. But if you're trying to be the next huge
5494880	5501280	semiconductor company, sell chips. Well, the interesting thing about Jensen is he is a big
5501280	5510400	vision guy. So he has a plan like for 50 years down the road. So it makes me wonder like...
5510400	5515920	How does price gouging fit into it? Yeah. How does that fit? It doesn't seem to make sense of the
5515920	5522400	plan. I worry that he's listening to the wrong people. Yeah. That's the sense I have too sometimes
5522400	5531040	because I, despite everything, I think Nvidia is an incredible company. Well, one, I'm deeply
5531040	5535120	grateful to Nvidia for the products they've created in the past. Me too. And so...
5536000	5538880	The 1080 Ti was a great GPU. Still have a lot of them.
5538880	5544080	Still is. Yeah. But at the same time, it just feels like...
5546720	5551040	It feels like you don't want to put all your stock in Nvidia. And so the Elon is doing...
5551920	5558320	What Tesla is doing with autopilot and Dojo is the Apple way. Because they're not going to share
5558320	5564560	Dojo with George Hott's. I know. They should sell that chip. Oh, they should sell that.
5564640	5568320	Even their accelerator. The accelerator that's in all the cars, the 30 watt one.
5569040	5575120	Sell it. Why not? So open it up. Make me... Why does Tesla have to be a car company?
5575680	5580480	Well, if you sell the chip, here's what you get. Yeah. Makes the money out of the chips. It doesn't
5580480	5586160	take away from your chip. You're going to make some money, free money. And also the world is going
5586160	5591040	to build an ecosystem of tooling for you. All right. You're not going to have to fix the bug
5591040	5596640	in your 10H layer. Someone else already did. Well, the question... That's an interesting question.
5596640	5599920	I mean, that's the question Steve Jobs asked. That's the question Elon Musk is
5601920	5610960	perhaps asking is, do you want Tesla stuff inside other vehicles inside? Potentially inside like
5610960	5617760	iRobot Vacuum Cleaner? Yeah. I think you should decide where your advantages are. I'm not saying
5617760	5621680	Tesla should start selling battery packs to automakers because battery packs to automakers,
5621680	5625360	they're straight up in competition with you. If I were Tesla, I'd keep the battery technology
5625360	5633120	totally as far as we make batteries. But the thing about the Tesla TPU is anybody can build that.
5633120	5638960	It's just a question of, are you willing to spend the money? It could be a huge source of revenue
5638960	5644640	potentially. Are you willing to spend $100 million? Anyone can build it. And someone will.
5644640	5648400	And a bunch of companies now are starting trying to build AI accelerators. Somebody's
5648400	5654640	going to get the idea right. And hopefully they don't get greedy because they'll just lose to
5654640	5657760	the next guy who finally... And then eventually the Chinese are going to make knockoff and video
5657760	5662080	chips and that's... From your perspective, I don't know if you're also paying attention to
5662080	5670320	Stan Tesla for a moment. Elon Musk has talked about a complete rewrite of the neural net that
5670400	5677680	they're using that seems to... Again, I'm half paying attention, but it seems to involve basically
5677680	5685920	a kind of integration of all the sensors to where it's a four-dimensional view. You have a 3D model
5685920	5692720	of the world over time and then you can... I think it's done both for the... Actually,
5693200	5698480	so the neural network is able to in a more holistic way deal with the world and make predictions
5698560	5707680	and so on, but also to make the annotation task more easier. You can annotate the world in one
5707680	5711840	place and they kind of distribute itself across the sensors and across the different...
5712880	5718560	Like the hundreds of tests that are involved in the hydranet. What are your thoughts about this
5718560	5723920	rewrite? Is it just like some details that are kind of obvious, there are steps that should be taken
5723920	5728240	or is there something fundamental that could challenge your idea that end-to-end
5728960	5733520	is the right solution? We're in the middle of a big review right now as well. We haven't shipped a
5733520	5739280	new model in a bit. Of what kind? We're going from 2D to 3D. Right now, all our stuff like, for
5739280	5744480	example, when the car pitches back, the lane lines also pitch back because we're assuming the flat
5745280	5749840	world hypothesis, the new models do not do this. The new models output everything in 3D.
5749840	5755920	But there's still no annotation, so the 3D is more about the output.
5757920	5766480	We have Zs and everything. We've... Zs. We added Zs. We unified a lot of stuff as well.
5766480	5773680	We switched from TensorFlow to PyTorch. My understanding of what Tesla's thing is,
5773680	5776560	is that their annotator now annotates across the time dimension.
5779840	5786400	I mean, cute. Why are you building an annotator? I find their entire pipeline.
5788160	5793840	I find your vision, I mean, the vision of end-to-end very compelling. But I also like the
5793840	5798800	engineering of the data engine that they've created. In terms of supervised learning
5799040	5806240	pipelines, that thing is damn impressive. You're basically the idea is that you have
5807360	5812240	hundreds of thousands of people that are doing data collection for you by doing their experience.
5812240	5820160	So that's kind of similar to the Kama AI model. And you're able to mine that data based on the
5820160	5826480	kind of education you need. I think it's harder to do in the end-to-end learning.
5827280	5833360	The mining of the right edge cases. That's what feature engineering is actually really powerful
5834080	5841040	because us humans are able to do this kind of mining a little better. But yeah, there's obvious,
5841040	5844560	as we know, there's obvious constraints and limitations to that idea.
5845760	5851120	Carpathian just tweeted, he's like, you get really interesting insights if you sort your
5851120	5861280	validation set by loss and look at the highest loss examples. So yeah, we have a little data
5861280	5865760	engine-like thing. We're training a Sagnat. Anyway, it's not fancy, it's just like, okay,
5866640	5871440	train the new Sagnat, run it on 100,000 images, and now take the thousand with highest loss,
5872080	5877120	select 100 of those by human, put those, get those ones labeled, retrain, do it again.
5877840	5883520	So it's a much less well-written data engine. And yeah, you can take these things really far.
5883520	5889760	And it is impressive engineering. And if you truly need supervised data for a problem,
5889760	5895440	yeah, things like data engine are at the high end of what is attention? Is a human paying
5895440	5899120	attention? I mean, we're going to probably build something that looks like data engine to push
5899120	5904480	our driver monitoring further. But for driving itself, you have it all annotated beautifully
5904560	5909200	by what the human does. Yeah, that's interesting. I mean, that applies to driver attention as well.
5909920	5913440	Do you want to detect the eyes? Do you want to detect blinking and pupil movement?
5913440	5918080	Do you want to detect all the like face alignments, the landmark detection and so on,
5918640	5923200	and then doing kind of reasoning based on that? Or do you want to take the entirety of the face
5923200	5928880	over time and do end to end? I mean, it's obvious that eventually you have to do end to end with
5928880	5935680	some calibration, some fixes and so on. But it's like, I don't know when that's the right move.
5935680	5942560	Even if it's end to end, there actually is, there is no kind of, you have to supervise that with
5942560	5946880	humans. Whether a human is paying attention or not is a completely subjective judgment.
5948480	5952320	Like you can try to like automatically do it with some stuff, but you don't have.
5953040	5958320	If I record a video of a human, I don't have true annotations anywhere in that video.
5958320	5962800	The only way to get them is with other humans labeling it really.
5962800	5970880	Well, I don't know. If you think deeply about it, you might be able to, depending on the task,
5970880	5976240	you might be able to discover self-anitating things like, you can look at like steering
5976240	5980240	wheel reverse or something like that. You can discover little moments of lapse of attention.
5983040	5987360	That's where psychology comes in. Is there an indicator? Because you have so much data to look
5987360	5994240	at. So you might be able to find moments when there's just inattention that even with smartphone,
5994240	5999360	if you want to take smartphone use, you can start to zoom in. I mean, that's the gold mine,
5999360	6003840	sort of the comma AI. I mean, Tesla is doing this too, right? They're doing
6005440	6011760	annotation based on like self-supervised learning too. It's just a small part of the
6011760	6018880	entire picture. That's kind of the challenge of solving a problem in machine learning if you
6018880	6026720	can discover self-anitating parts of the problem, right? Our driver monitoring team is half a person
6026720	6031920	right now. Half a person. You know, once we have... Scale to a full... Once we have two,
6031920	6036320	three people on that team, I definitely want to look at self-anitating stuff for attention.
6036880	6045360	Let's go back for a sec to a comma. For people who are curious to try it out,
6046240	6052720	how do you install a comma in say a 2020 Toyota Corolla? Or like, what are the cars that are
6052720	6058800	supported? What are the cars that you recommend? And what does it take? You have a few videos out,
6058800	6062800	but maybe through words, can you explain what's it take to actually install a thing?
6062800	6069120	So we support... I think it's 91 cars. 91 makes models. We get to 100 this year.
6070080	6081200	Nice. The 2020 Corolla, great choice. The 2020 Sonata, it's using the stock longitudinal. It's
6081200	6086560	using just our lateral control, but it's a very refined car. Their longitudinal control is not
6086560	6094160	bad at all. So yeah, Corolla, Sonata. Or if you're willing to get your hands a little dirty and
6094160	6098480	look in the right places on the internet, the Honda Civic is great, but you're going to have to
6098480	6102880	install a modified EPS firmware in order to get a little bit more torque. And I can't help you
6102880	6108480	with that. Comma does not efficiently endorse that, but we have been doing it. We didn't ever release
6108480	6114400	it. We waited for someone else to discover it. And then, you know... And you have a Discord server
6114480	6122320	where people... There's a very active developer community, I suppose. So depending on the level
6122320	6129440	of experimentation you're willing to do, that's a community. If you just want to buy it and you
6129440	6136960	have a supported car, it's 10 minutes to install. There's YouTube videos. It's IKEA furniture level.
6136960	6141200	If you can set up a table from IKEA, you can install a Comma 2 in your supported car,
6141200	6145680	and it will just work. Now you're like, oh, but I want this high-end feature or I want to fix this
6145680	6151600	bug. Okay, well, welcome to the developer community. So what... If I wanted to... This is something I
6151600	6162080	asked you offline, like a few months ago. If I wanted to run my own code to... So use Comma as a
6162640	6166960	platform and try to run something like OpenPilot, what does it take to do that?
6167760	6174000	So there's a toggle in the settings called enable SSH. And if you toggle that, you can SSH into
6174000	6178560	your device. You can modify the code. You can upload whatever code you want to it. There's a whole
6178560	6184800	lot of people. So about 60% of people are running stock Comma. About 40% of people are running Forks.
6185360	6190800	And there's a community of... There's a bunch of people who maintain these Forks. And these Forks
6190800	6197200	support different cars or they have different toggles. We try to keep away from the toggles
6197200	6202320	that are disabled driver monitoring. But some people might want that kind of thing. And yeah,
6202320	6211200	you can... It's your car. It's your... I'm not here to tell you. We have some... We ban... If you're
6211200	6214480	trying to subvert safety features, you're banned from our Discord. I don't want anything to do with
6214480	6223360	you. But there's some Forks doing that. Got it. So you encourage responsible Forking. Yeah. Yeah,
6223360	6228560	we encourage... Some people... Yeah, some people... Like there's Forks that will do... Some people just
6228560	6234160	like having a lot of readouts on the UI. Like a lot of flashing numbers. So there's Forks that do
6234160	6238400	that. Some people don't like the fact that it disengages when you press the gas pedal. There's
6238400	6245760	Forks that disable that. Got it. Now, the stock experience is what like... So it does both lane
6245760	6251120	keeping and longitudinal control altogether. So it's not separate like it is an autopilot.
6251120	6255760	No. So, okay. Some cars, we use the stock longitudinal control. We don't do the longitudinal
6255760	6259920	control in all the cars. Some cars, the ACCs are pretty good in the cars. It's the lane
6259920	6265440	keep that's atrocious in anything except for autopilot and supercruise. But you just turn it on
6266000	6271680	and it works. What does disengagement look like? Yeah. So we have... I mean, I'm very concerned
6271680	6278800	about mode confusion. I've experienced it on supercruise and autopilot where like autopilot
6278800	6284880	disengages. I don't realize that the ACC is still on. The lead car moves slightly over and then the
6284880	6289520	Tesla accelerates to like whatever my set speed is super fast and like what's going on here.
6290480	6296320	We have engaged and disengaged. And this is similar to my understanding. I'm not a pilot,
6296320	6301520	but my understanding is either the pilot is in control or the copilot is in control.
6302080	6307440	And we have the same kind of transition system. Either open pilot is engaged or open pilot is
6307440	6312400	disengaged. Engage with cruise control, disengage with either gas break or cancel.
6313200	6317280	Let's talk about money. What's the business strategy for karma?
6317360	6326080	Profitable. Well, you did it. So congratulations. What... So it's basically selling, we should
6326080	6331520	say, karma costs a thousand bucks, comma two. Two hundred for the interface to the car as well.
6331520	6338480	It's 1200. I'll send that. Nobody's usually upfront like this. You gotta add the tack on, right?
6338480	6343760	I love it. I'm not gonna lie to you. Trust me, it will add $1,200 of value to your life.
6343760	6347840	Yes, it's still super cheap. 30 days, no questions asked, money back guarantee,
6347840	6352880	and prices are only going up. If there ever is future hardware, it could cost a lot more than
6352880	6359760	$1,200. So comma three is in the works. It could be. All I will say is future hardware is going
6359760	6365760	to cost a lot more than the current hardware. Yeah. The people that use... The people I've
6365760	6371440	spoken with that use comma, they use open pilot, they... At first, they use it a lot.
6372080	6376000	So people that use it, they fall in love with it. Oh, our retention rate is insane.
6376640	6383680	It's a good sign. Yeah. It's a really good sign. 70% of comma two buyers are daily active users.
6383680	6390000	Yeah, it's amazing. Oh, also, we don't plan on stopping selling the comma two.
6391840	6400160	So whatever you create that's beyond comma two, it would be potentially a phase shift.
6401120	6405520	It's so much better that... You could use comma two and you can use comma whatever.
6405520	6411920	Depends what you want. It's 3.41, 42. Yeah. Auto pilot, hardware one versus hardware two.
6411920	6415280	The comma two is kind of like hardware one. Got it. You can still use both. Got it.
6416160	6420960	I think I heard you talk about retention rate with the BR headsets that the average is just once.
6422240	6426640	I mean, it's such a fascinating way to think about technology. And this is a really,
6426640	6429760	really good sign. And the other thing that people say about comma is they can't believe
6429760	6439040	they're getting this 4,000 bucks. It seems like some kind of steal. But in terms of long-term
6439040	6453040	business strategies, it's currently in 1,000 plus cars. 1,200. More. So yeah,
6453520	6458800	uh, daily is about 2,000. Weekly is about 2,500. Monthly is over 3,000.
6458800	6462080	Wow. We've grown a lot since we last talked.
6462080	6467360	Is the goal... Can we talk crazy for a second? I mean, what's the goal to overtake Tesla?
6468560	6471440	Let's talk... Okay, so... I mean, Android did overtake iOS.
6471440	6474640	That's exactly it, right? So... Yeah.
6474640	6480400	They did it. I actually don't know the timeline of that one. But let's talk...
6480400	6484640	Because everything is in alpha now. The autopilot, you could argue, is in alpha in terms of
6484640	6489760	towards the big mission of autonomous driving, right? And so what... Yeah,
6489760	6496080	it's your goal to overtake millions of cars, essentially. Of course. Where would it stop?
6496640	6500320	Like, it's open-source software. It might not be millions of cars with a piece of comma hardware.
6500320	6506800	But yeah, I think OpenPilot at some point will cross over autopilot in users,
6506880	6511040	just like Android crossed over iOS. How does Google make money from Android?
6514080	6516160	It's complicated. Their own devices make money.
6517280	6521040	Google... Google makes money by just kind of having you on the internet.
6522080	6525440	Yes. Google Search is built in. Gmail is built in.
6525440	6528080	Android is just a shield for the rest of Google's ecosystem kind.
6528080	6532640	Yeah, but the problem is, Android is not... It's a brilliant thing. I mean,
6533360	6540720	Android arguably changed the world. So there you go. You can feel good ethically speaking.
6540720	6544160	But as a business strategy, it's questionable.
6544160	6544880	It's so hardware.
6545680	6548000	So hardware. I mean, it took Google a long time to come around to it,
6548000	6549360	but they are now making money on the Pixel.
6550080	6553120	You're not about money. You're more about winning.
6553120	6554000	Yeah, of course.
6554000	6559840	But if only 10% of OpenPilot devices come from comma AI...
6559840	6560720	They still make a lot.
6560720	6562720	That is still yes. That is a ton of money for our company.
6562720	6566400	But can't somebody create a better comma using OpenPilot?
6566960	6568640	Or are you basically saying we'll out-compete them?
6568640	6569360	We'll out-compete you.
6569360	6572160	Is... Can you create a better Android phone than the Google Pixel?
6572160	6572560	Right.
6572560	6574560	I mean, you can, but like, you know...
6574560	6578240	I love that. So you're confident, like, you know what the hell you're doing.
6578240	6578480	Yeah.
6579920	6583280	It's competence and merit.
6583280	6586080	I mean, our money... Yeah, our money comes from... We're a consumer electronics company.
6586080	6586400	Yeah.
6586400	6589840	And put it this way. So we sold... We sold like 3,000 comma twos.
6591520	6592320	2,500 right now.
6598640	6601040	Okay. We're probably going to sell 10,000 units next year.
6601840	6608160	10,000 units. Even just $1,000 a unit. Okay. We're at $10 million in revenue.
6609280	6612000	Get that up to $100,000. Maybe double the price of the unit.
6612000	6613440	Now we're talking like $200 million revenue.
6613440	6613840	We're talking like a series.
6613840	6614800	Yeah, actually making money.
6615760	6620480	One of the rare semi-autonomous or autonomous vehicle companies that are actually making
6620480	6621280	money. Yeah.
6622480	6625200	You know, if you have... If you look at a model,
6625200	6628240	when we were just talking about this yesterday, if you look at a model and like you're testing,
6628240	6632240	like you're A-B testing your model, and if you're one branch of the A-B test,
6632240	6635200	the losses go down very fast in the first five epochs.
6635200	6639200	That model is probably going to converge to something considerably better than the one
6639200	6642880	where the losses are going down slower. Why do people think this is going to stop?
6642880	6645520	Why do people think one day there's going to be a great like,
6645520	6648000	well, Waymo's eventually going to surpass you guys?
6648240	6650080	Well, they're not.
6651840	6657760	Do you see like a world where like a Tesla or a car like a Tesla would be able to basically
6657760	6663680	press a button and you like switch to open pilot, you know, you load in?
6664480	6670000	No. So I think, so first off, I think that we may surpass Tesla in terms of users.
6670560	6673360	I do not think we're going to surpass Tesla ever in terms of revenue.
6673360	6677200	I think Tesla can capture a lot more revenue per user than we can.
6677200	6680400	But this mimics the Android iOS model exactly.
6680400	6682400	There may be more Android devices, but you know,
6682400	6684160	there's a lot more iPhones than Google pixels.
6684160	6687600	So I think there'll be a lot more Tesla cars sold than pieces of comma hardware.
6690240	6695600	And then as far as a Tesla owner being able to switch to open pilot,
6696560	6699120	does iOS, does iPhones run Android?
6700960	6704320	No, but you can if you really want to do it, but it doesn't really make sense.
6704320	6705200	Like it's not.
6705200	6706160	It doesn't make sense.
6706160	6706720	Who cares?
6706720	6713280	What about if a large company like automakers for GM Toyota came to George
6713280	6719600	Hotz or on the tech space, Amazon, Facebook, Google came with a large pile of cash?
6722320	6725120	Would you consider being purchased?
6727840	6729360	Do you see that as a one possible?
6730480	6731440	Not seriously, no.
6732240	6738640	I would probably see how much shit they'll entertain for me.
6739520	6742800	And if they're willing to jump through a bunch of my hoops, then maybe.
6742800	6745040	But no, not the way that M&A works today.
6745040	6746480	I mean, we've been approached.
6746480	6747840	And I laugh in these people's faces.
6747840	6748880	I'm like, are you kidding?
6750880	6753520	Yeah, because it's so demeaning.
6753520	6756880	The M&A people are so demeaning to companies.
6756880	6761200	They treat the startup world as their innovation ecosystem.
6761200	6764960	And they think that I'm cool with going along with that so I can have some of their scam fake
6764960	6766800	Fed dollars, Fed coin.
6767440	6771200	What am I going to do with more Fed coin, Fed coin, man?
6771200	6772000	I love that.
6772000	6776080	So that's the cool thing about podcasting actually is people criticize.
6776080	6781040	I don't know if you're familiar with Spotify giving Joe Rogan 100 million.
6781760	6782640	I'd talk about that.
6783680	6789440	And they respect, despite all the shit that people are talking about Spotify,
6791360	6796320	people understand that podcasters like Joe Rogan know what the hell they're doing.
6797040	6800160	So they give them money and say, just do what you do.
6801040	6808320	And the equivalent for you would be like, George, do what the hell you do because you're good at it.
6808320	6810080	Try not to murder too many people.
6811280	6817280	There's some kind of common sense things like just don't go on a weird rampage of...
6817280	6820240	Yeah, it comes down to what companies I could respect.
6822160	6824640	You know, could I respect GM? Never.
6826480	6827280	No, I couldn't.
6827280	6830160	I mean, could I respect like a Hyundai?
6830800	6831440	More so.
6832080	6832400	Right?
6832400	6833360	That's a lot closer.
6833360	6833920	Toyota?
6834560	6834960	What's your...
6835760	6838480	Nah, no, Korean is the way.
6839200	6843200	I think that the Japanese, the Germans, the US, they're all too...
6844080	6845600	They all think they're too great to be honest.
6845600	6846800	What about the tech companies?
6847440	6847840	Apple?
6848720	6850880	Apple is of the tech companies that I could respect.
6850880	6852080	Apple is the closest.
6852080	6852400	Yeah.
6852400	6853200	I mean, I could never...
6853200	6854240	It would be ironic.
6854240	6859680	It would be ironic if Kama AI is acquired by Apple.
6859680	6860320	I mean, Facebook.
6860320	6863200	Look, I quit Facebook 10 years ago because I didn't respect the business model.
6864240	6867760	Google has declined so fast in the last five years.
6868400	6872800	What are your thoughts about Waymo and its present and its future?
6872800	6876640	Let me start by saying something nice,
6877280	6884400	which is I've visited them a few times and I've written in their cars,
6885280	6891600	and the engineering that they're doing, both the research and the actual development,
6891600	6895040	and the engineering they're doing and the scale they're actually achieving
6895040	6897680	by doing it all themselves, is really impressive.
6898320	6900800	And the balance of safety and innovation.
6901440	6907120	And the cars work really well for the routes they drive.
6907120	6910560	Like, they drive fast, which was very surprising to me.
6910560	6915360	Like, it drives the speed limit, or faster than the speed limit, it goes.
6916080	6919040	And it works really damn well, and the interface is nice.
6919040	6920240	And channel Arizona, yeah.
6920240	6922320	Yeah, and channel Arizona is in a very specific environment.
6924480	6928480	It gives me enough material in my mind to push back
6928560	6932240	against the madmen of the world, like George Hutz, to be like...
6934560	6937920	Because you kind of imply there's zero probability they're going to win.
6937920	6938800	Yeah.
6938800	6944400	And after I've written in it, to me it's not zero.
6944400	6945920	Oh, it's not for technology reasons.
6946640	6948080	Bureaucracy?
6948080	6949440	No, it's worse than that.
6949440	6951040	It's actually for product reasons, I think.
6951920	6954480	Oh, you think they're just not capable of creating an amazing product?
6955440	6958960	No, I think that the product that they're building doesn't make sense.
6960960	6962080	So, a few things.
6963200	6964400	You say the Waymo's are fast.
6965760	6968400	Benchmark a Waymo against a competent Uber driver.
6968960	6969520	Right.
6969520	6969840	Right.
6969840	6970960	The Uber driver's faster.
6970960	6972080	It's not even about speed.
6972080	6973040	It's the thing you said.
6973040	6976160	It's about the experience of being stuck at a stop sign.
6976160	6978560	Because pedestrians are crossing non-stop.
6980000	6983760	I like when my Uber driver doesn't come to a full stop at the stop sign, you know?
6984320	6991440	And so, let's say the Waymo's are 20% slower than an Uber, right?
6992800	6994240	You can argue that they're going to be cheaper.
6994880	6999200	And I argue that users already have the choice to trade off money for speed.
6999200	7000080	It's called Uber Pool.
7002080	7004480	I think it's like 15% of rides at Uber pools.
7005360	7005840	Right.
7005840	7008640	Users are not willing to trade off money for speed.
7009280	7013840	So, the whole product that they're building is not going to be competitive
7014640	7016640	with traditional ride-sharing networks.
7016640	7017120	Right.
7019200	7027280	Like, and also, whether there's profit to be made depends entirely on one company having a monopoly.
7027280	7033360	I think that the level for autonomous ride-sharing vehicles market is going to look a lot like
7033360	7037600	the scooter market if even the technology does come to exist, which I question.
7038640	7040080	Who's doing well in that market?
7040080	7040320	Yeah.
7040320	7042240	It's a race to the bottom, you know.
7042240	7047440	Well, it could be closer like an Uber and a Lyft where it's just a one or two players.
7048000	7054800	Well, the scooter people have given up trying to market scooters as a practical means of
7054800	7057680	transportation and they're just like, they're super fun to ride.
7057680	7058320	Look at wheels.
7058320	7060080	I love those things and they're great on that front.
7060640	7060960	Yeah.
7060960	7066160	But from an actual transportation product perspective, I do not think scooters are viable
7066160	7068240	and I do not think level four autonomous cars are viable.
7069120	7071520	If you, let's play a fun experiment.
7071520	7076240	If you ran, let's do Tesla and let's do Waymo.
7076240	7076880	All right.
7076880	7082240	If Elon Musk took a vacation for a year, he just said, screw it.
7082240	7085280	I'm going to go live on an island, no electronics.
7085280	7088320	And the board decides that we need to find somebody to run the company
7089040	7092160	and they decide that you should run the company for a year.
7092160	7093840	How do you run Tesla differently?
7094720	7095600	I wouldn't change much.
7096400	7097840	Do you think they're on the right track?
7097840	7098640	I wouldn't change.
7098640	7101600	I mean, I'd have some minor changes.
7101600	7109040	But even my debate with Tesla about end-to-end versus segnets, that's just software.
7109040	7109520	Who cares?
7111920	7114320	It's not like you're doing something terrible with segnets.
7114320	7117200	You're probably building something that's at least going to help you debug the end-to-end
7117200	7117840	system a lot.
7119360	7124240	It's very easy to transition from what they have to an end-to-end kind of thing.
7125200	7125840	Right.
7125840	7131520	And then I presume you would, in the Model Y or maybe in the Model 3,
7131520	7133520	start adding driver sensing with infrared.
7133520	7139520	Yes, I would add infrared lights right away to those cars.
7142160	7144960	And start collecting that data and do all that kind of stuff.
7144960	7145520	Yeah.
7145520	7146080	Very much.
7146080	7147360	I think they're already kind of doing it.
7147360	7149280	It's an incredibly minor change.
7149280	7152320	If I actually were CEO of Tesla first off, I'd be horrified that I wouldn't be able
7152320	7154000	to do a better job as Elon.
7154000	7157600	And then I would try to understand the way he's done things before.
7157600	7159120	You would also have to take over as Twitter.
7160160	7161360	God, I don't tweet.
7162080	7163520	Yeah, what's your Twitter situation?
7163520	7165440	Why are you so quiet on Twitter?
7165440	7165920	Oh, I mean.
7165920	7170240	I says Dukama is like, what's your social network presence like?
7170240	7174160	Because on Instagram, you do live streams.
7176720	7181520	You understand the music of the internet, but you don't always fully engage into it.
7182000	7182640	Part-time.
7182640	7183600	Why do you still have a Twitter?
7184160	7187600	Yeah, I mean, Instagram is a pretty place.
7187600	7188960	Instagram is a beautiful place.
7188960	7189840	It glorifies beauty.
7189840	7192560	I like Instagram's values as a network.
7193360	7201280	Twitter glorifies conflict, glorifies like taking shots of people.
7201280	7208320	And it's like, Twitter and Donald Trump are perfect for each other.
7208400	7211600	So Tesla is on the right track in your view.
7212640	7216480	Okay, so let's really try this experiment.
7216480	7220400	If you ran Waymo, let's say they're, I don't know if you agree,
7220400	7223280	but they seem to be at the head of the pack of the kind of...
7225520	7226800	What would you call that approach?
7227440	7230080	It's not necessarily lighter-based because it's not about lighter.
7230080	7231680	Level four robotaxi.
7231680	7235520	Level four robotaxi all in before making any revenue.
7236400	7238560	So they're probably at the head of the pack.
7238560	7244480	If you said, hey, George, can you please run this company for a year?
7244480	7245920	How would you change it?
7247120	7249760	I would go, I would get Anthony Lewandowski out of jail,
7249760	7251200	and I would put him in charge of the company.
7256720	7258080	Let's try to break that apart.
7259760	7261520	You want to destroy the company by doing that?
7261840	7268560	Or do you mean you like renegade style thinking that pushes,
7269360	7272480	that throws away bureaucracy and goes to first principle thinking?
7272480	7274000	What do you mean by that?
7274000	7276000	I think Anthony Lewandowski is a genius,
7276000	7279200	and I think he would come up with a much better idea
7279200	7280720	of what to do with Waymo than me.
7282240	7283760	So you mean that unironically.
7283760	7284720	He is a genius.
7284720	7286000	Oh, yes. Oh, absolutely.
7286560	7287600	Without a doubt.
7287600	7290320	I mean, I'm not saying there's no shortcomings,
7290960	7293760	but in the interactions I've had with him, yeah.
7295680	7297440	He's also willing to take, like,
7297440	7299280	who knows what he would do with Waymo?
7299280	7301680	I mean, he's also out there, like far more out there than I am.
7301680	7303360	Yeah, his big risks.
7303360	7304240	What do you make of him?
7304240	7306880	I was going to talk to him in his pockets,
7306880	7308080	and I was going back and forth.
7308880	7313120	I'm such a gullible naive human, like I see the best in people.
7313840	7318080	And I slowly started to realize that there might be some people out there
7319040	7324320	that, like, have multiple faces to the world.
7325120	7328000	They're, like, deceiving and dishonest.
7328000	7332960	I still refuse to, like, I just, I trust people,
7332960	7334480	and I don't care if I get hurt by it,
7334480	7337680	but, like, you know, sometimes you have to be a little bit careful,
7337680	7340400	especially platform-wise and podcast-wise.
7341440	7342480	What am I supposed to think?
7343040	7345040	So you think, you think he's a good person?
7346000	7347600	Oh, I don't know.
7347600	7349040	I don't really make moral judgments.
7349680	7350640	And it's difficult to...
7350640	7352400	Oh, I mean this about the Waymo.
7352400	7354800	Actually, I mean that whole idea, very non-ironically,
7354800	7356000	about what I would do.
7356000	7357920	The problem with putting me in charge of Waymo
7357920	7361520	is Waymo is already $10 billion in the haul, right?
7361520	7363600	Whatever idea Waymo does, look,
7363600	7365840	comma's profitable, comma's raised $8.1 million.
7366480	7368000	That's small, you know, that's small money.
7368000	7370720	Like, I can build a reasonable consumer electronics company
7370720	7372640	and succeed wildly at that
7372720	7375120	and still never be able to pay back Waymo's $10 billion.
7375680	7378400	So I think the basic idea with Waymo,
7378400	7380800	well, forget the $10 billion because they have some backing,
7380800	7383200	but your basic thing is, like,
7383200	7385600	what can we do to start making some money?
7385600	7387840	Well, no, I mean, my bigger idea is, like,
7387840	7390160	whatever the idea is that's going to save Waymo,
7390160	7391280	I don't have it.
7391280	7393360	It's going to have to be a big risk idea,
7393360	7395120	and I cannot think of a better person
7395120	7396480	than Anthony Lewandowski to do it.
7397680	7400080	So that is completely what I would do with CEO of Waymo.
7400080	7402560	I've called myself a transitionary CEO.
7402560	7404640	Do everything I can to fix that situation up.
7404640	7405600	Transitionary CEO, yeah.
7407920	7408320	Yeah.
7408320	7409600	Because I can't do it, right?
7409600	7412080	Like, I can't, I can't, I mean,
7412080	7414240	I can talk about how what I really want to do
7414240	7417280	is just apologize for all those corny, you know,
7417280	7418240	ad campaigns and be like,
7418240	7419760	here's the real estate of the technology.
7419760	7422160	Yeah, that's, like, I have several criticism.
7422160	7426000	I'm a little bit more bullish on Waymo than you seem to be,
7426000	7430720	but one criticism I have is it went into corny mode too early.
7430720	7432080	Like, it's still a startup.
7432080	7433600	It hasn't delivered on anything.
7433600	7435680	So it should be, like, more renegade
7436320	7439200	and show off the engineering that they're doing,
7439200	7440480	which just can be impressive,
7440480	7442640	as opposed to doing these weird commercials of, like,
7443600	7446880	your friendly car company.
7446880	7447840	I mean, that's my biggest,
7447840	7449920	my biggest snipe at Waymo was always,
7449920	7451280	that guy's a paid actor.
7451280	7452720	That guy's not a Waymo user.
7452720	7453520	He's a paid actor.
7453520	7455280	Look here, I found his call sheet.
7455280	7458720	Do kind of like what SpaceX is doing with the rocket launch
7458720	7460880	is just get, put the nerds up front,
7460880	7462080	put the engineers up front,
7462080	7465600	and just, like, show failures too, just...
7465600	7467760	I love SpaceX's, yeah.
7467760	7469760	Yeah, the thing that they're doing is right,
7469760	7471600	and it just feels like the right...
7471600	7474320	But we're all so excited to see them succeed.
7474320	7474640	Yeah.
7474640	7476640	I can't wait to see Waymo fail, you know?
7477200	7478080	Like, you lie to me.
7478080	7479280	I want you to fail.
7479280	7480240	You tell me the truth.
7480240	7481040	You be honest with me.
7481040	7482000	I want you to succeed.
7482000	7482320	Yeah.
7484880	7488720	Yeah, and that requires the renegade CEO, right?
7489680	7490880	I'm with you.
7490880	7491360	I'm with you.
7491360	7492960	I still have a little bit of faith in Waymo
7493920	7496160	for the renegade CEO to step forward, but...
7497760	7499440	It's not, it's not John Kraft.
7500560	7501280	Yeah.
7501280	7502800	It's, you can't...
7502800	7503760	It's not Chris Omston.
7504800	7507600	And those people may be very good at certain things.
7507600	7508160	Yeah.
7508160	7509280	But they're not renegades.
7510240	7511920	Yeah, because these companies are fundamentally,
7511920	7514240	even though we're talking about billion dollars,
7514240	7515600	all these crazy numbers,
7515600	7518400	they're still, like, early-stage startups.
7519120	7521760	I mean, I just, if you are pre-revenue
7521760	7524160	and you've raised $10 billion, I have no idea.
7524160	7525760	Like, this just doesn't work.
7526320	7527840	You know, it's against everything Silicon Valley.
7527840	7529120	Where's your minimum viable product?
7529680	7531360	You know, where's your users?
7531360	7532320	Where's your growth numbers?
7533200	7535520	This is traditional Silicon Valley.
7536160	7537920	Why do you not apply it to what you think
7537920	7539280	you're too big to fail already?
7539280	7539520	Like...
7541600	7545680	How do you think autonomous driving will change society?
7545680	7550800	So the mission is, for comma, to solve self-driving.
7552560	7555520	Do you have, like, a vision of the world of how it'll be different?
7557840	7560000	Is it as simple as A to B transportation?
7560000	7561040	Or is there, like...
7561040	7562160	Because these are robots.
7563520	7565760	It's not about autonomous driving in and of itself.
7565760	7566960	It's what the technology enables.
7569600	7572080	It's, I think it's the coolest applied AI problem.
7572080	7574560	I like it because it has a clear path
7575040	7575840	to monetary value.
7577600	7580560	But as far as that being the thing that changes the world...
7581280	7583520	I mean, no.
7583520	7585360	Like, there's cute things we're doing in comma.
7585360	7586800	Like, who'd have thought you could stick a phone
7586800	7588080	on the windshield and it'll drive.
7588880	7591040	But, like, really the product that you're building
7591040	7593760	is not something that people were not capable
7593760	7595040	of imagining 50 years ago.
7595600	7597040	So, no, it doesn't change the world on that front.
7597600	7599360	Could people have imagined the Internet 50 years ago?
7599360	7601760	Only true genius visionaries.
7602320	7604880	Everyone could have imagined autonomous cars 50 years ago.
7604880	7606960	It's like a car, but I don't drive it.
7606960	7609360	See, I have the sense, and I told you, like,
7609360	7617200	I'm my long-term dream is robots with whom you have deep connections.
7619200	7622240	And there's different trajectories towards that.
7623440	7626720	And I've been thinking of launching a startup.
7628240	7631280	I see autonomous vehicles as a potential trajectory to that.
7632240	7636480	That's not where the direction I would like to go.
7636480	7639840	But I also see Tesla or even Kamii like pivoting
7639840	7645840	into robotics broadly defined at some stage.
7645840	7648240	In the way, like you're mentioning, the Internet didn't expect.
7649520	7652400	Let's solve, you know, when I say a comma about this.
7652400	7654560	We could talk about this, but let's solve self-driving cars first.
7655520	7656800	Gotta stay focused on the mission.
7657920	7659120	You're not too big to fail.
7659120	7662720	For however much I think comm is winning, like, no, no, no, no.
7662720	7664880	You're winning when you solve level five self-driving cars.
7664880	7666720	And until then, you haven't won and won.
7666720	7669920	And, you know, again, you want to be arrogant in the face of other people?
7669920	7671760	Great. You want to be arrogant in the face of nature?
7671760	7672400	You're an idiot.
7672400	7675360	Right. Stay mission focused, brilliantly put.
7676320	7678480	Like I mentioned, thinking of launching a startup,
7678480	7681120	I've been considering, actually, before COVID,
7681120	7683200	I've been thinking of moving to San Francisco.
7683200	7684880	Oh, I wouldn't go there.
7685840	7690240	So why is, well, and now I'm thinking about potentially Austin.
7692000	7693440	And we're in San Diego now.
7693440	7694800	San Diego, come here.
7694800	7700400	So why, what, I mean, you're such an interesting human.
7700400	7702160	You've launched so many successful things.
7703040	7706080	What, why San Diego?
7706080	7706880	What do you recommend?
7706880	7708160	Why not San Francisco?
7709120	7711760	Have you thought, so in your case,
7711760	7716080	San Diego with Qualcomm and Staff Dragon, I mean, that's an amazing combination.
7716880	7717920	But that wasn't really why.
7718480	7719440	That wasn't the why?
7719440	7721200	No. I mean, Qualcomm was an afterthought.
7721200	7722720	Qualcomm was, it was a nice thing to think about.
7722720	7725600	It's like you can have a tech company here and a good one.
7725600	7728320	I mean, you know, I like Qualcomm, but no.
7728320	7730400	Well, so why is San Diego better than San Francisco?
7730400	7731760	Why does San Francisco suck?
7731760	7732240	Well, so, okay.
7732240	7735200	So first off, we all kind of said, like, we want to stay in California,
7735200	7739840	people like the ocean, you know, California for its flaws.
7739920	7743760	It's like a lot of the flaws of California are not necessarily California as a whole,
7743760	7745440	and they're much more San Francisco specific.
7746560	7751200	San Francisco, so I think first-tier cities in general have stopped wanting growth.
7753200	7757040	Well, you have like in San Francisco, you know, the voting class always
7757040	7760960	votes to not build more houses because they own all the houses and they're like, well,
7760960	7765040	you know, once people have figured out how to vote themselves more money, they're going to do it.
7765040	7766800	It is so insanely corrupt.
7767760	7771440	It is not balanced at all, like political party-wise.
7771440	7773680	You know, it's a one-party city.
7773680	7782000	And for all the discussion of diversity, it stops lacking real diversity of thought,
7782000	7787280	of background, of approaches, of strategies, of ideas.
7788880	7793760	It's kind of a strange place that it's the loudest people about diversity
7794320	7796400	and the biggest lack of diversity.
7796400	7798560	I mean, that's what they say, right?
7798560	7799520	It's the projection.
7800240	7801040	Projection, yeah.
7802000	7802800	Yeah, it's interesting.
7802800	7807200	And even people in Silicon Valley telling me that's like high up people,
7807920	7809920	everybody is like, this is a terrible place.
7809920	7810480	It doesn't make sense.
7810480	7813120	I mean, and coronavirus is really what killed it.
7813120	7818000	San Francisco was the number one Exodus during coronavirus.
7818800	7821440	We still think San Diego is a good place to be.
7823360	7824560	Yeah, I mean, we'll see.
7824640	7829040	We'll see what happens with California a bit longer term.
7829760	7831920	Like, Austin's an interesting choice.
7831920	7834480	I wouldn't, I wouldn't, I don't have really anything bad to say about Austin
7835280	7838400	either except for the extreme heat in the summer, which, you know,
7838400	7840080	but that's like very on the surface, right?
7840080	7843600	I think as far as like an ecosystem goes, it's cool.
7843600	7845120	I personally love Colorado.
7845120	7846080	Colorado is great.
7846960	7851040	Yeah, I mean, you have these states that are, you know, like just way better run.
7851920	7855200	Um, California is, you know, it's especially San Francisco.
7855200	7858320	It's on its high horse and like, yeah.
7858320	7862880	Can I ask you for advice to me and to others about
7864560	7866720	what's the take to build a successful startup?
7867360	7867840	Oh, I don't know.
7867840	7868480	I haven't done that.
7869040	7870000	Talk to someone who did that.
7870640	7878480	Well, you know, this is like another book of yours that I'll buy for $67, I suppose.
7879200	7880400	Uh, so there's, um,
7882640	7884000	one of these days I'll sell out.
7884000	7884640	Yeah, that's right.
7884640	7887200	Jail breaks are going to be a dollar and books are going to be 67.
7887840	7891920	How I, uh, how I, you know, broke the iPhone by George Hots.
7891920	7892720	That's right.
7892720	7895680	How I jail broke the iPhone and you can do it.
7895680	7898800	You can't do it in 21 days.
7898800	7899360	That's right.
7899360	7900240	That's right.
7900240	7901040	Oh God.
7901040	7901440	Okay.
7901440	7905680	I can't wait, but quite, so you have an introspective, you have built
7906640	7909360	a very unique company.
7909360	7911840	I mean, not, not you, but you and others.
7913040	7916880	But I don't know, um, there's no, there's nothing.
7916880	7920480	You have an interest, but you haven't really sat down and thought about like,
7921600	7925200	well, like if you and I were having a bunch of, we're having some beers
7926080	7929120	and you're seeing that I'm depressed and whatever I'm struggling.
7929760	7930960	There's no advice you can give.
7931520	7933600	Oh, I mean, more beer.
7934400	7934960	More beer.
7936240	7938720	Uh, uh, yeah.
7938720	7942160	I think it's all very like situation dependent.
7942720	7944160	Um, here's, okay.
7944160	7947120	If I can give a generic piece of advice, it's the technology always wins.
7948080	7953040	The better technology always wins and lying always loses.
7955120	7956960	Build technology and don't lie.
7958480	7959120	I'm with you.
7959120	7960400	I agree very much.
7960400	7960880	Long run.
7960880	7961360	Long run.
7961360	7961440	Sure.
7961440	7962240	That's the long run.
7962240	7962880	And you know what?
7962880	7965840	The market can remain irrational longer than you can remain solvent.
7966400	7967120	True fact.
7967120	7972800	Well, this is, this is an interesting point because I ethically and just as a human believe that, um,
7974320	7981840	like, like hype and smoke and mirrors is not at any stage of the company is a good strategy.
7982560	7988080	I mean, there's some like, you know, PR magic kind of like, you know, you want a new product,
7988080	7988400	right?
7988400	7992560	If there's a call to action, if there's like a call to action like buy my new GPU,
7992560	7992960	look at it.
7992960	7994880	It takes up three slots and it's this big.
7994880	7996160	It's huge buy my GPU.
7996160	7996960	Yeah, that's great.
7996960	8002000	If you look at, you know, especially in that, in AI space broadly, but autonomous vehicles,
8002640	8005680	like you can raise a huge amount of money on nothing.
8006480	8008880	And the question to me is like, I'm against that.
8009920	8011440	I'll never be part of that.
8011440	8015520	I don't think I hope not willingly not.
8015680	8024880	But like, is there something to be said to, uh, essentially lying to raise money,
8024880	8026640	like fake it till you make it kind of thing?
8027760	8030080	I mean, this is Billy McFarlane the fire festival.
8030080	8034240	Like we all, we all experienced, uh, you know, what happens with that?
8034240	8037440	No, no, don't fake it till you make it.
8037440	8040480	Be honest and hope you make it the whole way.
8040480	8041840	The technology wins.
8041840	8042160	Right.
8042160	8042960	The technology wins.
8042960	8047600	And like there is, I'm not used to like the anti-hype, you know, that's, that's
8047600	8053120	Slava KPSS reference, but, um, hype isn't necessarily bad.
8053120	8055440	I loved camping out for the iPhones.
8056960	8061760	You know, and as long as the hype is backed by like substance, as long as it's backed by
8061760	8067840	something I can actually buy and like it's real, then hype is great and it's a great feeling.
8068560	8071920	It's when the hype is backed by lies that it's a bad feeling.
8071920	8074400	I mean, a lot of people call Elon Musk a fraud.
8074400	8075520	How could he be a fraud?
8075520	8081280	I've noticed this, this kind of interesting effect, which is he does tend to over promise
8082320	8085680	and deliver, what's, what's the better way to phrase it?
8085680	8090800	Promise a timeline that he doesn't deliver on, he delivers much later on.
8091600	8092640	What do you think about that?
8092640	8093520	Because I do that.
8093520	8095280	I think that's a programmer thing too.
8096080	8097600	I do that as well.
8097600	8101040	You think that's a really bad thing to do or is that okay?
8101040	8106400	Oh, I think that's again, as long as like you're working toward it and you're going to deliver
8106400	8109520	on it and it's not too far off, right?
8109520	8110160	Yeah.
8110160	8110720	Right?
8110720	8115200	Like, like, you know, the whole, the whole autonomous vehicle thing, it's like,
8116000	8119680	I mean, I still think Tesla's on track to beat us.
8119680	8124000	I still think even with their, even with their missteps, they have advantages we don't have.
8124880	8132160	You know, Elon is better than me at like marshalling massive amounts of resources.
8133040	8138160	So, you know, I still think given the fact they're maybe making some wrong decisions,
8138160	8139360	they'll end up winning.
8139360	8145120	And like, it's fine to hype it if you're actually going to win, right?
8145120	8149360	If Elon says, look, we're going to be landing rockets back on Earth in a year and it takes four,
8149360	8155360	like, you know, he landed a rocket back on Earth and he was working toward it the whole time.
8155360	8159440	I think there's some amount of like, I think what it becomes wrong is if you know you're not
8159440	8160400	going to meet that deadline.
8160400	8161520	If you're lying.
8161520	8162080	Yeah.
8162080	8163200	That's brilliantly put.
8163200	8166800	Like, this is what people don't understand, I think.
8166800	8169120	Like, Elon believes everything he says.
8169120	8170320	He does.
8170320	8172080	As far as I can tell, he does.
8172080	8174720	And I detected that in myself too.
8174720	8181280	Like, if I, it's only bullshit if you're like conscious of yourself lying.
8181280	8182480	Yeah, I think so.
8182480	8183280	Yeah.
8183280	8185600	No, you can't take that to such an extreme, right?
8185600	8188960	Like, in a way, I think maybe Billy McFarland believed everything he said too.
8189920	8190320	Right.
8190320	8193680	That's how you start a cult and then everybody kills themselves.
8193680	8194240	Yeah.
8194240	8194640	Yeah.
8194640	8199200	Like, it's you need, you need, if there's like some factor on it, it's fine.
8199200	8202800	And you need some people to like, you know, keep you in check.
8202800	8208720	But like, if you deliver on most of the things you say and just the timelines are off, man.
8208720	8210240	It does piss people off though.
8210240	8216800	I wonder, but who cares in a long arc of history that people, everybody gets pissed off at the
8216800	8223280	people who succeed, which is one of the things that frustrates me about this world is they
8223280	8226480	don't celebrate the success of others.
8227600	8231760	Like, there's so many people that want Elon to fail.
8232880	8234800	It's so fascinating to me.
8234800	8236960	Like, what is wrong with you?
8238080	8243440	Like, so Elon Musk talks about like people at short, like they talk about financial,
8243440	8245280	but I think it's much bigger than the financials.
8245280	8250480	I've seen like the human factors community, they want, they want other people to fail.
8251520	8251840	What?
8251840	8252240	What?
8252240	8252800	What?
8252800	8258480	Like, even people, the harshest thing is like, you know, even people that like seem to really
8258480	8261280	hate Donald Trump, they want him to fail.
8261280	8261840	Yeah, I know.
8261840	8265040	Or like the other president, or they want Barack Obama to fail.
8265760	8266080	It's like.
8266960	8268400	We're all on the same boat, man.
8269600	8274400	It's weird, but I want that, I would love to inspire that part of the world to change because,
8275520	8280400	well, damn it, if the human species is going to survive, we can celebrate success.
8280400	8285040	Like, it seems like the efficient thing to do in this objective function that like we're all
8285040	8290400	striving for is to celebrate the ones that like figure out how to like do better at that
8290480	8295840	objective function as opposed to like dragging them down back into them, into the mud.
8296560	8300720	I think there is, this is the speech I always give about the commenters on hacker news.
8301600	8304720	So first off, something to remember about the internet in general,
8304720	8308560	is commenters are not representative of the population.
8309200	8310560	I don't comment on anything.
8311440	8316080	You know, commenters are representative of a certain sliver of the population.
8316640	8320960	And on hacker news, a common thing I'll say is when you'll see something that's like,
8322000	8326640	you know, promises to be wild out there and innovative.
8327440	8332240	There is some amount of, you know, checking them back to earth, but there's also some amount of,
8332240	8340000	if this thing succeeds, well, I'm 36 and I've worked at large tech companies my whole life.
8340000	8346880	They can't succeed because if they succeed, that would mean that I could have done something
8346880	8350160	different with my life, but we know that I could have, we know that I couldn't have,
8350160	8351840	and that's why they're going to fail.
8351840	8355040	And they have to root for them to fail to kind of maintain their world image.
8357440	8357840	Tune it out.
8357840	8359600	And they comment, well, it's hard.
8360480	8367520	So one of the things, one of the things I'm considering startup wise is to change that
8367520	8371680	because I think the, I think it's also a technology problem.
8371680	8372960	It's a platform problem.
8372960	8373520	I agree.
8373520	8377040	It's like, because the thing you said most people don't comment,
8379600	8381440	I think most people want to comment.
8382880	8385840	They just don't because it's all the assholes for commenting.
8385840	8386240	Exactly.
8386240	8387920	I don't want to be grouped in with them on that.
8387920	8390720	You don't want to be at a party where everyone's an asshole.
8390720	8394400	And so they, but that's a platform problem that's.
8394400	8396000	I can't believe what Reddit's become.
8396000	8398960	I can't believe the group think in Reddit comments.
8400480	8404240	There's a, Reddit is interesting one because they're subreddits.
8405040	8410000	And so you can still see, especially small subreddits that like,
8410000	8417040	that are little like havens of like joy and positivity and like deep, even disagreement,
8417040	8420800	but like nuanced discussion, but it's only like small little pockets.
8421600	8423360	But that's, that's emergent.
8423360	8426080	The platform is not helping that or herring that.
8426960	8430240	So I guess naturally something about the internet,
8431040	8436560	if you don't put in a lot of effort to encourage nuance and positive, good vibes,
8437120	8441120	it's naturally going to decline into chaos.
8441120	8442800	I would love to see someone do this well.
8442800	8443040	Yeah.
8443680	8445440	I think it's, yeah, very doable.
8446000	8451280	I think actually, so I feel like Twitter could be overthrown.
8452080	8457360	Yasho Bach talked about how like, if you have like and retweet,
8457920	8462000	like that's only positive wiring, right?
8462000	8467520	The only way to do anything like negative there is with a comment.
8468240	8472880	And that's like that asymmetry is what gives, you know,
8472880	8474880	Twitter its particular toxicness.
8475440	8479040	Whereas I find YouTube comments to be much better because YouTube comments have a,
8479040	8482560	have a, have an up and a down and they don't show the downloads.
8483680	8486720	Without getting into depth of this particular discussion,
8486720	8490800	the point is to explore possibilities and get a lot of data on it.
8490800	8494000	Because I mean, I could disagree with what you just said.
8494000	8495920	It's, it's on the point is it's unclear.
8495920	8498800	It's a, it hasn't been explored in a really rich way.
8499760	8505760	Like the, these questions of how to create platforms that encourage positivity.
8506960	8507280	Yeah.
8507280	8509440	I think it's a, it's a technology problem.
8509440	8511840	And I think we'll look back at Twitter as it is now.
8511840	8516640	Maybe it'll happen within Twitter, but most likely somebody overthrows them is,
8517680	8523040	we'll look back at Twitter and say, we can't believe we put up with this level of toxicity.
8523040	8524320	You need a different business model too.
8524960	8528560	Any, any social network that fundamentally has advertising as a business model,
8528560	8531600	this was in the social dilemma, which I didn't watch, but I liked it.
8531600	8534160	It's like, you know, there's always the, you know, you're the product, you're not the,
8535280	8537840	but they had a nuanced take on it that I really liked.
8537840	8542960	And it said, the product being sold is influence over you.
8544480	8548480	The product being sold is literally your, you know, influence on you.
8550640	8553600	That can't be, if that's your idea, okay.
8553600	8555440	Well, you know, guess what?
8555440	8556960	It can't not be toxic.
8556960	8557520	Yeah.
8557520	8562400	Maybe there's ways to spin it, like with, with giving a lot more control to the user
8562400	8567200	and transparency to see what is happening to them as opposed to in the shadows as possible.
8567200	8569200	But that can't be the primary source of,
8569200	8570720	But the users aren't, no one's going to use that.
8571840	8572480	It depends.
8572480	8573040	It depends.
8573040	8573840	It depends.
8573840	8580160	I think, I think that the, your, your not going to, you can't depend on self-awareness of the users.
8580160	8584560	It's a, it's another, it's a longer discussion because you can't depend on it, but
8585040	8588240	you can reward self-awareness.
8588240	8592480	Like if, for the ones who are willing to put in the work of self-awareness,
8592480	8597680	you can reward them and incentivize and perhaps be pleasantly surprised how many people
8598960	8601840	are willing to be self-aware on the internet.
8601840	8603200	Like we are in real life.
8603200	8607120	Like I'm putting in a lot of effort with you right now being self-aware about,
8607120	8611440	if I say something stupid or mean, I'll like look at your, like body language.
8611440	8614240	Like I'm putting in that effort, it's costly.
8614240	8616240	For an introvert, it's a great costly thing.
8616240	8618480	But on the internet, fuck it.
8619440	8623360	Like most people are like, I don't care if this hurts somebody.
8623360	8628560	I don't care if this is not interesting or if this is, yeah, the mean or whatever.
8628560	8632320	I think so much of the engagement today on the internet is so disingenuine too.
8632880	8635360	You're not doing this out of a genuine, this is what you think.
8635360	8637520	You're doing this just straight up to manipulate others.
8637520	8639200	Whether you're in, you just became an ad.
8639760	8643600	Yeah. Okay, let's talk about a fun topic, which is programming.
8644320	8645680	Here's another book idea for you.
8645680	8646320	Let me pitch.
8647200	8649600	What's your perfect programming setup?
8649600	8652640	So like, this by George Hott's.
8652640	8656960	So like what, listen, you're-
8656960	8661040	Give me a MacBook Air, sit me in a corner of a hotel room and you know I'll still have food.
8661040	8661920	So you really don't care.
8661920	8665920	You don't fetishize like multiple monitors, keyboard.
8666880	8670160	Those things are nice and I'm not going to say no to them,
8670160	8673120	but do they automatically unlock tons of productivity?
8673120	8673920	No, not at all.
8673920	8677360	I have definitely been more productive on a MacBook Air in a corner of a hotel room.
8678160	8680640	What about IDE?
8681520	8685120	So which operating system do you love?
8685920	8688880	What text editor do you use, IDE?
8688880	8693760	What, is there something that is like the perfect, if you could just say
8694560	8697600	the perfect productivity setup for George Hott's?
8697600	8698160	Doesn't matter.
8698160	8698960	Doesn't matter.
8698960	8700240	Literally doesn't matter.
8700240	8702960	You know, I guess I code most of the time in VIM.
8702960	8707120	Like literally I'm using an editor from the 70s, you know, you didn't make anything better.
8707120	8708880	You know, okay, VS Code is nice for reading code.
8708880	8710160	There's a few things that are nice about it.
8710960	8713360	I think that there, you can build much better tools.
8713360	8716960	How like, Ida's X refs work way better than VS codes, why?
8718480	8720080	Yeah, actually, that's a good question.
8720080	8720640	Like why?
8720640	8727760	I still use, sorry, Emacs for most, I've actually never, I have to confess something dark.
8728720	8730720	So I've never used VIM.
8732480	8739040	I think maybe I'm just afraid that my life has been like a waste.
8740560	8743360	I'm so, I'm not, I'm not even gelical about Emacs.
8744480	8747040	This is how I feel about TensorFlow versus PyTorch.
8747040	8747680	Yeah.
8747680	8750000	Having just like, we've switched everything to PyTorch.
8750000	8751760	Now put months into the switch.
8751760	8754400	I have felt like I've wasted years on TensorFlow.
8754400	8755360	I can't believe it.
8756160	8758240	I can't believe how much better PyTorch is.
8758240	8758560	Yeah.
8759440	8760720	I've used Emacs in VIM.
8760720	8761360	Doesn't matter.
8761360	8763440	Yeah, still just my heart somehow.
8763440	8764560	I fell in love with Lisp.
8764560	8765200	I don't know why.
8765200	8767920	You can't, the heart wants what the heart wants.
8767920	8770320	I don't, I don't understand it, but it just connected with me.
8770320	8773120	Maybe it's the functional language at first I connected with.
8773120	8777360	Maybe it's because so many of the AI courses before the deep learning revolution were
8777360	8778640	taught with Lisp in mind.
8779280	8779840	I don't know.
8779840	8782240	I don't know what it is, but I'm stuck with it.
8782240	8786240	But at the same time, like why am I not using a modern ID for some of these programming?
8786240	8787120	I don't know.
8787120	8788320	They're not that much better.
8788320	8789440	I've used modern IDs there.
8790000	8793440	But at the same time, so like to just, we're not to disagree with you, but
8793440	8794960	like I like multiple monitors.
8795680	8801360	Like I have to do work on a laptop and it's a pain in the ass.
8801360	8804640	And also I'm addicted to the Kinesis weird keyboard.
8805440	8806480	You could see there.
8807360	8809920	Yeah, so you don't have any of that.
8809920	8811600	You can just be on a MacBook.
8811600	8812800	I mean, look at work.
8812800	8815040	I have three 24 inch monitors.
8815040	8816400	I have a happy hacking keyboard.
8816400	8818560	I have a razor death header mouse.
8819680	8821120	But it's not essential for you.
8821120	8821840	No.
8821840	8824560	Let's go to a day in the life of George Hots.
8824560	8828560	What is the perfect day productivity wise?
8828560	8836160	So we're not talking about like Hunter S. Thompson drugs and let's look at productivity.
8836800	8839680	What's the day look like on like hour by hour?
8839680	8845200	Is there any regularities that create a magical George Hots experience?
8845760	8853040	I can remember three days in my life and I remember these days vividly when I've gone
8853040	8857760	through kind of radical transformations to the way I think.
8857760	8860000	And what I would give, I would pay $100,000.
8860000	8864800	If I could have one of these days tomorrow, the days have been so impactful.
8864800	8870640	And one was first discovering LEIs of Yudkowsky on the Singularity and reading that stuff.
8870640	8872560	And like my mind was blown.
8874160	8879200	And the next was discovering the Hunter price and that AI is just compression.
8879840	8883680	Like finally understanding AIXI and what all of that was.
8883680	8886320	You know, I like read about it when I was 18, 19, I didn't understand it.
8886320	8889920	And then the fact that like lossless compression implies intelligence.
8889920	8891440	The day that I was shown that.
8892320	8894000	And then the third one is controversial.
8894000	8899120	The day I found a blog called Unqualified Reservations and read that.
8899120	8901360	And I was like, wait, which one is that?
8901360	8902880	That's what's the guy's name?
8902880	8904080	Curtis Garvin.
8904080	8904320	Yeah.
8905120	8907600	So many people tell me I'm supposed to talk to him.
8907600	8912480	Yeah, the day he sounds insane or brilliant, but insane or both.
8912480	8913040	I don't know.
8913040	8916480	The day I found that blog was another like this was during like like
8916480	8918960	Gamergate and kind of the run up to the 2016 election.
8918960	8922800	And I'm like, wow, okay, the world makes sense now.
8922800	8925520	This is like, I had a framework now to interpret this,
8925520	8929360	just like I got the framework for AI and a framework to interpret technological progress.
8929360	8932720	Like those days when I discovered these new frameworks or.
8932720	8933440	Oh, interesting.
8933440	8937040	So it's not about, but what was special about those days?
8937040	8938720	How did those days come to be?
8938720	8939920	Is it just you got lucky?
8939920	8940480	Like, sure.
8941360	8946960	I like, you just encountered a hunter prize on on hacking news or something like that.
8947680	8948800	Um, like what?
8949680	8953200	But you see, I don't think it's just, see, I don't think it's just that like,
8953200	8954720	I could have gotten lucky at any point.
8954720	8956160	I think that in a way.
8956160	8957760	You were ready at that moment.
8957760	8958480	Yeah, exactly.
8958480	8959600	To receive the information.
8961440	8964720	But is there some magic to the day today of like,
8965520	8967040	like eating breakfast?
8967040	8968400	And it's the mundane things.
8969040	8969600	Nah.
8969600	8970160	Nothing.
8970160	8972160	No, I drift through, I drift through life.
8972800	8974080	Without structure.
8974080	8978480	I drift through life hoping and praying that I will get another day like those days.
8978480	8982080	And there's nothing in particular you do to, uh, to be a receptacle
8982960	8984480	for another for day number four.
8986000	8988080	No, I didn't do anything to get the other ones.
8988080	8991040	So I don't think I have to really do anything now.
8991040	8993520	I took a month long trip to New York and
8994560	8997920	I mean, the Ethereum thing was the highlight of it, but the rest of it was pretty terrible.
8997920	9001680	I did a two week road trip and I got, I had to turn around.
9001680	9006560	I had to turn around, I'm driving in, uh, in Gunnison, Colorado.
9006560	9010480	I passed through Gunnison and, uh, the snow starts coming down.
9010480	9012960	There's a pass up there called Monarch Pass in order to get through to Denver.
9012960	9013920	You got to get over the Rockies.
9014640	9016160	And I had to turn my car around.
9016720	9020080	I couldn't, I watched, uh, I watched a F-150 go off the road.
9020080	9021600	I'm like, I got to go back.
9021600	9025920	And like that day was meaningful because like, like it was real.
9025920	9027920	Like I actually had to turn my car around.
9028800	9031200	It's rare that anything even real happens in my life.
9031200	9035040	Even as, you know, mundane is the fact that, yeah, there was snow.
9035040	9037600	I had to turn around, stay in Gunnison and leave the next day.
9037600	9039040	Something about that moment felt real.
9040160	9045280	Okay. So it's interesting to break apart the three moments you mentioned if it's okay.
9045280	9050320	So it'll, uh, I always have trouble pronouncing his name, but allows a Yurkowski.
9053200	9060240	So what, how did your worldview change in starting to consider the
9061440	9065520	exponential growth of AI and AGI that he thinks about and the, uh,
9065520	9068720	the threats of artificial intelligence and all that kind of ideas?
9068720	9072400	Like, can you, is it just like, can you maybe, uh, break apart?
9072400	9077280	Like what exactly was so magical to you as a transformation experience?
9077280	9079600	Today, everyone knows him for threats and AI safety.
9080240	9082080	This was pre that stuff.
9082080	9084240	There was, I don't think a mention of AI safety on the page.
9085840	9087760	This is, this is old Yurkowski stuff.
9087760	9088960	He'd probably denounce it all now.
9088960	9091600	He'd probably be like, that's exactly what I didn't want to happen.
9091600	9092560	Oh, sorry, man.
9095200	9098640	Is there something specific you can take from his work that you can remember?
9098640	9105920	Yeah. Uh, it was this realization that, uh, computers double in power every 18 months
9105920	9107040	and humans do not.
9107680	9112640	And they haven't crossed yet, but if you have one thing that's doubling every 18 months
9112640	9116000	and one thing that's staying like this, you know, here's your log graph.
9116880	9119600	Here's your line, you know, you calculate that.
9121920	9125120	And then did that open the door to the exponential thinking?
9125120	9130880	Like thinking that, like, you know what, with technology, we can actually transform the world.
9131440	9133040	It opened the door to human obsolescence.
9133600	9139360	It, it opened the door to realize that in my lifetime, humans are going to be replaced.
9140560	9144480	And then the matching idea to that of artificial intelligence with the Hutter Prize.
9146960	9147840	You know, I'm torn.
9147840	9150080	I go back and forth on what I think about it.
9150080	9150400	Yeah.
9151360	9154320	But the, the, the basic thesis is it's nice.
9154320	9159600	It's a nice compelling notion that we can reduce the task of creating an intelligent system,
9159600	9162720	a generally intelligent system, into the task of compression.
9163840	9166240	So you, you can think of all of intelligence in the universe.
9166240	9168160	In fact, this is a kind of compression.
9170160	9173840	Do you find that a, was that just at the time you found that as a compelling idea?
9173840	9176080	Or do you still find that a compelling idea?
9176880	9178240	I still find that a compelling idea.
9179040	9186160	I think that it's not that useful day-to-day, but actually one of maybe my quests before that
9186160	9188560	was a search for the definition of the word intelligence.
9189120	9190080	And I never had one.
9190800	9193680	And I definitely have a definition of the word compression.
9194560	9197600	It's a very simple, straightforward one.
9198320	9199600	And you know what compression is.
9199600	9202320	You know what lossless, is lossless compression, not lossy, lossless compression.
9202880	9206480	And that that is equivalent to intelligence, which I believe,
9206480	9208800	I'm not sure how useful that definition is day-to-day,
9208800	9211280	but like I now have a framework to understand what it is.
9212160	9217520	And he just 10x'd the prize for that competition, like recently a few months ago.
9217520	9219360	You ever thought of taking a crack at that?
9219360	9220960	Oh, I did.
9220960	9221440	Oh, I did.
9221440	9226960	I spent, I spent the next, after I found the prize, I spent the next six months of my life
9226960	9227760	trying it.
9227760	9231200	And well, that's when I started learning everything about AI.
9231200	9233200	And then I worked at Vicarious for a bit.
9233200	9235200	And then I learned, read all the deep learning stuff.
9235200	9237680	And I'm like, okay, now I like, I'm caught up to modern AI.
9238720	9242400	And I had, I had a really good framework to put it all in from the compression stuff.
9244000	9244320	Right.
9244320	9247440	Like some of the first, some of the first deep learning models I played with were,
9248240	9250720	were GTT, GPT basically.
9250720	9257840	But before Transformers, before it was still RNNs to do character prediction.
9257840	9262800	But by the way, on the compression side, I mean, the, especially with neural networks,
9262800	9266480	what do you make of the lossless requirement with the harder prize?
9266480	9272960	So, you know, human intelligence and neural networks can probably compress stuff pretty
9272960	9274480	well, but there will be lossy.
9275120	9275920	It's imperfect.
9276560	9280240	You can turn a lossy compression into a lossless compressor pretty easily using an arithmetic
9280240	9280960	encoder, right?
9280960	9285920	You can take an arithmetic encoder and you can just encode the noise with maximum efficiency.
9285920	9289840	Right. So even if you can't predict exactly what the next character is,
9290400	9294160	the better a probability distribution you can put over the next character,
9294160	9297200	you can then use an arithmetic encoder to, right?
9297200	9299120	You don't have to know whether it's an E or an I.
9299120	9302880	You just have to put good probabilities on them and then, you know, code those.
9302880	9305600	And if you have, it's a bit of entropy thing, right?
9306320	9310160	So let me, on that topic, it could be interesting as a little side tour.
9310160	9314960	What are your thoughts in this year about GPT3 and these language models and these
9314960	9316080	transformers?
9316080	9319920	Is there something interesting to you as an AI researcher?
9320560	9324160	Or is there something interesting to you as an autonomous vehicle developer?
9324720	9327280	Nah, I think, uh, I think it's all right.
9327280	9328960	I mean, it's not like it's cool.
9328960	9330080	It's cool for what it is.
9330080	9334480	But no, we're not just going to be able to scale up to GPT12 and get general purpose
9334480	9335280	intelligence.
9335280	9340480	Like your loss function is literally just, you know, you know, cross entropy loss on the
9340480	9341280	character, right?
9341280	9343360	Like that's not the loss function of general intelligence.
9343600	9344800	Is that obvious to you?
9344800	9345280	Yes.
9346160	9354080	Can you imagine that, like to play devil's advocate on yourself, is it possible that
9354080	9359440	you can, the GPT12 will achieve general intelligence with something as dumb as this
9359440	9360720	kind of loss function?
9360720	9363680	I guess it depends what you mean by general intelligence.
9363680	9369920	So there's another problem with the GPTs and that's that they don't have a, uh, they
9369920	9370880	don't have long-term memory.
9371680	9372160	Right.
9372880	9373440	Right.
9373440	9373840	All right.
9373840	9384960	So like just GPT12, a scaled up version of GPT2 or 3, I find it hard to believe.
9385920	9387040	Well, you can scale it in.
9388080	9393840	It's, uh, so it's a hard-coded, hard-coded length, but you can make it wider and wider
9393840	9394240	and wider.
9394240	9399920	Yeah, you're going to get, you're going to get cool things from those systems.
9400880	9406080	But I don't think you're ever going to get something that can like, you know,
9406080	9407520	build me a rocket ship.
9407520	9408880	What about solve driving?
9409440	9413680	So, you know, you can use transformer with video, for example.
9414640	9417200	You think, is there something in there?
9417200	9420560	No, because look, we use, we use a grew.
9421120	9421760	We use a grew.
9421760	9423520	We could change that grew out to a transformer.
9424880	9428560	Um, I think driving is much more Markovian than language.
9429360	9433600	So Markovian, you mean like the memory, which, which aspect of Markovian?
9433600	9438640	I mean that like most of the information in the state at T minus one is also in the,
9438640	9440240	is in state T, right?
9440240	9443840	And it kind of like drops off nicely like this, whereas sometime with language,
9443840	9446640	you have to refer back to the third paragraph on the second page.
9447200	9450800	I feel like there's not many, like, like you can say like speed limit signs,
9450800	9453920	but there's really not many things in autonomous driving that look like that.
9453920	9458880	But if you look at, uh, to play devil's advocate is, uh, the risk estimation thing
9458880	9460560	that you've talked about is kind of interesting.
9461200	9467280	Is, uh, it feels like there might be some longer term, uh, aggregation of context
9467280	9470960	necessary to be able to figure out like the context.
9472080	9473200	Yeah, I'm not even sure.
9473200	9475360	I'm, I'm believing my, my own devil's.
9475360	9479200	We have a nice, we have a nice like vision model, which outputs like a,
9479200	9481760	a one to four dimensional perception space.
9482240	9484720	Um, can I try transformers on it?
9484720	9485040	Sure.
9485040	9485680	I probably will.
9486480	9488800	At some point we'll try transformers and then we'll just see.
9488800	9489600	Do they do better?
9489600	9489840	Sure.
9489840	9490320	I'm, I'm,
9490320	9491760	but it might not be a game changer.
9491760	9496080	No, well, I'm not like, like might transformers work better than grooves for autonomous driving?
9496080	9496640	Sure.
9496640	9497520	Might we switch?
9497520	9497840	Sure.
9497840	9499200	Is this some radical change?
9499200	9499920	No.
9499920	9500240	Okay.
9500240	9503360	We used a slightly different, you know, we switched from RNNs to grooves like,
9503360	9505600	okay, maybe it's grooves to transformers, but no, it's not.
9507680	9511200	Well, on the, on the topic of general intelligence, I don't know how much
9511200	9512080	I've talked to you about it.
9512080	9517200	Like what, um, do you think we'll actually build an AGI?
9517920	9521680	Like if, if you look at Ray Kurzweil with a singularity, do you, do you have like an
9521680	9525120	intuition about, you're kind of saying driving is easy.
9525120	9525520	Yeah.
9526160	9534880	And I, I tend to personally believe that solving driving will have really deep,
9534880	9539200	important impacts on our ability to solve general intelligence.
9539200	9544320	Like I think driving doesn't require general intelligence, but I think they're going to
9544320	9550960	be neighbors in a way that it's like deeply tied because it's so, like driving is so deeply
9550960	9557280	connected to the human experience that I think solving one will help solve the other.
9557280	9562240	But, but so I don't see, I don't see driving as like easy and almost like separate than
9562240	9566560	general intelligence, but like what's your vision of a future with a singularity?
9566560	9570480	Do you see there'll be a single moment, like a singularity where it'll be a face shift?
9570480	9572240	Are we in the singularity now?
9572240	9575520	Like what, do you have crazy ideas about the future in terms of AGI?
9575520	9577120	We're definitely in the singularity now.
9577920	9578320	We are.
9578320	9580000	Of course, of course.
9580000	9581360	Look at the bandwidth between people.
9581360	9582640	The bandwidth between people goes up.
9583200	9583600	All right.
9584640	9587120	The singularity is just, you know, when the bandwidth, but
9587120	9588560	What do you mean by the bandwidth of people?
9588560	9591200	Communications, tools, the whole world is networked.
9591200	9593760	The whole world is networked and we raise the speed of that network, right?
9594560	9600000	Oh, so you think the communication of information in a distributed way is a
9600000	9602160	empowering thing for collective intelligence?
9602160	9604400	Oh, I didn't say it's necessarily a good thing, but I think that's like
9604400	9607360	when I think of the definition of the singularity, yeah, it seems kind of right.
9608080	9612640	I see, like it's a change in the world beyond which
9613440	9616560	like the world be transformed in ways that we can't possibly imagine.
9616560	9619600	No, I mean, I think we're in the singularity now in the sense that there's like, you know,
9619600	9622320	one world in a monoculture and it's also linked.
9622320	9628080	Yeah, I mean, I kind of shared the intuition that the singularity will originate from the
9628080	9635200	collective intelligence of us ants versus the like some single system AGI type thing.
9635200	9636160	Oh, I totally agree with that.
9636880	9640960	Yeah, I don't, I don't really believe in like like a hard takeoff AGI kind of thing.
9645120	9649440	Yeah, I don't think, I don't even think AI is all that different in kind
9649440	9650960	from what we've already been building.
9651920	9656160	With respect to driving, I think driving is a subset of general intelligence,
9656160	9657920	and I think it's a pretty complete subset.
9657920	9662160	I think the tools we develop at comma will also be extremely helpful
9662160	9663920	to solving general intelligence.
9663920	9666400	And that's, I think the real reason why I'm doing it.
9666400	9667680	I don't care about self-driving cars.
9668240	9669600	It's a cool problem to beat people at.
9670800	9674400	But yeah, I mean, yeah, you're kind of, you're of two minds.
9674400	9678400	So one, you do have to have a mission and you want to focus and make sure you get,
9678400	9678960	you get there.
9678960	9680560	You can't forget that.
9680560	9686960	But at the same time, there is a thread that's much bigger than the
9686960	9689840	capacity entirety of your effort that's much bigger than just driving.
9691120	9695120	With AI and with general intelligence, it is so easy to delude yourself
9695120	9697280	into thinking you've figured something out when you haven't.
9697280	9702560	If we build a level five self-driving car, we have indisputably built something.
9702560	9703200	Yeah.
9703200	9704640	Is it general intelligence?
9704640	9705840	I'm not going to debate that.
9705840	9709600	I will say we've built something that provides huge financial value.
9709600	9710480	Yeah, beautifully put.
9710480	9713520	That's the engineer and credo, just build the thing.
9713520	9718640	It's like, that's why I'm with Elon on Go to Mars.
9718640	9719680	Yeah, that's a great one.
9719680	9722960	You can argue who the hell cares about Go to Mars.
9723520	9729040	But the reality is, set that as a mission, get it done, and then you're going to crack some
9729040	9732640	problem that you've never even expected in the process of doing that.
9732640	9732960	Yeah.
9733840	9737680	Yeah, I mean, I think if I had a choice between humanity going to Mars and solving
9737680	9741120	self-driving cars, I think going to Mars is better.
9741120	9743440	But I don't know, I'm more suited for self-driving cars.
9743440	9744240	I'm an information guy.
9744240	9744880	I'm not a modernist.
9744880	9745600	I'm a post-modernist.
9746480	9748240	Post-modernist, all right.
9748240	9748880	Beautifully put.
9749840	9752240	Let me drag you back to programming for a sec.
9752240	9756480	What three, maybe three to five programming languages should people learn, do you think?
9756480	9761600	Like if you look at yourself, what did you get the most out of from learning?
9762560	9765840	Well, so everybody should learn C and assembly.
9765840	9767280	We'll start with those two, right?
9767280	9768000	Assembly.
9768000	9768480	Yeah.
9768480	9771520	If you can't code in assembly, you don't know what the computer's doing.
9771520	9776080	You don't understand, you don't have to be great in assembly, but you have to code in it.
9776640	9781200	And then you have to appreciate assembly in order to appreciate all the great things C gets you.
9781920	9785440	And then you have to code in C in order to appreciate all the great things Python gets you.
9786160	9788640	So I'll just say assembly C and Python, we'll start with those three.
9789600	9796640	The memory allocation of C and the fact that assembly is to give you a sense of just how
9796640	9800560	many levels of abstraction you get to work on in modern-day programming.
9800560	9801040	Yeah, yeah.
9801040	9804400	Graph coloring for assignment, register assignment, and compilers.
9806080	9808400	The computer only has a certain number of registers.
9808400	9810320	You can have all the variables you want in a C function.
9811040	9815600	So you get to start to build intuition about compilation, like what a compiler gets you.
9817280	9817680	What else?
9818400	9823840	Well, then there's kind of... So those are all very imperative programming languages.
9825680	9829120	Then there's two other paradigms for programming that everybody should be familiar with.
9829120	9830080	One of them is functional.
9831120	9835120	You should learn Haskell and take that all the way through, learn a language with dependent
9835120	9841600	types like Coq. Learn that whole space, like the very PL theory heavy languages.
9842560	9846400	And Haskell is your favorite functional? Is that the go-to, you'd say?
9846400	9850400	Yeah, I'm not a great Haskell programmer. I wrote a compiler in Haskell once.
9850400	9853600	There's another paradigm, and actually there's one more paradigm that I'll even talk about
9853600	9855760	after that that I never used to talk about when I would think about this.
9855760	9858160	But the next paradigm is learn Verilog or HDL.
9860160	9862880	Understand this idea of all of the instructions executed once.
9865360	9869760	If I have a block in Verilog and I write stuff in it, it's not sequential.
9869760	9870960	They all execute it once.
9873600	9875440	And then think like that. That's how hard it works.
9876480	9879840	So I guess assembly doesn't quite get you that.
9879840	9884080	Assembly is more about compilation, and Verilog is more about the hardware,
9884080	9887680	like giving a sense of what actually the hardware is doing.
9888400	9892320	Assembly, C, Python are straight like they sit right on top of each other.
9892320	9895520	In fact, C is kind of coded in C.
9895520	9899040	But you could imagine the first C was coded in assembly, and Python is actually coded in C.
9899920	9902240	So you can straight up go on that.
9903520	9905520	Got it. And then Verilog gives you that.
9905600	9907440	That's brilliant. Okay.
9907440	9909760	And then I think there's another one now.
9909760	9914480	Everyone's Carpathian calls it programming 2.0, which is learn a...
9915280	9918480	I'm not even gonna... Don't learn TensorFlow. Learn PyTorch.
9918480	9919440	So machine learning.
9920080	9922800	We've got to come up with a better term than programming 2.0 or...
9925360	9926000	But yeah.
9926000	9926880	It's a programming language.
9929760	9932560	I wonder if it can be formalized a little bit better.
9932560	9935920	We feel like we're in the early days of what that actually entails.
9936960	9938160	Data-driven programming?
9939040	9940640	Data-driven programming, yeah.
9941600	9944480	But it's so fundamentally different as a paradigm than the others.
9945840	9947920	Like it almost requires a different skill set.
9950240	9951440	But you think it's still... Yeah.
9953600	9955680	And PyTorch versus TensorFlow, PyTorch wins.
9956480	9957360	It's the fourth paradigm.
9957360	9959280	It's the fourth paradigm that I've kind of seen.
9959280	9964720	There's like this imperative functional hardware.
9964720	9966240	I don't know a better word for it.
9966240	9967120	And then ML.
9968160	9975120	Do you have advice for people that want to get into programming
9975120	9976080	and want to learn programming?
9976080	9977760	You have a video.
9979920	9980880	What is programming?
9980880	9982800	Newb lessons, exclamation point.
9982800	9984640	And I think the top comment is like,
9984640	9986240	warning, this is not for noobs.
9987200	9992480	Do you have a newb, like TLDW for that video,
9992480	9999520	but also a newb friendly advice on how to get into programming?
9999520	10002880	We're never going to learn programming by watching a video
10002880	10004560	called Learn Programming.
10004560	10006560	The only way to learn programming, I think,
10006560	10009040	and the only one is the only way everyone I've ever met
10009040	10011840	who can program well learned it all in the same way.
10011840	10013360	They had something they wanted to do,
10013680	10015360	and then they tried to do it.
10015360	10019600	And then they were like, oh, well, okay, this is kind of,
10019600	10021600	you know, it'd be nice if the computer could kind of do this.
10021600	10023360	And then, you know, that's how you learn.
10023360	10025360	You just keep pushing on a project.
10027360	10030720	So the only advice I have for learning programming is go program.
10030720	10033120	Somebody wrote to me a question like,
10033120	10035840	we don't really, they're looking to learn
10035840	10037440	about recurring neural networks.
10037440	10039760	And it's saying like, my company's thinking of doing,
10039760	10042560	using recurring neural networks for time series data.
10042880	10046000	But we don't really have an idea of where to use it yet.
10046000	10048480	We just want to, like, do you have any advice on how to learn about,
10048480	10051840	these are these kind of general machine learning questions.
10051840	10056480	And I think the answer is like, actually have a problem
10056480	10058480	that you're trying to solve and just.
10058480	10059440	I see that stuff.
10059440	10061600	Oh my God, when people talk like that, they're like,
10061600	10064000	I heard machine learning is important.
10064000	10066000	Could you help us integrate machine learning
10066000	10068000	with macaroni and cheese production?
10068720	10073840	You just, I don't even, you can't help these people.
10073840	10075840	Like who lets you run anything?
10075840	10077680	Who lets that kind of person run anything?
10078240	10082560	I think we're all, we're all beginners at some point.
10082560	10083040	So.
10083040	10084800	It's not like they're a beginner.
10084800	10088480	It's like, my problem is not that they don't know about machine learning.
10088480	10091600	My problem is that they think that machine learning has something to say
10091600	10093200	about macaroni and cheese production.
10094480	10097040	Or like, I heard about this new technology.
10097040	10098560	How can I use it for why?
10099760	10102640	Like, I don't know what it is, but how can I use it for why?
10103440	10104080	That's true.
10104080	10106000	And you have to build up an intuition of how,
10106000	10107520	because you might be able to figure out a way,
10107520	10112160	but like the prerequisites is you should have a macaroni and cheese problem to solve first.
10112160	10112640	Exactly.
10113360	10116800	And then two, you should have more traditional,
10116800	10121920	like the learning process involved more traditionally applicable problems
10121920	10124480	in the space of whatever that is of machine learning.
10124480	10126960	And then see if it can be applied to macrophages.
10126960	10129040	At least start with, tell me about a problem.
10129040	10130560	Like, if you have a problem, you're like,
10130560	10133600	you know, some of my boxes aren't getting enough macaroni in them.
10134400	10136640	Can we use machine learning to solve this problem?
10136640	10138400	That's much, much better than,
10138400	10141520	how do I apply machine learning to macaroni and cheese?
10141520	10146320	One big thing, maybe this is me talking to the audience a little bit,
10146320	10149840	because I get these days so many messages,
10149840	10153360	advice on how to like, learn stuff.
10153360	10158080	Okay. My, this is not me being mean.
10158080	10162080	I think this is quite profound actually, is you should Google it.
10162720	10163200	Oh yeah.
10163760	10170480	Like one of the like skills that you should really acquire as an engineer,
10171040	10174000	as a researcher, as a thinker, like one,
10174000	10176560	there's two complementary skills.
10176560	10180720	Like one is with a blank sheet of paper with no internet to think deeply.
10181520	10185840	And then the other is to Google the crap out of the questions you have.
10185840	10187040	Like that's actually a skill.
10187840	10190560	People often talk about, but like doing research,
10190560	10193280	like pulling at the thread and like looking up different words,
10193840	10197920	going into like GitHub repositories with two stars,
10197920	10199600	and like looking how they did stuff,
10199600	10203360	like looking at the code or going on Twitter,
10203360	10207440	seeing like there's little pockets of brilliant people that are like having discussions.
10207520	10211520	Like if you're a neuroscientist, go into signal processing community.
10211520	10215760	If you're an AI person going into the psychology community,
10215760	10219840	like the switch communities that keep searching, searching, searching,
10219840	10225680	because it's so much better to invest in like finding somebody else
10225680	10230240	who already solved your problem than is to try to solve the problem.
10230800	10234240	And because they've often invested years of their life,
10234320	10239120	like entire communities are probably already out there who have tried to solve your problem.
10239120	10240240	I think they're the same thing.
10240880	10244080	I think you go try to solve the problem.
10244080	10246080	And then in trying to solve the problem,
10246080	10247680	if you're good at solving problems,
10247680	10249760	you'll stumble upon the person who solved it already.
10249760	10252240	Yeah, but the stumbling is really important.
10252240	10254160	I think that's a skill that people should really put,
10254160	10256880	especially in undergrad, like search.
10257600	10260320	If you ask me a question, how should I get started in deep learning?
10261200	10267120	Especially that is just so Google-able.
10268000	10271520	The whole point is you Google that and you get a million pages
10271520	10272960	and just start looking at them.
10273840	10274400	Yeah.
10274400	10277200	Start pulling at the thread, start exploring, start taking notes,
10277200	10282720	start getting advice from a million people that already spent their life
10282720	10284160	answering that question, actually.
10284880	10285280	Oh, well, yeah.
10285280	10287680	I mean, that's definitely also when people ask me things like that,
10287760	10290320	I'm like, trust me, the top answer on Google is much, much better
10290320	10291440	than anything I'm going to tell you.
10291440	10292000	Anything you want to say.
10292000	10292800	Right?
10292800	10293120	Yeah.
10294400	10295120	People ask.
10295920	10297040	It's an interesting question.
10298000	10299840	Let me know if you have any recommendations.
10299840	10303680	What three books, technical or fiction or philosophical,
10303680	10307440	had an impact on your life or you wouldn't recommend, perhaps?
10309040	10311040	Maybe we'll start with the least controversial.
10311040	10311760	Infinite Jest.
10314080	10315920	Infinite Jest is a...
10316880	10318800	David Foster Wallace.
10318800	10320640	Yeah, it's a book about wireheading, really.
10323760	10326880	Very enjoyable to read, very well-written.
10328320	10330000	You will grow as a person reading this book.
10331040	10331600	It's effort.
10332960	10336000	And I'll set that up for the second book, which is pornography.
10336000	10340240	That's called Atlas Shrugged, which...
10341040	10342480	Atlas Shrugged is pornography.
10342480	10343760	Yeah, I mean, it is.
10343760	10345600	I will not defend the...
10345600	10347840	I will not say Atlas Shrugged is a well-written book.
10348480	10351440	It is entertaining to read, certainly, just like pornography.
10351440	10353040	The production value isn't great.
10354080	10358800	There's a 60-page monologue in there that Anne Rand's editor really wanted to take out.
10358800	10364880	And she paid out of her pocket to keep that 60-page monologue in the book.
10365840	10374560	But it is a great book for a kind of framework of human relations.
10374560	10377360	And I know a lot of people are like, yeah, but it's a terrible framework.
10377920	10379280	Yeah, but it's a framework.
10380320	10386240	Just for context, in a couple of days, I'm speaking for probably four plus hours with
10386240	10391440	Yaron Brooke, who's the main living, remaining objectivist.
10392160	10393200	Objectivist.
10393200	10393680	Interesting.
10394560	10400320	So I've always found this philosophy quite interesting on many levels.
10400320	10408880	One of how repulsive some large percent of the population find it, which is always funny to
10408880	10415280	me when people are unable to even read a philosophy because of some...
10415280	10420080	I think that says more about their psychological perspective on it.
10420960	10428640	But there is something about objectivism and Anne Rand's philosophy that's deeply connected
10428640	10434320	to this idea of capitalism, of the ethical life is the productive life,
10436480	10439920	that was always compelling to me.
10441760	10445440	I didn't seem to interpret it in the negative sense that some people do.
10445440	10447200	To be fair, I read the book when I was 19.
10447840	10449120	So you had an impact at that point.
10449520	10455200	Yeah, and the bad guys in the book have this slogan, from each according to their ability
10455200	10456480	to each according to their need.
10458480	10460240	I'm looking at this and I'm like, these are the most...
10460240	10462800	This is team rocket level cartoonishness, right?
10462800	10463680	No, bad guy.
10463680	10468160	And then when I realized that was actually the slogan of the Communist Party, I'm like,
10468160	10471520	wait a second, wait, no, no, no, no, no.
10471520	10472960	You're telling me this really happened?
10473920	10474720	Yeah, it's interesting.
10474960	10479040	One of the criticisms of her work is she has a cartoonish view of good and evil.
10482080	10487520	The reality, as Jordan Peterson says, is that each of us have the capacity for good and evil in
10487520	10492080	us as opposed to there's some characters who are purely evil and some characters are purely good.
10492080	10493920	And that's in a way why it's pornographic.
10495040	10496880	The production value, I love it.
10496880	10501920	Well, evil is punished and they're very clearly like, you know, there's no, you know,
10502640	10507360	you know, just like porn doesn't have, you know, like character growth, you know,
10507360	10508880	neither does Alice Rodney to like.
10509440	10510800	Brilliant, well put.
10510800	10514080	But at 19 year old George Hots, it was good enough.
10514080	10515280	Yeah, yeah, yeah, yeah.
10515280	10516800	What's the third?
10516800	10517440	You have something?
10519040	10521360	I could give these two I'll just throw out.
10521360	10523360	They're sci-fi, Perbutation City.
10524160	10526560	Great thing, just try thinking about copies yourself.
10526560	10527360	And then the minute...
10527360	10527840	Isn't that by?
10527840	10528560	Sorry to interrupt.
10528560	10530720	That is Greg Egan.
10532320	10533440	That might not be his real name.
10533440	10534320	Some Australian guy.
10534320	10535600	It might not be Australian.
10535600	10536080	I don't know.
10536960	10538560	And then this one's online.
10538560	10540720	It's called the Metamorphosis of Prime Intellect.
10542960	10545280	It's a story set in a post-singularity world.
10545280	10545760	It's interesting.
10546640	10550160	Is there, can you, in either of the worlds, do you find something
10550160	10552240	philosophical interesting in them that you can comment on?
10553520	10555520	I mean, it is clear to me that
10557920	10561200	Metamorphosis of Prime Intellect is like written by an engineer.
10562720	10571040	Which is, it's very, it's very almost a pragmatic take on a utopia, in a way.
10572480	10573440	Positive or negative?
10575120	10577840	That's up to you to decide reading the book.
10577840	10581440	And the ending of it is very interesting as well.
10581440	10583520	And I didn't realize what it was.
10583520	10585120	I first read that when I was 15.
10585120	10586800	I've reread that book several times in my life.
10587360	10589040	And it's short, it's 50 pages.
10589040	10589920	I want you to go read it.
10590720	10593040	What's, sorry, it's a little tangent.
10593040	10594560	I've been working through the foundation.
10594560	10596960	I've been, I haven't read much sci-fi in my whole life.
10596960	10600080	And I'm trying to fix that in the last few months.
10600080	10601520	That's been a little side project.
10602080	10607520	What's to use the greatest sci-fi novel that people should read?
10607520	10609200	Or is it, or?
10609200	10611040	I mean, I would, yeah, I would, I would say like, yeah,
10611040	10612800	Permutation City, Metamorphosis of Prime Intellect.
10612800	10613680	Got it.
10613680	10614320	I don't know.
10614320	10615520	I didn't like Foundation.
10616320	10617680	I thought it was way too modernist.
10618560	10620640	Be like Dune and like all of those.
10620640	10621760	I've never read Dune.
10621760	10622800	I've never read Dune.
10622800	10623760	I have to read it.
10624480	10626720	Fire Upon the Deep is interesting.
10628960	10630400	Okay, I mean, look, everyone should read,
10630400	10631280	everyone should read Neuromancer.
10631280	10632720	Everyone should read Snow Crash.
10632720	10634400	If you haven't read those, like, start there.
10635360	10636320	Yeah, I haven't read Snow Crash.
10636320	10637280	I haven't read Snow Crash.
10637280	10638240	Oh, it's, I mean, it's very entertaining.
10638240	10638720	It's surprising.
10639840	10640640	Go to Lesher Bach.
10640640	10643040	And if you want the controversial one, Bronze Age Mindset.
10645200	10646560	All right, I'll look into that one.
10647520	10650080	Those aren't sci-fi, but just to round out books.
10651600	10655200	So a bunch of people asked me on Twitter and Reddit and so on
10655840	10656800	for advice.
10656800	10660080	So what advice would you give a young person today about life?
10664720	10667440	What, yeah, I mean, looking back,
10667440	10669760	especially when you're a young, younger, you did,
10670400	10674800	and you continued it, you've accomplished a lot of interesting things.
10674800	10677120	Is there some advice from those,
10679760	10681760	from that life of yours that you can pass on?
10681760	10686480	If college ever opens again, I would love to give a graduation speech.
10687520	10692160	At that point, I will put a lot of somewhat satirical effort into this question.
10692160	10695280	Yeah, at this, you haven't written anything at this point.
10696240	10698080	You know what, always wear sunscreen.
10698080	10699680	This is water, like.
10699680	10701040	I think you're plagiarizing.
10701040	10704720	I mean, you know, but that's the, that's the, like,
10704880	10706080	put clean your room.
10706080	10708480	You know, yeah, you can plagiarize them from all of this stuff.
10708480	10709040	And it's, it's,
10713200	10713920	there is no,
10715840	10717520	self-help books aren't designed to help you.
10717520	10718880	They're designed to make you feel good.
10719920	10724000	Like, whatever advice I could give, you already know.
10724000	10725120	Everyone already knows.
10725760	10727200	Sorry, it doesn't feel good.
10730080	10730480	Right?
10730480	10732000	Like, you know, you know.
10732960	10735760	If I tell you that you should, you know,
10736800	10741120	eat well and read more and it's not going to do anything.
10741760	10746160	I think the whole, like, genre of those kind of questions is meaningless.
10747360	10747840	I don't know.
10747840	10750480	If anything, it's don't worry so much about that stuff.
10750480	10751760	Don't be so caught up in your head.
10752320	10752960	Right.
10752960	10756400	I mean, you're, yeah, in the sense that your whole life,
10756400	10759600	your whole existence is like moving version of that advice.
10760560	10761040	I don't know.
10763760	10765360	There's, there's something, I mean,
10765360	10768160	there's something in you that resists that kind of thinking in that,
10768160	10774400	in itself is, it's just illustrative of who you are.
10774400	10775760	And there's something to learn from that.
10776640	10779440	I think you're, you're clearly not overthinking stuff.
10781280	10781600	Yeah.
10781600	10782000	And you know what?
10782000	10783040	There's a gut thing.
10783040	10785120	I, even when I talk about my advice, I'm like,
10785120	10786720	my advice is only relevant to me.
10787280	10788640	It's not relevant to anybody else.
10788640	10789840	I'm not saying you should go out.
10789840	10791520	If you're the kind of person who overthinks things,
10791520	10793280	to stop overthinking things, it's not bad.
10793920	10794800	It doesn't work for me.
10794800	10795600	Maybe it works for you.
10795600	10796880	I, you know, I don't know.
10797840	10799200	Let me ask you about love.
10799200	10799520	Yeah.
10801840	10805040	So I think last time we talked about the meaning of life,
10805040	10808480	and it was, it was kind of about winning.
10808480	10808800	Of course.
10810720	10812960	I don't think I've talked to you about love much,
10812960	10817280	whether romantic or just love for the common humanity amongst us all.
10818000	10821280	What role has love played in your life?
10821280	10825600	In this, in this quest for winning, where does love fit in?
10826160	10829760	Well, where love, I think means several different things.
10829760	10832800	There's love in the sense of, maybe I could just say,
10832800	10837840	there's like love in the sense of opiates and love in the sense of oxytocin,
10837840	10844320	and then love in the sense of maybe like a love for math.
10844320	10846720	I don't think fits into either of those first two paradigms.
10847840	10856000	So each of those, have they, have they, have they given something to you in your life?
10856720	10858640	I'm not that big of a fan of the first two.
10860640	10860880	Why?
10863440	10866160	The same reason I'm not a fan of, you know,
10866160	10869600	the same reason I don't do opiates and don't take ecstasy.
10871280	10872960	And there were times, look, I've tried both.
10873920	10877040	I like opiates way more than ecstasy.
10878240	10883200	But they're not, the ethical life is the productive life.
10884320	10886880	So maybe that's my problem with those.
10886880	10891200	And then like, yeah, a sense of, I don't know, like abstract love for humanity.
10892000	10896080	I mean, the abstract love for humanity, I'm like, yeah, I've always felt that.
10896080	10900320	And I guess it's hard for me to imagine not feeling it.
10900320	10902560	And maybe there's people who don't and I don't know.
10903440	10903680	Yeah.
10903680	10905680	That's just like a background thing that's there.
10906320	10909200	I mean, since we brought up drugs, let me ask you,
10911600	10915360	this is becoming more and more a part of my life because I'm talking to a few researchers
10915360	10916640	that are working on psychedelics.
10917600	10920240	I've eaten shrooms a couple of times.
10920240	10924800	And it was fascinating to me that like the mind can go to like,
10926080	10929280	just fascinating, the mind can go to places I didn't imagine it could go.
10929360	10932640	And I was very friendly and positive and exciting.
10932640	10936080	And everything was kind of hilarious in the place.
10936080	10938160	Wherever my mind went, that's where I went.
10938160	10941040	Is what do you think about psychedelics?
10941040	10944560	Do you think they have, where do you think the mind goes?
10944560	10946080	Have you done psychedelics?
10946080	10947360	Where do you think the mind goes?
10948560	10951040	Is there something useful to learn about the places it goes?
10952240	10953120	Once you come back.
10953760	10960400	I find it interesting that this idea that psychedelics have something to teach
10960400	10962400	is almost unique to psychedelics.
10963760	10965600	People don't argue this about amphetamines.
10968400	10970160	And I'm not really sure why.
10970160	10972960	I think all of the drugs have lessons to teach.
10973680	10975040	I think there's things to learn from opiates.
10975040	10976480	I think there's things to learn from amphetamines.
10976480	10978080	I think there's things to learn from psychedelics,
10978080	10979120	things to learn from marijuana.
10980000	10986640	But also at the same time, recognize that I don't think you're learning things
10986640	10987280	about the world.
10987280	10988800	I think you're learning things about yourself.
10990640	10994160	And what's the, it might have even been.
10995600	10997280	Might have even been a Timothy Leary quote.
10997280	10998080	I don't know his quote.
10998080	11001440	But the idea is basically like, everybody should look behind the door.
11001440	11003680	But then once you've seen behind the door, you don't need to keep going back.
11004400	11009040	So, I mean, and that's my thoughts on all real drug use too.
11009760	11010800	So maybe for caffeine.
11012640	11016160	It's a little experience that it's good to have, but.
11017040	11017520	Oh yeah.
11017520	11020400	No, I mean, yeah, I guess, yeah, psychedelics are definitely.
11021760	11023840	So you're a fan of new experiences, I suppose.
11023840	11024160	Yes.
11024160	11026880	Because they all contain a little, especially the first few times,
11026880	11028640	it contains some lessons that can be picked up.
11029600	11030000	Yeah.
11030000	11033760	And I'll, I'll revisit psychedelics maybe once a year.
11035600	11037200	Usually small, smaller doses.
11038640	11040160	Maybe they turn up the learning rate of your brain.
11041280	11043120	I've heard that, I like that.
11043120	11044160	Yeah, that's cool.
11044160	11045760	Big learning rates have frozen comms.
11047360	11049360	Last question, and this is a little weird one,
11049360	11051520	but you've called yourself crazy in the past.
11054000	11057840	First of all, on a scale of one to 10, how crazy would you say, are you?
11057840	11060240	Oh, I mean, it depends how you, you know, when you compare me to
11060240	11062320	Elon Musk and I say Levin Dalsky, not so crazy.
11063360	11064560	So like, like a seven?
11065760	11067040	Let's go with six.
11067040	11068560	Six, six, six.
11069360	11070080	What, uh.
11071520	11073200	I like seven, seven's a good number.
11073200	11074160	Seven, sorry.
11074160	11076480	Well, yeah, I'm sure day by day changes, right?
11076480	11079840	So, but you're in that, in that area.
11079840	11085120	What, uh, in thinking about that, what do you think is the role of madness?
11085680	11090320	Is that a feature or a bug if you were to dissect your brain?
11091680	11096960	So, okay, from like a, like mental health lens on crazy,
11096960	11098880	I'm not sure I really believe in that.
11098880	11102960	I'm not sure I really believe in like a lot of that stuff, right?
11102960	11105840	This concept of, okay, you know, when you get over to like,
11106640	11109520	like, like, like hardcore bipolar and schizophrenia,
11109520	11113120	these things are clearly real, somewhat biological.
11113120	11118320	And then over here on the spectrum, you have like ADD and oppositional defiance disorder
11118320	11122800	and these things that are like, wait, this is normal spectrum human behavior.
11122800	11128560	Like this isn't, you know, where's the, the line here?
11128560	11131200	And why is this like a problem?
11131200	11135760	So there's this whole, you know, the neurodiversity of humanity is huge.
11135760	11137520	Like people think I'm always on drugs.
11137520	11139360	People are always saying this to me on my streams and like guys,
11139360	11141280	you know, like I'm real open with my drug use.
11141280	11145520	I would tell you if I was on drugs and I mean, I had like a cup of coffee this morning,
11145520	11147120	but other than that, this is just me.
11147120	11149520	You're witnessing my brain in action.
11151440	11155520	So, so the word madness doesn't even make sense.
11155520	11159360	And then you're in the rich neurodiversity of humans.
11160880	11166960	I think it makes sense, but only for like some insane extremes.
11166960	11174880	Like if you are actually like visibly hallucinating, you know, that's okay.
11174880	11177360	But there is the kind of spectrum on which you stand out.
11177360	11183680	Like that, that's like, if I were to look, you know, at decorations on a Christmas tree
11183680	11188720	or something like that, like if you were a decoration that would catch my eye.
11188720	11190560	Like that thing is sparkly.
11192720	11196480	Whatever the hell that thing is, there's something to that.
11197280	11204240	Just like refusing to be boring or maybe boring is the wrong word, but to,
11206640	11210240	yeah, I mean, be willing to sparkle.
11211200	11214080	You know, it's, it's like somewhat constructed.
11214080	11216400	I mean, I am who I choose to be.
11218400	11220800	I'm going to say things as true as I can see them.
11220800	11224240	I'm not going to, I'm not going to lie.
11224240	11226480	And but that's a really important feature in itself.
11226480	11230240	So like whatever the neurodiversity of your, whatever your brain is,
11231040	11237760	not putting constraints on it that force it to fit into the mold of what society is,
11237760	11240560	like defines what you're supposed to be.
11240560	11246160	So you're one of the specimens that, that doesn't mind being yourself.
11247760	11253440	Being right is super important, except at the expense of being wrong.
11257120	11258320	Without breaking that apart.
11258320	11260240	I think it's a beautiful way to end it.
11260240	11262960	George, you're, you're one of the most special humans I know.
11262960	11264480	It's truly an honor to talk to you.
11264480	11265600	Thanks so much for doing it.
11265600	11267600	Thank you for having me.
11267600	11270240	Thanks for listening to this conversation with George Hots.
11270240	11276400	And thank you to our sponsors for Sigmatic, which is the maker of delicious mushroom coffee,
11277120	11281360	Decoding Digital, which is a tech podcast that I listen to and enjoy,
11282080	11286400	and ExpressVPN, which is the VPN I've used for many years.
11286960	11290320	Please check out these sponsors in the description to get a discount
11290320	11292320	and to support this podcast.
11292960	11297760	If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple podcast,
11297760	11303680	follow on Spotify, support on Patreon, or connect with me on Twitter at Lex Freedman.
11304320	11309920	And now let me leave you with some words from the great and powerful Linus Torvald.
11310640	11312000	Talk is cheap.
11312000	11314160	Show me the code.
11314160	11322160	Thank you for listening and hope to see you next time.
