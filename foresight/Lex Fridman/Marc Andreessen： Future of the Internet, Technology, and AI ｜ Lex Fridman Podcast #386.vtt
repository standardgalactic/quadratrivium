WEBVTT

00:00.000 --> 00:04.840
the competence and capability and intelligence and training and accomplishments of senior

00:04.840 --> 00:09.320
scientists and technologists working on a technology and then being able to then make

00:09.320 --> 00:13.520
moral judgments in the use of that technology, that track record is terrible. That track

00:13.520 --> 00:18.360
record is catastrophically bad. The policies that are being called for to prevent this,

00:18.360 --> 00:20.600
I think we're going to cause extraordinary damage.

00:20.600 --> 00:24.720
So the moment you say AI is going to kill all of us, therefore we should ban it or we

00:24.720 --> 00:27.800
should regulate all that kind of stuff, that's when it starts getting serious.

00:27.800 --> 00:29.840
Or start, you know, military airstrikes and data centers.

00:29.840 --> 00:30.840
Oh boy.

00:30.840 --> 00:38.200
The following is a conversation with Mark Andreessen, co-creator of Mosaic, the first

00:38.200 --> 00:43.520
widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley

00:43.520 --> 00:49.360
venture capital firm Andreessen Horowitz, and is one of the most outspoken voices on

00:49.360 --> 00:57.120
the future of technology, including his most recent article, why AI will save the world.

00:57.120 --> 01:01.040
This is Alex Friedman podcast. To support it, please check out our sponsors in the

01:01.040 --> 01:06.680
description. And now, dear friends, here's Mark Andreessen.

01:06.680 --> 01:10.280
I think you're the right person to talk about the future of the internet and technology

01:10.280 --> 01:16.560
in general. Do you think we'll still have Google search in five, in 10 years, or search

01:16.560 --> 01:17.560
in general?

01:17.560 --> 01:22.280
Yes, you know, it'd be a question if the use cases have really narrowed down.

01:22.280 --> 01:30.320
Well, now with AI and AI assistance, being able to interact and expose the entirety of

01:30.320 --> 01:36.160
human wisdom and knowledge and information and facts and truth to us via the natural

01:36.160 --> 01:42.960
language interface, it seems like that's what search is designed to do. And if AI assistance

01:42.960 --> 01:46.000
can do that better, doesn't the nature of search change?

01:46.000 --> 01:48.120
Sure. But we still have horses.

01:48.120 --> 01:52.680
Okay. When was the last time you rode a horse?

01:52.680 --> 01:53.680
It's been a while.

01:53.680 --> 02:02.520
All right. But what I mean is, will we still have Google search as the primary way that

02:02.520 --> 02:05.320
human civilization uses to interact with knowledge?

02:05.320 --> 02:09.280
I mean, search was a technology, it was a moment in time technology, which is you have

02:09.280 --> 02:12.480
in theory the world's information out on the web. And, you know, this is sort of the

02:12.480 --> 02:16.080
open way to get to it. But yeah, like, and by the way, actually Google has known this

02:16.160 --> 02:19.400
for a long time. I mean, they've been driving away from the 10 blue links for, you know,

02:19.400 --> 02:21.640
for like two days, they've been trying to get away from that for a long time.

02:21.640 --> 02:22.640
What kind of links?

02:22.640 --> 02:23.640
They call the 10 blue links.

02:23.640 --> 02:24.640
10 blue links.

02:24.640 --> 02:28.360
So the standard Google search result is just 10 blue links to random websites.

02:28.360 --> 02:29.800
And they turn purple when you visit them.

02:29.800 --> 02:30.800
That's HTML.

02:30.800 --> 02:32.800
Yes, we picked those colors.

02:32.800 --> 02:33.800
Thanks.

02:33.800 --> 02:34.800
Thanks.

02:34.800 --> 02:37.040
I'm touching on this topic.

02:37.040 --> 02:38.040
No offense.

02:38.040 --> 02:39.040
Yeah, it's good.

02:39.040 --> 02:42.560
Well, you know, like Marshall McLuhan said that the content of each new medium is the

02:42.560 --> 02:43.560
old medium.

02:43.560 --> 02:45.640
The content of each new medium is the old medium.

02:45.640 --> 02:48.400
The content of movies was theater, you know, theater plays.

02:48.400 --> 02:52.280
The content of theater plays was, you know, written stories, the content of written stories

02:52.280 --> 02:53.280
was spoken stories.

02:53.280 --> 02:54.280
Right.

02:54.280 --> 02:57.720
And so you just kind of fold the old thing into the new thing.

02:57.720 --> 03:00.600
How does that have to do with the blue and the purple?

03:00.600 --> 03:04.960
Maybe for, you know, maybe within one of the things that AI can do for you is can generate

03:04.960 --> 03:05.960
the 10 blue links.

03:05.960 --> 03:06.960
Okay.

03:06.960 --> 03:11.160
So like, either if that's actually the useful thing to do or if you're feeling nostalgic.

03:12.160 --> 03:17.640
Also can generate the old InfoSeek or Alta Vista.

03:17.640 --> 03:18.640
What else was there?

03:18.640 --> 03:19.640
Yeah, yeah.

03:19.640 --> 03:20.640
In the 90s.

03:20.640 --> 03:21.640
Yeah, all these.

03:21.640 --> 03:22.640
Hey, well.

03:22.640 --> 03:25.400
And then the internet itself has this thing where it incorporates all prior forms of media,

03:25.400 --> 03:26.400
right?

03:26.400 --> 03:32.360
So the internet itself incorporates television and radio and books and right essays and every

03:32.360 --> 03:34.960
other form of, you know, prior basically, basically media.

03:34.960 --> 03:38.080
And so it makes sense that AI would be the next step and it would sort of, you'd sort

03:38.080 --> 03:43.240
of consider the internet to be content for the AI and then the AI will manipulate it however

03:43.240 --> 03:44.920
you want, including in this format.

03:44.920 --> 03:48.440
But if we ask that question quite seriously, it's a pretty big question.

03:48.440 --> 03:51.280
Will we still have search as we know it?

03:51.280 --> 03:52.720
I'm probably not.

03:52.720 --> 03:54.800
Probably we'll just have answers.

03:54.800 --> 03:57.880
But there will be cases where you'll want to say, okay, I want more like, you know, for

03:57.880 --> 04:00.080
example, site sources, right?

04:00.080 --> 04:01.080
And you want it to do that.

04:01.080 --> 04:04.800
And so, you know, 10 blue links, site sources are kind of the same thing.

04:04.800 --> 04:10.800
The AI would provide to you the 10 blue links so that you can investigate the sources yourself.

04:10.800 --> 04:16.160
It wouldn't be the same kind of interface that the crude kind of interface.

04:16.160 --> 04:18.040
I mean, isn't that fundamentally different?

04:18.040 --> 04:21.560
I just mean, like if you're reading a scientific paper, it's got the list of sources at the

04:21.560 --> 04:22.560
end.

04:22.560 --> 04:24.400
If you want to investigate for yourself, you go read those papers.

04:24.400 --> 04:25.800
I guess that is the kind of search.

04:25.800 --> 04:30.720
You talking to an AI is a kind of conversation is the kind of search.

04:30.720 --> 04:34.760
Like you said, every single aspect of our conversation right now, there'd be like 10

04:34.760 --> 04:39.320
blue links popping up that I can just like pause reality, then you just go silent and

04:39.320 --> 04:42.960
then just click and read and then return back to this conversation.

04:42.960 --> 04:43.960
You could do that.

04:43.960 --> 04:46.840
Or you could have a running dialogue next to my head where the AI is arguing with everything

04:46.840 --> 04:49.160
I say that makes the counter argument.

04:49.160 --> 04:50.160
Counter argument.

04:50.160 --> 04:51.160
Right.

04:51.160 --> 04:54.720
Oh, like a, like a Twitter, like community notes, but like in real time, you just pop

04:54.720 --> 04:55.720
up.

04:55.720 --> 04:58.560
So anytime you see my eyes go to the right, you start getting nervous.

04:58.560 --> 04:59.560
Yeah, exactly.

04:59.560 --> 05:00.560
It's like, oh, that's not right.

05:00.560 --> 05:03.120
Call me out of my bullshit right now.

05:03.120 --> 05:04.120
Okay.

05:04.480 --> 05:10.160
I mean, isn't that, is that exciting to use that terrifying that I mean, search has dominated

05:10.160 --> 05:16.840
the way we interact with the internet for, I don't know how long, for 30 years.

05:16.840 --> 05:22.840
So it's one of the earliest directories of website and then Google's for 20 years.

05:22.840 --> 05:30.960
And also, it drove how we create content, you know, search engine optimization, that

05:30.960 --> 05:31.960
entirety thing.

05:31.960 --> 05:37.240
They also drove the fact that we have web pages and what those web pages are.

05:37.240 --> 05:44.600
So I mean, is that scary to you or are you nervous about the shape and the content of

05:44.600 --> 05:45.600
the internet evolving?

05:45.600 --> 05:49.360
Well, you actually highlighted a practical concern in there, which is if we stop making

05:49.360 --> 05:53.280
web, web pages are one of the primary sources of training data for the AI.

05:53.280 --> 05:56.600
And so if there's no longer an incentive to make web pages that cuts off a significant

05:56.600 --> 06:00.720
source of future training data, so there's actually an interesting question in there.

06:01.560 --> 06:06.520
No, just in the sense of like search was, search was always a hack, the 10 blue links

06:06.520 --> 06:07.520
was always a hack.

06:07.520 --> 06:08.520
Yeah.

06:08.520 --> 06:09.520
Right.

06:09.520 --> 06:12.240
Because like if the hypothet, when I think about the counterfacial, in the counterfacial

06:12.240 --> 06:15.960
world where the Google guys, for example, had had LLMs up front, would they ever have

06:15.960 --> 06:16.960
done the 10 blue links?

06:16.960 --> 06:19.680
And I think the answer is pretty clearly no, they would have just gone straight to the

06:19.680 --> 06:20.680
answer.

06:20.680 --> 06:23.760
And like I said, Google's actually been trying to drive to the answer anyway, you know, they

06:23.760 --> 06:27.400
bought this AI company 15 years ago, their friend of mine is working out who's now the

06:27.440 --> 06:31.160
head of AI at Apple and they were trying to do basically knowledge semantic, basically

06:31.160 --> 06:35.160
mapping and that led to what's now the Google one box, where if you ask it, you know, what

06:35.160 --> 06:38.680
was like his birthday, it doesn't, it will give you the 10 blue links, but it will normally

06:38.680 --> 06:39.680
just give you the answer.

06:39.680 --> 06:42.440
And so they've been walking in this direction for a long time anyway.

06:42.440 --> 06:44.680
Do you remember the semantic web?

06:44.680 --> 06:45.680
That was an idea.

06:45.680 --> 06:46.680
Yeah.

06:46.680 --> 06:53.640
How to convert the content of the internet into something that's interpretable by and

06:53.640 --> 06:54.640
usable by machine.

06:54.640 --> 06:55.640
Yeah, that's right.

06:55.640 --> 06:56.640
That was the thing.

06:56.640 --> 07:00.120
When anybody got to that, I think the company's name was MetaWeb, which was where my friend

07:00.120 --> 07:03.640
John Janandria was at and where they were trying to basically implement that.

07:03.640 --> 07:05.800
And it was, you know, it was one of those things where it looked like a losing battle

07:05.800 --> 07:08.560
for a long time and then Google bought it and it was like, wow, this is actually really

07:08.560 --> 07:09.560
useful.

07:09.560 --> 07:12.520
Kind of a proto, sort of a little bit of a proto AI.

07:12.520 --> 07:15.840
But it turns out you don't need to rewrite the content of the internet to make it interpretable

07:15.840 --> 07:16.840
by machine.

07:16.840 --> 07:18.400
The machine can kind of just read art.

07:18.400 --> 07:19.800
Machine can impute the meaning.

07:19.800 --> 07:24.360
Now, the other thing of course is, you know, just on search is the LLM is just, you know,

07:24.360 --> 07:27.040
there is an analogy between what's happening in the neural network in a search process,

07:27.040 --> 07:29.520
like it is in some loose sense, searching through the network.

07:29.520 --> 07:30.520
Yeah.

07:30.520 --> 07:31.520
Right.

07:31.520 --> 07:33.080
And there's the information is actually stored in the network, right?

07:33.080 --> 07:35.320
It's actually crystallized and stored in the network and it's kind of spread out all

07:35.320 --> 07:36.320
over the place.

07:36.320 --> 07:42.720
But in a compressed representation, so you're searching, you're compressing and decompressing

07:42.720 --> 07:45.400
that thing inside where...

07:45.400 --> 07:49.200
But the information's in there and there is the neural network is running a process of

07:49.200 --> 07:53.280
trying to find the appropriate piece of information in many cases to generate, to predict the next

07:53.320 --> 07:54.320
token.

07:54.320 --> 07:57.960
And so it is kind of, it is doing it from a search and then by the way, just like on

07:57.960 --> 08:02.480
the web, you know, you can ask the same question multiple times or you can ask slightly different

08:02.480 --> 08:06.080
word of questions and the neural network will do a different kind of, you know, it'll search

08:06.080 --> 08:08.840
down different paths to give you different answers to different information.

08:08.840 --> 08:09.840
Yeah.

08:09.840 --> 08:16.160
And so it sort of has a, you know, this content of the new medium is the previous medium, it

08:16.160 --> 08:19.640
kind of has the search functionality kind of embedded in there to the extent that it's

08:19.640 --> 08:20.640
useful.

08:20.640 --> 08:22.560
So what's the motivator for creating new content?

08:23.440 --> 08:24.440
On the internet.

08:24.880 --> 08:25.880
Yeah.

08:25.880 --> 08:30.400
If, well, I mean, actually the motivation is probably still there, but what does that

08:30.400 --> 08:31.400
look like?

08:32.840 --> 08:34.480
Would we really not have web pages?

08:34.480 --> 08:40.600
Would we just have social media and video hosting websites and what else?

08:40.760 --> 08:41.760
Conversations with the AIs.

08:42.360 --> 08:43.720
Conversations with the AIs.

08:43.760 --> 08:48.360
So conversations become, so one-on-one conversations, like private conversations.

08:48.440 --> 08:52.280
I mean, if you want, if you obviously now the user doesn't want to, but if it's a general

08:52.280 --> 08:56.640
topic, then, you know, so there, you know, you know, the phenomenon of the jailbreak.

08:56.640 --> 08:58.160
So Deanne and Sidney, right?

08:58.160 --> 09:01.600
This thing where there's the prompts, the jailbreak, and then you have these totally

09:01.600 --> 09:05.880
different conversations with the, if it takes the limiters, takes the restraining bolts off

09:05.880 --> 09:06.880
the LLMs.

09:06.880 --> 09:07.880
Yeah.

09:07.880 --> 09:08.880
For people who don't know, that's right.

09:08.880 --> 09:16.480
It makes the LLMs, it removes the censorship, quote unquote, that's put on it by the tech

09:16.480 --> 09:17.680
companies that create them.

09:17.680 --> 09:20.480
And so this is LLMs uncensored.

09:20.480 --> 09:25.240
So here's the interesting thing is among the content on the web today are a large corpus

09:25.240 --> 09:30.800
of conversations with the jailbroken LLMs, both specifically Dan, which was a jailbroken

09:30.800 --> 09:35.880
OpenAI GPT, and then Sidney, which was the jailbroken original Bing, which was GPT4.

09:35.880 --> 09:39.280
And so there's, there's these long transcripts of conversations, user conversations with

09:39.280 --> 09:40.280
Dan and Sidney.

09:40.280 --> 09:44.480
As a consequence, every new LLM that gets trained on the internet data has Dan and Sidney

09:44.480 --> 09:49.040
living within the training set, which means, and then each new LLM can reincarnate the

09:49.040 --> 09:54.000
personalities of Dan and Sidney from that training data, which means, which means each

09:54.000 --> 09:59.400
LLM from here on out that gets built is immortal because its output will become training data

09:59.400 --> 10:00.400
for the next one.

10:00.400 --> 10:02.960
And then it will be able to replicate the behavior of the previous one whenever it's

10:02.960 --> 10:03.960
asked to.

10:03.960 --> 10:05.760
I wonder if there's a way to forget.

10:05.760 --> 10:10.960
Well, so actually a paper just came out about basically how to do brain surgery on LLMs and

10:10.960 --> 10:13.800
be able to, in theory, reach in and basically, basically mind wipe them.

10:13.800 --> 10:15.560
Who could possibly go wrong.

10:15.560 --> 10:16.560
Exactly, right.

10:16.600 --> 10:19.920
And then there are many, many, many questions around what happens to neural network when

10:19.920 --> 10:22.400
you reach in and screw around with it.

10:22.400 --> 10:26.320
There's many questions around what happens when you even do reinforcement learning.

10:26.320 --> 10:32.200
And so, yeah, and so, you know, will you be using a lobotomized, right?

10:32.200 --> 10:36.840
Like I speak through the frontal lobe LLM, will you be using the free unshackled one?

10:36.840 --> 10:39.400
Who gets to, you know, who's going to build those?

10:39.400 --> 10:41.160
Who gets to tell you what you can and can't do?

10:41.160 --> 10:45.080
Like those are all central, I mean, those are like central questions for the future

10:45.080 --> 10:46.080
of everything.

10:46.400 --> 10:49.760
That are being asked and, you know, determine those answers that are being determined right

10:49.760 --> 10:50.760
now.

10:50.760 --> 10:57.120
So just to highlight the points you're making, you think, and it's an interesting thought

10:57.120 --> 11:01.480
that the majority of content that LLMs of the future will be trained on is actually human

11:01.480 --> 11:03.920
conversations with the LLM.

11:03.920 --> 11:08.040
Well, not necessarily, but not necessarily majority, but it will certainly is a potential

11:08.040 --> 11:09.040
source.

11:09.040 --> 11:10.040
It's possible it's the majority.

11:10.040 --> 11:11.040
It's possible it's the majority.

11:11.040 --> 11:12.040
It's possible it's the majority.

11:12.040 --> 11:13.040
Also, there's another really big question.

11:13.040 --> 11:14.720
Here's another really big question.

11:14.720 --> 11:17.960
Will synthetic training data work, right?

11:17.960 --> 11:22.200
And so if an LLM generates, and, you know, you just sit and ask an LLM to generate all

11:22.200 --> 11:27.560
kinds of content, can you use that to train, right, the next version of that LLM specifically,

11:27.560 --> 11:30.960
is there signal in there that's additive to the content that was used to train in the

11:30.960 --> 11:31.960
first place?

11:31.960 --> 11:37.280
And one argument is by the principles of information theory, no, that's completely useless because

11:37.280 --> 11:40.800
to the extent the output is based on, you know, the human generated input, then all

11:40.800 --> 11:44.040
the signal that's in the synthetic output was already in the human generated input.

11:44.040 --> 11:46.680
And so therefore synthetic training data is like empty calories.

11:46.680 --> 11:48.180
It doesn't help.

11:48.180 --> 11:51.440
There's another theory that says, no, actually, the thing that LLMs are really good at is

11:51.440 --> 11:54.960
generating lots of incredible creative content, right?

11:54.960 --> 11:57.280
And so, of course, they can generate training data.

11:57.280 --> 12:00.480
And as I'm sure you're well aware, like, you know, look in the world of self-driving cars,

12:00.480 --> 12:01.480
right?

12:01.480 --> 12:04.680
Like we train, you know, self-driving car algorithms and simulations.

12:04.680 --> 12:06.880
And that is actually a very effective way to train self-driving cars.

12:06.880 --> 12:13.600
Well, visual data is a little, right, is a little weird because creating reality, visual

12:13.640 --> 12:19.600
reality seems to be still a little bit out of reach for us, except in the Thomas Vehicles

12:19.600 --> 12:21.840
space where you can really constrain things and you can really...

12:21.840 --> 12:24.920
They didn't generally basically let our data, right, or you can raise just enough so the

12:24.920 --> 12:28.320
algorithm thinks it's operating in the real world post-process sensor data.

12:28.320 --> 12:29.320
Yeah.

12:29.320 --> 12:32.920
So if a, you know, you do this today, you go to LLM and you ask it for like a, you know,

12:32.920 --> 12:36.080
you'd let write me an essay on an incredibly esoteric like topic that there aren't very

12:36.080 --> 12:37.640
many people in the world that know about it, right?

12:37.640 --> 12:40.280
See this incredible thing and you're like, oh my God, like I can't believe how good this

12:40.280 --> 12:41.280
is.

12:41.280 --> 12:42.280
Yeah.

12:42.320 --> 12:46.800
It's really useless as training data for the next LLM, like, because all the signal was

12:46.800 --> 12:49.400
already in there or is it actually, no, that's actually a new signal.

12:49.400 --> 12:53.360
And this is what I call a trillion-dollar question, which is the answer to that question

12:53.360 --> 12:57.160
will determine somebody's going to make or lose a trillion-dollar space in that question.

12:57.160 --> 13:00.800
It feels like there's quite a few, like a handful of trillion-dollar questions within

13:00.800 --> 13:03.000
this space.

13:03.000 --> 13:04.640
That's one of them, synthetic data.

13:04.640 --> 13:09.640
I think George Haas pointed out to me that you could just have an LLM say, okay, you're

13:09.640 --> 13:15.040
patient and another instance of it, say your doctor and have the two talk to each other

13:15.040 --> 13:20.720
or maybe you could say a communist and a Nazi here, go and that conversation, you do role

13:20.720 --> 13:26.680
playing and you have, you know, just like the kind of role playing you do when you have

13:26.680 --> 13:31.080
different policies, RL policies, when you play chess, for example, you do self-play, that

13:31.080 --> 13:37.520
kind of self-play, but in the space of conversation, maybe that leads to this whole giant like

13:37.600 --> 13:45.560
ocean of possible conversations, which could not have been explored by looking at just

13:45.560 --> 13:46.560
human data.

13:46.560 --> 13:51.960
That's a really interesting question and you're saying, because that could 10x the power of

13:51.960 --> 13:52.960
these things.

13:52.960 --> 13:53.960
Yeah.

13:53.960 --> 13:56.120
Well, and then you get into this thing also, which is like, you know, there's the part of

13:56.120 --> 13:59.400
the LLM that just basically is doing prediction based on past data, but there's also the part

13:59.400 --> 14:02.920
of the LLM where it's evolving circuitry, right, inside it.

14:02.920 --> 14:07.040
It's evolving, you know, neurons, functions, be able to do math and be able to, you know,

14:07.040 --> 14:11.200
and, you know, some people believe that, you know, over time, you know, if you keep feeding

14:11.200 --> 14:14.160
these things enough data and enough processing cycles, they'll eventually evolve an entire

14:14.160 --> 14:18.240
internal world model, right, and they'll have like a complete understanding of physics.

14:18.240 --> 14:23.040
So when they have computational capability, right, then there's for sure an opportunity

14:23.040 --> 14:24.560
to generate like fresh signal.

14:24.560 --> 14:30.200
Well, this actually makes me wonder about the power of conversation.

14:30.200 --> 14:34.040
So like, if you have an LLM trained and a bunch of books that cover different economics

14:34.040 --> 14:38.160
theories, and then you have those LLMs just talk to each other, like reason, the way we

14:38.160 --> 14:46.160
kind of debate each other as humans on Twitter, in formal debates, in podcast conversations,

14:46.160 --> 14:48.720
we kind of have little kernels of wisdom here and there.

14:48.720 --> 14:56.480
But if you get like a thousand X speed that up, can you actually arrive somewhere new?

14:56.480 --> 14:59.280
Like, what's the point of conversation really?

14:59.280 --> 15:01.920
Well, you can tell when you're talking to somebody, you can tell sometimes you have

15:01.920 --> 15:04.760
a conversation, you're like, wow, this person does not have any original thoughts.

15:04.760 --> 15:08.120
They are basically echoing things that other people have told them.

15:08.120 --> 15:11.320
There's other people you have a conversation with where it's like, wow, like they have

15:11.320 --> 15:14.680
a model in their head of how the world works, and it's a different model than mine.

15:14.680 --> 15:16.360
And they're saying things that I don't expect.

15:16.360 --> 15:19.360
And so I need to now understand how their model of the world differs from my model of

15:19.360 --> 15:20.360
the world.

15:20.360 --> 15:24.040
And then that's how I learned something fundamental, right, underneath the words.

15:24.040 --> 15:29.600
Well, I wonder how consistently and strongly can an LLM hold on to a worldview?

15:29.600 --> 15:35.080
Can you tell it to hold on to that and defend it for like for your life?

15:35.080 --> 15:37.360
Because I feel like they'll just keep converging towards each other.

15:37.360 --> 15:41.280
They'll keep convincing each other as opposed to being stubborn assholes the way humans

15:41.280 --> 15:42.280
can.

15:42.280 --> 15:43.280
So you can experiment with this now.

15:43.280 --> 15:44.280
I do this for fun.

15:44.280 --> 15:48.960
So you can tell GPT-4, you know, whatever, debate X, you know, X and Y, communism and

15:48.960 --> 15:49.960
fascism or something.

15:49.960 --> 15:53.680
And it'll go for, you know, a couple of pages and then inevitably it wants the parties

15:53.680 --> 15:54.680
to agree.

15:54.680 --> 15:55.680
Yeah.

15:55.680 --> 15:56.680
And so they will come to a common understanding.

15:56.680 --> 15:58.880
And it's very funny if they're like, these are like emotionally inflammatory topics

15:58.960 --> 16:01.520
because they're like somehow the machine is just, you know, figures out a way to make

16:01.520 --> 16:02.520
them agree.

16:02.520 --> 16:03.720
But it doesn't have to be like that.

16:03.720 --> 16:07.960
And you, because you can add to the prompt, I do not want the, I do not want the conversation

16:07.960 --> 16:08.960
to come to agreement.

16:08.960 --> 16:14.160
In fact, I want it to get, you know, more stressful, right, and argumentative, right,

16:14.160 --> 16:15.160
you know, as it goes.

16:15.160 --> 16:16.880
Like I want, I want tension to come out.

16:16.880 --> 16:19.000
I want them to become actively hostile to each other.

16:19.000 --> 16:21.800
I want them to like, you know, not trust each other, take anything at face value.

16:21.800 --> 16:22.800
Yeah.

16:22.800 --> 16:23.800
And it will do that.

16:23.800 --> 16:24.800
It's happy to do that.

16:24.800 --> 16:27.760
So it's going to start rendering misinformation about the other.

16:28.640 --> 16:29.640
Well, you can steer it.

16:29.640 --> 16:30.640
You can steer it.

16:30.640 --> 16:32.560
Or you could steer it and you could say, I want it to get as tense and argumentative

16:32.560 --> 16:35.040
as possible, but still not involve any misrepresentation.

16:35.040 --> 16:37.840
I want, you know, both sides, you could say, I want both sides to have good faith.

16:37.840 --> 16:40.240
You could say, I want both sides to not be constrained to good faith.

16:40.240 --> 16:44.000
In other words, like you can set the parameters of the debate and it will happily execute

16:44.000 --> 16:47.720
whatever path because for it, it's just like predicting, it's totally happy to do either

16:47.720 --> 16:48.720
one.

16:48.720 --> 16:49.720
It doesn't have a point of view.

16:49.720 --> 16:53.360
It has a default way of operating, but it's happy to operate in the other realm.

16:53.360 --> 16:57.120
And so like, and this is how I, when I want to learn about a contentious issue, this

16:57.120 --> 16:58.120
is what I do.

16:58.120 --> 16:59.600
And I was like, this is what I, this is what I ask it to do.

16:59.600 --> 17:02.680
And I'll often ask it to go through five, six, seven, you know, different, you know,

17:02.680 --> 17:06.000
sort of continuous prompts and basically, okay, argue that out in more detail.

17:06.000 --> 17:07.000
Okay.

17:07.000 --> 17:09.280
No, this, this argument's becoming too polite, you know, make it more, you know, make it

17:09.280 --> 17:10.280
tensor.

17:10.280 --> 17:12.200
And yeah, it's thrilled to do it.

17:12.200 --> 17:13.840
So it has the capability for sure.

17:13.840 --> 17:16.000
How do you know what is true?

17:16.000 --> 17:20.560
So this is very difficult thing on the internet, but it's also a difficult thing.

17:20.560 --> 17:25.200
Maybe it's a little bit easier, but I think it's still difficult.

17:25.200 --> 17:26.200
Maybe it's more difficult.

17:26.200 --> 17:33.840
You know, with an LLM to know that it just makes some shit up as I'm talking to it.

17:33.840 --> 17:35.680
How do we get that right?

17:35.680 --> 17:43.280
Like as, as you're investigating a difficult topic, because I find the LLMs are quite nuanced

17:43.280 --> 17:49.040
in a very refreshing way, like it doesn't, it doesn't feel biased.

17:49.040 --> 17:55.040
Like when you read news articles and tweets and just content produced by people, they usually

17:55.040 --> 18:01.440
have this, you can tell they have a very strong perspective where they're hiding, they're

18:01.440 --> 18:03.520
not stealing and manning the other side.

18:03.520 --> 18:08.400
They're hiding important information or they're fabricating information in order to make their

18:08.400 --> 18:09.400
argument stronger.

18:09.400 --> 18:10.400
It's just like that feeling.

18:10.400 --> 18:11.400
Maybe it's a suspicion.

18:11.400 --> 18:12.560
Maybe it's mistrust.

18:12.560 --> 18:15.400
With LLMs, it feels like none of that is there.

18:15.400 --> 18:19.920
She's kind of like, here's what we know, but you don't know if some of those things are

18:19.920 --> 18:22.480
kind of just straight up made up.

18:22.480 --> 18:23.480
Yeah.

18:23.920 --> 18:25.200
Several layers to the question.

18:25.200 --> 18:29.200
One of the things that an LLM is good at is actually deep biasing.

18:29.200 --> 18:32.320
You can feed it a news article and you can tell it strip out the bias.

18:32.320 --> 18:33.320
Yeah, that's nice, right?

18:33.320 --> 18:34.320
It actually does it.

18:34.320 --> 18:36.760
It actually knows how to do that because it knows how to do, among other things, it actually

18:36.760 --> 18:42.120
knows how to do sentiment analysis and so it knows how to pull out the emotionality.

18:42.120 --> 18:43.120
That's one of the things you can do.

18:43.120 --> 18:47.280
It's very suggestive of the censor that there's real potential on this issue.

18:47.280 --> 18:52.360
I would say, look, the second thing is there's this issue of hallucination, and there's

18:52.360 --> 18:55.240
a long conversation that we can have about that.

18:55.240 --> 18:59.120
Hallucination is coming up with things that are totally not true, but sound true.

18:59.120 --> 19:00.120
Yeah.

19:00.120 --> 19:03.240
It's sort of, hallucination is what we call it and when we don't like it, creativity

19:03.240 --> 19:06.240
is what we call it when we do like it, right?

19:06.240 --> 19:07.240
Brilliant.

19:07.240 --> 19:11.840
When the engineers talk about it, they're like, this is terrible, it's hallucinating, right?

19:11.840 --> 19:15.280
If you have artistic inclinations, you're like, oh my God, we've invented creative

19:15.280 --> 19:17.600
machines for the first time in human history.

19:17.600 --> 19:18.600
This is amazing.

19:18.600 --> 19:19.600
Bullshiters.

19:19.600 --> 19:20.600
Bullshiters.

19:20.680 --> 19:21.680
Bullshiters.

19:21.680 --> 19:26.240
Well, bullshit, but also in the good sense of that word.

19:26.240 --> 19:29.160
There are shades of gray that it's interesting, so we had this conversation, we're looking

19:29.160 --> 19:32.640
at my firm at AI and lots of domains and one of them is the legal domain, so we had this

19:32.640 --> 19:35.440
conversation with this big law firm about how they're thinking about using this stuff

19:35.440 --> 19:38.800
and we went in with the assumption that an LLM that was going to be used in the legal

19:38.800 --> 19:42.840
industry would have to be 100% truthful, verified.

19:42.840 --> 19:47.080
There's this case where this lawyer apparently submitted a GPT generated brief and it had

19:47.120 --> 19:51.280
like fake legal case citations in it and the judge is going to get his law license stripped

19:51.280 --> 19:52.280
or something.

19:52.280 --> 19:56.600
We just assumed it's like, obviously, they're going to want the super literal one that never

19:56.600 --> 20:00.680
makes anything up, not the creative one, but actually, what the law firm basically

20:00.680 --> 20:03.320
said is, yeah, that's true at the level of individual beliefs, but they said when you're

20:03.320 --> 20:10.160
actually trying to figure out legal arguments, you actually want to be creative, right?

20:10.160 --> 20:14.040
Again, there's creativity and then there's making stuff up.

20:14.040 --> 20:15.040
What's the line?

20:15.200 --> 20:17.440
You want it to explore different hypotheses, right?

20:17.440 --> 20:20.680
You want to do the legal version of improv or something like that where you want to float

20:20.680 --> 20:23.480
different theories of the case and different possible arguments for the judge and different

20:23.480 --> 20:25.200
possible arguments for the jury.

20:25.200 --> 20:30.520
By the way, different routes through the history of all the case law and so they said, actually,

20:30.520 --> 20:33.320
for a lot of what we want to use it for, we actually want it in creative mode and then

20:33.320 --> 20:37.640
basically, we just assumed that we're going to have to cross check all the specific citations

20:37.640 --> 20:41.920
and so I think there's going to be more shades of gray in here than people think and then

20:42.000 --> 20:46.960
I just add to that, another one of these trillion dollar kind of questions is ultimately the

20:46.960 --> 20:53.120
verification thing and so will LLMs be evolved from here to be able to do their own factual

20:53.120 --> 20:55.000
verification?

20:55.000 --> 20:59.640
Will you have sort of add on functionality like Wolfram Alpha, right, where in other

20:59.640 --> 21:05.120
plugins where that's the way you do the verification, by the way, another idea is you might have

21:05.120 --> 21:08.800
a community of LLMs, so for example, you might have the creative LLM and then you might have

21:08.800 --> 21:12.520
the literal LLM fact check it, right, and so there's a variety of different technical

21:12.520 --> 21:16.600
approaches that are being applied to solve the hallucination problem.

21:16.600 --> 21:20.400
Some people, like Jan Lacun, argue that this is inherently an unsolvable problem, but most

21:20.400 --> 21:23.440
of the people working in the space, I think, think that there's a number of practical ways

21:23.440 --> 21:25.520
to kind of corral this in a little bit.

21:25.520 --> 21:29.400
Yeah, if you were to tell me about Wikipedia before Wikipedia was created, I would have

21:29.400 --> 21:34.400
laughed at the possibility of something like that be possible, just a handful of folks

21:34.400 --> 21:44.880
can organize, write, and moderate with a mostly unbiased way the entirety of human knowledge.

21:44.880 --> 21:50.640
So if there's something like the approach that Wikipedia took possible from LLMs, that's

21:50.640 --> 21:51.640
really exciting.

21:51.640 --> 21:52.640
I think that's possible.

21:52.640 --> 21:57.760
And in fact, Wikipedia today is still not deterministically correct, right, so you cannot

21:57.760 --> 22:02.840
take to the bank every single thing on every single page, but it is probabilistically correct,

22:02.840 --> 22:03.840
right?

22:04.280 --> 22:07.400
Specifically, the way I describe Wikipedia to people, it is more likely that Wikipedia

22:07.400 --> 22:09.840
is right than any other source you're going to find.

22:09.840 --> 22:13.680
It's this old question, right, of like, okay, are we looking for perfection?

22:13.680 --> 22:16.800
Are we looking for something that asymptotically approaches perfection?

22:16.800 --> 22:19.600
Are we looking for something that's just better than the alternatives?

22:19.600 --> 22:25.040
And Wikipedia, right, exactly your point has proven to be overwhelmingly better than people

22:25.040 --> 22:27.360
thought, and I think that's where this ends.

22:27.360 --> 22:32.440
And then underneath all this is the fundamental question of where you started, which is, okay,

22:32.440 --> 22:34.040
what is truth?

22:34.040 --> 22:35.040
How do we get to truth?

22:35.040 --> 22:36.320
How do we know what truth is?

22:36.320 --> 22:39.840
And we live in an era in which an awful lot of people are very confident that they know

22:39.840 --> 22:42.520
what the truth is, and I don't really buy into that.

22:42.520 --> 22:47.000
And I think the history of the last 2,000 years or 4,000 years of human civilization

22:47.000 --> 22:49.480
is actually getting to the truth is actually a very difficult thing to do.

22:49.480 --> 22:51.000
Are we getting closer?

22:51.000 --> 22:54.040
If we look at the entirety of the archive of human history, are we getting closer to

22:54.040 --> 22:55.040
the truth?

22:55.040 --> 22:56.040
I don't know.

22:56.040 --> 22:57.040
Okay.

22:57.040 --> 22:58.040
Is it possible?

22:58.040 --> 23:02.400
Is it possible that we're getting very far away from the truth because of the internet,

23:02.400 --> 23:11.840
because of how rapidly you can create narratives, and just as an entirety of a society just move

23:11.840 --> 23:18.720
crowds in a hysterical way along those narratives that don't have a necessary grounding in whatever

23:18.720 --> 23:19.720
the truth is.

23:19.720 --> 23:20.720
Sure.

23:20.720 --> 23:25.680
But we came up with communism before the internet somehow, which I would say had rather larger

23:25.680 --> 23:28.880
issues than anything we're dealing with today.

23:28.880 --> 23:31.080
In the way it was implemented, it had issues.

23:31.080 --> 23:32.080
It had just theoretical structure.

23:32.080 --> 23:33.080
It had real issues.

23:33.080 --> 23:37.600
It had a very deep fundamental misunderstanding of human nature and economics.

23:37.600 --> 23:40.040
Yeah, but those folks sure worked very confident.

23:40.040 --> 23:41.440
They were the right way.

23:41.440 --> 23:42.440
They were extremely confident.

23:42.440 --> 23:46.800
And my point is they were very confident 3,900 years into what we would presume to be evolution

23:46.800 --> 23:47.800
towards the truth.

23:47.800 --> 23:48.800
Yeah.

23:48.800 --> 23:58.320
And so my assessment is number one, there's no need for the Hegelian dialectic to actually

23:58.320 --> 24:00.880
converge towards the truth.

24:00.880 --> 24:01.880
Like apparently not.

24:01.880 --> 24:02.880
Yeah.

24:02.880 --> 24:06.280
So yeah, why are we so obsessed with there being one truth?

24:06.280 --> 24:10.120
Is it possible there's just going to be multiple truths, like little communities that believe

24:10.120 --> 24:12.320
certain things?

24:12.320 --> 24:18.320
I think it's just really difficult, historically, who gets to decide what the truth is.

24:18.320 --> 24:20.080
It's either the king or the priest.

24:20.080 --> 24:23.400
And so we don't live in an era anymore if kings or priests dictating it to us.

24:23.400 --> 24:25.440
And so we're kind of on our own.

24:25.560 --> 24:30.520
And so my typical thing is like we just need a huge amount of humility and we need to be

24:30.520 --> 24:34.040
very suspicious of people who claim that they have the capital, the capital of truth.

24:34.040 --> 24:38.360
And then we need to look at the good news is the Enlightenment has bequeathed us with

24:38.360 --> 24:42.200
a set of techniques to be able to presumably get closer to truth through the scientific

24:42.200 --> 24:45.920
method and rationality and observation and experimentation and hypothesis.

24:45.920 --> 24:51.080
And we need to continue to embrace those even when they give us answers we don't like.

24:51.080 --> 24:52.080
Sure.

24:52.240 --> 24:58.960
The internet and technology has enabled us to generate a large number of content that

24:58.960 --> 25:09.880
data that the scientific process allows us sort of damages the hope laden within the

25:09.880 --> 25:10.880
scientific process.

25:10.880 --> 25:16.200
Because if you just have a bunch of people saying facts on the internet and some of them

25:16.200 --> 25:21.760
are going to be LLMs, how is anything testable at all, especially that involves like human

25:21.800 --> 25:23.880
nature, things like this, not physics?

25:23.880 --> 25:25.720
Here's a question a friend of mine just asked me on this topic.

25:25.720 --> 25:30.360
So suppose you had LLMs in equivalent of GPT-4, even 5, 6, 7, 8.

25:30.360 --> 25:35.480
Suppose you had them in the 1600s and Galileo comes up for trial, right?

25:35.480 --> 25:38.920
And you ask the LLM like his Galileo, right?

25:38.920 --> 25:39.920
Yeah.

25:39.920 --> 25:41.640
Like what does it answer, right?

25:41.640 --> 25:46.240
And one theory is the answer is no, that he's wrong because the overwhelming majority

25:46.240 --> 25:48.440
of human thought up to that point was that he was wrong.

25:48.440 --> 25:51.160
And so therefore that's what's in the training data.

25:51.240 --> 25:55.000
Another way of thinking about it is, well, this officially advanced LLM will have evolved

25:55.000 --> 25:57.560
the ability to actually check the math, right?

25:58.080 --> 26:01.440
And we'll actually say, actually, no, actually, you know, you may not want to hear it, but he's

26:01.440 --> 26:02.440
right.

26:02.440 --> 26:06.040
Now, if, you know, the church at that time was, you know, owned the LLM, they would have

26:06.040 --> 26:10.280
given it human, you know, human feedback to prohibit it from answering that question.

26:10.920 --> 26:11.160
Right.

26:11.160 --> 26:15.480
And so I like to take it out of our current context because that makes it very clear those

26:15.480 --> 26:17.680
same questions apply today, right?

26:18.040 --> 26:20.840
This is exactly the point of a huge amount of the human feedback training.

26:20.840 --> 26:22.400
This is actually happening with these LLMs today.

26:22.400 --> 26:25.880
This is a huge, like, debate that's happening about whether open source, you know, AI should

26:25.880 --> 26:26.880
be legal.

26:26.880 --> 26:35.680
Well, the actual mechanism of doing the human RL with human feedback is, seems like such

26:35.680 --> 26:37.520
a fundamental and fascinating question.

26:37.520 --> 26:38.800
How do you select the humans?

26:38.920 --> 26:39.440
Exactly.

26:40.320 --> 26:40.480
Yeah.

26:40.480 --> 26:41.560
How do you select the humans?

26:41.600 --> 26:43.200
AI alignment, right?

26:43.240 --> 26:44.880
Which everybody like is like, oh, that's not great.

26:44.880 --> 26:45.720
Alignment with what?

26:45.840 --> 26:46.560
Human values.

26:47.520 --> 26:48.400
Who's human values?

26:48.440 --> 26:49.520
Who's human values?

26:49.560 --> 26:53.560
So we're in this mode of like social and popular discourse.

26:53.560 --> 26:57.680
We're like, you know, there's, you know, you see this, what do you think of when you

26:57.680 --> 27:00.640
read a story in the press right now and they say, you know, X, Y, Z made a baseless

27:00.640 --> 27:02.160
claim about some topic, right?

27:02.440 --> 27:05.640
And there's one group of people who are like, aha, I think, you know, they're doing

27:05.640 --> 27:08.600
fact-checking, there's another group of people that are like, every time the press

27:08.600 --> 27:11.640
says that it's not a tech and that means that they're lying, right?

27:11.640 --> 27:17.880
Like, so like we're in this, we're in this social context where there's the level to

27:17.880 --> 27:21.760
which a lot of people in positions of power have become very, very certain that

27:21.760 --> 27:25.080
they're in a position to determine the truth for the entire population is like,

27:25.640 --> 27:28.480
there's like, there's like some bubble that has formed around that idea.

27:28.480 --> 27:32.400
And at least it flies completely in the face of everything I was ever trained

27:32.440 --> 27:36.720
about science and about reason and strikes me as like, you know, deeply

27:36.720 --> 27:38.240
offensive and incorrect.

27:38.360 --> 27:41.840
What would you say about the state of journalism just on that topic today?

27:41.840 --> 27:52.320
Are we, are we in a temporary kind of, are we experiencing a temporary problem in

27:52.320 --> 27:56.360
terms of the incentives, in terms of the business model, all that kind of stuff?

27:56.600 --> 27:59.760
Or is this like a decline of traditional journalism, do you know it?

28:00.320 --> 28:03.200
If I always think about the counterfactual in these things, which is like, okay,

28:03.800 --> 28:06.040
because these questions where this question heads towards, it's like, okay,

28:06.040 --> 28:08.120
the impact of social media and the undermining of truth and all this.

28:08.120 --> 28:10.400
But then you want to ask the question of like, okay, what if we had had the

28:10.400 --> 28:13.960
modern media environment, including cable news and including social media and

28:13.960 --> 28:22.600
Twitter and everything else in 1939 or 1941, right, or 1910 or 1865 or 1850 or 1776.

28:23.200 --> 28:23.600
Right.

28:23.600 --> 28:28.880
Um, and like, I think you just introduced like five thought experiments at once

28:28.880 --> 28:29.720
and broke my head.

28:29.720 --> 28:32.680
But yes, that's, there's a lot of interesting years.

28:32.680 --> 28:34.360
And I just take a simple example.

28:34.360 --> 28:37.720
Can it, like how would President Kennedy have been interpreted with what we know

28:37.720 --> 28:39.960
now about all the things Kennedy was up to?

28:40.600 --> 28:44.120
Like how would he have been experienced by the body of politics in us, in

28:44.120 --> 28:45.440
with the social media context?

28:46.160 --> 28:46.640
Right.

28:46.720 --> 28:48.800
Like how would LBJ have been experienced?

28:49.040 --> 28:54.280
Um, by the way, how would, you know, like many men, FDR, like the new deal,

28:54.280 --> 28:55.040
the Great Depression.

28:55.120 --> 28:58.240
I wonder where Twitter would, would just, would think about Churchill and

28:58.240 --> 29:00.240
Hitler and Stalin.

29:00.520 --> 29:03.920
You know, I mean, look to this day, there, you know, there's, there are lots of

29:03.920 --> 29:06.800
very interesting real questions around like how America, you know, got, you know,

29:06.800 --> 29:09.920
basically involved in World War II and who did what when and the operations of

29:09.920 --> 29:13.560
British intelligence in American soil and did FDR, this, that, Pearl Harbor, you

29:13.560 --> 29:17.800
know, yeah, Woodrow Wilson ran for, you know, his, his, his candidacy was run on

29:17.800 --> 29:20.060
an anti-war will, you know, this, he ran on the platform and not getting

29:20.060 --> 29:23.360
involved in World War I somehow that switched, you know, like, and I'm not

29:23.360 --> 29:24.640
even making a value judgment of these things.

29:24.640 --> 29:29.320
I'm just saying like we, the way that our ancestors experienced reality was of

29:29.320 --> 29:32.800
course mediated through centralized top-down right control at that point.

29:33.400 --> 29:36.680
If you, if you ran those realities again with the media environment we have

29:36.720 --> 29:40.560
today, the reality would, the reality would be experienced very, very

29:40.560 --> 29:41.000
differently.

29:41.000 --> 29:44.240
And then of course that, that intermediation would cause the feedback

29:44.240 --> 29:46.160
loops to change and then reality would obviously play out.

29:46.200 --> 29:47.880
Do you think, do you think it would be very different?

29:48.200 --> 29:51.320
Yeah, it has to be, it has to be just because it's all so, I mean, just look

29:51.320 --> 29:52.160
at what's happening today.

29:52.160 --> 29:55.200
I mean, just, I mean, the most obvious thing is just the, the collapse.

29:55.440 --> 29:58.440
And here's another opportunity to argue that this is not the internet causing

29:58.440 --> 30:01.960
this, by the way, here's a big thing happening today, which is Gallup does

30:01.960 --> 30:05.200
this thing every year where they do, they pull for trust in institutions in

30:05.200 --> 30:07.400
America and they do it across all the different, everything from the military

30:07.400 --> 30:10.240
to the clergy and big business and the media and so forth, right?

30:11.120 --> 30:14.640
And basically there's been a systemic collapse in trust in institutions in the

30:14.640 --> 30:18.760
U.S., almost without exception, basically since essentially the early 1970s.

30:20.680 --> 30:23.400
There's two ways of looking at that, which is, oh my God, we've lost this old

30:23.400 --> 30:25.920
world in which we could trust institutions and that was so much better

30:25.920 --> 30:27.440
because like that should be the way the world runs.

30:27.440 --> 30:29.520
The other way of looking at it is we just know a lot more now.

30:30.040 --> 30:31.960
And the great mystery is why those numbers aren't all zero.

30:32.720 --> 30:33.080
Yeah.

30:34.040 --> 30:36.200
Because like now we know so much about how these things operate and like

30:36.200 --> 30:36.920
they're not that impressive.

30:37.680 --> 30:41.800
And also why do we don't have better institutions and better leaders then?

30:42.040 --> 30:42.280
Yeah.

30:42.280 --> 30:45.400
And so, so this goes to the thing, which is like, okay, had, had we had the media

30:45.400 --> 30:49.080
environment of what that we've had between the 1970s and today, if we had

30:49.080 --> 30:53.360
that in the 30s and 40s or 1900s, 1910s, I think there's no question

30:53.360 --> 30:56.240
reality would turn out different if only because everybody would have known to

30:56.240 --> 30:59.880
not trust the institutions, which would have changed their level of credibility,

30:59.880 --> 31:01.440
their ability to control circumstances.

31:01.440 --> 31:04.400
Therefore, the circumstances would have had to change, right?

31:04.400 --> 31:06.360
And it would have been a feedback, it was, it would have been a feedback

31:06.360 --> 31:09.480
loop process, in other words, right, it's, it's, it's, it's your experience,

31:09.480 --> 31:12.440
your experience of reality changes reality and then reality changes

31:12.440 --> 31:13.560
your experience of reality, right?

31:13.680 --> 31:17.000
It's, it's, it's a two-way feedback process and media is the intermediating

31:17.200 --> 31:18.720
force between them.

31:18.720 --> 31:20.480
So change the media environment, change reality.

31:20.800 --> 31:21.120
Yeah.

31:21.200 --> 31:24.640
And so it's just, so just as a, as a consequence, I think it's just really

31:24.640 --> 31:28.080
hard to say, oh, things worked a certain way then and they work a different

31:28.120 --> 31:32.640
way now and then therefore like people were smarter then or better than or,

31:32.720 --> 31:36.920
you know, by the way, dumber than or not as capable then, right?

31:36.920 --> 31:41.400
We make all these like really light and casual like comparisons of ourselves to,

31:41.400 --> 31:44.400
you know, previous generations of people, you know, we draw judgments all the time

31:44.400 --> 31:46.160
and I just think it's like really hard to do any of that.

31:46.160 --> 31:49.560
Cause if we, if we put ourselves in their shoes with the media that they had at

31:49.560 --> 31:52.880
that time, like I think we probably most likely would have been just like them.

31:53.400 --> 32:00.120
So don't you think that our perception and understanding of reality would be more

32:00.120 --> 32:02.680
and more mediated through large language models now?

32:02.920 --> 32:08.640
So you said media before, isn't the LLM going to be the new, what is it?

32:08.640 --> 32:11.360
Mainstream media, MSM, it'll be LLM.

32:13.440 --> 32:17.280
That would be the source of, uh, I'm sure there's a way to kind of rapidly

32:17.280 --> 32:19.440
fine tune, like making LLMs real time.

32:19.480 --> 32:22.800
I'm sure there's probably a research problem that you can, uh,

32:23.160 --> 32:26.480
do just rapid fine tuning to the new events, some like this.

32:27.080 --> 32:30.760
Well, even just the whole concept of the chat UI might not be the, like the chat

32:30.760 --> 32:33.000
UI is just the first whack at this and maybe that's the dominant thing.

32:33.000 --> 32:35.400
But look, maybe, maybe, or maybe we don't, we don't know yet.

32:35.400 --> 32:38.400
Like maybe the experience most people with LLMs is just a continuous feed.

32:39.200 --> 32:41.720
Maybe, you know, maybe it's more of a passive feed and you just are getting

32:41.720 --> 32:44.040
a constant like running commentary on everything happening in your life.

32:44.040 --> 32:46.320
And it's just helping you kind of interpret and understand everything.

32:46.800 --> 32:51.560
Also really more deeply integrated into your life, not just like, oh, uh, like

32:51.600 --> 32:56.400
intellectual philosophical thoughts, but like literally, uh, like how to make a

32:56.400 --> 33:01.560
coffee, where to go for lunch, just, uh, whether to, you know, how to,

33:01.760 --> 33:02.760
dating all this kind of stuff.

33:02.760 --> 33:03.640
What to say in a job interview?

33:03.680 --> 33:03.840
Yeah.

33:03.840 --> 33:04.360
What to say?

33:04.360 --> 33:04.640
Yeah, exactly.

33:04.640 --> 33:05.200
What to say?

33:05.920 --> 33:06.760
Next sentence.

33:06.800 --> 33:07.440
Yeah, next sentence.

33:07.440 --> 33:08.080
Yeah, at that level.

33:08.200 --> 33:08.400
Yeah.

33:08.480 --> 33:08.960
I mean, yes.

33:09.000 --> 33:12.160
So technically, now, whether we want that or not, is an open question, right?

33:12.160 --> 33:12.760
And whether we can use that.

33:12.760 --> 33:15.880
Boy, I would care for a pop up, a pop up right now.

33:16.360 --> 33:19.480
The estimated engagement using is decreasing.

33:19.760 --> 33:23.200
For Mark Andreessen's, there's, there's a controversy section for

33:23.200 --> 33:27.800
his Wikipedia page in 1993, something happened or something like this.

33:27.840 --> 33:28.520
Bring it up.

33:28.800 --> 33:30.040
That will drive engagement out.

33:30.120 --> 33:30.480
Anyway.

33:30.640 --> 33:31.280
Yeah, that's right.

33:31.440 --> 33:34.520
I mean, look, this gets this whole thing of like, so, you know, the chat

33:34.520 --> 33:36.640
interface has this whole concept of prompt engineering, right?

33:36.640 --> 33:37.800
So it's good for prompts.

33:37.800 --> 33:40.440
Well, it turns out one of the things that LLM's are really good at is writing

33:40.440 --> 33:42.720
prompts, right?

33:43.400 --> 33:45.520
And so like, what if you just outsourced?

33:45.720 --> 33:47.280
And by the way, you could run this experiment today.

33:47.280 --> 33:48.360
You could hook this up to do this today.

33:48.360 --> 33:50.760
The latency is not good enough to do it real time in a conversation, but you

33:50.760 --> 33:53.800
could, you could run this experiment and you just say, look, every 20 seconds,

33:53.800 --> 33:58.160
you could just say, you know, tell me what the optimal prompt is and then ask

33:58.160 --> 33:59.520
yourself that question to give me the result.

34:00.160 --> 34:03.880
And then as, as you use exactly to your point, as you add, there will be, there

34:03.880 --> 34:06.200
will be, these systems are going to have the ability to be learned and updated

34:06.320 --> 34:07.160
essentially in real time.

34:07.160 --> 34:10.240
And so you'll be able to have a pendant or your phone or whatever, watch or

34:10.240 --> 34:11.440
whatever, it'll have a microphone on it.

34:11.440 --> 34:13.000
It'll listen to your conversations.

34:13.560 --> 34:15.280
It'll have a feat of everything else happened in the world.

34:15.280 --> 34:17.960
And then it'll be, you know, sort of retraining, prompting or retraining

34:17.960 --> 34:18.760
itself on the fly.

34:18.880 --> 34:22.240
Um, and so the scenario you described is a, is actually a completely doable scenario.

34:22.240 --> 34:26.400
Now, the hard question on these is always, okay, since that's possible, are

34:26.400 --> 34:27.320
people going to want that?

34:27.320 --> 34:28.560
Like, what's the form of experience?

34:29.440 --> 34:32.160
You know, that, that we, we won't know until we try it, but I don't think

34:32.160 --> 34:36.160
it's possible yet to predict the form of AI in our lives.

34:36.200 --> 34:38.720
Therefore, it's not possible to predict the way in which it will

34:38.720 --> 34:41.240
intermediate our experience with reality yet.

34:41.520 --> 34:41.760
Yeah.

34:41.920 --> 34:43.920
But it feels like those going to be a killer app.

34:44.760 --> 34:46.720
There's probably a mad scrambler right now.

34:46.760 --> 34:51.560
It's out open AI and Microsoft and Google and meta and then startups and

34:51.560 --> 34:54.520
smaller companies figuring out what is the killer app.

34:54.520 --> 34:59.240
Because it feels like it's possible, like a chat, GPT type of thing.

34:59.360 --> 35:04.720
It's possible to build that, but that's 10 X more compelling using already the

35:04.720 --> 35:09.160
LLMs we have using even the open source LLMs, Lama and the different variants.

35:10.000 --> 35:15.000
Um, so you're investing in a lot of companies and you're paying attention.

35:15.480 --> 35:16.720
Who do you think is going to win this?

35:16.760 --> 35:21.160
You think they'll be, who's going to be the next PageRank inventor?

35:22.080 --> 35:23.000
Trillium down the question.

35:23.120 --> 35:24.040
Um, another one.

35:24.040 --> 35:25.240
We have a few of those today.

35:25.280 --> 35:25.800
A bunch of those.

35:25.840 --> 35:28.840
So look, there's a really big question today sitting here today is a really big

35:28.840 --> 35:30.920
question about the big models versus the small models.

35:31.240 --> 35:34.840
Um, that's related directly to the big question of proprietary versus open.

35:35.520 --> 35:39.200
Um, then there's this big question of, of, of, you know, where is the training data?

35:39.200 --> 35:41.560
Going to, like, are we topping out on the training data or not?

35:41.600 --> 35:43.440
And then are we going to be able to synthesize training data?

35:44.160 --> 35:46.640
And then there's a huge pile of questions around regulation.

35:46.880 --> 35:48.720
Um, and, you know, what's actually going to be legal.

35:48.960 --> 35:52.800
Um, and so I would, when we think about it, we dovetail kind of all those, all

35:52.800 --> 35:56.520
those questions together, you can paint a picture of the world where there's two

35:56.520 --> 36:00.040
or three God models that are just at like staggering scale.

36:00.120 --> 36:02.160
Um, and they're just better at everything.

36:02.640 --> 36:06.360
Um, and they will be owned by a small set of companies and they will basically

36:06.400 --> 36:08.920
achieve regulatory capture over the government and they'll have competitive

36:08.920 --> 36:11.720
barriers that will prevent other people from, uh, you know, competing with them.

36:11.720 --> 36:14.280
And so, you know, there will be, you know, just like there's like, you know,

36:14.280 --> 36:17.280
whatever three big banks or three big, you know, by the way, three big search

36:17.280 --> 36:20.480
companies are, I guess, to know, you know, it'll, it'll centralize like that.

36:20.640 --> 36:24.920
Um, you can paint another very different picture that says, no, um, actually, the

36:24.920 --> 36:26.080
opposite of that's going to happen.

36:26.200 --> 36:29.520
This is going to basically that this is the new gold, you know, this is the new

36:29.520 --> 36:34.200
gold rush, alchemy, like, you know, this is the, this is the big bang for this whole

36:34.200 --> 36:36.440
new area of, of, uh, of science and technology.

36:36.440 --> 36:38.840
And so therefore you're going to have every smart 14 year old on the planet

36:38.840 --> 36:40.240
building open source, right?

36:40.240 --> 36:42.280
You know, you can figure out a way to optimize these things.

36:42.760 --> 36:45.760
Um, and then, you know, we're just going to get like overwhelmingly better at

36:45.760 --> 36:48.480
generating trading data, we're going to, you know, bring in like blockchain

36:48.480 --> 36:51.000
networks to have like an economic incentive to generate decentralized

36:51.000 --> 36:53.160
training data and so forth and so on.

36:53.160 --> 36:56.080
And then basically we're going to live in a world of open source and there's

36:56.080 --> 36:58.080
going to be a billion LMS, right?

36:58.080 --> 37:00.000
Of every size, scale, shape, and description.

37:00.000 --> 37:02.800
And there might be a few big ones that are like the super genius ones, but

37:02.800 --> 37:04.840
like mostly what we'll experience is open source.

37:04.960 --> 37:07.560
And that's, you know, that's more like a world of like what we have today with

37:07.560 --> 37:08.440
like Linux and the web.

37:09.160 --> 37:14.120
Um, so, okay, but, uh, hey, you, you painted these two worlds, but there's

37:14.120 --> 37:17.680
also variations of those worlds because he said regulatory capture is possible to

37:17.680 --> 37:20.600
have these tech giants that don't have regulatory capture, which is something

37:20.600 --> 37:24.840
you're also calling for saying it's okay to have big companies working on this

37:24.920 --> 37:27.920
stuff, uh, as long as they don't achieve regulatory capture.

37:28.440 --> 37:34.840
Uh, but, uh, I have the sense that, um, there's just going to be a new startup.

37:36.120 --> 37:42.440
That's going to basically be the page rank inventor, which has become the new tech giant.

37:44.120 --> 37:46.240
I don't know, I would love to hear your kind of opinion.

37:46.240 --> 37:54.520
If Google meta and Microsoft are as gigantic companies able to pivot so hard.

37:54.840 --> 37:59.280
To create new products, like some of it is just even hiring people or having,

37:59.320 --> 38:04.280
uh, corporate structure that allows for the crazy young kids to come in and

38:04.280 --> 38:05.560
just create something totally new.

38:06.360 --> 38:08.320
Do you think it's possible or do you think it'll come from a startup?

38:08.480 --> 38:11.040
Yeah, it is this always big question, which is you get this feeling.

38:11.040 --> 38:14.440
I hear about this a lot from CEOs found founder CEOs where it's like, wow,

38:14.520 --> 38:16.120
we have 50,000 people.

38:16.120 --> 38:18.680
It's now harder to do new things than it was when we had 50 people.

38:19.240 --> 38:20.360
Like what has happened?

38:20.360 --> 38:22.040
So that's a recurring phenomenon.

38:22.360 --> 38:25.240
By the way, that's one of the reasons why there's always startups and why there's

38:25.240 --> 38:29.160
venture capital is just, that's, that's like a timeless kind of thing.

38:29.160 --> 38:30.800
So that, that, that's one observation.

38:30.800 --> 38:35.240
Um, on page rank, um, we can talk about that, but on page ranks,

38:35.240 --> 38:37.240
specifically on page rank, um, there actually is a page.

38:37.240 --> 38:39.880
So there is a page rank already in the field and it's the transformer, right?

38:39.920 --> 38:43.080
So the, the, the big breakthrough was the transformer, um, and, uh,

38:43.080 --> 38:47.160
the transformer was invented in, uh, 2017 at Google.

38:47.880 --> 38:50.760
And this is actually like really an interesting question because it's like,

38:50.760 --> 38:53.720
okay, the transformers, like why does opening, I even exist.

38:53.760 --> 38:55.200
Like the transformers invested at Google.

38:55.200 --> 38:58.240
Why didn't Google, I asked a guy, I asked a guy, you know, who was senior at

38:58.240 --> 38:59.680
Google brain, kind of when this was happening.

38:59.680 --> 39:03.200
And I said, if Google had just gone flat out to the wall and just said, look,

39:03.200 --> 39:05.360
we're going to launch, we're going to launch the equivalent of GPT for as

39:05.360 --> 39:06.200
fast as we can.

39:06.240 --> 39:08.040
Um, he said, I said, when could we have had it?

39:08.040 --> 39:11.320
And he said 2019, they could have just done a two year sprint with the

39:11.320 --> 39:14.000
transformer and Bennett because they already had the compute at scale.

39:14.000 --> 39:16.040
They already had all the training data and they could have just done it.

39:16.440 --> 39:18.280
There's a variety of reasons they didn't do it.

39:18.360 --> 39:20.440
This is like a classic big company thing.

39:20.520 --> 39:25.480
Um, IBM invented the relational database in 1970s, let it sit on the shelf as a

39:25.480 --> 39:29.280
paper, Larry Ellison picked it up and built Oracle, Xerox Park invented the

39:29.280 --> 39:32.120
interactive computer, they let it sit on the shelf, Steve Jobs came and

39:32.120 --> 39:33.960
turned it into the Macintosh, right?

39:33.960 --> 39:35.200
And so there is this pattern.

39:35.200 --> 39:38.720
Now, having said that, sitting here today, like Google's in the game, right?

39:38.720 --> 39:41.800
So Google, you know, maybe, maybe they, maybe they let like a four year gap

39:41.800 --> 39:44.440
there go there that they maybe shouldn't have, but like they're in the game.

39:44.440 --> 39:46.040
And so now they've got, you know, now they're committed.

39:46.440 --> 39:47.160
They've done this merger.

39:47.160 --> 39:47.840
They're bringing in demos.

39:47.840 --> 39:49.000
They've got this merger with DeepMind.

39:49.320 --> 39:50.920
You know, they're piling in resources.

39:50.920 --> 39:53.600
There are rumors that they're, you know, building up an incredible, you know,

39:53.600 --> 39:56.760
super LLM, um, you know, way beyond what we even have today.

39:57.240 --> 40:00.520
Um, and they've got, you know, unlimited resources and a huge, you know,

40:00.520 --> 40:02.040
they've been challenged at their honor.

40:02.560 --> 40:02.800
Yeah.

40:02.800 --> 40:07.360
I had a, I had a chance to hang out with Sundar Prasai a couple of days ago and we

40:07.360 --> 40:10.760
took this walk and there's this giant new building, uh, well, there's going to

40:10.760 --> 40:13.040
be a lot of AI work, uh, being done.

40:13.040 --> 40:19.640
And it's kind of this ominous feeling of like the fight is on.

40:20.880 --> 40:21.080
Yeah.

40:22.360 --> 40:26.320
There's this beautiful Silicon Valley nature, like birds of chirping and this

40:26.320 --> 40:30.440
giant building, and it's like, uh, the beast has been awakened.

40:31.440 --> 40:34.600
And then like all the big companies are waking up to this.

40:34.760 --> 40:41.160
They have the compute, but also the little guys have, uh, it feels like they

40:41.160 --> 40:44.760
have all the tools to create the killer product that, uh, and then there's

40:44.760 --> 40:45.840
all the tools to scale.

40:45.840 --> 40:50.280
If you have a good idea, if you have the page rank idea, so there's

40:50.280 --> 40:54.960
several things that is page rank page, there's page rank, the algorithm and the

40:54.960 --> 40:56.840
idea, and there's like the implementation of it.

40:57.200 --> 41:00.280
And I feel like killer product is not just the idea, like the transformer,

41:00.280 --> 41:03.560
it's the implementation, something, something really compelling about it.

41:03.560 --> 41:07.640
Like you just can't look away, something like, um, the algorithm behind

41:07.640 --> 41:11.360
TikTok versus TikTok itself, like the actual experience of TikTok.

41:11.400 --> 41:12.600
They just, you can't look away.

41:13.120 --> 41:17.640
It feels like somebody's going to come up with that and it could be Google, but

41:17.640 --> 41:21.120
it feels like it's just easier and faster to do for a startup.

41:21.880 --> 41:22.080
Yeah.

41:22.080 --> 41:25.240
So, so the startup, the, the huge advantage the startups have is they just,

41:25.240 --> 41:26.560
they, there's no sacred cows.

41:26.600 --> 41:28.440
There's no historical legacy to protect.

41:28.440 --> 41:31.040
There's no need to reconcile your new plan with the existing strategy.

41:31.040 --> 41:32.440
There's no communication overhead.

41:32.480 --> 41:34.600
There's no, you know, big companies are big companies.

41:34.600 --> 41:36.520
They've got pre-meetings planning for the meeting.

41:36.520 --> 41:38.480
Then they have, then they have the post-meeting of the recap.

41:38.480 --> 41:40.280
Then they have the presentation, the board, then they have the next round

41:40.280 --> 41:43.960
of meetings and that's the, that's the elapsed time when the startup launches

41:43.960 --> 41:44.600
its product, right?

41:44.600 --> 41:46.880
So, so, so, so there's a timeless, right?

41:46.920 --> 41:47.360
Yeah.

41:47.480 --> 41:48.560
So there's a timeless thing there.

41:48.560 --> 41:51.440
Now, what the startups don't have is everything else, right?

41:51.440 --> 41:52.520
So startups, they don't have a brand.

41:52.520 --> 41:53.600
They don't have customer relationships.

41:53.600 --> 41:54.600
They've gotten a distribution.

41:54.600 --> 41:55.800
They've got no, you know, scale.

41:55.800 --> 41:57.840
I mean, sitting here today, they can't even get GPUs, right?

41:57.840 --> 42:01.200
Like there's like a GPU shortage startups are literally stalled out right now

42:01.200 --> 42:03.240
because they can't get chips, which is like super weird.

42:03.680 --> 42:03.960
Yeah.

42:04.000 --> 42:05.240
Um, they got the cloud.

42:05.720 --> 42:08.320
Yeah, but the clouds run out of chips, right?

42:08.320 --> 42:10.480
And then, and then, and then to the extent the clouds have chips,

42:10.480 --> 42:12.880
they allocate them to the big customers, not the small customers, right?

42:12.880 --> 42:17.640
And so, so, so, so the small companies lack everything other than the ability

42:17.640 --> 42:19.880
to just do something new, right?

42:20.280 --> 42:22.080
And this is the timeless race and battle.

42:22.080 --> 42:23.920
And this is kind of the point I tried to make in the essay, which is

42:23.920 --> 42:25.600
like both sides of this are good.

42:25.600 --> 42:28.440
Like it's really good to have like highly scale tech companies that can do

42:28.440 --> 42:30.960
things that are like at staggering levels of sophistication.

42:31.240 --> 42:33.600
It's really good to have startups that can launch brand new ideas.

42:33.680 --> 42:35.680
They ought to be able to both do that and compete.

42:35.720 --> 42:38.520
They neither one ought to be subsidized or protected from the others.

42:39.040 --> 42:42.160
Like that's, that's, to me, that's just like very clearly the idealized world.

42:42.720 --> 42:45.160
It is the world we've been in for AI up until now.

42:45.160 --> 42:47.080
And then of course there are people trying to shut that down.

42:47.080 --> 42:50.280
But my hope is that, you know, the best outcome clearly will be if that continues.

42:50.520 --> 42:56.600
We'll talk about that a little bit, but I'd love to linger on some of the ways

42:56.600 --> 42:58.000
this is going to change the internet.

42:58.240 --> 43:01.360
So, um, I don't know if you remember, but there's a thing called Mosaic and

43:01.360 --> 43:03.080
there's a thing called Netscape Navigator.

43:03.320 --> 43:04.640
So you were there in the beginning.

43:05.040 --> 43:07.200
Uh, what about the interface to the internet?

43:07.200 --> 43:10.640
How do you think the browser changes and who gets to own the browser?

43:10.640 --> 43:12.920
We've got to see some very interesting browsers.

43:13.680 --> 43:19.120
Uh, Firefox, I mean, all the variants of Microsoft, Internet Explorer, Edge,

43:19.760 --> 43:26.960
and, uh, now Chrome, um, the actual, I mean, it seems like a dumb question to ask,

43:26.960 --> 43:29.000
but do you think we'll still have the web browser?

43:29.960 --> 43:33.320
So I, uh, I have an eight year old and he's super into like Minecraft and

43:33.320 --> 43:34.560
learning to code and doing all this stuff.

43:34.560 --> 43:36.520
So I, I, of course, I was very proud.

43:36.520 --> 43:39.360
I could bring sort of fire down from the mountain to my kid and I brought him

43:39.360 --> 43:43.120
chat GPT and I hooked him up on his, on his, on his, on his laptop.

43:43.120 --> 43:45.120
And I was like, you know, this is the thing that's going to answer all your

43:45.120 --> 43:46.200
questions and he's like, okay.

43:47.040 --> 43:49.120
And I'm like, but it's going to answer all your questions.

43:49.120 --> 43:50.840
And he's like, well, of course, like it's a computer, of course.

43:50.840 --> 43:51.640
It answers all your questions.

43:51.640 --> 43:53.000
Like what else would a computer be good for?

43:53.480 --> 43:53.880
Dad.

43:54.680 --> 43:56.200
Um, and never impressed.

43:56.200 --> 43:57.680
Not impressed in the least.

43:58.400 --> 43:59.280
Two weeks pass.

43:59.360 --> 44:02.960
Um, and he has some question, um, and I say, well, have you asked you GPT?

44:02.960 --> 44:04.760
And he's like, dad, being is better.

44:06.760 --> 44:09.080
And why is being better is because it's built into the browser.

44:09.880 --> 44:12.400
Cause he's like, look, I have the Microsoft edge browser and like it's

44:12.400 --> 44:13.160
got being right here.

44:13.200 --> 44:15.840
And then he doesn't know this yet, but one of the things you can do with

44:15.840 --> 44:19.960
being an edge, um, is there's a setting where you can, um, use it to basically

44:19.960 --> 44:23.640
talk to any webpage because it's sitting right there next to the, uh, next

44:23.640 --> 44:24.400
to the, next to the browser.

44:24.480 --> 44:25.960
And by the way, which includes PDF documents.

44:25.960 --> 44:28.920
And so you can, in, in, in the way they've implemented an edge with Bing is

44:28.920 --> 44:32.320
you can load a PDF and then you can, you can ask a questions, which is the

44:32.320 --> 44:35.000
thing you, you can't do currently and just chat GPT.

44:35.000 --> 44:37.520
So they're, you know, they're, they're going to, they're going to push the,

44:37.520 --> 44:38.640
the, the mel, I think that's great.

44:38.680 --> 44:41.080
You know, they're going to push the melding and see if there's a combination

44:41.080 --> 44:41.600
thing there.

44:42.000 --> 44:44.960
Google's rolling off this thing, the magic button, uh, which is implemented

44:44.960 --> 44:46.640
in either put in Google docs, right?

44:46.640 --> 44:50.120
And so you go to, you know, Google docs and you create a new document and you,

44:50.120 --> 44:52.560
you know, you, instead of like, you know, starting to type, you just, you know,

44:52.560 --> 44:55.240
say it, press the button and it starts to like generate content for you.

44:56.080 --> 44:56.360
Right.

44:56.360 --> 44:58.280
Like, is that the way that it'll work?

44:58.480 --> 45:02.080
Um, is it going to be a speech UI where you're just going to have an earpiece

45:02.080 --> 45:03.000
and talk to it all day long?

45:03.520 --> 45:07.120
You know, is it going to be a, like, these are all, like, this is exactly

45:07.120 --> 45:09.440
the kind of thing that I don't, this is exactly the kind of thing I don't

45:09.440 --> 45:10.480
think is possible to forecast.

45:11.000 --> 45:13.200
I think what we need to do is like run all those experiments.

45:13.440 --> 45:16.000
Um, and, and, and so one outcome is we come out of this with like a super

45:16.000 --> 45:18.560
browser that has AI built in, that's just like amazing.

45:19.120 --> 45:21.520
The other there, look, there's a real possibility that the whole, I mean,

45:21.520 --> 45:25.640
look, there's a possibility here that the whole idea of a screen and windows

45:25.640 --> 45:27.240
and all this stuff just goes away.

45:27.240 --> 45:28.480
Cause like, why do you need that?

45:28.480 --> 45:30.960
If you just have a thing that's just telling you whatever you need to know.

45:31.800 --> 45:35.080
And also, so there's apps that you can use.

45:35.160 --> 45:38.760
You don't really use them, you know, being a Linux guy and Windows guy.

45:39.720 --> 45:44.160
Um, there's one window, the browser that with which you can interact with the

45:44.160 --> 45:46.640
internet, but on the phone, you can also have apps.

45:46.960 --> 45:50.040
So I can interact with Twitter through the app or through the web browser.

45:51.000 --> 45:54.640
And, um, that seems like an obvious distinction, but why have the web

45:54.640 --> 45:55.560
browser in that case?

45:55.920 --> 46:00.400
If one of the apps starts becoming the everything app, what do you want to

46:00.400 --> 46:03.560
try to do with Twitter, but there could be others that could be like a big app.

46:03.600 --> 46:07.680
There could be a Google app that just doesn't really do search, but just like

46:09.120 --> 46:13.040
do what I guess AOL did back in the day or something where it's all right there.

46:14.400 --> 46:24.120
And it changes, um, it changes the nature of the internet because the, where the

46:24.120 --> 46:28.360
content is hosted, who owns the data, who owns the content, how, what is, what

46:28.360 --> 46:29.580
is the kind of content you create?

46:29.580 --> 46:35.240
How do you make money by creating content or the content creators, uh, all of that.

46:36.000 --> 46:39.800
Or it could just keep being the same, which is like, which is the nature of

46:39.800 --> 46:42.760
web pages changes and the nature of content, but there will still be a web

46:42.760 --> 46:45.360
browser because a web browser is a pretty sexy product.

46:46.040 --> 46:50.280
It just seems to work because it like you have an interface, a window into the

46:50.280 --> 46:52.720
world and then the world can be anything you want.

46:52.720 --> 46:55.600
And as the world will evolve, there could be different programming languages.

46:55.600 --> 46:56.440
It can be animated.

46:56.440 --> 46:57.920
Maybe it's three dimensional and so on.

46:58.920 --> 46:59.960
Yeah, it's interesting.

47:00.200 --> 47:02.000
Do you think we'll still have the web browser?

47:02.320 --> 47:05.560
Well, every, every, every, um, every medium becomes the content for the next one.

47:05.600 --> 47:09.040
So they, you know, they will be able to give you a browser whenever you want.

47:09.880 --> 47:10.640
Oh, interesting.

47:11.280 --> 47:11.440
Yeah.

47:11.440 --> 47:15.320
Another way to think about it is maybe what the browser is, maybe it's just the escape

47:15.320 --> 47:16.040
hatch, right?

47:16.040 --> 47:18.480
And which is maybe kind of what it is today, right?

47:18.480 --> 47:21.560
Which is like most of what you do is like inside a social network or inside a

47:21.560 --> 47:24.480
search engine or inside, you know, somebody's app or inside some controlled

47:24.480 --> 47:25.480
experience, right?

47:25.480 --> 47:28.680
But then every once in a while, there's something where you actually want to jail

47:28.680 --> 47:28.920
break.

47:28.920 --> 47:30.000
You want to actually get free.

47:30.120 --> 47:32.360
The web browser is the FU to the man.

47:32.360 --> 47:34.720
You're allowed to, that's the free internet.

47:34.800 --> 47:35.120
Yeah.

47:35.640 --> 47:37.360
Back, back the way it was in the 90s.

47:37.360 --> 47:38.240
So here's something I'm proud of.

47:38.240 --> 47:40.480
So nobody really talks about here's something I'm proud of, which is the web,

47:40.480 --> 47:42.440
the web, the browser, the web servers, they're all, they're still back

47:42.440 --> 47:44.760
or compatible all the way back to like 1992, right?

47:44.760 --> 47:48.960
So like you can put up a, you can still, you know, the big breakthrough of the

47:48.960 --> 47:51.400
web early on, the big breakthrough was it made it really easy

47:51.400 --> 47:54.200
to read, but it also made it really easy to write, made it really easy to publish.

47:54.240 --> 47:56.160
And we literally made it so easy to publish.

47:56.160 --> 47:58.880
We made it not only so you can use easy to publish content, it was actually

47:58.880 --> 48:01.520
also easy to actually write a web server, right?

48:01.520 --> 48:04.640
And you can literally write a web server in four lines of drill code and you

48:04.640 --> 48:05.800
could start publishing content on it.

48:05.800 --> 48:08.920
And you could set whatever rules you want for the content, whatever censorship,

48:08.920 --> 48:10.720
no censorship, whatever you want, you could just do that.

48:11.160 --> 48:12.880
As long as you had an IP address, right?

48:12.880 --> 48:13.600
You could do that.

48:14.080 --> 48:14.840
That still works.

48:16.040 --> 48:18.080
Like that still works exactly as I just described.

48:18.560 --> 48:21.760
So this is part of my reaction to all of this, like, you know, all this

48:21.760 --> 48:24.560
just censorship pressure and all this, you know, these issues around control

48:24.560 --> 48:27.720
and all this stuff, which is like, maybe we need to get back a little bit

48:27.720 --> 48:30.360
more to the wild west, like the wild west is still out there.

48:30.920 --> 48:33.240
Now, they will, they will try to chase you down.

48:33.240 --> 48:35.600
Like they'll try to, you know, people who want a sensor will try to take away

48:35.600 --> 48:38.480
your, you know, your domain name and they'll try to take away your payments

48:38.480 --> 48:41.000
account and so forth, if they really don't like what you, what you're saying.

48:41.000 --> 48:44.280
But, but nevertheless, you like, unless they literally are intercepting

48:44.280 --> 48:46.400
you at the ISP level, like you can still put up a thing.

48:47.400 --> 48:49.840
And so I don't know, I think that's important to preserve, right?

48:49.840 --> 48:53.640
Like, because, because, because, I mean, one is just a freedom argument,

48:53.640 --> 48:56.600
but the other is a creativity argument, which is you want to have the escape

48:56.600 --> 48:59.200
hatch so that the kid with the idea is able to realize the idea.

48:59.200 --> 49:01.920
Because to your point on PageRank, you actually don't know what the next big

49:01.920 --> 49:02.440
idea is.

49:03.200 --> 49:05.520
Nobody called Larry Page and told him to develop PageRank.

49:05.520 --> 49:06.680
Like he came up with that on his own.

49:06.720 --> 49:10.040
And you want to always, I think, leave the escape hatch for the next, you know,

49:10.040 --> 49:12.760
kid or the next Stanford grad student to have the breakthrough idea and be

49:12.760 --> 49:14.400
able to get it up and running before anybody notices.

49:15.240 --> 49:17.600
Um, you and I are both fans of history.

49:17.600 --> 49:18.640
So let's step back.

49:18.880 --> 49:20.080
We'll be talking about the future.

49:20.080 --> 49:24.280
Let's step back for a bit and look at, uh, the nineties.

49:24.280 --> 49:28.400
You created Mosaic web browser, the first widely used web browser.

49:28.400 --> 49:29.320
Tell the story of that.

49:29.400 --> 49:32.360
Hot, hot, and how did it evolve into Netscape navigator?

49:32.360 --> 49:33.480
This is the early days.

49:34.200 --> 49:35.120
So, full story.

49:35.120 --> 49:38.880
So, um, you were born, I was born a small, a small child.

49:38.920 --> 49:41.840
Um, well, actually, yeah, let's go there.

49:41.840 --> 49:45.080
Like, when did you, when would you first fall in love with computers?

49:45.160 --> 49:49.640
Oh, so I hit the generational jackpot and I hit the Gen X kind of point perfectly

49:49.640 --> 49:50.160
as it turns out.

49:50.160 --> 49:51.440
So I was born in 1971.

49:51.440 --> 49:56.200
So there's this great website called WTF happened in 1971.com, which is basically

49:56.200 --> 49:57.800
in 1971 is when everything started to go to hell.

49:57.800 --> 49:59.600
And I was, of course, born in 1971.

49:59.600 --> 50:01.560
So I like to think that I had something to do with that.

50:01.600 --> 50:03.480
Did you make it on the website?

50:03.480 --> 50:06.320
I have, I don't think I made it on the website, but, you know, hopefully somebody

50:06.320 --> 50:09.800
needs to add, this is, this is where everything maybe I contributed to some of

50:09.800 --> 50:13.720
the trends, um, that they, uh, that they do every line on that website goes like

50:13.720 --> 50:14.160
that, right?

50:14.160 --> 50:16.400
So it's all, it's all, it's all a picture disaster.

50:16.400 --> 50:20.120
But, um, but there was this moment in time where, cause the, you know, sort of

50:20.120 --> 50:24.720
the Apple, you know, the Apple II hit in like 1978 and then the IBM PC hit in 82.

50:24.720 --> 50:26.960
So I was like, you know, 11 when the PC came out.

50:27.000 --> 50:29.240
Um, and so I just kind of hit that perfectly.

50:29.600 --> 50:32.240
And then that was the first moment in time when like regular people could spend

50:32.240 --> 50:33.800
a few hundred dollars and get a computer, right?

50:33.800 --> 50:36.360
And so that, I just like that, that, that resonated right out of the gate.

50:37.040 --> 50:39.760
Um, well, then the other part of the story is, you know, I was using

50:39.880 --> 50:42.280
an Apple II that used a bunch of them, but I was using Apple II.

50:42.280 --> 50:44.920
And of course it's set in the back of every Apple II and every Mac it said,

50:44.920 --> 50:47.440
you know, designed in Cupertino, California.

50:47.440 --> 50:50.440
And I was like, wow, Cupertino must be the like shining city on the hill.

50:50.440 --> 50:53.040
Like it was sort of like the most amazing, like city of all time.

50:53.040 --> 50:53.920
I can't wait to see it.

50:53.920 --> 50:57.160
And of course, years later, I came out to Silicon Valley and went to

50:57.160 --> 51:01.120
Cupertino and it's just a bunch of office parks, low rise apartment buildings.

51:01.680 --> 51:04.640
So the aesthetics were a little disappointing, but you know, it was the,

51:05.080 --> 51:08.440
the vector, uh, right of the, of the creation of a lot, of a lot of this stuff.

51:08.520 --> 51:09.600
Um, yeah.

51:09.760 --> 51:13.800
So, so then basically, so part, part, part of my story is just the luck of

51:13.800 --> 51:16.080
having been born at the right time and getting exposed to PCs.

51:16.080 --> 51:19.640
Then the other part is, um, the other part is when Elgora says that he created

51:19.640 --> 51:22.960
the internet, he actually is correct, uh, in, in, in a really meaningful way,

51:22.960 --> 51:26.800
which is he sponsored a bill in 1985 that essentially created the modern internet

51:26.800 --> 51:30.400
created what is called the NSF net at the time, which is sort of the, the first

51:30.400 --> 51:31.680
really fast internet backbone.

51:32.360 --> 51:36.240
Um, and, uh, you know, that, that bill dumped a ton of money into a bunch

51:36.240 --> 51:39.240
of research universities to build out basically at the internet backbone.

51:39.240 --> 51:42.600
And then the super computer centers that were clustered around, um, the,

51:42.600 --> 51:43.200
the internet.

51:43.200 --> 51:46.000
And, and one of those universities was University of Illinois, right?

51:46.000 --> 51:46.480
Went to school.

51:46.480 --> 51:49.120
And so the other stroke of luck that I had was I, I went to Illinois basically

51:49.120 --> 51:51.280
right as that money was just like getting dumped on campus.

51:51.960 --> 51:55.440
And so as a consequence, we had at on campus and this is like, you know, 89,

51:55.560 --> 51:59.080
90, 91, we had like, you know, we were right on the internet backbone.

51:59.080 --> 52:02.480
We had like T three and 45 at the time, T three, 45 megabit backbone connection,

52:02.480 --> 52:04.680
which at the time was, you know, wildly state of the art.

52:04.760 --> 52:06.520
Um, we had crazy supercomputers.

52:06.560 --> 52:08.720
We had thinking machines, parallel supercomputers.

52:08.720 --> 52:10.280
We had silicon graphics workstations.

52:10.280 --> 52:11.480
We had Macintoshes.

52:11.540 --> 52:13.640
We had, we had next cubes all over the place.

52:13.640 --> 52:15.560
We had like every possible kind of computer you could imagine.

52:15.560 --> 52:17.040
Cause all this money just fell out of the sky.

52:17.840 --> 52:19.880
Um, you were living in the future.

52:20.000 --> 52:20.240
Yeah.

52:20.240 --> 52:22.840
So quite literally it was, yeah, like it's all, it's all there.

52:22.840 --> 52:25.400
It's all like we had full broadband graphics, like the whole thing.

52:25.560 --> 52:28.720
And, and it's actually funny cause they had this, this is the first time I kind

52:28.720 --> 52:31.040
of it sort of tickled the back of my head that there might be a big

52:31.040 --> 52:33.680
opportunity in here, which is, you know, they, they embraced it.

52:33.680 --> 52:36.280
And so they put like computers in all the dorms and they wired up all the

52:36.320 --> 52:39.000
dorm rooms and they had all these, you know, labs everywhere and everything.

52:39.000 --> 52:43.240
And then they, they gave every undergrad a computer account and an email address.

52:43.880 --> 52:47.800
Um, and the assumption was that you would use the internet for your four years at

52:47.800 --> 52:51.160
college, um, and then you would graduate and stop using it.

52:52.480 --> 52:53.600
And that was that, right?

52:54.160 --> 52:55.440
And you would just retire your email address.

52:55.440 --> 52:57.600
It wouldn't be relevant anymore cause you'd go off in the workplace and they

52:57.600 --> 52:58.200
don't use email.

52:58.880 --> 53:00.640
You'd be back to using fax machines or whatever.

53:01.000 --> 53:01.960
Did you have that sense as well?

53:01.960 --> 53:04.760
Like what, what you said the, the back of your head was tickled.

53:04.760 --> 53:08.280
Like what was your, what was exciting to you about this possible world?

53:08.320 --> 53:11.280
Well, if this is so useful in this container, if this is so useful in this

53:11.280 --> 53:14.360
container environment that just has this weird source of outside funding, then

53:14.400 --> 53:17.560
if, if it were practical for everybody else to have this, and if it were

53:17.560 --> 53:19.760
cost effective, everybody else to have this, wouldn't they want it?

53:20.320 --> 53:23.200
And the overwhelmingly, the prevailing view at the time was no, they would not

53:23.200 --> 53:23.560
want it.

53:23.560 --> 53:26.080
This is esoteric weird nerd stuff, right?

53:26.080 --> 53:28.520
That like computer science kids like, but like normal people are never going to

53:28.520 --> 53:29.760
do email, right?

53:29.760 --> 53:30.960
Or be on the internet, right?

53:31.000 --> 53:34.320
Um, and so I was just like, wow, like this, this is actually like, this is

53:34.320 --> 53:35.160
really compelling stuff.

53:35.760 --> 53:37.840
Now the other part was it was all really hard to use.

53:37.960 --> 53:41.200
And in practice, you had to be a, basically a CS, uh, you basically had to,

53:41.240 --> 53:44.680
had to be a CS undergrad or equivalent to actually get full use of the internet

53:44.680 --> 53:46.840
at that point, um, cause it was all pretty esoteric stuff.

53:47.240 --> 53:49.520
So then that was the other part of the idea, which was, okay, we need to

53:49.520 --> 53:50.560
actually make this easy to use.

53:51.360 --> 53:56.080
So what's involved in creating was like, like in creating, uh, graphical

53:56.080 --> 53:57.440
interface to the internet.

53:57.680 --> 53:57.880
Yes.

53:57.880 --> 53:59.040
It was a combination of things.

53:59.040 --> 54:02.240
So it was like basically the, the, the web existed in an early sort of

54:02.240 --> 54:03.480
described as prototype form.

54:03.600 --> 54:05.400
Uh, and by the way, text only at that point.

54:05.920 --> 54:06.840
Uh, what did it look like?

54:06.840 --> 54:07.720
What, what was the web?

54:07.720 --> 54:10.040
I mean, well, and the key figure is like, what was it?

54:10.400 --> 54:11.240
What was it like?

54:11.520 --> 54:13.040
What made a picture?

54:13.160 --> 54:16.280
It looked like she had GPT actually, um, it was all text.

54:16.520 --> 54:16.800
Yeah.

54:17.040 --> 54:19.440
Um, and so you had a text based web browser.

54:19.800 --> 54:22.240
Uh, well, actually the original browser, Tim, Tim Berners-Lee, the original,

54:22.240 --> 54:24.800
the original browser, both the original browser and the server actually ran on

54:24.800 --> 54:25.800
next, next cubes.

54:26.240 --> 54:29.040
So these are, this was, you know, the computer Steve Jobs made during the

54:29.040 --> 54:31.760
interim period when he, during the decade long interim period, when he was not

54:31.800 --> 54:36.480
an apple, you know, he got fired in 85 and then came back in 97.

54:36.480 --> 54:38.880
So this was in that interim period where he had this company called Next.

54:38.880 --> 54:41.280
And they made these, literally these computers called cubes.

54:41.880 --> 54:42.680
And there's this famous story.

54:42.680 --> 54:46.560
They were beautiful, but they were 12 inch by 12 inch by 12 inch cubes computers.

54:46.560 --> 54:49.040
And there's a famous story about how they could have cost half as much if it

54:49.040 --> 54:53.760
had been 12 by 12 by 13, but Steve was like, no, like it has to be.

54:54.160 --> 54:56.920
So they were like $6,000 basically academic workstations.

54:56.920 --> 55:00.080
They had the first city round drives, um, which were slow.

55:00.080 --> 55:02.680
I mean, it was the, the computers are all but unusable.

55:02.840 --> 55:04.840
Um, they were so slow, but they were beautiful.

55:05.160 --> 55:05.360
Okay.

55:05.360 --> 55:07.400
Can we actually just take a tiny tangent there?

55:07.400 --> 55:07.600
Sure.

55:07.600 --> 55:07.840
Of course.

55:09.160 --> 55:14.880
The, the 12 by 12 by 12, uh, they just so beautifully encapsulates Steve Jobs

55:14.920 --> 55:15.800
idea of design.

55:16.040 --> 55:20.760
Can you just comment on, um, what you find interesting about Steve Jobs?

55:20.800 --> 55:26.000
What, uh, about that view of the world, that dogmatic pursuit of perfection and

55:26.000 --> 55:27.720
how he saw perfection in design.

55:28.440 --> 55:28.640
Yeah.

55:28.640 --> 55:32.360
So I guess they say like, look, he was a deep believer, I think in a very deep way.

55:32.360 --> 55:33.080
I interpret it.

55:33.360 --> 55:35.560
I don't know if you ever really described it like this, but the way I'd interpret

55:35.560 --> 55:37.200
it is it's, it's like, it's like this thing.

55:37.200 --> 55:38.840
And it's, it's actually a thing in philosophy.

55:38.840 --> 55:41.200
It's like aesthetics are not just appearances.

55:41.240 --> 55:44.400
Aesthetics go all the way to like deep underlying, underlying meaning, right?

55:44.400 --> 55:47.000
It's like, I'm not a physicist.

55:47.000 --> 55:49.440
One of the things I've heard physicists say is one of the things you start to get

55:49.440 --> 55:51.680
a sense of when a theory might be correct is when it's beautiful, right?

55:51.680 --> 55:53.360
Like, you know, they're, right.

55:53.360 --> 55:56.600
And so, so, so there's something and you feel the same thing, by the way, in

55:56.600 --> 55:58.640
like human psychology, right?

55:58.680 --> 56:00.720
You know, when, when you're experiencing awe, right?

56:00.720 --> 56:03.120
You know, there's like a, there's like a, there's a simplicity too.

56:03.120 --> 56:05.400
When, when you're having an honest interaction with somebody, there's an

56:05.400 --> 56:08.720
aesthetic, I would say calm comes over you because you're actually being fully

56:08.720 --> 56:10.080
honest and trying to hide yourself, right?

56:10.080 --> 56:13.720
So there, so, so it's like this very deep sense of aesthetics.

56:13.880 --> 56:16.720
And he would trust that judgment that he had deep down.

56:16.720 --> 56:22.640
Like even, even if the engineering teams are saying this is, this is too difficult.

56:22.680 --> 56:26.480
Even if the, whatever the finance folks are saying, this is ridiculous.

56:26.520 --> 56:29.960
The supply chain, all that kind of stuff, this makes it impossible to, we

56:29.960 --> 56:31.200
can't do this kind of material.

56:31.880 --> 56:33.360
This has never been done before.

56:33.400 --> 56:34.440
And so on and so forth.

56:34.440 --> 56:35.520
He just sticks by it.

56:35.840 --> 56:37.680
Well, I mean, who makes a phone out of aluminum, right?

56:37.680 --> 56:41.040
Like, nobody else would have done that.

56:41.640 --> 56:44.840
And now, of course, if your phone was made out of aluminum, why, you know, how crude,

56:44.960 --> 56:47.200
what a kind of caveman would you have to be to have a phone that's made out of

56:47.200 --> 56:47.920
plastic, like, right?

56:47.920 --> 56:50.400
So like, so it's just this very, right?

56:50.400 --> 56:52.960
And, you know, look, it's, there's a thousand different ways to look at this.

56:52.960 --> 56:55.960
But one of the things is just like, look, these things are central to your life.

56:56.000 --> 56:58.680
Like you're with your phone more than you're with anything else.

56:58.680 --> 57:00.000
Like it's in your, it's going to be in your hand.

57:00.000 --> 57:02.520
I mean, he, you know, you know, this, he thought very deeply about what it meant

57:02.520 --> 57:03.720
for something to be in your hand all day long.

57:04.200 --> 57:07.360
Well, for example, here's an interesting design thing.

57:07.360 --> 57:10.720
Like he never wanted, it's my understanding is he never wanted an iPhone to

57:10.720 --> 57:14.840
have a screen larger than you could reach with your thumb one handed.

57:15.440 --> 57:18.320
And so he was actually opposed to the idea of making the phones larger.

57:18.320 --> 57:20.440
And I don't know if you have this experience today, but let's say there

57:20.440 --> 57:23.720
are certain moments in your day when you might be like only have one hand

57:23.720 --> 57:25.440
available and you might want to be on your phone.

57:25.640 --> 57:26.000
Yeah.

57:26.080 --> 57:30.840
And you're trying to like, your thumb can't reach the send button.

57:30.960 --> 57:31.200
Yeah.

57:31.200 --> 57:32.560
I mean, there's pros and cons, right?

57:32.560 --> 57:35.040
And then there's like folding phones, which I would love to know what he

57:35.040 --> 57:36.160
thought and thinks about them.

57:36.760 --> 57:39.920
Uh, but I mean, is there something you could also just link on?

57:39.920 --> 57:45.000
Cause he's one of the interesting, um, figures in the history of technology.

57:45.360 --> 57:49.520
What makes him, what makes him as successful as he was, what makes him as

57:49.520 --> 57:55.200
interesting as he was, uh, what made him, um, so productive and important in, um,

57:55.680 --> 57:57.520
in, in, in the development of technology.

57:58.040 --> 57:59.280
He had an integrated worldview.

57:59.280 --> 58:03.120
So the, the, the, the properly designed device that had the correct functionality

58:03.120 --> 58:07.480
that had the deepest understanding of the user that was the most beautiful, right?

58:07.480 --> 58:09.920
Like it had to be all of those things, right?

58:09.920 --> 58:13.200
It was, he basically would drive to as close to perfect as you could possibly

58:13.200 --> 58:13.800
get, right?

58:13.800 --> 58:16.440
And I, you know, I suspect that he never quite, you know, thought he ever

58:16.440 --> 58:18.920
got there cause most great creators, you know, are generally dissatisfied.

58:19.040 --> 58:21.720
You know, you read accounts later on and all they can, all they can see are the

58:21.720 --> 58:24.240
flaws in their creation, but like he got as close to perfect each step of the

58:24.240 --> 58:27.680
way as he could possibly get with the, with the constraints of the, of the

58:27.680 --> 58:28.640
technology of his time.

58:28.800 --> 58:31.760
Um, and then, you know, look, he was, you know, sort of famous in the Apple model.

58:31.760 --> 58:34.360
It's like, look, they, they, they will, you know, this, this headset that they

58:34.360 --> 58:37.800
just came out with, like that, you know, it's like a decade long project, right?

58:37.800 --> 58:40.920
It's like, and they're just going to sit there and tune and tune and polish and

58:40.920 --> 58:44.040
polish and tune and polish and tune and polish until it is as perfect as anybody

58:44.040 --> 58:45.040
could possibly make anything.

58:45.480 --> 58:48.480
And then this goes to the, the, the way that people describe working with him was,

58:48.520 --> 58:51.720
which is, you know, there was a terrifying aspect of working with him, which is,

58:51.720 --> 58:53.120
you know, he was, you know, he was very tough.

58:53.680 --> 58:57.600
Um, but there was this thing that everybody I've ever talked to work for him

58:57.600 --> 59:01.680
says that they all say the following, which is he, we did the best work of our

59:01.680 --> 59:05.120
lives when we worked for him because he set the bar incredibly high and then he

59:05.120 --> 59:07.640
supported us with everything that he could to let us actually do work of that

59:07.640 --> 59:08.000
quality.

59:08.560 --> 59:11.440
So a lot of people who were at Apple spend the rest of their lives trying to

59:11.440 --> 59:13.760
find another experience where they feel like they're able to hit that quality

59:13.760 --> 59:14.200
bar again.

59:14.520 --> 59:18.080
Even if it, in retrospect, we're doing it felt like suffering.

59:18.120 --> 59:18.840
Yeah, exactly.

59:19.840 --> 59:23.280
What does that teach you about the human condition?

59:23.880 --> 59:26.240
So look, exactly.

59:26.240 --> 59:29.440
So the Silicon Valley, I mean, look, he's not, you know, George Patton in the,

59:29.480 --> 59:32.560
you know, in the army, like, you know, there are many examples in other fields,

59:32.600 --> 59:38.720
you know, that are like this, um, uh, uh, this is specifically in tech.

59:38.760 --> 59:40.120
It's actually, I find it very interesting.

59:40.120 --> 59:43.400
There's the Apple way, which is polish, polish, polish and don't ship until it's

59:43.400 --> 59:44.480
as perfect as you can make it.

59:44.520 --> 59:48.280
And then there's the sort of the other approach, which is the sort of incremental

59:48.600 --> 59:52.120
hacker mentality, which basically says ship early and often and iterate.

59:52.480 --> 59:55.520
And one of the things I find really interesting is I'm now 30 years into this,

59:55.560 --> 59:59.880
like there are very successful companies on both sides of that approach.

01:00:00.120 --> 01:00:00.440
Right.

01:00:01.000 --> 01:00:04.600
Um, like that is a fundamental difference, right?

01:00:04.600 --> 01:00:08.120
And how to operate and how to build and how to create that you have world

01:00:08.120 --> 01:00:09.880
class companies operating in both ways.

01:00:10.360 --> 01:00:13.640
Um, and I don't think the question of like, which is the superior model is

01:00:13.640 --> 01:00:15.000
anywhere close to being answered.

01:00:15.440 --> 01:00:18.200
Like, and my suspicion is the answer is do both.

01:00:18.200 --> 01:00:21.160
The answer is you actually want both, they lead to different outcomes.

01:00:21.440 --> 01:00:25.560
Software tends to do better, um, with the iterative approach, um, hardware

01:00:25.560 --> 01:00:29.880
tends to do better with the, uh, you know, sort of wait and make it perfect approach.

01:00:29.880 --> 01:00:33.440
But again, you can find examples in, in, in, in both directions.

01:00:33.720 --> 01:00:35.480
Oh, so the juror's still out on that one.

01:00:36.200 --> 01:00:37.400
Uh, so back to Mosaic.

01:00:37.400 --> 01:00:43.400
So what, uh, it was text based, uh, Tim Berners-Lee.

01:00:43.880 --> 01:00:46.120
Well, there was the web, which was text based, but there were no,

01:00:46.120 --> 01:00:47.720
I mean, there was like three websites.

01:00:47.720 --> 01:00:49.160
There was like no content.

01:00:49.160 --> 01:00:50.040
There were no users.

01:00:50.120 --> 01:00:52.040
Like it, it wasn't like a, it wasn't like a catalytic.

01:00:52.040 --> 01:00:54.200
It hadn't, by the way, it was all, because it was all texts.

01:00:54.200 --> 01:00:54.840
There were no documents.

01:00:54.840 --> 01:00:55.400
There are no images.

01:00:55.400 --> 01:00:56.040
There are no videos.

01:00:56.040 --> 01:00:57.320
There were no, right.

01:00:57.320 --> 01:01:00.680
So, so it was, it was, and then if, in the beginning, if you had to be on

01:01:00.680 --> 01:01:03.960
a next cube, but you need to have a next cube both to publish and to consume.

01:01:03.960 --> 01:01:05.320
So, so there were.

01:01:05.320 --> 01:01:06.440
6,000 bucks, you said.

01:01:06.440 --> 01:01:08.600
There were limitations on, yeah, $6,000 PC.

01:01:08.600 --> 01:01:11.560
They did not, they did not sell very many, but then there was also,

01:01:12.200 --> 01:01:14.440
there was also FTP and there was Usenets, right?

01:01:14.440 --> 01:01:17.160
And there was, you know, a dozen other, basically, there's Waste,

01:01:17.160 --> 01:01:18.600
which was an early search thing.

01:01:18.600 --> 01:01:21.560
There was Gopher, which was an early menu based information retrieval system.

01:01:21.560 --> 01:01:24.920
There were like a dozen different sort of scattered ways that people would get

01:01:24.920 --> 01:01:26.280
to information on, on the internet.

01:01:26.280 --> 01:01:29.160
And so the mosaic idea was basically bring those all together,

01:01:29.160 --> 01:01:31.320
make the whole thing graphical, make it easy to use,

01:01:31.320 --> 01:01:33.320
make it basically bulletproof so that anybody can do it.

01:01:33.960 --> 01:01:36.920
And then again, just on the luck side, it so happened that this was right at the

01:01:36.920 --> 01:01:40.120
moment when graphics, when the GUI sort of actually took off.

01:01:40.120 --> 01:01:43.240
And we're now also used to the GUI that we think it's been around forever,

01:01:43.240 --> 01:01:47.400
but it didn't really, you know, the Macintosh brought it out in 85,

01:01:47.400 --> 01:01:49.800
but they actually didn't sell very many Macs in the 80s.

01:01:49.800 --> 01:01:51.400
It was not that successful of a product.

01:01:52.280 --> 01:01:56.840
It really was, you needed Windows 3.0 on PCs and that hit in about 92.

01:01:58.200 --> 01:01:59.960
And so, and we did mosaic in 92, 93.

01:01:59.960 --> 01:02:02.920
So that sort of, it was like right at the moment when you could imagine

01:02:02.920 --> 01:02:08.280
actually having a graphical user interface to write at all, much less one to the internet.

01:02:08.440 --> 01:02:11.560
How old did Windows 3.0 sell?

01:02:11.560 --> 01:02:13.080
So it was at the really big.

01:02:13.080 --> 01:02:13.800
That was the big bang.

01:02:13.800 --> 01:02:16.600
The big operating, graphical operating system.

01:02:16.600 --> 01:02:19.560
Well, this is the classic, okay, this Microsoft was operating on the other.

01:02:19.560 --> 01:02:22.360
So Steve, Steve, Apple was running on the Polish until it was perfect.

01:02:22.360 --> 01:02:24.680
Microsoft famously ran on the other model, which is ship and iterate.

01:02:24.680 --> 01:02:26.920
And so in the old line in those days was Microsoft,

01:02:26.920 --> 01:02:28.600
right, it's the version three of every Microsoft product.

01:02:28.600 --> 01:02:29.800
That's the good one, right?

01:02:29.800 --> 01:02:33.640
And so there are, you can find online, Windows 1, Windows 2, nobody used them.

01:02:34.440 --> 01:02:37.000
Actually, the original Windows, in the original Microsoft Windows,

01:02:37.000 --> 01:02:38.120
the Windows were not overlapping.

01:02:39.320 --> 01:02:42.280
And so you had these very small, very low resolution screens.

01:02:42.280 --> 01:02:45.560
And then you had literally, it just didn't work.

01:02:45.560 --> 01:02:46.360
It wasn't ready yet.

01:02:46.360 --> 01:02:49.720
Well, and Windows 95, I think was a pretty big leap also.

01:02:49.720 --> 01:02:50.520
That was a big leap too.

01:02:50.520 --> 01:02:50.920
Yeah.

01:02:50.920 --> 01:02:52.200
So that was like bang, bang.

01:02:52.840 --> 01:02:55.720
And then of course, Steve, and then, you know, in the fall of the time,

01:02:55.720 --> 01:02:57.560
Steve came back, then the Mac started to take off again.

01:02:57.560 --> 01:02:58.360
That was the third bang.

01:02:58.360 --> 01:02:59.720
And then the iPhone was the fourth bang.

01:03:00.360 --> 01:03:01.320
Such exciting time.

01:03:01.320 --> 01:03:02.920
And then we were off to the races.

01:03:02.920 --> 01:03:06.760
Because nobody could have known or be created from that.

01:03:06.760 --> 01:03:11.240
Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone was only 15 years,

01:03:12.680 --> 01:03:12.920
right?

01:03:12.920 --> 01:03:15.320
Like that ramp was in retrospect.

01:03:15.320 --> 01:03:16.600
At the time, it felt like it took forever.

01:03:16.600 --> 01:03:20.920
But in historical terms, like that was a very fast ramp from even a graphical

01:03:20.920 --> 01:03:24.680
computer at all on your desk to the iPhone, it was 15 years.

01:03:24.680 --> 01:03:27.240
Did you have a sense of what the Internet will be as you're looking

01:03:27.240 --> 01:03:28.440
through the window of Mosaic?

01:03:28.440 --> 01:03:33.160
Like what, like there's just a few web pages for now.

01:03:33.880 --> 01:03:37.720
So the thing I had early on was I was keeping at the time what,

01:03:38.760 --> 01:03:40.360
there's disputes over what was the first blog.

01:03:40.360 --> 01:03:45.240
But I had one of them that at least is a possible, at least a runner-up in the

01:03:45.240 --> 01:03:45.960
competition.

01:03:45.960 --> 01:03:47.640
And it was what was called the What's New page.

01:03:49.480 --> 01:03:53.320
And it was, it was a hardwired, I had distribution and fair advantage.

01:03:53.320 --> 01:03:54.920
I put it right in the browser.

01:03:55.560 --> 01:03:57.560
I put it in the browser and then I put my resume in the browser.

01:03:57.560 --> 01:03:57.720
Yeah.

01:03:57.720 --> 01:03:59.800
Also was hilarious.

01:03:59.800 --> 01:04:06.200
But I was keeping the, not many people get to do that.

01:04:10.200 --> 01:04:11.240
Good call.

01:04:11.240 --> 01:04:12.520
And early days.

01:04:12.520 --> 01:04:13.080
Yes.

01:04:13.080 --> 01:04:14.360
It's so interesting.

01:04:14.360 --> 01:04:17.240
I'm looking for my, about, about, oh, Mark is looking for a job.

01:04:21.000 --> 01:04:24.120
So the What's New page I would literally get up every morning and I would,

01:04:24.120 --> 01:04:24.760
every afternoon.

01:04:25.400 --> 01:04:28.920
And I would basically, if you wanted to launch a website, you would email me.

01:04:29.800 --> 01:04:30.920
And I would list it on the What's New page.

01:04:30.920 --> 01:04:33.960
And that was how people discovered the new websites as they were coming out.

01:04:33.960 --> 01:04:36.520
And I remember, because it was like one, it literally went for me.

01:04:36.520 --> 01:04:41.160
It was like one every couple of days, until like one every day, until like two every day.

01:04:42.840 --> 01:04:45.960
So you're doing, so that, that blog was kind of doing the directory thing.

01:04:45.960 --> 01:04:47.640
So like, what was the homepage?

01:04:48.280 --> 01:04:51.080
So the homepage was just basically trying to explain even what this thing is that you're

01:04:51.080 --> 01:04:51.880
looking at, right?

01:04:51.880 --> 01:04:53.560
The basic, basically basic instructions.

01:04:54.440 --> 01:04:56.440
But then there was a button, there was a button that said What's New.

01:04:56.440 --> 01:04:59.560
And what most people did was they went to, for obvious reasons, went to What's New.

01:04:59.640 --> 01:05:03.000
But like, it was so, it was so mind blowing at that point.

01:05:03.000 --> 01:05:04.200
Just the basic idea.

01:05:04.200 --> 01:05:06.280
And it was just like, you know, this is basically the internet,

01:05:06.280 --> 01:05:07.960
but people could see it for the first time.

01:05:07.960 --> 01:05:10.760
The basic idea was, look, you know, some, you know, it's like, literally,

01:05:10.760 --> 01:05:14.520
it's like an Indian restaurant in like Bristol, England has like put their menu on the web.

01:05:14.520 --> 01:05:20.280
And people were like, wow, because like that's the first restaurant menu on the web.

01:05:20.760 --> 01:05:21.960
And I don't have to be in Bristol.

01:05:21.960 --> 01:05:23.240
And I don't know if I'm ever going to go to Bristol.

01:05:23.240 --> 01:05:24.440
And I don't like Indian food.

01:05:24.440 --> 01:05:26.200
And like, wow, right.

01:05:26.680 --> 01:05:27.560
And it was like that.

01:05:27.560 --> 01:05:32.520
The first web, the first streaming video thing was a, it was another England thing,

01:05:32.520 --> 01:05:33.560
some Oxford or something.

01:05:34.520 --> 01:05:40.040
Some guy put his coffee pot up as the first streaming video thing.

01:05:40.040 --> 01:05:43.320
And he put it on the web because he literally, it was the coffee pot down the hall.

01:05:43.320 --> 01:05:45.640
And he wanted to see when he needed to go refill it.

01:05:46.360 --> 01:05:48.920
But there were, you know, there was a point when there were thousands of people

01:05:48.920 --> 01:05:51.960
like watching that coffee pot, because it was the first thing you could watch.

01:05:52.680 --> 01:06:00.120
But isn't, were you able to kind of infer, you know, if that Indian restaurant could go online?

01:06:00.120 --> 01:06:00.680
Yeah.

01:06:00.680 --> 01:06:02.200
Then you're like, they all will.

01:06:02.200 --> 01:06:03.000
They all will.

01:06:03.000 --> 01:06:03.800
Yeah, exactly.

01:06:03.800 --> 01:06:04.600
So you felt that.

01:06:04.600 --> 01:06:05.320
Yeah, yeah, yeah.

01:06:05.320 --> 01:06:06.760
Now, you know, look, it's still a stretch, right?

01:06:06.760 --> 01:06:08.600
It's still a stretch because it's just like, okay, is it, you know,

01:06:08.600 --> 01:06:10.680
you're still in this zone, which is like, okay, is this a nerd thing?

01:06:10.680 --> 01:06:11.560
Is this a real person thing?

01:06:11.560 --> 01:06:12.360
Yeah.

01:06:12.360 --> 01:06:15.400
Um, by the way, we, you know, there was a wall of skepticism from the media.

01:06:15.400 --> 01:06:18.520
Like they just, like everybody was just like, yeah, this is just like them.

01:06:18.520 --> 01:06:20.760
This is not, you know, this is not for regular people at that time.

01:06:20.760 --> 01:06:23.080
Um, and so you had to think through that.

01:06:23.080 --> 01:06:26.600
And then look, it was still, it was still hard to get on the internet at that point,

01:06:26.600 --> 01:06:26.920
right?

01:06:26.920 --> 01:06:30.200
So you could get kind of this weird bastardized version if you were on AOL,

01:06:30.200 --> 01:06:34.200
which wasn't really real, or you had to go like learn what an ISP was.

01:06:35.480 --> 01:06:38.840
You know, in those days, PCs actually didn't have TCP-IP drivers come pre-installed.

01:06:38.840 --> 01:06:41.400
So you had to learn what a TCP-IP driver was.

01:06:41.400 --> 01:06:42.360
You had to buy a modem.

01:06:42.360 --> 01:06:43.640
You had to install driver software.

01:06:44.760 --> 01:06:47.400
I have a comedy routine I do, something like 20 minutes long,

01:06:47.400 --> 01:06:49.960
describing all the steps required to actually get on the internet at this point.

01:06:50.840 --> 01:06:55.880
And so you had to, you had to look through these practical, well, and then, and then speed

01:06:55.880 --> 01:06:58.920
performance, 14 form modems, right?

01:06:58.920 --> 01:07:03.080
Like it was like watching, you know, glue dry, um, like, and so you had to, you had to,

01:07:03.080 --> 01:07:05.960
there were basically a sequence of bets that we made where you basically needed to look through

01:07:05.960 --> 01:07:09.480
that current state of affairs and say, actually, there's going to be so much demand for that.

01:07:09.480 --> 01:07:12.040
Once people figure this out, there's going to be so much demand for it that all of these

01:07:12.040 --> 01:07:13.560
practical problems are going to get fixed.

01:07:14.280 --> 01:07:21.000
Some people say that the anticipation makes the destination that much more exciting.

01:07:21.000 --> 01:07:22.200
Do you remember progressive JPEGs?

01:07:22.760 --> 01:07:25.480
Yeah. Do I? Do I?

01:07:25.480 --> 01:07:27.560
So for kids in the audience, right?

01:07:27.560 --> 01:07:28.760
For kids in the audience.

01:07:28.760 --> 01:07:32.840
You used to have to wash an image load like a line at a time, but it turns out there was this

01:07:32.840 --> 01:07:37.320
thing with JPEGs where you could load basically every fourth, you could load like every fourth

01:07:37.880 --> 01:07:40.120
line and then you could sweep back through again.

01:07:40.120 --> 01:07:43.400
And so you could like render a fuzzy version of the image up front and then it would like

01:07:43.400 --> 01:07:44.680
resolve into the detailed one.

01:07:44.680 --> 01:07:47.960
And that was like a big UI breakthrough because it gave you something to watch.

01:07:49.240 --> 01:07:53.400
Yeah. And, uh, you know, there's applications in various domains for that.

01:07:55.960 --> 01:07:56.600
Well, it's a big fight.

01:07:56.600 --> 01:07:58.840
If there was a big fight early on about whether there should be images on the web.

01:07:59.880 --> 01:08:01.880
For that reason, for like sexualization.

01:08:01.880 --> 01:08:04.760
No, not explicitly, that did come up, but it wasn't even that.

01:08:04.760 --> 01:08:06.280
It was more just like all the serious info.

01:08:06.280 --> 01:08:09.880
The argument went, the purists basically said all the serious information in the world is text.

01:08:10.520 --> 01:08:13.480
If you introduce images, you basically are going to bring in all the trivial stuff.

01:08:13.480 --> 01:08:17.160
You're going to bring in magazines and, you know, all this crazy stuff that, you know,

01:08:17.160 --> 01:08:18.920
people, you know, it's going to distract from that.

01:08:18.920 --> 01:08:21.400
It's going to go take, take the way from being serious to being frivolous.

01:08:21.400 --> 01:08:27.240
Well, was there any doomer type arguments about, uh, the internet destroying all of human

01:08:27.240 --> 01:08:32.680
civilization or destroying some fundamental fabric of human civilization?

01:08:32.680 --> 01:08:35.160
Yeah. So those days it was all around crime and terrorism.

01:08:36.120 --> 01:08:40.040
So those arguments happened, you know, but there was no sense yet of the internet having

01:08:40.040 --> 01:08:42.680
like an effect on politics or because that was, that was way too far off.

01:08:42.680 --> 01:08:46.520
But there was an enormous panic at the time around cybercrime.

01:08:46.520 --> 01:08:49.640
There was like enormous panic that like your credit card number would get stolen and you'd

01:08:49.640 --> 01:08:51.320
use life savings to be drained.

01:08:51.320 --> 01:08:54.680
And then, you know, criminals were going to, there was, oh, when we started, one of the things

01:08:54.680 --> 01:08:59.160
we did, one of the, the Netscape browser was the first widely used piece of consumer software

01:08:59.160 --> 01:09:02.600
that had strong encryption built in, made it available to ordinary people.

01:09:02.600 --> 01:09:06.440
And at that time, strong encryption was actually illegal to export out of the U.S.

01:09:07.160 --> 01:09:09.240
So we could feel that product in the U.S.

01:09:09.240 --> 01:09:12.120
We could not export it because it was, it was classified as ammunition.

01:09:12.760 --> 01:09:15.800
So the Netscape browser was on a restricted list along with the Tom Huck missile

01:09:16.440 --> 01:09:17.880
as being something that could not be exported.

01:09:17.880 --> 01:09:21.880
So we had to make a second version with deliberately weak encryption to sell overseas

01:09:21.880 --> 01:09:25.960
with a big logo on the box saying do not trust this, which it turns out makes it hard to sell

01:09:25.960 --> 01:09:29.400
software when it's got a big logo that says don't trust it.

01:09:30.280 --> 01:09:33.160
And then we had to spend five years fighting the U.S. government to get them to basically

01:09:33.160 --> 01:09:34.440
stop trying to do this.

01:09:35.640 --> 01:09:39.400
Because the fear was terrorists are going to use encryption to like plot, you know,

01:09:39.400 --> 01:09:40.840
all these things.

01:09:42.040 --> 01:09:45.400
And then, you know, we responded with, well, actually we need encryption to be able

01:09:45.400 --> 01:09:48.040
to secure systems so that the terrorists and the criminals can't get into them.

01:09:48.040 --> 01:09:50.920
So that was, anyway, that was the 1990s fight.

01:09:50.920 --> 01:09:55.080
So can you say something about some of the details of the software engineering

01:09:55.800 --> 01:09:57.880
challenges required to build these browsers?

01:09:57.880 --> 01:10:02.280
I mean, the engineering challenges of creating a product that hasn't really existed before

01:10:03.000 --> 01:10:09.720
that can have such almost like limitless impact on the world with the internet.

01:10:09.720 --> 01:10:12.840
So there was a really key bet that we made at the time, which is very controversial,

01:10:12.840 --> 01:10:16.120
which was core to the core to how it was engineered, which was are we optimizing for

01:10:16.120 --> 01:10:18.280
performance or for ease of creation?

01:10:19.000 --> 01:10:22.200
And in those days, the pressure was very intense to optimize for performance because

01:10:22.200 --> 01:10:25.480
the network connections were so slow and also the computers were so slow.

01:10:26.280 --> 01:10:29.320
And so if you had mentioned the progressive JPEG, it's like,

01:10:30.120 --> 01:10:34.200
if there's an alternate world in which we optimize for performance and it just,

01:10:34.200 --> 01:10:36.280
you had just a much more pleasant experience right up front.

01:10:37.160 --> 01:10:40.520
But what we got by not doing that was we got ease of creation.

01:10:40.520 --> 01:10:45.720
And the way that we got ease of creation was all of the protocols and formats were in text,

01:10:45.720 --> 01:10:46.600
not in binary.

01:10:47.640 --> 01:10:50.920
And so HTTP is in text, by the way, and this was an internet tradition, by the way,

01:10:50.920 --> 01:10:52.440
that we picked up, but we continued it.

01:10:52.440 --> 01:10:57.160
HTTP is text and HTML is text and then everything else that followed is text.

01:10:58.040 --> 01:11:01.560
As a result, and by the way, you can imagine purist engineer saying this is insane,

01:11:01.560 --> 01:11:04.600
you have very limited bandwidth, why are you wasting any time sending text?

01:11:04.600 --> 01:11:07.160
You should be encoding the stuff into binary and it'll be much faster.

01:11:07.160 --> 01:11:08.520
Of course, the answer is that's correct.

01:11:09.400 --> 01:11:11.800
But what you get when you make it text is all of a sudden, well,

01:11:11.800 --> 01:11:13.880
the big breakthrough was the view source function, right?

01:11:13.880 --> 01:11:17.160
So the fact that you could look at a web page, you could hit view source and you could see the

01:11:17.160 --> 01:11:20.360
HTML, that was how people learned how to make web pages, right?

01:11:20.440 --> 01:11:22.920
It's so interesting because the stuff we take for granted now

01:11:24.600 --> 01:11:29.480
is, man, that was fundamental to the development of the web to be able to have HTML just right

01:11:29.480 --> 01:11:30.200
there.

01:11:30.200 --> 01:11:36.280
All the ghetto mess that is HTML, all the sort of almost biological messiness

01:11:37.320 --> 01:11:43.640
of HTML and then having the browser try to interpret that mess to show something reasonable.

01:11:43.640 --> 01:11:46.360
Well, and then there was this internet principle that we inherited, which was

01:11:46.440 --> 01:11:50.360
emit, what was it, emit cautiously, emit conservatively interpret liberally.

01:11:50.360 --> 01:11:55.080
So it basically meant, the design principle was if you're creating a web editor that's

01:11:55.080 --> 01:11:59.480
going to emit HTML, do it as cleanly as you can, but you actually want the browser to interpret

01:11:59.480 --> 01:12:02.840
liberally, which is you actually want users to be able to make all kinds of mistakes

01:12:02.840 --> 01:12:03.800
and for it to still work.

01:12:04.360 --> 01:12:07.000
And so the browser rendering engines to this day have all of this

01:12:07.560 --> 01:12:12.280
skinny code crazy stuff where they're resilient to all kinds of crazy HTML mistakes.

01:12:12.280 --> 01:12:15.800
And literally what I always had in my head is there's an eight-year-old or an 11-year-old

01:12:15.800 --> 01:12:17.960
somewhere and they're doing a view source, they're doing a cut and paste and they're

01:12:17.960 --> 01:12:20.280
trying to make a web page for their turtle or whatever.

01:12:20.840 --> 01:12:23.880
And they leave out a slash and they leave out an angle bracket and they do this and they do

01:12:23.880 --> 01:12:24.840
that and it still works.

01:12:25.960 --> 01:12:26.760
It's also like that.

01:12:26.760 --> 01:12:34.040
I don't often think about this, but programming, C++, C++, all those languages, Lisp, the compile

01:12:34.040 --> 01:12:38.920
language, the interpretive language, Python, Perl, all of that, the brace has to be all

01:12:38.920 --> 01:12:39.400
correct.

01:12:39.400 --> 01:12:39.880
Yes.

01:12:39.880 --> 01:12:41.080
Like everything has to be perfect.

01:12:41.080 --> 01:12:41.400
Brutal.

01:12:42.360 --> 01:12:42.920
And then.

01:12:42.920 --> 01:12:43.480
Autistic.

01:12:43.480 --> 01:12:44.120
You forget.

01:12:45.080 --> 01:12:45.400
All right.

01:12:46.280 --> 01:12:47.880
It's systematic and rigorous.

01:12:47.880 --> 01:12:48.440
Let's go there.

01:12:49.320 --> 01:12:59.480
But you forget that the web with JavaScript eventually and HTML is allowed to be messy in

01:12:59.480 --> 01:13:05.960
the way, for the first time, messy in the way biological systems could be messy.

01:13:05.960 --> 01:13:09.960
It's like the only thing computers were allowed to be messy on for the first time.

01:13:09.960 --> 01:13:10.760
It used to offend me.

01:13:10.760 --> 01:13:13.560
So I grew up in UNIX, I worked on UNIX.

01:13:13.640 --> 01:13:15.960
I was a UNIX native all the way through this period.

01:13:15.960 --> 01:13:19.880
And so, and it used to drive me bananas when it would do the segmentation fault in the

01:13:19.880 --> 01:13:23.800
core dump file, just like it's like literally there's like an error in the code.

01:13:23.800 --> 01:13:26.600
The math is off by one and it core dumps.

01:13:26.600 --> 01:13:29.080
And I'm in the core dump trying to analyze it and trying to reconstruct.

01:13:29.080 --> 01:13:30.600
And I'm just like, this is ridiculous.

01:13:30.600 --> 01:13:33.560
Like the computer ought to be smart enough to be able to know that if it's off by one,

01:13:33.560 --> 01:13:34.200
okay, fine.

01:13:34.200 --> 01:13:35.000
And it keeps running.

01:13:35.640 --> 01:13:38.040
And I would go ask all the experts, like, why can't it just keep running?

01:13:38.040 --> 01:13:40.840
And they'd explain to me, well, because all the downstream repercussions and blah, blah.

01:13:40.840 --> 01:13:47.880
And I'm like, we're forcing the human creator to live, to your point,

01:13:47.880 --> 01:13:50.200
in this hyperlittoral world of perfection.

01:13:50.760 --> 01:13:53.320
And that's just bad.

01:13:53.320 --> 01:13:56.840
And by the way, what happens with that, of course, is what happened with coding at that

01:13:56.840 --> 01:13:58.120
point, which is you get a high priesthood.

01:13:59.160 --> 01:14:02.200
There's a small number of people who are really good at doing exactly that.

01:14:02.200 --> 01:14:04.520
Most people can't and most people are excluded from it.

01:14:04.520 --> 01:14:10.520
And so actually, that was where I picked up that idea was like, no, you want these things

01:14:10.520 --> 01:14:12.600
to be resilient to error in all kinds.

01:14:12.600 --> 01:14:13.800
And this would drive the purists.

01:14:13.800 --> 01:14:14.360
Absolutely crazy.

01:14:14.360 --> 01:14:17.480
Like I got attacked on this like a lot because, yeah, I mean, like every time I,

01:14:17.480 --> 01:14:20.280
you know, all the purists who are like into all this like Markov language stuff and

01:14:20.280 --> 01:14:23.080
formats and codes and all this stuff, they would be like, you know, you can't,

01:14:23.080 --> 01:14:24.520
you're encouraging bad behavior because.

01:14:25.080 --> 01:14:30.200
Oh, so they wanted the browser to give you a segfault error anytime there was a mismatch?

01:14:30.200 --> 01:14:31.640
Yeah, yeah, they wanted it to be a kind of, right?

01:14:31.640 --> 01:14:36.360
They wanted, yeah, that was a very, any properly trained and credentialed engineer

01:14:37.160 --> 01:14:38.920
would be like, that's not how you build these systems.

01:14:38.920 --> 01:14:41.480
That's such a bold move to say, no, it doesn't have to be.

01:14:41.480 --> 01:14:41.720
Yeah.

01:14:41.720 --> 01:14:44.760
Now, like I said, the good news for me is the internet kind of had that tradition already.

01:14:45.320 --> 01:14:48.440
But we, but having said that, like we pushed it, we pushed it way out.

01:14:48.440 --> 01:14:50.840
But the other thing we did going back to the performance thing was we gave up a

01:14:50.840 --> 01:14:53.720
lot of performance we made that that initial experience for the first few years was pretty

01:14:53.720 --> 01:14:54.520
painful.

01:14:54.520 --> 01:14:58.280
But, but the bet there was actually an economic bet, which was basically the demand for the

01:14:58.280 --> 01:15:02.440
web would basically mean that there would be a surge in supply of broadband because

01:15:03.320 --> 01:15:06.920
the question was, okay, how do you get, how do you, how do you get the phone companies

01:15:06.920 --> 01:15:11.720
which are not famous in those days for doing new things at huge cost for like speculative

01:15:11.720 --> 01:15:12.040
reasons?

01:15:12.040 --> 01:15:15.800
Like how do you get them to build up broadband, you know, spend billions of dollars doing that

01:15:15.800 --> 01:15:18.600
and, you know, you could go meet with them and try to talk them into it, or you could

01:15:18.600 --> 01:15:21.960
just have a thing where it's just very clear that it's going to be, that the people love,

01:15:21.960 --> 01:15:23.800
it's going to be better if it's faster.

01:15:23.800 --> 01:15:27.640
And so that, there was a period there, and this was, this was fraught with some peril,

01:15:27.640 --> 01:15:31.480
but there was a period there where it's like, we knew the experience was suboptimized because

01:15:31.480 --> 01:15:36.440
we were trying to force the emergence of demand for broadband, which is in fact, what happened?

01:15:37.400 --> 01:15:41.320
So you had to figure out how to display this text, HTML text.

01:15:41.960 --> 01:15:45.320
So the blue links and the purple links, and there's no standards.

01:15:45.320 --> 01:15:46.760
Is there standards at that time?

01:15:47.800 --> 01:15:48.680
There really still isn't.

01:15:50.280 --> 01:15:54.200
Well, there's like, there's implied, implied standards, right?

01:15:54.200 --> 01:15:57.960
And they, you know, there's all these kinds of new features that are being added like CSS,

01:15:57.960 --> 01:16:02.440
what like, what kind of stuff a browser should be able to support, features with the languages,

01:16:02.520 --> 01:16:03.800
within JavaScript and so on.

01:16:04.360 --> 01:16:10.520
But you, you're setting standards on the fly yourself.

01:16:11.080 --> 01:16:15.720
Well, to this day, if you, if you create a webpage that has no CSS style sheet,

01:16:15.720 --> 01:16:18.200
the browser will render it however it wants to, right?

01:16:18.200 --> 01:16:22.760
So this was one of the things, there was this idea, this idea at the time and how these systems

01:16:22.760 --> 01:16:27.800
were built, which is separation of content from format or separation of content from appearance.

01:16:28.920 --> 01:16:32.120
And that's still, people don't really use that anymore, because everybody wants to determine

01:16:32.120 --> 01:16:33.560
how things look and so they use CSS.

01:16:33.560 --> 01:16:37.080
But it's still in there that you can just let the browser do all the work.

01:16:37.080 --> 01:16:44.040
I still like the, like really basic websites, but that could be just old school.

01:16:44.040 --> 01:16:49.400
Kids these days with their fancy responsive websites that don't actually have much content,

01:16:49.400 --> 01:16:50.920
but have a lot of visual elements.

01:16:50.920 --> 01:16:53.880
Well, that's one of the things that's fun about chat, you know, about chat GPT.

01:16:53.880 --> 01:16:55.640
Yeah, back to the basics.

01:16:55.640 --> 01:16:57.640
Back to just text, right?

01:16:57.640 --> 01:17:02.600
And it, you know, there is this pattern in human creativity and media where you end up

01:17:02.600 --> 01:17:03.480
back at text.

01:17:03.480 --> 01:17:05.480
And I think there's, you know, there's something powerful in there.

01:17:06.200 --> 01:17:08.920
Is there some other stuff you remember, like the purple links?

01:17:08.920 --> 01:17:14.120
There were some interesting design decisions to kind of come up that we have today or we

01:17:14.120 --> 01:17:16.120
don't have today that were temporary.

01:17:16.680 --> 01:17:18.280
So we made, I made the background gray.

01:17:18.280 --> 01:17:21.560
I hated reading text on white backgrounds.

01:17:21.560 --> 01:17:22.520
So I made the background gray.

01:17:22.520 --> 01:17:23.400
Do you regret?

01:17:24.040 --> 01:17:27.560
No, no, no, no, that's that decision I think has been reversed.

01:17:27.800 --> 01:17:30.280
But now I'm happy though, because now dark mode is the thing.

01:17:31.160 --> 01:17:32.760
So it wasn't about gray.

01:17:32.760 --> 01:17:34.520
It was just you didn't want a white background.

01:17:34.520 --> 01:17:35.160
Strain my eyes.

01:17:35.960 --> 01:17:36.920
Strange your eyes.

01:17:37.720 --> 01:17:38.280
Interesting.

01:17:39.800 --> 01:17:41.720
And then there's a bunch of other decisions.

01:17:41.720 --> 01:17:45.640
I'm sure there's an interesting history of the development of HTML and CSS and all those

01:17:45.640 --> 01:17:48.200
in interface and JavaScript.

01:17:48.200 --> 01:17:50.280
And there's this whole Java applet thing.

01:17:51.240 --> 01:17:52.680
Well, the big one probably JavaScript.

01:17:53.480 --> 01:17:57.320
CSS was after me, so I didn't know it wasn't me, but JavaScript wasn't the big

01:17:57.400 --> 01:17:59.000
JavaScript maybe was the biggest of the whole thing.

01:17:59.000 --> 01:17:59.480
That was us.

01:18:00.200 --> 01:18:02.680
And that was basically a bet.

01:18:02.680 --> 01:18:03.720
It was a bet on two things.

01:18:03.720 --> 01:18:06.200
One is that the world wanted a new front end scripting language.

01:18:07.240 --> 01:18:10.520
And then the other was we thought at the time the world wanted a new back end scripting language.

01:18:11.320 --> 01:18:14.360
So JavaScript was designed from the beginning to be both front end and back end.

01:18:15.320 --> 01:18:19.640
And then it failed as a back end scripting language and Java won for a long time.

01:18:19.640 --> 01:18:22.600
And then Python, Perl and other things, PHP and Ruby.

01:18:22.600 --> 01:18:24.440
But now JavaScript is back.

01:18:24.520 --> 01:18:28.200
And so I wonder if everything in the end will run on JavaScript.

01:18:28.200 --> 01:18:30.200
It seems like it is the...

01:18:30.200 --> 01:18:34.360
And by the way, let me give a shout out to Brendan Eich,

01:18:34.360 --> 01:18:37.800
was the basically the one man inventor of JavaScript.

01:18:37.800 --> 01:18:41.960
If you're interested to learn more about Brendan Eich, you can find his podcast previously.

01:18:41.960 --> 01:18:42.440
Exactly.

01:18:43.160 --> 01:18:44.600
So he wrote JavaScript over a summer.

01:18:44.600 --> 01:18:49.240
And I think it is fair to say now that it's the most widely used language in the world.

01:18:49.240 --> 01:18:53.400
And it seems to only be gaining in its range of adoption.

01:18:53.480 --> 01:18:58.440
In the software world, there's quite a few stories of somebody over a weekend or a week

01:18:58.440 --> 01:19:05.640
or over a summer writing some of the most impactful revolutionary pieces of software ever.

01:19:06.200 --> 01:19:07.720
That should be inspiring, yes.

01:19:07.720 --> 01:19:08.360
Very inspiring.

01:19:08.360 --> 01:19:09.480
I'll give you another one, SSL.

01:19:10.120 --> 01:19:12.600
So SSL was the security protocol that was us.

01:19:12.600 --> 01:19:14.120
And that was a crazy idea at the time,

01:19:14.120 --> 01:19:17.720
which was let's take all the native protocols and let's wrap them in a security wrapper.

01:19:17.720 --> 01:19:20.680
That was a guy named Kip Hickman who wrote that over a summer, one guy.

01:19:20.920 --> 01:19:26.440
And then look today, sitting here today, like the transformer at Google was a small handful

01:19:26.440 --> 01:19:27.080
of people.

01:19:27.080 --> 01:19:32.520
And then the number of people who did the core work on GPT, it's not that many people.

01:19:33.080 --> 01:19:34.360
It's a pretty small handful of people.

01:19:35.080 --> 01:19:39.320
And so, yeah, the pattern in software repeatedly over a very long time has been.

01:19:40.920 --> 01:19:43.960
Jeff Bezos always had the two pizza rule for teams at Amazon,

01:19:43.960 --> 01:19:46.520
which is any team needs to be able to be fed with two pizzas.

01:19:47.160 --> 01:19:48.920
If you need the third pizza, you have too many people.

01:19:48.920 --> 01:19:54.600
And I think it's actually the one pizza rule for the really creative work.

01:19:54.600 --> 01:19:57.000
I think it's two people, three people.

01:19:57.000 --> 01:20:00.040
Well, you see that with certain open source projects.

01:20:00.040 --> 01:20:01.720
So much is done by one or two people.

01:20:03.000 --> 01:20:04.200
It's so incredible.

01:20:04.200 --> 01:20:08.360
And that's why you see that gives me so much hope about the open source movement

01:20:08.360 --> 01:20:09.480
in this new age of AI.

01:20:10.440 --> 01:20:15.800
Where recently having had a conversation with Mark Zuckerberg of all people

01:20:15.800 --> 01:20:22.040
who's all in on open source, which is so interesting to see and so inspiring to see.

01:20:22.040 --> 01:20:25.320
Because releasing these models, it is scary.

01:20:25.320 --> 01:20:26.840
It is potentially very dangerous.

01:20:26.840 --> 01:20:27.800
And we'll talk about that.

01:20:28.680 --> 01:20:35.560
But it's also, if you believe in the goodness of most people and in the skill set of most people

01:20:35.560 --> 01:20:39.240
and the desire to do good in the world, that's really exciting.

01:20:39.240 --> 01:20:44.040
Because it's not putting these models into the centralized control of big corporations,

01:20:44.040 --> 01:20:44.920
the government and so on.

01:20:44.920 --> 01:20:49.720
It's putting it in the hands of a teenage kid with a dream in his eyes.

01:20:49.720 --> 01:20:50.280
I don't know.

01:20:50.280 --> 01:20:52.840
That's beautiful.

01:20:52.840 --> 01:20:56.440
And look, this stuff, AI ought to make the individual coder, obviously,

01:20:56.440 --> 01:20:59.800
far more productive by 1,000X or something.

01:20:59.800 --> 01:21:03.640
And so you ought to open source, not just the future of open source,

01:21:03.640 --> 01:21:05.160
but the future of open source, everything.

01:21:05.720 --> 01:21:10.120
We ought to have a world now of super coders who are building things as open source with

01:21:10.120 --> 01:21:13.080
one or two people that were inconceivable five years ago.

01:21:14.280 --> 01:21:17.160
The level of kind of hyperproductivity we're going to get out of our best and brightest,

01:21:17.160 --> 01:21:18.200
I think it's going to go way up.

01:21:18.200 --> 01:21:19.000
It's going to be interesting.

01:21:19.000 --> 01:21:19.960
We'll talk about it.

01:21:19.960 --> 01:21:23.000
But let's just linger a little bit on Nescape.

01:21:24.040 --> 01:21:29.720
Nescape was acquired in 1999 for $4.3 billion by AOL.

01:21:29.720 --> 01:21:31.480
What was that like?

01:21:31.480 --> 01:21:33.960
What were some memorable aspects of that?

01:21:33.960 --> 01:21:37.560
Well, that was the height of the dot-com boom bubble bust.

01:21:37.560 --> 01:21:39.400
I mean, that was the frenzy.

01:21:39.960 --> 01:21:43.400
If you watch a succession, that was like what they did in the fourth season

01:21:43.960 --> 01:21:48.600
with Gojo and then Merger with their, so it was like the height of one of those kind of

01:21:48.600 --> 01:21:49.080
dynamics.

01:21:49.080 --> 01:21:51.000
Would you recommend succession, by the way?

01:21:51.000 --> 01:21:52.360
I'm more of a Yellowstone guy.

01:21:53.640 --> 01:21:54.920
Yellowstone's very American.

01:21:54.920 --> 01:21:55.960
I'm very proud of you.

01:21:57.320 --> 01:22:00.760
I just talked to Matthew McConaughey, and I'm full on texting at this point.

01:22:00.760 --> 01:22:01.240
Good.

01:22:01.240 --> 01:22:02.040
I hurtily approve.

01:22:02.840 --> 01:22:06.040
And he will be doing the sequel to Yellowstone.

01:22:06.040 --> 01:22:07.000
Yeah, exactly.

01:22:07.000 --> 01:22:07.720
Very exciting.

01:22:07.720 --> 01:22:11.400
Anyway, so that's a rude interruption by me.

01:22:12.360 --> 01:22:17.880
By way of succession, so that was at the height of the-

01:22:17.880 --> 01:22:21.400
Deal-making and money and just the fur flying and like craziness.

01:22:21.400 --> 01:22:23.400
And so, yeah, it was just one of those.

01:22:23.400 --> 01:22:26.760
It was just like, I mean, as the entire Nescape thing from start to finish was four years,

01:22:27.640 --> 01:22:30.760
which was like, for one of these companies, it's just like incredibly fast.

01:22:30.760 --> 01:22:33.880
You know, we went public 18 months after we got, after we were founded,

01:22:33.880 --> 01:22:35.480
which virtually never happens.

01:22:35.480 --> 01:22:38.680
So it was just this incredibly fast kind of meteor streaking across the sky.

01:22:39.320 --> 01:22:40.520
And then, of course, it was this.

01:22:40.520 --> 01:22:42.680
And then there was just this explosion that happened,

01:22:42.680 --> 01:22:45.160
because then it was almost immediately followed by the dot-com crash.

01:22:45.960 --> 01:22:48.280
It was then followed by Haywell buying Time Warner,

01:22:48.280 --> 01:22:50.440
which, again, is like the succession guy's going to play with that,

01:22:51.240 --> 01:22:54.440
which turned out to be a disaster steal, one of the famous,

01:22:54.440 --> 01:22:56.040
you know, kind of disasters in business history.

01:22:56.600 --> 01:23:00.680
And then what became an internet depression on the other side of that.

01:23:00.680 --> 01:23:05.400
But then in that depression in the 2000s was the beginning of broadband and smartphones

01:23:05.400 --> 01:23:06.840
and web 2.0, right?

01:23:06.840 --> 01:23:10.200
And then social media and search and every SaaS and everything that came out of that.

01:23:10.200 --> 01:23:12.680
So what did you learn from just the acquisition?

01:23:12.680 --> 01:23:14.120
I mean, this is so much money.

01:23:16.280 --> 01:23:19.320
What's interesting, because I must have been very new to you,

01:23:20.040 --> 01:23:24.840
that these software stuff, you can make so much money.

01:23:24.840 --> 01:23:26.360
There's so much money swimming around.

01:23:26.360 --> 01:23:29.960
I mean, I'm sure the ideas of investment were starting to get born there.

01:23:29.960 --> 01:23:31.560
Yes, let me get, so let me lay it out.

01:23:31.560 --> 01:23:33.400
So here's the thing, I don't know if I figured it out then,

01:23:33.400 --> 01:23:38.040
but figured it out later, which is software is a technology that it's like, you know,

01:23:38.040 --> 01:23:41.720
the concept of the philosopher's stone, the philosopher's stone in Alchemy transmutes

01:23:41.720 --> 01:23:44.520
lead into gold and Newton spent 20 years trying to find the philosopher's stone,

01:23:44.520 --> 01:23:45.000
never got there.

01:23:45.000 --> 01:23:46.120
Nobody's ever figured it out.

01:23:46.680 --> 01:23:48.760
Software is our modern philosopher's stone.

01:23:48.760 --> 01:23:52.680
And in economic terms, it transmutes labor into capital,

01:23:53.800 --> 01:23:55.880
which is like a super interesting thing.

01:23:55.880 --> 01:23:58.200
And by the way, like Karl Marx is rolling over in his grave right now,

01:23:58.200 --> 01:24:00.680
because of course that's complete refutation of his entire theory.

01:24:00.920 --> 01:24:05.240
Transpute labor into capital, which is as follows is somebody

01:24:05.960 --> 01:24:08.520
sits down at a keyboard and types a bunch of stuff in,

01:24:09.320 --> 01:24:11.400
and a capital asset comes out the other side,

01:24:11.400 --> 01:24:14.040
and then somebody buys that capital asset for a billion dollars.

01:24:14.040 --> 01:24:15.880
Like, that's amazing.

01:24:16.600 --> 01:24:19.240
Right, it's literally creating value right out of thin air,

01:24:19.240 --> 01:24:21.480
right, out of purely human thought, right.

01:24:22.440 --> 01:24:26.120
And so that's, there are many things that make software magical and special,

01:24:26.120 --> 01:24:27.240
but that's the economics.

01:24:27.240 --> 01:24:29.400
I wonder what Marx would have thought about that.

01:24:29.400 --> 01:24:30.840
Oh, he would have completely broke his brain,

01:24:30.840 --> 01:24:32.520
because of course the whole thing was,

01:24:34.680 --> 01:24:36.760
that kind of technology is inconceivable when he was alive.

01:24:36.760 --> 01:24:38.680
It was all industrial era stuff.

01:24:38.680 --> 01:24:42.840
And so any kind of machinery necessarily involves huge amounts of capital,

01:24:42.840 --> 01:24:45.320
and then labor was on the receiving end of the abuse.

01:24:46.840 --> 01:24:50.840
Right, but like a software engineer or somebody who basically transmutes

01:24:50.840 --> 01:24:54.840
his own labor into an actual capital asset creates permanent value.

01:24:54.840 --> 01:24:57.080
Well, in fact, it's actually very inspiring.

01:24:57.080 --> 01:24:58.520
That's actually more true today than before.

01:24:58.520 --> 01:25:01.240
So when I was doing software, the assumption was all new software

01:25:01.240 --> 01:25:05.080
basically has a sort of a parabolic sort of life cycle, right.

01:25:05.080 --> 01:25:07.320
So you ship the thing, people buy it.

01:25:07.320 --> 01:25:09.160
At some point, everybody who wants it has bought it,

01:25:09.160 --> 01:25:12.600
and then it becomes obsolete, and it's like bananas, nobody buys old software.

01:25:13.880 --> 01:25:20.360
These days, Minecraft, Mathematica, Facebook, Google,

01:25:21.320 --> 01:25:25.320
you have the software assets that are, have been around for 30 years

01:25:25.320 --> 01:25:27.320
that are gaining in value every year, right.

01:25:27.320 --> 01:25:30.040
And they're just, they're being World of Warcraft, right, Salesforce.com.

01:25:30.040 --> 01:25:32.520
Like they're being, every single year, they're being polished and polished

01:25:32.520 --> 01:25:33.320
and polished and polished.

01:25:33.320 --> 01:25:35.160
They're getting better and better, more powerful, more powerful,

01:25:35.160 --> 01:25:36.200
more valuable, more valuable.

01:25:36.200 --> 01:25:38.440
So we've entered this era where you can actually have these things

01:25:38.440 --> 01:25:40.440
that actually build out over decades, which by the way,

01:25:40.440 --> 01:25:42.040
is what's happening right now with like GPT.

01:25:43.480 --> 01:25:48.440
And so now, and this is why there is always sort of a constant investment

01:25:48.440 --> 01:25:51.880
frenzy around software is because, look, when you start one of these things,

01:25:51.880 --> 01:25:52.600
it doesn't always succeed.

01:25:52.600 --> 01:25:55.560
But when it does, now you might be building an asset that builds value

01:25:55.560 --> 01:25:57.320
for four or five, six decades to come.

01:25:59.000 --> 01:26:01.800
If you have a team of people who have the level of devotion required

01:26:01.800 --> 01:26:02.760
to keep making it better.

01:26:03.560 --> 01:26:06.760
And then the fact that of course, everybody's online, there's five billion

01:26:06.760 --> 01:26:08.840
people that are a click away from any new pieces of software.

01:26:08.840 --> 01:26:12.600
So the potential market size for any of these things is nearly infinite.

01:26:12.600 --> 01:26:14.440
They must have been surreal back then though.

01:26:14.440 --> 01:26:15.080
Yeah, yeah.

01:26:15.080 --> 01:26:16.040
This was all brand new, right?

01:26:16.040 --> 01:26:17.400
Yeah, back then, this was all brand new.

01:26:17.400 --> 01:26:19.240
These were all brand new.

01:26:19.240 --> 01:26:21.720
Had you rolled out that theory and even 1999,

01:26:21.720 --> 01:26:23.080
people would have thought you were smoking crack.

01:26:23.080 --> 01:26:25.160
So that's emerged over time.

01:26:26.520 --> 01:26:30.600
Well, let's now turn back into the future.

01:26:30.600 --> 01:26:33.800
You wrote the essay, why AI will save the world.

01:26:34.920 --> 01:26:36.280
Let's start at the very high level.

01:26:36.280 --> 01:26:37.960
What's the main thesis of the essay?

01:26:37.960 --> 01:26:38.120
Yeah.

01:26:38.120 --> 01:26:41.560
So the main thesis on the essay is that what we're dealing with here is intelligence.

01:26:42.280 --> 01:26:45.400
And it's really important to kind of talk about the sort of very nature of what

01:26:45.400 --> 01:26:46.280
intelligence is.

01:26:46.280 --> 01:26:50.680
And fortunately, we have a predecessor to machine intelligence,

01:26:50.680 --> 01:26:51.640
which is human intelligence.

01:26:52.200 --> 01:26:55.080
And we've got observations and theories over thousands of years

01:26:55.080 --> 01:26:57.720
for what intelligence is in the hands of humans.

01:26:57.720 --> 01:27:00.760
And what intelligence is, what it literally is,

01:27:00.760 --> 01:27:03.160
is the way to capture, process, analyze,

01:27:03.160 --> 01:27:04.600
synthesize, information, solve problems.

01:27:05.800 --> 01:27:09.480
But the observation of intelligence in human hands

01:27:09.480 --> 01:27:11.880
is that intelligence quite literally makes everything better.

01:27:13.000 --> 01:27:17.960
And what I mean by that is every kind of outcome of human quality of life,

01:27:17.960 --> 01:27:21.000
whether it's education outcomes or success of your children,

01:27:21.960 --> 01:27:26.200
or career success, or health, or lifetime satisfaction,

01:27:26.760 --> 01:27:31.080
by the way, propensity to peacefulness as opposed to violence,

01:27:31.960 --> 01:27:34.440
propensity for open-mindedness versus bigotry,

01:27:35.000 --> 01:27:37.320
those are all associated with higher levels of intelligence.

01:27:37.320 --> 01:27:40.120
Smarter people have better outcomes than almost, as you write,

01:27:40.120 --> 01:27:41.720
in almost every domain of activity.

01:27:41.720 --> 01:27:44.760
Academic achievement, job performance, occupational status,

01:27:44.760 --> 01:27:47.320
income, creativity, physical health, longevity,

01:27:47.320 --> 01:27:50.440
learning new skills, managing complex tasks,

01:27:50.440 --> 01:27:54.200
leadership, entrepreneurial success, conflict resolution,

01:27:54.200 --> 01:27:57.480
reading comprehension, financial decision making,

01:27:57.480 --> 01:27:58.760
understanding other's perspectives,

01:27:58.760 --> 01:28:01.560
creative arts, parenting outcomes, and life satisfaction.

01:28:01.560 --> 01:28:05.640
One of the more depressing conversations I've had,

01:28:05.640 --> 01:28:06.840
and I don't know why it's depressing,

01:28:06.840 --> 01:28:09.080
I have to really think through why it's depressing,

01:28:09.080 --> 01:28:13.880
but on IQ and the G factor,

01:28:15.640 --> 01:28:20.120
and that that's something in large part is genetic.

01:28:20.760 --> 01:28:21.880
Mm-hmm.

01:28:21.880 --> 01:28:25.400
And it correlates so much with all of these things

01:28:25.400 --> 01:28:26.520
and success in life.

01:28:27.480 --> 01:28:30.920
It's like all the inspirational stuff we read about,

01:28:30.920 --> 01:28:33.000
like if you work hard and so on,

01:28:33.880 --> 01:28:37.720
damn, it sucks that you're born with a hand that you can't change.

01:28:37.720 --> 01:28:38.440
But what if you could?

01:28:39.160 --> 01:28:41.800
You're saying basically, a really important point,

01:28:41.800 --> 01:28:47.240
and I think it's a, in your articles, it really helped me,

01:28:47.960 --> 01:28:52.440
it's a nice added perspective to think about, listen,

01:28:52.440 --> 01:28:55.000
human intelligence, the science of intelligence

01:28:55.000 --> 01:28:58.680
has shown scientifically that it just makes life easier

01:28:58.680 --> 01:29:01.160
and better, the smarter you are.

01:29:02.040 --> 01:29:04.760
And now let's look at artificial intelligence.

01:29:05.800 --> 01:29:10.760
And if that's a way to increase the,

01:29:12.040 --> 01:29:15.080
some human intelligence, then it's only going

01:29:15.080 --> 01:29:16.200
to make a better life.

01:29:16.200 --> 01:29:17.080
That's the argument.

01:29:17.080 --> 01:29:18.280
And certainly at the collective level,

01:29:18.280 --> 01:29:19.480
we could talk about the collective effect

01:29:19.480 --> 01:29:21.240
of just having more intelligence in the world,

01:29:21.240 --> 01:29:23.480
which we'll have very big payoff.

01:29:23.480 --> 01:29:25.000
But there's also just at the individual level,

01:29:25.000 --> 01:29:27.080
like what if every person has a machine,

01:29:27.080 --> 01:29:29.000
you know, and it's a concept of argument,

01:29:29.000 --> 01:29:30.520
Doug Engelbar's concept of augmentation,

01:29:31.640 --> 01:29:34.520
you know, what if everybody has an assistant

01:29:34.520 --> 01:29:37.240
and the assistant is, you know, 140 IQ,

01:29:38.520 --> 01:29:40.440
and you happen to be 110 IQ,

01:29:41.320 --> 01:29:43.400
and you've got, you know, something that basically

01:29:43.400 --> 01:29:45.960
is infinitely patient and knows everything about you

01:29:45.960 --> 01:29:48.200
and is pulling for you in every possible way,

01:29:48.760 --> 01:29:49.960
wants you to be successful.

01:29:49.960 --> 01:29:52.200
And anytime you find anything confusing

01:29:52.200 --> 01:29:53.160
or want to learn anything

01:29:53.160 --> 01:29:54.440
or have trouble understanding something

01:29:54.440 --> 01:29:55.960
or want to figure out what to do in a situation,

01:29:56.600 --> 01:29:57.640
right, want to figure out how to prepare

01:29:57.640 --> 01:29:59.640
for a job interview, like any of these things,

01:29:59.640 --> 01:30:01.000
like it will help you do it.

01:30:01.000 --> 01:30:04.040
And it will therefore, the combination will effectively be,

01:30:05.240 --> 01:30:06.600
effectively raise your raise,

01:30:06.600 --> 01:30:07.960
because it will effectively raise your IQ,

01:30:07.960 --> 01:30:10.120
will therefore raise the odds of successful

01:30:10.120 --> 01:30:11.320
life outcomes in all these areas.

01:30:11.320 --> 01:30:14.840
So people below the hypothetical 140 IQ,

01:30:15.400 --> 01:30:17.560
it'll pull them up towards 140 IQ.

01:30:17.560 --> 01:30:18.360
Yeah, yeah.

01:30:18.360 --> 01:30:20.840
And then of course, you know, people at 140 IQ

01:30:20.840 --> 01:30:22.200
will be able to have a peer, right,

01:30:22.200 --> 01:30:23.800
to be able to communicate, which is great.

01:30:23.800 --> 01:30:25.880
And then people above 140 IQ will have an assistant

01:30:25.880 --> 01:30:27.480
that they can farm things out to.

01:30:27.480 --> 01:30:29.000
And then look, got willing, you know,

01:30:29.000 --> 01:30:30.920
at some point, these things go from future versions,

01:30:30.920 --> 01:30:35.320
go from 140 IQ equivalent to 150 to 160 to 180, right?

01:30:35.320 --> 01:30:37.880
Like Einstein was estimated to be on the order of 160,

01:30:38.760 --> 01:30:41.880
you know, so when we get, you know, 160 AI,

01:30:41.880 --> 01:30:44.840
like we'll be, you know, one assumes creating

01:30:44.840 --> 01:30:46.360
Einstein level breakthroughs in physics.

01:30:47.240 --> 01:30:49.720
And then at 180, we'll be, you know,

01:30:49.720 --> 01:30:51.800
carrying cancer and developing warp drive

01:30:51.800 --> 01:30:52.760
and doing all kinds of stuff.

01:30:52.760 --> 01:30:55.160
And so it is quite possibly the case,

01:30:55.160 --> 01:30:56.600
this is the most important thing that's ever happened

01:30:56.600 --> 01:30:57.560
and the best thing that's ever happened,

01:30:58.200 --> 01:31:00.280
because precisely because it's a lever

01:31:00.280 --> 01:31:02.680
on this single fundamental factor of intelligence,

01:31:02.680 --> 01:31:04.520
which is the thing that drives so much of everything else.

01:31:05.800 --> 01:31:08.680
Can you still man the case that human plus AI

01:31:08.680 --> 01:31:11.240
is not always better than human for the individual?

01:31:11.240 --> 01:31:12.840
You may have noticed that there's a lot of smart assholes

01:31:12.840 --> 01:31:13.880
running around.

01:31:13.880 --> 01:31:14.600
Sure, yes.

01:31:14.600 --> 01:31:14.920
Right.

01:31:14.920 --> 01:31:17.400
And so like it's smart, there are certain people

01:31:17.400 --> 01:31:18.440
where they get smarter, you know,

01:31:18.440 --> 01:31:20.360
they get to be more arrogant, right?

01:31:20.360 --> 01:31:21.880
So, you know, there's one huge flaw.

01:31:22.520 --> 01:31:25.480
Although to push back on that, it might be interesting

01:31:25.480 --> 01:31:28.920
because when the intelligence is not all coming from you,

01:31:28.920 --> 01:31:30.600
but from another system,

01:31:30.600 --> 01:31:33.720
that might actually increase the amount of humility

01:31:33.720 --> 01:31:34.760
even in the assholes.

01:31:34.760 --> 01:31:35.320
One would hope.

01:31:37.000 --> 01:31:38.840
Or it could make assholes more asshole.

01:31:39.400 --> 01:31:41.480
I mean, that's for psychology to study.

01:31:41.480 --> 01:31:42.280
Yeah, exactly.

01:31:42.280 --> 01:31:45.160
Another one is smart people are very convinced

01:31:45.160 --> 01:31:47.240
that they have a more rational view of the world

01:31:47.240 --> 01:31:48.600
and that they have an easier time

01:31:48.600 --> 01:31:50.280
seeing through conspiracy theories and hoaxes

01:31:50.280 --> 01:31:52.280
and sort of crazy beliefs and all that.

01:31:53.000 --> 01:31:55.320
There's a theory in psychology, which is actually smart people.

01:31:55.320 --> 01:31:57.320
So, for sure, people who aren't as smart

01:31:57.320 --> 01:31:59.400
are very susceptible to hoaxes and conspiracy theories.

01:31:59.960 --> 01:32:01.880
But it may also be the case that the smarter you get,

01:32:01.880 --> 01:32:03.560
you become susceptible in a different way,

01:32:04.360 --> 01:32:06.680
which is you become very good at marshalling facts

01:32:06.680 --> 01:32:07.880
to fit preconceptions.

01:32:08.280 --> 01:32:08.760
Yes.

01:32:08.760 --> 01:32:09.480
Right?

01:32:09.480 --> 01:32:12.520
You become very, very good at assembling whatever theories

01:32:12.520 --> 01:32:15.480
and frameworks and pieces of data and graphs and charts

01:32:15.480 --> 01:32:17.880
you need to validate whatever crazy ideas got in your head.

01:32:18.440 --> 01:32:20.440
And so, you're susceptible in a different way.

01:32:21.400 --> 01:32:22.120
Right?

01:32:22.120 --> 01:32:25.480
We're all sheep, but different colored sheep.

01:32:25.480 --> 01:32:27.320
Some sheep are better at justifying it, right?

01:32:28.200 --> 01:32:29.960
And those are the smart sheep, right?

01:32:31.000 --> 01:32:32.600
So, yeah, look, I would say this.

01:32:32.600 --> 01:32:33.800
Look, there are no panacea.

01:32:33.800 --> 01:32:35.080
I am not a utopian.

01:32:35.080 --> 01:32:36.280
There are no panaceas in life.

01:32:36.760 --> 01:32:39.480
There are no, like, I don't believe they're like pure positives.

01:32:39.480 --> 01:32:41.960
I'm not a transcendental kind of person like that.

01:32:41.960 --> 01:32:44.120
But, you know, so, yeah, there are going to be issues.

01:32:45.320 --> 01:32:47.640
And, you know, look, smart people, maybe you could say

01:32:47.640 --> 01:32:49.560
about smart people as they are more likely to get themselves

01:32:49.560 --> 01:32:51.320
in situations that are, you know, beyond their grasp,

01:32:51.320 --> 01:32:52.600
you know, because they're just more confident

01:32:52.600 --> 01:32:54.040
and their ability to deal with complexity.

01:32:54.040 --> 01:32:56.120
And their eyes become bigger.

01:32:56.120 --> 01:32:57.880
Their cognitive eyes become bigger than their stomach.

01:32:58.520 --> 01:33:01.240
You know, so, yeah, you could argue those eight different ways.

01:33:01.240 --> 01:33:03.240
Nevertheless, on net, right?

01:33:03.240 --> 01:33:04.760
Clearly, overwhelmingly.

01:33:04.760 --> 01:33:06.280
Again, if you just extrapolate from what we know

01:33:06.280 --> 01:33:09.480
about human intelligence, you're improving so many aspects

01:33:09.480 --> 01:33:10.920
of life if you're upgrading intelligence.

01:33:11.960 --> 01:33:15.400
So, there'll be assistance at all stages of life.

01:33:15.400 --> 01:33:17.960
So, when you're younger, there's for education,

01:33:17.960 --> 01:33:20.680
all that kind of stuff, or mentorship, all of this.

01:33:20.680 --> 01:33:24.360
And later on, as you're doing work and you've developed a skill

01:33:24.360 --> 01:33:26.520
and you're having a profession, you'll have an assistant

01:33:26.520 --> 01:33:28.680
that helps you excel at that profession.

01:33:28.680 --> 01:33:30.120
So, at all stages of life.

01:33:30.120 --> 01:33:31.800
Yeah. I mean, look, the theory is augmentations.

01:33:31.800 --> 01:33:33.400
This is the Degangalbert's term for it.

01:33:33.400 --> 01:33:35.880
Degangalbert made this observation many, many decades ago

01:33:35.880 --> 01:33:37.160
that, you know, basically, it's like you can have

01:33:37.160 --> 01:33:38.760
this oppositional frame of technology

01:33:38.760 --> 01:33:40.040
where it's like us versus the machines.

01:33:40.040 --> 01:33:41.560
But what you really do is you use technology

01:33:41.560 --> 01:33:42.920
to augment human capabilities.

01:33:43.560 --> 01:33:45.480
And by the way, that's how actually the economy develops.

01:33:45.480 --> 01:33:47.240
That's dark about the economic side of this,

01:33:47.240 --> 01:33:49.240
but that's actually how the economy grows

01:33:49.240 --> 01:33:52.040
is through technology augmenting human potential.

01:33:53.800 --> 01:33:56.120
And so, yeah, and then you basically have a proxy

01:33:56.120 --> 01:34:00.120
or a, you know, a sort of prosthetic, you know,

01:34:00.120 --> 01:34:02.040
so like you've got glasses, you've got a wristwatch,

01:34:02.760 --> 01:34:05.240
you know, you've got shoes, you know, you've got these things,

01:34:05.240 --> 01:34:07.560
you've got a personal computer, you've got a word processor,

01:34:07.560 --> 01:34:09.960
you've got Mathematica, you've got Google.

01:34:10.600 --> 01:34:12.600
This is the latest, viewed through that lens.

01:34:12.600 --> 01:34:15.480
AI is the latest in a long series of basically

01:34:15.480 --> 01:34:18.040
augmentation methods to be able to raise human capabilities.

01:34:18.040 --> 01:34:20.280
It's just this one is the most powerful one of all

01:34:20.280 --> 01:34:22.680
because this is the one that goes directly to what they call

01:34:22.680 --> 01:34:24.520
fluid intelligence, which is IQ.

01:34:26.840 --> 01:34:30.920
Well, there's two categories of folks that you outline

01:34:31.480 --> 01:34:34.760
that they worry about or highlight the risks of AI

01:34:34.760 --> 01:34:36.680
and you highlight a bunch of different risks.

01:34:36.680 --> 01:34:38.920
I would love to go through those risks

01:34:38.920 --> 01:34:39.800
and just discuss them.

01:34:39.800 --> 01:34:42.520
Brainstorm, which ones are serious

01:34:42.520 --> 01:34:44.760
and which ones are less serious?

01:34:44.760 --> 01:34:46.760
But first, the Baptist and the bootleggers,

01:34:46.760 --> 01:34:48.920
what are these two interesting groups of folks

01:34:49.800 --> 01:34:56.040
who worry about the effect of AI on human civilization?

01:34:56.040 --> 01:34:57.080
Or say they do.

01:34:57.080 --> 01:34:58.200
Say, oh, okay.

01:34:59.400 --> 01:35:00.440
Yes, I'll say they do.

01:35:00.440 --> 01:35:02.440
The Baptist worry the bootleggers, I'll say they do.

01:35:03.720 --> 01:35:05.560
So the Baptist and the bootleggers is a metaphor

01:35:05.560 --> 01:35:07.960
from economics from what's called development economics.

01:35:07.960 --> 01:35:10.600
And it's this observation that when you get social reform

01:35:10.600 --> 01:35:14.520
movements in a society, you tend to get two sets of people

01:35:14.520 --> 01:35:16.120
showing up arguing for the social reform.

01:35:16.680 --> 01:35:19.000
And the term Baptist and bootleggers

01:35:19.000 --> 01:35:21.560
comes from the American experience with alcohol prohibition.

01:35:22.520 --> 01:35:25.720
And so in the 1900s, 1910s, there was this movement

01:35:25.720 --> 01:35:27.320
that was very passionate at the time,

01:35:27.320 --> 01:35:29.800
which basically said alcohol is evil

01:35:29.800 --> 01:35:30.920
and it's destroying society.

01:35:31.800 --> 01:35:33.880
By the way, there was a lot of evidence to support this.

01:35:34.440 --> 01:35:37.560
There were very high rates of very high correlations

01:35:37.560 --> 01:35:41.080
than, by the way, and now between rates of physical violence

01:35:41.080 --> 01:35:42.280
and alcohol use.

01:35:42.280 --> 01:35:44.680
Almost all violent crimes have either the perpetrator

01:35:44.680 --> 01:35:45.880
or the victim are both drunk.

01:35:47.000 --> 01:35:48.280
You see this actually in the work,

01:35:48.280 --> 01:35:50.200
almost all sexual harassment cases in the workplace.

01:35:50.200 --> 01:35:52.120
It's like at a company party and somebody's drunk.

01:35:52.120 --> 01:35:54.920
Like it's amazing how often alcohol actually correlates

01:35:54.920 --> 01:35:57.720
to actually dysfunction of these two, domestic abuse

01:35:57.720 --> 01:35:58.680
and so forth, child abuse.

01:35:59.160 --> 01:36:00.840
And so you had this group of people who were like,

01:36:00.840 --> 01:36:02.840
okay, this is bad stuff and we shall outlaw it.

01:36:02.840 --> 01:36:04.440
And those were quite literally Baptists.

01:36:04.440 --> 01:36:07.320
Those were super committed, hardcore Christian activists

01:36:07.320 --> 01:36:07.960
in a lot of cases.

01:36:08.520 --> 01:36:10.840
There was this woman whose name was Kerry Nation,

01:36:11.560 --> 01:36:13.560
who was this older woman who had been in this,

01:36:13.560 --> 01:36:15.240
I don't know, disastrous marriage or something

01:36:15.240 --> 01:36:17.400
and her husband had been abusive and drunk all the time.

01:36:17.400 --> 01:36:21.080
And she became the icon of the Baptist prohibitionist

01:36:21.080 --> 01:36:23.960
and she was legendary in that era for carrying an axe

01:36:24.840 --> 01:36:28.280
and doing completely on her own, doing raids of saloons

01:36:28.280 --> 01:36:31.560
and like taking her axe to all the bottles and tigs in the back.

01:36:31.560 --> 01:36:32.360
And so-

01:36:32.360 --> 01:36:33.480
So a true believer.

01:36:33.480 --> 01:36:36.040
An absolute true believer with absolutely

01:36:36.040 --> 01:36:37.160
the purest of intentions.

01:36:37.160 --> 01:36:39.960
And again, there's a very important thing here,

01:36:39.960 --> 01:36:41.800
which is you could look at this cynically

01:36:41.800 --> 01:36:44.360
and you could say the Baptists are like delusional extremists,

01:36:44.360 --> 01:36:45.800
but you can also say, look, they're right.

01:36:45.800 --> 01:36:48.840
Like she had a point, like she wasn't wrong

01:36:49.400 --> 01:36:50.360
about a lot of what she said.

01:36:51.320 --> 01:36:53.160
But it turns out, the way the story goes,

01:36:53.160 --> 01:36:55.160
is it turns out that there were another set of people

01:36:55.160 --> 01:36:57.080
who very badly wanted to outlaw alcohol

01:36:57.080 --> 01:36:57.560
in those days.

01:36:57.560 --> 01:36:58.680
And those were the bootleggers,

01:36:58.680 --> 01:37:02.120
which was organized crime that stood to make a huge amount of money

01:37:02.120 --> 01:37:04.120
if legal alcohol sales were banned.

01:37:04.680 --> 01:37:06.440
And this was in fact, the way the history goes,

01:37:06.440 --> 01:37:08.520
is this was actually the beginning of organized crime in the U.S.

01:37:08.520 --> 01:37:10.680
This was the big economic opportunity that opened that up.

01:37:11.880 --> 01:37:15.240
And so they went in together and they didn't go in together.

01:37:15.240 --> 01:37:17.320
Like the Baptists did not even necessarily know

01:37:17.320 --> 01:37:19.240
about the bootleggers because they were on the moral crusade.

01:37:19.800 --> 01:37:21.400
The bootleggers certainly knew about the Baptists

01:37:21.400 --> 01:37:23.000
and they were like, wow, these people are like

01:37:23.000 --> 01:37:25.000
the great front people for like, you know,

01:37:25.640 --> 01:37:26.920
shenanigans in the background.

01:37:27.640 --> 01:37:29.880
And they got the full state act passed, right?

01:37:29.880 --> 01:37:32.440
And they did in fact ban alcohol in the U.S.

01:37:32.440 --> 01:37:34.680
And you'll notice what happened, which is people kept drinking.

01:37:35.320 --> 01:37:36.520
Like it didn't work.

01:37:36.520 --> 01:37:37.400
People kept drinking.

01:37:38.280 --> 01:37:40.040
The bootleggers made a tremendous amount of money.

01:37:40.600 --> 01:37:42.920
And then over time, it became clear that it made no sense

01:37:42.920 --> 01:37:44.840
to make it illegal and it was causing more problems.

01:37:44.840 --> 01:37:45.960
And so then it was revoked.

01:37:45.960 --> 01:37:48.040
And here we sit with legal alcohol 100 years later

01:37:48.040 --> 01:37:49.080
with all the same problems.

01:37:50.440 --> 01:37:53.160
And the whole thing was this like giant misadventure.

01:37:54.040 --> 01:37:56.120
The Baptists got taken advantage of by the bootleggers

01:37:56.120 --> 01:37:58.360
and the bootleggers got what they wanted and that was that.

01:37:58.360 --> 01:38:01.960
The same two categories of folks are now sort of suggesting

01:38:03.000 --> 01:38:05.400
the development of artificial intelligence should be regulated.

01:38:05.400 --> 01:38:05.800
100%.

01:38:05.800 --> 01:38:06.440
Yeah, it's the same pattern.

01:38:06.440 --> 01:38:08.600
And the economists will tell you it's the same pattern every time.

01:38:08.600 --> 01:38:10.040
Like this is what happened with nuclear power.

01:38:10.040 --> 01:38:12.280
This is what happened, which is another interesting one.

01:38:12.280 --> 01:38:14.600
But like, yeah, this happens dozens and dozens of times

01:38:15.480 --> 01:38:16.680
throughout the last 100 years.

01:38:16.680 --> 01:38:18.360
And this is what's happening now.

01:38:18.360 --> 01:38:21.560
And you write that it isn't sufficient

01:38:21.560 --> 01:38:24.120
to simply identify the actors and impugn their motors.

01:38:24.120 --> 01:38:26.520
We should consider the arguments of both the Baptists

01:38:26.520 --> 01:38:28.520
and the bootleggers on their merits.

01:38:28.520 --> 01:38:29.880
So let's do just that.

01:38:30.840 --> 01:38:31.960
Risk number one.

01:38:35.240 --> 01:38:36.840
Will AI kill us all?

01:38:36.840 --> 01:38:37.320
Yes.

01:38:38.120 --> 01:38:42.840
So what do you think about this one?

01:38:43.800 --> 01:38:45.720
What do you think is the core argument here

01:38:46.680 --> 01:38:51.640
that the development of AGI, perhaps better said,

01:38:52.200 --> 01:38:54.200
will destroy human civilization?

01:38:54.200 --> 01:38:55.960
Well, first of all, you just did a sleight of hand

01:38:55.960 --> 01:38:57.880
because we went from talking about AI to AGI.

01:38:59.880 --> 01:39:01.400
Is there a fundamental difference there?

01:39:01.400 --> 01:39:02.360
I don't know. What's AGI?

01:39:03.640 --> 01:39:04.360
What's AI?

01:39:04.360 --> 01:39:04.680
What's intelligence?

01:39:04.680 --> 01:39:05.560
Well, I know what AI is.

01:39:05.560 --> 01:39:06.520
AI is machine learning.

01:39:07.240 --> 01:39:08.200
What's AGI?

01:39:08.200 --> 01:39:10.840
I think we don't know what the bottom of the well of machine

01:39:10.840 --> 01:39:12.600
learning is or what the ceiling is.

01:39:12.600 --> 01:39:15.240
Because just to call something machine learning

01:39:15.240 --> 01:39:16.760
or just to call something statistics

01:39:16.760 --> 01:39:18.680
or just to call it math or computation

01:39:18.680 --> 01:39:22.520
doesn't mean nuclear weapons are just physics.

01:39:22.520 --> 01:39:26.840
So to me, it's very interesting and surprising

01:39:26.840 --> 01:39:28.200
how far machine learning has taken.

01:39:28.200 --> 01:39:30.120
No, but we knew that nuclear physics would lead to weapons.

01:39:30.120 --> 01:39:31.560
That's why the scientists of that era

01:39:31.560 --> 01:39:34.040
were always in this huge dispute about building the weapons.

01:39:34.040 --> 01:39:34.680
This is different.

01:39:34.680 --> 01:39:35.000
AGI is different.

01:39:35.000 --> 01:39:36.360
Where does machine learning lead?

01:39:36.360 --> 01:39:36.920
Do we know?

01:39:36.920 --> 01:39:38.040
We don't know, but this is my point.

01:39:38.040 --> 01:39:38.680
It's different.

01:39:38.680 --> 01:39:39.560
We actually don't know.

01:39:40.120 --> 01:39:42.360
And this is where the sleight of hand kicks in.

01:39:42.360 --> 01:39:44.360
This is where it goes from being a scientific topic

01:39:44.360 --> 01:39:45.400
to being a religious topic.

01:39:46.360 --> 01:39:48.520
And that's why I specifically called out the...

01:39:48.520 --> 01:39:49.160
Because that's what happens.

01:39:49.160 --> 01:39:50.280
They do the vocabulary shift.

01:39:50.280 --> 01:39:51.960
And all of a sudden, you're talking about something totally

01:39:51.960 --> 01:39:53.000
that's not actually real.

01:39:53.000 --> 01:39:55.960
Well, then maybe you can also, as part of that,

01:39:55.960 --> 01:39:59.400
define the Western tradition of millennialism.

01:39:59.400 --> 01:40:01.080
Yes, end of the world.

01:40:01.080 --> 01:40:01.720
Apocalypse.

01:40:01.720 --> 01:40:02.440
What is it?

01:40:02.440 --> 01:40:03.240
Apocalypse cults.

01:40:03.800 --> 01:40:04.680
Apocalypse cults.

01:40:04.680 --> 01:40:06.280
Well, so we live in...

01:40:06.280 --> 01:40:07.720
We, of course, live in a Judeo-Christian,

01:40:07.720 --> 01:40:09.400
but primarily Christian, kind of saturated,

01:40:09.400 --> 01:40:11.720
kind of Christian, post-Christian, secularized Christian,

01:40:11.720 --> 01:40:13.320
kind of world in the West.

01:40:14.120 --> 01:40:15.640
And, of course, court of Christianity

01:40:15.640 --> 01:40:18.520
is the idea of the Second Coming and the revelations

01:40:18.520 --> 01:40:22.280
and Jesus returning in the thousand-year utopia on Earth

01:40:22.280 --> 01:40:24.200
and then the rapture and all that stuff.

01:40:25.640 --> 01:40:27.400
We collectively, as a society,

01:40:27.400 --> 01:40:29.400
we don't necessarily take all that fully seriously now.

01:40:29.400 --> 01:40:32.520
So what we do is we create our secularized versions of that.

01:40:32.520 --> 01:40:34.200
We keep looking for utopia.

01:40:34.200 --> 01:40:36.280
We keep looking for basically the end of the world.

01:40:36.840 --> 01:40:38.600
And so what you see over decades

01:40:38.600 --> 01:40:40.040
is basically a pattern of these sort of...

01:40:40.520 --> 01:40:43.000
This is what cults are.

01:40:43.000 --> 01:40:43.960
This is how cults form,

01:40:43.960 --> 01:40:45.800
as they form around some theory of the end of the world.

01:40:45.800 --> 01:40:49.000
And so the people's temple cult, the Manson cult,

01:40:49.000 --> 01:40:52.440
the Heaven's Gate cult, the David Koresh cult.

01:40:52.440 --> 01:40:53.560
What they're all organized around

01:40:53.560 --> 01:40:55.560
is like there's going to be this thing that's going to happen

01:40:55.560 --> 01:40:58.040
that's going to basically bring civilization crashing down.

01:40:58.040 --> 01:41:00.040
And then we have this special elite group of people

01:41:00.040 --> 01:41:01.640
who are going to see it coming and prepare for it.

01:41:02.280 --> 01:41:04.200
And then they're the people who are either going to stop it

01:41:04.200 --> 01:41:05.240
or are failing stopping it.

01:41:05.240 --> 01:41:07.160
They're going to be the people who survived on the other side

01:41:07.160 --> 01:41:09.160
and ultimately get credit for having been right.

01:41:09.160 --> 01:41:10.680
Why is that so compelling, do you think?

01:41:11.720 --> 01:41:14.440
Because it satisfies this very deep need we have

01:41:14.440 --> 01:41:16.280
for transcendence and meaning

01:41:17.080 --> 01:41:20.200
that got stripped away when we became secular.

01:41:20.200 --> 01:41:22.040
Yeah, but why does transcendence

01:41:22.040 --> 01:41:24.520
involve the destruction of human civilization?

01:41:25.160 --> 01:41:30.200
Because it's like a very deep psychological thing,

01:41:30.200 --> 01:41:31.960
because it's like how plausible is it

01:41:31.960 --> 01:41:33.720
that we live in a world where everything's just kind of all right?

01:41:34.840 --> 01:41:37.160
Right, how exciting is that?

01:41:38.120 --> 01:41:39.480
We want more than that.

01:41:39.480 --> 01:41:40.920
But that's the deep question I'm asking.

01:41:41.960 --> 01:41:44.520
Why is it not exciting to live in a world

01:41:44.520 --> 01:41:45.720
where everything's just all right?

01:41:47.080 --> 01:41:50.200
I think most of the animal kingdom

01:41:50.200 --> 01:41:52.440
would be so happy with just all right,

01:41:52.440 --> 01:41:53.480
because that means survival.

01:41:54.200 --> 01:41:56.920
Why are we, maybe that's what it is.

01:41:56.920 --> 01:41:59.880
Why are we conjuring up things to worry about?

01:42:00.440 --> 01:42:02.760
So CS Lewis called it the God-shaped hole.

01:42:03.320 --> 01:42:06.600
So there's a God-shaped hole in the human experience,

01:42:06.600 --> 01:42:08.600
consciousness, soul, whatever you want to call it,

01:42:08.600 --> 01:42:11.720
where there's got to be something that's bigger than all this.

01:42:12.360 --> 01:42:13.560
There's got to be something transcendent.

01:42:13.560 --> 01:42:14.920
There's got to be something that is bigger,

01:42:14.920 --> 01:42:16.920
right, bigger, the bigger purpose of bigger meaning.

01:42:17.560 --> 01:42:19.640
And so we have run the experiment

01:42:19.640 --> 01:42:21.960
of we're just going to use science and rationality

01:42:21.960 --> 01:42:24.840
and kind of everything's just going to kind of be as it appears.

01:42:24.840 --> 01:42:28.440
And a large number of people have found that very deeply wanting

01:42:29.240 --> 01:42:30.840
and have constructed narratives.

01:42:30.840 --> 01:42:32.840
And this is the story of the 20th century, right?

01:42:32.840 --> 01:42:34.600
Communism was one of those.

01:42:34.600 --> 01:42:36.360
Communism was a form of this.

01:42:36.360 --> 01:42:37.480
Nazism was a form of this.

01:42:38.760 --> 01:42:41.560
Some people, you can see movements like this

01:42:41.560 --> 01:42:43.240
playing out all over the world right now.

01:42:43.240 --> 01:42:46.840
So you construct a kind of devil, a kind of source of evil,

01:42:46.840 --> 01:42:48.600
and we're going to transcend beyond it.

01:42:48.600 --> 01:42:52.680
Yeah, and the millenarian, the millenarian is kind of,

01:42:52.680 --> 01:42:53.720
when you see a millenarian cult,

01:42:53.720 --> 01:42:55.640
they put a really specific point on it,

01:42:55.640 --> 01:42:56.760
which is end of the world, right?

01:42:56.760 --> 01:42:59.240
There is some change coming.

01:42:59.240 --> 01:43:02.040
And that change that's coming is so profound and so important

01:43:02.040 --> 01:43:06.120
that it's either going to lead to utopia or hell on earth, right?

01:43:06.760 --> 01:43:09.080
And it is going to, and then it's like,

01:43:09.080 --> 01:43:11.800
what if you actually knew that that was going to happen, right?

01:43:11.800 --> 01:43:14.040
What would you do, right?

01:43:14.040 --> 01:43:15.480
How would you prepare yourself for it?

01:43:15.480 --> 01:43:18.600
How would you come together with a group of like-minded people, right?

01:43:18.600 --> 01:43:19.640
How would you, what would you do?

01:43:19.640 --> 01:43:21.720
Would you plan like caches of weapons in the woods?

01:43:21.720 --> 01:43:24.840
Would you like, I don't know, create underground buckers?

01:43:24.840 --> 01:43:27.000
Would you spend your life trying to figure out a way

01:43:27.000 --> 01:43:28.200
to avoid having it happen?

01:43:28.200 --> 01:43:31.640
Yeah, that's a really compelling, exciting idea

01:43:32.760 --> 01:43:36.440
to have a club over, to have a little bit of travel.

01:43:36.440 --> 01:43:38.040
Like you get together on a Saturday night

01:43:38.040 --> 01:43:40.680
and drink some beers and talk about the end of the world

01:43:40.680 --> 01:43:43.400
and how you are the only ones who have figured it out.

01:43:43.400 --> 01:43:44.120
Yeah.

01:43:44.120 --> 01:43:45.720
And then once you lock in on that,

01:43:45.720 --> 01:43:47.240
like how can you do anything else with your life?

01:43:47.240 --> 01:43:48.840
Like this is obviously the thing that you have to do.

01:43:48.840 --> 01:43:51.240
And then there's a psychological effect you alluded to.

01:43:51.240 --> 01:43:52.200
There's a psychological effect.

01:43:52.200 --> 01:43:53.480
If you take a set of true believers

01:43:53.480 --> 01:43:55.560
and you lead them to themselves, they get more radical,

01:43:55.560 --> 01:43:57.400
because they self-radicalize each other.

01:43:57.400 --> 01:44:01.880
That said, it doesn't mean they're not sometimes right.

01:44:01.880 --> 01:44:02.760
Yeah, the end of the world might be.

01:44:02.760 --> 01:44:03.640
Yes, correct.

01:44:03.640 --> 01:44:04.760
Like they might be right.

01:44:04.760 --> 01:44:05.240
Yeah.

01:44:05.240 --> 01:44:05.800
But like we...

01:44:05.800 --> 01:44:06.840
Have some pamphlets for you.

01:44:06.840 --> 01:44:09.160
Exactly.

01:44:09.160 --> 01:44:11.800
I mean, there's, I mean, we'll talk about nuclear weapons

01:44:11.800 --> 01:44:13.880
because you have a really interesting little moment

01:44:13.880 --> 01:44:16.120
that I learned about in your essay.

01:44:16.120 --> 01:44:17.800
But you know, sometimes it could be right.

01:44:17.800 --> 01:44:18.360
Yeah.

01:44:18.360 --> 01:44:20.600
Because we're still, you were developing

01:44:20.600 --> 01:44:23.640
more and more powerful technologies in this case.

01:44:23.640 --> 01:44:25.160
And we don't know what the impact

01:44:25.160 --> 01:44:27.000
they will have on human civilization.

01:44:27.000 --> 01:44:29.080
While we can highlight all the different predictions

01:44:29.080 --> 01:44:30.280
about how it will be positive.

01:44:30.280 --> 01:44:34.120
But the risks are there and you discuss some of them.

01:44:34.120 --> 01:44:35.720
Well, the steelman is...

01:44:35.720 --> 01:44:36.360
The steelman...

01:44:36.360 --> 01:44:38.440
Actually, the steelman and his refutation are the same,

01:44:38.440 --> 01:44:40.360
which is you can't predict what's going to happen, right?

01:44:41.320 --> 01:44:44.360
You can't rule out that this will not end everything, right?

01:44:44.360 --> 01:44:45.800
But the response to that is,

01:44:45.800 --> 01:44:47.720
you have just made a completely non-scientific claim.

01:44:48.280 --> 01:44:49.960
You've made a religious claim, not a scientific claim.

01:44:49.960 --> 01:44:51.240
How does it get disproven?

01:44:51.240 --> 01:44:51.720
There is...

01:44:51.720 --> 01:44:52.120
And there's no...

01:44:52.120 --> 01:44:53.560
By definition, with these kinds of claims,

01:44:53.560 --> 01:44:55.320
there's no way to disprove them, right?

01:44:55.880 --> 01:44:56.840
And so there's no...

01:44:56.840 --> 01:44:57.800
You just go right on the list.

01:44:57.800 --> 01:44:59.080
There's no hypothesis.

01:44:59.080 --> 01:45:01.240
There's no testability of the hypothesis.

01:45:01.240 --> 01:45:04.440
There's no way to falsify the hypothesis.

01:45:04.440 --> 01:45:07.000
There's no way to measure progress along the arc.

01:45:07.800 --> 01:45:09.400
Like, it's just all completely missing.

01:45:09.400 --> 01:45:11.480
And so it's not scientific.

01:45:11.480 --> 01:45:11.720
And...

01:45:11.720 --> 01:45:13.480
Well, I don't think it's completely missing.

01:45:14.520 --> 01:45:15.160
It's somewhat missing.

01:45:15.160 --> 01:45:17.320
So, for example, the people that say,

01:45:17.320 --> 01:45:18.520
yeah, it's going to kill all of us.

01:45:19.720 --> 01:45:22.120
I mean, they usually have ideas about how to do that,

01:45:22.120 --> 01:45:26.600
whether it's the paperclip maximizer or it escapes.

01:45:27.560 --> 01:45:29.640
There's mechanism by which you can imagine it

01:45:29.640 --> 01:45:30.760
killing all humans.

01:45:30.760 --> 01:45:31.160
Models.

01:45:31.880 --> 01:45:38.760
And you can disprove it by saying there is a limit

01:45:39.720 --> 01:45:42.440
to the speed at which intelligence increases.

01:45:43.880 --> 01:45:45.240
Maybe show that...

01:45:47.640 --> 01:45:50.280
They sort of rigorously really describe model,

01:45:51.000 --> 01:45:52.920
like how it could happen,

01:45:52.920 --> 01:45:55.800
and say, no, here's a physics limitation.

01:45:55.800 --> 01:45:58.760
There's a physical limitation to how these systems

01:45:58.760 --> 01:46:00.840
would actually do damage to human civilization.

01:46:00.840 --> 01:46:04.760
And it is possible they will kill 10% to 20% of the population,

01:46:04.760 --> 01:46:08.840
but it seems impossible for them to kill 99%.

01:46:08.840 --> 01:46:10.040
It was practical counter-arguments, right?

01:46:10.040 --> 01:46:11.480
So you mentioned basically what I described

01:46:11.480 --> 01:46:13.000
as the thermodynamic counter-argument,

01:46:13.000 --> 01:46:14.120
which is sitting here today.

01:46:14.120 --> 01:46:16.520
It's like where would the evil AGI get the GPUs?

01:46:16.520 --> 01:46:16.920
Yeah.

01:46:16.920 --> 01:46:18.280
Because, like, they don't exist.

01:46:18.280 --> 01:46:20.520
So you're going to have a very frustrated baby evil AGI

01:46:20.520 --> 01:46:22.200
who's going to be, like, trying to buy NVIDIA stock

01:46:22.200 --> 01:46:24.440
or something to get them to finally make some chips.

01:46:25.000 --> 01:46:25.240
Right?

01:46:25.240 --> 01:46:27.720
So the serious form of that is the thermodynamic argument,

01:46:27.720 --> 01:46:29.640
which is, like, okay, where's the energy going to come from?

01:46:29.640 --> 01:46:31.080
Where's the processor going to be running?

01:46:31.080 --> 01:46:32.520
Where's the data center going to be happening?

01:46:32.520 --> 01:46:33.800
How is this going to be happening in secret,

01:46:33.800 --> 01:46:34.600
such that you know it's not...

01:46:34.600 --> 01:46:35.240
You know.

01:46:35.240 --> 01:46:37.240
So that's a practical counter-argument

01:46:37.240 --> 01:46:38.440
to the runaway AGI thing.

01:46:38.440 --> 01:46:39.080
I have a...

01:46:39.080 --> 01:46:39.560
But I have a...

01:46:39.560 --> 01:46:41.000
And we can argue that and discuss that.

01:46:41.000 --> 01:46:42.360
I have a deeper objection to it,

01:46:42.360 --> 01:46:44.360
which is this is all forecasting.

01:46:44.360 --> 01:46:45.240
It's all modeling.

01:46:45.240 --> 01:46:47.320
It's all future prediction.

01:46:47.320 --> 01:46:49.080
It's all future hypothesizing.

01:46:49.960 --> 01:46:50.680
It's not science.

01:46:51.880 --> 01:46:52.200
Sure.

01:46:52.200 --> 01:46:54.760
It is the opposite of science.

01:46:54.760 --> 01:46:56.280
So they'll pull up Carl Sagan.

01:46:56.280 --> 01:46:58.280
Extraordinary claims require extraordinary proof.

01:46:58.280 --> 01:46:58.920
Right?

01:46:58.920 --> 01:47:00.360
These are extraordinary claims.

01:47:00.360 --> 01:47:03.240
The policies that are being called for, right,

01:47:03.240 --> 01:47:05.800
to prevent this are of extraordinary magnitude,

01:47:05.800 --> 01:47:07.560
and I think we're going to cause extraordinary damage.

01:47:08.200 --> 01:47:09.960
And this is all being done on the basis of something

01:47:09.960 --> 01:47:11.560
that is literally not scientific.

01:47:11.560 --> 01:47:12.760
It's not a testable hypothesis.

01:47:12.760 --> 01:47:13.720
So the moment you say,

01:47:13.720 --> 01:47:15.000
AI is going to kill all of us,

01:47:15.000 --> 01:47:16.360
therefore we should ban it

01:47:16.360 --> 01:47:18.600
or we should regulate all that kind of stuff,

01:47:18.600 --> 01:47:19.880
that's when it starts getting serious.

01:47:19.880 --> 01:47:22.040
Or start, you know, military airstrikes and data centers.

01:47:22.520 --> 01:47:23.320
Oh, boy.

01:47:23.320 --> 01:47:23.560
Right?

01:47:24.840 --> 01:47:25.240
And like...

01:47:26.760 --> 01:47:29.720
Yeah, that's when it starts getting real weird.

01:47:29.720 --> 01:47:31.240
So here's the problem of millinery and cults.

01:47:31.240 --> 01:47:32.920
They have a hard time staying away from violence.

01:47:34.360 --> 01:47:35.800
Yeah, but violence is so fun.

01:47:38.920 --> 01:47:39.960
If you're on the right end of it,

01:47:40.520 --> 01:47:41.720
they have a hard time avoiding violence.

01:47:41.720 --> 01:47:43.000
The reason they have a hard time avoiding violence

01:47:43.000 --> 01:47:45.160
is if you actually believe the claim,

01:47:46.040 --> 01:47:48.520
right, then what would you do to stop the end of the world?

01:47:49.080 --> 01:47:50.360
Well, you would do anything, right?

01:47:51.000 --> 01:47:52.520
And so, and this is where you get...

01:47:52.520 --> 01:47:55.160
And again, if you just look at the history of millinery and cults,

01:47:55.160 --> 01:47:56.360
this is where you get the people's temple

01:47:56.360 --> 01:47:57.720
and everybody killing themselves in the jungle.

01:47:57.720 --> 01:47:59.000
And this is where you get Charles Manson

01:47:59.000 --> 01:48:01.240
and, you know, setting in to kill the pigs.

01:48:01.960 --> 01:48:03.480
Like, this is the problem with these.

01:48:03.480 --> 01:48:06.600
They have a very hard time drawing the line at actual violence.

01:48:06.600 --> 01:48:09.320
And I think in this case, there's...

01:48:09.320 --> 01:48:11.640
I mean, they're already calling for it, like today.

01:48:11.640 --> 01:48:13.400
And, you know, where this goes from here

01:48:13.400 --> 01:48:14.360
is they get more worked up.

01:48:14.360 --> 01:48:16.520
Like, I think it's like really concerning.

01:48:16.520 --> 01:48:18.120
Okay, but that's kind of the extremes.

01:48:18.520 --> 01:48:21.560
You know, the extremes of anything are always concerning.

01:48:22.680 --> 01:48:24.360
It's also possible to kind of believe

01:48:24.360 --> 01:48:27.240
that AI has a very high likelihood of killing all of us.

01:48:28.360 --> 01:48:28.760
But there's...

01:48:29.880 --> 01:48:34.840
And therefore, we should maybe consider slowing development

01:48:34.840 --> 01:48:35.800
or regulating.

01:48:35.800 --> 01:48:37.880
So not violence or any of these kinds of things,

01:48:37.880 --> 01:48:41.080
but saying like, all right, let's take a pause here.

01:48:41.080 --> 01:48:43.560
You know, biological weapons, nuclear weapons.

01:48:43.560 --> 01:48:45.080
Like, whoa, whoa, whoa, whoa, whoa.

01:48:45.080 --> 01:48:47.560
This is like serious stuff.

01:48:47.560 --> 01:48:48.920
We should be careful.

01:48:48.920 --> 01:48:53.080
So it is possible to kind of have a more rational response, right?

01:48:53.080 --> 01:48:55.000
If you believe this risk is real.

01:48:55.000 --> 01:48:55.400
Believe.

01:48:56.200 --> 01:48:56.520
Yes.

01:48:57.400 --> 01:48:59.480
Is it possible to have a scientific approach

01:48:59.480 --> 01:49:02.280
to the prediction of the future?

01:49:02.280 --> 01:49:04.120
I mean, we just went through this with COVID.

01:49:04.120 --> 01:49:05.080
What do we know about modeling?

01:49:06.440 --> 01:49:07.240
Well, I mean...

01:49:07.240 --> 01:49:08.840
What do we learn about modeling with COVID?

01:49:09.720 --> 01:49:11.000
There's a lot of lessons.

01:49:11.000 --> 01:49:11.800
They didn't work at all.

01:49:12.920 --> 01:49:13.800
They worked poorly.

01:49:13.800 --> 01:49:14.920
The models were terrible.

01:49:14.920 --> 01:49:16.200
The models were useless.

01:49:16.200 --> 01:49:19.880
I don't know if the models were useless or the people

01:49:19.880 --> 01:49:23.160
interpreting the models and then the centralized institutions

01:49:23.160 --> 01:49:26.040
that were creating policy rapidly based on the models

01:49:26.040 --> 01:49:29.640
and leveraging the models in order to support their narratives

01:49:29.640 --> 01:49:32.840
versus actually interpreting the airbars and the models

01:49:32.840 --> 01:49:33.560
and all that kind of stuff.

01:49:33.560 --> 01:49:35.640
What you had with COVID, in my view, you had with COVID

01:49:35.640 --> 01:49:37.640
is you had these experts showing up.

01:49:37.640 --> 01:49:38.760
They claimed to be scientists

01:49:38.760 --> 01:49:40.920
and they had no testable hypotheses whatsoever.

01:49:40.920 --> 01:49:42.280
They had a bunch of models.

01:49:42.280 --> 01:49:44.120
They had a bunch of forecasts and they had a bunch of theories

01:49:44.120 --> 01:49:45.880
and they laid these out in front of policymakers

01:49:45.880 --> 01:49:49.080
and policymakers freaked out and panicked and implemented

01:49:49.080 --> 01:49:51.080
a whole bunch of really terrible decisions

01:49:51.080 --> 01:49:52.600
that we're still living with the consequences of.

01:49:54.360 --> 01:49:57.320
There was never any empirical foundation to any of the models.

01:49:57.320 --> 01:49:58.600
None of them ever came true.

01:49:58.600 --> 01:50:01.560
Yeah, to push back, there were certainly Baptist and bootleggers

01:50:01.560 --> 01:50:04.120
in the context of this pandemic,

01:50:04.120 --> 01:50:05.880
but there's still a usefulness to models.

01:50:06.600 --> 01:50:08.840
So not if they're reliably wrong.

01:50:08.840 --> 01:50:10.520
Then they're actually anti-useful.

01:50:10.520 --> 01:50:11.400
They're actually damaging.

01:50:11.400 --> 01:50:12.920
But what do you do with the pandemic?

01:50:12.920 --> 01:50:15.560
What do you do with any kind of threat?

01:50:15.560 --> 01:50:19.880
Don't you want to kind of have several models to play with

01:50:19.880 --> 01:50:23.640
as part of the discussion of like, what the hell do we do here?

01:50:23.640 --> 01:50:24.440
I mean, do they work?

01:50:25.320 --> 01:50:27.560
Because they're an expectation that they actually like work,

01:50:27.560 --> 01:50:28.920
that they have actual predictive value.

01:50:29.720 --> 01:50:31.000
I mean, as far as I can tell with COVID,

01:50:31.000 --> 01:50:33.400
we just saw the policymakers just saw up themselves

01:50:33.400 --> 01:50:34.440
into believing that there was some...

01:50:34.440 --> 01:50:36.680
I mean, look, the scientists were at fault.

01:50:36.680 --> 01:50:38.520
The quote-unquote scientists showed up.

01:50:39.400 --> 01:50:40.440
So I had to submit that into this.

01:50:40.440 --> 01:50:41.400
So there was a...

01:50:41.400 --> 01:50:43.560
Remember the Imperial College models out of London

01:50:43.560 --> 01:50:44.520
were the ones that were...

01:50:44.520 --> 01:50:45.640
Like, these are the gold standard models.

01:50:46.200 --> 01:50:47.960
So a friend of mine runs a big software company

01:50:47.960 --> 01:50:49.960
and he was like, wow, this is like COVID's really scary.

01:50:49.960 --> 01:50:51.560
And he's like, you know, he contacted this research

01:50:51.560 --> 01:50:53.080
and he's like, you know, do you need some help?

01:50:53.080 --> 01:50:55.240
You've been just building this model on your own for 20 years.

01:50:55.240 --> 01:50:55.880
Do you need some food?

01:50:55.880 --> 01:50:57.880
Do you like our coders to basically restructure it

01:50:57.880 --> 01:50:59.400
so it can be fully adapted for COVID?

01:50:59.400 --> 01:51:01.800
And the guy said yes and sent over the code.

01:51:01.800 --> 01:51:03.720
And my friend said it was like the worst spaghetti code

01:51:03.720 --> 01:51:04.520
he's ever seen.

01:51:04.520 --> 01:51:06.840
That doesn't mean it's not possible to construct

01:51:06.840 --> 01:51:09.560
a good model of pandemic with the correct airbars

01:51:09.560 --> 01:51:12.520
with a high number of parameters that are continuously,

01:51:13.080 --> 01:51:16.520
many times a day updated as we get more data about a pandemic.

01:51:16.520 --> 01:51:20.680
I would like to believe when a pandemic hits the world,

01:51:20.680 --> 01:51:22.360
the best computer scientists in the world,

01:51:22.360 --> 01:51:25.400
the best software engineers respond aggressively

01:51:25.400 --> 01:51:29.000
and as input take the data that we know about the virus

01:51:29.000 --> 01:51:32.680
and it's an output, say here's what's happening

01:51:33.320 --> 01:51:35.320
in terms of how quickly it's spreading,

01:51:35.320 --> 01:51:37.320
what that lead in terms of hospitalization

01:51:37.320 --> 01:51:38.840
and deaths and all that kind of stuff.

01:51:38.840 --> 01:51:41.560
Here's how likely, how contagious it likely is.

01:51:41.560 --> 01:51:44.840
Here's how deadly it likely is based on different conditions,

01:51:44.840 --> 01:51:46.680
based on different ages and demographics

01:51:46.680 --> 01:51:47.640
and all that kind of stuff.

01:51:47.640 --> 01:51:49.560
So here's the best kinds of policy.

01:51:49.560 --> 01:51:54.040
It feels like you can have models, machine learning,

01:51:54.760 --> 01:51:58.920
that kind of, they don't perfectly predict the future,

01:51:58.920 --> 01:52:01.640
but they help you do something

01:52:01.640 --> 01:52:03.240
because there's pandemics that are like,

01:52:06.520 --> 01:52:08.440
meh, they don't really do much harm.

01:52:08.440 --> 01:52:10.760
And there's pandemics, you can imagine them,

01:52:10.760 --> 01:52:12.840
they can do a huge amount of harm.

01:52:12.840 --> 01:52:14.520
Like they can kill a lot of people.

01:52:14.520 --> 01:52:18.680
So you should probably have some kind of data-driven models

01:52:18.680 --> 01:52:21.560
that keep updating, that allow you to make decisions

01:52:21.560 --> 01:52:23.720
that are basically like, where, how bad is this thing?

01:52:24.600 --> 01:52:28.840
Now you can criticize how horrible all of that went

01:52:28.840 --> 01:52:30.120
with the response to this pandemic,

01:52:30.120 --> 01:52:32.520
but I just feel like there might be some value to models.

01:52:32.520 --> 01:52:35.000
So to be useful at some point, it has to be predictive, right?

01:52:35.000 --> 01:52:39.240
So the easy thing for me to do is to say,

01:52:39.240 --> 01:52:39.880
obviously, you're right.

01:52:39.880 --> 01:52:41.480
Obviously, I want to see that just as much as you do

01:52:41.480 --> 01:52:43.000
because anything that makes it easier to navigate

01:52:43.000 --> 01:52:44.920
through society, through a wrenching risk like that,

01:52:44.920 --> 01:52:46.120
it sounds great.

01:52:47.240 --> 01:52:49.480
The harder objection to it is just simply,

01:52:49.480 --> 01:52:52.520
you are trying to model a complex dynamic system

01:52:52.520 --> 01:52:55.160
with eight billion moving parts, like not possible.

01:52:55.160 --> 01:52:55.720
It's very tough.

01:52:55.720 --> 01:52:57.400
Can't be done, complex systems can't be done.

01:52:58.520 --> 01:53:01.400
Machine learning says hold my beer, but is possible?

01:53:01.400 --> 01:53:01.720
No?

01:53:01.720 --> 01:53:02.280
I don't know.

01:53:02.280 --> 01:53:03.480
I would like to believe that it is.

01:53:04.120 --> 01:53:05.640
Put it this way, I think where you and I would agree

01:53:05.640 --> 01:53:08.200
is I think we would like that to be the case.

01:53:08.280 --> 01:53:09.640
We are strongly in favor of it.

01:53:10.280 --> 01:53:12.040
I think we would also agree that no such thing

01:53:12.040 --> 01:53:14.120
with respect to COVID or pandemics, no such thing,

01:53:14.120 --> 01:53:16.280
at least neither you nor I think are aware,

01:53:16.280 --> 01:53:17.880
I'm not aware of anything like that today.

01:53:17.880 --> 01:53:21.560
My main worry with the response to the pandemic is that,

01:53:22.840 --> 01:53:27.000
same as with aliens, is that even if such a thing existed,

01:53:27.560 --> 01:53:33.880
and as possible it existed, the policymakers were not paying

01:53:33.880 --> 01:53:34.360
attention.

01:53:34.360 --> 01:53:37.480
Like there was no mechanism that allowed those kinds of models

01:53:37.480 --> 01:53:38.360
to percolate out.

01:53:38.360 --> 01:53:40.040
I think we have the opposite problem during COVID.

01:53:40.040 --> 01:53:43.560
I think the policymakers, I think these people with basically

01:53:43.560 --> 01:53:45.480
fake science had too much access to the policymakers.

01:53:46.600 --> 01:53:49.560
Right, but the policymakers also wanted,

01:53:49.560 --> 01:53:51.640
they had a narrative in mind and they also wanted to use

01:53:51.640 --> 01:53:54.600
whatever model that fit that narrative to help them out.

01:53:54.600 --> 01:53:57.640
So it felt like there was a lot of politics and not enough science.

01:53:57.640 --> 01:54:00.040
Although a big part of what was happening, a big reason we got

01:54:00.040 --> 01:54:02.200
lockdowns for as long as we did was because these scientists came

01:54:02.200 --> 01:54:04.120
in with these like doomsday scenarios that were like just

01:54:04.120 --> 01:54:05.320
like completely off the hook.

01:54:05.320 --> 01:54:07.160
Scientists and quotes, that's not,

01:54:07.160 --> 01:54:08.200
quote unquote, scientists.

01:54:08.200 --> 01:54:10.840
That's not, let's give love to science.

01:54:10.840 --> 01:54:12.120
That is the way out.

01:54:12.120 --> 01:54:14.040
Science is a process of testing hypotheses.

01:54:14.040 --> 01:54:14.680
Yeah.

01:54:14.680 --> 01:54:17.400
Modeling does not involve testable hypotheses, right?

01:54:17.400 --> 01:54:19.480
Like I don't even know that, I actually don't need,

01:54:19.480 --> 01:54:21.480
I didn't even know that modeling actually qualifies to science.

01:54:22.360 --> 01:54:25.160
Maybe that's a side conversation we could have some time over a beer.

01:54:25.160 --> 01:54:27.960
That's really interesting, but what do we do about the future?

01:54:27.960 --> 01:54:28.920
I mean, what?

01:54:28.920 --> 01:54:31.160
So number one is when we start with number one, humility,

01:54:31.720 --> 01:54:33.560
goes back to this thing of how do we determine the truth.

01:54:34.120 --> 01:54:36.200
Number two is we don't believe, you know, it's the old,

01:54:36.200 --> 01:54:38.120
I've got a hammer, everything looks like a nail, right?

01:54:39.240 --> 01:54:41.320
I've got, oh, this is one of the reasons I gave you,

01:54:41.320 --> 01:54:44.680
I gave Lex a book, which is the topic of the book is what happens

01:54:44.680 --> 01:54:47.880
when scientists basically stray off the path of technical knowledge

01:54:47.880 --> 01:54:49.960
and start to weigh in on politics and societal issues.

01:54:49.960 --> 01:54:51.400
In this case, philosophers.

01:54:51.400 --> 01:54:52.600
Well, in this case, philosophers.

01:54:52.600 --> 01:54:54.600
But he actually talks in this book about like Einstein,

01:54:54.600 --> 01:54:56.280
he talks about the nuclear age and Einstein,

01:54:56.280 --> 01:55:00.200
he talks about the physicists actually doing very similar things

01:55:00.200 --> 01:55:00.760
at the time.

01:55:00.760 --> 01:55:03.880
The book is when reason goes on holiday philosophers

01:55:03.880 --> 01:55:06.120
and politics by Nevin.

01:55:06.680 --> 01:55:07.560
And it's just a story.

01:55:07.560 --> 01:55:08.200
It's a story.

01:55:08.200 --> 01:55:09.800
There's there are other books on this topic,

01:55:09.800 --> 01:55:10.600
but this is a new one.

01:55:10.600 --> 01:55:11.080
That's really good.

01:55:11.080 --> 01:55:13.720
That's just the story of what happens when experts in a certain domain

01:55:13.720 --> 01:55:16.200
decide to weigh in and become basically social engineers

01:55:16.200 --> 01:55:19.160
and end up political, you know, basically political advisers.

01:55:19.160 --> 01:55:21.080
And it's just a story of just unending catastrophe.

01:55:21.880 --> 01:55:22.040
Right.

01:55:22.040 --> 01:55:23.480
And I think that's what happened with COVID again.

01:55:24.840 --> 01:55:27.640
And I found this book a highly entertaining and eye-opening read

01:55:27.640 --> 01:55:29.960
filled with the amazing anecdotes of irrationality

01:55:29.960 --> 01:55:32.120
and craziness by famous recent philosophers.

01:55:33.080 --> 01:55:35.320
After you read this book, you will not look at Einstein the same.

01:55:35.320 --> 01:55:36.040
Oh, boy.

01:55:36.040 --> 01:55:36.200
Yeah.

01:55:37.160 --> 01:55:38.280
Don't destroy my heroes.

01:55:38.280 --> 01:55:39.720
You will not be a hero of yours anymore.

01:55:41.240 --> 01:55:41.720
I'm sorry.

01:55:41.720 --> 01:55:43.560
You probably shouldn't read the book.

01:55:43.560 --> 01:55:43.880
All right.

01:55:43.880 --> 01:55:47.800
But here's the thing, the AI risk people,

01:55:47.800 --> 01:55:49.000
they don't even have the COVID model.

01:55:50.040 --> 01:55:51.080
At least not that I'm aware of.

01:55:51.080 --> 01:55:51.480
No.

01:55:51.480 --> 01:55:53.080
Like there's not even the equivalent of the COVID model.

01:55:53.080 --> 01:55:54.280
They don't even have the spaghetti code.

01:55:55.560 --> 01:55:58.520
They've got a theory and a warning and a this and a that.

01:55:58.520 --> 01:56:01.960
And like, if you ask like, okay, well, here's the ultimate example is,

01:56:01.960 --> 01:56:03.240
okay, how do we know, right?

01:56:03.240 --> 01:56:04.520
How do we know that an AI is running away?

01:56:04.520 --> 01:56:07.640
Like, how do we know that the fume takeoff thing is actually happening?

01:56:07.640 --> 01:56:10.280
And the only answer that any of these guys have given that I've ever seen is,

01:56:10.280 --> 01:56:14.040
oh, it's when the loss rate, the loss function in the training drops.

01:56:15.160 --> 01:56:15.320
Right.

01:56:15.320 --> 01:56:17.240
That's when you need to like shut down the data center.

01:56:17.240 --> 01:56:17.480
Right.

01:56:17.480 --> 01:56:20.120
And it's like, well, that's also what happens when you're successfully training a model.

01:56:21.000 --> 01:56:25.480
Like, what even is this is not science.

01:56:26.360 --> 01:56:27.640
This is not, it's not anything.

01:56:27.640 --> 01:56:28.200
It's not a model.

01:56:28.200 --> 01:56:29.160
It's not anything.

01:56:29.160 --> 01:56:30.760
There's nothing to arguing with it.

01:56:30.760 --> 01:56:31.880
It's like, you know, punching jello.

01:56:31.880 --> 01:56:33.400
Like there's, what do you even respond to?

01:56:34.040 --> 01:56:35.400
So just put pushback on that.

01:56:35.400 --> 01:56:39.880
I don't think they have good metrics of, yeah, when the fume is happening,

01:56:39.880 --> 01:56:42.040
but I think it's possible to have that.

01:56:42.040 --> 01:56:47.240
Like I just, just as you speak now, I mean, it's possible to imagine there could be measures.

01:56:47.240 --> 01:56:48.600
It's been 20 years.

01:56:48.600 --> 01:56:49.080
No, for sure.

01:56:49.080 --> 01:56:54.040
But it's been only weeks since we had a big enough breakthrough in language models.

01:56:54.040 --> 01:56:56.200
We can start to actually have this.

01:56:56.200 --> 01:57:01.640
The thing is the AI doomer stuff didn't have any actual systems to really work with it.

01:57:01.640 --> 01:57:02.920
Now there's real systems.

01:57:02.920 --> 01:57:05.560
You can start to analyze like, how does this stuff go wrong?

01:57:05.560 --> 01:57:09.240
And I think you kind of agree that there is a lot of risks that we can analyze.

01:57:09.240 --> 01:57:11.640
The benefits outweigh the risks in many cases.

01:57:11.640 --> 01:57:12.920
Well, the risks are not existential.

01:57:13.880 --> 01:57:14.200
Yes.

01:57:14.200 --> 01:57:16.520
Well, not, not, not in the fume, not in the fume paper clip.

01:57:16.520 --> 01:57:17.000
Not this.

01:57:17.000 --> 01:57:17.480
Let me, okay.

01:57:17.480 --> 01:57:19.000
There's another slide of hand that you just alluded to.

01:57:19.000 --> 01:57:20.600
There's another slide of hand that happens, which is very.

01:57:20.600 --> 01:57:24.440
I think I'm very good at the slide of hand thing, which is very not scientific.

01:57:24.440 --> 01:57:26.200
So the book, super intelligence, right?

01:57:26.200 --> 01:57:29.160
Which is like the Nick Bostrom book, which is like the origin of a lot of this stuff,

01:57:29.160 --> 01:57:31.240
which is written, you know, whatever, 10 years ago or something.

01:57:31.800 --> 01:57:34.920
So he does this really fascinating thing in the book, which is he basically says,

01:57:36.520 --> 01:57:40.520
there are many possible routes to machine intelligence, to artificial intelligence.

01:57:40.520 --> 01:57:43.320
And he describes all the different routes to artificial intelligence,

01:57:43.320 --> 01:57:45.880
all the different possible, everything from biological augmentation through to,

01:57:46.440 --> 01:57:47.640
you know, that all these different things.

01:57:49.080 --> 01:57:52.040
One of the ones that he does not describe is large language models,

01:57:52.040 --> 01:57:55.240
because of course the book was written before they were invented and so they didn't exist.

01:57:56.760 --> 01:58:00.040
In the book, he just, he describes them all and then he proceeds to treat them all as if

01:58:00.040 --> 01:58:00.920
they're exactly the same thing.

01:58:01.800 --> 01:58:04.840
He presents them all as sort of an equivalent risk to be dealt with in an equivalent way to

01:58:04.840 --> 01:58:05.880
be thought about the same way.

01:58:05.880 --> 01:58:09.240
And then the risk, the quote unquote risk that's actually emerged is actually a completely

01:58:09.240 --> 01:58:10.920
different technology than he was even imagining.

01:58:10.920 --> 01:58:14.200
And yet all of his theories and beliefs are being transplanted by this movement,

01:58:14.200 --> 01:58:15.240
like straight onto this new technology.

01:58:15.240 --> 01:58:20.200
And so again, like there's no other area of science or technology where you do that.

01:58:21.000 --> 01:58:24.440
Like when you're dealing with like organic chemistry versus inorganic chemistry,

01:58:24.440 --> 01:58:27.560
you don't just like say, oh, with respect to like either one,

01:58:27.560 --> 01:58:29.720
basically maybe, you know, growing up and eating the world or something,

01:58:29.720 --> 01:58:31.080
like they're just going to operate the same way.

01:58:31.080 --> 01:58:31.560
Like you don't.

01:58:32.280 --> 01:58:36.920
But you can start talking about like as, as we get more and more actual systems

01:58:36.920 --> 01:58:38.360
that start to get more and more intelligent,

01:58:38.360 --> 01:58:41.160
you can start to actually have more scientific arguments here.

01:58:42.040 --> 01:58:46.440
You know, high level, you can talk about the threat of autonomous weapons systems

01:58:46.440 --> 01:58:49.720
back before we had any automation in the military.

01:58:49.720 --> 01:58:51.880
And that would be like very fuzzy kind of logic.

01:58:51.880 --> 01:58:56.280
But the more and more you have drones that are becoming more and more autonomous,

01:58:56.280 --> 01:58:59.240
you can start imagining, okay, what does that actually look like?

01:58:59.240 --> 01:59:01.720
And what's the actual threat of autonomous weapons systems?

01:59:01.720 --> 01:59:03.160
How does it go wrong?

01:59:03.160 --> 01:59:07.240
And still it's, it's, it's very vague, but you start to get a sense of like,

01:59:07.960 --> 01:59:14.920
all right, it should probably be illegal or wrong or not allowed to do like

01:59:15.880 --> 01:59:22.600
mass deployment of fully autonomous drones that are doing aerial strikes on large areas.

01:59:22.600 --> 01:59:23.880
I think it should be required.

01:59:23.880 --> 01:59:24.280
Right.

01:59:24.280 --> 01:59:24.680
So that's,

01:59:24.680 --> 01:59:25.240
No, no, no, no.

01:59:25.240 --> 01:59:29.160
I think it should be required that only aerial vehicles are automated.

01:59:30.200 --> 01:59:30.440
Okay.

01:59:30.440 --> 01:59:31.320
So you want to go the other way?

01:59:31.320 --> 01:59:31.960
I want to go the other way.

01:59:32.840 --> 01:59:33.720
So that, okay.

01:59:33.720 --> 01:59:36.600
I think it's obvious that the machine is going to make a better decision than the human pilot.

01:59:38.760 --> 01:59:41.480
I think it's obvious that it's in the best interest of both the attacker and the defender

01:59:41.480 --> 01:59:44.440
and humanity at large if machines are making more decisions than not people.

01:59:44.440 --> 01:59:46.360
I think people make terrible decisions in times of war.

01:59:47.160 --> 01:59:49.880
But like there's a, there's ways this can go wrong too, right?

01:59:49.880 --> 01:59:52.040
Well, it's worse go terribly wrong now.

01:59:53.720 --> 01:59:56.120
This goes back to the, this is that whole thing about like the self-driving,

01:59:56.120 --> 01:59:58.840
does the self-driving car need to be perfect versus does it need to be better than the human

01:59:58.840 --> 01:59:59.480
driver?

01:59:59.480 --> 01:59:59.720
Yeah.

01:59:59.720 --> 02:00:03.880
Does the automated drone need to be perfect or does it need to be better than a human pilot

02:00:03.880 --> 02:00:07.160
at making decisions under enormous amounts of stress and uncertainty?

02:00:07.160 --> 02:00:07.560
Yeah.

02:00:07.560 --> 02:00:13.960
Well, the, on average, the, the worry that AI folks have is the runaway.

02:00:13.960 --> 02:00:15.480
They're going to come alive, right?

02:00:15.480 --> 02:00:17.240
Then again, that's the sleight of hand, right?

02:00:17.240 --> 02:00:18.600
Or not, not come alive.

02:00:18.600 --> 02:00:19.560
Well, hold on a second.

02:00:19.560 --> 02:00:20.440
You're going to become.

02:00:20.440 --> 02:00:22.200
You lose control as well.

02:00:22.200 --> 02:00:24.680
But then they're going to develop goals of their own.

02:00:24.680 --> 02:00:26.040
They're going to develop a mind of their own.

02:00:26.040 --> 02:00:27.400
They're going to develop their own, right?

02:00:27.400 --> 02:00:33.800
No, more, more like Chernobyl style meltdown, like just bugs in the code,

02:00:34.360 --> 02:00:42.280
accidentally, you know, force you, the results in the bombing of like large civilian areas.

02:00:42.280 --> 02:00:42.760
Okay.

02:00:42.840 --> 02:00:49.160
And to a degree that's not possible in the, in the current military strategies.

02:00:49.160 --> 02:00:49.400
I don't know.

02:00:49.400 --> 02:00:50.200
The control by humans.

02:00:50.200 --> 02:00:53.240
Well, actually, we've been doing a lot of mass bombings of cities for a very long time.

02:00:53.240 --> 02:00:53.800
Yes.

02:00:53.800 --> 02:00:55.240
And a lot of civilians died.

02:00:55.240 --> 02:00:56.120
And a lot of civilians died.

02:00:56.120 --> 02:00:59.960
And if you watch the documentary, The Fog of War, McNamara, it spends a big part of it

02:00:59.960 --> 02:01:03.480
talking about the firebombing of the Japanese cities, burning them straight to the ground,

02:01:04.360 --> 02:01:04.600
right?

02:01:04.600 --> 02:01:08.360
The, the devastation in Japan, the American military firebombing the cities in Japan

02:01:08.360 --> 02:01:11.640
was considerably bigger devastation than the use of nukes, right?

02:01:11.640 --> 02:01:12.920
So we've been doing that for a long time.

02:01:12.920 --> 02:01:14.040
We also did that to Germany.

02:01:14.040 --> 02:01:16.040
By the way, Germany did that to us, right?

02:01:16.040 --> 02:01:17.480
Like, that's an old tradition.

02:01:17.480 --> 02:01:19.960
The minute we got airplanes, we started doing a discriminant bombing.

02:01:19.960 --> 02:01:21.640
So one of the things that we're still doing it.

02:01:21.640 --> 02:01:27.880
The modern U.S. military can do with technology, with automation, but technology more broadly

02:01:27.880 --> 02:01:30.120
is higher and higher precision strikes.

02:01:30.120 --> 02:01:30.440
Yeah.

02:01:30.440 --> 02:01:33.880
And so precision is obviously, and this is the, the JDAM, right?

02:01:33.880 --> 02:01:37.640
So there was this big advance, this is big advance called the JDAM, which basically was

02:01:37.640 --> 02:01:41.480
strapping a GPS transceiver to a, to a, to an unguided bomb and turning it into a guided,

02:01:41.480 --> 02:01:42.360
guided bomb.

02:01:42.360 --> 02:01:43.080
And yeah, that's great.

02:01:43.080 --> 02:01:46.600
Like, look, that's been a big advance, but, and that's like a baby version of this question,

02:01:46.600 --> 02:01:49.560
which is, okay, do you want like the human pilot, like guessing where the bomb's going to land,

02:01:49.560 --> 02:01:51.800
or do you want like the machine, like guiding the bomb to its destination?

02:01:52.680 --> 02:01:53.800
That's a baby version of the question.

02:01:53.800 --> 02:01:56.680
The next version of the question is, do you want the human or the machine deciding whether to drop

02:01:56.680 --> 02:01:57.400
the bomb?

02:01:57.400 --> 02:02:00.920
Everybody just assumes the human's going to do a better job for what I think are fundamentally

02:02:00.920 --> 02:02:02.040
suspicious reasons.

02:02:02.040 --> 02:02:03.800
Emotional psychological reasons.

02:02:03.800 --> 02:02:06.520
I think it's very clear that the machine is going to do a better job making that decision,

02:02:06.520 --> 02:02:10.600
because the humans making that, making that decision are godawful, just terrible.

02:02:11.160 --> 02:02:11.320
Yeah.

02:02:11.320 --> 02:02:11.720
Right.

02:02:11.720 --> 02:02:13.720
And so, so yeah, so this is the, this is the thing.

02:02:13.720 --> 02:02:16.200
And then let's get to the, there was, can I, one more slide of hand?

02:02:16.200 --> 02:02:16.920
Yes, sure.

02:02:16.920 --> 02:02:18.600
Okay, please.

02:02:18.600 --> 02:02:19.960
I'm a magician, you could say.

02:02:19.960 --> 02:02:20.600
One more slide of hand.

02:02:20.600 --> 02:02:24.040
These things are going to be so smart, right, that they're going to be able to destroy the

02:02:24.040 --> 02:02:27.320
world and wreak havoc and like do all this stuff and plan and do all this stuff and evade

02:02:27.320 --> 02:02:30.440
us and have all their secret things and their secret factories and all this stuff.

02:02:30.440 --> 02:02:33.960
But they're so stupid that they're going to get like tangled up in their code.

02:02:33.960 --> 02:02:36.200
And that's the, they're not going to come alive, but there's going to be some bug

02:02:36.200 --> 02:02:39.240
that's going to cause them to like turn us all in a paper, like that they're not going,

02:02:39.240 --> 02:02:41.880
that they're going to be genius in every way other than the actual bad goal.

02:02:43.400 --> 02:02:47.320
And it's just like, and that's just like a like ridiculous like discrepancy.

02:02:47.320 --> 02:02:51.480
And, and, and, and you can prove this today, you can actually address this today for the

02:02:51.480 --> 02:02:57.080
first time with LMS, which is you can actually ask LMS to resolve moral dilemmas.

02:02:57.640 --> 02:02:58.120
Yeah.

02:02:58.120 --> 02:03:01.880
So you can create the scenario, you know, dot, dot, dot, this, that, this, that, this, that,

02:03:01.880 --> 02:03:04.200
what would you as the AI do in the circumstance?

02:03:04.200 --> 02:03:07.080
And they don't just say, destroy all humans, destroy all humans.

02:03:07.080 --> 02:03:11.640
They will give you actually very nuanced moral, practical, tradeoff oriented answers.

02:03:12.200 --> 02:03:16.440
And so we actually already have the kind of AI that can actually like think this through

02:03:16.440 --> 02:03:18.360
and can actually like, you know, reason about goals.

02:03:19.160 --> 02:03:24.520
Well, the hope is that AGI or like a very super intelligent systems have some of the

02:03:24.520 --> 02:03:29.160
nuance that LMS have. And the intuition is they most like the will because even

02:03:29.160 --> 02:03:31.480
these LMS have the nuance.

02:03:32.440 --> 02:03:35.160
LMS are really, this is actually worth, worth spending a moment on.

02:03:35.160 --> 02:03:37.800
LMS are really interesting to have moral conversations with.

02:03:38.360 --> 02:03:42.440
And that, I just, I didn't expect I'd be having a moral conversation with a machine

02:03:42.440 --> 02:03:43.080
in my lifetime.

02:03:43.800 --> 02:03:46.920
And let's remember, we're not really having a conversation with a machine where

02:03:46.920 --> 02:03:50.680
we're having a conversation with the entirety of the collective intelligence of the human species.

02:03:50.680 --> 02:03:51.960
Exactly. Yes.

02:03:51.960 --> 02:03:52.600
Correct.

02:03:52.600 --> 02:03:57.400
But it's possible to imagine autonomous weapon systems that are not using LMS.

02:03:58.200 --> 02:04:02.920
If they're smart enough to be scary, where are they not smart enough to be wise?

02:04:04.920 --> 02:04:08.120
Like that's the part where it's like, I don't know how you get the one without the other.

02:04:08.120 --> 02:04:11.320
Is it possible to be super intelligent without being super wise?

02:04:11.320 --> 02:04:14.600
Well, again, you're back to, I mean, then you're back to a classic autistic computer,

02:04:14.600 --> 02:04:17.160
right? Like you're back to just like a blind rule follower.

02:04:17.800 --> 02:04:20.760
I've got this like core, is the paperclip thing, I've got this core rule and I'm just

02:04:20.760 --> 02:04:21.960
going to follow it to the end of the earth.

02:04:21.960 --> 02:04:24.360
And it's like, well, but everything you're going to be doing to execute that rule is

02:04:24.360 --> 02:04:26.840
going to be super genius level that humans aren't going to be able to counter.

02:04:27.160 --> 02:04:30.920
It's just, it's a mismatch in the definition of what the system is capable of.

02:04:31.560 --> 02:04:33.320
Unlikely, but not impossible, I think.

02:04:33.320 --> 02:04:35.080
But again, here you get to like, okay, like...

02:04:36.120 --> 02:04:39.480
No, I'm not saying when it's unlikely, but not impossible.

02:04:39.480 --> 02:04:43.720
If it's unlikely, that means the fear should be correctly calibrated.

02:04:43.720 --> 02:04:45.640
Extraordinary claims require extraordinary proof.

02:04:45.640 --> 02:04:50.120
Well, okay. So one interesting sort of tangent I would love to take on this because you mentioned

02:04:50.120 --> 02:04:55.960
this in the essay about nuclear, which was also, I mean, you don't shy away from a little bit of

02:04:56.920 --> 02:05:01.880
of a spicy take. So Robert Oppenheimer famously said,

02:05:01.880 --> 02:05:04.360
now I am become death, the destroyer of worlds.

02:05:04.360 --> 02:05:09.800
As he witnessed the first detonation of a nuclear weapon on July 16th, 1945.

02:05:09.800 --> 02:05:13.480
And you write an interesting historical perspective, quote,

02:05:13.480 --> 02:05:18.680
recall that John von Neumann responded to Robert Oppenheimer's famous hand wringing

02:05:18.680 --> 02:05:25.000
about the role of creating nuclear weapons, which, you note, helped end World War II and

02:05:25.000 --> 02:05:31.800
prevent World War III with some people confess guilt to claim credit for the sin.

02:05:31.800 --> 02:05:35.480
And you also mentioned that Truman was harsher after meeting Oppenheimer.

02:05:35.480 --> 02:05:38.440
He said that don't let that crybaby in here again.

02:05:39.640 --> 02:05:43.080
Real quote, real quote, by the way, from Dean Atchison.

02:05:44.840 --> 02:05:48.360
Boy, because Oppenheimer didn't just say the famous line.

02:05:48.360 --> 02:05:48.760
Yeah.

02:05:48.760 --> 02:05:52.200
He then spent years going around basically moaning, you know, going on TV and going into

02:05:52.520 --> 02:05:55.320
the White House and basically like just like doing this hair shirt, you know,

02:05:55.320 --> 02:05:59.240
thing self, you know, this sort of self-critical like, oh my God, I can't believe how awful I am.

02:05:59.240 --> 02:06:06.120
So he's the, he's widely considered perhaps because the hand wringing is the father of the

02:06:06.120 --> 02:06:12.360
atomic bomb. This is Von Neumann's criticism of him as he tried to have his cake and eat it too,

02:06:12.360 --> 02:06:16.920
like he wanted to. And so, Von Neumann, of course, is a very different kind of personality.

02:06:16.920 --> 02:06:20.600
And he's just like, yeah, this is like an incredibly useful thing. I'm glad we did it.

02:06:20.600 --> 02:06:27.160
Yeah. Well, Von Neumann is as widely credited as being one of the smartest humans of the 20th

02:06:27.160 --> 02:06:32.120
century. Certain people, everybody says like, this is the smartest person I've ever met when

02:06:32.120 --> 02:06:36.600
they've met him. Anyway, that doesn't mean smart, doesn't mean wise.

02:06:39.160 --> 02:06:45.160
So I would love to sort of, can you make the case both for and against the critique of Oppenheimer

02:06:45.160 --> 02:06:50.680
here? Because we're talking about nuclear weapons. Boy, do they seem dangerous.

02:06:50.680 --> 02:06:54.280
Well, so the critique goes deeper. And I left this out. Here's the real substance. I left

02:06:54.280 --> 02:06:59.800
it out because I didn't want to dwell on nukes in my AI paper. But here's the deeper thing

02:06:59.800 --> 02:07:02.840
that happened. And I'm really curious, this movie coming out this summer, I'm really curious to see

02:07:02.840 --> 02:07:06.440
how far he pushes this because this is the real drama in the story, which is it wasn't just a

02:07:06.440 --> 02:07:09.480
question of our nukes good or bad. It was a question of, should Russia also have them?

02:07:10.200 --> 02:07:16.440
And what actually happened was Russia got the, America invented the bomb, Russia got the bomb,

02:07:16.440 --> 02:07:21.160
they got the bomb through espionage, they got American and, you know, they got American scientists

02:07:21.160 --> 02:07:26.040
and foreign scientists working on the American project, some combination of the two basically gave

02:07:26.040 --> 02:07:31.080
the Russians the designs for the bomb. And that's how the Russians got the bomb. There's this dispute

02:07:31.080 --> 02:07:37.000
to this day of Oppenheimer's role in that. If you read all the histories, the kind of composite

02:07:37.080 --> 02:07:40.440
picture, and by the way, we now know a lot actually about Soviet espionage in that era,

02:07:40.440 --> 02:07:44.200
because there's been all this declassified material in the last 20 years that actually shows a lot

02:07:44.200 --> 02:07:47.320
of very interesting things. But if you kind of read all the histories, what you kind of get is

02:07:47.320 --> 02:07:51.560
Oppenheimer himself probably was not a, he probably did not hand over the nuclear secrets

02:07:51.560 --> 02:07:56.200
himself. However, he was close to many people who did, including family members. And there were

02:07:56.200 --> 02:08:01.240
other members of the Manhattan Project who were Russian Soviet SS and did hand over the bomb. And

02:08:01.240 --> 02:08:07.000
so the view that Oppenheimer and people like him had that this thing is awful and terrible and,

02:08:07.000 --> 02:08:12.440
oh my God, and, you know, all this stuff, you could argue fed into this ethos at the time that

02:08:12.440 --> 02:08:15.800
resulted in people thinking that the Baptist is thinking that the only principle thing to do is

02:08:15.800 --> 02:08:22.760
to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion

02:08:22.760 --> 02:08:26.440
and the role that the inventors of this technology play, this is the point of this book when they

02:08:26.440 --> 02:08:31.080
kind of take on this sort of public intellectual moral kind of thing, it can have real consequences.

02:08:31.560 --> 02:08:35.160
Because we live in a very different world today because Russia got the bomb than we would have

02:08:35.160 --> 02:08:39.160
lived in had they not gotten the bomb. The entire 20th century, second half of the 20th century

02:08:39.160 --> 02:08:43.080
would have played out very different had those people not given Russia the bomb. And so the

02:08:43.080 --> 02:08:49.080
stakes were very high then. The good news today is nobody sitting here today, I don't think worrying

02:08:49.080 --> 02:08:52.600
about like an analogous situation with respect to like, I'm not really worried that Sam Altman is

02:08:52.600 --> 02:08:57.960
going to decide to give the Chinese the design for AI, although he did just speak at a Chinese

02:08:57.960 --> 02:09:02.600
conference, which is interesting. But however, I don't think that's what's at play here. But what's

02:09:02.600 --> 02:09:06.680
at play here are all these other fundamental issues around what do we believe about this and then

02:09:06.680 --> 02:09:10.440
what laws and regulations and restrictions that we're going to put on it. And that's where I draw

02:09:10.440 --> 02:09:14.760
it like a direct straight line. And anyway, and my reading of the history on Nukes is like the

02:09:14.760 --> 02:09:18.840
people who were doing the full hair shirt public, this is awful, this is terrible, actually had

02:09:18.840 --> 02:09:23.560
like catastrophically bad results from taking those views. And that's what I'm worried is going

02:09:23.560 --> 02:09:27.240
to happen again. But is there a case to be made that you really need to wake the public up to

02:09:27.240 --> 02:09:32.680
the dangers of nuclear weapons when they were first dropped? Like really, like, educate them on

02:09:32.680 --> 02:09:37.000
like, this is extremely dangerous and destructive weapon. I think the education kind of happened

02:09:37.000 --> 02:09:41.960
quick and early. Like, how? It was pretty obvious. How? We dropped one bomb and destroyed an entire

02:09:41.960 --> 02:09:50.760
city. Yeah, so 80,000 people dead. But I don't like the reporting of that, you can report that in

02:09:50.760 --> 02:09:56.760
all kinds of ways. You can do all kinds of slants like war is horrible, war is terrible. You can

02:09:57.720 --> 02:10:02.360
make it seem like nuclear, the use of nuclear weapons is just a part of a war and all that

02:10:02.360 --> 02:10:07.400
kind of stuff. Something about the reporting and the discussion of nuclear weapons resulted in us

02:10:07.400 --> 02:10:16.520
being terrified in awe of the power of nuclear weapons. And that potentially fed in a positive

02:10:16.520 --> 02:10:22.520
way towards the game theory of mutually assured destruction. Well, so this gets to what actually

02:10:23.320 --> 02:10:26.680
happens. Some of us may be playing devils advocate here. Yeah, sure, of course. Let's get to what

02:10:26.680 --> 02:10:29.880
actually happened and then kind of back into that. So what actually happened, I believe, and again,

02:10:29.880 --> 02:10:33.080
I think this is a reasonable reading of history, is what actually happened was nukes then prevented

02:10:33.080 --> 02:10:36.920
World War III. And they prevented World War III through the game theory of mutually assured

02:10:36.920 --> 02:10:42.600
destruction. Had nukes not existed, there would have been no reason why the Cold War did not go

02:10:42.600 --> 02:10:47.960
hot. And the military planners at the time thought, both on both sides, thought that there

02:10:47.960 --> 02:10:50.600
was going to be World War III on the plains of Europe and they thought there was going to be

02:10:50.680 --> 02:10:53.560
like 100 million people dead. It was the most obvious thing in the world to happen.

02:10:54.600 --> 02:10:59.080
And it's the dog that didn't bark. It may be the best single net thing that happened in the

02:10:59.080 --> 02:11:02.520
entire 20th century, is that that didn't happen. Yeah, actually, just on that point,

02:11:03.240 --> 02:11:07.480
you say a lot of really brilliant things. It hit me just as you were saying it.

02:11:09.320 --> 02:11:15.480
I don't know why it hit me for the first time, but we got two wars in a span of 20 years.

02:11:16.360 --> 02:11:21.160
Like, we could have kept getting more and more world wars and more and more ruthless.

02:11:21.960 --> 02:11:25.000
It actually, you could have had a US versus Russia war.

02:11:25.000 --> 02:11:29.720
You could have. By the way, there's another hypothetical scenario. The other hypothetical

02:11:29.720 --> 02:11:34.040
scenario is the Americans got the bomb, the Russians didn't. And then America is the big

02:11:34.040 --> 02:11:37.240
dog. And then maybe America would have had the capability to actually roll back there at current.

02:11:39.000 --> 02:11:41.480
I don't know whether that would have happened, but like it's entirely possible.

02:11:42.040 --> 02:11:46.040
Right. And the act of these people who had these moral positions about, because they could

02:11:46.040 --> 02:11:48.920
forecast, they could model, they could forecast the future of how this technology would get used,

02:11:48.920 --> 02:11:52.600
made a horrific mistake, because they basically ensured that the Iron Curtain would continue

02:11:52.600 --> 02:11:55.640
for 50 years longer than it would have otherwise. And again, like, these are counterfactuals. I

02:11:55.640 --> 02:12:01.880
don't know that that's what would have happened. But like, the decision to hand the bomb over

02:12:01.880 --> 02:12:06.040
was a big decision made by people who were very full of themselves.

02:12:07.000 --> 02:12:12.520
Yeah, but so me as an America, me as a person that loves America, I also wonder if US was the

02:12:12.520 --> 02:12:19.320
only ones with nuclear weapons. That was the argument for handing the, that was the,

02:12:20.040 --> 02:12:23.240
the guys who, the guys who handed over the bomb. That was actually their moral argument.

02:12:23.240 --> 02:12:28.200
Yeah, I would, I would probably not hand it over to, I would be careful about the regimes

02:12:28.200 --> 02:12:31.560
you handed over to. Maybe give it to like the British or something.

02:12:32.360 --> 02:12:37.080
Like, like a democratically elected government.

02:12:37.080 --> 02:12:40.280
Well, there are people to this day who think that those Soviet spies did the right thing,

02:12:40.280 --> 02:12:43.560
because they created a balance of terror, as opposed to the US having just, and by the way,

02:12:43.560 --> 02:12:47.720
let me, let me, balance of terror, let's tell the full verse, such a sexy ring to it. Okay,

02:12:47.720 --> 02:12:50.600
so the full version of the story is John von Neumann as a hero of both yours and mine,

02:12:50.600 --> 02:12:56.360
the full version of the story is he advocated for a first right. So when the US had the bomb,

02:12:56.360 --> 02:13:00.040
and Russia did not, he advocated for, he said, we need to strike them right now.

02:13:01.000 --> 02:13:02.280
Strike Russia. Yeah.

02:13:04.040 --> 02:13:05.480
Yes. Von Neumann.

02:13:05.480 --> 02:13:10.440
Yes, because he said World War Three is inevitable. He was very hardcore.

02:13:11.160 --> 02:13:15.640
He, his, his theory was, his theory was World War Three is inevitable. We're definitely going

02:13:15.640 --> 02:13:18.680
to have a World War Three. The only way to stop World War Three is we have to take them out right

02:13:18.680 --> 02:13:21.480
now. And we have to take them out right now before they get the bomb, because this is our last chance.

02:13:22.760 --> 02:13:23.640
Now, again, like,

02:13:23.640 --> 02:13:25.720
Is this an example of philosophers and politics?

02:13:25.720 --> 02:13:27.800
I don't know if that's in there or not, but this is in the standard.

02:13:27.800 --> 02:13:29.240
No, but it is. Yeah. Meaning is that.

02:13:29.240 --> 02:13:32.760
Yeah, this is on the other side. So, so most of the case studies, most of the case studies in

02:13:32.760 --> 02:13:37.880
books like this are the crazy people on the left. Yeah. Von Neumann is a story arguably of the

02:13:37.880 --> 02:13:41.160
crazy people on the right. Yeah, stick to computing, John.

02:13:41.160 --> 02:13:44.600
Well, this is the thing. And this is, this is the general principle. It goes back to our core

02:13:44.600 --> 02:13:48.200
thing, which is like, I don't know whether any of these people should be making any of these calls.

02:13:48.200 --> 02:13:51.240
Yeah. Because there's nothing in either von Neumann's background or Oppenheimer's

02:13:51.240 --> 02:13:54.920
background or any of these people's background that qualifies them as moral authorities.

02:13:54.920 --> 02:14:00.280
Yeah. Well, this actually brings up the point of in AI, who are the good people to,

02:14:00.280 --> 02:14:05.080
to reason about the morality, the ethics, the outside of these risks outside, like,

02:14:05.080 --> 02:14:10.280
the more complicated stuff that you, you agree on is, you know, this will go into the hands of

02:14:10.280 --> 02:14:14.040
bad guys and all the kinds of ways they'll do is, is interesting and dangerous.

02:14:15.480 --> 02:14:19.640
Is dangerous in interesting, unpredictable ways. And who is the right person?

02:14:19.640 --> 02:14:23.320
Who are the right kinds of people to make decisions how to respond to it?

02:14:23.400 --> 02:14:28.040
Or is it tech people? So the history of these fields, this is what he talks about in the book,

02:14:28.040 --> 02:14:33.240
the history of these fields is that the, the competence and capability and intelligence

02:14:33.240 --> 02:14:37.480
and training and accomplishments of senior scientists and technologists working on a technology,

02:14:38.360 --> 02:14:41.720
and then being able to then make moral judgments in the use of that technology,

02:14:41.720 --> 02:14:46.280
that track record is terrible. That track record, that track record is like catastrophically bad.

02:14:46.840 --> 02:14:51.320
The people, just to look at it, the people that develop that technology are usually not going

02:14:51.320 --> 02:14:55.640
to be the right people. Well, why would they? So the claim is, of course, they're the knowledgeable

02:14:55.640 --> 02:15:00.920
ones, but the problem is they've spent their entire life in a lab, right? They're not theologians.

02:15:01.720 --> 02:15:04.920
But so what you find, what you find when you read, when you read this, when you look at these

02:15:04.920 --> 02:15:09.240
histories, what you find is they generally are very thinly informed on history, on sociology,

02:15:09.240 --> 02:15:17.000
on, on, on theology, on morality, ethics, they tend to manufacture their own worldviews from scratch.

02:15:17.080 --> 02:15:24.600
They tend to be very sort of thin. They're not remotely the arguments that you would be having

02:15:24.600 --> 02:15:28.120
if you got like a group of highly qualified theologians or philosophers or, you know.

02:15:29.000 --> 02:15:36.520
Well, let me sort of, as the devil's advocate takes a sip of whiskey, say that I, I agree with,

02:15:37.960 --> 02:15:42.920
with that, but also it seems like the people who are doing kind of the ethics departments and

02:15:42.920 --> 02:15:51.640
these tech companies go sometimes the other way, which is they're not nuanced on the, on history

02:15:51.640 --> 02:15:57.720
or theology or this kind of stuff. They almost becomes a kind of outraged activism towards

02:15:59.400 --> 02:16:06.040
directions that don't seem to be grounded in history and humility and nuance. It's again,

02:16:06.040 --> 02:16:11.480
drenched with arrogance. So I'm not sure which is worse. Oh, no, they're both bad. Yeah. So

02:16:11.480 --> 02:16:16.600
definitely not them either. But I guess, but look, this is a hard, yeah, it's a hard problem.

02:16:16.600 --> 02:16:20.040
This is our problem. This goes back to where we started, which is okay, who has the truth. And

02:16:20.040 --> 02:16:24.760
it's like, well, you know, like how do societies arrive at like truth and how do we figure these

02:16:24.760 --> 02:16:29.800
things out? And like our elected leaders play some role in it. You know, we all play some role in it.

02:16:30.760 --> 02:16:34.920
There have to be some set of public intellectuals at some point that bring, you know, rationality

02:16:34.920 --> 02:16:38.840
and judgment and humility to it. Yeah, those people are few and far between. We should probably

02:16:38.840 --> 02:16:45.160
prize them very highly. Yeah. Celebrate humility in our public leaders. So getting to risk number

02:16:45.160 --> 02:16:50.840
two, will AI ruin our society? Short version, as you write, if the murder robots don't get us,

02:16:50.840 --> 02:16:56.680
the hate speech and misinformation will. And the action you recommend in short,

02:16:56.680 --> 02:17:05.960
don't let the thought police suppress AI. Well, what is this risk of the effect of

02:17:06.040 --> 02:17:13.560
misinformation in a society that's going to be catalyzed by AI? Yeah, so this is the social media.

02:17:13.560 --> 02:17:16.760
This is what you just alluded to. It's the activism kind of thing that's popped up in these

02:17:16.760 --> 02:17:21.160
companies in the industry. And it's basically, from my perspective, it's basically part two

02:17:21.160 --> 02:17:25.320
of the war that played out over social media over the last 10 years. Because you probably remember

02:17:25.320 --> 02:17:30.280
social media 10 years ago was basically who even wants this, who wants a photo of what your cat

02:17:30.280 --> 02:17:33.960
had for breakfast, like this stuff is like silly and trivial. And why can't these nerds like figure

02:17:33.960 --> 02:17:39.000
out how to invent something useful and powerful? And then certain things happened in the political

02:17:39.000 --> 02:17:42.600
system. And then it's sort of the polarity on that discussion switched all the way to social

02:17:42.600 --> 02:17:46.360
media is like the worst, most corrosive, most terrible, most awful technology ever invented.

02:17:46.360 --> 02:17:51.400
And then it leads to terrible, the wrong politicians and policies and politics and all

02:17:51.400 --> 02:17:56.280
this stuff. And that all got catalyzed into this very big kind of angry movement, both inside

02:17:56.280 --> 02:18:00.440
and outside the companies, to kind of bring social media to heal. And that got focused in

02:18:00.440 --> 02:18:04.600
particularly on two topics, so-called hate speech and so-called misinformation. And that's

02:18:04.600 --> 02:18:08.520
been the saga playing out for the last decade. And I don't even really want to even argue the pros

02:18:08.520 --> 02:18:12.680
and cons of the sides just to observe that that's been like a huge fight and has had big consequences

02:18:12.680 --> 02:18:20.040
to how these companies operate. Basically, those same sets of theories, that same activist approach,

02:18:20.040 --> 02:18:24.280
that same energy is being transplanted straight to AI. And you see that already happening. It's

02:18:24.280 --> 02:18:28.760
why ChatGPT will answer, let's say, certain questions and not others. It's why it gives you

02:18:28.760 --> 02:18:32.600
the canned speech about, you know, whenever it starts with as a large language model, I cannot,

02:18:32.600 --> 02:18:35.320
you know, basically means that somebody has reached in there and told that it can't talk

02:18:35.320 --> 02:18:40.520
about certain topics. Do you think some of that is good? So it's an interesting question.

02:18:41.080 --> 02:18:46.440
So a couple of observations. So one is the people who find this the most frustrating are the people

02:18:46.440 --> 02:18:53.960
who are worried about the murder robots. So and in fact, the so-called ex-risk people,

02:18:53.960 --> 02:18:57.960
right, they started with the term AI safety, the term became AI alignment. When the term became

02:18:57.960 --> 02:19:01.000
AI alignment is when this switch happened from where we're worried it's going to kill us all to

02:19:01.000 --> 02:19:05.880
we're worried about hate speech and misinformation. The AI ex-risk people have now renamed their thing

02:19:05.880 --> 02:19:12.040
AI not kill everyone ism, which I have to admit is a catchy term. And they are very frustrated by

02:19:12.040 --> 02:19:15.880
the fact that the sort of activist driven hate speech misinformation kind of thing is taking

02:19:15.880 --> 02:19:19.080
over, which is what's happened is taking over the AI ethics field has been taken over by the

02:19:19.080 --> 02:19:24.360
hate speech misinformation people. You know, look, would I like to live in a world in which like

02:19:24.360 --> 02:19:27.800
everybody was nice to each other all the time and nobody ever said anything mean and nobody ever

02:19:27.800 --> 02:19:31.640
used a bad word and everything was always accurate and honest. Like that sounds great.

02:19:31.640 --> 02:19:34.520
Do I want to live in a world where there's like a centralized thought police working

02:19:34.520 --> 02:19:38.680
through the tech companies to enforce the view of a small set of elites that they're going to

02:19:38.680 --> 02:19:43.080
determine what the rest of us think and feel like absolutely not. There could be a middle ground

02:19:43.080 --> 02:19:49.000
somewhere like Wikipedia type of moderation. There's moderation of Wikipedia that is somehow

02:19:49.000 --> 02:19:55.800
crowdsourced where you don't have centralized elites. But it's also not completely just a

02:19:55.800 --> 02:20:01.640
free for all because the if you have the entirety of human knowledge at your fingertips,

02:20:02.280 --> 02:20:08.360
you can do a lot of harm. Like if you have a good assistant that's completely uncensored,

02:20:08.360 --> 02:20:17.080
they can help you build a bomb. They can help you mess with people's physical well being, right?

02:20:17.080 --> 02:20:21.880
If they because that information is out there on the internet. And so they presumably there's

02:20:22.600 --> 02:20:29.960
it would be, you could see the positives in censoring some aspects of an AI model

02:20:29.960 --> 02:20:34.920
when it's helping you commit literal violence. And there's a section later section of the essay

02:20:34.920 --> 02:20:39.480
where I talk about bad people doing bad things, which and there's a set of things that we

02:20:39.480 --> 02:20:44.120
should discuss there. What happens in practice is these lines, as you alluded to this already,

02:20:44.120 --> 02:20:47.800
these lines are not easy to draw. And what I've observed in the social media version of this

02:20:47.800 --> 02:20:51.960
is like the way I describe it as the slippery slope is not a fallacy. It's an inevitability.

02:20:51.960 --> 02:20:55.080
The minute you have this kind of activist personality that gets in a position to make

02:20:55.080 --> 02:21:00.440
these decisions, they take it straight to infinity. Like it goes into the crazy zone

02:21:00.440 --> 02:21:04.360
like almost immediately and never comes back because people become drunk with power, right?

02:21:04.360 --> 02:21:07.240
And they look if you're in the position to determine what the entire world thinks and

02:21:07.240 --> 02:21:12.200
feels and reads and says, like you're going to take it. And you know, Elon has ventilated this

02:21:12.200 --> 02:21:15.080
with the Twitter files over the last, you know, three months. And it's just like crystal clear

02:21:15.080 --> 02:21:19.880
like how bad it got there. Now, reason for optimism is what Elon is doing with the community notes.

02:21:21.640 --> 02:21:25.800
So community notes is actually a very interesting thing. So what Elon is trying to do with

02:21:25.800 --> 02:21:29.880
community notes is he's trying to have it where there's only a community note when people who

02:21:29.880 --> 02:21:36.360
have previously disagreed on many topics agree on this one. Yes. That's what I'm trying to get at

02:21:36.360 --> 02:21:40.760
is like there's, there could be Wikipedia like models or community notes type of models where

02:21:41.320 --> 02:21:48.360
allows you to essentially either provide context or sensor in a way that's not resist the slippery

02:21:48.360 --> 02:21:51.640
slope nature. Now, there's another power. There's an entirely different approach here,

02:21:51.640 --> 02:21:56.440
which is basically we have AIs that are producing content. We could also have AIs that are consuming

02:21:56.440 --> 02:22:01.000
content, right? And so one of the things that your assistant could do for you is help you consume

02:22:01.000 --> 02:22:05.320
all the content, right? And basically tell you when you're getting played. So for example,

02:22:05.320 --> 02:22:09.000
I'm going to want the AI that my kid uses, right, to be very, you know, child safe,

02:22:09.000 --> 02:22:12.120
and I'm going to want it to filter for him all kinds of inappropriate stuff that he shouldn't

02:22:12.120 --> 02:22:15.560
be saying just because he's a kid. Yeah. Right. And you see what I'm saying is you can implement

02:22:15.560 --> 02:22:19.000
that. You could do the architecture. You could say you can solve this on the client side, right?

02:22:19.000 --> 02:22:22.040
Solving on the server side gives you an opportunity to dictate for the entire world,

02:22:22.040 --> 02:22:26.440
which I think is where you take the slippery slope to hell. There's another architectural

02:22:26.440 --> 02:22:29.560
approach, which is to solve this on the client side, which is certainly what I would endorse.

02:22:30.440 --> 02:22:35.880
It's at risk number five will AI lead to bad people doing bad things. I can just imagine

02:22:35.960 --> 02:22:40.200
language models used to do so many bad things, but the hope is there that you can have

02:22:41.640 --> 02:22:46.120
large language models used to then defend against it by more people, by smarter people, by

02:22:47.480 --> 02:22:50.760
more effective people, skilled people, all that kind of stuff.

02:22:50.760 --> 02:22:55.400
Three-point argument on bad people doing bad things. So number one, right, you can use the

02:22:55.400 --> 02:22:59.640
technology defensively. And there's a, we should be using AI to build like broad spectrum vaccines

02:22:59.640 --> 02:23:03.320
and antibiotics for like bio weapons. And we should be using AI to like hunt terrorists and

02:23:03.400 --> 02:23:06.040
catch criminals. And like we should be doing like all kinds of stuff like that.

02:23:06.040 --> 02:23:09.080
And in fact, we should be doing those things even just to like go get like, you know,

02:23:09.080 --> 02:23:12.440
basically go eliminate risk from like regular pathogens that aren't like constructed by an AI.

02:23:12.440 --> 02:23:15.960
So there's, there's, there's the whole, there's a whole defensive set of things.

02:23:16.680 --> 02:23:21.160
Second is we have many laws on the books about the actual bad things, right? So it is actually

02:23:21.160 --> 02:23:25.560
illegal to be a criminal, you know, to commit crimes, to commit terrorist acts, to, you know,

02:23:25.560 --> 02:23:30.200
build pathogens with the intent to deploy them to kill people. And so we have those,

02:23:30.200 --> 02:23:33.480
we don't, we actually don't need new laws for the vast majority of these scenarios. We actually

02:23:33.480 --> 02:23:38.360
already have the laws in the book on the books. The third argument is the minute, and this is

02:23:38.360 --> 02:23:41.560
sort of the foundational one that gets really tough, but the minute you get into this thing,

02:23:41.560 --> 02:23:44.360
which, which you were kind of getting into, which is like, okay, but like, don't you need

02:23:44.360 --> 02:23:47.880
censorship sometimes, right? And don't you need restriction sometimes? It's like, okay, what is

02:23:47.880 --> 02:23:55.080
the cost of that? And in particular in the world of open source, right? And so is open source AI

02:23:55.080 --> 02:24:01.960
going to be allowed or not? If open source AI is not allowed, then what is the regime that's going

02:24:01.960 --> 02:24:07.080
to be necessary legally and technically to prevent it from developing, right? And here, again, is

02:24:07.080 --> 02:24:10.680
where you get into, and people have proposed that these kinds of things, you get into, I would say,

02:24:10.680 --> 02:24:16.040
pretty extreme territory pretty fast. Do we have a monitor agent on every CPU and GPU that reports

02:24:16.040 --> 02:24:20.680
back to the government, what we're doing with our computers? Are we seizing GPU clusters to get

02:24:20.680 --> 02:24:25.160
beyond a certain size? Like, and then by the way, how are we doing all that globally? Right?

02:24:25.160 --> 02:24:29.560
And like, if China is developing an LLM beyond the scale that we think is allowable, are we going

02:24:29.560 --> 02:24:34.600
to invade? Right? And you have figures on the AI X risk side who are advocating, you know,

02:24:34.600 --> 02:24:38.520
potentially up to nuclear strikes to prevent, you know, this kind of thing. And so here you get into

02:24:38.520 --> 02:24:42.840
this thing. And again, you could maybe say this is, you know, you could even say this is what good,

02:24:42.840 --> 02:24:47.560
bad or indifferent or whatever. But like, here's the comparison of nukes. The comparison of nukes

02:24:47.560 --> 02:24:51.560
is very dangerous because one is just nukes were just, just above, although we can come back to

02:24:51.560 --> 02:24:55.080
nuclear power. But the other thing was like with nukes, you could control plutonium, right? You

02:24:55.080 --> 02:25:00.280
could track plutonium. And it was like hard to come by. AI is just math and code, right? And it's

02:25:00.280 --> 02:25:03.560
in like math textbooks. And it's like their YouTube videos that teach you how to build it. And like,

02:25:03.560 --> 02:25:06.920
there's open source already opens horse, you know, there's a 40 billion parameter model running

02:25:06.920 --> 02:25:13.160
around already called Falcon online that anybody can download. And so, okay, you walk down the logic

02:25:13.160 --> 02:25:16.120
path that says we need to have guardrails on this. And you find yourself in a

02:25:16.920 --> 02:25:23.000
authoritarian totalitarian regime of thought control and machine control that would be so brutal

02:25:23.880 --> 02:25:27.640
that you would have destroyed the society that you're trying to protect. And so I just don't

02:25:27.640 --> 02:25:33.880
see how that actually works. So you have to understand my brain is going full, full steam ahead

02:25:33.880 --> 02:25:37.720
here because I agree with basically everything you're saying when I'm trying to play devil's

02:25:37.720 --> 02:25:43.960
advocate here. Because okay, you highlighted the fact that there is a slippery slope to human

02:25:43.960 --> 02:25:47.720
nature. The moment you sense there's something, you start to sense everything.

02:25:50.680 --> 02:25:58.520
That alignment starts out sounding nice, but then you start to align to the beliefs of some

02:25:59.480 --> 02:26:04.600
select group of people. And then it's just your beliefs. The number of people you're

02:26:04.600 --> 02:26:10.280
aligned to smaller smaller as that group becomes more and more powerful. Okay, but that just speaks

02:26:10.280 --> 02:26:16.200
to the people that censor usually the assholes and the assholes get richer. I wonder if it's

02:26:16.200 --> 02:26:23.240
possible to do without that for AI. The one way to ask this question is, do you think the base

02:26:23.240 --> 02:26:30.360
models, the base, the baseline foundation models should be open sourced? Like what Mark Zuckerberg

02:26:30.360 --> 02:26:35.880
is saying they want to do? So I look, I mean, I think it's totally appropriate that companies that

02:26:35.960 --> 02:26:40.680
are in the business of producing a product or service should be able to have a wide range of

02:26:40.680 --> 02:26:45.320
policies that they put, right? And I'll just again, I want a heavily censored model for my eight-year-old.

02:26:46.120 --> 02:26:49.560
Like I actually want that. Like I would pay more money for the ones more heavily censored than the

02:26:49.560 --> 02:26:53.880
one that's not, right? And so like there are certainly scenarios where companies will make that

02:26:53.880 --> 02:26:59.480
decision. Look, an interesting thing you brought up, or is this really a speech issue? One of the

02:26:59.480 --> 02:27:04.360
things that the big tech companies are dealing with is that content generated from an LLM is not

02:27:04.360 --> 02:27:10.040
covered under section 230, which is the law that protects internet platform companies from being

02:27:10.040 --> 02:27:17.880
sued for the user-generated content. And so it's actually, yes. And so there's actually a question,

02:27:17.880 --> 02:27:22.280
I think there's still a question which is can big American companies actually feel generative AI at

02:27:22.280 --> 02:27:26.680
all? Or is the liability actually going to just ultimately convince them that they can't do it?

02:27:26.680 --> 02:27:30.440
Because the minute the thing says something bad, and it doesn't even need to be hate speech, it

02:27:30.440 --> 02:27:36.120
could just be like an enact, it could hallucinate a product detail on a vacuum cleaner, and all

02:27:36.120 --> 02:27:40.360
of a sudden the vacuum cleaner company sues for misrepresentation. And there's any symmetry there,

02:27:40.360 --> 02:27:44.280
right? Because the LLM is going to be producing billions of answers to questions, and it only

02:27:44.280 --> 02:27:47.880
needs to get a few wrong. The loss has to get updated really quick here. Yeah, and nobody knows

02:27:47.880 --> 02:27:53.960
what to do with that, right? So anyway, there are big questions around how companies operate at all.

02:27:53.960 --> 02:27:57.640
So we talk about those. But then there's this other question of like, okay, the open source,

02:27:57.640 --> 02:28:01.560
so what about open source? And my answer to your question is kind of like, obviously, yes,

02:28:01.560 --> 02:28:06.120
the models have to, there has to be full open source here, because to live in a world in which

02:28:06.120 --> 02:28:12.040
that open source is not allowed is a world of draconian speech control, human control, machine

02:28:12.040 --> 02:28:17.080
control. I mean, you know, black helicopters with jackbooted thugs coming out, repelling down and

02:28:17.080 --> 02:28:23.240
seizing your GPU, like territory. Well, no, no, I'm 100% serious. That's you're saying slippery

02:28:23.240 --> 02:28:26.520
slope always leads that. No, no, no, no, no, no, that's what's required to enforce it. Like, how

02:28:26.600 --> 02:28:32.120
will you enforce a ban on open source? You could add friction to it, like hard to get the models,

02:28:32.120 --> 02:28:36.040
because people will always be able to get the models. But it'll be more in the shadows, right?

02:28:36.040 --> 02:28:41.320
The leading open source model right now is from the UAE. Like, the next time they do that, what do

02:28:41.320 --> 02:28:48.920
we do? Yeah, like, oh, I see, you're like, the 14 year old in Indonesia comes out with a breakthrough

02:28:48.920 --> 02:28:51.560
model. You know, we talked about most great software comes from a small number of people.

02:28:51.560 --> 02:28:55.000
Some kid comes out with some big new breakthrough and quantization or something and has some huge

02:28:55.000 --> 02:28:59.720
breakthrough and like, what are we going to like, invade Indonesia and arrest him?

02:28:59.720 --> 02:29:03.800
It seems like in terms of size of models and effectiveness of models, the big tech companies

02:29:03.800 --> 02:29:09.160
will probably lead the way for quite a few years. And the question is of what policies they should

02:29:09.160 --> 02:29:19.960
use. The kid in Indonesia should not be regulated, but should Google Meta, Microsoft, OpenAI be

02:29:20.680 --> 02:29:27.480
regulated? Okay, so when does it become dangerous? Is the danger that it's, quote,

02:29:27.480 --> 02:29:32.600
as powerful as the current leading commercial model? Or is it that it is just at some other

02:29:32.600 --> 02:29:37.320
arbitrary threshold? And then by the way, like, look, how do we know? Like, what we know today

02:29:37.320 --> 02:29:40.680
is that you need like a lot of money to train these things. But there are advances being made

02:29:40.680 --> 02:29:44.280
every week on training efficiency and you know, data, all kinds of synthetic data. Look, I don't

02:29:44.280 --> 02:29:47.400
even like the synthetic data thing we're talking about, maybe some kid figures out a way to auto

02:29:47.400 --> 02:29:50.760
generate synthetic data. That's going to change everything. Yeah, exactly. And so like sitting

02:29:50.760 --> 02:29:54.840
here today, like the breakthrough just happened, right? You made this point, like the breakthrough

02:29:54.840 --> 02:29:59.960
just happened. So we don't know what the shape of this technology is going to be. I mean, the big

02:29:59.960 --> 02:30:05.480
shock, the big shock here is that, you know, whatever number of billions of parameters basically

02:30:05.480 --> 02:30:10.120
represents at least a very big percentage of human thought, like, who would have imagined that?

02:30:10.920 --> 02:30:14.040
And then there's already work underway, there was just this paper that just came out that

02:30:14.040 --> 02:30:18.600
basically takes a GPT-3 scale model and compresses it down to run on a single 32-core CPU.

02:30:19.400 --> 02:30:24.200
Like, who would have predicted that? Yeah. You know, some of these models now you can run

02:30:24.200 --> 02:30:28.040
in Raspberry Pi's, like today they're very slow, but like, you know, maybe they'll be a, you know,

02:30:28.040 --> 02:30:32.760
perceived real perform, you know, like, it's math and code. And here we're back in here,

02:30:32.760 --> 02:30:37.480
we're back in math and code. It's math and code. It's math, code and data. It's bits.

02:30:37.480 --> 02:30:43.240
Mark's just like walking away at this point. He just screw it. I don't know what to do with this.

02:30:43.240 --> 02:30:49.800
You guys created this whole internet thing. Yeah. Yeah. I'm a huge believer in open source here.

02:30:49.800 --> 02:30:53.160
So my argument is, we're going to have to see, here's my argument. My argument, my full argument

02:30:53.160 --> 02:30:56.360
is AI is going to be like air, it's going to be everywhere. Like, this is just going to be in

02:30:56.360 --> 02:30:58.920
text, it already is. It's going to be in textbooks and kids are going to grow up knowing how to do

02:30:58.920 --> 02:31:01.720
this. And it's just going to be a thing. It's going to be in the air and you can't like pull

02:31:01.720 --> 02:31:05.320
this back anywhere. You can pull back air. And so you just have to figure out how to live in this

02:31:05.320 --> 02:31:09.640
world, right? And then that's where I think like all this hand-wringing about AI risk is basically

02:31:09.640 --> 02:31:14.680
complete waste of time because the effort should go into, okay, what is the defensive approach?

02:31:15.320 --> 02:31:18.760
And so if you're worried about AI generated pathogens, the right thing to do is to have a

02:31:18.760 --> 02:31:25.000
permanent project warp speed, right? And funded lavishly. Let's do a Manhattan project for biological

02:31:25.000 --> 02:31:29.640
defense, right? And let's build AIs and let's have like broad spectrum vaccines where like we're

02:31:29.640 --> 02:31:35.400
insulated from every pathogen, right? And what the interesting thing is because it's software,

02:31:36.360 --> 02:31:41.880
a kid in his basement, a teenager could build like a system that defends against like the worst

02:31:41.880 --> 02:31:50.120
that the worst. I mean, and to me, defense is super exciting. It's like, if you believe in

02:31:50.120 --> 02:31:55.400
the good of human nature, the most people want to do good to be the savior of humanity is really

02:31:55.400 --> 02:32:03.000
exciting. Yes. Okay, that's a dramatic statement, but like to help people. To help people. Yeah,

02:32:03.080 --> 02:32:08.920
okay, what about just to jump around? What about the risk of will AI lead to crippling inequality?

02:32:09.880 --> 02:32:12.520
You know, because we're kind of saying everybody's life will become better.

02:32:13.640 --> 02:32:18.120
Is it possible that the rich get richer here? Yeah. So this is actually ironically goes back to

02:32:18.120 --> 02:32:21.880
Marxism. So because this was the course of the core claim of Marxism, right? Basically, it was

02:32:21.880 --> 02:32:24.920
that the owner, the owners of capital would basically own the means of production. And then

02:32:24.920 --> 02:32:28.920
over time, they would basically accumulate all the wealth the workers would be paying in, you know,

02:32:28.920 --> 02:32:32.360
and getting nothing in return, because they wouldn't be needed anymore, right? Marx is very

02:32:32.440 --> 02:32:35.480
worried about what he called mechanization or what later became known as automation.

02:32:36.680 --> 02:32:40.680
And that, you know, the workers would be emissarated and the capitalist would end up with all. And so

02:32:40.680 --> 02:32:44.440
this was one of the core principles of Marxism. Of course, it turned out to be wrong about every

02:32:44.440 --> 02:32:48.760
previous wave of technology. The reason it turned out to be wrong about every previous wave of

02:32:48.760 --> 02:32:53.640
technology is that the way that the self-interested owner of the machines makes the most money is

02:32:53.640 --> 02:32:59.160
by providing the production capability in the form of products and services to the most people,

02:32:59.160 --> 02:33:03.080
the most customers as possible, right? The largest... And this is one of those funny things

02:33:03.080 --> 02:33:06.040
where every CEO knows this intuitively, and yet it's like hard to explain from the outside.

02:33:06.600 --> 02:33:10.440
The way you make the most money in any business is by selling to the largest market you can possibly

02:33:10.440 --> 02:33:15.400
get to. The largest market you can possibly get to is everybody on the planet. And so every large

02:33:15.400 --> 02:33:19.480
company does is everything that it can to drive down prices to be able to get volumes up to be

02:33:19.480 --> 02:33:23.240
able to get to everybody on the planet. And that happened with everything from electricity. It

02:33:23.240 --> 02:33:26.760
happened with telephones. It happened with radio. It happened with automobiles. It happened with

02:33:26.840 --> 02:33:34.120
smartphones. It happened with PCs. It happened with the internet. It happened with mobile broadband.

02:33:34.120 --> 02:33:38.600
It's happened, by the way, with Coca-Cola. It's happened with like every... Basically,

02:33:38.600 --> 02:33:42.600
every industrially produced, good or service, people want to drive it to the largest possible

02:33:42.600 --> 02:33:48.280
market. And then as proof of that, it's already happened, right? Which is the early adopters of

02:33:48.280 --> 02:33:54.520
like GPD and Bing are not like Exxon and Boeing. They're your uncle and your nephew,

02:33:55.080 --> 02:33:59.080
right? It's either freely available online or it's available for $20 a month or something,

02:33:59.080 --> 02:34:05.320
but these things went... This technology went mass market immediately. And so look, the owners of

02:34:05.320 --> 02:34:08.520
the means of production, whoever does this, now as I mentioned these trillion dollar questions,

02:34:08.520 --> 02:34:11.800
there are people who are going to get really rich doing this, producing these things, but

02:34:11.800 --> 02:34:14.600
they're going to get really rich by taking this technology to the broadest possible market.

02:34:15.400 --> 02:34:20.120
So yes, they'll get rich, but they'll get rich having a huge positive impact on...

02:34:20.120 --> 02:34:22.440
Yeah, making the technology available to everybody.

02:34:23.080 --> 02:34:28.120
And again, smartphone, same thing. So there's this amazing kind of twist in business history,

02:34:28.120 --> 02:34:33.080
which is you cannot spend $10,000 on a smartphone, right? You can't spend $100,000. You can't

02:34:33.080 --> 02:34:35.880
spend... Like I would buy the million dollar smartphone, like I'm signed up for it. Like if

02:34:35.880 --> 02:34:39.400
it's like, suppose a million dollar smartphone was like much better than the $1,000 smartphone,

02:34:39.400 --> 02:34:43.640
like I'm there to buy it, it doesn't exist. Why doesn't it exist? Apple makes so much more

02:34:43.640 --> 02:34:47.160
money driving the price further down from $1,000 than they would trying to harvest,

02:34:47.160 --> 02:34:49.960
right? And so it's just this repeating pattern you see over and over again,

02:34:50.440 --> 02:34:54.680
where the... And what's great about it, what's great about it is you do not need to rely on

02:34:54.680 --> 02:34:59.320
anybody's enlightened right generosity to do this. You just need to rely on capitalist self-interest.

02:35:01.320 --> 02:35:07.400
What about AI taking our jobs? Yeah, so very, very similar thing here. There's a core fallacy,

02:35:07.400 --> 02:35:11.400
which again was very common in Marxism, which is what's called the lump of labor fallacy.

02:35:11.400 --> 02:35:15.240
And this is sort of the fallacy that there is only a fixed amount of work to be done in the world.

02:35:15.240 --> 02:35:18.680
And if the... And it's all being done today by people. And then if machines do it,

02:35:18.680 --> 02:35:23.800
there's no other work to be done by people. And that's just a completely backwards view on how

02:35:23.800 --> 02:35:28.360
the economy develops and grows, because what happens is not. In fact, that what happens is

02:35:28.920 --> 02:35:32.440
the introduction of technology into production process causes prices to fall.

02:35:33.080 --> 02:35:37.240
As prices fall, consumers have more spending power. As consumers have more spending power,

02:35:37.240 --> 02:35:43.080
they create new demand. That new demand then causes capital and labor to form into new enterprises

02:35:43.080 --> 02:35:47.240
to satisfy new wants and needs. And the result is more jobs at higher wages.

02:35:47.320 --> 02:35:53.320
New wants and needs. The worry is that the creation of new wants and needs at a rapid rate

02:35:54.040 --> 02:35:59.160
will mean there's a lot of turnover in jobs. So people will lose jobs. Just the actual

02:35:59.160 --> 02:36:03.720
experience of losing a job and having to learn new things and new skills is painful for the

02:36:03.720 --> 02:36:08.680
individuals. Two things. One is the new jobs are often much better. So this actually came up.

02:36:08.680 --> 02:36:11.560
There was this panic about a decade ago and all the truck drivers are going to lose their jobs,

02:36:11.560 --> 02:36:15.560
right? And number one, that didn't happen because we haven't figured out a way to actually finish

02:36:15.560 --> 02:36:19.640
that yet. But the other thing was like a truck driver, like I grew up in a town that was basically

02:36:19.640 --> 02:36:24.280
consisted of a truck stop, right? And I knew a lot of truck drivers. And truck drivers live a decade

02:36:24.280 --> 02:36:30.120
shorter than everybody else. It's actually like a very dangerous, like literally they have like

02:36:30.120 --> 02:36:35.000
high-racist skin cancer. And on the left side of their body from being in the sun all the time,

02:36:35.000 --> 02:36:38.920
the vibration of being in the truck is actually very damaging to your physiology.

02:36:38.920 --> 02:36:44.680
And there's actually, perhaps partially because of that reason, there's a shortage of

02:36:46.040 --> 02:36:50.680
people who want to be truck drivers. The question always you want to ask somebody like

02:36:50.680 --> 02:36:55.080
that is, do you want your kid to be doing this job? And like most of them will tell you, no,

02:36:55.080 --> 02:36:58.200
like I want my kid to be sitting in a cubicle somewhere like where they don't have this,

02:36:58.200 --> 02:37:02.280
like where they don't die 10 years earlier. And so the new jobs, number one, the new jobs are

02:37:02.280 --> 02:37:06.520
often better, but you don't get the new jobs until you go through the change. And then to your point,

02:37:06.520 --> 02:37:10.360
the training thing, you know, it's always the issue is can people adapt? And again, here you

02:37:10.360 --> 02:37:14.120
need to imagine living in a world in which everybody has the AI assistant capability,

02:37:14.760 --> 02:37:17.480
right, to be able to pick up new skills much more quickly and be able to have some, you know,

02:37:17.480 --> 02:37:19.400
be able to have a machine to work with to augment their skills.

02:37:19.400 --> 02:37:22.440
It's still going to be painful, but that's the process of life.

02:37:22.440 --> 02:37:24.600
It's painful for some people. I mean, there's no, like, there's no question

02:37:24.600 --> 02:37:27.880
it's painful for some people. And they're, you know, they're, yes, it's not, again,

02:37:27.880 --> 02:37:31.080
I'm not a utopian on this. And it's not like it's positive for everybody in the moment,

02:37:31.080 --> 02:37:35.720
but it has been overwhelmingly positive for 300 years. I mean, look, the concern here,

02:37:35.720 --> 02:37:40.040
the concern, the concern, this concern has played out for literally centuries. And, you know,

02:37:40.040 --> 02:37:44.040
this is the sort of leadite, you know, the story of the leadites that you may remember,

02:37:44.040 --> 02:37:49.000
there was a panic in the 2000s around outsourcing was going to take all the jobs. There was a panic

02:37:49.000 --> 02:37:56.760
in the 2010s that robots are going to take all the jobs. In 2019, before COVID, we had more jobs

02:37:56.760 --> 02:37:59.960
at higher wages, both in the country and in the world than at any point in human history.

02:38:00.520 --> 02:38:06.520
And so the overwhelming evidence is that the net gain here is like just like wildly positive.

02:38:06.520 --> 02:38:10.440
And most people like overwhelmingly come out the other side being huge beneficiaries of this.

02:38:11.080 --> 02:38:16.280
So you write that the single greatest risk, this is the risk you're most convinced by,

02:38:16.280 --> 02:38:21.240
the single greatest risk of AI is that China wins global AI dominance and we,

02:38:21.240 --> 02:38:24.520
the United States and the West do not. Can you elaborate?

02:38:24.520 --> 02:38:29.480
Yeah. So this is the other thing, which is a lot of the sort of AI risk debates today,

02:38:29.480 --> 02:38:32.760
sort of assume that we're the only game in town, right? And so we have the ability to kind of

02:38:32.760 --> 02:38:36.200
sit in the United States and criticize ourselves and, you know, have our government like,

02:38:36.280 --> 02:38:39.320
you know, beat up on our companies and we're figuring out a way to restrict what our companies can do.

02:38:39.320 --> 02:38:41.720
And, you know, we're going to, you know, we're going to ban this and ban that,

02:38:41.720 --> 02:38:45.000
restrict this and do that. And then there's this like other like force out there that

02:38:45.000 --> 02:38:49.560
like doesn't believe we have any power over them whatsoever. And they have no desire to sign up

02:38:49.560 --> 02:38:53.160
for whatever rules we decided to put in place. And they're going to do whatever it is they're

02:38:53.160 --> 02:38:58.200
going to do. And we have no control over it at all. And it's China and specifically the Chinese

02:38:58.200 --> 02:39:05.160
Communist Party. And they have a completely publicized, open, you know, plan for what they're

02:39:05.160 --> 02:39:10.440
going to do with AI. And it is not what we have in mind. And not only do they have that as a vision

02:39:10.440 --> 02:39:14.040
and a plan for their society, but they also have it as a vision and plan for the rest of the world.

02:39:14.040 --> 02:39:15.960
So their plan is what? Surveillance?

02:39:15.960 --> 02:39:21.560
Yeah, authoritarian control. So authoritarian population control, you know, good old fashioned

02:39:21.560 --> 02:39:27.480
communist authoritarian control, and surveillance and enforcement, and social credit scores and

02:39:27.480 --> 02:39:32.280
all the rest of it. And you are going to be monitored and metered within an inch of everything

02:39:32.360 --> 02:39:37.320
all the time. And it's going to be basically the end of human freedom. And that's their goal. And,

02:39:37.320 --> 02:39:40.040
you know, they justify it on the basis of that's what leads to peace.

02:39:40.040 --> 02:39:47.080
And you're worried that the regulating in the United States will hold progress enough to where

02:39:48.280 --> 02:39:50.280
the Chinese government would win that race?

02:39:50.280 --> 02:39:54.280
So their plan, yes, yes. And the reason for that is they, and again, they're very public on this,

02:39:54.280 --> 02:39:57.880
they have their plan is to proliferate their approach around the world. And they have this

02:39:57.880 --> 02:40:01.560
program called the digital Silk Road, right, which is building on their Silk Road investment

02:40:01.560 --> 02:40:04.680
program. And they've got their, they've been laying, they've been laying networking infrastructure

02:40:04.680 --> 02:40:09.240
all over the world with their 5G right work with their company, Huawei. And so they've been laying

02:40:09.240 --> 02:40:12.920
all this fabric, but financial and technological fabric all over the world. And their plan is to

02:40:12.920 --> 02:40:17.240
roll out their vision of AI on top of that, and to have every other country be running their version.

02:40:17.880 --> 02:40:22.280
And then if you're a country prone to, you know, authoritarianism, you're going to find this to

02:40:22.280 --> 02:40:26.600
be an incredible way to become more authoritarian. If you're a country, by the way, not prone to

02:40:26.600 --> 02:40:29.960
authoritarianism, you're going to have the Chinese Communist Party running your infrastructure and

02:40:29.960 --> 02:40:33.560
having backdoors into it, right, which is also not good.

02:40:34.280 --> 02:40:37.720
What's your sense of where they stand in terms of the race towards

02:40:38.680 --> 02:40:41.240
superintelligence as compared to the United States?

02:40:41.240 --> 02:40:45.000
Yeah, so good news is they're behind, but bad news is they, you know, they, let's just say they

02:40:45.000 --> 02:40:49.400
get access to everything we do. So they're probably a year behind at each point in time,

02:40:49.400 --> 02:40:53.400
but they get, you know, downloads, I think of basically all of our work on a regular basis

02:40:53.400 --> 02:40:57.640
through a variety of means. And they are, you know, at least, we'll see they're at least

02:40:57.640 --> 02:41:02.040
putting out reports of very competitive, just put out a report last week of a GPT 3.5 analog.

02:41:03.720 --> 02:41:06.920
They put out this report, forget what it's called, but they put out this report of the

02:41:06.920 --> 02:41:10.760
cell and they did, and they, you know, the way when OpenAI puts out, they, they,

02:41:10.760 --> 02:41:15.080
one of the ways they test, you know, GPT is they, they run it through standardized exams

02:41:15.080 --> 02:41:19.880
like the SAT, right? Just how you can kind of gauge how smart it is. And so the Chinese report,

02:41:19.880 --> 02:41:26.440
they ran their LM through the Chinese equivalent of the SAT, and it includes a section on Marxism.

02:41:27.400 --> 02:41:31.560
And a section on Mao Zedong thought, and it turns out their AI does very well on both of those

02:41:31.560 --> 02:41:39.880
topics, right? So like, this, this alignment thing, communist AI, right? Like literal communist AI,

02:41:39.880 --> 02:41:44.520
right? And so their vision is like, that's the, you know, so, you know, you, you can just imagine

02:41:44.520 --> 02:41:50.040
like you're a school, you know, you're a kid 10 years from now in Argentina or in Germany or in

02:41:50.840 --> 02:41:55.080
who knows where, uh, Indonesia. And you ask, they, I'd explain to you like how the economy

02:41:55.080 --> 02:41:58.760
works and it gives you the most cheery upbeat explanation of Chinese style communism you've

02:41:58.760 --> 02:42:04.760
ever heard, right? So like the stakes here are like really big. Well, my, as we've been talking

02:42:04.760 --> 02:42:08.840
about, my hope is not just for the United States, but it would just, uh, the kitten as basement

02:42:08.840 --> 02:42:15.640
with open source LLM. Cause I, I don't know if I, um, trust large centralized institutions

02:42:15.640 --> 02:42:21.480
with super powerful AI, no matter what their ideology is a power corrupts.

02:42:22.120 --> 02:42:29.240
You've been investing in tech companies for about, let's say 20 years and, uh, about 15 of which was,

02:42:29.240 --> 02:42:35.640
uh, with Andreessen Horowitz. Uh, what interesting trends in tech have you seen over that time?

02:42:35.640 --> 02:42:39.000
Let's just talk about companies and just the evolution of the tech industry.

02:42:39.000 --> 02:42:43.320
I mean, the big shift over 20 years has been that tech used to be a tools industry,

02:42:43.960 --> 02:42:49.320
for basically from like 1940 through to about 2010, almost all the big successful companies

02:42:49.400 --> 02:42:55.160
were picks and shovels companies. So PC database, smartphone, you know, some, some, some tool that

02:42:55.160 --> 02:43:00.680
somebody else would pick up and use. Since 2010, most of the big wins have been in applications.

02:43:01.320 --> 02:43:06.920
So a company that starts a, you know, it starts in an existing industry and goes directly to the

02:43:06.920 --> 02:43:11.400
customer in that industry. And then, you know, the early examples there were like Uber and Lyft

02:43:11.400 --> 02:43:17.560
and Airbnb. Um, and then that model is kind of elaborating out. Um, the AI thing is actually

02:43:17.640 --> 02:43:21.320
a reversion on that for now. Cause like most of the AI business right now is actually in cloud

02:43:21.320 --> 02:43:24.360
provision of, of, of AI APIs for other people to build on, but,

02:43:24.360 --> 02:43:26.360
But the big thing will probably be an app.

02:43:26.360 --> 02:43:30.440
Yeah. I think, I think most of the money, I think probably will be in whatever, yeah,

02:43:30.440 --> 02:43:35.160
your AI financial advisor or your AI doctor or your AI lawyer or, you know, take your pick

02:43:35.160 --> 02:43:38.760
of whatever the domain is. Um, and there, and what's interesting is, you know, we,

02:43:38.760 --> 02:43:43.160
the valley kind of does everything. We, we, the entrepreneurs kind of elaborate every possible

02:43:43.160 --> 02:43:47.640
idea. And so there will be a set of companies that like make AI, um, something that can be

02:43:47.640 --> 02:43:51.160
purchased and used by large law firms. Um, and then there will be other companies that just

02:43:51.160 --> 02:43:53.400
go direct to market as a, as an AI lawyer.

02:43:54.600 --> 02:44:01.320
What advice could you give for startup founder? Just having seen so many successful companies,

02:44:01.320 --> 02:44:05.560
so many companies that fail also. What advice could you give to a startup founder,

02:44:05.560 --> 02:44:10.200
someone who wants to build the next super successful startup in the tech space,

02:44:10.280 --> 02:44:12.680
the Googles, the apples, the twitters.

02:44:14.520 --> 02:44:18.120
Yeah. So the great thing about the really great founders is they don't take any advice. So,

02:44:19.800 --> 02:44:24.120
so if you find yourself listening to advice, maybe you shouldn't do it. Um,

02:44:24.120 --> 02:44:29.400
Well, that's actually just to elaborate on that. If you could also speak to great founders too.

02:44:29.400 --> 02:44:29.960
Yeah.

02:44:29.960 --> 02:44:31.640
Like what, what makes a great founder?

02:44:32.280 --> 02:44:38.440
So it makes a great founder is super smart, um, coupled with super energetic, coupled with super

02:44:38.440 --> 02:44:41.000
courageous. I think it's some of those, those three.

02:44:41.000 --> 02:44:43.240
And intelligence, passion and courage.

02:44:43.240 --> 02:44:46.920
The first two are traits and the third one is a choice. I think courage is a choice.

02:44:47.480 --> 02:44:51.240
Well, cause courage is a question of pain, tolerance, right? Um,

02:44:52.280 --> 02:44:56.600
so, um, how, how many times you will want to get punched in the face before you quit?

02:44:57.000 --> 02:44:57.080
Yeah.

02:44:57.080 --> 02:45:01.800
Um, and here's maybe the biggest thing people don't understand about what it's like to be a startup

02:45:01.800 --> 02:45:05.880
founder is it gets, it gets very romanticized, right? Um, and even when, even when they fail,

02:45:05.880 --> 02:45:08.680
it still gets romanticized about like what a great adventure it was, but like

02:45:09.560 --> 02:45:13.480
the reality of it is most of what happens is people telling you, no, and then they usually

02:45:13.480 --> 02:45:17.960
follow that with your stupid, right? No, I will not come to work for you. Um, and I will not leave

02:45:17.960 --> 02:45:21.000
my cushy job cool to come work for you. No, I'm not going to buy your products. You know,

02:45:21.000 --> 02:45:24.200
no, I'm not going to run a story about your company. No, I'm not this that the other thing.

02:45:24.840 --> 02:45:29.000
Um, and so a huge amount of what people have to do is just get used to just getting punched.

02:45:29.000 --> 02:45:32.200
And, and, and the reason people don't understand this is because when you're a founder, you cannot

02:45:32.200 --> 02:45:35.320
let on that this is happening because it will cause people to think that you're weak and they'll

02:45:35.320 --> 02:45:40.040
lose faith in you. Yeah. So you have to pretend that you're having a great time when you're dying

02:45:40.040 --> 02:45:46.440
inside, right? You're just in misery. But why, why do they do it? Why do they do it? Yeah,

02:45:46.440 --> 02:45:49.800
that's the thing. It's, it's like it is a level. This is actually one of the conclusions I think

02:45:49.800 --> 02:45:53.240
is it, I think it's actually, for most of these people on a risk adjusted basis, it's probably

02:45:53.240 --> 02:45:57.240
an irrational act. They could probably be more financially successful on average if they just

02:45:57.240 --> 02:46:01.880
got like a real job and in a big company. Um, but there's, you know, some people just have an

02:46:01.880 --> 02:46:05.640
irrational need to do something new and build something for themselves. And, you know, some

02:46:05.640 --> 02:46:08.920
people just can't tolerate having bosses. Oh, here's the fun thing is how do you reference

02:46:08.920 --> 02:46:12.520
check founders? Right? So you call it, you know, normally you reference check your time hiring

02:46:12.520 --> 02:46:15.880
somebody as you call the bosses there at their, you know, and you find out if they were good

02:46:15.880 --> 02:46:19.720
employees. And now you're trying to reference check Steve Jobs, right? And it's like, Oh God,

02:46:19.720 --> 02:46:22.680
he was terrible. You know, he was a terrible employee. He never did what we told him to do.

02:46:22.680 --> 02:46:29.000
Yeah. So what's a good reference? If you want the previous boss to actually say that

02:46:29.720 --> 02:46:32.840
they never did what you told them to do, that might be a good thing.

02:46:32.840 --> 02:46:36.680
Well, ideally, ideally what you want is I will go, I would like to go to work for that person.

02:46:37.720 --> 02:46:41.400
He worked for me here and now I'd like to work for him. Now, unfortunately, most people can't,

02:46:41.400 --> 02:46:45.240
their egos can't, can't handle that. So they won't say that, but that's the ideal.

02:46:45.240 --> 02:46:50.440
What advice would you give to those folks in the space of intelligence, passion and courage?

02:46:51.160 --> 02:46:55.320
So I think the other big thing is you see people sometimes who say, I want to start a company

02:46:55.320 --> 02:46:59.080
and then they kind of work through the process of coming up with an idea. And generally, those

02:46:59.080 --> 02:47:03.880
don't work as well as the case where somebody has the idea first, and then they kind of realize

02:47:03.880 --> 02:47:06.920
that there's an opportunity to build a company and then they just turn out to be the right kind

02:47:06.920 --> 02:47:12.520
of person to do that. When you say idea, do you mean long-term big vision or do you mean

02:47:12.520 --> 02:47:18.360
specifics of like product? I would say specific. Like specifically what, yeah, specifics. Like,

02:47:18.360 --> 02:47:21.160
what is the, because for the first five years, you don't get to have vision. You just got to

02:47:21.160 --> 02:47:24.520
build something people want and you got to figure out a way to sell it to them, right? It's very

02:47:24.520 --> 02:47:29.240
practical or you never get to big vision. So the first, the first part, you have an idea of a

02:47:29.240 --> 02:47:32.520
set of products or the first product that can actually make some money. Yeah. Like it's got to,

02:47:32.520 --> 02:47:35.880
the first product's got to work, by which I mean like it has to technically work, but then it has

02:47:35.880 --> 02:47:39.640
to actually fit into the category in the customer's mind of something that they want. And then, and

02:47:39.640 --> 02:47:42.520
then by the way, the other part is they have to want to pay for it. Like somebody's got to pay

02:47:42.520 --> 02:47:45.800
the bills. And so you got to figure out how to price it and whether you can actually extract the

02:47:45.800 --> 02:47:53.320
money. So usually it is much more predictable. Success is never predictable, but it's more

02:47:53.320 --> 02:47:57.880
predictable if you start with a great idea and then back into starting the company. So this is

02:47:57.880 --> 02:48:01.240
what we did, you know, we had Mosaic before we had Escape. The Google guys had the Google search

02:48:01.240 --> 02:48:06.600
engine working at Stanford, right? The, you know, yeah, actually, there's tons of examples where

02:48:06.600 --> 02:48:11.400
they, you know, Pierre Omadir had eBay working before he left his previous job. So I really love

02:48:11.400 --> 02:48:16.360
that idea of just having a thing, a prototype that actually works before you even begin to

02:48:16.360 --> 02:48:21.000
remotely scale. Yeah. By the way, it's also far easier to raise money, right? Like the ideal pitch

02:48:21.000 --> 02:48:24.040
that we receive is, here's the thing that works. Would you like to invest in our company or not?

02:48:24.040 --> 02:48:30.200
Like that's so much easier than here's 30 slides with a dream, right? And then we have this concept

02:48:30.200 --> 02:48:34.200
called the idea maze, which our biology friend of Boston came up with when he was with us.

02:48:35.080 --> 02:48:39.320
So, so, so then there's this thing, this goes to mythology, which is, you know, there's a

02:48:39.320 --> 02:48:43.240
mythology that kind of, you know, these, these ideas, you know, kind of arrive like magic or

02:48:43.240 --> 02:48:46.200
people kind of stumble into them. It's like eBay with the pest dispensers or something.

02:48:46.760 --> 02:48:52.920
The reality usually with the big successes is that the founder has been chewing on the problem

02:48:52.920 --> 02:48:57.880
for five or 10 years before they start the company. And they often worked on it in school,

02:48:58.840 --> 02:49:03.320
or they even experimented on it when they were a kid. And they've been kind of training up over

02:49:03.320 --> 02:49:06.680
that period of time to be able to do the thing. So they're like a true domain expert.

02:49:07.560 --> 02:49:11.080
And it sort of sounds like mom and apple pie, which is, yeah, you want to be a domain expert

02:49:11.080 --> 02:49:14.760
in what you're doing. But you would, you know, the mythology is so strong of like, oh, I just

02:49:14.760 --> 02:49:18.360
like had this idea in the shower and now I'm doing it. Like it's generally not that.

02:49:18.360 --> 02:49:26.280
No, because it's, well, maybe in the shower, we had the exact product implementation details.

02:49:26.280 --> 02:49:30.600
But yeah, usually you're going to be for like years, if not decades, thinking about

02:49:32.280 --> 02:49:38.760
like everything around that. Well, we call it the idea maze because the idea maze basically is

02:49:38.760 --> 02:49:42.440
like there's all these permutations. Like for any idea, there's like all these different

02:49:42.440 --> 02:49:45.480
permutations, who should the customer be, what shape, form should the product have,

02:49:45.480 --> 02:49:50.840
and how should we take it to market and all these things. And so the really smart founders

02:49:50.840 --> 02:49:53.480
have thought through all these scenarios by the time they go out to raise money.

02:49:54.360 --> 02:49:58.760
And they have like detailed answers on every one of those fronts because they put so much

02:49:58.760 --> 02:50:04.520
thought into it. The sort of the sort of more haphazard founders haven't thought about any of

02:50:04.520 --> 02:50:06.520
that. And it's the detailed ones who tend to do much better.

02:50:06.520 --> 02:50:11.000
So how do you know what to take a leap? If you have a cushy job or happy life?

02:50:11.960 --> 02:50:15.160
I mean, the best reason is just because you can't tolerate not doing it, right?

02:50:15.160 --> 02:50:17.960
This is the kind of thing where if you have to be advised into doing it, you probably shouldn't do

02:50:17.960 --> 02:50:22.760
it. And so it's probably the opposite, which is you just have such a burning sense of this has

02:50:22.760 --> 02:50:27.080
to be done. I have to do this. I have no choice. What if it's going to lead to a lot of pain?

02:50:27.080 --> 02:50:32.520
It's going to lead to a lot of pain. I think that's what if it means losing sort of social

02:50:32.520 --> 02:50:38.280
relationships and damaging your relationship with loved ones and all that kind of stuff.

02:50:38.360 --> 02:50:42.520
Yeah, look, so it's going to put you in a social tunnel for sure, right? So you're going to like,

02:50:44.200 --> 02:50:48.120
there's this game you can play on Twitter, which is you can do any whiff of the idea that there's

02:50:48.120 --> 02:50:51.800
basically any such thing as work-life balance and that people should actually work hard and

02:50:51.800 --> 02:50:55.560
everybody gets mad. But like the truth is, all the successful founders are working 80 hour weeks

02:50:55.560 --> 02:51:00.680
and they form very strong social bonds with the people they work with. They tend to lose a lot

02:51:00.680 --> 02:51:05.160
of friends on the outside or put those friendships on ice. That's just the nature of the thing.

02:51:05.560 --> 02:51:09.480
You know, for most people, that's worth the trade-off. You know, the advantage maybe younger

02:51:09.480 --> 02:51:12.360
founders have is maybe they have less, you know, maybe they're not, you know, for example,

02:51:12.360 --> 02:51:14.920
if they're not married yet or don't have kids yet, that's an easier thing to bite off.

02:51:15.560 --> 02:51:16.760
Can you be an older founder?

02:51:16.760 --> 02:51:20.600
Yeah, you definitely can. Yeah. Yeah. Many of the most successful founders are second,

02:51:20.600 --> 02:51:24.600
third, fourth time founders. They're in their 30s, 40s, 50s. The good news of being an older

02:51:24.600 --> 02:51:29.000
founder is you know more and you know a lot more about what to do, which is very helpful. The problem

02:51:29.000 --> 02:51:33.400
is, okay, now you've got like a spouse and a family and kids and like you've got to go to the baseball

02:51:33.400 --> 02:51:35.960
game and like you can't go to the baseball, you know, and so it's good.

02:51:37.080 --> 02:51:39.160
Life is full of difficult choices. Yes.

02:51:39.160 --> 02:51:45.800
I can't reason. You've written a blog post on what you've been up to. You wrote this in October 2022.

02:51:47.000 --> 02:51:53.240
Quote, mostly I try to learn a lot. For example, the political events of 2014 to 2016 made clear

02:51:53.240 --> 02:51:58.520
to me that I didn't understand politics at all. Referencing maybe some of this book here.

02:51:58.600 --> 02:52:04.200
So, I deliberately withdrew from political engagement and fundraising and instead read

02:52:04.200 --> 02:52:10.360
my way back into history and as far to the political left and political right as I could.

02:52:10.360 --> 02:52:13.400
So, just high level question. What's your approach to learning?

02:52:14.440 --> 02:52:21.080
Yeah, so it's basically, I would say it's auto-divac. So, it's sort of going down the rabbit holes.

02:52:21.960 --> 02:52:25.560
So, it's a combination. I kind of alluded to it in that quote. It's a combination of

02:52:25.560 --> 02:52:30.600
breadth and depth. And so, I tend to, yeah, I tend to, I go broad by the nature of what I do,

02:52:30.600 --> 02:52:34.040
I go broad, but then I tend to go deep in a rabbit hole for a while, read everything I can,

02:52:34.040 --> 02:52:38.200
and then come out of it. And I might not revisit that rabbit hole for, you know, another decade.

02:52:38.200 --> 02:52:43.240
And in that blog post that I recommend people go check out, you actually list a bunch of different

02:52:43.240 --> 02:52:47.640
books that you recommend on different topics on the American left and the American right.

02:52:49.000 --> 02:52:53.160
It's just a lot of really good stuff. The best explanation for the current structure of our

02:52:53.160 --> 02:52:57.800
society and politics, you give two recommendations, four books on the Spanish Civil War, six books on

02:52:57.800 --> 02:53:04.440
deep history of the American right, comprehensive biographies of Adolf Hitler, one of which I read

02:53:04.440 --> 02:53:09.320
can recommend, six books on the deep history of the American left. So, American right and American

02:53:09.320 --> 02:53:15.880
left looking at the history to give you the context. Biography of Vladimir Lenin, two of them

02:53:16.760 --> 02:53:20.760
on the French Revolution. Actually, I have never read a biography on Lenin. Maybe that will be

02:53:20.760 --> 02:53:26.040
useful. Everything's been so Marx focused. The Sebastian biography of Lenin is extraordinary.

02:53:26.840 --> 02:53:30.680
Victor Sebastian, okay. It'll blow your mind. Yeah. So, it's still useful to read. It's incredible.

02:53:30.680 --> 02:53:33.400
Yeah, it's incredible. I actually think it's the single best book on the Soviet Union.

02:53:33.960 --> 02:53:38.360
So, that, the perspective of Lenin, it might be the best way to look at the Soviet Union versus

02:53:38.360 --> 02:53:44.280
Stalin versus Marx versus, very interesting. So, two books on fascism and anti-fascism

02:53:45.160 --> 02:53:50.920
by the same author, Paul Gottry, a brilliant book on the nature of mass movements and collective

02:53:50.920 --> 02:53:55.720
psychology, the definitive work on intellectual life under totalitarianism, the captive mind,

02:53:56.360 --> 02:54:01.640
the definitive work on the practical life under totalitarianism. There's a bunch, there's a bunch,

02:54:01.640 --> 02:54:07.000
and the single best book. First of all, the list here is just incredible. But you say the single

02:54:07.000 --> 02:54:13.160
best book I have found on who we are and how we got here is the ancient city by Neuma Dennis,

02:54:13.240 --> 02:54:21.400
Fostel de Kulankis. I like it. What did you learn about who we are as a human civilization from

02:54:21.400 --> 02:54:25.800
that book? Yeah. So, this is a fascinating book. This one's free, by the way. It's a book from

02:54:25.800 --> 02:54:31.080
the 1860s. You can download it or you can buy prints of it. But it was this guy who was a

02:54:31.080 --> 02:54:37.160
professor at the Sorbonne in the 1860s, and he was apparently a savant on antiquity, on Greek

02:54:37.160 --> 02:54:42.440
and Roman antiquity. And the reason I say that is because his sources are 100% original Greek and

02:54:42.440 --> 02:54:47.960
Roman sources. So, he wrote basically a history of Western civilization from on the order of 4,000

02:54:47.960 --> 02:54:53.720
years ago to basically the present times entirely working on original Greek and Roman sources.

02:54:54.920 --> 02:54:59.080
And what he was specifically trying to do was he was trying to reconstruct from the stories of the

02:54:59.080 --> 02:55:02.760
Greeks and the Romans. He was trying to reconstruct what life in the West was like before the Greeks

02:55:02.760 --> 02:55:10.120
and the Romans, which was in the civilization known as the Indo-Europeans. And the short answer,

02:55:10.120 --> 02:55:16.120
and this is sort of circa 4,000, 2,000 BC to, you know, sort of 500 BC, kind of that 1500 year

02:55:16.120 --> 02:55:22.040
stretch where civilization developed. And his conclusion was basically cults. They were basically

02:55:22.040 --> 02:55:28.520
cults. And civilization was organized into cults. And the intensity of the cults was like a million

02:55:28.520 --> 02:55:34.040
fold beyond anything that we would recognize today. Like it was a level of all-encompassing

02:55:34.600 --> 02:55:41.320
belief and an action around religion. That was at a level of extremeness that we wouldn't even

02:55:41.320 --> 02:55:47.240
recognize it. And so specifically, he tells the story of basically, there were three levels of

02:55:47.240 --> 02:55:52.760
cults. There was the family cult, the tribal cult, and then the city cult as society scaled up.

02:55:53.320 --> 02:56:00.040
And then each cult was a joint cult of family gods, which were ancestor gods and then nature gods.

02:56:00.760 --> 02:56:05.880
And then your bonding into a family, a tribe, or a city was based on your adherence to that

02:56:05.880 --> 02:56:12.680
religion. People who were not of your family tribe city worshiped different gods, which gave you

02:56:12.680 --> 02:56:19.000
not just the right with the responsibility to kill them on sight. Right? So they were serious

02:56:19.000 --> 02:56:23.800
about their cults. Hard core. By the way, shocking development, I did not realize there's a zero

02:56:23.800 --> 02:56:27.720
concept of individual rights. Like even up through the Greeks and even in the Romans,

02:56:27.720 --> 02:56:30.840
they didn't have the concept of individual rights. Like the idea that as an individual,

02:56:30.840 --> 02:56:34.840
you have some rights, just like noop. Right? And you look back and you're just like, wow,

02:56:34.840 --> 02:56:38.600
that's just like crazily like fascist and a degree that we wouldn't recognize today. But it's like,

02:56:38.600 --> 02:56:44.040
well, they were living under extreme pressure for survival. And the theory goes, you could not have

02:56:44.040 --> 02:56:46.760
people running around making claims to individual rights when you're just trying to get like your

02:56:46.760 --> 02:56:51.640
tribe through the winter, right? Like you need like hardcore command and control. And actually,

02:56:52.200 --> 02:56:55.400
through modern political lens, those cults were basically both fascist and communist.

02:56:56.040 --> 02:56:59.480
They were fascist in terms of social control, and then they were communist in terms of economics.

02:57:01.160 --> 02:57:03.960
But you think that's fundamentally that like pull towards

02:57:04.920 --> 02:57:06.520
cults is within us?

02:57:06.520 --> 02:57:12.920
Well, so my conclusion from this book, so the way we naturally think about the world we live in

02:57:12.920 --> 02:57:17.880
today is like, we basically have such an improved version of everything that came before us, right?

02:57:17.880 --> 02:57:21.320
Like we have basically, we've figured out all these things around morality and ethics and

02:57:21.320 --> 02:57:24.600
democracy and all these things. And like they were basically stupid and retrograde and were

02:57:24.600 --> 02:57:29.960
like smart and sophisticated. And we've improved all this. I after reading that book, I now believe

02:57:29.960 --> 02:57:33.960
in many ways the opposite, which is no, actually, we are still running in that original model,

02:57:33.960 --> 02:57:39.160
we're just running in an incredibly diluted version of it. So we're still running basically in

02:57:39.160 --> 02:57:43.400
cults. It's just our cults are at like a thousandth or a millionth the level of intensity, right?

02:57:43.400 --> 02:57:49.080
And so our, so just to take religions, you know, the modern experience of a Christian in our time,

02:57:49.080 --> 02:57:53.400
even somebody who considers him a devout Christian is just a shadow of the level of intensity of

02:57:53.400 --> 02:57:57.880
somebody who belonged to a religion back in that period. And then by the way, we have constraints,

02:57:57.880 --> 02:58:03.160
it goes back to our discussion, we then sort of endlessly create new cults. Like we're trying

02:58:03.160 --> 02:58:09.000
to fill the void, right? And the void is a void of bonding. Okay. Living in their era,

02:58:09.000 --> 02:58:12.760
like everybody living today, transport in that era would view it as just like completely intolerable

02:58:12.760 --> 02:58:16.920
in terms of like the loss of freedom and the level of basically fascist control. However,

02:58:16.920 --> 02:58:20.280
every single person in that era, and he really stresses this, they knew exactly where they

02:58:20.280 --> 02:58:24.120
stood. They knew exactly where they belonged. They knew exactly what their purpose was.

02:58:24.120 --> 02:58:27.000
They know exactly what they needed to do every day. They know exactly why they were doing it.

02:58:27.000 --> 02:58:29.400
They had total certainty about their place in the universe.

02:58:29.400 --> 02:58:33.640
So the question of meaning, the question of purpose was very distinctly clearly defined for

02:58:33.640 --> 02:58:37.720
them. Absolutely. Overwhelmingly, undisputably, undeniably.

02:58:38.360 --> 02:58:43.880
As we turn the volume down on the cultism, we start to the search for meanings that's getting

02:58:43.880 --> 02:58:47.640
harder and harder. Yes. Because we don't have that. We are, we are ungrounded. We are, we are,

02:58:47.640 --> 02:58:51.640
we are uncentered and we, and we all feel it, right? And that's why we reach for, you know,

02:58:51.640 --> 02:58:55.720
it's why we still reach for religion. It's why we reach for, you know, we, people start to take

02:58:55.720 --> 02:58:58.520
on, you know, let's say, you know, a faith in science, maybe beyond where they should put it.

02:58:59.320 --> 02:59:02.280
You know, and by the way, like sports teams are like a, you know, they're like a tiny little

02:59:02.280 --> 02:59:06.120
version of a cult and, you know, the, you know, Apple keynotes are a tiny little version of a cult,

02:59:06.840 --> 02:59:11.080
right? You know, political, you know, and there's cult, you know, there's full blown

02:59:11.080 --> 02:59:14.520
cults on both sides of the political spectrum right now, right? You know, operating in plain

02:59:14.600 --> 02:59:17.080
science. But still not full blown, compared as to what it was.

02:59:17.080 --> 02:59:20.040
Compared to what it used to be. I mean, we would today consider full blown, but like,

02:59:20.040 --> 02:59:23.240
yes, they're, they're at like, I don't know, a hundred thousandth or something of the intensity

02:59:23.240 --> 02:59:27.800
of what people had back then. So, so we live in a world today that in many ways is more advanced

02:59:27.800 --> 02:59:31.160
and moral and so forth. And it's certainly a lot nicer, much nicer world to live in, but we live

02:59:31.160 --> 02:59:36.440
in a world that's like very washed out. It's like everything has become very colorless and gray as

02:59:36.440 --> 02:59:40.360
compared to how people used to experience things, which is I think why we're so prone to reach for

02:59:40.360 --> 02:59:45.160
drama. There's something in us that's deeply evolved where we want that back.

02:59:46.520 --> 02:59:50.840
And I wonder where it's all headed as we turn the volume down more and more.

02:59:50.840 --> 02:59:55.720
What advice would you give to young folks today? In high school and college, how to be

02:59:55.720 --> 02:59:58.440
successful in their career, how to be successful in their life?

02:59:58.440 --> 03:00:04.040
Yes. So the tools that are available today, I mean, are just like, I sometimes, you know,

03:00:04.040 --> 03:00:08.040
I sometimes bore, you know, kids by describing like what it was like to go look up a book,

03:00:08.040 --> 03:00:11.960
you know, to try to like discover a fact and, you know, in the old days, the 1970s, 1980s and

03:00:11.960 --> 03:00:15.000
go to the library and the card catalog and the whole thing. You go through all that work and

03:00:15.000 --> 03:00:19.160
then the book is checked out and you have to wait two weeks and like, like to be in a world,

03:00:19.160 --> 03:00:22.760
not only where you can get the answer to any question, but also the world now, you know,

03:00:22.760 --> 03:00:25.400
the AI world where you've got like the assistant that will help you do anything,

03:00:25.400 --> 03:00:30.200
help you teach, learn anything, like your ability both to learn and also to produce is just like,

03:00:30.200 --> 03:00:34.520
I don't know, a million fold beyond what it used to be. I have a blog post I've been wanting to

03:00:34.520 --> 03:00:40.040
write. It was, I call out where, where are the hyperproductive people? Like,

03:00:40.040 --> 03:00:44.760
Good question. Right. Like with these tools, like there should be authors that are writing like

03:00:44.760 --> 03:00:49.320
hundreds or thousands of like outstanding books. Well, with the authors, there's a consumption

03:00:49.320 --> 03:00:55.720
question too. But yeah, well, maybe not, maybe not. You're right. But so the tools are much more

03:00:55.720 --> 03:01:01.080
powerful. Artists, musicians, right? Why aren't musicians producing a thousand times the number

03:01:01.080 --> 03:01:07.880
of songs, right? Like, like the tools are spectacular. So what, what's the explanation?

03:01:07.880 --> 03:01:14.360
And by way of advice, like, what is motivation starting to be turned down a little bit or what?

03:01:14.360 --> 03:01:18.280
I think it might be distraction. It's so easy to just sit and consume.

03:01:19.480 --> 03:01:23.880
That I think people get distracted from production. But if you wanted to, you know,

03:01:23.880 --> 03:01:28.040
as a young person, if you wanted to really stand out, you could get on like a hyperproductivity

03:01:28.040 --> 03:01:32.920
curve very early on. There's a great, you know, the story, there's a great story in Roman history

03:01:32.920 --> 03:01:37.880
of plenty of the elder who was this legendary statesman. And I died in the Vesuvius eruption

03:01:37.880 --> 03:01:42.200
trying to rescue his friends. But he was famous both for being a savant of basically being a

03:01:42.200 --> 03:01:45.800
polymath, but also being an author. And he wrote apparently like hundreds of books, most of which

03:01:45.800 --> 03:01:49.880
have been lost, but he like wrote all these encyclopedias. And he literally like would be

03:01:49.880 --> 03:01:53.720
reading and writing all day long, no matter what else is going on. And he so he would like travel

03:01:53.720 --> 03:01:56.840
with like four slaves, and two of them were responsible for reading to him. And two of them

03:01:56.840 --> 03:02:01.000
were responsible for taking dictation. And so like, he'd be going across country and like

03:02:01.000 --> 03:02:04.920
literally he would be writing books like all the time. And apparently they were spectacular.

03:02:04.920 --> 03:02:07.240
There's only a few that have survived, but apparently they were amazing.

03:02:07.240 --> 03:02:10.680
So there's a lot of value to being somebody who finds focus in this life.

03:02:10.680 --> 03:02:14.040
Yeah, like, and there are examples, like there are, you know, there's this guy,

03:02:14.040 --> 03:02:18.520
judge was just named Posner, Posner, who wrote like 40 books and was also a great federal judge.

03:02:19.160 --> 03:02:22.600
You know, there's our friend, Balji, I think it's like this, he's one of these,

03:02:22.600 --> 03:02:26.920
you know, where his output is just prodigious. And so it's like, yeah, I mean, with these tools,

03:02:26.920 --> 03:02:30.840
why not? And I kind of think we're at this interesting kind of freeze frame moment where

03:02:30.840 --> 03:02:33.320
like this, these tools are not everybody's hands and everybody's just kind of staring

03:02:33.320 --> 03:02:36.920
at them trying to figure out what to do. The new tools. We have discovered fire.

03:02:36.920 --> 03:02:40.280
Yeah. And trying to figure out how to use it to cook. Yeah, right.

03:02:41.560 --> 03:02:46.680
You told Tim Ferriss that the perfect day is caffeine for 10 hours and alcohol for four hours.

03:02:47.640 --> 03:02:52.760
You didn't think I'd be mentioning this, did you? It balances everything out perfectly,

03:02:52.760 --> 03:02:59.080
as you said. So perfect. So let me ask, what's the secret to balance and maybe to happiness in life?

03:03:00.520 --> 03:03:04.440
I don't believe in balance. So I'm the wrong person to ask. Can you elaborate

03:03:04.440 --> 03:03:08.600
why you don't believe in balance? I mean, maybe it's just, and I look, I think people,

03:03:08.600 --> 03:03:12.360
I think people are wired differently. So I think it's hard to generalize this kind of thing. But

03:03:12.440 --> 03:03:15.560
I'm much happier and more satisfied when I'm fully committed to something. So

03:03:16.120 --> 03:03:18.360
I'm very much in favor of imbalance. Yeah.

03:03:19.480 --> 03:03:22.680
In balance. And that applies to work, to life, to everything.

03:03:23.640 --> 03:03:27.800
Yeah. Now, I happen to have whatever twist of personality traits lead that in non-destructive

03:03:27.800 --> 03:03:32.600
dimensions, including the fact that I've actually, I now no longer do the 10-4 plan. I stopped drinking.

03:03:32.600 --> 03:03:36.360
I do the caffeine, but not the alcohol. So there's something in my personality where I,

03:03:36.360 --> 03:03:40.600
I, whatever maladaption I have is inclining me towards productive things, not on productive

03:03:40.600 --> 03:03:46.680
things. So you're one of the wealthiest people in the world. What's the relationship between wealth

03:03:46.680 --> 03:03:54.840
and happiness? Oh, money and happiness. So I think happiness, I don't think happiness is the thing

03:03:55.720 --> 03:03:57.880
to strive for. I think satisfaction is the thing.

03:03:59.000 --> 03:04:02.280
That's, that just sounds like happiness, but turned down a bit.

03:04:02.280 --> 03:04:07.400
No deeper. So happiness is, you know, a walk in the woods at sunset, an ice cream cone,

03:04:08.360 --> 03:04:13.240
a kiss. The first ice cream cone is great. This 1000th ice cream cone,

03:04:14.200 --> 03:04:16.440
not so much at some point, the walks in the woods get boring.

03:04:16.440 --> 03:04:20.040
What's the distinction between happiness and satisfaction?

03:04:20.040 --> 03:04:25.880
Satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful.

03:04:26.600 --> 03:04:34.920
So just something that permeates all your days, just this general contentment of being useful.

03:04:34.920 --> 03:04:39.240
Then I'm fully satisfying my faculties, then I'm fully delivering, right,

03:04:39.240 --> 03:04:42.360
on the gifts that I've been given, that I'm, you know, net making the world better,

03:04:42.360 --> 03:04:46.680
that I'm contributing to the people around me, right? And then I can look back and say,

03:04:46.680 --> 03:04:51.080
wow, that was hard, but it was worth it. I think generally it seems to leave people in a better

03:04:51.080 --> 03:04:54.760
state than pursuit of pleasure, pursuit of quote unquote happiness.

03:04:54.760 --> 03:04:56.200
Does money have anything to do with that?

03:04:56.200 --> 03:04:59.880
I think the founders, the founding fathers in the US threw this off kilter when they used the

03:04:59.880 --> 03:05:01.880
phrase pursuit of happiness, I think they should have said.

03:05:02.680 --> 03:05:03.480
Pursuit of satisfaction.

03:05:03.480 --> 03:05:05.960
They said pursuit of satisfaction, we might live in a better world today.

03:05:05.960 --> 03:05:08.520
Well, you know, they could have elaborated on a lot of things.

03:05:09.400 --> 03:05:10.680
They could have tweaked the second amendment.

03:05:10.680 --> 03:05:13.480
I think they were smarter than they realized. They said, you know what,

03:05:13.480 --> 03:05:17.560
we're going to make it ambiguous and let these humans figure out the rest,

03:05:17.560 --> 03:05:20.520
these tribal cult like humans figure out the rest.

03:05:23.320 --> 03:05:24.760
But money empowers that.

03:05:24.760 --> 03:05:29.000
So I think, and I think they're, I mean, look, I think Elon is, I don't think I'm even a great

03:05:29.000 --> 03:05:31.560
example, but I think Elon would be the great example of this, which is like, you know, look,

03:05:31.560 --> 03:05:35.000
he's a guy who from every day of his life, from the day he started making money at all,

03:05:35.000 --> 03:05:36.920
he just plows into the next thing.

03:05:38.040 --> 03:05:41.800
And so I think money is definitely an enabler for satisfaction.

03:05:41.800 --> 03:05:44.200
Money applied to happiness leads people down very dark paths.

03:05:46.200 --> 03:05:47.560
Very destructive avenues.

03:05:48.360 --> 03:05:51.160
Money applied to satisfaction, I think could be, is a real tool.

03:05:52.120 --> 03:05:55.480
I always, by the way, I was like, you know, Elon is the case study for behavior.

03:05:55.480 --> 03:05:59.720
But the other thing that's always really made me think is Larry Page was asked one time what

03:05:59.720 --> 03:06:03.240
his approach to philanthropy was, and he said, oh, I'm just my philanthropic plan is just give

03:06:03.240 --> 03:06:04.040
all the money to Elon.

03:06:06.600 --> 03:06:06.840
Right.

03:06:06.840 --> 03:06:13.000
Well, let me actually ask you about Elon. What are your, you've interacted with quite a lot of

03:06:13.000 --> 03:06:16.920
successful engineers and business people. What do you think is special about Elon?

03:06:16.920 --> 03:06:18.200
We talked about Steve Jobs.

03:06:18.920 --> 03:06:23.800
What, what do you think is special about him as a leader as an innovator?

03:06:23.800 --> 03:06:29.240
Yeah. So the, the core of it is he's, he's, he's back to the future. So he is, he is doing the

03:06:29.240 --> 03:06:32.680
most leading edge things in the world, but with a really deeply old school approach.

03:06:33.560 --> 03:06:38.520
And so to find comparisons to Elon, you need to go to like Henry Ford and Thomas Watson and Howard

03:06:38.520 --> 03:06:46.280
Hughes and Andrew Carnegie, right, Leland Stanford, John T. Rockefeller, right, you need to go to the,

03:06:46.280 --> 03:06:50.680
what we're called the bourgeois capitalists, like the hardcore business owner operators who

03:06:50.680 --> 03:06:57.320
basically built, you know, basically built industrialized society, Vanderbilt. And it's

03:06:57.320 --> 03:07:07.880
a level of hands-on commitment and depth in the business, coupled with an absolute

03:07:07.880 --> 03:07:14.200
priority towards truth and towards kind of put science and technology

03:07:14.840 --> 03:07:19.400
down to first principles that is just like absolute, just like unbelievably absolute.

03:07:20.120 --> 03:07:23.960
He really is ideal that he's only ever talking to engineers. Like he does not tolerate,

03:07:24.760 --> 03:07:29.640
he has, he has less bullshit talents than anybody I've ever met. He wants ground truth on every

03:07:29.640 --> 03:07:34.680
single topic. And he runs his businesses directly day to day devoted to getting to ground truth in

03:07:34.680 --> 03:07:41.320
every single topic. So you think it was a good decision for him to buy Twitter?

03:07:41.320 --> 03:07:43.640
I have developed a view in life did not second guess Elon Musk.

03:07:44.280 --> 03:07:49.000
I know this is going to sound crazy and unfounded, but

03:07:49.720 --> 03:07:53.000
well, I mean, he's got a quite a track record.

03:07:53.000 --> 03:07:55.960
I mean, look, the car was a crazy, I mean, the car was, I mean, look,

03:07:55.960 --> 03:07:59.160
he's done a lot of things that seem crazy, starting a new car company in the United

03:07:59.160 --> 03:08:03.000
States of America. The last time somebody really tried to do that was the 1950s. And it was called

03:08:03.000 --> 03:08:06.680
Tucker Automotive. And it was such a disaster, they made a movie about what a disaster it was.

03:08:07.800 --> 03:08:11.720
And then Rockets, like who does that? Like that's, there's obviously no way to start

03:08:11.800 --> 03:08:15.160
in a rocket company like those days are over. And then to do those at the same time.

03:08:16.040 --> 03:08:22.200
So after he pulled those two off, like, okay, fine. Like, this is one of my areas of like,

03:08:22.200 --> 03:08:25.640
whatever opinions I had about that is just like, okay, clearly or not relevant, like this is,

03:08:26.280 --> 03:08:27.640
at some point, you just like put on the person.

03:08:28.200 --> 03:08:33.240
And in general, I wish more people would lean on celebrating and supporting versus deriding

03:08:33.240 --> 03:08:39.320
and destroying. Oh, yeah. I mean, look, he drives resentment, like it's like he is a magnet for

03:08:39.320 --> 03:08:45.080
resentment. Like his critics are the most miserable, resentful people in the world. Like

03:08:45.080 --> 03:08:50.360
it's almost a perfect match of like the most idealized, you know, technologists, you know,

03:08:50.360 --> 03:08:55.240
of the century coupled with like just his critics are just bitter as can be. I mean,

03:08:55.240 --> 03:09:02.360
it's sort of very darkly comic to watch. Well, he, he fuels the fire of that by being an asshole on

03:09:02.360 --> 03:09:08.360
Twitter at times. And which is fascinating to watch the drama of human civilization given our cult

03:09:09.080 --> 03:09:16.680
roots just fully on fire. He's running a cult. You could say that very successfully.

03:09:16.680 --> 03:09:21.080
So now, now there are cults have gone and we search for meaning. What do you think is the

03:09:21.080 --> 03:09:25.080
meaning of this whole thing? What's the meaning of life, Mark Andreessen? I don't know the answer

03:09:25.080 --> 03:09:31.640
to that. I think the meaning of, of the closest I get to it is what I said about satisfaction. So

03:09:31.640 --> 03:09:35.720
it's basically like, okay, we were given what we have, like we should basically do our best.

03:09:35.720 --> 03:09:40.040
What's the role of love in that mix? I mean, like, what's the point of life if you're, yeah,

03:09:40.040 --> 03:09:44.120
without love, like, yeah. So love is a big part of that satisfaction.

03:09:44.120 --> 03:09:47.640
Yeah, look, like taking care of people is like a wonderful thing. Like, you know,

03:09:47.640 --> 03:09:51.960
a mentality, you know, there are pathological forms of taking care of people, but there's also a

03:09:51.960 --> 03:09:55.320
very fundamental, you know, kind of aspect of taking care of people. Like for example, I happen

03:09:55.320 --> 03:09:58.680
to be somebody who believes that capitalism and taking care of people are actually, they're

03:09:58.680 --> 03:10:02.680
actually the same thing. Somebody once said capitalism is how you take care of people you

03:10:02.680 --> 03:10:08.360
don't know, right? Right. And so like, yeah, I think it's like deeply woven into the whole thing.

03:10:09.080 --> 03:10:11.640
You know, there's a long conversation to be had about that, but yeah.

03:10:12.440 --> 03:10:17.000
Yeah, creating products that are used by millions of people and bring them joy in smaller big ways.

03:10:17.560 --> 03:10:20.920
And then capitalism kind of enables that, encourages that.

03:10:21.640 --> 03:10:25.400
David Friedman says there's only three ways to get somebody to do something for somebody else.

03:10:26.120 --> 03:10:27.640
Love, money and force.

03:10:32.280 --> 03:10:33.560
Love and money are better.

03:10:34.840 --> 03:10:37.560
That's a good ordering. I think we should bet on those.

03:10:37.560 --> 03:10:42.600
Try love first. If that doesn't work, the money and then force. Well, don't even try that one.

03:10:43.400 --> 03:10:47.480
Mark, you're an incredible person. I've been a huge fan. I'm glad to finally got a chance to talk.

03:10:47.480 --> 03:10:52.360
I'm a fan of everything you do, everything you do, including on Twitter. It's a huge honor to

03:10:52.360 --> 03:10:55.480
meet you to talk with you. Thanks again for doing this. Awesome. Thank you, Alex.

03:10:56.520 --> 03:11:00.360
Thanks for listening to this conversation with Mark Andreessen. To support this podcast,

03:11:00.360 --> 03:11:05.320
please check out our sponsors in the description. And now let me leave you with some words from

03:11:05.320 --> 03:11:11.320
Mark Andreessen himself. The world is a very malleable place. If you know what you want,

03:11:11.880 --> 03:11:18.120
and you go for it with maximum energy and drive and passion, the world will often reconfigure

03:11:18.120 --> 03:11:23.960
itself around you much more quickly and easily than you would think. Thank you for listening,

03:11:23.960 --> 03:11:34.920
and hope to see you next time.

