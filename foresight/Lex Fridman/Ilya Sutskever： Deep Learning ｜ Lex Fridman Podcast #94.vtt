WEBVTT

00:00.000 --> 00:05.360
The following is a conversation with Ilya Satskeva, co-founder and chief scientist of Open AI,

00:06.080 --> 00:12.800
one of the most cited computer scientists in history with over 165,000 citations,

00:13.440 --> 00:19.040
and to me, one of the most brilliant and insightful minds ever in the field of deep learning.

00:19.920 --> 00:24.240
There are very few people in this world who I would rather talk to and brainstorm with about

00:24.240 --> 00:31.920
deep learning, intelligence, and life in general than Ilya, on and off the mic. This was an honor

00:31.920 --> 00:37.120
and a pleasure. This conversation was recorded before the outbreak of the pandemic,

00:37.120 --> 00:41.360
for everyone feeling the medical, psychological, and financial burden of this crisis,

00:41.360 --> 00:46.160
I'm sending love your way. Stay strong, we're in this together, we'll beat this thing.

00:47.120 --> 00:51.680
This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,

00:51.680 --> 00:56.240
review it with Five Stars and Apple Podcasts, support on Patreon, or simply connect with me on

00:56.240 --> 01:02.880
Twitter at Lex Freedman's belt F-R-I-D-M-A-N. As usual, I'll do a few minutes of ads now

01:02.880 --> 01:06.480
and never any ads in the middle that can break the flow of the conversation.

01:06.480 --> 01:09.840
I hope that works for you and doesn't hurt the listening experience.

01:10.880 --> 01:15.600
This show is presented by Cash App, the number one finance app in the App Store.

01:15.600 --> 01:20.880
When you get it, use code LexPodcast. Cash App lets you send money to friends,

01:20.880 --> 01:26.720
buy Bitcoin, invest in the stock market with as little as $1. Since Cash App allows you to buy

01:26.720 --> 01:32.400
Bitcoin, let me mention that cryptocurrency in the context of the history of money is fascinating.

01:32.960 --> 01:36.080
I recommend Ascent of Money as a great book on this history.

01:36.720 --> 01:42.000
Both the book and audiobook are great. Debits and credits on ledgers started around

01:42.000 --> 01:48.880
30,000 years ago. The US Dollar created over 200 years ago, and Bitcoin, the first decentralized

01:48.880 --> 01:54.880
cryptocurrency released just over 10 years ago. Given that history, cryptocurrency is still very

01:54.880 --> 02:00.720
much in its early days of development, but is still aiming to and just might redefine the nature of

02:00.720 --> 02:07.440
money. Again, if you get Cash App from the App Store or Google Play and use the code LexPodcast,

02:08.000 --> 02:14.160
you get $10 and Cash App will also donate $10 to FIRST, an organization that is helping advance

02:14.160 --> 02:20.560
robotics and STEM education for young people around the world. And now, here's my conversation

02:20.560 --> 02:27.680
with Ilya Satskeva. You were one of the three authors with Alex Koshchevsky, Jeff Hinton,

02:27.680 --> 02:35.200
of the famed AlexNet paper that is arguably the paper that marked the big catalytic moment that

02:35.200 --> 02:39.920
launched the deep learning revolution. At that time, take us back to that time. What was your

02:39.920 --> 02:44.880
intuition about neural networks, about the representation of power of neural networks?

02:45.920 --> 02:52.000
And maybe you could mention how did that evolve over the next few years, up to today, over the

02:52.000 --> 02:58.240
10 years? Yeah, I can answer that question. At some point in about 2010 or 2011,

02:59.920 --> 03:08.080
I connected two facts in my mind. Basically, the realization was this. At some point,

03:08.080 --> 03:13.520
we realized that we can train very large, I shouldn't say very tiny by today's standards, but

03:14.320 --> 03:19.200
large and deep neural networks end to end with back propagation. At some point,

03:20.640 --> 03:26.320
different people obtained this result. I obtained this result. The first moment in which I realized

03:26.320 --> 03:32.160
that deep neural networks are powerful was when James Martens invented the Hessian Free Optimizer

03:32.160 --> 03:38.080
in 2010. And he trained a 10 layer neural network end to end without pre-training

03:39.600 --> 03:45.120
from scratch. And when that happened, I thought this is it. Because if you can train a big neural

03:45.120 --> 03:50.160
network, a big neural network can represent very complicated function. Because if you have a neural

03:50.160 --> 03:58.240
network with 10 layers, it's as though you allow the human brain to run for some number of milliseconds,

03:58.320 --> 04:04.640
neuron firings are slow. And so in maybe 100 milliseconds, your neurons only fire 10 times.

04:04.640 --> 04:09.440
So it's also kind of like 10 layers. And in 100 milliseconds, you can perfectly recognize any

04:09.440 --> 04:14.800
object. So I thought, so I already had the idea then that we need to train a very big neural network

04:16.080 --> 04:20.560
on lots of supervised data. And then it must succeed, because we can find the best neural

04:20.560 --> 04:24.800
network. And then there's also theory that if you have more data than parameters, you won't

04:24.800 --> 04:28.880
overfit. Today, we know that actually, this theory is very incomplete, and you won't overfit even

04:28.880 --> 04:32.640
you have less data than parameters. But definitely, if you have more data than parameters, you won't

04:32.640 --> 04:38.640
overfit. So the fact that neural networks were heavily overparameterized, wasn't discouraging to

04:38.640 --> 04:43.920
you. So you were thinking about the theory that the number of parameters, the fact there's a huge

04:43.920 --> 04:47.360
number of parameters is okay, it's going to be okay. I mean, there was some evidence before that

04:47.360 --> 04:51.680
it was okay, but the theory was most the theory was that if you had a big data set and a big

04:51.680 --> 04:56.640
neural net, it was going to work. The overparameterization just didn't really figure much as a

04:56.640 --> 04:59.440
problem. I thought, well, with images, you just go and add some data augmentation, and it's going

04:59.440 --> 05:04.560
to be okay. So where was any doubt coming from? The main doubt was can we train a bigger, really

05:04.560 --> 05:08.160
have enough compute to train a big enough neural net with back propagation, back propagation,

05:08.160 --> 05:12.320
I thought was would work. This image wasn't clear would was whether there would be enough compute

05:12.320 --> 05:16.720
to get a very convincing result. And then at some point, Alex Krzewski wrote these insanely fast

05:16.800 --> 05:21.120
CUDA kernels for training convolutional neural nets. Net was bam, let's do this. Let's get

05:21.120 --> 05:26.160
image net and it's going to be the greatest thing. Was your intuition, most of your intuition from

05:26.160 --> 05:32.560
empirical results by you and by others? So like just actually demonstrating that a piece of program

05:32.560 --> 05:39.200
can train a 10 layer neural network? Or was there some pen and paper or marker and white board

05:39.200 --> 05:45.360
thinking intuition? Because you just connected a 10 layer large neural network to the brain.

05:45.360 --> 05:49.760
So you just mentioned the brain. So in your intuition about neural networks, does the human

05:49.760 --> 05:56.080
brain come into play as a intuition builder? Definitely. I mean, you know, you got to be

05:56.080 --> 06:00.480
precise with these analogies between neural artificial neural networks in the brain. But

06:01.120 --> 06:06.480
there's no question that the brain is a huge source of intuition and inspiration for deep

06:06.480 --> 06:12.320
learning researchers since all the way from Rosenblatt in the 60s. Like, if you look at the

06:12.320 --> 06:16.640
whole idea of a neural network is directly inspired by the brain. You had people like

06:16.640 --> 06:22.800
McCallum and Pitts who were saying, hey, you got these neurons in the brain. And hey, we recently

06:22.800 --> 06:26.480
learned about the computer and automata. Can we use some ideas from the computer and automata to

06:26.480 --> 06:31.920
design some kind of computational object that's going to be simple, computational and kind of

06:31.920 --> 06:36.320
like the brain and invented the neuron. So they were inspired by it back then. Then you had the

06:36.320 --> 06:40.960
convolutional neural network from Fukushima. And then later young Lacan, who said, Hey, if you

06:40.960 --> 06:45.280
limit the receptive fields of a neural network, it's going to be especially suitable for images,

06:45.280 --> 06:51.040
as it turned out to be true. So there was a very small number of examples where analogies to the

06:51.040 --> 06:56.240
brain were successful. And I thought, well, probably an artificial neuron is not that different from

06:56.240 --> 07:01.600
the brain if it's cleaned hard enough. So let's just assume it is and roll with it. So we're now

07:01.600 --> 07:08.480
at a time where deep learning is very successful. So let us squint less and say, let's open our

07:08.560 --> 07:14.080
eyes and say, what do you use an interesting difference between the human brain? Now, I know

07:14.080 --> 07:19.600
you're probably not an expert, neither in your scientists and your biologists, but loosely speaking,

07:19.600 --> 07:23.120
what's the difference between the human brain and artificial neural networks? That's interesting

07:23.120 --> 07:28.720
to you for the next decade or two. That's a good question to ask. What is an interesting difference

07:28.720 --> 07:34.000
between the neural between the brain and our artificial neural networks? So I feel like today,

07:34.880 --> 07:40.000
artificial neural networks, so we all agree that there are certain dimensions in which the human

07:40.000 --> 07:45.280
brain vastly outperforms our models. But I also think that there are some ways in which artificial

07:45.280 --> 07:51.440
neural networks have a number of very important advantages over the brain. Looking at the advantages

07:51.440 --> 07:56.720
versus disadvantages is a good way to figure out what is the important difference. So the brain

07:57.360 --> 08:01.760
uses spikes, which may or may not be important. Yes, that's a really interesting question. Do you

08:01.760 --> 08:07.200
think it's important or not? That's one big architectural difference between artificial

08:07.200 --> 08:13.600
neural networks. It's hard to tell, but my prior is not very high. And I can say why. There are

08:13.600 --> 08:17.920
people who are interested in spiking neural networks. And basically, what they figured out is

08:17.920 --> 08:23.280
that they need to simulate the non-spiking neural networks in spikes. And that's how they're going

08:23.280 --> 08:27.200
to make them work. If you don't simulate the non-spiking neural networks in spikes, it's not

08:27.200 --> 08:30.560
going to work because the question is, why should it work? And that connects to questions around

08:30.560 --> 08:36.800
back propagation and questions around deep learning. You've got this giant neural network.

08:36.800 --> 08:39.840
Why should it work at all? Why should that learning rule work at all?

08:43.120 --> 08:47.200
It's not a self-evident question, especially if you, let's say, if you were just starting in the

08:47.200 --> 08:52.640
field and you read the very early papers, you can say, hey, people are saying, let's build neural

08:52.640 --> 08:56.640
networks. That's a great idea because the brain is a neural network, so it would be useful to

08:56.640 --> 09:01.360
build neural networks. Now, let's figure out how to train them. It should be possible to train

09:01.360 --> 09:09.200
them probably, but how? And so the big idea is the cost function. That's the big idea. The cost

09:09.200 --> 09:14.720
function is a way of measuring the performance of the system according to some measure.

09:14.720 --> 09:21.280
By the way, that is a big, actually, let me think. Is that one, a difficult idea to arrive at? And

09:21.280 --> 09:29.040
how big of an idea is that? That there's a single cost function? Sorry, let me take a pause. Is

09:29.040 --> 09:35.760
supervised learning a difficult concept to come to? I don't know. All concepts are very easy in

09:35.760 --> 09:40.720
retrospect. Yeah, that's what it seems trivial now. Because the reason I asked that, and we'll

09:40.720 --> 09:47.040
talk about it, is there other things? Is there things that don't necessarily have a cost function,

09:47.120 --> 09:51.680
maybe have many cost functions, or maybe have dynamic cost functions, or maybe

09:51.680 --> 09:55.840
a totally different kind of architectures? Because we have to think like that in order to

09:55.840 --> 10:01.040
arrive at something new, right? So the good examples of things which don't have clear cost

10:01.040 --> 10:07.200
functions are GANs. And again, you have a game. So instead of thinking of a cost function,

10:08.080 --> 10:12.000
where you want to optimize, where you know that you have an algorithm gradient descent,

10:12.000 --> 10:16.240
which will optimize the cost function. And then you can reason about the behavior of your system

10:16.240 --> 10:21.680
in terms of what it optimizes. With GAN, you say, I have a game, and I'll reason about the behavior

10:21.680 --> 10:25.600
of the system in terms of the equilibrium of the game. But it's all about coming up with these

10:25.600 --> 10:30.640
mathematical objects that help us reason about the behavior of our system. Right, that's really

10:30.640 --> 10:35.600
interesting. Yeah, so GAN is the only one. It's kind of a, the cost function is emergent from the

10:35.600 --> 10:40.160
comparison. I don't know if it has a cost function. I don't know if it's meaningful to talk about the

10:40.160 --> 10:44.240
cost function of a GAN. It's kind of like the cost function of biological evolution or the cost

10:44.240 --> 10:51.920
function of the economy. It's, you can talk about regions to which it will go towards, but I don't

10:51.920 --> 10:59.440
think, I don't think the cost function analogy is the most useful. So evolution doesn't, that's

10:59.440 --> 11:04.240
really interesting. So if evolution doesn't really have a cost function, like a cost function based

11:04.240 --> 11:11.920
on it's something akin to our mathematical conception of a cost function, then do you think

11:11.920 --> 11:17.440
cost functions in deep learning are holding us back? Yeah, I, so you just kind of mentioned

11:17.440 --> 11:23.440
that cost function is a, is a nice first profound idea. Do you think that's a good idea? Do you

11:23.440 --> 11:30.240
think it's an idea will go past? So self play starts to touch on that a little bit in reinforcement

11:30.240 --> 11:35.840
learning systems. That's right. Self play and also ideas around exploration where you're trying to

11:35.840 --> 11:40.800
take action. That's, that's surprise a predictor. I'm a big fan of cost functions. I think cost

11:40.800 --> 11:44.560
functions are great and they serve us really well. And I think that whenever we can do things with

11:44.560 --> 11:50.240
cost functions, we should. And you know, maybe there is a chance that we will come up with some

11:50.240 --> 11:55.440
yet another profound way of looking at things that will involve cost functions in a less central way.

11:55.440 --> 11:57.280
But I don't know, I think cost functions are, I mean,

11:59.840 --> 12:04.640
I would not bet against against cost functions. Is there other things about the brain

12:05.360 --> 12:10.240
that pop into your mind that might be different and interesting for us to consider

12:10.960 --> 12:14.880
in designing artificial neural networks? So we talked about spiking a little bit.

12:16.080 --> 12:19.280
I mean, one, one thing which may potentially be useful, I think people,

12:19.280 --> 12:22.320
neuroscientists figured out something about the learning rule of the brain or

12:22.880 --> 12:26.480
talking about spike time independent plasticity. And it would be nice if some people were to

12:26.480 --> 12:31.200
study that in simulation. Wait, sorry, spike time independent plasticity. Yeah, that's

12:31.680 --> 12:37.360
STD. It's a particular learning rule that uses spike timing to figure out how to determine how

12:37.360 --> 12:43.280
to update the synapses. So it's kind of like, if a synapse fires into the neuron before the neuron

12:43.280 --> 12:48.320
fires, then it's strengthened the synapse. And if the synapse fires into the neurons shortly

12:48.320 --> 12:52.960
after the neuron fired, then it becomes the synapse something along this line. I'm 90%

12:52.960 --> 12:58.400
sure it's right. So if I said something wrong here, don't don't get too angry.

12:59.280 --> 13:03.440
But you sounded brilliant while saying it. But the timing, that's one thing that's missing.

13:04.160 --> 13:10.000
The temporal dynamics is not captured. I think that's like a fundamental property of the brain

13:10.000 --> 13:14.480
is the timing of the signals. Well, you're recording your networks.

13:15.360 --> 13:21.920
But you think of that as, I mean, that's a very crude simplified, what's that called?

13:22.640 --> 13:29.600
There's a clock, I guess, to recurrent neural networks. This seems like the brain is the

13:29.600 --> 13:35.440
general, the continuous version of that, the generalization where all possible timings are

13:35.440 --> 13:41.200
possible. And then within those timings, this contains some information. You think recurrent

13:41.200 --> 13:47.520
neural networks, the recurrence in recurrent neural networks can capture the same kind of

13:47.600 --> 13:55.360
phenomena as the timing that seems to be important for the brain in the firing of neurons in the

13:55.360 --> 14:01.920
brain? I mean, I think recurrent neural networks are amazing and they can do,

14:02.560 --> 14:08.000
I think they can do anything we'd want them to, we'd want a system to do. Right now,

14:08.000 --> 14:12.000
recurrent neural networks have been superseded by transformers, but maybe one day they'll make

14:12.000 --> 14:18.240
a comeback, maybe they'll be back, we'll see. Let me in a small tangent say, do you think they'll

14:18.240 --> 14:24.240
be back? So so much of the breakthroughs recently that we'll talk about on natural language processing

14:24.240 --> 14:31.120
and language modeling has been with transformers that don't emphasize recurrence. Do you think

14:31.120 --> 14:35.920
recurrence will make a comeback? Well, some kind of recurrence, I think very likely.

14:36.800 --> 14:42.960
Recurrent neural networks, as they're typically thought of for processing sequences, I think it's

14:42.960 --> 14:49.120
also possible. What is, to you, a recurrent neural network? In general speaking, I guess,

14:49.120 --> 14:53.520
what is a recurrent neural network? You have a neural network which maintains a high dimensional

14:53.520 --> 14:59.200
hidden state. And then when an observation arrives, it updates its high dimensional hidden state

14:59.760 --> 15:06.960
through its connections in some way. So do you think, you know, that's what like expert systems

15:06.960 --> 15:17.040
did, right? Symbolic AI, the knowledge based, growing a knowledge base is maintaining a hidden state,

15:17.040 --> 15:21.040
which is its knowledge base and is growing it by sequential processing. Do you think of it more

15:21.120 --> 15:29.920
generally in that way? Or is it simply, is it the more constrained form of a hidden state with

15:29.920 --> 15:34.960
certain kind of gating units that we think of as today with LSTMs and that? I mean, the hidden

15:34.960 --> 15:39.280
state is technically what you described there, the hidden state that goes inside the LSTM or

15:39.280 --> 15:43.600
there are an N or something like this. But then what should be contained, you know, if you want to

15:43.600 --> 15:50.000
make the expert system and analogy, I'm not, I mean, you could say that the knowledge is stored

15:50.080 --> 15:55.120
in the connections and then the short term processing is done in the hidden state.

15:56.240 --> 16:02.800
Yes. Could you say that? So sort of, do you think there's a future of building large scale

16:03.440 --> 16:06.080
knowledge bases within the neural networks? Definitely.

16:08.960 --> 16:14.160
So we're going to pause on that confidence because I want to explore that. Well, let me zoom back out

16:14.160 --> 16:21.360
and ask back to the history of ImageNet. Neural networks have been around for many decades as

16:21.360 --> 16:27.280
you mentioned. What do you think were the key ideas that led to their success, that ImageNet moment

16:27.280 --> 16:34.240
and beyond the success in the past 10 years? Okay. So the question is to make sure I didn't

16:34.240 --> 16:39.280
miss anything, the key ideas that led to the success of deep learning over the past 10 years.

16:39.360 --> 16:44.800
Exactly. Even though the fundamental thing behind deep learning has been around for much longer.

16:45.360 --> 16:55.600
So the key idea about deep learning, or rather the key fact about deep learning before deep

16:55.600 --> 17:02.480
learning started to be successful is that it was underestimated. People who worked in machine

17:02.480 --> 17:08.080
learning simply didn't think that neural networks could do much. People didn't believe that large

17:08.080 --> 17:13.840
neural networks could be trained. People thought that, well, there was lots of, there was a lot of

17:13.840 --> 17:18.720
debate going on in machine learning about what are the right methods and so on. And people were

17:18.720 --> 17:23.760
arguing because there were no, there were no, there was no way to get hard facts. And by that,

17:23.760 --> 17:28.480
I mean, there were no benchmarks which were truly hard, that if you do really well on them, then

17:28.480 --> 17:36.640
you can say, look, here's my system. That's when you switch from, that's when this field becomes

17:36.640 --> 17:40.480
a little bit more of an engineering field. So in terms of deep learning to answer the question

17:40.480 --> 17:47.040
directly, the ideas were all there. The thing that was missing was a lot of supervised data and a lot

17:47.040 --> 17:53.200
of compute. Once you have a lot of supervised data and a lot of compute, then there is a third

17:53.200 --> 17:57.680
thing which is needed as well. And that is conviction, conviction that if you take

17:58.320 --> 18:03.360
the right stuff, which already exists, and apply and mixed with a lot of data and a lot of compute,

18:03.440 --> 18:09.440
that it will in fact work. And so that was the missing piece. It was you had the, you needed

18:09.440 --> 18:15.040
the data, you needed the compute which showed up in terms of GPUs, and you needed the conviction

18:15.040 --> 18:21.920
to realize that you need to mix them together. So that's really interesting. So I guess the

18:21.920 --> 18:28.880
presence of compute and the presence supervised data allowed the empirical evidence to do the

18:28.960 --> 18:33.120
convincing of the majority of the computer science community. So I guess there's a

18:33.120 --> 18:42.720
key moment with Jitendra Malik and Alex, Alyosha Efros, who were very skeptical, right? And then

18:42.720 --> 18:48.160
there's a Jeffrey Hinton that was the opposite of skeptical. And there was a convincing moment.

18:48.160 --> 18:53.840
And I think ImageNet served as that moment. And that represented this kind of, or the big

18:53.840 --> 19:00.480
pillars of computer vision community, kind of the wizards got together. And then all of a sudden

19:00.480 --> 19:06.240
there was a shift. And it's not enough for the ideas to all be there and the computer to be there.

19:06.240 --> 19:13.040
It's for it to convince the cynicism that existed that that's interesting that people just didn't

19:13.040 --> 19:20.560
believe for a couple of decades. Yeah, well, but it's more than that. It's kind of when put this

19:20.560 --> 19:25.440
way, it sounds like, well, you know, those silly people who didn't believe what were they missing.

19:25.440 --> 19:30.080
But in reality, things were confusing because neural networks really did not work on anything.

19:30.080 --> 19:35.520
And they were not the best method on pretty much anything as well. And it was pretty rational to

19:35.520 --> 19:41.680
say, yeah, this stuff doesn't have any traction. And that's why you need to have these very hard

19:41.680 --> 19:47.280
tasks which are which produce undeniable evidence. And that's how we make progress. And that's why

19:47.280 --> 19:51.600
the field is making progress today, because we have these hard benchmarks, which represent true

19:51.600 --> 19:59.840
progress. And so, and this is why we were able to avoid endless debate. So incredibly, you've

19:59.840 --> 20:05.920
contributed some of the biggest recent ideas in AI in computer vision, language, natural

20:05.920 --> 20:12.080
language processing, reinforcement learning, sort of everything in between, maybe not GANs.

20:12.800 --> 20:17.840
Is there, there may not be a topic you haven't touched. And of course, the fundamental science

20:17.840 --> 20:25.600
of deep learning. What is the difference to you between vision, language, and as in reinforcement

20:25.600 --> 20:30.400
learning action, as learning problems? And what are the commonalities? Do you see them as all

20:30.400 --> 20:36.320
interconnected? Are they fundamentally different domains that require different approaches?

20:36.880 --> 20:42.640
Okay, that's a good question. Machine learning is a field with a lot of unity, a huge amount of

20:42.640 --> 20:49.680
unity. In fact, what do you mean by unity, like overlap of ideas? overlap of ideas overlap of

20:49.680 --> 20:54.320
principles. In fact, there's only one or two or three principles, which are very, very simple.

20:54.320 --> 20:59.840
And then they apply in almost the same way, in almost the same way to the different modalities

20:59.840 --> 21:04.720
through the different problems. And that's why today, when someone writes a paper on improving

21:04.720 --> 21:09.200
optimization of deep learning and vision, it improves the different NLP applications,

21:09.200 --> 21:13.200
and it improves the different reinforcement learning applications. Reinforcement learning.

21:13.200 --> 21:20.000
So I would say that computer vision and NLP are very similar to each other. Today, they differ in

21:20.000 --> 21:24.640
that they have slightly different architectures. We use transformers in NLP, and we use convolutional

21:24.640 --> 21:29.760
neural networks in vision. But it's also possible that one day this will change and everything

21:29.760 --> 21:33.840
will be unified with a single architecture. Because if you go back a few years ago in

21:33.840 --> 21:39.760
natural language processing, there were a huge, huge number of architectures for every

21:39.760 --> 21:46.320
different tiny problem had its own architecture. Today, there's just one transformer for all

21:46.320 --> 21:50.560
those different tasks. And if you go back in time even more, you had even more and more

21:50.560 --> 21:56.320
fragmentation and every little problem in AI had its own little subspecialization and sub,

21:56.320 --> 22:00.800
you know, little set of collection of skills, people who would know how to engineer the features.

22:00.880 --> 22:05.040
Now it's all been subsumed by deep learning. We have this unification. And so I expect

22:05.680 --> 22:09.520
vision to become unified with natural language as well. Or rather, I shouldn't say expect,

22:09.520 --> 22:13.360
I think it's possible. I don't want to be too sure, because I think on the commercial

22:13.360 --> 22:17.280
neural network, it is very computationally efficient. Arell is different. Arell doesn't

22:17.280 --> 22:21.360
require slightly different techniques, because you really do need to take action. You really

22:21.360 --> 22:26.400
need to do something about exploration, your variance is much higher. But I think there

22:26.400 --> 22:30.240
is a lot of unity even there. And I would expect, for example, that at some point, there will be

22:30.240 --> 22:36.080
some broader unification between Arell and supervised learning, where somehow the Arell

22:36.080 --> 22:40.640
will be making decisions to make the supervised learning go better. And it will be, I imagine

22:40.640 --> 22:45.120
one big black box and you just throw every, you know, you shovel, shovel things into it. And it

22:45.120 --> 22:49.840
just figures out what to do with whatever you shovel in it. I mean, reinforcement learning has

22:49.840 --> 22:57.600
some aspects of language and vision combined, almost, there's elements of a long term memory

22:57.600 --> 23:02.240
that you should be utilizing, and there's elements of a really rich sensory space.

23:02.960 --> 23:09.040
So it seems like the, it's like the union of the two or something like that. I'd say something

23:09.040 --> 23:14.720
slightly different. I'd say that reinforcement learning is neither, but it naturally interfaces

23:14.720 --> 23:19.600
and integrates with the two of them. Do you think action is fundamentally different? So yeah,

23:19.600 --> 23:26.800
what is interesting about, what is unique about policy of learning to act? Well, so one example,

23:26.800 --> 23:32.480
for instance, is that when you learn to act, you are fundamentally in a non stationary world.

23:33.120 --> 23:40.480
Because as your actions change, the things you see start changing. You, you experience the world

23:40.480 --> 23:45.040
in a different way. And this is not the case for the more traditional static problem where you have

23:45.040 --> 23:50.320
a some distribution and you just apply a model to that distribution. You think it's a fundamentally

23:50.320 --> 23:55.520
different problem or is it just a more difficult general, it's a generalization of the problem

23:55.520 --> 24:00.160
of understanding. I mean, it's, it's, it's a question of definitions almost. There is a huge

24:00.160 --> 24:04.800
amount of commonality for sure. You take gradients, you try, you take gradients, we try to approximate

24:04.800 --> 24:08.560
gradients in both cases. In some case, in the case of reinforcement learning, you have

24:08.560 --> 24:13.760
some tools to reduce the variance of the gradients. You do that. There's lots of commonality,

24:13.760 --> 24:18.320
use the same neural net in both cases. You compute the gradient, you apply atom in both cases.

24:18.400 --> 24:27.520
So, I mean, there's lots in common for sure, but there are some small differences which are not

24:27.520 --> 24:31.360
completely insignificant. It's really just a matter of your, of your point of view, what

24:31.360 --> 24:37.120
frame of reference you, what, how much do you want to zoom in or out as you look at these problems?

24:37.120 --> 24:42.720
Which problem do you think is harder? So people like no Chomsky believe that language is fundamental

24:42.800 --> 24:49.200
to everything. So it underlies everything. Do you think language understanding is harder than

24:49.200 --> 24:54.640
visual scene understanding or vice versa? I think that asking if a problem is hard is

24:54.640 --> 24:58.480
slightly wrong. I think the question is a little bit wrong, and I want to explain why.

24:59.440 --> 25:02.160
So what does it mean for a problem to be hard?

25:04.240 --> 25:10.560
Okay, the non-interesting, dumb answer to that is there's a, there's a benchmark,

25:10.560 --> 25:16.720
and there's a human level performance on that benchmark. And how is the effort required to

25:16.720 --> 25:22.000
reach the human level benchmark? So from the perspective of how much until we get to human

25:22.000 --> 25:28.720
level on a very good benchmark? Yeah, like some, I understand what you mean by that.

25:28.720 --> 25:33.040
So what I was going to say that a lot of it depends on, you know, once you solve a problem,

25:33.040 --> 25:37.680
it stops being hard. And that's, that's always true. And so, but if something is hard or not,

25:37.680 --> 25:42.800
depends on what our tools can do today. So you know, you say today, through human level,

25:43.600 --> 25:49.040
language understanding and visual perception are hard in the sense that there is no way of

25:49.040 --> 25:53.040
solving the problem completely in the next three months. Right. So I agree with that statement.

25:53.840 --> 25:57.440
Beyond that, I'm just, I'll be my, my guess would be as good as yours. I don't know.

25:57.440 --> 26:02.720
Oh, okay. So you don't have a fundamental intuition about how hard language understanding is.

26:02.720 --> 26:07.040
I think I, I know I changed my mind. I'd say language is probably going to be harder. I mean,

26:07.040 --> 26:12.880
it depends on how you define it. Like if you mean absolute top notch 100% language understanding,

26:12.880 --> 26:19.200
I'll go with language. So, but then if I show you a piece of paper with letters on it, is that

26:19.920 --> 26:24.960
you see what I mean? So you have a vision system, you say it's the best human level vision system.

26:24.960 --> 26:30.560
I show you, I open a book and I show you letters. Will it understand how these letters form into

26:30.560 --> 26:34.560
word and sentences and meaning is this part of the vision problem? Where does vision end and

26:34.560 --> 26:39.280
language begin? Yeah. So Chomsky would say it starts at language. So vision is just a little

26:39.280 --> 26:46.880
example of the kind of structure and, you know, fundamental hierarchy of ideas that's already

26:46.880 --> 26:55.360
represented in our brain. Somehow that's represented through language. But where does vision stop and

26:55.360 --> 27:09.280
language begin? That's a really interesting question. So one possibility is that it's

27:09.280 --> 27:16.240
impossible to achieve really deep understanding in either images or language without basically

27:16.240 --> 27:22.000
using the same kind of system. So you're going to get the other for free. I think, I think it's

27:22.000 --> 27:26.560
pretty likely that yes, if we can get one, our machine learning is probably that good that we

27:26.560 --> 27:34.080
can get the other. But it's not 100. I'm not 100% sure. And also, I think a lot of it really does

27:34.080 --> 27:42.400
depend on your definitions. Definitions of like perfect vision. Because reading is vision, but

27:42.400 --> 27:49.520
should it count? Yeah, to me, my definition is if a system looked at an image and then a system

27:50.480 --> 27:57.040
looked at a piece of text, and then told me something about that. And I was really impressed.

27:58.240 --> 28:02.080
That's relative. You'll be impressed for half an hour. And then you're going to say, well,

28:02.080 --> 28:06.160
I mean, all the systems do that. But here's the thing they don't do. Yeah, but I don't have that

28:06.160 --> 28:13.200
with humans. Humans continue to impress me. Is that true? Well, the ones, okay, so I'm a fan of

28:13.200 --> 28:18.960
monogamy. So I like the idea of marrying somebody being with them for several decades. So I believe

28:18.960 --> 28:25.040
in the fact that, yes, it's possible to have somebody continuously giving you pleasurable,

28:25.760 --> 28:31.120
interesting, witty, new ideas, friends. Yeah, I think so. They continue to surprise you.

28:32.000 --> 28:44.480
The surprise, it's that injection of randomness seems to be a nice source of continued

28:45.120 --> 28:54.800
inspiration, like the wit, the humor. I think, yeah, that would be, it's a very subjective test,

28:54.800 --> 29:00.640
but I think if you have enough humans in the room. Yeah, I understand what you mean. Yeah,

29:00.640 --> 29:03.840
I feel like I misunderstood what you meant by impressing you. I thought you meant to

29:03.840 --> 29:10.800
impress you with its intelligence, with how valid understands an image. I thought you meant

29:10.800 --> 29:13.840
something like, I'm going to show you a really complicated image and it's going to get it right,

29:13.920 --> 29:19.440
and you're going to say, wow, that's really cool, the systems of January 2020 have not been doing

29:19.440 --> 29:26.000
that. Yeah, I think it all boils down to the reason people click like on stuff on the internet,

29:26.000 --> 29:34.320
which is like it makes them laugh. So it's like humor or wit or insight. I'm sure we'll get that

29:34.320 --> 29:41.920
as well. So forgive the romanticized question, but looking back to you, what is the most beautiful

29:41.920 --> 29:47.600
or surprising idea in deep learning or AI in general you've come across? So I think the most

29:47.600 --> 29:52.560
beautiful thing about deep learning is that it actually works. And I mean it because you got

29:52.560 --> 29:56.240
these ideas, you got a little neural network, you got the back propagation algorithm.

29:58.800 --> 30:02.480
And then you got some theories as to, you know, this is kind of like the brain. So maybe if you

30:02.480 --> 30:06.320
make it large, if you make the neural network large and you train a lot of data, then it will

30:07.840 --> 30:11.280
do the same function that the brain does. And it turns out to be true. That's crazy.

30:12.400 --> 30:15.920
And now we just train these neural networks and you make them larger and they keep getting better.

30:16.560 --> 30:21.280
And I find it unbelievable. I find it unbelievable that this whole AI stuff with neural networks

30:21.280 --> 30:27.840
works. Have you built up an intuition of why are there a little bits and pieces of intuitions

30:27.840 --> 30:33.760
of insights of why this whole thing works? I mean, some definitely. Well, we know that

30:33.840 --> 30:40.800
optimization, we now have good, you know, we've had lots of empirical, you know,

30:40.800 --> 30:44.960
huge amounts of empirical reasons to believe that optimization should work on all most

30:44.960 --> 30:50.720
problems we care about. Do you have insights of what, so you just said empirical evidence.

30:50.720 --> 31:00.320
Is most of your sort of empirical evidence kind of convinces you, it's like evolution is empirical,

31:00.320 --> 31:04.480
it shows you that look, this evolutionary process seems to be a good way to design

31:05.760 --> 31:11.760
organisms that survive in their environment. But it doesn't really get you to the insights of how

31:12.640 --> 31:17.520
the whole thing works. I think it's a good analogy is physics. You know how you say, hey,

31:17.520 --> 31:21.600
let's do some physics calculation and come up with some new physics theory and make some prediction.

31:21.600 --> 31:25.920
But then you got around the experiment. You know, you got around the experiment, it's important.

31:25.920 --> 31:30.080
So it's a bit the same here, except that maybe sometimes the experiment came before

31:30.080 --> 31:34.480
the theory. But it still is the case, you know, you have some data and you come up with some

31:34.480 --> 31:37.840
prediction, you say, yeah, let's make a big neural network, let's train it, and it's going to work

31:38.400 --> 31:41.840
much better than anything before it. And it will in fact continue to get better as you make it

31:41.840 --> 31:46.880
larger. And it turns out to be true. That's, that's amazing when a theory is validated like this,

31:46.880 --> 31:50.560
you know, it's not a mathematical theory, it's more of a biological theory almost.

31:51.600 --> 31:55.840
So I think there are not terrible analogies between deep learning and biology. I would say

31:55.840 --> 31:59.360
it's like the geometric mean of biology and physics, that's deep learning.

32:00.080 --> 32:05.360
The geometric mean of biology and physics. I think I'm going to need a few hours to wrap

32:05.360 --> 32:15.360
my head around that. Because just to find the geometric, just to find the set of what biology

32:15.360 --> 32:20.240
represents. Well, biology, in biology, things are really complicated. The theories are really,

32:20.240 --> 32:24.320
really, it's really hard to have good predictive theory. And if in physics, the theories are too

32:24.320 --> 32:28.560
good. In theory, in physics, people make these super precise theories, which make these amazing

32:28.560 --> 32:33.440
predictions. And in machine learning, we're kind of in between. Kind of in between. But it'd be

32:33.440 --> 32:38.400
nice if machine learning somehow helped us discover the unification of the two as opposed to server

32:38.400 --> 32:45.680
the in between. But you're right, that's, you're kind of trying to juggle both. So do you think

32:45.680 --> 32:49.440
there are still beautiful and mysterious properties in your networks that are yet to be

32:49.440 --> 32:54.000
discovered? Definitely. I think that we are still massively underestimating deep learning.

32:55.280 --> 33:01.360
What do you think it will look like? Like what if I knew I would have done it? So,

33:02.560 --> 33:06.240
but if you look at all the progress from the past 10 years, I would say most of it,

33:06.960 --> 33:12.080
I would say there have been a few cases where some were things that felt like really new ideas

33:12.080 --> 33:17.120
showed up. But by and large, it was every year, we thought, okay, deep learning goes this far.

33:17.120 --> 33:21.680
Nope, it actually goes further. And then the next year, okay, now you know, this is this is

33:21.680 --> 33:25.440
big deep learning, we are really done. Nope, it goes further, it just keeps going further each

33:25.440 --> 33:30.240
year. So that means that we keep underestimating, we keep not understanding it as surprising properties

33:30.240 --> 33:35.120
all the time. You think it's getting harder and harder to make progress, need to make progress?

33:35.840 --> 33:39.840
It depends on what we mean. I think the field will continue to make very robust progress

33:39.840 --> 33:44.320
for quite a while. I think for individual researchers, especially people who are doing

33:45.040 --> 33:49.120
research, it can be harder because there is a very large number of researchers right now.

33:50.000 --> 33:54.080
I think that if you have a lot of compute, then you can make a lot of very interesting

33:54.080 --> 34:00.160
discoveries, but then you have to deal with the challenge of managing a huge computer,

34:00.160 --> 34:03.200
a huge class, a huge computer cluster to run your experiments. It's a little bit harder.

34:03.200 --> 34:07.760
So I'm asking all these questions that nobody knows the answer to, but you're one of the smartest

34:07.760 --> 34:12.800
people I know, so I'm going to keep asking. So let's imagine all the breakthroughs that happen

34:12.800 --> 34:18.000
in the next 30 years in deep learning. Do you think most of those breakthroughs can be done by

34:18.000 --> 34:28.320
one person with one computer? In the space of breakthroughs, do you think compute and

34:28.320 --> 34:35.760
large efforts will be necessary? I mean, I can't be sure. When you say one computer, you mean how

34:35.760 --> 34:45.840
large? You're clever. I mean, one GPU. I see. I think it's pretty unlikely.

34:46.800 --> 34:53.040
I think it's pretty unlikely. I think that the stack of deep learning is starting to be quite

34:53.040 --> 35:01.440
deep. If you look at it, you've got all the way from the ideas, the systems to build the datasets,

35:02.000 --> 35:07.360
the distributed programming, the building the actual cluster, the GPU programming,

35:08.080 --> 35:13.120
putting it all together. So the stack is getting really deep, and I think it can be quite hard

35:13.120 --> 35:18.320
for a single person to become to be world-class in every single layer of the stack. What about

35:19.920 --> 35:25.840
Vladimir Vapnik really insists on is taking MNIST and trying to learn from very few examples,

35:25.840 --> 35:32.000
so being able to learn more efficiently. Do you think there'll be breakthroughs in that space

35:32.000 --> 35:38.320
that may not need a huge compute? I think there will be a large number of breakthroughs in general

35:38.320 --> 35:42.960
that will not need a huge amount of compute. So maybe I should clarify that. I think that

35:42.960 --> 35:48.240
some breakthroughs will require a lot of compute, and I think building systems which actually do

35:48.240 --> 35:53.440
things will require a huge amount of compute. That one is pretty obvious. If you want to do X,

35:53.440 --> 35:58.240
and X requires a huge neural net, you've got to get a huge neural net, but I think there will be

35:58.240 --> 36:03.760
lots of, I think there is lots of room for very important work being done by small groups and

36:03.760 --> 36:10.480
individuals. Can you maybe sort of on the topic of the science of deep learning, talk about one

36:10.480 --> 36:17.200
of the recent papers that you've released, the Deep Double Descent, where bigger models and more

36:17.200 --> 36:22.720
data hurt. I think it's a really interesting paper. Can you describe the main idea? Yeah,

36:22.720 --> 36:28.400
definitely. So what happened is that some over the years, some small number of researchers

36:28.400 --> 36:32.080
noticed that it is kind of weird that when you make the neural net work larger, it works better,

36:32.080 --> 36:35.760
and it seems to go in contradiction with statistical ideas. And then some people made

36:35.760 --> 36:40.080
an analysis showing that actually you got this double descent bump. And what we've done was to

36:40.080 --> 36:45.600
show that double descent occurs for pretty much all practical deep learning systems.

36:46.320 --> 36:54.480
And that it'll be also, so can you step back? What's the X axis and the Y axis of a double

36:54.480 --> 37:03.280
descent plot? Okay, great. So you can look, you can do things like you can take your neural

37:03.280 --> 37:08.720
network, and you can start increasing its size slowly, while keeping your data set fixed.

37:10.000 --> 37:16.800
So if you increase the size of the neural network slowly, and if you don't do early stopping,

37:16.800 --> 37:23.360
that's a pretty important detail. Then when the neural network is really small, you make it larger,

37:23.440 --> 37:27.520
you get a very rapid increase in performance. Then you continue to make it larger. And at some

37:27.520 --> 37:34.000
point performance, you'll get worse. And it gets and it gets the worst exactly at the point at

37:34.000 --> 37:39.280
which it achieves zero training error, precisely zero training loss. And then as you make it

37:39.280 --> 37:43.600
larger, it starts to get better again. And it's kind of counterintuitive because you'd expect

37:43.600 --> 37:51.280
deep learning phenomena to be monotonic. And it's hard to be sure what it means, but it also occurs

37:51.360 --> 37:55.360
in the case of linear classifiers. And the intuition basically boils down to the following.

37:56.960 --> 38:04.000
When you, when you have a lot, when you have a large data set, and a small model, then small,

38:04.000 --> 38:11.440
tiny random. So basically, what is overfitting? Overfitting is when your model is somehow very

38:11.440 --> 38:17.200
sensitive to the small random, unimportant stuff in your data set in the training data in the

38:17.200 --> 38:22.400
training data set precisely. So if you have a small model, and you have a big data set,

38:23.280 --> 38:27.360
and there may be some random thing, you know, some training cases are randomly in the data set,

38:27.360 --> 38:31.680
and others may not be there. But the small model, but the small model is kind of insensitive to

38:31.680 --> 38:36.960
this randomness, because it's the same, you there is pretty much no uncertainty about the model

38:36.960 --> 38:42.400
when the data set is large. So okay, so at the very basic level, to me, it is the most surprising

38:42.400 --> 38:53.280
thing that neural networks don't overfit every time very quickly. Before ever being able to learn

38:53.280 --> 38:59.280
anything, the huge number of parameters. So here is so there is one way Okay, so maybe so let me try

38:59.280 --> 39:03.520
to give the explanation and maybe that will be that will work. So you got a huge neural network,

39:03.520 --> 39:08.640
let's suppose you got a your you have a huge neural network, you have a huge number of parameters.

39:09.600 --> 39:13.840
And now let's pretend everything is linear, which is not let's just pretend. Then there is this big

39:13.840 --> 39:21.040
subspace, where a neural network achieves zero error. And SGT is going to find approximately

39:21.040 --> 39:25.120
that's right, approximately the point with the smallest norm in that subspace.

39:27.040 --> 39:33.600
And that can also be proven to be insensitive to the small randomness in the data, when the

39:33.600 --> 39:38.480
dimensionality is high. But when the dimensionality of the data is equal to the dimensionality of

39:38.480 --> 39:44.400
the model, then there is a one to one correspondence between all the data sets and the models. So

39:44.400 --> 39:47.600
small changes in the data set actually lead to large changes in the model and that's why

39:47.600 --> 39:52.880
performance gets worse. So this is the best explanation more or less. So then it would be

39:52.880 --> 39:58.960
good for the model to have more parameters sort of to be bigger than the data. That's right,

39:58.960 --> 40:02.720
but only if you don't early stop. If you introduce early stop in your regularization,

40:02.720 --> 40:07.280
you can make a double as a descent pump almost completely disappear. What is early stop early

40:07.280 --> 40:12.480
stopping is when you train your model, and you monitor your validation performance.

40:13.440 --> 40:16.560
And then if at some point validation performance starts to get worse, you say, okay, let's stop

40:16.560 --> 40:23.040
training. We are good. We are good. We are good enough. So the magic happens after that moment.

40:23.040 --> 40:26.560
So you don't want to do the early stopping. Well, if you don't do the early stopping,

40:26.560 --> 40:31.920
you get this very, you get the very pronounced double descent. Do you have any intuition why

40:31.920 --> 40:36.960
this happens? Double descent or sorry, are you stopping? No, the double descent. So the

40:36.960 --> 40:43.680
well, yeah, so I try it. Let's see the intuition is basically is this that when the data set has

40:43.680 --> 40:49.680
as many degrees of freedom as the model, then there is a one to one correspondence between them.

40:49.680 --> 40:55.520
And so small changes to the data set lead to noticeable changes in the model. So your model

40:55.520 --> 41:01.360
is very sensitive to all the randomness. It is unable to discard it. Whereas it turns out that

41:01.360 --> 41:07.120
when you have a lot more data than parameters, or a lot more parameters than data, the resulting

41:07.120 --> 41:14.160
solution will be insensitive to small changes in the data set. So it's able to nicely put discard

41:14.160 --> 41:19.840
the small changes, the randomness. Exactly. The spurious correlations which you don't want.

41:20.480 --> 41:24.480
Jeff Hinton suggested we need to throw back propagation. We already kind of talked about

41:24.480 --> 41:28.880
this a little bit, but he suggested we need to throw away back propagation and start over.

41:29.680 --> 41:31.920
I mean, of course, some of that is a little bit

41:33.760 --> 41:38.640
wit and humor. But what do you think? What could be an alternative method of training neural

41:38.640 --> 41:43.520
networks? Well, the thing that he said precisely is that to the extent that you can't find back

41:43.520 --> 41:48.960
propagation in the brain, it's worth seeing if we can learn something from how the brain learns.

41:48.960 --> 41:53.440
But back propagation is very useful and we should keep using it. Oh, you're saying that

41:53.440 --> 41:58.000
once we discover the mechanism of learning in the brain or any aspects of that mechanism,

41:58.000 --> 42:02.160
we should also try to implement that in your networks. If it turns out that you can't find

42:02.160 --> 42:05.920
back propagation in the brain. If we can't find back propagation in the brain.

42:08.000 --> 42:14.960
Well, so I guess your answer to that is back propagation is pretty damn useful. So why are

42:14.960 --> 42:18.960
we complaining? I mean, I personally am a big fan of back propagation. I think it's a great

42:18.960 --> 42:25.280
algorithm because it solves an extremely fundamental problem which is finding a neural

42:25.280 --> 42:31.200
circuit subject to some constraints. I don't see that problem going away. So that's why I

42:32.560 --> 42:36.480
really, I think it's pretty unlikely that we'll have anything which is going to be

42:37.280 --> 42:41.200
dramatically different. It could happen. But I wouldn't bet on it right now.

42:43.200 --> 42:51.040
So let me ask a sort of big picture question. Do you think neural networks can be made to reason?

42:51.600 --> 42:56.560
Why not? Well, if you look, for example, at AlphaGo or AlphaZero,

42:58.160 --> 43:04.320
the neural network of AlphaZero plays Go, which we all agree is a game that requires reasoning,

43:05.120 --> 43:11.040
better than 99.9% of all humans. Just the neural network without the search, just the neural network

43:11.040 --> 43:16.320
itself. Doesn't that give us an existence proof that neural networks can reason?

43:16.560 --> 43:21.920
To push back and disagree a little bit, we all agree that Go is reasoning.

43:23.200 --> 43:28.640
I think I agree. I don't think it's a trivial. So obviously, reasoning like intelligence is

43:29.920 --> 43:36.640
a loose gray area term a little bit. Maybe you disagree with that. But yes, I think it has some

43:36.640 --> 43:44.560
of the same elements of reasoning. Reasoning is almost akin to search. There's a sequential element

43:45.200 --> 43:54.160
of stepwise consideration of possibilities and sort of building on top of those possibilities

43:54.160 --> 43:59.760
in a sequential manner until you arrive at some insight. So yeah, I guess playing Go is kind of

43:59.760 --> 44:04.640
like that. And when you have a single neural network doing that without search, that's kind of like

44:04.640 --> 44:10.080
that. So there's an existence proof in a particular constrained environment that a process akin to

44:10.880 --> 44:18.720
what many people call reasoning exists. But more general kind of reasoning. So off the board.

44:18.720 --> 44:22.720
There is one other existence proof. Oh boy, which one? Us humans?

44:22.720 --> 44:32.240
Yes. Okay. All right. So do you think the architecture that will allow neural

44:32.240 --> 44:37.920
networks to reason will look similar to the neural network architectures we have today?

44:38.720 --> 44:43.360
I think it will. I think, well, I don't want to make two overly definitive statements.

44:43.920 --> 44:49.440
I think it's definitely possible that the neural networks that will produce the reasoning breakthroughs

44:49.440 --> 44:54.480
of the future will be very similar to the architecture that exists today, maybe a little

44:54.480 --> 45:02.800
bit more recurrent, maybe a little bit deeper. But these neural nets are so insanely powerful.

45:02.880 --> 45:08.400
Why wouldn't they be able to learn to reason? Humans can reason. So why can't neural networks?

45:09.200 --> 45:14.480
So do you think the kind of stuff we've seen neural networks do is a kind of just weak reasoning?

45:14.480 --> 45:18.400
So it's not a fundamentally different process. Again, this is stuff we don't nobody knows the

45:18.400 --> 45:24.240
answer to. So when it comes to our neural networks, I would think which I would say

45:24.240 --> 45:30.720
is that neural networks are capable of reasoning. But if you train a neural network on a task which

45:30.720 --> 45:35.520
doesn't require reasoning, it's not going to reason. This is a well-known effect where the

45:35.520 --> 45:43.040
neural network will solve exactly the problem that you pose in front of it in the easiest way possible.

45:44.400 --> 45:53.120
Right. That takes us to one of the brilliant ways you describe neural networks, which is

45:54.080 --> 45:57.040
you've referred to neural networks as the search for small circuits

45:57.920 --> 46:03.040
and maybe general intelligence as the search for small programs,

46:04.400 --> 46:08.640
which I found is a metaphor very compelling. Can you elaborate on that difference?

46:09.200 --> 46:17.520
Yeah. So the thing which I said precisely was that if you can find the shortest program that

46:17.520 --> 46:23.520
outputs the data in your disposal, then you will be able to use it to make the best prediction

46:23.520 --> 46:29.680
possible. And that's a theoretical statement which can be proved mathematically. Now,

46:29.680 --> 46:34.800
you can also prove mathematically that it is that finding the shortest program which generates

46:34.800 --> 46:41.600
some data is not a computable operation. No finite amount of compute can do this.

46:42.560 --> 46:48.720
So then with neural networks, neural networks are the next best thing that actually works in

46:48.720 --> 46:54.560
practice. We are not able to find the best, the shortest program which generates our data,

46:55.600 --> 47:00.880
but we are able to find a small, but now that statement should be amended,

47:01.440 --> 47:04.480
even a large circuit which fits our data in some way.

47:05.120 --> 47:09.840
Well, I think what you meant by the small circuit is the smallest needed circuit.

47:09.920 --> 47:14.640
Well, the thing which I would change now, back then I really haven't fully internalized

47:14.640 --> 47:19.600
the overparameterized results. The things we know about overparameterized neural

47:19.600 --> 47:26.640
nets, now I would phrase it as a large circuit whose weights contain a small amount of information,

47:27.600 --> 47:31.680
which I think is what's going on. If you imagine the training process of a neural network as you

47:31.680 --> 47:39.680
slowly transmit entropy from the data set to the parameters, then somehow the amount of information

47:39.680 --> 47:44.560
in the weights ends up being not very large, which would explain whether generalized so well.

47:45.200 --> 47:51.840
So that's the large circuit might be one that's helpful for the generalization.

47:51.840 --> 47:52.640
Yeah, something like this.

47:54.720 --> 48:02.320
But do you see it important to be able to try to learn something like programs?

48:02.320 --> 48:08.960
I mean, if we can, definitely. I think the answer is kind of yes, if we can do it.

48:08.960 --> 48:13.760
We should do things that we can do it. It's the reason we are pushing on deep learning.

48:15.200 --> 48:20.000
The fundamental reason, the root cause is that we are able to train them.

48:21.360 --> 48:26.720
So in other words, training comes first. We've got our pillar, which is the training pillar.

48:27.440 --> 48:31.040
And now we are trying to contort our neural networks around the training pillar. We got

48:31.040 --> 48:36.800
to stay trainable. This is an invariant we cannot violate. And so

48:37.040 --> 48:41.520
being trainable means starting from scratch, knowing nothing, you can actually pretty quickly

48:41.520 --> 48:47.520
converge towards knowing a lot or even slowly. But it means that given the resources at your

48:47.520 --> 48:54.240
disposal, you can train the neural net and get it to achieve useful performance.

48:54.240 --> 48:57.920
Yeah, that's a pillar we can't move away from. That's right. Because if you can, whereas if you

48:57.920 --> 49:03.040
say, Hey, let's find the shortest program, we can't do that. So it doesn't matter how useful

49:04.000 --> 49:07.600
that would be. We can do it. So we want.

49:07.600 --> 49:11.520
So do you think you kind of mentioned that the neural networks are good at finding small

49:11.520 --> 49:18.560
circuits or large circuits? Do you think then the matter of finding small programs is just the data?

49:18.560 --> 49:19.040
No.

49:19.040 --> 49:28.240
So the, sorry, not the size or the quality, the type of data sort of ask giving it programs.

49:28.880 --> 49:34.880
Well, I think the thing is that right now, finding there are no good precedents of people

49:34.880 --> 49:41.920
successfully finding programs really well. And so the way you'd find programs is you'd train a

49:41.920 --> 49:47.440
deep neural network to do it basically. But which is the right way to go about it.

49:48.000 --> 49:54.000
But there's not good illustrations that hasn't been done yet. But in principle, it should be possible.

49:54.080 --> 50:01.200
Can you elaborate a little bit? What's your insight in principle? And put another way,

50:01.200 --> 50:07.520
you don't see why it's not possible. Well, it's kind of like more, it's more a statement of,

50:09.360 --> 50:13.440
I think that it's, I think that it's unwise to bet against deep learning. And

50:14.960 --> 50:18.880
if it's a fun, if it's a cognitive function that humans seem to be able to do, then

50:19.200 --> 50:24.320
it doesn't take too long for some deep neural net to pop up that can do it too.

50:25.680 --> 50:32.880
Yeah, I'm there with you. I can, I've stopped betting against neural networks at this point

50:32.880 --> 50:38.320
because they continue to surprise us. What about long term memory? Can neural networks have long

50:38.320 --> 50:45.360
term memory or something like knowledge basis? So being able to aggregate important information

50:45.440 --> 50:53.200
over long periods of time, that would then serve as useful sort of representations of

50:54.000 --> 51:00.480
state that you can make decisions by so have a long term context based on what you make in the

51:00.480 --> 51:07.600
decision. So in some sense, the parameters already do that. The parameters are an aggregation of the

51:07.600 --> 51:12.080
day of the neural of the entirety of the neural net experience. And so they count as the long

51:12.560 --> 51:19.040
form, long term knowledge. And people have trained various neural nets to act as knowledge basis

51:19.040 --> 51:23.200
and, you know, investigated with invest, people have investigated language models as knowledge

51:23.200 --> 51:29.760
basis. So there is work, there is work there. Yeah, but in some sense, do you think in every sense,

51:29.760 --> 51:37.200
do you think there's a, it's all just a matter of coming up with a better mechanism of forgetting

51:37.200 --> 51:42.400
the useless stuff and remembering the useful stuff? Because right now, I mean, there's not been

51:42.400 --> 51:47.920
mechanisms that do remember really long term information. What do you mean by that precisely?

51:48.880 --> 51:52.000
Precisely. I like the word precisely. So

51:54.720 --> 51:59.840
I'm thinking of the kind of compression of information the knowledge basis represent.

52:00.400 --> 52:07.120
Sort of creating a, now I apologize for my sort of human centric thinking about

52:07.680 --> 52:13.440
what knowledge is because neural networks aren't interpretable necessarily with the kind of

52:13.440 --> 52:19.680
knowledge they have discovered. But a good example for me is knowledge basis being able to build up

52:19.680 --> 52:25.840
over time something like the knowledge that Wikipedia represents. It's a really compressed,

52:25.840 --> 52:33.600
structured knowledge base. Obviously, not the actual Wikipedia or the language,

52:34.160 --> 52:39.200
but like a semantic web, the dream that semantic web represented. So it's a really nice compressed

52:39.200 --> 52:46.320
knowledge base or something akin to that in the noninterpretable sense as neural networks would

52:46.320 --> 52:50.080
have. Well, the neural networks would be noninterpretable if you look at their rates, but

52:50.080 --> 52:53.680
their outputs should be very interpretable. Okay, so yeah, how do you, how do you make

52:54.480 --> 52:57.360
very smart neural networks like language models interpretable?

52:58.000 --> 53:02.000
Well, you ask them to generate some text and the text will generally be interpretable.

53:02.000 --> 53:07.840
Do you find that the epitome of interpretability, like can you do better? Like can you, because

53:07.840 --> 53:13.440
you can't, okay, I'd like to know what does it know and what doesn't know. I would like the neural

53:13.440 --> 53:19.200
network to come up with examples where it's completely dumb and examples where it's completely

53:19.200 --> 53:24.960
brilliant. And the only way I know how to do that now is to generate a lot of examples and use my

53:24.960 --> 53:31.040
human judgment. But it would be nice if the neural network had some self-awareness about

53:31.600 --> 53:36.880
100%. I'm a big believer in self-awareness. And I think that I think, I think

53:38.720 --> 53:43.600
neural net self-awareness will allow for things like the capabilities, like the ones you describe,

53:43.600 --> 53:48.240
like for them to know what they know and what they don't know. And for them to know where to

53:48.240 --> 53:52.160
invest, to increase their skills most optimally. And to your question of interpretability,

53:52.160 --> 53:56.320
there are actually two answers to that question. One answer is, you know, we have the neural net,

53:56.320 --> 54:00.560
so we can analyze the neurons and we can try to understand what the different neurons and

54:00.560 --> 54:04.960
different layers mean. And you can actually do that. And OpenAI has done some work on that.

54:05.760 --> 54:11.360
But there is a different answer, which is that I would say that's the human centric answer where

54:11.360 --> 54:18.080
you say, you know, you look at a human being, you can't read. How do you know what a human

54:18.080 --> 54:20.800
being is thinking? You ask them, you say, Hey, what do you think about this? What do you think

54:20.800 --> 54:26.320
about that? And you get some answers. The answers you get are sticky. In the sense, you already

54:26.320 --> 54:33.360
have a mental model. You already have a mental model of that human being. You already have an

54:33.360 --> 54:40.240
understanding of like a big conception of what it of that human being, how they think, what they know,

54:40.240 --> 54:46.880
how they see the world, and then everything you ask, you're adding on to that. And that stickiness

54:47.520 --> 54:53.600
seems to be, that's one of the really interesting qualities of the human being is that information

54:53.600 --> 54:59.520
is sticky. You don't, you seem to remember the useful stuff, aggregate it well, and forget most

54:59.520 --> 55:05.040
of the information that's not useful, that process. But that's also pretty similar to the

55:05.040 --> 55:09.600
process that neural networks do is just that neural networks are much crappier at this time.

55:10.480 --> 55:15.600
It doesn't seem to be fundamentally that different. But just stick on reasoning for a little longer.

55:17.120 --> 55:24.560
He said, why not? Why can't I reason? What's a good impressive feat benchmark to you of reasoning

55:27.360 --> 55:31.600
that you'll be impressed by if neural networks were able to do? Is that something you already have

55:31.600 --> 55:38.640
in mind? Well, I think writing, writing really good code. I think proving really hard theorems,

55:39.360 --> 55:42.800
solving open ended problems without of the box solutions.

55:45.840 --> 55:51.600
And sort of theorem type mathematical problems. Yeah, I think those ones are a very natural

55:51.600 --> 55:55.680
example as well. You know, if you can prove an unproven theorem, then it's hard to argue

55:55.680 --> 56:00.880
don't reason. And so by the way, and this comes back to the point about the hard results, you know,

56:00.880 --> 56:06.000
if you got a hard, if you have machine learning, deep learning as a field is very fortunate,

56:06.000 --> 56:12.160
because we have the ability to sometimes produce these unambiguous results. And when they happen,

56:12.160 --> 56:17.040
the debate changes, the conversation changes, it's a converse, you have the ability to produce

56:17.040 --> 56:22.080
conversation changing results conversation. And then of course, just like you said, people kind

56:22.080 --> 56:26.240
of take that for granted, say that wasn't actually a hard problem. Well, I mean, at some point,

56:26.240 --> 56:32.880
we'll probably run out of hard problems. Yeah, that whole mortality thing is kind of kind of a

56:32.880 --> 56:38.320
sticky problem that we haven't quite figured out. Maybe we'll solve that one. I think one of the

56:38.320 --> 56:42.880
fascinating things in your entire body of work, but also the work at OpenAI recently,

56:42.880 --> 56:48.480
one of the conversation changers has been in the world of language models. Can you briefly kind of

56:49.280 --> 56:53.760
try to describe the recent history of using neural networks in the domain of language and text?

56:54.480 --> 57:00.160
Well, there's been lots of history. I think I think the Elman network was was a small,

57:00.240 --> 57:05.200
tiny recurrent neural network applied to language back in the 80s. So the history is really,

57:06.400 --> 57:13.280
you know, fairly long, at least. And the thing that started the thing that changed the trajectory

57:13.280 --> 57:17.920
of neural networks and language is the thing that changed the trajectory of all deep learning and

57:17.920 --> 57:23.680
that's data and compute. So suddenly you move from small language models, which learn a little bit.

57:24.240 --> 57:28.960
And with language models, in particular, you can, there's a very clear explanation for why

57:29.040 --> 57:33.200
they need to be large to be good, because they're trying to predict the next word.

57:34.480 --> 57:41.440
So when you don't know anything, you'll notice very, very broad strokes, surface level patterns,

57:41.440 --> 57:46.400
like sometimes there are characters and there is space between those characters,

57:46.400 --> 57:50.560
you'll notice this pattern. And you'll notice that sometimes there is a comma and then the

57:50.560 --> 57:54.880
next character is a capital letter, you'll notice that pattern. Eventually, you may start to notice

57:54.880 --> 57:59.280
that there are certain words occur often, you may notice that spellings are a thing,

57:59.280 --> 58:04.960
you may notice syntax. And when you get really good at all these, you start to notice the semantics,

58:05.760 --> 58:10.480
you start to notice the facts. But for that to happen, the language model needs to be larger.

58:11.360 --> 58:16.320
So that's, let's linger on that, because that's where you and Noam Chomsky disagree.

58:16.320 --> 58:25.600
So you think we're actually taking incremental steps, a sort of larger network, larger compute

58:25.600 --> 58:34.640
will be able to get to the semantics, be able to understand language without what Noam likes to

58:34.640 --> 58:42.000
sort of think of as a fundamental understandings of the structure of language, like imposing

58:42.080 --> 58:47.920
your theory of language onto the learning mechanism. So you're saying the learning,

58:47.920 --> 58:52.400
you can learn from raw data, the mechanism that underlies language?

58:53.360 --> 58:58.480
Well, I think it's pretty likely. But I also want to say that I don't really

59:00.400 --> 59:07.280
know precisely what Chomsky means when he talks about him. You said something about imposing

59:07.280 --> 59:12.800
your structure on language. I'm not 100% sure what he means. But empirically, it seems that when

59:12.800 --> 59:16.560
you inspect those larger language models, they exhibit signs of understanding the semantics,

59:16.560 --> 59:20.400
whereas the smaller language models do not. We've seen that a few years ago when we

59:20.400 --> 59:26.000
did work on the sentiment neuron, we trained a small, you know, smallish LSTM to predict the

59:26.000 --> 59:31.920
next character in Amazon reviews. And we noticed that when you increase the size of the LSTM from

59:31.920 --> 59:37.760
500 LSTM cells to 4,000 LSTM cells, then one of the neurons starts to represent the sentiment

59:38.480 --> 59:44.720
of the article, of sorry, of their view. Now, why is that sentiment is a pretty semantic

59:44.720 --> 59:48.640
attribute? It's not a syntactic attribute. And for people who might not know, I don't know if

59:48.640 --> 59:52.400
that's a standard term, but sentiment is whether it's a positive or negative review. That's right.

59:52.400 --> 59:57.680
Like, is the person happy with something or is the person unhappy with something? And so here we had

59:57.680 --> 01:00:02.800
very clear evidence that a small neural net does not capture sentiment while a large neural net

01:00:02.800 --> 01:00:08.720
does. And why is that? Well, our theory is that at some point, you run out of syntax to models,

01:00:08.720 --> 01:00:15.760
you start to gotta focus on something else. And with size, you quickly run out of syntax to model,

01:00:15.760 --> 01:00:20.560
and then you really start to focus on the semantics is would be the idea. That's right. And so I don't

01:00:20.560 --> 01:00:24.720
want to imply that our models have complete semantic understanding, because that's not true.

01:00:25.280 --> 01:00:30.720
But they definitely are showing signs of semantic understanding, partial semantic understanding,

01:00:30.720 --> 01:00:38.000
but the smaller models do not show that those signs. Can you take a step back and say, what is GPT2,

01:00:38.000 --> 01:00:43.040
which is one of the big language models that was the conversation changer in the past couple of

01:00:43.040 --> 01:00:51.360
years? Yes. So GPT2 is a transformer with one and a half billion parameters that was trained on

01:00:51.440 --> 01:01:00.080
up on about 40 billion tokens of text, which were obtained from web pages that were linked to

01:01:00.080 --> 01:01:04.720
from Reddit articles with more than three uploads. And what's the transformer? The transformer,

01:01:04.720 --> 01:01:08.960
it's the most important advance in neural network architectures in recent history.

01:01:09.680 --> 01:01:14.080
What is attention maybe to because I think that's an interesting idea, not necessarily sort of

01:01:14.080 --> 01:01:20.480
technically speaking, but the idea of attention versus maybe what recurrent neural networks

01:01:20.480 --> 01:01:25.760
represent. Yeah. So the thing is, the transformer is a combination of multiple ideas simultaneously

01:01:25.760 --> 01:01:31.760
of which attention is one. Do you think attention is the key? No, it's a key, but it's not the key.

01:01:32.320 --> 01:01:37.680
The transformer is successful because it is the simultaneous combination of multiple ideas. And

01:01:37.680 --> 01:01:43.040
if you were to remove either idea, it would be much less successful. So the transformer uses a

01:01:43.040 --> 01:01:47.280
lot of attention, but attention exists for a few years. So that can't be the main innovation.

01:01:48.240 --> 01:01:54.880
The transformer is designed in such a way that it runs really fast on the GPU.

01:01:56.000 --> 01:02:00.240
And that makes a huge amount of difference. This is one thing. The second thing is that

01:02:00.240 --> 01:02:06.320
transformer is not recurrent. And that is really important too, because it is more shallow and

01:02:06.320 --> 01:02:12.400
therefore much easier to optimize. So in other words, uses attention. It is, it is a really great

01:02:12.480 --> 01:02:17.760
fit to the GPU. And it is not recurrent. So therefore, less deep and easier to optimize.

01:02:17.760 --> 01:02:22.720
And the combination of those factors make it successful. So now it makes it makes great

01:02:22.720 --> 01:02:27.440
use of your GPU. It allows you to achieve better results for the same amount of compute.

01:02:28.560 --> 01:02:33.520
And that's why it's successful. Were you surprised how well transformers worked?

01:02:34.160 --> 01:02:41.280
And GPT2 worked? So you worked on language. You've had a lot of great ideas before transformers came

01:02:41.280 --> 01:02:46.320
about in language. So you got to see the whole set of revolutions before and after. Were you

01:02:46.320 --> 01:02:52.240
surprised? Yeah, a little. A little? Yeah. I mean, it's hard. It's hard to remember because

01:02:52.800 --> 01:02:57.680
you adapt really quickly. But it definitely was surprising. It definitely was. In fact, I'll,

01:02:57.680 --> 01:03:03.360
you know what, I'll, I'll retract my statement. It was, it was pretty amazing. It was just amazing

01:03:03.360 --> 01:03:08.160
to see, generate this text of this. And you know, you got to keep in mind that we've seen,

01:03:08.160 --> 01:03:12.480
at that time, we've seen all this progress in GANs, in improving the, you know, the samples

01:03:12.480 --> 01:03:17.120
produced by GANs were just amazing. You have these realistic faces, but text hasn't really moved that

01:03:17.120 --> 01:03:24.080
much. And suddenly we moved from, you know, whatever GANs were in 2015, to the best, most

01:03:24.080 --> 01:03:29.120
amazing GANs in one step. And I was really stunning. Even though theory predicted, yeah,

01:03:29.120 --> 01:03:32.800
you train a big language model, of course, you should get this, but then to see it with your own

01:03:32.800 --> 01:03:39.840
eyes. It's something else. And yet we adapt really quickly. And now there's a sort of

01:03:41.520 --> 01:03:49.200
some cognitive scientist, right? Articles saying that GPT2 models don't truly understand language.

01:03:49.200 --> 01:03:54.880
So we adapt quickly to how amazing the fact that they're able to model the language so well is.

01:03:55.600 --> 01:04:01.440
So what do you think is the bar? For what? For impressing us that it.

01:04:02.320 --> 01:04:06.080
I don't know. Do you think that bar will continuously be moved?

01:04:06.080 --> 01:04:12.160
Definitely. I think when you start to see really dramatic economic impact, that's when I think

01:04:12.160 --> 01:04:16.800
that's in some sense the next barrier. Because right now, if you think about the work in AI,

01:04:16.800 --> 01:04:21.680
it's really confusing. It's really hard to know what to make of all these advances.

01:04:22.400 --> 01:04:27.440
It's kind of like, okay, you got an advance and now you can do more things and you got another

01:04:28.160 --> 01:04:35.360
improvement and you got another cool demo. At some point, I think people who are outside of AI,

01:04:36.160 --> 01:04:38.640
they can no longer distinguish this progress anymore.

01:04:38.640 --> 01:04:42.160
So we were talking offline about translating Russian to English and

01:04:42.160 --> 01:04:46.400
how there's a lot of brilliant work in Russian that the rest of the world doesn't know about.

01:04:46.400 --> 01:04:52.080
That's true for Chinese. That's true for a lot of scientists and just artistic work in general.

01:04:52.080 --> 01:04:56.960
Do you think translation is the place where we're going to see sort of economic big impact?

01:04:57.280 --> 01:05:02.240
I don't know. I think there is a huge number of applications. First of all, I want to point out

01:05:02.240 --> 01:05:07.600
that translation already today is huge. I think billions of people interact with

01:05:08.800 --> 01:05:12.960
big chunks of the internet primarily through translation. So translation is already huge

01:05:12.960 --> 01:05:19.760
and it's hugely positive too. I think self-driving is going to be hugely impactful.

01:05:20.080 --> 01:05:26.880
And it's unknown exactly when it happens, but again, I would not bet against deep learning.

01:05:27.840 --> 01:05:31.200
So there's deep learning in general, but you think deep learning for self-driving?

01:05:31.760 --> 01:05:35.040
Yes, deep learning for self-driving. But I was talking about sort of language models.

01:05:36.880 --> 01:05:38.000
Be your duff a little bit.

01:05:38.000 --> 01:05:41.040
Just to check. You're not seeing a connection between driving and language.

01:05:41.040 --> 01:05:44.080
No, no. Okay. I'd rather both use neural nets.

01:05:44.080 --> 01:05:47.600
That would be a poetic connection. I think there might be some, like you said,

01:05:47.600 --> 01:05:54.320
there might be some kind of unification towards a kind of multitask transformers

01:05:54.320 --> 01:06:00.720
that can take on both language and vision tasks. That'd be an interesting unification.

01:06:01.440 --> 01:06:03.600
Let's see. What can I ask about GPT two more?

01:06:04.880 --> 01:06:09.840
It's simple. It's not much to ask. You take a transform, you make it bigger,

01:06:09.840 --> 01:06:12.560
give it more data, and suddenly it does all those amazing things.

01:06:12.560 --> 01:06:16.800
Yeah. One of the beautiful things is that GPT, the transformers are fundamentally

01:06:16.800 --> 01:06:25.920
simple to explain, to train. Do you think bigger will continue to show better results in language?

01:06:26.960 --> 01:06:27.440
Probably.

01:06:28.240 --> 01:06:31.360
Sort of like what are the next steps with GPT two? Do you think?

01:06:31.360 --> 01:06:38.000
I mean, I think for sure seeing what larger versions can do is one direction. Also,

01:06:39.760 --> 01:06:42.640
I mean, there are many questions. There's one question which I'm curious about,

01:06:42.640 --> 01:06:46.480
and that's the following. Right now, GPT two, so we feed it all this data from the

01:06:46.480 --> 01:06:50.400
internet, which means that it needs to memorize all those random facts about everything in the

01:06:50.400 --> 01:06:59.680
internet. It would be nice if the model could somehow use its own intelligence to decide

01:06:59.680 --> 01:07:04.240
what data it wants to start, accept, and what data it wants to reject, just like people.

01:07:04.240 --> 01:07:09.760
People don't learn all data indiscriminately. We are super selective about what we learn,

01:07:09.760 --> 01:07:13.040
and I think this kind of active learning I think would be very nice to have.

01:07:14.160 --> 01:07:21.040
Yeah. Listen, I love active learning. So let me ask, does the selection of data,

01:07:21.040 --> 01:07:25.920
can you just elaborate that a little bit more? Do you think the selection of data is,

01:07:28.000 --> 01:07:33.680
like I have this kind of sense that the optimization of how you select data,

01:07:33.760 --> 01:07:39.600
so the active learning process is going to be a place for a lot of breakthroughs

01:07:40.720 --> 01:07:44.960
even in the near future, because there hasn't been many breakthroughs there that are public.

01:07:44.960 --> 01:07:49.200
I feel like there might be private breakthroughs that companies keep to themselves,

01:07:49.200 --> 01:07:52.880
because the fundamental problem has to be solved if you want to solve self-driving,

01:07:52.880 --> 01:07:57.760
if you want to solve a particular task. What do you think about the space in general?

01:07:57.760 --> 01:08:02.400
Yeah, so I think that for something like active learning, or in fact for any kind of capability,

01:08:02.400 --> 01:08:07.680
like active learning, the thing that it really needs is a problem. It needs a problem that requires it.

01:08:09.280 --> 01:08:12.880
It's very hard to do research about the capability if you don't have a task,

01:08:12.880 --> 01:08:16.640
because then what's going to happen is you will come up with an artificial task,

01:08:16.640 --> 01:08:19.520
get good results, but not really convince anyone.

01:08:20.560 --> 01:08:27.520
Right. We're now past the stage where getting a result on MNIST,

01:08:27.520 --> 01:08:30.720
some clever formulation of MNIST will convince people.

01:08:30.720 --> 01:08:35.280
That's right. In fact, you could quite easily come up with a simple active learning scheme on MNIST

01:08:35.280 --> 01:08:44.880
and get a 10x speed up, but then so what? I think that active learning will naturally arise

01:08:45.440 --> 01:08:51.600
as problems that require it to pop up. That's my take on it.

01:08:52.720 --> 01:08:58.800
There's another interesting thing that OpenAS brought up with GPT2, which is when you create a

01:08:58.800 --> 01:09:04.560
powerful artificial intelligence system, and it was unclear what kind of detrimental,

01:09:04.560 --> 01:09:11.360
once you release GPT2, what kind of detrimental effect it'll have. Because if you have a model

01:09:11.360 --> 01:09:18.320
that can generate pretty realistic text, you can start to imagine that it would be used by bots in

01:09:19.360 --> 01:09:24.400
some way that we can't even imagine. There's this nervousness about what it's possible to do.

01:09:24.960 --> 01:09:30.000
You did a really brave and I think profound thing, which just started a conversation about this.

01:09:30.000 --> 01:09:38.400
How do we release powerful artificial intelligence models to the public? If we do it all, how do we

01:09:38.400 --> 01:09:45.440
privately discuss with other even competitors about how we manage the use of the systems and

01:09:45.440 --> 01:09:50.560
so on? From this whole experience, you've released a report on it, but in general,

01:09:50.560 --> 01:09:56.480
are there any insights that you've gathered from just thinking about this, about how you release

01:09:56.480 --> 01:10:03.760
models like this? I think that my take on this is that the field of AI has been in a state of

01:10:03.760 --> 01:10:10.240
childhood, and now it's exiting that state and it's entering a state of maturity. What that means

01:10:10.240 --> 01:10:15.600
is that AI is very successful and also very impactful, and its impact is not only large,

01:10:15.680 --> 01:10:23.840
but it's also growing. For that reason, it seems wise to start thinking about the impact of our

01:10:23.840 --> 01:10:28.560
systems before releasing them maybe a little bit too soon rather than a little bit too late.

01:10:29.760 --> 01:10:34.240
With the case of GPT-2, like I mentioned earlier, the results really were stunning,

01:10:35.040 --> 01:10:41.440
and it seemed plausible. It didn't seem certain. It seemed plausible that something like GPT-2

01:10:41.440 --> 01:10:48.800
could easily use to reduce the cost of disinformation. There was a question of what's

01:10:48.800 --> 01:10:52.880
the best way to release it, and a staged release seemed logical. A small model was released,

01:10:53.680 --> 01:10:59.840
and there was time to see the... Many people use these models in lots of cool ways. There've been

01:10:59.840 --> 01:11:06.080
lots of really cool applications. There haven't been any negative applications we know of,

01:11:06.080 --> 01:11:09.840
and so eventually it was released, but also other people replicated similar models.

01:11:09.840 --> 01:11:17.680
That's an interesting question though that we know of. In your view, staged release is at least

01:11:17.680 --> 01:11:25.360
part of the answer to the question of how do we... What do we do once we create a system like this?

01:11:25.920 --> 01:11:31.840
It's part of the answer, yes. Is there any other insights? Say you don't want to release the model

01:11:31.840 --> 01:11:37.440
at all because it's useful to you for whatever the business is. Well, there are plenty of people

01:11:37.440 --> 01:11:43.920
who don't release models already. Right, of course, but is there some moral ethical responsibility

01:11:44.480 --> 01:11:51.840
when you have a very powerful model to communicate? Just as you said, when you had GPT-2, it was

01:11:51.840 --> 01:11:55.200
unclear how much it could be used for misinformation. It's an open question,

01:11:56.240 --> 01:12:00.880
and getting an answer to that might require that you talk to other really smart people that are

01:12:01.600 --> 01:12:08.720
outside of your particular group. Have you... Please tell me there's some optimistic pathway for

01:12:09.280 --> 01:12:12.320
people across the world to collaborate on these kinds of cases,

01:12:14.560 --> 01:12:19.440
or is it still really difficult from one company to talk to another company?

01:12:19.440 --> 01:12:26.720
So it's definitely possible. It's definitely possible to discuss these kind of models with

01:12:26.720 --> 01:12:31.280
colleagues elsewhere and to get their take on what to do.

01:12:32.080 --> 01:12:34.000
How hard is it though? I mean...

01:12:36.320 --> 01:12:41.920
Do you see that happening? I think that's a place where it's important to gradually build trust

01:12:41.920 --> 01:12:47.840
between companies, because ultimately, all the AI developers are building technology,

01:12:47.840 --> 01:12:51.440
which is going to be increasingly more powerful, and so it's...

01:12:52.160 --> 01:12:57.040
The way to think about it is that ultimately we're only together.

01:12:58.640 --> 01:13:06.400
Yeah, it's... I tend to believe in the better angels of our nature, but I do hope that

01:13:09.040 --> 01:13:12.720
when you build a really powerful AI system in a particular domain,

01:13:12.720 --> 01:13:18.160
that you also think about the potential negative consequences of... Yeah.

01:13:22.240 --> 01:13:27.360
It's an interesting and scary possibility that there will be a race for AI development that

01:13:27.360 --> 01:13:34.400
would push people to close that development and not share ideas with others. I don't love this.

01:13:34.400 --> 01:13:39.440
I've been in a pure academic for 10 years. I really like sharing ideas and it's fun, it's exciting.

01:13:41.440 --> 01:13:45.440
What do you think it takes to... Let's talk about AGI a little bit. What do you think it takes to

01:13:45.440 --> 01:13:50.080
build a system of human level intelligence? We talked about reasoning, we talked about

01:13:50.080 --> 01:13:55.760
long-term memory, but in general, what does it take, do you think? Well, I can't be sure,

01:13:57.360 --> 01:14:05.520
but I think the deep learning plus maybe another small idea. Do you think self-play will be involved?

01:14:05.520 --> 01:14:11.040
Sort of like you've spoken about the powerful mechanism of self-play where systems learn by

01:14:12.480 --> 01:14:19.360
sort of exploring the world in a competitive setting against other entities that are similarly

01:14:19.360 --> 01:14:24.480
skilled as them and so incrementally improving this way? Do you think self-play will be a component

01:14:24.480 --> 01:14:31.200
of building an AGI system? Yeah, so what I would say to build AGI, I think it's going to be

01:14:32.480 --> 01:14:39.680
deep learning plus some ideas and I think self-play will be one of those ideas. I think that that is a

01:14:39.680 --> 01:14:50.320
very... Self-play has this amazing property that it can surprise us in truly novel ways. For example,

01:14:52.080 --> 01:14:59.280
like we, I mean, pretty much every self-play system, both are Dota bot. I don't know if

01:14:59.280 --> 01:15:05.200
OpenAI had a release about multi-agents where you had two little agents who were playing hide and

01:15:06.080 --> 01:15:11.520
seek and of course also AlphaZero. They all produce surprising behaviors. They all produce

01:15:11.520 --> 01:15:17.280
behaviors that we didn't expect. They are creative solutions to problems and that seems like an

01:15:17.280 --> 01:15:23.840
important part of AGI that our systems don't exhibit routinely right now. And so that's why I

01:15:23.840 --> 01:15:27.520
like this area, I like this direction because of its ability to surprises.

01:15:27.520 --> 01:15:32.560
To surprises and an AGI system would surprise us fundamentally. Yes, but and to be precise,

01:15:33.280 --> 01:15:38.640
not just a random surprise, but to find a surprising solution to a problem that's also useful.

01:15:39.920 --> 01:15:46.960
Now, a lot of the self-play mechanisms have been used in the game context or at least in the

01:15:46.960 --> 01:15:57.040
simulation context. How far along the path to AGI do you think will be done in simulation? How much

01:15:57.040 --> 01:16:04.960
faith promise do you have in simulation versus having to have a system that operates in the real

01:16:04.960 --> 01:16:11.680
world, whether it's the real world of digital real-world data or real-world like actual physical

01:16:11.680 --> 01:16:17.360
world with robotics? I don't think it's in either war. I think simulation is a tool and it helps.

01:16:17.360 --> 01:16:23.840
It has certain strengths and certain weaknesses and we should use it. Yeah, but okay, I understand

01:16:24.400 --> 01:16:34.320
that that's true. But one of the criticisms of self-play, one of the criticisms of reinforcement

01:16:34.320 --> 01:16:42.800
learning is one of the its current power, its current results while amazing have been demonstrated

01:16:42.800 --> 01:16:47.200
in a simulated environments or very constrained physical environments. Do you think it's possible

01:16:47.200 --> 01:16:53.280
to escape them? Escape the simulated environments and be able to learn in non-simulated environments?

01:16:53.280 --> 01:17:01.040
Or do you think it's possible to also just simulate in a photorealistic and physics realistic way

01:17:01.040 --> 01:17:06.080
the real world in a way that we can solve real problems with self-play in simulation?

01:17:07.440 --> 01:17:12.480
I think that transfer from simulation to the real world is definitely possible and has been

01:17:12.480 --> 01:17:17.600
exhibited many times by many different groups. It's been especially successful in vision.

01:17:18.560 --> 01:17:24.240
Also OpenAI in the summer has demonstrated a robot hand which was trained entirely in simulation

01:17:25.120 --> 01:17:28.160
in a certain way that allowed for sim to real transfer to occur.

01:17:29.680 --> 01:17:34.480
Is this for the Rubik's Cube? Yes, that's right. I wasn't aware that was trained in simulation.

01:17:34.480 --> 01:17:40.000
It was trained in simulation entirely. Really? So it wasn't in the physical that the hand wasn't

01:17:40.000 --> 01:17:46.240
trained? No, 100% of the training was done in simulation and the policy that was learned in

01:17:46.240 --> 01:17:51.360
simulation was trained to be very adaptive. So adaptive that when you transfer it, it could very

01:17:51.360 --> 01:17:57.760
quickly adapt to the physical world. So the kind of perturbations with the giraffe or whatever the

01:17:57.760 --> 01:18:05.200
heck it was, were those part of the simulation? Well, the simulation was generally, so the simulation

01:18:05.200 --> 01:18:10.160
was trained to be robust to many different things, but not the kind of perturbations we've had in the

01:18:10.160 --> 01:18:16.960
video. So it's never been trained with a glove. It's never been trained with a stuffed giraffe.

01:18:16.960 --> 01:18:20.800
So in theory, these are novel perturbations? Correct. It's not in theory in practice.

01:18:21.680 --> 01:18:28.320
That those are novel perturbations? Well, that's okay. That's a clean, small-scale,

01:18:28.320 --> 01:18:32.000
but clean example of a transfer from the simulated world to the physical world.

01:18:32.000 --> 01:18:36.960
Yeah, and I will also say that I expect the transfer capabilities of deep learning to increase in

01:18:36.960 --> 01:18:42.000
general and the better the transfer capabilities are, the more useful simulation will become

01:18:43.520 --> 01:18:47.760
because then you could take, you could experience something in simulation

01:18:48.400 --> 01:18:52.720
and then learn a moral of the story which you could then carry with you to the real world, right?

01:18:53.360 --> 01:19:01.040
As humans do all the time and they play computer games. So let me ask sort of an embodied question,

01:19:01.760 --> 01:19:08.400
staying on AGI for a sec. Do you think AGI system, we need to have a body? We need to have some of

01:19:08.400 --> 01:19:16.560
those human elements of self-awareness, consciousness, sort of fear of mortality, sort of self-preservation

01:19:16.560 --> 01:19:22.320
in the physical space which comes with having a body? I think having a body will be useful.

01:19:22.320 --> 01:19:26.720
I don't think it's necessary, but I think it's very useful to have a body for sure because you can

01:19:27.200 --> 01:19:31.600
learn a whole new, you can learn things which cannot be learned without a body,

01:19:32.400 --> 01:19:36.000
but at the same time, I think that you can, if you don't have a body, you could

01:19:36.000 --> 01:19:40.880
compensate for it and still succeed. You think so? Yes. Well, there is evidence for this. For

01:19:40.880 --> 01:19:46.720
example, there are many people who were born deaf and blind and they were able to compensate for

01:19:46.720 --> 01:19:52.560
the lack of modalities. I'm thinking about Helen Keller specifically. So even if you're not able

01:19:52.560 --> 01:19:58.320
to physically interact with the world and if you're not able to, I actually was getting at,

01:19:59.520 --> 01:20:05.280
maybe let me ask on the more particular, I'm not sure if it's connected to having a body or not,

01:20:05.280 --> 01:20:11.200
but the idea of consciousness and a more constrained version of that is self-awareness.

01:20:11.200 --> 01:20:17.760
Do you think an AGI system should have consciousness? We can't define whatever the

01:20:17.760 --> 01:20:22.880
heck you think consciousness is. Yeah. Hard question to answer, given how hard it is to define it.

01:20:24.640 --> 01:20:28.240
Do you think it's useful to think about? I mean, it's definitely interesting,

01:20:28.240 --> 01:20:32.880
it's fascinating. I think it's definitely possible that our systems will be conscious.

01:20:33.760 --> 01:20:37.680
Do you think that's an emergent thing that just comes from, do you think consciousness could emerge

01:20:37.680 --> 01:20:42.400
from the representation that's stored within your networks? So like that it naturally just

01:20:42.400 --> 01:20:47.040
emerges when you become more and more, you're able to represent more and more over the world?

01:20:47.040 --> 01:20:52.720
Well, I'd say I'd make the following argument, which is humans are conscious,

01:20:53.680 --> 01:20:56.960
and if you believe that artificial neural nets are sufficiently

01:20:57.840 --> 01:21:03.200
similar to the brain, then there should at least exist artificial neural nets you should be conscious

01:21:03.200 --> 01:21:11.520
to. You're leaning on that existence proof pretty heavily. Okay. But that's the best answer I can

01:21:11.600 --> 01:21:19.120
give. No, I know. I know. There's still an open question if there's not some magic in the brain

01:21:19.120 --> 01:21:27.040
that we're not. I mean, I don't mean a non-materialistic magic, but that the brain might be a lot

01:21:27.040 --> 01:21:31.440
more complicated and interesting than we give it credit for. If that's the case, then it should show

01:21:31.440 --> 01:21:37.280
up and at some point, we will find out that we can't continue to make progress. But I think

01:21:37.280 --> 01:21:41.920
it's unlikely. So we talk about consciousness, but let me talk about another poorly defined

01:21:41.920 --> 01:21:48.240
concept of intelligence. Again, we've talked about reasoning. We've talked about memory. What do

01:21:48.240 --> 01:21:54.480
you think is a good test of intelligence for you? Are you impressed by the test that Alan Turing

01:21:54.480 --> 01:22:01.200
formulated with the imitation game with natural language? Is there something in your mind that

01:22:01.280 --> 01:22:09.920
you would be deeply impressed by if a system was able to do? I mean, lots of things. There's a

01:22:09.920 --> 01:22:16.800
certain frontier of capabilities today. And there exist things outside of that frontier,

01:22:16.800 --> 01:22:23.200
and I would be impressed by any such thing. For example, I would be impressed by a deep learning

01:22:23.200 --> 01:22:30.080
system which solves a very pedestrian task like machine translation or computer vision task or

01:22:30.080 --> 01:22:37.600
something which never makes a mistake a human wouldn't make under any circumstances. I think

01:22:37.600 --> 01:22:41.680
that is something which have not yet been demonstrated and I would find it very impressive.

01:22:42.640 --> 01:22:46.480
Yes, so right now, they make mistakes in different, they might be more accurate than human beings,

01:22:46.480 --> 01:22:53.360
but they still make a different set of mistakes. So I would guess that a lot of the skepticism

01:22:53.360 --> 01:22:57.280
that some people have about deep learning is when they look at their mistakes and they say,

01:22:57.280 --> 01:23:02.160
well, those mistakes, they make no sense. If you understood the concept, you wouldn't make that

01:23:02.160 --> 01:23:11.680
mistake. And I think that changing that would inspire me. That would be, yes, this is progress.

01:23:12.400 --> 01:23:18.640
Yeah, that's a really nice way to put it. But I also just don't like that human instinct to

01:23:19.680 --> 01:23:24.320
criticize a model is not intelligent. That's the same instinct as we do when we criticize

01:23:24.320 --> 01:23:34.240
any group of creatures as the other. Because it's very possible that GPT2 is much smarter

01:23:34.240 --> 01:23:39.280
than human beings at many things. That's definitely true. It has a lot more breadth of knowledge.

01:23:39.280 --> 01:23:44.640
Yes, breadth of knowledge and even perhaps depth on certain topics.

01:23:46.000 --> 01:23:50.240
It's kind of hard to judge what depth means, but there's definitely a sense in which

01:23:51.120 --> 01:23:53.920
humans don't make mistakes that these models do.

01:23:54.480 --> 01:23:59.280
Yes, the same is applied to autonomous vehicles. The same is probably going to continue being

01:23:59.280 --> 01:24:07.360
applied to a lot of artificial intelligence systems. In the 21st century, the process of

01:24:07.360 --> 01:24:15.280
analyzing the progress of AI is the search for one case where the system fails in a big way

01:24:15.280 --> 01:24:21.520
where humans would not. And then many people writing articles about it. And then broadly,

01:24:22.720 --> 01:24:28.720
the public generally gets convinced that the system is not intelligent. And we pacify ourselves

01:24:28.720 --> 01:24:33.520
by thinking it's not intelligent because of this one anecdotal case. And this seems to continue

01:24:33.520 --> 01:24:38.400
happening. Yeah, I mean, there is truth to that. Although I'm sure that plenty of people are also

01:24:38.400 --> 01:24:42.160
extremely impressed by the systems that exist today. But I think this connects to the earlier

01:24:42.160 --> 01:24:49.520
point we discussed that it's just confusing to judge progress in AI. And you have a new robot

01:24:49.520 --> 01:24:55.440
demonstrating something. How impressed should you be? And I think that people will start to be

01:24:55.440 --> 01:25:01.520
impressed once AI starts to really move the needle on the GDP. So you're one of the people that

01:25:01.520 --> 01:25:08.000
might be able to create an AI system here, not you, but you and open AI. If you do create an

01:25:08.000 --> 01:25:16.000
AI system, and you get to spend sort of the evening with it, him, her, what would you talk about,

01:25:16.000 --> 01:25:20.800
do you think? The very first time? First time? Well, the first time I would just

01:25:21.760 --> 01:25:26.320
would just ask all kinds of questions and try to make it to get it to make a mistake. And that would

01:25:26.320 --> 01:25:33.920
be amazed that it doesn't make mistakes and just keep keep asking broad. Okay, what kind of questions

01:25:33.920 --> 01:25:41.120
do you think? Would they be factual or would they be personal, emotional, psychological? What do you

01:25:41.120 --> 01:25:51.520
think? All of the above. Would you ask for advice? Definitely. I mean, why would I limit myself

01:25:51.520 --> 01:25:57.440
talking to a system like this? Now, again, let me emphasize the fact that you truly are one of the

01:25:57.440 --> 01:26:05.120
people that might be in the room where this happens. So let me ask a sort of a profound question

01:26:05.120 --> 01:26:11.520
about, I've just talked to a Stalin historian. I've been talking to a lot of people who are

01:26:11.520 --> 01:26:18.560
studying power. Abraham Lincoln said, nearly all men can stand adversity. But if you want to test

01:26:18.560 --> 01:26:26.400
a man's character, give him power. I would say the power of the 21st century, maybe the 22nd,

01:26:26.400 --> 01:26:32.640
but hopefully the 21st would be the creation of an AGI system and the people who have control,

01:26:33.360 --> 01:26:40.000
direct possession and control the AGI system. So what do you think after spending that evening

01:26:41.120 --> 01:26:45.120
having a discussion with the AGI system? What do you think you would do?

01:26:45.520 --> 01:26:56.240
Well, the ideal world I'd like to imagine is one where humanity are like the board,

01:26:56.960 --> 01:27:03.760
the board members of a company where the AGI is the CEO. So it would be,

01:27:05.520 --> 01:27:09.840
I would like the picture of which I would imagine is you have some kind of different

01:27:10.720 --> 01:27:17.520
entities, different countries or cities, and the people that leave their vote for what the AGI

01:27:17.520 --> 01:27:22.160
that represents them should do and the AGI that represents them goes and does it. I think a picture

01:27:22.160 --> 01:27:28.560
like that, I find very appealing. You could have multiple AGI, you would have an AGI for a city,

01:27:28.560 --> 01:27:35.920
for a country, and it would be trying to in effect take the democratic process to the next level.

01:27:35.920 --> 01:27:40.080
And the board can almost fire the CEO? Essentially, press the reset button,

01:27:40.080 --> 01:27:47.200
say, re-randomize the parameters. But let me sort of, that's actually, okay, that's a beautiful

01:27:47.200 --> 01:27:54.560
vision, I think, as long as it's possible to press the reset button. Do you think it will always be

01:27:54.560 --> 01:28:00.080
possible to press the reset button? So I think that it's definitely really possible to build.

01:28:00.880 --> 01:28:11.600
So you're talking, so the question that I really understand from you is, will humans or humans

01:28:11.600 --> 01:28:16.720
people have control over the AI systems that they build? Yes. And my answer is, it's definitely

01:28:16.720 --> 01:28:22.720
possible to build AI systems which will want to be controlled by their humans. Wow, that's part of

01:28:22.720 --> 01:28:32.080
their, so it's not that just they can't help but be controlled, but that's the one of the

01:28:32.080 --> 01:28:37.360
objectives of their existence is to be controlled. In the same way that human parents

01:28:39.760 --> 01:28:45.280
generally want to help their children, they want their children to succeed. It's not a burden for

01:28:45.280 --> 01:28:51.600
them. They are excited to help the children to feed them and to dress them and to take care of them.

01:28:52.560 --> 01:28:59.360
And I believe with high conviction that the same will be possible for an AGI. It will be possible

01:28:59.360 --> 01:29:05.360
to program an AGI to design it in such a way that it will have a similar deep drive that it will be

01:29:05.360 --> 01:29:12.800
delighted to fulfill and the drive will be to help humans flourish. But let me take a step back

01:29:12.800 --> 01:29:16.880
to that moment where you create the AGI system. I think this is a really crucial moment.

01:29:17.760 --> 01:29:27.360
And between that moment and the Democratic Board members with the AGI at the head,

01:29:28.800 --> 01:29:36.160
there has to be a relinquishing of power. So as George Washington, despite all the bad things he

01:29:36.160 --> 01:29:40.960
did, one of the big things he did is he relinquished power. He first of all didn't want to be president

01:29:41.920 --> 01:29:47.760
and even when he became president, he didn't keep just serving as most dictators do for indefinitely.

01:29:49.120 --> 01:29:57.120
Do you see yourself being able to relinquish control over an AGI system, given how much

01:29:57.120 --> 01:30:03.040
power you can have over the world, at first financial, just make a lot of money and then

01:30:03.040 --> 01:30:09.280
control by having possession as a AGI system? I'd find it trivial to do that. I'd find it

01:30:09.360 --> 01:30:14.320
trivial to relinquish this kind of power. I mean, you know, the kind of scenario you are

01:30:14.320 --> 01:30:21.200
describing sounds terrifying to me. That's all. I would absolutely not want to be in that position.

01:30:22.400 --> 01:30:28.640
Do you think you represent the majority or the minority of people in the AGI community?

01:30:29.280 --> 01:30:35.600
Well, I mean, it's an open question, an important one. Are most people good is another way to ask it.

01:30:36.480 --> 01:30:39.600
So I don't know if most people are good. But

01:30:42.160 --> 01:30:45.760
I think that when it really counts, people can be better than we think.

01:30:46.880 --> 01:30:52.560
That's beautifully put. Yeah. Are there specific mechanisms you can think of of aligning AGI

01:30:52.560 --> 01:30:58.320
in values to human values? Is that do you think about these problems of continued alignment

01:30:58.320 --> 01:31:03.440
as we develop the systems? Yeah, definitely. In some sense,

01:31:04.400 --> 01:31:09.120
the kind of question which you are asking is so if you have to translate the question to today's

01:31:09.120 --> 01:31:18.400
terms, it would be a question about how to get an RL agent that's optimizing a value function

01:31:18.400 --> 01:31:23.920
which itself is learned. And if you look at humans, humans are like that because the reward

01:31:23.920 --> 01:31:29.920
function, the value function of humans is not external, it is internal. That's right. And

01:31:30.080 --> 01:31:38.080
and there are definite ideas of how to train a value function, basically an objective,

01:31:38.960 --> 01:31:44.240
an as objective as possible perception system that will be trained separately

01:31:45.680 --> 01:31:53.120
to recognize, to internalize human judgments on different situations. And then that component

01:31:53.120 --> 01:31:58.880
wouldn't be integrated as the value as the base value function for some more capable RL system.

01:31:58.880 --> 01:32:02.960
You could imagine a process like this. I'm not saying this is the process. I'm saying this is

01:32:02.960 --> 01:32:11.120
an example of the kind of thing you could do. So on that topic of the objective functions of

01:32:11.120 --> 01:32:16.720
human existence, what do you think is the objective function that's implicit in human

01:32:16.720 --> 01:32:32.480
existence? What's the meaning of life? Oh, I think the question is wrong in some way. I think that

01:32:32.480 --> 01:32:36.640
the question implies that there is an objective answer which is an external answer, you know,

01:32:36.640 --> 01:32:44.320
your meaning of life is X. I think what's going on is that we exist and that's amazing. And we

01:32:44.400 --> 01:32:48.960
should try to make the most of it and try to maximize our own value and enjoyment of

01:32:49.760 --> 01:32:56.160
a very short time while we do exist. It's funny because action does require an objective function.

01:32:56.160 --> 01:33:01.840
It's definitely theirs in some form, but it's difficult to make it explicit and maybe impossible

01:33:01.840 --> 01:33:07.120
to make it explicit. I guess is what you're getting at. And that's an interesting fact of an RL

01:33:07.120 --> 01:33:13.360
environment. Well, what I was making a slightly different point is that humans want things and

01:33:13.360 --> 01:33:19.760
their wants create the drives that cause them to, you know, our wants are our objective functions,

01:33:19.760 --> 01:33:24.160
our individual objective functions. We can later decide that we want to change,

01:33:24.160 --> 01:33:27.120
that what we wanted before is no longer good and we want something else.

01:33:27.120 --> 01:33:32.000
Yeah, but they're so dynamic, there's got to be some underlying sort of Freud,

01:33:32.000 --> 01:33:37.840
there's things like sexual stuff, there's people who think it's the fear of death and there's also

01:33:38.800 --> 01:33:42.720
the desire for knowledge and, you know, all these kinds of things, procreation,

01:33:43.760 --> 01:33:48.000
the sort of all the evolutionary arguments, it seems to be, there might be some kind of

01:33:48.000 --> 01:33:54.880
fundamental objective function from which everything else emerges. But it seems like

01:33:54.880 --> 01:33:58.160
it's very difficult to make it explicit. I think that probably is an evolutionary

01:33:58.160 --> 01:34:01.600
objective function, which is to survive and procreate and make your students succeed.

01:34:02.480 --> 01:34:07.360
That would be my guess, but it doesn't give an answer to the question of what's the meaning of

01:34:07.360 --> 01:34:14.560
life. I think you can see how humans are part of this big process, this ancient process we are,

01:34:16.640 --> 01:34:24.160
we exist on a small planet. And that's it. So given that we exist, try to make the most of it

01:34:24.160 --> 01:34:30.960
and try to enjoy more and suffer less as much as we can. Let me ask two silly questions about life.

01:34:31.680 --> 01:34:39.360
One, do you have regrets? Moments that if you went back, you would do differently. And two,

01:34:40.000 --> 01:34:43.360
are there moments that you're especially proud of that made you truly happy?

01:34:44.720 --> 01:34:52.080
So I can answer both questions. Of course, there's a huge number of choices and decisions that have

01:34:52.080 --> 01:34:56.880
made that with the benefit of hindsight, I wouldn't have made them. And I do experience some regret,

01:34:56.880 --> 01:35:01.680
but I tried to take solace in the knowledge that at the time I did the best I could.

01:35:02.720 --> 01:35:07.360
And in terms of things that I'm proud of, I'm very fortunate to have done things I'm proud of

01:35:08.640 --> 01:35:13.200
and they made me happy for some time, but I don't think that that is the source of happiness.

01:35:14.480 --> 01:35:19.280
So your academic accomplishments, all the papers, you're one of the most cited people in the world.

01:35:19.840 --> 01:35:23.440
All the breakthroughs I mentioned in computer vision and language and so on

01:35:23.440 --> 01:35:29.440
is what is the source of happiness and pride for you?

01:35:29.440 --> 01:35:33.920
I mean, all those things are a source of pride for sure. I'm very grateful for having done all

01:35:33.920 --> 01:35:40.160
those things. And it was very fun to do them. But happiness comes, you know, you can, happiness,

01:35:40.160 --> 01:35:46.160
well, my current view is that happiness comes from our to a very large degree from the way we

01:35:46.160 --> 01:35:51.600
look at things. You know, you can have a simple meal and be quite happy as a result or you can

01:35:51.600 --> 01:35:57.360
talk to someone and be happy as a result as well. Or conversely, you can have a meal and be

01:35:57.360 --> 01:36:02.240
disappointed that the meal wasn't a better meal. So I think a lot of happiness comes from that.

01:36:02.240 --> 01:36:08.000
But I'm not sure. I don't want to be too confident. Being humble in the face of the uncertainty seems

01:36:08.000 --> 01:36:13.840
to be also a part of this whole happiness thing. Well, I don't think there's a better way to end

01:36:13.840 --> 01:36:19.840
it than meaning of life and discussions of happiness. So Ilya, thank you so much. You've

01:36:19.840 --> 01:36:25.600
given me a few incredible ideas. You've given the world many incredible ideas. I really appreciate it.

01:36:25.600 --> 01:36:29.360
And thanks for talking today. Yeah, thanks for stopping by. I really enjoyed it.

01:36:30.400 --> 01:36:34.080
Thanks for listening to this conversation with Ilya Setskever. And thank you to our

01:36:34.080 --> 01:36:39.440
presenting sponsor, Cash App. Please consider supporting the podcast by downloading Cash App

01:36:39.440 --> 01:36:45.280
and using the code Lex podcast. If you enjoy this podcast, subscribe on YouTube,

01:36:45.280 --> 01:36:50.400
review it with five stars and Apple podcast, support on Patreon or simply connect with me

01:36:50.400 --> 01:36:58.240
on Twitter at Lex Friedman. And now let me leave you with some words from Alan Turing on machine

01:36:58.240 --> 01:37:04.960
learning. Instead of trying to produce a program to simulate the adult mind, why not rather try to

01:37:04.960 --> 01:37:11.280
produce one which simulates the child? If this were then subjected to an appropriate course of

01:37:11.280 --> 01:37:27.200
education, one would obtain the adult brain. Thank you for listening and hope to see you next time.

