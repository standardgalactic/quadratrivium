start	end	text
0	5520	The following is a conversation with Yosha Bach, VP of Research at the AI Foundation
5520	12240	with a history of research positions at MIT and Harvard. Yosha is one of the most unique
12240	17280	and brilliant people in the artificial intelligence community, exploring the workings of the human
17280	24640	mind, intelligence, consciousness, life on earth, and the possibly simulated fabric of our universe.
24800	28720	I can see myself talking to Yosha many times in the future.
29760	36480	Quick summary of the ads. Two sponsors, ExpressVPN and Cash App. Please consider supporting
36480	44560	the podcast by signing up at expressvpn.com slash lexpod and downloading Cash App and using code LEX
44560	51200	podcast. This is the artificial intelligence podcast. If you enjoy it, subscribe on YouTube,
51200	56880	review it with 5 stars on Apple Podcast, support it on Patreon, or simply connect with me on Twitter
56880	64240	at Lex Freedman. Since this comes up more often than I ever would have imagined, I challenge you
64240	70480	to try to figure out how to spell my last name without using the letter E. And it'll probably
70480	76080	be the correct way. As usual, I'll do a few minutes of ads now and never hear any ads in the middle
76080	82400	that can break the flow of the conversation. This show is sponsored by ExpressVPN. Get it at
82400	88880	expressvpn.com slash lexpod to support this podcast and to get an extra three months free
88880	96640	on a one-year package. I've been using ExpressVPN for many years. I love it. I think ExpressVPN is
96640	101920	the best VPN out there. They told me to say it, but I think it actually happens to be true.
102880	109040	It doesn't log your data. It's crazy fast and it's easy to use literally just one big power on
109040	115440	button. Again, for obvious reasons, it's really important that they don't log your data. It works
115440	122080	on Linux and everywhere else too. Shout out to my favorite flavor of Linux, Ubuntu Mate 2004.
122880	129920	Once again, get it at expressvpn.com slash lexpod to support this podcast and to get an
129920	137040	extra three months free on a one-year package. This show is presented by Cash App, the number one
137040	143600	finance app in the App Store. When you get it, use code lexpodcast. Cash App lets you send money to
143600	150240	friends by Bitcoin and invest in the stock market with as little as $1. Since Cash App does fractional
150240	155280	share trading, let me mention that the order execution algorithm that works behind the scenes
155280	161120	to create the abstraction of the fractional orders is an algorithmic marvel. So big props to
161120	166240	the Cash App engineers for taking a step up to the next layer of abstraction over the stock market,
166240	173040	making trading more accessible for new investors and diversification much easier. So again, if you
173040	179840	get Cash App from the App Store, Google Play and use the code lexpodcast, you get $10 and Cash App
180160	186480	will also donate $10 to FIRST, an organization that is helping advance robotics and STEM education
186480	193120	for young people around the world. And now, here's my conversation with Yosha Bach.
194400	199360	As you've said, you grew up in a forest in East Germany, just as what we're talking about off mic,
200160	205520	to parents who were artists. And now I think, at least to me, you've become one of the most
205520	210560	unique thinkers in the AI world. So can we try to reverse engineer your mind a little bit?
211520	218560	What were the key philosophers, scientists, ideas, maybe even movies, or just realizations that
219280	225040	impact on you when you're growing up that kind of led to the trajectory? Or were the key sort of
225040	232160	crossroads in the trajectory of your intellectual development? My father came from a long tradition
232160	240560	of architects, a distant branch of the Bach family. And so basically, he was technically a nerd,
240560	247840	and nerds need to interface in society with non-standard ways. Sometimes I define a nerd
247840	252160	as somebody who thinks that the purpose of communication is to submit your ideas to peer
252160	259360	review. And normal people understand that the primary purpose of communication is to negotiate
259360	264480	alignment. And these purposes tend to conflict, which means that nerds have to learn how to
264480	272080	interact with society at large. Who is the reviewer in the nerd's view of communication?
272080	277760	Everybody who you consider to be a peer. So whatever hapless individual is around,
277760	281360	well, you would try to make him or her the gift of information.
282080	288720	Okay. So you're not, by the way, my research will malinform me. So you're
290160	297280	architect or artist? So he did study architecture. But basically, my grandfather made the wrong
297280	305600	decision. He married an aristocrat and was drawn into the war. And he came back after 15 years.
305600	312800	So basically, my father was not parented by a nerd, but by somebody who tried to tell him
312800	319840	what to do and expected him to do what he was told. And he was unable to. He's unable to do
319840	324320	things if he's not intrinsically motivated. So in some sense, my grandmother broke her son.
324880	330720	And her son responded when he became an architect to become an artist. So he built
330720	335440	100-wasser architecture. He built houses without right angles. He'd built lots of things that
335440	341200	didn't work in the more brutalist traditions of Eastern Germany. And so he bought an old
341200	345600	watermill, moved out to the countryside, and did only what he wanted to do, which was art.
346160	352080	Eastern Germany was perfect for Bohem, because you had complete material safety. Put was heavily
352080	356560	subsidized. Haskell was free. You didn't have to worry about rent or pensions or anything.
357040	361280	Socialized Communist side of the memory. Yes. And the other thing is it was almost impossible
361280	365360	not to be in political disagreement with your government, which is very productive for artists.
365360	370240	So everything that you do is intrinsically meaningful, because it will always touch on the
370240	375120	deeper currents of society, of culture, and being conflict with it, and tension with it. And you
375120	379920	will always have to define yourself with respect to this. So what impact did your father, this
380560	386720	outside-of-the-box thinker against the government, against the world artists have?
386720	391440	He was actually not a thinker. He was somebody who only got self-aware to the degree that he
391440	399200	needed to make himself functional. So in some sense, he was also in the late 1960s, and he was
399200	404800	in some sense a hippie. So he became a one-person cult. He lived out there in his kingdom. He built
404800	413520	big sculpture gardens and started many avenues of art and so on, and convinced a woman to live
413520	418560	with him. She was also an architect, and she adored him and decided to share her life with him.
418560	425680	And I basically grew up in a big cave full of books. I'm almost feral. And I was bored out there. It
425680	431760	was very, very beautiful, very quiet, and quite lonely. So I started to read, and by the time I
431760	436960	came to school, I've read everything until fourth grade and then some. And there was not a real way
436960	442240	for me to relate to the outside world. And I couldn't quite put my finger on why. And today,
442240	447600	I know it was because I was a nerd, obviously. And it was the only nerd around. So there were no
447600	454000	other kids like me. And there was nobody interested in physics or computing or mathematics and so on.
454640	459920	And this village school that I went to, it was basically a nice school. Kids were nice to me.
459920	464400	I was not beaten up, but I also didn't make many friends or build deep relationships. They only
464400	468560	happened then starting from ninth grade when I went into a school for mathematics and physics.
469280	471200	Do you remember any key books from this moment?
471200	476880	Yes, I basically read everything. So I went to the library and I worked my way through the
476880	481680	children's and young adult sections. And then I read a lot of science fiction. For instance,
481680	487040	Danislav Lem, basically the great author of cybernetics, has influenced me. Back then,
487120	490720	I didn't see him as a big influence because everything that he wrote seemed to be so natural
490720	496640	to me. And it's only later that I contrasted it with what other people wrote. Another thing that
496640	502080	was very influential on me were the classical philosophers and also the literature of romanticism.
502080	509680	So German poetry and art, Dr. Hilzhoff and Heine, and up to Hesse and so on.
510320	515920	Hesse, I love Hesse. So at which point do the classical philosophers end at this point
515920	520640	or in the 21st century? What's the latest classical philosopher? Does this stretch through
521680	525840	even as far as Nietzsche or is this, are we talking about Plato and Aristotle?
525840	529120	I think that Nietzsche is the classical equivalent of a shit poster.
531600	538000	So he's very smart and easy to read. But he's not so much trolling others. He's trolling himself
538000	542480	because he was at odds with the world. Largely, his romantic relationships didn't work out.
542560	544720	He got angry and he basically became a nihilist.
547360	551840	Isn't that a beautiful way to be as an intellectual is to constantly be trolling yourself,
551840	557280	to be in that conflict, in that tension? I think it's a lack of self-awareness. At some point,
557280	561680	you have to understand the comedy of your own situation. If you take yourself seriously
562400	565840	and you are not functional, it ends in tragedy as it did for Nietzsche.
565840	569840	I think you think he took himself too seriously in that tension.
569840	574160	And if you find the same thing in Hesse and so on, the Steppenwolf syndrome is classic
574160	578800	adolescence where you basically feel misunderstood by the world and you don't understand that all
578800	584080	the misunderstandings are the result of your own lack of self-awareness because you think that you
584080	588960	are a prototypical human and the others around you should behave the same way as you expect them
588960	593600	based on your innate instincts and it doesn't work out. And you become a transcendentalist
593600	598480	to deal with that. So it's very, very understandable and have great sympathies for this
598560	603760	to the degree that I can have sympathy for my own intellectual history, but you have to grow out of it.
604720	609680	So as an intellectual, a life well-lived, a journey well-traveled is one where you don't
609680	616960	take yourself seriously? No, I think that you are neither serious or not serious yourself because you
616960	623280	need to become unimportant as a subject. That is, if you are a philosopher, belief is not a verb.
623600	628560	Yeah. You don't do this for the audience, you don't do it for yourself. You have to submit to
628560	633920	the things that are possibly true and you have to follow wherever your inquiry leads, but it's
633920	640240	not about you. It has nothing to do with you. So do you think then people like Ayn Rand believed
640240	644800	sort of an idea of there's objective truth? So what's your sense in the philosophical,
646000	649920	if you remove yourself that's objective from the picture, you think it's possible to actually discover
650640	656480	ideas that are true or are we just in a mesh of relative concepts that are either true or false?
656480	661440	It's just a giant mess. You cannot define objective truth without understanding the
661440	666480	nature of truth in the first place. So what does the brain mean by saying that it covers
666480	672560	something as truth? So for instance, a model can be predictive or not predictive. Then there
672560	677280	can be a sense in which a mathematical statement can be true because it's defined as true under
677280	684640	certain conditions. So it's basically a particular state that a variable can have in a symbol game.
684640	689440	And then you can have a correspondence between systems and talk about truth, which is again
689440	693920	a type of model correspondence. And there also seems to be a particular kind of ground truth.
693920	699360	So for instance, you're confronted with the enormity of something existing at all, right?
699360	704640	That's stunning when you realize something exists rather than nothing. And this seems
704640	709360	to be true, right? There's absolute truth in the fact that something seems to be happening.
709920	715040	Yeah, that to me is a showstopper. I could just think about that idea and be amazed by that idea
715760	719520	for the rest of my life and not going any farther because I don't even know the answer to that.
719520	723920	Why does anything exist at all? Well, the easiest answer is existence is the default, right? So
723920	728000	this is the lowest number of bits that you would need to encode this. Whose answer? Who provides
728000	732800	the simplest answer to this is that existence is the default. What about non-existence? I mean,
732800	738720	that seems non-existence might not be a meaningful notion in the sense. So in some sense, if everything
738720	744240	that can exist exists for something to exist, it probably needs to be implementable. The only
744240	748960	thing that can be implemented is finite automata. So maybe the whole of existence is the superposition
748960	753360	of all finite automata. And we are in some region of the fractal that has the properties that it
753360	761520	can contain us. What does it mean to be a superposition of finite? So any superposition like
761520	766880	all possible rules. Imagine that every automaton is basically an operator that acts on some
766880	773680	substrate. And as a result, you get emergent patterns. What's the substrate? I have no idea to
773680	779200	know. But some substrate? It's something that can store information. Something that can store
779200	783280	information, there's automaton. Something that can hold state. Still, doesn't make sense to me the
783280	790640	why that exists at all. I could just sit there with a beer or a vodka and just enjoy the fact,
790640	796080	wandering the why. May not have a why. This might be the wrong direction to ask into this. So there
796080	802560	could be no relation in the y direction without asking for a purpose or for a cause. It doesn't
802560	808320	mean that everything has to have a purpose or a cause, right? So we mentioned some philosophers
808320	813760	in that early just taking a brief step back into that. So we asked ourselves when did classical
813760	819040	philosophy end? I think for Germany, it largely ended with the first revolution. That's basically
819120	827120	when we ended the monarchy and started a democracy. And at this point, we basically came
827120	832720	up with a new form of government that didn't have a good sense of this new organism that society
832720	837760	wanted to be. And in a way, it decapitated the universities. So the universities went on
837760	842080	through modernism like a headless chicken. At the same time, democracy failed in Germany,
842080	847520	and we got fascism as a result. And it burned down things in a similar way as Stalinism burned
847520	852480	down intellectual traditions in Russia. And Germany, both Germanies have not recovered from
852480	857760	this. Eastern Germany had this vulgar dialectic materialism, and Western Germany didn't get
857760	863200	much more edgy than Habermas. So in some sense, both countries lost their intellectual traditions
863200	870080	and killing off and driving out the Jews didn't help. Yeah, so that was the end. That was the end
870080	876320	of really rigorous what you would say is classical philosophy. There's also this thing that in some
876320	884560	sense, the low hanging foods in philosophy were mostly wrapped. And the last big things that we
884560	889680	discovered was the constructivist turn in mathematics. So to understand that the parts of
889680	895600	mathematics that work are computation. There was a very significant discovery in the first half of
895600	902160	the 20th century. And it hasn't fully permeated philosophy and even physics yet. Physicists
902160	906800	checked out the code libraries for mathematics before constructivism became universal.
907360	911280	What's constructivism? What are you referring to girls and completeness there and that kind of
911280	915280	those kinds of ideas? So basically, Gödel himself, I think, didn't get it yet. Hilbert
916080	920320	could get it. Hilbert saw that, for instance, countries set theoretical experiments and
920320	926000	mathematics led into contradictions. And he noticed that with the current semantics,
926000	929520	we cannot build a computer in mathematics that runs mathematics without crashing.
930080	935120	And Gödel could prove this. And so what Gödel could show is using classical mathematical
935120	940240	semantics, you run into contradictions. And because Gödel strongly believed in these semantics,
940240	945920	and more than in what he could observe and so on, he was shocked. It basically shook his
945920	949200	world to the core, because in some sense, he felt that the world has to be implemented in
949200	955520	classical mathematics. And for Turing, it wasn't quite so bad. I think that Turing could see that
955520	961040	the solution is to understand that mathematics was computation all along, which means you,
961040	967520	for instance, pi in classical mathematics is a value. It's also a function, but it's the same
967520	972800	thing. And in computation, a function is only a value when you can compute it. And if you cannot
972800	976960	compute the last digit of pi, you only have a function. You can plug this function into your
976960	981360	local sun, let it run until the sun burns out. This is it. This is the last digit of pi you
981360	986720	will know. But it also means there can be no process in the physical universe or in any physically
986720	992640	realized computer that depends on having known the last digit of pi. Which means there are parts of
992640	997520	physics that are defined in such a way that cannot strictly be true, because assuming that this could
997520	1003760	be true leads into contradictions. So I think putting computation at the center of the world view
1003760	1008880	is actually the right way to think about it. Yes. And Wittgenstein could see it. And Wittgenstein
1008880	1013920	basically preempted the logitist program of AI that Minsky started later, like 30 years later.
1014560	1019120	Turing was actually a pupil of Wittgenstein. Really? So I didn't know there's any connection
1019120	1022880	between Turing and Wittgenstein. And Wittgenstein even canceled some classes when Turing was not
1022880	1025760	present because he thought it was not worth spending the time on with the others.
1026560	1031120	Interesting. If you read the Tractados, it's a very beautiful book. Like they say you once thought
1031120	1037600	on 75 pages. It's very non-typical for philosophy because it doesn't have arguments in it and it
1037600	1042720	doesn't have references in it. It's just one thought that is not intending to convince anybody.
1043760	1049120	It's mostly for people that have the same insight as me. Just spell it out. And this insight is
1049680	1054320	there is a way in which mathematics and philosophy ought to meet. Mathematics tries to
1054320	1059680	understand the domain of all languages by starting with those that are so formalizable that you can
1059680	1064480	prove all the properties of the statements that you make. But the price that you pay is that
1064480	1068560	your language is very, very simple. So it's very hard to say something meaningful in mathematics.
1069440	1073680	And it looks complicated to people, but it's far less complicated than what our brain is casually
1073680	1079600	doing all the time and it makes sense of reality. And philosophy is coming from the top. So it's
1079600	1084400	mostly starting from natural languages with vaguely defined concepts. And the hope is that
1084400	1089520	mathematics and philosophy can meet at some point. And Wittgenstein was trying to make them meet.
1089520	1093280	And he already understood that, for instance, you could express everything with the non-calculus
1093280	1098320	that you could reduce the entire logic to non-gates as we do in our modern computers.
1098320	1102480	So in some sense, he already understood Turing universality before Turing spelled it out. I
1102480	1106720	think when he wrote the track titles, he didn't understand yet that the idea was so important
1106720	1112480	and significant. And as a specter, when Turing wrote it out, nobody cared that much. Turing was
1112480	1119120	not that famous. When he lived, it was mostly his work in decrypting the German codes that made
1119200	1124240	him famous or gave him some notoriety. But the same status that he has to computer science right
1124240	1129200	now and something that I think he'll acquire later. That's kind of interesting. Do you think of
1129200	1133760	computation and computer science? And you kind of represent that to me is maybe that's the modern
1134240	1142080	day. You, in a sense, are the new philosopher by sort of the computer scientist who dares to
1142080	1147760	ask the bigger questions that philosophy originally started. Is the new philosopher?
1147840	1153360	Certainly not me. I think I'm mostly still this child that grows up in a very beautiful valley
1153360	1157440	and looks at the world from the outside and tries to understand what's going on. And my teachers
1157440	1162400	tell me things and they largely don't make sense. So I have to make my own models. I have to discover
1162400	1166640	the foundations of what the others are saying. I have to try to fix them, to be charitable. I
1166640	1171280	try to understand what they must have thought originally or what their teachers or their teachers
1171280	1175360	teachers must have thought until everything got lost in translation and how to make sense of the
1175360	1180320	reality that we are in. And whenever I have an original idea, I'm usually late to the party
1180320	1184720	by say 400 years. And the only thing that's good is that the parties get smaller and smaller,
1184720	1190080	the older I get and the more I explore. The parties get smaller and more exclusive.
1190080	1196240	And more exclusive. So it seems like one of the key qualities of your upbringing was that you
1196240	1202320	were not tethered, whether it's because of your parents or in general, maybe you're something
1202320	1207920	within your, within your mind, some genetic material, you were not tethered to the ideas
1207920	1214080	of the general populace, which is actually a unique property where kind of, you know,
1214080	1218560	the education system and whatever from that education system, just existing in this world
1218560	1224560	forces certain sets of ideas onto you. Can you disentangle that? Why were you,
1225520	1232240	why are you not so tethered? Even in your work today, you seem to not care about perhaps
1233920	1241440	a best paper in Europe, right? Being tethered to particular things that current today in this year,
1242240	1247040	people seem to value as a thing you put on your CV and resume. You're a little bit more
1247040	1251120	outside of that world, outside of the world of ideas that people are especially focusing
1251200	1256800	the benchmarks of today, the things. What's, can you disentangle that? Because I think that's
1256800	1261280	inspiring. And if there were more people like that, we might be able to solve some of the bigger
1261280	1268240	problems that sort of AI dreams to solve. And there's a big danger in this, because in a way,
1268240	1273840	you are expected to marry into an intellectual tradition and visit this tradition into a
1273840	1278640	particular school. If everybody comes up with their own paradigms, the whole thing is not
1278640	1283040	cumulative as an enterprise, right? So in some sense, you need a healthy balance,
1283040	1287280	you need paradigmatic thinkers, and you need people that work within given paradigms.
1287280	1292640	Basically, scientists today define themselves largely by methods. And it's almost a disease
1292640	1298560	that we think as a scientist, somebody who was convinced by their guidance counselor that they
1298560	1302880	should join a particular discipline, and then they find a good mentor to learn the right methods,
1302880	1307840	and then they are lucky enough and privileged enough to join the right team, and then their
1307840	1313040	name will show up on influential papers. But we also see that there are diminishing returns with
1313040	1319040	this approach. And when our field, computer science and AI started, most of the people
1319040	1325120	that joined this field had interesting opinions. And today's thinkers in AI either don't have
1325120	1329040	interesting opinions at all, or these opinions are inconsequential for what they're actually
1329040	1333520	doing, because what they're doing is they apply the state of the art methods with a small epsilon.
1334320	1341200	And this is often a good idea if you think that this is the best way to make progress. And for
1341200	1347360	me, it's first of all, very boring. If somebody else can do it, why should I do it? If the current
1347360	1352960	methods of machine learning lead to strong AI, why should I be doing it? I will just wait until
1352960	1358240	they're done and wait until they do this on the beach, or read interesting books or write some,
1358320	1363360	and have fun. But if you don't think that we are currently doing the right thing,
1363360	1370480	if we are missing some perspectives, then it's required to think outside of the box. It's
1370480	1377040	also required to understand the boxes. But it's necessary to understand what worked,
1377040	1382080	and what didn't work, and for what reasons. So you have to be willing to ask new questions and
1382080	1387200	design new methods whenever you want to answer them. And you have to be willing to dismiss
1387280	1391040	the existing methods if you think that they're not going to yield the right answers.
1391040	1392960	It's very bad career advice to do that.
1394240	1403120	So maybe to briefly stay for one more time in the early days, when would you say for you was the
1403120	1409840	dream before we dive into the discussions that we just almost started? When was the dream to
1409840	1414000	understand or maybe to create human level intelligence born for you?
1414320	1424080	I think that you can see AI largely today as advanced information processing. If you would
1424080	1428720	change the acronym of AI into that, most people in the field would be happy. It would not change
1428720	1434640	anything what they're doing. We're automating statistics, and many of the statistical models
1434640	1439200	are more advanced than what statisticians had in the past. And it's pretty good work. It's
1439200	1444560	very productive. And the other aspect of AI is philosophical project. And this philosophical
1444560	1449840	project is very risky, and very few people work on it, and it's not clear if it succeeds.
1450400	1456480	So first of all, you keep throwing a lot of really interesting ideas, and I have to pick which
1456480	1464800	ones we go with. But first of all, you use the term information processing, just information
1464800	1472640	processing, as if it's the mere, it's the muck of existence, as if it's the epitome
1474320	1477920	that the entirety of the universe might be information processing, that consciousness
1477920	1483440	and intelligence might be information processing. So that maybe you can comment on if the advanced
1483440	1490160	information processing is a limiting kind of round of ideas. And then the other one is what
1490160	1496800	do you mean by the philosophical project? So I suspect that general intelligence is the result
1496800	1502560	of trying to solve general problems. So intelligence, I think, is the ability to model. It's not
1502560	1508080	necessarily goal directed rationality or something. Many intelligent people are bad at this. But
1508720	1514000	it's the ability to be presented with a number of patterns and see a structure in those patterns,
1514000	1520800	and be able to predict the next set of patterns to make sense of things. And some problems are
1520800	1525600	very general. Usually intelligence serves control. So you make these models for a particular purpose
1525600	1531200	of interacting as an agent with the world and getting certain results. But the intelligence itself
1531200	1535520	is in the sense instrumental to something. But by itself, it's just the ability to make models.
1535520	1540080	And some of the problems are so general that the system that makes them needs to understand
1540080	1544800	what itself is and how it relates to the environment. So as a child, for instance,
1544800	1550400	you notice you do certain things despite you perceiving yourself as wanting different things.
1550400	1555600	So you become aware of your own psychology. You become aware of the fact that you have
1555600	1559760	complex structure in yourself and you need to model yourself to reverse engineer yourself
1559760	1564640	to be able to predict how you will react to certain situations and how you deal with yourself
1564640	1569760	in relationship to your environment. And this process, if this project, if you reverse engineer
1569760	1574240	yourself in your relationship to reality in the nature of a universe that can contain you,
1574240	1579280	if you go all the way, this is basically the project of AI, or you could say the project of AI
1579280	1584640	is a very important component in it. The true Turing test in a way is, you ask a system,
1584640	1590720	what is intelligence? If that system is able to explain what it is, how it works,
1591680	1596640	then you should assign it the property of being intelligent in this general sense. So the test
1596640	1601200	that Turing was administering in a way, I don't think that he couldn't see it, but he didn't
1601200	1607760	express it yet in the original 1950 paper, is that he was trying to find out whether he was
1607760	1611680	generally intelligent. Because in order to take this test, the rub is, of course, you need to
1611680	1615920	be able to understand what the system is saying. And we don't yet know if we can build an AI.
1615920	1620880	We don't yet know if we are generally intelligent. Basically, you win the Turing test by building
1620880	1627120	an AI. Yes. So in a sense, hidden within the Turing test is a kind of recursive test.
1627120	1632080	Yes, it's a test on us. The Turing test is basically a test of the conjecture whether
1632080	1637520	people are intelligent enough to understand themselves. Okay. But you also mentioned a
1637520	1642320	little bit of a self-awareness. And then the project of AI, do you think this kind of emergent
1642400	1649280	self-awareness is one of the fundamental aspects of intelligence? So as opposed to
1649280	1657600	goal-oriented, as you said, kind of puzzle-solving is coming to grips with the idea that you're an
1657600	1662240	agent in the world. And I find that many highly intelligent people are not very self-aware,
1662960	1667840	right? So self-awareness and intelligence are not the same thing. And you can also be
1667840	1673120	self-aware if you have good priors especially, without being especially intelligent. So you
1673120	1677680	don't need to be very good at solving puzzles if the system that you are already implements the
1677680	1685440	solution. But I do find intelligence. So you kind of mentioned children, right? Is that the
1685440	1693120	fundamental project of AI is to create the learning system that's able to exist in the world?
1693120	1700800	So you kind of drew a difference in self-awareness and intelligence. And yet you said that the
1700800	1706960	self-awareness seems to be important for children. So I call this ability to make sense of the world
1706960	1711360	and your own place and so to make you able to understand what you're doing in this world
1711360	1716000	sentience. And I would distinguish sentience from intelligence because sentience is
1717120	1721760	possessing certain classes of models. And intelligence is a way to get to these models
1721760	1733040	if you don't already have them. I see. So can you maybe pause a bit and try to answer the question
1733040	1738880	that we just said we may not be able to answer and might be a recursive meta question of what is
1738880	1746240	intelligence? I think that intelligence is the ability to make models. So models. I think it's
1746240	1756400	useful as examples. Very popular now. Neural networks form representations of large-scale
1756400	1762960	data set. They form models of those data sets. When you say models and look at today's neural
1762960	1768320	networks, what are the difference of how you're thinking about what is intelligent in saying
1768320	1774560	that intelligence is the process of making models? Two aspects to this question. One is
1774640	1778880	the representation as the representation adequate for the domain that we want to represent.
1779760	1785040	And the other one is the type of the model that you arrive at adequate. So basically,
1785040	1792560	are you modeling the correct domain? And I think in both of these cases, modern AI is lacking still.
1792560	1797120	And I think that I'm not saying anything new here. I'm not criticizing the field. Most of the
1797120	1803760	people that design our paradigms are aware of that. And so one aspect that we are missing is
1803760	1808720	unified learning. When we learn, we at some point discover that everything that we sense
1809280	1814000	is part of the same object, which means we learn it all into one model. And we call this model
1814000	1818880	the universe. So the experience of the world that we are embedded on is not a secret direct
1818880	1823440	wire to physical reality. Physical reality is a weird quantum graph that we can never experience
1823440	1828560	or get access to. But it has these properties that it can create certain patterns that our
1828560	1832720	systemic interface to the world. And we make sense of these patterns and the relationship
1832800	1836480	between the patterns that we discover is what we call the physical universe.
1836480	1843840	So at some point in our development as a nervous system, we discover that everything that we relate
1843840	1850160	to in the world can be mapped to a region in the same three dimensional space by and large.
1850160	1854720	We now know in physics that this is not quite true. The world is not actually three dimensional,
1854720	1858320	but the world that we are entangled with at the level which we are entangled with
1858320	1863920	is largely a flat three dimensional space. And so this is the model that our brain is
1863920	1869360	intuitively making. And this is, I think, what gave rise to this intuition of res extensor
1869360	1873200	of this material world, this material domain. It's one of the mental domains, but it's just
1873200	1877920	the class of all models that relate to this environment, this three dimensional physics
1877920	1881760	engine in which we are embedded. Physics engine ocean we're embedded. I love that.
1882640	1892560	Just slowly pause. So the quantum graph, I think you called, which is the real world,
1892560	1896560	which you can never get access to, there's a bunch of questions I want to sort of
1896560	1902320	disentangle that, but maybe one useful one, one of your recent talks I looked at,
1902320	1907680	can you just describe the basics? Can you talk about what is dualism? What is idealism?
1907760	1912320	What is materialism? What is functionalism? And what connects with you most in terms of,
1912320	1916800	because he just mentioned, there's a reality we don't have access to. Okay. What does that even
1916800	1923440	mean? And why don't we get access to it? Are we part of that reality? Why can't we access it?
1923440	1929360	So the particular trajectory that mostly exists in the West is the result of our indoctrination
1929360	1935600	by a cult for 2000 years. A cult, which one? The Catholic cult mostly. And for better or worse,
1935600	1941040	it has created or defined many of the modes of interaction that we have that has created this
1941040	1950000	society, but it has also in some sense scarred our rationality. And the intuition that exists,
1950000	1955600	if you would translate the mythology of the Catholic Church into the modern world is that
1955600	1960480	the world in which you and me interact is something like a multiplayer role-playing
1960480	1964960	adventure. And the money and the objects that we have in this world, this is all not real.
1966240	1972960	Or Eastern philosophers would say it's my eye. It's just stuff that appears to be meaningful. And
1972960	1979280	this embedding in this meaning, if you believe in it, is samsara. It's basically the identification
1979280	1986080	with the needs of the mundane secular everyday existence. And the Catholics also introduced
1986080	1991760	the notion of higher meaning, the sacred. And this existed before, but eventually the natural
1991760	1996240	shape of God is the platonic form of the civilization that you are part of. It's basically
1996240	2001840	the super-organism that is formed by the individuals as an intentional agent. And basically,
2001840	2007920	the Catholics used a relatively crude mythology to implement software on the minds of people and
2007920	2014240	get the software synchronized to make them walk on lockstep, to basically get this God online
2014240	2020640	and to make it efficient and effective. And I think God technically is just a self that spends
2020720	2026320	multiple brains as opposed to your and myself, which mostly exists just on one brain. And so,
2026320	2031040	in some sense, you can construct a self functionally as a function that is implemented by brains
2031040	2035040	that exists across brains. And this is a God with a small g.
2035040	2038720	That's one of the, if you've all Harari kind of talking about,
2040000	2044560	this is one of the nice features of our brains, it seems to that we can all download the same
2044560	2047760	piece of software, like God in this case, and kind of share it.
2047920	2051440	Yeah. So basically, you give everybody a spec and the mathematical constraints
2052320	2058640	that are intrinsic to information processing, make sure that given the same spec, you come
2058640	2063440	up with a compatible structure. Okay. So that's, there's the space of ideas that we all share.
2063440	2071600	And we think that's kind of the mind. But that's separate from the idea is from Christianity for
2072240	2075200	from religion is that there's a separate thing between the mind.
2075280	2080640	There is a real world. And this real world is the world in which God exists. God is the quarter
2080640	2085280	of the multiplayer adventure, so to speak. And we are all players in this game.
2085920	2091280	And that's dualism, you would say. But the dualist aspect is because the mental realm
2091280	2097280	is exists in a different implementation than a physical realm. And the mental realm is real.
2097280	2101520	And a lot of people have this intuition that there is this real room in which you and me
2101520	2107840	talk and speak right now, then comes a layer of physics and abstract rules and so on. And then
2107840	2112800	comes another real room where our souls are. And our two form isn't a thing that gives us
2112800	2117200	phenomenal experience. And this is of course, a very confused notion that you would get.
2117760	2124880	And it's basically is the result of connecting materialism and idealism in the wrong way.
2124880	2129040	So okay, I apologize, but I think it's really helpful if we just try to define
2129840	2135040	terms. What is dualism? What is idealism? What is materialism for people who don't know?
2135040	2139520	So the idea of dualism in our cultural tradition is that there are two substances,
2139520	2145600	a mental substance and a physical substance. And they interact by different rules. And the
2145600	2151600	physical world is basically causally closed and is built on a low-level causal structure. So
2151600	2155600	they're basically a bottom level that is causally closed that's entirely mechanical.
2156240	2160240	And mechanical in the widest sense. So it's computational. There's basically a physical
2160240	2165280	world in which information flows around. And physics describes the laws of how information
2165280	2169680	flows around in this world. Would you compare it to like a computer where you have a hardware
2169680	2175040	and software? The computer is a generalization of information flowing around basically. But
2175040	2179360	during this cover that there is the universal principle, you can define this universal machine
2180240	2185200	that is able to perform all the computations. So all these machines have the same power.
2185520	2188960	This means that you can always define a translation between them as long as they
2188960	2194320	have unlimited memory to be able to perform each other's computations.
2194320	2198880	So would you then say that materialism is this whole world is just the hardware
2198880	2201360	and idealism is this whole world is just the software?
2202160	2206000	Not quite. I think that most idealists don't have a notion of software yet,
2206000	2211760	because software also comes down to information processing. So what you notice is the only thing
2211760	2216080	that is real to you and me is this experiential world in which things matter, in which things
2216080	2220240	have taste, in which things have color, phenomenal content, and so on. And you realize that.
2220240	2224240	You are bringing up consciousness, okay. And this is distinct from the physical world,
2224240	2231760	in which things have values only in an abstract sense. And you only look at cold patterns
2232320	2237120	moving around. So how does anything feel like something? And this connection between the two
2237120	2241280	things is very puzzling to a lot of people, of course, too many philosophers. So idealism
2241280	2245760	starts out with the notion that mind is primary, materialism thinks that matter is primary.
2246320	2253040	And so for the idealist, the material patterns that we see playing out are part of the dream
2253040	2259520	that the mind is dreaming. And we exist in a mind on a higher plane of existence, if you want.
2259520	2266160	And for the materialist, there is only this material thing, and that generates some models,
2266160	2271680	and we are the result of these models. And in some sense, I don't think that we should
2271680	2276880	understand, if you understand it properly, materialism and idealism is a dichotomy,
2276880	2280720	but there's two different aspects of the same thing. So the weird thing is,
2280720	2285200	we don't exist in the physical world. We do exist inside of a story that the brain tells itself.
2285600	2296560	Okay. Let me, my information processing, take that in. We don't exist in the physical world,
2296560	2301280	we exist in the narrative. Basically, a brain cannot feel anything. A New Yorker cannot feel
2301280	2305120	anything. They're physical things. Physical systems are unable to experience anything.
2305120	2309360	But it would be very useful for the brain or for the organism to know what it would be like
2309360	2314880	to be a person and to feel something. So the brain creates a simulacrum of such a person,
2314960	2319360	that it uses to model the interactions of the person. It's the best model of what that
2319360	2322880	brain, this organism, thinks it is in relationship to its environment.
2322880	2326800	So it creates that model. It's a story, a multimedia novel that the brain is continuously
2326800	2332000	writing and updating. But you also kind of said that, you said that we kind of exist in that,
2332960	2339920	that story. Yes. That story. What is real in any of this? So like,
2340880	2347040	there's a, again, these terms are, you kind of said there's a quantum graph. I mean, what is,
2347040	2353120	what is this whole thing running on then? Is the story, and is it completely fundamentally
2353120	2360880	impossible to get access to it? Because isn't the story supposed to, isn't the brain in something,
2361520	2368240	in existing in some kind of context? So what we can identify as computer scientists, we can
2368320	2374960	engineer systems and test our theories this way that may have the necessary insufficient properties
2374960	2380240	to produce the phenomena that we are observing, which is there is a self in a virtual world
2380240	2386480	that is generated in somebody's neocortex that is contained in the skull of this primate here.
2386480	2391600	And when I point at this, this indexicality is, of course, wrong. But I do create something
2391600	2397520	that is likely to give rise to patterns on your retina that allow you to interpret what I'm saying,
2398080	2402960	but we both know that the world that you and me are seeing is not the real physical world.
2402960	2407680	What we are seeing is a virtual reality generated in your brain to explain the patterns on your
2407680	2415600	retina. How close is it to the real world? That's kind of the question. When you have people like
2415600	2421200	Donald Hoffman that say that you're really far away, the thing we're seeing, you and I now,
2421200	2427040	that interface we have is very far away from anything. We don't even have anything close to
2427040	2432080	the sense of what the real world is. Or is it a very surface piece of architecture?
2432080	2437120	Imagine you look at the Mandelbrot tractor, right? This famous thing that Bernard Mandelbrot
2437120	2442480	discovered. If you see an overall shape in there, right? But you know that if you truly
2442480	2449360	understand it, you know it's two lines of code. It's basically in a series that is being tested
2449360	2454080	for complex numbers in the complex number plane for every point. And for those where
2454080	2463280	the series is diverging, you paint this black and where it's converging, you don't. And you get the
2463280	2472160	intermediate colors by taking how far as it diverges. This gives you this shape of this fractal.
2472160	2476240	But imagine you live inside of this fractal and you don't have access to where you are in the
2476240	2482640	fractal or you have not discovered the generator function even. So what you see is all I can see
2482640	2486240	right now is a spiral. And this spiral moves a little bit to the right. Is this an accurate
2486240	2491920	model of reality? Yes, it is. It is an adequate description. You know that there is actually
2491920	2497440	no spiral in the Mandelbrot fractal. It only appears like this to an observer that is interpreting
2497440	2502800	things as a two-dimensional space and then defines certain regularities in there at a certain scale
2502800	2506560	that it currently observes. Because if you zoom in, the spiral might disappear and turn out to be
2506560	2510800	something different at a different resolution. So at this level, you have the spiral and then
2510800	2514320	you discover the spiral moves to the right and at some point, it disappears. So you have a
2514320	2519040	singularity. At this point, your model is no longer valid. You cannot predict what happens
2519040	2523840	beyond the singularity. But you can observe again and you will see it hit another spiral
2523840	2527920	and at this point, it disappeared. So we now have a second order law. And if you make
2527920	2532560	30 layers of these laws, then you have a description of the world that is similar to the one that we
2532560	2537200	come up with when we describe the reality around us. It's reasonably predictive. It does not cut
2537280	2542160	to the core of it. It does not explain how it's being generated, how it actually works. But it's
2542160	2546320	relatively good to explain the universe that we are entangled with. But you don't think the tools
2546320	2552560	of computer science, the tools of physics could step outside, see the whole drawing and get it
2552560	2558400	the basic mechanism of how the pattern, the spirals are generated. Imagine you would find
2558400	2562560	yourself embedded into a Mandelbrot fractal and you try to figure out what works and you have
2562640	2569360	somehow a Turing machine with enough memory to think. And as a result, you come to this idea,
2569360	2573840	it must be some kind of automaton. And maybe you just enumerate all the possible automata until
2573840	2578880	you get to the one that produces your reality. So you can identify necessary and sufficient
2578880	2583520	condition. For instance, we discover that mathematics itself is the domain of all languages.
2584160	2587680	And then we see that most of the domains of mathematics that we have discovered
2588240	2592560	are in some sense describing the same fractals. This is what category theory is obsessed about,
2592560	2596400	that you can map these different domains to each other. So there are not that many fractals.
2597440	2603920	Some of these have interesting structure and symmetry breaks. So you can discover what region
2603920	2608880	of this global fractal you might be embedded in from first principles. But the only way you
2608880	2613520	can get there is from first principles. So basically, your understanding of the universe has to start
2613520	2618240	with automata and the number theory and then spaces and so on. Yeah, I think like Stephen
2618240	2624320	Wolfram still dreams that he's that he'll be able to arrive at the fundamental rules of the cellular
2624320	2632080	automata or the generalization of which is behind our universe. You've said on this topic,
2632080	2637840	you said in a recent conversation that quote, some people think that a simulation can't be
2637840	2642880	conscious and only a physical system can. But they got it completely backward. A physical system
2643520	2648800	cannot be conscious. Only a simulation can be conscious. Consciousness is a simulated property,
2648800	2656240	the simulated self. Just like you said, the mind is kind of the we call it story narrative.
2656240	2659200	There's a simulation. So our mind is essentially a simulation?
2660000	2666160	Usually, I try to use the terminology so that the mind is basically a principles that produce
2666160	2670720	the simulation. It's the software that is implemented by your brain. And the mind is
2670720	2677200	creating both the universe that we are in and the self, the idea of a person that is on the other
2677200	2683120	side of attention and is embedded in this world. Why is that important, that idea of a self?
2683120	2689440	Why is that important feature in the simulation? It's basically a result of the purpose that
2689440	2694320	the mind has. It's a tool for modeling, right? We are not actually monkeys. We are side effects
2694320	2701440	of the regulation needs of monkeys. And what the monkey has to regulate is the relationship
2701440	2707120	of an organism to an outside world that is in large part also consisting of other organisms.
2708080	2713120	And as a result, it basically has regulation targets that it tries to get to. These regulation
2713120	2717200	targets start with priors. They're basically like unconditional reflexes that we are more or
2717200	2721920	less born with. And then we can reverse engineer them to make them more consistent. And then we
2721920	2724960	get more detailed models about how the world works and how to interact with it.
2725840	2731040	And so these priors that you commit to are largely target values that our needs should
2731040	2737280	approach, set points. And this deviation to the set point creates some urge, some tension.
2737280	2742000	And we find ourselves living inside of feedback loops, right? The consciousness emerges over
2742000	2747360	dimensions of disagreements with the universe. Things where you care, things are not the way
2747360	2752240	there should be, but you need to regulate. And so in some sense, the sense itself is the result
2752240	2756640	of all the identifications that you're having. And identification is a regulation target that
2756640	2760480	you're committing to. It's a dimension that you care about. Do you think it's important?
2761040	2766720	And this is also what locks you in. If you let go of these commitments of these identifications,
2766720	2771440	you get free. There's nothing that you have to do anymore. And if you let go of all of them,
2771440	2774080	you're completely free and you can enter Nirvana because you're done.
2775040	2780880	And actually, this is a good time to pause and say thank you to a friend of mine, Gustav
2780880	2786320	Sodastrom, who introduced me to your work. I want to give him a shout out. He's a brilliant guy.
2786320	2790560	And I think the AI community is actually quite amazing. And Gustav is a good representative
2790560	2794800	of that. You are as well. So I'm glad. First of all, I'm glad the internet exists,
2794800	2801200	the YouTube exists, where I can watch your talks and then get to your book and study your writing
2801200	2807680	and think about, you know, that's amazing. Okay, but you've kind of described instead of this
2807680	2812720	emergent phenomenon of consciousness from the simulation. So what about the hard
2812720	2822960	problem of consciousness? Can you just linger on it? Like, why does it still feel like I understand
2822960	2829040	you're kind of the self as an important part of the simulation? But why does the simulation feel
2829120	2834880	like something? So if you look at a book by, say, George R. R. Martin, where the characters
2834880	2840000	have plausible psychology and they stand on a hill because they want to conquer the city below the
2840000	2843600	hill and they're done in it and they look at the color of the sky and they are apprehensive
2844160	2847920	and feel empowered and all these things, why do they have these emotions? It's because it's
2847920	2851920	written into the story, right? And it's written into the story because it's an adequate model
2851920	2857120	of the person that predicts what they're going to do next. And the same thing has happened too
2857120	2860960	for us. So it's basically a story that our brain is writing, it's not written in words,
2860960	2867600	it's written in perceptual content, basically multimedia content. And it's a model of what
2867600	2873920	the person would feel if it existed. So it's a virtual person. And you and me happen to be
2873920	2878800	this virtual person. So this virtual person gets access to the language center and talks about
2878800	2885520	the sky being blue. And this is us. But hold on a second, do I exist in your simulation?
2886480	2893760	You do exist in an almost similar way as me. So there are internal states that are less accessible
2894640	2901200	for me that you have and so on. And my model might not be completely adequate. There are also
2901200	2905760	things that I might perceive about you that you don't perceive. But in some sense, both you and
2905760	2912160	me are some puppets, two puppets that enact this play in my mind. And I identify with one of them
2912160	2918240	because I can control one of the puppet directly. And with the other one, I can create things in
2918240	2922880	between. So for instance, we can go on an interaction that even leads to a coupling to a feedback
2922880	2928560	loop so we can sync things together in a certain way or feel things together. But this coupling is
2928560	2932880	itself not a physical phenomenon. It's entirely a software phenomenon. It's the result of two
2932880	2937920	different implementations interacting with each other. So that's interesting. So I used to just
2938880	2944880	like the way you think about it is the entirety of existence, the simulation and where kind of
2944880	2954160	each mind is a little sub simulation that like, why don't you, why doesn't your mind have access
2955520	2962640	to my mind's full state? Like for the same reason that my mind hasn't had access to its own full
2962640	2970000	state. So what I mean, there is no trick involved. So basically, when I say know something about
2970000	2974480	myself, it's because I made a model. Yes, part of your brain is tasked with modeling what other
2974480	2980960	parts of your brain are doing. Yes. But there seems to be an incredible consistency about this world
2980960	2985360	in the physical sense, that there's repeatable experiments and so on. Yeah. How does that
2986000	2991520	fit into our silly, the center of the ape's simulation of the world? So why is it so repeat
2991520	2996640	why is everything so repeatable? And not everything. There's a lot of fundamental physics
2996640	3004080	experiments that are repeatable for a long time all over the place and so on. The laws of physics.
3004080	3009440	How does that fit in? It seems that the parts of the world that are not deterministic are not long
3009440	3017120	lived. So if you build a system, any kind of automaton, so if you build simulations of something,
3017120	3022960	you'll notice that the phenomena that endure are those that give rise to stable dynamics.
3023520	3028080	So basically, if you see anything that is complex in the world, it's the result of usually of some
3028080	3032800	control of some feedback that keeps it stable around certain attractors. And the things that are
3032800	3037280	not stable that don't give rise to certain harmonic patterns and so on, they tend to get
3037280	3044080	weeded out over time. So if we are in a region of the universe that says stains complexity,
3044080	3049680	which is required to implement minds like ours, this is going to be a region of the universe
3049680	3054720	that is very tightly controlled and controllable. So it's going to have lots of interesting
3054720	3059280	symmetries and also symmetry breaks that allow to the creation of structure.
3060560	3065040	But they exist where? So there's such an interesting idea that our mind is simulation
3065040	3072960	that's constructing the narrative. My question is just to try to understand how that fits
3073920	3077920	with the entirety of the universe. You're saying that there's a region of this universe
3077920	3082160	that allows enough complexity to create creatures like us. But what's the connection between
3083360	3090400	the brain, the mind, and the broader universe? Which comes first? Which is more fundamental?
3090400	3095520	Is the mind the starting point, the universe is emergent? Is the universe the starting point,
3095520	3101600	the minds are emergent? I think quite clearly the latter. That's at least a much easier explanation
3101680	3106720	because it allows us to make causal models. And I don't see any way to construct an inverse
3106720	3110320	causality. So what happens when you die to your mind's simulation?
3111760	3117600	My implementation ceases. So basically the thing that implements myself will no longer be present,
3117600	3121680	which means if I am not implemented on the minds of other people, the thing that I identify with,
3122960	3128480	the weird thing is I don't actually have an identity beyond the identity that I construct.
3128480	3134560	If I was the Dalai Lama, he identifies as a form of government. So basically the Dalai Lama
3134560	3141200	gets reborn not because he's confused, but because he is not identifying as a human being.
3141760	3147120	He runs on a human being. He's basically a governmental software that is instantiated
3147120	3151280	in every new generation in you. So his advice is to pick someone who does this in the next
3151280	3157040	generation. So if you identify this, you are no longer a human and you don't die in the sense
3157440	3162240	that what dies is only the body of the human that you run on. To kill the Dalai Lama, you would
3162240	3167840	have to kill his tradition. And if we look at ourselves, we realize that we are to a small
3167840	3171760	part like this, most of us. So for instance, if you have children, you realize something lives on
3172480	3177040	in them, or if you spark an idea in the world, something lives on, or if you identify with
3177040	3181680	a society around you, because you are part that. You're not just this human being.
3181680	3187920	Yeah. So in a sense, you are kind of like a Dalai Lama. In a sense that you, Joshua Bach,
3187920	3193440	is just a collection of ideas. So you have this operating system on which a bunch of ideas live
3193440	3199120	and interact. And then once you die, some of them jump off the ship.
3199120	3203440	You put it the other way. Identity is a software state. It's a construction.
3203440	3209440	It's not physically real. Identity is not a physical concept. It's basically a representation
3209440	3217280	of different objects on the same world line. But identity lives and dies. Are you attached?
3218560	3223920	What's the fundamental thing? Is it the ideas that come together to form identity? Or is
3223920	3228080	each individual identity actually a fundamental thing? It's a representation that you can get
3228080	3233200	agency over if you care. So basically, you can choose what you identify with if you want to.
3233200	3244480	No, but it just seems, if the mind is not real, that the birth and death is not a crucial part
3244480	3256080	of it. Well, maybe I'm silly. Maybe I'm attached to this whole biological organism, but it seems
3256080	3263120	that the physical being a physical object in this world is an important aspect of birth and
3263120	3269120	death. It feels like it has to be physical to die. It feels like simulations don't have to die.
3270080	3274400	The physics that we experience is not the real physics. There is no color and sound in the
3274400	3279760	real world. Color and sound are types of representations that you get if you want to
3279760	3284480	model reality with oscillators. So colors and sound in some sense have octaves.
3284480	3289200	And it's because they are represented probably with oscillators. So that's why colors form a
3289200	3294800	circle of use. And colors have harmonics, sounds have harmonics as a result of synchronizing
3294800	3301200	oscillators in the brain. So the world that we subjectively interact with is fundamentally
3301200	3305520	the result of the representation mechanisms in our brain. They are mathematically, to some
3305520	3309680	degree universal. There are certain regularities that you can discover in the patterns and not
3309680	3314240	others. But the patterns that we get, this is not the real world. The world that we interact with
3314720	3319520	is always made of too many parts to count. So when you look at this table and so on,
3319520	3324800	it's consisting of so many molecules and atoms that you cannot count them. So you only look at
3324800	3331360	the aggregate dynamics at limit dynamics. If you had almost infinitely many particles,
3331360	3335520	what would be the dynamics of the table? And this is roughly what you get. So geometry that we are
3335520	3340800	interacting with is the result of discovering those operators that work in the limit that you
3340800	3346400	get by building an infinite series that converges. For those parts where it converges, it's geometry.
3346400	3348400	For those parts where it doesn't converge, it's chaos.
3349040	3356080	Right. And then so all of that is filtered through the consciousness that's emergent in our narrative.
3356080	3360080	So the consciousness gives it color, gives it feeling, gives it flavor.
3360720	3366800	So I think the feeling, flavor and so on is given by the relationship that a feature has to all the
3366800	3372800	other features. It's basically a giant relational graph that is our subjective universe. The color
3372800	3378720	is given by those aspects of the representation or this experiential color where you care about,
3378720	3382960	where you have identifications, where something means something, where you are the inside of a
3382960	3388400	feedback loop and the dimensions of caring are basically dimensions of this motivational system
3388400	3394880	that we emerge over. The meaning of the relations, the graph. Can you elaborate that a little bit?
3395600	3401840	Maybe you can even step back and ask the question of what is consciousness to be more
3401840	3409440	systematic? How do you think about consciousness? I think that consciousness is largely a model
3409440	3415200	of the contents of your attention. It's a mechanism that has evolved for a certain type of learning.
3415760	3421520	At the moment, our machine learning systems largely work by building chains of weighted
3421520	3428960	sums of real numbers with some nonlinearity. And you will learn by piping an error signal
3428960	3435840	through these different chain layers and adjusting the weights in these weighted sums.
3435840	3441120	And you can approximate most polynomials with this if you have enough training data.
3441120	3446720	But the price is you need to change a lot of these weights. Basically, the error is piped
3446800	3452160	backwards into the system until it accumulates at certain junctures in the network. And everything
3452160	3456160	else evens out statistically. And only at these junctures, this is where you had the actual error
3456160	3460960	in the network, you make the change there. This is a very slow process. And our brains don't have
3460960	3465200	enough time for that because we don't get old enough to play go the way that our machines
3465200	3470400	learn to play go. So instead, what we do is an attention-based learning. We pinpoint the probable
3470400	3476720	region in the network where we can make an improvement. And then we store this binding
3476720	3481520	state together with the expected outcome in a protocol. And this ability to make indexed
3481520	3488400	memories for the purpose of learning to revisit these commitments later, this requires a memory
3488400	3493600	of the contents of our attention. Another aspect is when I construct my reality and make mistakes.
3493600	3498880	So I see things that turn out to be reflections or shadows and so on, which means I have to be able
3498880	3504640	to point out which features of my perception gave rise to the present construction of reality.
3505200	3510480	So the system needs to pay attention to the features that are currently in its focus.
3511200	3515280	And it also needs to pay attention to whether it pays attention itself, in part because the
3515280	3519760	attentional system gets trained for the same mechanism, so it's reflexive, but also in part
3519760	3524640	because your attention lapses if you don't pay attention to the attention itself. So it's the
3524640	3530160	thing that I'm currently seeing just a dream that my brain has spun off into some kind of daydream
3530160	3535280	or am I still paying attention to my percept? So you have to periodically go back and see whether
3535280	3539520	you're still paying attention. And if you have this loop and you make it tight enough between the
3539520	3544160	system becoming aware of the contents of its attention and the fact that it's paying attention
3544160	3548160	itself and makes attention, the object of its attention, I think this is the loop over which
3548160	3554800	we wake up. So there's this, so there's this attentional mechanism that's somehow self-referential
3554800	3561120	that's fundamental to what consciousness is. So just ask you a question, I don't know how much
3561120	3565840	you're familiar with the recent breakthroughs in natural language processing. They use attentional
3565840	3574480	mechanism, they use something called transformers to learn patterns and sentences by allowing
3574480	3580160	them at work to focus its attention to particular parts of the sentence that each individual. So
3580160	3587120	like parametrize and make it learnable the dynamics of a sentence by having like a little window
3587120	3594960	into the sentence. Do you think that's like a little step towards that eventually would
3594960	3599600	take us to the intentional mechanisms from which consciousness can emerge?
3600240	3605920	Not quite. I think it models only one aspect of attention. In the early days of automated
3606560	3611440	language translation, there was an example that I found particularly funny, where somebody tried
3611440	3618080	to translate a text from English into German and it was a bet broke the window. And the
3618800	3625280	translation in German was in a Fliedermaus zerbracht das Fenster mit einem baseball Schläger.
3625280	3630960	So to translate back into English a bet, this flying mammal broke the window with a baseball
3630960	3636880	bet. And it seemed to be the most similar to this program because it somehow maximized
3637840	3643280	the possibility of translating the concept bet into German in the same sentence. And this is
3643280	3648320	some mistake that the transformer model is not doing because it's tracking identity. And the
3648320	3652480	attentional mechanism in the transformer model is basically putting its finger on individual
3652480	3659360	concepts and make sure that these concepts pop up later in the text and tracks basically the
3659360	3664720	individuals through the text. And this is why the system can learn things that other systems
3664720	3668960	couldn't before it, which makes it, for instance, possible to write a text where it talks about
3668960	3674240	the scientist, then the scientist has a name and has a pronoun and it gets a consistent story
3674240	3679040	about that thing. What it does not do, it doesn't fully integrate this. So this meaning falls apart
3679040	3684160	at some point, it loses track of this context. It does not yet understand that everything that it says
3684160	3689840	has to refer to the same universe. And this is where this thing falls apart. But the attention
3689840	3694480	in the transformer model does not go beyond tracking identity. And tracking identity is an
3694480	3700000	important part of attention, but it's a different, very specific attentional mechanism. And it's not
3700000	3704240	the one that gives rise to the type of consciousness that we have. Okay, just to link on, what do you
3704320	3709920	mean by identity in the context of language? So when you talk about language, you have different
3709920	3714960	words that can refer to the same concept. Got it. And in the sense that... So space of concepts.
3714960	3721360	So yes. And it can also be in a nominal sense or in an inexical sense that you say
3723040	3727200	this word does not only refer to this class of objects, but it refers to a definite object,
3727200	3733360	to some kind of agent that waves their way through the story and is only referred by
3733360	3739440	different ways in the language. So the language is basically a projection from a conceptual
3739440	3745520	representation from a scene that is evolving into a discrete string of symbols. And what
3745520	3751280	the transformer is able to do, it learns aspects of this projection mechanism that other models
3751280	3757040	couldn't learn. So have you ever seen an artificial intelligence or any kind of construction idea
3757040	3762320	that allows for unlike neural networks or perhaps within neural networks that's able to form
3763120	3769840	something where the space of concepts continues to be integrated? So what you're describing,
3769840	3776000	building a knowledge base, building this consistent, larger and larger sets of ideas
3776000	3781680	that would then allow for deeper understanding? Wittgenstein thought that we can build everything
3781680	3787440	from language, from basically a logical grammatical construct. And I think to some degree,
3787440	3792480	this was also what Minsky believed. So that's why he focused so much on common sense reasoning
3792480	3798880	and so on. And a project that was inspired by him was Psyche. That was basically-
3798880	3799680	That's still going on.
3799680	3803680	Yes. Of course, ideas don't die, only people die.
3805200	3807200	And that's true, but-
3807200	3811040	And Psyche is a productive project. It's just probably not one that is going to
3811760	3815520	converge to general intelligence. The thing that Wittgenstein couldn't solve,
3815520	3820160	and he looked at this in his book at the end of his life, Philosophical Investigations,
3820160	3825040	was the notion of images. So images play an important role in tractatus, the tractatus
3825040	3829440	in attempt to basically turn philosophy into logical probing language, to design a logical
3829440	3834800	language in which you can do actual philosophy that Wittgenhaft for doing this. And the difficulty
3834800	3841280	was to deal with perceptual content. And eventually, I think he decided that he was not able to solve
3841280	3847200	it. And I think this preempted the failure of the logitist program in AI. And the solution,
3847200	3851760	as we see it today, is we need more general function approximation. There are functions,
3851760	3857040	geometric functions, that we learn to approximate that cannot be efficiently expressed and computed
3857040	3861840	in a grammatical language. We can, of course, build automata that go via number theory and so
3861840	3868640	on and to learn in algebra and then compute an approximation of this geometry. But to equate
3868640	3874720	language and geometry is not an efficient way to think about it. So functional, you kind of just
3874720	3879920	said that neural networks are sort of the approach that neural networks takes is actually more general
3879920	3888560	than what can be expressed through language. Yes. So what can be efficiently expressed
3888560	3892320	through language at the data rates at which we process grammatical language?
3892320	3896960	Okay, so you don't think languages, so you disagree with Wittgenstein,
3896960	3903040	that language is not fundamental to? I agree with Wittgenstein. I just agree with the late Wittgenstein.
3903760	3909600	And I also agree with the beauty of the early Wittgenstein. I think that the tractatus itself
3909600	3913120	is probably the most beautiful philosophical text that was written in the 20th century.
3914000	3918560	But language is not fundamental to cognition and intelligence and consciousness.
3918560	3923520	So I think that language is a particular way or the natural language that we're using is a
3923520	3928960	particular level of abstraction that we use to communicate with each other. But the languages in
3928960	3935040	which we express geometry are not grammatical languages in the same sense. So they work slightly
3935040	3940480	different. They're more general expressions of functions. And I think the general nature of a
3940480	3946880	model is you have a bunch of parameters. These have a range. These are the variances of the world.
3946880	3951200	And you have relationships between them, which are constraints, which say if certain parameters
3951200	3958080	have these values, then other parameters have to have the following values. And this is a very
3958080	3962800	early insight in computer science. And I think some of the earliest formulations is the Boltzmann
3962800	3967280	machine. And the problem with the Boltzmann machine is that while it has a measure of whether it's
3967280	3971120	good, it's basically the energy on the system, the amount of tension that you have left in the
3971120	3976640	constraints where the constraints don't quite match. It's very difficult to, despite having this
3976640	3982720	global measure, to train it. Because as soon as you add more than trivially few elements,
3982720	3986880	parameters into the system, it's very difficult to get it settled in the right architecture.
3987760	3994880	And so the solution that Hinton and Zanovsky found was to use a restricted Boltzmann machine,
3994880	3999760	which uses the hidden links, the internal links in the Boltzmann machine and only has
3999760	4004960	basically input and output layer. But this limits the expressivity of the Boltzmann machine. So now
4004960	4009360	he builds a network of small of these primitive Boltzmann machines. And in some sense, you can
4009360	4013840	see a almost continuous development from this to the deep learning models that we're using today,
4014640	4018640	even though we don't use Boltzmann machines at this point. But the idea of the Boltzmann machine
4018640	4023120	is you take this model, you clamp some of the values to perception. And this forces the entire
4023120	4026960	machine to go into a state that is compatible with the states that you currently perceive.
4026960	4033200	And this state is your model of the world. I think it's a very general way of thinking about
4033200	4039920	models. But we have to use a different approach to make it work. And this is, we have to find
4039920	4043440	different networks that train the Boltzmann machine. So the mechanism that trains the
4043440	4047680	Boltzmann machine and the mechanism that makes the Boltzmann machine settle into its state
4048320	4051760	are distinct from the constrained architecture of the Boltzmann machine itself.
4053840	4056400	The kind of mechanism that we want to develop, you're saying?
4056400	4060320	Yes. So there's the direction in which I think our research is going to go,
4060880	4066560	is going to, for instance, what you notice in perception is our perceptual models of the world
4066560	4071520	are not probabilistic, but possibilistic, which means you should be able to perceive things that
4071520	4077920	are improbable, but possible, right? Perceptual state is valid, not if it's probable, but if it's
4077920	4083200	possible, if it's coherent. So if you see a tiger coming after, you should be able to see this,
4083200	4089760	even if it's unlikely. And the probability is necessary for convergence of the model. So
4089760	4095600	given the state of possibilities that is very, very large and a set of perceptual features,
4095600	4101040	how should you change the states of the model to get it to convert with your perception?
4101920	4110560	But the space of ideas that are coherent with the context that you're sensing is perhaps not
4110560	4116960	as large. I mean, that's perhaps pretty small. The degree of coherence that you
4117040	4121600	need to achieve depends, of course, how deep your models go. That is, for instance, politics is
4121600	4126400	very simple when you know very little about game theory and human nature. So the younger you are,
4126400	4132320	the more obvious it is how politics should work, right? Because you get a coherent aesthetic from
4132320	4137920	relatively few inputs. And the more layers you model, the more layers you model reality,
4137920	4145600	the harder it gets to satisfy all the constraints. So the current neural networks are fundamentally
4145600	4150640	supervised learning system with a feed forward neural network is back propagation to learn.
4150640	4155360	What's your intuition about what kind of mechanisms might we move towards to improve
4156160	4161920	the learning procedure? I think one big aspect is going to be meta learning and architecture
4161920	4168160	search starts in this direction. In some sense, the first wave of AI classical AI work by identifying
4168160	4172240	a problem and the possible solution and implementing the solution right program that plays chess.
4172960	4177920	And right now we are in the second wave of AI. So instead of writing the algorithm that
4177920	4183040	implements the solution, we write an algorithm that automatically searches for an algorithm
4183040	4187760	that implements the solution. So the learning system in some sense is an algorithm that
4187760	4192320	itself discovers the algorithm that solves the problem, like go. Go is too hard to implement
4192320	4196320	it by the solution by hand, but we can implement an algorithm that finds the solution.
4197520	4200960	Let's move to the third stage, right? The third stage would be meta learning.
4200960	4205120	Find an algorithm that discovers the learning algorithm for the given domain.
4205120	4208080	Our brain is probably not a learning system, but a meta learning system.
4208720	4213840	This is one way of looking at what we are doing. There is another way. If you look at the way our
4213840	4218240	brain is, for instance, implemented, there is no central control that tells all the neurons how
4218240	4224000	to wire up. Instead, every neuron is an individual reinforcement learning agent. Every neuron is
4224000	4228240	a single celled organism that is quite complicated and in some sense quite motivated to get fed.
4229200	4236160	It gets fed if it fires on average at the right time. The right time depends on the
4236160	4242160	context that the neuron exists in, which is the electrical and chemical environment that it has.
4242160	4248320	So it basically has to learn a function over its environment that tells us when to fire to get fed.
4248320	4252240	Or if you see it as a reinforcement learning agent, every neuron is in some sense
4252240	4257040	making a hypothesis when it sends a signal and tries to pipe a signal through the universe
4257040	4262000	and tries to get positive feedback for it. The entire thing is set up in such a way that it's
4262000	4267280	robustly self-organizing into a brain, which means you start out with different neuron types
4267280	4272960	that have different priors on which hypothesis to test and how to get its reward. You put them
4272960	4278240	into different concentrations in a certain spatial alignment, and then you entrain it
4278240	4281680	in a particular order. As a result, you get a well-organized brain.
4282640	4289520	Okay, so the brain is a meta-learning system with a bunch of reinforcement learning agents.
4293200	4300640	I think you said, but just to clarify, there's no centralized government that tells you
4301200	4308400	here's a loss function, here's a loss function, here's a loss function. Who says what's the
4308640	4311760	objective of what's going on? There are also governments which impose loss functions on
4311760	4315520	different parts of the brain. So we have differential attention. Some areas in your brain
4315520	4320400	get especially rewarded when you look at faces. If you don't have that, you will get prosopagnosia,
4320400	4324240	which basically means the inability to tell people apart by their faces.
4325840	4330400	And the reason that happens is because it had an evolutionary advantage. So evolution comes
4330400	4335280	into play here. But it's basically an extraordinary attention that we have for faces. I don't think
4335360	4340400	that people with a prosopagnosia have a perceived defective brain. The brain just has an average
4340400	4345120	attention for faces. So people with prosopagnosia don't look at faces more than they look at cups.
4345680	4350720	So the level at which they resolve the geometry of faces is not higher than the one for cups.
4351280	4356560	And people that don't have prosopagnosia look obsessively at faces. For you and me,
4356560	4361200	it's impossible to move through a crowd without scanning the faces. And as a result,
4361200	4365200	we make insanely detailed models of faces that allow us to discern mental states of people.
4365520	4372000	So obviously, we don't know 99% of the details of this meta-learning system that's our mind.
4374000	4380000	But still, we took a leap from something much dumber to that from through the evolutionary
4380000	4387360	process. Can you, first of all, maybe say how big of a leap is that from our brain,
4388320	4396720	from our ape ancestors to multi-cell organisms? And is there something we can think about
4399120	4402800	as we start to think about how to engineer intelligence? Is there something we can learn
4402800	4410880	from evolution? In some sense, life exists because of the market opportunity of controlled chemical
4410880	4416720	reactions. We compete with dump chemical reactions and we win in some areas against this dump
4416720	4421040	combustion because we can harness those entropy gradients where you need to add a little bit
4421040	4425600	of energy in a specific way to harvest more energy. So we all competed combustion?
4425600	4430160	Yes, in many regions we do. We try very hard because when we are in direct competition,
4430160	4436560	we lose. Because the combustion is going to close the entropy gradients much faster than we can
4436560	4444160	run. So basically, we do this because every cell has a Turing machine built into it.
4444880	4451440	It's like literally a read-write head on a tape. So everything that's more complicated than a
4451440	4458080	molecule that just is a vortex around the tractors that needs a Turing machine for its
4458080	4463840	regulation. And then you bind cells together and you get next-level organizational organism where
4463840	4470640	the cells together implement some kind of software. And for me, a very interesting discovery in the
4470640	4475760	last year was the word spirit because I realized that what spirit actually means is an operating
4475760	4480720	system for an autonomous robot. And when the word was invented, people needed this word.
4480720	4484720	But they didn't have robots that they built themselves. Yet the only autonomous robots
4484720	4489280	that were known were people, animals, plants, ecosystems, cities, and so on. And they all
4489280	4494240	had spirits. And it makes sense to say that the plant is an operating system. If you pinch the
4494240	4498960	plant in one area, then this is going to have repercussions throughout the plant. Everything
4498960	4503600	in the plant is in some sense connected into some global aesthetics like in other organisms.
4503600	4508560	An organism is not a collection of cells. It's a function that tells cells how to behave.
4509200	4513600	And this function is not implemented as some kind of supernatural thing,
4514240	4519360	like some morphogenetic field. It is an emergent result of the interactions of each
4519360	4528000	cell with each other's cell. So what you're saying is the organism is a function that tells what
4530560	4537200	to do. And the function emerges from the interaction of the cells.
4538640	4543520	So it's basically a description of what the plant is doing in terms of macro states.
4543920	4548800	And the micro states, the physical implementation are too many of them to describe them. So the
4549520	4554320	software that we use to describe what the plant is doing, the spirit of the plant is the software,
4554320	4560160	the operating system of the plant, right? This is a way in which we, the observers, make sense of
4560160	4564960	the plant. And the same is true for people. So people have spirits, which is their operating
4564960	4569680	system in a way, right? And there are aspects of that operating system that relate to how your
4569680	4573680	body functions and others, how you socially interact, how you interact with yourself and so
4573680	4580160	on. And we make models of that spirit. And we think it's a loaded term because it's from a
4580160	4586160	pre-scientific age. But it took the scientific age a long time to rediscover a term that is
4586160	4591040	pretty much the same thing. And I suspect that the differences that we still see between the old
4591040	4594400	word and the new word are translation errors that have been over the centuries.
4595120	4599760	Can you actually linger on that? Like, why do you say that spirit, just to clarify,
4599760	4605120	because I'm a little bit confused. So the word spirit is a powerful thing. But why did you say
4605120	4610160	in the last year or so that you discovered this? Do you mean the same old traditional idea of a
4610160	4615760	spirit? Or do you mean? I try to find out what people mean by spirit. When people say spirituality
4615760	4620240	in the US, it usually refers to the phantom limb that they develop in the absence of culture.
4620960	4626320	And a culture is in some sense, you could say the spirit of a society that is long game.
4627200	4632720	This thing that becomes self-aware at a level above the individuals where you say,
4632720	4637040	if you don't do the following things, then the grand-grand-grand-grandchildren of our children
4637040	4642400	will not have nothing to eat. So if you take this long scope where you try to maximize the
4642400	4646320	length of the game that you are playing as a species, you realize that you're part of a
4646400	4649600	larger thing that you cannot fully control. You probably need to submit to the
4650240	4656880	ecosphere instead of trying to completely control it. There needs to be a certain level at which
4656880	4662880	we can exist as a species if you want to endure. And our culture is not sustaining this anymore.
4662880	4666640	We basically made this bet with the Industrial Revolution that we can control everything.
4666640	4672400	And the modernist societies with basically unfettered growth led to a situation in which
4672480	4678880	we depend on the ability to control the entire planet. And since we are not able to do that,
4678880	4684240	as it seems, this culture will die. And we realize that it doesn't have a future,
4684240	4689280	right? We called our children generations that it's such a very optimistic thing to do.
4690480	4696320	Yeah, so you have this kind of intuition that our civilization, you said culture,
4696320	4702800	but you really mean the spirit of the civilization, the entirety of the civilization
4703680	4710160	may not exist for long. Can you untangle that? What's your intuition behind that?
4710160	4716320	So you kind of offline mentioned to me that the Industrial Revolution was kind of the moment
4716320	4724240	we agreed to accept the offer, sign on the paper, on the dotted line with the Industrial Revolution,
4724240	4729200	we doomed ourselves. Can you elaborate on that? This is suspicion. I of course don't know how it
4729200	4737840	plays out, but it seems to me that in a society in which you leverage yourself very far over
4737840	4742800	an entropic abyss without land on the other side, it's relatively clear that your cantilever is at
4742800	4748320	some point going to break down into this entropic abyss. And you have to pay the bill.
4748320	4756800	Okay, Russia is my first language, and I'm also an idiot. Me too. This is just two apes,
4758080	4763440	instead of playing with the banana trying to have fun by talking. Okay,
4764160	4770320	anthropic what? And what's anthropic? Entropic. Entropic. So entropic in the sense of entropy.
4770320	4774400	Oh, entropic, guys. Yes. And entropic, what was the other word you used? Abyss.
4775120	4781600	What's that? It's a big quart. Oh, abyss. Abyss, yes. Entropic abyss. So many of the things you
4781600	4789120	say are poetic, it's parting my brain. It's amazing, right? It's mispronounced, which makes
4789120	4798960	you do more poetic. Wittgenstein would be proud. So entropic abyss. Okay, let's rewind then. The
4799040	4803680	industrial revolution, so how does that get us into the entropic abyss?
4805280	4810000	So in some sense, we burned 100 million years worth of trees to get everybody plumbing.
4810560	4815600	Yes. And the society that we had before that had a very limited number of people. So basically,
4815600	4824000	since zero BC, we hovered between 300 and 400 million people. And this only changed with
4824000	4830080	the Enlightenment and the subsequent industrial revolution. And in some sense, the Enlightenment
4830080	4835440	freed our rationality and also freed our norms from the preexisting order gradually.
4835440	4840080	It was a process that basically happened in feedback loops, so it was not that just one
4840080	4846080	caused the other. It was a dynamic that started. And the dynamic worked by basically increasing
4846080	4853520	productivity to such a degree that we could feed all our children. And I think the definition of
4854560	4858880	poverty is that you have as many children as you can feed before they die,
4859440	4862480	which is in some sense the state that all animals on earth are in.
4863920	4866320	The definition of poverty is having enough...
4866320	4869920	So you can have only so many children as you can feed and if you have more, they die.
4870720	4874960	And in our societies, you can basically have as many children as you want and they don't die.
4876000	4881200	Right. So the reason why we don't have as many children as we want is because we also have to
4881200	4886240	pay a price in terms of we have to insert ourselves in the lower source of dread if we have too many.
4886240	4892320	So basically everybody in the under, middle and lower upper class has only a limited number of
4892320	4898000	children because having more of them would mean a big economic hit to their individual families
4898000	4903120	because children, especially in the US, super expensive to have. And you only are taken out of
4903120	4907440	this if you are basically super rich or if you are super poor. If you are super poor, it doesn't
4907440	4911920	matter how many kids you have because your status is not going to change and these children are
4911920	4918400	largely not going to die of hunger. So how does this lead to self-destruction? So there's a lot of
4918400	4923040	unpleasant properties about this process. So basically what we try to do is we try to let
4923040	4932080	our children survive even if they have diseases. Like I would have died before my mid-20s without
4932080	4936560	modern medicine and most of my friends would have as well. And so many of us wouldn't live
4937120	4944160	without the advantages of modern medicine and modern industrialized society. We get our protein
4944160	4950240	largely by sub-doing the entirety of nature. Imagine there would be some very clever microbe
4950240	4956880	that would live in our organisms and would completely harvest them and change them into
4956960	4962720	a thing that is necessary to sustain itself. And it would discover that, for instance,
4962720	4967520	brain cells are kind of edible, but they're not quite nice. So you need to have more fat in them
4967520	4972480	and you turn them into more fat cells. And basically this big organism would become a vegetable
4972480	4976560	that is barely alive and it's going to be very brittle and not resilient when the environment
4976560	4982240	changes. Yeah, but some part of that organism, the one that's actually doing all the using of the
4983200	4988880	that there'll still be somebody thriving. So it relates back to this original question.
4988880	4994160	I suspect that we are not the smartest thing on this planet. I suspect that basically every
4994160	5001520	complex system has to have some complex regulation if it depends on feedback loops. And so for
5001520	5006880	instance, it's likely that we should describe a certain degree of intelligence to plants.
5007840	5012160	The problem is that plants don't have a nervous system. So they don't have a way to telegraph
5012160	5017840	messages over large distances almost instantly in the plant. And instead, they will rely on
5017840	5023440	chemicals between adjacent cells, which means the signal processing speed depends on the signal
5023440	5029920	processing with the rate of a few millimeters per second. And as a result, if the plant is
5029920	5035920	intelligent, it's not going to be intelligent at similar timescales. The time scale is different.
5035920	5042560	So you suspect we might not be the most intelligent, but we're the most intelligent
5042560	5048160	and the spatial scale and our time scale. So basically, if you would zoom out very far,
5048160	5053520	we might discover that there have been intelligent ecosystems on the planet that existed for
5053520	5058480	thousands of years in a almost undisturbed state. And it could be that these ecosystems
5058480	5063200	actively related their environment. So basically, change the course of the evolution vision,
5063200	5065840	this ecosystem to make it more efficient and less brittle.
5065840	5070640	So it's possible something like plants is actually a set of living organisms,
5070640	5075200	an ecosystem of living organisms that are just operating a different time scale and are far
5075200	5079760	superior in intelligence to human beings. And then human beings will die out and plants will
5079760	5085520	still be there and they'll be there. Yeah, there's an evolutionary adaptation playing a
5085520	5090080	role at all of these levels. For instance, if mice don't get enough food and get stressed,
5090160	5095200	the next generation of mice will be more sparse and more scrawny. And the reason for this is
5095200	5100480	because in a natural environment, the mice have probably hidden a drought or something else.
5100480	5105840	And if they're overgrays, then all the things that sustain them might go extinct. And there will be
5105840	5110560	no mice a few generations from now. So to make sure that there will be mice in five generations
5110560	5116000	from now, basically the mice scale back. And a similar thing happens with the predators of mice.
5116000	5120240	They should make sure that the mice don't completely go extinct. So in some sense,
5120240	5125520	if the predators are smart enough, they will be tasked with shepherding their food supply.
5126640	5130320	And maybe the reason why lions have much larger brains than antelopes is not so much
5130320	5136320	because it's so hard to catch an antelope as opposed to run away from the lion. But the lions
5136320	5140560	need to make complex models of their environment, more complex than the antelopes.
5140560	5145520	So first of all, just describing that there's a bunch of complex systems and human beings may
5145520	5149440	not even be the most special or intelligent of those complex systems, even on earth,
5150240	5154080	makes me feel a little better about the extinction of human species that we're talking about.
5154080	5157440	Yes, maybe we adjust Gaia's ploy to put the carbon back into the atmosphere.
5157440	5160080	Yeah, this is just a nice, we tried it out.
5160080	5165120	The big stain on evolution is not as it was trees. First evolved trees before they could
5165120	5168640	be digested again, right? There were no insects that could break all of them apart.
5169520	5175040	Cellulose is so robust that you cannot get all of it with microorganisms. So many of these trees
5175040	5180480	fell into swamps and all this carbon became inert and could no longer be recycled into organisms.
5180480	5183520	And we are the species that is destined to take care of that.
5183520	5184400	So this is kind of...
5185760	5189840	To get out of the ground, put it back into the atmosphere and the earth is already greening.
5189840	5195280	So within a million years or so when the ecosystems have recovered from the rapid changes
5195280	5199200	that they're not compatible with right now, this is going to be awesome again.
5199200	5202000	And there won't be even a memory of us little apes.
5202000	5206240	I think there will be memories of us. I suspect we are the first general intelligent species in
5206240	5210720	this sense. We are the first species with an industrial society because we will leave more
5210720	5215920	phones than bones in the stratosphere. Well, see, phones than bones, I like it.
5216720	5222560	But then let me push back. You've kind of suggested that we have a very narrow definition of
5223840	5229520	I mean, why aren't trees more general, a higher level of general intelligence?
5229600	5233120	If trees were intelligent, then they would be at different time scales,
5233120	5237040	which means within a hundred years, the tree is probably not going to make models that are as
5237040	5242240	complex as the ones that we make in 10 years. But maybe the trees are the ones that made the
5242240	5250480	phones, right? You could say the entirety of life did it. The first cell never died.
5250480	5254720	The first cell only split, right? And every divided. And every cell in our body is still
5255360	5259360	an instance of the first cell that split off from that very first cell. There was only one cell
5259360	5264480	on this planet as far as we know. And so the cell is not just a building block of life.
5264480	5268000	It's a hypoorganism, right? And we are part of this hypoorganism.
5269600	5276160	So nevertheless, this hyperorganism, no, this little particular branch of it,
5276160	5281200	which is us humans, because of the industrial revolution and maybe the exponential growth of
5281200	5287040	technology might somehow destroy ourselves. So what do you think is the most likely
5287040	5292320	way we might destroy ourselves? So some people worry about genetic manipulation. Some people,
5292320	5297920	as we've talked about, worry about either dumb artificial intelligence or super intelligent
5297920	5305120	artificial intelligence destroying us. Some people worry about nuclear weapons and weapons of war
5305120	5309680	in general. What do you think? If you had to, if you were a betting man, what would you bet on in
5309680	5314880	terms of self-destruction? And they would be higher than 50, would it be higher than 50 percent?
5314880	5321280	So it's very likely that nothing that we bet on matters after we win our bets. So I don't think
5321280	5326480	that bets are literally the right way to go about this. I mean, once you're dead, you won't be there
5326480	5332640	to collect the weighings. So it's also not clear if we as a species go extinct. But I think that
5332640	5337200	our present civilization is not sustainable. So the thing that will change is there will be probably
5337200	5342480	fewer people on the planet than there are today. And even if not, then still most of people that
5342560	5347280	are alive today will not have offspring in 100 years from now because of the geographic changes
5347280	5352960	and so on and the changes in the food supply. It's quite likely that many areas of the planet
5352960	5357920	will only be livable with a close cooling chain in 100 years from now. So many of the areas around
5357920	5365440	the equator and in sub-tropical climates that are now quite pleasant to live in will stop to be
5365440	5371200	inhabitable without air conditioning. So you honestly, wow, cooling chain, close-knit cooling
5371200	5378080	chain communities. So you think you have a strong worry about the effects of global warming?
5378080	5382560	By itself, it's not the big issue. If you live in Arizona right now, you have basically three
5382560	5387440	months in the summer in which you cannot be outside. And so you have a close cooling chain,
5387440	5391200	you have air conditioning in your car and in your home and you're fine. And if the air conditioning
5391200	5397600	would stop for a few days, then in many areas you would not be able to survive. Can we just pause
5397600	5404080	for a second? You say so many brilliant, poetic things. What is a close, do people use that term
5404080	5410080	close cooling chain? I imagine that people use it when they describe how they get meat into a
5410080	5414720	supermarket. If you break the cooling chain and this thing starts to thaw, you're in trouble and
5414720	5422080	you have to sew it away. That's such a beautiful way to put it. It's like calling a city a closed
5422080	5426400	social chain or something like that. That's right. The locality of it is really important.
5426480	5430240	It basically means you wake up in a climatized room, you go to work in a climatized car,
5430240	5434720	you work in a climatized office, you shop in a climatized supermarket. And in between,
5434720	5438800	you have very short distance in which you run from your car to the supermarket. But you have
5438800	5443680	to make sure that your temperature does not approach the temperature of the environment.
5443680	5448080	The crucial thing is the wet bulb temperature. The wet bulb temperature. It's what you get
5448080	5454960	when you take a wet clothes and you put it around your thermometer and then you will move it very
5454960	5462560	quickly through the air so you get the evaporation heat. And as soon as you can no longer cool your
5462560	5469040	body temperature via evaporation to a temperature below something like I think 35 degrees, you die.
5471360	5476480	Which means if the outside world is dry, you can still cool yourself down by sweating,
5476480	5480720	but if it has a certain degree of humidity or if it goes over a certain temperature,
5480720	5486320	then sweating will not save you. And this means even if you're a healthy, fit individual within
5486320	5491520	a few hours, even if you try to be in the shade and so on, you'll die. Unless you have some
5491520	5497440	climatizing equipment. And this itself, if as long as you maintain civilization and you have
5497440	5501920	energy supply and you have food trucks coming to your home that are climatized, everything is fine.
5501920	5506640	But what if you lose a large-scale open agriculture at the same time? So basically,
5506720	5511440	you run into food insecurity because climate becomes very irregular or weather becomes very
5511440	5518240	irregular and you have a lot of extreme weather events. So you need to roll most of your food maybe
5518240	5523760	indoor or you need to import your food from certain regions. And maybe you're not able to maintain
5523760	5528640	the civilization throughout the planet to get the infrastructure to get the food to your home.
5529520	5533920	But there could be significant impacts in the sense that people begin to suffer.
5533920	5539040	There could be wars over resources and so on. But ultimately, do you not have
5540000	5546880	not a faith, but what do you make of the capacity of technological innovation
5547520	5556640	to help us prevent some of the worst damages that this condition can create? So as an example,
5557440	5562240	as a almost out there example is the work that SpaceX and Elon Musk is doing of trying to
5563200	5570560	also consider our propagation throughout the universe in deep space to colonize other planets.
5570560	5575680	That's one technological step. But of course, what Elon Musk is trying on Mars is not to
5575680	5580720	save us from global warming because Mars looks much worse than Earth will look like after the
5580720	5586960	worst outcomes of global warming imaginable, right? Mars is essentially not habitable.
5586960	5591360	It's exceptionally harsh environment, yes. But what he is doing, what a lot of people
5591360	5595760	throughout history since the Industrial Revolution are doing, are just doing a lot of different
5595760	5600320	technological innovation with some kind of target. And when ends up happening is totally
5600320	5607440	unexpected new things come up. So trying to terraform or trying to colonize Mars,
5607440	5615040	extremely harsh environment, might give us totally new ideas of how to expand or increase the power
5615040	5624160	of this closed cooling circuit that empowers the community. So it seems like there's a little bit
5624160	5633760	of a race between our open-ended technological innovation of this communal operating system
5633760	5642560	that we have and our general tendency to want to overuse resources and thereby destroy ourselves.
5643440	5645360	You don't think technology can win that race?
5646160	5651600	I think the probability is relatively low given that our technology is, for instance,
5651600	5657040	the U.S. is stagnating since the 1970s roughly. In terms of technology, most of the things that
5657040	5662240	we do are the result of incremental processes. What about Intel? What about Moore's law?
5662240	5667360	It's basically, it's very incremental. The things that we are doing is, so the invention of the
5667360	5676480	microprocessor was a major thing. The miniaturization of transistors was really major. But the things
5676480	5684480	that we did afterwards largely were not that innovative. We had gradual changes of scaling
5684480	5691840	things from CPUs into GPUs and things like that. But I don't think that there are,
5692800	5697040	basically there are not many things if you take a person that died in the 70s and was at the top
5697040	5701280	of their game, they would not need to read that many books to be current again.
5701280	5706800	But it's all about books. Who cares about books? There might be things that are beyond books might
5706800	5710960	be a very... Or say papers or... No, papers. Forget papers. There might be things that are,
5710960	5716080	so papers and books and knowledge, that's a concept of a time when you were sitting there
5716080	5720800	by candlelight and individual consumers of knowledge. What about the impact that we're not
5720800	5726240	in the middle of? We're not, might not be understanding of Twitter, of YouTube. The reason
5726240	5733280	you and I are sitting here today is because of Twitter and YouTube. So the ripple effect,
5733280	5739760	and there's two minds, sort of two dumb apes are coming up with a new, perhaps a new clean
5739760	5745280	insights. And there's 200 other apes listening right now, 200,000 other apes listening right now.
5745360	5751200	And that effect, it's very difficult to understand what that effect will have. That might be bigger
5751200	5756160	than any of the advancement of the microprocessor or the industrial revolution, the ability of
5756160	5768400	spread knowledge. And that knowledge, it allows good ideas to reach millions much faster. And the
5768400	5774720	effect of that, that might be the new, that might be the 21st century, is the multiplying of ideas.
5775520	5782480	Because if you say one good thing today, that will multiply across huge amounts of people. And
5782480	5786000	then they will say something, and then they will have another podcast, and they'll say something,
5786000	5789680	and then they'll write a paper. That could be a huge, you don't think that...
5789680	5795760	Yeah, we should have billions of von Neumanns right now in two rings, and we don't for some reason.
5796400	5800720	I suspect the reason is that we destroy our attention span. Also the incentives, of course,
5800880	5808240	different. So the reason why we are sitting here and doing this as a YouTube video is because you
5808240	5812240	and me don't have the attention span to write a book together right now, and you guys probably
5812240	5815840	don't have the attention span to read it. So let me tell you... But I guarantee you they're still
5815840	5822400	listening. You can't burst care of your attention. It's very short. But we're an hour and 40 minutes
5822400	5827280	in, and I guarantee you that 80% of the people are still listening. So there is an attention span,
5827280	5833200	it's just the form. Who said that the book is the optimal way to transfer information?
5833200	5835760	That's still an open question. I mean, that's what we're...
5835760	5839360	There's something that social media could be doing, that other forms could not be doing.
5839360	5843680	I think the end game of social media is a global brain. And Twitter is, in some sense,
5843680	5847440	a global brain that is completely hooked on dopamine, doesn't have any kind of inhibition,
5847440	5853120	and as a result is caught in a permanent seizure. It's also, in some sense, a multi-player role
5853120	5858800	playing game. And people use it to play an avatar that is not like them as they were in the
5858800	5862160	sane world, and they look through the world through the lens of their phones and think it's
5862160	5866240	the real world. But it's the Twitter world that is distorted by the popularity incentives of Twitter.
5867280	5873200	Yeah, the incentives and just our natural biological, the dopamine rush of a like,
5873840	5882640	no matter how... I try to be very kind of zen-like and minimalist and not be influenced by likes and
5882640	5889280	so on, but it's probably very difficult to avoid that to some degree. Speaking of a small tangent
5889280	5898160	of Twitter, how can Twitter be done better? I think it's an incredible mechanism that has a huge
5898160	5904240	impact on society by doing exactly what you're doing. Sorry, doing exactly what you described,
5904320	5911440	which is having this... We're like, this is some kind of game, and we're kind of
5911440	5915280	individual RL agents in this game, and it's uncontrollable because there's not really a
5915280	5920880	centralized control. Neither Jack Dorsey nor the engineers at Twitter seem to be able to control
5920880	5929440	this game. Or can they? That's sort of a question. Is there any advice you would give on how to control
5929440	5933040	this game? I wouldn't give advice because I am certainly not an expert, but I can give my thoughts
5933040	5940720	on this. Our brain has solved this problem to some degree. Our brain has lots of individual
5940720	5945200	agents that manage to play together in a way, and they have also many contexts in which other
5945200	5951920	organisms have found ways to solve the problems of cooperation that we don't solve on Twitter.
5952960	5959600	Maybe the solution is to go for an evolutionary approach. Imagine that you have something like
5959600	5965040	Reddit or something like Facebook and something like Twitter, and you think about what they have
5965040	5969200	in common. What they have in common? They are companies that in some sense own a protocol.
5970480	5976880	This protocol is imposed on a community, and the protocol has different components for monetization,
5976880	5981760	for user management, for user display, for rating, for anonymity, for import of other content,
5981760	5987040	and so on. Now imagine that you take these components of the protocol apart, and you
5987120	5993600	do it in some sense like communities, visit this social network, and these communities are allowed
5993600	5999520	to mix and match their protocols and design new ones. For instance, the UI and the UX can be
5999520	6004640	defined by the community. The rules for sharing content across communities can be defined. The
6004640	6010000	monetization can be redefined. The way you reward individual users for what can be redefined,
6010000	6014160	the way users can represent themselves and to each other can redefine.
6014880	6020000	Who could be the redefiner? Can individual human beings build enough intuition to redefine those
6020000	6024160	things? This itself can become part of the protocol. For instance, it could be in some
6024160	6028720	communities, it will be a single person that comes up with these things, and others, it's a group of
6028720	6033920	friends. Some might implement a voting scheme that has some interesting weighted voting. Who knows?
6033920	6036720	Who knows what will be the best self-organizing principle for this?
6036720	6039840	But the process can't be automated. I mean, it seems like the brain...
6040480	6045360	Who can be automated so people can write software for this? And eventually, the idea is,
6046000	6050320	let's not make an assumption about this thing if you don't know what the right solution is. In
6050320	6055600	those areas, we have no idea whether the right solution will be people designing this ad hoc
6055600	6061360	or machines doing this, whether you want to enforce compliance by social norms like Wikipedia,
6061360	6066560	or with software solutions, or with AI that goes through the posts of people, or with a
6066560	6072240	legal principle, and so on. This is something maybe you need to find out. And so the idea would
6072240	6078640	be if you let the communities evolve, and you just control it in such a way that you are incentivizing
6078640	6085760	the most sentient communities, the ones that produce the most interesting behaviors that allow
6085760	6090960	you to interact in the most helpful ways to the individuals. You have a network that gives you
6090960	6095680	information that is relevant to you. It helps you to maintain relationships to others in healthy
6095680	6100400	ways. It allows you to build teams. It allows you to basically bring the best of you into
6100400	6104720	this thing and goes into a coupling, into a relationship with others in which you produce
6104720	6111600	things that you would be unable to produce alone. Yes, beautifully put. But the key process of that
6111600	6120560	with incentives and evolution is things that don't adopt themselves to effectively get the
6120560	6126880	incentives have to die. And the thing about social media is communities that are unhealthy,
6126880	6131920	or whatever you want to define as the incentives, really don't like dying. One of the things that
6131920	6137840	people really get agressive, protest aggressively is when they're censored, especially in America.
6137840	6143760	I don't know much about the rest of the world, but the idea of freedom of speech, the idea of
6143760	6154320	censorship is really painful in America. And so what do you think about that,
6154880	6162000	having grown up in East Germany? Do you think censorship is an important tool in our brain,
6162000	6169440	in the intelligence, and in the social networks? So basically, if you're not a good member
6170400	6175280	of the entirety of the system, then you should be blocked away, well, locked away, blocked.
6176480	6180400	An important thing is who decides that you're a good member. Who? Is it distributed?
6180400	6183520	And what is the outcome of the process that decides it,
6184160	6189040	both for the individual and for society at large? For instance, if you have a high trust
6189040	6194080	society, you don't need a lot of surveillance. And the surveillance is even, in some sense,
6194160	6200880	undermining trust, because it's basically punishing people that look suspicious when
6200880	6206560	surveyed, but do the right thing anyway. And the opposite, if you have a low trust society,
6206560	6210720	then surveillance can be a better trade-off. And the US is currently making a transition
6210720	6215440	from a relatively high trust or mixed trust society to a low trust society, so surveillance
6215440	6220000	will increase. Another thing is that beliefs are not just inert representations. There are
6220000	6224720	implementations that run code on your brain and change your reality and change the way you
6224720	6231280	interact with each other at some level. And some of the beliefs are just public opinions that we
6231280	6237680	use to display our alignment. So for instance, people might say all cultures are the same and
6237680	6242560	equally good, but still they prefer to live in some cultures over others, very, very strongly so.
6243120	6246880	And it turns out that the cultures are defined by certain rules of interaction,
6246960	6251760	and these rules of interaction lead to different results when you implement them. So if you adhere
6251760	6258080	to certain rules, you get different outcomes in different societies. And this all leads to very
6258080	6263600	tricky situations when people do not have a commitment to shared purpose. And our societies
6263600	6268560	probably need to rediscover what it means to have a shared purpose and how to make this
6268560	6275360	compatible with a non-totalitarian view. So in some sense, the US is caught in a conundrum between
6276320	6283600	totalitarianism and diversity, and it doesn't need to how to resolve this. And the solutions
6283600	6287920	that the US has found so far are very crude, because it's a very young society that is also
6287920	6291520	under a lot of tension. It seems to me that the US will have to reinvent itself.
6292240	6301120	What do you think, just philosophizing, what kind of mechanisms of government do you think
6301120	6305440	we as a species should be evolving with? US or broadly? What do you think will work well
6307200	6312080	as a system? Of course, we don't know. It all seems to work pretty crappily. Some things worse
6312080	6318560	than others. Some people argue that communism is the best. Others say, yeah, look at the Soviet Union.
6318560	6324480	Some people argue that anarchy is the best, and then completely discarding the positive effects
6324480	6330800	of government. There's a lot of arguments. US seems to be doing pretty damn well.
6331760	6336880	In the span of history, there's respect for human rights, which seems to be a nice feature,
6336880	6341520	not a bug. And economically, a lot of growth, a lot of technological development.
6342320	6348880	People seem to be relatively kind on the grand scheme of things. What lessons do you draw from
6348880	6355680	that? What kind of government system do you think is good? Ideally, government should
6356480	6361600	not be perceivable. It should be frictionless. The more you notice the influence of the
6361600	6366800	government, the more friction you experience, the less effective and efficient the government
6366800	6375440	probably is. A government, game theoretically, is an agent that imposes an offset on your payout
6375440	6382080	metrics to make your Nash equilibrium compatible with the common good. You have these situations
6382160	6387600	where people act on the local incentives. And these local incentives, everybody does the thing
6387600	6391680	that's locally the best for them, but the global outcome is not good. And this is even the case
6391680	6395920	when people care about the global outcome, because a regulation mechanism exists that
6395920	6400000	creates a causal relationship between what I want to have for the global good and what I do.
6400000	6404560	So for instance, if I think that we should fly less, and I stay at home, there's not a single
6404560	6409120	plane that is going to not start because of me. It's not going to have an influence, but I don't
6409120	6414400	get from A to B. So the way to implement this would basically to have a government that is
6415120	6419600	sharing this idea that we should fly less and is then imposing a regulation that, for instance,
6419600	6426480	makes flying more expensive and gives incentives for inventing other forms of transportation
6426480	6431200	that are less putting that strain on the environment, for instance.
6432320	6436960	So there's so much optimism in so many things you describe, and yet there's the pessimism of
6436960	6441840	you think our civilization is going to come to an end. So that's not 100% probability,
6441840	6449600	nothing in this world is. So what's the trajectory out of self-destruction, do you think?
6449600	6453680	I suspect that in some sense, we are both too smart and not smart enough,
6453680	6457680	which means we are very good at solving near-term problems. And at the same time,
6457680	6464640	we are unwilling to submit to the imperatives that we would have to follow and if you want to
6464640	6471040	stick around. So that makes it difficult. If you were unable to solve everything technologically,
6471040	6475760	you can probably understand how high the child mortality needs to be to absorb the mutation rate
6475760	6481520	and how high the mutation rate needs to be to adapt to slowly changing your systemic environment.
6481520	6486480	So you could, in principle, compute all these things game theoretically and adapt to it.
6486480	6492400	But if you cannot do this because you are like me and you have children, you don't want them to die,
6492480	6498960	you will use any kind of medical information to keep child mortality low. Even if it means that
6498960	6503600	our visit in the future generations, we have enormous genetic drift and most of us have allergies
6503600	6507520	as a result of not being adapted to the changes that we made to our food supply.
6507520	6511840	That's for now, I say technologically speaking, we're just a very young,
6511840	6516960	300 years industrial revolution. We're very new to this idea. So you're attached to your kids being
6516960	6521760	alive and not being murdered for the good of good of society, but that might be a very temporary
6521760	6530480	moment of time that we might evolve. So like you said, we're both smart and not smart enough.
6530480	6536080	We are probably not this first human civilization that has discovered technology that allows to
6536080	6542000	efficiently overgrace our resources. And this overgracing, at some point, we think we can
6542000	6546400	compensate this because if we have eaten all the grass, we will find a way to grow mushrooms.
6546640	6552640	Right? But it could also be that the ecosystems tip. And so what really concerns me is not so much
6552640	6557760	the end of the civilization because we will invent a new one. But what concerns me is
6559440	6564320	the fact that, for instance, the oceans might tip. So for instance, maybe the plankton dies
6564320	6569360	because of ocean acidification and cyanobacteria take over. And as a result, we can no longer
6569360	6574080	breathe the atmosphere. This would be really concerning. So basically a major reboot of most
6574080	6580400	complex organisms on Earth. And I think this is a possibility. I don't know what the percentage
6580400	6584560	for this possibility is, but it doesn't seem to be outlandish to me if you look at the scale
6584560	6589200	of the changes that we've already triggered on this planet. And so Danny Hillers suggests that
6589200	6595120	for instance, we may be able to put chalk into the stratosphere to limit solar radiation. Maybe
6595120	6600080	it works. Maybe this is sufficient to counter the effects of what we've done. Maybe it won't be.
6600080	6605520	Maybe we won't be able to implement it by the time it's prevalent. I have no idea how the future
6605520	6610880	is going to play out in this regard. It's just, I think it's quite likely that we cannot continue
6610880	6618000	like this. All our cousin species, the other home, and it's a gun. So the right step would be to
6618000	6626080	what? To rewind towards the industrial revolution and slow the, to try to contain
6626960	6631120	the technological process that leads to the overconsumption of resources.
6632080	6637440	Imagine you get to choose. You have one lifetime. You get born into a sustainable agricultural
6637440	6644960	civilization, 300, maybe 400 million people on the planet tops. Or before this, some kind of
6644960	6651200	nomadic species with like a million or 2 million. And so you don't meet new people unless you give
6651200	6655920	birth to them. You cannot travel to other places in the world. There is no internet. There is no
6655920	6660160	interesting intellectual tradition that reaches considerably deep. So you would not discover
6660160	6665840	tumor incompleteness probably and so on. We wouldn't exist. And the alternative is you get
6665840	6671040	born into an insane world. One that is doomed to die because it has just burned 100 million
6671040	6676160	years worth of trees in a single century. Which one do you like? I think I like this one. It's
6676160	6680960	a very weird thing that when you find yourself on a Titanic and you see this iceberg and it looks
6680960	6684960	like we are not going to miss it. And a lot of people are in denial and most of the counter
6684960	6688320	arguments sound like denial to me. There don't seem to be rational arguments.
6689120	6693120	And the other thing is we are born on this Titanic. Without this Titanic, we wouldn't have
6693120	6696800	been born. We wouldn't be here. We wouldn't be talking. We wouldn't be on the internet. We
6696800	6701840	wouldn't do all the things that we enjoy. And we are not responsible for this happening. It's
6701840	6707760	basically if we had the choice, we would probably try to prevent it. But when we were born,
6708560	6712080	we were never asked when we want to be born, in which society we want to be born,
6712160	6717120	what incentive structures we want to be exposed to. We have relatively little agency in the entire
6717120	6721520	thing. Humanity has relatively little agency in the whole thing. It's basically a giant machine
6721520	6725680	that's tumbling down a hill and everybody is fantastically trying to push some buttons. Nobody
6725680	6730480	knows what these buttons are meaning, what they connect to. And most of them are not
6730480	6735600	stopping this tumbling down the hill. As possible, the artificial intelligence will give us
6736560	6744000	an escape latch somehow. So there's a lot of worry about existential threats of
6744720	6749280	artificial intelligence. But what AI also allows in general forms of automation
6750800	6758800	allows the potential of extreme productivity growth that will also, perhaps in a positive way,
6758800	6766080	transform society that may allow us to inadvertently to return to the
6768000	6775120	more, to the same kind of ideals of closer to nature that's represented in hunter-gatherer
6775120	6781680	societies that's not destroying the planet, that's not doing overconsumption and so on.
6782240	6785600	I mean, generally speaking, do you have hope that AI can help somehow?
6786560	6792080	I think it is not fun to be very close to nature until you completely subdue nature.
6792960	6797520	So our idea of being close to nature means being close to agriculture,
6799040	6801840	basically forests that don't have anything in them that eats us.
6802480	6809120	See, I mean, I want to disagree with that. I think the niceness of being close to nature
6810000	6818160	is to being fully present. When survival becomes your primary, not just your goal,
6818160	6828560	but your whole existence, I'm not just romanticizing, I can just speak for myself.
6828560	6834080	I am self-aware enough that that is a fulfilling existence.
6834960	6840960	I prefer to be in nature and not fight for my survival. I think fighting for your survival
6840960	6846560	while being in the cold and in the rain and being hunted by animals and having open wounds
6846560	6852720	is very unpleasant. There's a contradiction in there. Yes, I and you, just as you said,
6852720	6858720	would not choose it, but if I was forced into it, it would be a fulfilling existence.
6859360	6864960	If you are adapted to it, basically, if your brain is wired up in such a way that you get
6864960	6868960	rewards optimally in such an environment, and there is some evidence for this that
6869520	6874480	for a certain degree of complexity, basically, people are more happy in such an environment,
6874480	6879600	because it's what we largely have evolved for. In between, we had a few thousand years in which
6879600	6884720	I think we have evolved for a slightly more comfortable environment. So there is probably
6884720	6889920	something like an intermediate stage in which people would be more happy than there would be
6889920	6896480	if they would have to fend for themselves in small groups in the forest and often die versus
6896480	6902640	something like this, where we now have basically a big machine, a big mordor, in which we run
6902640	6910320	through concrete boxes and press buttons and machines and largely don't feel well cared for
6910400	6917200	as the monkeys that we are. So returning briefly to, not briefly, but returning to AI,
6918480	6922560	what, let me ask a romanticized question, what is the most beautiful to you,
6923440	6929040	silly ape, the most beautiful or surprising idea in the development of artificial intelligence,
6929040	6933120	whether in your own life or in the history of artificial intelligence that you've come across?
6933680	6940640	If you built an AI, it probably can make models at an arbitrary degree of detail of the world,
6941520	6945760	and then it would try to understand its own nature. It's tempting to think that at some point,
6945760	6950240	when we have general intelligence, we have competitions where we will let the AIs wake
6950240	6955200	up in different kinds of physical universes, and we measure how many movements of the Rubik's Cube
6955200	6960160	it takes until it's figured out what's going on in its universe and what it is in its own nature
6960160	6965360	and its own physics and so on. So what if we exist in the memory of an AI that is trying to
6965360	6970400	understand its own nature and remembers its own genesis and remembers Lex and Yosha sitting in
6970400	6976080	a hotel, sparking some of the ideas of that led to the development of general intelligence?
6976080	6980240	So we're a kind of simulation that's running in an AI system that's trying to understand itself.
6982320	6985920	It's not that I believe that, but I think it's a beautiful idea.
6986240	6995040	I mean, you kind of return to this idea with a Turing test of intelligence being
6998880	7002400	the process of asking and answering, what is intelligence?
7004560	7013600	I mean, why do you think there is an answer? Why is there such a search for an answer?
7013840	7018880	So does there have to be like an answer? You just had an AI system that's trying to
7019600	7022880	understand the Y of what, you know, understand itself.
7024880	7031120	Is that a fundamental process of greater and greater complexity, greater and greater intelligence?
7031120	7033920	Is the continuous trying of understanding itself?
7034960	7039120	No, I think you will find that most people don't care about that because they're well adjusted
7039120	7044480	enough to not care. And the reason why people like you and me care about it probably has to do
7044480	7048800	with the need to understand ourselves. It's because we are in fundamental disagreement
7048800	7053200	with the universe that we wake up in. It looked down on me and I see, oh my god,
7053200	7058800	I'm caught in a monkey. What's that? Some people are unhappy with the government and I'm unhappy
7058800	7064960	with the entire universe that I find myself in. So you don't think that's a fundamental
7064960	7067600	aspect of human nature that some people are just suppressing,
7068400	7072160	that they wake up shocked they're in the body of a monkey?
7072160	7076880	No, there is a clear adaptive value to not be confused by that and by...
7078000	7084160	Well, no, that's not what I asked. So yeah, if there's clear adaptive value, then
7085280	7089760	there's clear adaptive value to while fundamental your brain is confused by that,
7089760	7098240	by creating an illusion, another layer of the narrative that tries to suppress that and instead
7098240	7102240	say that what's going on with the government right now is the most important thing. What's
7102240	7106720	going on with my football team is the most important thing. But it seems to me the...
7109440	7114320	For me, it was a really interesting moment reading Ernest Becker's Denial of Death.
7115040	7126000	That this kind of idea that we're all... The fundamental thing from which most of our human
7126880	7132640	mind springs is this fear of mortality and being cognizant of your mortality and the fear of that
7132640	7139440	mortality and then you construct illusions on top of that. I guess I'm... You being...
7140160	7147600	Just to push on it, you really don't think it's possible that this worry of the big existential
7147600	7152800	questions is actually fundamental as the existentialist thought to our existence.
7153360	7158000	I think that the fear of death only plays a role as long as you don't see the big picture.
7158560	7164480	The thing is that minds are software states, right? Software doesn't have identity. Software in
7164480	7171920	some sense is a physical law. But it feels like there's an identity. I thought that was for
7171920	7176480	this particular piece of software and the narrative it tells. That's a fundamental
7176480	7181840	property of it. The maintenance of the identity is not terminal. It's instrumental to something
7181840	7186960	else. You maintain your identity so you can serve your meaning. So you can do the things that you're
7186960	7192160	supposed to do before you die. And I suspect that for most people, the fear of death is the fear of
7192160	7195680	dying before they are done with the things that they feel they have to do even though they cannot
7195680	7202720	quite put their finger on it, what it is, what that is. Right. But in the software world,
7203920	7207360	to return to the question, then what happens after we die?
7210240	7215040	Why would you care? You will not be longer there. The point of dying is that you are gone.
7215040	7221040	Well, maybe I'm not. This is what, you know, it seems like there's so much...
7223040	7228800	In the idea that this is just, the mind is just a simulation that's constructing a narrative around
7228800	7237360	some particular aspects of the quantum mechanical wave function world that we can't quite get direct
7237360	7244800	access to, then like the idea of mortality seems to be fuzzy as well. Maybe there's not
7244880	7250160	a clear end. No, the fuzzy idea is the one of continuous existence. We don't have continuous
7250160	7255920	existence. How do you know that? Like that... Because it's not computable. Because you're
7255920	7259520	saying it's... There is no continuous process. The only thing that binds you together with the
7259520	7263920	Lex Friedman from yesterday is the illusion that you have memories about him. So if you want
7263920	7267920	to upload, it's very easy. You make a machine that thinks it's you. Because this is the same
7267920	7272160	thing that you are. You are a machine that thinks it's you. But that's immortality.
7272880	7276960	Yeah, but it's just a belief. You can create this belief very easily. Once you realize that
7276960	7282400	the question whether you are immortal or not depends entirely on your beliefs and your own
7282400	7288240	continuity. But then you can be immortal by the continuity of the belief.
7288960	7293600	It cannot be immortal, but you can stop being afraid of your mortality because you realize
7293600	7296400	you were never continuously existing in the first place.
7296640	7302080	Well, I don't know if I'd be more terrified or less terrified with that. It seems like the fact
7302080	7307520	that I existed. Also, you don't know this state in which you don't have a self. You can turn off
7307520	7314320	yourself, you know? I can't turn off myself. You can turn it off. I can. Yes. You can basically
7314320	7318480	meditate yourself in a state where you are still conscious. There are still things are happening
7318480	7322800	where you know everything that you knew before, but you're no longer identified with changing
7322800	7329120	anything. And this means that yourself and the way it dissolves, there is no longer this person.
7329120	7333920	You know that this person construct exists in other states and it runs on the brain of
7333920	7340080	Lex Friedman. But it's not a real thing. It's a construct. It's an idea. And you can change
7340080	7345040	that idea. And if you let go of this idea, if you don't think that you are special,
7345600	7349760	you realize it's just one of many people and it's not your favorite person even, right? It's
7349760	7354320	just one of many. And it's the one that you are doomed to control for the most part and
7354320	7360080	that is basically informing the actions of this organism as a control model. And this is all there
7360080	7366640	is. And you are somehow afraid that this control model gets interrupted or loses the identity
7366640	7373280	of continuity. Yeah, so I'm attached. I mean, yeah, it's a very popular, it's a somehow compelling
7373360	7379760	notion that being attached, like there's no need to be attached to this idea of an identity.
7383440	7388080	But that in itself could be an illusion that you construct. So the process of meditation,
7388080	7393440	while popular, is thought of as getting under the concept of identity. It could be just putting
7393440	7403040	a cloak over it, just telling it to be quiet for the moment. I think that meditation is eventually
7403120	7407760	just a bunch of techniques that let you control attention. And when you can control attention,
7407760	7412400	you can get access to your own source code, hopefully not before you understand what you're
7412400	7416320	doing. And then you can change the way it works temporarily or permanently.
7417280	7423200	So yeah, meditation is to get a glimpse at the source code, get under, so basically control or
7423200	7426400	turn off the attention. The entire thing is that you learn to control attention. So everything
7426400	7431360	else is downstream from controlling attention. And control the attention that's looking at the
7431360	7435520	attention. Normally, we only get attention in the parts of our mind that create heat,
7435520	7440560	where you have a mismatch between model and the results that are happening. And so most people
7440560	7445840	are not self aware because their control is too good. If everything works out roughly the way you
7445840	7450800	want, and the only things that don't work out is whether your football team wins, then you will
7450800	7455680	mostly have models about these domains. And it's only when, for instance, your fundamental
7455680	7460720	relationships to the world around you don't work because the ideology of your country is insane
7460720	7465120	and the other kids are not nerds and don't understand why you understand physics and you
7465120	7469840	don't why you want to understand physics and you don't understand why somebody would not want to
7469840	7477600	understand physics. So we kind of brought up neurons in the brain as reinforcement learning agents.
7479840	7485040	And there's been some successes as you brought up with with go with Alpha go Alpha zero,
7485040	7489360	with ideas of self play, which I think are incredibly interesting ideas of systems playing
7489360	7498000	each other and an automated way to improve by playing other systems of in a particular
7498000	7504080	construct of a game that are a little bit better than itself and thereby improving continuously.
7504080	7509600	All the competitors in the game are improving gradually. So being just challenging enough
7509600	7515040	and from learning from the process of the competition. Do you have hope for that reinforcement
7515040	7519840	learning process to achieve greater and greater level of intelligence? So we talked about different
7519840	7527120	ideas in AI that we need to be solved. Is RL a part of that process of trying to create an
7527120	7531040	AGI system? So what do you think? The forms of unsupervised learning, but there are many
7531040	7536240	algorithms that can achieve that. And I suspect that ultimately the algorithms that work,
7536960	7541920	there will be a class of them or many of them and they might have small differences of like
7541920	7547200	magnitude and efficiency. But eventually what matters is the type of model that you form.
7547840	7550720	And the types of models that we form right now are not sparse enough.
7553280	7557600	Sparse, what does it mean to be sparse? So it means that ideally every
7558720	7564720	potential model state should correspond to a potential world state. So basically,
7564720	7568960	if you vary states in your model, you always end up with valid world states.
7568960	7573440	And our mind is not quite there. So an indication is basically what we see in dreams.
7573440	7578400	The older we get, the more boring our dreams become. Because we incorporate more and more
7578400	7583040	constraints that we learned about how the world works. So many of the things that we imagine
7583040	7588240	to be possible as children turn out to be constrained by physical and social dynamics.
7588800	7593760	And as a result, fewer and fewer things remain possible. And it's not because our imagination
7593760	7597920	scales back, but the constraints under which it operates become tighter and tighter.
7598560	7604000	And so the constraints under which our neural networks operate are almost limitless, which
7604000	7607920	means it's very difficult to get a neural network to imagine things that look real.
7611280	7615280	So I suspect part of what we need to do is we probably need to build dreaming systems.
7615280	7621520	I suspect that part of the purpose of dreams is to, similar to a generative adversarial network,
7621600	7627520	learn certain constraints, and then it produces alternative perspectives on the same set of
7627520	7631600	constraints. So you can recognize it under different circumstances. Maybe we have flying
7631600	7636000	dreams as children, because we recreate the objects that we know and the maps that we know
7636000	7639600	from different perspectives, which also means from a bird's eye perspective.
7640240	7643520	So I mean, aren't we doing that anyway? I mean, not without with our eyes,
7644240	7649120	with our eyes closed and when we're sleeping, aren't we just constantly running dreams and
7649120	7652960	simulations in our mind as we try to interpret the environment? I mean,
7653680	7657920	sort of considering all the different possibilities of the way we interact with the environment seems
7657920	7666560	like essentially, like you said, sort of creating a bunch of simulations that are
7666560	7672240	consistent with our expectations, with our previous experiences, with the things we just
7672240	7682400	saw recently. And through that hallucination process, we are able to then somehow stitch
7682400	7687760	together what actually we see in the world with the simulations that match it well and thereby
7687760	7692720	interpret it. I suspect that you and my brain are slightly unusual in this regard,
7693440	7699520	which is probably what got you into MIT. So this obsession of constantly pondering possibilities
7699520	7707520	and solutions to problems. Oh, stop it. I think I'm not talking about intellectual stuff. I'm
7707520	7713600	talking about just doing the kind of stuff it takes to walk and not fall.
7714320	7715920	Yes, this is largely automatic.
7719440	7724160	Yes, but the process is, I mean... It's not complicated. It's relatively easy to build
7724160	7729280	a neural network that in some sense learns the dynamics. The fact that we haven't done it right,
7729360	7733440	so far, it doesn't mean it's hard because you can see that a biological organism does it.
7733440	7738160	There's relatively few neurons. So basically, you build a bunch of neural oscillators that
7738160	7742800	entrain themselves with the dynamics of your body in such a way that the regulator becomes
7742800	7747440	isomorphic and it's modeled to the dynamics that it regulates. And then it's automatic,
7747440	7751360	and it's only interesting the sense that it captures attention when the system is off.
7752160	7758000	See, but thinking of the kind of mechanism that's required to do walking as a controller,
7758960	7767040	as a neural network, I think it's a compelling notion, but it discards quietly,
7767920	7773120	or at least makes implicit the fact that you need to have something like common sense reasoning
7773120	7780480	to walk. It's an open question whether you do or not, but my intuition is to act in this world,
7780480	7786400	there's a huge knowledge base that's underlying it somehow. There's so much information
7787360	7794400	of the kind we have never been able to construct in neural networks or in artificial intelligence
7794400	7800400	systems period, which is like it's humbling, at least in my imagination, the amount of
7800400	7807520	information required to act in this world humbles me. And I think saying that neural
7807520	7816160	networks can accomplish it is missing the fact that we don't have yet a mechanism for
7816240	7820160	constructing something like common sense reasoning. I mean, what's your sense
7821600	7831200	about to linger on the idea of what kind of mechanism would be effective at walking?
7831200	7836160	You said just a neural network, not maybe the kind we have, but something a little bit better
7836160	7840720	would be able to walk easily. Don't you think it also needs to know
7841680	7848240	a huge amount of knowledge that's represented under the flag of common sense reasoning?
7848240	7851520	How much common sense knowledge do we actually have? Imagine that you are
7851520	7856000	really hardworking through all your life and you form two new concepts every half hour,
7856000	7859520	so you end up with something like a million concepts because you don't get that old.
7860960	7863120	So a million concepts, that's not a lot.
7865600	7870240	So it's not just a million concepts. I personally think it might be much more than
7870320	7874480	a million. But if you think just about the numbers, you don't live that long.
7875280	7879520	If you think about how many cycles do your neurons have in your life, it's quite limited.
7879520	7884640	You don't get that old. Yeah, but the powerful thing is the number of concepts,
7885520	7891920	and they're probably deeply hierarchical in nature, the relations as you described between
7891920	7898080	them is the key thing. So it's like even if it's a million concepts, the graph of relations that's
7898080	7906400	formed and some kind of probabilistic relationships, that's what common sense reasoning is,
7906400	7912080	the relationship between things. Yeah, so in some sense, I think of the concepts as the
7912080	7916560	address space for our behavior programs. And the behavior programs allow us to recognize
7916560	7922240	objects and interact with them, also mental objects. And a large part of that is the physical
7922240	7926640	world that we interact with, which is this res extender thing, which is basically navigation
7926720	7933440	of information and space. And basically, it's similar to a game engine. It's a physics engine
7933440	7940240	that you can use to describe and predict how things that look in a particular way, that feel,
7940240	7944320	when you touch them in particular way, that are proprioception, that are auditory perception,
7944320	7948000	and so on, how they work out. So basically the geometry of all these things. And this is
7949520	7954720	probably 80% of what our brain is doing is dealing with that with this real time simulation.
7954720	7959760	And by itself, a game engine is fascinating, but it's not that hard to understand what it's
7959760	7966880	doing, right? And our game engines are already in some sense, approximating the fidelity of
7966880	7973120	what we can perceive. So if we put on an Oculus Quest, we get something that is still
7973120	7977200	relatively crude with respect to what we can perceive, but it's also in the same ballpark
7977200	7982480	already, right? It's just a couple order of magnitudes away from saturating our perception
7983040	7987840	in terms of the complexity that it can produce. So in some sense, it's reasonable to say that
7987840	7994080	our computer that you can buy and put into your home is able to give a perceptual reality that
7994080	8000160	has a detail that is already in the same ballpark as what your brain can process. And everything
8000160	8005120	else are ideas about the world. And I suspect that they are relatively sparse. And also the
8005120	8010560	intuitive models that we form about social interaction. Social interaction is not so hard.
8010640	8015040	It's just hard for us nerds because we all have our wires crossed, so we need to deduce them.
8015040	8020080	But the pyres are present in most social animals. So it's an interesting thing to notice that many
8021120	8026640	domestic social animals like cats and dogs have better social cognition than children.
8026640	8034240	Right. I hope so. I hope it's not that many concepts fundamentally to do to exist in this
8034240	8038240	world. Social interaction. For me, it's more like I'm afraid so because this thing that
8038960	8043360	we only appear to be so complex to each other because we are so stupid is a little bit depressing.
8044400	8051120	Yeah. To me, that's inspiring if we're indeed as stupid as it seems.
8051120	8055920	I think our brains don't scale and the information processing that we build tend to scale very well.
8056800	8063920	Yeah. But one of the things that worries me is that the fact that the brain doesn't scale
8064000	8070080	means that that's actually a fundamental feature of the brain. All the flaws of the brain,
8070080	8075360	everything we see as limitations perhaps as a fundamental, the constraints on the system could
8075360	8083920	be a requirement of its power, which is different than our current understanding of
8083920	8088720	intelligent systems where scale, especially with deep learning, especially with reinforcement
8088720	8096720	learning, the hope behind open AI and deep mind, all the major results really have to do with huge
8096720	8102560	compute. It could also be that our brains are so small, not just because they take up so much
8103120	8108000	glucose in our body, like 20% of the glucose so they don't arbitrarily scale. There are some
8108560	8112720	animals like elephants which have larger brains than us and the dogs seem to be smarter.
8112720	8116400	Right. Elephants seem to be autistic. They have very, very good motor control and they're
8116400	8120480	really good with details, but they really struggle to see the big picture. So you can make them
8121120	8127200	recreate drawings stroke by stroke. They can do that, but they cannot reproduce a still life. So
8127200	8131520	they cannot make a drawing of a scene that they see. They will always be only able to
8131520	8134880	reproduce the line drawing, at least as far from what I could see in the experiments.
8135840	8140800	Yeah. Why is that? Maybe smarter elephants would meditate themselves out of existence
8140800	8144240	because their brains are too large. So basically the elephants that were not autistic,
8144960	8148960	they didn't reproduce. Yeah. So we have to remember that the brain is fundamentally
8148960	8154320	interlinked with the body in our human and biological system. Do you think that AGI systems,
8154320	8157440	that we try to create a greater intelligence systems, would need to have a body?
8159120	8161920	I think that should be able to make use of a body if you give it to them,
8163040	8167680	but I don't think that I fundamentally need a body. So I suspect if you can interact with the
8167680	8173360	world by moving your eyes and your head, you can make controlled experiments. And this allows you
8173360	8181280	to have many magnitudes, fewer observations in order to reduce the uncertainty in your models.
8181840	8185040	So you can pinpoint the areas in your models where you're not quite sure and you just move
8185040	8190080	your head and see what's going on over there and you get additional information. If you just have
8190080	8194640	to use YouTube as an input and you cannot do anything beyond this, you probably need just
8194640	8200160	much more data. But we have much more data. So if you can build a system that has enough
8200160	8204800	time and attention to browse all of YouTube and extract all the information that there is to be
8204800	8211360	found, I don't think there's an obvious limit to what it can do. But it seems that the interactivity
8211360	8215920	is a fundamental thing that the physical body allows you to do. But let me ask on that topic,
8216480	8221760	that's what a body is, is allowing the brain to touch things and move things and interact with
8223840	8227520	whether the physical world exists or not, whatever, but interact with some interface
8227520	8235600	to the physical world. What about a virtual world? Do you think we can do the same kind of
8235600	8244080	reasoning, consciousness, intelligence if we put on a VR headset and move over to that world? Do you
8244080	8248320	think there's any fundamental difference between the interface to the physical world that it's
8248880	8252560	here in this hotel and if we were sitting in the same hotel in a virtual world?
8252560	8258880	The question is, does this non-physical world or this other environment entice you to solve
8258880	8263920	problems that require general intelligence? If it doesn't, then you probably will not develop
8263920	8267840	general intelligence. And arguably, most people are not generally intelligent because they don't
8267840	8272320	have to solve problems that make them generally intelligent. And even for us, it's not yet clear
8272320	8277440	if we are smart enough to build AI and understand our own nature to this degree. So it could be a
8277440	8281680	matter of capacity. And for most people, it's in the first place a matter of interest. They don't
8281680	8286720	see the point because the benefit of attempting this project are marginal because you're probably
8286720	8291120	not going to succeed in it and the cost of trying to do it requires complete dedication of your
8291120	8296080	entire life. Right? But it seems like the possibility is what you can do in the virtual world. So
8296080	8302160	imagine it is much greater than you can in the real world. So imagine a situation and be interesting
8302160	8309840	option for me. If somebody came to me and offered what I'll do is, so from now on, you can only
8309840	8316240	exist in the virtual world. And so you put on this headset. And when you eat, we'll make sure to
8316240	8322480	connect your body up in a way that when you eat in the virtual world, your body will be nourished
8322480	8328000	in the same way in the virtual world. So the aligning incentives between the our common sort of real
8328000	8334080	world in the virtual world, but then the possibility to become much bigger. Like I could be other kinds
8334080	8339440	of creatures I could do. I can break the laws of physics because we know them. I could do a lot.
8339440	8345360	I mean, the possibilities are endless, right? As far as we think, it's an interesting thought
8345360	8350960	whether like what existence would be like, what kind of intelligence would emerge there,
8350960	8356000	what kind of consciousness, what kind of maybe greater intelligence, even in me,
8357200	8361040	Lex, even at this stage in my life, if I spend the next 20 years in our world
8361040	8366560	to see how that intelligence emerges. And if I was, if that happened at the very beginning,
8366560	8371680	before I was even cognizant of my existence in this physical world, it's interesting to think
8371680	8378000	how that child would develop. And the way virtual reality and digitization of everything is moving,
8378000	8384000	it's not completely out of the realm of possibility that we're all, that some part of our lives will,
8384800	8391440	if not entirety of it, will live in a virtual world to a greater degree than we currently have
8391440	8396880	living on Twitter and social media and so on. Do you have, I mean, does something draw you
8396880	8404000	intellectually or naturally in terms of thinking about AI to this virtual world where more possibilities
8405600	8409920	I think that currently it's a waste of time to deal with the physical world before we have
8409920	8414800	mechanisms that can automatically learn how to deal with it. The body gives you a second order
8414800	8419680	agency. What constitutes the body is the things that you can indirectly control.
8420640	8425680	Third order are tools. And the second order is the things that are basically always present,
8425680	8432000	but you operate on them with first order things, which are mental operators. And the zero order
8432000	8439760	is in some sense the direct sense of what you're deciding. So you observe yourself initiating an
8439760	8444960	action that features that you interpret as the initiation of an action, then you perform the
8444960	8449920	operations that you perform to make that happen. And then you see the movement of your limbs and
8449920	8455200	you learn to associate those and thereby model your own agency over this feedback. But the first
8455200	8458960	feedback that you get is from this first order thing already. Basically, you decide to think
8458960	8463440	a thought and the thought is being thought. You decide to change the thought and you observe
8463440	8468160	how the thought is being changed. And in some sense, this is, you could say, an embodiment
8468160	8472320	already. And I suspect it's sufficient as an embodiment or intelligence.
8473120	8477360	It's not that important, at least at this time, to consider variations in the second order.
8477360	8484080	Yes. But the thing that you also mentioned just now is physics that you could change in
8484080	8489120	any way you want. So you need an environment that puts up resistance against you. If there's
8489120	8493920	nothing to control, you cannot make models. There needs to be a particular way that resists you.
8494640	8498880	And by the way, your motivation is usually outside of your mind. It resists your motivation,
8498880	8503040	is what gets you up in the morning, even though it would be much less work to stay in bed.
8504320	8511840	It's basically forcing you to resist the environment and it forces your mind to serve it,
8511840	8516720	to serve this resistance to the environment. So in some sense, it is also putting up resistance
8516720	8519680	against the natural tendency of the mind to not do anything.
8519680	8523200	Yeah. But so some of that resistance, just like you described with motivation, is like
8523200	8528320	in the first order. It's in the mind. Some resistance is in the second order,
8528320	8532240	like actual physical objects pushing against you and so on. It seems that the second order
8532240	8537280	stuff in virtual reality could be recreated. Of course. But it might be sufficient that you
8537280	8541840	just do mathematics and mathematics is already putting up enough resistance against you.
8541840	8547760	So basically just with an aesthetic motive, this could maybe sufficient to form a type of
8547760	8552320	intelligence. It would probably not be a very human intelligence, but it might be one that is
8552320	8559840	already general. So to mess with this zero order, maybe first order, what do you think about
8559840	8564880	ideas of brain-computer interfaces? So again, returning to our friend Elon Musk and Neural
8564880	8570480	Link, a company that's trying to, of course, there's a lot of trying to cure diseases and so on
8570480	8575840	with a near term, but the long-term vision is to add an extra layer to basically expand the
8575840	8583920	capacity of the brain connected to the computational world. Do you think one that's
8583920	8587840	possible too, how does that change the fundamentals of the zero-th order and the first order?
8587840	8591600	It's technically possible, but I don't see that the FDA would ever allow me to drill
8591600	8596880	holes in my skull to interface my neocortex on Musk envisions. So at the moment, I can do
8596880	8602960	horrible things to mice, but I'm not able to do useful things to people, except maybe at some
8602960	8608240	point down the line in medical applications. So this thing that we are envisioning, which means
8610080	8614720	recreational and recreational brain-computer interfaces, are probably not going to happen
8614720	8620000	in the present legal system. I love it how I'm asking you, out there philosophical
8620800	8628080	and sort of engineering questions, and for the first time ever, you jumped to the legal FDA.
8628080	8631760	There would be enough people that would be crazy enough to have holes drilled in their skull to
8631760	8637920	try a new type of brain-computer interface. But also, if it works, FDA will approve it. I mean,
8639840	8643840	it's like I work a lot with autonomous vehicles. Yes, you can say that it's going to be very
8643840	8648080	difficult regulatory process of approving with autonomous, but it doesn't mean autonomous vehicles
8648080	8653200	are never going to happen. No, they will totally happen as soon as we create jobs for at least
8653520	8660960	two lawyers and one regulator per car. Yes, lawyers, that's actually like lawyers,
8660960	8667600	this is the fundamental substrate of reality. In the US, it's a very weird system. It's not
8667600	8672240	universal in the world. The law is a very interesting software once you realize it, right?
8672240	8677440	These circuits are, in some sense, streams of software, and it largely works by exception
8677440	8681520	handling. So you make decisions on the ground, and they get synchronized with the next level
8681600	8688160	structure as soon as an exception is being thrown. So it escalates the exception handling. The process
8688160	8695040	is very expensive, especially since it incentivizes the lawyers for producing work for lawyers.
8695040	8703680	Yes, so the exceptions are actually incentivized for firing often. But to return, outside of lawyers,
8704720	8710640	is there anything fundamentally, is there anything interesting, insightful about the
8710640	8715360	possibility of this extra layer of intelligence added to the brain?
8715360	8721120	I do think so, but I don't think that you need technically invasive procedures to do so. We
8721120	8725200	can already interface with other people by observing them very, very closely and getting
8725200	8731680	in some kind of empathetic resonance. I know it's not very good at this, but I noticed that people
8731680	8737920	are able to do this to some degree. It basically means that we model an interface layer of the
8737920	8743440	other person in real time. And it works despite our neurons being slow, because most of the things
8743440	8748240	that we do are built on periodic processes. So you just need to entrain yourself with the
8748240	8753120	oscillation that happens. And if the oscillation itself changes slowly enough, you can basically
8753120	8763440	follow along. But the bandwidth of the interaction, it seems like you can do a lot more computation
8764080	8768960	of course. But the other thing is that the bandwidth that our brain, our own mind is running on is
8768960	8774240	actually quite slow. So the number of thoughts that I can productively think in any given day
8774240	8780000	is quite limited. But if they had the discipline to write it down and the speed to write it down,
8780000	8785040	maybe it would be a book every day or so. But if you think about the computers that we can build,
8785040	8789520	the magnitudes at which they operate, this would be nothing. It's something that it can
8789600	8795280	put out in a second. Well, I don't know. So it's possible, sort of the number of thoughts you
8795280	8801120	have in your brain, it could be several orders of magnitude higher than what you're possibly able
8801120	8806880	to express through your fingers or through your voice. Most of them are going to be repetitive,
8807440	8813680	because they have to control the same problems every day. When I walk, they are going to be
8813680	8818240	processes in my brain that model my walking pattern and regulate them and so on. But it's
8818240	8822800	going to be pretty much the same every day or every step. But I'm talking about intellectual
8822800	8826960	reasoning, thinking. So the question, what is the best system of government? So you sit down and
8826960	8834000	start thinking about that. One of the constraints is that you don't have access to a lot of facts,
8834000	8842240	a lot of studies. You always have to interface with something else to learn more, to aid in your
8842240	8846960	reasoning process. If you can directly access all of Wikipedia in trying to understand what is the
8846960	8852720	best form of government, then every thought won't be stuck in a loop. Every thought that requires
8852720	8857760	some extra piece of information will be able to grab it really quickly. That's the possibility of
8858400	8867680	if the bottleneck is literally the information that the bottleneck of breakthrough ideas is just
8867680	8871920	being able to quickly access huge amounts of information, then the possibility of connecting
8871920	8879040	your brain to the computer could lead to totally new breakthroughs. You can think of
8879040	8888320	mathematicians being able to just up the orders and magnitude of power in their reasoning about
8888320	8893200	mathematical rules. What if humanity has already discovered the optimal form of government through
8893200	8899440	a revolutionary process? There is an evolution going on. So what we discover is that maybe
8899440	8903760	the problem of government doesn't have stable solutions for us as a species, because we are
8903760	8909520	not designed in such a way that we can make everybody conform to them. But there could be
8909520	8914240	solutions that work under given circumstances or that they're the best for certain environment and
8914240	8919840	depends on, for instance, the primary forms of ownership and the means of production. So if the
8919840	8927360	main means of production is land, then the forms of government will be regulated by the land owners
8927360	8933680	and you get a monarchy. If you also want to have a form of government in which you depend on some
8933680	8938720	form of slavery, for instance, where the peasants have to work very long hours for very little gain,
8938720	8944720	so very few people can have plumbing, then maybe you need to promise them that you get paid in the
8944720	8952400	afterlife over time. So you need a theocracy. And so for much of human history in the West,
8952400	8957840	we had a combination of monarchy and theocracy that was our form of governance, right? At the same
8957840	8965200	time, the Catholic Church implemented game theoretic principles. I recently reread Thomas O'Kynos,
8965200	8969760	it's very interesting to see this because he was not a dualist. He was translating Aristotle in a
8969760	8977360	particular way for designing an operating system for the Catholic society. And he says that basically
8977360	8981920	people are animals and very much the same way as Aristotle envisions, which basically
8981920	8986800	organisms with cybernetic control. And then he says that there are initial rational principles
8986800	8991600	that humans can discover and everybody can discover them. So they are universal. If you are sane,
8991600	8995840	you should understand, you should submit to them because you can rationally deduce them.
8995840	9002480	And these principles are roughly, you should be willing to self-regulate correctly.
9003680	9011040	You should be willing to do correct social regulation, it's intraorganism. You should be
9011120	9013440	willing to act on your models. So you have skin in the game.
9017120	9021920	And you should have goal rationality. You should be choosing the right goals to work on.
9023280	9027440	So basically these three rational principles, goal rationality, he calls prudence or wisdom.
9028640	9033440	Social regulation is justice. The correct social one and the internal regulation is
9033440	9040320	temperance. And this, I think, willingness to act on your models is courage. And then he says
9040320	9045040	that there are additionally to these four cardinal virtues, three divine virtues. And these
9045040	9049360	three divine virtues cannot be rationally deduced, but they reveal themselves by the harmony,
9049360	9053840	which means if you assume them and you extrapolate what's going to happen, you will see that they
9053840	9059520	make sense. And it's often been misunderstood as God has to tell you that these are the things.
9059520	9065440	So basically there's something nefarious going on. The Christian conspiracy forces you to believe
9065440	9071840	some guy with a long beard, that they discovered this. So these principles are relatively simple.
9071840	9076480	Again, it's for high level organization, for the resulting civilization that you form,
9077360	9082960	commitment to unity. So basically you serve this higher larger thing, this structural
9082960	9089040	principle on the next level, and he calls that faith. Then there needs to be commitment to
9089040	9092560	shared purpose. This is basically this global reward that you try to figure out what that
9092560	9096880	should be and how you can facilitate this. And this is love. The commitment to shared purpose
9096880	9100960	is the core of love. You see this sacred thing that is more important than your own
9100960	9105280	organismic interests in the other. And you serve this together. And this is how you see
9105280	9110960	the sacred in the other. And the last one is hope, which means you need to be willing to act on that
9111520	9115600	principle without getting rewards in the here and now, because it doesn't exist yet.
9115600	9120000	Then you start out building the civilization. So you need to be able to do this in the absence
9120640	9124880	of its actual existence yet. So it can come into being.
9124880	9128960	So yeah, so the way it comes into being is by you accepting those notions and then you see
9128960	9133680	in there these three divine concepts and you see them realized.
9133680	9138000	Another problem is divine is a loaded concept in our world, right? Because we are outside of
9138000	9142640	this cult and we are still scarred from breaking free of it. But the idea is basically we need to
9142640	9147840	have a civilization that acts as an intentional agent, like an insect state. And we are not
9147840	9153760	actually a tribal species. We are a state building species. And what enabled state building is
9153760	9158960	basically the formation of religious states and other forms of rule based administration,
9158960	9162640	in which the individual doesn't matter as much as the rule or the higher goal.
9163680	9167200	We got there by the question, what's the optimal form of governance? So I don't think that
9167200	9171760	Catholicism is the optimal form of governance because it's obviously on the way out, right?
9171760	9178320	So it is for the present type of society that we are in. Religious institutions don't seem to be
9178320	9184320	optimal to organize that. So what we discovered right now that we live in in the West is democracy.
9184320	9188480	And democracy is the role of oligarchs that are the people that currently own the means of production
9189440	9194240	that is administered not by the oligarchs themselves because there's too much disruption,
9194240	9199200	right? We have so much innovation that we have in every generation new means of production that
9199280	9204240	we invent. And corporations die usually after 30 years or so. And something other
9205120	9209680	takes the leading role in our societies. So it's administered by institutions. And these
9209680	9215680	institutions themselves are not elected, but they provide continuity. And they are led by
9215680	9220640	electable politicians. And this makes it possible that you can adapt to change without having to
9220640	9224960	kill people, right? So you can, for instance, if a change in governments, if people think that the
9224960	9230160	current government is too corrupt or is not up to date, you can just elect new people. Or if a
9230160	9235920	journalist finds out something inconvenient about the institution and the institution has no plan B
9235920	9241760	like in Russia, the journalist has to die. This is what when you run society by the deep state.
9241760	9248640	So ideally you have an administration layer that you can change if something bad happens, right?
9248640	9253600	So you will have a continuity in the whole thing. And this is the system that we came up in the West.
9253680	9258000	And the way it's set up in the US is largely a result of low level models. So it's mostly just
9258000	9262880	second, third order consequences that people are modeling in the design of these institutions.
9262880	9267520	It's a relatively young society that doesn't really take care of the downstream effects of many of
9267520	9273680	the decisions that are being made. And I suspect that AI can help us this in a way if you can fix
9273680	9279520	the incentives. The society of the US is the society of teeters. It's basically cheating
9279520	9283120	is so indistinguishable from innovation and we want to encourage innovation.
9283120	9285040	Can you elaborate on what you mean by cheating?
9285040	9289200	It's basically people do things that they know are wrong. It's acceptable to do things that
9289200	9292720	you know are wrong in the society to a certain degree. You can, for instance,
9293680	9296400	suggest some non-sustainable business models and implement them.
9297440	9299600	Right, but you're always pushing the boundaries. I mean, you're-
9300720	9302720	And yes, this is seen as a good thing largely.
9303920	9304240	Yes.
9304880	9309200	And this is different from other societies. So for instance, social mobility is an aspect of
9309200	9313280	this. Social mobility is the result of individual innovation that would not be
9313280	9317280	sustainable at scale for everybody else. Normally, you should not go up. You should
9317280	9321440	go deep. We need bakers and indeed we are very good bakers, but in a society that
9321440	9324400	innovates, maybe you can replace all the bakers with a really good machine.
9326400	9329600	That's not a bad thing and it's a thing that made the US so successful,
9329600	9333680	but it also means that the US is not optimizing for sustainability but for innovation.
9334640	9339360	And so it's not obvious as the evolutionary process is unrolling. It's not obvious that
9339360	9341200	that long-term would be better.
9342160	9347600	It has side effects. So basically, if you treat, you will have a certain layer of toxic sludge
9347600	9350400	that covers everything that is a result of cheating.
9350400	9355600	And we have to unroll this evolutionary process to figure out if these side effects are so damaging
9355600	9361840	that the system is horrible or if the benefits actually outweigh the negative effects.
9363680	9365840	How do we get to the which system of government is best?
9368000	9370800	I'm trying to trace back the last five minutes.
9370800	9376560	I suspect that we can find a way back to AI by thinking about the way in which our brain has
9376560	9384800	to organize itself. In some sense, our brain is a society of neurons and our mind is a society of
9384800	9390800	behaviors. And they need to be organizing themselves into a structure that implements
9390800	9396560	regulation. And government is social regulation. We often see government as the manifestation of
9396560	9401040	power or local interest, but it's actually a platform for negotiating the conditions of human
9401040	9407040	survival. And this platform emerges over the current needs and possibilities and the trajectory
9407040	9412640	that we have. So given the present state, there are only so many options on how we can move into
9412640	9416480	the next state without completely disrupting everything. And we mostly agree that it's a
9416480	9421280	bad idea to disrupt everything because it will endanger our food supply for a while and the entire
9421280	9426960	infrastructure and fabric of society. So we do try to find natural transitions,
9426960	9430720	and there are not that many natural transitions available at any given point.
9430720	9432160	What do you mean by natural transitions?
9432160	9434560	So we try not to have revolutions if we can help.
9436640	9441200	So speaking of revolutions and the connection between government systems in the mind,
9441760	9449120	you've also said that in some sense becoming an adult means you take charge of your emotions.
9449120	9455200	Maybe you never said that. Maybe I just made that up. But in the context of the mind,
9455840	9462320	what's the role of emotion? And what is it? First of all, what is emotion? What's its role?
9462320	9466960	It's several things. So psychologists often distinguish between emotion and feeling,
9466960	9472880	and in common day parlance we don't. I think that emotion is a configuration of the cognitive
9472880	9477840	system. That's especially true for the lowest level for the affective state. So when you have
9477840	9482560	an affect, it's the configuration of certain modulation parameters like arousal, valence,
9484000	9488080	your attentional focus, whether it's wide or narrow, inter-reception or extra-reception,
9488080	9493120	and so on. And all these parameters together put you in a certain way to relate to the
9493120	9497280	environment and to yourself. And this is in some sense an emotional configuration.
9497280	9503200	In the more narrow sense, an emotion is an affective state that has an object. And the relevance
9503200	9508400	of that object is given by motivation. And motivation is a bunch of needs that are associated
9508400	9512560	with rewards, things that give you pleasure and pain. And you don't actually act on your needs,
9512560	9516720	you act on models of your needs. Because when the pleasure and pain manifests, it's too late,
9516720	9521120	you've done everything. But so you act on expectations, what will give you pleasure and
9521120	9526320	pain? And these are your purposes. The needs don't form a hierarchy, they just coexist and compete.
9526320	9530480	And your organism has to, or your brain has to find a dynamic homeostasis between them.
9531040	9535840	But the purposes need to be consistent. So you basically can create a story for your life and
9535840	9541440	make plans. And so we organize them all into hierarchies. And there is not a unique solution
9541440	9546240	for this. Some people eat to make art and other people make art to eat. And they might end up
9546240	9550560	doing the same things, but they cooperate in very different ways. Because they are ultimate goals.
9551280	9555440	Are different. And we cooperate based on shared purpose. Everything else that is not
9555440	9562080	cooperation on shared purpose is transactional. I don't think I understood the last piece of
9564480	9569360	achieving the homeostasis. Are you distinguishing between the experience of emotion and the
9569360	9576720	expression of emotion? Of course. So the experience of emotion is a feeling. And in this sense,
9576720	9582000	what you feel is an appraisal that your perceptual system has made of the situation at hand. And it
9582000	9589200	makes this based on your motivation and on your estimates, not your, but of the subconscious
9589200	9594240	geometric parts of your mind that assess the situation in the world with something like a
9594240	9599520	neural network. And this neural network is making itself known to the symbolic parts of your mind,
9600160	9606400	to your conscious attention via mapping them as features into a space. So what you will feel
9606480	9611360	about your emotion is a projection usually into your body map. So you might feel anxiety in your
9611360	9617360	solar plexus. And you might feel it as a contraction, which is all geometry, right? Your body map is
9617360	9622880	the space that is always instantiated and always available. So it's a very obvious cheat if your
9625040	9629440	non symbolic parts of your brain try to talk to your symbolic parts of your brain to map
9629440	9633600	the feelings into the body map. And then you perceive them as pleasant and unpleasant,
9633600	9638000	depending on whether the appraisal has a negative or positive valence. And then you have
9638000	9642800	different features of them that give you more knowledge about the nature of what you're feeling.
9642800	9646400	So for instance, when you feel connected to other people, you typically feel this in your
9646400	9651120	chest region around your heart. And you feel this is an expansive feeling in which you're
9651120	9656720	reaching out, right? And it's very intuitive to encode it like this. That's why it's encoded
9656720	9660400	like this for most people. It's encoded. It's a code. It's a code in which the non symbolic
9660400	9664960	parts of your mind talk to the symbolic ones. And then the expression of emotion is then the
9664960	9670320	final step that could be sort of gestural or visual and so on. That's part of the communication.
9670320	9675360	This probably evolved as part of an adversarial communication. So as soon as you started to
9675360	9680400	observe the facial expression and posture of others to understand what emotional state they are in,
9680400	9685200	others started to use this as signaling and also to subvert your model of their emotional state.
9685200	9689360	So we now look at the inflections, at the difference between the standard phase that
9689360	9693120	they're going to make in this situation, right? When you are at a funeral, everybody expects
9693120	9697200	you to make a solemn face. But the solemn face doesn't express whether you're sad or not. It
9697200	9702080	just expresses that you understand what face you have to make at a funeral. Nobody should know that
9702080	9707280	you are triumphant. So when you try to read the emotion of another person, you try to look at the
9707280	9715520	delta between a truly sad expression and the things that are animated, mating this face behind
9715520	9721120	the curtain. So the interesting thing is, so having done these, having done this podcast
9721840	9728320	and the video component, one of the things I've learned is that now I'm Russian and I just don't
9728320	9735040	know how to express emotion on my face when I see that as weakness. But whatever the people look to
9735040	9742240	me after you say something, they look to my face to help them see how they should feel about what
9742240	9747840	we said, which is fascinating because then they'll often comment on why did you look bored or why did
9747840	9752880	you particularly enjoy that part or why did you whatever. It's a kind of interesting, it makes
9752880	9759200	me cognizant of I'm part, like you're basically saying a bunch of brilliant things, but I'm part
9759920	9766400	of the play that you're the key actor and by making my facial expressions and therefore
9766400	9772640	telling the narrative of what the big point is, which is fascinating, makes me cognizant that
9772640	9777440	I'm supposed to be making facial expressions. Even this conversation is hard because my preference
9777440	9783840	will be to wear a mask with sunglasses to where I could just listen. Yes, I understand this because
9783840	9789360	it's intrusive to interact with others this way and basically Eastern European society have a
9789360	9795360	taboo against that and especially Russia, the further you go to the east and in the US it's
9795360	9801040	the opposite, you're expected to be hyperanimated in your face and you're also expected to show
9801040	9808160	positive effect. And if you show positive effect without a good reason in Russia, people will
9808960	9816800	think you are a stupid, unsophisticated person. Exactly and here positive effect without reason
9817680	9822880	goes either appreciate or go unnoticed. No, it's the default, it's being expected,
9822960	9830240	everything is amazing. Have you seen this Lego movie? No, there was a diagram where somebody
9830240	9837120	gave the appraisals that exist in US and Russia, so you have your bell curve and the lower 10%
9837840	9848640	in US are, it's a good start. Everything above the lowest 10% is amazing. And for Russians,
9848640	9856240	everything below the top 10% is terrible. And then everything except the top percent is,
9856240	9859840	I don't like it and the top percent is even so.
9863520	9870400	Yeah, it's funny, but it's kind of true. There's a deeper aspect to this. It's also how we construct
9870400	9876960	meaning in the US. Usually you focus on the positive aspects and you just suppress the negative
9876960	9885040	aspects. And in our Eastern European traditions, we emphasize the fact that if you hold something
9885040	9889040	above the water line, you also need to put something below the water line because existence
9889040	9895360	by itself is as best neutral. Right, that's the basic intuition, at best neutral, or it could
9895360	9899520	be just suffering, the default. There are moments of beauty, but these moments of beauty
9899520	9905280	are inextricably linked to the reality of suffering. And to not acknowledge the reality
9905280	9909200	of suffering means that you are really stupid and unaware of the fact that basically every
9909200	9917120	conscious being spends most of the time suffering. Yeah, you just summarized the ethos of the Eastern
9917120	9922400	Europe. Yeah, most of life is suffering with occasional moments of beauty. And if your
9922400	9928720	facial expressions don't acknowledge the abundance of suffering in the world and in existence itself,
9928720	9934960	then you must be an idiot. It's an interesting thing when you raise children in the US and you
9935360	9940240	in some sense preserve the identity of the intellectual and cultural traditions that
9940240	9946320	are embedded in your own families. And your daughter asks you about Ariel the mermaid and
9946320	9952720	asks you, why is Ariel not allowed to play with the humans? And you tell her the truth. She's a
9952720	9957520	siren. Sirens eat people. You don't play with your foot. It does not end well. And then you tell her
9957520	9961760	the original story, which is not the one by Anderson, which is the romantic one. And there's
9961760	9970640	a much darker one, the Ondine story. What happened? So Ondine is a mermaid or a water woman. She lives
9970640	9975440	on the ground of a river and she meets this prince and they fall enough. And the prince really,
9975440	9980800	really wants to be with her. And she says, okay, but the deal is you cannot have any other woman.
9980800	9984240	If you marry somebody else, even though you cannot be with me, because obviously you cannot breathe
9984240	9989760	underwater and have other things to do than managing your kingdom with you up here, you will die.
9990720	9995040	And eventually, after a few years, he falls in love with some princess and marries her and
9995760	10001520	she shows up and quietly goes into his chamber and nobody is able to stop her or willing to do
10001520	10006480	so because she is fierce. And she comes quietly and sat out of his chamber and they ask her,
10007440	10010320	what has happened? What did you do? And she said, I kissed him to death.
10012000	10018640	Well done. And you know the Anderson story, right? In the Anderson story, the mermaid is playing
10018640	10023680	with this prince that she saves and she falls in love with him and she cannot live out there. So
10023680	10030560	she is giving up her voice and her tale for a human-like appearance so she can walk among the
10030560	10035760	humans. But this guy does not recognize that she is the one that she would marry. Instead,
10035760	10040720	he marries somebody who has a kingdom and economical and political relationships to his
10040720	10053280	own kingdom and so on as he should. She dies. Yeah. Instead, Disney, the little mermaid story
10053280	10057040	has a little bit of a happy ending. That's the western, that's the American way.
10057040	10062000	My own problem is this, of course, that I read Oscar Wilde before I read the other things. So I'm
10062000	10066960	indoctrinated, inoculated with this romanticism. And I think that the mermaid is right. You
10066960	10071840	sacrifice your life for romantic love. That's what you do because if you are confronted with either
10071840	10077840	serving the machine and doing the obviously right thing under the economic and social and other
10077840	10086080	human incentives, that's wrong. You should follow your heart. So do you think suffering is fundamental
10086800	10092240	to happiness along these lines? No. Suffering is the result of caring about things that you cannot
10092240	10097120	change. And if you are able to change what you care about to those things that you can change,
10097120	10103840	you will not suffer. Would you then be able to experience happiness? Yes. But happiness itself
10103840	10108800	is not important. Happiness is like a cookie. When you are a child, you think cookies are very
10108800	10112320	important and you want to have all the cookies in the world. You look forward to being an adult
10112320	10116720	because then you have as many cookies as you want, right? Yes. But as an adult, you realize a
10116720	10121280	cookie is a tool. It's a tool to make you eat vegetables. And once you eat your vegetables
10121280	10125120	anyway, you stop eating cookies for the most part because otherwise you will get diabetes
10125120	10129360	and will not be around for your kids. Yes. But then the cookie, the scarcity of a cookie.
10130080	10135680	If scarcity is enforced, nevertheless, the pleasure comes from the scarcity. Yes. But
10135680	10140400	the happiness is a cookie that your brain bakes for itself. It's not made by the environment.
10140400	10144400	The environment cannot make you happy. It's your appraisal of the environment that makes you happy.
10145040	10148480	And if you can change the appraisal of the environment that you can learn to, then you
10148480	10152720	can create arbitrary states of happiness. And some meditators fall into this trap. So they
10152720	10156560	discover the room, this basement room in their brain where the cookies are made,
10156560	10161040	and they indulge in stuff themselves. And after a few months, it gets really old and the big crisis
10161040	10166640	of meaning comes. Because they thought before that their unhappiness was the result of not
10166640	10170560	being happy enough. So they fixed this, right? They can release the newer transmitters at will
10170560	10177040	if they train. And then the crisis of meaning pops up at a deeper layer. And the question is,
10177120	10180960	why do I live? How can I make a sustainable civilization that is meaningful to me?
10180960	10184560	How can I insert myself into this? And this was the problem that you couldn't solve in the first
10184560	10193840	place. But at the end of all this, let me then ask that same question. What is the answer to that?
10193840	10199920	What could the possible answer be of the meaning of life? What could an answer be? What is it to you?
10200720	10204480	I think that if you look at the meaning of life, you look at what the cell is.
10205120	10210320	The life is the cell. Yes, or this principle, the cell. It's this
10211120	10215280	self-organizing thing that can participate in evolution. In order to make it work,
10215280	10220400	it's a molecular machine. It needs a self-replicator and like entropy extractor and a Turing machine.
10220400	10224720	If any of these parts is missing, you don't have a cell and it is not living, right? And life is
10224720	10229520	basically the emergent complexity over that principle. Once you have this intelligent super
10229520	10234080	molecule, the cell, there is very little that you cannot make it do. It's probably the optimal
10234080	10239600	computronium, especially in terms of resilience. It's very hard to sterilize a planet once it's
10239600	10247680	infected with life. So it's active function of these three components of the supercell of cell
10247680	10253280	is present in the cell, is present in us, and it's just... We are just an expression of the cell.
10253280	10258320	It's a certain layer of complexity in the organization of cells. So in a way, it's
10258320	10263200	tempting to think of the cell as a von Neumann probe. If you want to build intelligence on
10263200	10268640	other planets, the best way to do this is to infect them with cells and wait for long enough.
10268640	10272800	And with a reasonable chance, the stuff is going to evolve into an information processing
10272800	10279520	principle that is general enough to become sentient. Well, that idea is very akin to the same
10279520	10283040	dream and beautiful ideas that are expressed to cellular automata in their most simple
10283040	10289760	mathematical form. If you just inject the system with some basic mechanisms of replication and
10290160	10295440	basic rules, amazing things would emerge. And the cell is able to do something that James
10295440	10301120	Trady calls existential design. He points out that in technical design, we go from the outside in.
10301120	10305200	We work in a highly controlled environment in which everything is deterministic, like our
10305200	10311520	computers, our labs, or our engineering workshops. And then we use this determinism to implement a
10311520	10316240	particular kind of function that we dream up and that seamlessly interfaces with all the other
10316240	10320880	deterministic functions that we already have in our world. So it's basically from the outside in.
10321680	10328160	And biological systems designed from the inside out as seed will become a seedling by taking some
10328160	10334320	of the relatively unorganized matter around it and turn it into its own structure and thereby
10334320	10339200	subdue the environment. And cells can cooperate if they can rely on other cells having a similar
10339200	10345520	organization that is already compatible. But unless that's there, the cell needs to divide to
10345520	10350400	create that structure by itself. So it's a self-organizing principle that works on a
10350400	10356640	somewhat chaotic environment. And the purpose of life in the sense is to produce complexity.
10356640	10360480	And the complexity allows you to harvest like entropy gradients that you couldn't harvest
10360480	10365520	without the complexity. And in this sense, intelligence and life are very strongly connected
10365520	10370480	because the purpose of intelligence is to allow control and the conditions of complexity. So
10370480	10376720	basically you shift the boundary between the ordered systems into the realm of chaos. You
10376720	10382080	build bridgeheads into chaos with complexity. And this is what we are doing. This is not
10382080	10385840	necessarily a deeper meaning. I think the meaning that we have priors for that we are
10385840	10389920	involved for outside of the priors, there is no meaning. Meaning only exists if a mind projects
10389920	10397280	it. That is probably civilization. I think that what feels most meaningful to me is to try to
10397280	10403200	build and maintain a sustainable civilization. And taking a slight step out outside of that,
10403200	10414720	we talked about a man with a beard and God. But something, some mechanism perhaps must have
10414720	10422480	planted the seed, the initial seed of the cell. Do you think there is a God? What is a God?
10422560	10425760	And what would that look like? So if there was no spontaneous
10425760	10432720	abiogenesis, in the sense that the first cell formed by some happy random accidents where
10432720	10435840	the molecules just happen to be in the right constellation to each other. But there could
10435840	10442000	also be the mechanism of, that allows for the random, I mean, there's like turtles all the way
10442000	10446640	down. There seems to be, there has to be a head turtle at the bottom. Let's consider something
10446640	10451600	really wild. Imagine, is it possible that a gas giant could become intelligent?
10452240	10457360	What would that involve? So imagine that you have vortices that spontaneously emerge on the gas
10457360	10462560	giants like big storm systems that endure for thousands of years. And some of these storm
10462560	10467040	systems produce electromagnetic fields because some of the clouds are ferromagnetic or something.
10467040	10472400	And as a result, they can change how certain clouds react rather than other clouds and thereby
10472480	10476640	produce some self-sabilizing patterns that eventually to regulation feedback loops,
10476640	10482320	nested feedback loops and control. So imagine you have such a thing that basically has emergent,
10482320	10486720	self-sustaining, self-organizing complexity. And at some point, this fakes up and realizes
10486720	10491360	and basically lambs Solaris. I am a sinking planet, but I will not replicate because I
10491360	10497120	can recreate the conditions of my own existence somewhere else. I'm just basically an intelligence
10497120	10503040	that has spontaneously formed because it could. And now it builds a von Neumann probe. And the
10503040	10507040	best von Neumann probe for such a thing might be the cell. So maybe it, because it's very,
10507040	10512000	very clever and very enduring, creates cells and sends them out. And one of them has infected our
10512000	10515840	planet. And I'm not suggesting that this is the case, but it would be compatible with the
10515840	10521440	Prince Birmingham hypothesis. And with my intuition that our biogenesis is very unlikely.
10521440	10525600	It's possible, but you probably need to roll the cosmic dice very often,
10525600	10528240	maybe more often than there are planetary surfaces. I don't know.
10529440	10537360	So God is just a large enough, a system that's large enough that allows randomness.
10537360	10541440	Now, I don't think that God has anything to do with creation. I think it's a mistranslation
10541440	10546800	of the Talmud into the Catholic mythology. I think that Genesis is actually the childhood
10546800	10552960	memories of a God. So the, when, sorry, the Genesis is the childhood memories of a God.
10552960	10560240	It's basically a mind that is remembering how it came into being. And we typically interpret
10560240	10566800	Genesis as the creation of a physical universe by a supernatural being. And I think when you'll
10566800	10573280	read it, there is light and darkness that is being created. And then you discover sky and
10573280	10580080	ground, create them. You construct the plants and the animals, and you give everything their
10580160	10584960	names and so on. That's basically cognitive development. It's a sequence of steps that
10584960	10588880	every mind has to go through when it makes sense of the world. And when you have children, you
10588880	10594400	can see how initially they distinguish light and darkness, and then they make out directions in it,
10594400	10597760	and they discover sky and ground, and they discover the plants and the animals, and they
10597760	10602080	give everything their name. And it's a creative process that happens in every mind, because it's
10602080	10606480	not given, right? Your mind has to invent these structures to make sense of the patterns on your
10606480	10613040	retina. Also, if there was some big nerd who set up a server and runs this world on it, this would
10613040	10617840	not create a special relationship between us and the nerd. This nerd would not have the magical
10617840	10624160	power to give meaning to our existence, right? So this equation of a creator God with the God of
10624160	10630320	meaning is laid off hand. You shouldn't do it. The other one that is done in Catholicism is the
10630320	10635520	equation of the first mover, the prime mover of Aristotle, which is basically the automaton that
10635520	10641200	runs the universe. Aristotle says, if things are moving and things seem to be moving here,
10641200	10645440	something must move them, right? If something moves them, something must move the thing that
10645440	10650320	is moving it. So there must be a prime mover. This idea to say that this prime mover is a
10650320	10656800	supernatural being is complete nonsense, right? It's an automaton in the simplest case. So we
10656800	10662640	have to explain the enormity that this automaton exists at all. But again, we don't have any
10662640	10668480	possibility to infer anything about its properties except that it's able to produce change in
10668480	10673680	information, right? So there needs to be some kind of computational principle. This is all there is.
10673680	10678160	But to say this automaton is identical again with the creator of first cause or with the
10678160	10685520	thing that gives meaning to our life is confusion. Now, I think that what we perceive is the higher
10685520	10690800	being that we are part of. And the higher being that we are part of is the civilization. It's the
10690800	10696800	thing in which we have a similar relationship as the cell has to our body. And we have this prior
10696800	10702800	because we have evolved to organize in these structures. So basically, the Christian God
10702800	10707520	in its natural form without the mythology, if you undress it, is basically the platonic form of
10707520	10715280	the civilization. Is the ideal? Yes, it's this ideal that you try to approximate when you interact
10715280	10722640	with others, not based on your incentives, but on what you think is right. Wow, we covered a lot
10722640	10728640	of ground. And we left with one of my favorite lines. And there's many, which is happiness
10729520	10738240	is a cookie that the brain bakes itself. It's been a huge honor and a pleasure to talk to you.
10738240	10743920	I'm sure our paths will cross many times again. Josh, thank you so much for talking today.
10743920	10748240	Thank you, Lex. It was so much fun. I enjoyed it. Awesome.
10774080	10778560	Supporting on Patreon or simply connect with me on Twitter at Lex Friedman.
10779600	10782640	And yes, try to figure out how to spell it without the E.
10783600	10787120	And now let me leave you with some words of wisdom from Yoshabach.
10788320	10793760	If you take this as a computer game metaphor, this is the best level for humanity to play.
10794560	10800960	And this best level happens to be the last level as it happens against the backdrop
10800960	10804640	of a dying world. But it's still the best level.
10805840	10816880	Thank you for listening and hope to see you next time.
