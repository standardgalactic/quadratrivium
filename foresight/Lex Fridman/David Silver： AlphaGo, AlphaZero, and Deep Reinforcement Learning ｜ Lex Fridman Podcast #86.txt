The following is a conversation with David Silver, who leads the Reinforcement Learning
Research Group at DeepMind, and was the lead researcher on AlphaGo, AlphaZero, and co-led
the AlphaStar and MuZero efforts, and a lot of important work in reinforcement learning
in general.
I believe AlphaZero is one of the most important accomplishments in the history of artificial
intelligence, and David is one of the key humans who brought AlphaZero to life together
with a lot of other great researchers at DeepMind.
He's humble, kind, and brilliant.
We were both jet lagged, but didn't care and made it happen.
It was a pleasure and truly an honor to talk with David.
This conversation was recorded before the outbreak of the pandemic.
For everyone feeling the medical, psychological, and financial burden of this crisis, I'm
sending love your way.
Stay strong, or in this together, we'll beat this thing.
This is the Artificial Intelligence Podcast.
If you enjoy it, subscribe on YouTube, review it with 5 stars on Apple Podcasts, support
on Patreon, or simply connect with me on Twitter at Lex Freedman, spelled F-R-I-D-M-A-N.
As usual, I'll do a few minutes of ads now and never any ads in the middle that can break
the flow of the conversation.
I hope that works for you and doesn't hurt the listening experience.
Quick summary of the ads.
Two sponsors.
Masterclass and Cash App.
Please consider supporting the podcast by signing up to masterclass and masterclass.com
slash lex and downloading Cash App and using code Lex Podcast.
This show is presented by Cash App, the number one finance app in the App Store.
When you get it, use code Lex Podcast.
Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with
as little as $1.
Since Cash App allows you to buy Bitcoin, let me mention that cryptocurrency in the
context of the history of money is fascinating.
I recommend Ascent of Money as a great book on this history.
Debits and credits on ledgers started around 30,000 years ago.
The US Dollar created over 200 years ago and Bitcoin, the first decentralized cryptocurrency,
released just over 10 years ago.
So given that history, cryptocurrency is still very much in its early days of development,
but it's still aiming to and just might redefine the nature of money.
So again, if you get Cash App from the App Store or Google Play and use the code Lex
Podcast, you get $10 and Cash App will also donate $10 the first, an organization that
is helping to advance robotics and STEM education for young people around the world.
This show is sponsored by Masterclass.
Sign up at masterclass.com slash Lex to get a discount and to support this podcast.
In fact, for a limited time now, if you sign up for an All Access Pass for a year, you
get to get another All Access Pass to share with a friend.
Buy one, get one free.
When I first heard about Masterclass, I thought it was too good to be true.
For $180 a year, you get an All Access Pass to watch courses from to list some of my favorites.
Chris Hatfield on space exploration, Neil deGrasse Tyson on scientific thinking communication,
Will Wright, the creator of SimCity and Sims, on game design, Jane Goodall on conservation,
Carl Santana on guitar, his song Europa could be the most beautiful guitar song ever written.
Gary Kasparov on chess, Daniel Nagrano on poker, and many, many more.
Chris Hatfield explaining how rockets work and the experience of being launched into
space alone is worth the money.
For me, the key is to not be overwhelmed by the abundance of choice, pick three courses
you want to complete, watch each of them all the way through.
It's not that long, but it's an experience that will stick with you for a long time.
I promise.
It's easily worth the money.
You can watch it on basically any device.
Once again, sign up on masterclass.com slash Lex to get a discount and to support this
podcast.
And now here's my conversation with David Silver.
What was the first program you ever written and what programming language?
Do you remember?
I remember very clearly, yeah, my parents brought home this BBC model B microcomputer.
It was just this fascinating thing to me.
I was about seven years old and couldn't resist just playing around with it.
So I think first program ever was writing my name out in different colors and getting
it to loop and repeat that.
There was something magical about that, which just led to more and more.
How did you think about computers back then?
The magical aspect of it, that you can write a program and there's this thing that you
just gave birth to that's able to create visual elements and live in its own.
Or did you not think of it in those romantic notions?
Was it more like, oh, that's cool.
I can solve some puzzles.
It was always more than solving puzzles.
It was something where there was this limitless possibilities once you have a computer in
front of you.
You can do anything with it.
I used to play with Lego with the same feeling.
You can make anything you want out of Lego, but even more so with a computer.
You're not constrained by the amount of kit you've got.
And so I was fascinated by it and started pulling out the user guide and the advanced
user guide and then learning.
So I started in basic and then later 6502.
My father also became interested in this machine and gave up his career to go back to school
and study for a master's degree in artificial intelligence, funnily enough, at Essex University
when I was seven.
So I was exposed to those things at an early age.
He showed me how to program in Prologue and do things like querying your family tree.
And those are some of my earliest memories of trying to figure things out on a computer.
Those are the early steps in computer science programming, but when did you first fall in
love with artificial intelligence or with the ideas, the dreams of AI?
I think it was really when I went to study at university.
So I was an undergrad at Cambridge and studying computer science.
And I really started to question, you know, what really are the goals?
What's the goal?
Where do we want to go with computer science?
And it seemed to me that the only step of major significance to take was to try and recreate
something akin to human intelligence.
If we could do that, that would be a major leap forward.
And that idea certainly wasn't the first to have it, but it, you know, nestled within
me somewhere and became like a bug, you know, I really wanted to crack that problem.
So you thought it was, like, you had a notion that this is something that human beings can do,
that it is possible to create an intelligent machine?
Well, I mean, unless you believe in something metaphysical, then what are our brains doing?
Well, at some level, their information processing systems, which are able to take whatever
information is in there, transform it through some form of program and produce some kind
of output, which enables that human being to do all the amazing things that they can
do in this incredible world.
So then, do you remember the first time you've written a program that, because you also had
an interest in games, do you remember the first time you were in a program that beat
you in a game, that more beat you at anything, sort of achieved super David Silver level
performance?
So I used to work in the games industry.
So for five years, I programmed games for my first job.
So it was an amazing opportunity to get involved in a startup company.
And so I was involved in building AI at that time.
And so for sure, there was a sense of building, handcrafted, what people used to call AI in
the games industry, which I think is not really what we might think of as AI in its fullest
sense, but something which is able to take actions in a way which makes things interesting
and challenging for the human player.
And at that time, I was able to build these handcrafted agents, which in certain limited
cases could do things which were able to do better than me, but mostly in these kind of
twitch-like scenarios where they were able to do things faster or because they had some
pattern which was able to exploit repeatedly.
I think if we're talking about real AI, the first experience for me came after that when
I realized that this path I was on wasn't taking me towards, it wasn't dealing with
that bug which I still had inside me to really understand intelligence and try and solve it.
Everything people were doing in games was short-term fixes rather than long-term vision.
So I went back to study for my PhD, which was, finally enough, trying to apply reinforcement
learning to the game of Go.
And I built my first Go program using reinforcement learning, a system which would, by trial and
error, play against itself and was able to learn which patterns were actually helpful
to predict whether it was going to win or lose the game and then choose the moves that
led to the combination of patterns that would mean that you're more likely to win.
That system, that system beat me.
And how did that make you feel?
It made me feel good.
It's a mix of a sort of excitement and was there a tinge of sort of almost like a fearful
awe?
It's like in 2001 Space Odyssey kind of realizing that you've created something that's achieved
human-level intelligence in this one particular little task.
And in that case, I suppose neural networks weren't involved.
There were no neural networks in those days.
This was pre-deep learning revolution, but it was a principled self-learning system based
on a lot of the principles which people still use in deep reinforcement learning.
How did I feel?
I think I found it immensely satisfying that a system which was able to learn from first
principles for itself was able to reach the point that it was understanding this domain
better than I could and able to outwit me.
I don't think it was a sense of awe.
It was a sense that satisfaction, that something I felt should work, had worked.
To me AlphaGo, and I don't know how else to put it, but to me AlphaGo and AlphaGo Zero
mastering the game of Go is, again, to me the most profound and inspiring moment in the
history of artificial intelligence.
So you're one of the key people behind this achievement.
And I'm Russian, so I really felt the first sort of seminal achievement when Deep Blue
be Gare Kasparov in 1997.
So as far as I know, the AI community at that point largely saw the game of Go as unbeatable
in AI using the state-of-the-art to brute-force search methods.
Even if you consider, at least the way I saw it, even if you consider arbitrary exponential
scaling of compute, Go would still not be solvable, hence why it was thought to be impossible.
So given that the game of Go was impossible to master, when was the dream for you, you
just mentioned your PhD thesis of building the system that plays Go, when was the dream
for you that you could actually build a computer program that achieves the world class, not
necessarily beats the world champion, but achieves that kind of level of playing Go?
First of all, thank you.
That was very kind words.
Funnily enough, I just came from a panel where I was actually in a conversation with
Gare Kasparov and Murray Campbell, who was the author of Deep Blue, and it was their
first meeting together since the match, so I'm just acquired yesterday, so I'm literally
fresh from that experience.
So these are amazing moments when they happen, but where did it all start?
Well, for me, it started when I became fascinated in the game of Go.
So Go, for me, I've grown up playing games, I've always had a fascination in board games.
I played chess as a kid, I played Scrabble as a kid.
When I was at university, I discovered the game of Go, and to me, it just blew all of
those other games out of the water.
It was just so deep and profound in its complexity with endless levels to it.
What I discovered was that I could devote endless hours to this game, and I knew in
my heart of hearts that no matter how many hours I would devote to it, I would never
become a grandmaster, or there was another path, and the other path was to try and understand
how you could get some other intelligence to play this game better than I would be able
to.
So even in those days, I had this idea that, what if it was possible to build a program
that could crack this?
And as I started to explore the domain, I discovered that this was really the domain
where people felt deeply that if progress could be made in Go, it would really mean
a giant leap forward for AI.
It was the challenge where all other approaches had failed.
This is coming out of the era you mentioned, which was in some sense the golden era for
the classical methods of AI, like heuristic search.
In the 90s, they all fell one after another, not just chess with deep blue, but checkers,
batgammon, Othello.
There were numerous cases where systems built on top of heuristic search methods with these
high-performance systems had been able to defeat the human world champion in each of
those domains.
And yet, in that same time period, there was a million-dollar prize available for the
game of Go, for the first system to be a human professional player.
And at the end of that time period, at year 2000, when the prize expired, the strongest
Go program in the world was defeated by a nine-year-old child.
When that nine-year-old child was giving nine free moves to the computer at the start of
the game to try and even things up.
And computer Go expert beat that same strongest program with 29 handicap stones, 29 free moves.
So that's what the state of affairs was when I became interested in this problem in around
2003 when I started working on computer Go.
There was nothing.
There was just very, very little in the way of progress towards meaningful performance
again at anything approaching human level.
And so it wasn't through lack of effort people have tried many, many things.
And so there was a strong sense that something different would be required for Go than had
been needed for all of these other domains where AI had been successful.
And maybe the single clearest example is that Go, unlike those other domains, had this kind
of intuitive property that a Go player would look at a position and say, hey, here's this
mess of black and white stones.
But from this mess, oh, I can predict that this part of the board has become my territory,
this part of the board has become your territory, and I've got this overall sense that I'm going
to win and that this is about the right move to play.
And that intuitive sense of judgment of being able to evaluate what's going on in a position,
it was pivotal to humans being able to play this game and something that people had no
idea how to put into computers.
So this question of how to evaluate a position, how to come up with these intuitive judgments
was the key reason why Go was so hard in addition to its enormous search space and the reason
why methods which had succeeded so well elsewhere failed in Go.
And so people really felt deep down that in order to crack Go, we would need to get something
akin to human intuition.
And if we got something akin to human intuition, we'd be able to solve many, many more problems
in AI.
So for me, that was the moment where it's like, okay, this is not just about playing
the game of Go.
This is about something profound.
And it was back to that bug which had been itching me all those years.
This is the opportunity to do something meaningful and transformative and I guess a dream was
born.
That's a really interesting way to put it.
So almost this realization that you need to find formulate Go as a kind of a prediction
problem versus a search problem was the intuition.
I mean, maybe that's the wrong crude term, but to give it the ability to kind of intuit
things about positional structure of the board.
Now, okay, but what about the learning part of it?
Did you have a sense that learning has to be part of the system?
Again, something that hasn't really, as far as I think, except with TD Gammon and the
90s with RL a little bit, hasn't been part of those day-to-day art game-playing systems?
So I strongly felt that learning would be necessary and that's why my PhD topic back
then was trying to apply reinforcement learning to the game of Go.
I'm not just learning of any type, but I felt that the only way to really have a system
to progress beyond human levels of performance wouldn't just be to mimic how humans do it,
but to understand for themselves.
How else can a machine hope to understand what's going on except through learning?
If you're not learning, what else are you doing?
Well, you're putting all the knowledge into the system and that just feels like something
which decades of AI have told us is maybe not a dead end, but certainly has a ceiling
to the capabilities.
It's known as the knowledge acquisition bottleneck.
The more you try to put into something, the more brittle the system becomes.
So you just have to have learning.
You have to have learning.
That's the only way you're going to be able to get a system which has sufficient knowledge
in it, millions and millions of pieces of knowledge, billions, trillions, of a form
that it can actually apply for itself and understand how those billions and trillions
of pieces of knowledge can be leveraged in a way which will actually lead it towards
its goal without conflict or other issues.
Yeah.
I mean, if I put myself back in that time, I just wouldn't think like that without a
good demonstration of RL.
I would think more in the symbolic AI, like the not learning, but sort of a simulation
of knowledge base, like a growing knowledge base, but it would still be sort of pattern-based,
like basically have little rules that you kind of assemble together into a large knowledge
base.
Well, in a sense, that was the state of the art back then.
So if you look at the Go programs which had been competing for this prize I mentioned,
they were an assembly of different specialized systems, some of which used huge amounts of
human knowledge to describe how you should play the opening, how you should all the different
patterns that were required to play well in the game of Go, end game theory, combinatorial
game theory, and combined with more principled search-based methods which were trying to
solve for particular sub-parts of the game, like life and death, connecting groups together,
all these amazing sub-problems that just emerged in the game of Go, there were different pieces
all put together into this collage which together would try and play against a human.
And although not all of the pieces were handcrafted, the overall effect was nevertheless still
brittle and it was hard to make all these pieces work well together.
And so really what I was pressing for and the main innovation of the approach I took
was to go back to first principles and say, well, let's back off that and try and find a
principled approach where the system can learn for itself just from the outcome, like, you know,
learn for itself if you try something, did that help or did it not help?
And only through that procedure can you arrive at knowledge which is verified,
the system has to verify it for itself, not relying on any other third party to say this
is right or this is wrong. And so that principle was already very important in those days,
that unfortunately we were missing some important pieces back then.
So before we dive into maybe discussing the beauty of reinforcement learning,
let's take a step back, we kind of skipped it a bit, but the rules of the game of Go,
what the elements of it perhaps contrasting to chess that sort of you really enjoy as a human
being and also that make it really difficult as a AI machine learning problem.
So the game of Go has remarkably simple rules. In fact, so simple that people have speculated
that if we were to meet alien life at some point that we wouldn't be able to communicate with them,
but we would be able to play Go with them, they probably have discovered the same ruleset.
So the game is played on a 19 by 19 grid, and you play on the intersections of the grid and
the players take turns. And the aim of the game is very simple, it's to surround as much territory
as you can as many of these intersections with your stones and to surround more than your opponent
does. And the only nuance to the game is that if you fully surround your opponent's piece,
then you get to capture it and remove it from the board and it counts as your own territory.
Now, from those very simple rules, immense complexity arises. There's kind of profound
strategies in how to surround territory, how to kind of trade off between making solid territory
yourself now, compared to building up influence that will help you acquire territory later in
the game, how to connect groups together, how to keep your own groups alive,
which patterns of stones are most useful compared to others.
There's just immense knowledge and human Go players have played this game for,
it was discovered thousands of years ago, and human Go players have built up this immense
knowledge base over the years. It's studied very deeply and played by something like 50 million
players across the world, mostly in China, Japan and Korea, where it's an important part of the
culture, so much so that it's considered one of the four ancient arts that was required by
Chinese scholars. So, there's a deep history there. But there's interesting quality. So,
if I were to compare to chess, chess is in the same way as it is in Chinese culture for Go,
and chess in Russia is also considered one of the sacred arts. So, if we contrast Go with
chess, there's interesting qualities about Go. Maybe you can correct me if I'm wrong, but the
evaluation of a particular static board is not as reliable. In chess, you can kind of assign
points to the different units, and it's kind of a pretty good measure of who's winning, who's
losing. It's not so clear. So, in the game of Go, you find yourself in a situation where
both players have played the same number of stones, actually captures a strong level of play
happen very rarely, which means that at any moment in the game, you've got the same number
of white stones and black stones. And the only thing which differentiates how well you're doing
is this intuitive sense of where are the territories ultimately going to form on this board?
And if you look at the complexity of a real Go position, it's mind-boggling that kind of
question of what will happen in 300 moves from now when you see just a scattering of 20
white and black stones intermingled. And so, that challenge is the reason why
position evaluation is so hard in Go compared to other games. In addition to that, it has an
enormous search space. So, there's around 10 to 170 positions in the game of Go. That's an
astronomical number. And that search space is so great that traditional heuristic search methods
that were so successful and things like Deep Blue and chess programs just kind of fall over in Go.
Which point did reinforcement learning enter your life, your research life, your way of thinking?
We just talked about learning, but reinforcement learning is a very particular kind of learning,
one that's both philosophically sort of profound, but also one that's pretty difficult to get to
work as if you look back in the early days. So, when did that enter your life and how did
that work progress? So, I had just finished working in the games industry at this startup
company. And I took a year out to discover for myself exactly which path I wanted to take. I
knew I wanted to study intelligence, but I wasn't sure what that meant at that stage. I really
didn't feel I had the tools to decide on exactly which path I wanted to follow.
So, during that year, I read a lot. And one of the things I read was Saturn and Bartow,
the sort of seminal textbook on an introduction to reinforcement learning. And when I read that
textbook, I just had this resonating feeling that this is what I understood intelligence to be.
And this was the path that I felt would be necessary to go down to make progress in AI.
So, I got in touch with Rich Saturn and asked him if he would be interested in supervising me
on a PhD thesis in computer go. And he basically said that if he's still alive, he'd be happy to.
But unfortunately, he'd been struggling with very serious cancer for some years. And he really
wasn't confident at that stage that he'd even be around to see the end of it. But fortunately,
that part of the story worked out very happily. And I found myself out there in Alberta. They've
got a great games group out there with a history of fantastic work in board games as well as Rich
Saturn, the father of RL. So, it was the natural place for me to go in some sense to study this
question. And the more I looked into it, the more strongly I felt that this wasn't just the path to
progress in computer go. But really, this was the thing I'd been looking for. This was
really an opportunity to frame what intelligence means. What are the goals of AI in a single
problem definition such that if we're able to solve that clear single problem definition,
in some sense, we've cracked the problem of AI? So, to you, reinforcement learning ideas,
at least sort of echoes of it, would be at the core of intelligence. It is at the core of intelligence.
And if we ever create a human level intelligence system, it would be at the core of that kind of
system. Let me say it this way that I think it's helpful to separate out the problem from the solution.
So, I see the problem of intelligence, I would say it can be formalized as the reinforcement
learning problem. And that that formalization is enough to capture most, if not all of the things
that we mean by intelligence, that they can all be brought within this framework and gives us
a way to access them in a meaningful way that allows us as scientists to understand intelligence
and us as computer scientists to build them. And so, in that sense, I feel that it gives us a path,
maybe not the only path, but a path towards AI. And so, do I think that any system in the future
that's, you know, solved AI would have to have RL within it? Well, I think if you ask that,
you're asking about the solution methods. I would say that if we have such a thing,
it would be a solution to the RL problem. Now, what particular methods have been used to get there?
Well, we should keep an open mind about the best approaches to actually solve any problem.
And, you know, the things we have right now for reinforcement learning, maybe they, maybe
I believe they've got a lot of legs, but maybe we're missing some things. Maybe there's going
to be better ideas. I think we should keep, you know, let's remain modest and we're at the early
days of this field and there are many amazing discoveries ahead of us. For sure. The specifics,
especially of the different kinds of RL approaches currently, there could be other things that fall
into the very large umbrella of RL. But if it's, if it's okay, can we take a step back and kind of
ask the basic question of what is, do you, reinforcement learning? So, reinforcement learning
is the study and the science and the problem of intelligence in the form of an agent that
interacts with an environment. So, the problem you're trying to solve is represented by some
environment like the world in which that agent is situated. And the goal of RL is clear that the
agent gets to take actions. Those actions have some effect on the environment and the environment
gives back an observation to the agent saying, you know, this is what you see or sense. And one
special thing which it gives back is called the reward signal, how well it's doing in the environment.
And the reinforcement learning problem is to simply take actions over time so as to maximize
that reward signal. So, a couple of basic questions. What types of RL approaches are there? So,
I don't know if there's a nice brief inwards way to paint the picture of sort of value-based,
model-based, policy-based reinforcement learning. Yeah. So, now if we think about, okay, so there's
this ambitious problem definition of RL. It's really, you know, it's truly ambitious. It's
trying to capture and encircle all of the things in which an agent interacts with an environment and
say, well, how can we formalize and understand what it means to crack that? Now, let's think about
the solution method. Well, how do you solve a really hard problem like that? Well, one approach
you can take is to decompose that very hard problem into pieces that work together to solve
that hard problem. And so, you can kind of look at the decomposition that's inside the agent's
head, if you like, and ask, well, what form does that decomposition take? And some of the most
common pieces that people use when they're kind of putting the solution method together,
some of the most common pieces that people use are whether or not that solution has a value
function. That means, is it trying to predict, explicitly trying to predict how much reward
it will get in the future? Does it have a representation of a policy? That means something
which is deciding how to pick actions. Is that decision-making process explicitly represented?
And is there a model in the system? Is there something which is explicitly trying to predict
what will happen in the environment? And so, those three pieces are, to me, some of the most
common building blocks. And I understand the different choices in RL as choices of whether
or not to use those building blocks when you're trying to decompose the solution.
Should I have a value function represented? Should I have a policy represented? Should I have a
model represented? And there are combinations of those pieces and, of course, other things that
you could add into the picture as well. But those three fundamental choices give rise to some of
the branches of RL with which we are very familiar. And so, those, as you mentioned,
there is the choice of what's specified or modeled explicitly. And the idea is that
all of these are somehow implicitly learned within the system. So, it's almost a choice of
how you approach a problem. Do you see those as fundamental differences or these almost like
small specifics, like the details of how you solve the problem, but they're not fundamentally
different from each other? I think the fundamental idea is maybe at the higher level, the fundamental
idea is the first step of the decomposition is really to say, well, how are we really going to
solve any kind of problem where you're trying to figure out how to take actions and just from this
stream of observations, you know, you've got some agent situated in its sensory motor stream and
getting all these observations in, getting to take these actions, and what should it do? How
can you even broach that problem? You know, maybe the complexity of the world is so great
that you can't even imagine how to build a system that would understand how to deal with that.
And so, the first step of this decomposition is to say, well, you have to learn. The system has to
learn for itself. And so, note that the reinforcement learning problem doesn't actually stipulate
that you have to learn, like you could maximize your rewards without learning, it would just
wouldn't do a very good job of it. So, learning is required because it's the only way to achieve
good performance in any sufficiently large and complex environment. So, that's the first step.
And so, that step gives commonality to all of the other pieces, because now you might ask, well,
what should you be learning? What does learning even mean? You know, in this sense,
learning might mean, well, you're trying to update the parameters of some system, which
is then the thing that actually picks the actions. And those parameters could be
representing anything. They could be parameterizing a value function or a model or a policy.
And so, in that sense, there's a lot of commonality in that whatever is being represented there is
the thing which is being learned, and it's being learned with the ultimate goal of maximizing rewards.
But the way in which you decompose the problem is really what gives the semantics to the whole
system. Like, are you trying to learn something to predict well, like a value function or a model?
Are you learning something to perform well, like a policy? And the form of that objective,
like, is kind of giving the semantics to the system. And so, it really is, at the next level
down, a fundamental choice. And we have to make those fundamental choices as system designers,
or enable our algorithms to be able to learn how to make those choices for themselves.
So, then the next step you mentioned, the very first thing you have to deal with is,
can you even take in this huge stream of observations and do anything with it? So,
the natural next basic question is, what is the, what is deeper enforcement learning?
And what is this idea of using neural networks to deal with this huge incoming stream?
So, amongst all the approaches for reinforcement learning, deep reinforcement learning is one
family of solution methods that tries to utilize powerful representations that are offered by
neural networks to represent any of these different components of the solution, of the agent.
Like, whether it's the value function, or the model, or the policy, the idea of deep learning
is to say, well, here's a powerful toolkit that's so powerful that it's universal in the sense that
it can represent any function, and it can learn any function. And so, if we can leverage that
universality, that means that whatever we need to represent for our policy, or for our value
function for a model, deep learning can do it. So, that deep learning is one approach
that offers us a toolkit that is, has no ceiling to its performance, that as we start to put more
resources into the system, more memory and more computation, and more data, more experience of
more interactions with the environment, that these are systems that can just get better and better
and better at doing whatever the job is they've asked them to do, whatever we've asked that
function to represent, it can learn a function that does a better and better job of representing
that knowledge, whether that knowledge be estimating how well you're going to do in the
world, the value function, whether it's going to be choosing what to do in the world, the policy,
or whether it's understanding the world itself, what's going to happen next, the model.
Nevertheless, the fact that neural networks are able to learn incredibly complex representations
that allow you to do the policy, the model, or the value function is, at least to my mind,
exceptionally beautiful and surprising. Was it surprising to you? Can you still
believe it works as well as it does? Do you have good intuition about why it works at all
and works as well as it does? I think let me take two parts to that question. I think
it's not surprising to me that the idea of reinforcement learning
works because in some sense, I feel it's the only thing which can, ultimately,
and so I feel we have to address it and there must be success as possible because we have
examples of intelligence and it must at some level be able to possible to acquire experience and use
that experience to do better in a way which is meaningful to environments of the complexity
that humans can deal with. It must be. Am I surprised that our current systems can do as well
as they can do? I think one of the big surprises for me and a lot of the community
is really the fact that deep learning can continue to perform so well despite the
facts that these neural networks that they're representing have these incredibly nonlinear
bumpy surfaces which to our low-dimensional intuitions make it feel like surely you're
just going to get stuck and learning will get stuck because you won't be able to make any
further progress and yet the big surprise is that learning continues and these what appear to be
local optima turn out not to be because in high dimensions when we make really big neural nets
there's always a way out and there's a way to go even lower and then
you're still not a local optima because there's some other pathway that will take you out and
take you lower still and so no matter where you are learning can proceed and do better and better
and better without bound and so that is a surprising and beautiful property of neural nets
which I find elegant and beautiful and and somewhat shocking that it turns out to be the case
as you said which I really like to our low-dimensional intuitions that's surprising
yeah yeah we're very we're very tuned to working within a three-dimensional environment and so
to start to visualize what a billion dimensional neural network surface that you're trying to
optimize over what that even looks like is very hard for us and so I think that really if you try
to account for for the essentially the AI winter where where people gave up on neural networks
I think it's really down to that that lack of ability to generalize from from low dimensions
to high dimensions because back then we were in the low-dimensional case people could only
build neural nets with you know 50 nodes in them or something and to to imagine that it might be
possible to build a billion dimensional neural net and that it might have a completely different
qualitatively different property was very hard to anticipate and I think even now we're starting
to build the the theory to support that and and it's incomplete at the moment but all of the
theory seems to be pointing in the direction that indeed this is an approach which which truly is
universal both in its representational capacity which was known but also in its learning ability
which is which is surprising and it makes one wonder what else we're missing due to our low
dimensional intuitions that that will seem obvious once it's discovered I often wonder
you know when we one day do have AI's which are superhuman in their abilities to to understand
the world what will they think of of the algorithms that we developed back now will it be you know
looking back at these these days and you know and and thinking that well will we look back and feel
that these algorithms were were naive first steps or will they still be the fundamental ideas which
are used even in a hundred thousand ten thousand years yeah I know they'll they'll watch back to
this conversation and and uh with a smile maybe a little bit of a laugh I mean my my sense is um
I think it's just like when we used to think that the the sun revolved around the earth
they'll see our systems of today reinforcement learning as too complicated that the answer was
simple all along there's something just just like you said in the game of go I mean I love the
systems of like cellular automata that there's simple rules from which incredible complexity
emerges so it feels like there might be some very least simple approaches just like where Sutton
says right these simple methods with compute over time seem to prove to be the most effective
I 100% agree I think that if we try to anticipate what will generalize well into the future I think
it's likely to be the case that it's the simple clear ideas which will have the longest legs and
which will carry us furthest into the future nevertheless we're in a situation where we need
to make things work right and today and sometimes that requires putting together more complex systems
where we don't have the the full answers yet as to what those minimal ingredients might be
so speaking of which if we could take a step back to go uh what was mogo and what was the key idea
behind the system so back during my um phd on computer go around about that time there was a
a major new development in in which actually happened in the context of computer go and
and it was really a revolution in the way that heuristic search was was done and and the idea was
essentially that um a position could be evaluated or a state in general could be evaluated
not by humans saying whether that um position is good or not or even humans providing rules as to
how you might evaluate it but instead by allowing the system to randomly play out the game until the
end multiple times and taking the average of those outcomes as the prediction of what will happen
so for example if you're in the game of go the intuition is that you take a position and you
get the system to kind of play random moves against itself all the way to the end of the game and you
see who wins and if black ends up winning more of those random games than white well you say hey
this is a position that favors white and if white ends up winning more of those random games than
black then it favors white um so that idea um was known as Monte Carlo um um search and a particular
form of Monte Carlo search that became very effective and was developed in computer go
first by Remy Coulomb in 2006 and then taken further by others was something called Monte
Carlo tree search which basically takes that same idea and uses that that insight to evaluate every
node of a search tree is evaluated by the average of the random playouts from that from that node
onwards um and this idea was very powerful and suddenly led to huge leaps forward in the strength
of computer go playing programs um and among those the the strongest of the go playing programs in
those days was a program called mogo which was the first program to actually reach human master
level on small boards nine by nine boards and so this was a program by someone called sylvan jelly
who's a good colleague of mine but i worked with him a little bit um in those days part of my phd
and mogo was a a first step towards the latest successes we saw in computer go
but it was still missing a key ingredient mogo was evaluating purely by random rollouts against
itself and in a way it's it's truly remarkable that random play should give you anything at all
yeah like why why in this perfectly deterministic game that's very precise and involves these very
exact sequences why is it that that random randomization is is is helpful and so the intuition
is that randomization captures something about the the nature of the of the the search tree that
from a position that you're you're understanding the nature of the search tree um from that node
onwards by by by using randomization and this was a very powerful idea and i've seen this in
other spaces uh i'm going to talk to Richard karp and so on randomized algorithms somehow
magically are able to do exceptionally well and and simplifying the problem somehow makes you wonder
about the fundamental nature of randomness in our universe it seems to be a useful thing but so from
that moment can you maybe tell the origin story in the journey of alpha go yeah so programs based on
Monte Carlo tree search were a first revolution in the sense that they led to um suddenly programs
that could play the game to any reasonable level but they they plateaued it seemed that no matter
how much effort people put into these techniques they couldn't exceed the level of um amateur
dan level go players so strong players but not not anywhere near the level of of professionals
never mind the world champion and so that brings us to the birth of alpha go which happened in the
context of a startup company known as deep mind i heard them where a a project was born and the
project was really a scientific investigation um where um myself and adjo huang and an intern
chris madison were exploring a scientific question and that scientific question was really
is there another fundamentally different approach to to this key question of of go the key challenge
of of how can you build that intuition and how can you just have a system that could look at a
position and understand um what moved to play or or how well you're doing in that position who's
going to win and so the deep learning revolution had just begun that systems like image net had
suddenly been won by deep learning techniques back in 2012 and following that it was natural to ask
well you know if if deep learning is able to scale up so effectively with images to to understand
them enough to to classify them well why not go why why why not take a um uh the black and white
stones of the go board and build some a system which can understand for itself what that means in
terms of what moved to pick or who's going to win the game black or white and so that was our
scientific question which we we were probing and trying to understand and as we started to look at
it we discovered that we could build a a system so in fact our very first paper on alpha go was
actually a pure deep learning system which was trying to answer this question and we showed
that actually a pure deep learning system with no search at all was actually able to reach human
band level master level at the full game of go 19 by 19 boards um and so without any search at all
suddenly we had systems which were playing at the level of the best
Monte Carlo tree set systems the ones with randomized rollouts so first I was sorry to
interrupt but uh that's kind of a groundbreaking notion that's a that's like basically a definitive
step away from the a couple of decades of essentially search dominating AI yeah so what how
did that make you feel would you think it was a surprising from a scientific perspective
in general how to make you feel I I found this to be profoundly surprising um in fact it was so
surprising that um that we had a bet back then and like many good projects you know bets are quite
motivating and and the bet was you know whether it was possible for a a a system based purely on
on deep learning no search at all to beat a a down level human player um and so we had um someone
who joined our team um who was a down level player he came in and um and we had this first
match um against him and which side of the bet were you on by the way did you hit the losing
on the winning side I tend to be an optimist um with the with the power of of of deep learning
and and reinforcement learning so the the system won and we were able to beat this um human down
level player and for me that was the moment where where it was like okay something something special
is afoot here we have a system which um without search is able to to already just look at this
position and understand things as well as a strong human player and from that point onwards
I really felt that um reaching that reaching the top levels of human play you know professional
level well champion level I felt it was actually an inevitability um and and if it was inevitable
outcome I was rather keen that it would be us that achieved it so we scaled up this was something
where you know so had lots of conversations back then with um Demisus Arbus that um um the um head
of of DeepMind who was extremely excited um and we we made the decision to to scale up the project
brought more people on board and and so AlphaGo became something where where we we had a clear
goal which was to try and um crack this outstanding challenge of AI to see if we could beat the world's
best players and this led within the space of um not so many months to playing against the
European champion Fan Hui in a match which became you know memorable in history as the
first time a go program had ever beaten a professional player and at that time we had to
make a judgment as to whether when and and whether we should go and challenge the world
champion and and this was a difficult decision to make again we were basing our predictions on
on our own progress and had to estimate based on the rapidity of our own progress when we thought we
would um exceed the level of the human world champion and and we tried to make an estimate
and set up a match and that became the the AlphaGo versus LisaDoll match in um 2016
and we should say spoiler alert that AlphaGo was able to defeat LisaDoll that's right yeah
so maybe uh we could take even a broader view AlphaGo involves both learning from expert games and
as far as I remember a self-played component to where it learns by playing against itself
but in your sense what was the role of learning from expert games there and in terms of your
self-evaluation whether you can take on the world champion what was the thing that they're trying
to do more of sort of train more on expert games or was there's now another I'm asking so many
poorly phrased questions but uh did you have a hope or dream that self-play would be the
key component at that moment yet so in the early days of AlphaGo we we used human data
to explore the science of what deep learning can achieve and so when we had our first paper
that showed um that it was possible to predict um the winner of the game that it was possible to
suggest moves that was done using human data or solely human data yeah and and and and so the
reason that we did it that way was at that time we were exploring separately the deep learning
aspect from the reinforcement learning aspect that was the part which was which was new and
unknown to to to me at that time was how far could that be stretched once we had that it then
became natural to try and use that same representation and see if we could learn for ourselves using
that same representation and so right from the beginning actually our goal had been to build
a system using self-play and to us the human data right from the beginning was an expedient step
to help us for pragmatic reasons to go faster towards the goals of the project
than we might be able to starting solely from self-play and so in those days we were very
aware that we were choosing to to use human data and that might not be the long-term
holy grail of AI but that it was something which was extremely useful to us it helped us to understand
the system it helped us to build deep learning representations which were clear and simple and
easy to use and so really I would say it's it served a purpose not just as part of the algorithm but
something which I continue to use in our research today which is trying to break down a very hard
challenge into pieces which are easier to understand for us as researchers and develop so if you if you
use a component based on human data it can help you to understand the system such that then you
can build the more principled version later that that does it for itself so as I said the Alpha
Go victory and I don't think I'm being sort of uh romanticizing this notion I think it's one of the
greatest moments in the history of AI so were you cognizant of this magnitude of the accomplishment
at the time I mean are you cognizant of it even now because to me I feel like it's something that
would we mentioned what the AGI systems of the future will look back I think they'll look back at
the Alpha Go victory as like holy crap they figured it out this is where this is where it started
well thank you again I mean it's funny because I guess I've been working on I've been working on
computer go for a long time so I've been working at the time of the Alpha Go match on computer go
for more more than a decade and throughout that decade I'd had this dream of what would it be like
to what would it be like really to to actually be able to build a system that could play against
the world champion and and I imagined that that would be an interesting moment that maybe you
know some people might care about that and that this might be you know a nice achievement
but I think when I arrived in in Seoul and discovered the legions of journalists that were
following us around and 100 million people that were watching the match online live I realized
that I'd been off in my estimation of how significant this moment was by several orders of magnitude
and so there was definitely an adjustment process to to realize that this this was something which
the world really cared about and which was a watershed moment and I think there was that
moment of realization which was also a little bit scary because you know if you go into something
thinking it's going to be maybe of interest and then discover that 100 million people are watching
it suddenly makes you worry about whether some of the decisions you've made were really the
best ones or the wisest or were going to lead to the best outcome and we knew for sure that there
were still imperfections in Alpha Go which were going to be exposed to the whole world watching
and so yeah it was a it was I think a great experience and I feel privileged to have been
part of it, privileged to have led that amazing team, I feel privileged to have been in a moment
of history like you say but also lucky that you know in a sense I was insulated from from the
knowledge of I think it would have been harder to focus on the research if the full kind of reality
of of what was going to come to pass had been known to me and the team I think it was you know
we were in our bubble and we were working on research and we were trying to answer the scientific
questions and then bam you know the public sees it and I think it was it was it was better that
way in retrospect. Were you confident that I guess what were the chances that you could get the win
so just like you said I'm a little bit more familiar with another accomplishment
than we may not even get a chance to talk to I talked to Oriel Venialis about Alpha Star which
is another incredible accomplishment but here you know with Alpha Star and beating the Starcraft
there was like already a track record with Alpha Go this is like the really first time you get to
see reinforcement learning face the best human in the world so what was your confidence like what
was the odds? Well we actually um was there a bet? Funnily enough there was so so just before the
match we weren't betting on anything concrete but we all held out a hand everyone in the team held
out a hand at the beginning of the match and the number of fingers that they had out on that hand
was supposed to represent how many games they thought we would win against Lisa Dahl and there
was an amazing spread in the team's predictions but I have to say I predicted 4-1
and the reason was based purely on data so I'm a scientist first and foremost and one of the things
which we had established was that Alpha Go in around one in five games would develop something
which we called a delusion which was a kind of you know hole in its knowledge where it wasn't
able to fully understand everything about the position and that hole in its knowledge would
persist for tens of moves throughout the game and we knew two things we knew that if there were no
delusions that Alpha Go seemed to be playing at a level that was far beyond any human capabilities
but we also knew that if there were delusions the opposite was true and in fact you know that's
what came to pass we saw all of those outcomes and Lisa Dahl in one of the games played a really
beautiful sequence that Alpha Go just hadn't predicted and after that it led it into this
situation where it was unable to really understand the position fully and found itself in one of
these delusions so indeed 4-1 was the outcome. So yeah and can you maybe speak to it a little bit
more what were the five games like what happened is there interesting things that come to memory
in terms of the play of the human machine? So I remember all of these games vividly of course
you know moments like these don't come too often in the lifetime of a scientist and
the first game was magical because it was the first time that a computer program had
defeated a world champion in this grand challenge of Go and there was a moment where
Alpha Go invaded Lisa Dahl's territory towards the end of the game
and that's quite an audacious thing to do it's like saying hey you thought this was going to be
your territory in the game but I'm going to stick a stone right in the middle of it and
and prove to you that I can break it up and Lisa Dahl's face just dropped he wasn't expecting a
computer to do something that audacious. The second game became famous for a move known as
Move 37 this was a move that was played by Alpha Go that broke all of the conventions of Go that
Go players were so shocked by this they thought that maybe the operator had made a mistake
they thought there was something crazy going on and it just broke every rule that Go players
are taught from a very young age they're just taught you know this kind of move called a shoulder
hit you can only play it on the third line or the fourth line and Alpha Go played it on the fifth
line and it turned out to be a brilliant move and made this beautiful pattern in the middle
of the board that ended up winning the game and so this really was a clear instance where we could
say computers exhibited creativity that this was really a move that was something humans hadn't
known about hadn't anticipated and computers discovered this idea they were the ones to say
actually you know here's a new idea something new not not in the domains of of human knowledge of the
game and and now the humans think this is a reasonable thing to do and and it's part of
Go knowledge now. The third game something special happens when you play against a human
world champion which again I hadn't anticipated before going there which is you know these these
players are amazing Lisa Doll was a true champion 18 time world champion and had this amazing ability
to to probe Alpha Go for weaknesses of any kind and in the third game he was losing and we felt
we were sailing comfortably to victory but he managed to from nothing stir up this fight and
build what's called a double co these kind of repetitive positions and he knew that historically
no no computer Go program had ever been able to deal correctly with double code positions
and he managed to summon one out of out of nothing and so for us you know this was this
was a real challenge like would Alpha Go be able to to to deal with this or would it just kind of
crumble in the face of of this situation and fortunately it dealt with it perfectly. The
fourth game was was amazing in that Lisa doll appeared to be losing this game Alpha Go thought
it was winning and then Lisa doll did something which I think only a true world champion can do
which is he found a brilliant sequence in the middle of the game a brilliant sequence that
led him to really just transform the position it kind of it it he found just a piece of genius
really and after that Alpha Go its its evaluation just tumbled it thought it was winning this game
and all of a sudden it tumbled and said oh now I've got no chance and it starts to behave rather
oddly at that point in the final game for some reason we as a team were convinced having seen
Alpha Go in the previous game suffer from delusions we as a team were convinced
that it was suffering from another delusion we were convinced that it was mis-evaluating the
position and that that something was going terribly wrong and it was only in the last
few moves of the game that we realized that actually although it had been predicting it
was going to win all the way through it really was and um and so somehow you know it just taught us
yet again that you have to have faith in in your systems when they when they exceed your own level
of ability in your own judgment you have to trust in them to to know better than than you the designer
once um you've you've bestowed in them the ability to to judge better than you can then trust the
system to do so. So just like in the case of Deep Blue beating Gary Kasparov so Gary is I think
the first time he's ever lost actually to anybody and I mean there's a similar situation at least
at all it's a it's a tragic it's a tragic loss for humans but a beautiful one I think that's kind of
from the tragedy sort of emerges over time emerges the kind of inspiring story but
Lisa Dahl recently analyses her time and I don't know if we can look too deeply into it but he did
say that even if I become number one there's an entity that cannot be defeated so what do you think
about these words what do you think about his retirement from the game ago? Well let me take
you back first of all to the first part of your comment about Gary Kasparov because actually
at the panel yesterday um he specifically said that when he first lost to Deep Blue he he viewed
it as a failure he viewed that this this had been a failure of his but later on in his career he
said he'd come to realize that actually it was a success it was a success for everyone because
this marked a transformational moment for for AI and so even for Gary Kasparov he came to to
realize at that moment was was was pivotal and actually meant something much more than than you
know his personal loss in that moment. Lisa Dahl I think was a much more cognizant of that even
at the time so in his closing remarks to the match he really felt very strongly that what
had happened in the AlphaGo match was not only meaningful for AI but but for humans as well
and he felt as a go player that it had opened his horizons and meant that he could start exploring
new things it brought his joy back for the game of go because it had broken all of the the conventions
and barriers and meant that you know suddenly suddenly anything was possible again and so
you know I was sad to hear that he'd retired but you know he's been a great a great world champion
over many many years and I think you know that he'll be he'll be remembered for that ever more
he'll be remembered as the last person to to beat AlphaGo I mean after after that we we
increased the power of the system and and the next version of AlphaGo beats the other strong
human players 60 games to nil so you know what a great moment for him and something to be remembered
for it's interesting that you spent time at AAAI on a panel with Gary Kasparov
what I mean it's almost I'm just curious to learn
the conversations you've had with Gary and the because he's also now he's written a book about
artificial intelligence he's thinking about AI he has kind of a view of it and he talks about AlphaGo
a lot what what's your sense I arguably I'm not just being Russian but I think Gary is the greatest
chess player of all time the probably one of the greatest game players of all time and you sort of
at the center of creating a system that beats one of the greatest players of all time so what
is that conversation like is there anything yeah any interesting digs any bets any come
any funny things any profound things so Gary Kasparov has an incredible respect for
what we did with AlphaGo and you know it's it's an amazing tribute coming from from him of all people
that he really appreciates and respects what what we've done and I think he feels that the progress
which has happened in in computer chess which later after AlphaGo we we built the AlphaZero system
which defeated the the world's strongest chess programs and to Gary Kasparov that moment in
computer chess was more profound than than than deep blue and the reason he believes it mattered more
was because it was done with with learning and a system which was able to discover for itself
new principles new ideas which were able to play the game in a in a in a way which he hadn't always
known about or anyone and in fact one of the things I discovered at this panel was that
the current world champion Magnus Carlsen apparently recently commented on his improvement
in performance and he attributes it to AlphaZero that he's been studying the games of AlphaZero
he's changed his style to play more like AlphaZero and it's led to him actually increasing his his
his rating to a new peak yeah I guess to me just like to Gary the inspiring thing is that and just
like you said with reinforcement learning reinforcement learning and deep learning machine
learning feels like what intelligence is yeah and you know you could attribute it to sort of
a bitter viewpoint from Gary's perspective from us humans perspective saying that
cert pure search that IBM deep blue was doing is not really intelligence but somehow it didn't feel
like it and so that's the magical I'm not sure what it is about learning that feels like intelligence
but but it does so I think we should not demean the achievements of what was done in previous
areas of AI I think that deep blue was an amazing achievement in itself and that heuristic search
of the kind that was used by deep blue had some powerful ideas that were in there but it also
missed some things so so the fact that the that the evaluation function the way that the chess
position was understood was created by humans and not by the machine is a limitation which means that
there's a ceiling on how well it can do but maybe more importantly it means that the same idea
cannot be applied in other domains where we don't have access to the kind of human grandmasters
and that ability to kind of encode exactly their knowledge into an evaluation function and the
reality is that the story of AI is that you know most domains turn out to be of the second type
where when knowledge is messy it's hard to extract from experts or it isn't even available and so
and so so we need to solve problems in a different way and I think alpha goes a step towards solving
things in a way which which puts learning as a first class citizen and says systems need to
understand for themselves how to understand the world how to judge their the value of of of
any action that they might take within that world and any state they might find themselves in and
in order to do that we we make progress towards AI yeah so one of the nice things about this
about taking a learning approach to the game of go or game playing is that the things you learn the
things you figure out are actually going to be applicable to other problems that are real world
problems that's sort of that's ultimately I mean there's two really interesting things about alpha
go one is the science of it just the science of learning the science of intelligence and then the
other is well you're actually learning to figuring out how to build systems that would be potentially
applicable in in other applications medical autonomous vehicles robotics all I mean it's
just open the door to all kinds of applications so the next incredible step right really the
profound step is probably alpha go zero I mean it's arguable I kind of see them all as the same
place but really and perhaps you were already thinking that alpha go zero is the natural it was
always going to be the next step but it's removing the reliance on human expert games
for pre-training as you mentioned so how big of an intellectual leap was this that that self-play
could achieve superhuman level performance in its own and maybe could you also say what is self-play
I kind of mentioned it a few times but so let me start with self-play so the idea of self-play
is something which is really about systems learning for themselves but in the situation
where there's more than one agent and so if you're in a game and a game is a played between
two players then self-play is really about understanding that game just by playing games
against yourself rather than against any actual real opponent and so it's a way to kind of
discover strategies without having to actually need to go out and play against
any particular human player for example
um the main idea of alpha zero was really to you know try and step back from any of the
knowledge that we put into the system and ask the question is it possible to come up with a
a single elegant principle by which a system can learn for itself all of the knowledge which it
requires to play to play a game such as go importantly by taking knowledge out you not only
make the system less brittle in the sense that perhaps the knowledge you were putting in was
was just getting in the way and maybe stopping the system learning for itself but also you make it
more general the more knowledge you put in the harder it is for a system to actually be placed
taken out of the system in which it's kind of been designed and placed in some other system
that maybe would need a completely different knowledge base to to understand and perform well
and so the real goal here is to strip out all of the knowledge that we put in to the point that we
can just plug it into something totally different um and that to me is really you know the the
promise of AI is that we can have systems such as that which you know no matter what the goal is
um no matter what goal we set to the system we can come up with we have an algorithm which
can be placed into that world into that environment and can succeed in achieving that goal and then
that that's to me is almost the the essence of intelligence if we can achieve that and so alpha
zero is a step towards that um and it's a step that was taken in the context of of two player
perfect information games like go and chess um we also applied it to Japanese chess
so just to clarify the first step was alpha go zero the first step was to try and take all of
the knowledge out of alpha go in such a way that it it could play in a in a fully um self-discovered
way purely from self-play and to me the the motivation for that was always that we could
then plug it into other domains um but we saved that that until later well and in fact I mean
just for fun I could tell you exactly the moment where where the idea for alpha zero occurred to
me um because I think there's maybe a lesson there for for researchers who are kind of too deeply
embedded in there in their research and you know working 24 sevens try and come up with the next
idea um which is it actually occurred to me um on honeymoon um and um and I was like at my most
fully relaxed state really enjoying myself um and and just bing this like the algorithm for alpha
zero just appeared like um and like in in its full form and this was actually before we played
against um Lisa doll but we we just didn't I think we were so busy trying to make sure we could beat
the um the the world champion that it was only later that we had the the opportunity to step
back and and start examining that that sort of deeper scientific question of of whether this
could really work so nevertheless so self-play is probably one of the most sort of profound ideas
that it represents to me at least artificial intelligence but the fact that you could use
that kind of mechanism to uh again beat world-class players that's very surprising so we kind of
to me it feels like you have to train in a large number of expert games so was it surprising to
you what was the intuition can you sort of think not necessarily at that time even now what's your
intuition why this thing works so well why it's able to learn from scratch well let me first say
why we tried it so we tried it both because I I feel that it was the deeper scientific question
to to be asking to make progress towards AI and also because in general in my research I don't
like to do research on questions for which we already know the likely outcome I don't see much
value in running an experiment where you're 95 confident that that you will succeed and so we
could have tried you know maybe to to take out the go and do something which we we knew for sure
it would succeed on but much more interesting to me was to try try on the things which we weren't
sure about and one of the big questions on our minds back then was you know could you really
do this with self-play alone how far could that go would it be as strong and honestly
we weren't sure yeah it was 50-50 I think you know we I really if you'd asked me I wasn't confident
that it could reach the same level as these systems but it felt like the right question to ask
and even if even if it had not achieved the same level I felt that that was an important
direction to be studying and so
then low and behold it actually ended up outperforming the the previous version of of
AlphaGo and indeed was able to beat it by a hundred games to zero so what's the intuition as to as
to why I think the intuition to me is clear that whenever you have errors in a in a system
as we did in AlphaGo AlphaGo suffered from these delusions
occasionally it would misunderstand what was going on in a position and misevaluate it
how can how can you remove all of these these errors errors arise from many sources for us
they were arising both from you know starting from the human data but also from the from the
nature of the search and the nature of the algorithm itself but the only way to address them
in any complex system is to give the system the ability to correct its own errors it must be able
to correct them it must be able to learn for itself when it's doing something wrong and correct
for it and so it seemed to me that the way to correct delusions was indeed to have more iterations
of reinforcement learning that you know no matter where you start you should be able to correct for
those errors until it gets to play that out and understand oh well I thought that I was going
to win in this situation but then I ended up losing that suggests that I was misevaluating
something there's a hole in my knowledge and now now the system can correct for itself and
and understand how to do better now if you take that same idea and trace it back all the way to
the beginning it should be able to take you from no knowledge from completely random starting point
all the way to the highest levels of knowledge that you can achieve in a in a domain and the
principle is the same that if you give if you bestow a system with the ability to correct its own
errors then it can take you from random to something slightly better than random because it sees the
stupid things that the random is doing and it can correct them and then it can take you from that
slightly better system and understand well what's that doing wrong and it takes you on to the next
level and the next level and and this progress it can go on indefinitely and indeed you know
what would have happened if we'd carried on training AlphaGo Zero for longer
we saw no sign of it slowing down its improvements or at least it was certainly
carrying on to improve and presumably if you had the computational resources this
this could lead to better and better systems that discover more and more.
So your intuition is fundamentally there's not a ceiling to this process
one of the surprising things just like you said is the process of patching errors
that's intuitively makes sense that this is a reinforcement learning should be part of that
process but what is surprising is in the process of patching your own lack of knowledge you don't
open up other patches you keep sort of like there's a monotonic decrease of your weaknesses.
Well let me let me back this up you know I think science always should make
falsifiable hypotheses yes so let me let me back up this claim with a falsifiable hypothesis
which is that if someone was to in the future take Alpha Zero as an algorithm and run it on
with greater computational resources that we had available today then I would predict that they
would be able to beat the previous system 100 games to zero and that if they were then to do
the same thing a couple of years later that that would beat that previous system 100 games to zero
and that that process would continue indefinitely throughout at least my human lifetime presumably
the game of go would set the the ceiling I mean the game of go would set the ceiling but the game
of go has 10 to the 170 states in it so so the ceiling is is unreachable by any computational
device that can be built out of the you know 10 to the 80 atoms in the universe.
You asked a really good question which is you know do you not open up other errors
when you when you correct your previous ones and the answer is is yes you do and so
so it's a remarkable fact about about this class of of two player game and also true of
single agent games that essentially progress will always lead you to if you have sufficient
representational resource like imagine you had could represent every state in a big table
of the game then we we know for sure that a progress of self-improvement will lead all the way
in the single agent case to the optimal possible behavior and in the two player case to the
minimax optimal behavior that is the the best way that I can play knowing that you're playing
perfectly against me and so so for those cases we know that even if you do open up some new error
that in some sense you've made progress you've you've you're progressing towards the the best
that can be done so alpha go was initially trained on expert games with some self-play alpha go zero
remove the need to be trained on expert games and then another incredible step for me because I just
love chess is to generalize that further to be in alpha zero to be able to play the game of go
beating alpha go zero and alpha go and then also being able to play the game of chess
and others so what was that step like what's the interesting aspects there that required to make
that happen I think the remarkable observation which we saw with alpha zero was that actually
without modifying the algorithm at all it was able to play and crack some of ai's greatest
previous challenges in particular we dropped it into the game of chess and unlike the previous
systems like deep blue which had been worked on for you know years and years and we were able to beat
the world's strongest computer chess program convincingly using a system that was fully
discovered by its own from from scratch with its own principles and in fact one of the nice things
that that we found was that in fact we also achieved the same result in in Japanese chess a variant
of chess where where you get to capture pieces and then place them back down on your on your own
side as an extra piece so a much more complicated variant of chess and we also beat the world's
strongest programs and reach superhuman performance in that game too and it was the very first time
that we'd ever run the system on that particular game was the version that we published in the
paper on on alpha zero it just worked out of the box literally no no no touching it we didn't have
to do anything and and there it was superhuman performance no tweaking no no twiddling and so
I think there's something beautiful about that principle that you can take an algorithm and
without twiddling anything it just it just works now to go beyond alpha zero what's required alpha
zero is is just a step and there's a long way to go beyond that to really crack the deep problems of
AI but one of the important steps is to acknowledge that the world is a really messy place you know
it's this rich complex beautiful but messy environment that we live in and no one gives us
the rules like no one knows the rules of the world at least maybe we understand that it operates
according to Newtonian or or quantum mechanics at the micro level or according to relativity at the
macro level but that's not a model that's useful useful for us as people to to operate in it somehow
the agent needs to understand the world for itself in a way where no one tells it the rules of the
game and yet it can still figure out what to do in that world deal with this stream of
observations coming in rich sensory input coming in actions going out in a way that allows it to
reason in the way that alpha go or alpha zero can reason in the way that these go and chess playing
programs can reason but in a way that allows it to take actions in that messy world to to achieve
its goals and so this led us to the most recent step in the story of of of alpha go which was a
system called mu zero and mu zero is a system which learns for itself even when the rules
are not given to it it actually can be dropped into a system with messy perceptual inputs we
actually tried it in the in some Atari games the canonical domains of Atari that have been used
for reinforcement learning and and this system learned to build a model of these Atari games
that was sufficiently rich and useful enough for it to be able to plan successfully and in fact
that system not only went on to to beat the state of the art in Atari but the same system without
modification was able to reach the same level of superhuman performance in go chess and shogi
that we'd seen in alpha zero showing that even without the rules the system can learn for itself
just by trial and error just by playing this game of go and no one tells you what the rules are but
you just get to the end and and someone says you know win or loss you play this game of chess and
someone says win or loss or you you play a game of breakout in Atari and someone just tells you
you know your score at the end and the system for itself figures out essentially the rules of the
system the dynamics of the world how the world works and that not in any explicit way but just
implicitly enough understanding for it to be able to plan in that in that system in order to
achieve its goals and that's the you know that's the fundamental process they have to go through when
you're facing in any uncertain kind of environment that you would in the real world is figuring out
the sort of the rules the basic rules of the game that's right so there's a lot I mean yeah that
that allows it to be applicable to basically any domain that could be digitized in the way that it
needs to in order to be consumable sort of in order for the reinforcement learning framework to
be able to sense the environment to be able to act in the environment and so on the full reinforcement
learning problem needs to deal with with worlds that are unknown and and complex and and the agent
needs to learn for itself how to deal with that and so Musero is there's a step a further step in
that direction one of the things that inspire the general public in just in conversations I have like
with my parents or something with my mom that just loves what was done is kind of at least a notion
that there was some display of creativity some new strategies new behaviors that were created that
that again has echoes of intelligence so is there something that stands up do you see it the same
way that there's creativity and there's some behaviors patterns that you saw that alpha
zero was able to display that are truly creative so let me start by I think saying that I think
we should ask what creativity really means so to me creativity means discovering something
which wasn't known before something unexpected something outside of our norms and so in that sense
the process of reinforcement learning or the self-play approach that was used by alpha zero
is it's the essence of creativity it's really saying at every stage you're playing according to
your current norms and you try something and if it works out you say hey here's something great
I'm going to start using that and then that process it's like a micro discovery that happens
millions and millions of times over the course of the algorithm's life where it just discovers some
new idea oh this pattern this pattern's working really well for me I'm gonna I'm gonna start using
that oh now oh here's this other thing I can do I can start to to connect these stones together in
this way or I can start to you know sacrifice stones or give up on on pieces or play shoulder
hits on the fifth line or whatever it is the system's discovering things like this for itself
continually repeatedly all the time and so it should come as no surprise to us then when
if you leave these systems going that they discover things that are not known to humans
that the to the human norms are considered creative and we've seen this several times in fact
in alpha go zero we saw this beautiful timeline of discovery where what we saw was that there
were these opening patterns that humans play called joseki these are like the patterns that
that humans learn to play in the corners and they've been developed and refined over
over literally thousands of years in the game of go and what we saw was in the course of the
training alpha go zero over the course of the the 40 days that we trained this system
it starts to discover exactly these patterns that human players play and over time we found
that all of the joseki that humans played were discovered by the system through this process
of of self play and this sort of essential notion of creativity but what was really interesting
was that over time it then starts to discard some of these in favor of its own joseki that humans
didn't know about and it starts to say oh well you thought that the knights move pincer joseki
was a great idea but here's something different you can do there which makes some new variations
that humans didn't know about and actually now the human go players study the joseki that alpha
go played and they become the new norms that are used in in today's top level go competitions
that never gets old even just the first to me maybe just makes me feel good as a human being
that a self playing mechanism that knows nothing about us humans discovers patterns that we humans
do it's this is like an affirmation that we're all doing we're doing okay as humans yeah we've
in this domain in other domains we figured out it's like the church will quote about democracy
it's the you know it's the but it sucks but it's the best one we've tried so um in general
taking a step outside of go and you have like a million accomplishments that have no time to talk
about the with alpha star and so on and and the current work but in general this self playing
mechanism that you've inspired the world with by beating the world champion go player do you see
that as um do you see being applied in other domains do you have sort of dreams and hopes that
is applied in both the simulated environments in the constrained environments of games constrained
i mean alpha star really demonstrates that you can remove a lot of the constraints but nevertheless
it's in a digital simulated environment do you have a hope a dream that it starts being applied
in the robotics environment and maybe even in domains that are safety critical and so on and
have you know have a real impact in the real world like autonomous vehicles for example which
seems like a very far out dream at this point so i absolutely do um hope and and imagine that we
will we will get to the point where ideas just like these are used in all kinds of different
domains in fact one of the most satisfying things as a researcher is when you start to see other
people use your your algorithms in unexpected ways so in the last couple of years there have been
you know a couple of nature papers where different teams unbeknownst to to us took alpha zero and
applied exactly those same algorithms and ideas to real world problems of huge meaning to to
society so one of them was the problem of chemical synthesis and they were able to beat the state of
the art in finding pathways of how to actually synthesize chemicals retro retro chemical synthesis
and the second paper actually actually just came out a couple of weeks ago in nature
showed that in quantum computation you know one of the big questions is how to how to
understand the nature of the the the function in quantum computation and a system based on alpha
zero beat the state of the art by quite some distance there again so so these are just examples
and i think you know that the lesson which we've seen elsewhere in in in machine learning time
and time again is that if you make something general it will be used in all kinds of ways you
know you provide a really powerful tools to society and and those tools can be used in in
amazing ways and so i think we're just at the beginning and and for sure i hope that we we
see all kinds of outcomes so the the and the the other side of the question of a reinforcement
learning framework is you know usually want to specify reward function and an objective function
what do you think about sort of ideas of intrinsic rewards of and when we're not really sure about
you know of if we take you know human beings as existence proof that we don't seem to be
operating according to a single reward do you think that there's interesting ideas
for when you don't know how to truly specify the reward you know that there's some flexibility
for discovering it intrinsically or so on in the context of reinforcement learning
so i think you know when we think about intelligence it's really important to be clear
about the problem of intelligence and i think it's clearest to understand that problem in
terms of some ultimate goal that we want the system to to try and solve for and after all if
if we don't understand the ultimate purpose of the system do we really even have a clearly defined
problem that we're solving at all now within that as with your example for humans
the system may choose to create its own motivations and sub-goals that help the system to achieve its
ultimate goal and that may indeed be a hugely important mechanism to achieve those ultimate
goals but there is still some ultimate goal i think the system needs to be measurable and and
evaluated against and even for humans i mean humans we're incredibly flexible we feel that we
we can you know any goal that we're given we feel we can we can master to some some degree
but if we think of those goals really you know like the the goal of being able to pick up an
object or the goal of of being able to communicate or influence people to do things in a in a particular
way or whatever those goals are really they are their sub-goals really that we set ourselves
you know we choose to pick up the the object we choose to communicate we choose to to influence
someone else and we choose those because it we think it will lead us to something you know in
later on we think that's helpful to us to achieve some ultimate goal now i don't want to speculate
whether or not humans as a system necessarily have a singular overall goal of survival or whatever
it is but i think the principle for understanding and implementing intelligence is has to be that if
we're trying to understand intelligence or implement our own there has to be a well-defined
problem otherwise if it's not i think it's it's like an admission of defeat that for there to be
hope for for understanding or implementing intelligence we have to know what we're doing
we have to know what we're asking the system to do otherwise if you if you don't have a clearly
defined purpose you're not going to get a clearly defined answer the the ridiculous big question that
has to naturally follow because i have to pin you down on this on this thing that nevertheless one
of the big silly or big real questions before humans is the meaning of life is us trying to
figure out our own reward function yeah and you just kind of mentioned that if you want to build
intelligent systems and you know what you're doing you should be at least cognizant to some degree
of what the reward function is so the natural question is what do you think is the reward
function of human life the meaning of life for us humans the meaning of our existence
i think you know i'd be speculating beyond my own expertise but but just for fun let me do that
yes please and say i think that there are many levels at which you can understand the system
and and you can understand something as as optimizing for for a goal at many levels and so
so you can understand the you know let's start with the universe like does the universe have a
purpose well it feels like it's just at one level just following certain mechanical laws of physics
and that that's led to the development of the universe but at another level you can view it as
actually there's the second law of thermodynamics that says that this is
increasing in entropy over time forever and now there's a view that's been developed by
certain people at MIT that this you can think of this as as almost like a goal of the universe
that the purpose of the universe is to maximize entropy so there are multiple levels at which
you can understand a system the next level down you might say well if the goal is to is to maximize
entropy well how do how does how can that be done by a particular system and maybe evolution
is something that the universe discovered in order in order to kind of dissipate energy as
efficiently as possible and by the way i'm borrowing from max tegmark for some of these
metaphors the physicist but if you can think of evolution as a mechanism for for dispersing energy
then then evolution you might say is is then becomes a goal which is if if evolution disperses
energy by reproducing as efficiently as possible what's evolution then well it's now got its own
goal within that which is to actually reproduce as effectively as possible and now how does
reproduction how is that made as effective as possible well you need entities within that
that can survive and reproduce as effectively as possible and so it's natural that in order to
achieve that high level goal those individual organisms discover brains intelligences which
enable them to support the the goals of evolution and those brains what do they do well perhaps the
early brains maybe they were controlling things at some direct level you know maybe they were the
equivalent of pre-programmed systems which were directly controlling what was going on
and setting certain you know things in order to achieve these particular particular goals
but that led to a another level of discovery which was learning systems you know parts of
the brain which are able to to learn for themselves and learn how to to program themselves to achieve
any goal and presumably there are parts of the game of the brain where goals are set to parts of
that that system and provides this very flexible notion of intelligence that that we as humans
presumably have which is the ability to kind of why the reason we feel that we can we can we can
achieve any goal so so it's a very long-winded answer to say that you know I think there are many
perspectives and many levels at which intelligence can be understood and and each of those levels
you can take multiple perspectives like you know you can view the system as as something which is
optimizing for a goal which is understanding it at a level by which we can maybe implement it and
understand it as AI researchers or computer scientists or you can understand it at the
level of the mechanistic thing which is going on that there are these you know atoms bouncing
around in the brain and they lead to the the outcome of that system is not in contradiction
with the fact that it's it's also a decision-making system that's optimizing for some goal and and
purpose I've never heard the the description of the meaning of life structured so beautifully in
layers but you did miss one layer which is the next step which you're responsible for which is
creating the the artificial intelligence indeed layer on top of that and indeed I can't wait to
see well I may not be around but the can't wait to see what the next layer beyond that well well
let's just take that that argument you know and pursue it to its natural conclusion so so the
next level indeed is for for how can our how can our learning brain achieve its goals most
effectively well maybe it does so by by us as learning beings building a system which is able
to solve for those goals more effectively than we can and so when we build a system to play the game
of go you know when I said that I wanted to build a system that can play go better than I can I've
enabled myself to achieve that goal of playing go better than I could by by directly playing it
and learning it myself and so now a new layer has been created which is systems which are able to
achieve goals for themselves and ultimately there may be layers beyond that where they set sub-goals
to parts of their own system in all in order to to achieve those and so forth so incredible so the
story of intelligence I think I think is is a multi-layered one and a multi-perspective one
we live in an incredible universe David thank you so much first of all for dreaming of using
learning to solve go and building intelligence systems and for actually making it happen and
for inspiring millions of people in the process it's truly an honor thank you so much for talking
today okay thank you thanks for listening to this conversation with David Silver and thank you to
our sponsors masterclass and cash app please consider supporting the podcast by signing up
to masterclass at masterclass.com slash lex and downloading cash app and using code lex podcast
if you enjoy this podcast subscribe on youtube review it with five stars an apple podcast
support on patreon or simply connect with me on twitter at lex freedman and now let me leave you
some words from david silver my personal belief is that we've seen something of a turning point
where we're starting to understand that many abilities like intuition and creativity that
we've previously thought were in the domain only of the human mind are actually accessible to machine
intelligence as well and I think that's a really exciting moment in history thank you for listening
and hope to see you next time
