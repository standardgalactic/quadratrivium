start	end	text
0	2880	The following is a conversation with Greg Brockman.
2880	5360	He's the co-founder and CTO of OpenAI,
5360	9040	a world-class research organization developing ideas in AI
9040	12880	with the goal of eventually creating a safe and friendly
12880	15360	artificial general intelligence.
15360	18800	One that benefits and empowers humanity.
18800	24400	OpenAI is not only a source of publications, algorithms, tools, and data sets.
24400	28080	Their mission is a catalyst for an important public discourse
28080	33200	about our future with both narrow and general intelligence systems.
33920	38800	This conversation is part of the Artificial Intelligence Podcast at MIT and BEYOND.
39440	44400	If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter
44400	48000	at Lex Friedman, spelled F-R-I-D.
48000	51600	And now here's my conversation with Greg Brockman.
52640	56080	So in high school and right after you wrote a draft of a chemistry textbook,
56640	60720	I saw that that covers everything from basic structure of the atom to quantum mechanics.
61440	68240	So it's clear you have an intuition and a passion for both the physical world with chemistry
68240	74400	and non-robotics to the digital world with AI, deep learning, reinforcement learning,
74400	78480	so on. Do you see the physical world and the digital world as different?
78480	79760	And what do you think is the gap?
80400	83200	A lot of it actually boils down to iteration speed, right?
83200	86480	That I think that a lot of what really motivates me is building things, right?
87680	90800	Think about mathematics, for example, where you think really hard about a problem,
90800	93920	you understand it. You're right down in this very obscure form that we call proof.
94480	98320	But then this is in humanity's library, right? It's there forever.
98320	99760	This is some truth that we've discovered.
100480	102880	And maybe only five people in your field will ever read it,
102880	104720	but somehow you've kind of moved humanity forward.
105520	108640	And so I actually used to really think that I was going to be a mathematician.
108640	111520	And then I actually started writing this chemistry textbook.
111520	114880	One of my friends told me, you'll never publish it because you don't have a PhD.
114880	119840	So instead, I decided to build a website and try to promote my ideas that way.
119840	125200	And then I discovered programming. And in programming, you think hard about a problem,
125200	129040	you understand it, you're right down in a very obscure form that we call a program.
129840	132160	But then once again, it's in humanity's library, right?
132160	135600	And anyone can get the benefit from it and the scalability is massive.
135600	139040	And so I think that the thing that really appeals to me about the digital world
139040	141920	is that you can have this insane leverage, right?
141920	146240	A single individual with an idea is able to affect the entire planet.
146240	150240	And that's something I think is really hard to do if you're moving around physical atoms.
150240	156320	But you said mathematics. So if you look at the wet thing over here, our mind,
157120	161680	you ultimately see it as just math, as just information processing,
161680	167040	or is there some other magic if you've seen through biology and chemistry and so on?
167040	170960	I think it's really interesting to think about humans as just information processing systems,
170960	174960	and that it seems like it's actually a pretty good way of describing a lot of
176240	180560	how the world works, a lot of what we're capable of, to think that, again,
180560	184480	if you just look at technological innovations over time, that in some ways,
184480	187680	the most transformative innovation that we've had has been the computer, right?
187680	190400	In some ways, the internet, what has the internet done?
190400	192640	The internet is not about these physical cables.
192640	196480	It's about the fact that I am suddenly able to instantly communicate with any other human
196480	200720	on the planet. I'm able to retrieve any piece of knowledge that, in some ways,
200720	206000	the human race has ever had, and that those are these insane transformations.
206000	211360	Do you see our society as a whole the collective as another extension of the intelligence of the
211360	214960	human being? So if you look at the human being as an information processing system,
214960	219280	you mentioned the internet, the networking. Do you see us all together as a civilization
219280	221600	as a kind of intelligent system?
221600	225760	Yeah, I think this is actually a really interesting perspective to take and to think about
225760	229360	that you sort of have this collective intelligence of all of society.
229360	234320	The economy itself is this superhuman machine that is optimizing something, right?
234320	237840	And in some ways, a company has a will of its own, right?
237840	241040	That you have all these individuals who are all pursuing their own individual goals and thinking
241040	245280	really hard and thinking about the right things to do, but somehow the company does something
245280	250480	that is this emergent thing and that it's a really useful abstraction.
250480	255040	And so I think that in some ways, we think of ourselves as the most intelligent things on the
255040	258800	planet and the most powerful things on the planet, but there are things that are bigger
258800	262400	than us that are these systems that we all contribute to. And so I think actually,
262400	267040	you know, it's interesting to think about, if you've read Asa Geismov's Foundation,
267040	270880	right, that there's this concept of psycho history in there, which is effectively this,
270880	275040	that if you have trillions or quadrillions of beings, then maybe you could actually
275040	280880	predict what that being, that huge macro being will do and almost independent of what the
280880	284880	individuals want. And I actually have a second angle on this that I think is interesting,
284880	290560	which is thinking about technological determinism. One thing that I actually think a lot about with
290560	296320	OpenAI, right, is that we're kind of coming on onto this insanely transformational technology of
296320	299920	general intelligence, right, that will happen at some point. And there's a question of,
299920	304000	how can you take actions that will actually steer it to go better rather than worse?
304720	309280	And that I think one question you need to ask is, as a scientist, as an event as a creator,
309280	313280	what impact can you have in general, right? You look at things like the telephone invented by
313280	317040	two people on the same day. Like, what does that mean? Like, what does that mean about the shape
317040	321040	of innovation? And I think that what's going on is everyone's building on the shoulders of the same
321040	325600	giants. And so you can kind of, you can't really hope to create something no one else ever would.
325600	329040	You know, if Einstein wasn't born, someone else would have come up with relativity.
329040	332880	You know, you change the timeline a bit, right, that maybe it would have taken another 20 years,
332880	337120	but it wouldn't be that fundamentally humanity would never discover these fundamental truths.
337520	344320	There's some kind of invisible momentum that some people like Einstein or OpenAI is plugging into
345280	350240	that anybody else can also plug into. And ultimately, that wave takes us into a certain
350240	353120	direction. That's what you mean by digitalism. That's right. That's right. And, you know,
353120	356800	this kind of seems to play out in a bunch of different ways, that there's some exponential
356800	360880	that is being written, and that the exponential itself, which one it is, changes, think about
360880	366080	Moore's law, an entire industry set, it's clocked to it for 50 years. Like, how can that be, right?
366080	371760	How is that possible? And yet somehow it happened. And so I think you can't hope to ever invent
371760	376080	something that no one else will. Maybe you can change the timeline a little bit. But if you
376080	379280	really want to make a difference, I think that the thing that you really have to do,
379280	383840	the only real degree of freedom you have is to set the initial conditions under which a technology
383840	387760	is born. And so you think about the internet, right, that there are lots of other competitors
387760	393440	trying to build similar things. And the internet one, and that the initial conditions were that
393440	398480	it was created by this group that really valued people being able to be, you know, anyone being
398480	403040	able to plug in this very academic mindset of being open and connected. And I think that the
403040	407920	internet for the next 40 years really played out that way. You know, maybe today, things are
407920	411440	starting to shift in a different direction. But I think that those initial conditions were really
411440	416560	important to determine the next 40 years worth of progress. That's really beautifully put. So
416560	421680	another example of that I think about, you know, I recently looked at it, I looked at Wikipedia,
421680	426880	the formation of Wikipedia. And I wonder what the internet would be like if Wikipedia had ads.
427600	433200	You know, there's a interesting argument that why they chose not to make it put advertisement
433200	438800	on Wikipedia. I think it's, I think Wikipedia is one of the greatest resources we have on the internet.
438800	443920	It's extremely surprising how well it works and how well it was able to aggregate all this kind of
443920	448640	good information. And essentially the creator of Wikipedia, I don't know, there's probably some
448640	453440	debates there, but set the initial conditions. And now it carried itself forward. That's really
453440	457920	interesting. So you're the way you're thinking about AGI or artificial intelligence is you're
457920	463360	focused on setting the initial conditions for the progress. That's right. That's powerful. Okay, so
463920	470080	look into the future. If you create an AGI system, like one that can ace the Turing test,
470080	476080	natural language, what do you think would be the interactions you would have with it? What do you
476080	479280	think are the questions you would ask? Like what would be the first question you would ask?
480480	485840	It, her, him. That's right. I think that at that point, if you've really built a powerful system
485840	490240	that is capable of shaping the future of humanity, the first question that you really should ask is
490240	494560	how do we make sure that this plays out well? And so that's actually the first question that I would
494560	500640	ask a powerful AGI system is. So you wouldn't ask your colleague, you wouldn't ask like Ilya,
500640	504320	you would ask the AGI system. Oh, we've already had the conversation with Ilya,
504320	507600	right? And everyone here. And so you want as many perspectives and
508400	512400	piece of wisdom as you can for answering this question. So I don't think you necessarily defer
512400	517680	to whatever your powerful system tells you, but you use it as one input to try to figure out what
517680	522960	to do. But I guess fundamentally, what it really comes down to is if you built something really
522960	527600	powerful, and you think about, think about, for example, the creation of, of shortly after the
527600	531280	creation of nuclear weapons, right? The most important question in the world was, what's the
531280	535760	world we're going to be like? How do we set ourselves up in a place where we're going to be
535760	540640	able to survive as a species? With AGI, I think the question is slightly different, right? That
540640	544720	there is a question of how do we make sure that we don't get the negative effects? But there's
544720	549600	also the positive side, right? You imagine that, you know, like, like, what will AGI be like?
549600	554400	Like, what will it be capable of? And I think that one of the core reasons that an AGI can be
554400	559120	powerful and transformative is actually due to technological development, right? If you have
559120	565280	something that's capable, as capable as a human, and that it's much more scalable, that you absolutely
565280	568800	want that thing to go read the whole scientific literature and think about how to create cures
568800	573040	for all the diseases, right? You want it to think about how to go and build technologies to help
573040	577920	us create material abundance and to figure out societal problems that we have trouble with,
577920	581200	like how are we supposed to clean up the environment? And, you know, maybe you want
581200	586480	this to go and invent a bunch of little robots that will go out and be biodegradable and turn
586480	594720	ocean debris into harmless molecules. And I think that that positive side is something that I think
594720	599680	people miss sometimes when thinking about what an AGI will be like. And so I think that if you
599680	603840	have a system that's capable of all of that, you absolutely want its advice about how do I make sure
603840	609040	that we're using your capabilities in a positive way for humanity.
609040	614080	So what do you think about that psychology that looks at all the different possible
614080	619840	trajectories of an AGI system, many of which perhaps the majority of which are positive,
619840	624560	and nevertheless focuses on the negative trajectories? I mean, you get to interact with folks,
624560	630480	you get to think about this, maybe within yourself as well. You look at Sam Harris and so on.
630480	634960	It seems to be, sorry to put it this way, but almost more fun to think about the negative
635600	640720	possibilities. Whatever that's deep in our psychology, what do you think about that?
640800	644400	And how do we deal with it? Because we want AI to help us.
644400	647200	So I think there's kind of two problems
648160	653840	entailed in that question. The first is more of the question of how can you even picture
653840	658320	what a world with a new technology will be like? Now imagine we're in 1950 and I'm trying to
658320	668240	describe Uber to someone. Apps and the internet. Yeah, I mean, that's going to be extremely
668240	674560	complicated, but it's imaginable. It's imaginable, right? And now imagine being in 1950 and predicting
674560	679760	Uber, right? And you need to describe the internet, you need to describe GPS, you need to describe
679760	685680	the fact that everyone's going to have this phone in their pocket. And so I think that just the first
685680	690240	truth is that it is hard to picture how a transformative technology will play out in the world.
691040	695440	We've seen that before with technologies that are far less transformative than AGI will be.
695440	701120	And so I think that one piece is that it's just even hard to imagine and to really put yourself
701120	707760	in a world where you can predict what that positive vision would be like. And I think the
707760	714720	second thing is that I think it is always easier to support the negative side than the positive
714720	721600	side. It's always easier to destroy than create. And less in a physical sense and more just in an
721600	726160	intellectual sense, right? Because I think that with creating something, you need to just get a
726160	731360	bunch of things right and to destroy, you just need to get one thing wrong. And so I think that
731360	735360	what that means is that I think a lot of people's thinking dead ends as soon as they see the negative
735360	743040	story. But that being said, I actually have some hope, right? I think that the positive vision
743040	748640	is something that I think can be, is something that we can talk about. And I think that just
748720	752400	simply saying this fact of, yeah, like there's positive, there's negatives, everyone likes to
752400	756160	dwell on the negative, people actually respond well to that message and say, huh, you're right,
756160	760080	there's a part of this that we're not talking about, not thinking about. And that's actually
760080	766640	something that's I think really been a key part of how we think about AGI at OpenAI, right? You
766640	770880	can kind of look at it as like, okay, like OpenAI talks about the fact that there are risks,
770880	775360	and yet they're trying to build this system. Like, how do you square those two facts?
775920	781440	So do you share the intuition that some people have, I mean, from Sam Harris to even Elon Musk
781440	790400	himself, that it's tricky as you develop AGI to keep it from slipping into the existential threats,
790400	798240	into the negative? What's your intuition about how hard is it to keep AI development on the
798240	802240	positive track? What's your intuition there? To answer that question, you can really look at
802240	807120	how we structure OpenAI. So we really have three main arms. So we have capabilities, which is
807120	812400	actually doing the technical work and pushing forward what these systems can do. There's safety,
812400	817520	which is working on technical mechanisms to ensure that the systems we build are aligned with human
817520	822240	values. And then there's policy, which is making sure that we have governance mechanisms, answering
822240	827760	that question of, well, whose values. And so I think that the technical safety one is the one
827760	832160	that people kind of talk about the most, right? You talk about, like, think about, you know,
832160	837040	all of the dystopic AI movies, a lot of that is about not having good technical safety in place.
837760	841280	And what we've been finding is that, you know, I think that actually a lot of people
841280	843760	look at the technical safety problem and think it's just intractable.
845040	849040	Right, this question of, what do humans want? How am I supposed to write that down?
849040	855280	Can I even write down what I want? No way. And then they stop there. But the thing is,
855280	861280	we've already built systems that are able to learn things that humans can't specify, you know,
861280	864880	even the rules for how to recognize if there's a cat or a dog in an image.
864880	867760	Turns out it's intractable to write that down. And yet we're able to learn it.
868320	871680	And that what we're seeing with systems we build at OpenAI, and they're still
871680	876480	in early proof of concept stage, is that you are able to learn human preferences. You're able
876480	880960	to learn what humans want from data. And so that's kind of the core focus for our technical
880960	885680	safety team. And I think that they're actually, we've had some pretty encouraging updates in
885680	890880	terms of what we've been able to make work. So you have an intuition and a hope that from data,
891600	896160	you know, looking at the value alignment problem, from data, we can build systems that align
896960	904080	with the collective better angels of our nature. So align with the ethics and the morals of human
904080	908240	beings. To even say this in a different way, I mean, think about how do we align humans,
908240	913120	right? Think about like a human baby can grow up to be an evil person or a great person. And a lot
913120	917520	of that is from learning from data, right? That you have some feedback as a child is growing up,
917520	923680	they get to see positive examples. And so I think that just like the only example we have of a
923680	930160	general intelligence that is able to learn from data, to align with human values and to learn
930160	936160	values, I think we shouldn't be surprised that we can do the same sorts of techniques or whether
936160	940320	the same sort of techniques end up being how we, we, we saw value alignment for AGI's.
940960	946800	So let's go even higher. I don't know if you've read the book sapiens, but there's an idea that,
946800	951680	you know, that as a collective as us human beings, we kind of develop together
952480	958640	and ideas that we hold. There's no, in that context, objective truth, we just kind of all
958640	962720	agree to certain ideas and hold them as a collective. Did you have a sense that there is
963360	967440	in the world of good and evil, do you have a sense that to the first approximation,
967440	973520	there are some things that are good and that you could teach systems to behave to be good?
974400	979840	So I think that this actually blends into our third team, right, which is the policy team.
979840	984960	And this is the one, the aspect that I think people really talk about way less than they should,
984960	988960	right? Because imagine that we build super powerful systems that we've managed to figure
988960	993280	out all the mechanisms for these things to do whatever the operator wants. The most important
993280	998400	question becomes, who's the operator? What do they want? And how is that going to affect everyone
998400	1004640	else? Right? And, and I think that this question of what is good, what are those values? I mean,
1004640	1008320	I think you don't even have to go to those, those, those very grand existential places
1008320	1013280	to start to realize how hard this problem is. You just look at different countries and cultures
1013280	1018800	across the world and that there's, there's a very different conception of how the world works and
1018960	1024800	you know, what, what, what kinds of ways that society wants to operate. And so I think that,
1024800	1030880	that the really core question is, is actually very concrete. And I think it's not a question
1030880	1036480	that we have ready answers to, right? Is how do you have a world where all the different countries
1036480	1042720	that we have, United States, China, Russia, and you know, the hundreds of other countries out there
1042720	1049440	are able to continue to not just operate in the way that they, that they see fit, but in, in a,
1049440	1054320	that the world that emerges in these, where you have these very powerful systems,
1056000	1060560	operating alongside humans, ends up being something that empowers humans more, that makes
1060560	1066480	like, human existence be a more meaningful thing and that people are happier and wealthier and
1067040	1070960	able to live more fulfilling lives. It's not an obvious thing for how to design that world
1071520	1076480	once you have that very powerful system. So if we take a little step back and we're having
1076480	1081920	like a fascinating conversation and Open AI is in many ways a tech leader in the world.
1081920	1086240	And yet we're thinking about these big existential questions, which is fascinating,
1086240	1090720	really important. I think you're a leader in that space and that's a really important space
1090720	1095760	of just thinking how AI affects society in a big picture view. So Oscar Wilde said,
1096320	1100800	we're all in the gutter, but some of us are looking at the stars. And I think Open AI has a
1101360	1106240	charter that looks to the stars, I would say, to create intelligence, to create general
1106240	1110080	intelligence, make it beneficial, safe, and collaborative. So can you tell me
1112160	1117760	how that came about? How a mission like that and the path to creating a mission like that at Open
1117760	1124000	AI was founded? Yeah. So I think that in some ways it really boils down to taking a look at the
1124000	1129840	landscape. So if you think about the history of AI that basically for the past 60 or 70 years,
1129840	1134960	people have thought about this goal of what could happen if you could automate human intellectual
1134960	1140560	labor. Imagine you can build a computer system that could do that. What becomes possible? We have
1140560	1144960	a lot of sci-fi that tells stories of various dystopias and increasingly you have movies like
1144960	1150000	Her that tell you a little bit about maybe more of a little bit utopic vision. You think about
1150560	1157040	the impacts that we've seen from being able to have bicycles for our minds in computers,
1157760	1164320	and that I think that the impact of computers on the internet has just far outstripped what anyone
1164320	1169280	really could have predicted. And so I think that it's very clear that if you can build an AGI,
1169280	1172640	it will be the most transformative technology that humans will ever create.
1174480	1179680	And so what it boils down to then is a question of, well, is there a path? Is there hope? Is there
1179680	1185040	a way to build such a system? And I think that for 60 or 70 years that people got excited and
1186080	1191920	that ended up not being able to deliver on the hopes that people had pinned on them. And I think
1191920	1199520	that then that after two winters of AI development, that people I think kind of almost stopped daring
1199520	1205360	to dream that really talking about AGI or thinking about AGI became almost this taboo in the community.
1206320	1210880	But I actually think that people took the wrong lesson from AI history. And if you look back,
1210880	1215760	starting in 1959 is when the Perceptron was released. And this is basically one of the
1215760	1220640	earliest neural networks. It was released to what was perceived as this massive over-hype.
1220640	1227200	So in the New York Times in 1959, you have this article saying that the Perceptron will one day
1227200	1231360	recognize people, call out their names, instantly translate speech between languages,
1231360	1236080	and people at the time looked at this and said, this is your system can't do any of that. And
1236080	1240560	basically spent 10 years trying to discredit the whole Perceptron direction and succeeded.
1240560	1245840	And all the funding dried up and people kind of went in other directions. And in the 80s,
1245840	1250160	there was this resurgence. And I'd always heard that the resurgence in the 80s was due to the
1250160	1254960	invention of back propagation and these algorithms that got people excited. But actually, the causality
1254960	1259040	was due to people building larger computers. That you can find these articles from the 80s
1259040	1262800	saying that the democratization of computing power suddenly meant that you could run these
1262800	1266960	larger neural networks. And then people started to do all these amazing things back propagation
1266960	1270880	algorithm was invented. And the neural nets people were running were these tiny little
1270880	1276080	like 20 neuron neural nets. What are you supposed to learn with 20 neurons? And so, of course,
1276720	1281840	they weren't able to get great results. And it really wasn't until 2012 that this approach
1281840	1286800	that's almost the most simple, natural approach that people had come up with in the 50s.
1287360	1290640	Right. And in some ways, even in the 40s, before there were computers with the pits,
1290640	1296480	McCullin there in Iran, suddenly this became the best way of solving problems.
1297120	1300640	Right. And I think there are three core properties that deep learning has
1301360	1306560	that I think are very worth paying attention to. The first is generality. We have a very small
1306560	1312000	number of deep learning tools, SGD, deep neural net, maybe some, some, you know, RL,
1312960	1317200	and it solves this huge variety of problems, speech recognition, machine translation,
1317200	1321920	game playing, all of these problems, small set of tools. So there's the generality.
1322800	1326160	There's a second piece, which is the competence. You want to solve any of those problems
1326880	1331360	throughout 40 years worth of normal computer vision research, replace it with a deep neural
1331360	1336160	net. It's kind of worked better. And there's a third piece, which is the scalability, right?
1336160	1340400	That one thing that has been shown time and time again is that you, if you have a larger neural
1340400	1346880	network, throw more compute, more data at it, it will work better. Those three properties together
1346880	1351920	feel like essential parts of building a general intelligence. Now, it doesn't just mean that
1351920	1356640	if we scale up what we have, that we will have an AGI, right? There are clearly missing pieces.
1356640	1362320	There are missing ideas. We need to have answers for reasoning. But I think that the core here
1362320	1367840	is that for the first time, it feels that we have a paradigm that gives us hope
1367840	1372080	that general intelligence can be achievable. And so as soon as you believe that,
1372080	1376880	everything else becomes, comes into focus, right? If you imagine that you may be able to and,
1376880	1381040	you know, that the timeline, I think, remains uncertain. But I think that, you know,
1381040	1384880	certainly within our lifetimes and possibly within a much shorter period of time than,
1384880	1389440	than people would expect, if you can really build the most transformative technology that will
1389440	1393520	ever exist, you stop thinking about yourself so much, right? And you start thinking about just
1393520	1397280	like, how do you have a world where this goes well, and that you need to think about the
1397280	1401600	practicalities of how do you build an organization and get together a bunch of people and resources,
1402240	1409760	and to make sure that people feel motivated and ready to do it. But I think that then
1409760	1413520	you start thinking about, well, what if we succeed? And how do we make sure that when we
1413520	1418640	succeed, that the world is actually the place that we want ourselves to exist in? And, you know,
1418640	1423840	almost in the Rawlsian Bale sense of the word. And so that's kind of the broader landscape.
1423840	1431440	And OpenAI was really formed in 2015 with that high level picture of AGI might be possible
1431440	1437440	sooner than people think, and that we need to try to do our best to make sure it's going to go well.
1437440	1441040	And then we spent the next couple of years really trying to figure out, what does that mean? How do
1441040	1447440	we do it? And, you know, I think that typically with a company, you start out very small. So you
1447440	1450960	want to co-founder and you build a product, you get some users, you get a product market fit,
1451280	1456560	then at some point you raise some money, you hire people, you scale, and then down the road,
1456560	1461120	then the big companies realize you exist and try to kill you. And for OpenAI, it was basically
1461120	1467440	everything in exactly the opposite order. Let me just pause for a second. You said a lot of things.
1467440	1475040	And let me just admire the jarring aspect of what OpenAI stands for, which is daring to dream.
1475040	1479200	I mean, you said it's pretty powerful. You caught me off guard because I think that's very true.
1480160	1486640	The step of just daring to dream about the possibilities of creating intelligence
1486640	1493520	in a positive and a safe way, but just even creating intelligence is a much needed, refreshing
1495600	1500320	catalyst for the AI community. So that's the starting point. Okay. So then the formation of
1500320	1506880	OpenAI. I would just say that, you know, when we're starting OpenAI, that kind of the first
1506880	1511280	question that we had is, is it too late to start a lab with a bunch of the best people?
1511920	1515200	Right? Was that even possible? That was an actual question. That was really,
1515200	1520400	that was the core question of, you know, we had this dinner in July of 2015, and that was
1520400	1525120	really what we spent the whole time talking about. And, you know, because it's, you think
1525120	1531440	about kind of where AI was, is that it transitioned from being an academic pursuit to an industrial
1531440	1536000	pursuit. And so a lot of the best people were in these big research labs, and that we wanted to
1536000	1541200	start our own one that, you know, no matter how much resources we could accumulate would be,
1541200	1545280	you know, pale in comparison to the big tech companies. And we knew that. And there's a
1545280	1549280	question of, are we going to be actually able to get this thing off the ground? You need critical
1549280	1553440	mass. You can't just do you and a co-founder build a product, right? You really need to have a group
1553440	1559600	of, you know, five to 10 people. And we kind of concluded it wasn't obviously impossible. So it
1559600	1565760	seemed worth trying. Well, you're also a dreamer. So who knows, right? That's right. Okay. So
1566880	1574400	speaking of that, competing with the big players, let's talk about some of the tricky things as you
1574400	1581440	think through this process of growing, of seeing how you can develop these systems at scale that
1581440	1590160	competes. So you recently formed OpenAI LP, a new cap profit company that now carries the name
1590160	1596560	OpenAI. So OpenAI is now this official company. The original nonprofit company still exists and
1596560	1602480	carries the OpenAI nonprofit name. So can you explain what this company is, what the purpose
1602480	1610720	of its creation is, and how did you arrive at the decision to create it? OpenAI, the whole entity
1610720	1616960	and OpenAI LP as a vehicle is trying to accomplish the mission of ensuring that artificial general
1616960	1620800	intelligence benefits everyone. And the main way that we're trying to do that is by actually
1620800	1624800	trying to build general intelligence to ourselves and make sure the benefits are distributed to the
1624800	1629840	world. That's the primary way. We're also fine if someone else does this, right? It doesn't have
1629840	1634560	to be us. If someone else is going to build an AGI and make sure that the benefits don't get locked
1634560	1640320	up in one company or, you know, with one set of people, like, we're actually fine with that.
1641040	1648240	And so those ideas are baked into our charter, which is kind of the foundational document
1648240	1654320	that describes kind of our values and how we operate. But it's also really baked into the
1654320	1660640	structure of OpenAI LP. And so the way that we've set up OpenAI LP is that in the case where we
1660640	1666480	succeed, right, if we actually build what we're trying to build, then investors are able to get
1666560	1672560	a return. But that return is something that is capped. And so if you think of AGI in terms of the
1672560	1676480	value that you could really create, you're talking about the most transformative technology ever
1676480	1681120	created, it's going to create, or does the magnitude more value than any existing company?
1681760	1687760	And that all of that value will be owned by the world, like legally titled to the nonprofit
1687760	1691680	to fulfill that mission. And so that's the structure.
1692640	1698880	So the mission is a powerful one. And it's one that I think most people would agree with.
1698880	1705600	It's how we would hope AI progresses. And so how do you tie yourself to that mission? How do you
1705600	1713360	make sure you do not deviate from that mission? That, you know, other incentives that are profit
1713360	1718960	driven wouldn't don't interfere with the mission. So this was actually a really core question for
1718960	1723200	us for the past couple of years, because, you know, I'd say that like the way that our history
1723200	1726960	went was that for the first year, we were getting off the ground, right? We had this high level
1726960	1733360	picture, but we didn't know exactly how we wanted to accomplish it. And really two years ago is
1733360	1737840	when we first started realizing in order to build AGI, we're just going to need to raise way more
1737840	1743120	money than we can as a nonprofit. And you know, we're talking many billions of dollars. And so
1744320	1747920	the first question is, how are you supposed to do that and stay true to this mission?
1748560	1752000	And we looked at every legal structure out there and included none of them are quite right for
1752000	1755200	what we wanted to do. And I guess it shouldn't be too surprising if you're going to do some like
1755200	1758800	crazy unprecedented technology that you're going to have to come with some crazy unprecedented
1758800	1766000	structure to do it in. And a lot of our conversation was with people at OpenAI, right,
1766000	1770400	the people who really joined because they believe so much in this mission, and thinking about how
1770400	1776160	do we actually raise the resources to do it and also stay true to what we stand for. And the place
1776160	1780000	you've got to start is to really align on what is it that we stand for, right? What are those
1780000	1784880	values? What's really important to us? And so I'd say that we spent about a year really compiling
1784880	1789680	the OpenAI charter and that determines, and if you even look at the first the first line item in
1789680	1793600	there, it says that look, we expect we're going to have to marshal huge amounts of resources,
1793600	1798560	but we're going to make sure that we minimize conflict of interest with the mission. And that
1798560	1803280	kind of aligning on all of those pieces was the most important step towards figuring out
1804160	1809600	how do we structure a company that can actually raise the resources to do what we need to do.
1810240	1817040	I imagine OpenAI, the decision to create OpenAI LP was a really difficult one. And there was a
1817040	1823760	lot of discussions as you mentioned for a year. And there's different ideas, perhaps detractors
1823760	1830080	within OpenAI, sort of different paths that you could have taken. What were those concerns?
1830080	1833920	What were the different paths considered? What was that process of making that decision like?
1834800	1840240	But so if you look actually at the OpenAI charter, that there's almost two paths embedded within it.
1840800	1846160	There is, we are primarily trying to build AGI ourselves, but we're also okay if someone else
1846160	1851440	does it. And this is a weird thing for a company. It's really interesting, actually. Yeah. There
1851440	1857360	is an element of competition that you do want to be the one that does it. But at the same time,
1857360	1860880	you're okay if somebody else doesn't. We'll talk about that a little bit, that trade-off,
1860880	1864960	that dance that's really interesting. And I think this was the core tension as we were
1864960	1869920	designing OpenAI LP and really the OpenAI strategy, is how do you make sure that both,
1869920	1875120	you have a shot at being a primary actor, which really requires building an organization,
1875760	1880480	raising massive resources, and really having the will to go and execute on some really,
1880480	1885120	really hard vision. You need to really sign up for a long period to go and take on a lot of
1885120	1891600	pain and a lot of risk. And to do that, normally you just import the startup mindset, right?
1891600	1895360	And that you think about, okay, how do we out-execute everyone? You have this very competitive
1895360	1899760	angle. But you also have the second angle of saying that, well, the true mission isn't for
1899760	1906560	OpenAI to build AGI. The true mission is for AGI to go well for humanity. And so how do you take
1906560	1912000	all of those first actions and make sure you don't close the door on outcomes that would actually
1912000	1916560	be positive and fulfill the mission? And so I think it's a very delicate balance, right?
1916560	1921200	And I think that going 100% one direction or the other is clearly not the correct answer.
1921200	1925280	And so I think that even in terms of just how we talk about OpenAI and think about it,
1925280	1929600	there's just like one thing that's always in the back of my mind is to make sure
1929600	1934640	that we're not just saying OpenAI's goal is to build AGI, right? That it's actually much
1934640	1940160	broader than that, right? That, first of all, it's not just AGI. It's safe AGI that's very important.
1940160	1943920	But secondly, our goal isn't to be the ones to build it. Our goal is to make sure it goes well
1943920	1948720	for the world. And so I think that figuring out how do you balance all of those and to get people
1948720	1955760	to really come to the table and compile the like a single document that encompasses all of that,
1956480	1963440	wasn't trivial. So part of the challenge here is your mission is, I would say, beautiful,
1963440	1968160	empowering and a beacon of hope for people in the research community and just people thinking
1968160	1975760	about AI. So your decisions are scrutinized more than I think a regular profit driven company.
1975760	1980080	Do you feel the burden of this in the creation of the charter and just in the way you operate?
1980080	1980320	Yes.
1982880	1990320	So why do you lean into the burden by creating such a charter? Why not keep it quiet?
1990320	1995360	I mean, it just boils down to the mission, right? Like, I'm here and everyone else is here because
1995360	1997840	we think this is the most important mission, right?
1997840	2004640	Dare to dream. All right. So do you think you can be good for the world or create an AGI system
2004640	2011520	that's good when you're a for profit company? From my perspective, I don't understand why profit
2012880	2021280	interferes with positive impact on society. I don't understand why Google that makes most
2021280	2027360	of its money from ads can't also do good for the world or other companies, Facebook, anything.
2027360	2035920	I don't understand why those have to interfere. Profit isn't the thing in my view that affects
2035920	2041280	the impact of a company. What affects the impact of the company is the charter, is the culture,
2041280	2048640	is the people inside and profit is the thing that just fuels those people. What are your views there?
2048640	2054080	Yeah. So I think that's a really good question and there's some real long-standing debates
2054080	2058560	in human society that are wrapped up in it. The way that I think about it is just think about
2058560	2065840	what are the most impactful nonprofits in the world? What are the most impactful for profits in
2065840	2070800	the world? Right. It's much easier to list the for profits. That's right. And I think that there's
2070800	2077120	some real truth here that the system that we set up, the system for kind of how today's world is
2077120	2084320	organized is one that really allows for huge impact and that kind of part of that is that you
2084320	2091280	need to be, that for profits are self-sustaining and able to kind of build on their own momentum.
2091280	2095920	And I think that's a really powerful thing. It's something that when it turns out that we
2095920	2100000	haven't set the guardrails correctly causes problems. Think about logging companies that go
2100000	2105840	into the rainforest. That's really bad. We don't want that. And it's actually really interesting
2105840	2111200	to me that kind of this question of how do you get positive benefits out of a for-profit company?
2111200	2114560	It's actually very similar to how do you get positive benefits out of an AGI,
2115600	2120240	that you have this very powerful system. It's more powerful than any human and
2120240	2124400	it's kind of autonomous in some ways. It's super human in a lot of axes and somehow you have to
2124400	2130000	set the guardrails to get good things to happen. But when you do, the benefits are massive. And so
2130000	2135600	I think that when I think about nonprofit versus for-profit, I think it's just not enough
2135680	2139920	happens in nonprofits. They're very pure, but it's just kind of, you know, it's just hard to do things
2139920	2146320	there. And for-profits in some ways, like too much happens. But if kind of shaped in the right way,
2146320	2152320	it can actually be very positive. And so with OpenILP, we're picking a road in between. Now,
2152320	2156000	the thing that I think is really important to recognize is that the way that we think about
2156000	2161520	OpenILP is that in the world where AGI actually happens, right? In a world where we are successful,
2161520	2164960	we build the most transformative technology ever, the amount of value we're going to create will
2164960	2174160	be astronomical. And so then in that case, the cap that we have will be a small fraction of the
2174160	2178880	value we create. And the amount of value that goes back to investors and employees looks pretty
2178880	2185200	similar to what would happen in a pretty successful startup. And that's really the case that we're
2185200	2190560	optimizing for, right? That we're thinking about in the success case, making sure that the value we
2190560	2195200	create doesn't get locked up. And I expect that in other, you know, for-profit companies that it's
2195200	2200320	possible to do something like that, I think it's not obvious how to do it, right? And I think that
2200320	2204400	as a for-profit company, you have a lot of fiduciary duty to your shareholders and that
2204400	2209360	there are certain decisions that you just cannot make. In our structure, we've set it up so that
2210080	2215200	we have a fiduciary duty to the charter that we always get to make the decision that is right
2215200	2220080	for the charter rather than even if it comes at the expense of our own stakeholders.
2220640	2224880	And so I think that when I think about what's really important, it's not really about
2224880	2230320	non-profit versus for-profit. It's really a question of if you build a GI and you kind of,
2230320	2236240	you know, humanity is now at this new age, who benefits? Whose lives are better? And I think
2236240	2242080	that what's really important is to have an answer that is everyone. Yeah, which is one of the core
2242080	2247040	aspects of the charter. So one concern people have, not just with OpenAI, but with Google,
2247040	2256400	Facebook, Amazon, anybody, really, that's creating impact at scale is how do we avoid,
2256400	2262480	as your charter says, avoid enabling the use of AI or AGI to unduly concentrate power?
2263520	2268480	Why would not a company like OpenAI keep all the power of an AGI system to itself?
2268480	2269360	The charter.
2269360	2271520	The charter. So, you know, how does the charter
2271920	2276640	actualize itself in day to day?
2277200	2282000	So I think that the first to zoom out, right, that the way that we structure the company is so
2282000	2287280	that the power for sort of, you know, dictating the actions that OpenAI takes ultimately rests
2287280	2292240	with the board, right? The board of the non-profit and the board is set up in certain ways with
2292240	2296640	certain restrictions that you can read about in the OpenAI LP blog post. But effectively,
2296720	2304320	the board is the governing body for OpenAI LP. And the board has a duty to fulfill the mission
2304320	2310080	of the non-profit. And so that's kind of how we tie, how we thread all these things together.
2310800	2315520	Now, there's a question of so day to day, how do people, the individuals who in some ways are
2315520	2319760	the most empowered ones, right? You know, the board sort of gets to call the shots at the high level,
2319760	2323840	but the people who are actually executing are the employees, right? The people here
2323840	2327360	on a day to day basis who have the, you know, the keys to the technical whole kingdom.
2328880	2333840	And there, I think that the answer looks a lot like, well, how does any company's values
2333840	2338080	get actualized, right? And I think that a lot of that comes down to the unique people who are here
2338080	2343440	because they really believe in that mission and they believe in the charter and that they
2343440	2348720	are willing to take actions that maybe are worse for them, but are better for the charter.
2348720	2353120	And that's something that's really baked into the culture. And honestly, I think it's, you know,
2353120	2357600	I think that that's one of the things that we really have to work to preserve as time goes on.
2358240	2362080	And that's a really important part of how we think about hiring people and bringing people
2362080	2367440	into open AI. So there's people here, there's people here who could speak up and say,
2368960	2374480	like, hold on a second, this is totally against what we stand for, culture wise.
2374480	2378000	Yeah, yeah, for sure. I mean, I think that we actually have, I think that's like a pretty
2378000	2384320	important part of how we operate and how we have, even again, with designing the charter and
2384320	2389440	designing open ALP in the first place, that there has been a lot of conversation with employees
2389440	2393200	here and a lot of times where employees said, wait a second, this seems like it's going in
2393200	2397280	the wrong direction. And let's talk about it. And so I think one thing that's, I think a really,
2397280	2402080	and you know, here's actually one thing that I think is very unique about us as a small company
2402080	2405760	is that if you're at a massive tech giant, that's a little bit hard for someone who's
2405760	2410000	aligned employee to go and talk to the CEO and say, I think that we're doing this wrong.
2410720	2415600	And, you know, you'll get companies like Google that have had some collective action from employees
2415600	2420080	to, you know, make ethical changes around things like Maven. And so maybe there are
2420080	2424320	mechanisms that other companies that work. But here, super easy for anyone to pull me aside,
2424320	2427040	to pull Sam aside, to pull Eli aside, and people do it all the time.
2427680	2432080	One of the interesting things in the charter is this idea that it'd be great if you could try to
2432080	2438720	describe or untangle switching from competition to collaboration and lead stage AGI development.
2438720	2442400	It's really interesting this dance between competition and collaboration. How do you
2442400	2446880	think about that? Yeah, assuming that you can actually do the technical side of AGI development,
2446880	2450320	I think there's going to be two key problems with figuring out how do you actually deploy it,
2450320	2455280	make it go well. The first one of these is the run up to building the first AGI.
2456240	2459840	You look at how self-driving cars are being developed, and it's a competitive race.
2460800	2464080	The thing that always happens in competitive race is that you have huge amounts of pressure
2464080	2469920	to get rid of safety. And so that's one thing we're very concerned about, right, is that people,
2469920	2476640	multiple teams figuring out, we can actually get there. But, you know, if we took the slower path
2476640	2482240	that is more guaranteed to be safe, we will lose. And so we're going to take the fast path.
2482240	2487360	And so the more that we can, both ourselves, be in a position where we don't generate that
2487360	2492400	competitive race, where we say, if the race is being run and that someone else is further
2492400	2496800	ahead than we are, we're not going to try to leapfrog. We're going to actually work with them,
2496800	2501360	right? We will help them succeed. As long as what they're trying to do is to fulfill our mission,
2502000	2505760	then we're good. We don't have to build AGI ourselves. And I think that's a really important
2505760	2509600	commitment from us. But it can't just be unilateral, right? I think that it's really
2509600	2514320	important that other players who are serious about building AGI make similar commitments,
2514320	2518240	right? And I think that that, you know, again, to the extent that everyone believes that AGI
2518240	2521600	should be something to benefit everyone, then it actually really shouldn't matter which company
2521600	2525920	builds it. And we should all be concerned about the case where we just race so hard to get there
2525920	2532000	that something goes wrong. So what role do you think government, our favorite entity,
2532000	2538400	has in setting policy and rules about this domain, from research to the development to
2539360	2542800	early stage to late stage AI and AGI development?
2542800	2547120	So I think that, first of all, it's really important that government's in there,
2547680	2550800	right? In some way, shape or form, you know, at the end of the day, we're talking about
2550800	2557120	building technology that will shape how the world operates and that there needs to be government
2557120	2563520	as part of that answer. And so that's why we've done a number of different congressional testimonies.
2563520	2569200	We interact with a number of different lawmakers and that, you know, right now, a lot of our message
2569200	2576800	to them is that it's not the time for regulation, it is the time for measurement, right? That our
2576800	2580560	main policy recommendation is that people, and, you know, the government does this all the time
2580560	2586480	with bodies like NIST, spend time trying to figure out just where the technology is,
2586480	2592160	how fast it's moving and can really become literate and up to speed with respect to what to
2592160	2598080	expect. So I think that today, the answer really is about measurement. And I think that there will
2598080	2604160	be a time and place where that will change. And I think it's a little bit hard to predict exactly
2605440	2607040	what exactly that trajectory should look like.
2607040	2613440	So there will be a point at which regulation, federal and the United States, the government
2613440	2620720	steps in and helps be the, I don't want to say the adult in the room, to make sure that there is
2620800	2624480	strict rules, maybe conservative rules that nobody can cross.
2625040	2629760	Well, I think there's kind of maybe two angles to it. So today with narrow AI applications,
2629760	2633840	that I think there are already existing bodies that are responsible and should be responsible
2633840	2638320	for regulation. You think about, for example, with self-driving cars, that you want the, you know,
2638320	2643920	the national highway, NETSA, exactly to be regulated in that, that makes sense, right?
2643920	2648160	That basically what we're saying is that we're going to have these technological systems that
2648160	2653760	are going to be performing applications that humans already do. Great. We already have ways
2653760	2657920	of thinking about standards and safety for those. So I think actually empowering those regulators
2657920	2664080	today is also pretty important. And then I think for, for a GI, you know, that there's going to
2664080	2667520	be a point where we'll have better answers. And I think that maybe a similar approach
2667520	2671440	of first measurement and, you know, start thinking about what the rules should be.
2671440	2676240	I think it's really important that we don't prematurely squash, you know, progress. Like,
2676240	2681120	I think it's very easy to kind of smother the budding field. And I think that's something to
2681120	2686240	really avoid. But I don't think that the right way of doing it is to say, let's just try to blaze
2686240	2696240	ahead and not involve all these other stakeholders. So you've recently released a paper on GPT2
2696240	2703680	language modeling, but did not release the full model because you had concerns about
2703680	2710640	the possible negative effects of the availability of such model. It's outside of just that decision.
2710640	2716960	It's super interesting because of the discussion at a societal level, the discourse it creates. So
2716960	2723440	it's fascinating in that aspect. But if you think it's the specifics here at first, what are some
2723440	2728480	negative effects that you envisioned? And of course, what are some of the positive effects?
2728480	2734960	Yeah. So again, I think to zoom out, like the way that we thought about GPT2 is that with language
2734960	2741440	modeling, we are clearly on a trajectory right now where we scale up our models and we get
2742400	2747680	qualitatively better performance. Right. GPT2 itself was actually just a scale up of a model
2747680	2752720	that we've released in the previous June. Right. And we just ran it at, you know, much larger scale
2752720	2757760	and we got these results where suddenly starting to write coherent pros, which was not something
2757760	2764480	we'd seen previously. And what are we doing now? Well, we're going to scale up GPT2 by 10x, by 100x,
2764480	2770560	by 1000x, and we don't know what we're going to get. And so it's very clear that the model that
2770560	2776720	we released last June, you know, I think it's kind of like, it's a good academic toy. It's not
2776720	2780720	something that we think is something that can really have negative applications or, you know,
2780720	2785360	to the extent that it can, that the positive of people being able to play with it is, you know,
2785600	2791600	far outweighs the possible harms. You fast forward to not GPT2, but GPT220,
2792400	2797120	and you think about what that's going to be like. And I think that the capabilities are going to be
2797120	2803360	substantive. And so there needs to be a point in between the two where you say, this is something
2803360	2807920	where we are drawing the line, and that we need to start thinking about the safety aspects.
2807920	2811600	And I think for GPT2, we could have gone either way. And in fact, when we had conversations
2811600	2816640	internally, that we had a bunch of pros and cons, and it wasn't clear which one, which one
2816640	2821040	outweighed the other. And I think that when we announced that, hey, we decide not to release
2821040	2825120	this model, then there was a bunch of conversation where various people said, it's so obvious that
2825120	2827920	you should have just released it. There are other people said, it's so obvious you should not have
2827920	2832640	released it. And I think that that almost definitionally means that holding it back was the correct
2832640	2837760	decision, right? If it's not obvious whether something is beneficial or not, you should
2837760	2843680	probably default to caution. And so I think that the overall landscape for how we think about it
2843680	2847840	is that this decision could have gone either way. There are great arguments in both directions.
2847840	2852640	But for future models down the road, and possibly sooner than you'd expect, because
2852640	2857360	scaling these things up doesn't actually take that long, those ones, you're definitely not
2857360	2862480	going to want to release into the wild. And so I think that we almost view this as a test case,
2862480	2868160	and to see, can we even design, how do you have a society, or how do you have a system that goes
2868160	2873280	from having no concept of responsible disclosure, where the mere idea of not releasing something
2873280	2878560	for safety reasons is unfamiliar to a world where you say, okay, we have a powerful model,
2878560	2882080	let's at least think about it, let's go through some process. And you think about the security
2882080	2886160	community, it took them a long time to design responsible disclosure, right? You think about
2886160	2890560	this question of, well, I have a security exploit, I send it to the company, the company is like,
2890560	2897360	tries to prosecute me, or just ignores it. What do I do? And so the alternatives of, oh,
2897360	2901040	I just always publish your exploits, that doesn't seem good either. And so it really took a long
2901040	2907120	time, and it was bigger than any individual. It's really about building a whole community that
2907120	2910880	believe that, okay, we'll have this process where you send it to the company, if they don't act in
2910880	2915280	a certain time, then you can go public, and you're not a bad person, you've done the right thing.
2916080	2922560	And I think that in AI, part of the response to GPT-2 just proves that we don't have any concept
2922560	2930400	of this. So that's the high level picture. And so I think that this was a really important move
2930400	2935920	to make, and we could have maybe delayed it for GPT-3, but I'm really glad we did it for GPT-2.
2935920	2939280	And so now you look at GPT-2 itself, and you think about the substance of, okay,
2939280	2943600	what are potential negative applications? So you have this model that's been trained on the
2943600	2948240	internet, which is also going to be a bunch of very biased data, a bunch of very offensive
2948240	2954640	content in there, and you can ask it to generate content for you on basically any topic. You just
2954640	2959120	give it a prompt, and it'll just start writing, and it writes content like you see on the internet,
2959120	2965360	even down to saying advertisement in the middle of some of its generations. And you think about
2965360	2970160	the possibilities for generating fake news or abusive content. And it's interesting seeing
2970240	2975600	what people have done with... We released a smaller version of GPT-2, and the people have
2975600	2981280	done things like try to generate, take my own Facebook message history and generate more
2981280	2988960	Facebook messages like me, and people generating fake politician content or there's a bunch of
2988960	2993280	things there where you at least have to think, is this going to be good for the world?
2994560	2997760	There's the flip side, which is I think that there's a lot of awesome applications that
2997760	3004400	we really want to see, like creative applications in terms of if you have sci-fi authors that can
3004400	3009600	work with this tool and come with cool ideas, that seems awesome if we can write better sci-fi
3009600	3013360	through the use of these tools. And we've actually had a bunch of people write into us asking,
3013360	3017680	hey, can we use it for a variety of different creative applications?
3018240	3027040	So the positive are actually pretty easy to imagine. The usual NLP applications are
3027680	3033120	really interesting, but let's go there. It's kind of interesting to think about a world where,
3034160	3041040	look at Twitter, where not just fake news, but smarter and smarter bots being able to
3042480	3049440	spread in an interesting complex networking way information that just floods out us regular
3049440	3057920	human beings with our original thoughts. So what are your views of this world with GPT-20,
3058640	3064000	right? How do we think about it? Again, it's like one of those things about in the 50s trying to
3064000	3070320	describe the internet or the smartphone. What do you think about that world, the nature of
3070320	3078640	information? One possibility is that we'll always try to design systems that identify robot versus
3078640	3084800	human and we'll do so successfully. And so we'll authenticate that we're still human. And the
3084800	3090720	other world is that we just accept the fact that we're swimming in a sea of fake news and just
3090720	3101440	learn to swim there. Have you ever seen the popular meme of a robot with a physical arm and pen
3101520	3108880	clicking the I'm not a robot button? I think the truth is that really trying to distinguish between
3109520	3114080	robot and human is a losing battle. Ultimately, you think it's a losing battle? I think it's a
3114080	3118480	losing battle ultimately, right? I think that in terms of the content, in terms of the actions
3118480	3122320	that you can take, think about how captures have gone. The captures used to be a very nice,
3122320	3128320	simple. You just have this image, all of our OCR is terrible. You put a couple of artifacts in it,
3128800	3133840	humans are going to be able to tell what it is an AI system wouldn't be able to. Today,
3133840	3138480	I could barely do captures. And I think that this is just kind of where we're going. I think
3138480	3143600	captures were a moment in time thing. And as AI systems become more powerful, that there being
3143600	3149280	human capabilities that can be measured in a very easy automated way that the AIs will not be
3149280	3152960	capable of. I think that's just like, it's just an increasingly hard technical battle.
3153920	3158800	But it's not that all hope is lost, right? You think about how do we already authenticate
3159600	3164000	ourselves, right? We have systems, we have social security numbers, if you're in the U.S.
3164000	3171360	or you have ways of identifying individual people and having real world identity tied to digital
3171360	3176800	identity seems like a step towards authenticating the source of content rather than the content
3176800	3182800	itself. Now, there are problems with that. How can you have privacy and anonymity in a world
3182880	3187120	where the only content you can really trust is, or the only way you can trust content is by looking
3187120	3192240	at where it comes from? And so I think that building out good reputation networks may be
3192240	3198000	one possible solution. But yeah, I think that this question is not an obvious one. And I think that
3199200	3204400	maybe sooner than we think we'll be in a world where today, I often will read a tweet and be like,
3204400	3207920	do I feel like a real human wrote this? Or do I feel like this is genuine? I feel like I can
3207920	3212480	kind of judge the content a little bit. And I think in the future, it just won't be the case.
3212480	3218000	You look at, for example, the FCC comments on net neutrality. It came out later that millions of
3218000	3222880	those were auto-generated and that the researchers were able to do various statistical techniques
3222880	3227920	to do that. What do you do in a world where those statistical techniques don't exist? It's just
3227920	3233840	impossible to tell the difference between humans and AIs. And in fact, the most persuasive arguments
3233840	3240400	are written by AI. All that stuff, it's not sci-fi anymore. You look at GPT2 making a great argument
3240400	3244240	for why recycling is bad for the world. You got to read that and be like, huh, you're right.
3244240	3248240	We are addressing just the symptoms. Yeah, that's, that's quite interesting. I mean,
3248240	3253520	ultimately, it boils down to the physical world being the last frontier of proving,
3253520	3259280	so you said like basically networks of people, humans vouching for humans in the physical world
3259280	3266880	and somehow the authentication ends there. I mean, if I had to ask you, I mean, you're way too
3266880	3272400	eloquent for a human. So if I had to ask you to authenticate, like prove, how do I know you're
3272400	3280560	not a robot and how do you know I'm not a robot? I think that's so far we're in this space, this
3280560	3286960	conversation we just had, the physical movements we did, is the biggest gap between us and AI systems,
3286960	3292240	is the physical manipulation. So maybe that's the last frontier. Well, here's another question,
3292960	3299520	why is solving this problem important? Like what aspects are really important to us? I think that
3299520	3304800	probably where we'll end up is we'll hone in on what do we really want out of knowing if we're
3304800	3310800	talking to a human. And I think that again, this comes down to identity. And so I think that the
3310800	3314960	internet of the future, I expect to be one that will have lots of agents out there that will
3314960	3321920	interact with you. But I think that the question of is this real flesh and blood human or is this
3322240	3328880	an automated system may actually just be less important. Let's actually go there. It's GPT2
3329600	3340160	is impressive. And let's look at GPT20. Why is it so bad that all my friends are GPT20? Why is it
3340160	3347680	so important on the internet? Do you think to interact with only human beings? Why can't we
3347760	3352560	live in a world where ideas can come from models trained on human data?
3354080	3356880	I think this is actually a really interesting question. This comes back to the how do you even
3356880	3362000	picture a world with some new technology? And I think that one thing that I think is important
3362000	3370960	is, let's say honesty. And I think that if you have almost the Turing test style sense of technology,
3370960	3376800	you have AIs that are pretending to be humans and deceiving you. I think that feels like a bad
3376800	3381120	thing. I think that it's really important that we feel like we're in control of our environment,
3381120	3384880	that we understand who we're interacting with. And if it's an AI or a human,
3386560	3390480	that's not something that we're being deceived about. But I think that the flip side of can I
3390480	3395440	have as meaningful of an interaction with an AI as I can with a human? Well, I actually think here
3395440	3401280	you can turn to sci-fi and her I think is a great example of asking this very question. One thing
3401280	3406080	I really love about her is it really starts out almost by asking how meaningful our human virtual
3406080	3412880	relationships and then you have a human who has a relationship with an AI and that you really
3412880	3417360	start to be drawn into that and that all of your emotional buttons get triggered in the same way
3417360	3422560	as if there was a real human that was on the other side of that phone. And so I think that this is
3422560	3427680	one way of thinking about it is that I think that we can have meaningful interactions and that if
3427680	3432800	there's a funny joke, some sense it doesn't really matter if it was written by a human or an AI,
3432800	3436320	but what you don't want in a way where I think we should really draw hard lines
3436320	3441760	is deception. And I think that as long as we're in a world where why do we build AI systems at
3441760	3446000	all? The reason we want to build them is to enhance human lives, to make humans be able to do more
3446000	3451040	things, to have humans feel more fulfilled. And if we can build AI systems that do that,
3452160	3459200	sign me up. So the process of language modeling, how far do you think it take us? Let's look at
3459200	3466080	Movie Her. Do you think a dialogue, natural language conversation is formulated by the
3466080	3471040	Turing test, for example? Do you think that process could be achieved through this kind
3471040	3477760	of unsupervised language modeling? So I think the Turing test in its real form isn't just about
3477760	3482240	language. It's really about reasoning too. To really pass the Turing test, I should be able
3482240	3487760	to teach calculus to whoever's on the other side and have it really understand calculus and be able
3487760	3493360	to, you know, go and solve new calculus problems. And so I think that to really solve the Turing
3493360	3497600	test, we need more than what we're seeing with language models. We need some way of plugging
3497600	3503520	in reasoning. Now, how different will that be from what we already do? That's an open question,
3503520	3508000	right? It might be that we need some sequence of totally radical new ideas, or it might be that we
3508000	3513680	just need to kind of shape our existing systems in a slightly different way. But I think that
3513680	3518240	in terms of how far language modeling will go, it's already gone way further than many people
3518240	3522240	would have expected, right? I think that things like, and I think there's a lot of really interesting
3522240	3528720	angles to poke in terms of how much does GPT2 understand physical world? Like, you know, you
3528720	3534000	read a little bit about fire underwater in GPT2. So it's like, okay, maybe it doesn't quite understand
3534000	3539680	what these things are. But at the same time, I think that you also see various things like smoke
3539680	3544320	coming from flame and, you know, a bunch of these things that GPT2, it has no body, it has no physical
3544320	3551520	experience, it's just statically read data. And I think that I think that if the answer is like,
3551520	3556080	we don't know yet, then these questions, though, we're starting to be able to actually ask them
3556080	3560480	to physical systems, the real systems that exist. And that's very exciting. Do you think, what's
3560480	3567600	your intuition? Do you think if you just scale language modeling, like significantly scale,
3567600	3571200	that reasoning can emerge from the same exact mechanisms?
3571200	3577840	I think it's unlikely that if we just scale GPT2 that we'll have reasoning in the full fledged
3577840	3581360	way. And I think that there's like, you know, the type signature is a little bit wrong, right?
3581360	3586720	That like, there's something we do with, that we call thinking, right? Where we spend a lot of
3586720	3591120	compute, like a variable amount of compute to get to better answers, right? I think a little bit
3591120	3597920	harder, I get a better answer. And that that kind of type signature isn't quite encoded in a GPT,
3598560	3603920	right? GPT will kind of like, it's spent a long time in it's like evolutionary history, baking
3603920	3608400	and all this information getting very, very good at this predictive process. And then at runtime,
3608400	3614320	I just kind of do one forward pass and, and I'm able to generate stuff. And so, you know, there
3614320	3619120	might be small tweaks to what we do in order to get the type signature, right? For example, well,
3619120	3622480	you know, it's not really one forward pass, right? You know, you generate symbol by symbol.
3622480	3626800	And so maybe you generate like a whole sequence of thoughts and you only keep like the last bit
3626800	3630960	or something. Right. But I think that at the very least, I would expect you have to make changes
3630960	3638000	like that. Yeah, just exactly how we, you said think is the process of generating thought by
3638000	3642960	thought in the same kind of way, like you said, keep the last bit, the thing that we converge
3642960	3647200	towards. Yep. And I think there's, there's another piece, which is, which is interesting,
3647200	3652240	which is this out of distribution generalization, right? That like thinking somehow lets us do that,
3652240	3656000	right? That we have an experience of thing. And yet somehow we just kind of keep refining
3656000	3662240	our mental model of it. This is again, something that feels tied to whatever reasoning is.
3663360	3668000	And maybe it's a small tweak to what we do. Maybe it's many ideas and we'll take as many decades.
3668000	3674560	Yeah. So the assumption there, generalization out of distribution is that it's possible to create
3675440	3680880	new, new ideas. The pot, you know, it's possible that nobody's ever created any new ideas. And
3680880	3689760	then we're scaling GPT-2 to GPT-20, you would, you would essentially generalize to all possible
3689760	3695440	thoughts that us humans can have. Just to play devil's advocate. Right. Right. I mean, how many,
3695440	3699920	how many new, new story ideas have we come up with since Shakespeare, right? Yeah, exactly.
3700320	3704880	It's just all different forms of love and drama and so on. Okay.
3705680	3709200	Not sure if you read Biddle Lesson, a recent blog post by Ray Sutton.
3710800	3715680	He basically says something that echoes some of the ideas that you've been talking about, which is
3716720	3721120	he says the biggest lesson that can be read from 70 years of AI research is that
3721120	3726880	general methods that leverage computation are ultimately going to ultimately win out.
3727760	3734160	Do you agree with this? So basically open AI in general about the ideas you're exploring,
3734160	3740000	about coming up with methods, whether it's GPT-2 modeling, or whether it's open AI-5
3740000	3749120	playing Dota, where a general method is better than a more fine-tuned, expert-tuned method.
3749680	3753680	Yeah. So I think that, well, one thing that I think was really interesting about the reaction
3753680	3758400	to that blog post was that a lot of people have read this as saying that compute is all that
3758400	3763360	matters. And that's a very threatening idea, right? And I don't think it's a true idea either,
3763360	3766880	right? It's very clear that we have algorithmic ideas that have been very important
3766880	3770720	for making progress. And to really build AGI, you want to push as far as you can on the
3770720	3776000	computational scale, and you want to push as far as you can on human ingenuity. And so I think
3776000	3779280	you need both. But I think the way that you phrase the question is actually very good,
3779280	3784640	right? That it's really about what kind of ideas should we be striving for? And absolutely,
3784640	3789680	if you can find a scalable idea, you pour more compute into it, you pour more data into it,
3789680	3796480	it gets better. Like that's the real Holy Grail. And so I think that the answer to the question,
3796480	3801760	I think, is yes. That's really how we think about it, and that part of why we're excited
3801760	3806800	about the power of deep learning and the potential for building AGI is because we look at the system
3806800	3812720	that exists in the most successful AI systems, and we realize that you scale those up, they're
3812720	3816800	going to work better. And I think that that scalability is something that really gives us
3816800	3821360	hope for being able to build transformative systems. So I'll tell you, this is partially an
3821360	3826880	emotional, you know, a thing that a response that people often have, if compute is so important
3826880	3831680	for state of the art performance, you know, individual developers, maybe a 13 year old
3831680	3835600	sitting somewhere in Kansas or something like that, you know, they're sitting, they might not
3835600	3841600	even have a GPU and or maybe have a single GPU or 1080 or something like that. And there's this
3841600	3848560	feeling like, well, how can I possibly compete or contribute to this world of AI if scale is so
3848560	3854400	important? So if you can comment on that, and in general, do you think we need to also in the
3854400	3862480	future focus on democratizing compute resources more more or as much as we democratize the algorithms?
3862480	3868720	Well, so the way that I think about it is that there's this space of possible progress, right?
3868720	3873520	There's a space of ideas and sort of systems that will work that will move us forward. And there's a
3873520	3877760	portion of that space, and to some extent, an increasingly significant portion of that space
3877760	3884080	that does just require massive compute resources. And for that, that I think that the answer is
3884080	3888640	kind of clear, and that part of why we have the structure that we do is because we think it's
3888640	3892720	really important to be pushing the scale and to be, you know, building these large clusters and
3892720	3898000	systems. But there's another portion of the space that isn't about the large scale compute that are
3898000	3903200	these ideas that, and again, I think that for the ideas to really be impactful and really shine,
3903200	3907280	that they should be ideas that if you scale them up, would work way better than they do at small
3907280	3913120	scale. But you can discover them without massive computational resources. And if you look at the
3913120	3918080	history of recent developments, you think about things like the GAN or the VAE, that these are
3918080	3922560	ones that I think you could come up with them without having, and, you know, in practice,
3922560	3926320	people did come up with them without having massive, massive computational resources.
3926320	3932560	Right. I just talked to Ian Goodfellow, but the thing is, the initial GAN produced pretty terrible
3932560	3939840	results, right? So only because they're smart enough to know that this is quite surprising,
3939840	3945840	it can generate anything that they know. Do you see a world, or is that too optimistic and dream
3945840	3952320	or like, to imagine that the compute resources are something that's owned by governments and
3952320	3958960	provided as utility? Actually, to some extent, this question reminds me of a blog post from
3958960	3963600	one of my former professors at Harvard, this guy, Matt Welsh, who was a systems professor.
3963600	3968080	I remember sitting in his tenure talk, right? And, you know, that he had literally just gotten
3968080	3975600	tenure. He went to Google for the summer and then decided he wasn't going back to academia, right?
3975600	3980240	And that kind of in his blog post, he makes this point that, look, as a systems researcher,
3980880	3985040	that I come up with these cool system ideas, right? And I kind of build a little proof of concept.
3985040	3990880	And the best thing I could hope for is that the people at Google or Yahoo, which was around at
3990880	3995840	the time, will implement it and like, actually make it work at scale, right? That's like the
3995840	3998480	dream for me, right? I built the little thing and they've turned it into the big thing that's
3998480	4004800	actually working. And for him, he said, I'm done with that. I want to be the person who's actually
4004800	4009680	doing building and deploying. And I think that there's a similar dichotomy here, right? I think
4009680	4015040	that there are people who've really actually find value. And I think it is a valuable thing to do,
4015040	4019120	to be the person who produces those ideas, right? Who builds the proof of concept. And yeah, you
4019120	4025040	don't get to generate the coolest possible GAN images, but you invented the GAN, right? And so
4025040	4028880	that there's a real trade-off there. And I think that that's a very personal choice,
4028880	4030480	but I think there's value in both sides.
4030480	4036240	So do you think creating AGI, something or some new models,
4037840	4043440	we would see echoes of the brilliance even at the prototype level. So you would be able to develop
4043440	4050000	those ideas without scale, the initial seeds. So take a look at, I always like to look at
4050000	4056240	examples that exist, right? Look at real precedent. And so take a look at the June 2018 model that
4056240	4061920	we released that we scaled up to turn to GPT2. And you can see that at small scale, it set some
4061920	4067040	records, right? This was the original GPT. We actually had some cool generations that weren't
4067040	4072960	nearly as amazing and really stunning as the GPT2 ones, but it was promising. It was interesting.
4072960	4077440	And so I think it is the case that with a lot of these ideas that you see promise at small scale.
4078160	4082000	But there is an asterisk here, a very big asterisk, which is sometimes we see
4082480	4087600	behaviors that emerge that are qualitatively different from anything we saw at small scale.
4087600	4092720	And that the original inventor of whatever algorithm looks at and says, I didn't think
4092720	4098000	it could do that. This is what we saw in Dota, right? So PPO was created by John Shulman,
4098000	4104640	who's a researcher here. And with Dota, we basically just ran PPO at massive, massive scale.
4104640	4109520	And there's some tweaks in order to make it work, but fundamentally it's PPO at the core.
4110320	4116880	And we were able to get this long-term planning, these behaviors to really play out
4117840	4122560	on a time scale that we just thought was not possible. And John looked at that and was like,
4122560	4125760	I didn't think it could do that. That's what happens when you're at three orders of magnitude,
4125760	4130080	more scale than you tested at. Yeah, but it still has the same flavors of,
4131120	4137840	you know, at least echoes of the expected billions. Although I suspect with GPT,
4137920	4144160	scaled more and more, you might get surprising things. So yeah, you're right. It's interesting.
4144800	4150080	It's difficult to see how far an idea will go when it's scaled. It's an open question.
4150960	4155040	Well, so to that point with Dota and PPO, like, I mean, here's a very concrete one, right? It's
4156000	4159120	actually one thing that's very surprising about Dota that I think people don't really pay that
4159120	4164560	much attention to is the decree of generalization out of distribution that happens, right? That
4164560	4170240	you have this AI that's trained against other bots for its entirety, the entirety of its existence.
4170240	4178640	Sorry to take a step back. Can you talk through, you know, a story of Dota, a story of leading
4178640	4184560	up to opening I-5 and that past, and what was the process of self-play and so on of training?
4185280	4187280	Yeah, yeah, yeah. So with Dota. What is Dota?
4188480	4192640	Dota is a complex video game. And we started training, we started trying to solve Dota
4192640	4197680	because we felt like this was a step towards the real world relative to other games like Chess or Go,
4197680	4201760	right? Those very cerebral games where you just kind of have this board of very discrete moves.
4201760	4205520	Dota starts to be much more continuous time. So you have this huge variety of different
4205520	4211040	actions that you have a 45-minute game with all these different units and it's got a lot of messiness
4211040	4216640	to it that really hasn't been captured by previous games. And famously, all of the hard-coded bots
4216640	4220400	for Dota were terrible, right? Just impossible to write anything good for it because it's so
4220400	4225520	complex. And so this seemed like a really good place to push what's the state of the art in
4225520	4229920	reinforcement learning. And so we started by focusing on the one versus one version of the game
4229920	4235600	and we're able to solve that. We're able to beat the world champions and the learning,
4235600	4239840	you know, the skill curve was this crazy exponential, right? And it was like constantly
4239840	4243920	we were just scaling up, that we were fixing bugs and, you know, that you look at the skill
4243920	4247760	curve and it was really a very, very smooth one. So it's actually really interesting to see how
4247760	4252560	that like human iteration loop yielded very steady exponential progress.
4252560	4257360	And to one side note, first of all, it's an exceptionally popular video game. The side
4257360	4263120	effect is that there's a lot of incredible human experts at that video game. So the benchmark
4263120	4267680	that you're trying to reach is very high. And the other, can you talk about the approach
4267680	4271920	that was used initially and throughout training these agents to play this game?
4271920	4276480	Yep. And so the person we used is self-play. And so you have two agents that don't know
4276480	4281360	anything. They battle each other. They discover something a little bit good. And now they both
4281360	4284960	know it. And they just get better and better and better without bound. And that's a really
4284960	4290960	powerful idea, right? That we then went from the one versus one version of the game and scaled up
4290960	4294720	to five versus five, right? So you think about kind of like with basketball, where you have this like
4294720	4301280	team sport and you need to do all this coordination. And we were able to push the same idea, the same
4301280	4308320	self-play to really get to the professional level at the full five versus five version of the game.
4309040	4315040	And the things I think are really interesting here is that these agents, in some ways, they're
4315040	4319600	almost like an insect-like intelligence, right? Where they have a lot in common with how an insect
4319600	4323760	is trained, right? An insect kind of lives in this environment for a very long time, or the
4323760	4327440	ancestors of this insect have been around for a long time and had a lot of experience that gets
4327440	4332960	baked into this agent. And, you know, it's not really smart in the sense of a human, right? It's
4332960	4337200	not able to go and learn calculus, but it's able to navigate its environment extremely well. It's
4337200	4342480	able to handle unexpected things in the environment that's never seen before pretty well. And we see
4342480	4346960	the same sort of thing with our Dota bots, right? That they're able to, within this game, they're
4346960	4351520	able to play against humans, which is something that never existed in its evolutionary environment.
4351520	4356480	Totally different play styles from humans versus the bots. And yet it's able to handle it extremely
4356480	4362960	well. And that's something that I think was very surprising to us, was something that doesn't really
4362960	4368080	emerge from what we've seen with PPO at smaller scale, right? And the kind of scale we're running
4368080	4374160	this stuff at was, you know, I could take 100,000 CPU cores running with like hundreds of GPUs.
4374160	4380320	It's probably about, you know, like, you know, something like hundreds of years of experience
4380320	4387200	going into this bot every single real day. And so that scale is massive. And we start to see
4387200	4390160	very different kinds of behaviors out of the algorithms that we all know and love.
4390800	4400560	Dota, you mentioned beat the world expert 1v1. And then you weren't able to win 5v5 this year
4401200	4406960	at the best players in the world. So what's the comeback story? What's, first of all,
4407040	4412480	talk through that. That's an exceptionally exciting event. And what's the following months in this
4412480	4417200	year look like? Yeah. Yeah. So one thing that's interesting is that, you know, we lose all the
4417200	4424240	time because we play, so the Dota team at OpenAI, we play the bot against better players
4424240	4429360	than our system all the time. Or at least we used to, right? Like, you know, the first time we lost
4429360	4434000	publicly was we went up on stage at the international and we played against some of the best teams in
4434000	4438480	the world. And we ended up losing both games, but we gave them a run for their money, right?
4438480	4442880	That both games were kind of 30 minutes, 25 minutes, and they went back and forth, back and
4442880	4448400	forth, back and forth. And so I think that really shows that we're at the professional level. And
4448400	4452240	that kind of looking at those games, we think that the coin could have gone a different direction
4452240	4456720	and it could have had some wins. That was actually very encouraging for us. And, you know, it's
4456720	4461680	interesting because the international was at a fixed time, right? So we knew exactly what day
4461680	4464560	we were going to be playing and we pushed as far as we could, as fast as we could.
4465520	4470320	Two weeks later, we had a bot that had an 80% win rate versus the one that played at TI. So the
4470320	4474320	march of progress, you know, you should think of as a snapshot rather than as an end state.
4474960	4480960	And so in fact, we'll be announcing our finals pretty soon. I actually think that we'll announce
4480960	4487360	our final match prior to this podcast being released. So there should be, we'll be playing,
4488160	4493200	against the world champions. And, you know, for us, it's really less about like the way that we
4493200	4500080	think about what's upcoming is the final milestone, the final competitive milestone for the project,
4500080	4506160	right? That our goal in all of this isn't really about beating humans at Dota. Our goal is to push
4506160	4509440	the state of the art and reinforcement learning. And we've done that, right? And we've actually
4509440	4513840	learned a lot from our system. And that we have, you know, I think a lot of exciting next steps
4513840	4517680	that we want to take. And so, you know, kind of as a final showcase of what we built, we're going
4517680	4523280	to do this match. But for us, it's not really the successor failure to see, you know, do we have
4523280	4528720	the coin flip go in our direction or against? Where do you see the field of deep learning
4528720	4536720	heading in the next few years? Where do you see the work and reinforcement learning perhaps heading?
4536720	4543520	And more specifically with OpenAI, all the exciting projects that you're working on,
4544400	4549680	what does 2019 hold for you? Massive scale. Scale. I will put an actress on that and just say, you
4549680	4555200	know, I think that it's about ideas plus scale, you need both. So that's a really good point. So
4555200	4563360	the question, in terms of ideas, you have a lot of projects that are exploring different areas of
4563360	4568240	intelligence. And the question is, when you, when you think of scale, do you think about
4568800	4573760	growing the scale of those individual projects? Or do you think about adding new projects? And
4573760	4578880	sorry to do that. And if you're thinking about adding new projects, or if you look at the past,
4578880	4583360	what's the process of coming up with new projects and new ideas? So we really have a
4583360	4588480	life cycle of project here. So we start with a few people just working on a small scale idea.
4588480	4592080	And language is actually a very good example of this, that it was really, you know, one person
4592080	4597040	here who was pushing on language for a long time. And then you get signs of life, right? And so this
4597040	4602640	is like, let's say, you know, with, with the original GPT, we had something that was interesting.
4602640	4606000	And we said, okay, it's time to scale this, right? It's time to put more people on it,
4606000	4611040	put more computational resources behind it. And, and then we just kind of keep pushing and keep
4611040	4614720	pushing. And the end state is something that looks like Dota or robotics, where you have a
4614720	4619520	large team of, you know, 10 or 15 people that are running things at very large scale, and that
4619520	4624880	you're able to really have material engineering, and, and, and, and, you know, sort of machine
4624880	4630560	learning science coming together to make systems that work and get material results that just
4630560	4634480	would have been impossible otherwise. So we do that whole life cycle, we've done it a number of
4634480	4640400	times, you know, typically end to end, it's probably two, two years or so to do it, you know,
4640400	4643360	the organization's been around for three years. So maybe we'll find that we also have longer
4643360	4650800	life cycle projects. But, you know, we will work up to those. We have, so one team that we were
4650800	4654640	actually just starting, Illy and I are kicking off a new team called the reasoning team. And
4654640	4660080	that this is to really try to tackle, how do you get neural networks to reason? And we think that
4660080	4663680	this will be a long term project. It's one that we're very excited about.
4664640	4671840	In terms of reasoning, super exciting topic. What do you, what kind of benchmarks? What kind of
4671920	4676560	tests of reasoning do you envision? What, what would, if you set back
4677840	4682720	whatever drink and you would be impressed that this system is able to do something,
4682720	4683600	what would that look like?
4683600	4684800	Not theorem proving.
4684800	4690480	Theorem proving. So some kind of logic and especially mathematical logic.
4690480	4693760	I think so, right? And I think that there's, there's, there's kind of other problems that are
4693760	4698640	dual to theorem proving in particular. You know, you think about programming, I think about even
4698640	4705280	like security analysis of, of code, that these all kind of capture the same sorts of core reasoning
4705280	4708000	and being able to do some out of distribution generalization.
4709280	4714720	It would be quite exciting if open AI reasoning team was able to prove that P equals NP, that
4714720	4715440	would be very nice.
4716000	4719680	It would be very, very, very exciting, especially if it turns out that P equals NP,
4719680	4720640	that'll be interesting too.
4721600	4725760	It just, it would be ironic and humorous.
4727760	4734160	So what problem stands out to you as the most exciting and challenging and impactful to the
4734160	4739520	work for us as a community in general and for open AI this year? You mentioned reasoning,
4739520	4741280	I think that's, that's a heck of a problem.
4741280	4744160	Yeah. So I think reasoning is an important one. I think it's going to be hard to get good results
4744160	4749680	in 2019. You know, again, just like we think about the life cycle takes time. I think for 2019,
4749680	4753520	language modeling seems to be kind of on that ramp, right? It's at the point that we have a
4753520	4756960	technique that works. We want to scale 100x, 1000x see what happens.
4758000	4760560	Awesome. Do you think we're living in a simulation?
4761760	4765920	I think it's, I think it's hard to have a real opinion about it. You know, it's actually
4765920	4770000	interesting. I separate out things that I think can have like, you know, yield
4770000	4774080	materially different predictions about the world from ones that are just kind of,
4774080	4777840	you know, fun, fun to speculate about. And I kind of view simulation as more like,
4777840	4780480	is there a flying teapot between Mars and Jupiter? Like,
4781760	4785040	maybe, but it's a little bit hard to know what that would mean for my life.
4785040	4791360	So there is something actionable. So some of the best work opening has done is in the field of
4791360	4797680	reinforcement learning. And some of the success of reinforcement learning come from being able
4797680	4803760	to simulate the problem you're trying to solve. So do you have a hope for reinforcement for the
4803760	4807680	future of reinforcement learning and for the future of simulation? Like, whether it's we're
4807680	4813440	talking about autonomous vehicles or any kind of system, do you see that scaling? So we'll be able
4813440	4820720	to simulate systems and hence be able to create a simulator that echoes our real world and proving
4820720	4823840	once and for all, even though you're denying it that we're living in a simulation?
4824880	4828240	I feel like I've used that for questions, right? So, you know, kind of at the core there of like,
4828240	4833520	can we use simulation for self-driving cars? Take a look at our robotic system, Dackel,
4833520	4839200	right? That was trained in simulation using the Dota system, in fact, and it transfers to a physical
4839200	4843600	robot. And I think everyone looks at our Dota system, they're like, okay, it's just a game. How
4843600	4847040	are you ever going to escape to the real world? And the answer is, well, we did it with the physical
4847040	4851120	robot, the no-wink program. And so I think the answer is simulation goes a lot further than you
4851120	4855680	think if you apply the right techniques to it. Now, there's a question of, you know, are the
4855680	4861360	beings in that simulation going to wake up and have consciousness? I think that one seems a lot
4861360	4865280	harder to again reason about. I think that, you know, you really should think about like,
4865280	4869680	where exactly does human consciousness come from in our own self-awareness? And, you know,
4869680	4872640	is it just that like, once you have like a complicated enough neural net, do you have to
4872640	4878640	worry about the agent's feeling pain? And, you know, I think there's like interesting speculation
4878640	4882960	to do there. But, you know, again, I think it's a little bit hard to know for sure.
4882960	4887600	Well, let me just keep with the speculation. Do you think to create intelligence, general
4887600	4894560	intelligence, you need one consciousness and two a body? Do you think any of those elements are
4894560	4898400	needed or is intelligence something that's orthogonal to those?
4898400	4902800	I'll stick to the kind of like the non-grand answer first, right? So the non-grand answer
4902800	4906800	is just to look at, you know, what are we already making work? You look at GPT2,
4906800	4910560	a lot of people would have said that to even get these kinds of results, you need real world
4910560	4914480	experience. You need a body, you need grounding. How are you supposed to reason about any of these
4914480	4918000	things? How are you supposed to like even kind of know about smoke and fire and those things
4918000	4923040	if you've never experienced them? And GPT2 shows that you can actually go way further
4923040	4930480	than that kind of reasoning would predict. So I think that in terms of do we need consciousness,
4930480	4933920	do we need a body? It seems the answer is probably not, right? That we probably just
4933920	4939520	continue to push kind of the systems we have. They already feel general. They're not as competent
4939520	4944240	or as general or able to learn as quickly as an AGI would. But, you know, they're at least like
4944560	4951440	proto-AGI in some way, and they don't need any of those things. Now, let's move to the grand
4951440	4957680	answer, which is, you know, if our neural nets conscious already, would we ever know? How can
4957680	4964880	we tell, right? Here's where the speculation starts to become, you know, at least interesting or fun
4964880	4969680	and maybe a little bit disturbing, depending on where you take it. But it certainly seems that
4969680	4973920	when we think about animals, that there's some continuum of consciousness. You know, my cat,
4973920	4978960	I think, is conscious in some way, right? You know, not as conscious as a human. And you could
4978960	4982160	imagine that you could build a little consciousness meter, right? You pointed a cat, it gives you
4982160	4986960	a little reading, pointed a human, it gives you much bigger reading. What would happen if you
4986960	4992000	pointed one of those at a Dota neural net? And if you're training in this massive simulation,
4992000	4999120	do the neural nets feel pain? You know, it becomes pretty hard to know that the answer is no. And
4999120	5003920	it becomes pretty hard to really think about what that would mean if the answer were yes.
5005360	5010160	And it's very possible, you know, for example, you could imagine that maybe the reason these humans
5010160	5015520	are have consciousness is because it's a convenient computational shortcut, right? If you think about
5015520	5020080	it, if you have a being that wants to avoid pain, which seems pretty important to survive in this
5020080	5025680	environment and wants to like, you know, eat food, then that maybe the best way of doing it is to
5025680	5029520	have a being that's conscious, right? That, you know, in order to succeed in the environment,
5029520	5033360	you need to have those properties and how are you supposed to implement them? And maybe this
5033360	5037920	consciousness is a way of doing that. If that's true, then actually, maybe we should expect that
5037920	5042480	really competent reinforcement learning agents will also have consciousness. But, you know,
5042480	5045920	that's a big if and I think there are a lot of other arguments that you can make in other directions.
5046640	5051440	I think that's a really interesting idea that even GPT2 has some degree of consciousness,
5051440	5056000	that something is actually not as crazy to think about. It's useful to think about
5057040	5060880	as we think about what it means to create intelligence of a dog, intelligence of a cat,
5062560	5071120	and the intelligence of a human. So last question, do you think we will ever fall in love like in
5071120	5076240	the movie, her with an artificial intelligence system, or an artificial intelligence system
5076320	5083680	falling out in love with a human? I hope so. If there's any better way to end it is on love.
5083680	5086480	So, Greg, thanks so much for talking today. Thank you for having me.
