{"text": " On one axis, you have more hardware coming in. On the other hand, you have an explosion of innovation in AI. And so what happened with both TensorFlow and PyTorch is that the explosion of innovation in AI has led to, it's not just about matrix multiplication and convolution, these things have now like 2,000 different operators. And on the other hand, you have, I don't know how many pieces of hardware there are out there, it's a lot. Part of my thesis, part of my belief of where computing goes, if you look at 10 years from now, is it's not going to get simpler. Physics isn't going back to where we came from. It's only going to get weirder from here on out. And so to me, the exciting part about what we're building is it's about building that universal platform, which the world can continue to get weird, because again, I don't think it's avoidable, it's physics. But we can help lift people scale, do things with it, and they don't have to rewrite their code every time a new device comes out. And I think that's pretty cool. The following is a conversation with Chris Latner, his third time on this podcast. As I've said many times before, he's one of the most brilliant engineers in modern computing, having created LLM Compiler Infrastructure Project, the Klang Compiler, the Swift programming language, a lot of key contributions to TensorFlow and TPUs as part of Google. He served as vice president of autopilot software at Tesla, was a software innovator and leader at Apple, and now he co-created a new full stack AI infrastructure for distributed training, inference, and deployment on all kinds of hardware called Modular, and a new programming language called Mojo, that is a superset of Python, giving you all the usability of Python, but with the performance of C++. In many cases, Mojo code has demonstrated over 30,000 x speed up over Python. If you love machine learning, if you love Python, you should definitely give Mojo a try. This programming language, this new AI framework, and infrastructure, and this conversation with Chris is mind blowing. I love it. It gets pretty technical at times, so I hope you hang on for the ride. This is the Lex Friedman podcast. To support it, please check out our sponsors in the description, and now, dear friends, here's Chris Lattner. It's been, I think, two years since we last talked, and in that time, you somehow went and co-created a new programming language called Mojo. So it's optimized for AI. It's a superset of Python. Let's look at the big picture. What is the vision for Mojo? For Mojo? Well, I think you have to zoom out. So I've been working on a lot of related technologies for many, many years. So I've worked on LVM, and a lot of things, and mobile, and servers, and things like this. But the world's changing. And what's happened with AI is we have new GPUs, and new machine learning accelerators, and other ASICs, and things like that that make AI go real fast. At Google, I worked on TPUs. That's one of the biggest, largest scale deployed systems that exist for AI. And really what you see is if you look across all of the things that are happening in the industry, there's this new compute platform coming. And it's not just about CPUs, or GPUs, or TPUs, or NPUs, or IPUs, or whatever, all the PUs. It's about how do we program these things. And so for software folks like us, it doesn't do us any good if there's this amazing hardware that we can't use. And one of the things you find out really quick is that having the theoretical capability of programming something, and then having the world's power and the innovation of all the smart people in the world get unleashed on something can be quite different. And so really where Mojo came from was starting from a problem of we need to be able to take machine learning, take the infrastructure underneath it, and make it way more accessible, way more usable, way more understandable by normal people and researchers and other folks that are not themselves like experts in GPUs and things like this. And then through that journey, we realize, hey, we need syntax for this, we need to do a program language. So one of the main features of the language, I say so fully in jest, is that it allows you to have the file extension to be an emoji, or the fire emoji, which is one of the first emojis used as a file extension I've ever seen in my life. And then you ask yourself the question, why in the 21st century, are we not using Unicode for file extensions? This I mean, it's an epic decision. I think clearly the most important decision you made the most, but you could also just use Mojo as the file extension. Well, so okay, so take a step back. I mean, come on, Lex, do you think that the world's ready for this? This is a big moment in the world, right? This is we're releasing this onto the world. This is innovation. I mean, it really is kind of brilliant. Emojis are such a big part of our daily lives. Why isn't it not in programming? Well, and like you take a step back and look at what file extensions are, right? They're basically metadata, right? And so why are we spending all the screen space on them and all this stuff? Also, you know, you have them stacked up next to text files and PDF files and whatever else, like, if you're gonna do something cool, you want to stand out, right? Emojis are colorful, they're visual, they're beautiful, right? What's been the response so far from, is there support on like Windows on operating systems in displaying like File Explorer? Yeah, the one problem I've seen is that Git doesn't escape it right? And so it thinks that the Fire Emoji is unprintable. And so it like prints out weird hex things if you use the command line Git tool. But everything else as far as I'm aware works fine. And I have faith that Git can be improved. So GitHub is fine. GitHub is fine. Yep. GitHub is fine. Visual Studio Code, Windows, like all this stuff totally ready, because people have internationalization in their normal part of their paths. So this is just like taking the next step, right? Somewhere between, oh, wow, that makes sense. Cool. I like new things too. Oh my god, you're killing my baby. Like, what are you talking about? This can never be like, I can never hand it and list how am I going to type this like all these things. And so this is something where I think that the world will get there. We don't have to bet the whole farm on this. I think we can provide both paths. But I think it'll be great. When can we have emojis as part of the code? I wonder. Yeah, so I mean, lots of languages provide that. So I think that we have partial support for that. It's probably not fully done yet. But but yeah, you can you can do that. For example, in Swift, you can do that for sure. So an example we give gave it Apple was the dog cow. Yeah, so that's a classical Mac heritage thing. And so you use the dog and the cow emoji together. And that could be your variable name. But of course, the internet went and made pile of poop for everything. Yeah. So you know, if you want to name your function pile of poop, then you can totally go to town and see how that gets through code review. Okay, so let me just ask a bunch of random questions. So is Mojo primarily designed for AIs? Or is it a general purpose program? Good question. So it's AI first. And so AI is driving a lot of the requirements. And so modular is building and designing and driving Mojo forward. And it's not because it's an interesting project theoretically to build, it's because we need it. And so modular, we're really tackling the AI infrastructure landscape and the big problems in AI, and the reasons that it is so difficult to use and scale and adopt and deploy and like all these big problems in AI. And so we're coming out from that perspective. Now, when you do that, when you start tackling these problems, you realize that the solution to these problems isn't actually an AI specific solution. And so while we're doing this, we're building Mojo to be a fully general programming language. And that means that you can obviously tackle GPUs and CPUs and like these AI things, but it's also a really great way to build NumPy and other things like that or you know, just if you look at what many Python libraries are today, often they're a layer of Python for the API, and they end up being C and C++ code underneath them. That's very true in AI, that's true in lots of other demands as well. And so anytime you see this pattern, that's an opportunity for Mojo to help simplify the world and help people have one thing. To optimize through simplification, by having one thing. So you mentioned modular. Mojo is the programming language, modular is the whole software stack. So just over a year ago, we started this company called modular. What modular is about is it's about taking AI and up leveling it into the next generation. And so if you take a step back, what's gone on in the last five, six, seven, eight years is that we've had things like TensorFlow and PyTorch and these other systems come in, you've used them, you know this. And what's happened is these things have grown like crazy, and they get tons of users, it's in production deployment scenarios, it's being used to power so many systems. AI is all around us now. It used to be controversial years ago, but now it's a thing. But the challenge with these systems is that they haven't always been thought out with current demands in mind. So you think about it, where were LLMs eight years ago? Well, they didn't exist, right? AI has changed so much. And a lot of what people are doing today are very different than when these systems were built. And meanwhile, the hardware side of this has gotten into a huge mess. There's tons of new chips and accelerators and every big company's announcing a new chip every day it feels like. And so between that, you have like this moving system on one side, moving system on the other side, and it just turns into this gigantic mess, which makes it very difficult for people to actually use AI, particularly in production deployment scenarios. And it's what modular students were helping build out that software stack to help solve some of those problems. So then people can be more productive and get more AI research into production. Now, what Mojo does is it's a really, really, really important piece of that. And so that is, you know, part of that engine and part of the technology that allows us to solve these problems. So Mojo is a programming language that allows you to do a high level program and the low level programming, they do all kinds of programming in that spectrum that gets you closer and closer to the hardware. So take a step back. So Lex, what do you love about Python? Oh boy. Where do I begin? What is love? What do I love about Python? You're a guy who knows love. I know this. Yes. How intuitive it is. How it feels like I'm writing natural language, English. How when I can not just write but read other people's code somehow I can understand it faster. It's more condensed than other languages like ones I'm really familiar with like C++ and C. There's a bunch of sexy little features. Yeah. We'll probably talk about some of them, but list comprehensions and stuff like this. Also, and don't forget the entire ecosystem of all the packages. Oh yeah, that's probably huge. Because there's always something. If you want to do anything, there's always a package. Yeah. So it's not just the ecosystem of the packages and the ecosystem of the humans that do it. That's a really, that's an interesting dynamic. That's good. Because I think something about the usability and the ecosystem makes the thing viral. It grows and then it's a virtuous cycle, I think. Well, there's many things that went into that. Like, so I think that ML was very good for Python. And so I think that TensorFlow and PyTorch and these systems embracing Python really took and helped Python grow. But I think that the major thing underlying it is that Python is like the universal connector. It really helps bring together lots of different systems so you can compose them and build out larger systems without having to understand how it works. But then what is the problem with Python? Well, I guess you could say several things, but probably that it's slow. I think that's usually what people complain about. And so other people complain about tabs in spaces versus curly braces or whatever. But those people are just wrong because it is actually just better to use an indentation. Wow, strong words. So actually, I'm a small tangent. Let's actually take that. Let's take all kinds of tangents. Oh, come on, Lex. You can push me on it. I could take it. Design. Listen, I've recently left Emacs for VS Code. The kind of hate mail I had to receive. Because on the way to doing that, I also said I've considered Vim and chose not to and went with VS Code. You're touching on deep religions, right? Anyway, tabs is an interesting design decision. And so you've really written a new programming language here. Yes, it is a superset of Python, but you can make a bunch of different interesting decisions here. Totally. And you chose actually to stick with Python in terms of some of the syntax. So let me explain why. So I mean, you can explain this in many rational ways. I think that the indentation is beautiful, but that's not a rational explanation. But I can defend it rationally. So first of all, Python 1 has millions of programmers. It's huge. It's everywhere. It owns machine learning. So factually, it is the thing, right? Second of all, if you look at it, C codes, C++ code, Java, whatever, Swift, curly brace languages also run through formatting tools and get indented. And so if they're not indented correctly, first of all, it will twist your brain around. It can lead to bugs. There's notorious bugs that have happened across time where the indentation was wrong or misleading and it wasn't formatted, right? And so it turned into an issue, right? And so what ends up happening in modern large-scale code bases is people run automatic formatters. So now what you end up with is indentation and curly braces. Well, if you're going to have the notion of grouping, why not have one thing, right? And get rid of all the clutter and have a more beautiful thing, right? Also, you look at many of these languages, it's like, okay, well, you can have curly braces or you can omit them if there's one statement or you just enter this entire world of complicated design space that objectively you don't need if you have Python style indentation. So yeah, I would love to actually see statistics on errors made because of indentation. Like how many errors are made in Python versus in C++ that have to do with basic formatting, all that kind of stuff. I would love to see. I think it's probably pretty minor because once you get like, you use VS code, I do too. So if you get VS code set up, it does the indentation for you generally. Right. And so you don't, you know, it's actually really nice to not have to fight it. And then what you can see is the editor is telling you how your code will work by indenting it, which I think is pretty cool. I honestly don't think I've ever, I don't remember having an error in Python because I indented stuff wrong. So I mean, I think that there's, again, this is a religious thing. And so I can joke about it. And I love, I love to kind of, you know, I realize that this is such a polarizing thing and everybody wants to argue about it. And so I like poking at the bear a little bit, right? But, but frankly, right, come back to the first point, Python one, like it's huge, it's an AI, it's the right thing. For us, like we see mojo as being an incredible part of the Python ecosystem, we're not looking to break Python or change it or quote unquote, fix it. We love Python for what it is. Our view is that Python is just not done yet. And so if you look at, you know, you mentioned Python being slow, well, there's a couple of different things that go into that, which we can talk about if you want. But one of them is it just doesn't have those features that you would use to do C like programming. And so if you say, okay, well, I'm forced out of Python into C for certain use cases. Well, then what we're doing is we're saying, okay, well, why, why is that? Can we just add those features that are missing from Python back up to mojo? And then you can have everything that's great about Python, all the things you're talking about the love, plus not be forced out of it when you do something a little bit more computationally intense or weird or hardwarey or whatever it is that you're doing. Well, a million questions. I want to ask what high level again, is it compiled or is it an interpretive language? So Python is just in time compilation. What's, what's mojo? So mojo, the complicated answer does all the things. So it's interpreted, it's just compiled and it's statically compiled. And so this is for a variety of reasons. So one of the things that makes Python beautiful is that it's very dynamic. And because it's dynamic, one of the things they added is that it has this powerful metaprogramming feature. And so if you look at something like PyTorch or TensorFlow, or, or, I mean, even a simple, simple use case, like you'd find a class that has the plus method, right, you can overload the Dunder methods like Dunder add, for example, and then the plus method works on your class. And so it has very nice and very expressive, dynamic metaprogramming features. In mojo, we want all those features come in. Like, we don't want to break Python, we want it all to work. But the problem is, is you can't run those super dynamic features on an embedded processor, or on a GPU, right? Or if you could, you probably don't want to just because of the performance. And so we entered this question of saying, okay, how do you get the power of this dynamic metaprogramming into a language that has to be super efficient in specific cases? And so what we did was we said, okay, we'll take that interpreter, Python has an interpreter in it, right, take that interpreter and allow it to run it compile time. And so now what you get is you get compile time metaprogramming. And so this is super interesting, super powerful, because one of the big advantages you get is you get Python style expressive APIs, you get the ability to have overloaded operators. And if you look at what happens inside of like PyTorch, for example, with automatic differentiation and eager mode, and like all these things, they're using these really dynamic and powerful features at runtime. But we can take those features and lift them so that they run at compile time. So you're, because C++ has metaprogramming with templates, but it's really messy. It's super messy. It's always, it was accidentally, I mean, different people have different interpretations. My interpretation is that it was made accidentally powerful. It was not designed to be terrain complete, for example, but that was discovered kind of along the way accidentally. And so there have been a number of languages in the space. And so they usually have templates or code instantiation, code copying features of various sorts. Some more modern languages, or some more newer newer languages, let's say, like, you know, they're fairly unknown, like zig, for example, says, okay, well, let's take all of those types so you can run it, all those things you can do at runtime and allow them to happen at compile time. And so one of the problems with C++, I mean, which is one of one of the problems with C++ Here we go. Wrong words. We're going to offend everybody today. Oh, it's okay. I mean, everybody hates me for a variety of reasons. Anyways, I'm sure, right? I've written enough the way they show love. I've written enough C++ code to earn a little bit of grumpyness with C++. But, but one of the problems with it is that the metaprogramming system templates is just a completely different universe from the normal runtime programming world. And so if you do metaprogramming and programming, it's just like a different universe, different syntax, different concepts, different stuff going on. And so again, one of our goals with Mojo is to make things really easy to use, easy to learn. And so there's a natural stepping stone. And so as you do this, you say, okay, well, I have to do programming at runtime, after you're programming at compile time. Why are these different things? How hard is that to pull it out? Because that sounds to me as a fan of metaprogramming in C++ even. How hard is it to pull that off? That sounds really, really exciting, because you can do the same style programming at compile time and at runtime. That's really, really exciting. Yep. And so I mean, in terms of the compiler implementation details, it's hard. I won't be shy about that. It's super hard. It requires, I mean, what Mojo has underneath the covers is a completely new approach to the design of the compiler itself. And so this builds on these technologies like MLIR that you mentioned, that also includes other like caching and other interpreters and jit compilers and other stuff like that. Do you have like an interpreter inside the compiler? Within the compiler, yes. And so it really takes the standard model of programming languages and kind of twists it and unifies it with the runtime model, right, which I think is really cool. And to me, the value of that is that, again, many of these languages have metaprogramming features, like they grow macros or something, right? You list, right? Yes. I know your roots, right? And this is a powerful thing, right? And so if you go back to list, one of the most powerful things about it is that it said that the metaprogramming and the programming are the same, right? And so that made it way simpler, way more consistent, way easier to understand reason about, and it made it more composable. So if you build a library, you can use it both at runtime and compile time, which is pretty cool. Yeah. And for machine learning, I think metaprogramming, I think we could generally say is extremely useful. And so you get features, I mean, I'll jump around, but there's the feature of auto-tuning and adaptive compilation just blows my mind. Yeah. Well, so, okay, so let's come back to that. Okay, all right. So what is machine learning? Like, what is a machine learning model? Like, you take a PyTorch model off there, right? It's really interesting to me because what PyTorch and what TensorFlow and all these frameworks are kind of pushing compute into, is they're pushing into like this abstract specification of a compute problem, which then gets mapped in a whole bunch of different ways, right? And so this is why it became a metaprogramming problem, is that you want to be able to say, cool, I have this neural net, now run with batch size 1000, right? Do a mapping across batch, or okay, I want to take this problem now running across 1000 CPUs or GPUs, right? And so like, this problem of like, describe the compute and then map it and do things and transform it are like, actually, it's very profound. And that's one of the things that makes machine learning systems really special. Maybe can you describe auto-tuning and how do you pull off? I mean, I guess adaptive compilation is what we're talking about as metaprogramming. Yeah. How do you pull off auto-tuning? I mean, is that as profound as I think it is? It just seems like I really like, you know, we'll mention list comprehensions to me, from a quick glance at Mojo, which by the way, I have to absolutely like dive in. As I realize how amazing this is, I absolutely must dive in. That looks like just an incredible feature for machine learning people. Yeah. Well, so what is auto-tuning? So take a step back. Auto-tuning is a feature in Mojo. It's not, so very little of what we're doing is actually research. Like many of these ideas have existed in other systems and other places. And so what we're doing is we're pulling together good ideas, remixing them, and making them into hopefully a beautiful system, right? And so auto-tuning, the observation is that it turns out hardware systems, algorithms are really complicated. It turns out maybe you don't actually want to know how the hardware works, right? A lot of people don't, right? And so there are lots of really smart hardware people. I know a lot of them, where they know everything about, okay, that the cache size is this and the number of registers is that. And if you use this, what length of vector, it's going to be super efficient because it maps directly onto what it can do. And like all this kind of stuff, or the GPU has SMs and it has a warp size of whatever, right? All the stuff that goes into these things or the tile size of a TPU is 128, like these factoids, right? My belief is that most normal people, and I love hardware people also, I'm not trying to offend literally everybody on the internet, but most programmers actually don't want to know this stuff, right? And so if you come at it from the perspective of how do we allow people to build both more abstracted, but also more portable code, because, you know, could be that the vector length changes or the cache size changes, or it could be that the tile size of your matrix changes or the number, you know, an A100 versus an H100 versus a Volta versus a whatever GPU have different characteristics, right? A lot of the algorithms that you run are actually the same, but the parameters, these magic numbers you have to fill in end up being really fiddly numbers that an expert has to go figure out. And so what autotuning does, it says, okay, well, guess what? There's a lot of compute out there, right? So instead of having humans go randomly try all the things or do a grid search or go search some complicated multi-dimensional space, how about we have computers do that, right? And so what autotuning does is you can say, hey, here's my algorithm. If it's a matrix operation or something like that, you can say, okay, I'm going to carve it up into blocks. I'm going to do those blocks in parallel. And I want this with 128 things that I'm running on, I want to cut it this way or that way or whatever. And you can say, hey, go see which one's actually empirically better on the system. And then the result of that, you cache for that system. Yep. You save it. And so come back to twisting your compiler brain, right? So not only does the compiler have an interpreter that's used to do metaprogramming, that compiler that interpreter that metaprogramming now has to actually take your code and go run it on a target machine. See which one likes the best and then stitch it in and then keep going, right? So part of the compilation is machine specific. Yeah. Well, so I mean, this is an optional feature, right? So you don't have to use it for everything. But yeah, if you're, so one of, one of the things that we're in the quest of is ultimate performance. Yes. Right. And ultimate performance is important for a couple of reasons, right? So if you're an enterprise, you're looking to save cost and compute and things like this, ultimate performance translates to, you know, fewer servers. Like, if you care about the environment, hey, better performance leads to more efficiency. Right? I mean, you could joke and say like, you know, Python's bad for the environment. Right. And so if you move to Mojo, it's like at least 10x better just out of the box and keep going, right? But, but performance is also interesting because it leads to better products. And so in the space of machine learning, right, if you reduce the latency of a model, so it runs faster. So every time you query the server running the model, it takes less time. Well, then the product team can go and make the model bigger. Well, that's actually makes it, so you have a better experience as a customer. And so a lot of people care about that. So for auto tuning, for like tile size, you mentioned 128 for TPU, you would specify like a bunch of options to try. Just in the code. It's just a simple statement. And then you just set and forget and know, depending on where it compiles, it'll actually be the fastest. And yeah, exactly. And the beauty of this is that it helps you in a whole bunch of different ways, right? So if you're building, so often what'll happen is that, you know, you've written a bunch of software yourself, right? You, you wake up one day, you say, I have an idea, I'm going to go code up some code. I get to work. I forget about it. I move on with life. I come back six months or a year or two years or three years later, you dust it off and you go use it again in a new environment. And maybe your GPU is different. Maybe you're running on a server instead of a laptop, maybe whatever, right? And so the problem now is you say, okay, well, I mean, again, not everybody cares about performance. But if you do, you say, okay, well, I want to take advantage of all these new features. I don't want to break the old thing, though. Right. And so the typical way of handling this kind of stuff before is, you know, if you're talking about C++ templates, or you're talking about C with macros, you end up with if deaths, you get like all these weird things get layered in, make the code super complicated. And then how do you test it? Right? Because this crazy complexity, multi-dimensional space you have to worry about. And, you know, that just doesn't scale very well. Actually, let me just jump around before I go to some specific features. Like the increase in performance here that we're talking about can be just insane. You write that Moja can provide a 35,000 x speed up over Python. How does it do that? Yeah, so it can even do more. But we'll get to that. So first of all, when we say that we're talking about what's called C Python, it's the default Python that everybody uses when you type Python three, that's like typically the one you use, right? C Python is an interpreter. And so interpreters, they have an extra layer of like byte codes and things like this that they have to go read, parse, interpret and make some kind of slow from that perspective. And so one of the first things we do is we move to a compiler. And so I'm just moving to a compiler getting the interpreter out of the loop is two to five to 10 x speed up depending on the code. So just out of the gate, just using more modern techniques, right? Now, if you do that, one of the things you can do is you can start to look at how C Python started to lay out data. And so one of the things that C Python did, and this isn't part of the Python spec necessarily, but this is sets of decisions, is that if you take an integer, for example, it'll put it in an object. In Python, everything's an object. And so they do the very logical thing of keeping the memory representation of all objects the same. So all objects have a header, they have like payload data. And what this means is that every time you pass around an object, you're passing around a pointer to the data. Well, this has overhead. It turns out that modern computers don't like chasing pointers very much in things like this. It means that you have to allocate the data. It means you have to reference count it, which is another way that Python uses to keep track of memory. And so this has a lot of overhead. And so if you say, okay, let's try to get that out of the heap, out of a box, out of an indirection, and into the registers. That's that's another 10 x. So it adds up if you if you're reference counting every single every single thing you create that adds up. Yep. And if you look at, you know, people complain about the Python Gil, this is one of the things that hurts parallelism. That's because the reference counting. Right. And so the Gil and reference counting are very tightly intertwined in Python. It's not the only thing, but it's very tightly intertwined. And so then you lean into this and you say, okay, cool, well, modern computers, they can do more than one operation at a time. And so they have vectors. What is a vector? Well, a vector allows you to take one, instead of taking one piece of data doing an ad or a multiply and then pick up the next one, you can now do four or eight or 16 or 32 at a time. Right. Well, Python doesn't expose that because of reasons. And so now you can say, okay, well, you can adopt that. Now you have threads. Now you have like additional things like you can control memory hierarchy. And so what mojo allows you to do is it allows you to start taking advantage of all these powerful things have been built into the hardware over time. And it gives the library gives very nice features. So you can say, just parallelize, let's do this in parallel. So it's very, very powerful weapons against slowness, which is why people have been I think having fun like just taking code and making go fast because it's just kind of an adrenaline rush to see like how fast you can get things. Before I talk about some of the interesting stuff with parallelization, all that, let's first talk about like the basics. We talked about indentation, right? So this thing looks like Python. It's sexy and beautiful like Python, as I mentioned. Is it a typed language? So what's the role of types? Yeah, good question. So Python has types. It has strings as integers, it has dictionaries and like all that stuff. But they all live at runtime. Right. And so because all those types live at runtime and Python, you never, you don't have to spell them. Python also has like this whole typing thing going on now. And a lot of people use it. Yeah, I'm not talking about that. That's kind of a different thing. We can go back to that if you want. But, but typically the, you know, you just say I take, I have a def and my def takes two parameters. I'm going to call them A and B and I don't have to write a type. Okay. So that is great. But what that does is that forces what's called a consistent representation. So these things have to be a pointer to an object with the object header, and they all have to look the same. And then when you dispatch a method, you go through all the same different paths, no matter what the receiver or whatever that type is. So what Mojo does is it allows you to have more than one kind of type. And so what it does is allows you to say, okay, cool, I have, I have an object and objects behave like Python does. And so it's fully dynamic and that's all great. And for many things classes, like that's all very powerful and very important. But if you want to say, Hey, it's an integer, and it's 32 bits or 64 bits or whatever it is, or it's a floating point value, it's 64 bits. Well, then the compiler can take that and it can use that to do way better optimization. And it turns out again, getting rid of the interactions, that's huge, means you can get better code completion because you have, because compiler knows what the type is. And so it knows what operations work on it. And so that's actually pretty huge. And so what Mojo does allows you to progressively adopt types into your program. And so you can start again, it's compatible with Python. And so then you can add however many types you want, wherever you want them. And if you don't want to deal with it, you don't have to deal with it. Right. And so one of one of, you know, our opinions on this is it's not that types are the right thing or the wrong thing. It's that they're a useful thing. Which was kind of optional. It's not strict typing, you don't have to specify type. Exactly. Okay, so starting from the thing that Python is kind of reaching towards right now with trying to inject types into it. Yeah, with a very different approach. But yes. What's the different approach? I'm actually one of the people that have not been using types very much in Python. That's okay. Why did you say? It just, well, because I know the importance it's like adults use strict typing. And so I refuse to grow up in that sense. It's a kind of rebellion. But I just know that it probably reduces the amount of errors even just for forget about performance improvements, it probably reduces errors when you do strict typing. Yeah, so I mean, I think it's interesting if you look at that, right? And the reason I'm giving a hard time is that there's this cultural norm, this pressure, this like, there has to be a right way to do things. Like, you know, grown-ups only do it one way. And if you don't do that, you should feel bad. Right. Like some people feel like Python's a guilty pleasure or something. And it's like, when it gets serious, I need to go rewrite it. I mean, cool. I understand history and I understand kind of where this comes from. But I don't think it has to be guilty pleasure. Yeah. So if you look at that, you say, why do you have to rewrite it? Well, you have to rewrite it to deploy. Well, why do you want to deploy? Well, you care about performance, you care about productivity, or you want, you know, a tiny thing on the server that has no dependencies, or, you know, you have objectives that you're trying to attain. So what if Python can achieve those objectives? So if you want types, well, maybe you want types because you want to make sure you're passing the right thing. Sure, you can add a type. If you don't care, you're prototyping some stuff, you're hacking some things out, you're like pulling some MAM code off the internet, it should just work. Right. And you shouldn't be like pressured. You shouldn't feel bad about doing the right thing or the thing that feels good. Now, if you're in a team, right, you're working at some massive internet company and you have 400 million lines of Python code, well, they may have a house rule that you use types. Yeah. Right. Because it makes it easier for different humans to talk to each other and understand what's going on and bugs at scale. Right. And so there are lots of good reasons why you might want to use types. But that doesn't mean that everybody should use them all the time. Right. So what Mojo does is it says, cool, well, allow people to use types. And if you use types, you get nice things out of it. Right. You get better performance and things like this. Right. But Mojo is a full compatible superset of Python. Right. And so that means it has to work without types. It has to support all the dynamic things, has to support all the packages, has to support for comprehension, list comprehensions and things like this. Right. And so that starting point, I think, is really important. And I think that, again, you can look at why it cares so much about this. And there's many different aspects of that, one of which is the world went through a very challenging migration from Python 2 to Python 3. Right. And this migration took many years. And it was very painful for many teams. Right. And there's a lot of things that went on in that. I'm not an expert in all the details. I honestly don't want to be. I don't want the world to have to go through that. Right. And people can ignore Mojo. And if it's not their thing, that's cool. But if they want to use Mojo, I don't want them to have to rewrite all their code. Yeah. I mean, this, okay, the superset part is, it's just, I mean, there's so much brilliant stuff here. That definitely is incredible. We'll talk about that. Yeah. First of all, how's the typing implemented differently in Python versus Mojo? Yeah. So this heterogeneous flexibility, you said, is definitely implemented. Yeah. So I'm not a full expert in the whole backstory on types in Python. So I'll give you that. I can give you my understanding. My understanding is basically like many dynamic languages, the ecosystem went through a phase where people went from ranked scripts to ranked large scale, huge code bases in Python. And at scale, it kind of helps have types. Yeah. People want to be able to reason about interfaces, what do you expect to string or an int or like, what are these basic things, right? And so what the Python community started doing is it started saying, okay, let's have tools on the side, checker tools, right? The go and like enforce and variance, check for bugs, try to identify things. These are called static analysis tools generally. And so these tools run over your code and try to look for bugs. What ended up happening is there's so many of these things, so many different weird patterns and different approaches on specifying the types and different things going on that the Python community realized and recognized, hey, hey, hey, there's a thing here. And so what they started to do is they started to standardize the syntax for adding types to Python. Now, one of the challenges that they had is that they're coming from kind of this fragmented world where there's lots of different tools, they have different trade-offs and interpretations and the types mean different things. And so if you look at types in Python, according to the Python spec, the types are ignored, right? So according to the Python spec, you can write pretty much anything in a type position, okay? And technically, you can write any expression, okay? Now, that's beautiful because you can extend it, you can do cool things, you can write, build your own tools, you can build your own house, linter or something like that, right? But it's also a problem because any existing Python program may be using different tools and they have different interpretations. And so if you adopt somebody's package into your ecosystem, try around the tool you prefer, it may throw out tons of weird errors and warnings and problems just because it's incompatible with how these things work. Also because they're added late and they're not checked by the Python interpreter, it's always kind of more of a hint than it is a requirement. Also, the C Python implementation can't use them for performance. And so it's really that's a big one, right? So you can't utilize the for the compilation for the just entire compilation. Okay, exactly. And this all comes back to the design principle of it's, it's kind of, they're kind of hints, they're kind of the definition is a little bit murky, it's unclear exactly the interpretation in a bunch of cases. And so because of that, you can't actually, even if you want to, it's really difficult to use them to say like it is going to be an int. And if it's not, it's a problem, right? A lot of code would break if you did that. So, so in mojo, right, so you can still use those kind of type annotations, it's fine. But in mojo, if you declare a type and you use it, then it means it is going to be that type. And the compiler helps you check that and force it and it's safe. And it's not it's not a like best effort hint kind of a thing. So if you try to shove a string type thing into a integer, you get an error from the compiler, compile time. Nice. Okay, what kind of basic types are there? Yeah, so mojo is pretty hardcore in terms of what it tries to do in the language, which is the philosophy there is that we, again, if you, if you look at Python, right, Python's beautiful language because it's so extensible, right? And so all of the different things in Python like for loops and plus and like all these things can be accessed through these underbar and bar methods. Okay, so you have to say, okay, if I make something that is super fast, I can go all the way down to the metal. Why do I need to have integers built into the language? Right. And so what mojo does is it says, okay, well, we can have this notion of structs. So we have classes in Python, now you can have structs. Classes are dynamic, structs are static. Cool, we can get high performance, we can write C++ kind of code with structs if you want. These things mix and work beautifully together. But what that means is that you can go and implement strings and ints and floats and arrays and all that kind of stuff in the language. Right. And so that's really cool because, you know, to me as a ideal, idealizing compiler language type of person, what I want to do is I want to get magic out of the compiler and put in the libraries. Because if somebody can, you know, if we can build an integer that's beautiful and it has an amazing API and does all the things you'd expect an integer to do, but you don't like it, maybe you want a big integer, maybe you want like sideways integer, I don't know, like what all the space of integers are, then you can do that and it's not a second class citizen. And so if you look at certain other languages like C++, one I also love and use a lot, int is hard code in the language, but complex is not. And so it's kind of weird that, you know, you have this std complex class, but you have int and complex tries to look like a natural numeric type and things like this. But integers and floating point have these like special promotion rules and other things like that that are magic and they're hacked into the compiler. And because of that, you can't actually make something that works like the built-in types. Is there something provided as a standard because, you know, because it's AI first, you know, numerical types are so important here. So is there something like a nice standard implementation of integer and float? Yeah, so we're still building all this stuff out. So we provide integers and floats and all that kind of stuff. We also provide like buffers and tensors and things like that that you'd expect in an ML context. Honestly, we need to keep designing and redesigning and working with the community to build that out and make that better. That's not our strength right now. Give us six months or a year and I think it'll be way better. But the power of putting in the library means that we can have teams of experts that aren't compiler engineers that can help us design and refine and drive us forward. So one of the exciting things we should mention here is that this is new and fresh. This cake is unbaked. It's almost baked. You can tell it's delicious, but it's not fully ready to be consumed. Yep, that's very fair. It is very useful, but it's very useful if you're a super low-level programmer right now. And what we're doing is we're working our way up the stack. And so the way I would look at Mojo today in May and 2023 is that it's like a 0.1. So I think that a year from now, it's going to be way more interesting to a variety of people. But what we're doing is we decided to release it early so that people can get access to it and play with it. We can build it with the community. We have a big roadmap fully published, being transparent about this, and a lot of people are involved in this stuff. And so what we're doing is we're really optimizing for building this thing the right way. And building it the right way is kind of interesting working with the community because everybody wants it yesterday. And so sometimes there's some dynamics there, but I think it's the right thing. So there's a discord also, so the dynamics is pretty interesting. Sometimes the community probably can be very chaotic and introduce a lot of stress. Guido famously quit over the stress of the Walrus operator. Broke, straw the brook with camels there. Exactly. And so it could be very stressful to develop. But can you just add tangent upon a tangent? Is it stressful to work through the design of various features here given that the community is so racially involved? Well, so I've been doing open development and community stuff for decades now. Somehow this has happened to me. So I've learned some tricks. But the thing that always gets me is I want to make people happy. And so this is maybe not all people all happy all the time. But generally, I want people to be happy. And so the challenge is that, again, we're tapping into some long, some deep-seated long tensions and pressures both in the Python world, but also in the AI world, in the hardware world, and things like this. And so people just want us to move faster. And so, again, our decision was let's release this early. Let's get people used to it or access to and play with it. And let's build in the open, which we could have had the language monk sitting in the cloister up on the hilltop bevaring away trying to build something. But in my experience, you get something that's way better if you work with the community. And so, yes, it can be frustrating. It can be challenging for lots of people involved. And if you mention our Discord, we have over 10,000 people on the Discord, 11,000 people or something. Keep in mind, we released Mojo like two weeks ago. So it's very cool. But what that means is that 10, 11,000 people all will want something different. And so what we've done is we've tried to say, okay, cool, here's our roadmap. And the roadmap isn't completely arbitrary. It's based on, here's the logical order in which to build these features or add these capabilities and things like that. And what we've done is we've spun really fast on bug fixes. And so we actually have very few bugs, which is cool. I mean, actually for projects in the state. But then what we're doing is we're dropping in features very deliberately. I mean, this is fun to watch because you got the two gigantic communities of like hardware, like systems engineers. And then you have the machine learning Python people that are like higher level. And it's just too like army, like they've been at war. Yeah, they've been at war. Right. And so here's a Tolkien novel or something. Okay, so here's the test. Again, like it's super funny for something that's only been out for two weeks, right? People are so impatient, right? But okay, cool. Let's fast forward a year. Like any year's time, Mojo will be actually quite amazing and solve tons of problems and be very good. People still have these problems. Right. And so you look at this and you say, and the way I look at this at least is to say, okay, well, we're solving big longstanding problems. To me, I, again, working on many different problems, I want to make sure we do it right. Right? There's like a responsibility you feel because if you mess it up, right? There's very few opportunities to do projects like this and have them really have impact on the world. If we do it right, then maybe we can take those feuding armies and actually heal some of those wounds. Yeah, this is like, this feels, this feels like a speech by George Washington or Abraham Lincoln or something. And you look at this and it's like, okay, well, how different are we? Yeah. We all want beautiful things. We all want something that's nice. We all want to be able to work together. We all want our stuff to be used, right? And so if we can help heal that, now I'm not optimistic that all people will use Mojo and they'll stop using C++. Like that's not my goal, right? But if we can heal some of that, I think that'd be pretty cool. Yeah. And we start by putting the people who like braces into the gulag. No. So there are proposals for adding braces to Mojo. We just tell them no. Okay. Politely. Yeah. Anyway, so there's a lot of amazing features on the roadmap and those ready to implement it. It'd be awesome. I can just ask you a few things. So the other performance improvement comes from immutability. So what's the what's this var and this let thing that we got going on? What's immutability? Yeah. So one of the things that is useful and it's not always required, but it's useful is knowing whether something can change out from underneath you. Right. And so in Python, you have a pointer to an array, right? And so you pass that pointer to an array around to things. If you pass into a function, maybe take that and scroll away in some other data structure. And so you get your array back and you go to use it. Now somebody else is like putting stuff in your array. How do you reason about that? Because to be very complicated, at least a lot of bugs, right? And so one of the things that, you know, again, this is not something Mojo forces on you, but something that Mojo enables is a thing called value semantics. And what value semantics do is they take collections like arrays, like dictionaries, also tensors and strings and things like this that are much higher level and make them behave like proper values. And so it makes it look like if you pass these things around, you get a logical copy of all the data. And so if I pass you an array, sure array, you can go do what you want to it. You're not going to hurt my array. Now, that is an interesting and very powerful design principle. It defines the way a ton of bugs. You have to be careful to implement it in an efficient way. Yeah, is there a performance hit that's significant? Generally not, if you implement it the right way. But it requires a lot of very low level, getting the language right bits. I assume there'll be a huge performance hit, because it's a really nice, the benefit is really nice, because you don't get into the complex. Absolutely. Well, the trick is, is you can't do copies. So you have to provide the behavior of copying without doing the copy. Yeah. How do you do that? How do you do that? It's not magic. It's just, it's actually pretty cool. Well, so first, before we talk about how that works, let's talk about how it works in Python, right? So in Python, you define a person class, or maybe a person class is a bad idea. You define a database class, right? And database class has an array of records, something like that, right? And so the problem is that if you pass in a record or a class instance into the database, it'll take a hold of that object. And then it assumes it has it. And if you're passing an object in, you have to know that that database is going to take, take it. And therefore you shouldn't change it after you put in the database, right? This is, this is kind of have to know that you just have to kind of know that, right? And so you roll out version one of the database, you just kind of have to know that. Of course, Lex uses his own database, right? Yeah, right? Because you built it, you understand this works, right? Somebody else joins the team, they don't know this. Yes. Right. And so now they suddenly get bugs, you're having to maintain the database, you shake your fist, you argue the 10th time this happens, you're like, okay, we have to do something different. Right. And so what you do is you go change your Python code, and you change your database class to copy the record every time you add it. And so what ends up happening is you say, okay, I will do what's called a defensive copy inside the database. And then that way, if somebody passes something in, I will have my own copy of it. And they can go do whatever, and they're not going to break my thing. Okay, this is usually the, the two design patterns. If you look in PyTorch, for example, this is cloning a tensor, like there's a specific thing, and you have to know where to call it. And if you don't call in the right place, you get these bugs, and this is state of the art. Right. So a different approach. So it's used in many languages. So I worked with it in Swift, as you say, okay, well, let's provide value semantics. And so we want to provide the view that you get a logically independent copy. But we want to do that lazily. And so what we do is you say, okay, if you pass something into a function, it doesn't actually make a copy. What it actually does is it just increments a reference to it. And if you pass it around, you stick in your database, you can go into the database, you own it. And then you come back out of the stack, nobody's copied anything, you come back out of the stack, and then the caller, let's go of it. Well, then you've just handed it off to the database, you've transferred it, and there's no copies made. Now, on the other hand, if, you know, your coworker goes and hands you a record, and you pass it in, you stick it in the database, and then you go to town, and you start modifying it, what happens is you get a copy lazily on demand. And so what this does, this gives you copies only when you need them. And it also, so it defines the way the bugs, but it also generally reduces the number of copies in practice. And so the implementation details are tricky here. Yes. So this is, yes, something with reference counting, but to make it performant across the number of different kinds of objects. Yeah, so you need a couple of things. And so there's many, so this concept has existed in many different worlds. And so, again, it's not novel research at all, right? The magic is getting the design right so that you can do this in a reasonable way, right? And so there's a number of components that go into this. One is when you're passing around, so we're talking about Python and reference counting at the expense of doing that, when you're passing values around, you don't want to do extra reference counting for no good reason. And so you have to make sure that you're efficient and you transfer ownership instead of duplicating references and things like that, which is a very low level problem. You also have to adopt this and you have to build these data structures. And so if you say, you know, Mojo has to be compatible with Python. So of course, the default list is a reference semantic list that works the way you'd expect in Python. But then you have to design a value semantic list. And so you just have to implement that and then you implement the logic within. And so the role of the language here is to provide all the low level hooks that allow the author of the type to be able to get and express this behavior without forcing it into all cases or hard coding this into the language itself. But there's ownership. So you're constantly transferring, you're tracking who owns the thing. Yes. And so there's a whole system called ownership. And so this is related to work done in the Rust community. Also the Swift community has done a bunch of work. And there's a bunch of different other languages that evolve kind of C++ actually has copy constructors and destructors and things like that. And so I mean, C++ has everything. So it has move constructors and has like this whole world of things. And so this is a body of work that's kind of been developing for many, many years now. And so Mojo takes some of the best ideas out of all these systems and remixes it in a nice way so that you get the power of something like the Rust programming language. But you don't have to deal with it when you don't want to, which is a major thing in terms of teaching and learning and being able to use and scale these systems. How does that play with argument conventions? What are they? Why are they important? How does the value semantics? How does the transfer ownership work with the arguments when they're passed into functions? So if you go deep into systems programming land, so this isn't again, this is not something for everybody, but if you go deep into systems programming land, what you encounter is you encounter these types that get weird. So if you're used to Python, you think about everything, I can just copy it around, I can go change it and mutate it and do these things. And it's all cool. If you get into systems programming land, you get into these things like I have an atomic number, or I have a mutex, or I have a uniquely owned database handle, things like this, right? So these types, you can't necessarily copy. Sometimes you can't necessarily even move them to a different address. And so what Mojo allows you to do is it allows you to express, hey, I don't want to get a copy of this thing. I want to actually just get a reference to it. And by doing that, well, you can say, as you can say, okay, if I'm defining something weird, like a atomic number or something, it has to be, so an atomic number is an area in memory that multiple threads can access at a time without locks. And so the definition of atomic numbers, multiple different things have to be poking it. Therefore, they have to agree on where it is. And so you can't just move it out from underneath one because it kind of breaks what it means. And so that's an example of a type that you can't copy, you can't move. Once you create, it has to be where it was. Now, if you look at many other examples, like a database handle, so okay, well, what happens, how do you copy a database handle? Do you copy the whole database? That's not something you necessarily want to do. There's a lot of types like that where you want to be able to say that they are uniquely owned. So there's always one of this thing. And or if I create a thing, I don't copy it. And so what Mojo allows you to do is it allows you to say, hey, I want to pass around a reference to this thing without copying it. And so it has borrowed conventions. So you can say, you can use it, but you don't get to change it. You can pass it by mutable reference. And so if you do that, then you can, you get a reference to it, but you can change it. And so it manages all that kind of stuff. So it's just a really nice implementation of like C++ has, you know, the different kinds of pointers, different kinds of applications, smart pointers that you can explicitly define this logic. But you're saying that's more like the weird case versus the common case. Well, it depends on where, I mean, I mean, I don't think I'm a normal person. So I mean, I'm not one to call other people weird. But the, but, you know, if you talk to a normal Python, a typical Python programmer, you're typically not thinking about this, right? This is a lower level of abstraction. Now, if you talk to a C++ programmer, certainly if you talk to a Rust programmer, again, they're not weird, they're delightful, like these are all good people, right? Those folks will think about all the time. Right. And so I look at this as there's a spectrum between very deep low level systems. I'm going to go poke the bits and care about how they're laid out in memory all the way up to application and scripting and other things like this. And so it's not that anybody's right or wrong. It's about how do we build one system that scales? By the way, the idea of an atomic number has been something that always brought me deep happiness. Because the flip side of that, the idea that threads can just modify stuff asynchronously, just the whole idea of concurrent programming is a source of infinite stress for me. Well, so this is where you jump into, you know, again, you zoom out and get out of programming languages or compilers and you just look at what the industry has done. My mind is constantly blown by this, right? And you look at what, you know, Moore's law, Moore's law has this idea that like computers for a long time, single thread performance has got faster and faster and faster and faster for free. But then physics and other things intervened and power consumption, like other things started to matter. And so what ended up happening is we went from single core computers to multi core, then we went to accelerators, right? And this trend towards specialization of hardware is only going to continue. And so for years, us programming language nerds and compiler people have been saying, okay, well, how do we tackle multi core, right? For a while, it was like, multi core is the future, we have to get on top of this thing. And then it was multi cores to default. What are we doing with this thing? And then it's like, there's chips with hundreds of cores in them. What happened, right? And so I'm super inspired by the fact that, you know, in the face of this, you know, those machine learning people invented this idea of a tensor, right? And what is a tensor? A tensor isn't like an arithmetic and algebraic concept, it's like an abstraction around a gigantic parallelizable data set, right? And because of that, and because of things like TensorFlow and PyTorch, we're able to say, okay, we'll express the math of the system. This enables you to do automatic differentiations, enables you to do like all these cool things. And it's an abstract representation. Because you have that abstract representation, you can now map it onto these parallel machines without having to control, okay, put that right here, put that right there, put that right there. And this has enabled an explosion in terms of AI, compute, accelerators, like all the stuff. And so that's super, super excited. What about the deployment, the execution across multiple machines? So you write that the modular compute platform dynamically partitions models with billions of parameters and distributes their execution across multiple machines, enabling unparalleled efficiency. By the way, the use of unparalleled in that sentence, anyway, enabling unparalleled efficiency scale and reliability for the largest workloads. So how do you do this abstraction of distributed deployment of large models? Yeah, so one of the really interesting tensions. So there's a whole bunch of stuff that goes into that. I'll pick a random walkthrough. If you go back and replay the history of machine learning, right? I mean, the brief, the most recent history of machine learning, because this is, as you know, very deep. I knew Lex when he had an AI podcast. So if you look at just TensorFlow and PyTorch, which is pretty recent history in the big picture, right? But TensorFlow is all about graphs. PyTorch, I think, pretty unarguably ended up winning. And why did it win? Mostly because of usability, right? And the usability of PyTorch is, I think, huge. And I think, again, that's a huge testament to the power of taking abstract theoretical technical concepts and bring it to the masses, right? Now, the challenge with what the TensorFlow versus the PyTorch design points was that TensorFlow is kind of difficult to use for researchers, but it was actually pretty good for deployment. PyTorch is really good for researchers. It kind of not super great for deployment, right? And so I think that we as an industry have been struggling. And if you look at what deploying a machine learning model today means is that you'll have researchers who are, I mean, wicked smart, of course, but they're wicked smart at model architecture and data and calculus. They're wicked smart in various domains. They don't want to know anything about the hardware or deployment or C++ or things like this, right? And so what's happened is you get people who train the model, they throw it over the fence, and then you have people that try to deploy the model. Well, every time you have a team A does X, they throw it over the fence, and team B does Y, you have a problem. Because, of course, it never works the first time. And so you throw it over the fence, they figure out, okay, it's too slow, won't fit, doesn't use the right operator, the tool crashes, whatever the problem is, then they have to throw it back over the fence. And every time you throw a thing over a fence, it takes three weeks of project managers and meetings and things like this. And so what we've seen today is that getting models in production can take weeks or months. Like it's not atypical, right? I talked to lots of people and you talk about like VP of software at some internet company trying to deploy a model, and they're like, why do I need a team of 45 people? It's so easy to train a model, why can't I deploy it, right? And if you dig into this, every layer is problematic. So if you look at the language piece, I mean, this is tip of the iceberg. It's a very exciting tip of the iceberg for folks, but you've got Python on one side and C++ on the other side. Python doesn't really deploy. I mean, can theoretically, technically in some cases, but often a lot of production teams will want to get things out of Python because they get their performance and control and whatever else. So Mojo can help with that. If you look at serving, so you talk about gigantic models, well, a gigantic model won't fit on one machine, right? And so now you have this model, it's written Python, it has to be rewritten in C++. Now it also has to be carved up so that half of it runs on one machine, half of it runs on another machine, or maybe it runs on 10 machines. So now suddenly, the complexity is exploding, right? And the reason for this is that if you look into TensorFlow, PyTorps, these systems, they weren't really designed for this world, right? They were designed for, you know, back in the day when we were starting and doing things where it was a different, much simpler world, like you want to run ResNet 50 or some ancient model architecture like this. It was just a, it was a completely different world. Trained on 1GPU. Exactly. Doing some 1GPU. Yeah, AlexNet, right? The major breakthrough. And the world has changed, right? And so now the challenge is that TensorFlow, PyTorps, these systems, they weren't actually designed for LLMs. That was not a thing. And so where TensorFlow actually has amazing power in terms of scale and deployment and things like that. And I think Google is, I mean, maybe not unmatched, but they're incredible in terms of their capabilities and gigantic scale. Many researchers using PyTorps, right? And so PyTorps doesn't have those same capabilities. And so what Modular can do is it can help with that. Now, if you take a step back and say like, what is Modular doing, right? So Modular has like a bitter enemy that we're fighting against in the industry. And it's one of these things where everybody knows it, but nobody is usually willing to talk about it. The bitter enemy. The bitter thing that we have to destroy, that we're all struggling with, and it's like fish can't see water, is complexity. Sure. Yes. It's complexity. That was very close. And so if you look at it, yes, it is on the hardware side. Yes. All these accelerators, all these software stacks that go with the accelerator, all these massive complexity over there. You look at what's happening on the modeling side, massive amount of complexity. Like things are changing all the time. People are inventing, turns out the research is not done, right? And so people want to be able to move fast. Transformers are amazing, but there's a ton of diversity even within transformers. And what's the next transformer, right? And you look into serving also huge amounts of complexity. It turns out that all the cloud providers, right, have all their very weird, but very cool hardware for networking and all this kind of stuff. And it's all very complicated. People aren't using that. You look at classical serving, right? There's this whole world of people who know how to write high-performance servers with zero-copy networking and like all this fancy, asynchronous I.O. and like all these fancy things in the serving community, very little that has pervaded into the machine learning world, right? And why is that? Well, it's because, again, these systems have been built up over many years. They haven't been rethought. There hasn't been a first principles approach to this. And so what Modular's doing is we're saying, okay, we've built many of these things. So I've worked on TensorFlow and TPUs and things like that. Other folks on our team have worked on PyTorch core. We've worked on Onyx one time. We've worked on many of these other systems. And so systems like the Apple accelerators and all that kind of stuff, our team is quite amazing. And so one of the things that roughly everybody at Modular's grumpy about is that when you're working on one of these projects, you have a first order goal, get the hardware to work, get the system to enable one more model, get this product out the door, enable this specific workload or solve this problem for this product team, right? And nobody's been given a chance to actually do that step back. And so we as an industry, we didn't take two steps forward. We took like 18 steps forward in terms of all this really cool technology across compilers and systems and runtimes and heterogeneous computing, like all this kind of stuff. And like all this technology has been, you know, I wouldn't say beautifully designed, but it's been proven in different quadrants. Like, you know, you look at Google with TPUs, massive, huge exoflops of compute strapped together into machines that researchers are programming in Python in a notebook. That's huge. That's amazing. That's incredible, right? It's incredible. And so you look at the technology that goes into that. And the algorithms are actually quite general. And so lots of other hardware out there and lots of other teams out there don't have the sophistication or maybe the years working on it or the budget or whatever that Google does, right? And so they should be getting access to the same algorithms, but they just don't have that, right? And so what Modular's doing is we're saying, cool, this is not research anymore. Like, we've built auto tuning in many systems. We've built programming languages, right? And so like have, have, you know, implemented C++ have implemented Swift have implemented many of these things. And so, you know, it's hard, but it's not research. And you look at accelerators. Well, we know there's a bunch of different weird kind of accelerators, but they actually cluster together, right? And you look at GPUs. Well, there's a couple of major vendors of GPUs, and they maybe don't always get along, but their architectures are very similar. You look at CPUs. CPUs are still super important for the deployment side of things. And you see new, new architectures coming out from all the cloud providers and things like this, and they're all super important to the world, right? But they don't have the 30 years of development that the entrenched people do, right? And so what Modular can do is we're saying, okay, all this complexity, like, it's not, it's not bad complexity. It's actually innovation, right? And so it's innovation that's happening. And it's for good reasons. But I have sympathy for the poor software people, right? I mean, again, I'm generally a software person too, I love hardware. But software people want to build applications and products and solutions that scale over many years. They don't want to build a solution for one generation of hardware with one vendor's tools. Right? And because of this, they need something that scales with them, they need something that works on cloud and mobile, right? Because, you know, their product manager said, Hey, I want to be at have lower latency, it's better for personalization or whatever they decide, right? Products evolve. And so the challenge with the machine learning technology and the infrastructure we have today, in the industry, is that it's all these point solutions. And because they're all these point solutions, it means that as your product evolves, you have to like switch different technology stacks or switch to different vendor. And what that does is that slows down progress. So basically, a lot of the things we've developed in those little silos for machine learning tasks, you want to make that the first class citizen of a general purpose programming language, they can then be compiled across all these kinds of hardware. Well, so it's not really about a programming language. I mean, the program language is a component of the mission, right? And the mission is are not literal, but our joking mission is to save the world from terrible AI software. Excellent. Okay. So, so, you know, if you look at this mission, you need a syntax. So that's the so yeah, she need a program language, right? And and like, we wouldn't have to build the programming language if one existed. Right? So if Python was already good enough, then cool, we would just used it, right? We're not just doing very large scale expensive engineering projects for the sake of it, like, it's to solve a problem, right? It's also about accelerators. It's also about exotic numerics and b-float 16 and matrix multiplications and convolutions and like this, this kind of stuff. Within the stack, there are things like kernel fusion. It's a esoteric but really important thing that leads to much better performance and much more general research hackability together, right? And that's enabled by the ASICS, that's enabled by certain hardware. So it's like, where's the dance between I mean, there's several questions here, like, how do you add a piece of hardware to this stack? Yeah. If you need pieces, like if I have this genius invention of a specialized accelerator, how do I add that to the modular framework? And also, how does modular as a standard start to define the kind of hardware that should be developed? Yeah. So let me take a step back and talk about status quo. Okay. And so if you go back to TensorFlow 1, PyTorch 1, this kind of timeframe, and these have all evolved and gotten way more complicated. So let's go back to the glorious simple days, right? These things basically were CPUs and CUDA. And so what you do is you say, go do a dense layer and a dense layer has a matrix multiplication, right? And so when you say that, you say, go do this big operation, a matrix multiplication. And if it's on a GPU, kick off CUDA kernel, if it's on a CPU, go do like an Intel algorithm or something like that with the Intel MKL. Okay. Now, that's really cool. If you're either video or Intel, right? But then more hardware comes in. Right. And on one axis, you have more hardware coming in. On the other hand, you have an explosion of innovation in AI. And so what happened with both TensorFlow and PyTorch is that the explosion of innovation in AI has led to, it's not just about matrix multiplication and convolution. These things have now like 2,000 different operators. And on the other hand, you have, I don't know how many pieces of hardware there are there, it's a lot. It's not even hundreds, it's probably thousands. Okay. And across all the edge and across all the different things that are used at scale. Yeah, exactly. I mean, it's not just like everywhere. Yeah. It's not a handful of TPU alternatives. Correct. It's every phone, often with many different chips inside of it from different vendors. Right. Like it's, AI is everywhere. It's a thing, right? Why are they all making their own chips? Like, why is everybody making their own thing? Well, so because, was that a good thing? First of all, so Chris's philosophy on hardware. Yeah. Right. So my philosophy is that there isn't one right solution. Right. And so I think that, again, we're at the end of Moore's Law, specialization happens. Yeah. If you're building, if you're training GPT-5, he wants some crazy supercomputer data center thingy. If you're making a smart camera that runs on batteries, you want something that looks very different. If you're building a phone, you want something that looks very different. If you have something like a laptop, you want something that looks maybe similar, but a different scale. Right. And so AI ends up touching all of our lives, robotics. Right. And like, lots of different things. And so as you look into this, these have different power envelopes. There's different trade-offs in terms of the algorithms. There's new innovations in sparsity and other data formats and things like that. And so hardware innovation, I think is a really good thing. Right. And what I'm interested in is unlocking that innovation. There's also like analog and quantum and like, although the really weird stuff. Right. And so if somebody can come up with a chip that uses analog computing and it's 100x more power efficient, I think what that would mean in terms of the daily impact on the products we use, that'd be huge. Now, if you're building an analog computer, you may not be a compiler specialist. Right. These are different skill sets. Right. And so you can hire some compiler people if you're running a big company, maybe. But it turns out these are really like exotic new generation of compilers. Like this is a different thing. Right. And so if you take a step back out and come back to it, what is the status quo? The status quo is that if you're Intel or you're Nvidia, you continue to keep up with the industry and you chase and, okay, there's 1900 now. There's 2000 now. There's 2100. And you have a huge team of people that are like trying to keep up and tune and optimize. And even when one of the big guys comes out with a new generation of their chip, they have to go back and rewrite all these things. Right. So really, it's only powered by having hundreds of people that are all like frantically trying to keep up. And what that does is that keeps out the little guys. And sometimes they're not so little guys, the big guys that are also just not in those dominant positions. And so what has been happening, and so a lot of you talk about the rise of new exotic crazy accelerators is people have been trying to turn this from a let's go write lots of special kernels problem into a compiler problem. And so we and I contributed to this as well. We as an industry went into it like let's go make this compiler problem phase, let's call it. And much of the industry is still in this phase, by the way. So I wouldn't say this phase is over. And so the idea is to say, look, okay, what a compiler does is it provides a much more general, extensible, hackable interface for dealing with the general case. Right. And so within machine learning algorithms, for example, people figured out that, hey, if I do a matrix multiplication, I do a value, right, the classic activation function, it is way faster to do one pass over the data and then do the value on the output, where I'm writing out the data, because really, it's just a maximum operation, right, max is zero. And so it's an amazing optimization, take map more value, squish together one operation, now I have map more value. Well, wait a second, if I do that, now I just went from having, you know, two operators to three. But now I figure out, okay, well, there's a lot of activation functions. What about leaky value? What about like a million things that are out there, right. And so as I start fusing these in, now I get permutations of all these algorithms, right. And so what the compiler people said is they said, hey, cool, I will go enumerate all the algorithms and I will enumerate all the pairs and I will actually generate a kernel for you. And I think that this has been very, very useful for the industry. This is one of the things that powers Google TPUs, PyTorch 2s, like rolling out really cool compiler stuff with Triton, this other technology and things like this. And so the compiler people are kind of coming into their fore and saying like, awesome, this is a compiler problem, we'll compiler it. Here's the problem. Not everybody's a compiler person. I love compiler people, trust me, right, but not everybody can or should be a compiler person. It turns out that there are people that know analog computers really well, or they know some GPU internal architecture thing really well, or they know some crazy, sparse, numeric, interesting algorithm that is the cusp of research, but they're not compiler people. And so one of the challenges with this new wave of technology trying to turn everything into a compiler is again, it is excluded a ton of people. And so you look at what does Mojo do, what does the modular stack do is brings programmability back into this world, like it enables, I wouldn't say normal people, but a different kind of delightful nerd that cares about numerics or cares about hardware or cares about things like this to be able to express that in the stack and extend the stack without having to actually go hack the compiler itself. So extend the stack on the algorithm side, and then on the hardware side. Yeah, so again, go back to the simplest example of int. And so both Swift and Mojo and other things like this did is we said, okay, pull magic out of the compiler and put it in the standard library. And so what modular is doing with the engine that we're providing and like this, this very deep technology stack, right, which goes into heterogeneous runtimes and like a whole bunch of really cool, really cool things. This whole stack allows that stack to be extended and hacked and changed by researchers and by hardware innovators and by people who know things that we don't know, because you know, modular has some smart people, but we don't have all the smart people, it turns out. What are heterogeneous runtimes? Yeah, so what is heterogeneous, right? So heterogeneous means many different kinds of things together. And so the simplest example you might come up with is a CPU and a GPU. And so it's a simple heterogeneous computer to say, I will run my data loading and preprocessing and other algorithms on the CPU. And then once I get it into the right shape, I shove it into the GPU, I do a lot of matrix multiplications and convolutions and things like this. And I get it back out and I do some reductions and summaries and they shove it across the wire to across the network to another machine, right? And so you've got now what are effectively two computers, a CPU and a GPU talking to each other, working together in a heterogeneous system. But that was 10 years ago. Okay, look at a modern cell phone. Modern cell phone, you've got CPUs. And they're not just CPUs, there's like big dot little CPUs. And so there's multiple different kinds of CPUs are working together. They're multi core. You've got GPUs, you've got neural network accelerators, you got dedicated hardware blocks for for media. So for video decode and JPEG decode and things like this. And so you've got this massively complicated system. And this isn't just cell phones, every laptop these days is doing the same thing. And all these blocks can run at the same time and need to be choreographed, right? And so again, one of the cool things about machine learning is it's moving things to like data flow graphs and higher level of abstractions and tensors and these things that it doesn't specify. Here's how to do the algorithm. It gives the system a lot more flexibility in terms of how to translate or map or compile it onto the system that you have. And so what you need, you know, at the bottomest part of the layer there is a way for all these devices to talk to each other. And so this is one thing that, you know, I'm very passionate about. I mean, you know, I'm a nerd, but, but all these, all these machines and all these systems are effectively parallel computers running at the same time, sending messages to each other. And so they're all fully asynchronous. Well, this is actually a small version of the same problem you have in a data center, right? In a data center, you now have multiple different machines, sometimes very specialized, sometimes with GPUs or TPs and one node and sometimes with disks and other nodes. And so you get a much larger scale, heterogeneous computer. And so what ends up happening is you have this like multi-layer abstraction of hierarchical parallelism, hierarchical asynchronous communication and making that again, the enemy, my enemy is complexity by getting that away from being different specialized systems at every different part of the stack and having more consistency and uniformity. I think we can help lift the world and make it much simpler and actually get used. But how do you leverage like the strengths of the different specialized systems? So we're looking inside the smartphone. Yeah. Like there's just what I don't know, five, six computers essentially inside a smartphone. How do you, without trying to minimize the explicit, making it explicit, which, which computer is supposed to be used for which operation? Yeah. So there's, there's a pretty well known algorithm. And what you're doing is you're looking at two, two factors. You're looking at the factor of sending data from one thing to another. Right. So it takes time to get it from that side of the chip to that side of the chip and things like this. And then you're looking at what is the time it takes to do an operation on a particular block. So take CPUs. CPUs are fully general. They can do anything. Right. But then you have a neural net accelerator that's really good at matrix multiplications. Okay. And so you say, okay, well, if my workload is all matrix multiplications, I start up, I send the data over the neural net thing, it goes and does matrix multiplications, when it's done, it sends me back the result, all is good. Right. And so the simplest thing is just saying, do matrix, do matrix operations over there. Right. But then you realize you get a little bit more complicated because you can do matrix multiplications on a GPU, you can do it on a neural net accelerator, you can do it on CPU, and they'll have different tradeoffs and costs. And it's not just matrix multiplication. And so what you actually look at is you look at, I have generally a graph of compute. I want to do a partitioning. I want to look at the communication, the bisection bandwidth and like the overhead and the sending of all these different things and build a model for this and then decide, okay, it's an optimization problem. Where do I want to place this compute? So the old school theoretical computer science problem of scheduling. And then how does presumably it's possible to somehow magically include auto tune into this? Absolutely. So I mean, in my opinion, this is an opinion, this is not, not everybody would agree with this, but in my opinion, the world benefits from simple and predictable systems at the bottom that you can control. But then once you have a predictable execution layer, you can build lots of different policies on top of it. Right. And so one policy can be that the human programmer says, do that here, do that here, do that here, do that here and like fully manually controls everything. And the systems just do it. Right. Then you quickly get in the mode of like, I don't want to have to tell it to do it. Yeah. And so the next logical step that people typically take, because they write some terrible heuristic, if it's a major small location, do it over there. Or if it's floating point, do it on the GPU, if it's integer, do it on the CPU, like something like that. Right. And, and then you then get into this mode of like people care more and more and more and you say, okay, well, let's actually like make the heuristic better. Let's get into auto tint. Let's actually do a search of the space to decide, well, what is actually better? Right. Well, then you get into this problem where you realize this is not a small space. This is a many dimensional, hyper dimensional space that you cannot exhaustively search. So do you know of any algorithms that are good at searching very complicated spaces for? Don't tell me you're going to turn this into a machine learning problem. So then you turn into a machine learning problem. And then you have a space of generic algorithms and reinforcement learning and like all these, all these, can you include that into the stack, into the, into the module stack? Yeah. Yeah. And where does it sit? Where does it live? Is it a separate thing or is it part of the compilation? So you start from simple and predictable models. And so you can have full control and you can have coarse grain knobs that like nudge, nudge systems, you don't have to do this. But if you really care about getting the best, you know, the last ounce out of a problem, then you can use additional tools. And they're the cool thing is you don't want to do this every time you run a model. You want to figure out the right answer and then cash it. And once you do that, you can get, you can say, okay, cool, I can get up and running very quickly. I can get good execution out of my system. I can decide if something's important. And if it's important, I can go throw a bunch of machines at it and do a big expensive search over the space using whatever technique I feel like it's probably up to the problem. And then when I get the right answer, cool, I can just start using it. And so you can get out of this, this trade off between, okay, am I going to like spend forever doing a thing or do I get up and running quickly? And as a quality result, like these, these are actually not in contention with each other if the system's designed to scale. You started and did a little bit of a whirlwind overview of how you get the 35,000 X speed up or more over Python. Jeremy Howard did a really great presentation about sort of the basic, like looking at the code, here's how you get the speed up. Like you said, that's something we could probably developers can do for their own code to see how you can get these gigantic speed up. But can you maybe speak to the machine learning tasks in general? How do you, how do you make some of this code fast, some specifics like what would you say is the main bottleneck for machine learning tasks? So are we talking about metmall, matrix multiplication, how do you make that fast? So I mean, if you just look at the Python problem, right, you can say, how do I make Python faster? There's been a lot of people that have been working on the, okay, how do I make Python 2x faster, 10x faster or something like that, right? And there've been a ton of projects in that vein, right? Mojo started from the, what can the hardware do? Like what is the limit of physics? Yeah, what is the speed of light? What is it? Like how fast can this thing go? And then how do I express that? Yeah, right. And so it wasn't, it wasn't anchored relatively on make Python a little bit faster. It's saying, cool, I know what the hardware can do. Let's unlock that, right? Now, when you, wait, and just say how, how gutsy that is to be in the meeting, and as opposed to trying to see how do we get the improvement? It's like, what can the physics do? I mean, maybe I'm a special kind of nerd, but you look at that, what is the limit of physics, how fast can these things go, right? When you start looking at that, typically, it ends up being a memory problem, right? And so today, particularly with these specialized accelerators, the problem is that you can do a lot of math within them, but yet you get bottleneck sending data back and forth to memory, whether it be local memory or distant memory or disk or whatever it is. And that bottleneck, particularly as the training sizes get large, as you start doing tons of inferences all over the place, like that becomes a huge bottleneck for people, right? So again, what happened is we went through a phase of many years where people took the special case and hand tuned it and tweaked it and tricked it out and they knew exactly how the hardware worked and they knew the model and they made it, they made it fast. Didn't generalize. And so you can make, you know, ResNet 50 or some, or AlexNet or something, Inception V1, like you can, you can do that, right? Because the models are small, they fit in your head, right? But as the models get bigger, more complicated, as the machines get more complicated, it stops working, right? And so this is where things like kernel fusion come in. So what is kernel fusion? This is this idea of saying, let's avoid going to memory. And let's do that by building a new hybrid kernel and a numerical algorithm that actually keeps things in the accelerator instead of having to write it all the way out to memory. What's happened with these accelerators now is you get multiple levels of memory. Like in a GPU, for example, you'll have global memory and local memory and like all these things. If you zoom way into how hardware works, the register file is actually a memory. So the registers are like an L0 cache. And so a lot of taking advantage of the hardware ends up being fully utilizing the full power in all of its capability. And this has a number of problems, right? One of which is, again, the complexity disaster, right? There's too much hardware. Even if you just say, let's look at the chips from one line of vendor like Apple or Intel or whatever it is, each version of the chip comes out with new features. And they change things so that it takes more time or less time to do different things. And you can't rewrite all the software whenever a new chip comes in, right? And so this is where you need a much more scalable approach. And this is what Mojo and what the modular stack provides is it provides this infrastructure and the system for factoring all this complexity and then allowing people to express algorithms. You talk about auto tuning, for example, express algorithms in a more portable way so that when a new chip comes out, you don't have to rewrite it all. So to me, like, you know, I kind of joke like, what is a compiler? Well, there's many ways to explain that. You convert thing A into thing B, and you convert source code to machine code. Like you can talk about many, many things that compilers do. But to me, it's about a bag of tricks. It's about a system and a framework that you can hang complexity. It's a system that can then generalize and it can work on problems that are bigger than fit in one human's head, right? And so what that means, what a good stack and what the modular stack provides is the ability to walk up to it with a new problem and it'll generally work quite well. And that's something that a lot of machine learning infrastructure and tools and technologies don't have. Typical state of the art today is you walk up, particularly if you're deploying, if you walk up with a new model, you try to push it through the converter and the converter crashes. That's crazy. The state of ML tooling today is not anything that a C programmer would ever accept, right? And it's always been this kind of flaky set of tooling that's never been integrated well and it's been never worked together because it's not designed together. It's built by different teams. It's built by different hardware vendors. It's built by different systems. It's built by different internet companies that are trying to solve their problems, right? And so that means that we get this fragmented, terrible mess of complexity. So, I mean, the specifics of, I mean, Jeremy showed this. There's the vectorized function, which I guess is built into Mojo. Vectorized as he showed is built into the library. Into the library, instead of the library. Vectorized, parallelized, which vectorizes more low level, parallelizes higher level. There's the tiling thing, which is how he demonstrated the autotune, I think. So, think about this in like levels, hierarchical levels of abstraction, right? And so, at the very, if you zoom all the way into a compute problem, you have one floating point number, right? And so, then you say, okay, I want to be, I can do things one at a time in an interpreter. It's pretty slow, right? So, I can get to doing one at a time in a compiler, I can see. Then I can get to doing four or eight or 16 at a time with vectors. That's called vectorization. Then you can say, hey, I have a whole bunch of different, you know, what a multi-core computer is, it's basically a bunch of computers, right? So, they're all independent computers that can talk to each other and they share memory. And so, now what parallelized does, it says, okay, run multiple instances on different computers. And now they can all work together on a problem, right? And so, what you're doing is you're saying, keep going out to the next level out. And as you do that, how do I take advantage of this? So, tiling is a memory optimization, right? It says, okay, let's make sure that we're keeping the data close to the compute part of the problem, instead of sending it all back and forth through memory every, every time I load a block. And the size of the block, size is all, that's how you get to the autotune to make sure it's optimized. Right. Yeah. Well, so all of these, the details matter so much to get good performance. This is another funny thing about machine learning and high performance computing that is very different than C compilers we all grew up with, where, you know, if you get a new version of GCC or a new version of Clang or something like that, you know, maybe something will go 1% faster, right? And so compiler insurers will work really, really, really hard to get half a percent out of your C code, something like that. But when you're talking about an accelerator or an AI application, or you're talking about these kinds of algorithms, and these are things people used to write in Fortran, for example, right? If you get it wrong, it's not 5% or 1%. It could be 2x or 10x. Right. If you think about it, you really want to make use of the full memory you have the cache, for example. But if you use too much space, it doesn't fit in the cache. Now you're going to be thrashing all the way back out to main memory. And these can be 2x, 10x, major performance differences. And so this is where getting these magic numbers and these things right is really actually quite important. So you mentioned that Moji is a superset of Python. Can you run Python code as if it's Mojo code? Yes. Yes. And this has two sides of it. So Mojo's not done yet. So I'll give you a disclaimer. Mojo's not done yet. But already we see people that take small pieces of Python code, move it over. They don't change it. And you can get 12x speedups. Somebody was just tweeting about that yesterday, which is pretty cool. And again, interpreters, compilers. And so without changing any code, also this is not JIT compiling or do anything fancy. This is just basic stuff. Move it straight over. Now Mojo will continue to grow out. And as it grows out, it will have more and more and more features. And our North Star is to be a full superset of Python. And so you can bring over basically arbitrary Python code and have it just work. And it may not always be 12x faster, but it should be at least as fast and way faster in many cases. This is cool. Right. Now, it will take time to do that. And Python is a complicated language. There's not just the obvious things, but there's also non-obvious things that are complicated. Like we have to be able to talk to CPython packages that talk to the C API. And there's a bunch of pieces to this. So you have to, I mean, just to make explicit, the obvious may not be so obvious until you think about it. So to run Python code, that means you have to run all the Python packages and libraries. So that means what? What's the relationship between Mojo and CPython, the interpreter that presumably would be tasked with getting those packages to work? So in the fullness of time, Mojo will solve for all the problems and you'll be able to move Python packages over and run them in Mojo. Without the CPython. Without CPython. Someday. Yeah. Right. It's not today, but someday. And that'll be a beautiful day because then you'll get a whole bunch of advantages and you'll get massive speedups and things like this. But you can do that one at a time, right? You can move packages one at a time. Exactly. But we're not willing to wait for that. Python is too important. The ecosystem is too broad. We want to both be able to build Mojo out. We also want to do it the right way without time, without intense time pressure. We're obviously moving fast. And so what we do is we say, okay, well, let's make it so you can import an arbitrary existing package, arbitrary, including you write your own on your local disk or whatever. It's not like a standard like an arbitrary package. And import that using CPython. Because CPython already runs all the packages. And so what we do is we built an integration layer where we can actually use CPython. Again, I'm practical to actually just load and use all the existing packages as they are. The downside of that is you don't get the benefits of Mojo for those packages. Right. And so they run as fast as they do in the traditional CPython way. But what that does is that gives you an incremental migration path. And so if you say, hey, cool, well, here's a, you know, the Python ecosystem is vast. I want all of it to just work. But there's certain things that are really important. And so if I, if I'm doing weather forecasting or something, well, I want to be able to load all the data. I want to be able to work with it. And then I have my own crazy algorithm inside of it. Well, normally I'd write that in C++. If I can write in Mojo and have one system that scales, well, that's way easier to work with. Is it hard to do that to have that layer that's running CPython? Because is there some communication back and forth? Yes, it's complicated. I mean, this is what we do. So I mean, we make it look easy, but it is, it is complicated. But what we do is we use the CPython existing interpreter. So it's running its own byte codes, and that's how it provides full compatibility. And then it gives us CPython objects. And we use those objects as is. And so that way, we're fully compatible with all the CPython objects and all the, you know, it's not just the Python part, it's also the C packages, the C libraries underneath them, because they're often hybrid. And so we can fully run and we're fully compatible with all that. And the way we do that is that we have to play by the rules, right? And so we keep objects in that representation when they're coming from that world. What's the representation that's being used in memory? We'd have to know a lot about how the CPython interpreter works. It has, for example, reference counting, but also different rules on how to pass pointers around and things like this. Super low level fiddly. And it's not like Python, it's like how the interpreter works. Okay. And so that gets all exposed out. And then you have to define wrappers around the low level C code. Right. And so what this means is you have to know not only C, which is a different world from Python, obviously, not only Python, but the wrappers, but the interpreter and the wrappers and the implementation details and the conventions. And it's just this really complicated mess. And when you do that, now suddenly you have a debugger that debugs Python, they can't step into C code. So you have this two world problem, right? And so by pulling this all into Mojo, what you get is you get one world. You get the ability to say, cool, I have untyped, very dynamic, beautiful, simple code. Okay, I care about performance for whatever reason, right? There's lots of reasons you could, you might care. And so then you add types, you can parallelize things, you can vectorize things, you can use these techniques, which are general techniques to solve a problem. And then you can do that by staying in the system. And if you're, you have that one Python package is really important to you, you can move it to Mojo, you get massive performance benefits on that. And other advantages, you know, if you like SAC types, it's nice if they're enforced. Some people like that, right, rather than being hints. So there's other advantages too. And then, and then you can do that incrementally as you go. So one different perspective on this will be why Mojo instead of making C Python faster, redesigning C Python. Yeah, well, I mean, you can argue Mojo is redesigning C Python, but, but, but why not make C Python faster and better and other things like that. There's lots of people working on that. So actually, there's a team at Microsoft that is really improving, I think C Python 3.11 came out in October or something like that. And it was, you know, 15% faster, 20% faster across the board, which is pretty huge, given how mature Python is and things like this. And so that's awesome. I love it. Doesn't run on GPU. It doesn't do AI stuff, like it doesn't do vectors, doesn't do things. I'm 20% good, 35,000 times is better. Right. So like they're, they're, they're, they're definitely, I'm a huge fan of that work, by the way, and it composes well with what we're doing. And so it's not, it's not like we're fighting or anything like that. It's actually just general, it's goodness for the world. But it's just a different path. Right. And again, we're not working forwards from making Python a little bit better, we're working backwards from what is the limit of physics. What's the process of porting Python code to Mojo? Is there, what's involved in that, in the process? Is there tooling for that? Not yet. So we're missing some basic features right now. And so we're continuing to drop out new features, like on a weekly basis. But, you know, at the fullness of time, give us a year and a half, maybe two years. Is it an automatable process? So when we're ready, it will be very automatable. Yes. Is it automatable? Like is it possible to automate in the general case, the Python to Mojo conversion? Yeah. Well, you're saying it's possible. Well, so, and this is why, I mean, among other reasons why we use tabs. Yes. Right. So first of all, by being a superset, yeah, you can, it's like C versus C plus plus. Can you move C code to C plus plus? Yes. Yeah. Right. And you move, you can move C code to C plus plus. And then you can adopt classes, you can add, adopt templates, you can adopt other references or whatever C plus plus features you want. After you move C to C code to C plus plus, like you can't use templates in C. Right. And so if you leave it to C, fine, you can't use the cool features, but it still works. Right. And C and C plus plus could work together. And so that's the analogy. Right. Now, here, right, you, you, there's not a Python is bad and then Mojo is good. Right. Mojo just gives you superpowers. Right. And so if you want to stay with Python, that's cool. But the tooling should be actually very beautiful and simple because we're doing the hard work of defining a superset. Right. So you're right. So there's several things to say there, but also the conversion tooling should probably give you hints as to like how you can improve the code. And then exactly once you're in the new world, then you can build all kinds of cool tools to say like, Hey, should you adopt this feature? Or like, and we haven't built those tools yet, but I fully expect those tools will exist. And then you can like, you know, quote unquote modernize your code or however you want to look at it. Right. So I mean, one of the things that I think is really interesting about Mojo is that there have been a lot of projects to improve Python over the years. Everything from, you know, getting Python run on the Java virtual machine, PyPy, which is a JIT compiler, there's tons of these projects out there that have been working on improving Python in various ways. They've fallen to one of two camps. So PyPy is a great example of a camp that is trying to be compatible with Python. Even there, not really doesn't work with all the C packages and stuff like that. But but they're trying to be compatible with Python. There's also another category of these things where they're saying, well, Python is too complicated. And, you know, I'm going to cheat on the edges. And, you know, like integers in Python can be an arbitrary size integer. If you care about it fitting in a going fast on a register and a computer, that's really annoying. Right. And so you can, you can choose to pass on that. Right. You can say, well, people don't really use big integers that often. Therefore, I'm going to just not do it. And it will be fine. Not not a Python superset. Or you can do the hard thing and say, okay, this is Python. You can't be a superset of Python without being a superset of Python. And that's a really hard technical problem. But it's, in my opinion, worth it. Right. And it's worth it because it's not about anyone packages about this ecosystem. It's about what Python means for the world. And it also means we don't want to repeat the Python 2 to Python 3 transition. Like we want, we want people to be able to adopt this stuff quickly. And so by doing that work, we can help lift people. Yeah, the challenge, it's really interesting technical philosophical challenge of really making a language a superset of another language. That's breaking my brain a little bit. Well, it paints you into corners. So again, I'm very happy with Python. So joking, all joking aside, I think that the indentation thing is not the actual important part of the problem. Right. But the fact that Python has amazing dynamic metaprogramming features, and they translate to beautiful static metaprogramming features, I think is profound. I think that's huge. Right. And so Python, I've talked with Guido about this. It's like, it was not designed to do what we're doing. That was not the reason they built it this way. But because they really cared and they were very thoughtful about how they designed the language, it scales very elegantly in the space. But if you look at other languages, for example, C and C++, right, if you're building a superset, you get stuck with the design decisions of the subset. Right. And so, you know, C++ is way more complicated because of C in the legacy than it would have been if they would have theoretically designed a from scratch thing. And there's lots of people right now that are trying to make C++ better and re-syntax C++. It's going to be great. We'll just change all the syntax. But if you do that, now suddenly you have zero packages. You don't have compatibility. So what are the, if you could just linger on that, what are the biggest challenges of keeping that superset status? What are the things just struggling with? Is it all boiled down to having a big integer? No, I mean, what are the other things like? Usually it's the, it's a long tail of weird things. So let me, let me give you a war story. So war story in the space is you go way back in time. Project I worked on is called Clang. Clang, what it is, is a C C++ parser, right. And when I started working on Clang, it must have been like 2006 or something was when I, 2007, 2006, when I first started working on it, right? It's funny how time flies. I started that project and I'm like, okay, well, I want to build a C parser, C++ parser for LLVM. It's going to be the word GCC is yucky. You know, this is me in earlier times. It's yucky. It's unprincipled. It has all these weird features, like all these bugs, like it's yucky. So I'm going to build a standard compliant C and C++ parser. It's going to be beautiful. It'll be amazing. Well, engineered, all the cool things an engineer wants to do. And so I start implementing building it out, building it out, building it out. And then I got to include standard IO.h. And all of the headers in the world use all the GCC stuff. So again, come back away from theory back to reality, right? I was at a fork on the road. I could have built an amazingly beautiful academic thing that nobody would ever use. Or I could say, well, it's yucky in various ways. All these design mistakes, accents of history, the legacy at that point, GCC was like over 20 years old, which, by the way, now LLVM is over 20 years old. So it's funny how time catches up to you, right? And so you say, okay, well, what is easier, right? I mean, as an engineer, it's actually much easier for me to go implement long tail compatibility weird features, even if they're distasteful, and just do the hard work and like figure it out, reverse engineer, understand what it is, write a bunch of test cases, like try to understand behavior. It's way easier to do all that work as an engineer than it is to go talk to all C programmers and get, argue with them and try to get them to rewrite their code. Yeah. Right. And because that breaks a lot more things. Yeah. And you have realities like nobody actually even understands how the code works, because it was written by the person who quit 10 years ago, right? And so this software is kind of frustrating that way, but it's, that's how the world works. Yeah. Unfortunately, it can never be this perfect, beautiful thing. Well, there are, there are occasions in which you get to build, like, you know, you invent a new data structure or something like that, or there's this beautiful algorithm that just like makes you super happy. I love that moment. But when you're working with people, you're working with code and dusty that code bases and things like this, right? It's not about what's theoretically beautiful, it's about what's practical, what's real, what people will actually use. And I don't meet a lot of people that say, I want to rewrite all my code just for the sake of it. By the way, there could be interesting possibilities and we'll probably talk about it where AI can help rewrite some code that might be farther out future, but it's a really interesting one, how that could create more, be a tool in the battle against this monster of complexity that you mentioned. You mentioned Guido, the benevolent dictator for life of Python. What does he think about Mojo? Have you talked to him much about it? I have talked with him about it. He found it very interesting. We actually talked with Guido before it launched. And so he was aware of it before it went public. I have a ton of respect for Guido for a bunch of different reasons. You talk about Waller's operator and Guido is pretty amazing in terms of steering such a huge and diverse community and driving it forward. And I think Python is what it is thanks to him. And so to me, it was really important starting to work on Mojo to get his feedback and get his input and get his eyes on this. Now, a lot of what Guido was and is, I think, and thought about is, have we not fragment the community? We don't want to Python 2 to Python 3 thing. That was really painful for everybody involved. And so we spent quite a bit of time talking about that and some of the tricks I learned from Swift, for example. So in the migration from Swift, we managed to not just convert Objective-C into a slightly prettier Objective-C, which we did. We then converted not entirely, but almost an entire community to a completely different language. And so there's a bunch of tricks that you learn along the way that are directly relevant to what we do. And so this is where, for example, you leverage C Python while bringing up the new thing. That approach is, I think, proven and comes from experience. And so Guido is very interested in, like, okay, cool. I think that Python is really his legacy. It's his baby. I have tons of respect for that. Incidentally, I see Mojo as a member of the Python family. We're not trying to take Python away from Guido and from the Python community. And so to me, it's really important that we're a good member of that community. And so I think that, again, you would have to ask Guido this, but I think that he was very interested in this notion of, like, cool, Python gets beaten up for being slow. So maybe there's a path out of that, right? And that, you know, if the future is Python, right? I mean, look at the far outside case on this, right? And I'm not saying this is Guido's perspective, but, you know, there's this path of saying, like, okay, well, suddenly Python can suddenly go all the places it's never been able to go before. Right. And that means that Python can go even further and can have even more impact on the world. So in some sense, Mojo could be seen as Python 4.0. I would not say that. I think that would drive a lot of people really crazy. Because of the PTSD of the 3.02. I'm willing to annoy people about Emax versus VIM or about text versus spaces. That's that one. I don't know. That might be a little bit far even for me. Like my skin may not be that thick. But the point is the step to being a super set and allowing all of these capabilities, I think, is the evolution of a language. It feels like an evolution of a language. So he he's interested by the ideas that you're playing with, but also concerned about the fragmentation. So how, what are the ideas you've learned? What are you thinking about? How do we avoid fragmenting the community? Where the the Pythonistas and the, I don't know what to call the Mojo people. Magicians. I like it. Can coexist happily and share code and basically just have these big code bases that are using C Python and more and more moving towards Mojo. Well, so again, these are lessons I learned from Swift. And here we face very similar problems, right? In Swift, you have Objective C, Super Dynamic. They're very different syntax, right? But you're talking to people who have large scale code bases. I mean, Apple's got the biggest, largest scale code base of Objective C code, right? And so, you know, none of the companies, none of the iOS developers, none of the other developers want to rewrite everything all at once. And so you want to be able to adopt things piece at a time. And so a thing that I found that worked very well in the Swift community was saying, okay, cool. And this is when Swift was very young. As you say, okay, you have a million line of code Objective C app. Don't rewrite it all. But when you implement a new feature, go implement that new class using Swift. Right. And so now this turns out is a very wonderful thing for an app developer. But it's a huge challenge for this compiler team and the systems people that are implementing. That's right. And this comes back to what is this trade off between doing the hard thing that enables scale versus doing the theoretically pure and ideal thing, right? And so Swift had adopted and built a lot of different machinery to deeply integrate with the Objective C runtime. And we're doing the same thing with Python, right? Now, what happened in the case of Swift is that Swift's language got more and more mature over time, right? And incidentally, Mojo is a much simpler language than Swift in many ways. And so I think that Mojo will develop way faster than Swift for a variety of reasons. But as the language gets more mature and parallel with that, you have new people starting new projects, right? And so when the language is mature and somebody is starting a new project, that's when they say, okay, cool, I'm not dealing with a million lines of code. I'll just start and use the new thing for my whole stack. Now, the problem is, again, you come back to where communities and where people that work together, you build new subsystem or new feature or new thing in Swift or you build new thing in Mojo, then you want to be end up being used on the other side, right? And so then you need to work on integration back the other way. And so it's not just Mojo talking to Python, it's also Python talking to Mojo, right? And so what I would love to see, and I don't want to see this next month, right? But what I want to see over the course of time is I would love to see people that are building these packages, like NumPy or TensorFlow or these packages that are half Python, half C++. And if you say, okay, cool, I want to get out of this Python C++ world into a unified world, and so I can move to Mojo, but I can't give up all my Python clients because these libraries get used by everybody and they're not all going to switch all once and maybe never, right? Well, so the way we should do that is we should vend Python interfaces to the Mojo types. And that's what we did in Swift and worked great. I mean, it was a huge implementation challenge for the compiler people, right? But there's only a dozen of those compiler people and there are millions of users. And so it's a very expensive, capital intensive, like skill set intensive problem. But once you solve that problem, it really helps adoption and really helps the community progressively adopt technologies. And so I think that this approach will work quite well with the Python and the Mojo world. So for a package ported to Mojo and then create a Python interface? Yep. So how do you just to link on these packages, NumPy, PyTorch, and TensorFlow? Yeah. How do they play nicely together? So is Mojo supposed to be, let's talk about the machine learning ones. Is Mojo kind of vision to replace PyTorch and TensorFlow to incorporate it? What's the relationship in this? All right. So let's dance. So take a step back. So I wear many hats. So you're angling it on the Mojo side. Mojo is a programming language. And so it can help solve the C++ Python feud that's happening. The fire Mojo got me. I'm sorry. We should be talking modular. Yes. Yes. Okay. So the fire emoji is amazing. I love it. It's a big deal. The other side of this is the fire emoji is in service of solving some big AI problems. And so the big AI problems are again, this fragmentation, this hardware nightmare, this explosion of new potential, but that's not getting felt by the industry. And so when you look at how does the modular engine help TensorFlow and PyTorch, it's not replacing them. In fact, when I talk to people, again, they don't like to rewrite all their code, you have people that are using a bunch of PyTorch, a bunch of TensorFlow. They have models that they've been building over the course of many years. And when I talk to them, there's a few exceptions, but generally they don't want to rewrite all their code. And so what we're doing is we're saying, okay, well, you don't have to rewrite all your code. What happens is the modular engine goes in there and goes underneath TensorFlow and PyTorch. It's fully compatible. And it just provides better performance, better predictability, better tooling. It's a better experience that helps lift TensorFlow and PyTorch and make them even better. I love Python. I love TensorFlow. I love PyTorch, right? This is about making the world better, because we need AI to go further. But if I have a process that trains a model and I have a process that performs inference on that model, and I have the model itself, what should I do with that in the long arc of history in terms of if I use PyTorch to train it? Should I rewrite stuff in Mojo? Would that, if I care about performance? Oh, so I mean, again, it depends. So if you care about performance, then writing in Mojo is going to be way better than writing in Python. But if you look at LLM companies, for example, if you look at OpenAI, rumored, and you look at many of the other folks that are working on many of these LLMs and other innovative machine learning models, on the one hand, they're innovating in the data collection and the model billions of parameters and the model architecture and the RLE or HF and the like all the all the cool things that people are talking about. But on the other hand, they're spending a lot of time writing CUDA curls, right? And so you say, wait a second, how much faster could all this progress go if they were not having to handwrite all these CUDA curls? Right. And so there are a few technologies that are out there and people have been working on this problem for a while. And they're trying to solve subsets to the problem again, kind of fragmenting the space. And so what Mojo provides for these kinds of companies is the ability to say, cool, I can have a unifying theory. Right. And again, the better together the unifying theory, the the two world problem or the three world problem or the n world problem, like this is the thing that is slowing people down. And so as we help solve this problem, I think it'll be very helpful for making this whole cycle go faster. So obviously, we talked about the transition from Objective C to Swift, if design this programming language. And you've also talked quite a bit about the use of Swift for machine learning context. Why have you decided to move away from maybe an intense focus on Swift for the machine learning context versus sort of designing a new programming language that happens to be a super surprise. You're saying this is an irrational set of life choices I make? Did you go to the desert? And did you meditate on it? Okay. All right. No, it was bold and needed. And I think, I mean, it's just bold and sometimes to take those leaps is a difficult leap to take. Yeah. Well, so okay, I mean, I think there's a couple of different things. So actually, I left Apple back in 2017, like January 2017. So it's been a number of years that I left Apple. And the reason I left Apple was to do AI. Okay. So and again, I won't come on Apple and AI, but at the time, right, I wanted to get into and understand and understand the technology, understand the applications, the workloads. And so, okay, I'm going to go dive deep into applied and AI and then the technology underneath it. Right. I found myself at Google. And that was like when TPUs were waking up. Exactly. And so I found myself at Google and Jeff Dean, who's a rock star, as you know, right? And in 2017, TensorFlow is like really taking off and doing incredible things. And I was attracted to Google to help them with the TPUs, right? And TPUs are an innovative hardware accelerator platform have now, I mean, I think proven massive scale and like done incredible things, right? And so one of the things that this led into is a bunch of different projects, which I'll skip over, right? One of which was this Swift for TensorFlow project, right? And so that project was a research project. And so the idea of that is say, okay, well, let's look at innovative new programming models, where we can get a fast programming language, we can get automatic differentiation into language, let's push the boundaries of these things in a research setting, right? Now, that project, I think lasted two, three years. There's some really cool outcomes of that. So one of the things that's really interesting is I published a talk at an LLVM conference in 2018. And this seems like so long ago about graph program abstraction, which is basically the thing that's in PyTorch 2. And so PyTorch 2 with all this Dynamo real thing, it's all about this graph program abstraction thing from Python bytecodes. And so a lot of the research that was done ended up pursuing and going out through the industry and influencing things. And I think it's super exciting and awesome to see that. But the Swift for TensorFlow project itself did not work out super well. And so there's a couple of different problems with that, one of which is that you may have noticed Swift is not Python. There's a few people that write Python code. Yes. And so it turns out that all of ML is pretty happy with Python. It's actually a problem that other programming languages have as well, that they're not Python. We'll probably maybe briefly talk about Julia, who's a very interesting, beautiful programming language, but it's not Python. Exactly. Well, and so like if you're saying, I'm going to solve a machine learning problem where all the programmers are Python programmers. And you say the first thing you have to do is switch to a different language. Well, your new thing may be good or bad or whatever, but if it's a new thing, the adoption barrier is massive. It's still possible. Still possible. Yeah, absolutely. The world changes and evolves. And there's definitely room for new and good ideas, but it just makes it so much harder. And so lesson learned, Swift is not Python. And people are not always in search of learning a new thing for the sake of learning a new thing. And if you want to be compatible with all the world's code, it turns out, meet the world where it is. Right. Second thing is that, you know, a lesson learned is that Swift as a very fast and efficient language, kind of like Mojo, but a different take on it still really worked well with eager mode. And so eager mode is something that PyTorch does, and it proved out really well, and it enables really expressive and dynamic and easy to debug programming. TensorFlow at the time was not set up for that. Let's say that was not the timing is also important in this world. Yeah. Yeah. And TensorFlow is a good thing and it has many, many strengths, but you could say Swift for TensorFlow is a good idea, except for the Swift and except for the TensorFlow part. So because it's not Python and TensorFlow because it's not wasn't set up for eager mode at the time. Yeah. That is 1.0. Exactly. And so one of the things about that is in the context of it being a research project, I'm very happy with the fact that we built a lot of really cool technology. We learned a lot of things. I think the ideas went on to have influence and other systems like PyTorch. A few people use that right here. Right. And so I think that's super cool. And for me personally, I learned so much from it. Right. And I think a lot of the engineers that worked on it also learned a tremendous amount. And so, you know, I think that that's just really exciting to see. And, you know, I'm sorry that the project didn't work out. I wish it did, of course. Right. But, you know, it's a research project and so you're there to learn from it. Well, it's interesting to think about the evolution of programming as we come up with these whole new set of algorithms in machine learning and artificial intelligence and what's going to win out. Because it could be a new programming language. Yeah. It could be, I mean, I just mentioned Julia. I think there's a lot of ideas behind Julia that Mojo shares. What are your thoughts about Julia in general? So, I will have to say that when we launched Mojo, one of the biggest things I didn't predict was the response from the Julia community. And so, I was not, I mean, I've, okay, let me take a step back. I've known the Julia folks for a really long time. They were an adopter of LLVM a long time ago. They've been pushing state of the art in a bunch of different ways. Julia is a really cool system. I had always thought of Julia as being mostly a scientific computing focused environment. And I thought that was its focus. I neglected to understand that one of their missions is to like help make Python work end to end. And so, I think that was my error for not understanding that. And so, I could have been maybe more sensitive to that. But there's major differences between what Mojo is doing and what Julia is doing. So, as you say, Julia is not Python. And so, one of the things that a lot of the Julia people came out and said is like, okay, well, if we put a ton of more energy and ton more money or engineering or whatever into Julia, maybe that would be better than starting Mojo, right? Well, I mean, maybe that's true, but it still wouldn't make Julia into Python. So, if you've worked backwards from the goal of let's build something for Python programmers without requiring them to relearn syntax, then Julia just isn't there, right? I mean, that's a different thing, right? And so, if you anchor on, I love Julia and I want Julia to go further, then you can look at it from a different lens. But the lens we were coming at was, hey, everybody is using Python. Python isn't, syntax isn't broken. Let's take what's great about Python and make it even better. And so, it's just a different starting point. So, I think Julia is a great language. The community is a lovely community. They're doing really cool stuff, but it's just a slightly different angle. But it does seem that Python is quite sticky. Is there some philosophical almost thing you could say about why Python, by many measures, seems to be the most popular programming language in the world? Well, I can tell you things I love about it. Maybe that's one way to answer the question, right? So, huge package ecosystem. Super lightweight and easy to integrate. It has very low startup time. So, what startup time? You mean like learning curve or what? Yeah, so, if you look at certain other languages, you say like, go. And it just takes a, like Java, for example, takes a long time to compile all the things. And then the VM starts up and the garbage clusters kicks in and then it revs its engines and then it can plow through a lot of internet stuff or whatever, right? Python is like scripting. It just goes, right? Python has very low compile time. So, you're not sitting there waiting. Python integrates into notebooks in a very elegant way that makes exploration super interactive and it's awesome, right? Python is also it's like almost the glue of computing because it has such a simple object representation, a lot of things plug into it. That dynamic metaprogramming thing we were talking about also enables really expressive and beautiful APIs, right? So, there's lots of reasons that you can look at technical things that Python has done and say like, okay, wow, this is actually a pretty amazing thing and any one of those you can neglect. People all just talk about indentation and ignore like the fundamental things. But then you also look at the community side, right? So, Python owns machine learning. Machine learning is pretty big. Yeah, and it's growing. It's growing, right? And it's growing in importance, right? And so, and there's a reputation of prestige to machine learning to where like, if you're a new programmer, you're thinking about like, which programming language do I use? Well, I should probably care about machine learning. Therefore, let me try Python and kind of builds and builds and builds. And you can go back before that, like my kids learn Python, right? Not because I'm telling them to learn Python, but because what they were buying against you or what? Well, no, right? Well, they also learned Scratch, right? And things like this too. But it's because Python is taught everywhere, right? Because it's easy to learn, right? And because it's pervasive, right? Back to my day, we learned Java and C++. I'll pale both directions. But yes, I guess Python is the main language of teaching software engineering in schools now. Yeah. Well, and if you look at this, there's these growth cycles, right? If you look at what causes things to become popular, and then gain in popularity, there's reinforcing feedback loops and things like this. And I think Python has done, again, the whole community has done a really good job of building those growth loops and help propel the ecosystem. And I think that again, you look at what you can get done with just a few lines of code, it's amazing. So this kind of self building loop, it's interesting to understand because when you look at Mojo, what it stands for some of the features, it seems sort of clear that this is a good direction for programming languages to evolve in the machine learning community. But it's still not obvious that it will, because of this, whatever the engine of popularity, of virality, is there something you could speak to like how, how do you get people to switch? Yeah, well, I mean, I think that the viral growth loop is to switch people to Unicode. I think the Unicode file extensions are what I'm betting on. I think that's going to be the thing. Tell the kids that you could use the fire emoji and they'd be like, what? Exactly. Well, in all seriousness, I mean, I think there's really, I'll give you two opposite answers. One is, I hope if it's useful, if it solves problems and people care about those problems being solved, they'll adopt the tech. That's kind of the simple answer. And when you're looking to get tech adopted, the question is, is it solving an important problem people need solved? And is the adoption cost low enough that they're willing to make the switch and cut over and do the pain up front so that they can actually do it? And so hopefully, Mojo will be that for a bunch of people and people building these hybrid packages are suffering. It's really painful. And so I think that we have a good shot of helping people. But the other side is, it's okay if people don't use Mojo. It's not my job to say, everybody should do this. I'm not saying Python is bad. I hope Python, see Python, like all these implementations, because Python ecosystem is not just see Python. It's also a bunch of different implementations with different tradeoffs. And this ecosystem is really powerful and exciting, as are other programming languages. It's not like TypeScript or something is going to go away. And so there's not a winner take all thing. And so I hope that Mojo is exciting and useful to people. But if it's not, that's also fine. But I also wonder what the use case for why you should try Mojo would be. So practically speaking. It seems like, so there's entertainment. There's a dopamine hit of saying, holy shit, this is 10 times faster. This little piece of code is 10 times faster in Mojo. Box before he gets to 35,000. Exactly. I mean, just even that, I mean, that's the dopamine hit that every programmer sort of dreams of is the optimization. It's also the drug that can pull you in and have you waste way too much of your life optimizing and over optimizing, right? But so what would you see it would be like comedy. It's very hard to predict, of course. But if you look 10 years from now, Mojo is super successful. What do you think would be the thing where people try it and then use it regularly and it kind of grows and grows and grows and grows? So you talk about dopamine hit. And so again, humans are not one thing. And some people love rewriting their code and learning new things and throwing themselves in the deep end and trying out a new thing. In my experience, most people don't. Like, they're too busy. They have other things going on. By number, most people don't like this. I want to rewrite all my code. But even those people, the two busy people, the people that don't actually care about the language that just care about getting stuff done, those people do like learning new things, right? And so you talk about the dopamine rush of 10x faster. Wow, that's cool. I want to do that again. Well, it's also like, here's a thing I've heard about in a different domain. And I don't have to rewrite all my code. I can learn a new trick, right? Well, that's called growth. And so one thing that I think is cool about Mojo, and again, those will take a little bit of time for, for example, the blog posts and the books and like all that kind of stuff to develop and the language needs to get further along. But what we're doing, you talk about types, like you can say, look, you can start with the world you already know, and you can progressively learn new things and adopt them where it makes sense. If you never do that, that's cool. You're not a bad person. If you, if you get really excited about it, want to go all the way in the deep end and want to rewrite everything and like whatever, that's cool, right? But I think the middle path is actually the more likely one where it's, you know, you come out with a new, a new idea and you discover, wow, that makes my code way simpler, way more beautiful, way faster, way whatever. And I think that's what people like. Now, if you fast forward and you said like 10 years out, right? I can give you a very different answer on that, which is, I mean, if you go back and look at what computers looked like 20 years ago, every 18 months, they got faster for free, right? 2x faster every 18 months, it was like clockwork, it was, it was free, right? You go back 10 years ago, and we entered in this world where suddenly we had multicore CPUs and we had GPUs. And if you squint and turn your head, what are GPUs? It's just a many core, very simple CPU thing kind of, right? And so, and 10 years ago, it was CPUs and GPUs and graphics. Today, we have CPUs, GPUs, graphics, and AI, because it's so important because the compute is so demanding because of the smart cameras and the watches and all the different places that AI needs to work in our lives, it's caused this explosion of hardware. And so, part of my thesis, part of my belief of where computing goes, if you look at 10 years from now, is it's not going to get simpler. Physics isn't going back to where we came from. It's only going to get weirder from here on out, right? And so, to me, the exciting part about what we're building is it's about building that universal platform, which the world can continue to get weird, because again, I don't think it's avoidable, it's physics, but we can help lift people scale, do things with it, and they don't have to rewrite their code every time a new device comes out. And I think that's pretty cool. And so, if Mojo can help with that problem, then I think that it will be hopefully quite interesting and quite useful to a wide range of people because there's so much potential and like there's so, you know, maybe analog computers will become a thing or something, right? And we need to be able to get into a mode where we can move this programming model forward, but do so in a way where we're lifting people and growing them instead of forcing them to rewrite all their code and exploding them. Do you think there'll be a few major libraries that go Mojo first? Well, so, I mean, the modular engine is all Mojo. So, again, come back to like, we're not building Mojo because it's fun, we're building Mojo because we had to dissolve these accelerators. That's the origin story. But I mean, ones that are currently in Python. Yeah. So, I think that a number of these projects will. And so, one of the things, again, this is just my best guess. Like, each of the package maintainers also has, I'm sure, plenty of other things going on. People don't like really don't like rewriting code just for the sake of rewriting code. But sometimes, like, people are excited about like adopting a new idea. Yeah. And it turns out that while rewriting code is generally not people's first thing, turns out that redesigning something while you rewrite it and using a rewrite as an excuse to redesign can lead to the 2.0 of your thing that's way better than the 1.0. And so, I have no idea. I can't predict that. But there's a lot of these places where, again, if you have a package that is half C and half Python, right, you just solve the pain, make it easier to move things faster, make it easier to debug and evolve your tech. Adopting Mojo kind of makes sense to start with. And then it gives you this opportunity to rethink these things. So the two big gains are that there's a performance gain and then there's the portability to all kinds of different devices. And there's safety, right? So you talk about real types. I mean, not saying this is for everybody, but that's actually a pretty big thing, right? And so there's a bunch of different aspects of what value Mojo provides. And so, I mean, it's funny for me. I've been working on these kinds of technologies and tools for too many years now. But you look at Swift, right? And we talked about Swift for TensorFlow, but Swift as a programming language, right? Swift's now 13 years old from when I started it. So, because I started in 2010, if I remember. And so, that project, and I was involved with it for 12 years or something, right? That project has gone through its own really interesting story arc, right? And it's a mature, successful, used by millions of people's system, right? Certainly not dead yet, right? But also, going through that story arc, I learned a tremendous amount about building languages, about building compilers, about working with community, and things like this. And so, that experience, like I'm helping channel and bring directly in Mojo. And other systems, same thing. Apparently, I like building and iterating and evolving things. And so, you look at this LLVM thing I worked on 20 years ago, and you look at MLIR, right? And so, a lot of the lessons learned in LLVM got fed into MLIR. And I think that MLIR is a way better system than LLVM was. And Swift is a really good system. And it's amazing. But I hope that Mojo will take the next step forward in terms of design. In terms of running Mojo, people can play with it. What's Mojo Playground? Yeah. And from the interface perspective, and from the hardware perspective, what's this incredible thing running on? Yeah. So, right now, so here we are two weeks after launch. We decided that, okay, we have this incredible set of technology that we think might be good, but we have not given it to lots of people yet. And so, we were very conservative and said, let's put it in a workbook so that if it crashes, we can do something about it. We can monitor and track that. Right. And so, again, things are still super early, but we're having one person a minute sign up with over 70,000 people two weeks in. It's kind of crazy. So, you can sign up to Mojo Playground and you can use it in the cloud. Yeah. In your browser. And so, what that's running on a workbook? Yeah. What that's running on is, that's running on cloud VMs. And so, you share a machine with a bunch of other people, but it turns out there's a bunch of them now because there's a lot of people. And so, what you're doing is you're getting free compute and you're getting to play with this thing in kind of a limited controlled way so that we can make sure that it doesn't totally crash and be embarrassing. Right? Yeah. So, now a lot of the feedback we've gotten is people want to download it around locally. So, we're working on that right now. So, that's the goal to be able to download locally? Yeah. Yeah. That's what everybody expects. And so, we're working on that right now. And so, we just want to make sure that we do it right. And I think this is one of the lessons I learned from Swift also, by the way. Is it when we launched Swift? Gosh, it feels like forever ago. It was 2014. And we, I mean, it was super exciting. I and we, the team had worked on Swift for a number of years in secrecy. Okay. And we, four years into this development, roughly, of working on this thing, at that point, about 250 people at Apple knew about it. Okay. So, it was secret. Apple's good at secrecy. And it was a secret project. And so, we launched this at WWDC, a bunch of hoopla and excitement, and said, developers, you're going to be able to develop and submit apps to the App Store in three months. Okay. Well, several interesting things happened, right? So, first of all, we learned that, A, it had a lot of bugs. It was not actually production quality. And it was extremely stressful in terms of like, trying to get it working for a bunch of people. And so, what happened was we went from zero to, you know, I don't know how many developers, Apple had at the time, but a lot of developers overnight, and they ran into a lot of bugs, and it was really embarrassing. And it was very stressful for everybody involved, right? It was also very exciting because everybody was excited about that. The other thing I learned is that, when that happened, roughly every software engineer who did not know about the project at Apple, their head exploded when it was launched, because they didn't know it was coming. And so, they're like, wait, what is this? I signed up to work for Apple because I love Objective C. Why is there a new thing, right? And so, now, what that meant practically is that the push from launch to, first of all, the fall, but then to 2.0 and 3.0 and like, all the way forward was super painful for the engineering team and myself. It was very stressful. The developer community was very grumpy about it because they're like, okay, well, wait a second, you're changing and breaking my code and we have to fix the bugs. And it was just a lot of tension and friction on all sides. There's a lot of technical debt in the compiler because we have to run really fast. You have to go implement the thing and unblock the use case and do the thing. And you know it's not right, but you never have time to go back and do it, right? And I'm very proud of the Swift team because they've come, I mean, we, but they came so far and made so much progress over this time since launch. It's pretty incredible and Swift is a very, very good thing, but I just don't want to do that again, right? And so, iterate more through the development process. And so, what we're doing is we're not launching it when it's hopefully 0.9 with no testers. We're launching it and saying it's 0.1, right? And so, we're saying expectations of saying like, okay, well, don't use this for production, right? If you're interested in what we're doing, we'll do it in an open way and we can do it together, but don't use it in production yet. Like, we'll get there, but let's do it the right way. And I'm also saying we're not in a race. The thing that I want to do is build the world's best thing, right? Because if you do it right and it lifts the industry, it doesn't matter if it takes an extra two months. Like, two months is worth waiting. And so, doing it right and not being overwhelmed with technical debt and things like this is like, again, war wounds, lessons learned, whatever you want to say, I think is absolutely the right thing to do. Even though, right now, people are very frustrated that you can't download it or it doesn't have feature X or something like this. What have you learned in a little bit of time since it's been released into the wild that people have been complaining about feature X or Y or Z? What have they been complaining about? What have they been excited about? Like, almost like detailed things versus a big vision. I think everyone would be very excited about the big vision. Yeah. Yeah. Well, so, I mean, I've been very pleased. In fact, I mean, we've been massively overwhelmed with response, which is a good problem to have. It's kind of like a success disaster in a sense, right? And so, I mean, if you go back in time, when we started Modular, which is just not yet a year and a half ago, so it's still a pretty new company, new team, small but very good team of people, like we started with extreme conviction that there's a set of problems that we need to solve. And if we solve it, then people will be interested in what we're doing, right? But again, you're building in basically secret, right? You're trying to figure it out. Creation is a messy process. You're having to go through different paths and understand what you want to do and how to explain it. Often, when you're doing disruptive and new kinds of things, just knowing how to explain it is super difficult, right? And so, when we launched, we hoped people would be excited. But, you know, I'm an optimist, but I'm also like, don't want to get ahead of myself. And so, when people found out about Mojo, I think their heads exploded a little bit, right? And, you know, here's, I think, a pretty credible team that has built some languages and some tools before. And so, they have some lessons learned and are tackling some of the deep problems in the Python ecosystem and giving it the love and attention that it should be getting. And I think people got very excited about that. And so, if you look at that, I mean, I think people are excited about ownership and taking a step beyond Rust, right? And there's people that are very excited about that. And there's people that are excited about, you know, just like, I made Game of Life go 400 times faster, right? And things like that. And that's really cool. There are people that are really excited about the, okay, I really hate writing stuff in C++. Save me. Well, like systems in your, they're like stepping up like, oh, yes. So, that's me, by the way. Also, I really want to stop writing C++. But the, I get third-person excitement when people tweet, hey, I made this code, Game of Life, or whatever, faster. And you're like, yeah. Yeah. And also, like, I would also say that, let me cast blame out to people who deserve it. Sure. These terrible people who convinced me to do some of this. Yes. Jeremy Howard. Yes. That guy. Well, he's been pushing for this kind of thing. He's been pushing for more years. Yeah, he's wanted this for a long, long time. He's wanted this for years. For people who don't know Jeremy Howard, he's like one of the most legit people in the machine learning community. He's a grassroots. He really teaches, he's an incredible educator, he's an incredible teacher, but also legit in terms of a machine learning engineer himself. He's been running the fast.ai and looking, I think, for exactly what you've done. Exactly. And so, I mean, the first time, so I met Jeremy pretty early on, but the first time I sat up and I'm like, this guy is ridiculous is when I was at Google and we were bringing up TPUs and we had a whole team of people and where there was this competition called Don Bench of who can train ImageNet fastest. And Jeremy and one of his researchers crushed Google by not through sheer force of the amazing amount of compute and the number of TPUs and stuff like that, that he just decided that progressive imagery sizing was the right way to train the model and if you were epochs faster and make the whole thing go go vroom, right? And I'm like, this guy is incredible. So you can say, anyways, come back to where's Mojo coming from. Chris finally listened to Jeremy. It's all his fault. But there's a kind of very refreshing, pragmatic view that he has about machine learning that I don't know if it's this mix of a desire for efficiency, but ultimately grounded and desired to make machine learning more accessible to a lot of people. I don't know what that is. I guess that's coupled with efficiency and performance, but it's not just obsessed about performance. So a lot of AI and AI research ends up being that it has to go fast enough to get scale. So a lot of people don't actually care about performance, particularly on the research side, until it allows them to have a bigger data set, right? And so suddenly now you care about distributed compute and like all these exotic HPC, like you don't actually want to know about that. You just want to be able to do more experiments faster and do so with bigger data sets, right? And so Jeremy has been really pushing the limits. And one of the things I'll say about Jeremy, and there's many things I could say about Jeremy, because I'm a fanboy of his, but he fits in his head. And Jeremy actually takes the time where many people don't to really dive deep into why is the beta parameter of the atom optimizer equal to this, right? And he'll go survey and understand what are all the activation functions in the trade-offs and why is it that everybody that does this model pick that thing. So the why, not just trying different values, like really what is going on here. Right. And so as a consequence of that, he's always, again, he makes time, but he spends time to understand things at a depth that a lot of people don't. And as you say, he then brings it and teaches people. And his mission is to help lift, his website says, making AI uncool again. Like, it's about, like, forget about the hype, it's actually practical and useful. Let's teach people how to do this, right? Now, the problem Jeremy struggled with is that he's pushing the envelope, right? Research isn't about doing the thing that is staying on the happy path or the well-paid road, right? And so a lot of the systems today have been these really fragile, fragmented things or special case in this happy path. And if you fall off the happy path, you get eaten by an alligator. So what about, so Python has this giant ecosystem of packages and is a package repository. Do you have ideas of how to do that well for Mojo? Yeah. How to do a repository of packages? Well, so that's another really interesting problem that I knew about, but I didn't understand how big of a problem it was. Python packaging, a lot of people have very big pain points and a lot of scars with Python packaging. Oh, you mean, so there's several things to say. Building and distributing and managing dependencies and versioning and all this stuff. So from the perspective of if you want to create your own package. Yes. And then, or you want to build on top of a bunch of other people's packages and then they get updated and things like this. Now, I'm not an expert in this, so I don't know the answer. I think this is one of the reasons why it's great that we work as a team and there's other really good and smart people involved. But one of the things I've heard from smart people who've done a lot of this is that the packaging becomes a huge disaster when you get the Python and C together. And so if you have this problem where you have code split between Python and C, now not only do you have to package the C code, you have to build the C code. C doesn't have a package manager, right? C doesn't have a dependency versioning management system, right? And so I'm not experienced in the state of the art and all the different Python package managers, but my understanding is that's a massive part of the problem. And I think Mojo solves that part of the problem directly heads on. Now, one of the things I think we'll do with the community, and this isn't, again, we're not solving all the world's problems at once, we have to be kind of focused to start with, is that I think that we will have an opportunity to reevaluate packaging, right? And so I think that we can come back and say, okay, well, given the new tools and technologies and the cool things we have that we've built up, because we have not just syntax, we have an entirely new compiler stack that works in a new way, maybe there's other innovations we can bring together and maybe we can help solve that problem. So almost that tangent to that question from the user perspective of packages, it was always surprising to me that it was not easier to sort of explore and find packages. With PIP install, it feels, it's an incredible ecosystem. It's just interesting that it wasn't made, it's still, I think, not made easier to discover packages to do, like search and discovery, as YouTube calls it. Well, I mean, it's kind of funny because this is one of the challenges of these intentionally decentralized communities. And so I don't know what the right answer is for Python. I mean, there are many people that were, I don't even know the right answer for Mojo. So there are many people that would have much more informed opinions than I do, but it's interesting if you look at this, right? Open source communities, you know, there's Git, Git is a fully decentralized, and they can do it any way they want, but then there's GitHub, right? And GitHub centralized, commercial in that case, right? Thing, really help pull together and help solve some of the discovery problems and help build a more consistent community. And so maybe there's opportunities for something like a GitHub. Although even GitHub, I might be wrong on this, but the search and discovery for GitHub is not that great. Like I still use Google search. Yeah, well, I mean, maybe that's because GitHub doesn't want to replace Google search, right? And I think there is room for specialized solutions to specific problems. I don't know, I don't know the right answer for GitHub either. That's, they can go figure that out. But the point is to have an interface that's usable, that's accessible to people of all different skill levels and so on. Well, and again, like what are the benefit of standards, right? Standards allow you to build these next level up ecosystem, the next level up infrastructure, the next level up things. And so again, come back to, I hate complexity. C plus Python is complicated. It makes everything more difficult to deal with. It makes it difficult to port, move code around, work with. All these things get more complicated. And so, I mean, I'm not an expert, but maybe Mojo can help a little bit by helping reduce the amount of C in this ecosystem and make it therefore scale better. So when you kind of package this, the hybrid in nature would be a natural fit to move to Mojo. Which is a lot of them, by the way. A lot of them, especially they're doing some interesting stuff computation-wise. Let me ask you about some features. Yeah. So we talked about, obviously, the indentation that it's the type language or optionally typed. Is that the right way to say it? It's either optionally or progressively. Progressively. I think, so people have very strong opinions on the right word to use. I don't know. I look forward to your letters. So there's the var versus let. But let is for constants. Var is an optional. Yeah, var makes it mutable, so you can reassign. Okay, then there's function overloading. Oh, okay, yeah. I mean, there's a lot of source of happiness for me, but function overloading that's, I guess, is that for performance? Or is that, why does Python not have function overloading? So I can speculate. So Python is a dynamic language. The way it works is that Python and Objective-C are actually very similar worlds if you ignore syntax. And so Objective-C is straight line derived from Smalltalk, a really venerable, interesting language that much of the world has forgotten about, but the people that remember it, love it, generally. And the way that Smalltalk works is that every object has a dictionary in it, and the dictionary maps from the name of a function or the name of a value within an object to its implementation. And so the way you call a method in Objective-C is you say, go look up, the way I call foo is I go look up foo, I get a pointer to the function back, and then I call it. Okay, that's how Python works. Right, and so now the problem with that is that the dictionary within a Python object, all the keys are strings, and it's a dictionary, so you can only have one entry per name. You think it's as simple as that? I think it's as simple as that. And so now, why do they never fix this? Like, why do they not change it to not be a dictionary? Like, do other things. Well, you don't really have to in Python because it's dynamic. And so you can say, I get into the function, now if I got past an integer, do some dynamic test for it, if it's a string, go do another thing. There's another additional challenge, which is, even if you did support overloading, you're saying, okay, well, here's a version of a function for integers and a function for strings. Well, you'd have, even if you could put it in that dictionary, you'd have to have the caller do the dispatch. And so every time you call the function, you'd have to say, like, is an integer a string? And so you'd have to figure out where to do that test. And so in a dynamic language, overloading is something you don't have to have. But now you get into a typed language. And in Python, if you subscript with an integer, then you get typically one element out of a collection. If you subscript with a range, you get a different thing out. Right? And so often in typed languages, you'll want to be able to express the fact that, cool, I have different behavior depending on what I actually pass into this thing. If you can model that, it can make it safer and more predictable and faster and like all these things. It somehow feels safe for yes, but also feels empowering. Make it in terms of clarity, like you don't have to design whole different functions. Yeah. Well, this is also one of the challenges with the existing Python typing systems, is that in practice, like you take subscript, in practice, a lot of these functions, they don't have one signature. They actually have different behavior in different cases. And so this is why it's difficult to retrofit this into existing Python code and make it play well with typing. You kind of have to design for that. Okay. So there's an interesting distinction that people that program Python might be interested in is def versus fn. So it's two different ways to define a function. And fn is a stricter version of def. What's the coolness that comes from the strictness? So here you get into what is the trade off with the superset? Yes. So superset, you have to, or you really want to be compatible. If you're doing a superset, you've decided compatibility with existing code is the important thing, even if some of the decisions they made were maybe not what you'd choose. Okay. So that means you put a lot of time in compatibility, and it means that you get locked into decisions of the past, even if they may not have been a good thing. Now, systems programmers typically like to control things. And they want to make sure that, not in all cases, of course, and even systems programmers are not one thing, but often you want predictability. And so one of the things that Python has, for example, as you know, is that if you define a variable, you just say x equals four. I have a variable name to x. Now I say some long, some long name equals 17. Print out some long name. Oops, I typoed it. Right. Well, the compiler, the Python compiler doesn't know, in all cases, what you're defining and what you're using. And did you typo the use of it or the definition? Right. And so for people coming from type languages, again, I'm not saying the right or wrong, but that drives them crazy because they want the compiler to tell them you typoed the name of this thing. Right. And so what fn does is it turns on, as you say, it's a strict mode. And so it says, okay, well, you have to actually declare, intentionally declare your variables before you use them. That gives you more predictability, more error checking and things like this. But you don't have to use it. And this is a way that Mojo is both compatible because devs work the same way that devs have already always worked. But it provides a new alternative that gives you more control and allows certain kinds of people to have a different philosophy to be able to express that and get that. But usually, if you're writing Mojo code from scratch, you'll be using fn. It depends. Again, it depends on your mentality. It's not that dev is Python and fn is Mojo. Mojo has both and it loves both. It really depends on... Time is just strict. Yeah, exactly. Are you playing around and scripting something out? Is it a one off throwaway script? Cool. Python is great at that. I will still be using fn, but yeah. I love strictness. Control, power. You also like suffering, right? Yes. You go hand in hand. How many pull-ups? I've lost count at this point. And that's cool. I love you for that. And I love other people who like strict things, right? But I don't want to say that that's the right thing because Python is also very beautiful for hacking around and doing stuff and research and these other cases where you may not want that. You see, I just feel like... Maybe I'm wrong with that, but it feels like strictness leads to faster debugging. So in terms of going from... Even on a small project from zero to completion, it just... I guess it depends how many bugs you generate usually. Well, so I mean, it's again lessons learned and looking at the ecosystem. It's really... I mean, I think it's... If you study some of these languages over time, like the Ruby community, for example. Now, Ruby is a pretty well-developed, pretty established community, but along their path, they really invested in unit testing. So I think that the Ruby community is really pushed forward the state-of-the-art of testing because they didn't have a type system that caught a lot of bugs at compile time. And so you can have the best of both worlds. You can have good testing and good types and things like this. But I thought that it was really interesting to see how certain challenges get solved. And in Python, for example, the interactive notebook kind of experiences and stuff like this are really amazing. And if you typo something, it doesn't matter. It just tells you. That's fine, right? And so I think that the tryouts are very different if you're building a large-scale production system versus you're building and exploring in a notebook. And speaking of control, the hilarious thing, if you look at code, I write just for myself for fun. It's like littered with asserts everywhere. It's a kind of... Yeah, you'd like that. Yes. It's basically saying in a dictatorial way, this should be true now. Otherwise, everything stops. And that is the sign. I can't... I love you, man. But that is the sign of somebody who likes control. And so, yes, I think that you'll like... And I think you'll like mojo. Therapy session. Yes, I definitely will. Speaking of asserts, exceptions are called errors. Why is it called errors? So, I mean, we use the same... We're the same as Python, right? But we implement it a very different way. And so, if you look at other languages, like we'll pick on C++, our favorite, right? C++ has a thing called zero-cost exception handling. Okay. And this is, in my opinion, something to learn lessons from. It's a nice polite way of saying it. And so, zero-cost exception handling, the way it works is that it's called zero-cost because if you don't throw an exception, there's supposed to be no overhead for the non-error code. And so, it takes the error path out of the common path. It does this by making throwing an error extremely expensive. And so, if you actually throw an error with a C++ compiler using exceptions, let's go look up in tables on the side and do all this stuff. And so, throwing an error could be like 10,000 times more expensive than returning from a function, right? Also, it's called zero-cost exceptions, but it's not zero-cost. By any stretch of the imagination, because it massively blows out your code, your binary, it also adds a whole bunch of different paths because of destructors and other things like that that exist in C++. And it reduces the number of optimizations. It adds like all these effects. And so, this thing that was called zero-cost exceptions, it really ain't. Okay. Now, if you fast forward to newer languages, and this includes Swift and Rust and Go and now Mojo, well, and Python's a little bit different because it's interpreted. And so, it's got a little bit of a different thing going on. But if you look at compiled languages, many newer languages say, okay, well, let's not do that zero-cost exception handling thing. Let's actually treat throwing an error the same as returning a variant, returning either the normal result or an error. Now, programmers generally don't want to deal with all the typing machinery and like pushing around a variant. And so, you use all the syntax that Python gives us, for example, try and catch, you know, functions that raise and things like this, you can put a raises, decorator on your functions, stuff like this. And if you want to control that, and then the language can provide syntax for it. But under the hood, the way the computer executes it, throwing an error is basically as fast as returning something. I think so it's exactly the same way from a compiled perspective. And so, this is actually, I mean, it's a fairly nerdy thing, right? Which is why I love it. But this has a huge impact on the way you design your APIs, right? So in C++, huge communities turn off exceptions, because the cost is just so high, right? And so the zero cost cost is so high, right? And so that means you can't actually use exceptions in many libraries, right? And even for the people that do use it, well, okay, how and when do you want to pay the cost? If I try to open a file, should I throw an error? Well, what if I'm probing around looking for something, right? I'm looking it up in many different paths. Well, if it's really slow to do that, maybe I'll add another function that doesn't throw an error returns an error code instead. And I have two different versions the same thing. And so it causes you to fork your APIs. And so, you know, one of the things I learned from Apple and I so love is the art of API design is actually really profound. I think this is something that Python's also done a pretty good job at in terms of building out this large scale package ecosystem, it's about having standards and things like this. And so, you know, we wouldn't want to enter a mode where, you know, there's this theoretical feature that exists in language, but people don't use it in practice. Now, I'll also say one of the other really cool things about this implementation approach is that it can run on GPUs and it can run accelerators and things like this. And that standard zero cost exception thing would never work on an accelerator. And so this is also part of how Mojo can scale all the way down to like little embedded systems and to running on GPUs and things like that. Can you actually say about the maybe is there some high level way to describe the challenge of exceptions and how they work in code during compilation? So just this idea of percolating up a thing, an error. Yeah. Yeah. So the way the way to think about it is think about a function that doesn't return anything. Just as a simple case, right? And so you have function one calls function two calls function three calls function four, along that call stack that are tri blocks. Right. And so if you have function one calls function two function two has a tri block, and then within it, it calls function three, right? Well, what happens if function three throws? Well, actually start simpler. What happens if it returns? Well, if it returns, it's supposed to go back out and continue executing and then fall off the bottom of the tri block and keep going and it all's good. If the function throws, you're supposed to exit the current function and then get into the accept clause, right? And then do whatever codes there and then keep following on and going on. And so the way that a compiler like Mojo works is that the call to that function, which happens in the accept block calls a function and then instead of returning nothing, it actually returns, you know, a variant between nothing and an error. And so if you return normally off the bottom or do return, you return nothing. And if you throw through an error, you return the variant that is I'm an error, right? So when you get to the call, you say, okay, cool, I called a function. Hey, I know locally I'm in a tri block. Right. And so I, I call the function and then I check to see what it returns. A half is that error thing jump to the accept block. And that's all done for you behind the scenes. Exactly. And so the compiler does all this for you. And I mean, one of the things if you dig into how this stuff works in Python, it gets a little bit more complicated because you have finally blocks, which now need, you need to go into do some stuff. And then those can also throw and return. Wait, what? Like the stuff matters compatibility. Like there's, there's nest them. There's with clauses. And so with clauses are kind of like finally blocks of some special stuff going on. And so there's nesting in general, nesting of anything nesting of functions should be illegal. It just feels like it adds a level of complexity. I'm merely an implementer. And so this is again, one of the trade offs you get when you decide to build a super set is you get to implement a full fidelity implementation of the thing that you decided is good. And so, yeah, I mean, we can, we can complain about the reality of the world and shake our fists, but it always feels like you shouldn't be a lot to do that, like to declare functions and sudden functions inside functions. Wait, wait, wait, what happened to Lex the Lisp guy? No, I understand that. But Lisp is what I used to do in college. So now you've grown up. You know, we've all done things in college. We're not part of, no, I love Lisp. I love Lisp. Okay. Yeah, I was going to say, you're afraid of me. You're taking the whole internet. It's, it's, uh, it worked. It worked as a joke in my head. So nested functions are joking aside, actually really great. And for certain things, right? And so these are also called closures. Closures are pretty cool. And you can pass callbacks. There's a lot of good patterns. And so, uh, so speaking of which, I don't think you have, uh, nested functions implemented yet in Mojo. Uh, we don't have Lambda syntax, but we do have, uh, there's a few things on the roadmap that you have that it'd be cool to sort of just fly through. Cause it's interesting to see, you know, how many features there are in a language, small and big, they have to implement. Yeah. So first of all, there's tuple support and that has to do with some very specific aspect of it. Like the parentheses are not parentheses that. Yeah. This is just a totally a syntactic thing. A syntactic thing. Okay. There's, but it's cool. It's still, uh, so keyword arguments and functions. Yeah. So this is where in Python, you can say call a function x equals four. Yeah. And x is the name of the argument. That's a nice sort of documenting self-documenting feature. Yeah. I mean, and again, this isn't rocket science to implement. That's just the laundry. It's just on the list. Uh, the bigger features are things like traits. So traits are when you want to define abstract. So when you get into typed languages, you need the ability to write generics. And so you want to say, I want to write this function. And now I want to work on all things that are arithmetic like. Well, what does arithmetic like mean? Well, arithmetic like is a categorization of a bunch of types. And so it's, again, you can define many different ways and I'm not going to go into ring theory or something. But the, uh, you know, you can say it's arithmetic like if you can add the track multiply, divide it, for example. Right. And so what you're saying is you're saying there's a set of traits that apply to a broad variety of types. And so there, all these types are arithmetic like all these tensors and floating point integer. And like there's this category of types. And then I can define on an orthogonal access algorithms that then work against types that have those properties. And so this is a, again, it's a widely known thing. It's been implemented in Swift and Rust and many languages. So it's not Haskell, which is where everybody learns, learns their tricks from. But the, but we need to implement that and that'll enable a new level of expressivity. So classes. Yeah, classes are a big deal. It's a big deal still to be implemented. Um, like you said, a Lambda syntax, and there's like detail stuff like whole module import, um, support for top level code at file scope. So, and then global variables also. So being able to have variables outside of a top level. Well, and so this comes back to the where module came from and the fact that this is your point one, right? And so we're building, so modular is building an AI stack, right? And an AI stack has a bunch of problems working with hardware and writing high performance kernels and doing this kernel fusion thing I was talking about and getting the most out of the hardware. And so we've really prioritized and built Mojo to solve modules problem, right? Now our North Star is build out and support all the things. And so we're making incredible progress. By the way, Mojo is only like seven months old. So that's another interesting thing. I mean, part of the reason I wanted to mention some of these things is like, there's a lot to do and it's pretty cool how you just kind of, sometimes you take for granted how much there is in a programming language, how many cool features you kind of rely on. And this is kind of a nice reminder when you lay it as a to do list. Yeah. And so I mean, but also you look into, it's amazing how much is also there. And you take it for granted that a value, if you define it, it will get destroyed automatically. Like that little feature itself is actually really complicated, given the way the ownership system has to work. And the way that works within Mojo is a huge step forward from what Rust and Swift have done. But can you say that again, when a value, when you define it gets destroyed on the map? Yeah. So like say you have a string, right? So you just find a string on the stack or whatever that means, like in your local function, right? And so you say, like, whether it be in a def, and so you just say x equals hello world, right? Well, if your string type requires you to allocate memory, then when it's destroyed, you have to deallocate it. So in Python and Mojo, you define that with the Dell method, right? Where does that get run? Well, it gets run sometime between the last use of the value and the end of the program. Like in this, you now get into garbage collection, you get into like all these long debated, you talk about religions and tradeoffs and things like this. This is a hugely hotly contested world. If you look at C++, the way this works is that if you define a variable, or a set of variables within a function, they get destroyed in a last in first out order. So it's like nesting. This has a huge problem because if you define, you have a big scope, and you define a whole bunch of values at the top, and then you use them, and then you do a whole bunch of code that doesn't use them, they don't get destroyed until the very end of that scope. And so this also destroys tail calls, so good functional programming, right? This has a bunch of different impacts on, you talk about reference counting optimizations and things like this, a bunch of very low level things. And so what Mojo does is it has a different approach on that from any language I'm familiar with, where it destroys them as soon as possible. And by doing that, you get better memory use, you get better predictability, you get tail calls that work, like you get a bunch of other things, you get better ownership tracking, there's a bunch of these very simple things that are very fundamental, that are already built in there in Mojo today, that are the things that nobody talks about generally, but when they don't work right, you find out and you have to complain about. Is it trivial to know what's the soonest possible to delete a thing that's not going to be used again? Yeah, well, I mean, it's generally trivial, it's after the last use of it. So if you just find x as a string, and then you have some use of x somewhere in your code. Within that scope? You mean within the scope that is accessible? It's, yeah, exactly. So you can only use something within its scope. And so then it doesn't wait until the end of the scope to delete it. It destroys it after the last use. So there's kind of some very ego machine that's just sitting there and deleting. Yeah, and it's all in the compiler, so it's not at runtime, which is also cool. And so, yeah, and so what, and this is actually non-trivial because you have control flow. And so it gets complicated pretty quickly. And so like getting this right was not, Oh, so you have to insert delete like in a lot of places? Potentially, yeah, exactly. So the compiler has to reason about this. And this is where, again, it's experience building languages and not getting this right. So again, you get another chance to do it and you get basic things like this, right? But it's extremely powerful when you do that, right? And so there's a bunch of things like that that kind of combine together. And this comes back to the, you get a chance to do it the right way, do it the right way and make sure that every brick you put down is really good, so that when you put more bricks on top of it, they stack up to something that's beautiful. Well, there's also like, how many design discussions do there have to be about particular details like implementation of particular small features? Because the features that seem small, I bet some of them might be like really require really big design decisions. Yeah. Well, so, I mean, let me give you another example of this. Python has a feature called async await. So it's a new feature, I mean, in the long arc of history, it's a relatively new feature, right? That allows way more expressive asynchronous programming. Okay. Again, this is a Python's a beautiful thing and they did things that are great for Mojo for completely different reasons. The reason the async await got added to Python, as far as I know, is because Python doesn't support threads. Okay. And so Python doesn't support threads, but you want to work with networking and other things like that that can block. I mean, Python does support threads, it's just not its strength. And so they added this feature called async await. It's also seen in other languages like Swift and JavaScript and many other places as well. Async await in Mojo is amazing. Because we have a high-performance heterogeneous compute runtime underneath the covers that then allows non-blocking IO, so you get full use of your accelerator. That's huge, turns out. It's actually really an important part of fully utilizing the machine. You talk about design discussions. That took a lot of discussions, right? And it probably will require more iteration. And so my philosophy with Mojo is that, you know, we have a small team of really good people that are pushing forward, and they're very good at the extremely deep knowing how the compiler and runtime and all the low-level stuff works together. But they're not perfect. Same thing as the Swift team, right? And this is where one of the reasons we released Mojo much earlier is so we can get feedback. And we've already renamed a keyword due to a community feedback. We use an ampersand, and now it's named in and out. We're not renaming existing Python keywords because that breaks compatibility. We're naming things we're adding and making sure that they are designed well. We get usage experience. We iterate and work with the community because, again, if you scale something really fast and everybody writes all their code and they start using it in production, then it's impossible to change. And so you want to learn from people. You want to iterate and work on that early on. And this is where design discussions, it's actually quite important. Could you incorporate an emoji into the language, into the main language? Do you have a favorite one? Why really inters the humor, like rawful, whatever, rolling on the floor laughing? So that could be like, what would that be, the use case for that? Like throw an exception of some sort? You should totally file a feature request. Or maybe a hard one. It has to be a hard one. People have told me that I'm insane. I'm liking this. I'm going to use the viral nature of the internet to actually get this past. I mean, it's funny you come back to the flame emoji, file extension, right? We have the option to use the flame emoji, which just even that concept cause, for example, the people at GitHub to say, now I've seen everything. Yeah, there's something, it's reinvigorating. It's like, oh, that's possible. That's really cool that for some reason that makes everything else seem really exciting. I think the world is ready for this stuff, right? And so, you know, when we have a package manager, we'll clearly have to innovate by having the compiled package saying be the little box with the bow on it, right? I mean, it has to be done. It has to be done. Is there some stuff on the roadmap that you're particularly stressed about or excited about that you're thinking about a lot? I mean, as a today snapshot, which will be obsolete tomorrow, the lifetime stuff is really exciting. And so lifetimes give you safe references to memory without dangling pointers. And so this has been done in languages like Rust before. And so we have a new approach, which is really cool. I'm very excited about that. That'll be out to the community very soon. The traits feature is really a big deal. And so that's blocking a lot of API design. And so there's that. I think that's really exciting. A lot of it is these kind of table stakes features. One of the things that is, again, also lessons learned with Swift is that programmers in general like to add syntactic sugar. And so it's like, oh, well, this annoying thing, like in Python, you have to spell unbar unbar add. Why can't I just use plus? Def plus, come on. Why can't I just do that, right? And so trivial bit of syntactic sugar, it makes sense. It's beautiful. It's obvious. We're trying not to do that. And so for two different reasons, one of which is that, again, lesson learned with Swift. Swift has a lot of syntactic sugar, which may be a good thing, maybe not. I don't know. But because it's such an easy and addictive thing to do, sugar, like make sure blood get crazy, right? Like the community will really dig into that and want to do a lot of that. And I think it's very distracting from building the core abstractions. The second is we want to be a good member of the Python community, right? And so we want to work with the broader Python community. And yeah, we're pushing forward a bunch of systems programming features, and we need to build them out to understand them. But once we get a long ways forward, I want to make sure that we go back to the Python community and say, okay, let's do some design reviews. Let's actually talk about this stuff. Let's figure out how we want this stuff all to work together. And syntactic sugar just makes all that more complicated. And yeah, list comprehension is like yet to be implemented. And my favorite, I mean, dictionaries. Yeah, there's some basic zero point one, zero point one. But nonetheless, it's actually still quite interesting and useful. As you mentioned, modular is very new. Mojo is very new. It's a relatively small team. Yeah, that's building up this gigantic stack. This incredible stack that's going to perhaps define the future of development of our AI overlords. We just hope it will be useful. As do all of us. So what, what have you learned from this process of building up a team? Maybe one question is, how do you hire? Yeah, great programmers, great people that operate in this compiler, hardware, machine learning, software, interface design space. And maybe you're a little bit fluid in what they can do. So okay, so language design too. So building a company is just as interesting in different ways as building a language, like different skill sets, different things, but super interesting. And I've built a lot of teams in a lot of different places. If you zoom in from the big problem into recruiting, well, so here's our problem. Okay, I'll just, I'll be very straightforward about this. We started modular with a lot of conviction about we understand the problems, we understand the customer pain points, we need to work backwards from the suffering in the industry. And if we solve those problems, we think it'll be useful for people. But the problem is, is that the people we need to hire, as you say, are all these super specialized people that have jobs at big tech big tech worlds, right? And, you know, we, I don't think we have product market fit in the way that a normal startup does, we don't have product market fit challenges, because right now, everybody's using AI and so many of them are suffering and they want help. And so again, we started with strong conviction. Now, again, you have to hire and recruit the best and the best all have jobs. And so what we've done is we said, okay, well, let's build an amazing culture. Start with that. That's usually not something a company starts with. Usually you hire a bunch of people and then people start fighting and it turns into a gigantic mess. And then you try to figure out how to improve your culture later. My co-founder, Tim, in particular, is super passionate about making sure that that's right. And we've spent a lot of time early on to make sure that we can scale. Can you comment, sorry, before we get to the second, what makes for a good culture? So I mean, there's many different cultures. And I have learned many things from several very unique, almost famously unique cultures. And some of them I learned what to do and some of them I learned what not to do. And so we want an inclusive culture. I believe in amazing people working together. And so I've seen cultures where people, you have amazing people and they're fighting each other. I see amazing people and they're told what to do. Like, doubt, shout, line up and do what I say. It doesn't matter if it's the right thing. Do it. And neither of these, and I've seen people that have no direction. They're just kind of floating in different places. And they want to be amazing. They just don't know how. And so a lot of it starts with have a clear vision. And so we have a clear vision of what we're doing. And so I kind of grew up at Apple in my engineering life. And so a lot of the Apple DNA rubbed off on me. My co-founder, Tim, also is like a strong product guy. And so what we learned is, you know, I decided Apple that you don't work from building cool technology. You don't work from, like, come up with a cool product and think about the features you'll have in the big checkboxes and stuff like this. Because if you go talk to customers, they don't actually care about your product. They don't care about your technology. What they care about is their problems. Right? And if your product can help solve their problems, well, hey, they might be interested in that. And so if you speak to them about their problems, if you understand and you have compassion, you understand what people are working with, then you can work backwards to building an amazing product. So the vision starts by defining the problem. And then you can work backwards in solving technology. And at Apple, like it's, I think, pretty famously said that, you know, for every, you know, there's 100 no's for every yes. I would refine that to say that there's 100 not yet for every yes. But famously, if you go back to the iPhone, for example, right, the iPhone one, I read, I mean, many people laughed at it because it didn't have 3G. It didn't have copy and paste. Right. And then a year later, okay, finally, it has 3G, but it still doesn't have copy and paste. It's a joke. Nobody will ever use this product, blah, blah, blah, blah, blah, blah, blah, blah, right? Well, your three had copy and paste and people stopped talking about it. Right. And so, and so being laser focused and having conviction and understanding what the core problems are and giving the team the space to be able to build the right tech is really important. Also, I mean, you come back to recruiting, you have to pay well. Right. So we have to pay industry leading salaries and have good benefits and things like this. That's a big piece. We're a remote first company. And so we have to, so remote first has a very strong set of pros and cons. On the one hand, you can hire people from wherever they are and you can attract amazing talent, even if they live in strange places or unusual places. On the other hand, you have time zones. On the other hand, you have like everybody on the internet will fight if they don't understand each other. And so we've had to learn how to like have a system where we actually fly people in and we get the whole company together periodically and then we get work groups together and we plan and execute together. And there's like an intimacy to the in-person brainstorming. Yeah, I guess you lose, but maybe you don't. Maybe if you get to know each other well and you trust each other, maybe you can do that. Yeah. Well, so when the pandemic first hit, I mean, I'm curious about your experience too. The first thing I missed was having whiteboards. Yeah. Right. Those design discussions are like, I can high intensity work through things, get things done, work through the problem of the day, understand where you're on, figure out and solve the problem and move forward. But we figured out ways to work around that now with all these screen sharing and other things like that that we do. The thing I miss now is sitting down at a lunch table with the team, the spontaneous things like the coffee bar things and the bumping into each other and getting to know people outside of the transactional solve a problem over Zoom thing. And I think there's just a lot of stuff that I'm not an expert at this. I don't know who is, hopefully there's some people, but there's stuff that somehow is missing on Zoom. Even with the whiteboard, if you look at that, if you have a room with one person at the whiteboard and there's like three other people at a table, there's a, first of all, there's a social aspect of that where you're just shooting the shit a little bit, almost like. Yeah. As people just kind of coming in and yeah, that, but also while like it's a breakout discussion that happens for like seconds at a time, maybe an inside joke or it's like this interesting dynamic that happens that Zoom. And you're bonding. Yeah. You're bonding. You're bonding, but through that bonding, you get the excitement. There's certain ideas that are like complete bullshit and you'll see that in the faces of others that you won't see necessarily on Zoom. And like something, it feels like that should be possible to do without being in person. Well, I mean, being in person is a very different thing. It's worth it, but you can't always do it. And so again, we're still learning and we're also learning as like humanity with this new reality, right? But what we found is that getting people together, whether it be a team or the whole company or whatever, is worth the expense because people work together and are happier after that. Like there's a massive period of time where you go out and things start getting frayed, pull people together, and then you realize that we're all working together. We see things the same way. We work through the disagreement or the misunderstanding. We're talking across each other and then you work much better together. And so things like that, I think are really quite important. What about people that are kind of specialized in very different aspects of the stack working together? What are some interesting challenges there? Yeah. Well, so I mean, I mean, there's lots of interesting people, as you can tell, I'm, you know, hard to deal with too. You're one of the most lovable people. So one of the, so there's different philosophies in building teams. For me, and so some people say, hire 10x programmers, and that's the only thing that whatever that means, right? What I believe in is building well balanced teams. Teams that have people that are different in them. Like if you have all generals and no troops, or all troops and no generals, or you have all people that think in one way, and not the other way, what you get is you get a very biased and skewed and weird situation where people end up being unhappy. And so what I like to do is I like to build teams of people where they're not all the same. You know, we do have teams that are focused on like runtime or compiler, GPU or whatever the specialty is, but people bring a different take and have a different perspective. And I look for people that complement each other. And particularly if you look at leadership teams and things like this, you don't want everybody thinking the same way. You want people bringing different perspectives and experiences. And so I think that's really important. That's team, but what about building a company as ambitious as modular? So what are some interesting questions there? Oh, I mean, so many. Like, so one of the things I love about, okay, so modular is the first company I built from scratch. One of the first things that was profound was I'm not cleaning up somebody else's mess. Right. And so if you look at that's liberating to some degree. It's super liberating. And also, many of the projects I've built in the past have not been core to the product of the company. Swift is not Apple's product, right? MLIR is not Google's revenue machine or whatever, right? It's not, it's important. But it's like working on the accounting software for, you know, the retail giant or something, right? It's like enabling infrastructure and technology. And so at modular, the tech we're building is here to solve people's problems. Like it is directly the thing we're giving to people. And so this is a really big difference. And what it means for me as a leader, but also for many of our engineers is they're working on the thing that matters. And that's actually pretty, I mean, again, for compiler people and things like that, that's usually not the case, right? And so that's also pretty exciting and quite nice. But one of the ways that this manifests is it makes it easier to make decisions. And so one of the challenges I've had in other worlds is it's like, okay, well, community matters somehow for the goodness of the world, like, or open-source matters theoretically, but I don't want to pay for a t-shirt, right? Or some swag. Like, well, t-shirts cost $10 each. You can have 100 t-shirts for $1,000 to a mega-corp. $1,000 is uncountably, can't count that low, right? But justifying it and getting a t-shirt, by the way, if you'd like a t-shirt, I can give you a t-shirt. Well, I would 100% like a t-shirt. Are you joking? You can have a fire emoji t-shirt. I will treasure this. Is that a good thing? I will pass it down to my grandchildren. And so it's very liberating to be able to decide, I think that Lex should have a t-shirt, right? And it becomes very simple because I like Lex. This is awesome. So I have to ask you about the... One of the interesting developments with large language models is that they're able to generate code recently really well. Yes. To a degree that maybe I don't know if you understand, but I have... I struggle to understand because it forces me to ask questions about the nature of programming, of the nature of thought, because the language models are able to predict the kind of code I was about to write so well that it makes me wonder how unique my brain is and where the valuable ideas actually come from. How much do I contribute in terms of ingenuity, innovation, to code, I write, or design, and that kind of stuff. When you stand on the shoulders of giants, are you really doing anything? And what LLMs are helping you do is they help you stand on the shoulders of giants in your program. There's mistakes. They're interesting that you learned from, but I would love to get your opinion first high-level of what you think about this impact of large language models when they do program synthesis, when they generate code. I don't know where it all goes. I'm an optimist and I'm a human optimist. I think that things I've seen are that a lot of the LLMs are really good at crushing leak code projects, and they can reverse the link list like crazy. Well, it turns out there's a lot of instances of that on the internet, and it's a pretty stock thing. And so if you want to see standard questions answered, LLMs can memorize all the answers, and that can be amazing. And also, they do generalize out from that, and so there's good work on that. But I think that in my experience, building things, building something like you talk about mojo or you talk about these things or you talk about building an applied solution to a problem, it's also about working with people. It's about understanding the problem. What is the product that you want to build? What are the use case? What are the customers? You can't just go survey all the customers because they'll tell you that they want a faster horse. Maybe they need a car. And so a lot of it comes into, I don't feel like we have to compete with LLMs. I think they'll help automate a ton of the mechanical stuff out of the way. And just like, I think we all try to scale through delegation and things like this. Delegating wrote things to an LLM, I think, because it's extremely valuable and approach that will help us all scale and be more productive. But I think it's a fascinating companion. But I'd say I don't think that that means that we're going to be done with coding. Sure. But there's power in it as a companion. And from there, I would love to zoom in on to mojo a little bit. Do you think about that? Do you think about LLMs generating mojo code and helping sort of like, we design new programming language, it almost seems like, man, it would be nice to sort of, almost as a way to learn how I'm supposed to use this thing for them to be trained on some of the mojo code. So I do lead an AI company. So maybe there'll be a mojo LLM at some point. But if your question is like, how do we make a language to be suitable for LLMs? I think the cool thing about LLMs is you don't have to. And so if you look at what is English or any of these other terrible languages that we as humans deal with on a continuous basis, they're never designed for machines. And yet, they're the intermediate representation, they're the exchange format that we humans use to get stuff done. And so these programming languages, they're an intermediate representation between the human and the computer or the human and the compiler, roughly, right? And so I think the LLMs will have no problem learning whatever keyword we pick. Maybe the phi emoji is going to... Maybe that's going to break it, it doesn't tokenize. No, the reverse of that, it will actually enable it because one of the issues I could see with being a superset of Python is there would be confusion by the gray area. So it would be mixing stuff. But... Well, I'm a human optimist, I'm also an LLM optimist. I think that we'll solve that problem. But you look at that and you say, okay, well, reducing the rote thing, right? Turns out compilers are very particular and they really want things, they really want the indentation to be right. They really want the colon to be there on your else or else that will complain, right? I mean, compilers can do better at this, but LLMs can totally help solve that problem. And so I'm very happy about the new predictive coding and co-pilot type features and things like this because I think it will all just make us more productive. It's still messy and fuzzy and uncertain, unpredictable, so... But is there a future you see given how big of a leap GPT-4 was, where you start to see something like LLMs inside a compiler or no? I mean, you could do that. Yeah, absolutely. I mean, I think that would be interesting. Is that wise? Well, I mean, it would be very expensive. So compilers run fast and they're very efficient and LLMs are currently very expensive. There's on-device LLMs and there's other things going on and so maybe there's an answer there. I think that one of the things that I haven't seen enough of is that... So LLMs to me are amazing when you tap into the creative potential of the hallucinations, right? And so if you're doing creative brainstorming or creative writing or things like that, the hallucinations work in your favor. If you're writing code that has to be correct because you're going to ship it in production, then maybe that's not actually a feature. And so I think that there has been research and there has been work on building algebraic reasoning systems and figuring out more things that feel like proofs. And so I think that there could be interesting work in terms of building more reliable at scale systems and that could be interesting. But if you chase that rabbit hole down, the question then becomes how do you express your intent to the machine? And so maybe you want LLMs to provide the spec, but you have a different kind of net that then actually implements the code. Right. So it's used as documentation and inspiration versus the actual implementation. Yeah, potentially. Since a successful modular will be the thing that runs, I say so jokingly, are AI overlords. But AI systems that are used across... I know it's a cliche term, but in a lot of things. So across... So I'll joke and say like AGI should be written in Mojo. Yeah, AGI should be written in Mojo. You're joking, but it's also possible that it's not a joke. That a lot of the ideas behind Mojo is seems like the natural set of ideas that would enable at scale training and inference of AI systems. So I just have to ask you about the big philosophical question about human civilization. So folks like Eliezer Yatkowski are really concerned about the threat of AI. Do you think about the good and the bad that can happen at scale deployment of AI systems? Well, so I've thought a lot about it and there's a lot of different parts to this problem, everything from job displacement to sky nut, things like this. And so you can zoom in to sub parts of this problem. I'm not super optimistic about AGI being solved next year. I don't think that's going to happen personally. So you have a kind of zen like calm about. Is there's a nervousness because the leap of GBT4 seemed so big. Sure. It's like we're almost... There's some kind of transition era period. You're thinking... Well, so I mean, there's a couple of things going on there. One is I'm sure GPT5 and 7 and 19 will be also huge leaps. They're also getting much more expensive to run. And so there may be a limiting function in terms of just expense on one hand and train. That could be a limiter that slows things down. But I think the bigger limiter outside of sky nut takes over and I don't spend any time thinking about that because if sky nut takes over and kills us all, then I'll be dead. So I don't worry about that. Other things worry about, I'll just focus on. I'll focus and not worry about that one. But I think that the other thing I'd say is that AI moves quickly, but humans move slowly and we adapt slowly. And so what I expect to happen is just like any technology diffusion, the promise and then the application takes time to roll out. And so I think that I'm not even too worried about autonomous cars defining away all the taxi drivers. Remember, autonomy is supposed to be solved by 2020? Boy, do I remember. And so I think that on the one hand we can see amazing progress, but on the other hand we can see that the reality is a little bit more complicated and it may take longer to roll out than you might expect. Well, that's in the physical space. I do think in the digital space is the stuff that's built on top of LLMs that runs millions of apps that could be built on top of them. And that could be run on millions of devices, millions of types of devices. I just think that the rapid effect it has on human civilization could be truly transformative to it. And there I think it depends on are you an optimist or a pessimist or a masochist? Just to clarify, optimist about human civilization. Me too. And so I look at that as saying, okay, cool, what will AI do? And so some people say, oh my god, is it going to destroy us all? How do we prevent that? I kind of look at it from a, is it going to unlock us all? You talk about coding, is it going to make so I don't have to do all the repetitive stuff? Well, suddenly that's a very optimistic way to look at it. And you look at what a lot of these technologies have done to improve our lives. And I want that to go faster. What do you think the future of programming looks like in the next 10, 20, 30, 50 years with LLMs and with Mojo with modular, like your vision for devices, the hardware to the compilers to this, to the different stacks of software. Yeah. Well, so what I want, I mean, coming back to my arch nemesis, it's complexity. So again, me being the optimist, if we drive down complexity, we can make these tools, these technologies, these cool hardware widgets accessible to way more people. And so what I'd love to see is more personalized experiences, more things, the research getting into production instead of being lost at NeurIPS. Right. And like these things that impact people's lives by entering products. And so one of the things that I'm a little bit concerned about is right now, the big companies are investing huge amounts of money and are driving the top line of AI capability forward really quickly. But if it means that you have to have $100 million to train a model or more $100 billion, right? Well, that's going to make it very concentrated with very few people in the world that can actually do this stuff. I would much rather see lots of people across the industry be able to participate and use this, right? And you look at this, you know, I mean, a lot of great research has been done in the health world and looking at like detecting pathologies and doing radiology with AI and like doing all these things. Well, the problem today is that to deploy and build these systems, you have to be an expert in radiology and an expert in AI. And if we can break down the barriers so that more people can use AI techniques, it's more like programming Python, which roughly everybody can do if they want to, right? Then I think that we'll get a lot more practical application of these techniques and a lot more niche year, cool, but narrower demands. And I think that's that's going to be really cool. Do you think we'll have more or less programmers in the world than now? Well, so I think we'll have more more programmers, but they may not consider themselves to be programmers. That'd be a different name for you, right? I mean, do you consider somebody that uses, you know, I think that arguably the most popular programming language is Excel? Yeah. Right? Yep. And so do they consider themselves to be programmers? Maybe not. I mean, some of them make crazy macros and stuff like that. But but but what what you mentioned, Steve Jobs, is it's the bicycle for the mind that allows you to go faster, right? And so I think that as we look forward, right? What is AI? I look at it as hopefully a new programming paradigm. It's like object-oriented programming, right? If you want to write a cat to texture, you don't use for loops. Turns out that's not the right tool for the job, right? And so right now, unfortunately, because I mean, it's not unfortunate, but it's just kind of where where things are. AI is this weird, different thing that's not integrated into programming languages and normal tool chains and all the technology is really weird and doesn't work right. And you have to babysit it. And every time you switch hardware, it's different. It shouldn't be that way. When you change that, when you fix that, suddenly, again, the tools technologies can be way easier to use. You can start using them for many more things. And so that's that's why I would be excited about. What kind of advice could you give to somebody in high school right now or maybe early college who's curious about programming and feeling like the world is changing really quickly here? Yeah. Well, what kind of stuff to learn? What kind of stuff to work on? Should they finish college? Should they go work at a company? Should they build a thing? What do you think? Well, so I mean, one of the things I'd say is that you'll be most successful if you work on something you're excited by. And so don't get the book and read the book cover to cover and study and memorize and recite and flashcard and go build something. Like go solve a problem. Go build the thing that you want to exist. Go build an app. Go train a model. Like go build something and actually use it and set a goal for yourself. And if you do that, then you'll, you know, there's a success. There's the adrenaline rush. There's the achievement. There's the unlock that I think is where, you know, if you keep setting goals and you keep doing things and building things, learning by building is really powerful. In terms of career advice, I mean, everybody's different. It's very hard to give generalized advice. I'll speak as a compiler nerd. If everybody's going left, sometimes it's pretty cool to go right. And so just because everybody's doing a thing, it doesn't mean you have to do the same thing and follow the herd. In fact, I think that sometimes the most exciting paths through life lead to being curious about things that nobody else actually focuses on, right? And turns out that understanding deeply parts of the problem that people want to take for granted makes you extremely valuable and specialized in ways that the herd is not. And so again, I mean, there's lots of rooms for specialization, lots of rooms for generalists. There's lots of room for different kinds and parts of the problem. But I think that it's, you know, just because everybody's doing one thing doesn't mean you should necessarily do it. And now the herd is using Python. So if you want to be a rebel, go check out Mojo and help Chris and the rest of the world fight the arch nemesis of complexity, because simple is beautiful. There you go. Because you're an incredible person. You've been so kind to me ever since we met. You've been extremely supportive. I'm forever grateful for that. Thank you for being who you are, for being legit, for being kind, for fighting this really interesting problem of how to make AI accessible to a huge number of people, a huge number of devices. Yeah. Well, so Lex, you're a pretty special person too, right? And so I think that, you know, one of the funny things about you is that besides being curious and pretty damn smart, you're actually willing to push on things. And I think that you've got an agenda to like make the world think, which I think is a pretty good agenda. It's a pretty good one. Thank you so much for talking, Chris. Yeah, thanks, Lex. Thanks for listening to this conversation with Chris Ladner. To support this podcast, please check out our sponsors in the description. And now, let me leave you some words from Isaac Asimov. I do not fear computers. I fear the lack of them. Thank you for listening and hope to see you next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.08, "text": " On one axis, you have more hardware coming in. On the other hand, you have an explosion of", "tokens": [50364, 1282, 472, 10298, 11, 291, 362, 544, 8837, 1348, 294, 13, 1282, 264, 661, 1011, 11, 291, 362, 364, 15673, 295, 50568], "temperature": 0.0, "avg_logprob": -0.12506082322862414, "compression_ratio": 1.75, "no_speech_prob": 0.06173762306571007}, {"id": 1, "seek": 0, "start": 4.08, "end": 8.8, "text": " innovation in AI. And so what happened with both TensorFlow and PyTorch is that the explosion", "tokens": [50568, 8504, 294, 7318, 13, 400, 370, 437, 2011, 365, 1293, 37624, 293, 9953, 51, 284, 339, 307, 300, 264, 15673, 50804], "temperature": 0.0, "avg_logprob": -0.12506082322862414, "compression_ratio": 1.75, "no_speech_prob": 0.06173762306571007}, {"id": 2, "seek": 0, "start": 8.8, "end": 13.68, "text": " of innovation in AI has led to, it's not just about matrix multiplication and convolution,", "tokens": [50804, 295, 8504, 294, 7318, 575, 4684, 281, 11, 309, 311, 406, 445, 466, 8141, 27290, 293, 45216, 11, 51048], "temperature": 0.0, "avg_logprob": -0.12506082322862414, "compression_ratio": 1.75, "no_speech_prob": 0.06173762306571007}, {"id": 3, "seek": 0, "start": 13.68, "end": 18.16, "text": " these things have now like 2,000 different operators. And on the other hand, you have,", "tokens": [51048, 613, 721, 362, 586, 411, 568, 11, 1360, 819, 19077, 13, 400, 322, 264, 661, 1011, 11, 291, 362, 11, 51272], "temperature": 0.0, "avg_logprob": -0.12506082322862414, "compression_ratio": 1.75, "no_speech_prob": 0.06173762306571007}, {"id": 4, "seek": 0, "start": 18.16, "end": 21.12, "text": " I don't know how many pieces of hardware there are out there, it's a lot.", "tokens": [51272, 286, 500, 380, 458, 577, 867, 3755, 295, 8837, 456, 366, 484, 456, 11, 309, 311, 257, 688, 13, 51420], "temperature": 0.0, "avg_logprob": -0.12506082322862414, "compression_ratio": 1.75, "no_speech_prob": 0.06173762306571007}, {"id": 5, "seek": 0, "start": 21.12, "end": 25.6, "text": " Part of my thesis, part of my belief of where computing goes, if you look at 10 years from now,", "tokens": [51420, 4100, 295, 452, 22288, 11, 644, 295, 452, 7107, 295, 689, 15866, 1709, 11, 498, 291, 574, 412, 1266, 924, 490, 586, 11, 51644], "temperature": 0.0, "avg_logprob": -0.12506082322862414, "compression_ratio": 1.75, "no_speech_prob": 0.06173762306571007}, {"id": 6, "seek": 2560, "start": 26.16, "end": 30.160000000000004, "text": " is it's not going to get simpler. Physics isn't going back to where we came from.", "tokens": [50392, 307, 309, 311, 406, 516, 281, 483, 18587, 13, 38355, 1943, 380, 516, 646, 281, 689, 321, 1361, 490, 13, 50592], "temperature": 0.0, "avg_logprob": -0.10978027573205475, "compression_ratio": 1.7517482517482517, "no_speech_prob": 0.060059305280447006}, {"id": 7, "seek": 2560, "start": 30.720000000000002, "end": 35.92, "text": " It's only going to get weirder from here on out. And so to me, the exciting part about what we're", "tokens": [50620, 467, 311, 787, 516, 281, 483, 321, 347, 1068, 490, 510, 322, 484, 13, 400, 370, 281, 385, 11, 264, 4670, 644, 466, 437, 321, 434, 50880], "temperature": 0.0, "avg_logprob": -0.10978027573205475, "compression_ratio": 1.7517482517482517, "no_speech_prob": 0.060059305280447006}, {"id": 8, "seek": 2560, "start": 35.92, "end": 42.32, "text": " building is it's about building that universal platform, which the world can continue to get", "tokens": [50880, 2390, 307, 309, 311, 466, 2390, 300, 11455, 3663, 11, 597, 264, 1002, 393, 2354, 281, 483, 51200], "temperature": 0.0, "avg_logprob": -0.10978027573205475, "compression_ratio": 1.7517482517482517, "no_speech_prob": 0.060059305280447006}, {"id": 9, "seek": 2560, "start": 42.32, "end": 47.6, "text": " weird, because again, I don't think it's avoidable, it's physics. But we can help lift people scale,", "tokens": [51200, 3657, 11, 570, 797, 11, 286, 500, 380, 519, 309, 311, 5042, 712, 11, 309, 311, 10649, 13, 583, 321, 393, 854, 5533, 561, 4373, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10978027573205475, "compression_ratio": 1.7517482517482517, "no_speech_prob": 0.060059305280447006}, {"id": 10, "seek": 2560, "start": 47.6, "end": 50.480000000000004, "text": " do things with it, and they don't have to rewrite their code every time a new device comes out.", "tokens": [51464, 360, 721, 365, 309, 11, 293, 436, 500, 380, 362, 281, 28132, 641, 3089, 633, 565, 257, 777, 4302, 1487, 484, 13, 51608], "temperature": 0.0, "avg_logprob": -0.10978027573205475, "compression_ratio": 1.7517482517482517, "no_speech_prob": 0.060059305280447006}, {"id": 11, "seek": 2560, "start": 51.040000000000006, "end": 52.160000000000004, "text": " And I think that's pretty cool.", "tokens": [51636, 400, 286, 519, 300, 311, 1238, 1627, 13, 51692], "temperature": 0.0, "avg_logprob": -0.10978027573205475, "compression_ratio": 1.7517482517482517, "no_speech_prob": 0.060059305280447006}, {"id": 12, "seek": 5216, "start": 52.31999999999999, "end": 59.44, "text": " The following is a conversation with Chris Latner, his third time on this podcast.", "tokens": [50372, 440, 3480, 307, 257, 3761, 365, 6688, 7354, 1193, 11, 702, 2636, 565, 322, 341, 7367, 13, 50728], "temperature": 0.0, "avg_logprob": -0.14461830946115348, "compression_ratio": 1.528169014084507, "no_speech_prob": 0.013015763834118843}, {"id": 13, "seek": 5216, "start": 59.44, "end": 64.72, "text": " As I've said many times before, he's one of the most brilliant engineers in modern computing,", "tokens": [50728, 1018, 286, 600, 848, 867, 1413, 949, 11, 415, 311, 472, 295, 264, 881, 10248, 11955, 294, 4363, 15866, 11, 50992], "temperature": 0.0, "avg_logprob": -0.14461830946115348, "compression_ratio": 1.528169014084507, "no_speech_prob": 0.013015763834118843}, {"id": 14, "seek": 5216, "start": 64.72, "end": 70.16, "text": " having created LLM Compiler Infrastructure Project, the Klang Compiler, the Swift programming", "tokens": [50992, 1419, 2942, 441, 43, 44, 6620, 5441, 38425, 2885, 9849, 11, 264, 16053, 656, 6620, 5441, 11, 264, 25539, 9410, 51264], "temperature": 0.0, "avg_logprob": -0.14461830946115348, "compression_ratio": 1.528169014084507, "no_speech_prob": 0.013015763834118843}, {"id": 15, "seek": 5216, "start": 70.16, "end": 74.72, "text": " language, a lot of key contributions to TensorFlow and TPUs as part of Google.", "tokens": [51264, 2856, 11, 257, 688, 295, 2141, 15725, 281, 37624, 293, 314, 8115, 82, 382, 644, 295, 3329, 13, 51492], "temperature": 0.0, "avg_logprob": -0.14461830946115348, "compression_ratio": 1.528169014084507, "no_speech_prob": 0.013015763834118843}, {"id": 16, "seek": 5216, "start": 74.72, "end": 80.8, "text": " He served as vice president of autopilot software at Tesla, was a software innovator", "tokens": [51492, 634, 7584, 382, 11964, 3868, 295, 31090, 31516, 4722, 412, 13666, 11, 390, 257, 4722, 5083, 1639, 51796], "temperature": 0.0, "avg_logprob": -0.14461830946115348, "compression_ratio": 1.528169014084507, "no_speech_prob": 0.013015763834118843}, {"id": 17, "seek": 8080, "start": 80.8, "end": 87.67999999999999, "text": " and leader at Apple, and now he co-created a new full stack AI infrastructure for distributed", "tokens": [50364, 293, 5263, 412, 6373, 11, 293, 586, 415, 598, 12, 66, 26559, 257, 777, 1577, 8630, 7318, 6896, 337, 12631, 50708], "temperature": 0.0, "avg_logprob": -0.13373171199451794, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.005137695465236902}, {"id": 18, "seek": 8080, "start": 87.67999999999999, "end": 93.92, "text": " training, inference, and deployment on all kinds of hardware called Modular, and a new", "tokens": [50708, 3097, 11, 38253, 11, 293, 19317, 322, 439, 3685, 295, 8837, 1219, 6583, 1040, 11, 293, 257, 777, 51020], "temperature": 0.0, "avg_logprob": -0.13373171199451794, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.005137695465236902}, {"id": 19, "seek": 8080, "start": 93.92, "end": 99.92, "text": " programming language called Mojo, that is a superset of Python, giving you all the usability of", "tokens": [51020, 9410, 2856, 1219, 3335, 5134, 11, 300, 307, 257, 37906, 302, 295, 15329, 11, 2902, 291, 439, 264, 46878, 295, 51320], "temperature": 0.0, "avg_logprob": -0.13373171199451794, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.005137695465236902}, {"id": 20, "seek": 8080, "start": 99.92, "end": 107.12, "text": " Python, but with the performance of C++. In many cases, Mojo code has demonstrated over", "tokens": [51320, 15329, 11, 457, 365, 264, 3389, 295, 383, 25472, 13, 682, 867, 3331, 11, 3335, 5134, 3089, 575, 18772, 670, 51680], "temperature": 0.0, "avg_logprob": -0.13373171199451794, "compression_ratio": 1.5622317596566524, "no_speech_prob": 0.005137695465236902}, {"id": 21, "seek": 10712, "start": 107.76, "end": 114.48, "text": " 30,000 x speed up over Python. If you love machine learning, if you love Python,", "tokens": [50396, 2217, 11, 1360, 2031, 3073, 493, 670, 15329, 13, 759, 291, 959, 3479, 2539, 11, 498, 291, 959, 15329, 11, 50732], "temperature": 0.0, "avg_logprob": -0.08950908885282628, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.016652841120958328}, {"id": 22, "seek": 10712, "start": 114.48, "end": 120.16000000000001, "text": " you should definitely give Mojo a try. This programming language, this new AI framework,", "tokens": [50732, 291, 820, 2138, 976, 3335, 5134, 257, 853, 13, 639, 9410, 2856, 11, 341, 777, 7318, 8388, 11, 51016], "temperature": 0.0, "avg_logprob": -0.08950908885282628, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.016652841120958328}, {"id": 23, "seek": 10712, "start": 120.16000000000001, "end": 126.4, "text": " and infrastructure, and this conversation with Chris is mind blowing. I love it.", "tokens": [51016, 293, 6896, 11, 293, 341, 3761, 365, 6688, 307, 1575, 15068, 13, 286, 959, 309, 13, 51328], "temperature": 0.0, "avg_logprob": -0.08950908885282628, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.016652841120958328}, {"id": 24, "seek": 10712, "start": 127.28, "end": 132.4, "text": " It gets pretty technical at times, so I hope you hang on for the ride. This is the Lex", "tokens": [51372, 467, 2170, 1238, 6191, 412, 1413, 11, 370, 286, 1454, 291, 3967, 322, 337, 264, 5077, 13, 639, 307, 264, 24086, 51628], "temperature": 0.0, "avg_logprob": -0.08950908885282628, "compression_ratio": 1.4977777777777779, "no_speech_prob": 0.016652841120958328}, {"id": 25, "seek": 13240, "start": 132.4, "end": 136.64000000000001, "text": " Friedman podcast. To support it, please check out our sponsors in the description,", "tokens": [50364, 17605, 1601, 7367, 13, 1407, 1406, 309, 11, 1767, 1520, 484, 527, 22593, 294, 264, 3855, 11, 50576], "temperature": 0.0, "avg_logprob": -0.16277816479022686, "compression_ratio": 1.5472972972972974, "no_speech_prob": 0.0938892737030983}, {"id": 26, "seek": 13240, "start": 136.64000000000001, "end": 142.8, "text": " and now, dear friends, here's Chris Lattner. It's been, I think, two years since we last", "tokens": [50576, 293, 586, 11, 6875, 1855, 11, 510, 311, 6688, 441, 1591, 1193, 13, 467, 311, 668, 11, 286, 519, 11, 732, 924, 1670, 321, 1036, 50884], "temperature": 0.0, "avg_logprob": -0.16277816479022686, "compression_ratio": 1.5472972972972974, "no_speech_prob": 0.0938892737030983}, {"id": 27, "seek": 13240, "start": 142.8, "end": 149.04000000000002, "text": " talked, and in that time, you somehow went and co-created a new programming language called Mojo.", "tokens": [50884, 2825, 11, 293, 294, 300, 565, 11, 291, 6063, 1437, 293, 598, 12, 66, 26559, 257, 777, 9410, 2856, 1219, 3335, 5134, 13, 51196], "temperature": 0.0, "avg_logprob": -0.16277816479022686, "compression_ratio": 1.5472972972972974, "no_speech_prob": 0.0938892737030983}, {"id": 28, "seek": 13240, "start": 149.84, "end": 155.36, "text": " So it's optimized for AI. It's a superset of Python. Let's look at the big picture. What is", "tokens": [51236, 407, 309, 311, 26941, 337, 7318, 13, 467, 311, 257, 37906, 302, 295, 15329, 13, 961, 311, 574, 412, 264, 955, 3036, 13, 708, 307, 51512], "temperature": 0.0, "avg_logprob": -0.16277816479022686, "compression_ratio": 1.5472972972972974, "no_speech_prob": 0.0938892737030983}, {"id": 29, "seek": 13240, "start": 155.36, "end": 162.0, "text": " the vision for Mojo? For Mojo? Well, I think you have to zoom out. So I've been working on a lot", "tokens": [51512, 264, 5201, 337, 3335, 5134, 30, 1171, 3335, 5134, 30, 1042, 11, 286, 519, 291, 362, 281, 8863, 484, 13, 407, 286, 600, 668, 1364, 322, 257, 688, 51844], "temperature": 0.0, "avg_logprob": -0.16277816479022686, "compression_ratio": 1.5472972972972974, "no_speech_prob": 0.0938892737030983}, {"id": 30, "seek": 16200, "start": 162.0, "end": 166.8, "text": " of related technologies for many, many years. So I've worked on LVM, and a lot of things, and", "tokens": [50364, 295, 4077, 7943, 337, 867, 11, 867, 924, 13, 407, 286, 600, 2732, 322, 441, 53, 44, 11, 293, 257, 688, 295, 721, 11, 293, 50604], "temperature": 0.0, "avg_logprob": -0.1016405423482259, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004196727182716131}, {"id": 31, "seek": 16200, "start": 166.8, "end": 172.16, "text": " mobile, and servers, and things like this. But the world's changing. And what's happened with", "tokens": [50604, 6013, 11, 293, 15909, 11, 293, 721, 411, 341, 13, 583, 264, 1002, 311, 4473, 13, 400, 437, 311, 2011, 365, 50872], "temperature": 0.0, "avg_logprob": -0.1016405423482259, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004196727182716131}, {"id": 32, "seek": 16200, "start": 172.16, "end": 177.92, "text": " AI is we have new GPUs, and new machine learning accelerators, and other ASICs, and things like", "tokens": [50872, 7318, 307, 321, 362, 777, 18407, 82, 11, 293, 777, 3479, 2539, 10172, 3391, 11, 293, 661, 7469, 2532, 82, 11, 293, 721, 411, 51160], "temperature": 0.0, "avg_logprob": -0.1016405423482259, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004196727182716131}, {"id": 33, "seek": 16200, "start": 177.92, "end": 183.28, "text": " that that make AI go real fast. At Google, I worked on TPUs. That's one of the biggest,", "tokens": [51160, 300, 300, 652, 7318, 352, 957, 2370, 13, 1711, 3329, 11, 286, 2732, 322, 314, 8115, 82, 13, 663, 311, 472, 295, 264, 3880, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1016405423482259, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004196727182716131}, {"id": 34, "seek": 16200, "start": 183.28, "end": 189.28, "text": " largest scale deployed systems that exist for AI. And really what you see is if you look across", "tokens": [51428, 6443, 4373, 17826, 3652, 300, 2514, 337, 7318, 13, 400, 534, 437, 291, 536, 307, 498, 291, 574, 2108, 51728], "temperature": 0.0, "avg_logprob": -0.1016405423482259, "compression_ratio": 1.6619217081850535, "no_speech_prob": 0.004196727182716131}, {"id": 35, "seek": 18928, "start": 189.28, "end": 192.0, "text": " all of the things that are happening in the industry, there's this new compute platform", "tokens": [50364, 439, 295, 264, 721, 300, 366, 2737, 294, 264, 3518, 11, 456, 311, 341, 777, 14722, 3663, 50500], "temperature": 0.0, "avg_logprob": -0.09611467520395915, "compression_ratio": 1.703125, "no_speech_prob": 0.005218200385570526}, {"id": 36, "seek": 18928, "start": 192.0, "end": 198.88, "text": " coming. And it's not just about CPUs, or GPUs, or TPUs, or NPUs, or IPUs, or whatever,", "tokens": [50500, 1348, 13, 400, 309, 311, 406, 445, 466, 13199, 82, 11, 420, 18407, 82, 11, 420, 314, 8115, 82, 11, 420, 426, 8115, 82, 11, 420, 8671, 29211, 11, 420, 2035, 11, 50844], "temperature": 0.0, "avg_logprob": -0.09611467520395915, "compression_ratio": 1.703125, "no_speech_prob": 0.005218200385570526}, {"id": 37, "seek": 18928, "start": 198.88, "end": 205.92000000000002, "text": " all the PUs. It's about how do we program these things. And so for software folks like us,", "tokens": [50844, 439, 264, 430, 29211, 13, 467, 311, 466, 577, 360, 321, 1461, 613, 721, 13, 400, 370, 337, 4722, 4024, 411, 505, 11, 51196], "temperature": 0.0, "avg_logprob": -0.09611467520395915, "compression_ratio": 1.703125, "no_speech_prob": 0.005218200385570526}, {"id": 38, "seek": 18928, "start": 206.96, "end": 210.24, "text": " it doesn't do us any good if there's this amazing hardware that we can't use.", "tokens": [51248, 309, 1177, 380, 360, 505, 604, 665, 498, 456, 311, 341, 2243, 8837, 300, 321, 393, 380, 764, 13, 51412], "temperature": 0.0, "avg_logprob": -0.09611467520395915, "compression_ratio": 1.703125, "no_speech_prob": 0.005218200385570526}, {"id": 39, "seek": 18928, "start": 211.04, "end": 215.76, "text": " And one of the things you find out really quick is that having the theoretical capability of", "tokens": [51452, 400, 472, 295, 264, 721, 291, 915, 484, 534, 1702, 307, 300, 1419, 264, 20864, 13759, 295, 51688], "temperature": 0.0, "avg_logprob": -0.09611467520395915, "compression_ratio": 1.703125, "no_speech_prob": 0.005218200385570526}, {"id": 40, "seek": 21576, "start": 215.76, "end": 221.04, "text": " programming something, and then having the world's power and the innovation of all the", "tokens": [50364, 9410, 746, 11, 293, 550, 1419, 264, 1002, 311, 1347, 293, 264, 8504, 295, 439, 264, 50628], "temperature": 0.0, "avg_logprob": -0.08348731505565155, "compression_ratio": 1.756578947368421, "no_speech_prob": 0.007575657684355974}, {"id": 41, "seek": 21576, "start": 221.04, "end": 225.67999999999998, "text": " smart people in the world get unleashed on something can be quite different. And so really", "tokens": [50628, 4069, 561, 294, 264, 1002, 483, 25272, 12219, 322, 746, 393, 312, 1596, 819, 13, 400, 370, 534, 50860], "temperature": 0.0, "avg_logprob": -0.08348731505565155, "compression_ratio": 1.756578947368421, "no_speech_prob": 0.007575657684355974}, {"id": 42, "seek": 21576, "start": 225.67999999999998, "end": 230.88, "text": " where Mojo came from was starting from a problem of we need to be able to take machine learning,", "tokens": [50860, 689, 3335, 5134, 1361, 490, 390, 2891, 490, 257, 1154, 295, 321, 643, 281, 312, 1075, 281, 747, 3479, 2539, 11, 51120], "temperature": 0.0, "avg_logprob": -0.08348731505565155, "compression_ratio": 1.756578947368421, "no_speech_prob": 0.007575657684355974}, {"id": 43, "seek": 21576, "start": 230.88, "end": 234.72, "text": " take the infrastructure underneath it, and make it way more accessible, way more usable,", "tokens": [51120, 747, 264, 6896, 7223, 309, 11, 293, 652, 309, 636, 544, 9515, 11, 636, 544, 29975, 11, 51312], "temperature": 0.0, "avg_logprob": -0.08348731505565155, "compression_ratio": 1.756578947368421, "no_speech_prob": 0.007575657684355974}, {"id": 44, "seek": 21576, "start": 234.72, "end": 239.51999999999998, "text": " way more understandable by normal people and researchers and other folks that are not", "tokens": [51312, 636, 544, 25648, 538, 2710, 561, 293, 10309, 293, 661, 4024, 300, 366, 406, 51552], "temperature": 0.0, "avg_logprob": -0.08348731505565155, "compression_ratio": 1.756578947368421, "no_speech_prob": 0.007575657684355974}, {"id": 45, "seek": 21576, "start": 239.51999999999998, "end": 244.23999999999998, "text": " themselves like experts in GPUs and things like this. And then through that journey,", "tokens": [51552, 2969, 411, 8572, 294, 18407, 82, 293, 721, 411, 341, 13, 400, 550, 807, 300, 4671, 11, 51788], "temperature": 0.0, "avg_logprob": -0.08348731505565155, "compression_ratio": 1.756578947368421, "no_speech_prob": 0.007575657684355974}, {"id": 46, "seek": 24424, "start": 244.24, "end": 246.88, "text": " we realize, hey, we need syntax for this, we need to do a program language.", "tokens": [50364, 321, 4325, 11, 4177, 11, 321, 643, 28431, 337, 341, 11, 321, 643, 281, 360, 257, 1461, 2856, 13, 50496], "temperature": 0.0, "avg_logprob": -0.11384378020296392, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0031710765324532986}, {"id": 47, "seek": 24424, "start": 247.44, "end": 254.48000000000002, "text": " So one of the main features of the language, I say so fully in jest, is that it allows you to have", "tokens": [50524, 407, 472, 295, 264, 2135, 4122, 295, 264, 2856, 11, 286, 584, 370, 4498, 294, 3492, 11, 307, 300, 309, 4045, 291, 281, 362, 50876], "temperature": 0.0, "avg_logprob": -0.11384378020296392, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0031710765324532986}, {"id": 48, "seek": 24424, "start": 254.48000000000002, "end": 265.28000000000003, "text": " the file extension to be an emoji, or the fire emoji, which is one of the first emojis used", "tokens": [50876, 264, 3991, 10320, 281, 312, 364, 31595, 11, 420, 264, 2610, 31595, 11, 597, 307, 472, 295, 264, 700, 19611, 40371, 1143, 51416], "temperature": 0.0, "avg_logprob": -0.11384378020296392, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0031710765324532986}, {"id": 49, "seek": 24424, "start": 265.28000000000003, "end": 269.68, "text": " as a file extension I've ever seen in my life. And then you ask yourself the question, why in", "tokens": [51416, 382, 257, 3991, 10320, 286, 600, 1562, 1612, 294, 452, 993, 13, 400, 550, 291, 1029, 1803, 264, 1168, 11, 983, 294, 51636], "temperature": 0.0, "avg_logprob": -0.11384378020296392, "compression_ratio": 1.6822429906542056, "no_speech_prob": 0.0031710765324532986}, {"id": 50, "seek": 26968, "start": 269.68, "end": 276.40000000000003, "text": " the 21st century, are we not using Unicode for file extensions? This I mean, it's an epic decision.", "tokens": [50364, 264, 5080, 372, 4901, 11, 366, 321, 406, 1228, 1156, 299, 1429, 337, 3991, 25129, 30, 639, 286, 914, 11, 309, 311, 364, 13581, 3537, 13, 50700], "temperature": 0.0, "avg_logprob": -0.16123321874817806, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.1309131681919098}, {"id": 51, "seek": 26968, "start": 276.40000000000003, "end": 281.04, "text": " I think clearly the most important decision you made the most, but you could also just use Mojo", "tokens": [50700, 286, 519, 4448, 264, 881, 1021, 3537, 291, 1027, 264, 881, 11, 457, 291, 727, 611, 445, 764, 3335, 5134, 50932], "temperature": 0.0, "avg_logprob": -0.16123321874817806, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.1309131681919098}, {"id": 52, "seek": 26968, "start": 281.04, "end": 284.8, "text": " as the file extension. Well, so okay, so take a step back. I mean, come on, Lex, do you think that", "tokens": [50932, 382, 264, 3991, 10320, 13, 1042, 11, 370, 1392, 11, 370, 747, 257, 1823, 646, 13, 286, 914, 11, 808, 322, 11, 24086, 11, 360, 291, 519, 300, 51120], "temperature": 0.0, "avg_logprob": -0.16123321874817806, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.1309131681919098}, {"id": 53, "seek": 26968, "start": 284.8, "end": 288.72, "text": " the world's ready for this? This is a big moment in the world, right? This is we're releasing this", "tokens": [51120, 264, 1002, 311, 1919, 337, 341, 30, 639, 307, 257, 955, 1623, 294, 264, 1002, 11, 558, 30, 639, 307, 321, 434, 16327, 341, 51316], "temperature": 0.0, "avg_logprob": -0.16123321874817806, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.1309131681919098}, {"id": 54, "seek": 26968, "start": 288.72, "end": 295.2, "text": " onto the world. This is innovation. I mean, it really is kind of brilliant. Emojis are such a", "tokens": [51316, 3911, 264, 1002, 13, 639, 307, 8504, 13, 286, 914, 11, 309, 534, 307, 733, 295, 10248, 13, 462, 3280, 40371, 366, 1270, 257, 51640], "temperature": 0.0, "avg_logprob": -0.16123321874817806, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.1309131681919098}, {"id": 55, "seek": 29520, "start": 295.2, "end": 301.59999999999997, "text": " big part of our daily lives. Why isn't it not in programming? Well, and like you take a step back", "tokens": [50364, 955, 644, 295, 527, 5212, 2909, 13, 1545, 1943, 380, 309, 406, 294, 9410, 30, 1042, 11, 293, 411, 291, 747, 257, 1823, 646, 50684], "temperature": 0.0, "avg_logprob": -0.11671824645996094, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01242798287421465}, {"id": 56, "seek": 29520, "start": 301.59999999999997, "end": 306.88, "text": " and look at what file extensions are, right? They're basically metadata, right? And so why are", "tokens": [50684, 293, 574, 412, 437, 3991, 25129, 366, 11, 558, 30, 814, 434, 1936, 26603, 11, 558, 30, 400, 370, 983, 366, 50948], "temperature": 0.0, "avg_logprob": -0.11671824645996094, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01242798287421465}, {"id": 57, "seek": 29520, "start": 306.88, "end": 311.12, "text": " we spending all the screen space on them and all this stuff? Also, you know, you have them stacked", "tokens": [50948, 321, 6434, 439, 264, 2568, 1901, 322, 552, 293, 439, 341, 1507, 30, 2743, 11, 291, 458, 11, 291, 362, 552, 28867, 51160], "temperature": 0.0, "avg_logprob": -0.11671824645996094, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01242798287421465}, {"id": 58, "seek": 29520, "start": 311.12, "end": 315.12, "text": " up next to text files and PDF files and whatever else, like, if you're gonna do something cool,", "tokens": [51160, 493, 958, 281, 2487, 7098, 293, 17752, 7098, 293, 2035, 1646, 11, 411, 11, 498, 291, 434, 799, 360, 746, 1627, 11, 51360], "temperature": 0.0, "avg_logprob": -0.11671824645996094, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01242798287421465}, {"id": 59, "seek": 29520, "start": 315.12, "end": 319.84, "text": " you want to stand out, right? Emojis are colorful, they're visual, they're beautiful, right?", "tokens": [51360, 291, 528, 281, 1463, 484, 11, 558, 30, 462, 3280, 40371, 366, 18506, 11, 436, 434, 5056, 11, 436, 434, 2238, 11, 558, 30, 51596], "temperature": 0.0, "avg_logprob": -0.11671824645996094, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01242798287421465}, {"id": 60, "seek": 31984, "start": 319.84, "end": 325.35999999999996, "text": " What's been the response so far from, is there support on like Windows on operating systems", "tokens": [50364, 708, 311, 668, 264, 4134, 370, 1400, 490, 11, 307, 456, 1406, 322, 411, 8591, 322, 7447, 3652, 50640], "temperature": 0.0, "avg_logprob": -0.23061812219540934, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.08266906440258026}, {"id": 61, "seek": 31984, "start": 325.35999999999996, "end": 330.15999999999997, "text": " in displaying like File Explorer? Yeah, the one problem I've seen is that Git doesn't escape it", "tokens": [50640, 294, 36834, 411, 26196, 31895, 30, 865, 11, 264, 472, 1154, 286, 600, 1612, 307, 300, 16939, 1177, 380, 7615, 309, 50880], "temperature": 0.0, "avg_logprob": -0.23061812219540934, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.08266906440258026}, {"id": 62, "seek": 31984, "start": 330.15999999999997, "end": 334.56, "text": " right? And so it thinks that the Fire Emoji is unprintable. And so it like prints out weird", "tokens": [50880, 558, 30, 400, 370, 309, 7309, 300, 264, 7652, 462, 3280, 4013, 307, 517, 14030, 712, 13, 400, 370, 309, 411, 22305, 484, 3657, 51100], "temperature": 0.0, "avg_logprob": -0.23061812219540934, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.08266906440258026}, {"id": 63, "seek": 31984, "start": 334.56, "end": 339.12, "text": " hex things if you use the command line Git tool. But everything else as far as I'm aware works", "tokens": [51100, 23291, 721, 498, 291, 764, 264, 5622, 1622, 16939, 2290, 13, 583, 1203, 1646, 382, 1400, 382, 286, 478, 3650, 1985, 51328], "temperature": 0.0, "avg_logprob": -0.23061812219540934, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.08266906440258026}, {"id": 64, "seek": 31984, "start": 339.12, "end": 345.59999999999997, "text": " fine. And I have faith that Git can be improved. So GitHub is fine. GitHub is fine. Yep. GitHub is", "tokens": [51328, 2489, 13, 400, 286, 362, 4522, 300, 16939, 393, 312, 9689, 13, 407, 23331, 307, 2489, 13, 23331, 307, 2489, 13, 7010, 13, 23331, 307, 51652], "temperature": 0.0, "avg_logprob": -0.23061812219540934, "compression_ratio": 1.6480836236933798, "no_speech_prob": 0.08266906440258026}, {"id": 65, "seek": 34560, "start": 345.6, "end": 350.16, "text": " fine. Visual Studio Code, Windows, like all this stuff totally ready, because people have", "tokens": [50364, 2489, 13, 23187, 13500, 15549, 11, 8591, 11, 411, 439, 341, 1507, 3879, 1919, 11, 570, 561, 362, 50592], "temperature": 0.0, "avg_logprob": -0.13771433408568506, "compression_ratio": 1.6812865497076024, "no_speech_prob": 0.019717395305633545}, {"id": 66, "seek": 34560, "start": 350.16, "end": 355.52000000000004, "text": " internationalization in their normal part of their paths. So this is just like taking the next step,", "tokens": [50592, 5058, 2144, 294, 641, 2710, 644, 295, 641, 14518, 13, 407, 341, 307, 445, 411, 1940, 264, 958, 1823, 11, 50860], "temperature": 0.0, "avg_logprob": -0.13771433408568506, "compression_ratio": 1.6812865497076024, "no_speech_prob": 0.019717395305633545}, {"id": 67, "seek": 34560, "start": 355.52000000000004, "end": 362.48, "text": " right? Somewhere between, oh, wow, that makes sense. Cool. I like new things too. Oh my god,", "tokens": [50860, 558, 30, 34500, 1296, 11, 1954, 11, 6076, 11, 300, 1669, 2020, 13, 8561, 13, 286, 411, 777, 721, 886, 13, 876, 452, 3044, 11, 51208], "temperature": 0.0, "avg_logprob": -0.13771433408568506, "compression_ratio": 1.6812865497076024, "no_speech_prob": 0.019717395305633545}, {"id": 68, "seek": 34560, "start": 362.48, "end": 366.16, "text": " you're killing my baby. Like, what are you talking about? This can never be like, I can never hand", "tokens": [51208, 291, 434, 8011, 452, 3186, 13, 1743, 11, 437, 366, 291, 1417, 466, 30, 639, 393, 1128, 312, 411, 11, 286, 393, 1128, 1011, 51392], "temperature": 0.0, "avg_logprob": -0.13771433408568506, "compression_ratio": 1.6812865497076024, "no_speech_prob": 0.019717395305633545}, {"id": 69, "seek": 34560, "start": 366.16, "end": 371.12, "text": " it and list how am I going to type this like all these things. And so this is something where I", "tokens": [51392, 309, 293, 1329, 577, 669, 286, 516, 281, 2010, 341, 411, 439, 613, 721, 13, 400, 370, 341, 307, 746, 689, 286, 51640], "temperature": 0.0, "avg_logprob": -0.13771433408568506, "compression_ratio": 1.6812865497076024, "no_speech_prob": 0.019717395305633545}, {"id": 70, "seek": 34560, "start": 371.12, "end": 374.88, "text": " think that the world will get there. We don't have to bet the whole farm on this. I think we can", "tokens": [51640, 519, 300, 264, 1002, 486, 483, 456, 13, 492, 500, 380, 362, 281, 778, 264, 1379, 5421, 322, 341, 13, 286, 519, 321, 393, 51828], "temperature": 0.0, "avg_logprob": -0.13771433408568506, "compression_ratio": 1.6812865497076024, "no_speech_prob": 0.019717395305633545}, {"id": 71, "seek": 37488, "start": 375.44, "end": 381.44, "text": " provide both paths. But I think it'll be great. When can we have emojis as part of the code? I", "tokens": [50392, 2893, 1293, 14518, 13, 583, 286, 519, 309, 603, 312, 869, 13, 1133, 393, 321, 362, 19611, 40371, 382, 644, 295, 264, 3089, 30, 286, 50692], "temperature": 0.0, "avg_logprob": -0.11698966393103967, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0009398750262334943}, {"id": 72, "seek": 37488, "start": 381.44, "end": 386.4, "text": " wonder. Yeah, so I mean, lots of languages provide that. So I think that we have partial support", "tokens": [50692, 2441, 13, 865, 11, 370, 286, 914, 11, 3195, 295, 8650, 2893, 300, 13, 407, 286, 519, 300, 321, 362, 14641, 1406, 50940], "temperature": 0.0, "avg_logprob": -0.11698966393103967, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0009398750262334943}, {"id": 73, "seek": 37488, "start": 386.4, "end": 390.8, "text": " for that. It's probably not fully done yet. But but yeah, you can you can do that. For example,", "tokens": [50940, 337, 300, 13, 467, 311, 1391, 406, 4498, 1096, 1939, 13, 583, 457, 1338, 11, 291, 393, 291, 393, 360, 300, 13, 1171, 1365, 11, 51160], "temperature": 0.0, "avg_logprob": -0.11698966393103967, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0009398750262334943}, {"id": 74, "seek": 37488, "start": 390.8, "end": 397.28, "text": " in Swift, you can do that for sure. So an example we give gave it Apple was the dog cow. Yeah,", "tokens": [51160, 294, 25539, 11, 291, 393, 360, 300, 337, 988, 13, 407, 364, 1365, 321, 976, 2729, 309, 6373, 390, 264, 3000, 8408, 13, 865, 11, 51484], "temperature": 0.0, "avg_logprob": -0.11698966393103967, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0009398750262334943}, {"id": 75, "seek": 37488, "start": 397.28, "end": 401.52, "text": " so that's a classical Mac heritage thing. And so you use the dog and the cow emoji together.", "tokens": [51484, 370, 300, 311, 257, 13735, 5707, 16040, 551, 13, 400, 370, 291, 764, 264, 3000, 293, 264, 8408, 31595, 1214, 13, 51696], "temperature": 0.0, "avg_logprob": -0.11698966393103967, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0009398750262334943}, {"id": 76, "seek": 40152, "start": 401.52, "end": 404.79999999999995, "text": " And that could be your variable name. But of course, the internet went and made", "tokens": [50364, 400, 300, 727, 312, 428, 7006, 1315, 13, 583, 295, 1164, 11, 264, 4705, 1437, 293, 1027, 50528], "temperature": 0.0, "avg_logprob": -0.11431813873020949, "compression_ratio": 1.568840579710145, "no_speech_prob": 0.012428452260792255}, {"id": 77, "seek": 40152, "start": 404.79999999999995, "end": 409.2, "text": " pile of poop for everything. Yeah. So you know, if you want to name your function pile of poop,", "tokens": [50528, 14375, 295, 17153, 337, 1203, 13, 865, 13, 407, 291, 458, 11, 498, 291, 528, 281, 1315, 428, 2445, 14375, 295, 17153, 11, 50748], "temperature": 0.0, "avg_logprob": -0.11431813873020949, "compression_ratio": 1.568840579710145, "no_speech_prob": 0.012428452260792255}, {"id": 78, "seek": 40152, "start": 409.2, "end": 412.08, "text": " then you can totally go to town and see how that gets through code review.", "tokens": [50748, 550, 291, 393, 3879, 352, 281, 3954, 293, 536, 577, 300, 2170, 807, 3089, 3131, 13, 50892], "temperature": 0.0, "avg_logprob": -0.11431813873020949, "compression_ratio": 1.568840579710145, "no_speech_prob": 0.012428452260792255}, {"id": 79, "seek": 40152, "start": 414.24, "end": 421.12, "text": " Okay, so let me just ask a bunch of random questions. So is Mojo primarily designed for", "tokens": [51000, 1033, 11, 370, 718, 385, 445, 1029, 257, 3840, 295, 4974, 1651, 13, 407, 307, 3335, 5134, 10029, 4761, 337, 51344], "temperature": 0.0, "avg_logprob": -0.11431813873020949, "compression_ratio": 1.568840579710145, "no_speech_prob": 0.012428452260792255}, {"id": 80, "seek": 40152, "start": 421.12, "end": 426.88, "text": " AIs? Or is it a general purpose program? Good question. So it's AI first. And so AI is driving", "tokens": [51344, 316, 6802, 30, 1610, 307, 309, 257, 2674, 4334, 1461, 30, 2205, 1168, 13, 407, 309, 311, 7318, 700, 13, 400, 370, 7318, 307, 4840, 51632], "temperature": 0.0, "avg_logprob": -0.11431813873020949, "compression_ratio": 1.568840579710145, "no_speech_prob": 0.012428452260792255}, {"id": 81, "seek": 42688, "start": 426.96, "end": 432.96, "text": " a lot of the requirements. And so modular is building and designing and driving Mojo forward.", "tokens": [50368, 257, 688, 295, 264, 7728, 13, 400, 370, 31111, 307, 2390, 293, 14685, 293, 4840, 3335, 5134, 2128, 13, 50668], "temperature": 0.0, "avg_logprob": -0.08563577601339965, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.020957117900252342}, {"id": 82, "seek": 42688, "start": 432.96, "end": 438.08, "text": " And it's not because it's an interesting project theoretically to build, it's because we need it.", "tokens": [50668, 400, 309, 311, 406, 570, 309, 311, 364, 1880, 1716, 29400, 281, 1322, 11, 309, 311, 570, 321, 643, 309, 13, 50924], "temperature": 0.0, "avg_logprob": -0.08563577601339965, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.020957117900252342}, {"id": 83, "seek": 42688, "start": 439.2, "end": 443.84, "text": " And so modular, we're really tackling the AI infrastructure landscape and the big problems", "tokens": [50980, 400, 370, 31111, 11, 321, 434, 534, 34415, 264, 7318, 6896, 9661, 293, 264, 955, 2740, 51212], "temperature": 0.0, "avg_logprob": -0.08563577601339965, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.020957117900252342}, {"id": 84, "seek": 42688, "start": 443.84, "end": 449.28, "text": " in AI, and the reasons that it is so difficult to use and scale and adopt and deploy and like all", "tokens": [51212, 294, 7318, 11, 293, 264, 4112, 300, 309, 307, 370, 2252, 281, 764, 293, 4373, 293, 6878, 293, 7274, 293, 411, 439, 51484], "temperature": 0.0, "avg_logprob": -0.08563577601339965, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.020957117900252342}, {"id": 85, "seek": 42688, "start": 449.28, "end": 454.8, "text": " these big problems in AI. And so we're coming out from that perspective. Now, when you do that,", "tokens": [51484, 613, 955, 2740, 294, 7318, 13, 400, 370, 321, 434, 1348, 484, 490, 300, 4585, 13, 823, 11, 562, 291, 360, 300, 11, 51760], "temperature": 0.0, "avg_logprob": -0.08563577601339965, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.020957117900252342}, {"id": 86, "seek": 45480, "start": 454.8, "end": 460.24, "text": " when you start tackling these problems, you realize that the solution to these problems", "tokens": [50364, 562, 291, 722, 34415, 613, 2740, 11, 291, 4325, 300, 264, 3827, 281, 613, 2740, 50636], "temperature": 0.0, "avg_logprob": -0.08067741728665535, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001700444146990776}, {"id": 87, "seek": 45480, "start": 460.24, "end": 464.64, "text": " isn't actually an AI specific solution. And so while we're doing this, we're building Mojo to", "tokens": [50636, 1943, 380, 767, 364, 7318, 2685, 3827, 13, 400, 370, 1339, 321, 434, 884, 341, 11, 321, 434, 2390, 3335, 5134, 281, 50856], "temperature": 0.0, "avg_logprob": -0.08067741728665535, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001700444146990776}, {"id": 88, "seek": 45480, "start": 464.64, "end": 471.04, "text": " be a fully general programming language. And that means that you can obviously tackle GPUs and CPUs", "tokens": [50856, 312, 257, 4498, 2674, 9410, 2856, 13, 400, 300, 1355, 300, 291, 393, 2745, 14896, 18407, 82, 293, 13199, 82, 51176], "temperature": 0.0, "avg_logprob": -0.08067741728665535, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001700444146990776}, {"id": 89, "seek": 45480, "start": 471.04, "end": 476.16, "text": " and like these AI things, but it's also a really great way to build NumPy and other things like", "tokens": [51176, 293, 411, 613, 7318, 721, 11, 457, 309, 311, 611, 257, 534, 869, 636, 281, 1322, 22592, 47, 88, 293, 661, 721, 411, 51432], "temperature": 0.0, "avg_logprob": -0.08067741728665535, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001700444146990776}, {"id": 90, "seek": 45480, "start": 476.16, "end": 481.04, "text": " that or you know, just if you look at what many Python libraries are today, often they're a layer", "tokens": [51432, 300, 420, 291, 458, 11, 445, 498, 291, 574, 412, 437, 867, 15329, 15148, 366, 965, 11, 2049, 436, 434, 257, 4583, 51676], "temperature": 0.0, "avg_logprob": -0.08067741728665535, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.001700444146990776}, {"id": 91, "seek": 48104, "start": 481.04, "end": 487.20000000000005, "text": " of Python for the API, and they end up being C and C++ code underneath them. That's very true in AI,", "tokens": [50364, 295, 15329, 337, 264, 9362, 11, 293, 436, 917, 493, 885, 383, 293, 383, 25472, 3089, 7223, 552, 13, 663, 311, 588, 2074, 294, 7318, 11, 50672], "temperature": 0.0, "avg_logprob": -0.10232947805653447, "compression_ratio": 1.6654676258992807, "no_speech_prob": 0.00970471277832985}, {"id": 92, "seek": 48104, "start": 487.20000000000005, "end": 490.48, "text": " that's true in lots of other demands as well. And so anytime you see this pattern,", "tokens": [50672, 300, 311, 2074, 294, 3195, 295, 661, 15107, 382, 731, 13, 400, 370, 13038, 291, 536, 341, 5102, 11, 50836], "temperature": 0.0, "avg_logprob": -0.10232947805653447, "compression_ratio": 1.6654676258992807, "no_speech_prob": 0.00970471277832985}, {"id": 93, "seek": 48104, "start": 490.48, "end": 494.8, "text": " that's an opportunity for Mojo to help simplify the world and help people have one thing.", "tokens": [50836, 300, 311, 364, 2650, 337, 3335, 5134, 281, 854, 20460, 264, 1002, 293, 854, 561, 362, 472, 551, 13, 51052], "temperature": 0.0, "avg_logprob": -0.10232947805653447, "compression_ratio": 1.6654676258992807, "no_speech_prob": 0.00970471277832985}, {"id": 94, "seek": 48104, "start": 496.0, "end": 502.24, "text": " To optimize through simplification, by having one thing. So you mentioned modular. Mojo is", "tokens": [51112, 1407, 19719, 807, 6883, 3774, 11, 538, 1419, 472, 551, 13, 407, 291, 2835, 31111, 13, 3335, 5134, 307, 51424], "temperature": 0.0, "avg_logprob": -0.10232947805653447, "compression_ratio": 1.6654676258992807, "no_speech_prob": 0.00970471277832985}, {"id": 95, "seek": 48104, "start": 502.24, "end": 507.12, "text": " the programming language, modular is the whole software stack. So just over a year ago, we started", "tokens": [51424, 264, 9410, 2856, 11, 31111, 307, 264, 1379, 4722, 8630, 13, 407, 445, 670, 257, 1064, 2057, 11, 321, 1409, 51668], "temperature": 0.0, "avg_logprob": -0.10232947805653447, "compression_ratio": 1.6654676258992807, "no_speech_prob": 0.00970471277832985}, {"id": 96, "seek": 50712, "start": 507.2, "end": 512.4, "text": " this company called modular. What modular is about is it's about taking AI and up leveling it", "tokens": [50368, 341, 2237, 1219, 31111, 13, 708, 31111, 307, 466, 307, 309, 311, 466, 1940, 7318, 293, 493, 40617, 309, 50628], "temperature": 0.0, "avg_logprob": -0.11675200186485102, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.05257338285446167}, {"id": 97, "seek": 50712, "start": 512.96, "end": 519.36, "text": " into the next generation. And so if you take a step back, what's gone on in the last five, six,", "tokens": [50656, 666, 264, 958, 5125, 13, 400, 370, 498, 291, 747, 257, 1823, 646, 11, 437, 311, 2780, 322, 294, 264, 1036, 1732, 11, 2309, 11, 50976], "temperature": 0.0, "avg_logprob": -0.11675200186485102, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.05257338285446167}, {"id": 98, "seek": 50712, "start": 519.36, "end": 524.24, "text": " seven, eight years is that we've had things like TensorFlow and PyTorch and these other systems", "tokens": [50976, 3407, 11, 3180, 924, 307, 300, 321, 600, 632, 721, 411, 37624, 293, 9953, 51, 284, 339, 293, 613, 661, 3652, 51220], "temperature": 0.0, "avg_logprob": -0.11675200186485102, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.05257338285446167}, {"id": 99, "seek": 50712, "start": 524.24, "end": 529.52, "text": " come in, you've used them, you know this. And what's happened is these things have grown like crazy,", "tokens": [51220, 808, 294, 11, 291, 600, 1143, 552, 11, 291, 458, 341, 13, 400, 437, 311, 2011, 307, 613, 721, 362, 7709, 411, 3219, 11, 51484], "temperature": 0.0, "avg_logprob": -0.11675200186485102, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.05257338285446167}, {"id": 100, "seek": 50712, "start": 529.52, "end": 534.4, "text": " and they get tons of users, it's in production deployment scenarios, it's being used to power", "tokens": [51484, 293, 436, 483, 9131, 295, 5022, 11, 309, 311, 294, 4265, 19317, 15077, 11, 309, 311, 885, 1143, 281, 1347, 51728], "temperature": 0.0, "avg_logprob": -0.11675200186485102, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.05257338285446167}, {"id": 101, "seek": 53440, "start": 534.4, "end": 540.24, "text": " so many systems. AI is all around us now. It used to be controversial years ago, but now it's a", "tokens": [50364, 370, 867, 3652, 13, 7318, 307, 439, 926, 505, 586, 13, 467, 1143, 281, 312, 17323, 924, 2057, 11, 457, 586, 309, 311, 257, 50656], "temperature": 0.0, "avg_logprob": -0.09832595005508296, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.004197756759822369}, {"id": 102, "seek": 53440, "start": 540.24, "end": 547.04, "text": " thing. But the challenge with these systems is that they haven't always been thought out with", "tokens": [50656, 551, 13, 583, 264, 3430, 365, 613, 3652, 307, 300, 436, 2378, 380, 1009, 668, 1194, 484, 365, 50996], "temperature": 0.0, "avg_logprob": -0.09832595005508296, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.004197756759822369}, {"id": 103, "seek": 53440, "start": 547.04, "end": 553.92, "text": " current demands in mind. So you think about it, where were LLMs eight years ago? Well, they didn't", "tokens": [50996, 2190, 15107, 294, 1575, 13, 407, 291, 519, 466, 309, 11, 689, 645, 441, 43, 26386, 3180, 924, 2057, 30, 1042, 11, 436, 994, 380, 51340], "temperature": 0.0, "avg_logprob": -0.09832595005508296, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.004197756759822369}, {"id": 104, "seek": 53440, "start": 553.92, "end": 558.3199999999999, "text": " exist, right? AI has changed so much. And a lot of what people are doing today are very different", "tokens": [51340, 2514, 11, 558, 30, 7318, 575, 3105, 370, 709, 13, 400, 257, 688, 295, 437, 561, 366, 884, 965, 366, 588, 819, 51560], "temperature": 0.0, "avg_logprob": -0.09832595005508296, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.004197756759822369}, {"id": 105, "seek": 53440, "start": 558.3199999999999, "end": 562.72, "text": " than when these systems were built. And meanwhile, the hardware side of this has gotten into a huge", "tokens": [51560, 813, 562, 613, 3652, 645, 3094, 13, 400, 29252, 11, 264, 8837, 1252, 295, 341, 575, 5768, 666, 257, 2603, 51780], "temperature": 0.0, "avg_logprob": -0.09832595005508296, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.004197756759822369}, {"id": 106, "seek": 56272, "start": 562.72, "end": 567.2, "text": " mess. There's tons of new chips and accelerators and every big company's announcing a new chip every", "tokens": [50364, 2082, 13, 821, 311, 9131, 295, 777, 11583, 293, 10172, 3391, 293, 633, 955, 2237, 311, 28706, 257, 777, 11409, 633, 50588], "temperature": 0.0, "avg_logprob": -0.11415996551513671, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0041975160129368305}, {"id": 107, "seek": 56272, "start": 567.2, "end": 574.24, "text": " day it feels like. And so between that, you have like this moving system on one side, moving system", "tokens": [50588, 786, 309, 3417, 411, 13, 400, 370, 1296, 300, 11, 291, 362, 411, 341, 2684, 1185, 322, 472, 1252, 11, 2684, 1185, 50940], "temperature": 0.0, "avg_logprob": -0.11415996551513671, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0041975160129368305}, {"id": 108, "seek": 56272, "start": 574.24, "end": 577.9200000000001, "text": " on the other side, and it just turns into this gigantic mess, which makes it very difficult for", "tokens": [50940, 322, 264, 661, 1252, 11, 293, 309, 445, 4523, 666, 341, 26800, 2082, 11, 597, 1669, 309, 588, 2252, 337, 51124], "temperature": 0.0, "avg_logprob": -0.11415996551513671, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0041975160129368305}, {"id": 109, "seek": 56272, "start": 577.9200000000001, "end": 582.8000000000001, "text": " people to actually use AI, particularly in production deployment scenarios. And it's what", "tokens": [51124, 561, 281, 767, 764, 7318, 11, 4098, 294, 4265, 19317, 15077, 13, 400, 309, 311, 437, 51368], "temperature": 0.0, "avg_logprob": -0.11415996551513671, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0041975160129368305}, {"id": 110, "seek": 56272, "start": 582.8000000000001, "end": 586.88, "text": " modular students were helping build out that software stack to help solve some of those problems.", "tokens": [51368, 31111, 1731, 645, 4315, 1322, 484, 300, 4722, 8630, 281, 854, 5039, 512, 295, 729, 2740, 13, 51572], "temperature": 0.0, "avg_logprob": -0.11415996551513671, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0041975160129368305}, {"id": 111, "seek": 56272, "start": 586.88, "end": 590.24, "text": " So then people can be more productive and get more AI research into production.", "tokens": [51572, 407, 550, 561, 393, 312, 544, 13304, 293, 483, 544, 7318, 2132, 666, 4265, 13, 51740], "temperature": 0.0, "avg_logprob": -0.11415996551513671, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0041975160129368305}, {"id": 112, "seek": 59024, "start": 590.88, "end": 595.84, "text": " Now, what Mojo does is it's a really, really, really important piece of that. And so that is,", "tokens": [50396, 823, 11, 437, 3335, 5134, 775, 307, 309, 311, 257, 534, 11, 534, 11, 534, 1021, 2522, 295, 300, 13, 400, 370, 300, 307, 11, 50644], "temperature": 0.0, "avg_logprob": -0.11661005020141602, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.001754018827341497}, {"id": 113, "seek": 59024, "start": 596.4, "end": 600.5600000000001, "text": " you know, part of that engine and part of the technology that allows us to solve these problems.", "tokens": [50672, 291, 458, 11, 644, 295, 300, 2848, 293, 644, 295, 264, 2899, 300, 4045, 505, 281, 5039, 613, 2740, 13, 50880], "temperature": 0.0, "avg_logprob": -0.11661005020141602, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.001754018827341497}, {"id": 114, "seek": 59024, "start": 600.5600000000001, "end": 606.96, "text": " So Mojo is a programming language that allows you to do a high level program and the low level", "tokens": [50880, 407, 3335, 5134, 307, 257, 9410, 2856, 300, 4045, 291, 281, 360, 257, 1090, 1496, 1461, 293, 264, 2295, 1496, 51200], "temperature": 0.0, "avg_logprob": -0.11661005020141602, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.001754018827341497}, {"id": 115, "seek": 59024, "start": 606.96, "end": 612.88, "text": " programming, they do all kinds of programming in that spectrum that gets you closer and closer", "tokens": [51200, 9410, 11, 436, 360, 439, 3685, 295, 9410, 294, 300, 11143, 300, 2170, 291, 4966, 293, 4966, 51496], "temperature": 0.0, "avg_logprob": -0.11661005020141602, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.001754018827341497}, {"id": 116, "seek": 59024, "start": 612.88, "end": 616.32, "text": " to the hardware. So take a step back. So Lex, what do you love about Python?", "tokens": [51496, 281, 264, 8837, 13, 407, 747, 257, 1823, 646, 13, 407, 24086, 11, 437, 360, 291, 959, 466, 15329, 30, 51668], "temperature": 0.0, "avg_logprob": -0.11661005020141602, "compression_ratio": 1.7921568627450981, "no_speech_prob": 0.001754018827341497}, {"id": 117, "seek": 61632, "start": 616.96, "end": 623.6, "text": " Oh boy. Where do I begin? What is love? What do I love about Python?", "tokens": [50396, 876, 3237, 13, 2305, 360, 286, 1841, 30, 708, 307, 959, 30, 708, 360, 286, 959, 466, 15329, 30, 50728], "temperature": 0.0, "avg_logprob": -0.16191767454147338, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0037647122517228127}, {"id": 118, "seek": 61632, "start": 623.6, "end": 628.4000000000001, "text": " You're a guy who knows love. I know this. Yes. How intuitive it is.", "tokens": [50728, 509, 434, 257, 2146, 567, 3255, 959, 13, 286, 458, 341, 13, 1079, 13, 1012, 21769, 309, 307, 13, 50968], "temperature": 0.0, "avg_logprob": -0.16191767454147338, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0037647122517228127}, {"id": 119, "seek": 61632, "start": 630.96, "end": 633.5200000000001, "text": " How it feels like I'm writing natural language, English.", "tokens": [51096, 1012, 309, 3417, 411, 286, 478, 3579, 3303, 2856, 11, 3669, 13, 51224], "temperature": 0.0, "avg_logprob": -0.16191767454147338, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0037647122517228127}, {"id": 120, "seek": 61632, "start": 636.32, "end": 642.1600000000001, "text": " How when I can not just write but read other people's code somehow I can understand it faster.", "tokens": [51364, 1012, 562, 286, 393, 406, 445, 2464, 457, 1401, 661, 561, 311, 3089, 6063, 286, 393, 1223, 309, 4663, 13, 51656], "temperature": 0.0, "avg_logprob": -0.16191767454147338, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0037647122517228127}, {"id": 121, "seek": 64216, "start": 642.16, "end": 648.88, "text": " It's more condensed than other languages like ones I'm really familiar with like C++", "tokens": [50364, 467, 311, 544, 36398, 813, 661, 8650, 411, 2306, 286, 478, 534, 4963, 365, 411, 383, 25472, 50700], "temperature": 0.0, "avg_logprob": -0.19182650939277981, "compression_ratio": 1.5020242914979758, "no_speech_prob": 0.003323339158669114}, {"id": 122, "seek": 64216, "start": 649.52, "end": 656.48, "text": " and C. There's a bunch of sexy little features. Yeah. We'll probably talk about some of them,", "tokens": [50732, 293, 383, 13, 821, 311, 257, 3840, 295, 13701, 707, 4122, 13, 865, 13, 492, 603, 1391, 751, 466, 512, 295, 552, 11, 51080], "temperature": 0.0, "avg_logprob": -0.19182650939277981, "compression_ratio": 1.5020242914979758, "no_speech_prob": 0.003323339158669114}, {"id": 123, "seek": 64216, "start": 656.48, "end": 662.56, "text": " but list comprehensions and stuff like this. Also, and don't forget the entire ecosystem", "tokens": [51080, 457, 1329, 10753, 8302, 293, 1507, 411, 341, 13, 2743, 11, 293, 500, 380, 2870, 264, 2302, 11311, 51384], "temperature": 0.0, "avg_logprob": -0.19182650939277981, "compression_ratio": 1.5020242914979758, "no_speech_prob": 0.003323339158669114}, {"id": 124, "seek": 64216, "start": 662.56, "end": 666.16, "text": " of all the packages. Oh yeah, that's probably huge. Because there's always something. If you want to do", "tokens": [51384, 295, 439, 264, 17401, 13, 876, 1338, 11, 300, 311, 1391, 2603, 13, 1436, 456, 311, 1009, 746, 13, 759, 291, 528, 281, 360, 51564], "temperature": 0.0, "avg_logprob": -0.19182650939277981, "compression_ratio": 1.5020242914979758, "no_speech_prob": 0.003323339158669114}, {"id": 125, "seek": 66616, "start": 666.16, "end": 672.16, "text": " anything, there's always a package. Yeah. So it's not just the ecosystem of the packages", "tokens": [50364, 1340, 11, 456, 311, 1009, 257, 7372, 13, 865, 13, 407, 309, 311, 406, 445, 264, 11311, 295, 264, 17401, 50664], "temperature": 0.0, "avg_logprob": -0.1375402993169324, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.13110344111919403}, {"id": 126, "seek": 66616, "start": 672.16, "end": 677.52, "text": " and the ecosystem of the humans that do it. That's a really, that's an interesting dynamic.", "tokens": [50664, 293, 264, 11311, 295, 264, 6255, 300, 360, 309, 13, 663, 311, 257, 534, 11, 300, 311, 364, 1880, 8546, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1375402993169324, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.13110344111919403}, {"id": 127, "seek": 66616, "start": 677.52, "end": 684.0, "text": " That's good. Because I think something about the usability and the ecosystem makes the thing viral.", "tokens": [50932, 663, 311, 665, 13, 1436, 286, 519, 746, 466, 264, 46878, 293, 264, 11311, 1669, 264, 551, 16132, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1375402993169324, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.13110344111919403}, {"id": 128, "seek": 66616, "start": 684.0, "end": 688.48, "text": " It grows and then it's a virtuous cycle, I think. Well, there's many things that went into that.", "tokens": [51256, 467, 13156, 293, 550, 309, 311, 257, 48918, 6586, 11, 286, 519, 13, 1042, 11, 456, 311, 867, 721, 300, 1437, 666, 300, 13, 51480], "temperature": 0.0, "avg_logprob": -0.1375402993169324, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.13110344111919403}, {"id": 129, "seek": 66616, "start": 688.48, "end": 692.9599999999999, "text": " Like, so I think that ML was very good for Python. And so I think that TensorFlow and", "tokens": [51480, 1743, 11, 370, 286, 519, 300, 21601, 390, 588, 665, 337, 15329, 13, 400, 370, 286, 519, 300, 37624, 293, 51704], "temperature": 0.0, "avg_logprob": -0.1375402993169324, "compression_ratio": 1.7945736434108528, "no_speech_prob": 0.13110344111919403}, {"id": 130, "seek": 69296, "start": 693.0400000000001, "end": 699.36, "text": " PyTorch and these systems embracing Python really took and helped Python grow. But I think that the", "tokens": [50368, 9953, 51, 284, 339, 293, 613, 3652, 31596, 15329, 534, 1890, 293, 4254, 15329, 1852, 13, 583, 286, 519, 300, 264, 50684], "temperature": 0.0, "avg_logprob": -0.09999285902932425, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.031128032132983208}, {"id": 131, "seek": 69296, "start": 699.36, "end": 704.64, "text": " major thing underlying it is that Python is like the universal connector. It really helps bring", "tokens": [50684, 2563, 551, 14217, 309, 307, 300, 15329, 307, 411, 264, 11455, 19127, 13, 467, 534, 3665, 1565, 50948], "temperature": 0.0, "avg_logprob": -0.09999285902932425, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.031128032132983208}, {"id": 132, "seek": 69296, "start": 704.64, "end": 708.4000000000001, "text": " together lots of different systems so you can compose them and build out larger systems without", "tokens": [50948, 1214, 3195, 295, 819, 3652, 370, 291, 393, 35925, 552, 293, 1322, 484, 4833, 3652, 1553, 51136], "temperature": 0.0, "avg_logprob": -0.09999285902932425, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.031128032132983208}, {"id": 133, "seek": 69296, "start": 708.4000000000001, "end": 713.9200000000001, "text": " having to understand how it works. But then what is the problem with Python? Well, I guess you", "tokens": [51136, 1419, 281, 1223, 577, 309, 1985, 13, 583, 550, 437, 307, 264, 1154, 365, 15329, 30, 1042, 11, 286, 2041, 291, 51412], "temperature": 0.0, "avg_logprob": -0.09999285902932425, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.031128032132983208}, {"id": 134, "seek": 69296, "start": 713.9200000000001, "end": 718.5600000000001, "text": " could say several things, but probably that it's slow. I think that's usually what people complain", "tokens": [51412, 727, 584, 2940, 721, 11, 457, 1391, 300, 309, 311, 2964, 13, 286, 519, 300, 311, 2673, 437, 561, 11024, 51644], "temperature": 0.0, "avg_logprob": -0.09999285902932425, "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.031128032132983208}, {"id": 135, "seek": 71856, "start": 718.64, "end": 724.8, "text": " about. And so other people complain about tabs in spaces versus curly braces or whatever. But", "tokens": [50368, 466, 13, 400, 370, 661, 561, 11024, 466, 20743, 294, 7673, 5717, 32066, 41537, 420, 2035, 13, 583, 50676], "temperature": 0.0, "avg_logprob": -0.21058374292710247, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.06556861102581024}, {"id": 136, "seek": 71856, "start": 725.68, "end": 729.76, "text": " those people are just wrong because it is actually just better to use an indentation.", "tokens": [50720, 729, 561, 366, 445, 2085, 570, 309, 307, 767, 445, 1101, 281, 764, 364, 44494, 399, 13, 50924], "temperature": 0.0, "avg_logprob": -0.21058374292710247, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.06556861102581024}, {"id": 137, "seek": 71856, "start": 731.1999999999999, "end": 736.2399999999999, "text": " Wow, strong words. So actually, I'm a small tangent. Let's actually take that. Let's take", "tokens": [50996, 3153, 11, 2068, 2283, 13, 407, 767, 11, 286, 478, 257, 1359, 27747, 13, 961, 311, 767, 747, 300, 13, 961, 311, 747, 51248], "temperature": 0.0, "avg_logprob": -0.21058374292710247, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.06556861102581024}, {"id": 138, "seek": 71856, "start": 736.2399999999999, "end": 739.1999999999999, "text": " all kinds of tangents. Oh, come on, Lex. You can push me on it. I could take it.", "tokens": [51248, 439, 3685, 295, 10266, 791, 13, 876, 11, 808, 322, 11, 24086, 13, 509, 393, 2944, 385, 322, 309, 13, 286, 727, 747, 309, 13, 51396], "temperature": 0.0, "avg_logprob": -0.21058374292710247, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.06556861102581024}, {"id": 139, "seek": 71856, "start": 739.1999999999999, "end": 746.7199999999999, "text": " Design. Listen, I've recently left Emacs for VS Code. The kind of hate mail I had to receive.", "tokens": [51396, 12748, 13, 7501, 11, 286, 600, 3938, 1411, 3968, 44937, 337, 25091, 15549, 13, 440, 733, 295, 4700, 10071, 286, 632, 281, 4774, 13, 51772], "temperature": 0.0, "avg_logprob": -0.21058374292710247, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.06556861102581024}, {"id": 140, "seek": 74672, "start": 746.72, "end": 752.96, "text": " Because on the way to doing that, I also said I've considered Vim and chose not to and went with", "tokens": [50364, 1436, 322, 264, 636, 281, 884, 300, 11, 286, 611, 848, 286, 600, 4888, 691, 332, 293, 5111, 406, 281, 293, 1437, 365, 50676], "temperature": 0.0, "avg_logprob": -0.13333990218791555, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.005299267824739218}, {"id": 141, "seek": 74672, "start": 752.96, "end": 759.28, "text": " VS Code. You're touching on deep religions, right? Anyway, tabs is an interesting design decision.", "tokens": [50676, 25091, 15549, 13, 509, 434, 11175, 322, 2452, 21212, 11, 558, 30, 5684, 11, 20743, 307, 364, 1880, 1715, 3537, 13, 50992], "temperature": 0.0, "avg_logprob": -0.13333990218791555, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.005299267824739218}, {"id": 142, "seek": 74672, "start": 759.28, "end": 765.6800000000001, "text": " And so you've really written a new programming language here. Yes, it is a superset of Python,", "tokens": [50992, 400, 370, 291, 600, 534, 3720, 257, 777, 9410, 2856, 510, 13, 1079, 11, 309, 307, 257, 37906, 302, 295, 15329, 11, 51312], "temperature": 0.0, "avg_logprob": -0.13333990218791555, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.005299267824739218}, {"id": 143, "seek": 74672, "start": 765.6800000000001, "end": 769.84, "text": " but you can make a bunch of different interesting decisions here. Totally. And you chose actually", "tokens": [51312, 457, 291, 393, 652, 257, 3840, 295, 819, 1880, 5327, 510, 13, 22837, 13, 400, 291, 5111, 767, 51520], "temperature": 0.0, "avg_logprob": -0.13333990218791555, "compression_ratio": 1.5582329317269077, "no_speech_prob": 0.005299267824739218}, {"id": 144, "seek": 76984, "start": 769.84, "end": 777.0400000000001, "text": " to stick with Python in terms of some of the syntax. So let me explain why. So", "tokens": [50364, 281, 2897, 365, 15329, 294, 2115, 295, 512, 295, 264, 28431, 13, 407, 718, 385, 2903, 983, 13, 407, 50724], "temperature": 0.0, "avg_logprob": -0.12540252312369968, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.08031630516052246}, {"id": 145, "seek": 76984, "start": 778.48, "end": 784.48, "text": " I mean, you can explain this in many rational ways. I think that the indentation is beautiful,", "tokens": [50796, 286, 914, 11, 291, 393, 2903, 341, 294, 867, 15090, 2098, 13, 286, 519, 300, 264, 44494, 399, 307, 2238, 11, 51096], "temperature": 0.0, "avg_logprob": -0.12540252312369968, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.08031630516052246}, {"id": 146, "seek": 76984, "start": 784.48, "end": 788.8000000000001, "text": " but that's not a rational explanation. But I can defend it rationally. So first of all,", "tokens": [51096, 457, 300, 311, 406, 257, 15090, 10835, 13, 583, 286, 393, 8602, 309, 24258, 379, 13, 407, 700, 295, 439, 11, 51312], "temperature": 0.0, "avg_logprob": -0.12540252312369968, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.08031630516052246}, {"id": 147, "seek": 76984, "start": 789.44, "end": 795.2, "text": " Python 1 has millions of programmers. It's huge. It's everywhere. It owns machine learning. So", "tokens": [51344, 15329, 502, 575, 6803, 295, 41504, 13, 467, 311, 2603, 13, 467, 311, 5315, 13, 467, 19143, 3479, 2539, 13, 407, 51632], "temperature": 0.0, "avg_logprob": -0.12540252312369968, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.08031630516052246}, {"id": 148, "seek": 79520, "start": 796.0, "end": 801.84, "text": " factually, it is the thing, right? Second of all, if you look at it, C codes, C++ code, Java,", "tokens": [50404, 1186, 671, 11, 309, 307, 264, 551, 11, 558, 30, 5736, 295, 439, 11, 498, 291, 574, 412, 309, 11, 383, 14211, 11, 383, 25472, 3089, 11, 10745, 11, 50696], "temperature": 0.0, "avg_logprob": -0.12197789278897372, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0005357529735192657}, {"id": 149, "seek": 79520, "start": 801.84, "end": 807.9200000000001, "text": " whatever, Swift, curly brace languages also run through formatting tools and get indented.", "tokens": [50696, 2035, 11, 25539, 11, 32066, 38458, 8650, 611, 1190, 807, 39366, 3873, 293, 483, 1016, 6003, 13, 51000], "temperature": 0.0, "avg_logprob": -0.12197789278897372, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0005357529735192657}, {"id": 150, "seek": 79520, "start": 809.0400000000001, "end": 813.0400000000001, "text": " And so if they're not indented correctly, first of all, it will twist your brain around.", "tokens": [51056, 400, 370, 498, 436, 434, 406, 1016, 6003, 8944, 11, 700, 295, 439, 11, 309, 486, 8203, 428, 3567, 926, 13, 51256], "temperature": 0.0, "avg_logprob": -0.12197789278897372, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0005357529735192657}, {"id": 151, "seek": 79520, "start": 814.0, "end": 818.72, "text": " It can lead to bugs. There's notorious bugs that have happened across time where the indentation", "tokens": [51304, 467, 393, 1477, 281, 15120, 13, 821, 311, 38045, 15120, 300, 362, 2011, 2108, 565, 689, 264, 44494, 399, 51540], "temperature": 0.0, "avg_logprob": -0.12197789278897372, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0005357529735192657}, {"id": 152, "seek": 79520, "start": 818.72, "end": 823.44, "text": " was wrong or misleading and it wasn't formatted, right? And so it turned into an issue, right?", "tokens": [51540, 390, 2085, 420, 36429, 293, 309, 2067, 380, 1254, 32509, 11, 558, 30, 400, 370, 309, 3574, 666, 364, 2734, 11, 558, 30, 51776], "temperature": 0.0, "avg_logprob": -0.12197789278897372, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0005357529735192657}, {"id": 153, "seek": 82344, "start": 823.44, "end": 828.32, "text": " And so what ends up happening in modern large-scale code bases is people run automatic formatters.", "tokens": [50364, 400, 370, 437, 5314, 493, 2737, 294, 4363, 2416, 12, 20033, 3089, 17949, 307, 561, 1190, 12509, 1254, 37690, 13, 50608], "temperature": 0.0, "avg_logprob": -0.12103178462044137, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001501000253483653}, {"id": 154, "seek": 82344, "start": 829.0400000000001, "end": 834.0, "text": " So now what you end up with is indentation and curly braces. Well, if you're going to have", "tokens": [50644, 407, 586, 437, 291, 917, 493, 365, 307, 44494, 399, 293, 32066, 41537, 13, 1042, 11, 498, 291, 434, 516, 281, 362, 50892], "temperature": 0.0, "avg_logprob": -0.12103178462044137, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001501000253483653}, {"id": 155, "seek": 82344, "start": 836.32, "end": 841.36, "text": " the notion of grouping, why not have one thing, right? And get rid of all the clutter and have", "tokens": [51008, 264, 10710, 295, 40149, 11, 983, 406, 362, 472, 551, 11, 558, 30, 400, 483, 3973, 295, 439, 264, 40614, 293, 362, 51260], "temperature": 0.0, "avg_logprob": -0.12103178462044137, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001501000253483653}, {"id": 156, "seek": 82344, "start": 841.36, "end": 844.48, "text": " a more beautiful thing, right? Also, you look at many of these languages, it's like, okay, well,", "tokens": [51260, 257, 544, 2238, 551, 11, 558, 30, 2743, 11, 291, 574, 412, 867, 295, 613, 8650, 11, 309, 311, 411, 11, 1392, 11, 731, 11, 51416], "temperature": 0.0, "avg_logprob": -0.12103178462044137, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001501000253483653}, {"id": 157, "seek": 82344, "start": 844.48, "end": 848.8800000000001, "text": " you can have curly braces or you can omit them if there's one statement or you just enter this", "tokens": [51416, 291, 393, 362, 32066, 41537, 420, 291, 393, 3406, 270, 552, 498, 456, 311, 472, 5629, 420, 291, 445, 3242, 341, 51636], "temperature": 0.0, "avg_logprob": -0.12103178462044137, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.001501000253483653}, {"id": 158, "seek": 84888, "start": 848.88, "end": 853.84, "text": " entire world of complicated design space that objectively you don't need if you have Python", "tokens": [50364, 2302, 1002, 295, 6179, 1715, 1901, 300, 46067, 291, 500, 380, 643, 498, 291, 362, 15329, 50612], "temperature": 0.0, "avg_logprob": -0.12575194795252914, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.018544072285294533}, {"id": 159, "seek": 84888, "start": 853.84, "end": 858.32, "text": " style indentation. So yeah, I would love to actually see statistics on errors made because", "tokens": [50612, 3758, 44494, 399, 13, 407, 1338, 11, 286, 576, 959, 281, 767, 536, 12523, 322, 13603, 1027, 570, 50836], "temperature": 0.0, "avg_logprob": -0.12575194795252914, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.018544072285294533}, {"id": 160, "seek": 84888, "start": 858.32, "end": 864.88, "text": " of indentation. Like how many errors are made in Python versus in C++ that have to do with basic", "tokens": [50836, 295, 44494, 399, 13, 1743, 577, 867, 13603, 366, 1027, 294, 15329, 5717, 294, 383, 25472, 300, 362, 281, 360, 365, 3875, 51164], "temperature": 0.0, "avg_logprob": -0.12575194795252914, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.018544072285294533}, {"id": 161, "seek": 84888, "start": 864.88, "end": 868.88, "text": " formatting, all that kind of stuff. I would love to see. I think it's probably pretty minor because", "tokens": [51164, 39366, 11, 439, 300, 733, 295, 1507, 13, 286, 576, 959, 281, 536, 13, 286, 519, 309, 311, 1391, 1238, 6696, 570, 51364], "temperature": 0.0, "avg_logprob": -0.12575194795252914, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.018544072285294533}, {"id": 162, "seek": 84888, "start": 868.88, "end": 874.08, "text": " once you get like, you use VS code, I do too. So if you get VS code set up, it does the indentation", "tokens": [51364, 1564, 291, 483, 411, 11, 291, 764, 25091, 3089, 11, 286, 360, 886, 13, 407, 498, 291, 483, 25091, 3089, 992, 493, 11, 309, 775, 264, 44494, 399, 51624], "temperature": 0.0, "avg_logprob": -0.12575194795252914, "compression_ratio": 1.704626334519573, "no_speech_prob": 0.018544072285294533}, {"id": 163, "seek": 87408, "start": 874.08, "end": 877.9200000000001, "text": " for you generally. Right. And so you don't, you know, it's actually really nice to not have to", "tokens": [50364, 337, 291, 5101, 13, 1779, 13, 400, 370, 291, 500, 380, 11, 291, 458, 11, 309, 311, 767, 534, 1481, 281, 406, 362, 281, 50556], "temperature": 0.0, "avg_logprob": -0.10509781245767635, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.07583362609148026}, {"id": 164, "seek": 87408, "start": 878.5600000000001, "end": 883.6800000000001, "text": " fight it. And then what you can see is the editor is telling you how your code will work by indenting", "tokens": [50588, 2092, 309, 13, 400, 550, 437, 291, 393, 536, 307, 264, 9839, 307, 3585, 291, 577, 428, 3089, 486, 589, 538, 44494, 278, 50844], "temperature": 0.0, "avg_logprob": -0.10509781245767635, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.07583362609148026}, {"id": 165, "seek": 87408, "start": 883.6800000000001, "end": 891.2800000000001, "text": " it, which I think is pretty cool. I honestly don't think I've ever, I don't remember having an error", "tokens": [50844, 309, 11, 597, 286, 519, 307, 1238, 1627, 13, 286, 6095, 500, 380, 519, 286, 600, 1562, 11, 286, 500, 380, 1604, 1419, 364, 6713, 51224], "temperature": 0.0, "avg_logprob": -0.10509781245767635, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.07583362609148026}, {"id": 166, "seek": 87408, "start": 891.2800000000001, "end": 895.6, "text": " in Python because I indented stuff wrong. So I mean, I think that there's, again, this is a religious", "tokens": [51224, 294, 15329, 570, 286, 1016, 6003, 1507, 2085, 13, 407, 286, 914, 11, 286, 519, 300, 456, 311, 11, 797, 11, 341, 307, 257, 7185, 51440], "temperature": 0.0, "avg_logprob": -0.10509781245767635, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.07583362609148026}, {"id": 167, "seek": 87408, "start": 895.6, "end": 901.2, "text": " thing. And so I can joke about it. And I love, I love to kind of, you know, I realize that this is", "tokens": [51440, 551, 13, 400, 370, 286, 393, 7647, 466, 309, 13, 400, 286, 959, 11, 286, 959, 281, 733, 295, 11, 291, 458, 11, 286, 4325, 300, 341, 307, 51720], "temperature": 0.0, "avg_logprob": -0.10509781245767635, "compression_ratio": 1.7291666666666667, "no_speech_prob": 0.07583362609148026}, {"id": 168, "seek": 90120, "start": 901.2, "end": 905.5200000000001, "text": " such a polarizing thing and everybody wants to argue about it. And so I like poking at the bear", "tokens": [50364, 1270, 257, 12367, 3319, 551, 293, 2201, 2738, 281, 9695, 466, 309, 13, 400, 370, 286, 411, 42684, 412, 264, 6155, 50580], "temperature": 0.0, "avg_logprob": -0.11117736866932042, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.009410982020199299}, {"id": 169, "seek": 90120, "start": 905.5200000000001, "end": 910.32, "text": " a little bit, right? But, but frankly, right, come back to the first point, Python one,", "tokens": [50580, 257, 707, 857, 11, 558, 30, 583, 11, 457, 11939, 11, 558, 11, 808, 646, 281, 264, 700, 935, 11, 15329, 472, 11, 50820], "temperature": 0.0, "avg_logprob": -0.11117736866932042, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.009410982020199299}, {"id": 170, "seek": 90120, "start": 910.32, "end": 915.12, "text": " like it's huge, it's an AI, it's the right thing. For us, like we see mojo as being an incredible", "tokens": [50820, 411, 309, 311, 2603, 11, 309, 311, 364, 7318, 11, 309, 311, 264, 558, 551, 13, 1171, 505, 11, 411, 321, 536, 705, 5134, 382, 885, 364, 4651, 51060], "temperature": 0.0, "avg_logprob": -0.11117736866932042, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.009410982020199299}, {"id": 171, "seek": 90120, "start": 915.12, "end": 920.32, "text": " part of the Python ecosystem, we're not looking to break Python or change it or quote unquote,", "tokens": [51060, 644, 295, 264, 15329, 11311, 11, 321, 434, 406, 1237, 281, 1821, 15329, 420, 1319, 309, 420, 6513, 37557, 11, 51320], "temperature": 0.0, "avg_logprob": -0.11117736866932042, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.009410982020199299}, {"id": 172, "seek": 90120, "start": 920.32, "end": 925.2, "text": " fix it. We love Python for what it is. Our view is that Python is just not done yet.", "tokens": [51320, 3191, 309, 13, 492, 959, 15329, 337, 437, 309, 307, 13, 2621, 1910, 307, 300, 15329, 307, 445, 406, 1096, 1939, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11117736866932042, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.009410982020199299}, {"id": 173, "seek": 90120, "start": 926.24, "end": 929.36, "text": " And so if you look at, you know, you mentioned Python being slow, well, there's a couple of", "tokens": [51616, 400, 370, 498, 291, 574, 412, 11, 291, 458, 11, 291, 2835, 15329, 885, 2964, 11, 731, 11, 456, 311, 257, 1916, 295, 51772], "temperature": 0.0, "avg_logprob": -0.11117736866932042, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.009410982020199299}, {"id": 174, "seek": 92936, "start": 929.36, "end": 932.96, "text": " different things that go into that, which we can talk about if you want. But one of them is it just", "tokens": [50364, 819, 721, 300, 352, 666, 300, 11, 597, 321, 393, 751, 466, 498, 291, 528, 13, 583, 472, 295, 552, 307, 309, 445, 50544], "temperature": 0.0, "avg_logprob": -0.0914876912933549, "compression_ratio": 1.8157099697885197, "no_speech_prob": 0.015420754440128803}, {"id": 175, "seek": 92936, "start": 932.96, "end": 938.48, "text": " doesn't have those features that you would use to do C like programming. And so if you say, okay,", "tokens": [50544, 1177, 380, 362, 729, 4122, 300, 291, 576, 764, 281, 360, 383, 411, 9410, 13, 400, 370, 498, 291, 584, 11, 1392, 11, 50820], "temperature": 0.0, "avg_logprob": -0.0914876912933549, "compression_ratio": 1.8157099697885197, "no_speech_prob": 0.015420754440128803}, {"id": 176, "seek": 92936, "start": 938.48, "end": 943.76, "text": " well, I'm forced out of Python into C for certain use cases. Well, then what we're doing is we're", "tokens": [50820, 731, 11, 286, 478, 7579, 484, 295, 15329, 666, 383, 337, 1629, 764, 3331, 13, 1042, 11, 550, 437, 321, 434, 884, 307, 321, 434, 51084], "temperature": 0.0, "avg_logprob": -0.0914876912933549, "compression_ratio": 1.8157099697885197, "no_speech_prob": 0.015420754440128803}, {"id": 177, "seek": 92936, "start": 943.76, "end": 948.64, "text": " saying, okay, well, why, why is that? Can we just add those features that are missing from Python back", "tokens": [51084, 1566, 11, 1392, 11, 731, 11, 983, 11, 983, 307, 300, 30, 1664, 321, 445, 909, 729, 4122, 300, 366, 5361, 490, 15329, 646, 51328], "temperature": 0.0, "avg_logprob": -0.0914876912933549, "compression_ratio": 1.8157099697885197, "no_speech_prob": 0.015420754440128803}, {"id": 178, "seek": 92936, "start": 948.64, "end": 952.88, "text": " up to mojo? And then you can have everything that's great about Python, all the things you're talking", "tokens": [51328, 493, 281, 705, 5134, 30, 400, 550, 291, 393, 362, 1203, 300, 311, 869, 466, 15329, 11, 439, 264, 721, 291, 434, 1417, 51540], "temperature": 0.0, "avg_logprob": -0.0914876912933549, "compression_ratio": 1.8157099697885197, "no_speech_prob": 0.015420754440128803}, {"id": 179, "seek": 92936, "start": 952.88, "end": 959.04, "text": " about the love, plus not be forced out of it when you do something a little bit more computationally", "tokens": [51540, 466, 264, 959, 11, 1804, 406, 312, 7579, 484, 295, 309, 562, 291, 360, 746, 257, 707, 857, 544, 24903, 379, 51848], "temperature": 0.0, "avg_logprob": -0.0914876912933549, "compression_ratio": 1.8157099697885197, "no_speech_prob": 0.015420754440128803}, {"id": 180, "seek": 95904, "start": 959.12, "end": 962.3199999999999, "text": " intense or weird or hardwarey or whatever it is that you're doing.", "tokens": [50368, 9447, 420, 3657, 420, 8837, 88, 420, 2035, 309, 307, 300, 291, 434, 884, 13, 50528], "temperature": 0.0, "avg_logprob": -0.1710679720988316, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.00033530304790474474}, {"id": 181, "seek": 95904, "start": 962.88, "end": 967.5999999999999, "text": " Well, a million questions. I want to ask what high level again, is it compiled or is it an", "tokens": [50556, 1042, 11, 257, 2459, 1651, 13, 286, 528, 281, 1029, 437, 1090, 1496, 797, 11, 307, 309, 36548, 420, 307, 309, 364, 50792], "temperature": 0.0, "avg_logprob": -0.1710679720988316, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.00033530304790474474}, {"id": 182, "seek": 95904, "start": 967.5999999999999, "end": 972.24, "text": " interpretive language? So Python is just in time compilation. What's, what's mojo?", "tokens": [50792, 7302, 488, 2856, 30, 407, 15329, 307, 445, 294, 565, 40261, 13, 708, 311, 11, 437, 311, 705, 5134, 30, 51024], "temperature": 0.0, "avg_logprob": -0.1710679720988316, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.00033530304790474474}, {"id": 183, "seek": 95904, "start": 973.8399999999999, "end": 978.3199999999999, "text": " So mojo, the complicated answer does all the things. So it's interpreted, it's just compiled", "tokens": [51104, 407, 705, 5134, 11, 264, 6179, 1867, 775, 439, 264, 721, 13, 407, 309, 311, 26749, 11, 309, 311, 445, 36548, 51328], "temperature": 0.0, "avg_logprob": -0.1710679720988316, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.00033530304790474474}, {"id": 184, "seek": 95904, "start": 978.3199999999999, "end": 986.0, "text": " and it's statically compiled. And so this is for a variety of reasons. So one of the things that", "tokens": [51328, 293, 309, 311, 2219, 984, 36548, 13, 400, 370, 341, 307, 337, 257, 5673, 295, 4112, 13, 407, 472, 295, 264, 721, 300, 51712], "temperature": 0.0, "avg_logprob": -0.1710679720988316, "compression_ratio": 1.6929133858267718, "no_speech_prob": 0.00033530304790474474}, {"id": 185, "seek": 98600, "start": 986.0, "end": 991.76, "text": " makes Python beautiful is that it's very dynamic. And because it's dynamic, one of the things they", "tokens": [50364, 1669, 15329, 2238, 307, 300, 309, 311, 588, 8546, 13, 400, 570, 309, 311, 8546, 11, 472, 295, 264, 721, 436, 50652], "temperature": 0.0, "avg_logprob": -0.07402948349241227, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.003272515255957842}, {"id": 186, "seek": 98600, "start": 991.76, "end": 996.32, "text": " added is that it has this powerful metaprogramming feature. And so if you look at something like", "tokens": [50652, 3869, 307, 300, 309, 575, 341, 4005, 1131, 569, 340, 1342, 2810, 4111, 13, 400, 370, 498, 291, 574, 412, 746, 411, 50880], "temperature": 0.0, "avg_logprob": -0.07402948349241227, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.003272515255957842}, {"id": 187, "seek": 98600, "start": 996.32, "end": 1003.04, "text": " PyTorch or TensorFlow, or, or, I mean, even a simple, simple use case, like you'd find a class", "tokens": [50880, 9953, 51, 284, 339, 420, 37624, 11, 420, 11, 420, 11, 286, 914, 11, 754, 257, 2199, 11, 2199, 764, 1389, 11, 411, 291, 1116, 915, 257, 1508, 51216], "temperature": 0.0, "avg_logprob": -0.07402948349241227, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.003272515255957842}, {"id": 188, "seek": 98600, "start": 1003.04, "end": 1008.72, "text": " that has the plus method, right, you can overload the Dunder methods like Dunder add, for example,", "tokens": [51216, 300, 575, 264, 1804, 3170, 11, 558, 11, 291, 393, 28777, 264, 413, 6617, 7150, 411, 413, 6617, 909, 11, 337, 1365, 11, 51500], "temperature": 0.0, "avg_logprob": -0.07402948349241227, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.003272515255957842}, {"id": 189, "seek": 98600, "start": 1008.72, "end": 1012.88, "text": " and then the plus method works on your class. And so it has very nice and very expressive,", "tokens": [51500, 293, 550, 264, 1804, 3170, 1985, 322, 428, 1508, 13, 400, 370, 309, 575, 588, 1481, 293, 588, 40189, 11, 51708], "temperature": 0.0, "avg_logprob": -0.07402948349241227, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.003272515255957842}, {"id": 190, "seek": 101288, "start": 1013.68, "end": 1018.8, "text": " dynamic metaprogramming features. In mojo, we want all those features come in. Like,", "tokens": [50404, 8546, 1131, 569, 340, 1342, 2810, 4122, 13, 682, 705, 5134, 11, 321, 528, 439, 729, 4122, 808, 294, 13, 1743, 11, 50660], "temperature": 0.0, "avg_logprob": -0.09047135230033629, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0018099085427820683}, {"id": 191, "seek": 101288, "start": 1018.8, "end": 1022.8, "text": " we don't want to break Python, we want it all to work. But the problem is, is you can't run those", "tokens": [50660, 321, 500, 380, 528, 281, 1821, 15329, 11, 321, 528, 309, 439, 281, 589, 13, 583, 264, 1154, 307, 11, 307, 291, 393, 380, 1190, 729, 50860], "temperature": 0.0, "avg_logprob": -0.09047135230033629, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0018099085427820683}, {"id": 192, "seek": 101288, "start": 1022.8, "end": 1030.16, "text": " super dynamic features on an embedded processor, or on a GPU, right? Or if you could, you probably", "tokens": [50860, 1687, 8546, 4122, 322, 364, 16741, 15321, 11, 420, 322, 257, 18407, 11, 558, 30, 1610, 498, 291, 727, 11, 291, 1391, 51228], "temperature": 0.0, "avg_logprob": -0.09047135230033629, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0018099085427820683}, {"id": 193, "seek": 101288, "start": 1030.16, "end": 1034.56, "text": " don't want to just because of the performance. And so we entered this question of saying,", "tokens": [51228, 500, 380, 528, 281, 445, 570, 295, 264, 3389, 13, 400, 370, 321, 9065, 341, 1168, 295, 1566, 11, 51448], "temperature": 0.0, "avg_logprob": -0.09047135230033629, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0018099085427820683}, {"id": 194, "seek": 101288, "start": 1034.56, "end": 1040.88, "text": " okay, how do you get the power of this dynamic metaprogramming into a language that has to be", "tokens": [51448, 1392, 11, 577, 360, 291, 483, 264, 1347, 295, 341, 8546, 1131, 569, 340, 1342, 2810, 666, 257, 2856, 300, 575, 281, 312, 51764], "temperature": 0.0, "avg_logprob": -0.09047135230033629, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0018099085427820683}, {"id": 195, "seek": 104088, "start": 1040.88, "end": 1045.7600000000002, "text": " super efficient in specific cases? And so what we did was we said, okay, we'll take that interpreter,", "tokens": [50364, 1687, 7148, 294, 2685, 3331, 30, 400, 370, 437, 321, 630, 390, 321, 848, 11, 1392, 11, 321, 603, 747, 300, 34132, 11, 50608], "temperature": 0.0, "avg_logprob": -0.1023631025763119, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.007575382944196463}, {"id": 196, "seek": 104088, "start": 1045.7600000000002, "end": 1050.0800000000002, "text": " Python has an interpreter in it, right, take that interpreter and allow it to run it compile time.", "tokens": [50608, 15329, 575, 364, 34132, 294, 309, 11, 558, 11, 747, 300, 34132, 293, 2089, 309, 281, 1190, 309, 31413, 565, 13, 50824], "temperature": 0.0, "avg_logprob": -0.1023631025763119, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.007575382944196463}, {"id": 197, "seek": 104088, "start": 1051.3600000000001, "end": 1055.3600000000001, "text": " And so now what you get is you get compile time metaprogramming. And so this is super", "tokens": [50888, 400, 370, 586, 437, 291, 483, 307, 291, 483, 31413, 565, 1131, 569, 340, 1342, 2810, 13, 400, 370, 341, 307, 1687, 51088], "temperature": 0.0, "avg_logprob": -0.1023631025763119, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.007575382944196463}, {"id": 198, "seek": 104088, "start": 1055.3600000000001, "end": 1060.88, "text": " interesting, super powerful, because one of the big advantages you get is you get Python style", "tokens": [51088, 1880, 11, 1687, 4005, 11, 570, 472, 295, 264, 955, 14906, 291, 483, 307, 291, 483, 15329, 3758, 51364], "temperature": 0.0, "avg_logprob": -0.1023631025763119, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.007575382944196463}, {"id": 199, "seek": 104088, "start": 1060.88, "end": 1065.8400000000001, "text": " expressive APIs, you get the ability to have overloaded operators. And if you look at what", "tokens": [51364, 40189, 21445, 11, 291, 483, 264, 3485, 281, 362, 28777, 292, 19077, 13, 400, 498, 291, 574, 412, 437, 51612], "temperature": 0.0, "avg_logprob": -0.1023631025763119, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.007575382944196463}, {"id": 200, "seek": 104088, "start": 1065.8400000000001, "end": 1069.8400000000001, "text": " happens inside of like PyTorch, for example, with automatic differentiation and eager mode,", "tokens": [51612, 2314, 1854, 295, 411, 9953, 51, 284, 339, 11, 337, 1365, 11, 365, 12509, 38902, 293, 18259, 4391, 11, 51812], "temperature": 0.0, "avg_logprob": -0.1023631025763119, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.007575382944196463}, {"id": 201, "seek": 106984, "start": 1069.84, "end": 1074.0, "text": " and like all these things, they're using these really dynamic and powerful features at runtime.", "tokens": [50364, 293, 411, 439, 613, 721, 11, 436, 434, 1228, 613, 534, 8546, 293, 4005, 4122, 412, 34474, 13, 50572], "temperature": 0.0, "avg_logprob": -0.13187137785412018, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.001597374095581472}, {"id": 202, "seek": 106984, "start": 1074.56, "end": 1077.6, "text": " But we can take those features and lift them so that they run at compile time.", "tokens": [50600, 583, 321, 393, 747, 729, 4122, 293, 5533, 552, 370, 300, 436, 1190, 412, 31413, 565, 13, 50752], "temperature": 0.0, "avg_logprob": -0.13187137785412018, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.001597374095581472}, {"id": 203, "seek": 106984, "start": 1078.1599999999999, "end": 1085.28, "text": " So you're, because C++ has metaprogramming with templates, but it's really messy.", "tokens": [50780, 407, 291, 434, 11, 570, 383, 25472, 575, 1131, 569, 340, 1342, 2810, 365, 21165, 11, 457, 309, 311, 534, 16191, 13, 51136], "temperature": 0.0, "avg_logprob": -0.13187137785412018, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.001597374095581472}, {"id": 204, "seek": 106984, "start": 1085.28, "end": 1090.56, "text": " It's super messy. It's always, it was accidentally, I mean, different people have different", "tokens": [51136, 467, 311, 1687, 16191, 13, 467, 311, 1009, 11, 309, 390, 15715, 11, 286, 914, 11, 819, 561, 362, 819, 51400], "temperature": 0.0, "avg_logprob": -0.13187137785412018, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.001597374095581472}, {"id": 205, "seek": 106984, "start": 1090.56, "end": 1096.08, "text": " interpretations. My interpretation is that it was made accidentally powerful. It was not designed", "tokens": [51400, 37547, 13, 1222, 14174, 307, 300, 309, 390, 1027, 15715, 4005, 13, 467, 390, 406, 4761, 51676], "temperature": 0.0, "avg_logprob": -0.13187137785412018, "compression_ratio": 1.735408560311284, "no_speech_prob": 0.001597374095581472}, {"id": 206, "seek": 109608, "start": 1096.08, "end": 1100.48, "text": " to be terrain complete, for example, but that was discovered kind of along the way accidentally.", "tokens": [50364, 281, 312, 17674, 3566, 11, 337, 1365, 11, 457, 300, 390, 6941, 733, 295, 2051, 264, 636, 15715, 13, 50584], "temperature": 0.0, "avg_logprob": -0.12361737723662475, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.024412568658590317}, {"id": 207, "seek": 109608, "start": 1101.36, "end": 1106.0, "text": " And so there have been a number of languages in the space. And so they usually have templates", "tokens": [50628, 400, 370, 456, 362, 668, 257, 1230, 295, 8650, 294, 264, 1901, 13, 400, 370, 436, 2673, 362, 21165, 50860], "temperature": 0.0, "avg_logprob": -0.12361737723662475, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.024412568658590317}, {"id": 208, "seek": 109608, "start": 1106.56, "end": 1109.4399999999998, "text": " or code instantiation, code copying features of various sorts.", "tokens": [50888, 420, 3089, 9836, 6642, 11, 3089, 27976, 4122, 295, 3683, 7527, 13, 51032], "temperature": 0.0, "avg_logprob": -0.12361737723662475, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.024412568658590317}, {"id": 209, "seek": 109608, "start": 1110.8799999999999, "end": 1115.36, "text": " Some more modern languages, or some more newer newer languages, let's say, like, you know,", "tokens": [51104, 2188, 544, 4363, 8650, 11, 420, 512, 544, 17628, 17628, 8650, 11, 718, 311, 584, 11, 411, 11, 291, 458, 11, 51328], "temperature": 0.0, "avg_logprob": -0.12361737723662475, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.024412568658590317}, {"id": 210, "seek": 109608, "start": 1115.36, "end": 1122.6399999999999, "text": " they're fairly unknown, like zig, for example, says, okay, well, let's take all of those types", "tokens": [51328, 436, 434, 6457, 9841, 11, 411, 38290, 11, 337, 1365, 11, 1619, 11, 1392, 11, 731, 11, 718, 311, 747, 439, 295, 729, 3467, 51692], "temperature": 0.0, "avg_logprob": -0.12361737723662475, "compression_ratio": 1.7215686274509805, "no_speech_prob": 0.024412568658590317}, {"id": 211, "seek": 112264, "start": 1122.64, "end": 1127.1200000000001, "text": " so you can run it, all those things you can do at runtime and allow them to happen at compile time.", "tokens": [50364, 370, 291, 393, 1190, 309, 11, 439, 729, 721, 291, 393, 360, 412, 34474, 293, 2089, 552, 281, 1051, 412, 31413, 565, 13, 50588], "temperature": 0.0, "avg_logprob": -0.17888039710537698, "compression_ratio": 1.8150684931506849, "no_speech_prob": 0.005219461861997843}, {"id": 212, "seek": 112264, "start": 1128.16, "end": 1133.44, "text": " And so one of the problems with C++, I mean, which is one of one of the problems with C++", "tokens": [50640, 400, 370, 472, 295, 264, 2740, 365, 383, 25472, 11, 286, 914, 11, 597, 307, 472, 295, 472, 295, 264, 2740, 365, 383, 25472, 50904], "temperature": 0.0, "avg_logprob": -0.17888039710537698, "compression_ratio": 1.8150684931506849, "no_speech_prob": 0.005219461861997843}, {"id": 213, "seek": 112264, "start": 1134.0, "end": 1137.0400000000002, "text": " Here we go. Wrong words. We're going to offend everybody today.", "tokens": [50932, 1692, 321, 352, 13, 28150, 2283, 13, 492, 434, 516, 281, 41836, 2201, 965, 13, 51084], "temperature": 0.0, "avg_logprob": -0.17888039710537698, "compression_ratio": 1.8150684931506849, "no_speech_prob": 0.005219461861997843}, {"id": 214, "seek": 112264, "start": 1137.0400000000002, "end": 1140.48, "text": " Oh, it's okay. I mean, everybody hates me for a variety of reasons. Anyways, I'm sure, right?", "tokens": [51084, 876, 11, 309, 311, 1392, 13, 286, 914, 11, 2201, 23000, 385, 337, 257, 5673, 295, 4112, 13, 15585, 11, 286, 478, 988, 11, 558, 30, 51256], "temperature": 0.0, "avg_logprob": -0.17888039710537698, "compression_ratio": 1.8150684931506849, "no_speech_prob": 0.005219461861997843}, {"id": 215, "seek": 112264, "start": 1141.5200000000002, "end": 1146.0, "text": " I've written enough the way they show love. I've written enough C++ code to earn a little bit", "tokens": [51308, 286, 600, 3720, 1547, 264, 636, 436, 855, 959, 13, 286, 600, 3720, 1547, 383, 25472, 3089, 281, 6012, 257, 707, 857, 51532], "temperature": 0.0, "avg_logprob": -0.17888039710537698, "compression_ratio": 1.8150684931506849, "no_speech_prob": 0.005219461861997843}, {"id": 216, "seek": 112264, "start": 1146.0, "end": 1151.6000000000001, "text": " of grumpyness with C++. But, but one of the problems with it is that the metaprogramming", "tokens": [51532, 295, 677, 36142, 1287, 365, 383, 25472, 13, 583, 11, 457, 472, 295, 264, 2740, 365, 309, 307, 300, 264, 1131, 569, 340, 1342, 2810, 51812], "temperature": 0.0, "avg_logprob": -0.17888039710537698, "compression_ratio": 1.8150684931506849, "no_speech_prob": 0.005219461861997843}, {"id": 217, "seek": 115160, "start": 1151.6, "end": 1157.4399999999998, "text": " system templates is just a completely different universe from the normal runtime programming", "tokens": [50364, 1185, 21165, 307, 445, 257, 2584, 819, 6445, 490, 264, 2710, 34474, 9410, 50656], "temperature": 0.0, "avg_logprob": -0.08911852251019395, "compression_ratio": 1.8790322580645162, "no_speech_prob": 0.003944551572203636}, {"id": 218, "seek": 115160, "start": 1157.4399999999998, "end": 1161.28, "text": " world. And so if you do metaprogramming and programming, it's just like a different universe,", "tokens": [50656, 1002, 13, 400, 370, 498, 291, 360, 1131, 569, 340, 1342, 2810, 293, 9410, 11, 309, 311, 445, 411, 257, 819, 6445, 11, 50848], "temperature": 0.0, "avg_logprob": -0.08911852251019395, "compression_ratio": 1.8790322580645162, "no_speech_prob": 0.003944551572203636}, {"id": 219, "seek": 115160, "start": 1161.28, "end": 1166.32, "text": " different syntax, different concepts, different stuff going on. And so again, one of our goals", "tokens": [50848, 819, 28431, 11, 819, 10392, 11, 819, 1507, 516, 322, 13, 400, 370, 797, 11, 472, 295, 527, 5493, 51100], "temperature": 0.0, "avg_logprob": -0.08911852251019395, "compression_ratio": 1.8790322580645162, "no_speech_prob": 0.003944551572203636}, {"id": 220, "seek": 115160, "start": 1166.32, "end": 1171.1999999999998, "text": " with Mojo is to make things really easy to use, easy to learn. And so there's a natural stepping", "tokens": [51100, 365, 3335, 5134, 307, 281, 652, 721, 534, 1858, 281, 764, 11, 1858, 281, 1466, 13, 400, 370, 456, 311, 257, 3303, 16821, 51344], "temperature": 0.0, "avg_logprob": -0.08911852251019395, "compression_ratio": 1.8790322580645162, "no_speech_prob": 0.003944551572203636}, {"id": 221, "seek": 115160, "start": 1171.1999999999998, "end": 1176.32, "text": " stone. And so as you do this, you say, okay, well, I have to do programming at runtime,", "tokens": [51344, 7581, 13, 400, 370, 382, 291, 360, 341, 11, 291, 584, 11, 1392, 11, 731, 11, 286, 362, 281, 360, 9410, 412, 34474, 11, 51600], "temperature": 0.0, "avg_logprob": -0.08911852251019395, "compression_ratio": 1.8790322580645162, "no_speech_prob": 0.003944551572203636}, {"id": 222, "seek": 117632, "start": 1176.3999999999999, "end": 1180.3999999999999, "text": " after you're programming at compile time. Why are these different things?", "tokens": [50368, 934, 291, 434, 9410, 412, 31413, 565, 13, 1545, 366, 613, 819, 721, 30, 50568], "temperature": 0.0, "avg_logprob": -0.12608051976413592, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0052195522002875805}, {"id": 223, "seek": 117632, "start": 1181.2, "end": 1185.9199999999998, "text": " How hard is that to pull it out? Because that sounds to me as a fan of metaprogramming in C++", "tokens": [50608, 1012, 1152, 307, 300, 281, 2235, 309, 484, 30, 1436, 300, 3263, 281, 385, 382, 257, 3429, 295, 1131, 569, 340, 1342, 2810, 294, 383, 25472, 50844], "temperature": 0.0, "avg_logprob": -0.12608051976413592, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0052195522002875805}, {"id": 224, "seek": 117632, "start": 1185.9199999999998, "end": 1191.04, "text": " even. How hard is it to pull that off? That sounds really, really exciting, because you can do the", "tokens": [50844, 754, 13, 1012, 1152, 307, 309, 281, 2235, 300, 766, 30, 663, 3263, 534, 11, 534, 4670, 11, 570, 291, 393, 360, 264, 51100], "temperature": 0.0, "avg_logprob": -0.12608051976413592, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0052195522002875805}, {"id": 225, "seek": 117632, "start": 1191.04, "end": 1195.36, "text": " same style programming at compile time and at runtime. That's really, really exciting.", "tokens": [51100, 912, 3758, 9410, 412, 31413, 565, 293, 412, 34474, 13, 663, 311, 534, 11, 534, 4670, 13, 51316], "temperature": 0.0, "avg_logprob": -0.12608051976413592, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0052195522002875805}, {"id": 226, "seek": 117632, "start": 1195.36, "end": 1201.28, "text": " Yep. And so I mean, in terms of the compiler implementation details, it's hard. I won't be", "tokens": [51316, 7010, 13, 400, 370, 286, 914, 11, 294, 2115, 295, 264, 31958, 11420, 4365, 11, 309, 311, 1152, 13, 286, 1582, 380, 312, 51612], "temperature": 0.0, "avg_logprob": -0.12608051976413592, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0052195522002875805}, {"id": 227, "seek": 117632, "start": 1201.28, "end": 1205.84, "text": " shy about that. It's super hard. It requires, I mean, what Mojo has underneath the covers is a", "tokens": [51612, 12685, 466, 300, 13, 467, 311, 1687, 1152, 13, 467, 7029, 11, 286, 914, 11, 437, 3335, 5134, 575, 7223, 264, 10538, 307, 257, 51840], "temperature": 0.0, "avg_logprob": -0.12608051976413592, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0052195522002875805}, {"id": 228, "seek": 120584, "start": 1205.84, "end": 1210.6399999999999, "text": " completely new approach to the design of the compiler itself. And so this builds on these", "tokens": [50364, 2584, 777, 3109, 281, 264, 1715, 295, 264, 31958, 2564, 13, 400, 370, 341, 15182, 322, 613, 50604], "temperature": 0.0, "avg_logprob": -0.1437752141361743, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0007320009754039347}, {"id": 229, "seek": 120584, "start": 1210.6399999999999, "end": 1215.6, "text": " technologies like MLIR that you mentioned, that also includes other like caching and other", "tokens": [50604, 7943, 411, 21601, 7740, 300, 291, 2835, 11, 300, 611, 5974, 661, 411, 269, 2834, 293, 661, 50852], "temperature": 0.0, "avg_logprob": -0.1437752141361743, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0007320009754039347}, {"id": 230, "seek": 120584, "start": 1216.3999999999999, "end": 1219.12, "text": " interpreters and jit compilers and other stuff like that.", "tokens": [50892, 17489, 1559, 293, 361, 270, 715, 388, 433, 293, 661, 1507, 411, 300, 13, 51028], "temperature": 0.0, "avg_logprob": -0.1437752141361743, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0007320009754039347}, {"id": 231, "seek": 120584, "start": 1219.12, "end": 1220.56, "text": " Do you have like an interpreter inside the compiler?", "tokens": [51028, 1144, 291, 362, 411, 364, 34132, 1854, 264, 31958, 30, 51100], "temperature": 0.0, "avg_logprob": -0.1437752141361743, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0007320009754039347}, {"id": 232, "seek": 120584, "start": 1220.56, "end": 1228.8, "text": " Within the compiler, yes. And so it really takes the standard model of programming languages and", "tokens": [51100, 15996, 264, 31958, 11, 2086, 13, 400, 370, 309, 534, 2516, 264, 3832, 2316, 295, 9410, 8650, 293, 51512], "temperature": 0.0, "avg_logprob": -0.1437752141361743, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0007320009754039347}, {"id": 233, "seek": 120584, "start": 1228.8, "end": 1234.08, "text": " kind of twists it and unifies it with the runtime model, right, which I think is really cool. And", "tokens": [51512, 733, 295, 35290, 309, 293, 517, 11221, 309, 365, 264, 34474, 2316, 11, 558, 11, 597, 286, 519, 307, 534, 1627, 13, 400, 51776], "temperature": 0.0, "avg_logprob": -0.1437752141361743, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0007320009754039347}, {"id": 234, "seek": 123408, "start": 1234.08, "end": 1238.08, "text": " to me, the value of that is that, again, many of these languages have metaprogramming features,", "tokens": [50364, 281, 385, 11, 264, 2158, 295, 300, 307, 300, 11, 797, 11, 867, 295, 613, 8650, 362, 1131, 569, 340, 1342, 2810, 4122, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11034122129686835, "compression_ratio": 1.9184397163120568, "no_speech_prob": 0.0003459505387581885}, {"id": 235, "seek": 123408, "start": 1238.08, "end": 1240.8, "text": " like they grow macros or something, right? You list, right?", "tokens": [50564, 411, 436, 1852, 7912, 2635, 420, 746, 11, 558, 30, 509, 1329, 11, 558, 30, 50700], "temperature": 0.0, "avg_logprob": -0.11034122129686835, "compression_ratio": 1.9184397163120568, "no_speech_prob": 0.0003459505387581885}, {"id": 236, "seek": 123408, "start": 1241.76, "end": 1242.24, "text": " Yes.", "tokens": [50748, 1079, 13, 50772], "temperature": 0.0, "avg_logprob": -0.11034122129686835, "compression_ratio": 1.9184397163120568, "no_speech_prob": 0.0003459505387581885}, {"id": 237, "seek": 123408, "start": 1242.24, "end": 1248.32, "text": " I know your roots, right? And this is a powerful thing, right? And so if you go back to list,", "tokens": [50772, 286, 458, 428, 10669, 11, 558, 30, 400, 341, 307, 257, 4005, 551, 11, 558, 30, 400, 370, 498, 291, 352, 646, 281, 1329, 11, 51076], "temperature": 0.0, "avg_logprob": -0.11034122129686835, "compression_ratio": 1.9184397163120568, "no_speech_prob": 0.0003459505387581885}, {"id": 238, "seek": 123408, "start": 1248.32, "end": 1252.56, "text": " one of the most powerful things about it is that it said that the metaprogramming and the programming", "tokens": [51076, 472, 295, 264, 881, 4005, 721, 466, 309, 307, 300, 309, 848, 300, 264, 1131, 569, 340, 1342, 2810, 293, 264, 9410, 51288], "temperature": 0.0, "avg_logprob": -0.11034122129686835, "compression_ratio": 1.9184397163120568, "no_speech_prob": 0.0003459505387581885}, {"id": 239, "seek": 123408, "start": 1252.56, "end": 1257.04, "text": " are the same, right? And so that made it way simpler, way more consistent, way easier to", "tokens": [51288, 366, 264, 912, 11, 558, 30, 400, 370, 300, 1027, 309, 636, 18587, 11, 636, 544, 8398, 11, 636, 3571, 281, 51512], "temperature": 0.0, "avg_logprob": -0.11034122129686835, "compression_ratio": 1.9184397163120568, "no_speech_prob": 0.0003459505387581885}, {"id": 240, "seek": 123408, "start": 1257.04, "end": 1260.8799999999999, "text": " understand reason about, and it made it more composable. So if you build a library, you can use", "tokens": [51512, 1223, 1778, 466, 11, 293, 309, 1027, 309, 544, 10199, 712, 13, 407, 498, 291, 1322, 257, 6405, 11, 291, 393, 764, 51704], "temperature": 0.0, "avg_logprob": -0.11034122129686835, "compression_ratio": 1.9184397163120568, "no_speech_prob": 0.0003459505387581885}, {"id": 241, "seek": 126088, "start": 1260.96, "end": 1264.24, "text": " it both at runtime and compile time, which is pretty cool.", "tokens": [50368, 309, 1293, 412, 34474, 293, 31413, 565, 11, 597, 307, 1238, 1627, 13, 50532], "temperature": 0.0, "avg_logprob": -0.1552980124544935, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.020959777757525444}, {"id": 242, "seek": 126088, "start": 1264.24, "end": 1270.5600000000002, "text": " Yeah. And for machine learning, I think metaprogramming, I think we could generally say is extremely", "tokens": [50532, 865, 13, 400, 337, 3479, 2539, 11, 286, 519, 1131, 569, 340, 1342, 2810, 11, 286, 519, 321, 727, 5101, 584, 307, 4664, 50848], "temperature": 0.0, "avg_logprob": -0.1552980124544935, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.020959777757525444}, {"id": 243, "seek": 126088, "start": 1270.5600000000002, "end": 1277.2, "text": " useful. And so you get features, I mean, I'll jump around, but there's the feature of auto-tuning", "tokens": [50848, 4420, 13, 400, 370, 291, 483, 4122, 11, 286, 914, 11, 286, 603, 3012, 926, 11, 457, 456, 311, 264, 4111, 295, 8399, 12, 83, 37726, 51180], "temperature": 0.0, "avg_logprob": -0.1552980124544935, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.020959777757525444}, {"id": 244, "seek": 126088, "start": 1277.2, "end": 1280.3200000000002, "text": " and adaptive compilation just blows my mind.", "tokens": [51180, 293, 27912, 40261, 445, 18458, 452, 1575, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1552980124544935, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.020959777757525444}, {"id": 245, "seek": 126088, "start": 1280.3200000000002, "end": 1281.92, "text": " Yeah. Well, so, okay, so let's come back to that.", "tokens": [51336, 865, 13, 1042, 11, 370, 11, 1392, 11, 370, 718, 311, 808, 646, 281, 300, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1552980124544935, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.020959777757525444}, {"id": 246, "seek": 126088, "start": 1281.92, "end": 1282.64, "text": " Okay, all right.", "tokens": [51416, 1033, 11, 439, 558, 13, 51452], "temperature": 0.0, "avg_logprob": -0.1552980124544935, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.020959777757525444}, {"id": 247, "seek": 126088, "start": 1282.64, "end": 1286.48, "text": " So what is machine learning? Like, what is a machine learning model? Like,", "tokens": [51452, 407, 437, 307, 3479, 2539, 30, 1743, 11, 437, 307, 257, 3479, 2539, 2316, 30, 1743, 11, 51644], "temperature": 0.0, "avg_logprob": -0.1552980124544935, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.020959777757525444}, {"id": 248, "seek": 126088, "start": 1286.48, "end": 1290.8000000000002, "text": " you take a PyTorch model off there, right? It's really interesting to me because what", "tokens": [51644, 291, 747, 257, 9953, 51, 284, 339, 2316, 766, 456, 11, 558, 30, 467, 311, 534, 1880, 281, 385, 570, 437, 51860], "temperature": 0.0, "avg_logprob": -0.1552980124544935, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.020959777757525444}, {"id": 249, "seek": 129080, "start": 1291.36, "end": 1295.52, "text": " PyTorch and what TensorFlow and all these frameworks are kind of pushing compute into,", "tokens": [50392, 9953, 51, 284, 339, 293, 437, 37624, 293, 439, 613, 29834, 366, 733, 295, 7380, 14722, 666, 11, 50600], "temperature": 0.0, "avg_logprob": -0.11884112690770349, "compression_ratio": 1.6801346801346801, "no_speech_prob": 0.0011333448346704245}, {"id": 250, "seek": 129080, "start": 1295.52, "end": 1300.8, "text": " is they're pushing into like this abstract specification of a compute problem,", "tokens": [50600, 307, 436, 434, 7380, 666, 411, 341, 12649, 31256, 295, 257, 14722, 1154, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11884112690770349, "compression_ratio": 1.6801346801346801, "no_speech_prob": 0.0011333448346704245}, {"id": 251, "seek": 129080, "start": 1300.8, "end": 1303.28, "text": " which then gets mapped in a whole bunch of different ways, right?", "tokens": [50864, 597, 550, 2170, 33318, 294, 257, 1379, 3840, 295, 819, 2098, 11, 558, 30, 50988], "temperature": 0.0, "avg_logprob": -0.11884112690770349, "compression_ratio": 1.6801346801346801, "no_speech_prob": 0.0011333448346704245}, {"id": 252, "seek": 129080, "start": 1303.28, "end": 1306.8, "text": " And so this is why it became a metaprogramming problem, is that you want to be able to say,", "tokens": [50988, 400, 370, 341, 307, 983, 309, 3062, 257, 1131, 569, 340, 1342, 2810, 1154, 11, 307, 300, 291, 528, 281, 312, 1075, 281, 584, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11884112690770349, "compression_ratio": 1.6801346801346801, "no_speech_prob": 0.0011333448346704245}, {"id": 253, "seek": 129080, "start": 1306.8, "end": 1314.3999999999999, "text": " cool, I have this neural net, now run with batch size 1000, right? Do a mapping across batch,", "tokens": [51164, 1627, 11, 286, 362, 341, 18161, 2533, 11, 586, 1190, 365, 15245, 2744, 9714, 11, 558, 30, 1144, 257, 18350, 2108, 15245, 11, 51544], "temperature": 0.0, "avg_logprob": -0.11884112690770349, "compression_ratio": 1.6801346801346801, "no_speech_prob": 0.0011333448346704245}, {"id": 254, "seek": 129080, "start": 1314.3999999999999, "end": 1319.52, "text": " or okay, I want to take this problem now running across 1000 CPUs or GPUs, right?", "tokens": [51544, 420, 1392, 11, 286, 528, 281, 747, 341, 1154, 586, 2614, 2108, 9714, 13199, 82, 420, 18407, 82, 11, 558, 30, 51800], "temperature": 0.0, "avg_logprob": -0.11884112690770349, "compression_ratio": 1.6801346801346801, "no_speech_prob": 0.0011333448346704245}, {"id": 255, "seek": 131952, "start": 1319.52, "end": 1325.28, "text": " And so like, this problem of like, describe the compute and then map it and do things and", "tokens": [50364, 400, 370, 411, 11, 341, 1154, 295, 411, 11, 6786, 264, 14722, 293, 550, 4471, 309, 293, 360, 721, 293, 50652], "temperature": 0.0, "avg_logprob": -0.1409943498025729, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0005192453390918672}, {"id": 256, "seek": 131952, "start": 1325.28, "end": 1329.2, "text": " transform it are like, actually, it's very profound. And that's one of the things that", "tokens": [50652, 4088, 309, 366, 411, 11, 767, 11, 309, 311, 588, 14382, 13, 400, 300, 311, 472, 295, 264, 721, 300, 50848], "temperature": 0.0, "avg_logprob": -0.1409943498025729, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0005192453390918672}, {"id": 257, "seek": 131952, "start": 1329.2, "end": 1331.2, "text": " makes machine learning systems really special.", "tokens": [50848, 1669, 3479, 2539, 3652, 534, 2121, 13, 50948], "temperature": 0.0, "avg_logprob": -0.1409943498025729, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0005192453390918672}, {"id": 258, "seek": 131952, "start": 1332.08, "end": 1336.72, "text": " Maybe can you describe auto-tuning and how do you pull off? I mean, I guess adaptive", "tokens": [50992, 2704, 393, 291, 6786, 8399, 12, 83, 37726, 293, 577, 360, 291, 2235, 766, 30, 286, 914, 11, 286, 2041, 27912, 51224], "temperature": 0.0, "avg_logprob": -0.1409943498025729, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0005192453390918672}, {"id": 259, "seek": 131952, "start": 1336.72, "end": 1339.44, "text": " compilation is what we're talking about as metaprogramming.", "tokens": [51224, 40261, 307, 437, 321, 434, 1417, 466, 382, 1131, 569, 340, 1342, 2810, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1409943498025729, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0005192453390918672}, {"id": 260, "seek": 131952, "start": 1339.44, "end": 1339.6, "text": " Yeah.", "tokens": [51360, 865, 13, 51368], "temperature": 0.0, "avg_logprob": -0.1409943498025729, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0005192453390918672}, {"id": 261, "seek": 131952, "start": 1339.6, "end": 1343.84, "text": " How do you pull off auto-tuning? I mean, is that as profound as I think it is? It just seems like", "tokens": [51368, 1012, 360, 291, 2235, 766, 8399, 12, 83, 37726, 30, 286, 914, 11, 307, 300, 382, 14382, 382, 286, 519, 309, 307, 30, 467, 445, 2544, 411, 51580], "temperature": 0.0, "avg_logprob": -0.1409943498025729, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0005192453390918672}, {"id": 262, "seek": 134384, "start": 1343.84, "end": 1350.08, "text": " I really like, you know, we'll mention list comprehensions to me, from a quick glance at", "tokens": [50364, 286, 534, 411, 11, 291, 458, 11, 321, 603, 2152, 1329, 10753, 8302, 281, 385, 11, 490, 257, 1702, 21094, 412, 50676], "temperature": 0.0, "avg_logprob": -0.15527627563476562, "compression_ratio": 1.6245614035087719, "no_speech_prob": 0.002396443160250783}, {"id": 263, "seek": 134384, "start": 1350.08, "end": 1357.4399999999998, "text": " Mojo, which by the way, I have to absolutely like dive in. As I realize how amazing this is,", "tokens": [50676, 3335, 5134, 11, 597, 538, 264, 636, 11, 286, 362, 281, 3122, 411, 9192, 294, 13, 1018, 286, 4325, 577, 2243, 341, 307, 11, 51044], "temperature": 0.0, "avg_logprob": -0.15527627563476562, "compression_ratio": 1.6245614035087719, "no_speech_prob": 0.002396443160250783}, {"id": 264, "seek": 134384, "start": 1357.4399999999998, "end": 1363.12, "text": " I absolutely must dive in. That looks like just an incredible feature for machine learning people.", "tokens": [51044, 286, 3122, 1633, 9192, 294, 13, 663, 1542, 411, 445, 364, 4651, 4111, 337, 3479, 2539, 561, 13, 51328], "temperature": 0.0, "avg_logprob": -0.15527627563476562, "compression_ratio": 1.6245614035087719, "no_speech_prob": 0.002396443160250783}, {"id": 265, "seek": 134384, "start": 1363.12, "end": 1367.6799999999998, "text": " Yeah. Well, so what is auto-tuning? So take a step back. Auto-tuning is a feature in Mojo.", "tokens": [51328, 865, 13, 1042, 11, 370, 437, 307, 8399, 12, 83, 37726, 30, 407, 747, 257, 1823, 646, 13, 13738, 12, 83, 37726, 307, 257, 4111, 294, 3335, 5134, 13, 51556], "temperature": 0.0, "avg_logprob": -0.15527627563476562, "compression_ratio": 1.6245614035087719, "no_speech_prob": 0.002396443160250783}, {"id": 266, "seek": 134384, "start": 1367.6799999999998, "end": 1372.32, "text": " It's not, so very little of what we're doing is actually research. Like many of these ideas", "tokens": [51556, 467, 311, 406, 11, 370, 588, 707, 295, 437, 321, 434, 884, 307, 767, 2132, 13, 1743, 867, 295, 613, 3487, 51788], "temperature": 0.0, "avg_logprob": -0.15527627563476562, "compression_ratio": 1.6245614035087719, "no_speech_prob": 0.002396443160250783}, {"id": 267, "seek": 137232, "start": 1372.8799999999999, "end": 1376.24, "text": " have existed in other systems and other places. And so what we're doing is we're pulling together", "tokens": [50392, 362, 13135, 294, 661, 3652, 293, 661, 3190, 13, 400, 370, 437, 321, 434, 884, 307, 321, 434, 8407, 1214, 50560], "temperature": 0.0, "avg_logprob": -0.10935277071866122, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0012841611169278622}, {"id": 268, "seek": 137232, "start": 1376.24, "end": 1380.8, "text": " good ideas, remixing them, and making them into hopefully a beautiful system, right?", "tokens": [50560, 665, 3487, 11, 47788, 278, 552, 11, 293, 1455, 552, 666, 4696, 257, 2238, 1185, 11, 558, 30, 50788], "temperature": 0.0, "avg_logprob": -0.10935277071866122, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0012841611169278622}, {"id": 269, "seek": 137232, "start": 1381.4399999999998, "end": 1388.08, "text": " And so auto-tuning, the observation is that it turns out hardware systems, algorithms are really", "tokens": [50820, 400, 370, 8399, 12, 83, 37726, 11, 264, 14816, 307, 300, 309, 4523, 484, 8837, 3652, 11, 14642, 366, 534, 51152], "temperature": 0.0, "avg_logprob": -0.10935277071866122, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0012841611169278622}, {"id": 270, "seek": 137232, "start": 1388.08, "end": 1392.08, "text": " complicated. It turns out maybe you don't actually want to know how the hardware works,", "tokens": [51152, 6179, 13, 467, 4523, 484, 1310, 291, 500, 380, 767, 528, 281, 458, 577, 264, 8837, 1985, 11, 51352], "temperature": 0.0, "avg_logprob": -0.10935277071866122, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0012841611169278622}, {"id": 271, "seek": 137232, "start": 1393.12, "end": 1397.4399999999998, "text": " right? A lot of people don't, right? And so there are lots of really smart hardware people.", "tokens": [51404, 558, 30, 316, 688, 295, 561, 500, 380, 11, 558, 30, 400, 370, 456, 366, 3195, 295, 534, 4069, 8837, 561, 13, 51620], "temperature": 0.0, "avg_logprob": -0.10935277071866122, "compression_ratio": 1.8070866141732282, "no_speech_prob": 0.0012841611169278622}, {"id": 272, "seek": 139744, "start": 1397.44, "end": 1402.96, "text": " I know a lot of them, where they know everything about, okay, that the cache size is this and", "tokens": [50364, 286, 458, 257, 688, 295, 552, 11, 689, 436, 458, 1203, 466, 11, 1392, 11, 300, 264, 19459, 2744, 307, 341, 293, 50640], "temperature": 0.0, "avg_logprob": -0.12613246523100755, "compression_ratio": 1.70392749244713, "no_speech_prob": 0.04884790629148483}, {"id": 273, "seek": 139744, "start": 1402.96, "end": 1406.48, "text": " the number of registers is that. And if you use this, what length of vector, it's going to be", "tokens": [50640, 264, 1230, 295, 38351, 307, 300, 13, 400, 498, 291, 764, 341, 11, 437, 4641, 295, 8062, 11, 309, 311, 516, 281, 312, 50816], "temperature": 0.0, "avg_logprob": -0.12613246523100755, "compression_ratio": 1.70392749244713, "no_speech_prob": 0.04884790629148483}, {"id": 274, "seek": 139744, "start": 1406.48, "end": 1409.92, "text": " super efficient because it maps directly onto what it can do. And like all this kind of stuff,", "tokens": [50816, 1687, 7148, 570, 309, 11317, 3838, 3911, 437, 309, 393, 360, 13, 400, 411, 439, 341, 733, 295, 1507, 11, 50988], "temperature": 0.0, "avg_logprob": -0.12613246523100755, "compression_ratio": 1.70392749244713, "no_speech_prob": 0.04884790629148483}, {"id": 275, "seek": 139744, "start": 1409.92, "end": 1414.4, "text": " or the GPU has SMs and it has a warp size of whatever, right? All the stuff that goes into", "tokens": [50988, 420, 264, 18407, 575, 13115, 82, 293, 309, 575, 257, 36030, 2744, 295, 2035, 11, 558, 30, 1057, 264, 1507, 300, 1709, 666, 51212], "temperature": 0.0, "avg_logprob": -0.12613246523100755, "compression_ratio": 1.70392749244713, "no_speech_prob": 0.04884790629148483}, {"id": 276, "seek": 139744, "start": 1414.4, "end": 1421.92, "text": " these things or the tile size of a TPU is 128, like these factoids, right? My belief is that", "tokens": [51212, 613, 721, 420, 264, 20590, 2744, 295, 257, 314, 8115, 307, 29810, 11, 411, 613, 42225, 3742, 11, 558, 30, 1222, 7107, 307, 300, 51588], "temperature": 0.0, "avg_logprob": -0.12613246523100755, "compression_ratio": 1.70392749244713, "no_speech_prob": 0.04884790629148483}, {"id": 277, "seek": 139744, "start": 1421.92, "end": 1425.92, "text": " most normal people, and I love hardware people also, I'm not trying to offend literally everybody", "tokens": [51588, 881, 2710, 561, 11, 293, 286, 959, 8837, 561, 611, 11, 286, 478, 406, 1382, 281, 41836, 3736, 2201, 51788], "temperature": 0.0, "avg_logprob": -0.12613246523100755, "compression_ratio": 1.70392749244713, "no_speech_prob": 0.04884790629148483}, {"id": 278, "seek": 142592, "start": 1425.92, "end": 1432.0, "text": " on the internet, but most programmers actually don't want to know this stuff, right? And so if", "tokens": [50364, 322, 264, 4705, 11, 457, 881, 41504, 767, 500, 380, 528, 281, 458, 341, 1507, 11, 558, 30, 400, 370, 498, 50668], "temperature": 0.0, "avg_logprob": -0.10713504677388205, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.0044675590470433235}, {"id": 279, "seek": 142592, "start": 1432.0, "end": 1436.48, "text": " you come at it from the perspective of how do we allow people to build both more abstracted,", "tokens": [50668, 291, 808, 412, 309, 490, 264, 4585, 295, 577, 360, 321, 2089, 561, 281, 1322, 1293, 544, 12649, 292, 11, 50892], "temperature": 0.0, "avg_logprob": -0.10713504677388205, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.0044675590470433235}, {"id": 280, "seek": 142592, "start": 1436.48, "end": 1441.2, "text": " but also more portable code, because, you know, could be that the vector length changes or the", "tokens": [50892, 457, 611, 544, 21800, 3089, 11, 570, 11, 291, 458, 11, 727, 312, 300, 264, 8062, 4641, 2962, 420, 264, 51128], "temperature": 0.0, "avg_logprob": -0.10713504677388205, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.0044675590470433235}, {"id": 281, "seek": 142592, "start": 1441.2, "end": 1444.96, "text": " cache size changes, or it could be that the tile size of your matrix changes or the number,", "tokens": [51128, 19459, 2744, 2962, 11, 420, 309, 727, 312, 300, 264, 20590, 2744, 295, 428, 8141, 2962, 420, 264, 1230, 11, 51316], "temperature": 0.0, "avg_logprob": -0.10713504677388205, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.0044675590470433235}, {"id": 282, "seek": 142592, "start": 1444.96, "end": 1449.92, "text": " you know, an A100 versus an H100 versus a Volta versus a whatever GPU have different", "tokens": [51316, 291, 458, 11, 364, 316, 6879, 5717, 364, 389, 6879, 5717, 257, 8911, 1328, 5717, 257, 2035, 18407, 362, 819, 51564], "temperature": 0.0, "avg_logprob": -0.10713504677388205, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.0044675590470433235}, {"id": 283, "seek": 142592, "start": 1449.92, "end": 1455.04, "text": " characteristics, right? A lot of the algorithms that you run are actually the same, but the", "tokens": [51564, 10891, 11, 558, 30, 316, 688, 295, 264, 14642, 300, 291, 1190, 366, 767, 264, 912, 11, 457, 264, 51820], "temperature": 0.0, "avg_logprob": -0.10713504677388205, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.0044675590470433235}, {"id": 284, "seek": 145504, "start": 1455.04, "end": 1459.44, "text": " parameters, these magic numbers you have to fill in end up being really fiddly numbers that an", "tokens": [50364, 9834, 11, 613, 5585, 3547, 291, 362, 281, 2836, 294, 917, 493, 885, 534, 283, 14273, 356, 3547, 300, 364, 50584], "temperature": 0.0, "avg_logprob": -0.10795669033102794, "compression_ratio": 1.7578616352201257, "no_speech_prob": 0.0007321464945562184}, {"id": 285, "seek": 145504, "start": 1459.44, "end": 1464.56, "text": " expert has to go figure out. And so what autotuning does, it says, okay, well, guess what? There's", "tokens": [50584, 5844, 575, 281, 352, 2573, 484, 13, 400, 370, 437, 1476, 310, 37726, 775, 11, 309, 1619, 11, 1392, 11, 731, 11, 2041, 437, 30, 821, 311, 50840], "temperature": 0.0, "avg_logprob": -0.10795669033102794, "compression_ratio": 1.7578616352201257, "no_speech_prob": 0.0007321464945562184}, {"id": 286, "seek": 145504, "start": 1464.56, "end": 1469.6, "text": " a lot of compute out there, right? So instead of having humans go randomly try all the things or", "tokens": [50840, 257, 688, 295, 14722, 484, 456, 11, 558, 30, 407, 2602, 295, 1419, 6255, 352, 16979, 853, 439, 264, 721, 420, 51092], "temperature": 0.0, "avg_logprob": -0.10795669033102794, "compression_ratio": 1.7578616352201257, "no_speech_prob": 0.0007321464945562184}, {"id": 287, "seek": 145504, "start": 1469.6, "end": 1474.56, "text": " do a grid search or go search some complicated multi-dimensional space, how about we have", "tokens": [51092, 360, 257, 10748, 3164, 420, 352, 3164, 512, 6179, 4825, 12, 18759, 1901, 11, 577, 466, 321, 362, 51340], "temperature": 0.0, "avg_logprob": -0.10795669033102794, "compression_ratio": 1.7578616352201257, "no_speech_prob": 0.0007321464945562184}, {"id": 288, "seek": 145504, "start": 1474.56, "end": 1479.04, "text": " computers do that, right? And so what autotuning does is you can say, hey, here's my algorithm.", "tokens": [51340, 10807, 360, 300, 11, 558, 30, 400, 370, 437, 1476, 310, 37726, 775, 307, 291, 393, 584, 11, 4177, 11, 510, 311, 452, 9284, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10795669033102794, "compression_ratio": 1.7578616352201257, "no_speech_prob": 0.0007321464945562184}, {"id": 289, "seek": 145504, "start": 1479.92, "end": 1484.24, "text": " If it's a matrix operation or something like that, you can say, okay, I'm going to", "tokens": [51608, 759, 309, 311, 257, 8141, 6916, 420, 746, 411, 300, 11, 291, 393, 584, 11, 1392, 11, 286, 478, 516, 281, 51824], "temperature": 0.0, "avg_logprob": -0.10795669033102794, "compression_ratio": 1.7578616352201257, "no_speech_prob": 0.0007321464945562184}, {"id": 290, "seek": 148424, "start": 1484.32, "end": 1489.84, "text": " carve it up into blocks. I'm going to do those blocks in parallel. And I want this with 128 things", "tokens": [50368, 33832, 309, 493, 666, 8474, 13, 286, 478, 516, 281, 360, 729, 8474, 294, 8952, 13, 400, 286, 528, 341, 365, 29810, 721, 50644], "temperature": 0.0, "avg_logprob": -0.12762033939361572, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.0019875015132129192}, {"id": 291, "seek": 148424, "start": 1489.84, "end": 1494.0, "text": " that I'm running on, I want to cut it this way or that way or whatever. And you can say, hey, go see", "tokens": [50644, 300, 286, 478, 2614, 322, 11, 286, 528, 281, 1723, 309, 341, 636, 420, 300, 636, 420, 2035, 13, 400, 291, 393, 584, 11, 4177, 11, 352, 536, 50852], "temperature": 0.0, "avg_logprob": -0.12762033939361572, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.0019875015132129192}, {"id": 292, "seek": 148424, "start": 1494.0, "end": 1499.36, "text": " which one's actually empirically better on the system. And then the result of that, you cache for", "tokens": [50852, 597, 472, 311, 767, 25790, 984, 1101, 322, 264, 1185, 13, 400, 550, 264, 1874, 295, 300, 11, 291, 19459, 337, 51120], "temperature": 0.0, "avg_logprob": -0.12762033939361572, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.0019875015132129192}, {"id": 293, "seek": 148424, "start": 1499.36, "end": 1506.0, "text": " that system. Yep. You save it. And so come back to twisting your compiler brain, right? So not only", "tokens": [51120, 300, 1185, 13, 7010, 13, 509, 3155, 309, 13, 400, 370, 808, 646, 281, 34491, 428, 31958, 3567, 11, 558, 30, 407, 406, 787, 51452], "temperature": 0.0, "avg_logprob": -0.12762033939361572, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.0019875015132129192}, {"id": 294, "seek": 148424, "start": 1506.0, "end": 1510.8, "text": " does the compiler have an interpreter that's used to do metaprogramming, that compiler that", "tokens": [51452, 775, 264, 31958, 362, 364, 34132, 300, 311, 1143, 281, 360, 1131, 569, 340, 1342, 2810, 11, 300, 31958, 300, 51692], "temperature": 0.0, "avg_logprob": -0.12762033939361572, "compression_ratio": 1.7402135231316727, "no_speech_prob": 0.0019875015132129192}, {"id": 295, "seek": 151080, "start": 1510.8, "end": 1515.36, "text": " interpreter that metaprogramming now has to actually take your code and go run it on a target", "tokens": [50364, 34132, 300, 1131, 569, 340, 1342, 2810, 586, 575, 281, 767, 747, 428, 3089, 293, 352, 1190, 309, 322, 257, 3779, 50592], "temperature": 0.0, "avg_logprob": -0.11307579398955275, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.0014549769693985581}, {"id": 296, "seek": 151080, "start": 1515.36, "end": 1521.2, "text": " machine. See which one likes the best and then stitch it in and then keep going, right? So part", "tokens": [50592, 3479, 13, 3008, 597, 472, 5902, 264, 1151, 293, 550, 5635, 309, 294, 293, 550, 1066, 516, 11, 558, 30, 407, 644, 50884], "temperature": 0.0, "avg_logprob": -0.11307579398955275, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.0014549769693985581}, {"id": 297, "seek": 151080, "start": 1521.2, "end": 1525.68, "text": " of the compilation is machine specific. Yeah. Well, so I mean, this is an optional feature, right?", "tokens": [50884, 295, 264, 40261, 307, 3479, 2685, 13, 865, 13, 1042, 11, 370, 286, 914, 11, 341, 307, 364, 17312, 4111, 11, 558, 30, 51108], "temperature": 0.0, "avg_logprob": -0.11307579398955275, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.0014549769693985581}, {"id": 298, "seek": 151080, "start": 1525.68, "end": 1530.3999999999999, "text": " So you don't have to use it for everything. But yeah, if you're, so one of, one of the things", "tokens": [51108, 407, 291, 500, 380, 362, 281, 764, 309, 337, 1203, 13, 583, 1338, 11, 498, 291, 434, 11, 370, 472, 295, 11, 472, 295, 264, 721, 51344], "temperature": 0.0, "avg_logprob": -0.11307579398955275, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.0014549769693985581}, {"id": 299, "seek": 151080, "start": 1530.3999999999999, "end": 1535.44, "text": " that we're in the quest of is ultimate performance. Yes. Right. And ultimate performance is important", "tokens": [51344, 300, 321, 434, 294, 264, 866, 295, 307, 9705, 3389, 13, 1079, 13, 1779, 13, 400, 9705, 3389, 307, 1021, 51596], "temperature": 0.0, "avg_logprob": -0.11307579398955275, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.0014549769693985581}, {"id": 300, "seek": 151080, "start": 1535.44, "end": 1539.28, "text": " for a couple of reasons, right? So if you're an enterprise, you're looking to save cost and compute", "tokens": [51596, 337, 257, 1916, 295, 4112, 11, 558, 30, 407, 498, 291, 434, 364, 14132, 11, 291, 434, 1237, 281, 3155, 2063, 293, 14722, 51788], "temperature": 0.0, "avg_logprob": -0.11307579398955275, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.0014549769693985581}, {"id": 301, "seek": 153928, "start": 1539.28, "end": 1544.08, "text": " and things like this, ultimate performance translates to, you know, fewer servers. Like,", "tokens": [50364, 293, 721, 411, 341, 11, 9705, 3389, 28468, 281, 11, 291, 458, 11, 13366, 15909, 13, 1743, 11, 50604], "temperature": 0.0, "avg_logprob": -0.1277620400955428, "compression_ratio": 1.7766666666666666, "no_speech_prob": 0.0038234172388911247}, {"id": 302, "seek": 153928, "start": 1544.08, "end": 1549.28, "text": " if you care about the environment, hey, better performance leads to more efficiency. Right?", "tokens": [50604, 498, 291, 1127, 466, 264, 2823, 11, 4177, 11, 1101, 3389, 6689, 281, 544, 10493, 13, 1779, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1277620400955428, "compression_ratio": 1.7766666666666666, "no_speech_prob": 0.0038234172388911247}, {"id": 303, "seek": 153928, "start": 1549.28, "end": 1552.8, "text": " I mean, you could joke and say like, you know, Python's bad for the environment.", "tokens": [50864, 286, 914, 11, 291, 727, 7647, 293, 584, 411, 11, 291, 458, 11, 15329, 311, 1578, 337, 264, 2823, 13, 51040], "temperature": 0.0, "avg_logprob": -0.1277620400955428, "compression_ratio": 1.7766666666666666, "no_speech_prob": 0.0038234172388911247}, {"id": 304, "seek": 153928, "start": 1553.76, "end": 1557.76, "text": " Right. And so if you move to Mojo, it's like at least 10x better just out of the box and", "tokens": [51088, 1779, 13, 400, 370, 498, 291, 1286, 281, 3335, 5134, 11, 309, 311, 411, 412, 1935, 1266, 87, 1101, 445, 484, 295, 264, 2424, 293, 51288], "temperature": 0.0, "avg_logprob": -0.1277620400955428, "compression_ratio": 1.7766666666666666, "no_speech_prob": 0.0038234172388911247}, {"id": 305, "seek": 153928, "start": 1557.76, "end": 1563.12, "text": " keep going, right? But, but performance is also interesting because it leads to better products.", "tokens": [51288, 1066, 516, 11, 558, 30, 583, 11, 457, 3389, 307, 611, 1880, 570, 309, 6689, 281, 1101, 3383, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1277620400955428, "compression_ratio": 1.7766666666666666, "no_speech_prob": 0.0038234172388911247}, {"id": 306, "seek": 153928, "start": 1563.76, "end": 1567.84, "text": " And so in the space of machine learning, right, if you reduce the latency of a model,", "tokens": [51588, 400, 370, 294, 264, 1901, 295, 3479, 2539, 11, 558, 11, 498, 291, 5407, 264, 27043, 295, 257, 2316, 11, 51792], "temperature": 0.0, "avg_logprob": -0.1277620400955428, "compression_ratio": 1.7766666666666666, "no_speech_prob": 0.0038234172388911247}, {"id": 307, "seek": 156784, "start": 1568.48, "end": 1572.6399999999999, "text": " so it runs faster. So every time you query the server running the model, it takes less time.", "tokens": [50396, 370, 309, 6676, 4663, 13, 407, 633, 565, 291, 14581, 264, 7154, 2614, 264, 2316, 11, 309, 2516, 1570, 565, 13, 50604], "temperature": 0.0, "avg_logprob": -0.16857451580940408, "compression_ratio": 1.7, "no_speech_prob": 0.0006262437091208994}, {"id": 308, "seek": 156784, "start": 1572.6399999999999, "end": 1576.8799999999999, "text": " Well, then the product team can go and make the model bigger. Well, that's actually makes it,", "tokens": [50604, 1042, 11, 550, 264, 1674, 1469, 393, 352, 293, 652, 264, 2316, 3801, 13, 1042, 11, 300, 311, 767, 1669, 309, 11, 50816], "temperature": 0.0, "avg_logprob": -0.16857451580940408, "compression_ratio": 1.7, "no_speech_prob": 0.0006262437091208994}, {"id": 309, "seek": 156784, "start": 1576.8799999999999, "end": 1581.36, "text": " so you have a better experience as a customer. And so a lot of people care about that.", "tokens": [50816, 370, 291, 362, 257, 1101, 1752, 382, 257, 5474, 13, 400, 370, 257, 688, 295, 561, 1127, 466, 300, 13, 51040], "temperature": 0.0, "avg_logprob": -0.16857451580940408, "compression_ratio": 1.7, "no_speech_prob": 0.0006262437091208994}, {"id": 310, "seek": 156784, "start": 1581.36, "end": 1586.32, "text": " So for auto tuning, for like tile size, you mentioned 128 for TPU, you would specify like a", "tokens": [51040, 407, 337, 8399, 15164, 11, 337, 411, 20590, 2744, 11, 291, 2835, 29810, 337, 314, 8115, 11, 291, 576, 16500, 411, 257, 51288], "temperature": 0.0, "avg_logprob": -0.16857451580940408, "compression_ratio": 1.7, "no_speech_prob": 0.0006262437091208994}, {"id": 311, "seek": 156784, "start": 1586.32, "end": 1592.1599999999999, "text": " bunch of options to try. Just in the code. It's just a simple statement. And then you just set", "tokens": [51288, 3840, 295, 3956, 281, 853, 13, 1449, 294, 264, 3089, 13, 467, 311, 445, 257, 2199, 5629, 13, 400, 550, 291, 445, 992, 51580], "temperature": 0.0, "avg_logprob": -0.16857451580940408, "compression_ratio": 1.7, "no_speech_prob": 0.0006262437091208994}, {"id": 312, "seek": 156784, "start": 1592.1599999999999, "end": 1596.72, "text": " and forget and know, depending on where it compiles, it'll actually be the fastest.", "tokens": [51580, 293, 2870, 293, 458, 11, 5413, 322, 689, 309, 715, 4680, 11, 309, 603, 767, 312, 264, 14573, 13, 51808], "temperature": 0.0, "avg_logprob": -0.16857451580940408, "compression_ratio": 1.7, "no_speech_prob": 0.0006262437091208994}, {"id": 313, "seek": 159672, "start": 1596.96, "end": 1600.08, "text": " And yeah, exactly. And the beauty of this is that it helps you in a whole bunch of different", "tokens": [50376, 400, 1338, 11, 2293, 13, 400, 264, 6643, 295, 341, 307, 300, 309, 3665, 291, 294, 257, 1379, 3840, 295, 819, 50532], "temperature": 0.0, "avg_logprob": -0.11216950106930423, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0012446421897038817}, {"id": 314, "seek": 159672, "start": 1600.08, "end": 1604.24, "text": " ways, right? So if you're building, so often what'll happen is that, you know, you've written a", "tokens": [50532, 2098, 11, 558, 30, 407, 498, 291, 434, 2390, 11, 370, 2049, 437, 603, 1051, 307, 300, 11, 291, 458, 11, 291, 600, 3720, 257, 50740], "temperature": 0.0, "avg_logprob": -0.11216950106930423, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0012446421897038817}, {"id": 315, "seek": 159672, "start": 1604.24, "end": 1607.84, "text": " bunch of software yourself, right? You, you wake up one day, you say, I have an idea,", "tokens": [50740, 3840, 295, 4722, 1803, 11, 558, 30, 509, 11, 291, 6634, 493, 472, 786, 11, 291, 584, 11, 286, 362, 364, 1558, 11, 50920], "temperature": 0.0, "avg_logprob": -0.11216950106930423, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0012446421897038817}, {"id": 316, "seek": 159672, "start": 1607.84, "end": 1613.76, "text": " I'm going to go code up some code. I get to work. I forget about it. I move on with life. I come", "tokens": [50920, 286, 478, 516, 281, 352, 3089, 493, 512, 3089, 13, 286, 483, 281, 589, 13, 286, 2870, 466, 309, 13, 286, 1286, 322, 365, 993, 13, 286, 808, 51216], "temperature": 0.0, "avg_logprob": -0.11216950106930423, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0012446421897038817}, {"id": 317, "seek": 159672, "start": 1613.76, "end": 1617.3600000000001, "text": " back six months or a year or two years or three years later, you dust it off and you go use it", "tokens": [51216, 646, 2309, 2493, 420, 257, 1064, 420, 732, 924, 420, 1045, 924, 1780, 11, 291, 8634, 309, 766, 293, 291, 352, 764, 309, 51396], "temperature": 0.0, "avg_logprob": -0.11216950106930423, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0012446421897038817}, {"id": 318, "seek": 159672, "start": 1617.3600000000001, "end": 1622.32, "text": " again in a new environment. And maybe your GPU is different. Maybe you're running on a server", "tokens": [51396, 797, 294, 257, 777, 2823, 13, 400, 1310, 428, 18407, 307, 819, 13, 2704, 291, 434, 2614, 322, 257, 7154, 51644], "temperature": 0.0, "avg_logprob": -0.11216950106930423, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.0012446421897038817}, {"id": 319, "seek": 162232, "start": 1622.32, "end": 1627.28, "text": " instead of a laptop, maybe whatever, right? And so the problem now is you say, okay, well,", "tokens": [50364, 2602, 295, 257, 10732, 11, 1310, 2035, 11, 558, 30, 400, 370, 264, 1154, 586, 307, 291, 584, 11, 1392, 11, 731, 11, 50612], "temperature": 0.0, "avg_logprob": -0.10462655247868718, "compression_ratio": 1.7955271565495208, "no_speech_prob": 0.023684008046984673}, {"id": 320, "seek": 162232, "start": 1627.28, "end": 1630.3999999999999, "text": " I mean, again, not everybody cares about performance. But if you do, you say, okay,", "tokens": [50612, 286, 914, 11, 797, 11, 406, 2201, 12310, 466, 3389, 13, 583, 498, 291, 360, 11, 291, 584, 11, 1392, 11, 50768], "temperature": 0.0, "avg_logprob": -0.10462655247868718, "compression_ratio": 1.7955271565495208, "no_speech_prob": 0.023684008046984673}, {"id": 321, "seek": 162232, "start": 1630.3999999999999, "end": 1634.32, "text": " well, I want to take advantage of all these new features. I don't want to break the old thing,", "tokens": [50768, 731, 11, 286, 528, 281, 747, 5002, 295, 439, 613, 777, 4122, 13, 286, 500, 380, 528, 281, 1821, 264, 1331, 551, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10462655247868718, "compression_ratio": 1.7955271565495208, "no_speech_prob": 0.023684008046984673}, {"id": 322, "seek": 162232, "start": 1634.32, "end": 1640.3999999999999, "text": " though. Right. And so the typical way of handling this kind of stuff before is, you know, if you're", "tokens": [50964, 1673, 13, 1779, 13, 400, 370, 264, 7476, 636, 295, 13175, 341, 733, 295, 1507, 949, 307, 11, 291, 458, 11, 498, 291, 434, 51268], "temperature": 0.0, "avg_logprob": -0.10462655247868718, "compression_ratio": 1.7955271565495208, "no_speech_prob": 0.023684008046984673}, {"id": 323, "seek": 162232, "start": 1640.3999999999999, "end": 1645.36, "text": " talking about C++ templates, or you're talking about C with macros, you end up with if deaths,", "tokens": [51268, 1417, 466, 383, 25472, 21165, 11, 420, 291, 434, 1417, 466, 383, 365, 7912, 2635, 11, 291, 917, 493, 365, 498, 13027, 11, 51516], "temperature": 0.0, "avg_logprob": -0.10462655247868718, "compression_ratio": 1.7955271565495208, "no_speech_prob": 0.023684008046984673}, {"id": 324, "seek": 162232, "start": 1645.36, "end": 1649.4399999999998, "text": " you get like all these weird things get layered in, make the code super complicated. And then how", "tokens": [51516, 291, 483, 411, 439, 613, 3657, 721, 483, 34666, 294, 11, 652, 264, 3089, 1687, 6179, 13, 400, 550, 577, 51720], "temperature": 0.0, "avg_logprob": -0.10462655247868718, "compression_ratio": 1.7955271565495208, "no_speech_prob": 0.023684008046984673}, {"id": 325, "seek": 164944, "start": 1649.52, "end": 1654.4, "text": " do you test it? Right? Because this crazy complexity, multi-dimensional space you have to", "tokens": [50368, 360, 291, 1500, 309, 30, 1779, 30, 1436, 341, 3219, 14024, 11, 4825, 12, 18759, 1901, 291, 362, 281, 50612], "temperature": 0.0, "avg_logprob": -0.13901610015540994, "compression_ratio": 1.45703125, "no_speech_prob": 0.0012447100598365068}, {"id": 326, "seek": 164944, "start": 1654.4, "end": 1660.16, "text": " worry about. And, you know, that just doesn't scale very well. Actually, let me just jump around", "tokens": [50612, 3292, 466, 13, 400, 11, 291, 458, 11, 300, 445, 1177, 380, 4373, 588, 731, 13, 5135, 11, 718, 385, 445, 3012, 926, 50900], "temperature": 0.0, "avg_logprob": -0.13901610015540994, "compression_ratio": 1.45703125, "no_speech_prob": 0.0012447100598365068}, {"id": 327, "seek": 164944, "start": 1660.16, "end": 1665.28, "text": " before I go to some specific features. Like the increase in performance here that we're talking", "tokens": [50900, 949, 286, 352, 281, 512, 2685, 4122, 13, 1743, 264, 3488, 294, 3389, 510, 300, 321, 434, 1417, 51156], "temperature": 0.0, "avg_logprob": -0.13901610015540994, "compression_ratio": 1.45703125, "no_speech_prob": 0.0012447100598365068}, {"id": 328, "seek": 164944, "start": 1665.28, "end": 1675.28, "text": " about can be just insane. You write that Moja can provide a 35,000 x speed up over Python.", "tokens": [51156, 466, 393, 312, 445, 10838, 13, 509, 2464, 300, 3335, 2938, 393, 2893, 257, 6976, 11, 1360, 2031, 3073, 493, 670, 15329, 13, 51656], "temperature": 0.0, "avg_logprob": -0.13901610015540994, "compression_ratio": 1.45703125, "no_speech_prob": 0.0012447100598365068}, {"id": 329, "seek": 167528, "start": 1675.92, "end": 1683.84, "text": " How does it do that? Yeah, so it can even do more. But we'll get to that. So first of all,", "tokens": [50396, 1012, 775, 309, 360, 300, 30, 865, 11, 370, 309, 393, 754, 360, 544, 13, 583, 321, 603, 483, 281, 300, 13, 407, 700, 295, 439, 11, 50792], "temperature": 0.0, "avg_logprob": -0.1353242905413518, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0284279715269804}, {"id": 330, "seek": 167528, "start": 1683.84, "end": 1688.24, "text": " when we say that we're talking about what's called C Python, it's the default Python that", "tokens": [50792, 562, 321, 584, 300, 321, 434, 1417, 466, 437, 311, 1219, 383, 15329, 11, 309, 311, 264, 7576, 15329, 300, 51012], "temperature": 0.0, "avg_logprob": -0.1353242905413518, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0284279715269804}, {"id": 331, "seek": 167528, "start": 1688.24, "end": 1693.2, "text": " everybody uses when you type Python three, that's like typically the one you use, right? C Python is", "tokens": [51012, 2201, 4960, 562, 291, 2010, 15329, 1045, 11, 300, 311, 411, 5850, 264, 472, 291, 764, 11, 558, 30, 383, 15329, 307, 51260], "temperature": 0.0, "avg_logprob": -0.1353242905413518, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0284279715269804}, {"id": 332, "seek": 167528, "start": 1693.2, "end": 1699.28, "text": " an interpreter. And so interpreters, they have an extra layer of like byte codes and things like this", "tokens": [51260, 364, 34132, 13, 400, 370, 17489, 1559, 11, 436, 362, 364, 2857, 4583, 295, 411, 40846, 14211, 293, 721, 411, 341, 51564], "temperature": 0.0, "avg_logprob": -0.1353242905413518, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0284279715269804}, {"id": 333, "seek": 167528, "start": 1699.28, "end": 1703.36, "text": " that they have to go read, parse, interpret and make some kind of slow from that perspective.", "tokens": [51564, 300, 436, 362, 281, 352, 1401, 11, 48377, 11, 7302, 293, 652, 512, 733, 295, 2964, 490, 300, 4585, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1353242905413518, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.0284279715269804}, {"id": 334, "seek": 170336, "start": 1703.4399999999998, "end": 1708.56, "text": " And so one of the first things we do is we move to a compiler. And so I'm just moving to a compiler", "tokens": [50368, 400, 370, 472, 295, 264, 700, 721, 321, 360, 307, 321, 1286, 281, 257, 31958, 13, 400, 370, 286, 478, 445, 2684, 281, 257, 31958, 50624], "temperature": 0.0, "avg_logprob": -0.08777333796024323, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0008558016270399094}, {"id": 335, "seek": 170336, "start": 1708.56, "end": 1713.36, "text": " getting the interpreter out of the loop is two to five to 10 x speed up depending on the code.", "tokens": [50624, 1242, 264, 34132, 484, 295, 264, 6367, 307, 732, 281, 1732, 281, 1266, 2031, 3073, 493, 5413, 322, 264, 3089, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08777333796024323, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0008558016270399094}, {"id": 336, "seek": 170336, "start": 1713.36, "end": 1720.4799999999998, "text": " So just out of the gate, just using more modern techniques, right? Now, if you do that, one of", "tokens": [50864, 407, 445, 484, 295, 264, 8539, 11, 445, 1228, 544, 4363, 7512, 11, 558, 30, 823, 11, 498, 291, 360, 300, 11, 472, 295, 51220], "temperature": 0.0, "avg_logprob": -0.08777333796024323, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0008558016270399094}, {"id": 337, "seek": 170336, "start": 1720.4799999999998, "end": 1726.32, "text": " the things you can do is you can start to look at how C Python started to lay out data. And so one", "tokens": [51220, 264, 721, 291, 393, 360, 307, 291, 393, 722, 281, 574, 412, 577, 383, 15329, 1409, 281, 2360, 484, 1412, 13, 400, 370, 472, 51512], "temperature": 0.0, "avg_logprob": -0.08777333796024323, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0008558016270399094}, {"id": 338, "seek": 170336, "start": 1726.32, "end": 1731.76, "text": " of the things that C Python did, and this isn't part of the Python spec necessarily, but this is", "tokens": [51512, 295, 264, 721, 300, 383, 15329, 630, 11, 293, 341, 1943, 380, 644, 295, 264, 15329, 1608, 4725, 11, 457, 341, 307, 51784], "temperature": 0.0, "avg_logprob": -0.08777333796024323, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.0008558016270399094}, {"id": 339, "seek": 173176, "start": 1732.16, "end": 1739.12, "text": " sets of decisions, is that if you take an integer, for example, it'll put it in an object. In Python,", "tokens": [50384, 6352, 295, 5327, 11, 307, 300, 498, 291, 747, 364, 24922, 11, 337, 1365, 11, 309, 603, 829, 309, 294, 364, 2657, 13, 682, 15329, 11, 50732], "temperature": 0.0, "avg_logprob": -0.10974353046740516, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.0017544700531288981}, {"id": 340, "seek": 173176, "start": 1739.12, "end": 1745.04, "text": " everything's an object. And so they do the very logical thing of keeping the memory representation", "tokens": [50732, 1203, 311, 364, 2657, 13, 400, 370, 436, 360, 264, 588, 14978, 551, 295, 5145, 264, 4675, 10290, 51028], "temperature": 0.0, "avg_logprob": -0.10974353046740516, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.0017544700531288981}, {"id": 341, "seek": 173176, "start": 1745.04, "end": 1750.72, "text": " of all objects the same. So all objects have a header, they have like payload data. And what this", "tokens": [51028, 295, 439, 6565, 264, 912, 13, 407, 439, 6565, 362, 257, 23117, 11, 436, 362, 411, 30918, 1412, 13, 400, 437, 341, 51312], "temperature": 0.0, "avg_logprob": -0.10974353046740516, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.0017544700531288981}, {"id": 342, "seek": 173176, "start": 1750.72, "end": 1754.32, "text": " means is that every time you pass around an object, you're passing around a pointer to the data.", "tokens": [51312, 1355, 307, 300, 633, 565, 291, 1320, 926, 364, 2657, 11, 291, 434, 8437, 926, 257, 23918, 281, 264, 1412, 13, 51492], "temperature": 0.0, "avg_logprob": -0.10974353046740516, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.0017544700531288981}, {"id": 343, "seek": 173176, "start": 1755.36, "end": 1760.16, "text": " Well, this has overhead. It turns out that modern computers don't like chasing pointers very much", "tokens": [51544, 1042, 11, 341, 575, 19922, 13, 467, 4523, 484, 300, 4363, 10807, 500, 380, 411, 17876, 44548, 588, 709, 51784], "temperature": 0.0, "avg_logprob": -0.10974353046740516, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.0017544700531288981}, {"id": 344, "seek": 176016, "start": 1760.16, "end": 1764.3200000000002, "text": " in things like this. It means that you have to allocate the data. It means you have to reference", "tokens": [50364, 294, 721, 411, 341, 13, 467, 1355, 300, 291, 362, 281, 35713, 264, 1412, 13, 467, 1355, 291, 362, 281, 6408, 50572], "temperature": 0.0, "avg_logprob": -0.13530668123500553, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.014498850330710411}, {"id": 345, "seek": 176016, "start": 1764.3200000000002, "end": 1768.72, "text": " count it, which is another way that Python uses to keep track of memory. And so this has a lot", "tokens": [50572, 1207, 309, 11, 597, 307, 1071, 636, 300, 15329, 4960, 281, 1066, 2837, 295, 4675, 13, 400, 370, 341, 575, 257, 688, 50792], "temperature": 0.0, "avg_logprob": -0.13530668123500553, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.014498850330710411}, {"id": 346, "seek": 176016, "start": 1768.72, "end": 1776.0, "text": " of overhead. And so if you say, okay, let's try to get that out of the heap, out of a box, out of", "tokens": [50792, 295, 19922, 13, 400, 370, 498, 291, 584, 11, 1392, 11, 718, 311, 853, 281, 483, 300, 484, 295, 264, 33591, 11, 484, 295, 257, 2424, 11, 484, 295, 51156], "temperature": 0.0, "avg_logprob": -0.13530668123500553, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.014498850330710411}, {"id": 347, "seek": 176016, "start": 1776.0, "end": 1783.52, "text": " an indirection, and into the registers. That's that's another 10 x. So it adds up if you if", "tokens": [51156, 364, 1016, 621, 882, 11, 293, 666, 264, 38351, 13, 663, 311, 300, 311, 1071, 1266, 2031, 13, 407, 309, 10860, 493, 498, 291, 498, 51532], "temperature": 0.0, "avg_logprob": -0.13530668123500553, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.014498850330710411}, {"id": 348, "seek": 176016, "start": 1783.52, "end": 1788.3200000000002, "text": " you're reference counting every single every single thing you create that adds up. Yep. And", "tokens": [51532, 291, 434, 6408, 13251, 633, 2167, 633, 2167, 551, 291, 1884, 300, 10860, 493, 13, 7010, 13, 400, 51772], "temperature": 0.0, "avg_logprob": -0.13530668123500553, "compression_ratio": 1.8122605363984674, "no_speech_prob": 0.014498850330710411}, {"id": 349, "seek": 178832, "start": 1788.32, "end": 1791.9199999999998, "text": " if you look at, you know, people complain about the Python Gil, this is one of the things that", "tokens": [50364, 498, 291, 574, 412, 11, 291, 458, 11, 561, 11024, 466, 264, 15329, 17654, 11, 341, 307, 472, 295, 264, 721, 300, 50544], "temperature": 0.0, "avg_logprob": -0.12134515422664277, "compression_ratio": 1.8233438485804416, "no_speech_prob": 0.0003353231877554208}, {"id": 350, "seek": 178832, "start": 1791.9199999999998, "end": 1797.4399999999998, "text": " hurts parallelism. That's because the reference counting. Right. And so the Gil and reference", "tokens": [50544, 11051, 8952, 1434, 13, 663, 311, 570, 264, 6408, 13251, 13, 1779, 13, 400, 370, 264, 17654, 293, 6408, 50820], "temperature": 0.0, "avg_logprob": -0.12134515422664277, "compression_ratio": 1.8233438485804416, "no_speech_prob": 0.0003353231877554208}, {"id": 351, "seek": 178832, "start": 1797.4399999999998, "end": 1801.04, "text": " counting are very tightly intertwined in Python. It's not the only thing, but it's very tightly", "tokens": [50820, 13251, 366, 588, 21952, 44400, 2001, 294, 15329, 13, 467, 311, 406, 264, 787, 551, 11, 457, 309, 311, 588, 21952, 51000], "temperature": 0.0, "avg_logprob": -0.12134515422664277, "compression_ratio": 1.8233438485804416, "no_speech_prob": 0.0003353231877554208}, {"id": 352, "seek": 178832, "start": 1801.04, "end": 1805.6, "text": " intertwined. And so then you lean into this and you say, okay, cool, well, modern computers, they", "tokens": [51000, 44400, 2001, 13, 400, 370, 550, 291, 11659, 666, 341, 293, 291, 584, 11, 1392, 11, 1627, 11, 731, 11, 4363, 10807, 11, 436, 51228], "temperature": 0.0, "avg_logprob": -0.12134515422664277, "compression_ratio": 1.8233438485804416, "no_speech_prob": 0.0003353231877554208}, {"id": 353, "seek": 178832, "start": 1805.6, "end": 1809.76, "text": " can do more than one operation at a time. And so they have vectors. What is a vector? Well, a vector", "tokens": [51228, 393, 360, 544, 813, 472, 6916, 412, 257, 565, 13, 400, 370, 436, 362, 18875, 13, 708, 307, 257, 8062, 30, 1042, 11, 257, 8062, 51436], "temperature": 0.0, "avg_logprob": -0.12134515422664277, "compression_ratio": 1.8233438485804416, "no_speech_prob": 0.0003353231877554208}, {"id": 354, "seek": 178832, "start": 1809.76, "end": 1813.6, "text": " allows you to take one, instead of taking one piece of data doing an ad or a multiply and then", "tokens": [51436, 4045, 291, 281, 747, 472, 11, 2602, 295, 1940, 472, 2522, 295, 1412, 884, 364, 614, 420, 257, 12972, 293, 550, 51628], "temperature": 0.0, "avg_logprob": -0.12134515422664277, "compression_ratio": 1.8233438485804416, "no_speech_prob": 0.0003353231877554208}, {"id": 355, "seek": 181360, "start": 1814.3999999999999, "end": 1819.1999999999998, "text": " pick up the next one, you can now do four or eight or 16 or 32 at a time. Right. Well, Python", "tokens": [50404, 1888, 493, 264, 958, 472, 11, 291, 393, 586, 360, 1451, 420, 3180, 420, 3165, 420, 8858, 412, 257, 565, 13, 1779, 13, 1042, 11, 15329, 50644], "temperature": 0.0, "avg_logprob": -0.1247626242877768, "compression_ratio": 1.7484076433121019, "no_speech_prob": 0.005553873255848885}, {"id": 356, "seek": 181360, "start": 1819.1999999999998, "end": 1823.1999999999998, "text": " doesn't expose that because of reasons. And so now you can say, okay, well, you can adopt that.", "tokens": [50644, 1177, 380, 19219, 300, 570, 295, 4112, 13, 400, 370, 586, 291, 393, 584, 11, 1392, 11, 731, 11, 291, 393, 6878, 300, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1247626242877768, "compression_ratio": 1.7484076433121019, "no_speech_prob": 0.005553873255848885}, {"id": 357, "seek": 181360, "start": 1823.9199999999998, "end": 1827.36, "text": " Now you have threads. Now you have like additional things like you can control memory", "tokens": [50880, 823, 291, 362, 19314, 13, 823, 291, 362, 411, 4497, 721, 411, 291, 393, 1969, 4675, 51052], "temperature": 0.0, "avg_logprob": -0.1247626242877768, "compression_ratio": 1.7484076433121019, "no_speech_prob": 0.005553873255848885}, {"id": 358, "seek": 181360, "start": 1827.36, "end": 1831.12, "text": " hierarchy. And so what mojo allows you to do is it allows you to start taking advantage of", "tokens": [51052, 22333, 13, 400, 370, 437, 705, 5134, 4045, 291, 281, 360, 307, 309, 4045, 291, 281, 722, 1940, 5002, 295, 51240], "temperature": 0.0, "avg_logprob": -0.1247626242877768, "compression_ratio": 1.7484076433121019, "no_speech_prob": 0.005553873255848885}, {"id": 359, "seek": 181360, "start": 1831.12, "end": 1834.8, "text": " all these powerful things have been built into the hardware over time. And it gives", "tokens": [51240, 439, 613, 4005, 721, 362, 668, 3094, 666, 264, 8837, 670, 565, 13, 400, 309, 2709, 51424], "temperature": 0.0, "avg_logprob": -0.1247626242877768, "compression_ratio": 1.7484076433121019, "no_speech_prob": 0.005553873255848885}, {"id": 360, "seek": 181360, "start": 1835.4399999999998, "end": 1841.52, "text": " the library gives very nice features. So you can say, just parallelize, let's do this in parallel.", "tokens": [51456, 264, 6405, 2709, 588, 1481, 4122, 13, 407, 291, 393, 584, 11, 445, 8952, 1125, 11, 718, 311, 360, 341, 294, 8952, 13, 51760], "temperature": 0.0, "avg_logprob": -0.1247626242877768, "compression_ratio": 1.7484076433121019, "no_speech_prob": 0.005553873255848885}, {"id": 361, "seek": 184152, "start": 1841.92, "end": 1848.16, "text": " So it's very, very powerful weapons against slowness, which is why people have been I think", "tokens": [50384, 407, 309, 311, 588, 11, 588, 4005, 7278, 1970, 1061, 648, 442, 11, 597, 307, 983, 561, 362, 668, 286, 519, 50696], "temperature": 0.0, "avg_logprob": -0.13455590468186598, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.0008294879226014018}, {"id": 362, "seek": 184152, "start": 1848.16, "end": 1852.4, "text": " having fun like just taking code and making go fast because it's just kind of an adrenaline rush", "tokens": [50696, 1419, 1019, 411, 445, 1940, 3089, 293, 1455, 352, 2370, 570, 309, 311, 445, 733, 295, 364, 35649, 9300, 50908], "temperature": 0.0, "avg_logprob": -0.13455590468186598, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.0008294879226014018}, {"id": 363, "seek": 184152, "start": 1852.4, "end": 1855.92, "text": " to see like how fast you can get things. Before I talk about some of the interesting", "tokens": [50908, 281, 536, 411, 577, 2370, 291, 393, 483, 721, 13, 4546, 286, 751, 466, 512, 295, 264, 1880, 51084], "temperature": 0.0, "avg_logprob": -0.13455590468186598, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.0008294879226014018}, {"id": 364, "seek": 184152, "start": 1855.92, "end": 1860.24, "text": " stuff with parallelization, all that, let's first talk about like the basics. We talked", "tokens": [51084, 1507, 365, 8952, 2144, 11, 439, 300, 11, 718, 311, 700, 751, 466, 411, 264, 14688, 13, 492, 2825, 51300], "temperature": 0.0, "avg_logprob": -0.13455590468186598, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.0008294879226014018}, {"id": 365, "seek": 184152, "start": 1860.24, "end": 1865.92, "text": " about indentation, right? So this thing looks like Python. It's sexy and beautiful like Python,", "tokens": [51300, 466, 44494, 399, 11, 558, 30, 407, 341, 551, 1542, 411, 15329, 13, 467, 311, 13701, 293, 2238, 411, 15329, 11, 51584], "temperature": 0.0, "avg_logprob": -0.13455590468186598, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.0008294879226014018}, {"id": 366, "seek": 184152, "start": 1865.92, "end": 1870.72, "text": " as I mentioned. Is it a typed language? So what's the role of types?", "tokens": [51584, 382, 286, 2835, 13, 1119, 309, 257, 33941, 2856, 30, 407, 437, 311, 264, 3090, 295, 3467, 30, 51824], "temperature": 0.0, "avg_logprob": -0.13455590468186598, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.0008294879226014018}, {"id": 367, "seek": 187072, "start": 1870.72, "end": 1877.28, "text": " Yeah, good question. So Python has types. It has strings as integers, it has dictionaries and", "tokens": [50364, 865, 11, 665, 1168, 13, 407, 15329, 575, 3467, 13, 467, 575, 13985, 382, 41674, 11, 309, 575, 22352, 4889, 293, 50692], "temperature": 0.0, "avg_logprob": -0.1293085454925289, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0014548631152138114}, {"id": 368, "seek": 187072, "start": 1877.28, "end": 1882.56, "text": " like all that stuff. But they all live at runtime. Right. And so because all those types live at", "tokens": [50692, 411, 439, 300, 1507, 13, 583, 436, 439, 1621, 412, 34474, 13, 1779, 13, 400, 370, 570, 439, 729, 3467, 1621, 412, 50956], "temperature": 0.0, "avg_logprob": -0.1293085454925289, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0014548631152138114}, {"id": 369, "seek": 187072, "start": 1882.56, "end": 1888.0, "text": " runtime and Python, you never, you don't have to spell them. Python also has like this whole", "tokens": [50956, 34474, 293, 15329, 11, 291, 1128, 11, 291, 500, 380, 362, 281, 9827, 552, 13, 15329, 611, 575, 411, 341, 1379, 51228], "temperature": 0.0, "avg_logprob": -0.1293085454925289, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0014548631152138114}, {"id": 370, "seek": 187072, "start": 1888.0, "end": 1891.68, "text": " typing thing going on now. And a lot of people use it. Yeah, I'm not talking about that. That's", "tokens": [51228, 18444, 551, 516, 322, 586, 13, 400, 257, 688, 295, 561, 764, 309, 13, 865, 11, 286, 478, 406, 1417, 466, 300, 13, 663, 311, 51412], "temperature": 0.0, "avg_logprob": -0.1293085454925289, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0014548631152138114}, {"id": 371, "seek": 187072, "start": 1891.68, "end": 1895.2, "text": " kind of a different thing. We can go back to that if you want. But, but typically the,", "tokens": [51412, 733, 295, 257, 819, 551, 13, 492, 393, 352, 646, 281, 300, 498, 291, 528, 13, 583, 11, 457, 5850, 264, 11, 51588], "temperature": 0.0, "avg_logprob": -0.1293085454925289, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.0014548631152138114}, {"id": 372, "seek": 189520, "start": 1895.8400000000001, "end": 1900.8, "text": " you know, you just say I take, I have a def and my def takes two parameters. I'm going to call them", "tokens": [50396, 291, 458, 11, 291, 445, 584, 286, 747, 11, 286, 362, 257, 1060, 293, 452, 1060, 2516, 732, 9834, 13, 286, 478, 516, 281, 818, 552, 50644], "temperature": 0.0, "avg_logprob": -0.1270948425541079, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.003483211388811469}, {"id": 373, "seek": 189520, "start": 1900.8, "end": 1907.1200000000001, "text": " A and B and I don't have to write a type. Okay. So that is great. But what that does is that forces", "tokens": [50644, 316, 293, 363, 293, 286, 500, 380, 362, 281, 2464, 257, 2010, 13, 1033, 13, 407, 300, 307, 869, 13, 583, 437, 300, 775, 307, 300, 5874, 50960], "temperature": 0.0, "avg_logprob": -0.1270948425541079, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.003483211388811469}, {"id": 374, "seek": 189520, "start": 1907.1200000000001, "end": 1911.68, "text": " what's called a consistent representation. So these things have to be a pointer to an object", "tokens": [50960, 437, 311, 1219, 257, 8398, 10290, 13, 407, 613, 721, 362, 281, 312, 257, 23918, 281, 364, 2657, 51188], "temperature": 0.0, "avg_logprob": -0.1270948425541079, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.003483211388811469}, {"id": 375, "seek": 189520, "start": 1911.68, "end": 1916.32, "text": " with the object header, and they all have to look the same. And then when you dispatch a method,", "tokens": [51188, 365, 264, 2657, 23117, 11, 293, 436, 439, 362, 281, 574, 264, 912, 13, 400, 550, 562, 291, 36729, 257, 3170, 11, 51420], "temperature": 0.0, "avg_logprob": -0.1270948425541079, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.003483211388811469}, {"id": 376, "seek": 189520, "start": 1916.32, "end": 1920.4, "text": " you go through all the same different paths, no matter what the receiver or whatever that type", "tokens": [51420, 291, 352, 807, 439, 264, 912, 819, 14518, 11, 572, 1871, 437, 264, 20086, 420, 2035, 300, 2010, 51624], "temperature": 0.0, "avg_logprob": -0.1270948425541079, "compression_ratio": 1.7285714285714286, "no_speech_prob": 0.003483211388811469}, {"id": 377, "seek": 192040, "start": 1920.4, "end": 1925.76, "text": " is. So what Mojo does is it allows you to have more than one kind of type. And so what it does", "tokens": [50364, 307, 13, 407, 437, 3335, 5134, 775, 307, 309, 4045, 291, 281, 362, 544, 813, 472, 733, 295, 2010, 13, 400, 370, 437, 309, 775, 50632], "temperature": 0.0, "avg_logprob": -0.13001921829903962, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.021612079814076424}, {"id": 378, "seek": 192040, "start": 1925.76, "end": 1930.0800000000002, "text": " is allows you to say, okay, cool, I have, I have an object and objects behave like Python does. And", "tokens": [50632, 307, 4045, 291, 281, 584, 11, 1392, 11, 1627, 11, 286, 362, 11, 286, 362, 364, 2657, 293, 6565, 15158, 411, 15329, 775, 13, 400, 50848], "temperature": 0.0, "avg_logprob": -0.13001921829903962, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.021612079814076424}, {"id": 379, "seek": 192040, "start": 1930.0800000000002, "end": 1934.48, "text": " so it's fully dynamic and that's all great. And for many things classes, like that's all very", "tokens": [50848, 370, 309, 311, 4498, 8546, 293, 300, 311, 439, 869, 13, 400, 337, 867, 721, 5359, 11, 411, 300, 311, 439, 588, 51068], "temperature": 0.0, "avg_logprob": -0.13001921829903962, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.021612079814076424}, {"id": 380, "seek": 192040, "start": 1934.48, "end": 1939.76, "text": " powerful and very important. But if you want to say, Hey, it's an integer, and it's 32 bits or 64", "tokens": [51068, 4005, 293, 588, 1021, 13, 583, 498, 291, 528, 281, 584, 11, 1911, 11, 309, 311, 364, 24922, 11, 293, 309, 311, 8858, 9239, 420, 12145, 51332], "temperature": 0.0, "avg_logprob": -0.13001921829903962, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.021612079814076424}, {"id": 381, "seek": 192040, "start": 1939.76, "end": 1945.2, "text": " bits or whatever it is, or it's a floating point value, it's 64 bits. Well, then the compiler can", "tokens": [51332, 9239, 420, 2035, 309, 307, 11, 420, 309, 311, 257, 12607, 935, 2158, 11, 309, 311, 12145, 9239, 13, 1042, 11, 550, 264, 31958, 393, 51604], "temperature": 0.0, "avg_logprob": -0.13001921829903962, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.021612079814076424}, {"id": 382, "seek": 192040, "start": 1945.2, "end": 1949.68, "text": " take that and it can use that to do way better optimization. And it turns out again, getting", "tokens": [51604, 747, 300, 293, 309, 393, 764, 300, 281, 360, 636, 1101, 19618, 13, 400, 309, 4523, 484, 797, 11, 1242, 51828], "temperature": 0.0, "avg_logprob": -0.13001921829903962, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.021612079814076424}, {"id": 383, "seek": 194968, "start": 1949.68, "end": 1954.16, "text": " rid of the interactions, that's huge, means you can get better code completion because you have,", "tokens": [50364, 3973, 295, 264, 13280, 11, 300, 311, 2603, 11, 1355, 291, 393, 483, 1101, 3089, 19372, 570, 291, 362, 11, 50588], "temperature": 0.0, "avg_logprob": -0.09453767244933081, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.006901855580508709}, {"id": 384, "seek": 194968, "start": 1954.8, "end": 1959.44, "text": " because compiler knows what the type is. And so it knows what operations work on it. And so that's", "tokens": [50620, 570, 31958, 3255, 437, 264, 2010, 307, 13, 400, 370, 309, 3255, 437, 7705, 589, 322, 309, 13, 400, 370, 300, 311, 50852], "temperature": 0.0, "avg_logprob": -0.09453767244933081, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.006901855580508709}, {"id": 385, "seek": 194968, "start": 1959.44, "end": 1966.4, "text": " actually pretty huge. And so what Mojo does allows you to progressively adopt types into your program.", "tokens": [50852, 767, 1238, 2603, 13, 400, 370, 437, 3335, 5134, 775, 4045, 291, 281, 46667, 6878, 3467, 666, 428, 1461, 13, 51200], "temperature": 0.0, "avg_logprob": -0.09453767244933081, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.006901855580508709}, {"id": 386, "seek": 194968, "start": 1966.4, "end": 1970.8, "text": " And so you can start again, it's compatible with Python. And so then you can add however many types", "tokens": [51200, 400, 370, 291, 393, 722, 797, 11, 309, 311, 18218, 365, 15329, 13, 400, 370, 550, 291, 393, 909, 4461, 867, 3467, 51420], "temperature": 0.0, "avg_logprob": -0.09453767244933081, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.006901855580508709}, {"id": 387, "seek": 194968, "start": 1970.8, "end": 1974.0, "text": " you want, wherever you want them. And if you don't want to deal with it, you don't have to deal with", "tokens": [51420, 291, 528, 11, 8660, 291, 528, 552, 13, 400, 498, 291, 500, 380, 528, 281, 2028, 365, 309, 11, 291, 500, 380, 362, 281, 2028, 365, 51580], "temperature": 0.0, "avg_logprob": -0.09453767244933081, "compression_ratio": 1.897338403041825, "no_speech_prob": 0.006901855580508709}, {"id": 388, "seek": 197400, "start": 1974.0, "end": 1980.88, "text": " it. Right. And so one of one of, you know, our opinions on this is it's not that types are the", "tokens": [50364, 309, 13, 1779, 13, 400, 370, 472, 295, 472, 295, 11, 291, 458, 11, 527, 11819, 322, 341, 307, 309, 311, 406, 300, 3467, 366, 264, 50708], "temperature": 0.0, "avg_logprob": -0.18317236341871657, "compression_ratio": 1.6535433070866141, "no_speech_prob": 0.014952939003705978}, {"id": 389, "seek": 197400, "start": 1980.88, "end": 1984.24, "text": " right thing or the wrong thing. It's that they're a useful thing.", "tokens": [50708, 558, 551, 420, 264, 2085, 551, 13, 467, 311, 300, 436, 434, 257, 4420, 551, 13, 50876], "temperature": 0.0, "avg_logprob": -0.18317236341871657, "compression_ratio": 1.6535433070866141, "no_speech_prob": 0.014952939003705978}, {"id": 390, "seek": 197400, "start": 1985.36, "end": 1989.2, "text": " Which was kind of optional. It's not strict typing, you don't have to specify type.", "tokens": [50932, 3013, 390, 733, 295, 17312, 13, 467, 311, 406, 10910, 18444, 11, 291, 500, 380, 362, 281, 16500, 2010, 13, 51124], "temperature": 0.0, "avg_logprob": -0.18317236341871657, "compression_ratio": 1.6535433070866141, "no_speech_prob": 0.014952939003705978}, {"id": 391, "seek": 197400, "start": 1989.2, "end": 1994.4, "text": " Exactly. Okay, so starting from the thing that Python is kind of reaching towards right now with", "tokens": [51124, 7587, 13, 1033, 11, 370, 2891, 490, 264, 551, 300, 15329, 307, 733, 295, 9906, 3030, 558, 586, 365, 51384], "temperature": 0.0, "avg_logprob": -0.18317236341871657, "compression_ratio": 1.6535433070866141, "no_speech_prob": 0.014952939003705978}, {"id": 392, "seek": 197400, "start": 1995.04, "end": 1999.68, "text": " trying to inject types into it. Yeah, with a very different approach. But yes.", "tokens": [51416, 1382, 281, 10711, 3467, 666, 309, 13, 865, 11, 365, 257, 588, 819, 3109, 13, 583, 2086, 13, 51648], "temperature": 0.0, "avg_logprob": -0.18317236341871657, "compression_ratio": 1.6535433070866141, "no_speech_prob": 0.014952939003705978}, {"id": 393, "seek": 199968, "start": 2000.0800000000002, "end": 2002.8, "text": " What's the different approach? I'm actually one of the people", "tokens": [50384, 708, 311, 264, 819, 3109, 30, 286, 478, 767, 472, 295, 264, 561, 50520], "temperature": 0.0, "avg_logprob": -0.1302867295607081, "compression_ratio": 1.5756457564575646, "no_speech_prob": 0.007228788919746876}, {"id": 394, "seek": 199968, "start": 2004.3200000000002, "end": 2006.72, "text": " that have not been using types very much in Python.", "tokens": [50596, 300, 362, 406, 668, 1228, 3467, 588, 709, 294, 15329, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1302867295607081, "compression_ratio": 1.5756457564575646, "no_speech_prob": 0.007228788919746876}, {"id": 395, "seek": 199968, "start": 2006.72, "end": 2008.48, "text": " That's okay. Why did you say?", "tokens": [50716, 663, 311, 1392, 13, 1545, 630, 291, 584, 30, 50804], "temperature": 0.0, "avg_logprob": -0.1302867295607081, "compression_ratio": 1.5756457564575646, "no_speech_prob": 0.007228788919746876}, {"id": 396, "seek": 199968, "start": 2009.52, "end": 2016.8, "text": " It just, well, because I know the importance it's like adults use strict typing. And so I refuse", "tokens": [50856, 467, 445, 11, 731, 11, 570, 286, 458, 264, 7379, 309, 311, 411, 8865, 764, 10910, 18444, 13, 400, 370, 286, 16791, 51220], "temperature": 0.0, "avg_logprob": -0.1302867295607081, "compression_ratio": 1.5756457564575646, "no_speech_prob": 0.007228788919746876}, {"id": 397, "seek": 199968, "start": 2016.8, "end": 2023.6000000000001, "text": " to grow up in that sense. It's a kind of rebellion. But I just know that it probably reduces the", "tokens": [51220, 281, 1852, 493, 294, 300, 2020, 13, 467, 311, 257, 733, 295, 29793, 13, 583, 286, 445, 458, 300, 309, 1391, 18081, 264, 51560], "temperature": 0.0, "avg_logprob": -0.1302867295607081, "compression_ratio": 1.5756457564575646, "no_speech_prob": 0.007228788919746876}, {"id": 398, "seek": 199968, "start": 2023.6000000000001, "end": 2027.68, "text": " amount of errors even just for forget about performance improvements, it probably reduces", "tokens": [51560, 2372, 295, 13603, 754, 445, 337, 2870, 466, 3389, 13797, 11, 309, 1391, 18081, 51764], "temperature": 0.0, "avg_logprob": -0.1302867295607081, "compression_ratio": 1.5756457564575646, "no_speech_prob": 0.007228788919746876}, {"id": 399, "seek": 202768, "start": 2027.68, "end": 2031.04, "text": " errors when you do strict typing. Yeah, so I mean, I think it's interesting if you look at that,", "tokens": [50364, 13603, 562, 291, 360, 10910, 18444, 13, 865, 11, 370, 286, 914, 11, 286, 519, 309, 311, 1880, 498, 291, 574, 412, 300, 11, 50532], "temperature": 0.0, "avg_logprob": -0.17437854566072164, "compression_ratio": 1.7134146341463414, "no_speech_prob": 0.0059089879505336285}, {"id": 400, "seek": 202768, "start": 2031.04, "end": 2037.6000000000001, "text": " right? And the reason I'm giving a hard time is that there's this cultural norm, this pressure,", "tokens": [50532, 558, 30, 400, 264, 1778, 286, 478, 2902, 257, 1152, 565, 307, 300, 456, 311, 341, 6988, 2026, 11, 341, 3321, 11, 50860], "temperature": 0.0, "avg_logprob": -0.17437854566072164, "compression_ratio": 1.7134146341463414, "no_speech_prob": 0.0059089879505336285}, {"id": 401, "seek": 202768, "start": 2037.6000000000001, "end": 2041.6000000000001, "text": " this like, there has to be a right way to do things. Like, you know, grown-ups only do it one", "tokens": [50860, 341, 411, 11, 456, 575, 281, 312, 257, 558, 636, 281, 360, 721, 13, 1743, 11, 291, 458, 11, 7709, 12, 7528, 787, 360, 309, 472, 51060], "temperature": 0.0, "avg_logprob": -0.17437854566072164, "compression_ratio": 1.7134146341463414, "no_speech_prob": 0.0059089879505336285}, {"id": 402, "seek": 202768, "start": 2041.6000000000001, "end": 2045.52, "text": " way. And if you don't do that, you should feel bad. Right. Like some people feel like Python's a", "tokens": [51060, 636, 13, 400, 498, 291, 500, 380, 360, 300, 11, 291, 820, 841, 1578, 13, 1779, 13, 1743, 512, 561, 841, 411, 15329, 311, 257, 51256], "temperature": 0.0, "avg_logprob": -0.17437854566072164, "compression_ratio": 1.7134146341463414, "no_speech_prob": 0.0059089879505336285}, {"id": 403, "seek": 202768, "start": 2045.52, "end": 2049.12, "text": " guilty pleasure or something. And it's like, when it gets serious, I need to go rewrite it.", "tokens": [51256, 12341, 6834, 420, 746, 13, 400, 309, 311, 411, 11, 562, 309, 2170, 3156, 11, 286, 643, 281, 352, 28132, 309, 13, 51436], "temperature": 0.0, "avg_logprob": -0.17437854566072164, "compression_ratio": 1.7134146341463414, "no_speech_prob": 0.0059089879505336285}, {"id": 404, "seek": 202768, "start": 2051.12, "end": 2054.4, "text": " I mean, cool. I understand history and I understand kind of where this comes from. But", "tokens": [51536, 286, 914, 11, 1627, 13, 286, 1223, 2503, 293, 286, 1223, 733, 295, 689, 341, 1487, 490, 13, 583, 51700], "temperature": 0.0, "avg_logprob": -0.17437854566072164, "compression_ratio": 1.7134146341463414, "no_speech_prob": 0.0059089879505336285}, {"id": 405, "seek": 205440, "start": 2055.04, "end": 2059.12, "text": " I don't think it has to be guilty pleasure. Yeah. So if you look at that, you say,", "tokens": [50396, 286, 500, 380, 519, 309, 575, 281, 312, 12341, 6834, 13, 865, 13, 407, 498, 291, 574, 412, 300, 11, 291, 584, 11, 50600], "temperature": 0.0, "avg_logprob": -0.10485632523246434, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.0003053372201975435}, {"id": 406, "seek": 205440, "start": 2059.12, "end": 2062.88, "text": " why do you have to rewrite it? Well, you have to rewrite it to deploy. Well, why do you want to", "tokens": [50600, 983, 360, 291, 362, 281, 28132, 309, 30, 1042, 11, 291, 362, 281, 28132, 309, 281, 7274, 13, 1042, 11, 983, 360, 291, 528, 281, 50788], "temperature": 0.0, "avg_logprob": -0.10485632523246434, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.0003053372201975435}, {"id": 407, "seek": 205440, "start": 2062.88, "end": 2067.6800000000003, "text": " deploy? Well, you care about performance, you care about productivity, or you want, you know,", "tokens": [50788, 7274, 30, 1042, 11, 291, 1127, 466, 3389, 11, 291, 1127, 466, 15604, 11, 420, 291, 528, 11, 291, 458, 11, 51028], "temperature": 0.0, "avg_logprob": -0.10485632523246434, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.0003053372201975435}, {"id": 408, "seek": 205440, "start": 2067.6800000000003, "end": 2071.12, "text": " a tiny thing on the server that has no dependencies, or, you know, you have", "tokens": [51028, 257, 5870, 551, 322, 264, 7154, 300, 575, 572, 36606, 11, 420, 11, 291, 458, 11, 291, 362, 51200], "temperature": 0.0, "avg_logprob": -0.10485632523246434, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.0003053372201975435}, {"id": 409, "seek": 205440, "start": 2071.12, "end": 2076.7200000000003, "text": " objectives that you're trying to attain. So what if Python can achieve those objectives?", "tokens": [51200, 15961, 300, 291, 434, 1382, 281, 23766, 13, 407, 437, 498, 15329, 393, 4584, 729, 15961, 30, 51480], "temperature": 0.0, "avg_logprob": -0.10485632523246434, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.0003053372201975435}, {"id": 410, "seek": 205440, "start": 2077.52, "end": 2080.32, "text": " So if you want types, well, maybe you want types because you want to make sure you're", "tokens": [51520, 407, 498, 291, 528, 3467, 11, 731, 11, 1310, 291, 528, 3467, 570, 291, 528, 281, 652, 988, 291, 434, 51660], "temperature": 0.0, "avg_logprob": -0.10485632523246434, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.0003053372201975435}, {"id": 411, "seek": 208032, "start": 2080.32, "end": 2085.6800000000003, "text": " passing the right thing. Sure, you can add a type. If you don't care, you're prototyping some stuff,", "tokens": [50364, 8437, 264, 558, 551, 13, 4894, 11, 291, 393, 909, 257, 2010, 13, 759, 291, 500, 380, 1127, 11, 291, 434, 46219, 3381, 512, 1507, 11, 50632], "temperature": 0.0, "avg_logprob": -0.11702623501629897, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.028430212289094925}, {"id": 412, "seek": 208032, "start": 2085.6800000000003, "end": 2088.88, "text": " you're hacking some things out, you're like pulling some MAM code off the internet,", "tokens": [50632, 291, 434, 31422, 512, 721, 484, 11, 291, 434, 411, 8407, 512, 376, 2865, 3089, 766, 264, 4705, 11, 50792], "temperature": 0.0, "avg_logprob": -0.11702623501629897, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.028430212289094925}, {"id": 413, "seek": 208032, "start": 2088.88, "end": 2094.2400000000002, "text": " it should just work. Right. And you shouldn't be like pressured. You shouldn't feel bad", "tokens": [50792, 309, 820, 445, 589, 13, 1779, 13, 400, 291, 4659, 380, 312, 411, 45306, 13, 509, 4659, 380, 841, 1578, 51060], "temperature": 0.0, "avg_logprob": -0.11702623501629897, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.028430212289094925}, {"id": 414, "seek": 208032, "start": 2094.2400000000002, "end": 2098.7200000000003, "text": " about doing the right thing or the thing that feels good. Now, if you're in a team, right,", "tokens": [51060, 466, 884, 264, 558, 551, 420, 264, 551, 300, 3417, 665, 13, 823, 11, 498, 291, 434, 294, 257, 1469, 11, 558, 11, 51284], "temperature": 0.0, "avg_logprob": -0.11702623501629897, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.028430212289094925}, {"id": 415, "seek": 208032, "start": 2098.7200000000003, "end": 2103.6800000000003, "text": " you're working at some massive internet company and you have 400 million lines of Python code,", "tokens": [51284, 291, 434, 1364, 412, 512, 5994, 4705, 2237, 293, 291, 362, 8423, 2459, 3876, 295, 15329, 3089, 11, 51532], "temperature": 0.0, "avg_logprob": -0.11702623501629897, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.028430212289094925}, {"id": 416, "seek": 208032, "start": 2103.6800000000003, "end": 2107.76, "text": " well, they may have a house rule that you use types. Yeah. Right. Because it makes it easier", "tokens": [51532, 731, 11, 436, 815, 362, 257, 1782, 4978, 300, 291, 764, 3467, 13, 865, 13, 1779, 13, 1436, 309, 1669, 309, 3571, 51736], "temperature": 0.0, "avg_logprob": -0.11702623501629897, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.028430212289094925}, {"id": 417, "seek": 210776, "start": 2107.76, "end": 2111.92, "text": " for different humans to talk to each other and understand what's going on and bugs at scale.", "tokens": [50364, 337, 819, 6255, 281, 751, 281, 1184, 661, 293, 1223, 437, 311, 516, 322, 293, 15120, 412, 4373, 13, 50572], "temperature": 0.0, "avg_logprob": -0.1061345697418461, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.01406020950525999}, {"id": 418, "seek": 210776, "start": 2111.92, "end": 2116.88, "text": " Right. And so there are lots of good reasons why you might want to use types. But that doesn't", "tokens": [50572, 1779, 13, 400, 370, 456, 366, 3195, 295, 665, 4112, 983, 291, 1062, 528, 281, 764, 3467, 13, 583, 300, 1177, 380, 50820], "temperature": 0.0, "avg_logprob": -0.1061345697418461, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.01406020950525999}, {"id": 419, "seek": 210776, "start": 2116.88, "end": 2120.48, "text": " mean that everybody should use them all the time. Right. So what Mojo does is it says, cool, well,", "tokens": [50820, 914, 300, 2201, 820, 764, 552, 439, 264, 565, 13, 1779, 13, 407, 437, 3335, 5134, 775, 307, 309, 1619, 11, 1627, 11, 731, 11, 51000], "temperature": 0.0, "avg_logprob": -0.1061345697418461, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.01406020950525999}, {"id": 420, "seek": 210776, "start": 2121.2000000000003, "end": 2125.6000000000004, "text": " allow people to use types. And if you use types, you get nice things out of it. Right. You get", "tokens": [51036, 2089, 561, 281, 764, 3467, 13, 400, 498, 291, 764, 3467, 11, 291, 483, 1481, 721, 484, 295, 309, 13, 1779, 13, 509, 483, 51256], "temperature": 0.0, "avg_logprob": -0.1061345697418461, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.01406020950525999}, {"id": 421, "seek": 210776, "start": 2125.6000000000004, "end": 2132.1600000000003, "text": " better performance and things like this. Right. But Mojo is a full compatible superset of Python.", "tokens": [51256, 1101, 3389, 293, 721, 411, 341, 13, 1779, 13, 583, 3335, 5134, 307, 257, 1577, 18218, 37906, 302, 295, 15329, 13, 51584], "temperature": 0.0, "avg_logprob": -0.1061345697418461, "compression_ratio": 1.698581560283688, "no_speech_prob": 0.01406020950525999}, {"id": 422, "seek": 213216, "start": 2132.96, "end": 2138.3999999999996, "text": " Right. And so that means it has to work without types. It has to support all the dynamic things,", "tokens": [50404, 1779, 13, 400, 370, 300, 1355, 309, 575, 281, 589, 1553, 3467, 13, 467, 575, 281, 1406, 439, 264, 8546, 721, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1325830801939353, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.021611051633954048}, {"id": 423, "seek": 213216, "start": 2138.3999999999996, "end": 2143.2, "text": " has to support all the packages, has to support for comprehension, list comprehensions and things", "tokens": [50676, 575, 281, 1406, 439, 264, 17401, 11, 575, 281, 1406, 337, 44991, 11, 1329, 10753, 8302, 293, 721, 50916], "temperature": 0.0, "avg_logprob": -0.1325830801939353, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.021611051633954048}, {"id": 424, "seek": 213216, "start": 2143.2, "end": 2147.68, "text": " like this. Right. And so that starting point, I think, is really important. And I think that,", "tokens": [50916, 411, 341, 13, 1779, 13, 400, 370, 300, 2891, 935, 11, 286, 519, 11, 307, 534, 1021, 13, 400, 286, 519, 300, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1325830801939353, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.021611051633954048}, {"id": 425, "seek": 213216, "start": 2148.7999999999997, "end": 2152.56, "text": " again, you can look at why it cares so much about this. And there's many different aspects of that,", "tokens": [51196, 797, 11, 291, 393, 574, 412, 983, 309, 12310, 370, 709, 466, 341, 13, 400, 456, 311, 867, 819, 7270, 295, 300, 11, 51384], "temperature": 0.0, "avg_logprob": -0.1325830801939353, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.021611051633954048}, {"id": 426, "seek": 213216, "start": 2152.56, "end": 2157.52, "text": " one of which is the world went through a very challenging migration from Python 2 to Python 3.", "tokens": [51384, 472, 295, 597, 307, 264, 1002, 1437, 807, 257, 588, 7595, 17011, 490, 15329, 568, 281, 15329, 805, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1325830801939353, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.021611051633954048}, {"id": 427, "seek": 215752, "start": 2158.4, "end": 2163.92, "text": " Right. And this migration took many years. And it was very painful for many teams. Right.", "tokens": [50408, 1779, 13, 400, 341, 17011, 1890, 867, 924, 13, 400, 309, 390, 588, 11697, 337, 867, 5491, 13, 1779, 13, 50684], "temperature": 0.0, "avg_logprob": -0.13563994507291424, "compression_ratio": 1.768060836501901, "no_speech_prob": 0.005218992009758949}, {"id": 428, "seek": 215752, "start": 2163.92, "end": 2169.6, "text": " And there's a lot of things that went on in that. I'm not an expert in all the details. I honestly", "tokens": [50684, 400, 456, 311, 257, 688, 295, 721, 300, 1437, 322, 294, 300, 13, 286, 478, 406, 364, 5844, 294, 439, 264, 4365, 13, 286, 6095, 50968], "temperature": 0.0, "avg_logprob": -0.13563994507291424, "compression_ratio": 1.768060836501901, "no_speech_prob": 0.005218992009758949}, {"id": 429, "seek": 215752, "start": 2169.6, "end": 2174.48, "text": " don't want to be. I don't want the world to have to go through that. Right. And people can ignore", "tokens": [50968, 500, 380, 528, 281, 312, 13, 286, 500, 380, 528, 264, 1002, 281, 362, 281, 352, 807, 300, 13, 1779, 13, 400, 561, 393, 11200, 51212], "temperature": 0.0, "avg_logprob": -0.13563994507291424, "compression_ratio": 1.768060836501901, "no_speech_prob": 0.005218992009758949}, {"id": 430, "seek": 215752, "start": 2174.48, "end": 2178.4, "text": " Mojo. And if it's not their thing, that's cool. But if they want to use Mojo, I don't want them", "tokens": [51212, 3335, 5134, 13, 400, 498, 309, 311, 406, 641, 551, 11, 300, 311, 1627, 13, 583, 498, 436, 528, 281, 764, 3335, 5134, 11, 286, 500, 380, 528, 552, 51408], "temperature": 0.0, "avg_logprob": -0.13563994507291424, "compression_ratio": 1.768060836501901, "no_speech_prob": 0.005218992009758949}, {"id": 431, "seek": 215752, "start": 2178.4, "end": 2182.16, "text": " to have to rewrite all their code. Yeah. I mean, this, okay, the superset part is,", "tokens": [51408, 281, 362, 281, 28132, 439, 641, 3089, 13, 865, 13, 286, 914, 11, 341, 11, 1392, 11, 264, 37906, 302, 644, 307, 11, 51596], "temperature": 0.0, "avg_logprob": -0.13563994507291424, "compression_ratio": 1.768060836501901, "no_speech_prob": 0.005218992009758949}, {"id": 432, "seek": 218216, "start": 2183.12, "end": 2187.6, "text": " it's just, I mean, there's so much brilliant stuff here. That definitely is incredible.", "tokens": [50412, 309, 311, 445, 11, 286, 914, 11, 456, 311, 370, 709, 10248, 1507, 510, 13, 663, 2138, 307, 4651, 13, 50636], "temperature": 0.0, "avg_logprob": -0.16833009071720456, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.005553719121962786}, {"id": 433, "seek": 218216, "start": 2188.8799999999997, "end": 2192.24, "text": " We'll talk about that. Yeah. First of all, how's the typing implemented differently", "tokens": [50700, 492, 603, 751, 466, 300, 13, 865, 13, 2386, 295, 439, 11, 577, 311, 264, 18444, 12270, 7614, 50868], "temperature": 0.0, "avg_logprob": -0.16833009071720456, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.005553719121962786}, {"id": 434, "seek": 218216, "start": 2192.96, "end": 2199.44, "text": " in Python versus Mojo? Yeah. So this heterogeneous flexibility,", "tokens": [50904, 294, 15329, 5717, 3335, 5134, 30, 865, 13, 407, 341, 20789, 31112, 12635, 11, 51228], "temperature": 0.0, "avg_logprob": -0.16833009071720456, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.005553719121962786}, {"id": 435, "seek": 218216, "start": 2199.44, "end": 2203.7599999999998, "text": " you said, is definitely implemented. Yeah. So I'm not a full expert in the whole", "tokens": [51228, 291, 848, 11, 307, 2138, 12270, 13, 865, 13, 407, 286, 478, 406, 257, 1577, 5844, 294, 264, 1379, 51444], "temperature": 0.0, "avg_logprob": -0.16833009071720456, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.005553719121962786}, {"id": 436, "seek": 218216, "start": 2203.7599999999998, "end": 2207.6, "text": " backstory on types in Python. So I'll give you that. I can give you my understanding.", "tokens": [51444, 36899, 322, 3467, 294, 15329, 13, 407, 286, 603, 976, 291, 300, 13, 286, 393, 976, 291, 452, 3701, 13, 51636], "temperature": 0.0, "avg_logprob": -0.16833009071720456, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.005553719121962786}, {"id": 437, "seek": 220760, "start": 2207.7599999999998, "end": 2214.4, "text": " My understanding is basically like many dynamic languages, the ecosystem went through a phase", "tokens": [50372, 1222, 3701, 307, 1936, 411, 867, 8546, 8650, 11, 264, 11311, 1437, 807, 257, 5574, 50704], "temperature": 0.0, "avg_logprob": -0.2108820632652, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.0026311357505619526}, {"id": 438, "seek": 220760, "start": 2214.4, "end": 2221.2, "text": " where people went from ranked scripts to ranked large scale, huge code bases in Python. And at", "tokens": [50704, 689, 561, 1437, 490, 20197, 23294, 281, 20197, 2416, 4373, 11, 2603, 3089, 17949, 294, 15329, 13, 400, 412, 51044], "temperature": 0.0, "avg_logprob": -0.2108820632652, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.0026311357505619526}, {"id": 439, "seek": 220760, "start": 2221.2, "end": 2225.6, "text": " scale, it kind of helps have types. Yeah. People want to be able to reason about interfaces,", "tokens": [51044, 4373, 11, 309, 733, 295, 3665, 362, 3467, 13, 865, 13, 3432, 528, 281, 312, 1075, 281, 1778, 466, 28416, 11, 51264], "temperature": 0.0, "avg_logprob": -0.2108820632652, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.0026311357505619526}, {"id": 440, "seek": 220760, "start": 2225.6, "end": 2230.08, "text": " what do you expect to string or an int or like, what are these basic things, right?", "tokens": [51264, 437, 360, 291, 2066, 281, 6798, 420, 364, 560, 420, 411, 11, 437, 366, 613, 3875, 721, 11, 558, 30, 51488], "temperature": 0.0, "avg_logprob": -0.2108820632652, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.0026311357505619526}, {"id": 441, "seek": 220760, "start": 2230.08, "end": 2234.72, "text": " And so what the Python community started doing is it started saying, okay, let's have tools on the", "tokens": [51488, 400, 370, 437, 264, 15329, 1768, 1409, 884, 307, 309, 1409, 1566, 11, 1392, 11, 718, 311, 362, 3873, 322, 264, 51720], "temperature": 0.0, "avg_logprob": -0.2108820632652, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.0026311357505619526}, {"id": 442, "seek": 223472, "start": 2234.72, "end": 2241.2799999999997, "text": " side, checker tools, right? The go and like enforce and variance, check for bugs, try to", "tokens": [50364, 1252, 11, 1520, 260, 3873, 11, 558, 30, 440, 352, 293, 411, 24825, 293, 21977, 11, 1520, 337, 15120, 11, 853, 281, 50692], "temperature": 0.0, "avg_logprob": -0.10104677570399953, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.004467914812266827}, {"id": 443, "seek": 223472, "start": 2241.2799999999997, "end": 2245.8399999999997, "text": " identify things. These are called static analysis tools generally. And so these tools run over", "tokens": [50692, 5876, 721, 13, 1981, 366, 1219, 13437, 5215, 3873, 5101, 13, 400, 370, 613, 3873, 1190, 670, 50920], "temperature": 0.0, "avg_logprob": -0.10104677570399953, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.004467914812266827}, {"id": 444, "seek": 223472, "start": 2245.8399999999997, "end": 2249.68, "text": " your code and try to look for bugs. What ended up happening is there's so many of these things,", "tokens": [50920, 428, 3089, 293, 853, 281, 574, 337, 15120, 13, 708, 4590, 493, 2737, 307, 456, 311, 370, 867, 295, 613, 721, 11, 51112], "temperature": 0.0, "avg_logprob": -0.10104677570399953, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.004467914812266827}, {"id": 445, "seek": 223472, "start": 2249.68, "end": 2253.52, "text": " so many different weird patterns and different approaches on specifying the types and different", "tokens": [51112, 370, 867, 819, 3657, 8294, 293, 819, 11587, 322, 1608, 5489, 264, 3467, 293, 819, 51304], "temperature": 0.0, "avg_logprob": -0.10104677570399953, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.004467914812266827}, {"id": 446, "seek": 223472, "start": 2253.52, "end": 2257.8399999999997, "text": " things going on that the Python community realized and recognized, hey, hey, hey, there's a thing here.", "tokens": [51304, 721, 516, 322, 300, 264, 15329, 1768, 5334, 293, 9823, 11, 4177, 11, 4177, 11, 4177, 11, 456, 311, 257, 551, 510, 13, 51520], "temperature": 0.0, "avg_logprob": -0.10104677570399953, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.004467914812266827}, {"id": 447, "seek": 223472, "start": 2258.72, "end": 2262.72, "text": " And so what they started to do is they started to standardize the syntax for adding types to Python.", "tokens": [51564, 400, 370, 437, 436, 1409, 281, 360, 307, 436, 1409, 281, 3832, 1125, 264, 28431, 337, 5127, 3467, 281, 15329, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10104677570399953, "compression_ratio": 1.8709677419354838, "no_speech_prob": 0.004467914812266827}, {"id": 448, "seek": 226272, "start": 2263.3599999999997, "end": 2266.9599999999996, "text": " Now, one of the challenges that they had is that they're coming from kind of this fragmented", "tokens": [50396, 823, 11, 472, 295, 264, 4759, 300, 436, 632, 307, 300, 436, 434, 1348, 490, 733, 295, 341, 9241, 14684, 50576], "temperature": 0.0, "avg_logprob": -0.08960819244384766, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.00154857465531677}, {"id": 449, "seek": 226272, "start": 2266.9599999999996, "end": 2270.64, "text": " world where there's lots of different tools, they have different trade-offs and interpretations", "tokens": [50576, 1002, 689, 456, 311, 3195, 295, 819, 3873, 11, 436, 362, 819, 4923, 12, 19231, 293, 37547, 50760], "temperature": 0.0, "avg_logprob": -0.08960819244384766, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.00154857465531677}, {"id": 450, "seek": 226272, "start": 2270.64, "end": 2274.8799999999997, "text": " and the types mean different things. And so if you look at types in Python, according to the", "tokens": [50760, 293, 264, 3467, 914, 819, 721, 13, 400, 370, 498, 291, 574, 412, 3467, 294, 15329, 11, 4650, 281, 264, 50972], "temperature": 0.0, "avg_logprob": -0.08960819244384766, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.00154857465531677}, {"id": 451, "seek": 226272, "start": 2274.8799999999997, "end": 2281.3599999999997, "text": " Python spec, the types are ignored, right? So according to the Python spec, you can write", "tokens": [50972, 15329, 1608, 11, 264, 3467, 366, 19735, 11, 558, 30, 407, 4650, 281, 264, 15329, 1608, 11, 291, 393, 2464, 51296], "temperature": 0.0, "avg_logprob": -0.08960819244384766, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.00154857465531677}, {"id": 452, "seek": 226272, "start": 2281.3599999999997, "end": 2289.2799999999997, "text": " pretty much anything in a type position, okay? And technically, you can write any expression,", "tokens": [51296, 1238, 709, 1340, 294, 257, 2010, 2535, 11, 1392, 30, 400, 12120, 11, 291, 393, 2464, 604, 6114, 11, 51692], "temperature": 0.0, "avg_logprob": -0.08960819244384766, "compression_ratio": 1.8379446640316206, "no_speech_prob": 0.00154857465531677}, {"id": 453, "seek": 228928, "start": 2289.28, "end": 2294.48, "text": " okay? Now, that's beautiful because you can extend it, you can do cool things, you can", "tokens": [50364, 1392, 30, 823, 11, 300, 311, 2238, 570, 291, 393, 10101, 309, 11, 291, 393, 360, 1627, 721, 11, 291, 393, 50624], "temperature": 0.0, "avg_logprob": -0.0790929207435021, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0011334195733070374}, {"id": 454, "seek": 228928, "start": 2294.48, "end": 2298.7200000000003, "text": " write, build your own tools, you can build your own house, linter or something like that, right?", "tokens": [50624, 2464, 11, 1322, 428, 1065, 3873, 11, 291, 393, 1322, 428, 1065, 1782, 11, 287, 5106, 420, 746, 411, 300, 11, 558, 30, 50836], "temperature": 0.0, "avg_logprob": -0.0790929207435021, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0011334195733070374}, {"id": 455, "seek": 228928, "start": 2298.7200000000003, "end": 2304.1600000000003, "text": " But it's also a problem because any existing Python program may be using different tools", "tokens": [50836, 583, 309, 311, 611, 257, 1154, 570, 604, 6741, 15329, 1461, 815, 312, 1228, 819, 3873, 51108], "temperature": 0.0, "avg_logprob": -0.0790929207435021, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0011334195733070374}, {"id": 456, "seek": 228928, "start": 2304.1600000000003, "end": 2308.7200000000003, "text": " and they have different interpretations. And so if you adopt somebody's package into your ecosystem,", "tokens": [51108, 293, 436, 362, 819, 37547, 13, 400, 370, 498, 291, 6878, 2618, 311, 7372, 666, 428, 11311, 11, 51336], "temperature": 0.0, "avg_logprob": -0.0790929207435021, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0011334195733070374}, {"id": 457, "seek": 228928, "start": 2308.7200000000003, "end": 2312.88, "text": " try around the tool you prefer, it may throw out tons of weird errors and warnings and problems", "tokens": [51336, 853, 926, 264, 2290, 291, 4382, 11, 309, 815, 3507, 484, 9131, 295, 3657, 13603, 293, 30009, 293, 2740, 51544], "temperature": 0.0, "avg_logprob": -0.0790929207435021, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0011334195733070374}, {"id": 458, "seek": 228928, "start": 2312.88, "end": 2317.52, "text": " just because it's incompatible with how these things work. Also because they're added late", "tokens": [51544, 445, 570, 309, 311, 40393, 267, 964, 365, 577, 613, 721, 589, 13, 2743, 570, 436, 434, 3869, 3469, 51776], "temperature": 0.0, "avg_logprob": -0.0790929207435021, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0011334195733070374}, {"id": 459, "seek": 231752, "start": 2317.52, "end": 2321.12, "text": " and they're not checked by the Python interpreter, it's always kind of more of a hint than it is a", "tokens": [50364, 293, 436, 434, 406, 10033, 538, 264, 15329, 34132, 11, 309, 311, 1009, 733, 295, 544, 295, 257, 12075, 813, 309, 307, 257, 50544], "temperature": 0.0, "avg_logprob": -0.17799302222023547, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.023675929754972458}, {"id": 460, "seek": 231752, "start": 2321.12, "end": 2327.28, "text": " requirement. Also, the C Python implementation can't use them for performance. And so it's really", "tokens": [50544, 11695, 13, 2743, 11, 264, 383, 15329, 11420, 393, 380, 764, 552, 337, 3389, 13, 400, 370, 309, 311, 534, 50852], "temperature": 0.0, "avg_logprob": -0.17799302222023547, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.023675929754972458}, {"id": 461, "seek": 231752, "start": 2327.28, "end": 2331.36, "text": " that's a big one, right? So you can't utilize the for the compilation for the just entire", "tokens": [50852, 300, 311, 257, 955, 472, 11, 558, 30, 407, 291, 393, 380, 16117, 264, 337, 264, 40261, 337, 264, 445, 2302, 51056], "temperature": 0.0, "avg_logprob": -0.17799302222023547, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.023675929754972458}, {"id": 462, "seek": 231752, "start": 2331.36, "end": 2335.2, "text": " compilation. Okay, exactly. And this all comes back to the design principle of it's,", "tokens": [51056, 40261, 13, 1033, 11, 2293, 13, 400, 341, 439, 1487, 646, 281, 264, 1715, 8665, 295, 309, 311, 11, 51248], "temperature": 0.0, "avg_logprob": -0.17799302222023547, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.023675929754972458}, {"id": 463, "seek": 231752, "start": 2335.7599999999998, "end": 2339.2, "text": " it's kind of, they're kind of hints, they're kind of the definition is a little bit murky,", "tokens": [51276, 309, 311, 733, 295, 11, 436, 434, 733, 295, 27271, 11, 436, 434, 733, 295, 264, 7123, 307, 257, 707, 857, 5257, 4133, 11, 51448], "temperature": 0.0, "avg_logprob": -0.17799302222023547, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.023675929754972458}, {"id": 464, "seek": 231752, "start": 2339.2, "end": 2343.28, "text": " it's unclear exactly the interpretation in a bunch of cases. And so because of that, you can't", "tokens": [51448, 309, 311, 25636, 2293, 264, 14174, 294, 257, 3840, 295, 3331, 13, 400, 370, 570, 295, 300, 11, 291, 393, 380, 51652], "temperature": 0.0, "avg_logprob": -0.17799302222023547, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.023675929754972458}, {"id": 465, "seek": 234328, "start": 2343.28, "end": 2348.2400000000002, "text": " actually, even if you want to, it's really difficult to use them to say like it is going", "tokens": [50364, 767, 11, 754, 498, 291, 528, 281, 11, 309, 311, 534, 2252, 281, 764, 552, 281, 584, 411, 309, 307, 516, 50612], "temperature": 0.0, "avg_logprob": -0.10460292509872549, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.010982711799442768}, {"id": 466, "seek": 234328, "start": 2348.2400000000002, "end": 2352.2400000000002, "text": " to be an int. And if it's not, it's a problem, right? A lot of code would break if you did that.", "tokens": [50612, 281, 312, 364, 560, 13, 400, 498, 309, 311, 406, 11, 309, 311, 257, 1154, 11, 558, 30, 316, 688, 295, 3089, 576, 1821, 498, 291, 630, 300, 13, 50812], "temperature": 0.0, "avg_logprob": -0.10460292509872549, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.010982711799442768}, {"id": 467, "seek": 234328, "start": 2352.96, "end": 2357.0400000000004, "text": " So, so in mojo, right, so you can still use those kind of type annotations, it's fine.", "tokens": [50848, 407, 11, 370, 294, 705, 5134, 11, 558, 11, 370, 291, 393, 920, 764, 729, 733, 295, 2010, 25339, 763, 11, 309, 311, 2489, 13, 51052], "temperature": 0.0, "avg_logprob": -0.10460292509872549, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.010982711799442768}, {"id": 468, "seek": 234328, "start": 2357.0400000000004, "end": 2362.96, "text": " But in mojo, if you declare a type and you use it, then it means it is going to be that type.", "tokens": [51052, 583, 294, 705, 5134, 11, 498, 291, 19710, 257, 2010, 293, 291, 764, 309, 11, 550, 309, 1355, 309, 307, 516, 281, 312, 300, 2010, 13, 51348], "temperature": 0.0, "avg_logprob": -0.10460292509872549, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.010982711799442768}, {"id": 469, "seek": 234328, "start": 2362.96, "end": 2368.0800000000004, "text": " And the compiler helps you check that and force it and it's safe. And it's not it's not a like", "tokens": [51348, 400, 264, 31958, 3665, 291, 1520, 300, 293, 3464, 309, 293, 309, 311, 3273, 13, 400, 309, 311, 406, 309, 311, 406, 257, 411, 51604], "temperature": 0.0, "avg_logprob": -0.10460292509872549, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.010982711799442768}, {"id": 470, "seek": 236808, "start": 2369.04, "end": 2374.24, "text": " best effort hint kind of a thing. So if you try to shove a string type thing into a integer,", "tokens": [50412, 1151, 4630, 12075, 733, 295, 257, 551, 13, 407, 498, 291, 853, 281, 35648, 257, 6798, 2010, 551, 666, 257, 24922, 11, 50672], "temperature": 0.0, "avg_logprob": -0.15696468549905365, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.020017890259623528}, {"id": 471, "seek": 236808, "start": 2374.24, "end": 2381.84, "text": " you get an error from the compiler, compile time. Nice. Okay, what kind of basic types are there?", "tokens": [50672, 291, 483, 364, 6713, 490, 264, 31958, 11, 31413, 565, 13, 5490, 13, 1033, 11, 437, 733, 295, 3875, 3467, 366, 456, 30, 51052], "temperature": 0.0, "avg_logprob": -0.15696468549905365, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.020017890259623528}, {"id": 472, "seek": 236808, "start": 2381.84, "end": 2389.2, "text": " Yeah, so mojo is pretty hardcore in terms of what it tries to do in the language,", "tokens": [51052, 865, 11, 370, 705, 5134, 307, 1238, 28196, 294, 2115, 295, 437, 309, 9898, 281, 360, 294, 264, 2856, 11, 51420], "temperature": 0.0, "avg_logprob": -0.15696468549905365, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.020017890259623528}, {"id": 473, "seek": 236808, "start": 2389.7599999999998, "end": 2396.3199999999997, "text": " which is the philosophy there is that we, again, if you, if you look at Python, right,", "tokens": [51448, 597, 307, 264, 10675, 456, 307, 300, 321, 11, 797, 11, 498, 291, 11, 498, 291, 574, 412, 15329, 11, 558, 11, 51776], "temperature": 0.0, "avg_logprob": -0.15696468549905365, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.020017890259623528}, {"id": 474, "seek": 239632, "start": 2396.32, "end": 2400.88, "text": " Python's beautiful language because it's so extensible, right? And so all of the different", "tokens": [50364, 15329, 311, 2238, 2856, 570, 309, 311, 370, 1279, 30633, 11, 558, 30, 400, 370, 439, 295, 264, 819, 50592], "temperature": 0.0, "avg_logprob": -0.13457608032226562, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.003074615728110075}, {"id": 475, "seek": 239632, "start": 2400.88, "end": 2404.48, "text": " things in Python like for loops and plus and like all these things can be", "tokens": [50592, 721, 294, 15329, 411, 337, 16121, 293, 1804, 293, 411, 439, 613, 721, 393, 312, 50772], "temperature": 0.0, "avg_logprob": -0.13457608032226562, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.003074615728110075}, {"id": 476, "seek": 239632, "start": 2405.04, "end": 2410.2400000000002, "text": " accessed through these underbar and bar methods. Okay, so you have to say, okay,", "tokens": [50800, 34211, 807, 613, 833, 5356, 293, 2159, 7150, 13, 1033, 11, 370, 291, 362, 281, 584, 11, 1392, 11, 51060], "temperature": 0.0, "avg_logprob": -0.13457608032226562, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.003074615728110075}, {"id": 477, "seek": 239632, "start": 2410.2400000000002, "end": 2412.96, "text": " if I make something that is super fast, I can go all the way down to the metal.", "tokens": [51060, 498, 286, 652, 746, 300, 307, 1687, 2370, 11, 286, 393, 352, 439, 264, 636, 760, 281, 264, 5760, 13, 51196], "temperature": 0.0, "avg_logprob": -0.13457608032226562, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.003074615728110075}, {"id": 478, "seek": 239632, "start": 2413.6000000000004, "end": 2415.76, "text": " Why do I need to have integers built into the language?", "tokens": [51228, 1545, 360, 286, 643, 281, 362, 41674, 3094, 666, 264, 2856, 30, 51336], "temperature": 0.0, "avg_logprob": -0.13457608032226562, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.003074615728110075}, {"id": 479, "seek": 239632, "start": 2416.96, "end": 2420.7200000000003, "text": " Right. And so what mojo does is it says, okay, well, we can have this notion of structs. So", "tokens": [51396, 1779, 13, 400, 370, 437, 705, 5134, 775, 307, 309, 1619, 11, 1392, 11, 731, 11, 321, 393, 362, 341, 10710, 295, 6594, 82, 13, 407, 51584], "temperature": 0.0, "avg_logprob": -0.13457608032226562, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.003074615728110075}, {"id": 480, "seek": 242072, "start": 2420.7999999999997, "end": 2426.16, "text": " we have classes in Python, now you can have structs. Classes are dynamic, structs are static.", "tokens": [50368, 321, 362, 5359, 294, 15329, 11, 586, 291, 393, 362, 6594, 82, 13, 9471, 279, 366, 8546, 11, 6594, 82, 366, 13437, 13, 50636], "temperature": 0.0, "avg_logprob": -0.12688488879446255, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.003706715302541852}, {"id": 481, "seek": 242072, "start": 2427.04, "end": 2431.4399999999996, "text": " Cool, we can get high performance, we can write C++ kind of code with structs if you want.", "tokens": [50680, 8561, 11, 321, 393, 483, 1090, 3389, 11, 321, 393, 2464, 383, 25472, 733, 295, 3089, 365, 6594, 82, 498, 291, 528, 13, 50900], "temperature": 0.0, "avg_logprob": -0.12688488879446255, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.003706715302541852}, {"id": 482, "seek": 242072, "start": 2431.4399999999996, "end": 2435.68, "text": " These things mix and work beautifully together. But what that means is that you can go and", "tokens": [50900, 1981, 721, 2890, 293, 589, 16525, 1214, 13, 583, 437, 300, 1355, 307, 300, 291, 393, 352, 293, 51112], "temperature": 0.0, "avg_logprob": -0.12688488879446255, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.003706715302541852}, {"id": 483, "seek": 242072, "start": 2435.68, "end": 2440.56, "text": " implement strings and ints and floats and arrays and all that kind of stuff in the language.", "tokens": [51112, 4445, 13985, 293, 560, 82, 293, 37878, 293, 41011, 293, 439, 300, 733, 295, 1507, 294, 264, 2856, 13, 51356], "temperature": 0.0, "avg_logprob": -0.12688488879446255, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.003706715302541852}, {"id": 484, "seek": 242072, "start": 2441.3599999999997, "end": 2449.2799999999997, "text": " Right. And so that's really cool because, you know, to me as a ideal, idealizing compiler language", "tokens": [51396, 1779, 13, 400, 370, 300, 311, 534, 1627, 570, 11, 291, 458, 11, 281, 385, 382, 257, 7157, 11, 7157, 3319, 31958, 2856, 51792], "temperature": 0.0, "avg_logprob": -0.12688488879446255, "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.003706715302541852}, {"id": 485, "seek": 244928, "start": 2449.28, "end": 2453.36, "text": " type of person, what I want to do is I want to get magic out of the compiler and put in the", "tokens": [50364, 2010, 295, 954, 11, 437, 286, 528, 281, 360, 307, 286, 528, 281, 483, 5585, 484, 295, 264, 31958, 293, 829, 294, 264, 50568], "temperature": 0.0, "avg_logprob": -0.1213635929295274, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.007119989953935146}, {"id": 486, "seek": 244928, "start": 2453.36, "end": 2457.76, "text": " libraries. Because if somebody can, you know, if we can build an integer that's beautiful and it", "tokens": [50568, 15148, 13, 1436, 498, 2618, 393, 11, 291, 458, 11, 498, 321, 393, 1322, 364, 24922, 300, 311, 2238, 293, 309, 50788], "temperature": 0.0, "avg_logprob": -0.1213635929295274, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.007119989953935146}, {"id": 487, "seek": 244928, "start": 2457.76, "end": 2462.4, "text": " has an amazing API and does all the things you'd expect an integer to do, but you don't like it,", "tokens": [50788, 575, 364, 2243, 9362, 293, 775, 439, 264, 721, 291, 1116, 2066, 364, 24922, 281, 360, 11, 457, 291, 500, 380, 411, 309, 11, 51020], "temperature": 0.0, "avg_logprob": -0.1213635929295274, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.007119989953935146}, {"id": 488, "seek": 244928, "start": 2463.0400000000004, "end": 2466.88, "text": " maybe you want a big integer, maybe you want like sideways integer, I don't know, like what", "tokens": [51052, 1310, 291, 528, 257, 955, 24922, 11, 1310, 291, 528, 411, 26092, 24922, 11, 286, 500, 380, 458, 11, 411, 437, 51244], "temperature": 0.0, "avg_logprob": -0.1213635929295274, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.007119989953935146}, {"id": 489, "seek": 244928, "start": 2466.88, "end": 2473.28, "text": " all the space of integers are, then you can do that and it's not a second class citizen.", "tokens": [51244, 439, 264, 1901, 295, 41674, 366, 11, 550, 291, 393, 360, 300, 293, 309, 311, 406, 257, 1150, 1508, 13326, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1213635929295274, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.007119989953935146}, {"id": 490, "seek": 247328, "start": 2473.52, "end": 2479.6800000000003, "text": " And so if you look at certain other languages like C++, one I also love and use a lot,", "tokens": [50376, 400, 370, 498, 291, 574, 412, 1629, 661, 8650, 411, 383, 25472, 11, 472, 286, 611, 959, 293, 764, 257, 688, 11, 50684], "temperature": 0.0, "avg_logprob": -0.1448999706067537, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0069029550068080425}, {"id": 491, "seek": 247328, "start": 2481.0400000000004, "end": 2488.6400000000003, "text": " int is hard code in the language, but complex is not. And so it's kind of weird that, you know,", "tokens": [50752, 560, 307, 1152, 3089, 294, 264, 2856, 11, 457, 3997, 307, 406, 13, 400, 370, 309, 311, 733, 295, 3657, 300, 11, 291, 458, 11, 51132], "temperature": 0.0, "avg_logprob": -0.1448999706067537, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0069029550068080425}, {"id": 492, "seek": 247328, "start": 2488.6400000000003, "end": 2494.96, "text": " you have this std complex class, but you have int and complex tries to look like a natural", "tokens": [51132, 291, 362, 341, 342, 67, 3997, 1508, 11, 457, 291, 362, 560, 293, 3997, 9898, 281, 574, 411, 257, 3303, 51448], "temperature": 0.0, "avg_logprob": -0.1448999706067537, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0069029550068080425}, {"id": 493, "seek": 247328, "start": 2494.96, "end": 2500.2400000000002, "text": " numeric type and things like this. But integers and floating point have these like special promotion", "tokens": [51448, 7866, 299, 2010, 293, 721, 411, 341, 13, 583, 41674, 293, 12607, 935, 362, 613, 411, 2121, 15783, 51712], "temperature": 0.0, "avg_logprob": -0.1448999706067537, "compression_ratio": 1.6771300448430493, "no_speech_prob": 0.0069029550068080425}, {"id": 494, "seek": 250024, "start": 2500.24, "end": 2503.68, "text": " rules and other things like that that are magic and they're hacked into the compiler.", "tokens": [50364, 4474, 293, 661, 721, 411, 300, 300, 366, 5585, 293, 436, 434, 36218, 666, 264, 31958, 13, 50536], "temperature": 0.0, "avg_logprob": -0.11906452629509873, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.01098360400646925}, {"id": 495, "seek": 250024, "start": 2503.68, "end": 2506.56, "text": " And because of that, you can't actually make something that works like the built-in types.", "tokens": [50536, 400, 570, 295, 300, 11, 291, 393, 380, 767, 652, 746, 300, 1985, 411, 264, 3094, 12, 259, 3467, 13, 50680], "temperature": 0.0, "avg_logprob": -0.11906452629509873, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.01098360400646925}, {"id": 496, "seek": 250024, "start": 2507.52, "end": 2512.9599999999996, "text": " Is there something provided as a standard because, you know, because it's AI first,", "tokens": [50728, 1119, 456, 746, 5649, 382, 257, 3832, 570, 11, 291, 458, 11, 570, 309, 311, 7318, 700, 11, 51000], "temperature": 0.0, "avg_logprob": -0.11906452629509873, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.01098360400646925}, {"id": 497, "seek": 250024, "start": 2513.7599999999998, "end": 2518.56, "text": " you know, numerical types are so important here. So is there something like a nice", "tokens": [51040, 291, 458, 11, 29054, 3467, 366, 370, 1021, 510, 13, 407, 307, 456, 746, 411, 257, 1481, 51280], "temperature": 0.0, "avg_logprob": -0.11906452629509873, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.01098360400646925}, {"id": 498, "seek": 250024, "start": 2519.2, "end": 2523.2799999999997, "text": " standard implementation of integer and float? Yeah, so we're still building all this stuff out. So we", "tokens": [51312, 3832, 11420, 295, 24922, 293, 15706, 30, 865, 11, 370, 321, 434, 920, 2390, 439, 341, 1507, 484, 13, 407, 321, 51516], "temperature": 0.0, "avg_logprob": -0.11906452629509873, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.01098360400646925}, {"id": 499, "seek": 250024, "start": 2523.2799999999997, "end": 2527.2, "text": " provide integers and floats and all that kind of stuff. We also provide like buffers and tensors", "tokens": [51516, 2893, 41674, 293, 37878, 293, 439, 300, 733, 295, 1507, 13, 492, 611, 2893, 411, 9204, 433, 293, 10688, 830, 51712], "temperature": 0.0, "avg_logprob": -0.11906452629509873, "compression_ratio": 1.856164383561644, "no_speech_prob": 0.01098360400646925}, {"id": 500, "seek": 252720, "start": 2527.2, "end": 2532.56, "text": " and things like that that you'd expect in an ML context. Honestly, we need to keep designing and", "tokens": [50364, 293, 721, 411, 300, 300, 291, 1116, 2066, 294, 364, 21601, 4319, 13, 12348, 11, 321, 643, 281, 1066, 14685, 293, 50632], "temperature": 0.0, "avg_logprob": -0.0787820816040039, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.0018099949229508638}, {"id": 501, "seek": 252720, "start": 2532.56, "end": 2535.2799999999997, "text": " redesigning and working with the community to build that out and make that better. That's not", "tokens": [50632, 16762, 9676, 293, 1364, 365, 264, 1768, 281, 1322, 300, 484, 293, 652, 300, 1101, 13, 663, 311, 406, 50768], "temperature": 0.0, "avg_logprob": -0.0787820816040039, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.0018099949229508638}, {"id": 502, "seek": 252720, "start": 2535.2799999999997, "end": 2539.52, "text": " our strength right now. Give us six months or a year and I think it'll be way better. But", "tokens": [50768, 527, 3800, 558, 586, 13, 5303, 505, 2309, 2493, 420, 257, 1064, 293, 286, 519, 309, 603, 312, 636, 1101, 13, 583, 50980], "temperature": 0.0, "avg_logprob": -0.0787820816040039, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.0018099949229508638}, {"id": 503, "seek": 252720, "start": 2540.72, "end": 2544.7999999999997, "text": " the power of putting in the library means that we can have teams of experts that aren't compiler", "tokens": [51040, 264, 1347, 295, 3372, 294, 264, 6405, 1355, 300, 321, 393, 362, 5491, 295, 8572, 300, 3212, 380, 31958, 51244], "temperature": 0.0, "avg_logprob": -0.0787820816040039, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.0018099949229508638}, {"id": 504, "seek": 252720, "start": 2544.7999999999997, "end": 2550.16, "text": " engineers that can help us design and refine and drive us forward. So one of the exciting things", "tokens": [51244, 11955, 300, 393, 854, 505, 1715, 293, 33906, 293, 3332, 505, 2128, 13, 407, 472, 295, 264, 4670, 721, 51512], "temperature": 0.0, "avg_logprob": -0.0787820816040039, "compression_ratio": 1.686832740213523, "no_speech_prob": 0.0018099949229508638}, {"id": 505, "seek": 255016, "start": 2550.16, "end": 2558.64, "text": " we should mention here is that this is new and fresh. This cake is unbaked. It's almost baked.", "tokens": [50364, 321, 820, 2152, 510, 307, 300, 341, 307, 777, 293, 4451, 13, 639, 5908, 307, 517, 65, 7301, 13, 467, 311, 1920, 19453, 13, 50788], "temperature": 0.0, "avg_logprob": -0.08788151513962518, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.265748530626297}, {"id": 506, "seek": 255016, "start": 2558.64, "end": 2564.24, "text": " You can tell it's delicious, but it's not fully ready to be consumed. Yep, that's very fair. It is", "tokens": [50788, 509, 393, 980, 309, 311, 4809, 11, 457, 309, 311, 406, 4498, 1919, 281, 312, 21226, 13, 7010, 11, 300, 311, 588, 3143, 13, 467, 307, 51068], "temperature": 0.0, "avg_logprob": -0.08788151513962518, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.265748530626297}, {"id": 507, "seek": 255016, "start": 2564.24, "end": 2568.08, "text": " very useful, but it's very useful if you're a super low-level programmer right now. And what", "tokens": [51068, 588, 4420, 11, 457, 309, 311, 588, 4420, 498, 291, 434, 257, 1687, 2295, 12, 12418, 32116, 558, 586, 13, 400, 437, 51260], "temperature": 0.0, "avg_logprob": -0.08788151513962518, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.265748530626297}, {"id": 508, "seek": 255016, "start": 2568.08, "end": 2571.68, "text": " we're doing is we're working our way up the stack. And so the way I would look at Mojo", "tokens": [51260, 321, 434, 884, 307, 321, 434, 1364, 527, 636, 493, 264, 8630, 13, 400, 370, 264, 636, 286, 576, 574, 412, 3335, 5134, 51440], "temperature": 0.0, "avg_logprob": -0.08788151513962518, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.265748530626297}, {"id": 509, "seek": 257168, "start": 2572.16, "end": 2580.24, "text": " today in May and 2023 is that it's like a 0.1. So I think that a year from now,", "tokens": [50388, 965, 294, 1891, 293, 44377, 307, 300, 309, 311, 411, 257, 1958, 13, 16, 13, 407, 286, 519, 300, 257, 1064, 490, 586, 11, 50792], "temperature": 0.0, "avg_logprob": -0.11113958035485219, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.11912945657968521}, {"id": 510, "seek": 257168, "start": 2580.24, "end": 2585.3599999999997, "text": " it's going to be way more interesting to a variety of people. But what we're doing is we", "tokens": [50792, 309, 311, 516, 281, 312, 636, 544, 1880, 281, 257, 5673, 295, 561, 13, 583, 437, 321, 434, 884, 307, 321, 51048], "temperature": 0.0, "avg_logprob": -0.11113958035485219, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.11912945657968521}, {"id": 511, "seek": 257168, "start": 2585.3599999999997, "end": 2589.04, "text": " decided to release it early so that people can get access to it and play with it. We can build", "tokens": [51048, 3047, 281, 4374, 309, 2440, 370, 300, 561, 393, 483, 2105, 281, 309, 293, 862, 365, 309, 13, 492, 393, 1322, 51232], "temperature": 0.0, "avg_logprob": -0.11113958035485219, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.11912945657968521}, {"id": 512, "seek": 257168, "start": 2589.04, "end": 2595.52, "text": " it with the community. We have a big roadmap fully published, being transparent about this,", "tokens": [51232, 309, 365, 264, 1768, 13, 492, 362, 257, 955, 35738, 4498, 6572, 11, 885, 12737, 466, 341, 11, 51556], "temperature": 0.0, "avg_logprob": -0.11113958035485219, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.11912945657968521}, {"id": 513, "seek": 257168, "start": 2595.52, "end": 2599.04, "text": " and a lot of people are involved in this stuff. And so what we're doing is we're really optimizing", "tokens": [51556, 293, 257, 688, 295, 561, 366, 3288, 294, 341, 1507, 13, 400, 370, 437, 321, 434, 884, 307, 321, 434, 534, 40425, 51732], "temperature": 0.0, "avg_logprob": -0.11113958035485219, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.11912945657968521}, {"id": 514, "seek": 259904, "start": 2599.04, "end": 2603.92, "text": " for building this thing the right way. And building it the right way is kind of interesting", "tokens": [50364, 337, 2390, 341, 551, 264, 558, 636, 13, 400, 2390, 309, 264, 558, 636, 307, 733, 295, 1880, 50608], "temperature": 0.0, "avg_logprob": -0.14051619443026456, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.008306468836963177}, {"id": 515, "seek": 259904, "start": 2603.92, "end": 2610.88, "text": " working with the community because everybody wants it yesterday. And so sometimes there's", "tokens": [50608, 1364, 365, 264, 1768, 570, 2201, 2738, 309, 5186, 13, 400, 370, 2171, 456, 311, 50956], "temperature": 0.0, "avg_logprob": -0.14051619443026456, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.008306468836963177}, {"id": 516, "seek": 259904, "start": 2610.88, "end": 2615.44, "text": " some dynamics there, but I think it's the right thing. So there's a discord also,", "tokens": [50956, 512, 15679, 456, 11, 457, 286, 519, 309, 311, 264, 558, 551, 13, 407, 456, 311, 257, 32989, 611, 11, 51184], "temperature": 0.0, "avg_logprob": -0.14051619443026456, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.008306468836963177}, {"id": 517, "seek": 259904, "start": 2615.44, "end": 2620.08, "text": " so the dynamics is pretty interesting. Sometimes the community probably can be very chaotic", "tokens": [51184, 370, 264, 15679, 307, 1238, 1880, 13, 4803, 264, 1768, 1391, 393, 312, 588, 27013, 51416], "temperature": 0.0, "avg_logprob": -0.14051619443026456, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.008306468836963177}, {"id": 518, "seek": 259904, "start": 2621.6, "end": 2627.7599999999998, "text": " and introduce a lot of stress. Guido famously quit over the stress of the Walrus operator.", "tokens": [51492, 293, 5366, 257, 688, 295, 4244, 13, 2694, 2925, 34360, 10366, 670, 264, 4244, 295, 264, 9707, 13783, 12973, 13, 51800], "temperature": 0.0, "avg_logprob": -0.14051619443026456, "compression_ratio": 1.7911646586345382, "no_speech_prob": 0.008306468836963177}, {"id": 519, "seek": 262904, "start": 2629.04, "end": 2634.72, "text": " Broke, straw the brook with camels there. Exactly. And so it could be very stressful to", "tokens": [50364, 5425, 330, 11, 10099, 264, 2006, 453, 365, 1945, 1625, 456, 13, 7587, 13, 400, 370, 309, 727, 312, 588, 19108, 281, 50648], "temperature": 0.0, "avg_logprob": -0.21911510852498745, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.003762745065614581}, {"id": 520, "seek": 262904, "start": 2634.72, "end": 2642.8, "text": " develop. But can you just add tangent upon a tangent? Is it stressful to work through the", "tokens": [50648, 1499, 13, 583, 393, 291, 445, 909, 27747, 3564, 257, 27747, 30, 1119, 309, 19108, 281, 589, 807, 264, 51052], "temperature": 0.0, "avg_logprob": -0.21911510852498745, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.003762745065614581}, {"id": 521, "seek": 262904, "start": 2642.8, "end": 2647.12, "text": " design of various features here given that the community is so racially involved?", "tokens": [51052, 1715, 295, 3683, 4122, 510, 2212, 300, 264, 1768, 307, 370, 4129, 2270, 3288, 30, 51268], "temperature": 0.0, "avg_logprob": -0.21911510852498745, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.003762745065614581}, {"id": 522, "seek": 262904, "start": 2647.12, "end": 2652.72, "text": " Well, so I've been doing open development and community stuff for decades now. Somehow this", "tokens": [51268, 1042, 11, 370, 286, 600, 668, 884, 1269, 3250, 293, 1768, 1507, 337, 7878, 586, 13, 28357, 341, 51548], "temperature": 0.0, "avg_logprob": -0.21911510852498745, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.003762745065614581}, {"id": 523, "seek": 262904, "start": 2652.72, "end": 2657.68, "text": " has happened to me. So I've learned some tricks. But the thing that always gets me is I want to", "tokens": [51548, 575, 2011, 281, 385, 13, 407, 286, 600, 3264, 512, 11733, 13, 583, 264, 551, 300, 1009, 2170, 385, 307, 286, 528, 281, 51796], "temperature": 0.0, "avg_logprob": -0.21911510852498745, "compression_ratio": 1.5964285714285715, "no_speech_prob": 0.003762745065614581}, {"id": 524, "seek": 265768, "start": 2657.68, "end": 2664.16, "text": " make people happy. And so this is maybe not all people all happy all the time. But generally,", "tokens": [50364, 652, 561, 2055, 13, 400, 370, 341, 307, 1310, 406, 439, 561, 439, 2055, 439, 264, 565, 13, 583, 5101, 11, 50688], "temperature": 0.0, "avg_logprob": -0.14287534307260982, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.019704004749655724}, {"id": 525, "seek": 265768, "start": 2664.16, "end": 2669.7599999999998, "text": " I want people to be happy. And so the challenge is that, again, we're tapping into some long,", "tokens": [50688, 286, 528, 561, 281, 312, 2055, 13, 400, 370, 264, 3430, 307, 300, 11, 797, 11, 321, 434, 21444, 666, 512, 938, 11, 50968], "temperature": 0.0, "avg_logprob": -0.14287534307260982, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.019704004749655724}, {"id": 526, "seek": 265768, "start": 2670.7999999999997, "end": 2676.48, "text": " some deep-seated long tensions and pressures both in the Python world, but also in the AI world,", "tokens": [51020, 512, 2452, 12, 405, 770, 938, 28303, 293, 23573, 1293, 294, 264, 15329, 1002, 11, 457, 611, 294, 264, 7318, 1002, 11, 51304], "temperature": 0.0, "avg_logprob": -0.14287534307260982, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.019704004749655724}, {"id": 527, "seek": 265768, "start": 2676.48, "end": 2680.0, "text": " in the hardware world, and things like this. And so people just want us to move faster.", "tokens": [51304, 294, 264, 8837, 1002, 11, 293, 721, 411, 341, 13, 400, 370, 561, 445, 528, 505, 281, 1286, 4663, 13, 51480], "temperature": 0.0, "avg_logprob": -0.14287534307260982, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.019704004749655724}, {"id": 528, "seek": 265768, "start": 2680.96, "end": 2687.44, "text": " And so, again, our decision was let's release this early. Let's get people used to it or access to", "tokens": [51528, 400, 370, 11, 797, 11, 527, 3537, 390, 718, 311, 4374, 341, 2440, 13, 961, 311, 483, 561, 1143, 281, 309, 420, 2105, 281, 51852], "temperature": 0.0, "avg_logprob": -0.14287534307260982, "compression_ratio": 1.7773584905660378, "no_speech_prob": 0.019704004749655724}, {"id": 529, "seek": 268744, "start": 2687.44, "end": 2694.8, "text": " and play with it. And let's build in the open, which we could have had the language monk sitting", "tokens": [50364, 293, 862, 365, 309, 13, 400, 718, 311, 1322, 294, 264, 1269, 11, 597, 321, 727, 362, 632, 264, 2856, 27698, 3798, 50732], "temperature": 0.0, "avg_logprob": -0.13317880137213345, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.002548675984144211}, {"id": 530, "seek": 268744, "start": 2694.8, "end": 2700.4, "text": " in the cloister up on the hilltop bevaring away trying to build something. But in my experience,", "tokens": [50732, 294, 264, 20123, 1964, 493, 322, 264, 10997, 19337, 312, 85, 1921, 1314, 1382, 281, 1322, 746, 13, 583, 294, 452, 1752, 11, 51012], "temperature": 0.0, "avg_logprob": -0.13317880137213345, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.002548675984144211}, {"id": 531, "seek": 268744, "start": 2700.4, "end": 2702.64, "text": " you get something that's way better if you work with the community.", "tokens": [51012, 291, 483, 746, 300, 311, 636, 1101, 498, 291, 589, 365, 264, 1768, 13, 51124], "temperature": 0.0, "avg_logprob": -0.13317880137213345, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.002548675984144211}, {"id": 532, "seek": 268744, "start": 2704.16, "end": 2707.44, "text": " And so, yes, it can be frustrating. It can be challenging for lots of people involved. And", "tokens": [51200, 400, 370, 11, 2086, 11, 309, 393, 312, 16522, 13, 467, 393, 312, 7595, 337, 3195, 295, 561, 3288, 13, 400, 51364], "temperature": 0.0, "avg_logprob": -0.13317880137213345, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.002548675984144211}, {"id": 533, "seek": 268744, "start": 2708.8, "end": 2713.2000000000003, "text": " if you mention our Discord, we have over 10,000 people on the Discord, 11,000 people or something.", "tokens": [51432, 498, 291, 2152, 527, 32623, 11, 321, 362, 670, 1266, 11, 1360, 561, 322, 264, 32623, 11, 2975, 11, 1360, 561, 420, 746, 13, 51652], "temperature": 0.0, "avg_logprob": -0.13317880137213345, "compression_ratio": 1.7213740458015268, "no_speech_prob": 0.002548675984144211}, {"id": 534, "seek": 271320, "start": 2713.2, "end": 2721.12, "text": " Keep in mind, we released Mojo like two weeks ago. So it's very cool. But what that means is that", "tokens": [50364, 5527, 294, 1575, 11, 321, 4736, 3335, 5134, 411, 732, 3259, 2057, 13, 407, 309, 311, 588, 1627, 13, 583, 437, 300, 1355, 307, 300, 50760], "temperature": 0.0, "avg_logprob": -0.10104042146264053, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.00538251455873251}, {"id": 535, "seek": 271320, "start": 2722.64, "end": 2728.3199999999997, "text": " 10, 11,000 people all will want something different. And so what we've done is we've tried to say,", "tokens": [50836, 1266, 11, 2975, 11, 1360, 561, 439, 486, 528, 746, 819, 13, 400, 370, 437, 321, 600, 1096, 307, 321, 600, 3031, 281, 584, 11, 51120], "temperature": 0.0, "avg_logprob": -0.10104042146264053, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.00538251455873251}, {"id": 536, "seek": 271320, "start": 2728.3199999999997, "end": 2734.3199999999997, "text": " okay, cool, here's our roadmap. And the roadmap isn't completely arbitrary. It's based on,", "tokens": [51120, 1392, 11, 1627, 11, 510, 311, 527, 35738, 13, 400, 264, 35738, 1943, 380, 2584, 23211, 13, 467, 311, 2361, 322, 11, 51420], "temperature": 0.0, "avg_logprob": -0.10104042146264053, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.00538251455873251}, {"id": 537, "seek": 271320, "start": 2734.3199999999997, "end": 2738.16, "text": " here's the logical order in which to build these features or add these capabilities and", "tokens": [51420, 510, 311, 264, 14978, 1668, 294, 597, 281, 1322, 613, 4122, 420, 909, 613, 10862, 293, 51612], "temperature": 0.0, "avg_logprob": -0.10104042146264053, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.00538251455873251}, {"id": 538, "seek": 271320, "start": 2738.16, "end": 2742.96, "text": " things like that. And what we've done is we've spun really fast on bug fixes. And so we actually", "tokens": [51612, 721, 411, 300, 13, 400, 437, 321, 600, 1096, 307, 321, 600, 37038, 534, 2370, 322, 7426, 32539, 13, 400, 370, 321, 767, 51852], "temperature": 0.0, "avg_logprob": -0.10104042146264053, "compression_ratio": 1.6797153024911031, "no_speech_prob": 0.00538251455873251}, {"id": 539, "seek": 274296, "start": 2742.96, "end": 2748.4, "text": " have very few bugs, which is cool. I mean, actually for projects in the state. But then", "tokens": [50364, 362, 588, 1326, 15120, 11, 597, 307, 1627, 13, 286, 914, 11, 767, 337, 4455, 294, 264, 1785, 13, 583, 550, 50636], "temperature": 0.0, "avg_logprob": -0.1445156168714862, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.01638488657772541}, {"id": 540, "seek": 274296, "start": 2748.4, "end": 2751.12, "text": " what we're doing is we're dropping in features very deliberately.", "tokens": [50636, 437, 321, 434, 884, 307, 321, 434, 13601, 294, 4122, 588, 23506, 13, 50772], "temperature": 0.0, "avg_logprob": -0.1445156168714862, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.01638488657772541}, {"id": 541, "seek": 274296, "start": 2751.12, "end": 2756.08, "text": " I mean, this is fun to watch because you got the two gigantic communities of like hardware,", "tokens": [50772, 286, 914, 11, 341, 307, 1019, 281, 1159, 570, 291, 658, 264, 732, 26800, 4456, 295, 411, 8837, 11, 51020], "temperature": 0.0, "avg_logprob": -0.1445156168714862, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.01638488657772541}, {"id": 542, "seek": 274296, "start": 2756.08, "end": 2762.0, "text": " like systems engineers. And then you have the machine learning Python people that are like", "tokens": [51020, 411, 3652, 11955, 13, 400, 550, 291, 362, 264, 3479, 2539, 15329, 561, 300, 366, 411, 51316], "temperature": 0.0, "avg_logprob": -0.1445156168714862, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.01638488657772541}, {"id": 543, "seek": 274296, "start": 2762.0, "end": 2769.76, "text": " higher level. And it's just too like army, like they've been at war. Yeah, they've been at war.", "tokens": [51316, 2946, 1496, 13, 400, 309, 311, 445, 886, 411, 7267, 11, 411, 436, 600, 668, 412, 1516, 13, 865, 11, 436, 600, 668, 412, 1516, 13, 51704], "temperature": 0.0, "avg_logprob": -0.1445156168714862, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.01638488657772541}, {"id": 544, "seek": 276976, "start": 2770.48, "end": 2774.48, "text": " Right. And so here's a Tolkien novel or something. Okay, so here's the test. Again,", "tokens": [50400, 1779, 13, 400, 370, 510, 311, 257, 48824, 7613, 420, 746, 13, 1033, 11, 370, 510, 311, 264, 1500, 13, 3764, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1446724090576172, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.007344744633883238}, {"id": 545, "seek": 276976, "start": 2774.48, "end": 2778.7200000000003, "text": " like it's super funny for something that's only been out for two weeks, right? People are so", "tokens": [50600, 411, 309, 311, 1687, 4074, 337, 746, 300, 311, 787, 668, 484, 337, 732, 3259, 11, 558, 30, 3432, 366, 370, 50812], "temperature": 0.0, "avg_logprob": -0.1446724090576172, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.007344744633883238}, {"id": 546, "seek": 276976, "start": 2778.7200000000003, "end": 2784.6400000000003, "text": " impatient, right? But okay, cool. Let's fast forward a year. Like any year's time, Mojo will be", "tokens": [50812, 36895, 11, 558, 30, 583, 1392, 11, 1627, 13, 961, 311, 2370, 2128, 257, 1064, 13, 1743, 604, 1064, 311, 565, 11, 3335, 5134, 486, 312, 51108], "temperature": 0.0, "avg_logprob": -0.1446724090576172, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.007344744633883238}, {"id": 547, "seek": 276976, "start": 2784.6400000000003, "end": 2790.2400000000002, "text": " actually quite amazing and solve tons of problems and be very good. People still have these problems.", "tokens": [51108, 767, 1596, 2243, 293, 5039, 9131, 295, 2740, 293, 312, 588, 665, 13, 3432, 920, 362, 613, 2740, 13, 51388], "temperature": 0.0, "avg_logprob": -0.1446724090576172, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.007344744633883238}, {"id": 548, "seek": 276976, "start": 2791.0400000000004, "end": 2795.6800000000003, "text": " Right. And so you look at this and you say, and the way I look at this at least is to say,", "tokens": [51428, 1779, 13, 400, 370, 291, 574, 412, 341, 293, 291, 584, 11, 293, 264, 636, 286, 574, 412, 341, 412, 1935, 307, 281, 584, 11, 51660], "temperature": 0.0, "avg_logprob": -0.1446724090576172, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.007344744633883238}, {"id": 549, "seek": 279568, "start": 2795.68, "end": 2802.64, "text": " okay, well, we're solving big longstanding problems. To me, I, again, working on many", "tokens": [50364, 1392, 11, 731, 11, 321, 434, 12606, 955, 938, 8618, 2740, 13, 1407, 385, 11, 286, 11, 797, 11, 1364, 322, 867, 50712], "temperature": 0.0, "avg_logprob": -0.10107813043109441, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.002889077877625823}, {"id": 550, "seek": 279568, "start": 2802.64, "end": 2806.3999999999996, "text": " different problems, I want to make sure we do it right. Right? There's like a responsibility you", "tokens": [50712, 819, 2740, 11, 286, 528, 281, 652, 988, 321, 360, 309, 558, 13, 1779, 30, 821, 311, 411, 257, 6357, 291, 50900], "temperature": 0.0, "avg_logprob": -0.10107813043109441, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.002889077877625823}, {"id": 551, "seek": 279568, "start": 2806.3999999999996, "end": 2811.6, "text": " feel because if you mess it up, right? There's very few opportunities to do projects like this", "tokens": [50900, 841, 570, 498, 291, 2082, 309, 493, 11, 558, 30, 821, 311, 588, 1326, 4786, 281, 360, 4455, 411, 341, 51160], "temperature": 0.0, "avg_logprob": -0.10107813043109441, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.002889077877625823}, {"id": 552, "seek": 279568, "start": 2811.6, "end": 2816.0, "text": " and have them really have impact on the world. If we do it right, then maybe we can take those", "tokens": [51160, 293, 362, 552, 534, 362, 2712, 322, 264, 1002, 13, 759, 321, 360, 309, 558, 11, 550, 1310, 321, 393, 747, 729, 51380], "temperature": 0.0, "avg_logprob": -0.10107813043109441, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.002889077877625823}, {"id": 553, "seek": 279568, "start": 2816.0, "end": 2820.8799999999997, "text": " feuding armies and actually heal some of those wounds. Yeah, this is like, this feels, this", "tokens": [51380, 579, 33703, 28217, 293, 767, 10526, 512, 295, 729, 21969, 13, 865, 11, 341, 307, 411, 11, 341, 3417, 11, 341, 51624], "temperature": 0.0, "avg_logprob": -0.10107813043109441, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.002889077877625823}, {"id": 554, "seek": 282088, "start": 2820.88, "end": 2825.76, "text": " feels like a speech by George Washington or Abraham Lincoln or something. And you look at", "tokens": [50364, 3417, 411, 257, 6218, 538, 7136, 6149, 420, 17782, 15993, 420, 746, 13, 400, 291, 574, 412, 50608], "temperature": 0.0, "avg_logprob": -0.08722743235136333, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.003172455355525017}, {"id": 555, "seek": 282088, "start": 2825.76, "end": 2829.76, "text": " this and it's like, okay, well, how different are we? Yeah. We all want beautiful things. We all", "tokens": [50608, 341, 293, 309, 311, 411, 11, 1392, 11, 731, 11, 577, 819, 366, 321, 30, 865, 13, 492, 439, 528, 2238, 721, 13, 492, 439, 50808], "temperature": 0.0, "avg_logprob": -0.08722743235136333, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.003172455355525017}, {"id": 556, "seek": 282088, "start": 2829.76, "end": 2832.6400000000003, "text": " want something that's nice. We all want to be able to work together. We all want our stuff to be", "tokens": [50808, 528, 746, 300, 311, 1481, 13, 492, 439, 528, 281, 312, 1075, 281, 589, 1214, 13, 492, 439, 528, 527, 1507, 281, 312, 50952], "temperature": 0.0, "avg_logprob": -0.08722743235136333, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.003172455355525017}, {"id": 557, "seek": 282088, "start": 2832.6400000000003, "end": 2838.0, "text": " used, right? And so if we can help heal that, now I'm not optimistic that all people will use Mojo", "tokens": [50952, 1143, 11, 558, 30, 400, 370, 498, 321, 393, 854, 10526, 300, 11, 586, 286, 478, 406, 19397, 300, 439, 561, 486, 764, 3335, 5134, 51220], "temperature": 0.0, "avg_logprob": -0.08722743235136333, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.003172455355525017}, {"id": 558, "seek": 282088, "start": 2838.0, "end": 2842.7200000000003, "text": " and they'll stop using C++. Like that's not my goal, right? But if we can heal some of that,", "tokens": [51220, 293, 436, 603, 1590, 1228, 383, 25472, 13, 1743, 300, 311, 406, 452, 3387, 11, 558, 30, 583, 498, 321, 393, 10526, 512, 295, 300, 11, 51456], "temperature": 0.0, "avg_logprob": -0.08722743235136333, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.003172455355525017}, {"id": 559, "seek": 282088, "start": 2842.7200000000003, "end": 2847.6, "text": " I think that'd be pretty cool. Yeah. And we start by putting the people who like braces into the", "tokens": [51456, 286, 519, 300, 1116, 312, 1238, 1627, 13, 865, 13, 400, 321, 722, 538, 3372, 264, 561, 567, 411, 41537, 666, 264, 51700], "temperature": 0.0, "avg_logprob": -0.08722743235136333, "compression_ratio": 1.7708978328173375, "no_speech_prob": 0.003172455355525017}, {"id": 560, "seek": 284760, "start": 2847.6, "end": 2855.36, "text": " gulag. No. So there are proposals for adding braces to Mojo. We just tell them no. Okay.", "tokens": [50364, 290, 425, 559, 13, 883, 13, 407, 456, 366, 20198, 337, 5127, 41537, 281, 3335, 5134, 13, 492, 445, 980, 552, 572, 13, 1033, 13, 50752], "temperature": 0.0, "avg_logprob": -0.21531757354736328, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.002757142297923565}, {"id": 561, "seek": 284760, "start": 2857.44, "end": 2861.92, "text": " Politely. Yeah. Anyway, so there's a lot of amazing features on the roadmap and those ready", "tokens": [50856, 3635, 1959, 13, 865, 13, 5684, 11, 370, 456, 311, 257, 688, 295, 2243, 4122, 322, 264, 35738, 293, 729, 1919, 51080], "temperature": 0.0, "avg_logprob": -0.21531757354736328, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.002757142297923565}, {"id": 562, "seek": 284760, "start": 2861.92, "end": 2868.08, "text": " to implement it. It'd be awesome. I can just ask you a few things. So the other performance", "tokens": [51080, 281, 4445, 309, 13, 467, 1116, 312, 3476, 13, 286, 393, 445, 1029, 291, 257, 1326, 721, 13, 407, 264, 661, 3389, 51388], "temperature": 0.0, "avg_logprob": -0.21531757354736328, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.002757142297923565}, {"id": 563, "seek": 284760, "start": 2868.08, "end": 2873.92, "text": " improvement comes from immutability. So what's the what's this var and this let thing that we got", "tokens": [51388, 10444, 1487, 490, 3397, 325, 2310, 13, 407, 437, 311, 264, 437, 311, 341, 1374, 293, 341, 718, 551, 300, 321, 658, 51680], "temperature": 0.0, "avg_logprob": -0.21531757354736328, "compression_ratio": 1.5481171548117154, "no_speech_prob": 0.002757142297923565}, {"id": 564, "seek": 287392, "start": 2874.0, "end": 2881.2000000000003, "text": " going on? What's immutability? Yeah. So one of the things that is useful and it's not always", "tokens": [50368, 516, 322, 30, 708, 311, 3397, 325, 2310, 30, 865, 13, 407, 472, 295, 264, 721, 300, 307, 4420, 293, 309, 311, 406, 1009, 50728], "temperature": 0.0, "avg_logprob": -0.11094437126352005, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0028892557602375746}, {"id": 565, "seek": 287392, "start": 2881.2000000000003, "end": 2884.8, "text": " required, but it's useful is knowing whether something can change out from underneath you.", "tokens": [50728, 4739, 11, 457, 309, 311, 4420, 307, 5276, 1968, 746, 393, 1319, 484, 490, 7223, 291, 13, 50908], "temperature": 0.0, "avg_logprob": -0.11094437126352005, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0028892557602375746}, {"id": 566, "seek": 287392, "start": 2885.44, "end": 2890.64, "text": " Right. And so in Python, you have a pointer to an array, right? And so you pass that pointer", "tokens": [50940, 1779, 13, 400, 370, 294, 15329, 11, 291, 362, 257, 23918, 281, 364, 10225, 11, 558, 30, 400, 370, 291, 1320, 300, 23918, 51200], "temperature": 0.0, "avg_logprob": -0.11094437126352005, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0028892557602375746}, {"id": 567, "seek": 287392, "start": 2890.64, "end": 2896.48, "text": " to an array around to things. If you pass into a function, maybe take that and scroll away in", "tokens": [51200, 281, 364, 10225, 926, 281, 721, 13, 759, 291, 1320, 666, 257, 2445, 11, 1310, 747, 300, 293, 11369, 1314, 294, 51492], "temperature": 0.0, "avg_logprob": -0.11094437126352005, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0028892557602375746}, {"id": 568, "seek": 287392, "start": 2896.48, "end": 2901.04, "text": " some other data structure. And so you get your array back and you go to use it. Now somebody else", "tokens": [51492, 512, 661, 1412, 3877, 13, 400, 370, 291, 483, 428, 10225, 646, 293, 291, 352, 281, 764, 309, 13, 823, 2618, 1646, 51720], "temperature": 0.0, "avg_logprob": -0.11094437126352005, "compression_ratio": 1.701818181818182, "no_speech_prob": 0.0028892557602375746}, {"id": 569, "seek": 290104, "start": 2901.04, "end": 2904.88, "text": " is like putting stuff in your array. How do you reason about that? Because to be very", "tokens": [50364, 307, 411, 3372, 1507, 294, 428, 10225, 13, 1012, 360, 291, 1778, 466, 300, 30, 1436, 281, 312, 588, 50556], "temperature": 0.0, "avg_logprob": -0.09895059518646776, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.010324999690055847}, {"id": 570, "seek": 290104, "start": 2905.7599999999998, "end": 2910.64, "text": " complicated, at least a lot of bugs, right? And so one of the things that, you know, again,", "tokens": [50600, 6179, 11, 412, 1935, 257, 688, 295, 15120, 11, 558, 30, 400, 370, 472, 295, 264, 721, 300, 11, 291, 458, 11, 797, 11, 50844], "temperature": 0.0, "avg_logprob": -0.09895059518646776, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.010324999690055847}, {"id": 571, "seek": 290104, "start": 2910.64, "end": 2914.72, "text": " this is not something Mojo forces on you, but something that Mojo enables is a thing called", "tokens": [50844, 341, 307, 406, 746, 3335, 5134, 5874, 322, 291, 11, 457, 746, 300, 3335, 5134, 17077, 307, 257, 551, 1219, 51048], "temperature": 0.0, "avg_logprob": -0.09895059518646776, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.010324999690055847}, {"id": 572, "seek": 290104, "start": 2914.72, "end": 2922.08, "text": " value semantics. And what value semantics do is they take collections like arrays, like dictionaries,", "tokens": [51048, 2158, 4361, 45298, 13, 400, 437, 2158, 4361, 45298, 360, 307, 436, 747, 16641, 411, 41011, 11, 411, 22352, 4889, 11, 51416], "temperature": 0.0, "avg_logprob": -0.09895059518646776, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.010324999690055847}, {"id": 573, "seek": 290104, "start": 2922.88, "end": 2928.24, "text": " also tensors and strings and things like this that are much higher level and make them behave", "tokens": [51456, 611, 10688, 830, 293, 13985, 293, 721, 411, 341, 300, 366, 709, 2946, 1496, 293, 652, 552, 15158, 51724], "temperature": 0.0, "avg_logprob": -0.09895059518646776, "compression_ratio": 1.728624535315985, "no_speech_prob": 0.010324999690055847}, {"id": 574, "seek": 292824, "start": 2928.3199999999997, "end": 2933.6, "text": " like proper values. And so it makes it look like if you pass these things around, you get a logical", "tokens": [50368, 411, 2296, 4190, 13, 400, 370, 309, 1669, 309, 574, 411, 498, 291, 1320, 613, 721, 926, 11, 291, 483, 257, 14978, 50632], "temperature": 0.0, "avg_logprob": -0.1143680820620157, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.002714693546295166}, {"id": 575, "seek": 292824, "start": 2933.6, "end": 2938.56, "text": " copy of all the data. And so if I pass you an array, sure array, you can go do what you want", "tokens": [50632, 5055, 295, 439, 264, 1412, 13, 400, 370, 498, 286, 1320, 291, 364, 10225, 11, 988, 10225, 11, 291, 393, 352, 360, 437, 291, 528, 50880], "temperature": 0.0, "avg_logprob": -0.1143680820620157, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.002714693546295166}, {"id": 576, "seek": 292824, "start": 2938.56, "end": 2943.68, "text": " to it. You're not going to hurt my array. Now, that is an interesting and very powerful design", "tokens": [50880, 281, 309, 13, 509, 434, 406, 516, 281, 4607, 452, 10225, 13, 823, 11, 300, 307, 364, 1880, 293, 588, 4005, 1715, 51136], "temperature": 0.0, "avg_logprob": -0.1143680820620157, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.002714693546295166}, {"id": 577, "seek": 292824, "start": 2943.68, "end": 2947.8399999999997, "text": " principle. It defines the way a ton of bugs. You have to be careful to implement it in an efficient", "tokens": [51136, 8665, 13, 467, 23122, 264, 636, 257, 2952, 295, 15120, 13, 509, 362, 281, 312, 5026, 281, 4445, 309, 294, 364, 7148, 51344], "temperature": 0.0, "avg_logprob": -0.1143680820620157, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.002714693546295166}, {"id": 578, "seek": 292824, "start": 2947.8399999999997, "end": 2954.4799999999996, "text": " way. Yeah, is there a performance hit that's significant? Generally not, if you implement it", "tokens": [51344, 636, 13, 865, 11, 307, 456, 257, 3389, 2045, 300, 311, 4776, 30, 21082, 406, 11, 498, 291, 4445, 309, 51676], "temperature": 0.0, "avg_logprob": -0.1143680820620157, "compression_ratio": 1.6724738675958188, "no_speech_prob": 0.002714693546295166}, {"id": 579, "seek": 295448, "start": 2954.48, "end": 2959.84, "text": " the right way. But it requires a lot of very low level, getting the language right bits.", "tokens": [50364, 264, 558, 636, 13, 583, 309, 7029, 257, 688, 295, 588, 2295, 1496, 11, 1242, 264, 2856, 558, 9239, 13, 50632], "temperature": 0.0, "avg_logprob": -0.17251097771429247, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.009410214610397816}, {"id": 580, "seek": 295448, "start": 2960.56, "end": 2964.48, "text": " I assume there'll be a huge performance hit, because it's a really nice, the benefit is really", "tokens": [50668, 286, 6552, 456, 603, 312, 257, 2603, 3389, 2045, 11, 570, 309, 311, 257, 534, 1481, 11, 264, 5121, 307, 534, 50864], "temperature": 0.0, "avg_logprob": -0.17251097771429247, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.009410214610397816}, {"id": 581, "seek": 295448, "start": 2964.48, "end": 2968.56, "text": " nice, because you don't get into the complex. Absolutely. Well, the trick is, is you can't do", "tokens": [50864, 1481, 11, 570, 291, 500, 380, 483, 666, 264, 3997, 13, 7021, 13, 1042, 11, 264, 4282, 307, 11, 307, 291, 393, 380, 360, 51068], "temperature": 0.0, "avg_logprob": -0.17251097771429247, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.009410214610397816}, {"id": 582, "seek": 295448, "start": 2968.56, "end": 2976.2400000000002, "text": " copies. So you have to provide the behavior of copying without doing the copy. Yeah. How do you", "tokens": [51068, 14341, 13, 407, 291, 362, 281, 2893, 264, 5223, 295, 27976, 1553, 884, 264, 5055, 13, 865, 13, 1012, 360, 291, 51452], "temperature": 0.0, "avg_logprob": -0.17251097771429247, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.009410214610397816}, {"id": 583, "seek": 295448, "start": 2976.2400000000002, "end": 2982.2400000000002, "text": " do that? How do you do that? It's not magic. It's just, it's actually pretty cool. Well,", "tokens": [51452, 360, 300, 30, 1012, 360, 291, 360, 300, 30, 467, 311, 406, 5585, 13, 467, 311, 445, 11, 309, 311, 767, 1238, 1627, 13, 1042, 11, 51752], "temperature": 0.0, "avg_logprob": -0.17251097771429247, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.009410214610397816}, {"id": 584, "seek": 298224, "start": 2982.24, "end": 2986.0, "text": " so first, before we talk about how that works, let's talk about how it works in Python, right?", "tokens": [50364, 370, 700, 11, 949, 321, 751, 466, 577, 300, 1985, 11, 718, 311, 751, 466, 577, 309, 1985, 294, 15329, 11, 558, 30, 50552], "temperature": 0.0, "avg_logprob": -0.10211327611183633, "compression_ratio": 1.9421768707482994, "no_speech_prob": 0.0009398096590302885}, {"id": 585, "seek": 298224, "start": 2986.0, "end": 2991.2, "text": " So in Python, you define a person class, or maybe a person class is a bad idea. You define a", "tokens": [50552, 407, 294, 15329, 11, 291, 6964, 257, 954, 1508, 11, 420, 1310, 257, 954, 1508, 307, 257, 1578, 1558, 13, 509, 6964, 257, 50812], "temperature": 0.0, "avg_logprob": -0.10211327611183633, "compression_ratio": 1.9421768707482994, "no_speech_prob": 0.0009398096590302885}, {"id": 586, "seek": 298224, "start": 2991.2, "end": 2995.3599999999997, "text": " database class, right? And database class has an array of records, something like that, right?", "tokens": [50812, 8149, 1508, 11, 558, 30, 400, 8149, 1508, 575, 364, 10225, 295, 7724, 11, 746, 411, 300, 11, 558, 30, 51020], "temperature": 0.0, "avg_logprob": -0.10211327611183633, "compression_ratio": 1.9421768707482994, "no_speech_prob": 0.0009398096590302885}, {"id": 587, "seek": 298224, "start": 2995.3599999999997, "end": 3000.72, "text": " And so the problem is that if you pass in a record or a class instance into the database,", "tokens": [51020, 400, 370, 264, 1154, 307, 300, 498, 291, 1320, 294, 257, 2136, 420, 257, 1508, 5197, 666, 264, 8149, 11, 51288], "temperature": 0.0, "avg_logprob": -0.10211327611183633, "compression_ratio": 1.9421768707482994, "no_speech_prob": 0.0009398096590302885}, {"id": 588, "seek": 298224, "start": 3000.72, "end": 3006.72, "text": " it'll take a hold of that object. And then it assumes it has it. And if you're passing an object in,", "tokens": [51288, 309, 603, 747, 257, 1797, 295, 300, 2657, 13, 400, 550, 309, 37808, 309, 575, 309, 13, 400, 498, 291, 434, 8437, 364, 2657, 294, 11, 51588], "temperature": 0.0, "avg_logprob": -0.10211327611183633, "compression_ratio": 1.9421768707482994, "no_speech_prob": 0.0009398096590302885}, {"id": 589, "seek": 298224, "start": 3006.72, "end": 3010.72, "text": " you have to know that that database is going to take, take it. And therefore you shouldn't change", "tokens": [51588, 291, 362, 281, 458, 300, 300, 8149, 307, 516, 281, 747, 11, 747, 309, 13, 400, 4412, 291, 4659, 380, 1319, 51788], "temperature": 0.0, "avg_logprob": -0.10211327611183633, "compression_ratio": 1.9421768707482994, "no_speech_prob": 0.0009398096590302885}, {"id": 590, "seek": 301072, "start": 3010.7999999999997, "end": 3013.9199999999996, "text": " it after you put in the database, right? This is, this is kind of have to know that you just", "tokens": [50368, 309, 934, 291, 829, 294, 264, 8149, 11, 558, 30, 639, 307, 11, 341, 307, 733, 295, 362, 281, 458, 300, 291, 445, 50524], "temperature": 0.0, "avg_logprob": -0.1240124099556057, "compression_ratio": 2.0090361445783134, "no_speech_prob": 0.026748502627015114}, {"id": 591, "seek": 301072, "start": 3013.9199999999996, "end": 3018.3999999999996, "text": " have to kind of know that, right? And so you roll out version one of the database, you just kind", "tokens": [50524, 362, 281, 733, 295, 458, 300, 11, 558, 30, 400, 370, 291, 3373, 484, 3037, 472, 295, 264, 8149, 11, 291, 445, 733, 50748], "temperature": 0.0, "avg_logprob": -0.1240124099556057, "compression_ratio": 2.0090361445783134, "no_speech_prob": 0.026748502627015114}, {"id": 592, "seek": 301072, "start": 3018.3999999999996, "end": 3022.72, "text": " of have to know that. Of course, Lex uses his own database, right? Yeah, right? Because you built", "tokens": [50748, 295, 362, 281, 458, 300, 13, 2720, 1164, 11, 24086, 4960, 702, 1065, 8149, 11, 558, 30, 865, 11, 558, 30, 1436, 291, 3094, 50964], "temperature": 0.0, "avg_logprob": -0.1240124099556057, "compression_ratio": 2.0090361445783134, "no_speech_prob": 0.026748502627015114}, {"id": 593, "seek": 301072, "start": 3022.72, "end": 3026.9599999999996, "text": " it, you understand this works, right? Somebody else joins the team, they don't know this. Yes.", "tokens": [50964, 309, 11, 291, 1223, 341, 1985, 11, 558, 30, 13463, 1646, 24397, 264, 1469, 11, 436, 500, 380, 458, 341, 13, 1079, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1240124099556057, "compression_ratio": 2.0090361445783134, "no_speech_prob": 0.026748502627015114}, {"id": 594, "seek": 301072, "start": 3026.9599999999996, "end": 3031.68, "text": " Right. And so now they suddenly get bugs, you're having to maintain the database, you shake your", "tokens": [51176, 1779, 13, 400, 370, 586, 436, 5800, 483, 15120, 11, 291, 434, 1419, 281, 6909, 264, 8149, 11, 291, 10283, 428, 51412], "temperature": 0.0, "avg_logprob": -0.1240124099556057, "compression_ratio": 2.0090361445783134, "no_speech_prob": 0.026748502627015114}, {"id": 595, "seek": 301072, "start": 3031.68, "end": 3036.56, "text": " fist, you argue the 10th time this happens, you're like, okay, we have to do something different.", "tokens": [51412, 21849, 11, 291, 9695, 264, 1266, 392, 565, 341, 2314, 11, 291, 434, 411, 11, 1392, 11, 321, 362, 281, 360, 746, 819, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1240124099556057, "compression_ratio": 2.0090361445783134, "no_speech_prob": 0.026748502627015114}, {"id": 596, "seek": 301072, "start": 3036.56, "end": 3040.0, "text": " Right. And so what you do is you go change your Python code, and you change your database", "tokens": [51656, 1779, 13, 400, 370, 437, 291, 360, 307, 291, 352, 1319, 428, 15329, 3089, 11, 293, 291, 1319, 428, 8149, 51828], "temperature": 0.0, "avg_logprob": -0.1240124099556057, "compression_ratio": 2.0090361445783134, "no_speech_prob": 0.026748502627015114}, {"id": 597, "seek": 304000, "start": 3040.0, "end": 3045.44, "text": " class to copy the record every time you add it. And so what ends up happening is you say, okay,", "tokens": [50364, 1508, 281, 5055, 264, 2136, 633, 565, 291, 909, 309, 13, 400, 370, 437, 5314, 493, 2737, 307, 291, 584, 11, 1392, 11, 50636], "temperature": 0.0, "avg_logprob": -0.07783969461101375, "compression_ratio": 1.7606557377049181, "no_speech_prob": 0.0028003461193293333}, {"id": 598, "seek": 304000, "start": 3045.44, "end": 3050.32, "text": " I will do what's called a defensive copy inside the database. And then that way,", "tokens": [50636, 286, 486, 360, 437, 311, 1219, 257, 16468, 5055, 1854, 264, 8149, 13, 400, 550, 300, 636, 11, 50880], "temperature": 0.0, "avg_logprob": -0.07783969461101375, "compression_ratio": 1.7606557377049181, "no_speech_prob": 0.0028003461193293333}, {"id": 599, "seek": 304000, "start": 3050.32, "end": 3055.44, "text": " if somebody passes something in, I will have my own copy of it. And they can go do whatever,", "tokens": [50880, 498, 2618, 11335, 746, 294, 11, 286, 486, 362, 452, 1065, 5055, 295, 309, 13, 400, 436, 393, 352, 360, 2035, 11, 51136], "temperature": 0.0, "avg_logprob": -0.07783969461101375, "compression_ratio": 1.7606557377049181, "no_speech_prob": 0.0028003461193293333}, {"id": 600, "seek": 304000, "start": 3055.44, "end": 3060.48, "text": " and they're not going to break my thing. Okay, this is usually the, the two design patterns.", "tokens": [51136, 293, 436, 434, 406, 516, 281, 1821, 452, 551, 13, 1033, 11, 341, 307, 2673, 264, 11, 264, 732, 1715, 8294, 13, 51388], "temperature": 0.0, "avg_logprob": -0.07783969461101375, "compression_ratio": 1.7606557377049181, "no_speech_prob": 0.0028003461193293333}, {"id": 601, "seek": 304000, "start": 3060.48, "end": 3064.88, "text": " If you look in PyTorch, for example, this is cloning a tensor, like there's a specific thing,", "tokens": [51388, 759, 291, 574, 294, 9953, 51, 284, 339, 11, 337, 1365, 11, 341, 307, 596, 16638, 257, 40863, 11, 411, 456, 311, 257, 2685, 551, 11, 51608], "temperature": 0.0, "avg_logprob": -0.07783969461101375, "compression_ratio": 1.7606557377049181, "no_speech_prob": 0.0028003461193293333}, {"id": 602, "seek": 304000, "start": 3064.88, "end": 3066.8, "text": " and you have to know where to call it. And if you don't call in the right place,", "tokens": [51608, 293, 291, 362, 281, 458, 689, 281, 818, 309, 13, 400, 498, 291, 500, 380, 818, 294, 264, 558, 1081, 11, 51704], "temperature": 0.0, "avg_logprob": -0.07783969461101375, "compression_ratio": 1.7606557377049181, "no_speech_prob": 0.0028003461193293333}, {"id": 603, "seek": 306680, "start": 3066.88, "end": 3072.2400000000002, "text": " you get these bugs, and this is state of the art. Right. So a different approach. So it's used in", "tokens": [50368, 291, 483, 613, 15120, 11, 293, 341, 307, 1785, 295, 264, 1523, 13, 1779, 13, 407, 257, 819, 3109, 13, 407, 309, 311, 1143, 294, 50636], "temperature": 0.0, "avg_logprob": -0.10171962506843335, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0033761339727789164}, {"id": 604, "seek": 306680, "start": 3072.2400000000002, "end": 3077.52, "text": " many languages. So I worked with it in Swift, as you say, okay, well, let's provide value semantics.", "tokens": [50636, 867, 8650, 13, 407, 286, 2732, 365, 309, 294, 25539, 11, 382, 291, 584, 11, 1392, 11, 731, 11, 718, 311, 2893, 2158, 4361, 45298, 13, 50900], "temperature": 0.0, "avg_logprob": -0.10171962506843335, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0033761339727789164}, {"id": 605, "seek": 306680, "start": 3077.52, "end": 3083.44, "text": " And so we want to provide the view that you get a logically independent copy. But we want to do", "tokens": [50900, 400, 370, 321, 528, 281, 2893, 264, 1910, 300, 291, 483, 257, 38887, 6695, 5055, 13, 583, 321, 528, 281, 360, 51196], "temperature": 0.0, "avg_logprob": -0.10171962506843335, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0033761339727789164}, {"id": 606, "seek": 306680, "start": 3083.44, "end": 3089.36, "text": " that lazily. And so what we do is you say, okay, if you pass something into a function, it doesn't", "tokens": [51196, 300, 19320, 953, 13, 400, 370, 437, 321, 360, 307, 291, 584, 11, 1392, 11, 498, 291, 1320, 746, 666, 257, 2445, 11, 309, 1177, 380, 51492], "temperature": 0.0, "avg_logprob": -0.10171962506843335, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0033761339727789164}, {"id": 607, "seek": 306680, "start": 3089.36, "end": 3093.44, "text": " actually make a copy. What it actually does is it just increments a reference to it. And if you", "tokens": [51492, 767, 652, 257, 5055, 13, 708, 309, 767, 775, 307, 309, 445, 1946, 1117, 257, 6408, 281, 309, 13, 400, 498, 291, 51696], "temperature": 0.0, "avg_logprob": -0.10171962506843335, "compression_ratio": 1.7157894736842105, "no_speech_prob": 0.0033761339727789164}, {"id": 608, "seek": 309344, "start": 3093.44, "end": 3098.2400000000002, "text": " pass it around, you stick in your database, you can go into the database, you own it. And then", "tokens": [50364, 1320, 309, 926, 11, 291, 2897, 294, 428, 8149, 11, 291, 393, 352, 666, 264, 8149, 11, 291, 1065, 309, 13, 400, 550, 50604], "temperature": 0.0, "avg_logprob": -0.08617851434164489, "compression_ratio": 2.0073260073260073, "no_speech_prob": 0.00884640496224165}, {"id": 609, "seek": 309344, "start": 3098.2400000000002, "end": 3102.32, "text": " you come back out of the stack, nobody's copied anything, you come back out of the stack, and", "tokens": [50604, 291, 808, 646, 484, 295, 264, 8630, 11, 5079, 311, 25365, 1340, 11, 291, 808, 646, 484, 295, 264, 8630, 11, 293, 50808], "temperature": 0.0, "avg_logprob": -0.08617851434164489, "compression_ratio": 2.0073260073260073, "no_speech_prob": 0.00884640496224165}, {"id": 610, "seek": 309344, "start": 3102.32, "end": 3107.44, "text": " then the caller, let's go of it. Well, then you've just handed it off to the database,", "tokens": [50808, 550, 264, 48324, 11, 718, 311, 352, 295, 309, 13, 1042, 11, 550, 291, 600, 445, 16013, 309, 766, 281, 264, 8149, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08617851434164489, "compression_ratio": 2.0073260073260073, "no_speech_prob": 0.00884640496224165}, {"id": 611, "seek": 309344, "start": 3107.44, "end": 3112.88, "text": " you've transferred it, and there's no copies made. Now, on the other hand, if, you know,", "tokens": [51064, 291, 600, 15809, 309, 11, 293, 456, 311, 572, 14341, 1027, 13, 823, 11, 322, 264, 661, 1011, 11, 498, 11, 291, 458, 11, 51336], "temperature": 0.0, "avg_logprob": -0.08617851434164489, "compression_ratio": 2.0073260073260073, "no_speech_prob": 0.00884640496224165}, {"id": 612, "seek": 309344, "start": 3112.88, "end": 3117.36, "text": " your coworker goes and hands you a record, and you pass it in, you stick it in the database,", "tokens": [51336, 428, 31998, 260, 1709, 293, 2377, 291, 257, 2136, 11, 293, 291, 1320, 309, 294, 11, 291, 2897, 309, 294, 264, 8149, 11, 51560], "temperature": 0.0, "avg_logprob": -0.08617851434164489, "compression_ratio": 2.0073260073260073, "no_speech_prob": 0.00884640496224165}, {"id": 613, "seek": 309344, "start": 3117.36, "end": 3122.16, "text": " and then you go to town, and you start modifying it, what happens is you get a copy lazily", "tokens": [51560, 293, 550, 291, 352, 281, 3954, 11, 293, 291, 722, 42626, 309, 11, 437, 2314, 307, 291, 483, 257, 5055, 19320, 953, 51800], "temperature": 0.0, "avg_logprob": -0.08617851434164489, "compression_ratio": 2.0073260073260073, "no_speech_prob": 0.00884640496224165}, {"id": 614, "seek": 312216, "start": 3122.7999999999997, "end": 3127.12, "text": " on demand. And so what this does, this gives you copies only when you need them.", "tokens": [50396, 322, 4733, 13, 400, 370, 437, 341, 775, 11, 341, 2709, 291, 14341, 787, 562, 291, 643, 552, 13, 50612], "temperature": 0.0, "avg_logprob": -0.15833566041119332, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.013217967003583908}, {"id": 615, "seek": 312216, "start": 3127.68, "end": 3131.7599999999998, "text": " And it also, so it defines the way the bugs, but it also generally reduces the number of copies", "tokens": [50640, 400, 309, 611, 11, 370, 309, 23122, 264, 636, 264, 15120, 11, 457, 309, 611, 5101, 18081, 264, 1230, 295, 14341, 50844], "temperature": 0.0, "avg_logprob": -0.15833566041119332, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.013217967003583908}, {"id": 616, "seek": 312216, "start": 3131.7599999999998, "end": 3137.44, "text": " in practice. And so the implementation details are tricky here. Yes. So this is, yes, something", "tokens": [50844, 294, 3124, 13, 400, 370, 264, 11420, 4365, 366, 12414, 510, 13, 1079, 13, 407, 341, 307, 11, 2086, 11, 746, 51128], "temperature": 0.0, "avg_logprob": -0.15833566041119332, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.013217967003583908}, {"id": 617, "seek": 312216, "start": 3137.44, "end": 3144.08, "text": " with reference counting, but to make it performant across the number of different kinds of", "tokens": [51128, 365, 6408, 13251, 11, 457, 281, 652, 309, 2042, 394, 2108, 264, 1230, 295, 819, 3685, 295, 51460], "temperature": 0.0, "avg_logprob": -0.15833566041119332, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.013217967003583908}, {"id": 618, "seek": 312216, "start": 3145.12, "end": 3150.0, "text": " objects. Yeah, so you need a couple of things. And so there's many, so this concept has existed", "tokens": [51512, 6565, 13, 865, 11, 370, 291, 643, 257, 1916, 295, 721, 13, 400, 370, 456, 311, 867, 11, 370, 341, 3410, 575, 13135, 51756], "temperature": 0.0, "avg_logprob": -0.15833566041119332, "compression_ratio": 1.7452471482889733, "no_speech_prob": 0.013217967003583908}, {"id": 619, "seek": 315000, "start": 3150.0, "end": 3156.16, "text": " in many different worlds. And so, again, it's not novel research at all, right? The magic is", "tokens": [50364, 294, 867, 819, 13401, 13, 400, 370, 11, 797, 11, 309, 311, 406, 7613, 2132, 412, 439, 11, 558, 30, 440, 5585, 307, 50672], "temperature": 0.0, "avg_logprob": -0.08438138528303667, "compression_ratio": 1.802588996763754, "no_speech_prob": 0.005553808528929949}, {"id": 620, "seek": 315000, "start": 3156.16, "end": 3160.0, "text": " getting the design right so that you can do this in a reasonable way, right? And so there's a number", "tokens": [50672, 1242, 264, 1715, 558, 370, 300, 291, 393, 360, 341, 294, 257, 10585, 636, 11, 558, 30, 400, 370, 456, 311, 257, 1230, 50864], "temperature": 0.0, "avg_logprob": -0.08438138528303667, "compression_ratio": 1.802588996763754, "no_speech_prob": 0.005553808528929949}, {"id": 621, "seek": 315000, "start": 3160.0, "end": 3165.04, "text": " of components that go into this. One is when you're passing around, so we're talking about", "tokens": [50864, 295, 6677, 300, 352, 666, 341, 13, 1485, 307, 562, 291, 434, 8437, 926, 11, 370, 321, 434, 1417, 466, 51116], "temperature": 0.0, "avg_logprob": -0.08438138528303667, "compression_ratio": 1.802588996763754, "no_speech_prob": 0.005553808528929949}, {"id": 622, "seek": 315000, "start": 3165.04, "end": 3169.2, "text": " Python and reference counting at the expense of doing that, when you're passing values around,", "tokens": [51116, 15329, 293, 6408, 13251, 412, 264, 18406, 295, 884, 300, 11, 562, 291, 434, 8437, 4190, 926, 11, 51324], "temperature": 0.0, "avg_logprob": -0.08438138528303667, "compression_ratio": 1.802588996763754, "no_speech_prob": 0.005553808528929949}, {"id": 623, "seek": 315000, "start": 3169.2, "end": 3172.72, "text": " you don't want to do extra reference counting for no good reason. And so you have to make", "tokens": [51324, 291, 500, 380, 528, 281, 360, 2857, 6408, 13251, 337, 572, 665, 1778, 13, 400, 370, 291, 362, 281, 652, 51500], "temperature": 0.0, "avg_logprob": -0.08438138528303667, "compression_ratio": 1.802588996763754, "no_speech_prob": 0.005553808528929949}, {"id": 624, "seek": 315000, "start": 3172.72, "end": 3177.2, "text": " sure that you're efficient and you transfer ownership instead of duplicating references", "tokens": [51500, 988, 300, 291, 434, 7148, 293, 291, 5003, 15279, 2602, 295, 17154, 990, 15400, 51724], "temperature": 0.0, "avg_logprob": -0.08438138528303667, "compression_ratio": 1.802588996763754, "no_speech_prob": 0.005553808528929949}, {"id": 625, "seek": 317720, "start": 3177.2, "end": 3182.3999999999996, "text": " and things like that, which is a very low level problem. You also have to adopt this and you have", "tokens": [50364, 293, 721, 411, 300, 11, 597, 307, 257, 588, 2295, 1496, 1154, 13, 509, 611, 362, 281, 6878, 341, 293, 291, 362, 50624], "temperature": 0.0, "avg_logprob": -0.07744899846739688, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.004330258816480637}, {"id": 626, "seek": 317720, "start": 3182.3999999999996, "end": 3187.68, "text": " to build these data structures. And so if you say, you know, Mojo has to be compatible with Python.", "tokens": [50624, 281, 1322, 613, 1412, 9227, 13, 400, 370, 498, 291, 584, 11, 291, 458, 11, 3335, 5134, 575, 281, 312, 18218, 365, 15329, 13, 50888], "temperature": 0.0, "avg_logprob": -0.07744899846739688, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.004330258816480637}, {"id": 627, "seek": 317720, "start": 3187.68, "end": 3193.2, "text": " So of course, the default list is a reference semantic list that works the way you'd expect", "tokens": [50888, 407, 295, 1164, 11, 264, 7576, 1329, 307, 257, 6408, 47982, 1329, 300, 1985, 264, 636, 291, 1116, 2066, 51164], "temperature": 0.0, "avg_logprob": -0.07744899846739688, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.004330258816480637}, {"id": 628, "seek": 317720, "start": 3193.2, "end": 3197.68, "text": " in Python. But then you have to design a value semantic list. And so you just have to implement", "tokens": [51164, 294, 15329, 13, 583, 550, 291, 362, 281, 1715, 257, 2158, 47982, 1329, 13, 400, 370, 291, 445, 362, 281, 4445, 51388], "temperature": 0.0, "avg_logprob": -0.07744899846739688, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.004330258816480637}, {"id": 629, "seek": 317720, "start": 3197.68, "end": 3202.48, "text": " that and then you implement the logic within. And so the role of the language here is to provide", "tokens": [51388, 300, 293, 550, 291, 4445, 264, 9952, 1951, 13, 400, 370, 264, 3090, 295, 264, 2856, 510, 307, 281, 2893, 51628], "temperature": 0.0, "avg_logprob": -0.07744899846739688, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.004330258816480637}, {"id": 630, "seek": 320248, "start": 3202.48, "end": 3208.08, "text": " all the low level hooks that allow the author of the type to be able to get and express this", "tokens": [50364, 439, 264, 2295, 1496, 26485, 300, 2089, 264, 3793, 295, 264, 2010, 281, 312, 1075, 281, 483, 293, 5109, 341, 50644], "temperature": 0.0, "avg_logprob": -0.11560371310211891, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.02160666696727276}, {"id": 631, "seek": 320248, "start": 3208.08, "end": 3212.8, "text": " behavior without forcing it into all cases or hard coding this into the language itself.", "tokens": [50644, 5223, 1553, 19030, 309, 666, 439, 3331, 420, 1152, 17720, 341, 666, 264, 2856, 2564, 13, 50880], "temperature": 0.0, "avg_logprob": -0.11560371310211891, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.02160666696727276}, {"id": 632, "seek": 320248, "start": 3212.8, "end": 3217.04, "text": " But there's ownership. So you're constantly transferring, you're tracking who owns the thing.", "tokens": [50880, 583, 456, 311, 15279, 13, 407, 291, 434, 6460, 31437, 11, 291, 434, 11603, 567, 19143, 264, 551, 13, 51092], "temperature": 0.0, "avg_logprob": -0.11560371310211891, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.02160666696727276}, {"id": 633, "seek": 320248, "start": 3217.04, "end": 3221.76, "text": " Yes. And so there's a whole system called ownership. And so this is related to work done", "tokens": [51092, 1079, 13, 400, 370, 456, 311, 257, 1379, 1185, 1219, 15279, 13, 400, 370, 341, 307, 4077, 281, 589, 1096, 51328], "temperature": 0.0, "avg_logprob": -0.11560371310211891, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.02160666696727276}, {"id": 634, "seek": 320248, "start": 3221.76, "end": 3225.6, "text": " in the Rust community. Also the Swift community has done a bunch of work. And there's a bunch of", "tokens": [51328, 294, 264, 34952, 1768, 13, 2743, 264, 25539, 1768, 575, 1096, 257, 3840, 295, 589, 13, 400, 456, 311, 257, 3840, 295, 51520], "temperature": 0.0, "avg_logprob": -0.11560371310211891, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.02160666696727276}, {"id": 635, "seek": 320248, "start": 3225.6, "end": 3230.72, "text": " different other languages that evolve kind of C++ actually has copy constructors and destructors", "tokens": [51520, 819, 661, 8650, 300, 16693, 733, 295, 383, 25472, 767, 575, 5055, 7690, 830, 293, 2677, 1757, 830, 51776], "temperature": 0.0, "avg_logprob": -0.11560371310211891, "compression_ratio": 1.8116883116883118, "no_speech_prob": 0.02160666696727276}, {"id": 636, "seek": 323072, "start": 3230.72, "end": 3235.52, "text": " and things like that. And so I mean, C++ has everything. So it has move constructors and", "tokens": [50364, 293, 721, 411, 300, 13, 400, 370, 286, 914, 11, 383, 25472, 575, 1203, 13, 407, 309, 575, 1286, 7690, 830, 293, 50604], "temperature": 0.0, "avg_logprob": -0.0857342981523083, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0003919847367797047}, {"id": 637, "seek": 323072, "start": 3235.52, "end": 3241.7599999999998, "text": " has like this whole world of things. And so this is a body of work that's kind of been developing", "tokens": [50604, 575, 411, 341, 1379, 1002, 295, 721, 13, 400, 370, 341, 307, 257, 1772, 295, 589, 300, 311, 733, 295, 668, 6416, 50916], "temperature": 0.0, "avg_logprob": -0.0857342981523083, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0003919847367797047}, {"id": 638, "seek": 323072, "start": 3241.7599999999998, "end": 3247.3599999999997, "text": " for many, many years now. And so Mojo takes some of the best ideas out of all these systems and", "tokens": [50916, 337, 867, 11, 867, 924, 586, 13, 400, 370, 3335, 5134, 2516, 512, 295, 264, 1151, 3487, 484, 295, 439, 613, 3652, 293, 51196], "temperature": 0.0, "avg_logprob": -0.0857342981523083, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0003919847367797047}, {"id": 639, "seek": 323072, "start": 3247.3599999999997, "end": 3252.9599999999996, "text": " remixes it in a nice way so that you get the power of something like the Rust programming language.", "tokens": [51196, 890, 36005, 309, 294, 257, 1481, 636, 370, 300, 291, 483, 264, 1347, 295, 746, 411, 264, 34952, 9410, 2856, 13, 51476], "temperature": 0.0, "avg_logprob": -0.0857342981523083, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0003919847367797047}, {"id": 640, "seek": 323072, "start": 3252.9599999999996, "end": 3257.2, "text": " But you don't have to deal with it when you don't want to, which is a major thing in terms of", "tokens": [51476, 583, 291, 500, 380, 362, 281, 2028, 365, 309, 562, 291, 500, 380, 528, 281, 11, 597, 307, 257, 2563, 551, 294, 2115, 295, 51688], "temperature": 0.0, "avg_logprob": -0.0857342981523083, "compression_ratio": 1.6939501779359432, "no_speech_prob": 0.0003919847367797047}, {"id": 641, "seek": 325720, "start": 3257.2, "end": 3259.7599999999998, "text": " teaching and learning and being able to use and scale these systems.", "tokens": [50364, 4571, 293, 2539, 293, 885, 1075, 281, 764, 293, 4373, 613, 3652, 13, 50492], "temperature": 0.0, "avg_logprob": -0.11911871603557042, "compression_ratio": 1.9224806201550388, "no_speech_prob": 0.014059528708457947}, {"id": 642, "seek": 325720, "start": 3260.8799999999997, "end": 3265.4399999999996, "text": " How does that play with argument conventions? What are they? Why are they important?", "tokens": [50548, 1012, 775, 300, 862, 365, 6770, 33520, 30, 708, 366, 436, 30, 1545, 366, 436, 1021, 30, 50776], "temperature": 0.0, "avg_logprob": -0.11911871603557042, "compression_ratio": 1.9224806201550388, "no_speech_prob": 0.014059528708457947}, {"id": 643, "seek": 325720, "start": 3265.4399999999996, "end": 3269.8399999999997, "text": " How does the value semantics? How does the transfer ownership work with the arguments", "tokens": [50776, 1012, 775, 264, 2158, 4361, 45298, 30, 1012, 775, 264, 5003, 15279, 589, 365, 264, 12869, 50996], "temperature": 0.0, "avg_logprob": -0.11911871603557042, "compression_ratio": 1.9224806201550388, "no_speech_prob": 0.014059528708457947}, {"id": 644, "seek": 325720, "start": 3269.8399999999997, "end": 3274.16, "text": " when they're passed into functions? So if you go deep into systems programming land,", "tokens": [50996, 562, 436, 434, 4678, 666, 6828, 30, 407, 498, 291, 352, 2452, 666, 3652, 9410, 2117, 11, 51212], "temperature": 0.0, "avg_logprob": -0.11911871603557042, "compression_ratio": 1.9224806201550388, "no_speech_prob": 0.014059528708457947}, {"id": 645, "seek": 325720, "start": 3274.16, "end": 3277.8399999999997, "text": " so this isn't again, this is not something for everybody, but if you go deep into systems", "tokens": [51212, 370, 341, 1943, 380, 797, 11, 341, 307, 406, 746, 337, 2201, 11, 457, 498, 291, 352, 2452, 666, 3652, 51396], "temperature": 0.0, "avg_logprob": -0.11911871603557042, "compression_ratio": 1.9224806201550388, "no_speech_prob": 0.014059528708457947}, {"id": 646, "seek": 325720, "start": 3277.8399999999997, "end": 3282.64, "text": " programming land, what you encounter is you encounter these types that get weird.", "tokens": [51396, 9410, 2117, 11, 437, 291, 8593, 307, 291, 8593, 613, 3467, 300, 483, 3657, 13, 51636], "temperature": 0.0, "avg_logprob": -0.11911871603557042, "compression_ratio": 1.9224806201550388, "no_speech_prob": 0.014059528708457947}, {"id": 647, "seek": 328264, "start": 3283.6, "end": 3287.2, "text": " So if you're used to Python, you think about everything, I can just copy it around,", "tokens": [50412, 407, 498, 291, 434, 1143, 281, 15329, 11, 291, 519, 466, 1203, 11, 286, 393, 445, 5055, 309, 926, 11, 50592], "temperature": 0.0, "avg_logprob": -0.09929652724947248, "compression_ratio": 1.7745901639344261, "no_speech_prob": 0.027581389993429184}, {"id": 648, "seek": 328264, "start": 3287.2, "end": 3290.56, "text": " I can go change it and mutate it and do these things. And it's all cool.", "tokens": [50592, 286, 393, 352, 1319, 309, 293, 5839, 473, 309, 293, 360, 613, 721, 13, 400, 309, 311, 439, 1627, 13, 50760], "temperature": 0.0, "avg_logprob": -0.09929652724947248, "compression_ratio": 1.7745901639344261, "no_speech_prob": 0.027581389993429184}, {"id": 649, "seek": 328264, "start": 3292.08, "end": 3296.56, "text": " If you get into systems programming land, you get into these things like I have an atomic number,", "tokens": [50836, 759, 291, 483, 666, 3652, 9410, 2117, 11, 291, 483, 666, 613, 721, 411, 286, 362, 364, 22275, 1230, 11, 51060], "temperature": 0.0, "avg_logprob": -0.09929652724947248, "compression_ratio": 1.7745901639344261, "no_speech_prob": 0.027581389993429184}, {"id": 650, "seek": 328264, "start": 3296.56, "end": 3303.68, "text": " or I have a mutex, or I have a uniquely owned database handle, things like this, right?", "tokens": [51060, 420, 286, 362, 257, 24523, 87, 11, 420, 286, 362, 257, 31474, 11684, 8149, 4813, 11, 721, 411, 341, 11, 558, 30, 51416], "temperature": 0.0, "avg_logprob": -0.09929652724947248, "compression_ratio": 1.7745901639344261, "no_speech_prob": 0.027581389993429184}, {"id": 651, "seek": 328264, "start": 3303.68, "end": 3307.7599999999998, "text": " So these types, you can't necessarily copy. Sometimes you can't necessarily even move them", "tokens": [51416, 407, 613, 3467, 11, 291, 393, 380, 4725, 5055, 13, 4803, 291, 393, 380, 4725, 754, 1286, 552, 51620], "temperature": 0.0, "avg_logprob": -0.09929652724947248, "compression_ratio": 1.7745901639344261, "no_speech_prob": 0.027581389993429184}, {"id": 652, "seek": 330776, "start": 3307.76, "end": 3312.7200000000003, "text": " to a different address. And so what Mojo allows you to do is it allows you to express,", "tokens": [50364, 281, 257, 819, 2985, 13, 400, 370, 437, 3335, 5134, 4045, 291, 281, 360, 307, 309, 4045, 291, 281, 5109, 11, 50612], "temperature": 0.0, "avg_logprob": -0.11896252435100965, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.03845132142305374}, {"id": 653, "seek": 330776, "start": 3312.7200000000003, "end": 3317.44, "text": " hey, I don't want to get a copy of this thing. I want to actually just get a reference to it.", "tokens": [50612, 4177, 11, 286, 500, 380, 528, 281, 483, 257, 5055, 295, 341, 551, 13, 286, 528, 281, 767, 445, 483, 257, 6408, 281, 309, 13, 50848], "temperature": 0.0, "avg_logprob": -0.11896252435100965, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.03845132142305374}, {"id": 654, "seek": 330776, "start": 3318.0800000000004, "end": 3322.0, "text": " And by doing that, well, you can say, as you can say, okay, if I'm defining something weird,", "tokens": [50880, 400, 538, 884, 300, 11, 731, 11, 291, 393, 584, 11, 382, 291, 393, 584, 11, 1392, 11, 498, 286, 478, 17827, 746, 3657, 11, 51076], "temperature": 0.0, "avg_logprob": -0.11896252435100965, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.03845132142305374}, {"id": 655, "seek": 330776, "start": 3322.0, "end": 3329.44, "text": " like a atomic number or something, it has to be, so an atomic number is an area in memory", "tokens": [51076, 411, 257, 22275, 1230, 420, 746, 11, 309, 575, 281, 312, 11, 370, 364, 22275, 1230, 307, 364, 1859, 294, 4675, 51448], "temperature": 0.0, "avg_logprob": -0.11896252435100965, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.03845132142305374}, {"id": 656, "seek": 330776, "start": 3329.44, "end": 3337.2000000000003, "text": " that multiple threads can access at a time without locks. And so the definition of atomic", "tokens": [51448, 300, 3866, 19314, 393, 2105, 412, 257, 565, 1553, 20703, 13, 400, 370, 264, 7123, 295, 22275, 51836], "temperature": 0.0, "avg_logprob": -0.11896252435100965, "compression_ratio": 1.7626459143968871, "no_speech_prob": 0.03845132142305374}, {"id": 657, "seek": 333720, "start": 3337.2, "end": 3340.72, "text": " numbers, multiple different things have to be poking it. Therefore, they have to agree on where", "tokens": [50364, 3547, 11, 3866, 819, 721, 362, 281, 312, 42684, 309, 13, 7504, 11, 436, 362, 281, 3986, 322, 689, 50540], "temperature": 0.0, "avg_logprob": -0.13470356750488283, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.002396296476945281}, {"id": 658, "seek": 333720, "start": 3340.72, "end": 3345.68, "text": " it is. And so you can't just move it out from underneath one because it kind of breaks what", "tokens": [50540, 309, 307, 13, 400, 370, 291, 393, 380, 445, 1286, 309, 484, 490, 7223, 472, 570, 309, 733, 295, 9857, 437, 50788], "temperature": 0.0, "avg_logprob": -0.13470356750488283, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.002396296476945281}, {"id": 659, "seek": 333720, "start": 3345.68, "end": 3351.3599999999997, "text": " it means. And so that's an example of a type that you can't copy, you can't move. Once you create,", "tokens": [50788, 309, 1355, 13, 400, 370, 300, 311, 364, 1365, 295, 257, 2010, 300, 291, 393, 380, 5055, 11, 291, 393, 380, 1286, 13, 3443, 291, 1884, 11, 51072], "temperature": 0.0, "avg_logprob": -0.13470356750488283, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.002396296476945281}, {"id": 660, "seek": 333720, "start": 3351.3599999999997, "end": 3357.52, "text": " it has to be where it was. Now, if you look at many other examples, like a database handle,", "tokens": [51072, 309, 575, 281, 312, 689, 309, 390, 13, 823, 11, 498, 291, 574, 412, 867, 661, 5110, 11, 411, 257, 8149, 4813, 11, 51380], "temperature": 0.0, "avg_logprob": -0.13470356750488283, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.002396296476945281}, {"id": 661, "seek": 333720, "start": 3357.52, "end": 3362.8799999999997, "text": " so okay, well, what happens, how do you copy a database handle? Do you copy the whole database?", "tokens": [51380, 370, 1392, 11, 731, 11, 437, 2314, 11, 577, 360, 291, 5055, 257, 8149, 4813, 30, 1144, 291, 5055, 264, 1379, 8149, 30, 51648], "temperature": 0.0, "avg_logprob": -0.13470356750488283, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.002396296476945281}, {"id": 662, "seek": 336288, "start": 3362.96, "end": 3368.4, "text": " That's not something you necessarily want to do. There's a lot of types like that", "tokens": [50368, 663, 311, 406, 746, 291, 4725, 528, 281, 360, 13, 821, 311, 257, 688, 295, 3467, 411, 300, 50640], "temperature": 0.0, "avg_logprob": -0.07266687601804733, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.02675016224384308}, {"id": 663, "seek": 336288, "start": 3368.4, "end": 3374.0, "text": " where you want to be able to say that they are uniquely owned. So there's always one of this", "tokens": [50640, 689, 291, 528, 281, 312, 1075, 281, 584, 300, 436, 366, 31474, 11684, 13, 407, 456, 311, 1009, 472, 295, 341, 50920], "temperature": 0.0, "avg_logprob": -0.07266687601804733, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.02675016224384308}, {"id": 664, "seek": 336288, "start": 3374.0, "end": 3381.2000000000003, "text": " thing. And or if I create a thing, I don't copy it. And so what Mojo allows you to do is it allows", "tokens": [50920, 551, 13, 400, 420, 498, 286, 1884, 257, 551, 11, 286, 500, 380, 5055, 309, 13, 400, 370, 437, 3335, 5134, 4045, 291, 281, 360, 307, 309, 4045, 51280], "temperature": 0.0, "avg_logprob": -0.07266687601804733, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.02675016224384308}, {"id": 665, "seek": 336288, "start": 3381.2000000000003, "end": 3385.44, "text": " you to say, hey, I want to pass around a reference to this thing without copying it. And so it has", "tokens": [51280, 291, 281, 584, 11, 4177, 11, 286, 528, 281, 1320, 926, 257, 6408, 281, 341, 551, 1553, 27976, 309, 13, 400, 370, 309, 575, 51492], "temperature": 0.0, "avg_logprob": -0.07266687601804733, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.02675016224384308}, {"id": 666, "seek": 336288, "start": 3386.0, "end": 3391.76, "text": " borrowed conventions. So you can say, you can use it, but you don't get to change it. You can pass", "tokens": [51520, 26805, 33520, 13, 407, 291, 393, 584, 11, 291, 393, 764, 309, 11, 457, 291, 500, 380, 483, 281, 1319, 309, 13, 509, 393, 1320, 51808], "temperature": 0.0, "avg_logprob": -0.07266687601804733, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.02675016224384308}, {"id": 667, "seek": 339176, "start": 3391.84, "end": 3396.32, "text": " it by mutable reference. And so if you do that, then you can, you get a reference to it, but", "tokens": [50368, 309, 538, 5839, 712, 6408, 13, 400, 370, 498, 291, 360, 300, 11, 550, 291, 393, 11, 291, 483, 257, 6408, 281, 309, 11, 457, 50592], "temperature": 0.0, "avg_logprob": -0.16431316407788701, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.003943900112062693}, {"id": 668, "seek": 339176, "start": 3396.32, "end": 3402.0800000000004, "text": " you can change it. And so it manages all that kind of stuff. So it's just a really nice implementation", "tokens": [50592, 291, 393, 1319, 309, 13, 400, 370, 309, 22489, 439, 300, 733, 295, 1507, 13, 407, 309, 311, 445, 257, 534, 1481, 11420, 50880], "temperature": 0.0, "avg_logprob": -0.16431316407788701, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.003943900112062693}, {"id": 669, "seek": 339176, "start": 3402.0800000000004, "end": 3410.0800000000004, "text": " of like C++ has, you know, the different kinds of pointers, different kinds of applications,", "tokens": [50880, 295, 411, 383, 25472, 575, 11, 291, 458, 11, 264, 819, 3685, 295, 44548, 11, 819, 3685, 295, 5821, 11, 51280], "temperature": 0.0, "avg_logprob": -0.16431316407788701, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.003943900112062693}, {"id": 670, "seek": 339176, "start": 3410.0800000000004, "end": 3414.88, "text": " smart pointers that you can explicitly define this logic. But you're saying that's more like", "tokens": [51280, 4069, 44548, 300, 291, 393, 20803, 6964, 341, 9952, 13, 583, 291, 434, 1566, 300, 311, 544, 411, 51520], "temperature": 0.0, "avg_logprob": -0.16431316407788701, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.003943900112062693}, {"id": 671, "seek": 339176, "start": 3415.6000000000004, "end": 3420.0, "text": " the weird case versus the common case. Well, it depends on where, I mean, I mean,", "tokens": [51556, 264, 3657, 1389, 5717, 264, 2689, 1389, 13, 1042, 11, 309, 5946, 322, 689, 11, 286, 914, 11, 286, 914, 11, 51776], "temperature": 0.0, "avg_logprob": -0.16431316407788701, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.003943900112062693}, {"id": 672, "seek": 342000, "start": 3420.48, "end": 3423.76, "text": " I don't think I'm a normal person. So I mean, I'm not one to call other people weird.", "tokens": [50388, 286, 500, 380, 519, 286, 478, 257, 2710, 954, 13, 407, 286, 914, 11, 286, 478, 406, 472, 281, 818, 661, 561, 3657, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11366242832607693, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0018672009464353323}, {"id": 673, "seek": 342000, "start": 3425.28, "end": 3430.64, "text": " But the, but, you know, if you talk to a normal Python, a typical Python programmer,", "tokens": [50628, 583, 264, 11, 457, 11, 291, 458, 11, 498, 291, 751, 281, 257, 2710, 15329, 11, 257, 7476, 15329, 32116, 11, 50896], "temperature": 0.0, "avg_logprob": -0.11366242832607693, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0018672009464353323}, {"id": 674, "seek": 342000, "start": 3430.64, "end": 3434.0, "text": " you're typically not thinking about this, right? This is a lower level of abstraction. Now,", "tokens": [50896, 291, 434, 5850, 406, 1953, 466, 341, 11, 558, 30, 639, 307, 257, 3126, 1496, 295, 37765, 13, 823, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11366242832607693, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0018672009464353323}, {"id": 675, "seek": 342000, "start": 3434.0, "end": 3438.24, "text": " if you talk to a C++ programmer, certainly if you talk to a Rust programmer, again, they're", "tokens": [51064, 498, 291, 751, 281, 257, 383, 25472, 32116, 11, 3297, 498, 291, 751, 281, 257, 34952, 32116, 11, 797, 11, 436, 434, 51276], "temperature": 0.0, "avg_logprob": -0.11366242832607693, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0018672009464353323}, {"id": 676, "seek": 342000, "start": 3438.24, "end": 3443.2, "text": " not weird, they're delightful, like these are all good people, right? Those folks will think about", "tokens": [51276, 406, 3657, 11, 436, 434, 35194, 11, 411, 613, 366, 439, 665, 561, 11, 558, 30, 3950, 4024, 486, 519, 466, 51524], "temperature": 0.0, "avg_logprob": -0.11366242832607693, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0018672009464353323}, {"id": 677, "seek": 342000, "start": 3443.2, "end": 3448.64, "text": " all the time. Right. And so I look at this as there's a spectrum between very deep low level", "tokens": [51524, 439, 264, 565, 13, 1779, 13, 400, 370, 286, 574, 412, 341, 382, 456, 311, 257, 11143, 1296, 588, 2452, 2295, 1496, 51796], "temperature": 0.0, "avg_logprob": -0.11366242832607693, "compression_ratio": 1.813953488372093, "no_speech_prob": 0.0018672009464353323}, {"id": 678, "seek": 344864, "start": 3448.64, "end": 3452.4, "text": " systems. I'm going to go poke the bits and care about how they're laid out in memory all the", "tokens": [50364, 3652, 13, 286, 478, 516, 281, 352, 19712, 264, 9239, 293, 1127, 466, 577, 436, 434, 9897, 484, 294, 4675, 439, 264, 50552], "temperature": 0.0, "avg_logprob": -0.08683653800718245, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.0012446235632523894}, {"id": 679, "seek": 344864, "start": 3452.4, "end": 3457.12, "text": " way up to application and scripting and other things like this. And so it's not that anybody's", "tokens": [50552, 636, 493, 281, 3861, 293, 5755, 278, 293, 661, 721, 411, 341, 13, 400, 370, 309, 311, 406, 300, 4472, 311, 50788], "temperature": 0.0, "avg_logprob": -0.08683653800718245, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.0012446235632523894}, {"id": 680, "seek": 344864, "start": 3457.12, "end": 3463.68, "text": " right or wrong. It's about how do we build one system that scales? By the way, the idea of an", "tokens": [50788, 558, 420, 2085, 13, 467, 311, 466, 577, 360, 321, 1322, 472, 1185, 300, 17408, 30, 3146, 264, 636, 11, 264, 1558, 295, 364, 51116], "temperature": 0.0, "avg_logprob": -0.08683653800718245, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.0012446235632523894}, {"id": 681, "seek": 344864, "start": 3463.68, "end": 3471.68, "text": " atomic number has been something that always brought me deep happiness. Because the flip side", "tokens": [51116, 22275, 1230, 575, 668, 746, 300, 1009, 3038, 385, 2452, 8324, 13, 1436, 264, 7929, 1252, 51516], "temperature": 0.0, "avg_logprob": -0.08683653800718245, "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.0012446235632523894}, {"id": 682, "seek": 347168, "start": 3471.68, "end": 3481.8399999999997, "text": " of that, the idea that threads can just modify stuff asynchronously, just the whole idea of", "tokens": [50364, 295, 300, 11, 264, 1558, 300, 19314, 393, 445, 16927, 1507, 42642, 5098, 11, 445, 264, 1379, 1558, 295, 50872], "temperature": 0.0, "avg_logprob": -0.0952954914258874, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.01691039279103279}, {"id": 683, "seek": 347168, "start": 3481.8399999999997, "end": 3486.64, "text": " concurrent programming is a source of infinite stress for me. Well, so this is where you jump into,", "tokens": [50872, 37702, 9410, 307, 257, 4009, 295, 13785, 4244, 337, 385, 13, 1042, 11, 370, 341, 307, 689, 291, 3012, 666, 11, 51112], "temperature": 0.0, "avg_logprob": -0.0952954914258874, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.01691039279103279}, {"id": 684, "seek": 347168, "start": 3488.64, "end": 3492.16, "text": " you know, again, you zoom out and get out of programming languages or compilers and you just", "tokens": [51212, 291, 458, 11, 797, 11, 291, 8863, 484, 293, 483, 484, 295, 9410, 8650, 420, 715, 388, 433, 293, 291, 445, 51388], "temperature": 0.0, "avg_logprob": -0.0952954914258874, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.01691039279103279}, {"id": 685, "seek": 347168, "start": 3492.16, "end": 3497.6, "text": " look at what the industry has done. My mind is constantly blown by this, right? And you look", "tokens": [51388, 574, 412, 437, 264, 3518, 575, 1096, 13, 1222, 1575, 307, 6460, 16479, 538, 341, 11, 558, 30, 400, 291, 574, 51660], "temperature": 0.0, "avg_logprob": -0.0952954914258874, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.01691039279103279}, {"id": 686, "seek": 349760, "start": 3497.6, "end": 3502.7999999999997, "text": " at what, you know, Moore's law, Moore's law has this idea that like computers for a long time,", "tokens": [50364, 412, 437, 11, 291, 458, 11, 21644, 311, 2101, 11, 21644, 311, 2101, 575, 341, 1558, 300, 411, 10807, 337, 257, 938, 565, 11, 50624], "temperature": 0.0, "avg_logprob": -0.13114543144519514, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.004754279740154743}, {"id": 687, "seek": 349760, "start": 3503.36, "end": 3506.4, "text": " single thread performance has got faster and faster and faster and faster for free.", "tokens": [50652, 2167, 7207, 3389, 575, 658, 4663, 293, 4663, 293, 4663, 293, 4663, 337, 1737, 13, 50804], "temperature": 0.0, "avg_logprob": -0.13114543144519514, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.004754279740154743}, {"id": 688, "seek": 349760, "start": 3507.04, "end": 3512.4, "text": " But then physics and other things intervened and power consumption, like other things started", "tokens": [50836, 583, 550, 10649, 293, 661, 721, 17104, 292, 293, 1347, 12126, 11, 411, 661, 721, 1409, 51104], "temperature": 0.0, "avg_logprob": -0.13114543144519514, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.004754279740154743}, {"id": 689, "seek": 349760, "start": 3512.4, "end": 3517.2799999999997, "text": " to matter. And so what ended up happening is we went from single core computers to multi core,", "tokens": [51104, 281, 1871, 13, 400, 370, 437, 4590, 493, 2737, 307, 321, 1437, 490, 2167, 4965, 10807, 281, 4825, 4965, 11, 51348], "temperature": 0.0, "avg_logprob": -0.13114543144519514, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.004754279740154743}, {"id": 690, "seek": 349760, "start": 3517.2799999999997, "end": 3521.68, "text": " then we went to accelerators, right? And this trend towards specialization of hardware", "tokens": [51348, 550, 321, 1437, 281, 10172, 3391, 11, 558, 30, 400, 341, 6028, 3030, 2121, 2144, 295, 8837, 51568], "temperature": 0.0, "avg_logprob": -0.13114543144519514, "compression_ratio": 1.7874015748031495, "no_speech_prob": 0.004754279740154743}, {"id": 691, "seek": 352168, "start": 3521.68, "end": 3528.48, "text": " is only going to continue. And so for years, us programming language nerds and compiler people", "tokens": [50364, 307, 787, 516, 281, 2354, 13, 400, 370, 337, 924, 11, 505, 9410, 2856, 23229, 82, 293, 31958, 561, 50704], "temperature": 0.0, "avg_logprob": -0.13576151040884166, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.10663365572690964}, {"id": 692, "seek": 352168, "start": 3528.48, "end": 3532.64, "text": " have been saying, okay, well, how do we tackle multi core, right? For a while, it was like,", "tokens": [50704, 362, 668, 1566, 11, 1392, 11, 731, 11, 577, 360, 321, 14896, 4825, 4965, 11, 558, 30, 1171, 257, 1339, 11, 309, 390, 411, 11, 50912], "temperature": 0.0, "avg_logprob": -0.13576151040884166, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.10663365572690964}, {"id": 693, "seek": 352168, "start": 3532.64, "end": 3536.72, "text": " multi core is the future, we have to get on top of this thing. And then it was multi cores to default.", "tokens": [50912, 4825, 4965, 307, 264, 2027, 11, 321, 362, 281, 483, 322, 1192, 295, 341, 551, 13, 400, 550, 309, 390, 4825, 24826, 281, 7576, 13, 51116], "temperature": 0.0, "avg_logprob": -0.13576151040884166, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.10663365572690964}, {"id": 694, "seek": 352168, "start": 3536.72, "end": 3540.72, "text": " What are we doing with this thing? And then it's like, there's chips with hundreds of cores in them.", "tokens": [51116, 708, 366, 321, 884, 365, 341, 551, 30, 400, 550, 309, 311, 411, 11, 456, 311, 11583, 365, 6779, 295, 24826, 294, 552, 13, 51316], "temperature": 0.0, "avg_logprob": -0.13576151040884166, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.10663365572690964}, {"id": 695, "seek": 352168, "start": 3541.52, "end": 3547.68, "text": " What happened, right? And so I'm super inspired by the fact that, you know, in the face of this,", "tokens": [51356, 708, 2011, 11, 558, 30, 400, 370, 286, 478, 1687, 7547, 538, 264, 1186, 300, 11, 291, 458, 11, 294, 264, 1851, 295, 341, 11, 51664], "temperature": 0.0, "avg_logprob": -0.13576151040884166, "compression_ratio": 1.7644927536231885, "no_speech_prob": 0.10663365572690964}, {"id": 696, "seek": 354768, "start": 3548.16, "end": 3553.6, "text": " you know, those machine learning people invented this idea of a tensor, right? And what is a tensor?", "tokens": [50388, 291, 458, 11, 729, 3479, 2539, 561, 14479, 341, 1558, 295, 257, 40863, 11, 558, 30, 400, 437, 307, 257, 40863, 30, 50660], "temperature": 0.0, "avg_logprob": -0.13695282685129265, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.006902446039021015}, {"id": 697, "seek": 354768, "start": 3553.6, "end": 3560.16, "text": " A tensor isn't like an arithmetic and algebraic concept, it's like an abstraction around a", "tokens": [50660, 316, 40863, 1943, 380, 411, 364, 42973, 293, 21989, 299, 3410, 11, 309, 311, 411, 364, 37765, 926, 257, 50988], "temperature": 0.0, "avg_logprob": -0.13695282685129265, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.006902446039021015}, {"id": 698, "seek": 354768, "start": 3560.16, "end": 3565.44, "text": " gigantic parallelizable data set, right? And because of that, and because of things like", "tokens": [50988, 26800, 8952, 22395, 1412, 992, 11, 558, 30, 400, 570, 295, 300, 11, 293, 570, 295, 721, 411, 51252], "temperature": 0.0, "avg_logprob": -0.13695282685129265, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.006902446039021015}, {"id": 699, "seek": 354768, "start": 3565.44, "end": 3570.7999999999997, "text": " TensorFlow and PyTorch, we're able to say, okay, we'll express the math of the system.", "tokens": [51252, 37624, 293, 9953, 51, 284, 339, 11, 321, 434, 1075, 281, 584, 11, 1392, 11, 321, 603, 5109, 264, 5221, 295, 264, 1185, 13, 51520], "temperature": 0.0, "avg_logprob": -0.13695282685129265, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.006902446039021015}, {"id": 700, "seek": 354768, "start": 3571.3599999999997, "end": 3575.12, "text": " This enables you to do automatic differentiations, enables you to do like all these cool things.", "tokens": [51548, 639, 17077, 291, 281, 360, 12509, 27372, 763, 11, 17077, 291, 281, 360, 411, 439, 613, 1627, 721, 13, 51736], "temperature": 0.0, "avg_logprob": -0.13695282685129265, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.006902446039021015}, {"id": 701, "seek": 357512, "start": 3575.12, "end": 3581.3599999999997, "text": " And it's an abstract representation. Because you have that abstract representation,", "tokens": [50364, 400, 309, 311, 364, 12649, 10290, 13, 1436, 291, 362, 300, 12649, 10290, 11, 50676], "temperature": 0.0, "avg_logprob": -0.12264161951401654, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.0016481931088492274}, {"id": 702, "seek": 357512, "start": 3581.3599999999997, "end": 3586.96, "text": " you can now map it onto these parallel machines without having to control, okay, put that right", "tokens": [50676, 291, 393, 586, 4471, 309, 3911, 613, 8952, 8379, 1553, 1419, 281, 1969, 11, 1392, 11, 829, 300, 558, 50956], "temperature": 0.0, "avg_logprob": -0.12264161951401654, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.0016481931088492274}, {"id": 703, "seek": 357512, "start": 3586.96, "end": 3591.68, "text": " here, put that right there, put that right there. And this has enabled an explosion in terms of AI,", "tokens": [50956, 510, 11, 829, 300, 558, 456, 11, 829, 300, 558, 456, 13, 400, 341, 575, 15172, 364, 15673, 294, 2115, 295, 7318, 11, 51192], "temperature": 0.0, "avg_logprob": -0.12264161951401654, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.0016481931088492274}, {"id": 704, "seek": 357512, "start": 3591.68, "end": 3596.24, "text": " compute, accelerators, like all the stuff. And so that's super, super excited.", "tokens": [51192, 14722, 11, 10172, 3391, 11, 411, 439, 264, 1507, 13, 400, 370, 300, 311, 1687, 11, 1687, 2919, 13, 51420], "temperature": 0.0, "avg_logprob": -0.12264161951401654, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.0016481931088492274}, {"id": 705, "seek": 357512, "start": 3596.24, "end": 3603.04, "text": " What about the deployment, the execution across multiple machines? So you write that", "tokens": [51420, 708, 466, 264, 19317, 11, 264, 15058, 2108, 3866, 8379, 30, 407, 291, 2464, 300, 51760], "temperature": 0.0, "avg_logprob": -0.12264161951401654, "compression_ratio": 1.7649402390438247, "no_speech_prob": 0.0016481931088492274}, {"id": 706, "seek": 360304, "start": 3603.04, "end": 3608.24, "text": " the modular compute platform dynamically partitions models with billions of parameters", "tokens": [50364, 264, 31111, 14722, 3663, 43492, 644, 2451, 5245, 365, 17375, 295, 9834, 50624], "temperature": 0.0, "avg_logprob": -0.09148192405700684, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.002148599363863468}, {"id": 707, "seek": 360304, "start": 3608.24, "end": 3614.4, "text": " and distributes their execution across multiple machines, enabling unparalleled efficiency.", "tokens": [50624, 293, 4400, 1819, 641, 15058, 2108, 3866, 8379, 11, 23148, 517, 2181, 336, 31689, 10493, 13, 50932], "temperature": 0.0, "avg_logprob": -0.09148192405700684, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.002148599363863468}, {"id": 708, "seek": 360304, "start": 3615.36, "end": 3620.24, "text": " By the way, the use of unparalleled in that sentence, anyway, enabling unparalleled efficiency", "tokens": [50980, 3146, 264, 636, 11, 264, 764, 295, 517, 2181, 336, 31689, 294, 300, 8174, 11, 4033, 11, 23148, 517, 2181, 336, 31689, 10493, 51224], "temperature": 0.0, "avg_logprob": -0.09148192405700684, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.002148599363863468}, {"id": 709, "seek": 360304, "start": 3620.24, "end": 3628.88, "text": " scale and reliability for the largest workloads. So how do you do this abstraction of distributed", "tokens": [51224, 4373, 293, 24550, 337, 264, 6443, 32452, 13, 407, 577, 360, 291, 360, 341, 37765, 295, 12631, 51656], "temperature": 0.0, "avg_logprob": -0.09148192405700684, "compression_ratio": 1.701834862385321, "no_speech_prob": 0.002148599363863468}, {"id": 710, "seek": 362888, "start": 3628.88, "end": 3633.04, "text": " deployment of large models? Yeah, so one of the really interesting", "tokens": [50364, 19317, 295, 2416, 5245, 30, 865, 11, 370, 472, 295, 264, 534, 1880, 50572], "temperature": 0.0, "avg_logprob": -0.15702896118164061, "compression_ratio": 1.5811320754716982, "no_speech_prob": 0.03256784379482269}, {"id": 711, "seek": 362888, "start": 3633.92, "end": 3637.92, "text": " tensions. So there's a whole bunch of stuff that goes into that. I'll pick a random walkthrough.", "tokens": [50616, 28303, 13, 407, 456, 311, 257, 1379, 3840, 295, 1507, 300, 1709, 666, 300, 13, 286, 603, 1888, 257, 4974, 1792, 11529, 13, 50816], "temperature": 0.0, "avg_logprob": -0.15702896118164061, "compression_ratio": 1.5811320754716982, "no_speech_prob": 0.03256784379482269}, {"id": 712, "seek": 362888, "start": 3638.8, "end": 3643.12, "text": " If you go back and replay the history of machine learning, right? I mean, the brief,", "tokens": [50860, 759, 291, 352, 646, 293, 23836, 264, 2503, 295, 3479, 2539, 11, 558, 30, 286, 914, 11, 264, 5353, 11, 51076], "temperature": 0.0, "avg_logprob": -0.15702896118164061, "compression_ratio": 1.5811320754716982, "no_speech_prob": 0.03256784379482269}, {"id": 713, "seek": 362888, "start": 3643.12, "end": 3646.48, "text": " the most recent history of machine learning, because this is, as you know, very deep.", "tokens": [51076, 264, 881, 5162, 2503, 295, 3479, 2539, 11, 570, 341, 307, 11, 382, 291, 458, 11, 588, 2452, 13, 51244], "temperature": 0.0, "avg_logprob": -0.15702896118164061, "compression_ratio": 1.5811320754716982, "no_speech_prob": 0.03256784379482269}, {"id": 714, "seek": 362888, "start": 3648.1600000000003, "end": 3657.52, "text": " I knew Lex when he had an AI podcast. So if you look at just TensorFlow and PyTorch,", "tokens": [51328, 286, 2586, 24086, 562, 415, 632, 364, 7318, 7367, 13, 407, 498, 291, 574, 412, 445, 37624, 293, 9953, 51, 284, 339, 11, 51796], "temperature": 0.0, "avg_logprob": -0.15702896118164061, "compression_ratio": 1.5811320754716982, "no_speech_prob": 0.03256784379482269}, {"id": 715, "seek": 365752, "start": 3657.52, "end": 3662.32, "text": " which is pretty recent history in the big picture, right? But TensorFlow is all about graphs.", "tokens": [50364, 597, 307, 1238, 5162, 2503, 294, 264, 955, 3036, 11, 558, 30, 583, 37624, 307, 439, 466, 24877, 13, 50604], "temperature": 0.0, "avg_logprob": -0.11054127957640576, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0029800059273838997}, {"id": 716, "seek": 365752, "start": 3662.96, "end": 3668.08, "text": " PyTorch, I think, pretty unarguably ended up winning. And why did it win? Mostly because", "tokens": [50636, 9953, 51, 284, 339, 11, 286, 519, 11, 1238, 517, 289, 2794, 1188, 4590, 493, 8224, 13, 400, 983, 630, 309, 1942, 30, 29035, 570, 50892], "temperature": 0.0, "avg_logprob": -0.11054127957640576, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0029800059273838997}, {"id": 717, "seek": 365752, "start": 3668.08, "end": 3672.4, "text": " of usability, right? And the usability of PyTorch is, I think, huge. And I think, again,", "tokens": [50892, 295, 46878, 11, 558, 30, 400, 264, 46878, 295, 9953, 51, 284, 339, 307, 11, 286, 519, 11, 2603, 13, 400, 286, 519, 11, 797, 11, 51108], "temperature": 0.0, "avg_logprob": -0.11054127957640576, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0029800059273838997}, {"id": 718, "seek": 365752, "start": 3672.4, "end": 3678.56, "text": " that's a huge testament to the power of taking abstract theoretical technical concepts and bring", "tokens": [51108, 300, 311, 257, 2603, 35499, 281, 264, 1347, 295, 1940, 12649, 20864, 6191, 10392, 293, 1565, 51416], "temperature": 0.0, "avg_logprob": -0.11054127957640576, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0029800059273838997}, {"id": 719, "seek": 365752, "start": 3678.56, "end": 3684.4, "text": " it to the masses, right? Now, the challenge with what the TensorFlow versus the PyTorch", "tokens": [51416, 309, 281, 264, 23935, 11, 558, 30, 823, 11, 264, 3430, 365, 437, 264, 37624, 5717, 264, 9953, 51, 284, 339, 51708], "temperature": 0.0, "avg_logprob": -0.11054127957640576, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.0029800059273838997}, {"id": 720, "seek": 368440, "start": 3684.56, "end": 3690.4, "text": " design points was that TensorFlow is kind of difficult to use for researchers, but it was", "tokens": [50372, 1715, 2793, 390, 300, 37624, 307, 733, 295, 2252, 281, 764, 337, 10309, 11, 457, 309, 390, 50664], "temperature": 0.0, "avg_logprob": -0.09658551216125488, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0006878015701659024}, {"id": 721, "seek": 368440, "start": 3690.4, "end": 3694.32, "text": " actually pretty good for deployment. PyTorch is really good for researchers. It kind of", "tokens": [50664, 767, 1238, 665, 337, 19317, 13, 9953, 51, 284, 339, 307, 534, 665, 337, 10309, 13, 467, 733, 295, 50860], "temperature": 0.0, "avg_logprob": -0.09658551216125488, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0006878015701659024}, {"id": 722, "seek": 368440, "start": 3694.32, "end": 3700.08, "text": " not super great for deployment, right? And so I think that we as an industry have been struggling.", "tokens": [50860, 406, 1687, 869, 337, 19317, 11, 558, 30, 400, 370, 286, 519, 300, 321, 382, 364, 3518, 362, 668, 9314, 13, 51148], "temperature": 0.0, "avg_logprob": -0.09658551216125488, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0006878015701659024}, {"id": 723, "seek": 368440, "start": 3700.08, "end": 3704.56, "text": " And if you look at what deploying a machine learning model today means is that you'll have", "tokens": [51148, 400, 498, 291, 574, 412, 437, 34198, 257, 3479, 2539, 2316, 965, 1355, 307, 300, 291, 603, 362, 51372], "temperature": 0.0, "avg_logprob": -0.09658551216125488, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0006878015701659024}, {"id": 724, "seek": 368440, "start": 3704.56, "end": 3710.1600000000003, "text": " researchers who are, I mean, wicked smart, of course, but they're wicked smart at model architecture", "tokens": [51372, 10309, 567, 366, 11, 286, 914, 11, 22663, 4069, 11, 295, 1164, 11, 457, 436, 434, 22663, 4069, 412, 2316, 9482, 51652], "temperature": 0.0, "avg_logprob": -0.09658551216125488, "compression_ratio": 1.7269372693726937, "no_speech_prob": 0.0006878015701659024}, {"id": 725, "seek": 371016, "start": 3710.16, "end": 3716.3999999999996, "text": " and data and calculus. They're wicked smart in various domains. They don't want to know anything", "tokens": [50364, 293, 1412, 293, 33400, 13, 814, 434, 22663, 4069, 294, 3683, 25514, 13, 814, 500, 380, 528, 281, 458, 1340, 50676], "temperature": 0.0, "avg_logprob": -0.11931019923726066, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002889380557462573}, {"id": 726, "seek": 371016, "start": 3716.3999999999996, "end": 3720.7999999999997, "text": " about the hardware or deployment or C++ or things like this, right? And so what's happened is you", "tokens": [50676, 466, 264, 8837, 420, 19317, 420, 383, 25472, 420, 721, 411, 341, 11, 558, 30, 400, 370, 437, 311, 2011, 307, 291, 50896], "temperature": 0.0, "avg_logprob": -0.11931019923726066, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002889380557462573}, {"id": 727, "seek": 371016, "start": 3720.7999999999997, "end": 3725.2, "text": " get people who train the model, they throw it over the fence, and then you have people that", "tokens": [50896, 483, 561, 567, 3847, 264, 2316, 11, 436, 3507, 309, 670, 264, 15422, 11, 293, 550, 291, 362, 561, 300, 51116], "temperature": 0.0, "avg_logprob": -0.11931019923726066, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002889380557462573}, {"id": 728, "seek": 371016, "start": 3725.2, "end": 3732.64, "text": " try to deploy the model. Well, every time you have a team A does X, they throw it over the fence,", "tokens": [51116, 853, 281, 7274, 264, 2316, 13, 1042, 11, 633, 565, 291, 362, 257, 1469, 316, 775, 1783, 11, 436, 3507, 309, 670, 264, 15422, 11, 51488], "temperature": 0.0, "avg_logprob": -0.11931019923726066, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002889380557462573}, {"id": 729, "seek": 371016, "start": 3732.64, "end": 3739.8399999999997, "text": " and team B does Y, you have a problem. Because, of course, it never works the first time.", "tokens": [51488, 293, 1469, 363, 775, 398, 11, 291, 362, 257, 1154, 13, 1436, 11, 295, 1164, 11, 309, 1128, 1985, 264, 700, 565, 13, 51848], "temperature": 0.0, "avg_logprob": -0.11931019923726066, "compression_ratio": 1.7490774907749078, "no_speech_prob": 0.002889380557462573}, {"id": 730, "seek": 373984, "start": 3740.0, "end": 3744.0, "text": " And so you throw it over the fence, they figure out, okay, it's too slow, won't fit,", "tokens": [50372, 400, 370, 291, 3507, 309, 670, 264, 15422, 11, 436, 2573, 484, 11, 1392, 11, 309, 311, 886, 2964, 11, 1582, 380, 3318, 11, 50572], "temperature": 0.0, "avg_logprob": -0.089030809638914, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0005702461930923164}, {"id": 731, "seek": 373984, "start": 3744.56, "end": 3750.8, "text": " doesn't use the right operator, the tool crashes, whatever the problem is, then they have to throw", "tokens": [50600, 1177, 380, 764, 264, 558, 12973, 11, 264, 2290, 28642, 11, 2035, 264, 1154, 307, 11, 550, 436, 362, 281, 3507, 50912], "temperature": 0.0, "avg_logprob": -0.089030809638914, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0005702461930923164}, {"id": 732, "seek": 373984, "start": 3750.8, "end": 3755.52, "text": " it back over the fence. And every time you throw a thing over a fence, it takes three weeks of", "tokens": [50912, 309, 646, 670, 264, 15422, 13, 400, 633, 565, 291, 3507, 257, 551, 670, 257, 15422, 11, 309, 2516, 1045, 3259, 295, 51148], "temperature": 0.0, "avg_logprob": -0.089030809638914, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0005702461930923164}, {"id": 733, "seek": 373984, "start": 3755.52, "end": 3760.4, "text": " project managers and meetings and things like this. And so what we've seen today is that getting", "tokens": [51148, 1716, 14084, 293, 8410, 293, 721, 411, 341, 13, 400, 370, 437, 321, 600, 1612, 965, 307, 300, 1242, 51392], "temperature": 0.0, "avg_logprob": -0.089030809638914, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0005702461930923164}, {"id": 734, "seek": 373984, "start": 3760.4, "end": 3765.28, "text": " models in production can take weeks or months. Like it's not atypical, right? I talked to lots", "tokens": [51392, 5245, 294, 4265, 393, 747, 3259, 420, 2493, 13, 1743, 309, 311, 406, 412, 88, 34061, 11, 558, 30, 286, 2825, 281, 3195, 51636], "temperature": 0.0, "avg_logprob": -0.089030809638914, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.0005702461930923164}, {"id": 735, "seek": 376528, "start": 3765.28, "end": 3770.48, "text": " of people and you talk about like VP of software at some internet company trying to deploy a model,", "tokens": [50364, 295, 561, 293, 291, 751, 466, 411, 35812, 295, 4722, 412, 512, 4705, 2237, 1382, 281, 7274, 257, 2316, 11, 50624], "temperature": 0.0, "avg_logprob": -0.10529045592573352, "compression_ratio": 1.664406779661017, "no_speech_prob": 0.34133294224739075}, {"id": 736, "seek": 376528, "start": 3770.48, "end": 3776.48, "text": " and they're like, why do I need a team of 45 people? It's so easy to train a model, why can't I", "tokens": [50624, 293, 436, 434, 411, 11, 983, 360, 286, 643, 257, 1469, 295, 6905, 561, 30, 467, 311, 370, 1858, 281, 3847, 257, 2316, 11, 983, 393, 380, 286, 50924], "temperature": 0.0, "avg_logprob": -0.10529045592573352, "compression_ratio": 1.664406779661017, "no_speech_prob": 0.34133294224739075}, {"id": 737, "seek": 376528, "start": 3776.48, "end": 3783.1200000000003, "text": " deploy it, right? And if you dig into this, every layer is problematic. So if you look at the language", "tokens": [50924, 7274, 309, 11, 558, 30, 400, 498, 291, 2528, 666, 341, 11, 633, 4583, 307, 19011, 13, 407, 498, 291, 574, 412, 264, 2856, 51256], "temperature": 0.0, "avg_logprob": -0.10529045592573352, "compression_ratio": 1.664406779661017, "no_speech_prob": 0.34133294224739075}, {"id": 738, "seek": 376528, "start": 3783.1200000000003, "end": 3787.6000000000004, "text": " piece, I mean, this is tip of the iceberg. It's a very exciting tip of the iceberg for folks, but", "tokens": [51256, 2522, 11, 286, 914, 11, 341, 307, 4125, 295, 264, 38880, 13, 467, 311, 257, 588, 4670, 4125, 295, 264, 38880, 337, 4024, 11, 457, 51480], "temperature": 0.0, "avg_logprob": -0.10529045592573352, "compression_ratio": 1.664406779661017, "no_speech_prob": 0.34133294224739075}, {"id": 739, "seek": 376528, "start": 3788.1600000000003, "end": 3793.52, "text": " you've got Python on one side and C++ on the other side. Python doesn't really deploy. I mean,", "tokens": [51508, 291, 600, 658, 15329, 322, 472, 1252, 293, 383, 25472, 322, 264, 661, 1252, 13, 15329, 1177, 380, 534, 7274, 13, 286, 914, 11, 51776], "temperature": 0.0, "avg_logprob": -0.10529045592573352, "compression_ratio": 1.664406779661017, "no_speech_prob": 0.34133294224739075}, {"id": 740, "seek": 379352, "start": 3793.6, "end": 3797.68, "text": " can theoretically, technically in some cases, but often a lot of production teams will want to get", "tokens": [50368, 393, 29400, 11, 12120, 294, 512, 3331, 11, 457, 2049, 257, 688, 295, 4265, 5491, 486, 528, 281, 483, 50572], "temperature": 0.0, "avg_logprob": -0.09419581929191215, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.005909997969865799}, {"id": 741, "seek": 379352, "start": 3797.68, "end": 3801.68, "text": " things out of Python because they get their performance and control and whatever else. So", "tokens": [50572, 721, 484, 295, 15329, 570, 436, 483, 641, 3389, 293, 1969, 293, 2035, 1646, 13, 407, 50772], "temperature": 0.0, "avg_logprob": -0.09419581929191215, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.005909997969865799}, {"id": 742, "seek": 379352, "start": 3801.68, "end": 3808.08, "text": " Mojo can help with that. If you look at serving, so you talk about gigantic models, well, a gigantic", "tokens": [50772, 3335, 5134, 393, 854, 365, 300, 13, 759, 291, 574, 412, 8148, 11, 370, 291, 751, 466, 26800, 5245, 11, 731, 11, 257, 26800, 51092], "temperature": 0.0, "avg_logprob": -0.09419581929191215, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.005909997969865799}, {"id": 743, "seek": 379352, "start": 3808.08, "end": 3814.64, "text": " model won't fit on one machine, right? And so now you have this model, it's written Python, it has", "tokens": [51092, 2316, 1582, 380, 3318, 322, 472, 3479, 11, 558, 30, 400, 370, 586, 291, 362, 341, 2316, 11, 309, 311, 3720, 15329, 11, 309, 575, 51420], "temperature": 0.0, "avg_logprob": -0.09419581929191215, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.005909997969865799}, {"id": 744, "seek": 379352, "start": 3814.64, "end": 3819.12, "text": " to be rewritten in C++. Now it also has to be carved up so that half of it runs on one machine,", "tokens": [51420, 281, 312, 319, 26859, 294, 383, 25472, 13, 823, 309, 611, 575, 281, 312, 28613, 493, 370, 300, 1922, 295, 309, 6676, 322, 472, 3479, 11, 51644], "temperature": 0.0, "avg_logprob": -0.09419581929191215, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.005909997969865799}, {"id": 745, "seek": 381912, "start": 3819.12, "end": 3825.2799999999997, "text": " half of it runs on another machine, or maybe it runs on 10 machines. So now suddenly, the complexity", "tokens": [50364, 1922, 295, 309, 6676, 322, 1071, 3479, 11, 420, 1310, 309, 6676, 322, 1266, 8379, 13, 407, 586, 5800, 11, 264, 14024, 50672], "temperature": 0.0, "avg_logprob": -0.13575142474213908, "compression_ratio": 1.6552901023890785, "no_speech_prob": 0.0259479321539402}, {"id": 746, "seek": 381912, "start": 3825.2799999999997, "end": 3831.3599999999997, "text": " is exploding, right? And the reason for this is that if you look into TensorFlow, PyTorps, these", "tokens": [50672, 307, 35175, 11, 558, 30, 400, 264, 1778, 337, 341, 307, 300, 498, 291, 574, 666, 37624, 11, 9953, 51, 284, 1878, 11, 613, 50976], "temperature": 0.0, "avg_logprob": -0.13575142474213908, "compression_ratio": 1.6552901023890785, "no_speech_prob": 0.0259479321539402}, {"id": 747, "seek": 381912, "start": 3831.3599999999997, "end": 3836.3199999999997, "text": " systems, they weren't really designed for this world, right? They were designed for, you know,", "tokens": [50976, 3652, 11, 436, 4999, 380, 534, 4761, 337, 341, 1002, 11, 558, 30, 814, 645, 4761, 337, 11, 291, 458, 11, 51224], "temperature": 0.0, "avg_logprob": -0.13575142474213908, "compression_ratio": 1.6552901023890785, "no_speech_prob": 0.0259479321539402}, {"id": 748, "seek": 381912, "start": 3836.3199999999997, "end": 3841.3599999999997, "text": " back in the day when we were starting and doing things where it was a different, much simpler", "tokens": [51224, 646, 294, 264, 786, 562, 321, 645, 2891, 293, 884, 721, 689, 309, 390, 257, 819, 11, 709, 18587, 51476], "temperature": 0.0, "avg_logprob": -0.13575142474213908, "compression_ratio": 1.6552901023890785, "no_speech_prob": 0.0259479321539402}, {"id": 749, "seek": 381912, "start": 3841.3599999999997, "end": 3846.56, "text": " world, like you want to run ResNet 50 or some ancient model architecture like this. It was just a,", "tokens": [51476, 1002, 11, 411, 291, 528, 281, 1190, 5015, 31890, 2625, 420, 512, 7832, 2316, 9482, 411, 341, 13, 467, 390, 445, 257, 11, 51736], "temperature": 0.0, "avg_logprob": -0.13575142474213908, "compression_ratio": 1.6552901023890785, "no_speech_prob": 0.0259479321539402}, {"id": 750, "seek": 384656, "start": 3846.56, "end": 3848.0, "text": " it was a completely different world.", "tokens": [50364, 309, 390, 257, 2584, 819, 1002, 13, 50436], "temperature": 0.0, "avg_logprob": -0.19455687204996744, "compression_ratio": 1.5680272108843538, "no_speech_prob": 0.0008294038707390428}, {"id": 751, "seek": 384656, "start": 3848.0, "end": 3850.0, "text": " Trained on 1GPU. Exactly.", "tokens": [50436, 5403, 2001, 322, 502, 38, 8115, 13, 7587, 13, 50536], "temperature": 0.0, "avg_logprob": -0.19455687204996744, "compression_ratio": 1.5680272108843538, "no_speech_prob": 0.0008294038707390428}, {"id": 752, "seek": 384656, "start": 3850.0, "end": 3851.36, "text": " Doing some 1GPU.", "tokens": [50536, 18496, 512, 502, 38, 8115, 13, 50604], "temperature": 0.0, "avg_logprob": -0.19455687204996744, "compression_ratio": 1.5680272108843538, "no_speech_prob": 0.0008294038707390428}, {"id": 753, "seek": 384656, "start": 3851.36, "end": 3858.08, "text": " Yeah, AlexNet, right? The major breakthrough. And the world has changed, right? And so now the", "tokens": [50604, 865, 11, 5202, 31890, 11, 558, 30, 440, 2563, 22397, 13, 400, 264, 1002, 575, 3105, 11, 558, 30, 400, 370, 586, 264, 50940], "temperature": 0.0, "avg_logprob": -0.19455687204996744, "compression_ratio": 1.5680272108843538, "no_speech_prob": 0.0008294038707390428}, {"id": 754, "seek": 384656, "start": 3858.08, "end": 3861.92, "text": " challenge is that TensorFlow, PyTorps, these systems, they weren't actually designed for", "tokens": [50940, 3430, 307, 300, 37624, 11, 9953, 51, 284, 1878, 11, 613, 3652, 11, 436, 4999, 380, 767, 4761, 337, 51132], "temperature": 0.0, "avg_logprob": -0.19455687204996744, "compression_ratio": 1.5680272108843538, "no_speech_prob": 0.0008294038707390428}, {"id": 755, "seek": 384656, "start": 3861.92, "end": 3868.4, "text": " LLMs. That was not a thing. And so where TensorFlow actually has amazing power in terms of scale and", "tokens": [51132, 441, 43, 26386, 13, 663, 390, 406, 257, 551, 13, 400, 370, 689, 37624, 767, 575, 2243, 1347, 294, 2115, 295, 4373, 293, 51456], "temperature": 0.0, "avg_logprob": -0.19455687204996744, "compression_ratio": 1.5680272108843538, "no_speech_prob": 0.0008294038707390428}, {"id": 756, "seek": 384656, "start": 3868.4, "end": 3873.04, "text": " deployment and things like that. And I think Google is, I mean, maybe not unmatched, but they're", "tokens": [51456, 19317, 293, 721, 411, 300, 13, 400, 286, 519, 3329, 307, 11, 286, 914, 11, 1310, 406, 19334, 24102, 11, 457, 436, 434, 51688], "temperature": 0.0, "avg_logprob": -0.19455687204996744, "compression_ratio": 1.5680272108843538, "no_speech_prob": 0.0008294038707390428}, {"id": 757, "seek": 387304, "start": 3873.2799999999997, "end": 3880.16, "text": " incredible in terms of their capabilities and gigantic scale. Many researchers using PyTorps,", "tokens": [50376, 4651, 294, 2115, 295, 641, 10862, 293, 26800, 4373, 13, 5126, 10309, 1228, 9953, 51, 284, 1878, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11687317541090109, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0053838216699659824}, {"id": 758, "seek": 387304, "start": 3880.16, "end": 3883.44, "text": " right? And so PyTorps doesn't have those same capabilities. And so what Modular can do is", "tokens": [50720, 558, 30, 400, 370, 9953, 51, 284, 1878, 1177, 380, 362, 729, 912, 10862, 13, 400, 370, 437, 6583, 1040, 393, 360, 307, 50884], "temperature": 0.0, "avg_logprob": -0.11687317541090109, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0053838216699659824}, {"id": 759, "seek": 387304, "start": 3883.44, "end": 3888.0, "text": " it can help with that. Now, if you take a step back and say like, what is Modular doing, right?", "tokens": [50884, 309, 393, 854, 365, 300, 13, 823, 11, 498, 291, 747, 257, 1823, 646, 293, 584, 411, 11, 437, 307, 6583, 1040, 884, 11, 558, 30, 51112], "temperature": 0.0, "avg_logprob": -0.11687317541090109, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0053838216699659824}, {"id": 760, "seek": 387304, "start": 3888.0, "end": 3894.4, "text": " So Modular has like a bitter enemy that we're fighting against in the industry. And it's one", "tokens": [51112, 407, 6583, 1040, 575, 411, 257, 13871, 5945, 300, 321, 434, 5237, 1970, 294, 264, 3518, 13, 400, 309, 311, 472, 51432], "temperature": 0.0, "avg_logprob": -0.11687317541090109, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0053838216699659824}, {"id": 761, "seek": 387304, "start": 3894.4, "end": 3900.16, "text": " of these things where everybody knows it, but nobody is usually willing to talk about it.", "tokens": [51432, 295, 613, 721, 689, 2201, 3255, 309, 11, 457, 5079, 307, 2673, 4950, 281, 751, 466, 309, 13, 51720], "temperature": 0.0, "avg_logprob": -0.11687317541090109, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0053838216699659824}, {"id": 762, "seek": 390016, "start": 3900.7999999999997, "end": 3902.24, "text": " The bitter enemy.", "tokens": [50396, 440, 13871, 5945, 13, 50468], "temperature": 0.0, "avg_logprob": -0.1792945180620466, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.008061382919549942}, {"id": 763, "seek": 390016, "start": 3902.24, "end": 3906.96, "text": " The bitter thing that we have to destroy, that we're all struggling with, and it's like fish", "tokens": [50468, 440, 13871, 551, 300, 321, 362, 281, 5293, 11, 300, 321, 434, 439, 9314, 365, 11, 293, 309, 311, 411, 3506, 50704], "temperature": 0.0, "avg_logprob": -0.1792945180620466, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.008061382919549942}, {"id": 764, "seek": 390016, "start": 3906.96, "end": 3912.96, "text": " can't see water, is complexity. Sure. Yes. It's complexity. That was very close.", "tokens": [50704, 393, 380, 536, 1281, 11, 307, 14024, 13, 4894, 13, 1079, 13, 467, 311, 14024, 13, 663, 390, 588, 1998, 13, 51004], "temperature": 0.0, "avg_logprob": -0.1792945180620466, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.008061382919549942}, {"id": 765, "seek": 390016, "start": 3914.8799999999997, "end": 3918.64, "text": " And so if you look at it, yes, it is on the hardware side. Yes.", "tokens": [51100, 400, 370, 498, 291, 574, 412, 309, 11, 2086, 11, 309, 307, 322, 264, 8837, 1252, 13, 1079, 13, 51288], "temperature": 0.0, "avg_logprob": -0.1792945180620466, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.008061382919549942}, {"id": 766, "seek": 390016, "start": 3918.64, "end": 3922.16, "text": " All these accelerators, all these software stacks that go with the accelerator,", "tokens": [51288, 1057, 613, 10172, 3391, 11, 439, 613, 4722, 30792, 300, 352, 365, 264, 39889, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1792945180620466, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.008061382919549942}, {"id": 767, "seek": 390016, "start": 3922.16, "end": 3927.8399999999997, "text": " all these massive complexity over there. You look at what's happening on the modeling side,", "tokens": [51464, 439, 613, 5994, 14024, 670, 456, 13, 509, 574, 412, 437, 311, 2737, 322, 264, 15983, 1252, 11, 51748], "temperature": 0.0, "avg_logprob": -0.1792945180620466, "compression_ratio": 1.7717842323651452, "no_speech_prob": 0.008061382919549942}, {"id": 768, "seek": 392784, "start": 3928.48, "end": 3931.6800000000003, "text": " massive amount of complexity. Like things are changing all the time. People are inventing,", "tokens": [50396, 5994, 2372, 295, 14024, 13, 1743, 721, 366, 4473, 439, 264, 565, 13, 3432, 366, 7962, 278, 11, 50556], "temperature": 0.0, "avg_logprob": -0.10532016323921375, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.0008295430452562869}, {"id": 769, "seek": 392784, "start": 3931.6800000000003, "end": 3935.84, "text": " turns out the research is not done, right? And so people want to be able to move fast.", "tokens": [50556, 4523, 484, 264, 2132, 307, 406, 1096, 11, 558, 30, 400, 370, 561, 528, 281, 312, 1075, 281, 1286, 2370, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10532016323921375, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.0008295430452562869}, {"id": 770, "seek": 392784, "start": 3935.84, "end": 3940.56, "text": " Transformers are amazing, but there's a ton of diversity even within transformers. And what's", "tokens": [50764, 27938, 433, 366, 2243, 11, 457, 456, 311, 257, 2952, 295, 8811, 754, 1951, 4088, 433, 13, 400, 437, 311, 51000], "temperature": 0.0, "avg_logprob": -0.10532016323921375, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.0008295430452562869}, {"id": 771, "seek": 392784, "start": 3940.56, "end": 3946.8, "text": " the next transformer, right? And you look into serving also huge amounts of complexity. It turns", "tokens": [51000, 264, 958, 31782, 11, 558, 30, 400, 291, 574, 666, 8148, 611, 2603, 11663, 295, 14024, 13, 467, 4523, 51312], "temperature": 0.0, "avg_logprob": -0.10532016323921375, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.0008295430452562869}, {"id": 772, "seek": 392784, "start": 3946.8, "end": 3952.08, "text": " out that all the cloud providers, right, have all their very weird, but very cool hardware for", "tokens": [51312, 484, 300, 439, 264, 4588, 11330, 11, 558, 11, 362, 439, 641, 588, 3657, 11, 457, 588, 1627, 8837, 337, 51576], "temperature": 0.0, "avg_logprob": -0.10532016323921375, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.0008295430452562869}, {"id": 773, "seek": 392784, "start": 3952.08, "end": 3956.1600000000003, "text": " networking and all this kind of stuff. And it's all very complicated. People aren't using that.", "tokens": [51576, 17985, 293, 439, 341, 733, 295, 1507, 13, 400, 309, 311, 439, 588, 6179, 13, 3432, 3212, 380, 1228, 300, 13, 51780], "temperature": 0.0, "avg_logprob": -0.10532016323921375, "compression_ratio": 1.8208469055374592, "no_speech_prob": 0.0008295430452562869}, {"id": 774, "seek": 395616, "start": 3956.24, "end": 3961.3599999999997, "text": " You look at classical serving, right? There's this whole world of people who know how to write", "tokens": [50368, 509, 574, 412, 13735, 8148, 11, 558, 30, 821, 311, 341, 1379, 1002, 295, 561, 567, 458, 577, 281, 2464, 50624], "temperature": 0.0, "avg_logprob": -0.14603771796593298, "compression_ratio": 1.6561514195583595, "no_speech_prob": 0.0009109275415539742}, {"id": 775, "seek": 395616, "start": 3961.3599999999997, "end": 3965.2799999999997, "text": " high-performance servers with zero-copy networking and like all this fancy,", "tokens": [50624, 1090, 12, 50242, 15909, 365, 4018, 12, 13084, 88, 17985, 293, 411, 439, 341, 10247, 11, 50820], "temperature": 0.0, "avg_logprob": -0.14603771796593298, "compression_ratio": 1.6561514195583595, "no_speech_prob": 0.0009109275415539742}, {"id": 776, "seek": 395616, "start": 3966.16, "end": 3970.48, "text": " asynchronous I.O. and like all these fancy things in the serving community,", "tokens": [50864, 49174, 286, 13, 46, 13, 293, 411, 439, 613, 10247, 721, 294, 264, 8148, 1768, 11, 51080], "temperature": 0.0, "avg_logprob": -0.14603771796593298, "compression_ratio": 1.6561514195583595, "no_speech_prob": 0.0009109275415539742}, {"id": 777, "seek": 395616, "start": 3971.2799999999997, "end": 3975.68, "text": " very little that has pervaded into the machine learning world, right? And why is that? Well,", "tokens": [51120, 588, 707, 300, 575, 680, 85, 12777, 666, 264, 3479, 2539, 1002, 11, 558, 30, 400, 983, 307, 300, 30, 1042, 11, 51340], "temperature": 0.0, "avg_logprob": -0.14603771796593298, "compression_ratio": 1.6561514195583595, "no_speech_prob": 0.0009109275415539742}, {"id": 778, "seek": 395616, "start": 3975.68, "end": 3980.64, "text": " it's because, again, these systems have been built up over many years. They haven't been", "tokens": [51340, 309, 311, 570, 11, 797, 11, 613, 3652, 362, 668, 3094, 493, 670, 867, 924, 13, 814, 2378, 380, 668, 51588], "temperature": 0.0, "avg_logprob": -0.14603771796593298, "compression_ratio": 1.6561514195583595, "no_speech_prob": 0.0009109275415539742}, {"id": 779, "seek": 395616, "start": 3980.64, "end": 3985.04, "text": " rethought. There hasn't been a first principles approach to this. And so what Modular's doing is", "tokens": [51588, 319, 43135, 13, 821, 6132, 380, 668, 257, 700, 9156, 3109, 281, 341, 13, 400, 370, 437, 6583, 1040, 311, 884, 307, 51808], "temperature": 0.0, "avg_logprob": -0.14603771796593298, "compression_ratio": 1.6561514195583595, "no_speech_prob": 0.0009109275415539742}, {"id": 780, "seek": 398504, "start": 3985.04, "end": 3990.72, "text": " we're saying, okay, we've built many of these things. So I've worked on TensorFlow and TPUs", "tokens": [50364, 321, 434, 1566, 11, 1392, 11, 321, 600, 3094, 867, 295, 613, 721, 13, 407, 286, 600, 2732, 322, 37624, 293, 314, 8115, 82, 50648], "temperature": 0.0, "avg_logprob": -0.10642415477383521, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.002115335315465927}, {"id": 781, "seek": 398504, "start": 3990.72, "end": 3995.44, "text": " and things like that. Other folks on our team have worked on PyTorch core. We've worked on", "tokens": [50648, 293, 721, 411, 300, 13, 5358, 4024, 322, 527, 1469, 362, 2732, 322, 9953, 51, 284, 339, 4965, 13, 492, 600, 2732, 322, 50884], "temperature": 0.0, "avg_logprob": -0.10642415477383521, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.002115335315465927}, {"id": 782, "seek": 398504, "start": 3995.44, "end": 4001.04, "text": " Onyx one time. We've worked on many of these other systems. And so systems like the Apple", "tokens": [50884, 1282, 88, 87, 472, 565, 13, 492, 600, 2732, 322, 867, 295, 613, 661, 3652, 13, 400, 370, 3652, 411, 264, 6373, 51164], "temperature": 0.0, "avg_logprob": -0.10642415477383521, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.002115335315465927}, {"id": 783, "seek": 398504, "start": 4001.04, "end": 4006.48, "text": " accelerators and all that kind of stuff, our team is quite amazing. And so one of the things that", "tokens": [51164, 10172, 3391, 293, 439, 300, 733, 295, 1507, 11, 527, 1469, 307, 1596, 2243, 13, 400, 370, 472, 295, 264, 721, 300, 51436], "temperature": 0.0, "avg_logprob": -0.10642415477383521, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.002115335315465927}, {"id": 784, "seek": 398504, "start": 4007.12, "end": 4011.6, "text": " roughly everybody at Modular's grumpy about is that when you're working on one of these projects,", "tokens": [51468, 9810, 2201, 412, 6583, 1040, 311, 677, 36142, 466, 307, 300, 562, 291, 434, 1364, 322, 472, 295, 613, 4455, 11, 51692], "temperature": 0.0, "avg_logprob": -0.10642415477383521, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.002115335315465927}, {"id": 785, "seek": 401160, "start": 4011.68, "end": 4017.44, "text": " you have a first order goal, get the hardware to work, get the system to enable one more model,", "tokens": [50368, 291, 362, 257, 700, 1668, 3387, 11, 483, 264, 8837, 281, 589, 11, 483, 264, 1185, 281, 9528, 472, 544, 2316, 11, 50656], "temperature": 0.0, "avg_logprob": -0.11377446834857648, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.0038233278319239616}, {"id": 786, "seek": 401160, "start": 4017.44, "end": 4022.88, "text": " get this product out the door, enable this specific workload or solve this problem for", "tokens": [50656, 483, 341, 1674, 484, 264, 2853, 11, 9528, 341, 2685, 20139, 420, 5039, 341, 1154, 337, 50928], "temperature": 0.0, "avg_logprob": -0.11377446834857648, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.0038233278319239616}, {"id": 787, "seek": 401160, "start": 4022.88, "end": 4027.8399999999997, "text": " this product team, right? And nobody's been given a chance to actually do that step back.", "tokens": [50928, 341, 1674, 1469, 11, 558, 30, 400, 5079, 311, 668, 2212, 257, 2931, 281, 767, 360, 300, 1823, 646, 13, 51176], "temperature": 0.0, "avg_logprob": -0.11377446834857648, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.0038233278319239616}, {"id": 788, "seek": 401160, "start": 4027.8399999999997, "end": 4032.3199999999997, "text": " And so we as an industry, we didn't take two steps forward. We took like 18 steps forward", "tokens": [51176, 400, 370, 321, 382, 364, 3518, 11, 321, 994, 380, 747, 732, 4439, 2128, 13, 492, 1890, 411, 2443, 4439, 2128, 51400], "temperature": 0.0, "avg_logprob": -0.11377446834857648, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.0038233278319239616}, {"id": 789, "seek": 401160, "start": 4032.3199999999997, "end": 4036.48, "text": " in terms of all this really cool technology across compilers and systems and runtimes and", "tokens": [51400, 294, 2115, 295, 439, 341, 534, 1627, 2899, 2108, 715, 388, 433, 293, 3652, 293, 49435, 1532, 293, 51608], "temperature": 0.0, "avg_logprob": -0.11377446834857648, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.0038233278319239616}, {"id": 790, "seek": 401160, "start": 4036.48, "end": 4039.8399999999997, "text": " heterogeneous computing, like all this kind of stuff. And like all this technology has been,", "tokens": [51608, 20789, 31112, 15866, 11, 411, 439, 341, 733, 295, 1507, 13, 400, 411, 439, 341, 2899, 575, 668, 11, 51776], "temperature": 0.0, "avg_logprob": -0.11377446834857648, "compression_ratio": 1.8412162162162162, "no_speech_prob": 0.0038233278319239616}, {"id": 791, "seek": 403984, "start": 4040.7200000000003, "end": 4045.76, "text": " you know, I wouldn't say beautifully designed, but it's been proven in different quadrants.", "tokens": [50408, 291, 458, 11, 286, 2759, 380, 584, 16525, 4761, 11, 457, 309, 311, 668, 12785, 294, 819, 10787, 10968, 13, 50660], "temperature": 0.0, "avg_logprob": -0.13191165924072265, "compression_ratio": 1.6131386861313868, "no_speech_prob": 0.0010320816654711962}, {"id": 792, "seek": 403984, "start": 4045.76, "end": 4051.92, "text": " Like, you know, you look at Google with TPUs, massive, huge exoflops of compute strapped", "tokens": [50660, 1743, 11, 291, 458, 11, 291, 574, 412, 3329, 365, 314, 8115, 82, 11, 5994, 11, 2603, 454, 2670, 75, 3370, 295, 14722, 2148, 3320, 50968], "temperature": 0.0, "avg_logprob": -0.13191165924072265, "compression_ratio": 1.6131386861313868, "no_speech_prob": 0.0010320816654711962}, {"id": 793, "seek": 403984, "start": 4051.92, "end": 4056.2400000000002, "text": " together into machines that researchers are programming in Python in a notebook.", "tokens": [50968, 1214, 666, 8379, 300, 10309, 366, 9410, 294, 15329, 294, 257, 21060, 13, 51184], "temperature": 0.0, "avg_logprob": -0.13191165924072265, "compression_ratio": 1.6131386861313868, "no_speech_prob": 0.0010320816654711962}, {"id": 794, "seek": 403984, "start": 4056.88, "end": 4060.96, "text": " That's huge. That's amazing. That's incredible, right? It's incredible. And so you look at the", "tokens": [51216, 663, 311, 2603, 13, 663, 311, 2243, 13, 663, 311, 4651, 11, 558, 30, 467, 311, 4651, 13, 400, 370, 291, 574, 412, 264, 51420], "temperature": 0.0, "avg_logprob": -0.13191165924072265, "compression_ratio": 1.6131386861313868, "no_speech_prob": 0.0010320816654711962}, {"id": 795, "seek": 403984, "start": 4060.96, "end": 4066.4, "text": " technology that goes into that. And the algorithms are actually quite general. And so", "tokens": [51420, 2899, 300, 1709, 666, 300, 13, 400, 264, 14642, 366, 767, 1596, 2674, 13, 400, 370, 51692], "temperature": 0.0, "avg_logprob": -0.13191165924072265, "compression_ratio": 1.6131386861313868, "no_speech_prob": 0.0010320816654711962}, {"id": 796, "seek": 406640, "start": 4067.12, "end": 4071.04, "text": " lots of other hardware out there and lots of other teams out there don't have the sophistication", "tokens": [50400, 3195, 295, 661, 8837, 484, 456, 293, 3195, 295, 661, 5491, 484, 456, 500, 380, 362, 264, 15572, 399, 50596], "temperature": 0.0, "avg_logprob": -0.1467489454481337, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0001739997969707474}, {"id": 797, "seek": 406640, "start": 4071.04, "end": 4076.96, "text": " or maybe the years working on it or the budget or whatever that Google does, right? And so", "tokens": [50596, 420, 1310, 264, 924, 1364, 322, 309, 420, 264, 4706, 420, 2035, 300, 3329, 775, 11, 558, 30, 400, 370, 50892], "temperature": 0.0, "avg_logprob": -0.1467489454481337, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0001739997969707474}, {"id": 798, "seek": 406640, "start": 4076.96, "end": 4080.2400000000002, "text": " they should be getting access to the same algorithms, but they just don't have that, right?", "tokens": [50892, 436, 820, 312, 1242, 2105, 281, 264, 912, 14642, 11, 457, 436, 445, 500, 380, 362, 300, 11, 558, 30, 51056], "temperature": 0.0, "avg_logprob": -0.1467489454481337, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0001739997969707474}, {"id": 799, "seek": 406640, "start": 4080.2400000000002, "end": 4085.92, "text": " And so what Modular's doing is we're saying, cool, this is not research anymore. Like, we've built", "tokens": [51056, 400, 370, 437, 6583, 1040, 311, 884, 307, 321, 434, 1566, 11, 1627, 11, 341, 307, 406, 2132, 3602, 13, 1743, 11, 321, 600, 3094, 51340], "temperature": 0.0, "avg_logprob": -0.1467489454481337, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0001739997969707474}, {"id": 800, "seek": 406640, "start": 4085.92, "end": 4090.7200000000003, "text": " auto tuning in many systems. We've built programming languages, right? And so like have,", "tokens": [51340, 8399, 15164, 294, 867, 3652, 13, 492, 600, 3094, 9410, 8650, 11, 558, 30, 400, 370, 411, 362, 11, 51580], "temperature": 0.0, "avg_logprob": -0.1467489454481337, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0001739997969707474}, {"id": 801, "seek": 406640, "start": 4090.7200000000003, "end": 4094.96, "text": " have, you know, implemented C++ have implemented Swift have implemented many of these things.", "tokens": [51580, 362, 11, 291, 458, 11, 12270, 383, 25472, 362, 12270, 25539, 362, 12270, 867, 295, 613, 721, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1467489454481337, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0001739997969707474}, {"id": 802, "seek": 409496, "start": 4094.96, "end": 4101.68, "text": " And so, you know, it's hard, but it's not research. And you look at accelerators. Well,", "tokens": [50364, 400, 370, 11, 291, 458, 11, 309, 311, 1152, 11, 457, 309, 311, 406, 2132, 13, 400, 291, 574, 412, 10172, 3391, 13, 1042, 11, 50700], "temperature": 0.0, "avg_logprob": -0.08074892484224759, "compression_ratio": 1.8241042345276872, "no_speech_prob": 0.0001795183343347162}, {"id": 803, "seek": 409496, "start": 4101.68, "end": 4106.4, "text": " we know there's a bunch of different weird kind of accelerators, but they actually cluster together,", "tokens": [50700, 321, 458, 456, 311, 257, 3840, 295, 819, 3657, 733, 295, 10172, 3391, 11, 457, 436, 767, 13630, 1214, 11, 50936], "temperature": 0.0, "avg_logprob": -0.08074892484224759, "compression_ratio": 1.8241042345276872, "no_speech_prob": 0.0001795183343347162}, {"id": 804, "seek": 409496, "start": 4107.04, "end": 4111.36, "text": " right? And you look at GPUs. Well, there's a couple of major vendors of GPUs, and they maybe", "tokens": [50968, 558, 30, 400, 291, 574, 412, 18407, 82, 13, 1042, 11, 456, 311, 257, 1916, 295, 2563, 22056, 295, 18407, 82, 11, 293, 436, 1310, 51184], "temperature": 0.0, "avg_logprob": -0.08074892484224759, "compression_ratio": 1.8241042345276872, "no_speech_prob": 0.0001795183343347162}, {"id": 805, "seek": 409496, "start": 4111.36, "end": 4116.08, "text": " don't always get along, but their architectures are very similar. You look at CPUs. CPUs are", "tokens": [51184, 500, 380, 1009, 483, 2051, 11, 457, 641, 6331, 1303, 366, 588, 2531, 13, 509, 574, 412, 13199, 82, 13, 13199, 82, 366, 51420], "temperature": 0.0, "avg_logprob": -0.08074892484224759, "compression_ratio": 1.8241042345276872, "no_speech_prob": 0.0001795183343347162}, {"id": 806, "seek": 409496, "start": 4116.08, "end": 4120.72, "text": " still super important for the deployment side of things. And you see new, new architectures coming", "tokens": [51420, 920, 1687, 1021, 337, 264, 19317, 1252, 295, 721, 13, 400, 291, 536, 777, 11, 777, 6331, 1303, 1348, 51652], "temperature": 0.0, "avg_logprob": -0.08074892484224759, "compression_ratio": 1.8241042345276872, "no_speech_prob": 0.0001795183343347162}, {"id": 807, "seek": 409496, "start": 4120.72, "end": 4124.16, "text": " out from all the cloud providers and things like this, and they're all super important", "tokens": [51652, 484, 490, 439, 264, 4588, 11330, 293, 721, 411, 341, 11, 293, 436, 434, 439, 1687, 1021, 51824], "temperature": 0.0, "avg_logprob": -0.08074892484224759, "compression_ratio": 1.8241042345276872, "no_speech_prob": 0.0001795183343347162}, {"id": 808, "seek": 412416, "start": 4124.16, "end": 4129.04, "text": " to the world, right? But they don't have the 30 years of development that the entrenched people", "tokens": [50364, 281, 264, 1002, 11, 558, 30, 583, 436, 500, 380, 362, 264, 2217, 924, 295, 3250, 300, 264, 948, 42388, 561, 50608], "temperature": 0.0, "avg_logprob": -0.11933951568603515, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004069124348461628}, {"id": 809, "seek": 412416, "start": 4129.04, "end": 4134.88, "text": " do, right? And so what Modular can do is we're saying, okay, all this complexity, like, it's not,", "tokens": [50608, 360, 11, 558, 30, 400, 370, 437, 6583, 1040, 393, 360, 307, 321, 434, 1566, 11, 1392, 11, 439, 341, 14024, 11, 411, 11, 309, 311, 406, 11, 50900], "temperature": 0.0, "avg_logprob": -0.11933951568603515, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004069124348461628}, {"id": 810, "seek": 412416, "start": 4134.88, "end": 4141.28, "text": " it's not bad complexity. It's actually innovation, right? And so it's innovation that's happening.", "tokens": [50900, 309, 311, 406, 1578, 14024, 13, 467, 311, 767, 8504, 11, 558, 30, 400, 370, 309, 311, 8504, 300, 311, 2737, 13, 51220], "temperature": 0.0, "avg_logprob": -0.11933951568603515, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004069124348461628}, {"id": 811, "seek": 412416, "start": 4141.28, "end": 4146.639999999999, "text": " And it's for good reasons. But I have sympathy for the poor software people, right? I mean, again,", "tokens": [51220, 400, 309, 311, 337, 665, 4112, 13, 583, 286, 362, 33240, 337, 264, 4716, 4722, 561, 11, 558, 30, 286, 914, 11, 797, 11, 51488], "temperature": 0.0, "avg_logprob": -0.11933951568603515, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004069124348461628}, {"id": 812, "seek": 412416, "start": 4146.639999999999, "end": 4151.12, "text": " I'm generally a software person too, I love hardware. But software people want to build", "tokens": [51488, 286, 478, 5101, 257, 4722, 954, 886, 11, 286, 959, 8837, 13, 583, 4722, 561, 528, 281, 1322, 51712], "temperature": 0.0, "avg_logprob": -0.11933951568603515, "compression_ratio": 1.7806691449814127, "no_speech_prob": 0.004069124348461628}, {"id": 813, "seek": 415112, "start": 4151.12, "end": 4156.5599999999995, "text": " applications and products and solutions that scale over many years. They don't want to build a", "tokens": [50364, 5821, 293, 3383, 293, 6547, 300, 4373, 670, 867, 924, 13, 814, 500, 380, 528, 281, 1322, 257, 50636], "temperature": 0.0, "avg_logprob": -0.11752731569351689, "compression_ratio": 1.7813504823151125, "no_speech_prob": 0.0059098438359797}, {"id": 814, "seek": 415112, "start": 4156.5599999999995, "end": 4161.36, "text": " solution for one generation of hardware with one vendor's tools. Right? And because of this,", "tokens": [50636, 3827, 337, 472, 5125, 295, 8837, 365, 472, 24321, 311, 3873, 13, 1779, 30, 400, 570, 295, 341, 11, 50876], "temperature": 0.0, "avg_logprob": -0.11752731569351689, "compression_ratio": 1.7813504823151125, "no_speech_prob": 0.0059098438359797}, {"id": 815, "seek": 415112, "start": 4161.36, "end": 4166.48, "text": " they need something that scales with them, they need something that works on cloud and mobile,", "tokens": [50876, 436, 643, 746, 300, 17408, 365, 552, 11, 436, 643, 746, 300, 1985, 322, 4588, 293, 6013, 11, 51132], "temperature": 0.0, "avg_logprob": -0.11752731569351689, "compression_ratio": 1.7813504823151125, "no_speech_prob": 0.0059098438359797}, {"id": 816, "seek": 415112, "start": 4167.599999999999, "end": 4171.599999999999, "text": " right? Because, you know, their product manager said, Hey, I want to be at have lower latency,", "tokens": [51188, 558, 30, 1436, 11, 291, 458, 11, 641, 1674, 6598, 848, 11, 1911, 11, 286, 528, 281, 312, 412, 362, 3126, 27043, 11, 51388], "temperature": 0.0, "avg_logprob": -0.11752731569351689, "compression_ratio": 1.7813504823151125, "no_speech_prob": 0.0059098438359797}, {"id": 817, "seek": 415112, "start": 4172.16, "end": 4176.8, "text": " it's better for personalization or whatever they decide, right? Products evolve. And so", "tokens": [51416, 309, 311, 1101, 337, 2973, 2144, 420, 2035, 436, 4536, 11, 558, 30, 47699, 16693, 13, 400, 370, 51648], "temperature": 0.0, "avg_logprob": -0.11752731569351689, "compression_ratio": 1.7813504823151125, "no_speech_prob": 0.0059098438359797}, {"id": 818, "seek": 415112, "start": 4177.44, "end": 4180.88, "text": " the challenge with the machine learning technology and the infrastructure we have today,", "tokens": [51680, 264, 3430, 365, 264, 3479, 2539, 2899, 293, 264, 6896, 321, 362, 965, 11, 51852], "temperature": 0.0, "avg_logprob": -0.11752731569351689, "compression_ratio": 1.7813504823151125, "no_speech_prob": 0.0059098438359797}, {"id": 819, "seek": 418088, "start": 4180.88, "end": 4185.68, "text": " in the industry, is that it's all these point solutions. And because they're all these point", "tokens": [50364, 294, 264, 3518, 11, 307, 300, 309, 311, 439, 613, 935, 6547, 13, 400, 570, 436, 434, 439, 613, 935, 50604], "temperature": 0.0, "avg_logprob": -0.07897448539733887, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.0005192015087231994}, {"id": 820, "seek": 418088, "start": 4185.68, "end": 4189.28, "text": " solutions, it means that as your product evolves, you have to like switch different technology", "tokens": [50604, 6547, 11, 309, 1355, 300, 382, 428, 1674, 43737, 11, 291, 362, 281, 411, 3679, 819, 2899, 50784], "temperature": 0.0, "avg_logprob": -0.07897448539733887, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.0005192015087231994}, {"id": 821, "seek": 418088, "start": 4189.28, "end": 4193.28, "text": " stacks or switch to different vendor. And what that does is that slows down progress.", "tokens": [50784, 30792, 420, 3679, 281, 819, 24321, 13, 400, 437, 300, 775, 307, 300, 35789, 760, 4205, 13, 50984], "temperature": 0.0, "avg_logprob": -0.07897448539733887, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.0005192015087231994}, {"id": 822, "seek": 418088, "start": 4194.08, "end": 4201.76, "text": " So basically, a lot of the things we've developed in those little silos for machine learning tasks,", "tokens": [51024, 407, 1936, 11, 257, 688, 295, 264, 721, 321, 600, 4743, 294, 729, 707, 48893, 337, 3479, 2539, 9608, 11, 51408], "temperature": 0.0, "avg_logprob": -0.07897448539733887, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.0005192015087231994}, {"id": 823, "seek": 418088, "start": 4201.76, "end": 4206.0, "text": " you want to make that the first class citizen of a general purpose programming language,", "tokens": [51408, 291, 528, 281, 652, 300, 264, 700, 1508, 13326, 295, 257, 2674, 4334, 9410, 2856, 11, 51620], "temperature": 0.0, "avg_logprob": -0.07897448539733887, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.0005192015087231994}, {"id": 824, "seek": 418088, "start": 4206.0, "end": 4208.56, "text": " they can then be compiled across all these kinds of hardware.", "tokens": [51620, 436, 393, 550, 312, 36548, 2108, 439, 613, 3685, 295, 8837, 13, 51748], "temperature": 0.0, "avg_logprob": -0.07897448539733887, "compression_ratio": 1.7583892617449663, "no_speech_prob": 0.0005192015087231994}, {"id": 825, "seek": 420856, "start": 4208.56, "end": 4211.92, "text": " Well, so it's not really about a programming language. I mean, the program language is a", "tokens": [50364, 1042, 11, 370, 309, 311, 406, 534, 466, 257, 9410, 2856, 13, 286, 914, 11, 264, 1461, 2856, 307, 257, 50532], "temperature": 0.0, "avg_logprob": -0.14808394269245426, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.011323628947138786}, {"id": 826, "seek": 420856, "start": 4211.92, "end": 4217.200000000001, "text": " component of the mission, right? And the mission is are not literal, but our joking mission is to", "tokens": [50532, 6542, 295, 264, 4447, 11, 558, 30, 400, 264, 4447, 307, 366, 406, 20411, 11, 457, 527, 17396, 4447, 307, 281, 50796], "temperature": 0.0, "avg_logprob": -0.14808394269245426, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.011323628947138786}, {"id": 827, "seek": 420856, "start": 4217.200000000001, "end": 4223.84, "text": " save the world from terrible AI software. Excellent. Okay. So, so, you know, if you look at this", "tokens": [50796, 3155, 264, 1002, 490, 6237, 7318, 4722, 13, 16723, 13, 1033, 13, 407, 11, 370, 11, 291, 458, 11, 498, 291, 574, 412, 341, 51128], "temperature": 0.0, "avg_logprob": -0.14808394269245426, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.011323628947138786}, {"id": 828, "seek": 420856, "start": 4223.84, "end": 4230.240000000001, "text": " mission, you need a syntax. So that's the so yeah, she need a program language, right? And and like,", "tokens": [51128, 4447, 11, 291, 643, 257, 28431, 13, 407, 300, 311, 264, 370, 1338, 11, 750, 643, 257, 1461, 2856, 11, 558, 30, 400, 293, 411, 11, 51448], "temperature": 0.0, "avg_logprob": -0.14808394269245426, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.011323628947138786}, {"id": 829, "seek": 420856, "start": 4230.240000000001, "end": 4234.4800000000005, "text": " we wouldn't have to build the programming language if one existed. Right? So if Python was already", "tokens": [51448, 321, 2759, 380, 362, 281, 1322, 264, 9410, 2856, 498, 472, 13135, 13, 1779, 30, 407, 498, 15329, 390, 1217, 51660], "temperature": 0.0, "avg_logprob": -0.14808394269245426, "compression_ratio": 1.743682310469314, "no_speech_prob": 0.011323628947138786}, {"id": 830, "seek": 423448, "start": 4234.48, "end": 4238.719999999999, "text": " good enough, then cool, we would just used it, right? We're not just doing very large scale", "tokens": [50364, 665, 1547, 11, 550, 1627, 11, 321, 576, 445, 1143, 309, 11, 558, 30, 492, 434, 406, 445, 884, 588, 2416, 4373, 50576], "temperature": 0.0, "avg_logprob": -0.13921424670097154, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.010984486900269985}, {"id": 831, "seek": 423448, "start": 4238.719999999999, "end": 4243.679999999999, "text": " expensive engineering projects for the sake of it, like, it's to solve a problem, right? It's also", "tokens": [50576, 5124, 7043, 4455, 337, 264, 9717, 295, 309, 11, 411, 11, 309, 311, 281, 5039, 257, 1154, 11, 558, 30, 467, 311, 611, 50824], "temperature": 0.0, "avg_logprob": -0.13921424670097154, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.010984486900269985}, {"id": 832, "seek": 423448, "start": 4243.679999999999, "end": 4251.28, "text": " about accelerators. It's also about exotic numerics and b-float 16 and matrix multiplications and", "tokens": [50824, 466, 10172, 3391, 13, 467, 311, 611, 466, 27063, 7866, 1167, 293, 272, 12, 43645, 267, 3165, 293, 8141, 17596, 763, 293, 51204], "temperature": 0.0, "avg_logprob": -0.13921424670097154, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.010984486900269985}, {"id": 833, "seek": 423448, "start": 4251.28, "end": 4256.4, "text": " convolutions and like this, this kind of stuff. Within the stack, there are things like kernel", "tokens": [51204, 3754, 15892, 293, 411, 341, 11, 341, 733, 295, 1507, 13, 15996, 264, 8630, 11, 456, 366, 721, 411, 28256, 51460], "temperature": 0.0, "avg_logprob": -0.13921424670097154, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.010984486900269985}, {"id": 834, "seek": 423448, "start": 4256.4, "end": 4262.0, "text": " fusion. It's a esoteric but really important thing that leads to much better performance", "tokens": [51460, 23100, 13, 467, 311, 257, 785, 21585, 299, 457, 534, 1021, 551, 300, 6689, 281, 709, 1101, 3389, 51740], "temperature": 0.0, "avg_logprob": -0.13921424670097154, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.010984486900269985}, {"id": 835, "seek": 426200, "start": 4262.0, "end": 4268.96, "text": " and much more general research hackability together, right? And that's enabled by the ASICS,", "tokens": [50364, 293, 709, 544, 2674, 2132, 10339, 2310, 1214, 11, 558, 30, 400, 300, 311, 15172, 538, 264, 7469, 2532, 50, 11, 50712], "temperature": 0.0, "avg_logprob": -0.20379631798546594, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0013666808372363448}, {"id": 836, "seek": 426200, "start": 4268.96, "end": 4272.48, "text": " that's enabled by certain hardware. So it's like, where's the dance between", "tokens": [50712, 300, 311, 15172, 538, 1629, 8837, 13, 407, 309, 311, 411, 11, 689, 311, 264, 4489, 1296, 50888], "temperature": 0.0, "avg_logprob": -0.20379631798546594, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0013666808372363448}, {"id": 837, "seek": 426200, "start": 4274.48, "end": 4277.76, "text": " I mean, there's several questions here, like, how do you add a piece of hardware to this stack?", "tokens": [50988, 286, 914, 11, 456, 311, 2940, 1651, 510, 11, 411, 11, 577, 360, 291, 909, 257, 2522, 295, 8837, 281, 341, 8630, 30, 51152], "temperature": 0.0, "avg_logprob": -0.20379631798546594, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0013666808372363448}, {"id": 838, "seek": 426200, "start": 4278.32, "end": 4284.8, "text": " Yeah. If you need pieces, like if I have this genius invention of a specialized accelerator,", "tokens": [51180, 865, 13, 759, 291, 643, 3755, 11, 411, 498, 286, 362, 341, 14017, 22265, 295, 257, 19813, 39889, 11, 51504], "temperature": 0.0, "avg_logprob": -0.20379631798546594, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0013666808372363448}, {"id": 839, "seek": 426200, "start": 4284.8, "end": 4289.92, "text": " how do I add that to the modular framework? And also, how does modular as a standard", "tokens": [51504, 577, 360, 286, 909, 300, 281, 264, 31111, 8388, 30, 400, 611, 11, 577, 775, 31111, 382, 257, 3832, 51760], "temperature": 0.0, "avg_logprob": -0.20379631798546594, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0013666808372363448}, {"id": 840, "seek": 428992, "start": 4290.64, "end": 4295.04, "text": " start to define the kind of hardware that should be developed?", "tokens": [50400, 722, 281, 6964, 264, 733, 295, 8837, 300, 820, 312, 4743, 30, 50620], "temperature": 0.0, "avg_logprob": -0.1283048320020366, "compression_ratio": 1.6, "no_speech_prob": 0.001867150072939694}, {"id": 841, "seek": 428992, "start": 4295.04, "end": 4301.12, "text": " Yeah. So let me take a step back and talk about status quo. Okay. And so if you go back to TensorFlow", "tokens": [50620, 865, 13, 407, 718, 385, 747, 257, 1823, 646, 293, 751, 466, 6558, 28425, 13, 1033, 13, 400, 370, 498, 291, 352, 646, 281, 37624, 50924], "temperature": 0.0, "avg_logprob": -0.1283048320020366, "compression_ratio": 1.6, "no_speech_prob": 0.001867150072939694}, {"id": 842, "seek": 428992, "start": 4301.12, "end": 4307.36, "text": " 1, PyTorch 1, this kind of timeframe, and these have all evolved and gotten way more", "tokens": [50924, 502, 11, 9953, 51, 284, 339, 502, 11, 341, 733, 295, 34830, 11, 293, 613, 362, 439, 14178, 293, 5768, 636, 544, 51236], "temperature": 0.0, "avg_logprob": -0.1283048320020366, "compression_ratio": 1.6, "no_speech_prob": 0.001867150072939694}, {"id": 843, "seek": 428992, "start": 4307.36, "end": 4312.72, "text": " complicated. So let's go back to the glorious simple days, right? These things basically were CPUs and", "tokens": [51236, 6179, 13, 407, 718, 311, 352, 646, 281, 264, 24026, 2199, 1708, 11, 558, 30, 1981, 721, 1936, 645, 13199, 82, 293, 51504], "temperature": 0.0, "avg_logprob": -0.1283048320020366, "compression_ratio": 1.6, "no_speech_prob": 0.001867150072939694}, {"id": 844, "seek": 428992, "start": 4312.72, "end": 4319.04, "text": " CUDA. And so what you do is you say, go do a dense layer and a dense layer has a matrix", "tokens": [51504, 29777, 7509, 13, 400, 370, 437, 291, 360, 307, 291, 584, 11, 352, 360, 257, 18011, 4583, 293, 257, 18011, 4583, 575, 257, 8141, 51820], "temperature": 0.0, "avg_logprob": -0.1283048320020366, "compression_ratio": 1.6, "no_speech_prob": 0.001867150072939694}, {"id": 845, "seek": 431904, "start": 4319.04, "end": 4323.36, "text": " multiplication, right? And so when you say that, you say, go do this big operation,", "tokens": [50364, 27290, 11, 558, 30, 400, 370, 562, 291, 584, 300, 11, 291, 584, 11, 352, 360, 341, 955, 6916, 11, 50580], "temperature": 0.0, "avg_logprob": -0.16239484151204428, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0014548404142260551}, {"id": 846, "seek": 431904, "start": 4323.36, "end": 4329.2, "text": " a matrix multiplication. And if it's on a GPU, kick off CUDA kernel, if it's on a CPU, go do", "tokens": [50580, 257, 8141, 27290, 13, 400, 498, 309, 311, 322, 257, 18407, 11, 4437, 766, 29777, 7509, 28256, 11, 498, 309, 311, 322, 257, 13199, 11, 352, 360, 50872], "temperature": 0.0, "avg_logprob": -0.16239484151204428, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0014548404142260551}, {"id": 847, "seek": 431904, "start": 4330.64, "end": 4333.76, "text": " like an Intel algorithm or something like that with the Intel MKL. Okay.", "tokens": [50944, 411, 364, 19762, 9284, 420, 746, 411, 300, 365, 264, 19762, 30770, 43, 13, 1033, 13, 51100], "temperature": 0.0, "avg_logprob": -0.16239484151204428, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0014548404142260551}, {"id": 848, "seek": 431904, "start": 4334.56, "end": 4341.12, "text": " Now, that's really cool. If you're either video or Intel, right? But then more hardware comes in.", "tokens": [51140, 823, 11, 300, 311, 534, 1627, 13, 759, 291, 434, 2139, 960, 420, 19762, 11, 558, 30, 583, 550, 544, 8837, 1487, 294, 13, 51468], "temperature": 0.0, "avg_logprob": -0.16239484151204428, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0014548404142260551}, {"id": 849, "seek": 431904, "start": 4342.16, "end": 4346.64, "text": " Right. And on one axis, you have more hardware coming in. On the other hand, you have", "tokens": [51520, 1779, 13, 400, 322, 472, 10298, 11, 291, 362, 544, 8837, 1348, 294, 13, 1282, 264, 661, 1011, 11, 291, 362, 51744], "temperature": 0.0, "avg_logprob": -0.16239484151204428, "compression_ratio": 1.6848249027237354, "no_speech_prob": 0.0014548404142260551}, {"id": 850, "seek": 434664, "start": 4346.64, "end": 4351.84, "text": " an explosion of innovation in AI. And so what happened with both TensorFlow and PyTorch is that", "tokens": [50364, 364, 15673, 295, 8504, 294, 7318, 13, 400, 370, 437, 2011, 365, 1293, 37624, 293, 9953, 51, 284, 339, 307, 300, 50624], "temperature": 0.0, "avg_logprob": -0.15191547427557212, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023226614575833082}, {"id": 851, "seek": 434664, "start": 4351.84, "end": 4356.64, "text": " the explosion of innovation in AI has led to, it's not just about matrix multiplication and", "tokens": [50624, 264, 15673, 295, 8504, 294, 7318, 575, 4684, 281, 11, 309, 311, 406, 445, 466, 8141, 27290, 293, 50864], "temperature": 0.0, "avg_logprob": -0.15191547427557212, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023226614575833082}, {"id": 852, "seek": 434664, "start": 4356.64, "end": 4361.360000000001, "text": " convolution. These things have now like 2,000 different operators. And on the other hand,", "tokens": [50864, 45216, 13, 1981, 721, 362, 586, 411, 568, 11, 1360, 819, 19077, 13, 400, 322, 264, 661, 1011, 11, 51100], "temperature": 0.0, "avg_logprob": -0.15191547427557212, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023226614575833082}, {"id": 853, "seek": 434664, "start": 4361.360000000001, "end": 4364.400000000001, "text": " you have, I don't know how many pieces of hardware there are there, it's a lot.", "tokens": [51100, 291, 362, 11, 286, 500, 380, 458, 577, 867, 3755, 295, 8837, 456, 366, 456, 11, 309, 311, 257, 688, 13, 51252], "temperature": 0.0, "avg_logprob": -0.15191547427557212, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023226614575833082}, {"id": 854, "seek": 434664, "start": 4365.4400000000005, "end": 4371.76, "text": " It's not even hundreds, it's probably thousands. Okay. And across all the edge and across all the", "tokens": [51304, 467, 311, 406, 754, 6779, 11, 309, 311, 1391, 5383, 13, 1033, 13, 400, 2108, 439, 264, 4691, 293, 2108, 439, 264, 51620], "temperature": 0.0, "avg_logprob": -0.15191547427557212, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023226614575833082}, {"id": 855, "seek": 437176, "start": 4371.76, "end": 4377.52, "text": " different things that are used at scale. Yeah, exactly. I mean, it's not just like everywhere.", "tokens": [50364, 819, 721, 300, 366, 1143, 412, 4373, 13, 865, 11, 2293, 13, 286, 914, 11, 309, 311, 406, 445, 411, 5315, 13, 50652], "temperature": 0.0, "avg_logprob": -0.22306483245092976, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.098037488758564}, {"id": 856, "seek": 437176, "start": 4377.52, "end": 4383.04, "text": " Yeah. It's not a handful of TPU alternatives. Correct. It's every phone, often with many", "tokens": [50652, 865, 13, 467, 311, 406, 257, 16458, 295, 314, 8115, 20478, 13, 12753, 13, 467, 311, 633, 2593, 11, 2049, 365, 867, 50928], "temperature": 0.0, "avg_logprob": -0.22306483245092976, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.098037488758564}, {"id": 857, "seek": 437176, "start": 4383.04, "end": 4390.0, "text": " different chips inside of it from different vendors. Right. Like it's, AI is everywhere.", "tokens": [50928, 819, 11583, 1854, 295, 309, 490, 819, 22056, 13, 1779, 13, 1743, 309, 311, 11, 7318, 307, 5315, 13, 51276], "temperature": 0.0, "avg_logprob": -0.22306483245092976, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.098037488758564}, {"id": 858, "seek": 437176, "start": 4390.0, "end": 4393.76, "text": " It's a thing, right? Why are they all making their own chips? Like, why is everybody making", "tokens": [51276, 467, 311, 257, 551, 11, 558, 30, 1545, 366, 436, 439, 1455, 641, 1065, 11583, 30, 1743, 11, 983, 307, 2201, 1455, 51464], "temperature": 0.0, "avg_logprob": -0.22306483245092976, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.098037488758564}, {"id": 859, "seek": 437176, "start": 4393.76, "end": 4398.88, "text": " their own thing? Well, so because, was that a good thing? First of all, so Chris's philosophy on", "tokens": [51464, 641, 1065, 551, 30, 1042, 11, 370, 570, 11, 390, 300, 257, 665, 551, 30, 2386, 295, 439, 11, 370, 6688, 311, 10675, 322, 51720], "temperature": 0.0, "avg_logprob": -0.22306483245092976, "compression_ratio": 1.6886446886446886, "no_speech_prob": 0.098037488758564}, {"id": 860, "seek": 439888, "start": 4398.88, "end": 4405.4400000000005, "text": " hardware. Yeah. Right. So my philosophy is that there isn't one right solution. Right. And so I", "tokens": [50364, 8837, 13, 865, 13, 1779, 13, 407, 452, 10675, 307, 300, 456, 1943, 380, 472, 558, 3827, 13, 1779, 13, 400, 370, 286, 50692], "temperature": 0.0, "avg_logprob": -0.1009824154740673, "compression_ratio": 1.7748091603053435, "no_speech_prob": 0.032082509249448776}, {"id": 861, "seek": 439888, "start": 4405.4400000000005, "end": 4410.400000000001, "text": " think that, again, we're at the end of Moore's Law, specialization happens. Yeah. If you're", "tokens": [50692, 519, 300, 11, 797, 11, 321, 434, 412, 264, 917, 295, 21644, 311, 7744, 11, 2121, 2144, 2314, 13, 865, 13, 759, 291, 434, 50940], "temperature": 0.0, "avg_logprob": -0.1009824154740673, "compression_ratio": 1.7748091603053435, "no_speech_prob": 0.032082509249448776}, {"id": 862, "seek": 439888, "start": 4410.400000000001, "end": 4417.28, "text": " building, if you're training GPT-5, he wants some crazy supercomputer data center thingy.", "tokens": [50940, 2390, 11, 498, 291, 434, 3097, 26039, 51, 12, 20, 11, 415, 2738, 512, 3219, 36708, 1412, 3056, 551, 88, 13, 51284], "temperature": 0.0, "avg_logprob": -0.1009824154740673, "compression_ratio": 1.7748091603053435, "no_speech_prob": 0.032082509249448776}, {"id": 863, "seek": 439888, "start": 4417.92, "end": 4422.8, "text": " If you're making a smart camera that runs on batteries, you want something that looks very", "tokens": [51316, 759, 291, 434, 1455, 257, 4069, 2799, 300, 6676, 322, 13070, 11, 291, 528, 746, 300, 1542, 588, 51560], "temperature": 0.0, "avg_logprob": -0.1009824154740673, "compression_ratio": 1.7748091603053435, "no_speech_prob": 0.032082509249448776}, {"id": 864, "seek": 439888, "start": 4422.8, "end": 4426.24, "text": " different. If you're building a phone, you want something that looks very different. If you have", "tokens": [51560, 819, 13, 759, 291, 434, 2390, 257, 2593, 11, 291, 528, 746, 300, 1542, 588, 819, 13, 759, 291, 362, 51732], "temperature": 0.0, "avg_logprob": -0.1009824154740673, "compression_ratio": 1.7748091603053435, "no_speech_prob": 0.032082509249448776}, {"id": 865, "seek": 442624, "start": 4426.32, "end": 4431.04, "text": " something like a laptop, you want something that looks maybe similar, but a different scale. Right.", "tokens": [50368, 746, 411, 257, 10732, 11, 291, 528, 746, 300, 1542, 1310, 2531, 11, 457, 257, 819, 4373, 13, 1779, 13, 50604], "temperature": 0.0, "avg_logprob": -0.10807904013752068, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004608729854226112}, {"id": 866, "seek": 442624, "start": 4431.04, "end": 4436.5599999999995, "text": " And so AI ends up touching all of our lives, robotics. Right. And like, lots of different", "tokens": [50604, 400, 370, 7318, 5314, 493, 11175, 439, 295, 527, 2909, 11, 34145, 13, 1779, 13, 400, 411, 11, 3195, 295, 819, 50880], "temperature": 0.0, "avg_logprob": -0.10807904013752068, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004608729854226112}, {"id": 867, "seek": 442624, "start": 4436.5599999999995, "end": 4441.44, "text": " things. And so as you look into this, these have different power envelopes. There's different", "tokens": [50880, 721, 13, 400, 370, 382, 291, 574, 666, 341, 11, 613, 362, 819, 1347, 33860, 279, 13, 821, 311, 819, 51124], "temperature": 0.0, "avg_logprob": -0.10807904013752068, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004608729854226112}, {"id": 868, "seek": 442624, "start": 4441.44, "end": 4445.599999999999, "text": " trade-offs in terms of the algorithms. There's new innovations in sparsity and other data formats", "tokens": [51124, 4923, 12, 19231, 294, 2115, 295, 264, 14642, 13, 821, 311, 777, 24283, 294, 637, 685, 507, 293, 661, 1412, 25879, 51332], "temperature": 0.0, "avg_logprob": -0.10807904013752068, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004608729854226112}, {"id": 869, "seek": 442624, "start": 4445.599999999999, "end": 4450.88, "text": " and things like that. And so hardware innovation, I think is a really good thing. Right. And what", "tokens": [51332, 293, 721, 411, 300, 13, 400, 370, 8837, 8504, 11, 286, 519, 307, 257, 534, 665, 551, 13, 1779, 13, 400, 437, 51596], "temperature": 0.0, "avg_logprob": -0.10807904013752068, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004608729854226112}, {"id": 870, "seek": 442624, "start": 4450.88, "end": 4454.639999999999, "text": " I'm interested in is unlocking that innovation. There's also like analog and quantum and like,", "tokens": [51596, 286, 478, 3102, 294, 307, 49620, 300, 8504, 13, 821, 311, 611, 411, 16660, 293, 13018, 293, 411, 11, 51784], "temperature": 0.0, "avg_logprob": -0.10807904013752068, "compression_ratio": 1.881967213114754, "no_speech_prob": 0.004608729854226112}, {"id": 871, "seek": 445464, "start": 4454.64, "end": 4460.72, "text": " although the really weird stuff. Right. And so if somebody can come up with a chip that uses", "tokens": [50364, 4878, 264, 534, 3657, 1507, 13, 1779, 13, 400, 370, 498, 2618, 393, 808, 493, 365, 257, 11409, 300, 4960, 50668], "temperature": 0.0, "avg_logprob": -0.11431924228010507, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.00041729610529728234}, {"id": 872, "seek": 445464, "start": 4460.72, "end": 4465.4400000000005, "text": " analog computing and it's 100x more power efficient, I think what that would mean in terms of the", "tokens": [50668, 16660, 15866, 293, 309, 311, 2319, 87, 544, 1347, 7148, 11, 286, 519, 437, 300, 576, 914, 294, 2115, 295, 264, 50904], "temperature": 0.0, "avg_logprob": -0.11431924228010507, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.00041729610529728234}, {"id": 873, "seek": 445464, "start": 4465.4400000000005, "end": 4472.08, "text": " daily impact on the products we use, that'd be huge. Now, if you're building an analog computer,", "tokens": [50904, 5212, 2712, 322, 264, 3383, 321, 764, 11, 300, 1116, 312, 2603, 13, 823, 11, 498, 291, 434, 2390, 364, 16660, 3820, 11, 51236], "temperature": 0.0, "avg_logprob": -0.11431924228010507, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.00041729610529728234}, {"id": 874, "seek": 445464, "start": 4472.08, "end": 4477.12, "text": " you may not be a compiler specialist. Right. These are different skill sets. Right. And so", "tokens": [51236, 291, 815, 406, 312, 257, 31958, 17008, 13, 1779, 13, 1981, 366, 819, 5389, 6352, 13, 1779, 13, 400, 370, 51488], "temperature": 0.0, "avg_logprob": -0.11431924228010507, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.00041729610529728234}, {"id": 875, "seek": 445464, "start": 4477.12, "end": 4481.200000000001, "text": " you can hire some compiler people if you're running a big company, maybe. But it turns out", "tokens": [51488, 291, 393, 11158, 512, 31958, 561, 498, 291, 434, 2614, 257, 955, 2237, 11, 1310, 13, 583, 309, 4523, 484, 51692], "temperature": 0.0, "avg_logprob": -0.11431924228010507, "compression_ratio": 1.6631205673758864, "no_speech_prob": 0.00041729610529728234}, {"id": 876, "seek": 448120, "start": 4481.92, "end": 4487.92, "text": " these are really like exotic new generation of compilers. Like this is a different thing. Right.", "tokens": [50400, 613, 366, 534, 411, 27063, 777, 5125, 295, 715, 388, 433, 13, 1743, 341, 307, 257, 819, 551, 13, 1779, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1308530467425206, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0014102108543738723}, {"id": 877, "seek": 448120, "start": 4487.92, "end": 4492.639999999999, "text": " And so if you take a step back out and come back to it, what is the status quo? The status quo is", "tokens": [50700, 400, 370, 498, 291, 747, 257, 1823, 646, 484, 293, 808, 646, 281, 309, 11, 437, 307, 264, 6558, 28425, 30, 440, 6558, 28425, 307, 50936], "temperature": 0.0, "avg_logprob": -0.1308530467425206, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0014102108543738723}, {"id": 878, "seek": 448120, "start": 4492.639999999999, "end": 4498.0, "text": " that if you're Intel or you're Nvidia, you continue to keep up with the industry and you chase and,", "tokens": [50936, 300, 498, 291, 434, 19762, 420, 291, 434, 46284, 11, 291, 2354, 281, 1066, 493, 365, 264, 3518, 293, 291, 15359, 293, 11, 51204], "temperature": 0.0, "avg_logprob": -0.1308530467425206, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0014102108543738723}, {"id": 879, "seek": 448120, "start": 4498.0, "end": 4502.96, "text": " okay, there's 1900 now. There's 2000 now. There's 2100. And you have a huge team of people that", "tokens": [51204, 1392, 11, 456, 311, 28898, 586, 13, 821, 311, 8132, 586, 13, 821, 311, 5080, 628, 13, 400, 291, 362, 257, 2603, 1469, 295, 561, 300, 51452], "temperature": 0.0, "avg_logprob": -0.1308530467425206, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0014102108543738723}, {"id": 880, "seek": 448120, "start": 4502.96, "end": 4507.84, "text": " are like trying to keep up and tune and optimize. And even when one of the big guys comes out with", "tokens": [51452, 366, 411, 1382, 281, 1066, 493, 293, 10864, 293, 19719, 13, 400, 754, 562, 472, 295, 264, 955, 1074, 1487, 484, 365, 51696], "temperature": 0.0, "avg_logprob": -0.1308530467425206, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.0014102108543738723}, {"id": 881, "seek": 450784, "start": 4507.84, "end": 4512.56, "text": " a new generation of their chip, they have to go back and rewrite all these things. Right. So really,", "tokens": [50364, 257, 777, 5125, 295, 641, 11409, 11, 436, 362, 281, 352, 646, 293, 28132, 439, 613, 721, 13, 1779, 13, 407, 534, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1127014021942581, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.007576455362141132}, {"id": 882, "seek": 450784, "start": 4512.56, "end": 4517.12, "text": " it's only powered by having hundreds of people that are all like frantically trying to keep up.", "tokens": [50600, 309, 311, 787, 17786, 538, 1419, 6779, 295, 561, 300, 366, 439, 411, 431, 49505, 1382, 281, 1066, 493, 13, 50828], "temperature": 0.0, "avg_logprob": -0.1127014021942581, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.007576455362141132}, {"id": 883, "seek": 450784, "start": 4517.12, "end": 4521.12, "text": " And what that does is that keeps out the little guys. And sometimes they're not so little guys,", "tokens": [50828, 400, 437, 300, 775, 307, 300, 5965, 484, 264, 707, 1074, 13, 400, 2171, 436, 434, 406, 370, 707, 1074, 11, 51028], "temperature": 0.0, "avg_logprob": -0.1127014021942581, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.007576455362141132}, {"id": 884, "seek": 450784, "start": 4521.12, "end": 4528.0, "text": " the big guys that are also just not in those dominant positions. And so what has been happening,", "tokens": [51028, 264, 955, 1074, 300, 366, 611, 445, 406, 294, 729, 15657, 8432, 13, 400, 370, 437, 575, 668, 2737, 11, 51372], "temperature": 0.0, "avg_logprob": -0.1127014021942581, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.007576455362141132}, {"id": 885, "seek": 450784, "start": 4528.0, "end": 4532.56, "text": " and so a lot of you talk about the rise of new exotic crazy accelerators is people have been", "tokens": [51372, 293, 370, 257, 688, 295, 291, 751, 466, 264, 6272, 295, 777, 27063, 3219, 10172, 3391, 307, 561, 362, 668, 51600], "temperature": 0.0, "avg_logprob": -0.1127014021942581, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.007576455362141132}, {"id": 886, "seek": 450784, "start": 4532.56, "end": 4537.52, "text": " trying to turn this from a let's go write lots of special kernels problem into a compiler problem.", "tokens": [51600, 1382, 281, 1261, 341, 490, 257, 718, 311, 352, 2464, 3195, 295, 2121, 23434, 1625, 1154, 666, 257, 31958, 1154, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1127014021942581, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.007576455362141132}, {"id": 887, "seek": 453784, "start": 4538.56, "end": 4543.76, "text": " And so we and I contributed to this as well. We as an industry went into it like let's go make", "tokens": [50400, 400, 370, 321, 293, 286, 18434, 281, 341, 382, 731, 13, 492, 382, 364, 3518, 1437, 666, 309, 411, 718, 311, 352, 652, 50660], "temperature": 0.0, "avg_logprob": -0.11194398761850542, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00018521318270359188}, {"id": 888, "seek": 453784, "start": 4543.76, "end": 4549.2, "text": " this compiler problem phase, let's call it. And much of the industry is still in this phase,", "tokens": [50660, 341, 31958, 1154, 5574, 11, 718, 311, 818, 309, 13, 400, 709, 295, 264, 3518, 307, 920, 294, 341, 5574, 11, 50932], "temperature": 0.0, "avg_logprob": -0.11194398761850542, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00018521318270359188}, {"id": 889, "seek": 453784, "start": 4549.2, "end": 4553.92, "text": " by the way. So I wouldn't say this phase is over. And so the idea is to say, look, okay,", "tokens": [50932, 538, 264, 636, 13, 407, 286, 2759, 380, 584, 341, 5574, 307, 670, 13, 400, 370, 264, 1558, 307, 281, 584, 11, 574, 11, 1392, 11, 51168], "temperature": 0.0, "avg_logprob": -0.11194398761850542, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00018521318270359188}, {"id": 890, "seek": 453784, "start": 4554.64, "end": 4558.16, "text": " what a compiler does is it provides a much more general, extensible,", "tokens": [51204, 437, 257, 31958, 775, 307, 309, 6417, 257, 709, 544, 2674, 11, 1279, 30633, 11, 51380], "temperature": 0.0, "avg_logprob": -0.11194398761850542, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00018521318270359188}, {"id": 891, "seek": 453784, "start": 4560.0, "end": 4567.360000000001, "text": " hackable interface for dealing with the general case. Right. And so within machine", "tokens": [51472, 10339, 712, 9226, 337, 6260, 365, 264, 2674, 1389, 13, 1779, 13, 400, 370, 1951, 3479, 51840], "temperature": 0.0, "avg_logprob": -0.11194398761850542, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.00018521318270359188}, {"id": 892, "seek": 456736, "start": 4567.36, "end": 4571.599999999999, "text": " learning algorithms, for example, people figured out that, hey, if I do a matrix multiplication,", "tokens": [50364, 2539, 14642, 11, 337, 1365, 11, 561, 8932, 484, 300, 11, 4177, 11, 498, 286, 360, 257, 8141, 27290, 11, 50576], "temperature": 0.0, "avg_logprob": -0.21196538460354844, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0013668438186869025}, {"id": 893, "seek": 456736, "start": 4571.599999999999, "end": 4579.44, "text": " I do a value, right, the classic activation function, it is way faster to do one pass over", "tokens": [50576, 286, 360, 257, 2158, 11, 558, 11, 264, 7230, 24433, 2445, 11, 309, 307, 636, 4663, 281, 360, 472, 1320, 670, 50968], "temperature": 0.0, "avg_logprob": -0.21196538460354844, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0013668438186869025}, {"id": 894, "seek": 456736, "start": 4579.44, "end": 4584.4, "text": " the data and then do the value on the output, where I'm writing out the data, because really,", "tokens": [50968, 264, 1412, 293, 550, 360, 264, 2158, 322, 264, 5598, 11, 689, 286, 478, 3579, 484, 264, 1412, 11, 570, 534, 11, 51216], "temperature": 0.0, "avg_logprob": -0.21196538460354844, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0013668438186869025}, {"id": 895, "seek": 456736, "start": 4584.4, "end": 4590.799999999999, "text": " it's just a maximum operation, right, max is zero. And so it's an amazing optimization, take", "tokens": [51216, 309, 311, 445, 257, 6674, 6916, 11, 558, 11, 11469, 307, 4018, 13, 400, 370, 309, 311, 364, 2243, 19618, 11, 747, 51536], "temperature": 0.0, "avg_logprob": -0.21196538460354844, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0013668438186869025}, {"id": 896, "seek": 456736, "start": 4590.799999999999, "end": 4596.4, "text": " map more value, squish together one operation, now I have map more value. Well, wait a second,", "tokens": [51536, 4471, 544, 2158, 11, 31379, 1214, 472, 6916, 11, 586, 286, 362, 4471, 544, 2158, 13, 1042, 11, 1699, 257, 1150, 11, 51816], "temperature": 0.0, "avg_logprob": -0.21196538460354844, "compression_ratio": 1.7054545454545456, "no_speech_prob": 0.0013668438186869025}, {"id": 897, "seek": 459640, "start": 4596.4, "end": 4600.16, "text": " if I do that, now I just went from having, you know, two operators to three.", "tokens": [50364, 498, 286, 360, 300, 11, 586, 286, 445, 1437, 490, 1419, 11, 291, 458, 11, 732, 19077, 281, 1045, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11632101503137039, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0011334000155329704}, {"id": 898, "seek": 459640, "start": 4600.799999999999, "end": 4604.0, "text": " But now I figure out, okay, well, there's a lot of activation functions. What about", "tokens": [50584, 583, 586, 286, 2573, 484, 11, 1392, 11, 731, 11, 456, 311, 257, 688, 295, 24433, 6828, 13, 708, 466, 50744], "temperature": 0.0, "avg_logprob": -0.11632101503137039, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0011334000155329704}, {"id": 899, "seek": 459640, "start": 4605.679999999999, "end": 4610.719999999999, "text": " leaky value? What about like a million things that are out there, right. And so as I start fusing", "tokens": [50828, 476, 15681, 2158, 30, 708, 466, 411, 257, 2459, 721, 300, 366, 484, 456, 11, 558, 13, 400, 370, 382, 286, 722, 283, 7981, 51080], "temperature": 0.0, "avg_logprob": -0.11632101503137039, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0011334000155329704}, {"id": 900, "seek": 459640, "start": 4610.719999999999, "end": 4615.12, "text": " these in, now I get permutations of all these algorithms, right. And so what the compiler", "tokens": [51080, 613, 294, 11, 586, 286, 483, 4784, 325, 763, 295, 439, 613, 14642, 11, 558, 13, 400, 370, 437, 264, 31958, 51300], "temperature": 0.0, "avg_logprob": -0.11632101503137039, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0011334000155329704}, {"id": 901, "seek": 459640, "start": 4615.12, "end": 4619.28, "text": " people said is they said, hey, cool, I will go enumerate all the algorithms and I will enumerate", "tokens": [51300, 561, 848, 307, 436, 848, 11, 4177, 11, 1627, 11, 286, 486, 352, 465, 15583, 473, 439, 264, 14642, 293, 286, 486, 465, 15583, 473, 51508], "temperature": 0.0, "avg_logprob": -0.11632101503137039, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0011334000155329704}, {"id": 902, "seek": 459640, "start": 4619.28, "end": 4624.0, "text": " all the pairs and I will actually generate a kernel for you. And I think that this has been very,", "tokens": [51508, 439, 264, 15494, 293, 286, 486, 767, 8460, 257, 28256, 337, 291, 13, 400, 286, 519, 300, 341, 575, 668, 588, 11, 51744], "temperature": 0.0, "avg_logprob": -0.11632101503137039, "compression_ratio": 1.8344594594594594, "no_speech_prob": 0.0011334000155329704}, {"id": 903, "seek": 462400, "start": 4624.0, "end": 4627.28, "text": " very useful for the industry. This is one of the things that powers Google TPUs,", "tokens": [50364, 588, 4420, 337, 264, 3518, 13, 639, 307, 472, 295, 264, 721, 300, 8674, 3329, 314, 8115, 82, 11, 50528], "temperature": 0.0, "avg_logprob": -0.1240387103136848, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.011684714816510677}, {"id": 904, "seek": 462400, "start": 4628.0, "end": 4632.8, "text": " PyTorch 2s, like rolling out really cool compiler stuff with Triton, this other technology and", "tokens": [50564, 9953, 51, 284, 339, 568, 82, 11, 411, 9439, 484, 534, 1627, 31958, 1507, 365, 1765, 270, 266, 11, 341, 661, 2899, 293, 50804], "temperature": 0.0, "avg_logprob": -0.1240387103136848, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.011684714816510677}, {"id": 905, "seek": 462400, "start": 4632.8, "end": 4637.44, "text": " things like this. And so the compiler people are kind of coming into their fore and saying like,", "tokens": [50804, 721, 411, 341, 13, 400, 370, 264, 31958, 561, 366, 733, 295, 1348, 666, 641, 2091, 293, 1566, 411, 11, 51036], "temperature": 0.0, "avg_logprob": -0.1240387103136848, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.011684714816510677}, {"id": 906, "seek": 462400, "start": 4637.44, "end": 4642.72, "text": " awesome, this is a compiler problem, we'll compiler it. Here's the problem. Not everybody's", "tokens": [51036, 3476, 11, 341, 307, 257, 31958, 1154, 11, 321, 603, 31958, 309, 13, 1692, 311, 264, 1154, 13, 1726, 2201, 311, 51300], "temperature": 0.0, "avg_logprob": -0.1240387103136848, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.011684714816510677}, {"id": 907, "seek": 462400, "start": 4642.72, "end": 4647.04, "text": " a compiler person. I love compiler people, trust me, right, but not everybody can or should be a", "tokens": [51300, 257, 31958, 954, 13, 286, 959, 31958, 561, 11, 3361, 385, 11, 558, 11, 457, 406, 2201, 393, 420, 820, 312, 257, 51516], "temperature": 0.0, "avg_logprob": -0.1240387103136848, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.011684714816510677}, {"id": 908, "seek": 462400, "start": 4647.04, "end": 4651.76, "text": " compiler person. It turns out that there are people that know analog computers really well,", "tokens": [51516, 31958, 954, 13, 467, 4523, 484, 300, 456, 366, 561, 300, 458, 16660, 10807, 534, 731, 11, 51752], "temperature": 0.0, "avg_logprob": -0.1240387103136848, "compression_ratio": 1.8131147540983608, "no_speech_prob": 0.011684714816510677}, {"id": 909, "seek": 465176, "start": 4651.84, "end": 4657.6, "text": " or they know some GPU internal architecture thing really well, or they know some crazy, sparse,", "tokens": [50368, 420, 436, 458, 512, 18407, 6920, 9482, 551, 534, 731, 11, 420, 436, 458, 512, 3219, 11, 637, 11668, 11, 50656], "temperature": 0.0, "avg_logprob": -0.09983611728834069, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.006691226735711098}, {"id": 910, "seek": 465176, "start": 4657.6, "end": 4663.52, "text": " numeric, interesting algorithm that is the cusp of research, but they're not compiler people.", "tokens": [50656, 7866, 299, 11, 1880, 9284, 300, 307, 264, 269, 22490, 295, 2132, 11, 457, 436, 434, 406, 31958, 561, 13, 50952], "temperature": 0.0, "avg_logprob": -0.09983611728834069, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.006691226735711098}, {"id": 911, "seek": 465176, "start": 4663.52, "end": 4667.6, "text": " And so one of the challenges with this new wave of technology trying to turn everything into a", "tokens": [50952, 400, 370, 472, 295, 264, 4759, 365, 341, 777, 5772, 295, 2899, 1382, 281, 1261, 1203, 666, 257, 51156], "temperature": 0.0, "avg_logprob": -0.09983611728834069, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.006691226735711098}, {"id": 912, "seek": 465176, "start": 4667.6, "end": 4673.76, "text": " compiler is again, it is excluded a ton of people. And so you look at what does Mojo do, what does", "tokens": [51156, 31958, 307, 797, 11, 309, 307, 29486, 257, 2952, 295, 561, 13, 400, 370, 291, 574, 412, 437, 775, 3335, 5134, 360, 11, 437, 775, 51464], "temperature": 0.0, "avg_logprob": -0.09983611728834069, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.006691226735711098}, {"id": 913, "seek": 465176, "start": 4673.76, "end": 4679.4400000000005, "text": " the modular stack do is brings programmability back into this world, like it enables, I wouldn't", "tokens": [51464, 264, 31111, 8630, 360, 307, 5607, 37648, 2310, 646, 666, 341, 1002, 11, 411, 309, 17077, 11, 286, 2759, 380, 51748], "temperature": 0.0, "avg_logprob": -0.09983611728834069, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.006691226735711098}, {"id": 914, "seek": 467944, "start": 4679.44, "end": 4684.879999999999, "text": " say normal people, but a different kind of delightful nerd that cares about numerics or", "tokens": [50364, 584, 2710, 561, 11, 457, 257, 819, 733, 295, 35194, 23229, 300, 12310, 466, 7866, 1167, 420, 50636], "temperature": 0.0, "avg_logprob": -0.14702482390822025, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.010325762443244457}, {"id": 915, "seek": 467944, "start": 4684.879999999999, "end": 4688.879999999999, "text": " cares about hardware or cares about things like this to be able to express that in the stack and", "tokens": [50636, 12310, 466, 8837, 420, 12310, 466, 721, 411, 341, 281, 312, 1075, 281, 5109, 300, 294, 264, 8630, 293, 50836], "temperature": 0.0, "avg_logprob": -0.14702482390822025, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.010325762443244457}, {"id": 916, "seek": 467944, "start": 4688.879999999999, "end": 4694.799999999999, "text": " extend the stack without having to actually go hack the compiler itself. So extend the stack on the", "tokens": [50836, 10101, 264, 8630, 1553, 1419, 281, 767, 352, 10339, 264, 31958, 2564, 13, 407, 10101, 264, 8630, 322, 264, 51132], "temperature": 0.0, "avg_logprob": -0.14702482390822025, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.010325762443244457}, {"id": 917, "seek": 467944, "start": 4694.799999999999, "end": 4700.5599999999995, "text": " algorithm side, and then on the hardware side. Yeah, so again, go back to the simplest example", "tokens": [51132, 9284, 1252, 11, 293, 550, 322, 264, 8837, 1252, 13, 865, 11, 370, 797, 11, 352, 646, 281, 264, 22811, 1365, 51420], "temperature": 0.0, "avg_logprob": -0.14702482390822025, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.010325762443244457}, {"id": 918, "seek": 467944, "start": 4700.5599999999995, "end": 4706.5599999999995, "text": " of int. And so both Swift and Mojo and other things like this did is we said, okay, pull magic out of", "tokens": [51420, 295, 560, 13, 400, 370, 1293, 25539, 293, 3335, 5134, 293, 661, 721, 411, 341, 630, 307, 321, 848, 11, 1392, 11, 2235, 5585, 484, 295, 51720], "temperature": 0.0, "avg_logprob": -0.14702482390822025, "compression_ratio": 1.8150943396226416, "no_speech_prob": 0.010325762443244457}, {"id": 919, "seek": 470656, "start": 4706.56, "end": 4710.96, "text": " the compiler and put it in the standard library. And so what modular is doing with the engine that", "tokens": [50364, 264, 31958, 293, 829, 309, 294, 264, 3832, 6405, 13, 400, 370, 437, 31111, 307, 884, 365, 264, 2848, 300, 50584], "temperature": 0.0, "avg_logprob": -0.12956126960548195, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.009705954231321812}, {"id": 920, "seek": 470656, "start": 4710.96, "end": 4716.080000000001, "text": " we're providing and like this, this very deep technology stack, right, which goes into heterogeneous", "tokens": [50584, 321, 434, 6530, 293, 411, 341, 11, 341, 588, 2452, 2899, 8630, 11, 558, 11, 597, 1709, 666, 20789, 31112, 50840], "temperature": 0.0, "avg_logprob": -0.12956126960548195, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.009705954231321812}, {"id": 921, "seek": 470656, "start": 4716.080000000001, "end": 4722.56, "text": " runtimes and like a whole bunch of really cool, really cool things. This whole stack allows that", "tokens": [50840, 49435, 1532, 293, 411, 257, 1379, 3840, 295, 534, 1627, 11, 534, 1627, 721, 13, 639, 1379, 8630, 4045, 300, 51164], "temperature": 0.0, "avg_logprob": -0.12956126960548195, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.009705954231321812}, {"id": 922, "seek": 470656, "start": 4722.56, "end": 4728.160000000001, "text": " stack to be extended and hacked and changed by researchers and by hardware innovators and by", "tokens": [51164, 8630, 281, 312, 10913, 293, 36218, 293, 3105, 538, 10309, 293, 538, 8837, 5083, 3391, 293, 538, 51444], "temperature": 0.0, "avg_logprob": -0.12956126960548195, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.009705954231321812}, {"id": 923, "seek": 470656, "start": 4728.160000000001, "end": 4733.280000000001, "text": " people who know things that we don't know, because you know, modular has some smart people, but we", "tokens": [51444, 561, 567, 458, 721, 300, 321, 500, 380, 458, 11, 570, 291, 458, 11, 31111, 575, 512, 4069, 561, 11, 457, 321, 51700], "temperature": 0.0, "avg_logprob": -0.12956126960548195, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.009705954231321812}, {"id": 924, "seek": 473328, "start": 4733.28, "end": 4740.719999999999, "text": " don't have all the smart people, it turns out. What are heterogeneous runtimes? Yeah, so what is", "tokens": [50364, 500, 380, 362, 439, 264, 4069, 561, 11, 309, 4523, 484, 13, 708, 366, 20789, 31112, 49435, 1532, 30, 865, 11, 370, 437, 307, 50736], "temperature": 0.0, "avg_logprob": -0.09352245175741553, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0032726749777793884}, {"id": 925, "seek": 473328, "start": 4740.719999999999, "end": 4745.5199999999995, "text": " heterogeneous, right? So heterogeneous means many different kinds of things together. And so", "tokens": [50736, 20789, 31112, 11, 558, 30, 407, 20789, 31112, 1355, 867, 819, 3685, 295, 721, 1214, 13, 400, 370, 50976], "temperature": 0.0, "avg_logprob": -0.09352245175741553, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0032726749777793884}, {"id": 926, "seek": 473328, "start": 4745.5199999999995, "end": 4751.36, "text": " the simplest example you might come up with is a CPU and a GPU. And so it's a simple heterogeneous", "tokens": [50976, 264, 22811, 1365, 291, 1062, 808, 493, 365, 307, 257, 13199, 293, 257, 18407, 13, 400, 370, 309, 311, 257, 2199, 20789, 31112, 51268], "temperature": 0.0, "avg_logprob": -0.09352245175741553, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0032726749777793884}, {"id": 927, "seek": 473328, "start": 4751.36, "end": 4756.719999999999, "text": " computer to say, I will run my data loading and preprocessing and other algorithms on the CPU.", "tokens": [51268, 3820, 281, 584, 11, 286, 486, 1190, 452, 1412, 15114, 293, 2666, 340, 780, 278, 293, 661, 14642, 322, 264, 13199, 13, 51536], "temperature": 0.0, "avg_logprob": -0.09352245175741553, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0032726749777793884}, {"id": 928, "seek": 473328, "start": 4756.719999999999, "end": 4760.48, "text": " And then once I get it into the right shape, I shove it into the GPU, I do a lot of matrix", "tokens": [51536, 400, 550, 1564, 286, 483, 309, 666, 264, 558, 3909, 11, 286, 35648, 309, 666, 264, 18407, 11, 286, 360, 257, 688, 295, 8141, 51724], "temperature": 0.0, "avg_logprob": -0.09352245175741553, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0032726749777793884}, {"id": 929, "seek": 476048, "start": 4760.48, "end": 4766.4, "text": " multiplications and convolutions and things like this. And I get it back out and I do some reductions", "tokens": [50364, 17596, 763, 293, 3754, 15892, 293, 721, 411, 341, 13, 400, 286, 483, 309, 646, 484, 293, 286, 360, 512, 40296, 50660], "temperature": 0.0, "avg_logprob": -0.12891260106512842, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.005384007003158331}, {"id": 930, "seek": 476048, "start": 4766.4, "end": 4771.2, "text": " and summaries and they shove it across the wire to across the network to another machine, right?", "tokens": [50660, 293, 8367, 4889, 293, 436, 35648, 309, 2108, 264, 6234, 281, 2108, 264, 3209, 281, 1071, 3479, 11, 558, 30, 50900], "temperature": 0.0, "avg_logprob": -0.12891260106512842, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.005384007003158331}, {"id": 931, "seek": 476048, "start": 4771.2, "end": 4778.32, "text": " And so you've got now what are effectively two computers, a CPU and a GPU talking to each other,", "tokens": [50900, 400, 370, 291, 600, 658, 586, 437, 366, 8659, 732, 10807, 11, 257, 13199, 293, 257, 18407, 1417, 281, 1184, 661, 11, 51256], "temperature": 0.0, "avg_logprob": -0.12891260106512842, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.005384007003158331}, {"id": 932, "seek": 476048, "start": 4778.32, "end": 4786.16, "text": " working together in a heterogeneous system. But that was 10 years ago. Okay, look at a modern", "tokens": [51256, 1364, 1214, 294, 257, 20789, 31112, 1185, 13, 583, 300, 390, 1266, 924, 2057, 13, 1033, 11, 574, 412, 257, 4363, 51648], "temperature": 0.0, "avg_logprob": -0.12891260106512842, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.005384007003158331}, {"id": 933, "seek": 478616, "start": 4786.16, "end": 4791.92, "text": " cell phone. Modern cell phone, you've got CPUs. And they're not just CPUs, there's like big dot", "tokens": [50364, 2815, 2593, 13, 19814, 2815, 2593, 11, 291, 600, 658, 13199, 82, 13, 400, 436, 434, 406, 445, 13199, 82, 11, 456, 311, 411, 955, 5893, 50652], "temperature": 0.0, "avg_logprob": -0.13311178315945757, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.05030288174748421}, {"id": 934, "seek": 478616, "start": 4791.92, "end": 4796.24, "text": " little CPUs. And so there's multiple different kinds of CPUs are working together. They're", "tokens": [50652, 707, 13199, 82, 13, 400, 370, 456, 311, 3866, 819, 3685, 295, 13199, 82, 366, 1364, 1214, 13, 814, 434, 50868], "temperature": 0.0, "avg_logprob": -0.13311178315945757, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.05030288174748421}, {"id": 935, "seek": 478616, "start": 4796.24, "end": 4802.88, "text": " multi core. You've got GPUs, you've got neural network accelerators, you got dedicated hardware", "tokens": [50868, 4825, 4965, 13, 509, 600, 658, 18407, 82, 11, 291, 600, 658, 18161, 3209, 10172, 3391, 11, 291, 658, 8374, 8837, 51200], "temperature": 0.0, "avg_logprob": -0.13311178315945757, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.05030288174748421}, {"id": 936, "seek": 478616, "start": 4802.88, "end": 4807.76, "text": " blocks for for media. So for video decode and JPEG decode and things like this. And so you've", "tokens": [51200, 8474, 337, 337, 3021, 13, 407, 337, 960, 979, 1429, 293, 508, 5208, 38, 979, 1429, 293, 721, 411, 341, 13, 400, 370, 291, 600, 51444], "temperature": 0.0, "avg_logprob": -0.13311178315945757, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.05030288174748421}, {"id": 937, "seek": 478616, "start": 4807.76, "end": 4811.92, "text": " got this massively complicated system. And this isn't just cell phones, every laptop these days is", "tokens": [51444, 658, 341, 29379, 6179, 1185, 13, 400, 341, 1943, 380, 445, 2815, 10216, 11, 633, 10732, 613, 1708, 307, 51652], "temperature": 0.0, "avg_logprob": -0.13311178315945757, "compression_ratio": 1.7723880597014925, "no_speech_prob": 0.05030288174748421}, {"id": 938, "seek": 481192, "start": 4811.92, "end": 4819.28, "text": " doing the same thing. And all these blocks can run at the same time and need to be choreographed,", "tokens": [50364, 884, 264, 912, 551, 13, 400, 439, 613, 8474, 393, 1190, 412, 264, 912, 565, 293, 643, 281, 312, 14625, 3108, 292, 11, 50732], "temperature": 0.0, "avg_logprob": -0.08565778732299804, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.022971652448177338}, {"id": 939, "seek": 481192, "start": 4819.28, "end": 4823.36, "text": " right? And so again, one of the cool things about machine learning is it's moving things to like", "tokens": [50732, 558, 30, 400, 370, 797, 11, 472, 295, 264, 1627, 721, 466, 3479, 2539, 307, 309, 311, 2684, 721, 281, 411, 50936], "temperature": 0.0, "avg_logprob": -0.08565778732299804, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.022971652448177338}, {"id": 940, "seek": 481192, "start": 4823.36, "end": 4828.72, "text": " data flow graphs and higher level of abstractions and tensors and these things that it doesn't specify.", "tokens": [50936, 1412, 3095, 24877, 293, 2946, 1496, 295, 12649, 626, 293, 10688, 830, 293, 613, 721, 300, 309, 1177, 380, 16500, 13, 51204], "temperature": 0.0, "avg_logprob": -0.08565778732299804, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.022971652448177338}, {"id": 941, "seek": 481192, "start": 4828.72, "end": 4833.4400000000005, "text": " Here's how to do the algorithm. It gives the system a lot more flexibility in terms of how to", "tokens": [51204, 1692, 311, 577, 281, 360, 264, 9284, 13, 467, 2709, 264, 1185, 257, 688, 544, 12635, 294, 2115, 295, 577, 281, 51440], "temperature": 0.0, "avg_logprob": -0.08565778732299804, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.022971652448177338}, {"id": 942, "seek": 481192, "start": 4833.4400000000005, "end": 4838.32, "text": " translate or map or compile it onto the system that you have. And so what you need, you know,", "tokens": [51440, 13799, 420, 4471, 420, 31413, 309, 3911, 264, 1185, 300, 291, 362, 13, 400, 370, 437, 291, 643, 11, 291, 458, 11, 51684], "temperature": 0.0, "avg_logprob": -0.08565778732299804, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.022971652448177338}, {"id": 943, "seek": 483832, "start": 4838.32, "end": 4843.36, "text": " at the bottomest part of the layer there is a way for all these devices to talk to each other.", "tokens": [50364, 412, 264, 2767, 377, 644, 295, 264, 4583, 456, 307, 257, 636, 337, 439, 613, 5759, 281, 751, 281, 1184, 661, 13, 50616], "temperature": 0.0, "avg_logprob": -0.11287155013153519, "compression_ratio": 1.8258064516129033, "no_speech_prob": 0.0014549288898706436}, {"id": 944, "seek": 483832, "start": 4843.36, "end": 4847.759999999999, "text": " And so this is one thing that, you know, I'm very passionate about. I mean, you know, I'm a nerd,", "tokens": [50616, 400, 370, 341, 307, 472, 551, 300, 11, 291, 458, 11, 286, 478, 588, 11410, 466, 13, 286, 914, 11, 291, 458, 11, 286, 478, 257, 23229, 11, 50836], "temperature": 0.0, "avg_logprob": -0.11287155013153519, "compression_ratio": 1.8258064516129033, "no_speech_prob": 0.0014549288898706436}, {"id": 945, "seek": 483832, "start": 4847.759999999999, "end": 4853.599999999999, "text": " but, but all these, all these machines and all these systems are effectively parallel computers", "tokens": [50836, 457, 11, 457, 439, 613, 11, 439, 613, 8379, 293, 439, 613, 3652, 366, 8659, 8952, 10807, 51128], "temperature": 0.0, "avg_logprob": -0.11287155013153519, "compression_ratio": 1.8258064516129033, "no_speech_prob": 0.0014549288898706436}, {"id": 946, "seek": 483832, "start": 4853.599999999999, "end": 4858.16, "text": " running at the same time, sending messages to each other. And so they're all fully asynchronous.", "tokens": [51128, 2614, 412, 264, 912, 565, 11, 7750, 7897, 281, 1184, 661, 13, 400, 370, 436, 434, 439, 4498, 49174, 13, 51356], "temperature": 0.0, "avg_logprob": -0.11287155013153519, "compression_ratio": 1.8258064516129033, "no_speech_prob": 0.0014549288898706436}, {"id": 947, "seek": 483832, "start": 4859.04, "end": 4862.5599999999995, "text": " Well, this is actually a small version of the same problem you have in a data center,", "tokens": [51400, 1042, 11, 341, 307, 767, 257, 1359, 3037, 295, 264, 912, 1154, 291, 362, 294, 257, 1412, 3056, 11, 51576], "temperature": 0.0, "avg_logprob": -0.11287155013153519, "compression_ratio": 1.8258064516129033, "no_speech_prob": 0.0014549288898706436}, {"id": 948, "seek": 483832, "start": 4863.28, "end": 4867.5199999999995, "text": " right? In a data center, you now have multiple different machines, sometimes very specialized,", "tokens": [51612, 558, 30, 682, 257, 1412, 3056, 11, 291, 586, 362, 3866, 819, 8379, 11, 2171, 588, 19813, 11, 51824], "temperature": 0.0, "avg_logprob": -0.11287155013153519, "compression_ratio": 1.8258064516129033, "no_speech_prob": 0.0014549288898706436}, {"id": 949, "seek": 486752, "start": 4867.52, "end": 4872.72, "text": " sometimes with GPUs or TPs and one node and sometimes with disks and other nodes. And so you", "tokens": [50364, 2171, 365, 18407, 82, 420, 314, 23043, 293, 472, 9984, 293, 2171, 365, 41617, 293, 661, 13891, 13, 400, 370, 291, 50624], "temperature": 0.0, "avg_logprob": -0.11545694936620127, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.00015842645370867103}, {"id": 950, "seek": 486752, "start": 4872.72, "end": 4877.52, "text": " get a much larger scale, heterogeneous computer. And so what ends up happening is you have this", "tokens": [50624, 483, 257, 709, 4833, 4373, 11, 20789, 31112, 3820, 13, 400, 370, 437, 5314, 493, 2737, 307, 291, 362, 341, 50864], "temperature": 0.0, "avg_logprob": -0.11545694936620127, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.00015842645370867103}, {"id": 951, "seek": 486752, "start": 4877.52, "end": 4883.84, "text": " like multi-layer abstraction of hierarchical parallelism, hierarchical asynchronous communication", "tokens": [50864, 411, 4825, 12, 8376, 260, 37765, 295, 35250, 804, 8952, 1434, 11, 35250, 804, 49174, 6101, 51180], "temperature": 0.0, "avg_logprob": -0.11545694936620127, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.00015842645370867103}, {"id": 952, "seek": 486752, "start": 4884.400000000001, "end": 4890.56, "text": " and making that again, the enemy, my enemy is complexity by getting that away from being", "tokens": [51208, 293, 1455, 300, 797, 11, 264, 5945, 11, 452, 5945, 307, 14024, 538, 1242, 300, 1314, 490, 885, 51516], "temperature": 0.0, "avg_logprob": -0.11545694936620127, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.00015842645370867103}, {"id": 953, "seek": 486752, "start": 4890.56, "end": 4894.64, "text": " different specialized systems at every different part of the stack and having more consistency", "tokens": [51516, 819, 19813, 3652, 412, 633, 819, 644, 295, 264, 8630, 293, 1419, 544, 14416, 51720], "temperature": 0.0, "avg_logprob": -0.11545694936620127, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.00015842645370867103}, {"id": 954, "seek": 489464, "start": 4894.64, "end": 4899.6, "text": " and uniformity. I think we can help lift the world and make it much simpler and actually get used.", "tokens": [50364, 293, 9452, 507, 13, 286, 519, 321, 393, 854, 5533, 264, 1002, 293, 652, 309, 709, 18587, 293, 767, 483, 1143, 13, 50612], "temperature": 0.0, "avg_logprob": -0.16717747908372146, "compression_ratio": 1.7165109034267914, "no_speech_prob": 0.03257540985941887}, {"id": 955, "seek": 489464, "start": 4899.6, "end": 4903.52, "text": " But how do you leverage like the strengths of the different specialized systems? So we're looking", "tokens": [50612, 583, 577, 360, 291, 13982, 411, 264, 16986, 295, 264, 819, 19813, 3652, 30, 407, 321, 434, 1237, 50808], "temperature": 0.0, "avg_logprob": -0.16717747908372146, "compression_ratio": 1.7165109034267914, "no_speech_prob": 0.03257540985941887}, {"id": 956, "seek": 489464, "start": 4903.52, "end": 4908.56, "text": " inside the smartphone. Yeah. Like there's just what I don't know, five, six computers essentially", "tokens": [50808, 1854, 264, 13307, 13, 865, 13, 1743, 456, 311, 445, 437, 286, 500, 380, 458, 11, 1732, 11, 2309, 10807, 4476, 51060], "temperature": 0.0, "avg_logprob": -0.16717747908372146, "compression_ratio": 1.7165109034267914, "no_speech_prob": 0.03257540985941887}, {"id": 957, "seek": 489464, "start": 4908.56, "end": 4918.08, "text": " inside a smartphone. How do you, without trying to minimize the explicit, making it explicit,", "tokens": [51060, 1854, 257, 13307, 13, 1012, 360, 291, 11, 1553, 1382, 281, 17522, 264, 13691, 11, 1455, 309, 13691, 11, 51536], "temperature": 0.0, "avg_logprob": -0.16717747908372146, "compression_ratio": 1.7165109034267914, "no_speech_prob": 0.03257540985941887}, {"id": 958, "seek": 489464, "start": 4918.08, "end": 4920.4800000000005, "text": " which, which computer is supposed to be used for which operation?", "tokens": [51536, 597, 11, 597, 3820, 307, 3442, 281, 312, 1143, 337, 597, 6916, 30, 51656], "temperature": 0.0, "avg_logprob": -0.16717747908372146, "compression_ratio": 1.7165109034267914, "no_speech_prob": 0.03257540985941887}, {"id": 959, "seek": 489464, "start": 4920.4800000000005, "end": 4924.400000000001, "text": " Yeah. So there's, there's a pretty well known algorithm. And what you're doing is you're looking", "tokens": [51656, 865, 13, 407, 456, 311, 11, 456, 311, 257, 1238, 731, 2570, 9284, 13, 400, 437, 291, 434, 884, 307, 291, 434, 1237, 51852], "temperature": 0.0, "avg_logprob": -0.16717747908372146, "compression_ratio": 1.7165109034267914, "no_speech_prob": 0.03257540985941887}, {"id": 960, "seek": 492440, "start": 4924.48, "end": 4928.96, "text": " at two, two factors. You're looking at the factor of sending data from one thing to another.", "tokens": [50368, 412, 732, 11, 732, 6771, 13, 509, 434, 1237, 412, 264, 5952, 295, 7750, 1412, 490, 472, 551, 281, 1071, 13, 50592], "temperature": 0.0, "avg_logprob": -0.12050604994279625, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.001987440511584282}, {"id": 961, "seek": 492440, "start": 4928.96, "end": 4932.0, "text": " Right. So it takes time to get it from that side of the chip to that side of the chip", "tokens": [50592, 1779, 13, 407, 309, 2516, 565, 281, 483, 309, 490, 300, 1252, 295, 264, 11409, 281, 300, 1252, 295, 264, 11409, 50744], "temperature": 0.0, "avg_logprob": -0.12050604994279625, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.001987440511584282}, {"id": 962, "seek": 492440, "start": 4932.0, "end": 4935.839999999999, "text": " and things like this. And then you're looking at what is the time it takes to do", "tokens": [50744, 293, 721, 411, 341, 13, 400, 550, 291, 434, 1237, 412, 437, 307, 264, 565, 309, 2516, 281, 360, 50936], "temperature": 0.0, "avg_logprob": -0.12050604994279625, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.001987440511584282}, {"id": 963, "seek": 492440, "start": 4935.839999999999, "end": 4942.879999999999, "text": " an operation on a particular block. So take CPUs. CPUs are fully general. They can do anything.", "tokens": [50936, 364, 6916, 322, 257, 1729, 3461, 13, 407, 747, 13199, 82, 13, 13199, 82, 366, 4498, 2674, 13, 814, 393, 360, 1340, 13, 51288], "temperature": 0.0, "avg_logprob": -0.12050604994279625, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.001987440511584282}, {"id": 964, "seek": 492440, "start": 4942.879999999999, "end": 4947.28, "text": " Right. But then you have a neural net accelerator that's really good at matrix multiplications.", "tokens": [51288, 1779, 13, 583, 550, 291, 362, 257, 18161, 2533, 39889, 300, 311, 534, 665, 412, 8141, 17596, 763, 13, 51508], "temperature": 0.0, "avg_logprob": -0.12050604994279625, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.001987440511584282}, {"id": 965, "seek": 492440, "start": 4947.28, "end": 4951.12, "text": " Okay. And so you say, okay, well, if my workload is all matrix multiplications,", "tokens": [51508, 1033, 13, 400, 370, 291, 584, 11, 1392, 11, 731, 11, 498, 452, 20139, 307, 439, 8141, 17596, 763, 11, 51700], "temperature": 0.0, "avg_logprob": -0.12050604994279625, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.001987440511584282}, {"id": 966, "seek": 495112, "start": 4951.68, "end": 4956.08, "text": " I start up, I send the data over the neural net thing, it goes and does matrix multiplications,", "tokens": [50392, 286, 722, 493, 11, 286, 2845, 264, 1412, 670, 264, 18161, 2533, 551, 11, 309, 1709, 293, 775, 8141, 17596, 763, 11, 50612], "temperature": 0.0, "avg_logprob": -0.09543620289622487, "compression_ratio": 1.9220338983050846, "no_speech_prob": 0.0007096428889781237}, {"id": 967, "seek": 495112, "start": 4956.08, "end": 4960.5599999999995, "text": " when it's done, it sends me back the result, all is good. Right. And so the simplest thing is just", "tokens": [50612, 562, 309, 311, 1096, 11, 309, 14790, 385, 646, 264, 1874, 11, 439, 307, 665, 13, 1779, 13, 400, 370, 264, 22811, 551, 307, 445, 50836], "temperature": 0.0, "avg_logprob": -0.09543620289622487, "compression_ratio": 1.9220338983050846, "no_speech_prob": 0.0007096428889781237}, {"id": 968, "seek": 495112, "start": 4960.5599999999995, "end": 4966.4, "text": " saying, do matrix, do matrix operations over there. Right. But then you realize you get a little bit", "tokens": [50836, 1566, 11, 360, 8141, 11, 360, 8141, 7705, 670, 456, 13, 1779, 13, 583, 550, 291, 4325, 291, 483, 257, 707, 857, 51128], "temperature": 0.0, "avg_logprob": -0.09543620289622487, "compression_ratio": 1.9220338983050846, "no_speech_prob": 0.0007096428889781237}, {"id": 969, "seek": 495112, "start": 4966.4, "end": 4970.16, "text": " more complicated because you can do matrix multiplications on a GPU, you can do it on", "tokens": [51128, 544, 6179, 570, 291, 393, 360, 8141, 17596, 763, 322, 257, 18407, 11, 291, 393, 360, 309, 322, 51316], "temperature": 0.0, "avg_logprob": -0.09543620289622487, "compression_ratio": 1.9220338983050846, "no_speech_prob": 0.0007096428889781237}, {"id": 970, "seek": 495112, "start": 4971.5199999999995, "end": 4976.08, "text": " a neural net accelerator, you can do it on CPU, and they'll have different tradeoffs and costs.", "tokens": [51384, 257, 18161, 2533, 39889, 11, 291, 393, 360, 309, 322, 13199, 11, 293, 436, 603, 362, 819, 4923, 19231, 293, 5497, 13, 51612], "temperature": 0.0, "avg_logprob": -0.09543620289622487, "compression_ratio": 1.9220338983050846, "no_speech_prob": 0.0007096428889781237}, {"id": 971, "seek": 495112, "start": 4976.08, "end": 4980.32, "text": " And it's not just matrix multiplication. And so what you actually look at is you look at,", "tokens": [51612, 400, 309, 311, 406, 445, 8141, 27290, 13, 400, 370, 437, 291, 767, 574, 412, 307, 291, 574, 412, 11, 51824], "temperature": 0.0, "avg_logprob": -0.09543620289622487, "compression_ratio": 1.9220338983050846, "no_speech_prob": 0.0007096428889781237}, {"id": 972, "seek": 498032, "start": 4980.4, "end": 4986.24, "text": " I have generally a graph of compute. I want to do a partitioning. I want to look at the communication,", "tokens": [50368, 286, 362, 5101, 257, 4295, 295, 14722, 13, 286, 528, 281, 360, 257, 24808, 278, 13, 286, 528, 281, 574, 412, 264, 6101, 11, 50660], "temperature": 0.0, "avg_logprob": -0.13016009780595889, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0006561698974110186}, {"id": 973, "seek": 498032, "start": 4986.96, "end": 4990.96, "text": " the bisection bandwidth and like the overhead and the sending of all these different things and", "tokens": [50696, 264, 7393, 10183, 23647, 293, 411, 264, 19922, 293, 264, 7750, 295, 439, 613, 819, 721, 293, 50896], "temperature": 0.0, "avg_logprob": -0.13016009780595889, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0006561698974110186}, {"id": 974, "seek": 498032, "start": 4991.679999999999, "end": 4995.84, "text": " build a model for this and then decide, okay, it's an optimization problem. Where do I want to", "tokens": [50932, 1322, 257, 2316, 337, 341, 293, 550, 4536, 11, 1392, 11, 309, 311, 364, 19618, 1154, 13, 2305, 360, 286, 528, 281, 51140], "temperature": 0.0, "avg_logprob": -0.13016009780595889, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0006561698974110186}, {"id": 975, "seek": 498032, "start": 4995.84, "end": 5002.719999999999, "text": " place this compute? So the old school theoretical computer science problem of scheduling. And then", "tokens": [51140, 1081, 341, 14722, 30, 407, 264, 1331, 1395, 20864, 3820, 3497, 1154, 295, 29055, 13, 400, 550, 51484], "temperature": 0.0, "avg_logprob": -0.13016009780595889, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0006561698974110186}, {"id": 976, "seek": 498032, "start": 5002.719999999999, "end": 5009.12, "text": " how does presumably it's possible to somehow magically include auto tune into this?", "tokens": [51484, 577, 775, 26742, 309, 311, 1944, 281, 6063, 39763, 4090, 8399, 10864, 666, 341, 30, 51804], "temperature": 0.0, "avg_logprob": -0.13016009780595889, "compression_ratio": 1.730909090909091, "no_speech_prob": 0.0006561698974110186}, {"id": 977, "seek": 501032, "start": 5010.4, "end": 5016.32, "text": " Absolutely. So I mean, in my opinion, this is an opinion, this is not, not everybody would agree", "tokens": [50368, 7021, 13, 407, 286, 914, 11, 294, 452, 4800, 11, 341, 307, 364, 4800, 11, 341, 307, 406, 11, 406, 2201, 576, 3986, 50664], "temperature": 0.0, "avg_logprob": -0.07524548258100237, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.0016481915954500437}, {"id": 978, "seek": 501032, "start": 5016.32, "end": 5021.92, "text": " with this, but in my opinion, the world benefits from simple and predictable systems at the bottom", "tokens": [50664, 365, 341, 11, 457, 294, 452, 4800, 11, 264, 1002, 5311, 490, 2199, 293, 27737, 3652, 412, 264, 2767, 50944], "temperature": 0.0, "avg_logprob": -0.07524548258100237, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.0016481915954500437}, {"id": 979, "seek": 501032, "start": 5021.92, "end": 5027.599999999999, "text": " that you can control. But then once you have a predictable execution layer, you can build", "tokens": [50944, 300, 291, 393, 1969, 13, 583, 550, 1564, 291, 362, 257, 27737, 15058, 4583, 11, 291, 393, 1322, 51228], "temperature": 0.0, "avg_logprob": -0.07524548258100237, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.0016481915954500437}, {"id": 980, "seek": 501032, "start": 5027.599999999999, "end": 5031.44, "text": " lots of different policies on top of it. Right. And so one policy can be that", "tokens": [51228, 3195, 295, 819, 7657, 322, 1192, 295, 309, 13, 1779, 13, 400, 370, 472, 3897, 393, 312, 300, 51420], "temperature": 0.0, "avg_logprob": -0.07524548258100237, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.0016481915954500437}, {"id": 981, "seek": 501032, "start": 5032.48, "end": 5037.599999999999, "text": " the human programmer says, do that here, do that here, do that here, do that here and like", "tokens": [51472, 264, 1952, 32116, 1619, 11, 360, 300, 510, 11, 360, 300, 510, 11, 360, 300, 510, 11, 360, 300, 510, 293, 411, 51728], "temperature": 0.0, "avg_logprob": -0.07524548258100237, "compression_ratio": 1.860655737704918, "no_speech_prob": 0.0016481915954500437}, {"id": 982, "seek": 503760, "start": 5037.6, "end": 5043.280000000001, "text": " fully manually controls everything. And the systems just do it. Right. Then you quickly", "tokens": [50364, 4498, 16945, 9003, 1203, 13, 400, 264, 3652, 445, 360, 309, 13, 1779, 13, 1396, 291, 2661, 50648], "temperature": 0.0, "avg_logprob": -0.13193082485069224, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.008060495369136333}, {"id": 983, "seek": 503760, "start": 5043.280000000001, "end": 5047.4400000000005, "text": " get in the mode of like, I don't want to have to tell it to do it. Yeah. And so the next logical", "tokens": [50648, 483, 294, 264, 4391, 295, 411, 11, 286, 500, 380, 528, 281, 362, 281, 980, 309, 281, 360, 309, 13, 865, 13, 400, 370, 264, 958, 14978, 50856], "temperature": 0.0, "avg_logprob": -0.13193082485069224, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.008060495369136333}, {"id": 984, "seek": 503760, "start": 5047.4400000000005, "end": 5052.160000000001, "text": " step that people typically take, because they write some terrible heuristic, if it's a major", "tokens": [50856, 1823, 300, 561, 5850, 747, 11, 570, 436, 2464, 512, 6237, 415, 374, 3142, 11, 498, 309, 311, 257, 2563, 51092], "temperature": 0.0, "avg_logprob": -0.13193082485069224, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.008060495369136333}, {"id": 985, "seek": 503760, "start": 5052.160000000001, "end": 5055.360000000001, "text": " small location, do it over there. Or if it's floating point, do it on the GPU, if it's integer,", "tokens": [51092, 1359, 4914, 11, 360, 309, 670, 456, 13, 1610, 498, 309, 311, 12607, 935, 11, 360, 309, 322, 264, 18407, 11, 498, 309, 311, 24922, 11, 51252], "temperature": 0.0, "avg_logprob": -0.13193082485069224, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.008060495369136333}, {"id": 986, "seek": 503760, "start": 5055.360000000001, "end": 5060.72, "text": " do it on the CPU, like something like that. Right. And, and then you then get into this", "tokens": [51252, 360, 309, 322, 264, 13199, 11, 411, 746, 411, 300, 13, 1779, 13, 400, 11, 293, 550, 291, 550, 483, 666, 341, 51520], "temperature": 0.0, "avg_logprob": -0.13193082485069224, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.008060495369136333}, {"id": 987, "seek": 503760, "start": 5060.72, "end": 5063.280000000001, "text": " mode of like people care more and more and more and you say, okay, well, let's actually", "tokens": [51520, 4391, 295, 411, 561, 1127, 544, 293, 544, 293, 544, 293, 291, 584, 11, 1392, 11, 731, 11, 718, 311, 767, 51648], "temperature": 0.0, "avg_logprob": -0.13193082485069224, "compression_ratio": 1.7941176470588236, "no_speech_prob": 0.008060495369136333}, {"id": 988, "seek": 506328, "start": 5064.16, "end": 5070.5599999999995, "text": " like make the heuristic better. Let's get into auto tint. Let's actually do a search of the space", "tokens": [50408, 411, 652, 264, 415, 374, 3142, 1101, 13, 961, 311, 483, 666, 8399, 28738, 13, 961, 311, 767, 360, 257, 3164, 295, 264, 1901, 50728], "temperature": 0.0, "avg_logprob": -0.16134275469863624, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0071207256987690926}, {"id": 989, "seek": 506328, "start": 5071.12, "end": 5076.88, "text": " to decide, well, what is actually better? Right. Well, then you get into this problem where you", "tokens": [50756, 281, 4536, 11, 731, 11, 437, 307, 767, 1101, 30, 1779, 13, 1042, 11, 550, 291, 483, 666, 341, 1154, 689, 291, 51044], "temperature": 0.0, "avg_logprob": -0.16134275469863624, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0071207256987690926}, {"id": 990, "seek": 506328, "start": 5076.88, "end": 5082.88, "text": " realize this is not a small space. This is a many dimensional, hyper dimensional space that you", "tokens": [51044, 4325, 341, 307, 406, 257, 1359, 1901, 13, 639, 307, 257, 867, 18795, 11, 9848, 18795, 1901, 300, 291, 51344], "temperature": 0.0, "avg_logprob": -0.16134275469863624, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0071207256987690926}, {"id": 991, "seek": 506328, "start": 5082.88, "end": 5087.84, "text": " cannot exhaustively search. So do you know of any algorithms that are good at searching very", "tokens": [51344, 2644, 14687, 3413, 3164, 13, 407, 360, 291, 458, 295, 604, 14642, 300, 366, 665, 412, 10808, 588, 51592], "temperature": 0.0, "avg_logprob": -0.16134275469863624, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0071207256987690926}, {"id": 992, "seek": 506328, "start": 5087.84, "end": 5093.12, "text": " complicated spaces for? Don't tell me you're going to turn this into a machine learning problem.", "tokens": [51592, 6179, 7673, 337, 30, 1468, 380, 980, 385, 291, 434, 516, 281, 1261, 341, 666, 257, 3479, 2539, 1154, 13, 51856], "temperature": 0.0, "avg_logprob": -0.16134275469863624, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.0071207256987690926}, {"id": 993, "seek": 509328, "start": 5093.36, "end": 5097.44, "text": " So then you turn into a machine learning problem. And then you have a space of generic algorithms", "tokens": [50368, 407, 550, 291, 1261, 666, 257, 3479, 2539, 1154, 13, 400, 550, 291, 362, 257, 1901, 295, 19577, 14642, 50572], "temperature": 0.0, "avg_logprob": -0.15594117906358507, "compression_ratio": 1.8125, "no_speech_prob": 0.00040445197373628616}, {"id": 994, "seek": 509328, "start": 5097.44, "end": 5102.0, "text": " and reinforcement learning and like all these, all these, can you include that into the stack,", "tokens": [50572, 293, 29280, 2539, 293, 411, 439, 613, 11, 439, 613, 11, 393, 291, 4090, 300, 666, 264, 8630, 11, 50800], "temperature": 0.0, "avg_logprob": -0.15594117906358507, "compression_ratio": 1.8125, "no_speech_prob": 0.00040445197373628616}, {"id": 995, "seek": 509328, "start": 5102.0, "end": 5106.32, "text": " into the, into the module stack? Yeah. Yeah. And where does it sit? Where does it live?", "tokens": [50800, 666, 264, 11, 666, 264, 10088, 8630, 30, 865, 13, 865, 13, 400, 689, 775, 309, 1394, 30, 2305, 775, 309, 1621, 30, 51016], "temperature": 0.0, "avg_logprob": -0.15594117906358507, "compression_ratio": 1.8125, "no_speech_prob": 0.00040445197373628616}, {"id": 996, "seek": 509328, "start": 5106.32, "end": 5110.32, "text": " Is it a separate thing or is it part of the compilation? So you start from simple and", "tokens": [51016, 1119, 309, 257, 4994, 551, 420, 307, 309, 644, 295, 264, 40261, 30, 407, 291, 722, 490, 2199, 293, 51216], "temperature": 0.0, "avg_logprob": -0.15594117906358507, "compression_ratio": 1.8125, "no_speech_prob": 0.00040445197373628616}, {"id": 997, "seek": 509328, "start": 5110.32, "end": 5115.599999999999, "text": " predictable models. And so you can have full control and you can have coarse grain knobs", "tokens": [51216, 27737, 5245, 13, 400, 370, 291, 393, 362, 1577, 1969, 293, 291, 393, 362, 39312, 12837, 46999, 51480], "temperature": 0.0, "avg_logprob": -0.15594117906358507, "compression_ratio": 1.8125, "no_speech_prob": 0.00040445197373628616}, {"id": 998, "seek": 509328, "start": 5115.599999999999, "end": 5120.08, "text": " that like nudge, nudge systems, you don't have to do this. But if you really care about getting", "tokens": [51480, 300, 411, 297, 16032, 11, 297, 16032, 3652, 11, 291, 500, 380, 362, 281, 360, 341, 13, 583, 498, 291, 534, 1127, 466, 1242, 51704], "temperature": 0.0, "avg_logprob": -0.15594117906358507, "compression_ratio": 1.8125, "no_speech_prob": 0.00040445197373628616}, {"id": 999, "seek": 512008, "start": 5120.16, "end": 5125.2, "text": " the best, you know, the last ounce out of a problem, then you can use additional tools.", "tokens": [50368, 264, 1151, 11, 291, 458, 11, 264, 1036, 29860, 484, 295, 257, 1154, 11, 550, 291, 393, 764, 4497, 3873, 13, 50620], "temperature": 0.0, "avg_logprob": -0.0698202922426421, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.011329488828778267}, {"id": 1000, "seek": 512008, "start": 5125.2, "end": 5128.96, "text": " And they're the cool thing is you don't want to do this every time you run a model. You want to", "tokens": [50620, 400, 436, 434, 264, 1627, 551, 307, 291, 500, 380, 528, 281, 360, 341, 633, 565, 291, 1190, 257, 2316, 13, 509, 528, 281, 50808], "temperature": 0.0, "avg_logprob": -0.0698202922426421, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.011329488828778267}, {"id": 1001, "seek": 512008, "start": 5128.96, "end": 5133.12, "text": " figure out the right answer and then cash it. And once you do that, you can get,", "tokens": [50808, 2573, 484, 264, 558, 1867, 293, 550, 6388, 309, 13, 400, 1564, 291, 360, 300, 11, 291, 393, 483, 11, 51016], "temperature": 0.0, "avg_logprob": -0.0698202922426421, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.011329488828778267}, {"id": 1002, "seek": 512008, "start": 5133.12, "end": 5138.88, "text": " you can say, okay, cool, I can get up and running very quickly. I can get good execution out of my", "tokens": [51016, 291, 393, 584, 11, 1392, 11, 1627, 11, 286, 393, 483, 493, 293, 2614, 588, 2661, 13, 286, 393, 483, 665, 15058, 484, 295, 452, 51304], "temperature": 0.0, "avg_logprob": -0.0698202922426421, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.011329488828778267}, {"id": 1003, "seek": 512008, "start": 5138.88, "end": 5143.36, "text": " system. I can decide if something's important. And if it's important, I can go throw a bunch of", "tokens": [51304, 1185, 13, 286, 393, 4536, 498, 746, 311, 1021, 13, 400, 498, 309, 311, 1021, 11, 286, 393, 352, 3507, 257, 3840, 295, 51528], "temperature": 0.0, "avg_logprob": -0.0698202922426421, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.011329488828778267}, {"id": 1004, "seek": 512008, "start": 5143.36, "end": 5147.68, "text": " machines at it and do a big expensive search over the space using whatever technique I feel like", "tokens": [51528, 8379, 412, 309, 293, 360, 257, 955, 5124, 3164, 670, 264, 1901, 1228, 2035, 6532, 286, 841, 411, 51744], "temperature": 0.0, "avg_logprob": -0.0698202922426421, "compression_ratio": 1.8169934640522876, "no_speech_prob": 0.011329488828778267}, {"id": 1005, "seek": 514768, "start": 5147.68, "end": 5152.08, "text": " it's probably up to the problem. And then when I get the right answer, cool, I can just start using", "tokens": [50364, 309, 311, 1391, 493, 281, 264, 1154, 13, 400, 550, 562, 286, 483, 264, 558, 1867, 11, 1627, 11, 286, 393, 445, 722, 1228, 50584], "temperature": 0.0, "avg_logprob": -0.13559162710595318, "compression_ratio": 1.6212624584717608, "no_speech_prob": 0.0014102859422564507}, {"id": 1006, "seek": 514768, "start": 5152.08, "end": 5158.0, "text": " it. And so you can get out of this, this trade off between, okay, am I going to like spend forever", "tokens": [50584, 309, 13, 400, 370, 291, 393, 483, 484, 295, 341, 11, 341, 4923, 766, 1296, 11, 1392, 11, 669, 286, 516, 281, 411, 3496, 5680, 50880], "temperature": 0.0, "avg_logprob": -0.13559162710595318, "compression_ratio": 1.6212624584717608, "no_speech_prob": 0.0014102859422564507}, {"id": 1007, "seek": 514768, "start": 5158.0, "end": 5162.240000000001, "text": " doing a thing or do I get up and running quickly? And as a quality result, like these, these are", "tokens": [50880, 884, 257, 551, 420, 360, 286, 483, 493, 293, 2614, 2661, 30, 400, 382, 257, 3125, 1874, 11, 411, 613, 11, 613, 366, 51092], "temperature": 0.0, "avg_logprob": -0.13559162710595318, "compression_ratio": 1.6212624584717608, "no_speech_prob": 0.0014102859422564507}, {"id": 1008, "seek": 514768, "start": 5162.240000000001, "end": 5168.72, "text": " actually not in contention with each other if the system's designed to scale. You started and did", "tokens": [51092, 767, 406, 294, 660, 1251, 365, 1184, 661, 498, 264, 1185, 311, 4761, 281, 4373, 13, 509, 1409, 293, 630, 51416], "temperature": 0.0, "avg_logprob": -0.13559162710595318, "compression_ratio": 1.6212624584717608, "no_speech_prob": 0.0014102859422564507}, {"id": 1009, "seek": 514768, "start": 5168.72, "end": 5176.96, "text": " a little bit of a whirlwind overview of how you get the 35,000 X speed up or more over Python.", "tokens": [51416, 257, 707, 857, 295, 257, 35706, 12199, 12492, 295, 577, 291, 483, 264, 6976, 11, 1360, 1783, 3073, 493, 420, 544, 670, 15329, 13, 51828], "temperature": 0.0, "avg_logprob": -0.13559162710595318, "compression_ratio": 1.6212624584717608, "no_speech_prob": 0.0014102859422564507}, {"id": 1010, "seek": 517768, "start": 5177.84, "end": 5182.320000000001, "text": " Jeremy Howard did a really great presentation about sort of the basic, like looking at the code,", "tokens": [50372, 17809, 17626, 630, 257, 534, 869, 5860, 466, 1333, 295, 264, 3875, 11, 411, 1237, 412, 264, 3089, 11, 50596], "temperature": 0.0, "avg_logprob": -0.13704219486402427, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.0011511699995025992}, {"id": 1011, "seek": 517768, "start": 5182.320000000001, "end": 5186.8, "text": " here's how you get the speed up. Like you said, that's something we could probably developers", "tokens": [50596, 510, 311, 577, 291, 483, 264, 3073, 493, 13, 1743, 291, 848, 11, 300, 311, 746, 321, 727, 1391, 8849, 50820], "temperature": 0.0, "avg_logprob": -0.13704219486402427, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.0011511699995025992}, {"id": 1012, "seek": 517768, "start": 5186.8, "end": 5192.400000000001, "text": " can do for their own code to see how you can get these gigantic speed up. But can you maybe speak", "tokens": [50820, 393, 360, 337, 641, 1065, 3089, 281, 536, 577, 291, 393, 483, 613, 26800, 3073, 493, 13, 583, 393, 291, 1310, 1710, 51100], "temperature": 0.0, "avg_logprob": -0.13704219486402427, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.0011511699995025992}, {"id": 1013, "seek": 517768, "start": 5192.400000000001, "end": 5196.0, "text": " to the machine learning tasks in general? How do you, how do you make some of this code fast,", "tokens": [51100, 281, 264, 3479, 2539, 9608, 294, 2674, 30, 1012, 360, 291, 11, 577, 360, 291, 652, 512, 295, 341, 3089, 2370, 11, 51280], "temperature": 0.0, "avg_logprob": -0.13704219486402427, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.0011511699995025992}, {"id": 1014, "seek": 517768, "start": 5196.0, "end": 5205.4400000000005, "text": " some specifics like what would you say is the main bottleneck for machine learning tasks? So are we", "tokens": [51280, 512, 28454, 411, 437, 576, 291, 584, 307, 264, 2135, 44641, 547, 337, 3479, 2539, 9608, 30, 407, 366, 321, 51752], "temperature": 0.0, "avg_logprob": -0.13704219486402427, "compression_ratio": 1.7463768115942029, "no_speech_prob": 0.0011511699995025992}, {"id": 1015, "seek": 520544, "start": 5205.44, "end": 5210.24, "text": " talking about metmall, matrix multiplication, how do you make that fast?", "tokens": [50364, 1417, 466, 1131, 76, 336, 11, 8141, 27290, 11, 577, 360, 291, 652, 300, 2370, 30, 50604], "temperature": 0.0, "avg_logprob": -0.16686323526743296, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.008843221701681614}, {"id": 1016, "seek": 520544, "start": 5210.24, "end": 5214.96, "text": " So I mean, if you just look at the Python problem, right, you can say, how do I make Python faster?", "tokens": [50604, 407, 286, 914, 11, 498, 291, 445, 574, 412, 264, 15329, 1154, 11, 558, 11, 291, 393, 584, 11, 577, 360, 286, 652, 15329, 4663, 30, 50840], "temperature": 0.0, "avg_logprob": -0.16686323526743296, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.008843221701681614}, {"id": 1017, "seek": 520544, "start": 5215.839999999999, "end": 5219.839999999999, "text": " There's been a lot of people that have been working on the, okay, how do I make Python 2x faster,", "tokens": [50884, 821, 311, 668, 257, 688, 295, 561, 300, 362, 668, 1364, 322, 264, 11, 1392, 11, 577, 360, 286, 652, 15329, 568, 87, 4663, 11, 51084], "temperature": 0.0, "avg_logprob": -0.16686323526743296, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.008843221701681614}, {"id": 1018, "seek": 520544, "start": 5219.839999999999, "end": 5223.36, "text": " 10x faster or something like that, right? And there've been a ton of projects in that vein,", "tokens": [51084, 1266, 87, 4663, 420, 746, 411, 300, 11, 558, 30, 400, 456, 600, 668, 257, 2952, 295, 4455, 294, 300, 30669, 11, 51260], "temperature": 0.0, "avg_logprob": -0.16686323526743296, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.008843221701681614}, {"id": 1019, "seek": 520544, "start": 5223.36, "end": 5229.679999999999, "text": " right? Mojo started from the, what can the hardware do? Like what is the limit of physics?", "tokens": [51260, 558, 30, 3335, 5134, 1409, 490, 264, 11, 437, 393, 264, 8837, 360, 30, 1743, 437, 307, 264, 4948, 295, 10649, 30, 51576], "temperature": 0.0, "avg_logprob": -0.16686323526743296, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.008843221701681614}, {"id": 1020, "seek": 520544, "start": 5229.679999999999, "end": 5233.44, "text": " Yeah, what is the speed of light? What is it? Like how fast can this thing go? And then", "tokens": [51576, 865, 11, 437, 307, 264, 3073, 295, 1442, 30, 708, 307, 309, 30, 1743, 577, 2370, 393, 341, 551, 352, 30, 400, 550, 51764], "temperature": 0.0, "avg_logprob": -0.16686323526743296, "compression_ratio": 1.8464163822525597, "no_speech_prob": 0.008843221701681614}, {"id": 1021, "seek": 523344, "start": 5234.08, "end": 5238.879999999999, "text": " how do I express that? Yeah, right. And so it wasn't, it wasn't anchored relatively on", "tokens": [50396, 577, 360, 286, 5109, 300, 30, 865, 11, 558, 13, 400, 370, 309, 2067, 380, 11, 309, 2067, 380, 12723, 2769, 7226, 322, 50636], "temperature": 0.0, "avg_logprob": -0.13551574989601417, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.0013248795876279473}, {"id": 1022, "seek": 523344, "start": 5238.879999999999, "end": 5242.719999999999, "text": " make Python a little bit faster. It's saying, cool, I know what the hardware can do. Let's", "tokens": [50636, 652, 15329, 257, 707, 857, 4663, 13, 467, 311, 1566, 11, 1627, 11, 286, 458, 437, 264, 8837, 393, 360, 13, 961, 311, 50828], "temperature": 0.0, "avg_logprob": -0.13551574989601417, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.0013248795876279473}, {"id": 1023, "seek": 523344, "start": 5242.719999999999, "end": 5249.12, "text": " unlock that, right? Now, when you, wait, and just say how, how gutsy that is to be in the meeting,", "tokens": [50828, 11634, 300, 11, 558, 30, 823, 11, 562, 291, 11, 1699, 11, 293, 445, 584, 577, 11, 577, 5228, 3187, 300, 307, 281, 312, 294, 264, 3440, 11, 51148], "temperature": 0.0, "avg_logprob": -0.13551574989601417, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.0013248795876279473}, {"id": 1024, "seek": 523344, "start": 5249.12, "end": 5253.04, "text": " and as opposed to trying to see how do we get the improvement? It's like, what can the physics do?", "tokens": [51148, 293, 382, 8851, 281, 1382, 281, 536, 577, 360, 321, 483, 264, 10444, 30, 467, 311, 411, 11, 437, 393, 264, 10649, 360, 30, 51344], "temperature": 0.0, "avg_logprob": -0.13551574989601417, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.0013248795876279473}, {"id": 1025, "seek": 523344, "start": 5254.0, "end": 5258.5599999999995, "text": " I mean, maybe I'm a special kind of nerd, but you look at that, what is the limit of physics,", "tokens": [51392, 286, 914, 11, 1310, 286, 478, 257, 2121, 733, 295, 23229, 11, 457, 291, 574, 412, 300, 11, 437, 307, 264, 4948, 295, 10649, 11, 51620], "temperature": 0.0, "avg_logprob": -0.13551574989601417, "compression_ratio": 1.611683848797251, "no_speech_prob": 0.0013248795876279473}, {"id": 1026, "seek": 525856, "start": 5258.56, "end": 5263.280000000001, "text": " how fast can these things go, right? When you start looking at that, typically,", "tokens": [50364, 577, 2370, 393, 613, 721, 352, 11, 558, 30, 1133, 291, 722, 1237, 412, 300, 11, 5850, 11, 50600], "temperature": 0.0, "avg_logprob": -0.09724564118818803, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.007814831100404263}, {"id": 1027, "seek": 525856, "start": 5263.280000000001, "end": 5268.56, "text": " it ends up being a memory problem, right? And so today, particularly with these specialized", "tokens": [50600, 309, 5314, 493, 885, 257, 4675, 1154, 11, 558, 30, 400, 370, 965, 11, 4098, 365, 613, 19813, 50864], "temperature": 0.0, "avg_logprob": -0.09724564118818803, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.007814831100404263}, {"id": 1028, "seek": 525856, "start": 5268.56, "end": 5274.160000000001, "text": " accelerators, the problem is that you can do a lot of math within them, but yet you get bottleneck", "tokens": [50864, 10172, 3391, 11, 264, 1154, 307, 300, 291, 393, 360, 257, 688, 295, 5221, 1951, 552, 11, 457, 1939, 291, 483, 44641, 547, 51144], "temperature": 0.0, "avg_logprob": -0.09724564118818803, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.007814831100404263}, {"id": 1029, "seek": 525856, "start": 5274.160000000001, "end": 5280.240000000001, "text": " sending data back and forth to memory, whether it be local memory or distant memory or disk or", "tokens": [51144, 7750, 1412, 646, 293, 5220, 281, 4675, 11, 1968, 309, 312, 2654, 4675, 420, 17275, 4675, 420, 12355, 420, 51448], "temperature": 0.0, "avg_logprob": -0.09724564118818803, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.007814831100404263}, {"id": 1030, "seek": 525856, "start": 5280.240000000001, "end": 5285.84, "text": " whatever it is. And that bottleneck, particularly as the training sizes get large, as you start", "tokens": [51448, 2035, 309, 307, 13, 400, 300, 44641, 547, 11, 4098, 382, 264, 3097, 11602, 483, 2416, 11, 382, 291, 722, 51728], "temperature": 0.0, "avg_logprob": -0.09724564118818803, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.007814831100404263}, {"id": 1031, "seek": 528584, "start": 5285.84, "end": 5290.56, "text": " doing tons of inferences all over the place, like that becomes a huge bottleneck for people,", "tokens": [50364, 884, 9131, 295, 13596, 2667, 439, 670, 264, 1081, 11, 411, 300, 3643, 257, 2603, 44641, 547, 337, 561, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1368580264645023, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.006902999244630337}, {"id": 1032, "seek": 528584, "start": 5290.56, "end": 5296.0, "text": " right? So again, what happened is we went through a phase of many years where people", "tokens": [50600, 558, 30, 407, 797, 11, 437, 2011, 307, 321, 1437, 807, 257, 5574, 295, 867, 924, 689, 561, 50872], "temperature": 0.0, "avg_logprob": -0.1368580264645023, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.006902999244630337}, {"id": 1033, "seek": 528584, "start": 5296.0, "end": 5300.08, "text": " took the special case and hand tuned it and tweaked it and tricked it out and they knew", "tokens": [50872, 1890, 264, 2121, 1389, 293, 1011, 10870, 309, 293, 6986, 7301, 309, 293, 39345, 309, 484, 293, 436, 2586, 51076], "temperature": 0.0, "avg_logprob": -0.1368580264645023, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.006902999244630337}, {"id": 1034, "seek": 528584, "start": 5300.08, "end": 5302.96, "text": " exactly how the hardware worked and they knew the model and they made it, they made it fast.", "tokens": [51076, 2293, 577, 264, 8837, 2732, 293, 436, 2586, 264, 2316, 293, 436, 1027, 309, 11, 436, 1027, 309, 2370, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1368580264645023, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.006902999244630337}, {"id": 1035, "seek": 528584, "start": 5303.92, "end": 5309.4400000000005, "text": " Didn't generalize. And so you can make, you know, ResNet 50 or some, or AlexNet or something,", "tokens": [51268, 11151, 380, 2674, 1125, 13, 400, 370, 291, 393, 652, 11, 291, 458, 11, 5015, 31890, 2625, 420, 512, 11, 420, 5202, 31890, 420, 746, 11, 51544], "temperature": 0.0, "avg_logprob": -0.1368580264645023, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.006902999244630337}, {"id": 1036, "seek": 528584, "start": 5309.4400000000005, "end": 5314.0, "text": " Inception V1, like you can, you can do that, right? Because the models are small, they fit in your", "tokens": [51544, 682, 7311, 691, 16, 11, 411, 291, 393, 11, 291, 393, 360, 300, 11, 558, 30, 1436, 264, 5245, 366, 1359, 11, 436, 3318, 294, 428, 51772], "temperature": 0.0, "avg_logprob": -0.1368580264645023, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.006902999244630337}, {"id": 1037, "seek": 531400, "start": 5314.0, "end": 5319.28, "text": " head, right? But as the models get bigger, more complicated, as the machines get more complicated,", "tokens": [50364, 1378, 11, 558, 30, 583, 382, 264, 5245, 483, 3801, 11, 544, 6179, 11, 382, 264, 8379, 483, 544, 6179, 11, 50628], "temperature": 0.0, "avg_logprob": -0.07462485490647038, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0028891977854073048}, {"id": 1038, "seek": 531400, "start": 5319.28, "end": 5324.8, "text": " it stops working, right? And so this is where things like kernel fusion come in. So what is", "tokens": [50628, 309, 10094, 1364, 11, 558, 30, 400, 370, 341, 307, 689, 721, 411, 28256, 23100, 808, 294, 13, 407, 437, 307, 50904], "temperature": 0.0, "avg_logprob": -0.07462485490647038, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0028891977854073048}, {"id": 1039, "seek": 531400, "start": 5324.8, "end": 5329.76, "text": " kernel fusion? This is this idea of saying, let's avoid going to memory. And let's do that by building", "tokens": [50904, 28256, 23100, 30, 639, 307, 341, 1558, 295, 1566, 11, 718, 311, 5042, 516, 281, 4675, 13, 400, 718, 311, 360, 300, 538, 2390, 51152], "temperature": 0.0, "avg_logprob": -0.07462485490647038, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0028891977854073048}, {"id": 1040, "seek": 531400, "start": 5329.76, "end": 5338.16, "text": " a new hybrid kernel and a numerical algorithm that actually keeps things in the accelerator", "tokens": [51152, 257, 777, 13051, 28256, 293, 257, 29054, 9284, 300, 767, 5965, 721, 294, 264, 39889, 51572], "temperature": 0.0, "avg_logprob": -0.07462485490647038, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0028891977854073048}, {"id": 1041, "seek": 531400, "start": 5338.16, "end": 5342.8, "text": " instead of having to write it all the way out to memory. What's happened with these accelerators", "tokens": [51572, 2602, 295, 1419, 281, 2464, 309, 439, 264, 636, 484, 281, 4675, 13, 708, 311, 2011, 365, 613, 10172, 3391, 51804], "temperature": 0.0, "avg_logprob": -0.07462485490647038, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0028891977854073048}, {"id": 1042, "seek": 534280, "start": 5342.8, "end": 5346.88, "text": " now is you get multiple levels of memory. Like in a GPU, for example, you'll have global memory", "tokens": [50364, 586, 307, 291, 483, 3866, 4358, 295, 4675, 13, 1743, 294, 257, 18407, 11, 337, 1365, 11, 291, 603, 362, 4338, 4675, 50568], "temperature": 0.0, "avg_logprob": -0.07140545451312984, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.005727724172174931}, {"id": 1043, "seek": 534280, "start": 5346.88, "end": 5353.6, "text": " and local memory and like all these things. If you zoom way into how hardware works,", "tokens": [50568, 293, 2654, 4675, 293, 411, 439, 613, 721, 13, 759, 291, 8863, 636, 666, 577, 8837, 1985, 11, 50904], "temperature": 0.0, "avg_logprob": -0.07140545451312984, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.005727724172174931}, {"id": 1044, "seek": 534280, "start": 5353.6, "end": 5359.6, "text": " the register file is actually a memory. So the registers are like an L0 cache. And so", "tokens": [50904, 264, 7280, 3991, 307, 767, 257, 4675, 13, 407, 264, 38351, 366, 411, 364, 441, 15, 19459, 13, 400, 370, 51204], "temperature": 0.0, "avg_logprob": -0.07140545451312984, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.005727724172174931}, {"id": 1045, "seek": 534280, "start": 5359.6, "end": 5365.76, "text": " a lot of taking advantage of the hardware ends up being fully utilizing the full power", "tokens": [51204, 257, 688, 295, 1940, 5002, 295, 264, 8837, 5314, 493, 885, 4498, 26775, 264, 1577, 1347, 51512], "temperature": 0.0, "avg_logprob": -0.07140545451312984, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.005727724172174931}, {"id": 1046, "seek": 534280, "start": 5366.72, "end": 5371.68, "text": " in all of its capability. And this has a number of problems, right? One of which is,", "tokens": [51560, 294, 439, 295, 1080, 13759, 13, 400, 341, 575, 257, 1230, 295, 2740, 11, 558, 30, 1485, 295, 597, 307, 11, 51808], "temperature": 0.0, "avg_logprob": -0.07140545451312984, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.005727724172174931}, {"id": 1047, "seek": 537168, "start": 5371.68, "end": 5376.240000000001, "text": " again, the complexity disaster, right? There's too much hardware. Even if you just say, let's look at", "tokens": [50364, 797, 11, 264, 14024, 11293, 11, 558, 30, 821, 311, 886, 709, 8837, 13, 2754, 498, 291, 445, 584, 11, 718, 311, 574, 412, 50592], "temperature": 0.0, "avg_logprob": -0.0952558663055187, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0008294553845189512}, {"id": 1048, "seek": 537168, "start": 5377.04, "end": 5380.96, "text": " the chips from one line of vendor like Apple or Intel or whatever it is,", "tokens": [50632, 264, 11583, 490, 472, 1622, 295, 24321, 411, 6373, 420, 19762, 420, 2035, 309, 307, 11, 50828], "temperature": 0.0, "avg_logprob": -0.0952558663055187, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0008294553845189512}, {"id": 1049, "seek": 537168, "start": 5381.76, "end": 5386.16, "text": " each version of the chip comes out with new features. And they change things so that it", "tokens": [50868, 1184, 3037, 295, 264, 11409, 1487, 484, 365, 777, 4122, 13, 400, 436, 1319, 721, 370, 300, 309, 51088], "temperature": 0.0, "avg_logprob": -0.0952558663055187, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0008294553845189512}, {"id": 1050, "seek": 537168, "start": 5386.16, "end": 5389.92, "text": " takes more time or less time to do different things. And you can't rewrite all the software", "tokens": [51088, 2516, 544, 565, 420, 1570, 565, 281, 360, 819, 721, 13, 400, 291, 393, 380, 28132, 439, 264, 4722, 51276], "temperature": 0.0, "avg_logprob": -0.0952558663055187, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0008294553845189512}, {"id": 1051, "seek": 537168, "start": 5389.92, "end": 5394.400000000001, "text": " whenever a new chip comes in, right? And so this is where you need a much more scalable approach.", "tokens": [51276, 5699, 257, 777, 11409, 1487, 294, 11, 558, 30, 400, 370, 341, 307, 689, 291, 643, 257, 709, 544, 38481, 3109, 13, 51500], "temperature": 0.0, "avg_logprob": -0.0952558663055187, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0008294553845189512}, {"id": 1052, "seek": 537168, "start": 5394.400000000001, "end": 5399.52, "text": " And this is what Mojo and what the modular stack provides is it provides this infrastructure and", "tokens": [51500, 400, 341, 307, 437, 3335, 5134, 293, 437, 264, 31111, 8630, 6417, 307, 309, 6417, 341, 6896, 293, 51756], "temperature": 0.0, "avg_logprob": -0.0952558663055187, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0008294553845189512}, {"id": 1053, "seek": 539952, "start": 5399.52, "end": 5404.080000000001, "text": " the system for factoring all this complexity and then allowing people to express algorithms.", "tokens": [50364, 264, 1185, 337, 1186, 3662, 439, 341, 14024, 293, 550, 8293, 561, 281, 5109, 14642, 13, 50592], "temperature": 0.0, "avg_logprob": -0.10731114378762902, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.0021154959686100483}, {"id": 1054, "seek": 539952, "start": 5404.080000000001, "end": 5409.040000000001, "text": " You talk about auto tuning, for example, express algorithms in a more portable way", "tokens": [50592, 509, 751, 466, 8399, 15164, 11, 337, 1365, 11, 5109, 14642, 294, 257, 544, 21800, 636, 50840], "temperature": 0.0, "avg_logprob": -0.10731114378762902, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.0021154959686100483}, {"id": 1055, "seek": 539952, "start": 5409.040000000001, "end": 5412.400000000001, "text": " so that when a new chip comes out, you don't have to rewrite it all.", "tokens": [50840, 370, 300, 562, 257, 777, 11409, 1487, 484, 11, 291, 500, 380, 362, 281, 28132, 309, 439, 13, 51008], "temperature": 0.0, "avg_logprob": -0.10731114378762902, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.0021154959686100483}, {"id": 1056, "seek": 539952, "start": 5413.4400000000005, "end": 5418.400000000001, "text": " So to me, like, you know, I kind of joke like, what is a compiler? Well, there's many ways to", "tokens": [51060, 407, 281, 385, 11, 411, 11, 291, 458, 11, 286, 733, 295, 7647, 411, 11, 437, 307, 257, 31958, 30, 1042, 11, 456, 311, 867, 2098, 281, 51308], "temperature": 0.0, "avg_logprob": -0.10731114378762902, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.0021154959686100483}, {"id": 1057, "seek": 539952, "start": 5418.400000000001, "end": 5423.52, "text": " explain that. You convert thing A into thing B, and you convert source code to machine code.", "tokens": [51308, 2903, 300, 13, 509, 7620, 551, 316, 666, 551, 363, 11, 293, 291, 7620, 4009, 3089, 281, 3479, 3089, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10731114378762902, "compression_ratio": 1.6576923076923078, "no_speech_prob": 0.0021154959686100483}, {"id": 1058, "seek": 542352, "start": 5423.52, "end": 5430.0, "text": " Like you can talk about many, many things that compilers do. But to me, it's about a bag of tricks.", "tokens": [50364, 1743, 291, 393, 751, 466, 867, 11, 867, 721, 300, 715, 388, 433, 360, 13, 583, 281, 385, 11, 309, 311, 466, 257, 3411, 295, 11733, 13, 50688], "temperature": 0.0, "avg_logprob": -0.08476024203830296, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.09266466647386551}, {"id": 1059, "seek": 542352, "start": 5430.64, "end": 5436.320000000001, "text": " It's about a system and a framework that you can hang complexity. It's a system that can then", "tokens": [50720, 467, 311, 466, 257, 1185, 293, 257, 8388, 300, 291, 393, 3967, 14024, 13, 467, 311, 257, 1185, 300, 393, 550, 51004], "temperature": 0.0, "avg_logprob": -0.08476024203830296, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.09266466647386551}, {"id": 1060, "seek": 542352, "start": 5436.320000000001, "end": 5439.84, "text": " generalize and it can work on problems that are bigger than fit in one human's head,", "tokens": [51004, 2674, 1125, 293, 309, 393, 589, 322, 2740, 300, 366, 3801, 813, 3318, 294, 472, 1952, 311, 1378, 11, 51180], "temperature": 0.0, "avg_logprob": -0.08476024203830296, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.09266466647386551}, {"id": 1061, "seek": 542352, "start": 5440.96, "end": 5445.68, "text": " right? And so what that means, what a good stack and what the modular stack provides is", "tokens": [51236, 558, 30, 400, 370, 437, 300, 1355, 11, 437, 257, 665, 8630, 293, 437, 264, 31111, 8630, 6417, 307, 51472], "temperature": 0.0, "avg_logprob": -0.08476024203830296, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.09266466647386551}, {"id": 1062, "seek": 542352, "start": 5446.240000000001, "end": 5450.080000000001, "text": " the ability to walk up to it with a new problem and it'll generally work quite well.", "tokens": [51500, 264, 3485, 281, 1792, 493, 281, 309, 365, 257, 777, 1154, 293, 309, 603, 5101, 589, 1596, 731, 13, 51692], "temperature": 0.0, "avg_logprob": -0.08476024203830296, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.09266466647386551}, {"id": 1063, "seek": 545008, "start": 5450.88, "end": 5453.76, "text": " And that's something that a lot of machine learning infrastructure and tools and", "tokens": [50404, 400, 300, 311, 746, 300, 257, 688, 295, 3479, 2539, 6896, 293, 3873, 293, 50548], "temperature": 0.0, "avg_logprob": -0.1606964076961483, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0011334615992382169}, {"id": 1064, "seek": 545008, "start": 5453.76, "end": 5458.5599999999995, "text": " technologies don't have. Typical state of the art today is you walk up, particularly if you're", "tokens": [50548, 7943, 500, 380, 362, 13, 17722, 804, 1785, 295, 264, 1523, 965, 307, 291, 1792, 493, 11, 4098, 498, 291, 434, 50788], "temperature": 0.0, "avg_logprob": -0.1606964076961483, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0011334615992382169}, {"id": 1065, "seek": 545008, "start": 5458.5599999999995, "end": 5462.5599999999995, "text": " deploying, if you walk up with a new model, you try to push it through the converter and the converter", "tokens": [50788, 34198, 11, 498, 291, 1792, 493, 365, 257, 777, 2316, 11, 291, 853, 281, 2944, 309, 807, 264, 33905, 293, 264, 33905, 50988], "temperature": 0.0, "avg_logprob": -0.1606964076961483, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0011334615992382169}, {"id": 1066, "seek": 545008, "start": 5462.5599999999995, "end": 5471.28, "text": " crashes. That's crazy. The state of ML tooling today is not anything that a C programmer would", "tokens": [50988, 28642, 13, 663, 311, 3219, 13, 440, 1785, 295, 21601, 46593, 965, 307, 406, 1340, 300, 257, 383, 32116, 576, 51424], "temperature": 0.0, "avg_logprob": -0.1606964076961483, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0011334615992382169}, {"id": 1067, "seek": 545008, "start": 5471.28, "end": 5476.5599999999995, "text": " ever accept, right? And it's always been this kind of flaky set of tooling that's never been", "tokens": [51424, 1562, 3241, 11, 558, 30, 400, 309, 311, 1009, 668, 341, 733, 295, 932, 15681, 992, 295, 46593, 300, 311, 1128, 668, 51688], "temperature": 0.0, "avg_logprob": -0.1606964076961483, "compression_ratio": 1.713235294117647, "no_speech_prob": 0.0011334615992382169}, {"id": 1068, "seek": 547656, "start": 5477.200000000001, "end": 5482.88, "text": " integrated well and it's been never worked together because it's not designed together.", "tokens": [50396, 10919, 731, 293, 309, 311, 668, 1128, 2732, 1214, 570, 309, 311, 406, 4761, 1214, 13, 50680], "temperature": 0.0, "avg_logprob": -0.12312289004055958, "compression_ratio": 1.825910931174089, "no_speech_prob": 0.004131750203669071}, {"id": 1069, "seek": 547656, "start": 5482.88, "end": 5486.240000000001, "text": " It's built by different teams. It's built by different hardware vendors. It's built by different", "tokens": [50680, 467, 311, 3094, 538, 819, 5491, 13, 467, 311, 3094, 538, 819, 8837, 22056, 13, 467, 311, 3094, 538, 819, 50848], "temperature": 0.0, "avg_logprob": -0.12312289004055958, "compression_ratio": 1.825910931174089, "no_speech_prob": 0.004131750203669071}, {"id": 1070, "seek": 547656, "start": 5486.240000000001, "end": 5489.76, "text": " systems. It's built by different internet companies that are trying to solve their", "tokens": [50848, 3652, 13, 467, 311, 3094, 538, 819, 4705, 3431, 300, 366, 1382, 281, 5039, 641, 51024], "temperature": 0.0, "avg_logprob": -0.12312289004055958, "compression_ratio": 1.825910931174089, "no_speech_prob": 0.004131750203669071}, {"id": 1071, "seek": 547656, "start": 5489.76, "end": 5494.8, "text": " problems, right? And so that means that we get this fragmented, terrible mess of complexity.", "tokens": [51024, 2740, 11, 558, 30, 400, 370, 300, 1355, 300, 321, 483, 341, 9241, 14684, 11, 6237, 2082, 295, 14024, 13, 51276], "temperature": 0.0, "avg_logprob": -0.12312289004055958, "compression_ratio": 1.825910931174089, "no_speech_prob": 0.004131750203669071}, {"id": 1072, "seek": 547656, "start": 5495.84, "end": 5500.64, "text": " So, I mean, the specifics of, I mean, Jeremy showed this. There's the vectorized function,", "tokens": [51328, 407, 11, 286, 914, 11, 264, 28454, 295, 11, 286, 914, 11, 17809, 4712, 341, 13, 821, 311, 264, 8062, 1602, 2445, 11, 51568], "temperature": 0.0, "avg_logprob": -0.12312289004055958, "compression_ratio": 1.825910931174089, "no_speech_prob": 0.004131750203669071}, {"id": 1073, "seek": 550064, "start": 5500.72, "end": 5509.360000000001, "text": " which I guess is built into Mojo. Vectorized as he showed is built into the library.", "tokens": [50368, 597, 286, 2041, 307, 3094, 666, 3335, 5134, 13, 691, 20814, 1602, 382, 415, 4712, 307, 3094, 666, 264, 6405, 13, 50800], "temperature": 0.0, "avg_logprob": -0.2050426403681437, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.040803827345371246}, {"id": 1074, "seek": 550064, "start": 5509.360000000001, "end": 5515.76, "text": " Into the library, instead of the library. Vectorized, parallelized, which vectorizes more", "tokens": [50800, 23373, 264, 6405, 11, 2602, 295, 264, 6405, 13, 691, 20814, 1602, 11, 8952, 1602, 11, 597, 8062, 5660, 544, 51120], "temperature": 0.0, "avg_logprob": -0.2050426403681437, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.040803827345371246}, {"id": 1075, "seek": 550064, "start": 5515.76, "end": 5520.8, "text": " low level, parallelizes higher level. There's the tiling thing, which is how he demonstrated the", "tokens": [51120, 2295, 1496, 11, 8952, 5660, 2946, 1496, 13, 821, 311, 264, 256, 4883, 551, 11, 597, 307, 577, 415, 18772, 264, 51372], "temperature": 0.0, "avg_logprob": -0.2050426403681437, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.040803827345371246}, {"id": 1076, "seek": 550064, "start": 5522.400000000001, "end": 5529.52, "text": " autotune, I think. So, think about this in like levels, hierarchical levels of abstraction, right?", "tokens": [51452, 1476, 310, 2613, 11, 286, 519, 13, 407, 11, 519, 466, 341, 294, 411, 4358, 11, 35250, 804, 4358, 295, 37765, 11, 558, 30, 51808], "temperature": 0.0, "avg_logprob": -0.2050426403681437, "compression_ratio": 1.7452830188679245, "no_speech_prob": 0.040803827345371246}, {"id": 1077, "seek": 552952, "start": 5529.52, "end": 5534.160000000001, "text": " And so, at the very, if you zoom all the way into a compute problem, you have one floating", "tokens": [50364, 400, 370, 11, 412, 264, 588, 11, 498, 291, 8863, 439, 264, 636, 666, 257, 14722, 1154, 11, 291, 362, 472, 12607, 50596], "temperature": 0.0, "avg_logprob": -0.11492515677836404, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.002550360281020403}, {"id": 1078, "seek": 552952, "start": 5534.160000000001, "end": 5538.72, "text": " point number, right? And so, then you say, okay, I want to be, I can do things one at a time", "tokens": [50596, 935, 1230, 11, 558, 30, 400, 370, 11, 550, 291, 584, 11, 1392, 11, 286, 528, 281, 312, 11, 286, 393, 360, 721, 472, 412, 257, 565, 50824], "temperature": 0.0, "avg_logprob": -0.11492515677836404, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.002550360281020403}, {"id": 1079, "seek": 552952, "start": 5538.72, "end": 5544.8, "text": " in an interpreter. It's pretty slow, right? So, I can get to doing one at a time in a compiler,", "tokens": [50824, 294, 364, 34132, 13, 467, 311, 1238, 2964, 11, 558, 30, 407, 11, 286, 393, 483, 281, 884, 472, 412, 257, 565, 294, 257, 31958, 11, 51128], "temperature": 0.0, "avg_logprob": -0.11492515677836404, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.002550360281020403}, {"id": 1080, "seek": 552952, "start": 5544.8, "end": 5551.120000000001, "text": " I can see. Then I can get to doing four or eight or 16 at a time with vectors. That's called", "tokens": [51128, 286, 393, 536, 13, 1396, 286, 393, 483, 281, 884, 1451, 420, 3180, 420, 3165, 412, 257, 565, 365, 18875, 13, 663, 311, 1219, 51444], "temperature": 0.0, "avg_logprob": -0.11492515677836404, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.002550360281020403}, {"id": 1081, "seek": 552952, "start": 5551.120000000001, "end": 5556.400000000001, "text": " vectorization. Then you can say, hey, I have a whole bunch of different, you know, what a", "tokens": [51444, 8062, 2144, 13, 1396, 291, 393, 584, 11, 4177, 11, 286, 362, 257, 1379, 3840, 295, 819, 11, 291, 458, 11, 437, 257, 51708], "temperature": 0.0, "avg_logprob": -0.11492515677836404, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.002550360281020403}, {"id": 1082, "seek": 555640, "start": 5556.4, "end": 5561.92, "text": " multi-core computer is, it's basically a bunch of computers, right? So, they're all independent", "tokens": [50364, 4825, 12, 12352, 3820, 307, 11, 309, 311, 1936, 257, 3840, 295, 10807, 11, 558, 30, 407, 11, 436, 434, 439, 6695, 50640], "temperature": 0.0, "avg_logprob": -0.10892616548845845, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0007321469602175057}, {"id": 1083, "seek": 555640, "start": 5561.92, "end": 5566.5599999999995, "text": " computers that can talk to each other and they share memory. And so, now what parallelized does,", "tokens": [50640, 10807, 300, 393, 751, 281, 1184, 661, 293, 436, 2073, 4675, 13, 400, 370, 11, 586, 437, 8952, 1602, 775, 11, 50872], "temperature": 0.0, "avg_logprob": -0.10892616548845845, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0007321469602175057}, {"id": 1084, "seek": 555640, "start": 5566.5599999999995, "end": 5571.5199999999995, "text": " it says, okay, run multiple instances on different computers. And now they can all work together", "tokens": [50872, 309, 1619, 11, 1392, 11, 1190, 3866, 14519, 322, 819, 10807, 13, 400, 586, 436, 393, 439, 589, 1214, 51120], "temperature": 0.0, "avg_logprob": -0.10892616548845845, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0007321469602175057}, {"id": 1085, "seek": 555640, "start": 5571.5199999999995, "end": 5575.679999999999, "text": " on a problem, right? And so, what you're doing is you're saying, keep going out to the next level", "tokens": [51120, 322, 257, 1154, 11, 558, 30, 400, 370, 11, 437, 291, 434, 884, 307, 291, 434, 1566, 11, 1066, 516, 484, 281, 264, 958, 1496, 51328], "temperature": 0.0, "avg_logprob": -0.10892616548845845, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0007321469602175057}, {"id": 1086, "seek": 555640, "start": 5575.679999999999, "end": 5582.24, "text": " out. And as you do that, how do I take advantage of this? So, tiling is a memory optimization,", "tokens": [51328, 484, 13, 400, 382, 291, 360, 300, 11, 577, 360, 286, 747, 5002, 295, 341, 30, 407, 11, 256, 4883, 307, 257, 4675, 19618, 11, 51656], "temperature": 0.0, "avg_logprob": -0.10892616548845845, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0007321469602175057}, {"id": 1087, "seek": 558224, "start": 5582.24, "end": 5586.719999999999, "text": " right? It says, okay, let's make sure that we're keeping the data close to the compute", "tokens": [50364, 558, 30, 467, 1619, 11, 1392, 11, 718, 311, 652, 988, 300, 321, 434, 5145, 264, 1412, 1998, 281, 264, 14722, 50588], "temperature": 0.0, "avg_logprob": -0.12580586137442754, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0052193449810147285}, {"id": 1088, "seek": 558224, "start": 5586.719999999999, "end": 5592.08, "text": " part of the problem, instead of sending it all back and forth through memory every, every time I", "tokens": [50588, 644, 295, 264, 1154, 11, 2602, 295, 7750, 309, 439, 646, 293, 5220, 807, 4675, 633, 11, 633, 565, 286, 50856], "temperature": 0.0, "avg_logprob": -0.12580586137442754, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0052193449810147285}, {"id": 1089, "seek": 558224, "start": 5592.08, "end": 5597.12, "text": " load a block. And the size of the block, size is all, that's how you get to the autotune to make", "tokens": [50856, 3677, 257, 3461, 13, 400, 264, 2744, 295, 264, 3461, 11, 2744, 307, 439, 11, 300, 311, 577, 291, 483, 281, 264, 1476, 310, 2613, 281, 652, 51108], "temperature": 0.0, "avg_logprob": -0.12580586137442754, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0052193449810147285}, {"id": 1090, "seek": 558224, "start": 5597.12, "end": 5600.719999999999, "text": " sure it's optimized. Right. Yeah. Well, so all of these, the details matter so much to get good", "tokens": [51108, 988, 309, 311, 26941, 13, 1779, 13, 865, 13, 1042, 11, 370, 439, 295, 613, 11, 264, 4365, 1871, 370, 709, 281, 483, 665, 51288], "temperature": 0.0, "avg_logprob": -0.12580586137442754, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0052193449810147285}, {"id": 1091, "seek": 558224, "start": 5600.719999999999, "end": 5605.84, "text": " performance. This is another funny thing about machine learning and high performance computing", "tokens": [51288, 3389, 13, 639, 307, 1071, 4074, 551, 466, 3479, 2539, 293, 1090, 3389, 15866, 51544], "temperature": 0.0, "avg_logprob": -0.12580586137442754, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0052193449810147285}, {"id": 1092, "seek": 558224, "start": 5605.84, "end": 5611.679999999999, "text": " that is very different than C compilers we all grew up with, where, you know, if you get a new", "tokens": [51544, 300, 307, 588, 819, 813, 383, 715, 388, 433, 321, 439, 6109, 493, 365, 11, 689, 11, 291, 458, 11, 498, 291, 483, 257, 777, 51836], "temperature": 0.0, "avg_logprob": -0.12580586137442754, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.0052193449810147285}, {"id": 1093, "seek": 561168, "start": 5611.68, "end": 5616.240000000001, "text": " version of GCC or a new version of Clang or something like that, you know, maybe something", "tokens": [50364, 3037, 295, 460, 11717, 420, 257, 777, 3037, 295, 2033, 656, 420, 746, 411, 300, 11, 291, 458, 11, 1310, 746, 50592], "temperature": 0.0, "avg_logprob": -0.10484642028808594, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.010324724018573761}, {"id": 1094, "seek": 561168, "start": 5616.240000000001, "end": 5622.88, "text": " will go 1% faster, right? And so compiler insurers will work really, really, really hard to get half", "tokens": [50592, 486, 352, 502, 4, 4663, 11, 558, 30, 400, 370, 31958, 1028, 14198, 486, 589, 534, 11, 534, 11, 534, 1152, 281, 483, 1922, 50924], "temperature": 0.0, "avg_logprob": -0.10484642028808594, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.010324724018573761}, {"id": 1095, "seek": 561168, "start": 5622.88, "end": 5628.96, "text": " a percent out of your C code, something like that. But when you're talking about an accelerator or", "tokens": [50924, 257, 3043, 484, 295, 428, 383, 3089, 11, 746, 411, 300, 13, 583, 562, 291, 434, 1417, 466, 364, 39889, 420, 51228], "temperature": 0.0, "avg_logprob": -0.10484642028808594, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.010324724018573761}, {"id": 1096, "seek": 561168, "start": 5628.96, "end": 5634.240000000001, "text": " an AI application, or you're talking about these kinds of algorithms, and these are things people", "tokens": [51228, 364, 7318, 3861, 11, 420, 291, 434, 1417, 466, 613, 3685, 295, 14642, 11, 293, 613, 366, 721, 561, 51492], "temperature": 0.0, "avg_logprob": -0.10484642028808594, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.010324724018573761}, {"id": 1097, "seek": 561168, "start": 5634.240000000001, "end": 5640.08, "text": " used to write in Fortran, for example, right? If you get it wrong, it's not 5% or 1%. It could", "tokens": [51492, 1143, 281, 2464, 294, 11002, 4257, 11, 337, 1365, 11, 558, 30, 759, 291, 483, 309, 2085, 11, 309, 311, 406, 1025, 4, 420, 502, 6856, 467, 727, 51784], "temperature": 0.0, "avg_logprob": -0.10484642028808594, "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.010324724018573761}, {"id": 1098, "seek": 564008, "start": 5640.08, "end": 5647.44, "text": " be 2x or 10x. Right. If you think about it, you really want to make use of the full memory you", "tokens": [50364, 312, 568, 87, 420, 1266, 87, 13, 1779, 13, 759, 291, 519, 466, 309, 11, 291, 534, 528, 281, 652, 764, 295, 264, 1577, 4675, 291, 50732], "temperature": 0.0, "avg_logprob": -0.09138554142367455, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.004331024829298258}, {"id": 1099, "seek": 564008, "start": 5647.44, "end": 5651.92, "text": " have the cache, for example. But if you use too much space, it doesn't fit in the cache. Now you're", "tokens": [50732, 362, 264, 19459, 11, 337, 1365, 13, 583, 498, 291, 764, 886, 709, 1901, 11, 309, 1177, 380, 3318, 294, 264, 19459, 13, 823, 291, 434, 50956], "temperature": 0.0, "avg_logprob": -0.09138554142367455, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.004331024829298258}, {"id": 1100, "seek": 564008, "start": 5651.92, "end": 5657.44, "text": " going to be thrashing all the way back out to main memory. And these can be 2x, 10x, major", "tokens": [50956, 516, 281, 312, 739, 11077, 439, 264, 636, 646, 484, 281, 2135, 4675, 13, 400, 613, 393, 312, 568, 87, 11, 1266, 87, 11, 2563, 51232], "temperature": 0.0, "avg_logprob": -0.09138554142367455, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.004331024829298258}, {"id": 1101, "seek": 564008, "start": 5657.44, "end": 5661.84, "text": " performance differences. And so this is where getting these magic numbers and these things right", "tokens": [51232, 3389, 7300, 13, 400, 370, 341, 307, 689, 1242, 613, 5585, 3547, 293, 613, 721, 558, 51452], "temperature": 0.0, "avg_logprob": -0.09138554142367455, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.004331024829298258}, {"id": 1102, "seek": 564008, "start": 5661.84, "end": 5667.28, "text": " is really actually quite important. So you mentioned that Moji is a superset of Python.", "tokens": [51452, 307, 534, 767, 1596, 1021, 13, 407, 291, 2835, 300, 3335, 4013, 307, 257, 37906, 302, 295, 15329, 13, 51724], "temperature": 0.0, "avg_logprob": -0.09138554142367455, "compression_ratio": 1.6151202749140894, "no_speech_prob": 0.004331024829298258}, {"id": 1103, "seek": 566728, "start": 5667.36, "end": 5682.16, "text": " Can you run Python code as if it's Mojo code? Yes. Yes. And this has two sides of it. So Mojo's not", "tokens": [50368, 1664, 291, 1190, 15329, 3089, 382, 498, 309, 311, 3335, 5134, 3089, 30, 1079, 13, 1079, 13, 400, 341, 575, 732, 4881, 295, 309, 13, 407, 3335, 5134, 311, 406, 51108], "temperature": 0.0, "avg_logprob": -0.1882523840123957, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.007344291545450687}, {"id": 1104, "seek": 566728, "start": 5682.16, "end": 5686.32, "text": " done yet. So I'll give you a disclaimer. Mojo's not done yet. But already we see people that take", "tokens": [51108, 1096, 1939, 13, 407, 286, 603, 976, 291, 257, 40896, 13, 3335, 5134, 311, 406, 1096, 1939, 13, 583, 1217, 321, 536, 561, 300, 747, 51316], "temperature": 0.0, "avg_logprob": -0.1882523840123957, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.007344291545450687}, {"id": 1105, "seek": 566728, "start": 5686.88, "end": 5692.639999999999, "text": " small pieces of Python code, move it over. They don't change it. And you can get 12x speedups.", "tokens": [51344, 1359, 3755, 295, 15329, 3089, 11, 1286, 309, 670, 13, 814, 500, 380, 1319, 309, 13, 400, 291, 393, 483, 2272, 87, 3073, 7528, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1882523840123957, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.007344291545450687}, {"id": 1106, "seek": 569264, "start": 5692.72, "end": 5696.8, "text": " Somebody was just tweeting about that yesterday, which is pretty cool. And again,", "tokens": [50368, 13463, 390, 445, 40090, 466, 300, 5186, 11, 597, 307, 1238, 1627, 13, 400, 797, 11, 50572], "temperature": 0.0, "avg_logprob": -0.15586874040506654, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.03845540061593056}, {"id": 1107, "seek": 569264, "start": 5696.8, "end": 5703.76, "text": " interpreters, compilers. And so without changing any code, also this is not JIT compiling or do", "tokens": [50572, 17489, 1559, 11, 715, 388, 433, 13, 400, 370, 1553, 4473, 604, 3089, 11, 611, 341, 307, 406, 508, 3927, 715, 4883, 420, 360, 50920], "temperature": 0.0, "avg_logprob": -0.15586874040506654, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.03845540061593056}, {"id": 1108, "seek": 569264, "start": 5703.76, "end": 5710.0, "text": " anything fancy. This is just basic stuff. Move it straight over. Now Mojo will continue to grow", "tokens": [50920, 1340, 10247, 13, 639, 307, 445, 3875, 1507, 13, 10475, 309, 2997, 670, 13, 823, 3335, 5134, 486, 2354, 281, 1852, 51232], "temperature": 0.0, "avg_logprob": -0.15586874040506654, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.03845540061593056}, {"id": 1109, "seek": 569264, "start": 5710.0, "end": 5714.64, "text": " out. And as it grows out, it will have more and more and more features. And our North Star is to be", "tokens": [51232, 484, 13, 400, 382, 309, 13156, 484, 11, 309, 486, 362, 544, 293, 544, 293, 544, 4122, 13, 400, 527, 4067, 5705, 307, 281, 312, 51464], "temperature": 0.0, "avg_logprob": -0.15586874040506654, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.03845540061593056}, {"id": 1110, "seek": 569264, "start": 5714.64, "end": 5719.200000000001, "text": " a full superset of Python. And so you can bring over basically arbitrary Python code and have it", "tokens": [51464, 257, 1577, 37906, 302, 295, 15329, 13, 400, 370, 291, 393, 1565, 670, 1936, 23211, 15329, 3089, 293, 362, 309, 51692], "temperature": 0.0, "avg_logprob": -0.15586874040506654, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.03845540061593056}, {"id": 1111, "seek": 571920, "start": 5719.2, "end": 5725.76, "text": " just work. And it may not always be 12x faster, but it should be at least as fast and way faster", "tokens": [50364, 445, 589, 13, 400, 309, 815, 406, 1009, 312, 2272, 87, 4663, 11, 457, 309, 820, 312, 412, 1935, 382, 2370, 293, 636, 4663, 50692], "temperature": 0.0, "avg_logprob": -0.16654014587402344, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.01098327524960041}, {"id": 1112, "seek": 571920, "start": 5725.76, "end": 5732.0, "text": " in many cases. This is cool. Right. Now, it will take time to do that. And Python is a complicated", "tokens": [50692, 294, 867, 3331, 13, 639, 307, 1627, 13, 1779, 13, 823, 11, 309, 486, 747, 565, 281, 360, 300, 13, 400, 15329, 307, 257, 6179, 51004], "temperature": 0.0, "avg_logprob": -0.16654014587402344, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.01098327524960041}, {"id": 1113, "seek": 571920, "start": 5732.0, "end": 5737.12, "text": " language. There's not just the obvious things, but there's also non-obvious things that are", "tokens": [51004, 2856, 13, 821, 311, 406, 445, 264, 6322, 721, 11, 457, 456, 311, 611, 2107, 12, 996, 1502, 721, 300, 366, 51260], "temperature": 0.0, "avg_logprob": -0.16654014587402344, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.01098327524960041}, {"id": 1114, "seek": 571920, "start": 5737.12, "end": 5741.76, "text": " complicated. Like we have to be able to talk to CPython packages that talk to the C API.", "tokens": [51260, 6179, 13, 1743, 321, 362, 281, 312, 1075, 281, 751, 281, 22431, 88, 11943, 17401, 300, 751, 281, 264, 383, 9362, 13, 51492], "temperature": 0.0, "avg_logprob": -0.16654014587402344, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.01098327524960041}, {"id": 1115, "seek": 571920, "start": 5741.76, "end": 5747.44, "text": " And there's a bunch of pieces to this. So you have to, I mean, just to make explicit,", "tokens": [51492, 400, 456, 311, 257, 3840, 295, 3755, 281, 341, 13, 407, 291, 362, 281, 11, 286, 914, 11, 445, 281, 652, 13691, 11, 51776], "temperature": 0.0, "avg_logprob": -0.16654014587402344, "compression_ratio": 1.686131386861314, "no_speech_prob": 0.01098327524960041}, {"id": 1116, "seek": 574744, "start": 5747.44, "end": 5753.44, "text": " the obvious may not be so obvious until you think about it. So to run Python code, that means you", "tokens": [50364, 264, 6322, 815, 406, 312, 370, 6322, 1826, 291, 519, 466, 309, 13, 407, 281, 1190, 15329, 3089, 11, 300, 1355, 291, 50664], "temperature": 0.0, "avg_logprob": -0.1162081567864669, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.003706157673150301}, {"id": 1117, "seek": 574744, "start": 5753.44, "end": 5762.48, "text": " have to run all the Python packages and libraries. So that means what? What's the relationship between", "tokens": [50664, 362, 281, 1190, 439, 264, 15329, 17401, 293, 15148, 13, 407, 300, 1355, 437, 30, 708, 311, 264, 2480, 1296, 51116], "temperature": 0.0, "avg_logprob": -0.1162081567864669, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.003706157673150301}, {"id": 1118, "seek": 574744, "start": 5762.48, "end": 5769.36, "text": " Mojo and CPython, the interpreter that presumably would be tasked with getting those packages to", "tokens": [51116, 3335, 5134, 293, 22431, 88, 11943, 11, 264, 34132, 300, 26742, 576, 312, 38621, 365, 1242, 729, 17401, 281, 51460], "temperature": 0.0, "avg_logprob": -0.1162081567864669, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.003706157673150301}, {"id": 1119, "seek": 574744, "start": 5769.36, "end": 5775.44, "text": " work? So in the fullness of time, Mojo will solve for all the problems and you'll be able to move", "tokens": [51460, 589, 30, 407, 294, 264, 45262, 295, 565, 11, 3335, 5134, 486, 5039, 337, 439, 264, 2740, 293, 291, 603, 312, 1075, 281, 1286, 51764], "temperature": 0.0, "avg_logprob": -0.1162081567864669, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.003706157673150301}, {"id": 1120, "seek": 577544, "start": 5775.44, "end": 5780.719999999999, "text": " Python packages over and run them in Mojo. Without the CPython. Without CPython. Someday.", "tokens": [50364, 15329, 17401, 670, 293, 1190, 552, 294, 3335, 5134, 13, 9129, 264, 22431, 88, 11943, 13, 9129, 22431, 88, 11943, 13, 12297, 16826, 13, 50628], "temperature": 0.0, "avg_logprob": -0.10681402115594774, "compression_ratio": 1.7206349206349207, "no_speech_prob": 0.014951057732105255}, {"id": 1121, "seek": 577544, "start": 5781.28, "end": 5785.759999999999, "text": " Yeah. Right. It's not today, but someday. And that'll be a beautiful day because then you'll get", "tokens": [50656, 865, 13, 1779, 13, 467, 311, 406, 965, 11, 457, 19412, 13, 400, 300, 603, 312, 257, 2238, 786, 570, 550, 291, 603, 483, 50880], "temperature": 0.0, "avg_logprob": -0.10681402115594774, "compression_ratio": 1.7206349206349207, "no_speech_prob": 0.014951057732105255}, {"id": 1122, "seek": 577544, "start": 5785.759999999999, "end": 5788.96, "text": " a whole bunch of advantages and you'll get massive speedups and things like this.", "tokens": [50880, 257, 1379, 3840, 295, 14906, 293, 291, 603, 483, 5994, 3073, 7528, 293, 721, 411, 341, 13, 51040], "temperature": 0.0, "avg_logprob": -0.10681402115594774, "compression_ratio": 1.7206349206349207, "no_speech_prob": 0.014951057732105255}, {"id": 1123, "seek": 577544, "start": 5788.96, "end": 5791.5199999999995, "text": " But you can do that one at a time, right? You can move packages one at a time.", "tokens": [51040, 583, 291, 393, 360, 300, 472, 412, 257, 565, 11, 558, 30, 509, 393, 1286, 17401, 472, 412, 257, 565, 13, 51168], "temperature": 0.0, "avg_logprob": -0.10681402115594774, "compression_ratio": 1.7206349206349207, "no_speech_prob": 0.014951057732105255}, {"id": 1124, "seek": 577544, "start": 5791.5199999999995, "end": 5796.799999999999, "text": " Exactly. But we're not willing to wait for that. Python is too important. The ecosystem is too", "tokens": [51168, 7587, 13, 583, 321, 434, 406, 4950, 281, 1699, 337, 300, 13, 15329, 307, 886, 1021, 13, 440, 11311, 307, 886, 51432], "temperature": 0.0, "avg_logprob": -0.10681402115594774, "compression_ratio": 1.7206349206349207, "no_speech_prob": 0.014951057732105255}, {"id": 1125, "seek": 577544, "start": 5796.799999999999, "end": 5802.639999999999, "text": " broad. We want to both be able to build Mojo out. We also want to do it the right way without time,", "tokens": [51432, 4152, 13, 492, 528, 281, 1293, 312, 1075, 281, 1322, 3335, 5134, 484, 13, 492, 611, 528, 281, 360, 309, 264, 558, 636, 1553, 565, 11, 51724], "temperature": 0.0, "avg_logprob": -0.10681402115594774, "compression_ratio": 1.7206349206349207, "no_speech_prob": 0.014951057732105255}, {"id": 1126, "seek": 580264, "start": 5803.200000000001, "end": 5808.4800000000005, "text": " without intense time pressure. We're obviously moving fast. And so what we do is we say, okay,", "tokens": [50392, 1553, 9447, 565, 3321, 13, 492, 434, 2745, 2684, 2370, 13, 400, 370, 437, 321, 360, 307, 321, 584, 11, 1392, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1366478337181939, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0030750157311558723}, {"id": 1127, "seek": 580264, "start": 5808.4800000000005, "end": 5814.64, "text": " well, let's make it so you can import an arbitrary existing package, arbitrary,", "tokens": [50656, 731, 11, 718, 311, 652, 309, 370, 291, 393, 974, 364, 23211, 6741, 7372, 11, 23211, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1366478337181939, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0030750157311558723}, {"id": 1128, "seek": 580264, "start": 5815.68, "end": 5819.76, "text": " including you write your own on your local disk or whatever. It's not like a standard", "tokens": [51016, 3009, 291, 2464, 428, 1065, 322, 428, 2654, 12355, 420, 2035, 13, 467, 311, 406, 411, 257, 3832, 51220], "temperature": 0.0, "avg_logprob": -0.1366478337181939, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0030750157311558723}, {"id": 1129, "seek": 580264, "start": 5819.76, "end": 5825.68, "text": " like an arbitrary package. And import that using CPython. Because CPython already runs all the", "tokens": [51220, 411, 364, 23211, 7372, 13, 400, 974, 300, 1228, 22431, 88, 11943, 13, 1436, 22431, 88, 11943, 1217, 6676, 439, 264, 51516], "temperature": 0.0, "avg_logprob": -0.1366478337181939, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0030750157311558723}, {"id": 1130, "seek": 580264, "start": 5825.68, "end": 5831.360000000001, "text": " packages. And so what we do is we built an integration layer where we can actually use", "tokens": [51516, 17401, 13, 400, 370, 437, 321, 360, 307, 321, 3094, 364, 10980, 4583, 689, 321, 393, 767, 764, 51800], "temperature": 0.0, "avg_logprob": -0.1366478337181939, "compression_ratio": 1.7198443579766538, "no_speech_prob": 0.0030750157311558723}, {"id": 1131, "seek": 583136, "start": 5831.36, "end": 5837.759999999999, "text": " CPython. Again, I'm practical to actually just load and use all the existing packages as they are.", "tokens": [50364, 22431, 88, 11943, 13, 3764, 11, 286, 478, 8496, 281, 767, 445, 3677, 293, 764, 439, 264, 6741, 17401, 382, 436, 366, 13, 50684], "temperature": 0.0, "avg_logprob": -0.09918095203156167, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.004330293275415897}, {"id": 1132, "seek": 583136, "start": 5838.4, "end": 5841.28, "text": " The downside of that is you don't get the benefits of Mojo for those packages.", "tokens": [50716, 440, 25060, 295, 300, 307, 291, 500, 380, 483, 264, 5311, 295, 3335, 5134, 337, 729, 17401, 13, 50860], "temperature": 0.0, "avg_logprob": -0.09918095203156167, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.004330293275415897}, {"id": 1133, "seek": 583136, "start": 5841.759999999999, "end": 5845.36, "text": " Right. And so they run as fast as they do in the traditional CPython way.", "tokens": [50884, 1779, 13, 400, 370, 436, 1190, 382, 2370, 382, 436, 360, 294, 264, 5164, 22431, 88, 11943, 636, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09918095203156167, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.004330293275415897}, {"id": 1134, "seek": 583136, "start": 5846.48, "end": 5850.48, "text": " But what that does is that gives you an incremental migration path. And so if you say,", "tokens": [51120, 583, 437, 300, 775, 307, 300, 2709, 291, 364, 35759, 17011, 3100, 13, 400, 370, 498, 291, 584, 11, 51320], "temperature": 0.0, "avg_logprob": -0.09918095203156167, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.004330293275415897}, {"id": 1135, "seek": 583136, "start": 5850.48, "end": 5855.839999999999, "text": " hey, cool, well, here's a, you know, the Python ecosystem is vast. I want all of it to just work.", "tokens": [51320, 4177, 11, 1627, 11, 731, 11, 510, 311, 257, 11, 291, 458, 11, 264, 15329, 11311, 307, 8369, 13, 286, 528, 439, 295, 309, 281, 445, 589, 13, 51588], "temperature": 0.0, "avg_logprob": -0.09918095203156167, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.004330293275415897}, {"id": 1136, "seek": 583136, "start": 5855.839999999999, "end": 5859.679999999999, "text": " But there's certain things that are really important. And so if I, if I'm doing weather", "tokens": [51588, 583, 456, 311, 1629, 721, 300, 366, 534, 1021, 13, 400, 370, 498, 286, 11, 498, 286, 478, 884, 5503, 51780], "temperature": 0.0, "avg_logprob": -0.09918095203156167, "compression_ratio": 1.6903225806451614, "no_speech_prob": 0.004330293275415897}, {"id": 1137, "seek": 585968, "start": 5859.68, "end": 5864.320000000001, "text": " forecasting or something, well, I want to be able to load all the data. I want to be able to work", "tokens": [50364, 44331, 420, 746, 11, 731, 11, 286, 528, 281, 312, 1075, 281, 3677, 439, 264, 1412, 13, 286, 528, 281, 312, 1075, 281, 589, 50596], "temperature": 0.0, "avg_logprob": -0.10515960294808914, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0029807991813868284}, {"id": 1138, "seek": 585968, "start": 5864.320000000001, "end": 5868.56, "text": " with it. And then I have my own crazy algorithm inside of it. Well, normally I'd write that in", "tokens": [50596, 365, 309, 13, 400, 550, 286, 362, 452, 1065, 3219, 9284, 1854, 295, 309, 13, 1042, 11, 5646, 286, 1116, 2464, 300, 294, 50808], "temperature": 0.0, "avg_logprob": -0.10515960294808914, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0029807991813868284}, {"id": 1139, "seek": 585968, "start": 5868.56, "end": 5874.88, "text": " C++. If I can write in Mojo and have one system that scales, well, that's way easier to work with.", "tokens": [50808, 383, 25472, 13, 759, 286, 393, 2464, 294, 3335, 5134, 293, 362, 472, 1185, 300, 17408, 11, 731, 11, 300, 311, 636, 3571, 281, 589, 365, 13, 51124], "temperature": 0.0, "avg_logprob": -0.10515960294808914, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0029807991813868284}, {"id": 1140, "seek": 585968, "start": 5874.88, "end": 5880.88, "text": " Is it hard to do that to have that layer that's running CPython? Because is there some", "tokens": [51124, 1119, 309, 1152, 281, 360, 300, 281, 362, 300, 4583, 300, 311, 2614, 22431, 88, 11943, 30, 1436, 307, 456, 512, 51424], "temperature": 0.0, "avg_logprob": -0.10515960294808914, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0029807991813868284}, {"id": 1141, "seek": 585968, "start": 5880.88, "end": 5886.16, "text": " communication back and forth? Yes, it's complicated. I mean, this is what we do. So I mean, we make it", "tokens": [51424, 6101, 646, 293, 5220, 30, 1079, 11, 309, 311, 6179, 13, 286, 914, 11, 341, 307, 437, 321, 360, 13, 407, 286, 914, 11, 321, 652, 309, 51688], "temperature": 0.0, "avg_logprob": -0.10515960294808914, "compression_ratio": 1.6701388888888888, "no_speech_prob": 0.0029807991813868284}, {"id": 1142, "seek": 588616, "start": 5886.16, "end": 5893.28, "text": " look easy, but it is, it is complicated. But what we do is we use the CPython existing interpreter.", "tokens": [50364, 574, 1858, 11, 457, 309, 307, 11, 309, 307, 6179, 13, 583, 437, 321, 360, 307, 321, 764, 264, 22431, 88, 11943, 6741, 34132, 13, 50720], "temperature": 0.0, "avg_logprob": -0.08908018001840134, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.005553740076720715}, {"id": 1143, "seek": 588616, "start": 5893.28, "end": 5896.48, "text": " So it's running its own byte codes, and that's how it provides full compatibility.", "tokens": [50720, 407, 309, 311, 2614, 1080, 1065, 40846, 14211, 11, 293, 300, 311, 577, 309, 6417, 1577, 34237, 13, 50880], "temperature": 0.0, "avg_logprob": -0.08908018001840134, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.005553740076720715}, {"id": 1144, "seek": 588616, "start": 5897.12, "end": 5903.92, "text": " And then it gives us CPython objects. And we use those objects as is. And so that way,", "tokens": [50912, 400, 550, 309, 2709, 505, 22431, 88, 11943, 6565, 13, 400, 321, 764, 729, 6565, 382, 307, 13, 400, 370, 300, 636, 11, 51252], "temperature": 0.0, "avg_logprob": -0.08908018001840134, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.005553740076720715}, {"id": 1145, "seek": 588616, "start": 5903.92, "end": 5909.44, "text": " we're fully compatible with all the CPython objects and all the, you know, it's not just the", "tokens": [51252, 321, 434, 4498, 18218, 365, 439, 264, 22431, 88, 11943, 6565, 293, 439, 264, 11, 291, 458, 11, 309, 311, 406, 445, 264, 51528], "temperature": 0.0, "avg_logprob": -0.08908018001840134, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.005553740076720715}, {"id": 1146, "seek": 588616, "start": 5909.44, "end": 5913.599999999999, "text": " Python part, it's also the C packages, the C libraries underneath them, because they're often", "tokens": [51528, 15329, 644, 11, 309, 311, 611, 264, 383, 17401, 11, 264, 383, 15148, 7223, 552, 11, 570, 436, 434, 2049, 51736], "temperature": 0.0, "avg_logprob": -0.08908018001840134, "compression_ratio": 1.7606177606177607, "no_speech_prob": 0.005553740076720715}, {"id": 1147, "seek": 591360, "start": 5913.6, "end": 5918.0, "text": " hybrid. And so we can fully run and we're fully compatible with all that. And the way we do that", "tokens": [50364, 13051, 13, 400, 370, 321, 393, 4498, 1190, 293, 321, 434, 4498, 18218, 365, 439, 300, 13, 400, 264, 636, 321, 360, 300, 50584], "temperature": 0.0, "avg_logprob": -0.08269290924072266, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.013633600436151028}, {"id": 1148, "seek": 591360, "start": 5918.0, "end": 5923.04, "text": " is that we have to play by the rules, right? And so we keep objects in that representation", "tokens": [50584, 307, 300, 321, 362, 281, 862, 538, 264, 4474, 11, 558, 30, 400, 370, 321, 1066, 6565, 294, 300, 10290, 50836], "temperature": 0.0, "avg_logprob": -0.08269290924072266, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.013633600436151028}, {"id": 1149, "seek": 591360, "start": 5923.04, "end": 5927.280000000001, "text": " when they're coming from that world. What's the representation that's being used in memory?", "tokens": [50836, 562, 436, 434, 1348, 490, 300, 1002, 13, 708, 311, 264, 10290, 300, 311, 885, 1143, 294, 4675, 30, 51048], "temperature": 0.0, "avg_logprob": -0.08269290924072266, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.013633600436151028}, {"id": 1150, "seek": 591360, "start": 5927.280000000001, "end": 5933.200000000001, "text": " We'd have to know a lot about how the CPython interpreter works. It has, for example, reference", "tokens": [51048, 492, 1116, 362, 281, 458, 257, 688, 466, 577, 264, 22431, 88, 11943, 34132, 1985, 13, 467, 575, 11, 337, 1365, 11, 6408, 51344], "temperature": 0.0, "avg_logprob": -0.08269290924072266, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.013633600436151028}, {"id": 1151, "seek": 591360, "start": 5933.200000000001, "end": 5937.84, "text": " counting, but also different rules on how to pass pointers around and things like this. Super", "tokens": [51344, 13251, 11, 457, 611, 819, 4474, 322, 577, 281, 1320, 44548, 926, 293, 721, 411, 341, 13, 4548, 51576], "temperature": 0.0, "avg_logprob": -0.08269290924072266, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.013633600436151028}, {"id": 1152, "seek": 593784, "start": 5937.84, "end": 5943.4400000000005, "text": " low level fiddly. And it's not like Python, it's like how the interpreter works. Okay. And so that", "tokens": [50364, 2295, 1496, 283, 14273, 356, 13, 400, 309, 311, 406, 411, 15329, 11, 309, 311, 411, 577, 264, 34132, 1985, 13, 1033, 13, 400, 370, 300, 50644], "temperature": 0.0, "avg_logprob": -0.11084351172814003, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.21720267832279205}, {"id": 1153, "seek": 593784, "start": 5943.4400000000005, "end": 5948.0, "text": " gets all exposed out. And then you have to define wrappers around the low level C code.", "tokens": [50644, 2170, 439, 9495, 484, 13, 400, 550, 291, 362, 281, 6964, 7843, 15226, 926, 264, 2295, 1496, 383, 3089, 13, 50872], "temperature": 0.0, "avg_logprob": -0.11084351172814003, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.21720267832279205}, {"id": 1154, "seek": 593784, "start": 5948.56, "end": 5955.360000000001, "text": " Right. And so what this means is you have to know not only C, which is a different world from Python,", "tokens": [50900, 1779, 13, 400, 370, 437, 341, 1355, 307, 291, 362, 281, 458, 406, 787, 383, 11, 597, 307, 257, 819, 1002, 490, 15329, 11, 51240], "temperature": 0.0, "avg_logprob": -0.11084351172814003, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.21720267832279205}, {"id": 1155, "seek": 593784, "start": 5955.360000000001, "end": 5960.56, "text": " obviously, not only Python, but the wrappers, but the interpreter and the wrappers and the", "tokens": [51240, 2745, 11, 406, 787, 15329, 11, 457, 264, 7843, 15226, 11, 457, 264, 34132, 293, 264, 7843, 15226, 293, 264, 51500], "temperature": 0.0, "avg_logprob": -0.11084351172814003, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.21720267832279205}, {"id": 1156, "seek": 593784, "start": 5960.56, "end": 5964.88, "text": " implementation details and the conventions. And it's just this really complicated mess.", "tokens": [51500, 11420, 4365, 293, 264, 33520, 13, 400, 309, 311, 445, 341, 534, 6179, 2082, 13, 51716], "temperature": 0.0, "avg_logprob": -0.11084351172814003, "compression_ratio": 1.8458498023715415, "no_speech_prob": 0.21720267832279205}, {"id": 1157, "seek": 596488, "start": 5964.88, "end": 5968.08, "text": " And when you do that, now suddenly you have a debugger that debugs Python,", "tokens": [50364, 400, 562, 291, 360, 300, 11, 586, 5800, 291, 362, 257, 24083, 1321, 300, 3001, 14950, 15329, 11, 50524], "temperature": 0.0, "avg_logprob": -0.13014382934570312, "compression_ratio": 1.6512455516014235, "no_speech_prob": 0.0028006513603031635}, {"id": 1158, "seek": 596488, "start": 5968.64, "end": 5975.52, "text": " they can't step into C code. So you have this two world problem, right? And so by pulling this all", "tokens": [50552, 436, 393, 380, 1823, 666, 383, 3089, 13, 407, 291, 362, 341, 732, 1002, 1154, 11, 558, 30, 400, 370, 538, 8407, 341, 439, 50896], "temperature": 0.0, "avg_logprob": -0.13014382934570312, "compression_ratio": 1.6512455516014235, "no_speech_prob": 0.0028006513603031635}, {"id": 1159, "seek": 596488, "start": 5975.52, "end": 5981.2, "text": " into Mojo, what you get is you get one world. You get the ability to say, cool, I have untyped,", "tokens": [50896, 666, 3335, 5134, 11, 437, 291, 483, 307, 291, 483, 472, 1002, 13, 509, 483, 264, 3485, 281, 584, 11, 1627, 11, 286, 362, 517, 874, 3452, 11, 51180], "temperature": 0.0, "avg_logprob": -0.13014382934570312, "compression_ratio": 1.6512455516014235, "no_speech_prob": 0.0028006513603031635}, {"id": 1160, "seek": 596488, "start": 5981.2, "end": 5986.56, "text": " very dynamic, beautiful, simple code. Okay, I care about performance for whatever reason, right?", "tokens": [51180, 588, 8546, 11, 2238, 11, 2199, 3089, 13, 1033, 11, 286, 1127, 466, 3389, 337, 2035, 1778, 11, 558, 30, 51448], "temperature": 0.0, "avg_logprob": -0.13014382934570312, "compression_ratio": 1.6512455516014235, "no_speech_prob": 0.0028006513603031635}, {"id": 1161, "seek": 596488, "start": 5986.56, "end": 5991.76, "text": " There's lots of reasons you could, you might care. And so then you add types, you can parallelize", "tokens": [51448, 821, 311, 3195, 295, 4112, 291, 727, 11, 291, 1062, 1127, 13, 400, 370, 550, 291, 909, 3467, 11, 291, 393, 8952, 1125, 51708], "temperature": 0.0, "avg_logprob": -0.13014382934570312, "compression_ratio": 1.6512455516014235, "no_speech_prob": 0.0028006513603031635}, {"id": 1162, "seek": 599176, "start": 5991.76, "end": 5996.08, "text": " things, you can vectorize things, you can use these techniques, which are general techniques to", "tokens": [50364, 721, 11, 291, 393, 8062, 1125, 721, 11, 291, 393, 764, 613, 7512, 11, 597, 366, 2674, 7512, 281, 50580], "temperature": 0.0, "avg_logprob": -0.13598184024586396, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.006096664350479841}, {"id": 1163, "seek": 599176, "start": 5996.08, "end": 6002.0, "text": " solve a problem. And then you can do that by staying in the system. And if you're, you have", "tokens": [50580, 5039, 257, 1154, 13, 400, 550, 291, 393, 360, 300, 538, 7939, 294, 264, 1185, 13, 400, 498, 291, 434, 11, 291, 362, 50876], "temperature": 0.0, "avg_logprob": -0.13598184024586396, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.006096664350479841}, {"id": 1164, "seek": 599176, "start": 6002.0, "end": 6005.92, "text": " that one Python package is really important to you, you can move it to Mojo, you get massive", "tokens": [50876, 300, 472, 15329, 7372, 307, 534, 1021, 281, 291, 11, 291, 393, 1286, 309, 281, 3335, 5134, 11, 291, 483, 5994, 51072], "temperature": 0.0, "avg_logprob": -0.13598184024586396, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.006096664350479841}, {"id": 1165, "seek": 599176, "start": 6005.92, "end": 6010.88, "text": " performance benefits on that. And other advantages, you know, if you like SAC types, it's nice if", "tokens": [51072, 3389, 5311, 322, 300, 13, 400, 661, 14906, 11, 291, 458, 11, 498, 291, 411, 318, 4378, 3467, 11, 309, 311, 1481, 498, 51320], "temperature": 0.0, "avg_logprob": -0.13598184024586396, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.006096664350479841}, {"id": 1166, "seek": 599176, "start": 6010.88, "end": 6015.6, "text": " they're enforced. Some people like that, right, rather than being hints. So there's other advantages", "tokens": [51320, 436, 434, 40953, 13, 2188, 561, 411, 300, 11, 558, 11, 2831, 813, 885, 27271, 13, 407, 456, 311, 661, 14906, 51556], "temperature": 0.0, "avg_logprob": -0.13598184024586396, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.006096664350479841}, {"id": 1167, "seek": 601560, "start": 6015.6, "end": 6020.240000000001, "text": " too. And then, and then you can do that incrementally as you go.", "tokens": [50364, 886, 13, 400, 550, 11, 293, 550, 291, 393, 360, 300, 26200, 379, 382, 291, 352, 13, 50596], "temperature": 0.0, "avg_logprob": -0.16354657279120552, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.048842549324035645}, {"id": 1168, "seek": 601560, "start": 6022.240000000001, "end": 6030.88, "text": " So one different perspective on this will be why Mojo instead of making C Python faster,", "tokens": [50696, 407, 472, 819, 4585, 322, 341, 486, 312, 983, 3335, 5134, 2602, 295, 1455, 383, 15329, 4663, 11, 51128], "temperature": 0.0, "avg_logprob": -0.16354657279120552, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.048842549324035645}, {"id": 1169, "seek": 601560, "start": 6030.88, "end": 6036.08, "text": " redesigning C Python. Yeah, well, I mean, you can argue Mojo is redesigning C Python,", "tokens": [51128, 16762, 9676, 383, 15329, 13, 865, 11, 731, 11, 286, 914, 11, 291, 393, 9695, 3335, 5134, 307, 16762, 9676, 383, 15329, 11, 51388], "temperature": 0.0, "avg_logprob": -0.16354657279120552, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.048842549324035645}, {"id": 1170, "seek": 601560, "start": 6036.08, "end": 6041.4400000000005, "text": " but, but, but why not make C Python faster and better and other things like that. There's lots", "tokens": [51388, 457, 11, 457, 11, 457, 983, 406, 652, 383, 15329, 4663, 293, 1101, 293, 661, 721, 411, 300, 13, 821, 311, 3195, 51656], "temperature": 0.0, "avg_logprob": -0.16354657279120552, "compression_ratio": 1.6534653465346534, "no_speech_prob": 0.048842549324035645}, {"id": 1171, "seek": 604144, "start": 6041.44, "end": 6046.639999999999, "text": " of people working on that. So actually, there's a team at Microsoft that is really improving,", "tokens": [50364, 295, 561, 1364, 322, 300, 13, 407, 767, 11, 456, 311, 257, 1469, 412, 8116, 300, 307, 534, 11470, 11, 50624], "temperature": 0.0, "avg_logprob": -0.09387451059678022, "compression_ratio": 1.484, "no_speech_prob": 0.021607931703329086}, {"id": 1172, "seek": 604144, "start": 6046.639999999999, "end": 6052.32, "text": " I think C Python 3.11 came out in October or something like that. And it was, you know,", "tokens": [50624, 286, 519, 383, 15329, 805, 13, 5348, 1361, 484, 294, 7617, 420, 746, 411, 300, 13, 400, 309, 390, 11, 291, 458, 11, 50908], "temperature": 0.0, "avg_logprob": -0.09387451059678022, "compression_ratio": 1.484, "no_speech_prob": 0.021607931703329086}, {"id": 1173, "seek": 604144, "start": 6052.32, "end": 6059.679999999999, "text": " 15% faster, 20% faster across the board, which is pretty huge, given how mature Python is and", "tokens": [50908, 2119, 4, 4663, 11, 945, 4, 4663, 2108, 264, 3150, 11, 597, 307, 1238, 2603, 11, 2212, 577, 14442, 15329, 307, 293, 51276], "temperature": 0.0, "avg_logprob": -0.09387451059678022, "compression_ratio": 1.484, "no_speech_prob": 0.021607931703329086}, {"id": 1174, "seek": 604144, "start": 6059.679999999999, "end": 6067.839999999999, "text": " things like this. And so that's awesome. I love it. Doesn't run on GPU. It doesn't do AI stuff,", "tokens": [51276, 721, 411, 341, 13, 400, 370, 300, 311, 3476, 13, 286, 959, 309, 13, 12955, 380, 1190, 322, 18407, 13, 467, 1177, 380, 360, 7318, 1507, 11, 51684], "temperature": 0.0, "avg_logprob": -0.09387451059678022, "compression_ratio": 1.484, "no_speech_prob": 0.021607931703329086}, {"id": 1175, "seek": 606784, "start": 6067.84, "end": 6075.2, "text": " like it doesn't do vectors, doesn't do things. I'm 20% good, 35,000 times is better. Right. So", "tokens": [50364, 411, 309, 1177, 380, 360, 18875, 11, 1177, 380, 360, 721, 13, 286, 478, 945, 4, 665, 11, 6976, 11, 1360, 1413, 307, 1101, 13, 1779, 13, 407, 50732], "temperature": 0.0, "avg_logprob": -0.14578397723211758, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.011330166831612587}, {"id": 1176, "seek": 606784, "start": 6075.2, "end": 6079.6, "text": " like they're, they're, they're, they're definitely, I'm a huge fan of that work, by the way, and it", "tokens": [50732, 411, 436, 434, 11, 436, 434, 11, 436, 434, 11, 436, 434, 2138, 11, 286, 478, 257, 2603, 3429, 295, 300, 589, 11, 538, 264, 636, 11, 293, 309, 50952], "temperature": 0.0, "avg_logprob": -0.14578397723211758, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.011330166831612587}, {"id": 1177, "seek": 606784, "start": 6079.6, "end": 6083.52, "text": " composes well with what we're doing. And so it's not, it's not like we're fighting or anything", "tokens": [50952, 715, 4201, 731, 365, 437, 321, 434, 884, 13, 400, 370, 309, 311, 406, 11, 309, 311, 406, 411, 321, 434, 5237, 420, 1340, 51148], "temperature": 0.0, "avg_logprob": -0.14578397723211758, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.011330166831612587}, {"id": 1178, "seek": 606784, "start": 6083.52, "end": 6087.52, "text": " like that. It's actually just general, it's goodness for the world. But it's just a different path.", "tokens": [51148, 411, 300, 13, 467, 311, 767, 445, 2674, 11, 309, 311, 8387, 337, 264, 1002, 13, 583, 309, 311, 445, 257, 819, 3100, 13, 51348], "temperature": 0.0, "avg_logprob": -0.14578397723211758, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.011330166831612587}, {"id": 1179, "seek": 606784, "start": 6087.52, "end": 6092.32, "text": " Right. And again, we're not working forwards from making Python a little bit better, we're working", "tokens": [51348, 1779, 13, 400, 797, 11, 321, 434, 406, 1364, 30126, 490, 1455, 15329, 257, 707, 857, 1101, 11, 321, 434, 1364, 51588], "temperature": 0.0, "avg_logprob": -0.14578397723211758, "compression_ratio": 1.7366548042704626, "no_speech_prob": 0.011330166831612587}, {"id": 1180, "seek": 609232, "start": 6092.88, "end": 6098.88, "text": " backwards from what is the limit of physics. What's the process of porting Python code to Mojo? Is", "tokens": [50392, 12204, 490, 437, 307, 264, 4948, 295, 10649, 13, 708, 311, 264, 1399, 295, 2436, 278, 15329, 3089, 281, 3335, 5134, 30, 1119, 50692], "temperature": 0.0, "avg_logprob": -0.14085422112391546, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.09133826941251755}, {"id": 1181, "seek": 609232, "start": 6098.88, "end": 6106.88, "text": " there, what's involved in that, in the process? Is there tooling for that? Not yet. So we're missing", "tokens": [50692, 456, 11, 437, 311, 3288, 294, 300, 11, 294, 264, 1399, 30, 1119, 456, 46593, 337, 300, 30, 1726, 1939, 13, 407, 321, 434, 5361, 51092], "temperature": 0.0, "avg_logprob": -0.14085422112391546, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.09133826941251755}, {"id": 1182, "seek": 609232, "start": 6106.88, "end": 6110.719999999999, "text": " some basic features right now. And so we're continuing to drop out new features, like on a", "tokens": [51092, 512, 3875, 4122, 558, 586, 13, 400, 370, 321, 434, 9289, 281, 3270, 484, 777, 4122, 11, 411, 322, 257, 51284], "temperature": 0.0, "avg_logprob": -0.14085422112391546, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.09133826941251755}, {"id": 1183, "seek": 609232, "start": 6110.719999999999, "end": 6117.92, "text": " weekly basis. But, you know, at the fullness of time, give us a year and a half, maybe two years.", "tokens": [51284, 12460, 5143, 13, 583, 11, 291, 458, 11, 412, 264, 45262, 295, 565, 11, 976, 505, 257, 1064, 293, 257, 1922, 11, 1310, 732, 924, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14085422112391546, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.09133826941251755}, {"id": 1184, "seek": 611792, "start": 6117.92, "end": 6123.4400000000005, "text": " Is it an automatable process? So when we're ready, it will be very automatable. Yes.", "tokens": [50364, 1119, 309, 364, 28034, 712, 1399, 30, 407, 562, 321, 434, 1919, 11, 309, 486, 312, 588, 28034, 712, 13, 1079, 13, 50640], "temperature": 0.0, "avg_logprob": -0.16134964403255966, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.07472258806228638}, {"id": 1185, "seek": 611792, "start": 6123.4400000000005, "end": 6126.4, "text": " Is it automatable? Like is it possible to automate", "tokens": [50640, 1119, 309, 28034, 712, 30, 1743, 307, 309, 1944, 281, 31605, 50788], "temperature": 0.0, "avg_logprob": -0.16134964403255966, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.07472258806228638}, {"id": 1186, "seek": 611792, "start": 6127.68, "end": 6132.08, "text": " in the general case, the Python to Mojo conversion? Yeah. Well, you're saying it's possible.", "tokens": [50852, 294, 264, 2674, 1389, 11, 264, 15329, 281, 3335, 5134, 14298, 30, 865, 13, 1042, 11, 291, 434, 1566, 309, 311, 1944, 13, 51072], "temperature": 0.0, "avg_logprob": -0.16134964403255966, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.07472258806228638}, {"id": 1187, "seek": 611792, "start": 6132.08, "end": 6135.76, "text": " Well, so, and this is why, I mean, among other reasons why we use tabs.", "tokens": [51072, 1042, 11, 370, 11, 293, 341, 307, 983, 11, 286, 914, 11, 3654, 661, 4112, 983, 321, 764, 20743, 13, 51256], "temperature": 0.0, "avg_logprob": -0.16134964403255966, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.07472258806228638}, {"id": 1188, "seek": 611792, "start": 6136.56, "end": 6139.76, "text": " Yes. Right. So first of all, by being a superset,", "tokens": [51296, 1079, 13, 1779, 13, 407, 700, 295, 439, 11, 538, 885, 257, 37906, 302, 11, 51456], "temperature": 0.0, "avg_logprob": -0.16134964403255966, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.07472258806228638}, {"id": 1189, "seek": 611792, "start": 6139.76, "end": 6144.32, "text": " yeah, you can, it's like C versus C plus plus. Can you move C code to C plus plus?", "tokens": [51456, 1338, 11, 291, 393, 11, 309, 311, 411, 383, 5717, 383, 1804, 1804, 13, 1664, 291, 1286, 383, 3089, 281, 383, 1804, 1804, 30, 51684], "temperature": 0.0, "avg_logprob": -0.16134964403255966, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.07472258806228638}, {"id": 1190, "seek": 614432, "start": 6144.48, "end": 6149.84, "text": " Yes. Yeah. Right. And you move, you can move C code to C plus plus. And", "tokens": [50372, 1079, 13, 865, 13, 1779, 13, 400, 291, 1286, 11, 291, 393, 1286, 383, 3089, 281, 383, 1804, 1804, 13, 400, 50640], "temperature": 0.0, "avg_logprob": -0.1705596507096491, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.0012064662296324968}, {"id": 1191, "seek": 614432, "start": 6151.28, "end": 6155.5199999999995, "text": " then you can adopt classes, you can add, adopt templates, you can adopt other references or", "tokens": [50712, 550, 291, 393, 6878, 5359, 11, 291, 393, 909, 11, 6878, 21165, 11, 291, 393, 6878, 661, 15400, 420, 50924], "temperature": 0.0, "avg_logprob": -0.1705596507096491, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.0012064662296324968}, {"id": 1192, "seek": 614432, "start": 6155.5199999999995, "end": 6159.92, "text": " whatever C plus plus features you want. After you move C to C code to C plus plus,", "tokens": [50924, 2035, 383, 1804, 1804, 4122, 291, 528, 13, 2381, 291, 1286, 383, 281, 383, 3089, 281, 383, 1804, 1804, 11, 51144], "temperature": 0.0, "avg_logprob": -0.1705596507096491, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.0012064662296324968}, {"id": 1193, "seek": 614432, "start": 6159.92, "end": 6164.799999999999, "text": " like you can't use templates in C. Right. And so if you leave it to C, fine, you can't use the", "tokens": [51144, 411, 291, 393, 380, 764, 21165, 294, 383, 13, 1779, 13, 400, 370, 498, 291, 1856, 309, 281, 383, 11, 2489, 11, 291, 393, 380, 764, 264, 51388], "temperature": 0.0, "avg_logprob": -0.1705596507096491, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.0012064662296324968}, {"id": 1194, "seek": 614432, "start": 6164.799999999999, "end": 6168.799999999999, "text": " cool features, but it still works. Right. And C and C plus plus could work together.", "tokens": [51388, 1627, 4122, 11, 457, 309, 920, 1985, 13, 1779, 13, 400, 383, 293, 383, 1804, 1804, 727, 589, 1214, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1705596507096491, "compression_ratio": 1.9813953488372094, "no_speech_prob": 0.0012064662296324968}, {"id": 1195, "seek": 616880, "start": 6168.88, "end": 6178.08, "text": " And so that's the analogy. Right. Now, here, right, you, you, there's not a Python is bad and", "tokens": [50368, 400, 370, 300, 311, 264, 21663, 13, 1779, 13, 823, 11, 510, 11, 558, 11, 291, 11, 291, 11, 456, 311, 406, 257, 15329, 307, 1578, 293, 50828], "temperature": 0.0, "avg_logprob": -0.1389580427431593, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.01048653107136488}, {"id": 1196, "seek": 616880, "start": 6178.08, "end": 6183.04, "text": " then Mojo is good. Right. Mojo just gives you superpowers. Right. And so if you want to stay", "tokens": [50828, 550, 3335, 5134, 307, 665, 13, 1779, 13, 3335, 5134, 445, 2709, 291, 1687, 47953, 13, 1779, 13, 400, 370, 498, 291, 528, 281, 1754, 51076], "temperature": 0.0, "avg_logprob": -0.1389580427431593, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.01048653107136488}, {"id": 1197, "seek": 616880, "start": 6183.04, "end": 6188.96, "text": " with Python, that's cool. But the tooling should be actually very beautiful and simple", "tokens": [51076, 365, 15329, 11, 300, 311, 1627, 13, 583, 264, 46593, 820, 312, 767, 588, 2238, 293, 2199, 51372], "temperature": 0.0, "avg_logprob": -0.1389580427431593, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.01048653107136488}, {"id": 1198, "seek": 616880, "start": 6188.96, "end": 6194.56, "text": " because we're doing the hard work of defining a superset. Right. So you're right. So there's", "tokens": [51372, 570, 321, 434, 884, 264, 1152, 589, 295, 17827, 257, 37906, 302, 13, 1779, 13, 407, 291, 434, 558, 13, 407, 456, 311, 51652], "temperature": 0.0, "avg_logprob": -0.1389580427431593, "compression_ratio": 1.6561085972850678, "no_speech_prob": 0.01048653107136488}, {"id": 1199, "seek": 619456, "start": 6194.64, "end": 6198.88, "text": " several things to say there, but also the conversion tooling should probably give you hints", "tokens": [50368, 2940, 721, 281, 584, 456, 11, 457, 611, 264, 14298, 46593, 820, 1391, 976, 291, 27271, 50580], "temperature": 0.0, "avg_logprob": -0.08085481222573813, "compression_ratio": 1.805232558139535, "no_speech_prob": 0.025171637535095215}, {"id": 1200, "seek": 619456, "start": 6198.88, "end": 6202.4800000000005, "text": " as to like how you can improve the code. And then exactly once you're in the new world,", "tokens": [50580, 382, 281, 411, 577, 291, 393, 3470, 264, 3089, 13, 400, 550, 2293, 1564, 291, 434, 294, 264, 777, 1002, 11, 50760], "temperature": 0.0, "avg_logprob": -0.08085481222573813, "compression_ratio": 1.805232558139535, "no_speech_prob": 0.025171637535095215}, {"id": 1201, "seek": 619456, "start": 6202.4800000000005, "end": 6206.0, "text": " then you can build all kinds of cool tools to say like, Hey, should you adopt this feature?", "tokens": [50760, 550, 291, 393, 1322, 439, 3685, 295, 1627, 3873, 281, 584, 411, 11, 1911, 11, 820, 291, 6878, 341, 4111, 30, 50936], "temperature": 0.0, "avg_logprob": -0.08085481222573813, "compression_ratio": 1.805232558139535, "no_speech_prob": 0.025171637535095215}, {"id": 1202, "seek": 619456, "start": 6206.0, "end": 6209.84, "text": " Or like, and we haven't built those tools yet, but I fully expect those tools will exist. And", "tokens": [50936, 1610, 411, 11, 293, 321, 2378, 380, 3094, 729, 3873, 1939, 11, 457, 286, 4498, 2066, 729, 3873, 486, 2514, 13, 400, 51128], "temperature": 0.0, "avg_logprob": -0.08085481222573813, "compression_ratio": 1.805232558139535, "no_speech_prob": 0.025171637535095215}, {"id": 1203, "seek": 619456, "start": 6209.84, "end": 6213.76, "text": " then you can like, you know, quote unquote modernize your code or however you want to look at it.", "tokens": [51128, 550, 291, 393, 411, 11, 291, 458, 11, 6513, 37557, 4363, 1125, 428, 3089, 420, 4461, 291, 528, 281, 574, 412, 309, 13, 51324], "temperature": 0.0, "avg_logprob": -0.08085481222573813, "compression_ratio": 1.805232558139535, "no_speech_prob": 0.025171637535095215}, {"id": 1204, "seek": 619456, "start": 6213.76, "end": 6217.68, "text": " Right. So I mean, one of the things that I think is really interesting about Mojo is that", "tokens": [51324, 1779, 13, 407, 286, 914, 11, 472, 295, 264, 721, 300, 286, 519, 307, 534, 1880, 466, 3335, 5134, 307, 300, 51520], "temperature": 0.0, "avg_logprob": -0.08085481222573813, "compression_ratio": 1.805232558139535, "no_speech_prob": 0.025171637535095215}, {"id": 1205, "seek": 619456, "start": 6218.320000000001, "end": 6221.68, "text": " there have been a lot of projects to improve Python over the years.", "tokens": [51552, 456, 362, 668, 257, 688, 295, 4455, 281, 3470, 15329, 670, 264, 924, 13, 51720], "temperature": 0.0, "avg_logprob": -0.08085481222573813, "compression_ratio": 1.805232558139535, "no_speech_prob": 0.025171637535095215}, {"id": 1206, "seek": 622168, "start": 6222.64, "end": 6226.4800000000005, "text": " Everything from, you know, getting Python run on the Java virtual machine,", "tokens": [50412, 5471, 490, 11, 291, 458, 11, 1242, 15329, 1190, 322, 264, 10745, 6374, 3479, 11, 50604], "temperature": 0.0, "avg_logprob": -0.1515606921652089, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.008060512132942677}, {"id": 1207, "seek": 622168, "start": 6227.52, "end": 6231.360000000001, "text": " PyPy, which is a JIT compiler, there's tons of these projects out there that have been working", "tokens": [50656, 9953, 47, 88, 11, 597, 307, 257, 508, 3927, 31958, 11, 456, 311, 9131, 295, 613, 4455, 484, 456, 300, 362, 668, 1364, 50848], "temperature": 0.0, "avg_logprob": -0.1515606921652089, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.008060512132942677}, {"id": 1208, "seek": 622168, "start": 6231.360000000001, "end": 6237.4400000000005, "text": " on improving Python in various ways. They've fallen to one of two camps. So PyPy is a great", "tokens": [50848, 322, 11470, 15329, 294, 3683, 2098, 13, 814, 600, 11547, 281, 472, 295, 732, 16573, 13, 407, 9953, 47, 88, 307, 257, 869, 51152], "temperature": 0.0, "avg_logprob": -0.1515606921652089, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.008060512132942677}, {"id": 1209, "seek": 622168, "start": 6237.4400000000005, "end": 6242.96, "text": " example of a camp that is trying to be compatible with Python. Even there, not really doesn't work", "tokens": [51152, 1365, 295, 257, 2255, 300, 307, 1382, 281, 312, 18218, 365, 15329, 13, 2754, 456, 11, 406, 534, 1177, 380, 589, 51428], "temperature": 0.0, "avg_logprob": -0.1515606921652089, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.008060512132942677}, {"id": 1210, "seek": 622168, "start": 6242.96, "end": 6248.16, "text": " with all the C packages and stuff like that. But but they're trying to be compatible with Python.", "tokens": [51428, 365, 439, 264, 383, 17401, 293, 1507, 411, 300, 13, 583, 457, 436, 434, 1382, 281, 312, 18218, 365, 15329, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1515606921652089, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.008060512132942677}, {"id": 1211, "seek": 624816, "start": 6248.16, "end": 6252.24, "text": " There's also another category of these things where they're saying, well, Python is too complicated.", "tokens": [50364, 821, 311, 611, 1071, 7719, 295, 613, 721, 689, 436, 434, 1566, 11, 731, 11, 15329, 307, 886, 6179, 13, 50568], "temperature": 0.0, "avg_logprob": -0.11849799007177353, "compression_ratio": 1.75, "no_speech_prob": 0.008845938369631767}, {"id": 1212, "seek": 624816, "start": 6253.28, "end": 6259.04, "text": " And, you know, I'm going to cheat on the edges. And, you know, like integers in Python can be", "tokens": [50620, 400, 11, 291, 458, 11, 286, 478, 516, 281, 17470, 322, 264, 8819, 13, 400, 11, 291, 458, 11, 411, 41674, 294, 15329, 393, 312, 50908], "temperature": 0.0, "avg_logprob": -0.11849799007177353, "compression_ratio": 1.75, "no_speech_prob": 0.008845938369631767}, {"id": 1213, "seek": 624816, "start": 6259.04, "end": 6265.2, "text": " an arbitrary size integer. If you care about it fitting in a going fast on a register and a computer,", "tokens": [50908, 364, 23211, 2744, 24922, 13, 759, 291, 1127, 466, 309, 15669, 294, 257, 516, 2370, 322, 257, 7280, 293, 257, 3820, 11, 51216], "temperature": 0.0, "avg_logprob": -0.11849799007177353, "compression_ratio": 1.75, "no_speech_prob": 0.008845938369631767}, {"id": 1214, "seek": 624816, "start": 6265.2, "end": 6270.48, "text": " that's really annoying. Right. And so you can, you can choose to pass on that. Right. You can say,", "tokens": [51216, 300, 311, 534, 11304, 13, 1779, 13, 400, 370, 291, 393, 11, 291, 393, 2826, 281, 1320, 322, 300, 13, 1779, 13, 509, 393, 584, 11, 51480], "temperature": 0.0, "avg_logprob": -0.11849799007177353, "compression_ratio": 1.75, "no_speech_prob": 0.008845938369631767}, {"id": 1215, "seek": 624816, "start": 6270.48, "end": 6274.4, "text": " well, people don't really use big integers that often. Therefore, I'm going to just not do it.", "tokens": [51480, 731, 11, 561, 500, 380, 534, 764, 955, 41674, 300, 2049, 13, 7504, 11, 286, 478, 516, 281, 445, 406, 360, 309, 13, 51676], "temperature": 0.0, "avg_logprob": -0.11849799007177353, "compression_ratio": 1.75, "no_speech_prob": 0.008845938369631767}, {"id": 1216, "seek": 627440, "start": 6274.4, "end": 6280.799999999999, "text": " And it will be fine. Not not a Python superset. Or you can do the hard thing and say, okay,", "tokens": [50364, 400, 309, 486, 312, 2489, 13, 1726, 406, 257, 15329, 37906, 302, 13, 1610, 291, 393, 360, 264, 1152, 551, 293, 584, 11, 1392, 11, 50684], "temperature": 0.0, "avg_logprob": -0.12255691736936569, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.025952262803912163}, {"id": 1217, "seek": 627440, "start": 6280.799999999999, "end": 6287.599999999999, "text": " this is Python. You can't be a superset of Python without being a superset of Python. And that's", "tokens": [50684, 341, 307, 15329, 13, 509, 393, 380, 312, 257, 37906, 302, 295, 15329, 1553, 885, 257, 37906, 302, 295, 15329, 13, 400, 300, 311, 51024], "temperature": 0.0, "avg_logprob": -0.12255691736936569, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.025952262803912163}, {"id": 1218, "seek": 627440, "start": 6287.599999999999, "end": 6293.36, "text": " a really hard technical problem. But it's, in my opinion, worth it. Right. And it's worth it because", "tokens": [51024, 257, 534, 1152, 6191, 1154, 13, 583, 309, 311, 11, 294, 452, 4800, 11, 3163, 309, 13, 1779, 13, 400, 309, 311, 3163, 309, 570, 51312], "temperature": 0.0, "avg_logprob": -0.12255691736936569, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.025952262803912163}, {"id": 1219, "seek": 627440, "start": 6294.08, "end": 6298.96, "text": " it's not about anyone packages about this ecosystem. It's about what Python means for the world. And", "tokens": [51348, 309, 311, 406, 466, 2878, 17401, 466, 341, 11311, 13, 467, 311, 466, 437, 15329, 1355, 337, 264, 1002, 13, 400, 51592], "temperature": 0.0, "avg_logprob": -0.12255691736936569, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.025952262803912163}, {"id": 1220, "seek": 627440, "start": 6298.96, "end": 6303.599999999999, "text": " it also means we don't want to repeat the Python 2 to Python 3 transition. Like we want,", "tokens": [51592, 309, 611, 1355, 321, 500, 380, 528, 281, 7149, 264, 15329, 568, 281, 15329, 805, 6034, 13, 1743, 321, 528, 11, 51824], "temperature": 0.0, "avg_logprob": -0.12255691736936569, "compression_ratio": 1.7610294117647058, "no_speech_prob": 0.025952262803912163}, {"id": 1221, "seek": 630360, "start": 6303.68, "end": 6308.8, "text": " we want people to be able to adopt this stuff quickly. And so by doing that work, we can help", "tokens": [50368, 321, 528, 561, 281, 312, 1075, 281, 6878, 341, 1507, 2661, 13, 400, 370, 538, 884, 300, 589, 11, 321, 393, 854, 50624], "temperature": 0.0, "avg_logprob": -0.12221812284909762, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0006262516253627837}, {"id": 1222, "seek": 630360, "start": 6308.8, "end": 6313.360000000001, "text": " lift people. Yeah, the challenge, it's really interesting technical philosophical challenge of", "tokens": [50624, 5533, 561, 13, 865, 11, 264, 3430, 11, 309, 311, 534, 1880, 6191, 25066, 3430, 295, 50852], "temperature": 0.0, "avg_logprob": -0.12221812284909762, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0006262516253627837}, {"id": 1223, "seek": 630360, "start": 6314.8, "end": 6317.92, "text": " really making a language a superset of another language.", "tokens": [50924, 534, 1455, 257, 2856, 257, 37906, 302, 295, 1071, 2856, 13, 51080], "temperature": 0.0, "avg_logprob": -0.12221812284909762, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0006262516253627837}, {"id": 1224, "seek": 630360, "start": 6319.68, "end": 6324.240000000001, "text": " That's breaking my brain a little bit. Well, it paints you into corners. So again,", "tokens": [51168, 663, 311, 7697, 452, 3567, 257, 707, 857, 13, 1042, 11, 309, 28076, 291, 666, 12413, 13, 407, 797, 11, 51396], "temperature": 0.0, "avg_logprob": -0.12221812284909762, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0006262516253627837}, {"id": 1225, "seek": 630360, "start": 6324.240000000001, "end": 6329.04, "text": " I'm very happy with Python. So joking, all joking aside, I think that the indentation thing is not", "tokens": [51396, 286, 478, 588, 2055, 365, 15329, 13, 407, 17396, 11, 439, 17396, 7359, 11, 286, 519, 300, 264, 44494, 399, 551, 307, 406, 51636], "temperature": 0.0, "avg_logprob": -0.12221812284909762, "compression_ratio": 1.6174242424242424, "no_speech_prob": 0.0006262516253627837}, {"id": 1226, "seek": 632904, "start": 6329.76, "end": 6335.28, "text": " the actual important part of the problem. Right. But the fact that Python has amazing", "tokens": [50400, 264, 3539, 1021, 644, 295, 264, 1154, 13, 1779, 13, 583, 264, 1186, 300, 15329, 575, 2243, 50676], "temperature": 0.0, "avg_logprob": -0.100934571233289, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.04738292843103409}, {"id": 1227, "seek": 632904, "start": 6335.28, "end": 6339.68, "text": " dynamic metaprogramming features, and they translate to beautiful static metaprogramming", "tokens": [50676, 8546, 1131, 569, 340, 1342, 2810, 4122, 11, 293, 436, 13799, 281, 2238, 13437, 1131, 569, 340, 1342, 2810, 50896], "temperature": 0.0, "avg_logprob": -0.100934571233289, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.04738292843103409}, {"id": 1228, "seek": 632904, "start": 6339.68, "end": 6345.28, "text": " features, I think is profound. I think that's huge. Right. And so Python, I've talked with Guido", "tokens": [50896, 4122, 11, 286, 519, 307, 14382, 13, 286, 519, 300, 311, 2603, 13, 1779, 13, 400, 370, 15329, 11, 286, 600, 2825, 365, 2694, 2925, 51176], "temperature": 0.0, "avg_logprob": -0.100934571233289, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.04738292843103409}, {"id": 1229, "seek": 632904, "start": 6345.28, "end": 6350.8, "text": " about this. It's like, it was not designed to do what we're doing. That was not the reason they", "tokens": [51176, 466, 341, 13, 467, 311, 411, 11, 309, 390, 406, 4761, 281, 360, 437, 321, 434, 884, 13, 663, 390, 406, 264, 1778, 436, 51452], "temperature": 0.0, "avg_logprob": -0.100934571233289, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.04738292843103409}, {"id": 1230, "seek": 632904, "start": 6350.8, "end": 6354.4, "text": " built it this way. But because they really cared and they were very thoughtful about how they designed", "tokens": [51452, 3094, 309, 341, 636, 13, 583, 570, 436, 534, 19779, 293, 436, 645, 588, 21566, 466, 577, 436, 4761, 51632], "temperature": 0.0, "avg_logprob": -0.100934571233289, "compression_ratio": 1.7472118959107807, "no_speech_prob": 0.04738292843103409}, {"id": 1231, "seek": 635440, "start": 6354.4, "end": 6359.36, "text": " the language, it scales very elegantly in the space. But if you look at other languages,", "tokens": [50364, 264, 2856, 11, 309, 17408, 588, 14459, 3627, 294, 264, 1901, 13, 583, 498, 291, 574, 412, 661, 8650, 11, 50612], "temperature": 0.0, "avg_logprob": -0.0707886679130688, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.022280747070908546}, {"id": 1232, "seek": 635440, "start": 6359.36, "end": 6365.92, "text": " for example, C and C++, right, if you're building a superset, you get stuck with the", "tokens": [50612, 337, 1365, 11, 383, 293, 383, 25472, 11, 558, 11, 498, 291, 434, 2390, 257, 37906, 302, 11, 291, 483, 5541, 365, 264, 50940], "temperature": 0.0, "avg_logprob": -0.0707886679130688, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.022280747070908546}, {"id": 1233, "seek": 635440, "start": 6365.92, "end": 6373.759999999999, "text": " design decisions of the subset. Right. And so, you know, C++ is way more complicated because", "tokens": [50940, 1715, 5327, 295, 264, 25993, 13, 1779, 13, 400, 370, 11, 291, 458, 11, 383, 25472, 307, 636, 544, 6179, 570, 51332], "temperature": 0.0, "avg_logprob": -0.0707886679130688, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.022280747070908546}, {"id": 1234, "seek": 635440, "start": 6373.759999999999, "end": 6378.4, "text": " of C in the legacy than it would have been if they would have theoretically designed a from", "tokens": [51332, 295, 383, 294, 264, 11711, 813, 309, 576, 362, 668, 498, 436, 576, 362, 29400, 4761, 257, 490, 51564], "temperature": 0.0, "avg_logprob": -0.0707886679130688, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.022280747070908546}, {"id": 1235, "seek": 635440, "start": 6378.4, "end": 6384.16, "text": " scratch thing. And there's lots of people right now that are trying to make C++ better and", "tokens": [51564, 8459, 551, 13, 400, 456, 311, 3195, 295, 561, 558, 586, 300, 366, 1382, 281, 652, 383, 25472, 1101, 293, 51852], "temperature": 0.0, "avg_logprob": -0.0707886679130688, "compression_ratio": 1.6446886446886446, "no_speech_prob": 0.022280747070908546}, {"id": 1236, "seek": 638416, "start": 6384.16, "end": 6388.5599999999995, "text": " re-syntax C++. It's going to be great. We'll just change all the syntax. But if you do that,", "tokens": [50364, 319, 12, 3187, 580, 2797, 383, 25472, 13, 467, 311, 516, 281, 312, 869, 13, 492, 603, 445, 1319, 439, 264, 28431, 13, 583, 498, 291, 360, 300, 11, 50584], "temperature": 0.0, "avg_logprob": -0.13296991879822778, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.0010985300177708268}, {"id": 1237, "seek": 638416, "start": 6388.5599999999995, "end": 6394.08, "text": " now suddenly you have zero packages. You don't have compatibility. So what are the, if you could", "tokens": [50584, 586, 5800, 291, 362, 4018, 17401, 13, 509, 500, 380, 362, 34237, 13, 407, 437, 366, 264, 11, 498, 291, 727, 50860], "temperature": 0.0, "avg_logprob": -0.13296991879822778, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.0010985300177708268}, {"id": 1238, "seek": 638416, "start": 6394.08, "end": 6399.92, "text": " just linger on that, what are the biggest challenges of keeping that superset status?", "tokens": [50860, 445, 45657, 322, 300, 11, 437, 366, 264, 3880, 4759, 295, 5145, 300, 37906, 302, 6558, 30, 51152], "temperature": 0.0, "avg_logprob": -0.13296991879822778, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.0010985300177708268}, {"id": 1239, "seek": 638416, "start": 6401.12, "end": 6404.72, "text": " What are the things just struggling with? Is it all boiled down to having a big integer?", "tokens": [51212, 708, 366, 264, 721, 445, 9314, 365, 30, 1119, 309, 439, 21058, 760, 281, 1419, 257, 955, 24922, 30, 51392], "temperature": 0.0, "avg_logprob": -0.13296991879822778, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.0010985300177708268}, {"id": 1240, "seek": 638416, "start": 6405.599999999999, "end": 6410.48, "text": " No, I mean, what are the other things like? Usually it's the, it's a long tail of weird", "tokens": [51436, 883, 11, 286, 914, 11, 437, 366, 264, 661, 721, 411, 30, 11419, 309, 311, 264, 11, 309, 311, 257, 938, 6838, 295, 3657, 51680], "temperature": 0.0, "avg_logprob": -0.13296991879822778, "compression_ratio": 1.6436363636363636, "no_speech_prob": 0.0010985300177708268}, {"id": 1241, "seek": 641048, "start": 6410.48, "end": 6417.12, "text": " things. So let me, let me give you a war story. So war story in the space is you go way back in", "tokens": [50364, 721, 13, 407, 718, 385, 11, 718, 385, 976, 291, 257, 1516, 1657, 13, 407, 1516, 1657, 294, 264, 1901, 307, 291, 352, 636, 646, 294, 50696], "temperature": 0.0, "avg_logprob": -0.19545867226340555, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.022965488955378532}, {"id": 1242, "seek": 641048, "start": 6417.12, "end": 6424.959999999999, "text": " time. Project I worked on is called Clang. Clang, what it is, is a C C++ parser, right. And when", "tokens": [50696, 565, 13, 9849, 286, 2732, 322, 307, 1219, 2033, 656, 13, 2033, 656, 11, 437, 309, 307, 11, 307, 257, 383, 383, 25472, 21156, 260, 11, 558, 13, 400, 562, 51088], "temperature": 0.0, "avg_logprob": -0.19545867226340555, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.022965488955378532}, {"id": 1243, "seek": 641048, "start": 6424.959999999999, "end": 6431.2, "text": " I started working on Clang, it must have been like 2006 or something was when I, 2007, 2006,", "tokens": [51088, 286, 1409, 1364, 322, 2033, 656, 11, 309, 1633, 362, 668, 411, 14062, 420, 746, 390, 562, 286, 11, 12656, 11, 14062, 11, 51400], "temperature": 0.0, "avg_logprob": -0.19545867226340555, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.022965488955378532}, {"id": 1244, "seek": 641048, "start": 6431.2, "end": 6438.5599999999995, "text": " when I first started working on it, right? It's funny how time flies. I started that project", "tokens": [51400, 562, 286, 700, 1409, 1364, 322, 309, 11, 558, 30, 467, 311, 4074, 577, 565, 17414, 13, 286, 1409, 300, 1716, 51768], "temperature": 0.0, "avg_logprob": -0.19545867226340555, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.022965488955378532}, {"id": 1245, "seek": 643856, "start": 6438.56, "end": 6445.4400000000005, "text": " and I'm like, okay, well, I want to build a C parser, C++ parser for LLVM. It's going to be", "tokens": [50364, 293, 286, 478, 411, 11, 1392, 11, 731, 11, 286, 528, 281, 1322, 257, 383, 21156, 260, 11, 383, 25472, 21156, 260, 337, 441, 43, 53, 44, 13, 467, 311, 516, 281, 312, 50708], "temperature": 0.0, "avg_logprob": -0.11740786592725297, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.0011693885317072272}, {"id": 1246, "seek": 643856, "start": 6446.080000000001, "end": 6452.88, "text": " the word GCC is yucky. You know, this is me in earlier times. It's yucky. It's unprincipled.", "tokens": [50740, 264, 1349, 460, 11717, 307, 288, 10616, 13, 509, 458, 11, 341, 307, 385, 294, 3071, 1413, 13, 467, 311, 288, 10616, 13, 467, 311, 517, 1424, 21961, 15551, 13, 51080], "temperature": 0.0, "avg_logprob": -0.11740786592725297, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.0011693885317072272}, {"id": 1247, "seek": 643856, "start": 6452.88, "end": 6458.320000000001, "text": " It has all these weird features, like all these bugs, like it's yucky. So I'm going to build", "tokens": [51080, 467, 575, 439, 613, 3657, 4122, 11, 411, 439, 613, 15120, 11, 411, 309, 311, 288, 10616, 13, 407, 286, 478, 516, 281, 1322, 51352], "temperature": 0.0, "avg_logprob": -0.11740786592725297, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.0011693885317072272}, {"id": 1248, "seek": 643856, "start": 6458.320000000001, "end": 6463.92, "text": " a standard compliant C and C++ parser. It's going to be beautiful. It'll be amazing. Well,", "tokens": [51352, 257, 3832, 36248, 383, 293, 383, 25472, 21156, 260, 13, 467, 311, 516, 281, 312, 2238, 13, 467, 603, 312, 2243, 13, 1042, 11, 51632], "temperature": 0.0, "avg_logprob": -0.11740786592725297, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.0011693885317072272}, {"id": 1249, "seek": 643856, "start": 6463.92, "end": 6468.0, "text": " engineered, all the cool things an engineer wants to do. And so I start implementing building it", "tokens": [51632, 38648, 11, 439, 264, 1627, 721, 364, 11403, 2738, 281, 360, 13, 400, 370, 286, 722, 18114, 2390, 309, 51836], "temperature": 0.0, "avg_logprob": -0.11740786592725297, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.0011693885317072272}, {"id": 1250, "seek": 646800, "start": 6468.32, "end": 6472.16, "text": " out, building it out, building it out. And then I got to include standard IO.h.", "tokens": [50380, 484, 11, 2390, 309, 484, 11, 2390, 309, 484, 13, 400, 550, 286, 658, 281, 4090, 3832, 39839, 13, 71, 13, 50572], "temperature": 0.0, "avg_logprob": -0.17157351403009324, "compression_ratio": 1.526829268292683, "no_speech_prob": 0.0020504712592810392}, {"id": 1251, "seek": 646800, "start": 6474.0, "end": 6477.12, "text": " And all of the headers in the world use all the GCC stuff.", "tokens": [50664, 400, 439, 295, 264, 45101, 294, 264, 1002, 764, 439, 264, 460, 11717, 1507, 13, 50820], "temperature": 0.0, "avg_logprob": -0.17157351403009324, "compression_ratio": 1.526829268292683, "no_speech_prob": 0.0020504712592810392}, {"id": 1252, "seek": 646800, "start": 6479.92, "end": 6488.08, "text": " So again, come back away from theory back to reality, right? I was at a fork on the road.", "tokens": [50960, 407, 797, 11, 808, 646, 1314, 490, 5261, 646, 281, 4103, 11, 558, 30, 286, 390, 412, 257, 17716, 322, 264, 3060, 13, 51368], "temperature": 0.0, "avg_logprob": -0.17157351403009324, "compression_ratio": 1.526829268292683, "no_speech_prob": 0.0020504712592810392}, {"id": 1253, "seek": 646800, "start": 6488.08, "end": 6492.08, "text": " I could have built an amazingly beautiful academic thing that nobody would ever use.", "tokens": [51368, 286, 727, 362, 3094, 364, 31762, 2238, 7778, 551, 300, 5079, 576, 1562, 764, 13, 51568], "temperature": 0.0, "avg_logprob": -0.17157351403009324, "compression_ratio": 1.526829268292683, "no_speech_prob": 0.0020504712592810392}, {"id": 1254, "seek": 649208, "start": 6492.4, "end": 6500.8, "text": " Or I could say, well, it's yucky in various ways. All these design mistakes, accents of history,", "tokens": [50380, 1610, 286, 727, 584, 11, 731, 11, 309, 311, 288, 10616, 294, 3683, 2098, 13, 1057, 613, 1715, 8038, 11, 35012, 295, 2503, 11, 50800], "temperature": 0.0, "avg_logprob": -0.1556224994831257, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0351380929350853}, {"id": 1255, "seek": 649208, "start": 6500.8, "end": 6506.88, "text": " the legacy at that point, GCC was like over 20 years old, which, by the way, now LLVM is over", "tokens": [50800, 264, 11711, 412, 300, 935, 11, 460, 11717, 390, 411, 670, 945, 924, 1331, 11, 597, 11, 538, 264, 636, 11, 586, 441, 43, 53, 44, 307, 670, 51104], "temperature": 0.0, "avg_logprob": -0.1556224994831257, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0351380929350853}, {"id": 1256, "seek": 649208, "start": 6506.88, "end": 6513.04, "text": " 20 years old. So it's funny how time catches up to you, right? And so you say, okay, well,", "tokens": [51104, 945, 924, 1331, 13, 407, 309, 311, 4074, 577, 565, 25496, 493, 281, 291, 11, 558, 30, 400, 370, 291, 584, 11, 1392, 11, 731, 11, 51412], "temperature": 0.0, "avg_logprob": -0.1556224994831257, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0351380929350853}, {"id": 1257, "seek": 649208, "start": 6514.32, "end": 6519.2, "text": " what is easier, right? I mean, as an engineer, it's actually much easier for me to go implement", "tokens": [51476, 437, 307, 3571, 11, 558, 30, 286, 914, 11, 382, 364, 11403, 11, 309, 311, 767, 709, 3571, 337, 385, 281, 352, 4445, 51720], "temperature": 0.0, "avg_logprob": -0.1556224994831257, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0351380929350853}, {"id": 1258, "seek": 651920, "start": 6519.84, "end": 6524.639999999999, "text": " long tail compatibility weird features, even if they're distasteful, and just do the hard work", "tokens": [50396, 938, 6838, 34237, 3657, 4122, 11, 754, 498, 436, 434, 1483, 9079, 906, 11, 293, 445, 360, 264, 1152, 589, 50636], "temperature": 0.0, "avg_logprob": -0.1106084092219073, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.004904641304165125}, {"id": 1259, "seek": 651920, "start": 6524.639999999999, "end": 6528.639999999999, "text": " and like figure it out, reverse engineer, understand what it is, write a bunch of test", "tokens": [50636, 293, 411, 2573, 309, 484, 11, 9943, 11403, 11, 1223, 437, 309, 307, 11, 2464, 257, 3840, 295, 1500, 50836], "temperature": 0.0, "avg_logprob": -0.1106084092219073, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.004904641304165125}, {"id": 1260, "seek": 651920, "start": 6528.639999999999, "end": 6533.679999999999, "text": " cases, like try to understand behavior. It's way easier to do all that work as an engineer", "tokens": [50836, 3331, 11, 411, 853, 281, 1223, 5223, 13, 467, 311, 636, 3571, 281, 360, 439, 300, 589, 382, 364, 11403, 51088], "temperature": 0.0, "avg_logprob": -0.1106084092219073, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.004904641304165125}, {"id": 1261, "seek": 651920, "start": 6533.679999999999, "end": 6537.36, "text": " than it is to go talk to all C programmers and get, argue with them and try to get them to", "tokens": [51088, 813, 309, 307, 281, 352, 751, 281, 439, 383, 41504, 293, 483, 11, 9695, 365, 552, 293, 853, 281, 483, 552, 281, 51272], "temperature": 0.0, "avg_logprob": -0.1106084092219073, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.004904641304165125}, {"id": 1262, "seek": 651920, "start": 6537.36, "end": 6544.32, "text": " rewrite their code. Yeah. Right. And because that breaks a lot more things. Yeah. And you have", "tokens": [51272, 28132, 641, 3089, 13, 865, 13, 1779, 13, 400, 570, 300, 9857, 257, 688, 544, 721, 13, 865, 13, 400, 291, 362, 51620], "temperature": 0.0, "avg_logprob": -0.1106084092219073, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.004904641304165125}, {"id": 1263, "seek": 651920, "start": 6544.32, "end": 6548.48, "text": " realities like nobody actually even understands how the code works, because it was written by", "tokens": [51620, 27785, 411, 5079, 767, 754, 15146, 577, 264, 3089, 1985, 11, 570, 309, 390, 3720, 538, 51828], "temperature": 0.0, "avg_logprob": -0.1106084092219073, "compression_ratio": 1.7358490566037736, "no_speech_prob": 0.004904641304165125}, {"id": 1264, "seek": 654848, "start": 6548.48, "end": 6556.32, "text": " the person who quit 10 years ago, right? And so this software is kind of frustrating that way,", "tokens": [50364, 264, 954, 567, 10366, 1266, 924, 2057, 11, 558, 30, 400, 370, 341, 4722, 307, 733, 295, 16522, 300, 636, 11, 50756], "temperature": 0.0, "avg_logprob": -0.12280622281526264, "compression_ratio": 1.6161971830985915, "no_speech_prob": 0.001809962559491396}, {"id": 1265, "seek": 654848, "start": 6556.32, "end": 6560.48, "text": " but it's, that's how the world works. Yeah. Unfortunately, it can never be this", "tokens": [50756, 457, 309, 311, 11, 300, 311, 577, 264, 1002, 1985, 13, 865, 13, 8590, 11, 309, 393, 1128, 312, 341, 50964], "temperature": 0.0, "avg_logprob": -0.12280622281526264, "compression_ratio": 1.6161971830985915, "no_speech_prob": 0.001809962559491396}, {"id": 1266, "seek": 654848, "start": 6561.36, "end": 6566.08, "text": " perfect, beautiful thing. Well, there are, there are occasions in which you get to build,", "tokens": [51008, 2176, 11, 2238, 551, 13, 1042, 11, 456, 366, 11, 456, 366, 20641, 294, 597, 291, 483, 281, 1322, 11, 51244], "temperature": 0.0, "avg_logprob": -0.12280622281526264, "compression_ratio": 1.6161971830985915, "no_speech_prob": 0.001809962559491396}, {"id": 1267, "seek": 654848, "start": 6566.08, "end": 6570.24, "text": " like, you know, you invent a new data structure or something like that, or there's this beautiful", "tokens": [51244, 411, 11, 291, 458, 11, 291, 7962, 257, 777, 1412, 3877, 420, 746, 411, 300, 11, 420, 456, 311, 341, 2238, 51452], "temperature": 0.0, "avg_logprob": -0.12280622281526264, "compression_ratio": 1.6161971830985915, "no_speech_prob": 0.001809962559491396}, {"id": 1268, "seek": 654848, "start": 6570.24, "end": 6575.04, "text": " algorithm that just like makes you super happy. I love that moment. But when you're working with", "tokens": [51452, 9284, 300, 445, 411, 1669, 291, 1687, 2055, 13, 286, 959, 300, 1623, 13, 583, 562, 291, 434, 1364, 365, 51692], "temperature": 0.0, "avg_logprob": -0.12280622281526264, "compression_ratio": 1.6161971830985915, "no_speech_prob": 0.001809962559491396}, {"id": 1269, "seek": 657504, "start": 6575.04, "end": 6579.5199999999995, "text": " people, you're working with code and dusty that code bases and things like this, right?", "tokens": [50364, 561, 11, 291, 434, 1364, 365, 3089, 293, 41973, 300, 3089, 17949, 293, 721, 411, 341, 11, 558, 30, 50588], "temperature": 0.0, "avg_logprob": -0.1307748077262161, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.004197905771434307}, {"id": 1270, "seek": 657504, "start": 6580.56, "end": 6584.48, "text": " It's not about what's theoretically beautiful, it's about what's practical, what's real, what", "tokens": [50640, 467, 311, 406, 466, 437, 311, 29400, 2238, 11, 309, 311, 466, 437, 311, 8496, 11, 437, 311, 957, 11, 437, 50836], "temperature": 0.0, "avg_logprob": -0.1307748077262161, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.004197905771434307}, {"id": 1271, "seek": 657504, "start": 6584.48, "end": 6589.5199999999995, "text": " people will actually use. And I don't meet a lot of people that say, I want to rewrite all my code", "tokens": [50836, 561, 486, 767, 764, 13, 400, 286, 500, 380, 1677, 257, 688, 295, 561, 300, 584, 11, 286, 528, 281, 28132, 439, 452, 3089, 51088], "temperature": 0.0, "avg_logprob": -0.1307748077262161, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.004197905771434307}, {"id": 1272, "seek": 657504, "start": 6590.56, "end": 6594.72, "text": " just for the sake of it. By the way, there could be interesting possibilities and we'll probably", "tokens": [51140, 445, 337, 264, 9717, 295, 309, 13, 3146, 264, 636, 11, 456, 727, 312, 1880, 12178, 293, 321, 603, 1391, 51348], "temperature": 0.0, "avg_logprob": -0.1307748077262161, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.004197905771434307}, {"id": 1273, "seek": 657504, "start": 6594.72, "end": 6600.32, "text": " talk about it where AI can help rewrite some code that might be farther out future, but it's a", "tokens": [51348, 751, 466, 309, 689, 7318, 393, 854, 28132, 512, 3089, 300, 1062, 312, 20344, 484, 2027, 11, 457, 309, 311, 257, 51628], "temperature": 0.0, "avg_logprob": -0.1307748077262161, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.004197905771434307}, {"id": 1274, "seek": 660032, "start": 6600.32, "end": 6608.0, "text": " really interesting one, how that could create more, be a tool in the battle against this monster", "tokens": [50364, 534, 1880, 472, 11, 577, 300, 727, 1884, 544, 11, 312, 257, 2290, 294, 264, 4635, 1970, 341, 10090, 50748], "temperature": 0.0, "avg_logprob": -0.10165974010120739, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.015178464353084564}, {"id": 1275, "seek": 660032, "start": 6608.0, "end": 6617.28, "text": " of complexity that you mentioned. You mentioned Guido, the benevolent dictator for life of Python.", "tokens": [50748, 295, 14024, 300, 291, 2835, 13, 509, 2835, 2694, 2925, 11, 264, 48567, 317, 42852, 337, 993, 295, 15329, 13, 51212], "temperature": 0.0, "avg_logprob": -0.10165974010120739, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.015178464353084564}, {"id": 1276, "seek": 660032, "start": 6617.28, "end": 6620.32, "text": " What does he think about Mojo? Have you talked to him much about it?", "tokens": [51212, 708, 775, 415, 519, 466, 3335, 5134, 30, 3560, 291, 2825, 281, 796, 709, 466, 309, 30, 51364], "temperature": 0.0, "avg_logprob": -0.10165974010120739, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.015178464353084564}, {"id": 1277, "seek": 660032, "start": 6621.12, "end": 6625.2, "text": " I have talked with him about it. He found it very interesting. We actually talked with Guido", "tokens": [51404, 286, 362, 2825, 365, 796, 466, 309, 13, 634, 1352, 309, 588, 1880, 13, 492, 767, 2825, 365, 2694, 2925, 51608], "temperature": 0.0, "avg_logprob": -0.10165974010120739, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.015178464353084564}, {"id": 1278, "seek": 660032, "start": 6625.2, "end": 6629.44, "text": " before it launched. And so he was aware of it before it went public. I have a ton of respect", "tokens": [51608, 949, 309, 8730, 13, 400, 370, 415, 390, 3650, 295, 309, 949, 309, 1437, 1908, 13, 286, 362, 257, 2952, 295, 3104, 51820], "temperature": 0.0, "avg_logprob": -0.10165974010120739, "compression_ratio": 1.6917293233082706, "no_speech_prob": 0.015178464353084564}, {"id": 1279, "seek": 662944, "start": 6629.44, "end": 6635.599999999999, "text": " for Guido for a bunch of different reasons. You talk about Waller's operator and Guido is pretty", "tokens": [50364, 337, 2694, 2925, 337, 257, 3840, 295, 819, 4112, 13, 509, 751, 466, 9551, 260, 311, 12973, 293, 2694, 2925, 307, 1238, 50672], "temperature": 0.0, "avg_logprob": -0.154197274422159, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.008568628691136837}, {"id": 1280, "seek": 662944, "start": 6635.599999999999, "end": 6644.879999999999, "text": " amazing in terms of steering such a huge and diverse community and driving it forward. And", "tokens": [50672, 2243, 294, 2115, 295, 14823, 1270, 257, 2603, 293, 9521, 1768, 293, 4840, 309, 2128, 13, 400, 51136], "temperature": 0.0, "avg_logprob": -0.154197274422159, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.008568628691136837}, {"id": 1281, "seek": 662944, "start": 6644.879999999999, "end": 6651.36, "text": " I think Python is what it is thanks to him. And so to me, it was really important starting to work", "tokens": [51136, 286, 519, 15329, 307, 437, 309, 307, 3231, 281, 796, 13, 400, 370, 281, 385, 11, 309, 390, 534, 1021, 2891, 281, 589, 51460], "temperature": 0.0, "avg_logprob": -0.154197274422159, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.008568628691136837}, {"id": 1282, "seek": 662944, "start": 6651.36, "end": 6658.08, "text": " on Mojo to get his feedback and get his input and get his eyes on this. Now, a lot of what Guido", "tokens": [51460, 322, 3335, 5134, 281, 483, 702, 5824, 293, 483, 702, 4846, 293, 483, 702, 2575, 322, 341, 13, 823, 11, 257, 688, 295, 437, 2694, 2925, 51796], "temperature": 0.0, "avg_logprob": -0.154197274422159, "compression_ratio": 1.5761316872427984, "no_speech_prob": 0.008568628691136837}, {"id": 1283, "seek": 665808, "start": 6658.96, "end": 6664.32, "text": " was and is, I think, and thought about is, have we not fragment the community? We don't want to", "tokens": [50408, 390, 293, 307, 11, 286, 519, 11, 293, 1194, 466, 307, 11, 362, 321, 406, 26424, 264, 1768, 30, 492, 500, 380, 528, 281, 50676], "temperature": 0.0, "avg_logprob": -0.13050406554649616, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003171369666233659}, {"id": 1284, "seek": 665808, "start": 6664.32, "end": 6669.76, "text": " Python 2 to Python 3 thing. That was really painful for everybody involved. And so we spent", "tokens": [50676, 15329, 568, 281, 15329, 805, 551, 13, 663, 390, 534, 11697, 337, 2201, 3288, 13, 400, 370, 321, 4418, 50948], "temperature": 0.0, "avg_logprob": -0.13050406554649616, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003171369666233659}, {"id": 1285, "seek": 665808, "start": 6669.76, "end": 6673.44, "text": " quite a bit of time talking about that and some of the tricks I learned from Swift, for example.", "tokens": [50948, 1596, 257, 857, 295, 565, 1417, 466, 300, 293, 512, 295, 264, 11733, 286, 3264, 490, 25539, 11, 337, 1365, 13, 51132], "temperature": 0.0, "avg_logprob": -0.13050406554649616, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003171369666233659}, {"id": 1286, "seek": 665808, "start": 6673.44, "end": 6680.4, "text": " So in the migration from Swift, we managed to not just convert Objective-C into a slightly", "tokens": [51132, 407, 294, 264, 17011, 490, 25539, 11, 321, 6453, 281, 406, 445, 7620, 24753, 488, 12, 34, 666, 257, 4748, 51480], "temperature": 0.0, "avg_logprob": -0.13050406554649616, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003171369666233659}, {"id": 1287, "seek": 665808, "start": 6680.4, "end": 6686.4, "text": " prettier Objective-C, which we did. We then converted not entirely, but almost an entire", "tokens": [51480, 36825, 24753, 488, 12, 34, 11, 597, 321, 630, 13, 492, 550, 16424, 406, 7696, 11, 457, 1920, 364, 2302, 51780], "temperature": 0.0, "avg_logprob": -0.13050406554649616, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003171369666233659}, {"id": 1288, "seek": 668640, "start": 6686.48, "end": 6691.92, "text": " community to a completely different language. And so there's a bunch of tricks that you learn", "tokens": [50368, 1768, 281, 257, 2584, 819, 2856, 13, 400, 370, 456, 311, 257, 3840, 295, 11733, 300, 291, 1466, 50640], "temperature": 0.0, "avg_logprob": -0.11351127624511718, "compression_ratio": 1.6298932384341638, "no_speech_prob": 0.00884312391281128}, {"id": 1289, "seek": 668640, "start": 6691.92, "end": 6696.48, "text": " along the way that are directly relevant to what we do. And so this is where, for example,", "tokens": [50640, 2051, 264, 636, 300, 366, 3838, 7340, 281, 437, 321, 360, 13, 400, 370, 341, 307, 689, 11, 337, 1365, 11, 50868], "temperature": 0.0, "avg_logprob": -0.11351127624511718, "compression_ratio": 1.6298932384341638, "no_speech_prob": 0.00884312391281128}, {"id": 1290, "seek": 668640, "start": 6697.599999999999, "end": 6703.839999999999, "text": " you leverage C Python while bringing up the new thing. That approach is, I think, proven and", "tokens": [50924, 291, 13982, 383, 15329, 1339, 5062, 493, 264, 777, 551, 13, 663, 3109, 307, 11, 286, 519, 11, 12785, 293, 51236], "temperature": 0.0, "avg_logprob": -0.11351127624511718, "compression_ratio": 1.6298932384341638, "no_speech_prob": 0.00884312391281128}, {"id": 1291, "seek": 668640, "start": 6703.839999999999, "end": 6708.96, "text": " comes from experience. And so Guido is very interested in, like, okay, cool. I think that", "tokens": [51236, 1487, 490, 1752, 13, 400, 370, 2694, 2925, 307, 588, 3102, 294, 11, 411, 11, 1392, 11, 1627, 13, 286, 519, 300, 51492], "temperature": 0.0, "avg_logprob": -0.11351127624511718, "compression_ratio": 1.6298932384341638, "no_speech_prob": 0.00884312391281128}, {"id": 1292, "seek": 668640, "start": 6708.96, "end": 6713.44, "text": " Python is really his legacy. It's his baby. I have tons of respect for that. Incidentally,", "tokens": [51492, 15329, 307, 534, 702, 11711, 13, 467, 311, 702, 3186, 13, 286, 362, 9131, 295, 3104, 337, 300, 13, 7779, 36578, 11, 51716], "temperature": 0.0, "avg_logprob": -0.11351127624511718, "compression_ratio": 1.6298932384341638, "no_speech_prob": 0.00884312391281128}, {"id": 1293, "seek": 671344, "start": 6713.5199999999995, "end": 6717.599999999999, "text": " I see Mojo as a member of the Python family. We're not trying to take Python away from Guido", "tokens": [50368, 286, 536, 3335, 5134, 382, 257, 4006, 295, 264, 15329, 1605, 13, 492, 434, 406, 1382, 281, 747, 15329, 1314, 490, 2694, 2925, 50572], "temperature": 0.0, "avg_logprob": -0.10710990905761719, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.05030626803636551}, {"id": 1294, "seek": 671344, "start": 6717.599999999999, "end": 6724.5599999999995, "text": " and from the Python community. And so to me, it's really important that we're a good member", "tokens": [50572, 293, 490, 264, 15329, 1768, 13, 400, 370, 281, 385, 11, 309, 311, 534, 1021, 300, 321, 434, 257, 665, 4006, 50920], "temperature": 0.0, "avg_logprob": -0.10710990905761719, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.05030626803636551}, {"id": 1295, "seek": 671344, "start": 6724.5599999999995, "end": 6729.12, "text": " of that community. And so I think that, again, you would have to ask Guido this, but I think", "tokens": [50920, 295, 300, 1768, 13, 400, 370, 286, 519, 300, 11, 797, 11, 291, 576, 362, 281, 1029, 2694, 2925, 341, 11, 457, 286, 519, 51148], "temperature": 0.0, "avg_logprob": -0.10710990905761719, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.05030626803636551}, {"id": 1296, "seek": 671344, "start": 6729.12, "end": 6734.0, "text": " that he was very interested in this notion of, like, cool, Python gets beaten up for being slow.", "tokens": [51148, 300, 415, 390, 588, 3102, 294, 341, 10710, 295, 11, 411, 11, 1627, 11, 15329, 2170, 17909, 493, 337, 885, 2964, 13, 51392], "temperature": 0.0, "avg_logprob": -0.10710990905761719, "compression_ratio": 1.6696428571428572, "no_speech_prob": 0.05030626803636551}, {"id": 1297, "seek": 673400, "start": 6734.56, "end": 6742.8, "text": " So maybe there's a path out of that, right? And that, you know, if the future is Python,", "tokens": [50392, 407, 1310, 456, 311, 257, 3100, 484, 295, 300, 11, 558, 30, 400, 300, 11, 291, 458, 11, 498, 264, 2027, 307, 15329, 11, 50804], "temperature": 0.0, "avg_logprob": -0.13320876084841216, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08754245191812515}, {"id": 1298, "seek": 673400, "start": 6742.8, "end": 6749.92, "text": " right? I mean, look at the far outside case on this, right? And I'm not saying this is Guido's", "tokens": [50804, 558, 30, 286, 914, 11, 574, 412, 264, 1400, 2380, 1389, 322, 341, 11, 558, 30, 400, 286, 478, 406, 1566, 341, 307, 2694, 2925, 311, 51160], "temperature": 0.0, "avg_logprob": -0.13320876084841216, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08754245191812515}, {"id": 1299, "seek": 673400, "start": 6749.92, "end": 6754.56, "text": " perspective, but, you know, there's this path of saying, like, okay, well, suddenly Python can", "tokens": [51160, 4585, 11, 457, 11, 291, 458, 11, 456, 311, 341, 3100, 295, 1566, 11, 411, 11, 1392, 11, 731, 11, 5800, 15329, 393, 51392], "temperature": 0.0, "avg_logprob": -0.13320876084841216, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08754245191812515}, {"id": 1300, "seek": 673400, "start": 6754.56, "end": 6759.36, "text": " suddenly go all the places it's never been able to go before. Right. And that means that Python", "tokens": [51392, 5800, 352, 439, 264, 3190, 309, 311, 1128, 668, 1075, 281, 352, 949, 13, 1779, 13, 400, 300, 1355, 300, 15329, 51632], "temperature": 0.0, "avg_logprob": -0.13320876084841216, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.08754245191812515}, {"id": 1301, "seek": 675936, "start": 6759.36, "end": 6763.36, "text": " can go even further and can have even more impact on the world. So in some sense,", "tokens": [50364, 393, 352, 754, 3052, 293, 393, 362, 754, 544, 2712, 322, 264, 1002, 13, 407, 294, 512, 2020, 11, 50564], "temperature": 0.0, "avg_logprob": -0.16257433928260506, "compression_ratio": 1.57679180887372, "no_speech_prob": 0.12585517764091492}, {"id": 1302, "seek": 675936, "start": 6764.799999999999, "end": 6770.639999999999, "text": " Mojo could be seen as Python 4.0. I would not say that. I think that would drive a lot of people", "tokens": [50636, 3335, 5134, 727, 312, 1612, 382, 15329, 1017, 13, 15, 13, 286, 576, 406, 584, 300, 13, 286, 519, 300, 576, 3332, 257, 688, 295, 561, 50928], "temperature": 0.0, "avg_logprob": -0.16257433928260506, "compression_ratio": 1.57679180887372, "no_speech_prob": 0.12585517764091492}, {"id": 1303, "seek": 675936, "start": 6770.639999999999, "end": 6776.24, "text": " really crazy. Because of the PTSD of the 3.02. I'm willing to annoy people about Emax versus VIM", "tokens": [50928, 534, 3219, 13, 1436, 295, 264, 33069, 295, 264, 805, 13, 12756, 13, 286, 478, 4950, 281, 8759, 561, 466, 462, 41167, 5717, 691, 6324, 51208], "temperature": 0.0, "avg_logprob": -0.16257433928260506, "compression_ratio": 1.57679180887372, "no_speech_prob": 0.12585517764091492}, {"id": 1304, "seek": 675936, "start": 6776.24, "end": 6779.839999999999, "text": " or about text versus spaces. That's that one. I don't know. That might be a little bit far", "tokens": [51208, 420, 466, 2487, 5717, 7673, 13, 663, 311, 300, 472, 13, 286, 500, 380, 458, 13, 663, 1062, 312, 257, 707, 857, 1400, 51388], "temperature": 0.0, "avg_logprob": -0.16257433928260506, "compression_ratio": 1.57679180887372, "no_speech_prob": 0.12585517764091492}, {"id": 1305, "seek": 675936, "start": 6779.839999999999, "end": 6785.04, "text": " even for me. Like my skin may not be that thick. But the point is the step to being a super set", "tokens": [51388, 754, 337, 385, 13, 1743, 452, 3178, 815, 406, 312, 300, 5060, 13, 583, 264, 935, 307, 264, 1823, 281, 885, 257, 1687, 992, 51648], "temperature": 0.0, "avg_logprob": -0.16257433928260506, "compression_ratio": 1.57679180887372, "no_speech_prob": 0.12585517764091492}, {"id": 1306, "seek": 678504, "start": 6785.04, "end": 6790.8, "text": " and allowing all of these capabilities, I think, is the evolution of a language. It feels like", "tokens": [50364, 293, 8293, 439, 295, 613, 10862, 11, 286, 519, 11, 307, 264, 9303, 295, 257, 2856, 13, 467, 3417, 411, 50652], "temperature": 0.0, "avg_logprob": -0.1148656706015269, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0033749816939234734}, {"id": 1307, "seek": 678504, "start": 6790.8, "end": 6796.8, "text": " an evolution of a language. So he he's interested by the ideas that you're playing with, but also", "tokens": [50652, 364, 9303, 295, 257, 2856, 13, 407, 415, 415, 311, 3102, 538, 264, 3487, 300, 291, 434, 2433, 365, 11, 457, 611, 50952], "temperature": 0.0, "avg_logprob": -0.1148656706015269, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0033749816939234734}, {"id": 1308, "seek": 678504, "start": 6796.8, "end": 6801.5199999999995, "text": " concerned about the fragmentation. So how, what are the ideas you've learned? What are you thinking", "tokens": [50952, 5922, 466, 264, 9241, 19631, 13, 407, 577, 11, 437, 366, 264, 3487, 291, 600, 3264, 30, 708, 366, 291, 1953, 51188], "temperature": 0.0, "avg_logprob": -0.1148656706015269, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0033749816939234734}, {"id": 1309, "seek": 678504, "start": 6801.5199999999995, "end": 6810.48, "text": " about? How do we avoid fragmenting the community? Where the the Pythonistas and the, I don't know", "tokens": [51188, 466, 30, 1012, 360, 321, 5042, 26424, 278, 264, 1768, 30, 2305, 264, 264, 15329, 14858, 293, 264, 11, 286, 500, 380, 458, 51636], "temperature": 0.0, "avg_logprob": -0.1148656706015269, "compression_ratio": 1.6883116883116882, "no_speech_prob": 0.0033749816939234734}, {"id": 1310, "seek": 681048, "start": 6810.48, "end": 6818.799999999999, "text": " what to call the Mojo people. Magicians. I like it. Can coexist happily and share code and basically", "tokens": [50364, 437, 281, 818, 264, 3335, 5134, 561, 13, 6395, 8455, 13, 286, 411, 309, 13, 1664, 48086, 19909, 293, 2073, 3089, 293, 1936, 50780], "temperature": 0.0, "avg_logprob": -0.20247522989908853, "compression_ratio": 1.5219123505976095, "no_speech_prob": 0.05829055234789848}, {"id": 1311, "seek": 681048, "start": 6818.799999999999, "end": 6826.08, "text": " just have these big code bases that are using C Python and more and more moving towards Mojo.", "tokens": [50780, 445, 362, 613, 955, 3089, 17949, 300, 366, 1228, 383, 15329, 293, 544, 293, 544, 2684, 3030, 3335, 5134, 13, 51144], "temperature": 0.0, "avg_logprob": -0.20247522989908853, "compression_ratio": 1.5219123505976095, "no_speech_prob": 0.05829055234789848}, {"id": 1312, "seek": 681048, "start": 6826.08, "end": 6830.879999999999, "text": " Well, so again, these are lessons I learned from Swift. And here we face very similar problems,", "tokens": [51144, 1042, 11, 370, 797, 11, 613, 366, 8820, 286, 3264, 490, 25539, 13, 400, 510, 321, 1851, 588, 2531, 2740, 11, 51384], "temperature": 0.0, "avg_logprob": -0.20247522989908853, "compression_ratio": 1.5219123505976095, "no_speech_prob": 0.05829055234789848}, {"id": 1313, "seek": 681048, "start": 6830.879999999999, "end": 6839.28, "text": " right? In Swift, you have Objective C, Super Dynamic. They're very different syntax, right?", "tokens": [51384, 558, 30, 682, 25539, 11, 291, 362, 24753, 488, 383, 11, 4548, 45440, 13, 814, 434, 588, 819, 28431, 11, 558, 30, 51804], "temperature": 0.0, "avg_logprob": -0.20247522989908853, "compression_ratio": 1.5219123505976095, "no_speech_prob": 0.05829055234789848}, {"id": 1314, "seek": 683928, "start": 6839.84, "end": 6845.28, "text": " But you're talking to people who have large scale code bases. I mean, Apple's got the biggest,", "tokens": [50392, 583, 291, 434, 1417, 281, 561, 567, 362, 2416, 4373, 3089, 17949, 13, 286, 914, 11, 6373, 311, 658, 264, 3880, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09015934467315674, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0009397768881171942}, {"id": 1315, "seek": 683928, "start": 6845.28, "end": 6850.08, "text": " largest scale code base of Objective C code, right? And so, you know, none of the companies,", "tokens": [50664, 6443, 4373, 3089, 3096, 295, 24753, 488, 383, 3089, 11, 558, 30, 400, 370, 11, 291, 458, 11, 6022, 295, 264, 3431, 11, 50904], "temperature": 0.0, "avg_logprob": -0.09015934467315674, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0009397768881171942}, {"id": 1316, "seek": 683928, "start": 6850.08, "end": 6853.36, "text": " none of the iOS developers, none of the other developers want to rewrite everything all at", "tokens": [50904, 6022, 295, 264, 17430, 8849, 11, 6022, 295, 264, 661, 8849, 528, 281, 28132, 1203, 439, 412, 51068], "temperature": 0.0, "avg_logprob": -0.09015934467315674, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0009397768881171942}, {"id": 1317, "seek": 683928, "start": 6853.36, "end": 6857.759999999999, "text": " once. And so you want to be able to adopt things piece at a time. And so a thing that I found that", "tokens": [51068, 1564, 13, 400, 370, 291, 528, 281, 312, 1075, 281, 6878, 721, 2522, 412, 257, 565, 13, 400, 370, 257, 551, 300, 286, 1352, 300, 51288], "temperature": 0.0, "avg_logprob": -0.09015934467315674, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0009397768881171942}, {"id": 1318, "seek": 683928, "start": 6857.759999999999, "end": 6862.639999999999, "text": " worked very well in the Swift community was saying, okay, cool. And this is when Swift was very", "tokens": [51288, 2732, 588, 731, 294, 264, 25539, 1768, 390, 1566, 11, 1392, 11, 1627, 13, 400, 341, 307, 562, 25539, 390, 588, 51532], "temperature": 0.0, "avg_logprob": -0.09015934467315674, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.0009397768881171942}, {"id": 1319, "seek": 686264, "start": 6862.64, "end": 6869.280000000001, "text": " young. As you say, okay, you have a million line of code Objective C app. Don't rewrite it all.", "tokens": [50364, 2037, 13, 1018, 291, 584, 11, 1392, 11, 291, 362, 257, 2459, 1622, 295, 3089, 24753, 488, 383, 724, 13, 1468, 380, 28132, 309, 439, 13, 50696], "temperature": 0.0, "avg_logprob": -0.14157705307006835, "compression_ratio": 1.6383763837638377, "no_speech_prob": 0.1258765012025833}, {"id": 1320, "seek": 686264, "start": 6869.280000000001, "end": 6874.160000000001, "text": " But when you implement a new feature, go implement that new class using Swift.", "tokens": [50696, 583, 562, 291, 4445, 257, 777, 4111, 11, 352, 4445, 300, 777, 1508, 1228, 25539, 13, 50940], "temperature": 0.0, "avg_logprob": -0.14157705307006835, "compression_ratio": 1.6383763837638377, "no_speech_prob": 0.1258765012025833}, {"id": 1321, "seek": 686264, "start": 6875.04, "end": 6879.52, "text": " Right. And so now this turns out is a very wonderful thing for an app developer.", "tokens": [50984, 1779, 13, 400, 370, 586, 341, 4523, 484, 307, 257, 588, 3715, 551, 337, 364, 724, 10754, 13, 51208], "temperature": 0.0, "avg_logprob": -0.14157705307006835, "compression_ratio": 1.6383763837638377, "no_speech_prob": 0.1258765012025833}, {"id": 1322, "seek": 686264, "start": 6880.320000000001, "end": 6884.96, "text": " But it's a huge challenge for this compiler team and the systems people that are implementing.", "tokens": [51248, 583, 309, 311, 257, 2603, 3430, 337, 341, 31958, 1469, 293, 264, 3652, 561, 300, 366, 18114, 13, 51480], "temperature": 0.0, "avg_logprob": -0.14157705307006835, "compression_ratio": 1.6383763837638377, "no_speech_prob": 0.1258765012025833}, {"id": 1323, "seek": 686264, "start": 6884.96, "end": 6889.280000000001, "text": " That's right. And this comes back to what is this trade off between doing the hard thing that", "tokens": [51480, 663, 311, 558, 13, 400, 341, 1487, 646, 281, 437, 307, 341, 4923, 766, 1296, 884, 264, 1152, 551, 300, 51696], "temperature": 0.0, "avg_logprob": -0.14157705307006835, "compression_ratio": 1.6383763837638377, "no_speech_prob": 0.1258765012025833}, {"id": 1324, "seek": 688928, "start": 6890.08, "end": 6895.599999999999, "text": " enables scale versus doing the theoretically pure and ideal thing, right? And so Swift had", "tokens": [50404, 17077, 4373, 5717, 884, 264, 29400, 6075, 293, 7157, 551, 11, 558, 30, 400, 370, 25539, 632, 50680], "temperature": 0.0, "avg_logprob": -0.09839081553231298, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.001648283563554287}, {"id": 1325, "seek": 688928, "start": 6895.599999999999, "end": 6900.16, "text": " adopted and built a lot of different machinery to deeply integrate with the Objective C runtime.", "tokens": [50680, 12175, 293, 3094, 257, 688, 295, 819, 27302, 281, 8760, 13365, 365, 264, 24753, 488, 383, 34474, 13, 50908], "temperature": 0.0, "avg_logprob": -0.09839081553231298, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.001648283563554287}, {"id": 1326, "seek": 688928, "start": 6900.16, "end": 6904.8, "text": " And we're doing the same thing with Python, right? Now, what happened in the case of Swift is that", "tokens": [50908, 400, 321, 434, 884, 264, 912, 551, 365, 15329, 11, 558, 30, 823, 11, 437, 2011, 294, 264, 1389, 295, 25539, 307, 300, 51140], "temperature": 0.0, "avg_logprob": -0.09839081553231298, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.001648283563554287}, {"id": 1327, "seek": 688928, "start": 6905.679999999999, "end": 6911.5199999999995, "text": " Swift's language got more and more mature over time, right? And incidentally, Mojo is a much", "tokens": [51184, 25539, 311, 2856, 658, 544, 293, 544, 14442, 670, 565, 11, 558, 30, 400, 9348, 379, 11, 3335, 5134, 307, 257, 709, 51476], "temperature": 0.0, "avg_logprob": -0.09839081553231298, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.001648283563554287}, {"id": 1328, "seek": 688928, "start": 6911.5199999999995, "end": 6915.84, "text": " simpler language than Swift in many ways. And so I think that Mojo will develop way faster than Swift", "tokens": [51476, 18587, 2856, 813, 25539, 294, 867, 2098, 13, 400, 370, 286, 519, 300, 3335, 5134, 486, 1499, 636, 4663, 813, 25539, 51692], "temperature": 0.0, "avg_logprob": -0.09839081553231298, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.001648283563554287}, {"id": 1329, "seek": 691584, "start": 6915.84, "end": 6920.56, "text": " for a variety of reasons. But as the language gets more mature and parallel with that, you have", "tokens": [50364, 337, 257, 5673, 295, 4112, 13, 583, 382, 264, 2856, 2170, 544, 14442, 293, 8952, 365, 300, 11, 291, 362, 50600], "temperature": 0.0, "avg_logprob": -0.08936829852242754, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.003824051469564438}, {"id": 1330, "seek": 691584, "start": 6920.56, "end": 6926.0, "text": " new people starting new projects, right? And so when the language is mature and somebody", "tokens": [50600, 777, 561, 2891, 777, 4455, 11, 558, 30, 400, 370, 562, 264, 2856, 307, 14442, 293, 2618, 50872], "temperature": 0.0, "avg_logprob": -0.08936829852242754, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.003824051469564438}, {"id": 1331, "seek": 691584, "start": 6926.0, "end": 6929.52, "text": " is starting a new project, that's when they say, okay, cool, I'm not dealing with a million lines", "tokens": [50872, 307, 2891, 257, 777, 1716, 11, 300, 311, 562, 436, 584, 11, 1392, 11, 1627, 11, 286, 478, 406, 6260, 365, 257, 2459, 3876, 51048], "temperature": 0.0, "avg_logprob": -0.08936829852242754, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.003824051469564438}, {"id": 1332, "seek": 691584, "start": 6929.52, "end": 6934.56, "text": " of code. I'll just start and use the new thing for my whole stack. Now, the problem is, again,", "tokens": [51048, 295, 3089, 13, 286, 603, 445, 722, 293, 764, 264, 777, 551, 337, 452, 1379, 8630, 13, 823, 11, 264, 1154, 307, 11, 797, 11, 51300], "temperature": 0.0, "avg_logprob": -0.08936829852242754, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.003824051469564438}, {"id": 1333, "seek": 691584, "start": 6934.56, "end": 6940.32, "text": " you come back to where communities and where people that work together, you build new subsystem or", "tokens": [51300, 291, 808, 646, 281, 689, 4456, 293, 689, 561, 300, 589, 1214, 11, 291, 1322, 777, 2090, 9321, 420, 51588], "temperature": 0.0, "avg_logprob": -0.08936829852242754, "compression_ratio": 1.7435897435897436, "no_speech_prob": 0.003824051469564438}, {"id": 1334, "seek": 694032, "start": 6940.32, "end": 6946.639999999999, "text": " new feature or new thing in Swift or you build new thing in Mojo, then you want to be end up being", "tokens": [50364, 777, 4111, 420, 777, 551, 294, 25539, 420, 291, 1322, 777, 551, 294, 3335, 5134, 11, 550, 291, 528, 281, 312, 917, 493, 885, 50680], "temperature": 0.0, "avg_logprob": -0.09843961334228515, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.03621084243059158}, {"id": 1335, "seek": 694032, "start": 6946.639999999999, "end": 6951.92, "text": " used on the other side, right? And so then you need to work on integration back the other way.", "tokens": [50680, 1143, 322, 264, 661, 1252, 11, 558, 30, 400, 370, 550, 291, 643, 281, 589, 322, 10980, 646, 264, 661, 636, 13, 50944], "temperature": 0.0, "avg_logprob": -0.09843961334228515, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.03621084243059158}, {"id": 1336, "seek": 694032, "start": 6952.719999999999, "end": 6956.639999999999, "text": " And so it's not just Mojo talking to Python, it's also Python talking to Mojo,", "tokens": [50984, 400, 370, 309, 311, 406, 445, 3335, 5134, 1417, 281, 15329, 11, 309, 311, 611, 15329, 1417, 281, 3335, 5134, 11, 51180], "temperature": 0.0, "avg_logprob": -0.09843961334228515, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.03621084243059158}, {"id": 1337, "seek": 694032, "start": 6957.679999999999, "end": 6961.36, "text": " right? And so what I would love to see, and I don't want to see this next month, right? But", "tokens": [51232, 558, 30, 400, 370, 437, 286, 576, 959, 281, 536, 11, 293, 286, 500, 380, 528, 281, 536, 341, 958, 1618, 11, 558, 30, 583, 51416], "temperature": 0.0, "avg_logprob": -0.09843961334228515, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.03621084243059158}, {"id": 1338, "seek": 694032, "start": 6961.36, "end": 6965.759999999999, "text": " what I want to see over the course of time is I would love to see people that are building these", "tokens": [51416, 437, 286, 528, 281, 536, 670, 264, 1164, 295, 565, 307, 286, 576, 959, 281, 536, 561, 300, 366, 2390, 613, 51636], "temperature": 0.0, "avg_logprob": -0.09843961334228515, "compression_ratio": 1.8588709677419355, "no_speech_prob": 0.03621084243059158}, {"id": 1339, "seek": 696576, "start": 6965.76, "end": 6974.08, "text": " packages, like NumPy or TensorFlow or these packages that are half Python, half C++.", "tokens": [50364, 17401, 11, 411, 22592, 47, 88, 420, 37624, 420, 613, 17401, 300, 366, 1922, 15329, 11, 1922, 383, 25472, 13, 50780], "temperature": 0.0, "avg_logprob": -0.12027174053770123, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.07156039774417877}, {"id": 1340, "seek": 696576, "start": 6975.04, "end": 6982.0, "text": " And if you say, okay, cool, I want to get out of this Python C++ world into a unified world,", "tokens": [50828, 400, 498, 291, 584, 11, 1392, 11, 1627, 11, 286, 528, 281, 483, 484, 295, 341, 15329, 383, 25472, 1002, 666, 257, 26787, 1002, 11, 51176], "temperature": 0.0, "avg_logprob": -0.12027174053770123, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.07156039774417877}, {"id": 1341, "seek": 696576, "start": 6982.0, "end": 6989.04, "text": " and so I can move to Mojo, but I can't give up all my Python clients because these libraries", "tokens": [51176, 293, 370, 286, 393, 1286, 281, 3335, 5134, 11, 457, 286, 393, 380, 976, 493, 439, 452, 15329, 6982, 570, 613, 15148, 51528], "temperature": 0.0, "avg_logprob": -0.12027174053770123, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.07156039774417877}, {"id": 1342, "seek": 696576, "start": 6989.04, "end": 6995.12, "text": " get used by everybody and they're not all going to switch all once and maybe never, right? Well,", "tokens": [51528, 483, 1143, 538, 2201, 293, 436, 434, 406, 439, 516, 281, 3679, 439, 1564, 293, 1310, 1128, 11, 558, 30, 1042, 11, 51832], "temperature": 0.0, "avg_logprob": -0.12027174053770123, "compression_ratio": 1.5228215767634854, "no_speech_prob": 0.07156039774417877}, {"id": 1343, "seek": 699512, "start": 6995.12, "end": 6999.76, "text": " so the way we should do that is we should vend Python interfaces to the Mojo types.", "tokens": [50364, 370, 264, 636, 321, 820, 360, 300, 307, 321, 820, 10169, 15329, 28416, 281, 264, 3335, 5134, 3467, 13, 50596], "temperature": 0.0, "avg_logprob": -0.09693417855358999, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.000391965702874586}, {"id": 1344, "seek": 699512, "start": 7000.88, "end": 7004.64, "text": " And that's what we did in Swift and worked great. I mean, it was a huge implementation challenge", "tokens": [50652, 400, 300, 311, 437, 321, 630, 294, 25539, 293, 2732, 869, 13, 286, 914, 11, 309, 390, 257, 2603, 11420, 3430, 50840], "temperature": 0.0, "avg_logprob": -0.09693417855358999, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.000391965702874586}, {"id": 1345, "seek": 699512, "start": 7004.64, "end": 7009.68, "text": " for the compiler people, right? But there's only a dozen of those compiler people and there are", "tokens": [50840, 337, 264, 31958, 561, 11, 558, 30, 583, 456, 311, 787, 257, 16654, 295, 729, 31958, 561, 293, 456, 366, 51092], "temperature": 0.0, "avg_logprob": -0.09693417855358999, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.000391965702874586}, {"id": 1346, "seek": 699512, "start": 7009.68, "end": 7016.8, "text": " millions of users. And so it's a very expensive, capital intensive, like skill set intensive", "tokens": [51092, 6803, 295, 5022, 13, 400, 370, 309, 311, 257, 588, 5124, 11, 4238, 18957, 11, 411, 5389, 992, 18957, 51448], "temperature": 0.0, "avg_logprob": -0.09693417855358999, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.000391965702874586}, {"id": 1347, "seek": 699512, "start": 7016.8, "end": 7021.2, "text": " problem. But once you solve that problem, it really helps adoption and really helps the community", "tokens": [51448, 1154, 13, 583, 1564, 291, 5039, 300, 1154, 11, 309, 534, 3665, 19215, 293, 534, 3665, 264, 1768, 51668], "temperature": 0.0, "avg_logprob": -0.09693417855358999, "compression_ratio": 1.7106227106227105, "no_speech_prob": 0.000391965702874586}, {"id": 1348, "seek": 702120, "start": 7021.2, "end": 7025.76, "text": " progressively adopt technologies. And so I think that this approach will work quite well with the", "tokens": [50364, 46667, 6878, 7943, 13, 400, 370, 286, 519, 300, 341, 3109, 486, 589, 1596, 731, 365, 264, 50592], "temperature": 0.0, "avg_logprob": -0.14841624668666295, "compression_ratio": 1.5204918032786885, "no_speech_prob": 0.002510832855477929}, {"id": 1349, "seek": 702120, "start": 7025.76, "end": 7031.599999999999, "text": " Python and the Mojo world. So for a package ported to Mojo and then create a Python interface?", "tokens": [50592, 15329, 293, 264, 3335, 5134, 1002, 13, 407, 337, 257, 7372, 2436, 292, 281, 3335, 5134, 293, 550, 1884, 257, 15329, 9226, 30, 50884], "temperature": 0.0, "avg_logprob": -0.14841624668666295, "compression_ratio": 1.5204918032786885, "no_speech_prob": 0.002510832855477929}, {"id": 1350, "seek": 702120, "start": 7031.599999999999, "end": 7039.5199999999995, "text": " Yep. So how do you just to link on these packages, NumPy, PyTorch, and TensorFlow?", "tokens": [50884, 7010, 13, 407, 577, 360, 291, 445, 281, 2113, 322, 613, 17401, 11, 22592, 47, 88, 11, 9953, 51, 284, 339, 11, 293, 37624, 30, 51280], "temperature": 0.0, "avg_logprob": -0.14841624668666295, "compression_ratio": 1.5204918032786885, "no_speech_prob": 0.002510832855477929}, {"id": 1351, "seek": 702120, "start": 7039.5199999999995, "end": 7045.04, "text": " Yeah. How do they play nicely together? So is Mojo supposed to be, let's talk about the machine", "tokens": [51280, 865, 13, 1012, 360, 436, 862, 9594, 1214, 30, 407, 307, 3335, 5134, 3442, 281, 312, 11, 718, 311, 751, 466, 264, 3479, 51556], "temperature": 0.0, "avg_logprob": -0.14841624668666295, "compression_ratio": 1.5204918032786885, "no_speech_prob": 0.002510832855477929}, {"id": 1352, "seek": 704504, "start": 7045.04, "end": 7053.36, "text": " learning ones. Is Mojo kind of vision to replace PyTorch and TensorFlow to incorporate it? What's", "tokens": [50364, 2539, 2306, 13, 1119, 3335, 5134, 733, 295, 5201, 281, 7406, 9953, 51, 284, 339, 293, 37624, 281, 16091, 309, 30, 708, 311, 50780], "temperature": 0.0, "avg_logprob": -0.1692862466116932, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.09664022922515869}, {"id": 1353, "seek": 704504, "start": 7053.36, "end": 7059.2, "text": " the relationship in this? All right. So let's dance. So take a step back. So I wear many hats.", "tokens": [50780, 264, 2480, 294, 341, 30, 1057, 558, 13, 407, 718, 311, 4489, 13, 407, 747, 257, 1823, 646, 13, 407, 286, 3728, 867, 20549, 13, 51072], "temperature": 0.0, "avg_logprob": -0.1692862466116932, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.09664022922515869}, {"id": 1354, "seek": 704504, "start": 7060.32, "end": 7066.24, "text": " So you're angling it on the Mojo side. Mojo is a programming language. And so it can help solve", "tokens": [51128, 407, 291, 434, 2562, 1688, 309, 322, 264, 3335, 5134, 1252, 13, 3335, 5134, 307, 257, 9410, 2856, 13, 400, 370, 309, 393, 854, 5039, 51424], "temperature": 0.0, "avg_logprob": -0.1692862466116932, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.09664022922515869}, {"id": 1355, "seek": 704504, "start": 7066.88, "end": 7072.56, "text": " the C++ Python feud that's happening. The fire Mojo got me. I'm sorry. We should be talking", "tokens": [51456, 264, 383, 25472, 15329, 36377, 300, 311, 2737, 13, 440, 2610, 3335, 5134, 658, 385, 13, 286, 478, 2597, 13, 492, 820, 312, 1417, 51740], "temperature": 0.0, "avg_logprob": -0.1692862466116932, "compression_ratio": 1.4901960784313726, "no_speech_prob": 0.09664022922515869}, {"id": 1356, "seek": 707256, "start": 7073.04, "end": 7080.160000000001, "text": " modular. Yes. Yes. Okay. So the fire emoji is amazing. I love it. It's a big deal. The other", "tokens": [50388, 31111, 13, 1079, 13, 1079, 13, 1033, 13, 407, 264, 2610, 31595, 307, 2243, 13, 286, 959, 309, 13, 467, 311, 257, 955, 2028, 13, 440, 661, 50744], "temperature": 0.0, "avg_logprob": -0.0992938839659399, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00475296238437295}, {"id": 1357, "seek": 707256, "start": 7080.160000000001, "end": 7086.080000000001, "text": " side of this is the fire emoji is in service of solving some big AI problems. And so the big AI", "tokens": [50744, 1252, 295, 341, 307, 264, 2610, 31595, 307, 294, 2643, 295, 12606, 512, 955, 7318, 2740, 13, 400, 370, 264, 955, 7318, 51040], "temperature": 0.0, "avg_logprob": -0.0992938839659399, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00475296238437295}, {"id": 1358, "seek": 707256, "start": 7086.080000000001, "end": 7092.8, "text": " problems are again, this fragmentation, this hardware nightmare, this explosion of new potential,", "tokens": [51040, 2740, 366, 797, 11, 341, 9241, 19631, 11, 341, 8837, 18724, 11, 341, 15673, 295, 777, 3995, 11, 51376], "temperature": 0.0, "avg_logprob": -0.0992938839659399, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00475296238437295}, {"id": 1359, "seek": 707256, "start": 7092.8, "end": 7098.4800000000005, "text": " but that's not getting felt by the industry. And so when you look at how does the modular engine", "tokens": [51376, 457, 300, 311, 406, 1242, 2762, 538, 264, 3518, 13, 400, 370, 562, 291, 574, 412, 577, 775, 264, 31111, 2848, 51660], "temperature": 0.0, "avg_logprob": -0.0992938839659399, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.00475296238437295}, {"id": 1360, "seek": 709848, "start": 7098.48, "end": 7104.4, "text": " help TensorFlow and PyTorch, it's not replacing them. In fact, when I talk to people, again,", "tokens": [50364, 854, 37624, 293, 9953, 51, 284, 339, 11, 309, 311, 406, 19139, 552, 13, 682, 1186, 11, 562, 286, 751, 281, 561, 11, 797, 11, 50660], "temperature": 0.0, "avg_logprob": -0.07607014555680125, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.07365339249372482}, {"id": 1361, "seek": 709848, "start": 7104.4, "end": 7108.48, "text": " they don't like to rewrite all their code, you have people that are using a bunch of PyTorch,", "tokens": [50660, 436, 500, 380, 411, 281, 28132, 439, 641, 3089, 11, 291, 362, 561, 300, 366, 1228, 257, 3840, 295, 9953, 51, 284, 339, 11, 50864], "temperature": 0.0, "avg_logprob": -0.07607014555680125, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.07365339249372482}, {"id": 1362, "seek": 709848, "start": 7108.48, "end": 7113.2, "text": " a bunch of TensorFlow. They have models that they've been building over the course of many years.", "tokens": [50864, 257, 3840, 295, 37624, 13, 814, 362, 5245, 300, 436, 600, 668, 2390, 670, 264, 1164, 295, 867, 924, 13, 51100], "temperature": 0.0, "avg_logprob": -0.07607014555680125, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.07365339249372482}, {"id": 1363, "seek": 709848, "start": 7113.2, "end": 7117.839999999999, "text": " And when I talk to them, there's a few exceptions, but generally they don't want to rewrite all their", "tokens": [51100, 400, 562, 286, 751, 281, 552, 11, 456, 311, 257, 1326, 22847, 11, 457, 5101, 436, 500, 380, 528, 281, 28132, 439, 641, 51332], "temperature": 0.0, "avg_logprob": -0.07607014555680125, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.07365339249372482}, {"id": 1364, "seek": 709848, "start": 7117.839999999999, "end": 7122.879999999999, "text": " code. And so what we're doing is we're saying, okay, well, you don't have to rewrite all your code.", "tokens": [51332, 3089, 13, 400, 370, 437, 321, 434, 884, 307, 321, 434, 1566, 11, 1392, 11, 731, 11, 291, 500, 380, 362, 281, 28132, 439, 428, 3089, 13, 51584], "temperature": 0.0, "avg_logprob": -0.07607014555680125, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.07365339249372482}, {"id": 1365, "seek": 709848, "start": 7122.879999999999, "end": 7127.2, "text": " What happens is the modular engine goes in there and goes underneath TensorFlow and PyTorch.", "tokens": [51584, 708, 2314, 307, 264, 31111, 2848, 1709, 294, 456, 293, 1709, 7223, 37624, 293, 9953, 51, 284, 339, 13, 51800], "temperature": 0.0, "avg_logprob": -0.07607014555680125, "compression_ratio": 1.8983606557377048, "no_speech_prob": 0.07365339249372482}, {"id": 1366, "seek": 712720, "start": 7127.28, "end": 7131.2, "text": " It's fully compatible. And it just provides better performance, better predictability,", "tokens": [50368, 467, 311, 4498, 18218, 13, 400, 309, 445, 6417, 1101, 3389, 11, 1101, 6069, 2310, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09798279930563535, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0035928122233599424}, {"id": 1367, "seek": 712720, "start": 7131.2, "end": 7136.16, "text": " better tooling. It's a better experience that helps lift TensorFlow and PyTorch and make them even", "tokens": [50564, 1101, 46593, 13, 467, 311, 257, 1101, 1752, 300, 3665, 5533, 37624, 293, 9953, 51, 284, 339, 293, 652, 552, 754, 50812], "temperature": 0.0, "avg_logprob": -0.09798279930563535, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0035928122233599424}, {"id": 1368, "seek": 712720, "start": 7136.16, "end": 7141.28, "text": " better. I love Python. I love TensorFlow. I love PyTorch, right? This is about making the world", "tokens": [50812, 1101, 13, 286, 959, 15329, 13, 286, 959, 37624, 13, 286, 959, 9953, 51, 284, 339, 11, 558, 30, 639, 307, 466, 1455, 264, 1002, 51068], "temperature": 0.0, "avg_logprob": -0.09798279930563535, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0035928122233599424}, {"id": 1369, "seek": 712720, "start": 7141.28, "end": 7147.5199999999995, "text": " better, because we need AI to go further. But if I have a process that trains a model and I have a", "tokens": [51068, 1101, 11, 570, 321, 643, 7318, 281, 352, 3052, 13, 583, 498, 286, 362, 257, 1399, 300, 16329, 257, 2316, 293, 286, 362, 257, 51380], "temperature": 0.0, "avg_logprob": -0.09798279930563535, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0035928122233599424}, {"id": 1370, "seek": 712720, "start": 7147.5199999999995, "end": 7153.92, "text": " process that performs inference on that model, and I have the model itself, what should I do with that", "tokens": [51380, 1399, 300, 26213, 38253, 322, 300, 2316, 11, 293, 286, 362, 264, 2316, 2564, 11, 437, 820, 286, 360, 365, 300, 51700], "temperature": 0.0, "avg_logprob": -0.09798279930563535, "compression_ratio": 1.775735294117647, "no_speech_prob": 0.0035928122233599424}, {"id": 1371, "seek": 715392, "start": 7153.92, "end": 7161.6, "text": " in the long arc of history in terms of if I use PyTorch to train it? Should I rewrite stuff in", "tokens": [50364, 294, 264, 938, 10346, 295, 2503, 294, 2115, 295, 498, 286, 764, 9953, 51, 284, 339, 281, 3847, 309, 30, 6454, 286, 28132, 1507, 294, 50748], "temperature": 0.0, "avg_logprob": -0.115170009208448, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0028885994106531143}, {"id": 1372, "seek": 715392, "start": 7161.6, "end": 7167.2, "text": " Mojo? Would that, if I care about performance? Oh, so I mean, again, it depends. So if you care", "tokens": [50748, 3335, 5134, 30, 6068, 300, 11, 498, 286, 1127, 466, 3389, 30, 876, 11, 370, 286, 914, 11, 797, 11, 309, 5946, 13, 407, 498, 291, 1127, 51028], "temperature": 0.0, "avg_logprob": -0.115170009208448, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0028885994106531143}, {"id": 1373, "seek": 715392, "start": 7167.2, "end": 7171.12, "text": " about performance, then writing in Mojo is going to be way better than writing in Python. But if", "tokens": [51028, 466, 3389, 11, 550, 3579, 294, 3335, 5134, 307, 516, 281, 312, 636, 1101, 813, 3579, 294, 15329, 13, 583, 498, 51224], "temperature": 0.0, "avg_logprob": -0.115170009208448, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0028885994106531143}, {"id": 1374, "seek": 715392, "start": 7171.12, "end": 7177.2, "text": " you look at LLM companies, for example, if you look at OpenAI, rumored, and you look at many of", "tokens": [51224, 291, 574, 412, 441, 43, 44, 3431, 11, 337, 1365, 11, 498, 291, 574, 412, 7238, 48698, 11, 8347, 2769, 11, 293, 291, 574, 412, 867, 295, 51528], "temperature": 0.0, "avg_logprob": -0.115170009208448, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0028885994106531143}, {"id": 1375, "seek": 715392, "start": 7177.2, "end": 7183.76, "text": " the other folks that are working on many of these LLMs and other innovative machine learning models,", "tokens": [51528, 264, 661, 4024, 300, 366, 1364, 322, 867, 295, 613, 441, 43, 26386, 293, 661, 12999, 3479, 2539, 5245, 11, 51856], "temperature": 0.0, "avg_logprob": -0.115170009208448, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.0028885994106531143}, {"id": 1376, "seek": 718376, "start": 7184.320000000001, "end": 7188.56, "text": " on the one hand, they're innovating in the data collection and the model billions of parameters", "tokens": [50392, 322, 264, 472, 1011, 11, 436, 434, 5083, 990, 294, 264, 1412, 5765, 293, 264, 2316, 17375, 295, 9834, 50604], "temperature": 0.0, "avg_logprob": -0.13454982662989087, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0013244857545942068}, {"id": 1377, "seek": 718376, "start": 7188.56, "end": 7194.96, "text": " and the model architecture and the RLE or HF and the like all the all the cool things that people", "tokens": [50604, 293, 264, 2316, 9482, 293, 264, 497, 2634, 420, 389, 37, 293, 264, 411, 439, 264, 439, 264, 1627, 721, 300, 561, 50924], "temperature": 0.0, "avg_logprob": -0.13454982662989087, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0013244857545942068}, {"id": 1378, "seek": 718376, "start": 7194.96, "end": 7201.04, "text": " are talking about. But on the other hand, they're spending a lot of time writing CUDA curls, right?", "tokens": [50924, 366, 1417, 466, 13, 583, 322, 264, 661, 1011, 11, 436, 434, 6434, 257, 688, 295, 565, 3579, 29777, 7509, 36950, 11, 558, 30, 51228], "temperature": 0.0, "avg_logprob": -0.13454982662989087, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0013244857545942068}, {"id": 1379, "seek": 718376, "start": 7201.84, "end": 7206.24, "text": " And so you say, wait a second, how much faster could all this progress go if they were not having", "tokens": [51268, 400, 370, 291, 584, 11, 1699, 257, 1150, 11, 577, 709, 4663, 727, 439, 341, 4205, 352, 498, 436, 645, 406, 1419, 51488], "temperature": 0.0, "avg_logprob": -0.13454982662989087, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0013244857545942068}, {"id": 1380, "seek": 718376, "start": 7206.24, "end": 7210.320000000001, "text": " to handwrite all these CUDA curls? Right. And so there are a few technologies that are out there", "tokens": [51488, 281, 1011, 21561, 439, 613, 29777, 7509, 36950, 30, 1779, 13, 400, 370, 456, 366, 257, 1326, 7943, 300, 366, 484, 456, 51692], "temperature": 0.0, "avg_logprob": -0.13454982662989087, "compression_ratio": 1.7491039426523298, "no_speech_prob": 0.0013244857545942068}, {"id": 1381, "seek": 721032, "start": 7210.32, "end": 7215.12, "text": " and people have been working on this problem for a while. And they're trying to solve subsets to", "tokens": [50364, 293, 561, 362, 668, 1364, 322, 341, 1154, 337, 257, 1339, 13, 400, 436, 434, 1382, 281, 5039, 2090, 1385, 281, 50604], "temperature": 0.0, "avg_logprob": -0.10494457164280852, "compression_ratio": 1.892156862745098, "no_speech_prob": 0.0010647251037880778}, {"id": 1382, "seek": 721032, "start": 7215.12, "end": 7219.599999999999, "text": " the problem again, kind of fragmenting the space. And so what Mojo provides for these kinds of companies", "tokens": [50604, 264, 1154, 797, 11, 733, 295, 26424, 278, 264, 1901, 13, 400, 370, 437, 3335, 5134, 6417, 337, 613, 3685, 295, 3431, 50828], "temperature": 0.0, "avg_logprob": -0.10494457164280852, "compression_ratio": 1.892156862745098, "no_speech_prob": 0.0010647251037880778}, {"id": 1383, "seek": 721032, "start": 7219.599999999999, "end": 7225.2, "text": " is the ability to say, cool, I can have a unifying theory. Right. And again, the better", "tokens": [50828, 307, 264, 3485, 281, 584, 11, 1627, 11, 286, 393, 362, 257, 517, 5489, 5261, 13, 1779, 13, 400, 797, 11, 264, 1101, 51108], "temperature": 0.0, "avg_logprob": -0.10494457164280852, "compression_ratio": 1.892156862745098, "no_speech_prob": 0.0010647251037880778}, {"id": 1384, "seek": 721032, "start": 7225.2, "end": 7229.36, "text": " together the unifying theory, the the two world problem or the three world problem or the n world", "tokens": [51108, 1214, 264, 517, 5489, 5261, 11, 264, 264, 732, 1002, 1154, 420, 264, 1045, 1002, 1154, 420, 264, 297, 1002, 51316], "temperature": 0.0, "avg_logprob": -0.10494457164280852, "compression_ratio": 1.892156862745098, "no_speech_prob": 0.0010647251037880778}, {"id": 1385, "seek": 721032, "start": 7229.36, "end": 7234.0, "text": " problem, like this is the thing that is slowing people down. And so as we help solve this problem,", "tokens": [51316, 1154, 11, 411, 341, 307, 264, 551, 300, 307, 26958, 561, 760, 13, 400, 370, 382, 321, 854, 5039, 341, 1154, 11, 51548], "temperature": 0.0, "avg_logprob": -0.10494457164280852, "compression_ratio": 1.892156862745098, "no_speech_prob": 0.0010647251037880778}, {"id": 1386, "seek": 721032, "start": 7234.0, "end": 7239.12, "text": " I think it'll be very helpful for making this whole cycle go faster. So obviously, we talked", "tokens": [51548, 286, 519, 309, 603, 312, 588, 4961, 337, 1455, 341, 1379, 6586, 352, 4663, 13, 407, 2745, 11, 321, 2825, 51804], "temperature": 0.0, "avg_logprob": -0.10494457164280852, "compression_ratio": 1.892156862745098, "no_speech_prob": 0.0010647251037880778}, {"id": 1387, "seek": 723912, "start": 7239.12, "end": 7245.84, "text": " about the transition from Objective C to Swift, if design this programming language. And you've", "tokens": [50364, 466, 264, 6034, 490, 24753, 488, 383, 281, 25539, 11, 498, 1715, 341, 9410, 2856, 13, 400, 291, 600, 50700], "temperature": 0.0, "avg_logprob": -0.16366631471658055, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.004605503287166357}, {"id": 1388, "seek": 723912, "start": 7245.84, "end": 7254.08, "text": " also talked quite a bit about the use of Swift for machine learning context. Why have you decided", "tokens": [50700, 611, 2825, 1596, 257, 857, 466, 264, 764, 295, 25539, 337, 3479, 2539, 4319, 13, 1545, 362, 291, 3047, 51112], "temperature": 0.0, "avg_logprob": -0.16366631471658055, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.004605503287166357}, {"id": 1389, "seek": 723912, "start": 7254.08, "end": 7261.28, "text": " to move away from maybe an intense focus on Swift for the machine learning context versus sort of", "tokens": [51112, 281, 1286, 1314, 490, 1310, 364, 9447, 1879, 322, 25539, 337, 264, 3479, 2539, 4319, 5717, 1333, 295, 51472], "temperature": 0.0, "avg_logprob": -0.16366631471658055, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.004605503287166357}, {"id": 1390, "seek": 723912, "start": 7262.0, "end": 7266.16, "text": " designing a new programming language that happens to be a super surprise.", "tokens": [51508, 14685, 257, 777, 9410, 2856, 300, 2314, 281, 312, 257, 1687, 6365, 13, 51716], "temperature": 0.0, "avg_logprob": -0.16366631471658055, "compression_ratio": 1.6743119266055047, "no_speech_prob": 0.004605503287166357}, {"id": 1391, "seek": 726616, "start": 7266.16, "end": 7268.639999999999, "text": " You're saying this is an irrational set of life choices I make?", "tokens": [50364, 509, 434, 1566, 341, 307, 364, 39914, 992, 295, 993, 7994, 286, 652, 30, 50488], "temperature": 0.0, "avg_logprob": -0.13542462170608643, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.05336746573448181}, {"id": 1392, "seek": 726616, "start": 7270.48, "end": 7277.28, "text": " Did you go to the desert? And did you meditate on it? Okay. All right. No, it was bold and needed.", "tokens": [50580, 2589, 291, 352, 281, 264, 11029, 30, 400, 630, 291, 29989, 322, 309, 30, 1033, 13, 1057, 558, 13, 883, 11, 309, 390, 11928, 293, 2978, 13, 50920], "temperature": 0.0, "avg_logprob": -0.13542462170608643, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.05336746573448181}, {"id": 1393, "seek": 726616, "start": 7277.28, "end": 7282.88, "text": " And I think, I mean, it's just bold and sometimes to take those leaps is a difficult leap to take.", "tokens": [50920, 400, 286, 519, 11, 286, 914, 11, 309, 311, 445, 11928, 293, 2171, 281, 747, 729, 476, 2382, 307, 257, 2252, 19438, 281, 747, 13, 51200], "temperature": 0.0, "avg_logprob": -0.13542462170608643, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.05336746573448181}, {"id": 1394, "seek": 726616, "start": 7282.88, "end": 7285.68, "text": " Yeah. Well, so okay, I mean, I think there's a couple of different things. So", "tokens": [51200, 865, 13, 1042, 11, 370, 1392, 11, 286, 914, 11, 286, 519, 456, 311, 257, 1916, 295, 819, 721, 13, 407, 51340], "temperature": 0.0, "avg_logprob": -0.13542462170608643, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.05336746573448181}, {"id": 1395, "seek": 726616, "start": 7286.72, "end": 7293.84, "text": " actually, I left Apple back in 2017, like January 2017. So it's been a number of years that I left", "tokens": [51392, 767, 11, 286, 1411, 6373, 646, 294, 6591, 11, 411, 7061, 6591, 13, 407, 309, 311, 668, 257, 1230, 295, 924, 300, 286, 1411, 51748], "temperature": 0.0, "avg_logprob": -0.13542462170608643, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.05336746573448181}, {"id": 1396, "seek": 729384, "start": 7293.84, "end": 7301.92, "text": " Apple. And the reason I left Apple was to do AI. Okay. So and again, I won't come on Apple and AI,", "tokens": [50364, 6373, 13, 400, 264, 1778, 286, 1411, 6373, 390, 281, 360, 7318, 13, 1033, 13, 407, 293, 797, 11, 286, 1582, 380, 808, 322, 6373, 293, 7318, 11, 50768], "temperature": 0.0, "avg_logprob": -0.15217734888980264, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.007343174424022436}, {"id": 1397, "seek": 729384, "start": 7301.92, "end": 7308.400000000001, "text": " but at the time, right, I wanted to get into and understand and understand the technology,", "tokens": [50768, 457, 412, 264, 565, 11, 558, 11, 286, 1415, 281, 483, 666, 293, 1223, 293, 1223, 264, 2899, 11, 51092], "temperature": 0.0, "avg_logprob": -0.15217734888980264, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.007343174424022436}, {"id": 1398, "seek": 729384, "start": 7308.400000000001, "end": 7312.56, "text": " understand the applications, the workloads. And so, okay, I'm going to go dive deep into", "tokens": [51092, 1223, 264, 5821, 11, 264, 32452, 13, 400, 370, 11, 1392, 11, 286, 478, 516, 281, 352, 9192, 2452, 666, 51300], "temperature": 0.0, "avg_logprob": -0.15217734888980264, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.007343174424022436}, {"id": 1399, "seek": 729384, "start": 7312.56, "end": 7318.64, "text": " applied and AI and then the technology underneath it. Right. I found myself at Google.", "tokens": [51300, 6456, 293, 7318, 293, 550, 264, 2899, 7223, 309, 13, 1779, 13, 286, 1352, 2059, 412, 3329, 13, 51604], "temperature": 0.0, "avg_logprob": -0.15217734888980264, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.007343174424022436}, {"id": 1400, "seek": 731864, "start": 7319.280000000001, "end": 7322.56, "text": " And that was like when TPUs were waking up.", "tokens": [50396, 400, 300, 390, 411, 562, 314, 8115, 82, 645, 20447, 493, 13, 50560], "temperature": 0.0, "avg_logprob": -0.15954674414868625, "compression_ratio": 1.5807692307692307, "no_speech_prob": 0.004198112059384584}, {"id": 1401, "seek": 731864, "start": 7322.56, "end": 7329.12, "text": " Exactly. And so I found myself at Google and Jeff Dean, who's a rock star, as you know, right?", "tokens": [50560, 7587, 13, 400, 370, 286, 1352, 2059, 412, 3329, 293, 7506, 13324, 11, 567, 311, 257, 3727, 3543, 11, 382, 291, 458, 11, 558, 30, 50888], "temperature": 0.0, "avg_logprob": -0.15954674414868625, "compression_ratio": 1.5807692307692307, "no_speech_prob": 0.004198112059384584}, {"id": 1402, "seek": 731864, "start": 7329.12, "end": 7335.76, "text": " And in 2017, TensorFlow is like really taking off and doing incredible things. And I was", "tokens": [50888, 400, 294, 6591, 11, 37624, 307, 411, 534, 1940, 766, 293, 884, 4651, 721, 13, 400, 286, 390, 51220], "temperature": 0.0, "avg_logprob": -0.15954674414868625, "compression_ratio": 1.5807692307692307, "no_speech_prob": 0.004198112059384584}, {"id": 1403, "seek": 731864, "start": 7335.76, "end": 7339.76, "text": " attracted to Google to help them with the TPUs, right? And TPUs are an innovative hardware", "tokens": [51220, 15912, 281, 3329, 281, 854, 552, 365, 264, 314, 8115, 82, 11, 558, 30, 400, 314, 8115, 82, 366, 364, 12999, 8837, 51420], "temperature": 0.0, "avg_logprob": -0.15954674414868625, "compression_ratio": 1.5807692307692307, "no_speech_prob": 0.004198112059384584}, {"id": 1404, "seek": 731864, "start": 7339.76, "end": 7345.360000000001, "text": " accelerator platform have now, I mean, I think proven massive scale and like done incredible", "tokens": [51420, 39889, 3663, 362, 586, 11, 286, 914, 11, 286, 519, 12785, 5994, 4373, 293, 411, 1096, 4651, 51700], "temperature": 0.0, "avg_logprob": -0.15954674414868625, "compression_ratio": 1.5807692307692307, "no_speech_prob": 0.004198112059384584}, {"id": 1405, "seek": 734536, "start": 7345.36, "end": 7350.96, "text": " things, right? And so one of the things that this led into is a bunch of different projects,", "tokens": [50364, 721, 11, 558, 30, 400, 370, 472, 295, 264, 721, 300, 341, 4684, 666, 307, 257, 3840, 295, 819, 4455, 11, 50644], "temperature": 0.0, "avg_logprob": -0.09870236260550362, "compression_ratio": 1.83984375, "no_speech_prob": 0.0029804145451635122}, {"id": 1406, "seek": 734536, "start": 7350.96, "end": 7356.24, "text": " which I'll skip over, right? One of which was this Swift for TensorFlow project, right? And so", "tokens": [50644, 597, 286, 603, 10023, 670, 11, 558, 30, 1485, 295, 597, 390, 341, 25539, 337, 37624, 1716, 11, 558, 30, 400, 370, 50908], "temperature": 0.0, "avg_logprob": -0.09870236260550362, "compression_ratio": 1.83984375, "no_speech_prob": 0.0029804145451635122}, {"id": 1407, "seek": 734536, "start": 7356.24, "end": 7361.599999999999, "text": " that project was a research project. And so the idea of that is say, okay, well, let's look at", "tokens": [50908, 300, 1716, 390, 257, 2132, 1716, 13, 400, 370, 264, 1558, 295, 300, 307, 584, 11, 1392, 11, 731, 11, 718, 311, 574, 412, 51176], "temperature": 0.0, "avg_logprob": -0.09870236260550362, "compression_ratio": 1.83984375, "no_speech_prob": 0.0029804145451635122}, {"id": 1408, "seek": 734536, "start": 7361.599999999999, "end": 7366.32, "text": " innovative new programming models, where we can get a fast programming language, we can get", "tokens": [51176, 12999, 777, 9410, 5245, 11, 689, 321, 393, 483, 257, 2370, 9410, 2856, 11, 321, 393, 483, 51412], "temperature": 0.0, "avg_logprob": -0.09870236260550362, "compression_ratio": 1.83984375, "no_speech_prob": 0.0029804145451635122}, {"id": 1409, "seek": 734536, "start": 7367.12, "end": 7371.36, "text": " automatic differentiation into language, let's push the boundaries of these things in a research", "tokens": [51452, 12509, 38902, 666, 2856, 11, 718, 311, 2944, 264, 13180, 295, 613, 721, 294, 257, 2132, 51664], "temperature": 0.0, "avg_logprob": -0.09870236260550362, "compression_ratio": 1.83984375, "no_speech_prob": 0.0029804145451635122}, {"id": 1410, "seek": 737136, "start": 7371.36, "end": 7378.16, "text": " setting, right? Now, that project, I think lasted two, three years. There's some really cool outcomes", "tokens": [50364, 3287, 11, 558, 30, 823, 11, 300, 1716, 11, 286, 519, 21116, 732, 11, 1045, 924, 13, 821, 311, 512, 534, 1627, 10070, 50704], "temperature": 0.0, "avg_logprob": -0.12487399193548387, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.00116940145380795}, {"id": 1411, "seek": 737136, "start": 7378.16, "end": 7384.48, "text": " of that. So one of the things that's really interesting is I published a talk at an LLVM", "tokens": [50704, 295, 300, 13, 407, 472, 295, 264, 721, 300, 311, 534, 1880, 307, 286, 6572, 257, 751, 412, 364, 441, 43, 53, 44, 51020], "temperature": 0.0, "avg_logprob": -0.12487399193548387, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.00116940145380795}, {"id": 1412, "seek": 737136, "start": 7384.48, "end": 7390.48, "text": " conference in 2018. And this seems like so long ago about graph program abstraction, which is", "tokens": [51020, 7586, 294, 6096, 13, 400, 341, 2544, 411, 370, 938, 2057, 466, 4295, 1461, 37765, 11, 597, 307, 51320], "temperature": 0.0, "avg_logprob": -0.12487399193548387, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.00116940145380795}, {"id": 1413, "seek": 737136, "start": 7390.48, "end": 7395.2, "text": " basically the thing that's in PyTorch 2. And so PyTorch 2 with all this Dynamo real thing,", "tokens": [51320, 1936, 264, 551, 300, 311, 294, 9953, 51, 284, 339, 568, 13, 400, 370, 9953, 51, 284, 339, 568, 365, 439, 341, 22947, 78, 957, 551, 11, 51556], "temperature": 0.0, "avg_logprob": -0.12487399193548387, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.00116940145380795}, {"id": 1414, "seek": 737136, "start": 7395.2, "end": 7399.759999999999, "text": " it's all about this graph program abstraction thing from Python bytecodes. And so a lot of the", "tokens": [51556, 309, 311, 439, 466, 341, 4295, 1461, 37765, 551, 490, 15329, 40846, 66, 4789, 13, 400, 370, 257, 688, 295, 264, 51784], "temperature": 0.0, "avg_logprob": -0.12487399193548387, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.00116940145380795}, {"id": 1415, "seek": 739976, "start": 7399.84, "end": 7405.68, "text": " research that was done ended up pursuing and going out through the industry and influencing", "tokens": [50368, 2132, 300, 390, 1096, 4590, 493, 20222, 293, 516, 484, 807, 264, 3518, 293, 40396, 50660], "temperature": 0.0, "avg_logprob": -0.06207180898123925, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.002251439029350877}, {"id": 1416, "seek": 739976, "start": 7405.68, "end": 7410.0, "text": " things. And I think it's super exciting and awesome to see that. But the Swift for TensorFlow project", "tokens": [50660, 721, 13, 400, 286, 519, 309, 311, 1687, 4670, 293, 3476, 281, 536, 300, 13, 583, 264, 25539, 337, 37624, 1716, 50876], "temperature": 0.0, "avg_logprob": -0.06207180898123925, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.002251439029350877}, {"id": 1417, "seek": 739976, "start": 7410.0, "end": 7414.08, "text": " itself did not work out super well. And so there's a couple of different problems with that,", "tokens": [50876, 2564, 630, 406, 589, 484, 1687, 731, 13, 400, 370, 456, 311, 257, 1916, 295, 819, 2740, 365, 300, 11, 51080], "temperature": 0.0, "avg_logprob": -0.06207180898123925, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.002251439029350877}, {"id": 1418, "seek": 739976, "start": 7414.08, "end": 7420.400000000001, "text": " one of which is that you may have noticed Swift is not Python. There's a few people", "tokens": [51080, 472, 295, 597, 307, 300, 291, 815, 362, 5694, 25539, 307, 406, 15329, 13, 821, 311, 257, 1326, 561, 51396], "temperature": 0.0, "avg_logprob": -0.06207180898123925, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.002251439029350877}, {"id": 1419, "seek": 739976, "start": 7420.400000000001, "end": 7426.400000000001, "text": " that write Python code. Yes. And so it turns out that all of ML is pretty happy with Python.", "tokens": [51396, 300, 2464, 15329, 3089, 13, 1079, 13, 400, 370, 309, 4523, 484, 300, 439, 295, 21601, 307, 1238, 2055, 365, 15329, 13, 51696], "temperature": 0.0, "avg_logprob": -0.06207180898123925, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.002251439029350877}, {"id": 1420, "seek": 742640, "start": 7426.4, "end": 7430.08, "text": " It's actually a problem that other programming languages have as well,", "tokens": [50364, 467, 311, 767, 257, 1154, 300, 661, 9410, 8650, 362, 382, 731, 11, 50548], "temperature": 0.0, "avg_logprob": -0.11010557973486745, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.001048105419613421}, {"id": 1421, "seek": 742640, "start": 7430.08, "end": 7434.4, "text": " that they're not Python. We'll probably maybe briefly talk about Julia,", "tokens": [50548, 300, 436, 434, 406, 15329, 13, 492, 603, 1391, 1310, 10515, 751, 466, 18551, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11010557973486745, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.001048105419613421}, {"id": 1422, "seek": 742640, "start": 7434.4, "end": 7438.4, "text": " who's a very interesting, beautiful programming language, but it's not Python.", "tokens": [50764, 567, 311, 257, 588, 1880, 11, 2238, 9410, 2856, 11, 457, 309, 311, 406, 15329, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11010557973486745, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.001048105419613421}, {"id": 1423, "seek": 742640, "start": 7438.4, "end": 7443.599999999999, "text": " Exactly. Well, and so like if you're saying, I'm going to solve a machine learning problem", "tokens": [50964, 7587, 13, 1042, 11, 293, 370, 411, 498, 291, 434, 1566, 11, 286, 478, 516, 281, 5039, 257, 3479, 2539, 1154, 51224], "temperature": 0.0, "avg_logprob": -0.11010557973486745, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.001048105419613421}, {"id": 1424, "seek": 742640, "start": 7443.599999999999, "end": 7448.16, "text": " where all the programmers are Python programmers. And you say the first thing you have to do is", "tokens": [51224, 689, 439, 264, 41504, 366, 15329, 41504, 13, 400, 291, 584, 264, 700, 551, 291, 362, 281, 360, 307, 51452], "temperature": 0.0, "avg_logprob": -0.11010557973486745, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.001048105419613421}, {"id": 1425, "seek": 742640, "start": 7448.16, "end": 7453.2, "text": " switch to a different language. Well, your new thing may be good or bad or whatever,", "tokens": [51452, 3679, 281, 257, 819, 2856, 13, 1042, 11, 428, 777, 551, 815, 312, 665, 420, 1578, 420, 2035, 11, 51704], "temperature": 0.0, "avg_logprob": -0.11010557973486745, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.001048105419613421}, {"id": 1426, "seek": 745320, "start": 7453.2, "end": 7456.8, "text": " but if it's a new thing, the adoption barrier is massive.", "tokens": [50364, 457, 498, 309, 311, 257, 777, 551, 11, 264, 19215, 13357, 307, 5994, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11583751914775477, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.002472295891493559}, {"id": 1427, "seek": 745320, "start": 7457.44, "end": 7458.4, "text": " It's still possible.", "tokens": [50576, 467, 311, 920, 1944, 13, 50624], "temperature": 0.0, "avg_logprob": -0.11583751914775477, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.002472295891493559}, {"id": 1428, "seek": 745320, "start": 7458.4, "end": 7462.08, "text": " Still possible. Yeah, absolutely. The world changes and evolves. And there's definitely", "tokens": [50624, 8291, 1944, 13, 865, 11, 3122, 13, 440, 1002, 2962, 293, 43737, 13, 400, 456, 311, 2138, 50808], "temperature": 0.0, "avg_logprob": -0.11583751914775477, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.002472295891493559}, {"id": 1429, "seek": 745320, "start": 7462.08, "end": 7468.24, "text": " room for new and good ideas, but it just makes it so much harder. And so lesson learned,", "tokens": [50808, 1808, 337, 777, 293, 665, 3487, 11, 457, 309, 445, 1669, 309, 370, 709, 6081, 13, 400, 370, 6898, 3264, 11, 51116], "temperature": 0.0, "avg_logprob": -0.11583751914775477, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.002472295891493559}, {"id": 1430, "seek": 745320, "start": 7468.24, "end": 7472.48, "text": " Swift is not Python. And people are not always in search of learning a new thing for the sake", "tokens": [51116, 25539, 307, 406, 15329, 13, 400, 561, 366, 406, 1009, 294, 3164, 295, 2539, 257, 777, 551, 337, 264, 9717, 51328], "temperature": 0.0, "avg_logprob": -0.11583751914775477, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.002472295891493559}, {"id": 1431, "seek": 745320, "start": 7472.48, "end": 7476.0, "text": " of learning a new thing. And if you want to be compatible with all the world's code, it turns", "tokens": [51328, 295, 2539, 257, 777, 551, 13, 400, 498, 291, 528, 281, 312, 18218, 365, 439, 264, 1002, 311, 3089, 11, 309, 4523, 51504], "temperature": 0.0, "avg_logprob": -0.11583751914775477, "compression_ratio": 1.6973180076628354, "no_speech_prob": 0.002472295891493559}, {"id": 1432, "seek": 747600, "start": 7476.0, "end": 7483.2, "text": " out, meet the world where it is. Right. Second thing is that, you know, a lesson learned is that", "tokens": [50364, 484, 11, 1677, 264, 1002, 689, 309, 307, 13, 1779, 13, 5736, 551, 307, 300, 11, 291, 458, 11, 257, 6898, 3264, 307, 300, 50724], "temperature": 0.0, "avg_logprob": -0.13031014990299306, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.028427381068468094}, {"id": 1433, "seek": 747600, "start": 7484.08, "end": 7489.92, "text": " Swift as a very fast and efficient language, kind of like Mojo, but a different take on it still", "tokens": [50768, 25539, 382, 257, 588, 2370, 293, 7148, 2856, 11, 733, 295, 411, 3335, 5134, 11, 457, 257, 819, 747, 322, 309, 920, 51060], "temperature": 0.0, "avg_logprob": -0.13031014990299306, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.028427381068468094}, {"id": 1434, "seek": 747600, "start": 7491.92, "end": 7497.2, "text": " really worked well with eager mode. And so eager mode is something that PyTorch does,", "tokens": [51160, 534, 2732, 731, 365, 18259, 4391, 13, 400, 370, 18259, 4391, 307, 746, 300, 9953, 51, 284, 339, 775, 11, 51424], "temperature": 0.0, "avg_logprob": -0.13031014990299306, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.028427381068468094}, {"id": 1435, "seek": 747600, "start": 7497.2, "end": 7503.44, "text": " and it proved out really well, and it enables really expressive and dynamic and easy to debug", "tokens": [51424, 293, 309, 14617, 484, 534, 731, 11, 293, 309, 17077, 534, 40189, 293, 8546, 293, 1858, 281, 24083, 51736], "temperature": 0.0, "avg_logprob": -0.13031014990299306, "compression_ratio": 1.5872340425531914, "no_speech_prob": 0.028427381068468094}, {"id": 1436, "seek": 750344, "start": 7503.5199999999995, "end": 7507.2, "text": " programming. TensorFlow at the time was not set up for that.", "tokens": [50368, 9410, 13, 37624, 412, 264, 565, 390, 406, 992, 493, 337, 300, 13, 50552], "temperature": 0.0, "avg_logprob": -0.15753052185992805, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.014059356413781643}, {"id": 1437, "seek": 750344, "start": 7508.32, "end": 7511.599999999999, "text": " Let's say that was not the timing is also important in this world.", "tokens": [50608, 961, 311, 584, 300, 390, 406, 264, 10822, 307, 611, 1021, 294, 341, 1002, 13, 50772], "temperature": 0.0, "avg_logprob": -0.15753052185992805, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.014059356413781643}, {"id": 1438, "seek": 750344, "start": 7511.599999999999, "end": 7516.24, "text": " Yeah. Yeah. And TensorFlow is a good thing and it has many, many strengths, but", "tokens": [50772, 865, 13, 865, 13, 400, 37624, 307, 257, 665, 551, 293, 309, 575, 867, 11, 867, 16986, 11, 457, 51004], "temperature": 0.0, "avg_logprob": -0.15753052185992805, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.014059356413781643}, {"id": 1439, "seek": 750344, "start": 7518.32, "end": 7521.759999999999, "text": " you could say Swift for TensorFlow is a good idea, except for the Swift and except for the", "tokens": [51108, 291, 727, 584, 25539, 337, 37624, 307, 257, 665, 1558, 11, 3993, 337, 264, 25539, 293, 3993, 337, 264, 51280], "temperature": 0.0, "avg_logprob": -0.15753052185992805, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.014059356413781643}, {"id": 1440, "seek": 750344, "start": 7521.759999999999, "end": 7528.08, "text": " TensorFlow part. So because it's not Python and TensorFlow because it's not wasn't set up for", "tokens": [51280, 37624, 644, 13, 407, 570, 309, 311, 406, 15329, 293, 37624, 570, 309, 311, 406, 2067, 380, 992, 493, 337, 51596], "temperature": 0.0, "avg_logprob": -0.15753052185992805, "compression_ratio": 1.8578199052132702, "no_speech_prob": 0.014059356413781643}, {"id": 1441, "seek": 752808, "start": 7528.4, "end": 7534.72, "text": " eager mode at the time. Yeah. That is 1.0. Exactly. And so one of the things about that is in the", "tokens": [50380, 18259, 4391, 412, 264, 565, 13, 865, 13, 663, 307, 502, 13, 15, 13, 7587, 13, 400, 370, 472, 295, 264, 721, 466, 300, 307, 294, 264, 50696], "temperature": 0.0, "avg_logprob": -0.11697012541309887, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.046000219881534576}, {"id": 1442, "seek": 752808, "start": 7534.72, "end": 7539.6, "text": " context of it being a research project, I'm very happy with the fact that we built a lot of really", "tokens": [50696, 4319, 295, 309, 885, 257, 2132, 1716, 11, 286, 478, 588, 2055, 365, 264, 1186, 300, 321, 3094, 257, 688, 295, 534, 50940], "temperature": 0.0, "avg_logprob": -0.11697012541309887, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.046000219881534576}, {"id": 1443, "seek": 752808, "start": 7539.6, "end": 7543.44, "text": " cool technology. We learned a lot of things. I think the ideas went on to have influence", "tokens": [50940, 1627, 2899, 13, 492, 3264, 257, 688, 295, 721, 13, 286, 519, 264, 3487, 1437, 322, 281, 362, 6503, 51132], "temperature": 0.0, "avg_logprob": -0.11697012541309887, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.046000219881534576}, {"id": 1444, "seek": 752808, "start": 7543.44, "end": 7547.36, "text": " and other systems like PyTorch. A few people use that right here. Right. And so I think that's", "tokens": [51132, 293, 661, 3652, 411, 9953, 51, 284, 339, 13, 316, 1326, 561, 764, 300, 558, 510, 13, 1779, 13, 400, 370, 286, 519, 300, 311, 51328], "temperature": 0.0, "avg_logprob": -0.11697012541309887, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.046000219881534576}, {"id": 1445, "seek": 752808, "start": 7547.36, "end": 7552.32, "text": " super cool. And for me personally, I learned so much from it. Right. And I think a lot of the", "tokens": [51328, 1687, 1627, 13, 400, 337, 385, 5665, 11, 286, 3264, 370, 709, 490, 309, 13, 1779, 13, 400, 286, 519, 257, 688, 295, 264, 51576], "temperature": 0.0, "avg_logprob": -0.11697012541309887, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.046000219881534576}, {"id": 1446, "seek": 752808, "start": 7552.32, "end": 7556.5599999999995, "text": " engineers that worked on it also learned a tremendous amount. And so, you know, I think that", "tokens": [51576, 11955, 300, 2732, 322, 309, 611, 3264, 257, 10048, 2372, 13, 400, 370, 11, 291, 458, 11, 286, 519, 300, 51788], "temperature": 0.0, "avg_logprob": -0.11697012541309887, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.046000219881534576}, {"id": 1447, "seek": 755656, "start": 7557.4400000000005, "end": 7561.6, "text": " that's just really exciting to see. And, you know, I'm sorry that the project didn't work out. I", "tokens": [50408, 300, 311, 445, 534, 4670, 281, 536, 13, 400, 11, 291, 458, 11, 286, 478, 2597, 300, 264, 1716, 994, 380, 589, 484, 13, 286, 50616], "temperature": 0.0, "avg_logprob": -0.1110551016671317, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.0006557588931173086}, {"id": 1448, "seek": 755656, "start": 7561.6, "end": 7568.64, "text": " wish it did, of course. Right. But, you know, it's a research project and so you're there to learn", "tokens": [50616, 3172, 309, 630, 11, 295, 1164, 13, 1779, 13, 583, 11, 291, 458, 11, 309, 311, 257, 2132, 1716, 293, 370, 291, 434, 456, 281, 1466, 50968], "temperature": 0.0, "avg_logprob": -0.1110551016671317, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.0006557588931173086}, {"id": 1449, "seek": 755656, "start": 7568.64, "end": 7576.72, "text": " from it. Well, it's interesting to think about the evolution of programming as we come up with these", "tokens": [50968, 490, 309, 13, 1042, 11, 309, 311, 1880, 281, 519, 466, 264, 9303, 295, 9410, 382, 321, 808, 493, 365, 613, 51372], "temperature": 0.0, "avg_logprob": -0.1110551016671317, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.0006557588931173086}, {"id": 1450, "seek": 755656, "start": 7577.4400000000005, "end": 7582.88, "text": " whole new set of algorithms in machine learning and artificial intelligence and what's going to", "tokens": [51408, 1379, 777, 992, 295, 14642, 294, 3479, 2539, 293, 11677, 7599, 293, 437, 311, 516, 281, 51680], "temperature": 0.0, "avg_logprob": -0.1110551016671317, "compression_ratio": 1.6265560165975104, "no_speech_prob": 0.0006557588931173086}, {"id": 1451, "seek": 758288, "start": 7582.88, "end": 7589.28, "text": " win out. Because it could be a new programming language. Yeah. It could be, I mean, I just", "tokens": [50364, 1942, 484, 13, 1436, 309, 727, 312, 257, 777, 9410, 2856, 13, 865, 13, 467, 727, 312, 11, 286, 914, 11, 286, 445, 50684], "temperature": 0.0, "avg_logprob": -0.10542890003749303, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.009262862615287304}, {"id": 1452, "seek": 758288, "start": 7589.28, "end": 7598.0, "text": " mentioned Julia. I think there's a lot of ideas behind Julia that Mojo shares. What are your", "tokens": [50684, 2835, 18551, 13, 286, 519, 456, 311, 257, 688, 295, 3487, 2261, 18551, 300, 3335, 5134, 12182, 13, 708, 366, 428, 51120], "temperature": 0.0, "avg_logprob": -0.10542890003749303, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.009262862615287304}, {"id": 1453, "seek": 758288, "start": 7598.0, "end": 7605.36, "text": " thoughts about Julia in general? So, I will have to say that when we launched Mojo, one of the", "tokens": [51120, 4598, 466, 18551, 294, 2674, 30, 407, 11, 286, 486, 362, 281, 584, 300, 562, 321, 8730, 3335, 5134, 11, 472, 295, 264, 51488], "temperature": 0.0, "avg_logprob": -0.10542890003749303, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.009262862615287304}, {"id": 1454, "seek": 758288, "start": 7605.36, "end": 7611.12, "text": " biggest things I didn't predict was the response from the Julia community. And so, I was not,", "tokens": [51488, 3880, 721, 286, 994, 380, 6069, 390, 264, 4134, 490, 264, 18551, 1768, 13, 400, 370, 11, 286, 390, 406, 11, 51776], "temperature": 0.0, "avg_logprob": -0.10542890003749303, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.009262862615287304}, {"id": 1455, "seek": 761112, "start": 7611.12, "end": 7616.0, "text": " I mean, I've, okay, let me take a step back. I've known the Julia folks for a really long time.", "tokens": [50364, 286, 914, 11, 286, 600, 11, 1392, 11, 718, 385, 747, 257, 1823, 646, 13, 286, 600, 2570, 264, 18551, 4024, 337, 257, 534, 938, 565, 13, 50608], "temperature": 0.0, "avg_logprob": -0.12228085814403887, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0019871708936989307}, {"id": 1456, "seek": 761112, "start": 7616.0, "end": 7621.04, "text": " They were an adopter of LLVM a long time ago. They've been pushing state of the art in a bunch", "tokens": [50608, 814, 645, 364, 22486, 391, 295, 441, 43, 53, 44, 257, 938, 565, 2057, 13, 814, 600, 668, 7380, 1785, 295, 264, 1523, 294, 257, 3840, 50860], "temperature": 0.0, "avg_logprob": -0.12228085814403887, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0019871708936989307}, {"id": 1457, "seek": 761112, "start": 7621.04, "end": 7626.64, "text": " of different ways. Julia is a really cool system. I had always thought of Julia as being mostly a", "tokens": [50860, 295, 819, 2098, 13, 18551, 307, 257, 534, 1627, 1185, 13, 286, 632, 1009, 1194, 295, 18551, 382, 885, 5240, 257, 51140], "temperature": 0.0, "avg_logprob": -0.12228085814403887, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0019871708936989307}, {"id": 1458, "seek": 761112, "start": 7626.64, "end": 7634.48, "text": " scientific computing focused environment. And I thought that was its focus. I neglected to", "tokens": [51140, 8134, 15866, 5178, 2823, 13, 400, 286, 1194, 300, 390, 1080, 1879, 13, 286, 32701, 281, 51532], "temperature": 0.0, "avg_logprob": -0.12228085814403887, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0019871708936989307}, {"id": 1459, "seek": 761112, "start": 7634.48, "end": 7639.2, "text": " understand that one of their missions is to like help make Python work end to end.", "tokens": [51532, 1223, 300, 472, 295, 641, 13744, 307, 281, 411, 854, 652, 15329, 589, 917, 281, 917, 13, 51768], "temperature": 0.0, "avg_logprob": -0.12228085814403887, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.0019871708936989307}, {"id": 1460, "seek": 763920, "start": 7640.16, "end": 7644.0, "text": " And so, I think that was my error for not understanding that. And so, I could have been", "tokens": [50412, 400, 370, 11, 286, 519, 300, 390, 452, 6713, 337, 406, 3701, 300, 13, 400, 370, 11, 286, 727, 362, 668, 50604], "temperature": 0.0, "avg_logprob": -0.08987951686239651, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.0010648315073922276}, {"id": 1461, "seek": 763920, "start": 7644.0, "end": 7648.88, "text": " maybe more sensitive to that. But there's major differences between what Mojo is doing", "tokens": [50604, 1310, 544, 9477, 281, 300, 13, 583, 456, 311, 2563, 7300, 1296, 437, 3335, 5134, 307, 884, 50848], "temperature": 0.0, "avg_logprob": -0.08987951686239651, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.0010648315073922276}, {"id": 1462, "seek": 763920, "start": 7648.88, "end": 7654.24, "text": " and what Julia is doing. So, as you say, Julia is not Python. And so, one of the things that", "tokens": [50848, 293, 437, 18551, 307, 884, 13, 407, 11, 382, 291, 584, 11, 18551, 307, 406, 15329, 13, 400, 370, 11, 472, 295, 264, 721, 300, 51116], "temperature": 0.0, "avg_logprob": -0.08987951686239651, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.0010648315073922276}, {"id": 1463, "seek": 763920, "start": 7654.8, "end": 7660.0, "text": " a lot of the Julia people came out and said is like, okay, well, if we put a ton of more energy", "tokens": [51144, 257, 688, 295, 264, 18551, 561, 1361, 484, 293, 848, 307, 411, 11, 1392, 11, 731, 11, 498, 321, 829, 257, 2952, 295, 544, 2281, 51404], "temperature": 0.0, "avg_logprob": -0.08987951686239651, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.0010648315073922276}, {"id": 1464, "seek": 763920, "start": 7660.0, "end": 7666.08, "text": " and ton more money or engineering or whatever into Julia, maybe that would be better than", "tokens": [51404, 293, 2952, 544, 1460, 420, 7043, 420, 2035, 666, 18551, 11, 1310, 300, 576, 312, 1101, 813, 51708], "temperature": 0.0, "avg_logprob": -0.08987951686239651, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.0010648315073922276}, {"id": 1465, "seek": 766608, "start": 7666.16, "end": 7671.36, "text": " starting Mojo, right? Well, I mean, maybe that's true, but it still wouldn't make Julia into Python.", "tokens": [50368, 2891, 3335, 5134, 11, 558, 30, 1042, 11, 286, 914, 11, 1310, 300, 311, 2074, 11, 457, 309, 920, 2759, 380, 652, 18551, 666, 15329, 13, 50628], "temperature": 0.0, "avg_logprob": -0.07473457336425782, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0035933132749050856}, {"id": 1466, "seek": 766608, "start": 7672.4, "end": 7677.28, "text": " So, if you've worked backwards from the goal of let's build something for Python programmers", "tokens": [50680, 407, 11, 498, 291, 600, 2732, 12204, 490, 264, 3387, 295, 718, 311, 1322, 746, 337, 15329, 41504, 50924], "temperature": 0.0, "avg_logprob": -0.07473457336425782, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0035933132749050856}, {"id": 1467, "seek": 766608, "start": 7677.28, "end": 7684.48, "text": " without requiring them to relearn syntax, then Julia just isn't there, right? I mean,", "tokens": [50924, 1553, 24165, 552, 281, 2951, 1083, 28431, 11, 550, 18551, 445, 1943, 380, 456, 11, 558, 30, 286, 914, 11, 51284], "temperature": 0.0, "avg_logprob": -0.07473457336425782, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0035933132749050856}, {"id": 1468, "seek": 766608, "start": 7684.48, "end": 7689.92, "text": " that's a different thing, right? And so, if you anchor on, I love Julia and I want Julia to go", "tokens": [51284, 300, 311, 257, 819, 551, 11, 558, 30, 400, 370, 11, 498, 291, 18487, 322, 11, 286, 959, 18551, 293, 286, 528, 18551, 281, 352, 51556], "temperature": 0.0, "avg_logprob": -0.07473457336425782, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0035933132749050856}, {"id": 1469, "seek": 766608, "start": 7689.92, "end": 7694.8, "text": " further, then you can look at it from a different lens. But the lens we were coming at was, hey,", "tokens": [51556, 3052, 11, 550, 291, 393, 574, 412, 309, 490, 257, 819, 6765, 13, 583, 264, 6765, 321, 645, 1348, 412, 390, 11, 4177, 11, 51800], "temperature": 0.0, "avg_logprob": -0.07473457336425782, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0035933132749050856}, {"id": 1470, "seek": 769480, "start": 7694.8, "end": 7700.16, "text": " everybody is using Python. Python isn't, syntax isn't broken. Let's take what's great about Python", "tokens": [50364, 2201, 307, 1228, 15329, 13, 15329, 1943, 380, 11, 28431, 1943, 380, 5463, 13, 961, 311, 747, 437, 311, 869, 466, 15329, 50632], "temperature": 0.0, "avg_logprob": -0.1056575524179559, "compression_ratio": 1.675, "no_speech_prob": 0.0013247514143586159}, {"id": 1471, "seek": 769480, "start": 7700.16, "end": 7704.0, "text": " and make it even better. And so, it's just a different starting point. So, I think Julia is", "tokens": [50632, 293, 652, 309, 754, 1101, 13, 400, 370, 11, 309, 311, 445, 257, 819, 2891, 935, 13, 407, 11, 286, 519, 18551, 307, 50824], "temperature": 0.0, "avg_logprob": -0.1056575524179559, "compression_ratio": 1.675, "no_speech_prob": 0.0013247514143586159}, {"id": 1472, "seek": 769480, "start": 7704.0, "end": 7707.4400000000005, "text": " a great language. The community is a lovely community. They're doing really cool stuff,", "tokens": [50824, 257, 869, 2856, 13, 440, 1768, 307, 257, 7496, 1768, 13, 814, 434, 884, 534, 1627, 1507, 11, 50996], "temperature": 0.0, "avg_logprob": -0.1056575524179559, "compression_ratio": 1.675, "no_speech_prob": 0.0013247514143586159}, {"id": 1473, "seek": 769480, "start": 7707.4400000000005, "end": 7713.84, "text": " but it's just a slightly different angle. But it does seem that Python is quite sticky. Is there", "tokens": [50996, 457, 309, 311, 445, 257, 4748, 819, 5802, 13, 583, 309, 775, 1643, 300, 15329, 307, 1596, 14470, 13, 1119, 456, 51316], "temperature": 0.0, "avg_logprob": -0.1056575524179559, "compression_ratio": 1.675, "no_speech_prob": 0.0013247514143586159}, {"id": 1474, "seek": 769480, "start": 7713.84, "end": 7720.56, "text": " some philosophical almost thing you could say about why Python, by many measures, seems to be", "tokens": [51316, 512, 25066, 1920, 551, 291, 727, 584, 466, 983, 15329, 11, 538, 867, 8000, 11, 2544, 281, 312, 51652], "temperature": 0.0, "avg_logprob": -0.1056575524179559, "compression_ratio": 1.675, "no_speech_prob": 0.0013247514143586159}, {"id": 1475, "seek": 772056, "start": 7720.56, "end": 7724.8, "text": " the most popular programming language in the world? Well, I can tell you things I love about it.", "tokens": [50364, 264, 881, 3743, 9410, 2856, 294, 264, 1002, 30, 1042, 11, 286, 393, 980, 291, 721, 286, 959, 466, 309, 13, 50576], "temperature": 0.0, "avg_logprob": -0.16756165412164503, "compression_ratio": 1.619205298013245, "no_speech_prob": 0.046007562428712845}, {"id": 1476, "seek": 772056, "start": 7724.8, "end": 7730.4800000000005, "text": " Maybe that's one way to answer the question, right? So, huge package ecosystem. Super lightweight and", "tokens": [50576, 2704, 300, 311, 472, 636, 281, 1867, 264, 1168, 11, 558, 30, 407, 11, 2603, 7372, 11311, 13, 4548, 22052, 293, 50860], "temperature": 0.0, "avg_logprob": -0.16756165412164503, "compression_ratio": 1.619205298013245, "no_speech_prob": 0.046007562428712845}, {"id": 1477, "seek": 772056, "start": 7730.4800000000005, "end": 7736.72, "text": " easy to integrate. It has very low startup time. So, what startup time? You mean like learning curve", "tokens": [50860, 1858, 281, 13365, 13, 467, 575, 588, 2295, 18578, 565, 13, 407, 11, 437, 18578, 565, 30, 509, 914, 411, 2539, 7605, 51172], "temperature": 0.0, "avg_logprob": -0.16756165412164503, "compression_ratio": 1.619205298013245, "no_speech_prob": 0.046007562428712845}, {"id": 1478, "seek": 772056, "start": 7736.72, "end": 7742.080000000001, "text": " or what? Yeah, so, if you look at certain other languages, you say like, go. And it just takes", "tokens": [51172, 420, 437, 30, 865, 11, 370, 11, 498, 291, 574, 412, 1629, 661, 8650, 11, 291, 584, 411, 11, 352, 13, 400, 309, 445, 2516, 51440], "temperature": 0.0, "avg_logprob": -0.16756165412164503, "compression_ratio": 1.619205298013245, "no_speech_prob": 0.046007562428712845}, {"id": 1479, "seek": 772056, "start": 7742.080000000001, "end": 7747.84, "text": " a, like Java, for example, takes a long time to compile all the things. And then the VM starts", "tokens": [51440, 257, 11, 411, 10745, 11, 337, 1365, 11, 2516, 257, 938, 565, 281, 31413, 439, 264, 721, 13, 400, 550, 264, 18038, 3719, 51728], "temperature": 0.0, "avg_logprob": -0.16756165412164503, "compression_ratio": 1.619205298013245, "no_speech_prob": 0.046007562428712845}, {"id": 1480, "seek": 774784, "start": 7747.84, "end": 7751.4400000000005, "text": " up and the garbage clusters kicks in and then it revs its engines and then it can plow through a", "tokens": [50364, 493, 293, 264, 14150, 23313, 21293, 294, 293, 550, 309, 3698, 82, 1080, 12982, 293, 550, 309, 393, 499, 305, 807, 257, 50544], "temperature": 0.0, "avg_logprob": -0.12365808740126348, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.006902031134814024}, {"id": 1481, "seek": 774784, "start": 7751.4400000000005, "end": 7757.84, "text": " lot of internet stuff or whatever, right? Python is like scripting. It just goes, right? Python", "tokens": [50544, 688, 295, 4705, 1507, 420, 2035, 11, 558, 30, 15329, 307, 411, 5755, 278, 13, 467, 445, 1709, 11, 558, 30, 15329, 50864], "temperature": 0.0, "avg_logprob": -0.12365808740126348, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.006902031134814024}, {"id": 1482, "seek": 774784, "start": 7757.84, "end": 7762.24, "text": " has very low compile time. So, you're not sitting there waiting. Python integrates into notebooks", "tokens": [50864, 575, 588, 2295, 31413, 565, 13, 407, 11, 291, 434, 406, 3798, 456, 3806, 13, 15329, 3572, 1024, 666, 43782, 51084], "temperature": 0.0, "avg_logprob": -0.12365808740126348, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.006902031134814024}, {"id": 1483, "seek": 774784, "start": 7762.24, "end": 7768.4800000000005, "text": " in a very elegant way that makes exploration super interactive and it's awesome, right? Python is also", "tokens": [51084, 294, 257, 588, 21117, 636, 300, 1669, 16197, 1687, 15141, 293, 309, 311, 3476, 11, 558, 30, 15329, 307, 611, 51396], "temperature": 0.0, "avg_logprob": -0.12365808740126348, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.006902031134814024}, {"id": 1484, "seek": 774784, "start": 7769.12, "end": 7775.360000000001, "text": " it's like almost the glue of computing because it has such a simple object representation,", "tokens": [51428, 309, 311, 411, 1920, 264, 8998, 295, 15866, 570, 309, 575, 1270, 257, 2199, 2657, 10290, 11, 51740], "temperature": 0.0, "avg_logprob": -0.12365808740126348, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.006902031134814024}, {"id": 1485, "seek": 777536, "start": 7775.36, "end": 7779.5199999999995, "text": " a lot of things plug into it. That dynamic metaprogramming thing we were talking about also", "tokens": [50364, 257, 688, 295, 721, 5452, 666, 309, 13, 663, 8546, 1131, 569, 340, 1342, 2810, 551, 321, 645, 1417, 466, 611, 50572], "temperature": 0.0, "avg_logprob": -0.09128128321824876, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0021153842099010944}, {"id": 1486, "seek": 777536, "start": 7779.5199999999995, "end": 7783.759999999999, "text": " enables really expressive and beautiful APIs, right? So, there's lots of reasons that you can", "tokens": [50572, 17077, 534, 40189, 293, 2238, 21445, 11, 558, 30, 407, 11, 456, 311, 3195, 295, 4112, 300, 291, 393, 50784], "temperature": 0.0, "avg_logprob": -0.09128128321824876, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0021153842099010944}, {"id": 1487, "seek": 777536, "start": 7784.4, "end": 7789.5199999999995, "text": " look at technical things that Python has done and say like, okay, wow, this is actually a pretty", "tokens": [50816, 574, 412, 6191, 721, 300, 15329, 575, 1096, 293, 584, 411, 11, 1392, 11, 6076, 11, 341, 307, 767, 257, 1238, 51072], "temperature": 0.0, "avg_logprob": -0.09128128321824876, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0021153842099010944}, {"id": 1488, "seek": 777536, "start": 7789.5199999999995, "end": 7796.32, "text": " amazing thing and any one of those you can neglect. People all just talk about indentation and ignore", "tokens": [51072, 2243, 551, 293, 604, 472, 295, 729, 291, 393, 17745, 13, 3432, 439, 445, 751, 466, 44494, 399, 293, 11200, 51412], "temperature": 0.0, "avg_logprob": -0.09128128321824876, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0021153842099010944}, {"id": 1489, "seek": 777536, "start": 7796.32, "end": 7800.88, "text": " like the fundamental things. But then you also look at the community side, right? So, Python", "tokens": [51412, 411, 264, 8088, 721, 13, 583, 550, 291, 611, 574, 412, 264, 1768, 1252, 11, 558, 30, 407, 11, 15329, 51640], "temperature": 0.0, "avg_logprob": -0.09128128321824876, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.0021153842099010944}, {"id": 1490, "seek": 780088, "start": 7800.88, "end": 7805.76, "text": " owns machine learning. Machine learning is pretty big. Yeah, and it's growing. It's growing, right?", "tokens": [50364, 19143, 3479, 2539, 13, 22155, 2539, 307, 1238, 955, 13, 865, 11, 293, 309, 311, 4194, 13, 467, 311, 4194, 11, 558, 30, 50608], "temperature": 0.0, "avg_logprob": -0.16453047359690948, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.07156834006309509}, {"id": 1491, "seek": 780088, "start": 7805.76, "end": 7809.76, "text": " And it's growing in importance, right? And so, and there's a reputation of prestige to machine", "tokens": [50608, 400, 309, 311, 4194, 294, 7379, 11, 558, 30, 400, 370, 11, 293, 456, 311, 257, 13061, 295, 42531, 281, 3479, 50808], "temperature": 0.0, "avg_logprob": -0.16453047359690948, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.07156834006309509}, {"id": 1492, "seek": 780088, "start": 7809.76, "end": 7813.68, "text": " learning to where like, if you're a new programmer, you're thinking about like,", "tokens": [50808, 2539, 281, 689, 411, 11, 498, 291, 434, 257, 777, 32116, 11, 291, 434, 1953, 466, 411, 11, 51004], "temperature": 0.0, "avg_logprob": -0.16453047359690948, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.07156834006309509}, {"id": 1493, "seek": 780088, "start": 7814.4800000000005, "end": 7818.96, "text": " which programming language do I use? Well, I should probably care about machine learning. Therefore,", "tokens": [51044, 597, 9410, 2856, 360, 286, 764, 30, 1042, 11, 286, 820, 1391, 1127, 466, 3479, 2539, 13, 7504, 11, 51268], "temperature": 0.0, "avg_logprob": -0.16453047359690948, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.07156834006309509}, {"id": 1494, "seek": 780088, "start": 7818.96, "end": 7824.16, "text": " let me try Python and kind of builds and builds and builds. And you can go back before that,", "tokens": [51268, 718, 385, 853, 15329, 293, 733, 295, 15182, 293, 15182, 293, 15182, 13, 400, 291, 393, 352, 646, 949, 300, 11, 51528], "temperature": 0.0, "avg_logprob": -0.16453047359690948, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.07156834006309509}, {"id": 1495, "seek": 780088, "start": 7824.16, "end": 7829.6, "text": " like my kids learn Python, right? Not because I'm telling them to learn Python, but because", "tokens": [51528, 411, 452, 2301, 1466, 15329, 11, 558, 30, 1726, 570, 286, 478, 3585, 552, 281, 1466, 15329, 11, 457, 570, 51800], "temperature": 0.0, "avg_logprob": -0.16453047359690948, "compression_ratio": 1.951219512195122, "no_speech_prob": 0.07156834006309509}, {"id": 1496, "seek": 782960, "start": 7829.68, "end": 7833.76, "text": " what they were buying against you or what? Well, no, right? Well, they also learned Scratch,", "tokens": [50368, 437, 436, 645, 6382, 1970, 291, 420, 437, 30, 1042, 11, 572, 11, 558, 30, 1042, 11, 436, 611, 3264, 34944, 852, 11, 50572], "temperature": 0.0, "avg_logprob": -0.1665398712158203, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0032725525088608265}, {"id": 1497, "seek": 782960, "start": 7833.76, "end": 7837.76, "text": " right? And things like this too. But it's because Python is taught everywhere, right? Because it's", "tokens": [50572, 558, 30, 400, 721, 411, 341, 886, 13, 583, 309, 311, 570, 15329, 307, 5928, 5315, 11, 558, 30, 1436, 309, 311, 50772], "temperature": 0.0, "avg_logprob": -0.1665398712158203, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0032725525088608265}, {"id": 1498, "seek": 782960, "start": 7837.76, "end": 7843.68, "text": " easy to learn, right? And because it's pervasive, right? Back to my day, we learned Java and C++.", "tokens": [50772, 1858, 281, 1466, 11, 558, 30, 400, 570, 309, 311, 680, 39211, 11, 558, 30, 5833, 281, 452, 786, 11, 321, 3264, 10745, 293, 383, 25472, 13, 51068], "temperature": 0.0, "avg_logprob": -0.1665398712158203, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0032725525088608265}, {"id": 1499, "seek": 782960, "start": 7845.6, "end": 7850.8, "text": " I'll pale both directions. But yes, I guess Python is the main language of teaching software", "tokens": [51164, 286, 603, 19546, 1293, 11095, 13, 583, 2086, 11, 286, 2041, 15329, 307, 264, 2135, 2856, 295, 4571, 4722, 51424], "temperature": 0.0, "avg_logprob": -0.1665398712158203, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0032725525088608265}, {"id": 1500, "seek": 782960, "start": 7850.8, "end": 7855.92, "text": " engineering in schools now. Yeah. Well, and if you look at this, there's these growth cycles,", "tokens": [51424, 7043, 294, 4656, 586, 13, 865, 13, 1042, 11, 293, 498, 291, 574, 412, 341, 11, 456, 311, 613, 4599, 17796, 11, 51680], "temperature": 0.0, "avg_logprob": -0.1665398712158203, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.0032725525088608265}, {"id": 1501, "seek": 785592, "start": 7856.16, "end": 7860.8, "text": " right? If you look at what causes things to become popular, and then gain in popularity,", "tokens": [50376, 558, 30, 759, 291, 574, 412, 437, 7700, 721, 281, 1813, 3743, 11, 293, 550, 6052, 294, 19301, 11, 50608], "temperature": 0.0, "avg_logprob": -0.0987783518704501, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.00029593947692774236}, {"id": 1502, "seek": 785592, "start": 7860.8, "end": 7864.88, "text": " there's reinforcing feedback loops and things like this. And I think Python has done,", "tokens": [50608, 456, 311, 48262, 5824, 16121, 293, 721, 411, 341, 13, 400, 286, 519, 15329, 575, 1096, 11, 50812], "temperature": 0.0, "avg_logprob": -0.0987783518704501, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.00029593947692774236}, {"id": 1503, "seek": 785592, "start": 7864.88, "end": 7868.72, "text": " again, the whole community has done a really good job of building those growth loops and help", "tokens": [50812, 797, 11, 264, 1379, 1768, 575, 1096, 257, 534, 665, 1691, 295, 2390, 729, 4599, 16121, 293, 854, 51004], "temperature": 0.0, "avg_logprob": -0.0987783518704501, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.00029593947692774236}, {"id": 1504, "seek": 785592, "start": 7868.72, "end": 7872.72, "text": " propel the ecosystem. And I think that again, you look at what you can get done with just a few", "tokens": [51004, 2365, 338, 264, 11311, 13, 400, 286, 519, 300, 797, 11, 291, 574, 412, 437, 291, 393, 483, 1096, 365, 445, 257, 1326, 51204], "temperature": 0.0, "avg_logprob": -0.0987783518704501, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.00029593947692774236}, {"id": 1505, "seek": 785592, "start": 7872.72, "end": 7880.72, "text": " lines of code, it's amazing. So this kind of self building loop, it's interesting to understand", "tokens": [51204, 3876, 295, 3089, 11, 309, 311, 2243, 13, 407, 341, 733, 295, 2698, 2390, 6367, 11, 309, 311, 1880, 281, 1223, 51604], "temperature": 0.0, "avg_logprob": -0.0987783518704501, "compression_ratio": 1.7228464419475655, "no_speech_prob": 0.00029593947692774236}, {"id": 1506, "seek": 788072, "start": 7880.72, "end": 7887.4400000000005, "text": " because when you look at Mojo, what it stands for some of the features, it seems sort of clear that", "tokens": [50364, 570, 562, 291, 574, 412, 3335, 5134, 11, 437, 309, 7382, 337, 512, 295, 264, 4122, 11, 309, 2544, 1333, 295, 1850, 300, 50700], "temperature": 0.0, "avg_logprob": -0.12302200483239216, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.014487490057945251}, {"id": 1507, "seek": 788072, "start": 7888.08, "end": 7892.88, "text": " this is a good direction for programming languages to evolve in the machine learning community.", "tokens": [50732, 341, 307, 257, 665, 3513, 337, 9410, 8650, 281, 16693, 294, 264, 3479, 2539, 1768, 13, 50972], "temperature": 0.0, "avg_logprob": -0.12302200483239216, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.014487490057945251}, {"id": 1508, "seek": 788072, "start": 7893.4400000000005, "end": 7899.280000000001, "text": " But it's still not obvious that it will, because of this, whatever the engine of popularity,", "tokens": [51000, 583, 309, 311, 920, 406, 6322, 300, 309, 486, 11, 570, 295, 341, 11, 2035, 264, 2848, 295, 19301, 11, 51292], "temperature": 0.0, "avg_logprob": -0.12302200483239216, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.014487490057945251}, {"id": 1509, "seek": 788072, "start": 7899.280000000001, "end": 7905.52, "text": " of virality, is there something you could speak to like how, how do you get people to switch?", "tokens": [51292, 295, 4107, 1860, 11, 307, 456, 746, 291, 727, 1710, 281, 411, 577, 11, 577, 360, 291, 483, 561, 281, 3679, 30, 51604], "temperature": 0.0, "avg_logprob": -0.12302200483239216, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.014487490057945251}, {"id": 1510, "seek": 788072, "start": 7905.52, "end": 7910.08, "text": " Yeah, well, I mean, I think that the viral growth loop is to switch people to Unicode.", "tokens": [51604, 865, 11, 731, 11, 286, 914, 11, 286, 519, 300, 264, 16132, 4599, 6367, 307, 281, 3679, 561, 281, 1156, 299, 1429, 13, 51832], "temperature": 0.0, "avg_logprob": -0.12302200483239216, "compression_ratio": 1.6514084507042253, "no_speech_prob": 0.014487490057945251}, {"id": 1511, "seek": 791072, "start": 7911.12, "end": 7914.400000000001, "text": " I think the Unicode file extensions are what I'm betting on. I think that's going to be the thing.", "tokens": [50384, 286, 519, 264, 1156, 299, 1429, 3991, 25129, 366, 437, 286, 478, 34246, 322, 13, 286, 519, 300, 311, 516, 281, 312, 264, 551, 13, 50548], "temperature": 0.0, "avg_logprob": -0.13724084333939987, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.0023224640171974897}, {"id": 1512, "seek": 791072, "start": 7915.92, "end": 7919.04, "text": " Tell the kids that you could use the fire emoji and they'd be like, what?", "tokens": [50624, 5115, 264, 2301, 300, 291, 727, 764, 264, 2610, 31595, 293, 436, 1116, 312, 411, 11, 437, 30, 50780], "temperature": 0.0, "avg_logprob": -0.13724084333939987, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.0023224640171974897}, {"id": 1513, "seek": 791072, "start": 7919.04, "end": 7925.12, "text": " Exactly. Well, in all seriousness, I mean, I think there's really, I'll give you two", "tokens": [50780, 7587, 13, 1042, 11, 294, 439, 44880, 11, 286, 914, 11, 286, 519, 456, 311, 534, 11, 286, 603, 976, 291, 732, 51084], "temperature": 0.0, "avg_logprob": -0.13724084333939987, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.0023224640171974897}, {"id": 1514, "seek": 791072, "start": 7925.68, "end": 7932.08, "text": " opposite answers. One is, I hope if it's useful, if it solves problems and people care about those", "tokens": [51112, 6182, 6338, 13, 1485, 307, 11, 286, 1454, 498, 309, 311, 4420, 11, 498, 309, 39890, 2740, 293, 561, 1127, 466, 729, 51432], "temperature": 0.0, "avg_logprob": -0.13724084333939987, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.0023224640171974897}, {"id": 1515, "seek": 791072, "start": 7932.08, "end": 7938.56, "text": " problems being solved, they'll adopt the tech. That's kind of the simple answer. And when you're", "tokens": [51432, 2740, 885, 13041, 11, 436, 603, 6878, 264, 7553, 13, 663, 311, 733, 295, 264, 2199, 1867, 13, 400, 562, 291, 434, 51756], "temperature": 0.0, "avg_logprob": -0.13724084333939987, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.0023224640171974897}, {"id": 1516, "seek": 793856, "start": 7938.56, "end": 7944.0, "text": " looking to get tech adopted, the question is, is it solving an important problem people need solved?", "tokens": [50364, 1237, 281, 483, 7553, 12175, 11, 264, 1168, 307, 11, 307, 309, 12606, 364, 1021, 1154, 561, 643, 13041, 30, 50636], "temperature": 0.0, "avg_logprob": -0.1118436669403652, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0010648545576259494}, {"id": 1517, "seek": 793856, "start": 7944.0, "end": 7949.200000000001, "text": " And is the adoption cost low enough that they're willing to make the switch and", "tokens": [50636, 400, 307, 264, 19215, 2063, 2295, 1547, 300, 436, 434, 4950, 281, 652, 264, 3679, 293, 50896], "temperature": 0.0, "avg_logprob": -0.1118436669403652, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0010648545576259494}, {"id": 1518, "seek": 793856, "start": 7949.84, "end": 7955.200000000001, "text": " cut over and do the pain up front so that they can actually do it? And so hopefully,", "tokens": [50928, 1723, 670, 293, 360, 264, 1822, 493, 1868, 370, 300, 436, 393, 767, 360, 309, 30, 400, 370, 4696, 11, 51196], "temperature": 0.0, "avg_logprob": -0.1118436669403652, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0010648545576259494}, {"id": 1519, "seek": 793856, "start": 7955.200000000001, "end": 7961.120000000001, "text": " Mojo will be that for a bunch of people and people building these hybrid packages are suffering.", "tokens": [51196, 3335, 5134, 486, 312, 300, 337, 257, 3840, 295, 561, 293, 561, 2390, 613, 13051, 17401, 366, 7755, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1118436669403652, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0010648545576259494}, {"id": 1520, "seek": 793856, "start": 7961.120000000001, "end": 7965.04, "text": " It's really painful. And so I think that we have a good shot of helping people.", "tokens": [51492, 467, 311, 534, 11697, 13, 400, 370, 286, 519, 300, 321, 362, 257, 665, 3347, 295, 4315, 561, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1118436669403652, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.0010648545576259494}, {"id": 1521, "seek": 796504, "start": 7965.04, "end": 7970.0, "text": " But the other side is, it's okay if people don't use Mojo. It's not my job to say,", "tokens": [50364, 583, 264, 661, 1252, 307, 11, 309, 311, 1392, 498, 561, 500, 380, 764, 3335, 5134, 13, 467, 311, 406, 452, 1691, 281, 584, 11, 50612], "temperature": 0.0, "avg_logprob": -0.12679398328738106, "compression_ratio": 1.7751677852348993, "no_speech_prob": 0.005218724254518747}, {"id": 1522, "seek": 796504, "start": 7970.0, "end": 7973.5199999999995, "text": " everybody should do this. I'm not saying Python is bad. I hope Python,", "tokens": [50612, 2201, 820, 360, 341, 13, 286, 478, 406, 1566, 15329, 307, 1578, 13, 286, 1454, 15329, 11, 50788], "temperature": 0.0, "avg_logprob": -0.12679398328738106, "compression_ratio": 1.7751677852348993, "no_speech_prob": 0.005218724254518747}, {"id": 1523, "seek": 796504, "start": 7973.5199999999995, "end": 7977.2, "text": " see Python, like all these implementations, because Python ecosystem is not just see Python.", "tokens": [50788, 536, 15329, 11, 411, 439, 613, 4445, 763, 11, 570, 15329, 11311, 307, 406, 445, 536, 15329, 13, 50972], "temperature": 0.0, "avg_logprob": -0.12679398328738106, "compression_ratio": 1.7751677852348993, "no_speech_prob": 0.005218724254518747}, {"id": 1524, "seek": 796504, "start": 7977.2, "end": 7981.36, "text": " It's also a bunch of different implementations with different tradeoffs. And this ecosystem is", "tokens": [50972, 467, 311, 611, 257, 3840, 295, 819, 4445, 763, 365, 819, 4923, 19231, 13, 400, 341, 11311, 307, 51180], "temperature": 0.0, "avg_logprob": -0.12679398328738106, "compression_ratio": 1.7751677852348993, "no_speech_prob": 0.005218724254518747}, {"id": 1525, "seek": 796504, "start": 7981.36, "end": 7986.8, "text": " really powerful and exciting, as are other programming languages. It's not like TypeScript", "tokens": [51180, 534, 4005, 293, 4670, 11, 382, 366, 661, 9410, 8650, 13, 467, 311, 406, 411, 15576, 14237, 51452], "temperature": 0.0, "avg_logprob": -0.12679398328738106, "compression_ratio": 1.7751677852348993, "no_speech_prob": 0.005218724254518747}, {"id": 1526, "seek": 796504, "start": 7986.8, "end": 7992.56, "text": " or something is going to go away. And so there's not a winner take all thing. And so I hope that", "tokens": [51452, 420, 746, 307, 516, 281, 352, 1314, 13, 400, 370, 456, 311, 406, 257, 8507, 747, 439, 551, 13, 400, 370, 286, 1454, 300, 51740], "temperature": 0.0, "avg_logprob": -0.12679398328738106, "compression_ratio": 1.7751677852348993, "no_speech_prob": 0.005218724254518747}, {"id": 1527, "seek": 799256, "start": 7992.56, "end": 7996.0, "text": " Mojo is exciting and useful to people. But if it's not, that's also fine.", "tokens": [50364, 3335, 5134, 307, 4670, 293, 4420, 281, 561, 13, 583, 498, 309, 311, 406, 11, 300, 311, 611, 2489, 13, 50536], "temperature": 0.0, "avg_logprob": -0.11778200589693509, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.009408907033503056}, {"id": 1528, "seek": 799256, "start": 7996.0, "end": 8004.8, "text": " But I also wonder what the use case for why you should try Mojo would be. So practically speaking.", "tokens": [50536, 583, 286, 611, 2441, 437, 264, 764, 1389, 337, 983, 291, 820, 853, 3335, 5134, 576, 312, 13, 407, 15667, 4124, 13, 50976], "temperature": 0.0, "avg_logprob": -0.11778200589693509, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.009408907033503056}, {"id": 1529, "seek": 799256, "start": 8005.84, "end": 8011.84, "text": " It seems like, so there's entertainment. There's a dopamine hit of saying, holy", "tokens": [51028, 467, 2544, 411, 11, 370, 456, 311, 12393, 13, 821, 311, 257, 37219, 2045, 295, 1566, 11, 10622, 51328], "temperature": 0.0, "avg_logprob": -0.11778200589693509, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.009408907033503056}, {"id": 1530, "seek": 799256, "start": 8011.84, "end": 8017.84, "text": " shit, this is 10 times faster. This little piece of code is 10 times faster in Mojo.", "tokens": [51328, 4611, 11, 341, 307, 1266, 1413, 4663, 13, 639, 707, 2522, 295, 3089, 307, 1266, 1413, 4663, 294, 3335, 5134, 13, 51628], "temperature": 0.0, "avg_logprob": -0.11778200589693509, "compression_ratio": 1.5601851851851851, "no_speech_prob": 0.009408907033503056}, {"id": 1531, "seek": 801784, "start": 8018.16, "end": 8022.8, "text": " Box before he gets to 35,000. Exactly. I mean, just even that, I mean,", "tokens": [50380, 15112, 949, 415, 2170, 281, 6976, 11, 1360, 13, 7587, 13, 286, 914, 11, 445, 754, 300, 11, 286, 914, 11, 50612], "temperature": 0.0, "avg_logprob": -0.16843797827279697, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.03674956411123276}, {"id": 1532, "seek": 801784, "start": 8022.8, "end": 8030.56, "text": " that's the dopamine hit that every programmer sort of dreams of is the optimization. It's also the", "tokens": [50612, 300, 311, 264, 37219, 2045, 300, 633, 32116, 1333, 295, 7505, 295, 307, 264, 19618, 13, 467, 311, 611, 264, 51000], "temperature": 0.0, "avg_logprob": -0.16843797827279697, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.03674956411123276}, {"id": 1533, "seek": 801784, "start": 8030.56, "end": 8037.68, "text": " drug that can pull you in and have you waste way too much of your life optimizing and over", "tokens": [51000, 4110, 300, 393, 2235, 291, 294, 293, 362, 291, 5964, 636, 886, 709, 295, 428, 993, 40425, 293, 670, 51356], "temperature": 0.0, "avg_logprob": -0.16843797827279697, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.03674956411123276}, {"id": 1534, "seek": 801784, "start": 8037.68, "end": 8044.32, "text": " optimizing, right? But so what would you see it would be like comedy. It's very hard to predict,", "tokens": [51356, 40425, 11, 558, 30, 583, 370, 437, 576, 291, 536, 309, 576, 312, 411, 13394, 13, 467, 311, 588, 1152, 281, 6069, 11, 51688], "temperature": 0.0, "avg_logprob": -0.16843797827279697, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.03674956411123276}, {"id": 1535, "seek": 804432, "start": 8044.32, "end": 8051.12, "text": " of course. But if you look 10 years from now, Mojo is super successful. What do you think", "tokens": [50364, 295, 1164, 13, 583, 498, 291, 574, 1266, 924, 490, 586, 11, 3335, 5134, 307, 1687, 4406, 13, 708, 360, 291, 519, 50704], "temperature": 0.0, "avg_logprob": -0.1618729494930653, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.012235370464622974}, {"id": 1536, "seek": 804432, "start": 8051.12, "end": 8057.84, "text": " would be the thing where people try it and then use it regularly and it kind of grows and grows", "tokens": [50704, 576, 312, 264, 551, 689, 561, 853, 309, 293, 550, 764, 309, 11672, 293, 309, 733, 295, 13156, 293, 13156, 51040], "temperature": 0.0, "avg_logprob": -0.1618729494930653, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.012235370464622974}, {"id": 1537, "seek": 804432, "start": 8057.84, "end": 8063.599999999999, "text": " and grows and grows? So you talk about dopamine hit. And so again, humans are not one thing.", "tokens": [51040, 293, 13156, 293, 13156, 30, 407, 291, 751, 466, 37219, 2045, 13, 400, 370, 797, 11, 6255, 366, 406, 472, 551, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1618729494930653, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.012235370464622974}, {"id": 1538, "seek": 804432, "start": 8064.24, "end": 8068.48, "text": " And some people love rewriting their code and learning new things and throwing themselves", "tokens": [51360, 400, 512, 561, 959, 319, 19868, 641, 3089, 293, 2539, 777, 721, 293, 10238, 2969, 51572], "temperature": 0.0, "avg_logprob": -0.1618729494930653, "compression_ratio": 1.6576576576576576, "no_speech_prob": 0.012235370464622974}, {"id": 1539, "seek": 806848, "start": 8068.48, "end": 8073.759999999999, "text": " in the deep end and trying out a new thing. In my experience, most people don't. Like,", "tokens": [50364, 294, 264, 2452, 917, 293, 1382, 484, 257, 777, 551, 13, 682, 452, 1752, 11, 881, 561, 500, 380, 13, 1743, 11, 50628], "temperature": 0.0, "avg_logprob": -0.11678277292559223, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.1479882150888443}, {"id": 1540, "seek": 806848, "start": 8073.759999999999, "end": 8079.599999999999, "text": " they're too busy. They have other things going on. By number, most people don't like this. I want", "tokens": [50628, 436, 434, 886, 5856, 13, 814, 362, 661, 721, 516, 322, 13, 3146, 1230, 11, 881, 561, 500, 380, 411, 341, 13, 286, 528, 50920], "temperature": 0.0, "avg_logprob": -0.11678277292559223, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.1479882150888443}, {"id": 1541, "seek": 806848, "start": 8079.599999999999, "end": 8087.599999999999, "text": " to rewrite all my code. But even those people, the two busy people, the people that don't actually", "tokens": [50920, 281, 28132, 439, 452, 3089, 13, 583, 754, 729, 561, 11, 264, 732, 5856, 561, 11, 264, 561, 300, 500, 380, 767, 51320], "temperature": 0.0, "avg_logprob": -0.11678277292559223, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.1479882150888443}, {"id": 1542, "seek": 806848, "start": 8087.599999999999, "end": 8092.0, "text": " care about the language that just care about getting stuff done, those people do like learning new", "tokens": [51320, 1127, 466, 264, 2856, 300, 445, 1127, 466, 1242, 1507, 1096, 11, 729, 561, 360, 411, 2539, 777, 51540], "temperature": 0.0, "avg_logprob": -0.11678277292559223, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.1479882150888443}, {"id": 1543, "seek": 806848, "start": 8092.0, "end": 8097.28, "text": " things, right? And so you talk about the dopamine rush of 10x faster. Wow, that's cool. I want to", "tokens": [51540, 721, 11, 558, 30, 400, 370, 291, 751, 466, 264, 37219, 9300, 295, 1266, 87, 4663, 13, 3153, 11, 300, 311, 1627, 13, 286, 528, 281, 51804], "temperature": 0.0, "avg_logprob": -0.11678277292559223, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.1479882150888443}, {"id": 1544, "seek": 809728, "start": 8097.28, "end": 8101.599999999999, "text": " do that again. Well, it's also like, here's a thing I've heard about in a different domain.", "tokens": [50364, 360, 300, 797, 13, 1042, 11, 309, 311, 611, 411, 11, 510, 311, 257, 551, 286, 600, 2198, 466, 294, 257, 819, 9274, 13, 50580], "temperature": 0.0, "avg_logprob": -0.09511154810587565, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.013633313588798046}, {"id": 1545, "seek": 809728, "start": 8101.599999999999, "end": 8105.84, "text": " And I don't have to rewrite all my code. I can learn a new trick, right? Well, that's called", "tokens": [50580, 400, 286, 500, 380, 362, 281, 28132, 439, 452, 3089, 13, 286, 393, 1466, 257, 777, 4282, 11, 558, 30, 1042, 11, 300, 311, 1219, 50792], "temperature": 0.0, "avg_logprob": -0.09511154810587565, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.013633313588798046}, {"id": 1546, "seek": 809728, "start": 8105.84, "end": 8112.4, "text": " growth. And so one thing that I think is cool about Mojo, and again, those will take a little bit", "tokens": [50792, 4599, 13, 400, 370, 472, 551, 300, 286, 519, 307, 1627, 466, 3335, 5134, 11, 293, 797, 11, 729, 486, 747, 257, 707, 857, 51120], "temperature": 0.0, "avg_logprob": -0.09511154810587565, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.013633313588798046}, {"id": 1547, "seek": 809728, "start": 8112.4, "end": 8117.04, "text": " of time for, for example, the blog posts and the books and like all that kind of stuff to develop", "tokens": [51120, 295, 565, 337, 11, 337, 1365, 11, 264, 6968, 12300, 293, 264, 3642, 293, 411, 439, 300, 733, 295, 1507, 281, 1499, 51352], "temperature": 0.0, "avg_logprob": -0.09511154810587565, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.013633313588798046}, {"id": 1548, "seek": 809728, "start": 8117.04, "end": 8121.2, "text": " and the language needs to get further along. But what we're doing, you talk about types,", "tokens": [51352, 293, 264, 2856, 2203, 281, 483, 3052, 2051, 13, 583, 437, 321, 434, 884, 11, 291, 751, 466, 3467, 11, 51560], "temperature": 0.0, "avg_logprob": -0.09511154810587565, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.013633313588798046}, {"id": 1549, "seek": 809728, "start": 8121.2, "end": 8125.679999999999, "text": " like you can say, look, you can start with the world you already know, and you can progressively", "tokens": [51560, 411, 291, 393, 584, 11, 574, 11, 291, 393, 722, 365, 264, 1002, 291, 1217, 458, 11, 293, 291, 393, 46667, 51784], "temperature": 0.0, "avg_logprob": -0.09511154810587565, "compression_ratio": 1.725609756097561, "no_speech_prob": 0.013633313588798046}, {"id": 1550, "seek": 812568, "start": 8125.68, "end": 8131.200000000001, "text": " learn new things and adopt them where it makes sense. If you never do that, that's cool. You're", "tokens": [50364, 1466, 777, 721, 293, 6878, 552, 689, 309, 1669, 2020, 13, 759, 291, 1128, 360, 300, 11, 300, 311, 1627, 13, 509, 434, 50640], "temperature": 0.0, "avg_logprob": -0.09631143510341644, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.02095911279320717}, {"id": 1551, "seek": 812568, "start": 8131.200000000001, "end": 8135.92, "text": " not a bad person. If you, if you get really excited about it, want to go all the way in the deep end", "tokens": [50640, 406, 257, 1578, 954, 13, 759, 291, 11, 498, 291, 483, 534, 2919, 466, 309, 11, 528, 281, 352, 439, 264, 636, 294, 264, 2452, 917, 50876], "temperature": 0.0, "avg_logprob": -0.09631143510341644, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.02095911279320717}, {"id": 1552, "seek": 812568, "start": 8135.92, "end": 8140.4800000000005, "text": " and want to rewrite everything and like whatever, that's cool, right? But I think the middle path", "tokens": [50876, 293, 528, 281, 28132, 1203, 293, 411, 2035, 11, 300, 311, 1627, 11, 558, 30, 583, 286, 519, 264, 2808, 3100, 51104], "temperature": 0.0, "avg_logprob": -0.09631143510341644, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.02095911279320717}, {"id": 1553, "seek": 812568, "start": 8140.4800000000005, "end": 8146.400000000001, "text": " is actually the more likely one where it's, you know, you come out with a new, a new idea and", "tokens": [51104, 307, 767, 264, 544, 3700, 472, 689, 309, 311, 11, 291, 458, 11, 291, 808, 484, 365, 257, 777, 11, 257, 777, 1558, 293, 51400], "temperature": 0.0, "avg_logprob": -0.09631143510341644, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.02095911279320717}, {"id": 1554, "seek": 812568, "start": 8146.400000000001, "end": 8150.96, "text": " you discover, wow, that makes my code way simpler, way more beautiful, way faster, way whatever.", "tokens": [51400, 291, 4411, 11, 6076, 11, 300, 1669, 452, 3089, 636, 18587, 11, 636, 544, 2238, 11, 636, 4663, 11, 636, 2035, 13, 51628], "temperature": 0.0, "avg_logprob": -0.09631143510341644, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.02095911279320717}, {"id": 1555, "seek": 815096, "start": 8150.96, "end": 8156.16, "text": " And I think that's what people like. Now, if you fast forward and you said like 10 years out,", "tokens": [50364, 400, 286, 519, 300, 311, 437, 561, 411, 13, 823, 11, 498, 291, 2370, 2128, 293, 291, 848, 411, 1266, 924, 484, 11, 50624], "temperature": 0.0, "avg_logprob": -0.1134292156465592, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.0850609615445137}, {"id": 1556, "seek": 815096, "start": 8156.72, "end": 8160.4, "text": " right? I can give you a very different answer on that, which is, I mean,", "tokens": [50652, 558, 30, 286, 393, 976, 291, 257, 588, 819, 1867, 322, 300, 11, 597, 307, 11, 286, 914, 11, 50836], "temperature": 0.0, "avg_logprob": -0.1134292156465592, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.0850609615445137}, {"id": 1557, "seek": 815096, "start": 8161.36, "end": 8166.4800000000005, "text": " if you go back and look at what computers looked like 20 years ago, every 18 months, they got", "tokens": [50884, 498, 291, 352, 646, 293, 574, 412, 437, 10807, 2956, 411, 945, 924, 2057, 11, 633, 2443, 2493, 11, 436, 658, 51140], "temperature": 0.0, "avg_logprob": -0.1134292156465592, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.0850609615445137}, {"id": 1558, "seek": 815096, "start": 8166.4800000000005, "end": 8172.4800000000005, "text": " faster for free, right? 2x faster every 18 months, it was like clockwork, it was, it was free,", "tokens": [51140, 4663, 337, 1737, 11, 558, 30, 568, 87, 4663, 633, 2443, 2493, 11, 309, 390, 411, 7830, 1902, 11, 309, 390, 11, 309, 390, 1737, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1134292156465592, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.0850609615445137}, {"id": 1559, "seek": 815096, "start": 8172.4800000000005, "end": 8178.0, "text": " right? You go back 10 years ago, and we entered in this world where suddenly we had multicore CPUs", "tokens": [51440, 558, 30, 509, 352, 646, 1266, 924, 2057, 11, 293, 321, 9065, 294, 341, 1002, 689, 5800, 321, 632, 30608, 418, 13199, 82, 51716], "temperature": 0.0, "avg_logprob": -0.1134292156465592, "compression_ratio": 1.7328244274809161, "no_speech_prob": 0.0850609615445137}, {"id": 1560, "seek": 817800, "start": 8178.0, "end": 8184.48, "text": " and we had GPUs. And if you squint and turn your head, what are GPUs? It's just a many core,", "tokens": [50364, 293, 321, 632, 18407, 82, 13, 400, 498, 291, 2339, 686, 293, 1261, 428, 1378, 11, 437, 366, 18407, 82, 30, 467, 311, 445, 257, 867, 4965, 11, 50688], "temperature": 0.0, "avg_logprob": -0.09782463770646316, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00271443254314363}, {"id": 1561, "seek": 817800, "start": 8184.48, "end": 8191.36, "text": " very simple CPU thing kind of, right? And so, and 10 years ago, it was CPUs and GPUs and graphics.", "tokens": [50688, 588, 2199, 13199, 551, 733, 295, 11, 558, 30, 400, 370, 11, 293, 1266, 924, 2057, 11, 309, 390, 13199, 82, 293, 18407, 82, 293, 11837, 13, 51032], "temperature": 0.0, "avg_logprob": -0.09782463770646316, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00271443254314363}, {"id": 1562, "seek": 817800, "start": 8194.0, "end": 8201.36, "text": " Today, we have CPUs, GPUs, graphics, and AI, because it's so important because the compute", "tokens": [51164, 2692, 11, 321, 362, 13199, 82, 11, 18407, 82, 11, 11837, 11, 293, 7318, 11, 570, 309, 311, 370, 1021, 570, 264, 14722, 51532], "temperature": 0.0, "avg_logprob": -0.09782463770646316, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00271443254314363}, {"id": 1563, "seek": 817800, "start": 8201.36, "end": 8205.68, "text": " is so demanding because of the smart cameras and the watches and all the different places", "tokens": [51532, 307, 370, 19960, 570, 295, 264, 4069, 8622, 293, 264, 17062, 293, 439, 264, 819, 3190, 51748], "temperature": 0.0, "avg_logprob": -0.09782463770646316, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.00271443254314363}, {"id": 1564, "seek": 820568, "start": 8205.76, "end": 8211.12, "text": " that AI needs to work in our lives, it's caused this explosion of hardware. And so,", "tokens": [50368, 300, 7318, 2203, 281, 589, 294, 527, 2909, 11, 309, 311, 7008, 341, 15673, 295, 8837, 13, 400, 370, 11, 50636], "temperature": 0.0, "avg_logprob": -0.1053484092324467, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.002550704637542367}, {"id": 1565, "seek": 820568, "start": 8211.12, "end": 8215.52, "text": " part of my thesis, part of my belief of where computing goes, if you look at 10 years from now,", "tokens": [50636, 644, 295, 452, 22288, 11, 644, 295, 452, 7107, 295, 689, 15866, 1709, 11, 498, 291, 574, 412, 1266, 924, 490, 586, 11, 50856], "temperature": 0.0, "avg_logprob": -0.1053484092324467, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.002550704637542367}, {"id": 1566, "seek": 820568, "start": 8216.16, "end": 8220.08, "text": " is it's not going to get simpler. Physics isn't going back to where we came from.", "tokens": [50888, 307, 309, 311, 406, 516, 281, 483, 18587, 13, 38355, 1943, 380, 516, 646, 281, 689, 321, 1361, 490, 13, 51084], "temperature": 0.0, "avg_logprob": -0.1053484092324467, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.002550704637542367}, {"id": 1567, "seek": 820568, "start": 8220.720000000001, "end": 8225.44, "text": " It's only going to get weirder from here on out, right? And so, to me, the exciting part about", "tokens": [51116, 467, 311, 787, 516, 281, 483, 321, 347, 1068, 490, 510, 322, 484, 11, 558, 30, 400, 370, 11, 281, 385, 11, 264, 4670, 644, 466, 51352], "temperature": 0.0, "avg_logprob": -0.1053484092324467, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.002550704637542367}, {"id": 1568, "seek": 820568, "start": 8225.44, "end": 8231.28, "text": " what we're building is it's about building that universal platform, which the world can", "tokens": [51352, 437, 321, 434, 2390, 307, 309, 311, 466, 2390, 300, 11455, 3663, 11, 597, 264, 1002, 393, 51644], "temperature": 0.0, "avg_logprob": -0.1053484092324467, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.002550704637542367}, {"id": 1569, "seek": 823128, "start": 8231.28, "end": 8234.720000000001, "text": " continue to get weird, because again, I don't think it's avoidable, it's physics,", "tokens": [50364, 2354, 281, 483, 3657, 11, 570, 797, 11, 286, 500, 380, 519, 309, 311, 5042, 712, 11, 309, 311, 10649, 11, 50536], "temperature": 0.0, "avg_logprob": -0.10926781237964898, "compression_ratio": 1.7523510971786833, "no_speech_prob": 0.05831841379404068}, {"id": 1570, "seek": 823128, "start": 8235.36, "end": 8239.28, "text": " but we can help lift people scale, do things with it, and they don't have to rewrite their code", "tokens": [50568, 457, 321, 393, 854, 5533, 561, 4373, 11, 360, 721, 365, 309, 11, 293, 436, 500, 380, 362, 281, 28132, 641, 3089, 50764], "temperature": 0.0, "avg_logprob": -0.10926781237964898, "compression_ratio": 1.7523510971786833, "no_speech_prob": 0.05831841379404068}, {"id": 1571, "seek": 823128, "start": 8239.28, "end": 8243.6, "text": " every time a new device comes out. And I think that's pretty cool. And so, if Mojo can help with", "tokens": [50764, 633, 565, 257, 777, 4302, 1487, 484, 13, 400, 286, 519, 300, 311, 1238, 1627, 13, 400, 370, 11, 498, 3335, 5134, 393, 854, 365, 50980], "temperature": 0.0, "avg_logprob": -0.10926781237964898, "compression_ratio": 1.7523510971786833, "no_speech_prob": 0.05831841379404068}, {"id": 1572, "seek": 823128, "start": 8243.6, "end": 8247.76, "text": " that problem, then I think that it will be hopefully quite interesting and quite useful to", "tokens": [50980, 300, 1154, 11, 550, 286, 519, 300, 309, 486, 312, 4696, 1596, 1880, 293, 1596, 4420, 281, 51188], "temperature": 0.0, "avg_logprob": -0.10926781237964898, "compression_ratio": 1.7523510971786833, "no_speech_prob": 0.05831841379404068}, {"id": 1573, "seek": 823128, "start": 8247.76, "end": 8252.800000000001, "text": " a wide range of people because there's so much potential and like there's so, you know, maybe", "tokens": [51188, 257, 4874, 3613, 295, 561, 570, 456, 311, 370, 709, 3995, 293, 411, 456, 311, 370, 11, 291, 458, 11, 1310, 51440], "temperature": 0.0, "avg_logprob": -0.10926781237964898, "compression_ratio": 1.7523510971786833, "no_speech_prob": 0.05831841379404068}, {"id": 1574, "seek": 823128, "start": 8252.800000000001, "end": 8257.04, "text": " analog computers will become a thing or something, right? And we need to be able to get into a mode", "tokens": [51440, 16660, 10807, 486, 1813, 257, 551, 420, 746, 11, 558, 30, 400, 321, 643, 281, 312, 1075, 281, 483, 666, 257, 4391, 51652], "temperature": 0.0, "avg_logprob": -0.10926781237964898, "compression_ratio": 1.7523510971786833, "no_speech_prob": 0.05831841379404068}, {"id": 1575, "seek": 825704, "start": 8257.04, "end": 8261.84, "text": " where we can move this programming model forward, but do so in a way where we're lifting people", "tokens": [50364, 689, 321, 393, 1286, 341, 9410, 2316, 2128, 11, 457, 360, 370, 294, 257, 636, 689, 321, 434, 15798, 561, 50604], "temperature": 0.0, "avg_logprob": -0.12453473698009145, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.006487411912530661}, {"id": 1576, "seek": 825704, "start": 8261.84, "end": 8266.640000000001, "text": " and growing them instead of forcing them to rewrite all their code and exploding them.", "tokens": [50604, 293, 4194, 552, 2602, 295, 19030, 552, 281, 28132, 439, 641, 3089, 293, 35175, 552, 13, 50844], "temperature": 0.0, "avg_logprob": -0.12453473698009145, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.006487411912530661}, {"id": 1577, "seek": 825704, "start": 8266.640000000001, "end": 8270.720000000001, "text": " Do you think there'll be a few major libraries that go Mojo first?", "tokens": [50844, 1144, 291, 519, 456, 603, 312, 257, 1326, 2563, 15148, 300, 352, 3335, 5134, 700, 30, 51048], "temperature": 0.0, "avg_logprob": -0.12453473698009145, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.006487411912530661}, {"id": 1578, "seek": 825704, "start": 8273.68, "end": 8278.560000000001, "text": " Well, so, I mean, the modular engine is all Mojo. So, again, come back to like, we're not", "tokens": [51196, 1042, 11, 370, 11, 286, 914, 11, 264, 31111, 2848, 307, 439, 3335, 5134, 13, 407, 11, 797, 11, 808, 646, 281, 411, 11, 321, 434, 406, 51440], "temperature": 0.0, "avg_logprob": -0.12453473698009145, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.006487411912530661}, {"id": 1579, "seek": 825704, "start": 8278.560000000001, "end": 8282.640000000001, "text": " building Mojo because it's fun, we're building Mojo because we had to dissolve these accelerators.", "tokens": [51440, 2390, 3335, 5134, 570, 309, 311, 1019, 11, 321, 434, 2390, 3335, 5134, 570, 321, 632, 281, 30150, 613, 10172, 3391, 13, 51644], "temperature": 0.0, "avg_logprob": -0.12453473698009145, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.006487411912530661}, {"id": 1580, "seek": 825704, "start": 8282.640000000001, "end": 8285.68, "text": " That's the origin story. But I mean, ones that are currently in Python.", "tokens": [51644, 663, 311, 264, 4957, 1657, 13, 583, 286, 914, 11, 2306, 300, 366, 4362, 294, 15329, 13, 51796], "temperature": 0.0, "avg_logprob": -0.12453473698009145, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.006487411912530661}, {"id": 1581, "seek": 828568, "start": 8285.68, "end": 8289.2, "text": " Yeah. So, I think that a number of these projects will. And so, one of the things, again, this is", "tokens": [50364, 865, 13, 407, 11, 286, 519, 300, 257, 1230, 295, 613, 4455, 486, 13, 400, 370, 11, 472, 295, 264, 721, 11, 797, 11, 341, 307, 50540], "temperature": 0.0, "avg_logprob": -0.1316562076266721, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.004197758622467518}, {"id": 1582, "seek": 828568, "start": 8289.2, "end": 8293.68, "text": " just my best guess. Like, each of the package maintainers also has, I'm sure, plenty of other", "tokens": [50540, 445, 452, 1151, 2041, 13, 1743, 11, 1184, 295, 264, 7372, 6909, 433, 611, 575, 11, 286, 478, 988, 11, 7140, 295, 661, 50764], "temperature": 0.0, "avg_logprob": -0.1316562076266721, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.004197758622467518}, {"id": 1583, "seek": 828568, "start": 8293.68, "end": 8297.44, "text": " things going on. People don't like really don't like rewriting code just for the sake of rewriting", "tokens": [50764, 721, 516, 322, 13, 3432, 500, 380, 411, 534, 500, 380, 411, 319, 19868, 3089, 445, 337, 264, 9717, 295, 319, 19868, 50952], "temperature": 0.0, "avg_logprob": -0.1316562076266721, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.004197758622467518}, {"id": 1584, "seek": 828568, "start": 8297.44, "end": 8303.84, "text": " code. But sometimes, like, people are excited about like adopting a new idea.", "tokens": [50952, 3089, 13, 583, 2171, 11, 411, 11, 561, 366, 2919, 466, 411, 32328, 257, 777, 1558, 13, 51272], "temperature": 0.0, "avg_logprob": -0.1316562076266721, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.004197758622467518}, {"id": 1585, "seek": 828568, "start": 8303.84, "end": 8309.36, "text": " Yeah. And it turns out that while rewriting code is generally not people's first thing,", "tokens": [51272, 865, 13, 400, 309, 4523, 484, 300, 1339, 319, 19868, 3089, 307, 5101, 406, 561, 311, 700, 551, 11, 51548], "temperature": 0.0, "avg_logprob": -0.1316562076266721, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.004197758622467518}, {"id": 1586, "seek": 828568, "start": 8309.36, "end": 8314.56, "text": " turns out that redesigning something while you rewrite it and using a rewrite as an excuse to", "tokens": [51548, 4523, 484, 300, 16762, 9676, 746, 1339, 291, 28132, 309, 293, 1228, 257, 28132, 382, 364, 8960, 281, 51808], "temperature": 0.0, "avg_logprob": -0.1316562076266721, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.004197758622467518}, {"id": 1587, "seek": 831456, "start": 8314.56, "end": 8322.48, "text": " redesign can lead to the 2.0 of your thing that's way better than the 1.0. And so, I have no idea.", "tokens": [50364, 39853, 393, 1477, 281, 264, 568, 13, 15, 295, 428, 551, 300, 311, 636, 1101, 813, 264, 502, 13, 15, 13, 400, 370, 11, 286, 362, 572, 1558, 13, 50760], "temperature": 0.0, "avg_logprob": -0.10658654158677512, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.012817813083529472}, {"id": 1588, "seek": 831456, "start": 8322.48, "end": 8327.039999999999, "text": " I can't predict that. But there's a lot of these places where, again, if you have a package that", "tokens": [50760, 286, 393, 380, 6069, 300, 13, 583, 456, 311, 257, 688, 295, 613, 3190, 689, 11, 797, 11, 498, 291, 362, 257, 7372, 300, 50988], "temperature": 0.0, "avg_logprob": -0.10658654158677512, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.012817813083529472}, {"id": 1589, "seek": 831456, "start": 8327.039999999999, "end": 8332.64, "text": " is half C and half Python, right, you just solve the pain, make it easier to move things faster,", "tokens": [50988, 307, 1922, 383, 293, 1922, 15329, 11, 558, 11, 291, 445, 5039, 264, 1822, 11, 652, 309, 3571, 281, 1286, 721, 4663, 11, 51268], "temperature": 0.0, "avg_logprob": -0.10658654158677512, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.012817813083529472}, {"id": 1590, "seek": 831456, "start": 8332.64, "end": 8337.76, "text": " make it easier to debug and evolve your tech. Adopting Mojo kind of makes sense to start with.", "tokens": [51268, 652, 309, 3571, 281, 24083, 293, 16693, 428, 7553, 13, 1999, 404, 783, 3335, 5134, 733, 295, 1669, 2020, 281, 722, 365, 13, 51524], "temperature": 0.0, "avg_logprob": -0.10658654158677512, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.012817813083529472}, {"id": 1591, "seek": 831456, "start": 8337.76, "end": 8340.08, "text": " And then it gives you this opportunity to rethink these things.", "tokens": [51524, 400, 550, 309, 2709, 291, 341, 2650, 281, 34595, 613, 721, 13, 51640], "temperature": 0.0, "avg_logprob": -0.10658654158677512, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.012817813083529472}, {"id": 1592, "seek": 834008, "start": 8340.08, "end": 8349.12, "text": " So the two big gains are that there's a performance gain and then there's the portability to all kinds", "tokens": [50364, 407, 264, 732, 955, 16823, 366, 300, 456, 311, 257, 3389, 6052, 293, 550, 456, 311, 264, 2436, 2310, 281, 439, 3685, 50816], "temperature": 0.0, "avg_logprob": -0.13663686363442432, "compression_ratio": 1.6431535269709543, "no_speech_prob": 0.00100033194757998}, {"id": 1593, "seek": 834008, "start": 8349.12, "end": 8354.96, "text": " of different devices. And there's safety, right? So you talk about real types. I mean, not saying", "tokens": [50816, 295, 819, 5759, 13, 400, 456, 311, 4514, 11, 558, 30, 407, 291, 751, 466, 957, 3467, 13, 286, 914, 11, 406, 1566, 51108], "temperature": 0.0, "avg_logprob": -0.13663686363442432, "compression_ratio": 1.6431535269709543, "no_speech_prob": 0.00100033194757998}, {"id": 1594, "seek": 834008, "start": 8354.96, "end": 8360.16, "text": " this is for everybody, but that's actually a pretty big thing, right? And so there's a bunch of", "tokens": [51108, 341, 307, 337, 2201, 11, 457, 300, 311, 767, 257, 1238, 955, 551, 11, 558, 30, 400, 370, 456, 311, 257, 3840, 295, 51368], "temperature": 0.0, "avg_logprob": -0.13663686363442432, "compression_ratio": 1.6431535269709543, "no_speech_prob": 0.00100033194757998}, {"id": 1595, "seek": 834008, "start": 8360.16, "end": 8364.96, "text": " different aspects of what value Mojo provides. And so, I mean, it's funny for me. I've been working", "tokens": [51368, 819, 7270, 295, 437, 2158, 3335, 5134, 6417, 13, 400, 370, 11, 286, 914, 11, 309, 311, 4074, 337, 385, 13, 286, 600, 668, 1364, 51608], "temperature": 0.0, "avg_logprob": -0.13663686363442432, "compression_ratio": 1.6431535269709543, "no_speech_prob": 0.00100033194757998}, {"id": 1596, "seek": 836496, "start": 8364.96, "end": 8372.0, "text": " on these kinds of technologies and tools for too many years now. But you look at Swift, right?", "tokens": [50364, 322, 613, 3685, 295, 7943, 293, 3873, 337, 886, 867, 924, 586, 13, 583, 291, 574, 412, 25539, 11, 558, 30, 50716], "temperature": 0.0, "avg_logprob": -0.11957946736761864, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.14793899655342102}, {"id": 1597, "seek": 836496, "start": 8372.0, "end": 8375.279999999999, "text": " And we talked about Swift for TensorFlow, but Swift as a programming language, right?", "tokens": [50716, 400, 321, 2825, 466, 25539, 337, 37624, 11, 457, 25539, 382, 257, 9410, 2856, 11, 558, 30, 50880], "temperature": 0.0, "avg_logprob": -0.11957946736761864, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.14793899655342102}, {"id": 1598, "seek": 836496, "start": 8376.4, "end": 8384.4, "text": " Swift's now 13 years old from when I started it. So, because I started in 2010, if I remember.", "tokens": [50936, 25539, 311, 586, 3705, 924, 1331, 490, 562, 286, 1409, 309, 13, 407, 11, 570, 286, 1409, 294, 9657, 11, 498, 286, 1604, 13, 51336], "temperature": 0.0, "avg_logprob": -0.11957946736761864, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.14793899655342102}, {"id": 1599, "seek": 836496, "start": 8384.4, "end": 8391.199999999999, "text": " And so, that project, and I was involved with it for 12 years or something, right? That project", "tokens": [51336, 400, 370, 11, 300, 1716, 11, 293, 286, 390, 3288, 365, 309, 337, 2272, 924, 420, 746, 11, 558, 30, 663, 1716, 51676], "temperature": 0.0, "avg_logprob": -0.11957946736761864, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.14793899655342102}, {"id": 1600, "seek": 839120, "start": 8391.2, "end": 8395.36, "text": " has gone through its own really interesting story arc, right? And it's a mature, successful,", "tokens": [50364, 575, 2780, 807, 1080, 1065, 534, 1880, 1657, 10346, 11, 558, 30, 400, 309, 311, 257, 14442, 11, 4406, 11, 50572], "temperature": 0.0, "avg_logprob": -0.11558656617412418, "compression_ratio": 1.7725752508361203, "no_speech_prob": 0.03730398043990135}, {"id": 1601, "seek": 839120, "start": 8395.36, "end": 8400.720000000001, "text": " used by millions of people's system, right? Certainly not dead yet, right? But also,", "tokens": [50572, 1143, 538, 6803, 295, 561, 311, 1185, 11, 558, 30, 16628, 406, 3116, 1939, 11, 558, 30, 583, 611, 11, 50840], "temperature": 0.0, "avg_logprob": -0.11558656617412418, "compression_ratio": 1.7725752508361203, "no_speech_prob": 0.03730398043990135}, {"id": 1602, "seek": 839120, "start": 8400.720000000001, "end": 8404.16, "text": " going through that story arc, I learned a tremendous amount about building languages,", "tokens": [50840, 516, 807, 300, 1657, 10346, 11, 286, 3264, 257, 10048, 2372, 466, 2390, 8650, 11, 51012], "temperature": 0.0, "avg_logprob": -0.11558656617412418, "compression_ratio": 1.7725752508361203, "no_speech_prob": 0.03730398043990135}, {"id": 1603, "seek": 839120, "start": 8404.16, "end": 8408.320000000002, "text": " about building compilers, about working with community, and things like this. And so,", "tokens": [51012, 466, 2390, 715, 388, 433, 11, 466, 1364, 365, 1768, 11, 293, 721, 411, 341, 13, 400, 370, 11, 51220], "temperature": 0.0, "avg_logprob": -0.11558656617412418, "compression_ratio": 1.7725752508361203, "no_speech_prob": 0.03730398043990135}, {"id": 1604, "seek": 839120, "start": 8408.320000000002, "end": 8413.44, "text": " that experience, like I'm helping channel and bring directly in Mojo. And other systems,", "tokens": [51220, 300, 1752, 11, 411, 286, 478, 4315, 2269, 293, 1565, 3838, 294, 3335, 5134, 13, 400, 661, 3652, 11, 51476], "temperature": 0.0, "avg_logprob": -0.11558656617412418, "compression_ratio": 1.7725752508361203, "no_speech_prob": 0.03730398043990135}, {"id": 1605, "seek": 839120, "start": 8413.44, "end": 8417.84, "text": " same thing. Apparently, I like building and iterating and evolving things. And so, you look", "tokens": [51476, 912, 551, 13, 16755, 11, 286, 411, 2390, 293, 17138, 990, 293, 21085, 721, 13, 400, 370, 11, 291, 574, 51696], "temperature": 0.0, "avg_logprob": -0.11558656617412418, "compression_ratio": 1.7725752508361203, "no_speech_prob": 0.03730398043990135}, {"id": 1606, "seek": 841784, "start": 8417.84, "end": 8423.36, "text": " at this LLVM thing I worked on 20 years ago, and you look at MLIR, right? And so, a lot of the", "tokens": [50364, 412, 341, 441, 43, 53, 44, 551, 286, 2732, 322, 945, 924, 2057, 11, 293, 291, 574, 412, 21601, 7740, 11, 558, 30, 400, 370, 11, 257, 688, 295, 264, 50640], "temperature": 0.0, "avg_logprob": -0.08871146252280787, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.00609580660238862}, {"id": 1607, "seek": 841784, "start": 8423.36, "end": 8428.64, "text": " lessons learned in LLVM got fed into MLIR. And I think that MLIR is a way better system than", "tokens": [50640, 8820, 3264, 294, 441, 43, 53, 44, 658, 4636, 666, 21601, 7740, 13, 400, 286, 519, 300, 21601, 7740, 307, 257, 636, 1101, 1185, 813, 50904], "temperature": 0.0, "avg_logprob": -0.08871146252280787, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.00609580660238862}, {"id": 1608, "seek": 841784, "start": 8428.64, "end": 8434.960000000001, "text": " LLVM was. And Swift is a really good system. And it's amazing. But I hope that Mojo will take", "tokens": [50904, 441, 43, 53, 44, 390, 13, 400, 25539, 307, 257, 534, 665, 1185, 13, 400, 309, 311, 2243, 13, 583, 286, 1454, 300, 3335, 5134, 486, 747, 51220], "temperature": 0.0, "avg_logprob": -0.08871146252280787, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.00609580660238862}, {"id": 1609, "seek": 841784, "start": 8434.960000000001, "end": 8443.44, "text": " the next step forward in terms of design. In terms of running Mojo, people can play with it.", "tokens": [51220, 264, 958, 1823, 2128, 294, 2115, 295, 1715, 13, 682, 2115, 295, 2614, 3335, 5134, 11, 561, 393, 862, 365, 309, 13, 51644], "temperature": 0.0, "avg_logprob": -0.08871146252280787, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.00609580660238862}, {"id": 1610, "seek": 844344, "start": 8443.44, "end": 8449.2, "text": " What's Mojo Playground? Yeah. And from the interface perspective,", "tokens": [50364, 708, 311, 3335, 5134, 5506, 2921, 30, 865, 13, 400, 490, 264, 9226, 4585, 11, 50652], "temperature": 0.0, "avg_logprob": -0.1286745427924896, "compression_ratio": 1.656, "no_speech_prob": 0.031594932079315186}, {"id": 1611, "seek": 844344, "start": 8449.2, "end": 8453.92, "text": " and from the hardware perspective, what's this incredible thing running on?", "tokens": [50652, 293, 490, 264, 8837, 4585, 11, 437, 311, 341, 4651, 551, 2614, 322, 30, 50888], "temperature": 0.0, "avg_logprob": -0.1286745427924896, "compression_ratio": 1.656, "no_speech_prob": 0.031594932079315186}, {"id": 1612, "seek": 844344, "start": 8454.480000000001, "end": 8460.08, "text": " Yeah. So, right now, so here we are two weeks after launch. We decided that, okay, we have this", "tokens": [50916, 865, 13, 407, 11, 558, 586, 11, 370, 510, 321, 366, 732, 3259, 934, 4025, 13, 492, 3047, 300, 11, 1392, 11, 321, 362, 341, 51196], "temperature": 0.0, "avg_logprob": -0.1286745427924896, "compression_ratio": 1.656, "no_speech_prob": 0.031594932079315186}, {"id": 1613, "seek": 844344, "start": 8460.08, "end": 8464.640000000001, "text": " incredible set of technology that we think might be good, but we have not given it to", "tokens": [51196, 4651, 992, 295, 2899, 300, 321, 519, 1062, 312, 665, 11, 457, 321, 362, 406, 2212, 309, 281, 51424], "temperature": 0.0, "avg_logprob": -0.1286745427924896, "compression_ratio": 1.656, "no_speech_prob": 0.031594932079315186}, {"id": 1614, "seek": 844344, "start": 8465.36, "end": 8469.84, "text": " lots of people yet. And so, we were very conservative and said, let's put it in a workbook", "tokens": [51460, 3195, 295, 561, 1939, 13, 400, 370, 11, 321, 645, 588, 13780, 293, 848, 11, 718, 311, 829, 309, 294, 257, 589, 2939, 51684], "temperature": 0.0, "avg_logprob": -0.1286745427924896, "compression_ratio": 1.656, "no_speech_prob": 0.031594932079315186}, {"id": 1615, "seek": 846984, "start": 8469.84, "end": 8473.28, "text": " so that if it crashes, we can do something about it. We can monitor and track that.", "tokens": [50364, 370, 300, 498, 309, 28642, 11, 321, 393, 360, 746, 466, 309, 13, 492, 393, 6002, 293, 2837, 300, 13, 50536], "temperature": 0.0, "avg_logprob": -0.13803579511433622, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0018099219305440784}, {"id": 1616, "seek": 846984, "start": 8473.28, "end": 8479.36, "text": " Right. And so, again, things are still super early, but we're having one person a minute", "tokens": [50536, 1779, 13, 400, 370, 11, 797, 11, 721, 366, 920, 1687, 2440, 11, 457, 321, 434, 1419, 472, 954, 257, 3456, 50840], "temperature": 0.0, "avg_logprob": -0.13803579511433622, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0018099219305440784}, {"id": 1617, "seek": 846984, "start": 8480.08, "end": 8486.0, "text": " sign up with over 70,000 people two weeks in. It's kind of crazy.", "tokens": [50876, 1465, 493, 365, 670, 5285, 11, 1360, 561, 732, 3259, 294, 13, 467, 311, 733, 295, 3219, 13, 51172], "temperature": 0.0, "avg_logprob": -0.13803579511433622, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0018099219305440784}, {"id": 1618, "seek": 846984, "start": 8486.0, "end": 8490.72, "text": " So, you can sign up to Mojo Playground and you can use it in the cloud.", "tokens": [51172, 407, 11, 291, 393, 1465, 493, 281, 3335, 5134, 5506, 2921, 293, 291, 393, 764, 309, 294, 264, 4588, 13, 51408], "temperature": 0.0, "avg_logprob": -0.13803579511433622, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0018099219305440784}, {"id": 1619, "seek": 846984, "start": 8490.72, "end": 8493.76, "text": " Yeah. In your browser. And so, what that's running on a workbook?", "tokens": [51408, 865, 13, 682, 428, 11185, 13, 400, 370, 11, 437, 300, 311, 2614, 322, 257, 589, 2939, 30, 51560], "temperature": 0.0, "avg_logprob": -0.13803579511433622, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0018099219305440784}, {"id": 1620, "seek": 846984, "start": 8493.76, "end": 8499.52, "text": " Yeah. What that's running on is, that's running on cloud VMs. And so, you share a machine with a", "tokens": [51560, 865, 13, 708, 300, 311, 2614, 322, 307, 11, 300, 311, 2614, 322, 4588, 18038, 82, 13, 400, 370, 11, 291, 2073, 257, 3479, 365, 257, 51848], "temperature": 0.0, "avg_logprob": -0.13803579511433622, "compression_ratio": 1.6953405017921146, "no_speech_prob": 0.0018099219305440784}, {"id": 1621, "seek": 849952, "start": 8499.52, "end": 8503.76, "text": " bunch of other people, but it turns out there's a bunch of them now because there's a lot of people.", "tokens": [50364, 3840, 295, 661, 561, 11, 457, 309, 4523, 484, 456, 311, 257, 3840, 295, 552, 586, 570, 456, 311, 257, 688, 295, 561, 13, 50576], "temperature": 0.0, "avg_logprob": -0.12133216857910156, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.001809596549719572}, {"id": 1622, "seek": 849952, "start": 8503.76, "end": 8506.720000000001, "text": " And so, what you're doing is you're getting free compute and you're getting to play with", "tokens": [50576, 400, 370, 11, 437, 291, 434, 884, 307, 291, 434, 1242, 1737, 14722, 293, 291, 434, 1242, 281, 862, 365, 50724], "temperature": 0.0, "avg_logprob": -0.12133216857910156, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.001809596549719572}, {"id": 1623, "seek": 849952, "start": 8506.720000000001, "end": 8511.12, "text": " this thing in kind of a limited controlled way so that we can make sure that it doesn't", "tokens": [50724, 341, 551, 294, 733, 295, 257, 5567, 10164, 636, 370, 300, 321, 393, 652, 988, 300, 309, 1177, 380, 50944], "temperature": 0.0, "avg_logprob": -0.12133216857910156, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.001809596549719572}, {"id": 1624, "seek": 849952, "start": 8512.16, "end": 8515.36, "text": " totally crash and be embarrassing. Right? Yeah.", "tokens": [50996, 3879, 8252, 293, 312, 17299, 13, 1779, 30, 865, 13, 51156], "temperature": 0.0, "avg_logprob": -0.12133216857910156, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.001809596549719572}, {"id": 1625, "seek": 849952, "start": 8515.36, "end": 8518.720000000001, "text": " So, now a lot of the feedback we've gotten is people want to download it around locally.", "tokens": [51156, 407, 11, 586, 257, 688, 295, 264, 5824, 321, 600, 5768, 307, 561, 528, 281, 5484, 309, 926, 16143, 13, 51324], "temperature": 0.0, "avg_logprob": -0.12133216857910156, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.001809596549719572}, {"id": 1626, "seek": 849952, "start": 8518.720000000001, "end": 8522.880000000001, "text": " So, we're working on that right now. So, that's the goal to be able to download locally?", "tokens": [51324, 407, 11, 321, 434, 1364, 322, 300, 558, 586, 13, 407, 11, 300, 311, 264, 3387, 281, 312, 1075, 281, 5484, 16143, 30, 51532], "temperature": 0.0, "avg_logprob": -0.12133216857910156, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.001809596549719572}, {"id": 1627, "seek": 849952, "start": 8522.880000000001, "end": 8525.84, "text": " Yeah. Yeah. That's what everybody expects. And so, we're working on that right now.", "tokens": [51532, 865, 13, 865, 13, 663, 311, 437, 2201, 33280, 13, 400, 370, 11, 321, 434, 1364, 322, 300, 558, 586, 13, 51680], "temperature": 0.0, "avg_logprob": -0.12133216857910156, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.001809596549719572}, {"id": 1628, "seek": 849952, "start": 8525.84, "end": 8527.68, "text": " And so, we just want to make sure that we do it right.", "tokens": [51680, 400, 370, 11, 321, 445, 528, 281, 652, 988, 300, 321, 360, 309, 558, 13, 51772], "temperature": 0.0, "avg_logprob": -0.12133216857910156, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.001809596549719572}, {"id": 1629, "seek": 852768, "start": 8527.68, "end": 8531.68, "text": " And I think this is one of the lessons I learned from Swift also, by the way.", "tokens": [50364, 400, 286, 519, 341, 307, 472, 295, 264, 8820, 286, 3264, 490, 25539, 611, 11, 538, 264, 636, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14781249364217122, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.0015972382389008999}, {"id": 1630, "seek": 852768, "start": 8532.48, "end": 8536.800000000001, "text": " Is it when we launched Swift? Gosh, it feels like forever ago. It was 2014.", "tokens": [50604, 1119, 309, 562, 321, 8730, 25539, 30, 19185, 11, 309, 3417, 411, 5680, 2057, 13, 467, 390, 8227, 13, 50820], "temperature": 0.0, "avg_logprob": -0.14781249364217122, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.0015972382389008999}, {"id": 1631, "seek": 852768, "start": 8537.36, "end": 8543.76, "text": " And we, I mean, it was super exciting. I and we, the team had worked on Swift for a number of years", "tokens": [50848, 400, 321, 11, 286, 914, 11, 309, 390, 1687, 4670, 13, 286, 293, 321, 11, 264, 1469, 632, 2732, 322, 25539, 337, 257, 1230, 295, 924, 51168], "temperature": 0.0, "avg_logprob": -0.14781249364217122, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.0015972382389008999}, {"id": 1632, "seek": 852768, "start": 8543.76, "end": 8551.04, "text": " in secrecy. Okay. And we, four years into this development, roughly, of working on this thing,", "tokens": [51168, 294, 34432, 1344, 13, 1033, 13, 400, 321, 11, 1451, 924, 666, 341, 3250, 11, 9810, 11, 295, 1364, 322, 341, 551, 11, 51532], "temperature": 0.0, "avg_logprob": -0.14781249364217122, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.0015972382389008999}, {"id": 1633, "seek": 852768, "start": 8551.92, "end": 8556.48, "text": " at that point, about 250 people at Apple knew about it. Okay. So, it was secret.", "tokens": [51576, 412, 300, 935, 11, 466, 11650, 561, 412, 6373, 2586, 466, 309, 13, 1033, 13, 407, 11, 309, 390, 4054, 13, 51804], "temperature": 0.0, "avg_logprob": -0.14781249364217122, "compression_ratio": 1.618867924528302, "no_speech_prob": 0.0015972382389008999}, {"id": 1634, "seek": 855648, "start": 8556.48, "end": 8561.439999999999, "text": " Apple's good at secrecy. And it was a secret project. And so, we launched this at WWDC,", "tokens": [50364, 6373, 311, 665, 412, 34432, 1344, 13, 400, 309, 390, 257, 4054, 1716, 13, 400, 370, 11, 321, 8730, 341, 412, 12040, 25619, 11, 50612], "temperature": 0.0, "avg_logprob": -0.12972454870900801, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.001032075029797852}, {"id": 1635, "seek": 855648, "start": 8561.439999999999, "end": 8565.52, "text": " a bunch of hoopla and excitement, and said, developers, you're going to be able to develop", "tokens": [50612, 257, 3840, 295, 29749, 875, 293, 14755, 11, 293, 848, 11, 8849, 11, 291, 434, 516, 281, 312, 1075, 281, 1499, 50816], "temperature": 0.0, "avg_logprob": -0.12972454870900801, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.001032075029797852}, {"id": 1636, "seek": 855648, "start": 8565.52, "end": 8571.359999999999, "text": " and submit apps to the App Store in three months. Okay. Well, several interesting things happened,", "tokens": [50816, 293, 10315, 7733, 281, 264, 3132, 17242, 294, 1045, 2493, 13, 1033, 13, 1042, 11, 2940, 1880, 721, 2011, 11, 51108], "temperature": 0.0, "avg_logprob": -0.12972454870900801, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.001032075029797852}, {"id": 1637, "seek": 855648, "start": 8571.359999999999, "end": 8577.359999999999, "text": " right? So, first of all, we learned that, A, it had a lot of bugs. It was not actually production", "tokens": [51108, 558, 30, 407, 11, 700, 295, 439, 11, 321, 3264, 300, 11, 316, 11, 309, 632, 257, 688, 295, 15120, 13, 467, 390, 406, 767, 4265, 51408], "temperature": 0.0, "avg_logprob": -0.12972454870900801, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.001032075029797852}, {"id": 1638, "seek": 855648, "start": 8577.359999999999, "end": 8583.119999999999, "text": " quality. And it was extremely stressful in terms of like, trying to get it working for a bunch of", "tokens": [51408, 3125, 13, 400, 309, 390, 4664, 19108, 294, 2115, 295, 411, 11, 1382, 281, 483, 309, 1364, 337, 257, 3840, 295, 51696], "temperature": 0.0, "avg_logprob": -0.12972454870900801, "compression_ratio": 1.6254295532646048, "no_speech_prob": 0.001032075029797852}, {"id": 1639, "seek": 858312, "start": 8583.12, "end": 8587.84, "text": " people. And so, what happened was we went from zero to, you know, I don't know how many developers,", "tokens": [50364, 561, 13, 400, 370, 11, 437, 2011, 390, 321, 1437, 490, 4018, 281, 11, 291, 458, 11, 286, 500, 380, 458, 577, 867, 8849, 11, 50600], "temperature": 0.0, "avg_logprob": -0.09635838218357252, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.009707173332571983}, {"id": 1640, "seek": 858312, "start": 8587.84, "end": 8592.800000000001, "text": " Apple had at the time, but a lot of developers overnight, and they ran into a lot of bugs,", "tokens": [50600, 6373, 632, 412, 264, 565, 11, 457, 257, 688, 295, 8849, 13935, 11, 293, 436, 5872, 666, 257, 688, 295, 15120, 11, 50848], "temperature": 0.0, "avg_logprob": -0.09635838218357252, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.009707173332571983}, {"id": 1641, "seek": 858312, "start": 8592.800000000001, "end": 8596.800000000001, "text": " and it was really embarrassing. And it was very stressful for everybody involved, right? It was", "tokens": [50848, 293, 309, 390, 534, 17299, 13, 400, 309, 390, 588, 19108, 337, 2201, 3288, 11, 558, 30, 467, 390, 51048], "temperature": 0.0, "avg_logprob": -0.09635838218357252, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.009707173332571983}, {"id": 1642, "seek": 858312, "start": 8596.800000000001, "end": 8601.04, "text": " also very exciting because everybody was excited about that. The other thing I learned is that,", "tokens": [51048, 611, 588, 4670, 570, 2201, 390, 2919, 466, 300, 13, 440, 661, 551, 286, 3264, 307, 300, 11, 51260], "temperature": 0.0, "avg_logprob": -0.09635838218357252, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.009707173332571983}, {"id": 1643, "seek": 858312, "start": 8601.04, "end": 8605.6, "text": " when that happened, roughly every software engineer who did not know about the project at Apple,", "tokens": [51260, 562, 300, 2011, 11, 9810, 633, 4722, 11403, 567, 630, 406, 458, 466, 264, 1716, 412, 6373, 11, 51488], "temperature": 0.0, "avg_logprob": -0.09635838218357252, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.009707173332571983}, {"id": 1644, "seek": 858312, "start": 8605.6, "end": 8610.08, "text": " their head exploded when it was launched, because they didn't know it was coming. And so, they're", "tokens": [51488, 641, 1378, 27049, 562, 309, 390, 8730, 11, 570, 436, 994, 380, 458, 309, 390, 1348, 13, 400, 370, 11, 436, 434, 51712], "temperature": 0.0, "avg_logprob": -0.09635838218357252, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.009707173332571983}, {"id": 1645, "seek": 861008, "start": 8610.08, "end": 8613.84, "text": " like, wait, what is this? I signed up to work for Apple because I love Objective C. Why is there a", "tokens": [50364, 411, 11, 1699, 11, 437, 307, 341, 30, 286, 8175, 493, 281, 589, 337, 6373, 570, 286, 959, 24753, 488, 383, 13, 1545, 307, 456, 257, 50552], "temperature": 0.0, "avg_logprob": -0.10865672059761461, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0023964587599039078}, {"id": 1646, "seek": 861008, "start": 8613.84, "end": 8622.24, "text": " new thing, right? And so, now, what that meant practically is that the push from launch to,", "tokens": [50552, 777, 551, 11, 558, 30, 400, 370, 11, 586, 11, 437, 300, 4140, 15667, 307, 300, 264, 2944, 490, 4025, 281, 11, 50972], "temperature": 0.0, "avg_logprob": -0.10865672059761461, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0023964587599039078}, {"id": 1647, "seek": 861008, "start": 8622.24, "end": 8628.88, "text": " first of all, the fall, but then to 2.0 and 3.0 and like, all the way forward was super painful", "tokens": [50972, 700, 295, 439, 11, 264, 2100, 11, 457, 550, 281, 568, 13, 15, 293, 805, 13, 15, 293, 411, 11, 439, 264, 636, 2128, 390, 1687, 11697, 51304], "temperature": 0.0, "avg_logprob": -0.10865672059761461, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0023964587599039078}, {"id": 1648, "seek": 861008, "start": 8628.88, "end": 8634.56, "text": " for the engineering team and myself. It was very stressful. The developer community was very grumpy", "tokens": [51304, 337, 264, 7043, 1469, 293, 2059, 13, 467, 390, 588, 19108, 13, 440, 10754, 1768, 390, 588, 677, 36142, 51588], "temperature": 0.0, "avg_logprob": -0.10865672059761461, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0023964587599039078}, {"id": 1649, "seek": 861008, "start": 8634.56, "end": 8638.0, "text": " about it because they're like, okay, well, wait a second, you're changing and breaking my code and", "tokens": [51588, 466, 309, 570, 436, 434, 411, 11, 1392, 11, 731, 11, 1699, 257, 1150, 11, 291, 434, 4473, 293, 7697, 452, 3089, 293, 51760], "temperature": 0.0, "avg_logprob": -0.10865672059761461, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.0023964587599039078}, {"id": 1650, "seek": 863800, "start": 8638.48, "end": 8643.6, "text": " we have to fix the bugs. And it was just a lot of tension and friction on all sides.", "tokens": [50388, 321, 362, 281, 3191, 264, 15120, 13, 400, 309, 390, 445, 257, 688, 295, 8980, 293, 17710, 322, 439, 4881, 13, 50644], "temperature": 0.0, "avg_logprob": -0.0846493705626457, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.002182544209063053}, {"id": 1651, "seek": 863800, "start": 8644.96, "end": 8649.44, "text": " There's a lot of technical debt in the compiler because we have to run really fast. You have to", "tokens": [50712, 821, 311, 257, 688, 295, 6191, 7831, 294, 264, 31958, 570, 321, 362, 281, 1190, 534, 2370, 13, 509, 362, 281, 50936], "temperature": 0.0, "avg_logprob": -0.0846493705626457, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.002182544209063053}, {"id": 1652, "seek": 863800, "start": 8649.44, "end": 8653.04, "text": " go implement the thing and unblock the use case and do the thing. And you know it's not right,", "tokens": [50936, 352, 4445, 264, 551, 293, 517, 28830, 264, 764, 1389, 293, 360, 264, 551, 13, 400, 291, 458, 309, 311, 406, 558, 11, 51116], "temperature": 0.0, "avg_logprob": -0.0846493705626457, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.002182544209063053}, {"id": 1653, "seek": 863800, "start": 8653.04, "end": 8657.6, "text": " but you never have time to go back and do it, right? And I'm very proud of the Swift team because", "tokens": [51116, 457, 291, 1128, 362, 565, 281, 352, 646, 293, 360, 309, 11, 558, 30, 400, 286, 478, 588, 4570, 295, 264, 25539, 1469, 570, 51344], "temperature": 0.0, "avg_logprob": -0.0846493705626457, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.002182544209063053}, {"id": 1654, "seek": 863800, "start": 8657.6, "end": 8665.6, "text": " they've come, I mean, we, but they came so far and made so much progress over this time since", "tokens": [51344, 436, 600, 808, 11, 286, 914, 11, 321, 11, 457, 436, 1361, 370, 1400, 293, 1027, 370, 709, 4205, 670, 341, 565, 1670, 51744], "temperature": 0.0, "avg_logprob": -0.0846493705626457, "compression_ratio": 1.7169117647058822, "no_speech_prob": 0.002182544209063053}, {"id": 1655, "seek": 866560, "start": 8665.6, "end": 8670.16, "text": " launch. It's pretty incredible and Swift is a very, very good thing, but I just don't want to do", "tokens": [50364, 4025, 13, 467, 311, 1238, 4651, 293, 25539, 307, 257, 588, 11, 588, 665, 551, 11, 457, 286, 445, 500, 380, 528, 281, 360, 50592], "temperature": 0.0, "avg_logprob": -0.10910097520742844, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.015421259216964245}, {"id": 1656, "seek": 866560, "start": 8670.16, "end": 8676.48, "text": " that again, right? And so, iterate more through the development process. And so, what we're doing is", "tokens": [50592, 300, 797, 11, 558, 30, 400, 370, 11, 44497, 544, 807, 264, 3250, 1399, 13, 400, 370, 11, 437, 321, 434, 884, 307, 50908], "temperature": 0.0, "avg_logprob": -0.10910097520742844, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.015421259216964245}, {"id": 1657, "seek": 866560, "start": 8676.48, "end": 8681.6, "text": " we're not launching it when it's hopefully 0.9 with no testers. We're launching it and saying it's", "tokens": [50908, 321, 434, 406, 18354, 309, 562, 309, 311, 4696, 1958, 13, 24, 365, 572, 1500, 433, 13, 492, 434, 18354, 309, 293, 1566, 309, 311, 51164], "temperature": 0.0, "avg_logprob": -0.10910097520742844, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.015421259216964245}, {"id": 1658, "seek": 866560, "start": 8681.6, "end": 8686.4, "text": " 0.1, right? And so, we're saying expectations of saying like, okay, well, don't use this for", "tokens": [51164, 1958, 13, 16, 11, 558, 30, 400, 370, 11, 321, 434, 1566, 9843, 295, 1566, 411, 11, 1392, 11, 731, 11, 500, 380, 764, 341, 337, 51404], "temperature": 0.0, "avg_logprob": -0.10910097520742844, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.015421259216964245}, {"id": 1659, "seek": 866560, "start": 8686.4, "end": 8691.92, "text": " production, right? If you're interested in what we're doing, we'll do it in an open way and we", "tokens": [51404, 4265, 11, 558, 30, 759, 291, 434, 3102, 294, 437, 321, 434, 884, 11, 321, 603, 360, 309, 294, 364, 1269, 636, 293, 321, 51680], "temperature": 0.0, "avg_logprob": -0.10910097520742844, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.015421259216964245}, {"id": 1660, "seek": 869192, "start": 8691.92, "end": 8696.64, "text": " can do it together, but don't use it in production yet. Like, we'll get there, but let's do it the", "tokens": [50364, 393, 360, 309, 1214, 11, 457, 500, 380, 764, 309, 294, 4265, 1939, 13, 1743, 11, 321, 603, 483, 456, 11, 457, 718, 311, 360, 309, 264, 50600], "temperature": 0.0, "avg_logprob": -0.07971348134122154, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.015419445931911469}, {"id": 1661, "seek": 869192, "start": 8696.64, "end": 8702.88, "text": " right way. And I'm also saying we're not in a race. The thing that I want to do is build the", "tokens": [50600, 558, 636, 13, 400, 286, 478, 611, 1566, 321, 434, 406, 294, 257, 4569, 13, 440, 551, 300, 286, 528, 281, 360, 307, 1322, 264, 50912], "temperature": 0.0, "avg_logprob": -0.07971348134122154, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.015419445931911469}, {"id": 1662, "seek": 869192, "start": 8702.88, "end": 8708.64, "text": " world's best thing, right? Because if you do it right and it lifts the industry, it doesn't matter", "tokens": [50912, 1002, 311, 1151, 551, 11, 558, 30, 1436, 498, 291, 360, 309, 558, 293, 309, 30501, 264, 3518, 11, 309, 1177, 380, 1871, 51200], "temperature": 0.0, "avg_logprob": -0.07971348134122154, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.015419445931911469}, {"id": 1663, "seek": 869192, "start": 8708.64, "end": 8714.08, "text": " if it takes an extra two months. Like, two months is worth waiting. And so, doing it right and not", "tokens": [51200, 498, 309, 2516, 364, 2857, 732, 2493, 13, 1743, 11, 732, 2493, 307, 3163, 3806, 13, 400, 370, 11, 884, 309, 558, 293, 406, 51472], "temperature": 0.0, "avg_logprob": -0.07971348134122154, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.015419445931911469}, {"id": 1664, "seek": 869192, "start": 8714.08, "end": 8720.32, "text": " being overwhelmed with technical debt and things like this is like, again, war wounds, lessons", "tokens": [51472, 885, 19042, 365, 6191, 7831, 293, 721, 411, 341, 307, 411, 11, 797, 11, 1516, 21969, 11, 8820, 51784], "temperature": 0.0, "avg_logprob": -0.07971348134122154, "compression_ratio": 1.7163120567375887, "no_speech_prob": 0.015419445931911469}, {"id": 1665, "seek": 872032, "start": 8720.32, "end": 8724.48, "text": " learned, whatever you want to say, I think is absolutely the right thing to do. Even though,", "tokens": [50364, 3264, 11, 2035, 291, 528, 281, 584, 11, 286, 519, 307, 3122, 264, 558, 551, 281, 360, 13, 2754, 1673, 11, 50572], "temperature": 0.0, "avg_logprob": -0.13041982650756836, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.01541710365563631}, {"id": 1666, "seek": 872032, "start": 8724.48, "end": 8729.039999999999, "text": " right now, people are very frustrated that you can't download it or it doesn't have feature X", "tokens": [50572, 558, 586, 11, 561, 366, 588, 15751, 300, 291, 393, 380, 5484, 309, 420, 309, 1177, 380, 362, 4111, 1783, 50800], "temperature": 0.0, "avg_logprob": -0.13041982650756836, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.01541710365563631}, {"id": 1667, "seek": 872032, "start": 8729.039999999999, "end": 8736.88, "text": " or something like this. What have you learned in a little bit of time since it's been released", "tokens": [50800, 420, 746, 411, 341, 13, 708, 362, 291, 3264, 294, 257, 707, 857, 295, 565, 1670, 309, 311, 668, 4736, 51192], "temperature": 0.0, "avg_logprob": -0.13041982650756836, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.01541710365563631}, {"id": 1668, "seek": 872032, "start": 8736.88, "end": 8742.48, "text": " into the wild that people have been complaining about feature X or Y or Z? What have they been", "tokens": [51192, 666, 264, 4868, 300, 561, 362, 668, 20740, 466, 4111, 1783, 420, 398, 420, 1176, 30, 708, 362, 436, 668, 51472], "temperature": 0.0, "avg_logprob": -0.13041982650756836, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.01541710365563631}, {"id": 1669, "seek": 872032, "start": 8742.48, "end": 8748.72, "text": " complaining about? What have they been excited about? Like, almost like detailed things versus", "tokens": [51472, 20740, 466, 30, 708, 362, 436, 668, 2919, 466, 30, 1743, 11, 1920, 411, 9942, 721, 5717, 51784], "temperature": 0.0, "avg_logprob": -0.13041982650756836, "compression_ratio": 1.8185328185328185, "no_speech_prob": 0.01541710365563631}, {"id": 1670, "seek": 874872, "start": 8748.72, "end": 8752.8, "text": " a big vision. I think everyone would be very excited about the big vision.", "tokens": [50364, 257, 955, 5201, 13, 286, 519, 1518, 576, 312, 588, 2919, 466, 264, 955, 5201, 13, 50568], "temperature": 0.0, "avg_logprob": -0.12277477787386987, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0043290965259075165}, {"id": 1671, "seek": 874872, "start": 8752.8, "end": 8756.48, "text": " Yeah. Yeah. Well, so, I mean, I've been very pleased. In fact, I mean, we've been massively", "tokens": [50568, 865, 13, 865, 13, 1042, 11, 370, 11, 286, 914, 11, 286, 600, 668, 588, 10587, 13, 682, 1186, 11, 286, 914, 11, 321, 600, 668, 29379, 50752], "temperature": 0.0, "avg_logprob": -0.12277477787386987, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0043290965259075165}, {"id": 1672, "seek": 874872, "start": 8756.48, "end": 8761.679999999998, "text": " overwhelmed with response, which is a good problem to have. It's kind of like a success", "tokens": [50752, 19042, 365, 4134, 11, 597, 307, 257, 665, 1154, 281, 362, 13, 467, 311, 733, 295, 411, 257, 2245, 51012], "temperature": 0.0, "avg_logprob": -0.12277477787386987, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0043290965259075165}, {"id": 1673, "seek": 874872, "start": 8761.679999999998, "end": 8769.039999999999, "text": " disaster in a sense, right? And so, I mean, if you go back in time, when we started Modular,", "tokens": [51012, 11293, 294, 257, 2020, 11, 558, 30, 400, 370, 11, 286, 914, 11, 498, 291, 352, 646, 294, 565, 11, 562, 321, 1409, 6583, 1040, 11, 51380], "temperature": 0.0, "avg_logprob": -0.12277477787386987, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0043290965259075165}, {"id": 1674, "seek": 874872, "start": 8769.039999999999, "end": 8774.48, "text": " which is just not yet a year and a half ago, so it's still a pretty new company, new team,", "tokens": [51380, 597, 307, 445, 406, 1939, 257, 1064, 293, 257, 1922, 2057, 11, 370, 309, 311, 920, 257, 1238, 777, 2237, 11, 777, 1469, 11, 51652], "temperature": 0.0, "avg_logprob": -0.12277477787386987, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0043290965259075165}, {"id": 1675, "seek": 877448, "start": 8775.039999999999, "end": 8781.039999999999, "text": " small but very good team of people, like we started with extreme conviction that there's a set of", "tokens": [50392, 1359, 457, 588, 665, 1469, 295, 561, 11, 411, 321, 1409, 365, 8084, 24837, 300, 456, 311, 257, 992, 295, 50692], "temperature": 0.0, "avg_logprob": -0.09745461253796593, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.044666871428489685}, {"id": 1676, "seek": 877448, "start": 8781.039999999999, "end": 8784.8, "text": " problems that we need to solve. And if we solve it, then people will be interested in what we're", "tokens": [50692, 2740, 300, 321, 643, 281, 5039, 13, 400, 498, 321, 5039, 309, 11, 550, 561, 486, 312, 3102, 294, 437, 321, 434, 50880], "temperature": 0.0, "avg_logprob": -0.09745461253796593, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.044666871428489685}, {"id": 1677, "seek": 877448, "start": 8784.8, "end": 8790.72, "text": " doing, right? But again, you're building in basically secret, right? You're trying to figure", "tokens": [50880, 884, 11, 558, 30, 583, 797, 11, 291, 434, 2390, 294, 1936, 4054, 11, 558, 30, 509, 434, 1382, 281, 2573, 51176], "temperature": 0.0, "avg_logprob": -0.09745461253796593, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.044666871428489685}, {"id": 1678, "seek": 877448, "start": 8790.72, "end": 8795.84, "text": " it out. Creation is a messy process. You're having to go through different paths and understand what", "tokens": [51176, 309, 484, 13, 42874, 307, 257, 16191, 1399, 13, 509, 434, 1419, 281, 352, 807, 819, 14518, 293, 1223, 437, 51432], "temperature": 0.0, "avg_logprob": -0.09745461253796593, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.044666871428489685}, {"id": 1679, "seek": 877448, "start": 8795.84, "end": 8799.84, "text": " you want to do and how to explain it. Often, when you're doing disruptive and new kinds of things,", "tokens": [51432, 291, 528, 281, 360, 293, 577, 281, 2903, 309, 13, 20043, 11, 562, 291, 434, 884, 37865, 293, 777, 3685, 295, 721, 11, 51632], "temperature": 0.0, "avg_logprob": -0.09745461253796593, "compression_ratio": 1.6508474576271186, "no_speech_prob": 0.044666871428489685}, {"id": 1680, "seek": 879984, "start": 8800.8, "end": 8806.16, "text": " just knowing how to explain it is super difficult, right? And so, when we launched,", "tokens": [50412, 445, 5276, 577, 281, 2903, 309, 307, 1687, 2252, 11, 558, 30, 400, 370, 11, 562, 321, 8730, 11, 50680], "temperature": 0.0, "avg_logprob": -0.10215627846597623, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.007119887974113226}, {"id": 1681, "seek": 879984, "start": 8806.16, "end": 8811.68, "text": " we hoped people would be excited. But, you know, I'm an optimist, but I'm also like,", "tokens": [50680, 321, 19737, 561, 576, 312, 2919, 13, 583, 11, 291, 458, 11, 286, 478, 364, 5028, 468, 11, 457, 286, 478, 611, 411, 11, 50956], "temperature": 0.0, "avg_logprob": -0.10215627846597623, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.007119887974113226}, {"id": 1682, "seek": 879984, "start": 8811.68, "end": 8815.52, "text": " don't want to get ahead of myself. And so, when people found out about Mojo,", "tokens": [50956, 500, 380, 528, 281, 483, 2286, 295, 2059, 13, 400, 370, 11, 562, 561, 1352, 484, 466, 3335, 5134, 11, 51148], "temperature": 0.0, "avg_logprob": -0.10215627846597623, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.007119887974113226}, {"id": 1683, "seek": 879984, "start": 8815.52, "end": 8821.12, "text": " I think their heads exploded a little bit, right? And, you know, here's, I think, a pretty credible", "tokens": [51148, 286, 519, 641, 8050, 27049, 257, 707, 857, 11, 558, 30, 400, 11, 291, 458, 11, 510, 311, 11, 286, 519, 11, 257, 1238, 32757, 51428], "temperature": 0.0, "avg_logprob": -0.10215627846597623, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.007119887974113226}, {"id": 1684, "seek": 879984, "start": 8821.12, "end": 8825.28, "text": " team that has built some languages and some tools before. And so, they have some lessons learned", "tokens": [51428, 1469, 300, 575, 3094, 512, 8650, 293, 512, 3873, 949, 13, 400, 370, 11, 436, 362, 512, 8820, 3264, 51636], "temperature": 0.0, "avg_logprob": -0.10215627846597623, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.007119887974113226}, {"id": 1685, "seek": 882528, "start": 8826.0, "end": 8830.480000000001, "text": " and are tackling some of the deep problems in the Python ecosystem and giving it the love", "tokens": [50400, 293, 366, 34415, 512, 295, 264, 2452, 2740, 294, 264, 15329, 11311, 293, 2902, 309, 264, 959, 50624], "temperature": 0.0, "avg_logprob": -0.061805925507476364, "compression_ratio": 2.00354609929078, "no_speech_prob": 0.1293460577726364}, {"id": 1686, "seek": 882528, "start": 8830.480000000001, "end": 8834.24, "text": " and attention that it should be getting. And I think people got very excited about that. And", "tokens": [50624, 293, 3202, 300, 309, 820, 312, 1242, 13, 400, 286, 519, 561, 658, 588, 2919, 466, 300, 13, 400, 50812], "temperature": 0.0, "avg_logprob": -0.061805925507476364, "compression_ratio": 2.00354609929078, "no_speech_prob": 0.1293460577726364}, {"id": 1687, "seek": 882528, "start": 8834.24, "end": 8838.320000000002, "text": " so, if you look at that, I mean, I think people are excited about ownership and taking a step", "tokens": [50812, 370, 11, 498, 291, 574, 412, 300, 11, 286, 914, 11, 286, 519, 561, 366, 2919, 466, 15279, 293, 1940, 257, 1823, 51016], "temperature": 0.0, "avg_logprob": -0.061805925507476364, "compression_ratio": 2.00354609929078, "no_speech_prob": 0.1293460577726364}, {"id": 1688, "seek": 882528, "start": 8838.320000000002, "end": 8841.28, "text": " beyond Rust, right? And there's people that are very excited about that. And there's people that", "tokens": [51016, 4399, 34952, 11, 558, 30, 400, 456, 311, 561, 300, 366, 588, 2919, 466, 300, 13, 400, 456, 311, 561, 300, 51164], "temperature": 0.0, "avg_logprob": -0.061805925507476364, "compression_ratio": 2.00354609929078, "no_speech_prob": 0.1293460577726364}, {"id": 1689, "seek": 882528, "start": 8841.28, "end": 8848.320000000002, "text": " are excited about, you know, just like, I made Game of Life go 400 times faster, right? And things", "tokens": [51164, 366, 2919, 466, 11, 291, 458, 11, 445, 411, 11, 286, 1027, 7522, 295, 7720, 352, 8423, 1413, 4663, 11, 558, 30, 400, 721, 51516], "temperature": 0.0, "avg_logprob": -0.061805925507476364, "compression_ratio": 2.00354609929078, "no_speech_prob": 0.1293460577726364}, {"id": 1690, "seek": 882528, "start": 8848.320000000002, "end": 8851.68, "text": " like that. And that's really cool. There are people that are really excited about the, okay,", "tokens": [51516, 411, 300, 13, 400, 300, 311, 534, 1627, 13, 821, 366, 561, 300, 366, 534, 2919, 466, 264, 11, 1392, 11, 51684], "temperature": 0.0, "avg_logprob": -0.061805925507476364, "compression_ratio": 2.00354609929078, "no_speech_prob": 0.1293460577726364}, {"id": 1691, "seek": 885168, "start": 8851.68, "end": 8854.48, "text": " I really hate writing stuff in C++. Save me.", "tokens": [50364, 286, 534, 4700, 3579, 1507, 294, 383, 25472, 13, 15541, 385, 13, 50504], "temperature": 0.0, "avg_logprob": -0.22693230853817328, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.03160044550895691}, {"id": 1692, "seek": 885168, "start": 8854.48, "end": 8857.76, "text": " Well, like systems in your, they're like stepping up like, oh, yes.", "tokens": [50504, 1042, 11, 411, 3652, 294, 428, 11, 436, 434, 411, 16821, 493, 411, 11, 1954, 11, 2086, 13, 50668], "temperature": 0.0, "avg_logprob": -0.22693230853817328, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.03160044550895691}, {"id": 1693, "seek": 885168, "start": 8857.76, "end": 8864.4, "text": " So, that's me, by the way. Also, I really want to stop writing C++. But the,", "tokens": [50668, 407, 11, 300, 311, 385, 11, 538, 264, 636, 13, 2743, 11, 286, 534, 528, 281, 1590, 3579, 383, 25472, 13, 583, 264, 11, 51000], "temperature": 0.0, "avg_logprob": -0.22693230853817328, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.03160044550895691}, {"id": 1694, "seek": 885168, "start": 8865.12, "end": 8871.12, "text": " I get third-person excitement when people tweet, hey, I made this code, Game of Life,", "tokens": [51036, 286, 483, 2636, 12, 10813, 14755, 562, 561, 15258, 11, 4177, 11, 286, 1027, 341, 3089, 11, 7522, 295, 7720, 11, 51336], "temperature": 0.0, "avg_logprob": -0.22693230853817328, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.03160044550895691}, {"id": 1695, "seek": 885168, "start": 8871.12, "end": 8874.0, "text": " or whatever, faster. And you're like, yeah.", "tokens": [51336, 420, 2035, 11, 4663, 13, 400, 291, 434, 411, 11, 1338, 13, 51480], "temperature": 0.0, "avg_logprob": -0.22693230853817328, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.03160044550895691}, {"id": 1696, "seek": 885168, "start": 8874.0, "end": 8881.52, "text": " Yeah. And also, like, I would also say that, let me cast blame out to people who deserve", "tokens": [51480, 865, 13, 400, 611, 11, 411, 11, 286, 576, 611, 584, 300, 11, 718, 385, 4193, 10127, 484, 281, 561, 567, 9948, 51856], "temperature": 0.0, "avg_logprob": -0.22693230853817328, "compression_ratio": 1.6062992125984252, "no_speech_prob": 0.03160044550895691}, {"id": 1697, "seek": 888152, "start": 8881.6, "end": 8882.48, "text": " it. Sure.", "tokens": [50368, 309, 13, 4894, 13, 50412], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1698, "seek": 888152, "start": 8882.48, "end": 8885.76, "text": " These terrible people who convinced me to do some of this.", "tokens": [50412, 1981, 6237, 561, 567, 12561, 385, 281, 360, 512, 295, 341, 13, 50576], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1699, "seek": 888152, "start": 8885.76, "end": 8886.560000000001, "text": " Yes.", "tokens": [50576, 1079, 13, 50616], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1700, "seek": 888152, "start": 8886.560000000001, "end": 8887.44, "text": " Jeremy Howard.", "tokens": [50616, 17809, 17626, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1701, "seek": 888152, "start": 8887.44, "end": 8888.0, "text": " Yes.", "tokens": [50660, 1079, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1702, "seek": 888152, "start": 8888.0, "end": 8888.560000000001, "text": " That guy.", "tokens": [50688, 663, 2146, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1703, "seek": 888152, "start": 8889.52, "end": 8891.28, "text": " Well, he's been pushing for this kind of thing.", "tokens": [50764, 1042, 11, 415, 311, 668, 7380, 337, 341, 733, 295, 551, 13, 50852], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1704, "seek": 888152, "start": 8891.28, "end": 8893.04, "text": " He's been pushing for more years.", "tokens": [50852, 634, 311, 668, 7380, 337, 544, 924, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1705, "seek": 888152, "start": 8893.04, "end": 8894.4, "text": " Yeah, he's wanted this for a long, long time.", "tokens": [50940, 865, 11, 415, 311, 1415, 341, 337, 257, 938, 11, 938, 565, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1706, "seek": 888152, "start": 8894.4, "end": 8895.92, "text": " He's wanted this for years.", "tokens": [51008, 634, 311, 1415, 341, 337, 924, 13, 51084], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1707, "seek": 888152, "start": 8895.92, "end": 8899.52, "text": " For people who don't know Jeremy Howard, he's like one of the most legit people in the machine", "tokens": [51084, 1171, 561, 567, 500, 380, 458, 17809, 17626, 11, 415, 311, 411, 472, 295, 264, 881, 10275, 561, 294, 264, 3479, 51264], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1708, "seek": 888152, "start": 8899.52, "end": 8905.84, "text": " learning community. He's a grassroots. He really teaches, he's an incredible educator,", "tokens": [51264, 2539, 1768, 13, 634, 311, 257, 39522, 13, 634, 534, 16876, 11, 415, 311, 364, 4651, 31237, 11, 51580], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1709, "seek": 888152, "start": 8905.84, "end": 8910.800000000001, "text": " he's an incredible teacher, but also legit in terms of a machine learning engineer himself.", "tokens": [51580, 415, 311, 364, 4651, 5027, 11, 457, 611, 10275, 294, 2115, 295, 257, 3479, 2539, 11403, 3647, 13, 51828], "temperature": 0.0, "avg_logprob": -0.1393040158101265, "compression_ratio": 1.9381818181818182, "no_speech_prob": 0.0028885523788630962}, {"id": 1710, "seek": 891080, "start": 8911.039999999999, "end": 8916.72, "text": " He's been running the fast.ai and looking, I think, for exactly what you've done.", "tokens": [50376, 634, 311, 668, 2614, 264, 2370, 13, 1301, 293, 1237, 11, 286, 519, 11, 337, 2293, 437, 291, 600, 1096, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1930233882023738, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0016998476348817348}, {"id": 1711, "seek": 891080, "start": 8916.72, "end": 8917.279999999999, "text": " Exactly.", "tokens": [50660, 7587, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1930233882023738, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0016998476348817348}, {"id": 1712, "seek": 891080, "start": 8917.279999999999, "end": 8924.64, "text": " And so, I mean, the first time, so I met Jeremy pretty early on, but the first time I sat up", "tokens": [50688, 400, 370, 11, 286, 914, 11, 264, 700, 565, 11, 370, 286, 1131, 17809, 1238, 2440, 322, 11, 457, 264, 700, 565, 286, 3227, 493, 51056], "temperature": 0.0, "avg_logprob": -0.1930233882023738, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0016998476348817348}, {"id": 1713, "seek": 891080, "start": 8924.64, "end": 8930.64, "text": " and I'm like, this guy is ridiculous is when I was at Google and we were bringing up TPUs and", "tokens": [51056, 293, 286, 478, 411, 11, 341, 2146, 307, 11083, 307, 562, 286, 390, 412, 3329, 293, 321, 645, 5062, 493, 314, 8115, 82, 293, 51356], "temperature": 0.0, "avg_logprob": -0.1930233882023738, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0016998476348817348}, {"id": 1714, "seek": 891080, "start": 8930.64, "end": 8937.119999999999, "text": " we had a whole team of people and where there was this competition called Don Bench of who can", "tokens": [51356, 321, 632, 257, 1379, 1469, 295, 561, 293, 689, 456, 390, 341, 6211, 1219, 1468, 3964, 339, 295, 567, 393, 51680], "temperature": 0.0, "avg_logprob": -0.1930233882023738, "compression_ratio": 1.5696202531645569, "no_speech_prob": 0.0016998476348817348}, {"id": 1715, "seek": 893712, "start": 8937.12, "end": 8940.720000000001, "text": " train ImageNet fastest.", "tokens": [50364, 3847, 29903, 31890, 14573, 13, 50544], "temperature": 0.0, "avg_logprob": -0.17849516868591309, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.017977656796574593}, {"id": 1716, "seek": 893712, "start": 8941.6, "end": 8948.800000000001, "text": " And Jeremy and one of his researchers crushed Google by not through sheer force of the amazing", "tokens": [50588, 400, 17809, 293, 472, 295, 702, 10309, 19889, 3329, 538, 406, 807, 23061, 3464, 295, 264, 2243, 50948], "temperature": 0.0, "avg_logprob": -0.17849516868591309, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.017977656796574593}, {"id": 1717, "seek": 893712, "start": 8948.800000000001, "end": 8953.6, "text": " amount of compute and the number of TPUs and stuff like that, that he just decided that progressive", "tokens": [50948, 2372, 295, 14722, 293, 264, 1230, 295, 314, 8115, 82, 293, 1507, 411, 300, 11, 300, 415, 445, 3047, 300, 16131, 51188], "temperature": 0.0, "avg_logprob": -0.17849516868591309, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.017977656796574593}, {"id": 1718, "seek": 893712, "start": 8953.6, "end": 8957.76, "text": " imagery sizing was the right way to train the model and if you were epochs faster and", "tokens": [51188, 24340, 45435, 390, 264, 558, 636, 281, 3847, 264, 2316, 293, 498, 291, 645, 30992, 28346, 4663, 293, 51396], "temperature": 0.0, "avg_logprob": -0.17849516868591309, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.017977656796574593}, {"id": 1719, "seek": 893712, "start": 8958.320000000002, "end": 8960.720000000001, "text": " make the whole thing go go vroom, right?", "tokens": [51424, 652, 264, 1379, 551, 352, 352, 371, 2861, 11, 558, 30, 51544], "temperature": 0.0, "avg_logprob": -0.17849516868591309, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.017977656796574593}, {"id": 1720, "seek": 893712, "start": 8960.720000000001, "end": 8963.44, "text": " And I'm like, this guy is incredible.", "tokens": [51544, 400, 286, 478, 411, 11, 341, 2146, 307, 4651, 13, 51680], "temperature": 0.0, "avg_logprob": -0.17849516868591309, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.017977656796574593}, {"id": 1721, "seek": 896344, "start": 8964.0, "end": 8968.0, "text": " So you can say, anyways, come back to where's Mojo coming from.", "tokens": [50392, 407, 291, 393, 584, 11, 13448, 11, 808, 646, 281, 689, 311, 3335, 5134, 1348, 490, 13, 50592], "temperature": 0.0, "avg_logprob": -0.166933353130634, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0021821679547429085}, {"id": 1722, "seek": 896344, "start": 8968.640000000001, "end": 8970.32, "text": " Chris finally listened to Jeremy.", "tokens": [50624, 6688, 2721, 13207, 281, 17809, 13, 50708], "temperature": 0.0, "avg_logprob": -0.166933353130634, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0021821679547429085}, {"id": 1723, "seek": 896344, "start": 8972.08, "end": 8973.04, "text": " It's all his fault.", "tokens": [50796, 467, 311, 439, 702, 7441, 13, 50844], "temperature": 0.0, "avg_logprob": -0.166933353130634, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0021821679547429085}, {"id": 1724, "seek": 896344, "start": 8973.04, "end": 8980.800000000001, "text": " But there's a kind of very refreshing, pragmatic view that he has about machine learning that", "tokens": [50844, 583, 456, 311, 257, 733, 295, 588, 19772, 11, 46904, 1910, 300, 415, 575, 466, 3479, 2539, 300, 51232], "temperature": 0.0, "avg_logprob": -0.166933353130634, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0021821679547429085}, {"id": 1725, "seek": 896344, "start": 8982.4, "end": 8989.76, "text": " I don't know if it's this mix of a desire for efficiency, but ultimately grounded and", "tokens": [51312, 286, 500, 380, 458, 498, 309, 311, 341, 2890, 295, 257, 7516, 337, 10493, 11, 457, 6284, 23535, 293, 51680], "temperature": 0.0, "avg_logprob": -0.166933353130634, "compression_ratio": 1.448780487804878, "no_speech_prob": 0.0021821679547429085}, {"id": 1726, "seek": 898976, "start": 8989.84, "end": 8993.68, "text": " desired to make machine learning more accessible to a lot of people.", "tokens": [50368, 14721, 281, 652, 3479, 2539, 544, 9515, 281, 257, 688, 295, 561, 13, 50560], "temperature": 0.0, "avg_logprob": -0.09889684617519379, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.007344190962612629}, {"id": 1727, "seek": 898976, "start": 8993.68, "end": 8994.56, "text": " I don't know what that is.", "tokens": [50560, 286, 500, 380, 458, 437, 300, 307, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09889684617519379, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.007344190962612629}, {"id": 1728, "seek": 898976, "start": 8994.56, "end": 9000.24, "text": " I guess that's coupled with efficiency and performance, but it's not just obsessed about", "tokens": [50604, 286, 2041, 300, 311, 29482, 365, 10493, 293, 3389, 11, 457, 309, 311, 406, 445, 16923, 466, 50888], "temperature": 0.0, "avg_logprob": -0.09889684617519379, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.007344190962612629}, {"id": 1729, "seek": 898976, "start": 9000.24, "end": 9001.2, "text": " performance.", "tokens": [50888, 3389, 13, 50936], "temperature": 0.0, "avg_logprob": -0.09889684617519379, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.007344190962612629}, {"id": 1730, "seek": 898976, "start": 9001.2, "end": 9006.48, "text": " So a lot of AI and AI research ends up being that it has to go fast enough to get scale.", "tokens": [50936, 407, 257, 688, 295, 7318, 293, 7318, 2132, 5314, 493, 885, 300, 309, 575, 281, 352, 2370, 1547, 281, 483, 4373, 13, 51200], "temperature": 0.0, "avg_logprob": -0.09889684617519379, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.007344190962612629}, {"id": 1731, "seek": 898976, "start": 9007.2, "end": 9010.800000000001, "text": " So a lot of people don't actually care about performance, particularly on the research side,", "tokens": [51236, 407, 257, 688, 295, 561, 500, 380, 767, 1127, 466, 3389, 11, 4098, 322, 264, 2132, 1252, 11, 51416], "temperature": 0.0, "avg_logprob": -0.09889684617519379, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.007344190962612629}, {"id": 1732, "seek": 898976, "start": 9010.800000000001, "end": 9014.0, "text": " until it allows them to have a bigger data set, right?", "tokens": [51416, 1826, 309, 4045, 552, 281, 362, 257, 3801, 1412, 992, 11, 558, 30, 51576], "temperature": 0.0, "avg_logprob": -0.09889684617519379, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.007344190962612629}, {"id": 1733, "seek": 898976, "start": 9014.0, "end": 9018.56, "text": " And so suddenly now you care about distributed compute and like all these exotic HPC,", "tokens": [51576, 400, 370, 5800, 586, 291, 1127, 466, 12631, 14722, 293, 411, 439, 613, 27063, 12557, 34, 11, 51804], "temperature": 0.0, "avg_logprob": -0.09889684617519379, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.007344190962612629}, {"id": 1734, "seek": 901856, "start": 9018.56, "end": 9020.16, "text": " like you don't actually want to know about that.", "tokens": [50364, 411, 291, 500, 380, 767, 528, 281, 458, 466, 300, 13, 50444], "temperature": 0.0, "avg_logprob": -0.10382884740829468, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0037064137868583202}, {"id": 1735, "seek": 901856, "start": 9020.16, "end": 9024.96, "text": " You just want to be able to do more experiments faster and do so with bigger data sets, right?", "tokens": [50444, 509, 445, 528, 281, 312, 1075, 281, 360, 544, 12050, 4663, 293, 360, 370, 365, 3801, 1412, 6352, 11, 558, 30, 50684], "temperature": 0.0, "avg_logprob": -0.10382884740829468, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0037064137868583202}, {"id": 1736, "seek": 901856, "start": 9024.96, "end": 9027.84, "text": " And so Jeremy has been really pushing the limits.", "tokens": [50684, 400, 370, 17809, 575, 668, 534, 7380, 264, 10406, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10382884740829468, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0037064137868583202}, {"id": 1737, "seek": 901856, "start": 9027.84, "end": 9031.519999999999, "text": " And one of the things I'll say about Jeremy, and there's many things I could say about Jeremy,", "tokens": [50828, 400, 472, 295, 264, 721, 286, 603, 584, 466, 17809, 11, 293, 456, 311, 867, 721, 286, 727, 584, 466, 17809, 11, 51012], "temperature": 0.0, "avg_logprob": -0.10382884740829468, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0037064137868583202}, {"id": 1738, "seek": 901856, "start": 9031.519999999999, "end": 9035.68, "text": " because I'm a fanboy of his, but he fits in his head.", "tokens": [51012, 570, 286, 478, 257, 3429, 12795, 295, 702, 11, 457, 415, 9001, 294, 702, 1378, 13, 51220], "temperature": 0.0, "avg_logprob": -0.10382884740829468, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0037064137868583202}, {"id": 1739, "seek": 901856, "start": 9036.72, "end": 9041.359999999999, "text": " And Jeremy actually takes the time where many people don't to really dive deep into", "tokens": [51272, 400, 17809, 767, 2516, 264, 565, 689, 867, 561, 500, 380, 281, 534, 9192, 2452, 666, 51504], "temperature": 0.0, "avg_logprob": -0.10382884740829468, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0037064137868583202}, {"id": 1740, "seek": 901856, "start": 9042.08, "end": 9046.32, "text": " why is the beta parameter of the atom optimizer equal to this, right?", "tokens": [51540, 983, 307, 264, 9861, 13075, 295, 264, 12018, 5028, 6545, 2681, 281, 341, 11, 558, 30, 51752], "temperature": 0.0, "avg_logprob": -0.10382884740829468, "compression_ratio": 1.7464788732394365, "no_speech_prob": 0.0037064137868583202}, {"id": 1741, "seek": 904632, "start": 9046.32, "end": 9051.199999999999, "text": " And he'll go survey and understand what are all the activation functions in the trade-offs", "tokens": [50364, 400, 415, 603, 352, 8984, 293, 1223, 437, 366, 439, 264, 24433, 6828, 294, 264, 4923, 12, 19231, 50608], "temperature": 0.0, "avg_logprob": -0.11334896902752738, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.005058678332716227}, {"id": 1742, "seek": 904632, "start": 9051.199999999999, "end": 9055.68, "text": " and why is it that everybody that does this model pick that thing.", "tokens": [50608, 293, 983, 307, 309, 300, 2201, 300, 775, 341, 2316, 1888, 300, 551, 13, 50832], "temperature": 0.0, "avg_logprob": -0.11334896902752738, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.005058678332716227}, {"id": 1743, "seek": 904632, "start": 9055.68, "end": 9060.64, "text": " So the why, not just trying different values, like really what is going on here.", "tokens": [50832, 407, 264, 983, 11, 406, 445, 1382, 819, 4190, 11, 411, 534, 437, 307, 516, 322, 510, 13, 51080], "temperature": 0.0, "avg_logprob": -0.11334896902752738, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.005058678332716227}, {"id": 1744, "seek": 904632, "start": 9060.64, "end": 9061.119999999999, "text": " Right.", "tokens": [51080, 1779, 13, 51104], "temperature": 0.0, "avg_logprob": -0.11334896902752738, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.005058678332716227}, {"id": 1745, "seek": 904632, "start": 9061.119999999999, "end": 9066.8, "text": " And so as a consequence of that, he's always, again, he makes time, but he spends time", "tokens": [51104, 400, 370, 382, 257, 18326, 295, 300, 11, 415, 311, 1009, 11, 797, 11, 415, 1669, 565, 11, 457, 415, 25620, 565, 51388], "temperature": 0.0, "avg_logprob": -0.11334896902752738, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.005058678332716227}, {"id": 1746, "seek": 904632, "start": 9066.8, "end": 9069.6, "text": " to understand things at a depth that a lot of people don't.", "tokens": [51388, 281, 1223, 721, 412, 257, 7161, 300, 257, 688, 295, 561, 500, 380, 13, 51528], "temperature": 0.0, "avg_logprob": -0.11334896902752738, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.005058678332716227}, {"id": 1747, "seek": 904632, "start": 9069.6, "end": 9072.32, "text": " And as you say, he then brings it and teaches people.", "tokens": [51528, 400, 382, 291, 584, 11, 415, 550, 5607, 309, 293, 16876, 561, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11334896902752738, "compression_ratio": 1.6704119850187267, "no_speech_prob": 0.005058678332716227}, {"id": 1748, "seek": 907232, "start": 9072.88, "end": 9078.64, "text": " And his mission is to help lift, his website says, making AI uncool again.", "tokens": [50392, 400, 702, 4447, 307, 281, 854, 5533, 11, 702, 3144, 1619, 11, 1455, 7318, 6219, 1092, 797, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14219467039030742, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.02032572217285633}, {"id": 1749, "seek": 907232, "start": 9078.64, "end": 9082.48, "text": " Like, it's about, like, forget about the hype, it's actually practical and useful.", "tokens": [50680, 1743, 11, 309, 311, 466, 11, 411, 11, 2870, 466, 264, 24144, 11, 309, 311, 767, 8496, 293, 4420, 13, 50872], "temperature": 0.0, "avg_logprob": -0.14219467039030742, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.02032572217285633}, {"id": 1750, "seek": 907232, "start": 9082.48, "end": 9084.08, "text": " Let's teach people how to do this, right?", "tokens": [50872, 961, 311, 2924, 561, 577, 281, 360, 341, 11, 558, 30, 50952], "temperature": 0.0, "avg_logprob": -0.14219467039030742, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.02032572217285633}, {"id": 1751, "seek": 907232, "start": 9084.72, "end": 9088.48, "text": " Now, the problem Jeremy struggled with is that he's pushing the envelope, right?", "tokens": [50984, 823, 11, 264, 1154, 17809, 19023, 365, 307, 300, 415, 311, 7380, 264, 19989, 11, 558, 30, 51172], "temperature": 0.0, "avg_logprob": -0.14219467039030742, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.02032572217285633}, {"id": 1752, "seek": 907232, "start": 9088.48, "end": 9093.36, "text": " Research isn't about doing the thing that is staying on the happy path or the well-paid road,", "tokens": [51172, 10303, 1943, 380, 466, 884, 264, 551, 300, 307, 7939, 322, 264, 2055, 3100, 420, 264, 731, 12, 35035, 3060, 11, 51416], "temperature": 0.0, "avg_logprob": -0.14219467039030742, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.02032572217285633}, {"id": 1753, "seek": 907232, "start": 9093.36, "end": 9094.08, "text": " right?", "tokens": [51416, 558, 30, 51452], "temperature": 0.0, "avg_logprob": -0.14219467039030742, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.02032572217285633}, {"id": 1754, "seek": 907232, "start": 9094.08, "end": 9098.64, "text": " And so a lot of the systems today have been these really fragile, fragmented things or", "tokens": [51452, 400, 370, 257, 688, 295, 264, 3652, 965, 362, 668, 613, 534, 23847, 11, 9241, 14684, 721, 420, 51680], "temperature": 0.0, "avg_logprob": -0.14219467039030742, "compression_ratio": 1.6537102473498233, "no_speech_prob": 0.02032572217285633}, {"id": 1755, "seek": 909864, "start": 9098.64, "end": 9100.16, "text": " special case in this happy path.", "tokens": [50364, 2121, 1389, 294, 341, 2055, 3100, 13, 50440], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1756, "seek": 909864, "start": 9100.16, "end": 9103.439999999999, "text": " And if you fall off the happy path, you get eaten by an alligator.", "tokens": [50440, 400, 498, 291, 2100, 766, 264, 2055, 3100, 11, 291, 483, 12158, 538, 364, 48095, 13, 50604], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1757, "seek": 909864, "start": 9105.279999999999, "end": 9112.0, "text": " So what about, so Python has this giant ecosystem of packages", "tokens": [50696, 407, 437, 466, 11, 370, 15329, 575, 341, 7410, 11311, 295, 17401, 51032], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1758, "seek": 909864, "start": 9112.88, "end": 9114.64, "text": " and is a package repository.", "tokens": [51076, 293, 307, 257, 7372, 25841, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1759, "seek": 909864, "start": 9114.64, "end": 9117.519999999999, "text": " Do you have ideas of how to do that well for Mojo?", "tokens": [51164, 1144, 291, 362, 3487, 295, 577, 281, 360, 300, 731, 337, 3335, 5134, 30, 51308], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1760, "seek": 909864, "start": 9118.32, "end": 9118.56, "text": " Yeah.", "tokens": [51348, 865, 13, 51360], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1761, "seek": 909864, "start": 9118.56, "end": 9120.64, "text": " How to do a repository of packages?", "tokens": [51360, 1012, 281, 360, 257, 25841, 295, 17401, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1762, "seek": 909864, "start": 9120.64, "end": 9124.0, "text": " Well, so that's another really interesting problem that I knew about,", "tokens": [51464, 1042, 11, 370, 300, 311, 1071, 534, 1880, 1154, 300, 286, 2586, 466, 11, 51632], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1763, "seek": 909864, "start": 9124.0, "end": 9126.96, "text": " but I didn't understand how big of a problem it was.", "tokens": [51632, 457, 286, 994, 380, 1223, 577, 955, 295, 257, 1154, 309, 390, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1349949070385524, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.014953885227441788}, {"id": 1764, "seek": 912696, "start": 9126.96, "end": 9131.039999999999, "text": " Python packaging, a lot of people have very big pain points", "tokens": [50364, 15329, 16836, 11, 257, 688, 295, 561, 362, 588, 955, 1822, 2793, 50568], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1765, "seek": 912696, "start": 9131.039999999999, "end": 9132.8, "text": " and a lot of scars with Python packaging.", "tokens": [50568, 293, 257, 688, 295, 31353, 365, 15329, 16836, 13, 50656], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1766, "seek": 912696, "start": 9132.8, "end": 9134.88, "text": " Oh, you mean, so there's several things to say.", "tokens": [50656, 876, 11, 291, 914, 11, 370, 456, 311, 2940, 721, 281, 584, 13, 50760], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1767, "seek": 912696, "start": 9134.88, "end": 9139.839999999998, "text": " Building and distributing and managing dependencies and versioning and all this stuff.", "tokens": [50760, 18974, 293, 41406, 293, 11642, 36606, 293, 3037, 278, 293, 439, 341, 1507, 13, 51008], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1768, "seek": 912696, "start": 9139.839999999998, "end": 9143.039999999999, "text": " So from the perspective of if you want to create your own package.", "tokens": [51008, 407, 490, 264, 4585, 295, 498, 291, 528, 281, 1884, 428, 1065, 7372, 13, 51168], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1769, "seek": 912696, "start": 9143.039999999999, "end": 9143.599999999999, "text": " Yes.", "tokens": [51168, 1079, 13, 51196], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1770, "seek": 912696, "start": 9143.599999999999, "end": 9146.8, "text": " And then, or you want to build on top of a bunch of other people's packages", "tokens": [51196, 400, 550, 11, 420, 291, 528, 281, 1322, 322, 1192, 295, 257, 3840, 295, 661, 561, 311, 17401, 51356], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1771, "seek": 912696, "start": 9146.8, "end": 9148.48, "text": " and then they get updated and things like this.", "tokens": [51356, 293, 550, 436, 483, 10588, 293, 721, 411, 341, 13, 51440], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1772, "seek": 912696, "start": 9149.039999999999, "end": 9153.439999999999, "text": " Now, I'm not an expert in this, so I don't know the answer.", "tokens": [51468, 823, 11, 286, 478, 406, 364, 5844, 294, 341, 11, 370, 286, 500, 380, 458, 264, 1867, 13, 51688], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1773, "seek": 912696, "start": 9153.439999999999, "end": 9156.64, "text": " I think this is one of the reasons why it's great that we work as a team and", "tokens": [51688, 286, 519, 341, 307, 472, 295, 264, 4112, 983, 309, 311, 869, 300, 321, 589, 382, 257, 1469, 293, 51848], "temperature": 0.0, "avg_logprob": -0.11781968434651692, "compression_ratio": 1.7507692307692309, "no_speech_prob": 0.014496560208499432}, {"id": 1774, "seek": 915696, "start": 9156.96, "end": 9159.039999999999, "text": " there's other really good and smart people involved.", "tokens": [50364, 456, 311, 661, 534, 665, 293, 4069, 561, 3288, 13, 50468], "temperature": 0.0, "avg_logprob": -0.07957553075364798, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0007319984724745154}, {"id": 1775, "seek": 915696, "start": 9160.8, "end": 9165.439999999999, "text": " But one of the things I've heard from smart people who've done a lot of this", "tokens": [50556, 583, 472, 295, 264, 721, 286, 600, 2198, 490, 4069, 561, 567, 600, 1096, 257, 688, 295, 341, 50788], "temperature": 0.0, "avg_logprob": -0.07957553075364798, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0007319984724745154}, {"id": 1776, "seek": 915696, "start": 9165.439999999999, "end": 9170.08, "text": " is that the packaging becomes a huge disaster when you get the Python and C together.", "tokens": [50788, 307, 300, 264, 16836, 3643, 257, 2603, 11293, 562, 291, 483, 264, 15329, 293, 383, 1214, 13, 51020], "temperature": 0.0, "avg_logprob": -0.07957553075364798, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0007319984724745154}, {"id": 1777, "seek": 915696, "start": 9171.039999999999, "end": 9175.839999999998, "text": " And so if you have this problem where you have code split between Python and C,", "tokens": [51068, 400, 370, 498, 291, 362, 341, 1154, 689, 291, 362, 3089, 7472, 1296, 15329, 293, 383, 11, 51308], "temperature": 0.0, "avg_logprob": -0.07957553075364798, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0007319984724745154}, {"id": 1778, "seek": 915696, "start": 9175.839999999998, "end": 9179.199999999999, "text": " now not only do you have to package the C code, you have to build the C code.", "tokens": [51308, 586, 406, 787, 360, 291, 362, 281, 7372, 264, 383, 3089, 11, 291, 362, 281, 1322, 264, 383, 3089, 13, 51476], "temperature": 0.0, "avg_logprob": -0.07957553075364798, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0007319984724745154}, {"id": 1779, "seek": 915696, "start": 9180.16, "end": 9182.48, "text": " C doesn't have a package manager, right?", "tokens": [51524, 383, 1177, 380, 362, 257, 7372, 6598, 11, 558, 30, 51640], "temperature": 0.0, "avg_logprob": -0.07957553075364798, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0007319984724745154}, {"id": 1780, "seek": 915696, "start": 9182.48, "end": 9185.919999999998, "text": " C doesn't have a dependency versioning management system, right?", "tokens": [51640, 383, 1177, 380, 362, 257, 33621, 3037, 278, 4592, 1185, 11, 558, 30, 51812], "temperature": 0.0, "avg_logprob": -0.07957553075364798, "compression_ratio": 1.8494208494208495, "no_speech_prob": 0.0007319984724745154}, {"id": 1781, "seek": 918592, "start": 9185.92, "end": 9189.04, "text": " And so I'm not experienced in the state of the art", "tokens": [50364, 400, 370, 286, 478, 406, 6751, 294, 264, 1785, 295, 264, 1523, 50520], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1782, "seek": 918592, "start": 9189.04, "end": 9192.48, "text": " and all the different Python package managers,", "tokens": [50520, 293, 439, 264, 819, 15329, 7372, 14084, 11, 50692], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1783, "seek": 918592, "start": 9192.48, "end": 9195.84, "text": " but my understanding is that's a massive part of the problem.", "tokens": [50692, 457, 452, 3701, 307, 300, 311, 257, 5994, 644, 295, 264, 1154, 13, 50860], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1784, "seek": 918592, "start": 9195.84, "end": 9198.72, "text": " And I think Mojo solves that part of the problem directly heads on.", "tokens": [50860, 400, 286, 519, 3335, 5134, 39890, 300, 644, 295, 264, 1154, 3838, 8050, 322, 13, 51004], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1785, "seek": 918592, "start": 9199.28, "end": 9201.68, "text": " Now, one of the things I think we'll do with the community,", "tokens": [51032, 823, 11, 472, 295, 264, 721, 286, 519, 321, 603, 360, 365, 264, 1768, 11, 51152], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1786, "seek": 918592, "start": 9201.68, "end": 9205.12, "text": " and this isn't, again, we're not solving all the world's problems at once,", "tokens": [51152, 293, 341, 1943, 380, 11, 797, 11, 321, 434, 406, 12606, 439, 264, 1002, 311, 2740, 412, 1564, 11, 51324], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1787, "seek": 918592, "start": 9205.12, "end": 9207.36, "text": " we have to be kind of focused to start with,", "tokens": [51324, 321, 362, 281, 312, 733, 295, 5178, 281, 722, 365, 11, 51436], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1788, "seek": 918592, "start": 9207.36, "end": 9212.08, "text": " is that I think that we will have an opportunity to reevaluate packaging, right?", "tokens": [51436, 307, 300, 286, 519, 300, 321, 486, 362, 364, 2650, 281, 43060, 3337, 10107, 16836, 11, 558, 30, 51672], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1789, "seek": 918592, "start": 9212.08, "end": 9213.68, "text": " And so I think that we can come back and say,", "tokens": [51672, 400, 370, 286, 519, 300, 321, 393, 808, 646, 293, 584, 11, 51752], "temperature": 0.0, "avg_logprob": -0.09745999316235522, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.00048775647883303463}, {"id": 1790, "seek": 921368, "start": 9213.68, "end": 9216.16, "text": " okay, well, given the new tools and technologies", "tokens": [50364, 1392, 11, 731, 11, 2212, 264, 777, 3873, 293, 7943, 50488], "temperature": 0.0, "avg_logprob": -0.10496256181171962, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0010811706306412816}, {"id": 1791, "seek": 921368, "start": 9216.16, "end": 9218.0, "text": " and the cool things we have that we've built up,", "tokens": [50488, 293, 264, 1627, 721, 321, 362, 300, 321, 600, 3094, 493, 11, 50580], "temperature": 0.0, "avg_logprob": -0.10496256181171962, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0010811706306412816}, {"id": 1792, "seek": 921368, "start": 9218.0, "end": 9219.76, "text": " because we have not just syntax,", "tokens": [50580, 570, 321, 362, 406, 445, 28431, 11, 50668], "temperature": 0.0, "avg_logprob": -0.10496256181171962, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0010811706306412816}, {"id": 1793, "seek": 921368, "start": 9219.76, "end": 9222.720000000001, "text": " we have an entirely new compiler stack that works in a new way,", "tokens": [50668, 321, 362, 364, 7696, 777, 31958, 8630, 300, 1985, 294, 257, 777, 636, 11, 50816], "temperature": 0.0, "avg_logprob": -0.10496256181171962, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0010811706306412816}, {"id": 1794, "seek": 921368, "start": 9222.720000000001, "end": 9224.960000000001, "text": " maybe there's other innovations we can bring together", "tokens": [50816, 1310, 456, 311, 661, 24283, 321, 393, 1565, 1214, 50928], "temperature": 0.0, "avg_logprob": -0.10496256181171962, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0010811706306412816}, {"id": 1795, "seek": 921368, "start": 9224.960000000001, "end": 9226.960000000001, "text": " and maybe we can help solve that problem.", "tokens": [50928, 293, 1310, 321, 393, 854, 5039, 300, 1154, 13, 51028], "temperature": 0.0, "avg_logprob": -0.10496256181171962, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0010811706306412816}, {"id": 1796, "seek": 921368, "start": 9226.960000000001, "end": 9230.48, "text": " So almost that tangent to that question from the user perspective of packages,", "tokens": [51028, 407, 1920, 300, 27747, 281, 300, 1168, 490, 264, 4195, 4585, 295, 17401, 11, 51204], "temperature": 0.0, "avg_logprob": -0.10496256181171962, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0010811706306412816}, {"id": 1797, "seek": 921368, "start": 9231.84, "end": 9238.16, "text": " it was always surprising to me that it was not easier to sort of explore and find packages.", "tokens": [51272, 309, 390, 1009, 8830, 281, 385, 300, 309, 390, 406, 3571, 281, 1333, 295, 6839, 293, 915, 17401, 13, 51588], "temperature": 0.0, "avg_logprob": -0.10496256181171962, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.0010811706306412816}, {"id": 1798, "seek": 923816, "start": 9239.119999999999, "end": 9246.0, "text": " With PIP install, it feels, it's an incredible ecosystem.", "tokens": [50412, 2022, 430, 9139, 3625, 11, 309, 3417, 11, 309, 311, 364, 4651, 11311, 13, 50756], "temperature": 0.0, "avg_logprob": -0.2519278909968234, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.03306655213236809}, {"id": 1799, "seek": 923816, "start": 9246.0, "end": 9249.28, "text": " It's just interesting that it wasn't made,", "tokens": [50756, 467, 311, 445, 1880, 300, 309, 2067, 380, 1027, 11, 50920], "temperature": 0.0, "avg_logprob": -0.2519278909968234, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.03306655213236809}, {"id": 1800, "seek": 923816, "start": 9249.84, "end": 9253.119999999999, "text": " it's still, I think, not made easier to discover packages to do,", "tokens": [50948, 309, 311, 920, 11, 286, 519, 11, 406, 1027, 3571, 281, 4411, 17401, 281, 360, 11, 51112], "temperature": 0.0, "avg_logprob": -0.2519278909968234, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.03306655213236809}, {"id": 1801, "seek": 923816, "start": 9253.84, "end": 9259.76, "text": " like search and discovery, as YouTube calls it.", "tokens": [51148, 411, 3164, 293, 12114, 11, 382, 3088, 5498, 309, 13, 51444], "temperature": 0.0, "avg_logprob": -0.2519278909968234, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.03306655213236809}, {"id": 1802, "seek": 923816, "start": 9259.76, "end": 9263.119999999999, "text": " Well, I mean, it's kind of funny because this is one of the challenges of these", "tokens": [51444, 1042, 11, 286, 914, 11, 309, 311, 733, 295, 4074, 570, 341, 307, 472, 295, 264, 4759, 295, 613, 51612], "temperature": 0.0, "avg_logprob": -0.2519278909968234, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.03306655213236809}, {"id": 1803, "seek": 923816, "start": 9264.16, "end": 9267.2, "text": " intentionally decentralized communities.", "tokens": [51664, 22062, 32870, 4456, 13, 51816], "temperature": 0.0, "avg_logprob": -0.2519278909968234, "compression_ratio": 1.5462962962962963, "no_speech_prob": 0.03306655213236809}, {"id": 1804, "seek": 926720, "start": 9267.28, "end": 9269.36, "text": " And so I don't know what the right answer is for Python.", "tokens": [50368, 400, 370, 286, 500, 380, 458, 437, 264, 558, 1867, 307, 337, 15329, 13, 50472], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1805, "seek": 926720, "start": 9269.36, "end": 9272.320000000002, "text": " I mean, there are many people that were,", "tokens": [50472, 286, 914, 11, 456, 366, 867, 561, 300, 645, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1806, "seek": 926720, "start": 9272.320000000002, "end": 9273.68, "text": " I don't even know the right answer for Mojo.", "tokens": [50620, 286, 500, 380, 754, 458, 264, 558, 1867, 337, 3335, 5134, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1807, "seek": 926720, "start": 9275.12, "end": 9278.480000000001, "text": " So there are many people that would have much more informed opinions than I do,", "tokens": [50760, 407, 456, 366, 867, 561, 300, 576, 362, 709, 544, 11740, 11819, 813, 286, 360, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1808, "seek": 926720, "start": 9278.480000000001, "end": 9280.480000000001, "text": " but it's interesting if you look at this, right?", "tokens": [50928, 457, 309, 311, 1880, 498, 291, 574, 412, 341, 11, 558, 30, 51028], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1809, "seek": 926720, "start": 9280.480000000001, "end": 9283.68, "text": " Open source communities, you know, there's Git,", "tokens": [51028, 7238, 4009, 4456, 11, 291, 458, 11, 456, 311, 16939, 11, 51188], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1810, "seek": 926720, "start": 9284.320000000002, "end": 9286.08, "text": " Git is a fully decentralized,", "tokens": [51220, 16939, 307, 257, 4498, 32870, 11, 51308], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1811, "seek": 926720, "start": 9286.08, "end": 9287.36, "text": " and they can do it any way they want,", "tokens": [51308, 293, 436, 393, 360, 309, 604, 636, 436, 528, 11, 51372], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1812, "seek": 926720, "start": 9287.36, "end": 9289.36, "text": " but then there's GitHub, right?", "tokens": [51372, 457, 550, 456, 311, 23331, 11, 558, 30, 51472], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1813, "seek": 926720, "start": 9289.36, "end": 9293.12, "text": " And GitHub centralized, commercial in that case, right?", "tokens": [51472, 400, 23331, 32395, 11, 6841, 294, 300, 1389, 11, 558, 30, 51660], "temperature": 0.0, "avg_logprob": -0.1685649553934733, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.002322506858035922}, {"id": 1814, "seek": 929312, "start": 9293.12, "end": 9296.880000000001, "text": " Thing, really help pull together and help solve some of the discovery problems", "tokens": [50364, 30902, 11, 534, 854, 2235, 1214, 293, 854, 5039, 512, 295, 264, 12114, 2740, 50552], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1815, "seek": 929312, "start": 9296.880000000001, "end": 9300.240000000002, "text": " and help build a more consistent community.", "tokens": [50552, 293, 854, 1322, 257, 544, 8398, 1768, 13, 50720], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1816, "seek": 929312, "start": 9300.240000000002, "end": 9303.04, "text": " And so maybe there's opportunities for something like a GitHub.", "tokens": [50720, 400, 370, 1310, 456, 311, 4786, 337, 746, 411, 257, 23331, 13, 50860], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1817, "seek": 929312, "start": 9304.160000000002, "end": 9306.480000000001, "text": " Although even GitHub, I might be wrong on this,", "tokens": [50916, 5780, 754, 23331, 11, 286, 1062, 312, 2085, 322, 341, 11, 51032], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1818, "seek": 929312, "start": 9306.480000000001, "end": 9310.480000000001, "text": " but the search and discovery for GitHub is not that great.", "tokens": [51032, 457, 264, 3164, 293, 12114, 337, 23331, 307, 406, 300, 869, 13, 51232], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1819, "seek": 929312, "start": 9310.480000000001, "end": 9312.160000000002, "text": " Like I still use Google search.", "tokens": [51232, 1743, 286, 920, 764, 3329, 3164, 13, 51316], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1820, "seek": 929312, "start": 9313.04, "end": 9316.800000000001, "text": " Yeah, well, I mean, maybe that's because GitHub doesn't want to replace Google search,", "tokens": [51360, 865, 11, 731, 11, 286, 914, 11, 1310, 300, 311, 570, 23331, 1177, 380, 528, 281, 7406, 3329, 3164, 11, 51548], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1821, "seek": 929312, "start": 9317.76, "end": 9318.0, "text": " right?", "tokens": [51596, 558, 30, 51608], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1822, "seek": 929312, "start": 9318.0, "end": 9322.480000000001, "text": " And I think there is room for specialized solutions to specific problems.", "tokens": [51608, 400, 286, 519, 456, 307, 1808, 337, 19813, 6547, 281, 2685, 2740, 13, 51832], "temperature": 0.0, "avg_logprob": -0.16913072941666943, "compression_ratio": 1.68259385665529, "no_speech_prob": 0.01882319338619709}, {"id": 1823, "seek": 932312, "start": 9323.36, "end": 9325.28, "text": " I don't know, I don't know the right answer for GitHub either.", "tokens": [50376, 286, 500, 380, 458, 11, 286, 500, 380, 458, 264, 558, 1867, 337, 23331, 2139, 13, 50472], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1824, "seek": 932312, "start": 9325.28, "end": 9327.68, "text": " That's, they can go figure that out.", "tokens": [50472, 663, 311, 11, 436, 393, 352, 2573, 300, 484, 13, 50592], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1825, "seek": 932312, "start": 9328.720000000001, "end": 9330.960000000001, "text": " But the point is to have an interface that's usable,", "tokens": [50644, 583, 264, 935, 307, 281, 362, 364, 9226, 300, 311, 29975, 11, 50756], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1826, "seek": 932312, "start": 9330.960000000001, "end": 9333.76, "text": " that's accessible to people of all different skill levels and so on.", "tokens": [50756, 300, 311, 9515, 281, 561, 295, 439, 819, 5389, 4358, 293, 370, 322, 13, 50896], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1827, "seek": 932312, "start": 9333.76, "end": 9336.160000000002, "text": " Well, and again, like what are the benefit of standards, right?", "tokens": [50896, 1042, 11, 293, 797, 11, 411, 437, 366, 264, 5121, 295, 7787, 11, 558, 30, 51016], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1828, "seek": 932312, "start": 9336.160000000002, "end": 9339.28, "text": " Standards allow you to build these next level up ecosystem,", "tokens": [51016, 44546, 2089, 291, 281, 1322, 613, 958, 1496, 493, 11311, 11, 51172], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1829, "seek": 932312, "start": 9339.28, "end": 9342.0, "text": " the next level up infrastructure, the next level up things.", "tokens": [51172, 264, 958, 1496, 493, 6896, 11, 264, 958, 1496, 493, 721, 13, 51308], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1830, "seek": 932312, "start": 9342.0, "end": 9345.6, "text": " And so again, come back to, I hate complexity.", "tokens": [51308, 400, 370, 797, 11, 808, 646, 281, 11, 286, 4700, 14024, 13, 51488], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1831, "seek": 932312, "start": 9347.12, "end": 9348.640000000001, "text": " C plus Python is complicated.", "tokens": [51564, 383, 1804, 15329, 307, 6179, 13, 51640], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1832, "seek": 932312, "start": 9349.2, "end": 9351.28, "text": " It makes everything more difficult to deal with.", "tokens": [51668, 467, 1669, 1203, 544, 2252, 281, 2028, 365, 13, 51772], "temperature": 0.0, "avg_logprob": -0.17977250737251996, "compression_ratio": 1.6964856230031948, "no_speech_prob": 0.0012064415495842695}, {"id": 1833, "seek": 935128, "start": 9351.28, "end": 9354.640000000001, "text": " It makes it difficult to port, move code around, work with.", "tokens": [50364, 467, 1669, 309, 2252, 281, 2436, 11, 1286, 3089, 926, 11, 589, 365, 13, 50532], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1834, "seek": 935128, "start": 9354.640000000001, "end": 9356.0, "text": " All these things get more complicated.", "tokens": [50532, 1057, 613, 721, 483, 544, 6179, 13, 50600], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1835, "seek": 935128, "start": 9356.0, "end": 9359.68, "text": " And so, I mean, I'm not an expert, but maybe Mojo can help a little bit", "tokens": [50600, 400, 370, 11, 286, 914, 11, 286, 478, 406, 364, 5844, 11, 457, 1310, 3335, 5134, 393, 854, 257, 707, 857, 50784], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1836, "seek": 935128, "start": 9359.68, "end": 9362.640000000001, "text": " by helping reduce the amount of C in this ecosystem", "tokens": [50784, 538, 4315, 5407, 264, 2372, 295, 383, 294, 341, 11311, 50932], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1837, "seek": 935128, "start": 9362.640000000001, "end": 9363.76, "text": " and make it therefore scale better.", "tokens": [50932, 293, 652, 309, 4412, 4373, 1101, 13, 50988], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1838, "seek": 935128, "start": 9363.76, "end": 9366.880000000001, "text": " So when you kind of package this, the hybrid in nature", "tokens": [50988, 407, 562, 291, 733, 295, 7372, 341, 11, 264, 13051, 294, 3687, 51144], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1839, "seek": 935128, "start": 9366.880000000001, "end": 9369.44, "text": " would be a natural fit to move to Mojo.", "tokens": [51144, 576, 312, 257, 3303, 3318, 281, 1286, 281, 3335, 5134, 13, 51272], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1840, "seek": 935128, "start": 9369.44, "end": 9370.880000000001, "text": " Which is a lot of them, by the way.", "tokens": [51272, 3013, 307, 257, 688, 295, 552, 11, 538, 264, 636, 13, 51344], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1841, "seek": 935128, "start": 9372.560000000001, "end": 9375.6, "text": " A lot of them, especially they're doing some interesting stuff computation-wise.", "tokens": [51428, 316, 688, 295, 552, 11, 2318, 436, 434, 884, 512, 1880, 1507, 24903, 12, 3711, 13, 51580], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1842, "seek": 935128, "start": 9376.960000000001, "end": 9378.320000000002, "text": " Let me ask you about some features.", "tokens": [51648, 961, 385, 1029, 291, 466, 512, 4122, 13, 51716], "temperature": 0.0, "avg_logprob": -0.14638508309563286, "compression_ratio": 1.6375404530744337, "no_speech_prob": 0.0008039246313273907}, {"id": 1843, "seek": 937832, "start": 9378.8, "end": 9379.6, "text": " Yeah.", "tokens": [50388, 865, 13, 50428], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1844, "seek": 937832, "start": 9379.6, "end": 9383.84, "text": " So we talked about, obviously, the indentation that it's the type language", "tokens": [50428, 407, 321, 2825, 466, 11, 2745, 11, 264, 44494, 399, 300, 309, 311, 264, 2010, 2856, 50640], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1845, "seek": 937832, "start": 9383.84, "end": 9385.119999999999, "text": " or optionally typed.", "tokens": [50640, 420, 3614, 379, 33941, 13, 50704], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1846, "seek": 937832, "start": 9385.92, "end": 9387.119999999999, "text": " Is that the right way to say it?", "tokens": [50744, 1119, 300, 264, 558, 636, 281, 584, 309, 30, 50804], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1847, "seek": 937832, "start": 9387.119999999999, "end": 9388.72, "text": " It's either optionally or progressively.", "tokens": [50804, 467, 311, 2139, 3614, 379, 420, 46667, 13, 50884], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1848, "seek": 937832, "start": 9388.72, "end": 9389.6, "text": " Progressively.", "tokens": [50884, 32587, 3413, 13, 50928], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1849, "seek": 937832, "start": 9389.6, "end": 9393.279999999999, "text": " I think, so people have very strong opinions on the right word to use.", "tokens": [50928, 286, 519, 11, 370, 561, 362, 588, 2068, 11819, 322, 264, 558, 1349, 281, 764, 13, 51112], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1850, "seek": 937832, "start": 9394.32, "end": 9394.96, "text": " I don't know.", "tokens": [51164, 286, 500, 380, 458, 13, 51196], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1851, "seek": 937832, "start": 9394.96, "end": 9396.4, "text": " I look forward to your letters.", "tokens": [51196, 286, 574, 2128, 281, 428, 7825, 13, 51268], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1852, "seek": 937832, "start": 9397.76, "end": 9399.92, "text": " So there's the var versus let.", "tokens": [51336, 407, 456, 311, 264, 1374, 5717, 718, 13, 51444], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1853, "seek": 937832, "start": 9399.92, "end": 9401.199999999999, "text": " But let is for constants.", "tokens": [51444, 583, 718, 307, 337, 35870, 13, 51508], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1854, "seek": 937832, "start": 9402.56, "end": 9403.52, "text": " Var is an optional.", "tokens": [51576, 14662, 307, 364, 17312, 13, 51624], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1855, "seek": 937832, "start": 9404.32, "end": 9407.039999999999, "text": " Yeah, var makes it mutable, so you can reassign.", "tokens": [51664, 865, 11, 1374, 1669, 309, 5839, 712, 11, 370, 291, 393, 19486, 788, 13, 51800], "temperature": 0.0, "avg_logprob": -0.17019144694010416, "compression_ratio": 1.6653846153846155, "no_speech_prob": 0.0015484605683013797}, {"id": 1856, "seek": 940704, "start": 9407.12, "end": 9412.480000000001, "text": " Okay, then there's function overloading.", "tokens": [50368, 1033, 11, 550, 456, 311, 2445, 28777, 278, 13, 50636], "temperature": 0.0, "avg_logprob": -0.16066864283397944, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.00028681399999186397}, {"id": 1857, "seek": 940704, "start": 9412.480000000001, "end": 9413.36, "text": " Oh, okay, yeah.", "tokens": [50636, 876, 11, 1392, 11, 1338, 13, 50680], "temperature": 0.0, "avg_logprob": -0.16066864283397944, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.00028681399999186397}, {"id": 1858, "seek": 940704, "start": 9414.320000000002, "end": 9416.320000000002, "text": " I mean, there's a lot of source of happiness for me,", "tokens": [50728, 286, 914, 11, 456, 311, 257, 688, 295, 4009, 295, 8324, 337, 385, 11, 50828], "temperature": 0.0, "avg_logprob": -0.16066864283397944, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.00028681399999186397}, {"id": 1859, "seek": 940704, "start": 9416.320000000002, "end": 9423.28, "text": " but function overloading that's, I guess, is that for performance?", "tokens": [50828, 457, 2445, 28777, 278, 300, 311, 11, 286, 2041, 11, 307, 300, 337, 3389, 30, 51176], "temperature": 0.0, "avg_logprob": -0.16066864283397944, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.00028681399999186397}, {"id": 1860, "seek": 940704, "start": 9423.28, "end": 9425.92, "text": " Or is that, why does Python not have function overloading?", "tokens": [51176, 1610, 307, 300, 11, 983, 775, 15329, 406, 362, 2445, 28777, 278, 30, 51308], "temperature": 0.0, "avg_logprob": -0.16066864283397944, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.00028681399999186397}, {"id": 1861, "seek": 940704, "start": 9427.04, "end": 9428.080000000002, "text": " So I can speculate.", "tokens": [51364, 407, 286, 393, 40775, 13, 51416], "temperature": 0.0, "avg_logprob": -0.16066864283397944, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.00028681399999186397}, {"id": 1862, "seek": 940704, "start": 9428.080000000002, "end": 9430.400000000001, "text": " So Python is a dynamic language.", "tokens": [51416, 407, 15329, 307, 257, 8546, 2856, 13, 51532], "temperature": 0.0, "avg_logprob": -0.16066864283397944, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.00028681399999186397}, {"id": 1863, "seek": 940704, "start": 9430.400000000001, "end": 9435.2, "text": " The way it works is that Python and Objective-C", "tokens": [51532, 440, 636, 309, 1985, 307, 300, 15329, 293, 24753, 488, 12, 34, 51772], "temperature": 0.0, "avg_logprob": -0.16066864283397944, "compression_ratio": 1.6390243902439023, "no_speech_prob": 0.00028681399999186397}, {"id": 1864, "seek": 943520, "start": 9435.2, "end": 9439.84, "text": " are actually very similar worlds if you ignore syntax.", "tokens": [50364, 366, 767, 588, 2531, 13401, 498, 291, 11200, 28431, 13, 50596], "temperature": 0.0, "avg_logprob": -0.12452937055517126, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.001597219263203442}, {"id": 1865, "seek": 943520, "start": 9440.880000000001, "end": 9445.76, "text": " And so Objective-C is straight line derived from Smalltalk,", "tokens": [50648, 400, 370, 24753, 488, 12, 34, 307, 2997, 1622, 18949, 490, 15287, 29302, 11, 50892], "temperature": 0.0, "avg_logprob": -0.12452937055517126, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.001597219263203442}, {"id": 1866, "seek": 943520, "start": 9446.880000000001, "end": 9451.76, "text": " a really venerable, interesting language that much of the world has forgotten about,", "tokens": [50948, 257, 534, 6138, 260, 712, 11, 1880, 2856, 300, 709, 295, 264, 1002, 575, 11832, 466, 11, 51192], "temperature": 0.0, "avg_logprob": -0.12452937055517126, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.001597219263203442}, {"id": 1867, "seek": 943520, "start": 9451.76, "end": 9454.240000000002, "text": " but the people that remember it, love it, generally.", "tokens": [51192, 457, 264, 561, 300, 1604, 309, 11, 959, 309, 11, 5101, 13, 51316], "temperature": 0.0, "avg_logprob": -0.12452937055517126, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.001597219263203442}, {"id": 1868, "seek": 943520, "start": 9455.04, "end": 9458.400000000001, "text": " And the way that Smalltalk works is that every object has a dictionary in it,", "tokens": [51356, 400, 264, 636, 300, 15287, 29302, 1985, 307, 300, 633, 2657, 575, 257, 25890, 294, 309, 11, 51524], "temperature": 0.0, "avg_logprob": -0.12452937055517126, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.001597219263203442}, {"id": 1869, "seek": 943520, "start": 9458.960000000001, "end": 9463.28, "text": " and the dictionary maps from the name of a function or the name of a value within an object", "tokens": [51552, 293, 264, 25890, 11317, 490, 264, 1315, 295, 257, 2445, 420, 264, 1315, 295, 257, 2158, 1951, 364, 2657, 51768], "temperature": 0.0, "avg_logprob": -0.12452937055517126, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.001597219263203442}, {"id": 1870, "seek": 943520, "start": 9463.84, "end": 9464.880000000001, "text": " to its implementation.", "tokens": [51796, 281, 1080, 11420, 13, 51848], "temperature": 0.0, "avg_logprob": -0.12452937055517126, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.001597219263203442}, {"id": 1871, "seek": 946520, "start": 9465.6, "end": 9468.400000000001, "text": " And so the way you call a method in Objective-C is you say,", "tokens": [50384, 400, 370, 264, 636, 291, 818, 257, 3170, 294, 24753, 488, 12, 34, 307, 291, 584, 11, 50524], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1872, "seek": 946520, "start": 9469.04, "end": 9472.0, "text": " go look up, the way I call foo is I go look up foo,", "tokens": [50556, 352, 574, 493, 11, 264, 636, 286, 818, 726, 78, 307, 286, 352, 574, 493, 726, 78, 11, 50704], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1873, "seek": 946520, "start": 9472.0, "end": 9473.84, "text": " I get a pointer to the function back, and then I call it.", "tokens": [50704, 286, 483, 257, 23918, 281, 264, 2445, 646, 11, 293, 550, 286, 818, 309, 13, 50796], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1874, "seek": 946520, "start": 9474.400000000001, "end": 9476.0, "text": " Okay, that's how Python works.", "tokens": [50824, 1033, 11, 300, 311, 577, 15329, 1985, 13, 50904], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1875, "seek": 946520, "start": 9476.640000000001, "end": 9478.480000000001, "text": " Right, and so now the problem with that is that", "tokens": [50936, 1779, 11, 293, 370, 586, 264, 1154, 365, 300, 307, 300, 51028], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1876, "seek": 946520, "start": 9479.36, "end": 9482.480000000001, "text": " the dictionary within a Python object, all the keys are strings,", "tokens": [51072, 264, 25890, 1951, 257, 15329, 2657, 11, 439, 264, 9317, 366, 13985, 11, 51228], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1877, "seek": 946520, "start": 9483.52, "end": 9486.480000000001, "text": " and it's a dictionary, so you can only have one entry per name.", "tokens": [51280, 293, 309, 311, 257, 25890, 11, 370, 291, 393, 787, 362, 472, 8729, 680, 1315, 13, 51428], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1878, "seek": 946520, "start": 9486.480000000001, "end": 9487.92, "text": " You think it's as simple as that?", "tokens": [51428, 509, 519, 309, 311, 382, 2199, 382, 300, 30, 51500], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1879, "seek": 946520, "start": 9487.92, "end": 9489.28, "text": " I think it's as simple as that.", "tokens": [51500, 286, 519, 309, 311, 382, 2199, 382, 300, 13, 51568], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1880, "seek": 946520, "start": 9489.28, "end": 9492.880000000001, "text": " And so now, why do they never fix this?", "tokens": [51568, 400, 370, 586, 11, 983, 360, 436, 1128, 3191, 341, 30, 51748], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1881, "seek": 946520, "start": 9492.880000000001, "end": 9494.720000000001, "text": " Like, why do they not change it to not be a dictionary?", "tokens": [51748, 1743, 11, 983, 360, 436, 406, 1319, 309, 281, 406, 312, 257, 25890, 30, 51840], "temperature": 0.0, "avg_logprob": -0.11065174449573864, "compression_ratio": 1.8395904436860069, "no_speech_prob": 0.0002377993514528498}, {"id": 1882, "seek": 949472, "start": 9495.679999999998, "end": 9496.96, "text": " Like, do other things.", "tokens": [50412, 1743, 11, 360, 661, 721, 13, 50476], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1883, "seek": 949472, "start": 9497.92, "end": 9501.519999999999, "text": " Well, you don't really have to in Python because it's dynamic.", "tokens": [50524, 1042, 11, 291, 500, 380, 534, 362, 281, 294, 15329, 570, 309, 311, 8546, 13, 50704], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1884, "seek": 949472, "start": 9501.519999999999, "end": 9503.439999999999, "text": " And so you can say, I get into the function,", "tokens": [50704, 400, 370, 291, 393, 584, 11, 286, 483, 666, 264, 2445, 11, 50800], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1885, "seek": 949472, "start": 9504.16, "end": 9507.119999999999, "text": " now if I got past an integer, do some dynamic test for it,", "tokens": [50836, 586, 498, 286, 658, 1791, 364, 24922, 11, 360, 512, 8546, 1500, 337, 309, 11, 50984], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1886, "seek": 949472, "start": 9507.119999999999, "end": 9509.119999999999, "text": " if it's a string, go do another thing.", "tokens": [50984, 498, 309, 311, 257, 6798, 11, 352, 360, 1071, 551, 13, 51084], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1887, "seek": 949472, "start": 9510.08, "end": 9511.84, "text": " There's another additional challenge, which is,", "tokens": [51132, 821, 311, 1071, 4497, 3430, 11, 597, 307, 11, 51220], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1888, "seek": 949472, "start": 9511.84, "end": 9513.84, "text": " even if you did support overloading, you're saying,", "tokens": [51220, 754, 498, 291, 630, 1406, 28777, 278, 11, 291, 434, 1566, 11, 51320], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1889, "seek": 949472, "start": 9513.84, "end": 9517.359999999999, "text": " okay, well, here's a version of a function for integers and a function for strings.", "tokens": [51320, 1392, 11, 731, 11, 510, 311, 257, 3037, 295, 257, 2445, 337, 41674, 293, 257, 2445, 337, 13985, 13, 51496], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1890, "seek": 949472, "start": 9518.0, "end": 9520.24, "text": " Well, you'd have, even if you could put it in that dictionary,", "tokens": [51528, 1042, 11, 291, 1116, 362, 11, 754, 498, 291, 727, 829, 309, 294, 300, 25890, 11, 51640], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1891, "seek": 949472, "start": 9520.24, "end": 9522.32, "text": " you'd have to have the caller do the dispatch.", "tokens": [51640, 291, 1116, 362, 281, 362, 264, 48324, 360, 264, 36729, 13, 51744], "temperature": 0.0, "avg_logprob": -0.11213761406975824, "compression_ratio": 1.818815331010453, "no_speech_prob": 0.0005702312919311225}, {"id": 1892, "seek": 952232, "start": 9522.96, "end": 9524.8, "text": " And so every time you call the function, you'd have to say,", "tokens": [50396, 400, 370, 633, 565, 291, 818, 264, 2445, 11, 291, 1116, 362, 281, 584, 11, 50488], "temperature": 0.0, "avg_logprob": -0.11943340301513672, "compression_ratio": 1.7800829875518671, "no_speech_prob": 0.0010984973050653934}, {"id": 1893, "seek": 952232, "start": 9524.8, "end": 9526.4, "text": " like, is an integer a string?", "tokens": [50488, 411, 11, 307, 364, 24922, 257, 6798, 30, 50568], "temperature": 0.0, "avg_logprob": -0.11943340301513672, "compression_ratio": 1.7800829875518671, "no_speech_prob": 0.0010984973050653934}, {"id": 1894, "seek": 952232, "start": 9526.4, "end": 9528.16, "text": " And so you'd have to figure out where to do that test.", "tokens": [50568, 400, 370, 291, 1116, 362, 281, 2573, 484, 689, 281, 360, 300, 1500, 13, 50656], "temperature": 0.0, "avg_logprob": -0.11943340301513672, "compression_ratio": 1.7800829875518671, "no_speech_prob": 0.0010984973050653934}, {"id": 1895, "seek": 952232, "start": 9528.72, "end": 9534.08, "text": " And so in a dynamic language, overloading is something you don't have to have.", "tokens": [50684, 400, 370, 294, 257, 8546, 2856, 11, 28777, 278, 307, 746, 291, 500, 380, 362, 281, 362, 13, 50952], "temperature": 0.0, "avg_logprob": -0.11943340301513672, "compression_ratio": 1.7800829875518671, "no_speech_prob": 0.0010984973050653934}, {"id": 1896, "seek": 952232, "start": 9535.92, "end": 9537.92, "text": " But now you get into a typed language.", "tokens": [51044, 583, 586, 291, 483, 666, 257, 33941, 2856, 13, 51144], "temperature": 0.0, "avg_logprob": -0.11943340301513672, "compression_ratio": 1.7800829875518671, "no_speech_prob": 0.0010984973050653934}, {"id": 1897, "seek": 952232, "start": 9537.92, "end": 9541.76, "text": " And in Python, if you subscript with an integer,", "tokens": [51144, 400, 294, 15329, 11, 498, 291, 2325, 662, 365, 364, 24922, 11, 51336], "temperature": 0.0, "avg_logprob": -0.11943340301513672, "compression_ratio": 1.7800829875518671, "no_speech_prob": 0.0010984973050653934}, {"id": 1898, "seek": 952232, "start": 9542.64, "end": 9546.08, "text": " then you get typically one element out of a collection.", "tokens": [51380, 550, 291, 483, 5850, 472, 4478, 484, 295, 257, 5765, 13, 51552], "temperature": 0.0, "avg_logprob": -0.11943340301513672, "compression_ratio": 1.7800829875518671, "no_speech_prob": 0.0010984973050653934}, {"id": 1899, "seek": 952232, "start": 9546.08, "end": 9549.44, "text": " If you subscript with a range, you get a different thing out.", "tokens": [51552, 759, 291, 2325, 662, 365, 257, 3613, 11, 291, 483, 257, 819, 551, 484, 13, 51720], "temperature": 0.0, "avg_logprob": -0.11943340301513672, "compression_ratio": 1.7800829875518671, "no_speech_prob": 0.0010984973050653934}, {"id": 1900, "seek": 954944, "start": 9550.16, "end": 9554.16, "text": " Right? And so often in typed languages, you'll want to be able to express the fact that,", "tokens": [50400, 1779, 30, 400, 370, 2049, 294, 33941, 8650, 11, 291, 603, 528, 281, 312, 1075, 281, 5109, 264, 1186, 300, 11, 50600], "temperature": 0.0, "avg_logprob": -0.16174376306454996, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0034825834445655346}, {"id": 1901, "seek": 954944, "start": 9554.16, "end": 9559.04, "text": " cool, I have different behavior depending on what I actually pass into this thing.", "tokens": [50600, 1627, 11, 286, 362, 819, 5223, 5413, 322, 437, 286, 767, 1320, 666, 341, 551, 13, 50844], "temperature": 0.0, "avg_logprob": -0.16174376306454996, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0034825834445655346}, {"id": 1902, "seek": 954944, "start": 9559.04, "end": 9563.04, "text": " If you can model that, it can make it safer and more predictable and faster and like all these things.", "tokens": [50844, 759, 291, 393, 2316, 300, 11, 309, 393, 652, 309, 15856, 293, 544, 27737, 293, 4663, 293, 411, 439, 613, 721, 13, 51044], "temperature": 0.0, "avg_logprob": -0.16174376306454996, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0034825834445655346}, {"id": 1903, "seek": 954944, "start": 9563.68, "end": 9568.0, "text": " It somehow feels safe for yes, but also feels empowering.", "tokens": [51076, 467, 6063, 3417, 3273, 337, 2086, 11, 457, 611, 3417, 28261, 13, 51292], "temperature": 0.0, "avg_logprob": -0.16174376306454996, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0034825834445655346}, {"id": 1904, "seek": 954944, "start": 9568.0, "end": 9572.4, "text": " Make it in terms of clarity, like you don't have to design whole different functions.", "tokens": [51292, 4387, 309, 294, 2115, 295, 16992, 11, 411, 291, 500, 380, 362, 281, 1715, 1379, 819, 6828, 13, 51512], "temperature": 0.0, "avg_logprob": -0.16174376306454996, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0034825834445655346}, {"id": 1905, "seek": 954944, "start": 9572.4, "end": 9578.560000000001, "text": " Yeah. Well, this is also one of the challenges with the existing Python typing systems,", "tokens": [51512, 865, 13, 1042, 11, 341, 307, 611, 472, 295, 264, 4759, 365, 264, 6741, 15329, 18444, 3652, 11, 51820], "temperature": 0.0, "avg_logprob": -0.16174376306454996, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.0034825834445655346}, {"id": 1906, "seek": 957856, "start": 9578.56, "end": 9583.279999999999, "text": " is that in practice, like you take subscript, in practice, a lot of these functions,", "tokens": [50364, 307, 300, 294, 3124, 11, 411, 291, 747, 2325, 662, 11, 294, 3124, 11, 257, 688, 295, 613, 6828, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1300418174872964, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0002199200534960255}, {"id": 1907, "seek": 957856, "start": 9583.279999999999, "end": 9585.76, "text": " they don't have one signature.", "tokens": [50600, 436, 500, 380, 362, 472, 13397, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1300418174872964, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0002199200534960255}, {"id": 1908, "seek": 957856, "start": 9585.76, "end": 9587.68, "text": " They actually have different behavior in different cases.", "tokens": [50724, 814, 767, 362, 819, 5223, 294, 819, 3331, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1300418174872964, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0002199200534960255}, {"id": 1909, "seek": 957856, "start": 9587.68, "end": 9592.4, "text": " And so this is why it's difficult to retrofit this into existing Python code and make it", "tokens": [50820, 400, 370, 341, 307, 983, 309, 311, 2252, 281, 18820, 6845, 341, 666, 6741, 15329, 3089, 293, 652, 309, 51056], "temperature": 0.0, "avg_logprob": -0.1300418174872964, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0002199200534960255}, {"id": 1910, "seek": 957856, "start": 9593.76, "end": 9597.439999999999, "text": " play well with typing. You kind of have to design for that.", "tokens": [51124, 862, 731, 365, 18444, 13, 509, 733, 295, 362, 281, 1715, 337, 300, 13, 51308], "temperature": 0.0, "avg_logprob": -0.1300418174872964, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0002199200534960255}, {"id": 1911, "seek": 957856, "start": 9597.439999999999, "end": 9602.64, "text": " Okay. So there's an interesting distinction that people that program Python might be interested in", "tokens": [51308, 1033, 13, 407, 456, 311, 364, 1880, 16844, 300, 561, 300, 1461, 15329, 1062, 312, 3102, 294, 51568], "temperature": 0.0, "avg_logprob": -0.1300418174872964, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0002199200534960255}, {"id": 1912, "seek": 957856, "start": 9602.64, "end": 9607.68, "text": " is def versus fn. So it's two different ways to define a function.", "tokens": [51568, 307, 1060, 5717, 283, 77, 13, 407, 309, 311, 732, 819, 2098, 281, 6964, 257, 2445, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1300418174872964, "compression_ratio": 1.7243816254416962, "no_speech_prob": 0.0002199200534960255}, {"id": 1913, "seek": 960856, "start": 9608.96, "end": 9615.6, "text": " And fn is a stricter version of def. What's the coolness that comes from the strictness?", "tokens": [50384, 400, 283, 77, 307, 257, 1056, 299, 391, 3037, 295, 1060, 13, 708, 311, 264, 1627, 1287, 300, 1487, 490, 264, 10910, 1287, 30, 50716], "temperature": 0.0, "avg_logprob": -0.15476862589518228, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007095627370290458}, {"id": 1914, "seek": 960856, "start": 9616.16, "end": 9619.119999999999, "text": " So here you get into what is the trade off with the superset?", "tokens": [50744, 407, 510, 291, 483, 666, 437, 307, 264, 4923, 766, 365, 264, 37906, 302, 30, 50892], "temperature": 0.0, "avg_logprob": -0.15476862589518228, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007095627370290458}, {"id": 1915, "seek": 960856, "start": 9619.76, "end": 9620.0, "text": " Yes.", "tokens": [50924, 1079, 13, 50936], "temperature": 0.0, "avg_logprob": -0.15476862589518228, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007095627370290458}, {"id": 1916, "seek": 960856, "start": 9621.119999999999, "end": 9624.96, "text": " So superset, you have to, or you really want to be compatible.", "tokens": [50992, 407, 37906, 302, 11, 291, 362, 281, 11, 420, 291, 534, 528, 281, 312, 18218, 13, 51184], "temperature": 0.0, "avg_logprob": -0.15476862589518228, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007095627370290458}, {"id": 1917, "seek": 960856, "start": 9625.68, "end": 9630.0, "text": " If you're doing a superset, you've decided compatibility with existing code", "tokens": [51220, 759, 291, 434, 884, 257, 37906, 302, 11, 291, 600, 3047, 34237, 365, 6741, 3089, 51436], "temperature": 0.0, "avg_logprob": -0.15476862589518228, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007095627370290458}, {"id": 1918, "seek": 960856, "start": 9630.0, "end": 9633.76, "text": " is the important thing, even if some of the decisions they made were maybe not what you'd", "tokens": [51436, 307, 264, 1021, 551, 11, 754, 498, 512, 295, 264, 5327, 436, 1027, 645, 1310, 406, 437, 291, 1116, 51624], "temperature": 0.0, "avg_logprob": -0.15476862589518228, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0007095627370290458}, {"id": 1919, "seek": 963376, "start": 9633.76, "end": 9638.32, "text": " choose. Okay. So that means you put a lot of time in compatibility,", "tokens": [50364, 2826, 13, 1033, 13, 407, 300, 1355, 291, 829, 257, 688, 295, 565, 294, 34237, 11, 50592], "temperature": 0.0, "avg_logprob": -0.11623042399489035, "compression_ratio": 1.7422680412371134, "no_speech_prob": 0.006691512651741505}, {"id": 1920, "seek": 963376, "start": 9638.32, "end": 9640.800000000001, "text": " and it means that you get locked into decisions of the past,", "tokens": [50592, 293, 309, 1355, 300, 291, 483, 9376, 666, 5327, 295, 264, 1791, 11, 50716], "temperature": 0.0, "avg_logprob": -0.11623042399489035, "compression_ratio": 1.7422680412371134, "no_speech_prob": 0.006691512651741505}, {"id": 1921, "seek": 963376, "start": 9641.84, "end": 9647.36, "text": " even if they may not have been a good thing. Now, systems programmers typically like to control", "tokens": [50768, 754, 498, 436, 815, 406, 362, 668, 257, 665, 551, 13, 823, 11, 3652, 41504, 5850, 411, 281, 1969, 51044], "temperature": 0.0, "avg_logprob": -0.11623042399489035, "compression_ratio": 1.7422680412371134, "no_speech_prob": 0.006691512651741505}, {"id": 1922, "seek": 963376, "start": 9647.36, "end": 9654.0, "text": " things. And they want to make sure that, not in all cases, of course, and even systems programmers", "tokens": [51044, 721, 13, 400, 436, 528, 281, 652, 988, 300, 11, 406, 294, 439, 3331, 11, 295, 1164, 11, 293, 754, 3652, 41504, 51376], "temperature": 0.0, "avg_logprob": -0.11623042399489035, "compression_ratio": 1.7422680412371134, "no_speech_prob": 0.006691512651741505}, {"id": 1923, "seek": 963376, "start": 9654.0, "end": 9659.52, "text": " are not one thing, but often you want predictability. And so one of the things that Python has,", "tokens": [51376, 366, 406, 472, 551, 11, 457, 2049, 291, 528, 6069, 2310, 13, 400, 370, 472, 295, 264, 721, 300, 15329, 575, 11, 51652], "temperature": 0.0, "avg_logprob": -0.11623042399489035, "compression_ratio": 1.7422680412371134, "no_speech_prob": 0.006691512651741505}, {"id": 1924, "seek": 963376, "start": 9659.52, "end": 9663.36, "text": " for example, as you know, is that if you define a variable, you just say x equals four.", "tokens": [51652, 337, 1365, 11, 382, 291, 458, 11, 307, 300, 498, 291, 6964, 257, 7006, 11, 291, 445, 584, 2031, 6915, 1451, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11623042399489035, "compression_ratio": 1.7422680412371134, "no_speech_prob": 0.006691512651741505}, {"id": 1925, "seek": 966376, "start": 9664.0, "end": 9670.48, "text": " I have a variable name to x. Now I say some long, some long name equals 17.", "tokens": [50376, 286, 362, 257, 7006, 1315, 281, 2031, 13, 823, 286, 584, 512, 938, 11, 512, 938, 1315, 6915, 3282, 13, 50700], "temperature": 0.0, "avg_logprob": -0.13784509364182387, "compression_ratio": 1.6875, "no_speech_prob": 0.0002694502763915807}, {"id": 1926, "seek": 966376, "start": 9671.28, "end": 9677.28, "text": " Print out some long name. Oops, I typoed it. Right. Well, the compiler, the Python compiler", "tokens": [50740, 34439, 484, 512, 938, 1315, 13, 21726, 11, 286, 2125, 78, 292, 309, 13, 1779, 13, 1042, 11, 264, 31958, 11, 264, 15329, 31958, 51040], "temperature": 0.0, "avg_logprob": -0.13784509364182387, "compression_ratio": 1.6875, "no_speech_prob": 0.0002694502763915807}, {"id": 1927, "seek": 966376, "start": 9677.28, "end": 9682.56, "text": " doesn't know, in all cases, what you're defining and what you're using. And did you typo the use", "tokens": [51040, 1177, 380, 458, 11, 294, 439, 3331, 11, 437, 291, 434, 17827, 293, 437, 291, 434, 1228, 13, 400, 630, 291, 2125, 78, 264, 764, 51304], "temperature": 0.0, "avg_logprob": -0.13784509364182387, "compression_ratio": 1.6875, "no_speech_prob": 0.0002694502763915807}, {"id": 1928, "seek": 966376, "start": 9682.56, "end": 9689.36, "text": " of it or the definition? Right. And so for people coming from type languages, again, I'm not saying", "tokens": [51304, 295, 309, 420, 264, 7123, 30, 1779, 13, 400, 370, 337, 561, 1348, 490, 2010, 8650, 11, 797, 11, 286, 478, 406, 1566, 51644], "temperature": 0.0, "avg_logprob": -0.13784509364182387, "compression_ratio": 1.6875, "no_speech_prob": 0.0002694502763915807}, {"id": 1929, "seek": 966376, "start": 9689.36, "end": 9693.28, "text": " the right or wrong, but that drives them crazy because they want the compiler to tell them you", "tokens": [51644, 264, 558, 420, 2085, 11, 457, 300, 11754, 552, 3219, 570, 436, 528, 264, 31958, 281, 980, 552, 291, 51840], "temperature": 0.0, "avg_logprob": -0.13784509364182387, "compression_ratio": 1.6875, "no_speech_prob": 0.0002694502763915807}, {"id": 1930, "seek": 969328, "start": 9693.28, "end": 9698.16, "text": " typoed the name of this thing. Right. And so what fn does is it turns on, as you say, it's a strict", "tokens": [50364, 2125, 78, 292, 264, 1315, 295, 341, 551, 13, 1779, 13, 400, 370, 437, 283, 77, 775, 307, 309, 4523, 322, 11, 382, 291, 584, 11, 309, 311, 257, 10910, 50608], "temperature": 0.0, "avg_logprob": -0.11265874416270155, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.002396155381575227}, {"id": 1931, "seek": 969328, "start": 9698.16, "end": 9702.08, "text": " mode. And so it says, okay, well, you have to actually declare, intentionally declare your", "tokens": [50608, 4391, 13, 400, 370, 309, 1619, 11, 1392, 11, 731, 11, 291, 362, 281, 767, 19710, 11, 22062, 19710, 428, 50804], "temperature": 0.0, "avg_logprob": -0.11265874416270155, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.002396155381575227}, {"id": 1932, "seek": 969328, "start": 9702.08, "end": 9706.640000000001, "text": " variables before you use them. That gives you more predictability, more error checking and", "tokens": [50804, 9102, 949, 291, 764, 552, 13, 663, 2709, 291, 544, 6069, 2310, 11, 544, 6713, 8568, 293, 51032], "temperature": 0.0, "avg_logprob": -0.11265874416270155, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.002396155381575227}, {"id": 1933, "seek": 969328, "start": 9706.640000000001, "end": 9714.960000000001, "text": " things like this. But you don't have to use it. And this is a way that Mojo is both compatible", "tokens": [51032, 721, 411, 341, 13, 583, 291, 500, 380, 362, 281, 764, 309, 13, 400, 341, 307, 257, 636, 300, 3335, 5134, 307, 1293, 18218, 51448], "temperature": 0.0, "avg_logprob": -0.11265874416270155, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.002396155381575227}, {"id": 1934, "seek": 969328, "start": 9714.960000000001, "end": 9718.800000000001, "text": " because devs work the same way that devs have already always worked. But it provides a new", "tokens": [51448, 570, 1905, 82, 589, 264, 912, 636, 300, 1905, 82, 362, 1217, 1009, 2732, 13, 583, 309, 6417, 257, 777, 51640], "temperature": 0.0, "avg_logprob": -0.11265874416270155, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.002396155381575227}, {"id": 1935, "seek": 969328, "start": 9718.800000000001, "end": 9722.640000000001, "text": " alternative that gives you more control and allows certain kinds of people to have a different", "tokens": [51640, 8535, 300, 2709, 291, 544, 1969, 293, 4045, 1629, 3685, 295, 561, 281, 362, 257, 819, 51832], "temperature": 0.0, "avg_logprob": -0.11265874416270155, "compression_ratio": 1.7672955974842768, "no_speech_prob": 0.002396155381575227}, {"id": 1936, "seek": 972264, "start": 9722.64, "end": 9728.24, "text": " philosophy to be able to express that and get that. But usually, if you're writing Mojo code", "tokens": [50364, 10675, 281, 312, 1075, 281, 5109, 300, 293, 483, 300, 13, 583, 2673, 11, 498, 291, 434, 3579, 3335, 5134, 3089, 50644], "temperature": 0.0, "avg_logprob": -0.1695831210114235, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.0043305689468979836}, {"id": 1937, "seek": 972264, "start": 9728.24, "end": 9734.16, "text": " from scratch, you'll be using fn. It depends. Again, it depends on your mentality. It's not", "tokens": [50644, 490, 8459, 11, 291, 603, 312, 1228, 283, 77, 13, 467, 5946, 13, 3764, 11, 309, 5946, 322, 428, 21976, 13, 467, 311, 406, 50940], "temperature": 0.0, "avg_logprob": -0.1695831210114235, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.0043305689468979836}, {"id": 1938, "seek": 972264, "start": 9734.16, "end": 9740.4, "text": " that dev is Python and fn is Mojo. Mojo has both and it loves both. It really depends on...", "tokens": [50940, 300, 1905, 307, 15329, 293, 283, 77, 307, 3335, 5134, 13, 3335, 5134, 575, 1293, 293, 309, 6752, 1293, 13, 467, 534, 5946, 322, 485, 51252], "temperature": 0.0, "avg_logprob": -0.1695831210114235, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.0043305689468979836}, {"id": 1939, "seek": 972264, "start": 9740.4, "end": 9745.039999999999, "text": " Time is just strict. Yeah, exactly. Are you playing around and scripting something out?", "tokens": [51252, 6161, 307, 445, 10910, 13, 865, 11, 2293, 13, 2014, 291, 2433, 926, 293, 5755, 278, 746, 484, 30, 51484], "temperature": 0.0, "avg_logprob": -0.1695831210114235, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.0043305689468979836}, {"id": 1940, "seek": 972264, "start": 9745.039999999999, "end": 9750.96, "text": " Is it a one off throwaway script? Cool. Python is great at that. I will still be using fn, but yeah.", "tokens": [51484, 1119, 309, 257, 472, 766, 3507, 10318, 5755, 30, 8561, 13, 15329, 307, 869, 412, 300, 13, 286, 486, 920, 312, 1228, 283, 77, 11, 457, 1338, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1695831210114235, "compression_ratio": 1.6373239436619718, "no_speech_prob": 0.0043305689468979836}, {"id": 1941, "seek": 975096, "start": 9751.679999999998, "end": 9757.359999999999, "text": " I love strictness. Control, power. You also like suffering, right?", "tokens": [50400, 286, 959, 10910, 1287, 13, 12912, 11, 1347, 13, 509, 611, 411, 7755, 11, 558, 30, 50684], "temperature": 0.0, "avg_logprob": -0.17225607451017913, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.005909950006753206}, {"id": 1942, "seek": 975096, "start": 9758.0, "end": 9765.599999999999, "text": " Yes. You go hand in hand. How many pull-ups? I've lost count at this point.", "tokens": [50716, 1079, 13, 509, 352, 1011, 294, 1011, 13, 1012, 867, 2235, 12, 7528, 30, 286, 600, 2731, 1207, 412, 341, 935, 13, 51096], "temperature": 0.0, "avg_logprob": -0.17225607451017913, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.005909950006753206}, {"id": 1943, "seek": 975096, "start": 9766.32, "end": 9770.24, "text": " And that's cool. I love you for that. And I love other people who like strict things,", "tokens": [51132, 400, 300, 311, 1627, 13, 286, 959, 291, 337, 300, 13, 400, 286, 959, 661, 561, 567, 411, 10910, 721, 11, 51328], "temperature": 0.0, "avg_logprob": -0.17225607451017913, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.005909950006753206}, {"id": 1944, "seek": 975096, "start": 9770.24, "end": 9775.119999999999, "text": " right? But I don't want to say that that's the right thing because Python is also very beautiful", "tokens": [51328, 558, 30, 583, 286, 500, 380, 528, 281, 584, 300, 300, 311, 264, 558, 551, 570, 15329, 307, 611, 588, 2238, 51572], "temperature": 0.0, "avg_logprob": -0.17225607451017913, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.005909950006753206}, {"id": 1945, "seek": 975096, "start": 9775.119999999999, "end": 9779.519999999999, "text": " for hacking around and doing stuff and research and these other cases where you may not want that.", "tokens": [51572, 337, 31422, 926, 293, 884, 1507, 293, 2132, 293, 613, 661, 3331, 689, 291, 815, 406, 528, 300, 13, 51792], "temperature": 0.0, "avg_logprob": -0.17225607451017913, "compression_ratio": 1.6627450980392158, "no_speech_prob": 0.005909950006753206}, {"id": 1946, "seek": 977952, "start": 9779.52, "end": 9783.92, "text": " You see, I just feel like... Maybe I'm wrong with that, but it feels like strictness", "tokens": [50364, 509, 536, 11, 286, 445, 841, 411, 485, 2704, 286, 478, 2085, 365, 300, 11, 457, 309, 3417, 411, 10910, 1287, 50584], "temperature": 0.0, "avg_logprob": -0.15958084737447867, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0006985316285863519}, {"id": 1947, "seek": 977952, "start": 9783.92, "end": 9789.92, "text": " leads to faster debugging. So in terms of going from... Even on a small project from zero to", "tokens": [50584, 6689, 281, 4663, 45592, 13, 407, 294, 2115, 295, 516, 490, 485, 2754, 322, 257, 1359, 1716, 490, 4018, 281, 50884], "temperature": 0.0, "avg_logprob": -0.15958084737447867, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0006985316285863519}, {"id": 1948, "seek": 977952, "start": 9789.92, "end": 9794.32, "text": " completion, it just... I guess it depends how many bugs you generate usually.", "tokens": [50884, 19372, 11, 309, 445, 485, 286, 2041, 309, 5946, 577, 867, 15120, 291, 8460, 2673, 13, 51104], "temperature": 0.0, "avg_logprob": -0.15958084737447867, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0006985316285863519}, {"id": 1949, "seek": 977952, "start": 9794.880000000001, "end": 9798.4, "text": " Well, so I mean, it's again lessons learned and looking at the ecosystem. It's really...", "tokens": [51132, 1042, 11, 370, 286, 914, 11, 309, 311, 797, 8820, 3264, 293, 1237, 412, 264, 11311, 13, 467, 311, 534, 485, 51308], "temperature": 0.0, "avg_logprob": -0.15958084737447867, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0006985316285863519}, {"id": 1950, "seek": 977952, "start": 9799.28, "end": 9804.16, "text": " I mean, I think it's... If you study some of these languages over time, like the Ruby community,", "tokens": [51352, 286, 914, 11, 286, 519, 309, 311, 485, 759, 291, 2979, 512, 295, 613, 8650, 670, 565, 11, 411, 264, 19907, 1768, 11, 51596], "temperature": 0.0, "avg_logprob": -0.15958084737447867, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0006985316285863519}, {"id": 1951, "seek": 977952, "start": 9804.16, "end": 9808.48, "text": " for example. Now, Ruby is a pretty well-developed, pretty established community,", "tokens": [51596, 337, 1365, 13, 823, 11, 19907, 307, 257, 1238, 731, 12, 35464, 292, 11, 1238, 7545, 1768, 11, 51812], "temperature": 0.0, "avg_logprob": -0.15958084737447867, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0006985316285863519}, {"id": 1952, "seek": 980848, "start": 9808.48, "end": 9813.92, "text": " but along their path, they really invested in unit testing. So I think that the Ruby community is", "tokens": [50364, 457, 2051, 641, 3100, 11, 436, 534, 13104, 294, 4985, 4997, 13, 407, 286, 519, 300, 264, 19907, 1768, 307, 50636], "temperature": 0.0, "avg_logprob": -0.09352977692134797, "compression_ratio": 1.733974358974359, "no_speech_prob": 0.000882889551576227}, {"id": 1953, "seek": 980848, "start": 9813.92, "end": 9818.48, "text": " really pushed forward the state-of-the-art of testing because they didn't have a type system", "tokens": [50636, 534, 9152, 2128, 264, 1785, 12, 2670, 12, 3322, 12, 446, 295, 4997, 570, 436, 994, 380, 362, 257, 2010, 1185, 50864], "temperature": 0.0, "avg_logprob": -0.09352977692134797, "compression_ratio": 1.733974358974359, "no_speech_prob": 0.000882889551576227}, {"id": 1954, "seek": 980848, "start": 9818.48, "end": 9823.119999999999, "text": " that caught a lot of bugs at compile time. And so you can have the best of both worlds. You", "tokens": [50864, 300, 5415, 257, 688, 295, 15120, 412, 31413, 565, 13, 400, 370, 291, 393, 362, 264, 1151, 295, 1293, 13401, 13, 509, 51096], "temperature": 0.0, "avg_logprob": -0.09352977692134797, "compression_ratio": 1.733974358974359, "no_speech_prob": 0.000882889551576227}, {"id": 1955, "seek": 980848, "start": 9823.119999999999, "end": 9827.359999999999, "text": " can have good testing and good types and things like this. But I thought that it was really", "tokens": [51096, 393, 362, 665, 4997, 293, 665, 3467, 293, 721, 411, 341, 13, 583, 286, 1194, 300, 309, 390, 534, 51308], "temperature": 0.0, "avg_logprob": -0.09352977692134797, "compression_ratio": 1.733974358974359, "no_speech_prob": 0.000882889551576227}, {"id": 1956, "seek": 980848, "start": 9827.359999999999, "end": 9831.119999999999, "text": " interesting to see how certain challenges get solved. And in Python, for example,", "tokens": [51308, 1880, 281, 536, 577, 1629, 4759, 483, 13041, 13, 400, 294, 15329, 11, 337, 1365, 11, 51496], "temperature": 0.0, "avg_logprob": -0.09352977692134797, "compression_ratio": 1.733974358974359, "no_speech_prob": 0.000882889551576227}, {"id": 1957, "seek": 980848, "start": 9831.92, "end": 9835.439999999999, "text": " the interactive notebook kind of experiences and stuff like this are really amazing.", "tokens": [51536, 264, 15141, 21060, 733, 295, 5235, 293, 1507, 411, 341, 366, 534, 2243, 13, 51712], "temperature": 0.0, "avg_logprob": -0.09352977692134797, "compression_ratio": 1.733974358974359, "no_speech_prob": 0.000882889551576227}, {"id": 1958, "seek": 983544, "start": 9835.44, "end": 9839.68, "text": " And if you typo something, it doesn't matter. It just tells you. That's fine, right? And so", "tokens": [50364, 400, 498, 291, 2125, 78, 746, 11, 309, 1177, 380, 1871, 13, 467, 445, 5112, 291, 13, 663, 311, 2489, 11, 558, 30, 400, 370, 50576], "temperature": 0.0, "avg_logprob": -0.19038600581032888, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.012049604207277298}, {"id": 1959, "seek": 983544, "start": 9839.68, "end": 9844.640000000001, "text": " I think that the tryouts are very different if you're building a large-scale production system", "tokens": [50576, 286, 519, 300, 264, 853, 7711, 366, 588, 819, 498, 291, 434, 2390, 257, 2416, 12, 20033, 4265, 1185, 50824], "temperature": 0.0, "avg_logprob": -0.19038600581032888, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.012049604207277298}, {"id": 1960, "seek": 983544, "start": 9844.640000000001, "end": 9849.2, "text": " versus you're building and exploring in a notebook. And speaking of control, the hilarious thing, if", "tokens": [50824, 5717, 291, 434, 2390, 293, 12736, 294, 257, 21060, 13, 400, 4124, 295, 1969, 11, 264, 19796, 551, 11, 498, 51052], "temperature": 0.0, "avg_logprob": -0.19038600581032888, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.012049604207277298}, {"id": 1961, "seek": 983544, "start": 9849.2, "end": 9854.24, "text": " you look at code, I write just for myself for fun. It's like littered with asserts everywhere.", "tokens": [51052, 291, 574, 412, 3089, 11, 286, 2464, 445, 337, 2059, 337, 1019, 13, 467, 311, 411, 26540, 292, 365, 19810, 82, 5315, 13, 51304], "temperature": 0.0, "avg_logprob": -0.19038600581032888, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.012049604207277298}, {"id": 1962, "seek": 983544, "start": 9856.24, "end": 9858.24, "text": " It's a kind of... Yeah, you'd like that.", "tokens": [51404, 467, 311, 257, 733, 295, 485, 865, 11, 291, 1116, 411, 300, 13, 51504], "temperature": 0.0, "avg_logprob": -0.19038600581032888, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.012049604207277298}, {"id": 1963, "seek": 985824, "start": 9858.48, "end": 9865.92, "text": " Yes. It's basically saying in a dictatorial way, this should be true now. Otherwise,", "tokens": [50376, 1079, 13, 467, 311, 1936, 1566, 294, 257, 42852, 831, 636, 11, 341, 820, 312, 2074, 586, 13, 10328, 11, 50748], "temperature": 0.0, "avg_logprob": -0.19635557420182936, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.0367487370967865}, {"id": 1964, "seek": 985824, "start": 9865.92, "end": 9873.52, "text": " everything stops. And that is the sign. I can't... I love you, man. But that is the sign of somebody", "tokens": [50748, 1203, 10094, 13, 400, 300, 307, 264, 1465, 13, 286, 393, 380, 485, 286, 959, 291, 11, 587, 13, 583, 300, 307, 264, 1465, 295, 2618, 51128], "temperature": 0.0, "avg_logprob": -0.19635557420182936, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.0367487370967865}, {"id": 1965, "seek": 985824, "start": 9873.52, "end": 9878.8, "text": " who likes control. And so, yes, I think that you'll like... And I think you'll like mojo.", "tokens": [51128, 567, 5902, 1969, 13, 400, 370, 11, 2086, 11, 286, 519, 300, 291, 603, 411, 485, 400, 286, 519, 291, 603, 411, 705, 5134, 13, 51392], "temperature": 0.0, "avg_logprob": -0.19635557420182936, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.0367487370967865}, {"id": 1966, "seek": 985824, "start": 9878.8, "end": 9886.08, "text": " Therapy session. Yes, I definitely will. Speaking of asserts, exceptions are called errors.", "tokens": [51392, 36812, 88, 5481, 13, 1079, 11, 286, 2138, 486, 13, 13069, 295, 19810, 82, 11, 22847, 366, 1219, 13603, 13, 51756], "temperature": 0.0, "avg_logprob": -0.19635557420182936, "compression_ratio": 1.5751072961373391, "no_speech_prob": 0.0367487370967865}, {"id": 1967, "seek": 988608, "start": 9886.16, "end": 9892.32, "text": " Why is it called errors? So, I mean, we use the same... We're the same as Python, right? But we", "tokens": [50368, 1545, 307, 309, 1219, 13603, 30, 407, 11, 286, 914, 11, 321, 764, 264, 912, 485, 492, 434, 264, 912, 382, 15329, 11, 558, 30, 583, 321, 50676], "temperature": 0.0, "avg_logprob": -0.15374964759463355, "compression_ratio": 1.5, "no_speech_prob": 0.016397103667259216}, {"id": 1968, "seek": 988608, "start": 9892.32, "end": 9897.6, "text": " implement it a very different way. And so, if you look at other languages, like we'll pick on C++,", "tokens": [50676, 4445, 309, 257, 588, 819, 636, 13, 400, 370, 11, 498, 291, 574, 412, 661, 8650, 11, 411, 321, 603, 1888, 322, 383, 25472, 11, 50940], "temperature": 0.0, "avg_logprob": -0.15374964759463355, "compression_ratio": 1.5, "no_speech_prob": 0.016397103667259216}, {"id": 1969, "seek": 988608, "start": 9897.6, "end": 9904.4, "text": " our favorite, right? C++ has a thing called zero-cost exception handling. Okay. And this is,", "tokens": [50940, 527, 2954, 11, 558, 30, 383, 25472, 575, 257, 551, 1219, 4018, 12, 27718, 11183, 13175, 13, 1033, 13, 400, 341, 307, 11, 51280], "temperature": 0.0, "avg_logprob": -0.15374964759463355, "compression_ratio": 1.5, "no_speech_prob": 0.016397103667259216}, {"id": 1970, "seek": 988608, "start": 9905.68, "end": 9911.52, "text": " in my opinion, something to learn lessons from. It's a nice polite way of saying it.", "tokens": [51344, 294, 452, 4800, 11, 746, 281, 1466, 8820, 490, 13, 467, 311, 257, 1481, 25171, 636, 295, 1566, 309, 13, 51636], "temperature": 0.0, "avg_logprob": -0.15374964759463355, "compression_ratio": 1.5, "no_speech_prob": 0.016397103667259216}, {"id": 1971, "seek": 991152, "start": 9911.52, "end": 9918.560000000001, "text": " And so, zero-cost exception handling, the way it works is that it's called zero-cost because", "tokens": [50364, 400, 370, 11, 4018, 12, 27718, 11183, 13175, 11, 264, 636, 309, 1985, 307, 300, 309, 311, 1219, 4018, 12, 27718, 570, 50716], "temperature": 0.0, "avg_logprob": -0.06733349180713143, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0007793118129484355}, {"id": 1972, "seek": 991152, "start": 9919.44, "end": 9925.44, "text": " if you don't throw an exception, there's supposed to be no overhead for the non-error code. And so,", "tokens": [50760, 498, 291, 500, 380, 3507, 364, 11183, 11, 456, 311, 3442, 281, 312, 572, 19922, 337, 264, 2107, 12, 260, 2874, 3089, 13, 400, 370, 11, 51060], "temperature": 0.0, "avg_logprob": -0.06733349180713143, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0007793118129484355}, {"id": 1973, "seek": 991152, "start": 9925.44, "end": 9933.76, "text": " it takes the error path out of the common path. It does this by making throwing an error extremely", "tokens": [51060, 309, 2516, 264, 6713, 3100, 484, 295, 264, 2689, 3100, 13, 467, 775, 341, 538, 1455, 10238, 364, 6713, 4664, 51476], "temperature": 0.0, "avg_logprob": -0.06733349180713143, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0007793118129484355}, {"id": 1974, "seek": 991152, "start": 9933.76, "end": 9939.12, "text": " expensive. And so, if you actually throw an error with a C++ compiler using exceptions,", "tokens": [51476, 5124, 13, 400, 370, 11, 498, 291, 767, 3507, 364, 6713, 365, 257, 383, 25472, 31958, 1228, 22847, 11, 51744], "temperature": 0.0, "avg_logprob": -0.06733349180713143, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.0007793118129484355}, {"id": 1975, "seek": 993912, "start": 9939.2, "end": 9942.960000000001, "text": " let's go look up in tables on the side and do all this stuff. And so, throwing an error could be", "tokens": [50368, 718, 311, 352, 574, 493, 294, 8020, 322, 264, 1252, 293, 360, 439, 341, 1507, 13, 400, 370, 11, 10238, 364, 6713, 727, 312, 50556], "temperature": 0.0, "avg_logprob": -0.0926186852421321, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.001926514320075512}, {"id": 1976, "seek": 993912, "start": 9942.960000000001, "end": 9949.2, "text": " like 10,000 times more expensive than returning from a function, right? Also, it's called zero-cost", "tokens": [50556, 411, 1266, 11, 1360, 1413, 544, 5124, 813, 12678, 490, 257, 2445, 11, 558, 30, 2743, 11, 309, 311, 1219, 4018, 12, 27718, 50868], "temperature": 0.0, "avg_logprob": -0.0926186852421321, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.001926514320075512}, {"id": 1977, "seek": 993912, "start": 9949.2, "end": 9953.84, "text": " exceptions, but it's not zero-cost. By any stretch of the imagination, because it massively blows", "tokens": [50868, 22847, 11, 457, 309, 311, 406, 4018, 12, 27718, 13, 3146, 604, 5985, 295, 264, 12938, 11, 570, 309, 29379, 18458, 51100], "temperature": 0.0, "avg_logprob": -0.0926186852421321, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.001926514320075512}, {"id": 1978, "seek": 993912, "start": 9953.84, "end": 9959.6, "text": " out your code, your binary, it also adds a whole bunch of different paths because of", "tokens": [51100, 484, 428, 3089, 11, 428, 17434, 11, 309, 611, 10860, 257, 1379, 3840, 295, 819, 14518, 570, 295, 51388], "temperature": 0.0, "avg_logprob": -0.0926186852421321, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.001926514320075512}, {"id": 1979, "seek": 993912, "start": 9959.6, "end": 9963.76, "text": " destructors and other things like that that exist in C++. And it reduces the number of", "tokens": [51388, 2677, 1757, 830, 293, 661, 721, 411, 300, 300, 2514, 294, 383, 25472, 13, 400, 309, 18081, 264, 1230, 295, 51596], "temperature": 0.0, "avg_logprob": -0.0926186852421321, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.001926514320075512}, {"id": 1980, "seek": 993912, "start": 9963.76, "end": 9968.240000000002, "text": " optimizations. It adds like all these effects. And so, this thing that was called zero-cost", "tokens": [51596, 5028, 14455, 13, 467, 10860, 411, 439, 613, 5065, 13, 400, 370, 11, 341, 551, 300, 390, 1219, 4018, 12, 27718, 51820], "temperature": 0.0, "avg_logprob": -0.0926186852421321, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.001926514320075512}, {"id": 1981, "seek": 996824, "start": 9968.24, "end": 9977.28, "text": " exceptions, it really ain't. Okay. Now, if you fast forward to newer languages, and this includes", "tokens": [50364, 22847, 11, 309, 534, 7862, 380, 13, 1033, 13, 823, 11, 498, 291, 2370, 2128, 281, 17628, 8650, 11, 293, 341, 5974, 50816], "temperature": 0.0, "avg_logprob": -0.11614023909276845, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0024722011294215918}, {"id": 1982, "seek": 996824, "start": 9977.28, "end": 9984.4, "text": " Swift and Rust and Go and now Mojo, well, and Python's a little bit different because it's", "tokens": [50816, 25539, 293, 34952, 293, 1037, 293, 586, 3335, 5134, 11, 731, 11, 293, 15329, 311, 257, 707, 857, 819, 570, 309, 311, 51172], "temperature": 0.0, "avg_logprob": -0.11614023909276845, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0024722011294215918}, {"id": 1983, "seek": 996824, "start": 9984.4, "end": 9987.92, "text": " interpreted. And so, it's got a little bit of a different thing going on. But if you look at", "tokens": [51172, 26749, 13, 400, 370, 11, 309, 311, 658, 257, 707, 857, 295, 257, 819, 551, 516, 322, 13, 583, 498, 291, 574, 412, 51348], "temperature": 0.0, "avg_logprob": -0.11614023909276845, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0024722011294215918}, {"id": 1984, "seek": 996824, "start": 9987.92, "end": 9995.68, "text": " compiled languages, many newer languages say, okay, well, let's not do that zero-cost exception", "tokens": [51348, 36548, 8650, 11, 867, 17628, 8650, 584, 11, 1392, 11, 731, 11, 718, 311, 406, 360, 300, 4018, 12, 27718, 11183, 51736], "temperature": 0.0, "avg_logprob": -0.11614023909276845, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0024722011294215918}, {"id": 1985, "seek": 999568, "start": 9995.68, "end": 10003.84, "text": " handling thing. Let's actually treat throwing an error the same as returning a variant, returning", "tokens": [50364, 13175, 551, 13, 961, 311, 767, 2387, 10238, 364, 6713, 264, 912, 382, 12678, 257, 17501, 11, 12678, 50772], "temperature": 0.0, "avg_logprob": -0.10616317662325772, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.004467891529202461}, {"id": 1986, "seek": 999568, "start": 10003.84, "end": 10010.720000000001, "text": " either the normal result or an error. Now, programmers generally don't want to deal with", "tokens": [50772, 2139, 264, 2710, 1874, 420, 364, 6713, 13, 823, 11, 41504, 5101, 500, 380, 528, 281, 2028, 365, 51116], "temperature": 0.0, "avg_logprob": -0.10616317662325772, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.004467891529202461}, {"id": 1987, "seek": 999568, "start": 10010.720000000001, "end": 10016.24, "text": " all the typing machinery and like pushing around a variant. And so, you use all the syntax that", "tokens": [51116, 439, 264, 18444, 27302, 293, 411, 7380, 926, 257, 17501, 13, 400, 370, 11, 291, 764, 439, 264, 28431, 300, 51392], "temperature": 0.0, "avg_logprob": -0.10616317662325772, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.004467891529202461}, {"id": 1988, "seek": 999568, "start": 10016.24, "end": 10021.44, "text": " Python gives us, for example, try and catch, you know, functions that raise and things like this,", "tokens": [51392, 15329, 2709, 505, 11, 337, 1365, 11, 853, 293, 3745, 11, 291, 458, 11, 6828, 300, 5300, 293, 721, 411, 341, 11, 51652], "temperature": 0.0, "avg_logprob": -0.10616317662325772, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.004467891529202461}, {"id": 1989, "seek": 1002144, "start": 10021.44, "end": 10026.640000000001, "text": " you can put a raises, decorator on your functions, stuff like this. And if you want to control that,", "tokens": [50364, 291, 393, 829, 257, 19658, 11, 7919, 1639, 322, 428, 6828, 11, 1507, 411, 341, 13, 400, 498, 291, 528, 281, 1969, 300, 11, 50624], "temperature": 0.0, "avg_logprob": -0.16713823590959823, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.004330493975430727}, {"id": 1990, "seek": 1002144, "start": 10026.640000000001, "end": 10032.480000000001, "text": " and then the language can provide syntax for it. But under the hood, the way the computer executes it,", "tokens": [50624, 293, 550, 264, 2856, 393, 2893, 28431, 337, 309, 13, 583, 833, 264, 13376, 11, 264, 636, 264, 3820, 4454, 1819, 309, 11, 50916], "temperature": 0.0, "avg_logprob": -0.16713823590959823, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.004330493975430727}, {"id": 1991, "seek": 1002144, "start": 10032.480000000001, "end": 10035.52, "text": " throwing an error is basically as fast as returning something.", "tokens": [50916, 10238, 364, 6713, 307, 1936, 382, 2370, 382, 12678, 746, 13, 51068], "temperature": 0.0, "avg_logprob": -0.16713823590959823, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.004330493975430727}, {"id": 1992, "seek": 1002144, "start": 10035.52, "end": 10039.04, "text": " I think so it's exactly the same way from a compiled perspective.", "tokens": [51068, 286, 519, 370, 309, 311, 2293, 264, 912, 636, 490, 257, 36548, 4585, 13, 51244], "temperature": 0.0, "avg_logprob": -0.16713823590959823, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.004330493975430727}, {"id": 1993, "seek": 1002144, "start": 10039.04, "end": 10044.960000000001, "text": " And so, this is actually, I mean, it's a fairly nerdy thing, right? Which is why I love it. But", "tokens": [51244, 400, 370, 11, 341, 307, 767, 11, 286, 914, 11, 309, 311, 257, 6457, 18219, 3173, 551, 11, 558, 30, 3013, 307, 983, 286, 959, 309, 13, 583, 51540], "temperature": 0.0, "avg_logprob": -0.16713823590959823, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.004330493975430727}, {"id": 1994, "seek": 1004496, "start": 10045.919999999998, "end": 10053.279999999999, "text": " this has a huge impact on the way you design your APIs, right? So in C++, huge communities turn", "tokens": [50412, 341, 575, 257, 2603, 2712, 322, 264, 636, 291, 1715, 428, 21445, 11, 558, 30, 407, 294, 383, 25472, 11, 2603, 4456, 1261, 50780], "temperature": 0.0, "avg_logprob": -0.10919210843950788, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.010984898544847965}, {"id": 1995, "seek": 1004496, "start": 10053.279999999999, "end": 10060.32, "text": " off exceptions, because the cost is just so high, right? And so the zero cost cost is so high, right?", "tokens": [50780, 766, 22847, 11, 570, 264, 2063, 307, 445, 370, 1090, 11, 558, 30, 400, 370, 264, 4018, 2063, 2063, 307, 370, 1090, 11, 558, 30, 51132], "temperature": 0.0, "avg_logprob": -0.10919210843950788, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.010984898544847965}, {"id": 1996, "seek": 1004496, "start": 10060.32, "end": 10067.519999999999, "text": " And so that means you can't actually use exceptions in many libraries, right? And even for the people", "tokens": [51132, 400, 370, 300, 1355, 291, 393, 380, 767, 764, 22847, 294, 867, 15148, 11, 558, 30, 400, 754, 337, 264, 561, 51492], "temperature": 0.0, "avg_logprob": -0.10919210843950788, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.010984898544847965}, {"id": 1997, "seek": 1004496, "start": 10067.519999999999, "end": 10073.439999999999, "text": " that do use it, well, okay, how and when do you want to pay the cost? If I try to open a file,", "tokens": [51492, 300, 360, 764, 309, 11, 731, 11, 1392, 11, 577, 293, 562, 360, 291, 528, 281, 1689, 264, 2063, 30, 759, 286, 853, 281, 1269, 257, 3991, 11, 51788], "temperature": 0.0, "avg_logprob": -0.10919210843950788, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.010984898544847965}, {"id": 1998, "seek": 1007344, "start": 10073.44, "end": 10078.4, "text": " should I throw an error? Well, what if I'm probing around looking for something, right?", "tokens": [50364, 820, 286, 3507, 364, 6713, 30, 1042, 11, 437, 498, 286, 478, 1239, 278, 926, 1237, 337, 746, 11, 558, 30, 50612], "temperature": 0.0, "avg_logprob": -0.09701587132045202, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.0020503464620560408}, {"id": 1999, "seek": 1007344, "start": 10078.4, "end": 10082.4, "text": " I'm looking it up in many different paths. Well, if it's really slow to do that, maybe I'll add", "tokens": [50612, 286, 478, 1237, 309, 493, 294, 867, 819, 14518, 13, 1042, 11, 498, 309, 311, 534, 2964, 281, 360, 300, 11, 1310, 286, 603, 909, 50812], "temperature": 0.0, "avg_logprob": -0.09701587132045202, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.0020503464620560408}, {"id": 2000, "seek": 1007344, "start": 10082.4, "end": 10087.6, "text": " another function that doesn't throw an error returns an error code instead. And I have two", "tokens": [50812, 1071, 2445, 300, 1177, 380, 3507, 364, 6713, 11247, 364, 6713, 3089, 2602, 13, 400, 286, 362, 732, 51072], "temperature": 0.0, "avg_logprob": -0.09701587132045202, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.0020503464620560408}, {"id": 2001, "seek": 1007344, "start": 10087.6, "end": 10093.04, "text": " different versions the same thing. And so it causes you to fork your APIs. And so, you know,", "tokens": [51072, 819, 9606, 264, 912, 551, 13, 400, 370, 309, 7700, 291, 281, 17716, 428, 21445, 13, 400, 370, 11, 291, 458, 11, 51344], "temperature": 0.0, "avg_logprob": -0.09701587132045202, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.0020503464620560408}, {"id": 2002, "seek": 1007344, "start": 10093.04, "end": 10097.52, "text": " one of the things I learned from Apple and I so love is the art of API design is actually", "tokens": [51344, 472, 295, 264, 721, 286, 3264, 490, 6373, 293, 286, 370, 959, 307, 264, 1523, 295, 9362, 1715, 307, 767, 51568], "temperature": 0.0, "avg_logprob": -0.09701587132045202, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.0020503464620560408}, {"id": 2003, "seek": 1007344, "start": 10097.52, "end": 10101.68, "text": " really profound. I think this is something that Python's also done a pretty good job at in terms", "tokens": [51568, 534, 14382, 13, 286, 519, 341, 307, 746, 300, 15329, 311, 611, 1096, 257, 1238, 665, 1691, 412, 294, 2115, 51776], "temperature": 0.0, "avg_logprob": -0.09701587132045202, "compression_ratio": 1.720496894409938, "no_speech_prob": 0.0020503464620560408}, {"id": 2004, "seek": 1010168, "start": 10101.68, "end": 10106.64, "text": " of building out this large scale package ecosystem, it's about having standards and things like this.", "tokens": [50364, 295, 2390, 484, 341, 2416, 4373, 7372, 11311, 11, 309, 311, 466, 1419, 7787, 293, 721, 411, 341, 13, 50612], "temperature": 0.0, "avg_logprob": -0.06746961744569188, "compression_ratio": 1.804953560371517, "no_speech_prob": 0.0017005177214741707}, {"id": 2005, "seek": 1010168, "start": 10106.64, "end": 10111.92, "text": " And so, you know, we wouldn't want to enter a mode where, you know, there's this theoretical feature", "tokens": [50612, 400, 370, 11, 291, 458, 11, 321, 2759, 380, 528, 281, 3242, 257, 4391, 689, 11, 291, 458, 11, 456, 311, 341, 20864, 4111, 50876], "temperature": 0.0, "avg_logprob": -0.06746961744569188, "compression_ratio": 1.804953560371517, "no_speech_prob": 0.0017005177214741707}, {"id": 2006, "seek": 1010168, "start": 10111.92, "end": 10116.720000000001, "text": " that exists in language, but people don't use it in practice. Now, I'll also say one of the other", "tokens": [50876, 300, 8198, 294, 2856, 11, 457, 561, 500, 380, 764, 309, 294, 3124, 13, 823, 11, 286, 603, 611, 584, 472, 295, 264, 661, 51116], "temperature": 0.0, "avg_logprob": -0.06746961744569188, "compression_ratio": 1.804953560371517, "no_speech_prob": 0.0017005177214741707}, {"id": 2007, "seek": 1010168, "start": 10116.720000000001, "end": 10120.800000000001, "text": " really cool things about this implementation approach is that it can run on GPUs and it can run", "tokens": [51116, 534, 1627, 721, 466, 341, 11420, 3109, 307, 300, 309, 393, 1190, 322, 18407, 82, 293, 309, 393, 1190, 51320], "temperature": 0.0, "avg_logprob": -0.06746961744569188, "compression_ratio": 1.804953560371517, "no_speech_prob": 0.0017005177214741707}, {"id": 2008, "seek": 1010168, "start": 10120.800000000001, "end": 10126.08, "text": " accelerators and things like this. And that standard zero cost exception thing would never", "tokens": [51320, 10172, 3391, 293, 721, 411, 341, 13, 400, 300, 3832, 4018, 2063, 11183, 551, 576, 1128, 51584], "temperature": 0.0, "avg_logprob": -0.06746961744569188, "compression_ratio": 1.804953560371517, "no_speech_prob": 0.0017005177214741707}, {"id": 2009, "seek": 1010168, "start": 10126.08, "end": 10130.800000000001, "text": " work on an accelerator. And so this is also part of how Mojo can scale all the way down to like", "tokens": [51584, 589, 322, 364, 39889, 13, 400, 370, 341, 307, 611, 644, 295, 577, 3335, 5134, 393, 4373, 439, 264, 636, 760, 281, 411, 51820], "temperature": 0.0, "avg_logprob": -0.06746961744569188, "compression_ratio": 1.804953560371517, "no_speech_prob": 0.0017005177214741707}, {"id": 2010, "seek": 1013080, "start": 10130.8, "end": 10134.08, "text": " little embedded systems and to running on GPUs and things like that.", "tokens": [50364, 707, 16741, 3652, 293, 281, 2614, 322, 18407, 82, 293, 721, 411, 300, 13, 50528], "temperature": 0.0, "avg_logprob": -0.1676539140589097, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.000755285145714879}, {"id": 2011, "seek": 1013080, "start": 10134.64, "end": 10141.92, "text": " Can you actually say about the maybe is there some high level way to describe the challenge of", "tokens": [50556, 1664, 291, 767, 584, 466, 264, 1310, 307, 456, 512, 1090, 1496, 636, 281, 6786, 264, 3430, 295, 50920], "temperature": 0.0, "avg_logprob": -0.1676539140589097, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.000755285145714879}, {"id": 2012, "seek": 1013080, "start": 10142.8, "end": 10149.119999999999, "text": " exceptions and how they work in code during compilation? So just this idea of percolating", "tokens": [50964, 22847, 293, 577, 436, 589, 294, 3089, 1830, 40261, 30, 407, 445, 341, 1558, 295, 680, 8768, 990, 51280], "temperature": 0.0, "avg_logprob": -0.1676539140589097, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.000755285145714879}, {"id": 2013, "seek": 1013080, "start": 10149.119999999999, "end": 10155.679999999998, "text": " up a thing, an error. Yeah. Yeah. So the way the way to think about it is think about a function", "tokens": [51280, 493, 257, 551, 11, 364, 6713, 13, 865, 13, 865, 13, 407, 264, 636, 264, 636, 281, 519, 466, 309, 307, 519, 466, 257, 2445, 51608], "temperature": 0.0, "avg_logprob": -0.1676539140589097, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.000755285145714879}, {"id": 2014, "seek": 1015568, "start": 10155.68, "end": 10162.0, "text": " that doesn't return anything. Just as a simple case, right? And so you have function one calls", "tokens": [50364, 300, 1177, 380, 2736, 1340, 13, 1449, 382, 257, 2199, 1389, 11, 558, 30, 400, 370, 291, 362, 2445, 472, 5498, 50680], "temperature": 0.0, "avg_logprob": -0.17095682262319378, "compression_ratio": 2.1402714932126696, "no_speech_prob": 0.18704929947853088}, {"id": 2015, "seek": 1015568, "start": 10162.0, "end": 10167.04, "text": " function two calls function three calls function four, along that call stack that are tri blocks.", "tokens": [50680, 2445, 732, 5498, 2445, 1045, 5498, 2445, 1451, 11, 2051, 300, 818, 8630, 300, 366, 1376, 8474, 13, 50932], "temperature": 0.0, "avg_logprob": -0.17095682262319378, "compression_ratio": 2.1402714932126696, "no_speech_prob": 0.18704929947853088}, {"id": 2016, "seek": 1015568, "start": 10167.76, "end": 10171.68, "text": " Right. And so if you have function one calls function two function two has a tri block,", "tokens": [50968, 1779, 13, 400, 370, 498, 291, 362, 2445, 472, 5498, 2445, 732, 2445, 732, 575, 257, 1376, 3461, 11, 51164], "temperature": 0.0, "avg_logprob": -0.17095682262319378, "compression_ratio": 2.1402714932126696, "no_speech_prob": 0.18704929947853088}, {"id": 2017, "seek": 1015568, "start": 10171.68, "end": 10176.16, "text": " and then within it, it calls function three, right? Well, what happens if function three throws?", "tokens": [51164, 293, 550, 1951, 309, 11, 309, 5498, 2445, 1045, 11, 558, 30, 1042, 11, 437, 2314, 498, 2445, 1045, 19251, 30, 51388], "temperature": 0.0, "avg_logprob": -0.17095682262319378, "compression_ratio": 2.1402714932126696, "no_speech_prob": 0.18704929947853088}, {"id": 2018, "seek": 1015568, "start": 10177.84, "end": 10181.76, "text": " Well, actually start simpler. What happens if it returns? Well, if it returns, it's supposed to", "tokens": [51472, 1042, 11, 767, 722, 18587, 13, 708, 2314, 498, 309, 11247, 30, 1042, 11, 498, 309, 11247, 11, 309, 311, 3442, 281, 51668], "temperature": 0.0, "avg_logprob": -0.17095682262319378, "compression_ratio": 2.1402714932126696, "no_speech_prob": 0.18704929947853088}, {"id": 2019, "seek": 1018176, "start": 10181.76, "end": 10185.6, "text": " go back out and continue executing and then fall off the bottom of the tri block and keep going", "tokens": [50364, 352, 646, 484, 293, 2354, 32368, 293, 550, 2100, 766, 264, 2767, 295, 264, 1376, 3461, 293, 1066, 516, 50556], "temperature": 0.0, "avg_logprob": -0.1179473841631854, "compression_ratio": 1.8125, "no_speech_prob": 0.0024723801761865616}, {"id": 2020, "seek": 1018176, "start": 10185.6, "end": 10190.48, "text": " and it all's good. If the function throws, you're supposed to exit the current function", "tokens": [50556, 293, 309, 439, 311, 665, 13, 759, 264, 2445, 19251, 11, 291, 434, 3442, 281, 11043, 264, 2190, 2445, 50800], "temperature": 0.0, "avg_logprob": -0.1179473841631854, "compression_ratio": 1.8125, "no_speech_prob": 0.0024723801761865616}, {"id": 2021, "seek": 1018176, "start": 10191.2, "end": 10195.76, "text": " and then get into the accept clause, right? And then do whatever codes there and then keep", "tokens": [50836, 293, 550, 483, 666, 264, 3241, 25925, 11, 558, 30, 400, 550, 360, 2035, 14211, 456, 293, 550, 1066, 51064], "temperature": 0.0, "avg_logprob": -0.1179473841631854, "compression_ratio": 1.8125, "no_speech_prob": 0.0024723801761865616}, {"id": 2022, "seek": 1018176, "start": 10195.76, "end": 10201.76, "text": " following on and going on. And so the way that a compiler like Mojo works is that the call to", "tokens": [51064, 3480, 322, 293, 516, 322, 13, 400, 370, 264, 636, 300, 257, 31958, 411, 3335, 5134, 1985, 307, 300, 264, 818, 281, 51364], "temperature": 0.0, "avg_logprob": -0.1179473841631854, "compression_ratio": 1.8125, "no_speech_prob": 0.0024723801761865616}, {"id": 2023, "seek": 1018176, "start": 10201.76, "end": 10206.72, "text": " that function, which happens in the accept block calls a function and then instead of returning", "tokens": [51364, 300, 2445, 11, 597, 2314, 294, 264, 3241, 3461, 5498, 257, 2445, 293, 550, 2602, 295, 12678, 51612], "temperature": 0.0, "avg_logprob": -0.1179473841631854, "compression_ratio": 1.8125, "no_speech_prob": 0.0024723801761865616}, {"id": 2024, "seek": 1020672, "start": 10206.72, "end": 10214.16, "text": " nothing, it actually returns, you know, a variant between nothing and an error. And so if you return", "tokens": [50364, 1825, 11, 309, 767, 11247, 11, 291, 458, 11, 257, 17501, 1296, 1825, 293, 364, 6713, 13, 400, 370, 498, 291, 2736, 50736], "temperature": 0.0, "avg_logprob": -0.12124710439521576, "compression_ratio": 1.8110599078341014, "no_speech_prob": 0.012430386617779732}, {"id": 2025, "seek": 1020672, "start": 10214.16, "end": 10220.08, "text": " normally off the bottom or do return, you return nothing. And if you throw through an error, you", "tokens": [50736, 5646, 766, 264, 2767, 420, 360, 2736, 11, 291, 2736, 1825, 13, 400, 498, 291, 3507, 807, 364, 6713, 11, 291, 51032], "temperature": 0.0, "avg_logprob": -0.12124710439521576, "compression_ratio": 1.8110599078341014, "no_speech_prob": 0.012430386617779732}, {"id": 2026, "seek": 1020672, "start": 10221.359999999999, "end": 10226.4, "text": " return the variant that is I'm an error, right? So when you get to the call, you say, okay, cool,", "tokens": [51096, 2736, 264, 17501, 300, 307, 286, 478, 364, 6713, 11, 558, 30, 407, 562, 291, 483, 281, 264, 818, 11, 291, 584, 11, 1392, 11, 1627, 11, 51348], "temperature": 0.0, "avg_logprob": -0.12124710439521576, "compression_ratio": 1.8110599078341014, "no_speech_prob": 0.012430386617779732}, {"id": 2027, "seek": 1020672, "start": 10226.4, "end": 10232.24, "text": " I called a function. Hey, I know locally I'm in a tri block. Right. And so I, I call the function", "tokens": [51348, 286, 1219, 257, 2445, 13, 1911, 11, 286, 458, 16143, 286, 478, 294, 257, 1376, 3461, 13, 1779, 13, 400, 370, 286, 11, 286, 818, 264, 2445, 51640], "temperature": 0.0, "avg_logprob": -0.12124710439521576, "compression_ratio": 1.8110599078341014, "no_speech_prob": 0.012430386617779732}, {"id": 2028, "seek": 1023224, "start": 10232.24, "end": 10236.56, "text": " and then I check to see what it returns. A half is that error thing jump to the accept block.", "tokens": [50364, 293, 550, 286, 1520, 281, 536, 437, 309, 11247, 13, 316, 1922, 307, 300, 6713, 551, 3012, 281, 264, 3241, 3461, 13, 50580], "temperature": 0.0, "avg_logprob": -0.14824163613199187, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.016399968415498734}, {"id": 2029, "seek": 1023224, "start": 10237.199999999999, "end": 10241.36, "text": " And that's all done for you behind the scenes. Exactly. And so the compiler does all this for", "tokens": [50612, 400, 300, 311, 439, 1096, 337, 291, 2261, 264, 8026, 13, 7587, 13, 400, 370, 264, 31958, 775, 439, 341, 337, 50820], "temperature": 0.0, "avg_logprob": -0.14824163613199187, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.016399968415498734}, {"id": 2030, "seek": 1023224, "start": 10241.36, "end": 10245.44, "text": " you. And I mean, one of the things if you dig into how this stuff works in Python,", "tokens": [50820, 291, 13, 400, 286, 914, 11, 472, 295, 264, 721, 498, 291, 2528, 666, 577, 341, 1507, 1985, 294, 15329, 11, 51024], "temperature": 0.0, "avg_logprob": -0.14824163613199187, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.016399968415498734}, {"id": 2031, "seek": 1023224, "start": 10245.44, "end": 10250.16, "text": " it gets a little bit more complicated because you have finally blocks, which now need, you need to", "tokens": [51024, 309, 2170, 257, 707, 857, 544, 6179, 570, 291, 362, 2721, 8474, 11, 597, 586, 643, 11, 291, 643, 281, 51260], "temperature": 0.0, "avg_logprob": -0.14824163613199187, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.016399968415498734}, {"id": 2032, "seek": 1023224, "start": 10250.16, "end": 10257.119999999999, "text": " go into do some stuff. And then those can also throw and return. Wait, what? Like the stuff matters", "tokens": [51260, 352, 666, 360, 512, 1507, 13, 400, 550, 729, 393, 611, 3507, 293, 2736, 13, 3802, 11, 437, 30, 1743, 264, 1507, 7001, 51608], "temperature": 0.0, "avg_logprob": -0.14824163613199187, "compression_ratio": 1.6870503597122302, "no_speech_prob": 0.016399968415498734}, {"id": 2033, "seek": 1025712, "start": 10257.2, "end": 10262.960000000001, "text": " compatibility. Like there's, there's nest them. There's with clauses. And so with clauses are", "tokens": [50368, 34237, 13, 1743, 456, 311, 11, 456, 311, 15646, 552, 13, 821, 311, 365, 49072, 13, 400, 370, 365, 49072, 366, 50656], "temperature": 0.0, "avg_logprob": -0.17181158065795898, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.1440906673669815}, {"id": 2034, "seek": 1025712, "start": 10262.960000000001, "end": 10267.04, "text": " kind of like finally blocks of some special stuff going on. And so there's nesting in general,", "tokens": [50656, 733, 295, 411, 2721, 8474, 295, 512, 2121, 1507, 516, 322, 13, 400, 370, 456, 311, 297, 8714, 294, 2674, 11, 50860], "temperature": 0.0, "avg_logprob": -0.17181158065795898, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.1440906673669815}, {"id": 2035, "seek": 1025712, "start": 10267.04, "end": 10274.08, "text": " nesting of anything nesting of functions should be illegal. It just feels like it adds a level", "tokens": [50860, 297, 8714, 295, 1340, 297, 8714, 295, 6828, 820, 312, 11905, 13, 467, 445, 3417, 411, 309, 10860, 257, 1496, 51212], "temperature": 0.0, "avg_logprob": -0.17181158065795898, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.1440906673669815}, {"id": 2036, "seek": 1025712, "start": 10274.08, "end": 10281.52, "text": " of complexity. I'm merely an implementer. And so this is again, one of the trade offs you get", "tokens": [51212, 295, 14024, 13, 286, 478, 17003, 364, 4445, 260, 13, 400, 370, 341, 307, 797, 11, 472, 295, 264, 4923, 39457, 291, 483, 51584], "temperature": 0.0, "avg_logprob": -0.17181158065795898, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.1440906673669815}, {"id": 2037, "seek": 1025712, "start": 10281.52, "end": 10286.400000000001, "text": " when you decide to build a super set is you get to implement a full fidelity implementation of the", "tokens": [51584, 562, 291, 4536, 281, 1322, 257, 1687, 992, 307, 291, 483, 281, 4445, 257, 1577, 46404, 11420, 295, 264, 51828], "temperature": 0.0, "avg_logprob": -0.17181158065795898, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.1440906673669815}, {"id": 2038, "seek": 1028640, "start": 10286.4, "end": 10294.0, "text": " thing that you decided is good. And so, yeah, I mean, we can, we can complain about the reality", "tokens": [50364, 551, 300, 291, 3047, 307, 665, 13, 400, 370, 11, 1338, 11, 286, 914, 11, 321, 393, 11, 321, 393, 11024, 466, 264, 4103, 50744], "temperature": 0.0, "avg_logprob": -0.15582176208496093, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0003301258257124573}, {"id": 2039, "seek": 1028640, "start": 10294.0, "end": 10299.199999999999, "text": " of the world and shake our fists, but it always feels like you shouldn't be a lot to do that,", "tokens": [50744, 295, 264, 1002, 293, 10283, 527, 49384, 11, 457, 309, 1009, 3417, 411, 291, 4659, 380, 312, 257, 688, 281, 360, 300, 11, 51004], "temperature": 0.0, "avg_logprob": -0.15582176208496093, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0003301258257124573}, {"id": 2040, "seek": 1028640, "start": 10299.199999999999, "end": 10305.359999999999, "text": " like to declare functions and sudden functions inside functions. Wait, wait, wait, what happened", "tokens": [51004, 411, 281, 19710, 6828, 293, 3990, 6828, 1854, 6828, 13, 3802, 11, 1699, 11, 1699, 11, 437, 2011, 51312], "temperature": 0.0, "avg_logprob": -0.15582176208496093, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0003301258257124573}, {"id": 2041, "seek": 1028640, "start": 10305.359999999999, "end": 10311.44, "text": " to Lex the Lisp guy? No, I understand that. But Lisp is what I used to do in college.", "tokens": [51312, 281, 24086, 264, 441, 7631, 2146, 30, 883, 11, 286, 1223, 300, 13, 583, 441, 7631, 307, 437, 286, 1143, 281, 360, 294, 3859, 13, 51616], "temperature": 0.0, "avg_logprob": -0.15582176208496093, "compression_ratio": 1.617391304347826, "no_speech_prob": 0.0003301258257124573}, {"id": 2042, "seek": 1031144, "start": 10312.4, "end": 10313.36, "text": " So now you've grown up.", "tokens": [50412, 407, 586, 291, 600, 7709, 493, 13, 50460], "temperature": 0.0, "avg_logprob": -0.24784918710695092, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.05032063275575638}, {"id": 2043, "seek": 1031144, "start": 10314.720000000001, "end": 10320.08, "text": " You know, we've all done things in college. We're not part of, no, I love Lisp. I love Lisp.", "tokens": [50528, 509, 458, 11, 321, 600, 439, 1096, 721, 294, 3859, 13, 492, 434, 406, 644, 295, 11, 572, 11, 286, 959, 441, 7631, 13, 286, 959, 441, 7631, 13, 50796], "temperature": 0.0, "avg_logprob": -0.24784918710695092, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.05032063275575638}, {"id": 2044, "seek": 1031144, "start": 10320.08, "end": 10323.2, "text": " Okay. Yeah, I was going to say, you're afraid of me. You're taking the whole internet.", "tokens": [50796, 1033, 13, 865, 11, 286, 390, 516, 281, 584, 11, 291, 434, 4638, 295, 385, 13, 509, 434, 1940, 264, 1379, 4705, 13, 50952], "temperature": 0.0, "avg_logprob": -0.24784918710695092, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.05032063275575638}, {"id": 2045, "seek": 1031144, "start": 10325.2, "end": 10330.640000000001, "text": " It's, it's, uh, it worked. It worked as a joke in my head. So nested functions are", "tokens": [51052, 467, 311, 11, 309, 311, 11, 2232, 11, 309, 2732, 13, 467, 2732, 382, 257, 7647, 294, 452, 1378, 13, 407, 15646, 292, 6828, 366, 51324], "temperature": 0.0, "avg_logprob": -0.24784918710695092, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.05032063275575638}, {"id": 2046, "seek": 1031144, "start": 10330.640000000001, "end": 10334.720000000001, "text": " joking aside, actually really great. And for certain things, right? And so these are also", "tokens": [51324, 17396, 7359, 11, 767, 534, 869, 13, 400, 337, 1629, 721, 11, 558, 30, 400, 370, 613, 366, 611, 51528], "temperature": 0.0, "avg_logprob": -0.24784918710695092, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.05032063275575638}, {"id": 2047, "seek": 1031144, "start": 10334.720000000001, "end": 10339.44, "text": " called closures. Closures are pretty cool. And you can pass callbacks. There's a lot of good", "tokens": [51528, 1219, 2611, 1303, 13, 2033, 329, 1303, 366, 1238, 1627, 13, 400, 291, 393, 1320, 818, 17758, 13, 821, 311, 257, 688, 295, 665, 51764], "temperature": 0.0, "avg_logprob": -0.24784918710695092, "compression_ratio": 1.6341463414634145, "no_speech_prob": 0.05032063275575638}, {"id": 2048, "seek": 1033944, "start": 10339.44, "end": 10346.32, "text": " patterns. And so, uh, so speaking of which, I don't think you have, uh, nested functions", "tokens": [50364, 8294, 13, 400, 370, 11, 2232, 11, 370, 4124, 295, 597, 11, 286, 500, 380, 519, 291, 362, 11, 2232, 11, 15646, 292, 6828, 50708], "temperature": 0.0, "avg_logprob": -0.16895025021561952, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.0017543848371133208}, {"id": 2049, "seek": 1033944, "start": 10346.32, "end": 10353.36, "text": " implemented yet in Mojo. Uh, we don't have Lambda syntax, but we do have, uh, there's a few things", "tokens": [50708, 12270, 1939, 294, 3335, 5134, 13, 4019, 11, 321, 500, 380, 362, 45691, 28431, 11, 457, 321, 360, 362, 11, 2232, 11, 456, 311, 257, 1326, 721, 51060], "temperature": 0.0, "avg_logprob": -0.16895025021561952, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.0017543848371133208}, {"id": 2050, "seek": 1033944, "start": 10353.36, "end": 10358.4, "text": " on the roadmap that you have that it'd be cool to sort of just fly through. Cause it's interesting", "tokens": [51060, 322, 264, 35738, 300, 291, 362, 300, 309, 1116, 312, 1627, 281, 1333, 295, 445, 3603, 807, 13, 10865, 309, 311, 1880, 51312], "temperature": 0.0, "avg_logprob": -0.16895025021561952, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.0017543848371133208}, {"id": 2051, "seek": 1033944, "start": 10358.4, "end": 10364.800000000001, "text": " to see, you know, how many features there are in a language, small and big, they have to implement.", "tokens": [51312, 281, 536, 11, 291, 458, 11, 577, 867, 4122, 456, 366, 294, 257, 2856, 11, 1359, 293, 955, 11, 436, 362, 281, 4445, 13, 51632], "temperature": 0.0, "avg_logprob": -0.16895025021561952, "compression_ratio": 1.5950413223140496, "no_speech_prob": 0.0017543848371133208}, {"id": 2052, "seek": 1036480, "start": 10364.8, "end": 10369.119999999999, "text": " Yeah. So first of all, there's tuple support and that has to do with some very specific", "tokens": [50364, 865, 13, 407, 700, 295, 439, 11, 456, 311, 2604, 781, 1406, 293, 300, 575, 281, 360, 365, 512, 588, 2685, 50580], "temperature": 0.0, "avg_logprob": -0.15994576613108316, "compression_ratio": 1.7287581699346406, "no_speech_prob": 0.043350499123334885}, {"id": 2053, "seek": 1036480, "start": 10369.119999999999, "end": 10373.439999999999, "text": " aspect of it. Like the parentheses are not parentheses that. Yeah. This is just a totally", "tokens": [50580, 4171, 295, 309, 13, 1743, 264, 34153, 366, 406, 34153, 300, 13, 865, 13, 639, 307, 445, 257, 3879, 50796], "temperature": 0.0, "avg_logprob": -0.15994576613108316, "compression_ratio": 1.7287581699346406, "no_speech_prob": 0.043350499123334885}, {"id": 2054, "seek": 1036480, "start": 10373.439999999999, "end": 10377.759999999998, "text": " a syntactic thing. A syntactic thing. Okay. There's, but it's cool. It's still, uh,", "tokens": [50796, 257, 23980, 19892, 551, 13, 316, 23980, 19892, 551, 13, 1033, 13, 821, 311, 11, 457, 309, 311, 1627, 13, 467, 311, 920, 11, 2232, 11, 51012], "temperature": 0.0, "avg_logprob": -0.15994576613108316, "compression_ratio": 1.7287581699346406, "no_speech_prob": 0.043350499123334885}, {"id": 2055, "seek": 1036480, "start": 10379.039999999999, "end": 10383.119999999999, "text": " so keyword arguments and functions. Yeah. So this is where in Python, you can say", "tokens": [51076, 370, 20428, 12869, 293, 6828, 13, 865, 13, 407, 341, 307, 689, 294, 15329, 11, 291, 393, 584, 51280], "temperature": 0.0, "avg_logprob": -0.15994576613108316, "compression_ratio": 1.7287581699346406, "no_speech_prob": 0.043350499123334885}, {"id": 2056, "seek": 1036480, "start": 10383.679999999998, "end": 10388.56, "text": " call a function x equals four. Yeah. And x is the name of the argument. That's a nice sort of", "tokens": [51308, 818, 257, 2445, 2031, 6915, 1451, 13, 865, 13, 400, 2031, 307, 264, 1315, 295, 264, 6770, 13, 663, 311, 257, 1481, 1333, 295, 51552], "temperature": 0.0, "avg_logprob": -0.15994576613108316, "compression_ratio": 1.7287581699346406, "no_speech_prob": 0.043350499123334885}, {"id": 2057, "seek": 1036480, "start": 10388.56, "end": 10393.039999999999, "text": " documenting self-documenting feature. Yeah. I mean, and again, this isn't rocket science to", "tokens": [51552, 42360, 2698, 12, 67, 30439, 278, 4111, 13, 865, 13, 286, 914, 11, 293, 797, 11, 341, 1943, 380, 13012, 3497, 281, 51776], "temperature": 0.0, "avg_logprob": -0.15994576613108316, "compression_ratio": 1.7287581699346406, "no_speech_prob": 0.043350499123334885}, {"id": 2058, "seek": 1039304, "start": 10393.04, "end": 10398.0, "text": " implement. That's just the laundry. It's just on the list. Uh, the bigger features are things", "tokens": [50364, 4445, 13, 663, 311, 445, 264, 19811, 13, 467, 311, 445, 322, 264, 1329, 13, 4019, 11, 264, 3801, 4122, 366, 721, 50612], "temperature": 0.0, "avg_logprob": -0.14112934144605108, "compression_ratio": 1.8125, "no_speech_prob": 0.0044651939533650875}, {"id": 2059, "seek": 1039304, "start": 10398.0, "end": 10404.640000000001, "text": " like traits. So traits are when you want to define abstract. So when you get into typed languages,", "tokens": [50612, 411, 19526, 13, 407, 19526, 366, 562, 291, 528, 281, 6964, 12649, 13, 407, 562, 291, 483, 666, 33941, 8650, 11, 50944], "temperature": 0.0, "avg_logprob": -0.14112934144605108, "compression_ratio": 1.8125, "no_speech_prob": 0.0044651939533650875}, {"id": 2060, "seek": 1039304, "start": 10405.2, "end": 10409.36, "text": " you need the ability to write generics. And so you want to say, I want to write this function.", "tokens": [50972, 291, 643, 264, 3485, 281, 2464, 1337, 1167, 13, 400, 370, 291, 528, 281, 584, 11, 286, 528, 281, 2464, 341, 2445, 13, 51180], "temperature": 0.0, "avg_logprob": -0.14112934144605108, "compression_ratio": 1.8125, "no_speech_prob": 0.0044651939533650875}, {"id": 2061, "seek": 1039304, "start": 10409.36, "end": 10414.240000000002, "text": " And now I want to work on all things that are arithmetic like. Well, what does arithmetic", "tokens": [51180, 400, 586, 286, 528, 281, 589, 322, 439, 721, 300, 366, 42973, 411, 13, 1042, 11, 437, 775, 42973, 51424], "temperature": 0.0, "avg_logprob": -0.14112934144605108, "compression_ratio": 1.8125, "no_speech_prob": 0.0044651939533650875}, {"id": 2062, "seek": 1039304, "start": 10414.240000000002, "end": 10419.92, "text": " like mean? Well, arithmetic like is a categorization of a bunch of types. And so it's,", "tokens": [51424, 411, 914, 30, 1042, 11, 42973, 411, 307, 257, 19250, 2144, 295, 257, 3840, 295, 3467, 13, 400, 370, 309, 311, 11, 51708], "temperature": 0.0, "avg_logprob": -0.14112934144605108, "compression_ratio": 1.8125, "no_speech_prob": 0.0044651939533650875}, {"id": 2063, "seek": 1041992, "start": 10420.48, "end": 10423.6, "text": " again, you can define many different ways and I'm not going to go into ring theory or something.", "tokens": [50392, 797, 11, 291, 393, 6964, 867, 819, 2098, 293, 286, 478, 406, 516, 281, 352, 666, 4875, 5261, 420, 746, 13, 50548], "temperature": 0.0, "avg_logprob": -0.142740507771198, "compression_ratio": 1.8372881355932202, "no_speech_prob": 0.001987588359043002}, {"id": 2064, "seek": 1041992, "start": 10424.64, "end": 10428.16, "text": " But the, uh, you know, you can say it's arithmetic like if you can add the track", "tokens": [50600, 583, 264, 11, 2232, 11, 291, 458, 11, 291, 393, 584, 309, 311, 42973, 411, 498, 291, 393, 909, 264, 2837, 50776], "temperature": 0.0, "avg_logprob": -0.142740507771198, "compression_ratio": 1.8372881355932202, "no_speech_prob": 0.001987588359043002}, {"id": 2065, "seek": 1041992, "start": 10428.16, "end": 10431.6, "text": " multiply, divide it, for example. Right. And so what you're saying is you're saying", "tokens": [50776, 12972, 11, 9845, 309, 11, 337, 1365, 13, 1779, 13, 400, 370, 437, 291, 434, 1566, 307, 291, 434, 1566, 50948], "temperature": 0.0, "avg_logprob": -0.142740507771198, "compression_ratio": 1.8372881355932202, "no_speech_prob": 0.001987588359043002}, {"id": 2066, "seek": 1041992, "start": 10432.24, "end": 10439.28, "text": " there's a set of traits that apply to a broad variety of types. And so there, all these types", "tokens": [50980, 456, 311, 257, 992, 295, 19526, 300, 3079, 281, 257, 4152, 5673, 295, 3467, 13, 400, 370, 456, 11, 439, 613, 3467, 51332], "temperature": 0.0, "avg_logprob": -0.142740507771198, "compression_ratio": 1.8372881355932202, "no_speech_prob": 0.001987588359043002}, {"id": 2067, "seek": 1041992, "start": 10439.28, "end": 10443.84, "text": " are arithmetic like all these tensors and floating point integer. And like there's this category", "tokens": [51332, 366, 42973, 411, 439, 613, 10688, 830, 293, 12607, 935, 24922, 13, 400, 411, 456, 311, 341, 7719, 51560], "temperature": 0.0, "avg_logprob": -0.142740507771198, "compression_ratio": 1.8372881355932202, "no_speech_prob": 0.001987588359043002}, {"id": 2068, "seek": 1041992, "start": 10443.84, "end": 10449.52, "text": " of types. And then I can define on an orthogonal access algorithms that then work against", "tokens": [51560, 295, 3467, 13, 400, 550, 286, 393, 6964, 322, 364, 41488, 2105, 14642, 300, 550, 589, 1970, 51844], "temperature": 0.0, "avg_logprob": -0.142740507771198, "compression_ratio": 1.8372881355932202, "no_speech_prob": 0.001987588359043002}, {"id": 2069, "seek": 1044952, "start": 10449.52, "end": 10455.92, "text": " types that have those properties. And so this is a, again, it's a widely known thing. It's been", "tokens": [50364, 3467, 300, 362, 729, 7221, 13, 400, 370, 341, 307, 257, 11, 797, 11, 309, 311, 257, 13371, 2570, 551, 13, 467, 311, 668, 50684], "temperature": 0.0, "avg_logprob": -0.1436909580230713, "compression_ratio": 1.5850622406639003, "no_speech_prob": 0.0007671645726077259}, {"id": 2070, "seek": 1044952, "start": 10455.92, "end": 10462.48, "text": " implemented in Swift and Rust and many languages. So it's not Haskell, which is where everybody", "tokens": [50684, 12270, 294, 25539, 293, 34952, 293, 867, 8650, 13, 407, 309, 311, 406, 8646, 43723, 11, 597, 307, 689, 2201, 51012], "temperature": 0.0, "avg_logprob": -0.1436909580230713, "compression_ratio": 1.5850622406639003, "no_speech_prob": 0.0007671645726077259}, {"id": 2071, "seek": 1044952, "start": 10462.48, "end": 10468.48, "text": " learns, learns their tricks from. But the, but we need to implement that and that'll enable a new", "tokens": [51012, 27152, 11, 27152, 641, 11733, 490, 13, 583, 264, 11, 457, 321, 643, 281, 4445, 300, 293, 300, 603, 9528, 257, 777, 51312], "temperature": 0.0, "avg_logprob": -0.1436909580230713, "compression_ratio": 1.5850622406639003, "no_speech_prob": 0.0007671645726077259}, {"id": 2072, "seek": 1044952, "start": 10468.48, "end": 10475.2, "text": " level of expressivity. So classes. Yeah, classes are a big deal. It's a big deal still to be", "tokens": [51312, 1496, 295, 5109, 4253, 13, 407, 5359, 13, 865, 11, 5359, 366, 257, 955, 2028, 13, 467, 311, 257, 955, 2028, 920, 281, 312, 51648], "temperature": 0.0, "avg_logprob": -0.1436909580230713, "compression_ratio": 1.5850622406639003, "no_speech_prob": 0.0007671645726077259}, {"id": 2073, "seek": 1047520, "start": 10475.2, "end": 10482.320000000002, "text": " implemented. Um, like you said, a Lambda syntax, and there's like detail stuff like whole module", "tokens": [50364, 12270, 13, 3301, 11, 411, 291, 848, 11, 257, 45691, 28431, 11, 293, 456, 311, 411, 2607, 1507, 411, 1379, 10088, 50720], "temperature": 0.0, "avg_logprob": -0.18623309888337788, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.025174535810947418}, {"id": 2074, "seek": 1047520, "start": 10482.320000000002, "end": 10489.92, "text": " import, um, support for top level code at file scope. So, and then global variables also.", "tokens": [50720, 974, 11, 1105, 11, 1406, 337, 1192, 1496, 3089, 412, 3991, 11923, 13, 407, 11, 293, 550, 4338, 9102, 611, 13, 51100], "temperature": 0.0, "avg_logprob": -0.18623309888337788, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.025174535810947418}, {"id": 2075, "seek": 1047520, "start": 10491.6, "end": 10495.6, "text": " So being able to have variables outside of a top level. Well, and so this comes back to the", "tokens": [51184, 407, 885, 1075, 281, 362, 9102, 2380, 295, 257, 1192, 1496, 13, 1042, 11, 293, 370, 341, 1487, 646, 281, 264, 51384], "temperature": 0.0, "avg_logprob": -0.18623309888337788, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.025174535810947418}, {"id": 2076, "seek": 1047520, "start": 10495.6, "end": 10501.28, "text": " where module came from and the fact that this is your point one, right? And so we're building,", "tokens": [51384, 689, 10088, 1361, 490, 293, 264, 1186, 300, 341, 307, 428, 935, 472, 11, 558, 30, 400, 370, 321, 434, 2390, 11, 51668], "temperature": 0.0, "avg_logprob": -0.18623309888337788, "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.025174535810947418}, {"id": 2077, "seek": 1050128, "start": 10501.28, "end": 10506.0, "text": " so modular is building an AI stack, right? And an AI stack has a bunch of problems working with", "tokens": [50364, 370, 31111, 307, 2390, 364, 7318, 8630, 11, 558, 30, 400, 364, 7318, 8630, 575, 257, 3840, 295, 2740, 1364, 365, 50600], "temperature": 0.0, "avg_logprob": -0.12008299343827841, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.03731124848127365}, {"id": 2078, "seek": 1050128, "start": 10506.0, "end": 10510.560000000001, "text": " hardware and writing high performance kernels and doing this kernel fusion thing I was talking about", "tokens": [50600, 8837, 293, 3579, 1090, 3389, 23434, 1625, 293, 884, 341, 28256, 23100, 551, 286, 390, 1417, 466, 50828], "temperature": 0.0, "avg_logprob": -0.12008299343827841, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.03731124848127365}, {"id": 2079, "seek": 1050128, "start": 10510.560000000001, "end": 10515.68, "text": " and getting the most out of the hardware. And so we've really prioritized and built Mojo to solve", "tokens": [50828, 293, 1242, 264, 881, 484, 295, 264, 8837, 13, 400, 370, 321, 600, 534, 14846, 1602, 293, 3094, 3335, 5134, 281, 5039, 51084], "temperature": 0.0, "avg_logprob": -0.12008299343827841, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.03731124848127365}, {"id": 2080, "seek": 1050128, "start": 10515.68, "end": 10521.76, "text": " modules problem, right? Now our North Star is build out and support all the things. And so we're", "tokens": [51084, 16679, 1154, 11, 558, 30, 823, 527, 4067, 5705, 307, 1322, 484, 293, 1406, 439, 264, 721, 13, 400, 370, 321, 434, 51388], "temperature": 0.0, "avg_logprob": -0.12008299343827841, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.03731124848127365}, {"id": 2081, "seek": 1050128, "start": 10521.76, "end": 10526.880000000001, "text": " making incredible progress. By the way, Mojo is only like seven months old. So that's another", "tokens": [51388, 1455, 4651, 4205, 13, 3146, 264, 636, 11, 3335, 5134, 307, 787, 411, 3407, 2493, 1331, 13, 407, 300, 311, 1071, 51644], "temperature": 0.0, "avg_logprob": -0.12008299343827841, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.03731124848127365}, {"id": 2082, "seek": 1050128, "start": 10526.880000000001, "end": 10530.480000000001, "text": " interesting thing. I mean, part of the reason I wanted to mention some of these things is like,", "tokens": [51644, 1880, 551, 13, 286, 914, 11, 644, 295, 264, 1778, 286, 1415, 281, 2152, 512, 295, 613, 721, 307, 411, 11, 51824], "temperature": 0.0, "avg_logprob": -0.12008299343827841, "compression_ratio": 1.7932098765432098, "no_speech_prob": 0.03731124848127365}, {"id": 2083, "seek": 1053048, "start": 10530.56, "end": 10537.119999999999, "text": " there's a lot to do and it's pretty cool how you just kind of, sometimes you take for granted how", "tokens": [50368, 456, 311, 257, 688, 281, 360, 293, 309, 311, 1238, 1627, 577, 291, 445, 733, 295, 11, 2171, 291, 747, 337, 12344, 577, 50696], "temperature": 0.0, "avg_logprob": -0.1296804181991085, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.0014098340179771185}, {"id": 2084, "seek": 1053048, "start": 10537.119999999999, "end": 10540.64, "text": " much there is in a programming language, how many cool features you kind of rely on. And this is", "tokens": [50696, 709, 456, 307, 294, 257, 9410, 2856, 11, 577, 867, 1627, 4122, 291, 733, 295, 10687, 322, 13, 400, 341, 307, 50872], "temperature": 0.0, "avg_logprob": -0.1296804181991085, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.0014098340179771185}, {"id": 2085, "seek": 1053048, "start": 10540.64, "end": 10546.32, "text": " kind of a nice reminder when you lay it as a to do list. Yeah. And so I mean, but also you look into,", "tokens": [50872, 733, 295, 257, 1481, 13548, 562, 291, 2360, 309, 382, 257, 281, 360, 1329, 13, 865, 13, 400, 370, 286, 914, 11, 457, 611, 291, 574, 666, 11, 51156], "temperature": 0.0, "avg_logprob": -0.1296804181991085, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.0014098340179771185}, {"id": 2086, "seek": 1053048, "start": 10547.039999999999, "end": 10554.08, "text": " it's amazing how much is also there. And you take it for granted that a value, if you define it,", "tokens": [51192, 309, 311, 2243, 577, 709, 307, 611, 456, 13, 400, 291, 747, 309, 337, 12344, 300, 257, 2158, 11, 498, 291, 6964, 309, 11, 51544], "temperature": 0.0, "avg_logprob": -0.1296804181991085, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.0014098340179771185}, {"id": 2087, "seek": 1053048, "start": 10554.08, "end": 10559.68, "text": " it will get destroyed automatically. Like that little feature itself is actually really complicated,", "tokens": [51544, 309, 486, 483, 8937, 6772, 13, 1743, 300, 707, 4111, 2564, 307, 767, 534, 6179, 11, 51824], "temperature": 0.0, "avg_logprob": -0.1296804181991085, "compression_ratio": 1.7706093189964158, "no_speech_prob": 0.0014098340179771185}, {"id": 2088, "seek": 1055968, "start": 10559.76, "end": 10564.48, "text": " given the way the ownership system has to work. And the way that works within Mojo is a huge step", "tokens": [50368, 2212, 264, 636, 264, 15279, 1185, 575, 281, 589, 13, 400, 264, 636, 300, 1985, 1951, 3335, 5134, 307, 257, 2603, 1823, 50604], "temperature": 0.0, "avg_logprob": -0.18711014091968536, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0013668029569089413}, {"id": 2089, "seek": 1055968, "start": 10564.48, "end": 10568.720000000001, "text": " forward from what Rust and Swift have done. But can you say that again, when a value, when you", "tokens": [50604, 2128, 490, 437, 34952, 293, 25539, 362, 1096, 13, 583, 393, 291, 584, 300, 797, 11, 562, 257, 2158, 11, 562, 291, 50816], "temperature": 0.0, "avg_logprob": -0.18711014091968536, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0013668029569089413}, {"id": 2090, "seek": 1055968, "start": 10568.720000000001, "end": 10572.4, "text": " define it gets destroyed on the map? Yeah. So like say you have a string, right? So you just find a", "tokens": [50816, 6964, 309, 2170, 8937, 322, 264, 4471, 30, 865, 13, 407, 411, 584, 291, 362, 257, 6798, 11, 558, 30, 407, 291, 445, 915, 257, 51000], "temperature": 0.0, "avg_logprob": -0.18711014091968536, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0013668029569089413}, {"id": 2091, "seek": 1055968, "start": 10572.4, "end": 10578.16, "text": " string on the stack or whatever that means, like in your local function, right? And so you say,", "tokens": [51000, 6798, 322, 264, 8630, 420, 2035, 300, 1355, 11, 411, 294, 428, 2654, 2445, 11, 558, 30, 400, 370, 291, 584, 11, 51288], "temperature": 0.0, "avg_logprob": -0.18711014091968536, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0013668029569089413}, {"id": 2092, "seek": 1055968, "start": 10579.04, "end": 10584.64, "text": " like, whether it be in a def, and so you just say x equals hello world, right? Well, if your", "tokens": [51332, 411, 11, 1968, 309, 312, 294, 257, 1060, 11, 293, 370, 291, 445, 584, 2031, 6915, 7751, 1002, 11, 558, 30, 1042, 11, 498, 428, 51612], "temperature": 0.0, "avg_logprob": -0.18711014091968536, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0013668029569089413}, {"id": 2093, "seek": 1058464, "start": 10584.64, "end": 10590.48, "text": " string type requires you to allocate memory, then when it's destroyed, you have to deallocate it.", "tokens": [50364, 6798, 2010, 7029, 291, 281, 35713, 4675, 11, 550, 562, 309, 311, 8937, 11, 291, 362, 281, 368, 336, 42869, 309, 13, 50656], "temperature": 0.0, "avg_logprob": -0.10611244906549869, "compression_ratio": 1.584070796460177, "no_speech_prob": 0.0004442032950464636}, {"id": 2094, "seek": 1058464, "start": 10590.48, "end": 10595.599999999999, "text": " So in Python and Mojo, you define that with the Dell method, right? Where does that get run?", "tokens": [50656, 407, 294, 15329, 293, 3335, 5134, 11, 291, 6964, 300, 365, 264, 33319, 3170, 11, 558, 30, 2305, 775, 300, 483, 1190, 30, 50912], "temperature": 0.0, "avg_logprob": -0.10611244906549869, "compression_ratio": 1.584070796460177, "no_speech_prob": 0.0004442032950464636}, {"id": 2095, "seek": 1058464, "start": 10598.8, "end": 10606.16, "text": " Well, it gets run sometime between the last use of the value and the end of the program.", "tokens": [51072, 1042, 11, 309, 2170, 1190, 15053, 1296, 264, 1036, 764, 295, 264, 2158, 293, 264, 917, 295, 264, 1461, 13, 51440], "temperature": 0.0, "avg_logprob": -0.10611244906549869, "compression_ratio": 1.584070796460177, "no_speech_prob": 0.0004442032950464636}, {"id": 2096, "seek": 1058464, "start": 10607.119999999999, "end": 10610.96, "text": " Like in this, you now get into garbage collection, you get into like all these", "tokens": [51488, 1743, 294, 341, 11, 291, 586, 483, 666, 14150, 5765, 11, 291, 483, 666, 411, 439, 613, 51680], "temperature": 0.0, "avg_logprob": -0.10611244906549869, "compression_ratio": 1.584070796460177, "no_speech_prob": 0.0004442032950464636}, {"id": 2097, "seek": 1061096, "start": 10611.039999999999, "end": 10617.119999999999, "text": " long debated, you talk about religions and tradeoffs and things like this. This is a hugely", "tokens": [50368, 938, 42212, 11, 291, 751, 466, 21212, 293, 4923, 19231, 293, 721, 411, 341, 13, 639, 307, 257, 27417, 50672], "temperature": 0.0, "avg_logprob": -0.11384001786146707, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0021153634879738092}, {"id": 2098, "seek": 1061096, "start": 10617.119999999999, "end": 10624.0, "text": " hotly contested world. If you look at C++, the way this works is that if you define a variable,", "tokens": [50672, 2368, 356, 10287, 292, 1002, 13, 759, 291, 574, 412, 383, 25472, 11, 264, 636, 341, 1985, 307, 300, 498, 291, 6964, 257, 7006, 11, 51016], "temperature": 0.0, "avg_logprob": -0.11384001786146707, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0021153634879738092}, {"id": 2099, "seek": 1061096, "start": 10624.0, "end": 10630.64, "text": " or a set of variables within a function, they get destroyed in a last in first out order.", "tokens": [51016, 420, 257, 992, 295, 9102, 1951, 257, 2445, 11, 436, 483, 8937, 294, 257, 1036, 294, 700, 484, 1668, 13, 51348], "temperature": 0.0, "avg_logprob": -0.11384001786146707, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0021153634879738092}, {"id": 2100, "seek": 1061096, "start": 10630.64, "end": 10636.64, "text": " So it's like nesting. This has a huge problem because if you define, you have a big scope,", "tokens": [51348, 407, 309, 311, 411, 297, 8714, 13, 639, 575, 257, 2603, 1154, 570, 498, 291, 6964, 11, 291, 362, 257, 955, 11923, 11, 51648], "temperature": 0.0, "avg_logprob": -0.11384001786146707, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0021153634879738092}, {"id": 2101, "seek": 1061096, "start": 10636.64, "end": 10640.64, "text": " and you define a whole bunch of values at the top, and then you use them, and then you do a whole", "tokens": [51648, 293, 291, 6964, 257, 1379, 3840, 295, 4190, 412, 264, 1192, 11, 293, 550, 291, 764, 552, 11, 293, 550, 291, 360, 257, 1379, 51848], "temperature": 0.0, "avg_logprob": -0.11384001786146707, "compression_ratio": 1.78544061302682, "no_speech_prob": 0.0021153634879738092}, {"id": 2102, "seek": 1064064, "start": 10640.64, "end": 10644.64, "text": " bunch of code that doesn't use them, they don't get destroyed until the very end of that scope.", "tokens": [50364, 3840, 295, 3089, 300, 1177, 380, 764, 552, 11, 436, 500, 380, 483, 8937, 1826, 264, 588, 917, 295, 300, 11923, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10012313297816686, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.001064673182554543}, {"id": 2103, "seek": 1064064, "start": 10645.68, "end": 10650.88, "text": " And so this also destroys tail calls, so good functional programming, right? This has a bunch", "tokens": [50616, 400, 370, 341, 611, 36714, 6838, 5498, 11, 370, 665, 11745, 9410, 11, 558, 30, 639, 575, 257, 3840, 50876], "temperature": 0.0, "avg_logprob": -0.10012313297816686, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.001064673182554543}, {"id": 2104, "seek": 1064064, "start": 10650.88, "end": 10655.599999999999, "text": " of different impacts on, you talk about reference counting optimizations and things like this,", "tokens": [50876, 295, 819, 11606, 322, 11, 291, 751, 466, 6408, 13251, 5028, 14455, 293, 721, 411, 341, 11, 51112], "temperature": 0.0, "avg_logprob": -0.10012313297816686, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.001064673182554543}, {"id": 2105, "seek": 1064064, "start": 10655.599999999999, "end": 10660.88, "text": " a bunch of very low level things. And so what Mojo does is it has a different approach on that", "tokens": [51112, 257, 3840, 295, 588, 2295, 1496, 721, 13, 400, 370, 437, 3335, 5134, 775, 307, 309, 575, 257, 819, 3109, 322, 300, 51376], "temperature": 0.0, "avg_logprob": -0.10012313297816686, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.001064673182554543}, {"id": 2106, "seek": 1064064, "start": 10660.88, "end": 10666.72, "text": " from any language I'm familiar with, where it destroys them as soon as possible. And by doing", "tokens": [51376, 490, 604, 2856, 286, 478, 4963, 365, 11, 689, 309, 36714, 552, 382, 2321, 382, 1944, 13, 400, 538, 884, 51668], "temperature": 0.0, "avg_logprob": -0.10012313297816686, "compression_ratio": 1.6892857142857143, "no_speech_prob": 0.001064673182554543}, {"id": 2107, "seek": 1066672, "start": 10666.72, "end": 10671.119999999999, "text": " that, you get better memory use, you get better predictability, you get tail calls that work,", "tokens": [50364, 300, 11, 291, 483, 1101, 4675, 764, 11, 291, 483, 1101, 6069, 2310, 11, 291, 483, 6838, 5498, 300, 589, 11, 50584], "temperature": 0.0, "avg_logprob": -0.07875214223786602, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.019713839516043663}, {"id": 2108, "seek": 1066672, "start": 10671.119999999999, "end": 10674.72, "text": " like you get a bunch of other things, you get better ownership tracking, there's a bunch of", "tokens": [50584, 411, 291, 483, 257, 3840, 295, 661, 721, 11, 291, 483, 1101, 15279, 11603, 11, 456, 311, 257, 3840, 295, 50764], "temperature": 0.0, "avg_logprob": -0.07875214223786602, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.019713839516043663}, {"id": 2109, "seek": 1066672, "start": 10674.72, "end": 10681.279999999999, "text": " these very simple things that are very fundamental, that are already built in there in Mojo today,", "tokens": [50764, 613, 588, 2199, 721, 300, 366, 588, 8088, 11, 300, 366, 1217, 3094, 294, 456, 294, 3335, 5134, 965, 11, 51092], "temperature": 0.0, "avg_logprob": -0.07875214223786602, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.019713839516043663}, {"id": 2110, "seek": 1066672, "start": 10681.279999999999, "end": 10685.199999999999, "text": " that are the things that nobody talks about generally, but when they don't work right,", "tokens": [51092, 300, 366, 264, 721, 300, 5079, 6686, 466, 5101, 11, 457, 562, 436, 500, 380, 589, 558, 11, 51288], "temperature": 0.0, "avg_logprob": -0.07875214223786602, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.019713839516043663}, {"id": 2111, "seek": 1066672, "start": 10685.199999999999, "end": 10688.96, "text": " you find out and you have to complain about. Is it trivial to know", "tokens": [51288, 291, 915, 484, 293, 291, 362, 281, 11024, 466, 13, 1119, 309, 26703, 281, 458, 51476], "temperature": 0.0, "avg_logprob": -0.07875214223786602, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.019713839516043663}, {"id": 2112, "seek": 1066672, "start": 10690.64, "end": 10693.92, "text": " what's the soonest possible to delete a thing that's not going to be used again?", "tokens": [51560, 437, 311, 264, 2321, 377, 1944, 281, 12097, 257, 551, 300, 311, 406, 516, 281, 312, 1143, 797, 30, 51724], "temperature": 0.0, "avg_logprob": -0.07875214223786602, "compression_ratio": 1.8535714285714286, "no_speech_prob": 0.019713839516043663}, {"id": 2113, "seek": 1069392, "start": 10693.92, "end": 10698.4, "text": " Yeah, well, I mean, it's generally trivial, it's after the last use of it. So if you just find x", "tokens": [50364, 865, 11, 731, 11, 286, 914, 11, 309, 311, 5101, 26703, 11, 309, 311, 934, 264, 1036, 764, 295, 309, 13, 407, 498, 291, 445, 915, 2031, 50588], "temperature": 0.0, "avg_logprob": -0.13966071141230596, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.001500830752775073}, {"id": 2114, "seek": 1069392, "start": 10698.4, "end": 10701.76, "text": " as a string, and then you have some use of x somewhere in your code.", "tokens": [50588, 382, 257, 6798, 11, 293, 550, 291, 362, 512, 764, 295, 2031, 4079, 294, 428, 3089, 13, 50756], "temperature": 0.0, "avg_logprob": -0.13966071141230596, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.001500830752775073}, {"id": 2115, "seek": 1069392, "start": 10701.76, "end": 10705.12, "text": " Within that scope? You mean within the scope that is accessible?", "tokens": [50756, 15996, 300, 11923, 30, 509, 914, 1951, 264, 11923, 300, 307, 9515, 30, 50924], "temperature": 0.0, "avg_logprob": -0.13966071141230596, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.001500830752775073}, {"id": 2116, "seek": 1069392, "start": 10705.12, "end": 10709.12, "text": " It's, yeah, exactly. So you can only use something within its scope. And so then", "tokens": [50924, 467, 311, 11, 1338, 11, 2293, 13, 407, 291, 393, 787, 764, 746, 1951, 1080, 11923, 13, 400, 370, 550, 51124], "temperature": 0.0, "avg_logprob": -0.13966071141230596, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.001500830752775073}, {"id": 2117, "seek": 1069392, "start": 10709.12, "end": 10714.16, "text": " it doesn't wait until the end of the scope to delete it. It destroys it after the last use.", "tokens": [51124, 309, 1177, 380, 1699, 1826, 264, 917, 295, 264, 11923, 281, 12097, 309, 13, 467, 36714, 309, 934, 264, 1036, 764, 13, 51376], "temperature": 0.0, "avg_logprob": -0.13966071141230596, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.001500830752775073}, {"id": 2118, "seek": 1069392, "start": 10714.16, "end": 10718.24, "text": " So there's kind of some very ego machine that's just sitting there and deleting.", "tokens": [51376, 407, 456, 311, 733, 295, 512, 588, 14495, 3479, 300, 311, 445, 3798, 456, 293, 48946, 13, 51580], "temperature": 0.0, "avg_logprob": -0.13966071141230596, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.001500830752775073}, {"id": 2119, "seek": 1069392, "start": 10718.24, "end": 10721.12, "text": " Yeah, and it's all in the compiler, so it's not at runtime, which is also cool.", "tokens": [51580, 865, 11, 293, 309, 311, 439, 294, 264, 31958, 11, 370, 309, 311, 406, 412, 34474, 11, 597, 307, 611, 1627, 13, 51724], "temperature": 0.0, "avg_logprob": -0.13966071141230596, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.001500830752775073}, {"id": 2120, "seek": 1072112, "start": 10721.84, "end": 10727.44, "text": " And so, yeah, and so what, and this is actually non-trivial because you have control flow.", "tokens": [50400, 400, 370, 11, 1338, 11, 293, 370, 437, 11, 293, 341, 307, 767, 2107, 12, 83, 470, 22640, 570, 291, 362, 1969, 3095, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1843319068083892, "compression_ratio": 1.8184713375796178, "no_speech_prob": 0.006900906562805176}, {"id": 2121, "seek": 1072112, "start": 10728.480000000001, "end": 10731.44, "text": " And so it gets complicated pretty quickly. And so like getting this right was not,", "tokens": [50732, 400, 370, 309, 2170, 6179, 1238, 2661, 13, 400, 370, 411, 1242, 341, 558, 390, 406, 11, 50880], "temperature": 0.0, "avg_logprob": -0.1843319068083892, "compression_ratio": 1.8184713375796178, "no_speech_prob": 0.006900906562805176}, {"id": 2122, "seek": 1072112, "start": 10731.44, "end": 10734.240000000002, "text": " Oh, so you have to insert delete like in a lot of places?", "tokens": [50880, 876, 11, 370, 291, 362, 281, 8969, 12097, 411, 294, 257, 688, 295, 3190, 30, 51020], "temperature": 0.0, "avg_logprob": -0.1843319068083892, "compression_ratio": 1.8184713375796178, "no_speech_prob": 0.006900906562805176}, {"id": 2123, "seek": 1072112, "start": 10734.240000000002, "end": 10738.240000000002, "text": " Potentially, yeah, exactly. So the compiler has to reason about this. And this is where,", "tokens": [51020, 9145, 3137, 11, 1338, 11, 2293, 13, 407, 264, 31958, 575, 281, 1778, 466, 341, 13, 400, 341, 307, 689, 11, 51220], "temperature": 0.0, "avg_logprob": -0.1843319068083892, "compression_ratio": 1.8184713375796178, "no_speech_prob": 0.006900906562805176}, {"id": 2124, "seek": 1072112, "start": 10738.240000000002, "end": 10741.84, "text": " again, it's experience building languages and not getting this right. So again,", "tokens": [51220, 797, 11, 309, 311, 1752, 2390, 8650, 293, 406, 1242, 341, 558, 13, 407, 797, 11, 51400], "temperature": 0.0, "avg_logprob": -0.1843319068083892, "compression_ratio": 1.8184713375796178, "no_speech_prob": 0.006900906562805176}, {"id": 2125, "seek": 1072112, "start": 10741.84, "end": 10745.6, "text": " you get another chance to do it and you get basic things like this, right?", "tokens": [51400, 291, 483, 1071, 2931, 281, 360, 309, 293, 291, 483, 3875, 721, 411, 341, 11, 558, 30, 51588], "temperature": 0.0, "avg_logprob": -0.1843319068083892, "compression_ratio": 1.8184713375796178, "no_speech_prob": 0.006900906562805176}, {"id": 2126, "seek": 1072112, "start": 10745.6, "end": 10749.84, "text": " But it's extremely powerful when you do that, right? And so there's a bunch of things like that", "tokens": [51588, 583, 309, 311, 4664, 4005, 562, 291, 360, 300, 11, 558, 30, 400, 370, 456, 311, 257, 3840, 295, 721, 411, 300, 51800], "temperature": 0.0, "avg_logprob": -0.1843319068083892, "compression_ratio": 1.8184713375796178, "no_speech_prob": 0.006900906562805176}, {"id": 2127, "seek": 1074984, "start": 10749.84, "end": 10755.28, "text": " that kind of combine together. And this comes back to the, you get a chance to do it the right way,", "tokens": [50364, 300, 733, 295, 10432, 1214, 13, 400, 341, 1487, 646, 281, 264, 11, 291, 483, 257, 2931, 281, 360, 309, 264, 558, 636, 11, 50636], "temperature": 0.0, "avg_logprob": -0.10473396159984448, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.0007553024333901703}, {"id": 2128, "seek": 1074984, "start": 10755.28, "end": 10758.8, "text": " do it the right way and make sure that every brick you put down is really good,", "tokens": [50636, 360, 309, 264, 558, 636, 293, 652, 988, 300, 633, 16725, 291, 829, 760, 307, 534, 665, 11, 50812], "temperature": 0.0, "avg_logprob": -0.10473396159984448, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.0007553024333901703}, {"id": 2129, "seek": 1074984, "start": 10758.8, "end": 10762.56, "text": " so that when you put more bricks on top of it, they stack up to something that's beautiful.", "tokens": [50812, 370, 300, 562, 291, 829, 544, 25497, 322, 1192, 295, 309, 11, 436, 8630, 493, 281, 746, 300, 311, 2238, 13, 51000], "temperature": 0.0, "avg_logprob": -0.10473396159984448, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.0007553024333901703}, {"id": 2130, "seek": 1074984, "start": 10762.56, "end": 10770.0, "text": " Well, there's also like, how many design discussions do there have to be about particular", "tokens": [51000, 1042, 11, 456, 311, 611, 411, 11, 577, 867, 1715, 11088, 360, 456, 362, 281, 312, 466, 1729, 51372], "temperature": 0.0, "avg_logprob": -0.10473396159984448, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.0007553024333901703}, {"id": 2131, "seek": 1074984, "start": 10770.0, "end": 10775.92, "text": " details like implementation of particular small features? Because the features that seem small,", "tokens": [51372, 4365, 411, 11420, 295, 1729, 1359, 4122, 30, 1436, 264, 4122, 300, 1643, 1359, 11, 51668], "temperature": 0.0, "avg_logprob": -0.10473396159984448, "compression_ratio": 1.731060606060606, "no_speech_prob": 0.0007553024333901703}, {"id": 2132, "seek": 1077592, "start": 10776.4, "end": 10782.56, "text": " I bet some of them might be like really require really big design decisions.", "tokens": [50388, 286, 778, 512, 295, 552, 1062, 312, 411, 534, 3651, 534, 955, 1715, 5327, 13, 50696], "temperature": 0.0, "avg_logprob": -0.17300193921654625, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004397627431899309}, {"id": 2133, "seek": 1077592, "start": 10782.56, "end": 10786.72, "text": " Yeah. Well, so, I mean, let me give you another example of this. Python has a feature called", "tokens": [50696, 865, 13, 1042, 11, 370, 11, 286, 914, 11, 718, 385, 976, 291, 1071, 1365, 295, 341, 13, 15329, 575, 257, 4111, 1219, 50904], "temperature": 0.0, "avg_logprob": -0.17300193921654625, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004397627431899309}, {"id": 2134, "seek": 1077592, "start": 10786.72, "end": 10794.08, "text": " async await. So it's a new feature, I mean, in the long arc of history, it's a relatively new", "tokens": [50904, 382, 34015, 19670, 13, 407, 309, 311, 257, 777, 4111, 11, 286, 914, 11, 294, 264, 938, 10346, 295, 2503, 11, 309, 311, 257, 7226, 777, 51272], "temperature": 0.0, "avg_logprob": -0.17300193921654625, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004397627431899309}, {"id": 2135, "seek": 1077592, "start": 10794.08, "end": 10800.64, "text": " feature, right? That allows way more expressive asynchronous programming. Okay. Again, this is", "tokens": [51272, 4111, 11, 558, 30, 663, 4045, 636, 544, 40189, 49174, 9410, 13, 1033, 13, 3764, 11, 341, 307, 51600], "temperature": 0.0, "avg_logprob": -0.17300193921654625, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004397627431899309}, {"id": 2136, "seek": 1077592, "start": 10800.64, "end": 10805.36, "text": " a Python's a beautiful thing and they did things that are great for Mojo for completely different", "tokens": [51600, 257, 15329, 311, 257, 2238, 551, 293, 436, 630, 721, 300, 366, 869, 337, 3335, 5134, 337, 2584, 819, 51836], "temperature": 0.0, "avg_logprob": -0.17300193921654625, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.004397627431899309}, {"id": 2137, "seek": 1080536, "start": 10805.44, "end": 10811.44, "text": " reasons. The reason the async await got added to Python, as far as I know, is because Python doesn't", "tokens": [50368, 4112, 13, 440, 1778, 264, 382, 34015, 19670, 658, 3869, 281, 15329, 11, 382, 1400, 382, 286, 458, 11, 307, 570, 15329, 1177, 380, 50668], "temperature": 0.0, "avg_logprob": -0.10316641434379246, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.004608252551406622}, {"id": 2138, "seek": 1080536, "start": 10811.44, "end": 10817.52, "text": " support threads. Okay. And so Python doesn't support threads, but you want to work with", "tokens": [50668, 1406, 19314, 13, 1033, 13, 400, 370, 15329, 1177, 380, 1406, 19314, 11, 457, 291, 528, 281, 589, 365, 50972], "temperature": 0.0, "avg_logprob": -0.10316641434379246, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.004608252551406622}, {"id": 2139, "seek": 1080536, "start": 10818.08, "end": 10821.84, "text": " networking and other things like that that can block. I mean, Python does support threads,", "tokens": [51000, 17985, 293, 661, 721, 411, 300, 300, 393, 3461, 13, 286, 914, 11, 15329, 775, 1406, 19314, 11, 51188], "temperature": 0.0, "avg_logprob": -0.10316641434379246, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.004608252551406622}, {"id": 2140, "seek": 1080536, "start": 10821.84, "end": 10828.24, "text": " it's just not its strength. And so they added this feature called async await. It's also seen", "tokens": [51188, 309, 311, 445, 406, 1080, 3800, 13, 400, 370, 436, 3869, 341, 4111, 1219, 382, 34015, 19670, 13, 467, 311, 611, 1612, 51508], "temperature": 0.0, "avg_logprob": -0.10316641434379246, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.004608252551406622}, {"id": 2141, "seek": 1080536, "start": 10828.24, "end": 10834.400000000001, "text": " in other languages like Swift and JavaScript and many other places as well. Async await in Mojo", "tokens": [51508, 294, 661, 8650, 411, 25539, 293, 15778, 293, 867, 661, 3190, 382, 731, 13, 1018, 34015, 19670, 294, 3335, 5134, 51816], "temperature": 0.0, "avg_logprob": -0.10316641434379246, "compression_ratio": 1.7900763358778626, "no_speech_prob": 0.004608252551406622}, {"id": 2142, "seek": 1083440, "start": 10834.4, "end": 10838.48, "text": " is amazing. Because we have a high-performance heterogeneous compute runtime underneath the", "tokens": [50364, 307, 2243, 13, 1436, 321, 362, 257, 1090, 12, 50242, 20789, 31112, 14722, 34474, 7223, 264, 50568], "temperature": 0.0, "avg_logprob": -0.13595664284446024, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.014496463350951672}, {"id": 2143, "seek": 1083440, "start": 10838.48, "end": 10846.56, "text": " covers that then allows non-blocking IO, so you get full use of your accelerator. That's huge,", "tokens": [50568, 10538, 300, 550, 4045, 2107, 12, 28830, 278, 39839, 11, 370, 291, 483, 1577, 764, 295, 428, 39889, 13, 663, 311, 2603, 11, 50972], "temperature": 0.0, "avg_logprob": -0.13595664284446024, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.014496463350951672}, {"id": 2144, "seek": 1083440, "start": 10846.56, "end": 10851.119999999999, "text": " turns out. It's actually really an important part of fully utilizing the machine. You talk about", "tokens": [50972, 4523, 484, 13, 467, 311, 767, 534, 364, 1021, 644, 295, 4498, 26775, 264, 3479, 13, 509, 751, 466, 51200], "temperature": 0.0, "avg_logprob": -0.13595664284446024, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.014496463350951672}, {"id": 2145, "seek": 1083440, "start": 10851.119999999999, "end": 10856.0, "text": " design discussions. That took a lot of discussions, right? And it probably will require more", "tokens": [51200, 1715, 11088, 13, 663, 1890, 257, 688, 295, 11088, 11, 558, 30, 400, 309, 1391, 486, 3651, 544, 51444], "temperature": 0.0, "avg_logprob": -0.13595664284446024, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.014496463350951672}, {"id": 2146, "seek": 1083440, "start": 10856.0, "end": 10860.72, "text": " iteration. And so my philosophy with Mojo is that, you know, we have a small team of really", "tokens": [51444, 24784, 13, 400, 370, 452, 10675, 365, 3335, 5134, 307, 300, 11, 291, 458, 11, 321, 362, 257, 1359, 1469, 295, 534, 51680], "temperature": 0.0, "avg_logprob": -0.13595664284446024, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.014496463350951672}, {"id": 2147, "seek": 1086072, "start": 10860.8, "end": 10865.76, "text": " good people that are pushing forward, and they're very good at the extremely deep knowing how the", "tokens": [50368, 665, 561, 300, 366, 7380, 2128, 11, 293, 436, 434, 588, 665, 412, 264, 4664, 2452, 5276, 577, 264, 50616], "temperature": 0.0, "avg_logprob": -0.1421920657157898, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.02516787312924862}, {"id": 2148, "seek": 1086072, "start": 10865.76, "end": 10871.679999999998, "text": " compiler and runtime and all the low-level stuff works together. But they're not perfect. Same", "tokens": [50616, 31958, 293, 34474, 293, 439, 264, 2295, 12, 12418, 1507, 1985, 1214, 13, 583, 436, 434, 406, 2176, 13, 10635, 50912], "temperature": 0.0, "avg_logprob": -0.1421920657157898, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.02516787312924862}, {"id": 2149, "seek": 1086072, "start": 10871.679999999998, "end": 10876.8, "text": " thing as the Swift team, right? And this is where one of the reasons we released Mojo much earlier", "tokens": [50912, 551, 382, 264, 25539, 1469, 11, 558, 30, 400, 341, 307, 689, 472, 295, 264, 4112, 321, 4736, 3335, 5134, 709, 3071, 51168], "temperature": 0.0, "avg_logprob": -0.1421920657157898, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.02516787312924862}, {"id": 2150, "seek": 1086072, "start": 10876.8, "end": 10879.92, "text": " is so we can get feedback. And we've already renamed a keyword", "tokens": [51168, 307, 370, 321, 393, 483, 5824, 13, 400, 321, 600, 1217, 40949, 257, 20428, 51324], "temperature": 0.0, "avg_logprob": -0.1421920657157898, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.02516787312924862}, {"id": 2151, "seek": 1086072, "start": 10880.72, "end": 10887.119999999999, "text": " due to a community feedback. We use an ampersand, and now it's named in and out. We're not", "tokens": [51364, 3462, 281, 257, 1768, 5824, 13, 492, 764, 364, 18648, 433, 474, 11, 293, 586, 309, 311, 4926, 294, 293, 484, 13, 492, 434, 406, 51684], "temperature": 0.0, "avg_logprob": -0.1421920657157898, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.02516787312924862}, {"id": 2152, "seek": 1088712, "start": 10887.12, "end": 10891.68, "text": " renaming existing Python keywords because that breaks compatibility. We're naming things we're", "tokens": [50364, 8124, 5184, 6741, 15329, 21009, 570, 300, 9857, 34237, 13, 492, 434, 25290, 721, 321, 434, 50592], "temperature": 0.0, "avg_logprob": -0.09902346801757812, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013017197139561176}, {"id": 2153, "seek": 1088712, "start": 10891.68, "end": 10897.28, "text": " adding and making sure that they are designed well. We get usage experience. We iterate and work", "tokens": [50592, 5127, 293, 1455, 988, 300, 436, 366, 4761, 731, 13, 492, 483, 14924, 1752, 13, 492, 44497, 293, 589, 50872], "temperature": 0.0, "avg_logprob": -0.09902346801757812, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013017197139561176}, {"id": 2154, "seek": 1088712, "start": 10897.28, "end": 10901.04, "text": " with the community because, again, if you scale something really fast and everybody writes all", "tokens": [50872, 365, 264, 1768, 570, 11, 797, 11, 498, 291, 4373, 746, 534, 2370, 293, 2201, 13657, 439, 51060], "temperature": 0.0, "avg_logprob": -0.09902346801757812, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013017197139561176}, {"id": 2155, "seek": 1088712, "start": 10901.04, "end": 10904.960000000001, "text": " their code and they start using it in production, then it's impossible to change. And so you want", "tokens": [51060, 641, 3089, 293, 436, 722, 1228, 309, 294, 4265, 11, 550, 309, 311, 6243, 281, 1319, 13, 400, 370, 291, 528, 51256], "temperature": 0.0, "avg_logprob": -0.09902346801757812, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013017197139561176}, {"id": 2156, "seek": 1088712, "start": 10904.960000000001, "end": 10909.28, "text": " to learn from people. You want to iterate and work on that early on. And this is where design", "tokens": [51256, 281, 1466, 490, 561, 13, 509, 528, 281, 44497, 293, 589, 322, 300, 2440, 322, 13, 400, 341, 307, 689, 1715, 51472], "temperature": 0.0, "avg_logprob": -0.09902346801757812, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013017197139561176}, {"id": 2157, "seek": 1088712, "start": 10909.28, "end": 10915.04, "text": " discussions, it's actually quite important. Could you incorporate an emoji into the language,", "tokens": [51472, 11088, 11, 309, 311, 767, 1596, 1021, 13, 7497, 291, 16091, 364, 31595, 666, 264, 2856, 11, 51760], "temperature": 0.0, "avg_logprob": -0.09902346801757812, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.013017197139561176}, {"id": 2158, "seek": 1091504, "start": 10915.04, "end": 10918.880000000001, "text": " into the main language? Do you have a favorite one?", "tokens": [50364, 666, 264, 2135, 2856, 30, 1144, 291, 362, 257, 2954, 472, 30, 50556], "temperature": 0.0, "avg_logprob": -0.2358392752133883, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.017169872298836708}, {"id": 2159, "seek": 1091504, "start": 10919.52, "end": 10925.44, "text": " Why really inters the humor, like rawful, whatever, rolling on the floor laughing?", "tokens": [50588, 1545, 534, 728, 82, 264, 14318, 11, 411, 8936, 906, 11, 2035, 11, 9439, 322, 264, 4123, 5059, 30, 50884], "temperature": 0.0, "avg_logprob": -0.2358392752133883, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.017169872298836708}, {"id": 2160, "seek": 1091504, "start": 10926.240000000002, "end": 10931.76, "text": " So that could be like, what would that be, the use case for that? Like throw an exception", "tokens": [50924, 407, 300, 727, 312, 411, 11, 437, 576, 300, 312, 11, 264, 764, 1389, 337, 300, 30, 1743, 3507, 364, 11183, 51200], "temperature": 0.0, "avg_logprob": -0.2358392752133883, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.017169872298836708}, {"id": 2161, "seek": 1091504, "start": 10931.76, "end": 10938.400000000001, "text": " of some sort? You should totally file a feature request. Or maybe a hard one. It has to be a", "tokens": [51200, 295, 512, 1333, 30, 509, 820, 3879, 3991, 257, 4111, 5308, 13, 1610, 1310, 257, 1152, 472, 13, 467, 575, 281, 312, 257, 51532], "temperature": 0.0, "avg_logprob": -0.2358392752133883, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.017169872298836708}, {"id": 2162, "seek": 1091504, "start": 10938.400000000001, "end": 10943.36, "text": " hard one. People have told me that I'm insane. I'm liking this.", "tokens": [51532, 1152, 472, 13, 3432, 362, 1907, 385, 300, 286, 478, 10838, 13, 286, 478, 16933, 341, 13, 51780], "temperature": 0.0, "avg_logprob": -0.2358392752133883, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.017169872298836708}, {"id": 2163, "seek": 1094336, "start": 10944.32, "end": 10949.44, "text": " I'm going to use the viral nature of the internet to actually get this past.", "tokens": [50412, 286, 478, 516, 281, 764, 264, 16132, 3687, 295, 264, 4705, 281, 767, 483, 341, 1791, 13, 50668], "temperature": 0.0, "avg_logprob": -0.2222001441171236, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004608559887856245}, {"id": 2164, "seek": 1094336, "start": 10950.16, "end": 10953.12, "text": " I mean, it's funny you come back to the flame emoji, file extension, right?", "tokens": [50704, 286, 914, 11, 309, 311, 4074, 291, 808, 646, 281, 264, 13287, 31595, 11, 3991, 10320, 11, 558, 30, 50852], "temperature": 0.0, "avg_logprob": -0.2222001441171236, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004608559887856245}, {"id": 2165, "seek": 1094336, "start": 10955.44, "end": 10961.04, "text": " We have the option to use the flame emoji, which just even that concept cause, for example,", "tokens": [50968, 492, 362, 264, 3614, 281, 764, 264, 13287, 31595, 11, 597, 445, 754, 300, 3410, 3082, 11, 337, 1365, 11, 51248], "temperature": 0.0, "avg_logprob": -0.2222001441171236, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004608559887856245}, {"id": 2166, "seek": 1094336, "start": 10961.04, "end": 10963.28, "text": " the people at GitHub to say, now I've seen everything.", "tokens": [51248, 264, 561, 412, 23331, 281, 584, 11, 586, 286, 600, 1612, 1203, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2222001441171236, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004608559887856245}, {"id": 2167, "seek": 1094336, "start": 10965.6, "end": 10972.720000000001, "text": " Yeah, there's something, it's reinvigorating. It's like, oh, that's possible. That's really", "tokens": [51476, 865, 11, 456, 311, 746, 11, 309, 311, 6561, 85, 47131, 990, 13, 467, 311, 411, 11, 1954, 11, 300, 311, 1944, 13, 663, 311, 534, 51832], "temperature": 0.0, "avg_logprob": -0.2222001441171236, "compression_ratio": 1.6024590163934427, "no_speech_prob": 0.004608559887856245}, {"id": 2168, "seek": 1097272, "start": 10972.72, "end": 10976.4, "text": " cool that for some reason that makes everything else seem really exciting.", "tokens": [50364, 1627, 300, 337, 512, 1778, 300, 1669, 1203, 1646, 1643, 534, 4670, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1471858866074506, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.002631093142554164}, {"id": 2169, "seek": 1097272, "start": 10976.4, "end": 10979.92, "text": " I think the world is ready for this stuff, right? And so, you know, when we have a package manager,", "tokens": [50548, 286, 519, 264, 1002, 307, 1919, 337, 341, 1507, 11, 558, 30, 400, 370, 11, 291, 458, 11, 562, 321, 362, 257, 7372, 6598, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1471858866074506, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.002631093142554164}, {"id": 2170, "seek": 1097272, "start": 10979.92, "end": 10984.08, "text": " we'll clearly have to innovate by having the compiled package saying be the little", "tokens": [50724, 321, 603, 4448, 362, 281, 33444, 538, 1419, 264, 36548, 7372, 1566, 312, 264, 707, 50932], "temperature": 0.0, "avg_logprob": -0.1471858866074506, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.002631093142554164}, {"id": 2171, "seek": 1097272, "start": 10984.08, "end": 10988.64, "text": " box with the bow on it, right? I mean, it has to be done.", "tokens": [50932, 2424, 365, 264, 4503, 322, 309, 11, 558, 30, 286, 914, 11, 309, 575, 281, 312, 1096, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1471858866074506, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.002631093142554164}, {"id": 2172, "seek": 1097272, "start": 10988.64, "end": 10992.24, "text": " It has to be done. Is there some stuff on the roadmap that you're particularly", "tokens": [51160, 467, 575, 281, 312, 1096, 13, 1119, 456, 512, 1507, 322, 264, 35738, 300, 291, 434, 4098, 51340], "temperature": 0.0, "avg_logprob": -0.1471858866074506, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.002631093142554164}, {"id": 2173, "seek": 1097272, "start": 10992.88, "end": 10995.92, "text": " stressed about or excited about that you're thinking about a lot?", "tokens": [51372, 14471, 466, 420, 2919, 466, 300, 291, 434, 1953, 466, 257, 688, 30, 51524], "temperature": 0.0, "avg_logprob": -0.1471858866074506, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.002631093142554164}, {"id": 2174, "seek": 1097272, "start": 10995.92, "end": 11001.119999999999, "text": " I mean, as a today snapshot, which will be obsolete tomorrow, the lifetime stuff is really", "tokens": [51524, 286, 914, 11, 382, 257, 965, 30163, 11, 597, 486, 312, 46333, 4153, 11, 264, 11364, 1507, 307, 534, 51784], "temperature": 0.0, "avg_logprob": -0.1471858866074506, "compression_ratio": 1.8184818481848184, "no_speech_prob": 0.002631093142554164}, {"id": 2175, "seek": 1100112, "start": 11001.12, "end": 11007.04, "text": " exciting. And so lifetimes give you safe references to memory without dangling pointers.", "tokens": [50364, 4670, 13, 400, 370, 4545, 302, 1532, 976, 291, 3273, 15400, 281, 4675, 1553, 21892, 1688, 44548, 13, 50660], "temperature": 0.0, "avg_logprob": -0.07590466988186877, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003944109659641981}, {"id": 2176, "seek": 1100112, "start": 11007.68, "end": 11010.720000000001, "text": " And so this has been done in languages like Rust before. And so we have a new approach,", "tokens": [50692, 400, 370, 341, 575, 668, 1096, 294, 8650, 411, 34952, 949, 13, 400, 370, 321, 362, 257, 777, 3109, 11, 50844], "temperature": 0.0, "avg_logprob": -0.07590466988186877, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003944109659641981}, {"id": 2177, "seek": 1100112, "start": 11010.720000000001, "end": 11014.800000000001, "text": " which is really cool. I'm very excited about that. That'll be out to the community very soon.", "tokens": [50844, 597, 307, 534, 1627, 13, 286, 478, 588, 2919, 466, 300, 13, 663, 603, 312, 484, 281, 264, 1768, 588, 2321, 13, 51048], "temperature": 0.0, "avg_logprob": -0.07590466988186877, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003944109659641981}, {"id": 2178, "seek": 1100112, "start": 11015.52, "end": 11021.04, "text": " The traits feature is really a big deal. And so that's blocking a lot of API design.", "tokens": [51084, 440, 19526, 4111, 307, 534, 257, 955, 2028, 13, 400, 370, 300, 311, 17776, 257, 688, 295, 9362, 1715, 13, 51360], "temperature": 0.0, "avg_logprob": -0.07590466988186877, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003944109659641981}, {"id": 2179, "seek": 1100112, "start": 11021.04, "end": 11023.28, "text": " And so there's that. I think that's really exciting.", "tokens": [51360, 400, 370, 456, 311, 300, 13, 286, 519, 300, 311, 534, 4670, 13, 51472], "temperature": 0.0, "avg_logprob": -0.07590466988186877, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003944109659641981}, {"id": 2180, "seek": 1100112, "start": 11025.2, "end": 11027.76, "text": " A lot of it is these kind of table stakes features.", "tokens": [51568, 316, 688, 295, 309, 307, 613, 733, 295, 3199, 28429, 4122, 13, 51696], "temperature": 0.0, "avg_logprob": -0.07590466988186877, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003944109659641981}, {"id": 2181, "seek": 1102776, "start": 11028.72, "end": 11032.16, "text": " One of the things that is, again, also lessons learned with Swift", "tokens": [50412, 1485, 295, 264, 721, 300, 307, 11, 797, 11, 611, 8820, 3264, 365, 25539, 50584], "temperature": 0.0, "avg_logprob": -0.15382739230319187, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.005383773241192102}, {"id": 2182, "seek": 1102776, "start": 11033.84, "end": 11037.68, "text": " is that programmers in general like to add syntactic sugar.", "tokens": [50668, 307, 300, 41504, 294, 2674, 411, 281, 909, 23980, 19892, 5076, 13, 50860], "temperature": 0.0, "avg_logprob": -0.15382739230319187, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.005383773241192102}, {"id": 2183, "seek": 1102776, "start": 11038.880000000001, "end": 11043.2, "text": " And so it's like, oh, well, this annoying thing, like in Python, you have to spell", "tokens": [50920, 400, 370, 309, 311, 411, 11, 1954, 11, 731, 11, 341, 11304, 551, 11, 411, 294, 15329, 11, 291, 362, 281, 9827, 51136], "temperature": 0.0, "avg_logprob": -0.15382739230319187, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.005383773241192102}, {"id": 2184, "seek": 1102776, "start": 11043.2, "end": 11049.28, "text": " unbar unbar add. Why can't I just use plus? Def plus, come on. Why can't I just do that, right?", "tokens": [51136, 517, 5356, 517, 5356, 909, 13, 1545, 393, 380, 286, 445, 764, 1804, 30, 9548, 1804, 11, 808, 322, 13, 1545, 393, 380, 286, 445, 360, 300, 11, 558, 30, 51440], "temperature": 0.0, "avg_logprob": -0.15382739230319187, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.005383773241192102}, {"id": 2185, "seek": 1102776, "start": 11049.28, "end": 11053.92, "text": " And so trivial bit of syntactic sugar, it makes sense. It's beautiful. It's obvious.", "tokens": [51440, 400, 370, 26703, 857, 295, 23980, 19892, 5076, 11, 309, 1669, 2020, 13, 467, 311, 2238, 13, 467, 311, 6322, 13, 51672], "temperature": 0.0, "avg_logprob": -0.15382739230319187, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.005383773241192102}, {"id": 2186, "seek": 1105392, "start": 11053.92, "end": 11060.56, "text": " We're trying not to do that. And so for two different reasons, one of which is that, again,", "tokens": [50364, 492, 434, 1382, 406, 281, 360, 300, 13, 400, 370, 337, 732, 819, 4112, 11, 472, 295, 597, 307, 300, 11, 797, 11, 50696], "temperature": 0.0, "avg_logprob": -0.09700776749298352, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.002251666970551014}, {"id": 2187, "seek": 1105392, "start": 11060.56, "end": 11066.72, "text": " lesson learned with Swift. Swift has a lot of syntactic sugar, which may be a good thing,", "tokens": [50696, 6898, 3264, 365, 25539, 13, 25539, 575, 257, 688, 295, 23980, 19892, 5076, 11, 597, 815, 312, 257, 665, 551, 11, 51004], "temperature": 0.0, "avg_logprob": -0.09700776749298352, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.002251666970551014}, {"id": 2188, "seek": 1105392, "start": 11066.72, "end": 11071.76, "text": " maybe not. I don't know. But because it's such an easy and addictive thing to do,", "tokens": [51004, 1310, 406, 13, 286, 500, 380, 458, 13, 583, 570, 309, 311, 1270, 364, 1858, 293, 36486, 551, 281, 360, 11, 51256], "temperature": 0.0, "avg_logprob": -0.09700776749298352, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.002251666970551014}, {"id": 2189, "seek": 1105392, "start": 11071.76, "end": 11077.68, "text": " sugar, like make sure blood get crazy, right? Like the community will really dig into that and", "tokens": [51256, 5076, 11, 411, 652, 988, 3390, 483, 3219, 11, 558, 30, 1743, 264, 1768, 486, 534, 2528, 666, 300, 293, 51552], "temperature": 0.0, "avg_logprob": -0.09700776749298352, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.002251666970551014}, {"id": 2190, "seek": 1105392, "start": 11077.68, "end": 11081.36, "text": " want to do a lot of that. And I think it's very distracting from building the core abstractions.", "tokens": [51552, 528, 281, 360, 257, 688, 295, 300, 13, 400, 286, 519, 309, 311, 588, 36689, 490, 2390, 264, 4965, 12649, 626, 13, 51736], "temperature": 0.0, "avg_logprob": -0.09700776749298352, "compression_ratio": 1.6366906474820144, "no_speech_prob": 0.002251666970551014}, {"id": 2191, "seek": 1108136, "start": 11081.92, "end": 11084.24, "text": " The second is we want to be a good member of the Python community,", "tokens": [50392, 440, 1150, 307, 321, 528, 281, 312, 257, 665, 4006, 295, 264, 15329, 1768, 11, 50508], "temperature": 0.0, "avg_logprob": -0.06754670107275024, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.0034829701762646437}, {"id": 2192, "seek": 1108136, "start": 11086.24, "end": 11091.92, "text": " right? And so we want to work with the broader Python community. And yeah, we're pushing forward", "tokens": [50608, 558, 30, 400, 370, 321, 528, 281, 589, 365, 264, 13227, 15329, 1768, 13, 400, 1338, 11, 321, 434, 7380, 2128, 50892], "temperature": 0.0, "avg_logprob": -0.06754670107275024, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.0034829701762646437}, {"id": 2193, "seek": 1108136, "start": 11091.92, "end": 11095.28, "text": " a bunch of systems programming features, and we need to build them out to understand them.", "tokens": [50892, 257, 3840, 295, 3652, 9410, 4122, 11, 293, 321, 643, 281, 1322, 552, 484, 281, 1223, 552, 13, 51060], "temperature": 0.0, "avg_logprob": -0.06754670107275024, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.0034829701762646437}, {"id": 2194, "seek": 1108136, "start": 11095.28, "end": 11099.6, "text": " But once we get a long ways forward, I want to make sure that we go back to the Python community", "tokens": [51060, 583, 1564, 321, 483, 257, 938, 2098, 2128, 11, 286, 528, 281, 652, 988, 300, 321, 352, 646, 281, 264, 15329, 1768, 51276], "temperature": 0.0, "avg_logprob": -0.06754670107275024, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.0034829701762646437}, {"id": 2195, "seek": 1108136, "start": 11099.6, "end": 11102.720000000001, "text": " and say, okay, let's do some design reviews. Let's actually talk about this stuff. Let's figure", "tokens": [51276, 293, 584, 11, 1392, 11, 718, 311, 360, 512, 1715, 10229, 13, 961, 311, 767, 751, 466, 341, 1507, 13, 961, 311, 2573, 51432], "temperature": 0.0, "avg_logprob": -0.06754670107275024, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.0034829701762646437}, {"id": 2196, "seek": 1108136, "start": 11102.720000000001, "end": 11107.28, "text": " out how we want this stuff all to work together. And syntactic sugar just makes all that more", "tokens": [51432, 484, 577, 321, 528, 341, 1507, 439, 281, 589, 1214, 13, 400, 23980, 19892, 5076, 445, 1669, 439, 300, 544, 51660], "temperature": 0.0, "avg_logprob": -0.06754670107275024, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.0034829701762646437}, {"id": 2197, "seek": 1110728, "start": 11107.28, "end": 11113.76, "text": " complicated. And yeah, list comprehension is like yet to be implemented. And my favorite,", "tokens": [50364, 6179, 13, 400, 1338, 11, 1329, 44991, 307, 411, 1939, 281, 312, 12270, 13, 400, 452, 2954, 11, 50688], "temperature": 0.0, "avg_logprob": -0.19993786189867102, "compression_ratio": 1.6, "no_speech_prob": 0.013015019707381725}, {"id": 2198, "seek": 1110728, "start": 11113.76, "end": 11121.36, "text": " I mean, dictionaries. Yeah, there's some basic zero point one, zero point one. But nonetheless,", "tokens": [50688, 286, 914, 11, 22352, 4889, 13, 865, 11, 456, 311, 512, 3875, 4018, 935, 472, 11, 4018, 935, 472, 13, 583, 26756, 11, 51068], "temperature": 0.0, "avg_logprob": -0.19993786189867102, "compression_ratio": 1.6, "no_speech_prob": 0.013015019707381725}, {"id": 2199, "seek": 1110728, "start": 11121.36, "end": 11125.68, "text": " it's actually still quite interesting and useful. As you mentioned, modular is very new.", "tokens": [51068, 309, 311, 767, 920, 1596, 1880, 293, 4420, 13, 1018, 291, 2835, 11, 31111, 307, 588, 777, 13, 51284], "temperature": 0.0, "avg_logprob": -0.19993786189867102, "compression_ratio": 1.6, "no_speech_prob": 0.013015019707381725}, {"id": 2200, "seek": 1110728, "start": 11127.12, "end": 11134.24, "text": " Mojo is very new. It's a relatively small team. Yeah, that's building up this gigantic stack.", "tokens": [51356, 3335, 5134, 307, 588, 777, 13, 467, 311, 257, 7226, 1359, 1469, 13, 865, 11, 300, 311, 2390, 493, 341, 26800, 8630, 13, 51712], "temperature": 0.0, "avg_logprob": -0.19993786189867102, "compression_ratio": 1.6, "no_speech_prob": 0.013015019707381725}, {"id": 2201, "seek": 1113424, "start": 11134.96, "end": 11138.96, "text": " This incredible stack that's going to perhaps define the future of", "tokens": [50400, 639, 4651, 8630, 300, 311, 516, 281, 4317, 6964, 264, 2027, 295, 50600], "temperature": 0.0, "avg_logprob": -0.10733727216720582, "compression_ratio": 1.4434389140271493, "no_speech_prob": 0.0071172392927110195}, {"id": 2202, "seek": 1113424, "start": 11140.0, "end": 11145.039999999999, "text": " development of our AI overlords. We just hope it will be useful.", "tokens": [50652, 3250, 295, 527, 7318, 15986, 5703, 13, 492, 445, 1454, 309, 486, 312, 4420, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10733727216720582, "compression_ratio": 1.4434389140271493, "no_speech_prob": 0.0071172392927110195}, {"id": 2203, "seek": 1113424, "start": 11146.72, "end": 11154.4, "text": " As do all of us. So what, what have you learned from this process of building up a team? Maybe", "tokens": [50988, 1018, 360, 439, 295, 505, 13, 407, 437, 11, 437, 362, 291, 3264, 490, 341, 1399, 295, 2390, 493, 257, 1469, 30, 2704, 51372], "temperature": 0.0, "avg_logprob": -0.10733727216720582, "compression_ratio": 1.4434389140271493, "no_speech_prob": 0.0071172392927110195}, {"id": 2204, "seek": 1113424, "start": 11154.4, "end": 11161.44, "text": " one question is, how do you hire? Yeah, great programmers, great people that operate in this", "tokens": [51372, 472, 1168, 307, 11, 577, 360, 291, 11158, 30, 865, 11, 869, 41504, 11, 869, 561, 300, 9651, 294, 341, 51724], "temperature": 0.0, "avg_logprob": -0.10733727216720582, "compression_ratio": 1.4434389140271493, "no_speech_prob": 0.0071172392927110195}, {"id": 2205, "seek": 1116144, "start": 11162.4, "end": 11171.52, "text": " compiler, hardware, machine learning, software, interface design space. And maybe you're a", "tokens": [50412, 31958, 11, 8837, 11, 3479, 2539, 11, 4722, 11, 9226, 1715, 1901, 13, 400, 1310, 291, 434, 257, 50868], "temperature": 0.0, "avg_logprob": -0.17404726418581876, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0008692019619047642}, {"id": 2206, "seek": 1116144, "start": 11171.52, "end": 11177.44, "text": " little bit fluid in what they can do. So okay, so language design too. So building a company is", "tokens": [50868, 707, 857, 9113, 294, 437, 436, 393, 360, 13, 407, 1392, 11, 370, 2856, 1715, 886, 13, 407, 2390, 257, 2237, 307, 51164], "temperature": 0.0, "avg_logprob": -0.17404726418581876, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0008692019619047642}, {"id": 2207, "seek": 1116144, "start": 11177.44, "end": 11183.12, "text": " just as interesting in different ways as building a language, like different skill sets, different", "tokens": [51164, 445, 382, 1880, 294, 819, 2098, 382, 2390, 257, 2856, 11, 411, 819, 5389, 6352, 11, 819, 51448], "temperature": 0.0, "avg_logprob": -0.17404726418581876, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0008692019619047642}, {"id": 2208, "seek": 1116144, "start": 11183.12, "end": 11186.720000000001, "text": " things, but super interesting. And I've built a lot of teams in a lot of different places.", "tokens": [51448, 721, 11, 457, 1687, 1880, 13, 400, 286, 600, 3094, 257, 688, 295, 5491, 294, 257, 688, 295, 819, 3190, 13, 51628], "temperature": 0.0, "avg_logprob": -0.17404726418581876, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.0008692019619047642}, {"id": 2209, "seek": 1118672, "start": 11187.679999999998, "end": 11190.4, "text": " If you zoom in from the big problem into recruiting,", "tokens": [50412, 759, 291, 8863, 294, 490, 264, 955, 1154, 666, 25987, 11, 50548], "temperature": 0.0, "avg_logprob": -0.09224318695068359, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.009123371914029121}, {"id": 2210, "seek": 1118672, "start": 11191.519999999999, "end": 11196.4, "text": " well, so here's our problem. Okay, I'll just, I'll be very straightforward about this.", "tokens": [50604, 731, 11, 370, 510, 311, 527, 1154, 13, 1033, 11, 286, 603, 445, 11, 286, 603, 312, 588, 15325, 466, 341, 13, 50848], "temperature": 0.0, "avg_logprob": -0.09224318695068359, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.009123371914029121}, {"id": 2211, "seek": 1118672, "start": 11196.4, "end": 11200.88, "text": " We started modular with a lot of conviction about we understand the problems, we understand the", "tokens": [50848, 492, 1409, 31111, 365, 257, 688, 295, 24837, 466, 321, 1223, 264, 2740, 11, 321, 1223, 264, 51072], "temperature": 0.0, "avg_logprob": -0.09224318695068359, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.009123371914029121}, {"id": 2212, "seek": 1118672, "start": 11200.88, "end": 11205.92, "text": " customer pain points, we need to work backwards from the suffering in the industry. And if we", "tokens": [51072, 5474, 1822, 2793, 11, 321, 643, 281, 589, 12204, 490, 264, 7755, 294, 264, 3518, 13, 400, 498, 321, 51324], "temperature": 0.0, "avg_logprob": -0.09224318695068359, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.009123371914029121}, {"id": 2213, "seek": 1118672, "start": 11205.92, "end": 11210.64, "text": " solve those problems, we think it'll be useful for people. But the problem is, is that the people", "tokens": [51324, 5039, 729, 2740, 11, 321, 519, 309, 603, 312, 4420, 337, 561, 13, 583, 264, 1154, 307, 11, 307, 300, 264, 561, 51560], "temperature": 0.0, "avg_logprob": -0.09224318695068359, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.009123371914029121}, {"id": 2214, "seek": 1118672, "start": 11210.64, "end": 11215.92, "text": " we need to hire, as you say, are all these super specialized people that have jobs at big tech", "tokens": [51560, 321, 643, 281, 11158, 11, 382, 291, 584, 11, 366, 439, 613, 1687, 19813, 561, 300, 362, 4782, 412, 955, 7553, 51824], "temperature": 0.0, "avg_logprob": -0.09224318695068359, "compression_ratio": 1.7635135135135136, "no_speech_prob": 0.009123371914029121}, {"id": 2215, "seek": 1121672, "start": 11217.279999999999, "end": 11223.039999999999, "text": " big tech worlds, right? And, you know, we, I don't think we have product market fit in the way that", "tokens": [50392, 955, 7553, 13401, 11, 558, 30, 400, 11, 291, 458, 11, 321, 11, 286, 500, 380, 519, 321, 362, 1674, 2142, 3318, 294, 264, 636, 300, 50680], "temperature": 0.0, "avg_logprob": -0.11207152976364386, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0017002118984237313}, {"id": 2216, "seek": 1121672, "start": 11223.039999999999, "end": 11228.48, "text": " a normal startup does, we don't have product market fit challenges, because right now,", "tokens": [50680, 257, 2710, 18578, 775, 11, 321, 500, 380, 362, 1674, 2142, 3318, 4759, 11, 570, 558, 586, 11, 50952], "temperature": 0.0, "avg_logprob": -0.11207152976364386, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0017002118984237313}, {"id": 2217, "seek": 1121672, "start": 11228.48, "end": 11232.72, "text": " everybody's using AI and so many of them are suffering and they want help. And so again,", "tokens": [50952, 2201, 311, 1228, 7318, 293, 370, 867, 295, 552, 366, 7755, 293, 436, 528, 854, 13, 400, 370, 797, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11207152976364386, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0017002118984237313}, {"id": 2218, "seek": 1121672, "start": 11232.72, "end": 11237.519999999999, "text": " we started with strong conviction. Now, again, you have to hire and recruit the best and the", "tokens": [51164, 321, 1409, 365, 2068, 24837, 13, 823, 11, 797, 11, 291, 362, 281, 11158, 293, 15119, 264, 1151, 293, 264, 51404], "temperature": 0.0, "avg_logprob": -0.11207152976364386, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0017002118984237313}, {"id": 2219, "seek": 1121672, "start": 11237.519999999999, "end": 11242.08, "text": " best all have jobs. And so what we've done is we said, okay, well, let's build an amazing culture.", "tokens": [51404, 1151, 439, 362, 4782, 13, 400, 370, 437, 321, 600, 1096, 307, 321, 848, 11, 1392, 11, 731, 11, 718, 311, 1322, 364, 2243, 3713, 13, 51632], "temperature": 0.0, "avg_logprob": -0.11207152976364386, "compression_ratio": 1.6798561151079137, "no_speech_prob": 0.0017002118984237313}, {"id": 2220, "seek": 1124208, "start": 11243.039999999999, "end": 11246.96, "text": " Start with that. That's usually not something a company starts with. Usually you hire a bunch", "tokens": [50412, 6481, 365, 300, 13, 663, 311, 2673, 406, 746, 257, 2237, 3719, 365, 13, 11419, 291, 11158, 257, 3840, 50608], "temperature": 0.0, "avg_logprob": -0.12519698212112207, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.1258038729429245}, {"id": 2221, "seek": 1124208, "start": 11246.96, "end": 11251.68, "text": " of people and then people start fighting and it turns into a gigantic mess. And then you try to", "tokens": [50608, 295, 561, 293, 550, 561, 722, 5237, 293, 309, 4523, 666, 257, 26800, 2082, 13, 400, 550, 291, 853, 281, 50844], "temperature": 0.0, "avg_logprob": -0.12519698212112207, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.1258038729429245}, {"id": 2222, "seek": 1124208, "start": 11251.68, "end": 11255.76, "text": " figure out how to improve your culture later. My co-founder, Tim, in particular, is super", "tokens": [50844, 2573, 484, 577, 281, 3470, 428, 3713, 1780, 13, 1222, 598, 12, 33348, 11, 7172, 11, 294, 1729, 11, 307, 1687, 51048], "temperature": 0.0, "avg_logprob": -0.12519698212112207, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.1258038729429245}, {"id": 2223, "seek": 1124208, "start": 11255.76, "end": 11260.24, "text": " passionate about making sure that that's right. And we've spent a lot of time early on to make", "tokens": [51048, 11410, 466, 1455, 988, 300, 300, 311, 558, 13, 400, 321, 600, 4418, 257, 688, 295, 565, 2440, 322, 281, 652, 51272], "temperature": 0.0, "avg_logprob": -0.12519698212112207, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.1258038729429245}, {"id": 2224, "seek": 1124208, "start": 11260.24, "end": 11264.64, "text": " sure that we can scale. Can you comment, sorry, before we get to the second, what makes for a", "tokens": [51272, 988, 300, 321, 393, 4373, 13, 1664, 291, 2871, 11, 2597, 11, 949, 321, 483, 281, 264, 1150, 11, 437, 1669, 337, 257, 51492], "temperature": 0.0, "avg_logprob": -0.12519698212112207, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.1258038729429245}, {"id": 2225, "seek": 1124208, "start": 11264.64, "end": 11269.92, "text": " good culture? So I mean, there's many different cultures. And I have learned many things from", "tokens": [51492, 665, 3713, 30, 407, 286, 914, 11, 456, 311, 867, 819, 12951, 13, 400, 286, 362, 3264, 867, 721, 490, 51756], "temperature": 0.0, "avg_logprob": -0.12519698212112207, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.1258038729429245}, {"id": 2226, "seek": 1126992, "start": 11270.88, "end": 11276.56, "text": " several very unique, almost famously unique cultures. And some of them I learned what to do", "tokens": [50412, 2940, 588, 3845, 11, 1920, 34360, 3845, 12951, 13, 400, 512, 295, 552, 286, 3264, 437, 281, 360, 50696], "temperature": 0.0, "avg_logprob": -0.11668364921312653, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.009403452277183533}, {"id": 2227, "seek": 1126992, "start": 11276.56, "end": 11284.8, "text": " and some of them I learned what not to do. And so we want an inclusive culture. I believe in", "tokens": [50696, 293, 512, 295, 552, 286, 3264, 437, 406, 281, 360, 13, 400, 370, 321, 528, 364, 13429, 3713, 13, 286, 1697, 294, 51108], "temperature": 0.0, "avg_logprob": -0.11668364921312653, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.009403452277183533}, {"id": 2228, "seek": 1126992, "start": 11286.56, "end": 11291.36, "text": " amazing people working together. And so I've seen cultures where people, you have amazing people", "tokens": [51196, 2243, 561, 1364, 1214, 13, 400, 370, 286, 600, 1612, 12951, 689, 561, 11, 291, 362, 2243, 561, 51436], "temperature": 0.0, "avg_logprob": -0.11668364921312653, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.009403452277183533}, {"id": 2229, "seek": 1126992, "start": 11291.36, "end": 11296.64, "text": " and they're fighting each other. I see amazing people and they're told what to do. Like,", "tokens": [51436, 293, 436, 434, 5237, 1184, 661, 13, 286, 536, 2243, 561, 293, 436, 434, 1907, 437, 281, 360, 13, 1743, 11, 51700], "temperature": 0.0, "avg_logprob": -0.11668364921312653, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.009403452277183533}, {"id": 2230, "seek": 1129664, "start": 11296.64, "end": 11300.4, "text": " doubt, shout, line up and do what I say. It doesn't matter if it's the right thing. Do it.", "tokens": [50364, 6385, 11, 8043, 11, 1622, 493, 293, 360, 437, 286, 584, 13, 467, 1177, 380, 1871, 498, 309, 311, 264, 558, 551, 13, 1144, 309, 13, 50552], "temperature": 0.0, "avg_logprob": -0.08695326131932876, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.010324455797672272}, {"id": 2231, "seek": 1129664, "start": 11301.519999999999, "end": 11305.76, "text": " And neither of these, and I've seen people that have no direction. They're just kind of floating", "tokens": [50608, 400, 9662, 295, 613, 11, 293, 286, 600, 1612, 561, 300, 362, 572, 3513, 13, 814, 434, 445, 733, 295, 12607, 50820], "temperature": 0.0, "avg_logprob": -0.08695326131932876, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.010324455797672272}, {"id": 2232, "seek": 1129664, "start": 11305.76, "end": 11310.48, "text": " in different places. And they want to be amazing. They just don't know how. And so a lot of it starts", "tokens": [50820, 294, 819, 3190, 13, 400, 436, 528, 281, 312, 2243, 13, 814, 445, 500, 380, 458, 577, 13, 400, 370, 257, 688, 295, 309, 3719, 51056], "temperature": 0.0, "avg_logprob": -0.08695326131932876, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.010324455797672272}, {"id": 2233, "seek": 1129664, "start": 11310.48, "end": 11317.039999999999, "text": " with have a clear vision. And so we have a clear vision of what we're doing. And so I kind of grew", "tokens": [51056, 365, 362, 257, 1850, 5201, 13, 400, 370, 321, 362, 257, 1850, 5201, 295, 437, 321, 434, 884, 13, 400, 370, 286, 733, 295, 6109, 51384], "temperature": 0.0, "avg_logprob": -0.08695326131932876, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.010324455797672272}, {"id": 2234, "seek": 1129664, "start": 11317.039999999999, "end": 11324.0, "text": " up at Apple in my engineering life. And so a lot of the Apple DNA rubbed off on me. My co-founder,", "tokens": [51384, 493, 412, 6373, 294, 452, 7043, 993, 13, 400, 370, 257, 688, 295, 264, 6373, 8272, 5915, 2883, 766, 322, 385, 13, 1222, 598, 12, 33348, 11, 51732], "temperature": 0.0, "avg_logprob": -0.08695326131932876, "compression_ratio": 1.7147887323943662, "no_speech_prob": 0.010324455797672272}, {"id": 2235, "seek": 1132400, "start": 11324.08, "end": 11329.12, "text": " Tim, also is like a strong product guy. And so what we learned is, you know, I decided Apple that", "tokens": [50368, 7172, 11, 611, 307, 411, 257, 2068, 1674, 2146, 13, 400, 370, 437, 321, 3264, 307, 11, 291, 458, 11, 286, 3047, 6373, 300, 50620], "temperature": 0.0, "avg_logprob": -0.10190116193957795, "compression_ratio": 1.7959866220735785, "no_speech_prob": 0.07364624738693237}, {"id": 2236, "seek": 1132400, "start": 11329.12, "end": 11334.32, "text": " you don't work from building cool technology. You don't work from, like, come up with a cool", "tokens": [50620, 291, 500, 380, 589, 490, 2390, 1627, 2899, 13, 509, 500, 380, 589, 490, 11, 411, 11, 808, 493, 365, 257, 1627, 50880], "temperature": 0.0, "avg_logprob": -0.10190116193957795, "compression_ratio": 1.7959866220735785, "no_speech_prob": 0.07364624738693237}, {"id": 2237, "seek": 1132400, "start": 11334.32, "end": 11337.6, "text": " product and think about the features you'll have in the big checkboxes and stuff like this.", "tokens": [50880, 1674, 293, 519, 466, 264, 4122, 291, 603, 362, 294, 264, 955, 1520, 4995, 279, 293, 1507, 411, 341, 13, 51044], "temperature": 0.0, "avg_logprob": -0.10190116193957795, "compression_ratio": 1.7959866220735785, "no_speech_prob": 0.07364624738693237}, {"id": 2238, "seek": 1132400, "start": 11338.24, "end": 11340.96, "text": " Because if you go talk to customers, they don't actually care about your product.", "tokens": [51076, 1436, 498, 291, 352, 751, 281, 4581, 11, 436, 500, 380, 767, 1127, 466, 428, 1674, 13, 51212], "temperature": 0.0, "avg_logprob": -0.10190116193957795, "compression_ratio": 1.7959866220735785, "no_speech_prob": 0.07364624738693237}, {"id": 2239, "seek": 1132400, "start": 11341.52, "end": 11345.04, "text": " They don't care about your technology. What they care about is their problems.", "tokens": [51240, 814, 500, 380, 1127, 466, 428, 2899, 13, 708, 436, 1127, 466, 307, 641, 2740, 13, 51416], "temperature": 0.0, "avg_logprob": -0.10190116193957795, "compression_ratio": 1.7959866220735785, "no_speech_prob": 0.07364624738693237}, {"id": 2240, "seek": 1132400, "start": 11346.0, "end": 11350.48, "text": " Right? And if your product can help solve their problems, well, hey, they might be interested", "tokens": [51464, 1779, 30, 400, 498, 428, 1674, 393, 854, 5039, 641, 2740, 11, 731, 11, 4177, 11, 436, 1062, 312, 3102, 51688], "temperature": 0.0, "avg_logprob": -0.10190116193957795, "compression_ratio": 1.7959866220735785, "no_speech_prob": 0.07364624738693237}, {"id": 2241, "seek": 1135048, "start": 11350.56, "end": 11354.0, "text": " in that. And so if you speak to them about their problems, if you understand and you", "tokens": [50368, 294, 300, 13, 400, 370, 498, 291, 1710, 281, 552, 466, 641, 2740, 11, 498, 291, 1223, 293, 291, 50540], "temperature": 0.0, "avg_logprob": -0.12404313706259691, "compression_ratio": 1.862190812720848, "no_speech_prob": 0.04740544408559799}, {"id": 2242, "seek": 1135048, "start": 11354.0, "end": 11358.0, "text": " have compassion, you understand what people are working with, then you can work backwards to", "tokens": [50540, 362, 12601, 11, 291, 1223, 437, 561, 366, 1364, 365, 11, 550, 291, 393, 589, 12204, 281, 50740], "temperature": 0.0, "avg_logprob": -0.12404313706259691, "compression_ratio": 1.862190812720848, "no_speech_prob": 0.04740544408559799}, {"id": 2243, "seek": 1135048, "start": 11358.0, "end": 11361.68, "text": " building an amazing product. So the vision starts by defining the problem.", "tokens": [50740, 2390, 364, 2243, 1674, 13, 407, 264, 5201, 3719, 538, 17827, 264, 1154, 13, 50924], "temperature": 0.0, "avg_logprob": -0.12404313706259691, "compression_ratio": 1.862190812720848, "no_speech_prob": 0.04740544408559799}, {"id": 2244, "seek": 1135048, "start": 11361.68, "end": 11365.92, "text": " And then you can work backwards in solving technology. And at Apple, like it's, I think,", "tokens": [50924, 400, 550, 291, 393, 589, 12204, 294, 12606, 2899, 13, 400, 412, 6373, 11, 411, 309, 311, 11, 286, 519, 11, 51136], "temperature": 0.0, "avg_logprob": -0.12404313706259691, "compression_ratio": 1.862190812720848, "no_speech_prob": 0.04740544408559799}, {"id": 2245, "seek": 1135048, "start": 11365.92, "end": 11371.68, "text": " pretty famously said that, you know, for every, you know, there's 100 no's for every yes.", "tokens": [51136, 1238, 34360, 848, 300, 11, 291, 458, 11, 337, 633, 11, 291, 458, 11, 456, 311, 2319, 572, 311, 337, 633, 2086, 13, 51424], "temperature": 0.0, "avg_logprob": -0.12404313706259691, "compression_ratio": 1.862190812720848, "no_speech_prob": 0.04740544408559799}, {"id": 2246, "seek": 1135048, "start": 11372.88, "end": 11378.32, "text": " I would refine that to say that there's 100 not yet for every yes. But famously, if you go back", "tokens": [51484, 286, 576, 33906, 300, 281, 584, 300, 456, 311, 2319, 406, 1939, 337, 633, 2086, 13, 583, 34360, 11, 498, 291, 352, 646, 51756], "temperature": 0.0, "avg_logprob": -0.12404313706259691, "compression_ratio": 1.862190812720848, "no_speech_prob": 0.04740544408559799}, {"id": 2247, "seek": 1137832, "start": 11378.32, "end": 11382.8, "text": " to the iPhone, for example, right, the iPhone one, I read, I mean, many people laughed at it", "tokens": [50364, 281, 264, 7252, 11, 337, 1365, 11, 558, 11, 264, 7252, 472, 11, 286, 1401, 11, 286, 914, 11, 867, 561, 20881, 412, 309, 50588], "temperature": 0.0, "avg_logprob": -0.14208525799690408, "compression_ratio": 1.867158671586716, "no_speech_prob": 0.009411274455487728}, {"id": 2248, "seek": 1137832, "start": 11382.8, "end": 11385.199999999999, "text": " because it didn't have 3G. It didn't have copy and paste.", "tokens": [50588, 570, 309, 994, 380, 362, 805, 38, 13, 467, 994, 380, 362, 5055, 293, 9163, 13, 50708], "temperature": 0.0, "avg_logprob": -0.14208525799690408, "compression_ratio": 1.867158671586716, "no_speech_prob": 0.009411274455487728}, {"id": 2249, "seek": 1137832, "start": 11386.56, "end": 11391.84, "text": " Right. And then a year later, okay, finally, it has 3G, but it still doesn't have copy and paste.", "tokens": [50776, 1779, 13, 400, 550, 257, 1064, 1780, 11, 1392, 11, 2721, 11, 309, 575, 805, 38, 11, 457, 309, 920, 1177, 380, 362, 5055, 293, 9163, 13, 51040], "temperature": 0.0, "avg_logprob": -0.14208525799690408, "compression_ratio": 1.867158671586716, "no_speech_prob": 0.009411274455487728}, {"id": 2250, "seek": 1137832, "start": 11391.84, "end": 11394.88, "text": " It's a joke. Nobody will ever use this product, blah, blah, blah, blah, blah, blah, blah,", "tokens": [51040, 467, 311, 257, 7647, 13, 9297, 486, 1562, 764, 341, 1674, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 12288, 11, 51192], "temperature": 0.0, "avg_logprob": -0.14208525799690408, "compression_ratio": 1.867158671586716, "no_speech_prob": 0.009411274455487728}, {"id": 2251, "seek": 1137832, "start": 11394.88, "end": 11399.6, "text": " blah, right? Well, your three had copy and paste and people stopped talking about it.", "tokens": [51192, 12288, 11, 558, 30, 1042, 11, 428, 1045, 632, 5055, 293, 9163, 293, 561, 5936, 1417, 466, 309, 13, 51428], "temperature": 0.0, "avg_logprob": -0.14208525799690408, "compression_ratio": 1.867158671586716, "no_speech_prob": 0.009411274455487728}, {"id": 2252, "seek": 1137832, "start": 11399.6, "end": 11404.96, "text": " Right. And so, and so being laser focused and having conviction and understanding", "tokens": [51428, 1779, 13, 400, 370, 11, 293, 370, 885, 12530, 5178, 293, 1419, 24837, 293, 3701, 51696], "temperature": 0.0, "avg_logprob": -0.14208525799690408, "compression_ratio": 1.867158671586716, "no_speech_prob": 0.009411274455487728}, {"id": 2253, "seek": 1140496, "start": 11404.96, "end": 11408.88, "text": " what the core problems are and giving the team the space to be able to build the right tech", "tokens": [50364, 437, 264, 4965, 2740, 366, 293, 2902, 264, 1469, 264, 1901, 281, 312, 1075, 281, 1322, 264, 558, 7553, 50560], "temperature": 0.0, "avg_logprob": -0.09727485690798078, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0043305568397045135}, {"id": 2254, "seek": 1140496, "start": 11409.439999999999, "end": 11414.56, "text": " is really important. Also, I mean, you come back to recruiting, you have to pay well.", "tokens": [50588, 307, 534, 1021, 13, 2743, 11, 286, 914, 11, 291, 808, 646, 281, 25987, 11, 291, 362, 281, 1689, 731, 13, 50844], "temperature": 0.0, "avg_logprob": -0.09727485690798078, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0043305568397045135}, {"id": 2255, "seek": 1140496, "start": 11415.599999999999, "end": 11418.56, "text": " Right. So we have to pay industry leading salaries and have good benefits and things", "tokens": [50896, 1779, 13, 407, 321, 362, 281, 1689, 3518, 5775, 35057, 293, 362, 665, 5311, 293, 721, 51044], "temperature": 0.0, "avg_logprob": -0.09727485690798078, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0043305568397045135}, {"id": 2256, "seek": 1140496, "start": 11418.56, "end": 11423.599999999999, "text": " like this. That's a big piece. We're a remote first company. And so we have to,", "tokens": [51044, 411, 341, 13, 663, 311, 257, 955, 2522, 13, 492, 434, 257, 8607, 700, 2237, 13, 400, 370, 321, 362, 281, 11, 51296], "temperature": 0.0, "avg_logprob": -0.09727485690798078, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0043305568397045135}, {"id": 2257, "seek": 1140496, "start": 11426.32, "end": 11432.96, "text": " so remote first has a very strong set of pros and cons. On the one hand, you can hire people", "tokens": [51432, 370, 8607, 700, 575, 257, 588, 2068, 992, 295, 6267, 293, 1014, 13, 1282, 264, 472, 1011, 11, 291, 393, 11158, 561, 51764], "temperature": 0.0, "avg_logprob": -0.09727485690798078, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0043305568397045135}, {"id": 2258, "seek": 1143296, "start": 11433.039999999999, "end": 11436.8, "text": " from wherever they are and you can attract amazing talent, even if they live in", "tokens": [50368, 490, 8660, 436, 366, 293, 291, 393, 5049, 2243, 8301, 11, 754, 498, 436, 1621, 294, 50556], "temperature": 0.0, "avg_logprob": -0.09314693668024326, "compression_ratio": 1.8362369337979094, "no_speech_prob": 0.008183596655726433}, {"id": 2259, "seek": 1143296, "start": 11436.8, "end": 11440.88, "text": " strange places or unusual places. On the other hand, you have time zones.", "tokens": [50556, 5861, 3190, 420, 10901, 3190, 13, 1282, 264, 661, 1011, 11, 291, 362, 565, 16025, 13, 50760], "temperature": 0.0, "avg_logprob": -0.09314693668024326, "compression_ratio": 1.8362369337979094, "no_speech_prob": 0.008183596655726433}, {"id": 2260, "seek": 1143296, "start": 11442.0, "end": 11447.039999999999, "text": " On the other hand, you have like everybody on the internet will fight if they don't understand", "tokens": [50816, 1282, 264, 661, 1011, 11, 291, 362, 411, 2201, 322, 264, 4705, 486, 2092, 498, 436, 500, 380, 1223, 51068], "temperature": 0.0, "avg_logprob": -0.09314693668024326, "compression_ratio": 1.8362369337979094, "no_speech_prob": 0.008183596655726433}, {"id": 2261, "seek": 1143296, "start": 11447.039999999999, "end": 11451.439999999999, "text": " each other. And so we've had to learn how to like have a system where we actually fly people in", "tokens": [51068, 1184, 661, 13, 400, 370, 321, 600, 632, 281, 1466, 577, 281, 411, 362, 257, 1185, 689, 321, 767, 3603, 561, 294, 51288], "temperature": 0.0, "avg_logprob": -0.09314693668024326, "compression_ratio": 1.8362369337979094, "no_speech_prob": 0.008183596655726433}, {"id": 2262, "seek": 1143296, "start": 11451.439999999999, "end": 11455.599999999999, "text": " and we get the whole company together periodically and then we get work groups together and we plan", "tokens": [51288, 293, 321, 483, 264, 1379, 2237, 1214, 38916, 293, 550, 321, 483, 589, 3935, 1214, 293, 321, 1393, 51496], "temperature": 0.0, "avg_logprob": -0.09314693668024326, "compression_ratio": 1.8362369337979094, "no_speech_prob": 0.008183596655726433}, {"id": 2263, "seek": 1143296, "start": 11455.599999999999, "end": 11459.839999999998, "text": " and execute together. And there's like an intimacy to the in-person brainstorming.", "tokens": [51496, 293, 14483, 1214, 13, 400, 456, 311, 411, 364, 34450, 281, 264, 294, 12, 10813, 35245, 278, 13, 51708], "temperature": 0.0, "avg_logprob": -0.09314693668024326, "compression_ratio": 1.8362369337979094, "no_speech_prob": 0.008183596655726433}, {"id": 2264, "seek": 1145984, "start": 11460.56, "end": 11464.56, "text": " Yeah, I guess you lose, but maybe you don't. Maybe if you get to know each other well and", "tokens": [50400, 865, 11, 286, 2041, 291, 3624, 11, 457, 1310, 291, 500, 380, 13, 2704, 498, 291, 483, 281, 458, 1184, 661, 731, 293, 50600], "temperature": 0.0, "avg_logprob": -0.14758100229151108, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.007008741609752178}, {"id": 2265, "seek": 1145984, "start": 11464.56, "end": 11468.24, "text": " you trust each other, maybe you can do that. Yeah. Well, so when the pandemic first hit,", "tokens": [50600, 291, 3361, 1184, 661, 11, 1310, 291, 393, 360, 300, 13, 865, 13, 1042, 11, 370, 562, 264, 5388, 700, 2045, 11, 50784], "temperature": 0.0, "avg_logprob": -0.14758100229151108, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.007008741609752178}, {"id": 2266, "seek": 1145984, "start": 11468.24, "end": 11472.64, "text": " I mean, I'm curious about your experience too. The first thing I missed was having whiteboards.", "tokens": [50784, 286, 914, 11, 286, 478, 6369, 466, 428, 1752, 886, 13, 440, 700, 551, 286, 6721, 390, 1419, 2418, 17228, 13, 51004], "temperature": 0.0, "avg_logprob": -0.14758100229151108, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.007008741609752178}, {"id": 2267, "seek": 1145984, "start": 11472.64, "end": 11479.04, "text": " Yeah. Right. Those design discussions are like, I can high intensity work through things, get", "tokens": [51004, 865, 13, 1779, 13, 3950, 1715, 11088, 366, 411, 11, 286, 393, 1090, 13749, 589, 807, 721, 11, 483, 51324], "temperature": 0.0, "avg_logprob": -0.14758100229151108, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.007008741609752178}, {"id": 2268, "seek": 1145984, "start": 11479.04, "end": 11482.880000000001, "text": " things done, work through the problem of the day, understand where you're on, figure out and solve", "tokens": [51324, 721, 1096, 11, 589, 807, 264, 1154, 295, 264, 786, 11, 1223, 689, 291, 434, 322, 11, 2573, 484, 293, 5039, 51516], "temperature": 0.0, "avg_logprob": -0.14758100229151108, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.007008741609752178}, {"id": 2269, "seek": 1145984, "start": 11482.880000000001, "end": 11489.36, "text": " the problem and move forward. But we figured out ways to work around that now with", "tokens": [51516, 264, 1154, 293, 1286, 2128, 13, 583, 321, 8932, 484, 2098, 281, 589, 926, 300, 586, 365, 51840], "temperature": 0.0, "avg_logprob": -0.14758100229151108, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.007008741609752178}, {"id": 2270, "seek": 1148984, "start": 11490.08, "end": 11494.48, "text": " all these screen sharing and other things like that that we do. The thing I miss now", "tokens": [50376, 439, 613, 2568, 5414, 293, 661, 721, 411, 300, 300, 321, 360, 13, 440, 551, 286, 1713, 586, 50596], "temperature": 0.0, "avg_logprob": -0.13338874919073923, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.000731967855244875}, {"id": 2271, "seek": 1148984, "start": 11494.48, "end": 11501.6, "text": " is sitting down at a lunch table with the team, the spontaneous things like the coffee bar", "tokens": [50596, 307, 3798, 760, 412, 257, 6349, 3199, 365, 264, 1469, 11, 264, 32744, 721, 411, 264, 4982, 2159, 50952], "temperature": 0.0, "avg_logprob": -0.13338874919073923, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.000731967855244875}, {"id": 2272, "seek": 1148984, "start": 11501.6, "end": 11506.4, "text": " things and the bumping into each other and getting to know people outside of the transactional", "tokens": [50952, 721, 293, 264, 9961, 278, 666, 1184, 661, 293, 1242, 281, 458, 561, 2380, 295, 264, 46688, 1966, 51192], "temperature": 0.0, "avg_logprob": -0.13338874919073923, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.000731967855244875}, {"id": 2273, "seek": 1148984, "start": 11506.960000000001, "end": 11513.36, "text": " solve a problem over Zoom thing. And I think there's just a lot of stuff that I'm not an expert", "tokens": [51220, 5039, 257, 1154, 670, 13453, 551, 13, 400, 286, 519, 456, 311, 445, 257, 688, 295, 1507, 300, 286, 478, 406, 364, 5844, 51540], "temperature": 0.0, "avg_logprob": -0.13338874919073923, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.000731967855244875}, {"id": 2274, "seek": 1148984, "start": 11513.36, "end": 11517.68, "text": " at this. I don't know who is, hopefully there's some people, but there's stuff that somehow is", "tokens": [51540, 412, 341, 13, 286, 500, 380, 458, 567, 307, 11, 4696, 456, 311, 512, 561, 11, 457, 456, 311, 1507, 300, 6063, 307, 51756], "temperature": 0.0, "avg_logprob": -0.13338874919073923, "compression_ratio": 1.752851711026616, "no_speech_prob": 0.000731967855244875}, {"id": 2275, "seek": 1151768, "start": 11517.68, "end": 11524.24, "text": " missing on Zoom. Even with the whiteboard, if you look at that, if you have a room with one", "tokens": [50364, 5361, 322, 13453, 13, 2754, 365, 264, 2418, 3787, 11, 498, 291, 574, 412, 300, 11, 498, 291, 362, 257, 1808, 365, 472, 50692], "temperature": 0.0, "avg_logprob": -0.16516508102416994, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.00490458682179451}, {"id": 2276, "seek": 1151768, "start": 11524.24, "end": 11531.12, "text": " person at the whiteboard and there's like three other people at a table, there's a, first of all,", "tokens": [50692, 954, 412, 264, 2418, 3787, 293, 456, 311, 411, 1045, 661, 561, 412, 257, 3199, 11, 456, 311, 257, 11, 700, 295, 439, 11, 51036], "temperature": 0.0, "avg_logprob": -0.16516508102416994, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.00490458682179451}, {"id": 2277, "seek": 1151768, "start": 11531.12, "end": 11535.68, "text": " there's a social aspect of that where you're just shooting the shit a little bit, almost like.", "tokens": [51036, 456, 311, 257, 2093, 4171, 295, 300, 689, 291, 434, 445, 5942, 264, 4611, 257, 707, 857, 11, 1920, 411, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16516508102416994, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.00490458682179451}, {"id": 2278, "seek": 1151768, "start": 11535.68, "end": 11542.32, "text": " Yeah. As people just kind of coming in and yeah, that, but also while like it's a breakout", "tokens": [51264, 865, 13, 1018, 561, 445, 733, 295, 1348, 294, 293, 1338, 11, 300, 11, 457, 611, 1339, 411, 309, 311, 257, 30067, 51596], "temperature": 0.0, "avg_logprob": -0.16516508102416994, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.00490458682179451}, {"id": 2279, "seek": 1154232, "start": 11542.32, "end": 11548.24, "text": " discussion that happens for like seconds at a time, maybe an inside joke or it's like this", "tokens": [50364, 5017, 300, 2314, 337, 411, 3949, 412, 257, 565, 11, 1310, 364, 1854, 7647, 420, 309, 311, 411, 341, 50660], "temperature": 0.0, "avg_logprob": -0.14792526315111632, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.17098313570022583}, {"id": 2280, "seek": 1154232, "start": 11548.24, "end": 11552.24, "text": " interesting dynamic that happens that Zoom. And you're bonding. Yeah. You're bonding. You're", "tokens": [50660, 1880, 8546, 300, 2314, 300, 13453, 13, 400, 291, 434, 28824, 13, 865, 13, 509, 434, 28824, 13, 509, 434, 50860], "temperature": 0.0, "avg_logprob": -0.14792526315111632, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.17098313570022583}, {"id": 2281, "seek": 1154232, "start": 11552.24, "end": 11556.96, "text": " bonding, but through that bonding, you get the excitement. There's certain ideas that are like", "tokens": [50860, 28824, 11, 457, 807, 300, 28824, 11, 291, 483, 264, 14755, 13, 821, 311, 1629, 3487, 300, 366, 411, 51096], "temperature": 0.0, "avg_logprob": -0.14792526315111632, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.17098313570022583}, {"id": 2282, "seek": 1154232, "start": 11557.6, "end": 11563.199999999999, "text": " complete bullshit and you'll see that in the faces of others that you won't see necessarily on Zoom.", "tokens": [51128, 3566, 22676, 293, 291, 603, 536, 300, 294, 264, 8475, 295, 2357, 300, 291, 1582, 380, 536, 4725, 322, 13453, 13, 51408], "temperature": 0.0, "avg_logprob": -0.14792526315111632, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.17098313570022583}, {"id": 2283, "seek": 1154232, "start": 11564.16, "end": 11570.64, "text": " And like something, it feels like that should be possible to do without being in person.", "tokens": [51456, 400, 411, 746, 11, 309, 3417, 411, 300, 820, 312, 1944, 281, 360, 1553, 885, 294, 954, 13, 51780], "temperature": 0.0, "avg_logprob": -0.14792526315111632, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.17098313570022583}, {"id": 2284, "seek": 1157064, "start": 11570.64, "end": 11576.32, "text": " Well, I mean, being in person is a very different thing. It's worth it, but you can't always do it.", "tokens": [50364, 1042, 11, 286, 914, 11, 885, 294, 954, 307, 257, 588, 819, 551, 13, 467, 311, 3163, 309, 11, 457, 291, 393, 380, 1009, 360, 309, 13, 50648], "temperature": 0.0, "avg_logprob": -0.1200001365260074, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.0015974033158272505}, {"id": 2285, "seek": 1157064, "start": 11576.32, "end": 11582.24, "text": " And so again, we're still learning and we're also learning as like humanity with this new reality,", "tokens": [50648, 400, 370, 797, 11, 321, 434, 920, 2539, 293, 321, 434, 611, 2539, 382, 411, 10243, 365, 341, 777, 4103, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1200001365260074, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.0015974033158272505}, {"id": 2286, "seek": 1157064, "start": 11582.24, "end": 11587.199999999999, "text": " right? But what we found is that getting people together, whether it be a team or the whole", "tokens": [50944, 558, 30, 583, 437, 321, 1352, 307, 300, 1242, 561, 1214, 11, 1968, 309, 312, 257, 1469, 420, 264, 1379, 51192], "temperature": 0.0, "avg_logprob": -0.1200001365260074, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.0015974033158272505}, {"id": 2287, "seek": 1157064, "start": 11587.199999999999, "end": 11591.76, "text": " company or whatever, is worth the expense because people work together and are happier", "tokens": [51192, 2237, 420, 2035, 11, 307, 3163, 264, 18406, 570, 561, 589, 1214, 293, 366, 20423, 51420], "temperature": 0.0, "avg_logprob": -0.1200001365260074, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.0015974033158272505}, {"id": 2288, "seek": 1157064, "start": 11592.64, "end": 11597.92, "text": " after that. Like there's a massive period of time where you go out and things start getting", "tokens": [51464, 934, 300, 13, 1743, 456, 311, 257, 5994, 2896, 295, 565, 689, 291, 352, 484, 293, 721, 722, 1242, 51728], "temperature": 0.0, "avg_logprob": -0.1200001365260074, "compression_ratio": 1.6690391459074734, "no_speech_prob": 0.0015974033158272505}, {"id": 2289, "seek": 1159792, "start": 11597.92, "end": 11602.56, "text": " frayed, pull people together, and then you realize that we're all working together. We see things the", "tokens": [50364, 431, 47315, 11, 2235, 561, 1214, 11, 293, 550, 291, 4325, 300, 321, 434, 439, 1364, 1214, 13, 492, 536, 721, 264, 50596], "temperature": 0.0, "avg_logprob": -0.12735523647732205, "compression_ratio": 1.8274760383386581, "no_speech_prob": 0.025939028710126877}, {"id": 2290, "seek": 1159792, "start": 11602.56, "end": 11606.24, "text": " same way. We work through the disagreement or the misunderstanding. We're talking across each other", "tokens": [50596, 912, 636, 13, 492, 589, 807, 264, 38947, 420, 264, 29227, 13, 492, 434, 1417, 2108, 1184, 661, 50780], "temperature": 0.0, "avg_logprob": -0.12735523647732205, "compression_ratio": 1.8274760383386581, "no_speech_prob": 0.025939028710126877}, {"id": 2291, "seek": 1159792, "start": 11606.24, "end": 11610.64, "text": " and then you work much better together. And so things like that, I think are really quite important.", "tokens": [50780, 293, 550, 291, 589, 709, 1101, 1214, 13, 400, 370, 721, 411, 300, 11, 286, 519, 366, 534, 1596, 1021, 13, 51000], "temperature": 0.0, "avg_logprob": -0.12735523647732205, "compression_ratio": 1.8274760383386581, "no_speech_prob": 0.025939028710126877}, {"id": 2292, "seek": 1159792, "start": 11610.64, "end": 11615.84, "text": " What about people that are kind of specialized in very different aspects of the stack working", "tokens": [51000, 708, 466, 561, 300, 366, 733, 295, 19813, 294, 588, 819, 7270, 295, 264, 8630, 1364, 51260], "temperature": 0.0, "avg_logprob": -0.12735523647732205, "compression_ratio": 1.8274760383386581, "no_speech_prob": 0.025939028710126877}, {"id": 2293, "seek": 1159792, "start": 11615.84, "end": 11619.6, "text": " together? What are some interesting challenges there? Yeah. Well, so I mean, I mean, there's", "tokens": [51260, 1214, 30, 708, 366, 512, 1880, 4759, 456, 30, 865, 13, 1042, 11, 370, 286, 914, 11, 286, 914, 11, 456, 311, 51448], "temperature": 0.0, "avg_logprob": -0.12735523647732205, "compression_ratio": 1.8274760383386581, "no_speech_prob": 0.025939028710126877}, {"id": 2294, "seek": 1159792, "start": 11619.6, "end": 11622.72, "text": " lots of interesting people, as you can tell, I'm, you know, hard to deal with too.", "tokens": [51448, 3195, 295, 1880, 561, 11, 382, 291, 393, 980, 11, 286, 478, 11, 291, 458, 11, 1152, 281, 2028, 365, 886, 13, 51604], "temperature": 0.0, "avg_logprob": -0.12735523647732205, "compression_ratio": 1.8274760383386581, "no_speech_prob": 0.025939028710126877}, {"id": 2295, "seek": 1162272, "start": 11623.439999999999, "end": 11630.08, "text": " You're one of the most lovable people. So one of the, so there's different", "tokens": [50400, 509, 434, 472, 295, 264, 881, 450, 17915, 561, 13, 407, 472, 295, 264, 11, 370, 456, 311, 819, 50732], "temperature": 0.0, "avg_logprob": -0.16069112505231584, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.020324690267443657}, {"id": 2296, "seek": 1162272, "start": 11630.08, "end": 11635.92, "text": " philosophies in building teams. For me, and so some people say, hire 10x programmers,", "tokens": [50732, 14529, 530, 294, 2390, 5491, 13, 1171, 385, 11, 293, 370, 512, 561, 584, 11, 11158, 1266, 87, 41504, 11, 51024], "temperature": 0.0, "avg_logprob": -0.16069112505231584, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.020324690267443657}, {"id": 2297, "seek": 1162272, "start": 11635.92, "end": 11640.48, "text": " and that's the only thing that whatever that means, right? What I believe in is building", "tokens": [51024, 293, 300, 311, 264, 787, 551, 300, 2035, 300, 1355, 11, 558, 30, 708, 286, 1697, 294, 307, 2390, 51252], "temperature": 0.0, "avg_logprob": -0.16069112505231584, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.020324690267443657}, {"id": 2298, "seek": 1162272, "start": 11640.48, "end": 11646.64, "text": " well balanced teams. Teams that have people that are different in them. Like if you have all generals", "tokens": [51252, 731, 13902, 5491, 13, 24702, 300, 362, 561, 300, 366, 819, 294, 552, 13, 1743, 498, 291, 362, 439, 41346, 51560], "temperature": 0.0, "avg_logprob": -0.16069112505231584, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.020324690267443657}, {"id": 2299, "seek": 1162272, "start": 11646.64, "end": 11652.24, "text": " and no troops, or all troops and no generals, or you have all people that think in one way,", "tokens": [51560, 293, 572, 11522, 11, 420, 439, 11522, 293, 572, 41346, 11, 420, 291, 362, 439, 561, 300, 519, 294, 472, 636, 11, 51840], "temperature": 0.0, "avg_logprob": -0.16069112505231584, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.020324690267443657}, {"id": 2300, "seek": 1165224, "start": 11652.24, "end": 11656.4, "text": " and not the other way, what you get is you get a very biased and skewed and weird situation where", "tokens": [50364, 293, 406, 264, 661, 636, 11, 437, 291, 483, 307, 291, 483, 257, 588, 28035, 293, 8756, 26896, 293, 3657, 2590, 689, 50572], "temperature": 0.0, "avg_logprob": -0.08315977167200159, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.00348263350315392}, {"id": 2301, "seek": 1165224, "start": 11656.4, "end": 11660.72, "text": " people end up being unhappy. And so what I like to do is I like to build teams of people where", "tokens": [50572, 561, 917, 493, 885, 22172, 13, 400, 370, 437, 286, 411, 281, 360, 307, 286, 411, 281, 1322, 5491, 295, 561, 689, 50788], "temperature": 0.0, "avg_logprob": -0.08315977167200159, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.00348263350315392}, {"id": 2302, "seek": 1165224, "start": 11660.72, "end": 11666.16, "text": " they're not all the same. You know, we do have teams that are focused on like runtime or compiler,", "tokens": [50788, 436, 434, 406, 439, 264, 912, 13, 509, 458, 11, 321, 360, 362, 5491, 300, 366, 5178, 322, 411, 34474, 420, 31958, 11, 51060], "temperature": 0.0, "avg_logprob": -0.08315977167200159, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.00348263350315392}, {"id": 2303, "seek": 1165224, "start": 11666.16, "end": 11671.76, "text": " GPU or whatever the specialty is, but people bring a different take and have a different", "tokens": [51060, 18407, 420, 2035, 264, 22000, 307, 11, 457, 561, 1565, 257, 819, 747, 293, 362, 257, 819, 51340], "temperature": 0.0, "avg_logprob": -0.08315977167200159, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.00348263350315392}, {"id": 2304, "seek": 1165224, "start": 11671.76, "end": 11676.32, "text": " perspective. And I look for people that complement each other. And particularly if you look at", "tokens": [51340, 4585, 13, 400, 286, 574, 337, 561, 300, 17103, 1184, 661, 13, 400, 4098, 498, 291, 574, 412, 51568], "temperature": 0.0, "avg_logprob": -0.08315977167200159, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.00348263350315392}, {"id": 2305, "seek": 1165224, "start": 11676.32, "end": 11680.96, "text": " leadership teams and things like this, you don't want everybody thinking the same way. You want", "tokens": [51568, 5848, 5491, 293, 721, 411, 341, 11, 291, 500, 380, 528, 2201, 1953, 264, 912, 636, 13, 509, 528, 51800], "temperature": 0.0, "avg_logprob": -0.08315977167200159, "compression_ratio": 1.8069620253164558, "no_speech_prob": 0.00348263350315392}, {"id": 2306, "seek": 1168096, "start": 11680.96, "end": 11685.359999999999, "text": " people bringing different perspectives and experiences. And so I think that's really important.", "tokens": [50364, 561, 5062, 819, 16766, 293, 5235, 13, 400, 370, 286, 519, 300, 311, 534, 1021, 13, 50584], "temperature": 0.0, "avg_logprob": -0.15960505734319272, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.00757430586963892}, {"id": 2307, "seek": 1168096, "start": 11685.359999999999, "end": 11690.96, "text": " That's team, but what about building a company as ambitious as modular? So what are some", "tokens": [50584, 663, 311, 1469, 11, 457, 437, 466, 2390, 257, 2237, 382, 20239, 382, 31111, 30, 407, 437, 366, 512, 50864], "temperature": 0.0, "avg_logprob": -0.15960505734319272, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.00757430586963892}, {"id": 2308, "seek": 1168096, "start": 11690.96, "end": 11697.439999999999, "text": " interesting questions there? Oh, I mean, so many. Like, so one of the things I love about, okay, so", "tokens": [50864, 1880, 1651, 456, 30, 876, 11, 286, 914, 11, 370, 867, 13, 1743, 11, 370, 472, 295, 264, 721, 286, 959, 466, 11, 1392, 11, 370, 51188], "temperature": 0.0, "avg_logprob": -0.15960505734319272, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.00757430586963892}, {"id": 2309, "seek": 1168096, "start": 11697.439999999999, "end": 11705.519999999999, "text": " modular is the first company I built from scratch. One of the first things that was profound was I'm", "tokens": [51188, 31111, 307, 264, 700, 2237, 286, 3094, 490, 8459, 13, 1485, 295, 264, 700, 721, 300, 390, 14382, 390, 286, 478, 51592], "temperature": 0.0, "avg_logprob": -0.15960505734319272, "compression_ratio": 1.5778688524590163, "no_speech_prob": 0.00757430586963892}, {"id": 2310, "seek": 1170552, "start": 11705.52, "end": 11710.880000000001, "text": " not cleaning up somebody else's mess. Right. And so if you look at that's liberating to some degree.", "tokens": [50364, 406, 8924, 493, 2618, 1646, 311, 2082, 13, 1779, 13, 400, 370, 498, 291, 574, 412, 300, 311, 6774, 990, 281, 512, 4314, 13, 50632], "temperature": 0.0, "avg_logprob": -0.16349210833558941, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.04740465432405472}, {"id": 2311, "seek": 1170552, "start": 11710.880000000001, "end": 11717.52, "text": " It's super liberating. And also, many of the projects I've built in the past have not been", "tokens": [50632, 467, 311, 1687, 6774, 990, 13, 400, 611, 11, 867, 295, 264, 4455, 286, 600, 3094, 294, 264, 1791, 362, 406, 668, 50964], "temperature": 0.0, "avg_logprob": -0.16349210833558941, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.04740465432405472}, {"id": 2312, "seek": 1170552, "start": 11717.52, "end": 11726.16, "text": " core to the product of the company. Swift is not Apple's product, right? MLIR is not Google's", "tokens": [50964, 4965, 281, 264, 1674, 295, 264, 2237, 13, 25539, 307, 406, 6373, 311, 1674, 11, 558, 30, 21601, 7740, 307, 406, 3329, 311, 51396], "temperature": 0.0, "avg_logprob": -0.16349210833558941, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.04740465432405472}, {"id": 2313, "seek": 1170552, "start": 11726.16, "end": 11731.28, "text": " revenue machine or whatever, right? It's not, it's important. But it's like working on the", "tokens": [51396, 9324, 3479, 420, 2035, 11, 558, 30, 467, 311, 406, 11, 309, 311, 1021, 13, 583, 309, 311, 411, 1364, 322, 264, 51652], "temperature": 0.0, "avg_logprob": -0.16349210833558941, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.04740465432405472}, {"id": 2314, "seek": 1173128, "start": 11731.28, "end": 11737.76, "text": " accounting software for, you know, the retail giant or something, right? It's like enabling", "tokens": [50364, 19163, 4722, 337, 11, 291, 458, 11, 264, 10800, 7410, 420, 746, 11, 558, 30, 467, 311, 411, 23148, 50688], "temperature": 0.0, "avg_logprob": -0.08094968293842517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.007814498618245125}, {"id": 2315, "seek": 1173128, "start": 11737.76, "end": 11744.400000000001, "text": " infrastructure and technology. And so at modular, the tech we're building is here to solve people's", "tokens": [50688, 6896, 293, 2899, 13, 400, 370, 412, 31111, 11, 264, 7553, 321, 434, 2390, 307, 510, 281, 5039, 561, 311, 51020], "temperature": 0.0, "avg_logprob": -0.08094968293842517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.007814498618245125}, {"id": 2316, "seek": 1173128, "start": 11744.400000000001, "end": 11749.12, "text": " problems. Like it is directly the thing we're giving to people. And so this is a really big", "tokens": [51020, 2740, 13, 1743, 309, 307, 3838, 264, 551, 321, 434, 2902, 281, 561, 13, 400, 370, 341, 307, 257, 534, 955, 51256], "temperature": 0.0, "avg_logprob": -0.08094968293842517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.007814498618245125}, {"id": 2317, "seek": 1173128, "start": 11749.12, "end": 11753.68, "text": " difference. And what it means for me as a leader, but also for many of our engineers is they're", "tokens": [51256, 2649, 13, 400, 437, 309, 1355, 337, 385, 382, 257, 5263, 11, 457, 611, 337, 867, 295, 527, 11955, 307, 436, 434, 51484], "temperature": 0.0, "avg_logprob": -0.08094968293842517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.007814498618245125}, {"id": 2318, "seek": 1173128, "start": 11753.68, "end": 11758.560000000001, "text": " working on the thing that matters. And that's actually pretty, I mean, again, for compiler", "tokens": [51484, 1364, 322, 264, 551, 300, 7001, 13, 400, 300, 311, 767, 1238, 11, 286, 914, 11, 797, 11, 337, 31958, 51728], "temperature": 0.0, "avg_logprob": -0.08094968293842517, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.007814498618245125}, {"id": 2319, "seek": 1175856, "start": 11758.56, "end": 11763.439999999999, "text": " people and things like that, that's usually not the case, right? And so that's also pretty exciting", "tokens": [50364, 561, 293, 721, 411, 300, 11, 300, 311, 2673, 406, 264, 1389, 11, 558, 30, 400, 370, 300, 311, 611, 1238, 4670, 50608], "temperature": 0.0, "avg_logprob": -0.10578621848154876, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.029304293915629387}, {"id": 2320, "seek": 1175856, "start": 11763.439999999999, "end": 11770.4, "text": " and quite nice. But one of the ways that this manifests is it makes it easier to make decisions.", "tokens": [50608, 293, 1596, 1481, 13, 583, 472, 295, 264, 2098, 300, 341, 50252, 307, 309, 1669, 309, 3571, 281, 652, 5327, 13, 50956], "temperature": 0.0, "avg_logprob": -0.10578621848154876, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.029304293915629387}, {"id": 2321, "seek": 1175856, "start": 11771.199999999999, "end": 11774.72, "text": " And so one of the challenges I've had in other worlds is it's like, okay, well,", "tokens": [50996, 400, 370, 472, 295, 264, 4759, 286, 600, 632, 294, 661, 13401, 307, 309, 311, 411, 11, 1392, 11, 731, 11, 51172], "temperature": 0.0, "avg_logprob": -0.10578621848154876, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.029304293915629387}, {"id": 2322, "seek": 1175856, "start": 11775.439999999999, "end": 11781.039999999999, "text": " community matters somehow for the goodness of the world, like, or open-source matters", "tokens": [51208, 1768, 7001, 6063, 337, 264, 8387, 295, 264, 1002, 11, 411, 11, 420, 1269, 12, 41676, 7001, 51488], "temperature": 0.0, "avg_logprob": -0.10578621848154876, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.029304293915629387}, {"id": 2323, "seek": 1175856, "start": 11781.039999999999, "end": 11787.039999999999, "text": " theoretically, but I don't want to pay for a t-shirt, right? Or some swag. Like, well,", "tokens": [51488, 29400, 11, 457, 286, 500, 380, 528, 281, 1689, 337, 257, 256, 12, 15313, 11, 558, 30, 1610, 512, 42064, 13, 1743, 11, 731, 11, 51788], "temperature": 0.0, "avg_logprob": -0.10578621848154876, "compression_ratio": 1.6816479400749065, "no_speech_prob": 0.029304293915629387}, {"id": 2324, "seek": 1178704, "start": 11787.04, "end": 11792.800000000001, "text": " t-shirts cost $10 each. You can have 100 t-shirts for $1,000 to a mega-corp. $1,000 is", "tokens": [50364, 256, 12, 25892, 2063, 1848, 3279, 1184, 13, 509, 393, 362, 2319, 256, 12, 25892, 337, 1848, 16, 11, 1360, 281, 257, 17986, 12, 66, 18703, 13, 1848, 16, 11, 1360, 307, 50652], "temperature": 0.0, "avg_logprob": -0.1521056377104599, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.012429279275238514}, {"id": 2325, "seek": 1178704, "start": 11793.68, "end": 11799.12, "text": " uncountably, can't count that low, right? But justifying it and getting a t-shirt,", "tokens": [50696, 6219, 792, 1188, 11, 393, 380, 1207, 300, 2295, 11, 558, 30, 583, 445, 5489, 309, 293, 1242, 257, 256, 12, 15313, 11, 50968], "temperature": 0.0, "avg_logprob": -0.1521056377104599, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.012429279275238514}, {"id": 2326, "seek": 1178704, "start": 11799.12, "end": 11801.04, "text": " by the way, if you'd like a t-shirt, I can give you a t-shirt.", "tokens": [50968, 538, 264, 636, 11, 498, 291, 1116, 411, 257, 256, 12, 15313, 11, 286, 393, 976, 291, 257, 256, 12, 15313, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1521056377104599, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.012429279275238514}, {"id": 2327, "seek": 1178704, "start": 11801.04, "end": 11805.12, "text": " Well, I would 100% like a t-shirt. Are you joking?", "tokens": [51064, 1042, 11, 286, 576, 2319, 4, 411, 257, 256, 12, 15313, 13, 2014, 291, 17396, 30, 51268], "temperature": 0.0, "avg_logprob": -0.1521056377104599, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.012429279275238514}, {"id": 2328, "seek": 1178704, "start": 11805.12, "end": 11810.720000000001, "text": " You can have a fire emoji t-shirt. I will treasure this.", "tokens": [51268, 509, 393, 362, 257, 2610, 31595, 256, 12, 15313, 13, 286, 486, 12985, 341, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1521056377104599, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.012429279275238514}, {"id": 2329, "seek": 1178704, "start": 11810.720000000001, "end": 11813.12, "text": " Is that a good thing? I will pass it down to my grandchildren.", "tokens": [51548, 1119, 300, 257, 665, 551, 30, 286, 486, 1320, 309, 760, 281, 452, 28112, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1521056377104599, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.012429279275238514}, {"id": 2330, "seek": 1181312, "start": 11813.12, "end": 11817.2, "text": " And so it's very liberating to be able to decide, I think that Lex should have a t-shirt,", "tokens": [50364, 400, 370, 309, 311, 588, 6774, 990, 281, 312, 1075, 281, 4536, 11, 286, 519, 300, 24086, 820, 362, 257, 256, 12, 15313, 11, 50568], "temperature": 0.0, "avg_logprob": -0.1526044677285587, "compression_ratio": 1.381720430107527, "no_speech_prob": 0.0035372013226151466}, {"id": 2331, "seek": 1181312, "start": 11818.800000000001, "end": 11822.08, "text": " right? And it becomes very simple because I like Lex.", "tokens": [50648, 558, 30, 400, 309, 3643, 588, 2199, 570, 286, 411, 24086, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1526044677285587, "compression_ratio": 1.381720430107527, "no_speech_prob": 0.0035372013226151466}, {"id": 2332, "seek": 1181312, "start": 11824.800000000001, "end": 11831.84, "text": " This is awesome. So I have to ask you about the...", "tokens": [50948, 639, 307, 3476, 13, 407, 286, 362, 281, 1029, 291, 466, 264, 485, 51300], "temperature": 0.0, "avg_logprob": -0.1526044677285587, "compression_ratio": 1.381720430107527, "no_speech_prob": 0.0035372013226151466}, {"id": 2333, "seek": 1181312, "start": 11833.84, "end": 11836.720000000001, "text": " One of the interesting developments with large language models", "tokens": [51400, 1485, 295, 264, 1880, 20862, 365, 2416, 2856, 5245, 51544], "temperature": 0.0, "avg_logprob": -0.1526044677285587, "compression_ratio": 1.381720430107527, "no_speech_prob": 0.0035372013226151466}, {"id": 2334, "seek": 1183672, "start": 11836.96, "end": 11843.76, "text": " is that they're able to generate code recently really well.", "tokens": [50376, 307, 300, 436, 434, 1075, 281, 8460, 3089, 3938, 534, 731, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1554113045716897, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.036724481731653214}, {"id": 2335, "seek": 1183672, "start": 11845.039999999999, "end": 11845.599999999999, "text": " Yes.", "tokens": [50780, 1079, 13, 50808], "temperature": 0.0, "avg_logprob": -0.1554113045716897, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.036724481731653214}, {"id": 2336, "seek": 1183672, "start": 11845.599999999999, "end": 11851.76, "text": " To a degree that maybe I don't know if you understand, but I have... I struggle to", "tokens": [50808, 1407, 257, 4314, 300, 1310, 286, 500, 380, 458, 498, 291, 1223, 11, 457, 286, 362, 485, 286, 7799, 281, 51116], "temperature": 0.0, "avg_logprob": -0.1554113045716897, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.036724481731653214}, {"id": 2337, "seek": 1183672, "start": 11851.76, "end": 11856.72, "text": " understand because it forces me to ask questions about the nature of programming,", "tokens": [51116, 1223, 570, 309, 5874, 385, 281, 1029, 1651, 466, 264, 3687, 295, 9410, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1554113045716897, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.036724481731653214}, {"id": 2338, "seek": 1183672, "start": 11856.72, "end": 11863.359999999999, "text": " of the nature of thought, because the language models are able to predict the kind of code", "tokens": [51364, 295, 264, 3687, 295, 1194, 11, 570, 264, 2856, 5245, 366, 1075, 281, 6069, 264, 733, 295, 3089, 51696], "temperature": 0.0, "avg_logprob": -0.1554113045716897, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.036724481731653214}, {"id": 2339, "seek": 1186336, "start": 11863.36, "end": 11868.960000000001, "text": " I was about to write so well that it makes me wonder how unique my brain is and where the", "tokens": [50364, 286, 390, 466, 281, 2464, 370, 731, 300, 309, 1669, 385, 2441, 577, 3845, 452, 3567, 307, 293, 689, 264, 50644], "temperature": 0.0, "avg_logprob": -0.13243768926252397, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.014046762138605118}, {"id": 2340, "seek": 1186336, "start": 11868.960000000001, "end": 11876.560000000001, "text": " valuable ideas actually come from. How much do I contribute in terms of ingenuity, innovation,", "tokens": [50644, 8263, 3487, 767, 808, 490, 13, 1012, 709, 360, 286, 10586, 294, 2115, 295, 21600, 21757, 11, 8504, 11, 51024], "temperature": 0.0, "avg_logprob": -0.13243768926252397, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.014046762138605118}, {"id": 2341, "seek": 1186336, "start": 11877.12, "end": 11883.12, "text": " to code, I write, or design, and that kind of stuff. When you stand on the shoulders of giants,", "tokens": [51052, 281, 3089, 11, 286, 2464, 11, 420, 1715, 11, 293, 300, 733, 295, 1507, 13, 1133, 291, 1463, 322, 264, 10245, 295, 31894, 11, 51352], "temperature": 0.0, "avg_logprob": -0.13243768926252397, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.014046762138605118}, {"id": 2342, "seek": 1186336, "start": 11883.12, "end": 11888.400000000001, "text": " are you really doing anything? And what LLMs are helping you do is they help you stand on", "tokens": [51352, 366, 291, 534, 884, 1340, 30, 400, 437, 441, 43, 26386, 366, 4315, 291, 360, 307, 436, 854, 291, 1463, 322, 51616], "temperature": 0.0, "avg_logprob": -0.13243768926252397, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.014046762138605118}, {"id": 2343, "seek": 1186336, "start": 11888.400000000001, "end": 11892.560000000001, "text": " the shoulders of giants in your program. There's mistakes. They're interesting that you learned", "tokens": [51616, 264, 10245, 295, 31894, 294, 428, 1461, 13, 821, 311, 8038, 13, 814, 434, 1880, 300, 291, 3264, 51824], "temperature": 0.0, "avg_logprob": -0.13243768926252397, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.014046762138605118}, {"id": 2344, "seek": 1189256, "start": 11892.56, "end": 11899.279999999999, "text": " from, but I would love to get your opinion first high-level of what you think about this", "tokens": [50364, 490, 11, 457, 286, 576, 959, 281, 483, 428, 4800, 700, 1090, 12, 12418, 295, 437, 291, 519, 466, 341, 50700], "temperature": 0.0, "avg_logprob": -0.13035308673817625, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0036472559440881014}, {"id": 2345, "seek": 1189256, "start": 11899.279999999999, "end": 11904.08, "text": " impact of large language models when they do program synthesis, when they generate code.", "tokens": [50700, 2712, 295, 2416, 2856, 5245, 562, 436, 360, 1461, 30252, 11, 562, 436, 8460, 3089, 13, 50940], "temperature": 0.0, "avg_logprob": -0.13035308673817625, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0036472559440881014}, {"id": 2346, "seek": 1189256, "start": 11907.039999999999, "end": 11913.279999999999, "text": " I don't know where it all goes. I'm an optimist and I'm a human optimist. I think that", "tokens": [51088, 286, 500, 380, 458, 689, 309, 439, 1709, 13, 286, 478, 364, 5028, 468, 293, 286, 478, 257, 1952, 5028, 468, 13, 286, 519, 300, 51400], "temperature": 0.0, "avg_logprob": -0.13035308673817625, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0036472559440881014}, {"id": 2347, "seek": 1189256, "start": 11914.0, "end": 11918.0, "text": " things I've seen are that a lot of the LLMs are really good at crushing leak code projects,", "tokens": [51436, 721, 286, 600, 1612, 366, 300, 257, 688, 295, 264, 441, 43, 26386, 366, 534, 665, 412, 31317, 17143, 3089, 4455, 11, 51636], "temperature": 0.0, "avg_logprob": -0.13035308673817625, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.0036472559440881014}, {"id": 2348, "seek": 1191800, "start": 11918.64, "end": 11922.88, "text": " and they can reverse the link list like crazy. Well, it turns out there's a lot of", "tokens": [50396, 293, 436, 393, 9943, 264, 2113, 1329, 411, 3219, 13, 1042, 11, 309, 4523, 484, 456, 311, 257, 688, 295, 50608], "temperature": 0.0, "avg_logprob": -0.1415004066798998, "compression_ratio": 1.7072243346007605, "no_speech_prob": 0.00538311991840601}, {"id": 2349, "seek": 1191800, "start": 11923.92, "end": 11927.92, "text": " instances of that on the internet, and it's a pretty stock thing. And so if you want to see", "tokens": [50660, 14519, 295, 300, 322, 264, 4705, 11, 293, 309, 311, 257, 1238, 4127, 551, 13, 400, 370, 498, 291, 528, 281, 536, 50860], "temperature": 0.0, "avg_logprob": -0.1415004066798998, "compression_ratio": 1.7072243346007605, "no_speech_prob": 0.00538311991840601}, {"id": 2350, "seek": 1191800, "start": 11928.96, "end": 11932.8, "text": " standard questions answered, LLMs can memorize all the answers, and that can be amazing.", "tokens": [50912, 3832, 1651, 10103, 11, 441, 43, 26386, 393, 27478, 439, 264, 6338, 11, 293, 300, 393, 312, 2243, 13, 51104], "temperature": 0.0, "avg_logprob": -0.1415004066798998, "compression_ratio": 1.7072243346007605, "no_speech_prob": 0.00538311991840601}, {"id": 2351, "seek": 1191800, "start": 11932.8, "end": 11938.48, "text": " And also, they do generalize out from that, and so there's good work on that. But I think that", "tokens": [51104, 400, 611, 11, 436, 360, 2674, 1125, 484, 490, 300, 11, 293, 370, 456, 311, 665, 589, 322, 300, 13, 583, 286, 519, 300, 51388], "temperature": 0.0, "avg_logprob": -0.1415004066798998, "compression_ratio": 1.7072243346007605, "no_speech_prob": 0.00538311991840601}, {"id": 2352, "seek": 1191800, "start": 11939.28, "end": 11943.68, "text": " in my experience, building things, building something like you talk about mojo or you talk", "tokens": [51428, 294, 452, 1752, 11, 2390, 721, 11, 2390, 746, 411, 291, 751, 466, 705, 5134, 420, 291, 751, 51648], "temperature": 0.0, "avg_logprob": -0.1415004066798998, "compression_ratio": 1.7072243346007605, "no_speech_prob": 0.00538311991840601}, {"id": 2353, "seek": 1194368, "start": 11943.76, "end": 11948.64, "text": " about these things or you talk about building an applied solution to a problem, it's also about", "tokens": [50368, 466, 613, 721, 420, 291, 751, 466, 2390, 364, 6456, 3827, 281, 257, 1154, 11, 309, 311, 611, 466, 50612], "temperature": 0.0, "avg_logprob": -0.092989481932728, "compression_ratio": 1.7763578274760383, "no_speech_prob": 0.013631363399326801}, {"id": 2354, "seek": 1194368, "start": 11948.64, "end": 11952.4, "text": " working with people. It's about understanding the problem. What is the product that you want to", "tokens": [50612, 1364, 365, 561, 13, 467, 311, 466, 3701, 264, 1154, 13, 708, 307, 264, 1674, 300, 291, 528, 281, 50800], "temperature": 0.0, "avg_logprob": -0.092989481932728, "compression_ratio": 1.7763578274760383, "no_speech_prob": 0.013631363399326801}, {"id": 2355, "seek": 1194368, "start": 11952.4, "end": 11956.24, "text": " build? What are the use case? What are the customers? You can't just go survey all the", "tokens": [50800, 1322, 30, 708, 366, 264, 764, 1389, 30, 708, 366, 264, 4581, 30, 509, 393, 380, 445, 352, 8984, 439, 264, 50992], "temperature": 0.0, "avg_logprob": -0.092989481932728, "compression_ratio": 1.7763578274760383, "no_speech_prob": 0.013631363399326801}, {"id": 2356, "seek": 1194368, "start": 11956.24, "end": 11960.720000000001, "text": " customers because they'll tell you that they want a faster horse. Maybe they need a car.", "tokens": [50992, 4581, 570, 436, 603, 980, 291, 300, 436, 528, 257, 4663, 6832, 13, 2704, 436, 643, 257, 1032, 13, 51216], "temperature": 0.0, "avg_logprob": -0.092989481932728, "compression_ratio": 1.7763578274760383, "no_speech_prob": 0.013631363399326801}, {"id": 2357, "seek": 1194368, "start": 11961.44, "end": 11966.800000000001, "text": " And so a lot of it comes into, I don't feel like we have to compete with LLMs. I think they'll", "tokens": [51252, 400, 370, 257, 688, 295, 309, 1487, 666, 11, 286, 500, 380, 841, 411, 321, 362, 281, 11831, 365, 441, 43, 26386, 13, 286, 519, 436, 603, 51520], "temperature": 0.0, "avg_logprob": -0.092989481932728, "compression_ratio": 1.7763578274760383, "no_speech_prob": 0.013631363399326801}, {"id": 2358, "seek": 1194368, "start": 11966.800000000001, "end": 11972.08, "text": " help automate a ton of the mechanical stuff out of the way. And just like, I think we all try", "tokens": [51520, 854, 31605, 257, 2952, 295, 264, 12070, 1507, 484, 295, 264, 636, 13, 400, 445, 411, 11, 286, 519, 321, 439, 853, 51784], "temperature": 0.0, "avg_logprob": -0.092989481932728, "compression_ratio": 1.7763578274760383, "no_speech_prob": 0.013631363399326801}, {"id": 2359, "seek": 1197208, "start": 11972.08, "end": 11977.039999999999, "text": " to scale through delegation and things like this. Delegating wrote things to an LLM, I think,", "tokens": [50364, 281, 4373, 807, 36602, 293, 721, 411, 341, 13, 1346, 6363, 990, 4114, 721, 281, 364, 441, 43, 44, 11, 286, 519, 11, 50612], "temperature": 0.0, "avg_logprob": -0.1371526330467162, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0106489984318614}, {"id": 2360, "seek": 1197208, "start": 11977.039999999999, "end": 11982.24, "text": " because it's extremely valuable and approach that will help us all scale and be more productive.", "tokens": [50612, 570, 309, 311, 4664, 8263, 293, 3109, 300, 486, 854, 505, 439, 4373, 293, 312, 544, 13304, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1371526330467162, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0106489984318614}, {"id": 2361, "seek": 1197208, "start": 11982.24, "end": 11986.48, "text": " But I think it's a fascinating companion. But I'd say I don't think that that means that we're", "tokens": [50872, 583, 286, 519, 309, 311, 257, 10343, 22363, 13, 583, 286, 1116, 584, 286, 500, 380, 519, 300, 300, 1355, 300, 321, 434, 51084], "temperature": 0.0, "avg_logprob": -0.1371526330467162, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0106489984318614}, {"id": 2362, "seek": 1197208, "start": 11986.48, "end": 11994.0, "text": " going to be done with coding. Sure. But there's power in it as a companion. And from there,", "tokens": [51084, 516, 281, 312, 1096, 365, 17720, 13, 4894, 13, 583, 456, 311, 1347, 294, 309, 382, 257, 22363, 13, 400, 490, 456, 11, 51460], "temperature": 0.0, "avg_logprob": -0.1371526330467162, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0106489984318614}, {"id": 2363, "seek": 1197208, "start": 11994.0, "end": 12000.48, "text": " I would love to zoom in on to mojo a little bit. Do you think about that? Do you think about", "tokens": [51460, 286, 576, 959, 281, 8863, 294, 322, 281, 705, 5134, 257, 707, 857, 13, 1144, 291, 519, 466, 300, 30, 1144, 291, 519, 466, 51784], "temperature": 0.0, "avg_logprob": -0.1371526330467162, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0106489984318614}, {"id": 2364, "seek": 1200048, "start": 12000.48, "end": 12008.0, "text": " LLMs generating mojo code and helping sort of like, we design new programming language, it almost", "tokens": [50364, 441, 43, 26386, 17746, 705, 5134, 3089, 293, 4315, 1333, 295, 411, 11, 321, 1715, 777, 9410, 2856, 11, 309, 1920, 50740], "temperature": 0.0, "avg_logprob": -0.1386822604258126, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0033744308166205883}, {"id": 2365, "seek": 1200048, "start": 12008.0, "end": 12015.359999999999, "text": " seems like, man, it would be nice to sort of, almost as a way to learn how I'm supposed to use", "tokens": [50740, 2544, 411, 11, 587, 11, 309, 576, 312, 1481, 281, 1333, 295, 11, 1920, 382, 257, 636, 281, 1466, 577, 286, 478, 3442, 281, 764, 51108], "temperature": 0.0, "avg_logprob": -0.1386822604258126, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0033744308166205883}, {"id": 2366, "seek": 1200048, "start": 12015.359999999999, "end": 12022.16, "text": " this thing for them to be trained on some of the mojo code. So I do lead an AI company. So maybe", "tokens": [51108, 341, 551, 337, 552, 281, 312, 8895, 322, 512, 295, 264, 705, 5134, 3089, 13, 407, 286, 360, 1477, 364, 7318, 2237, 13, 407, 1310, 51448], "temperature": 0.0, "avg_logprob": -0.1386822604258126, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0033744308166205883}, {"id": 2367, "seek": 1200048, "start": 12022.16, "end": 12029.279999999999, "text": " there'll be a mojo LLM at some point. But if your question is like, how do we make a language to be", "tokens": [51448, 456, 603, 312, 257, 705, 5134, 441, 43, 44, 412, 512, 935, 13, 583, 498, 428, 1168, 307, 411, 11, 577, 360, 321, 652, 257, 2856, 281, 312, 51804], "temperature": 0.0, "avg_logprob": -0.1386822604258126, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0033744308166205883}, {"id": 2368, "seek": 1202928, "start": 12029.28, "end": 12038.24, "text": " suitable for LLMs? I think the cool thing about LLMs is you don't have to. And so if you look at", "tokens": [50364, 12873, 337, 441, 43, 26386, 30, 286, 519, 264, 1627, 551, 466, 441, 43, 26386, 307, 291, 500, 380, 362, 281, 13, 400, 370, 498, 291, 574, 412, 50812], "temperature": 0.0, "avg_logprob": -0.07481867295724375, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.010007343254983425}, {"id": 2369, "seek": 1202928, "start": 12038.24, "end": 12042.400000000001, "text": " what is English or any of these other terrible languages that we as humans deal with on a", "tokens": [50812, 437, 307, 3669, 420, 604, 295, 613, 661, 6237, 8650, 300, 321, 382, 6255, 2028, 365, 322, 257, 51020], "temperature": 0.0, "avg_logprob": -0.07481867295724375, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.010007343254983425}, {"id": 2370, "seek": 1202928, "start": 12042.400000000001, "end": 12047.44, "text": " continuous basis, they're never designed for machines. And yet, they're the intermediate", "tokens": [51020, 10957, 5143, 11, 436, 434, 1128, 4761, 337, 8379, 13, 400, 1939, 11, 436, 434, 264, 19376, 51272], "temperature": 0.0, "avg_logprob": -0.07481867295724375, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.010007343254983425}, {"id": 2371, "seek": 1202928, "start": 12047.44, "end": 12052.640000000001, "text": " representation, they're the exchange format that we humans use to get stuff done. And so these", "tokens": [51272, 10290, 11, 436, 434, 264, 7742, 7877, 300, 321, 6255, 764, 281, 483, 1507, 1096, 13, 400, 370, 613, 51532], "temperature": 0.0, "avg_logprob": -0.07481867295724375, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.010007343254983425}, {"id": 2372, "seek": 1202928, "start": 12052.640000000001, "end": 12057.44, "text": " programming languages, they're an intermediate representation between the human and the computer", "tokens": [51532, 9410, 8650, 11, 436, 434, 364, 19376, 10290, 1296, 264, 1952, 293, 264, 3820, 51772], "temperature": 0.0, "avg_logprob": -0.07481867295724375, "compression_ratio": 1.782442748091603, "no_speech_prob": 0.010007343254983425}, {"id": 2373, "seek": 1205744, "start": 12057.44, "end": 12063.52, "text": " or the human and the compiler, roughly, right? And so I think the LLMs will have no problem learning", "tokens": [50364, 420, 264, 1952, 293, 264, 31958, 11, 9810, 11, 558, 30, 400, 370, 286, 519, 264, 441, 43, 26386, 486, 362, 572, 1154, 2539, 50668], "temperature": 0.0, "avg_logprob": -0.20416309269329974, "compression_ratio": 1.6385135135135136, "no_speech_prob": 0.003074886742979288}, {"id": 2374, "seek": 1205744, "start": 12064.24, "end": 12068.560000000001, "text": " whatever keyword we pick. Maybe the phi emoji is going to... Maybe that's going to break it,", "tokens": [50704, 2035, 20428, 321, 1888, 13, 2704, 264, 13107, 31595, 307, 516, 281, 485, 2704, 300, 311, 516, 281, 1821, 309, 11, 50920], "temperature": 0.0, "avg_logprob": -0.20416309269329974, "compression_ratio": 1.6385135135135136, "no_speech_prob": 0.003074886742979288}, {"id": 2375, "seek": 1205744, "start": 12068.560000000001, "end": 12073.76, "text": " it doesn't tokenize. No, the reverse of that, it will actually enable it because one of the issues", "tokens": [50920, 309, 1177, 380, 14862, 1125, 13, 883, 11, 264, 9943, 295, 300, 11, 309, 486, 767, 9528, 309, 570, 472, 295, 264, 2663, 51180], "temperature": 0.0, "avg_logprob": -0.20416309269329974, "compression_ratio": 1.6385135135135136, "no_speech_prob": 0.003074886742979288}, {"id": 2376, "seek": 1205744, "start": 12073.76, "end": 12079.6, "text": " I could see with being a superset of Python is there would be confusion by the gray area. So it", "tokens": [51180, 286, 727, 536, 365, 885, 257, 37906, 302, 295, 15329, 307, 456, 576, 312, 15075, 538, 264, 10855, 1859, 13, 407, 309, 51472], "temperature": 0.0, "avg_logprob": -0.20416309269329974, "compression_ratio": 1.6385135135135136, "no_speech_prob": 0.003074886742979288}, {"id": 2377, "seek": 1205744, "start": 12079.6, "end": 12085.92, "text": " would be mixing stuff. But... Well, I'm a human optimist, I'm also an LLM optimist. I think that", "tokens": [51472, 576, 312, 11983, 1507, 13, 583, 485, 1042, 11, 286, 478, 257, 1952, 5028, 468, 11, 286, 478, 611, 364, 441, 43, 44, 5028, 468, 13, 286, 519, 300, 51788], "temperature": 0.0, "avg_logprob": -0.20416309269329974, "compression_ratio": 1.6385135135135136, "no_speech_prob": 0.003074886742979288}, {"id": 2378, "seek": 1208592, "start": 12085.92, "end": 12094.32, "text": " we'll solve that problem. But you look at that and you say, okay, well, reducing the rote thing,", "tokens": [50364, 321, 603, 5039, 300, 1154, 13, 583, 291, 574, 412, 300, 293, 291, 584, 11, 1392, 11, 731, 11, 12245, 264, 367, 1370, 551, 11, 50784], "temperature": 0.0, "avg_logprob": -0.15071262576715733, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.013208160176873207}, {"id": 2379, "seek": 1208592, "start": 12094.32, "end": 12098.48, "text": " right? Turns out compilers are very particular and they really want things, they really want the", "tokens": [50784, 558, 30, 29524, 484, 715, 388, 433, 366, 588, 1729, 293, 436, 534, 528, 721, 11, 436, 534, 528, 264, 50992], "temperature": 0.0, "avg_logprob": -0.15071262576715733, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.013208160176873207}, {"id": 2380, "seek": 1208592, "start": 12098.48, "end": 12102.0, "text": " indentation to be right. They really want the colon to be there on your else or else that will", "tokens": [50992, 44494, 399, 281, 312, 558, 13, 814, 534, 528, 264, 8255, 281, 312, 456, 322, 428, 1646, 420, 1646, 300, 486, 51168], "temperature": 0.0, "avg_logprob": -0.15071262576715733, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.013208160176873207}, {"id": 2381, "seek": 1208592, "start": 12102.0, "end": 12108.24, "text": " complain, right? I mean, compilers can do better at this, but LLMs can totally help solve that", "tokens": [51168, 11024, 11, 558, 30, 286, 914, 11, 715, 388, 433, 393, 360, 1101, 412, 341, 11, 457, 441, 43, 26386, 393, 3879, 854, 5039, 300, 51480], "temperature": 0.0, "avg_logprob": -0.15071262576715733, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.013208160176873207}, {"id": 2382, "seek": 1208592, "start": 12108.24, "end": 12113.04, "text": " problem. And so I'm very happy about the new predictive coding and co-pilot type features", "tokens": [51480, 1154, 13, 400, 370, 286, 478, 588, 2055, 466, 264, 777, 35521, 17720, 293, 598, 12, 79, 31516, 2010, 4122, 51720], "temperature": 0.0, "avg_logprob": -0.15071262576715733, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.013208160176873207}, {"id": 2383, "seek": 1211304, "start": 12113.04, "end": 12115.84, "text": " and things like this because I think it will all just make us more productive.", "tokens": [50364, 293, 721, 411, 341, 570, 286, 519, 309, 486, 439, 445, 652, 505, 544, 13304, 13, 50504], "temperature": 0.0, "avg_logprob": -0.11548366702970911, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.016911284998059273}, {"id": 2384, "seek": 1211304, "start": 12115.84, "end": 12121.44, "text": " It's still messy and fuzzy and uncertain, unpredictable, so... But is there a future you", "tokens": [50504, 467, 311, 920, 16191, 293, 34710, 293, 11308, 11, 31160, 11, 370, 485, 583, 307, 456, 257, 2027, 291, 50784], "temperature": 0.0, "avg_logprob": -0.11548366702970911, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.016911284998059273}, {"id": 2385, "seek": 1211304, "start": 12121.44, "end": 12128.880000000001, "text": " see given how big of a leap GPT-4 was, where you start to see something like LLMs inside", "tokens": [50784, 536, 2212, 577, 955, 295, 257, 19438, 26039, 51, 12, 19, 390, 11, 689, 291, 722, 281, 536, 746, 411, 441, 43, 26386, 1854, 51156], "temperature": 0.0, "avg_logprob": -0.11548366702970911, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.016911284998059273}, {"id": 2386, "seek": 1211304, "start": 12129.52, "end": 12134.400000000001, "text": " a compiler or no? I mean, you could do that. Yeah, absolutely. I mean, I think that would be", "tokens": [51188, 257, 31958, 420, 572, 30, 286, 914, 11, 291, 727, 360, 300, 13, 865, 11, 3122, 13, 286, 914, 11, 286, 519, 300, 576, 312, 51432], "temperature": 0.0, "avg_logprob": -0.11548366702970911, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.016911284998059273}, {"id": 2387, "seek": 1211304, "start": 12134.400000000001, "end": 12141.2, "text": " interesting. Is that wise? Well, I mean, it would be very expensive. So compilers run fast and they're", "tokens": [51432, 1880, 13, 1119, 300, 10829, 30, 1042, 11, 286, 914, 11, 309, 576, 312, 588, 5124, 13, 407, 715, 388, 433, 1190, 2370, 293, 436, 434, 51772], "temperature": 0.0, "avg_logprob": -0.11548366702970911, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.016911284998059273}, {"id": 2388, "seek": 1214120, "start": 12141.2, "end": 12145.36, "text": " very efficient and LLMs are currently very expensive. There's on-device LLMs and there's", "tokens": [50364, 588, 7148, 293, 441, 43, 26386, 366, 4362, 588, 5124, 13, 821, 311, 322, 12, 40343, 573, 441, 43, 26386, 293, 456, 311, 50572], "temperature": 0.0, "avg_logprob": -0.06704311528481728, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0021154414862394333}, {"id": 2389, "seek": 1214120, "start": 12145.36, "end": 12150.240000000002, "text": " other things going on and so maybe there's an answer there. I think that one of the things that I", "tokens": [50572, 661, 721, 516, 322, 293, 370, 1310, 456, 311, 364, 1867, 456, 13, 286, 519, 300, 472, 295, 264, 721, 300, 286, 50816], "temperature": 0.0, "avg_logprob": -0.06704311528481728, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0021154414862394333}, {"id": 2390, "seek": 1214120, "start": 12150.240000000002, "end": 12157.6, "text": " haven't seen enough of is that... So LLMs to me are amazing when you tap into the creative potential", "tokens": [50816, 2378, 380, 1612, 1547, 295, 307, 300, 485, 407, 441, 43, 26386, 281, 385, 366, 2243, 562, 291, 5119, 666, 264, 5880, 3995, 51184], "temperature": 0.0, "avg_logprob": -0.06704311528481728, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0021154414862394333}, {"id": 2391, "seek": 1214120, "start": 12157.6, "end": 12163.92, "text": " of the hallucinations, right? And so if you're doing creative brainstorming or creative writing", "tokens": [51184, 295, 264, 35212, 10325, 11, 558, 30, 400, 370, 498, 291, 434, 884, 5880, 35245, 278, 420, 5880, 3579, 51500], "temperature": 0.0, "avg_logprob": -0.06704311528481728, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0021154414862394333}, {"id": 2392, "seek": 1214120, "start": 12163.92, "end": 12169.6, "text": " or things like that, the hallucinations work in your favor. If you're writing code that has to be", "tokens": [51500, 420, 721, 411, 300, 11, 264, 35212, 10325, 589, 294, 428, 2294, 13, 759, 291, 434, 3579, 3089, 300, 575, 281, 312, 51784], "temperature": 0.0, "avg_logprob": -0.06704311528481728, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.0021154414862394333}, {"id": 2393, "seek": 1216960, "start": 12169.6, "end": 12172.720000000001, "text": " correct because you're going to ship it in production, then maybe that's not actually a feature.", "tokens": [50364, 3006, 570, 291, 434, 516, 281, 5374, 309, 294, 4265, 11, 550, 1310, 300, 311, 406, 767, 257, 4111, 13, 50520], "temperature": 0.0, "avg_logprob": -0.09568770457122286, "compression_ratio": 1.8586572438162545, "no_speech_prob": 0.02032536454498768}, {"id": 2394, "seek": 1216960, "start": 12173.84, "end": 12177.92, "text": " And so I think that there has been research and there has been work on building", "tokens": [50576, 400, 370, 286, 519, 300, 456, 575, 668, 2132, 293, 456, 575, 668, 589, 322, 2390, 50780], "temperature": 0.0, "avg_logprob": -0.09568770457122286, "compression_ratio": 1.8586572438162545, "no_speech_prob": 0.02032536454498768}, {"id": 2395, "seek": 1216960, "start": 12178.640000000001, "end": 12184.24, "text": " algebraic reasoning systems and figuring out more things that feel like proofs.", "tokens": [50816, 21989, 299, 21577, 3652, 293, 15213, 484, 544, 721, 300, 841, 411, 8177, 82, 13, 51096], "temperature": 0.0, "avg_logprob": -0.09568770457122286, "compression_ratio": 1.8586572438162545, "no_speech_prob": 0.02032536454498768}, {"id": 2396, "seek": 1216960, "start": 12184.960000000001, "end": 12188.0, "text": " And so I think that there could be interesting work in terms of building more", "tokens": [51132, 400, 370, 286, 519, 300, 456, 727, 312, 1880, 589, 294, 2115, 295, 2390, 544, 51284], "temperature": 0.0, "avg_logprob": -0.09568770457122286, "compression_ratio": 1.8586572438162545, "no_speech_prob": 0.02032536454498768}, {"id": 2397, "seek": 1216960, "start": 12188.640000000001, "end": 12193.04, "text": " reliable at scale systems and that could be interesting. But if you chase that rabbit hole", "tokens": [51316, 12924, 412, 4373, 3652, 293, 300, 727, 312, 1880, 13, 583, 498, 291, 15359, 300, 19509, 5458, 51536], "temperature": 0.0, "avg_logprob": -0.09568770457122286, "compression_ratio": 1.8586572438162545, "no_speech_prob": 0.02032536454498768}, {"id": 2398, "seek": 1216960, "start": 12193.04, "end": 12197.44, "text": " down, the question then becomes how do you express your intent to the machine? And so maybe you want", "tokens": [51536, 760, 11, 264, 1168, 550, 3643, 577, 360, 291, 5109, 428, 8446, 281, 264, 3479, 30, 400, 370, 1310, 291, 528, 51756], "temperature": 0.0, "avg_logprob": -0.09568770457122286, "compression_ratio": 1.8586572438162545, "no_speech_prob": 0.02032536454498768}, {"id": 2399, "seek": 1219744, "start": 12197.44, "end": 12203.84, "text": " LLMs to provide the spec, but you have a different kind of net that then actually implements the code.", "tokens": [50364, 441, 43, 26386, 281, 2893, 264, 1608, 11, 457, 291, 362, 257, 819, 733, 295, 2533, 300, 550, 767, 704, 17988, 264, 3089, 13, 50684], "temperature": 0.0, "avg_logprob": -0.17065100516042403, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.010483436286449432}, {"id": 2400, "seek": 1219744, "start": 12203.84, "end": 12210.4, "text": " Right. So it's used as documentation and inspiration versus the actual implementation.", "tokens": [50684, 1779, 13, 407, 309, 311, 1143, 382, 14333, 293, 10249, 5717, 264, 3539, 11420, 13, 51012], "temperature": 0.0, "avg_logprob": -0.17065100516042403, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.010483436286449432}, {"id": 2401, "seek": 1219744, "start": 12210.4, "end": 12219.04, "text": " Yeah, potentially. Since a successful modular will be the thing that runs, I say so jokingly,", "tokens": [51012, 865, 11, 7263, 13, 4162, 257, 4406, 31111, 486, 312, 264, 551, 300, 6676, 11, 286, 584, 370, 17396, 356, 11, 51444], "temperature": 0.0, "avg_logprob": -0.17065100516042403, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.010483436286449432}, {"id": 2402, "seek": 1219744, "start": 12219.04, "end": 12226.560000000001, "text": " are AI overlords. But AI systems that are used across... I know it's a cliche term, but in", "tokens": [51444, 366, 7318, 15986, 5703, 13, 583, 7318, 3652, 300, 366, 1143, 2108, 485, 286, 458, 309, 311, 257, 46705, 1433, 11, 457, 294, 51820], "temperature": 0.0, "avg_logprob": -0.17065100516042403, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.010483436286449432}, {"id": 2403, "seek": 1222656, "start": 12226.64, "end": 12232.08, "text": " a lot of things. So across... So I'll joke and say like AGI should be written in Mojo.", "tokens": [50368, 257, 688, 295, 721, 13, 407, 2108, 485, 407, 286, 603, 7647, 293, 584, 411, 316, 26252, 820, 312, 3720, 294, 3335, 5134, 13, 50640], "temperature": 0.0, "avg_logprob": -0.15970124348555462, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.00159682787489146}, {"id": 2404, "seek": 1222656, "start": 12232.08, "end": 12236.96, "text": " Yeah, AGI should be written in Mojo. You're joking, but it's also possible that it's not a joke.", "tokens": [50640, 865, 11, 316, 26252, 820, 312, 3720, 294, 3335, 5134, 13, 509, 434, 17396, 11, 457, 309, 311, 611, 1944, 300, 309, 311, 406, 257, 7647, 13, 50884], "temperature": 0.0, "avg_logprob": -0.15970124348555462, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.00159682787489146}, {"id": 2405, "seek": 1222656, "start": 12238.64, "end": 12245.439999999999, "text": " That a lot of the ideas behind Mojo is seems like the natural set of ideas that would enable", "tokens": [50968, 663, 257, 688, 295, 264, 3487, 2261, 3335, 5134, 307, 2544, 411, 264, 3303, 992, 295, 3487, 300, 576, 9528, 51308], "temperature": 0.0, "avg_logprob": -0.15970124348555462, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.00159682787489146}, {"id": 2406, "seek": 1222656, "start": 12245.439999999999, "end": 12253.119999999999, "text": " at scale training and inference of AI systems. So I just have to ask you about the big philosophical", "tokens": [51308, 412, 4373, 3097, 293, 38253, 295, 7318, 3652, 13, 407, 286, 445, 362, 281, 1029, 291, 466, 264, 955, 25066, 51692], "temperature": 0.0, "avg_logprob": -0.15970124348555462, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.00159682787489146}, {"id": 2407, "seek": 1225312, "start": 12253.12, "end": 12259.04, "text": " question about human civilization. So folks like Eliezer Yatkowski are really concerned about the", "tokens": [50364, 1168, 466, 1952, 18036, 13, 407, 4024, 411, 2699, 414, 4527, 398, 267, 74, 21866, 366, 534, 5922, 466, 264, 50660], "temperature": 0.0, "avg_logprob": -0.17281195457945478, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0010811735410243273}, {"id": 2408, "seek": 1225312, "start": 12259.04, "end": 12269.6, "text": " threat of AI. Do you think about the good and the bad that can happen at scale deployment of AI systems?", "tokens": [50660, 4734, 295, 7318, 13, 1144, 291, 519, 466, 264, 665, 293, 264, 1578, 300, 393, 1051, 412, 4373, 19317, 295, 7318, 3652, 30, 51188], "temperature": 0.0, "avg_logprob": -0.17281195457945478, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0010811735410243273}, {"id": 2409, "seek": 1225312, "start": 12269.6, "end": 12273.92, "text": " Well, so I've thought a lot about it and there's a lot of different parts to this problem,", "tokens": [51188, 1042, 11, 370, 286, 600, 1194, 257, 688, 466, 309, 293, 456, 311, 257, 688, 295, 819, 3166, 281, 341, 1154, 11, 51404], "temperature": 0.0, "avg_logprob": -0.17281195457945478, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0010811735410243273}, {"id": 2410, "seek": 1225312, "start": 12273.92, "end": 12279.52, "text": " everything from job displacement to sky nut, things like this. And so you can zoom in to", "tokens": [51404, 1203, 490, 1691, 21899, 281, 5443, 5393, 11, 721, 411, 341, 13, 400, 370, 291, 393, 8863, 294, 281, 51684], "temperature": 0.0, "avg_logprob": -0.17281195457945478, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.0010811735410243273}, {"id": 2411, "seek": 1227952, "start": 12279.52, "end": 12287.36, "text": " sub parts of this problem. I'm not super optimistic about AGI being solved next year.", "tokens": [50364, 1422, 3166, 295, 341, 1154, 13, 286, 478, 406, 1687, 19397, 466, 316, 26252, 885, 13041, 958, 1064, 13, 50756], "temperature": 0.0, "avg_logprob": -0.19841432571411133, "compression_ratio": 1.5, "no_speech_prob": 0.007458867039531469}, {"id": 2412, "seek": 1227952, "start": 12288.32, "end": 12294.0, "text": " I don't think that's going to happen personally. So you have a kind of zen like calm about. Is", "tokens": [50804, 286, 500, 380, 519, 300, 311, 516, 281, 1051, 5665, 13, 407, 291, 362, 257, 733, 295, 37097, 411, 7151, 466, 13, 1119, 51088], "temperature": 0.0, "avg_logprob": -0.19841432571411133, "compression_ratio": 1.5, "no_speech_prob": 0.007458867039531469}, {"id": 2413, "seek": 1227952, "start": 12294.0, "end": 12301.36, "text": " there's a nervousness because the leap of GBT4 seemed so big. Sure. It's like we're almost...", "tokens": [51088, 456, 311, 257, 6296, 1287, 570, 264, 19438, 295, 26809, 51, 19, 6576, 370, 955, 13, 4894, 13, 467, 311, 411, 321, 434, 1920, 485, 51456], "temperature": 0.0, "avg_logprob": -0.19841432571411133, "compression_ratio": 1.5, "no_speech_prob": 0.007458867039531469}, {"id": 2414, "seek": 1227952, "start": 12302.0, "end": 12306.4, "text": " There's some kind of transition era period. You're thinking... Well, so I mean,", "tokens": [51488, 821, 311, 512, 733, 295, 6034, 4249, 2896, 13, 509, 434, 1953, 485, 1042, 11, 370, 286, 914, 11, 51708], "temperature": 0.0, "avg_logprob": -0.19841432571411133, "compression_ratio": 1.5, "no_speech_prob": 0.007458867039531469}, {"id": 2415, "seek": 1230640, "start": 12306.4, "end": 12311.76, "text": " there's a couple of things going on there. One is I'm sure GPT5 and 7 and 19 will be", "tokens": [50364, 456, 311, 257, 1916, 295, 721, 516, 322, 456, 13, 1485, 307, 286, 478, 988, 26039, 51, 20, 293, 1614, 293, 1294, 486, 312, 50632], "temperature": 0.0, "avg_logprob": -0.10551385804424136, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.006096689961850643}, {"id": 2416, "seek": 1230640, "start": 12311.76, "end": 12317.199999999999, "text": " also huge leaps. They're also getting much more expensive to run. And so there may be a limiting", "tokens": [50632, 611, 2603, 476, 2382, 13, 814, 434, 611, 1242, 709, 544, 5124, 281, 1190, 13, 400, 370, 456, 815, 312, 257, 22083, 50904], "temperature": 0.0, "avg_logprob": -0.10551385804424136, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.006096689961850643}, {"id": 2417, "seek": 1230640, "start": 12317.199999999999, "end": 12322.56, "text": " function in terms of just expense on one hand and train. That could be a limiter that slows things", "tokens": [50904, 2445, 294, 2115, 295, 445, 18406, 322, 472, 1011, 293, 3847, 13, 663, 727, 312, 257, 2364, 1681, 300, 35789, 721, 51172], "temperature": 0.0, "avg_logprob": -0.10551385804424136, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.006096689961850643}, {"id": 2418, "seek": 1230640, "start": 12322.56, "end": 12329.359999999999, "text": " down. But I think the bigger limiter outside of sky nut takes over and I don't spend any time thinking", "tokens": [51172, 760, 13, 583, 286, 519, 264, 3801, 2364, 1681, 2380, 295, 5443, 5393, 2516, 670, 293, 286, 500, 380, 3496, 604, 565, 1953, 51512], "temperature": 0.0, "avg_logprob": -0.10551385804424136, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.006096689961850643}, {"id": 2419, "seek": 1230640, "start": 12329.359999999999, "end": 12332.72, "text": " about that because if sky nut takes over and kills us all, then I'll be dead. So I don't worry about", "tokens": [51512, 466, 300, 570, 498, 5443, 5393, 2516, 670, 293, 14563, 505, 439, 11, 550, 286, 603, 312, 3116, 13, 407, 286, 500, 380, 3292, 466, 51680], "temperature": 0.0, "avg_logprob": -0.10551385804424136, "compression_ratio": 1.6747404844290656, "no_speech_prob": 0.006096689961850643}, {"id": 2420, "seek": 1233272, "start": 12332.72, "end": 12340.08, "text": " that. Other things worry about, I'll just focus on. I'll focus and not worry about that one.", "tokens": [50364, 300, 13, 5358, 721, 3292, 466, 11, 286, 603, 445, 1879, 322, 13, 286, 603, 1879, 293, 406, 3292, 466, 300, 472, 13, 50732], "temperature": 0.0, "avg_logprob": -0.10600874821345012, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.04599950462579727}, {"id": 2421, "seek": 1233272, "start": 12341.279999999999, "end": 12347.76, "text": " But I think that the other thing I'd say is that AI moves quickly, but humans move slowly and we", "tokens": [50792, 583, 286, 519, 300, 264, 661, 551, 286, 1116, 584, 307, 300, 7318, 6067, 2661, 11, 457, 6255, 1286, 5692, 293, 321, 51116], "temperature": 0.0, "avg_logprob": -0.10600874821345012, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.04599950462579727}, {"id": 2422, "seek": 1233272, "start": 12347.76, "end": 12354.8, "text": " adapt slowly. And so what I expect to happen is just like any technology diffusion, the promise", "tokens": [51116, 6231, 5692, 13, 400, 370, 437, 286, 2066, 281, 1051, 307, 445, 411, 604, 2899, 25242, 11, 264, 6228, 51468], "temperature": 0.0, "avg_logprob": -0.10600874821345012, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.04599950462579727}, {"id": 2423, "seek": 1233272, "start": 12354.8, "end": 12361.119999999999, "text": " and then the application takes time to roll out. And so I think that I'm not even too worried about", "tokens": [51468, 293, 550, 264, 3861, 2516, 565, 281, 3373, 484, 13, 400, 370, 286, 519, 300, 286, 478, 406, 754, 886, 5804, 466, 51784], "temperature": 0.0, "avg_logprob": -0.10600874821345012, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.04599950462579727}, {"id": 2424, "seek": 1236112, "start": 12361.76, "end": 12365.92, "text": " autonomous cars defining away all the taxi drivers. Remember, autonomy is supposed to be", "tokens": [50396, 23797, 5163, 17827, 1314, 439, 264, 18984, 11590, 13, 5459, 11, 27278, 307, 3442, 281, 312, 50604], "temperature": 0.0, "avg_logprob": -0.1241545421736581, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0027143920306116343}, {"id": 2425, "seek": 1236112, "start": 12365.92, "end": 12373.6, "text": " solved by 2020? Boy, do I remember. And so I think that on the one hand we can see amazing", "tokens": [50604, 13041, 538, 4808, 30, 9486, 11, 360, 286, 1604, 13, 400, 370, 286, 519, 300, 322, 264, 472, 1011, 321, 393, 536, 2243, 50988], "temperature": 0.0, "avg_logprob": -0.1241545421736581, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0027143920306116343}, {"id": 2426, "seek": 1236112, "start": 12373.6, "end": 12379.04, "text": " progress, but on the other hand we can see that the reality is a little bit more complicated", "tokens": [50988, 4205, 11, 457, 322, 264, 661, 1011, 321, 393, 536, 300, 264, 4103, 307, 257, 707, 857, 544, 6179, 51260], "temperature": 0.0, "avg_logprob": -0.1241545421736581, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0027143920306116343}, {"id": 2427, "seek": 1236112, "start": 12379.04, "end": 12383.84, "text": " and it may take longer to roll out than you might expect. Well, that's in the physical space. I do", "tokens": [51260, 293, 309, 815, 747, 2854, 281, 3373, 484, 813, 291, 1062, 2066, 13, 1042, 11, 300, 311, 294, 264, 4001, 1901, 13, 286, 360, 51500], "temperature": 0.0, "avg_logprob": -0.1241545421736581, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0027143920306116343}, {"id": 2428, "seek": 1236112, "start": 12383.84, "end": 12389.52, "text": " think in the digital space is the stuff that's built on top of LLMs that runs", "tokens": [51500, 519, 294, 264, 4562, 1901, 307, 264, 1507, 300, 311, 3094, 322, 1192, 295, 441, 43, 26386, 300, 6676, 51784], "temperature": 0.0, "avg_logprob": -0.1241545421736581, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.0027143920306116343}, {"id": 2429, "seek": 1239112, "start": 12391.12, "end": 12396.160000000002, "text": " millions of apps that could be built on top of them. And that could be run on millions of devices,", "tokens": [50364, 6803, 295, 7733, 300, 727, 312, 3094, 322, 1192, 295, 552, 13, 400, 300, 727, 312, 1190, 322, 6803, 295, 5759, 11, 50616], "temperature": 0.0, "avg_logprob": -0.18344740800454584, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.0021474375389516354}, {"id": 2430, "seek": 1239112, "start": 12396.160000000002, "end": 12404.880000000001, "text": " millions of types of devices. I just think that the rapid effect it has on human civilization could", "tokens": [50616, 6803, 295, 3467, 295, 5759, 13, 286, 445, 519, 300, 264, 7558, 1802, 309, 575, 322, 1952, 18036, 727, 51052], "temperature": 0.0, "avg_logprob": -0.18344740800454584, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.0021474375389516354}, {"id": 2431, "seek": 1239112, "start": 12404.880000000001, "end": 12413.76, "text": " be truly transformative to it. And there I think it depends on are you an optimist or a pessimist", "tokens": [51052, 312, 4908, 36070, 281, 309, 13, 400, 456, 286, 519, 309, 5946, 322, 366, 291, 364, 5028, 468, 420, 257, 37399, 468, 51496], "temperature": 0.0, "avg_logprob": -0.18344740800454584, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.0021474375389516354}, {"id": 2432, "seek": 1241376, "start": 12413.76, "end": 12422.08, "text": " or a masochist? Just to clarify, optimist about human civilization. Me too. And so I look at that", "tokens": [50364, 420, 257, 2300, 8997, 468, 30, 1449, 281, 17594, 11, 5028, 468, 466, 1952, 18036, 13, 1923, 886, 13, 400, 370, 286, 574, 412, 300, 50780], "temperature": 0.0, "avg_logprob": -0.17990156103063512, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.2093433141708374}, {"id": 2433, "seek": 1241376, "start": 12422.08, "end": 12426.880000000001, "text": " as saying, okay, cool, what will AI do? And so some people say, oh my god, is it going to destroy", "tokens": [50780, 382, 1566, 11, 1392, 11, 1627, 11, 437, 486, 7318, 360, 30, 400, 370, 512, 561, 584, 11, 1954, 452, 3044, 11, 307, 309, 516, 281, 5293, 51020], "temperature": 0.0, "avg_logprob": -0.17990156103063512, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.2093433141708374}, {"id": 2434, "seek": 1241376, "start": 12426.880000000001, "end": 12431.76, "text": " us all? How do we prevent that? I kind of look at it from a, is it going to unlock us all?", "tokens": [51020, 505, 439, 30, 1012, 360, 321, 4871, 300, 30, 286, 733, 295, 574, 412, 309, 490, 257, 11, 307, 309, 516, 281, 11634, 505, 439, 30, 51264], "temperature": 0.0, "avg_logprob": -0.17990156103063512, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.2093433141708374}, {"id": 2435, "seek": 1241376, "start": 12432.64, "end": 12435.52, "text": " You talk about coding, is it going to make so I don't have to do all the repetitive stuff?", "tokens": [51308, 509, 751, 466, 17720, 11, 307, 309, 516, 281, 652, 370, 286, 500, 380, 362, 281, 360, 439, 264, 29404, 1507, 30, 51452], "temperature": 0.0, "avg_logprob": -0.17990156103063512, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.2093433141708374}, {"id": 2436, "seek": 1241376, "start": 12436.56, "end": 12441.6, "text": " Well, suddenly that's a very optimistic way to look at it. And you look at what a lot of these", "tokens": [51504, 1042, 11, 5800, 300, 311, 257, 588, 19397, 636, 281, 574, 412, 309, 13, 400, 291, 574, 412, 437, 257, 688, 295, 613, 51756], "temperature": 0.0, "avg_logprob": -0.17990156103063512, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.2093433141708374}, {"id": 2437, "seek": 1244160, "start": 12441.6, "end": 12445.36, "text": " technologies have done to improve our lives. And I want that to go faster.", "tokens": [50364, 7943, 362, 1096, 281, 3470, 527, 2909, 13, 400, 286, 528, 300, 281, 352, 4663, 13, 50552], "temperature": 0.0, "avg_logprob": -0.17558672094857822, "compression_ratio": 1.5, "no_speech_prob": 0.0032719720620661974}, {"id": 2438, "seek": 1244160, "start": 12447.12, "end": 12451.12, "text": " What do you think the future of programming looks like in the next 10, 20, 30, 50 years", "tokens": [50640, 708, 360, 291, 519, 264, 2027, 295, 9410, 1542, 411, 294, 264, 958, 1266, 11, 945, 11, 2217, 11, 2625, 924, 50840], "temperature": 0.0, "avg_logprob": -0.17558672094857822, "compression_ratio": 1.5, "no_speech_prob": 0.0032719720620661974}, {"id": 2439, "seek": 1244160, "start": 12452.16, "end": 12460.720000000001, "text": " with LLMs and with Mojo with modular, like your vision for devices, the hardware to the", "tokens": [50892, 365, 441, 43, 26386, 293, 365, 3335, 5134, 365, 31111, 11, 411, 428, 5201, 337, 5759, 11, 264, 8837, 281, 264, 51320], "temperature": 0.0, "avg_logprob": -0.17558672094857822, "compression_ratio": 1.5, "no_speech_prob": 0.0032719720620661974}, {"id": 2440, "seek": 1244160, "start": 12460.720000000001, "end": 12465.52, "text": " compilers to this, to the different stacks of software. Yeah. Well, so what I want, I mean,", "tokens": [51320, 715, 388, 433, 281, 341, 11, 281, 264, 819, 30792, 295, 4722, 13, 865, 13, 1042, 11, 370, 437, 286, 528, 11, 286, 914, 11, 51560], "temperature": 0.0, "avg_logprob": -0.17558672094857822, "compression_ratio": 1.5, "no_speech_prob": 0.0032719720620661974}, {"id": 2441, "seek": 1246552, "start": 12465.92, "end": 12473.12, "text": " coming back to my arch nemesis, it's complexity. So again, me being the optimist, if we drive down", "tokens": [50384, 1348, 646, 281, 452, 3912, 408, 5814, 271, 11, 309, 311, 14024, 13, 407, 797, 11, 385, 885, 264, 5028, 468, 11, 498, 321, 3332, 760, 50744], "temperature": 0.0, "avg_logprob": -0.1445995027368719, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004330685827881098}, {"id": 2442, "seek": 1246552, "start": 12473.12, "end": 12478.0, "text": " complexity, we can make these tools, these technologies, these cool hardware widgets", "tokens": [50744, 14024, 11, 321, 393, 652, 613, 3873, 11, 613, 7943, 11, 613, 1627, 8837, 43355, 50988], "temperature": 0.0, "avg_logprob": -0.1445995027368719, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004330685827881098}, {"id": 2443, "seek": 1246552, "start": 12478.0, "end": 12482.560000000001, "text": " accessible to way more people. And so what I'd love to see is more personalized experiences,", "tokens": [50988, 9515, 281, 636, 544, 561, 13, 400, 370, 437, 286, 1116, 959, 281, 536, 307, 544, 28415, 5235, 11, 51216], "temperature": 0.0, "avg_logprob": -0.1445995027368719, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004330685827881098}, {"id": 2444, "seek": 1246552, "start": 12482.560000000001, "end": 12487.28, "text": " more things, the research getting into production instead of being lost at NeurIPS.", "tokens": [51216, 544, 721, 11, 264, 2132, 1242, 666, 4265, 2602, 295, 885, 2731, 412, 1734, 374, 40, 6273, 13, 51452], "temperature": 0.0, "avg_logprob": -0.1445995027368719, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004330685827881098}, {"id": 2445, "seek": 1248728, "start": 12488.24, "end": 12495.76, "text": " Right. And like these things that impact people's lives by entering products. And so one of the", "tokens": [50412, 1779, 13, 400, 411, 613, 721, 300, 2712, 561, 311, 2909, 538, 11104, 3383, 13, 400, 370, 472, 295, 264, 50788], "temperature": 0.0, "avg_logprob": -0.13513685041858303, "compression_ratio": 1.5708502024291497, "no_speech_prob": 0.23636694252490997}, {"id": 2446, "seek": 1248728, "start": 12495.76, "end": 12501.36, "text": " things that I'm a little bit concerned about is right now, the big companies are investing huge", "tokens": [50788, 721, 300, 286, 478, 257, 707, 857, 5922, 466, 307, 558, 586, 11, 264, 955, 3431, 366, 10978, 2603, 51068], "temperature": 0.0, "avg_logprob": -0.13513685041858303, "compression_ratio": 1.5708502024291497, "no_speech_prob": 0.23636694252490997}, {"id": 2447, "seek": 1248728, "start": 12501.36, "end": 12506.720000000001, "text": " amounts of money and are driving the top line of AI capability forward really quickly. But if it", "tokens": [51068, 11663, 295, 1460, 293, 366, 4840, 264, 1192, 1622, 295, 7318, 13759, 2128, 534, 2661, 13, 583, 498, 309, 51336], "temperature": 0.0, "avg_logprob": -0.13513685041858303, "compression_ratio": 1.5708502024291497, "no_speech_prob": 0.23636694252490997}, {"id": 2448, "seek": 1248728, "start": 12506.720000000001, "end": 12513.12, "text": " means that you have to have $100 million to train a model or more $100 billion, right? Well, that's", "tokens": [51336, 1355, 300, 291, 362, 281, 362, 1848, 6879, 2459, 281, 3847, 257, 2316, 420, 544, 1848, 6879, 5218, 11, 558, 30, 1042, 11, 300, 311, 51656], "temperature": 0.0, "avg_logprob": -0.13513685041858303, "compression_ratio": 1.5708502024291497, "no_speech_prob": 0.23636694252490997}, {"id": 2449, "seek": 1251312, "start": 12513.12, "end": 12517.84, "text": " going to make it very concentrated with very few people in the world that can actually do this", "tokens": [50364, 516, 281, 652, 309, 588, 21321, 365, 588, 1326, 561, 294, 264, 1002, 300, 393, 767, 360, 341, 50600], "temperature": 0.0, "avg_logprob": -0.08147859151384472, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.0069020590744912624}, {"id": 2450, "seek": 1251312, "start": 12517.84, "end": 12524.160000000002, "text": " stuff. I would much rather see lots of people across the industry be able to participate and use", "tokens": [50600, 1507, 13, 286, 576, 709, 2831, 536, 3195, 295, 561, 2108, 264, 3518, 312, 1075, 281, 8197, 293, 764, 50916], "temperature": 0.0, "avg_logprob": -0.08147859151384472, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.0069020590744912624}, {"id": 2451, "seek": 1251312, "start": 12524.160000000002, "end": 12528.08, "text": " this, right? And you look at this, you know, I mean, a lot of great research has been done", "tokens": [50916, 341, 11, 558, 30, 400, 291, 574, 412, 341, 11, 291, 458, 11, 286, 914, 11, 257, 688, 295, 869, 2132, 575, 668, 1096, 51112], "temperature": 0.0, "avg_logprob": -0.08147859151384472, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.0069020590744912624}, {"id": 2452, "seek": 1251312, "start": 12528.640000000001, "end": 12535.04, "text": " in the health world and looking at like detecting pathologies and doing radiology with AI and like", "tokens": [51140, 294, 264, 1585, 1002, 293, 1237, 412, 411, 40237, 3100, 6204, 293, 884, 16335, 1793, 365, 7318, 293, 411, 51460], "temperature": 0.0, "avg_logprob": -0.08147859151384472, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.0069020590744912624}, {"id": 2453, "seek": 1251312, "start": 12535.04, "end": 12539.68, "text": " doing all these things. Well, the problem today is that to deploy and build these systems, you have", "tokens": [51460, 884, 439, 613, 721, 13, 1042, 11, 264, 1154, 965, 307, 300, 281, 7274, 293, 1322, 613, 3652, 11, 291, 362, 51692], "temperature": 0.0, "avg_logprob": -0.08147859151384472, "compression_ratio": 1.7056737588652482, "no_speech_prob": 0.0069020590744912624}, {"id": 2454, "seek": 1253968, "start": 12539.68, "end": 12546.880000000001, "text": " to be an expert in radiology and an expert in AI. And if we can break down the barriers so that more", "tokens": [50364, 281, 312, 364, 5844, 294, 16335, 1793, 293, 364, 5844, 294, 7318, 13, 400, 498, 321, 393, 1821, 760, 264, 13565, 370, 300, 544, 50724], "temperature": 0.0, "avg_logprob": -0.09457108892243483, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0034829871729016304}, {"id": 2455, "seek": 1253968, "start": 12546.880000000001, "end": 12553.44, "text": " people can use AI techniques, it's more like programming Python, which roughly everybody", "tokens": [50724, 561, 393, 764, 7318, 7512, 11, 309, 311, 544, 411, 9410, 15329, 11, 597, 9810, 2201, 51052], "temperature": 0.0, "avg_logprob": -0.09457108892243483, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0034829871729016304}, {"id": 2456, "seek": 1253968, "start": 12553.44, "end": 12557.6, "text": " can do if they want to, right? Then I think that we'll get a lot more practical application of", "tokens": [51052, 393, 360, 498, 436, 528, 281, 11, 558, 30, 1396, 286, 519, 300, 321, 603, 483, 257, 688, 544, 8496, 3861, 295, 51260], "temperature": 0.0, "avg_logprob": -0.09457108892243483, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0034829871729016304}, {"id": 2457, "seek": 1253968, "start": 12557.6, "end": 12563.04, "text": " these techniques and a lot more niche year, cool, but narrower demands. And I think that's", "tokens": [51260, 613, 7512, 293, 257, 688, 544, 19956, 1064, 11, 1627, 11, 457, 46751, 15107, 13, 400, 286, 519, 300, 311, 51532], "temperature": 0.0, "avg_logprob": -0.09457108892243483, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0034829871729016304}, {"id": 2458, "seek": 1253968, "start": 12563.04, "end": 12567.52, "text": " that's going to be really cool. Do you think we'll have more or less programmers in the world than", "tokens": [51532, 300, 311, 516, 281, 312, 534, 1627, 13, 1144, 291, 519, 321, 603, 362, 544, 420, 1570, 41504, 294, 264, 1002, 813, 51756], "temperature": 0.0, "avg_logprob": -0.09457108892243483, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.0034829871729016304}, {"id": 2459, "seek": 1256752, "start": 12567.52, "end": 12572.720000000001, "text": " now? Well, so I think we'll have more more programmers, but they may not consider themselves", "tokens": [50364, 586, 30, 1042, 11, 370, 286, 519, 321, 603, 362, 544, 544, 41504, 11, 457, 436, 815, 406, 1949, 2969, 50624], "temperature": 0.0, "avg_logprob": -0.13513281449027684, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.007117669563740492}, {"id": 2460, "seek": 1256752, "start": 12572.720000000001, "end": 12576.08, "text": " to be programmers. That'd be a different name for you, right? I mean, do you consider somebody", "tokens": [50624, 281, 312, 41504, 13, 663, 1116, 312, 257, 819, 1315, 337, 291, 11, 558, 30, 286, 914, 11, 360, 291, 1949, 2618, 50792], "temperature": 0.0, "avg_logprob": -0.13513281449027684, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.007117669563740492}, {"id": 2461, "seek": 1256752, "start": 12576.08, "end": 12581.76, "text": " that uses, you know, I think that arguably the most popular programming language is Excel?", "tokens": [50792, 300, 4960, 11, 291, 458, 11, 286, 519, 300, 26771, 264, 881, 3743, 9410, 2856, 307, 19060, 30, 51076], "temperature": 0.0, "avg_logprob": -0.13513281449027684, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.007117669563740492}, {"id": 2462, "seek": 1256752, "start": 12582.640000000001, "end": 12588.4, "text": " Yeah. Right? Yep. And so do they consider themselves to be programmers? Maybe not. I mean,", "tokens": [51120, 865, 13, 1779, 30, 7010, 13, 400, 370, 360, 436, 1949, 2969, 281, 312, 41504, 30, 2704, 406, 13, 286, 914, 11, 51408], "temperature": 0.0, "avg_logprob": -0.13513281449027684, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.007117669563740492}, {"id": 2463, "seek": 1256752, "start": 12588.4, "end": 12595.44, "text": " some of them make crazy macros and stuff like that. But but but what what you mentioned, Steve", "tokens": [51408, 512, 295, 552, 652, 3219, 7912, 2635, 293, 1507, 411, 300, 13, 583, 457, 457, 437, 437, 291, 2835, 11, 7466, 51760], "temperature": 0.0, "avg_logprob": -0.13513281449027684, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.007117669563740492}, {"id": 2464, "seek": 1259544, "start": 12595.44, "end": 12602.16, "text": " Jobs, is it's the bicycle for the mind that allows you to go faster, right? And so I think that as", "tokens": [50364, 29169, 11, 307, 309, 311, 264, 20888, 337, 264, 1575, 300, 4045, 291, 281, 352, 4663, 11, 558, 30, 400, 370, 286, 519, 300, 382, 50700], "temperature": 0.0, "avg_logprob": -0.12539479159569555, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.02674642950296402}, {"id": 2465, "seek": 1259544, "start": 12602.16, "end": 12607.28, "text": " we look forward, right? What is AI? I look at it as hopefully a new programming paradigm. It's like", "tokens": [50700, 321, 574, 2128, 11, 558, 30, 708, 307, 7318, 30, 286, 574, 412, 309, 382, 4696, 257, 777, 9410, 24709, 13, 467, 311, 411, 50956], "temperature": 0.0, "avg_logprob": -0.12539479159569555, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.02674642950296402}, {"id": 2466, "seek": 1259544, "start": 12607.28, "end": 12611.52, "text": " object-oriented programming, right? If you want to write a cat to texture, you don't use for loops.", "tokens": [50956, 2657, 12, 27414, 9410, 11, 558, 30, 759, 291, 528, 281, 2464, 257, 3857, 281, 8091, 11, 291, 500, 380, 764, 337, 16121, 13, 51168], "temperature": 0.0, "avg_logprob": -0.12539479159569555, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.02674642950296402}, {"id": 2467, "seek": 1259544, "start": 12612.32, "end": 12616.560000000001, "text": " Turns out that's not the right tool for the job, right? And so right now, unfortunately,", "tokens": [51208, 29524, 484, 300, 311, 406, 264, 558, 2290, 337, 264, 1691, 11, 558, 30, 400, 370, 558, 586, 11, 7015, 11, 51420], "temperature": 0.0, "avg_logprob": -0.12539479159569555, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.02674642950296402}, {"id": 2468, "seek": 1259544, "start": 12616.560000000001, "end": 12620.640000000001, "text": " because I mean, it's not unfortunate, but it's just kind of where where things are. AI is this", "tokens": [51420, 570, 286, 914, 11, 309, 311, 406, 17843, 11, 457, 309, 311, 445, 733, 295, 689, 689, 721, 366, 13, 7318, 307, 341, 51624], "temperature": 0.0, "avg_logprob": -0.12539479159569555, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.02674642950296402}, {"id": 2469, "seek": 1262064, "start": 12620.64, "end": 12625.84, "text": " weird, different thing that's not integrated into programming languages and normal tool chains and", "tokens": [50364, 3657, 11, 819, 551, 300, 311, 406, 10919, 666, 9410, 8650, 293, 2710, 2290, 12626, 293, 50624], "temperature": 0.0, "avg_logprob": -0.12857341049309062, "compression_ratio": 1.7269938650306749, "no_speech_prob": 0.005553924012929201}, {"id": 2470, "seek": 1262064, "start": 12625.84, "end": 12630.72, "text": " all the technology is really weird and doesn't work right. And you have to babysit it. And", "tokens": [50624, 439, 264, 2899, 307, 534, 3657, 293, 1177, 380, 589, 558, 13, 400, 291, 362, 281, 39764, 270, 309, 13, 400, 50868], "temperature": 0.0, "avg_logprob": -0.12857341049309062, "compression_ratio": 1.7269938650306749, "no_speech_prob": 0.005553924012929201}, {"id": 2471, "seek": 1262064, "start": 12630.72, "end": 12635.039999999999, "text": " every time you switch hardware, it's different. It shouldn't be that way. When you change that,", "tokens": [50868, 633, 565, 291, 3679, 8837, 11, 309, 311, 819, 13, 467, 4659, 380, 312, 300, 636, 13, 1133, 291, 1319, 300, 11, 51084], "temperature": 0.0, "avg_logprob": -0.12857341049309062, "compression_ratio": 1.7269938650306749, "no_speech_prob": 0.005553924012929201}, {"id": 2472, "seek": 1262064, "start": 12635.039999999999, "end": 12639.359999999999, "text": " when you fix that, suddenly, again, the tools technologies can be way easier to use. You", "tokens": [51084, 562, 291, 3191, 300, 11, 5800, 11, 797, 11, 264, 3873, 7943, 393, 312, 636, 3571, 281, 764, 13, 509, 51300], "temperature": 0.0, "avg_logprob": -0.12857341049309062, "compression_ratio": 1.7269938650306749, "no_speech_prob": 0.005553924012929201}, {"id": 2473, "seek": 1262064, "start": 12639.359999999999, "end": 12642.96, "text": " can start using them for many more things. And so that's that's why I would be excited about.", "tokens": [51300, 393, 722, 1228, 552, 337, 867, 544, 721, 13, 400, 370, 300, 311, 300, 311, 983, 286, 576, 312, 2919, 466, 13, 51480], "temperature": 0.0, "avg_logprob": -0.12857341049309062, "compression_ratio": 1.7269938650306749, "no_speech_prob": 0.005553924012929201}, {"id": 2474, "seek": 1262064, "start": 12643.68, "end": 12648.16, "text": " What kind of advice could you give to somebody in high school right now or maybe early college", "tokens": [51516, 708, 733, 295, 5192, 727, 291, 976, 281, 2618, 294, 1090, 1395, 558, 586, 420, 1310, 2440, 3859, 51740], "temperature": 0.0, "avg_logprob": -0.12857341049309062, "compression_ratio": 1.7269938650306749, "no_speech_prob": 0.005553924012929201}, {"id": 2475, "seek": 1264816, "start": 12648.16, "end": 12655.44, "text": " who's curious about programming and feeling like the world is changing really quickly here?", "tokens": [50364, 567, 311, 6369, 466, 9410, 293, 2633, 411, 264, 1002, 307, 4473, 534, 2661, 510, 30, 50728], "temperature": 0.0, "avg_logprob": -0.0753386733878372, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.004005149472504854}, {"id": 2476, "seek": 1264816, "start": 12655.44, "end": 12660.96, "text": " Yeah. Well, what kind of stuff to learn? What kind of stuff to work on? Should they finish college?", "tokens": [50728, 865, 13, 1042, 11, 437, 733, 295, 1507, 281, 1466, 30, 708, 733, 295, 1507, 281, 589, 322, 30, 6454, 436, 2413, 3859, 30, 51004], "temperature": 0.0, "avg_logprob": -0.0753386733878372, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.004005149472504854}, {"id": 2477, "seek": 1264816, "start": 12660.96, "end": 12665.44, "text": " Should they go work at a company? Should they build a thing? What do you think?", "tokens": [51004, 6454, 436, 352, 589, 412, 257, 2237, 30, 6454, 436, 1322, 257, 551, 30, 708, 360, 291, 519, 30, 51228], "temperature": 0.0, "avg_logprob": -0.0753386733878372, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.004005149472504854}, {"id": 2478, "seek": 1264816, "start": 12665.44, "end": 12670.0, "text": " Well, so I mean, one of the things I'd say is that you'll be most successful if you work on", "tokens": [51228, 1042, 11, 370, 286, 914, 11, 472, 295, 264, 721, 286, 1116, 584, 307, 300, 291, 603, 312, 881, 4406, 498, 291, 589, 322, 51456], "temperature": 0.0, "avg_logprob": -0.0753386733878372, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.004005149472504854}, {"id": 2479, "seek": 1264816, "start": 12670.0, "end": 12676.64, "text": " something you're excited by. And so don't get the book and read the book cover to cover and study", "tokens": [51456, 746, 291, 434, 2919, 538, 13, 400, 370, 500, 380, 483, 264, 1446, 293, 1401, 264, 1446, 2060, 281, 2060, 293, 2979, 51788], "temperature": 0.0, "avg_logprob": -0.0753386733878372, "compression_ratio": 1.7265917602996255, "no_speech_prob": 0.004005149472504854}, {"id": 2480, "seek": 1267664, "start": 12676.64, "end": 12681.519999999999, "text": " and memorize and recite and flashcard and go build something. Like go solve a problem. Go", "tokens": [50364, 293, 27478, 293, 39434, 293, 7319, 22259, 293, 352, 1322, 746, 13, 1743, 352, 5039, 257, 1154, 13, 1037, 50608], "temperature": 0.0, "avg_logprob": -0.11170359824201186, "compression_ratio": 1.8907849829351535, "no_speech_prob": 0.017432790249586105}, {"id": 2481, "seek": 1267664, "start": 12681.519999999999, "end": 12688.0, "text": " build the thing that you want to exist. Go build an app. Go train a model. Like go build something", "tokens": [50608, 1322, 264, 551, 300, 291, 528, 281, 2514, 13, 1037, 1322, 364, 724, 13, 1037, 3847, 257, 2316, 13, 1743, 352, 1322, 746, 50932], "temperature": 0.0, "avg_logprob": -0.11170359824201186, "compression_ratio": 1.8907849829351535, "no_speech_prob": 0.017432790249586105}, {"id": 2482, "seek": 1267664, "start": 12688.0, "end": 12692.4, "text": " and actually use it and set a goal for yourself. And if you do that, then you'll, you know,", "tokens": [50932, 293, 767, 764, 309, 293, 992, 257, 3387, 337, 1803, 13, 400, 498, 291, 360, 300, 11, 550, 291, 603, 11, 291, 458, 11, 51152], "temperature": 0.0, "avg_logprob": -0.11170359824201186, "compression_ratio": 1.8907849829351535, "no_speech_prob": 0.017432790249586105}, {"id": 2483, "seek": 1267664, "start": 12692.4, "end": 12696.72, "text": " there's a success. There's the adrenaline rush. There's the achievement. There's the unlock that", "tokens": [51152, 456, 311, 257, 2245, 13, 821, 311, 264, 35649, 9300, 13, 821, 311, 264, 15838, 13, 821, 311, 264, 11634, 300, 51368], "temperature": 0.0, "avg_logprob": -0.11170359824201186, "compression_ratio": 1.8907849829351535, "no_speech_prob": 0.017432790249586105}, {"id": 2484, "seek": 1267664, "start": 12696.72, "end": 12700.72, "text": " I think is where, you know, if you keep setting goals and you keep doing things and building", "tokens": [51368, 286, 519, 307, 689, 11, 291, 458, 11, 498, 291, 1066, 3287, 5493, 293, 291, 1066, 884, 721, 293, 2390, 51568], "temperature": 0.0, "avg_logprob": -0.11170359824201186, "compression_ratio": 1.8907849829351535, "no_speech_prob": 0.017432790249586105}, {"id": 2485, "seek": 1267664, "start": 12700.72, "end": 12705.76, "text": " things, learning by building is really powerful. In terms of career advice, I mean,", "tokens": [51568, 721, 11, 2539, 538, 2390, 307, 534, 4005, 13, 682, 2115, 295, 3988, 5192, 11, 286, 914, 11, 51820], "temperature": 0.0, "avg_logprob": -0.11170359824201186, "compression_ratio": 1.8907849829351535, "no_speech_prob": 0.017432790249586105}, {"id": 2486, "seek": 1270576, "start": 12705.76, "end": 12711.68, "text": " everybody's different. It's very hard to give generalized advice. I'll speak as a compiler", "tokens": [50364, 2201, 311, 819, 13, 467, 311, 588, 1152, 281, 976, 44498, 5192, 13, 286, 603, 1710, 382, 257, 31958, 50660], "temperature": 0.0, "avg_logprob": -0.07748878796895345, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.002630832139402628}, {"id": 2487, "seek": 1270576, "start": 12711.68, "end": 12718.880000000001, "text": " nerd. If everybody's going left, sometimes it's pretty cool to go right. And so just because", "tokens": [50660, 23229, 13, 759, 2201, 311, 516, 1411, 11, 2171, 309, 311, 1238, 1627, 281, 352, 558, 13, 400, 370, 445, 570, 51020], "temperature": 0.0, "avg_logprob": -0.07748878796895345, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.002630832139402628}, {"id": 2488, "seek": 1270576, "start": 12718.880000000001, "end": 12723.84, "text": " everybody's doing a thing, it doesn't mean you have to do the same thing and follow the herd.", "tokens": [51020, 2201, 311, 884, 257, 551, 11, 309, 1177, 380, 914, 291, 362, 281, 360, 264, 912, 551, 293, 1524, 264, 29484, 13, 51268], "temperature": 0.0, "avg_logprob": -0.07748878796895345, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.002630832139402628}, {"id": 2489, "seek": 1270576, "start": 12723.84, "end": 12730.56, "text": " In fact, I think that sometimes the most exciting paths through life lead to being curious about", "tokens": [51268, 682, 1186, 11, 286, 519, 300, 2171, 264, 881, 4670, 14518, 807, 993, 1477, 281, 885, 6369, 466, 51604], "temperature": 0.0, "avg_logprob": -0.07748878796895345, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.002630832139402628}, {"id": 2490, "seek": 1273056, "start": 12730.56, "end": 12736.48, "text": " things that nobody else actually focuses on, right? And turns out that understanding deeply", "tokens": [50364, 721, 300, 5079, 1646, 767, 16109, 322, 11, 558, 30, 400, 4523, 484, 300, 3701, 8760, 50660], "temperature": 0.0, "avg_logprob": -0.07873955842490508, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.28756624460220337}, {"id": 2491, "seek": 1273056, "start": 12736.48, "end": 12741.279999999999, "text": " parts of the problem that people want to take for granted makes you extremely valuable and", "tokens": [50660, 3166, 295, 264, 1154, 300, 561, 528, 281, 747, 337, 12344, 1669, 291, 4664, 8263, 293, 50900], "temperature": 0.0, "avg_logprob": -0.07873955842490508, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.28756624460220337}, {"id": 2492, "seek": 1273056, "start": 12741.279999999999, "end": 12746.72, "text": " specialized in ways that the herd is not. And so again, I mean, there's lots of rooms for", "tokens": [50900, 19813, 294, 2098, 300, 264, 29484, 307, 406, 13, 400, 370, 797, 11, 286, 914, 11, 456, 311, 3195, 295, 9396, 337, 51172], "temperature": 0.0, "avg_logprob": -0.07873955842490508, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.28756624460220337}, {"id": 2493, "seek": 1273056, "start": 12746.72, "end": 12751.039999999999, "text": " specialization, lots of rooms for generalists. There's lots of room for different kinds and", "tokens": [51172, 2121, 2144, 11, 3195, 295, 9396, 337, 2674, 1751, 13, 821, 311, 3195, 295, 1808, 337, 819, 3685, 293, 51388], "temperature": 0.0, "avg_logprob": -0.07873955842490508, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.28756624460220337}, {"id": 2494, "seek": 1273056, "start": 12751.039999999999, "end": 12756.08, "text": " parts of the problem. But I think that it's, you know, just because everybody's doing one thing", "tokens": [51388, 3166, 295, 264, 1154, 13, 583, 286, 519, 300, 309, 311, 11, 291, 458, 11, 445, 570, 2201, 311, 884, 472, 551, 51640], "temperature": 0.0, "avg_logprob": -0.07873955842490508, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.28756624460220337}, {"id": 2495, "seek": 1275608, "start": 12756.16, "end": 12761.44, "text": " doesn't mean you should necessarily do it. And now the herd is using Python. So if you want to", "tokens": [50368, 1177, 380, 914, 291, 820, 4725, 360, 309, 13, 400, 586, 264, 29484, 307, 1228, 15329, 13, 407, 498, 291, 528, 281, 50632], "temperature": 0.0, "avg_logprob": -0.11427435874938965, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.013002139516174793}, {"id": 2496, "seek": 1275608, "start": 12761.44, "end": 12769.2, "text": " be a rebel, go check out Mojo and help Chris and the rest of the world fight the arch nemesis", "tokens": [50632, 312, 257, 28293, 11, 352, 1520, 484, 3335, 5134, 293, 854, 6688, 293, 264, 1472, 295, 264, 1002, 2092, 264, 3912, 408, 5814, 271, 51020], "temperature": 0.0, "avg_logprob": -0.11427435874938965, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.013002139516174793}, {"id": 2497, "seek": 1275608, "start": 12769.2, "end": 12774.08, "text": " of complexity, because simple is beautiful. There you go. Because you're an incredible person.", "tokens": [51020, 295, 14024, 11, 570, 2199, 307, 2238, 13, 821, 291, 352, 13, 1436, 291, 434, 364, 4651, 954, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11427435874938965, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.013002139516174793}, {"id": 2498, "seek": 1275608, "start": 12774.08, "end": 12779.039999999999, "text": " You've been so kind to me ever since we met. You've been extremely supportive. I'm forever", "tokens": [51264, 509, 600, 668, 370, 733, 281, 385, 1562, 1670, 321, 1131, 13, 509, 600, 668, 4664, 14435, 13, 286, 478, 5680, 51512], "temperature": 0.0, "avg_logprob": -0.11427435874938965, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.013002139516174793}, {"id": 2499, "seek": 1275608, "start": 12779.039999999999, "end": 12784.32, "text": " grateful for that. Thank you for being who you are, for being legit, for being kind, for fighting", "tokens": [51512, 7941, 337, 300, 13, 1044, 291, 337, 885, 567, 291, 366, 11, 337, 885, 10275, 11, 337, 885, 733, 11, 337, 5237, 51776], "temperature": 0.0, "avg_logprob": -0.11427435874938965, "compression_ratio": 1.656140350877193, "no_speech_prob": 0.013002139516174793}, {"id": 2500, "seek": 1278432, "start": 12784.32, "end": 12792.4, "text": " this really interesting problem of how to make AI accessible to a huge number of people, a huge", "tokens": [50364, 341, 534, 1880, 1154, 295, 577, 281, 652, 7318, 9515, 281, 257, 2603, 1230, 295, 561, 11, 257, 2603, 50768], "temperature": 0.0, "avg_logprob": -0.08562290473062484, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0033750382717698812}, {"id": 2501, "seek": 1278432, "start": 12792.4, "end": 12797.68, "text": " number of devices. Yeah. Well, so Lex, you're a pretty special person too, right? And so I think", "tokens": [50768, 1230, 295, 5759, 13, 865, 13, 1042, 11, 370, 24086, 11, 291, 434, 257, 1238, 2121, 954, 886, 11, 558, 30, 400, 370, 286, 519, 51032], "temperature": 0.0, "avg_logprob": -0.08562290473062484, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0033750382717698812}, {"id": 2502, "seek": 1278432, "start": 12797.68, "end": 12802.16, "text": " that, you know, one of the funny things about you is that besides being curious and pretty damn", "tokens": [51032, 300, 11, 291, 458, 11, 472, 295, 264, 4074, 721, 466, 291, 307, 300, 11868, 885, 6369, 293, 1238, 8151, 51256], "temperature": 0.0, "avg_logprob": -0.08562290473062484, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0033750382717698812}, {"id": 2503, "seek": 1278432, "start": 12802.16, "end": 12807.6, "text": " smart, you're actually willing to push on things. And I think that you've got an agenda to like", "tokens": [51256, 4069, 11, 291, 434, 767, 4950, 281, 2944, 322, 721, 13, 400, 286, 519, 300, 291, 600, 658, 364, 9829, 281, 411, 51528], "temperature": 0.0, "avg_logprob": -0.08562290473062484, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0033750382717698812}, {"id": 2504, "seek": 1278432, "start": 12807.6, "end": 12813.199999999999, "text": " make the world think, which I think is a pretty good agenda. It's a pretty good one. Thank you so", "tokens": [51528, 652, 264, 1002, 519, 11, 597, 286, 519, 307, 257, 1238, 665, 9829, 13, 467, 311, 257, 1238, 665, 472, 13, 1044, 291, 370, 51808], "temperature": 0.0, "avg_logprob": -0.08562290473062484, "compression_ratio": 1.7275985663082438, "no_speech_prob": 0.0033750382717698812}, {"id": 2505, "seek": 1281320, "start": 12813.2, "end": 12818.0, "text": " much for talking, Chris. Yeah, thanks, Lex. Thanks for listening to this conversation with Chris", "tokens": [50364, 709, 337, 1417, 11, 6688, 13, 865, 11, 3231, 11, 24086, 13, 2561, 337, 4764, 281, 341, 3761, 365, 6688, 50604], "temperature": 0.0, "avg_logprob": -0.14048138330149096, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.012226334773004055}, {"id": 2506, "seek": 1281320, "start": 12818.0, "end": 12823.2, "text": " Ladner. To support this podcast, please check out our sponsors in the description. And now,", "tokens": [50604, 12106, 1193, 13, 1407, 1406, 341, 7367, 11, 1767, 1520, 484, 527, 22593, 294, 264, 3855, 13, 400, 586, 11, 50864], "temperature": 0.0, "avg_logprob": -0.14048138330149096, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.012226334773004055}, {"id": 2507, "seek": 1281320, "start": 12823.2, "end": 12830.16, "text": " let me leave you some words from Isaac Asimov. I do not fear computers. I fear the lack of them.", "tokens": [50864, 718, 385, 1856, 291, 512, 2283, 490, 22505, 1018, 332, 5179, 13, 286, 360, 406, 4240, 10807, 13, 286, 4240, 264, 5011, 295, 552, 13, 51212], "temperature": 0.0, "avg_logprob": -0.14048138330149096, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.012226334773004055}, {"id": 2508, "seek": 1281320, "start": 12831.6, "end": 12834.480000000001, "text": " Thank you for listening and hope to see you next time.", "tokens": [51284, 1044, 291, 337, 4764, 293, 1454, 281, 536, 291, 958, 565, 13, 51428], "temperature": 0.0, "avg_logprob": -0.14048138330149096, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.012226334773004055}], "language": "en"}