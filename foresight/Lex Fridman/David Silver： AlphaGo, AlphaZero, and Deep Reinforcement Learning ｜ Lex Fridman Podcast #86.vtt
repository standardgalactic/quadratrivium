WEBVTT

00:00.000 --> 00:04.100
The following is a conversation with David Silver, who leads the Reinforcement Learning

00:04.100 --> 00:11.140
Research Group at DeepMind, and was the lead researcher on AlphaGo, AlphaZero, and co-led

00:11.140 --> 00:15.680
the AlphaStar and MuZero efforts, and a lot of important work in reinforcement learning

00:15.680 --> 00:17.180
in general.

00:17.180 --> 00:23.100
I believe AlphaZero is one of the most important accomplishments in the history of artificial

00:23.100 --> 00:28.900
intelligence, and David is one of the key humans who brought AlphaZero to life together

00:28.900 --> 00:32.060
with a lot of other great researchers at DeepMind.

00:32.060 --> 00:34.500
He's humble, kind, and brilliant.

00:34.500 --> 00:38.660
We were both jet lagged, but didn't care and made it happen.

00:38.660 --> 00:43.460
It was a pleasure and truly an honor to talk with David.

00:43.460 --> 00:47.100
This conversation was recorded before the outbreak of the pandemic.

00:47.100 --> 00:51.900
For everyone feeling the medical, psychological, and financial burden of this crisis, I'm

00:51.900 --> 00:53.540
sending love your way.

00:53.540 --> 00:57.860
Stay strong, or in this together, we'll beat this thing.

00:57.860 --> 01:00.100
This is the Artificial Intelligence Podcast.

01:00.100 --> 01:05.180
If you enjoy it, subscribe on YouTube, review it with 5 stars on Apple Podcasts, support

01:05.180 --> 01:12.220
on Patreon, or simply connect with me on Twitter at Lex Freedman, spelled F-R-I-D-M-A-N.

01:12.220 --> 01:16.540
As usual, I'll do a few minutes of ads now and never any ads in the middle that can break

01:16.540 --> 01:18.420
the flow of the conversation.

01:18.420 --> 01:22.780
I hope that works for you and doesn't hurt the listening experience.

01:22.780 --> 01:24.060
Quick summary of the ads.

01:24.060 --> 01:25.060
Two sponsors.

01:25.060 --> 01:27.580
Masterclass and Cash App.

01:27.580 --> 01:32.700
Please consider supporting the podcast by signing up to masterclass and masterclass.com

01:32.700 --> 01:38.940
slash lex and downloading Cash App and using code Lex Podcast.

01:38.940 --> 01:43.340
This show is presented by Cash App, the number one finance app in the App Store.

01:43.340 --> 01:47.220
When you get it, use code Lex Podcast.

01:47.220 --> 01:51.540
Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with

01:51.540 --> 01:53.980
as little as $1.

01:53.980 --> 01:58.020
Since Cash App allows you to buy Bitcoin, let me mention that cryptocurrency in the

01:58.020 --> 02:01.420
context of the history of money is fascinating.

02:01.420 --> 02:05.380
I recommend Ascent of Money as a great book on this history.

02:05.380 --> 02:09.740
Debits and credits on ledgers started around 30,000 years ago.

02:09.740 --> 02:15.940
The US Dollar created over 200 years ago and Bitcoin, the first decentralized cryptocurrency,

02:15.940 --> 02:18.740
released just over 10 years ago.

02:18.740 --> 02:24.020
So given that history, cryptocurrency is still very much in its early days of development,

02:24.020 --> 02:29.180
but it's still aiming to and just might redefine the nature of money.

02:29.180 --> 02:33.460
So again, if you get Cash App from the App Store or Google Play and use the code Lex

02:33.460 --> 02:39.700
Podcast, you get $10 and Cash App will also donate $10 the first, an organization that

02:39.700 --> 02:45.020
is helping to advance robotics and STEM education for young people around the world.

02:45.020 --> 02:47.180
This show is sponsored by Masterclass.

02:47.180 --> 02:51.660
Sign up at masterclass.com slash Lex to get a discount and to support this podcast.

02:51.660 --> 02:56.780
In fact, for a limited time now, if you sign up for an All Access Pass for a year, you

02:56.780 --> 03:01.300
get to get another All Access Pass to share with a friend.

03:01.300 --> 03:02.740
Buy one, get one free.

03:02.740 --> 03:06.420
When I first heard about Masterclass, I thought it was too good to be true.

03:06.420 --> 03:13.020
For $180 a year, you get an All Access Pass to watch courses from to list some of my favorites.

03:13.020 --> 03:18.220
Chris Hatfield on space exploration, Neil deGrasse Tyson on scientific thinking communication,

03:18.220 --> 03:24.780
Will Wright, the creator of SimCity and Sims, on game design, Jane Goodall on conservation,

03:24.780 --> 03:31.020
Carl Santana on guitar, his song Europa could be the most beautiful guitar song ever written.

03:31.020 --> 03:35.740
Gary Kasparov on chess, Daniel Nagrano on poker, and many, many more.

03:35.740 --> 03:39.460
Chris Hatfield explaining how rockets work and the experience of being launched into

03:39.460 --> 03:41.740
space alone is worth the money.

03:41.740 --> 03:46.940
For me, the key is to not be overwhelmed by the abundance of choice, pick three courses

03:46.940 --> 03:49.740
you want to complete, watch each of them all the way through.

03:49.740 --> 03:53.660
It's not that long, but it's an experience that will stick with you for a long time.

03:53.660 --> 03:54.660
I promise.

03:54.660 --> 03:56.860
It's easily worth the money.

03:56.860 --> 03:59.340
You can watch it on basically any device.

03:59.340 --> 04:03.940
Once again, sign up on masterclass.com slash Lex to get a discount and to support this

04:03.940 --> 04:05.740
podcast.

04:05.740 --> 04:09.900
And now here's my conversation with David Silver.

04:09.900 --> 04:14.020
What was the first program you ever written and what programming language?

04:14.020 --> 04:15.020
Do you remember?

04:15.020 --> 04:22.140
I remember very clearly, yeah, my parents brought home this BBC model B microcomputer.

04:22.140 --> 04:24.220
It was just this fascinating thing to me.

04:24.220 --> 04:30.100
I was about seven years old and couldn't resist just playing around with it.

04:30.100 --> 04:37.220
So I think first program ever was writing my name out in different colors and getting

04:37.220 --> 04:39.820
it to loop and repeat that.

04:40.380 --> 04:44.580
There was something magical about that, which just led to more and more.

04:44.580 --> 04:46.820
How did you think about computers back then?

04:46.820 --> 04:51.700
The magical aspect of it, that you can write a program and there's this thing that you

04:51.700 --> 04:57.660
just gave birth to that's able to create visual elements and live in its own.

04:57.660 --> 05:00.100
Or did you not think of it in those romantic notions?

05:00.100 --> 05:02.460
Was it more like, oh, that's cool.

05:02.460 --> 05:05.380
I can solve some puzzles.

05:05.380 --> 05:06.980
It was always more than solving puzzles.

05:06.980 --> 05:14.340
It was something where there was this limitless possibilities once you have a computer in

05:14.340 --> 05:15.340
front of you.

05:15.340 --> 05:16.340
You can do anything with it.

05:16.340 --> 05:18.100
I used to play with Lego with the same feeling.

05:18.100 --> 05:21.540
You can make anything you want out of Lego, but even more so with a computer.

05:21.540 --> 05:24.660
You're not constrained by the amount of kit you've got.

05:24.660 --> 05:29.020
And so I was fascinated by it and started pulling out the user guide and the advanced

05:29.020 --> 05:30.820
user guide and then learning.

05:30.820 --> 05:34.700
So I started in basic and then later 6502.

05:34.740 --> 05:40.180
My father also became interested in this machine and gave up his career to go back to school

05:40.180 --> 05:47.100
and study for a master's degree in artificial intelligence, funnily enough, at Essex University

05:47.100 --> 05:48.740
when I was seven.

05:48.740 --> 05:52.100
So I was exposed to those things at an early age.

05:52.100 --> 05:57.780
He showed me how to program in Prologue and do things like querying your family tree.

05:57.780 --> 06:04.300
And those are some of my earliest memories of trying to figure things out on a computer.

06:04.340 --> 06:09.020
Those are the early steps in computer science programming, but when did you first fall in

06:09.020 --> 06:14.900
love with artificial intelligence or with the ideas, the dreams of AI?

06:14.900 --> 06:19.140
I think it was really when I went to study at university.

06:19.140 --> 06:24.260
So I was an undergrad at Cambridge and studying computer science.

06:24.260 --> 06:29.660
And I really started to question, you know, what really are the goals?

06:29.660 --> 06:30.660
What's the goal?

06:30.660 --> 06:33.060
Where do we want to go with computer science?

06:33.060 --> 06:42.300
And it seemed to me that the only step of major significance to take was to try and recreate

06:42.300 --> 06:44.300
something akin to human intelligence.

06:44.300 --> 06:47.860
If we could do that, that would be a major leap forward.

06:47.860 --> 06:52.500
And that idea certainly wasn't the first to have it, but it, you know, nestled within

06:52.500 --> 06:58.700
me somewhere and became like a bug, you know, I really wanted to crack that problem.

06:58.940 --> 07:02.980
So you thought it was, like, you had a notion that this is something that human beings can do,

07:02.980 --> 07:07.300
that it is possible to create an intelligent machine?

07:07.300 --> 07:13.420
Well, I mean, unless you believe in something metaphysical, then what are our brains doing?

07:13.420 --> 07:21.580
Well, at some level, their information processing systems, which are able to take whatever

07:21.580 --> 07:25.620
information is in there, transform it through some form of program and produce some kind

07:25.620 --> 07:29.660
of output, which enables that human being to do all the amazing things that they can

07:29.660 --> 07:32.300
do in this incredible world.

07:32.300 --> 07:38.540
So then, do you remember the first time you've written a program that, because you also had

07:38.540 --> 07:42.380
an interest in games, do you remember the first time you were in a program that beat

07:42.380 --> 07:51.740
you in a game, that more beat you at anything, sort of achieved super David Silver level

07:51.740 --> 07:54.460
performance?

07:54.460 --> 07:56.580
So I used to work in the games industry.

07:56.580 --> 08:01.420
So for five years, I programmed games for my first job.

08:01.420 --> 08:05.980
So it was an amazing opportunity to get involved in a startup company.

08:05.980 --> 08:12.260
And so I was involved in building AI at that time.

08:12.260 --> 08:19.580
And so for sure, there was a sense of building, handcrafted, what people used to call AI in

08:19.580 --> 08:23.820
the games industry, which I think is not really what we might think of as AI in its fullest

08:24.180 --> 08:31.460
sense, but something which is able to take actions in a way which makes things interesting

08:31.460 --> 08:35.220
and challenging for the human player.

08:35.220 --> 08:40.380
And at that time, I was able to build these handcrafted agents, which in certain limited

08:40.380 --> 08:46.740
cases could do things which were able to do better than me, but mostly in these kind of

08:46.740 --> 08:51.260
twitch-like scenarios where they were able to do things faster or because they had some

08:51.260 --> 08:55.420
pattern which was able to exploit repeatedly.

08:55.420 --> 09:01.780
I think if we're talking about real AI, the first experience for me came after that when

09:01.780 --> 09:08.380
I realized that this path I was on wasn't taking me towards, it wasn't dealing with

09:08.380 --> 09:14.740
that bug which I still had inside me to really understand intelligence and try and solve it.

09:14.740 --> 09:20.860
Everything people were doing in games was short-term fixes rather than long-term vision.

09:20.980 --> 09:26.660
So I went back to study for my PhD, which was, finally enough, trying to apply reinforcement

09:26.660 --> 09:28.540
learning to the game of Go.

09:28.540 --> 09:33.860
And I built my first Go program using reinforcement learning, a system which would, by trial and

09:33.860 --> 09:40.540
error, play against itself and was able to learn which patterns were actually helpful

09:40.540 --> 09:44.740
to predict whether it was going to win or lose the game and then choose the moves that

09:44.740 --> 09:48.460
led to the combination of patterns that would mean that you're more likely to win.

09:48.500 --> 09:50.540
That system, that system beat me.

09:51.540 --> 09:52.860
And how did that make you feel?

09:53.380 --> 09:54.380
It made me feel good.

09:57.860 --> 10:03.780
It's a mix of a sort of excitement and was there a tinge of sort of almost like a fearful

10:03.780 --> 10:04.780
awe?

10:04.780 --> 10:17.780
It's like in 2001 Space Odyssey kind of realizing that you've created something that's achieved

10:17.820 --> 10:21.220
human-level intelligence in this one particular little task.

10:21.220 --> 10:24.460
And in that case, I suppose neural networks weren't involved.

10:24.460 --> 10:26.940
There were no neural networks in those days.

10:26.940 --> 10:33.260
This was pre-deep learning revolution, but it was a principled self-learning system based

10:33.260 --> 10:40.340
on a lot of the principles which people still use in deep reinforcement learning.

10:40.340 --> 10:41.340
How did I feel?

10:41.340 --> 10:50.100
I think I found it immensely satisfying that a system which was able to learn from first

10:50.100 --> 10:56.380
principles for itself was able to reach the point that it was understanding this domain

10:56.380 --> 11:00.100
better than I could and able to outwit me.

11:00.100 --> 11:01.580
I don't think it was a sense of awe.

11:01.580 --> 11:09.020
It was a sense that satisfaction, that something I felt should work, had worked.

11:09.060 --> 11:14.580
To me AlphaGo, and I don't know how else to put it, but to me AlphaGo and AlphaGo Zero

11:14.580 --> 11:20.660
mastering the game of Go is, again, to me the most profound and inspiring moment in the

11:20.660 --> 11:23.620
history of artificial intelligence.

11:23.620 --> 11:26.740
So you're one of the key people behind this achievement.

11:26.740 --> 11:32.700
And I'm Russian, so I really felt the first sort of seminal achievement when Deep Blue

11:32.700 --> 11:36.940
be Gare Kasparov in 1997.

11:36.940 --> 11:42.740
So as far as I know, the AI community at that point largely saw the game of Go as unbeatable

11:42.740 --> 11:49.100
in AI using the state-of-the-art to brute-force search methods.

11:49.100 --> 11:54.940
Even if you consider, at least the way I saw it, even if you consider arbitrary exponential

11:54.940 --> 12:02.660
scaling of compute, Go would still not be solvable, hence why it was thought to be impossible.

12:02.660 --> 12:09.580
So given that the game of Go was impossible to master, when was the dream for you, you

12:09.580 --> 12:14.580
just mentioned your PhD thesis of building the system that plays Go, when was the dream

12:14.580 --> 12:20.300
for you that you could actually build a computer program that achieves the world class, not

12:20.300 --> 12:25.180
necessarily beats the world champion, but achieves that kind of level of playing Go?

12:25.180 --> 12:26.180
First of all, thank you.

12:26.180 --> 12:28.700
That was very kind words.

12:28.700 --> 12:34.660
Funnily enough, I just came from a panel where I was actually in a conversation with

12:34.660 --> 12:39.540
Gare Kasparov and Murray Campbell, who was the author of Deep Blue, and it was their

12:39.540 --> 12:45.180
first meeting together since the match, so I'm just acquired yesterday, so I'm literally

12:45.180 --> 12:47.500
fresh from that experience.

12:47.500 --> 12:51.980
So these are amazing moments when they happen, but where did it all start?

12:51.980 --> 12:56.260
Well, for me, it started when I became fascinated in the game of Go.

12:56.260 --> 13:01.860
So Go, for me, I've grown up playing games, I've always had a fascination in board games.

13:01.860 --> 13:06.220
I played chess as a kid, I played Scrabble as a kid.

13:06.220 --> 13:10.620
When I was at university, I discovered the game of Go, and to me, it just blew all of

13:10.620 --> 13:11.620
those other games out of the water.

13:11.620 --> 13:17.980
It was just so deep and profound in its complexity with endless levels to it.

13:17.980 --> 13:27.380
What I discovered was that I could devote endless hours to this game, and I knew in

13:27.380 --> 13:30.740
my heart of hearts that no matter how many hours I would devote to it, I would never

13:30.740 --> 13:38.180
become a grandmaster, or there was another path, and the other path was to try and understand

13:38.180 --> 13:42.660
how you could get some other intelligence to play this game better than I would be able

13:42.660 --> 13:43.660
to.

13:43.700 --> 13:49.300
So even in those days, I had this idea that, what if it was possible to build a program

13:49.300 --> 13:51.220
that could crack this?

13:51.220 --> 13:57.380
And as I started to explore the domain, I discovered that this was really the domain

13:57.380 --> 14:04.580
where people felt deeply that if progress could be made in Go, it would really mean

14:04.580 --> 14:06.420
a giant leap forward for AI.

14:06.420 --> 14:11.060
It was the challenge where all other approaches had failed.

14:11.060 --> 14:16.820
This is coming out of the era you mentioned, which was in some sense the golden era for

14:16.820 --> 14:20.020
the classical methods of AI, like heuristic search.

14:20.020 --> 14:26.780
In the 90s, they all fell one after another, not just chess with deep blue, but checkers,

14:26.780 --> 14:28.980
batgammon, Othello.

14:28.980 --> 14:36.620
There were numerous cases where systems built on top of heuristic search methods with these

14:36.620 --> 14:40.700
high-performance systems had been able to defeat the human world champion in each of

14:40.700 --> 14:42.100
those domains.

14:42.100 --> 14:49.380
And yet, in that same time period, there was a million-dollar prize available for the

14:49.380 --> 14:52.980
game of Go, for the first system to be a human professional player.

14:52.980 --> 14:58.500
And at the end of that time period, at year 2000, when the prize expired, the strongest

14:58.500 --> 15:02.820
Go program in the world was defeated by a nine-year-old child.

15:02.820 --> 15:06.940
When that nine-year-old child was giving nine free moves to the computer at the start of

15:06.940 --> 15:09.980
the game to try and even things up.

15:09.980 --> 15:18.260
And computer Go expert beat that same strongest program with 29 handicap stones, 29 free moves.

15:18.260 --> 15:23.940
So that's what the state of affairs was when I became interested in this problem in around

15:23.940 --> 15:29.620
2003 when I started working on computer Go.

15:29.620 --> 15:30.620
There was nothing.

15:30.620 --> 15:36.660
There was just very, very little in the way of progress towards meaningful performance

15:36.660 --> 15:39.300
again at anything approaching human level.

15:39.300 --> 15:45.060
And so it wasn't through lack of effort people have tried many, many things.

15:45.060 --> 15:50.860
And so there was a strong sense that something different would be required for Go than had

15:50.860 --> 15:54.340
been needed for all of these other domains where AI had been successful.

15:54.340 --> 16:00.900
And maybe the single clearest example is that Go, unlike those other domains, had this kind

16:00.900 --> 16:07.100
of intuitive property that a Go player would look at a position and say, hey, here's this

16:07.100 --> 16:09.780
mess of black and white stones.

16:09.780 --> 16:15.980
But from this mess, oh, I can predict that this part of the board has become my territory,

16:15.980 --> 16:19.940
this part of the board has become your territory, and I've got this overall sense that I'm going

16:19.940 --> 16:22.500
to win and that this is about the right move to play.

16:22.500 --> 16:28.300
And that intuitive sense of judgment of being able to evaluate what's going on in a position,

16:28.300 --> 16:32.900
it was pivotal to humans being able to play this game and something that people had no

16:32.900 --> 16:35.180
idea how to put into computers.

16:35.180 --> 16:40.020
So this question of how to evaluate a position, how to come up with these intuitive judgments

16:40.020 --> 16:48.380
was the key reason why Go was so hard in addition to its enormous search space and the reason

16:48.380 --> 16:53.100
why methods which had succeeded so well elsewhere failed in Go.

16:53.100 --> 16:59.060
And so people really felt deep down that in order to crack Go, we would need to get something

16:59.060 --> 17:00.580
akin to human intuition.

17:00.580 --> 17:06.060
And if we got something akin to human intuition, we'd be able to solve many, many more problems

17:06.060 --> 17:07.060
in AI.

17:07.060 --> 17:11.260
So for me, that was the moment where it's like, okay, this is not just about playing

17:11.260 --> 17:12.260
the game of Go.

17:12.260 --> 17:13.740
This is about something profound.

17:13.740 --> 17:17.820
And it was back to that bug which had been itching me all those years.

17:17.820 --> 17:22.900
This is the opportunity to do something meaningful and transformative and I guess a dream was

17:22.900 --> 17:23.900
born.

17:23.900 --> 17:25.420
That's a really interesting way to put it.

17:25.420 --> 17:30.900
So almost this realization that you need to find formulate Go as a kind of a prediction

17:30.900 --> 17:34.780
problem versus a search problem was the intuition.

17:34.780 --> 17:43.580
I mean, maybe that's the wrong crude term, but to give it the ability to kind of intuit

17:43.580 --> 17:47.140
things about positional structure of the board.

17:47.140 --> 17:51.060
Now, okay, but what about the learning part of it?

17:51.900 --> 17:57.540
Did you have a sense that learning has to be part of the system?

17:57.540 --> 18:03.380
Again, something that hasn't really, as far as I think, except with TD Gammon and the

18:03.380 --> 18:08.820
90s with RL a little bit, hasn't been part of those day-to-day art game-playing systems?

18:08.820 --> 18:15.180
So I strongly felt that learning would be necessary and that's why my PhD topic back

18:15.180 --> 18:20.220
then was trying to apply reinforcement learning to the game of Go.

18:20.380 --> 18:26.060
I'm not just learning of any type, but I felt that the only way to really have a system

18:26.060 --> 18:31.140
to progress beyond human levels of performance wouldn't just be to mimic how humans do it,

18:31.140 --> 18:34.020
but to understand for themselves.

18:34.020 --> 18:39.140
How else can a machine hope to understand what's going on except through learning?

18:39.140 --> 18:40.540
If you're not learning, what else are you doing?

18:40.540 --> 18:45.540
Well, you're putting all the knowledge into the system and that just feels like something

18:45.540 --> 18:52.300
which decades of AI have told us is maybe not a dead end, but certainly has a ceiling

18:52.300 --> 18:53.300
to the capabilities.

18:53.300 --> 18:56.580
It's known as the knowledge acquisition bottleneck.

18:56.580 --> 19:01.060
The more you try to put into something, the more brittle the system becomes.

19:01.060 --> 19:02.860
So you just have to have learning.

19:02.860 --> 19:03.860
You have to have learning.

19:03.860 --> 19:08.980
That's the only way you're going to be able to get a system which has sufficient knowledge

19:08.980 --> 19:14.180
in it, millions and millions of pieces of knowledge, billions, trillions, of a form

19:14.220 --> 19:17.620
that it can actually apply for itself and understand how those billions and trillions

19:17.620 --> 19:22.380
of pieces of knowledge can be leveraged in a way which will actually lead it towards

19:22.380 --> 19:26.340
its goal without conflict or other issues.

19:26.340 --> 19:27.340
Yeah.

19:27.340 --> 19:33.580
I mean, if I put myself back in that time, I just wouldn't think like that without a

19:33.580 --> 19:34.900
good demonstration of RL.

19:34.900 --> 19:42.780
I would think more in the symbolic AI, like the not learning, but sort of a simulation

19:42.780 --> 19:50.500
of knowledge base, like a growing knowledge base, but it would still be sort of pattern-based,

19:50.500 --> 19:55.740
like basically have little rules that you kind of assemble together into a large knowledge

19:55.740 --> 19:56.740
base.

19:56.740 --> 19:59.900
Well, in a sense, that was the state of the art back then.

19:59.900 --> 20:05.460
So if you look at the Go programs which had been competing for this prize I mentioned,

20:05.460 --> 20:11.300
they were an assembly of different specialized systems, some of which used huge amounts of

20:11.340 --> 20:16.300
human knowledge to describe how you should play the opening, how you should all the different

20:16.300 --> 20:23.700
patterns that were required to play well in the game of Go, end game theory, combinatorial

20:23.700 --> 20:29.260
game theory, and combined with more principled search-based methods which were trying to

20:29.260 --> 20:36.940
solve for particular sub-parts of the game, like life and death, connecting groups together,

20:36.940 --> 20:42.460
all these amazing sub-problems that just emerged in the game of Go, there were different pieces

20:42.460 --> 20:49.380
all put together into this collage which together would try and play against a human.

20:49.380 --> 20:56.300
And although not all of the pieces were handcrafted, the overall effect was nevertheless still

20:56.300 --> 21:00.340
brittle and it was hard to make all these pieces work well together.

21:00.340 --> 21:05.500
And so really what I was pressing for and the main innovation of the approach I took

21:05.580 --> 21:11.900
was to go back to first principles and say, well, let's back off that and try and find a

21:11.900 --> 21:18.540
principled approach where the system can learn for itself just from the outcome, like, you know,

21:18.540 --> 21:22.620
learn for itself if you try something, did that help or did it not help?

21:22.620 --> 21:27.820
And only through that procedure can you arrive at knowledge which is verified,

21:27.820 --> 21:32.060
the system has to verify it for itself, not relying on any other third party to say this

21:32.060 --> 21:39.020
is right or this is wrong. And so that principle was already very important in those days,

21:39.020 --> 21:42.220
that unfortunately we were missing some important pieces back then.

21:43.180 --> 21:49.020
So before we dive into maybe discussing the beauty of reinforcement learning,

21:49.020 --> 21:55.340
let's take a step back, we kind of skipped it a bit, but the rules of the game of Go,

21:55.900 --> 22:06.300
what the elements of it perhaps contrasting to chess that sort of you really enjoy as a human

22:06.300 --> 22:12.300
being and also that make it really difficult as a AI machine learning problem.

22:12.940 --> 22:19.020
So the game of Go has remarkably simple rules. In fact, so simple that people have speculated

22:19.020 --> 22:23.660
that if we were to meet alien life at some point that we wouldn't be able to communicate with them,

22:23.660 --> 22:27.660
but we would be able to play Go with them, they probably have discovered the same ruleset.

22:28.860 --> 22:34.140
So the game is played on a 19 by 19 grid, and you play on the intersections of the grid and

22:34.140 --> 22:40.300
the players take turns. And the aim of the game is very simple, it's to surround as much territory

22:40.300 --> 22:45.340
as you can as many of these intersections with your stones and to surround more than your opponent

22:45.340 --> 22:50.380
does. And the only nuance to the game is that if you fully surround your opponent's piece,

22:50.380 --> 22:53.500
then you get to capture it and remove it from the board and it counts as your own territory.

22:54.300 --> 22:59.020
Now, from those very simple rules, immense complexity arises. There's kind of profound

22:59.020 --> 23:05.900
strategies in how to surround territory, how to kind of trade off between making solid territory

23:05.900 --> 23:10.780
yourself now, compared to building up influence that will help you acquire territory later in

23:10.780 --> 23:14.060
the game, how to connect groups together, how to keep your own groups alive,

23:14.220 --> 23:20.620
which patterns of stones are most useful compared to others.

23:21.340 --> 23:27.020
There's just immense knowledge and human Go players have played this game for,

23:27.020 --> 23:31.260
it was discovered thousands of years ago, and human Go players have built up this immense

23:31.260 --> 23:37.180
knowledge base over the years. It's studied very deeply and played by something like 50 million

23:37.180 --> 23:42.700
players across the world, mostly in China, Japan and Korea, where it's an important part of the

23:42.700 --> 23:48.380
culture, so much so that it's considered one of the four ancient arts that was required by

23:48.380 --> 23:53.180
Chinese scholars. So, there's a deep history there. But there's interesting quality. So,

23:53.740 --> 23:59.260
if I were to compare to chess, chess is in the same way as it is in Chinese culture for Go,

23:59.260 --> 24:05.980
and chess in Russia is also considered one of the sacred arts. So, if we contrast Go with

24:05.980 --> 24:11.260
chess, there's interesting qualities about Go. Maybe you can correct me if I'm wrong, but the

24:12.220 --> 24:21.340
evaluation of a particular static board is not as reliable. In chess, you can kind of assign

24:21.340 --> 24:27.500
points to the different units, and it's kind of a pretty good measure of who's winning, who's

24:27.500 --> 24:33.260
losing. It's not so clear. So, in the game of Go, you find yourself in a situation where

24:33.260 --> 24:38.300
both players have played the same number of stones, actually captures a strong level of play

24:38.300 --> 24:41.740
happen very rarely, which means that at any moment in the game, you've got the same number

24:41.740 --> 24:46.220
of white stones and black stones. And the only thing which differentiates how well you're doing

24:46.220 --> 24:51.740
is this intuitive sense of where are the territories ultimately going to form on this board?

24:52.380 --> 25:00.060
And if you look at the complexity of a real Go position, it's mind-boggling that kind of

25:00.060 --> 25:05.100
question of what will happen in 300 moves from now when you see just a scattering of 20

25:05.100 --> 25:13.180
white and black stones intermingled. And so, that challenge is the reason why

25:13.180 --> 25:18.300
position evaluation is so hard in Go compared to other games. In addition to that, it has an

25:18.300 --> 25:24.620
enormous search space. So, there's around 10 to 170 positions in the game of Go. That's an

25:24.620 --> 25:30.380
astronomical number. And that search space is so great that traditional heuristic search methods

25:30.380 --> 25:35.660
that were so successful and things like Deep Blue and chess programs just kind of fall over in Go.

25:37.420 --> 25:43.820
Which point did reinforcement learning enter your life, your research life, your way of thinking?

25:43.820 --> 25:48.620
We just talked about learning, but reinforcement learning is a very particular kind of learning,

25:49.580 --> 25:55.260
one that's both philosophically sort of profound, but also one that's pretty difficult to get to

25:55.260 --> 26:00.700
work as if you look back in the early days. So, when did that enter your life and how did

26:00.700 --> 26:06.940
that work progress? So, I had just finished working in the games industry at this startup

26:06.940 --> 26:14.780
company. And I took a year out to discover for myself exactly which path I wanted to take. I

26:14.780 --> 26:19.740
knew I wanted to study intelligence, but I wasn't sure what that meant at that stage. I really

26:19.740 --> 26:23.500
didn't feel I had the tools to decide on exactly which path I wanted to follow.

26:23.580 --> 26:31.100
So, during that year, I read a lot. And one of the things I read was Saturn and Bartow,

26:31.100 --> 26:38.220
the sort of seminal textbook on an introduction to reinforcement learning. And when I read that

26:38.220 --> 26:46.700
textbook, I just had this resonating feeling that this is what I understood intelligence to be.

26:47.660 --> 26:54.540
And this was the path that I felt would be necessary to go down to make progress in AI.

26:55.740 --> 27:03.740
So, I got in touch with Rich Saturn and asked him if he would be interested in supervising me

27:03.740 --> 27:15.420
on a PhD thesis in computer go. And he basically said that if he's still alive, he'd be happy to.

27:16.860 --> 27:22.300
But unfortunately, he'd been struggling with very serious cancer for some years. And he really

27:22.300 --> 27:27.420
wasn't confident at that stage that he'd even be around to see the end of it. But fortunately,

27:27.420 --> 27:32.940
that part of the story worked out very happily. And I found myself out there in Alberta. They've

27:32.940 --> 27:39.020
got a great games group out there with a history of fantastic work in board games as well as Rich

27:39.020 --> 27:44.380
Saturn, the father of RL. So, it was the natural place for me to go in some sense to study this

27:44.380 --> 27:55.020
question. And the more I looked into it, the more strongly I felt that this wasn't just the path to

27:55.020 --> 27:59.740
progress in computer go. But really, this was the thing I'd been looking for. This was

28:03.740 --> 28:12.780
really an opportunity to frame what intelligence means. What are the goals of AI in a single

28:13.100 --> 28:17.180
problem definition such that if we're able to solve that clear single problem definition,

28:18.700 --> 28:24.780
in some sense, we've cracked the problem of AI? So, to you, reinforcement learning ideas,

28:24.780 --> 28:31.260
at least sort of echoes of it, would be at the core of intelligence. It is at the core of intelligence.

28:31.260 --> 28:36.540
And if we ever create a human level intelligence system, it would be at the core of that kind of

28:36.540 --> 28:41.660
system. Let me say it this way that I think it's helpful to separate out the problem from the solution.

28:42.300 --> 28:49.580
So, I see the problem of intelligence, I would say it can be formalized as the reinforcement

28:49.580 --> 28:56.060
learning problem. And that that formalization is enough to capture most, if not all of the things

28:56.060 --> 29:01.660
that we mean by intelligence, that they can all be brought within this framework and gives us

29:01.660 --> 29:08.540
a way to access them in a meaningful way that allows us as scientists to understand intelligence

29:08.540 --> 29:16.220
and us as computer scientists to build them. And so, in that sense, I feel that it gives us a path,

29:16.220 --> 29:24.860
maybe not the only path, but a path towards AI. And so, do I think that any system in the future

29:24.860 --> 29:30.620
that's, you know, solved AI would have to have RL within it? Well, I think if you ask that,

29:30.620 --> 29:35.500
you're asking about the solution methods. I would say that if we have such a thing,

29:35.500 --> 29:41.100
it would be a solution to the RL problem. Now, what particular methods have been used to get there?

29:41.100 --> 29:44.540
Well, we should keep an open mind about the best approaches to actually solve any problem.

29:45.660 --> 29:50.700
And, you know, the things we have right now for reinforcement learning, maybe they, maybe

29:51.820 --> 29:55.260
I believe they've got a lot of legs, but maybe we're missing some things. Maybe there's going

29:55.260 --> 30:01.340
to be better ideas. I think we should keep, you know, let's remain modest and we're at the early

30:01.340 --> 30:06.300
days of this field and there are many amazing discoveries ahead of us. For sure. The specifics,

30:06.300 --> 30:11.180
especially of the different kinds of RL approaches currently, there could be other things that fall

30:11.180 --> 30:17.660
into the very large umbrella of RL. But if it's, if it's okay, can we take a step back and kind of

30:17.660 --> 30:24.140
ask the basic question of what is, do you, reinforcement learning? So, reinforcement learning

30:24.140 --> 30:34.140
is the study and the science and the problem of intelligence in the form of an agent that

30:34.140 --> 30:37.580
interacts with an environment. So, the problem you're trying to solve is represented by some

30:37.580 --> 30:42.940
environment like the world in which that agent is situated. And the goal of RL is clear that the

30:42.940 --> 30:48.060
agent gets to take actions. Those actions have some effect on the environment and the environment

30:48.060 --> 30:53.180
gives back an observation to the agent saying, you know, this is what you see or sense. And one

30:53.180 --> 30:57.980
special thing which it gives back is called the reward signal, how well it's doing in the environment.

30:57.980 --> 31:05.180
And the reinforcement learning problem is to simply take actions over time so as to maximize

31:05.180 --> 31:14.540
that reward signal. So, a couple of basic questions. What types of RL approaches are there? So,

31:14.540 --> 31:21.500
I don't know if there's a nice brief inwards way to paint the picture of sort of value-based,

31:21.500 --> 31:28.460
model-based, policy-based reinforcement learning. Yeah. So, now if we think about, okay, so there's

31:28.460 --> 31:33.420
this ambitious problem definition of RL. It's really, you know, it's truly ambitious. It's

31:33.420 --> 31:37.740
trying to capture and encircle all of the things in which an agent interacts with an environment and

31:37.740 --> 31:42.700
say, well, how can we formalize and understand what it means to crack that? Now, let's think about

31:42.700 --> 31:47.500
the solution method. Well, how do you solve a really hard problem like that? Well, one approach

31:47.500 --> 31:54.140
you can take is to decompose that very hard problem into pieces that work together to solve

31:54.140 --> 31:59.580
that hard problem. And so, you can kind of look at the decomposition that's inside the agent's

31:59.580 --> 32:04.540
head, if you like, and ask, well, what form does that decomposition take? And some of the most

32:04.540 --> 32:09.500
common pieces that people use when they're kind of putting the solution method together,

32:09.500 --> 32:14.300
some of the most common pieces that people use are whether or not that solution has a value

32:14.380 --> 32:18.540
function. That means, is it trying to predict, explicitly trying to predict how much reward

32:18.540 --> 32:23.740
it will get in the future? Does it have a representation of a policy? That means something

32:23.740 --> 32:28.380
which is deciding how to pick actions. Is that decision-making process explicitly represented?

32:29.100 --> 32:34.380
And is there a model in the system? Is there something which is explicitly trying to predict

32:34.380 --> 32:41.020
what will happen in the environment? And so, those three pieces are, to me, some of the most

32:41.020 --> 32:48.540
common building blocks. And I understand the different choices in RL as choices of whether

32:48.540 --> 32:51.900
or not to use those building blocks when you're trying to decompose the solution.

32:52.540 --> 32:57.100
Should I have a value function represented? Should I have a policy represented? Should I have a

32:57.100 --> 33:01.100
model represented? And there are combinations of those pieces and, of course, other things that

33:01.100 --> 33:05.820
you could add into the picture as well. But those three fundamental choices give rise to some of

33:05.820 --> 33:10.060
the branches of RL with which we are very familiar. And so, those, as you mentioned,

33:10.940 --> 33:18.940
there is the choice of what's specified or modeled explicitly. And the idea is that

33:19.740 --> 33:25.020
all of these are somehow implicitly learned within the system. So, it's almost a choice of

33:26.620 --> 33:32.460
how you approach a problem. Do you see those as fundamental differences or these almost like

33:33.420 --> 33:37.980
small specifics, like the details of how you solve the problem, but they're not fundamentally

33:37.980 --> 33:45.980
different from each other? I think the fundamental idea is maybe at the higher level, the fundamental

33:45.980 --> 33:52.540
idea is the first step of the decomposition is really to say, well, how are we really going to

33:52.540 --> 33:57.820
solve any kind of problem where you're trying to figure out how to take actions and just from this

33:58.380 --> 34:02.220
stream of observations, you know, you've got some agent situated in its sensory motor stream and

34:03.100 --> 34:06.220
getting all these observations in, getting to take these actions, and what should it do? How

34:06.220 --> 34:09.900
can you even broach that problem? You know, maybe the complexity of the world is so great

34:10.700 --> 34:15.100
that you can't even imagine how to build a system that would understand how to deal with that.

34:15.660 --> 34:20.060
And so, the first step of this decomposition is to say, well, you have to learn. The system has to

34:20.060 --> 34:25.980
learn for itself. And so, note that the reinforcement learning problem doesn't actually stipulate

34:25.980 --> 34:29.660
that you have to learn, like you could maximize your rewards without learning, it would just

34:30.220 --> 34:36.140
wouldn't do a very good job of it. So, learning is required because it's the only way to achieve

34:36.140 --> 34:42.140
good performance in any sufficiently large and complex environment. So, that's the first step.

34:42.140 --> 34:46.940
And so, that step gives commonality to all of the other pieces, because now you might ask, well,

34:46.940 --> 34:51.180
what should you be learning? What does learning even mean? You know, in this sense,

34:51.340 --> 34:57.500
learning might mean, well, you're trying to update the parameters of some system, which

34:58.300 --> 35:02.460
is then the thing that actually picks the actions. And those parameters could be

35:02.460 --> 35:07.420
representing anything. They could be parameterizing a value function or a model or a policy.

35:08.460 --> 35:12.380
And so, in that sense, there's a lot of commonality in that whatever is being represented there is

35:12.380 --> 35:16.780
the thing which is being learned, and it's being learned with the ultimate goal of maximizing rewards.

35:17.420 --> 35:22.700
But the way in which you decompose the problem is really what gives the semantics to the whole

35:22.700 --> 35:28.460
system. Like, are you trying to learn something to predict well, like a value function or a model?

35:28.460 --> 35:33.660
Are you learning something to perform well, like a policy? And the form of that objective,

35:33.660 --> 35:38.700
like, is kind of giving the semantics to the system. And so, it really is, at the next level

35:38.700 --> 35:43.820
down, a fundamental choice. And we have to make those fundamental choices as system designers,

35:43.820 --> 35:48.700
or enable our algorithms to be able to learn how to make those choices for themselves.

35:49.260 --> 35:55.900
So, then the next step you mentioned, the very first thing you have to deal with is,

35:55.900 --> 36:01.660
can you even take in this huge stream of observations and do anything with it? So,

36:01.660 --> 36:08.060
the natural next basic question is, what is the, what is deeper enforcement learning?

36:08.060 --> 36:13.820
And what is this idea of using neural networks to deal with this huge incoming stream?

36:14.460 --> 36:19.980
So, amongst all the approaches for reinforcement learning, deep reinforcement learning is one

36:20.940 --> 36:30.220
family of solution methods that tries to utilize powerful representations that are offered by

36:30.220 --> 36:37.820
neural networks to represent any of these different components of the solution, of the agent.

36:37.820 --> 36:42.620
Like, whether it's the value function, or the model, or the policy, the idea of deep learning

36:42.620 --> 36:48.220
is to say, well, here's a powerful toolkit that's so powerful that it's universal in the sense that

36:48.220 --> 36:53.580
it can represent any function, and it can learn any function. And so, if we can leverage that

36:53.580 --> 36:59.100
universality, that means that whatever we need to represent for our policy, or for our value

36:59.100 --> 37:04.700
function for a model, deep learning can do it. So, that deep learning is one approach

37:04.780 --> 37:11.260
that offers us a toolkit that is, has no ceiling to its performance, that as we start to put more

37:11.260 --> 37:18.220
resources into the system, more memory and more computation, and more data, more experience of

37:18.220 --> 37:22.540
more interactions with the environment, that these are systems that can just get better and better

37:22.540 --> 37:26.300
and better at doing whatever the job is they've asked them to do, whatever we've asked that

37:26.300 --> 37:32.780
function to represent, it can learn a function that does a better and better job of representing

37:32.780 --> 37:37.420
that knowledge, whether that knowledge be estimating how well you're going to do in the

37:37.420 --> 37:42.460
world, the value function, whether it's going to be choosing what to do in the world, the policy,

37:42.460 --> 37:46.300
or whether it's understanding the world itself, what's going to happen next, the model.

37:46.860 --> 37:54.780
Nevertheless, the fact that neural networks are able to learn incredibly complex representations

37:54.780 --> 38:01.660
that allow you to do the policy, the model, or the value function is, at least to my mind,

38:01.660 --> 38:09.740
exceptionally beautiful and surprising. Was it surprising to you? Can you still

38:09.740 --> 38:14.700
believe it works as well as it does? Do you have good intuition about why it works at all

38:14.700 --> 38:23.260
and works as well as it does? I think let me take two parts to that question. I think

38:26.060 --> 38:29.740
it's not surprising to me that the idea of reinforcement learning

38:30.380 --> 38:37.340
works because in some sense, I feel it's the only thing which can, ultimately,

38:37.340 --> 38:43.020
and so I feel we have to address it and there must be success as possible because we have

38:43.020 --> 38:50.380
examples of intelligence and it must at some level be able to possible to acquire experience and use

38:50.380 --> 38:56.380
that experience to do better in a way which is meaningful to environments of the complexity

38:56.380 --> 39:01.500
that humans can deal with. It must be. Am I surprised that our current systems can do as well

39:01.500 --> 39:07.100
as they can do? I think one of the big surprises for me and a lot of the community

39:09.500 --> 39:20.940
is really the fact that deep learning can continue to perform so well despite the

39:20.940 --> 39:25.500
facts that these neural networks that they're representing have these incredibly nonlinear

39:25.820 --> 39:32.940
bumpy surfaces which to our low-dimensional intuitions make it feel like surely you're

39:32.940 --> 39:37.260
just going to get stuck and learning will get stuck because you won't be able to make any

39:37.260 --> 39:45.660
further progress and yet the big surprise is that learning continues and these what appear to be

39:45.660 --> 39:50.300
local optima turn out not to be because in high dimensions when we make really big neural nets

39:50.300 --> 39:53.900
there's always a way out and there's a way to go even lower and then

39:54.620 --> 39:58.460
you're still not a local optima because there's some other pathway that will take you out and

39:58.460 --> 40:03.660
take you lower still and so no matter where you are learning can proceed and do better and better

40:03.660 --> 40:12.700
and better without bound and so that is a surprising and beautiful property of neural nets

40:13.580 --> 40:20.220
which I find elegant and beautiful and and somewhat shocking that it turns out to be the case

40:20.780 --> 40:27.580
as you said which I really like to our low-dimensional intuitions that's surprising

40:28.140 --> 40:34.940
yeah yeah we're very we're very tuned to working within a three-dimensional environment and so

40:34.940 --> 40:42.140
to start to visualize what a billion dimensional neural network surface that you're trying to

40:42.220 --> 40:48.300
optimize over what that even looks like is very hard for us and so I think that really if you try

40:48.300 --> 40:56.700
to account for for the essentially the AI winter where where people gave up on neural networks

40:56.700 --> 41:02.380
I think it's really down to that that lack of ability to generalize from from low dimensions

41:02.380 --> 41:06.380
to high dimensions because back then we were in the low-dimensional case people could only

41:06.380 --> 41:13.660
build neural nets with you know 50 nodes in them or something and to to imagine that it might be

41:13.660 --> 41:17.420
possible to build a billion dimensional neural net and that it might have a completely different

41:17.420 --> 41:22.940
qualitatively different property was very hard to anticipate and I think even now we're starting

41:22.940 --> 41:28.860
to build the the theory to support that and and it's incomplete at the moment but all of the

41:28.860 --> 41:34.140
theory seems to be pointing in the direction that indeed this is an approach which which truly is

41:34.220 --> 41:38.460
universal both in its representational capacity which was known but also in its learning ability

41:38.460 --> 41:45.420
which is which is surprising and it makes one wonder what else we're missing due to our low

41:45.420 --> 41:52.460
dimensional intuitions that that will seem obvious once it's discovered I often wonder

41:53.500 --> 42:02.220
you know when we one day do have AI's which are superhuman in their abilities to to understand

42:02.220 --> 42:09.660
the world what will they think of of the algorithms that we developed back now will it be you know

42:09.660 --> 42:17.020
looking back at these these days and you know and and thinking that well will we look back and feel

42:17.020 --> 42:21.580
that these algorithms were were naive first steps or will they still be the fundamental ideas which

42:21.580 --> 42:29.340
are used even in a hundred thousand ten thousand years yeah I know they'll they'll watch back to

42:29.340 --> 42:36.460
this conversation and and uh with a smile maybe a little bit of a laugh I mean my my sense is um

42:37.580 --> 42:45.100
I think it's just like when we used to think that the the sun revolved around the earth

42:45.900 --> 42:52.460
they'll see our systems of today reinforcement learning as too complicated that the answer was

42:52.460 --> 42:59.340
simple all along there's something just just like you said in the game of go I mean I love the

42:59.340 --> 43:04.940
systems of like cellular automata that there's simple rules from which incredible complexity

43:04.940 --> 43:11.740
emerges so it feels like there might be some very least simple approaches just like where Sutton

43:11.740 --> 43:20.620
says right these simple methods with compute over time seem to prove to be the most effective

43:20.620 --> 43:30.940
I 100% agree I think that if we try to anticipate what will generalize well into the future I think

43:30.940 --> 43:36.780
it's likely to be the case that it's the simple clear ideas which will have the longest legs and

43:36.780 --> 43:41.100
which will carry us furthest into the future nevertheless we're in a situation where we need

43:41.100 --> 43:45.900
to make things work right and today and sometimes that requires putting together more complex systems

43:46.620 --> 43:51.500
where we don't have the the full answers yet as to what those minimal ingredients might be

43:51.500 --> 43:59.340
so speaking of which if we could take a step back to go uh what was mogo and what was the key idea

43:59.340 --> 44:06.780
behind the system so back during my um phd on computer go around about that time there was a

44:06.780 --> 44:13.100
a major new development in in which actually happened in the context of computer go and

44:14.060 --> 44:20.140
and it was really a revolution in the way that heuristic search was was done and and the idea was

44:20.860 --> 44:27.260
essentially that um a position could be evaluated or a state in general could be evaluated

44:28.700 --> 44:35.260
not by humans saying whether that um position is good or not or even humans providing rules as to

44:35.260 --> 44:43.580
how you might evaluate it but instead by allowing the system to randomly play out the game until the

44:43.580 --> 44:50.460
end multiple times and taking the average of those outcomes as the prediction of what will happen

44:50.460 --> 44:56.140
so for example if you're in the game of go the intuition is that you take a position and you

44:56.140 --> 45:00.140
get the system to kind of play random moves against itself all the way to the end of the game and you

45:00.140 --> 45:05.740
see who wins and if black ends up winning more of those random games than white well you say hey

45:05.740 --> 45:09.660
this is a position that favors white and if white ends up winning more of those random games than

45:09.660 --> 45:21.180
black then it favors white um so that idea um was known as Monte Carlo um um search and a particular

45:21.180 --> 45:25.980
form of Monte Carlo search that became very effective and was developed in computer go

45:25.980 --> 45:31.820
first by Remy Coulomb in 2006 and then taken further by others was something called Monte

45:31.820 --> 45:39.740
Carlo tree search which basically takes that same idea and uses that that insight to evaluate every

45:39.740 --> 45:45.100
node of a search tree is evaluated by the average of the random playouts from that from that node

45:45.100 --> 45:51.980
onwards um and this idea was very powerful and suddenly led to huge leaps forward in the strength

45:52.060 --> 45:58.460
of computer go playing programs um and among those the the strongest of the go playing programs in

45:58.460 --> 46:04.860
those days was a program called mogo which was the first program to actually reach human master

46:04.860 --> 46:11.020
level on small boards nine by nine boards and so this was a program by someone called sylvan jelly

46:11.660 --> 46:17.420
who's a good colleague of mine but i worked with him a little bit um in those days part of my phd

46:18.300 --> 46:24.460
and mogo was a a first step towards the latest successes we saw in computer go

46:25.260 --> 46:32.860
but it was still missing a key ingredient mogo was evaluating purely by random rollouts against

46:32.860 --> 46:38.940
itself and in a way it's it's truly remarkable that random play should give you anything at all

46:38.940 --> 46:45.020
yeah like why why in this perfectly deterministic game that's very precise and involves these very

46:45.020 --> 46:52.860
exact sequences why is it that that random randomization is is is helpful and so the intuition

46:52.860 --> 46:58.940
is that randomization captures something about the the nature of the of the the search tree that

46:58.940 --> 47:04.140
from a position that you're you're understanding the nature of the search tree um from that node

47:04.140 --> 47:10.540
onwards by by by using randomization and this was a very powerful idea and i've seen this in

47:10.540 --> 47:16.700
other spaces uh i'm going to talk to Richard karp and so on randomized algorithms somehow

47:16.700 --> 47:24.060
magically are able to do exceptionally well and and simplifying the problem somehow makes you wonder

47:24.060 --> 47:30.460
about the fundamental nature of randomness in our universe it seems to be a useful thing but so from

47:30.460 --> 47:38.300
that moment can you maybe tell the origin story in the journey of alpha go yeah so programs based on

47:38.300 --> 47:44.620
Monte Carlo tree search were a first revolution in the sense that they led to um suddenly programs

47:44.620 --> 47:50.940
that could play the game to any reasonable level but they they plateaued it seemed that no matter

47:50.940 --> 47:56.300
how much effort people put into these techniques they couldn't exceed the level of um amateur

47:56.300 --> 48:02.460
dan level go players so strong players but not not anywhere near the level of of professionals

48:02.460 --> 48:09.100
never mind the world champion and so that brings us to the birth of alpha go which happened in the

48:09.100 --> 48:19.180
context of a startup company known as deep mind i heard them where a a project was born and the

48:19.180 --> 48:29.180
project was really a scientific investigation um where um myself and adjo huang and an intern

48:29.180 --> 48:35.180
chris madison were exploring a scientific question and that scientific question was really

48:37.180 --> 48:42.780
is there another fundamentally different approach to to this key question of of go the key challenge

48:42.780 --> 48:48.140
of of how can you build that intuition and how can you just have a system that could look at a

48:48.140 --> 48:53.420
position and understand um what moved to play or or how well you're doing in that position who's

48:53.420 --> 49:01.900
going to win and so the deep learning revolution had just begun that systems like image net had

49:02.540 --> 49:08.540
suddenly been won by deep learning techniques back in 2012 and following that it was natural to ask

49:08.540 --> 49:14.060
well you know if if deep learning is able to scale up so effectively with images to to understand

49:14.060 --> 49:21.420
them enough to to classify them well why not go why why why not take a um uh the black and white

49:21.500 --> 49:26.140
stones of the go board and build some a system which can understand for itself what that means in

49:26.140 --> 49:31.660
terms of what moved to pick or who's going to win the game black or white and so that was our

49:31.660 --> 49:37.580
scientific question which we we were probing and trying to understand and as we started to look at

49:37.580 --> 49:43.740
it we discovered that we could build a a system so in fact our very first paper on alpha go was

49:43.740 --> 49:50.060
actually a pure deep learning system which was trying to answer this question and we showed

49:50.060 --> 49:56.060
that actually a pure deep learning system with no search at all was actually able to reach human

49:56.620 --> 50:03.980
band level master level at the full game of go 19 by 19 boards um and so without any search at all

50:03.980 --> 50:07.180
suddenly we had systems which were playing at the level of the best

50:08.700 --> 50:12.620
Monte Carlo tree set systems the ones with randomized rollouts so first I was sorry to

50:12.620 --> 50:19.660
interrupt but uh that's kind of a groundbreaking notion that's a that's like basically a definitive

50:19.660 --> 50:26.700
step away from the a couple of decades of essentially search dominating AI yeah so what how

50:26.700 --> 50:31.580
did that make you feel would you think it was a surprising from a scientific perspective

50:32.300 --> 50:37.900
in general how to make you feel I I found this to be profoundly surprising um in fact it was so

50:37.900 --> 50:44.140
surprising that um that we had a bet back then and like many good projects you know bets are quite

50:44.140 --> 50:50.940
motivating and and the bet was you know whether it was possible for a a a system based purely on

50:50.940 --> 50:58.460
on deep learning no search at all to beat a a down level human player um and so we had um someone

50:58.460 --> 51:03.740
who joined our team um who was a down level player he came in and um and we had this first

51:03.740 --> 51:09.740
match um against him and which side of the bet were you on by the way did you hit the losing

51:09.740 --> 51:16.460
on the winning side I tend to be an optimist um with the with the power of of of deep learning

51:16.460 --> 51:23.100
and and reinforcement learning so the the system won and we were able to beat this um human down

51:23.100 --> 51:28.300
level player and for me that was the moment where where it was like okay something something special

51:28.380 --> 51:35.660
is afoot here we have a system which um without search is able to to already just look at this

51:35.660 --> 51:41.340
position and understand things as well as a strong human player and from that point onwards

51:41.340 --> 51:49.420
I really felt that um reaching that reaching the top levels of human play you know professional

51:49.420 --> 51:57.820
level well champion level I felt it was actually an inevitability um and and if it was inevitable

51:58.620 --> 52:05.820
outcome I was rather keen that it would be us that achieved it so we scaled up this was something

52:05.820 --> 52:13.340
where you know so had lots of conversations back then with um Demisus Arbus that um um the um head

52:13.340 --> 52:21.100
of of DeepMind who was extremely excited um and we we made the decision to to scale up the project

52:21.100 --> 52:29.740
brought more people on board and and so AlphaGo became something where where we we had a clear

52:29.740 --> 52:36.140
goal which was to try and um crack this outstanding challenge of AI to see if we could beat the world's

52:36.140 --> 52:43.900
best players and this led within the space of um not so many months to playing against the

52:43.900 --> 52:49.100
European champion Fan Hui in a match which became you know memorable in history as the

52:49.100 --> 52:55.420
first time a go program had ever beaten a professional player and at that time we had to

52:55.420 --> 53:00.620
make a judgment as to whether when and and whether we should go and challenge the world

53:00.620 --> 53:06.700
champion and and this was a difficult decision to make again we were basing our predictions on

53:06.700 --> 53:12.780
on our own progress and had to estimate based on the rapidity of our own progress when we thought we

53:12.780 --> 53:19.340
would um exceed the level of the human world champion and and we tried to make an estimate

53:19.340 --> 53:26.140
and set up a match and that became the the AlphaGo versus LisaDoll match in um 2016

53:27.100 --> 53:35.020
and we should say spoiler alert that AlphaGo was able to defeat LisaDoll that's right yeah

53:35.020 --> 53:46.140
so maybe uh we could take even a broader view AlphaGo involves both learning from expert games and

53:47.980 --> 53:52.940
as far as I remember a self-played component to where it learns by playing against itself

53:54.220 --> 54:00.220
but in your sense what was the role of learning from expert games there and in terms of your

54:00.300 --> 54:06.060
self-evaluation whether you can take on the world champion what was the thing that they're trying

54:06.060 --> 54:13.500
to do more of sort of train more on expert games or was there's now another I'm asking so many

54:14.300 --> 54:19.900
poorly phrased questions but uh did you have a hope or dream that self-play would be the

54:19.900 --> 54:28.460
key component at that moment yet so in the early days of AlphaGo we we used human data

54:28.460 --> 54:33.020
to explore the science of what deep learning can achieve and so when we had our first paper

54:33.020 --> 54:38.620
that showed um that it was possible to predict um the winner of the game that it was possible to

54:38.620 --> 54:43.980
suggest moves that was done using human data or solely human data yeah and and and and so the

54:43.980 --> 54:48.220
reason that we did it that way was at that time we were exploring separately the deep learning

54:48.220 --> 54:52.940
aspect from the reinforcement learning aspect that was the part which was which was new and

54:52.940 --> 54:59.900
unknown to to to me at that time was how far could that be stretched once we had that it then

54:59.900 --> 55:05.100
became natural to try and use that same representation and see if we could learn for ourselves using

55:05.100 --> 55:11.340
that same representation and so right from the beginning actually our goal had been to build

55:11.340 --> 55:18.700
a system using self-play and to us the human data right from the beginning was an expedient step

55:18.700 --> 55:23.500
to help us for pragmatic reasons to go faster towards the goals of the project

55:24.460 --> 55:29.420
than we might be able to starting solely from self-play and so in those days we were very

55:29.420 --> 55:34.140
aware that we were choosing to to use human data and that might not be the long-term

55:35.740 --> 55:41.660
holy grail of AI but that it was something which was extremely useful to us it helped us to understand

55:41.660 --> 55:46.700
the system it helped us to build deep learning representations which were clear and simple and

55:46.700 --> 55:53.740
easy to use and so really I would say it's it served a purpose not just as part of the algorithm but

55:53.740 --> 55:59.420
something which I continue to use in our research today which is trying to break down a very hard

55:59.420 --> 56:04.940
challenge into pieces which are easier to understand for us as researchers and develop so if you if you

56:04.940 --> 56:10.940
use a component based on human data it can help you to understand the system such that then you

56:10.940 --> 56:17.740
can build the more principled version later that that does it for itself so as I said the Alpha

56:17.740 --> 56:24.300
Go victory and I don't think I'm being sort of uh romanticizing this notion I think it's one of the

56:24.300 --> 56:30.700
greatest moments in the history of AI so were you cognizant of this magnitude of the accomplishment

56:31.260 --> 56:38.140
at the time I mean are you cognizant of it even now because to me I feel like it's something that

56:38.140 --> 56:43.660
would we mentioned what the AGI systems of the future will look back I think they'll look back at

56:43.660 --> 56:51.580
the Alpha Go victory as like holy crap they figured it out this is where this is where it started

56:51.580 --> 56:56.700
well thank you again I mean it's funny because I guess I've been working on I've been working on

56:56.700 --> 57:01.180
computer go for a long time so I've been working at the time of the Alpha Go match on computer go

57:01.180 --> 57:07.020
for more more than a decade and throughout that decade I'd had this dream of what would it be like

57:07.020 --> 57:12.940
to what would it be like really to to actually be able to build a system that could play against

57:12.940 --> 57:18.060
the world champion and and I imagined that that would be an interesting moment that maybe you

57:18.060 --> 57:22.940
know some people might care about that and that this might be you know a nice achievement

57:23.980 --> 57:31.820
but I think when I arrived in in Seoul and discovered the legions of journalists that were

57:31.820 --> 57:38.060
following us around and 100 million people that were watching the match online live I realized

57:38.060 --> 57:43.260
that I'd been off in my estimation of how significant this moment was by several orders of magnitude

57:44.300 --> 57:53.260
and so there was definitely an adjustment process to to realize that this this was something which

57:54.220 --> 57:58.700
the world really cared about and which was a watershed moment and I think there was that

57:58.700 --> 58:05.180
moment of realization which was also a little bit scary because you know if you go into something

58:05.180 --> 58:10.780
thinking it's going to be maybe of interest and then discover that 100 million people are watching

58:10.780 --> 58:14.380
it suddenly makes you worry about whether some of the decisions you've made were really the

58:14.380 --> 58:19.260
best ones or the wisest or were going to lead to the best outcome and we knew for sure that there

58:19.260 --> 58:23.660
were still imperfections in Alpha Go which were going to be exposed to the whole world watching

58:24.300 --> 58:30.860
and so yeah it was a it was I think a great experience and I feel privileged to have been

58:30.860 --> 58:38.220
part of it, privileged to have led that amazing team, I feel privileged to have been in a moment

58:38.220 --> 58:45.580
of history like you say but also lucky that you know in a sense I was insulated from from the

58:45.580 --> 58:50.380
knowledge of I think it would have been harder to focus on the research if the full kind of reality

58:50.460 --> 58:56.380
of of what was going to come to pass had been known to me and the team I think it was you know

58:56.380 --> 58:59.900
we were in our bubble and we were working on research and we were trying to answer the scientific

58:59.900 --> 59:06.460
questions and then bam you know the public sees it and I think it was it was it was better that

59:06.460 --> 59:12.380
way in retrospect. Were you confident that I guess what were the chances that you could get the win

59:13.500 --> 59:20.140
so just like you said I'm a little bit more familiar with another accomplishment

59:20.140 --> 59:24.540
than we may not even get a chance to talk to I talked to Oriel Venialis about Alpha Star which

59:24.540 --> 59:31.020
is another incredible accomplishment but here you know with Alpha Star and beating the Starcraft

59:31.020 --> 59:36.780
there was like already a track record with Alpha Go this is like the really first time you get to

59:36.780 --> 59:43.340
see reinforcement learning face the best human in the world so what was your confidence like what

59:43.340 --> 59:51.180
was the odds? Well we actually um was there a bet? Funnily enough there was so so just before the

59:51.180 --> 59:57.420
match we weren't betting on anything concrete but we all held out a hand everyone in the team held

59:57.420 --> 01:00:01.420
out a hand at the beginning of the match and the number of fingers that they had out on that hand

01:00:01.420 --> 01:00:06.540
was supposed to represent how many games they thought we would win against Lisa Dahl and there

01:00:06.540 --> 01:00:12.300
was an amazing spread in the team's predictions but I have to say I predicted 4-1

01:00:15.020 --> 01:00:21.740
and the reason was based purely on data so I'm a scientist first and foremost and one of the things

01:00:21.740 --> 01:00:28.460
which we had established was that Alpha Go in around one in five games would develop something

01:00:28.460 --> 01:00:32.540
which we called a delusion which was a kind of you know hole in its knowledge where it wasn't

01:00:32.540 --> 01:00:37.580
able to fully understand everything about the position and that hole in its knowledge would

01:00:37.580 --> 01:00:43.740
persist for tens of moves throughout the game and we knew two things we knew that if there were no

01:00:43.740 --> 01:00:49.260
delusions that Alpha Go seemed to be playing at a level that was far beyond any human capabilities

01:00:49.260 --> 01:00:57.260
but we also knew that if there were delusions the opposite was true and in fact you know that's

01:00:57.420 --> 01:01:03.500
what came to pass we saw all of those outcomes and Lisa Dahl in one of the games played a really

01:01:03.500 --> 01:01:11.100
beautiful sequence that Alpha Go just hadn't predicted and after that it led it into this

01:01:11.100 --> 01:01:16.620
situation where it was unable to really understand the position fully and found itself in one of

01:01:16.620 --> 01:01:22.780
these delusions so indeed 4-1 was the outcome. So yeah and can you maybe speak to it a little bit

01:01:22.780 --> 01:01:29.820
more what were the five games like what happened is there interesting things that come to memory

01:01:29.820 --> 01:01:36.140
in terms of the play of the human machine? So I remember all of these games vividly of course

01:01:36.780 --> 01:01:42.620
you know moments like these don't come too often in the lifetime of a scientist and

01:01:42.700 --> 01:01:52.940
the first game was magical because it was the first time that a computer program had

01:01:52.940 --> 01:01:58.540
defeated a world champion in this grand challenge of Go and there was a moment where

01:02:02.460 --> 01:02:06.380
Alpha Go invaded Lisa Dahl's territory towards the end of the game

01:02:07.820 --> 01:02:11.660
and that's quite an audacious thing to do it's like saying hey you thought this was going to be

01:02:11.660 --> 01:02:15.020
your territory in the game but I'm going to stick a stone right in the middle of it and

01:02:15.020 --> 01:02:21.100
and prove to you that I can break it up and Lisa Dahl's face just dropped he wasn't expecting a

01:02:21.100 --> 01:02:29.820
computer to do something that audacious. The second game became famous for a move known as

01:02:29.820 --> 01:02:38.540
Move 37 this was a move that was played by Alpha Go that broke all of the conventions of Go that

01:02:38.780 --> 01:02:44.300
Go players were so shocked by this they thought that maybe the operator had made a mistake

01:02:45.260 --> 01:02:50.460
they thought there was something crazy going on and it just broke every rule that Go players

01:02:50.460 --> 01:02:55.020
are taught from a very young age they're just taught you know this kind of move called a shoulder

01:02:55.020 --> 01:02:59.980
hit you can only play it on the third line or the fourth line and Alpha Go played it on the fifth

01:02:59.980 --> 01:03:05.180
line and it turned out to be a brilliant move and made this beautiful pattern in the middle

01:03:05.180 --> 01:03:12.780
of the board that ended up winning the game and so this really was a clear instance where we could

01:03:12.780 --> 01:03:19.340
say computers exhibited creativity that this was really a move that was something humans hadn't

01:03:20.060 --> 01:03:26.140
known about hadn't anticipated and computers discovered this idea they were the ones to say

01:03:26.140 --> 01:03:31.820
actually you know here's a new idea something new not not in the domains of of human knowledge of the

01:03:31.820 --> 01:03:39.420
game and and now the humans think this is a reasonable thing to do and and it's part of

01:03:39.420 --> 01:03:45.500
Go knowledge now. The third game something special happens when you play against a human

01:03:45.500 --> 01:03:51.660
world champion which again I hadn't anticipated before going there which is you know these these

01:03:51.660 --> 01:03:58.300
players are amazing Lisa Doll was a true champion 18 time world champion and had this amazing ability

01:03:58.300 --> 01:04:07.020
to to probe Alpha Go for weaknesses of any kind and in the third game he was losing and we felt

01:04:07.020 --> 01:04:14.860
we were sailing comfortably to victory but he managed to from nothing stir up this fight and

01:04:14.860 --> 01:04:22.140
build what's called a double co these kind of repetitive positions and he knew that historically

01:04:22.140 --> 01:04:26.620
no no computer Go program had ever been able to deal correctly with double code positions

01:04:26.620 --> 01:04:32.220
and he managed to summon one out of out of nothing and so for us you know this was this

01:04:32.220 --> 01:04:35.900
was a real challenge like would Alpha Go be able to to to deal with this or would it just kind of

01:04:35.900 --> 01:04:41.420
crumble in the face of of this situation and fortunately it dealt with it perfectly. The

01:04:41.420 --> 01:04:48.860
fourth game was was amazing in that Lisa doll appeared to be losing this game Alpha Go thought

01:04:48.860 --> 01:04:54.940
it was winning and then Lisa doll did something which I think only a true world champion can do

01:04:54.940 --> 01:04:59.980
which is he found a brilliant sequence in the middle of the game a brilliant sequence that

01:05:01.340 --> 01:05:09.900
led him to really just transform the position it kind of it it he found just a piece of genius

01:05:09.900 --> 01:05:17.100
really and after that Alpha Go its its evaluation just tumbled it thought it was winning this game

01:05:17.100 --> 01:05:22.380
and all of a sudden it tumbled and said oh now I've got no chance and it starts to behave rather

01:05:22.380 --> 01:05:28.860
oddly at that point in the final game for some reason we as a team were convinced having seen

01:05:28.860 --> 01:05:33.820
Alpha Go in the previous game suffer from delusions we as a team were convinced

01:05:33.820 --> 01:05:37.660
that it was suffering from another delusion we were convinced that it was mis-evaluating the

01:05:37.660 --> 01:05:42.780
position and that that something was going terribly wrong and it was only in the last

01:05:42.780 --> 01:05:47.420
few moves of the game that we realized that actually although it had been predicting it

01:05:47.420 --> 01:05:53.500
was going to win all the way through it really was and um and so somehow you know it just taught us

01:05:53.500 --> 01:05:57.820
yet again that you have to have faith in in your systems when they when they exceed your own level

01:05:57.820 --> 01:06:03.740
of ability in your own judgment you have to trust in them to to know better than than you the designer

01:06:03.740 --> 01:06:10.940
once um you've you've bestowed in them the ability to to judge better than you can then trust the

01:06:10.940 --> 01:06:21.180
system to do so. So just like in the case of Deep Blue beating Gary Kasparov so Gary is I think

01:06:21.180 --> 01:06:27.180
the first time he's ever lost actually to anybody and I mean there's a similar situation at least

01:06:27.180 --> 01:06:37.500
at all it's a it's a tragic it's a tragic loss for humans but a beautiful one I think that's kind of

01:06:38.460 --> 01:06:47.500
from the tragedy sort of emerges over time emerges the kind of inspiring story but

01:06:48.940 --> 01:06:56.860
Lisa Dahl recently analyses her time and I don't know if we can look too deeply into it but he did

01:06:56.860 --> 01:07:03.820
say that even if I become number one there's an entity that cannot be defeated so what do you think

01:07:03.820 --> 01:07:08.540
about these words what do you think about his retirement from the game ago? Well let me take

01:07:08.540 --> 01:07:12.940
you back first of all to the first part of your comment about Gary Kasparov because actually

01:07:12.940 --> 01:07:20.940
at the panel yesterday um he specifically said that when he first lost to Deep Blue he he viewed

01:07:20.940 --> 01:07:26.940
it as a failure he viewed that this this had been a failure of his but later on in his career he

01:07:26.940 --> 01:07:32.060
said he'd come to realize that actually it was a success it was a success for everyone because

01:07:32.060 --> 01:07:39.660
this marked a transformational moment for for AI and so even for Gary Kasparov he came to to

01:07:39.660 --> 01:07:46.220
realize at that moment was was was pivotal and actually meant something much more than than you

01:07:46.220 --> 01:07:53.980
know his personal loss in that moment. Lisa Dahl I think was a much more cognizant of that even

01:07:53.980 --> 01:08:01.580
at the time so in his closing remarks to the match he really felt very strongly that what

01:08:01.580 --> 01:08:06.380
had happened in the AlphaGo match was not only meaningful for AI but but for humans as well

01:08:06.380 --> 01:08:12.140
and he felt as a go player that it had opened his horizons and meant that he could start exploring

01:08:12.140 --> 01:08:17.980
new things it brought his joy back for the game of go because it had broken all of the the conventions

01:08:17.980 --> 01:08:24.460
and barriers and meant that you know suddenly suddenly anything was possible again and so

01:08:24.460 --> 01:08:29.660
you know I was sad to hear that he'd retired but you know he's been a great a great world champion

01:08:29.660 --> 01:08:35.260
over many many years and I think you know that he'll be he'll be remembered for that ever more

01:08:36.060 --> 01:08:41.820
he'll be remembered as the last person to to beat AlphaGo I mean after after that we we

01:08:41.820 --> 01:08:49.260
increased the power of the system and and the next version of AlphaGo beats the other strong

01:08:49.260 --> 01:08:56.140
human players 60 games to nil so you know what a great moment for him and something to be remembered

01:08:56.140 --> 01:09:03.820
for it's interesting that you spent time at AAAI on a panel with Gary Kasparov

01:09:05.260 --> 01:09:08.540
what I mean it's almost I'm just curious to learn

01:09:10.380 --> 01:09:16.460
the conversations you've had with Gary and the because he's also now he's written a book about

01:09:16.460 --> 01:09:22.700
artificial intelligence he's thinking about AI he has kind of a view of it and he talks about AlphaGo

01:09:22.700 --> 01:09:30.540
a lot what what's your sense I arguably I'm not just being Russian but I think Gary is the greatest

01:09:30.540 --> 01:09:38.620
chess player of all time the probably one of the greatest game players of all time and you sort of

01:09:40.060 --> 01:09:45.500
at the center of creating a system that beats one of the greatest players of all time so what

01:09:45.500 --> 01:09:51.100
is that conversation like is there anything yeah any interesting digs any bets any come

01:09:51.100 --> 01:09:58.460
any funny things any profound things so Gary Kasparov has an incredible respect for

01:10:00.060 --> 01:10:06.460
what we did with AlphaGo and you know it's it's an amazing tribute coming from from him of all people

01:10:07.420 --> 01:10:13.900
that he really appreciates and respects what what we've done and I think he feels that the progress

01:10:13.900 --> 01:10:21.340
which has happened in in computer chess which later after AlphaGo we we built the AlphaZero system

01:10:22.380 --> 01:10:28.780
which defeated the the world's strongest chess programs and to Gary Kasparov that moment in

01:10:28.780 --> 01:10:34.780
computer chess was more profound than than than deep blue and the reason he believes it mattered more

01:10:35.580 --> 01:10:39.980
was because it was done with with learning and a system which was able to discover for itself

01:10:39.980 --> 01:10:46.540
new principles new ideas which were able to play the game in a in a in a way which he hadn't always

01:10:47.740 --> 01:10:53.900
known about or anyone and in fact one of the things I discovered at this panel was that

01:10:53.900 --> 01:11:00.540
the current world champion Magnus Carlsen apparently recently commented on his improvement

01:11:00.540 --> 01:11:05.820
in performance and he attributes it to AlphaZero that he's been studying the games of AlphaZero

01:11:05.820 --> 01:11:12.620
he's changed his style to play more like AlphaZero and it's led to him actually increasing his his

01:11:12.620 --> 01:11:20.620
his rating to a new peak yeah I guess to me just like to Gary the inspiring thing is that and just

01:11:20.620 --> 01:11:26.460
like you said with reinforcement learning reinforcement learning and deep learning machine

01:11:26.460 --> 01:11:32.780
learning feels like what intelligence is yeah and you know you could attribute it to sort of

01:11:33.100 --> 01:11:40.140
a bitter viewpoint from Gary's perspective from us humans perspective saying that

01:11:40.140 --> 01:11:46.780
cert pure search that IBM deep blue was doing is not really intelligence but somehow it didn't feel

01:11:46.780 --> 01:11:52.460
like it and so that's the magical I'm not sure what it is about learning that feels like intelligence

01:11:52.460 --> 01:11:59.260
but but it does so I think we should not demean the achievements of what was done in previous

01:11:59.260 --> 01:12:06.540
areas of AI I think that deep blue was an amazing achievement in itself and that heuristic search

01:12:06.540 --> 01:12:11.820
of the kind that was used by deep blue had some powerful ideas that were in there but it also

01:12:11.820 --> 01:12:17.180
missed some things so so the fact that the that the evaluation function the way that the chess

01:12:17.180 --> 01:12:25.100
position was understood was created by humans and not by the machine is a limitation which means that

01:12:26.060 --> 01:12:31.580
there's a ceiling on how well it can do but maybe more importantly it means that the same idea

01:12:31.580 --> 01:12:38.060
cannot be applied in other domains where we don't have access to the kind of human grandmasters

01:12:38.060 --> 01:12:42.860
and that ability to kind of encode exactly their knowledge into an evaluation function and the

01:12:42.860 --> 01:12:48.140
reality is that the story of AI is that you know most domains turn out to be of the second type

01:12:48.140 --> 01:12:53.980
where when knowledge is messy it's hard to extract from experts or it isn't even available and so

01:12:54.060 --> 01:13:02.380
and so so we need to solve problems in a different way and I think alpha goes a step towards solving

01:13:02.380 --> 01:13:10.060
things in a way which which puts learning as a first class citizen and says systems need to

01:13:10.060 --> 01:13:17.580
understand for themselves how to understand the world how to judge their the value of of of

01:13:18.940 --> 01:13:23.180
any action that they might take within that world and any state they might find themselves in and

01:13:23.180 --> 01:13:30.940
in order to do that we we make progress towards AI yeah so one of the nice things about this

01:13:31.820 --> 01:13:38.460
about taking a learning approach to the game of go or game playing is that the things you learn the

01:13:38.460 --> 01:13:43.180
things you figure out are actually going to be applicable to other problems that are real world

01:13:43.180 --> 01:13:48.780
problems that's sort of that's ultimately I mean there's two really interesting things about alpha

01:13:48.780 --> 01:13:55.020
go one is the science of it just the science of learning the science of intelligence and then the

01:13:55.020 --> 01:14:00.780
other is well you're actually learning to figuring out how to build systems that would be potentially

01:14:00.780 --> 01:14:07.020
applicable in in other applications medical autonomous vehicles robotics all I mean it's

01:14:07.020 --> 01:14:15.420
just open the door to all kinds of applications so the next incredible step right really the

01:14:15.420 --> 01:14:22.060
profound step is probably alpha go zero I mean it's arguable I kind of see them all as the same

01:14:22.060 --> 01:14:27.180
place but really and perhaps you were already thinking that alpha go zero is the natural it was

01:14:27.180 --> 01:14:32.540
always going to be the next step but it's removing the reliance on human expert games

01:14:33.500 --> 01:14:42.140
for pre-training as you mentioned so how big of an intellectual leap was this that that self-play

01:14:42.140 --> 01:14:48.540
could achieve superhuman level performance in its own and maybe could you also say what is self-play

01:14:48.540 --> 01:14:57.260
I kind of mentioned it a few times but so let me start with self-play so the idea of self-play

01:14:57.260 --> 01:15:03.020
is something which is really about systems learning for themselves but in the situation

01:15:03.020 --> 01:15:09.420
where there's more than one agent and so if you're in a game and a game is a played between

01:15:09.420 --> 01:15:16.700
two players then self-play is really about understanding that game just by playing games

01:15:16.700 --> 01:15:21.180
against yourself rather than against any actual real opponent and so it's a way to kind of

01:15:22.540 --> 01:15:28.220
discover strategies without having to actually need to go out and play against

01:15:30.220 --> 01:15:32.540
any particular human player for example

01:15:33.500 --> 01:15:44.940
um the main idea of alpha zero was really to you know try and step back from any of the

01:15:44.940 --> 01:15:49.580
knowledge that we put into the system and ask the question is it possible to come up with a

01:15:49.580 --> 01:15:57.580
a single elegant principle by which a system can learn for itself all of the knowledge which it

01:15:57.580 --> 01:16:04.700
requires to play to play a game such as go importantly by taking knowledge out you not only

01:16:05.500 --> 01:16:10.860
make the system less brittle in the sense that perhaps the knowledge you were putting in was

01:16:10.860 --> 01:16:16.060
was just getting in the way and maybe stopping the system learning for itself but also you make it

01:16:16.060 --> 01:16:22.860
more general the more knowledge you put in the harder it is for a system to actually be placed

01:16:23.420 --> 01:16:28.620
taken out of the system in which it's kind of been designed and placed in some other system

01:16:28.620 --> 01:16:32.060
that maybe would need a completely different knowledge base to to understand and perform well

01:16:32.700 --> 01:16:38.540
and so the real goal here is to strip out all of the knowledge that we put in to the point that we

01:16:38.540 --> 01:16:44.220
can just plug it into something totally different um and that to me is really you know the the

01:16:44.220 --> 01:16:48.940
promise of AI is that we can have systems such as that which you know no matter what the goal is

01:16:49.900 --> 01:16:56.060
um no matter what goal we set to the system we can come up with we have an algorithm which

01:16:56.060 --> 01:17:02.060
can be placed into that world into that environment and can succeed in achieving that goal and then

01:17:02.060 --> 01:17:08.460
that that's to me is almost the the essence of intelligence if we can achieve that and so alpha

01:17:08.460 --> 01:17:15.260
zero is a step towards that um and it's a step that was taken in the context of of two player

01:17:15.260 --> 01:17:20.700
perfect information games like go and chess um we also applied it to Japanese chess

01:17:21.420 --> 01:17:28.140
so just to clarify the first step was alpha go zero the first step was to try and take all of

01:17:28.140 --> 01:17:37.420
the knowledge out of alpha go in such a way that it it could play in a in a fully um self-discovered

01:17:37.420 --> 01:17:42.060
way purely from self-play and to me the the motivation for that was always that we could

01:17:42.060 --> 01:17:49.980
then plug it into other domains um but we saved that that until later well and in fact I mean

01:17:51.500 --> 01:17:56.780
just for fun I could tell you exactly the moment where where the idea for alpha zero occurred to

01:17:56.780 --> 01:18:01.580
me um because I think there's maybe a lesson there for for researchers who are kind of too deeply

01:18:01.580 --> 01:18:06.300
embedded in there in their research and you know working 24 sevens try and come up with the next

01:18:06.380 --> 01:18:14.860
idea um which is it actually occurred to me um on honeymoon um and um and I was like at my most

01:18:14.860 --> 01:18:24.060
fully relaxed state really enjoying myself um and and just bing this like the algorithm for alpha

01:18:24.060 --> 01:18:31.180
zero just appeared like um and like in in its full form and this was actually before we played

01:18:31.180 --> 01:18:39.660
against um Lisa doll but we we just didn't I think we were so busy trying to make sure we could beat

01:18:39.660 --> 01:18:46.300
the um the the world champion that it was only later that we had the the opportunity to step

01:18:46.300 --> 01:18:51.100
back and and start examining that that sort of deeper scientific question of of whether this

01:18:51.100 --> 01:18:59.260
could really work so nevertheless so self-play is probably one of the most sort of profound ideas

01:18:59.820 --> 01:19:08.060
that it represents to me at least artificial intelligence but the fact that you could use

01:19:08.060 --> 01:19:15.740
that kind of mechanism to uh again beat world-class players that's very surprising so we kind of

01:19:17.020 --> 01:19:22.540
to me it feels like you have to train in a large number of expert games so was it surprising to

01:19:22.540 --> 01:19:27.420
you what was the intuition can you sort of think not necessarily at that time even now what's your

01:19:27.420 --> 01:19:32.700
intuition why this thing works so well why it's able to learn from scratch well let me first say

01:19:32.700 --> 01:19:38.460
why we tried it so we tried it both because I I feel that it was the deeper scientific question

01:19:38.460 --> 01:19:45.260
to to be asking to make progress towards AI and also because in general in my research I don't

01:19:45.260 --> 01:19:51.740
like to do research on questions for which we already know the likely outcome I don't see much

01:19:51.740 --> 01:19:57.980
value in running an experiment where you're 95 confident that that you will succeed and so we

01:19:57.980 --> 01:20:03.660
could have tried you know maybe to to take out the go and do something which we we knew for sure

01:20:03.660 --> 01:20:08.380
it would succeed on but much more interesting to me was to try try on the things which we weren't

01:20:08.380 --> 01:20:14.620
sure about and one of the big questions on our minds back then was you know could you really

01:20:14.620 --> 01:20:20.220
do this with self-play alone how far could that go would it be as strong and honestly

01:20:21.100 --> 01:20:27.260
we weren't sure yeah it was 50-50 I think you know we I really if you'd asked me I wasn't confident

01:20:27.260 --> 01:20:32.700
that it could reach the same level as these systems but it felt like the right question to ask

01:20:33.740 --> 01:20:39.660
and even if even if it had not achieved the same level I felt that that was an important

01:20:39.980 --> 01:20:43.260
direction to be studying and so

01:20:45.980 --> 01:20:51.820
then low and behold it actually ended up outperforming the the previous version of of

01:20:51.820 --> 01:20:58.140
AlphaGo and indeed was able to beat it by a hundred games to zero so what's the intuition as to as

01:20:58.140 --> 01:21:07.340
to why I think the intuition to me is clear that whenever you have errors in a in a system

01:21:08.300 --> 01:21:11.100
as we did in AlphaGo AlphaGo suffered from these delusions

01:21:11.980 --> 01:21:15.260
occasionally it would misunderstand what was going on in a position and misevaluate it

01:21:16.060 --> 01:21:22.060
how can how can you remove all of these these errors errors arise from many sources for us

01:21:22.060 --> 01:21:26.780
they were arising both from you know starting from the human data but also from the from the

01:21:26.780 --> 01:21:31.340
nature of the search and the nature of the algorithm itself but the only way to address them

01:21:31.340 --> 01:21:38.780
in any complex system is to give the system the ability to correct its own errors it must be able

01:21:38.780 --> 01:21:43.740
to correct them it must be able to learn for itself when it's doing something wrong and correct

01:21:43.740 --> 01:21:50.540
for it and so it seemed to me that the way to correct delusions was indeed to have more iterations

01:21:50.540 --> 01:21:54.780
of reinforcement learning that you know no matter where you start you should be able to correct for

01:21:54.780 --> 01:22:00.380
those errors until it gets to play that out and understand oh well I thought that I was going

01:22:00.380 --> 01:22:05.020
to win in this situation but then I ended up losing that suggests that I was misevaluating

01:22:05.020 --> 01:22:08.860
something there's a hole in my knowledge and now now the system can correct for itself and

01:22:08.860 --> 01:22:15.100
and understand how to do better now if you take that same idea and trace it back all the way to

01:22:15.100 --> 01:22:21.180
the beginning it should be able to take you from no knowledge from completely random starting point

01:22:21.740 --> 01:22:27.340
all the way to the highest levels of knowledge that you can achieve in a in a domain and the

01:22:27.340 --> 01:22:32.540
principle is the same that if you give if you bestow a system with the ability to correct its own

01:22:32.540 --> 01:22:38.860
errors then it can take you from random to something slightly better than random because it sees the

01:22:38.860 --> 01:22:42.540
stupid things that the random is doing and it can correct them and then it can take you from that

01:22:42.540 --> 01:22:47.020
slightly better system and understand well what's that doing wrong and it takes you on to the next

01:22:47.020 --> 01:22:54.540
level and the next level and and this progress it can go on indefinitely and indeed you know

01:22:54.540 --> 01:22:58.140
what would have happened if we'd carried on training AlphaGo Zero for longer

01:22:59.340 --> 01:23:04.220
we saw no sign of it slowing down its improvements or at least it was certainly

01:23:04.220 --> 01:23:11.420
carrying on to improve and presumably if you had the computational resources this

01:23:11.420 --> 01:23:15.660
this could lead to better and better systems that discover more and more.

01:23:15.660 --> 01:23:20.940
So your intuition is fundamentally there's not a ceiling to this process

01:23:21.740 --> 01:23:26.700
one of the surprising things just like you said is the process of patching errors

01:23:27.260 --> 01:23:32.620
that's intuitively makes sense that this is a reinforcement learning should be part of that

01:23:32.620 --> 01:23:39.580
process but what is surprising is in the process of patching your own lack of knowledge you don't

01:23:40.140 --> 01:23:47.500
open up other patches you keep sort of like there's a monotonic decrease of your weaknesses.

01:23:48.380 --> 01:23:51.580
Well let me let me back this up you know I think science always should make

01:23:51.580 --> 01:23:56.940
falsifiable hypotheses yes so let me let me back up this claim with a falsifiable hypothesis

01:23:56.940 --> 01:24:03.900
which is that if someone was to in the future take Alpha Zero as an algorithm and run it on

01:24:05.340 --> 01:24:12.060
with greater computational resources that we had available today then I would predict that they

01:24:12.060 --> 01:24:16.540
would be able to beat the previous system 100 games to zero and that if they were then to do

01:24:16.540 --> 01:24:21.980
the same thing a couple of years later that that would beat that previous system 100 games to zero

01:24:21.980 --> 01:24:28.060
and that that process would continue indefinitely throughout at least my human lifetime presumably

01:24:28.060 --> 01:24:33.500
the game of go would set the the ceiling I mean the game of go would set the ceiling but the game

01:24:33.500 --> 01:24:39.980
of go has 10 to the 170 states in it so so the ceiling is is unreachable by any computational

01:24:39.980 --> 01:24:45.100
device that can be built out of the you know 10 to the 80 atoms in the universe.

01:24:46.620 --> 01:24:51.180
You asked a really good question which is you know do you not open up other errors

01:24:51.180 --> 01:24:56.540
when you when you correct your previous ones and the answer is is yes you do and so

01:24:57.420 --> 01:25:03.020
so it's a remarkable fact about about this class of of two player game and also true of

01:25:03.020 --> 01:25:13.660
single agent games that essentially progress will always lead you to if you have sufficient

01:25:13.660 --> 01:25:18.620
representational resource like imagine you had could represent every state in a big table

01:25:18.620 --> 01:25:25.420
of the game then we we know for sure that a progress of self-improvement will lead all the way

01:25:26.140 --> 01:25:30.460
in the single agent case to the optimal possible behavior and in the two player case to the

01:25:30.460 --> 01:25:35.980
minimax optimal behavior that is the the best way that I can play knowing that you're playing

01:25:35.980 --> 01:25:44.540
perfectly against me and so so for those cases we know that even if you do open up some new error

01:25:44.540 --> 01:25:48.940
that in some sense you've made progress you've you've you're progressing towards the the best

01:25:48.940 --> 01:25:57.500
that can be done so alpha go was initially trained on expert games with some self-play alpha go zero

01:25:57.500 --> 01:26:04.460
remove the need to be trained on expert games and then another incredible step for me because I just

01:26:04.460 --> 01:26:11.260
love chess is to generalize that further to be in alpha zero to be able to play the game of go

01:26:12.300 --> 01:26:17.580
beating alpha go zero and alpha go and then also being able to play the game of chess

01:26:18.140 --> 01:26:25.020
and others so what was that step like what's the interesting aspects there that required to make

01:26:25.020 --> 01:26:32.780
that happen I think the remarkable observation which we saw with alpha zero was that actually

01:26:32.780 --> 01:26:39.420
without modifying the algorithm at all it was able to play and crack some of ai's greatest

01:26:39.420 --> 01:26:46.060
previous challenges in particular we dropped it into the game of chess and unlike the previous

01:26:46.060 --> 01:26:52.620
systems like deep blue which had been worked on for you know years and years and we were able to beat

01:26:52.620 --> 01:27:00.380
the world's strongest computer chess program convincingly using a system that was fully

01:27:00.460 --> 01:27:06.700
discovered by its own from from scratch with its own principles and in fact one of the nice things

01:27:06.700 --> 01:27:13.100
that that we found was that in fact we also achieved the same result in in Japanese chess a variant

01:27:13.100 --> 01:27:17.340
of chess where where you get to capture pieces and then place them back down on your on your own

01:27:17.340 --> 01:27:23.020
side as an extra piece so a much more complicated variant of chess and we also beat the world's

01:27:23.020 --> 01:27:30.060
strongest programs and reach superhuman performance in that game too and it was the very first time

01:27:30.060 --> 01:27:36.140
that we'd ever run the system on that particular game was the version that we published in the

01:27:36.140 --> 01:27:42.140
paper on on alpha zero it just worked out of the box literally no no no touching it we didn't have

01:27:42.140 --> 01:27:48.460
to do anything and and there it was superhuman performance no tweaking no no twiddling and so

01:27:48.460 --> 01:27:53.100
I think there's something beautiful about that principle that you can take an algorithm and

01:27:53.100 --> 01:28:03.020
without twiddling anything it just it just works now to go beyond alpha zero what's required alpha

01:28:03.020 --> 01:28:09.260
zero is is just a step and there's a long way to go beyond that to really crack the deep problems of

01:28:09.260 --> 01:28:16.220
AI but one of the important steps is to acknowledge that the world is a really messy place you know

01:28:16.220 --> 01:28:22.700
it's this rich complex beautiful but messy environment that we live in and no one gives us

01:28:22.700 --> 01:28:28.460
the rules like no one knows the rules of the world at least maybe we understand that it operates

01:28:28.460 --> 01:28:34.140
according to Newtonian or or quantum mechanics at the micro level or according to relativity at the

01:28:34.140 --> 01:28:40.620
macro level but that's not a model that's useful useful for us as people to to operate in it somehow

01:28:40.620 --> 01:28:45.900
the agent needs to understand the world for itself in a way where no one tells it the rules of the

01:28:45.900 --> 01:28:52.460
game and yet it can still figure out what to do in that world deal with this stream of

01:28:52.540 --> 01:28:57.820
observations coming in rich sensory input coming in actions going out in a way that allows it to

01:28:57.820 --> 01:29:03.260
reason in the way that alpha go or alpha zero can reason in the way that these go and chess playing

01:29:03.260 --> 01:29:09.580
programs can reason but in a way that allows it to take actions in that messy world to to achieve

01:29:09.580 --> 01:29:17.820
its goals and so this led us to the most recent step in the story of of of alpha go which was a

01:29:17.900 --> 01:29:24.220
system called mu zero and mu zero is a system which learns for itself even when the rules

01:29:24.220 --> 01:29:29.820
are not given to it it actually can be dropped into a system with messy perceptual inputs we

01:29:29.820 --> 01:29:37.020
actually tried it in the in some Atari games the canonical domains of Atari that have been used

01:29:37.020 --> 01:29:44.300
for reinforcement learning and and this system learned to build a model of these Atari games

01:29:44.300 --> 01:29:51.820
that was sufficiently rich and useful enough for it to be able to plan successfully and in fact

01:29:51.820 --> 01:29:57.820
that system not only went on to to beat the state of the art in Atari but the same system without

01:29:57.820 --> 01:30:04.620
modification was able to reach the same level of superhuman performance in go chess and shogi

01:30:04.620 --> 01:30:10.140
that we'd seen in alpha zero showing that even without the rules the system can learn for itself

01:30:10.140 --> 01:30:15.100
just by trial and error just by playing this game of go and no one tells you what the rules are but

01:30:15.100 --> 01:30:21.020
you just get to the end and and someone says you know win or loss you play this game of chess and

01:30:21.020 --> 01:30:26.460
someone says win or loss or you you play a game of breakout in Atari and someone just tells you

01:30:26.460 --> 01:30:31.500
you know your score at the end and the system for itself figures out essentially the rules of the

01:30:31.500 --> 01:30:38.540
system the dynamics of the world how the world works and that not in any explicit way but just

01:30:38.620 --> 01:30:44.380
implicitly enough understanding for it to be able to plan in that in that system in order to

01:30:44.380 --> 01:30:48.860
achieve its goals and that's the you know that's the fundamental process they have to go through when

01:30:48.860 --> 01:30:53.900
you're facing in any uncertain kind of environment that you would in the real world is figuring out

01:30:53.900 --> 01:30:59.340
the sort of the rules the basic rules of the game that's right so there's a lot I mean yeah that

01:30:59.340 --> 01:31:06.540
that allows it to be applicable to basically any domain that could be digitized in the way that it

01:31:06.540 --> 01:31:12.220
needs to in order to be consumable sort of in order for the reinforcement learning framework to

01:31:12.220 --> 01:31:16.460
be able to sense the environment to be able to act in the environment and so on the full reinforcement

01:31:16.460 --> 01:31:22.700
learning problem needs to deal with with worlds that are unknown and and complex and and the agent

01:31:22.700 --> 01:31:27.900
needs to learn for itself how to deal with that and so Musero is there's a step a further step in

01:31:27.900 --> 01:31:33.740
that direction one of the things that inspire the general public in just in conversations I have like

01:31:33.740 --> 01:31:40.220
with my parents or something with my mom that just loves what was done is kind of at least a notion

01:31:40.220 --> 01:31:46.060
that there was some display of creativity some new strategies new behaviors that were created that

01:31:46.060 --> 01:31:51.500
that again has echoes of intelligence so is there something that stands up do you see it the same

01:31:51.500 --> 01:31:57.740
way that there's creativity and there's some behaviors patterns that you saw that alpha

01:31:57.740 --> 01:32:06.060
zero was able to display that are truly creative so let me start by I think saying that I think

01:32:06.060 --> 01:32:13.100
we should ask what creativity really means so to me creativity means discovering something

01:32:13.660 --> 01:32:21.020
which wasn't known before something unexpected something outside of our norms and so in that sense

01:32:21.980 --> 01:32:28.780
the process of reinforcement learning or the self-play approach that was used by alpha zero

01:32:29.740 --> 01:32:35.420
is it's the essence of creativity it's really saying at every stage you're playing according to

01:32:35.420 --> 01:32:43.180
your current norms and you try something and if it works out you say hey here's something great

01:32:43.180 --> 01:32:48.060
I'm going to start using that and then that process it's like a micro discovery that happens

01:32:48.060 --> 01:32:53.660
millions and millions of times over the course of the algorithm's life where it just discovers some

01:32:53.660 --> 01:32:57.900
new idea oh this pattern this pattern's working really well for me I'm gonna I'm gonna start using

01:32:57.900 --> 01:33:03.180
that oh now oh here's this other thing I can do I can start to to connect these stones together in

01:33:03.180 --> 01:33:10.300
this way or I can start to you know sacrifice stones or give up on on pieces or play shoulder

01:33:10.300 --> 01:33:14.220
hits on the fifth line or whatever it is the system's discovering things like this for itself

01:33:14.300 --> 01:33:20.460
continually repeatedly all the time and so it should come as no surprise to us then when

01:33:20.460 --> 01:33:25.660
if you leave these systems going that they discover things that are not known to humans

01:33:25.660 --> 01:33:33.580
that the to the human norms are considered creative and we've seen this several times in fact

01:33:33.580 --> 01:33:42.780
in alpha go zero we saw this beautiful timeline of discovery where what we saw was that there

01:33:42.780 --> 01:33:46.940
were these opening patterns that humans play called joseki these are like the patterns that

01:33:46.940 --> 01:33:50.540
that humans learn to play in the corners and they've been developed and refined over

01:33:50.540 --> 01:33:54.940
over literally thousands of years in the game of go and what we saw was in the course of the

01:33:55.580 --> 01:34:00.940
training alpha go zero over the course of the the 40 days that we trained this system

01:34:01.820 --> 01:34:08.700
it starts to discover exactly these patterns that human players play and over time we found

01:34:08.700 --> 01:34:13.980
that all of the joseki that humans played were discovered by the system through this process

01:34:13.980 --> 01:34:21.100
of of self play and this sort of essential notion of creativity but what was really interesting

01:34:21.100 --> 01:34:26.940
was that over time it then starts to discard some of these in favor of its own joseki that humans

01:34:26.940 --> 01:34:32.940
didn't know about and it starts to say oh well you thought that the knights move pincer joseki

01:34:32.940 --> 01:34:38.620
was a great idea but here's something different you can do there which makes some new variations

01:34:38.780 --> 01:34:43.740
that humans didn't know about and actually now the human go players study the joseki that alpha

01:34:43.740 --> 01:34:50.540
go played and they become the new norms that are used in in today's top level go competitions

01:34:51.180 --> 01:34:58.300
that never gets old even just the first to me maybe just makes me feel good as a human being

01:34:58.300 --> 01:35:03.740
that a self playing mechanism that knows nothing about us humans discovers patterns that we humans

01:35:03.740 --> 01:35:09.500
do it's this is like an affirmation that we're all doing we're doing okay as humans yeah we've

01:35:10.460 --> 01:35:15.740
in this domain in other domains we figured out it's like the church will quote about democracy

01:35:15.740 --> 01:35:22.940
it's the you know it's the but it sucks but it's the best one we've tried so um in general

01:35:22.940 --> 01:35:28.380
taking a step outside of go and you have like a million accomplishments that have no time to talk

01:35:28.380 --> 01:35:36.060
about the with alpha star and so on and and the current work but in general this self playing

01:35:36.060 --> 01:35:42.780
mechanism that you've inspired the world with by beating the world champion go player do you see

01:35:42.780 --> 01:35:50.620
that as um do you see being applied in other domains do you have sort of dreams and hopes that

01:35:50.620 --> 01:35:57.260
is applied in both the simulated environments in the constrained environments of games constrained

01:35:57.260 --> 01:36:01.580
i mean alpha star really demonstrates that you can remove a lot of the constraints but nevertheless

01:36:01.580 --> 01:36:07.100
it's in a digital simulated environment do you have a hope a dream that it starts being applied

01:36:07.100 --> 01:36:13.500
in the robotics environment and maybe even in domains that are safety critical and so on and

01:36:14.380 --> 01:36:18.300
have you know have a real impact in the real world like autonomous vehicles for example which

01:36:18.300 --> 01:36:25.740
seems like a very far out dream at this point so i absolutely do um hope and and imagine that we

01:36:25.740 --> 01:36:30.060
will we will get to the point where ideas just like these are used in all kinds of different

01:36:30.060 --> 01:36:35.020
domains in fact one of the most satisfying things as a researcher is when you start to see other

01:36:35.020 --> 01:36:40.940
people use your your algorithms in unexpected ways so in the last couple of years there have been

01:36:40.940 --> 01:36:49.100
you know a couple of nature papers where different teams unbeknownst to to us took alpha zero and

01:36:49.100 --> 01:36:56.940
applied exactly those same algorithms and ideas to real world problems of huge meaning to to

01:36:56.940 --> 01:37:02.140
society so one of them was the problem of chemical synthesis and they were able to beat the state of

01:37:02.140 --> 01:37:10.860
the art in finding pathways of how to actually synthesize chemicals retro retro chemical synthesis

01:37:11.820 --> 01:37:15.900
and the second paper actually actually just came out a couple of weeks ago in nature

01:37:16.620 --> 01:37:22.220
showed that in quantum computation you know one of the big questions is how to how to

01:37:22.220 --> 01:37:29.020
understand the nature of the the the function in quantum computation and a system based on alpha

01:37:29.020 --> 01:37:33.980
zero beat the state of the art by quite some distance there again so so these are just examples

01:37:33.980 --> 01:37:38.700
and i think you know that the lesson which we've seen elsewhere in in in machine learning time

01:37:38.700 --> 01:37:44.140
and time again is that if you make something general it will be used in all kinds of ways you

01:37:44.140 --> 01:37:49.660
know you provide a really powerful tools to society and and those tools can be used in in

01:37:49.660 --> 01:37:56.060
amazing ways and so i think we're just at the beginning and and for sure i hope that we we

01:37:56.060 --> 01:38:03.260
see all kinds of outcomes so the the and the the other side of the question of a reinforcement

01:38:03.260 --> 01:38:08.860
learning framework is you know usually want to specify reward function and an objective function

01:38:08.940 --> 01:38:16.300
what do you think about sort of ideas of intrinsic rewards of and when we're not really sure about

01:38:16.940 --> 01:38:24.700
you know of if we take you know human beings as existence proof that we don't seem to be

01:38:24.700 --> 01:38:31.500
operating according to a single reward do you think that there's interesting ideas

01:38:32.220 --> 01:38:38.060
for when you don't know how to truly specify the reward you know that there's some flexibility

01:38:38.060 --> 01:38:42.620
for discovering it intrinsically or so on in the context of reinforcement learning

01:38:42.620 --> 01:38:46.700
so i think you know when we think about intelligence it's really important to be clear

01:38:46.700 --> 01:38:51.260
about the problem of intelligence and i think it's clearest to understand that problem in

01:38:51.260 --> 01:38:56.220
terms of some ultimate goal that we want the system to to try and solve for and after all if

01:38:56.220 --> 01:39:01.660
if we don't understand the ultimate purpose of the system do we really even have a clearly defined

01:39:01.980 --> 01:39:08.220
problem that we're solving at all now within that as with your example for humans

01:39:10.300 --> 01:39:16.940
the system may choose to create its own motivations and sub-goals that help the system to achieve its

01:39:16.940 --> 01:39:23.340
ultimate goal and that may indeed be a hugely important mechanism to achieve those ultimate

01:39:23.340 --> 01:39:27.740
goals but there is still some ultimate goal i think the system needs to be measurable and and

01:39:27.740 --> 01:39:33.180
evaluated against and even for humans i mean humans we're incredibly flexible we feel that we

01:39:33.180 --> 01:39:38.780
we can you know any goal that we're given we feel we can we can master to some some degree

01:39:40.060 --> 01:39:44.380
but if we think of those goals really you know like the the goal of being able to pick up an

01:39:44.380 --> 01:39:50.620
object or the goal of of being able to communicate or influence people to do things in a in a particular

01:39:50.620 --> 01:39:58.380
way or whatever those goals are really they are their sub-goals really that we set ourselves

01:39:58.380 --> 01:40:04.540
you know we choose to pick up the the object we choose to communicate we choose to to influence

01:40:04.540 --> 01:40:09.820
someone else and we choose those because it we think it will lead us to something you know in

01:40:09.820 --> 01:40:16.140
later on we think that's helpful to us to achieve some ultimate goal now i don't want to speculate

01:40:16.140 --> 01:40:22.300
whether or not humans as a system necessarily have a singular overall goal of survival or whatever

01:40:22.300 --> 01:40:28.220
it is but i think the principle for understanding and implementing intelligence is has to be that if

01:40:28.220 --> 01:40:32.460
we're trying to understand intelligence or implement our own there has to be a well-defined

01:40:32.460 --> 01:40:39.900
problem otherwise if it's not i think it's it's like an admission of defeat that for there to be

01:40:39.900 --> 01:40:43.900
hope for for understanding or implementing intelligence we have to know what we're doing

01:40:43.900 --> 01:40:48.060
we have to know what we're asking the system to do otherwise if you if you don't have a clearly

01:40:48.060 --> 01:40:54.460
defined purpose you're not going to get a clearly defined answer the the ridiculous big question that

01:40:54.460 --> 01:41:01.580
has to naturally follow because i have to pin you down on this on this thing that nevertheless one

01:41:01.580 --> 01:41:08.780
of the big silly or big real questions before humans is the meaning of life is us trying to

01:41:08.780 --> 01:41:13.180
figure out our own reward function yeah and you just kind of mentioned that if you want to build

01:41:13.260 --> 01:41:18.300
intelligent systems and you know what you're doing you should be at least cognizant to some degree

01:41:18.300 --> 01:41:25.020
of what the reward function is so the natural question is what do you think is the reward

01:41:25.020 --> 01:41:30.380
function of human life the meaning of life for us humans the meaning of our existence

01:41:32.860 --> 01:41:38.380
i think you know i'd be speculating beyond my own expertise but but just for fun let me do that

01:41:38.380 --> 01:41:42.940
yes please and say i think that there are many levels at which you can understand the system

01:41:42.940 --> 01:41:49.180
and and you can understand something as as optimizing for for a goal at many levels and so

01:41:50.220 --> 01:41:55.260
so you can understand the you know let's start with the universe like does the universe have a

01:41:55.260 --> 01:42:02.220
purpose well it feels like it's just at one level just following certain mechanical laws of physics

01:42:02.220 --> 01:42:06.700
and that that's led to the development of the universe but at another level you can view it as

01:42:07.660 --> 01:42:11.180
actually there's the second law of thermodynamics that says that this is

01:42:11.180 --> 01:42:15.500
increasing in entropy over time forever and now there's a view that's been developed by

01:42:16.220 --> 01:42:20.540
certain people at MIT that this you can think of this as as almost like a goal of the universe

01:42:20.540 --> 01:42:26.140
that the purpose of the universe is to maximize entropy so there are multiple levels at which

01:42:26.140 --> 01:42:33.580
you can understand a system the next level down you might say well if the goal is to is to maximize

01:42:33.580 --> 01:42:41.180
entropy well how do how does how can that be done by a particular system and maybe evolution

01:42:41.180 --> 01:42:45.900
is something that the universe discovered in order in order to kind of dissipate energy as

01:42:45.900 --> 01:42:50.780
efficiently as possible and by the way i'm borrowing from max tegmark for some of these

01:42:50.780 --> 01:42:58.140
metaphors the physicist but if you can think of evolution as a mechanism for for dispersing energy

01:42:59.100 --> 01:43:06.140
then then evolution you might say is is then becomes a goal which is if if evolution disperses

01:43:06.140 --> 01:43:12.700
energy by reproducing as efficiently as possible what's evolution then well it's now got its own

01:43:12.700 --> 01:43:20.220
goal within that which is to actually reproduce as effectively as possible and now how does

01:43:20.220 --> 01:43:27.340
reproduction how is that made as effective as possible well you need entities within that

01:43:27.420 --> 01:43:31.260
that can survive and reproduce as effectively as possible and so it's natural that in order to

01:43:31.260 --> 01:43:37.820
achieve that high level goal those individual organisms discover brains intelligences which

01:43:37.820 --> 01:43:46.300
enable them to support the the goals of evolution and those brains what do they do well perhaps the

01:43:46.300 --> 01:43:52.540
early brains maybe they were controlling things at some direct level you know maybe they were the

01:43:52.540 --> 01:43:56.380
equivalent of pre-programmed systems which were directly controlling what was going on

01:43:57.500 --> 01:44:01.980
and setting certain you know things in order to achieve these particular particular goals

01:44:02.940 --> 01:44:07.500
but that led to a another level of discovery which was learning systems you know parts of

01:44:07.500 --> 01:44:12.380
the brain which are able to to learn for themselves and learn how to to program themselves to achieve

01:44:12.380 --> 01:44:18.700
any goal and presumably there are parts of the game of the brain where goals are set to parts of

01:44:18.700 --> 01:44:23.900
that that system and provides this very flexible notion of intelligence that that we as humans

01:44:23.900 --> 01:44:28.620
presumably have which is the ability to kind of why the reason we feel that we can we can we can

01:44:28.620 --> 01:44:33.580
achieve any goal so so it's a very long-winded answer to say that you know I think there are many

01:44:33.580 --> 01:44:40.380
perspectives and many levels at which intelligence can be understood and and each of those levels

01:44:40.380 --> 01:44:44.300
you can take multiple perspectives like you know you can view the system as as something which is

01:44:44.300 --> 01:44:49.500
optimizing for a goal which is understanding it at a level by which we can maybe implement it and

01:44:49.500 --> 01:44:54.460
understand it as AI researchers or computer scientists or you can understand it at the

01:44:54.460 --> 01:44:57.980
level of the mechanistic thing which is going on that there are these you know atoms bouncing

01:44:57.980 --> 01:45:02.300
around in the brain and they lead to the the outcome of that system is not in contradiction

01:45:02.300 --> 01:45:08.780
with the fact that it's it's also a decision-making system that's optimizing for some goal and and

01:45:08.780 --> 01:45:15.900
purpose I've never heard the the description of the meaning of life structured so beautifully in

01:45:15.900 --> 01:45:22.620
layers but you did miss one layer which is the next step which you're responsible for which is

01:45:22.620 --> 01:45:29.740
creating the the artificial intelligence indeed layer on top of that and indeed I can't wait to

01:45:29.740 --> 01:45:36.860
see well I may not be around but the can't wait to see what the next layer beyond that well well

01:45:36.860 --> 01:45:41.740
let's just take that that argument you know and pursue it to its natural conclusion so so the

01:45:41.740 --> 01:45:48.140
next level indeed is for for how can our how can our learning brain achieve its goals most

01:45:48.140 --> 01:45:58.380
effectively well maybe it does so by by us as learning beings building a system which is able

01:45:58.380 --> 01:46:04.140
to solve for those goals more effectively than we can and so when we build a system to play the game

01:46:04.140 --> 01:46:08.860
of go you know when I said that I wanted to build a system that can play go better than I can I've

01:46:08.860 --> 01:46:14.460
enabled myself to achieve that goal of playing go better than I could by by directly playing it

01:46:14.460 --> 01:46:20.300
and learning it myself and so now a new layer has been created which is systems which are able to

01:46:20.300 --> 01:46:26.220
achieve goals for themselves and ultimately there may be layers beyond that where they set sub-goals

01:46:26.220 --> 01:46:34.460
to parts of their own system in all in order to to achieve those and so forth so incredible so the

01:46:34.540 --> 01:46:38.780
story of intelligence I think I think is is a multi-layered one and a multi-perspective one

01:46:39.900 --> 01:46:45.740
we live in an incredible universe David thank you so much first of all for dreaming of using

01:46:45.740 --> 01:46:52.300
learning to solve go and building intelligence systems and for actually making it happen and

01:46:52.300 --> 01:46:57.980
for inspiring millions of people in the process it's truly an honor thank you so much for talking

01:46:57.980 --> 01:47:03.020
today okay thank you thanks for listening to this conversation with David Silver and thank you to

01:47:03.020 --> 01:47:08.700
our sponsors masterclass and cash app please consider supporting the podcast by signing up

01:47:08.700 --> 01:47:15.580
to masterclass at masterclass.com slash lex and downloading cash app and using code lex podcast

01:47:15.580 --> 01:47:20.140
if you enjoy this podcast subscribe on youtube review it with five stars an apple podcast

01:47:20.140 --> 01:47:26.140
support on patreon or simply connect with me on twitter at lex freedman and now let me leave you

01:47:26.140 --> 01:47:32.540
some words from david silver my personal belief is that we've seen something of a turning point

01:47:32.620 --> 01:47:38.220
where we're starting to understand that many abilities like intuition and creativity that

01:47:38.220 --> 01:47:43.660
we've previously thought were in the domain only of the human mind are actually accessible to machine

01:47:43.660 --> 01:47:50.220
intelligence as well and I think that's a really exciting moment in history thank you for listening

01:47:50.220 --> 01:47:55.180
and hope to see you next time

