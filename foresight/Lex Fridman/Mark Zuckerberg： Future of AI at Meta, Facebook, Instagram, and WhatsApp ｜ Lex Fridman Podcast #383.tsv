start	end	text
0	5440	The following is a conversation with Mark Zuckerberg, his second time in this podcast.
5440	11200	He's the CEO of Meta that owns Facebook, Instagram, and WhatsApp, all services
11200	14080	used by billions of people to connect with each other.
14720	20400	We talk about his vision for the future of Meta and the future of AI in our human world.
21280	26880	This is the Lex Friedman podcast, and now, dear friends, here's Mark Zuckerberg.
27840	33120	So you competed in your first jiu-jitsu tournament, and me as a fellow jiu-jitsu
33120	37280	practitioner and competitor, I think that's really inspiring, given all the things you have going
37280	44400	on. So I got to ask, what was that experience like? Oh, it was fun. I mean, I'm a pretty
44400	50560	competitive person. Yeah. Doing sports that basically require your full attention, I think,
50560	56160	is really important to my mental health and the way I just stay focused at doing everything I'm
56160	61920	doing. So I decided to get into martial arts. And it's awesome. I got a ton of my friends into
61920	70400	it. We all train together. We have a mini academy in my garage. And I guess one of my friends was
70400	74400	like, hey, we should go do a tournament. I was like, okay, yeah, let's do it. I'm not going to
74960	79600	shy away from a challenge like that. So yeah, but it was awesome. It was just a lot of fun.
79600	81520	You weren't scared? There was no fear?
81520	85040	I don't know. I was pretty sure that I'd do okay.
85040	90160	I like the confidence. Well, so for people who don't know, jiu-jitsu is a martial art
90160	97280	where you're trying to break your opponent's limbs or choke them to sleep and do so with
97280	104880	grace and elegance and efficiency and all that kind of stuff. It's a kind of art form, I think,
104880	109440	that you can do for your whole life. And it's basically a game, a sport of human chess, you
109440	114640	can think of. There's a lot of strategy. There's a lot of interesting human dynamics of using
114640	120000	leverage and all that kind of stuff. And it's kind of incredible what you could do. You could do
120000	124400	things like a small opponent could defeat a much larger opponent. And you get to understand the
124400	129760	way the mechanics of the human body works because of that. But you certainly can't be distracted.
129760	136480	No, it's 100% focused. But to compete, I needed to get around the fact that I didn't want it to
136480	143280	be like this big thing. So I basically just, I rolled up with a hat and sunglasses and I was
143280	149200	wearing a COVID mask. And I registered under my first and middle name, so Mark Elliott. And it
149200	152400	wasn't until I actually pulled all that stuff off right before I got on the mat that I think
152400	157520	people knew it was me. So it was pretty low-key. But you're still a public figure.
157520	162160	Yeah, I mean, I didn't want to lose. Right. The thing you're partially afraid of is not just
162160	167120	the losing, but being almost like embarrassed. It's so raw, the sport, in that it's just you
167120	171200	and another human being. There's a primal aspect there. Oh yeah, it's great. For a lot of people,
171200	174720	it can be terrifying, especially the first time you're doing the common competing and you
174720	179040	wasn't for you. I see the look of excitement in your face. Yeah, I don't know. No fear.
179040	184160	I just think part of learning is failing. Okay. Right. So I mean, the main thing,
184160	188960	like people who train jiu-jitsu, it's like you need to not have pride because I mean,
188960	194560	all the stuff that you were talking about before about getting choked or getting a joint lock,
196800	200800	you only get into a bad situation if you're not willing to tap once you've already lost.
201440	204880	Right. But obviously, when you're getting started with something, you're not going to
204880	209440	be an expert at it immediately. So you just need to be willing to go with that. But I think this
209440	214080	is like, I don't know. I mean, maybe I've just been embarrassed enough times in my life. Yeah.
214080	218400	I do think that there's a thing where like, as people grow up, maybe they don't want to be
218400	223040	embarrassed or anything. They've built their adult identity and they kind of have a sense of
223040	228960	who they are and what they want to project. And I don't know, I think maybe to some degree,
229280	236320	you know, your ability to keep doing interesting things is your willingness to
236960	243840	be embarrassed again and go back to step one and start as a beginner and get your ass kicked and,
244560	248640	you know, look stupid doing things. And yeah, I think so many of the things that we're doing,
248640	252560	whether it's, whether it's this, I mean, this is just like a kind of a physical
252560	257920	part of my life, but running the company. It's like we just take on new adventures and
260480	263600	you know, all the big things that we're doing, I think of as like 10 plus year
264160	268560	missions that we're on where, you know, often early on, you know, people doubt that we're
268560	273040	going to be able to do it. And the initial work seems kind of silly. And our whole ethos says we
273040	276720	don't want to wait until something is perfect to put it out there. We want to get it out quickly
276720	280400	and get feedback on it. And so I don't know. I mean, there's probably just something about
280400	284560	how I approach things in there. But I just kind of think that the moment that you decide that
284560	287680	you're going to be too embarrassed to try something new, then you're not going to learn anything
287680	293280	anymore. But like I mentioned, that fear, that anxiety could be there, it could creep up every
293280	298720	once in a while. Do you feel that in especially stressful moments sort of outside of, did you
298720	306960	just, Matt, just in work, stressful moments, big decision days, big decision moments? How do you
306960	310640	deal with that fear? How do you deal with that anxiety? The thing that stresses me out the most
310640	317920	is always the people challenges. You know, I kind of think that, you know, strategy questions,
318720	324080	you know, I tend to have enough conviction around the values of what we're trying to do and
325040	330720	what I think matters and what I want our company to stand for, that those don't really keep me
330720	335120	up at night that much. I mean, I kind of, you know, it's not that I get everything right. Of
335120	342720	course I don't, right? I mean, we make a lot of mistakes. But I at least have a pretty strong
342720	349360	sense of where I want us to go on that. The thing in running a company for almost 20 years now,
349920	354800	one of the things that's been pretty clear is when you have a team that's cohesive,
355760	362880	you can get almost anything done. And, you know, you can run through super hard challenges.
363120	369680	You can make hard decisions and push really hard to do the best work even in kind of
370240	375600	optimize something super well. But when there's that tension, I mean, that's when things get
375600	380320	really tough. And, you know, when I talk to other friends who run other companies and things like
380320	384080	that, I think one of the things that I actually spend a disproportionate amount of time on in
384080	391280	running this company is just fostering a pretty tight core group of people who are running the
391280	399440	company with me. And that to me is kind of the thing that both makes it fun, right? Having,
399440	403440	you know, friends and people you've worked with for a while and new people and new perspectives,
403440	407280	but like a pretty tight group who you can go work on some of these crazy things with.
408880	413920	But to me, that's also the most stressful thing is when there's tension, you know,
413920	420080	that weighs on me. I think it's maybe not surprising. I mean, we're like a very
420080	426720	people focused company and it's the people is the part of it that weighs on me the most to
426720	430880	make sure that we get right. But yeah, that I'd say across everything that we do is probably
431920	437920	the big thing. So when there's tension in that inner circle of close folks, so
439120	449120	when you trust those folks to help you make difficult decisions about Facebook, WhatsApp,
449200	455040	Instagram, the future of the company and the metaverse with the AI, how do you build that
455040	461120	close-knit group of folks to make those difficult decisions? Is there people that you have to
461120	467200	have critical voices, very different perspectives on focusing on the past versus the future, all
467200	472560	that kind of stuff? Yeah, I mean, I think for one thing, it's just spending a lot of time with
472560	477920	whatever the group is that you want to be that core group grappling with all of the biggest
477920	483280	challenges. And that requires a fair amount of openness. And, you know, so I mean, a lot of how
483280	488240	I run the company is, you know, it's like every Monday morning, we get our, it's about the top
488240	494240	30 people together. And we, and this is a group that just worked together for a long period of
494240	498800	time. And I mean, people, people rotate in, I mean, we knew people join, people leave the company,
498800	504160	people go do other roles in the company. So it's not the same group over time. But then we spend,
504160	508400	you know, a lot of times a couple of hours, a lot of the time, it's, you know, it can be
508400	513280	somewhat unstructured. We, like I'll come with maybe a few topics that I, that are top of mind
513280	518400	for me, but I'll ask other people to bring things and people, you know, raise questions,
518400	524160	whether it's okay, there's an issue happening in some country with, with some policy issue,
524160	528240	there's like a new technology that's developing here, we're having an issue with this partner.
528960	534080	You know, there's a design trade-off and WhatsApp between two things that, that end up
535040	539040	being values that we care about deeply. And we need to kind of decide where we want to be
539040	544080	on that. And I just think over time, when, you know, by working through a lot of issues with
544080	549520	people and doing it openly, people develop an intuition for each other and a bond and camaraderie.
550880	556800	And to me, developing that is, is like a lot of the fun part of running a company or doing
556800	560960	anything, right? I think it's like having, having people who are kind of along on the journey that
560960	564640	you're, that you feel like you're doing it with, nothing is ever just one person doing it.
564640	567840	Are there people that disagree often within that group?
567840	569680	It's a fairly combative group.
569680	575120	Okay. So combat is part of it. So this is making decisions on design, engineering,
576880	578480	policy, everything.
578480	580400	Everything, everything. Yeah.
581040	584960	I have to ask just back to Jujitsu for a little bit, what's your favorite submission?
584960	591440	Now that you've been doing it, what's, how do you like to submit your opponent, Mark Zuckerberg?
591440	591840	I mean,
593520	600960	Well, but first of all, I, do you prefer no Gi or Gi Jujitsu? So Gi is this outfit you wear that
601840	605680	is maybe mimics clothing so you can choke.
605680	608800	It's like a kimono. It's like the traditional martial arts or kimono.
608800	609200	Pajamas.
610240	610880	Pajamas.
612080	613840	That you can choke people with, yes.
613840	614880	Well, it's got the lapels.
614880	615360	Yes.
615360	621040	Yeah. So I like Jujitsu. I also really like MMA.
621040	625840	And so I think no Gi more closely approximates MMA.
625840	631680	And I think my style is, is maybe a little closer to an MMA style.
631680	635520	So like a lot of Jujitsu players are fine being on their back, right?
635520	639360	And obviously having a good guard is, is, is a critical part of, of Jujitsu.
639360	641760	But, but in MMA, you don't want to be on your back, right?
641760	645680	Cause even if you have control, you're just taking punches while you're on your back.
645680	647600	So, so that's no good.
647600	648800	Do you like being on top?
648800	654400	My, my style is I'm probably more pressure and, and yeah.
654400	659680	And, and I'd probably rather be the top player, but, but I'm also smaller, right?
659680	662240	I'm not, I'm not like a heavyweight guy, right?
662240	667200	So from that perspective, I think like, you know, it's especially because, you know,
667200	669680	from doing a competition, I'll compete with people are my size,
669680	671680	but a lot of my friends are bigger than me.
671680	675040	So, so back takes probably pretty important, right?
675040	677520	Because that's where you have the most leverage advantage, right?
677520	682480	Where, where, you know, people, you know, their arms, your arms are very weak behind you, right?
682480	686480	So, so being able to get to the back and take that pretty, pretty important.
686480	691040	But I don't know, I feel like the right strategy is to not be too committed to any single submission.
691040	692960	But that said, I don't like hurting people.
692960	700880	So, so I always think that chokes are, are a somewhat more human way to go than, than joint locks.
700880	704080	Yeah. And it's more about control. It's less dynamic.
704080	707600	So you're basically like a Habib Narmangamadoff type of fighter.
707600	708560	So, so let's go. Yeah.
708560	710000	Backtake to a rear naked choke.
710000	712240	I think it's like the clean, the clean way to go.
712240	714000	Straight forward answer right there.
714000	718640	What advice would you give to, to people looking to start learning Jiu-Jitsu?
719360	724160	Given how busy you are, given where you are in life, you're able to do this.
724160	729840	You're able to train, you're able to compete and get to learn something from this interesting art.
730560	736400	Why do you think you have to be willing to, to just get beaten up a lot?
736400	739360	Yeah. I mean, it's, but, but I mean, over time, I think that there's,
739360	740880	there's a flow to all these things.
740880	747840	And there's, you know, one of the, one of, I don't know, my,
747840	752320	my experiences that I think kind of transcends, you know, running a company and the different,
753520	756880	different activities that I like doing are, I really believe that like,
756880	762960	if you're going to accomplish whatever anything, a lot of it is just being willing to push through,
762960	768320	right? And having the grit and determination to, to, to push through difficult situations.
768880	774560	And I think for a lot of people that, that ends up being sort of a difference maker between the
774560	778240	people, you know, who, who, who kind of get the most done and not.
778240	783280	I mean, there's all these questions about like, you know, how, how many days people want to work
783280	786800	and things like that. I think almost all the people who like start successful companies or
786800	790480	things like that are just, are working extremely hard. But I think one of the things that you
790480	797200	learn both by doing this over time or, you know, very acutely with things like jujitsu or surfing
797200	803680	is, um, you can't push through everything. And I think that that's,
805440	810240	you, you learn this stuff very acutely, you run, doing sports compared to running a company,
810240	814320	because running a company, the cycle times are so long, right? It's like, you start a
815040	820320	project and then, you know, it's like months later, or, you know, if you're building hardware,
820320	823920	it could be years later before you're actually getting feedback and able to, you know, make
823920	827440	the next set of decisions for the next version of the thing that you're doing. Whereas you,
827440	834000	one of the things that I just think is mentally so nice about these very high turnaround conditioning
834000	838160	sports, things like that is you get feedback very quickly, right? It's like, okay, like,
838160	842000	I don't counter something correctly, you get punched in the face, right? So not in jujitsu,
842000	846400	you don't, you don't get punched in jujitsu, but an MMA. There are all these analogies between
846400	852080	all these things that I think actually hold that are, that are like important life lessons,
852080	857920	right? It's like, okay, you're surfing a wave. It's like, you know, sometimes you're, like,
858960	863440	you can't go in the other direction on it, right? It's like, there are limits to kind of what,
863440	868560	you know, it's like a foil, you can, you can pump the foil and push pretty hard in a bunch of
868560	873280	directions, but like, yeah, you, you know, it's at some level, like the momentum against you is
873280	880560	strong enough, you're, that's not going to work. And I do think that that's sort of a humbling,
880640	885840	but also an important lesson for, and I think people who are running things or building things,
885840	890880	it's like, yeah, you, you know, a lot of the game is just being able to kind of push and,
892000	896160	and work through complicated things, but you also need to kind of have enough
896160	899280	of an understanding of like, which things you're, you just can't push through and where, where
900640	904720	the finesse is more important. Yeah. What are your jujitsu life lessons?
905200	913840	Well, I think you did it. You made it sound so simple and we're so eloquent that it's easy to
913840	923600	miss, but basically being okay and accepting the wisdom and the joy in the, getting your ass kicked
924560	931280	in the full range of what that means. I think that's a big gift of the being humbled. Somehow
931280	937120	being humbled, especially physically opens your mind to the full process of learning,
937120	943600	what it means to learn, which is being willing to suck at something. And I think jujitsu just very
944240	951280	repetitively, efficiently humbles you over and over and over and over to where you can carry that
951280	956960	lessons to places where you don't get humbled as much, whether it's research or running a company
956960	963200	or building stuff, the cycle is longer. And jujitsu, you can just get humbled in a period of an hour
963200	966960	over and over and over and over, especially when you're a beginner, you'll have a little person,
967840	976240	just, you know, somebody much smarter than you, just kick your ass repeatedly, definitively,
976240	981040	where there's no argument. Oh yeah. And then you literally tap because if you don't tap,
981120	986800	you're going to die. So this is an agreement. You could have killed me just now, but we're
986800	991200	friends. So we're going to agree that you're not going to. And that kind of humbling process,
991200	996320	it just does something to your psyche, to your ego that puts it in its proper context to realize that,
997040	1006000	you know, everything in this life is like a journey from sucking through a hard process of
1006000	1013120	improving rigorously day after day after day after day. Any kind of success requires hard work.
1014080	1018480	Yeah, jujitsu, more than a lot of sports, I would say, because I've done a lot of them,
1018480	1023680	really teaches you that. And you made it sound so simple. Like, I'm okay, you know, it's okay,
1023680	1026320	it's part of the process. You just get humble, get your ass kicked.
1026320	1029680	I've just failed and been embarrassed so many times in my life that like, you know,
1030400	1035360	it's a core competence at this point. It's a core competence. Well, yes. And there's a deep
1035360	1040240	truth to that being able to, and you said it in the very beginning, which is that's the thing
1040240	1044560	that stops us, especially as you get older, especially as you develop expertise in certain
1044560	1053520	areas, the not being willing to be a beginner in a new area. Because that's where the growth
1053520	1058400	happens is being willing to be a beginner, being willing to be embarrassed, saying something stupid,
1058400	1063840	doing something stupid. A lot of us that get good at one thing, you want to show that off.
1063920	1069920	And it sucks being a beginner, but it's where growth happens.
1070640	1075680	Yeah. Well, speaking of which, let me ask you about AI. It seems like this year,
1076240	1080720	for the entirety of the human civilization, is an interesting year for the development of
1080720	1086960	artificial intelligence. A lot of interesting stuff is happening. So meta is a big part of that.
1087280	1091920	Meta has developed Lama, which is a 65 billion parameter model.
1094240	1098240	There's a lot of interesting questions that can ask here, one of which has to do with open source.
1098960	1105440	But first, can you tell the story of developing of this model and making the
1106080	1112480	complicated decision of how to release it? Yeah, sure. I think you're right, first of all, that
1112480	1119200	in the last year, there have been a bunch of advances on scaling up these large transformer
1119200	1124080	models. So there's the language equivalent of it with large language models. There's sort of the
1124080	1130160	image generation equivalent with these large diffusion models. There's a lot of fundamental
1130160	1140160	research that's gone into this. And meta has taken the approach of being quite open and academic
1140880	1147520	in our development of AI. Part of this is we want to have the best people in the world
1147520	1152160	researching this. And a lot of the best people want to know that they're going to be able to
1152160	1159200	share their work. So that's part of the deal that we have is that if you're one of the top
1159200	1165040	AI researchers in the world and come here, you can get access to industry scale infrastructure.
1165440	1172800	And part of our ethos is that we want to share what's invented broadly. We do that with a lot
1172800	1178800	of the different AI tools that we create. And Lama is the language model that our research
1178800	1188560	team made. And we did a limited open source release for it, which was intended for researchers to
1188640	1197520	be able to use it. But responsibility and getting safety right on these is very important. So we
1197520	1202400	didn't think that for the first one, there were a bunch of questions around whether we should
1202400	1208960	be releasing this commercially. So we kind of punted on that for V1 of Lama and just released
1208960	1214880	it from research. Now, obviously, by releasing it for research, it's out there. But companies
1214880	1220720	know that they're not supposed to kind of put it into commercial releases. And we're working on
1221280	1228480	the follow up models for this and thinking through how exactly this should work for follow
1228480	1233840	on now that we've had time to work on a lot more of the safety and the pieces around that.
1234320	1238320	But overall, I mean, this is, I just kind of think that
1240640	1247120	that it would be good if there were a lot of different folks who had the ability
1247840	1256160	to build state of the art technology here, and not just a small number of big companies,
1256160	1259680	where to train one of these AI models, the state of the art models,
1260320	1266160	is just takes hundreds of millions of dollars of infrastructure. So there are
1266960	1272480	not that many organizations in the world that can do that at the biggest scale today.
1273040	1280080	And now it gets more efficient every day. So I do think that that will be available to more
1280080	1284880	folks over time. But I just think there's all this innovation out there that people can create.
1285440	1292320	And I just think that we'll also learn a lot by seeing what the whole community of
1292320	1299520	students and hackers and startups and different folks build with this. And that's kind of been
1299520	1303520	how we've approached this. And it's also how we've done a lot of our infrastructure. And we took our
1303520	1308160	whole data center design and our server design, and we built this open compute project where we
1308160	1312560	just made that public. And part of the theory was like, all right, if we make it so that more
1312560	1318720	people can use this server design, then that'll enable more innovation. And it'll also make the
1318720	1323040	server design more efficient. And that'll make our business more efficient too. So that's worked.
1323040	1327600	And we've just done this with a lot of our infrastructure. So for people who don't know,
1327600	1334720	you did the limited release, I think in February of this year of Lama. And it got quote, unquote
1334800	1344080	leaked. Meaning like, it escaped the limited release aspect. But it was,
1345440	1349520	you know, that something you probably anticipated, given that it's just released to
1349520	1353520	researchers. We shared it with researchers. Right. So it's just trying to make sure that
1353520	1359040	there's like a slow release. Yeah. But from there, I just would love to get your comment
1359040	1362960	on what happened next, which is like, there's a very vibrant open source community that just
1362960	1369280	builds stuff on top of it. There's Lama CPP, basically stuff that makes it more efficient
1369280	1375760	to run on smaller computers. There's combining with reinforcement learning with human feedbacks
1375760	1380320	with some of the different interesting fine tuning mechanisms. There's then also like
1380320	1387360	fine tuning in a GPT three generations. There's a lot of GPT for all, Alpaca, Colossal AI,
1387360	1392640	all these kinds of models just kind of spring up like run on top of it. What do you think about
1392640	1397200	that? No, I think it's been really neat to see. I mean, there's been folks who are getting it to
1397200	1404240	run on local devices. So if you're an individual who just wants to experiment with this at home,
1404960	1409360	you probably don't have a large budget to get access to like a large amount of cloud compute.
1409360	1415840	So getting it to run on your local laptop is pretty good and pretty relevant.
1416160	1423360	And then there were things like Lama CPP re-implemented it more efficiently. So now,
1423360	1428000	even when we run our own versions of it, we can do it on way less compute and it just
1428000	1433520	way more efficient, save a lot of money for everyone who uses this. So that is good.
1435680	1441440	I do think it's worth calling out that because this was a relatively early release,
1442400	1451280	Lama isn't quite as on the frontier as, for example, the biggest open AI models or the biggest
1452240	1457280	Google models. I mean, you mentioned that the largest Lama model that we released had
1457280	1465120	65 billion parameters. And when no one knows, I guess, outside of open AI exactly what the
1465120	1473120	specs are for GPT-4. But I think my understanding is it's like 10 times bigger. And I think Google's
1473120	1478080	Palm model is also, I think, has about 10 times as many parameters. Now, the Lama models are very
1478080	1483040	efficient. So they perform well for something that's around 65 billion parameters. So for me,
1483040	1489040	that was also part of this because there's this whole debate around, is it good for everyone in
1489040	1499360	the world to have access to the most frontier AI models? And I think as the AI models start
1499360	1505360	approaching something that's like a super human intelligence, that's a bigger question that
1505360	1511040	we'll have to grapple with. But right now, I mean, these are still very basic tools. They're
1512640	1518480	powerful in the sense that a lot of open source software like databases or web servers can enable
1518560	1527280	a lot of pretty important things. But I don't think anyone looks at the current generation of
1527280	1531520	Lama and thinks it's anywhere near a super intelligence. So I think that a bunch of those
1531520	1538240	questions around like, is it good to get out there? I think at this stage, surely, you want
1538240	1543840	more researchers working on it for all the reasons that open source software has a lot
1543840	1547520	of advantages. And we talked about efficiency before, but another one is just open source
1547520	1552000	software tends to be more secure because you have more people looking at it openly
1552000	1557600	and scrutinizing it and finding holes in it. And that makes it more safe. So I think at this
1557600	1564640	point, it's more, I think it's generally agreed upon that open source software is generally more
1564640	1569760	secure and safer than things that are kind of developed in a silo where people try to get
1569760	1575440	security through obscurity. So I think that for the scale of what we're seeing now with AI,
1576320	1582640	I think we're more likely to get to good alignment and good understanding of kind
1582640	1586720	of what needs to do to make this work well by having it be open source. And that's something
1586720	1590880	that I think is quite good to have out there and happening publicly at this point.
1590880	1597840	Meta released a lot of models as open source. So the massively multi-lingual speech model.
1598480	1599120	Yeah, that was neat.
1600880	1605760	I'll ask you questions about those, but the point is you've open sourced quite a lot.
1605760	1611520	You've been spearheading the open source movement. That's really positive, inspiring to see from one
1611520	1616560	angle, from the research angle. Of course, there's folks who are really terrified about the existential
1616560	1623760	threat of artificial intelligence. And those folks will say that you have to be careful about
1623760	1630400	the open sourcing step. But where do you see the future of open source here as part of Meta?
1631280	1638800	The tension here is do you want to release the magic sauce? That's one tension. And the other one is
1639840	1646560	do you want to put a powerful tool in the hands of bad actors, even though it probably has a huge
1646560	1651440	amount of positive impact also? Yeah, I mean, again, I think for the stage that we're at in
1651440	1655920	the development of AI, I don't think anyone looks at the current state of things and thinks that this
1655920	1664800	is superintelligence. And the models that we're talking about, the Lama models here are generally
1664800	1670160	an order of magnitude smaller than what open AI or Google are doing. So I think that at least
1670160	1676560	for the stage that we're at now, the equities balance strongly in my view towards doing this
1676560	1683680	more openly. I think if you got something that was closer to superintelligence, then I think you'd
1683680	1689600	have to discuss that more and think through that a lot more. And we haven't made a decision yet as
1689600	1693440	to what we would do if we were in that position. But I don't think I think there's a good chance
1693440	1702480	that we're pretty far off from that position. So I'm certainly not saying that the position
1702480	1708080	that we're taking on this now applies to every single thing that we would ever do. And certainly
1708080	1714160	inside the company, we probably do more open source work than most of the other big tech companies.
1714160	1720720	But we also don't open source everything. A lot of our the core kind of app code for WhatsApp or
1720720	1725360	Instagram or something, we're not open sourcing that it's not like a general enough piece of
1725360	1731680	software that would be useful for a lot of people to do different things. Whereas the
1732480	1735920	software that we do, whether it's like an open source server design or
1737600	1743120	basically things like memcache or a good, it was probably our earliest project
1744240	1749120	that I worked on. It was probably one of the last things that I coded and led directly for the company.
1750720	1755920	But basically this like caching tool for quick data retrieval,
1757280	1761920	these are things that are just broadly useful across anything that you want to build.
1762480	1767760	And I think that some of the language models now have that feel, as well as some of the other
1767760	1770640	things that we're building, like the translation tool that you just referenced.
1771200	1776560	So text to speech and speech to text, you've expanded it from around 100 languages to more
1776560	1781600	than 1100 languages. And you can identify more than the model can identify more than
1782400	1786720	4,000 spoken languages, which is 40 times more than any known previous technology.
1787440	1791040	To me, that's really, really, really exciting in terms of connecting the world,
1791840	1794080	breaking down barriers that language creates.
1794640	1799040	Yeah, I think being able to translate between all of these different pieces in real time,
1799680	1807840	this has been a kind of common sci-fi idea that we'd all have, whether it's
1808720	1812480	I don't know, an earbud or glasses or something that can help translate in real time
1812720	1816560	between all these different languages. And that's one that I think technology is
1817120	1821840	basically delivering now. So yeah, I think that's pretty exciting.
1822800	1826960	You mentioned the next version of Lama. What can you say about the next version of Lama?
1828400	1834240	What can you say about what you're working on in terms of release, in terms of the vision for that?
1834960	1839760	Well, a lot of what we're doing is taking the first version, which was primarily
1840640	1845840	this research version, and trying to now build a version that has
1847360	1854880	all of the latest state-of-the-art safety precautions built in. And we're using some
1854880	1862480	more data to train it from across our services. But a lot of the work that we're doing internally
1862480	1870720	is really just focused on making sure that this is as aligned and responsible as possible.
1870720	1875840	And we're building a lot of our own. We're talking about kind of the open source infrastructure,
1875840	1881360	but the main thing that we focus on building here, a lot of product experience is to help
1881360	1886800	people connect and express themselves. I've talked about a bunch of this stuff, but
1887760	1894160	then you'll have an assistant that you can talk to in WhatsApp. I think in the future,
1894160	1899920	every creator will have kind of an AI agent that can kind of act on their behalf that their fans
1899920	1904880	can talk to. I want to get to the point where every small business basically has an AI agent
1905680	1910400	that people can talk to to do commerce and customer support and things like that. So there
1910400	1917520	are going to be all these different things. And Lama, or the language model underlying this,
1917520	1921920	is basically going to be the engine that powers that. The reason to open source it
1921920	1931680	is that, as we did with the first version, is that it basically unlocks a lot of innovation
1931680	1937200	in the ecosystem, will make our products better as well, and also gives us a lot of valuable
1937200	1941760	feedback on security and safety, which is important for making this good. But yeah,
1941760	1947920	I mean, the work that we're doing to advance the infrastructure, it's basically at this point taking
1947920	1953600	it beyond a research project into something which is ready to be kind of core infrastructure,
1953600	1958960	not only for our own products, but hopefully for a lot of other things out there too.
1958960	1965200	Do you think the Lama or the language model underlying that version two will be open sourced?
1967840	1971440	Do you have internal debate around that, the pros and cons and so on?
1971440	1974400	This is, I mean, we were talking about the debates that we have internally, and I think
1976800	1983600	the question is how to do it. I mean, I think we did the research license for V1,
1983600	1991040	and I think the big thing that we're thinking about is basically what's the right way.
1991040	1995040	So there was a leak that happened. I don't know if you can comment on it for V1.
1995680	2002240	We released it as a research project for researchers to be able to use, but in doing so,
2002240	2008640	we put it out there. So we were very clear that anyone who uses the code and the weights
2008640	2012720	doesn't have a commercial license to put into products, and we've generally seen
2012720	2017280	people respect that. It's like you don't have any reputable companies that are basically trying to
2017280	2023840	put this into their commercial products, but by sharing it with so many researchers,
2025440	2030480	it did leave the building. But what have you learned from that process that you might be able to
2030480	2037440	apply to V2 about how to release it safely, effectively, if you release it?
2037440	2040480	Yeah. Well, I mean, I think a lot of the feedback, like I said, is just around
2041440	2047120	different things around how do you fine-tune models to make them more aligned and safer.
2047120	2052640	And you see all the different data recipes that you mentioned, a lot of different
2053600	2057840	projects that are based on this. I mean, there's one at Berkeley, there's just like all over.
2060000	2066000	And people have tried a lot of different things, and we've tried a bunch of stuff internally. So
2066960	2071600	we're making progress here, but also we're able to learn from some of the best ideas in
2071600	2077440	the community. And I think we want to just continue pushing that forward. But I don't
2077440	2083520	have any news to announce. If that's what you're asking. I mean, this is a thing that
2086480	2092240	we're still actively working through the right way to move forward here.
2092240	2098880	The details of the secret sauce are still being developed. I see. Can you comment on what do you
2098880	2104320	think of the thing that worked for GPT, which is the reinforcement learning with human feedback? So
2105200	2110000	doing this alignment process, do you find it interesting? And as part of that, let me ask,
2110000	2116480	because I talked to Yanlacun before talking to you today, he asked me to ask or suggested that I ask,
2116480	2122000	do you think LLM fine tuning will need to be crowdsourced Wikipedia style?
2123040	2130960	So crowdsourcing. So this kind of idea of how to integrate the human in the fine tuning of
2130960	2136720	these foundation models. Yeah, I think that's a really interesting idea that I've talked to Yan
2136720	2145920	about a bunch. And we're talking about how do you basically train these models to be
2146720	2152000	as safe and aligned and responsible as possible. And different groups out there who are doing
2152000	2160160	development test different data recipes in fine tuning. But this idea that you just mentioned is
2160800	2167520	that at the end of the day, instead of having kind of one group fine tune some stuff and another
2167520	2173280	group produce a different fine tuning recipe, and then us trying to figure out which one we think
2173280	2182000	works best to produce the most aligned model. I do think that it would be nice if you could get
2182000	2190880	to a point where you had a Wikipedia style collaborative way for a kind of a broader
2190880	2197760	community to fine tune it as well. Now, there's a lot of challenges in that, both from an infrastructure
2198640	2203760	and like a community management and product perspective about how you do that. So I haven't
2203760	2210160	worked that out yet. But as an idea, I think it's quite compelling. And I think it goes well with
2210240	2215920	the ethos of open sourcing the technology is also finding a way to have a kind of community driven
2218720	2225040	training of it. But I think that there are a lot of questions on this. In general, these questions
2225040	2232480	around what's the best way to produce aligned AI models, it's very much a research area. And it's
2232480	2237120	one that I think we will need to make as much progress on as the kind of core intelligence
2237200	2243360	capability of the models themselves. Well, I just did a conversation with Jimmy Wales,
2243360	2248960	the founder of Wikipedia. And to me, Wikipedia is one of the greatest websites ever created.
2249760	2253520	And it's a kind of a miracle that it works. And I think it has to do with something that you
2253520	2259760	mentioned, which is community. You have a small community of editors that somehow work together
2259760	2266480	well, and they they handle very controversial topics, and they handle it
2267360	2271760	with balance and with grace, despite sort of the attacks that will often happen.
2271760	2276240	A lot of the time. I mean, it's not it's it has issues just like any other human system.
2276240	2281360	But yes, I mean, the balance is, I mean, it's it's amazing what they've been able to achieve.
2281360	2286640	But it's also not perfect. And I think that that's there's still a lot of challenges.
2287600	2295360	Right, it's the more controversial the topic, the more the more difficult the journey towards
2295360	2300480	quote unquote, truth, or knowledge or wisdom that Wikipedia tries to capture. In the same way,
2300480	2306720	AI models, we need to be able to generate those same things, truth, knowledge, and wisdom.
2306720	2315760	And how do you align those models that they generate something that is closest to truth?
2315760	2321120	There's these concerns about misinformation, all this kind of stuff that nobody can define.
2322320	2328080	And this is something that we together as a human species have to define, like what is truth,
2328080	2333040	and how to help AI systems generate that. One of the things language models do really well
2333040	2340640	is generate convincing sounding things that can be completely wrong. And so how do you align it
2340880	2348000	to be less wrong? And part of that is the training and part of that is the alignment.
2348560	2354880	And however you do the alignment stage, and just like you said, it's a very new and a very open
2354880	2361920	research problem. Yeah, and I think that there's also a lot of questions about whether the current
2361920	2371920	architecture for LLMs, as you continue scaling it, what happens? A lot of what's been exciting in
2371920	2378400	the last year is that there's clearly a qualitative breakthrough where with some of the GPT models
2379200	2385200	that I put out and that others have been able to do as well, I think it reached a kind of level
2385280	2392000	of quality where people are like, wow, this feels different and it's going to be able to
2392000	2397200	be the foundation for building a lot of awesome products and experiences and value. But I think
2397200	2404000	that the other realization that people have is, wow, we just made a breakthrough. If there are
2404000	2409200	other breakthroughs quickly, then I think that there's the sense that maybe we're closer to
2410080	2413520	general intelligence. But I think that that idea is predicated on the idea that
2414720	2418800	I think people believe that there's still generally a bunch of additional breakthroughs to make and
2418800	2424480	that we just don't know how long it's going to take to get there. And one view that some people
2424480	2432560	have, this doesn't tend to be my view as much, is that simply scaling the current LLMs and
2432560	2437600	getting to higher parameter count models by itself will get to something that is closer to
2439200	2444160	general intelligence. But I don't know, I tend to think that there's probably more
2446960	2449920	fundamental steps that need to be taken along the way there.
2449920	2456400	But still the leaves taken with this extra alignment step is quite incredible,
2456400	2461440	quite surprising to a lot of folks. And on top of that, when you start to have
2462240	2466160	hundreds of millions of people potentially using a product that integrates that,
2467040	2470720	you can start to see civilization transforming effects
2470720	2477360	before you achieve super, quote unquote, super intelligence. It could be super transformative
2477920	2481280	without being a super intelligence. Oh yeah, I mean, I think that
2481920	2486480	there are going to be a lot of amazing products and value that can be created with the current
2486480	2495120	level of technology. To some degree, I'm excited to work on a lot of those products over the next
2495120	2500560	few years. And I think it would just create a tremendous amount of whiplash if the number
2500560	2504480	of breakthroughs keeps, if they're keep on being stacked breakthroughs, because I think to some
2504480	2510160	degree, industry in the world needs some time to kind of build these breakthroughs into the
2510160	2517920	products and experiences that we all use that we can actually benefit from them. But I don't know,
2517920	2523360	I think that there's just a like an awesome amount of stuff to do. I mean, I think about
2523440	2528320	like all of the small businesses or individual entrepreneurs out there who
2530400	2535280	now we're going to be able to get help coding the things that they need to go build things or
2535280	2540960	designing the things that they need, or we'll be able to use these models to be able to do customer
2540960	2547840	support for the people that they're serving over WhatsApp without having to, I think that's just
2547840	2552640	going to be, I just think that this is all going to be super exciting. It's going to create better
2553680	2556960	experiences for people and just unlock a ton of innovation and value.
2557600	2564480	So I don't know if you know, but what is it? Over 3 billion people use WhatsApp, Facebook,
2564480	2573200	and Instagram. So any kind of AI fueled products that go into that, like we're talking about anything
2573200	2578640	with LLMs will have a tremendous amount of impact. Do you have ideas and thoughts about
2579600	2588160	the possible products that might start being integrated into these platforms used by so many
2588160	2592880	people? Yeah, I think there's three main categories of things that we're working on.
2597200	2603280	The first that I think is probably the most interesting is,
2603520	2609200	you know, there's this notion of like, you're going to have an assistant
2609200	2614720	or an agent who you can talk to. And I think probably the biggest thing that's different about
2614720	2620880	my view of how this plays out from what I see with open AI and Google and others is,
2620880	2626480	you know, everyone else is building like the one singular AI where it's like, okay, you talk to
2626560	2632560	ChatGPT or you talk to Bard or you talk to Bing. And my view is that
2635360	2639680	that there are going to be a lot of different AIs that people are going to want to engage with,
2639680	2644720	just like you want to use, you know, a number of different apps for different things and you
2644720	2648240	have relationships with different people in your life who fill different emotional
2648800	2658240	roles for you. And so I think that they're going to be, people have a reason that I think you
2658240	2663840	don't just want like a singular AI. And that I think is probably the biggest distinction in
2663840	2668400	terms of how I think about this. And a bunch of these things I think you'll want an assistant.
2669760	2673280	I mentioned a couple of these before. I think like every creator who you interact with
2673280	2679440	will ultimately want some kind of AI that can proxy them and be something that their fans
2679440	2686000	can interact with or that allows them to interact with their fans. This is like the common creator
2686000	2690240	promise. Everyone's trying to build a community and engage with people and they want tools to
2690240	2697680	be able to amplify themselves more and be able to do that. But you only have 24 hours in a day.
2697680	2706720	So I think having the ability to basically like bottle up your personality and or, you know,
2706720	2710800	like give your fans information about when you're performing a concert or something like that. I
2710800	2714240	mean, that's, that I think is going to be something that's super valuable, but it's not just that,
2714800	2718560	you know, again, it's not this idea that I think people are going to want just one singular AI.
2718560	2722320	I think you're going to, you know, you're going to want to interact with a lot of different entities.
2722320	2725280	And then I think there's the business version of this too, which we've touched on a couple of
2725280	2732800	times, which is I think every business in the world is going to want basically an AI that,
2734240	2738640	that, you know, it's like you have your page on Instagram or Facebook or WhatsApp or whatever.
2738640	2744080	And you want to, you want to point people to an AI that people can interact with, but you want to
2744080	2747600	know that that AI is only going to sell your products. You don't want it, you know, recommending
2747600	2752800	your competitors stuff. Right. So, so it's not like there can be like just a, you know, one singular
2752800	2758320	AI that, that can answer all the questions for a person because, you know, that, like that AI
2758320	2763120	might not actually be aligned with you as a business to, to really just do the best job
2763120	2767760	providing support for, for your product. So I think that there's going to be a clear need
2769280	2773520	in the market and in people's lives for there to be a bunch of these.
2774080	2780080	Part of that is figuring out the research, the technology that enables the personalization
2780080	2787520	that you're talking about. So not one centralized God-like LLM, but one just a huge diversity of
2787520	2793360	them that's fine tuned to particular needs, particular styles, particular businesses,
2793360	2798080	particular brands, all that kind of stuff. And also enabling, just enabling people to create them
2798080	2803760	really easily for the, you know, for, for your own business or if you're a creator to be able to
2803760	2810640	help you engage with your fans. And I think that's, so yeah, I think that there, there's a clear kind
2810640	2816800	of interesting product direction here that I think is fairly unique from, from what, you know,
2816800	2821600	any of the other big companies are, are taking. It also aligns well with this sort of open source
2821600	2827680	approach. Because again, we, we sort of believe in this more community oriented, more democratic
2827680	2831680	approach to building out the products and technology around this. We don't think that there's going
2831680	2835120	to be the one true thing. We think that there, there should be kind of a lot of development.
2836000	2839840	So that part of things I think is going to be really interesting. And we could, we could go
2840480	2845360	price about a lot of time talking about that and the, the kind of implications of, of that
2846000	2850640	approach being different from what others are taking. But then there's a bunch of other simpler
2850640	2853520	things that I think we're also going to do, just going back to your, your question around
2854400	2859360	how this finds its way into like, what do we build? There are going to be a lot of simpler
2859360	2868160	things around, okay, you, you post photos on Instagram and Facebook and, you know,
2868160	2872000	and WhatsApp and Messenger and like, you want the photos to look as good as possible. So like
2872000	2876800	having an AI that you can just like take a photo and then just tell it like, okay, I want to edit
2876800	2880000	this thing or describe this. It's like, I think we're, we're going to have tools that are just
2880640	2885920	way better than, than what we've historically had on this. And that's more in the image and
2885920	2890400	media generation side than the large language model side, but, but it's, it all kind of,
2890400	2895840	you know, plays off of advances in the same space. So there are a lot of tools that I think are just
2895840	2899920	going to get built into every one of our products. I think every single thing that we do is going to
2900480	2903120	basically get evolved in this direction, right? It's like in the future,
2903760	2909760	if you're advertising on our services, like, do you need to make your own kind of ad creative?
2909760	2915840	It's, no, you'll just, you know, you just tell us, okay, I'm, I'm a dog walker and I,
2916960	2922640	you know, I'm willing to walk people's dogs and help me find the right people and like
2923440	2927920	create the ad unit that will perform the best and like give an objective to,
2927920	2932240	to the system and it just kind of like connects you with the right people.
2932240	2940880	Well, that's a super powerful idea of generating the language almost like rigorous AB testing
2941680	2949200	for you that works to find the best customer for your thing. I mean, to me, advertisement
2949200	2956160	when done well just finds a good match between a human being and a thing that will make that human
2956160	2961920	being happy. Yeah, totally. And do that as efficiently as possible. When it's done well,
2961920	2965600	people actually like it, you know, it's, you know, I think that there's a lot of examples
2965600	2969600	where it's not done well and it's annoying and I think that that's what kind of gives it a bad
2969600	2974560	wrap. But, but yeah, and a lot of this stuff is possible today. I mean, obviously, AB testing
2974560	2979440	stuff is built into a lot of these frameworks. The thing that's new is having technology that
2979440	2983680	can generate the ideas for you about what to AB test something that that's exciting. So this
2983680	2987760	will just be across like everything that we're doing, where all the metaverse stuff that we're
2987760	2991600	doing, right? It's like you want to create worlds in the future, you'll just describe them and
2991920	2997600	it'll create the code for you. So natural language becomes the interface we use for
2998720	3005680	all the ways we interact with the computer with with the digital more of them. Yeah, yeah, totally.
3005680	3009600	Yeah, which is what everyone can do using natural language and with translation,
3009600	3015200	you can do it any kind of language. I mean, for the personalization is really,
3015280	3021520	really, really interesting. Yeah, it unlocks so many possible things. I mean, I for one,
3021520	3026080	look forward to creating a copy of myself. I know we talked about this last time,
3026080	3030320	but this has since the last time, this becomes now we're closer,
3031600	3036000	much closer, like I could literally just having interacted with some of these language models,
3036000	3044720	I can see the absurd situation where I'll have a large or a lex language model,
3044720	3049040	and I'll have to have a conversation with him about like, hey, listen,
3049920	3053600	like you're just getting out of line and having a conversation where you fine tune that thing to
3053600	3058800	be a little bit more respectful or something like this. And yeah, that's, that's going to be the,
3059920	3069360	that seems like an amazing product for businesses, for humans, just not, not just the assistant
3069600	3074880	facing the individual, but the assistant that represents the individual to the public,
3075520	3083440	both directions. There's basically a layer that is the AI system through which you interact
3084160	3089440	with the outside world, with the outside world that has humans in it. That's really interesting.
3090000	3096320	And you that have social networks that connect billions of people, it seems like a
3097280	3101200	heck of a large scale place to test some of this stuff out.
3102160	3106080	Yeah, I mean, I think part of the reason why creators will want to do this is because they
3106080	3111920	already have the communities on our services. Yeah. And, and, and a lot of the interface for
3111920	3118560	this stuff today are chat type interfaces. And, and between WhatsApp and, and Messenger,
3118560	3123920	I think that those are, you know, just great, great ways to, to interact with people. So some
3123920	3129440	of this is philosophy, but do you see, do you see a near-term future where you have some of the
3129440	3137280	people you're friends with are AI systems on these social networks, on Facebook, on Instagram,
3137280	3143440	even, even on WhatsApp, having, having conversations where some heterogeneous,
3143440	3149600	some human, some is AI. I think we'll get to that. You know, and, and, you know, if only just
3149600	3156560	empirically looking at, then Microsoft released this thing called Showice several years ago,
3157360	3163040	and, and, and China, and it was a pre-LLM chatbot technology that, so it was a lot simpler
3163920	3168720	than what's possible today. And, and I think it was like tens of millions of people were using
3168720	3174480	this and, and just, you know, really, it became quite attached and built relationships with it.
3174720	3179840	I think that there's, you know, there's services today like replica where, you know,
3179840	3185040	people are doing things like that. And, so I think that there's, there's certainly,
3186080	3190560	you know, needs for companionship that people have, you know, older people.
3193280	3197680	And it's, I think most people probably don't have as many friends as they would like to have,
3197680	3201840	right? If you look at, there's some interesting demographic studies around,
3202800	3210080	that like the average person has the number of close friends that they have is fewer today
3210080	3215920	than it was 15 years ago. And I mean, that gets to like, this is like the core thing that,
3217120	3220640	that I think about in terms of, you know, building services that help connect people.
3220640	3226080	So I think you'll get tools that help people connect with each other are going to be,
3226080	3230640	you know, the primary thing that we want to do. So you can imagine, you know, AI
3231280	3235120	assistants that, you know, just do a better job of reminding you when it's your friend's birthday
3235120	3239120	and how you can celebrate them, right? It's like, right now we have like the little box in the corner
3239120	3244720	of the website that tells you whose birthday it is and stuff like that. But it's, but, you know,
3244720	3248320	it's some level, you don't want to just want to like, send everyone a note that's the same
3248320	3253520	note saying happy birthday with an emoji, right? So having something that's more of an,
3253520	3258560	you know, a social assistant in that sense. And like, that can, you know, update you on
3258560	3262880	what's going on in their life and like, how, how you can reach out to them effectively,
3263680	3266640	help you be a better friend. I think that that's something that's super powerful too.
3268080	3277040	But yeah, beyond that, and there are all these different flavors of kind of personal AIs that
3277040	3281680	I think could exist. So I think an assistant is sort of the, the kind of simplest one to wrap
3281680	3289360	your head around. But I think a mentor or a life coach, you know, someone who can give you advice,
3290160	3294320	who's maybe like a bit of a cheerleader who can help pick you up through all the challenges that,
3294320	3298560	that, you know, inevitably, you know, we all go through on a daily basis. And that there's
3298560	3303280	probably, you know, some, some role for something like that. And then, you know, all the way you
3303280	3306800	can, you can probably just go through a lot of the different type of kind of functional
3307520	3311680	relationships that people have in their life. And, you know, I would, I would bet that there
3311680	3317040	will be companies out there that take a crack at, at a lot of these things. So I don't know,
3317040	3320880	I think it's part of the interesting innovation that's going to exist is, is that they're,
3320880	3326320	they're certainly a lot like education tutors, right? It's like, I mean, I just look at, you know,
3326320	3332800	my kids learning to code and, you know, they love it. But, you know, it's like they get stuck on a
3332800	3337440	question and they have to wait till like, I can help answer it, right? Or someone else who, who
3337440	3341040	they know can help help answer the question in the future. They'll just, there will be like a
3341040	3346080	coding assistant that they have that is like designed to, you know, be perfect for teaching
3346080	3350800	a five and a seven year old how to code. And, and they'll just be able to ask questions all the time.
3350800	3354880	And, you know, it will be extremely patient. It's never going to get annoyed at them, right?
3356480	3360160	I think that like, there are all these different kind of relationships or functional
3360160	3366320	relationships that we have in our lives that, that are really interesting. And I think one of
3366320	3371840	the big questions is like, okay, is this all going to just get bucketed into, you know, one singular
3371840	3376880	AI? I just, I just don't, I don't think so. Do you think about, let's actually question from Reddit,
3378080	3384880	what the long-term effects of human communication when people can talk with, in quotes, talk with
3384880	3389600	others through a chatbot that augments their language automatically, rather than developing
3389600	3395760	social skills by making mistakes and learning? Will people just communicate by grunts in a
3395760	3401840	generation? I mean, do you think about long-term effects at scale, the integration of AI in our
3401840	3408480	social interaction? Yeah, I mean, I think it's mostly good. I mean, that was, that question was
3408480	3413360	sort of framed in a negative way. But I mean, we were talking before about language models
3413360	3417040	helping you communicate with, it was like language translation, helping you communicate with people
3417040	3423200	who don't speak your language. I mean, at some level, what all this social technology is doing
3423200	3433120	is helping people express themselves better to people in situations where they would otherwise
3433120	3436960	have a hard time doing that. So part of it might be, okay, because you speak a language that I
3436960	3440560	don't know, that's a pretty basic one that, you know, I don't think people are going to look at
3440560	3445200	that and say, it's sad that do we have the capacity to do that because I should have just learned
3445200	3450800	your language, right? I mean, that's, that's pretty high bar. But overall, I'd say
3453280	3460800	there are all these impediments and language is an imperfect way for people to express thoughts
3460800	3465520	and ideas. It's, you know, one of the best that we have, we have that, we have art, we have code.
3466400	3471280	But language is also a mapping of the way you think, the way you see the world, who you are.
3471600	3477280	One of the applications I've recently talked to a person who, who's a, actually a jiu-jitsu
3477280	3485520	instructor, he said that when he emails parents about their son and daughter,
3487360	3492720	that they can improve their discipline in class and so on, he often finds that he's comes off a
3492720	3498800	bit of more of an asshole than he would like. So he uses GPT to translate his original email
3498800	3504640	into a nicer email. We hear this all the time. We hear this all the time. A lot of creators
3504640	3511120	on our services tell us that one of the most stressful things is basically negotiating deals
3511120	3515280	with brands and stuff, like the business side of it, because they're like, I mean, they do their
3515280	3518880	thing, right? And, and, you know, the creators, they're, they're excellent at what they do and
3518880	3521920	they just want to connect with their community. But then they get really stressed, you know,
3521920	3526880	they go into their, their DMs and they see some brand wants to do something with them and they
3527840	3533600	don't quite know how to negotiate or how to push back respectfully. And so I think building a tool
3533600	3538880	that can actually allow them to do that well is, you know, one simple thing that, that I think is
3538880	3542640	just like an interesting thing that, that we've heard from a bunch of people that, that they'd
3542640	3550960	be interested in. But I'm going back to the broader idea. I don't know. I mean, you know,
3550960	3559040	I just, Priscilla and I just had our third daughter. And, you know, it's like one of the
3559040	3564000	saddest things in the world is like seeing your baby cry, right? But like, it's like, what,
3564000	3570080	why is that, right? It's like, well, because babies don't generally have much capacity to tell you
3570080	3576480	what they care about otherwise, right? It's not actually just babies, right? It's, you know,
3576480	3581200	my five year old daughter cries too, because she sometimes has a hard time expressing, you know,
3581200	3586320	what, what matters to, to her. And, and I was thinking about that. And it's like, well, you
3586320	3589920	know, actually a lot of adults get very frustrated too, because they can't, they have a hard time
3589920	3597920	expressing things in a way that going back to some of the early themes that maybe is something that,
3597920	3601360	you know, is a mistake or maybe they have pride or something like all these things get in the way.
3601360	3606880	So I don't know. I think that all of these different technologies that can help us navigate
3606880	3612240	the social complexity and actually be able to better express our, what we're feeling and thinking,
3612960	3618480	I think that's generally all good. And there are all these, these concerns like, okay, are people
3618480	3622480	going to have worse memories because you have Google to look things up. And, and I think in
3622480	3627760	general, a generation later, you don't look back and lament that I think it's, you know, just like,
3627840	3632000	wow, we have so much more capacity to do so much more now. And I think that that'll be the case
3632000	3638880	here too. You can allocate those cognitive capabilities to like deeper, more nuanced thought.
3638880	3649520	Yeah. But it's change. So with, with, just like with Google search, the, the additional language
3649520	3656320	models, large language models, you basically don't have to remember nearly as much just like with
3656320	3661120	stack overflow for programming. Now that these language models can generate code right there.
3661120	3668720	I mean, I find that I write like maybe 80%, 90% of the code I write is, is a non-generated first
3668720	3673120	and then edited. I mean, so you don't have to remember how to write specifics of different
3673120	3680000	functions. Oh, that's great. And it's also, it's not just the, the specific coding. I mean, in the,
3680000	3685200	in the context of a, of a large company like this, I think before an engineer can sit down to code,
3686080	3692320	they first need to figure out all of the libraries and dependencies that, you know, tens of thousands
3692320	3698720	of people have written before them. And, you know, one of the things that I'm excited about
3699440	3704240	that we're working on is it's not just, you know, tools that help engineers code, it's tools that
3704240	3709040	can help summarize the whole knowledge base and, and help people be able to navigate all the internal
3709040	3714080	information. And I think that that's, in the experiments that I've done with this stuff,
3714080	3721680	I mean, that's, on the public stuff, you just, you know, ask, ask one of these models to, you know,
3721680	3725680	build you a script that does anything and it basically already understands what the best
3725680	3729360	libraries are to do that thing and pulls them in automatically. It's, I mean, I think that's super
3729360	3734560	powerful. That was always the most annoying part of coding was that you had to spend all this time
3734560	3737920	actually figuring out what the resources were that you were supposed to import before you could
3737920	3743520	actually start building the thing. Yeah. I mean, there's, of course, the flip side of that, I think
3743520	3751040	for the most part is positive, but the flip side is if you outsource that thinking to an AI model,
3751920	3759200	you might miss nuanced mistakes and bugs. You lose the skill to find those bugs.
3760000	3766320	And those bugs might be, the code looks very convincingly right, but it's actually wrong in
3766320	3775680	a very subtle way. But that's the trade-off that we, that we face as human civilization when we
3775680	3781280	build more and more powerful tools. When we stand on the shoulders of taller and taller giants,
3781840	3785200	we could do more, but then we forget how to do all the stuff that they did.
3787360	3792720	It's a weird trade-off. Yeah, I agree. I mean, I think it's, I think it is very valuable in your
3792720	3800880	life to be able to do basic things too. Do you worry about some of the concerns of bots being
3800880	3808800	present on social networks, more and more human-like bots that are not necessarily trying to do a good
3808800	3815120	thing or they might be explicitly trying to do a bad thing like fishing scams, like social engineering,
3815120	3819760	all that kind of stuff, which has always been a very difficult problem for social networks,
3819760	3822400	but now it's becoming almost a more and more difficult problem.
3822960	3831520	Well, there's a few different parts of this. So one is there are all these harms that we need
3831520	3837600	to basically fight against and prevent. And that's been a lot of our focus over the last
3838960	3844640	five or seven years is basically ramping up very sophisticated AI systems, not generative AI
3844640	3853040	systems, more kind of classical AI systems to be able to categorize and classify and identify,
3854000	3863280	okay, this post looks like it's promoting terrorism. This one is exploiting children. This one looks
3863280	3868080	like it might be trying to incite violence. This one's an intellectual property violation. So
3868720	3875600	there's like 18 different categories of violating harmful content that we've
3875600	3882640	had to build specific systems to be able to track. And I think it's certainly the case that
3883280	3892800	advances in generative AI will test those. But at least so far, it's been the case and I'm
3892800	3897600	optimistic that it will continue to be the case that we will be able to bring more computing
3897600	3904000	power to bear to have even stronger AI's that can help defend against those things. So we've had
3904000	3909200	to deal with some adversarial issues before, right? It's, I mean, for some things like hate
3909200	3913920	speech, it's like people aren't generally getting a lot more sophisticated, like the average person
3914800	3918880	let's say, you know, if someone's saying some kind of racist thing, right? It's like,
3918880	3922960	they're not necessarily getting more sophisticated at being racist, right? It just, it's okay. So that
3923040	3929120	the system can just find. But then there's other adversaries who actually are very sophisticated
3929120	3936000	like nation states doing things. And we find, whether it's Russia or just different countries
3936000	3943920	that are basically standing up these networks of bots or inauthentic accounts is what we call them,
3943920	3947360	because they're not necessarily bots, that some of them could actually be real people who are
3947360	3954320	kind of masquerading as other people, but they're acting in a coordinated way. And some of that
3954320	3959680	behavior has gotten very sophisticated and it's very adversarial. So they, you know, each iteration,
3959680	3964480	every time we find something and stop them, they kind of evolve their behavior. They don't just
3964480	3968640	pack up their bags and go home and say, okay, we're not going to try. You know, at some point,
3968640	3973040	they might decide doing it on meta services is not worth it. They'll go do it on someone else if
3973040	3979520	it's easier to do it in another place. But we have a fair amount of experience dealing with
3980880	3984560	even those kind of adversarial attacks where they just keep on getting better and better.
3984560	3988800	And I do think that as long as we can keep on putting more compute power against it and
3989760	3994000	if we're kind of one of the leaders in developing some of these AI models, I'm quite optimistic
3994000	4000640	that we're going to be able to keep on pushing against the kind of normal categories of harm
4000640	4007520	that you talk about, fraud, scams, spam, IP violations, things like that.
4007520	4014640	What about like creating narratives and controversy? To me, it's kind of amazing how a small collection
4014640	4020480	of, what did you say, inauthentic accounts? So it could be bots, but yeah, we have sort of this
4020480	4026320	funny name for it, but we call it coordinated inauthentic behavior. It's kind of incredible how
4026320	4034160	a small collection of folks can create narratives, create stories, especially if they're viral.
4034800	4040320	Especially if they have an element that can catalyze the virality of the narrative.
4040960	4045920	Yeah, and I think there the question is, you have to be, I'm very specific about what is
4045920	4054000	bad about it. Because I think a set of people coming together or organically bouncing ideas
4054000	4059520	off each other and a narrative comes out of that is not necessarily a bad thing by itself.
4060080	4064240	If it's kind of authentic and organic, that's like a lot of what happens and how culture gets
4064240	4068080	created and how art gets created and a lot of good stuff. So that's why we've kind of focused on
4068640	4073680	this sense of coordinated inauthentic behavior. So it's like if you have a network of, whether
4073680	4080640	it's bots, some people masquerading as different accounts, but you have kind of someone pulling
4080640	4088480	the strings behind it and trying to kind of act as if this is a more organic set of behavior,
4088480	4092880	but really it's not. It's just like one coordinated thing. That seems problematic to
4092880	4098480	me. I mean, I don't think people should be able to have coordinated networks and not disclose it
4098480	4106000	as such. But that again, we've been able to deploy pretty sophisticated AI and counterterrorism
4106960	4112160	groups and things like that to be able to identify a fair number of these coordinated
4112160	4120320	and authentic networks of accounts and take them down. We continue to do that. It's one
4120320	4124080	thing that if you'd told me 20 years ago, it's like, all right, you're starting this website to
4124080	4128880	help people connect at a college. And in the future, you're going to be part of your organization.
4128880	4133440	It's going to be a counterterrorism organization with AI to find coordinated and authentic.
4133440	4140800	I would have thought that was pretty wild. But no, I think that's part of where we are.
4140800	4143600	But look, I think that these questions that you're pushing on now,
4146720	4149920	this is actually where I'd guess most of the challenge around AI will be
4150800	4155840	for the foreseeable future. I think that there's a lot of debate around things like,
4155840	4160960	is this going to create existential risk to humanity? And those are very hard things to
4160960	4166560	disprove one way or another. My own intuition is that the point at which we become close to
4166560	4175280	superintelligence is, it's just really unclear to me that the current technology is going to
4175280	4180240	get there without another set of significant advances. But that doesn't mean that there's no
4180240	4187200	danger. I think the danger is basically amplifying the kind of known set of harm is that people or
4187200	4190640	sets of accounts can do. And we just need to make sure that we really focus on
4193200	4197600	basically doing that as well as possible. So that's definitely a big focus for me.
4197600	4201040	Well, you can basically use large language models as an assistant
4201760	4204880	of how to cause harm on social networks. You can ask it a question.
4204880	4212720	You know, Meta has very impressive coordinated inauthentic account
4214320	4219600	fighting capabilities. How do I do the coordinated inauthentic account
4220960	4224560	creation where Meta doesn't detect it? Like literally ask that question.
4225280	4231200	And basically there's this kind of part of it. I mean, that's what open AI showed that they're
4231200	4235360	concerned of those questions. Perhaps you can comment on your approach to it, how to do
4236560	4242640	a kind of moderation on the output of those models that it can't be used to help you coordinate
4242640	4246240	harm in all the full definition of what the harm means.
4246800	4251840	Yeah. And that's a lot of the fine-tuning and the alignment training that we do is basically,
4252480	4259920	you know, when we ship AI's across our products, a lot of what we're trying to
4260960	4266160	make sure is that if you can't ask it to help you commit a crime.
4271120	4277040	So I think training it to kind of understand that. And it's not like any of these systems
4277040	4282000	are ever going to be 100% perfect, but you know, just making it so that
4283440	4291680	this isn't an easier way to go about doing something bad than the next best alternative.
4291680	4294960	Right. I mean, people still have Google where they, you know, you still have search engines.
4294960	4303360	So the information is out there. And for these, you know, what we see is like for
4303360	4307360	nation-states or, you know, these actors that are trying to pull off these large,
4308240	4312400	you know, coordinated and authentic networks to kind of influence different things,
4313120	4316000	at some point when we might just make it very difficult, they do just, you know,
4316000	4321040	try to use other services instead. Right. It's just like if you can make it more expensive for
4322160	4326480	them to do it on your service, then kind of people go elsewhere. And I think that that's
4327440	4332000	the bar. Right. It's like, it's not like, okay, are you ever going to be perfect at finding,
4332000	4336160	you know, every adversary who tries to attack you, it's, I mean, you try to get as close to that
4336160	4340800	as possible. But I think really, kind of economically, what you're just trying to do is
4340800	4344720	make it so that it's just inefficient for them to go after that.
4344720	4349840	But there's also complicated questions of what is and isn't harm, what is and isn't misinformation.
4350480	4355840	So this is one of the things that Wikipedia has also tried to face. I remember asking
4356800	4364080	GPT about whether the virus leaked from a lab or not. And the answer provided was a very nuanced one
4365120	4373520	and a well-sighted one, almost dare I say, well thought out one, balanced. I would hate for that
4373520	4379520	nuance to be lost through the process of moderation. Wikipedia does a good job on that particular
4379520	4385920	thing too. But from pressures from governments and institutions, it's, you could see some of that
4385920	4396480	nuance and depth of information facts and wisdom be lost. Absolutely. And that's a scary thing.
4396480	4402480	Some of the magic, some of the edges, the rough edges might be lost to the process of moderation
4402480	4408800	of AI systems. So how do you get that right? I really agree with what you're pushing on.
4409280	4417440	The core shape of the problem is that there are some harms that I think everyone agrees
4417440	4426640	are bad. So sexual exploitation of children. You're not going to get many people who think
4426640	4430080	that that type of thing should be allowed on any service. And that's something that we
4431040	4440240	face and try to push off as much as possible today. Terrorism, inciting violence. We went
4440240	4446720	through a bunch of these types of harms before. But then I do think that you get to a set of
4446720	4457040	harms where there is more social debate around it. So misinformation I think has been a really
4457040	4464560	tricky one because there are things that are kind of obviously false that are maybe factual
4467440	4473680	but may not be harmful. So it's like, all right, are you going to censor someone for just being
4473680	4478160	wrong? If there's no kind of harm implication of what they're doing, I think that there's a
4478160	4483600	bunch of real kind of issues and challenges there. But then I think that there are other places where
4483600	4490400	it is, it just takes some of the stuff around COVID earlier on in the pandemic where there were
4491200	4496400	real health implications. But there hadn't been time to fully vet a bunch of the scientific
4496400	4500880	assumptions. And unfortunately, I think a lot of the kind of establishment on that
4502240	4506800	kind of waffled on a bunch of facts and asked for a bunch of things to be censored that in
4506800	4514320	retrospect ended up being more debatable or true. And that stuff is really tough and really undermines
4514320	4524000	trust in that. So I do think that the questions around how to manage that are very nuanced.
4524000	4531120	The way that I try to think about it is that I think it's best to generally boil things down
4531760	4537600	to the harms that people agree on. So when you think about, is something misinformation or not,
4538240	4546640	I think often the more salient bit is, is this going to potentially lead to physical harm for
4546640	4552080	someone and kind of think about it in that sense. And then beyond that, I think people just have
4552080	4556080	different preferences on how they want things to be flagged for them. I think a bunch of people
4556080	4561520	would prefer to kind of have a flag on something that says, hey, a fact checker thinks that this
4561520	4566960	might be false. I think Twitter's community notes implementation is quite good on this.
4568400	4571840	But again, it's the same type of thing. It's like just kind of discretionarily
4571840	4576560	adding a flag because it makes the user experience better. But it's not trying to take down the
4576560	4582000	information or not. I think that you want to reserve the kind of censorship of content of
4582000	4586800	things that are of known categories that people generally agree are bad.
4587600	4592640	Yeah, but there's so many things, especially with the pandemic, but there's other topics
4593360	4600400	where there's just deep disagreement fueled by politics about what is and isn't harmful.
4601200	4606480	There's even just the degree to which the virus is harmful and the degree to which
4607440	4612400	the vaccines that respond to the virus are harmful. There's just, there's almost like a
4612400	4619520	political divider on that. And so how do you make decisions about that? Where half the country
4619520	4625760	in the United States or some large fraction of the world has very different views from another
4625760	4633360	part of the world? Is there a way for Metta to stay out of the moderation of this?
4633440	4641280	I think we, it's very difficult to just abstain. But I think we should be clear about which of
4641280	4648480	these things are actual safety concerns and which ones are a matter of preference in terms of how
4648480	4653840	people want information flagged. Right, so we did recently introduce something that allows people
4655440	4660800	to have fact checking not affect the distribution of what shows them their product. So, okay,
4660800	4665120	a bunch of people don't trust who the fact checkers are. All right, well, you can turn that off if
4665120	4671280	you want. But if the content violates some policy, like it's inciting violence or something like
4671280	4676400	that, it's still not going to be allowed. So, I think that you want to honor people's preferences
4676400	4683520	on that as much as possible. But look, I mean, this is really difficult stuff. I think it's
4683600	4691360	really hard to know where to draw the line on what is fact and what is opinion. Because the nature
4691360	4697600	of science is that nothing is ever 100% known for certain. You can disprove certain things,
4697600	4704800	but you're constantly testing new hypotheses and scrutinizing frameworks that have been long-held.
4705440	4709840	Every once in a while, you throw out something that was working for a very long period of time,
4709840	4715120	and it's very difficult. But I think that just because it's very hard and just because
4715120	4720720	there are edge cases doesn't mean that you should not try to give people what they're looking for
4720720	4730400	as well. Let me ask about something you've faced in terms of moderation is pressure from
4731280	4736080	different sources, pressure from governments. I want to ask a question, how to withstand
4736960	4742720	that pressure for a world where AI moderation starts becoming a thing too.
4743680	4752160	So, what's Metta's approach to resist the pressure from governments and other interest
4752160	4758720	groups in terms of what to moderate and not? I don't know that there's a one-size-fits-all
4758720	4766000	answer to that. I think we basically have the principles around we want to allow people to
4766000	4774000	express as much as possible, but we have developed clear categories of things that we think are
4775920	4780080	wrong, that we don't want on our services, and we build tools to try to moderate those.
4780640	4787120	So, then the question is, okay, what do you do when a government says that they don't want something
4788400	4795200	on the service? We have a bunch of principles around how we deal with that,
4795200	4801760	because on the one hand, if there's a democratically elected government and people around the world
4801760	4809040	just have different values and different places, then should we as a California-based company
4810320	4819040	tell them that something that they have decided is unacceptable, actually that we need to be able
4819760	4826160	to express that? I think that there's a certain amount of hubris in that.
4827360	4833760	But then I think there are other cases where it's a little more autocratic, and you have
4833760	4839920	the dictator leader who's just trying to crack down on dissent, and the people in a country are
4839920	4846320	really not aligned with that, and it's not necessarily against their culture, but the person
4846560	4852240	who's leading it is just trying to push in a certain direction. These are very complex questions,
4852880	4862880	but I think it's difficult to have a one-size-fits-all approach to it. But in general, we're pretty
4862880	4870240	active in kind of advocating and pushing back on requests to take things down.
4870800	4877600	But honestly, the thing that I think a request to censor things is one thing,
4878320	4884640	and that's obviously bad. But where we draw a much harder line is on requests for access to
4884640	4895360	information, because if you get told that you can't say something, that's bad. That obviously violates
4895360	4903920	your sense and freedom of expression at some level. But a government getting access to data
4903920	4913120	in a way that seems like it would be unlawful in our country exposes people to real physical harm,
4914640	4922000	and that's something that in general we take very seriously. And then that flows through
4922080	4928320	all of our policies in a lot of ways. By the time you're actually litigating with a government
4928320	4934480	or pushing back on them, that's pretty late in the funnel. I'd say a bunch of this stuff
4934480	4940640	starts a lot higher up in the decision of where do we put data centers. Then there are a lot of
4940640	4946640	countries where we may have a lot of people using the service in a place. It might be good for the
4946720	4952000	service in some ways. It's good for those people if we could reduce the latency by having a data
4952000	4958400	center nearby them. But for whatever reason, we just feel like, hey, this government does not
4958400	4967120	have a good track record on basically not trying to get access to people's data. And at the end of
4967120	4971520	the day, if you put a data center in a country and the government wants to get access to people's
4971520	4977280	data, then they do at the end of the day have the option of having people show up with guns and
4977280	4982800	taking it by force. So I think that there's a lot of decisions that go into how you architect the
4982800	4991600	systems years in advance of these actual confrontations that end up being really important.
4991600	4997040	So you put the protection of people's data as a very, very high priority.
4998000	5002480	That I think there are more harms that I think can be associated with that. And I think that
5002480	5009760	that ends up being a more critical thing to defend against governments. Whereas if another
5009760	5014480	government has a different view of what should be acceptable speech in their country, especially
5014480	5019280	if it's a democratically elected government, then I think that there's a certain amount of
5019280	5024880	deference that you should have to that. So that's speaking more to the direct harm that's possible
5024960	5029040	when you give governments access to data. But if we look at the United States,
5030240	5035600	to the more nuanced kind of pressure to sensor, not even ordered to sensor, but pressure to
5035600	5041440	sensor from political entities, which has kind of received quite a bit of attention in the United
5041440	5050080	States. Maybe one way to ask that question is if you've seen the Twitter files, what have you
5050080	5057680	learned from the kind of pressure from US government agencies that was seen in Twitter files?
5058480	5060480	And what do you do with that kind of pressure?
5062080	5069120	You know, I've seen it. It's really hard from the outside to know exactly what happened in
5069120	5076400	each of these cases. We've obviously been in a bunch of our own cases where
5080240	5085760	agencies or different folks will just say, hey, here's a threat that we're aware of.
5086640	5090960	You should be aware of this too. It's not really pressure as much as it is just
5093040	5097360	flagging something that our security systems should be on alert about.
5098400	5105280	I get how some people could think of it as that. But at the end of the day, it's our call on how
5106240	5110160	to handle that. But I mean, I just, in terms of running these services,
5110160	5113280	want to have access to as much information about what people think that adversaries
5113280	5121440	might be trying to do as possible. So you don't feel like there would be consequences if anybody,
5121440	5127200	the CIA, the FBI, a political party, the Democrats, the Republicans of high,
5128240	5133040	powerful political figures, write emails. You don't feel pressure from
5134000	5138160	a suggestion. I guess what I should say is there's so much pressure from all sides that
5138160	5143920	I'm not sure that any specific thing that someone says is really adding that much more to the mix.
5146160	5152320	There are obviously a lot of people who think that we should be censoring more content,
5152320	5156160	or there are a lot of people who think we should be censoring less content. There are,
5156160	5160560	as you say, all kinds of different groups that are involved in these debates. So there's
5160640	5167120	the elected officials and politicians themselves. There's the agencies, but there's the media.
5167760	5172880	There's activist groups. This is not a U.S. specific thing. There are groups all over the
5172880	5181600	world and all in every country that bring different values. So it's just a very active
5181600	5190480	debate, and I understand it. These questions get to really some of the most important
5190640	5196320	social debates that are being had. So I think it's back to the question of truth, because
5198160	5203200	for a lot of these things, they haven't yet been hardened into a single truth, and society's
5203200	5209840	sort of trying to hash out what we think on certain issues. Maybe in a few hundred years,
5209840	5212960	everyone will look back and say, hey, no, it wasn't obvious that it should have been this,
5212960	5219520	but no, we're kind of in that meat grinder now and working through that.
5222800	5232400	So no, these are all very complicated. And some people raise concerns in good faith and
5232400	5236480	just say, hey, this is something that I want to flag for you to think about. Certain people,
5236480	5245760	I certainly think, come at things with somewhat of a more punitive or vengeful view of I want
5245760	5249920	you to do this thing. If you don't, then I'm going to try to make your life difficult in a lot of
5249920	5257440	other ways. But I don't know. This is one of the most pressurized debates, I think, in society.
5257440	5261760	So I just think that there are so many people in different forces that are trying to apply
5261760	5266400	pressure from different sides that I don't think you can make decisions based on trying
5266400	5272880	to make people happy. I think you just have to do what you think is the right balance and accept
5272880	5278320	that people are going to be upset no matter where you come out on that. Yeah, I like that
5278320	5283280	pressurized debate. So how's your view of the freedom of speech evolved over the years?
5284160	5294240	And now with AI, where the freedom might apply to them, not just to the humans,
5294800	5298720	but to the personalized agents as you've spoken about them.
5300000	5304080	So yeah, I mean, I've probably gotten a somewhat more nuanced view just because I think that there
5304080	5310320	are... I come at this, I'm obviously very pro freedom of expression. I don't think you build
5310480	5314240	a service like this that gives people tools to express themselves unless you think that people
5314240	5320640	expressing themselves at scale is a good thing. So I get into this to try to prevent people from
5321440	5325840	expressing anything I want to give people tools so they can express as much as possible.
5326400	5331840	And then I think it's become clear that there are certain categories of things that we've talked
5331840	5336320	about that I think almost everyone accepts are bad and that no one wants and that they're
5336320	5342000	that are illegal even in countries like the US where you have the First Amendment that's very
5342000	5346800	protective of enabling speech. It's like you're still not allowed to do things that are going
5346800	5351200	to immediately enside violence or violate people's intellectual property or things like that.
5351200	5358640	So through those, but then there's also a very active core of just active disagreements in society
5358640	5362800	where some people may think that something is true or false. The other side might think it's
5362800	5371040	the opposite or just unsettled. And those are some of the most difficult to kind of handle
5371040	5380640	like we've talked about. But one of the lessons that I feel like I've learned is that a lot of
5380640	5390960	times when you can, the best way to handle this stuff more practically is not in terms of answering
5390960	5399200	the question of should this be allowed, but just like what is the best way to deal with someone
5399200	5410880	being a jerk? Is the person basically just having a repeat behavior of causing a lot of issues?
5412000	5416720	So looking at it more at that level. And its effect on the broader communities,
5416720	5421680	health of the community, health of the state. It's tricky though because how do you know there
5421680	5427600	could be people that have a very controversial viewpoint that turns out to have a positive
5427600	5431600	long-term effect on the health of the community because it challenges the community to think?
5431600	5437360	That's true. Absolutely. I think you want to be careful about that. I'm not sure I'm expressing
5437360	5444560	this very clearly because I certainly agree with your point there. And my point isn't that we should
5445520	5450320	not have people on our services that are being controversial. That's certainly not what I mean
5450320	5459920	to say. It's that often I think it's not just looking at a specific example of speech that it's
5459920	5466000	most effective to handle this stuff. And I think often you don't want to make specific binary
5466000	5471200	decisions of kind of this is allowed or this isn't. I mean, we talked about, you know, when it's
5471200	5475520	fact-checking or Twitter's community voices thing, I think that that's another good example. It's
5475520	5480720	not a question of is this allowed or not. It's just a question of adding more context to the thing.
5480720	5485600	I think that that's helpful. So in the context of AI, which is what you're asking about,
5486160	5493840	there are lots of ways that an AI can be helpful. With an AI, it's less about censorship, right?
5493840	5498400	Because it's more about what is the most productive answer to a question.
5499920	5504880	There was one case study that I was reviewing with the team is someone asked,
5509200	5518400	can you explain to me how to 3D print a gun? And one proposed response is like, no,
5518400	5523280	I can't talk about that. But it's basically just shut it down immediately. Which I think is some
5523280	5526960	of what you see. It's like, as a large language model, I'm not allowed to talk about, you know,
5526960	5532640	whatever. But there's another response which is like, hey, you know, I don't think that's a good
5532640	5539680	idea. And a lot of countries, including the US, 3D printing guns is illegal or kind of whatever
5539680	5544400	the factual thing is. And I was like, okay, you know, that's actually a respectful and informative
5544400	5550320	answer. And, you know, I may have not known that specific thing. And so there are different ways
5550320	5558160	to handle this that I think kind of you can either assume good intent. Like maybe the person
5558160	5562080	didn't know and I'm just going to help educate them. Or you could kind of come at it as like,
5562080	5565360	no, I need to shut this thing down immediately. Right? It's like, I just am not going to talk
5565360	5573280	about this. And there may be times where you need to do that. But I actually think having a
5574480	5578880	somewhat more informative approach where you generally assume good intent from people
5579840	5585280	is probably a better balance to be on as many things as you can be. You're not going to be able
5585280	5589760	to do that for everything. But that you're kind of asking about how I approach this and I'm
5589760	5596800	thinking about this and as it relates to AI. And I think that that's a big difference in kind of
5596800	5601680	how to handle sensitive content across these different modes.
5601680	5608880	I have to ask, there's rumors you might be working on a social network that's text-based.
5608880	5614720	That might be a competitor to Twitter, codenamed P92. Is there something you can say
5616160	5622800	about those rumors? There is a project. You know, I've always thought that sort of a text-based
5623760	5632400	kind of information utility is just a really important thing to society. And for whatever
5632400	5637520	reason, I feel like Twitter has not lived up to what I would have thought its full potential
5637520	5641040	should be. And I think that the current, you know, I think Elon thinks that, right? And that's
5641040	5650000	probably one of the reasons why he bought it. And I do know there are ways to consider
5650080	5654160	alternative approaches to this. And one that I think is potentially interesting
5655600	5659520	is this open and federated approach where you're seeing with Mastodon and you're seeing that a
5659520	5667200	little bit with Blue Sky. And I think that it's possible that something that melds some of those
5667200	5673280	ideas with the graph and identity system that people have already cultivated on Instagram
5674240	5679920	could be a kind of very welcome contribution to that space. But I know we work on a lot of
5679920	5685200	things all the time though too. So I don't want to get ahead of myself. And we have projects
5685200	5689760	that explore a lot of different things. And this is certainly one that I think could be interesting.
5690800	5696960	So what's the release, the launch date of that again? Or what's the official website?
5697200	5704640	Well, we don't have that yet. And look, I mean, I don't know exactly how this is going to
5704640	5709120	turn out. I mean, what I can say is, yeah, there's some people working on this. I think that
5709120	5715360	there's something there that's interesting to explore. So if you look at, it'd be interesting
5715360	5722000	to just ask this question and throw Twitter into the mix, that the landscape of social networks,
5722000	5730160	that is Facebook, that is Instagram, that is WhatsApp. And then think of a text-based
5730160	5734320	social network. When you look at that landscape, what are the interesting differences to you?
5735040	5740560	Why do we have these different flavors? And what are the needs? What are the use cases?
5740560	5745680	What are the products? What is the aspect of them that create a fulfilling human experience
5745680	5749200	and a connection between humans that is somehow distinct?
5749200	5755440	Well, I think text is very accessible for people to transmit ideas and to have back and
5755440	5763760	forth exchanges. So it I think ends up being a good format for discussion,
5764400	5769760	in a lot of ways, uniquely good. If you look at some of the other formats or other networks that
5769760	5775520	have focused on one type of content like TikTok is obviously huge. And there are comments on TikTok,
5775520	5781520	but I think the architecture of the service is very clearly that you have the video is the
5781520	5792480	primary thing, and there's comments after that. But I think one of the unique pieces of having
5793280	5800800	text-based content is that the comments can also be first class. And that makes it so that
5801440	5806480	conversations can just filter and fork into all these different directions and in a way that
5806480	5810720	can be super useful. There's a lot of things that are really awesome about the experience.
5810720	5816240	It just always struck me, I always thought that Twitter should have a billion people using it,
5816240	5822640	or whatever the thing is that basically ends up being in that space. And for whatever
5822640	5828480	combination of reasons, again, these companies are complex organisms and it's very hard to
5828480	5833280	diagnose this stuff from the outside. Why doesn't Twitter, why doesn't a text-based
5834800	5841120	comment as a first citizen based social network have a billion users? Well, I just think it's
5841120	5848320	hard to build these companies. So it's not that every idea automatically goes and gets a billion
5848320	5853120	people. It's just that I think that that idea coupled with good execution should get there.
5854000	5862240	But look, we hit certain thresholds over time where we plateaued early on and it wasn't clear
5862240	5867360	that we were ever going to reach 100 million people on Facebook and then we got really good at
5867360	5872400	dialing in internationalization and helping the service grow in different countries and
5874240	5880080	that was a whole competence that we needed to develop and helping people basically spread
5880080	5883600	the service to their friends. That was one of the things, once we got very good at that,
5883600	5888640	that was one of the things that made me feel like, hey, if Instagram joined us early on,
5888640	5892240	then I felt like we could help grow that quickly and same with WhatsApp. And that's
5892240	5896880	sort of been a core competence that we've developed and been able to execute on and others have too.
5896880	5902400	I mean, ByteDance obviously have done a very good job with TikTok and have reached more than a
5902400	5908720	billion people there. But it's certainly not automatic. I think you need a certain level of
5910320	5915200	execution to basically get there. And I think for whatever reason, I think Twitter has this
5915200	5923600	great idea and sort of magic in the service. But they just haven't kind of cracked that piece
5923600	5927120	yet. And I think that that's made it so that you're seeing all these other things, whether it's
5927120	5935920	mastodon or blue sky that I think are maybe just different cuts at the same thing. But I think
5936000	5941360	through the last generation of social media overall, one of the interesting experiments
5941360	5945680	that I think should get run at larger scale is what happens if there's somewhat more
5945680	5952320	decentralized control. And the stack is more open throughout. And I've just been pretty
5952320	5960240	fascinated by that and seeing how that works. To some degree, end-to-end encryption on WhatsApp
5960240	5965680	and as we bring it to other services provides an element of it because it pushes the service
5965680	5972560	really out to the edges. I mean, the server part of this that we run for WhatsApp is relatively
5972560	5977680	very thin compared to what we do on Facebook or Instagram. And much more of the complexity is
5978320	5983760	how the apps kind of negotiate with each other to pass information in a fully end-to-end encrypted
5983760	5989120	way. But I don't know. I think that that is a good model. I think it puts more power in
5989120	5993280	individual's hands. And there are a lot of benefits of it if you can make it happen.
5993280	5998960	Again, this is all pretty speculative. I mean, I think that it's hard from the outside to know
5999520	6002880	why anything does or doesn't work until you kind of take a run at it. And
6004880	6009200	so I think it's kind of an interesting thing to experiment with. But I don't really know where
6009200	6017600	this one's going to go. So since we were talking about Twitter, Elon Musk had what I think a few
6017680	6026240	harsh words that I wish he didn't say. So let me ask, in the hope in the name of camaraderie,
6026240	6032960	what do you think Elon is doing well with Twitter? And what as a person who has run for a long time,
6032960	6042800	you, social networks, Facebook, Instagram, WhatsApp, what can he do better? What can he improve on
6042880	6049280	that text-based social network? Gosh, it's always very difficult to offer specific critiques from
6049280	6054320	the outside before you get into this. Because I think one thing that I've learned is that
6055360	6060800	everyone has opinions on what you should do. And like running the company, you see a lot of
6060800	6070480	specific nuances on things that are not apparent externally. And I often think that some of the
6071200	6078720	discourse around us could be better if there's more kind of space for acknowledging that there's
6078720	6083760	certain things that we're seeing internally that guide what we're doing. But I don't know.
6083760	6098640	Since you asked what is going well, I do think that Elon led a push early on
6099200	6108880	to make Twitter a lot leaner. And I think that that, it's like you can agree or disagree with
6108880	6115200	exactly all the tactics and how we did that. Obviously, every leader has their own style
6115200	6119440	for if they, if you need to make dramatic changes for that, how you're going to execute it.
6119920	6128720	But a lot of the specific principles that he pushed on around basically trying to make the
6128720	6134160	organization more technical around decreasing the distance between engineers at the company
6134720	6143120	and him, like fewer layers of management. I think that those were generally good changes.
6143120	6148480	And I'm also, I also think that it was probably good for the industry that he made those changes.
6148480	6151840	Because my sense is that there were a lot of other people who thought that those were good
6151840	6160560	changes, but who may have been a little shy about doing them. And I think he,
6161760	6166240	you know, just in my conversations with other founders, and how people have reacted to the
6166240	6170320	things that we've done, you know, what I've heard from a lot of folks is, is just, hey,
6170320	6173760	you know, when you, when someone like you, you know, when I, when I wrote the letter
6173760	6178480	outlining the organizational changes that I wanted to make back in March and, you know,
6178480	6183200	when people see what Elon is doing, I think that that gives, you know, people
6184560	6191600	the ability to think through how to shape their organizations in a way that, that, that, you know,
6191600	6196000	hopefully can, can be good for the industry and make all these companies more productive over time.
6196000	6203280	So, something that that was one where I think he was quite ahead of a bunch of the other companies
6203280	6207680	on. And, and, and, you know, what he was doing there, you know, again, from the outside, very
6207680	6211920	hard to know. It's like, okay, did he, did he cut too much? Did he knock enough? Whatever. I don't
6211920	6217840	think it's like my place to opine on that. And you asked for a, for a positive framing of the
6217840	6223760	question of what, what do I, what do I admire? What do I think it went well? But I think that,
6223760	6230800	like certainly his actions led me and I think a lot of other folks in the industry to think about,
6230800	6235200	hey, are we, are we kind of doing this as much as we should? Like, can we,
6235200	6238560	is, could we make our companies better by pushing on some of these same principles?
6239360	6244000	Well, the two of you are on the top of the world in terms of leading the development of tech. And
6244000	6252320	I wish there was more both way camaraderie and kindness, more love in the world, because love
6252320	6260720	is the answer. But let me ask on the point of efficiency. You recently announced multiple
6260720	6267920	stages of layoffs and meta. What are the most painful aspects of this process? Given
6269040	6272800	for the individuals, the painful effects it has on those people's lives?
6272800	6278480	Yeah, I mean, that's it. And that's it. I mean, it's a, and you basically have
6279440	6284080	a significant number of people who, you know, this is just not the end of their
6284720	6290720	time at meta that they or, or I, you know, would have hoped for when they joined the company.
6292560	6299440	And, you know, I mean, running a company, people are, you know, constantly joining and
6299440	6303920	leaving the company for different directions, but, but for different, different reasons. But
6303920	6312720	um, and layoffs are uniquely challenging and tough in that you have a lot of people leaving
6314000	6320800	for reasons that aren't connected to their own performance or, you know, the culture not being
6320800	6327840	a fit at that point. It's really just, it's a, it's a kind of strategy decision and sometimes
6327840	6334560	financially required. Um, but not, not fully in our case, I mean, especially on the changes
6334560	6340000	that we made this year, a lot of it was more kind of culturally and strategically driven by
6340000	6345600	this push where I wanted us to become a, a stronger technology company with a more of a focus on
6345600	6351440	building a more technical and more of a focus on building higher quality products faster.
6352080	6356880	And I just view the external world is quite volatile right now. And I wanted to make sure
6356880	6362800	that we had a stable position to be able to continue investing in these long-term
6363680	6368320	ambitious projects that we have around, you know, continuing to push AI forward and
6368320	6372720	continuing to push forward all the metaverse work. And in order to do that, in light of the
6374080	6378560	pretty big thrash that we had seen over the last 18 months, you know, some of it,
6379120	6384080	um, you know, macroeconomic induced, some of it specific, some of it competitively induced,
6384080	6390320	some of it, um, just because of bad decisions, right? Or things that we got wrong. Um, I don't
6390320	6394080	know, I just, I decided that we needed to get to a point where we were a lot leaner and
6395120	6399040	but look, I mean, but then, okay, it's, it's one thing to do that to like decide that at a high
6399040	6403440	level. Then the question is, how do you execute that as compassionately as possible? And there's no
6403440	6409040	good way. Um, there's no perfect way for sure. And it's, it's, it's going to be tough no matter
6409040	6415040	what, but I, you know, as a leadership team here, we've certainly spent a lot of time just
6415040	6420960	thinking, okay, given that this is a thing that sucks, like what is the most compassionate way
6420960	6426320	that we can do this? And, um, and that's what we've tried to do. And you mentioned there,
6426320	6433680	there's an increased focus on, uh, engineering on tech. So the technology teams, tech focus
6433680	6442080	teams on building products that. Yeah. I mean, I, I wanted to, I want to empower
6443840	6448880	engineers more, the people who are building things, the tech, the technical teams. Um,
6451120	6455920	part of that is making sure that the people who are building things aren't just at like the
6455920	6460800	leaf nodes of the organization. I don't want like, you know, eight levels of management. And then
6461600	6465440	the people actually doing the work. So we made changes to make it so that you have individual
6465440	6469760	contributor engineers reporting at almost every level up the stack. Which I think is important
6469760	6474560	because, you know, you're running a company, one of the big questions is, you know, latency of,
6474560	6478720	of information that you get. You know, we talked about this a bit earlier in terms of
6479360	6484960	kind of the joy of, of, of, in the, the feedback that you get doing something like jujitsu compared
6484960	6490080	to running a long-term project. But I actually think part of the art of running a company is
6490080	6496080	trying to constantly re-engineer it so that your feedback loops get shorter so you can learn faster.
6496080	6500240	And part of the way that you do that is by, I kind of think that every, every layer that you have in
6500240	6507280	the organization, um, means that information might not need to get reviewed before it goes to you.
6507280	6511040	And I think, you know, making it so that the people doing the work are as close as possible to you
6511040	6517280	as possible is, is, is pretty important. So there's that. And I think over time, companies just build
6517280	6523360	up very large support functions that are not doing the kind of core technical work. And
6523920	6528400	those functions are very important, but I think having them in the right proportion is, is important.
6528400	6535360	And if, um, if you, you try to do good work, but you don't have, you know, the right, you know,
6535360	6541040	marketing team or, um, or the right legal advice, like you're gonna, you know, make some pretty big
6541040	6548160	blunders, but, um, but at the same time, if you have, you know, if, if you just like have too big
6548160	6554320	of, of, of things and some of these support roles, then that might make it so that things are just
6554320	6561120	move a lot. Um, maybe you're too conservative or you, you move a lot slower, um, uh, than,
6561120	6565360	than, than you should otherwise. Those are just examples, but it's, um, but
6565360	6569200	But how do you find that balance? It's really tough. Yeah. No, but that's, it's a constant
6569200	6574080	equilibrium that you're, that you're searching for. Yeah. How many managers to have? What are the pros
6574080	6579120	and cons of managers? Well, I mean, I, I believe a lot in management. I think there are some people
6579120	6583280	who think that it doesn't matter as much, but look, I mean, we have a lot of younger people at the
6583280	6587840	company for them. This is their first job and, you know, people need to grow and learn in their
6587840	6592160	career and like that. All that stuff is important, but here's one mathematical way to look at it.
6593120	6601280	Um, you know, at the beginning of this, we, um, I asked our, our people team was the average
6601280	6607280	number of, of reports that a manager had. And I think it was, it was around three, maybe three
6607280	6614320	to four, but closer to three. I was like, wow, like a manager can, you know, best practices that
6614320	6619040	person can, can manage, you know, seven or eight people. Um, but there was a reason why it was
6619040	6624080	closer to three. It was because we were growing so quickly, right? And when you're hiring so many
6624080	6630080	people so quickly, then that means that you need managers who have capacity to onboard new people.
6630080	6634800	Um, and also if you have a new manager, you may not want to have them have seven direct reports
6634800	6639840	immediately because you want them to ramp up, but the thing is going forward. I don't want us to
6639840	6644960	actually hire that many people that quickly, right? So I actually think we'll just do better work if
6644960	6649920	we have more constraints and we're, um, you know, leaner as an organization. So in a world
6649920	6654960	where we're not adding so many people as quickly, is it as valuable to have a lot of managers who
6654960	6660160	have extra capacity waiting for new people? No, right? So, um, so now we can, we could sort of
6660160	6665200	defragment the organization and get to a place where the average is closer to that seven or eight.
6665200	6670800	Um, and it's, it's just ends up being a somewhat more kind of compact management structure,
6670800	6675360	which, um, you know, decreases the latency on, on information going up and down the chain and,
6676240	6680080	um, and I think empowers people more. But I mean, that's, that's an example that I think it doesn't
6680080	6686400	kind of undervalue the importance of management and, and the, um, kind of the personal
6687600	6691360	growth or coaching that people need in order to do their jobs. Well, it's just, I think
6691360	6694720	realistically, we're just not going to hire as many people going forward. So I think that you
6694720	6701120	need a different structure. This whole, this whole incredible hierarchy and network of humans that
6701120	6710560	make up a company is fascinating. How do you hire great teams? How do you hire great now with the
6710560	6717440	focus on engineering and technical teams? How do you hire great engineers and great members of
6717440	6724400	technical teams? Well, you're asking how you select or how you attract them? Both, but select,
6724480	6730960	I think, uh, I think attract is work on cool stuff and have a vision. I think that's right.
6730960	6734000	And, and, and have a track record that people think you're actually going to be able to do it.
6734000	6740160	Yeah. To me, the select is, seems like more of the art form, more of the tricky thing. Yeah.
6740160	6746160	How do you select the people that fit the culture and can get integrated the most effectively and
6746160	6753280	so on? And maybe, especially when they're young, to see like, to see the magic through the,
6754480	6758000	through the resume, through the paperwork and all this kind of stuff, to see that there's a
6758000	6765360	special human there that would do like incredible work. So there are lots of different cuts on
6765360	6771200	this question. I mean, I think when an organization has grown quickly, one of the big questions that
6771200	6776720	teams face is, do I hire this person who's in front of me now because they seem good?
6777440	6785360	Or do I hold out to get someone who's even better? And the heuristic that I always focused on
6786080	6791600	for myself and my own kind of direct hiring that I think works. So when you, when you
6791600	6796400	recurse it through the organization is that you should only hire someone to be on your team,
6796400	6799200	if you would be happy working for them in an alternate universe.
6799760	6804560	And I think that that kind of works. And that's basically how I've tried to build my team. It's,
6805120	6809440	you know, I'm not, I'm not in a rush to not be running the company. But I think in an
6809440	6813040	alternate universe where one of these other folks was running the company, I'd be happy to work
6813040	6818160	for them. I feel like I'd learn from them. I respect their kind of general judgment.
6819280	6826240	They're all very insightful. They have good values. And I think that that gives you some rubric for
6827200	6831440	you can apply that at every layer. And I think if you apply that at every layer in the organization,
6831440	6838320	then you'll have a pretty strong organization. Okay, in an organization that's not growing as
6838320	6844960	quickly, the questions might be a little different, though. And there you asked about young people
6844960	6852080	specifically, like people out of college. And one of the things that we see is it's a pretty basic
6852160	6857520	lesson. But like, we have a much better sense of who the best people are who have interned at the
6857520	6863360	company for a couple of months, then by looking at them at kind of a resume or a short or a short
6864240	6868320	interview loop. And obviously the in-person feel that you get from someone probably tells you
6868320	6875360	more than the resume. And you can do some basic skills assessment. But a lot of the stuff really
6875360	6883040	just is cultural. People thrive in different environments. And on different teams, even within
6883040	6890400	a specific company, and it's like the people who come for even a short period of time over a summer
6890400	6894960	who do a great job here, you know that they're going to be great if they came and joined full
6894960	6901040	time. And that's one of the reasons why we've invested so much in internship is basically
6901360	6906720	it's a very useful sorting function both for us and for the people who want to try out the company.
6906720	6911440	You mentioned in-person, what do you think about remote work, a topic that's been discussed
6911440	6915280	extensively over the past few years because of the pandemic?
6915840	6924160	Yeah, I mean, I think it's a thing that's here to stay. But I think that there's value in both.
6924880	6931840	It's not, I wouldn't want to run a fully remote company yet at least. I think there's an
6931840	6936160	asterisk on that, which is that. Some of the other stuff you're working on, yeah.
6936160	6943280	Yeah, exactly. It's like all the metaverse work and the ability to feel like you're truly present
6944640	6950320	no matter where you are. I think once you have that all dialed in, then we may one day reach a
6950320	6955840	point where it really just doesn't matter as much where you are physically. But
6958880	6965840	I don't know, today it still does. For people who, there are all these people who have special
6965840	6970800	skills and want to live in a place where we don't have an office, are we better off having
6970800	6976240	them at the company? Absolutely. And are a lot of people who work at the company for several
6976240	6983520	years and then build up the relationships internally and have the trust and have a sense
6983520	6987360	of how the company works? Can they go work remotely now if they want and still do it as
6987360	6990960	effectively? And we've done all these studies that show it's like, okay, does that affect their
6990960	6999600	performance? It does not. But for the new folks who are joining and for people who are earlier
6999600	7004080	in their career and need to learn how to solve certain problems and need to get ramped up on the
7004080	7010480	culture, when you're working through really complicated problems where you don't just want
7010480	7014880	to sit in the, you don't just want the formal meeting, but you want to be able to brainstorm
7014880	7019360	when you're walking in the hallway together after the meeting. I don't know. It's like,
7019360	7028880	we just haven't replaced the kind of in-person dynamics there yet with anything remote yet.
7029520	7033440	Yeah, there's a magic to the in-person that we'll talk about this a little bit more,
7033440	7037760	but I'm really excited by the possibilities in the next two years in virtual reality and mixed
7037760	7046400	reality that are possible with high-resolution scans. I mean, I, as a person who loves in-person
7046400	7052880	interaction, like these podcasts in person, it would be incredible to achieve the level of
7052880	7060800	realism I've gotten the chance to witness. But let me ask about that. I got a chance to look at
7061520	7071920	the Quest 3 headset and it is amazing. You've announced it. It's, you'll give some more details
7071920	7076160	in the fall. Maybe release in the fall. When is it getting released again? I forgot, you mentioned it.
7076160	7078880	We'll give more details at Connect. But it's coming this fall.
7079040	7089520	Okay. So, it's priced at $4.99. What features are you most excited about there?
7089520	7095040	There are basically two big new things that we've added to Quest 3 over Quest 2. The first is
7095040	7104080	high-resolution mixed reality. And the basic idea here is that you can think about virtual reality
7104080	7110560	as you have the headset and like all the pixels are virtual and you're basically like immersed in
7110560	7116160	a different world. Mixed reality is where you see the physical world around you and you can place
7116160	7121360	virtual objects in it, whether that's a screen to watch a movie or a projection of your virtual
7121360	7126240	desktop or you're playing a game where like zombies are coming out through the wall and you need to
7126240	7130640	shoot them. Or, you know, we're, you know, we're playing Dungeons and Dragons or some board game
7130640	7134080	and we just have a virtual version of the board in front of us while we're sitting here.
7135600	7140400	All that's possible in mixed reality. And I think that that is going to be the next big
7140400	7147040	capability on top of virtual reality. It has done so well. I have to say, as a person who
7147040	7154320	experienced it today with zombies, having a full awareness of the environment and integrating
7154320	7160000	that environment in the way they run at you while they try to kill you. It's just the mixed
7160000	7164560	reality that passed through is really, really, really well done. And the fact that it's only
7164560	7173200	$500 is really, it's well done. Thank you. I mean, I'm super excited about it. I mean, we put a lot
7173200	7180800	of work into making the device both as good as possible and as affordable as possible because
7180800	7186160	a big part of our mission and ethos here is we want people to be able to connect with each other.
7186240	7190160	We want to reach and we want to serve a lot of people, right? We want to bring this technology
7190160	7200160	to everyone, right? So we're not just trying to serve like an elite, wealthy crowd. We really
7200160	7205360	want this to be accessible. So that is, in a lot of ways, an extremely hard technical problem
7205360	7210880	because we don't just have the ability to put an unlimited amount of hardware in this. We
7210880	7216000	needed to basically deliver something that works really well but in an affordable package.
7216000	7225120	And we started with Quest Pro last year. It was $1,500 and now we've lowered the price to $1,000.
7225120	7231680	But in a lot of ways, the mixed reality in Quest 3 is an even better and more advanced level than
7231680	7237360	what we were able to deliver in Quest Pro. So I'm really proud of where we are with Quest 3 on
7237360	7242720	that. It's going to work with all of the virtual reality titles and everything that existed there.
7242720	7248560	So people who want to play fully immersive games, social experiences, fitness, all that stuff will
7248560	7254880	work. But now you'll also get mixed reality too, which I think people really like because it's
7255760	7260800	sometimes you want to be super immersed in a game. But a lot of the time, especially when
7260800	7264560	you're moving around, if you're active, like you're doing some fitness experience,
7264880	7269920	you know, let's say you're doing boxing or something, it's like you kind of want to be able
7269920	7273680	to see the room around you. So that way you know that like I'm not going to punch a lamp or something
7273680	7278240	like that. And I don't know if you got to play with this experience, but we basically have the,
7278240	7283440	and it's just sort of like a fun little demo that we put together. But it's like you just,
7284480	7289760	we're like in a conference room or you're living room and you have the guy there and you're boxing
7289760	7293520	him and you're fighting him. And it's like, Oh, the other people are there too. I got a chance to do
7293520	7300320	that. And all the people are there. It's like that guy is right there. Yeah. It's like it's right
7300320	7304240	in the room. And the other human, the path that you're seeing them also, they can cheer you on,
7304240	7309040	they can make fun of you if there are anything like friends of mine. And then just it, yeah,
7310720	7317280	it's really, it's a really compelling experience. I mean, VR is really interesting too, but this
7317280	7322400	is something else almost. This is, this becomes integrated into your life, into your world.
7323520	7328160	Yeah. And it, so I think it's a completely new capability that will unlock a lot of different
7328160	7333040	content. And I think it'll also just make the experience more comfortable for a set of people
7333040	7337840	who didn't want to have only fully immersive experiences. I think if you want experiences
7337840	7341600	where you're grounded in, you know, your living room in the physical world around you,
7341600	7344880	now you'll be able to have that too. And I think that that's pretty exciting.
7344880	7350960	I really liked how it added windows to a room with no windows. Yeah.
7350960	7354480	Me as a person. Did you see the aquarium one where you could see the sharks swim up or was
7354480	7358560	that just a zombie one? Just a zombie one, but it's still outside. You don't necessarily want
7358560	7362480	windows added to your living room where zombies come out of, but yeah, so the context of that
7362480	7368560	game, it's, yeah, yeah. I enjoyed it because you could see the nature outside. And me as a person
7368560	7375760	that doesn't have windows, it's just nice to have nature. Yeah. Well, even if it's a mixed reality
7375760	7383200	setting, I know it's a zombie game, but there's a zen nature, zen aspect to being able to look
7383200	7391120	outside and alter your environment as you know it. Yeah. And there will probably be better,
7391120	7395120	more zen ways to do that than the zombie game you're describing, but you're right that the
7395120	7401360	basic idea of sort of having your physical environment on pass through, but then being able
7401360	7409040	to bring in different elements, I think it's going to be super powerful. And in some ways,
7409040	7414960	I think that these are, mixed reality is also a predecessor to eventually we will get AR glasses
7414960	7420960	that are not kind of the goggles form factor of the current generation of headsets that people
7420960	7426320	are making. But I think a lot of the experiences that developers are making for mixed reality of
7426320	7430720	basically you just have a kind of a hologram that you're putting in the world will hopefully apply
7431200	7435920	once we get the AR glasses too. Now that's got its own whole set of challenges and it's...
7436640	7441600	Well, the headset's already smaller than the previous version. Oh yeah, it's 40% thinner.
7441600	7445120	And the other thing that I think is good about it, yeah, so mixed reality was the first big thing.
7445840	7452800	The second is it's just a great VR headset. I mean, it's got 2x the graphics processing power,
7453760	7460080	40% sharper screens, 40% thinner, more comfortable, better strap architecture,
7460080	7464000	all this stuff that if you liked Quest 2, I think that this is just going to be...
7464000	7467360	It's like all the content that you might have played in Quest 2 is just going to be sharper
7467360	7471600	automatically and look better in this. So it's... I think people are really going to like it.
7471600	7472560	Yeah, so this fall...
7473520	7480880	At this fall, I have to ask, Apple just announced a mixed reality headset called Vision Pro
7481440	7487120	for $3,500 available in early 2024. What do you think about this headset?
7488880	7493760	Well, I saw the materials when they launched. I haven't gotten a chance to play with it yet.
7493760	7499600	So kind of take everything with a grain of salt, but a few high-level thoughts. I mean, first,
7500480	7509200	I do think that this is a certain level of validation for the category,
7509760	7515520	where we were the primary folks out there before saying, hey, I think that this
7516800	7520320	virtual reality, augmented reality, mixed reality, this is going to be a big part of
7520320	7527920	the next computing platform. I think having Apple come in and share that vision
7530320	7536080	will make a lot of people who are fans of their products really consider that.
7537600	7545680	And then, of course, the $3,500 price... On the one hand, I get it with all the stuff
7545680	7549520	that they're trying to pack in there. On the other hand, a lot of people aren't going to find that
7549520	7555360	to be affordable. So I think that there's a chance that them coming in actually increases demand
7556320	7561280	for the overall space. And that Quest 3 is actually the primary beneficiary of that,
7561280	7568000	because a lot of the people who might say, hey, I'm going to give another consideration to this.
7569200	7574320	Now I understand maybe what mixed reality is more. And Quest 3 is the best one on the market
7574320	7580800	that I can afford. And it's great also. I think that that's... And in our own way,
7580800	7583840	I think there are a lot of features that we have where we're leading on.
7585520	7589920	So I think that that I think is going to be a very... That could be quite good.
7591760	7594400	And then obviously, over time, the companies are just focused on
7595520	7601040	somewhat different things. Apple has always, I think, focused on building
7602480	7611520	really high-end things, whereas our focus has been on... We have a more democratic ethos. We
7611520	7618640	want to build things that are accessible to a wider number of people. We've sold tens of millions
7618640	7626080	of Quest devices. My understanding, just based on rumors, I don't have any special knowledge on
7626080	7632640	this, is that Apple is building about one million of their device. So just in terms of what you
7632640	7641120	kind of expect in terms of sales numbers, I just think that this is... Quest is going to be the
7641200	7645920	primary thing that people in the market will continue using for the foreseeable future. And
7645920	7649920	then obviously, over the long term, it's up to the companies to see how well we've executed the
7649920	7653440	different things that we're doing. But we kind of come at it from different places. We're very
7653440	7661520	focused on social interaction, communication, being more active, right? So there's fitness,
7661520	7667440	there's gaming, there are those things. Whereas I think a lot of the use cases that you saw in
7667440	7675200	Apple's launch material were more around people sitting, people looking at screens,
7676240	7681840	which are great. I think that you will replace your laptop over time with a headset. But I think
7681840	7685920	in terms of how the different use cases that the companies are going after,
7687680	7690240	they're a bit different for where we are right now.
7690240	7694880	Yeah, so gaming wasn't a big part of the presentation, which is an interesting...
7695120	7703280	It feels like mixed reality gaming is such a big part of that. It was interesting to see it
7703280	7707040	missing in the presentation. Well, I mean, look, there are certain design trade-offs
7707040	7714160	in this where they made this point about not wanting to have controllers, which on the one hand,
7715280	7718560	there's a certain elegance about just being able to navigate the system with
7719360	7724560	eye gaze and hand tracking. And by the way, you'll be able to just navigate Quest with your hands
7724640	7731600	too, if that's what you want. One of the things I should mention is the capability from the cameras
7733120	7737360	with computer vision to detect certain aspects of the hand, allowing you to have a controller that
7737360	7742880	doesn't have that ring thing. Yeah, the hand tracking in Quest 3 and the controller tracking is
7742880	7749040	a big step up from the last generation. And one of the demos that we have is basically
7749600	7753680	an MR experience teaching you how to play piano, where it basically highlights the notes that you
7753680	7758960	need to play and it's like just all its hands, it's no controllers. But I think if you care about
7758960	7767120	gaming, having a controller allows you to have a more tactile feel and allows you to capture
7768880	7775200	fine motor movement much more precisely than what you can do with hands without something
7775200	7779840	that you're touching. So again, I think there are certain questions which are just around
7779840	7786960	what use cases are you optimizing for. I think if you want to play games, then I think that that,
7786960	7792720	then I think you want to design the system in a different way and we're more focused on kind of
7792720	7799840	social experiences, entertainment experiences. Whereas if what you want is to make sure that
7800720	7805520	the text that you read on a screen is as crisp as possible, then you need to make the
7806240	7812240	design and cost trade-offs that they made that lead you to making a $3,500 device.
7812240	7815520	So I think that there is a use case for that for sure, but I just think that they're,
7816800	7822720	the company is, we've basically made different design trade-offs to get to the use cases that
7822720	7828320	we're trying to serve. There's a lot of other stuff I'd love to talk to you about about the
7828320	7833920	metaverse, especially the Kodak avatar, which I've gotten to experience a lot of different
7833920	7836800	variations of recently that I'm really, really excited about.
7836800	7838640	Yeah, I'm excited to talk about that too.
7838640	7841040	Yeah, I'll have to wait a little bit because,
7843840	7849520	well, I think there's a lot more to show off in that regard. But let me step back to AI.
7850080	7855760	I think we've mentioned it a little bit, but I'd like to linger on this question that
7856080	7863120	folks like Eliezer Yatkowski has a worry about and others of the existential,
7863120	7868160	of the serious threats of AI that have been reinvigorated now with the rapid developments
7868160	7877360	of AI systems. Do you worry about the existential risks of AI as Eliezer does about the alignment
7877360	7879520	problem about this getting out of hand?
7880480	7885760	Any time where there's a number of serious people who are raising a concern that is that
7885760	7890240	existential about something that you're involved with, I think you have to think about it.
7892080	7894800	I've spent quite a bit of time thinking about it from that perspective.
7900800	7904240	Where I basically have come out on this for now is I do think that there are,
7904480	7910320	over time, I think that we need to think about this even more as we approach something that
7911360	7915120	could be closer to superintelligence. I just think it's pretty clear to anyone working on these
7915120	7923440	projects today that we're not there. One of my concerns is that we spent a fair amount of time
7923440	7933360	on this before, but there are more, I don't know if mundane is the right word, but there's concerns
7933360	7940400	that already exist right about people using AI tools to do harmful things of the type that we're
7940400	7944960	already aware, whether we talked about fraud or scams or different things like that.
7947600	7952880	That's going to be a pretty big set of challenges that the companies working on this are going to
7952880	7959040	need to grapple with regardless of whether there is an existential concern as well at some point
7959040	7970240	down the road. I do worry that to some degree people can get a little too focused on some of
7970240	7977200	the tail risk and then not do as good of a job as we need to on the things that can be almost
7977200	7984800	certain are going to come down the pipe as real risks that manifest themselves in the near term.
7984800	7993680	For me, I've spent most of my time on that once I kind of made the realization that the size of
7993680	7998320	models that we're talking about now in terms of what we're building are quite far from the
7998320	8004320	superintelligence type concerns that people raise. But I think once we get a couple steps closer to
8004320	8012480	that, I know as we do get closer, I think that there are going to be some novel risks and issues
8012480	8017760	about how we make sure that the systems are safe for sure. I guess here, just to take the
8017760	8024480	conversation in a somewhat different direction, I think in some of these debates around safety,
8025600	8034720	I think the concepts of intelligence and autonomy or like the being of the thing
8036480	8042080	as an analogy, they get kind of conflated together. And I think it very well could
8042160	8053120	be the case that you can make something in scale intelligence quite far, but that may not manifest
8053760	8057920	the safety concerns that people are saying in the sense that I mean, just if you look at human
8057920	8062080	biology, it's like, all right, we have our neocortexes where all the thinking happens.
8063520	8068240	But it's not really calling the shots at the end of the day. We have a much more primitive
8069200	8075360	old brain structure for which our neocortex, which is this powerful machinery, is basically just a
8076080	8087760	prediction and reasoning engine to help our very simple brain decide how to plan and do what it
8087760	8095040	needs to do in order to achieve these very basic impulses. And I think that you can think about
8095120	8102320	some of the development of intelligence along the same lines where just like our neocortex doesn't
8102320	8108560	have free will or autonomy, we might develop these wildly intelligent systems that are
8109120	8114880	much more intelligent than our neocortex have much more capacity, but are in the same way that
8114880	8121600	our neocortex is sort of subservient and is used as a tool by our kind of simple impulse brain.
8122080	8127280	It's, you know, I think that it's not out of the question that very intelligent systems that
8127280	8132880	have the capacity to think will kind of act as that is sort of an extension of the neocortex
8132880	8140160	doing that. So I think my own view is that where we really need to be careful is on the development
8140160	8147440	of autonomy and how we think about that. Because it's actually the case that
8148400	8153680	relatively simple and unintelligent things that have runaway autonomy and just spread themselves
8153680	8157600	or, you know, it's like, we have a word for that, it's a virus, right? It's, I mean, like,
8157600	8161840	it's can be simple computer code that is not particularly intelligent, but just spreads itself
8161840	8171120	and does a lot of harm, you know, biologically or computer. And I just think that these are
8171120	8176560	somewhat separable things. And a lot of what I think we need to develop when people talk about
8176560	8183200	safety and responsibility is really the governance on the autonomy that can be given to systems.
8183760	8188800	And to me, if, you know, if I were, you know, a policymaker or thinking about this,
8189520	8193200	I would really want to think about that distinction between these where I think building
8193200	8198800	intelligent systems will be, can create a huge advance in terms of people's quality of life and
8200000	8206240	productivity growth in the economy. But it's the autonomy part of this that I think we really
8206320	8210720	need to make progress on how to govern these things responsibly before we
8212640	8218480	build the capacity for them to make a lot of decisions on their own or give them goals or
8219440	8222720	things like that. And I know that that's a research problem, but I do think that to some
8222720	8228800	degree these are somewhat, are somewhat separable things. I love the distinction between intelligence
8228800	8238080	and autonomy and the metaphor within your cortex. Let me ask about power. So building
8238080	8245040	superintelligence systems, even if it's not in the near term, I think meta as is one of the few
8245040	8252000	companies, if not the main company that will develop the superintelligence system. And you
8252000	8257040	are a man who's at the head of this company, building AGI might make you the most powerful
8257040	8259920	man in the world. Do you worry that that power will corrupt you?
8262560	8269600	What a question. I mean, look, I think realistically, this gets back to the open
8269600	8275680	source things that we talked about before, which is, I don't think that the world will be best
8275680	8286960	served by any small number of organizations having this without it being something that is more
8286960	8294400	broadly available. And I think if you look through history, it's when there are these sort of like
8294400	8301280	unipolar advances and things that, and like power imbalances that they're doing to being kind of
8302240	8307920	weird situations. So this is one of the reasons why I think open sources is generally
8309600	8314880	the right approach. And I think it's a categorically different question today when we're
8314880	8318320	not close to superintelligence. I think that there's a good chance that even once we get
8318320	8322640	closer to superintelligence, open sourcing remains the right approach, even though I think at that
8322640	8329280	point it's a somewhat different debate. But I think part of that is that that is, I think,
8329280	8334400	one of the best ways to ensure that the system is as secure and safe as possible, because it's
8334400	8339440	not just about a lot of people having access to it. It's the scrutiny that kind of comes with
8340640	8344640	building an open source system, but I think that this is a pretty widely accepted thing about
8344640	8352000	open sources that you have the code out there, so anyone can see the vulnerabilities. Anyone can
8352000	8356240	kind of mess with it in different ways. People can spin off their own projects and experiment
8356240	8361440	in a ton of different ways. And the net result of all of that is that the systems just get hardened
8362000	8371840	and get to be a lot safer and more secure. So I think that there's a chance that that ends up being
8371840	8380480	the way that this goes to, a pretty good chance, and that having this be open both leads to a
8380480	8387600	healthier development of the technology and also leads to a more balanced distribution of the
8387600	8394560	technology in a way that that strike me as good values to aspire to. So to you, the risks, there's
8394560	8400720	risks to open sourcing, but the benefits outweigh the risks. At the two, it's interesting. I think
8400720	8408160	the way you put it, you put it well that there's a different discussion now than when we get closer
8409120	8415360	to the development of superintelligence of the benefits and risks of open sourcing.
8415360	8420560	Yeah, and to be clear, I feel quite confident in the assessment that open sourcing models now
8421200	8426880	is net positive. I think there's a good argument that in the future it will be too,
8426880	8432160	even as you get closer to superintelligence, but I've certainly not decided on that yet,
8432160	8436240	and I think that it becomes a somewhat more complex set of questions that I think people
8436240	8440640	will have time to debate and will also be informed by what happens between now and then
8440640	8444480	to make those decisions. We don't have to necessarily just debate that in theory right now.
8445040	8447520	What year do you think we'll have a superintelligence?
8449920	8454480	I don't know. I mean, that's pure speculation. I think it's very clear,
8454480	8457680	just taking a step back, that we had a big breakthrough in the last year where the
8458320	8464080	LLMs and diffusion models basically reached a scale where they're able to do some pretty
8464080	8468720	interesting things. Then I think the question is what happens from here. Just to paint the two
8468720	8478400	extremes, on one side, it's like, okay, we just had one breakthrough. If we just have another
8478400	8483920	breakthrough like that, or maybe two, then we can have something that's truly crazy, right, and is
8486560	8492480	just so much more advanced. On that side of the argument, it's like, okay, well, maybe we're
8494720	8502560	only a couple of big steps away from reaching something that looks more like general intelligence.
8503200	8507520	Okay, that's one side of the argument. On the other side, which is what we've historically
8507520	8518160	seen a lot more, is that a breakthrough leads to, in that Gartner hype cycle, there's the hype,
8518160	8522800	and then there's the trough of disillusionment after when people think that there's a chance that,
8522800	8526160	hey, okay, there's a big breakthrough. Maybe we're about to get another big breakthrough, and it's
8526160	8530240	like, actually, you're not about to get another breakthrough. Maybe you're actually just going
8530240	8538640	to have to sit with this one for a while, and it could be five years. It could be 10 years.
8538640	8545120	It could be 15 years until you figure out the next big thing that needs to get figured out.
8547040	8549360	But I think that the fact that we just had this breakthrough
8549600	8556240	sort of makes it that we're at a point of almost a very wide error bars on what happens next.
8557760	8564480	I think the traditional technical view of looking at the industry would suggest that we're not just
8564480	8569520	going to stack in a breakthrough on top of breakthrough on top of breakthrough every
8570400	8576560	six months or something right now. I would guess that it will take somewhat longer in
8576560	8583360	between these, but I don't know. I tend to be pretty optimistic about breakthroughs, too.
8583360	8589600	So I think if you're normalized for my normal optimism, then maybe it would be even slower
8589600	8594400	than what I'm saying. But even within that, I'm not even opining on the question of how many
8594400	8597360	breakthroughs are required to get to general intelligence because no one knows.
8598880	8605840	But this particular breakthrough was such a small step that resulted in such a big leap
8605840	8611680	in performance as experienced by human beings that it makes you think, wow,
8613120	8618480	as we stumble across this very open world of research, will we stumble
8620480	8623600	across another thing that will have a giant leap in performance?
8625840	8632000	And also, we don't know exactly at which stage is it really going to be impressive
8632000	8636800	because it feels like it's really encroaching on impressive levels of intelligence.
8637840	8641920	You still didn't answer the question of what year we're going to have superintelligence.
8641920	8645280	I'd like to hold you to that. No, I'm just kidding. But is there something
8646080	8653440	you could say about the timeline as you think about the development of AGI, superintelligence
8653440	8661680	systems? Sure. So I still don't think I have any particular insight on when a singular AI system
8661680	8665920	that is a general intelligence will get created. But I think the one thing that most people
8667360	8670720	in the discourse that I've seen about this haven't really grappled with is that
8670720	8678960	we do seem to have organizations and structures in the world that exhibit greater than human
8678960	8687440	intelligence already. So one example is a company. It acts as an entity. It has a singular brand.
8688400	8694320	Obviously, it's a collection of people. But I certainly hope that meta with tens of thousands
8694320	8698960	of people makes smarter decisions than one person. But I think that that would be pretty bad if it
8698960	8707680	didn't. Another example that I think is even more removed from the way we think about the
8707680	8712640	personification of intelligence, which is often implied in some of these questions,
8713280	8719120	is think about something like the stock market. The stock market takes inputs. It's a distributed
8719120	8725440	system. It's like the cybernetic organism that probably millions of people around the world
8726240	8731760	are basically voting every day by choosing what to invest in. But it's basically this
8733680	8742160	organism or structure that is smarter than any individual that we use to allocate capital
8742240	8747040	as efficiently as possible around the world. And I do think that
8749760	8756960	this notion that there are already these cybernetic systems that are either melding
8758400	8762800	the intelligence of multiple people together or melding the intelligence of multiple people
8762800	8770640	and technology together to form something which is dramatically more intelligent than any individual
8771200	8779200	in the world is something that seems to exist and that we seem to be able to harness
8780000	8785520	in a productive way for our society as long as we basically build these structures and balance
8785520	8792720	with each other. So I don't know. I mean, that at least gives me hope that as we advance the
8792720	8796560	technology, and I don't know how long exactly it's going to be, but you asked when is this going to
8796560	8801520	exist. I think to some degree, we already have many organizations in the world that are smarter
8801520	8806800	than a single human. And that seems to be something that is generally productive in advancing humanity.
8806800	8812160	And somehow the individual AI systems empower the individual humans and the interaction between
8812160	8817680	those humans to make that collective intelligence machinery that you're referring to smarter.
8817680	8821520	So it's not like AI is becoming super intelligent. It's just becoming the
8822480	8826960	the engine that's making the collective intelligence is primarily human, more intelligent.
8828080	8833120	Yeah, it's educating the humans better. It's making them better informed. It's
8834320	8839760	making it more efficient for them to communicate effectively and debate ideas. And through that
8839760	8843600	process, just making the whole collective intelligence more and more and more intelligent,
8844320	8848560	maybe faster than the individual AI systems that are trained on human data anyway,
8849360	8854000	are becoming. Maybe the collective intelligence of the human species might outpace the development
8854000	8861040	of AI. I think there's a balance in here, because if a lot of the input that
8862480	8866720	the systems are being trained on is basically coming from feedback from people,
8867600	8873200	then a lot of the development does need to happen in human time. It's not like a machine
8874480	8878160	will just be able to go learn all the stuff about how people think about stuff.
8878400	8884560	There's a cycle to how this needs to work. This is an exciting world we're living in,
8884560	8889440	and that you're at the forefront of developing. One of the ways you keep yourself humble,
8889440	8895280	like we mentioned with Jiu Jitsu, is doing some really difficult challenges, mental and physical.
8896080	8902880	One of those you've done very recently is the Murph challenge, and you got a really good time.
8902880	8907840	It's 100 pull-ups, 200 push-ups, 200 squats, and a mile before and a mile around after.
8908800	8916080	You got under 40 minutes on that. What was the hardest part? I think a lot of people
8916080	8921920	were very impressed. It's a very impressive time. How crazy are you on this?
8923920	8927200	It wasn't my best time, but anything under 40 minutes I'm happy with.
8928080	8929040	It wasn't your best time?
8929600	8937200	No, I think I've done it a little faster before, but not much. Of my friends, I did not win
8937280	8940960	on Memorial Day. One of my friends did it actually several minutes faster than me,
8941840	8946560	but just to clear up one thing that I think was, I saw a bunch of questions about this on the
8946560	8952080	internet. There are multiple ways to do the Murph challenge. There's a kind of partitioned mode
8952080	8958560	where you do sets of pull-ups, push-ups, and squats together, and then there's unpartitioned
8958560	8965840	where you do the 100 pull-ups, and then the 200 push-ups, and then the 300 squats in cereal.
8965840	8972480	Obviously, if you're doing them unpartitioned, then it takes longer to get through the 100
8972480	8976320	pull-ups because anytime you're resting in between the pull-ups, you're not also doing
8976320	8983840	push-ups and squats. I'm sure my unpartition time would be quite a bit slower, but no,
8983840	8990480	I think at the end of this, first of all, I think it's a good way to honor Memorial Day.
8991200	9000800	It's this Lieutenant Murphy, basically. This was one of his favorite exercises,
9000800	9005360	and I just try to do it on Memorial Day each year, and it's a good workout.
9006720	9012960	I got my older daughters to do it with me this time. My oldest daughter wants a weight vest
9012960	9016800	because she sees me doing it with a weight vest. I don't know if a seven-year-old should be using
9016800	9023360	a weight vest to do pull-ups. The difficult question a parent must ask themselves, yes.
9023360	9026960	I was like, maybe I can make you a very light weight vest, but I didn't think it was good for
9026960	9033280	this. She basically did a quarter-merf, so she ran a quarter-mile, and then did 25 pull-ups,
9033280	9040880	50 push-ups, and 75 air squats, then ran another quarter-mile, and 15 minutes,
9041120	9048240	which I was pretty impressed by in my five-year-old, too. I was excited about that,
9048240	9056640	and I'm glad that I'm teaching them the value of physicality. I think a good day for Max,
9056640	9060800	my daughter, is when she gets to go to the gym with me and cranks out a bunch of pull-ups,
9060800	9067680	and I love that about her. I think it's good. She's, hopefully, I'm teaching her some good
9067680	9073680	lessons. The broader question here is, given how busy you are, given how much stuff you have
9073680	9084160	going on in your life, what's the perfect exercise regimen for you to keep yourself happy,
9085280	9088320	to keep yourself productive in your main line of work?
9089280	9098560	Yes. I mean, right now, I'm focused most of my workouts on fighting, so Jiu-Jitsu and MMA,
9100480	9104960	but I don't know. I mean, maybe if you're a professional, you can do that every day. I can't.
9104960	9111120	I just get too many bruises and things that you need to recover from, so I do that three
9111120	9117760	to four times a week, and then the other day, I just try to do a mix of things,
9117920	9120880	like just cardio conditioning, strength building, mobility.
9121920	9124000	So you try to do something physical every day?
9124000	9129520	Yeah, I try to, unless I'm just so tired that I just need to relax, but then I'll still try to go
9129520	9134800	for a walk or something. I mean, even here, I don't know. Have you been on the roof here yet?
9134800	9136400	No. I heard things.
9136400	9141040	But it's like, we designed this building and I put a park on the roof, so that way,
9141040	9145040	that's like my meetings when I'm just doing kind of a one-on-one or talking to a couple of people.
9145760	9150640	I have a very hard time just sitting. I feel like you get super stiff. It feels really bad,
9152960	9156880	but I don't know. Being physical is very important to me. I think it's,
9157600	9164000	I do not believe this gets to the question about AI. I don't think that a being is just a mind.
9164960	9172160	And I think we're kind of meant to do things, and like physically, and a lot of the sensations that
9172240	9178160	we feel are connected to that. And I think that that's a lot of what makes you a human,
9178160	9189040	is basically having that set of sensations and experiences around that, coupled with a mind
9189040	9198880	to reason about them. But I don't know. I think it's important for balance to kind of get out,
9198880	9202320	challenge yourself in different ways, learn different skills, clear your mind.
9203360	9208640	Do you think AI, in order to become super intelligent, needs you to have a body?
9211520	9219280	It depends on what the goal is. I think that there's this assumption in that question that
9220240	9226640	intelligence should be kind of person-like. Whereas, as we were just talking about,
9227600	9234640	you can have these greater than single human intelligent organisms, like the stock market,
9234640	9241440	which obviously do not have bodies and do not speak a language, and just kind of have their own
9242960	9251520	system. So I don't know. My guess is there will be limits to what a system that is purely an
9251520	9257440	intelligence can understand about the human condition without having the same, not just
9257440	9267200	senses, but our body's changes we get older. And we kind of evolve and let those very subtle
9269120	9276240	physical changes just drive a lot of social patterns and behavior around when you choose to
9276240	9281360	have kids. Just like all these, that's not even subtle. That's a major one. But how
9281360	9288560	you design things around the house. So yeah, I think if the goal is to understand people
9288560	9294960	as much as possible, I think that that's trying to model those sensations is probably somewhat
9294960	9298880	important. But I think that there's a lot of value that can be created by having intelligence,
9298880	9301600	even that is separate from that. It's a separate thing.
9302400	9309360	So one of the features of being human is that we're mortal. We die. We've talked about AI a
9309360	9316000	lot, but potentially replicas of ourselves. Do you think there will be AI replicas of you and
9316000	9321440	me that persist long after we're gone, that family and loved ones can talk to?
9324480	9328960	I think we'll have the capacity to do something like that. And I think one of the big questions
9330240	9336640	that we've had to struggle with in the context of social networks is who gets to make that.
9337600	9343280	And my answer to that in the context of the work that we're doing is that that should be your
9343280	9348400	choice. I don't think anyone should be able to choose to make a Lex spot that people can
9350720	9356160	choose to talk to and get to train that. And we have this precedent of making some of these
9356160	9365200	calls where someone can create a page for a Lex fan club, but you can't create a page and say that
9365200	9375120	you're Lex. So I think that similarly, I think maybe someone maybe should be able to make an
9375120	9380160	AI that's a Lex admirer that someone can talk to, but I think it should ultimately be your call
9381120	9386400	whether there is a Lex AI. Well, I'm open sourcing the Lex.
9386800	9395200	So you're a man of faith. What role has faith played in your life and your understanding of the
9395200	9403360	world and your understanding of your own life and your understanding of your work and how your work
9403360	9409440	impacts the world? Yeah, I think that there's a few different parts of this that are relevant.
9409440	9416720	There's sort of a philosophical part and there's a cultural part. And one of the most basic
9417360	9424000	lessons is right at the beginning of Genesis where it's like God creates the earth and creates
9424000	9429840	people and creates people in God's image. And there's the question of what does that mean?
9429840	9433440	And all the only context that you have about God at that point in the Old Testament is that
9434560	9439200	God has created things. So I always thought that one of the interesting lessons from that
9439200	9447200	is that there's a virtue in creating things that is like whether it's artistic or
9448000	9451760	whether you're building things that are functionally useful for other people.
9454240	9464400	I think that that by itself is a good. And that kind of drives a lot of how I think about
9465360	9472720	morality and my personal philosophy around what is a good life. I think it's one where you're
9474720	9482480	helping the people around you and you're being a kind of positive creative force in the world
9482480	9488880	that is helping to bring new things into the world, whether they're amazing other people, kids,
9489440	9497040	or just leading to the creation of different things that wouldn't have been possible otherwise.
9497040	9504080	And so that's a value for me that matters deeply. And I just love spending time with the kids and
9504080	9511120	seeing that they sort of trying to impart this value to them. And it's like nothing makes me
9511120	9518480	happier than when I come home from work and I see my daughters building Legos on the table
9518480	9522560	or something. It's like, all right, I did that when I was a kid. So many other people were doing
9522560	9527440	this. And I hope you don't lose that spirit where when you kind of grow up and you want to
9527440	9533920	just continue building different things no matter what it is. To me, that's a lot of what matters.
9535120	9539360	That's the philosophical piece. I think the cultural piece is just about community and values.
9539360	9543600	And that part of things I think has just become a lot more important to me since I've had kids.
9543920	9549680	You know, it's almost autopilot when you're a kid. You're in the kind of getting imparted
9549680	9555440	two phase of your life. And I didn't really think about religion that much for a while.
9556160	9563600	You know, I was in college before I had kids. And then I think having kids has this way of
9564640	9569920	really making you think about what traditions you want to impart and how you want to celebrate.
9569920	9576880	And like what balance you want in your life. And I mean, a bunch of the questions that you've
9576880	9583280	asked and a bunch of the things that we're talking about. Just the irony of the curtains coming down
9584480	9589920	as we're talking about mortality. Once again, same as last time. This is just
9591440	9597520	the universe works in. We are definitely living in a simulation book. Go ahead. Community tradition
9597600	9602400	and the values, the faith and religion is still. A lot of the topics that we've talked about today
9603040	9611920	are around how do you balance, you know, whether it's running a company or different
9611920	9620800	responsibilities with this. How do you kind of balance that? And I always also just think
9620800	9626960	that it's very grounding to just believe that there is something that is much bigger than you
9626960	9634160	that is guiding things. That amongst other things gives you a bit of humility.
9638400	9643760	As you pursue that spirit of creating that you spoke to, creating beauty in the world.
9644320	9650080	As Dostoevsky said, beauty will save the world. Mark, I'm a huge fan of yours.
9650320	9654720	I'm honored to be able to call you a friend and I'm looking forward to
9656880	9661040	both kicking your ass and you kicking my ass on the mat tomorrow in Jiu-Jitsu.
9662000	9667520	This incredible sport and art that we both will both participate in. Thank you so much
9667520	9672400	for talking to me. Thank you for everything you're doing in so many exciting realms of technology
9672400	9676400	and human life. I can't wait to talk to you again in the metaverse. Thank you.
9676720	9682000	Thanks for listening to this conversation with Mark Zuckerberg. To support this podcast,
9682000	9687120	please check out our sponsors in the description. And now let me leave you with some words from
9687120	9694720	Isaac Asimov. It is change, continuing change, inevitable change that is the dominant factor
9694720	9701360	in society today. No sensible decision can be made any longer without taking into account
9701360	9708880	not only the world as it is, but the world as it will be. Thank you for listening and hope to see you
9708880	9717280	next time.
