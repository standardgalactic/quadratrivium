1
00:00:00,000 --> 00:00:05,760
The following is a conversation with Marcus Hutter, senior research scientist at Google DeepMind.

2
00:00:06,640 --> 00:00:11,680
Throughout his career of research, including with JÃ¶rgen Schmidhuber and Shane Legg,

3
00:00:11,680 --> 00:00:16,320
he has proposed a lot of interesting ideas in and around the field of artificial general

4
00:00:16,320 --> 00:00:23,760
intelligence, including the development of IEXI, spelled AIXI model, which is the mathematical

5
00:00:23,760 --> 00:00:30,560
approach to AGI that incorporates ideas of Komogorov complexity, Solominov induction,

6
00:00:30,560 --> 00:00:38,320
and reinforcement learning. In 2006, Marcus launched the 50,000-euro Hutter prize for

7
00:00:38,320 --> 00:00:43,840
lossless compression of human knowledge. The idea behind this prize is that the ability to

8
00:00:43,840 --> 00:00:52,080
compress well is closely related to intelligence. This, to me, is a profound idea. Specifically,

9
00:00:52,160 --> 00:00:57,040
if you can compress the first 100 megabytes or one gigabyte of Wikipedia better than your

10
00:00:57,040 --> 00:01:03,600
predecessors, your compressor likely has to also be smarter. The intention of this prize

11
00:01:03,600 --> 00:01:08,160
is to encourage the development of intelligent compressors as a path to AGI.

12
00:01:09,520 --> 00:01:15,440
In conjunction with his podcast release just a few days ago, Marcus announced a 10x increase

13
00:01:15,440 --> 00:01:23,120
in several aspects of this prize, including the money, to 500,000 euros. The better your

14
00:01:23,120 --> 00:01:28,160
compressor works relative to the previous winners, the higher fraction of that prize money is awarded

15
00:01:28,160 --> 00:01:36,400
to you. You can learn more about it if you google simply Hutter prize. I'm a big fan of benchmarks

16
00:01:36,400 --> 00:01:41,920
for developing AI systems, and the Hutter prize may indeed be one that will spark some good ideas

17
00:01:41,920 --> 00:01:48,560
for approaches that will make progress on the path of developing AGI systems. This is the

18
00:01:48,560 --> 00:01:53,760
Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it 5 stars on Apple

19
00:01:53,760 --> 00:01:59,200
Podcasts, support it on Patreon, or simply connect with me on Twitter at Lex Freedman,

20
00:01:59,200 --> 00:02:06,400
spelled F-R-I-D-M-A-N. As usual, I'll do one or two minutes of ads now and never any ads in the

21
00:02:06,400 --> 00:02:11,280
middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the

22
00:02:11,360 --> 00:02:17,680
listening experience. This show is presented by Cash App, the number one finance app in the App Store.

23
00:02:17,680 --> 00:02:24,480
When you get it, use code LEX Podcast. Cash App lets you send money to friends, buy Bitcoin,

24
00:02:24,480 --> 00:02:30,000
and invest in the stock market with as little as $1. Broker services are provided by Cash App

25
00:02:30,000 --> 00:02:37,040
Investing, a subsidiary of Square, and member SIPC. Since Cash App allows you to send and receive

26
00:02:37,040 --> 00:02:42,560
money digitally, peer-to-peer, and security in all digital transactions very important,

27
00:02:42,560 --> 00:02:48,960
let me mention the PCI Data Security Standard that Cash App is compliant with. I'm a big fan of

28
00:02:48,960 --> 00:02:56,080
standards for safety and security. PCI DSS is a good example of that, where a bunch of competitors

29
00:02:56,080 --> 00:03:01,600
got together and agreed that there needs to be a global standard around the security of transactions.

30
00:03:02,480 --> 00:03:09,600
Now, we just need to do the same for autonomous vehicles and AI systems in general. So again,

31
00:03:09,600 --> 00:03:14,880
if you get Cash App from the App Store or Google Play and use the code LEX Podcast,

32
00:03:14,880 --> 00:03:21,120
you'll get $10 and Cash App will also donate $10 to FIRST, one of my favorite organizations

33
00:03:21,120 --> 00:03:26,480
that is helping to advance robotics and STEM education for young people around the world.

34
00:03:27,440 --> 00:03:31,360
And now, here's my conversation with Marcus Hodder.

35
00:03:32,400 --> 00:03:36,880
Do you think of the universe as a computer or maybe an information processing system?

36
00:03:36,880 --> 00:03:42,320
Let's go with a big question first. Okay, I have a big question first. I think it's a very

37
00:03:42,320 --> 00:03:49,360
interesting hypothesis or idea, and I have a background in physics, so I know a little bit

38
00:03:49,360 --> 00:03:54,320
about physical theories, the standard model of particle physics and general relativity theory,

39
00:03:54,400 --> 00:03:58,480
and they are amazing and describe virtually everything in the universe, and they're all in

40
00:03:58,480 --> 00:04:03,520
a sense, computable theories. I mean, they're very hard to compute, and it's very elegant,

41
00:04:03,520 --> 00:04:08,400
simple theories which describe virtually everything in the universe. So there's a strong

42
00:04:09,040 --> 00:04:16,960
indication that somehow the universe is computable, but it's a plausible hypothesis.

43
00:04:17,680 --> 00:04:22,160
So what do you think, just like you said, general relativity, quantum field theory,

44
00:04:22,160 --> 00:04:28,160
what do you think that the laws of physics are so nice and beautiful and simple and compressible?

45
00:04:29,040 --> 00:04:36,000
Do you think our universe was designed is naturally this way? Are we just focusing on the

46
00:04:36,000 --> 00:04:42,720
parts that are especially compressible? Are human minds just enjoying something about that simplicity

47
00:04:42,720 --> 00:04:46,640
and, in fact, there's other things that are not so compressible?

48
00:04:46,640 --> 00:04:52,000
No, I strongly believe and I'm pretty convinced that the universe is inherently beautiful

49
00:04:52,000 --> 00:04:56,880
elegant and simple and described by these equations, and we're not just picking that.

50
00:04:57,440 --> 00:05:04,160
I mean, if there were some phenomena which cannot be neatly described, scientists would try that,

51
00:05:04,160 --> 00:05:07,680
right? And, you know, there's biology which is more messy, but we understand that it's an

52
00:05:07,680 --> 00:05:12,320
emergent phenomena, and, you know, it's complex systems, but they still follow the same rules,

53
00:05:12,320 --> 00:05:16,640
right? Of quantum and electrodynamics, all of chemistry follows that, and we know that. I mean,

54
00:05:16,640 --> 00:05:20,720
we cannot compute everything because we have limited computational resources. No, I think it's

55
00:05:20,720 --> 00:05:25,440
not a bias of the humans, but it's objectively simple. I mean, of course, you never know, you

56
00:05:25,440 --> 00:05:30,480
know, maybe there's some corners very far out in the universe or super, super tiny below the

57
00:05:30,480 --> 00:05:39,520
nucleus of atoms or, well, parallel universes where which are not nice and simple, but there's no

58
00:05:39,520 --> 00:05:44,240
evidence for that, and we should apply Occam's razor and, you know, choose the simple streak

59
00:05:44,240 --> 00:05:47,920
consistent with it, but also it's a little bit self-referential.

60
00:05:47,920 --> 00:05:54,480
So maybe a quick pause. What is Occam's razor? So Occam's razor says that you should not multiply

61
00:05:54,480 --> 00:06:01,760
entities beyond necessity, which sort of if you translate it to proper English means, and, you

62
00:06:01,760 --> 00:06:06,880
know, in the scientific context means that if you have two theories or hypotheses or models which

63
00:06:06,880 --> 00:06:12,400
equally well describe the phenomenon you're studying or the data, you should choose the more

64
00:06:12,400 --> 00:06:20,400
simple one. So that's just a principle or sort of that's not like a provable law, perhaps, perhaps

65
00:06:20,400 --> 00:06:27,040
we'll kind of discuss it and think about it, but what's the intuition of why the simpler answer

66
00:06:28,000 --> 00:06:34,960
is the one that is likely to be more correct descriptor of whatever we're talking about?

67
00:06:34,960 --> 00:06:40,400
I believe that Occam's razor is probably the most important principle in science. I mean,

68
00:06:40,400 --> 00:06:47,280
of course, we need logical deduction and we do experimental design, but science is about finding

69
00:06:47,840 --> 00:06:53,600
understanding the world, finding models of the world, and we can come up with crazy complex models

70
00:06:53,600 --> 00:06:59,040
which, you know, explain everything but predict nothing, but the simple model seemed to have

71
00:06:59,040 --> 00:07:06,960
predictive power, and it's a valid question why. And the two answers to that, you can just accept

72
00:07:07,120 --> 00:07:12,000
that as the principle of science, and we use this principle and it seems to be successful.

73
00:07:12,720 --> 00:07:18,480
We don't know why, but it just happens to be. Or you can try, you know, find another principle

74
00:07:18,480 --> 00:07:25,440
which explains Occam's razor. And if we start with assumption that the world is governed by

75
00:07:25,440 --> 00:07:33,600
simple rules, then there's a bias to our simplicity, and applying Occam's razor

76
00:07:34,160 --> 00:07:38,880
is the mechanism to finding these rules. And actually, in a more quantitative sense,

77
00:07:38,880 --> 00:07:42,720
and we come back to that later in case of somnambular deduction, you can rigorously prove

78
00:07:42,720 --> 00:07:48,160
that you assume that the world is simple, then Occam's razor is the best you can do in a certain

79
00:07:48,160 --> 00:07:55,520
sense. So, I apologize for the romanticized question, but why do you think outside of its

80
00:07:55,520 --> 00:08:00,000
effectiveness, why do we do you think we find simplicity so appealing as human beings? Why

81
00:08:00,000 --> 00:08:07,120
does it just, why does E equals MC squared seem so beautiful to us humans?

82
00:08:08,240 --> 00:08:14,240
I guess mostly, in general, many things can be explained by an evolutionary argument.

83
00:08:14,880 --> 00:08:19,280
And, you know, there's some artifacts in humans which, you know, are just artifacts and not an

84
00:08:19,280 --> 00:08:28,240
evolutionary necessary. But with this beauty and simplicity, it's, I believe, at least the core

85
00:08:28,800 --> 00:08:35,840
is about, like science, finding regularities in the world, understanding the world,

86
00:08:35,840 --> 00:08:42,400
which is necessary for survival, right? You know, if I look at a bush, right, and I just see noise,

87
00:08:42,400 --> 00:08:47,200
and there is a tiger, right, and eats me, then I'm dead. But if I try to find a pattern, and

88
00:08:47,200 --> 00:08:54,800
we know that humans are prone to find more patterns in data than they are, you know, like, you know,

89
00:08:54,880 --> 00:09:01,040
Mars face and all these things, but these bias towards finding patterns, even if they are not,

90
00:09:01,040 --> 00:09:05,040
but I mean, it's best, of course, if they are, yeah, helps us for survival.

91
00:09:06,480 --> 00:09:11,600
Yeah, that's fascinating. I haven't thought really about the, I thought I just loved science, but

92
00:09:12,800 --> 00:09:20,320
indeed, from in terms of just for survival purposes, there is an evolutionary argument for why we find

93
00:09:21,280 --> 00:09:28,080
the work of Einstein so beautiful. Maybe a quick small tangent, could you describe what

94
00:09:28,080 --> 00:09:36,880
Solomon of induction is? Yeah, so that's a theory which I claim, and Resolominov sort of claimed

95
00:09:36,880 --> 00:09:42,800
a long time ago, that this solves the big philosophical problem of induction. And I believe

96
00:09:42,800 --> 00:09:49,360
the claim is essentially true. And what it does is the following. So, okay, for the

97
00:09:50,320 --> 00:09:58,720
picky listener, induction can be interpreted narrowly and wildly narrow means inferring models from data.

98
00:10:00,560 --> 00:10:05,920
And widely means also then using these models for doing predictions or predictions also part of

99
00:10:05,920 --> 00:10:10,640
the induction. So I'm a little sloppy sort of with the terminology and maybe that comes from

100
00:10:11,200 --> 00:10:14,400
Resolominov, you know, being sloppy, maybe I shouldn't say that.

101
00:10:15,920 --> 00:10:19,920
He can't complain anymore. So let me explain a little bit this theory.

102
00:10:20,880 --> 00:10:26,880
In simple terms, so assume we have a data sequence, make it very simple, the simplest one say 111111 and

103
00:10:26,880 --> 00:10:32,400
you see if 100 ones, what do you think comes next? The natural answer, I'm going to speed up a little

104
00:10:32,400 --> 00:10:38,720
bit, the natural answer is of course, you know, one. Okay, and the question is why? Okay, well,

105
00:10:38,720 --> 00:10:43,760
we see a pattern there. Yeah, okay, there's a one and we repeat it. And why should it suddenly

106
00:10:43,760 --> 00:10:48,480
after 100 ones be different? So what we're looking for is simple explanations or models

107
00:10:49,120 --> 00:10:54,480
for the data we have. And now the question is a model has to be presented in a certain language

108
00:10:55,360 --> 00:11:00,720
in which language to be used. In science, we want formal languages, and we can use mathematics,

109
00:11:00,720 --> 00:11:06,320
or we can use programs on a computer. So abstractly on a Turing machine, for instance,

110
00:11:06,320 --> 00:11:11,600
or can be a general purpose computer. So and there are of course, lots of models of you can

111
00:11:11,600 --> 00:11:16,560
say maybe it's 101 and then 100 zeros and 100 ones, that's a model, right? But they're simpler

112
00:11:16,560 --> 00:11:22,640
models, there's a model print one loop. Now that also explains the data. And if you push

113
00:11:22,640 --> 00:11:28,480
that to the extreme, you are looking for the shortest program, which if you run this program

114
00:11:28,480 --> 00:11:34,720
reproduces the data you have, it will not stop, it will continue naturally. And this you take

115
00:11:34,720 --> 00:11:39,760
for your prediction. And on the sequence of ones, it's very plausible, right, that print one loop

116
00:11:39,760 --> 00:11:45,760
is the shortest program, we can give some more complex examples like 12345. What comes next,

117
00:11:45,760 --> 00:11:51,840
the short program is again, you know, counter. And so that is, roughly speaking, how Solomar's

118
00:11:51,840 --> 00:11:58,400
induction works. The extra twist is that it can also deal with noisy data. So if you have, for

119
00:11:58,400 --> 00:12:03,040
instance, a coin flip, say a biased coin, which comes up head with 60% probability,

120
00:12:04,480 --> 00:12:09,200
then it will predict, it will learn and figure this out. And after a while, it predict or the next

121
00:12:10,240 --> 00:12:14,160
coin flip will be head with probability 60%. So it's the stochastic version of that.

122
00:12:14,720 --> 00:12:18,640
But the goal is the dream is always the search for the short program.

123
00:12:18,640 --> 00:12:23,920
Yes. Yeah. Well, in Solomar of induction, precisely what you do is, so you combine. So

124
00:12:23,920 --> 00:12:29,520
looking for the shortest program is like applying opax razor, like looking for the simplest theory.

125
00:12:29,520 --> 00:12:34,640
There's also Epicoros principle, which says, if you have multiple hypotheses, which equally well

126
00:12:34,640 --> 00:12:39,280
describe your data, don't discard any of them, keep all of them around, you never know. And you

127
00:12:39,280 --> 00:12:44,240
can put that together and say, okay, I have a bias towards simplicity, but I don't rule out the

128
00:12:44,240 --> 00:12:51,280
larger models. And technically, what we do is we weigh the shorter models higher and the longer

129
00:12:51,280 --> 00:13:00,000
models lower. And you use a Bayesian techniques, you have a prior, and which is precisely two to

130
00:13:00,000 --> 00:13:05,280
the minus the complexity of the program. And you weigh all this hypothesis and takes this mixture,

131
00:13:05,280 --> 00:13:09,760
and then you get also this stochasticity in. Yeah, like many of your ideas, that's just a

132
00:13:09,760 --> 00:13:15,920
beautiful idea of weighing based on the simplicity of the program. I love that. That seems to me

133
00:13:15,920 --> 00:13:23,200
maybe a very human-centric concept seems to be a very appealing way of discovering good programs

134
00:13:23,200 --> 00:13:30,720
in this world. You've used the term compression quite a bit. I think it's a beautiful idea,

135
00:13:30,720 --> 00:13:37,200
sort of, we just talked about simplicity, and maybe science or just all of our intellectual

136
00:13:37,200 --> 00:13:43,600
pursuits is basically the attempt to compress the complexity all around us into something simple.

137
00:13:43,600 --> 00:13:52,320
So what does this word mean to you, compression? I essentially have already explained it. So

138
00:13:52,320 --> 00:14:00,160
it compression means, for me, finding short programs for the data or the phenomenon at hand.

139
00:14:00,160 --> 00:14:05,840
You could interpret it more widely as finding simple theories, which can be mathematical theories,

140
00:14:05,840 --> 00:14:12,400
or maybe even informal, like just in words, compression means finding short descriptions,

141
00:14:12,400 --> 00:14:21,440
explanations, programs for the data. Do you see science as a kind of our human attempt at compression?

142
00:14:22,240 --> 00:14:26,320
So we're speaking more generally, because when you say programs, you're kind of zooming in on a

143
00:14:26,320 --> 00:14:30,720
particular, sort of, almost like a computer science, artificial intelligence focus. But

144
00:14:30,720 --> 00:14:35,920
do you see all of human endeavor as a kind of compression? Well, at least all of science,

145
00:14:35,920 --> 00:14:41,440
I see as an endeavor of compression, not all of humanity, maybe. And, well, there are also some

146
00:14:41,440 --> 00:14:46,800
other aspects of science, like experimental design, right? I mean, we create experiments

147
00:14:46,800 --> 00:14:52,480
specifically to get extra knowledge. And this is, that isn't part of the decision-making process.

148
00:14:53,200 --> 00:14:59,200
But once we have the data to understand the data is essentially compression. So I don't see any

149
00:14:59,200 --> 00:15:07,520
difference between compression, understanding, and prediction. So we're jumping around topics a

150
00:15:07,520 --> 00:15:13,760
little bit, but returning back to simplicity, a fascinating concept of comagra of complexity.

151
00:15:14,320 --> 00:15:21,200
So in your sense, do most objects in our mathematical universe have high comagra of

152
00:15:21,200 --> 00:15:25,840
complexity? And maybe what is, first of all, what is comagra of complexity?

153
00:15:25,840 --> 00:15:33,760
Okay, comagra of complexity is a notion of simplicity or complexity. And it takes the

154
00:15:33,760 --> 00:15:39,600
compression view to the extreme. So I explained before that if you have some data sequence,

155
00:15:39,600 --> 00:15:45,360
just think about a file on a computer and best sort of, you know, just a string of bits. And

156
00:15:46,320 --> 00:15:52,160
if you, and we have data compressors, like we compress big files into, say, zip files with

157
00:15:52,160 --> 00:15:57,920
certain compressors. And you can also produce self-extracting RKFs. That means as an executable,

158
00:15:57,920 --> 00:16:03,120
if you run it, it reproduces your original file without needing an extra decompressor. It's just

159
00:16:03,120 --> 00:16:08,880
a decompressor plus the RKF together in one. And now there are better and worse compressors. And

160
00:16:08,880 --> 00:16:14,320
you can ask, what is the ultimate compressor? So what is the shortest possible self-extracting

161
00:16:14,320 --> 00:16:20,480
RKF you could produce for a certain data set, which reproduces the data set. And the length of

162
00:16:20,480 --> 00:16:27,120
this is called the comagra of complexity. And arguably, that is the information content in

163
00:16:27,120 --> 00:16:31,200
the data set. I mean, if the data set is very redundant or very boring, you can compress it

164
00:16:31,200 --> 00:16:36,480
very well. So the information content should be low. And, you know, it is low according to this

165
00:16:36,480 --> 00:16:41,920
definition. Does the length of the shortest program that summarizes the data? Yes. Yeah.

166
00:16:41,920 --> 00:16:47,840
And what's your sense of our universe when we think about the different

167
00:16:50,000 --> 00:16:56,320
objects in our universe that we try concepts or whatever at every level? Do they have higher

168
00:16:56,320 --> 00:17:02,560
or low comagra of complexity? So what's the hope? Do we have a lot of hope in being able to summarize

169
00:17:03,280 --> 00:17:08,720
much of our world? That's a tricky and difficult question. So

170
00:17:09,360 --> 00:17:14,000
as I said before, I believe that the whole universe, based on the evidence we have,

171
00:17:14,000 --> 00:17:19,920
is very simple. So it has a very short description, the whole. Sorry to linger on that.

172
00:17:19,920 --> 00:17:25,920
The whole universe, what does that mean? Do you mean at the very basic fundamental level in order

173
00:17:25,920 --> 00:17:32,160
to create the universe? Yes. Yeah. So you need a very short program when you run it. To get the

174
00:17:32,160 --> 00:17:36,880
thing going. To get the thing going, and then it will reproduce our universe. There's a problem

175
00:17:37,840 --> 00:17:44,560
with noise. We can come back to that later, possibly. Is noise a problem or is it a bug or a

176
00:17:44,560 --> 00:17:52,400
feature? I would say it makes our life as a scientist really, really much harder. I mean,

177
00:17:52,400 --> 00:17:57,200
think about it without noise. We wouldn't need all of the statistics. But that maybe we wouldn't

178
00:17:57,200 --> 00:18:03,200
feel like there's a free will. Maybe we need that for the... This is an illusion that noise can give

179
00:18:03,280 --> 00:18:08,960
you free will. At least in that way, it's a feature. But also, if you don't have noise,

180
00:18:08,960 --> 00:18:15,120
you have chaotic phenomena, which are effectively like noise. So we can't get away with statistics

181
00:18:15,120 --> 00:18:19,520
even then. I mean, think about rolling a dice and forget about quantum mechanics and you know

182
00:18:19,520 --> 00:18:24,960
exactly how you throw it. But I mean, it's still so hard to compute the trajectory that, effectively,

183
00:18:24,960 --> 00:18:31,360
it is best to model it as coming out with a number, with probability one over six.

184
00:18:31,920 --> 00:18:39,280
But from this sort of philosophical Kolmogorov complexity perspective, if we didn't have noise,

185
00:18:39,840 --> 00:18:46,640
then arguably you could describe the whole universe as standard model plus

186
00:18:46,640 --> 00:18:50,800
generativity. I mean, we don't have a theory of everything yet, but sort of assuming we are

187
00:18:50,800 --> 00:18:55,600
close to it or have it here, plus the initial conditions, which may hopefully be simple. And

188
00:18:55,600 --> 00:19:00,640
then you just run it and then you would reproduce the universe. But that's spoiled by noise or by

189
00:19:00,720 --> 00:19:08,320
chaotic systems or by initial conditions, which may be complex. So now if we don't take the whole

190
00:19:08,320 --> 00:19:15,760
universe with just a subset, just take planet Earth. Planet Earth cannot be compressed into

191
00:19:15,760 --> 00:19:20,960
a couple of equations. This is a hugely complex system. So interesting. So when you look at the

192
00:19:20,960 --> 00:19:26,640
window, the whole thing might be simple, but when you just take a small window, then it may become

193
00:19:26,640 --> 00:19:33,200
complex. And that may be counterintuitive. But there's a very nice analogy, the book, the library

194
00:19:33,200 --> 00:19:38,160
of all books. So imagine you have a normal library with interesting books, and you go there, great.

195
00:19:38,160 --> 00:19:44,160
Lots of information and huge, quite complex. So now I create a library which contains all

196
00:19:44,160 --> 00:19:50,080
possible books, say, of 500 pages. So the first book just has AAA over all the pages. The next book

197
00:19:50,080 --> 00:19:55,200
AAA and ends with B. And so on. I create this library of all books. I can write a super short

198
00:19:55,200 --> 00:20:00,080
program which creates this library. So this library which has all books has zero information

199
00:20:00,080 --> 00:20:04,640
content. And you take a subset of this library and suddenly you have a lot of information in there.

200
00:20:05,200 --> 00:20:09,760
So that's fascinating. I think one of the most beautiful object, mathematical objects that

201
00:20:09,760 --> 00:20:14,160
at least today seems to be understudied or under talked about is cellular automata.

202
00:20:15,280 --> 00:20:20,000
What lessons do you draw from sort of the game of life for cellular automata, where you start with

203
00:20:20,000 --> 00:20:26,160
the simple rules, just like you're describing with the universe, and somehow complexity emerges.

204
00:20:26,160 --> 00:20:33,200
Do you feel like you have an intuitive grasp on the behavior, the fascinating behavior of such

205
00:20:33,200 --> 00:20:38,640
systems, where some, like you said, some chaotic behavior could happen, some complexity could

206
00:20:38,640 --> 00:20:44,640
emerge, some, it could die out in some very rigid structures. Do you have a sense about

207
00:20:45,360 --> 00:20:50,800
cellular automata that somehow transfers maybe to the bigger questions of our universe?

208
00:20:50,800 --> 00:20:55,200
Yeah, the cellular automata and especially the converse game of life is really great because

209
00:20:55,200 --> 00:20:59,360
this rule is so simple. You can explain it to every child and even by hand you can simulate a

210
00:20:59,360 --> 00:21:05,920
little bit. And you see this beautiful patterns emerge and people have proven that it's even

211
00:21:05,920 --> 00:21:10,480
touring complete. You cannot just use a computer to simulate game of life, but you can also use

212
00:21:10,480 --> 00:21:19,520
game of life to simulate any computer. That is truly amazing. And it's the prime example probably to

213
00:21:20,240 --> 00:21:26,480
demonstrate that very simple rules can lead to very rich phenomena. And people sometimes,

214
00:21:26,480 --> 00:21:31,600
you know, how can, how is chemistry and biology so rich? I mean, this can't be based on simple

215
00:21:31,600 --> 00:21:38,320
rules. But no, we know quantum electrodynamics describes all of chemistry. And we come later

216
00:21:38,400 --> 00:21:42,880
back to that. I claim intelligence can be explained or described in one single equation,

217
00:21:42,880 --> 00:21:49,200
this very rich phenomenon. You asked also about whether, you know, I understand this

218
00:21:49,200 --> 00:21:56,720
phenomenon and it's probably not. And this is saying you never understand really things,

219
00:21:56,720 --> 00:22:04,720
you just get used to them. And I think pretty used to cellular automata. So you believe that

220
00:22:04,720 --> 00:22:09,760
you understand now why this phenomenon happens. But I give you a different example. I didn't play

221
00:22:09,760 --> 00:22:15,360
too much this converse game of life, but a little bit more with fractals and with the

222
00:22:15,360 --> 00:22:19,760
Mandelbrot set. And you need beautiful, you know, patterns, just look Mandelbrot set.

223
00:22:20,960 --> 00:22:24,640
And well, when the computers were really slow and I just had a black and white

224
00:22:24,640 --> 00:22:28,480
monitor and programmed my own programs on an assembler too.

225
00:22:29,440 --> 00:22:36,960
Wow. Wow, you're legit to get these fractals on the screen. And it was mesmerized and much

226
00:22:36,960 --> 00:22:41,520
later. So I returned to this, you know, every couple of years. And then I tried to understand

227
00:22:41,520 --> 00:22:48,400
what is going on. And you can understand a little bit. So I tried to derive the locations,

228
00:22:48,400 --> 00:22:57,280
you know, there are these circles and the apple shape. And then you have smaller Mandelbrot sets

229
00:22:57,280 --> 00:23:03,360
recursively in this set. And there's a way to mathematically by solving high order polynomials

230
00:23:03,360 --> 00:23:09,440
to figure out where these centers are and what size they are approximately. And by sort of

231
00:23:09,440 --> 00:23:18,000
mathematically approaching this problem, you slowly get a feeling of why things are like they are.

232
00:23:18,000 --> 00:23:24,800
And that sort of isn't, you know, first step to understanding why this rich phenomenon.

233
00:23:24,800 --> 00:23:28,800
Do you think it's possible? What's your intuition? Do you think it's possible to reverse engineer

234
00:23:28,800 --> 00:23:35,520
and find the short program that generated the these fractals, sort of by what looking at the

235
00:23:35,520 --> 00:23:42,320
fractals? Well, in principle, yes. Yeah. So I mean, in principle, what you can do is you take,

236
00:23:42,320 --> 00:23:46,400
you know, any data set, you know, you take these fractals, or you take whatever your data set,

237
00:23:46,400 --> 00:23:53,280
whatever you have. So a picture of Conveys game of life. And you run through all programs, you

238
00:23:53,280 --> 00:23:57,200
take a program of size one, two, three, four, and all these programs around them all in parallel in

239
00:23:57,200 --> 00:24:02,960
so called dovetailing fashion, give them computational resources, first one 50%, second one half

240
00:24:02,960 --> 00:24:09,120
resources and so on and let them run, wait until they hold, give an output, compare it to your data.

241
00:24:09,120 --> 00:24:13,600
And if some of these programs produce the correct data, then you stop and then you have already

242
00:24:13,600 --> 00:24:18,320
some program, it may be a long program, because it's faster. And then you continue and you get

243
00:24:18,320 --> 00:24:23,040
shorter and shorter programs until you eventually find the shortest program. The interesting thing

244
00:24:23,040 --> 00:24:27,360
you can ever know whether it's a shortest program, because there could be an even shorter program,

245
00:24:27,360 --> 00:24:34,080
which is just even slower. And you just have to wait here. But asymptotically, and actually

246
00:24:34,080 --> 00:24:39,200
after the final time, you have the shortest program. So this is a theoretical, but completely

247
00:24:39,200 --> 00:24:47,840
impractical way of finding the underlying structure in every data set. And that was a

248
00:24:47,840 --> 00:24:51,840
lot more of induction does and Convogorov complexity. In practice, of course, we have to

249
00:24:51,840 --> 00:25:00,640
approach the problem more intelligently. And then if you take resource limitations into account,

250
00:25:00,640 --> 00:25:06,240
there's once the field of pseudo random numbers, yeah, and these are random numbers. So these are

251
00:25:06,240 --> 00:25:12,800
deterministic sequences, but no algorithm which is fast, fast means runs in polynomial time can

252
00:25:12,800 --> 00:25:18,560
detect that it's actually deterministic. So we can produce interesting, I mean, random numbers,

253
00:25:18,560 --> 00:25:23,360
maybe not that interesting, but just an example, we can produce complex looking data.

254
00:25:24,240 --> 00:25:28,800
And we can then prove that no fast algorithm can detect the underlying pattern.

255
00:25:31,600 --> 00:25:34,080
Which is unfortunately,

256
00:25:37,200 --> 00:25:41,280
that's a big challenge for our search for simple programs in the space of artificial intelligence,

257
00:25:41,280 --> 00:25:46,000
perhaps. Yes, it definitely is wanted vision intelligence. And it's quite surprising that

258
00:25:46,000 --> 00:25:53,280
it's, I can't say easy. I mean, physicists worked really hard to find these theories, but apparently

259
00:25:53,280 --> 00:25:57,840
it was possible for human minds to find these simple rules in the universe. It could have

260
00:25:57,840 --> 00:26:02,720
been different, right? It could have been different. It's, it's, it's awe inspiring.

261
00:26:04,560 --> 00:26:12,320
So let me ask another absurdly big question. What is intelligence in your view?

262
00:26:13,120 --> 00:26:15,040
So I have, of course, a definition.

263
00:26:16,960 --> 00:26:20,800
I wasn't sure what you're going to say, because you could have just as easy said, I have no clue.

264
00:26:21,360 --> 00:26:29,920
Which many people would say, but I'm not modest in this question. So the, the informal version,

265
00:26:31,600 --> 00:26:36,960
which I worked out together with Shane, like who co-founded the mind is that intelligence

266
00:26:36,960 --> 00:26:41,760
measures and agents ability to perform well in a wide range of environments.

267
00:26:43,120 --> 00:26:49,120
So that doesn't sound very impressive. And what it, these words have been very carefully chosen.

268
00:26:49,760 --> 00:26:56,800
And there is a mathematical theory behind that. And we come back to that later. And if you look at

269
00:26:56,800 --> 00:27:02,800
this, this definition by itself, it seems like, yeah, okay, but it seems a lot of things are

270
00:27:02,800 --> 00:27:10,320
missing. But if you think it's true, then you realize that most, and I claim all of the other

271
00:27:10,320 --> 00:27:14,640
traits, at least of rational intelligence, which we usually associate with intelligence,

272
00:27:14,640 --> 00:27:20,880
are emergent phenomena from this definition, like, you know, creativity, memorization, planning,

273
00:27:20,880 --> 00:27:26,560
knowledge. You all need that in order to perform well in a wide range of environments.

274
00:27:27,520 --> 00:27:30,160
So you don't have to explicitly mention that in a definition.

275
00:27:30,160 --> 00:27:35,200
Interesting. So yeah, so the consciousness, abstract reasoning, all these kinds of things

276
00:27:35,200 --> 00:27:42,000
are just emergent phenomena that help you in towards, can you say the definition again?

277
00:27:42,000 --> 00:27:45,520
So multiple environments. Did you mention the word goals?

278
00:27:46,240 --> 00:27:50,080
No, but we have an alternative definition instead of performing well, you can just replace it by

279
00:27:50,080 --> 00:27:55,280
goals. So intelligence measures and agents ability to achieve goals in a wide range of

280
00:27:55,280 --> 00:27:58,080
environments. That's more or less equal. But interesting, because in there,

281
00:27:58,080 --> 00:28:02,640
there's an injection of the word goals. So we want to specify there, there should be a goal.

282
00:28:03,200 --> 00:28:06,720
Yeah, but perform well is sort of, what does it mean? It's the same problem.

283
00:28:08,000 --> 00:28:12,160
There's a little bit gray area, but it's much closer to something that could be formalized.

284
00:28:14,240 --> 00:28:18,880
In your view, are humans, where do humans fit into that definition? Are they

285
00:28:19,600 --> 00:28:26,960
general intelligence systems that are able to perform in like, how good are they at fulfilling

286
00:28:26,960 --> 00:28:30,480
that definition at performing well in multiple environments?

287
00:28:31,280 --> 00:28:35,680
Yeah, that's a big question. I mean, the humans are performing best among all

288
00:28:36,880 --> 00:28:39,920
species on Earth. Species we know, we know of. Yeah.

289
00:28:40,720 --> 00:28:45,760
Depends. You could say that trees and plants are doing a better job. They'll probably outlast us.

290
00:28:46,400 --> 00:28:50,400
So yeah, but they are in a much more narrow environment, right? I mean, you just, you know,

291
00:28:50,400 --> 00:28:54,720
have a little bit of air pollution and these trees die and we can adapt, right? We build houses,

292
00:28:54,800 --> 00:29:01,120
we build filters, we do geo-engineering. So the multiple environment part.

293
00:29:01,120 --> 00:29:05,120
Yeah, that is very important, yes. So that distinguish narrow intelligence from wide

294
00:29:05,120 --> 00:29:13,040
intelligence, also in the AI research. So let me ask the alenturing question, can machines

295
00:29:13,840 --> 00:29:20,400
think? Can machines be intelligent? So in your view, I have to kind of ask, the answer is probably

296
00:29:20,400 --> 00:29:27,200
yes, but I want to kind of hear what your thoughts on it. Can machines be made to fulfill this

297
00:29:27,200 --> 00:29:33,680
definition of intelligence, to achieve intelligence? Well, we are sort of getting there and, you know,

298
00:29:33,680 --> 00:29:39,040
on a small scale, we are already there. The wide range of environments are missing,

299
00:29:39,040 --> 00:29:44,480
but we have self-driving cars, we have programs to play go and chess, we have speech recognition.

300
00:29:44,480 --> 00:29:48,240
So it's pretty amazing, but you can, you know, these are narrow environments.

301
00:29:48,480 --> 00:29:54,160
But if you look at AlphaZero, that was also developed by DeepMind. I mean,

302
00:29:54,160 --> 00:29:59,920
got famous with AlphaGo and then came AlphaZero a year later. That was truly amazing. So

303
00:29:59,920 --> 00:30:05,760
on reinforcement learning algorithm, which is able just by self-play to play chess

304
00:30:07,040 --> 00:30:11,760
and then also go. And I mean, yes, they're both games, but they're quite different games. And,

305
00:30:11,760 --> 00:30:16,640
you know, this, you didn't, don't feed them the rules of the game. And the most remarkable thing,

306
00:30:16,640 --> 00:30:21,760
which is still a mystery to me that usually for any decent chess program, I don't know much about

307
00:30:21,760 --> 00:30:29,120
Go, you need opening books and end game tables and so on, too. And nothing in there, nothing was

308
00:30:29,120 --> 00:30:33,440
put in there. Especially with AlphaZero, the self-play mechanism, starting from scratch,

309
00:30:33,440 --> 00:30:42,000
being able to learn actually new strategies. It really discovered, you know, all these

310
00:30:42,000 --> 00:30:48,000
famous openings within four hours by itself. What I was really happy about, I'm a terrible

311
00:30:48,000 --> 00:30:53,040
chess player, but I like Queen Gambi. And AlphaZero figured out that this is the best opening.

312
00:30:54,720 --> 00:31:01,920
Finally, somebody proved you correct. So yes, to answer your question, yes,

313
00:31:01,920 --> 00:31:09,120
I believe that general intelligence is possible. And it also depends how you define it. Do you say

314
00:31:09,120 --> 00:31:15,280
AGI with general intelligence, artificial intelligence, only refers to if you achieve

315
00:31:15,280 --> 00:31:19,840
human level or a sub-human level, but quite broad? Is it also general intelligence,

316
00:31:19,840 --> 00:31:25,040
so we have to distinguish or it's only super human intelligence, general artificial intelligence?

317
00:31:25,040 --> 00:31:29,760
Is there a test in your mind, like the Turing test for natural language or some other test

318
00:31:29,760 --> 00:31:34,400
that would impress the heck out of you, that would kind of cross the line of

319
00:31:35,360 --> 00:31:41,600
your sense of intelligence within the framework that you said? Well, the Turing test has been

320
00:31:41,600 --> 00:31:46,800
criticized a lot, but I think it's not as bad as some people think. And some people think it's too

321
00:31:46,800 --> 00:31:55,120
strong. So it tests not just for a system to be intelligent, but it also has to fake human

322
00:31:55,120 --> 00:32:00,000
deception. Disception, right? Which is, you know, much harder. And on the other hand,

323
00:32:00,000 --> 00:32:07,040
they say it's too weak because it's just maybe fakes, you know, emotions or intelligent behavior.

324
00:32:07,680 --> 00:32:13,920
It's not real. But I don't think that's the problem or big problem. So if you would pass the Turing

325
00:32:13,920 --> 00:32:22,560
test, so a conversation or a terminal with a bot for an hour, or maybe a day or so, and you can

326
00:32:22,560 --> 00:32:27,120
fool a human into, you know, not knowing whether this is a human or not, that it's the Turing test,

327
00:32:27,600 --> 00:32:32,320
I would be truly impressed. And we have this annual competition, the Lubna

328
00:32:33,360 --> 00:32:37,600
prize. And I mean, it started with Eliza, that was the first conversational program.

329
00:32:38,240 --> 00:32:43,200
And what is it called the Japanese Mitsuko or so, that's the winner of the last, you know,

330
00:32:43,200 --> 00:32:48,880
couple of years. And well, it's impressive. Yeah, it's quite impressive. And then Google has

331
00:32:48,880 --> 00:32:55,680
developed Mina, right? Just recently, that's an open domain conversational bot. Just a couple of

332
00:32:55,680 --> 00:33:01,520
weeks ago, I think. Yeah, I kind of like the metric that sort of the Alexa price has proposed.

333
00:33:01,520 --> 00:33:05,360
I mean, maybe it's obvious to you, it wasn't to me of setting sort of a length

334
00:33:06,480 --> 00:33:11,600
of a conversation. Like, you want the bot to be sufficiently interesting that you'd want to keep

335
00:33:11,600 --> 00:33:19,440
talking to it for like 20 minutes. And that's a surprisingly effective and aggregate metric.

336
00:33:19,440 --> 00:33:27,680
Because really, like, nobody has the patience to be able to talk to a bot that's not interesting

337
00:33:27,680 --> 00:33:33,440
and intelligent and witty and is able to go into different tangents, jump domains, be able to,

338
00:33:34,000 --> 00:33:38,240
you know, say something interesting to maintain your attention. Maybe many humans will also fail

339
00:33:38,240 --> 00:33:45,280
this test. Unfortunately, we set just like with autonomous vehicles with chatbots,

340
00:33:45,280 --> 00:33:49,920
we also set a bar that's way too hard to reach. I said, you know, the Turing test is not as bad

341
00:33:49,920 --> 00:33:56,560
as some people believe. But what is really not useful about the Turing test, it gives us no

342
00:33:56,560 --> 00:34:01,920
guidance how to develop these systems in the first place. Of course, you know, we can develop them

343
00:34:01,920 --> 00:34:06,800
by trial and error and, you know, do whatever and then run the test and see whether it works or not.

344
00:34:06,800 --> 00:34:16,880
But a mathematical definition of intelligence gives us, you know, an objective which we can then

345
00:34:16,880 --> 00:34:23,200
analyze by, you know, theoretical tools or computational and, you know, maybe even prove

346
00:34:23,200 --> 00:34:30,480
how close we are. And we will come back to that later with the ISE model. So I mentioned the

347
00:34:30,480 --> 00:34:36,720
compression, right? So in natural language processing, they have achieved amazing results.

348
00:34:36,720 --> 00:34:40,800
And one way to test this, of course, you know, take the system, you train it, and then you,

349
00:34:40,800 --> 00:34:48,000
you know, see how well it performs on the task. But a lot of performance measurement is done by

350
00:34:48,000 --> 00:34:53,280
so-called perplexity, which is essentially the same as complexity or compression length.

351
00:34:53,280 --> 00:34:57,520
So the NLP community develops new systems, and then they measure the compression length,

352
00:34:57,520 --> 00:35:03,440
and then they have ranking and leaks, because there's a strong correlation between

353
00:35:03,440 --> 00:35:08,400
compressing well, and then the systems performing well at the task at hand. It's not perfect,

354
00:35:08,400 --> 00:35:13,360
but it's good enough for them as an intermediate aim.

355
00:35:14,560 --> 00:35:19,840
So you mean measure, so this is kind of almost returning to the common growth complexity. So

356
00:35:19,840 --> 00:35:25,120
you're saying good compression usually means good intelligence. Yes.

357
00:35:26,880 --> 00:35:34,800
So you mentioned you're one of the, one of the only people who dared boldly to try to

358
00:35:34,800 --> 00:35:41,600
formalize the idea of artificial general intelligence, to have a mathematical framework

359
00:35:41,600 --> 00:35:50,560
for intelligence, just like as we mentioned, termed IEXI, A-I-X-I. So let me ask the basic

360
00:35:50,560 --> 00:35:58,560
question, what is IEXI? Okay, so let me first say what it stands for, because what it stands for,

361
00:35:58,560 --> 00:36:02,720
actually, that's probably the more basic question. The first question is usually how

362
00:36:03,520 --> 00:36:08,080
how it's pronounced, but finally I put it on the website, how it's pronounced, and you figured it out.

363
00:36:09,040 --> 00:36:15,360
Yeah. The name comes from AI, artificial intelligence, and the XI is the Greek letter

364
00:36:15,360 --> 00:36:23,200
XI, which are used for Solomonov's distribution for quite stupid reasons, which I'm not willing to

365
00:36:23,200 --> 00:36:30,640
repeat here in front of camera. So it does happen to be more or less arbitrary, I chose the XI,

366
00:36:31,440 --> 00:36:37,760
but it also has nice other interpretations. So there are actions and perceptions in this

367
00:36:37,760 --> 00:36:44,960
model, where an agent has actions and perceptions, and over time, so this is A-I-X-I, so there's

368
00:36:44,960 --> 00:36:50,560
an action at time I, and then followed by a perception at time I. We'll go with that. I'll

369
00:36:50,560 --> 00:36:56,720
edit out the first part. I'm just kidding. I have some more interpretations. So at some point,

370
00:36:56,720 --> 00:37:04,000
maybe five years ago or 10 years ago, I discovered in Barcelona, it was on a big church,

371
00:37:04,560 --> 00:37:11,200
there was a stone engraved, some text, and the word Aixi appeared there a couple of times.

372
00:37:12,720 --> 00:37:19,280
I was very surprised and happy about that, and I looked it up, so this is Catalan language,

373
00:37:19,280 --> 00:37:24,080
and it means with some interpretation, that's it, that's the right thing to do. Yeah, Heureka.

374
00:37:24,720 --> 00:37:32,000
Oh, so it's almost like destined, somehow came to you in a dream.

375
00:37:32,000 --> 00:37:36,480
And similar, there's a Chinese word, Aixi, also written like Aixi, if you transcribe that to

376
00:37:36,480 --> 00:37:42,480
Pingen. And the final one is that is A-I, crossed with induction, because that is, and that's going

377
00:37:42,480 --> 00:37:48,240
more to the content now. So good old fashioned A-I is more about planning a known deterministic

378
00:37:48,240 --> 00:37:53,760
world, and induction is more about often IID data and inferring models, and essentially,

379
00:37:53,760 --> 00:37:58,640
what this Aixi model does is combining these two. And I actually also recently,

380
00:37:58,640 --> 00:38:05,920
I think heard that in Japanese, A-I means love. So if you can combine X-I somehow with that,

381
00:38:06,560 --> 00:38:12,240
I think we can, there might be some interesting ideas there. So Aixi, let's then take the next

382
00:38:12,240 --> 00:38:19,440
step. Can you maybe talk at the big level of what is this mathematical framework?

383
00:38:19,680 --> 00:38:26,480
So it consists essentially of two parts. One is the learning and induction and prediction part.

384
00:38:26,480 --> 00:38:31,760
And the other one is the planning part. So let's come first to the learning induction

385
00:38:31,760 --> 00:38:38,480
prediction part, which essentially I explained already before. So what we need for any agent

386
00:38:39,200 --> 00:38:44,640
to act well is that it can somehow predict what happens. I mean, if you have no idea what your

387
00:38:44,640 --> 00:38:50,640
actions do, how can you decide which acts are good or not? So you need to have some model of

388
00:38:50,640 --> 00:38:57,840
what your actions effect. So what you do is you have some experience, you build models like scientists

389
00:38:57,840 --> 00:39:02,480
of your experience, then you hope these models are roughly correct, and then you use these models

390
00:39:02,480 --> 00:39:07,520
for prediction. And a model is, sorry to interrupt, and a model is based on your perception of the

391
00:39:07,520 --> 00:39:14,160
world, how your actions will affect that world. That's not the important part.

392
00:39:14,640 --> 00:39:18,400
It is technically important, but at this stage, we can just think about predicting, say,

393
00:39:19,120 --> 00:39:23,920
stock market data, whether data or IQ sequences, one, two, three, four, five, what comes next.

394
00:39:23,920 --> 00:39:30,160
Yeah. So of course, our actions affect what we're doing, but I'll come back to that in a second.

395
00:39:30,160 --> 00:39:36,400
So, and I'll keep just interrupting. So just to draw a line between prediction and planning.

396
00:39:37,440 --> 00:39:41,840
What do you mean by prediction in this way? It's trying to predict the

397
00:39:42,720 --> 00:39:48,000
environment without your long-term action in the environment. What is prediction?

398
00:39:49,440 --> 00:39:52,320
Okay. If you want to put the actions in now, okay, then let's put in them now.

399
00:39:54,720 --> 00:40:00,160
We don't have to put them now. Scratch it. Don't question. Okay. So the simplest form of prediction

400
00:40:00,160 --> 00:40:06,160
is that you just have data that you passively observe, and you want to predict what happens

401
00:40:06,160 --> 00:40:13,280
without, you know, interfering. As I said, weather forecasting, stock market, IQ sequences, or just

402
00:40:14,880 --> 00:40:20,000
anything. Okay. And Solominov's theory of induction based on compression. So you look for the shortest

403
00:40:20,000 --> 00:40:24,320
program which describes your data sequence. And then you take this program, run it,

404
00:40:24,320 --> 00:40:29,040
which reproduces your data sequence by definition, and then you let it continue running,

405
00:40:29,040 --> 00:40:34,240
and then it will produce some predictions. And you can rigorously prove that for any

406
00:40:34,240 --> 00:40:41,280
prediction task, this is essentially the best possible predictor. Of course, if there's a prediction

407
00:40:41,280 --> 00:40:47,120
task, or a task which is unpredictable, like, you know, your fair coin flips, yeah, I cannot

408
00:40:47,120 --> 00:40:50,880
predict the next fair coin, but what Solominov does is says, okay, next head is probably 50%.

409
00:40:51,520 --> 00:40:55,120
It's the best you can do. So if something is unpredictable, Solominov will also not

410
00:40:55,120 --> 00:41:00,560
magically predict it. But if there is some pattern and predictability, then Solominov

411
00:41:00,640 --> 00:41:06,000
induction will figure that out, eventually, and not just eventually, but rather quickly,

412
00:41:06,000 --> 00:41:13,760
and you can have proof convergence rates, whatever your data is. So there's pure magic in a sense.

413
00:41:14,640 --> 00:41:18,160
What's the catch? Well, the catch is that it's not computable. And we come back to that later.

414
00:41:18,160 --> 00:41:22,640
You cannot just implement it in even this Google resources here, and run it and, you know, predict

415
00:41:22,640 --> 00:41:27,040
the stock market and become rich. I mean, if it's raised Solominov already, you know, tried it at the

416
00:41:27,040 --> 00:41:32,720
time. But so the basic task is you're in the environment, and you're interacting with an

417
00:41:32,720 --> 00:41:37,520
environment to try to learn a model of that environment. And the model is in the space of

418
00:41:37,520 --> 00:41:41,200
these all these programs. And your goal is to get a bunch of programs that are simple.

419
00:41:41,200 --> 00:41:45,280
And so let's, let's go to the actions now. But actually, good that you asked usually,

420
00:41:45,280 --> 00:41:49,600
I skipped this part, although there is also a minor contribution, which I did. So the action part,

421
00:41:49,600 --> 00:41:53,360
but they usually sort of just jump to the decision part. So let me explain the action part now.

422
00:41:53,360 --> 00:42:01,040
Thanks for asking. So you have to modify it a little bit by now not just predicting a sequence

423
00:42:01,040 --> 00:42:07,680
which just comes to you, but you have an observation, then you act somehow. And then you want to predict

424
00:42:07,680 --> 00:42:13,920
the next observation based on the past observation and your action. Then you take the next action.

425
00:42:14,560 --> 00:42:18,960
You don't care about predicting it because you're doing it. And then you get the next observation.

426
00:42:18,960 --> 00:42:22,720
And you want, well, before you get it, you want to predict it again based on your past

427
00:42:22,720 --> 00:42:29,360
action and observation sequence. You just condition extra on your actions. There's an

428
00:42:29,360 --> 00:42:33,040
interesting alternative that you also try to predict your own actions.

429
00:42:35,520 --> 00:42:40,320
If you want. In the past or the future. Your future actions. That's interesting.

430
00:42:41,840 --> 00:42:47,360
Wait, let me wrap. I think my brain is broke. We should maybe discuss that later

431
00:42:47,360 --> 00:42:51,200
after I've explained the ICSE model. That's an interesting variation. But that is a really

432
00:42:51,200 --> 00:42:55,440
interesting variation. And a quick comment. I don't know if you want to insert that in here.

433
00:42:55,440 --> 00:43:01,120
But you're looking at that in terms of observations, you're looking at the entire big

434
00:43:01,120 --> 00:43:05,600
history, the long history of the observations. Exactly. That's very important, the whole history

435
00:43:05,600 --> 00:43:10,800
from birth sort of of the agent. And we can come back to that. Also why this is important here.

436
00:43:10,800 --> 00:43:15,760
Often, you know, in RL, you have MDPs, macro decision processes, which are much more limiting.

437
00:43:15,760 --> 00:43:21,520
Okay, so now we can predict conditioned on actions. So even if the influence environment.

438
00:43:21,520 --> 00:43:26,080
But prediction is not all we want to do, right? We also want to act really in the world.

439
00:43:26,800 --> 00:43:32,080
And the question is how to choose the actions. And we don't want to greedily choose the actions.

440
00:43:33,280 --> 00:43:37,520
You know, just, you know, what is best in the next time step. And we first I should say, you

441
00:43:37,520 --> 00:43:41,840
know, what is, you know, how do we measure performance? So we measure performance by giving

442
00:43:41,840 --> 00:43:47,040
the agent reward. That's the so called reinforcement learning framework. So every time step,

443
00:43:47,040 --> 00:43:51,120
you can give it a positive reward or negative reward, or maybe no reward, it could be a very

444
00:43:51,120 --> 00:43:55,360
scarce, right? Like if you play chess, just at the end of the game, you give plus one for winning

445
00:43:55,360 --> 00:43:59,840
or minus one for losing. So in the IXI framework, that's completely sufficient. So occasionally,

446
00:43:59,840 --> 00:44:05,120
you give a reward signal, and you ask the agent to maximize reward, but not greedily sort of,

447
00:44:05,120 --> 00:44:08,640
you know, the next one, next one, because that's very bad in the long run, if you're greedy.

448
00:44:08,880 --> 00:44:14,000
So, but over the lifetime of the agent. So let's assume the agent lives for M

449
00:44:14,000 --> 00:44:18,320
time steps as they dice in sort of 100 years sharp. That's just, you know, the simplest model

450
00:44:18,320 --> 00:44:25,120
to explain. So it looks at the future reward sum and ask what is my action sequence, or actually

451
00:44:25,120 --> 00:44:30,880
more precisely my policy, which leads in expectation, because I don't know the world,

452
00:44:32,080 --> 00:44:37,200
to the maximum reward sum. Let me give you an analogy. In chess, for instance,

453
00:44:38,160 --> 00:44:43,360
we know how to play optimally in theory, it's just a mini max strategy. I play the move which

454
00:44:43,360 --> 00:44:48,560
seems best to me under the assumption that the opponent plays the move which is best for him,

455
00:44:48,560 --> 00:44:54,480
so best, so worst for me, under the assumption that he I play again, the best move. And then you

456
00:44:54,480 --> 00:44:59,120
have this expecting max three to the end of the game. And then you back propagate and then you

457
00:44:59,120 --> 00:45:02,960
get the best possible move. So that is the optimal strategy, which for Neumann already

458
00:45:02,960 --> 00:45:10,640
figured out a long time ago, for playing adversarial games, luckily, or maybe unluckily,

459
00:45:10,640 --> 00:45:16,400
for the theory, it becomes harder that world is not always adversarial. So it can be,

460
00:45:16,400 --> 00:45:21,280
if the other humans even cooperative here, or nature is usually I mean, the dead nature is

461
00:45:21,280 --> 00:45:27,520
stochastic, you know, things just happen randomly, or don't care about you. So what you have to

462
00:45:27,520 --> 00:45:32,000
take into account is the noise, you know, and not necessarily adversariality. So you replace

463
00:45:32,000 --> 00:45:37,360
the minimum on the opponent's side by an expectation, which is general enough to include

464
00:45:37,360 --> 00:45:43,040
also adversarial cases. So now instead of a mini max strategy of an expecting max strategy.

465
00:45:43,760 --> 00:45:46,960
So far so good. So that is well known. It's called sequential decision theory.

466
00:45:47,920 --> 00:45:53,200
But the question is, on which probability distribution do you base that if I have the

467
00:45:53,200 --> 00:45:57,840
true probability distribution, like say I play beggining, right, there's dice,

468
00:45:57,840 --> 00:46:01,680
and there's certain randomness involved, yeah, I can calculate probabilities and feed it in the

469
00:46:01,680 --> 00:46:05,760
expecting max or the sequential decision tree, come up with the optimal decision if I have

470
00:46:05,760 --> 00:46:10,800
enough compute. But in the for the real world, we don't know that, you know, what is the probability

471
00:46:11,520 --> 00:46:17,040
the driver in front of me breaks. I don't know. Yeah, so depends on all kinds of things. And

472
00:46:17,680 --> 00:46:22,800
especially new situations, I don't know. So this is this unknown thing about prediction. And there's

473
00:46:22,800 --> 00:46:27,760
where Solomanov comes in. So what you do is in sequential decision tree, you just replace the

474
00:46:27,760 --> 00:46:34,000
true distribution, which we don't know, by this universal distribution, I didn't explicitly

475
00:46:34,000 --> 00:46:39,040
talk about it, but this is used for universal prediction, and plug it into the sequential

476
00:46:39,040 --> 00:46:44,000
decision tree mechanism. And then you get the best of both worlds. You have a long term planning

477
00:46:44,000 --> 00:46:49,760
agent. But it doesn't need to know anything about the world, because the Solomanov induction part

478
00:46:50,720 --> 00:46:58,480
learns. Can you explicitly try to describe the universal distribution and how Solomanov induction

479
00:46:58,480 --> 00:47:03,760
plays a role here? I'm trying to understand. So what he does it. So in the simplest case,

480
00:47:03,760 --> 00:47:08,000
I said, take the shortest program describing your data, run it, have a prediction which would be

481
00:47:08,000 --> 00:47:14,080
deterministic. Yes. Okay. But you should not just take the shortest program, but also consider the

482
00:47:14,080 --> 00:47:22,320
longer ones, but keep it lower a priori probability. So in the Bayesian framework, you say a priori,

483
00:47:22,320 --> 00:47:30,240
any distribution, which is a model, or a stochastic program has a certain a priori

484
00:47:30,240 --> 00:47:34,480
probability, which is two to the minus and y two to the minus length, you know, I could explain

485
00:47:34,480 --> 00:47:41,280
length of this program. So longer programs are punished, a priori. And then you multiply it

486
00:47:41,280 --> 00:47:48,080
with the so called likelihood function. Yeah, which is as the name suggests, is how likely

487
00:47:48,080 --> 00:47:54,160
is this model given the data at hand. So if you have a very wrong model, it's very unlikely

488
00:47:54,160 --> 00:47:58,800
that this model is true. And so it is very small numbers. So even if the model is simple, it gets

489
00:47:58,800 --> 00:48:04,480
penalized by that. And what you do is then you take just the sum, but this is the average over it.

490
00:48:04,480 --> 00:48:10,480
And this gives you a probability distribution. So universal distribution or Solomanov distribution.

491
00:48:10,480 --> 00:48:14,320
So it's weighed by the simplicity of the program and likelihood. Yes.

492
00:48:15,200 --> 00:48:22,480
It's kind of a nice idea. Yeah. So okay. And then you said there's your planning

493
00:48:22,480 --> 00:48:28,480
n or m or forgot the letter steps into the future. So how difficult is that problem? What's

494
00:48:28,480 --> 00:48:32,720
involved there? Okay, basic optimization problem? What are we talking about? Yeah, so you have a

495
00:48:32,720 --> 00:48:38,640
planning problem up to horizon m. And that's exponential time in the horizon m, which is,

496
00:48:38,640 --> 00:48:43,440
I mean, it's computable, but in fact, intractable. I mean, even for chess, it's already intractable

497
00:48:43,440 --> 00:48:48,080
to do that exactly. And, you know, for though, but it could be also discounted kind of framework

498
00:48:48,080 --> 00:48:54,560
where so, so having a hard horizon, you know, at 100 years, it's just for simplicity of

499
00:48:54,560 --> 00:49:00,000
discussing the model. And also sometimes the master simple. But there are lots of variations.

500
00:49:00,000 --> 00:49:07,120
Actually, quite interesting parameter is it's, there's nothing really problematic about it.

501
00:49:07,120 --> 00:49:10,960
But it's very interesting. So for instance, you think, no, let's, let's, let's, let's let the

502
00:49:10,960 --> 00:49:16,240
parameter m tend to infinity, right? You want an agent, which lives forever, right? If you do it

503
00:49:16,240 --> 00:49:20,560
now, you have two problems. First, the mathematics breaks down because you have an infinite reward

504
00:49:20,560 --> 00:49:26,240
sum, which may give infinity and getting reward 0.1 in the time step is infinity and giving reward

505
00:49:26,240 --> 00:49:32,640
one every time step is infinity. So equally good. Not really what we want. Other problem is that

506
00:49:33,600 --> 00:49:39,120
if you have an infinite life, you can be lazy for as long as you want for 10 years and then catch

507
00:49:39,120 --> 00:49:44,960
up with the same expected reward. And, you know, think about yourself or, you know, or maybe,

508
00:49:44,960 --> 00:49:51,280
you know, some friends or so, if they knew they lived forever, you know, why work hard now, you

509
00:49:51,280 --> 00:49:55,440
know, just enjoy your life, you know, and then catch up later. So that's another problem with

510
00:49:55,440 --> 00:50:00,560
the infinite horizon. And you mentioned, yes, we can go to discounting. But then the standard

511
00:50:00,640 --> 00:50:05,920
discounting is so-called geometric discounting. So a dollar today is about worth as much as,

512
00:50:05,920 --> 00:50:11,520
you know, $1.05 tomorrow. So if you do the so-called geometric discounting, you have introduced an

513
00:50:11,520 --> 00:50:18,320
effective horizon. So the agent is now motivated to look ahead a certain amount of time effectively.

514
00:50:18,320 --> 00:50:25,120
It's like a moving horizon. And for any fixed effective horizon, there is a problem

515
00:50:25,920 --> 00:50:30,400
to solve which requires larger horizons. So if I look ahead, you know, five time steps,

516
00:50:30,400 --> 00:50:35,520
I'm a terrible chess player, right? I'll need to look ahead longer. If I play go, I probably

517
00:50:35,520 --> 00:50:41,280
have to look ahead even longer. So for every problem, no, for every horizon, there is a problem

518
00:50:41,280 --> 00:50:47,280
which this horizon cannot solve. But I introduced the so-called near harmonic horizon, which goes

519
00:50:47,280 --> 00:50:52,640
down with one over t rather than exponentially t, which produces an agent which effectively looks

520
00:50:52,640 --> 00:50:57,280
into the future proportional to each age. So if it's five years old, it plans for five years.

521
00:50:57,280 --> 00:51:02,000
If it's 100 years old, it then plans for 100 years. And it's a little bit similar to humans,

522
00:51:02,000 --> 00:51:06,240
too, right? I mean, children don't plan ahead very long, but then we get adults, we play ahead

523
00:51:06,240 --> 00:51:10,160
more longer. Maybe when we get very old, I mean, we know that we don't live forever,

524
00:51:10,160 --> 00:51:16,960
you know, maybe then our horizon shrinks again. So that's really interesting. So adjusting the

525
00:51:16,960 --> 00:51:21,520
horizon, what is there some mathematical benefit of that? Or is it just a nice

526
00:51:22,880 --> 00:51:27,120
I mean, intuitively, empirically, it will probably be a good idea to sort of push the

527
00:51:27,120 --> 00:51:33,760
horizon back to extend the horizon as you experience more of the world. But is there

528
00:51:33,760 --> 00:51:36,480
some mathematical conclusions here that are beneficial?

529
00:51:37,040 --> 00:51:44,880
With Salomon's prediction part, we have extremely strong finite time, finite data results. So you

530
00:51:44,880 --> 00:51:49,200
have so and so much data, then you lose so and so much. So the deterioration is really great.

531
00:51:49,280 --> 00:51:55,920
With the IKC model with the planning part, many results are only asymptotic, which, well, this is

532
00:51:56,720 --> 00:52:01,680
what is asymptotic means you can prove, for instance, that in the long run, if the agent,

533
00:52:01,680 --> 00:52:06,320
you know, acts long enough, then, you know, it performs optimal or some nice thing happens.

534
00:52:06,320 --> 00:52:11,200
So but you don't know how fast it converges. Yeah, so it may converge fast, but we're just

535
00:52:11,200 --> 00:52:17,200
not able to prove it because a difficult problem. Or maybe there's a bug in the in the in the model

536
00:52:17,200 --> 00:52:22,720
so that it's really that slow. Yeah. So so that is what asymptotic means sort of eventually,

537
00:52:22,720 --> 00:52:30,800
but we don't know how fast. And if I give the agent a fixed horizon M, yeah, then I cannot prove

538
00:52:30,800 --> 00:52:36,240
asymptotic results, right? So I mean, sort of if it dies in 100 years, then in 100 years is over,

539
00:52:36,240 --> 00:52:41,760
I cannot say eventually. So this is the advantage of the discounting that I can prove asymptotic

540
00:52:41,760 --> 00:52:51,040
results. So just to clarify, so so I okay, I made I've built up a model with now in the moment of

541
00:52:51,920 --> 00:52:57,520
have this way of looking several steps ahead. How do I pick what action I will take?

542
00:52:58,800 --> 00:53:03,840
It's like with a playing chess, right, you do this mini max. In this case, here do you expect the

543
00:53:03,840 --> 00:53:11,440
max based on the solometer of distribution, you propagate back. And then, while an action falls

544
00:53:11,440 --> 00:53:16,720
out, the action which maximizes the future expected reward on the solometer of distribution,

545
00:53:16,720 --> 00:53:20,880
and then you just take this action, and then repeat. And then you get a new observation,

546
00:53:20,880 --> 00:53:25,760
and you feed it in this action observation, then you repeat and the reward so on. Yeah. So you wrote

547
00:53:25,760 --> 00:53:31,520
to you. And then maybe you can even predict your own action. I love the idea. But okay, this big

548
00:53:31,520 --> 00:53:39,840
framework. What is it? I mean, it's kind of a beautiful mathematical framework to think about

549
00:53:39,840 --> 00:53:46,160
artificial general intelligence. What can you, what does it help you into it about

550
00:53:47,600 --> 00:53:55,280
how to build such systems or maybe from another perspective? What does it help us in understanding

551
00:53:55,280 --> 00:54:03,280
AGI? So when I started in the field, I was always interested in two things. One was, you know, AGI.

552
00:54:04,240 --> 00:54:10,800
The name didn't exist then, what called general AI or strong AI. And the physics of everything.

553
00:54:10,800 --> 00:54:14,640
So I switched back and forth between computer science and physics quite often.

554
00:54:14,640 --> 00:54:16,560
You said the theory of everything. The theory of everything.

555
00:54:17,360 --> 00:54:21,120
It was basically the biggest problems before all humanity.

556
00:54:23,360 --> 00:54:29,520
Yeah, I can explain if you wanted some later time, you know, why I'm interested in these two

557
00:54:29,520 --> 00:54:37,040
questions. Can I ask you, in a small tangent, if, if, if one to be, it was one to be solved,

558
00:54:37,040 --> 00:54:42,720
which one would you, if one, if you were, if an apple fell in your head, and there was a brilliant

559
00:54:42,720 --> 00:54:48,480
insight and you could arrive at the solution to one, would it be AGI or the theory of everything?

560
00:54:49,120 --> 00:54:53,120
Definitely AGI, because once the AGI problem is solved, they can ask the AGI to solve the

561
00:54:53,120 --> 00:55:01,120
other problem for me. Yeah, brilliantly put. Okay. So, so as you were saying about it.

562
00:55:01,120 --> 00:55:06,640
Okay. So, and the reason why it didn't settle, I mean, this thought about, you know,

563
00:55:07,280 --> 00:55:11,120
once you have solved AGI, it solves all kinds of other, not just the theory of every problem,

564
00:55:11,120 --> 00:55:16,400
but all kinds of use, more useful problems to humanity is very appealing to many people. And,

565
00:55:16,400 --> 00:55:23,920
you know, I had this thought also, but I was quite disappointed with the state of the art

566
00:55:23,920 --> 00:55:28,800
of the field of AI. There was some theory, you know, about logical reasoning, but I was never

567
00:55:28,800 --> 00:55:33,680
convinced that this will fly. And then there was this more, more heuristic approaches with neural

568
00:55:33,680 --> 00:55:40,080
networks, and I didn't like these heuristics. So, and also I didn't have any good idea myself.

569
00:55:42,160 --> 00:55:45,360
So that's the reason why I toggled back and forth quite some while and even worked

570
00:55:45,360 --> 00:55:49,600
so four and a half years in a company developing software or something completely unrelated.

571
00:55:49,600 --> 00:55:56,960
But then I had this idea about the ICSE model. And so what it gives you, it gives you a gold

572
00:55:56,960 --> 00:56:03,760
standard. So I have proven that this is the most intelligent agents, which anybody could

573
00:56:04,960 --> 00:56:10,560
build built in quotation mark, because it's just mathematical and you need infinite compute.

574
00:56:11,040 --> 00:56:17,040
But this is the limit. And this is completely specified. It's not just a framework and, you know,

575
00:56:18,400 --> 00:56:23,440
every year, tens of frameworks are developed, which is just skeletons, and then pieces are

576
00:56:23,440 --> 00:56:27,280
missing. And usually these missing pieces, you know, turn out to be really, really difficult.

577
00:56:27,280 --> 00:56:33,680
And so this is completely and uniquely defined. And we can analyze that mathematically. And

578
00:56:35,120 --> 00:56:39,440
we've also developed some approximations, I can talk about that a little bit later,

579
00:56:40,160 --> 00:56:44,160
that would be sort of the top down approach, like, say, for Neumann's minimax theory,

580
00:56:44,160 --> 00:56:49,520
that's the theoretical optimal play of games. And now we need to approximate it, put heuristics

581
00:56:49,520 --> 00:56:53,040
in prune, the tree, blah, blah, blah, and so on. So we can do that also with the ICSE model,

582
00:56:53,040 --> 00:57:00,720
but for generally I, it can also inspire those. And most of most researchers go bottom upright,

583
00:57:00,720 --> 00:57:05,840
they have the systems that try to make it more general, more intelligent. It can inspire in which

584
00:57:05,840 --> 00:57:11,840
direction to go. What do you mean by that? So if you have some choice to make, right, so how should

585
00:57:11,840 --> 00:57:18,160
they evaluate my system if I can't do cross validation? How should they do my learning if

586
00:57:18,160 --> 00:57:23,120
my standard regularization doesn't work well? Yeah. So the answer is always this, we have a system

587
00:57:23,120 --> 00:57:28,480
which does everything that's ICSE. It's just, you know, completely in the ivory tower, completely

588
00:57:28,480 --> 00:57:33,520
useless from a practical point of view. But you can look at it and see, ah, yeah, maybe, you know,

589
00:57:33,520 --> 00:57:37,520
I can take some aspects and, you know, instead of Kolmogorov complexity, they just take some

590
00:57:37,520 --> 00:57:43,040
compressors which has been developed so far. And for the planning, well, we have UCT which has also,

591
00:57:43,040 --> 00:57:51,600
you know, been used in Go. And it, at least it's inspired me a lot to have this formal

592
00:57:53,280 --> 00:57:57,680
definition. And if you look at other fields, you know, like, I always come back to physics

593
00:57:57,680 --> 00:58:01,440
because I have a physics background, think about the phenomenon of energy that was

594
00:58:01,440 --> 00:58:05,120
long time a mysterious concept. And at some point, it was completely formalized.

595
00:58:06,160 --> 00:58:11,280
And that really helped a lot. And you can point out a lot of these things which were

596
00:58:11,280 --> 00:58:15,280
first mysterious and vague, and then they have been rigorously formalized.

597
00:58:15,280 --> 00:58:20,400
Speed and acceleration has been confused, right, until it was formally defined. There was a time

598
00:58:20,400 --> 00:58:25,840
like this. And people, you know, often, you know, don't have any background, you know, still confuse

599
00:58:25,840 --> 00:58:33,120
it. So, and this IXI model or the intelligence definitions, which is sort of the dual to it,

600
00:58:33,120 --> 00:58:38,320
we come back to that later, formalizes the notion of intelligence uniquely and rigorously.

601
00:58:38,880 --> 00:58:42,480
So in a sense, it serves as kind of the light at the end of the tunnel.

602
00:58:43,040 --> 00:58:48,320
Yes, yeah. So I mean, there's a million questions I could ask her. So maybe

603
00:58:49,440 --> 00:58:54,640
the kind of, okay, let's feel around in the dark a little bit. So there's been here a deep mind,

604
00:58:54,640 --> 00:58:58,000
but in general, been a lot of breakthrough ideas, just like we've been saying around

605
00:58:58,000 --> 00:59:03,600
reinforcement learning. So how do you see the progress in reinforcement learning is different?

606
00:59:04,320 --> 00:59:11,120
Like, which subset of IXI does it occupy the current, like you said, maybe

607
00:59:11,920 --> 00:59:15,600
the Markov assumption is made quite often in reinforcement learning.

608
00:59:16,400 --> 00:59:22,960
The, there's other assumptions made in order to make the system work. What do you see as the

609
00:59:22,960 --> 00:59:25,760
difference connection between reinforcement learning and IXI?

610
00:59:26,640 --> 00:59:32,480
And so the major difference is that essentially all other approaches,

611
00:59:33,360 --> 00:59:37,600
they make stronger assumptions. So in reinforcement learning, the Markov assumption

612
00:59:38,240 --> 00:59:43,280
is that the next state or next observation only depends on the on the previous observation

613
00:59:43,280 --> 00:59:48,000
and not the whole history, which makes, of course, the mathematics much easier rather than dealing

614
00:59:48,000 --> 00:59:53,360
with histories. Of course, they profit from it also, because then you have algorithms that run

615
00:59:53,360 --> 00:59:59,520
on current computers and do something practically useful. But for generally, I all the assumptions

616
00:59:59,520 --> 01:00:07,040
which are made by other approaches, we know already now they are limiting. So, for instance,

617
01:00:07,840 --> 01:00:11,520
usually you need a gothicity assumption in the MDP frameworks in order to learn.

618
01:00:11,520 --> 01:00:16,080
A gothicity essentially means that you can recover from your mistakes and that they are

619
01:00:16,080 --> 01:00:21,680
not traps in the environment. And if you make this assumption, then essentially you can go back

620
01:00:21,680 --> 01:00:28,640
to a previous state, go there a couple of times and then learn what, what statistics and what

621
01:00:28,640 --> 01:00:33,840
this state is like. And then in the long run perform well in this state. But there are no

622
01:00:33,840 --> 01:00:40,480
fundamental problems. But in real life, we know, there can be one single action, one second of

623
01:00:40,480 --> 01:00:46,880
being inattentive while driving a car fast, you know, can ruin the rest of my life, I can become

624
01:00:46,880 --> 01:00:51,440
quadruplegic or whatever. So, and there's no recovery anymore. So, the real world is not

625
01:00:51,440 --> 01:00:55,520
ergodic, I always say, you know, there are traps and there are situations where you're not recovered

626
01:00:55,520 --> 01:01:06,480
from. And very little theory has been developed for this case. What about what do you see in the

627
01:01:06,560 --> 01:01:15,200
context of IHC as the role of exploration, sort of, you mentioned, you know, in the real world

628
01:01:15,200 --> 01:01:20,240
and get into trouble when we make the wrong decisions and really pay for it. But exploration

629
01:01:20,240 --> 01:01:24,960
seems to be fundamentally important for learning about this world, for gaining new knowledge.

630
01:01:25,600 --> 01:01:31,360
So, is exploration baked in? Another way to ask it, what are the parameters

631
01:01:32,240 --> 01:01:38,240
of this of IHC that can be controlled? Yeah, I say the good thing is that there are no

632
01:01:38,240 --> 01:01:44,240
parameters to control. Some other people try knobs to control and you can do that. I mean,

633
01:01:44,240 --> 01:01:51,280
you can modify IHC so that you have some knobs to play with if you want to. But the exploration

634
01:01:51,280 --> 01:01:58,640
is directly baked in. And that comes from the Bayesian learning and the long term planning.

635
01:01:58,640 --> 01:02:08,560
So these together already imply exploration. You can nicely and explicitly prove that for

636
01:02:09,360 --> 01:02:17,920
simple problems like so-called banded problems, where you say to give a real-world example,

637
01:02:17,920 --> 01:02:22,160
say you have two medical treatments, A and B, you don't know the effectiveness, you try A a

638
01:02:22,160 --> 01:02:26,640
little bit, B a little bit, but you don't want to harm too many patients. So you have to sort of

639
01:02:27,600 --> 01:02:34,240
trade off exploring. And at some point you want to explore and you can do the mathematics and

640
01:02:34,240 --> 01:02:40,240
figure out the optimal strategy. It's called Bayesian agents, they're also non-Bayesian agents.

641
01:02:41,040 --> 01:02:46,560
But it shows that this Bayesian framework by taking a prior over possible worlds,

642
01:02:47,280 --> 01:02:51,360
doing the Bayesian mixture, then the Bayes optimal decision with long term planning that is important,

643
01:02:52,240 --> 01:02:58,960
automatically implies exploration also to the proper extent, not too much exploration

644
01:02:58,960 --> 01:03:04,560
and not too little. It is very simple settings. In the IHC model, I was also able to prove that

645
01:03:04,560 --> 01:03:08,800
it is a self-optimizing theorem or asymptotic optimality theorems, although they're only asymptotic,

646
01:03:08,800 --> 01:03:13,120
not finite time bounds. So it seems like the long term planning is a really important,

647
01:03:13,120 --> 01:03:18,880
the long term part of the planning is really important. And also, maybe a quick tangent,

648
01:03:18,880 --> 01:03:23,840
how important do you think is removing the Markov assumption and looking at the full

649
01:03:23,840 --> 01:03:30,080
history? So intuitively, of course, it's important, but is it like fundamentally

650
01:03:30,080 --> 01:03:35,440
transformative to the entirety of the problem? What's your sense of it? Because we all,

651
01:03:35,440 --> 01:03:39,840
we make that assumption quite often, just throwing away the past.

652
01:03:39,840 --> 01:03:47,360
Now, I think it's absolutely crucial. The question is whether there's a way to deal with it in a

653
01:03:47,360 --> 01:03:55,440
more heuristic and still sufficiently well way. So I have to come up with an example in the fly,

654
01:03:55,440 --> 01:04:02,000
but you have some key event in your life a long time ago, in some city or something,

655
01:04:02,000 --> 01:04:06,720
you realize it's a really dangerous street or whatever. And you want to remember that forever,

656
01:04:07,920 --> 01:04:12,000
in case you come back there. Kind of a selective kind of memory. So you remember

657
01:04:12,800 --> 01:04:16,640
all the important events in the past, but somehow selecting the important

658
01:04:17,360 --> 01:04:22,080
They're very hard. Yeah. And I'm not concerned about just storing the whole history. Just

659
01:04:22,080 --> 01:04:27,360
you can calculate human life, say 30 or 100 years doesn't matter, right?

660
01:04:28,720 --> 01:04:34,000
How much data comes in through the vision system and the auditory system, you compress it a little

661
01:04:34,000 --> 01:04:39,840
bit, in this case, lossily and store it. We are soon in the means of just storing it.

662
01:04:40,400 --> 01:04:46,000
But you still need to the selection for the planning part and the compression for the

663
01:04:46,000 --> 01:04:51,680
understanding part. The raw storage I'm really not concerned about. And I think we should just

664
01:04:51,680 --> 01:04:58,320
store if you develop an agent, preferably just store all the interaction history.

665
01:04:59,360 --> 01:05:04,800
And then you build, of course, models on top of it and you compress it and you are selective,

666
01:05:04,880 --> 01:05:11,440
but occasionally, you go back to the old data and reanalyze it based on your new experience

667
01:05:11,440 --> 01:05:16,240
you have. You know, sometimes you are in school, you learn all these things you think is totally

668
01:05:16,240 --> 01:05:21,040
useless. And you know, much later you read us, oh, they were not, you know, so useless as you

669
01:05:21,040 --> 01:05:27,120
thought. I'm looking at you linear algebra. Right. So maybe let me ask about objective

670
01:05:27,120 --> 01:05:34,640
functions because that rewards, it seems to be an important part. The rewards are kind of

671
01:05:34,640 --> 01:05:45,120
given to the system. For a lot of people, the specification of the objective function

672
01:05:46,320 --> 01:05:52,080
is a key part of intelligence. The agent itself figuring out what is important.

673
01:05:52,960 --> 01:06:00,160
What do you think about that? Is it possible within IACC framework to yourself discover

674
01:06:00,240 --> 01:06:06,800
the reward based on which you should operate? Okay, that will be a long answer.

675
01:06:08,800 --> 01:06:14,400
So, and that is a very interesting question. And I'm asked a lot about this question,

676
01:06:14,400 --> 01:06:22,960
where do the rewards come from? And that depends. So, and I give you now a couple of answers. So

677
01:06:22,960 --> 01:06:29,760
if we want to build agents, now let's start simple. So let's assume we want to build an agent

678
01:06:29,760 --> 01:06:35,120
based on the IACC model, which performs a particular task. Let's start with something

679
01:06:35,120 --> 01:06:38,960
super simple, like, I mean, super simple, like playing chess or go or something.

680
01:06:39,760 --> 01:06:43,360
Then you just, you know, the reward is, you know, winning the game is plus one,

681
01:06:43,360 --> 01:06:48,240
losing the game is minus one, done. You apply this agent, if you have enough compute,

682
01:06:48,240 --> 01:06:53,360
you let itself play, and it will learn the rules of the game will play perfect chess after some

683
01:06:53,360 --> 01:07:03,440
while problem solved. Okay, so if you have more complicated problems, then you may believe that

684
01:07:03,440 --> 01:07:09,600
you have the right reward, but it's not. So a nice cute example is elevator control that is also in

685
01:07:09,600 --> 01:07:16,080
Rich Sutton's book, which is a great book, by the way. So you control the elevator, and you think,

686
01:07:16,080 --> 01:07:20,320
well, maybe the reward should be coupled to how long people wait in front of the elevator,

687
01:07:20,400 --> 01:07:25,520
you know, long wait is bad. You program it and you do it. And what happens is the elevator

688
01:07:25,520 --> 01:07:33,040
eagerly picks up all the people, but never drops them off. So then you realize, maybe the time in

689
01:07:33,040 --> 01:07:38,800
the elevator also counts. So you minimize the sum. Yeah. And the elevator does that, but never picks

690
01:07:38,800 --> 01:07:42,880
up the people in the 10th floor and the top floor, because in expectation, it's not worth it. Just

691
01:07:42,960 --> 01:07:52,000
let them stay. So even in apparently simple problems, you can make mistakes. And that's

692
01:07:53,600 --> 01:07:59,600
what in more serious context, say, AGI safety researchers consider. So now let's go back to

693
01:08:00,240 --> 01:08:05,280
general agents. So assume we want to build an agent, which is generally useful to humans.

694
01:08:05,280 --> 01:08:11,040
Yes, we have a household robot, and it should do all kinds of tasks. So in this case,

695
01:08:11,840 --> 01:08:16,800
the human should give the reward on the fly. I mean, maybe it's pre trained in the factory and

696
01:08:16,800 --> 01:08:20,560
that there's some sort of internal reward for, you know, the battery level or whatever. But

697
01:08:21,600 --> 01:08:25,120
so it, you know, it does the dishes badly, you know, you punish the robot, you does it good,

698
01:08:25,120 --> 01:08:29,040
you reward the robot and then train it to a new task, like a child, right? So

699
01:08:30,000 --> 01:08:35,520
you need the human in the loop. If you want a system, which is useful to the human. And as long

700
01:08:35,520 --> 01:08:42,160
as this agent stays sub human level, that should work reasonably well. And apart from, you know,

701
01:08:42,160 --> 01:08:46,560
these examples, it becomes critical if they become, you know, on a human level, it's like

702
01:08:46,560 --> 01:08:51,440
with children, small children, you have reasonably well under control, they become older. The reward

703
01:08:51,440 --> 01:08:59,760
technique doesn't work so well anymore. So then finally, so this would be agents, which are just,

704
01:08:59,760 --> 01:09:04,880
you could say slaves to the humans. Yeah. So if you are more ambitious and just say we want to

705
01:09:04,880 --> 01:09:10,960
build a new spacious of intelligent beings, we put them on a new planet and we want them to

706
01:09:10,960 --> 01:09:18,080
develop this planet or whatever. So we don't give them any reward. So what could we do? And

707
01:09:18,080 --> 01:09:22,800
you could try to, you know, come up with some reward functions like, you know, it should maintain

708
01:09:22,800 --> 01:09:30,160
itself the robot, it should maybe multiply build more robots, right? And, you know, maybe

709
01:09:31,120 --> 01:09:34,640
for all kinds of things that you find useful, but that's pretty hard, right? You know,

710
01:09:34,640 --> 01:09:38,720
what does self maintenance mean? You know, what does it mean to build a copy? Should it be exact

711
01:09:38,720 --> 01:09:45,120
copy or an approximate copy? And so that's really hard. But Laurent or so, also at DeepMind,

712
01:09:46,000 --> 01:09:52,000
developed a beautiful model. So it just took the ICSE model and coupled the rewards

713
01:09:52,720 --> 01:09:59,040
to information gain. So he said the reward is proportional to how much the agent had

714
01:09:59,120 --> 01:10:03,840
learned about the world. And you can rigorously formally uniquely define that in terms of

715
01:10:03,840 --> 01:10:09,200
our cattle diversions. Okay. So if you put that in, you get a completely autonomous agent.

716
01:10:09,760 --> 01:10:13,440
And actually, interestingly, for this agent, we can prove much stronger result than for the

717
01:10:13,440 --> 01:10:19,040
general agent, which is also nice. And if you let this agent lose, it will be in a sense the

718
01:10:19,040 --> 01:10:24,800
optimal scientist is absolutely curious to learn as much as possible about the world. And of course,

719
01:10:24,800 --> 01:10:28,800
it will also have a lot of instrumental goals, right? In order to learn, it needs to at least

720
01:10:28,800 --> 01:10:33,920
survive, right? That that agent is not good for anything. So it needs to have self preservation.

721
01:10:33,920 --> 01:10:41,040
And if it builds small helpers acquiring more information, it will do that. Yeah, if exploration,

722
01:10:41,040 --> 01:10:46,240
space exploration or whatever is necessary, right, to gathering information and develop it. So it has

723
01:10:46,240 --> 01:10:52,240
a lot of instrumental goals following on this information gain. And this agent is completely

724
01:10:52,240 --> 01:10:58,240
autonomous of us. No rewards necessary anymore. Yeah, of course, you could find a way to gain

725
01:10:58,320 --> 01:11:05,680
the concept of information and get stuck in that library that you mentioned beforehand

726
01:11:05,680 --> 01:11:12,000
with a with a very large number of books. The first agent had this problem. It would get stuck

727
01:11:12,000 --> 01:11:17,760
in front of an old TV screen, which has just said white noise. Yeah, white noise. But the second

728
01:11:17,760 --> 01:11:24,560
version can deal with at least stochasticity. Well, yeah, what about curiosity, this kind of word

729
01:11:25,360 --> 01:11:31,920
curiosity, creativity? Is that kind of the reward function being of getting new information?

730
01:11:31,920 --> 01:11:41,280
Is that similar to idea of kind of injecting exploration for its own sake inside the reward

731
01:11:41,280 --> 01:11:46,240
function? Do you find this at all appealing, interesting? I think that's a nice definition.

732
01:11:46,240 --> 01:11:51,600
Curiosity is the reward. Sorry, curiosity is exploration for its own sake.

733
01:11:54,720 --> 01:12:00,960
Yeah, I would accept that. But most curiosity, while in humans and especially in children,

734
01:12:00,960 --> 01:12:06,320
yeah, is not just for its own sake, but for actually learning about the environment and for

735
01:12:06,320 --> 01:12:14,720
behaving better. So I would, I think most curiosity is tied in the end to what's performing better.

736
01:12:14,800 --> 01:12:21,200
Well, okay, so if intelligent systems need to have this reward function, let me, you're an

737
01:12:21,200 --> 01:12:29,600
intelligent system, currently passing the torrent test quite effectively. What's the reward function

738
01:12:30,800 --> 01:12:36,480
of our human intelligence existence? What's the reward function that Marcus Hutter is operating

739
01:12:36,480 --> 01:12:43,840
under? Okay, to the first question, the biological reward function is to survive and to spread.

740
01:12:44,480 --> 01:12:49,680
And very few humans sort of are able to overcome this biological reward function.

741
01:12:50,880 --> 01:12:57,280
But we live in a very nice world where we have lots of spare time and can still survive and

742
01:12:57,280 --> 01:13:03,280
spread. So we can develop arbitrary other interests, which is quite interesting.

743
01:13:03,280 --> 01:13:09,600
On top of that? On top of that, yeah. But the survival and spreading sort of is, I would say,

744
01:13:10,320 --> 01:13:14,080
the goal or the reward function of humans that the core one.

745
01:13:15,120 --> 01:13:19,040
I like how you avoided answering the second question, which a good intelligence system would.

746
01:13:19,600 --> 01:13:24,160
So my, your own meaning of life and the reward function?

747
01:13:24,160 --> 01:13:29,200
My own meaning of life and reward function is to find an AGI to build it.

748
01:13:31,040 --> 01:13:37,280
Beautifully put. Okay, let's dissect the X even further. So one of the assumptions is kind of

749
01:13:37,280 --> 01:13:45,920
infinity keeps creeping up everywhere, which, what are your thoughts on kind of bounded

750
01:13:45,920 --> 01:13:51,920
rationality and sort of the nature of our existence and intelligence systems is that we're operating

751
01:13:51,920 --> 01:13:58,720
always under constraints, under, you know, limited time, limited resources. How does that, how do

752
01:13:58,720 --> 01:14:05,040
you think about that within the IXE framework, within trying to create an AGI system that operates

753
01:14:05,040 --> 01:14:10,000
under these constraints? Yeah, that is one of the criticisms about IXE that it ignores

754
01:14:10,000 --> 01:14:16,160
computational completely. And some people believe that intelligence is inherently tied to what's

755
01:14:17,040 --> 01:14:21,600
bounded resources. What do you think on this one point? Do you think it's,

756
01:14:22,400 --> 01:14:25,280
do you think the bounded resources are fundamental to intelligence?

757
01:14:27,760 --> 01:14:34,560
I would say that an intelligence notion which ignores computational limits is extremely useful

758
01:14:35,440 --> 01:14:40,560
a good intelligence notion, which includes these resources would be even more useful,

759
01:14:40,560 --> 01:14:47,520
but we don't have that yet. And so look at other fields outside of computer science,

760
01:14:48,400 --> 01:14:54,800
computational aspects never play a fundamental role. You develop biological models for cells,

761
01:14:54,800 --> 01:14:59,120
something in physics, these theories, I mean, become more and more crazy and harder and harder

762
01:14:59,120 --> 01:15:03,520
to compute. Well, in the end, of course, we need to do something with this model, but there's more

763
01:15:03,600 --> 01:15:10,400
nuisance than a feature. And I'm sometimes wondering if artificial intelligence would not

764
01:15:10,400 --> 01:15:15,040
sit in a computer science department, but in a philosophy department, then this computational

765
01:15:15,040 --> 01:15:20,000
focus would be probably significantly less. I mean, think about the induction problem is more

766
01:15:20,000 --> 01:15:25,040
in the philosophy department. There's virtually no paper who cares about, you know, how long it

767
01:15:25,040 --> 01:15:30,480
takes to compute the answer that is completely secondary. Of course, once we have figured out

768
01:15:30,480 --> 01:15:36,080
the first problem, so intelligence without computational resources, then

769
01:15:37,360 --> 01:15:42,320
the next and very good question is, could we improve it by including computational resources,

770
01:15:42,320 --> 01:15:47,600
but nobody was able to do that so far in an even halfway satisfactory manner?

771
01:15:49,040 --> 01:15:53,680
I like that that's in the long run, the right department to belong to is philosophy.

772
01:15:54,640 --> 01:16:01,840
That's actually quite a deep idea of or even to at least to think about big picture

773
01:16:01,840 --> 01:16:07,440
philosophical questions, big picture questions, even in the computer science department. But

774
01:16:07,440 --> 01:16:13,840
you've mentioned approximation, sort of, there's a lot of infinity, a lot of huge resources needed.

775
01:16:13,840 --> 01:16:18,960
Are there approximations to IHC that within the IHC framework that are useful?

776
01:16:19,680 --> 01:16:27,840
Yeah, we have to develop a couple of approximations. And what we do there is that the

777
01:16:27,840 --> 01:16:33,520
Solomov induction part, which was, you know, find the shortest program describing your data,

778
01:16:33,520 --> 01:16:38,880
which just replaces by standard data compressors, right? And the better compressors get,

779
01:16:38,880 --> 01:16:43,520
you know, the better this part will become. We focus on a particular compressor called

780
01:16:43,520 --> 01:16:49,600
Context Rewaiting, which is pretty amazing, not so well known. It has beautiful theoretical

781
01:16:49,600 --> 01:16:53,840
properties also works reasonably well in practice. So we use that for the approximation

782
01:16:53,840 --> 01:17:00,560
of the induction and the learning and the prediction part. And for the planning part,

783
01:17:01,600 --> 01:17:08,000
we essentially just took the ideas from a computer go from 2006. It was Java,

784
01:17:08,000 --> 01:17:15,760
Cyprus, Bari, also now a DeepMind, who developed the so called UCT algorithm, upper confidence

785
01:17:15,760 --> 01:17:20,400
bound for trees algorithm, on top of the Monte Carlo tree search. So we approximate this planning

786
01:17:20,400 --> 01:17:32,240
part by sampling. And it's successful on some small toy problems. We don't want to lose the

787
01:17:32,240 --> 01:17:35,840
generality, right? And that's sort of the handicap, right? If you want to be general,

788
01:17:36,080 --> 01:17:41,840
you have to give up something. So but this single agent was able to play, you know, small games

789
01:17:41,840 --> 01:17:51,920
like Coon poker and tic-tac-toe and, and even Pac-Man. And the same architecture, no change.

790
01:17:51,920 --> 01:17:57,520
The agent doesn't know the rules of the game, really nothing at all by self or by a player

791
01:17:57,520 --> 01:18:03,680
with these environments. So you're gonna Schmidt, who were proposed something called

792
01:18:03,680 --> 01:18:08,160
Ghetto Machines, which is a self improving program that rewrites its own code.

793
01:18:10,800 --> 01:18:15,040
Sort of mathematically or philosophically, what's the relationship in your eyes,

794
01:18:15,040 --> 01:18:18,320
if you're familiar with it between AXI and the Ghetto Machines?

795
01:18:18,320 --> 01:18:21,120
Yeah, familiar with it. He developed it while I was in his lab.

796
01:18:22,160 --> 01:18:30,000
Yeah. So the Ghetto Machine, explain briefly. You give it a task. It could be a simple task as,

797
01:18:30,000 --> 01:18:33,920
you know, finding prime factors in numbers, right? You can formally write it down. There's

798
01:18:33,920 --> 01:18:39,200
a very slow algorithm to do that. Just all try all the factors. Yeah. Or play chess, right?

799
01:18:39,200 --> 01:18:43,920
Optimally, you write the algorithm to minimax to the end of the game. So you write down what the

800
01:18:43,920 --> 01:18:49,840
Ghetto Machine should do. Then it will take part of its resources to run this program.

801
01:18:50,640 --> 01:18:56,880
And other part of the sources to improve this program. And when it finds an improved version,

802
01:18:56,880 --> 01:19:04,000
which provably computes the same answer. So that's the key part. It needs to prove by itself

803
01:19:04,000 --> 01:19:10,000
that this change of program still satisfies the original specification. And if it does so,

804
01:19:10,000 --> 01:19:14,160
then it replaces the original program by the improved program. And by definition,

805
01:19:14,160 --> 01:19:19,120
it does the same job, but just faster. Okay. And then, you know, it proves over it and over it.

806
01:19:19,120 --> 01:19:26,640
And it's developed in a way that all parts of this Ghetto Machine can self improve.

807
01:19:26,640 --> 01:19:33,840
But it stays provably consistent with the original specification. So from this perspective,

808
01:19:33,840 --> 01:19:39,440
it has nothing to do with IxE. But if you would now put IxE as the starting axioms in,

809
01:19:40,560 --> 01:19:46,320
it would run IxE. But, you know, that takes forever. But then if it finds a provable

810
01:19:47,200 --> 01:19:52,160
speed up of IxE, it would replace it by this and this and this and maybe eventually it comes

811
01:19:52,160 --> 01:19:58,960
up with a model which is still the IxE model. It cannot be, I mean, just for the knowledgeable

812
01:19:58,960 --> 01:20:04,400
reader, IxE is incomputable. And that can prove that therefore there cannot be a computable

813
01:20:05,360 --> 01:20:11,120
exact algorithm of computers. There needs to be some approximations. And this is not dealt

814
01:20:11,120 --> 01:20:14,400
with the Ghetto Machine. So you have to do something about it. But there's the IxE TL model,

815
01:20:14,400 --> 01:20:19,120
which is finally computable, which we could put in. Which part of IxE is non computable?

816
01:20:19,120 --> 01:20:22,160
The Solomonov induction part. The induction. Okay, so.

817
01:20:22,160 --> 01:20:29,120
But there's ways of getting computable approximations of the IxE model. So then it's at least

818
01:20:29,120 --> 01:20:34,800
computable. It is still way beyond any resources anybody will ever have. But then the Ghetto Machine

819
01:20:34,800 --> 01:20:41,280
could sort of improve it further and further in an exact way. So is this theoretically possible that

820
01:20:42,160 --> 01:20:50,880
the Ghetto Machine process could improve? Isn't IxE already optimal?

821
01:20:51,760 --> 01:21:00,640
It is optimal in terms of the reward collected over its interaction cycles. But it takes infinite

822
01:21:00,640 --> 01:21:08,080
time to produce one action. And the world continues whether you want it or not. So the model is

823
01:21:08,080 --> 01:21:12,800
assuming it had an oracle, which solved this problem, and then in the next 100 milliseconds

824
01:21:12,800 --> 01:21:19,280
or the reaction time you need gives the answer, then IxE is optimal. It's optimal in sense of

825
01:21:19,280 --> 01:21:25,280
data, also from learning efficiency and data efficiency, but not in terms of computation

826
01:21:25,280 --> 01:21:30,160
time. And then the Ghetto Machine in theory, but probably not provably could make it go faster.

827
01:21:30,880 --> 01:21:39,280
Yes. Okay. Interesting. Those two components are super interesting. The perfect intelligence

828
01:21:39,280 --> 01:21:47,920
combined with self-improvement. Sort of provable self-improvement in sense you're always getting

829
01:21:47,920 --> 01:21:53,760
the correct answer and you're improving. Beautiful ideas. Okay, so you've also mentioned that

830
01:21:54,480 --> 01:22:01,360
different kinds of things in the chase of solving this reward, sort of optimizing for the goal,

831
01:22:03,040 --> 01:22:08,560
interesting human things could emerge. So is there a place for consciousness within IxE?

832
01:22:10,800 --> 01:22:17,360
Where does, maybe you can comment, because I suppose we humans are just another instantiation

833
01:22:17,360 --> 01:22:23,360
by IxE agents and we seem to have consciousness. You say humans are an instantiation of an IxE agent?

834
01:22:23,360 --> 01:22:28,240
Yes. Oh, that would be amazing. But I think that's not true even for the smartest and most

835
01:22:28,240 --> 01:22:34,240
rational humans. I think maybe we are very crude approximations. Interesting. I mean, I tend to

836
01:22:34,240 --> 01:22:43,760
believe, again, I'm Russian, so I tend to believe our flaws are part of the optimal. So we tend to

837
01:22:43,760 --> 01:22:49,920
laugh off and criticize our flaws and I tend to think that that's actually close to an optimal

838
01:22:49,920 --> 01:22:54,960
behavior. Well, some flaws, if you think more carefully about it, are actually not flaws here,

839
01:22:54,960 --> 01:23:01,840
but I think there are still enough flaws. I don't know. It's unclear. As a student of history,

840
01:23:01,840 --> 01:23:08,880
I think all the suffering that we've endured as a civilization, it's possible that that's the

841
01:23:08,880 --> 01:23:15,680
optimal amount of suffering we need to endure to minimize long-term suffering. That's your Russian

842
01:23:15,760 --> 01:23:21,760
background. That's the Russian. Whether humans are or not instantiations of an IxE agent,

843
01:23:21,760 --> 01:23:27,440
do you think there's consciousness is something that could emerge in a computational form of

844
01:23:27,440 --> 01:23:32,720
framework like IxE? Let me also ask you a question. Do you think I'm conscious?

845
01:23:36,720 --> 01:23:44,160
That's a good question. That tie is confusing me, but I think so.

846
01:23:44,240 --> 01:23:46,880
You think that makes me unconscious because it strangles me?

847
01:23:47,520 --> 01:23:51,280
If an agent were to solve the imitation game posed by Turing, I think that would be

848
01:23:51,280 --> 01:23:57,440
dressed similarly to you. Because there's a kind of flamboyant, interesting,

849
01:23:58,960 --> 01:24:05,440
complex behavior pattern that sells that you're human and you're conscious. But why do you ask?

850
01:24:06,080 --> 01:24:07,440
Was it a yes or was it a no?

851
01:24:07,840 --> 01:24:12,000
Yes, I think you're conscious, yes.

852
01:24:12,640 --> 01:24:19,600
So, and you explain somehow why, but you infer that from my behavior. You can never be sure

853
01:24:19,600 --> 01:24:26,720
about that. And I think the same thing will happen with any intelligent agent we develop

854
01:24:26,720 --> 01:24:33,200
if it behaves in a way sufficiently close to humans. Or maybe if not humans, maybe a dog

855
01:24:33,200 --> 01:24:39,920
is also sometimes a little bit self-conscious. So, if it behaves in a way where we attribute

856
01:24:39,920 --> 01:24:44,320
typically consciousness, we would attribute consciousness to these intelligent systems

857
01:24:44,320 --> 01:24:49,840
and IxE probably in particular. That, of course, doesn't answer the question whether it's really

858
01:24:49,840 --> 01:24:55,920
conscious. And that's the big hard problem of consciousness. Maybe I'm a zombie. I mean,

859
01:24:55,920 --> 01:24:58,400
not the movie zombie, but the philosophical zombie.

860
01:24:59,360 --> 01:25:06,480
It's to you, the display of consciousness close enough to consciousness from a perspective of AGI

861
01:25:06,480 --> 01:25:11,200
that the distinction of the heart problem of consciousness is not an interesting one.

862
01:25:11,200 --> 01:25:14,720
I think we don't have to worry about the consciousness problem, especially the heart

863
01:25:14,720 --> 01:25:22,480
problem for developing AGI. I think we progress. At some point, we have solved all the technical

864
01:25:22,560 --> 01:25:26,720
problems and this system will behave intelligent and then super intelligent and

865
01:25:27,760 --> 01:25:33,040
this consciousness will emerge. I mean, definitely it will display behavior which we

866
01:25:33,040 --> 01:25:39,040
will interpret as conscious. And then it's a philosophical question. Did this consciousness

867
01:25:39,040 --> 01:25:44,960
really emerge? Or is it a zombie which just fakes everything? We still don't have to figure that

868
01:25:44,960 --> 01:25:49,360
out. Although it may be interesting, at least from a philosophical point of view. It's very

869
01:25:49,360 --> 01:25:53,840
interesting, but it may also be sort of practically interesting. You know, there's some people

870
01:25:53,840 --> 01:25:57,920
saying, if it's just faking consciousness and feelings, then we don't need to be concerned

871
01:25:57,920 --> 01:26:02,720
about rights. But if it's real conscious and has feelings, then we need to be concerned.

872
01:26:05,840 --> 01:26:11,920
I can't wait till the day where AI systems exhibit consciousness because it'll truly

873
01:26:11,920 --> 01:26:15,600
be some of the hardest ethical questions of what we do with that.

874
01:26:15,600 --> 01:26:22,560
It is rather easy to build systems which people ascribe consciousness. And I give you an analogy.

875
01:26:22,560 --> 01:26:26,080
I mean, remember, maybe it was before you were born, the Tamagotchi.

876
01:26:28,640 --> 01:26:29,520
How dare you, sir.

877
01:26:30,880 --> 01:26:33,200
Why that's so... You're young, right?

878
01:26:33,200 --> 01:26:37,600
Yes, it's good to think. Thank you. Thank you very much. But I was also in the Soviet Union. We

879
01:26:37,600 --> 01:26:41,120
didn't have... We didn't have any of those fun things.

880
01:26:41,120 --> 01:26:43,840
But you have heard about this Tamagotchi, which was, you know, really,

881
01:26:43,920 --> 01:26:50,720
really primitive. Actually, for the time it was... And you could raise this. And kids got so

882
01:26:50,720 --> 01:26:57,360
attached to it and didn't want to let it die. And probably if we would have asked the children,

883
01:26:57,360 --> 01:27:00,320
do you think this Tamagotchi is conscious? They would have said yes.

884
01:27:01,520 --> 01:27:06,800
I think that's kind of a beautiful thing, actually, because that consciousness, ascribing

885
01:27:06,800 --> 01:27:13,120
consciousness seems to create a deeper connection, which is a powerful thing. But we have to be

886
01:27:13,120 --> 01:27:18,880
careful on the ethics side of that. Well, let me ask about the AGI community broadly. You kind of

887
01:27:18,880 --> 01:27:26,320
represent some of the most serious work on AGI, at least earlier. And DeepMind represents

888
01:27:27,120 --> 01:27:34,000
serious work on AGI these days. But why, in your sense, is the AGI community so small,

889
01:27:34,000 --> 01:27:40,240
or has been so small, until maybe DeepMind came along? Like, why aren't more people

890
01:27:40,240 --> 01:27:47,120
seriously working on human level and superhuman level intelligence from a formal perspective?

891
01:27:48,160 --> 01:27:54,080
Okay, from a formal perspective, that's sort of, you know, an extra point. So I think there are

892
01:27:54,080 --> 01:27:58,400
a couple of reasons. I mean, AI came in waves, right? You know, AI winters and AI summers,

893
01:27:58,400 --> 01:28:07,200
and then there were big promises, which were not fulfilled. And people got disappointed. But

894
01:28:08,080 --> 01:28:14,240
narrow AI, solving particular problems, which seemed to require intelligence, was

895
01:28:15,200 --> 01:28:20,480
always to some extent successful, and there were improvements, small steps. And if you build

896
01:28:20,480 --> 01:28:26,480
something which is, you know, useful for society or industrial useful, then there's a lot of funding.

897
01:28:26,480 --> 01:28:34,480
So I guess it was in parts the money, which drives people to develop specific system solving

898
01:28:34,480 --> 01:28:40,640
specific tasks. But you would think that, you know, at least in university, you should be able to do

899
01:28:41,280 --> 01:28:46,800
ivory tower research. And that was probably better a long time ago. But even nowadays,

900
01:28:46,800 --> 01:28:52,960
there's quite some pressure of doing applied research or translational research. And, you

901
01:28:52,960 --> 01:29:00,480
know, it's harder to get grants as a theorist. So that also drives people away. It's maybe also

902
01:29:00,480 --> 01:29:04,960
harder, attacking the general intelligence problem. So I think enough people, I mean,

903
01:29:04,960 --> 01:29:11,920
maybe a small number, we're still interested in, in formalizing intelligence and, and thinking of

904
01:29:11,920 --> 01:29:19,200
general intelligence. But, you know, not much came up, right? Or not not much great stuff came up.

905
01:29:19,760 --> 01:29:26,000
So what do you think we talked about the formal big light at the end of the tunnel,

906
01:29:26,000 --> 01:29:29,440
but from the engineering perspective, what do you think it takes to build an AI system?

907
01:29:30,240 --> 01:29:35,680
Is that, and I don't know if that's a stupid question or a distinct question from everything

908
01:29:35,680 --> 01:29:40,880
we've been talking about AIXE. But what do you see as the steps that are necessary to take

909
01:29:40,880 --> 01:29:46,160
to start to try to build something? So you want a blueprint now, and then you go off and do it?

910
01:29:46,160 --> 01:29:50,320
That's the whole point of this conversation, trying to squeeze that in there. Now, is there,

911
01:29:50,320 --> 01:29:55,360
I mean, what's your intuition? Is it is in the robotic space or something that has a body and

912
01:29:55,360 --> 01:29:59,920
tries to explore the world? Is in the reinforcement learning space, like the efforts of Alpha

913
01:29:59,920 --> 01:30:05,360
Zero and Alpha Star, they're kind of exploring how you can solve it through in the, in the simulation

914
01:30:05,360 --> 01:30:11,760
in the gaming world. Is there stuff in sort of the, all the transformer work in natural

915
01:30:11,760 --> 01:30:16,640
English processing, sort of maybe attacking the open domain dialogue? Like what, what,

916
01:30:16,640 --> 01:30:24,800
where do you see the promising pathways? Let me pick the embodiment maybe. So

917
01:30:25,600 --> 01:30:37,600
embodiment is important, yes and no. I don't believe that we need a physical robot

918
01:30:38,560 --> 01:30:45,440
walking or rolling around interacting with the real world in order to achieve AGI. And

919
01:30:47,680 --> 01:30:52,800
I think it's more of a distraction probably than helpful. It's sort of confusing the body

920
01:30:52,800 --> 01:30:59,440
with the mind. For industrial applications or near term applications, of course, we need

921
01:30:59,440 --> 01:31:06,400
robots for all kinds of things, but for solving the big problem, at least at this stage, I think

922
01:31:06,400 --> 01:31:13,520
it's not necessary. But the answer is also yes, that I think the most promising approach is that

923
01:31:13,520 --> 01:31:20,000
you have an agent, and that can be a virtual agent in a computer interacting with an environment,

924
01:31:20,080 --> 01:31:23,840
possibly, you know, a 3D simulated environment like in many computer games.

925
01:31:25,280 --> 01:31:33,040
And, and you train and learn the agent. Even if you don't intend to later put it sort of, you know,

926
01:31:33,040 --> 01:31:38,480
this algorithm in a, in a robot brain and leave it forever in the virtual reality,

927
01:31:38,480 --> 01:31:43,440
getting experience in a, although it's just simulated 3D world,

928
01:31:44,240 --> 01:31:53,520
is possibly, and as I possibly important to understand things on a similar level as humans do,

929
01:31:55,040 --> 01:31:59,920
especially if the agent or primarily if the agent wants, needs to interact with the humans,

930
01:31:59,920 --> 01:32:04,160
right? You know, if you talk about objects on top of each other in space and flying and cars and

931
01:32:04,160 --> 01:32:10,800
so on, and the agent has no experience with even virtual 3D worlds, it's probably hard to grasp.

932
01:32:11,120 --> 01:32:17,840
So if we develop an abstract agent, say we take the mathematical path, and we just want to build

933
01:32:17,840 --> 01:32:22,400
an agent which can prove theorems and becomes a better and better mathematician, then this agent

934
01:32:22,400 --> 01:32:27,920
needs to be able to reason in very abstract spaces, and then maybe sort of putting it into

935
01:32:27,920 --> 01:32:33,280
3D environments, simulated world is even harmful, it should sort of, you put it in, I don't know,

936
01:32:33,280 --> 01:32:38,400
an environment which it creates itself or so. It seems like you have an interesting,

937
01:32:38,400 --> 01:32:43,680
rich complex trajectory through life in terms of your journey of ideas. So it's interesting to

938
01:32:43,680 --> 01:32:52,640
ask what books, technical fiction, philosophical books, ideas, people had a transformative effect.

939
01:32:52,640 --> 01:32:57,920
Books are most interesting because maybe people could also read those books and see if they could

940
01:32:57,920 --> 01:33:05,440
be inspired as well. Yeah, luckily I asked books and not singular book, it's very hard and I try

941
01:33:05,440 --> 01:33:15,920
to pin down one book, and I can do that at the end. So the books which were most transformative

942
01:33:15,920 --> 01:33:26,320
for me or which I can most highly recommend to people interested in AI, I would always start

943
01:33:26,320 --> 01:33:33,600
with Russell and Norbic, Artificial Intelligence and Modern Approach, that's the AI Bible, it's

944
01:33:33,600 --> 01:33:40,400
an amazing book, it's very broad, it covers all approaches to AI and even if you focus on one

945
01:33:40,400 --> 01:33:44,560
approach, I think that is the minimum you should know about the other approaches out there,

946
01:33:44,560 --> 01:33:48,320
so that should be your first book. Fourth edition should be coming out soon.

947
01:33:48,320 --> 01:33:54,000
Oh, okay, interesting. There's a deep learning chapter now as there must be, written by Ian

948
01:33:54,000 --> 01:34:00,880
Goodfellow, okay. And then the next book I would recommend, The Reinforcement Learning Book by

949
01:34:00,880 --> 01:34:08,960
Sutton and Bartow. There's a beautiful book, if there's any problem with the book, it makes RL

950
01:34:10,400 --> 01:34:16,640
feel and look much easier than it actually is. It's very gentle book, it's very nice to read the

951
01:34:16,640 --> 01:34:23,280
exercises, you can very quickly get some RL systems to run, very toy problems, but it's a lot of fun

952
01:34:23,280 --> 01:34:30,800
and in a couple of days you feel you know what RL is about, but it's much harder than

953
01:34:30,800 --> 01:34:40,320
the book. Come on now, it's an awesome book. Yeah, no, no, it is, yeah. And maybe, I mean,

954
01:34:40,320 --> 01:34:43,680
there's so many books out there, if you like the information theoretic approach, then there's

955
01:34:43,680 --> 01:34:50,720
Kolmogorff Complexity by Aline Vitani, but probably, you know, some short article is enough,

956
01:34:50,720 --> 01:34:57,920
you don't need to read the whole book, but it's a great book. And if you have to mention one

957
01:34:57,920 --> 01:35:04,240
all-time favorite book, it's a different flavor, that's a book which is used in the international

958
01:35:04,240 --> 01:35:10,880
baccalaureate for high school students in several countries. That's from Nikolas Altjen,

959
01:35:10,880 --> 01:35:17,520
Theory of Knowledge, second edition, or first, not the third, please. The third one they put,

960
01:35:17,520 --> 01:35:26,480
they took out all the fun. Okay, so this asks all the interesting, or to me, interesting

961
01:35:26,480 --> 01:35:30,000
philosophical questions about how we acquire knowledge from all perspectives, you know,

962
01:35:30,000 --> 01:35:36,800
from math, from art, from physics, and ask how can we know anything? And the book is called

963
01:35:36,800 --> 01:35:40,800
Theory of Knowledge. From which, is this almost like a philosophical exploration of

964
01:35:41,680 --> 01:35:45,040
how we get knowledge from anything? Yes, yeah, I mean, can religion tell us, you know,

965
01:35:45,040 --> 01:35:48,800
about something about the world? Can science tell us something about the world? Can mathematics,

966
01:35:48,800 --> 01:35:55,120
or is it just playing with symbols? And you know, it's open-ended questions, and I mean,

967
01:35:55,120 --> 01:35:58,960
it's for high school students, so they have the resources from Hitchhiker's Guide to the Galaxy

968
01:35:58,960 --> 01:36:05,360
and from Star Wars and the Chicken Cross the Road, yeah. And it's fun to read, but it's also quite

969
01:36:05,360 --> 01:36:12,720
deep. If you could live one day of your life over again, because it made you truly happy,

970
01:36:12,720 --> 01:36:18,080
or maybe like we said with the books, it was truly transformative, what day, what moment would you

971
01:36:18,080 --> 01:36:23,840
choose? Does something pop into your mind? Does it need to be a day in the past, or can it be a

972
01:36:23,840 --> 01:36:30,720
day in the future? Well, space-time is an emerging phenomena, so it's all the same anyway. Okay.

973
01:36:31,920 --> 01:36:37,600
Okay, from the past. You're really going to say from the future, I love it. No, I will tell you

974
01:36:37,600 --> 01:36:42,960
from the future. Okay, from the past. So from the past, I would say when I discovered my AXI model,

975
01:36:43,680 --> 01:36:48,000
I mean, it was not in one day, but it was one moment where I realized

976
01:36:48,720 --> 01:36:55,440
comagor of complexity. I didn't even know that it existed, but I discovered sort of this compression

977
01:36:55,440 --> 01:37:00,640
idea myself, but immediately I knew I can't be the first one, but I had this idea. And then I

978
01:37:00,640 --> 01:37:05,680
knew about sequential decisionary, and I knew if I put it together, this is the right thing.

979
01:37:06,320 --> 01:37:12,320
And yeah, still when I think back about this moment, I'm super excited about it.

980
01:37:12,400 --> 01:37:17,680
Was there any more details in context that moment? Did an apple fall in your head?

981
01:37:20,000 --> 01:37:26,080
So like if you look at Ian Goodfellow talking about gans, there was beer involved. Is there

982
01:37:27,200 --> 01:37:33,120
some more context of what sparked your thought, or was it just? No, it was much more mundane. So I

983
01:37:33,120 --> 01:37:37,360
worked in this company. So in this sense, the four and a half years was not completely wasted.

984
01:37:37,600 --> 01:37:47,360
So and I worked on an image interpolation problem. And I developed quite neat new

985
01:37:47,360 --> 01:37:52,160
interpolation techniques, and they got patented. And then, you know, which happens quite often,

986
01:37:52,160 --> 01:37:55,840
I got sort of overboard and thought about, you know, yeah, that's pretty good, but it's not the

987
01:37:55,840 --> 01:38:01,200
best. So what is the best possible way of doing interpolation? And then I thought, yeah, you

988
01:38:01,200 --> 01:38:06,480
want the simplest picture, which is if you coarse-grain it, recovers your original picture.

989
01:38:06,480 --> 01:38:12,960
And then I thought about the simplicity concept more in quantitative terms. And yeah, then

990
01:38:12,960 --> 01:38:18,960
everything developed. And somehow the full beautiful mix of also being a physicist and

991
01:38:18,960 --> 01:38:25,120
thinking about the big picture of it, then led you to probably beg with ice. Yeah. So as a physicist,

992
01:38:25,120 --> 01:38:29,280
I was probably trained not to always think in computational terms, you know, just ignore that

993
01:38:29,280 --> 01:38:33,120
and think about the fundamental properties which you want to have.

994
01:38:33,920 --> 01:38:39,040
So what about if you could really live one day in the future? What would that be?

995
01:38:39,760 --> 01:38:45,040
When I solve the AGI problem. In practice, in practice. So in theory,

996
01:38:45,040 --> 01:38:50,000
I have solved it with the IHC model, but in practice. And then I asked the first question.

997
01:38:50,640 --> 01:38:54,160
What would be the first question? What's the meaning of life?

998
01:38:54,400 --> 01:38:59,600
I don't think there's a better way to end it. Thank you so much for talking today. It's

999
01:38:59,600 --> 01:39:03,360
a huge honor to finally meet you. Yeah, thank you too. It was a pleasure of mine side too.

1000
01:39:04,560 --> 01:39:08,160
Thanks for listening to this conversation with Marcus Hutter. And thank you to our

1001
01:39:08,160 --> 01:39:14,560
presenting sponsor, Cash App. Download it, use code LEX Podcast. You'll get $10 and $10 will go

1002
01:39:14,560 --> 01:39:19,440
to first, an organization that inspires and educates young minds to become science and

1003
01:39:19,440 --> 01:39:24,960
technology innovators of tomorrow. If you enjoy this podcast, subscribe on YouTube,

1004
01:39:24,960 --> 01:39:30,320
give it five stars on Apple Podcasts, support on Patreon, or simply connect with me on Twitter

1005
01:39:30,320 --> 01:39:37,120
at Lex Friedman. And now let me leave you with some words of wisdom from Albert Einstein.

1006
01:39:37,840 --> 01:39:45,600
The measure of intelligence is the ability to change. Thank you for listening and hope to see you

1007
01:39:45,600 --> 01:39:46,880
next time.

