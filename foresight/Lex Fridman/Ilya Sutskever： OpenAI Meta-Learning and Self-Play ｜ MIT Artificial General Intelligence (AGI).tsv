start	end	text
0	4040	Welcome back to 6SES 099, Artificial General Intelligence.
4040	12340	Today we have Ilya Setskever, co-founder and research director
12340	13440	of OpenAI.
13440	17320	He started in the ML group in Toronto with Geoffrey Hinton,
17320	19040	then at Stanford with Andrew Ang,
19040	21480	co-founded DNN Research for three years
21480	23520	as a research scientist at Google Brain,
23520	26280	and finally co-founded OpenAI.
26280	29520	Citations aren't everything, but they
29520	31160	do indicate impact.
31160	35040	And his work, recent work, in the past five years,
35040	39360	has been cited over 46,000 times.
39360	43320	He has been the key creative intellect and driver
43320	45400	behind some of the biggest breakthrough ideas
45400	49480	in deep learning and artificial intelligence ever.
49480	51880	So please welcome Ilya.
51880	62280	All right, thanks for the introduction, Lex.
62280	64560	All right, thanks for coming to my talk.
64560	66160	I will tell you about some work we've
66160	70280	done over the past year on meta-learning and self-play
70280	71920	at OpenAI.
71920	75960	And before I dive into some of the more technical details
75960	79520	of the work, I want to spend a little bit of time
79520	84440	talking about deep learning and why it works at all
84440	86440	in the first place, which I think
86440	89080	it's actually not a self-evident thing
89080	90640	that they should work.
90640	96720	One fact, it's actually a fact, some mathematical theorem
96720	102240	that you can prove, is that if you could find the shortest
102240	107320	program that does very well on your data,
107320	110560	then you will achieve the best generalization possible.
110560	111960	With a little bit of modification,
111960	114440	you can turn it into a precise theorem.
114440	118320	And on a very intuitive level, it's
118320	120920	easy to see why it should be the case.
120920	125040	If you have some data and you're able to find a shorter
125040	128000	program which generates this data,
128000	130920	then you've essentially extracted all conceivable
130920	133800	regularity from this data into your program.
133800	135640	And then you can use this object to make the best
135640	138080	predictions possible.
138080	141880	If you have data which is so complex
141880	146160	that there is no way to express it as a shorter program,
146160	148200	then it means that your data is totally random.
148200	152400	There is no way to extract any regularity from it whatsoever.
152400	155640	Now, there is little known mathematical theory behind this.
155640	157200	And the proofs of these statements
157200	159160	are actually not even that hard.
159160	162640	But the one minor slight disappointment
162640	164880	is that it's actually not possible, at least given
164880	167000	today's tools and understanding,
167000	172480	to find the best short program that explains or generates
172480	175800	or solves your problem given your data.
175800	179480	This problem is computationally intractable.
179480	184040	The space of all programs is a very nasty space.
184040	186760	Small changes to your program result in massive changes
186760	188840	to the behavior of the program, as it should be.
188840	190120	It makes sense.
190120	191320	You have a loop.
191320	193440	It changed the inside of the loop.
193440	195840	Of course, you get something totally different.
195840	197960	So the space of programs is so hard,
197960	199360	at least given what we know today,
199360	204480	search there seems to be completely off the table.
204480	208920	Well, if we give up on short programs,
208920	211960	what about small circuits?
211960	214840	Well, it turns out that we are lucky.
214840	217560	It turns out that when it comes to small circuits,
217560	220360	you can just find the best small circuit that
220360	223080	solves your problem using back propagation.
223120	229600	And this is the miraculous fact on which the rest of AI stands.
229600	232360	It is the fact that when you have a circuit
232360	234320	and you impose constraints on your circuits,
234320	238480	on your circuit using data, you can find a way
238480	241120	to satisfy these constraints, these constraints
241120	246000	using backprop, by iteratively making small changes
246000	248240	to the weights of your neural network
248240	253120	until its predictions satisfy the data.
253120	256880	What this means is that the computational problem that's
256880	259680	solved by back propagation is extremely profound.
259680	261400	It is circuit search.
261400	264800	Now, we know that you can solve it always,
264800	266680	but you can solve it sometimes.
266680	270480	And you can solve it at those times
270480	272240	where we have a practical data set.
272240	275120	It is easy to design artificial data sets for which you cannot
275120	276720	find the best neural network.
276720	280040	But in practice, that seems to be not a problem.
280040	282240	You can think of training a neural network
282240	285840	as solving a neural equation in many cases
285840	289880	where you have a large number of equation terms
289880	292760	like this, f of x i theta equals y i.
292760	294760	So you've got your parameters, and they represent
294760	297240	all your degrees of freedom.
297240	301160	And you use gradient descent to push the information
301160	305680	from these equations into the parameters to satisfy them all.
305680	308680	And you can see that a neural network, let's say one
308680	314080	with 50 layers, is basically a parallel computer that
314080	317440	is given 50 time steps to run.
317440	319800	And you can do quite a lot with 50 time steps
319800	323960	of a very, very powerful massively parallel computer.
323960	329400	So for example, I think it is not widely known
329440	336520	that you can learn to sort n n bit numbers using
336520	340600	a modestly sized neural network with just two hidden layers,
340600	342720	which is not bad.
342720	345680	It's not self-evident, especially since we've
345680	349680	been taught that sorting requires log n parallel steps.
349680	352720	With a neural network, you can sort successfully
352720	354840	using only two parallel steps.
354840	358440	So there's something slightly obvious going on.
358440	361200	Now these are parallel steps of threshold neurons.
361200	363160	So they're doing a little bit more work.
363160	364480	That's the answer to the mystery.
364480	365920	But if you've got 50 such layers,
365920	368520	you can do quite a bit of logic, quite a bit of reasoning,
368520	370320	all inside the neural network.
370320	372520	And that's why it works.
372520	377000	Given the data, we are able to find the best neural network.
377000	378760	And because the neural network is deep,
378760	382960	because it can run computation inside of its layers,
382960	385840	the best neural network is worth finding.
385840	387240	Because that's really what you need.
387240	389360	You need something.
389360	393480	You need a model class, which is worth optimizing.
393480	395720	But it also needs to be optimizable.
395720	399400	And deep neural networks satisfy both of these constraints.
399400	400800	And this is why everything works.
400800	405440	This is the basis on which everything else resides.
405440	408160	Now I want to talk a little bit about reinforcement learning.
408160	410840	So reinforcement learning is a framework.
410840	415520	It's a framework of evaluating agents in their ability
415520	419360	to achieve goals in complicated stochastic environments.
419360	421520	You've got an agent, which is plugged into an environment,
421520	424120	as shown in the figure right here.
424120	429760	And for any given agent, you can simply run it many times
429760	432800	and compute its average reward.
432800	435160	Now, the thing that's interesting about the reinforcement
435160	439960	learning framework is that there exist interesting,
439960	442960	useful reinforcement learning algorithms.
442960	445400	The framework existed for a long time.
445400	447480	It became interesting once we realized
447480	449040	that good algorithms exist.
449040	451000	Now, these are not perfect algorithms,
451000	455080	but they are good enough to do interesting things.
455080	458280	And all you want, the mathematical problem,
458280	463320	is one where you need to maximize the expected reward.
463320	466960	Now, one important way in which the reinforcement learning
466960	469160	framework is not quite complete is
469160	472440	that it assumes that the reward is given by the environment.
472440	474280	You see this picture.
474280	477400	The agent sends an action, while the reward sends it
477400	480000	an observation, and both the observation
480000	481520	and the reward backwards.
481520	484400	That's what the environment communicates back.
484400	487480	The way in which this is not the case in the real world
487480	494200	is that we figure out what the reward is from the observation.
494200	496200	We reward ourselves.
496200	496840	We are not told.
496840	500240	The environment doesn't say, hey, here's some negative reward.
500240	502720	It's our interpretation of our senses
502720	505960	that lets us determine what the reward is.
505960	508280	And there is only one real true reward in life,
508280	511120	and this is existence or nonexistence.
511120	514040	And everything else is a corollary of that.
514040	516760	So, well, what should the agent be?
516760	518000	You already know the answer.
518000	520200	It should be a neural network.
520200	522200	Because whenever you want to do something,
522200	524080	the answer is going to be a neural network,
524080	527320	and you want the agent to map observations to actions.
527320	529800	So you let it be parametrized with a neural net,
529800	531640	and you apply a learning algorithm.
531680	534680	So I want to explain to you how reinforcement learning works.
534680	536440	This is model-free reinforcement learning.
536440	537760	The reinforcement learning is actually
537760	539040	being used in practice everywhere.
541920	544320	But it's also deeply, it's very robust.
544320	545760	It's very simple.
545760	547680	It's also not very efficient.
547680	549040	So the way it works is the following.
549040	551520	This is literally the one-sentence description
551520	553760	of what happens.
553760	558040	In short, try something new.
558040	561160	Add randomness to your actions.
561160	565040	And compare the result to your expectation.
565040	568240	If the result surprises you, if you
568240	571560	find that the result exceeded your expectation,
571560	573880	then change your parameters to take those actions
573880	575680	in the future.
575680	576640	That's it.
576640	578960	This is the full idea of reinforcement learning.
578960	580160	Try it out.
580160	581280	See if you like it.
581280	585080	And if you do, do more of that in the future.
585080	585960	And that's it.
585960	587240	That's literally it.
587240	588880	This is the core idea.
588880	590680	Now, it turns out it's not difficult to formalize
590680	591720	mathematically.
591720	593160	But this is really what's going on.
593160	597120	If in a neural network, in a regular neural network, like
597120	599520	this, you might say, OK, what's the goal?
599520	601040	You run the neural network.
601040	602160	You get an answer.
602160	605040	You compare it to the desired answer.
605040	606720	And whatever difference you have between those two, you
606720	610560	send it back to change the neural network.
610560	611800	That's supervised learning.
611800	614920	In reinforcement learning, you run a neural network.
614920	617800	You add a bit of randomness to your action.
617800	620400	And then if you like the result, your randomness turns
620400	623360	into the desired target in effect.
623360	625320	So that's it.
625320	628320	Trivial.
628320	630200	Now, math exists.
633520	636400	Without explaining what these equations mean, the point is
636400	639800	not really to derive them, but just to show that they exist.
639800	641480	There are two classes of reinforcement learning
641480	642600	algorithms.
642600	646080	One of them is the policy gradient, where basically
646080	649080	what you do is that you take this expression right there,
649080	653160	the sum of rewards, and you just crunch through the
653160	657960	derivatives, you expand the terms, you do some algebra,
657960	660560	and you get a derivative.
660560	663880	And miraculously, the derivative has exactly the
663880	669360	form that I told you, which is try some actions.
669360	671080	And if you like them, increase the
671080	672600	low probability of the actions.
672600	674040	That literally follows from the math.
674040	677680	It's very nice when the intuitive explanation has a
677680	680200	one-to-one correspondence to what you get in the equation.
680200	682600	Even though you'll have to take my word for it, if you're not
682600	686240	familiar with it, that's the equation at the top.
686240	687920	Then there is a different class of reinforcement learning
687920	690480	algorithms, which is a little bit more difficult to explain.
690480	692720	It's called the Q learning-based algorithms.
692720	698840	They are a bit less stable, a bit more sample efficient.
698840	704360	And it has the property that it can learn not only from the
704360	707640	data generated by the actor, but from any other data as
707640	708140	well.
708140	712080	So it has a different robustness profile, which will be a
712080	713360	little bit important.
713360	716480	But it's only going to be a technicality.
716480	719240	So yeah, this is the on policy, off policy distinction.
719240	720360	But it's a little bit technical.
720360	723520	So if you find this hard to understand, don't worry
723520	724200	about it.
724200	728400	If you already know it, then you already know it.
728400	730000	So now what's the potential for reinforcement learning?
730000	731760	What's the promise?
731760	733440	What is it actually?
733440	736320	Why should we be excited about it?
736320	737800	Now, there are two reasons.
737800	741640	The reinforcement learning algorithms of today are already
741640	742760	useful and interesting.
742760	745360	And especially if you have a really good simulation of
745360	747720	your world, you could train agents to do lots of
747720	748960	interesting things.
751120	754120	But what's really exciting is if you can build a super
754120	756800	amazing sample efficient reinforcement learning
756800	760040	algorithm, which is give it a tiny amount of data, and the
760040	762200	algorithm just crunches through it and extracts every
762200	764920	bit of entropy out of it in order to learn in
764920	767160	the fastest way possible.
767160	769680	Now, today our algorithms are not particularly
769680	770360	data efficient.
770360	772800	They are data inefficient.
772800	777480	But as our field keeps making progress, this will change.
777480	782760	Next, I want to dive into the topic of meta learning.
782760	786000	The goal of meta learning, so meta learning is a beautiful
786000	790640	idea that doesn't really work, but it kind of works.
790640	791760	And it's really promising too.
791760	794600	It's another promising idea.
794640	796600	So what's the dream?
796600	799200	We have some learning algorithms.
799200	801480	Perhaps we could use those learning algorithms in order
801480	803720	to learn to learn.
803720	806760	It would be nice if we could learn to learn.
806760	808840	So how would you do that?
808840	814680	You would take a system which you train it, not on one task,
814680	817640	but on many tasks, and you ask it that it learns to solve
817640	820360	these tasks quickly.
820360	822480	And that may actually be enough.
822480	823320	So here's how it looks like.
823320	826080	Here's how most traditional meta learning
826080	827920	looks like.
827920	830960	You have a model which is a big neural network.
830960	836360	But what you do is that you treat every, instead of
836360	838960	training cases, you have training tasks.
838960	841240	And instead of test cases, you have test tasks.
841240	844960	So your input may be, instead of just your current test
844960	848520	case, it would be all the information about the test
848520	850960	tasks plus the test case.
850960	854000	And you'll try to output the prediction or action for that
854000	854840	test case.
854840	857920	So basically you say, yeah, I'm going to give you your 10
857920	861640	examples as part of your input to your model, figure out how
861640	864280	to make the best use of them.
864280	867520	It's a really straightforward idea.
867520	870880	You turn the neural network into the learning algorithm by
870880	874760	turning a training task into a training case.
874760	877520	So training task equals training case.
877520	879120	This is meta learning.
879120	880120	This one sentence.
883160	885680	And so there have been several success stories which I
885680	888440	think are very interesting.
888440	891280	One of the success stories of meta learning is learning to
891280	893400	recognize characters quickly.
893400	898960	So there have been a data set produced by MIT by Lake et al.
898960	902720	And this is a data set.
902720	904480	We have a large number of different handwritten
904480	906160	characters.
906160	908860	And people have been able to train extremely strong
908860	911940	meta learning system for this task.
911940	915460	Another very successful example of meta learning is that of
915460	921620	neural architecture search by Zopen Lee from Google, where
921620	924100	they found a neural architecture that solved one
924100	926180	problem well, a small problem.
926180	927900	And then it could generalize, and then it would successfully
927900	929260	solve large problems as well.
929260	934620	So this is kind of the small number of bits meta learning.
934620	936580	It's like when you learn the architecture, or maybe even
936580	938420	learn a program, a small program, or a learning
938420	940660	algorithm, it should apply to new tasks.
940660	943500	So this is the other way of doing meta learning.
943500	947060	So anyway, but the point is what's really happening in
947060	950900	meta learning in most cases is that you turn a training
950900	954500	task into a training case and pretend that this is totally
954500	956460	normal, normal deep learning.
956460	957420	That's it.
957420	959380	This is the entirety of meta learning.
959380	963660	Everything else suggests minor details.
963660	965140	Next, I want to dive in.
965140	967820	So now that I've finished the introduction section, I want to
967820	971820	start discussing different work by different people from
971820	972860	OpenAI.
972860	974980	And I want to start by talking about hindsight
974980	976620	experience replay.
976620	980380	There's been a large effort by Andriy Khopich et al. to
980380	985420	develop a learning algorithm for reinforcement learning that
985420	989940	doesn't solve just one task, but it solves many tasks.
989940	993660	And it learns to make use of its experience in a much
993660	995980	more efficient way.
995980	997860	And I want to discuss one problem in reinforcement
997860	998620	learning.
998620	1002140	It's actually, I guess, a set of problems which are related to
1002140	1003140	each other.
1007140	1009300	But one really important thing you need to learn to do is to
1009300	1010940	explore.
1010940	1015100	You start out in an environment you don't know what to do.
1015100	1016460	What do you do?
1016460	1018660	So one very important thing that has to happen is that you
1018660	1021140	must get rewards from time to time.
1021180	1026260	If you try something and you don't get rewards, then how
1026260	1029060	can you learn?
1029060	1031380	So that's the kind of the crux of the problem.
1031380	1032420	How do you learn?
1032420	1039460	And relatedly, is there any way to meaningfully benefit from
1039460	1043340	the experience, from your attempts, from your failures?
1043340	1045780	If you try to achieve a goal and you fail, can you still
1045780	1047220	learn from it?
1047220	1049980	Instead of asking your algorithm to achieve a single
1049980	1052780	goal, you want to learn a policy that can achieve a very
1052780	1054380	large family of goals.
1054380	1057220	For example, instead of reaching one state, you want to
1057220	1060820	learn a policy that reaches every state of your system.
1060820	1062740	Now, what's the implication?
1062740	1066860	Anytime you do something, you achieve some state.
1066860	1071340	So let's suppose you say, I want to achieve state A. I
1071340	1076220	try my best and I end up achieving state B. I can
1076220	1078780	either conclude, while I was disappointing, I haven't learned
1078780	1080500	almost anything.
1080500	1084260	I still have no idea how to achieve state A. But
1084260	1087180	alternatively, I can say, well, wait a second, I've just
1087180	1090660	reached a perfectly good state, which is B. Can I learn
1090660	1094820	how to achieve state B from my attempt to achieve state A?
1094820	1096460	And the answer is yes, you can.
1096460	1097860	And it just works.
1097860	1100460	And I just want to point out, this is the one case.
1100460	1105780	There's a small subtlety here, which may be interesting to
1105780	1108020	those of you who are very familiar with on-pot, be the
1108020	1111260	distinction between on-policy and off-policy.
1111260	1115380	When you try to achieve A, you're doing on-policy learning
1115380	1118980	for reaching the state A. But you're doing off-policy
1118980	1121700	learning for reaching the state B. Because you would take
1121700	1124660	different actions if you would actually try to reach state B.
1124660	1126780	So that's why it's very important that the algorithm
1126780	1129780	you use here can support off-policy learning.
1129780	1132020	But that's a minor technicality.
1132020	1137660	At the crux of the idea is you make the problem easier by
1137660	1140540	ostensibly making it harder.
1140540	1144260	By training a system which aspires to reach, to learn to
1144260	1147860	reach every state, to learn to achieve every goal, to learn
1147860	1152300	to master its environment in general, you build a system
1152300	1155100	which always learns something.
1155100	1157620	It learns from success as well as from failure.
1157620	1160500	Because if it tries to do one thing and it does something
1160500	1163180	else, it now has training data for how to achieve that
1163180	1164820	something else.
1164820	1166340	I want to show you a video of how this thing
1166340	1167900	works in practice.
1167900	1172260	So one challenge in reinforcement learning systems is
1172260	1174260	the need to shape the reward.
1174260	1176260	So what does it mean?
1176260	1178980	It means that at the beginning of the system, at the start
1178980	1182340	of learning when the system doesn't know much, it will
1182340	1183860	probably not achieve your goal.
1183860	1186180	And so it's important that you design your reward function
1186180	1188140	to give it gradual increments, to make it
1188140	1189980	smooth and continuous so that even when the system is not
1189980	1192260	very good, it achieves the goal.
1192260	1195780	Now, if you give your system a very sparse reward, where the
1195780	1200500	reward is achieved only when you reach a final state, then it
1200500	1203300	becomes very hard for normal reinforcement learning
1203300	1204540	algorithms to solve a problem.
1204540	1207180	Because naturally, you never get the reward, so you never
1207180	1207980	learn.
1207980	1210100	No reward means no learning.
1210100	1213500	But here, because you learn from failure as well as from
1213500	1217540	success, this problem simply doesn't occur.
1217540	1219140	And so this is nice.
1219140	1222740	I think let's look at the videos a little bit more.
1222740	1226300	It's nice how it confidently and energetically moves the
1226300	1229140	little green pack to its target.
1229140	1230380	And here's another one.
1250140	1251740	OK, so we can skip the physics.
1251740	1254540	It works if you do it on a physical robot as well, but we
1254540	1256460	can skip it.
1256460	1259940	So I think the point is that the hindsight experience replay
1259940	1265540	algorithm is directionally correct because you want to
1265540	1268820	make use of all your data and not only a small fraction of
1268820	1270020	it.
1270020	1274140	Now, one huge question is, where do you get the high level
1274140	1275700	states?
1275740	1279140	Where do the high level states come from?
1279140	1283140	Because in the work that I've shown you so far, the system
1283140	1285100	is asked to achieve low level states.
1285100	1288060	So I think one thing that will become very important for
1288060	1291220	these kind of approaches is representation learning and
1291220	1292660	unsupervised learning.
1292660	1296340	Figure out what are the right states, what's the state
1296340	1301340	space of goals that's worth achieving?
1301340	1307260	Now I want to go through some real meta-learning results.
1307260	1312140	And I'll show you a very simple way of doing seem-to-real
1312140	1316900	from simulation to the physical robot with meta-learning.
1316900	1320860	And this is work by Pangadal, was a really nice intern
1320860	1324060	project in 2017.
1324060	1329180	So I think we can agree that in the domain of robotics, it
1329180	1333820	would be nice if you could train your policy in simulation
1333820	1338580	and then somehow this knowledge would carry over to the
1338580	1341060	physical robot.
1341060	1347340	Now, we can build simulators that are OK, but they can never
1347340	1350620	perfectly match the real world unless you want to have an
1350620	1352620	insanely slow simulator.
1352620	1358020	And the reason for that is that it turns out that simulating
1358020	1360260	contacts is super hard.
1360260	1363260	And I heard somewhere, correct me if I'm wrong, that
1363260	1365620	simulating friction is NP-complete.
1365620	1366660	I'm not sure.
1366660	1369780	But it's like stuff like that.
1369780	1373900	So your simulation is just not going to match reality.
1373900	1376260	There'll be some resemblance, but that's it.
1376260	1379300	How can we address this problem?
1379300	1380820	And I want to show you one simple idea.
1381140	1386620	So let's say one thing that would be nice is that if you
1386620	1392980	could learn a policy that would quickly adapt itself to the
1392980	1396620	real world, well, if you want to learn a policy that can
1396620	1399740	quickly adapt, we need to make sure that it has opportunities
1399740	1401460	to adapt during training time.
1401460	1402900	So what do we do?
1402900	1408380	Instead of solving a problem in just one simulator, we
1408500	1410820	add a huge amount of variability to the simulator.
1410820	1413660	We say we will randomize the friction.
1413660	1417300	So we'll randomize the masses, the length of the different
1417300	1421700	objects and their dimensions.
1421700	1425660	So you try to randomize physics, the simulator, in
1425660	1426900	lots of different ways.
1426900	1430020	And then importantly, you don't tell the policy how you
1430020	1431740	randomized it.
1431740	1433020	So what is it going to do then?
1433020	1435500	You take your policy and you put it in an environment and
1435500	1437420	it says, well, this is really tough.
1437420	1439780	I don't know what the masses are and I don't know what the
1439780	1440900	frictions are.
1440900	1443700	I need to try things out and figure out what the friction
1443700	1447300	is as I get responses from the environment.
1447300	1452980	So you learn a certain degree of adaptability into the policy.
1452980	1454820	And it actually works.
1454820	1456140	I just want to show you.
1456140	1458860	This is what happens when you just train a policy in
1458860	1462020	simulation and deploy it on the physical robot.
1462020	1465260	And here the guy who's going to do that, he's going to do
1465260	1469540	the goal is to bring the hockey puck towards the red dot.
1469540	1471860	And you will see that it will struggle.
1477780	1480220	And the reason it struggles is because of the systematic
1480220	1484700	differences between the simulator and the real
1484700	1487300	physical robot.
1487300	1491020	So even the basic movement is difficult for the policy
1491020	1493300	because the assumptions are violated so much.
1493340	1496020	So if you do the training as I discussed, we train a
1496020	1499540	recurrent neural network policy which learns to quickly
1499540	1503220	infer properties of the simulator in order to
1503220	1504500	accomplish the task.
1504500	1507620	You can then give it the real thing, the real physics, and
1507620	1510060	it will do much better.
1510060	1511780	So now this is not a perfect technique, but it's
1511780	1512820	definitely very promising.
1512820	1515900	It's promising whenever you are able to sufficiently
1515900	1519020	randomize the simulator.
1519020	1522100	So it's definitely very nice to see the closed loop nature
1522140	1522820	of the policy.
1522820	1525540	You can see that it would push the hockey puck, and it would
1525540	1529180	correct it very, very gently to bring it to the goal.
1529180	1530500	Yeah, you saw that?
1530500	1531500	That was cool.
1533860	1539380	So that was a cool application of meta-learning.
1539380	1541980	I want to discuss one more application of meta-learning,
1541980	1546740	which is learning a hierarchy of actions.
1546740	1549260	And this was work done by France Aral.
1549300	1552700	Actually, Kevin France, the ancient who did it, was in
1552700	1557500	high school when he wrote this paper.
1557500	1567100	So one thing that would be nice is if reinforcement
1567100	1569300	learning was hierarchical.
1569300	1572340	If instead of simply taking microactions, you had some
1572340	1576460	kind of little subroutines that you could deploy.
1576460	1579100	Maybe the term subroutine is a little bit too crude, but if
1579140	1583300	you had some idea of which action primitives are
1583300	1585420	worth starting with.
1585420	1591860	Now, no one has been able to get actually real value add
1591860	1593860	from hierarchical reinforcement learning yet.
1593860	1596180	So far, all the really cool results, all the really
1596180	1597900	convincing results of reinforcement learning, do not
1597900	1599900	use it.
1599900	1603420	That's because we haven't quite figured out what's the
1603420	1608180	right way for hierarchical reinforcement learning.
1608180	1611100	And I just want to show you one very simple approach where
1611100	1617540	you use meta-learning to learn a hierarchy of actions.
1617540	1619740	So here's what you do.
1619740	1627660	You have, in this specific work, you have a certain number
1627660	1628780	of low-level primitives.
1628780	1631060	Let's say you have 10 of them.
1631060	1634140	And you have a distribution of tasks.
1634140	1641100	And your goal is to learn low-level primitives such that
1641100	1645700	when they're used inside a very brief run of some reinforcement
1645700	1647980	learning algorithm, you will make as much progress as
1647980	1650060	possible.
1650060	1653180	So the idea is you want to get the greatest amount of
1653180	1653580	progress.
1653580	1661140	You want to learn primitives that result in the greatest
1661140	1664540	amount of progress possible when used inside learning.
1664540	1666380	So this is a meta-learning setup because you need
1666380	1667780	distribution of tasks.
1667780	1672580	And here, we've had a little maze.
1672580	1673780	You have a distribution of amazes.
1673780	1676820	And in this case, the little bug learned three policies which
1676820	1680460	move it in a fixed direction.
1680460	1682780	And as a result of having this hierarchy, you're able to solve
1682780	1683900	problems really fast.
1683900	1686460	But only when the hierarchy is correct.
1686460	1688060	So hierarchical reinforcement learning is still
1688060	1688980	working progress.
1688980	1698620	And this work is an interesting proof point of how
1698620	1704020	hierarchical reinforcement learning could be like if it
1704020	1706660	worked.
1706660	1711540	Now, I want to just spend one slide addressing the
1711540	1715220	limitations of high-capacity meta-learning.
1715220	1721340	The specific limitation is that the training task
1721340	1724100	distribution has to be equal to the test
1724100	1726580	task distribution.
1726580	1730700	And I think this is a real limitation because in reality,
1730700	1734020	the new task that you want to learn will in some ways be
1734020	1738260	fundamentally different from anything you've seen so far.
1738260	1740540	So for example, if you go to school, you learn lots of
1740540	1742740	useful things.
1742740	1747340	But then when you go to work, only a fraction of the things
1747340	1749340	that you've learned carries over.
1749340	1752940	You need to learn quite a few more things from scratch.
1752940	1756380	So meta-learning would struggle with that because it
1756380	1760340	really assumes that the training data, the distribution
1760340	1762500	over the training tasks has to be equal to the distribution
1762500	1764100	over the test tasks.
1764100	1765020	So that's a limitation.
1765020	1769300	I think that as we develop better algorithms for being
1769300	1774860	robust when the test tasks are outside of the distribution
1774860	1777900	of the training task, then meta-learning
1777900	1780340	would work much better.
1780340	1784220	Now, I want to talk about self-play.
1784220	1789100	I think self-play is a very cool topic that's starting to
1789100	1791260	get attention only now.
1791260	1795540	And I want to start by reviewing very old work
1795540	1797460	called TD Gammon.
1797460	1801380	It's back from all the way from 1992, so it's 26 years old
1801380	1802420	now.
1802420	1804140	It was done by Jared Sauron.
1804140	1813500	So this work is really incredible because it has so much
1813500	1815060	relevance today.
1815060	1820260	What they did, basically, they said, OK, let's take two
1820260	1825900	neural networks and let them play against each other, let
1825900	1829300	them play backgammon against each other, and let them be
1829300	1831260	trained with Q-Learning.
1831260	1833940	So it's a super modern approach.
1833940	1839140	And you would think this was a paper from 2017, except that
1839140	1841540	when you look at this plot, it shows that you only have 10
1841540	1844300	hidden units, 20 hidden units, 40 and 80 for the
1844300	1848100	different colors, where you notice that the largest
1848100	1849580	neural network works best.
1849580	1852300	So in some ways, not much has changed, and this is the
1852300	1853300	evidence.
1854300	1856700	And in fact, they were able to beat the world champion in
1856700	1859100	backgammon, and they were able to discover new strategies
1859100	1865100	that the best human backgammon players have not noticed.
1865100	1867100	And they've determined that the strategy is covered by TD
1867100	1868900	Gammon are actually better.
1868900	1874700	So that's pure self-play with Q-Learning, which remained
1874700	1879100	dormant until the DQN worked with Atari, but in mind.
1879300	1887300	So now other examples of self-play include AlphaGo Zero,
1887300	1889900	which was able to learn to beat the world champion in Go
1889900	1893700	without using any external data whatsoever.
1893700	1896700	And now the result of this way is Biopen AI, which is our
1896700	1900700	Dota 2 bot, which was able to build the world champion on
1900700	1901900	the 1v1 version of the game.
1902100	1905100	And so I want to spend a little bit of time talking about the
1905100	1909100	allure of self-play and why I think it's exciting.
1911100	1920100	So one important problem that we must face as we try to build
1920100	1925100	truly intelligent systems is what is the task?
1925100	1927100	What are we actually doing?
1927300	1932300	And one very attractive attribute of self-play is that
1932300	1935300	the agents create the environment.
1935300	1939300	By virtue of the agent acting in the environment,
1939300	1942300	the environment becomes difficult for the other agents.
1942300	1946300	And you can see here an example of an iguana interacting
1946300	1948300	with snakes that try to eat it.
1948300	1954300	And that's why we're trying to build the world champion
1954300	1957500	of the iguana interacting with snakes that try to eat it
1957500	1959500	unsuccessfully this time.
1959500	1962500	So we can see what will happen in a moment.
1962500	1964500	The iguana is trying its best.
1964500	1968500	And so the fact that you have this arms race between the
1968500	1972500	snakes and the iguana motivates their development,
1972500	1975500	potentially without bound.
1975500	1980500	And this is what happened in effect in biological evolution.
1980700	1984700	Now, interesting work in this direction was done in 1994
1984700	1985700	by Carl Simms.
1985700	1989700	There is a really cool video on YouTube by Carl Simms.
1989700	1992200	You should check it out, which really kind of shows all the
1992200	1994200	work that he's done.
1994200	1997200	And here you have a little competition between agents
1997200	2000200	where you evolve both the behavior and the morphology
2000200	2005200	when the agent is trying to gain possession of a green cube.
2007200	2010200	And so you can see that the agents create the challenge
2010200	2011200	for each other.
2011200	2013200	And that's why they need to develop.
2014900	2020900	So one thing that we did, and this is work by Ansel et al
2020900	2023900	from OpenAI, is we said, okay, well,
2023900	2028600	can we demonstrate some unusual results in self play
2028600	2032400	that would really convince us that there is something there?
2032400	2036900	So what we did here is that we created a small ring
2036900	2038800	and you have these two humanoid figures.
2038800	2042700	And their goal is just to push each other outside the ring.
2042700	2045000	And they don't know anything about wrestling.
2045000	2047300	They don't know anything about standing your balance
2047300	2048000	in each other.
2048000	2050100	They don't know anything about centers of gravity.
2050100	2053100	All they know is that if you don't do a good job,
2053100	2056700	then your competition is going to do a better job.
2056700	2060400	Now, one of the really attractive things about self play
2060400	2065900	is that you always have an opponent that's roughly
2065900	2068400	as good as you are.
2068400	2071400	In order to learn, you need to sometimes win
2071400	2072700	and sometimes lose.
2072700	2074700	You can't always win.
2074700	2076100	Sometimes you must fail.
2076100	2079600	Sometimes you must succeed.
2079600	2082000	So let's see what will happen here.
2082000	2083700	Yeah, so it was able to be.
2083700	2088400	So the green humanoid was able to block the ball.
2088400	2092900	In a well-balanced self play environment,
2092900	2095500	the competition is always level.
2095500	2098400	No matter how good you are or how bad you are,
2098400	2101600	you have a competition that makes it exactly
2101600	2103100	of exactly the right challenge for you.
2103100	2104100	Oh, and one thing here.
2104100	2106300	So this video shows transfer learning.
2106300	2109100	You take the little wrestling humanoid
2109100	2111300	and you take its friend away
2111300	2114500	and you start applying big, large, random forces on it.
2114500	2117000	And you see if it can maintain its balance.
2117000	2119900	And the answer turns out to be that yes, it can
2119900	2123000	because it's been trained against an opponent
2123000	2124500	and it pushes it.
2124500	2127300	And so that's why even if it doesn't understand
2127300	2129500	where the pressure force is being applied on it,
2129500	2131500	it's still able to balance itself.
2131500	2135000	So this is one potentially attractive feature
2135000	2136600	of self play environments that you could learn
2136600	2140200	a certain broad set of skills.
2140200	2142300	Although it's a little hard to control those
2142300	2143800	what the skills will be.
2143800	2145900	And so the biggest open question with this research
2145900	2152300	is how do you learn agents in a self play environment
2152300	2154600	such that they do whatever they do,
2154600	2157000	but then they are able to solve a battery of tasks
2157000	2158200	that is useful for us,
2158200	2161800	that is explicitly specified externally.
2161800	2165000	Yeah.
2165000	2168400	I also want to highlight one attribute
2168400	2169500	of self play environments
2169500	2172000	that we've observed in our Dota bot.
2172000	2174400	And that is that we've seen a very rapid increase
2174400	2176100	in the competence of the bot.
2176100	2179000	So over the period, over the course of maybe five months,
2179000	2183900	we've seen the bot go from playing totally randomly
2183900	2188100	all the way to the world champion.
2188100	2189400	And the reason for that
2189400	2192200	is that once you have a self play environment,
2192200	2196400	if you put compute into it, you turn it into data.
2196400	2200300	Self play allows you to turn compute into data.
2200300	2202300	And I think we will see a lot more of that
2202300	2204900	as being an extremely important thing
2204900	2205900	to be able to turn compute
2205900	2208700	into essentially data or generalization.
2208700	2211700	Simply because the speed of neural net processors
2211700	2214900	will increase very dramatically over the next few years.
2214900	2216600	So neural net cycles will be cheap
2216600	2218800	and it will be important to make use of these
2218800	2223300	newly found over abundance of cycles.
2223300	2224400	I also want to talk a little bit
2224400	2228400	about the end game of the self play approach.
2228400	2232500	So one thing that we know about the human brain
2232500	2235100	is that it has increased in size fairly rapidly
2235100	2238700	over the past two million years.
2238700	2241900	My theory, the reason I think it happened
2241900	2246600	is because our ancestors got to a point
2246600	2249700	where the thing that's most important for your survival
2249700	2252200	is your standing in the tribe
2252200	2254800	and less the tiger in the lion.
2254800	2256900	Once the most important thing
2256900	2259200	is how you deal with those other things
2259200	2260600	which have a large brain,
2260600	2263200	then it really helps to have a slightly larger brain.
2263200	2264500	And I think that's what happened.
2264500	2267700	And there exists at least one paper from science
2267700	2270200	which supports this point of view.
2270200	2272400	So apparently there has been convergent evolution
2272400	2276700	between social apes and social birds
2276700	2280800	even though in terms of various behaviors.
2280800	2284300	Even though the divergence in evolution
2284300	2286500	and timescale between humans and birds
2286500	2288600	has occurred a very long time ago
2288600	2291500	and humans and sorry humans apes and humans apes
2291500	2296500	and birds have very different brain structure.
2296500	2299900	So I think what should happen if we succeed,
2299900	2303200	if we successfully follow the path of this approach
2303200	2305000	is that we should create a society of agents
2305000	2308700	which will have language and theory of mind,
2308700	2313600	negotiation, social skills, trade, economy, politics,
2313600	2315100	justice system.
2315100	2317000	All these things should happen
2317000	2319000	inside the multi-agent environment.
2319000	2320600	And there will also be some alignment issue
2320600	2323100	of how do you make sure that the agents we learn
2323100	2325200	behave in a way that we want.
2325200	2329800	Now I want to make a speculative digression here
2329800	2337100	which is, I want to make the following observation.
2337100	2342000	If you believe that this kind of society of agents
2342000	2348500	is a plausible place where truly,
2348500	2352200	where fully general intelligence will emerge.
2352200	2356700	And if you accept that our experience with the dotabot
2356700	2358700	where we've seen a very rapid increase in competence
2358700	2362000	will carry over once all the details are right.
2362000	2364600	If you assume both of these conditions
2364600	2368000	then it should follow that we should see a very rapid increase
2368000	2370700	in the competence of our agents
2370700	2374500	as they leave in the society of agents.
2374500	2380300	So now that we've talked about a potentially interesting way
2380300	2383100	of increasing the competence and teaching agents
2383100	2386600	social skills and language and a lot of things
2386600	2389200	that actually exist in humans as well.
2389200	2393700	We want to talk a little bit about how you convey goals
2393700	2395700	to agents.
2395700	2399500	And the question of conveying goals to agents
2399500	2401400	is just a technical problem.
2401400	2404700	But it will be important
2404700	2408100	because it is more likely than not
2408100	2412100	that the agents that we will train will eventually
2412100	2414300	be dramatically smarter than us.
2414300	2417500	And this is work by the OpenAI safety team
2417500	2421700	by Paul Cruciano and others.
2421700	2423500	So I'm just going to show you this video
2423500	2427600	which basically explains how the whole thing works.
2427600	2430600	There is some behavior you are looking for
2430600	2434700	and you the human gets to see pairs of behaviors.
2434700	2439500	And you simply click on the one that looks better.
2439500	2444200	And after a very modest number of clicks
2444200	2457500	you can get this little simulated leg to do backflips.
2457500	2458800	There you go.
2458800	2460000	You can now do backflips.
2460000	2462300	And to get this specific behavior
2462300	2468000	it took about 500 clicks by human annotators.
2468000	2470500	The way it works is that you take all the...
2470500	2473300	So this is a very data efficient reinforcement learning
2473300	2477100	algorithm but it is efficient in terms of rewards
2477100	2480400	and not in terms of the environment interactions.
2480400	2482900	So what you do here is that you take all the clicks
2482900	2484200	so you've got your...
2484200	2487700	Here is one behavior which is better than the other.
2487700	2492400	You fit a reward function, a numerical reward function
2492400	2493500	to those clicks.
2493500	2494800	So you want to fit a reward function
2494800	2496100	which satisfies those clicks
2496100	2497600	and then you optimize this reward function
2497600	2499500	with reinforcement learning.
2499500	2501200	And it actually works.
2501200	2504800	So this requires 500 bits of information.
2504800	2508000	You've also been able to train lots of Atari games
2508000	2509500	using several thousand bits of information.
2509500	2513000	So in all these cases you had human annotators
2513000	2516600	or human judges just like in the previous slide
2516700	2520500	looking at pairs of trajectories
2520500	2523900	and clicking on the one that they thought was better.
2523900	2528000	And here's an example of an unusual goal
2528000	2530000	where this is a car racing game
2530000	2536100	but the goal was to ask the agent to train the white car
2536100	2538800	drive right behind the orange car.
2538800	2539800	So it's a different goal
2539800	2543400	and it was very straightforward to communicate this goal
2543400	2546500	using this approach.
2547000	2551300	So then to finish off, alignment is a technical problem.
2551300	2553000	It has to be solved.
2553000	2556300	But of course the determination of the correct goals
2556300	2558600	we want RA systems to have
2558600	2562100	will be a very challenging political problem.
2562100	2563600	And on this note,
2563600	2565600	I want to thank you so much for your attention
2565600	2568200	and I just want to say that it will be a happy hour
2568200	2570400	at Cambridge Brewing Company at 8.45.
2570400	2573400	If you want to chat more about AI and other topics,
2573400	2574800	please come by.
2574800	2576500	I think that deserves an applause.
2576500	2577600	Thank you very much.
2583600	2587400	So back propagation is a...
2587400	2589100	or neural networks are inspired
2589100	2590700	but back propagation doesn't look as though
2590700	2592500	it's what's going on in the brain
2592500	2595800	because signals in the brain go one direction down the axons
2595800	2597700	whereas back propagation requires the errors
2597700	2600700	to be propagated back up the wires.
2600700	2604200	So can you just talk a little bit
2604200	2606200	about that whole situation
2606200	2608600	where it looks as though the brain is doing something a bit different
2608600	2611400	than our highly successful algorithms?
2611400	2613500	Are algorithms going to be improved
2613500	2615300	once we figure out what the brain is doing
2615300	2617400	or is the brain really sending signals back
2617400	2620200	even though it's got no obvious way of doing that?
2620200	2622300	What's happening in that area?
2622300	2624400	So that's a great question.
2624400	2626500	So first of all, I'll say that the true answer
2626500	2628900	is that the honest answer is that I don't know
2628900	2630600	but I have opinions.
2630700	2635000	And so I'll say two things.
2635000	2638400	Like first of all, given that...
2638400	2641200	like if you agree, if we agree like so rather
2641200	2644000	it is a true fact that back propagation
2644000	2648000	solves the problem of circuit search.
2648000	2652200	This problem feels like an extremely fundamental problem
2652200	2655100	and for this reason I think that it's unlikely to go away.
2655100	2658600	Now you also write that the brain doesn't obviously
2658600	2661000	do back propagation although there have been multiple proposals
2661000	2662900	of how it could be doing them.
2662900	2669000	For example, there's been a work by Tim Lillicrap and others
2669000	2671100	where they've shown that if you use...
2671100	2674400	that it's possible to learn a different set of connections
2674400	2676700	that can be used for the backward pass
2676700	2678700	and that can result in successful learning.
2678700	2682500	Now, the reason this hasn't been like really pushed to the limit
2682500	2684000	by practitioners is because they say,
2684000	2686500	well, I got tf.gradients,
2686500	2688600	I'm just not going to worry about it.
2688600	2690600	But you are right that this is an important issue
2690600	2693400	and one of two things is going to happen.
2693400	2696000	So my personal opinion is that back propagation
2696000	2698200	is just going to stay with us till the very end
2698200	2701800	and we'll actually build fully human level and beyond systems
2701800	2706200	before we understand how the brain does what it does.
2706200	2709600	So that's what I believe but of course
2709600	2712600	it is a difference that has to be acknowledged.
2712600	2714200	Okay, thank you.
2714300	2718200	Do you think it was a fair matchup for the Dota bot
2718200	2721700	and that person given the constraints of the system?
2721700	2727100	So I'd say that the biggest advantage computers have in games like this,
2727100	2732000	like one of the big advantages is that they obviously have a better reaction time.
2732000	2733900	Although in Dota in particular,
2733900	2739100	the number of clicks per second over the top layer is fairly small,
2739100	2740600	which is different from StarCraft.
2740600	2744700	So in StarCraft, StarCraft is a very mechanically heavy game
2744700	2746700	because you have a large number of units
2746700	2750600	and so the top layers they just click all the time.
2750600	2753400	In Dota, every player control is just one hero
2753400	2756900	and so that greatly reduces the total number of actions they need to make.
2756900	2758500	Now still precision matters.
2758500	2762200	I think that we'll discover that,
2762200	2765500	but what I think will really happen is that we'll discover that computers have
2765600	2772600	the advantage in any domain or rather every domain.
2774600	2775100	Not yet.
2775100	2778100	So do you think that the emergent behaviors from the agent
2778100	2780500	were actually kind of directed
2780500	2782700	because the constraints were already kind of in place?
2782700	2784100	Like so it was kind of forced to discover those
2784100	2788000	or do you think that like that was actually something quite novel
2788000	2791000	that like, wow, it actually discovered these on its own?
2791000	2793600	Like you didn't actually bias towards constraining it?
2793600	2795600	So it's definitely de-discovering new strategies
2795600	2799300	and I can share an anecdote where our tester,
2799300	2802100	we have a pro which would test the bot
2802100	2804600	and he played against it for a long time
2804600	2808000	and the bot would do all kinds of things against the player,
2808000	2810100	the human player, which were effective.
2810100	2815000	Then at some point that pro decided to play against the better pro
2815000	2818200	and he decided to imitate one of the things that the bot was doing
2818200	2823400	and by imitating it he was able to defeat a better pro.
2823400	2826200	So I think the strategies that he discovers are real
2826200	2831500	and so like it means that there's very real transfer between, you know,
2831500	2835800	I would say I think what that means is that the,
2835800	2838500	because the strategies discovered by the bot help the humans,
2838500	2843000	it means that the fundamental gameplay is deeply related.
2843000	2847400	For a long time now I've heard that the objective of reinforcement learning
2847400	2851300	is to determine a policy that chooses an action
2851300	2856200	to maximize the expected reward, which is what you said earlier.
2856200	2861500	Would you ever want to look at the standard deviation of possible rewards?
2861500	2862700	Does that even make sense?
2862700	2867500	Yeah, I mean I think for sure, I think it's really application dependent.
2867500	2870900	One of the reasons to maximize the expected reward
2870900	2874700	is because it's easier to design algorithms for it.
2874700	2879200	So you write down this equation, the formula,
2879200	2880800	you do a little bit of derivation,
2880800	2883900	you get something which amounts to a nice-looking algorithm.
2883900	2888700	Now, I think there exists like really,
2888700	2891000	there exist applications where you never want to make mistakes
2891000	2893900	and you want to work on the standard deviation as well.
2893900	2897800	But in practice it seems that the, just looking at the expected reward
2897800	2904900	covers a large fraction of the situation as you'd like to apply this to.
2904900	2905600	Okay, thanks.
2905600	2906100	Thank you.
2909500	2917100	We talked last week about motivations and that has a lot to do with the reinforcement.
2917100	2923800	And some of the ideas is that the, our motivations are actually connection
2923800	2926300	with others and cooperation.
2926300	2930300	And I'm wondering if, Thurnoff, and I understand it's very popular
2930300	2934800	to have the computers play these competitive games.
2934800	2943700	But is there any use in like having an agent self-play collaboratively, collaborative games?
2943700	2947600	Yeah, I think that's an extremely good question.
2947600	2950800	I think one place from which we can get some inspiration
2950800	2953500	is from the evolution of cooperation.
2953500	2959700	Like, I think cooperation, we cooperate ultimately
2959700	2964800	because it's much better for you, the person to be cooperative than not.
2964800	2973500	And so, I think what should happen, if you have a sufficiently open-ended game,
2973500	2976700	then cooperation will be the winning strategy.
2976700	2983800	And so I think we will get cooperation, whether we like it or not.
2983900	2990100	Hey, you mentioned the complexity of the simulation of friction.
2990100	2993600	I was wondering if you feel that there exists open complexity,
2993600	2996800	theoretic problems relevant to, relevant to AI,
2996800	2999700	or whether it's just a matter of finding good approximations
2999700	3004100	that humans, of the types of problems that humans tend to solve.
3004100	3007100	Yeah, so complexity theory.
3007100	3010800	Well, like at a very basic level,
3010900	3013900	we know that whatever algorithm we're going to run
3013900	3016900	is going to run fairly efficiently on some hardware.
3016900	3023400	So that puts a pretty strict upper bound on the true complexity of the problems we are solving.
3023400	3028700	Like, by definition, we are solving problems which aren't too hard in a complexly theoretic sense.
3028700	3033400	Now, it is also the case that many of the problems,
3033400	3038400	so while the overall thing that we do is not hard from a complexity-theoretic sense,
3038400	3043400	and indeed, humans cannot solve NP-complete problems in general.
3043400	3048400	It is true that many of the optimization problems that we pose to our algorithms
3048400	3053400	are intractable in the general case, starting from neural net optimization itself.
3053400	3058400	It is easy to create a family of datasets for a neural network with a very small number of neurons,
3058400	3061400	such that finding the global optimum is NP-complete.
3061400	3064400	And so, how do we avoid it?
3064400	3068400	Well, we just try gradient descent anyway, and somehow it works.
3068400	3077400	But without question, we do not solve problems which are truly intractable.
3077400	3080400	So, I mean, I hope this answers the question.
3080400	3088400	Hello. It seems like an important sub-problem on the path towards AGI will be understanding language,
3088400	3092400	and the state of generative language modeling right now is pretty abysmal.
3092400	3097400	What do you think are the most productive research trajectories towards generative language models?
3097400	3104400	So, I'll first say that you are completely correct that the situation with language is still far from great,
3104400	3112400	although progress has been made, even without any particular innovations beyond models that exist today.
3112400	3118400	Simply scaling up models that exist today on larger datasets is going to go surprisingly far,
3118400	3121400	not even larger datasets, but larger and deeper models.
3121400	3126400	For example, if you trained a language model with a thousand layers, and it's the same layer,
3126400	3130400	I think it's going to be a pretty amazing language model.
3130400	3135400	Like, we don't have the cycles for it yet, but I think it will change very soon.
3135400	3144400	Now, I also agree with you that there are some fundamental things missing in our current understanding of deep learning,
3144400	3148400	which prevent us from really solving the problem that we want.
3148400	3153400	So, I think one of these problems, one of the things that's missing is that, or that seems like patently wrong,
3153400	3161400	is the fact that we train a model, then we stop training the model, and we freeze it,
3161400	3167400	even though it's the training process where the magic really happens.
3167400	3176400	Like, the magic is like, if you think about it, like, the training process is the true general part of the whole story,
3176400	3179400	because your TensorFlow code doesn't care which dataset to optimize.
3179400	3183400	It just says, whatever, just give me the dataset, I don't care which one to solve, I'll solve them all.
3183400	3191400	So, like, the ability to do that feels really special, and I think we are not using it at test time.
3191400	3195400	Like, it's hard to speculate about, like, things of which we don't know the answer,
3195400	3202400	but all I'll say is that simply train bigger, deeper language models will go surprisingly far scaling up,
3202400	3205400	but also doing things like training at test time and inference at test time,
3205400	3209400	I think would be another important boost to performance.
3209400	3212400	Hi, thank you for the talk.
3212400	3216400	So, it seems like right now another interesting approach to solving reinforcement learning problems
3216400	3221400	could be to go for the evolutionary routes, using evolutionary strategies.
3221400	3225400	And although they have their caveats, I wanted to know if, at OpenAI in particular,
3225400	3230400	you're working on something related, and what is your general opinion on them?
3230400	3238400	So, like, at present, I believe that something like evolutionary strategies is not great for reinforcement learning.
3238400	3243400	I think that normal reinforcement learning algorithms, especially with big policies, are better.
3243400	3249400	But I think if you want to evolve a small compact object, like a piece of code, for example,
3249400	3254400	I think that would be a place where this would be seriously worth considering.
3254400	3260400	But this, you know, evolving a useful piece of code is a cool idea.
3260400	3264400	It hasn't been done yet, so still a lot of work to be done before we get there.
3264400	3266400	Hi, thank you so much for coming.
3266400	3271400	My question is, you mentioned what is the right goal is a political problem.
3271400	3274400	So, I'm wondering if you can elaborate a bit on that,
3274400	3279400	and also what do you think would be their approach for us to maybe get there?
3280400	3285400	Well, I can't really comment too much, because all the thoughts that, you know,
3285400	3290400	we now have a few people who are thinking about this full time at OpenAI,
3290400	3297400	I don't have enough of a super strong opinion to say anything too definitive.
3297400	3302400	All I can say at a very high level is given the size, like, if you go into the future,
3302400	3306400	whenever soon or later, you know, whenever it's going to happen when you build a computer
3306400	3310400	which can do anything better than a human.
3310400	3313400	It will happen, because the brain is physical.
3313400	3318400	The impact on society is going to be completely massive and overwhelming.
3318400	3323400	It's very difficult to imagine, even if you try really hard.
3323400	3327400	And I think what it means is that people will care a lot.
3327400	3331400	And that's what I was alluding to, the fact that this will be something
3331400	3334400	that many people will care about strongly.
3334400	3339400	And, like, as the impact increases, gradually we sell driving cars, more automation,
3339400	3342400	I think we will see a lot more people care.
3342400	3346400	Do we need to have a very accurate model of the physical world,
3346400	3353400	and then simulate that in order to have these agents that can eventually come out into the real world
3353400	3358400	and do something approaching, you know, human level intelligence tasks?
3358400	3360400	That's a very good question.
3360400	3364400	So I think if that were the case, we'd be in trouble.
3364400	3370400	And I am very certain that it could be avoided.
3370400	3376400	So specifically, the real answer has to be that, look, you learn to problem solve,
3376400	3378400	you learn to negotiate, you learn to persist,
3378400	3381400	you learn lots of different useful life lessons in the simulation.
3381400	3383400	And yes, you learn some physics, too.
3383400	3386400	But then you go outside of the real world, and you have to start over to some extent,
3386400	3390400	because many of your deeply held assumptions will be false.
3390400	3397400	And one of the goals, so that's one reason I care so much about never stopping training.
3397400	3401400	You've accumulated your knowledge, now you go into an environment where some of your assumptions are violated,
3401400	3404400	you continue training, you try to connect the new data to your old data.
3404400	3406400	And this is an important requirement from our algorithms,
3406400	3410400	which is already met to some extent, but it will have to be met a lot more,
3410400	3414400	so that you can take the partial knowledge that you've acquired
3414400	3417400	and go in a new situation, learn some more.
3417400	3422400	Literally the example of you go to school, you learn useful things, then you go to work.
3422400	3426400	It's not a perfect, it's not, you know, for your four years of CS and undergrad,
3426400	3430400	it's not going to fully prepare you for whatever it is you need to know at work.
3430400	3432400	It will help somewhat, you'll be able to get off the ground,
3432400	3434400	but there will be lots of new things you need to learn.
3434400	3436400	So that's the spirit of it.
3436400	3438400	Think of it as a school.
3438400	3442400	One of the things you mentioned pretty early on in your talk is that one of the limitations
3442400	3446400	of this sort of style of reinforcement learning is there's no self-organization.
3446400	3449400	So you have to tell it, went into a good thing or did a bad thing.
3449400	3451400	And that's actually a problem in neuroscience,
3451400	3454400	is when you're trying to teach a rat to, you know, navigate a maze,
3454400	3456400	you have to artificially tell it what to do.
3456400	3459400	So where do you see moving forward when we already have this problem with teaching,
3459400	3461400	you know, not necessarily learning, but also teaching.
3461400	3464400	So where do you see the research moving forward in that respect?
3464400	3467400	How do you sort of introduce this notion of self-organization?
3467400	3470400	So I think without question, one really important thing you need to do
3470400	3477400	is to be able to infer the goals and strategies of other agents by observing them.
3477400	3482400	That's a fundamental skill you need to be able to learn, to embed into the agents.
3482400	3485400	So that, for example, you have two agents, one of them is doing something,
3485400	3488400	and the other agent says, well, that's really cool, I want to be able to do that too.
3488400	3490400	And then you go on and do that.
3490400	3494400	And so I'd say that this is a very important component in terms of setting the reward.
3494400	3498400	You see what they do, you infer the reward,
3498400	3501400	and now we have a knob which says, you see what they're doing?
3501400	3503400	Now go and try to do the same thing.
3503400	3508400	So I'd say this is, as far as I know, this was one of the important ways
3508400	3513400	in which humans are quite different from other animals
3513400	3522400	in the way in the scale and scope in which we copy the behavior of other humans.
3522400	3524400	You might have asked a quick follow-up.
3524400	3525400	Go for it.
3525400	3528400	So that's kind of obvious how that works in the scope of competition,
3528400	3530400	but what about just sort of arbitrary tasks?
3530400	3534400	Like, I'm in a math class with someone and I see someone doing a problem a particular way
3534400	3537400	and I'm like, oh, that's a good strategy, maybe I should try that out.
3537400	3540400	How does that work in a sort of non-competitive environment?
3540400	3544400	So I think that this will be, I think that's going to be a little bit separate
3544400	3550400	from the competitive environment, but it will have to be somehow either,
3551400	3556400	probably baked in, maybe evolved into the system where like,
3556400	3561400	if you have other agents doing things, they're generating data which you observe
3561400	3564400	and the only way to truly make sense of the data that you see
3564400	3569400	is to infer the goal of the agent, the strategy, their belief state.
3569400	3572400	That's important also for communicating with them.
3572400	3574400	If you want to successfully communicate with someone,
3574400	3577400	you have to keep track both of their goal and of their belief state instead of knowledge.
3577400	3581400	So I think you will find that there are many, I guess, connections
3581400	3585400	between understanding what other agents are doing, inferring their goals,
3585400	3588400	imitating them and successfully communicating them.
3588400	3592400	Alright, let's give Ilya and the happy hour a big hand.
3607400	3609400	Thank you.
