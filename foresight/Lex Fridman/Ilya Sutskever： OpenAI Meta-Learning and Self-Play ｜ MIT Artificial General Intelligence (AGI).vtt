WEBVTT

00:00.000 --> 00:04.040
Welcome back to 6SES 099, Artificial General Intelligence.

00:04.040 --> 00:12.340
Today we have Ilya Setskever, co-founder and research director

00:12.340 --> 00:13.440
of OpenAI.

00:13.440 --> 00:17.320
He started in the ML group in Toronto with Geoffrey Hinton,

00:17.320 --> 00:19.040
then at Stanford with Andrew Ang,

00:19.040 --> 00:21.480
co-founded DNN Research for three years

00:21.480 --> 00:23.520
as a research scientist at Google Brain,

00:23.520 --> 00:26.280
and finally co-founded OpenAI.

00:26.280 --> 00:29.520
Citations aren't everything, but they

00:29.520 --> 00:31.160
do indicate impact.

00:31.160 --> 00:35.040
And his work, recent work, in the past five years,

00:35.040 --> 00:39.360
has been cited over 46,000 times.

00:39.360 --> 00:43.320
He has been the key creative intellect and driver

00:43.320 --> 00:45.400
behind some of the biggest breakthrough ideas

00:45.400 --> 00:49.480
in deep learning and artificial intelligence ever.

00:49.480 --> 00:51.880
So please welcome Ilya.

00:51.880 --> 01:02.280
All right, thanks for the introduction, Lex.

01:02.280 --> 01:04.560
All right, thanks for coming to my talk.

01:04.560 --> 01:06.160
I will tell you about some work we've

01:06.160 --> 01:10.280
done over the past year on meta-learning and self-play

01:10.280 --> 01:11.920
at OpenAI.

01:11.920 --> 01:15.960
And before I dive into some of the more technical details

01:15.960 --> 01:19.520
of the work, I want to spend a little bit of time

01:19.520 --> 01:24.440
talking about deep learning and why it works at all

01:24.440 --> 01:26.440
in the first place, which I think

01:26.440 --> 01:29.080
it's actually not a self-evident thing

01:29.080 --> 01:30.640
that they should work.

01:30.640 --> 01:36.720
One fact, it's actually a fact, some mathematical theorem

01:36.720 --> 01:42.240
that you can prove, is that if you could find the shortest

01:42.240 --> 01:47.320
program that does very well on your data,

01:47.320 --> 01:50.560
then you will achieve the best generalization possible.

01:50.560 --> 01:51.960
With a little bit of modification,

01:51.960 --> 01:54.440
you can turn it into a precise theorem.

01:54.440 --> 01:58.320
And on a very intuitive level, it's

01:58.320 --> 02:00.920
easy to see why it should be the case.

02:00.920 --> 02:05.040
If you have some data and you're able to find a shorter

02:05.040 --> 02:08.000
program which generates this data,

02:08.000 --> 02:10.920
then you've essentially extracted all conceivable

02:10.920 --> 02:13.800
regularity from this data into your program.

02:13.800 --> 02:15.640
And then you can use this object to make the best

02:15.640 --> 02:18.080
predictions possible.

02:18.080 --> 02:21.880
If you have data which is so complex

02:21.880 --> 02:26.160
that there is no way to express it as a shorter program,

02:26.160 --> 02:28.200
then it means that your data is totally random.

02:28.200 --> 02:32.400
There is no way to extract any regularity from it whatsoever.

02:32.400 --> 02:35.640
Now, there is little known mathematical theory behind this.

02:35.640 --> 02:37.200
And the proofs of these statements

02:37.200 --> 02:39.160
are actually not even that hard.

02:39.160 --> 02:42.640
But the one minor slight disappointment

02:42.640 --> 02:44.880
is that it's actually not possible, at least given

02:44.880 --> 02:47.000
today's tools and understanding,

02:47.000 --> 02:52.480
to find the best short program that explains or generates

02:52.480 --> 02:55.800
or solves your problem given your data.

02:55.800 --> 02:59.480
This problem is computationally intractable.

02:59.480 --> 03:04.040
The space of all programs is a very nasty space.

03:04.040 --> 03:06.760
Small changes to your program result in massive changes

03:06.760 --> 03:08.840
to the behavior of the program, as it should be.

03:08.840 --> 03:10.120
It makes sense.

03:10.120 --> 03:11.320
You have a loop.

03:11.320 --> 03:13.440
It changed the inside of the loop.

03:13.440 --> 03:15.840
Of course, you get something totally different.

03:15.840 --> 03:17.960
So the space of programs is so hard,

03:17.960 --> 03:19.360
at least given what we know today,

03:19.360 --> 03:24.480
search there seems to be completely off the table.

03:24.480 --> 03:28.920
Well, if we give up on short programs,

03:28.920 --> 03:31.960
what about small circuits?

03:31.960 --> 03:34.840
Well, it turns out that we are lucky.

03:34.840 --> 03:37.560
It turns out that when it comes to small circuits,

03:37.560 --> 03:40.360
you can just find the best small circuit that

03:40.360 --> 03:43.080
solves your problem using back propagation.

03:43.120 --> 03:49.600
And this is the miraculous fact on which the rest of AI stands.

03:49.600 --> 03:52.360
It is the fact that when you have a circuit

03:52.360 --> 03:54.320
and you impose constraints on your circuits,

03:54.320 --> 03:58.480
on your circuit using data, you can find a way

03:58.480 --> 04:01.120
to satisfy these constraints, these constraints

04:01.120 --> 04:06.000
using backprop, by iteratively making small changes

04:06.000 --> 04:08.240
to the weights of your neural network

04:08.240 --> 04:13.120
until its predictions satisfy the data.

04:13.120 --> 04:16.880
What this means is that the computational problem that's

04:16.880 --> 04:19.680
solved by back propagation is extremely profound.

04:19.680 --> 04:21.400
It is circuit search.

04:21.400 --> 04:24.800
Now, we know that you can solve it always,

04:24.800 --> 04:26.680
but you can solve it sometimes.

04:26.680 --> 04:30.480
And you can solve it at those times

04:30.480 --> 04:32.240
where we have a practical data set.

04:32.240 --> 04:35.120
It is easy to design artificial data sets for which you cannot

04:35.120 --> 04:36.720
find the best neural network.

04:36.720 --> 04:40.040
But in practice, that seems to be not a problem.

04:40.040 --> 04:42.240
You can think of training a neural network

04:42.240 --> 04:45.840
as solving a neural equation in many cases

04:45.840 --> 04:49.880
where you have a large number of equation terms

04:49.880 --> 04:52.760
like this, f of x i theta equals y i.

04:52.760 --> 04:54.760
So you've got your parameters, and they represent

04:54.760 --> 04:57.240
all your degrees of freedom.

04:57.240 --> 05:01.160
And you use gradient descent to push the information

05:01.160 --> 05:05.680
from these equations into the parameters to satisfy them all.

05:05.680 --> 05:08.680
And you can see that a neural network, let's say one

05:08.680 --> 05:14.080
with 50 layers, is basically a parallel computer that

05:14.080 --> 05:17.440
is given 50 time steps to run.

05:17.440 --> 05:19.800
And you can do quite a lot with 50 time steps

05:19.800 --> 05:23.960
of a very, very powerful massively parallel computer.

05:23.960 --> 05:29.400
So for example, I think it is not widely known

05:29.440 --> 05:36.520
that you can learn to sort n n bit numbers using

05:36.520 --> 05:40.600
a modestly sized neural network with just two hidden layers,

05:40.600 --> 05:42.720
which is not bad.

05:42.720 --> 05:45.680
It's not self-evident, especially since we've

05:45.680 --> 05:49.680
been taught that sorting requires log n parallel steps.

05:49.680 --> 05:52.720
With a neural network, you can sort successfully

05:52.720 --> 05:54.840
using only two parallel steps.

05:54.840 --> 05:58.440
So there's something slightly obvious going on.

05:58.440 --> 06:01.200
Now these are parallel steps of threshold neurons.

06:01.200 --> 06:03.160
So they're doing a little bit more work.

06:03.160 --> 06:04.480
That's the answer to the mystery.

06:04.480 --> 06:05.920
But if you've got 50 such layers,

06:05.920 --> 06:08.520
you can do quite a bit of logic, quite a bit of reasoning,

06:08.520 --> 06:10.320
all inside the neural network.

06:10.320 --> 06:12.520
And that's why it works.

06:12.520 --> 06:17.000
Given the data, we are able to find the best neural network.

06:17.000 --> 06:18.760
And because the neural network is deep,

06:18.760 --> 06:22.960
because it can run computation inside of its layers,

06:22.960 --> 06:25.840
the best neural network is worth finding.

06:25.840 --> 06:27.240
Because that's really what you need.

06:27.240 --> 06:29.360
You need something.

06:29.360 --> 06:33.480
You need a model class, which is worth optimizing.

06:33.480 --> 06:35.720
But it also needs to be optimizable.

06:35.720 --> 06:39.400
And deep neural networks satisfy both of these constraints.

06:39.400 --> 06:40.800
And this is why everything works.

06:40.800 --> 06:45.440
This is the basis on which everything else resides.

06:45.440 --> 06:48.160
Now I want to talk a little bit about reinforcement learning.

06:48.160 --> 06:50.840
So reinforcement learning is a framework.

06:50.840 --> 06:55.520
It's a framework of evaluating agents in their ability

06:55.520 --> 06:59.360
to achieve goals in complicated stochastic environments.

06:59.360 --> 07:01.520
You've got an agent, which is plugged into an environment,

07:01.520 --> 07:04.120
as shown in the figure right here.

07:04.120 --> 07:09.760
And for any given agent, you can simply run it many times

07:09.760 --> 07:12.800
and compute its average reward.

07:12.800 --> 07:15.160
Now, the thing that's interesting about the reinforcement

07:15.160 --> 07:19.960
learning framework is that there exist interesting,

07:19.960 --> 07:22.960
useful reinforcement learning algorithms.

07:22.960 --> 07:25.400
The framework existed for a long time.

07:25.400 --> 07:27.480
It became interesting once we realized

07:27.480 --> 07:29.040
that good algorithms exist.

07:29.040 --> 07:31.000
Now, these are not perfect algorithms,

07:31.000 --> 07:35.080
but they are good enough to do interesting things.

07:35.080 --> 07:38.280
And all you want, the mathematical problem,

07:38.280 --> 07:43.320
is one where you need to maximize the expected reward.

07:43.320 --> 07:46.960
Now, one important way in which the reinforcement learning

07:46.960 --> 07:49.160
framework is not quite complete is

07:49.160 --> 07:52.440
that it assumes that the reward is given by the environment.

07:52.440 --> 07:54.280
You see this picture.

07:54.280 --> 07:57.400
The agent sends an action, while the reward sends it

07:57.400 --> 08:00.000
an observation, and both the observation

08:00.000 --> 08:01.520
and the reward backwards.

08:01.520 --> 08:04.400
That's what the environment communicates back.

08:04.400 --> 08:07.480
The way in which this is not the case in the real world

08:07.480 --> 08:14.200
is that we figure out what the reward is from the observation.

08:14.200 --> 08:16.200
We reward ourselves.

08:16.200 --> 08:16.840
We are not told.

08:16.840 --> 08:20.240
The environment doesn't say, hey, here's some negative reward.

08:20.240 --> 08:22.720
It's our interpretation of our senses

08:22.720 --> 08:25.960
that lets us determine what the reward is.

08:25.960 --> 08:28.280
And there is only one real true reward in life,

08:28.280 --> 08:31.120
and this is existence or nonexistence.

08:31.120 --> 08:34.040
And everything else is a corollary of that.

08:34.040 --> 08:36.760
So, well, what should the agent be?

08:36.760 --> 08:38.000
You already know the answer.

08:38.000 --> 08:40.200
It should be a neural network.

08:40.200 --> 08:42.200
Because whenever you want to do something,

08:42.200 --> 08:44.080
the answer is going to be a neural network,

08:44.080 --> 08:47.320
and you want the agent to map observations to actions.

08:47.320 --> 08:49.800
So you let it be parametrized with a neural net,

08:49.800 --> 08:51.640
and you apply a learning algorithm.

08:51.680 --> 08:54.680
So I want to explain to you how reinforcement learning works.

08:54.680 --> 08:56.440
This is model-free reinforcement learning.

08:56.440 --> 08:57.760
The reinforcement learning is actually

08:57.760 --> 08:59.040
being used in practice everywhere.

09:01.920 --> 09:04.320
But it's also deeply, it's very robust.

09:04.320 --> 09:05.760
It's very simple.

09:05.760 --> 09:07.680
It's also not very efficient.

09:07.680 --> 09:09.040
So the way it works is the following.

09:09.040 --> 09:11.520
This is literally the one-sentence description

09:11.520 --> 09:13.760
of what happens.

09:13.760 --> 09:18.040
In short, try something new.

09:18.040 --> 09:21.160
Add randomness to your actions.

09:21.160 --> 09:25.040
And compare the result to your expectation.

09:25.040 --> 09:28.240
If the result surprises you, if you

09:28.240 --> 09:31.560
find that the result exceeded your expectation,

09:31.560 --> 09:33.880
then change your parameters to take those actions

09:33.880 --> 09:35.680
in the future.

09:35.680 --> 09:36.640
That's it.

09:36.640 --> 09:38.960
This is the full idea of reinforcement learning.

09:38.960 --> 09:40.160
Try it out.

09:40.160 --> 09:41.280
See if you like it.

09:41.280 --> 09:45.080
And if you do, do more of that in the future.

09:45.080 --> 09:45.960
And that's it.

09:45.960 --> 09:47.240
That's literally it.

09:47.240 --> 09:48.880
This is the core idea.

09:48.880 --> 09:50.680
Now, it turns out it's not difficult to formalize

09:50.680 --> 09:51.720
mathematically.

09:51.720 --> 09:53.160
But this is really what's going on.

09:53.160 --> 09:57.120
If in a neural network, in a regular neural network, like

09:57.120 --> 09:59.520
this, you might say, OK, what's the goal?

09:59.520 --> 10:01.040
You run the neural network.

10:01.040 --> 10:02.160
You get an answer.

10:02.160 --> 10:05.040
You compare it to the desired answer.

10:05.040 --> 10:06.720
And whatever difference you have between those two, you

10:06.720 --> 10:10.560
send it back to change the neural network.

10:10.560 --> 10:11.800
That's supervised learning.

10:11.800 --> 10:14.920
In reinforcement learning, you run a neural network.

10:14.920 --> 10:17.800
You add a bit of randomness to your action.

10:17.800 --> 10:20.400
And then if you like the result, your randomness turns

10:20.400 --> 10:23.360
into the desired target in effect.

10:23.360 --> 10:25.320
So that's it.

10:25.320 --> 10:28.320
Trivial.

10:28.320 --> 10:30.200
Now, math exists.

10:33.520 --> 10:36.400
Without explaining what these equations mean, the point is

10:36.400 --> 10:39.800
not really to derive them, but just to show that they exist.

10:39.800 --> 10:41.480
There are two classes of reinforcement learning

10:41.480 --> 10:42.600
algorithms.

10:42.600 --> 10:46.080
One of them is the policy gradient, where basically

10:46.080 --> 10:49.080
what you do is that you take this expression right there,

10:49.080 --> 10:53.160
the sum of rewards, and you just crunch through the

10:53.160 --> 10:57.960
derivatives, you expand the terms, you do some algebra,

10:57.960 --> 11:00.560
and you get a derivative.

11:00.560 --> 11:03.880
And miraculously, the derivative has exactly the

11:03.880 --> 11:09.360
form that I told you, which is try some actions.

11:09.360 --> 11:11.080
And if you like them, increase the

11:11.080 --> 11:12.600
low probability of the actions.

11:12.600 --> 11:14.040
That literally follows from the math.

11:14.040 --> 11:17.680
It's very nice when the intuitive explanation has a

11:17.680 --> 11:20.200
one-to-one correspondence to what you get in the equation.

11:20.200 --> 11:22.600
Even though you'll have to take my word for it, if you're not

11:22.600 --> 11:26.240
familiar with it, that's the equation at the top.

11:26.240 --> 11:27.920
Then there is a different class of reinforcement learning

11:27.920 --> 11:30.480
algorithms, which is a little bit more difficult to explain.

11:30.480 --> 11:32.720
It's called the Q learning-based algorithms.

11:32.720 --> 11:38.840
They are a bit less stable, a bit more sample efficient.

11:38.840 --> 11:44.360
And it has the property that it can learn not only from the

11:44.360 --> 11:47.640
data generated by the actor, but from any other data as

11:47.640 --> 11:48.140
well.

11:48.140 --> 11:52.080
So it has a different robustness profile, which will be a

11:52.080 --> 11:53.360
little bit important.

11:53.360 --> 11:56.480
But it's only going to be a technicality.

11:56.480 --> 11:59.240
So yeah, this is the on policy, off policy distinction.

11:59.240 --> 12:00.360
But it's a little bit technical.

12:00.360 --> 12:03.520
So if you find this hard to understand, don't worry

12:03.520 --> 12:04.200
about it.

12:04.200 --> 12:08.400
If you already know it, then you already know it.

12:08.400 --> 12:10.000
So now what's the potential for reinforcement learning?

12:10.000 --> 12:11.760
What's the promise?

12:11.760 --> 12:13.440
What is it actually?

12:13.440 --> 12:16.320
Why should we be excited about it?

12:16.320 --> 12:17.800
Now, there are two reasons.

12:17.800 --> 12:21.640
The reinforcement learning algorithms of today are already

12:21.640 --> 12:22.760
useful and interesting.

12:22.760 --> 12:25.360
And especially if you have a really good simulation of

12:25.360 --> 12:27.720
your world, you could train agents to do lots of

12:27.720 --> 12:28.960
interesting things.

12:31.120 --> 12:34.120
But what's really exciting is if you can build a super

12:34.120 --> 12:36.800
amazing sample efficient reinforcement learning

12:36.800 --> 12:40.040
algorithm, which is give it a tiny amount of data, and the

12:40.040 --> 12:42.200
algorithm just crunches through it and extracts every

12:42.200 --> 12:44.920
bit of entropy out of it in order to learn in

12:44.920 --> 12:47.160
the fastest way possible.

12:47.160 --> 12:49.680
Now, today our algorithms are not particularly

12:49.680 --> 12:50.360
data efficient.

12:50.360 --> 12:52.800
They are data inefficient.

12:52.800 --> 12:57.480
But as our field keeps making progress, this will change.

12:57.480 --> 13:02.760
Next, I want to dive into the topic of meta learning.

13:02.760 --> 13:06.000
The goal of meta learning, so meta learning is a beautiful

13:06.000 --> 13:10.640
idea that doesn't really work, but it kind of works.

13:10.640 --> 13:11.760
And it's really promising too.

13:11.760 --> 13:14.600
It's another promising idea.

13:14.640 --> 13:16.600
So what's the dream?

13:16.600 --> 13:19.200
We have some learning algorithms.

13:19.200 --> 13:21.480
Perhaps we could use those learning algorithms in order

13:21.480 --> 13:23.720
to learn to learn.

13:23.720 --> 13:26.760
It would be nice if we could learn to learn.

13:26.760 --> 13:28.840
So how would you do that?

13:28.840 --> 13:34.680
You would take a system which you train it, not on one task,

13:34.680 --> 13:37.640
but on many tasks, and you ask it that it learns to solve

13:37.640 --> 13:40.360
these tasks quickly.

13:40.360 --> 13:42.480
And that may actually be enough.

13:42.480 --> 13:43.320
So here's how it looks like.

13:43.320 --> 13:46.080
Here's how most traditional meta learning

13:46.080 --> 13:47.920
looks like.

13:47.920 --> 13:50.960
You have a model which is a big neural network.

13:50.960 --> 13:56.360
But what you do is that you treat every, instead of

13:56.360 --> 13:58.960
training cases, you have training tasks.

13:58.960 --> 14:01.240
And instead of test cases, you have test tasks.

14:01.240 --> 14:04.960
So your input may be, instead of just your current test

14:04.960 --> 14:08.520
case, it would be all the information about the test

14:08.520 --> 14:10.960
tasks plus the test case.

14:10.960 --> 14:14.000
And you'll try to output the prediction or action for that

14:14.000 --> 14:14.840
test case.

14:14.840 --> 14:17.920
So basically you say, yeah, I'm going to give you your 10

14:17.920 --> 14:21.640
examples as part of your input to your model, figure out how

14:21.640 --> 14:24.280
to make the best use of them.

14:24.280 --> 14:27.520
It's a really straightforward idea.

14:27.520 --> 14:30.880
You turn the neural network into the learning algorithm by

14:30.880 --> 14:34.760
turning a training task into a training case.

14:34.760 --> 14:37.520
So training task equals training case.

14:37.520 --> 14:39.120
This is meta learning.

14:39.120 --> 14:40.120
This one sentence.

14:43.160 --> 14:45.680
And so there have been several success stories which I

14:45.680 --> 14:48.440
think are very interesting.

14:48.440 --> 14:51.280
One of the success stories of meta learning is learning to

14:51.280 --> 14:53.400
recognize characters quickly.

14:53.400 --> 14:58.960
So there have been a data set produced by MIT by Lake et al.

14:58.960 --> 15:02.720
And this is a data set.

15:02.720 --> 15:04.480
We have a large number of different handwritten

15:04.480 --> 15:06.160
characters.

15:06.160 --> 15:08.860
And people have been able to train extremely strong

15:08.860 --> 15:11.940
meta learning system for this task.

15:11.940 --> 15:15.460
Another very successful example of meta learning is that of

15:15.460 --> 15:21.620
neural architecture search by Zopen Lee from Google, where

15:21.620 --> 15:24.100
they found a neural architecture that solved one

15:24.100 --> 15:26.180
problem well, a small problem.

15:26.180 --> 15:27.900
And then it could generalize, and then it would successfully

15:27.900 --> 15:29.260
solve large problems as well.

15:29.260 --> 15:34.620
So this is kind of the small number of bits meta learning.

15:34.620 --> 15:36.580
It's like when you learn the architecture, or maybe even

15:36.580 --> 15:38.420
learn a program, a small program, or a learning

15:38.420 --> 15:40.660
algorithm, it should apply to new tasks.

15:40.660 --> 15:43.500
So this is the other way of doing meta learning.

15:43.500 --> 15:47.060
So anyway, but the point is what's really happening in

15:47.060 --> 15:50.900
meta learning in most cases is that you turn a training

15:50.900 --> 15:54.500
task into a training case and pretend that this is totally

15:54.500 --> 15:56.460
normal, normal deep learning.

15:56.460 --> 15:57.420
That's it.

15:57.420 --> 15:59.380
This is the entirety of meta learning.

15:59.380 --> 16:03.660
Everything else suggests minor details.

16:03.660 --> 16:05.140
Next, I want to dive in.

16:05.140 --> 16:07.820
So now that I've finished the introduction section, I want to

16:07.820 --> 16:11.820
start discussing different work by different people from

16:11.820 --> 16:12.860
OpenAI.

16:12.860 --> 16:14.980
And I want to start by talking about hindsight

16:14.980 --> 16:16.620
experience replay.

16:16.620 --> 16:20.380
There's been a large effort by Andriy Khopich et al. to

16:20.380 --> 16:25.420
develop a learning algorithm for reinforcement learning that

16:25.420 --> 16:29.940
doesn't solve just one task, but it solves many tasks.

16:29.940 --> 16:33.660
And it learns to make use of its experience in a much

16:33.660 --> 16:35.980
more efficient way.

16:35.980 --> 16:37.860
And I want to discuss one problem in reinforcement

16:37.860 --> 16:38.620
learning.

16:38.620 --> 16:42.140
It's actually, I guess, a set of problems which are related to

16:42.140 --> 16:43.140
each other.

16:47.140 --> 16:49.300
But one really important thing you need to learn to do is to

16:49.300 --> 16:50.940
explore.

16:50.940 --> 16:55.100
You start out in an environment you don't know what to do.

16:55.100 --> 16:56.460
What do you do?

16:56.460 --> 16:58.660
So one very important thing that has to happen is that you

16:58.660 --> 17:01.140
must get rewards from time to time.

17:01.180 --> 17:06.260
If you try something and you don't get rewards, then how

17:06.260 --> 17:09.060
can you learn?

17:09.060 --> 17:11.380
So that's the kind of the crux of the problem.

17:11.380 --> 17:12.420
How do you learn?

17:12.420 --> 17:19.460
And relatedly, is there any way to meaningfully benefit from

17:19.460 --> 17:23.340
the experience, from your attempts, from your failures?

17:23.340 --> 17:25.780
If you try to achieve a goal and you fail, can you still

17:25.780 --> 17:27.220
learn from it?

17:27.220 --> 17:29.980
Instead of asking your algorithm to achieve a single

17:29.980 --> 17:32.780
goal, you want to learn a policy that can achieve a very

17:32.780 --> 17:34.380
large family of goals.

17:34.380 --> 17:37.220
For example, instead of reaching one state, you want to

17:37.220 --> 17:40.820
learn a policy that reaches every state of your system.

17:40.820 --> 17:42.740
Now, what's the implication?

17:42.740 --> 17:46.860
Anytime you do something, you achieve some state.

17:46.860 --> 17:51.340
So let's suppose you say, I want to achieve state A. I

17:51.340 --> 17:56.220
try my best and I end up achieving state B. I can

17:56.220 --> 17:58.780
either conclude, while I was disappointing, I haven't learned

17:58.780 --> 18:00.500
almost anything.

18:00.500 --> 18:04.260
I still have no idea how to achieve state A. But

18:04.260 --> 18:07.180
alternatively, I can say, well, wait a second, I've just

18:07.180 --> 18:10.660
reached a perfectly good state, which is B. Can I learn

18:10.660 --> 18:14.820
how to achieve state B from my attempt to achieve state A?

18:14.820 --> 18:16.460
And the answer is yes, you can.

18:16.460 --> 18:17.860
And it just works.

18:17.860 --> 18:20.460
And I just want to point out, this is the one case.

18:20.460 --> 18:25.780
There's a small subtlety here, which may be interesting to

18:25.780 --> 18:28.020
those of you who are very familiar with on-pot, be the

18:28.020 --> 18:31.260
distinction between on-policy and off-policy.

18:31.260 --> 18:35.380
When you try to achieve A, you're doing on-policy learning

18:35.380 --> 18:38.980
for reaching the state A. But you're doing off-policy

18:38.980 --> 18:41.700
learning for reaching the state B. Because you would take

18:41.700 --> 18:44.660
different actions if you would actually try to reach state B.

18:44.660 --> 18:46.780
So that's why it's very important that the algorithm

18:46.780 --> 18:49.780
you use here can support off-policy learning.

18:49.780 --> 18:52.020
But that's a minor technicality.

18:52.020 --> 18:57.660
At the crux of the idea is you make the problem easier by

18:57.660 --> 19:00.540
ostensibly making it harder.

19:00.540 --> 19:04.260
By training a system which aspires to reach, to learn to

19:04.260 --> 19:07.860
reach every state, to learn to achieve every goal, to learn

19:07.860 --> 19:12.300
to master its environment in general, you build a system

19:12.300 --> 19:15.100
which always learns something.

19:15.100 --> 19:17.620
It learns from success as well as from failure.

19:17.620 --> 19:20.500
Because if it tries to do one thing and it does something

19:20.500 --> 19:23.180
else, it now has training data for how to achieve that

19:23.180 --> 19:24.820
something else.

19:24.820 --> 19:26.340
I want to show you a video of how this thing

19:26.340 --> 19:27.900
works in practice.

19:27.900 --> 19:32.260
So one challenge in reinforcement learning systems is

19:32.260 --> 19:34.260
the need to shape the reward.

19:34.260 --> 19:36.260
So what does it mean?

19:36.260 --> 19:38.980
It means that at the beginning of the system, at the start

19:38.980 --> 19:42.340
of learning when the system doesn't know much, it will

19:42.340 --> 19:43.860
probably not achieve your goal.

19:43.860 --> 19:46.180
And so it's important that you design your reward function

19:46.180 --> 19:48.140
to give it gradual increments, to make it

19:48.140 --> 19:49.980
smooth and continuous so that even when the system is not

19:49.980 --> 19:52.260
very good, it achieves the goal.

19:52.260 --> 19:55.780
Now, if you give your system a very sparse reward, where the

19:55.780 --> 20:00.500
reward is achieved only when you reach a final state, then it

20:00.500 --> 20:03.300
becomes very hard for normal reinforcement learning

20:03.300 --> 20:04.540
algorithms to solve a problem.

20:04.540 --> 20:07.180
Because naturally, you never get the reward, so you never

20:07.180 --> 20:07.980
learn.

20:07.980 --> 20:10.100
No reward means no learning.

20:10.100 --> 20:13.500
But here, because you learn from failure as well as from

20:13.500 --> 20:17.540
success, this problem simply doesn't occur.

20:17.540 --> 20:19.140
And so this is nice.

20:19.140 --> 20:22.740
I think let's look at the videos a little bit more.

20:22.740 --> 20:26.300
It's nice how it confidently and energetically moves the

20:26.300 --> 20:29.140
little green pack to its target.

20:29.140 --> 20:30.380
And here's another one.

20:50.140 --> 20:51.740
OK, so we can skip the physics.

20:51.740 --> 20:54.540
It works if you do it on a physical robot as well, but we

20:54.540 --> 20:56.460
can skip it.

20:56.460 --> 20:59.940
So I think the point is that the hindsight experience replay

20:59.940 --> 21:05.540
algorithm is directionally correct because you want to

21:05.540 --> 21:08.820
make use of all your data and not only a small fraction of

21:08.820 --> 21:10.020
it.

21:10.020 --> 21:14.140
Now, one huge question is, where do you get the high level

21:14.140 --> 21:15.700
states?

21:15.740 --> 21:19.140
Where do the high level states come from?

21:19.140 --> 21:23.140
Because in the work that I've shown you so far, the system

21:23.140 --> 21:25.100
is asked to achieve low level states.

21:25.100 --> 21:28.060
So I think one thing that will become very important for

21:28.060 --> 21:31.220
these kind of approaches is representation learning and

21:31.220 --> 21:32.660
unsupervised learning.

21:32.660 --> 21:36.340
Figure out what are the right states, what's the state

21:36.340 --> 21:41.340
space of goals that's worth achieving?

21:41.340 --> 21:47.260
Now I want to go through some real meta-learning results.

21:47.260 --> 21:52.140
And I'll show you a very simple way of doing seem-to-real

21:52.140 --> 21:56.900
from simulation to the physical robot with meta-learning.

21:56.900 --> 22:00.860
And this is work by Pangadal, was a really nice intern

22:00.860 --> 22:04.060
project in 2017.

22:04.060 --> 22:09.180
So I think we can agree that in the domain of robotics, it

22:09.180 --> 22:13.820
would be nice if you could train your policy in simulation

22:13.820 --> 22:18.580
and then somehow this knowledge would carry over to the

22:18.580 --> 22:21.060
physical robot.

22:21.060 --> 22:27.340
Now, we can build simulators that are OK, but they can never

22:27.340 --> 22:30.620
perfectly match the real world unless you want to have an

22:30.620 --> 22:32.620
insanely slow simulator.

22:32.620 --> 22:38.020
And the reason for that is that it turns out that simulating

22:38.020 --> 22:40.260
contacts is super hard.

22:40.260 --> 22:43.260
And I heard somewhere, correct me if I'm wrong, that

22:43.260 --> 22:45.620
simulating friction is NP-complete.

22:45.620 --> 22:46.660
I'm not sure.

22:46.660 --> 22:49.780
But it's like stuff like that.

22:49.780 --> 22:53.900
So your simulation is just not going to match reality.

22:53.900 --> 22:56.260
There'll be some resemblance, but that's it.

22:56.260 --> 22:59.300
How can we address this problem?

22:59.300 --> 23:00.820
And I want to show you one simple idea.

23:01.140 --> 23:06.620
So let's say one thing that would be nice is that if you

23:06.620 --> 23:12.980
could learn a policy that would quickly adapt itself to the

23:12.980 --> 23:16.620
real world, well, if you want to learn a policy that can

23:16.620 --> 23:19.740
quickly adapt, we need to make sure that it has opportunities

23:19.740 --> 23:21.460
to adapt during training time.

23:21.460 --> 23:22.900
So what do we do?

23:22.900 --> 23:28.380
Instead of solving a problem in just one simulator, we

23:28.500 --> 23:30.820
add a huge amount of variability to the simulator.

23:30.820 --> 23:33.660
We say we will randomize the friction.

23:33.660 --> 23:37.300
So we'll randomize the masses, the length of the different

23:37.300 --> 23:41.700
objects and their dimensions.

23:41.700 --> 23:45.660
So you try to randomize physics, the simulator, in

23:45.660 --> 23:46.900
lots of different ways.

23:46.900 --> 23:50.020
And then importantly, you don't tell the policy how you

23:50.020 --> 23:51.740
randomized it.

23:51.740 --> 23:53.020
So what is it going to do then?

23:53.020 --> 23:55.500
You take your policy and you put it in an environment and

23:55.500 --> 23:57.420
it says, well, this is really tough.

23:57.420 --> 23:59.780
I don't know what the masses are and I don't know what the

23:59.780 --> 24:00.900
frictions are.

24:00.900 --> 24:03.700
I need to try things out and figure out what the friction

24:03.700 --> 24:07.300
is as I get responses from the environment.

24:07.300 --> 24:12.980
So you learn a certain degree of adaptability into the policy.

24:12.980 --> 24:14.820
And it actually works.

24:14.820 --> 24:16.140
I just want to show you.

24:16.140 --> 24:18.860
This is what happens when you just train a policy in

24:18.860 --> 24:22.020
simulation and deploy it on the physical robot.

24:22.020 --> 24:25.260
And here the guy who's going to do that, he's going to do

24:25.260 --> 24:29.540
the goal is to bring the hockey puck towards the red dot.

24:29.540 --> 24:31.860
And you will see that it will struggle.

24:37.780 --> 24:40.220
And the reason it struggles is because of the systematic

24:40.220 --> 24:44.700
differences between the simulator and the real

24:44.700 --> 24:47.300
physical robot.

24:47.300 --> 24:51.020
So even the basic movement is difficult for the policy

24:51.020 --> 24:53.300
because the assumptions are violated so much.

24:53.340 --> 24:56.020
So if you do the training as I discussed, we train a

24:56.020 --> 24:59.540
recurrent neural network policy which learns to quickly

24:59.540 --> 25:03.220
infer properties of the simulator in order to

25:03.220 --> 25:04.500
accomplish the task.

25:04.500 --> 25:07.620
You can then give it the real thing, the real physics, and

25:07.620 --> 25:10.060
it will do much better.

25:10.060 --> 25:11.780
So now this is not a perfect technique, but it's

25:11.780 --> 25:12.820
definitely very promising.

25:12.820 --> 25:15.900
It's promising whenever you are able to sufficiently

25:15.900 --> 25:19.020
randomize the simulator.

25:19.020 --> 25:22.100
So it's definitely very nice to see the closed loop nature

25:22.140 --> 25:22.820
of the policy.

25:22.820 --> 25:25.540
You can see that it would push the hockey puck, and it would

25:25.540 --> 25:29.180
correct it very, very gently to bring it to the goal.

25:29.180 --> 25:30.500
Yeah, you saw that?

25:30.500 --> 25:31.500
That was cool.

25:33.860 --> 25:39.380
So that was a cool application of meta-learning.

25:39.380 --> 25:41.980
I want to discuss one more application of meta-learning,

25:41.980 --> 25:46.740
which is learning a hierarchy of actions.

25:46.740 --> 25:49.260
And this was work done by France Aral.

25:49.300 --> 25:52.700
Actually, Kevin France, the ancient who did it, was in

25:52.700 --> 25:57.500
high school when he wrote this paper.

25:57.500 --> 26:07.100
So one thing that would be nice is if reinforcement

26:07.100 --> 26:09.300
learning was hierarchical.

26:09.300 --> 26:12.340
If instead of simply taking microactions, you had some

26:12.340 --> 26:16.460
kind of little subroutines that you could deploy.

26:16.460 --> 26:19.100
Maybe the term subroutine is a little bit too crude, but if

26:19.140 --> 26:23.300
you had some idea of which action primitives are

26:23.300 --> 26:25.420
worth starting with.

26:25.420 --> 26:31.860
Now, no one has been able to get actually real value add

26:31.860 --> 26:33.860
from hierarchical reinforcement learning yet.

26:33.860 --> 26:36.180
So far, all the really cool results, all the really

26:36.180 --> 26:37.900
convincing results of reinforcement learning, do not

26:37.900 --> 26:39.900
use it.

26:39.900 --> 26:43.420
That's because we haven't quite figured out what's the

26:43.420 --> 26:48.180
right way for hierarchical reinforcement learning.

26:48.180 --> 26:51.100
And I just want to show you one very simple approach where

26:51.100 --> 26:57.540
you use meta-learning to learn a hierarchy of actions.

26:57.540 --> 26:59.740
So here's what you do.

26:59.740 --> 27:07.660
You have, in this specific work, you have a certain number

27:07.660 --> 27:08.780
of low-level primitives.

27:08.780 --> 27:11.060
Let's say you have 10 of them.

27:11.060 --> 27:14.140
And you have a distribution of tasks.

27:14.140 --> 27:21.100
And your goal is to learn low-level primitives such that

27:21.100 --> 27:25.700
when they're used inside a very brief run of some reinforcement

27:25.700 --> 27:27.980
learning algorithm, you will make as much progress as

27:27.980 --> 27:30.060
possible.

27:30.060 --> 27:33.180
So the idea is you want to get the greatest amount of

27:33.180 --> 27:33.580
progress.

27:33.580 --> 27:41.140
You want to learn primitives that result in the greatest

27:41.140 --> 27:44.540
amount of progress possible when used inside learning.

27:44.540 --> 27:46.380
So this is a meta-learning setup because you need

27:46.380 --> 27:47.780
distribution of tasks.

27:47.780 --> 27:52.580
And here, we've had a little maze.

27:52.580 --> 27:53.780
You have a distribution of amazes.

27:53.780 --> 27:56.820
And in this case, the little bug learned three policies which

27:56.820 --> 28:00.460
move it in a fixed direction.

28:00.460 --> 28:02.780
And as a result of having this hierarchy, you're able to solve

28:02.780 --> 28:03.900
problems really fast.

28:03.900 --> 28:06.460
But only when the hierarchy is correct.

28:06.460 --> 28:08.060
So hierarchical reinforcement learning is still

28:08.060 --> 28:08.980
working progress.

28:08.980 --> 28:18.620
And this work is an interesting proof point of how

28:18.620 --> 28:24.020
hierarchical reinforcement learning could be like if it

28:24.020 --> 28:26.660
worked.

28:26.660 --> 28:31.540
Now, I want to just spend one slide addressing the

28:31.540 --> 28:35.220
limitations of high-capacity meta-learning.

28:35.220 --> 28:41.340
The specific limitation is that the training task

28:41.340 --> 28:44.100
distribution has to be equal to the test

28:44.100 --> 28:46.580
task distribution.

28:46.580 --> 28:50.700
And I think this is a real limitation because in reality,

28:50.700 --> 28:54.020
the new task that you want to learn will in some ways be

28:54.020 --> 28:58.260
fundamentally different from anything you've seen so far.

28:58.260 --> 29:00.540
So for example, if you go to school, you learn lots of

29:00.540 --> 29:02.740
useful things.

29:02.740 --> 29:07.340
But then when you go to work, only a fraction of the things

29:07.340 --> 29:09.340
that you've learned carries over.

29:09.340 --> 29:12.940
You need to learn quite a few more things from scratch.

29:12.940 --> 29:16.380
So meta-learning would struggle with that because it

29:16.380 --> 29:20.340
really assumes that the training data, the distribution

29:20.340 --> 29:22.500
over the training tasks has to be equal to the distribution

29:22.500 --> 29:24.100
over the test tasks.

29:24.100 --> 29:25.020
So that's a limitation.

29:25.020 --> 29:29.300
I think that as we develop better algorithms for being

29:29.300 --> 29:34.860
robust when the test tasks are outside of the distribution

29:34.860 --> 29:37.900
of the training task, then meta-learning

29:37.900 --> 29:40.340
would work much better.

29:40.340 --> 29:44.220
Now, I want to talk about self-play.

29:44.220 --> 29:49.100
I think self-play is a very cool topic that's starting to

29:49.100 --> 29:51.260
get attention only now.

29:51.260 --> 29:55.540
And I want to start by reviewing very old work

29:55.540 --> 29:57.460
called TD Gammon.

29:57.460 --> 30:01.380
It's back from all the way from 1992, so it's 26 years old

30:01.380 --> 30:02.420
now.

30:02.420 --> 30:04.140
It was done by Jared Sauron.

30:04.140 --> 30:13.500
So this work is really incredible because it has so much

30:13.500 --> 30:15.060
relevance today.

30:15.060 --> 30:20.260
What they did, basically, they said, OK, let's take two

30:20.260 --> 30:25.900
neural networks and let them play against each other, let

30:25.900 --> 30:29.300
them play backgammon against each other, and let them be

30:29.300 --> 30:31.260
trained with Q-Learning.

30:31.260 --> 30:33.940
So it's a super modern approach.

30:33.940 --> 30:39.140
And you would think this was a paper from 2017, except that

30:39.140 --> 30:41.540
when you look at this plot, it shows that you only have 10

30:41.540 --> 30:44.300
hidden units, 20 hidden units, 40 and 80 for the

30:44.300 --> 30:48.100
different colors, where you notice that the largest

30:48.100 --> 30:49.580
neural network works best.

30:49.580 --> 30:52.300
So in some ways, not much has changed, and this is the

30:52.300 --> 30:53.300
evidence.

30:54.300 --> 30:56.700
And in fact, they were able to beat the world champion in

30:56.700 --> 30:59.100
backgammon, and they were able to discover new strategies

30:59.100 --> 31:05.100
that the best human backgammon players have not noticed.

31:05.100 --> 31:07.100
And they've determined that the strategy is covered by TD

31:07.100 --> 31:08.900
Gammon are actually better.

31:08.900 --> 31:14.700
So that's pure self-play with Q-Learning, which remained

31:14.700 --> 31:19.100
dormant until the DQN worked with Atari, but in mind.

31:19.300 --> 31:27.300
So now other examples of self-play include AlphaGo Zero,

31:27.300 --> 31:29.900
which was able to learn to beat the world champion in Go

31:29.900 --> 31:33.700
without using any external data whatsoever.

31:33.700 --> 31:36.700
And now the result of this way is Biopen AI, which is our

31:36.700 --> 31:40.700
Dota 2 bot, which was able to build the world champion on

31:40.700 --> 31:41.900
the 1v1 version of the game.

31:42.100 --> 31:45.100
And so I want to spend a little bit of time talking about the

31:45.100 --> 31:49.100
allure of self-play and why I think it's exciting.

31:51.100 --> 32:00.100
So one important problem that we must face as we try to build

32:00.100 --> 32:05.100
truly intelligent systems is what is the task?

32:05.100 --> 32:07.100
What are we actually doing?

32:07.300 --> 32:12.300
And one very attractive attribute of self-play is that

32:12.300 --> 32:15.300
the agents create the environment.

32:15.300 --> 32:19.300
By virtue of the agent acting in the environment,

32:19.300 --> 32:22.300
the environment becomes difficult for the other agents.

32:22.300 --> 32:26.300
And you can see here an example of an iguana interacting

32:26.300 --> 32:28.300
with snakes that try to eat it.

32:28.300 --> 32:34.300
And that's why we're trying to build the world champion

32:34.300 --> 32:37.500
of the iguana interacting with snakes that try to eat it

32:37.500 --> 32:39.500
unsuccessfully this time.

32:39.500 --> 32:42.500
So we can see what will happen in a moment.

32:42.500 --> 32:44.500
The iguana is trying its best.

32:44.500 --> 32:48.500
And so the fact that you have this arms race between the

32:48.500 --> 32:52.500
snakes and the iguana motivates their development,

32:52.500 --> 32:55.500
potentially without bound.

32:55.500 --> 33:00.500
And this is what happened in effect in biological evolution.

33:00.700 --> 33:04.700
Now, interesting work in this direction was done in 1994

33:04.700 --> 33:05.700
by Carl Simms.

33:05.700 --> 33:09.700
There is a really cool video on YouTube by Carl Simms.

33:09.700 --> 33:12.200
You should check it out, which really kind of shows all the

33:12.200 --> 33:14.200
work that he's done.

33:14.200 --> 33:17.200
And here you have a little competition between agents

33:17.200 --> 33:20.200
where you evolve both the behavior and the morphology

33:20.200 --> 33:25.200
when the agent is trying to gain possession of a green cube.

33:27.200 --> 33:30.200
And so you can see that the agents create the challenge

33:30.200 --> 33:31.200
for each other.

33:31.200 --> 33:33.200
And that's why they need to develop.

33:34.900 --> 33:40.900
So one thing that we did, and this is work by Ansel et al

33:40.900 --> 33:43.900
from OpenAI, is we said, okay, well,

33:43.900 --> 33:48.600
can we demonstrate some unusual results in self play

33:48.600 --> 33:52.400
that would really convince us that there is something there?

33:52.400 --> 33:56.900
So what we did here is that we created a small ring

33:56.900 --> 33:58.800
and you have these two humanoid figures.

33:58.800 --> 34:02.700
And their goal is just to push each other outside the ring.

34:02.700 --> 34:05.000
And they don't know anything about wrestling.

34:05.000 --> 34:07.300
They don't know anything about standing your balance

34:07.300 --> 34:08.000
in each other.

34:08.000 --> 34:10.100
They don't know anything about centers of gravity.

34:10.100 --> 34:13.100
All they know is that if you don't do a good job,

34:13.100 --> 34:16.700
then your competition is going to do a better job.

34:16.700 --> 34:20.400
Now, one of the really attractive things about self play

34:20.400 --> 34:25.900
is that you always have an opponent that's roughly

34:25.900 --> 34:28.400
as good as you are.

34:28.400 --> 34:31.400
In order to learn, you need to sometimes win

34:31.400 --> 34:32.700
and sometimes lose.

34:32.700 --> 34:34.700
You can't always win.

34:34.700 --> 34:36.100
Sometimes you must fail.

34:36.100 --> 34:39.600
Sometimes you must succeed.

34:39.600 --> 34:42.000
So let's see what will happen here.

34:42.000 --> 34:43.700
Yeah, so it was able to be.

34:43.700 --> 34:48.400
So the green humanoid was able to block the ball.

34:48.400 --> 34:52.900
In a well-balanced self play environment,

34:52.900 --> 34:55.500
the competition is always level.

34:55.500 --> 34:58.400
No matter how good you are or how bad you are,

34:58.400 --> 35:01.600
you have a competition that makes it exactly

35:01.600 --> 35:03.100
of exactly the right challenge for you.

35:03.100 --> 35:04.100
Oh, and one thing here.

35:04.100 --> 35:06.300
So this video shows transfer learning.

35:06.300 --> 35:09.100
You take the little wrestling humanoid

35:09.100 --> 35:11.300
and you take its friend away

35:11.300 --> 35:14.500
and you start applying big, large, random forces on it.

35:14.500 --> 35:17.000
And you see if it can maintain its balance.

35:17.000 --> 35:19.900
And the answer turns out to be that yes, it can

35:19.900 --> 35:23.000
because it's been trained against an opponent

35:23.000 --> 35:24.500
and it pushes it.

35:24.500 --> 35:27.300
And so that's why even if it doesn't understand

35:27.300 --> 35:29.500
where the pressure force is being applied on it,

35:29.500 --> 35:31.500
it's still able to balance itself.

35:31.500 --> 35:35.000
So this is one potentially attractive feature

35:35.000 --> 35:36.600
of self play environments that you could learn

35:36.600 --> 35:40.200
a certain broad set of skills.

35:40.200 --> 35:42.300
Although it's a little hard to control those

35:42.300 --> 35:43.800
what the skills will be.

35:43.800 --> 35:45.900
And so the biggest open question with this research

35:45.900 --> 35:52.300
is how do you learn agents in a self play environment

35:52.300 --> 35:54.600
such that they do whatever they do,

35:54.600 --> 35:57.000
but then they are able to solve a battery of tasks

35:57.000 --> 35:58.200
that is useful for us,

35:58.200 --> 36:01.800
that is explicitly specified externally.

36:01.800 --> 36:05.000
Yeah.

36:05.000 --> 36:08.400
I also want to highlight one attribute

36:08.400 --> 36:09.500
of self play environments

36:09.500 --> 36:12.000
that we've observed in our Dota bot.

36:12.000 --> 36:14.400
And that is that we've seen a very rapid increase

36:14.400 --> 36:16.100
in the competence of the bot.

36:16.100 --> 36:19.000
So over the period, over the course of maybe five months,

36:19.000 --> 36:23.900
we've seen the bot go from playing totally randomly

36:23.900 --> 36:28.100
all the way to the world champion.

36:28.100 --> 36:29.400
And the reason for that

36:29.400 --> 36:32.200
is that once you have a self play environment,

36:32.200 --> 36:36.400
if you put compute into it, you turn it into data.

36:36.400 --> 36:40.300
Self play allows you to turn compute into data.

36:40.300 --> 36:42.300
And I think we will see a lot more of that

36:42.300 --> 36:44.900
as being an extremely important thing

36:44.900 --> 36:45.900
to be able to turn compute

36:45.900 --> 36:48.700
into essentially data or generalization.

36:48.700 --> 36:51.700
Simply because the speed of neural net processors

36:51.700 --> 36:54.900
will increase very dramatically over the next few years.

36:54.900 --> 36:56.600
So neural net cycles will be cheap

36:56.600 --> 36:58.800
and it will be important to make use of these

36:58.800 --> 37:03.300
newly found over abundance of cycles.

37:03.300 --> 37:04.400
I also want to talk a little bit

37:04.400 --> 37:08.400
about the end game of the self play approach.

37:08.400 --> 37:12.500
So one thing that we know about the human brain

37:12.500 --> 37:15.100
is that it has increased in size fairly rapidly

37:15.100 --> 37:18.700
over the past two million years.

37:18.700 --> 37:21.900
My theory, the reason I think it happened

37:21.900 --> 37:26.600
is because our ancestors got to a point

37:26.600 --> 37:29.700
where the thing that's most important for your survival

37:29.700 --> 37:32.200
is your standing in the tribe

37:32.200 --> 37:34.800
and less the tiger in the lion.

37:34.800 --> 37:36.900
Once the most important thing

37:36.900 --> 37:39.200
is how you deal with those other things

37:39.200 --> 37:40.600
which have a large brain,

37:40.600 --> 37:43.200
then it really helps to have a slightly larger brain.

37:43.200 --> 37:44.500
And I think that's what happened.

37:44.500 --> 37:47.700
And there exists at least one paper from science

37:47.700 --> 37:50.200
which supports this point of view.

37:50.200 --> 37:52.400
So apparently there has been convergent evolution

37:52.400 --> 37:56.700
between social apes and social birds

37:56.700 --> 38:00.800
even though in terms of various behaviors.

38:00.800 --> 38:04.300
Even though the divergence in evolution

38:04.300 --> 38:06.500
and timescale between humans and birds

38:06.500 --> 38:08.600
has occurred a very long time ago

38:08.600 --> 38:11.500
and humans and sorry humans apes and humans apes

38:11.500 --> 38:16.500
and birds have very different brain structure.

38:16.500 --> 38:19.900
So I think what should happen if we succeed,

38:19.900 --> 38:23.200
if we successfully follow the path of this approach

38:23.200 --> 38:25.000
is that we should create a society of agents

38:25.000 --> 38:28.700
which will have language and theory of mind,

38:28.700 --> 38:33.600
negotiation, social skills, trade, economy, politics,

38:33.600 --> 38:35.100
justice system.

38:35.100 --> 38:37.000
All these things should happen

38:37.000 --> 38:39.000
inside the multi-agent environment.

38:39.000 --> 38:40.600
And there will also be some alignment issue

38:40.600 --> 38:43.100
of how do you make sure that the agents we learn

38:43.100 --> 38:45.200
behave in a way that we want.

38:45.200 --> 38:49.800
Now I want to make a speculative digression here

38:49.800 --> 38:57.100
which is, I want to make the following observation.

38:57.100 --> 39:02.000
If you believe that this kind of society of agents

39:02.000 --> 39:08.500
is a plausible place where truly,

39:08.500 --> 39:12.200
where fully general intelligence will emerge.

39:12.200 --> 39:16.700
And if you accept that our experience with the dotabot

39:16.700 --> 39:18.700
where we've seen a very rapid increase in competence

39:18.700 --> 39:22.000
will carry over once all the details are right.

39:22.000 --> 39:24.600
If you assume both of these conditions

39:24.600 --> 39:28.000
then it should follow that we should see a very rapid increase

39:28.000 --> 39:30.700
in the competence of our agents

39:30.700 --> 39:34.500
as they leave in the society of agents.

39:34.500 --> 39:40.300
So now that we've talked about a potentially interesting way

39:40.300 --> 39:43.100
of increasing the competence and teaching agents

39:43.100 --> 39:46.600
social skills and language and a lot of things

39:46.600 --> 39:49.200
that actually exist in humans as well.

39:49.200 --> 39:53.700
We want to talk a little bit about how you convey goals

39:53.700 --> 39:55.700
to agents.

39:55.700 --> 39:59.500
And the question of conveying goals to agents

39:59.500 --> 40:01.400
is just a technical problem.

40:01.400 --> 40:04.700
But it will be important

40:04.700 --> 40:08.100
because it is more likely than not

40:08.100 --> 40:12.100
that the agents that we will train will eventually

40:12.100 --> 40:14.300
be dramatically smarter than us.

40:14.300 --> 40:17.500
And this is work by the OpenAI safety team

40:17.500 --> 40:21.700
by Paul Cruciano and others.

40:21.700 --> 40:23.500
So I'm just going to show you this video

40:23.500 --> 40:27.600
which basically explains how the whole thing works.

40:27.600 --> 40:30.600
There is some behavior you are looking for

40:30.600 --> 40:34.700
and you the human gets to see pairs of behaviors.

40:34.700 --> 40:39.500
And you simply click on the one that looks better.

40:39.500 --> 40:44.200
And after a very modest number of clicks

40:44.200 --> 40:57.500
you can get this little simulated leg to do backflips.

40:57.500 --> 40:58.800
There you go.

40:58.800 --> 41:00.000
You can now do backflips.

41:00.000 --> 41:02.300
And to get this specific behavior

41:02.300 --> 41:08.000
it took about 500 clicks by human annotators.

41:08.000 --> 41:10.500
The way it works is that you take all the...

41:10.500 --> 41:13.300
So this is a very data efficient reinforcement learning

41:13.300 --> 41:17.100
algorithm but it is efficient in terms of rewards

41:17.100 --> 41:20.400
and not in terms of the environment interactions.

41:20.400 --> 41:22.900
So what you do here is that you take all the clicks

41:22.900 --> 41:24.200
so you've got your...

41:24.200 --> 41:27.700
Here is one behavior which is better than the other.

41:27.700 --> 41:32.400
You fit a reward function, a numerical reward function

41:32.400 --> 41:33.500
to those clicks.

41:33.500 --> 41:34.800
So you want to fit a reward function

41:34.800 --> 41:36.100
which satisfies those clicks

41:36.100 --> 41:37.600
and then you optimize this reward function

41:37.600 --> 41:39.500
with reinforcement learning.

41:39.500 --> 41:41.200
And it actually works.

41:41.200 --> 41:44.800
So this requires 500 bits of information.

41:44.800 --> 41:48.000
You've also been able to train lots of Atari games

41:48.000 --> 41:49.500
using several thousand bits of information.

41:49.500 --> 41:53.000
So in all these cases you had human annotators

41:53.000 --> 41:56.600
or human judges just like in the previous slide

41:56.700 --> 42:00.500
looking at pairs of trajectories

42:00.500 --> 42:03.900
and clicking on the one that they thought was better.

42:03.900 --> 42:08.000
And here's an example of an unusual goal

42:08.000 --> 42:10.000
where this is a car racing game

42:10.000 --> 42:16.100
but the goal was to ask the agent to train the white car

42:16.100 --> 42:18.800
drive right behind the orange car.

42:18.800 --> 42:19.800
So it's a different goal

42:19.800 --> 42:23.400
and it was very straightforward to communicate this goal

42:23.400 --> 42:26.500
using this approach.

42:27.000 --> 42:31.300
So then to finish off, alignment is a technical problem.

42:31.300 --> 42:33.000
It has to be solved.

42:33.000 --> 42:36.300
But of course the determination of the correct goals

42:36.300 --> 42:38.600
we want RA systems to have

42:38.600 --> 42:42.100
will be a very challenging political problem.

42:42.100 --> 42:43.600
And on this note,

42:43.600 --> 42:45.600
I want to thank you so much for your attention

42:45.600 --> 42:48.200
and I just want to say that it will be a happy hour

42:48.200 --> 42:50.400
at Cambridge Brewing Company at 8.45.

42:50.400 --> 42:53.400
If you want to chat more about AI and other topics,

42:53.400 --> 42:54.800
please come by.

42:54.800 --> 42:56.500
I think that deserves an applause.

42:56.500 --> 42:57.600
Thank you very much.

43:03.600 --> 43:07.400
So back propagation is a...

43:07.400 --> 43:09.100
or neural networks are inspired

43:09.100 --> 43:10.700
but back propagation doesn't look as though

43:10.700 --> 43:12.500
it's what's going on in the brain

43:12.500 --> 43:15.800
because signals in the brain go one direction down the axons

43:15.800 --> 43:17.700
whereas back propagation requires the errors

43:17.700 --> 43:20.700
to be propagated back up the wires.

43:20.700 --> 43:24.200
So can you just talk a little bit

43:24.200 --> 43:26.200
about that whole situation

43:26.200 --> 43:28.600
where it looks as though the brain is doing something a bit different

43:28.600 --> 43:31.400
than our highly successful algorithms?

43:31.400 --> 43:33.500
Are algorithms going to be improved

43:33.500 --> 43:35.300
once we figure out what the brain is doing

43:35.300 --> 43:37.400
or is the brain really sending signals back

43:37.400 --> 43:40.200
even though it's got no obvious way of doing that?

43:40.200 --> 43:42.300
What's happening in that area?

43:42.300 --> 43:44.400
So that's a great question.

43:44.400 --> 43:46.500
So first of all, I'll say that the true answer

43:46.500 --> 43:48.900
is that the honest answer is that I don't know

43:48.900 --> 43:50.600
but I have opinions.

43:50.700 --> 43:55.000
And so I'll say two things.

43:55.000 --> 43:58.400
Like first of all, given that...

43:58.400 --> 44:01.200
like if you agree, if we agree like so rather

44:01.200 --> 44:04.000
it is a true fact that back propagation

44:04.000 --> 44:08.000
solves the problem of circuit search.

44:08.000 --> 44:12.200
This problem feels like an extremely fundamental problem

44:12.200 --> 44:15.100
and for this reason I think that it's unlikely to go away.

44:15.100 --> 44:18.600
Now you also write that the brain doesn't obviously

44:18.600 --> 44:21.000
do back propagation although there have been multiple proposals

44:21.000 --> 44:22.900
of how it could be doing them.

44:22.900 --> 44:29.000
For example, there's been a work by Tim Lillicrap and others

44:29.000 --> 44:31.100
where they've shown that if you use...

44:31.100 --> 44:34.400
that it's possible to learn a different set of connections

44:34.400 --> 44:36.700
that can be used for the backward pass

44:36.700 --> 44:38.700
and that can result in successful learning.

44:38.700 --> 44:42.500
Now, the reason this hasn't been like really pushed to the limit

44:42.500 --> 44:44.000
by practitioners is because they say,

44:44.000 --> 44:46.500
well, I got tf.gradients,

44:46.500 --> 44:48.600
I'm just not going to worry about it.

44:48.600 --> 44:50.600
But you are right that this is an important issue

44:50.600 --> 44:53.400
and one of two things is going to happen.

44:53.400 --> 44:56.000
So my personal opinion is that back propagation

44:56.000 --> 44:58.200
is just going to stay with us till the very end

44:58.200 --> 45:01.800
and we'll actually build fully human level and beyond systems

45:01.800 --> 45:06.200
before we understand how the brain does what it does.

45:06.200 --> 45:09.600
So that's what I believe but of course

45:09.600 --> 45:12.600
it is a difference that has to be acknowledged.

45:12.600 --> 45:14.200
Okay, thank you.

45:14.300 --> 45:18.200
Do you think it was a fair matchup for the Dota bot

45:18.200 --> 45:21.700
and that person given the constraints of the system?

45:21.700 --> 45:27.100
So I'd say that the biggest advantage computers have in games like this,

45:27.100 --> 45:32.000
like one of the big advantages is that they obviously have a better reaction time.

45:32.000 --> 45:33.900
Although in Dota in particular,

45:33.900 --> 45:39.100
the number of clicks per second over the top layer is fairly small,

45:39.100 --> 45:40.600
which is different from StarCraft.

45:40.600 --> 45:44.700
So in StarCraft, StarCraft is a very mechanically heavy game

45:44.700 --> 45:46.700
because you have a large number of units

45:46.700 --> 45:50.600
and so the top layers they just click all the time.

45:50.600 --> 45:53.400
In Dota, every player control is just one hero

45:53.400 --> 45:56.900
and so that greatly reduces the total number of actions they need to make.

45:56.900 --> 45:58.500
Now still precision matters.

45:58.500 --> 46:02.200
I think that we'll discover that,

46:02.200 --> 46:05.500
but what I think will really happen is that we'll discover that computers have

46:05.600 --> 46:12.600
the advantage in any domain or rather every domain.

46:14.600 --> 46:15.100
Not yet.

46:15.100 --> 46:18.100
So do you think that the emergent behaviors from the agent

46:18.100 --> 46:20.500
were actually kind of directed

46:20.500 --> 46:22.700
because the constraints were already kind of in place?

46:22.700 --> 46:24.100
Like so it was kind of forced to discover those

46:24.100 --> 46:28.000
or do you think that like that was actually something quite novel

46:28.000 --> 46:31.000
that like, wow, it actually discovered these on its own?

46:31.000 --> 46:33.600
Like you didn't actually bias towards constraining it?

46:33.600 --> 46:35.600
So it's definitely de-discovering new strategies

46:35.600 --> 46:39.300
and I can share an anecdote where our tester,

46:39.300 --> 46:42.100
we have a pro which would test the bot

46:42.100 --> 46:44.600
and he played against it for a long time

46:44.600 --> 46:48.000
and the bot would do all kinds of things against the player,

46:48.000 --> 46:50.100
the human player, which were effective.

46:50.100 --> 46:55.000
Then at some point that pro decided to play against the better pro

46:55.000 --> 46:58.200
and he decided to imitate one of the things that the bot was doing

46:58.200 --> 47:03.400
and by imitating it he was able to defeat a better pro.

47:03.400 --> 47:06.200
So I think the strategies that he discovers are real

47:06.200 --> 47:11.500
and so like it means that there's very real transfer between, you know,

47:11.500 --> 47:15.800
I would say I think what that means is that the,

47:15.800 --> 47:18.500
because the strategies discovered by the bot help the humans,

47:18.500 --> 47:23.000
it means that the fundamental gameplay is deeply related.

47:23.000 --> 47:27.400
For a long time now I've heard that the objective of reinforcement learning

47:27.400 --> 47:31.300
is to determine a policy that chooses an action

47:31.300 --> 47:36.200
to maximize the expected reward, which is what you said earlier.

47:36.200 --> 47:41.500
Would you ever want to look at the standard deviation of possible rewards?

47:41.500 --> 47:42.700
Does that even make sense?

47:42.700 --> 47:47.500
Yeah, I mean I think for sure, I think it's really application dependent.

47:47.500 --> 47:50.900
One of the reasons to maximize the expected reward

47:50.900 --> 47:54.700
is because it's easier to design algorithms for it.

47:54.700 --> 47:59.200
So you write down this equation, the formula,

47:59.200 --> 48:00.800
you do a little bit of derivation,

48:00.800 --> 48:03.900
you get something which amounts to a nice-looking algorithm.

48:03.900 --> 48:08.700
Now, I think there exists like really,

48:08.700 --> 48:11.000
there exist applications where you never want to make mistakes

48:11.000 --> 48:13.900
and you want to work on the standard deviation as well.

48:13.900 --> 48:17.800
But in practice it seems that the, just looking at the expected reward

48:17.800 --> 48:24.900
covers a large fraction of the situation as you'd like to apply this to.

48:24.900 --> 48:25.600
Okay, thanks.

48:25.600 --> 48:26.100
Thank you.

48:29.500 --> 48:37.100
We talked last week about motivations and that has a lot to do with the reinforcement.

48:37.100 --> 48:43.800
And some of the ideas is that the, our motivations are actually connection

48:43.800 --> 48:46.300
with others and cooperation.

48:46.300 --> 48:50.300
And I'm wondering if, Thurnoff, and I understand it's very popular

48:50.300 --> 48:54.800
to have the computers play these competitive games.

48:54.800 --> 49:03.700
But is there any use in like having an agent self-play collaboratively, collaborative games?

49:03.700 --> 49:07.600
Yeah, I think that's an extremely good question.

49:07.600 --> 49:10.800
I think one place from which we can get some inspiration

49:10.800 --> 49:13.500
is from the evolution of cooperation.

49:13.500 --> 49:19.700
Like, I think cooperation, we cooperate ultimately

49:19.700 --> 49:24.800
because it's much better for you, the person to be cooperative than not.

49:24.800 --> 49:33.500
And so, I think what should happen, if you have a sufficiently open-ended game,

49:33.500 --> 49:36.700
then cooperation will be the winning strategy.

49:36.700 --> 49:43.800
And so I think we will get cooperation, whether we like it or not.

49:43.900 --> 49:50.100
Hey, you mentioned the complexity of the simulation of friction.

49:50.100 --> 49:53.600
I was wondering if you feel that there exists open complexity,

49:53.600 --> 49:56.800
theoretic problems relevant to, relevant to AI,

49:56.800 --> 49:59.700
or whether it's just a matter of finding good approximations

49:59.700 --> 50:04.100
that humans, of the types of problems that humans tend to solve.

50:04.100 --> 50:07.100
Yeah, so complexity theory.

50:07.100 --> 50:10.800
Well, like at a very basic level,

50:10.900 --> 50:13.900
we know that whatever algorithm we're going to run

50:13.900 --> 50:16.900
is going to run fairly efficiently on some hardware.

50:16.900 --> 50:23.400
So that puts a pretty strict upper bound on the true complexity of the problems we are solving.

50:23.400 --> 50:28.700
Like, by definition, we are solving problems which aren't too hard in a complexly theoretic sense.

50:28.700 --> 50:33.400
Now, it is also the case that many of the problems,

50:33.400 --> 50:38.400
so while the overall thing that we do is not hard from a complexity-theoretic sense,

50:38.400 --> 50:43.400
and indeed, humans cannot solve NP-complete problems in general.

50:43.400 --> 50:48.400
It is true that many of the optimization problems that we pose to our algorithms

50:48.400 --> 50:53.400
are intractable in the general case, starting from neural net optimization itself.

50:53.400 --> 50:58.400
It is easy to create a family of datasets for a neural network with a very small number of neurons,

50:58.400 --> 51:01.400
such that finding the global optimum is NP-complete.

51:01.400 --> 51:04.400
And so, how do we avoid it?

51:04.400 --> 51:08.400
Well, we just try gradient descent anyway, and somehow it works.

51:08.400 --> 51:17.400
But without question, we do not solve problems which are truly intractable.

51:17.400 --> 51:20.400
So, I mean, I hope this answers the question.

51:20.400 --> 51:28.400
Hello. It seems like an important sub-problem on the path towards AGI will be understanding language,

51:28.400 --> 51:32.400
and the state of generative language modeling right now is pretty abysmal.

51:32.400 --> 51:37.400
What do you think are the most productive research trajectories towards generative language models?

51:37.400 --> 51:44.400
So, I'll first say that you are completely correct that the situation with language is still far from great,

51:44.400 --> 51:52.400
although progress has been made, even without any particular innovations beyond models that exist today.

51:52.400 --> 51:58.400
Simply scaling up models that exist today on larger datasets is going to go surprisingly far,

51:58.400 --> 52:01.400
not even larger datasets, but larger and deeper models.

52:01.400 --> 52:06.400
For example, if you trained a language model with a thousand layers, and it's the same layer,

52:06.400 --> 52:10.400
I think it's going to be a pretty amazing language model.

52:10.400 --> 52:15.400
Like, we don't have the cycles for it yet, but I think it will change very soon.

52:15.400 --> 52:24.400
Now, I also agree with you that there are some fundamental things missing in our current understanding of deep learning,

52:24.400 --> 52:28.400
which prevent us from really solving the problem that we want.

52:28.400 --> 52:33.400
So, I think one of these problems, one of the things that's missing is that, or that seems like patently wrong,

52:33.400 --> 52:41.400
is the fact that we train a model, then we stop training the model, and we freeze it,

52:41.400 --> 52:47.400
even though it's the training process where the magic really happens.

52:47.400 --> 52:56.400
Like, the magic is like, if you think about it, like, the training process is the true general part of the whole story,

52:56.400 --> 52:59.400
because your TensorFlow code doesn't care which dataset to optimize.

52:59.400 --> 53:03.400
It just says, whatever, just give me the dataset, I don't care which one to solve, I'll solve them all.

53:03.400 --> 53:11.400
So, like, the ability to do that feels really special, and I think we are not using it at test time.

53:11.400 --> 53:15.400
Like, it's hard to speculate about, like, things of which we don't know the answer,

53:15.400 --> 53:22.400
but all I'll say is that simply train bigger, deeper language models will go surprisingly far scaling up,

53:22.400 --> 53:25.400
but also doing things like training at test time and inference at test time,

53:25.400 --> 53:29.400
I think would be another important boost to performance.

53:29.400 --> 53:32.400
Hi, thank you for the talk.

53:32.400 --> 53:36.400
So, it seems like right now another interesting approach to solving reinforcement learning problems

53:36.400 --> 53:41.400
could be to go for the evolutionary routes, using evolutionary strategies.

53:41.400 --> 53:45.400
And although they have their caveats, I wanted to know if, at OpenAI in particular,

53:45.400 --> 53:50.400
you're working on something related, and what is your general opinion on them?

53:50.400 --> 53:58.400
So, like, at present, I believe that something like evolutionary strategies is not great for reinforcement learning.

53:58.400 --> 54:03.400
I think that normal reinforcement learning algorithms, especially with big policies, are better.

54:03.400 --> 54:09.400
But I think if you want to evolve a small compact object, like a piece of code, for example,

54:09.400 --> 54:14.400
I think that would be a place where this would be seriously worth considering.

54:14.400 --> 54:20.400
But this, you know, evolving a useful piece of code is a cool idea.

54:20.400 --> 54:24.400
It hasn't been done yet, so still a lot of work to be done before we get there.

54:24.400 --> 54:26.400
Hi, thank you so much for coming.

54:26.400 --> 54:31.400
My question is, you mentioned what is the right goal is a political problem.

54:31.400 --> 54:34.400
So, I'm wondering if you can elaborate a bit on that,

54:34.400 --> 54:39.400
and also what do you think would be their approach for us to maybe get there?

54:40.400 --> 54:45.400
Well, I can't really comment too much, because all the thoughts that, you know,

54:45.400 --> 54:50.400
we now have a few people who are thinking about this full time at OpenAI,

54:50.400 --> 54:57.400
I don't have enough of a super strong opinion to say anything too definitive.

54:57.400 --> 55:02.400
All I can say at a very high level is given the size, like, if you go into the future,

55:02.400 --> 55:06.400
whenever soon or later, you know, whenever it's going to happen when you build a computer

55:06.400 --> 55:10.400
which can do anything better than a human.

55:10.400 --> 55:13.400
It will happen, because the brain is physical.

55:13.400 --> 55:18.400
The impact on society is going to be completely massive and overwhelming.

55:18.400 --> 55:23.400
It's very difficult to imagine, even if you try really hard.

55:23.400 --> 55:27.400
And I think what it means is that people will care a lot.

55:27.400 --> 55:31.400
And that's what I was alluding to, the fact that this will be something

55:31.400 --> 55:34.400
that many people will care about strongly.

55:34.400 --> 55:39.400
And, like, as the impact increases, gradually we sell driving cars, more automation,

55:39.400 --> 55:42.400
I think we will see a lot more people care.

55:42.400 --> 55:46.400
Do we need to have a very accurate model of the physical world,

55:46.400 --> 55:53.400
and then simulate that in order to have these agents that can eventually come out into the real world

55:53.400 --> 55:58.400
and do something approaching, you know, human level intelligence tasks?

55:58.400 --> 56:00.400
That's a very good question.

56:00.400 --> 56:04.400
So I think if that were the case, we'd be in trouble.

56:04.400 --> 56:10.400
And I am very certain that it could be avoided.

56:10.400 --> 56:16.400
So specifically, the real answer has to be that, look, you learn to problem solve,

56:16.400 --> 56:18.400
you learn to negotiate, you learn to persist,

56:18.400 --> 56:21.400
you learn lots of different useful life lessons in the simulation.

56:21.400 --> 56:23.400
And yes, you learn some physics, too.

56:23.400 --> 56:26.400
But then you go outside of the real world, and you have to start over to some extent,

56:26.400 --> 56:30.400
because many of your deeply held assumptions will be false.

56:30.400 --> 56:37.400
And one of the goals, so that's one reason I care so much about never stopping training.

56:37.400 --> 56:41.400
You've accumulated your knowledge, now you go into an environment where some of your assumptions are violated,

56:41.400 --> 56:44.400
you continue training, you try to connect the new data to your old data.

56:44.400 --> 56:46.400
And this is an important requirement from our algorithms,

56:46.400 --> 56:50.400
which is already met to some extent, but it will have to be met a lot more,

56:50.400 --> 56:54.400
so that you can take the partial knowledge that you've acquired

56:54.400 --> 56:57.400
and go in a new situation, learn some more.

56:57.400 --> 57:02.400
Literally the example of you go to school, you learn useful things, then you go to work.

57:02.400 --> 57:06.400
It's not a perfect, it's not, you know, for your four years of CS and undergrad,

57:06.400 --> 57:10.400
it's not going to fully prepare you for whatever it is you need to know at work.

57:10.400 --> 57:12.400
It will help somewhat, you'll be able to get off the ground,

57:12.400 --> 57:14.400
but there will be lots of new things you need to learn.

57:14.400 --> 57:16.400
So that's the spirit of it.

57:16.400 --> 57:18.400
Think of it as a school.

57:18.400 --> 57:22.400
One of the things you mentioned pretty early on in your talk is that one of the limitations

57:22.400 --> 57:26.400
of this sort of style of reinforcement learning is there's no self-organization.

57:26.400 --> 57:29.400
So you have to tell it, went into a good thing or did a bad thing.

57:29.400 --> 57:31.400
And that's actually a problem in neuroscience,

57:31.400 --> 57:34.400
is when you're trying to teach a rat to, you know, navigate a maze,

57:34.400 --> 57:36.400
you have to artificially tell it what to do.

57:36.400 --> 57:39.400
So where do you see moving forward when we already have this problem with teaching,

57:39.400 --> 57:41.400
you know, not necessarily learning, but also teaching.

57:41.400 --> 57:44.400
So where do you see the research moving forward in that respect?

57:44.400 --> 57:47.400
How do you sort of introduce this notion of self-organization?

57:47.400 --> 57:50.400
So I think without question, one really important thing you need to do

57:50.400 --> 57:57.400
is to be able to infer the goals and strategies of other agents by observing them.

57:57.400 --> 58:02.400
That's a fundamental skill you need to be able to learn, to embed into the agents.

58:02.400 --> 58:05.400
So that, for example, you have two agents, one of them is doing something,

58:05.400 --> 58:08.400
and the other agent says, well, that's really cool, I want to be able to do that too.

58:08.400 --> 58:10.400
And then you go on and do that.

58:10.400 --> 58:14.400
And so I'd say that this is a very important component in terms of setting the reward.

58:14.400 --> 58:18.400
You see what they do, you infer the reward,

58:18.400 --> 58:21.400
and now we have a knob which says, you see what they're doing?

58:21.400 --> 58:23.400
Now go and try to do the same thing.

58:23.400 --> 58:28.400
So I'd say this is, as far as I know, this was one of the important ways

58:28.400 --> 58:33.400
in which humans are quite different from other animals

58:33.400 --> 58:42.400
in the way in the scale and scope in which we copy the behavior of other humans.

58:42.400 --> 58:44.400
You might have asked a quick follow-up.

58:44.400 --> 58:45.400
Go for it.

58:45.400 --> 58:48.400
So that's kind of obvious how that works in the scope of competition,

58:48.400 --> 58:50.400
but what about just sort of arbitrary tasks?

58:50.400 --> 58:54.400
Like, I'm in a math class with someone and I see someone doing a problem a particular way

58:54.400 --> 58:57.400
and I'm like, oh, that's a good strategy, maybe I should try that out.

58:57.400 --> 59:00.400
How does that work in a sort of non-competitive environment?

59:00.400 --> 59:04.400
So I think that this will be, I think that's going to be a little bit separate

59:04.400 --> 59:10.400
from the competitive environment, but it will have to be somehow either,

59:11.400 --> 59:16.400
probably baked in, maybe evolved into the system where like,

59:16.400 --> 59:21.400
if you have other agents doing things, they're generating data which you observe

59:21.400 --> 59:24.400
and the only way to truly make sense of the data that you see

59:24.400 --> 59:29.400
is to infer the goal of the agent, the strategy, their belief state.

59:29.400 --> 59:32.400
That's important also for communicating with them.

59:32.400 --> 59:34.400
If you want to successfully communicate with someone,

59:34.400 --> 59:37.400
you have to keep track both of their goal and of their belief state instead of knowledge.

59:37.400 --> 59:41.400
So I think you will find that there are many, I guess, connections

59:41.400 --> 59:45.400
between understanding what other agents are doing, inferring their goals,

59:45.400 --> 59:48.400
imitating them and successfully communicating them.

59:48.400 --> 59:52.400
Alright, let's give Ilya and the happy hour a big hand.

01:00:07.400 --> 01:00:09.400
Thank you.

