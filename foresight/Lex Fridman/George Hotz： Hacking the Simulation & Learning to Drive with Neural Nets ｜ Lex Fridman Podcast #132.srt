1
00:00:00,000 --> 00:00:06,560
The following is a conversation with George Hotz, a.k.a. Geohot, his second time on the podcast.

2
00:00:06,560 --> 00:00:12,880
He's the founder of Kamma AI, an autonomous and semi-autonomous vehicle technology company

3
00:00:12,880 --> 00:00:21,360
that seeks to be, to Tesla autopilot, what Android is, to the iOS. They sell the Kamma 2 device for

4
00:00:21,360 --> 00:00:27,040
$1,000 that, when installed in many of their supported cars, can keep the vehicle centered

5
00:00:27,040 --> 00:00:33,520
in the lane even when there are no lane markings. It includes driver sensing that ensures that the

6
00:00:33,520 --> 00:00:39,440
driver's eyes are on the road. As you may know, I'm a big fan of driver sensing. I do believe Tesla

7
00:00:39,440 --> 00:00:45,520
Autopilot and others should definitely include it in their sensor suite. Also, I'm a fan of Android

8
00:00:45,520 --> 00:00:52,400
and a big fan of George, for many reasons, including his non-linear out-of-the-box brilliance and the

9
00:00:52,400 --> 00:00:58,480
fact that he's a superstar programmer of a very different style than myself. Styles make fights

10
00:00:59,040 --> 00:01:04,480
and styles make conversations, so I really enjoyed this chat and I'm sure we'll talk many more times

11
00:01:04,480 --> 00:01:10,160
on this podcast. Quick mention of a sponsor followed by some thoughts related to the episode.

12
00:01:10,160 --> 00:01:17,520
First is ForSigmatic, the maker of delicious mushroom coffee. Second is the coding digital,

13
00:01:17,520 --> 00:01:24,480
a podcast on tech and entrepreneurship that I listen to and enjoy. And finally, ExpressVPN,

14
00:01:24,480 --> 00:01:30,000
the VPN I've used for many years to protect my privacy on the internet. Please check out the

15
00:01:30,000 --> 00:01:35,760
sponsors in the description to get a discount and to support this podcast. As a side note,

16
00:01:35,760 --> 00:01:41,840
let me say that my work at MIT on autonomous and semi-autonomous vehicles led me to study the human

17
00:01:41,840 --> 00:01:47,680
side of autonomy enough to understand that it's a beautifully complicated and interesting problem

18
00:01:47,680 --> 00:01:54,080
space, much richer than what can be studied in the lab. In that sense, the data that comma AI,

19
00:01:54,080 --> 00:01:59,600
Tesla Autopilot, and perhaps others like Cadillac Supercruiser Collecting, gives us a chance to

20
00:01:59,600 --> 00:02:06,320
understand how we can design safe semi-autonomous vehicles for real human beings in real-world

21
00:02:06,320 --> 00:02:12,960
conditions. I think this requires bold innovation and a serious exploration of the first principles

22
00:02:12,960 --> 00:02:18,400
of the driving task itself. If you enjoyed this thing, subscribe on YouTube, review it with

23
00:02:18,400 --> 00:02:24,320
five stars and up a podcast, follow on Spotify, support on Patreon, or connect with me on Twitter

24
00:02:24,320 --> 00:02:32,800
at Lex Freedman. And now here's my conversation with George Hotz. So last time we started talking

25
00:02:32,800 --> 00:02:37,520
about the simulation, this time let me ask you, do you think there's intelligent life out there in

26
00:02:37,520 --> 00:02:44,000
the universe? I always maintained my answer to the Fermi paradox. I think there has been intelligent

27
00:02:44,000 --> 00:02:48,800
life elsewhere in the universe. So intelligent civilizations existed, but they've blown themselves

28
00:02:48,800 --> 00:02:55,760
up. So your general intuition is that intelligent civilizations quickly, like there's that parameter

29
00:02:55,760 --> 00:03:01,360
in the Drake equation, your sense is they don't last very long. Yeah. How are we doing on that?

30
00:03:01,840 --> 00:03:07,760
Have we lasted pretty good? Oh, no. How do we do? Oh, yeah. I mean, not quite yet.

31
00:03:09,040 --> 00:03:15,360
Well, it's only as Yukowski, IQ required to destroy the world falls by one point every year.

32
00:03:15,360 --> 00:03:20,160
Okay. So technology democratizes the destruction of the world.

33
00:03:20,960 --> 00:03:22,320
When can a meme destroy the world?

34
00:03:25,200 --> 00:03:27,200
It kind of is already, right?

35
00:03:27,200 --> 00:03:33,040
Somewhat. I don't think we've seen anywhere near the worst of it yet. World's going to get weird.

36
00:03:33,840 --> 00:03:39,360
Well, maybe a meme can save the world. We thought about that, the meme Lord Elon Musk fighting on

37
00:03:39,360 --> 00:03:46,880
the side of good versus the meme Lord of the darkness, which is not saying anything bad about

38
00:03:46,880 --> 00:03:52,960
Donald Trump, but he is the Lord of the meme on the dark side. He's a Darth Vader of memes.

39
00:03:53,680 --> 00:03:59,760
I think in every fairy tale, they always end it with, and they lived happily ever after,

40
00:03:59,760 --> 00:04:04,320
and I'm like, please tell me more about this happily ever after. I've heard 50% of marriages

41
00:04:04,320 --> 00:04:09,360
end in divorce. Why doesn't your marriage end up there? You can't just say happily ever after. So

42
00:04:10,880 --> 00:04:16,320
the thing about destruction is it's over after the destruction. We have to do everything right

43
00:04:16,320 --> 00:04:22,000
in order to avoid it. And one thing wrong, I mean, actually, this is what I really like about

44
00:04:22,000 --> 00:04:25,680
cryptography. Cryptography, it seems like we live in a world where the defense wins.

45
00:04:27,760 --> 00:04:33,280
Versus nuclear weapons, the opposite is true. It is much easier to build a warhead that splits

46
00:04:33,280 --> 00:04:39,440
into 100 little warheads than to build something that can take out 100 little warheads. The offense

47
00:04:39,440 --> 00:04:46,960
has the advantage there. So maybe our future is in crypto. So cryptography, right? The Goliath

48
00:04:46,960 --> 00:04:55,920
is the defense. And then all the different hackers are the Davids, and that equation is flipped for

49
00:04:55,920 --> 00:05:02,000
nuclear war. Because there's so many, like one nuclear weapon destroys everything, essentially.

50
00:05:02,000 --> 00:05:08,400
Yeah, and it is much easier to attack with a nuclear weapon than it is to like, the technology

51
00:05:08,400 --> 00:05:13,120
required to intercept and destroy a rocket is much more complicated than the technology required to

52
00:05:13,120 --> 00:05:21,040
just orbital trajectory send a rocket to somebody. Okay, your intuition that there were intelligent

53
00:05:21,040 --> 00:05:26,880
civilizations out there, but it's very possible that they're no longer there. It's kind of a sad

54
00:05:26,880 --> 00:05:34,240
picture. They enter some steady state. They all wirehead themselves. What's wirehead? Stimulate

55
00:05:34,240 --> 00:05:42,480
their platter centers and just live forever in this kind of stasis. They become, well, I mean,

56
00:05:42,480 --> 00:05:49,120
I think the reason I believe this is because where are they? If there's some reason they stopped

57
00:05:49,120 --> 00:05:53,280
expanding, because otherwise they would have taken over the universe. The universe isn't that big.

58
00:05:53,280 --> 00:05:57,440
Or at least, you know, let's just talk about the galaxy, right? 70,000 light years across.

59
00:05:58,480 --> 00:06:04,560
I took that number from Star Trek Voyager. I don't know how true it is. But yeah, that's not big.

60
00:06:04,560 --> 00:06:10,080
Right? 70,000 light years is nothing. For some possible technology that you can imagine that

61
00:06:10,240 --> 00:06:13,920
leverage like wormholes or something like that. You don't even need wormholes. Just a von Neumann

62
00:06:13,920 --> 00:06:19,120
probe is enough. A von Neumann probe and a million years of sub light travel, and you'd have taken

63
00:06:19,120 --> 00:06:25,040
over the whole universe. That clearly didn't happen. So something stopped it. So you mean a few,

64
00:06:25,040 --> 00:06:30,640
right, for like a few million years, if you sent out probes that travel close, what's sub light?

65
00:06:30,640 --> 00:06:35,600
You mean close to the speed of light? Let's say 0.1c. And it just spreads. Interesting. Actually,

66
00:06:35,600 --> 00:06:41,840
that's an interesting calculation. So what makes you think that we'd be able to communicate with

67
00:06:41,840 --> 00:06:48,880
them? Like, yeah, what's why do you think we would be able to be able to comprehend

68
00:06:48,880 --> 00:06:55,440
intelligent lies that are out there? Like, even if they were among us kind of thing? Like,

69
00:06:55,440 --> 00:07:02,880
or even just flying around? Well, I mean, that's possible. It's possible that there is some sort

70
00:07:02,880 --> 00:07:07,840
of prime directive that'd be a really cool universe to live in. And there's some reason

71
00:07:07,840 --> 00:07:14,000
they're not making themselves visible to us. But it makes sense that they would use the same,

72
00:07:15,040 --> 00:07:18,960
well, at least the same entropy. Well, you're implying the same laws of physics. I don't know

73
00:07:18,960 --> 00:07:23,440
what you mean by entropy in this case. Oh, yeah. I mean, if entropy is the scarce resource in the

74
00:07:23,440 --> 00:07:28,800
universe. So what do you think about like Steven Wolfram and everything is a computation? And

75
00:07:28,880 --> 00:07:33,680
then what if they are traveling through this world of computation? So if you think of the

76
00:07:33,680 --> 00:07:41,440
universe as just information processing, then what you're referring to with entropy, and then

77
00:07:41,440 --> 00:07:46,240
these pockets of interesting complex computation swimming around, how do we know they're not

78
00:07:46,240 --> 00:07:53,440
already here? How do we know that this, like all the different amazing things that are full of

79
00:07:53,440 --> 00:08:00,560
mystery on earth are just like little footprints of intelligence from light years away?

80
00:08:01,520 --> 00:08:06,880
Maybe. I mean, I tend to think that as civilizations expand, they use more and more energy.

81
00:08:07,680 --> 00:08:11,680
And you can never overcome the problem of waste heat. So where is their waste heat?

82
00:08:11,680 --> 00:08:15,520
So we'd be able to, with our crude methods, be able to see like, there's a whole lot of

83
00:08:17,200 --> 00:08:22,080
energy here. But it could be something we're not, I mean, we don't understand dark energy,

84
00:08:22,400 --> 00:08:27,360
dark matter. It could be just stuff we don't understand at all. Or they can have a fundamentally

85
00:08:27,360 --> 00:08:33,360
different physics, you know, like that we just don't even comprehend. Well, I think, okay,

86
00:08:33,360 --> 00:08:36,960
I mean, it depends how far out you want to go. I don't think physics is very different on the

87
00:08:36,960 --> 00:08:43,520
other side of the galaxy. I would suspect that they have, I mean, if they're in our universe,

88
00:08:43,520 --> 00:08:48,800
they have the same physics. Well, yeah, that's the assumption we have. But there could be like

89
00:08:48,800 --> 00:08:57,840
super trippy things like, like our cognition only gets to a slice, and all the possible

90
00:08:57,840 --> 00:09:01,760
instruments that we can design only get to a particular slice of the universe. And there's

91
00:09:01,760 --> 00:09:09,280
something much like weirder. Maybe we can try a thought experiment. Would people from the past

92
00:09:09,920 --> 00:09:16,400
be able to detect the remnants of our, we'll be able to detect our modern civilization?

93
00:09:16,400 --> 00:09:20,560
And I think the answer is obviously yes. You mean past from a hundred years ago?

94
00:09:20,560 --> 00:09:23,200
Well, let's even go back further. Let's go to a million years ago.

95
00:09:24,320 --> 00:09:28,400
The humans who were lying around in the desert probably didn't even have, maybe they just barely

96
00:09:28,400 --> 00:09:41,840
had fire. They would understand if a 747 flew overhead. In this vicinity, but not if a 747 flew

97
00:09:41,840 --> 00:09:46,320
on Mars. Like, because they wouldn't be able to see far, because we're not actually communicating

98
00:09:46,320 --> 00:09:52,640
that well with the rest of the universe. We're doing okay, just sending out random like 50s tracks

99
00:09:52,640 --> 00:09:59,120
of music. True. And yeah, I mean, they'd have to, you know, the, we've only been broadcasting radio

100
00:09:59,120 --> 00:10:07,920
waves for 150 years. And well, there's your light cone. So yeah, okay. What do you make about all

101
00:10:08,000 --> 00:10:15,760
the, I recently came across this, having talked to David's favor. I don't know if you caught

102
00:10:15,760 --> 00:10:22,800
what the videos that Pentagon released and the New York Times reporting of the UFO sightings.

103
00:10:23,440 --> 00:10:31,600
So I kind of looked into it, quote unquote. And there's actually been like hundreds of thousands

104
00:10:31,600 --> 00:10:37,040
of UFO sightings, right? And a lot of it, you can explain it way in different kinds of ways.

105
00:10:37,040 --> 00:10:43,120
So one is it could be interesting physical phenomena. Two, it could be people wanting to

106
00:10:43,120 --> 00:10:47,680
believe. And therefore they conjure up a lot of different things that just, you know, when you

107
00:10:47,680 --> 00:10:53,600
see different kinds of lights, some basic physics phenomena, and then you just conjure up ideas

108
00:10:53,600 --> 00:11:00,000
of possible out there mysterious worlds. But, you know, it's also possible like you have a case of

109
00:11:00,960 --> 00:11:08,880
David Fravor, who is a Navy pilot, who's, you know, as legit as it gets in terms of humans who

110
00:11:08,880 --> 00:11:16,160
are able to perceive things in the environment and make conclusions, whether those things are

111
00:11:16,160 --> 00:11:23,280
a threat or not. And he and several other pilots saw a thing, I don't know if you followed this,

112
00:11:23,280 --> 00:11:28,320
but they saw a thing that they've since then called TikTok that moved in all kinds of weird ways.

113
00:11:29,280 --> 00:11:36,480
They don't know what it is. It could be technology developed by the United States,

114
00:11:36,480 --> 00:11:40,560
and they're just not aware of it and the surface level from the Navy, right? It could be

115
00:11:40,560 --> 00:11:45,120
different kind of lighting technology or drone technology, all that kind of stuff. It could

116
00:11:45,120 --> 00:11:51,280
be the Russians and the Chinese, all that kind of stuff. And of course their mind, our mind,

117
00:11:51,280 --> 00:11:57,280
can also venture into the possibility that it's from another world. Have you looked into this at

118
00:11:57,280 --> 00:12:04,960
all? What do you think about it? I think all the news is a scythe. I think that the most plausible...

119
00:12:04,960 --> 00:12:13,120
Nothing is real. Yeah, I listened to the, I think it was Bob Lazar on Joe Rogan. And like,

120
00:12:13,120 --> 00:12:18,000
I believe everything this guy is saying. And then I think that it's probably just some like MK

121
00:12:18,000 --> 00:12:24,640
Ultra kind of thing, you know? What do you mean? Like, they, you know, they made some weird thing

122
00:12:24,640 --> 00:12:28,320
and they called it an alien spaceship. You know, maybe it was just to like stimulate young

123
00:12:28,320 --> 00:12:32,080
physicists' minds and tell them it's alien technology and we'll see what they come up with,

124
00:12:32,080 --> 00:12:37,840
right? Do you find any conspiracy theories compelling? Like, have you pulled at the

125
00:12:37,840 --> 00:12:42,960
string of the, of the rich complex world of conspiracy theories that's out there?

126
00:12:43,760 --> 00:12:48,960
I think that I've heard a conspiracy theory that conspiracy theories were invented by the CIA in

127
00:12:48,960 --> 00:12:58,480
the 60s to discredit true things. Yeah. So, you know, you can go to ridiculous conspiracy theories

128
00:12:58,480 --> 00:13:06,880
like Flat Earth and Pizza Gate and, you know, these things are almost to hide like conspiracy

129
00:13:06,880 --> 00:13:10,480
theories that like, you know, remember when the Chinese like locked up the doctors who discovered

130
00:13:10,480 --> 00:13:14,560
coronavirus? Like I tell people this and I'm like, no, no, no, that's not a conspiracy theory. That

131
00:13:14,560 --> 00:13:18,720
actually happened. Do you remember the time that the money used to be backed by gold and now it's

132
00:13:18,720 --> 00:13:24,480
backed by nothing? This is not a conspiracy theory. This actually happened. Well, that's one of my

133
00:13:24,480 --> 00:13:35,680
worries today with the idea of fake news is that when nothing is real, then like you dilute the

134
00:13:35,680 --> 00:13:41,200
possibility of anything being true by conjuring up all kinds of conspiracy theories. And then you

135
00:13:41,200 --> 00:13:47,680
don't know what to believe. And then like the idea of truth of objectivity is lost completely.

136
00:13:47,760 --> 00:13:53,440
Everybody has their own truth. So you used to control information by censoring it.

137
00:13:53,440 --> 00:13:58,240
Then the internet happened and governments are like, oh, shit, we can't censor things anymore.

138
00:13:58,240 --> 00:14:03,120
I know what we'll do. You know, it's the old story of the story of like,

139
00:14:04,080 --> 00:14:08,320
tying a flag with a leprechaun tells you as gold is buried and you tie one flag and you make the

140
00:14:08,320 --> 00:14:11,680
leprechaun swear to not remove the flag and you come back to the field later with a shovel and

141
00:14:11,680 --> 00:14:20,160
this flag's everywhere. That's one way to maintain privacy, right? In order to protect the contents

142
00:14:20,160 --> 00:14:25,920
of this conversation, for example, we could just generate like millions of deep fake conversations

143
00:14:25,920 --> 00:14:31,200
where you and I talk and say random things. So this is just one of them and nobody knows which

144
00:14:31,200 --> 00:14:35,920
one was the real one. This could be fake right now. Classic steganography technique.

145
00:14:36,000 --> 00:14:43,840
Okay, another absurd question about intelligent life because you're an incredible programmer

146
00:14:43,840 --> 00:14:52,480
outside of everything else we'll talk about just as a programmer. Do you think intelligent beings

147
00:14:52,480 --> 00:14:58,960
out there, the civilizations that were out there had computers and programming? Did they

148
00:14:58,960 --> 00:15:05,680
do naturally have to develop something where we engineer machines and are able to encode both

149
00:15:06,880 --> 00:15:12,240
knowledge into those machines and instructions that process that knowledge, process that

150
00:15:12,240 --> 00:15:17,680
information to make decisions and actions and so on? And would those programming languages,

151
00:15:18,240 --> 00:15:22,480
if you think they exist, be at all similar to anything we've developed?

152
00:15:24,080 --> 00:15:29,840
So I don't see that much of a difference between quote unquote natural languages and programming

153
00:15:29,840 --> 00:15:40,960
languages. I think there's so many similarities. So when asked the question what do alien languages

154
00:15:40,960 --> 00:15:48,720
look like, I imagine they're not all that dissimilar from ours. And I think translating in and out of

155
00:15:48,720 --> 00:15:58,160
them wouldn't be that crazy. It was difficult to compile like DNA to Python and then to see.

156
00:15:59,120 --> 00:16:02,560
There is a little bit of a gap in the kind of languages we use for

157
00:16:04,720 --> 00:16:10,880
touring machines and the kind of languages nature seems to use a little bit. Maybe that's just,

158
00:16:11,520 --> 00:16:16,240
we just haven't understood the kind of language that nature uses well yet.

159
00:16:16,240 --> 00:16:25,200
DNA is a CAD model. It's not quite a programming language. It has no sort of serial execution.

160
00:16:25,200 --> 00:16:32,400
It's not quite a CAD model. So I think in that sense, we actually completely understand it.

161
00:16:32,400 --> 00:16:39,360
The problem is simulating on these CAD models. I played with it a bit this year. It's super

162
00:16:39,360 --> 00:16:44,800
computationally intensive. If you want to go down to the molecular level where you need to go to see

163
00:16:44,800 --> 00:16:52,080
a lot of these phenomenon like protein folding. So yeah, it's not that we don't understand it.

164
00:16:52,080 --> 00:16:54,400
It just requires a whole lot of compute to kind of compile it.

165
00:16:55,040 --> 00:17:00,400
For human minds, it's inefficient both for the data representation and for the programming.

166
00:17:00,400 --> 00:17:04,960
Yeah, it runs well on raw nature. It runs well on raw nature. And when we try to build

167
00:17:04,960 --> 00:17:09,360
emulators or simulators for that, well, they're mad slow and I've tried it.

168
00:17:10,480 --> 00:17:15,840
It runs in that, yeah, you've commented elsewhere. I don't remember where that

169
00:17:16,160 --> 00:17:23,920
one of the problems is simulating nature is tough. And if you want to sort of deploy a prototype,

170
00:17:25,440 --> 00:17:30,240
I forgot how you put it, but it made me laugh. But animals or humans would need to be involved

171
00:17:31,680 --> 00:17:40,480
in order to try to run some prototype code. If we're talking about COVID and viruses and so on,

172
00:17:41,120 --> 00:17:45,760
if you were to try to engineer some kind of defense mechanisms like a vaccine

173
00:17:47,280 --> 00:17:52,480
against COVID or all that kind of stuff, that doing any kind of experimentation like you can

174
00:17:52,480 --> 00:17:59,520
with autonomous vehicles would be very technically and ethically costly.

175
00:17:59,520 --> 00:18:05,440
I'm not sure about that. I think you can do tons of crazy biology and test tubes. I think

176
00:18:05,440 --> 00:18:09,680
my bigger complaint is more, all the tools are so bad.

177
00:18:11,360 --> 00:18:16,480
Like literally, you mean like like libraries and I'm not pipetting shit. Like your hand

178
00:18:16,480 --> 00:18:25,200
of me, I gotta, no, no, no, no, there has to be some like automating stuff. And like the

179
00:18:25,920 --> 00:18:31,520
yeah, but human biology is messy. Like it seems like, look at those Duranos videos. They were

180
00:18:31,520 --> 00:18:35,760
joke. It's like a little gantry. It's like a little XY gantry high school science project

181
00:18:35,760 --> 00:18:40,240
with the pipet. I'm like, really? Gotta be something better. You can't build like nice

182
00:18:40,240 --> 00:18:46,400
microfluidics and I can program the computation to biointerface. I mean, this is going to happen.

183
00:18:47,040 --> 00:18:53,360
But like right now, if you are asking me to pipet 50 milliliters of solution, I'm out.

184
00:18:54,240 --> 00:19:01,200
This is so crude. Yeah. Okay. Let's get all the crazy out of the way. So a bunch of people

185
00:19:01,200 --> 00:19:06,800
asked me, since we talked about the simulation last time, we talked about hacking the simulation.

186
00:19:06,800 --> 00:19:11,520
Do you have any updates, any insights about how we might be able to go about

187
00:19:12,400 --> 00:19:15,920
hacking simulation if we indeed do live in a simulation?

188
00:19:17,040 --> 00:19:22,800
I think a lot of people misinterpreted the point of that South by talk. The point of the

189
00:19:22,800 --> 00:19:27,360
South by talk was not literally to hack the simulation. I think that this

190
00:19:28,240 --> 00:19:35,440
is an idea is literally just I think theoretical physics. I think that's the whole

191
00:19:38,880 --> 00:19:43,120
goal. You want your grand unified theory, but then, okay, build a grand unified theory,

192
00:19:43,120 --> 00:19:49,280
search for exploits. I think we're nowhere near actually there yet. My hope with that was just

193
00:19:49,280 --> 00:19:55,200
more to like, are your people kidding me with the things you spend time thinking about? Do you

194
00:19:55,200 --> 00:20:01,760
understand kind of how small you are? You are bites and God's computer, really?

195
00:20:02,400 --> 00:20:10,800
And the things that people get worked up about. So basically, it was more a message of we should

196
00:20:10,800 --> 00:20:22,320
humble ourselves. What are we humans in this byte code? Yeah. And not just humble ourselves,

197
00:20:22,320 --> 00:20:27,200
but I'm not trying to make people feel guilty or anything like that. I'm trying to say literally,

198
00:20:27,200 --> 00:20:31,600
look at what you are spending time on, right? What are you referring to? You're referring to

199
00:20:31,600 --> 00:20:35,920
the Kardashians? What are we talking about? I'm referring to, no, the Kardashians,

200
00:20:35,920 --> 00:20:43,840
everyone knows that's kind of fun. I'm referring more to the economy, this idea that

201
00:20:44,400 --> 00:20:54,720
we got to up our stock price. Or what is the goal function of humanity?

202
00:20:55,280 --> 00:20:59,360
You don't like the game of capitalism? You don't like the games we've constructed for

203
00:20:59,360 --> 00:21:04,720
ourselves as humans? I'm a big fan of capitalism. I don't think that's really the game we're playing

204
00:21:04,720 --> 00:21:08,320
right now. I think we're playing a different game where the rules are rigged.

205
00:21:09,040 --> 00:21:14,240
Okay, which games are interesting to you that we humans have constructed and which

206
00:21:14,240 --> 00:21:21,840
aren't? Which are productive and which are not? Actually, maybe that's the real point of the talk.

207
00:21:21,840 --> 00:21:27,680
It's like, stop playing these fake human games. There's a real game here. We can play the real

208
00:21:27,680 --> 00:21:34,400
game. The real game is nature wrote the rules. This is a real game. There still is a game to play.

209
00:21:35,040 --> 00:21:38,320
But if you look at, sorry to interrupt, I don't know if you've seen the Instagram account,

210
00:21:38,320 --> 00:21:46,240
Nature is Metal. The game that nature seems to be playing is a lot more cruel than we humans

211
00:21:46,240 --> 00:21:52,240
want to put up with. Or at least we see it as cruel. It's like the bigger thing eats the smaller

212
00:21:52,240 --> 00:21:59,680
thing and does it to impress another big thing so it can mate with that thing.

213
00:22:00,320 --> 00:22:03,200
And that's it. That seems to be the entirety of it.

214
00:22:04,000 --> 00:22:10,720
Well, there's no art. There's no music. There's no comma AI. There's no comma one,

215
00:22:10,720 --> 00:22:16,160
no comma two, no George Hott's with his brilliant talks at South by Southwest.

216
00:22:16,880 --> 00:22:21,520
I disagree though. I disagree that this is what nature is. I think nature just provided

217
00:22:22,320 --> 00:22:30,800
basically a open world MMORPG. And here it's open world. I mean, if that's the game you want to

218
00:22:30,800 --> 00:22:36,160
play, you can play that game. Isn't that beautiful? I know if you play Diablo, they used to have,

219
00:22:36,160 --> 00:22:43,760
I think, cow level where it's, so everybody will go just, they figured out this,

220
00:22:44,320 --> 00:22:50,720
like the best way to gain like experience points is to just slaughter cows over and over and over.

221
00:22:52,160 --> 00:22:57,600
And so they figured out this little sub game within the bigger game that this is the most

222
00:22:57,600 --> 00:23:02,560
efficient way to get experience points. And everybody somehow agreed that getting experience

223
00:23:02,560 --> 00:23:08,000
points in RPG context where you always want to be getting more stuff, more skills, more levels,

224
00:23:08,000 --> 00:23:14,960
keep advancing. That seems to be good. So might as well spend sacrifice, actual enjoyment of

225
00:23:15,040 --> 00:23:22,240
playing a game, exploring a world and spending like hundreds of hours of your time in cow level.

226
00:23:22,240 --> 00:23:28,320
I mean, the number of hours I spent in cow level, I'm not like the most impressive person because

227
00:23:28,320 --> 00:23:33,920
people have probably thousands of hours there, but it's ridiculous. So that's a little absurd game

228
00:23:33,920 --> 00:23:39,920
that brought me joints and weird dopamine drug kind of way. So you don't like those games.

229
00:23:40,640 --> 00:23:47,200
You don't think that's us humans failing the nature. I think so.

230
00:23:47,200 --> 00:23:51,280
And that was the point of the talk. Yeah. So how do we hack it then?

231
00:23:51,280 --> 00:23:55,680
Well, I want to live forever. And wait, I want to live forever.

232
00:23:55,680 --> 00:23:57,840
And this is the goal. Well, that's a game against nature.

233
00:23:59,120 --> 00:24:02,320
Yeah. Immortality is the good objective function to you.

234
00:24:03,360 --> 00:24:06,000
I mean, start there and then you can do whatever else you want because you got a long time.

235
00:24:06,960 --> 00:24:11,280
What if immortality makes the game just totally not fun? I mean,

236
00:24:11,280 --> 00:24:18,080
like why do you assume immortality is somehow a good objective function?

237
00:24:18,080 --> 00:24:22,480
It's not immortality that I want. A true immortality where I could not die.

238
00:24:22,480 --> 00:24:26,560
I would prefer what we have right now. But I want to choose my own death, of course.

239
00:24:27,920 --> 00:24:31,440
I don't want nature to decide when I die. I'm going to win. I'm going to be you.

240
00:24:31,840 --> 00:24:40,080
And then at some point, if you choose commit suicide, how long do you think you'd live?

241
00:24:41,440 --> 00:24:42,240
Until I get bored?

242
00:24:42,960 --> 00:24:49,200
See, I don't think people, brilliant people like you that really ponder

243
00:24:50,240 --> 00:24:57,680
living a long time are really considering how meaningless life becomes.

244
00:24:58,400 --> 00:25:00,320
Well, I want to know everything and then I'm ready to die.

245
00:25:02,320 --> 00:25:07,360
But why do you want, isn't it possible that you want to know everything because

246
00:25:08,080 --> 00:25:13,200
it's finite? Like the reason you want to know quote unquote everything is because you

247
00:25:13,200 --> 00:25:19,440
don't have enough time to know everything. And once you have unlimited time, then you realize

248
00:25:19,440 --> 00:25:23,760
like why do anything? Like why learn anything?

249
00:25:24,880 --> 00:25:26,960
I don't want to know everything and then I'm ready to die.

250
00:25:27,120 --> 00:25:33,760
It's a terminal value. It's not in service of anything else.

251
00:25:34,640 --> 00:25:39,440
I'm conscious of the possibility. This is not a certainty. But the possibility of

252
00:25:39,440 --> 00:25:49,680
that engine of curiosity that you're speaking to is actually a symptom of the finiteness of life.

253
00:25:49,680 --> 00:25:56,640
Like without that finiteness, your curiosity would vanish like a morning fog.

254
00:25:57,680 --> 00:25:59,200
Okosky talked about love like that.

255
00:26:00,080 --> 00:26:03,840
Let me solve immortality. Let me change the thing in my brain that reminds me of the fact

256
00:26:03,840 --> 00:26:07,600
that I'm immortal tells me that life is finite shit. Maybe I'll have it tell me that life ends

257
00:26:07,600 --> 00:26:14,320
next week. I'm okay with some self manipulation like that. I'm okay with deceiving myself.

258
00:26:14,320 --> 00:26:17,120
Oh, Rika, changing the code.

259
00:26:17,120 --> 00:26:21,680
If that's the problem, right? If the problem is that I will no longer have that curiosity,

260
00:26:21,680 --> 00:26:28,000
I'd like to have backup copies of myself which I check in with occasionally to make sure they're

261
00:26:28,000 --> 00:26:32,320
okay with the trajectory and they can kind of override it. Maybe a nice like I think of like

262
00:26:32,320 --> 00:26:35,040
those wave nets, those like logarithmic go back to the copies.

263
00:26:35,040 --> 00:26:40,880
But sometimes it's not reversible. I've done this with video games. Once you figure out the

264
00:26:40,880 --> 00:26:46,640
cheat code or like you look up how to cheat old school like single player, it ruins the game for

265
00:26:46,720 --> 00:26:51,760
you. Absolutely. I know that feeling. But again, that just means our brain manipulation

266
00:26:51,760 --> 00:26:54,560
technology is not good enough yet. Remove that cheat code from your brain.

267
00:26:55,600 --> 00:27:02,720
So it's also possible that if we figure out immortality that all of us will kill ourselves

268
00:27:03,360 --> 00:27:08,640
before we advance far enough to be able to revert the change.

269
00:27:08,640 --> 00:27:10,240
I'm not killing myself till I know everything.

270
00:27:11,760 --> 00:27:14,720
That's what you say now because your life is finite.

271
00:27:15,680 --> 00:27:21,360
You know, I think self modifying systems comes up with all these hairy complexities and

272
00:27:21,360 --> 00:27:25,440
can I promise that I'll do it perfectly? No, but I think I can put good safety structures in place.

273
00:27:27,120 --> 00:27:30,720
So that talk in your thinking here is not literally

274
00:27:33,120 --> 00:27:41,360
referring to a simulation in that our universe is a kind of computer program running in a computer.

275
00:27:42,080 --> 00:27:49,040
That's more of a thought experiment. Do you also think of the potential of the sort of

276
00:27:50,560 --> 00:27:59,520
Bostrom, Elon Musk, and others that talk about an actual program that simulates our universe?

277
00:27:59,520 --> 00:28:05,120
Oh, I don't doubt that we're in a simulation. I just think that it's not quite that important.

278
00:28:05,120 --> 00:28:08,880
I mean, I'm interested only in simulation theory as far as like it gives me power over nature.

279
00:28:09,680 --> 00:28:12,960
If it's totally unfalsifiable, then who cares?

280
00:28:12,960 --> 00:28:17,040
I mean, what do you think that experiment would look like? Like somebody on Twitter asks

281
00:28:18,160 --> 00:28:22,800
George what signs we would look for to know whether or not we're in the simulation,

282
00:28:22,800 --> 00:28:29,680
which is exactly what you're asking is like the step that precedes the step of knowing how to

283
00:28:29,680 --> 00:28:35,120
get more power from this knowledge is to get an indication that there's some power to be gained.

284
00:28:35,840 --> 00:28:42,000
Get an indication that you can discover and exploit cracks in the simulation,

285
00:28:42,000 --> 00:28:44,960
or it doesn't have to be in the physics of the universe.

286
00:28:46,640 --> 00:28:50,240
Show me. I mean, like a memory leak would be cool.

287
00:28:51,520 --> 00:28:55,200
Like some scrying technology. What kind of technology?

288
00:28:55,200 --> 00:29:01,840
Scrying. What's that? That's a weird. Scrying is the paranormal ability to

289
00:29:02,800 --> 00:29:06,720
like remote viewing, like being able to see somewhere where you're not.

290
00:29:08,160 --> 00:29:14,000
So, I don't think you can do it by chanting in a room, but if we could find, it's a memory leak,

291
00:29:14,000 --> 00:29:19,920
basically. It's a memory leak. Yeah, you're able to access parts you're not supposed to.

292
00:29:19,920 --> 00:29:22,000
Yeah, yeah, yeah. And thereby discover shortcut.

293
00:29:22,000 --> 00:29:25,280
Yeah, memory leak means the other thing as well. But I mean like, yeah,

294
00:29:25,280 --> 00:29:29,760
like an ability to read arbitrary memory. And that one's not that horrifying.

295
00:29:29,760 --> 00:29:34,720
The right ones start to be horrifying. Read it right. So, the reading is not the problem.

296
00:29:34,720 --> 00:29:39,680
Yeah, it's like Heartbleed for the universe. Oh boy, the writing is a big, big problem.

297
00:29:40,560 --> 00:29:46,080
It's a big problem. It's the moment you can write anything, even if it's just random noise.

298
00:29:47,600 --> 00:29:52,480
That's terrifying. I mean, even without, even without that, like even some of the, you know,

299
00:29:52,480 --> 00:29:58,800
the nanotech stuff that's coming, I think. I don't know if you're paying attention, but

300
00:29:58,880 --> 00:30:03,840
actually Eric Weinstein came out with the theory of everything. I mean, that came out. He's been

301
00:30:03,840 --> 00:30:08,800
working on a theory of everything in the physics world called geometric unity. And then for me,

302
00:30:08,800 --> 00:30:14,480
from computer science person, like you, Stephen Wolfram's theory of everything of like,

303
00:30:14,480 --> 00:30:19,760
hypergraphs is super interesting and beautiful. But not from a physics perspective, but from a

304
00:30:19,760 --> 00:30:22,720
computational perspective. I don't know, have you paid attention to any of that?

305
00:30:23,040 --> 00:30:28,640
So, again, like what would make me pay attention and like why like a hate string theory is,

306
00:30:29,360 --> 00:30:34,640
okay, make a testable prediction, right? I'm only interested in, I'm not interested in theories

307
00:30:34,640 --> 00:30:38,160
for their intrinsic beauty. I'm interested in theories that give me power over the universe.

308
00:30:39,760 --> 00:30:42,080
So, if these theories do, I'm very interested.

309
00:30:42,960 --> 00:30:48,080
Can I just say how beautiful that is? Because a lot of physicists say, I'm interested in

310
00:30:48,160 --> 00:30:54,320
experimental validation. And they skip out the part where they say, to give me more power in

311
00:30:54,320 --> 00:31:02,800
the universe. I just love the clarity of that. I want 100 gigahertz processors. I want transistors

312
00:31:02,800 --> 00:31:12,240
that are smaller than atoms. I want like power. That's true. And that's where people from aliens

313
00:31:12,320 --> 00:31:18,160
to this kind of technology where people are worried that governments, like who owns that power?

314
00:31:19,200 --> 00:31:25,760
Is it George Haatz? Is it thousands of distributed hackers across the world? Is it governments?

315
00:31:26,560 --> 00:31:33,840
You know, is it Mark Zuckerberg? There's a lot of people that, I don't know if anyone trusts

316
00:31:33,840 --> 00:31:36,560
anyone individual with power. So, they're always worried.

317
00:31:37,120 --> 00:31:38,400
It's the beauty of blockchains.

318
00:31:39,200 --> 00:31:43,840
That's the beauty of blockchains, which we'll talk about on Twitter.

319
00:31:43,840 --> 00:31:49,200
Somebody pointed me to a story, a bunch of people pointed me to a story a few months ago,

320
00:31:49,200 --> 00:31:53,040
where you went into a restaurant in New York, and you can correct me if I'm saying this is wrong,

321
00:31:53,680 --> 00:32:00,320
and ran into a bunch of folks from a company in a crypto company who are trying to scale up Ethereum.

322
00:32:01,760 --> 00:32:06,560
And they had a technical deadline related to a solidity to OVM compiler.

323
00:32:07,280 --> 00:32:13,200
So, these are all Ethereum technologies. So, you stepped in, they recognized you,

324
00:32:14,640 --> 00:32:18,320
pulled you aside, explained their problem, and you stepped in and helped them solve the problem,

325
00:32:19,520 --> 00:32:29,520
thereby creating legend status story. So, can you tell me the story a little more detail? It seems

326
00:32:29,520 --> 00:32:34,320
kind of incredible. Did this happen? Yeah, yeah, it's a true story. It's a true story. I mean,

327
00:32:34,400 --> 00:32:43,680
they wrote a very flattering account of it. So, the company is called Optimism,

328
00:32:43,680 --> 00:32:48,400
spin-off of Plasma. They're trying to build L2 solutions on Ethereum. So, right now,

329
00:32:50,800 --> 00:32:55,680
every Ethereum node has to run every transaction on the Ethereum network.

330
00:32:56,560 --> 00:33:00,640
And this kind of doesn't scale, right? Because if you have N computers, well,

331
00:33:00,640 --> 00:33:04,160
if that becomes two N computers, you actually still get the same amount of compute.

332
00:33:04,960 --> 00:33:11,440
Right? This is like O of one scaling because they all have to run it. Okay, fine. You get more

333
00:33:11,440 --> 00:33:16,320
blockchain security, but like, blockchain is already so secure. Can we trade some of that off

334
00:33:16,320 --> 00:33:22,080
for speed? So, that's kind of what these L2 solutions are. They built this thing, which kind of

335
00:33:23,200 --> 00:33:29,040
sandbox for Ethereum contracts. So, they can run it in this L2 world, and it can't do certain

336
00:33:29,040 --> 00:33:35,760
things in L1. Can I ask you for some definitions? What's L2? Oh, L2 is Layer 2. So, L1 is like the

337
00:33:35,760 --> 00:33:41,600
base Ethereum chain, and then Layer 2 is like a computational layer that runs

338
00:33:43,360 --> 00:33:49,600
elsewhere, but still is kind of secured by Layer 1. And I'm sure a lot of people know,

339
00:33:49,600 --> 00:33:54,080
but Ethereum is a cryptocurrency, probably one of the most popular cryptocurrency, second to

340
00:33:54,080 --> 00:34:01,200
Bitcoin, and a lot of interesting technological innovations there. Maybe you can also slip in

341
00:34:01,840 --> 00:34:06,240
whenever you talk about this, any things that are exciting to you in the Ethereum space.

342
00:34:06,240 --> 00:34:13,200
And why Ethereum? Well, I mean, Bitcoin is not turned complete. Ethereum is not technically

343
00:34:13,200 --> 00:34:18,000
turned complete with the gas limit, but close enough. With the gas limit? What's the gas limit?

344
00:34:18,960 --> 00:34:20,800
Yeah, I mean, no computer is actually turned complete.

345
00:34:22,960 --> 00:34:25,520
They're just fine at RAM. I can actually solve the whole problem.

346
00:34:25,520 --> 00:34:29,280
What's the word gas limit? You just have so many brilliant words. I'm not even gonna ask.

347
00:34:29,280 --> 00:34:31,600
Well, that's not my word. That's Ethereum's word.

348
00:34:32,560 --> 00:34:37,040
Ethereum, you have to spend gas per instruction. So, different op codes use different amounts of

349
00:34:37,040 --> 00:34:41,920
gas, and you buy gas with Ether to prevent people from basically de-dossing the network.

350
00:34:42,640 --> 00:34:46,960
So, Bitcoin is proof of work, and then what's Ethereum?

351
00:34:47,040 --> 00:34:50,960
It's also proof of work. They're working on some proof of stake Ethereum 2.0 stuff,

352
00:34:50,960 --> 00:34:54,640
but right now, it's proof of work. It uses a different hash function from Bitcoin

353
00:34:54,640 --> 00:34:56,560
that's more ASIC resistance because you need RAM.

354
00:34:57,280 --> 00:35:02,880
So, we're all talking about Ethereum 1.0. So, what were they trying to do to scale

355
00:35:02,880 --> 00:35:06,960
this whole process? So, they were like, well, if we could run contracts elsewhere,

356
00:35:07,920 --> 00:35:13,680
and then only save the results of that computation, well, we don't actually have

357
00:35:13,680 --> 00:35:16,800
to do the compute on the chain. We can do the compute off chain and just post what the results

358
00:35:16,800 --> 00:35:21,120
are. Now, the problem with that is, well, somebody could lie about what the results are.

359
00:35:21,120 --> 00:35:25,520
So, you need a resolution mechanism, and the resolution mechanism can be really expensive

360
00:35:26,400 --> 00:35:32,320
because you just have to make sure that the person who is saying, look, I swear that this

361
00:35:32,320 --> 00:35:39,120
is the real computation. I'm staking $10,000 on that fact, and if you prove it wrong, yeah,

362
00:35:39,120 --> 00:35:44,640
it might cost you $3,000 in gas fees to prove wrong, but you'll get the $10,000 bounty.

363
00:35:44,640 --> 00:35:50,480
So, you can secure using those kind of systems. So, it's effectively a sandbox,

364
00:35:50,480 --> 00:35:56,720
which runs contracts, and like just like any kind of normal sandbox, you have to like replace

365
00:35:56,720 --> 00:36:00,320
syscalls with, you know, calls into the hypervisor.

366
00:36:02,880 --> 00:36:05,920
Sandbox, syscalls, hypervisor, what do these things mean?

367
00:36:07,520 --> 00:36:09,040
As long as it's interesting to talk about.

368
00:36:09,040 --> 00:36:12,640
Yeah, I mean, you can take like the Chrome sandbox is maybe the one to think about, right?

369
00:36:12,720 --> 00:36:17,680
So, the Chrome process is doing a rendering. I can't, for example, read a file from the file

370
00:36:17,680 --> 00:36:23,920
system. If it tries to make an open syscall in Linux, you can't make an open syscall. No, no,

371
00:36:23,920 --> 00:36:31,040
no. You have to request from the kind of hypervisor process or like, I don't know what's

372
00:36:31,040 --> 00:36:37,440
called in Chrome, but the, hey, could you open this file for me? And then it does all these checks,

373
00:36:37,440 --> 00:36:41,600
and then it passes the file, handle back in if it's approved. So, that's yeah.

374
00:36:43,040 --> 00:36:47,600
What's the, in the context of Ethereum, what are the boundaries of the sandbox that we're talking

375
00:36:47,600 --> 00:36:54,160
about? Well, like one of the calls that you actually reading and writing any state to the

376
00:36:54,160 --> 00:37:01,200
Ethereum contract or to the Ethereum blockchain. Writing state is one of those calls that you're

377
00:37:01,200 --> 00:37:08,240
going to have to sandbox in layer two, because if you let layer two just arbitrarily write to the

378
00:37:08,240 --> 00:37:15,520
Ethereum blockchain. So, layer two is really sitting on top of layer one. So, you're going to

379
00:37:15,520 --> 00:37:20,320
have a lot of different kinds of ideas that you can play with. And they're all, they're not fundamentally

380
00:37:20,320 --> 00:37:28,800
changing the source code level of Ethereum. Well, you have to replace a bunch of calls

381
00:37:28,800 --> 00:37:35,360
with calls into the hypervisor. So, instead of doing the syscall directly, you replace it with a

382
00:37:35,360 --> 00:37:42,400
call to the hypervisor. So, originally they were doing this by first running the, so solidity is

383
00:37:42,400 --> 00:37:47,760
the language that most Ethereum contracts are written in, it compiles to a bytecode. And then

384
00:37:47,760 --> 00:37:52,800
they wrote this thing they called the transpiler. And the transpiler took the bytecode and it

385
00:37:52,800 --> 00:37:58,160
transpiled it into OVM safe bytecode, basically bytecode that didn't make any of those restricted

386
00:37:58,160 --> 00:38:04,240
syscalls and added the calls to the hypervisor. This transpiler was a 3000 line mess.

387
00:38:05,520 --> 00:38:09,440
And it's hard to do. It's hard to do if you're trying to do it like that because you have to

388
00:38:09,440 --> 00:38:15,760
kind of like deconstruct the bytecode, change things about it, and then reconstruct it. And

389
00:38:15,760 --> 00:38:18,960
I mean, as soon as I hear this, I'm like, well, why don't you just change the compiler?

390
00:38:19,760 --> 00:38:23,280
Right? Why not the first place you build the bytecode, just do it in the compiler?

391
00:38:25,520 --> 00:38:30,960
So, yeah, you know, I asked them how much they wanted it. Of course, we're measured in dollars

392
00:38:30,960 --> 00:38:37,520
and I'm like, well, okay. And yeah, you wrote the compiler. Yeah, I modified, I wrote a 300

393
00:38:37,520 --> 00:38:42,400
line diff to the compiler. It's open source, you can look at it. Yeah, I looked at the code last

394
00:38:42,400 --> 00:38:55,920
night. Yeah, exactly. Qt is a good word for it. And it's C++. C++, yeah. So when asked how you

395
00:38:55,920 --> 00:39:03,760
were able to do it, you said, you just got to think and then do it right. So can you break that

396
00:39:03,760 --> 00:39:08,800
apart a little bit? What's your process of one thinking and two doing it right?

397
00:39:09,760 --> 00:39:14,080
You know, the people who I was working for are amused that I said that it doesn't really mean

398
00:39:14,080 --> 00:39:22,560
anything. Okay. I mean, is there some deep profound insights to draw from like how you problem solve

399
00:39:22,560 --> 00:39:26,640
from that? This is always what I say. I'm like, do you want to be a good programmer? Do it for 20

400
00:39:26,640 --> 00:39:36,240
years? Yeah, there's no shortcuts. What are your thoughts on crypto in general? So what parts

401
00:39:36,240 --> 00:39:41,600
technically or philosophically defined, especially beautiful, maybe? Oh, I'm extremely bullish on

402
00:39:41,600 --> 00:39:50,560
crypto long term, not any specific crypto project, but this idea of, well, two ideas. One,

403
00:39:51,360 --> 00:39:58,480
the Nakamoto consensus algorithm is I think one of the greatest innovations of the 21st century.

404
00:39:58,480 --> 00:40:04,480
This idea that people can reach consensus, you can reach a group consensus using a relatively

405
00:40:04,480 --> 00:40:14,880
straightforward algorithm is wild. And like Satoshi Nakamoto, people always ask me who

406
00:40:14,880 --> 00:40:21,360
I look up to. It's like, whoever that is. Who do you think it is? Elon Musk? Is it you?

407
00:40:22,160 --> 00:40:30,240
It is definitely not me. And I do not think it's Elon Musk. But yeah, this idea of groups

408
00:40:30,240 --> 00:40:37,440
reaching consensus in a decentralized yet formulaic way is one extremely powerful idea from crypto.

409
00:40:38,080 --> 00:40:48,400
Maybe the second idea is this idea of smart contracts. When you write a contract between

410
00:40:48,400 --> 00:40:55,120
two parties, any contract, this contract, if there are disputes, it's interpreted by lawyers.

411
00:40:56,080 --> 00:41:02,560
Lawyers are just really shitty overpaid interpreters. Let's talk about them in terms of like,

412
00:41:02,560 --> 00:41:09,920
let's compare a lawyer to Python. Well, okay. That's brilliant. I never thought of it that way.

413
00:41:09,920 --> 00:41:17,040
It's hilarious. So Python, I'm paying even 10 cents an hour. I'll use the nice Azure machine.

414
00:41:17,040 --> 00:41:24,880
I can run Python for 10 cents an hour. Lawyers cost $1,000 an hour. So Python is 10,000x better

415
00:41:24,880 --> 00:41:32,080
on that axis. Lawyers don't always return the same answer. Python almost always does.

416
00:41:36,640 --> 00:41:42,720
Cost. Yeah. I mean, just cost, reliability, everything about Python is so much better than

417
00:41:42,720 --> 00:41:52,400
lawyers. So if you can make smart contracts, this whole concept of code is law. I would love to

418
00:41:52,400 --> 00:42:00,000
live in a world where everybody accepted that fact. So maybe you can talk about what smart contracts

419
00:42:00,000 --> 00:42:13,040
are. So let's say we have even something as simple as a safety deposit box. Safety deposit box that

420
00:42:13,040 --> 00:42:18,640
holds a million dollars. I have a contract with the bank that says two out of these three parties

421
00:42:19,600 --> 00:42:25,840
must be present to open the safety deposit box and get the money out. So that's a contract with

422
00:42:25,840 --> 00:42:32,640
the bank. And it's only as good as the bank and the lawyers. Let's say somebody dies and now,

423
00:42:32,640 --> 00:42:36,400
oh, we're going to go through a big legal dispute about whether, oh, was it in the will? Was it not

424
00:42:36,400 --> 00:42:45,120
in the will? It's just so messy. And the cost to determine truth is so expensive versus a smart

425
00:42:45,200 --> 00:42:48,880
contract, which just uses cryptography to check if two out of three keys are present.

426
00:42:50,000 --> 00:42:55,760
Well, I can look at that and I can have certainty in the answer that it's going to return.

427
00:42:55,760 --> 00:42:59,520
And that's what all businesses want is certainty. You know, they say businesses don't care,

428
00:42:59,520 --> 00:43:04,720
Viacom YouTube. YouTube's like, look, we don't care which way this lawsuit goes. Just please

429
00:43:04,720 --> 00:43:10,320
tell us so we can have certainty. I wonder how many agreements in this, because we're talking about

430
00:43:10,400 --> 00:43:16,160
financial transactions only in this case, correct? The smart contracts. Oh, you can go to,

431
00:43:16,160 --> 00:43:19,360
you can go to anything. You can go, you can put a prenup in the theorem blockchain.

432
00:43:21,840 --> 00:43:26,640
Married smart contract. Sorry, divorce lawyer. Sorry, you're going to be replaced by Python.

433
00:43:29,520 --> 00:43:37,120
Okay, so that's, so that's another beautiful idea. Do you think there's something that's

434
00:43:37,120 --> 00:43:43,760
appealing to you about any one specific implementation? So if you look 10, 20, 50 years

435
00:43:43,760 --> 00:43:50,080
down the line, do you see any like Bitcoin, Ethereum, any of the other hundreds of cryptocurrencies

436
00:43:50,080 --> 00:43:54,320
winning out? Is there like, what's your intuition about the space? Are you just sitting back and

437
00:43:54,320 --> 00:43:59,040
watching the chaos and look who cares what emerges? Oh, I don't, I don't speculate. I don't

438
00:43:59,040 --> 00:44:03,760
really care. I don't really care which one of these projects wins. I'm kind of in the Bitcoin

439
00:44:03,760 --> 00:44:09,680
as a meme coin camp. I mean, why does Bitcoin have value? It's technically kind of, you know,

440
00:44:11,600 --> 00:44:16,080
not great. Like the block size debate, or when I found out what the block size debate was, I'm

441
00:44:16,080 --> 00:44:22,640
like, are you guys kidding? What's the block size debate? You know what, it's really, it's too stupid

442
00:44:22,640 --> 00:44:28,960
to even talk. People can look it up, but I'm like, wow, you know, Ethereum seems, the governance of

443
00:44:28,960 --> 00:44:35,680
Ethereum seems much better. I've come around a bit on proof of stake ideas. You know, very

444
00:44:35,680 --> 00:44:41,200
smart people thinking about some things. Yeah, you know, governance is interesting. It does feel

445
00:44:41,200 --> 00:44:49,280
like Vitalik, it could just feel like an open, even in these distributed systems, leaders are

446
00:44:49,280 --> 00:44:56,960
helpful because they kind of help you drive the mission and the vision. And they put a face to

447
00:44:56,960 --> 00:45:03,600
a project. It's a weird thing about us humans. Geniuses are helpful, like Vitalik. Yeah, brilliant.

448
00:45:06,480 --> 00:45:13,440
Leaders are not necessarily, yeah. So you think the reason he's the face

449
00:45:14,560 --> 00:45:19,280
of Ethereum is because he's a genius. That's interesting. I mean, that was,

450
00:45:20,240 --> 00:45:28,800
it's interesting to think about that we need to create systems in which the quote unquote leaders

451
00:45:28,800 --> 00:45:35,200
that emerge are the geniuses in the system. I mean, that's arguably why the current state of

452
00:45:35,200 --> 00:45:40,240
democracy is broken is the people who are emerging as the leaders are not the most competent, are

453
00:45:40,240 --> 00:45:46,160
not the superstars of the system. And it seems like at least for now in the crypto world, oftentimes

454
00:45:47,120 --> 00:45:50,880
the leaders are the superstars. Imagine at the debate, they asked,

455
00:45:51,600 --> 00:45:55,440
what's the sixth amendment? What are the four fundamental forces in the universe?

456
00:45:56,080 --> 00:46:01,920
What's the integral of two to the X? Yeah, I'd love to see those questions asked. And that's

457
00:46:01,920 --> 00:46:09,680
what I want as our leader. It's a little bit. What's Bayes rule? Yeah, I mean, even, oh, wow,

458
00:46:09,760 --> 00:46:16,480
you're hurting my brain. My standard was even lower, but I would have loved to see

459
00:46:17,600 --> 00:46:23,040
just this basic brilliance, like I've talked to historians. There's just these,

460
00:46:23,040 --> 00:46:27,120
they're not even like, they don't have a PhD or even education history. They just

461
00:46:27,760 --> 00:46:34,320
like a Dan Carlin type character who just like, holy shit, how did all this information get

462
00:46:34,320 --> 00:46:40,240
into your head? They were able to just connect Genghis Khan to the entirety of the history of

463
00:46:40,240 --> 00:46:46,800
the 20th century. They know everything about every single battle that happened. And they know the

464
00:46:48,480 --> 00:46:55,040
like the game of thrones of the different power plays and all that happened there.

465
00:46:55,040 --> 00:47:00,960
And they know like the individuals and all the documents involved. And they integrate that into

466
00:47:01,040 --> 00:47:06,240
their regular life. It's not like they're ultra history nerds. They're just, they know this information.

467
00:47:06,240 --> 00:47:10,720
That's what competence looks like. Yeah. Because I've seen that with programmers too, right? That's

468
00:47:10,720 --> 00:47:16,640
what great programmers do. But yeah, it'll be, it's really unfortunate that those kinds of people

469
00:47:16,640 --> 00:47:22,480
aren't emerging as our leaders. But for now, at least in the crypto world, that seems to be the

470
00:47:22,480 --> 00:47:28,320
case. I don't know if that always, you could imagine that in 100 years, it's not the case,

471
00:47:28,800 --> 00:47:32,960
crypto world has one very powerful idea going for it. And that's the idea of forks.

472
00:47:34,880 --> 00:47:44,000
I mean, imagine we'll use a less controversial example. This was actually in my joke

473
00:47:46,080 --> 00:47:50,720
app in 2012. I was like, Barack Obama, Mitt Romney, let's let them both be president.

474
00:47:50,720 --> 00:47:54,720
All right. Like imagine we could fork America and just let them both be president. And then

475
00:47:54,720 --> 00:47:59,360
the Americas could compete and people could invest in one, pull their liquidity out of one,

476
00:47:59,360 --> 00:48:04,160
put it in the other. You have this in the crypto world. Ethereum forks into Ethereum and Ethereum

477
00:48:04,160 --> 00:48:09,760
Classic. And you can pull your liquidity out of one and put it in another. And people vote with

478
00:48:09,760 --> 00:48:17,520
their dollars, which forks, companies should be able to fork. I'd love to fork Nvidia.

479
00:48:18,000 --> 00:48:25,520
Yeah. Like different business strategies. And then try them out and see what works.

480
00:48:26,080 --> 00:48:36,560
Like even take, yeah, take common AI that closes its source and then take one that's open source

481
00:48:36,560 --> 00:48:43,440
and see what works. Take one that's purchased by GM and one that remains Android Renegade

482
00:48:43,440 --> 00:48:47,360
in all these different versions and see the beauty of common AI. Someone can actually do that.

483
00:48:47,840 --> 00:48:52,160
Please take common AI and fork it. That's right. That's the beauty of open source.

484
00:48:52,960 --> 00:48:59,440
So you're, I mean, we'll talk about autonomous vehicle space, but it does seem that you're

485
00:49:01,120 --> 00:49:05,440
really knowledgeable about a lot of different topics. So the natural question a bunch of people

486
00:49:05,440 --> 00:49:11,600
ask this, which is how do you keep learning new things? Do you have like practical advice

487
00:49:12,320 --> 00:49:19,200
if you were to introspect like taking notes, allocate time, or do you just mess around and

488
00:49:19,200 --> 00:49:24,400
just allow your curiosity to drive? I'll write these people a self-help book and I'll charge $67

489
00:49:24,400 --> 00:49:31,680
for it. And I will write on the cover of the self-help book. All of this advice is completely

490
00:49:31,680 --> 00:49:36,800
meaningless. You're going to be a sucker and buy this book anyway. And the one lesson that I hope

491
00:49:36,800 --> 00:49:41,520
they take away from the book is that I can't give you a meaningful answer to that.

492
00:49:42,400 --> 00:49:50,080
That's interesting. Let me translate that as you haven't really thought about what it is you do

493
00:49:51,520 --> 00:49:55,840
systematically because you could reduce it. And then some people, I mean, I've met brilliant

494
00:49:55,840 --> 00:50:04,160
people that this is really clear with athletes. Some are just the best in the world at something

495
00:50:05,040 --> 00:50:11,440
and they have zero interest in writing like a self-help book or how to master this game.

496
00:50:11,440 --> 00:50:17,200
And then there are some athletes who become great coaches and they love the analysis,

497
00:50:17,200 --> 00:50:21,680
perhaps the over-analysis. And you right now, at least at your age, which is an interesting,

498
00:50:21,680 --> 00:50:25,920
you're in the middle of the battle. You're like the warriors that have zero interest in writing

499
00:50:25,920 --> 00:50:32,320
books. So you're in the middle of the battle. So yeah. This is a fair point. I do think I have

500
00:50:32,320 --> 00:50:39,520
a certain aversion to this kind of deliberate, intentional way of living life.

501
00:50:40,640 --> 00:50:45,120
You eventually, the hilarity of this, especially since this is recorded,

502
00:50:47,120 --> 00:50:50,640
it will reveal beautifully the absurdity when you finally do publish this book.

503
00:50:51,200 --> 00:50:58,720
That guarantee you will. The story of comma AI, maybe it'll be a biography written about you.

504
00:50:59,200 --> 00:51:02,880
That'll be better, I guess. And you might be able to learn some cute lessons if you're starting

505
00:51:02,880 --> 00:51:07,520
a company like comma AI from that book. But if you're asking generic questions like,

506
00:51:07,520 --> 00:51:14,160
how do I be good at things? Dude, I don't know. Well, I mean, the interesting- Do them a lot.

507
00:51:14,160 --> 00:51:21,760
Do them a lot. But the interesting thing here is learning things outside of your current trajectory,

508
00:51:21,760 --> 00:51:28,560
which is what it feels like from an outsider's perspective. I don't know if there's

509
00:51:29,200 --> 00:51:34,640
advice on that, but it is an interesting curiosity. When you become really busy,

510
00:51:34,640 --> 00:51:43,920
you're running a company. Hard time. Yeah. But there's a natural inclination and

511
00:51:45,040 --> 00:51:50,960
trend. Just the momentum of life carries you into a particular direction of wanting to focus.

512
00:51:50,960 --> 00:51:58,000
And this kind of dispersion that curiosity can lead to gets harder and harder with time.

513
00:51:58,560 --> 00:52:03,680
You get really good at certain things, and it sucks trying things that you're not good at,

514
00:52:03,680 --> 00:52:08,880
like trying to figure them out. I mean, you do this with your live streams. You're on the fly

515
00:52:08,880 --> 00:52:15,920
figuring stuff out. You don't mind looking dumb. No. You just figured out pretty quickly.

516
00:52:16,560 --> 00:52:20,880
Sometimes I try things and I don't figure them out quickly. My chest rating is like a 1400,

517
00:52:20,880 --> 00:52:25,840
despite putting like a couple hundred hours in. It's pathetic. I mean, to be fair, I know that

518
00:52:25,840 --> 00:52:30,960
I could do it better. If I did it better, don't play five minute games, play 15 minute games at

519
00:52:30,960 --> 00:52:36,080
least. I know these things, but it just doesn't. It doesn't stick nicely in my knowledge stream.

520
00:52:37,120 --> 00:52:43,360
All right. Let's talk about Kama AI. What's the mission of the company? Let's look at the biggest

521
00:52:43,360 --> 00:52:50,640
picture. Oh, I have an exact statement. Solve self-driving cars while delivering shipable intermediaries.

522
00:52:51,440 --> 00:52:57,520
So long-term vision is have fully autonomous vehicles and make sure you're making money along

523
00:52:57,520 --> 00:53:02,560
the way. I think it doesn't really speak to money, but I can talk about what solve self-driving cars

524
00:53:02,560 --> 00:53:08,400
means. Solve self-driving cars, of course, means you're not building a new car. You're building

525
00:53:08,400 --> 00:53:13,840
a person replacement. That person can sit in the driver's seat and drive you anywhere a person can

526
00:53:13,840 --> 00:53:19,200
drive with a human or better level of safety, speed, quality, comfort.

527
00:53:21,280 --> 00:53:23,040
And what's the second part of that?

528
00:53:23,040 --> 00:53:27,920
Delivering shipable intermediaries is, well, it's a way to fund the company. That's true,

529
00:53:27,920 --> 00:53:35,040
but it's also a way to keep us honest. If you don't have that, it is very easy with

530
00:53:35,840 --> 00:53:41,280
this technology to think you're making progress when you're not. I've heard it best described on

531
00:53:41,280 --> 00:53:48,960
Hacker News as you can set any arbitrary milestone, meet that milestone, and still be infinitely

532
00:53:48,960 --> 00:53:54,080
far away from solving self-driving cars. So it's hard to have real deadlines when you're

533
00:53:55,120 --> 00:54:06,880
cruise or Waymo when you don't have revenue. Is revenue essentially the thing we're talking

534
00:54:06,880 --> 00:54:12,640
about here? Capitalism is based around consent. Capitalism, the way that you get

535
00:54:12,640 --> 00:54:17,200
revenue is real capitalism. Common is in the real capitalism camp. There's definitely scams out there,

536
00:54:17,200 --> 00:54:21,200
but real capitalism is based around consent. It's based around this idea that if we're getting

537
00:54:21,200 --> 00:54:25,600
revenue, it's because we're providing at least that much value to another person. When someone buys

538
00:54:25,600 --> 00:54:30,080
$1,000 comma two from us, we're providing them at least $1,000 of value, but they wouldn't buy it.

539
00:54:30,080 --> 00:54:34,560
Brilliant. So can you give a world-wind overview of the products that Kamii provides?

540
00:54:34,880 --> 00:54:40,720
Like throughout its history and today? I mean, yeah, the past ones aren't really that interesting.

541
00:54:40,720 --> 00:54:47,440
It's kind of just been refinement of the same idea. The real only product we sell today is the comma

542
00:54:47,440 --> 00:54:54,560
two, which is a piece of hardware with cameras. So the comma two, I mean, you can think about it

543
00:54:54,560 --> 00:54:59,360
kind of like a person. You know, in future hardware will probably be even more and more person-like.

544
00:55:00,160 --> 00:55:08,320
So it has eyes, ears, a mouth, a brain, and a way to interface with the car.

545
00:55:09,280 --> 00:55:12,560
Does it have consciousness? Just kidding. That was a trick question.

546
00:55:13,280 --> 00:55:16,240
I don't have consciousness either. Me and the comma two are the same.

547
00:55:16,240 --> 00:55:16,960
They are the same.

548
00:55:16,960 --> 00:55:20,880
I have a little more compute than it. It only has like the same compute as a B.

549
00:55:23,120 --> 00:55:26,320
You're more efficient energy-wise for the compute you're doing.

550
00:55:26,400 --> 00:55:30,480
Far more efficient energy-wise. 20 pay-to-flops, 20 wants, crazy.

551
00:55:30,480 --> 00:55:32,480
Do you lack consciousness? Sure.

552
00:55:32,480 --> 00:55:35,280
Do you fear death? You do. You want immortality.

553
00:55:35,280 --> 00:55:35,840
Of course I fear death.

554
00:55:35,840 --> 00:55:38,640
Does Kamii fear death? I don't think so.

555
00:55:39,360 --> 00:55:43,280
Of course it does. It very much fears, well, it fears negative loss. Oh, yeah.

556
00:55:45,600 --> 00:55:50,800
Okay. So comma two, when did that come out? That was a year ago? No, two.

557
00:55:51,760 --> 00:55:52,640
Early this year.

558
00:55:53,440 --> 00:56:00,560
Wow, time, it feels, yeah. 2020 feels like it's taken 10 years to get to the end.

559
00:56:00,560 --> 00:56:01,680
It's a long year.

560
00:56:01,680 --> 00:56:09,040
It's a long year. So what's the sexiest thing about comma two, feature-wise?

561
00:56:09,840 --> 00:56:15,600
So, I mean, maybe you can also link on like, what is it? Like, what's its purpose?

562
00:56:15,600 --> 00:56:20,560
Because there's a hardware, there's a software component. You've mentioned the sensors, but

563
00:56:20,640 --> 00:56:22,960
also like, what is it, its features and capabilities?

564
00:56:22,960 --> 00:56:26,960
I think our slogan summarizes it well. A comma slogan is make driving chill.

565
00:56:28,800 --> 00:56:35,280
I love it. Okay. Yeah, I mean, it is, you know, if you like cruise control, imagine cruise control,

566
00:56:35,280 --> 00:56:42,000
but much, much more. So it can do adaptive cruise control things, which is like, slow down for

567
00:56:42,000 --> 00:56:47,120
cars in front of it, maintain a certain speed. And it can also do lane keeping, so stay in the

568
00:56:47,120 --> 00:56:51,680
lane and do it better and better and better over time. It's very much machine learning based.

569
00:56:53,040 --> 00:56:56,320
So there's cameras, there's a driver facing camera too.

570
00:57:01,040 --> 00:57:06,560
What else is there? What am I thinking? So the hardware versus software, so open pilot versus

571
00:57:06,560 --> 00:57:11,440
the actual hardware, the device. What's, can you draw that distinction? What's one? What's the other?

572
00:57:11,440 --> 00:57:15,840
I mean, the hardware is pretty much a cell phone with a few additions, a cell phone with a cooling

573
00:57:15,840 --> 00:57:24,240
system and with a car interface connected to it. And by cell phone, you mean like Qualcomm Snapdragon?

574
00:57:25,520 --> 00:57:31,760
Yeah, the current hardware is a Snapdragon 821. It has Wi-Fi radio, it has an LTE radio, it has a screen.

575
00:57:33,840 --> 00:57:38,400
We use every part of the cell phone. And then the interface of the car is specific to the car,

576
00:57:38,400 --> 00:57:42,800
so you keep supporting more and more cars? Yeah, so the interface to the car, I mean,

577
00:57:42,800 --> 00:57:47,200
the device itself just has four CAN buses, has four CAN interfaces on it, they're connected

578
00:57:47,200 --> 00:57:54,400
through the USB port to the phone. And then, yeah, on those four CAN buses, you connect it to the car

579
00:57:54,400 --> 00:57:59,280
and there's a little part to do this. Cars are actually surprisingly similar. So CAN is the

580
00:57:59,280 --> 00:58:04,480
protocol about which cars communicate and then you're able to read stuff and write stuff to be

581
00:58:04,480 --> 00:58:09,120
able to control the car depending on the car. So what's the software side? What's open pilot?

582
00:58:09,920 --> 00:58:14,480
So, I mean, open pilot is, the hardware is pretty simple compared to open pilot, open pilot is...

583
00:58:18,000 --> 00:58:24,960
Well, so you have a machine learning model, which it's an open pilot, it's a blob, it's just a blob

584
00:58:24,960 --> 00:58:28,720
of weights. It's not like people are like, oh, it's closed source, I'm like, it's a blob of weights,

585
00:58:28,720 --> 00:58:32,800
what do you expect? So it's primarily neural network based?

586
00:58:33,680 --> 00:58:37,920
Well, open pilot is all the software kind of around that neural network, that if you have

587
00:58:37,920 --> 00:58:42,240
a neural network that says, here's where you want to send the car, open pilot actually goes and

588
00:58:42,240 --> 00:58:48,000
executes all of that. It cleans up the input to the neural network, it cleans up the output and

589
00:58:48,000 --> 00:58:53,200
executes on it. So it's the glue that connects everything together. Runs the sensors, does a

590
00:58:53,200 --> 00:58:58,480
bunch of calibration for the neural network, does, you know, deals with like, you know, if the car

591
00:58:58,480 --> 00:59:03,360
is on a banked road, you have to counter steer against that. And the neural network can't necessarily

592
00:59:03,360 --> 00:59:09,120
know that by looking at the picture. So you can do that with other sensors, infusion and localizer.

593
00:59:09,760 --> 00:59:15,840
Open pilot also is responsible for sending the data up to our servers, so we can learn from it,

594
00:59:16,640 --> 00:59:20,640
logging it, recording it, running the cameras, thermally managing the device,

595
00:59:21,360 --> 00:59:24,640
managing the disk space on the device, managing all the resources on the device.

596
00:59:24,640 --> 00:59:29,280
So what, since we last spoke, I don't remember when, maybe a year ago, maybe a little bit longer,

597
00:59:30,080 --> 00:59:32,960
how has open pilot improved?

598
00:59:32,960 --> 00:59:36,560
We did exactly what I promised you. I promised you that by the end of the year,

599
00:59:36,560 --> 00:59:46,000
we would be able to remove the lanes. The lateral policy is now almost completely end to end. You

600
00:59:46,000 --> 00:59:50,880
can turn the lanes off and it will drive slightly worse on the highway if you turn the lanes off,

601
00:59:50,880 --> 00:59:56,720
but you can turn the lanes off and it will drive well trained completely end to end on user data.

602
00:59:57,280 --> 00:59:59,920
And this year we hope to do the same for the longitudinal policy.

603
00:59:59,920 --> 01:00:04,400
So that's the interesting thing is you're not doing, you don't appear to be,

604
01:00:04,400 --> 01:00:10,000
maybe you can correct me, you don't appear to be doing lane detection or lane marking

605
01:00:10,000 --> 01:00:15,040
detection or kind of the segmentation task or any kind of object detection task,

606
01:00:15,040 --> 01:00:18,560
you're doing what's traditionally more called like end to end learning.

607
01:00:19,360 --> 01:00:25,840
So entrained on actual behavior of drivers when they're driving the car manually.

608
01:00:27,600 --> 01:00:31,120
And this is hard to do. It's not supervised learning.

609
01:00:32,080 --> 01:00:35,840
Yeah, but so the nice thing is there's a lot of data, so it's hard and easy.

610
01:00:37,680 --> 01:00:39,920
We have a lot of high quality data, yeah.

611
01:00:39,920 --> 01:00:41,600
Like more than you need in the sun.

612
01:00:41,600 --> 01:00:44,800
Well, we've way more than we do. We've way more data than we need.

613
01:00:44,800 --> 01:00:49,600
I mean, it's an interesting question, actually, because in terms of amount, you have more than

614
01:00:49,680 --> 01:00:54,160
you need. But the, you know, driving is full of edge cases.

615
01:00:54,160 --> 01:00:56,880
So how do you select the data you train on?

616
01:00:58,160 --> 01:01:00,480
I think this is an interesting open question.

617
01:01:00,480 --> 01:01:04,080
Like what's the cleverest way to select data?

618
01:01:04,080 --> 01:01:06,240
That's the question Tesla is probably working on.

619
01:01:07,600 --> 01:01:09,840
That's, I mean, the entirety of machine learning can be,

620
01:01:09,840 --> 01:01:12,160
they don't seem to really care. They just kind of select data.

621
01:01:12,160 --> 01:01:16,160
But I feel like that if you want to solve, if you want to create intelligence systems,

622
01:01:16,160 --> 01:01:18,800
you have to pick data well, right?

623
01:01:18,800 --> 01:01:22,800
And so do you have any hints, ideas of how to do it well?

624
01:01:22,800 --> 01:01:28,400
So in some ways, that is the definition I like of reinforcement learning versus supervised learning.

625
01:01:29,200 --> 01:01:32,880
In supervised learning, the weights depend on the data, right?

626
01:01:34,400 --> 01:01:39,360
And this is obviously true, but the, in reinforcement learning, the data depends on the weights.

627
01:01:40,320 --> 01:01:40,800
Yeah.

628
01:01:40,800 --> 01:01:42,400
Right. And actually both ways.

629
01:01:42,400 --> 01:01:43,920
That's poetry.

630
01:01:43,920 --> 01:01:44,800
That's brilliant.

631
01:01:44,800 --> 01:01:46,160
How does it know what data to train on?

632
01:01:46,160 --> 01:01:47,360
Well, let it pick.

633
01:01:47,360 --> 01:01:49,440
We're not there yet, but that's the eventual.

634
01:01:49,440 --> 01:01:52,560
So you're thinking this almost like a reinforcement learning framework.

635
01:01:53,120 --> 01:01:54,320
We're going to do RL on the world.

636
01:01:55,280 --> 01:01:58,000
Every time a car makes a mistake, user disengages.

637
01:01:58,000 --> 01:02:00,000
We train on that and do RL on the world.

638
01:02:00,000 --> 01:02:01,760
Ship out a new model. That's an epoch, right?

639
01:02:03,200 --> 01:02:09,600
And for now, you're not doing the Elon style promising that it's going to be fully autonomous.

640
01:02:09,600 --> 01:02:14,400
You really are sticking to level two and like it's supposed to be supervised.

641
01:02:15,440 --> 01:02:18,240
It is definitely supposed to be supervised and we enforce the fact that it's supervised.

642
01:02:19,680 --> 01:02:23,520
We look at our rate of improvement in disengagements.

643
01:02:23,520 --> 01:02:26,640
OpenPilot now has an unplanned disengagement about every 100 miles.

644
01:02:27,280 --> 01:02:36,400
This is up from 10 miles, like maybe, maybe a, maybe a year ago.

645
01:02:36,400 --> 01:02:41,600
Yeah. So maybe we've seen 10x improvement in a year, but 100 miles is still a far cry

646
01:02:41,600 --> 01:02:43,200
from the 100,000 you're going to need.

647
01:02:43,760 --> 01:02:48,240
So you're going to somehow need to get three more 10x's in there.

648
01:02:49,760 --> 01:02:51,280
And you're, what's your intuition?

649
01:02:52,240 --> 01:02:55,440
You're basically hoping that there's exponential improvement built into the,

650
01:02:55,440 --> 01:02:56,720
baked into the cake somewhere.

651
01:02:56,720 --> 01:03:00,400
Well, that's even, I mean, 10x improvement, that's already assuming exponential, right?

652
01:03:00,400 --> 01:03:02,480
There's definitely exponential improvement.

653
01:03:02,480 --> 01:03:05,200
And I think when Elon talks about exponential, like these things,

654
01:03:05,200 --> 01:03:07,760
these systems are going to exponentially improve.

655
01:03:07,760 --> 01:03:12,800
Just exponential doesn't mean you're getting 100 gigahertz processors tomorrow, right?

656
01:03:13,200 --> 01:03:16,080
Like it's going to still take a while because the gap

657
01:03:16,080 --> 01:03:19,520
between even our best system and humans is still large.

658
01:03:20,160 --> 01:03:22,240
So that's an interesting distinction to draw.

659
01:03:22,240 --> 01:03:24,800
So if you look at the way Tesla is approaching the problem,

660
01:03:26,000 --> 01:03:30,640
and the way you're approaching the problem, which is very different than the rest of the

661
01:03:30,640 --> 01:03:32,560
self-driving car world.

662
01:03:32,560 --> 01:03:33,760
So let's put them aside.

663
01:03:33,760 --> 01:03:37,440
Is you're treating most, the driving task as a machine learning problem.

664
01:03:37,440 --> 01:03:40,960
And the way Tesla is approaching it is with the multitask learning,

665
01:03:40,960 --> 01:03:45,280
where you break the task of driving into hundreds of different tasks.

666
01:03:45,280 --> 01:03:51,520
And you have this multi-headed neural network that's very good at performing each task.

667
01:03:51,520 --> 01:03:56,240
And there's presumably something on top that's stitching stuff together

668
01:03:56,240 --> 01:04:02,080
in order to make control decisions, policy decisions about how you move the car.

669
01:04:02,080 --> 01:04:05,520
But what that allows you, there's a brilliance to this because it allows you to

670
01:04:05,760 --> 01:04:14,560
master each task, like lane detection, stop sign detection, the traffic light detection,

671
01:04:15,440 --> 01:04:21,280
drivable area segmentation, vehicle bicycle pedestrian detection.

672
01:04:22,880 --> 01:04:24,800
There's some localization tasks in there.

673
01:04:25,440 --> 01:04:34,000
Also predicting how the entities in the scene are going to move.

674
01:04:34,240 --> 01:04:37,920
Everything is basically a machine learning task, where there's a classification,

675
01:04:37,920 --> 01:04:44,320
segmentation, prediction, and it's nice because you can have this entire engine,

676
01:04:44,320 --> 01:04:49,360
data engine that's mining for edge cases for each one of these tasks.

677
01:04:49,360 --> 01:04:53,600
And you can have people, like engineers that are basically masters of that task,

678
01:04:53,600 --> 01:04:59,600
they become the best person in the world at, as you talk about, the cone guy for Waymo.

679
01:04:59,600 --> 01:05:01,040
Yeah, we're a good old cone guy.

680
01:05:01,600 --> 01:05:06,000
Become the best person in the world at cone detection.

681
01:05:07,200 --> 01:05:10,640
So that's a compelling notion from a supervised learning perspective,

682
01:05:12,160 --> 01:05:17,360
automating much of the process of edge case discovery and retraining neural network for

683
01:05:17,360 --> 01:05:22,000
each of the individual perception tasks. And then you're looking at the machine learning

684
01:05:22,000 --> 01:05:28,960
in a more holistic way, basically doing end-to-end learning on the driving tasks, supervised,

685
01:05:29,760 --> 01:05:36,240
trained on the data of the actual driving of people they use comma AI,

686
01:05:36,240 --> 01:05:41,760
like actual human drivers, their manual control, plus the moments of disengagement

687
01:05:42,560 --> 01:05:47,120
that maybe with some labeling could indicate the failure of the system.

688
01:05:48,800 --> 01:05:53,920
You have a huge amount of data for positive control of the vehicle, like successful control

689
01:05:54,000 --> 01:05:59,920
of the vehicle, both maintaining the lane as I think you're also working on

690
01:05:59,920 --> 01:06:04,640
longitude, no control of the vehicle, and then failure cases where the vehicle does

691
01:06:04,640 --> 01:06:12,320
something wrong that needs disengagement. So why do you think you're right and Tesla is wrong

692
01:06:13,040 --> 01:06:19,840
on this? Do you think you'll come around the Tesla way? Do you think Tesla will come around to your way?

693
01:06:20,160 --> 01:06:25,120
If you were to start a chess engine company, would you hire a bishop guy?

694
01:06:26,000 --> 01:06:33,360
See, we have, this is Monday morning quarterbacking, is yes, probably.

695
01:06:35,840 --> 01:06:40,800
Oh, our Rook guy. Oh, we stole the Rook guy from that company. Oh, we're gonna have real good Rooks.

696
01:06:40,800 --> 01:06:48,640
Well, there's not many pieces, right? There's not many guys and gals to hire.

697
01:06:48,720 --> 01:06:52,560
You just have a few that work on the bishop, a few that work on the Rook.

698
01:06:52,560 --> 01:06:56,640
But is that not ludicrous today to think about in a world of AlphaZero?

699
01:06:57,440 --> 01:07:03,600
But AlphaZero is a chess game. So the fundamental question is how hard is driving compared to chess?

700
01:07:04,320 --> 01:07:12,080
Because so long term, end to end will be the right solution. The question is how many years

701
01:07:12,080 --> 01:07:15,680
away is that? End to end is going to be the only solution for level five.

702
01:07:15,680 --> 01:07:19,200
For the only way we get there. Of course. And of course, Tesla is going to come around to my

703
01:07:19,200 --> 01:07:25,360
way. And if you're a Rook guy out there, I'm sorry. The cone guy. I don't know.

704
01:07:25,360 --> 01:07:29,360
We're going to specialize each task. We're going to really understand Rook placement. Yeah.

705
01:07:30,400 --> 01:07:37,040
I understand the intuition you have. I mean, that is a very compelling notion that we can

706
01:07:37,040 --> 01:07:41,200
learn the task end to end, like the same compelling notion you might have for natural

707
01:07:41,200 --> 01:07:49,840
language conversation. But I'm not sure. Because one thing you sneaked in there is assertion that

708
01:07:50,640 --> 01:07:56,400
it's impossible to get to level five without this kind of approach. I don't know if that's

709
01:07:56,400 --> 01:08:02,400
obvious. I don't know if that's obvious either. I don't actually mean that. I think that it is much

710
01:08:02,400 --> 01:08:06,880
easier to get to level five with an end to end approach. I think that the other approach is

711
01:08:07,760 --> 01:08:12,880
doable, but the magnitude of the engineering challenge may exceed what humanity is capable of.

712
01:08:13,680 --> 01:08:20,720
But what do you think of the Tesla data engine approach, which to me is an active learning

713
01:08:20,720 --> 01:08:27,360
task is kind of fascinating, is breaking it down into these multiple tasks and mining their data

714
01:08:27,360 --> 01:08:32,080
constantly for like edge cases for these different tasks. But the tasks themselves are not being

715
01:08:32,080 --> 01:08:41,120
learned. This is feature engineering. I mean, it's a higher abstraction level of feature

716
01:08:41,120 --> 01:08:45,920
engineering for the different tasks. It's task engineering in a sense. It's slightly better

717
01:08:45,920 --> 01:08:50,560
feature engineering, but it's still fundamentally as feature engineering. And if anything about the

718
01:08:50,560 --> 01:08:55,680
history of AI has taught us anything, it's that feature engineering approaches will always be

719
01:08:55,680 --> 01:09:02,000
replaced and lose to end to end. Now, to be fair, I cannot really make promises on timelines,

720
01:09:02,000 --> 01:09:06,960
but I can say that when you look at the code for stock fish and the code for alpha zero,

721
01:09:06,960 --> 01:09:11,440
one is a lot shorter than the other. A lot more elegant required a lot less programmer hours to

722
01:09:11,440 --> 01:09:25,840
write. Yeah, but there was a lot more murder of bad agents on the alpha zero side. By murder, I mean

723
01:09:27,840 --> 01:09:34,240
agents that played a game and failed miserably. Yeah. Oh, in simulation, that failure is less

724
01:09:34,240 --> 01:09:39,680
costly. Yeah. In real world. Wait, do you mean in practice, like alpha zero has lost games

725
01:09:39,760 --> 01:09:46,960
miserably? No, I haven't seen that. No, but I know the requirement for alpha zero is

726
01:09:48,000 --> 01:09:53,440
to be able to like evolution, human evolution, not human evolution, biological evolution of life

727
01:09:53,440 --> 01:10:01,280
on earth from the origin of life has murdered trillions upon trillions of organisms on the path

728
01:10:01,280 --> 01:10:07,680
thus humans. So the question is, can we stitch together a human like object without having to

729
01:10:07,680 --> 01:10:11,840
go through the entirety process of evolution? Well, no, but do the evolution in simulation?

730
01:10:11,840 --> 01:10:15,680
Yeah, that's the question. Can we simulate? So do you ever sense that it's possible to simulate

731
01:10:15,680 --> 01:10:22,320
some aspect of it? Mu zero is exactly this. Mu zero is the solution to this. Mu zero, I think,

732
01:10:22,320 --> 01:10:26,720
is going to be looked back as the canonical paper. And I don't think deep learning is everything.

733
01:10:26,720 --> 01:10:30,320
I think that there's still a bunch of things missing to get there. But Mu zero, I think,

734
01:10:30,320 --> 01:10:36,960
is going to be looked back as the kind of cornerstone paper of this whole deep learning era.

735
01:10:36,960 --> 01:10:41,280
And Mu zero is the solution to self-driving cars. You have to make a few tweaks to it. But

736
01:10:41,280 --> 01:10:47,040
Mu zero does effectively that. It does those rollouts and those murdering in a learned

737
01:10:47,040 --> 01:10:51,440
simulator and a learned dynamics model. It's interesting. It doesn't get enough love.

738
01:10:51,440 --> 01:10:56,320
I was blown away when I was blown away when I read that paper. I'm like, okay, I've always

739
01:10:56,320 --> 01:10:59,360
said a comma. I'm going to sit and I'm going to wait for the solution to self-driving cars to come

740
01:10:59,360 --> 01:11:08,160
along. This year, I saw it. It's Mu zero. So sit back and let the winning roll in.

741
01:11:09,040 --> 01:11:15,040
So your sense just to elaborate a little bit to link on the topic. Your sense is neural networks

742
01:11:15,040 --> 01:11:18,160
will solve driving. Yes. Like we don't need anything else.

743
01:11:18,720 --> 01:11:23,200
I think the same way chess was maybe the chess and maybe Google are the pinnacle of like

744
01:11:24,000 --> 01:11:31,440
search algorithms and things that look kind of like a star. The pinnacle of this error is going

745
01:11:31,440 --> 01:11:39,440
to be self-driving cars. But on the path that you have to deliver products and it's possible that

746
01:11:39,440 --> 01:11:46,720
the path to full self-driving cars will take decades. I doubt it. How long would you put on it?

747
01:11:47,680 --> 01:11:54,720
Like what are we? You're chasing it. Tesla's chasing it. What are we talking about? 5 years,

748
01:11:54,720 --> 01:12:00,960
10 years, 50 years? Let's say in the 2020s. In the 2020s. The later part of the 2020s.

749
01:12:03,520 --> 01:12:08,240
With the neural network, that would be nice to see. And on the path to that, you're delivering

750
01:12:08,240 --> 01:12:12,960
products, which is a nice L2 system. That's what Tesla is doing, a nice L2 system.

751
01:12:12,960 --> 01:12:16,560
Just gets better every time. The only difference between L2 and the other levels

752
01:12:16,560 --> 01:12:20,240
is who takes liability. And I'm not a liability guy. I don't want to take liability. I'm going to

753
01:12:20,240 --> 01:12:28,800
level 2 forever. Now on that little transition, I mean, how do you make the transition work?

754
01:12:29,520 --> 01:12:35,840
Is this where driver sensing comes in? Like how do you make the, because you said 100 miles,

755
01:12:35,840 --> 01:12:42,080
like is there some sort of human factor psychology thing where people start to

756
01:12:42,080 --> 01:12:46,080
over-trust the system, all those kinds of effects. Once it gets better and better and

757
01:12:46,080 --> 01:12:50,880
better and better, they get lazier and lazier and lazier. Is that, like how do you get that

758
01:12:50,880 --> 01:12:55,520
transition right? First off, our monitoring is already adaptive. Our monitoring is already seen

759
01:12:55,520 --> 01:13:00,800
adaptive. Driver monitoring is just the camera that's looking at the driver. You have an infrared

760
01:13:00,800 --> 01:13:08,560
camera. Our policy for how we enforce the driver monitoring is seen adaptive. What's that mean?

761
01:13:09,120 --> 01:13:16,800
For example, in one of the extreme cases, if the car is not moving, we do not actively enforce

762
01:13:16,800 --> 01:13:24,480
driver monitoring. If you are going through like a 45-mile-an-hour road with lights

763
01:13:25,680 --> 01:13:30,720
and stop signs and potentially pedestrians, we enforce a very tight driver monitoring policy.

764
01:13:30,720 --> 01:13:35,520
If you are alone on a perfectly straight highway, and it's all machine learning,

765
01:13:35,520 --> 01:13:38,960
none of that is hand-coded. Actually, the stop is hand-coded, but...

766
01:13:38,960 --> 01:13:42,080
So there's some kind of machine learning estimation of risk.

767
01:13:43,520 --> 01:13:49,120
Yeah. I mean, I've always been a huge fan of that. That's a, it's difficult to do

768
01:13:50,640 --> 01:13:56,240
every step into that direction is a worthwhile stop to take. It might be difficult to do really

769
01:13:56,240 --> 01:14:01,360
well. Like us humans are able to estimate risk pretty damn well. Whatever the hell that is,

770
01:14:01,440 --> 01:14:08,880
that feels like one of the nice features of us humans. Because we humans are really good drivers

771
01:14:08,880 --> 01:14:14,000
when we're really tuned in. And we're good at estimating risk, like when are we supposed to

772
01:14:14,000 --> 01:14:20,400
be tuned in? Yeah. And people are like, oh, well, why would you ever make the driver monitoring policy

773
01:14:20,400 --> 01:14:24,560
less aggressive? Why would you always not keep it at its most aggressive? Because then people

774
01:14:24,560 --> 01:14:27,680
are just going to get fatigued from it. Yeah. Well, they get annoyed. You want them,

775
01:14:28,240 --> 01:14:32,400
you want the experience to be pleasant. Obviously, I want the experience to be pleasant,

776
01:14:32,400 --> 01:14:38,240
but even just from a straight up safety perspective, if you alert people when they look

777
01:14:38,240 --> 01:14:42,400
around and they're like, why is this thing alerting me? There's nothing I could possibly hit right

778
01:14:42,400 --> 01:14:47,040
now. People will just learn to tune it out. People will just learn to tune it out, to put

779
01:14:47,040 --> 01:14:52,160
weights on the steering wheel, to do whatever, to overcome it. And remember that you're always

780
01:14:52,160 --> 01:14:56,880
part of this adaptive system. So all I can really say about how this scales going forward is,

781
01:14:56,880 --> 01:15:01,680
yeah, something we have to monitor for. We don't know. This is a great psychology experiment at

782
01:15:01,680 --> 01:15:05,920
scale. We'll see. Yeah, it's fascinating. Track it. And making sure you have a good

783
01:15:06,560 --> 01:15:11,360
understanding of attention is a very key part of that psychology problem.

784
01:15:11,360 --> 01:15:15,600
Yeah. I think you and I probably have a different come to it differently, but to me,

785
01:15:16,560 --> 01:15:22,160
it's a fascinating psychology problem to explore something much deeper than just driving. It's

786
01:15:23,040 --> 01:15:30,080
it's such a nice way to explore human attention and human behavior, which is why, again,

787
01:15:30,080 --> 01:15:37,280
we've probably both criticized Mr. Elon Musk on this one topic from different avenues.

788
01:15:38,000 --> 01:15:41,280
So both offline and online, I had little chats with Elon and

789
01:15:44,000 --> 01:15:50,160
like, I love human beings as a computer vision problem, as an AI problem. It's fascinating.

790
01:15:51,040 --> 01:15:52,960
He wasn't so much interested in that problem.

791
01:15:54,400 --> 01:15:59,040
It's like, in order to solve driving, the whole point is you want to remove the human from the

792
01:15:59,040 --> 01:16:06,640
picture. And it seems like you can't do that quite yet. Eventually, yes, but you can't quite do that

793
01:16:06,640 --> 01:16:17,600
yet. So this is the moment where you can't yet say, I told you so to Tesla, but it's getting there

794
01:16:17,600 --> 01:16:21,760
because I don't know if you've seen this, there's some reporting that they're in fact starting to do

795
01:16:22,400 --> 01:16:24,320
driver monitoring. Yeah, they ship the model in shadow mode.

796
01:16:26,160 --> 01:16:33,120
We though, I believe only a visible light camera. It might even be fisheye. It's like a low resolution.

797
01:16:33,120 --> 01:16:37,040
Low resolution visible light. I mean, to be fair, that's what we have in the Eon as well,

798
01:16:37,040 --> 01:16:42,000
our last generation product. This is the one area where I can say our hardware is ahead of Tesla.

799
01:16:42,000 --> 01:16:45,120
The rest of our hardware way way behind, but our driver monitoring camera.

800
01:16:45,840 --> 01:16:52,320
So you think, I think on the third row Tesla podcast or somewhere else, I've heard you say that

801
01:16:53,680 --> 01:16:56,240
obviously eventually they're going to have driver monitoring.

802
01:16:57,040 --> 01:17:01,200
I think what I've said is Elon will definitely ship driver monitoring before he ships level

803
01:17:01,200 --> 01:17:05,840
five. And I'm willing to bet 10 grand on that. And you bet 10 grand on that.

804
01:17:06,880 --> 01:17:09,600
I mean, now I don't want to take the bet, but before, maybe someone would have thought I should

805
01:17:09,600 --> 01:17:18,480
have got my money. It's an interesting bet. I think you're right. I'm actually on a human level

806
01:17:19,040 --> 01:17:26,880
because he's made the decision. He said that driver monitoring is the wrong way to go.

807
01:17:27,520 --> 01:17:34,400
But you have to think of as a human as a CEO. I think that's the right thing to say when

808
01:17:34,400 --> 01:17:41,680
like sometimes you have to say things publicly that different than when you actually believe

809
01:17:41,680 --> 01:17:46,800
because when you're producing a large number of vehicles and the decision was made not to

810
01:17:46,800 --> 01:17:51,920
include the camera, like what are you supposed to say? Like our cars don't have the thing that I

811
01:17:51,920 --> 01:17:58,160
think is right to have. It's an interesting thing. But like on the other side as a CEO,

812
01:17:58,160 --> 01:18:03,520
I mean, something you could probably speak to as a leader, I think about me as a human

813
01:18:04,800 --> 01:18:09,440
to publicly change your mind on something. How hard is that? Well, especially when assholes

814
01:18:09,440 --> 01:18:15,760
like George Haas say, I told you so. All I will say is I am not a leader and I am happy to change

815
01:18:15,760 --> 01:18:25,280
my mind. You think Elon will? Yeah, I do. I think he'll come up with a good way to make it

816
01:18:25,280 --> 01:18:30,480
psychologically okay for him. Well, it's such an important thing, man, especially for a first

817
01:18:30,480 --> 01:18:35,680
principles thinker because he made a decision that driver monitoring is not the right way to go.

818
01:18:35,680 --> 01:18:40,320
And I could see that decision and I could even make that decision. Like I was on the fence

819
01:18:41,280 --> 01:18:48,640
too. Like I'm not a driver monitoring is such an obvious simple solution to the problem of

820
01:18:48,640 --> 01:18:54,320
attention. It's not obvious to me that just by putting a camera there, you solve things. You

821
01:18:54,320 --> 01:19:00,960
have to create an incredible compelling experience just like you're talking about.

822
01:19:00,960 --> 01:19:06,160
I don't know if it's easy to do that. It's not at all easy to do that, in fact, I think. So

823
01:19:07,120 --> 01:19:12,800
as a creator of a car that's trying to create a product that people love, which is what Tesla

824
01:19:12,800 --> 01:19:19,840
tries to do, right? It's not obvious to me that as a design decision, whether adding a camera

825
01:19:19,920 --> 01:19:25,280
is a good idea. From a safety perspective either, like in the human factors community,

826
01:19:25,280 --> 01:19:31,120
everybody says that you should obviously have driver sensing, driver monitoring. But like

827
01:19:32,480 --> 01:19:38,880
that's like saying it's obvious as parents you shouldn't let your kids go out at night.

828
01:19:39,920 --> 01:19:45,440
But okay. But like they're still going to find ways to do drugs.

829
01:19:46,000 --> 01:19:52,000
Yeah. You have to also be good parents. So it's much more complicated than just

830
01:19:52,720 --> 01:19:58,800
you need to have driver monitoring. I totally disagree on, okay, if you have a camera there

831
01:19:58,800 --> 01:20:02,880
and the camera is watching the person but never throws an alert, they'll never think about it.

832
01:20:03,600 --> 01:20:09,840
Right? The driver monitoring policy that you choose to, how you choose to communicate with

833
01:20:09,840 --> 01:20:20,320
the user is entirely separate from the data collection perspective. Right? So there's one

834
01:20:20,320 --> 01:20:26,640
thing to say, tell your teenager they can't do something. There's another thing to gather the

835
01:20:26,640 --> 01:20:30,240
data. So you can make informed decisions. That's really interesting. But you have to make that,

836
01:20:31,040 --> 01:20:36,960
that's the interesting thing about cars. But even true with Comm AI, you don't have to

837
01:20:36,960 --> 01:20:41,120
manufacture the thing into the car is you have to make a decision that anticipates

838
01:20:41,920 --> 01:20:47,360
the right strategy long term. So like you have to start collecting the data and start making

839
01:20:47,360 --> 01:20:52,080
decisions. Started it, started it three years ago. I believe that we have the best driver

840
01:20:52,080 --> 01:20:57,680
monitoring solution in the world. I think that when you compare it to Supercruise is the only

841
01:20:57,680 --> 01:21:04,880
other one that I really know that shipped and ours is better. What do you like and not like about

842
01:21:04,880 --> 01:21:11,920
Supercruise? I mean, I had a few Supercruise, the sun would be shining through the window,

843
01:21:11,920 --> 01:21:15,440
would blind the camera, and it would say I wasn't paying attention when I was looking completely

844
01:21:15,440 --> 01:21:19,760
straight. I couldn't reset the attention with a steering wheel touch and Supercruise would

845
01:21:19,760 --> 01:21:24,320
disengage. Like I was communicating to the car, I'm like, look, I am here, I am paying attention.

846
01:21:24,320 --> 01:21:30,400
Why are you really going to force me to disengage? And it did. So it's a constant

847
01:21:30,400 --> 01:21:34,640
conversation with the user. And yeah, there's no way to ship a system like this if you can OTA.

848
01:21:35,680 --> 01:21:40,960
We're shipping a new one every month. Sometimes we balance it with our users on Discord. Sometimes

849
01:21:40,960 --> 01:21:44,320
we make the driver monitoring a little more aggressive and people complain. Sometimes they

850
01:21:44,320 --> 01:21:48,560
don't. We want it to be as aggressive as possible where people don't complain and it doesn't feel

851
01:21:48,560 --> 01:21:52,640
intrusive. So being able to update the system over the air is an essential component. I mean,

852
01:21:52,640 --> 01:22:02,880
that's probably, to me, that is the biggest innovation of Tesla, that it made it. People

853
01:22:02,880 --> 01:22:10,000
realize that over the air updates is essential. Yeah. Was that not obvious from the iPhone?

854
01:22:10,000 --> 01:22:14,560
The iPhone was the first real product that OTAed, I think. Was it? Actually, that's brilliant.

855
01:22:14,560 --> 01:22:17,840
You're right. I mean, the game consoles used to not, right? The game consoles were maybe the

856
01:22:17,840 --> 01:22:22,160
second thing that did. Well, I didn't really think about it. One of the amazing features

857
01:22:22,160 --> 01:22:29,520
of a smartphone isn't just like the touchscreen isn't the thing. It's the ability to constantly

858
01:22:29,520 --> 01:22:40,720
update. Yeah. It gets better. It gets better. I love my iOS 14. Yeah. One thing that I probably

859
01:22:40,720 --> 01:22:47,760
disagree with you on driver monitoring is you said that it's easy. I mean, you tend to say

860
01:22:47,760 --> 01:22:55,520
stuff is easy. I guess you said it's easy relative to the external perception problem there.

861
01:22:55,760 --> 01:23:03,680
Can you elaborate why you think it's easy? Feature engineering works for driver monitoring. Feature

862
01:23:03,680 --> 01:23:10,640
engineering does not work for the external. So human faces are not, human faces and the movement

863
01:23:10,640 --> 01:23:16,080
of human faces and head and body is not as variable as the external environment? Yeah.

864
01:23:16,080 --> 01:23:19,360
Yeah. Because you're intuition. Yes. And there's another big difference as well.

865
01:23:20,080 --> 01:23:24,240
Your reliability of a driver monitoring system doesn't actually need to be that high.

866
01:23:24,320 --> 01:23:28,320
The uncertainty, if you have something that's detecting whether the human's paying attention

867
01:23:28,320 --> 01:23:32,640
and it only works 92% of the time, you're still getting almost all the benefit of that

868
01:23:32,640 --> 01:23:37,520
because the human, you're training the human. You're dealing with a system that's

869
01:23:37,520 --> 01:23:43,360
really helping you out. It's a conversation. It's not the external thing where guess what?

870
01:23:43,360 --> 01:23:48,240
If you swerve into a tree, you swerve into a tree. You get no margin for error there.

871
01:23:48,240 --> 01:23:54,160
Yeah. I think that's really well put. I think that's the right, exactly the place where

872
01:23:56,720 --> 01:24:01,440
comparing to the external perception and the control problem, driver monitoring is easier

873
01:24:01,440 --> 01:24:07,840
because the bar for success is much lower. Yeah. But I still think the human face

874
01:24:09,120 --> 01:24:13,280
is more complicated actually than the external environment. But for driving, you don't give a

875
01:24:13,280 --> 01:24:22,000
damn. I don't need something that complicated to have to communicate the idea to the human

876
01:24:22,000 --> 01:24:26,480
that I want to communicate, which is, yo, system might mess up here. You got to pay attention.

877
01:24:27,680 --> 01:24:33,840
Yeah. See, that's my love and fascination is the human face. And it feels like this is

878
01:24:34,800 --> 01:24:42,240
a nice place to create products that create an experience in the car. It feels like there should

879
01:24:42,240 --> 01:24:52,080
be more richer experiences in the car. That's an opportunity for something like AMAI or just any

880
01:24:52,080 --> 01:24:57,040
kind of system like a Tesla or any of the autonomous vehicle companies is because software,

881
01:24:57,600 --> 01:25:01,920
there's much more sensors and so much is running on software and you're doing machine learning anyway,

882
01:25:02,720 --> 01:25:08,080
there's an opportunity to create totally new experiences that we're not even anticipating.

883
01:25:08,160 --> 01:25:14,080
You don't think so? No. You think it's a box that gets you from A to B and you want to do it chill?

884
01:25:14,960 --> 01:25:19,200
Yeah. I mean, I think as soon as we get to level three on highways, okay, enjoy your candy crush,

885
01:25:19,200 --> 01:25:24,720
enjoy your Hulu, enjoy your, you know, whatever, whatever. Sure, you get this, you can look at

886
01:25:24,720 --> 01:25:29,360
screens basically versus right now, what do you have? Music and audiobooks. So level three is

887
01:25:29,360 --> 01:25:37,280
where you can kind of disengage and stretch this of time. Well, you think level three is possible?

888
01:25:37,280 --> 01:25:44,560
Like on the highway going 400 miles and you can just go to sleep? Oh yeah, sleep. So again,

889
01:25:44,560 --> 01:25:50,480
I think it's really all on a spectrum. I think that being able to use your phone while you're on

890
01:25:50,480 --> 01:25:55,600
the highway and like this all being okay and being aware that the car might alert you and you have

891
01:25:55,600 --> 01:25:59,120
five seconds to basically. So the five second thing is that you think it's possible? Yeah,

892
01:25:59,120 --> 01:26:04,320
I think it is. Oh yeah. Not in all scenarios. Right. Some scenarios it's not. It's the whole

893
01:26:04,320 --> 01:26:10,480
risk thing that you mentioned is nice is to be able to estimate like how risk is this situation.

894
01:26:10,480 --> 01:26:17,280
That's really important to understand. One other thing you mentioned comparing comma and autopilot

895
01:26:17,280 --> 01:26:25,760
is that something about the haptic feel of the way comma controls the car when things are uncertain,

896
01:26:25,760 --> 01:26:30,400
like it behaves a little bit more uncertain when things are uncertain. That's kind of an

897
01:26:30,400 --> 01:26:35,440
interesting point. And then autopilot is much more confident always, even when it's uncertain,

898
01:26:35,440 --> 01:26:42,160
until it runs into trouble. That's a funny thing. I actually mentioned that to Elon,

899
01:26:42,160 --> 01:26:47,920
I think, and then the first time we talked he was inviting is like communicating uncertainty.

900
01:26:48,720 --> 01:26:53,280
I guess comma doesn't really communicate uncertainty explicitly. It communicates it

901
01:26:53,280 --> 01:26:58,000
through haptic feel. Like what's the role of communicating uncertainty, do you think?

902
01:26:58,080 --> 01:27:01,680
We do some stuff explicitly. We do detect the lanes when you're on the highway,

903
01:27:01,680 --> 01:27:05,280
and we'll show you how many lanes we're using to drive with. You can look at where it thinks

904
01:27:05,280 --> 01:27:11,440
the lanes are. You can look at the path. We want to be better about this. We're actually hiring.

905
01:27:11,440 --> 01:27:16,080
We want to hire some new UI people. UI people. You mentioned this because it's such a UI problem

906
01:27:16,080 --> 01:27:21,120
too. We have a great designer now, but we need people who are just going to build this and

907
01:27:21,120 --> 01:27:26,320
debug these UIs, QT people. QT. Is that what the UI has done with this QT?

908
01:27:26,400 --> 01:27:30,240
Moving. The new UI is in QT. C++, QT?

909
01:27:31,840 --> 01:27:35,760
Tesla uses it too. We had some React stuff in there.

910
01:27:37,760 --> 01:27:41,040
React.js or just React. React has its own language, right?

911
01:27:41,040 --> 01:27:49,680
React Native. React is a JavaScript framework. It's all based on JavaScript, but I like C++.

912
01:27:50,160 --> 01:27:58,240
QT. What do you think about Dojo with Tesla and their foray into what appears to be

913
01:28:00,160 --> 01:28:07,280
specialized hardware for training on that? I guess it's something, maybe you can correct me,

914
01:28:07,280 --> 01:28:12,000
for my shallow looking at it. It seems like something that Google did with TPUs,

915
01:28:12,000 --> 01:28:17,120
but specialized for driving data. I don't think it's specialized for driving data.

916
01:28:17,120 --> 01:28:22,560
QT. It's just legit, just TPU. They want to go the Apple way. Basically,

917
01:28:22,560 --> 01:28:25,200
everything required in the chain is done in-house.

918
01:28:25,200 --> 01:28:32,720
QT. You have a problem right now. This is one of my concerns. I really would like to see

919
01:28:32,720 --> 01:28:36,800
somebody deal with this. If anyone out there is doing it, I'd like to help them if I can.

920
01:28:37,840 --> 01:28:43,520
You basically have two options right now to train. Your options are Nvidia or Google.

921
01:28:44,480 --> 01:28:52,240
QT. Google is not even an option. Their TPUs are only available in Google Cloud.

922
01:28:53,040 --> 01:28:59,120
Google has absolutely onerous terms of service restrictions. They may have changed it,

923
01:28:59,120 --> 01:29:02,400
but back in Google's terms of service, it said explicitly you are not allowed to use

924
01:29:02,400 --> 01:29:07,200
Google Cloud ML for training autonomous vehicles or for doing anything that competes with Google

925
01:29:07,200 --> 01:29:12,080
without Google's prior written permission. Google is not a platform company.

926
01:29:14,480 --> 01:29:18,320
I wouldn't touch TPUs with a 10-foot pole. That leaves you with the monopoly.

927
01:29:18,320 --> 01:29:19,600
QT. Nvidia?

928
01:29:19,600 --> 01:29:21,440
QT. Nvidia.

929
01:29:21,440 --> 01:29:23,120
QT. You're not a fan of?

930
01:29:23,120 --> 01:29:32,640
QT. Well, look, I was a huge fan of 2016 Nvidia. Jensen came sat in the car. Cool guy

931
01:29:32,640 --> 01:29:39,760
when the stock was $30 a share. Nvidia stock has skyrocketed. I witnessed a real change

932
01:29:39,840 --> 01:29:47,280
in who was in management over there in 2018. Now they are, let's exploit, let's take every

933
01:29:47,280 --> 01:29:52,080
dollar we possibly can out of this ecosystem. Let's charge $10,000 for A100s because we know

934
01:29:52,080 --> 01:29:58,800
we got the best share in the game. Let's charge $10,000 for an A100 when it's really not that

935
01:29:58,800 --> 01:30:06,080
different from $3080, which is $699. The margins that they are making off of those high-end chips

936
01:30:06,080 --> 01:30:11,120
are so high that I think they're shooting themselves in the foot just from a business

937
01:30:11,120 --> 01:30:15,760
perspective because there's a lot of people talking like me now who are like, somebody's

938
01:30:15,760 --> 01:30:22,000
got to take Nvidia down. Where they could dominate, Nvidia could be the new Intel.

939
01:30:22,000 --> 01:30:31,120
QT. Yeah, to be inside everything essentially. Yet the winners in certain spaces like in

940
01:30:31,200 --> 01:30:36,800
autonomous driving, the winners, only the people who are like desperately falling back and trying

941
01:30:36,800 --> 01:30:41,520
to catch up and have a ton of money like the big automakers are the ones interested in partnering

942
01:30:41,520 --> 01:30:46,240
with Nvidia. Jensen. Oh, and I think a lot of those things are going to fall through. If I were

943
01:30:46,240 --> 01:30:53,440
in Nvidia, sell chips. Sell chips at a reasonable markup. QT. To everybody. Jensen. To everybody.

944
01:30:53,440 --> 01:30:58,080
QT. Without any restrictions. Jensen. Without any restrictions. Intel did this. Look at Intel.

945
01:30:58,160 --> 01:31:02,960
They had a great long run. Nvidia is trying to turn their, they're like trying to productize

946
01:31:02,960 --> 01:31:09,360
their chips way too much. They're trying to extract way more value than they can sustainably.

947
01:31:09,360 --> 01:31:12,960
Sure. You can do it tomorrow. Is it going to up your share price? Sure. If you're one of those

948
01:31:12,960 --> 01:31:17,760
CEOs, it's like, how much can I strip mine in this company? And that's what's weird about it too.

949
01:31:17,760 --> 01:31:22,160
Like the CEO is the founder. It's the same guy. I mean, I still think Jensen's a great guy.

950
01:31:22,240 --> 01:31:28,560
It is great. Why do this? You have a choice. You have a choice right now. Are you trying to cash

951
01:31:28,560 --> 01:31:34,880
out? Are you trying to buy a yacht? If you are, fine. But if you're trying to be the next huge

952
01:31:34,880 --> 01:31:41,280
semiconductor company, sell chips. Well, the interesting thing about Jensen is he is a big

953
01:31:41,280 --> 01:31:50,400
vision guy. So he has a plan like for 50 years down the road. So it makes me wonder like...

954
01:31:50,400 --> 01:31:55,920
How does price gouging fit into it? Yeah. How does that fit? It doesn't seem to make sense of the

955
01:31:55,920 --> 01:32:02,400
plan. I worry that he's listening to the wrong people. Yeah. That's the sense I have too sometimes

956
01:32:02,400 --> 01:32:11,040
because I, despite everything, I think Nvidia is an incredible company. Well, one, I'm deeply

957
01:32:11,040 --> 01:32:15,120
grateful to Nvidia for the products they've created in the past. Me too. And so...

958
01:32:16,000 --> 01:32:18,880
The 1080 Ti was a great GPU. Still have a lot of them.

959
01:32:18,880 --> 01:32:24,080
Still is. Yeah. But at the same time, it just feels like...

960
01:32:26,720 --> 01:32:31,040
It feels like you don't want to put all your stock in Nvidia. And so the Elon is doing...

961
01:32:31,920 --> 01:32:38,320
What Tesla is doing with autopilot and Dojo is the Apple way. Because they're not going to share

962
01:32:38,320 --> 01:32:44,560
Dojo with George Hott's. I know. They should sell that chip. Oh, they should sell that.

963
01:32:44,640 --> 01:32:48,320
Even their accelerator. The accelerator that's in all the cars, the 30 watt one.

964
01:32:49,040 --> 01:32:55,120
Sell it. Why not? So open it up. Make me... Why does Tesla have to be a car company?

965
01:32:55,680 --> 01:33:00,480
Well, if you sell the chip, here's what you get. Yeah. Makes the money out of the chips. It doesn't

966
01:33:00,480 --> 01:33:06,160
take away from your chip. You're going to make some money, free money. And also the world is going

967
01:33:06,160 --> 01:33:11,040
to build an ecosystem of tooling for you. All right. You're not going to have to fix the bug

968
01:33:11,040 --> 01:33:16,640
in your 10H layer. Someone else already did. Well, the question... That's an interesting question.

969
01:33:16,640 --> 01:33:19,920
I mean, that's the question Steve Jobs asked. That's the question Elon Musk is

970
01:33:21,920 --> 01:33:30,960
perhaps asking is, do you want Tesla stuff inside other vehicles inside? Potentially inside like

971
01:33:30,960 --> 01:33:37,760
iRobot Vacuum Cleaner? Yeah. I think you should decide where your advantages are. I'm not saying

972
01:33:37,760 --> 01:33:41,680
Tesla should start selling battery packs to automakers because battery packs to automakers,

973
01:33:41,680 --> 01:33:45,360
they're straight up in competition with you. If I were Tesla, I'd keep the battery technology

974
01:33:45,360 --> 01:33:53,120
totally as far as we make batteries. But the thing about the Tesla TPU is anybody can build that.

975
01:33:53,120 --> 01:33:58,960
It's just a question of, are you willing to spend the money? It could be a huge source of revenue

976
01:33:58,960 --> 01:34:04,640
potentially. Are you willing to spend $100 million? Anyone can build it. And someone will.

977
01:34:04,640 --> 01:34:08,400
And a bunch of companies now are starting trying to build AI accelerators. Somebody's

978
01:34:08,400 --> 01:34:14,640
going to get the idea right. And hopefully they don't get greedy because they'll just lose to

979
01:34:14,640 --> 01:34:17,760
the next guy who finally... And then eventually the Chinese are going to make knockoff and video

980
01:34:17,760 --> 01:34:22,080
chips and that's... From your perspective, I don't know if you're also paying attention to

981
01:34:22,080 --> 01:34:30,320
Stan Tesla for a moment. Elon Musk has talked about a complete rewrite of the neural net that

982
01:34:30,400 --> 01:34:37,680
they're using that seems to... Again, I'm half paying attention, but it seems to involve basically

983
01:34:37,680 --> 01:34:45,920
a kind of integration of all the sensors to where it's a four-dimensional view. You have a 3D model

984
01:34:45,920 --> 01:34:52,720
of the world over time and then you can... I think it's done both for the... Actually,

985
01:34:53,200 --> 01:34:58,480
so the neural network is able to in a more holistic way deal with the world and make predictions

986
01:34:58,560 --> 01:35:07,680
and so on, but also to make the annotation task more easier. You can annotate the world in one

987
01:35:07,680 --> 01:35:11,840
place and they kind of distribute itself across the sensors and across the different...

988
01:35:12,880 --> 01:35:18,560
Like the hundreds of tests that are involved in the hydranet. What are your thoughts about this

989
01:35:18,560 --> 01:35:23,920
rewrite? Is it just like some details that are kind of obvious, there are steps that should be taken

990
01:35:23,920 --> 01:35:28,240
or is there something fundamental that could challenge your idea that end-to-end

991
01:35:28,960 --> 01:35:33,520
is the right solution? We're in the middle of a big review right now as well. We haven't shipped a

992
01:35:33,520 --> 01:35:39,280
new model in a bit. Of what kind? We're going from 2D to 3D. Right now, all our stuff like, for

993
01:35:39,280 --> 01:35:44,480
example, when the car pitches back, the lane lines also pitch back because we're assuming the flat

994
01:35:45,280 --> 01:35:49,840
world hypothesis, the new models do not do this. The new models output everything in 3D.

995
01:35:49,840 --> 01:35:55,920
But there's still no annotation, so the 3D is more about the output.

996
01:35:57,920 --> 01:36:06,480
We have Zs and everything. We've... Zs. We added Zs. We unified a lot of stuff as well.

997
01:36:06,480 --> 01:36:13,680
We switched from TensorFlow to PyTorch. My understanding of what Tesla's thing is,

998
01:36:13,680 --> 01:36:16,560
is that their annotator now annotates across the time dimension.

999
01:36:19,840 --> 01:36:26,400
I mean, cute. Why are you building an annotator? I find their entire pipeline.

1000
01:36:28,160 --> 01:36:33,840
I find your vision, I mean, the vision of end-to-end very compelling. But I also like the

1001
01:36:33,840 --> 01:36:38,800
engineering of the data engine that they've created. In terms of supervised learning

1002
01:36:39,040 --> 01:36:46,240
pipelines, that thing is damn impressive. You're basically the idea is that you have

1003
01:36:47,360 --> 01:36:52,240
hundreds of thousands of people that are doing data collection for you by doing their experience.

1004
01:36:52,240 --> 01:37:00,160
So that's kind of similar to the Kama AI model. And you're able to mine that data based on the

1005
01:37:00,160 --> 01:37:06,480
kind of education you need. I think it's harder to do in the end-to-end learning.

1006
01:37:07,280 --> 01:37:13,360
The mining of the right edge cases. That's what feature engineering is actually really powerful

1007
01:37:14,080 --> 01:37:21,040
because us humans are able to do this kind of mining a little better. But yeah, there's obvious,

1008
01:37:21,040 --> 01:37:24,560
as we know, there's obvious constraints and limitations to that idea.

1009
01:37:25,760 --> 01:37:31,120
Carpathian just tweeted, he's like, you get really interesting insights if you sort your

1010
01:37:31,120 --> 01:37:41,280
validation set by loss and look at the highest loss examples. So yeah, we have a little data

1011
01:37:41,280 --> 01:37:45,760
engine-like thing. We're training a Sagnat. Anyway, it's not fancy, it's just like, okay,

1012
01:37:46,640 --> 01:37:51,440
train the new Sagnat, run it on 100,000 images, and now take the thousand with highest loss,

1013
01:37:52,080 --> 01:37:57,120
select 100 of those by human, put those, get those ones labeled, retrain, do it again.

1014
01:37:57,840 --> 01:38:03,520
So it's a much less well-written data engine. And yeah, you can take these things really far.

1015
01:38:03,520 --> 01:38:09,760
And it is impressive engineering. And if you truly need supervised data for a problem,

1016
01:38:09,760 --> 01:38:15,440
yeah, things like data engine are at the high end of what is attention? Is a human paying

1017
01:38:15,440 --> 01:38:19,120
attention? I mean, we're going to probably build something that looks like data engine to push

1018
01:38:19,120 --> 01:38:24,480
our driver monitoring further. But for driving itself, you have it all annotated beautifully

1019
01:38:24,560 --> 01:38:29,200
by what the human does. Yeah, that's interesting. I mean, that applies to driver attention as well.

1020
01:38:29,920 --> 01:38:33,440
Do you want to detect the eyes? Do you want to detect blinking and pupil movement?

1021
01:38:33,440 --> 01:38:38,080
Do you want to detect all the like face alignments, the landmark detection and so on,

1022
01:38:38,640 --> 01:38:43,200
and then doing kind of reasoning based on that? Or do you want to take the entirety of the face

1023
01:38:43,200 --> 01:38:48,880
over time and do end to end? I mean, it's obvious that eventually you have to do end to end with

1024
01:38:48,880 --> 01:38:55,680
some calibration, some fixes and so on. But it's like, I don't know when that's the right move.

1025
01:38:55,680 --> 01:39:02,560
Even if it's end to end, there actually is, there is no kind of, you have to supervise that with

1026
01:39:02,560 --> 01:39:06,880
humans. Whether a human is paying attention or not is a completely subjective judgment.

1027
01:39:08,480 --> 01:39:12,320
Like you can try to like automatically do it with some stuff, but you don't have.

1028
01:39:13,040 --> 01:39:18,320
If I record a video of a human, I don't have true annotations anywhere in that video.

1029
01:39:18,320 --> 01:39:22,800
The only way to get them is with other humans labeling it really.

1030
01:39:22,800 --> 01:39:30,880
Well, I don't know. If you think deeply about it, you might be able to, depending on the task,

1031
01:39:30,880 --> 01:39:36,240
you might be able to discover self-anitating things like, you can look at like steering

1032
01:39:36,240 --> 01:39:40,240
wheel reverse or something like that. You can discover little moments of lapse of attention.

1033
01:39:43,040 --> 01:39:47,360
That's where psychology comes in. Is there an indicator? Because you have so much data to look

1034
01:39:47,360 --> 01:39:54,240
at. So you might be able to find moments when there's just inattention that even with smartphone,

1035
01:39:54,240 --> 01:39:59,360
if you want to take smartphone use, you can start to zoom in. I mean, that's the gold mine,

1036
01:39:59,360 --> 01:40:03,840
sort of the comma AI. I mean, Tesla is doing this too, right? They're doing

1037
01:40:05,440 --> 01:40:11,760
annotation based on like self-supervised learning too. It's just a small part of the

1038
01:40:11,760 --> 01:40:18,880
entire picture. That's kind of the challenge of solving a problem in machine learning if you

1039
01:40:18,880 --> 01:40:26,720
can discover self-anitating parts of the problem, right? Our driver monitoring team is half a person

1040
01:40:26,720 --> 01:40:31,920
right now. Half a person. You know, once we have... Scale to a full... Once we have two,

1041
01:40:31,920 --> 01:40:36,320
three people on that team, I definitely want to look at self-anitating stuff for attention.

1042
01:40:36,880 --> 01:40:45,360
Let's go back for a sec to a comma. For people who are curious to try it out,

1043
01:40:46,240 --> 01:40:52,720
how do you install a comma in say a 2020 Toyota Corolla? Or like, what are the cars that are

1044
01:40:52,720 --> 01:40:58,800
supported? What are the cars that you recommend? And what does it take? You have a few videos out,

1045
01:40:58,800 --> 01:41:02,800
but maybe through words, can you explain what's it take to actually install a thing?

1046
01:41:02,800 --> 01:41:09,120
So we support... I think it's 91 cars. 91 makes models. We get to 100 this year.

1047
01:41:10,080 --> 01:41:21,200
Nice. The 2020 Corolla, great choice. The 2020 Sonata, it's using the stock longitudinal. It's

1048
01:41:21,200 --> 01:41:26,560
using just our lateral control, but it's a very refined car. Their longitudinal control is not

1049
01:41:26,560 --> 01:41:34,160
bad at all. So yeah, Corolla, Sonata. Or if you're willing to get your hands a little dirty and

1050
01:41:34,160 --> 01:41:38,480
look in the right places on the internet, the Honda Civic is great, but you're going to have to

1051
01:41:38,480 --> 01:41:42,880
install a modified EPS firmware in order to get a little bit more torque. And I can't help you

1052
01:41:42,880 --> 01:41:48,480
with that. Comma does not efficiently endorse that, but we have been doing it. We didn't ever release

1053
01:41:48,480 --> 01:41:54,400
it. We waited for someone else to discover it. And then, you know... And you have a Discord server

1054
01:41:54,480 --> 01:42:02,320
where people... There's a very active developer community, I suppose. So depending on the level

1055
01:42:02,320 --> 01:42:09,440
of experimentation you're willing to do, that's a community. If you just want to buy it and you

1056
01:42:09,440 --> 01:42:16,960
have a supported car, it's 10 minutes to install. There's YouTube videos. It's IKEA furniture level.

1057
01:42:16,960 --> 01:42:21,200
If you can set up a table from IKEA, you can install a Comma 2 in your supported car,

1058
01:42:21,200 --> 01:42:25,680
and it will just work. Now you're like, oh, but I want this high-end feature or I want to fix this

1059
01:42:25,680 --> 01:42:31,600
bug. Okay, well, welcome to the developer community. So what... If I wanted to... This is something I

1060
01:42:31,600 --> 01:42:42,080
asked you offline, like a few months ago. If I wanted to run my own code to... So use Comma as a

1061
01:42:42,640 --> 01:42:46,960
platform and try to run something like OpenPilot, what does it take to do that?

1062
01:42:47,760 --> 01:42:54,000
So there's a toggle in the settings called enable SSH. And if you toggle that, you can SSH into

1063
01:42:54,000 --> 01:42:58,560
your device. You can modify the code. You can upload whatever code you want to it. There's a whole

1064
01:42:58,560 --> 01:43:04,800
lot of people. So about 60% of people are running stock Comma. About 40% of people are running Forks.

1065
01:43:05,360 --> 01:43:10,800
And there's a community of... There's a bunch of people who maintain these Forks. And these Forks

1066
01:43:10,800 --> 01:43:17,200
support different cars or they have different toggles. We try to keep away from the toggles

1067
01:43:17,200 --> 01:43:22,320
that are disabled driver monitoring. But some people might want that kind of thing. And yeah,

1068
01:43:22,320 --> 01:43:31,200
you can... It's your car. It's your... I'm not here to tell you. We have some... We ban... If you're

1069
01:43:31,200 --> 01:43:34,480
trying to subvert safety features, you're banned from our Discord. I don't want anything to do with

1070
01:43:34,480 --> 01:43:43,360
you. But there's some Forks doing that. Got it. So you encourage responsible Forking. Yeah. Yeah,

1071
01:43:43,360 --> 01:43:48,560
we encourage... Some people... Yeah, some people... Like there's Forks that will do... Some people just

1072
01:43:48,560 --> 01:43:54,160
like having a lot of readouts on the UI. Like a lot of flashing numbers. So there's Forks that do

1073
01:43:54,160 --> 01:43:58,400
that. Some people don't like the fact that it disengages when you press the gas pedal. There's

1074
01:43:58,400 --> 01:44:05,760
Forks that disable that. Got it. Now, the stock experience is what like... So it does both lane

1075
01:44:05,760 --> 01:44:11,120
keeping and longitudinal control altogether. So it's not separate like it is an autopilot.

1076
01:44:11,120 --> 01:44:15,760
No. So, okay. Some cars, we use the stock longitudinal control. We don't do the longitudinal

1077
01:44:15,760 --> 01:44:19,920
control in all the cars. Some cars, the ACCs are pretty good in the cars. It's the lane

1078
01:44:19,920 --> 01:44:25,440
keep that's atrocious in anything except for autopilot and supercruise. But you just turn it on

1079
01:44:26,000 --> 01:44:31,680
and it works. What does disengagement look like? Yeah. So we have... I mean, I'm very concerned

1080
01:44:31,680 --> 01:44:38,800
about mode confusion. I've experienced it on supercruise and autopilot where like autopilot

1081
01:44:38,800 --> 01:44:44,880
disengages. I don't realize that the ACC is still on. The lead car moves slightly over and then the

1082
01:44:44,880 --> 01:44:49,520
Tesla accelerates to like whatever my set speed is super fast and like what's going on here.

1083
01:44:50,480 --> 01:44:56,320
We have engaged and disengaged. And this is similar to my understanding. I'm not a pilot,

1084
01:44:56,320 --> 01:45:01,520
but my understanding is either the pilot is in control or the copilot is in control.

1085
01:45:02,080 --> 01:45:07,440
And we have the same kind of transition system. Either open pilot is engaged or open pilot is

1086
01:45:07,440 --> 01:45:12,400
disengaged. Engage with cruise control, disengage with either gas break or cancel.

1087
01:45:13,200 --> 01:45:17,280
Let's talk about money. What's the business strategy for karma?

1088
01:45:17,360 --> 01:45:26,080
Profitable. Well, you did it. So congratulations. What... So it's basically selling, we should

1089
01:45:26,080 --> 01:45:31,520
say, karma costs a thousand bucks, comma two. Two hundred for the interface to the car as well.

1090
01:45:31,520 --> 01:45:38,480
It's 1200. I'll send that. Nobody's usually upfront like this. You gotta add the tack on, right?

1091
01:45:38,480 --> 01:45:43,760
I love it. I'm not gonna lie to you. Trust me, it will add $1,200 of value to your life.

1092
01:45:43,760 --> 01:45:47,840
Yes, it's still super cheap. 30 days, no questions asked, money back guarantee,

1093
01:45:47,840 --> 01:45:52,880
and prices are only going up. If there ever is future hardware, it could cost a lot more than

1094
01:45:52,880 --> 01:45:59,760
$1,200. So comma three is in the works. It could be. All I will say is future hardware is going

1095
01:45:59,760 --> 01:46:05,760
to cost a lot more than the current hardware. Yeah. The people that use... The people I've

1096
01:46:05,760 --> 01:46:11,440
spoken with that use comma, they use open pilot, they... At first, they use it a lot.

1097
01:46:12,080 --> 01:46:16,000
So people that use it, they fall in love with it. Oh, our retention rate is insane.

1098
01:46:16,640 --> 01:46:23,680
It's a good sign. Yeah. It's a really good sign. 70% of comma two buyers are daily active users.

1099
01:46:23,680 --> 01:46:30,000
Yeah, it's amazing. Oh, also, we don't plan on stopping selling the comma two.

1100
01:46:31,840 --> 01:46:40,160
So whatever you create that's beyond comma two, it would be potentially a phase shift.

1101
01:46:41,120 --> 01:46:45,520
It's so much better that... You could use comma two and you can use comma whatever.

1102
01:46:45,520 --> 01:46:51,920
Depends what you want. It's 3.41, 42. Yeah. Auto pilot, hardware one versus hardware two.

1103
01:46:51,920 --> 01:46:55,280
The comma two is kind of like hardware one. Got it. You can still use both. Got it.

1104
01:46:56,160 --> 01:47:00,960
I think I heard you talk about retention rate with the BR headsets that the average is just once.

1105
01:47:02,240 --> 01:47:06,640
I mean, it's such a fascinating way to think about technology. And this is a really,

1106
01:47:06,640 --> 01:47:09,760
really good sign. And the other thing that people say about comma is they can't believe

1107
01:47:09,760 --> 01:47:19,040
they're getting this 4,000 bucks. It seems like some kind of steal. But in terms of long-term

1108
01:47:19,040 --> 01:47:33,040
business strategies, it's currently in 1,000 plus cars. 1,200. More. So yeah,

1109
01:47:33,520 --> 01:47:38,800
uh, daily is about 2,000. Weekly is about 2,500. Monthly is over 3,000.

1110
01:47:38,800 --> 01:47:42,080
Wow. We've grown a lot since we last talked.

1111
01:47:42,080 --> 01:47:47,360
Is the goal... Can we talk crazy for a second? I mean, what's the goal to overtake Tesla?

1112
01:47:48,560 --> 01:47:51,440
Let's talk... Okay, so... I mean, Android did overtake iOS.

1113
01:47:51,440 --> 01:47:54,640
That's exactly it, right? So... Yeah.

1114
01:47:54,640 --> 01:48:00,400
They did it. I actually don't know the timeline of that one. But let's talk...

1115
01:48:00,400 --> 01:48:04,640
Because everything is in alpha now. The autopilot, you could argue, is in alpha in terms of

1116
01:48:04,640 --> 01:48:09,760
towards the big mission of autonomous driving, right? And so what... Yeah,

1117
01:48:09,760 --> 01:48:16,080
it's your goal to overtake millions of cars, essentially. Of course. Where would it stop?

1118
01:48:16,640 --> 01:48:20,320
Like, it's open-source software. It might not be millions of cars with a piece of comma hardware.

1119
01:48:20,320 --> 01:48:26,800
But yeah, I think OpenPilot at some point will cross over autopilot in users,

1120
01:48:26,880 --> 01:48:31,040
just like Android crossed over iOS. How does Google make money from Android?

1121
01:48:34,080 --> 01:48:36,160
It's complicated. Their own devices make money.

1122
01:48:37,280 --> 01:48:41,040
Google... Google makes money by just kind of having you on the internet.

1123
01:48:42,080 --> 01:48:45,440
Yes. Google Search is built in. Gmail is built in.

1124
01:48:45,440 --> 01:48:48,080
Android is just a shield for the rest of Google's ecosystem kind.

1125
01:48:48,080 --> 01:48:52,640
Yeah, but the problem is, Android is not... It's a brilliant thing. I mean,

1126
01:48:53,360 --> 01:49:00,720
Android arguably changed the world. So there you go. You can feel good ethically speaking.

1127
01:49:00,720 --> 01:49:04,160
But as a business strategy, it's questionable.

1128
01:49:04,160 --> 01:49:04,880
It's so hardware.

1129
01:49:05,680 --> 01:49:08,000
So hardware. I mean, it took Google a long time to come around to it,

1130
01:49:08,000 --> 01:49:09,360
but they are now making money on the Pixel.

1131
01:49:10,080 --> 01:49:13,120
You're not about money. You're more about winning.

1132
01:49:13,120 --> 01:49:14,000
Yeah, of course.

1133
01:49:14,000 --> 01:49:19,840
But if only 10% of OpenPilot devices come from comma AI...

1134
01:49:19,840 --> 01:49:20,720
They still make a lot.

1135
01:49:20,720 --> 01:49:22,720
That is still yes. That is a ton of money for our company.

1136
01:49:22,720 --> 01:49:26,400
But can't somebody create a better comma using OpenPilot?

1137
01:49:26,960 --> 01:49:28,640
Or are you basically saying we'll out-compete them?

1138
01:49:28,640 --> 01:49:29,360
We'll out-compete you.

1139
01:49:29,360 --> 01:49:32,160
Is... Can you create a better Android phone than the Google Pixel?

1140
01:49:32,160 --> 01:49:32,560
Right.

1141
01:49:32,560 --> 01:49:34,560
I mean, you can, but like, you know...

1142
01:49:34,560 --> 01:49:38,240
I love that. So you're confident, like, you know what the hell you're doing.

1143
01:49:38,240 --> 01:49:38,480
Yeah.

1144
01:49:39,920 --> 01:49:43,280
It's competence and merit.

1145
01:49:43,280 --> 01:49:46,080
I mean, our money... Yeah, our money comes from... We're a consumer electronics company.

1146
01:49:46,080 --> 01:49:46,400
Yeah.

1147
01:49:46,400 --> 01:49:49,840
And put it this way. So we sold... We sold like 3,000 comma twos.

1148
01:49:51,520 --> 01:49:52,320
2,500 right now.

1149
01:49:58,640 --> 01:50:01,040
Okay. We're probably going to sell 10,000 units next year.

1150
01:50:01,840 --> 01:50:08,160
10,000 units. Even just $1,000 a unit. Okay. We're at $10 million in revenue.

1151
01:50:09,280 --> 01:50:12,000
Get that up to $100,000. Maybe double the price of the unit.

1152
01:50:12,000 --> 01:50:13,440
Now we're talking like $200 million revenue.

1153
01:50:13,440 --> 01:50:13,840
We're talking like a series.

1154
01:50:13,840 --> 01:50:14,800
Yeah, actually making money.

1155
01:50:15,760 --> 01:50:20,480
One of the rare semi-autonomous or autonomous vehicle companies that are actually making

1156
01:50:20,480 --> 01:50:21,280
money. Yeah.

1157
01:50:22,480 --> 01:50:25,200
You know, if you have... If you look at a model,

1158
01:50:25,200 --> 01:50:28,240
when we were just talking about this yesterday, if you look at a model and like you're testing,

1159
01:50:28,240 --> 01:50:32,240
like you're A-B testing your model, and if you're one branch of the A-B test,

1160
01:50:32,240 --> 01:50:35,200
the losses go down very fast in the first five epochs.

1161
01:50:35,200 --> 01:50:39,200
That model is probably going to converge to something considerably better than the one

1162
01:50:39,200 --> 01:50:42,880
where the losses are going down slower. Why do people think this is going to stop?

1163
01:50:42,880 --> 01:50:45,520
Why do people think one day there's going to be a great like,

1164
01:50:45,520 --> 01:50:48,000
well, Waymo's eventually going to surpass you guys?

1165
01:50:48,240 --> 01:50:50,080
Well, they're not.

1166
01:50:51,840 --> 01:50:57,760
Do you see like a world where like a Tesla or a car like a Tesla would be able to basically

1167
01:50:57,760 --> 01:51:03,680
press a button and you like switch to open pilot, you know, you load in?

1168
01:51:04,480 --> 01:51:10,000
No. So I think, so first off, I think that we may surpass Tesla in terms of users.

1169
01:51:10,560 --> 01:51:13,360
I do not think we're going to surpass Tesla ever in terms of revenue.

1170
01:51:13,360 --> 01:51:17,200
I think Tesla can capture a lot more revenue per user than we can.

1171
01:51:17,200 --> 01:51:20,400
But this mimics the Android iOS model exactly.

1172
01:51:20,400 --> 01:51:22,400
There may be more Android devices, but you know,

1173
01:51:22,400 --> 01:51:24,160
there's a lot more iPhones than Google pixels.

1174
01:51:24,160 --> 01:51:27,600
So I think there'll be a lot more Tesla cars sold than pieces of comma hardware.

1175
01:51:30,240 --> 01:51:35,600
And then as far as a Tesla owner being able to switch to open pilot,

1176
01:51:36,560 --> 01:51:39,120
does iOS, does iPhones run Android?

1177
01:51:40,960 --> 01:51:44,320
No, but you can if you really want to do it, but it doesn't really make sense.

1178
01:51:44,320 --> 01:51:45,200
Like it's not.

1179
01:51:45,200 --> 01:51:46,160
It doesn't make sense.

1180
01:51:46,160 --> 01:51:46,720
Who cares?

1181
01:51:46,720 --> 01:51:53,280
What about if a large company like automakers for GM Toyota came to George

1182
01:51:53,280 --> 01:51:59,600
Hotz or on the tech space, Amazon, Facebook, Google came with a large pile of cash?

1183
01:52:02,320 --> 01:52:05,120
Would you consider being purchased?

1184
01:52:07,840 --> 01:52:09,360
Do you see that as a one possible?

1185
01:52:10,480 --> 01:52:11,440
Not seriously, no.

1186
01:52:12,240 --> 01:52:18,640
I would probably see how much shit they'll entertain for me.

1187
01:52:19,520 --> 01:52:22,800
And if they're willing to jump through a bunch of my hoops, then maybe.

1188
01:52:22,800 --> 01:52:25,040
But no, not the way that M&A works today.

1189
01:52:25,040 --> 01:52:26,480
I mean, we've been approached.

1190
01:52:26,480 --> 01:52:27,840
And I laugh in these people's faces.

1191
01:52:27,840 --> 01:52:28,880
I'm like, are you kidding?

1192
01:52:30,880 --> 01:52:33,520
Yeah, because it's so demeaning.

1193
01:52:33,520 --> 01:52:36,880
The M&A people are so demeaning to companies.

1194
01:52:36,880 --> 01:52:41,200
They treat the startup world as their innovation ecosystem.

1195
01:52:41,200 --> 01:52:44,960
And they think that I'm cool with going along with that so I can have some of their scam fake

1196
01:52:44,960 --> 01:52:46,800
Fed dollars, Fed coin.

1197
01:52:47,440 --> 01:52:51,200
What am I going to do with more Fed coin, Fed coin, man?

1198
01:52:51,200 --> 01:52:52,000
I love that.

1199
01:52:52,000 --> 01:52:56,080
So that's the cool thing about podcasting actually is people criticize.

1200
01:52:56,080 --> 01:53:01,040
I don't know if you're familiar with Spotify giving Joe Rogan 100 million.

1201
01:53:01,760 --> 01:53:02,640
I'd talk about that.

1202
01:53:03,680 --> 01:53:09,440
And they respect, despite all the shit that people are talking about Spotify,

1203
01:53:11,360 --> 01:53:16,320
people understand that podcasters like Joe Rogan know what the hell they're doing.

1204
01:53:17,040 --> 01:53:20,160
So they give them money and say, just do what you do.

1205
01:53:21,040 --> 01:53:28,320
And the equivalent for you would be like, George, do what the hell you do because you're good at it.

1206
01:53:28,320 --> 01:53:30,080
Try not to murder too many people.

1207
01:53:31,280 --> 01:53:37,280
There's some kind of common sense things like just don't go on a weird rampage of...

1208
01:53:37,280 --> 01:53:40,240
Yeah, it comes down to what companies I could respect.

1209
01:53:42,160 --> 01:53:44,640
You know, could I respect GM? Never.

1210
01:53:46,480 --> 01:53:47,280
No, I couldn't.

1211
01:53:47,280 --> 01:53:50,160
I mean, could I respect like a Hyundai?

1212
01:53:50,800 --> 01:53:51,440
More so.

1213
01:53:52,080 --> 01:53:52,400
Right?

1214
01:53:52,400 --> 01:53:53,360
That's a lot closer.

1215
01:53:53,360 --> 01:53:53,920
Toyota?

1216
01:53:54,560 --> 01:53:54,960
What's your...

1217
01:53:55,760 --> 01:53:58,480
Nah, no, Korean is the way.

1218
01:53:59,200 --> 01:54:03,200
I think that the Japanese, the Germans, the US, they're all too...

1219
01:54:04,080 --> 01:54:05,600
They all think they're too great to be honest.

1220
01:54:05,600 --> 01:54:06,800
What about the tech companies?

1221
01:54:07,440 --> 01:54:07,840
Apple?

1222
01:54:08,720 --> 01:54:10,880
Apple is of the tech companies that I could respect.

1223
01:54:10,880 --> 01:54:12,080
Apple is the closest.

1224
01:54:12,080 --> 01:54:12,400
Yeah.

1225
01:54:12,400 --> 01:54:13,200
I mean, I could never...

1226
01:54:13,200 --> 01:54:14,240
It would be ironic.

1227
01:54:14,240 --> 01:54:19,680
It would be ironic if Kama AI is acquired by Apple.

1228
01:54:19,680 --> 01:54:20,320
I mean, Facebook.

1229
01:54:20,320 --> 01:54:23,200
Look, I quit Facebook 10 years ago because I didn't respect the business model.

1230
01:54:24,240 --> 01:54:27,760
Google has declined so fast in the last five years.

1231
01:54:28,400 --> 01:54:32,800
What are your thoughts about Waymo and its present and its future?

1232
01:54:32,800 --> 01:54:36,640
Let me start by saying something nice,

1233
01:54:37,280 --> 01:54:44,400
which is I've visited them a few times and I've written in their cars,

1234
01:54:45,280 --> 01:54:51,600
and the engineering that they're doing, both the research and the actual development,

1235
01:54:51,600 --> 01:54:55,040
and the engineering they're doing and the scale they're actually achieving

1236
01:54:55,040 --> 01:54:57,680
by doing it all themselves, is really impressive.

1237
01:54:58,320 --> 01:55:00,800
And the balance of safety and innovation.

1238
01:55:01,440 --> 01:55:07,120
And the cars work really well for the routes they drive.

1239
01:55:07,120 --> 01:55:10,560
Like, they drive fast, which was very surprising to me.

1240
01:55:10,560 --> 01:55:15,360
Like, it drives the speed limit, or faster than the speed limit, it goes.

1241
01:55:16,080 --> 01:55:19,040
And it works really damn well, and the interface is nice.

1242
01:55:19,040 --> 01:55:20,240
And channel Arizona, yeah.

1243
01:55:20,240 --> 01:55:22,320
Yeah, and channel Arizona is in a very specific environment.

1244
01:55:24,480 --> 01:55:28,480
It gives me enough material in my mind to push back

1245
01:55:28,560 --> 01:55:32,240
against the madmen of the world, like George Hutz, to be like...

1246
01:55:34,560 --> 01:55:37,920
Because you kind of imply there's zero probability they're going to win.

1247
01:55:37,920 --> 01:55:38,800
Yeah.

1248
01:55:38,800 --> 01:55:44,400
And after I've written in it, to me it's not zero.

1249
01:55:44,400 --> 01:55:45,920
Oh, it's not for technology reasons.

1250
01:55:46,640 --> 01:55:48,080
Bureaucracy?

1251
01:55:48,080 --> 01:55:49,440
No, it's worse than that.

1252
01:55:49,440 --> 01:55:51,040
It's actually for product reasons, I think.

1253
01:55:51,920 --> 01:55:54,480
Oh, you think they're just not capable of creating an amazing product?

1254
01:55:55,440 --> 01:55:58,960
No, I think that the product that they're building doesn't make sense.

1255
01:56:00,960 --> 01:56:02,080
So, a few things.

1256
01:56:03,200 --> 01:56:04,400
You say the Waymo's are fast.

1257
01:56:05,760 --> 01:56:08,400
Benchmark a Waymo against a competent Uber driver.

1258
01:56:08,960 --> 01:56:09,520
Right.

1259
01:56:09,520 --> 01:56:09,840
Right.

1260
01:56:09,840 --> 01:56:10,960
The Uber driver's faster.

1261
01:56:10,960 --> 01:56:12,080
It's not even about speed.

1262
01:56:12,080 --> 01:56:13,040
It's the thing you said.

1263
01:56:13,040 --> 01:56:16,160
It's about the experience of being stuck at a stop sign.

1264
01:56:16,160 --> 01:56:18,560
Because pedestrians are crossing non-stop.

1265
01:56:20,000 --> 01:56:23,760
I like when my Uber driver doesn't come to a full stop at the stop sign, you know?

1266
01:56:24,320 --> 01:56:31,440
And so, let's say the Waymo's are 20% slower than an Uber, right?

1267
01:56:32,800 --> 01:56:34,240
You can argue that they're going to be cheaper.

1268
01:56:34,880 --> 01:56:39,200
And I argue that users already have the choice to trade off money for speed.

1269
01:56:39,200 --> 01:56:40,080
It's called Uber Pool.

1270
01:56:42,080 --> 01:56:44,480
I think it's like 15% of rides at Uber pools.

1271
01:56:45,360 --> 01:56:45,840
Right.

1272
01:56:45,840 --> 01:56:48,640
Users are not willing to trade off money for speed.

1273
01:56:49,280 --> 01:56:53,840
So, the whole product that they're building is not going to be competitive

1274
01:56:54,640 --> 01:56:56,640
with traditional ride-sharing networks.

1275
01:56:56,640 --> 01:56:57,120
Right.

1276
01:56:59,200 --> 01:57:07,280
Like, and also, whether there's profit to be made depends entirely on one company having a monopoly.

1277
01:57:07,280 --> 01:57:13,360
I think that the level for autonomous ride-sharing vehicles market is going to look a lot like

1278
01:57:13,360 --> 01:57:17,600
the scooter market if even the technology does come to exist, which I question.

1279
01:57:18,640 --> 01:57:20,080
Who's doing well in that market?

1280
01:57:20,080 --> 01:57:20,320
Yeah.

1281
01:57:20,320 --> 01:57:22,240
It's a race to the bottom, you know.

1282
01:57:22,240 --> 01:57:27,440
Well, it could be closer like an Uber and a Lyft where it's just a one or two players.

1283
01:57:28,000 --> 01:57:34,800
Well, the scooter people have given up trying to market scooters as a practical means of

1284
01:57:34,800 --> 01:57:37,680
transportation and they're just like, they're super fun to ride.

1285
01:57:37,680 --> 01:57:38,320
Look at wheels.

1286
01:57:38,320 --> 01:57:40,080
I love those things and they're great on that front.

1287
01:57:40,640 --> 01:57:40,960
Yeah.

1288
01:57:40,960 --> 01:57:46,160
But from an actual transportation product perspective, I do not think scooters are viable

1289
01:57:46,160 --> 01:57:48,240
and I do not think level four autonomous cars are viable.

1290
01:57:49,120 --> 01:57:51,520
If you, let's play a fun experiment.

1291
01:57:51,520 --> 01:57:56,240
If you ran, let's do Tesla and let's do Waymo.

1292
01:57:56,240 --> 01:57:56,880
All right.

1293
01:57:56,880 --> 01:58:02,240
If Elon Musk took a vacation for a year, he just said, screw it.

1294
01:58:02,240 --> 01:58:05,280
I'm going to go live on an island, no electronics.

1295
01:58:05,280 --> 01:58:08,320
And the board decides that we need to find somebody to run the company

1296
01:58:09,040 --> 01:58:12,160
and they decide that you should run the company for a year.

1297
01:58:12,160 --> 01:58:13,840
How do you run Tesla differently?

1298
01:58:14,720 --> 01:58:15,600
I wouldn't change much.

1299
01:58:16,400 --> 01:58:17,840
Do you think they're on the right track?

1300
01:58:17,840 --> 01:58:18,640
I wouldn't change.

1301
01:58:18,640 --> 01:58:21,600
I mean, I'd have some minor changes.

1302
01:58:21,600 --> 01:58:29,040
But even my debate with Tesla about end-to-end versus segnets, that's just software.

1303
01:58:29,040 --> 01:58:29,520
Who cares?

1304
01:58:31,920 --> 01:58:34,320
It's not like you're doing something terrible with segnets.

1305
01:58:34,320 --> 01:58:37,200
You're probably building something that's at least going to help you debug the end-to-end

1306
01:58:37,200 --> 01:58:37,840
system a lot.

1307
01:58:39,360 --> 01:58:44,240
It's very easy to transition from what they have to an end-to-end kind of thing.

1308
01:58:45,200 --> 01:58:45,840
Right.

1309
01:58:45,840 --> 01:58:51,520
And then I presume you would, in the Model Y or maybe in the Model 3,

1310
01:58:51,520 --> 01:58:53,520
start adding driver sensing with infrared.

1311
01:58:53,520 --> 01:58:59,520
Yes, I would add infrared lights right away to those cars.

1312
01:59:02,160 --> 01:59:04,960
And start collecting that data and do all that kind of stuff.

1313
01:59:04,960 --> 01:59:05,520
Yeah.

1314
01:59:05,520 --> 01:59:06,080
Very much.

1315
01:59:06,080 --> 01:59:07,360
I think they're already kind of doing it.

1316
01:59:07,360 --> 01:59:09,280
It's an incredibly minor change.

1317
01:59:09,280 --> 01:59:12,320
If I actually were CEO of Tesla first off, I'd be horrified that I wouldn't be able

1318
01:59:12,320 --> 01:59:14,000
to do a better job as Elon.

1319
01:59:14,000 --> 01:59:17,600
And then I would try to understand the way he's done things before.

1320
01:59:17,600 --> 01:59:19,120
You would also have to take over as Twitter.

1321
01:59:20,160 --> 01:59:21,360
God, I don't tweet.

1322
01:59:22,080 --> 01:59:23,520
Yeah, what's your Twitter situation?

1323
01:59:23,520 --> 01:59:25,440
Why are you so quiet on Twitter?

1324
01:59:25,440 --> 01:59:25,920
Oh, I mean.

1325
01:59:25,920 --> 01:59:30,240
I says Dukama is like, what's your social network presence like?

1326
01:59:30,240 --> 01:59:34,160
Because on Instagram, you do live streams.

1327
01:59:36,720 --> 01:59:41,520
You understand the music of the internet, but you don't always fully engage into it.

1328
01:59:42,000 --> 01:59:42,640
Part-time.

1329
01:59:42,640 --> 01:59:43,600
Why do you still have a Twitter?

1330
01:59:44,160 --> 01:59:47,600
Yeah, I mean, Instagram is a pretty place.

1331
01:59:47,600 --> 01:59:48,960
Instagram is a beautiful place.

1332
01:59:48,960 --> 01:59:49,840
It glorifies beauty.

1333
01:59:49,840 --> 01:59:52,560
I like Instagram's values as a network.

1334
01:59:53,360 --> 02:00:01,280
Twitter glorifies conflict, glorifies like taking shots of people.

1335
02:00:01,280 --> 02:00:08,320
And it's like, Twitter and Donald Trump are perfect for each other.

1336
02:00:08,400 --> 02:00:11,600
So Tesla is on the right track in your view.

1337
02:00:12,640 --> 02:00:16,480
Okay, so let's really try this experiment.

1338
02:00:16,480 --> 02:00:20,400
If you ran Waymo, let's say they're, I don't know if you agree,

1339
02:00:20,400 --> 02:00:23,280
but they seem to be at the head of the pack of the kind of...

1340
02:00:25,520 --> 02:00:26,800
What would you call that approach?

1341
02:00:27,440 --> 02:00:30,080
It's not necessarily lighter-based because it's not about lighter.

1342
02:00:30,080 --> 02:00:31,680
Level four robotaxi.

1343
02:00:31,680 --> 02:00:35,520
Level four robotaxi all in before making any revenue.

1344
02:00:36,400 --> 02:00:38,560
So they're probably at the head of the pack.

1345
02:00:38,560 --> 02:00:44,480
If you said, hey, George, can you please run this company for a year?

1346
02:00:44,480 --> 02:00:45,920
How would you change it?

1347
02:00:47,120 --> 02:00:49,760
I would go, I would get Anthony Lewandowski out of jail,

1348
02:00:49,760 --> 02:00:51,200
and I would put him in charge of the company.

1349
02:00:56,720 --> 02:00:58,080
Let's try to break that apart.

1350
02:00:59,760 --> 02:01:01,520
You want to destroy the company by doing that?

1351
02:01:01,840 --> 02:01:08,560
Or do you mean you like renegade style thinking that pushes,

1352
02:01:09,360 --> 02:01:12,480
that throws away bureaucracy and goes to first principle thinking?

1353
02:01:12,480 --> 02:01:14,000
What do you mean by that?

1354
02:01:14,000 --> 02:01:16,000
I think Anthony Lewandowski is a genius,

1355
02:01:16,000 --> 02:01:19,200
and I think he would come up with a much better idea

1356
02:01:19,200 --> 02:01:20,720
of what to do with Waymo than me.

1357
02:01:22,240 --> 02:01:23,760
So you mean that unironically.

1358
02:01:23,760 --> 02:01:24,720
He is a genius.

1359
02:01:24,720 --> 02:01:26,000
Oh, yes. Oh, absolutely.

1360
02:01:26,560 --> 02:01:27,600
Without a doubt.

1361
02:01:27,600 --> 02:01:30,320
I mean, I'm not saying there's no shortcomings,

1362
02:01:30,960 --> 02:01:33,760
but in the interactions I've had with him, yeah.

1363
02:01:35,680 --> 02:01:37,440
He's also willing to take, like,

1364
02:01:37,440 --> 02:01:39,280
who knows what he would do with Waymo?

1365
02:01:39,280 --> 02:01:41,680
I mean, he's also out there, like far more out there than I am.

1366
02:01:41,680 --> 02:01:43,360
Yeah, his big risks.

1367
02:01:43,360 --> 02:01:44,240
What do you make of him?

1368
02:01:44,240 --> 02:01:46,880
I was going to talk to him in his pockets,

1369
02:01:46,880 --> 02:01:48,080
and I was going back and forth.

1370
02:01:48,880 --> 02:01:53,120
I'm such a gullible naive human, like I see the best in people.

1371
02:01:53,840 --> 02:01:58,080
And I slowly started to realize that there might be some people out there

1372
02:01:59,040 --> 02:02:04,320
that, like, have multiple faces to the world.

1373
02:02:05,120 --> 02:02:08,000
They're, like, deceiving and dishonest.

1374
02:02:08,000 --> 02:02:12,960
I still refuse to, like, I just, I trust people,

1375
02:02:12,960 --> 02:02:14,480
and I don't care if I get hurt by it,

1376
02:02:14,480 --> 02:02:17,680
but, like, you know, sometimes you have to be a little bit careful,

1377
02:02:17,680 --> 02:02:20,400
especially platform-wise and podcast-wise.

1378
02:02:21,440 --> 02:02:22,480
What am I supposed to think?

1379
02:02:23,040 --> 02:02:25,040
So you think, you think he's a good person?

1380
02:02:26,000 --> 02:02:27,600
Oh, I don't know.

1381
02:02:27,600 --> 02:02:29,040
I don't really make moral judgments.

1382
02:02:29,680 --> 02:02:30,640
And it's difficult to...

1383
02:02:30,640 --> 02:02:32,400
Oh, I mean this about the Waymo.

1384
02:02:32,400 --> 02:02:34,800
Actually, I mean that whole idea, very non-ironically,

1385
02:02:34,800 --> 02:02:36,000
about what I would do.

1386
02:02:36,000 --> 02:02:37,920
The problem with putting me in charge of Waymo

1387
02:02:37,920 --> 02:02:41,520
is Waymo is already $10 billion in the haul, right?

1388
02:02:41,520 --> 02:02:43,600
Whatever idea Waymo does, look,

1389
02:02:43,600 --> 02:02:45,840
comma's profitable, comma's raised $8.1 million.

1390
02:02:46,480 --> 02:02:48,000
That's small, you know, that's small money.

1391
02:02:48,000 --> 02:02:50,720
Like, I can build a reasonable consumer electronics company

1392
02:02:50,720 --> 02:02:52,640
and succeed wildly at that

1393
02:02:52,720 --> 02:02:55,120
and still never be able to pay back Waymo's $10 billion.

1394
02:02:55,680 --> 02:02:58,400
So I think the basic idea with Waymo,

1395
02:02:58,400 --> 02:03:00,800
well, forget the $10 billion because they have some backing,

1396
02:03:00,800 --> 02:03:03,200
but your basic thing is, like,

1397
02:03:03,200 --> 02:03:05,600
what can we do to start making some money?

1398
02:03:05,600 --> 02:03:07,840
Well, no, I mean, my bigger idea is, like,

1399
02:03:07,840 --> 02:03:10,160
whatever the idea is that's going to save Waymo,

1400
02:03:10,160 --> 02:03:11,280
I don't have it.

1401
02:03:11,280 --> 02:03:13,360
It's going to have to be a big risk idea,

1402
02:03:13,360 --> 02:03:15,120
and I cannot think of a better person

1403
02:03:15,120 --> 02:03:16,480
than Anthony Lewandowski to do it.

1404
02:03:17,680 --> 02:03:20,080
So that is completely what I would do with CEO of Waymo.

1405
02:03:20,080 --> 02:03:22,560
I've called myself a transitionary CEO.

1406
02:03:22,560 --> 02:03:24,640
Do everything I can to fix that situation up.

1407
02:03:24,640 --> 02:03:25,600
Transitionary CEO, yeah.

1408
02:03:27,920 --> 02:03:28,320
Yeah.

1409
02:03:28,320 --> 02:03:29,600
Because I can't do it, right?

1410
02:03:29,600 --> 02:03:32,080
Like, I can't, I can't, I mean,

1411
02:03:32,080 --> 02:03:34,240
I can talk about how what I really want to do

1412
02:03:34,240 --> 02:03:37,280
is just apologize for all those corny, you know,

1413
02:03:37,280 --> 02:03:38,240
ad campaigns and be like,

1414
02:03:38,240 --> 02:03:39,760
here's the real estate of the technology.

1415
02:03:39,760 --> 02:03:42,160
Yeah, that's, like, I have several criticism.

1416
02:03:42,160 --> 02:03:46,000
I'm a little bit more bullish on Waymo than you seem to be,

1417
02:03:46,000 --> 02:03:50,720
but one criticism I have is it went into corny mode too early.

1418
02:03:50,720 --> 02:03:52,080
Like, it's still a startup.

1419
02:03:52,080 --> 02:03:53,600
It hasn't delivered on anything.

1420
02:03:53,600 --> 02:03:55,680
So it should be, like, more renegade

1421
02:03:56,320 --> 02:03:59,200
and show off the engineering that they're doing,

1422
02:03:59,200 --> 02:04:00,480
which just can be impressive,

1423
02:04:00,480 --> 02:04:02,640
as opposed to doing these weird commercials of, like,

1424
02:04:03,600 --> 02:04:06,880
your friendly car company.

1425
02:04:06,880 --> 02:04:07,840
I mean, that's my biggest,

1426
02:04:07,840 --> 02:04:09,920
my biggest snipe at Waymo was always,

1427
02:04:09,920 --> 02:04:11,280
that guy's a paid actor.

1428
02:04:11,280 --> 02:04:12,720
That guy's not a Waymo user.

1429
02:04:12,720 --> 02:04:13,520
He's a paid actor.

1430
02:04:13,520 --> 02:04:15,280
Look here, I found his call sheet.

1431
02:04:15,280 --> 02:04:18,720
Do kind of like what SpaceX is doing with the rocket launch

1432
02:04:18,720 --> 02:04:20,880
is just get, put the nerds up front,

1433
02:04:20,880 --> 02:04:22,080
put the engineers up front,

1434
02:04:22,080 --> 02:04:25,600
and just, like, show failures too, just...

1435
02:04:25,600 --> 02:04:27,760
I love SpaceX's, yeah.

1436
02:04:27,760 --> 02:04:29,760
Yeah, the thing that they're doing is right,

1437
02:04:29,760 --> 02:04:31,600
and it just feels like the right...

1438
02:04:31,600 --> 02:04:34,320
But we're all so excited to see them succeed.

1439
02:04:34,320 --> 02:04:34,640
Yeah.

1440
02:04:34,640 --> 02:04:36,640
I can't wait to see Waymo fail, you know?

1441
02:04:37,200 --> 02:04:38,080
Like, you lie to me.

1442
02:04:38,080 --> 02:04:39,280
I want you to fail.

1443
02:04:39,280 --> 02:04:40,240
You tell me the truth.

1444
02:04:40,240 --> 02:04:41,040
You be honest with me.

1445
02:04:41,040 --> 02:04:42,000
I want you to succeed.

1446
02:04:42,000 --> 02:04:42,320
Yeah.

1447
02:04:44,880 --> 02:04:48,720
Yeah, and that requires the renegade CEO, right?

1448
02:04:49,680 --> 02:04:50,880
I'm with you.

1449
02:04:50,880 --> 02:04:51,360
I'm with you.

1450
02:04:51,360 --> 02:04:52,960
I still have a little bit of faith in Waymo

1451
02:04:53,920 --> 02:04:56,160
for the renegade CEO to step forward, but...

1452
02:04:57,760 --> 02:04:59,440
It's not, it's not John Kraft.

1453
02:05:00,560 --> 02:05:01,280
Yeah.

1454
02:05:01,280 --> 02:05:02,800
It's, you can't...

1455
02:05:02,800 --> 02:05:03,760
It's not Chris Omston.

1456
02:05:04,800 --> 02:05:07,600
And those people may be very good at certain things.

1457
02:05:07,600 --> 02:05:08,160
Yeah.

1458
02:05:08,160 --> 02:05:09,280
But they're not renegades.

1459
02:05:10,240 --> 02:05:11,920
Yeah, because these companies are fundamentally,

1460
02:05:11,920 --> 02:05:14,240
even though we're talking about billion dollars,

1461
02:05:14,240 --> 02:05:15,600
all these crazy numbers,

1462
02:05:15,600 --> 02:05:18,400
they're still, like, early-stage startups.

1463
02:05:19,120 --> 02:05:21,760
I mean, I just, if you are pre-revenue

1464
02:05:21,760 --> 02:05:24,160
and you've raised $10 billion, I have no idea.

1465
02:05:24,160 --> 02:05:25,760
Like, this just doesn't work.

1466
02:05:26,320 --> 02:05:27,840
You know, it's against everything Silicon Valley.

1467
02:05:27,840 --> 02:05:29,120
Where's your minimum viable product?

1468
02:05:29,680 --> 02:05:31,360
You know, where's your users?

1469
02:05:31,360 --> 02:05:32,320
Where's your growth numbers?

1470
02:05:33,200 --> 02:05:35,520
This is traditional Silicon Valley.

1471
02:05:36,160 --> 02:05:37,920
Why do you not apply it to what you think

1472
02:05:37,920 --> 02:05:39,280
you're too big to fail already?

1473
02:05:39,280 --> 02:05:39,520
Like...

1474
02:05:41,600 --> 02:05:45,680
How do you think autonomous driving will change society?

1475
02:05:45,680 --> 02:05:50,800
So the mission is, for comma, to solve self-driving.

1476
02:05:52,560 --> 02:05:55,520
Do you have, like, a vision of the world of how it'll be different?

1477
02:05:57,840 --> 02:06:00,000
Is it as simple as A to B transportation?

1478
02:06:00,000 --> 02:06:01,040
Or is there, like...

1479
02:06:01,040 --> 02:06:02,160
Because these are robots.

1480
02:06:03,520 --> 02:06:05,760
It's not about autonomous driving in and of itself.

1481
02:06:05,760 --> 02:06:06,960
It's what the technology enables.

1482
02:06:09,600 --> 02:06:12,080
It's, I think it's the coolest applied AI problem.

1483
02:06:12,080 --> 02:06:14,560
I like it because it has a clear path

1484
02:06:15,040 --> 02:06:15,840
to monetary value.

1485
02:06:17,600 --> 02:06:20,560
But as far as that being the thing that changes the world...

1486
02:06:21,280 --> 02:06:23,520
I mean, no.

1487
02:06:23,520 --> 02:06:25,360
Like, there's cute things we're doing in comma.

1488
02:06:25,360 --> 02:06:26,800
Like, who'd have thought you could stick a phone

1489
02:06:26,800 --> 02:06:28,080
on the windshield and it'll drive.

1490
02:06:28,880 --> 02:06:31,040
But, like, really the product that you're building

1491
02:06:31,040 --> 02:06:33,760
is not something that people were not capable

1492
02:06:33,760 --> 02:06:35,040
of imagining 50 years ago.

1493
02:06:35,600 --> 02:06:37,040
So, no, it doesn't change the world on that front.

1494
02:06:37,600 --> 02:06:39,360
Could people have imagined the Internet 50 years ago?

1495
02:06:39,360 --> 02:06:41,760
Only true genius visionaries.

1496
02:06:42,320 --> 02:06:44,880
Everyone could have imagined autonomous cars 50 years ago.

1497
02:06:44,880 --> 02:06:46,960
It's like a car, but I don't drive it.

1498
02:06:46,960 --> 02:06:49,360
See, I have the sense, and I told you, like,

1499
02:06:49,360 --> 02:06:57,200
I'm my long-term dream is robots with whom you have deep connections.

1500
02:06:59,200 --> 02:07:02,240
And there's different trajectories towards that.

1501
02:07:03,440 --> 02:07:06,720
And I've been thinking of launching a startup.

1502
02:07:08,240 --> 02:07:11,280
I see autonomous vehicles as a potential trajectory to that.

1503
02:07:12,240 --> 02:07:16,480
That's not where the direction I would like to go.

1504
02:07:16,480 --> 02:07:19,840
But I also see Tesla or even Kamii like pivoting

1505
02:07:19,840 --> 02:07:25,840
into robotics broadly defined at some stage.

1506
02:07:25,840 --> 02:07:28,240
In the way, like you're mentioning, the Internet didn't expect.

1507
02:07:29,520 --> 02:07:32,400
Let's solve, you know, when I say a comma about this.

1508
02:07:32,400 --> 02:07:34,560
We could talk about this, but let's solve self-driving cars first.

1509
02:07:35,520 --> 02:07:36,800
Gotta stay focused on the mission.

1510
02:07:37,920 --> 02:07:39,120
You're not too big to fail.

1511
02:07:39,120 --> 02:07:42,720
For however much I think comm is winning, like, no, no, no, no.

1512
02:07:42,720 --> 02:07:44,880
You're winning when you solve level five self-driving cars.

1513
02:07:44,880 --> 02:07:46,720
And until then, you haven't won and won.

1514
02:07:46,720 --> 02:07:49,920
And, you know, again, you want to be arrogant in the face of other people?

1515
02:07:49,920 --> 02:07:51,760
Great. You want to be arrogant in the face of nature?

1516
02:07:51,760 --> 02:07:52,400
You're an idiot.

1517
02:07:52,400 --> 02:07:55,360
Right. Stay mission focused, brilliantly put.

1518
02:07:56,320 --> 02:07:58,480
Like I mentioned, thinking of launching a startup,

1519
02:07:58,480 --> 02:08:01,120
I've been considering, actually, before COVID,

1520
02:08:01,120 --> 02:08:03,200
I've been thinking of moving to San Francisco.

1521
02:08:03,200 --> 02:08:04,880
Oh, I wouldn't go there.

1522
02:08:05,840 --> 02:08:10,240
So why is, well, and now I'm thinking about potentially Austin.

1523
02:08:12,000 --> 02:08:13,440
And we're in San Diego now.

1524
02:08:13,440 --> 02:08:14,800
San Diego, come here.

1525
02:08:14,800 --> 02:08:20,400
So why, what, I mean, you're such an interesting human.

1526
02:08:20,400 --> 02:08:22,160
You've launched so many successful things.

1527
02:08:23,040 --> 02:08:26,080
What, why San Diego?

1528
02:08:26,080 --> 02:08:26,880
What do you recommend?

1529
02:08:26,880 --> 02:08:28,160
Why not San Francisco?

1530
02:08:29,120 --> 02:08:31,760
Have you thought, so in your case,

1531
02:08:31,760 --> 02:08:36,080
San Diego with Qualcomm and Staff Dragon, I mean, that's an amazing combination.

1532
02:08:36,880 --> 02:08:37,920
But that wasn't really why.

1533
02:08:38,480 --> 02:08:39,440
That wasn't the why?

1534
02:08:39,440 --> 02:08:41,200
No. I mean, Qualcomm was an afterthought.

1535
02:08:41,200 --> 02:08:42,720
Qualcomm was, it was a nice thing to think about.

1536
02:08:42,720 --> 02:08:45,600
It's like you can have a tech company here and a good one.

1537
02:08:45,600 --> 02:08:48,320
I mean, you know, I like Qualcomm, but no.

1538
02:08:48,320 --> 02:08:50,400
Well, so why is San Diego better than San Francisco?

1539
02:08:50,400 --> 02:08:51,760
Why does San Francisco suck?

1540
02:08:51,760 --> 02:08:52,240
Well, so, okay.

1541
02:08:52,240 --> 02:08:55,200
So first off, we all kind of said, like, we want to stay in California,

1542
02:08:55,200 --> 02:08:59,840
people like the ocean, you know, California for its flaws.

1543
02:08:59,920 --> 02:09:03,760
It's like a lot of the flaws of California are not necessarily California as a whole,

1544
02:09:03,760 --> 02:09:05,440
and they're much more San Francisco specific.

1545
02:09:06,560 --> 02:09:11,200
San Francisco, so I think first-tier cities in general have stopped wanting growth.

1546
02:09:13,200 --> 02:09:17,040
Well, you have like in San Francisco, you know, the voting class always

1547
02:09:17,040 --> 02:09:20,960
votes to not build more houses because they own all the houses and they're like, well,

1548
02:09:20,960 --> 02:09:25,040
you know, once people have figured out how to vote themselves more money, they're going to do it.

1549
02:09:25,040 --> 02:09:26,800
It is so insanely corrupt.

1550
02:09:27,760 --> 02:09:31,440
It is not balanced at all, like political party-wise.

1551
02:09:31,440 --> 02:09:33,680
You know, it's a one-party city.

1552
02:09:33,680 --> 02:09:42,000
And for all the discussion of diversity, it stops lacking real diversity of thought,

1553
02:09:42,000 --> 02:09:47,280
of background, of approaches, of strategies, of ideas.

1554
02:09:48,880 --> 02:09:53,760
It's kind of a strange place that it's the loudest people about diversity

1555
02:09:54,320 --> 02:09:56,400
and the biggest lack of diversity.

1556
02:09:56,400 --> 02:09:58,560
I mean, that's what they say, right?

1557
02:09:58,560 --> 02:09:59,520
It's the projection.

1558
02:10:00,240 --> 02:10:01,040
Projection, yeah.

1559
02:10:02,000 --> 02:10:02,800
Yeah, it's interesting.

1560
02:10:02,800 --> 02:10:07,200
And even people in Silicon Valley telling me that's like high up people,

1561
02:10:07,920 --> 02:10:09,920
everybody is like, this is a terrible place.

1562
02:10:09,920 --> 02:10:10,480
It doesn't make sense.

1563
02:10:10,480 --> 02:10:13,120
I mean, and coronavirus is really what killed it.

1564
02:10:13,120 --> 02:10:18,000
San Francisco was the number one Exodus during coronavirus.

1565
02:10:18,800 --> 02:10:21,440
We still think San Diego is a good place to be.

1566
02:10:23,360 --> 02:10:24,560
Yeah, I mean, we'll see.

1567
02:10:24,640 --> 02:10:29,040
We'll see what happens with California a bit longer term.

1568
02:10:29,760 --> 02:10:31,920
Like, Austin's an interesting choice.

1569
02:10:31,920 --> 02:10:34,480
I wouldn't, I wouldn't, I don't have really anything bad to say about Austin

1570
02:10:35,280 --> 02:10:38,400
either except for the extreme heat in the summer, which, you know,

1571
02:10:38,400 --> 02:10:40,080
but that's like very on the surface, right?

1572
02:10:40,080 --> 02:10:43,600
I think as far as like an ecosystem goes, it's cool.

1573
02:10:43,600 --> 02:10:45,120
I personally love Colorado.

1574
02:10:45,120 --> 02:10:46,080
Colorado is great.

1575
02:10:46,960 --> 02:10:51,040
Yeah, I mean, you have these states that are, you know, like just way better run.

1576
02:10:51,920 --> 02:10:55,200
Um, California is, you know, it's especially San Francisco.

1577
02:10:55,200 --> 02:10:58,320
It's on its high horse and like, yeah.

1578
02:10:58,320 --> 02:11:02,880
Can I ask you for advice to me and to others about

1579
02:11:04,560 --> 02:11:06,720
what's the take to build a successful startup?

1580
02:11:07,360 --> 02:11:07,840
Oh, I don't know.

1581
02:11:07,840 --> 02:11:08,480
I haven't done that.

1582
02:11:09,040 --> 02:11:10,000
Talk to someone who did that.

1583
02:11:10,640 --> 02:11:18,480
Well, you know, this is like another book of yours that I'll buy for $67, I suppose.

1584
02:11:19,200 --> 02:11:20,400
Uh, so there's, um,

1585
02:11:22,640 --> 02:11:24,000
one of these days I'll sell out.

1586
02:11:24,000 --> 02:11:24,640
Yeah, that's right.

1587
02:11:24,640 --> 02:11:27,200
Jail breaks are going to be a dollar and books are going to be 67.

1588
02:11:27,840 --> 02:11:31,920
How I, uh, how I, you know, broke the iPhone by George Hots.

1589
02:11:31,920 --> 02:11:32,720
That's right.

1590
02:11:32,720 --> 02:11:35,680
How I jail broke the iPhone and you can do it.

1591
02:11:35,680 --> 02:11:38,800
You can't do it in 21 days.

1592
02:11:38,800 --> 02:11:39,360
That's right.

1593
02:11:39,360 --> 02:11:40,240
That's right.

1594
02:11:40,240 --> 02:11:41,040
Oh God.

1595
02:11:41,040 --> 02:11:41,440
Okay.

1596
02:11:41,440 --> 02:11:45,680
I can't wait, but quite, so you have an introspective, you have built

1597
02:11:46,640 --> 02:11:49,360
a very unique company.

1598
02:11:49,360 --> 02:11:51,840
I mean, not, not you, but you and others.

1599
02:11:53,040 --> 02:11:56,880
But I don't know, um, there's no, there's nothing.

1600
02:11:56,880 --> 02:12:00,480
You have an interest, but you haven't really sat down and thought about like,

1601
02:12:01,600 --> 02:12:05,200
well, like if you and I were having a bunch of, we're having some beers

1602
02:12:06,080 --> 02:12:09,120
and you're seeing that I'm depressed and whatever I'm struggling.

1603
02:12:09,760 --> 02:12:10,960
There's no advice you can give.

1604
02:12:11,520 --> 02:12:13,600
Oh, I mean, more beer.

1605
02:12:14,400 --> 02:12:14,960
More beer.

1606
02:12:16,240 --> 02:12:18,720
Uh, uh, yeah.

1607
02:12:18,720 --> 02:12:22,160
I think it's all very like situation dependent.

1608
02:12:22,720 --> 02:12:24,160
Um, here's, okay.

1609
02:12:24,160 --> 02:12:27,120
If I can give a generic piece of advice, it's the technology always wins.

1610
02:12:28,080 --> 02:12:33,040
The better technology always wins and lying always loses.

1611
02:12:35,120 --> 02:12:36,960
Build technology and don't lie.

1612
02:12:38,480 --> 02:12:39,120
I'm with you.

1613
02:12:39,120 --> 02:12:40,400
I agree very much.

1614
02:12:40,400 --> 02:12:40,880
Long run.

1615
02:12:40,880 --> 02:12:41,360
Long run.

1616
02:12:41,360 --> 02:12:41,440
Sure.

1617
02:12:41,440 --> 02:12:42,240
That's the long run.

1618
02:12:42,240 --> 02:12:42,880
And you know what?

1619
02:12:42,880 --> 02:12:45,840
The market can remain irrational longer than you can remain solvent.

1620
02:12:46,400 --> 02:12:47,120
True fact.

1621
02:12:47,120 --> 02:12:52,800
Well, this is, this is an interesting point because I ethically and just as a human believe that, um,

1622
02:12:54,320 --> 02:13:01,840
like, like hype and smoke and mirrors is not at any stage of the company is a good strategy.

1623
02:13:02,560 --> 02:13:08,080
I mean, there's some like, you know, PR magic kind of like, you know, you want a new product,

1624
02:13:08,080 --> 02:13:08,400
right?

1625
02:13:08,400 --> 02:13:12,560
If there's a call to action, if there's like a call to action like buy my new GPU,

1626
02:13:12,560 --> 02:13:12,960
look at it.

1627
02:13:12,960 --> 02:13:14,880
It takes up three slots and it's this big.

1628
02:13:14,880 --> 02:13:16,160
It's huge buy my GPU.

1629
02:13:16,160 --> 02:13:16,960
Yeah, that's great.

1630
02:13:16,960 --> 02:13:22,000
If you look at, you know, especially in that, in AI space broadly, but autonomous vehicles,

1631
02:13:22,640 --> 02:13:25,680
like you can raise a huge amount of money on nothing.

1632
02:13:26,480 --> 02:13:28,880
And the question to me is like, I'm against that.

1633
02:13:29,920 --> 02:13:31,440
I'll never be part of that.

1634
02:13:31,440 --> 02:13:35,520
I don't think I hope not willingly not.

1635
02:13:35,680 --> 02:13:44,880
But like, is there something to be said to, uh, essentially lying to raise money,

1636
02:13:44,880 --> 02:13:46,640
like fake it till you make it kind of thing?

1637
02:13:47,760 --> 02:13:50,080
I mean, this is Billy McFarlane the fire festival.

1638
02:13:50,080 --> 02:13:54,240
Like we all, we all experienced, uh, you know, what happens with that?

1639
02:13:54,240 --> 02:13:57,440
No, no, don't fake it till you make it.

1640
02:13:57,440 --> 02:14:00,480
Be honest and hope you make it the whole way.

1641
02:14:00,480 --> 02:14:01,840
The technology wins.

1642
02:14:01,840 --> 02:14:02,160
Right.

1643
02:14:02,160 --> 02:14:02,960
The technology wins.

1644
02:14:02,960 --> 02:14:07,600
And like there is, I'm not used to like the anti-hype, you know, that's, that's

1645
02:14:07,600 --> 02:14:13,120
Slava KPSS reference, but, um, hype isn't necessarily bad.

1646
02:14:13,120 --> 02:14:15,440
I loved camping out for the iPhones.

1647
02:14:16,960 --> 02:14:21,760
You know, and as long as the hype is backed by like substance, as long as it's backed by

1648
02:14:21,760 --> 02:14:27,840
something I can actually buy and like it's real, then hype is great and it's a great feeling.

1649
02:14:28,560 --> 02:14:31,920
It's when the hype is backed by lies that it's a bad feeling.

1650
02:14:31,920 --> 02:14:34,400
I mean, a lot of people call Elon Musk a fraud.

1651
02:14:34,400 --> 02:14:35,520
How could he be a fraud?

1652
02:14:35,520 --> 02:14:41,280
I've noticed this, this kind of interesting effect, which is he does tend to over promise

1653
02:14:42,320 --> 02:14:45,680
and deliver, what's, what's the better way to phrase it?

1654
02:14:45,680 --> 02:14:50,800
Promise a timeline that he doesn't deliver on, he delivers much later on.

1655
02:14:51,600 --> 02:14:52,640
What do you think about that?

1656
02:14:52,640 --> 02:14:53,520
Because I do that.

1657
02:14:53,520 --> 02:14:55,280
I think that's a programmer thing too.

1658
02:14:56,080 --> 02:14:57,600
I do that as well.

1659
02:14:57,600 --> 02:15:01,040
You think that's a really bad thing to do or is that okay?

1660
02:15:01,040 --> 02:15:06,400
Oh, I think that's again, as long as like you're working toward it and you're going to deliver

1661
02:15:06,400 --> 02:15:09,520
on it and it's not too far off, right?

1662
02:15:09,520 --> 02:15:10,160
Yeah.

1663
02:15:10,160 --> 02:15:10,720
Right?

1664
02:15:10,720 --> 02:15:15,200
Like, like, you know, the whole, the whole autonomous vehicle thing, it's like,

1665
02:15:16,000 --> 02:15:19,680
I mean, I still think Tesla's on track to beat us.

1666
02:15:19,680 --> 02:15:24,000
I still think even with their, even with their missteps, they have advantages we don't have.

1667
02:15:24,880 --> 02:15:32,160
You know, Elon is better than me at like marshalling massive amounts of resources.

1668
02:15:33,040 --> 02:15:38,160
So, you know, I still think given the fact they're maybe making some wrong decisions,

1669
02:15:38,160 --> 02:15:39,360
they'll end up winning.

1670
02:15:39,360 --> 02:15:45,120
And like, it's fine to hype it if you're actually going to win, right?

1671
02:15:45,120 --> 02:15:49,360
If Elon says, look, we're going to be landing rockets back on Earth in a year and it takes four,

1672
02:15:49,360 --> 02:15:55,360
like, you know, he landed a rocket back on Earth and he was working toward it the whole time.

1673
02:15:55,360 --> 02:15:59,440
I think there's some amount of like, I think what it becomes wrong is if you know you're not

1674
02:15:59,440 --> 02:16:00,400
going to meet that deadline.

1675
02:16:00,400 --> 02:16:01,520
If you're lying.

1676
02:16:01,520 --> 02:16:02,080
Yeah.

1677
02:16:02,080 --> 02:16:03,200
That's brilliantly put.

1678
02:16:03,200 --> 02:16:06,800
Like, this is what people don't understand, I think.

1679
02:16:06,800 --> 02:16:09,120
Like, Elon believes everything he says.

1680
02:16:09,120 --> 02:16:10,320
He does.

1681
02:16:10,320 --> 02:16:12,080
As far as I can tell, he does.

1682
02:16:12,080 --> 02:16:14,720
And I detected that in myself too.

1683
02:16:14,720 --> 02:16:21,280
Like, if I, it's only bullshit if you're like conscious of yourself lying.

1684
02:16:21,280 --> 02:16:22,480
Yeah, I think so.

1685
02:16:22,480 --> 02:16:23,280
Yeah.

1686
02:16:23,280 --> 02:16:25,600
No, you can't take that to such an extreme, right?

1687
02:16:25,600 --> 02:16:28,960
Like, in a way, I think maybe Billy McFarland believed everything he said too.

1688
02:16:29,920 --> 02:16:30,320
Right.

1689
02:16:30,320 --> 02:16:33,680
That's how you start a cult and then everybody kills themselves.

1690
02:16:33,680 --> 02:16:34,240
Yeah.

1691
02:16:34,240 --> 02:16:34,640
Yeah.

1692
02:16:34,640 --> 02:16:39,200
Like, it's you need, you need, if there's like some factor on it, it's fine.

1693
02:16:39,200 --> 02:16:42,800
And you need some people to like, you know, keep you in check.

1694
02:16:42,800 --> 02:16:48,720
But like, if you deliver on most of the things you say and just the timelines are off, man.

1695
02:16:48,720 --> 02:16:50,240
It does piss people off though.

1696
02:16:50,240 --> 02:16:56,800
I wonder, but who cares in a long arc of history that people, everybody gets pissed off at the

1697
02:16:56,800 --> 02:17:03,280
people who succeed, which is one of the things that frustrates me about this world is they

1698
02:17:03,280 --> 02:17:06,480
don't celebrate the success of others.

1699
02:17:07,600 --> 02:17:11,760
Like, there's so many people that want Elon to fail.

1700
02:17:12,880 --> 02:17:14,800
It's so fascinating to me.

1701
02:17:14,800 --> 02:17:16,960
Like, what is wrong with you?

1702
02:17:18,080 --> 02:17:23,440
Like, so Elon Musk talks about like people at short, like they talk about financial,

1703
02:17:23,440 --> 02:17:25,280
but I think it's much bigger than the financials.

1704
02:17:25,280 --> 02:17:30,480
I've seen like the human factors community, they want, they want other people to fail.

1705
02:17:31,520 --> 02:17:31,840
What?

1706
02:17:31,840 --> 02:17:32,240
What?

1707
02:17:32,240 --> 02:17:32,800
What?

1708
02:17:32,800 --> 02:17:38,480
Like, even people, the harshest thing is like, you know, even people that like seem to really

1709
02:17:38,480 --> 02:17:41,280
hate Donald Trump, they want him to fail.

1710
02:17:41,280 --> 02:17:41,840
Yeah, I know.

1711
02:17:41,840 --> 02:17:45,040
Or like the other president, or they want Barack Obama to fail.

1712
02:17:45,760 --> 02:17:46,080
It's like.

1713
02:17:46,960 --> 02:17:48,400
We're all on the same boat, man.

1714
02:17:49,600 --> 02:17:54,400
It's weird, but I want that, I would love to inspire that part of the world to change because,

1715
02:17:55,520 --> 02:18:00,400
well, damn it, if the human species is going to survive, we can celebrate success.

1716
02:18:00,400 --> 02:18:05,040
Like, it seems like the efficient thing to do in this objective function that like we're all

1717
02:18:05,040 --> 02:18:10,400
striving for is to celebrate the ones that like figure out how to like do better at that

1718
02:18:10,480 --> 02:18:15,840
objective function as opposed to like dragging them down back into them, into the mud.

1719
02:18:16,560 --> 02:18:20,720
I think there is, this is the speech I always give about the commenters on hacker news.

1720
02:18:21,600 --> 02:18:24,720
So first off, something to remember about the internet in general,

1721
02:18:24,720 --> 02:18:28,560
is commenters are not representative of the population.

1722
02:18:29,200 --> 02:18:30,560
I don't comment on anything.

1723
02:18:31,440 --> 02:18:36,080
You know, commenters are representative of a certain sliver of the population.

1724
02:18:36,640 --> 02:18:40,960
And on hacker news, a common thing I'll say is when you'll see something that's like,

1725
02:18:42,000 --> 02:18:46,640
you know, promises to be wild out there and innovative.

1726
02:18:47,440 --> 02:18:52,240
There is some amount of, you know, checking them back to earth, but there's also some amount of,

1727
02:18:52,240 --> 02:19:00,000
if this thing succeeds, well, I'm 36 and I've worked at large tech companies my whole life.

1728
02:19:00,000 --> 02:19:06,880
They can't succeed because if they succeed, that would mean that I could have done something

1729
02:19:06,880 --> 02:19:10,160
different with my life, but we know that I could have, we know that I couldn't have,

1730
02:19:10,160 --> 02:19:11,840
and that's why they're going to fail.

1731
02:19:11,840 --> 02:19:15,040
And they have to root for them to fail to kind of maintain their world image.

1732
02:19:17,440 --> 02:19:17,840
Tune it out.

1733
02:19:17,840 --> 02:19:19,600
And they comment, well, it's hard.

1734
02:19:20,480 --> 02:19:27,520
So one of the things, one of the things I'm considering startup wise is to change that

1735
02:19:27,520 --> 02:19:31,680
because I think the, I think it's also a technology problem.

1736
02:19:31,680 --> 02:19:32,960
It's a platform problem.

1737
02:19:32,960 --> 02:19:33,520
I agree.

1738
02:19:33,520 --> 02:19:37,040
It's like, because the thing you said most people don't comment,

1739
02:19:39,600 --> 02:19:41,440
I think most people want to comment.

1740
02:19:42,880 --> 02:19:45,840
They just don't because it's all the assholes for commenting.

1741
02:19:45,840 --> 02:19:46,240
Exactly.

1742
02:19:46,240 --> 02:19:47,920
I don't want to be grouped in with them on that.

1743
02:19:47,920 --> 02:19:50,720
You don't want to be at a party where everyone's an asshole.

1744
02:19:50,720 --> 02:19:54,400
And so they, but that's a platform problem that's.

1745
02:19:54,400 --> 02:19:56,000
I can't believe what Reddit's become.

1746
02:19:56,000 --> 02:19:58,960
I can't believe the group think in Reddit comments.

1747
02:20:00,480 --> 02:20:04,240
There's a, Reddit is interesting one because they're subreddits.

1748
02:20:05,040 --> 02:20:10,000
And so you can still see, especially small subreddits that like,

1749
02:20:10,000 --> 02:20:17,040
that are little like havens of like joy and positivity and like deep, even disagreement,

1750
02:20:17,040 --> 02:20:20,800
but like nuanced discussion, but it's only like small little pockets.

1751
02:20:21,600 --> 02:20:23,360
But that's, that's emergent.

1752
02:20:23,360 --> 02:20:26,080
The platform is not helping that or herring that.

1753
02:20:26,960 --> 02:20:30,240
So I guess naturally something about the internet,

1754
02:20:31,040 --> 02:20:36,560
if you don't put in a lot of effort to encourage nuance and positive, good vibes,

1755
02:20:37,120 --> 02:20:41,120
it's naturally going to decline into chaos.

1756
02:20:41,120 --> 02:20:42,800
I would love to see someone do this well.

1757
02:20:42,800 --> 02:20:43,040
Yeah.

1758
02:20:43,680 --> 02:20:45,440
I think it's, yeah, very doable.

1759
02:20:46,000 --> 02:20:51,280
I think actually, so I feel like Twitter could be overthrown.

1760
02:20:52,080 --> 02:20:57,360
Yasho Bach talked about how like, if you have like and retweet,

1761
02:20:57,920 --> 02:21:02,000
like that's only positive wiring, right?

1762
02:21:02,000 --> 02:21:07,520
The only way to do anything like negative there is with a comment.

1763
02:21:08,240 --> 02:21:12,880
And that's like that asymmetry is what gives, you know,

1764
02:21:12,880 --> 02:21:14,880
Twitter its particular toxicness.

1765
02:21:15,440 --> 02:21:19,040
Whereas I find YouTube comments to be much better because YouTube comments have a,

1766
02:21:19,040 --> 02:21:22,560
have a, have an up and a down and they don't show the downloads.

1767
02:21:23,680 --> 02:21:26,720
Without getting into depth of this particular discussion,

1768
02:21:26,720 --> 02:21:30,800
the point is to explore possibilities and get a lot of data on it.

1769
02:21:30,800 --> 02:21:34,000
Because I mean, I could disagree with what you just said.

1770
02:21:34,000 --> 02:21:35,920
It's, it's on the point is it's unclear.

1771
02:21:35,920 --> 02:21:38,800
It's a, it hasn't been explored in a really rich way.

1772
02:21:39,760 --> 02:21:45,760
Like the, these questions of how to create platforms that encourage positivity.

1773
02:21:46,960 --> 02:21:47,280
Yeah.

1774
02:21:47,280 --> 02:21:49,440
I think it's a, it's a technology problem.

1775
02:21:49,440 --> 02:21:51,840
And I think we'll look back at Twitter as it is now.

1776
02:21:51,840 --> 02:21:56,640
Maybe it'll happen within Twitter, but most likely somebody overthrows them is,

1777
02:21:57,680 --> 02:22:03,040
we'll look back at Twitter and say, we can't believe we put up with this level of toxicity.

1778
02:22:03,040 --> 02:22:04,320
You need a different business model too.

1779
02:22:04,960 --> 02:22:08,560
Any, any social network that fundamentally has advertising as a business model,

1780
02:22:08,560 --> 02:22:11,600
this was in the social dilemma, which I didn't watch, but I liked it.

1781
02:22:11,600 --> 02:22:14,160
It's like, you know, there's always the, you know, you're the product, you're not the,

1782
02:22:15,280 --> 02:22:17,840
but they had a nuanced take on it that I really liked.

1783
02:22:17,840 --> 02:22:22,960
And it said, the product being sold is influence over you.

1784
02:22:24,480 --> 02:22:28,480
The product being sold is literally your, you know, influence on you.

1785
02:22:30,640 --> 02:22:33,600
That can't be, if that's your idea, okay.

1786
02:22:33,600 --> 02:22:35,440
Well, you know, guess what?

1787
02:22:35,440 --> 02:22:36,960
It can't not be toxic.

1788
02:22:36,960 --> 02:22:37,520
Yeah.

1789
02:22:37,520 --> 02:22:42,400
Maybe there's ways to spin it, like with, with giving a lot more control to the user

1790
02:22:42,400 --> 02:22:47,200
and transparency to see what is happening to them as opposed to in the shadows as possible.

1791
02:22:47,200 --> 02:22:49,200
But that can't be the primary source of,

1792
02:22:49,200 --> 02:22:50,720
But the users aren't, no one's going to use that.

1793
02:22:51,840 --> 02:22:52,480
It depends.

1794
02:22:52,480 --> 02:22:53,040
It depends.

1795
02:22:53,040 --> 02:22:53,840
It depends.

1796
02:22:53,840 --> 02:23:00,160
I think, I think that the, your, your not going to, you can't depend on self-awareness of the users.

1797
02:23:00,160 --> 02:23:04,560
It's a, it's another, it's a longer discussion because you can't depend on it, but

1798
02:23:05,040 --> 02:23:08,240
you can reward self-awareness.

1799
02:23:08,240 --> 02:23:12,480
Like if, for the ones who are willing to put in the work of self-awareness,

1800
02:23:12,480 --> 02:23:17,680
you can reward them and incentivize and perhaps be pleasantly surprised how many people

1801
02:23:18,960 --> 02:23:21,840
are willing to be self-aware on the internet.

1802
02:23:21,840 --> 02:23:23,200
Like we are in real life.

1803
02:23:23,200 --> 02:23:27,120
Like I'm putting in a lot of effort with you right now being self-aware about,

1804
02:23:27,120 --> 02:23:31,440
if I say something stupid or mean, I'll like look at your, like body language.

1805
02:23:31,440 --> 02:23:34,240
Like I'm putting in that effort, it's costly.

1806
02:23:34,240 --> 02:23:36,240
For an introvert, it's a great costly thing.

1807
02:23:36,240 --> 02:23:38,480
But on the internet, fuck it.

1808
02:23:39,440 --> 02:23:43,360
Like most people are like, I don't care if this hurts somebody.

1809
02:23:43,360 --> 02:23:48,560
I don't care if this is not interesting or if this is, yeah, the mean or whatever.

1810
02:23:48,560 --> 02:23:52,320
I think so much of the engagement today on the internet is so disingenuine too.

1811
02:23:52,880 --> 02:23:55,360
You're not doing this out of a genuine, this is what you think.

1812
02:23:55,360 --> 02:23:57,520
You're doing this just straight up to manipulate others.

1813
02:23:57,520 --> 02:23:59,200
Whether you're in, you just became an ad.

1814
02:23:59,760 --> 02:24:03,600
Yeah. Okay, let's talk about a fun topic, which is programming.

1815
02:24:04,320 --> 02:24:05,680
Here's another book idea for you.

1816
02:24:05,680 --> 02:24:06,320
Let me pitch.

1817
02:24:07,200 --> 02:24:09,600
What's your perfect programming setup?

1818
02:24:09,600 --> 02:24:12,640
So like, this by George Hott's.

1819
02:24:12,640 --> 02:24:16,960
So like what, listen, you're-

1820
02:24:16,960 --> 02:24:21,040
Give me a MacBook Air, sit me in a corner of a hotel room and you know I'll still have food.

1821
02:24:21,040 --> 02:24:21,920
So you really don't care.

1822
02:24:21,920 --> 02:24:25,920
You don't fetishize like multiple monitors, keyboard.

1823
02:24:26,880 --> 02:24:30,160
Those things are nice and I'm not going to say no to them,

1824
02:24:30,160 --> 02:24:33,120
but do they automatically unlock tons of productivity?

1825
02:24:33,120 --> 02:24:33,920
No, not at all.

1826
02:24:33,920 --> 02:24:37,360
I have definitely been more productive on a MacBook Air in a corner of a hotel room.

1827
02:24:38,160 --> 02:24:40,640
What about IDE?

1828
02:24:41,520 --> 02:24:45,120
So which operating system do you love?

1829
02:24:45,920 --> 02:24:48,880
What text editor do you use, IDE?

1830
02:24:48,880 --> 02:24:53,760
What, is there something that is like the perfect, if you could just say

1831
02:24:54,560 --> 02:24:57,600
the perfect productivity setup for George Hott's?

1832
02:24:57,600 --> 02:24:58,160
Doesn't matter.

1833
02:24:58,160 --> 02:24:58,960
Doesn't matter.

1834
02:24:58,960 --> 02:25:00,240
Literally doesn't matter.

1835
02:25:00,240 --> 02:25:02,960
You know, I guess I code most of the time in VIM.

1836
02:25:02,960 --> 02:25:07,120
Like literally I'm using an editor from the 70s, you know, you didn't make anything better.

1837
02:25:07,120 --> 02:25:08,880
You know, okay, VS Code is nice for reading code.

1838
02:25:08,880 --> 02:25:10,160
There's a few things that are nice about it.

1839
02:25:10,960 --> 02:25:13,360
I think that there, you can build much better tools.

1840
02:25:13,360 --> 02:25:16,960
How like, Ida's X refs work way better than VS codes, why?

1841
02:25:18,480 --> 02:25:20,080
Yeah, actually, that's a good question.

1842
02:25:20,080 --> 02:25:20,640
Like why?

1843
02:25:20,640 --> 02:25:27,760
I still use, sorry, Emacs for most, I've actually never, I have to confess something dark.

1844
02:25:28,720 --> 02:25:30,720
So I've never used VIM.

1845
02:25:32,480 --> 02:25:39,040
I think maybe I'm just afraid that my life has been like a waste.

1846
02:25:40,560 --> 02:25:43,360
I'm so, I'm not, I'm not even gelical about Emacs.

1847
02:25:44,480 --> 02:25:47,040
This is how I feel about TensorFlow versus PyTorch.

1848
02:25:47,040 --> 02:25:47,680
Yeah.

1849
02:25:47,680 --> 02:25:50,000
Having just like, we've switched everything to PyTorch.

1850
02:25:50,000 --> 02:25:51,760
Now put months into the switch.

1851
02:25:51,760 --> 02:25:54,400
I have felt like I've wasted years on TensorFlow.

1852
02:25:54,400 --> 02:25:55,360
I can't believe it.

1853
02:25:56,160 --> 02:25:58,240
I can't believe how much better PyTorch is.

1854
02:25:58,240 --> 02:25:58,560
Yeah.

1855
02:25:59,440 --> 02:26:00,720
I've used Emacs in VIM.

1856
02:26:00,720 --> 02:26:01,360
Doesn't matter.

1857
02:26:01,360 --> 02:26:03,440
Yeah, still just my heart somehow.

1858
02:26:03,440 --> 02:26:04,560
I fell in love with Lisp.

1859
02:26:04,560 --> 02:26:05,200
I don't know why.

1860
02:26:05,200 --> 02:26:07,920
You can't, the heart wants what the heart wants.

1861
02:26:07,920 --> 02:26:10,320
I don't, I don't understand it, but it just connected with me.

1862
02:26:10,320 --> 02:26:13,120
Maybe it's the functional language at first I connected with.

1863
02:26:13,120 --> 02:26:17,360
Maybe it's because so many of the AI courses before the deep learning revolution were

1864
02:26:17,360 --> 02:26:18,640
taught with Lisp in mind.

1865
02:26:19,280 --> 02:26:19,840
I don't know.

1866
02:26:19,840 --> 02:26:22,240
I don't know what it is, but I'm stuck with it.

1867
02:26:22,240 --> 02:26:26,240
But at the same time, like why am I not using a modern ID for some of these programming?

1868
02:26:26,240 --> 02:26:27,120
I don't know.

1869
02:26:27,120 --> 02:26:28,320
They're not that much better.

1870
02:26:28,320 --> 02:26:29,440
I've used modern IDs there.

1871
02:26:30,000 --> 02:26:33,440
But at the same time, so like to just, we're not to disagree with you, but

1872
02:26:33,440 --> 02:26:34,960
like I like multiple monitors.

1873
02:26:35,680 --> 02:26:41,360
Like I have to do work on a laptop and it's a pain in the ass.

1874
02:26:41,360 --> 02:26:44,640
And also I'm addicted to the Kinesis weird keyboard.

1875
02:26:45,440 --> 02:26:46,480
You could see there.

1876
02:26:47,360 --> 02:26:49,920
Yeah, so you don't have any of that.

1877
02:26:49,920 --> 02:26:51,600
You can just be on a MacBook.

1878
02:26:51,600 --> 02:26:52,800
I mean, look at work.

1879
02:26:52,800 --> 02:26:55,040
I have three 24 inch monitors.

1880
02:26:55,040 --> 02:26:56,400
I have a happy hacking keyboard.

1881
02:26:56,400 --> 02:26:58,560
I have a razor death header mouse.

1882
02:26:59,680 --> 02:27:01,120
But it's not essential for you.

1883
02:27:01,120 --> 02:27:01,840
No.

1884
02:27:01,840 --> 02:27:04,560
Let's go to a day in the life of George Hots.

1885
02:27:04,560 --> 02:27:08,560
What is the perfect day productivity wise?

1886
02:27:08,560 --> 02:27:16,160
So we're not talking about like Hunter S. Thompson drugs and let's look at productivity.

1887
02:27:16,800 --> 02:27:19,680
What's the day look like on like hour by hour?

1888
02:27:19,680 --> 02:27:25,200
Is there any regularities that create a magical George Hots experience?

1889
02:27:25,760 --> 02:27:33,040
I can remember three days in my life and I remember these days vividly when I've gone

1890
02:27:33,040 --> 02:27:37,760
through kind of radical transformations to the way I think.

1891
02:27:37,760 --> 02:27:40,000
And what I would give, I would pay $100,000.

1892
02:27:40,000 --> 02:27:44,800
If I could have one of these days tomorrow, the days have been so impactful.

1893
02:27:44,800 --> 02:27:50,640
And one was first discovering LEIs of Yudkowsky on the Singularity and reading that stuff.

1894
02:27:50,640 --> 02:27:52,560
And like my mind was blown.

1895
02:27:54,160 --> 02:27:59,200
And the next was discovering the Hunter price and that AI is just compression.

1896
02:27:59,840 --> 02:28:03,680
Like finally understanding AIXI and what all of that was.

1897
02:28:03,680 --> 02:28:06,320
You know, I like read about it when I was 18, 19, I didn't understand it.

1898
02:28:06,320 --> 02:28:09,920
And then the fact that like lossless compression implies intelligence.

1899
02:28:09,920 --> 02:28:11,440
The day that I was shown that.

1900
02:28:12,320 --> 02:28:14,000
And then the third one is controversial.

1901
02:28:14,000 --> 02:28:19,120
The day I found a blog called Unqualified Reservations and read that.

1902
02:28:19,120 --> 02:28:21,360
And I was like, wait, which one is that?

1903
02:28:21,360 --> 02:28:22,880
That's what's the guy's name?

1904
02:28:22,880 --> 02:28:24,080
Curtis Garvin.

1905
02:28:24,080 --> 02:28:24,320
Yeah.

1906
02:28:25,120 --> 02:28:27,600
So many people tell me I'm supposed to talk to him.

1907
02:28:27,600 --> 02:28:32,480
Yeah, the day he sounds insane or brilliant, but insane or both.

1908
02:28:32,480 --> 02:28:33,040
I don't know.

1909
02:28:33,040 --> 02:28:36,480
The day I found that blog was another like this was during like like

1910
02:28:36,480 --> 02:28:38,960
Gamergate and kind of the run up to the 2016 election.

1911
02:28:38,960 --> 02:28:42,800
And I'm like, wow, okay, the world makes sense now.

1912
02:28:42,800 --> 02:28:45,520
This is like, I had a framework now to interpret this,

1913
02:28:45,520 --> 02:28:49,360
just like I got the framework for AI and a framework to interpret technological progress.

1914
02:28:49,360 --> 02:28:52,720
Like those days when I discovered these new frameworks or.

1915
02:28:52,720 --> 02:28:53,440
Oh, interesting.

1916
02:28:53,440 --> 02:28:57,040
So it's not about, but what was special about those days?

1917
02:28:57,040 --> 02:28:58,720
How did those days come to be?

1918
02:28:58,720 --> 02:28:59,920
Is it just you got lucky?

1919
02:28:59,920 --> 02:29:00,480
Like, sure.

1920
02:29:01,360 --> 02:29:06,960
I like, you just encountered a hunter prize on on hacking news or something like that.

1921
02:29:07,680 --> 02:29:08,800
Um, like what?

1922
02:29:09,680 --> 02:29:13,200
But you see, I don't think it's just, see, I don't think it's just that like,

1923
02:29:13,200 --> 02:29:14,720
I could have gotten lucky at any point.

1924
02:29:14,720 --> 02:29:16,160
I think that in a way.

1925
02:29:16,160 --> 02:29:17,760
You were ready at that moment.

1926
02:29:17,760 --> 02:29:18,480
Yeah, exactly.

1927
02:29:18,480 --> 02:29:19,600
To receive the information.

1928
02:29:21,440 --> 02:29:24,720
But is there some magic to the day today of like,

1929
02:29:25,520 --> 02:29:27,040
like eating breakfast?

1930
02:29:27,040 --> 02:29:28,400
And it's the mundane things.

1931
02:29:29,040 --> 02:29:29,600
Nah.

1932
02:29:29,600 --> 02:29:30,160
Nothing.

1933
02:29:30,160 --> 02:29:32,160
No, I drift through, I drift through life.

1934
02:29:32,800 --> 02:29:34,080
Without structure.

1935
02:29:34,080 --> 02:29:38,480
I drift through life hoping and praying that I will get another day like those days.

1936
02:29:38,480 --> 02:29:42,080
And there's nothing in particular you do to, uh, to be a receptacle

1937
02:29:42,960 --> 02:29:44,480
for another for day number four.

1938
02:29:46,000 --> 02:29:48,080
No, I didn't do anything to get the other ones.

1939
02:29:48,080 --> 02:29:51,040
So I don't think I have to really do anything now.

1940
02:29:51,040 --> 02:29:53,520
I took a month long trip to New York and

1941
02:29:54,560 --> 02:29:57,920
I mean, the Ethereum thing was the highlight of it, but the rest of it was pretty terrible.

1942
02:29:57,920 --> 02:30:01,680
I did a two week road trip and I got, I had to turn around.

1943
02:30:01,680 --> 02:30:06,560
I had to turn around, I'm driving in, uh, in Gunnison, Colorado.

1944
02:30:06,560 --> 02:30:10,480
I passed through Gunnison and, uh, the snow starts coming down.

1945
02:30:10,480 --> 02:30:12,960
There's a pass up there called Monarch Pass in order to get through to Denver.

1946
02:30:12,960 --> 02:30:13,920
You got to get over the Rockies.

1947
02:30:14,640 --> 02:30:16,160
And I had to turn my car around.

1948
02:30:16,720 --> 02:30:20,080
I couldn't, I watched, uh, I watched a F-150 go off the road.

1949
02:30:20,080 --> 02:30:21,600
I'm like, I got to go back.

1950
02:30:21,600 --> 02:30:25,920
And like that day was meaningful because like, like it was real.

1951
02:30:25,920 --> 02:30:27,920
Like I actually had to turn my car around.

1952
02:30:28,800 --> 02:30:31,200
It's rare that anything even real happens in my life.

1953
02:30:31,200 --> 02:30:35,040
Even as, you know, mundane is the fact that, yeah, there was snow.

1954
02:30:35,040 --> 02:30:37,600
I had to turn around, stay in Gunnison and leave the next day.

1955
02:30:37,600 --> 02:30:39,040
Something about that moment felt real.

1956
02:30:40,160 --> 02:30:45,280
Okay. So it's interesting to break apart the three moments you mentioned if it's okay.

1957
02:30:45,280 --> 02:30:50,320
So it'll, uh, I always have trouble pronouncing his name, but allows a Yurkowski.

1958
02:30:53,200 --> 02:31:00,240
So what, how did your worldview change in starting to consider the

1959
02:31:01,440 --> 02:31:05,520
exponential growth of AI and AGI that he thinks about and the, uh,

1960
02:31:05,520 --> 02:31:08,720
the threats of artificial intelligence and all that kind of ideas?

1961
02:31:08,720 --> 02:31:12,400
Like, can you, is it just like, can you maybe, uh, break apart?

1962
02:31:12,400 --> 02:31:17,280
Like what exactly was so magical to you as a transformation experience?

1963
02:31:17,280 --> 02:31:19,600
Today, everyone knows him for threats and AI safety.

1964
02:31:20,240 --> 02:31:22,080
This was pre that stuff.

1965
02:31:22,080 --> 02:31:24,240
There was, I don't think a mention of AI safety on the page.

1966
02:31:25,840 --> 02:31:27,760
This is, this is old Yurkowski stuff.

1967
02:31:27,760 --> 02:31:28,960
He'd probably denounce it all now.

1968
02:31:28,960 --> 02:31:31,600
He'd probably be like, that's exactly what I didn't want to happen.

1969
02:31:31,600 --> 02:31:32,560
Oh, sorry, man.

1970
02:31:35,200 --> 02:31:38,640
Is there something specific you can take from his work that you can remember?

1971
02:31:38,640 --> 02:31:45,920
Yeah. Uh, it was this realization that, uh, computers double in power every 18 months

1972
02:31:45,920 --> 02:31:47,040
and humans do not.

1973
02:31:47,680 --> 02:31:52,640
And they haven't crossed yet, but if you have one thing that's doubling every 18 months

1974
02:31:52,640 --> 02:31:56,000
and one thing that's staying like this, you know, here's your log graph.

1975
02:31:56,880 --> 02:31:59,600
Here's your line, you know, you calculate that.

1976
02:32:01,920 --> 02:32:05,120
And then did that open the door to the exponential thinking?

1977
02:32:05,120 --> 02:32:10,880
Like thinking that, like, you know what, with technology, we can actually transform the world.

1978
02:32:11,440 --> 02:32:13,040
It opened the door to human obsolescence.

1979
02:32:13,600 --> 02:32:19,360
It, it opened the door to realize that in my lifetime, humans are going to be replaced.

1980
02:32:20,560 --> 02:32:24,480
And then the matching idea to that of artificial intelligence with the Hutter Prize.

1981
02:32:26,960 --> 02:32:27,840
You know, I'm torn.

1982
02:32:27,840 --> 02:32:30,080
I go back and forth on what I think about it.

1983
02:32:30,080 --> 02:32:30,400
Yeah.

1984
02:32:31,360 --> 02:32:34,320
But the, the, the basic thesis is it's nice.

1985
02:32:34,320 --> 02:32:39,600
It's a nice compelling notion that we can reduce the task of creating an intelligent system,

1986
02:32:39,600 --> 02:32:42,720
a generally intelligent system, into the task of compression.

1987
02:32:43,840 --> 02:32:46,240
So you, you can think of all of intelligence in the universe.

1988
02:32:46,240 --> 02:32:48,160
In fact, this is a kind of compression.

1989
02:32:50,160 --> 02:32:53,840
Do you find that a, was that just at the time you found that as a compelling idea?

1990
02:32:53,840 --> 02:32:56,080
Or do you still find that a compelling idea?

1991
02:32:56,880 --> 02:32:58,240
I still find that a compelling idea.

1992
02:32:59,040 --> 02:33:06,160
I think that it's not that useful day-to-day, but actually one of maybe my quests before that

1993
02:33:06,160 --> 02:33:08,560
was a search for the definition of the word intelligence.

1994
02:33:09,120 --> 02:33:10,080
And I never had one.

1995
02:33:10,800 --> 02:33:13,680
And I definitely have a definition of the word compression.

1996
02:33:14,560 --> 02:33:17,600
It's a very simple, straightforward one.

1997
02:33:18,320 --> 02:33:19,600
And you know what compression is.

1998
02:33:19,600 --> 02:33:22,320
You know what lossless, is lossless compression, not lossy, lossless compression.

1999
02:33:22,880 --> 02:33:26,480
And that that is equivalent to intelligence, which I believe,

2000
02:33:26,480 --> 02:33:28,800
I'm not sure how useful that definition is day-to-day,

2001
02:33:28,800 --> 02:33:31,280
but like I now have a framework to understand what it is.

2002
02:33:32,160 --> 02:33:37,520
And he just 10x'd the prize for that competition, like recently a few months ago.

2003
02:33:37,520 --> 02:33:39,360
You ever thought of taking a crack at that?

2004
02:33:39,360 --> 02:33:40,960
Oh, I did.

2005
02:33:40,960 --> 02:33:41,440
Oh, I did.

2006
02:33:41,440 --> 02:33:46,960
I spent, I spent the next, after I found the prize, I spent the next six months of my life

2007
02:33:46,960 --> 02:33:47,760
trying it.

2008
02:33:47,760 --> 02:33:51,200
And well, that's when I started learning everything about AI.

2009
02:33:51,200 --> 02:33:53,200
And then I worked at Vicarious for a bit.

2010
02:33:53,200 --> 02:33:55,200
And then I learned, read all the deep learning stuff.

2011
02:33:55,200 --> 02:33:57,680
And I'm like, okay, now I like, I'm caught up to modern AI.

2012
02:33:58,720 --> 02:34:02,400
And I had, I had a really good framework to put it all in from the compression stuff.

2013
02:34:04,000 --> 02:34:04,320
Right.

2014
02:34:04,320 --> 02:34:07,440
Like some of the first, some of the first deep learning models I played with were,

2015
02:34:08,240 --> 02:34:10,720
were GTT, GPT basically.

2016
02:34:10,720 --> 02:34:17,840
But before Transformers, before it was still RNNs to do character prediction.

2017
02:34:17,840 --> 02:34:22,800
But by the way, on the compression side, I mean, the, especially with neural networks,

2018
02:34:22,800 --> 02:34:26,480
what do you make of the lossless requirement with the harder prize?

2019
02:34:26,480 --> 02:34:32,960
So, you know, human intelligence and neural networks can probably compress stuff pretty

2020
02:34:32,960 --> 02:34:34,480
well, but there will be lossy.

2021
02:34:35,120 --> 02:34:35,920
It's imperfect.

2022
02:34:36,560 --> 02:34:40,240
You can turn a lossy compression into a lossless compressor pretty easily using an arithmetic

2023
02:34:40,240 --> 02:34:40,960
encoder, right?

2024
02:34:40,960 --> 02:34:45,920
You can take an arithmetic encoder and you can just encode the noise with maximum efficiency.

2025
02:34:45,920 --> 02:34:49,840
Right. So even if you can't predict exactly what the next character is,

2026
02:34:50,400 --> 02:34:54,160
the better a probability distribution you can put over the next character,

2027
02:34:54,160 --> 02:34:57,200
you can then use an arithmetic encoder to, right?

2028
02:34:57,200 --> 02:34:59,120
You don't have to know whether it's an E or an I.

2029
02:34:59,120 --> 02:35:02,880
You just have to put good probabilities on them and then, you know, code those.

2030
02:35:02,880 --> 02:35:05,600
And if you have, it's a bit of entropy thing, right?

2031
02:35:06,320 --> 02:35:10,160
So let me, on that topic, it could be interesting as a little side tour.

2032
02:35:10,160 --> 02:35:14,960
What are your thoughts in this year about GPT3 and these language models and these

2033
02:35:14,960 --> 02:35:16,080
transformers?

2034
02:35:16,080 --> 02:35:19,920
Is there something interesting to you as an AI researcher?

2035
02:35:20,560 --> 02:35:24,160
Or is there something interesting to you as an autonomous vehicle developer?

2036
02:35:24,720 --> 02:35:27,280
Nah, I think, uh, I think it's all right.

2037
02:35:27,280 --> 02:35:28,960
I mean, it's not like it's cool.

2038
02:35:28,960 --> 02:35:30,080
It's cool for what it is.

2039
02:35:30,080 --> 02:35:34,480
But no, we're not just going to be able to scale up to GPT12 and get general purpose

2040
02:35:34,480 --> 02:35:35,280
intelligence.

2041
02:35:35,280 --> 02:35:40,480
Like your loss function is literally just, you know, you know, cross entropy loss on the

2042
02:35:40,480 --> 02:35:41,280
character, right?

2043
02:35:41,280 --> 02:35:43,360
Like that's not the loss function of general intelligence.

2044
02:35:43,600 --> 02:35:44,800
Is that obvious to you?

2045
02:35:44,800 --> 02:35:45,280
Yes.

2046
02:35:46,160 --> 02:35:54,080
Can you imagine that, like to play devil's advocate on yourself, is it possible that

2047
02:35:54,080 --> 02:35:59,440
you can, the GPT12 will achieve general intelligence with something as dumb as this

2048
02:35:59,440 --> 02:36:00,720
kind of loss function?

2049
02:36:00,720 --> 02:36:03,680
I guess it depends what you mean by general intelligence.

2050
02:36:03,680 --> 02:36:09,920
So there's another problem with the GPTs and that's that they don't have a, uh, they

2051
02:36:09,920 --> 02:36:10,880
don't have long-term memory.

2052
02:36:11,680 --> 02:36:12,160
Right.

2053
02:36:12,880 --> 02:36:13,440
Right.

2054
02:36:13,440 --> 02:36:13,840
All right.

2055
02:36:13,840 --> 02:36:24,960
So like just GPT12, a scaled up version of GPT2 or 3, I find it hard to believe.

2056
02:36:25,920 --> 02:36:27,040
Well, you can scale it in.

2057
02:36:28,080 --> 02:36:33,840
It's, uh, so it's a hard-coded, hard-coded length, but you can make it wider and wider

2058
02:36:33,840 --> 02:36:34,240
and wider.

2059
02:36:34,240 --> 02:36:39,920
Yeah, you're going to get, you're going to get cool things from those systems.

2060
02:36:40,880 --> 02:36:46,080
But I don't think you're ever going to get something that can like, you know,

2061
02:36:46,080 --> 02:36:47,520
build me a rocket ship.

2062
02:36:47,520 --> 02:36:48,880
What about solve driving?

2063
02:36:49,440 --> 02:36:53,680
So, you know, you can use transformer with video, for example.

2064
02:36:54,640 --> 02:36:57,200
You think, is there something in there?

2065
02:36:57,200 --> 02:37:00,560
No, because look, we use, we use a grew.

2066
02:37:01,120 --> 02:37:01,760
We use a grew.

2067
02:37:01,760 --> 02:37:03,520
We could change that grew out to a transformer.

2068
02:37:04,880 --> 02:37:08,560
Um, I think driving is much more Markovian than language.

2069
02:37:09,360 --> 02:37:13,600
So Markovian, you mean like the memory, which, which aspect of Markovian?

2070
02:37:13,600 --> 02:37:18,640
I mean that like most of the information in the state at T minus one is also in the,

2071
02:37:18,640 --> 02:37:20,240
is in state T, right?

2072
02:37:20,240 --> 02:37:23,840
And it kind of like drops off nicely like this, whereas sometime with language,

2073
02:37:23,840 --> 02:37:26,640
you have to refer back to the third paragraph on the second page.

2074
02:37:27,200 --> 02:37:30,800
I feel like there's not many, like, like you can say like speed limit signs,

2075
02:37:30,800 --> 02:37:33,920
but there's really not many things in autonomous driving that look like that.

2076
02:37:33,920 --> 02:37:38,880
But if you look at, uh, to play devil's advocate is, uh, the risk estimation thing

2077
02:37:38,880 --> 02:37:40,560
that you've talked about is kind of interesting.

2078
02:37:41,200 --> 02:37:47,280
Is, uh, it feels like there might be some longer term, uh, aggregation of context

2079
02:37:47,280 --> 02:37:50,960
necessary to be able to figure out like the context.

2080
02:37:52,080 --> 02:37:53,200
Yeah, I'm not even sure.

2081
02:37:53,200 --> 02:37:55,360
I'm, I'm believing my, my own devil's.

2082
02:37:55,360 --> 02:37:59,200
We have a nice, we have a nice like vision model, which outputs like a,

2083
02:37:59,200 --> 02:38:01,760
a one to four dimensional perception space.

2084
02:38:02,240 --> 02:38:04,720
Um, can I try transformers on it?

2085
02:38:04,720 --> 02:38:05,040
Sure.

2086
02:38:05,040 --> 02:38:05,680
I probably will.

2087
02:38:06,480 --> 02:38:08,800
At some point we'll try transformers and then we'll just see.

2088
02:38:08,800 --> 02:38:09,600
Do they do better?

2089
02:38:09,600 --> 02:38:09,840
Sure.

2090
02:38:09,840 --> 02:38:10,320
I'm, I'm,

2091
02:38:10,320 --> 02:38:11,760
but it might not be a game changer.

2092
02:38:11,760 --> 02:38:16,080
No, well, I'm not like, like might transformers work better than grooves for autonomous driving?

2093
02:38:16,080 --> 02:38:16,640
Sure.

2094
02:38:16,640 --> 02:38:17,520
Might we switch?

2095
02:38:17,520 --> 02:38:17,840
Sure.

2096
02:38:17,840 --> 02:38:19,200
Is this some radical change?

2097
02:38:19,200 --> 02:38:19,920
No.

2098
02:38:19,920 --> 02:38:20,240
Okay.

2099
02:38:20,240 --> 02:38:23,360
We used a slightly different, you know, we switched from RNNs to grooves like,

2100
02:38:23,360 --> 02:38:25,600
okay, maybe it's grooves to transformers, but no, it's not.

2101
02:38:27,680 --> 02:38:31,200
Well, on the, on the topic of general intelligence, I don't know how much

2102
02:38:31,200 --> 02:38:32,080
I've talked to you about it.

2103
02:38:32,080 --> 02:38:37,200
Like what, um, do you think we'll actually build an AGI?

2104
02:38:37,920 --> 02:38:41,680
Like if, if you look at Ray Kurzweil with a singularity, do you, do you have like an

2105
02:38:41,680 --> 02:38:45,120
intuition about, you're kind of saying driving is easy.

2106
02:38:45,120 --> 02:38:45,520
Yeah.

2107
02:38:46,160 --> 02:38:54,880
And I, I tend to personally believe that solving driving will have really deep,

2108
02:38:54,880 --> 02:38:59,200
important impacts on our ability to solve general intelligence.

2109
02:38:59,200 --> 02:39:04,320
Like I think driving doesn't require general intelligence, but I think they're going to

2110
02:39:04,320 --> 02:39:10,960
be neighbors in a way that it's like deeply tied because it's so, like driving is so deeply

2111
02:39:10,960 --> 02:39:17,280
connected to the human experience that I think solving one will help solve the other.

2112
02:39:17,280 --> 02:39:22,240
But, but so I don't see, I don't see driving as like easy and almost like separate than

2113
02:39:22,240 --> 02:39:26,560
general intelligence, but like what's your vision of a future with a singularity?

2114
02:39:26,560 --> 02:39:30,480
Do you see there'll be a single moment, like a singularity where it'll be a face shift?

2115
02:39:30,480 --> 02:39:32,240
Are we in the singularity now?

2116
02:39:32,240 --> 02:39:35,520
Like what, do you have crazy ideas about the future in terms of AGI?

2117
02:39:35,520 --> 02:39:37,120
We're definitely in the singularity now.

2118
02:39:37,920 --> 02:39:38,320
We are.

2119
02:39:38,320 --> 02:39:40,000
Of course, of course.

2120
02:39:40,000 --> 02:39:41,360
Look at the bandwidth between people.

2121
02:39:41,360 --> 02:39:42,640
The bandwidth between people goes up.

2122
02:39:43,200 --> 02:39:43,600
All right.

2123
02:39:44,640 --> 02:39:47,120
The singularity is just, you know, when the bandwidth, but

2124
02:39:47,120 --> 02:39:48,560
What do you mean by the bandwidth of people?

2125
02:39:48,560 --> 02:39:51,200
Communications, tools, the whole world is networked.

2126
02:39:51,200 --> 02:39:53,760
The whole world is networked and we raise the speed of that network, right?

2127
02:39:54,560 --> 02:40:00,000
Oh, so you think the communication of information in a distributed way is a

2128
02:40:00,000 --> 02:40:02,160
empowering thing for collective intelligence?

2129
02:40:02,160 --> 02:40:04,400
Oh, I didn't say it's necessarily a good thing, but I think that's like

2130
02:40:04,400 --> 02:40:07,360
when I think of the definition of the singularity, yeah, it seems kind of right.

2131
02:40:08,080 --> 02:40:12,640
I see, like it's a change in the world beyond which

2132
02:40:13,440 --> 02:40:16,560
like the world be transformed in ways that we can't possibly imagine.

2133
02:40:16,560 --> 02:40:19,600
No, I mean, I think we're in the singularity now in the sense that there's like, you know,

2134
02:40:19,600 --> 02:40:22,320
one world in a monoculture and it's also linked.

2135
02:40:22,320 --> 02:40:28,080
Yeah, I mean, I kind of shared the intuition that the singularity will originate from the

2136
02:40:28,080 --> 02:40:35,200
collective intelligence of us ants versus the like some single system AGI type thing.

2137
02:40:35,200 --> 02:40:36,160
Oh, I totally agree with that.

2138
02:40:36,880 --> 02:40:40,960
Yeah, I don't, I don't really believe in like like a hard takeoff AGI kind of thing.

2139
02:40:45,120 --> 02:40:49,440
Yeah, I don't think, I don't even think AI is all that different in kind

2140
02:40:49,440 --> 02:40:50,960
from what we've already been building.

2141
02:40:51,920 --> 02:40:56,160
With respect to driving, I think driving is a subset of general intelligence,

2142
02:40:56,160 --> 02:40:57,920
and I think it's a pretty complete subset.

2143
02:40:57,920 --> 02:41:02,160
I think the tools we develop at comma will also be extremely helpful

2144
02:41:02,160 --> 02:41:03,920
to solving general intelligence.

2145
02:41:03,920 --> 02:41:06,400
And that's, I think the real reason why I'm doing it.

2146
02:41:06,400 --> 02:41:07,680
I don't care about self-driving cars.

2147
02:41:08,240 --> 02:41:09,600
It's a cool problem to beat people at.

2148
02:41:10,800 --> 02:41:14,400
But yeah, I mean, yeah, you're kind of, you're of two minds.

2149
02:41:14,400 --> 02:41:18,400
So one, you do have to have a mission and you want to focus and make sure you get,

2150
02:41:18,400 --> 02:41:18,960
you get there.

2151
02:41:18,960 --> 02:41:20,560
You can't forget that.

2152
02:41:20,560 --> 02:41:26,960
But at the same time, there is a thread that's much bigger than the

2153
02:41:26,960 --> 02:41:29,840
capacity entirety of your effort that's much bigger than just driving.

2154
02:41:31,120 --> 02:41:35,120
With AI and with general intelligence, it is so easy to delude yourself

2155
02:41:35,120 --> 02:41:37,280
into thinking you've figured something out when you haven't.

2156
02:41:37,280 --> 02:41:42,560
If we build a level five self-driving car, we have indisputably built something.

2157
02:41:42,560 --> 02:41:43,200
Yeah.

2158
02:41:43,200 --> 02:41:44,640
Is it general intelligence?

2159
02:41:44,640 --> 02:41:45,840
I'm not going to debate that.

2160
02:41:45,840 --> 02:41:49,600
I will say we've built something that provides huge financial value.

2161
02:41:49,600 --> 02:41:50,480
Yeah, beautifully put.

2162
02:41:50,480 --> 02:41:53,520
That's the engineer and credo, just build the thing.

2163
02:41:53,520 --> 02:41:58,640
It's like, that's why I'm with Elon on Go to Mars.

2164
02:41:58,640 --> 02:41:59,680
Yeah, that's a great one.

2165
02:41:59,680 --> 02:42:02,960
You can argue who the hell cares about Go to Mars.

2166
02:42:03,520 --> 02:42:09,040
But the reality is, set that as a mission, get it done, and then you're going to crack some

2167
02:42:09,040 --> 02:42:12,640
problem that you've never even expected in the process of doing that.

2168
02:42:12,640 --> 02:42:12,960
Yeah.

2169
02:42:13,840 --> 02:42:17,680
Yeah, I mean, I think if I had a choice between humanity going to Mars and solving

2170
02:42:17,680 --> 02:42:21,120
self-driving cars, I think going to Mars is better.

2171
02:42:21,120 --> 02:42:23,440
But I don't know, I'm more suited for self-driving cars.

2172
02:42:23,440 --> 02:42:24,240
I'm an information guy.

2173
02:42:24,240 --> 02:42:24,880
I'm not a modernist.

2174
02:42:24,880 --> 02:42:25,600
I'm a post-modernist.

2175
02:42:26,480 --> 02:42:28,240
Post-modernist, all right.

2176
02:42:28,240 --> 02:42:28,880
Beautifully put.

2177
02:42:29,840 --> 02:42:32,240
Let me drag you back to programming for a sec.

2178
02:42:32,240 --> 02:42:36,480
What three, maybe three to five programming languages should people learn, do you think?

2179
02:42:36,480 --> 02:42:41,600
Like if you look at yourself, what did you get the most out of from learning?

2180
02:42:42,560 --> 02:42:45,840
Well, so everybody should learn C and assembly.

2181
02:42:45,840 --> 02:42:47,280
We'll start with those two, right?

2182
02:42:47,280 --> 02:42:48,000
Assembly.

2183
02:42:48,000 --> 02:42:48,480
Yeah.

2184
02:42:48,480 --> 02:42:51,520
If you can't code in assembly, you don't know what the computer's doing.

2185
02:42:51,520 --> 02:42:56,080
You don't understand, you don't have to be great in assembly, but you have to code in it.

2186
02:42:56,640 --> 02:43:01,200
And then you have to appreciate assembly in order to appreciate all the great things C gets you.

2187
02:43:01,920 --> 02:43:05,440
And then you have to code in C in order to appreciate all the great things Python gets you.

2188
02:43:06,160 --> 02:43:08,640
So I'll just say assembly C and Python, we'll start with those three.

2189
02:43:09,600 --> 02:43:16,640
The memory allocation of C and the fact that assembly is to give you a sense of just how

2190
02:43:16,640 --> 02:43:20,560
many levels of abstraction you get to work on in modern-day programming.

2191
02:43:20,560 --> 02:43:21,040
Yeah, yeah.

2192
02:43:21,040 --> 02:43:24,400
Graph coloring for assignment, register assignment, and compilers.

2193
02:43:26,080 --> 02:43:28,400
The computer only has a certain number of registers.

2194
02:43:28,400 --> 02:43:30,320
You can have all the variables you want in a C function.

2195
02:43:31,040 --> 02:43:35,600
So you get to start to build intuition about compilation, like what a compiler gets you.

2196
02:43:37,280 --> 02:43:37,680
What else?

2197
02:43:38,400 --> 02:43:43,840
Well, then there's kind of... So those are all very imperative programming languages.

2198
02:43:45,680 --> 02:43:49,120
Then there's two other paradigms for programming that everybody should be familiar with.

2199
02:43:49,120 --> 02:43:50,080
One of them is functional.

2200
02:43:51,120 --> 02:43:55,120
You should learn Haskell and take that all the way through, learn a language with dependent

2201
02:43:55,120 --> 02:44:01,600
types like Coq. Learn that whole space, like the very PL theory heavy languages.

2202
02:44:02,560 --> 02:44:06,400
And Haskell is your favorite functional? Is that the go-to, you'd say?

2203
02:44:06,400 --> 02:44:10,400
Yeah, I'm not a great Haskell programmer. I wrote a compiler in Haskell once.

2204
02:44:10,400 --> 02:44:13,600
There's another paradigm, and actually there's one more paradigm that I'll even talk about

2205
02:44:13,600 --> 02:44:15,760
after that that I never used to talk about when I would think about this.

2206
02:44:15,760 --> 02:44:18,160
But the next paradigm is learn Verilog or HDL.

2207
02:44:20,160 --> 02:44:22,880
Understand this idea of all of the instructions executed once.

2208
02:44:25,360 --> 02:44:29,760
If I have a block in Verilog and I write stuff in it, it's not sequential.

2209
02:44:29,760 --> 02:44:30,960
They all execute it once.

2210
02:44:33,600 --> 02:44:35,440
And then think like that. That's how hard it works.

2211
02:44:36,480 --> 02:44:39,840
So I guess assembly doesn't quite get you that.

2212
02:44:39,840 --> 02:44:44,080
Assembly is more about compilation, and Verilog is more about the hardware,

2213
02:44:44,080 --> 02:44:47,680
like giving a sense of what actually the hardware is doing.

2214
02:44:48,400 --> 02:44:52,320
Assembly, C, Python are straight like they sit right on top of each other.

2215
02:44:52,320 --> 02:44:55,520
In fact, C is kind of coded in C.

2216
02:44:55,520 --> 02:44:59,040
But you could imagine the first C was coded in assembly, and Python is actually coded in C.

2217
02:44:59,920 --> 02:45:02,240
So you can straight up go on that.

2218
02:45:03,520 --> 02:45:05,520
Got it. And then Verilog gives you that.

2219
02:45:05,600 --> 02:45:07,440
That's brilliant. Okay.

2220
02:45:07,440 --> 02:45:09,760
And then I think there's another one now.

2221
02:45:09,760 --> 02:45:14,480
Everyone's Carpathian calls it programming 2.0, which is learn a...

2222
02:45:15,280 --> 02:45:18,480
I'm not even gonna... Don't learn TensorFlow. Learn PyTorch.

2223
02:45:18,480 --> 02:45:19,440
So machine learning.

2224
02:45:20,080 --> 02:45:22,800
We've got to come up with a better term than programming 2.0 or...

2225
02:45:25,360 --> 02:45:26,000
But yeah.

2226
02:45:26,000 --> 02:45:26,880
It's a programming language.

2227
02:45:29,760 --> 02:45:32,560
I wonder if it can be formalized a little bit better.

2228
02:45:32,560 --> 02:45:35,920
We feel like we're in the early days of what that actually entails.

2229
02:45:36,960 --> 02:45:38,160
Data-driven programming?

2230
02:45:39,040 --> 02:45:40,640
Data-driven programming, yeah.

2231
02:45:41,600 --> 02:45:44,480
But it's so fundamentally different as a paradigm than the others.

2232
02:45:45,840 --> 02:45:47,920
Like it almost requires a different skill set.

2233
02:45:50,240 --> 02:45:51,440
But you think it's still... Yeah.

2234
02:45:53,600 --> 02:45:55,680
And PyTorch versus TensorFlow, PyTorch wins.

2235
02:45:56,480 --> 02:45:57,360
It's the fourth paradigm.

2236
02:45:57,360 --> 02:45:59,280
It's the fourth paradigm that I've kind of seen.

2237
02:45:59,280 --> 02:46:04,720
There's like this imperative functional hardware.

2238
02:46:04,720 --> 02:46:06,240
I don't know a better word for it.

2239
02:46:06,240 --> 02:46:07,120
And then ML.

2240
02:46:08,160 --> 02:46:15,120
Do you have advice for people that want to get into programming

2241
02:46:15,120 --> 02:46:16,080
and want to learn programming?

2242
02:46:16,080 --> 02:46:17,760
You have a video.

2243
02:46:19,920 --> 02:46:20,880
What is programming?

2244
02:46:20,880 --> 02:46:22,800
Newb lessons, exclamation point.

2245
02:46:22,800 --> 02:46:24,640
And I think the top comment is like,

2246
02:46:24,640 --> 02:46:26,240
warning, this is not for noobs.

2247
02:46:27,200 --> 02:46:32,480
Do you have a newb, like TLDW for that video,

2248
02:46:32,480 --> 02:46:39,520
but also a newb friendly advice on how to get into programming?

2249
02:46:39,520 --> 02:46:42,880
We're never going to learn programming by watching a video

2250
02:46:42,880 --> 02:46:44,560
called Learn Programming.

2251
02:46:44,560 --> 02:46:46,560
The only way to learn programming, I think,

2252
02:46:46,560 --> 02:46:49,040
and the only one is the only way everyone I've ever met

2253
02:46:49,040 --> 02:46:51,840
who can program well learned it all in the same way.

2254
02:46:51,840 --> 02:46:53,360
They had something they wanted to do,

2255
02:46:53,680 --> 02:46:55,360
and then they tried to do it.

2256
02:46:55,360 --> 02:46:59,600
And then they were like, oh, well, okay, this is kind of,

2257
02:46:59,600 --> 02:47:01,600
you know, it'd be nice if the computer could kind of do this.

2258
02:47:01,600 --> 02:47:03,360
And then, you know, that's how you learn.

2259
02:47:03,360 --> 02:47:05,360
You just keep pushing on a project.

2260
02:47:07,360 --> 02:47:10,720
So the only advice I have for learning programming is go program.

2261
02:47:10,720 --> 02:47:13,120
Somebody wrote to me a question like,

2262
02:47:13,120 --> 02:47:15,840
we don't really, they're looking to learn

2263
02:47:15,840 --> 02:47:17,440
about recurring neural networks.

2264
02:47:17,440 --> 02:47:19,760
And it's saying like, my company's thinking of doing,

2265
02:47:19,760 --> 02:47:22,560
using recurring neural networks for time series data.

2266
02:47:22,880 --> 02:47:26,000
But we don't really have an idea of where to use it yet.

2267
02:47:26,000 --> 02:47:28,480
We just want to, like, do you have any advice on how to learn about,

2268
02:47:28,480 --> 02:47:31,840
these are these kind of general machine learning questions.

2269
02:47:31,840 --> 02:47:36,480
And I think the answer is like, actually have a problem

2270
02:47:36,480 --> 02:47:38,480
that you're trying to solve and just.

2271
02:47:38,480 --> 02:47:39,440
I see that stuff.

2272
02:47:39,440 --> 02:47:41,600
Oh my God, when people talk like that, they're like,

2273
02:47:41,600 --> 02:47:44,000
I heard machine learning is important.

2274
02:47:44,000 --> 02:47:46,000
Could you help us integrate machine learning

2275
02:47:46,000 --> 02:47:48,000
with macaroni and cheese production?

2276
02:47:48,720 --> 02:47:53,840
You just, I don't even, you can't help these people.

2277
02:47:53,840 --> 02:47:55,840
Like who lets you run anything?

2278
02:47:55,840 --> 02:47:57,680
Who lets that kind of person run anything?

2279
02:47:58,240 --> 02:48:02,560
I think we're all, we're all beginners at some point.

2280
02:48:02,560 --> 02:48:03,040
So.

2281
02:48:03,040 --> 02:48:04,800
It's not like they're a beginner.

2282
02:48:04,800 --> 02:48:08,480
It's like, my problem is not that they don't know about machine learning.

2283
02:48:08,480 --> 02:48:11,600
My problem is that they think that machine learning has something to say

2284
02:48:11,600 --> 02:48:13,200
about macaroni and cheese production.

2285
02:48:14,480 --> 02:48:17,040
Or like, I heard about this new technology.

2286
02:48:17,040 --> 02:48:18,560
How can I use it for why?

2287
02:48:19,760 --> 02:48:22,640
Like, I don't know what it is, but how can I use it for why?

2288
02:48:23,440 --> 02:48:24,080
That's true.

2289
02:48:24,080 --> 02:48:26,000
And you have to build up an intuition of how,

2290
02:48:26,000 --> 02:48:27,520
because you might be able to figure out a way,

2291
02:48:27,520 --> 02:48:32,160
but like the prerequisites is you should have a macaroni and cheese problem to solve first.

2292
02:48:32,160 --> 02:48:32,640
Exactly.

2293
02:48:33,360 --> 02:48:36,800
And then two, you should have more traditional,

2294
02:48:36,800 --> 02:48:41,920
like the learning process involved more traditionally applicable problems

2295
02:48:41,920 --> 02:48:44,480
in the space of whatever that is of machine learning.

2296
02:48:44,480 --> 02:48:46,960
And then see if it can be applied to macrophages.

2297
02:48:46,960 --> 02:48:49,040
At least start with, tell me about a problem.

2298
02:48:49,040 --> 02:48:50,560
Like, if you have a problem, you're like,

2299
02:48:50,560 --> 02:48:53,600
you know, some of my boxes aren't getting enough macaroni in them.

2300
02:48:54,400 --> 02:48:56,640
Can we use machine learning to solve this problem?

2301
02:48:56,640 --> 02:48:58,400
That's much, much better than,

2302
02:48:58,400 --> 02:49:01,520
how do I apply machine learning to macaroni and cheese?

2303
02:49:01,520 --> 02:49:06,320
One big thing, maybe this is me talking to the audience a little bit,

2304
02:49:06,320 --> 02:49:09,840
because I get these days so many messages,

2305
02:49:09,840 --> 02:49:13,360
advice on how to like, learn stuff.

2306
02:49:13,360 --> 02:49:18,080
Okay. My, this is not me being mean.

2307
02:49:18,080 --> 02:49:22,080
I think this is quite profound actually, is you should Google it.

2308
02:49:22,720 --> 02:49:23,200
Oh yeah.

2309
02:49:23,760 --> 02:49:30,480
Like one of the like skills that you should really acquire as an engineer,

2310
02:49:31,040 --> 02:49:34,000
as a researcher, as a thinker, like one,

2311
02:49:34,000 --> 02:49:36,560
there's two complementary skills.

2312
02:49:36,560 --> 02:49:40,720
Like one is with a blank sheet of paper with no internet to think deeply.

2313
02:49:41,520 --> 02:49:45,840
And then the other is to Google the crap out of the questions you have.

2314
02:49:45,840 --> 02:49:47,040
Like that's actually a skill.

2315
02:49:47,840 --> 02:49:50,560
People often talk about, but like doing research,

2316
02:49:50,560 --> 02:49:53,280
like pulling at the thread and like looking up different words,

2317
02:49:53,840 --> 02:49:57,920
going into like GitHub repositories with two stars,

2318
02:49:57,920 --> 02:49:59,600
and like looking how they did stuff,

2319
02:49:59,600 --> 02:50:03,360
like looking at the code or going on Twitter,

2320
02:50:03,360 --> 02:50:07,440
seeing like there's little pockets of brilliant people that are like having discussions.

2321
02:50:07,520 --> 02:50:11,520
Like if you're a neuroscientist, go into signal processing community.

2322
02:50:11,520 --> 02:50:15,760
If you're an AI person going into the psychology community,

2323
02:50:15,760 --> 02:50:19,840
like the switch communities that keep searching, searching, searching,

2324
02:50:19,840 --> 02:50:25,680
because it's so much better to invest in like finding somebody else

2325
02:50:25,680 --> 02:50:30,240
who already solved your problem than is to try to solve the problem.

2326
02:50:30,800 --> 02:50:34,240
And because they've often invested years of their life,

2327
02:50:34,320 --> 02:50:39,120
like entire communities are probably already out there who have tried to solve your problem.

2328
02:50:39,120 --> 02:50:40,240
I think they're the same thing.

2329
02:50:40,880 --> 02:50:44,080
I think you go try to solve the problem.

2330
02:50:44,080 --> 02:50:46,080
And then in trying to solve the problem,

2331
02:50:46,080 --> 02:50:47,680
if you're good at solving problems,

2332
02:50:47,680 --> 02:50:49,760
you'll stumble upon the person who solved it already.

2333
02:50:49,760 --> 02:50:52,240
Yeah, but the stumbling is really important.

2334
02:50:52,240 --> 02:50:54,160
I think that's a skill that people should really put,

2335
02:50:54,160 --> 02:50:56,880
especially in undergrad, like search.

2336
02:50:57,600 --> 02:51:00,320
If you ask me a question, how should I get started in deep learning?

2337
02:51:01,200 --> 02:51:07,120
Especially that is just so Google-able.

2338
02:51:08,000 --> 02:51:11,520
The whole point is you Google that and you get a million pages

2339
02:51:11,520 --> 02:51:12,960
and just start looking at them.

2340
02:51:13,840 --> 02:51:14,400
Yeah.

2341
02:51:14,400 --> 02:51:17,200
Start pulling at the thread, start exploring, start taking notes,

2342
02:51:17,200 --> 02:51:22,720
start getting advice from a million people that already spent their life

2343
02:51:22,720 --> 02:51:24,160
answering that question, actually.

2344
02:51:24,880 --> 02:51:25,280
Oh, well, yeah.

2345
02:51:25,280 --> 02:51:27,680
I mean, that's definitely also when people ask me things like that,

2346
02:51:27,760 --> 02:51:30,320
I'm like, trust me, the top answer on Google is much, much better

2347
02:51:30,320 --> 02:51:31,440
than anything I'm going to tell you.

2348
02:51:31,440 --> 02:51:32,000
Anything you want to say.

2349
02:51:32,000 --> 02:51:32,800
Right?

2350
02:51:32,800 --> 02:51:33,120
Yeah.

2351
02:51:34,400 --> 02:51:35,120
People ask.

2352
02:51:35,920 --> 02:51:37,040
It's an interesting question.

2353
02:51:38,000 --> 02:51:39,840
Let me know if you have any recommendations.

2354
02:51:39,840 --> 02:51:43,680
What three books, technical or fiction or philosophical,

2355
02:51:43,680 --> 02:51:47,440
had an impact on your life or you wouldn't recommend, perhaps?

2356
02:51:49,040 --> 02:51:51,040
Maybe we'll start with the least controversial.

2357
02:51:51,040 --> 02:51:51,760
Infinite Jest.

2358
02:51:54,080 --> 02:51:55,920
Infinite Jest is a...

2359
02:51:56,880 --> 02:51:58,800
David Foster Wallace.

2360
02:51:58,800 --> 02:52:00,640
Yeah, it's a book about wireheading, really.

2361
02:52:03,760 --> 02:52:06,880
Very enjoyable to read, very well-written.

2362
02:52:08,320 --> 02:52:10,000
You will grow as a person reading this book.

2363
02:52:11,040 --> 02:52:11,600
It's effort.

2364
02:52:12,960 --> 02:52:16,000
And I'll set that up for the second book, which is pornography.

2365
02:52:16,000 --> 02:52:20,240
That's called Atlas Shrugged, which...

2366
02:52:21,040 --> 02:52:22,480
Atlas Shrugged is pornography.

2367
02:52:22,480 --> 02:52:23,760
Yeah, I mean, it is.

2368
02:52:23,760 --> 02:52:25,600
I will not defend the...

2369
02:52:25,600 --> 02:52:27,840
I will not say Atlas Shrugged is a well-written book.

2370
02:52:28,480 --> 02:52:31,440
It is entertaining to read, certainly, just like pornography.

2371
02:52:31,440 --> 02:52:33,040
The production value isn't great.

2372
02:52:34,080 --> 02:52:38,800
There's a 60-page monologue in there that Anne Rand's editor really wanted to take out.

2373
02:52:38,800 --> 02:52:44,880
And she paid out of her pocket to keep that 60-page monologue in the book.

2374
02:52:45,840 --> 02:52:54,560
But it is a great book for a kind of framework of human relations.

2375
02:52:54,560 --> 02:52:57,360
And I know a lot of people are like, yeah, but it's a terrible framework.

2376
02:52:57,920 --> 02:52:59,280
Yeah, but it's a framework.

2377
02:53:00,320 --> 02:53:06,240
Just for context, in a couple of days, I'm speaking for probably four plus hours with

2378
02:53:06,240 --> 02:53:11,440
Yaron Brooke, who's the main living, remaining objectivist.

2379
02:53:12,160 --> 02:53:13,200
Objectivist.

2380
02:53:13,200 --> 02:53:13,680
Interesting.

2381
02:53:14,560 --> 02:53:20,320
So I've always found this philosophy quite interesting on many levels.

2382
02:53:20,320 --> 02:53:28,880
One of how repulsive some large percent of the population find it, which is always funny to

2383
02:53:28,880 --> 02:53:35,280
me when people are unable to even read a philosophy because of some...

2384
02:53:35,280 --> 02:53:40,080
I think that says more about their psychological perspective on it.

2385
02:53:40,960 --> 02:53:48,640
But there is something about objectivism and Anne Rand's philosophy that's deeply connected

2386
02:53:48,640 --> 02:53:54,320
to this idea of capitalism, of the ethical life is the productive life,

2387
02:53:56,480 --> 02:53:59,920
that was always compelling to me.

2388
02:54:01,760 --> 02:54:05,440
I didn't seem to interpret it in the negative sense that some people do.

2389
02:54:05,440 --> 02:54:07,200
To be fair, I read the book when I was 19.

2390
02:54:07,840 --> 02:54:09,120
So you had an impact at that point.

2391
02:54:09,520 --> 02:54:15,200
Yeah, and the bad guys in the book have this slogan, from each according to their ability

2392
02:54:15,200 --> 02:54:16,480
to each according to their need.

2393
02:54:18,480 --> 02:54:20,240
I'm looking at this and I'm like, these are the most...

2394
02:54:20,240 --> 02:54:22,800
This is team rocket level cartoonishness, right?

2395
02:54:22,800 --> 02:54:23,680
No, bad guy.

2396
02:54:23,680 --> 02:54:28,160
And then when I realized that was actually the slogan of the Communist Party, I'm like,

2397
02:54:28,160 --> 02:54:31,520
wait a second, wait, no, no, no, no, no.

2398
02:54:31,520 --> 02:54:32,960
You're telling me this really happened?

2399
02:54:33,920 --> 02:54:34,720
Yeah, it's interesting.

2400
02:54:34,960 --> 02:54:39,040
One of the criticisms of her work is she has a cartoonish view of good and evil.

2401
02:54:42,080 --> 02:54:47,520
The reality, as Jordan Peterson says, is that each of us have the capacity for good and evil in

2402
02:54:47,520 --> 02:54:52,080
us as opposed to there's some characters who are purely evil and some characters are purely good.

2403
02:54:52,080 --> 02:54:53,920
And that's in a way why it's pornographic.

2404
02:54:55,040 --> 02:54:56,880
The production value, I love it.

2405
02:54:56,880 --> 02:55:01,920
Well, evil is punished and they're very clearly like, you know, there's no, you know,

2406
02:55:02,640 --> 02:55:07,360
you know, just like porn doesn't have, you know, like character growth, you know,

2407
02:55:07,360 --> 02:55:08,880
neither does Alice Rodney to like.

2408
02:55:09,440 --> 02:55:10,800
Brilliant, well put.

2409
02:55:10,800 --> 02:55:14,080
But at 19 year old George Hots, it was good enough.

2410
02:55:14,080 --> 02:55:15,280
Yeah, yeah, yeah, yeah.

2411
02:55:15,280 --> 02:55:16,800
What's the third?

2412
02:55:16,800 --> 02:55:17,440
You have something?

2413
02:55:19,040 --> 02:55:21,360
I could give these two I'll just throw out.

2414
02:55:21,360 --> 02:55:23,360
They're sci-fi, Perbutation City.

2415
02:55:24,160 --> 02:55:26,560
Great thing, just try thinking about copies yourself.

2416
02:55:26,560 --> 02:55:27,360
And then the minute...

2417
02:55:27,360 --> 02:55:27,840
Isn't that by?

2418
02:55:27,840 --> 02:55:28,560
Sorry to interrupt.

2419
02:55:28,560 --> 02:55:30,720
That is Greg Egan.

2420
02:55:32,320 --> 02:55:33,440
That might not be his real name.

2421
02:55:33,440 --> 02:55:34,320
Some Australian guy.

2422
02:55:34,320 --> 02:55:35,600
It might not be Australian.

2423
02:55:35,600 --> 02:55:36,080
I don't know.

2424
02:55:36,960 --> 02:55:38,560
And then this one's online.

2425
02:55:38,560 --> 02:55:40,720
It's called the Metamorphosis of Prime Intellect.

2426
02:55:42,960 --> 02:55:45,280
It's a story set in a post-singularity world.

2427
02:55:45,280 --> 02:55:45,760
It's interesting.

2428
02:55:46,640 --> 02:55:50,160
Is there, can you, in either of the worlds, do you find something

2429
02:55:50,160 --> 02:55:52,240
philosophical interesting in them that you can comment on?

2430
02:55:53,520 --> 02:55:55,520
I mean, it is clear to me that

2431
02:55:57,920 --> 02:56:01,200
Metamorphosis of Prime Intellect is like written by an engineer.

2432
02:56:02,720 --> 02:56:11,040
Which is, it's very, it's very almost a pragmatic take on a utopia, in a way.

2433
02:56:12,480 --> 02:56:13,440
Positive or negative?

2434
02:56:15,120 --> 02:56:17,840
That's up to you to decide reading the book.

2435
02:56:17,840 --> 02:56:21,440
And the ending of it is very interesting as well.

2436
02:56:21,440 --> 02:56:23,520
And I didn't realize what it was.

2437
02:56:23,520 --> 02:56:25,120
I first read that when I was 15.

2438
02:56:25,120 --> 02:56:26,800
I've reread that book several times in my life.

2439
02:56:27,360 --> 02:56:29,040
And it's short, it's 50 pages.

2440
02:56:29,040 --> 02:56:29,920
I want you to go read it.

2441
02:56:30,720 --> 02:56:33,040
What's, sorry, it's a little tangent.

2442
02:56:33,040 --> 02:56:34,560
I've been working through the foundation.

2443
02:56:34,560 --> 02:56:36,960
I've been, I haven't read much sci-fi in my whole life.

2444
02:56:36,960 --> 02:56:40,080
And I'm trying to fix that in the last few months.

2445
02:56:40,080 --> 02:56:41,520
That's been a little side project.

2446
02:56:42,080 --> 02:56:47,520
What's to use the greatest sci-fi novel that people should read?

2447
02:56:47,520 --> 02:56:49,200
Or is it, or?

2448
02:56:49,200 --> 02:56:51,040
I mean, I would, yeah, I would, I would say like, yeah,

2449
02:56:51,040 --> 02:56:52,800
Permutation City, Metamorphosis of Prime Intellect.

2450
02:56:52,800 --> 02:56:53,680
Got it.

2451
02:56:53,680 --> 02:56:54,320
I don't know.

2452
02:56:54,320 --> 02:56:55,520
I didn't like Foundation.

2453
02:56:56,320 --> 02:56:57,680
I thought it was way too modernist.

2454
02:56:58,560 --> 02:57:00,640
Be like Dune and like all of those.

2455
02:57:00,640 --> 02:57:01,760
I've never read Dune.

2456
02:57:01,760 --> 02:57:02,800
I've never read Dune.

2457
02:57:02,800 --> 02:57:03,760
I have to read it.

2458
02:57:04,480 --> 02:57:06,720
Fire Upon the Deep is interesting.

2459
02:57:08,960 --> 02:57:10,400
Okay, I mean, look, everyone should read,

2460
02:57:10,400 --> 02:57:11,280
everyone should read Neuromancer.

2461
02:57:11,280 --> 02:57:12,720
Everyone should read Snow Crash.

2462
02:57:12,720 --> 02:57:14,400
If you haven't read those, like, start there.

2463
02:57:15,360 --> 02:57:16,320
Yeah, I haven't read Snow Crash.

2464
02:57:16,320 --> 02:57:17,280
I haven't read Snow Crash.

2465
02:57:17,280 --> 02:57:18,240
Oh, it's, I mean, it's very entertaining.

2466
02:57:18,240 --> 02:57:18,720
It's surprising.

2467
02:57:19,840 --> 02:57:20,640
Go to Lesher Bach.

2468
02:57:20,640 --> 02:57:23,040
And if you want the controversial one, Bronze Age Mindset.

2469
02:57:25,200 --> 02:57:26,560
All right, I'll look into that one.

2470
02:57:27,520 --> 02:57:30,080
Those aren't sci-fi, but just to round out books.

2471
02:57:31,600 --> 02:57:35,200
So a bunch of people asked me on Twitter and Reddit and so on

2472
02:57:35,840 --> 02:57:36,800
for advice.

2473
02:57:36,800 --> 02:57:40,080
So what advice would you give a young person today about life?

2474
02:57:44,720 --> 02:57:47,440
What, yeah, I mean, looking back,

2475
02:57:47,440 --> 02:57:49,760
especially when you're a young, younger, you did,

2476
02:57:50,400 --> 02:57:54,800
and you continued it, you've accomplished a lot of interesting things.

2477
02:57:54,800 --> 02:57:57,120
Is there some advice from those,

2478
02:57:59,760 --> 02:58:01,760
from that life of yours that you can pass on?

2479
02:58:01,760 --> 02:58:06,480
If college ever opens again, I would love to give a graduation speech.

2480
02:58:07,520 --> 02:58:12,160
At that point, I will put a lot of somewhat satirical effort into this question.

2481
02:58:12,160 --> 02:58:15,280
Yeah, at this, you haven't written anything at this point.

2482
02:58:16,240 --> 02:58:18,080
You know what, always wear sunscreen.

2483
02:58:18,080 --> 02:58:19,680
This is water, like.

2484
02:58:19,680 --> 02:58:21,040
I think you're plagiarizing.

2485
02:58:21,040 --> 02:58:24,720
I mean, you know, but that's the, that's the, like,

2486
02:58:24,880 --> 02:58:26,080
put clean your room.

2487
02:58:26,080 --> 02:58:28,480
You know, yeah, you can plagiarize them from all of this stuff.

2488
02:58:28,480 --> 02:58:29,040
And it's, it's,

2489
02:58:33,200 --> 02:58:33,920
there is no,

2490
02:58:35,840 --> 02:58:37,520
self-help books aren't designed to help you.

2491
02:58:37,520 --> 02:58:38,880
They're designed to make you feel good.

2492
02:58:39,920 --> 02:58:44,000
Like, whatever advice I could give, you already know.

2493
02:58:44,000 --> 02:58:45,120
Everyone already knows.

2494
02:58:45,760 --> 02:58:47,200
Sorry, it doesn't feel good.

2495
02:58:50,080 --> 02:58:50,480
Right?

2496
02:58:50,480 --> 02:58:52,000
Like, you know, you know.

2497
02:58:52,960 --> 02:58:55,760
If I tell you that you should, you know,

2498
02:58:56,800 --> 02:59:01,120
eat well and read more and it's not going to do anything.

2499
02:59:01,760 --> 02:59:06,160
I think the whole, like, genre of those kind of questions is meaningless.

2500
02:59:07,360 --> 02:59:07,840
I don't know.

2501
02:59:07,840 --> 02:59:10,480
If anything, it's don't worry so much about that stuff.

2502
02:59:10,480 --> 02:59:11,760
Don't be so caught up in your head.

2503
02:59:12,320 --> 02:59:12,960
Right.

2504
02:59:12,960 --> 02:59:16,400
I mean, you're, yeah, in the sense that your whole life,

2505
02:59:16,400 --> 02:59:19,600
your whole existence is like moving version of that advice.

2506
02:59:20,560 --> 02:59:21,040
I don't know.

2507
02:59:23,760 --> 02:59:25,360
There's, there's something, I mean,

2508
02:59:25,360 --> 02:59:28,160
there's something in you that resists that kind of thinking in that,

2509
02:59:28,160 --> 02:59:34,400
in itself is, it's just illustrative of who you are.

2510
02:59:34,400 --> 02:59:35,760
And there's something to learn from that.

2511
02:59:36,640 --> 02:59:39,440
I think you're, you're clearly not overthinking stuff.

2512
02:59:41,280 --> 02:59:41,600
Yeah.

2513
02:59:41,600 --> 02:59:42,000
And you know what?

2514
02:59:42,000 --> 02:59:43,040
There's a gut thing.

2515
02:59:43,040 --> 02:59:45,120
I, even when I talk about my advice, I'm like,

2516
02:59:45,120 --> 02:59:46,720
my advice is only relevant to me.

2517
02:59:47,280 --> 02:59:48,640
It's not relevant to anybody else.

2518
02:59:48,640 --> 02:59:49,840
I'm not saying you should go out.

2519
02:59:49,840 --> 02:59:51,520
If you're the kind of person who overthinks things,

2520
02:59:51,520 --> 02:59:53,280
to stop overthinking things, it's not bad.

2521
02:59:53,920 --> 02:59:54,800
It doesn't work for me.

2522
02:59:54,800 --> 02:59:55,600
Maybe it works for you.

2523
02:59:55,600 --> 02:59:56,880
I, you know, I don't know.

2524
02:59:57,840 --> 02:59:59,200
Let me ask you about love.

2525
02:59:59,200 --> 02:59:59,520
Yeah.

2526
03:00:01,840 --> 03:00:05,040
So I think last time we talked about the meaning of life,

2527
03:00:05,040 --> 03:00:08,480
and it was, it was kind of about winning.

2528
03:00:08,480 --> 03:00:08,800
Of course.

2529
03:00:10,720 --> 03:00:12,960
I don't think I've talked to you about love much,

2530
03:00:12,960 --> 03:00:17,280
whether romantic or just love for the common humanity amongst us all.

2531
03:00:18,000 --> 03:00:21,280
What role has love played in your life?

2532
03:00:21,280 --> 03:00:25,600
In this, in this quest for winning, where does love fit in?

2533
03:00:26,160 --> 03:00:29,760
Well, where love, I think means several different things.

2534
03:00:29,760 --> 03:00:32,800
There's love in the sense of, maybe I could just say,

2535
03:00:32,800 --> 03:00:37,840
there's like love in the sense of opiates and love in the sense of oxytocin,

2536
03:00:37,840 --> 03:00:44,320
and then love in the sense of maybe like a love for math.

2537
03:00:44,320 --> 03:00:46,720
I don't think fits into either of those first two paradigms.

2538
03:00:47,840 --> 03:00:56,000
So each of those, have they, have they, have they given something to you in your life?

2539
03:00:56,720 --> 03:00:58,640
I'm not that big of a fan of the first two.

2540
03:01:00,640 --> 03:01:00,880
Why?

2541
03:01:03,440 --> 03:01:06,160
The same reason I'm not a fan of, you know,

2542
03:01:06,160 --> 03:01:09,600
the same reason I don't do opiates and don't take ecstasy.

2543
03:01:11,280 --> 03:01:12,960
And there were times, look, I've tried both.

2544
03:01:13,920 --> 03:01:17,040
I like opiates way more than ecstasy.

2545
03:01:18,240 --> 03:01:23,200
But they're not, the ethical life is the productive life.

2546
03:01:24,320 --> 03:01:26,880
So maybe that's my problem with those.

2547
03:01:26,880 --> 03:01:31,200
And then like, yeah, a sense of, I don't know, like abstract love for humanity.

2548
03:01:32,000 --> 03:01:36,080
I mean, the abstract love for humanity, I'm like, yeah, I've always felt that.

2549
03:01:36,080 --> 03:01:40,320
And I guess it's hard for me to imagine not feeling it.

2550
03:01:40,320 --> 03:01:42,560
And maybe there's people who don't and I don't know.

2551
03:01:43,440 --> 03:01:43,680
Yeah.

2552
03:01:43,680 --> 03:01:45,680
That's just like a background thing that's there.

2553
03:01:46,320 --> 03:01:49,200
I mean, since we brought up drugs, let me ask you,

2554
03:01:51,600 --> 03:01:55,360
this is becoming more and more a part of my life because I'm talking to a few researchers

2555
03:01:55,360 --> 03:01:56,640
that are working on psychedelics.

2556
03:01:57,600 --> 03:02:00,240
I've eaten shrooms a couple of times.

2557
03:02:00,240 --> 03:02:04,800
And it was fascinating to me that like the mind can go to like,

2558
03:02:06,080 --> 03:02:09,280
just fascinating, the mind can go to places I didn't imagine it could go.

2559
03:02:09,360 --> 03:02:12,640
And I was very friendly and positive and exciting.

2560
03:02:12,640 --> 03:02:16,080
And everything was kind of hilarious in the place.

2561
03:02:16,080 --> 03:02:18,160
Wherever my mind went, that's where I went.

2562
03:02:18,160 --> 03:02:21,040
Is what do you think about psychedelics?

2563
03:02:21,040 --> 03:02:24,560
Do you think they have, where do you think the mind goes?

2564
03:02:24,560 --> 03:02:26,080
Have you done psychedelics?

2565
03:02:26,080 --> 03:02:27,360
Where do you think the mind goes?

2566
03:02:28,560 --> 03:02:31,040
Is there something useful to learn about the places it goes?

2567
03:02:32,240 --> 03:02:33,120
Once you come back.

2568
03:02:33,760 --> 03:02:40,400
I find it interesting that this idea that psychedelics have something to teach

2569
03:02:40,400 --> 03:02:42,400
is almost unique to psychedelics.

2570
03:02:43,760 --> 03:02:45,600
People don't argue this about amphetamines.

2571
03:02:48,400 --> 03:02:50,160
And I'm not really sure why.

2572
03:02:50,160 --> 03:02:52,960
I think all of the drugs have lessons to teach.

2573
03:02:53,680 --> 03:02:55,040
I think there's things to learn from opiates.

2574
03:02:55,040 --> 03:02:56,480
I think there's things to learn from amphetamines.

2575
03:02:56,480 --> 03:02:58,080
I think there's things to learn from psychedelics,

2576
03:02:58,080 --> 03:02:59,120
things to learn from marijuana.

2577
03:03:00,000 --> 03:03:06,640
But also at the same time, recognize that I don't think you're learning things

2578
03:03:06,640 --> 03:03:07,280
about the world.

2579
03:03:07,280 --> 03:03:08,800
I think you're learning things about yourself.

2580
03:03:10,640 --> 03:03:14,160
And what's the, it might have even been.

2581
03:03:15,600 --> 03:03:17,280
Might have even been a Timothy Leary quote.

2582
03:03:17,280 --> 03:03:18,080
I don't know his quote.

2583
03:03:18,080 --> 03:03:21,440
But the idea is basically like, everybody should look behind the door.

2584
03:03:21,440 --> 03:03:23,680
But then once you've seen behind the door, you don't need to keep going back.

2585
03:03:24,400 --> 03:03:29,040
So, I mean, and that's my thoughts on all real drug use too.

2586
03:03:29,760 --> 03:03:30,800
So maybe for caffeine.

2587
03:03:32,640 --> 03:03:36,160
It's a little experience that it's good to have, but.

2588
03:03:37,040 --> 03:03:37,520
Oh yeah.

2589
03:03:37,520 --> 03:03:40,400
No, I mean, yeah, I guess, yeah, psychedelics are definitely.

2590
03:03:41,760 --> 03:03:43,840
So you're a fan of new experiences, I suppose.

2591
03:03:43,840 --> 03:03:44,160
Yes.

2592
03:03:44,160 --> 03:03:46,880
Because they all contain a little, especially the first few times,

2593
03:03:46,880 --> 03:03:48,640
it contains some lessons that can be picked up.

2594
03:03:49,600 --> 03:03:50,000
Yeah.

2595
03:03:50,000 --> 03:03:53,760
And I'll, I'll revisit psychedelics maybe once a year.

2596
03:03:55,600 --> 03:03:57,200
Usually small, smaller doses.

2597
03:03:58,640 --> 03:04:00,160
Maybe they turn up the learning rate of your brain.

2598
03:04:01,280 --> 03:04:03,120
I've heard that, I like that.

2599
03:04:03,120 --> 03:04:04,160
Yeah, that's cool.

2600
03:04:04,160 --> 03:04:05,760
Big learning rates have frozen comms.

2601
03:04:07,360 --> 03:04:09,360
Last question, and this is a little weird one,

2602
03:04:09,360 --> 03:04:11,520
but you've called yourself crazy in the past.

2603
03:04:14,000 --> 03:04:17,840
First of all, on a scale of one to 10, how crazy would you say, are you?

2604
03:04:17,840 --> 03:04:20,240
Oh, I mean, it depends how you, you know, when you compare me to

2605
03:04:20,240 --> 03:04:22,320
Elon Musk and I say Levin Dalsky, not so crazy.

2606
03:04:23,360 --> 03:04:24,560
So like, like a seven?

2607
03:04:25,760 --> 03:04:27,040
Let's go with six.

2608
03:04:27,040 --> 03:04:28,560
Six, six, six.

2609
03:04:29,360 --> 03:04:30,080
What, uh.

2610
03:04:31,520 --> 03:04:33,200
I like seven, seven's a good number.

2611
03:04:33,200 --> 03:04:34,160
Seven, sorry.

2612
03:04:34,160 --> 03:04:36,480
Well, yeah, I'm sure day by day changes, right?

2613
03:04:36,480 --> 03:04:39,840
So, but you're in that, in that area.

2614
03:04:39,840 --> 03:04:45,120
What, uh, in thinking about that, what do you think is the role of madness?

2615
03:04:45,680 --> 03:04:50,320
Is that a feature or a bug if you were to dissect your brain?

2616
03:04:51,680 --> 03:04:56,960
So, okay, from like a, like mental health lens on crazy,

2617
03:04:56,960 --> 03:04:58,880
I'm not sure I really believe in that.

2618
03:04:58,880 --> 03:05:02,960
I'm not sure I really believe in like a lot of that stuff, right?

2619
03:05:02,960 --> 03:05:05,840
This concept of, okay, you know, when you get over to like,

2620
03:05:06,640 --> 03:05:09,520
like, like, like hardcore bipolar and schizophrenia,

2621
03:05:09,520 --> 03:05:13,120
these things are clearly real, somewhat biological.

2622
03:05:13,120 --> 03:05:18,320
And then over here on the spectrum, you have like ADD and oppositional defiance disorder

2623
03:05:18,320 --> 03:05:22,800
and these things that are like, wait, this is normal spectrum human behavior.

2624
03:05:22,800 --> 03:05:28,560
Like this isn't, you know, where's the, the line here?

2625
03:05:28,560 --> 03:05:31,200
And why is this like a problem?

2626
03:05:31,200 --> 03:05:35,760
So there's this whole, you know, the neurodiversity of humanity is huge.

2627
03:05:35,760 --> 03:05:37,520
Like people think I'm always on drugs.

2628
03:05:37,520 --> 03:05:39,360
People are always saying this to me on my streams and like guys,

2629
03:05:39,360 --> 03:05:41,280
you know, like I'm real open with my drug use.

2630
03:05:41,280 --> 03:05:45,520
I would tell you if I was on drugs and I mean, I had like a cup of coffee this morning,

2631
03:05:45,520 --> 03:05:47,120
but other than that, this is just me.

2632
03:05:47,120 --> 03:05:49,520
You're witnessing my brain in action.

2633
03:05:51,440 --> 03:05:55,520
So, so the word madness doesn't even make sense.

2634
03:05:55,520 --> 03:05:59,360
And then you're in the rich neurodiversity of humans.

2635
03:06:00,880 --> 03:06:06,960
I think it makes sense, but only for like some insane extremes.

2636
03:06:06,960 --> 03:06:14,880
Like if you are actually like visibly hallucinating, you know, that's okay.

2637
03:06:14,880 --> 03:06:17,360
But there is the kind of spectrum on which you stand out.

2638
03:06:17,360 --> 03:06:23,680
Like that, that's like, if I were to look, you know, at decorations on a Christmas tree

2639
03:06:23,680 --> 03:06:28,720
or something like that, like if you were a decoration that would catch my eye.

2640
03:06:28,720 --> 03:06:30,560
Like that thing is sparkly.

2641
03:06:32,720 --> 03:06:36,480
Whatever the hell that thing is, there's something to that.

2642
03:06:37,280 --> 03:06:44,240
Just like refusing to be boring or maybe boring is the wrong word, but to,

2643
03:06:46,640 --> 03:06:50,240
yeah, I mean, be willing to sparkle.

2644
03:06:51,200 --> 03:06:54,080
You know, it's, it's like somewhat constructed.

2645
03:06:54,080 --> 03:06:56,400
I mean, I am who I choose to be.

2646
03:06:58,400 --> 03:07:00,800
I'm going to say things as true as I can see them.

2647
03:07:00,800 --> 03:07:04,240
I'm not going to, I'm not going to lie.

2648
03:07:04,240 --> 03:07:06,480
And but that's a really important feature in itself.

2649
03:07:06,480 --> 03:07:10,240
So like whatever the neurodiversity of your, whatever your brain is,

2650
03:07:11,040 --> 03:07:17,760
not putting constraints on it that force it to fit into the mold of what society is,

2651
03:07:17,760 --> 03:07:20,560
like defines what you're supposed to be.

2652
03:07:20,560 --> 03:07:26,160
So you're one of the specimens that, that doesn't mind being yourself.

2653
03:07:27,760 --> 03:07:33,440
Being right is super important, except at the expense of being wrong.

2654
03:07:37,120 --> 03:07:38,320
Without breaking that apart.

2655
03:07:38,320 --> 03:07:40,240
I think it's a beautiful way to end it.

2656
03:07:40,240 --> 03:07:42,960
George, you're, you're one of the most special humans I know.

2657
03:07:42,960 --> 03:07:44,480
It's truly an honor to talk to you.

2658
03:07:44,480 --> 03:07:45,600
Thanks so much for doing it.

2659
03:07:45,600 --> 03:07:47,600
Thank you for having me.

2660
03:07:47,600 --> 03:07:50,240
Thanks for listening to this conversation with George Hots.

2661
03:07:50,240 --> 03:07:56,400
And thank you to our sponsors for Sigmatic, which is the maker of delicious mushroom coffee,

2662
03:07:57,120 --> 03:08:01,360
Decoding Digital, which is a tech podcast that I listen to and enjoy,

2663
03:08:02,080 --> 03:08:06,400
and ExpressVPN, which is the VPN I've used for many years.

2664
03:08:06,960 --> 03:08:10,320
Please check out these sponsors in the description to get a discount

2665
03:08:10,320 --> 03:08:12,320
and to support this podcast.

2666
03:08:12,960 --> 03:08:17,760
If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple podcast,

2667
03:08:17,760 --> 03:08:23,680
follow on Spotify, support on Patreon, or connect with me on Twitter at Lex Freedman.

2668
03:08:24,320 --> 03:08:29,920
And now let me leave you with some words from the great and powerful Linus Torvald.

2669
03:08:30,640 --> 03:08:32,000
Talk is cheap.

2670
03:08:32,000 --> 03:08:34,160
Show me the code.

2671
03:08:34,160 --> 03:08:42,160
Thank you for listening and hope to see you next time.

