start	end	text
0	5360	The following is a conversation with Ilya Satskeva, co-founder and chief scientist of Open AI,
6080	12800	one of the most cited computer scientists in history with over 165,000 citations,
13440	19040	and to me, one of the most brilliant and insightful minds ever in the field of deep learning.
19920	24240	There are very few people in this world who I would rather talk to and brainstorm with about
24240	31920	deep learning, intelligence, and life in general than Ilya, on and off the mic. This was an honor
31920	37120	and a pleasure. This conversation was recorded before the outbreak of the pandemic,
37120	41360	for everyone feeling the medical, psychological, and financial burden of this crisis,
41360	46160	I'm sending love your way. Stay strong, we're in this together, we'll beat this thing.
47120	51680	This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,
51680	56240	review it with Five Stars and Apple Podcasts, support on Patreon, or simply connect with me on
56240	62880	Twitter at Lex Freedman's belt F-R-I-D-M-A-N. As usual, I'll do a few minutes of ads now
62880	66480	and never any ads in the middle that can break the flow of the conversation.
66480	69840	I hope that works for you and doesn't hurt the listening experience.
70880	75600	This show is presented by Cash App, the number one finance app in the App Store.
75600	80880	When you get it, use code LexPodcast. Cash App lets you send money to friends,
80880	86720	buy Bitcoin, invest in the stock market with as little as $1. Since Cash App allows you to buy
86720	92400	Bitcoin, let me mention that cryptocurrency in the context of the history of money is fascinating.
92960	96080	I recommend Ascent of Money as a great book on this history.
96720	102000	Both the book and audiobook are great. Debits and credits on ledgers started around
102000	108880	30,000 years ago. The US Dollar created over 200 years ago, and Bitcoin, the first decentralized
108880	114880	cryptocurrency released just over 10 years ago. Given that history, cryptocurrency is still very
114880	120720	much in its early days of development, but is still aiming to and just might redefine the nature of
120720	127440	money. Again, if you get Cash App from the App Store or Google Play and use the code LexPodcast,
128000	134160	you get $10 and Cash App will also donate $10 to FIRST, an organization that is helping advance
134160	140560	robotics and STEM education for young people around the world. And now, here's my conversation
140560	147680	with Ilya Satskeva. You were one of the three authors with Alex Koshchevsky, Jeff Hinton,
147680	155200	of the famed AlexNet paper that is arguably the paper that marked the big catalytic moment that
155200	159920	launched the deep learning revolution. At that time, take us back to that time. What was your
159920	164880	intuition about neural networks, about the representation of power of neural networks?
165920	172000	And maybe you could mention how did that evolve over the next few years, up to today, over the
172000	178240	10 years? Yeah, I can answer that question. At some point in about 2010 or 2011,
179920	188080	I connected two facts in my mind. Basically, the realization was this. At some point,
188080	193520	we realized that we can train very large, I shouldn't say very tiny by today's standards, but
194320	199200	large and deep neural networks end to end with back propagation. At some point,
200640	206320	different people obtained this result. I obtained this result. The first moment in which I realized
206320	212160	that deep neural networks are powerful was when James Martens invented the Hessian Free Optimizer
212160	218080	in 2010. And he trained a 10 layer neural network end to end without pre-training
219600	225120	from scratch. And when that happened, I thought this is it. Because if you can train a big neural
225120	230160	network, a big neural network can represent very complicated function. Because if you have a neural
230160	238240	network with 10 layers, it's as though you allow the human brain to run for some number of milliseconds,
238320	244640	neuron firings are slow. And so in maybe 100 milliseconds, your neurons only fire 10 times.
244640	249440	So it's also kind of like 10 layers. And in 100 milliseconds, you can perfectly recognize any
249440	254800	object. So I thought, so I already had the idea then that we need to train a very big neural network
256080	260560	on lots of supervised data. And then it must succeed, because we can find the best neural
260560	264800	network. And then there's also theory that if you have more data than parameters, you won't
264800	268880	overfit. Today, we know that actually, this theory is very incomplete, and you won't overfit even
268880	272640	you have less data than parameters. But definitely, if you have more data than parameters, you won't
272640	278640	overfit. So the fact that neural networks were heavily overparameterized, wasn't discouraging to
278640	283920	you. So you were thinking about the theory that the number of parameters, the fact there's a huge
283920	287360	number of parameters is okay, it's going to be okay. I mean, there was some evidence before that
287360	291680	it was okay, but the theory was most the theory was that if you had a big data set and a big
291680	296640	neural net, it was going to work. The overparameterization just didn't really figure much as a
296640	299440	problem. I thought, well, with images, you just go and add some data augmentation, and it's going
299440	304560	to be okay. So where was any doubt coming from? The main doubt was can we train a bigger, really
304560	308160	have enough compute to train a big enough neural net with back propagation, back propagation,
308160	312320	I thought was would work. This image wasn't clear would was whether there would be enough compute
312320	316720	to get a very convincing result. And then at some point, Alex Krzewski wrote these insanely fast
316800	321120	CUDA kernels for training convolutional neural nets. Net was bam, let's do this. Let's get
321120	326160	image net and it's going to be the greatest thing. Was your intuition, most of your intuition from
326160	332560	empirical results by you and by others? So like just actually demonstrating that a piece of program
332560	339200	can train a 10 layer neural network? Or was there some pen and paper or marker and white board
339200	345360	thinking intuition? Because you just connected a 10 layer large neural network to the brain.
345360	349760	So you just mentioned the brain. So in your intuition about neural networks, does the human
349760	356080	brain come into play as a intuition builder? Definitely. I mean, you know, you got to be
356080	360480	precise with these analogies between neural artificial neural networks in the brain. But
361120	366480	there's no question that the brain is a huge source of intuition and inspiration for deep
366480	372320	learning researchers since all the way from Rosenblatt in the 60s. Like, if you look at the
372320	376640	whole idea of a neural network is directly inspired by the brain. You had people like
376640	382800	McCallum and Pitts who were saying, hey, you got these neurons in the brain. And hey, we recently
382800	386480	learned about the computer and automata. Can we use some ideas from the computer and automata to
386480	391920	design some kind of computational object that's going to be simple, computational and kind of
391920	396320	like the brain and invented the neuron. So they were inspired by it back then. Then you had the
396320	400960	convolutional neural network from Fukushima. And then later young Lacan, who said, Hey, if you
400960	405280	limit the receptive fields of a neural network, it's going to be especially suitable for images,
405280	411040	as it turned out to be true. So there was a very small number of examples where analogies to the
411040	416240	brain were successful. And I thought, well, probably an artificial neuron is not that different from
416240	421600	the brain if it's cleaned hard enough. So let's just assume it is and roll with it. So we're now
421600	428480	at a time where deep learning is very successful. So let us squint less and say, let's open our
428560	434080	eyes and say, what do you use an interesting difference between the human brain? Now, I know
434080	439600	you're probably not an expert, neither in your scientists and your biologists, but loosely speaking,
439600	443120	what's the difference between the human brain and artificial neural networks? That's interesting
443120	448720	to you for the next decade or two. That's a good question to ask. What is an interesting difference
448720	454000	between the neural between the brain and our artificial neural networks? So I feel like today,
454880	460000	artificial neural networks, so we all agree that there are certain dimensions in which the human
460000	465280	brain vastly outperforms our models. But I also think that there are some ways in which artificial
465280	471440	neural networks have a number of very important advantages over the brain. Looking at the advantages
471440	476720	versus disadvantages is a good way to figure out what is the important difference. So the brain
477360	481760	uses spikes, which may or may not be important. Yes, that's a really interesting question. Do you
481760	487200	think it's important or not? That's one big architectural difference between artificial
487200	493600	neural networks. It's hard to tell, but my prior is not very high. And I can say why. There are
493600	497920	people who are interested in spiking neural networks. And basically, what they figured out is
497920	503280	that they need to simulate the non-spiking neural networks in spikes. And that's how they're going
503280	507200	to make them work. If you don't simulate the non-spiking neural networks in spikes, it's not
507200	510560	going to work because the question is, why should it work? And that connects to questions around
510560	516800	back propagation and questions around deep learning. You've got this giant neural network.
516800	519840	Why should it work at all? Why should that learning rule work at all?
523120	527200	It's not a self-evident question, especially if you, let's say, if you were just starting in the
527200	532640	field and you read the very early papers, you can say, hey, people are saying, let's build neural
532640	536640	networks. That's a great idea because the brain is a neural network, so it would be useful to
536640	541360	build neural networks. Now, let's figure out how to train them. It should be possible to train
541360	549200	them probably, but how? And so the big idea is the cost function. That's the big idea. The cost
549200	554720	function is a way of measuring the performance of the system according to some measure.
554720	561280	By the way, that is a big, actually, let me think. Is that one, a difficult idea to arrive at? And
561280	569040	how big of an idea is that? That there's a single cost function? Sorry, let me take a pause. Is
569040	575760	supervised learning a difficult concept to come to? I don't know. All concepts are very easy in
575760	580720	retrospect. Yeah, that's what it seems trivial now. Because the reason I asked that, and we'll
580720	587040	talk about it, is there other things? Is there things that don't necessarily have a cost function,
587120	591680	maybe have many cost functions, or maybe have dynamic cost functions, or maybe
591680	595840	a totally different kind of architectures? Because we have to think like that in order to
595840	601040	arrive at something new, right? So the good examples of things which don't have clear cost
601040	607200	functions are GANs. And again, you have a game. So instead of thinking of a cost function,
608080	612000	where you want to optimize, where you know that you have an algorithm gradient descent,
612000	616240	which will optimize the cost function. And then you can reason about the behavior of your system
616240	621680	in terms of what it optimizes. With GAN, you say, I have a game, and I'll reason about the behavior
621680	625600	of the system in terms of the equilibrium of the game. But it's all about coming up with these
625600	630640	mathematical objects that help us reason about the behavior of our system. Right, that's really
630640	635600	interesting. Yeah, so GAN is the only one. It's kind of a, the cost function is emergent from the
635600	640160	comparison. I don't know if it has a cost function. I don't know if it's meaningful to talk about the
640160	644240	cost function of a GAN. It's kind of like the cost function of biological evolution or the cost
644240	651920	function of the economy. It's, you can talk about regions to which it will go towards, but I don't
651920	659440	think, I don't think the cost function analogy is the most useful. So evolution doesn't, that's
659440	664240	really interesting. So if evolution doesn't really have a cost function, like a cost function based
664240	671920	on it's something akin to our mathematical conception of a cost function, then do you think
671920	677440	cost functions in deep learning are holding us back? Yeah, I, so you just kind of mentioned
677440	683440	that cost function is a, is a nice first profound idea. Do you think that's a good idea? Do you
683440	690240	think it's an idea will go past? So self play starts to touch on that a little bit in reinforcement
690240	695840	learning systems. That's right. Self play and also ideas around exploration where you're trying to
695840	700800	take action. That's, that's surprise a predictor. I'm a big fan of cost functions. I think cost
700800	704560	functions are great and they serve us really well. And I think that whenever we can do things with
704560	710240	cost functions, we should. And you know, maybe there is a chance that we will come up with some
710240	715440	yet another profound way of looking at things that will involve cost functions in a less central way.
715440	717280	But I don't know, I think cost functions are, I mean,
719840	724640	I would not bet against against cost functions. Is there other things about the brain
725360	730240	that pop into your mind that might be different and interesting for us to consider
730960	734880	in designing artificial neural networks? So we talked about spiking a little bit.
736080	739280	I mean, one, one thing which may potentially be useful, I think people,
739280	742320	neuroscientists figured out something about the learning rule of the brain or
742880	746480	talking about spike time independent plasticity. And it would be nice if some people were to
746480	751200	study that in simulation. Wait, sorry, spike time independent plasticity. Yeah, that's
751680	757360	STD. It's a particular learning rule that uses spike timing to figure out how to determine how
757360	763280	to update the synapses. So it's kind of like, if a synapse fires into the neuron before the neuron
763280	768320	fires, then it's strengthened the synapse. And if the synapse fires into the neurons shortly
768320	772960	after the neuron fired, then it becomes the synapse something along this line. I'm 90%
772960	778400	sure it's right. So if I said something wrong here, don't don't get too angry.
779280	783440	But you sounded brilliant while saying it. But the timing, that's one thing that's missing.
784160	790000	The temporal dynamics is not captured. I think that's like a fundamental property of the brain
790000	794480	is the timing of the signals. Well, you're recording your networks.
795360	801920	But you think of that as, I mean, that's a very crude simplified, what's that called?
802640	809600	There's a clock, I guess, to recurrent neural networks. This seems like the brain is the
809600	815440	general, the continuous version of that, the generalization where all possible timings are
815440	821200	possible. And then within those timings, this contains some information. You think recurrent
821200	827520	neural networks, the recurrence in recurrent neural networks can capture the same kind of
827600	835360	phenomena as the timing that seems to be important for the brain in the firing of neurons in the
835360	841920	brain? I mean, I think recurrent neural networks are amazing and they can do,
842560	848000	I think they can do anything we'd want them to, we'd want a system to do. Right now,
848000	852000	recurrent neural networks have been superseded by transformers, but maybe one day they'll make
852000	858240	a comeback, maybe they'll be back, we'll see. Let me in a small tangent say, do you think they'll
858240	864240	be back? So so much of the breakthroughs recently that we'll talk about on natural language processing
864240	871120	and language modeling has been with transformers that don't emphasize recurrence. Do you think
871120	875920	recurrence will make a comeback? Well, some kind of recurrence, I think very likely.
876800	882960	Recurrent neural networks, as they're typically thought of for processing sequences, I think it's
882960	889120	also possible. What is, to you, a recurrent neural network? In general speaking, I guess,
889120	893520	what is a recurrent neural network? You have a neural network which maintains a high dimensional
893520	899200	hidden state. And then when an observation arrives, it updates its high dimensional hidden state
899760	906960	through its connections in some way. So do you think, you know, that's what like expert systems
906960	917040	did, right? Symbolic AI, the knowledge based, growing a knowledge base is maintaining a hidden state,
917040	921040	which is its knowledge base and is growing it by sequential processing. Do you think of it more
921120	929920	generally in that way? Or is it simply, is it the more constrained form of a hidden state with
929920	934960	certain kind of gating units that we think of as today with LSTMs and that? I mean, the hidden
934960	939280	state is technically what you described there, the hidden state that goes inside the LSTM or
939280	943600	there are an N or something like this. But then what should be contained, you know, if you want to
943600	950000	make the expert system and analogy, I'm not, I mean, you could say that the knowledge is stored
950080	955120	in the connections and then the short term processing is done in the hidden state.
956240	962800	Yes. Could you say that? So sort of, do you think there's a future of building large scale
963440	966080	knowledge bases within the neural networks? Definitely.
968960	974160	So we're going to pause on that confidence because I want to explore that. Well, let me zoom back out
974160	981360	and ask back to the history of ImageNet. Neural networks have been around for many decades as
981360	987280	you mentioned. What do you think were the key ideas that led to their success, that ImageNet moment
987280	994240	and beyond the success in the past 10 years? Okay. So the question is to make sure I didn't
994240	999280	miss anything, the key ideas that led to the success of deep learning over the past 10 years.
999360	1004800	Exactly. Even though the fundamental thing behind deep learning has been around for much longer.
1005360	1015600	So the key idea about deep learning, or rather the key fact about deep learning before deep
1015600	1022480	learning started to be successful is that it was underestimated. People who worked in machine
1022480	1028080	learning simply didn't think that neural networks could do much. People didn't believe that large
1028080	1033840	neural networks could be trained. People thought that, well, there was lots of, there was a lot of
1033840	1038720	debate going on in machine learning about what are the right methods and so on. And people were
1038720	1043760	arguing because there were no, there were no, there was no way to get hard facts. And by that,
1043760	1048480	I mean, there were no benchmarks which were truly hard, that if you do really well on them, then
1048480	1056640	you can say, look, here's my system. That's when you switch from, that's when this field becomes
1056640	1060480	a little bit more of an engineering field. So in terms of deep learning to answer the question
1060480	1067040	directly, the ideas were all there. The thing that was missing was a lot of supervised data and a lot
1067040	1073200	of compute. Once you have a lot of supervised data and a lot of compute, then there is a third
1073200	1077680	thing which is needed as well. And that is conviction, conviction that if you take
1078320	1083360	the right stuff, which already exists, and apply and mixed with a lot of data and a lot of compute,
1083440	1089440	that it will in fact work. And so that was the missing piece. It was you had the, you needed
1089440	1095040	the data, you needed the compute which showed up in terms of GPUs, and you needed the conviction
1095040	1101920	to realize that you need to mix them together. So that's really interesting. So I guess the
1101920	1108880	presence of compute and the presence supervised data allowed the empirical evidence to do the
1108960	1113120	convincing of the majority of the computer science community. So I guess there's a
1113120	1122720	key moment with Jitendra Malik and Alex, Alyosha Efros, who were very skeptical, right? And then
1122720	1128160	there's a Jeffrey Hinton that was the opposite of skeptical. And there was a convincing moment.
1128160	1133840	And I think ImageNet served as that moment. And that represented this kind of, or the big
1133840	1140480	pillars of computer vision community, kind of the wizards got together. And then all of a sudden
1140480	1146240	there was a shift. And it's not enough for the ideas to all be there and the computer to be there.
1146240	1153040	It's for it to convince the cynicism that existed that that's interesting that people just didn't
1153040	1160560	believe for a couple of decades. Yeah, well, but it's more than that. It's kind of when put this
1160560	1165440	way, it sounds like, well, you know, those silly people who didn't believe what were they missing.
1165440	1170080	But in reality, things were confusing because neural networks really did not work on anything.
1170080	1175520	And they were not the best method on pretty much anything as well. And it was pretty rational to
1175520	1181680	say, yeah, this stuff doesn't have any traction. And that's why you need to have these very hard
1181680	1187280	tasks which are which produce undeniable evidence. And that's how we make progress. And that's why
1187280	1191600	the field is making progress today, because we have these hard benchmarks, which represent true
1191600	1199840	progress. And so, and this is why we were able to avoid endless debate. So incredibly, you've
1199840	1205920	contributed some of the biggest recent ideas in AI in computer vision, language, natural
1205920	1212080	language processing, reinforcement learning, sort of everything in between, maybe not GANs.
1212800	1217840	Is there, there may not be a topic you haven't touched. And of course, the fundamental science
1217840	1225600	of deep learning. What is the difference to you between vision, language, and as in reinforcement
1225600	1230400	learning action, as learning problems? And what are the commonalities? Do you see them as all
1230400	1236320	interconnected? Are they fundamentally different domains that require different approaches?
1236880	1242640	Okay, that's a good question. Machine learning is a field with a lot of unity, a huge amount of
1242640	1249680	unity. In fact, what do you mean by unity, like overlap of ideas? overlap of ideas overlap of
1249680	1254320	principles. In fact, there's only one or two or three principles, which are very, very simple.
1254320	1259840	And then they apply in almost the same way, in almost the same way to the different modalities
1259840	1264720	through the different problems. And that's why today, when someone writes a paper on improving
1264720	1269200	optimization of deep learning and vision, it improves the different NLP applications,
1269200	1273200	and it improves the different reinforcement learning applications. Reinforcement learning.
1273200	1280000	So I would say that computer vision and NLP are very similar to each other. Today, they differ in
1280000	1284640	that they have slightly different architectures. We use transformers in NLP, and we use convolutional
1284640	1289760	neural networks in vision. But it's also possible that one day this will change and everything
1289760	1293840	will be unified with a single architecture. Because if you go back a few years ago in
1293840	1299760	natural language processing, there were a huge, huge number of architectures for every
1299760	1306320	different tiny problem had its own architecture. Today, there's just one transformer for all
1306320	1310560	those different tasks. And if you go back in time even more, you had even more and more
1310560	1316320	fragmentation and every little problem in AI had its own little subspecialization and sub,
1316320	1320800	you know, little set of collection of skills, people who would know how to engineer the features.
1320880	1325040	Now it's all been subsumed by deep learning. We have this unification. And so I expect
1325680	1329520	vision to become unified with natural language as well. Or rather, I shouldn't say expect,
1329520	1333360	I think it's possible. I don't want to be too sure, because I think on the commercial
1333360	1337280	neural network, it is very computationally efficient. Arell is different. Arell doesn't
1337280	1341360	require slightly different techniques, because you really do need to take action. You really
1341360	1346400	need to do something about exploration, your variance is much higher. But I think there
1346400	1350240	is a lot of unity even there. And I would expect, for example, that at some point, there will be
1350240	1356080	some broader unification between Arell and supervised learning, where somehow the Arell
1356080	1360640	will be making decisions to make the supervised learning go better. And it will be, I imagine
1360640	1365120	one big black box and you just throw every, you know, you shovel, shovel things into it. And it
1365120	1369840	just figures out what to do with whatever you shovel in it. I mean, reinforcement learning has
1369840	1377600	some aspects of language and vision combined, almost, there's elements of a long term memory
1377600	1382240	that you should be utilizing, and there's elements of a really rich sensory space.
1382960	1389040	So it seems like the, it's like the union of the two or something like that. I'd say something
1389040	1394720	slightly different. I'd say that reinforcement learning is neither, but it naturally interfaces
1394720	1399600	and integrates with the two of them. Do you think action is fundamentally different? So yeah,
1399600	1406800	what is interesting about, what is unique about policy of learning to act? Well, so one example,
1406800	1412480	for instance, is that when you learn to act, you are fundamentally in a non stationary world.
1413120	1420480	Because as your actions change, the things you see start changing. You, you experience the world
1420480	1425040	in a different way. And this is not the case for the more traditional static problem where you have
1425040	1430320	a some distribution and you just apply a model to that distribution. You think it's a fundamentally
1430320	1435520	different problem or is it just a more difficult general, it's a generalization of the problem
1435520	1440160	of understanding. I mean, it's, it's, it's a question of definitions almost. There is a huge
1440160	1444800	amount of commonality for sure. You take gradients, you try, you take gradients, we try to approximate
1444800	1448560	gradients in both cases. In some case, in the case of reinforcement learning, you have
1448560	1453760	some tools to reduce the variance of the gradients. You do that. There's lots of commonality,
1453760	1458320	use the same neural net in both cases. You compute the gradient, you apply atom in both cases.
1458400	1467520	So, I mean, there's lots in common for sure, but there are some small differences which are not
1467520	1471360	completely insignificant. It's really just a matter of your, of your point of view, what
1471360	1477120	frame of reference you, what, how much do you want to zoom in or out as you look at these problems?
1477120	1482720	Which problem do you think is harder? So people like no Chomsky believe that language is fundamental
1482800	1489200	to everything. So it underlies everything. Do you think language understanding is harder than
1489200	1494640	visual scene understanding or vice versa? I think that asking if a problem is hard is
1494640	1498480	slightly wrong. I think the question is a little bit wrong, and I want to explain why.
1499440	1502160	So what does it mean for a problem to be hard?
1504240	1510560	Okay, the non-interesting, dumb answer to that is there's a, there's a benchmark,
1510560	1516720	and there's a human level performance on that benchmark. And how is the effort required to
1516720	1522000	reach the human level benchmark? So from the perspective of how much until we get to human
1522000	1528720	level on a very good benchmark? Yeah, like some, I understand what you mean by that.
1528720	1533040	So what I was going to say that a lot of it depends on, you know, once you solve a problem,
1533040	1537680	it stops being hard. And that's, that's always true. And so, but if something is hard or not,
1537680	1542800	depends on what our tools can do today. So you know, you say today, through human level,
1543600	1549040	language understanding and visual perception are hard in the sense that there is no way of
1549040	1553040	solving the problem completely in the next three months. Right. So I agree with that statement.
1553840	1557440	Beyond that, I'm just, I'll be my, my guess would be as good as yours. I don't know.
1557440	1562720	Oh, okay. So you don't have a fundamental intuition about how hard language understanding is.
1562720	1567040	I think I, I know I changed my mind. I'd say language is probably going to be harder. I mean,
1567040	1572880	it depends on how you define it. Like if you mean absolute top notch 100% language understanding,
1572880	1579200	I'll go with language. So, but then if I show you a piece of paper with letters on it, is that
1579920	1584960	you see what I mean? So you have a vision system, you say it's the best human level vision system.
1584960	1590560	I show you, I open a book and I show you letters. Will it understand how these letters form into
1590560	1594560	word and sentences and meaning is this part of the vision problem? Where does vision end and
1594560	1599280	language begin? Yeah. So Chomsky would say it starts at language. So vision is just a little
1599280	1606880	example of the kind of structure and, you know, fundamental hierarchy of ideas that's already
1606880	1615360	represented in our brain. Somehow that's represented through language. But where does vision stop and
1615360	1629280	language begin? That's a really interesting question. So one possibility is that it's
1629280	1636240	impossible to achieve really deep understanding in either images or language without basically
1636240	1642000	using the same kind of system. So you're going to get the other for free. I think, I think it's
1642000	1646560	pretty likely that yes, if we can get one, our machine learning is probably that good that we
1646560	1654080	can get the other. But it's not 100. I'm not 100% sure. And also, I think a lot of it really does
1654080	1662400	depend on your definitions. Definitions of like perfect vision. Because reading is vision, but
1662400	1669520	should it count? Yeah, to me, my definition is if a system looked at an image and then a system
1670480	1677040	looked at a piece of text, and then told me something about that. And I was really impressed.
1678240	1682080	That's relative. You'll be impressed for half an hour. And then you're going to say, well,
1682080	1686160	I mean, all the systems do that. But here's the thing they don't do. Yeah, but I don't have that
1686160	1693200	with humans. Humans continue to impress me. Is that true? Well, the ones, okay, so I'm a fan of
1693200	1698960	monogamy. So I like the idea of marrying somebody being with them for several decades. So I believe
1698960	1705040	in the fact that, yes, it's possible to have somebody continuously giving you pleasurable,
1705760	1711120	interesting, witty, new ideas, friends. Yeah, I think so. They continue to surprise you.
1712000	1724480	The surprise, it's that injection of randomness seems to be a nice source of continued
1725120	1734800	inspiration, like the wit, the humor. I think, yeah, that would be, it's a very subjective test,
1734800	1740640	but I think if you have enough humans in the room. Yeah, I understand what you mean. Yeah,
1740640	1743840	I feel like I misunderstood what you meant by impressing you. I thought you meant to
1743840	1750800	impress you with its intelligence, with how valid understands an image. I thought you meant
1750800	1753840	something like, I'm going to show you a really complicated image and it's going to get it right,
1753920	1759440	and you're going to say, wow, that's really cool, the systems of January 2020 have not been doing
1759440	1766000	that. Yeah, I think it all boils down to the reason people click like on stuff on the internet,
1766000	1774320	which is like it makes them laugh. So it's like humor or wit or insight. I'm sure we'll get that
1774320	1781920	as well. So forgive the romanticized question, but looking back to you, what is the most beautiful
1781920	1787600	or surprising idea in deep learning or AI in general you've come across? So I think the most
1787600	1792560	beautiful thing about deep learning is that it actually works. And I mean it because you got
1792560	1796240	these ideas, you got a little neural network, you got the back propagation algorithm.
1798800	1802480	And then you got some theories as to, you know, this is kind of like the brain. So maybe if you
1802480	1806320	make it large, if you make the neural network large and you train a lot of data, then it will
1807840	1811280	do the same function that the brain does. And it turns out to be true. That's crazy.
1812400	1815920	And now we just train these neural networks and you make them larger and they keep getting better.
1816560	1821280	And I find it unbelievable. I find it unbelievable that this whole AI stuff with neural networks
1821280	1827840	works. Have you built up an intuition of why are there a little bits and pieces of intuitions
1827840	1833760	of insights of why this whole thing works? I mean, some definitely. Well, we know that
1833840	1840800	optimization, we now have good, you know, we've had lots of empirical, you know,
1840800	1844960	huge amounts of empirical reasons to believe that optimization should work on all most
1844960	1850720	problems we care about. Do you have insights of what, so you just said empirical evidence.
1850720	1860320	Is most of your sort of empirical evidence kind of convinces you, it's like evolution is empirical,
1860320	1864480	it shows you that look, this evolutionary process seems to be a good way to design
1865760	1871760	organisms that survive in their environment. But it doesn't really get you to the insights of how
1872640	1877520	the whole thing works. I think it's a good analogy is physics. You know how you say, hey,
1877520	1881600	let's do some physics calculation and come up with some new physics theory and make some prediction.
1881600	1885920	But then you got around the experiment. You know, you got around the experiment, it's important.
1885920	1890080	So it's a bit the same here, except that maybe sometimes the experiment came before
1890080	1894480	the theory. But it still is the case, you know, you have some data and you come up with some
1894480	1897840	prediction, you say, yeah, let's make a big neural network, let's train it, and it's going to work
1898400	1901840	much better than anything before it. And it will in fact continue to get better as you make it
1901840	1906880	larger. And it turns out to be true. That's, that's amazing when a theory is validated like this,
1906880	1910560	you know, it's not a mathematical theory, it's more of a biological theory almost.
1911600	1915840	So I think there are not terrible analogies between deep learning and biology. I would say
1915840	1919360	it's like the geometric mean of biology and physics, that's deep learning.
1920080	1925360	The geometric mean of biology and physics. I think I'm going to need a few hours to wrap
1925360	1935360	my head around that. Because just to find the geometric, just to find the set of what biology
1935360	1940240	represents. Well, biology, in biology, things are really complicated. The theories are really,
1940240	1944320	really, it's really hard to have good predictive theory. And if in physics, the theories are too
1944320	1948560	good. In theory, in physics, people make these super precise theories, which make these amazing
1948560	1953440	predictions. And in machine learning, we're kind of in between. Kind of in between. But it'd be
1953440	1958400	nice if machine learning somehow helped us discover the unification of the two as opposed to server
1958400	1965680	the in between. But you're right, that's, you're kind of trying to juggle both. So do you think
1965680	1969440	there are still beautiful and mysterious properties in your networks that are yet to be
1969440	1974000	discovered? Definitely. I think that we are still massively underestimating deep learning.
1975280	1981360	What do you think it will look like? Like what if I knew I would have done it? So,
1982560	1986240	but if you look at all the progress from the past 10 years, I would say most of it,
1986960	1992080	I would say there have been a few cases where some were things that felt like really new ideas
1992080	1997120	showed up. But by and large, it was every year, we thought, okay, deep learning goes this far.
1997120	2001680	Nope, it actually goes further. And then the next year, okay, now you know, this is this is
2001680	2005440	big deep learning, we are really done. Nope, it goes further, it just keeps going further each
2005440	2010240	year. So that means that we keep underestimating, we keep not understanding it as surprising properties
2010240	2015120	all the time. You think it's getting harder and harder to make progress, need to make progress?
2015840	2019840	It depends on what we mean. I think the field will continue to make very robust progress
2019840	2024320	for quite a while. I think for individual researchers, especially people who are doing
2025040	2029120	research, it can be harder because there is a very large number of researchers right now.
2030000	2034080	I think that if you have a lot of compute, then you can make a lot of very interesting
2034080	2040160	discoveries, but then you have to deal with the challenge of managing a huge computer,
2040160	2043200	a huge class, a huge computer cluster to run your experiments. It's a little bit harder.
2043200	2047760	So I'm asking all these questions that nobody knows the answer to, but you're one of the smartest
2047760	2052800	people I know, so I'm going to keep asking. So let's imagine all the breakthroughs that happen
2052800	2058000	in the next 30 years in deep learning. Do you think most of those breakthroughs can be done by
2058000	2068320	one person with one computer? In the space of breakthroughs, do you think compute and
2068320	2075760	large efforts will be necessary? I mean, I can't be sure. When you say one computer, you mean how
2075760	2085840	large? You're clever. I mean, one GPU. I see. I think it's pretty unlikely.
2086800	2093040	I think it's pretty unlikely. I think that the stack of deep learning is starting to be quite
2093040	2101440	deep. If you look at it, you've got all the way from the ideas, the systems to build the datasets,
2102000	2107360	the distributed programming, the building the actual cluster, the GPU programming,
2108080	2113120	putting it all together. So the stack is getting really deep, and I think it can be quite hard
2113120	2118320	for a single person to become to be world-class in every single layer of the stack. What about
2119920	2125840	Vladimir Vapnik really insists on is taking MNIST and trying to learn from very few examples,
2125840	2132000	so being able to learn more efficiently. Do you think there'll be breakthroughs in that space
2132000	2138320	that may not need a huge compute? I think there will be a large number of breakthroughs in general
2138320	2142960	that will not need a huge amount of compute. So maybe I should clarify that. I think that
2142960	2148240	some breakthroughs will require a lot of compute, and I think building systems which actually do
2148240	2153440	things will require a huge amount of compute. That one is pretty obvious. If you want to do X,
2153440	2158240	and X requires a huge neural net, you've got to get a huge neural net, but I think there will be
2158240	2163760	lots of, I think there is lots of room for very important work being done by small groups and
2163760	2170480	individuals. Can you maybe sort of on the topic of the science of deep learning, talk about one
2170480	2177200	of the recent papers that you've released, the Deep Double Descent, where bigger models and more
2177200	2182720	data hurt. I think it's a really interesting paper. Can you describe the main idea? Yeah,
2182720	2188400	definitely. So what happened is that some over the years, some small number of researchers
2188400	2192080	noticed that it is kind of weird that when you make the neural net work larger, it works better,
2192080	2195760	and it seems to go in contradiction with statistical ideas. And then some people made
2195760	2200080	an analysis showing that actually you got this double descent bump. And what we've done was to
2200080	2205600	show that double descent occurs for pretty much all practical deep learning systems.
2206320	2214480	And that it'll be also, so can you step back? What's the X axis and the Y axis of a double
2214480	2223280	descent plot? Okay, great. So you can look, you can do things like you can take your neural
2223280	2228720	network, and you can start increasing its size slowly, while keeping your data set fixed.
2230000	2236800	So if you increase the size of the neural network slowly, and if you don't do early stopping,
2236800	2243360	that's a pretty important detail. Then when the neural network is really small, you make it larger,
2243440	2247520	you get a very rapid increase in performance. Then you continue to make it larger. And at some
2247520	2254000	point performance, you'll get worse. And it gets and it gets the worst exactly at the point at
2254000	2259280	which it achieves zero training error, precisely zero training loss. And then as you make it
2259280	2263600	larger, it starts to get better again. And it's kind of counterintuitive because you'd expect
2263600	2271280	deep learning phenomena to be monotonic. And it's hard to be sure what it means, but it also occurs
2271360	2275360	in the case of linear classifiers. And the intuition basically boils down to the following.
2276960	2284000	When you, when you have a lot, when you have a large data set, and a small model, then small,
2284000	2291440	tiny random. So basically, what is overfitting? Overfitting is when your model is somehow very
2291440	2297200	sensitive to the small random, unimportant stuff in your data set in the training data in the
2297200	2302400	training data set precisely. So if you have a small model, and you have a big data set,
2303280	2307360	and there may be some random thing, you know, some training cases are randomly in the data set,
2307360	2311680	and others may not be there. But the small model, but the small model is kind of insensitive to
2311680	2316960	this randomness, because it's the same, you there is pretty much no uncertainty about the model
2316960	2322400	when the data set is large. So okay, so at the very basic level, to me, it is the most surprising
2322400	2333280	thing that neural networks don't overfit every time very quickly. Before ever being able to learn
2333280	2339280	anything, the huge number of parameters. So here is so there is one way Okay, so maybe so let me try
2339280	2343520	to give the explanation and maybe that will be that will work. So you got a huge neural network,
2343520	2348640	let's suppose you got a your you have a huge neural network, you have a huge number of parameters.
2349600	2353840	And now let's pretend everything is linear, which is not let's just pretend. Then there is this big
2353840	2361040	subspace, where a neural network achieves zero error. And SGT is going to find approximately
2361040	2365120	that's right, approximately the point with the smallest norm in that subspace.
2367040	2373600	And that can also be proven to be insensitive to the small randomness in the data, when the
2373600	2378480	dimensionality is high. But when the dimensionality of the data is equal to the dimensionality of
2378480	2384400	the model, then there is a one to one correspondence between all the data sets and the models. So
2384400	2387600	small changes in the data set actually lead to large changes in the model and that's why
2387600	2392880	performance gets worse. So this is the best explanation more or less. So then it would be
2392880	2398960	good for the model to have more parameters sort of to be bigger than the data. That's right,
2398960	2402720	but only if you don't early stop. If you introduce early stop in your regularization,
2402720	2407280	you can make a double as a descent pump almost completely disappear. What is early stop early
2407280	2412480	stopping is when you train your model, and you monitor your validation performance.
2413440	2416560	And then if at some point validation performance starts to get worse, you say, okay, let's stop
2416560	2423040	training. We are good. We are good. We are good enough. So the magic happens after that moment.
2423040	2426560	So you don't want to do the early stopping. Well, if you don't do the early stopping,
2426560	2431920	you get this very, you get the very pronounced double descent. Do you have any intuition why
2431920	2436960	this happens? Double descent or sorry, are you stopping? No, the double descent. So the
2436960	2443680	well, yeah, so I try it. Let's see the intuition is basically is this that when the data set has
2443680	2449680	as many degrees of freedom as the model, then there is a one to one correspondence between them.
2449680	2455520	And so small changes to the data set lead to noticeable changes in the model. So your model
2455520	2461360	is very sensitive to all the randomness. It is unable to discard it. Whereas it turns out that
2461360	2467120	when you have a lot more data than parameters, or a lot more parameters than data, the resulting
2467120	2474160	solution will be insensitive to small changes in the data set. So it's able to nicely put discard
2474160	2479840	the small changes, the randomness. Exactly. The spurious correlations which you don't want.
2480480	2484480	Jeff Hinton suggested we need to throw back propagation. We already kind of talked about
2484480	2488880	this a little bit, but he suggested we need to throw away back propagation and start over.
2489680	2491920	I mean, of course, some of that is a little bit
2493760	2498640	wit and humor. But what do you think? What could be an alternative method of training neural
2498640	2503520	networks? Well, the thing that he said precisely is that to the extent that you can't find back
2503520	2508960	propagation in the brain, it's worth seeing if we can learn something from how the brain learns.
2508960	2513440	But back propagation is very useful and we should keep using it. Oh, you're saying that
2513440	2518000	once we discover the mechanism of learning in the brain or any aspects of that mechanism,
2518000	2522160	we should also try to implement that in your networks. If it turns out that you can't find
2522160	2525920	back propagation in the brain. If we can't find back propagation in the brain.
2528000	2534960	Well, so I guess your answer to that is back propagation is pretty damn useful. So why are
2534960	2538960	we complaining? I mean, I personally am a big fan of back propagation. I think it's a great
2538960	2545280	algorithm because it solves an extremely fundamental problem which is finding a neural
2545280	2551200	circuit subject to some constraints. I don't see that problem going away. So that's why I
2552560	2556480	really, I think it's pretty unlikely that we'll have anything which is going to be
2557280	2561200	dramatically different. It could happen. But I wouldn't bet on it right now.
2563200	2571040	So let me ask a sort of big picture question. Do you think neural networks can be made to reason?
2571600	2576560	Why not? Well, if you look, for example, at AlphaGo or AlphaZero,
2578160	2584320	the neural network of AlphaZero plays Go, which we all agree is a game that requires reasoning,
2585120	2591040	better than 99.9% of all humans. Just the neural network without the search, just the neural network
2591040	2596320	itself. Doesn't that give us an existence proof that neural networks can reason?
2596560	2601920	To push back and disagree a little bit, we all agree that Go is reasoning.
2603200	2608640	I think I agree. I don't think it's a trivial. So obviously, reasoning like intelligence is
2609920	2616640	a loose gray area term a little bit. Maybe you disagree with that. But yes, I think it has some
2616640	2624560	of the same elements of reasoning. Reasoning is almost akin to search. There's a sequential element
2625200	2634160	of stepwise consideration of possibilities and sort of building on top of those possibilities
2634160	2639760	in a sequential manner until you arrive at some insight. So yeah, I guess playing Go is kind of
2639760	2644640	like that. And when you have a single neural network doing that without search, that's kind of like
2644640	2650080	that. So there's an existence proof in a particular constrained environment that a process akin to
2650880	2658720	what many people call reasoning exists. But more general kind of reasoning. So off the board.
2658720	2662720	There is one other existence proof. Oh boy, which one? Us humans?
2662720	2672240	Yes. Okay. All right. So do you think the architecture that will allow neural
2672240	2677920	networks to reason will look similar to the neural network architectures we have today?
2678720	2683360	I think it will. I think, well, I don't want to make two overly definitive statements.
2683920	2689440	I think it's definitely possible that the neural networks that will produce the reasoning breakthroughs
2689440	2694480	of the future will be very similar to the architecture that exists today, maybe a little
2694480	2702800	bit more recurrent, maybe a little bit deeper. But these neural nets are so insanely powerful.
2702880	2708400	Why wouldn't they be able to learn to reason? Humans can reason. So why can't neural networks?
2709200	2714480	So do you think the kind of stuff we've seen neural networks do is a kind of just weak reasoning?
2714480	2718400	So it's not a fundamentally different process. Again, this is stuff we don't nobody knows the
2718400	2724240	answer to. So when it comes to our neural networks, I would think which I would say
2724240	2730720	is that neural networks are capable of reasoning. But if you train a neural network on a task which
2730720	2735520	doesn't require reasoning, it's not going to reason. This is a well-known effect where the
2735520	2743040	neural network will solve exactly the problem that you pose in front of it in the easiest way possible.
2744400	2753120	Right. That takes us to one of the brilliant ways you describe neural networks, which is
2754080	2757040	you've referred to neural networks as the search for small circuits
2757920	2763040	and maybe general intelligence as the search for small programs,
2764400	2768640	which I found is a metaphor very compelling. Can you elaborate on that difference?
2769200	2777520	Yeah. So the thing which I said precisely was that if you can find the shortest program that
2777520	2783520	outputs the data in your disposal, then you will be able to use it to make the best prediction
2783520	2789680	possible. And that's a theoretical statement which can be proved mathematically. Now,
2789680	2794800	you can also prove mathematically that it is that finding the shortest program which generates
2794800	2801600	some data is not a computable operation. No finite amount of compute can do this.
2802560	2808720	So then with neural networks, neural networks are the next best thing that actually works in
2808720	2814560	practice. We are not able to find the best, the shortest program which generates our data,
2815600	2820880	but we are able to find a small, but now that statement should be amended,
2821440	2824480	even a large circuit which fits our data in some way.
2825120	2829840	Well, I think what you meant by the small circuit is the smallest needed circuit.
2829920	2834640	Well, the thing which I would change now, back then I really haven't fully internalized
2834640	2839600	the overparameterized results. The things we know about overparameterized neural
2839600	2846640	nets, now I would phrase it as a large circuit whose weights contain a small amount of information,
2847600	2851680	which I think is what's going on. If you imagine the training process of a neural network as you
2851680	2859680	slowly transmit entropy from the data set to the parameters, then somehow the amount of information
2859680	2864560	in the weights ends up being not very large, which would explain whether generalized so well.
2865200	2871840	So that's the large circuit might be one that's helpful for the generalization.
2871840	2872640	Yeah, something like this.
2874720	2882320	But do you see it important to be able to try to learn something like programs?
2882320	2888960	I mean, if we can, definitely. I think the answer is kind of yes, if we can do it.
2888960	2893760	We should do things that we can do it. It's the reason we are pushing on deep learning.
2895200	2900000	The fundamental reason, the root cause is that we are able to train them.
2901360	2906720	So in other words, training comes first. We've got our pillar, which is the training pillar.
2907440	2911040	And now we are trying to contort our neural networks around the training pillar. We got
2911040	2916800	to stay trainable. This is an invariant we cannot violate. And so
2917040	2921520	being trainable means starting from scratch, knowing nothing, you can actually pretty quickly
2921520	2927520	converge towards knowing a lot or even slowly. But it means that given the resources at your
2927520	2934240	disposal, you can train the neural net and get it to achieve useful performance.
2934240	2937920	Yeah, that's a pillar we can't move away from. That's right. Because if you can, whereas if you
2937920	2943040	say, Hey, let's find the shortest program, we can't do that. So it doesn't matter how useful
2944000	2947600	that would be. We can do it. So we want.
2947600	2951520	So do you think you kind of mentioned that the neural networks are good at finding small
2951520	2958560	circuits or large circuits? Do you think then the matter of finding small programs is just the data?
2958560	2959040	No.
2959040	2968240	So the, sorry, not the size or the quality, the type of data sort of ask giving it programs.
2968880	2974880	Well, I think the thing is that right now, finding there are no good precedents of people
2974880	2981920	successfully finding programs really well. And so the way you'd find programs is you'd train a
2981920	2987440	deep neural network to do it basically. But which is the right way to go about it.
2988000	2994000	But there's not good illustrations that hasn't been done yet. But in principle, it should be possible.
2994080	3001200	Can you elaborate a little bit? What's your insight in principle? And put another way,
3001200	3007520	you don't see why it's not possible. Well, it's kind of like more, it's more a statement of,
3009360	3013440	I think that it's, I think that it's unwise to bet against deep learning. And
3014960	3018880	if it's a fun, if it's a cognitive function that humans seem to be able to do, then
3019200	3024320	it doesn't take too long for some deep neural net to pop up that can do it too.
3025680	3032880	Yeah, I'm there with you. I can, I've stopped betting against neural networks at this point
3032880	3038320	because they continue to surprise us. What about long term memory? Can neural networks have long
3038320	3045360	term memory or something like knowledge basis? So being able to aggregate important information
3045440	3053200	over long periods of time, that would then serve as useful sort of representations of
3054000	3060480	state that you can make decisions by so have a long term context based on what you make in the
3060480	3067600	decision. So in some sense, the parameters already do that. The parameters are an aggregation of the
3067600	3072080	day of the neural of the entirety of the neural net experience. And so they count as the long
3072560	3079040	form, long term knowledge. And people have trained various neural nets to act as knowledge basis
3079040	3083200	and, you know, investigated with invest, people have investigated language models as knowledge
3083200	3089760	basis. So there is work, there is work there. Yeah, but in some sense, do you think in every sense,
3089760	3097200	do you think there's a, it's all just a matter of coming up with a better mechanism of forgetting
3097200	3102400	the useless stuff and remembering the useful stuff? Because right now, I mean, there's not been
3102400	3107920	mechanisms that do remember really long term information. What do you mean by that precisely?
3108880	3112000	Precisely. I like the word precisely. So
3114720	3119840	I'm thinking of the kind of compression of information the knowledge basis represent.
3120400	3127120	Sort of creating a, now I apologize for my sort of human centric thinking about
3127680	3133440	what knowledge is because neural networks aren't interpretable necessarily with the kind of
3133440	3139680	knowledge they have discovered. But a good example for me is knowledge basis being able to build up
3139680	3145840	over time something like the knowledge that Wikipedia represents. It's a really compressed,
3145840	3153600	structured knowledge base. Obviously, not the actual Wikipedia or the language,
3154160	3159200	but like a semantic web, the dream that semantic web represented. So it's a really nice compressed
3159200	3166320	knowledge base or something akin to that in the noninterpretable sense as neural networks would
3166320	3170080	have. Well, the neural networks would be noninterpretable if you look at their rates, but
3170080	3173680	their outputs should be very interpretable. Okay, so yeah, how do you, how do you make
3174480	3177360	very smart neural networks like language models interpretable?
3178000	3182000	Well, you ask them to generate some text and the text will generally be interpretable.
3182000	3187840	Do you find that the epitome of interpretability, like can you do better? Like can you, because
3187840	3193440	you can't, okay, I'd like to know what does it know and what doesn't know. I would like the neural
3193440	3199200	network to come up with examples where it's completely dumb and examples where it's completely
3199200	3204960	brilliant. And the only way I know how to do that now is to generate a lot of examples and use my
3204960	3211040	human judgment. But it would be nice if the neural network had some self-awareness about
3211600	3216880	100%. I'm a big believer in self-awareness. And I think that I think, I think
3218720	3223600	neural net self-awareness will allow for things like the capabilities, like the ones you describe,
3223600	3228240	like for them to know what they know and what they don't know. And for them to know where to
3228240	3232160	invest, to increase their skills most optimally. And to your question of interpretability,
3232160	3236320	there are actually two answers to that question. One answer is, you know, we have the neural net,
3236320	3240560	so we can analyze the neurons and we can try to understand what the different neurons and
3240560	3244960	different layers mean. And you can actually do that. And OpenAI has done some work on that.
3245760	3251360	But there is a different answer, which is that I would say that's the human centric answer where
3251360	3258080	you say, you know, you look at a human being, you can't read. How do you know what a human
3258080	3260800	being is thinking? You ask them, you say, Hey, what do you think about this? What do you think
3260800	3266320	about that? And you get some answers. The answers you get are sticky. In the sense, you already
3266320	3273360	have a mental model. You already have a mental model of that human being. You already have an
3273360	3280240	understanding of like a big conception of what it of that human being, how they think, what they know,
3280240	3286880	how they see the world, and then everything you ask, you're adding on to that. And that stickiness
3287520	3293600	seems to be, that's one of the really interesting qualities of the human being is that information
3293600	3299520	is sticky. You don't, you seem to remember the useful stuff, aggregate it well, and forget most
3299520	3305040	of the information that's not useful, that process. But that's also pretty similar to the
3305040	3309600	process that neural networks do is just that neural networks are much crappier at this time.
3310480	3315600	It doesn't seem to be fundamentally that different. But just stick on reasoning for a little longer.
3317120	3324560	He said, why not? Why can't I reason? What's a good impressive feat benchmark to you of reasoning
3327360	3331600	that you'll be impressed by if neural networks were able to do? Is that something you already have
3331600	3338640	in mind? Well, I think writing, writing really good code. I think proving really hard theorems,
3339360	3342800	solving open ended problems without of the box solutions.
3345840	3351600	And sort of theorem type mathematical problems. Yeah, I think those ones are a very natural
3351600	3355680	example as well. You know, if you can prove an unproven theorem, then it's hard to argue
3355680	3360880	don't reason. And so by the way, and this comes back to the point about the hard results, you know,
3360880	3366000	if you got a hard, if you have machine learning, deep learning as a field is very fortunate,
3366000	3372160	because we have the ability to sometimes produce these unambiguous results. And when they happen,
3372160	3377040	the debate changes, the conversation changes, it's a converse, you have the ability to produce
3377040	3382080	conversation changing results conversation. And then of course, just like you said, people kind
3382080	3386240	of take that for granted, say that wasn't actually a hard problem. Well, I mean, at some point,
3386240	3392880	we'll probably run out of hard problems. Yeah, that whole mortality thing is kind of kind of a
3392880	3398320	sticky problem that we haven't quite figured out. Maybe we'll solve that one. I think one of the
3398320	3402880	fascinating things in your entire body of work, but also the work at OpenAI recently,
3402880	3408480	one of the conversation changers has been in the world of language models. Can you briefly kind of
3409280	3413760	try to describe the recent history of using neural networks in the domain of language and text?
3414480	3420160	Well, there's been lots of history. I think I think the Elman network was was a small,
3420240	3425200	tiny recurrent neural network applied to language back in the 80s. So the history is really,
3426400	3433280	you know, fairly long, at least. And the thing that started the thing that changed the trajectory
3433280	3437920	of neural networks and language is the thing that changed the trajectory of all deep learning and
3437920	3443680	that's data and compute. So suddenly you move from small language models, which learn a little bit.
3444240	3448960	And with language models, in particular, you can, there's a very clear explanation for why
3449040	3453200	they need to be large to be good, because they're trying to predict the next word.
3454480	3461440	So when you don't know anything, you'll notice very, very broad strokes, surface level patterns,
3461440	3466400	like sometimes there are characters and there is space between those characters,
3466400	3470560	you'll notice this pattern. And you'll notice that sometimes there is a comma and then the
3470560	3474880	next character is a capital letter, you'll notice that pattern. Eventually, you may start to notice
3474880	3479280	that there are certain words occur often, you may notice that spellings are a thing,
3479280	3484960	you may notice syntax. And when you get really good at all these, you start to notice the semantics,
3485760	3490480	you start to notice the facts. But for that to happen, the language model needs to be larger.
3491360	3496320	So that's, let's linger on that, because that's where you and Noam Chomsky disagree.
3496320	3505600	So you think we're actually taking incremental steps, a sort of larger network, larger compute
3505600	3514640	will be able to get to the semantics, be able to understand language without what Noam likes to
3514640	3522000	sort of think of as a fundamental understandings of the structure of language, like imposing
3522080	3527920	your theory of language onto the learning mechanism. So you're saying the learning,
3527920	3532400	you can learn from raw data, the mechanism that underlies language?
3533360	3538480	Well, I think it's pretty likely. But I also want to say that I don't really
3540400	3547280	know precisely what Chomsky means when he talks about him. You said something about imposing
3547280	3552800	your structure on language. I'm not 100% sure what he means. But empirically, it seems that when
3552800	3556560	you inspect those larger language models, they exhibit signs of understanding the semantics,
3556560	3560400	whereas the smaller language models do not. We've seen that a few years ago when we
3560400	3566000	did work on the sentiment neuron, we trained a small, you know, smallish LSTM to predict the
3566000	3571920	next character in Amazon reviews. And we noticed that when you increase the size of the LSTM from
3571920	3577760	500 LSTM cells to 4,000 LSTM cells, then one of the neurons starts to represent the sentiment
3578480	3584720	of the article, of sorry, of their view. Now, why is that sentiment is a pretty semantic
3584720	3588640	attribute? It's not a syntactic attribute. And for people who might not know, I don't know if
3588640	3592400	that's a standard term, but sentiment is whether it's a positive or negative review. That's right.
3592400	3597680	Like, is the person happy with something or is the person unhappy with something? And so here we had
3597680	3602800	very clear evidence that a small neural net does not capture sentiment while a large neural net
3602800	3608720	does. And why is that? Well, our theory is that at some point, you run out of syntax to models,
3608720	3615760	you start to gotta focus on something else. And with size, you quickly run out of syntax to model,
3615760	3620560	and then you really start to focus on the semantics is would be the idea. That's right. And so I don't
3620560	3624720	want to imply that our models have complete semantic understanding, because that's not true.
3625280	3630720	But they definitely are showing signs of semantic understanding, partial semantic understanding,
3630720	3638000	but the smaller models do not show that those signs. Can you take a step back and say, what is GPT2,
3638000	3643040	which is one of the big language models that was the conversation changer in the past couple of
3643040	3651360	years? Yes. So GPT2 is a transformer with one and a half billion parameters that was trained on
3651440	3660080	up on about 40 billion tokens of text, which were obtained from web pages that were linked to
3660080	3664720	from Reddit articles with more than three uploads. And what's the transformer? The transformer,
3664720	3668960	it's the most important advance in neural network architectures in recent history.
3669680	3674080	What is attention maybe to because I think that's an interesting idea, not necessarily sort of
3674080	3680480	technically speaking, but the idea of attention versus maybe what recurrent neural networks
3680480	3685760	represent. Yeah. So the thing is, the transformer is a combination of multiple ideas simultaneously
3685760	3691760	of which attention is one. Do you think attention is the key? No, it's a key, but it's not the key.
3692320	3697680	The transformer is successful because it is the simultaneous combination of multiple ideas. And
3697680	3703040	if you were to remove either idea, it would be much less successful. So the transformer uses a
3703040	3707280	lot of attention, but attention exists for a few years. So that can't be the main innovation.
3708240	3714880	The transformer is designed in such a way that it runs really fast on the GPU.
3716000	3720240	And that makes a huge amount of difference. This is one thing. The second thing is that
3720240	3726320	transformer is not recurrent. And that is really important too, because it is more shallow and
3726320	3732400	therefore much easier to optimize. So in other words, uses attention. It is, it is a really great
3732480	3737760	fit to the GPU. And it is not recurrent. So therefore, less deep and easier to optimize.
3737760	3742720	And the combination of those factors make it successful. So now it makes it makes great
3742720	3747440	use of your GPU. It allows you to achieve better results for the same amount of compute.
3748560	3753520	And that's why it's successful. Were you surprised how well transformers worked?
3754160	3761280	And GPT2 worked? So you worked on language. You've had a lot of great ideas before transformers came
3761280	3766320	about in language. So you got to see the whole set of revolutions before and after. Were you
3766320	3772240	surprised? Yeah, a little. A little? Yeah. I mean, it's hard. It's hard to remember because
3772800	3777680	you adapt really quickly. But it definitely was surprising. It definitely was. In fact, I'll,
3777680	3783360	you know what, I'll, I'll retract my statement. It was, it was pretty amazing. It was just amazing
3783360	3788160	to see, generate this text of this. And you know, you got to keep in mind that we've seen,
3788160	3792480	at that time, we've seen all this progress in GANs, in improving the, you know, the samples
3792480	3797120	produced by GANs were just amazing. You have these realistic faces, but text hasn't really moved that
3797120	3804080	much. And suddenly we moved from, you know, whatever GANs were in 2015, to the best, most
3804080	3809120	amazing GANs in one step. And I was really stunning. Even though theory predicted, yeah,
3809120	3812800	you train a big language model, of course, you should get this, but then to see it with your own
3812800	3819840	eyes. It's something else. And yet we adapt really quickly. And now there's a sort of
3821520	3829200	some cognitive scientist, right? Articles saying that GPT2 models don't truly understand language.
3829200	3834880	So we adapt quickly to how amazing the fact that they're able to model the language so well is.
3835600	3841440	So what do you think is the bar? For what? For impressing us that it.
3842320	3846080	I don't know. Do you think that bar will continuously be moved?
3846080	3852160	Definitely. I think when you start to see really dramatic economic impact, that's when I think
3852160	3856800	that's in some sense the next barrier. Because right now, if you think about the work in AI,
3856800	3861680	it's really confusing. It's really hard to know what to make of all these advances.
3862400	3867440	It's kind of like, okay, you got an advance and now you can do more things and you got another
3868160	3875360	improvement and you got another cool demo. At some point, I think people who are outside of AI,
3876160	3878640	they can no longer distinguish this progress anymore.
3878640	3882160	So we were talking offline about translating Russian to English and
3882160	3886400	how there's a lot of brilliant work in Russian that the rest of the world doesn't know about.
3886400	3892080	That's true for Chinese. That's true for a lot of scientists and just artistic work in general.
3892080	3896960	Do you think translation is the place where we're going to see sort of economic big impact?
3897280	3902240	I don't know. I think there is a huge number of applications. First of all, I want to point out
3902240	3907600	that translation already today is huge. I think billions of people interact with
3908800	3912960	big chunks of the internet primarily through translation. So translation is already huge
3912960	3919760	and it's hugely positive too. I think self-driving is going to be hugely impactful.
3920080	3926880	And it's unknown exactly when it happens, but again, I would not bet against deep learning.
3927840	3931200	So there's deep learning in general, but you think deep learning for self-driving?
3931760	3935040	Yes, deep learning for self-driving. But I was talking about sort of language models.
3936880	3938000	Be your duff a little bit.
3938000	3941040	Just to check. You're not seeing a connection between driving and language.
3941040	3944080	No, no. Okay. I'd rather both use neural nets.
3944080	3947600	That would be a poetic connection. I think there might be some, like you said,
3947600	3954320	there might be some kind of unification towards a kind of multitask transformers
3954320	3960720	that can take on both language and vision tasks. That'd be an interesting unification.
3961440	3963600	Let's see. What can I ask about GPT two more?
3964880	3969840	It's simple. It's not much to ask. You take a transform, you make it bigger,
3969840	3972560	give it more data, and suddenly it does all those amazing things.
3972560	3976800	Yeah. One of the beautiful things is that GPT, the transformers are fundamentally
3976800	3985920	simple to explain, to train. Do you think bigger will continue to show better results in language?
3986960	3987440	Probably.
3988240	3991360	Sort of like what are the next steps with GPT two? Do you think?
3991360	3998000	I mean, I think for sure seeing what larger versions can do is one direction. Also,
3999760	4002640	I mean, there are many questions. There's one question which I'm curious about,
4002640	4006480	and that's the following. Right now, GPT two, so we feed it all this data from the
4006480	4010400	internet, which means that it needs to memorize all those random facts about everything in the
4010400	4019680	internet. It would be nice if the model could somehow use its own intelligence to decide
4019680	4024240	what data it wants to start, accept, and what data it wants to reject, just like people.
4024240	4029760	People don't learn all data indiscriminately. We are super selective about what we learn,
4029760	4033040	and I think this kind of active learning I think would be very nice to have.
4034160	4041040	Yeah. Listen, I love active learning. So let me ask, does the selection of data,
4041040	4045920	can you just elaborate that a little bit more? Do you think the selection of data is,
4048000	4053680	like I have this kind of sense that the optimization of how you select data,
4053760	4059600	so the active learning process is going to be a place for a lot of breakthroughs
4060720	4064960	even in the near future, because there hasn't been many breakthroughs there that are public.
4064960	4069200	I feel like there might be private breakthroughs that companies keep to themselves,
4069200	4072880	because the fundamental problem has to be solved if you want to solve self-driving,
4072880	4077760	if you want to solve a particular task. What do you think about the space in general?
4077760	4082400	Yeah, so I think that for something like active learning, or in fact for any kind of capability,
4082400	4087680	like active learning, the thing that it really needs is a problem. It needs a problem that requires it.
4089280	4092880	It's very hard to do research about the capability if you don't have a task,
4092880	4096640	because then what's going to happen is you will come up with an artificial task,
4096640	4099520	get good results, but not really convince anyone.
4100560	4107520	Right. We're now past the stage where getting a result on MNIST,
4107520	4110720	some clever formulation of MNIST will convince people.
4110720	4115280	That's right. In fact, you could quite easily come up with a simple active learning scheme on MNIST
4115280	4124880	and get a 10x speed up, but then so what? I think that active learning will naturally arise
4125440	4131600	as problems that require it to pop up. That's my take on it.
4132720	4138800	There's another interesting thing that OpenAS brought up with GPT2, which is when you create a
4138800	4144560	powerful artificial intelligence system, and it was unclear what kind of detrimental,
4144560	4151360	once you release GPT2, what kind of detrimental effect it'll have. Because if you have a model
4151360	4158320	that can generate pretty realistic text, you can start to imagine that it would be used by bots in
4159360	4164400	some way that we can't even imagine. There's this nervousness about what it's possible to do.
4164960	4170000	You did a really brave and I think profound thing, which just started a conversation about this.
4170000	4178400	How do we release powerful artificial intelligence models to the public? If we do it all, how do we
4178400	4185440	privately discuss with other even competitors about how we manage the use of the systems and
4185440	4190560	so on? From this whole experience, you've released a report on it, but in general,
4190560	4196480	are there any insights that you've gathered from just thinking about this, about how you release
4196480	4203760	models like this? I think that my take on this is that the field of AI has been in a state of
4203760	4210240	childhood, and now it's exiting that state and it's entering a state of maturity. What that means
4210240	4215600	is that AI is very successful and also very impactful, and its impact is not only large,
4215680	4223840	but it's also growing. For that reason, it seems wise to start thinking about the impact of our
4223840	4228560	systems before releasing them maybe a little bit too soon rather than a little bit too late.
4229760	4234240	With the case of GPT-2, like I mentioned earlier, the results really were stunning,
4235040	4241440	and it seemed plausible. It didn't seem certain. It seemed plausible that something like GPT-2
4241440	4248800	could easily use to reduce the cost of disinformation. There was a question of what's
4248800	4252880	the best way to release it, and a staged release seemed logical. A small model was released,
4253680	4259840	and there was time to see the... Many people use these models in lots of cool ways. There've been
4259840	4266080	lots of really cool applications. There haven't been any negative applications we know of,
4266080	4269840	and so eventually it was released, but also other people replicated similar models.
4269840	4277680	That's an interesting question though that we know of. In your view, staged release is at least
4277680	4285360	part of the answer to the question of how do we... What do we do once we create a system like this?
4285920	4291840	It's part of the answer, yes. Is there any other insights? Say you don't want to release the model
4291840	4297440	at all because it's useful to you for whatever the business is. Well, there are plenty of people
4297440	4303920	who don't release models already. Right, of course, but is there some moral ethical responsibility
4304480	4311840	when you have a very powerful model to communicate? Just as you said, when you had GPT-2, it was
4311840	4315200	unclear how much it could be used for misinformation. It's an open question,
4316240	4320880	and getting an answer to that might require that you talk to other really smart people that are
4321600	4328720	outside of your particular group. Have you... Please tell me there's some optimistic pathway for
4329280	4332320	people across the world to collaborate on these kinds of cases,
4334560	4339440	or is it still really difficult from one company to talk to another company?
4339440	4346720	So it's definitely possible. It's definitely possible to discuss these kind of models with
4346720	4351280	colleagues elsewhere and to get their take on what to do.
4352080	4354000	How hard is it though? I mean...
4356320	4361920	Do you see that happening? I think that's a place where it's important to gradually build trust
4361920	4367840	between companies, because ultimately, all the AI developers are building technology,
4367840	4371440	which is going to be increasingly more powerful, and so it's...
4372160	4377040	The way to think about it is that ultimately we're only together.
4378640	4386400	Yeah, it's... I tend to believe in the better angels of our nature, but I do hope that
4389040	4392720	when you build a really powerful AI system in a particular domain,
4392720	4398160	that you also think about the potential negative consequences of... Yeah.
4402240	4407360	It's an interesting and scary possibility that there will be a race for AI development that
4407360	4414400	would push people to close that development and not share ideas with others. I don't love this.
4414400	4419440	I've been in a pure academic for 10 years. I really like sharing ideas and it's fun, it's exciting.
4421440	4425440	What do you think it takes to... Let's talk about AGI a little bit. What do you think it takes to
4425440	4430080	build a system of human level intelligence? We talked about reasoning, we talked about
4430080	4435760	long-term memory, but in general, what does it take, do you think? Well, I can't be sure,
4437360	4445520	but I think the deep learning plus maybe another small idea. Do you think self-play will be involved?
4445520	4451040	Sort of like you've spoken about the powerful mechanism of self-play where systems learn by
4452480	4459360	sort of exploring the world in a competitive setting against other entities that are similarly
4459360	4464480	skilled as them and so incrementally improving this way? Do you think self-play will be a component
4464480	4471200	of building an AGI system? Yeah, so what I would say to build AGI, I think it's going to be
4472480	4479680	deep learning plus some ideas and I think self-play will be one of those ideas. I think that that is a
4479680	4490320	very... Self-play has this amazing property that it can surprise us in truly novel ways. For example,
4492080	4499280	like we, I mean, pretty much every self-play system, both are Dota bot. I don't know if
4499280	4505200	OpenAI had a release about multi-agents where you had two little agents who were playing hide and
4506080	4511520	seek and of course also AlphaZero. They all produce surprising behaviors. They all produce
4511520	4517280	behaviors that we didn't expect. They are creative solutions to problems and that seems like an
4517280	4523840	important part of AGI that our systems don't exhibit routinely right now. And so that's why I
4523840	4527520	like this area, I like this direction because of its ability to surprises.
4527520	4532560	To surprises and an AGI system would surprise us fundamentally. Yes, but and to be precise,
4533280	4538640	not just a random surprise, but to find a surprising solution to a problem that's also useful.
4539920	4546960	Now, a lot of the self-play mechanisms have been used in the game context or at least in the
4546960	4557040	simulation context. How far along the path to AGI do you think will be done in simulation? How much
4557040	4564960	faith promise do you have in simulation versus having to have a system that operates in the real
4564960	4571680	world, whether it's the real world of digital real-world data or real-world like actual physical
4571680	4577360	world with robotics? I don't think it's in either war. I think simulation is a tool and it helps.
4577360	4583840	It has certain strengths and certain weaknesses and we should use it. Yeah, but okay, I understand
4584400	4594320	that that's true. But one of the criticisms of self-play, one of the criticisms of reinforcement
4594320	4602800	learning is one of the its current power, its current results while amazing have been demonstrated
4602800	4607200	in a simulated environments or very constrained physical environments. Do you think it's possible
4607200	4613280	to escape them? Escape the simulated environments and be able to learn in non-simulated environments?
4613280	4621040	Or do you think it's possible to also just simulate in a photorealistic and physics realistic way
4621040	4626080	the real world in a way that we can solve real problems with self-play in simulation?
4627440	4632480	I think that transfer from simulation to the real world is definitely possible and has been
4632480	4637600	exhibited many times by many different groups. It's been especially successful in vision.
4638560	4644240	Also OpenAI in the summer has demonstrated a robot hand which was trained entirely in simulation
4645120	4648160	in a certain way that allowed for sim to real transfer to occur.
4649680	4654480	Is this for the Rubik's Cube? Yes, that's right. I wasn't aware that was trained in simulation.
4654480	4660000	It was trained in simulation entirely. Really? So it wasn't in the physical that the hand wasn't
4660000	4666240	trained? No, 100% of the training was done in simulation and the policy that was learned in
4666240	4671360	simulation was trained to be very adaptive. So adaptive that when you transfer it, it could very
4671360	4677760	quickly adapt to the physical world. So the kind of perturbations with the giraffe or whatever the
4677760	4685200	heck it was, were those part of the simulation? Well, the simulation was generally, so the simulation
4685200	4690160	was trained to be robust to many different things, but not the kind of perturbations we've had in the
4690160	4696960	video. So it's never been trained with a glove. It's never been trained with a stuffed giraffe.
4696960	4700800	So in theory, these are novel perturbations? Correct. It's not in theory in practice.
4701680	4708320	That those are novel perturbations? Well, that's okay. That's a clean, small-scale,
4708320	4712000	but clean example of a transfer from the simulated world to the physical world.
4712000	4716960	Yeah, and I will also say that I expect the transfer capabilities of deep learning to increase in
4716960	4722000	general and the better the transfer capabilities are, the more useful simulation will become
4723520	4727760	because then you could take, you could experience something in simulation
4728400	4732720	and then learn a moral of the story which you could then carry with you to the real world, right?
4733360	4741040	As humans do all the time and they play computer games. So let me ask sort of an embodied question,
4741760	4748400	staying on AGI for a sec. Do you think AGI system, we need to have a body? We need to have some of
4748400	4756560	those human elements of self-awareness, consciousness, sort of fear of mortality, sort of self-preservation
4756560	4762320	in the physical space which comes with having a body? I think having a body will be useful.
4762320	4766720	I don't think it's necessary, but I think it's very useful to have a body for sure because you can
4767200	4771600	learn a whole new, you can learn things which cannot be learned without a body,
4772400	4776000	but at the same time, I think that you can, if you don't have a body, you could
4776000	4780880	compensate for it and still succeed. You think so? Yes. Well, there is evidence for this. For
4780880	4786720	example, there are many people who were born deaf and blind and they were able to compensate for
4786720	4792560	the lack of modalities. I'm thinking about Helen Keller specifically. So even if you're not able
4792560	4798320	to physically interact with the world and if you're not able to, I actually was getting at,
4799520	4805280	maybe let me ask on the more particular, I'm not sure if it's connected to having a body or not,
4805280	4811200	but the idea of consciousness and a more constrained version of that is self-awareness.
4811200	4817760	Do you think an AGI system should have consciousness? We can't define whatever the
4817760	4822880	heck you think consciousness is. Yeah. Hard question to answer, given how hard it is to define it.
4824640	4828240	Do you think it's useful to think about? I mean, it's definitely interesting,
4828240	4832880	it's fascinating. I think it's definitely possible that our systems will be conscious.
4833760	4837680	Do you think that's an emergent thing that just comes from, do you think consciousness could emerge
4837680	4842400	from the representation that's stored within your networks? So like that it naturally just
4842400	4847040	emerges when you become more and more, you're able to represent more and more over the world?
4847040	4852720	Well, I'd say I'd make the following argument, which is humans are conscious,
4853680	4856960	and if you believe that artificial neural nets are sufficiently
4857840	4863200	similar to the brain, then there should at least exist artificial neural nets you should be conscious
4863200	4871520	to. You're leaning on that existence proof pretty heavily. Okay. But that's the best answer I can
4871600	4879120	give. No, I know. I know. There's still an open question if there's not some magic in the brain
4879120	4887040	that we're not. I mean, I don't mean a non-materialistic magic, but that the brain might be a lot
4887040	4891440	more complicated and interesting than we give it credit for. If that's the case, then it should show
4891440	4897280	up and at some point, we will find out that we can't continue to make progress. But I think
4897280	4901920	it's unlikely. So we talk about consciousness, but let me talk about another poorly defined
4901920	4908240	concept of intelligence. Again, we've talked about reasoning. We've talked about memory. What do
4908240	4914480	you think is a good test of intelligence for you? Are you impressed by the test that Alan Turing
4914480	4921200	formulated with the imitation game with natural language? Is there something in your mind that
4921280	4929920	you would be deeply impressed by if a system was able to do? I mean, lots of things. There's a
4929920	4936800	certain frontier of capabilities today. And there exist things outside of that frontier,
4936800	4943200	and I would be impressed by any such thing. For example, I would be impressed by a deep learning
4943200	4950080	system which solves a very pedestrian task like machine translation or computer vision task or
4950080	4957600	something which never makes a mistake a human wouldn't make under any circumstances. I think
4957600	4961680	that is something which have not yet been demonstrated and I would find it very impressive.
4962640	4966480	Yes, so right now, they make mistakes in different, they might be more accurate than human beings,
4966480	4973360	but they still make a different set of mistakes. So I would guess that a lot of the skepticism
4973360	4977280	that some people have about deep learning is when they look at their mistakes and they say,
4977280	4982160	well, those mistakes, they make no sense. If you understood the concept, you wouldn't make that
4982160	4991680	mistake. And I think that changing that would inspire me. That would be, yes, this is progress.
4992400	4998640	Yeah, that's a really nice way to put it. But I also just don't like that human instinct to
4999680	5004320	criticize a model is not intelligent. That's the same instinct as we do when we criticize
5004320	5014240	any group of creatures as the other. Because it's very possible that GPT2 is much smarter
5014240	5019280	than human beings at many things. That's definitely true. It has a lot more breadth of knowledge.
5019280	5024640	Yes, breadth of knowledge and even perhaps depth on certain topics.
5026000	5030240	It's kind of hard to judge what depth means, but there's definitely a sense in which
5031120	5033920	humans don't make mistakes that these models do.
5034480	5039280	Yes, the same is applied to autonomous vehicles. The same is probably going to continue being
5039280	5047360	applied to a lot of artificial intelligence systems. In the 21st century, the process of
5047360	5055280	analyzing the progress of AI is the search for one case where the system fails in a big way
5055280	5061520	where humans would not. And then many people writing articles about it. And then broadly,
5062720	5068720	the public generally gets convinced that the system is not intelligent. And we pacify ourselves
5068720	5073520	by thinking it's not intelligent because of this one anecdotal case. And this seems to continue
5073520	5078400	happening. Yeah, I mean, there is truth to that. Although I'm sure that plenty of people are also
5078400	5082160	extremely impressed by the systems that exist today. But I think this connects to the earlier
5082160	5089520	point we discussed that it's just confusing to judge progress in AI. And you have a new robot
5089520	5095440	demonstrating something. How impressed should you be? And I think that people will start to be
5095440	5101520	impressed once AI starts to really move the needle on the GDP. So you're one of the people that
5101520	5108000	might be able to create an AI system here, not you, but you and open AI. If you do create an
5108000	5116000	AI system, and you get to spend sort of the evening with it, him, her, what would you talk about,
5116000	5120800	do you think? The very first time? First time? Well, the first time I would just
5121760	5126320	would just ask all kinds of questions and try to make it to get it to make a mistake. And that would
5126320	5133920	be amazed that it doesn't make mistakes and just keep keep asking broad. Okay, what kind of questions
5133920	5141120	do you think? Would they be factual or would they be personal, emotional, psychological? What do you
5141120	5151520	think? All of the above. Would you ask for advice? Definitely. I mean, why would I limit myself
5151520	5157440	talking to a system like this? Now, again, let me emphasize the fact that you truly are one of the
5157440	5165120	people that might be in the room where this happens. So let me ask a sort of a profound question
5165120	5171520	about, I've just talked to a Stalin historian. I've been talking to a lot of people who are
5171520	5178560	studying power. Abraham Lincoln said, nearly all men can stand adversity. But if you want to test
5178560	5186400	a man's character, give him power. I would say the power of the 21st century, maybe the 22nd,
5186400	5192640	but hopefully the 21st would be the creation of an AGI system and the people who have control,
5193360	5200000	direct possession and control the AGI system. So what do you think after spending that evening
5201120	5205120	having a discussion with the AGI system? What do you think you would do?
5205520	5216240	Well, the ideal world I'd like to imagine is one where humanity are like the board,
5216960	5223760	the board members of a company where the AGI is the CEO. So it would be,
5225520	5229840	I would like the picture of which I would imagine is you have some kind of different
5230720	5237520	entities, different countries or cities, and the people that leave their vote for what the AGI
5237520	5242160	that represents them should do and the AGI that represents them goes and does it. I think a picture
5242160	5248560	like that, I find very appealing. You could have multiple AGI, you would have an AGI for a city,
5248560	5255920	for a country, and it would be trying to in effect take the democratic process to the next level.
5255920	5260080	And the board can almost fire the CEO? Essentially, press the reset button,
5260080	5267200	say, re-randomize the parameters. But let me sort of, that's actually, okay, that's a beautiful
5267200	5274560	vision, I think, as long as it's possible to press the reset button. Do you think it will always be
5274560	5280080	possible to press the reset button? So I think that it's definitely really possible to build.
5280880	5291600	So you're talking, so the question that I really understand from you is, will humans or humans
5291600	5296720	people have control over the AI systems that they build? Yes. And my answer is, it's definitely
5296720	5302720	possible to build AI systems which will want to be controlled by their humans. Wow, that's part of
5302720	5312080	their, so it's not that just they can't help but be controlled, but that's the one of the
5312080	5317360	objectives of their existence is to be controlled. In the same way that human parents
5319760	5325280	generally want to help their children, they want their children to succeed. It's not a burden for
5325280	5331600	them. They are excited to help the children to feed them and to dress them and to take care of them.
5332560	5339360	And I believe with high conviction that the same will be possible for an AGI. It will be possible
5339360	5345360	to program an AGI to design it in such a way that it will have a similar deep drive that it will be
5345360	5352800	delighted to fulfill and the drive will be to help humans flourish. But let me take a step back
5352800	5356880	to that moment where you create the AGI system. I think this is a really crucial moment.
5357760	5367360	And between that moment and the Democratic Board members with the AGI at the head,
5368800	5376160	there has to be a relinquishing of power. So as George Washington, despite all the bad things he
5376160	5380960	did, one of the big things he did is he relinquished power. He first of all didn't want to be president
5381920	5387760	and even when he became president, he didn't keep just serving as most dictators do for indefinitely.
5389120	5397120	Do you see yourself being able to relinquish control over an AGI system, given how much
5397120	5403040	power you can have over the world, at first financial, just make a lot of money and then
5403040	5409280	control by having possession as a AGI system? I'd find it trivial to do that. I'd find it
5409360	5414320	trivial to relinquish this kind of power. I mean, you know, the kind of scenario you are
5414320	5421200	describing sounds terrifying to me. That's all. I would absolutely not want to be in that position.
5422400	5428640	Do you think you represent the majority or the minority of people in the AGI community?
5429280	5435600	Well, I mean, it's an open question, an important one. Are most people good is another way to ask it.
5436480	5439600	So I don't know if most people are good. But
5442160	5445760	I think that when it really counts, people can be better than we think.
5446880	5452560	That's beautifully put. Yeah. Are there specific mechanisms you can think of of aligning AGI
5452560	5458320	in values to human values? Is that do you think about these problems of continued alignment
5458320	5463440	as we develop the systems? Yeah, definitely. In some sense,
5464400	5469120	the kind of question which you are asking is so if you have to translate the question to today's
5469120	5478400	terms, it would be a question about how to get an RL agent that's optimizing a value function
5478400	5483920	which itself is learned. And if you look at humans, humans are like that because the reward
5483920	5489920	function, the value function of humans is not external, it is internal. That's right. And
5490080	5498080	and there are definite ideas of how to train a value function, basically an objective,
5498960	5504240	an as objective as possible perception system that will be trained separately
5505680	5513120	to recognize, to internalize human judgments on different situations. And then that component
5513120	5518880	wouldn't be integrated as the value as the base value function for some more capable RL system.
5518880	5522960	You could imagine a process like this. I'm not saying this is the process. I'm saying this is
5522960	5531120	an example of the kind of thing you could do. So on that topic of the objective functions of
5531120	5536720	human existence, what do you think is the objective function that's implicit in human
5536720	5552480	existence? What's the meaning of life? Oh, I think the question is wrong in some way. I think that
5552480	5556640	the question implies that there is an objective answer which is an external answer, you know,
5556640	5564320	your meaning of life is X. I think what's going on is that we exist and that's amazing. And we
5564400	5568960	should try to make the most of it and try to maximize our own value and enjoyment of
5569760	5576160	a very short time while we do exist. It's funny because action does require an objective function.
5576160	5581840	It's definitely theirs in some form, but it's difficult to make it explicit and maybe impossible
5581840	5587120	to make it explicit. I guess is what you're getting at. And that's an interesting fact of an RL
5587120	5593360	environment. Well, what I was making a slightly different point is that humans want things and
5593360	5599760	their wants create the drives that cause them to, you know, our wants are our objective functions,
5599760	5604160	our individual objective functions. We can later decide that we want to change,
5604160	5607120	that what we wanted before is no longer good and we want something else.
5607120	5612000	Yeah, but they're so dynamic, there's got to be some underlying sort of Freud,
5612000	5617840	there's things like sexual stuff, there's people who think it's the fear of death and there's also
5618800	5622720	the desire for knowledge and, you know, all these kinds of things, procreation,
5623760	5628000	the sort of all the evolutionary arguments, it seems to be, there might be some kind of
5628000	5634880	fundamental objective function from which everything else emerges. But it seems like
5634880	5638160	it's very difficult to make it explicit. I think that probably is an evolutionary
5638160	5641600	objective function, which is to survive and procreate and make your students succeed.
5642480	5647360	That would be my guess, but it doesn't give an answer to the question of what's the meaning of
5647360	5654560	life. I think you can see how humans are part of this big process, this ancient process we are,
5656640	5664160	we exist on a small planet. And that's it. So given that we exist, try to make the most of it
5664160	5670960	and try to enjoy more and suffer less as much as we can. Let me ask two silly questions about life.
5671680	5679360	One, do you have regrets? Moments that if you went back, you would do differently. And two,
5680000	5683360	are there moments that you're especially proud of that made you truly happy?
5684720	5692080	So I can answer both questions. Of course, there's a huge number of choices and decisions that have
5692080	5696880	made that with the benefit of hindsight, I wouldn't have made them. And I do experience some regret,
5696880	5701680	but I tried to take solace in the knowledge that at the time I did the best I could.
5702720	5707360	And in terms of things that I'm proud of, I'm very fortunate to have done things I'm proud of
5708640	5713200	and they made me happy for some time, but I don't think that that is the source of happiness.
5714480	5719280	So your academic accomplishments, all the papers, you're one of the most cited people in the world.
5719840	5723440	All the breakthroughs I mentioned in computer vision and language and so on
5723440	5729440	is what is the source of happiness and pride for you?
5729440	5733920	I mean, all those things are a source of pride for sure. I'm very grateful for having done all
5733920	5740160	those things. And it was very fun to do them. But happiness comes, you know, you can, happiness,
5740160	5746160	well, my current view is that happiness comes from our to a very large degree from the way we
5746160	5751600	look at things. You know, you can have a simple meal and be quite happy as a result or you can
5751600	5757360	talk to someone and be happy as a result as well. Or conversely, you can have a meal and be
5757360	5762240	disappointed that the meal wasn't a better meal. So I think a lot of happiness comes from that.
5762240	5768000	But I'm not sure. I don't want to be too confident. Being humble in the face of the uncertainty seems
5768000	5773840	to be also a part of this whole happiness thing. Well, I don't think there's a better way to end
5773840	5779840	it than meaning of life and discussions of happiness. So Ilya, thank you so much. You've
5779840	5785600	given me a few incredible ideas. You've given the world many incredible ideas. I really appreciate it.
5785600	5789360	And thanks for talking today. Yeah, thanks for stopping by. I really enjoyed it.
5790400	5794080	Thanks for listening to this conversation with Ilya Setskever. And thank you to our
5794080	5799440	presenting sponsor, Cash App. Please consider supporting the podcast by downloading Cash App
5799440	5805280	and using the code Lex podcast. If you enjoy this podcast, subscribe on YouTube,
5805280	5810400	review it with five stars and Apple podcast, support on Patreon or simply connect with me
5810400	5818240	on Twitter at Lex Friedman. And now let me leave you with some words from Alan Turing on machine
5818240	5824960	learning. Instead of trying to produce a program to simulate the adult mind, why not rather try to
5824960	5831280	produce one which simulates the child? If this were then subjected to an appropriate course of
5831280	5847200	education, one would obtain the adult brain. Thank you for listening and hope to see you next time.
