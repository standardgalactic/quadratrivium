1
00:00:00,000 --> 00:00:04,100
The following is a conversation with David Silver, who leads the Reinforcement Learning

2
00:00:04,100 --> 00:00:11,140
Research Group at DeepMind, and was the lead researcher on AlphaGo, AlphaZero, and co-led

3
00:00:11,140 --> 00:00:15,680
the AlphaStar and MuZero efforts, and a lot of important work in reinforcement learning

4
00:00:15,680 --> 00:00:17,180
in general.

5
00:00:17,180 --> 00:00:23,100
I believe AlphaZero is one of the most important accomplishments in the history of artificial

6
00:00:23,100 --> 00:00:28,900
intelligence, and David is one of the key humans who brought AlphaZero to life together

7
00:00:28,900 --> 00:00:32,060
with a lot of other great researchers at DeepMind.

8
00:00:32,060 --> 00:00:34,500
He's humble, kind, and brilliant.

9
00:00:34,500 --> 00:00:38,660
We were both jet lagged, but didn't care and made it happen.

10
00:00:38,660 --> 00:00:43,460
It was a pleasure and truly an honor to talk with David.

11
00:00:43,460 --> 00:00:47,100
This conversation was recorded before the outbreak of the pandemic.

12
00:00:47,100 --> 00:00:51,900
For everyone feeling the medical, psychological, and financial burden of this crisis, I'm

13
00:00:51,900 --> 00:00:53,540
sending love your way.

14
00:00:53,540 --> 00:00:57,860
Stay strong, or in this together, we'll beat this thing.

15
00:00:57,860 --> 00:01:00,100
This is the Artificial Intelligence Podcast.

16
00:01:00,100 --> 00:01:05,180
If you enjoy it, subscribe on YouTube, review it with 5 stars on Apple Podcasts, support

17
00:01:05,180 --> 00:01:12,220
on Patreon, or simply connect with me on Twitter at Lex Freedman, spelled F-R-I-D-M-A-N.

18
00:01:12,220 --> 00:01:16,540
As usual, I'll do a few minutes of ads now and never any ads in the middle that can break

19
00:01:16,540 --> 00:01:18,420
the flow of the conversation.

20
00:01:18,420 --> 00:01:22,780
I hope that works for you and doesn't hurt the listening experience.

21
00:01:22,780 --> 00:01:24,060
Quick summary of the ads.

22
00:01:24,060 --> 00:01:25,060
Two sponsors.

23
00:01:25,060 --> 00:01:27,580
Masterclass and Cash App.

24
00:01:27,580 --> 00:01:32,700
Please consider supporting the podcast by signing up to masterclass and masterclass.com

25
00:01:32,700 --> 00:01:38,940
slash lex and downloading Cash App and using code Lex Podcast.

26
00:01:38,940 --> 00:01:43,340
This show is presented by Cash App, the number one finance app in the App Store.

27
00:01:43,340 --> 00:01:47,220
When you get it, use code Lex Podcast.

28
00:01:47,220 --> 00:01:51,540
Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with

29
00:01:51,540 --> 00:01:53,980
as little as $1.

30
00:01:53,980 --> 00:01:58,020
Since Cash App allows you to buy Bitcoin, let me mention that cryptocurrency in the

31
00:01:58,020 --> 00:02:01,420
context of the history of money is fascinating.

32
00:02:01,420 --> 00:02:05,380
I recommend Ascent of Money as a great book on this history.

33
00:02:05,380 --> 00:02:09,740
Debits and credits on ledgers started around 30,000 years ago.

34
00:02:09,740 --> 00:02:15,940
The US Dollar created over 200 years ago and Bitcoin, the first decentralized cryptocurrency,

35
00:02:15,940 --> 00:02:18,740
released just over 10 years ago.

36
00:02:18,740 --> 00:02:24,020
So given that history, cryptocurrency is still very much in its early days of development,

37
00:02:24,020 --> 00:02:29,180
but it's still aiming to and just might redefine the nature of money.

38
00:02:29,180 --> 00:02:33,460
So again, if you get Cash App from the App Store or Google Play and use the code Lex

39
00:02:33,460 --> 00:02:39,700
Podcast, you get $10 and Cash App will also donate $10 the first, an organization that

40
00:02:39,700 --> 00:02:45,020
is helping to advance robotics and STEM education for young people around the world.

41
00:02:45,020 --> 00:02:47,180
This show is sponsored by Masterclass.

42
00:02:47,180 --> 00:02:51,660
Sign up at masterclass.com slash Lex to get a discount and to support this podcast.

43
00:02:51,660 --> 00:02:56,780
In fact, for a limited time now, if you sign up for an All Access Pass for a year, you

44
00:02:56,780 --> 00:03:01,300
get to get another All Access Pass to share with a friend.

45
00:03:01,300 --> 00:03:02,740
Buy one, get one free.

46
00:03:02,740 --> 00:03:06,420
When I first heard about Masterclass, I thought it was too good to be true.

47
00:03:06,420 --> 00:03:13,020
For $180 a year, you get an All Access Pass to watch courses from to list some of my favorites.

48
00:03:13,020 --> 00:03:18,220
Chris Hatfield on space exploration, Neil deGrasse Tyson on scientific thinking communication,

49
00:03:18,220 --> 00:03:24,780
Will Wright, the creator of SimCity and Sims, on game design, Jane Goodall on conservation,

50
00:03:24,780 --> 00:03:31,020
Carl Santana on guitar, his song Europa could be the most beautiful guitar song ever written.

51
00:03:31,020 --> 00:03:35,740
Gary Kasparov on chess, Daniel Nagrano on poker, and many, many more.

52
00:03:35,740 --> 00:03:39,460
Chris Hatfield explaining how rockets work and the experience of being launched into

53
00:03:39,460 --> 00:03:41,740
space alone is worth the money.

54
00:03:41,740 --> 00:03:46,940
For me, the key is to not be overwhelmed by the abundance of choice, pick three courses

55
00:03:46,940 --> 00:03:49,740
you want to complete, watch each of them all the way through.

56
00:03:49,740 --> 00:03:53,660
It's not that long, but it's an experience that will stick with you for a long time.

57
00:03:53,660 --> 00:03:54,660
I promise.

58
00:03:54,660 --> 00:03:56,860
It's easily worth the money.

59
00:03:56,860 --> 00:03:59,340
You can watch it on basically any device.

60
00:03:59,340 --> 00:04:03,940
Once again, sign up on masterclass.com slash Lex to get a discount and to support this

61
00:04:03,940 --> 00:04:05,740
podcast.

62
00:04:05,740 --> 00:04:09,900
And now here's my conversation with David Silver.

63
00:04:09,900 --> 00:04:14,020
What was the first program you ever written and what programming language?

64
00:04:14,020 --> 00:04:15,020
Do you remember?

65
00:04:15,020 --> 00:04:22,140
I remember very clearly, yeah, my parents brought home this BBC model B microcomputer.

66
00:04:22,140 --> 00:04:24,220
It was just this fascinating thing to me.

67
00:04:24,220 --> 00:04:30,100
I was about seven years old and couldn't resist just playing around with it.

68
00:04:30,100 --> 00:04:37,220
So I think first program ever was writing my name out in different colors and getting

69
00:04:37,220 --> 00:04:39,820
it to loop and repeat that.

70
00:04:40,380 --> 00:04:44,580
There was something magical about that, which just led to more and more.

71
00:04:44,580 --> 00:04:46,820
How did you think about computers back then?

72
00:04:46,820 --> 00:04:51,700
The magical aspect of it, that you can write a program and there's this thing that you

73
00:04:51,700 --> 00:04:57,660
just gave birth to that's able to create visual elements and live in its own.

74
00:04:57,660 --> 00:05:00,100
Or did you not think of it in those romantic notions?

75
00:05:00,100 --> 00:05:02,460
Was it more like, oh, that's cool.

76
00:05:02,460 --> 00:05:05,380
I can solve some puzzles.

77
00:05:05,380 --> 00:05:06,980
It was always more than solving puzzles.

78
00:05:06,980 --> 00:05:14,340
It was something where there was this limitless possibilities once you have a computer in

79
00:05:14,340 --> 00:05:15,340
front of you.

80
00:05:15,340 --> 00:05:16,340
You can do anything with it.

81
00:05:16,340 --> 00:05:18,100
I used to play with Lego with the same feeling.

82
00:05:18,100 --> 00:05:21,540
You can make anything you want out of Lego, but even more so with a computer.

83
00:05:21,540 --> 00:05:24,660
You're not constrained by the amount of kit you've got.

84
00:05:24,660 --> 00:05:29,020
And so I was fascinated by it and started pulling out the user guide and the advanced

85
00:05:29,020 --> 00:05:30,820
user guide and then learning.

86
00:05:30,820 --> 00:05:34,700
So I started in basic and then later 6502.

87
00:05:34,740 --> 00:05:40,180
My father also became interested in this machine and gave up his career to go back to school

88
00:05:40,180 --> 00:05:47,100
and study for a master's degree in artificial intelligence, funnily enough, at Essex University

89
00:05:47,100 --> 00:05:48,740
when I was seven.

90
00:05:48,740 --> 00:05:52,100
So I was exposed to those things at an early age.

91
00:05:52,100 --> 00:05:57,780
He showed me how to program in Prologue and do things like querying your family tree.

92
00:05:57,780 --> 00:06:04,300
And those are some of my earliest memories of trying to figure things out on a computer.

93
00:06:04,340 --> 00:06:09,020
Those are the early steps in computer science programming, but when did you first fall in

94
00:06:09,020 --> 00:06:14,900
love with artificial intelligence or with the ideas, the dreams of AI?

95
00:06:14,900 --> 00:06:19,140
I think it was really when I went to study at university.

96
00:06:19,140 --> 00:06:24,260
So I was an undergrad at Cambridge and studying computer science.

97
00:06:24,260 --> 00:06:29,660
And I really started to question, you know, what really are the goals?

98
00:06:29,660 --> 00:06:30,660
What's the goal?

99
00:06:30,660 --> 00:06:33,060
Where do we want to go with computer science?

100
00:06:33,060 --> 00:06:42,300
And it seemed to me that the only step of major significance to take was to try and recreate

101
00:06:42,300 --> 00:06:44,300
something akin to human intelligence.

102
00:06:44,300 --> 00:06:47,860
If we could do that, that would be a major leap forward.

103
00:06:47,860 --> 00:06:52,500
And that idea certainly wasn't the first to have it, but it, you know, nestled within

104
00:06:52,500 --> 00:06:58,700
me somewhere and became like a bug, you know, I really wanted to crack that problem.

105
00:06:58,940 --> 00:07:02,980
So you thought it was, like, you had a notion that this is something that human beings can do,

106
00:07:02,980 --> 00:07:07,300
that it is possible to create an intelligent machine?

107
00:07:07,300 --> 00:07:13,420
Well, I mean, unless you believe in something metaphysical, then what are our brains doing?

108
00:07:13,420 --> 00:07:21,580
Well, at some level, their information processing systems, which are able to take whatever

109
00:07:21,580 --> 00:07:25,620
information is in there, transform it through some form of program and produce some kind

110
00:07:25,620 --> 00:07:29,660
of output, which enables that human being to do all the amazing things that they can

111
00:07:29,660 --> 00:07:32,300
do in this incredible world.

112
00:07:32,300 --> 00:07:38,540
So then, do you remember the first time you've written a program that, because you also had

113
00:07:38,540 --> 00:07:42,380
an interest in games, do you remember the first time you were in a program that beat

114
00:07:42,380 --> 00:07:51,740
you in a game, that more beat you at anything, sort of achieved super David Silver level

115
00:07:51,740 --> 00:07:54,460
performance?

116
00:07:54,460 --> 00:07:56,580
So I used to work in the games industry.

117
00:07:56,580 --> 00:08:01,420
So for five years, I programmed games for my first job.

118
00:08:01,420 --> 00:08:05,980
So it was an amazing opportunity to get involved in a startup company.

119
00:08:05,980 --> 00:08:12,260
And so I was involved in building AI at that time.

120
00:08:12,260 --> 00:08:19,580
And so for sure, there was a sense of building, handcrafted, what people used to call AI in

121
00:08:19,580 --> 00:08:23,820
the games industry, which I think is not really what we might think of as AI in its fullest

122
00:08:24,180 --> 00:08:31,460
sense, but something which is able to take actions in a way which makes things interesting

123
00:08:31,460 --> 00:08:35,220
and challenging for the human player.

124
00:08:35,220 --> 00:08:40,380
And at that time, I was able to build these handcrafted agents, which in certain limited

125
00:08:40,380 --> 00:08:46,740
cases could do things which were able to do better than me, but mostly in these kind of

126
00:08:46,740 --> 00:08:51,260
twitch-like scenarios where they were able to do things faster or because they had some

127
00:08:51,260 --> 00:08:55,420
pattern which was able to exploit repeatedly.

128
00:08:55,420 --> 00:09:01,780
I think if we're talking about real AI, the first experience for me came after that when

129
00:09:01,780 --> 00:09:08,380
I realized that this path I was on wasn't taking me towards, it wasn't dealing with

130
00:09:08,380 --> 00:09:14,740
that bug which I still had inside me to really understand intelligence and try and solve it.

131
00:09:14,740 --> 00:09:20,860
Everything people were doing in games was short-term fixes rather than long-term vision.

132
00:09:20,980 --> 00:09:26,660
So I went back to study for my PhD, which was, finally enough, trying to apply reinforcement

133
00:09:26,660 --> 00:09:28,540
learning to the game of Go.

134
00:09:28,540 --> 00:09:33,860
And I built my first Go program using reinforcement learning, a system which would, by trial and

135
00:09:33,860 --> 00:09:40,540
error, play against itself and was able to learn which patterns were actually helpful

136
00:09:40,540 --> 00:09:44,740
to predict whether it was going to win or lose the game and then choose the moves that

137
00:09:44,740 --> 00:09:48,460
led to the combination of patterns that would mean that you're more likely to win.

138
00:09:48,500 --> 00:09:50,540
That system, that system beat me.

139
00:09:51,540 --> 00:09:52,860
And how did that make you feel?

140
00:09:53,380 --> 00:09:54,380
It made me feel good.

141
00:09:57,860 --> 00:10:03,780
It's a mix of a sort of excitement and was there a tinge of sort of almost like a fearful

142
00:10:03,780 --> 00:10:04,780
awe?

143
00:10:04,780 --> 00:10:17,780
It's like in 2001 Space Odyssey kind of realizing that you've created something that's achieved

144
00:10:17,820 --> 00:10:21,220
human-level intelligence in this one particular little task.

145
00:10:21,220 --> 00:10:24,460
And in that case, I suppose neural networks weren't involved.

146
00:10:24,460 --> 00:10:26,940
There were no neural networks in those days.

147
00:10:26,940 --> 00:10:33,260
This was pre-deep learning revolution, but it was a principled self-learning system based

148
00:10:33,260 --> 00:10:40,340
on a lot of the principles which people still use in deep reinforcement learning.

149
00:10:40,340 --> 00:10:41,340
How did I feel?

150
00:10:41,340 --> 00:10:50,100
I think I found it immensely satisfying that a system which was able to learn from first

151
00:10:50,100 --> 00:10:56,380
principles for itself was able to reach the point that it was understanding this domain

152
00:10:56,380 --> 00:11:00,100
better than I could and able to outwit me.

153
00:11:00,100 --> 00:11:01,580
I don't think it was a sense of awe.

154
00:11:01,580 --> 00:11:09,020
It was a sense that satisfaction, that something I felt should work, had worked.

155
00:11:09,060 --> 00:11:14,580
To me AlphaGo, and I don't know how else to put it, but to me AlphaGo and AlphaGo Zero

156
00:11:14,580 --> 00:11:20,660
mastering the game of Go is, again, to me the most profound and inspiring moment in the

157
00:11:20,660 --> 00:11:23,620
history of artificial intelligence.

158
00:11:23,620 --> 00:11:26,740
So you're one of the key people behind this achievement.

159
00:11:26,740 --> 00:11:32,700
And I'm Russian, so I really felt the first sort of seminal achievement when Deep Blue

160
00:11:32,700 --> 00:11:36,940
be Gare Kasparov in 1997.

161
00:11:36,940 --> 00:11:42,740
So as far as I know, the AI community at that point largely saw the game of Go as unbeatable

162
00:11:42,740 --> 00:11:49,100
in AI using the state-of-the-art to brute-force search methods.

163
00:11:49,100 --> 00:11:54,940
Even if you consider, at least the way I saw it, even if you consider arbitrary exponential

164
00:11:54,940 --> 00:12:02,660
scaling of compute, Go would still not be solvable, hence why it was thought to be impossible.

165
00:12:02,660 --> 00:12:09,580
So given that the game of Go was impossible to master, when was the dream for you, you

166
00:12:09,580 --> 00:12:14,580
just mentioned your PhD thesis of building the system that plays Go, when was the dream

167
00:12:14,580 --> 00:12:20,300
for you that you could actually build a computer program that achieves the world class, not

168
00:12:20,300 --> 00:12:25,180
necessarily beats the world champion, but achieves that kind of level of playing Go?

169
00:12:25,180 --> 00:12:26,180
First of all, thank you.

170
00:12:26,180 --> 00:12:28,700
That was very kind words.

171
00:12:28,700 --> 00:12:34,660
Funnily enough, I just came from a panel where I was actually in a conversation with

172
00:12:34,660 --> 00:12:39,540
Gare Kasparov and Murray Campbell, who was the author of Deep Blue, and it was their

173
00:12:39,540 --> 00:12:45,180
first meeting together since the match, so I'm just acquired yesterday, so I'm literally

174
00:12:45,180 --> 00:12:47,500
fresh from that experience.

175
00:12:47,500 --> 00:12:51,980
So these are amazing moments when they happen, but where did it all start?

176
00:12:51,980 --> 00:12:56,260
Well, for me, it started when I became fascinated in the game of Go.

177
00:12:56,260 --> 00:13:01,860
So Go, for me, I've grown up playing games, I've always had a fascination in board games.

178
00:13:01,860 --> 00:13:06,220
I played chess as a kid, I played Scrabble as a kid.

179
00:13:06,220 --> 00:13:10,620
When I was at university, I discovered the game of Go, and to me, it just blew all of

180
00:13:10,620 --> 00:13:11,620
those other games out of the water.

181
00:13:11,620 --> 00:13:17,980
It was just so deep and profound in its complexity with endless levels to it.

182
00:13:17,980 --> 00:13:27,380
What I discovered was that I could devote endless hours to this game, and I knew in

183
00:13:27,380 --> 00:13:30,740
my heart of hearts that no matter how many hours I would devote to it, I would never

184
00:13:30,740 --> 00:13:38,180
become a grandmaster, or there was another path, and the other path was to try and understand

185
00:13:38,180 --> 00:13:42,660
how you could get some other intelligence to play this game better than I would be able

186
00:13:42,660 --> 00:13:43,660
to.

187
00:13:43,700 --> 00:13:49,300
So even in those days, I had this idea that, what if it was possible to build a program

188
00:13:49,300 --> 00:13:51,220
that could crack this?

189
00:13:51,220 --> 00:13:57,380
And as I started to explore the domain, I discovered that this was really the domain

190
00:13:57,380 --> 00:14:04,580
where people felt deeply that if progress could be made in Go, it would really mean

191
00:14:04,580 --> 00:14:06,420
a giant leap forward for AI.

192
00:14:06,420 --> 00:14:11,060
It was the challenge where all other approaches had failed.

193
00:14:11,060 --> 00:14:16,820
This is coming out of the era you mentioned, which was in some sense the golden era for

194
00:14:16,820 --> 00:14:20,020
the classical methods of AI, like heuristic search.

195
00:14:20,020 --> 00:14:26,780
In the 90s, they all fell one after another, not just chess with deep blue, but checkers,

196
00:14:26,780 --> 00:14:28,980
batgammon, Othello.

197
00:14:28,980 --> 00:14:36,620
There were numerous cases where systems built on top of heuristic search methods with these

198
00:14:36,620 --> 00:14:40,700
high-performance systems had been able to defeat the human world champion in each of

199
00:14:40,700 --> 00:14:42,100
those domains.

200
00:14:42,100 --> 00:14:49,380
And yet, in that same time period, there was a million-dollar prize available for the

201
00:14:49,380 --> 00:14:52,980
game of Go, for the first system to be a human professional player.

202
00:14:52,980 --> 00:14:58,500
And at the end of that time period, at year 2000, when the prize expired, the strongest

203
00:14:58,500 --> 00:15:02,820
Go program in the world was defeated by a nine-year-old child.

204
00:15:02,820 --> 00:15:06,940
When that nine-year-old child was giving nine free moves to the computer at the start of

205
00:15:06,940 --> 00:15:09,980
the game to try and even things up.

206
00:15:09,980 --> 00:15:18,260
And computer Go expert beat that same strongest program with 29 handicap stones, 29 free moves.

207
00:15:18,260 --> 00:15:23,940
So that's what the state of affairs was when I became interested in this problem in around

208
00:15:23,940 --> 00:15:29,620
2003 when I started working on computer Go.

209
00:15:29,620 --> 00:15:30,620
There was nothing.

210
00:15:30,620 --> 00:15:36,660
There was just very, very little in the way of progress towards meaningful performance

211
00:15:36,660 --> 00:15:39,300
again at anything approaching human level.

212
00:15:39,300 --> 00:15:45,060
And so it wasn't through lack of effort people have tried many, many things.

213
00:15:45,060 --> 00:15:50,860
And so there was a strong sense that something different would be required for Go than had

214
00:15:50,860 --> 00:15:54,340
been needed for all of these other domains where AI had been successful.

215
00:15:54,340 --> 00:16:00,900
And maybe the single clearest example is that Go, unlike those other domains, had this kind

216
00:16:00,900 --> 00:16:07,100
of intuitive property that a Go player would look at a position and say, hey, here's this

217
00:16:07,100 --> 00:16:09,780
mess of black and white stones.

218
00:16:09,780 --> 00:16:15,980
But from this mess, oh, I can predict that this part of the board has become my territory,

219
00:16:15,980 --> 00:16:19,940
this part of the board has become your territory, and I've got this overall sense that I'm going

220
00:16:19,940 --> 00:16:22,500
to win and that this is about the right move to play.

221
00:16:22,500 --> 00:16:28,300
And that intuitive sense of judgment of being able to evaluate what's going on in a position,

222
00:16:28,300 --> 00:16:32,900
it was pivotal to humans being able to play this game and something that people had no

223
00:16:32,900 --> 00:16:35,180
idea how to put into computers.

224
00:16:35,180 --> 00:16:40,020
So this question of how to evaluate a position, how to come up with these intuitive judgments

225
00:16:40,020 --> 00:16:48,380
was the key reason why Go was so hard in addition to its enormous search space and the reason

226
00:16:48,380 --> 00:16:53,100
why methods which had succeeded so well elsewhere failed in Go.

227
00:16:53,100 --> 00:16:59,060
And so people really felt deep down that in order to crack Go, we would need to get something

228
00:16:59,060 --> 00:17:00,580
akin to human intuition.

229
00:17:00,580 --> 00:17:06,060
And if we got something akin to human intuition, we'd be able to solve many, many more problems

230
00:17:06,060 --> 00:17:07,060
in AI.

231
00:17:07,060 --> 00:17:11,260
So for me, that was the moment where it's like, okay, this is not just about playing

232
00:17:11,260 --> 00:17:12,260
the game of Go.

233
00:17:12,260 --> 00:17:13,740
This is about something profound.

234
00:17:13,740 --> 00:17:17,820
And it was back to that bug which had been itching me all those years.

235
00:17:17,820 --> 00:17:22,900
This is the opportunity to do something meaningful and transformative and I guess a dream was

236
00:17:22,900 --> 00:17:23,900
born.

237
00:17:23,900 --> 00:17:25,420
That's a really interesting way to put it.

238
00:17:25,420 --> 00:17:30,900
So almost this realization that you need to find formulate Go as a kind of a prediction

239
00:17:30,900 --> 00:17:34,780
problem versus a search problem was the intuition.

240
00:17:34,780 --> 00:17:43,580
I mean, maybe that's the wrong crude term, but to give it the ability to kind of intuit

241
00:17:43,580 --> 00:17:47,140
things about positional structure of the board.

242
00:17:47,140 --> 00:17:51,060
Now, okay, but what about the learning part of it?

243
00:17:51,900 --> 00:17:57,540
Did you have a sense that learning has to be part of the system?

244
00:17:57,540 --> 00:18:03,380
Again, something that hasn't really, as far as I think, except with TD Gammon and the

245
00:18:03,380 --> 00:18:08,820
90s with RL a little bit, hasn't been part of those day-to-day art game-playing systems?

246
00:18:08,820 --> 00:18:15,180
So I strongly felt that learning would be necessary and that's why my PhD topic back

247
00:18:15,180 --> 00:18:20,220
then was trying to apply reinforcement learning to the game of Go.

248
00:18:20,380 --> 00:18:26,060
I'm not just learning of any type, but I felt that the only way to really have a system

249
00:18:26,060 --> 00:18:31,140
to progress beyond human levels of performance wouldn't just be to mimic how humans do it,

250
00:18:31,140 --> 00:18:34,020
but to understand for themselves.

251
00:18:34,020 --> 00:18:39,140
How else can a machine hope to understand what's going on except through learning?

252
00:18:39,140 --> 00:18:40,540
If you're not learning, what else are you doing?

253
00:18:40,540 --> 00:18:45,540
Well, you're putting all the knowledge into the system and that just feels like something

254
00:18:45,540 --> 00:18:52,300
which decades of AI have told us is maybe not a dead end, but certainly has a ceiling

255
00:18:52,300 --> 00:18:53,300
to the capabilities.

256
00:18:53,300 --> 00:18:56,580
It's known as the knowledge acquisition bottleneck.

257
00:18:56,580 --> 00:19:01,060
The more you try to put into something, the more brittle the system becomes.

258
00:19:01,060 --> 00:19:02,860
So you just have to have learning.

259
00:19:02,860 --> 00:19:03,860
You have to have learning.

260
00:19:03,860 --> 00:19:08,980
That's the only way you're going to be able to get a system which has sufficient knowledge

261
00:19:08,980 --> 00:19:14,180
in it, millions and millions of pieces of knowledge, billions, trillions, of a form

262
00:19:14,220 --> 00:19:17,620
that it can actually apply for itself and understand how those billions and trillions

263
00:19:17,620 --> 00:19:22,380
of pieces of knowledge can be leveraged in a way which will actually lead it towards

264
00:19:22,380 --> 00:19:26,340
its goal without conflict or other issues.

265
00:19:26,340 --> 00:19:27,340
Yeah.

266
00:19:27,340 --> 00:19:33,580
I mean, if I put myself back in that time, I just wouldn't think like that without a

267
00:19:33,580 --> 00:19:34,900
good demonstration of RL.

268
00:19:34,900 --> 00:19:42,780
I would think more in the symbolic AI, like the not learning, but sort of a simulation

269
00:19:42,780 --> 00:19:50,500
of knowledge base, like a growing knowledge base, but it would still be sort of pattern-based,

270
00:19:50,500 --> 00:19:55,740
like basically have little rules that you kind of assemble together into a large knowledge

271
00:19:55,740 --> 00:19:56,740
base.

272
00:19:56,740 --> 00:19:59,900
Well, in a sense, that was the state of the art back then.

273
00:19:59,900 --> 00:20:05,460
So if you look at the Go programs which had been competing for this prize I mentioned,

274
00:20:05,460 --> 00:20:11,300
they were an assembly of different specialized systems, some of which used huge amounts of

275
00:20:11,340 --> 00:20:16,300
human knowledge to describe how you should play the opening, how you should all the different

276
00:20:16,300 --> 00:20:23,700
patterns that were required to play well in the game of Go, end game theory, combinatorial

277
00:20:23,700 --> 00:20:29,260
game theory, and combined with more principled search-based methods which were trying to

278
00:20:29,260 --> 00:20:36,940
solve for particular sub-parts of the game, like life and death, connecting groups together,

279
00:20:36,940 --> 00:20:42,460
all these amazing sub-problems that just emerged in the game of Go, there were different pieces

280
00:20:42,460 --> 00:20:49,380
all put together into this collage which together would try and play against a human.

281
00:20:49,380 --> 00:20:56,300
And although not all of the pieces were handcrafted, the overall effect was nevertheless still

282
00:20:56,300 --> 00:21:00,340
brittle and it was hard to make all these pieces work well together.

283
00:21:00,340 --> 00:21:05,500
And so really what I was pressing for and the main innovation of the approach I took

284
00:21:05,580 --> 00:21:11,900
was to go back to first principles and say, well, let's back off that and try and find a

285
00:21:11,900 --> 00:21:18,540
principled approach where the system can learn for itself just from the outcome, like, you know,

286
00:21:18,540 --> 00:21:22,620
learn for itself if you try something, did that help or did it not help?

287
00:21:22,620 --> 00:21:27,820
And only through that procedure can you arrive at knowledge which is verified,

288
00:21:27,820 --> 00:21:32,060
the system has to verify it for itself, not relying on any other third party to say this

289
00:21:32,060 --> 00:21:39,020
is right or this is wrong. And so that principle was already very important in those days,

290
00:21:39,020 --> 00:21:42,220
that unfortunately we were missing some important pieces back then.

291
00:21:43,180 --> 00:21:49,020
So before we dive into maybe discussing the beauty of reinforcement learning,

292
00:21:49,020 --> 00:21:55,340
let's take a step back, we kind of skipped it a bit, but the rules of the game of Go,

293
00:21:55,900 --> 00:22:06,300
what the elements of it perhaps contrasting to chess that sort of you really enjoy as a human

294
00:22:06,300 --> 00:22:12,300
being and also that make it really difficult as a AI machine learning problem.

295
00:22:12,940 --> 00:22:19,020
So the game of Go has remarkably simple rules. In fact, so simple that people have speculated

296
00:22:19,020 --> 00:22:23,660
that if we were to meet alien life at some point that we wouldn't be able to communicate with them,

297
00:22:23,660 --> 00:22:27,660
but we would be able to play Go with them, they probably have discovered the same ruleset.

298
00:22:28,860 --> 00:22:34,140
So the game is played on a 19 by 19 grid, and you play on the intersections of the grid and

299
00:22:34,140 --> 00:22:40,300
the players take turns. And the aim of the game is very simple, it's to surround as much territory

300
00:22:40,300 --> 00:22:45,340
as you can as many of these intersections with your stones and to surround more than your opponent

301
00:22:45,340 --> 00:22:50,380
does. And the only nuance to the game is that if you fully surround your opponent's piece,

302
00:22:50,380 --> 00:22:53,500
then you get to capture it and remove it from the board and it counts as your own territory.

303
00:22:54,300 --> 00:22:59,020
Now, from those very simple rules, immense complexity arises. There's kind of profound

304
00:22:59,020 --> 00:23:05,900
strategies in how to surround territory, how to kind of trade off between making solid territory

305
00:23:05,900 --> 00:23:10,780
yourself now, compared to building up influence that will help you acquire territory later in

306
00:23:10,780 --> 00:23:14,060
the game, how to connect groups together, how to keep your own groups alive,

307
00:23:14,220 --> 00:23:20,620
which patterns of stones are most useful compared to others.

308
00:23:21,340 --> 00:23:27,020
There's just immense knowledge and human Go players have played this game for,

309
00:23:27,020 --> 00:23:31,260
it was discovered thousands of years ago, and human Go players have built up this immense

310
00:23:31,260 --> 00:23:37,180
knowledge base over the years. It's studied very deeply and played by something like 50 million

311
00:23:37,180 --> 00:23:42,700
players across the world, mostly in China, Japan and Korea, where it's an important part of the

312
00:23:42,700 --> 00:23:48,380
culture, so much so that it's considered one of the four ancient arts that was required by

313
00:23:48,380 --> 00:23:53,180
Chinese scholars. So, there's a deep history there. But there's interesting quality. So,

314
00:23:53,740 --> 00:23:59,260
if I were to compare to chess, chess is in the same way as it is in Chinese culture for Go,

315
00:23:59,260 --> 00:24:05,980
and chess in Russia is also considered one of the sacred arts. So, if we contrast Go with

316
00:24:05,980 --> 00:24:11,260
chess, there's interesting qualities about Go. Maybe you can correct me if I'm wrong, but the

317
00:24:12,220 --> 00:24:21,340
evaluation of a particular static board is not as reliable. In chess, you can kind of assign

318
00:24:21,340 --> 00:24:27,500
points to the different units, and it's kind of a pretty good measure of who's winning, who's

319
00:24:27,500 --> 00:24:33,260
losing. It's not so clear. So, in the game of Go, you find yourself in a situation where

320
00:24:33,260 --> 00:24:38,300
both players have played the same number of stones, actually captures a strong level of play

321
00:24:38,300 --> 00:24:41,740
happen very rarely, which means that at any moment in the game, you've got the same number

322
00:24:41,740 --> 00:24:46,220
of white stones and black stones. And the only thing which differentiates how well you're doing

323
00:24:46,220 --> 00:24:51,740
is this intuitive sense of where are the territories ultimately going to form on this board?

324
00:24:52,380 --> 00:25:00,060
And if you look at the complexity of a real Go position, it's mind-boggling that kind of

325
00:25:00,060 --> 00:25:05,100
question of what will happen in 300 moves from now when you see just a scattering of 20

326
00:25:05,100 --> 00:25:13,180
white and black stones intermingled. And so, that challenge is the reason why

327
00:25:13,180 --> 00:25:18,300
position evaluation is so hard in Go compared to other games. In addition to that, it has an

328
00:25:18,300 --> 00:25:24,620
enormous search space. So, there's around 10 to 170 positions in the game of Go. That's an

329
00:25:24,620 --> 00:25:30,380
astronomical number. And that search space is so great that traditional heuristic search methods

330
00:25:30,380 --> 00:25:35,660
that were so successful and things like Deep Blue and chess programs just kind of fall over in Go.

331
00:25:37,420 --> 00:25:43,820
Which point did reinforcement learning enter your life, your research life, your way of thinking?

332
00:25:43,820 --> 00:25:48,620
We just talked about learning, but reinforcement learning is a very particular kind of learning,

333
00:25:49,580 --> 00:25:55,260
one that's both philosophically sort of profound, but also one that's pretty difficult to get to

334
00:25:55,260 --> 00:26:00,700
work as if you look back in the early days. So, when did that enter your life and how did

335
00:26:00,700 --> 00:26:06,940
that work progress? So, I had just finished working in the games industry at this startup

336
00:26:06,940 --> 00:26:14,780
company. And I took a year out to discover for myself exactly which path I wanted to take. I

337
00:26:14,780 --> 00:26:19,740
knew I wanted to study intelligence, but I wasn't sure what that meant at that stage. I really

338
00:26:19,740 --> 00:26:23,500
didn't feel I had the tools to decide on exactly which path I wanted to follow.

339
00:26:23,580 --> 00:26:31,100
So, during that year, I read a lot. And one of the things I read was Saturn and Bartow,

340
00:26:31,100 --> 00:26:38,220
the sort of seminal textbook on an introduction to reinforcement learning. And when I read that

341
00:26:38,220 --> 00:26:46,700
textbook, I just had this resonating feeling that this is what I understood intelligence to be.

342
00:26:47,660 --> 00:26:54,540
And this was the path that I felt would be necessary to go down to make progress in AI.

343
00:26:55,740 --> 00:27:03,740
So, I got in touch with Rich Saturn and asked him if he would be interested in supervising me

344
00:27:03,740 --> 00:27:15,420
on a PhD thesis in computer go. And he basically said that if he's still alive, he'd be happy to.

345
00:27:16,860 --> 00:27:22,300
But unfortunately, he'd been struggling with very serious cancer for some years. And he really

346
00:27:22,300 --> 00:27:27,420
wasn't confident at that stage that he'd even be around to see the end of it. But fortunately,

347
00:27:27,420 --> 00:27:32,940
that part of the story worked out very happily. And I found myself out there in Alberta. They've

348
00:27:32,940 --> 00:27:39,020
got a great games group out there with a history of fantastic work in board games as well as Rich

349
00:27:39,020 --> 00:27:44,380
Saturn, the father of RL. So, it was the natural place for me to go in some sense to study this

350
00:27:44,380 --> 00:27:55,020
question. And the more I looked into it, the more strongly I felt that this wasn't just the path to

351
00:27:55,020 --> 00:27:59,740
progress in computer go. But really, this was the thing I'd been looking for. This was

352
00:28:03,740 --> 00:28:12,780
really an opportunity to frame what intelligence means. What are the goals of AI in a single

353
00:28:13,100 --> 00:28:17,180
problem definition such that if we're able to solve that clear single problem definition,

354
00:28:18,700 --> 00:28:24,780
in some sense, we've cracked the problem of AI? So, to you, reinforcement learning ideas,

355
00:28:24,780 --> 00:28:31,260
at least sort of echoes of it, would be at the core of intelligence. It is at the core of intelligence.

356
00:28:31,260 --> 00:28:36,540
And if we ever create a human level intelligence system, it would be at the core of that kind of

357
00:28:36,540 --> 00:28:41,660
system. Let me say it this way that I think it's helpful to separate out the problem from the solution.

358
00:28:42,300 --> 00:28:49,580
So, I see the problem of intelligence, I would say it can be formalized as the reinforcement

359
00:28:49,580 --> 00:28:56,060
learning problem. And that that formalization is enough to capture most, if not all of the things

360
00:28:56,060 --> 00:29:01,660
that we mean by intelligence, that they can all be brought within this framework and gives us

361
00:29:01,660 --> 00:29:08,540
a way to access them in a meaningful way that allows us as scientists to understand intelligence

362
00:29:08,540 --> 00:29:16,220
and us as computer scientists to build them. And so, in that sense, I feel that it gives us a path,

363
00:29:16,220 --> 00:29:24,860
maybe not the only path, but a path towards AI. And so, do I think that any system in the future

364
00:29:24,860 --> 00:29:30,620
that's, you know, solved AI would have to have RL within it? Well, I think if you ask that,

365
00:29:30,620 --> 00:29:35,500
you're asking about the solution methods. I would say that if we have such a thing,

366
00:29:35,500 --> 00:29:41,100
it would be a solution to the RL problem. Now, what particular methods have been used to get there?

367
00:29:41,100 --> 00:29:44,540
Well, we should keep an open mind about the best approaches to actually solve any problem.

368
00:29:45,660 --> 00:29:50,700
And, you know, the things we have right now for reinforcement learning, maybe they, maybe

369
00:29:51,820 --> 00:29:55,260
I believe they've got a lot of legs, but maybe we're missing some things. Maybe there's going

370
00:29:55,260 --> 00:30:01,340
to be better ideas. I think we should keep, you know, let's remain modest and we're at the early

371
00:30:01,340 --> 00:30:06,300
days of this field and there are many amazing discoveries ahead of us. For sure. The specifics,

372
00:30:06,300 --> 00:30:11,180
especially of the different kinds of RL approaches currently, there could be other things that fall

373
00:30:11,180 --> 00:30:17,660
into the very large umbrella of RL. But if it's, if it's okay, can we take a step back and kind of

374
00:30:17,660 --> 00:30:24,140
ask the basic question of what is, do you, reinforcement learning? So, reinforcement learning

375
00:30:24,140 --> 00:30:34,140
is the study and the science and the problem of intelligence in the form of an agent that

376
00:30:34,140 --> 00:30:37,580
interacts with an environment. So, the problem you're trying to solve is represented by some

377
00:30:37,580 --> 00:30:42,940
environment like the world in which that agent is situated. And the goal of RL is clear that the

378
00:30:42,940 --> 00:30:48,060
agent gets to take actions. Those actions have some effect on the environment and the environment

379
00:30:48,060 --> 00:30:53,180
gives back an observation to the agent saying, you know, this is what you see or sense. And one

380
00:30:53,180 --> 00:30:57,980
special thing which it gives back is called the reward signal, how well it's doing in the environment.

381
00:30:57,980 --> 00:31:05,180
And the reinforcement learning problem is to simply take actions over time so as to maximize

382
00:31:05,180 --> 00:31:14,540
that reward signal. So, a couple of basic questions. What types of RL approaches are there? So,

383
00:31:14,540 --> 00:31:21,500
I don't know if there's a nice brief inwards way to paint the picture of sort of value-based,

384
00:31:21,500 --> 00:31:28,460
model-based, policy-based reinforcement learning. Yeah. So, now if we think about, okay, so there's

385
00:31:28,460 --> 00:31:33,420
this ambitious problem definition of RL. It's really, you know, it's truly ambitious. It's

386
00:31:33,420 --> 00:31:37,740
trying to capture and encircle all of the things in which an agent interacts with an environment and

387
00:31:37,740 --> 00:31:42,700
say, well, how can we formalize and understand what it means to crack that? Now, let's think about

388
00:31:42,700 --> 00:31:47,500
the solution method. Well, how do you solve a really hard problem like that? Well, one approach

389
00:31:47,500 --> 00:31:54,140
you can take is to decompose that very hard problem into pieces that work together to solve

390
00:31:54,140 --> 00:31:59,580
that hard problem. And so, you can kind of look at the decomposition that's inside the agent's

391
00:31:59,580 --> 00:32:04,540
head, if you like, and ask, well, what form does that decomposition take? And some of the most

392
00:32:04,540 --> 00:32:09,500
common pieces that people use when they're kind of putting the solution method together,

393
00:32:09,500 --> 00:32:14,300
some of the most common pieces that people use are whether or not that solution has a value

394
00:32:14,380 --> 00:32:18,540
function. That means, is it trying to predict, explicitly trying to predict how much reward

395
00:32:18,540 --> 00:32:23,740
it will get in the future? Does it have a representation of a policy? That means something

396
00:32:23,740 --> 00:32:28,380
which is deciding how to pick actions. Is that decision-making process explicitly represented?

397
00:32:29,100 --> 00:32:34,380
And is there a model in the system? Is there something which is explicitly trying to predict

398
00:32:34,380 --> 00:32:41,020
what will happen in the environment? And so, those three pieces are, to me, some of the most

399
00:32:41,020 --> 00:32:48,540
common building blocks. And I understand the different choices in RL as choices of whether

400
00:32:48,540 --> 00:32:51,900
or not to use those building blocks when you're trying to decompose the solution.

401
00:32:52,540 --> 00:32:57,100
Should I have a value function represented? Should I have a policy represented? Should I have a

402
00:32:57,100 --> 00:33:01,100
model represented? And there are combinations of those pieces and, of course, other things that

403
00:33:01,100 --> 00:33:05,820
you could add into the picture as well. But those three fundamental choices give rise to some of

404
00:33:05,820 --> 00:33:10,060
the branches of RL with which we are very familiar. And so, those, as you mentioned,

405
00:33:10,940 --> 00:33:18,940
there is the choice of what's specified or modeled explicitly. And the idea is that

406
00:33:19,740 --> 00:33:25,020
all of these are somehow implicitly learned within the system. So, it's almost a choice of

407
00:33:26,620 --> 00:33:32,460
how you approach a problem. Do you see those as fundamental differences or these almost like

408
00:33:33,420 --> 00:33:37,980
small specifics, like the details of how you solve the problem, but they're not fundamentally

409
00:33:37,980 --> 00:33:45,980
different from each other? I think the fundamental idea is maybe at the higher level, the fundamental

410
00:33:45,980 --> 00:33:52,540
idea is the first step of the decomposition is really to say, well, how are we really going to

411
00:33:52,540 --> 00:33:57,820
solve any kind of problem where you're trying to figure out how to take actions and just from this

412
00:33:58,380 --> 00:34:02,220
stream of observations, you know, you've got some agent situated in its sensory motor stream and

413
00:34:03,100 --> 00:34:06,220
getting all these observations in, getting to take these actions, and what should it do? How

414
00:34:06,220 --> 00:34:09,900
can you even broach that problem? You know, maybe the complexity of the world is so great

415
00:34:10,700 --> 00:34:15,100
that you can't even imagine how to build a system that would understand how to deal with that.

416
00:34:15,660 --> 00:34:20,060
And so, the first step of this decomposition is to say, well, you have to learn. The system has to

417
00:34:20,060 --> 00:34:25,980
learn for itself. And so, note that the reinforcement learning problem doesn't actually stipulate

418
00:34:25,980 --> 00:34:29,660
that you have to learn, like you could maximize your rewards without learning, it would just

419
00:34:30,220 --> 00:34:36,140
wouldn't do a very good job of it. So, learning is required because it's the only way to achieve

420
00:34:36,140 --> 00:34:42,140
good performance in any sufficiently large and complex environment. So, that's the first step.

421
00:34:42,140 --> 00:34:46,940
And so, that step gives commonality to all of the other pieces, because now you might ask, well,

422
00:34:46,940 --> 00:34:51,180
what should you be learning? What does learning even mean? You know, in this sense,

423
00:34:51,340 --> 00:34:57,500
learning might mean, well, you're trying to update the parameters of some system, which

424
00:34:58,300 --> 00:35:02,460
is then the thing that actually picks the actions. And those parameters could be

425
00:35:02,460 --> 00:35:07,420
representing anything. They could be parameterizing a value function or a model or a policy.

426
00:35:08,460 --> 00:35:12,380
And so, in that sense, there's a lot of commonality in that whatever is being represented there is

427
00:35:12,380 --> 00:35:16,780
the thing which is being learned, and it's being learned with the ultimate goal of maximizing rewards.

428
00:35:17,420 --> 00:35:22,700
But the way in which you decompose the problem is really what gives the semantics to the whole

429
00:35:22,700 --> 00:35:28,460
system. Like, are you trying to learn something to predict well, like a value function or a model?

430
00:35:28,460 --> 00:35:33,660
Are you learning something to perform well, like a policy? And the form of that objective,

431
00:35:33,660 --> 00:35:38,700
like, is kind of giving the semantics to the system. And so, it really is, at the next level

432
00:35:38,700 --> 00:35:43,820
down, a fundamental choice. And we have to make those fundamental choices as system designers,

433
00:35:43,820 --> 00:35:48,700
or enable our algorithms to be able to learn how to make those choices for themselves.

434
00:35:49,260 --> 00:35:55,900
So, then the next step you mentioned, the very first thing you have to deal with is,

435
00:35:55,900 --> 00:36:01,660
can you even take in this huge stream of observations and do anything with it? So,

436
00:36:01,660 --> 00:36:08,060
the natural next basic question is, what is the, what is deeper enforcement learning?

437
00:36:08,060 --> 00:36:13,820
And what is this idea of using neural networks to deal with this huge incoming stream?

438
00:36:14,460 --> 00:36:19,980
So, amongst all the approaches for reinforcement learning, deep reinforcement learning is one

439
00:36:20,940 --> 00:36:30,220
family of solution methods that tries to utilize powerful representations that are offered by

440
00:36:30,220 --> 00:36:37,820
neural networks to represent any of these different components of the solution, of the agent.

441
00:36:37,820 --> 00:36:42,620
Like, whether it's the value function, or the model, or the policy, the idea of deep learning

442
00:36:42,620 --> 00:36:48,220
is to say, well, here's a powerful toolkit that's so powerful that it's universal in the sense that

443
00:36:48,220 --> 00:36:53,580
it can represent any function, and it can learn any function. And so, if we can leverage that

444
00:36:53,580 --> 00:36:59,100
universality, that means that whatever we need to represent for our policy, or for our value

445
00:36:59,100 --> 00:37:04,700
function for a model, deep learning can do it. So, that deep learning is one approach

446
00:37:04,780 --> 00:37:11,260
that offers us a toolkit that is, has no ceiling to its performance, that as we start to put more

447
00:37:11,260 --> 00:37:18,220
resources into the system, more memory and more computation, and more data, more experience of

448
00:37:18,220 --> 00:37:22,540
more interactions with the environment, that these are systems that can just get better and better

449
00:37:22,540 --> 00:37:26,300
and better at doing whatever the job is they've asked them to do, whatever we've asked that

450
00:37:26,300 --> 00:37:32,780
function to represent, it can learn a function that does a better and better job of representing

451
00:37:32,780 --> 00:37:37,420
that knowledge, whether that knowledge be estimating how well you're going to do in the

452
00:37:37,420 --> 00:37:42,460
world, the value function, whether it's going to be choosing what to do in the world, the policy,

453
00:37:42,460 --> 00:37:46,300
or whether it's understanding the world itself, what's going to happen next, the model.

454
00:37:46,860 --> 00:37:54,780
Nevertheless, the fact that neural networks are able to learn incredibly complex representations

455
00:37:54,780 --> 00:38:01,660
that allow you to do the policy, the model, or the value function is, at least to my mind,

456
00:38:01,660 --> 00:38:09,740
exceptionally beautiful and surprising. Was it surprising to you? Can you still

457
00:38:09,740 --> 00:38:14,700
believe it works as well as it does? Do you have good intuition about why it works at all

458
00:38:14,700 --> 00:38:23,260
and works as well as it does? I think let me take two parts to that question. I think

459
00:38:26,060 --> 00:38:29,740
it's not surprising to me that the idea of reinforcement learning

460
00:38:30,380 --> 00:38:37,340
works because in some sense, I feel it's the only thing which can, ultimately,

461
00:38:37,340 --> 00:38:43,020
and so I feel we have to address it and there must be success as possible because we have

462
00:38:43,020 --> 00:38:50,380
examples of intelligence and it must at some level be able to possible to acquire experience and use

463
00:38:50,380 --> 00:38:56,380
that experience to do better in a way which is meaningful to environments of the complexity

464
00:38:56,380 --> 00:39:01,500
that humans can deal with. It must be. Am I surprised that our current systems can do as well

465
00:39:01,500 --> 00:39:07,100
as they can do? I think one of the big surprises for me and a lot of the community

466
00:39:09,500 --> 00:39:20,940
is really the fact that deep learning can continue to perform so well despite the

467
00:39:20,940 --> 00:39:25,500
facts that these neural networks that they're representing have these incredibly nonlinear

468
00:39:25,820 --> 00:39:32,940
bumpy surfaces which to our low-dimensional intuitions make it feel like surely you're

469
00:39:32,940 --> 00:39:37,260
just going to get stuck and learning will get stuck because you won't be able to make any

470
00:39:37,260 --> 00:39:45,660
further progress and yet the big surprise is that learning continues and these what appear to be

471
00:39:45,660 --> 00:39:50,300
local optima turn out not to be because in high dimensions when we make really big neural nets

472
00:39:50,300 --> 00:39:53,900
there's always a way out and there's a way to go even lower and then

473
00:39:54,620 --> 00:39:58,460
you're still not a local optima because there's some other pathway that will take you out and

474
00:39:58,460 --> 00:40:03,660
take you lower still and so no matter where you are learning can proceed and do better and better

475
00:40:03,660 --> 00:40:12,700
and better without bound and so that is a surprising and beautiful property of neural nets

476
00:40:13,580 --> 00:40:20,220
which I find elegant and beautiful and and somewhat shocking that it turns out to be the case

477
00:40:20,780 --> 00:40:27,580
as you said which I really like to our low-dimensional intuitions that's surprising

478
00:40:28,140 --> 00:40:34,940
yeah yeah we're very we're very tuned to working within a three-dimensional environment and so

479
00:40:34,940 --> 00:40:42,140
to start to visualize what a billion dimensional neural network surface that you're trying to

480
00:40:42,220 --> 00:40:48,300
optimize over what that even looks like is very hard for us and so I think that really if you try

481
00:40:48,300 --> 00:40:56,700
to account for for the essentially the AI winter where where people gave up on neural networks

482
00:40:56,700 --> 00:41:02,380
I think it's really down to that that lack of ability to generalize from from low dimensions

483
00:41:02,380 --> 00:41:06,380
to high dimensions because back then we were in the low-dimensional case people could only

484
00:41:06,380 --> 00:41:13,660
build neural nets with you know 50 nodes in them or something and to to imagine that it might be

485
00:41:13,660 --> 00:41:17,420
possible to build a billion dimensional neural net and that it might have a completely different

486
00:41:17,420 --> 00:41:22,940
qualitatively different property was very hard to anticipate and I think even now we're starting

487
00:41:22,940 --> 00:41:28,860
to build the the theory to support that and and it's incomplete at the moment but all of the

488
00:41:28,860 --> 00:41:34,140
theory seems to be pointing in the direction that indeed this is an approach which which truly is

489
00:41:34,220 --> 00:41:38,460
universal both in its representational capacity which was known but also in its learning ability

490
00:41:38,460 --> 00:41:45,420
which is which is surprising and it makes one wonder what else we're missing due to our low

491
00:41:45,420 --> 00:41:52,460
dimensional intuitions that that will seem obvious once it's discovered I often wonder

492
00:41:53,500 --> 00:42:02,220
you know when we one day do have AI's which are superhuman in their abilities to to understand

493
00:42:02,220 --> 00:42:09,660
the world what will they think of of the algorithms that we developed back now will it be you know

494
00:42:09,660 --> 00:42:17,020
looking back at these these days and you know and and thinking that well will we look back and feel

495
00:42:17,020 --> 00:42:21,580
that these algorithms were were naive first steps or will they still be the fundamental ideas which

496
00:42:21,580 --> 00:42:29,340
are used even in a hundred thousand ten thousand years yeah I know they'll they'll watch back to

497
00:42:29,340 --> 00:42:36,460
this conversation and and uh with a smile maybe a little bit of a laugh I mean my my sense is um

498
00:42:37,580 --> 00:42:45,100
I think it's just like when we used to think that the the sun revolved around the earth

499
00:42:45,900 --> 00:42:52,460
they'll see our systems of today reinforcement learning as too complicated that the answer was

500
00:42:52,460 --> 00:42:59,340
simple all along there's something just just like you said in the game of go I mean I love the

501
00:42:59,340 --> 00:43:04,940
systems of like cellular automata that there's simple rules from which incredible complexity

502
00:43:04,940 --> 00:43:11,740
emerges so it feels like there might be some very least simple approaches just like where Sutton

503
00:43:11,740 --> 00:43:20,620
says right these simple methods with compute over time seem to prove to be the most effective

504
00:43:20,620 --> 00:43:30,940
I 100% agree I think that if we try to anticipate what will generalize well into the future I think

505
00:43:30,940 --> 00:43:36,780
it's likely to be the case that it's the simple clear ideas which will have the longest legs and

506
00:43:36,780 --> 00:43:41,100
which will carry us furthest into the future nevertheless we're in a situation where we need

507
00:43:41,100 --> 00:43:45,900
to make things work right and today and sometimes that requires putting together more complex systems

508
00:43:46,620 --> 00:43:51,500
where we don't have the the full answers yet as to what those minimal ingredients might be

509
00:43:51,500 --> 00:43:59,340
so speaking of which if we could take a step back to go uh what was mogo and what was the key idea

510
00:43:59,340 --> 00:44:06,780
behind the system so back during my um phd on computer go around about that time there was a

511
00:44:06,780 --> 00:44:13,100
a major new development in in which actually happened in the context of computer go and

512
00:44:14,060 --> 00:44:20,140
and it was really a revolution in the way that heuristic search was was done and and the idea was

513
00:44:20,860 --> 00:44:27,260
essentially that um a position could be evaluated or a state in general could be evaluated

514
00:44:28,700 --> 00:44:35,260
not by humans saying whether that um position is good or not or even humans providing rules as to

515
00:44:35,260 --> 00:44:43,580
how you might evaluate it but instead by allowing the system to randomly play out the game until the

516
00:44:43,580 --> 00:44:50,460
end multiple times and taking the average of those outcomes as the prediction of what will happen

517
00:44:50,460 --> 00:44:56,140
so for example if you're in the game of go the intuition is that you take a position and you

518
00:44:56,140 --> 00:45:00,140
get the system to kind of play random moves against itself all the way to the end of the game and you

519
00:45:00,140 --> 00:45:05,740
see who wins and if black ends up winning more of those random games than white well you say hey

520
00:45:05,740 --> 00:45:09,660
this is a position that favors white and if white ends up winning more of those random games than

521
00:45:09,660 --> 00:45:21,180
black then it favors white um so that idea um was known as Monte Carlo um um search and a particular

522
00:45:21,180 --> 00:45:25,980
form of Monte Carlo search that became very effective and was developed in computer go

523
00:45:25,980 --> 00:45:31,820
first by Remy Coulomb in 2006 and then taken further by others was something called Monte

524
00:45:31,820 --> 00:45:39,740
Carlo tree search which basically takes that same idea and uses that that insight to evaluate every

525
00:45:39,740 --> 00:45:45,100
node of a search tree is evaluated by the average of the random playouts from that from that node

526
00:45:45,100 --> 00:45:51,980
onwards um and this idea was very powerful and suddenly led to huge leaps forward in the strength

527
00:45:52,060 --> 00:45:58,460
of computer go playing programs um and among those the the strongest of the go playing programs in

528
00:45:58,460 --> 00:46:04,860
those days was a program called mogo which was the first program to actually reach human master

529
00:46:04,860 --> 00:46:11,020
level on small boards nine by nine boards and so this was a program by someone called sylvan jelly

530
00:46:11,660 --> 00:46:17,420
who's a good colleague of mine but i worked with him a little bit um in those days part of my phd

531
00:46:18,300 --> 00:46:24,460
and mogo was a a first step towards the latest successes we saw in computer go

532
00:46:25,260 --> 00:46:32,860
but it was still missing a key ingredient mogo was evaluating purely by random rollouts against

533
00:46:32,860 --> 00:46:38,940
itself and in a way it's it's truly remarkable that random play should give you anything at all

534
00:46:38,940 --> 00:46:45,020
yeah like why why in this perfectly deterministic game that's very precise and involves these very

535
00:46:45,020 --> 00:46:52,860
exact sequences why is it that that random randomization is is is helpful and so the intuition

536
00:46:52,860 --> 00:46:58,940
is that randomization captures something about the the nature of the of the the search tree that

537
00:46:58,940 --> 00:47:04,140
from a position that you're you're understanding the nature of the search tree um from that node

538
00:47:04,140 --> 00:47:10,540
onwards by by by using randomization and this was a very powerful idea and i've seen this in

539
00:47:10,540 --> 00:47:16,700
other spaces uh i'm going to talk to Richard karp and so on randomized algorithms somehow

540
00:47:16,700 --> 00:47:24,060
magically are able to do exceptionally well and and simplifying the problem somehow makes you wonder

541
00:47:24,060 --> 00:47:30,460
about the fundamental nature of randomness in our universe it seems to be a useful thing but so from

542
00:47:30,460 --> 00:47:38,300
that moment can you maybe tell the origin story in the journey of alpha go yeah so programs based on

543
00:47:38,300 --> 00:47:44,620
Monte Carlo tree search were a first revolution in the sense that they led to um suddenly programs

544
00:47:44,620 --> 00:47:50,940
that could play the game to any reasonable level but they they plateaued it seemed that no matter

545
00:47:50,940 --> 00:47:56,300
how much effort people put into these techniques they couldn't exceed the level of um amateur

546
00:47:56,300 --> 00:48:02,460
dan level go players so strong players but not not anywhere near the level of of professionals

547
00:48:02,460 --> 00:48:09,100
never mind the world champion and so that brings us to the birth of alpha go which happened in the

548
00:48:09,100 --> 00:48:19,180
context of a startup company known as deep mind i heard them where a a project was born and the

549
00:48:19,180 --> 00:48:29,180
project was really a scientific investigation um where um myself and adjo huang and an intern

550
00:48:29,180 --> 00:48:35,180
chris madison were exploring a scientific question and that scientific question was really

551
00:48:37,180 --> 00:48:42,780
is there another fundamentally different approach to to this key question of of go the key challenge

552
00:48:42,780 --> 00:48:48,140
of of how can you build that intuition and how can you just have a system that could look at a

553
00:48:48,140 --> 00:48:53,420
position and understand um what moved to play or or how well you're doing in that position who's

554
00:48:53,420 --> 00:49:01,900
going to win and so the deep learning revolution had just begun that systems like image net had

555
00:49:02,540 --> 00:49:08,540
suddenly been won by deep learning techniques back in 2012 and following that it was natural to ask

556
00:49:08,540 --> 00:49:14,060
well you know if if deep learning is able to scale up so effectively with images to to understand

557
00:49:14,060 --> 00:49:21,420
them enough to to classify them well why not go why why why not take a um uh the black and white

558
00:49:21,500 --> 00:49:26,140
stones of the go board and build some a system which can understand for itself what that means in

559
00:49:26,140 --> 00:49:31,660
terms of what moved to pick or who's going to win the game black or white and so that was our

560
00:49:31,660 --> 00:49:37,580
scientific question which we we were probing and trying to understand and as we started to look at

561
00:49:37,580 --> 00:49:43,740
it we discovered that we could build a a system so in fact our very first paper on alpha go was

562
00:49:43,740 --> 00:49:50,060
actually a pure deep learning system which was trying to answer this question and we showed

563
00:49:50,060 --> 00:49:56,060
that actually a pure deep learning system with no search at all was actually able to reach human

564
00:49:56,620 --> 00:50:03,980
band level master level at the full game of go 19 by 19 boards um and so without any search at all

565
00:50:03,980 --> 00:50:07,180
suddenly we had systems which were playing at the level of the best

566
00:50:08,700 --> 00:50:12,620
Monte Carlo tree set systems the ones with randomized rollouts so first I was sorry to

567
00:50:12,620 --> 00:50:19,660
interrupt but uh that's kind of a groundbreaking notion that's a that's like basically a definitive

568
00:50:19,660 --> 00:50:26,700
step away from the a couple of decades of essentially search dominating AI yeah so what how

569
00:50:26,700 --> 00:50:31,580
did that make you feel would you think it was a surprising from a scientific perspective

570
00:50:32,300 --> 00:50:37,900
in general how to make you feel I I found this to be profoundly surprising um in fact it was so

571
00:50:37,900 --> 00:50:44,140
surprising that um that we had a bet back then and like many good projects you know bets are quite

572
00:50:44,140 --> 00:50:50,940
motivating and and the bet was you know whether it was possible for a a a system based purely on

573
00:50:50,940 --> 00:50:58,460
on deep learning no search at all to beat a a down level human player um and so we had um someone

574
00:50:58,460 --> 00:51:03,740
who joined our team um who was a down level player he came in and um and we had this first

575
00:51:03,740 --> 00:51:09,740
match um against him and which side of the bet were you on by the way did you hit the losing

576
00:51:09,740 --> 00:51:16,460
on the winning side I tend to be an optimist um with the with the power of of of deep learning

577
00:51:16,460 --> 00:51:23,100
and and reinforcement learning so the the system won and we were able to beat this um human down

578
00:51:23,100 --> 00:51:28,300
level player and for me that was the moment where where it was like okay something something special

579
00:51:28,380 --> 00:51:35,660
is afoot here we have a system which um without search is able to to already just look at this

580
00:51:35,660 --> 00:51:41,340
position and understand things as well as a strong human player and from that point onwards

581
00:51:41,340 --> 00:51:49,420
I really felt that um reaching that reaching the top levels of human play you know professional

582
00:51:49,420 --> 00:51:57,820
level well champion level I felt it was actually an inevitability um and and if it was inevitable

583
00:51:58,620 --> 00:52:05,820
outcome I was rather keen that it would be us that achieved it so we scaled up this was something

584
00:52:05,820 --> 00:52:13,340
where you know so had lots of conversations back then with um Demisus Arbus that um um the um head

585
00:52:13,340 --> 00:52:21,100
of of DeepMind who was extremely excited um and we we made the decision to to scale up the project

586
00:52:21,100 --> 00:52:29,740
brought more people on board and and so AlphaGo became something where where we we had a clear

587
00:52:29,740 --> 00:52:36,140
goal which was to try and um crack this outstanding challenge of AI to see if we could beat the world's

588
00:52:36,140 --> 00:52:43,900
best players and this led within the space of um not so many months to playing against the

589
00:52:43,900 --> 00:52:49,100
European champion Fan Hui in a match which became you know memorable in history as the

590
00:52:49,100 --> 00:52:55,420
first time a go program had ever beaten a professional player and at that time we had to

591
00:52:55,420 --> 00:53:00,620
make a judgment as to whether when and and whether we should go and challenge the world

592
00:53:00,620 --> 00:53:06,700
champion and and this was a difficult decision to make again we were basing our predictions on

593
00:53:06,700 --> 00:53:12,780
on our own progress and had to estimate based on the rapidity of our own progress when we thought we

594
00:53:12,780 --> 00:53:19,340
would um exceed the level of the human world champion and and we tried to make an estimate

595
00:53:19,340 --> 00:53:26,140
and set up a match and that became the the AlphaGo versus LisaDoll match in um 2016

596
00:53:27,100 --> 00:53:35,020
and we should say spoiler alert that AlphaGo was able to defeat LisaDoll that's right yeah

597
00:53:35,020 --> 00:53:46,140
so maybe uh we could take even a broader view AlphaGo involves both learning from expert games and

598
00:53:47,980 --> 00:53:52,940
as far as I remember a self-played component to where it learns by playing against itself

599
00:53:54,220 --> 00:54:00,220
but in your sense what was the role of learning from expert games there and in terms of your

600
00:54:00,300 --> 00:54:06,060
self-evaluation whether you can take on the world champion what was the thing that they're trying

601
00:54:06,060 --> 00:54:13,500
to do more of sort of train more on expert games or was there's now another I'm asking so many

602
00:54:14,300 --> 00:54:19,900
poorly phrased questions but uh did you have a hope or dream that self-play would be the

603
00:54:19,900 --> 00:54:28,460
key component at that moment yet so in the early days of AlphaGo we we used human data

604
00:54:28,460 --> 00:54:33,020
to explore the science of what deep learning can achieve and so when we had our first paper

605
00:54:33,020 --> 00:54:38,620
that showed um that it was possible to predict um the winner of the game that it was possible to

606
00:54:38,620 --> 00:54:43,980
suggest moves that was done using human data or solely human data yeah and and and and so the

607
00:54:43,980 --> 00:54:48,220
reason that we did it that way was at that time we were exploring separately the deep learning

608
00:54:48,220 --> 00:54:52,940
aspect from the reinforcement learning aspect that was the part which was which was new and

609
00:54:52,940 --> 00:54:59,900
unknown to to to me at that time was how far could that be stretched once we had that it then

610
00:54:59,900 --> 00:55:05,100
became natural to try and use that same representation and see if we could learn for ourselves using

611
00:55:05,100 --> 00:55:11,340
that same representation and so right from the beginning actually our goal had been to build

612
00:55:11,340 --> 00:55:18,700
a system using self-play and to us the human data right from the beginning was an expedient step

613
00:55:18,700 --> 00:55:23,500
to help us for pragmatic reasons to go faster towards the goals of the project

614
00:55:24,460 --> 00:55:29,420
than we might be able to starting solely from self-play and so in those days we were very

615
00:55:29,420 --> 00:55:34,140
aware that we were choosing to to use human data and that might not be the long-term

616
00:55:35,740 --> 00:55:41,660
holy grail of AI but that it was something which was extremely useful to us it helped us to understand

617
00:55:41,660 --> 00:55:46,700
the system it helped us to build deep learning representations which were clear and simple and

618
00:55:46,700 --> 00:55:53,740
easy to use and so really I would say it's it served a purpose not just as part of the algorithm but

619
00:55:53,740 --> 00:55:59,420
something which I continue to use in our research today which is trying to break down a very hard

620
00:55:59,420 --> 00:56:04,940
challenge into pieces which are easier to understand for us as researchers and develop so if you if you

621
00:56:04,940 --> 00:56:10,940
use a component based on human data it can help you to understand the system such that then you

622
00:56:10,940 --> 00:56:17,740
can build the more principled version later that that does it for itself so as I said the Alpha

623
00:56:17,740 --> 00:56:24,300
Go victory and I don't think I'm being sort of uh romanticizing this notion I think it's one of the

624
00:56:24,300 --> 00:56:30,700
greatest moments in the history of AI so were you cognizant of this magnitude of the accomplishment

625
00:56:31,260 --> 00:56:38,140
at the time I mean are you cognizant of it even now because to me I feel like it's something that

626
00:56:38,140 --> 00:56:43,660
would we mentioned what the AGI systems of the future will look back I think they'll look back at

627
00:56:43,660 --> 00:56:51,580
the Alpha Go victory as like holy crap they figured it out this is where this is where it started

628
00:56:51,580 --> 00:56:56,700
well thank you again I mean it's funny because I guess I've been working on I've been working on

629
00:56:56,700 --> 00:57:01,180
computer go for a long time so I've been working at the time of the Alpha Go match on computer go

630
00:57:01,180 --> 00:57:07,020
for more more than a decade and throughout that decade I'd had this dream of what would it be like

631
00:57:07,020 --> 00:57:12,940
to what would it be like really to to actually be able to build a system that could play against

632
00:57:12,940 --> 00:57:18,060
the world champion and and I imagined that that would be an interesting moment that maybe you

633
00:57:18,060 --> 00:57:22,940
know some people might care about that and that this might be you know a nice achievement

634
00:57:23,980 --> 00:57:31,820
but I think when I arrived in in Seoul and discovered the legions of journalists that were

635
00:57:31,820 --> 00:57:38,060
following us around and 100 million people that were watching the match online live I realized

636
00:57:38,060 --> 00:57:43,260
that I'd been off in my estimation of how significant this moment was by several orders of magnitude

637
00:57:44,300 --> 00:57:53,260
and so there was definitely an adjustment process to to realize that this this was something which

638
00:57:54,220 --> 00:57:58,700
the world really cared about and which was a watershed moment and I think there was that

639
00:57:58,700 --> 00:58:05,180
moment of realization which was also a little bit scary because you know if you go into something

640
00:58:05,180 --> 00:58:10,780
thinking it's going to be maybe of interest and then discover that 100 million people are watching

641
00:58:10,780 --> 00:58:14,380
it suddenly makes you worry about whether some of the decisions you've made were really the

642
00:58:14,380 --> 00:58:19,260
best ones or the wisest or were going to lead to the best outcome and we knew for sure that there

643
00:58:19,260 --> 00:58:23,660
were still imperfections in Alpha Go which were going to be exposed to the whole world watching

644
00:58:24,300 --> 00:58:30,860
and so yeah it was a it was I think a great experience and I feel privileged to have been

645
00:58:30,860 --> 00:58:38,220
part of it, privileged to have led that amazing team, I feel privileged to have been in a moment

646
00:58:38,220 --> 00:58:45,580
of history like you say but also lucky that you know in a sense I was insulated from from the

647
00:58:45,580 --> 00:58:50,380
knowledge of I think it would have been harder to focus on the research if the full kind of reality

648
00:58:50,460 --> 00:58:56,380
of of what was going to come to pass had been known to me and the team I think it was you know

649
00:58:56,380 --> 00:58:59,900
we were in our bubble and we were working on research and we were trying to answer the scientific

650
00:58:59,900 --> 00:59:06,460
questions and then bam you know the public sees it and I think it was it was it was better that

651
00:59:06,460 --> 00:59:12,380
way in retrospect. Were you confident that I guess what were the chances that you could get the win

652
00:59:13,500 --> 00:59:20,140
so just like you said I'm a little bit more familiar with another accomplishment

653
00:59:20,140 --> 00:59:24,540
than we may not even get a chance to talk to I talked to Oriel Venialis about Alpha Star which

654
00:59:24,540 --> 00:59:31,020
is another incredible accomplishment but here you know with Alpha Star and beating the Starcraft

655
00:59:31,020 --> 00:59:36,780
there was like already a track record with Alpha Go this is like the really first time you get to

656
00:59:36,780 --> 00:59:43,340
see reinforcement learning face the best human in the world so what was your confidence like what

657
00:59:43,340 --> 00:59:51,180
was the odds? Well we actually um was there a bet? Funnily enough there was so so just before the

658
00:59:51,180 --> 00:59:57,420
match we weren't betting on anything concrete but we all held out a hand everyone in the team held

659
00:59:57,420 --> 01:00:01,420
out a hand at the beginning of the match and the number of fingers that they had out on that hand

660
01:00:01,420 --> 01:00:06,540
was supposed to represent how many games they thought we would win against Lisa Dahl and there

661
01:00:06,540 --> 01:00:12,300
was an amazing spread in the team's predictions but I have to say I predicted 4-1

662
01:00:15,020 --> 01:00:21,740
and the reason was based purely on data so I'm a scientist first and foremost and one of the things

663
01:00:21,740 --> 01:00:28,460
which we had established was that Alpha Go in around one in five games would develop something

664
01:00:28,460 --> 01:00:32,540
which we called a delusion which was a kind of you know hole in its knowledge where it wasn't

665
01:00:32,540 --> 01:00:37,580
able to fully understand everything about the position and that hole in its knowledge would

666
01:00:37,580 --> 01:00:43,740
persist for tens of moves throughout the game and we knew two things we knew that if there were no

667
01:00:43,740 --> 01:00:49,260
delusions that Alpha Go seemed to be playing at a level that was far beyond any human capabilities

668
01:00:49,260 --> 01:00:57,260
but we also knew that if there were delusions the opposite was true and in fact you know that's

669
01:00:57,420 --> 01:01:03,500
what came to pass we saw all of those outcomes and Lisa Dahl in one of the games played a really

670
01:01:03,500 --> 01:01:11,100
beautiful sequence that Alpha Go just hadn't predicted and after that it led it into this

671
01:01:11,100 --> 01:01:16,620
situation where it was unable to really understand the position fully and found itself in one of

672
01:01:16,620 --> 01:01:22,780
these delusions so indeed 4-1 was the outcome. So yeah and can you maybe speak to it a little bit

673
01:01:22,780 --> 01:01:29,820
more what were the five games like what happened is there interesting things that come to memory

674
01:01:29,820 --> 01:01:36,140
in terms of the play of the human machine? So I remember all of these games vividly of course

675
01:01:36,780 --> 01:01:42,620
you know moments like these don't come too often in the lifetime of a scientist and

676
01:01:42,700 --> 01:01:52,940
the first game was magical because it was the first time that a computer program had

677
01:01:52,940 --> 01:01:58,540
defeated a world champion in this grand challenge of Go and there was a moment where

678
01:02:02,460 --> 01:02:06,380
Alpha Go invaded Lisa Dahl's territory towards the end of the game

679
01:02:07,820 --> 01:02:11,660
and that's quite an audacious thing to do it's like saying hey you thought this was going to be

680
01:02:11,660 --> 01:02:15,020
your territory in the game but I'm going to stick a stone right in the middle of it and

681
01:02:15,020 --> 01:02:21,100
and prove to you that I can break it up and Lisa Dahl's face just dropped he wasn't expecting a

682
01:02:21,100 --> 01:02:29,820
computer to do something that audacious. The second game became famous for a move known as

683
01:02:29,820 --> 01:02:38,540
Move 37 this was a move that was played by Alpha Go that broke all of the conventions of Go that

684
01:02:38,780 --> 01:02:44,300
Go players were so shocked by this they thought that maybe the operator had made a mistake

685
01:02:45,260 --> 01:02:50,460
they thought there was something crazy going on and it just broke every rule that Go players

686
01:02:50,460 --> 01:02:55,020
are taught from a very young age they're just taught you know this kind of move called a shoulder

687
01:02:55,020 --> 01:02:59,980
hit you can only play it on the third line or the fourth line and Alpha Go played it on the fifth

688
01:02:59,980 --> 01:03:05,180
line and it turned out to be a brilliant move and made this beautiful pattern in the middle

689
01:03:05,180 --> 01:03:12,780
of the board that ended up winning the game and so this really was a clear instance where we could

690
01:03:12,780 --> 01:03:19,340
say computers exhibited creativity that this was really a move that was something humans hadn't

691
01:03:20,060 --> 01:03:26,140
known about hadn't anticipated and computers discovered this idea they were the ones to say

692
01:03:26,140 --> 01:03:31,820
actually you know here's a new idea something new not not in the domains of of human knowledge of the

693
01:03:31,820 --> 01:03:39,420
game and and now the humans think this is a reasonable thing to do and and it's part of

694
01:03:39,420 --> 01:03:45,500
Go knowledge now. The third game something special happens when you play against a human

695
01:03:45,500 --> 01:03:51,660
world champion which again I hadn't anticipated before going there which is you know these these

696
01:03:51,660 --> 01:03:58,300
players are amazing Lisa Doll was a true champion 18 time world champion and had this amazing ability

697
01:03:58,300 --> 01:04:07,020
to to probe Alpha Go for weaknesses of any kind and in the third game he was losing and we felt

698
01:04:07,020 --> 01:04:14,860
we were sailing comfortably to victory but he managed to from nothing stir up this fight and

699
01:04:14,860 --> 01:04:22,140
build what's called a double co these kind of repetitive positions and he knew that historically

700
01:04:22,140 --> 01:04:26,620
no no computer Go program had ever been able to deal correctly with double code positions

701
01:04:26,620 --> 01:04:32,220
and he managed to summon one out of out of nothing and so for us you know this was this

702
01:04:32,220 --> 01:04:35,900
was a real challenge like would Alpha Go be able to to to deal with this or would it just kind of

703
01:04:35,900 --> 01:04:41,420
crumble in the face of of this situation and fortunately it dealt with it perfectly. The

704
01:04:41,420 --> 01:04:48,860
fourth game was was amazing in that Lisa doll appeared to be losing this game Alpha Go thought

705
01:04:48,860 --> 01:04:54,940
it was winning and then Lisa doll did something which I think only a true world champion can do

706
01:04:54,940 --> 01:04:59,980
which is he found a brilliant sequence in the middle of the game a brilliant sequence that

707
01:05:01,340 --> 01:05:09,900
led him to really just transform the position it kind of it it he found just a piece of genius

708
01:05:09,900 --> 01:05:17,100
really and after that Alpha Go its its evaluation just tumbled it thought it was winning this game

709
01:05:17,100 --> 01:05:22,380
and all of a sudden it tumbled and said oh now I've got no chance and it starts to behave rather

710
01:05:22,380 --> 01:05:28,860
oddly at that point in the final game for some reason we as a team were convinced having seen

711
01:05:28,860 --> 01:05:33,820
Alpha Go in the previous game suffer from delusions we as a team were convinced

712
01:05:33,820 --> 01:05:37,660
that it was suffering from another delusion we were convinced that it was mis-evaluating the

713
01:05:37,660 --> 01:05:42,780
position and that that something was going terribly wrong and it was only in the last

714
01:05:42,780 --> 01:05:47,420
few moves of the game that we realized that actually although it had been predicting it

715
01:05:47,420 --> 01:05:53,500
was going to win all the way through it really was and um and so somehow you know it just taught us

716
01:05:53,500 --> 01:05:57,820
yet again that you have to have faith in in your systems when they when they exceed your own level

717
01:05:57,820 --> 01:06:03,740
of ability in your own judgment you have to trust in them to to know better than than you the designer

718
01:06:03,740 --> 01:06:10,940
once um you've you've bestowed in them the ability to to judge better than you can then trust the

719
01:06:10,940 --> 01:06:21,180
system to do so. So just like in the case of Deep Blue beating Gary Kasparov so Gary is I think

720
01:06:21,180 --> 01:06:27,180
the first time he's ever lost actually to anybody and I mean there's a similar situation at least

721
01:06:27,180 --> 01:06:37,500
at all it's a it's a tragic it's a tragic loss for humans but a beautiful one I think that's kind of

722
01:06:38,460 --> 01:06:47,500
from the tragedy sort of emerges over time emerges the kind of inspiring story but

723
01:06:48,940 --> 01:06:56,860
Lisa Dahl recently analyses her time and I don't know if we can look too deeply into it but he did

724
01:06:56,860 --> 01:07:03,820
say that even if I become number one there's an entity that cannot be defeated so what do you think

725
01:07:03,820 --> 01:07:08,540
about these words what do you think about his retirement from the game ago? Well let me take

726
01:07:08,540 --> 01:07:12,940
you back first of all to the first part of your comment about Gary Kasparov because actually

727
01:07:12,940 --> 01:07:20,940
at the panel yesterday um he specifically said that when he first lost to Deep Blue he he viewed

728
01:07:20,940 --> 01:07:26,940
it as a failure he viewed that this this had been a failure of his but later on in his career he

729
01:07:26,940 --> 01:07:32,060
said he'd come to realize that actually it was a success it was a success for everyone because

730
01:07:32,060 --> 01:07:39,660
this marked a transformational moment for for AI and so even for Gary Kasparov he came to to

731
01:07:39,660 --> 01:07:46,220
realize at that moment was was was pivotal and actually meant something much more than than you

732
01:07:46,220 --> 01:07:53,980
know his personal loss in that moment. Lisa Dahl I think was a much more cognizant of that even

733
01:07:53,980 --> 01:08:01,580
at the time so in his closing remarks to the match he really felt very strongly that what

734
01:08:01,580 --> 01:08:06,380
had happened in the AlphaGo match was not only meaningful for AI but but for humans as well

735
01:08:06,380 --> 01:08:12,140
and he felt as a go player that it had opened his horizons and meant that he could start exploring

736
01:08:12,140 --> 01:08:17,980
new things it brought his joy back for the game of go because it had broken all of the the conventions

737
01:08:17,980 --> 01:08:24,460
and barriers and meant that you know suddenly suddenly anything was possible again and so

738
01:08:24,460 --> 01:08:29,660
you know I was sad to hear that he'd retired but you know he's been a great a great world champion

739
01:08:29,660 --> 01:08:35,260
over many many years and I think you know that he'll be he'll be remembered for that ever more

740
01:08:36,060 --> 01:08:41,820
he'll be remembered as the last person to to beat AlphaGo I mean after after that we we

741
01:08:41,820 --> 01:08:49,260
increased the power of the system and and the next version of AlphaGo beats the other strong

742
01:08:49,260 --> 01:08:56,140
human players 60 games to nil so you know what a great moment for him and something to be remembered

743
01:08:56,140 --> 01:09:03,820
for it's interesting that you spent time at AAAI on a panel with Gary Kasparov

744
01:09:05,260 --> 01:09:08,540
what I mean it's almost I'm just curious to learn

745
01:09:10,380 --> 01:09:16,460
the conversations you've had with Gary and the because he's also now he's written a book about

746
01:09:16,460 --> 01:09:22,700
artificial intelligence he's thinking about AI he has kind of a view of it and he talks about AlphaGo

747
01:09:22,700 --> 01:09:30,540
a lot what what's your sense I arguably I'm not just being Russian but I think Gary is the greatest

748
01:09:30,540 --> 01:09:38,620
chess player of all time the probably one of the greatest game players of all time and you sort of

749
01:09:40,060 --> 01:09:45,500
at the center of creating a system that beats one of the greatest players of all time so what

750
01:09:45,500 --> 01:09:51,100
is that conversation like is there anything yeah any interesting digs any bets any come

751
01:09:51,100 --> 01:09:58,460
any funny things any profound things so Gary Kasparov has an incredible respect for

752
01:10:00,060 --> 01:10:06,460
what we did with AlphaGo and you know it's it's an amazing tribute coming from from him of all people

753
01:10:07,420 --> 01:10:13,900
that he really appreciates and respects what what we've done and I think he feels that the progress

754
01:10:13,900 --> 01:10:21,340
which has happened in in computer chess which later after AlphaGo we we built the AlphaZero system

755
01:10:22,380 --> 01:10:28,780
which defeated the the world's strongest chess programs and to Gary Kasparov that moment in

756
01:10:28,780 --> 01:10:34,780
computer chess was more profound than than than deep blue and the reason he believes it mattered more

757
01:10:35,580 --> 01:10:39,980
was because it was done with with learning and a system which was able to discover for itself

758
01:10:39,980 --> 01:10:46,540
new principles new ideas which were able to play the game in a in a in a way which he hadn't always

759
01:10:47,740 --> 01:10:53,900
known about or anyone and in fact one of the things I discovered at this panel was that

760
01:10:53,900 --> 01:11:00,540
the current world champion Magnus Carlsen apparently recently commented on his improvement

761
01:11:00,540 --> 01:11:05,820
in performance and he attributes it to AlphaZero that he's been studying the games of AlphaZero

762
01:11:05,820 --> 01:11:12,620
he's changed his style to play more like AlphaZero and it's led to him actually increasing his his

763
01:11:12,620 --> 01:11:20,620
his rating to a new peak yeah I guess to me just like to Gary the inspiring thing is that and just

764
01:11:20,620 --> 01:11:26,460
like you said with reinforcement learning reinforcement learning and deep learning machine

765
01:11:26,460 --> 01:11:32,780
learning feels like what intelligence is yeah and you know you could attribute it to sort of

766
01:11:33,100 --> 01:11:40,140
a bitter viewpoint from Gary's perspective from us humans perspective saying that

767
01:11:40,140 --> 01:11:46,780
cert pure search that IBM deep blue was doing is not really intelligence but somehow it didn't feel

768
01:11:46,780 --> 01:11:52,460
like it and so that's the magical I'm not sure what it is about learning that feels like intelligence

769
01:11:52,460 --> 01:11:59,260
but but it does so I think we should not demean the achievements of what was done in previous

770
01:11:59,260 --> 01:12:06,540
areas of AI I think that deep blue was an amazing achievement in itself and that heuristic search

771
01:12:06,540 --> 01:12:11,820
of the kind that was used by deep blue had some powerful ideas that were in there but it also

772
01:12:11,820 --> 01:12:17,180
missed some things so so the fact that the that the evaluation function the way that the chess

773
01:12:17,180 --> 01:12:25,100
position was understood was created by humans and not by the machine is a limitation which means that

774
01:12:26,060 --> 01:12:31,580
there's a ceiling on how well it can do but maybe more importantly it means that the same idea

775
01:12:31,580 --> 01:12:38,060
cannot be applied in other domains where we don't have access to the kind of human grandmasters

776
01:12:38,060 --> 01:12:42,860
and that ability to kind of encode exactly their knowledge into an evaluation function and the

777
01:12:42,860 --> 01:12:48,140
reality is that the story of AI is that you know most domains turn out to be of the second type

778
01:12:48,140 --> 01:12:53,980
where when knowledge is messy it's hard to extract from experts or it isn't even available and so

779
01:12:54,060 --> 01:13:02,380
and so so we need to solve problems in a different way and I think alpha goes a step towards solving

780
01:13:02,380 --> 01:13:10,060
things in a way which which puts learning as a first class citizen and says systems need to

781
01:13:10,060 --> 01:13:17,580
understand for themselves how to understand the world how to judge their the value of of of

782
01:13:18,940 --> 01:13:23,180
any action that they might take within that world and any state they might find themselves in and

783
01:13:23,180 --> 01:13:30,940
in order to do that we we make progress towards AI yeah so one of the nice things about this

784
01:13:31,820 --> 01:13:38,460
about taking a learning approach to the game of go or game playing is that the things you learn the

785
01:13:38,460 --> 01:13:43,180
things you figure out are actually going to be applicable to other problems that are real world

786
01:13:43,180 --> 01:13:48,780
problems that's sort of that's ultimately I mean there's two really interesting things about alpha

787
01:13:48,780 --> 01:13:55,020
go one is the science of it just the science of learning the science of intelligence and then the

788
01:13:55,020 --> 01:14:00,780
other is well you're actually learning to figuring out how to build systems that would be potentially

789
01:14:00,780 --> 01:14:07,020
applicable in in other applications medical autonomous vehicles robotics all I mean it's

790
01:14:07,020 --> 01:14:15,420
just open the door to all kinds of applications so the next incredible step right really the

791
01:14:15,420 --> 01:14:22,060
profound step is probably alpha go zero I mean it's arguable I kind of see them all as the same

792
01:14:22,060 --> 01:14:27,180
place but really and perhaps you were already thinking that alpha go zero is the natural it was

793
01:14:27,180 --> 01:14:32,540
always going to be the next step but it's removing the reliance on human expert games

794
01:14:33,500 --> 01:14:42,140
for pre-training as you mentioned so how big of an intellectual leap was this that that self-play

795
01:14:42,140 --> 01:14:48,540
could achieve superhuman level performance in its own and maybe could you also say what is self-play

796
01:14:48,540 --> 01:14:57,260
I kind of mentioned it a few times but so let me start with self-play so the idea of self-play

797
01:14:57,260 --> 01:15:03,020
is something which is really about systems learning for themselves but in the situation

798
01:15:03,020 --> 01:15:09,420
where there's more than one agent and so if you're in a game and a game is a played between

799
01:15:09,420 --> 01:15:16,700
two players then self-play is really about understanding that game just by playing games

800
01:15:16,700 --> 01:15:21,180
against yourself rather than against any actual real opponent and so it's a way to kind of

801
01:15:22,540 --> 01:15:28,220
discover strategies without having to actually need to go out and play against

802
01:15:30,220 --> 01:15:32,540
any particular human player for example

803
01:15:33,500 --> 01:15:44,940
um the main idea of alpha zero was really to you know try and step back from any of the

804
01:15:44,940 --> 01:15:49,580
knowledge that we put into the system and ask the question is it possible to come up with a

805
01:15:49,580 --> 01:15:57,580
a single elegant principle by which a system can learn for itself all of the knowledge which it

806
01:15:57,580 --> 01:16:04,700
requires to play to play a game such as go importantly by taking knowledge out you not only

807
01:16:05,500 --> 01:16:10,860
make the system less brittle in the sense that perhaps the knowledge you were putting in was

808
01:16:10,860 --> 01:16:16,060
was just getting in the way and maybe stopping the system learning for itself but also you make it

809
01:16:16,060 --> 01:16:22,860
more general the more knowledge you put in the harder it is for a system to actually be placed

810
01:16:23,420 --> 01:16:28,620
taken out of the system in which it's kind of been designed and placed in some other system

811
01:16:28,620 --> 01:16:32,060
that maybe would need a completely different knowledge base to to understand and perform well

812
01:16:32,700 --> 01:16:38,540
and so the real goal here is to strip out all of the knowledge that we put in to the point that we

813
01:16:38,540 --> 01:16:44,220
can just plug it into something totally different um and that to me is really you know the the

814
01:16:44,220 --> 01:16:48,940
promise of AI is that we can have systems such as that which you know no matter what the goal is

815
01:16:49,900 --> 01:16:56,060
um no matter what goal we set to the system we can come up with we have an algorithm which

816
01:16:56,060 --> 01:17:02,060
can be placed into that world into that environment and can succeed in achieving that goal and then

817
01:17:02,060 --> 01:17:08,460
that that's to me is almost the the essence of intelligence if we can achieve that and so alpha

818
01:17:08,460 --> 01:17:15,260
zero is a step towards that um and it's a step that was taken in the context of of two player

819
01:17:15,260 --> 01:17:20,700
perfect information games like go and chess um we also applied it to Japanese chess

820
01:17:21,420 --> 01:17:28,140
so just to clarify the first step was alpha go zero the first step was to try and take all of

821
01:17:28,140 --> 01:17:37,420
the knowledge out of alpha go in such a way that it it could play in a in a fully um self-discovered

822
01:17:37,420 --> 01:17:42,060
way purely from self-play and to me the the motivation for that was always that we could

823
01:17:42,060 --> 01:17:49,980
then plug it into other domains um but we saved that that until later well and in fact I mean

824
01:17:51,500 --> 01:17:56,780
just for fun I could tell you exactly the moment where where the idea for alpha zero occurred to

825
01:17:56,780 --> 01:18:01,580
me um because I think there's maybe a lesson there for for researchers who are kind of too deeply

826
01:18:01,580 --> 01:18:06,300
embedded in there in their research and you know working 24 sevens try and come up with the next

827
01:18:06,380 --> 01:18:14,860
idea um which is it actually occurred to me um on honeymoon um and um and I was like at my most

828
01:18:14,860 --> 01:18:24,060
fully relaxed state really enjoying myself um and and just bing this like the algorithm for alpha

829
01:18:24,060 --> 01:18:31,180
zero just appeared like um and like in in its full form and this was actually before we played

830
01:18:31,180 --> 01:18:39,660
against um Lisa doll but we we just didn't I think we were so busy trying to make sure we could beat

831
01:18:39,660 --> 01:18:46,300
the um the the world champion that it was only later that we had the the opportunity to step

832
01:18:46,300 --> 01:18:51,100
back and and start examining that that sort of deeper scientific question of of whether this

833
01:18:51,100 --> 01:18:59,260
could really work so nevertheless so self-play is probably one of the most sort of profound ideas

834
01:18:59,820 --> 01:19:08,060
that it represents to me at least artificial intelligence but the fact that you could use

835
01:19:08,060 --> 01:19:15,740
that kind of mechanism to uh again beat world-class players that's very surprising so we kind of

836
01:19:17,020 --> 01:19:22,540
to me it feels like you have to train in a large number of expert games so was it surprising to

837
01:19:22,540 --> 01:19:27,420
you what was the intuition can you sort of think not necessarily at that time even now what's your

838
01:19:27,420 --> 01:19:32,700
intuition why this thing works so well why it's able to learn from scratch well let me first say

839
01:19:32,700 --> 01:19:38,460
why we tried it so we tried it both because I I feel that it was the deeper scientific question

840
01:19:38,460 --> 01:19:45,260
to to be asking to make progress towards AI and also because in general in my research I don't

841
01:19:45,260 --> 01:19:51,740
like to do research on questions for which we already know the likely outcome I don't see much

842
01:19:51,740 --> 01:19:57,980
value in running an experiment where you're 95 confident that that you will succeed and so we

843
01:19:57,980 --> 01:20:03,660
could have tried you know maybe to to take out the go and do something which we we knew for sure

844
01:20:03,660 --> 01:20:08,380
it would succeed on but much more interesting to me was to try try on the things which we weren't

845
01:20:08,380 --> 01:20:14,620
sure about and one of the big questions on our minds back then was you know could you really

846
01:20:14,620 --> 01:20:20,220
do this with self-play alone how far could that go would it be as strong and honestly

847
01:20:21,100 --> 01:20:27,260
we weren't sure yeah it was 50-50 I think you know we I really if you'd asked me I wasn't confident

848
01:20:27,260 --> 01:20:32,700
that it could reach the same level as these systems but it felt like the right question to ask

849
01:20:33,740 --> 01:20:39,660
and even if even if it had not achieved the same level I felt that that was an important

850
01:20:39,980 --> 01:20:43,260
direction to be studying and so

851
01:20:45,980 --> 01:20:51,820
then low and behold it actually ended up outperforming the the previous version of of

852
01:20:51,820 --> 01:20:58,140
AlphaGo and indeed was able to beat it by a hundred games to zero so what's the intuition as to as

853
01:20:58,140 --> 01:21:07,340
to why I think the intuition to me is clear that whenever you have errors in a in a system

854
01:21:08,300 --> 01:21:11,100
as we did in AlphaGo AlphaGo suffered from these delusions

855
01:21:11,980 --> 01:21:15,260
occasionally it would misunderstand what was going on in a position and misevaluate it

856
01:21:16,060 --> 01:21:22,060
how can how can you remove all of these these errors errors arise from many sources for us

857
01:21:22,060 --> 01:21:26,780
they were arising both from you know starting from the human data but also from the from the

858
01:21:26,780 --> 01:21:31,340
nature of the search and the nature of the algorithm itself but the only way to address them

859
01:21:31,340 --> 01:21:38,780
in any complex system is to give the system the ability to correct its own errors it must be able

860
01:21:38,780 --> 01:21:43,740
to correct them it must be able to learn for itself when it's doing something wrong and correct

861
01:21:43,740 --> 01:21:50,540
for it and so it seemed to me that the way to correct delusions was indeed to have more iterations

862
01:21:50,540 --> 01:21:54,780
of reinforcement learning that you know no matter where you start you should be able to correct for

863
01:21:54,780 --> 01:22:00,380
those errors until it gets to play that out and understand oh well I thought that I was going

864
01:22:00,380 --> 01:22:05,020
to win in this situation but then I ended up losing that suggests that I was misevaluating

865
01:22:05,020 --> 01:22:08,860
something there's a hole in my knowledge and now now the system can correct for itself and

866
01:22:08,860 --> 01:22:15,100
and understand how to do better now if you take that same idea and trace it back all the way to

867
01:22:15,100 --> 01:22:21,180
the beginning it should be able to take you from no knowledge from completely random starting point

868
01:22:21,740 --> 01:22:27,340
all the way to the highest levels of knowledge that you can achieve in a in a domain and the

869
01:22:27,340 --> 01:22:32,540
principle is the same that if you give if you bestow a system with the ability to correct its own

870
01:22:32,540 --> 01:22:38,860
errors then it can take you from random to something slightly better than random because it sees the

871
01:22:38,860 --> 01:22:42,540
stupid things that the random is doing and it can correct them and then it can take you from that

872
01:22:42,540 --> 01:22:47,020
slightly better system and understand well what's that doing wrong and it takes you on to the next

873
01:22:47,020 --> 01:22:54,540
level and the next level and and this progress it can go on indefinitely and indeed you know

874
01:22:54,540 --> 01:22:58,140
what would have happened if we'd carried on training AlphaGo Zero for longer

875
01:22:59,340 --> 01:23:04,220
we saw no sign of it slowing down its improvements or at least it was certainly

876
01:23:04,220 --> 01:23:11,420
carrying on to improve and presumably if you had the computational resources this

877
01:23:11,420 --> 01:23:15,660
this could lead to better and better systems that discover more and more.

878
01:23:15,660 --> 01:23:20,940
So your intuition is fundamentally there's not a ceiling to this process

879
01:23:21,740 --> 01:23:26,700
one of the surprising things just like you said is the process of patching errors

880
01:23:27,260 --> 01:23:32,620
that's intuitively makes sense that this is a reinforcement learning should be part of that

881
01:23:32,620 --> 01:23:39,580
process but what is surprising is in the process of patching your own lack of knowledge you don't

882
01:23:40,140 --> 01:23:47,500
open up other patches you keep sort of like there's a monotonic decrease of your weaknesses.

883
01:23:48,380 --> 01:23:51,580
Well let me let me back this up you know I think science always should make

884
01:23:51,580 --> 01:23:56,940
falsifiable hypotheses yes so let me let me back up this claim with a falsifiable hypothesis

885
01:23:56,940 --> 01:24:03,900
which is that if someone was to in the future take Alpha Zero as an algorithm and run it on

886
01:24:05,340 --> 01:24:12,060
with greater computational resources that we had available today then I would predict that they

887
01:24:12,060 --> 01:24:16,540
would be able to beat the previous system 100 games to zero and that if they were then to do

888
01:24:16,540 --> 01:24:21,980
the same thing a couple of years later that that would beat that previous system 100 games to zero

889
01:24:21,980 --> 01:24:28,060
and that that process would continue indefinitely throughout at least my human lifetime presumably

890
01:24:28,060 --> 01:24:33,500
the game of go would set the the ceiling I mean the game of go would set the ceiling but the game

891
01:24:33,500 --> 01:24:39,980
of go has 10 to the 170 states in it so so the ceiling is is unreachable by any computational

892
01:24:39,980 --> 01:24:45,100
device that can be built out of the you know 10 to the 80 atoms in the universe.

893
01:24:46,620 --> 01:24:51,180
You asked a really good question which is you know do you not open up other errors

894
01:24:51,180 --> 01:24:56,540
when you when you correct your previous ones and the answer is is yes you do and so

895
01:24:57,420 --> 01:25:03,020
so it's a remarkable fact about about this class of of two player game and also true of

896
01:25:03,020 --> 01:25:13,660
single agent games that essentially progress will always lead you to if you have sufficient

897
01:25:13,660 --> 01:25:18,620
representational resource like imagine you had could represent every state in a big table

898
01:25:18,620 --> 01:25:25,420
of the game then we we know for sure that a progress of self-improvement will lead all the way

899
01:25:26,140 --> 01:25:30,460
in the single agent case to the optimal possible behavior and in the two player case to the

900
01:25:30,460 --> 01:25:35,980
minimax optimal behavior that is the the best way that I can play knowing that you're playing

901
01:25:35,980 --> 01:25:44,540
perfectly against me and so so for those cases we know that even if you do open up some new error

902
01:25:44,540 --> 01:25:48,940
that in some sense you've made progress you've you've you're progressing towards the the best

903
01:25:48,940 --> 01:25:57,500
that can be done so alpha go was initially trained on expert games with some self-play alpha go zero

904
01:25:57,500 --> 01:26:04,460
remove the need to be trained on expert games and then another incredible step for me because I just

905
01:26:04,460 --> 01:26:11,260
love chess is to generalize that further to be in alpha zero to be able to play the game of go

906
01:26:12,300 --> 01:26:17,580
beating alpha go zero and alpha go and then also being able to play the game of chess

907
01:26:18,140 --> 01:26:25,020
and others so what was that step like what's the interesting aspects there that required to make

908
01:26:25,020 --> 01:26:32,780
that happen I think the remarkable observation which we saw with alpha zero was that actually

909
01:26:32,780 --> 01:26:39,420
without modifying the algorithm at all it was able to play and crack some of ai's greatest

910
01:26:39,420 --> 01:26:46,060
previous challenges in particular we dropped it into the game of chess and unlike the previous

911
01:26:46,060 --> 01:26:52,620
systems like deep blue which had been worked on for you know years and years and we were able to beat

912
01:26:52,620 --> 01:27:00,380
the world's strongest computer chess program convincingly using a system that was fully

913
01:27:00,460 --> 01:27:06,700
discovered by its own from from scratch with its own principles and in fact one of the nice things

914
01:27:06,700 --> 01:27:13,100
that that we found was that in fact we also achieved the same result in in Japanese chess a variant

915
01:27:13,100 --> 01:27:17,340
of chess where where you get to capture pieces and then place them back down on your on your own

916
01:27:17,340 --> 01:27:23,020
side as an extra piece so a much more complicated variant of chess and we also beat the world's

917
01:27:23,020 --> 01:27:30,060
strongest programs and reach superhuman performance in that game too and it was the very first time

918
01:27:30,060 --> 01:27:36,140
that we'd ever run the system on that particular game was the version that we published in the

919
01:27:36,140 --> 01:27:42,140
paper on on alpha zero it just worked out of the box literally no no no touching it we didn't have

920
01:27:42,140 --> 01:27:48,460
to do anything and and there it was superhuman performance no tweaking no no twiddling and so

921
01:27:48,460 --> 01:27:53,100
I think there's something beautiful about that principle that you can take an algorithm and

922
01:27:53,100 --> 01:28:03,020
without twiddling anything it just it just works now to go beyond alpha zero what's required alpha

923
01:28:03,020 --> 01:28:09,260
zero is is just a step and there's a long way to go beyond that to really crack the deep problems of

924
01:28:09,260 --> 01:28:16,220
AI but one of the important steps is to acknowledge that the world is a really messy place you know

925
01:28:16,220 --> 01:28:22,700
it's this rich complex beautiful but messy environment that we live in and no one gives us

926
01:28:22,700 --> 01:28:28,460
the rules like no one knows the rules of the world at least maybe we understand that it operates

927
01:28:28,460 --> 01:28:34,140
according to Newtonian or or quantum mechanics at the micro level or according to relativity at the

928
01:28:34,140 --> 01:28:40,620
macro level but that's not a model that's useful useful for us as people to to operate in it somehow

929
01:28:40,620 --> 01:28:45,900
the agent needs to understand the world for itself in a way where no one tells it the rules of the

930
01:28:45,900 --> 01:28:52,460
game and yet it can still figure out what to do in that world deal with this stream of

931
01:28:52,540 --> 01:28:57,820
observations coming in rich sensory input coming in actions going out in a way that allows it to

932
01:28:57,820 --> 01:29:03,260
reason in the way that alpha go or alpha zero can reason in the way that these go and chess playing

933
01:29:03,260 --> 01:29:09,580
programs can reason but in a way that allows it to take actions in that messy world to to achieve

934
01:29:09,580 --> 01:29:17,820
its goals and so this led us to the most recent step in the story of of of alpha go which was a

935
01:29:17,900 --> 01:29:24,220
system called mu zero and mu zero is a system which learns for itself even when the rules

936
01:29:24,220 --> 01:29:29,820
are not given to it it actually can be dropped into a system with messy perceptual inputs we

937
01:29:29,820 --> 01:29:37,020
actually tried it in the in some Atari games the canonical domains of Atari that have been used

938
01:29:37,020 --> 01:29:44,300
for reinforcement learning and and this system learned to build a model of these Atari games

939
01:29:44,300 --> 01:29:51,820
that was sufficiently rich and useful enough for it to be able to plan successfully and in fact

940
01:29:51,820 --> 01:29:57,820
that system not only went on to to beat the state of the art in Atari but the same system without

941
01:29:57,820 --> 01:30:04,620
modification was able to reach the same level of superhuman performance in go chess and shogi

942
01:30:04,620 --> 01:30:10,140
that we'd seen in alpha zero showing that even without the rules the system can learn for itself

943
01:30:10,140 --> 01:30:15,100
just by trial and error just by playing this game of go and no one tells you what the rules are but

944
01:30:15,100 --> 01:30:21,020
you just get to the end and and someone says you know win or loss you play this game of chess and

945
01:30:21,020 --> 01:30:26,460
someone says win or loss or you you play a game of breakout in Atari and someone just tells you

946
01:30:26,460 --> 01:30:31,500
you know your score at the end and the system for itself figures out essentially the rules of the

947
01:30:31,500 --> 01:30:38,540
system the dynamics of the world how the world works and that not in any explicit way but just

948
01:30:38,620 --> 01:30:44,380
implicitly enough understanding for it to be able to plan in that in that system in order to

949
01:30:44,380 --> 01:30:48,860
achieve its goals and that's the you know that's the fundamental process they have to go through when

950
01:30:48,860 --> 01:30:53,900
you're facing in any uncertain kind of environment that you would in the real world is figuring out

951
01:30:53,900 --> 01:30:59,340
the sort of the rules the basic rules of the game that's right so there's a lot I mean yeah that

952
01:30:59,340 --> 01:31:06,540
that allows it to be applicable to basically any domain that could be digitized in the way that it

953
01:31:06,540 --> 01:31:12,220
needs to in order to be consumable sort of in order for the reinforcement learning framework to

954
01:31:12,220 --> 01:31:16,460
be able to sense the environment to be able to act in the environment and so on the full reinforcement

955
01:31:16,460 --> 01:31:22,700
learning problem needs to deal with with worlds that are unknown and and complex and and the agent

956
01:31:22,700 --> 01:31:27,900
needs to learn for itself how to deal with that and so Musero is there's a step a further step in

957
01:31:27,900 --> 01:31:33,740
that direction one of the things that inspire the general public in just in conversations I have like

958
01:31:33,740 --> 01:31:40,220
with my parents or something with my mom that just loves what was done is kind of at least a notion

959
01:31:40,220 --> 01:31:46,060
that there was some display of creativity some new strategies new behaviors that were created that

960
01:31:46,060 --> 01:31:51,500
that again has echoes of intelligence so is there something that stands up do you see it the same

961
01:31:51,500 --> 01:31:57,740
way that there's creativity and there's some behaviors patterns that you saw that alpha

962
01:31:57,740 --> 01:32:06,060
zero was able to display that are truly creative so let me start by I think saying that I think

963
01:32:06,060 --> 01:32:13,100
we should ask what creativity really means so to me creativity means discovering something

964
01:32:13,660 --> 01:32:21,020
which wasn't known before something unexpected something outside of our norms and so in that sense

965
01:32:21,980 --> 01:32:28,780
the process of reinforcement learning or the self-play approach that was used by alpha zero

966
01:32:29,740 --> 01:32:35,420
is it's the essence of creativity it's really saying at every stage you're playing according to

967
01:32:35,420 --> 01:32:43,180
your current norms and you try something and if it works out you say hey here's something great

968
01:32:43,180 --> 01:32:48,060
I'm going to start using that and then that process it's like a micro discovery that happens

969
01:32:48,060 --> 01:32:53,660
millions and millions of times over the course of the algorithm's life where it just discovers some

970
01:32:53,660 --> 01:32:57,900
new idea oh this pattern this pattern's working really well for me I'm gonna I'm gonna start using

971
01:32:57,900 --> 01:33:03,180
that oh now oh here's this other thing I can do I can start to to connect these stones together in

972
01:33:03,180 --> 01:33:10,300
this way or I can start to you know sacrifice stones or give up on on pieces or play shoulder

973
01:33:10,300 --> 01:33:14,220
hits on the fifth line or whatever it is the system's discovering things like this for itself

974
01:33:14,300 --> 01:33:20,460
continually repeatedly all the time and so it should come as no surprise to us then when

975
01:33:20,460 --> 01:33:25,660
if you leave these systems going that they discover things that are not known to humans

976
01:33:25,660 --> 01:33:33,580
that the to the human norms are considered creative and we've seen this several times in fact

977
01:33:33,580 --> 01:33:42,780
in alpha go zero we saw this beautiful timeline of discovery where what we saw was that there

978
01:33:42,780 --> 01:33:46,940
were these opening patterns that humans play called joseki these are like the patterns that

979
01:33:46,940 --> 01:33:50,540
that humans learn to play in the corners and they've been developed and refined over

980
01:33:50,540 --> 01:33:54,940
over literally thousands of years in the game of go and what we saw was in the course of the

981
01:33:55,580 --> 01:34:00,940
training alpha go zero over the course of the the 40 days that we trained this system

982
01:34:01,820 --> 01:34:08,700
it starts to discover exactly these patterns that human players play and over time we found

983
01:34:08,700 --> 01:34:13,980
that all of the joseki that humans played were discovered by the system through this process

984
01:34:13,980 --> 01:34:21,100
of of self play and this sort of essential notion of creativity but what was really interesting

985
01:34:21,100 --> 01:34:26,940
was that over time it then starts to discard some of these in favor of its own joseki that humans

986
01:34:26,940 --> 01:34:32,940
didn't know about and it starts to say oh well you thought that the knights move pincer joseki

987
01:34:32,940 --> 01:34:38,620
was a great idea but here's something different you can do there which makes some new variations

988
01:34:38,780 --> 01:34:43,740
that humans didn't know about and actually now the human go players study the joseki that alpha

989
01:34:43,740 --> 01:34:50,540
go played and they become the new norms that are used in in today's top level go competitions

990
01:34:51,180 --> 01:34:58,300
that never gets old even just the first to me maybe just makes me feel good as a human being

991
01:34:58,300 --> 01:35:03,740
that a self playing mechanism that knows nothing about us humans discovers patterns that we humans

992
01:35:03,740 --> 01:35:09,500
do it's this is like an affirmation that we're all doing we're doing okay as humans yeah we've

993
01:35:10,460 --> 01:35:15,740
in this domain in other domains we figured out it's like the church will quote about democracy

994
01:35:15,740 --> 01:35:22,940
it's the you know it's the but it sucks but it's the best one we've tried so um in general

995
01:35:22,940 --> 01:35:28,380
taking a step outside of go and you have like a million accomplishments that have no time to talk

996
01:35:28,380 --> 01:35:36,060
about the with alpha star and so on and and the current work but in general this self playing

997
01:35:36,060 --> 01:35:42,780
mechanism that you've inspired the world with by beating the world champion go player do you see

998
01:35:42,780 --> 01:35:50,620
that as um do you see being applied in other domains do you have sort of dreams and hopes that

999
01:35:50,620 --> 01:35:57,260
is applied in both the simulated environments in the constrained environments of games constrained

1000
01:35:57,260 --> 01:36:01,580
i mean alpha star really demonstrates that you can remove a lot of the constraints but nevertheless

1001
01:36:01,580 --> 01:36:07,100
it's in a digital simulated environment do you have a hope a dream that it starts being applied

1002
01:36:07,100 --> 01:36:13,500
in the robotics environment and maybe even in domains that are safety critical and so on and

1003
01:36:14,380 --> 01:36:18,300
have you know have a real impact in the real world like autonomous vehicles for example which

1004
01:36:18,300 --> 01:36:25,740
seems like a very far out dream at this point so i absolutely do um hope and and imagine that we

1005
01:36:25,740 --> 01:36:30,060
will we will get to the point where ideas just like these are used in all kinds of different

1006
01:36:30,060 --> 01:36:35,020
domains in fact one of the most satisfying things as a researcher is when you start to see other

1007
01:36:35,020 --> 01:36:40,940
people use your your algorithms in unexpected ways so in the last couple of years there have been

1008
01:36:40,940 --> 01:36:49,100
you know a couple of nature papers where different teams unbeknownst to to us took alpha zero and

1009
01:36:49,100 --> 01:36:56,940
applied exactly those same algorithms and ideas to real world problems of huge meaning to to

1010
01:36:56,940 --> 01:37:02,140
society so one of them was the problem of chemical synthesis and they were able to beat the state of

1011
01:37:02,140 --> 01:37:10,860
the art in finding pathways of how to actually synthesize chemicals retro retro chemical synthesis

1012
01:37:11,820 --> 01:37:15,900
and the second paper actually actually just came out a couple of weeks ago in nature

1013
01:37:16,620 --> 01:37:22,220
showed that in quantum computation you know one of the big questions is how to how to

1014
01:37:22,220 --> 01:37:29,020
understand the nature of the the the function in quantum computation and a system based on alpha

1015
01:37:29,020 --> 01:37:33,980
zero beat the state of the art by quite some distance there again so so these are just examples

1016
01:37:33,980 --> 01:37:38,700
and i think you know that the lesson which we've seen elsewhere in in in machine learning time

1017
01:37:38,700 --> 01:37:44,140
and time again is that if you make something general it will be used in all kinds of ways you

1018
01:37:44,140 --> 01:37:49,660
know you provide a really powerful tools to society and and those tools can be used in in

1019
01:37:49,660 --> 01:37:56,060
amazing ways and so i think we're just at the beginning and and for sure i hope that we we

1020
01:37:56,060 --> 01:38:03,260
see all kinds of outcomes so the the and the the other side of the question of a reinforcement

1021
01:38:03,260 --> 01:38:08,860
learning framework is you know usually want to specify reward function and an objective function

1022
01:38:08,940 --> 01:38:16,300
what do you think about sort of ideas of intrinsic rewards of and when we're not really sure about

1023
01:38:16,940 --> 01:38:24,700
you know of if we take you know human beings as existence proof that we don't seem to be

1024
01:38:24,700 --> 01:38:31,500
operating according to a single reward do you think that there's interesting ideas

1025
01:38:32,220 --> 01:38:38,060
for when you don't know how to truly specify the reward you know that there's some flexibility

1026
01:38:38,060 --> 01:38:42,620
for discovering it intrinsically or so on in the context of reinforcement learning

1027
01:38:42,620 --> 01:38:46,700
so i think you know when we think about intelligence it's really important to be clear

1028
01:38:46,700 --> 01:38:51,260
about the problem of intelligence and i think it's clearest to understand that problem in

1029
01:38:51,260 --> 01:38:56,220
terms of some ultimate goal that we want the system to to try and solve for and after all if

1030
01:38:56,220 --> 01:39:01,660
if we don't understand the ultimate purpose of the system do we really even have a clearly defined

1031
01:39:01,980 --> 01:39:08,220
problem that we're solving at all now within that as with your example for humans

1032
01:39:10,300 --> 01:39:16,940
the system may choose to create its own motivations and sub-goals that help the system to achieve its

1033
01:39:16,940 --> 01:39:23,340
ultimate goal and that may indeed be a hugely important mechanism to achieve those ultimate

1034
01:39:23,340 --> 01:39:27,740
goals but there is still some ultimate goal i think the system needs to be measurable and and

1035
01:39:27,740 --> 01:39:33,180
evaluated against and even for humans i mean humans we're incredibly flexible we feel that we

1036
01:39:33,180 --> 01:39:38,780
we can you know any goal that we're given we feel we can we can master to some some degree

1037
01:39:40,060 --> 01:39:44,380
but if we think of those goals really you know like the the goal of being able to pick up an

1038
01:39:44,380 --> 01:39:50,620
object or the goal of of being able to communicate or influence people to do things in a in a particular

1039
01:39:50,620 --> 01:39:58,380
way or whatever those goals are really they are their sub-goals really that we set ourselves

1040
01:39:58,380 --> 01:40:04,540
you know we choose to pick up the the object we choose to communicate we choose to to influence

1041
01:40:04,540 --> 01:40:09,820
someone else and we choose those because it we think it will lead us to something you know in

1042
01:40:09,820 --> 01:40:16,140
later on we think that's helpful to us to achieve some ultimate goal now i don't want to speculate

1043
01:40:16,140 --> 01:40:22,300
whether or not humans as a system necessarily have a singular overall goal of survival or whatever

1044
01:40:22,300 --> 01:40:28,220
it is but i think the principle for understanding and implementing intelligence is has to be that if

1045
01:40:28,220 --> 01:40:32,460
we're trying to understand intelligence or implement our own there has to be a well-defined

1046
01:40:32,460 --> 01:40:39,900
problem otherwise if it's not i think it's it's like an admission of defeat that for there to be

1047
01:40:39,900 --> 01:40:43,900
hope for for understanding or implementing intelligence we have to know what we're doing

1048
01:40:43,900 --> 01:40:48,060
we have to know what we're asking the system to do otherwise if you if you don't have a clearly

1049
01:40:48,060 --> 01:40:54,460
defined purpose you're not going to get a clearly defined answer the the ridiculous big question that

1050
01:40:54,460 --> 01:41:01,580
has to naturally follow because i have to pin you down on this on this thing that nevertheless one

1051
01:41:01,580 --> 01:41:08,780
of the big silly or big real questions before humans is the meaning of life is us trying to

1052
01:41:08,780 --> 01:41:13,180
figure out our own reward function yeah and you just kind of mentioned that if you want to build

1053
01:41:13,260 --> 01:41:18,300
intelligent systems and you know what you're doing you should be at least cognizant to some degree

1054
01:41:18,300 --> 01:41:25,020
of what the reward function is so the natural question is what do you think is the reward

1055
01:41:25,020 --> 01:41:30,380
function of human life the meaning of life for us humans the meaning of our existence

1056
01:41:32,860 --> 01:41:38,380
i think you know i'd be speculating beyond my own expertise but but just for fun let me do that

1057
01:41:38,380 --> 01:41:42,940
yes please and say i think that there are many levels at which you can understand the system

1058
01:41:42,940 --> 01:41:49,180
and and you can understand something as as optimizing for for a goal at many levels and so

1059
01:41:50,220 --> 01:41:55,260
so you can understand the you know let's start with the universe like does the universe have a

1060
01:41:55,260 --> 01:42:02,220
purpose well it feels like it's just at one level just following certain mechanical laws of physics

1061
01:42:02,220 --> 01:42:06,700
and that that's led to the development of the universe but at another level you can view it as

1062
01:42:07,660 --> 01:42:11,180
actually there's the second law of thermodynamics that says that this is

1063
01:42:11,180 --> 01:42:15,500
increasing in entropy over time forever and now there's a view that's been developed by

1064
01:42:16,220 --> 01:42:20,540
certain people at MIT that this you can think of this as as almost like a goal of the universe

1065
01:42:20,540 --> 01:42:26,140
that the purpose of the universe is to maximize entropy so there are multiple levels at which

1066
01:42:26,140 --> 01:42:33,580
you can understand a system the next level down you might say well if the goal is to is to maximize

1067
01:42:33,580 --> 01:42:41,180
entropy well how do how does how can that be done by a particular system and maybe evolution

1068
01:42:41,180 --> 01:42:45,900
is something that the universe discovered in order in order to kind of dissipate energy as

1069
01:42:45,900 --> 01:42:50,780
efficiently as possible and by the way i'm borrowing from max tegmark for some of these

1070
01:42:50,780 --> 01:42:58,140
metaphors the physicist but if you can think of evolution as a mechanism for for dispersing energy

1071
01:42:59,100 --> 01:43:06,140
then then evolution you might say is is then becomes a goal which is if if evolution disperses

1072
01:43:06,140 --> 01:43:12,700
energy by reproducing as efficiently as possible what's evolution then well it's now got its own

1073
01:43:12,700 --> 01:43:20,220
goal within that which is to actually reproduce as effectively as possible and now how does

1074
01:43:20,220 --> 01:43:27,340
reproduction how is that made as effective as possible well you need entities within that

1075
01:43:27,420 --> 01:43:31,260
that can survive and reproduce as effectively as possible and so it's natural that in order to

1076
01:43:31,260 --> 01:43:37,820
achieve that high level goal those individual organisms discover brains intelligences which

1077
01:43:37,820 --> 01:43:46,300
enable them to support the the goals of evolution and those brains what do they do well perhaps the

1078
01:43:46,300 --> 01:43:52,540
early brains maybe they were controlling things at some direct level you know maybe they were the

1079
01:43:52,540 --> 01:43:56,380
equivalent of pre-programmed systems which were directly controlling what was going on

1080
01:43:57,500 --> 01:44:01,980
and setting certain you know things in order to achieve these particular particular goals

1081
01:44:02,940 --> 01:44:07,500
but that led to a another level of discovery which was learning systems you know parts of

1082
01:44:07,500 --> 01:44:12,380
the brain which are able to to learn for themselves and learn how to to program themselves to achieve

1083
01:44:12,380 --> 01:44:18,700
any goal and presumably there are parts of the game of the brain where goals are set to parts of

1084
01:44:18,700 --> 01:44:23,900
that that system and provides this very flexible notion of intelligence that that we as humans

1085
01:44:23,900 --> 01:44:28,620
presumably have which is the ability to kind of why the reason we feel that we can we can we can

1086
01:44:28,620 --> 01:44:33,580
achieve any goal so so it's a very long-winded answer to say that you know I think there are many

1087
01:44:33,580 --> 01:44:40,380
perspectives and many levels at which intelligence can be understood and and each of those levels

1088
01:44:40,380 --> 01:44:44,300
you can take multiple perspectives like you know you can view the system as as something which is

1089
01:44:44,300 --> 01:44:49,500
optimizing for a goal which is understanding it at a level by which we can maybe implement it and

1090
01:44:49,500 --> 01:44:54,460
understand it as AI researchers or computer scientists or you can understand it at the

1091
01:44:54,460 --> 01:44:57,980
level of the mechanistic thing which is going on that there are these you know atoms bouncing

1092
01:44:57,980 --> 01:45:02,300
around in the brain and they lead to the the outcome of that system is not in contradiction

1093
01:45:02,300 --> 01:45:08,780
with the fact that it's it's also a decision-making system that's optimizing for some goal and and

1094
01:45:08,780 --> 01:45:15,900
purpose I've never heard the the description of the meaning of life structured so beautifully in

1095
01:45:15,900 --> 01:45:22,620
layers but you did miss one layer which is the next step which you're responsible for which is

1096
01:45:22,620 --> 01:45:29,740
creating the the artificial intelligence indeed layer on top of that and indeed I can't wait to

1097
01:45:29,740 --> 01:45:36,860
see well I may not be around but the can't wait to see what the next layer beyond that well well

1098
01:45:36,860 --> 01:45:41,740
let's just take that that argument you know and pursue it to its natural conclusion so so the

1099
01:45:41,740 --> 01:45:48,140
next level indeed is for for how can our how can our learning brain achieve its goals most

1100
01:45:48,140 --> 01:45:58,380
effectively well maybe it does so by by us as learning beings building a system which is able

1101
01:45:58,380 --> 01:46:04,140
to solve for those goals more effectively than we can and so when we build a system to play the game

1102
01:46:04,140 --> 01:46:08,860
of go you know when I said that I wanted to build a system that can play go better than I can I've

1103
01:46:08,860 --> 01:46:14,460
enabled myself to achieve that goal of playing go better than I could by by directly playing it

1104
01:46:14,460 --> 01:46:20,300
and learning it myself and so now a new layer has been created which is systems which are able to

1105
01:46:20,300 --> 01:46:26,220
achieve goals for themselves and ultimately there may be layers beyond that where they set sub-goals

1106
01:46:26,220 --> 01:46:34,460
to parts of their own system in all in order to to achieve those and so forth so incredible so the

1107
01:46:34,540 --> 01:46:38,780
story of intelligence I think I think is is a multi-layered one and a multi-perspective one

1108
01:46:39,900 --> 01:46:45,740
we live in an incredible universe David thank you so much first of all for dreaming of using

1109
01:46:45,740 --> 01:46:52,300
learning to solve go and building intelligence systems and for actually making it happen and

1110
01:46:52,300 --> 01:46:57,980
for inspiring millions of people in the process it's truly an honor thank you so much for talking

1111
01:46:57,980 --> 01:47:03,020
today okay thank you thanks for listening to this conversation with David Silver and thank you to

1112
01:47:03,020 --> 01:47:08,700
our sponsors masterclass and cash app please consider supporting the podcast by signing up

1113
01:47:08,700 --> 01:47:15,580
to masterclass at masterclass.com slash lex and downloading cash app and using code lex podcast

1114
01:47:15,580 --> 01:47:20,140
if you enjoy this podcast subscribe on youtube review it with five stars an apple podcast

1115
01:47:20,140 --> 01:47:26,140
support on patreon or simply connect with me on twitter at lex freedman and now let me leave you

1116
01:47:26,140 --> 01:47:32,540
some words from david silver my personal belief is that we've seen something of a turning point

1117
01:47:32,620 --> 01:47:38,220
where we're starting to understand that many abilities like intuition and creativity that

1118
01:47:38,220 --> 01:47:43,660
we've previously thought were in the domain only of the human mind are actually accessible to machine

1119
01:47:43,660 --> 01:47:50,220
intelligence as well and I think that's a really exciting moment in history thank you for listening

1120
01:47:50,220 --> 01:47:55,180
and hope to see you next time

