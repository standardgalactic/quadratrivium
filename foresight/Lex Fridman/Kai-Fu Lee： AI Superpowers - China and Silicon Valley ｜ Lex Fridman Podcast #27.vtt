WEBVTT

00:00.000 --> 00:02.960
The following is a conversation with Kai-Fu Li.

00:02.960 --> 00:06.480
He's the chairman and CEO of Sinovation Ventures

00:06.480 --> 00:10.480
that manages a $2 billion dual currency investment fund

00:10.480 --> 00:14.800
with a focus on developing the next generation of Chinese high-tech companies.

00:15.440 --> 00:17.840
He's the former president of Google China

00:17.840 --> 00:21.680
and the founder of what is now called Microsoft Research Asia,

00:21.680 --> 00:26.560
an institute that trained many of the artificial intelligence leaders in China,

00:26.560 --> 00:33.600
including CTOs or AI execs at Baidu, Tencent, Alibaba, Lenovo, and Huawei.

00:34.720 --> 00:40.000
He was named one of the 100 most influential people in the world by Time Magazine.

00:40.560 --> 00:44.880
He's the author of seven best-selling books in Chinese and most recently

00:44.880 --> 00:48.800
the New York Times bestseller called AI Superpowers,

00:48.800 --> 00:51.760
China, Silicon Valley, and the New World Order.

00:52.720 --> 00:57.600
He has unparalleled experience in working across major tech companies

00:57.600 --> 00:59.920
and governments on applications of AI,

00:59.920 --> 01:04.880
and so he has a unique perspective on global innovation in the future of AI

01:04.880 --> 01:07.920
that I think is important to listen to and think about.

01:08.880 --> 01:11.840
This is the Artificial Intelligence Podcast.

01:11.840 --> 01:15.120
If you enjoy it, subscribe on YouTube and iTunes,

01:15.120 --> 01:20.240
support it on Patreon, or simply connect with me on Twitter at Lex Freedman.

01:20.880 --> 01:24.560
And now here's my conversation with Kaifu Li.

01:25.920 --> 01:29.280
I immigrated from Russia to U.S. when I was 13.

01:29.280 --> 01:31.600
You immigrated to U.S. at about the same age.

01:32.320 --> 01:35.840
The Russian people, the American people, the Chinese people,

01:35.840 --> 01:41.280
each have a certain soul, a spirit that permeates throughout the generations.

01:42.000 --> 01:45.040
So maybe it's a little bit of a poetic question,

01:45.040 --> 01:50.960
but could you describe your sense of what defines the Chinese soul?

01:51.920 --> 01:56.000
I think the Chinese soul of people today, right?

01:56.000 --> 02:01.920
We're talking about people who have had centuries of burden

02:01.920 --> 02:05.120
because of the poverty that the country has gone through

02:05.120 --> 02:10.400
and suddenly shined with hope of prosperity

02:10.400 --> 02:13.280
in the past 40 years as China opened up

02:13.360 --> 02:15.120
and embraced market economy.

02:16.320 --> 02:20.960
And undoubtedly, there are two sets of pressures on the people,

02:21.600 --> 02:27.920
that of the tradition, that of facing difficult situations,

02:27.920 --> 02:33.760
and that of hope of wanting to be the first to become successful and wealthy.

02:33.760 --> 02:38.240
So that's a very strong hunger and a strong desire

02:38.240 --> 02:41.040
and strong work ethic that drives China forward.

02:41.040 --> 02:43.840
And is there roots to not just this generation,

02:43.840 --> 02:49.920
but before that's deeper than just the new economic developments?

02:49.920 --> 02:54.560
Is there something that's unique to China that you could speak to that's in the people?

02:55.840 --> 03:02.000
Well, the Chinese tradition is about excellence, dedication, and results.

03:02.560 --> 03:07.120
And the Chinese exams and study subjects in schools

03:07.120 --> 03:10.720
have traditionally started from memorizing 10,000 characters.

03:11.040 --> 03:13.440
Not an easy task to start with.

03:13.440 --> 03:18.960
And further by memorizing historic philosophers' literature poetry.

03:18.960 --> 03:24.160
So it really is probably the strongest road learning mechanism

03:24.160 --> 03:28.800
created to make sure people had good memory and remembered things extremely well.

03:30.080 --> 03:35.840
That, I think, at the same time suppresses the breakthrough innovation

03:36.160 --> 03:41.840
and also enhances the speed execution, get results.

03:42.480 --> 03:46.720
And that, I think, characterizes the historic basis of China.

03:47.280 --> 03:50.640
That's interesting because there's echoes of that in Russian education as well,

03:50.640 --> 03:51.920
as rote memorization.

03:51.920 --> 03:53.760
So you memorize a lot of poetry.

03:53.760 --> 03:58.400
I mean, there's just an emphasis on perfection in all forms

03:59.200 --> 04:03.520
that's not conducive to perhaps what you're speaking to, which is creativity.

04:03.520 --> 04:08.720
But you think that kind of education holds back the innovative spirit

04:08.720 --> 04:10.320
that you might see in the United States?

04:10.880 --> 04:16.400
Well, it holds back the breakthrough innovative spirits that we see in the United States.

04:16.400 --> 04:21.840
But it does not hold back the valuable execution-oriented,

04:21.840 --> 04:27.200
result-oriented value-creating engines, which we see China being very successful.

04:27.920 --> 04:34.720
So is there a difference between a Chinese AI engineer today and an American AI engineer,

04:34.720 --> 04:36.960
perhaps rooted in the culture that we just talked about,

04:36.960 --> 04:41.040
or the education, or the very soul of the people, or no?

04:41.040 --> 04:44.560
And what would your advice be to each if there's a difference?

04:45.440 --> 04:48.560
Well, there's a lot that's similar because AI is about

04:49.840 --> 04:54.800
mastering sciences, about using known technologies and trying new things.

04:54.800 --> 05:00.320
But it's also about picking from many parts of possible networks to use

05:00.320 --> 05:02.960
and different types of parameters to tune.

05:02.960 --> 05:05.360
And that part is somewhat rote.

05:05.360 --> 05:10.160
And it is also, as anyone who's built AI products can tell you,

05:10.160 --> 05:15.200
a lot about cleansing the data, because AI runs better with more data.

05:15.200 --> 05:22.400
And data is generally unstructured, errorful, and unclean.

05:22.400 --> 05:25.520
And the effort to clean the data is immense.

05:26.320 --> 05:32.720
So I think the better part of the American AI engineering process

05:33.280 --> 05:37.200
is to try new things, to do things people haven't done before,

05:37.920 --> 05:42.320
and to use technology to solve most, if not all, problems.

05:43.280 --> 05:47.120
So to make the algorithm work, despite not so great data,

05:47.120 --> 05:49.840
find error-tolerant ways to deal with the data.

05:50.560 --> 05:56.960
The Chinese way would be to basically enumerate, to the fullest extent,

05:56.960 --> 06:02.240
all the possible ways by a lot of machines, try lots of different ways to get it to work,

06:02.240 --> 06:07.600
and spend a lot of resources and money and time cleaning up data.

06:07.600 --> 06:12.880
That means the AI engineer may be writing data cleansing algorithms,

06:12.880 --> 06:19.040
working with thousands of people who label or correct or do things with the data.

06:19.040 --> 06:23.840
That is the incredible hard work that might lead to better results.

06:23.840 --> 06:28.880
So the Chinese engineer would rely on and ask for more and more and more data,

06:28.880 --> 06:32.400
and find ways to cleanse them and make them work in the system,

06:32.400 --> 06:39.440
and probably less time thinking about new algorithms that can overcome data or other issues.

06:39.440 --> 06:40.720
So where's your intuition?

06:40.720 --> 06:44.080
What do you think the biggest impact in the next 10 years lies?

06:44.080 --> 06:52.320
Is it in some breakthrough algorithms or is it in just this at scale rigor,

06:53.040 --> 06:58.320
a rigorous approach to data, cleaning data, organizing data onto the same algorithms?

06:58.320 --> 07:01.680
What do you think the big impact in the applied world is?

07:02.480 --> 07:06.800
Well, if you're really in the company and you have to deliver results,

07:06.800 --> 07:12.160
using known techniques and enhancing data seems like the more expedient approach.

07:12.160 --> 07:17.120
That's very low risk and likely to generate better and better results.

07:17.120 --> 07:19.840
And that's why the Chinese approach has done quite well.

07:20.480 --> 07:27.440
Now, there are a lot of more challenging startups and problems, such as autonomous vehicles,

07:28.400 --> 07:33.200
medical diagnosis, that existing algorithms may probably won't solve.

07:34.240 --> 07:38.640
And that would put the Chinese approach more challenged

07:38.640 --> 07:45.360
and give them more breakthrough innovation approach, more of an edge on those kinds of problems.

07:45.360 --> 07:46.960
So let me talk to that a little more.

07:46.960 --> 07:52.800
So my intuition, personally, is that data can take us extremely far.

07:53.600 --> 07:56.320
So you brought up autonomous vehicles and medical diagnosis.

07:56.320 --> 08:02.960
So your intuition is that huge amounts of data might not be able to completely help us solve that problem.

08:03.920 --> 08:08.000
Right. So breaking that down further in autonomous vehicle,

08:08.000 --> 08:13.040
I think huge amounts of data probably will solve trucks driving on highways,

08:13.040 --> 08:15.520
which will deliver significant value.

08:15.520 --> 08:17.760
And China will probably lead in that.

08:19.280 --> 08:25.680
And full L5 autonomous is likely to require new technologies we don't yet know.

08:26.240 --> 08:30.320
And that might require academia and great industrial research,

08:30.320 --> 08:32.400
both innovating and working together.

08:32.400 --> 08:34.720
And in that case, US has an advantage.

08:35.200 --> 08:39.280
So the interesting question there is, I don't know if you're familiar on the autonomous vehicle

08:39.280 --> 08:45.520
space and the developments with Tesla and Elon Musk, where they are, in fact,

08:46.480 --> 08:54.240
a full steam ahead into this mysterious complex world of full autonomy, L5, L4, L5,

08:55.120 --> 08:58.720
and they're trying to solve that purely with data.

08:58.720 --> 09:02.000
So the same kind of thing that you're saying is just for highway,

09:02.000 --> 09:04.560
which is what a lot of people share your intuition.

09:05.360 --> 09:07.120
They're trying to solve with data.

09:07.120 --> 09:09.280
It's just to linger on that moment further.

09:09.280 --> 09:12.480
Do you think possible for them to achieve success

09:13.520 --> 09:17.840
with simply just a huge amount of this training on edge cases,

09:17.840 --> 09:21.520
on difficult cases in urban environments, not just highway and so on?

09:22.720 --> 09:24.480
I think they'll be very hard.

09:24.480 --> 09:29.760
One could characterize Tesla's approach as kind of a Chinese strength approach,

09:29.760 --> 09:33.280
gather all the data you can and hope that will overcome the problems.

09:33.920 --> 09:41.440
But in autonomous driving, clearly a lot of the decisions aren't merely solved by aggregating data

09:41.440 --> 09:43.440
and having feedback loop.

09:43.440 --> 09:47.280
There are things that are more akin to human thinking.

09:47.920 --> 09:50.560
And how would those be integrated and built?

09:51.520 --> 09:56.320
There has not yet been a lot of success integrating human intelligence,

09:56.320 --> 10:02.320
or call it expert systems, if you will, even though that's a taboo word with the machine learning.

10:02.800 --> 10:07.120
And the integration of the two types of thinking hasn't yet been demonstrated.

10:07.760 --> 10:11.520
And the question is, how much can you push a purely machine learning approach?

10:12.320 --> 10:17.440
And of course, Tesla also has an additional constraint that they don't have all the sensors.

10:18.320 --> 10:20.960
I know that they think it's foolish to use lidars,

10:20.960 --> 10:27.520
but that's clearly a one less very valuable and reliable source of input

10:27.520 --> 10:30.720
that they're foregoing, which may also have consequences.

10:32.320 --> 10:36.320
I think the advantage, of course, is capturing data that no one has ever seen before.

10:36.960 --> 10:42.080
And in some cases, such as computer vision and speech recognition,

10:42.080 --> 10:47.200
I have seen Chinese companies accumulate data that's not seen anywhere in the Western world,

10:47.200 --> 10:50.080
and they have delivered superior results.

10:50.080 --> 10:57.040
But then speech recognition and object recognition are relatively suitable problems for deep learning

10:57.040 --> 11:04.320
and don't have the potentially need for the human intelligence analytical planning elements.

11:04.320 --> 11:06.320
And the same on the speech recognition side,

11:06.320 --> 11:10.640
your intuition that speech recognition and the machine learning approaches to speech

11:10.640 --> 11:15.840
recognition won't take us to a conversational system that can pass the Turing test,

11:15.840 --> 11:20.000
which is sort of maybe akin to what driving is.

11:20.000 --> 11:22.480
So it needs to have something more than just simply

11:23.440 --> 11:26.800
simple language understanding, simple language generation.

11:26.800 --> 11:32.400
Roughly right, I would say that based on purely machine learning approaches,

11:32.400 --> 11:41.760
it's hard to imagine it could lead to a full conversational experience across arbitrary domains,

11:41.760 --> 11:43.760
which is akin to L5.

11:43.760 --> 11:49.760
I'm a little hesitant to use the word Turing test because the original definition was probably too easy.

11:50.160 --> 11:51.280
We probably do that.

11:52.240 --> 11:55.120
The spirit of the Turing test is what I was referring to.

11:56.400 --> 12:01.520
So you've had major leadership research positions at Apple, Microsoft, Google.

12:01.520 --> 12:07.280
So continuing on the discussion of America, Russia, Chinese soul and culture and so on,

12:09.120 --> 12:16.320
what is the culture of Silicon Valley in contrast to China and maybe U.S. broadly,

12:16.320 --> 12:22.000
and what is the unique culture of each of these three major companies in your view?

12:22.000 --> 12:27.120
I think in aggregate, Silicon Valley companies, and we could probably include Microsoft in that,

12:27.120 --> 12:33.280
even though they're not in the Valley, is really dream big and have visionary goals

12:33.920 --> 12:42.240
and believe that technology will conquer all and also the self-confidence and the self-entitlement

12:42.240 --> 12:46.080
that whatever they produce, the whole world should use and must use.

12:47.200 --> 12:51.920
And those are historically important.

12:51.920 --> 12:58.960
I think Steve Jobs' famous quote that he doesn't do focus groups.

12:58.960 --> 13:03.520
He looks in the mirror and asks the person in the mirror, what do you want?

13:03.520 --> 13:09.760
And that really is an inspirational comment that says the great company shouldn't just

13:09.760 --> 13:16.160
ask users what they want, but develop something that users will know they want when they see it,

13:16.160 --> 13:18.880
but they could never come up with themselves.

13:18.880 --> 13:25.840
I think that is probably the most exhilarating description of what the essence of Silicon Valley

13:25.840 --> 13:32.640
is, that this brilliant idea could cause you to build something that couldn't come out of

13:32.640 --> 13:38.000
focus groups or A.B. tests. And iPhone would be an example of that.

13:38.000 --> 13:42.160
No one in the age of Blackberry would write down they want an iPhone or multi-touch,

13:42.720 --> 13:47.360
a browser might be another example, no one would say they want that in the days of FTP,

13:47.360 --> 13:49.360
but once they see it, they want it.

13:49.360 --> 13:53.200
So I think that is what Silicon Valley is best at.

13:55.520 --> 13:58.800
But it also came with a lot of success.

13:58.800 --> 14:04.400
These products became global platforms and there were basically no competitors anywhere.

14:05.040 --> 14:12.640
And that has also led to a belief that these are the only things that one should do,

14:13.200 --> 14:17.360
that companies should not tread on other company's territory.

14:18.000 --> 14:26.160
So that's a, you know, Groupon and the Yelp and then OpenTable and the Grubhub would each feel,

14:26.160 --> 14:30.960
okay, I'm not going to do the other company's business because that would not be the pride

14:31.040 --> 14:35.600
of innovating what each of these four companies have innovated.

14:36.880 --> 14:42.080
But I think the Chinese approach is do whatever it takes to win.

14:42.720 --> 14:44.960
And it's a winner take all market.

14:44.960 --> 14:50.880
And in fact, in the internet space, the market leader will get predominantly all the value

14:50.880 --> 14:52.800
extracted out of the system.

14:53.360 --> 14:59.520
So and the system isn't just defined as one narrow category,

14:59.520 --> 15:01.360
but gets broader and broader.

15:01.360 --> 15:09.600
So it's amazing ambition for success and domination of increasingly larger

15:10.240 --> 15:17.040
product categories leading to clear market winner status and the opportunity to extract

15:17.040 --> 15:18.160
tremendous value.

15:19.040 --> 15:28.640
And that develops a practical, result oriented, ultra ambitious winner take all,

15:28.720 --> 15:30.720
gladiatorial mentality.

15:31.520 --> 15:38.640
And if what it takes is to build what the competitors built, essentially a copycat,

15:38.640 --> 15:41.840
that can be done without infringing laws.

15:41.840 --> 15:47.680
If what it takes is to satisfy a foreign countries need by forking the code base and

15:47.680 --> 15:50.880
building something that looks really ugly and different, they'll do it.

15:51.440 --> 15:55.680
So it's contrasted very sharply with the Silicon Valley approach.

15:56.160 --> 16:01.840
And I think the flexibility and the speed and execution has helped the Chinese approach.

16:01.840 --> 16:09.600
And I think the Silicon Valley approach is potentially challenged if every Chinese

16:09.600 --> 16:14.560
entrepreneurs learning from the whole world, US and China and the American entrepreneurs

16:14.560 --> 16:18.640
only look internally and write off China as a copycat.

16:19.520 --> 16:23.440
And the second part of your question about the three companies,

16:23.440 --> 16:26.000
the unique elements of the three companies perhaps.

16:26.000 --> 16:37.440
Yeah, I think Apple represents while the user please the user and the essence of design and

16:37.440 --> 16:46.560
brand and it's the one company and perhaps the only tech company that draws people with a

16:48.160 --> 16:53.120
strong serious desire for the product and the need and the willingness to pay a premium.

16:53.520 --> 16:59.440
Because of the halo effect of the brand, which came from the attention to detail

17:00.000 --> 17:01.920
and great respect for user needs.

17:03.360 --> 17:13.680
Microsoft represents a platform approach that builds giant products that become very strong

17:13.680 --> 17:20.880
modes that others can't do because it's well architected at the bottom level.

17:21.520 --> 17:26.720
And the work is efficiently delegated to individuals.

17:26.720 --> 17:33.600
And then the whole product is built by adding small parts that sum together.

17:33.600 --> 17:39.920
So it's probably the most effective high tech assembly line that builds a very difficult

17:39.920 --> 17:49.920
product that and the whole process of doing that is kind of a differentiation and something

17:49.920 --> 17:52.480
competitors can't easily repeat.

17:52.480 --> 17:58.480
Are there elements of the Chinese approach in the way Microsoft went about assembling those

17:58.480 --> 18:04.560
little pieces and dominating essentially dominating the market for a long time or do you see those

18:04.560 --> 18:05.520
as distinct?

18:05.520 --> 18:07.360
I think there are elements that are the same.

18:08.160 --> 18:14.400
I think the three American companies that had or have Chinese characteristics and obviously

18:14.400 --> 18:19.680
as well as American characteristics are Microsoft, Facebook and Amazon.

18:20.320 --> 18:21.200
Yes, that's right.

18:21.200 --> 18:28.880
Because these are companies that will tenaciously go after adjacent markets, build up strong

18:29.920 --> 18:39.120
product offering and find ways to extract greater value from a sphere that's ever increasing.

18:40.000 --> 18:42.720
And they understand the value of the platforms.

18:43.520 --> 18:44.880
So that's the similarity.

18:45.520 --> 18:55.840
And then with Google, I think it's a genuinely value oriented company that does have a heart

18:55.840 --> 19:01.040
and soul and that wants to do great things for the world by connecting information.

19:01.840 --> 19:14.160
And that has also very strong technology genes and wants to use technology and has found

19:15.920 --> 19:22.720
out of the box ways to use technology to deliver incredible value to the end user.

19:23.520 --> 19:26.400
If we can look at Google, for example, you mentioned heart and soul.

19:26.880 --> 19:33.920
There seems to be an element where Google is after making the world better.

19:33.920 --> 19:35.520
There's a more positive view.

19:35.520 --> 19:39.840
I mean, they used to have the slogan, don't be evil and Facebook a little bit more has

19:39.840 --> 19:44.480
a negative tend to it, at least in the perception of privacy and so on.

19:44.480 --> 19:51.440
Do you have a sense of how these different companies can achieve because you've talked

19:51.440 --> 19:55.280
about how much we can make the world better in all these kinds of ways with AI?

19:55.600 --> 20:01.120
What is it about a company that can make, give it a heart and soul, gain the trust

20:01.120 --> 20:05.760
of the public and just actually just not be evil and do good for the world?

20:06.800 --> 20:07.760
It's really hard.

20:07.760 --> 20:10.560
And I think Google has struggled with that.

20:11.760 --> 20:18.160
First, that don't do evil mantra is very dangerous because every employee's definition

20:18.160 --> 20:19.360
of evil is different.

20:19.360 --> 20:23.120
And that has led to some difficult employee situations for them.

20:23.840 --> 20:29.600
So I don't necessarily think that's a good value statement, but just watching the kinds

20:29.600 --> 20:35.920
of things Google or its parent company Alphabet does in new areas like healthcare,

20:35.920 --> 20:40.560
like, you know, eradicating mosquitoes, things that are really not in the business

20:40.560 --> 20:42.720
of a internet tech company.

20:42.720 --> 20:48.720
I think that shows that there is a heart and soul and desire to do good and willingness

20:49.680 --> 20:56.880
to put in the resources to do something when they see it's good, they will pursue it.

20:58.240 --> 21:02.400
That doesn't necessarily mean it has all the trust of the users.

21:02.400 --> 21:09.680
I realize while most people would view Facebook as the primary target of their recent unhappiness

21:09.680 --> 21:14.000
about Silicon Valley companies, many would put Google in that category.

21:14.000 --> 21:19.200
And some have named Google's business practices as predatory also.

21:19.840 --> 21:26.720
So it's kind of difficult to have the two parts of a body, the brain wants to do what

21:26.720 --> 21:30.560
it's supposed to do for a shareholder, maximize profit, and then the heart and soul

21:30.560 --> 21:36.080
wants to do good things that may run against what the brain wants to do.

21:36.080 --> 21:41.680
So in this complex balancing that these companies have to do, you've mentioned that

21:41.680 --> 21:47.280
you're concerned about a future where too few companies like Google, Facebook, Amazon

21:47.280 --> 21:53.280
are controlling our data or controlling too much of our digital lives.

21:53.280 --> 21:57.280
Can you elaborate on this concern and perhaps do you have a better way forward?

21:58.640 --> 22:03.600
I think I'm hardly the most vocal complainer of this.

22:04.880 --> 22:07.280
There are a lot louder complainers out there.

22:07.280 --> 22:16.880
I do observe that having a lot of data does perpetuate their strength and limit competition

22:16.880 --> 22:18.320
in many spaces.

22:19.360 --> 22:24.080
But I also believe AI is much broader than the internet space.

22:24.080 --> 22:32.400
So the entrepreneurial opportunities still exists in using AI to empower financial, retail,

22:32.400 --> 22:35.360
manufacturing, education, and applications.

22:35.360 --> 22:42.720
So I don't think it's quite a case of full monopolistic dominance that totally stifles

22:42.720 --> 22:43.840
innovation.

22:43.840 --> 22:48.480
But I do believe in their areas of strength, it's hard to dislodge them.

22:49.680 --> 22:52.640
I don't know if I have a good solution.

22:53.280 --> 22:58.320
Probably the best solution is let the entrepreneurial VC ecosystem work well

22:58.960 --> 23:04.240
and find all the places that can create the next Google, the next Facebook.

23:04.240 --> 23:07.920
So there will always be increasing number of challengers.

23:08.480 --> 23:11.280
In some sense that has happened a little bit.

23:11.280 --> 23:17.280
You see Uber, Airbnb having emerged despite the strength of the big three.

23:20.240 --> 23:25.200
I think China as an environment may be more interesting for the emergence

23:25.200 --> 23:32.800
because if you look at companies between let's say 50 to 300 billion dollars,

23:33.520 --> 23:40.320
China has emerged more of such companies than the US in the last three to four years because

23:40.320 --> 23:45.440
of the larger marketplace, because of the more fearless nature of the entrepreneurs

23:46.400 --> 23:50.800
that and the Chinese giants are just as powerful as American ones.

23:50.800 --> 23:58.080
Tencent Alibaba are very strong, but Bytes Dance has emerged worth 75 billion and financial.

23:58.080 --> 24:03.040
While it's Alibaba affiliated, it's nevertheless independent and worth 150 billion.

24:03.840 --> 24:11.120
And so I do think if we start to extend to traditional businesses, we will see very

24:11.120 --> 24:12.640
valuable companies.

24:12.640 --> 24:19.600
So it's probably not the case that in five or 10 years we'll still see the whole world

24:19.600 --> 24:21.920
with these five companies having such dominance.

24:22.560 --> 24:28.320
So you mentioned a couple of times this fascinating world of entrepreneurship in China

24:29.120 --> 24:31.040
of the fearless nature of the entrepreneurs.

24:31.040 --> 24:35.360
So can you maybe talk a little bit about what it takes to be an entrepreneur in China?

24:35.360 --> 24:37.600
What are the strategies that are undertaken?

24:38.320 --> 24:41.040
What are the ways that you success?

24:41.040 --> 24:46.400
What is the dynamic of VCF funding of the way the government helps companies and so on?

24:46.400 --> 24:49.360
What are the interesting aspects here that are distinct from,

24:49.360 --> 24:54.000
that are different from the Silicon Valley world of entrepreneurship?

24:55.120 --> 25:02.320
Well, many of the listeners probably still would brand Chinese entrepreneur as copycats.

25:02.960 --> 25:08.000
And no doubt 10 years ago, that would not be an inaccurate description.

25:09.040 --> 25:15.280
Back 10 years ago, an entrepreneur probably could not get funding if he or she could not describe

25:15.920 --> 25:19.200
what product he or she is copying from the US.

25:20.320 --> 25:23.440
The first question is, who has proven this business model,

25:23.440 --> 25:26.400
which is a nice way of asking, who are you copying?

25:27.120 --> 25:34.080
And that reason is understandable because China had a much lower internet penetration

25:34.800 --> 25:42.480
and didn't have enough indigenous experience to build innovative products.

25:43.200 --> 25:47.600
And secondly, internet was emerging.

25:47.600 --> 25:52.880
Link startup was the way to do things, building a first minimally viable product

25:52.880 --> 25:54.640
and then expanding was the right way to go.

25:55.280 --> 26:02.480
And the American successes have given a shortcut that if you built your minimally viable product

26:02.480 --> 26:06.640
based on an American product, it's guaranteed to be a decent starting point.

26:06.640 --> 26:07.840
Then you tweak it afterwards.

26:08.400 --> 26:12.160
So as long as there are no IP infringement, which as far as I know,

26:12.160 --> 26:18.560
there hasn't been in the mobile and AI spaces, that's a much better shortcut.

26:19.360 --> 26:24.400
And I think Silicon Valley would view that as still not very honorable

26:25.120 --> 26:29.120
because that's not your own idea to start with.

26:29.120 --> 26:35.040
But you can't really at the same time believe every idea must be your own

26:35.120 --> 26:38.480
and believe in the link startup methodology because

26:39.360 --> 26:43.520
link startup is intended to try many, many things and then converge when that works.

26:44.160 --> 26:46.720
And it's meant to be iterated and changed.

26:46.720 --> 26:50.000
So finding a decent starting point without legal violations,

26:51.120 --> 26:55.440
there should be nothing morally dishonorable about that.

26:55.440 --> 26:57.040
So just a quick pause on that.

26:57.040 --> 27:01.840
It's fascinating that that's why is that not honorable, right?

27:01.920 --> 27:06.960
Exactly as you formulated, it seems like a perfect start for business

27:08.000 --> 27:14.400
is to take a look at Amazon and say, okay, we'll do exactly what Amazon is doing.

27:14.400 --> 27:19.120
Let's start there in this particular market and then let's out innovate them

27:19.120 --> 27:22.080
from that starting point and come up with new ways.

27:22.080 --> 27:27.040
I mean, is it wrong to be, except the word copycat just sounds bad,

27:27.040 --> 27:28.720
but is it wrong to be a copycat?

27:28.720 --> 27:31.520
It just seems like a smart strategy.

27:31.520 --> 27:38.960
But yes, doesn't have a heroic nature to it that like Steve Jobs,

27:39.920 --> 27:43.760
Elon Musk sort of in something completely coming up with something completely new.

27:43.760 --> 27:45.200
Yeah, I like the way you describe it.

27:45.200 --> 27:52.240
It's a non-heroic acceptable way to start the company and maybe more expedient.

27:52.880 --> 28:00.080
So that's, I think, a baggage for Silicon Valley, that if it doesn't let go,

28:00.720 --> 28:05.040
then it may limit the ultimate ceiling of the company.

28:05.040 --> 28:06.560
Take Snapchat as an example.

28:07.120 --> 28:11.440
I think, you know, Evan's brilliant, he built a great product,

28:11.440 --> 28:16.720
but he's very proud that he wants to build his own features, not copy others.

28:16.720 --> 28:20.960
While Facebook was more willing to copy his features,

28:20.960 --> 28:22.800
and you see what happens in the competition.

28:23.440 --> 28:30.160
So I think putting that handcuff on the company would limit its ability to reach the maximum

28:30.160 --> 28:36.720
potential. So back to the Chinese environment, copying was merely a way to learn from the

28:36.720 --> 28:43.360
American masters. Just like we, if you would, we learned to play piano or painting,

28:43.360 --> 28:44.480
you start by copying.

28:44.480 --> 28:48.160
You don't start by innovating when you don't have the basic skill sets.

28:48.160 --> 28:56.080
So very amazingly, the Chinese entrepreneurs, about six years ago, started to branch off

28:56.080 --> 29:02.160
with these lean startups built on American ideas to build better products than American products.

29:02.160 --> 29:04.240
But they did start from the American idea.

29:04.960 --> 29:08.560
And today, WeChat is better than WhatsApp.

29:08.560 --> 29:09.920
Weibo is better than Twitter.

29:10.480 --> 29:12.880
Zihu is better than Quora and so on.

29:12.880 --> 29:17.840
So that I think is Chinese entrepreneurs going to step two.

29:18.400 --> 29:23.680
And then step three is once these entrepreneurs have done one or two of these companies,

29:23.680 --> 29:27.360
they now look at the Chinese market and the opportunities

29:27.360 --> 29:30.000
and come up with ideas that didn't exist elsewhere.

29:30.640 --> 29:37.520
So products like and financial under which includes AliPay, which is mobile payments.

29:38.320 --> 29:44.320
And also the financial products for loans built on that.

29:44.320 --> 29:47.280
And also in education VIP kid.

29:48.560 --> 29:57.600
And in social video, social network, TikTok, and in social e-commerce, Pinduoduo.

29:58.640 --> 30:01.680
And then in ride sharing, Mobike.

30:01.680 --> 30:08.080
These are all Chinese innovative products that now are being copied elsewhere.

30:08.720 --> 30:15.440
So and an additional interesting observation is some of these products are built on unique

30:15.440 --> 30:21.760
Chinese demographics, which may not work in the US, but may work very well in Southeast Asia,

30:22.400 --> 30:27.760
Africa, and other developing worlds that are a few years behind China.

30:27.760 --> 30:32.160
And a few of these products maybe are universal and are getting traction,

30:32.160 --> 30:34.640
even in the United States, such as TikTok.

30:35.360 --> 30:43.440
So this whole ecosystem is supported by VCs as a virtuous cycle,

30:43.440 --> 30:48.720
because a large market with with the innovative entrepreneurs will draw a lot of money.

30:49.360 --> 30:53.680
And then investing these companies, as the market gets larger and larger,

30:53.680 --> 30:57.440
US mark China market is easily three, four times larger than the US.

30:58.320 --> 31:02.160
They will create greater value and greater returns for the VCs,

31:02.160 --> 31:04.480
thereby raising even more money.

31:05.360 --> 31:09.840
So at Sinovation Ventures, our first fund was 15 million.

31:09.840 --> 31:11.440
Our last fund was 500 million.

31:12.000 --> 31:19.040
So it reflects the valuation of the companies and our us going multi stage and things like that.

31:19.680 --> 31:21.600
It also has government support.

31:22.880 --> 31:26.000
But not in the way most Americans would think of it.

31:26.000 --> 31:31.120
The government actually leaves the entrepreneurial space as a private enterprise,

31:31.200 --> 31:36.640
so the self regulating and the government would build infrastructures that would

31:37.280 --> 31:39.200
around it to make it work better.

31:39.200 --> 31:44.800
For example, the mass entrepreneur mass innovation plan built 8000 incubators.

31:44.800 --> 31:49.520
So the pipeline is very strong to the VCs for autonomous vehicles.

31:49.520 --> 31:54.240
The Chinese government is building smart highways with sensors,

31:54.240 --> 32:00.720
smart cities that separate pedestrians from cars that may allow initially an inferior

32:01.120 --> 32:06.800
autonomous vehicle company to launch a car without increasing with lower casualty,

32:07.600 --> 32:10.800
because the roads or the city is smart.

32:11.440 --> 32:16.560
And the Chinese government at local levels would have these guiding funds acting as

32:16.560 --> 32:19.360
LPs, passive LPs to funds.

32:19.360 --> 32:25.920
And when the fund makes money, part of the money made is given back to the GPs and potentially

32:25.920 --> 32:33.600
other LPs to increase everybody's return at the expense of the government's return.

32:33.600 --> 32:41.520
So that's an interesting incentive that entrusts the task of choosing entrepreneurs to VCs

32:41.520 --> 32:46.560
who are better at it than the government by letting some of the profits move that way.

32:46.560 --> 32:48.640
So this is really fascinating, right?

32:48.640 --> 32:54.240
So I look at the Russian government as a case study where let me put it this way,

32:54.240 --> 33:00.080
there's no such government driven large scale support of entrepreneurship.

33:00.800 --> 33:06.960
And probably the same is true in the United States, but the entrepreneurs themselves kind of find a way.

33:07.600 --> 33:15.520
So maybe in a form of advice or explanation, how did the Chinese government arrive to be this way,

33:15.520 --> 33:21.920
so supportive on entrepreneurship to be in this particular way so forward thinking at such a

33:21.920 --> 33:26.720
large scale, and also perhaps, how can we copy it in other countries?

33:28.160 --> 33:31.440
How can we encourage other governments, like even the United States government,

33:31.440 --> 33:35.520
to support infrastructure for autonomous vehicles in that same kind of way, perhaps?

33:35.520 --> 33:45.920
Yes. So these techniques are the result of several key things, some of which may be learnable,

33:45.920 --> 33:47.280
some of which may be very hard.

33:48.000 --> 33:52.800
One is just trial and error and watching what everyone else is doing.

33:52.800 --> 33:56.800
I think it's important to be humble and not feel like you know all the answers.

33:56.800 --> 34:00.560
The guiding funds idea came from Singapore, which came from Israel,

34:01.360 --> 34:07.760
and China made a few tweaks and turned it into a, because the Chinese cities

34:07.760 --> 34:12.720
and government officials kind of compete with each other because they all want to

34:12.720 --> 34:19.440
make their city more successful, so they can get the next level in their political career,

34:20.240 --> 34:25.120
and it's somewhat competitive. So the central government made it a bit of a competition.

34:25.120 --> 34:31.040
Everybody has a budget, they can put it on AI, or they can put it on bio, or they can put it on

34:31.040 --> 34:36.000
energy, and then whoever gets the results, the city shines, the people are better off,

34:36.000 --> 34:42.240
the mayor gets a promotion. So the tools, this is kind of almost like an entrepreneurial environment

34:42.800 --> 34:46.400
for local governments to see who can do a better job.

34:47.440 --> 34:55.040
And also many of them tried different experiments. Some have given award to

34:55.760 --> 35:00.800
very smart researchers, just give them money and hope they'll start a company.

35:00.800 --> 35:08.000
Some have given money to academic research labs, maybe government research labs,

35:08.000 --> 35:13.040
to see if they can spin off some companies from the science lab or something like that.

35:14.000 --> 35:18.960
Some have tried to recruit overseas Chinese to come back and start companies,

35:18.960 --> 35:23.360
and they've had mixed results. The one that worked the best was the guiding funds.

35:23.360 --> 35:27.920
So it's almost like a lean startup idea where people try different things and

35:27.920 --> 35:32.800
what works sticks and everybody copies. So now every city has a guiding fund.

35:32.800 --> 35:41.120
So that's how that came about. The autonomous vehicle and the massive spending in highways

35:41.120 --> 35:49.440
and smart cities, that's a Chinese way. It's about building infrastructure to facilitate.

35:49.440 --> 35:55.920
It's a clear division of the government's responsibility from the market. The market

35:55.920 --> 36:02.480
should do everything in a private freeway, but there are things the market can't afford to do.

36:02.880 --> 36:09.520
Like infrastructure. So the government always appropriates large amounts of money

36:09.520 --> 36:16.800
for infrastructure building. This happens with not only autonomous vehicle and AI,

36:16.800 --> 36:25.920
but happened with the 3G and 4G. You'll find that the Chinese wireless reception is better than the

36:25.920 --> 36:31.920
US because massive spending that tries to cover the whole country, whereas in the US it may be

36:31.920 --> 36:38.560
a little spotty. It's a government driven because I think they view the coverage of

36:41.040 --> 36:46.160
cell access and 3G, 4G access to be a governmental infrastructure spending

36:46.960 --> 36:54.160
as opposed to capitalistic. So of course their state-owned enterprises are also publicly traded,

36:54.160 --> 37:00.080
but they also carry a government responsibility to deliver infrastructure to all.

37:00.080 --> 37:05.280
So it's a different way of thinking that may be very hard to inject into western countries

37:05.280 --> 37:10.320
to say starting tomorrow, bandwidth infrastructure and highways are going to be

37:12.080 --> 37:15.040
governmental spending with some characteristics.

37:16.160 --> 37:20.160
What's your sense and sorry to interrupt, but because it's such a fascinating point.

37:21.520 --> 37:23.760
Do you think on the autonomous vehicle space

37:24.400 --> 37:31.440
it's possible to solve the problem of full autonomy without significant investment in

37:31.440 --> 37:38.160
infrastructure? Well, that's really hard to speculate. I think it's not a yes-no question,

37:38.160 --> 37:45.360
but how long does it take question? You know, 15 years, 30 years, 45 years. Clearly with

37:45.360 --> 37:52.640
infrastructure augmentation, whether it's road, city or whole city planning, building a new city,

37:53.280 --> 38:00.800
I'm sure that will accelerate the day of the L5. I'm not knowledgeable enough and it's hard to

38:00.800 --> 38:07.520
predict even when we're knowledgeable because a lot of it is speculative. But in the U.S.,

38:07.520 --> 38:12.960
I don't think people would consider building a new city the size of Chicago to make it the AI

38:12.960 --> 38:19.360
slash autonomous city. There are smaller ones being built, I'm aware of that. But is infrastructure

38:20.320 --> 38:26.720
spent really impossible for U.S. or Western countries? I don't think so. The U.S. highway

38:26.720 --> 38:36.400
system was built. Was that during President Eisenhower or Kennedy? So maybe historians can

38:36.400 --> 38:42.640
study how the President Eisenhower get the resources to build this massive infrastructure

38:42.720 --> 38:50.720
that surely gave U.S. a tremendous amount of prosperity over the next decade, if not century.

38:50.720 --> 38:55.920
If I may comment on that then, it takes us to artificial intelligence a little bit because

38:55.920 --> 39:01.600
in order to build infrastructure, it creates a lot of jobs. So I'll be actually interested

39:02.320 --> 39:07.520
if you would say that you're talking your book about all kinds of jobs that could and could not

39:07.520 --> 39:14.240
be automated. I wonder if building infrastructure is one of the jobs that would not be easily

39:14.240 --> 39:18.960
automated. It's something you can think about because I think you've mentioned somewhere in a

39:18.960 --> 39:26.080
talk or that there might be, as jobs are being automated, a role for government to create jobs

39:26.080 --> 39:33.200
that can't be automated. Yes, I think that's a possibility. Back in the last financial crisis,

39:34.160 --> 39:43.280
China put a lot of money to basically give this economy a boost and a lot of it went into

39:43.280 --> 39:50.080
infrastructure building. And I think that's a legitimate way at the government level to

39:51.840 --> 39:57.680
deal with the employment issues as well as build out the infrastructure. As long as the

39:57.680 --> 40:02.320
infrastructures are truly needed and as long as there isn't an employment problem, which

40:03.040 --> 40:11.920
no, we don't know. So maybe taking a little step back, if you've been a leader and a researcher

40:11.920 --> 40:20.880
in AI for several decades, at least 30 years, so how has AI changed in the West and the East

40:20.880 --> 40:24.160
as you've observed as you've been deep in it over the past 30 years?

40:24.960 --> 40:31.520
Well, AI began as the pursuit of understanding human intelligence and the term itself

40:32.560 --> 40:38.800
represents that. But it kind of drifted into the one subarea that worked extremely well,

40:38.800 --> 40:45.200
which is machine intelligence. And that's actually more using pattern recognition techniques to

40:46.560 --> 40:53.840
basically do incredibly well on a limited domain, large amount of data, but relatively

40:53.920 --> 41:01.360
simple kinds of planning tasks and not very creative. So we didn't end up building human

41:01.360 --> 41:08.000
intelligence. We built a different machine that was a lot better than us, some problems,

41:08.000 --> 41:14.960
but nowhere close to us on other problems. So today, I think a lot of people still misunderstand

41:16.160 --> 41:21.840
when we say artificial intelligence and what various products can do, people still think

41:21.920 --> 41:27.760
it's about replicating human intelligence. But the products out there really are closer to

41:29.120 --> 41:34.240
having invented the internet or the spreadsheet or the database and getting broader adoption.

41:35.280 --> 41:41.200
And speaking further to the fears, near-term fears that people have about AI, so you're commenting

41:41.200 --> 41:48.560
on the general intelligence that people in the popular culture from sci-fi movies have a sense

41:48.560 --> 41:54.720
about AI. But there's practical fears about AI, the kind, the narrow AI that you're talking about

41:54.720 --> 42:00.480
of automating particular kinds of jobs. And you talk about them in the book. So what are the kinds

42:00.480 --> 42:07.680
of jobs in your view that you see in the next five, 10 years beginning to be automated by AI systems,

42:07.680 --> 42:15.200
algorithms? Yes, this is also maybe a little bit counterintuitive, because it's the routine jobs

42:15.200 --> 42:24.000
that will be displaced the soonest. And they may not be displaced entirely, maybe 50%, 80% of a job,

42:24.000 --> 42:30.640
but when the workload drops by that much, employment will come down. And also another part of

42:30.640 --> 42:36.560
misunderstanding is most people think of AI replacing routine jobs, then they think of the

42:36.560 --> 42:42.960
assembly line, the workers. Well, that will have some effect, but it's actually the routine white

42:42.960 --> 42:49.680
color workers that's easiest to replace. Because to replace a white color worker, you just need

42:49.680 --> 42:58.400
software. To replace a blue color worker, you need robotics, mechanical excellence, and the ability to

42:58.400 --> 43:07.040
deal with dexterity, and maybe even unknown environments, very, very difficult. So if we were

43:07.040 --> 43:14.960
to categorize the most dangerous white collar jobs, they would be things like back office,

43:15.520 --> 43:24.240
people who copy and paste and deal with simple computer programs and data, and maybe paper and

43:24.240 --> 43:32.960
OCR, and they don't make strategic decisions, they basically facilitate the process, these softwares

43:32.960 --> 43:38.880
and paper systems don't work. So you have people dealing with new employee orientation,

43:40.640 --> 43:47.280
searching for past lawsuits and financial documents, and doing reference check,

43:47.840 --> 43:53.360
the basic searching and management of data, that's the most in danger of being lost. In addition to

43:53.360 --> 44:00.480
the white collar repetitive work, a lot of simple interaction work can also be taken care of, such

44:00.480 --> 44:08.480
as tele-sales, telemarketing, customer service, as well as many physical jobs that are in the same

44:08.480 --> 44:16.720
location and don't require a high degree of dexterity. So fruit picking, dishwashing, assembly line,

44:16.720 --> 44:24.000
inspection are jobs in that category. So altogether, back office is a big part.

44:24.960 --> 44:31.920
And the other, the blue collar may be smaller initially, but over time AI will get better.

44:31.920 --> 44:38.720
And when we start to get to over the next 15, 20 years, the ability to actually have the dexterity

44:38.720 --> 44:44.960
of doing assembly line, that's a huge chunk of jobs. And when autonomous vehicles start to work,

44:44.960 --> 44:50.560
initially starting with truck drivers, but eventually to all drivers, that's another huge group of

44:50.960 --> 44:58.000
workers. So I see modest numbers in the next five years, but increasing rapidly after that.

44:58.000 --> 45:02.720
On the worry of the jobs that are in danger and the gradual loss of jobs.

45:03.920 --> 45:06.000
I'm not sure if you're familiar with Andrew Yang.

45:06.560 --> 45:07.120
Yes, I am.

45:07.760 --> 45:13.760
So there's a candidate for president of the United States whose platform Andrew Yang is based around,

45:14.320 --> 45:21.680
in part, around job loss due to automation. And also in addition, the need perhaps of universal

45:21.680 --> 45:28.560
basic income to support jobs that are folks who lose their job due to automation and so on. And

45:28.560 --> 45:36.000
in general, support people under complex, unstable job market. So what are your thoughts about his

45:36.000 --> 45:42.400
concerns, him as a candidate, his ideas in general? I think his thinking is generally in the right

45:42.400 --> 45:50.560
direction. But his approach as a presidential candidate may be a little bit ahead of the time.

45:52.160 --> 45:58.800
I think the displacements will happen. But will they happen soon enough for people to agree to

45:58.800 --> 46:06.720
vote for him? The unemployment numbers are not very high yet. And I think he and I have the same

46:06.720 --> 46:12.400
challenge. If I want to theoretically convince people this is an issue and he wants to become

46:12.400 --> 46:19.680
the president, people have to see how can this be the case when unemployment numbers are low.

46:19.680 --> 46:26.400
So that is the challenge. And I think we do, I do agree with him on the displacement issue,

46:27.280 --> 46:35.440
on universal basic income. At a very vanilla level, I don't agree with it. Because I think

46:35.440 --> 46:43.920
the main issue is retraining. So people need to be incented, not by just giving a monthly $2,000

46:43.920 --> 46:52.400
check or $1,000 check and do whatever they want, because they don't have the know how to know what

46:52.400 --> 47:00.400
to retrain to go into what type of a job and guidance is needed. And retraining is needed

47:00.400 --> 47:05.040
because historically, when technology revolutions, when routine jobs were displaced,

47:05.040 --> 47:11.760
new routine jobs came up. So there was always room for that. But with AI and automation,

47:11.760 --> 47:16.480
the whole point is replacing all routine jobs eventually. So there will be fewer and fewer

47:16.480 --> 47:23.840
routine jobs. And AI will create jobs, but it won't create routine jobs. Because if it creates

47:23.840 --> 47:30.320
routine jobs, why wouldn't AI just do it? So therefore, the people who are losing the jobs

47:30.320 --> 47:36.720
are losing routine jobs, the jobs that are becoming available are non-routine jobs. So the social

47:36.720 --> 47:43.280
stipend needs to be put in place is for the routine workers who lost their jobs to be retrained,

47:43.280 --> 47:48.960
maybe in six months, maybe in three years, takes a while to retrain on the non-routine job, and then

47:48.960 --> 47:55.680
take on a job that will last for that person's lifetime. Now, having said that, if you look

47:55.680 --> 48:01.840
deeply into Andrew's document, he does cater for that. So I'm not disagreeing with what he's trying

48:01.840 --> 48:08.640
to do. But for simplification, sometimes he just says UBI, but simple UBI wouldn't work.

48:08.640 --> 48:15.680
And I think you've mentioned elsewhere that the goal isn't necessarily to give people enough money

48:15.760 --> 48:22.080
to survive or live or even to prosper. The point is to give them a job that gives them meaning.

48:22.640 --> 48:28.480
That meaning is extremely important. That our employment, at least in the United States,

48:28.480 --> 48:34.480
and perhaps it cares across the world, provides something that's, forgive me for saying,

48:34.480 --> 48:44.640
greater than money. It provides meaning. So now, what kind of jobs do you think can't be automated?

48:44.720 --> 48:49.600
You talk a little bit about creativity and compassion in your book. What aspects do you

48:49.600 --> 48:56.320
think it's difficult to automate for an AI system? Because an AI system is currently

48:56.320 --> 49:02.880
merely optimizing. It's not able to reason, plan, or think creatively or strategically.

49:03.520 --> 49:10.160
It's not able to deal with complex problems. It can't come up with a new problem and solve it.

49:10.240 --> 49:16.240
A human needs to find the problem and pose it as an optimization problem,

49:16.240 --> 49:24.560
then have the AI work at it. So an AI would have a very hard time discovering a new drug or

49:24.560 --> 49:32.880
discovering a new style of painting or dealing with complex tasks such as managing a company

49:32.880 --> 49:38.000
that isn't just about optimizing the bottom line, but also about employee satisfaction,

49:38.640 --> 49:45.360
corporate brand, and many, many other things. So that is one category of things. And because

49:45.360 --> 49:50.560
these things are challenging, creative, complex, doing them creates a higher,

49:50.560 --> 49:56.160
high degree of satisfaction, and therefore appealing to our desire for working, which

49:56.160 --> 50:01.120
isn't just to make the money, make the ends meet, but also that we've accomplished something that

50:01.120 --> 50:07.600
others maybe can't do or can't do as well. Another type of job that is much numerous

50:07.680 --> 50:14.960
would be compassionate jobs, jobs that require compassion, empathy, human touch, human trust.

50:14.960 --> 50:22.400
AI can't do that because AI is cold, calculating, and even if it can fake that to some extent,

50:23.200 --> 50:29.680
it will make errors and that will make it look very silly. And also, I think even if AI did okay,

50:29.680 --> 50:37.200
people would want to interact with another person, whether it's for some kind of a service or teacher

50:37.200 --> 50:45.520
or doctor or concierge or a masseuse or bartender. There are so many jobs where people just don't

50:45.520 --> 50:53.120
want to interact with a cold robot or software. I've had an entrepreneur who built an elderly care

50:53.120 --> 50:59.760
robot, and they found that the elderly really only use it for customer service. But not to

50:59.760 --> 51:05.360
service the product, but they click on customer service and the video of a person comes up,

51:05.440 --> 51:10.320
and then the person says, how come my daughter didn't call me? Let me show you a picture of

51:10.320 --> 51:17.520
her grandkids. So people earn for that, people people interaction. So even if robots improved,

51:17.520 --> 51:23.280
people just don't want it. And those jobs are going to be increasing because AI will create a lot

51:23.280 --> 51:30.960
of value, $16 trillion to the world in next 11 years, according to PWC. And that will give people

51:31.520 --> 51:39.520
money to enjoy services, whether it's eating a gourmet meal, or tourism and traveling,

51:39.520 --> 51:46.080
or having concierge services, the services revolving around every dollar of that $16

51:46.080 --> 51:52.640
trillion will be tremendous. It will create more opportunities to service the people who did well

51:52.720 --> 52:02.320
through AI with things. But even at the same time, the entire society is very much short in need of

52:02.320 --> 52:09.280
many service oriented, compassionate oriented jobs. The best example is probably in healthcare

52:09.280 --> 52:16.800
services. There's going to be 2 million new jobs, not counting replacement, just brand new incremental

52:16.800 --> 52:24.160
jobs in the next six years in healthcare services. That includes nurses, orderly in the hospital,

52:24.880 --> 52:33.600
elderly care, and also at home care. It's particularly lacking. And those jobs are not

52:33.600 --> 52:38.480
likely to be filled. So there's likely to be a shortage. And the reason they're not filled

52:38.480 --> 52:45.200
is simply because they don't pay very well. And that the social status of these jobs are

52:45.920 --> 52:52.240
not very good. So they pay about half as much as a heavy equipment operator,

52:52.240 --> 52:59.120
which will be replaced a lot sooner. And they pay probably comparably to someone on the assembly line.

52:59.840 --> 53:07.200
And if so, if we ignoring all the other issues and just think about satisfaction from one's job,

53:07.200 --> 53:13.120
someone repetitively doing the same manual action and assembly line, that can't create a lot of

53:13.120 --> 53:19.680
job satisfaction, but someone taking care of a sick person and getting a hug and thank you from

53:19.680 --> 53:27.200
that person and the family, I think is quite satisfying. So if only we could fix the pay

53:27.200 --> 53:33.360
for service jobs, there are plenty of jobs that require some training or a lot of training

53:33.360 --> 53:41.680
for the people coming off the routine jobs to take. We can easily imagine someone who was maybe a

53:41.680 --> 53:48.560
cashier at the grocery store, as stores become automated, learns to become a nurse or a at home

53:48.560 --> 53:54.720
care. Also do want to point out the blue collar jobs are going to stay around a bit longer,

53:54.720 --> 54:03.120
some of them quite a bit longer. AI cannot be told go clean an arbitrary home. That's incredibly

54:03.120 --> 54:10.000
hard, arguably is an L five level of difficulty, right? And then AI cannot be a good plumber,

54:10.000 --> 54:15.680
because plumber is almost like a mini detective that has to figure out where the leak came from.

54:15.680 --> 54:24.640
So yet AI probably can be an assembly line and auto mechanic and so on. So one has to study

54:24.640 --> 54:29.760
which blue collar jobs are going away and facilitate retraining for the people to go into

54:29.760 --> 54:34.560
the ones that won't go away or maybe even will increase. I mean, it is fascinating that it's

54:34.560 --> 54:41.680
easier to build a world champion chess player than it is to build a mediocre plumber.

54:43.040 --> 54:48.880
Very true. And that goes counterintuitive to a lot of people's understanding of what artificial

54:48.880 --> 54:54.640
intelligence is. So it sounds, I mean, you're painting a pretty optimistic picture about

54:54.640 --> 54:58.880
retraining about the number of jobs and actually the meaningful nature of those jobs

54:59.440 --> 55:05.200
once we automate repetitive tasks. So overall, are you optimistic about

55:06.800 --> 55:13.120
the future where much of the repetitive tasks are automated, that there is a lot of room for

55:13.120 --> 55:18.880
humans for the compassionate for the creative input that only humans can provide?

55:19.920 --> 55:27.520
I am optimistic if we start to take action. If we have no action in the next five years,

55:27.520 --> 55:33.040
I think it's going to be hard to deal with the devastating losses that will emerge.

55:34.400 --> 55:38.560
So if we start thinking about retraining, maybe with the low hanging fruits,

55:39.280 --> 55:45.360
explaining to vocational schools why they should train more plumbers than auto mechanics,

55:46.560 --> 55:51.440
maybe starting with some government subsidy for corporations to have more training

55:52.400 --> 55:58.960
positions. We start to explain to people why retraining is important. We start to think about

55:58.960 --> 56:05.520
what the future of education, how that needs to be tweaked for the era of AI. If we start to make

56:05.520 --> 56:10.560
incremental progress and a greater number of people understand, then there's no reason to think we

56:10.560 --> 56:17.200
can't deal with this because this technological revolution is arguably similar to what electricity,

56:17.200 --> 56:22.560
industrial revolutions and internet brought about. Do you think there's a role for policy,

56:22.560 --> 56:26.880
for governments to step in to help with policy to create a better world?

56:28.080 --> 56:33.280
Absolutely, and the governments don't have to believe an employment will go up,

56:33.920 --> 56:38.160
and they don't have to believe automation will be this fast to do something.

56:39.280 --> 56:45.520
Revamping vocational school would be one example. Another is if there's a big gap in healthcare

56:45.520 --> 56:52.720
service employment, and we know that a country's population is growing older, more longevity,

56:52.720 --> 56:59.040
living older, because people over 80 require five times as much care as those under 80,

56:59.600 --> 57:06.880
then it is a good time to incent training programs for elderly care to find ways to improve the pay.

57:07.440 --> 57:13.760
Maybe one way would be to offer as part of Medicare or the equivalent program for people

57:13.760 --> 57:22.000
over 80 to be entitled to a few hours of elderly care at home, and then that might be reimbursable,

57:22.000 --> 57:28.720
and that will stimulate the service industry around the policy.

57:28.720 --> 57:35.280
Do you have concerns about large entities, whether it's governments or companies,

57:35.280 --> 57:40.800
controlling the future of AI development in general? So we talked about companies,

57:40.800 --> 57:49.360
do you have a better sense that governments can better represent the interests of the people

57:49.360 --> 57:54.720
than companies, or do you believe companies are better at representing the interests of the people,

57:54.720 --> 57:56.560
or is there no easy answer?

57:56.560 --> 57:59.440
I don't think there's an easy answer, because it's a double-edged sword.

58:00.000 --> 58:05.440
The companies and governments can provide better services with more access to data and more access

58:05.440 --> 58:13.440
to AI, but that also leads to greater power, which can lead to uncontrollable problems,

58:13.440 --> 58:17.040
whether it's monopoly or corruption in the government.

58:17.680 --> 58:24.880
So I think one has to be careful to look at how much data that companies and governments have,

58:24.880 --> 58:29.120
and some kind of checks and balances would be helpful.

58:29.200 --> 58:35.680
So again, I come from Russia. There's something called the Cold War.

58:36.720 --> 58:40.640
So let me ask a difficult question here, looking at conflict.

58:40.640 --> 58:45.280
Steven Pinker wrote a great book that conflict all over the world is decreasing in general,

58:45.280 --> 58:51.760
but do you have a sense that having written the book AI Superpowers,

58:51.760 --> 58:57.760
do you see a major international conflict potentially arising between major nations,

58:57.760 --> 59:02.960
whatever they are, whether it's Russia, China, European nations, United States,

59:02.960 --> 59:10.080
or others in the next 10, 20, 50 years around AI, around the digital space, cyber space?

59:10.080 --> 59:18.320
Do you worry about that? Is that something we need to think about and try to alleviate or prevent?

59:19.520 --> 59:26.000
I believe in greater engagement. A lot of the worries about more powerful AI

59:26.720 --> 59:40.320
are based on a arms race metaphor. And when you extrapolate into military kinds of scenarios,

59:41.440 --> 59:48.080
AI can automate and autonomous weapons that needs to be controlled somehow,

59:48.800 --> 59:56.880
and autonomous decision making can lead to not enough time to fix international crises.

59:57.600 --> 01:00:04.000
So I actually believe a Cold War mentality would be very dangerous, because should

01:00:04.000 --> 01:00:09.920
two countries rely on AI to make certain decisions and they don't talk to each other,

01:00:09.920 --> 01:00:14.000
they do their own scenario planning, then something could easily go wrong.

01:00:14.960 --> 01:00:23.280
I think engagement, interaction, some protocols to avoid inadvertent disasters

01:00:23.280 --> 01:00:29.040
is actually needed. So it's natural for each country to want to be the best,

01:00:29.040 --> 01:00:37.840
whether it's in nuclear technologies or AI or bio. But I think it's important to realize if

01:00:38.640 --> 01:00:44.880
each country has a black box AI and don't talk to each other, that probably presents greater

01:00:46.400 --> 01:00:53.600
challenges to humanity than if they interacted. I think there can still be competition,

01:00:53.600 --> 01:01:01.120
but with some degree of protocol for interaction. Just like when there was a nuclear competition,

01:01:02.080 --> 01:01:08.400
there were some protocol for deterrence among US, Russia and China. And I think

01:01:08.960 --> 01:01:15.280
that engagement is needed. So of course, we're still far from AI presenting that kind of danger.

01:01:15.920 --> 01:01:22.880
But what I worry the most about is the level of engagement seems to be coming down.

01:01:22.880 --> 01:01:29.680
The level of distrust seems to be going up, especially from the US towards other large

01:01:29.680 --> 01:01:35.360
countries such as China and Russia. Is there a way to make that better? So that's beautifully

01:01:35.360 --> 01:01:46.960
put, level of engagement and even just basic trust and communication as opposed to making

01:01:46.960 --> 01:01:56.320
artificial enemies out of particular countries. Do you have a sense how we can make it better?

01:01:57.120 --> 01:02:04.320
actionable items that as a society we can take on? I'm not an expert at geopolitics,

01:02:04.880 --> 01:02:13.120
but I would say that we look pretty foolish as humankind when we are faced with the opportunity

01:02:13.120 --> 01:02:26.160
to create 16 trillion dollars for humanity. And yet we're not solving fundamental problems with

01:02:26.640 --> 01:02:33.040
parts of the world still in poverty. And for the first time, we have the resources to overcome

01:02:33.040 --> 01:02:38.640
poverty and hunger. We're not using it on that, but we're fueling competition among superpowers.

01:02:38.640 --> 01:02:45.520
And that's a very unfortunate thing. If we become utopian for a moment, imagine a

01:02:45.840 --> 01:02:56.400
benevolent world government that has this 16 trillion dollars and maybe some AI to figure

01:02:56.400 --> 01:03:02.480
out how to use it to deal with diseases and problems and hate and things like that,

01:03:02.480 --> 01:03:08.640
world would be a lot better off. So what is wrong with the current world? I think the people

01:03:08.720 --> 01:03:15.680
with more skill than I should think about this. And then the geopolitics issue with

01:03:15.680 --> 01:03:21.520
superpower competition is one side of the issue. There's another side which I worry maybe even

01:03:23.200 --> 01:03:30.560
more, which is as the 16 trillion dollars all gets made by US and China and a few of the other

01:03:30.560 --> 01:03:35.840
developed countries, the poorer country will get nothing because they don't have technology.

01:03:36.800 --> 01:03:44.480
And the wealth disparity and inequality will increase. So a poorer country with a large

01:03:44.480 --> 01:03:51.280
population will not only benefit from the AI boom or other technology booms, but they will have

01:03:51.280 --> 01:03:57.440
their workers who previously had hoped they could do the China model and do outsource manufacturing

01:03:57.440 --> 01:04:03.360
or the India model. So they could do the outsource process or call center while all those jobs are

01:04:03.360 --> 01:04:11.920
going to be gone in 10 or 15 years. So the individual citizen may be a net liability,

01:04:11.920 --> 01:04:19.120
I mean financially speaking to a poorer country and not an asset to claw itself out of poverty.

01:04:19.760 --> 01:04:27.280
So in that kind of situation, these large countries with not much tech are going to be

01:04:27.280 --> 01:04:34.960
facing a downward spiral and it's unclear what could be done. And then when we look back and say

01:04:34.960 --> 01:04:40.560
there's 16 trillion dollars being created and it's all being kept by US, China and other developed

01:04:40.560 --> 01:04:48.000
countries, it just doesn't feel right. So I hope people who know about geopolitics can find solutions

01:04:48.000 --> 01:04:54.240
that's beyond my expertise. So different countries that we've talked about have different value

01:04:54.240 --> 01:05:01.280
systems. If you look at the United States to an almost extreme degree, there is an absolute

01:05:01.280 --> 01:05:06.000
desire for freedom of speech. If you look at a country where I was raised, that desire just

01:05:06.000 --> 01:05:14.880
amongst the people is not as elevated as it is to basically fundamental level

01:05:14.880 --> 01:05:19.040
to the essence of what it means to be America. And the same is true with China,

01:05:19.040 --> 01:05:28.400
there's different value systems. There is some censorship of internet content that China and

01:05:28.400 --> 01:05:36.000
Russia and many other countries undertake. Do you see that having effects on innovation,

01:05:36.640 --> 01:05:41.520
other aspects of some of the tech stuff AI development we talked about, and maybe from

01:05:41.520 --> 01:05:47.840
another angle, do you see that changing in different ways over the next 10 years, 20 years,

01:05:47.840 --> 01:05:54.480
50 years as China continues to grow as it does now in its tech innovation?

01:05:55.600 --> 01:06:02.880
There's a common belief that full freedom of speech and expression is correlated with creativity,

01:06:02.880 --> 01:06:11.760
which is correlated with entrepreneurial success. I think empirically, we have seen that is not

01:06:11.760 --> 01:06:19.680
true. And China has been successful. That's not to say the fundamental values are not right or

01:06:19.680 --> 01:06:27.840
not the best, but it's just that that perfect correlation isn't there. It's hard to read the

01:06:27.840 --> 01:06:34.880
tea leaves on an opening up or not in any country. And I've not been very good at that in my past

01:06:34.880 --> 01:06:44.880
predictions. But I do believe every country shares some fundamental value, a lot of fundamental

01:06:44.880 --> 01:06:57.200
values for the long term. So you know, China is drafting its privacy policy for individual citizens.

01:06:57.200 --> 01:07:05.200
And they don't look that different from the American or European ones. So people do want

01:07:05.200 --> 01:07:13.360
to protect their privacy and have the opportunity to express. And I think the fundamental values

01:07:13.360 --> 01:07:21.040
are there. The question is in the execution and timing, how soon or when will that start to open

01:07:21.040 --> 01:07:27.920
up? So as long as each government knows, ultimately, people want that kind of protection,

01:07:29.120 --> 01:07:34.960
there should be a plan to move towards that as to when or how, and I'm not an expert.

01:07:36.160 --> 01:07:42.480
On the point of privacy to me, it's really interesting. So AI needs data to create a

01:07:42.480 --> 01:07:47.360
personalized, awesome experience. I'm just speaking generally in terms of products.

01:07:48.080 --> 01:07:53.040
And then we have currently, depending on the age and depending on the demographics of who we're

01:07:53.040 --> 01:07:58.960
talking about, some people are more or less concerned about the amount of data they hand over.

01:07:58.960 --> 01:08:07.280
So in your view, how do we get this balance right? That we provide an amazing experience to

01:08:07.920 --> 01:08:13.440
people that use products? You look at Facebook, you know, the more Facebook knows about you,

01:08:13.440 --> 01:08:21.040
yes, it's scary to say, the better it can probably, a better experience it can probably create.

01:08:21.040 --> 01:08:23.280
So in your view, how do we get that balance right?

01:08:25.120 --> 01:08:34.320
Yes, I think a lot of people have a misunderstanding that it's okay and possible to just rip all the

01:08:34.320 --> 01:08:41.120
data out from a provider and give it back to you. So you can deny them access to further data

01:08:41.120 --> 01:08:47.440
and still enjoy the services we have. If we take back all the data, all the services will give us

01:08:47.440 --> 01:08:53.520
nonsense. We'll no longer be able to use products that function well in terms of, you know,

01:08:54.480 --> 01:09:00.880
right ranking, right products, right user experience. So yet I do understand we don't

01:09:00.880 --> 01:09:10.240
want to permit misuse of the data. From legal policy standpoint, I think there can be severe

01:09:10.240 --> 01:09:19.680
punishment for those who have egregious misuse of the data. That's, I think, a good first step,

01:09:19.680 --> 01:09:25.920
actually China in this side, on this aspect has very strong laws about people who sell or give

01:09:25.920 --> 01:09:33.600
data to other companies. And that over the past few years, since that law came into effect,

01:09:33.600 --> 01:09:43.760
pretty much eradicated the illegal distribution sharing of data. Additionally, I think giving,

01:09:45.440 --> 01:09:54.640
I think technology is often a very good way to solve technology misuse. So can we come up with

01:09:54.640 --> 01:10:00.080
new technologies that will let us have our cake and eat it too? People are looking into

01:10:00.080 --> 01:10:06.240
homomorphic encryption, which is letting you keep the data, have it encrypted and train on

01:10:06.240 --> 01:10:11.760
encrypted data. Of course, we haven't solved that one yet, but that kind of direction may be worth

01:10:11.760 --> 01:10:18.720
pursuing. Also federated learning, which would allow one hospital to train on its hospitals,

01:10:18.720 --> 01:10:24.080
patient data fully, because they have a license for that. And then hospitals will then share

01:10:24.080 --> 01:10:30.640
their models, not data but models, to create a supra AI. And that also maybe has some promise.

01:10:30.640 --> 01:10:38.720
So I would want to encourage us to be open-minded and think of this as not just the policy binary,

01:10:38.720 --> 01:10:44.880
yes, no, but letting the technologists try to find solutions to let us have our cake and eat it too,

01:10:44.880 --> 01:10:51.680
or have most of our cake and eat most of it too. Finally, I think giving each end user a choice

01:10:51.760 --> 01:10:57.680
is important. And having transparency is important. Also, I think that's universal. But

01:10:58.480 --> 01:11:03.840
the choice you give to the user should not be at a granular level that the user cannot understand.

01:11:04.720 --> 01:11:11.360
GDPR today causes all these pop-ups of yes, no, will you give this site this right to use this part

01:11:11.360 --> 01:11:17.840
of your data? I don't think any user understands what they're saying yes or no to. And I suspect

01:11:17.840 --> 01:11:24.400
most are just saying yes because they don't understand it. So while GDPR in its current

01:11:24.400 --> 01:11:31.440
implementation has lived up to its promise of transparency and user choice, it implemented

01:11:31.440 --> 01:11:40.640
it in such a way that really didn't deliver the spirit of GDPR. They fit the letter but not the

01:11:40.640 --> 01:11:48.640
spirit. So again, I think we need to think about is there a way to fit the spirit of GDPR by using

01:11:48.640 --> 01:11:56.080
some kind of technology? Can we have a slider that's an AI trying to figure out how much you want to

01:11:56.080 --> 01:12:04.160
slide between perfect protection security of your personal data versus a high degree of convenience

01:12:04.160 --> 01:12:10.640
with some risks of not having full privacy? Each user should have some preference and that gives

01:12:10.640 --> 01:12:16.560
you the user choice. But maybe we should turn the problem on its head and ask, can there be an AI

01:12:16.560 --> 01:12:22.080
algorithm that can customize this? Because we can understand the slider but we sure cannot

01:12:22.080 --> 01:12:29.040
understand every pop-up question. And I think getting that right requires getting the balance

01:12:29.120 --> 01:12:36.080
between what we talked about earlier, which is hard and soul versus profit driven decisions and

01:12:36.080 --> 01:12:43.680
strategy. I think from my perspective, the best way to make a lot of money in the long term is to

01:12:43.680 --> 01:12:51.840
keep your heart and soul intact. I think getting that slider right in the short term may feel like

01:12:51.840 --> 01:12:58.560
you'll be sacrificing profit. But in the long term, you'll be getting user trust and providing a

01:12:58.560 --> 01:13:05.680
great experience. Do you share that kind of view in general? Yes, absolutely. I sure would hope

01:13:05.680 --> 01:13:12.960
there is a way we can do long term projects that really do the right thing. I think a lot of people

01:13:12.960 --> 01:13:19.040
who embrace GDPR, their heart's in the right place. I think they just need to figure out

01:13:19.040 --> 01:13:24.640
how to build a solution. I've heard utopians talk about solutions that get me excited but

01:13:24.640 --> 01:13:31.200
I'm not sure how in the current funding environment they can get started. People talk about, imagine

01:13:31.200 --> 01:13:41.200
this crowdsourced data collection that we all trust and then we have these agents that we

01:13:41.920 --> 01:13:50.720
ask them to ask the trusted agent, that agent only, that platform. So a trusted joint platform

01:13:51.200 --> 01:14:03.040
that we all believe is trustworthy that can give us all the close loop personal suggestions

01:14:03.040 --> 01:14:08.960
by the new social network, new search engine, new e-commerce engine that has access to even

01:14:08.960 --> 01:14:16.720
more of our data but not directly but indirectly. So I think that general concept of licensing

01:14:16.720 --> 01:14:22.160
to some trusted engine and finding a way to trust that engine seems like a great idea

01:14:22.160 --> 01:14:27.600
but if you think how long it's going to take to implement and tweak and develop it right

01:14:27.600 --> 01:14:32.560
as well as to collect all the trusts and the data from the people, it's beyond the

01:14:32.560 --> 01:14:37.200
current cycle of venture capital. So how do you do that is a big question.

01:14:38.000 --> 01:14:42.960
You've recently had a fight with cancer, stage 4 lymphoma

01:14:44.640 --> 01:14:51.760
and in a sort of deep personal level, what did it feel like in the darker moments to face your

01:14:51.760 --> 01:15:01.360
own mortality? Well I've been the workaholic my whole life and I've basically worked 9.96,

01:15:01.360 --> 01:15:08.800
9am to 9pm, 6 days a week roughly and I didn't really pay a lot of attention to my family,

01:15:08.800 --> 01:15:14.960
friends and people who loved me and my life revolved around optimizing for work. While my

01:15:14.960 --> 01:15:24.560
work was not routine, my optimization really what made my life basically a very mechanical

01:15:24.560 --> 01:15:31.840
process but I got a lot of highs out of it because of accomplishments that I thought were

01:15:32.800 --> 01:15:39.440
really important and dear and the highest priority to me but when I faced mortality and the possible

01:15:39.440 --> 01:15:46.000
death in matter of months, I suddenly realized that this really meant nothing to me, that I

01:15:46.000 --> 01:15:51.680
didn't feel like working for another minute, that if I had six months left in my life I would spend

01:15:51.920 --> 01:15:59.920
all with my loved ones and thanking them, giving them love back and apologizing to them that I

01:15:59.920 --> 01:16:09.520
lived my life the wrong way. So that moment of reckoning caused me to really rethink that why

01:16:09.520 --> 01:16:18.800
we exist in this world is something that we might be too much shaped by the society to think that

01:16:18.800 --> 01:16:28.160
success and accomplishments is why we live but while that can get you periodic successes and

01:16:28.160 --> 01:16:37.440
satisfaction, it's really in them facing death you see what's truly important to you. So as a result

01:16:37.440 --> 01:16:44.480
of going through them, the challenges with cancer, I've resolved to live a more balanced

01:16:44.480 --> 01:16:52.880
lifestyle. I'm now in remission, knock on wood and I'm spending more time with my family, my wife

01:16:52.880 --> 01:17:01.120
travels with me, when my kids need me, I spend more time with them and before I used to prioritize

01:17:01.120 --> 01:17:06.400
everything around work when I had a little bit of time I would dole it out to my family. Now

01:17:07.040 --> 01:17:11.680
when my family needs something, really needs something, I drop everything at work and go to

01:17:11.680 --> 01:17:18.880
them and then in the time remaining, I allocate to work. But one's family is very understanding,

01:17:18.880 --> 01:17:27.040
it's not like they will take 50 hours a week from me. So I'm actually able to still work pretty hard,

01:17:27.040 --> 01:17:34.560
maybe 10 hours less per week. So I realize the most important thing in my life is really love

01:17:34.560 --> 01:17:41.280
and the people I love and I give that the highest priority. It isn't the only thing I do but when

01:17:41.280 --> 01:17:49.360
that is needed, I put that at the top priority and I feel much better and I feel much more balanced.

01:17:50.000 --> 01:17:58.160
And I think this also gives a hint as to a life of routine work, a life of pursuit of numbers.

01:17:58.720 --> 01:18:05.520
While my job was not routine, it wasn't pursuit of numbers, pursuit of can I make more money,

01:18:05.520 --> 01:18:12.320
can I fund more great companies, can I raise more money, can I make sure our VCs ranked higher and

01:18:12.320 --> 01:18:20.240
higher every year. This competitive nature of driving for bigger numbers and better numbers

01:18:20.800 --> 01:18:31.760
became a endless pursuit of that's mechanical. And bigger numbers really didn't make me happier

01:18:31.760 --> 01:18:35.600
and faced with death, I realized bigger numbers really meant nothing.

01:18:36.320 --> 01:18:43.680
And what was important is that people who have given their heart and their love to me deserve

01:18:43.680 --> 01:18:50.960
for me to do the same. So there's deep profound truth in that, that everyone should hear and

01:18:50.960 --> 01:18:59.360
internalize and that's really powerful for you to say that. I have to ask sort of a difficult

01:19:00.320 --> 01:19:08.800
question here. So I've competed in sports my whole life, looking historically. I'd like to

01:19:09.600 --> 01:19:17.440
challenge some aspect of that a little bit on the point of hard work, that it feels that there are

01:19:17.440 --> 01:19:24.560
certain aspects that is the greatest, the most beautiful aspects of human nature is the ability

01:19:24.560 --> 01:19:32.880
to become obsessed of becoming extremely passionate to the point where yes, flaws are revealed

01:19:33.600 --> 01:19:40.960
and just giving yourself fully to a task. That is, in another sense, you mentioned love being

01:19:40.960 --> 01:19:46.800
important, but in another sense, this kind of obsession, this pure exhibition of passion and

01:19:46.800 --> 01:19:53.120
hard work is truly what it means to be human. What lessons should we take this deeper because

01:19:53.120 --> 01:19:59.440
you've accomplished incredible things. You say it chasing numbers, but really, there's some incredible

01:19:59.440 --> 01:20:08.480
work there. So how do you think about that? When you look back in your 20s, your 30s, what would

01:20:08.480 --> 01:20:14.480
you do differently? Would you really take back some of the incredible hard work?

01:20:15.280 --> 01:20:24.800
I would, but it's in percentages. We're both computer scientists. So I think when one balances

01:20:24.800 --> 01:20:31.920
one's life, when one is younger, you might give a smaller percentage to family, but you would still

01:20:31.920 --> 01:20:36.800
give them high priority. And when you get older, you would give a larger percentage to them and

01:20:36.800 --> 01:20:42.800
still the high priority. And when you're near retirement, you give most of it to them and the

01:20:42.800 --> 01:20:50.000
highest priority. So I think the key point is not that we would work 20 hours less for the whole

01:20:50.000 --> 01:20:58.800
life and just spend it aimlessly with the family, but that when the family has a need, when your

01:20:58.800 --> 01:21:05.520
wife is having a baby, when your daughter has a birthday, or when they're depressed, or when

01:21:05.520 --> 01:21:10.720
they're celebrating something, or when they have a get together, or when we have family time,

01:21:11.360 --> 01:21:20.320
that it's important for us to put down our phone and PC and be 100% with them. And that priority

01:21:21.200 --> 01:21:29.440
on the things that really matter isn't going to be so taxing that it would eliminate or even

01:21:29.440 --> 01:21:35.360
dramatically reduce our accomplishments. It might have some impact, but it might also have other

01:21:35.360 --> 01:21:40.480
impact because if you have a happier family, maybe you fight less. If you fight less, you don't

01:21:40.480 --> 01:21:47.760
spend time taking care of all the aftermath of a fight. So it's unclear that it would take more

01:21:47.760 --> 01:21:55.280
time. And if it did, I'd be willing to take that reduction. And it's not a dramatic number,

01:21:55.280 --> 01:22:01.440
but it's a number that I think would give me a greater degree of happiness and knowing that

01:22:01.520 --> 01:22:09.280
I've done the right thing and still have plenty of hours to get the success that I want to get.

01:22:09.920 --> 01:22:16.320
So given the many successful companies that you've launched and much success throughout your career,

01:22:17.680 --> 01:22:26.720
what advice would you give to young people today looking, or it doesn't have to be young,

01:22:26.720 --> 01:22:33.600
but people today looking to launch and to create the next $1 billion tech startup or even AI-based

01:22:33.600 --> 01:22:43.680
startup? I would suggest that people understand technology waves move quickly. What worked two

01:22:43.680 --> 01:22:50.960
years ago may not work today. And that is very much a case in point for AI. I think two years ago,

01:22:51.600 --> 01:22:56.720
or maybe three years ago, you certainly could say, I have a couple of super smart PhDs,

01:22:57.360 --> 01:23:03.120
and we're not sure what we're going to do, but here's how we're going to start and get funding

01:23:03.120 --> 01:23:10.400
for a very high valuation. Those days are over because AI is going from rocket science towards

01:23:10.400 --> 01:23:19.680
mainstream, not yet commodity, but more mainstream. So first, the creation of any company to

01:23:19.680 --> 01:23:28.240
venture capitalists has to be creation of business value and monetary value. And when you have a very

01:23:28.240 --> 01:23:37.200
scarce commodity, VCs may be willing to accept greater uncertainty. But now the number of people

01:23:37.200 --> 01:23:43.760
who have the equivalent of PhD three years ago, because that can be learned more quickly,

01:23:43.760 --> 01:23:50.320
platforms are emerging, the cost to become an AI engineer is much lower, and there are many more

01:23:50.320 --> 01:23:56.560
AI engineers. So the market is different. So I would suggest someone who wants to build an AI

01:23:56.560 --> 01:24:05.120
company be thinking about the normal business questions. What customer cases are you trying to

01:24:05.120 --> 01:24:10.800
address? What kind of pain are you trying to address? How does that translate to value?

01:24:10.800 --> 01:24:18.400
How will you extract value and get paid through what channel? And how much business value will

01:24:18.400 --> 01:24:26.640
get created? That today needs to be thought about much earlier up front than it did three years ago.

01:24:26.640 --> 01:24:33.440
The scarcity question of AI talent has changed. The number of AI talent has changed. So now you

01:24:33.440 --> 01:24:40.560
need not just AI, but also understanding of business customer and the marketplace.

01:24:41.920 --> 01:24:52.160
So I also think you should have a more reasonable valuation expectation and growth expectation.

01:24:52.160 --> 01:24:57.760
There's going to be more competition. But the good news, though, is that AI technologies

01:24:57.760 --> 01:25:06.080
are now more available in open source. TensorFlow, PyTorch and such tools are much easier to use.

01:25:06.640 --> 01:25:14.400
So you should be able to experiment and get results iteratively faster than before. So

01:25:15.200 --> 01:25:22.240
take more of a business mindset to this. Think less of this as a laboratory taken

01:25:22.240 --> 01:25:28.720
into a company because we've gone beyond that stage. The only exception is if you truly have a

01:25:28.720 --> 01:25:35.040
breakthrough in some technology that really no one has, then the old way still works. But I think

01:25:35.040 --> 01:25:41.920
that's harder and harder now. So I know you believe, as many do, that we're far from creating an

01:25:41.920 --> 01:25:50.720
artificial general intelligence system. But say once we do, and you get to ask her one question,

01:25:51.280 --> 01:25:52.720
what would that question be?

01:25:57.520 --> 01:25:59.840
What is it that differentiates you and me?

01:26:02.080 --> 01:26:05.600
Beautifully put. Kaifu, thank you so much for your time today.

01:26:05.600 --> 01:26:06.320
Thank you.

