start	end	text
0	4320	A lot of people have said for many years that there will come a time when we want to pause a little bit.
6240	7120	That time is now.
10800	15440	The following is a conversation with Max Tegmark, his third time in the podcast.
15440	20000	In fact, his first appearance was episode number one of this very podcast.
20560	26240	He is a physicist and artificial intelligence researcher at MIT, co-founder of FutureLeft
27200	32720	Institute and author of Life 3.0, being human in the age of artificial intelligence.
33440	39120	Most recently, he's a key figure in spearheading the open letter calling for a six-month pause
39120	44720	on giant AI experiments like training GPT-4. The letter reads,
45440	51200	We're calling for a pause on training of models larger than GPT-4 for six months.
51840	55840	This does not imply a pause or ban on all AI research and development,
55840	59120	or the use of systems that have already been placed on the market.
59760	66000	Our call is specific and addresses a very small pool of actors who possess this capability.
66800	73280	The letter has been signed by over 50,000 individuals, including 1,800 CEOs and over 1,500
73280	79760	professors. Signatories include Joshua Benio, Steele Russell, Elon Musk, Steve Wozniak,
79760	86320	Yuval Noah Harari, Andrew Yang, and many others. This is a defining moment in the history of human
86320	94240	civilization, where the balance of power between human and AI begins to shift. And Max's mind
94240	100480	and his voice is one of the most valuable and powerful in a time like this. His support,
100480	105760	his wisdom, his friendship has been a gift I'm forever deeply grateful for.
106640	111760	This is Alex Friedman podcast. To support it, please check out our sponsors in the description.
111760	119200	And now, dear friends, here's Max Tagmark. You were the first ever guest on this podcast,
119200	125120	episode number one. So first of all, Max, I just have to say thank you for giving me a chance.
125120	128960	Thank you for starting this journey. It's been an incredible journey. Just thank you for
129920	134960	sitting down with me and just acting like I'm somebody who matters, that I'm somebody who's
134960	139360	interesting to talk to. And thank you for doing it. That meant a lot.
140160	146400	Thanks to you for putting your heart and soul into this. I know when you delve into controversial
146400	152240	topics, it's inevitable to get hit by what Hamlet talks about, the slings and arrows and stuff.
152240	157920	And I really admire this. It's in an era where YouTube videos are too long and now it has to
157920	164640	be like a 20-second TikTok clip. It's just so refreshing to see you going exactly against
164640	169680	all of the advice and doing these really long-form things. And the people appreciate it.
169680	174800	Reality is nuanced. And thanks for sharing it that way.
175760	180320	So let me ask you again, the first question I've ever asked on this podcast, episode number one,
180880	185680	talking to you. Do you think there's intelligent life out there in the universe?
185680	192240	Let's revisit that question. Do you have any updates? What's your view when you look out to the stars?
192240	198880	So when we look out to the stars, if you define our universe the way most astrophysicists do,
198880	203760	not as all of space, but the spherical region of space that we can see with our telescopes,
203760	209520	from which light has the time to reach us since our big man. I'm in the minority. I
211360	216800	estimate that we are the only life in this spherical volume that has
216960	223360	invented internet, radios, gotten our level of tech. And if that's true,
224720	231280	then it puts a lot of responsibility on us to not mess this one up. Because if it's true,
231280	239360	it means that life is quite rare and we are stewards of this one spark of advanced consciousness,
239360	245520	which if we nurture it and help it grow, eventually life can spread from here out
245520	249680	into much of our universe. And we can have this just amazing future. Whereas if we instead
250640	255200	are reckless with the technology we build and just snuff it out due to the stupidity,
256160	262960	or infighting, then maybe the rest of cosmic history in our universe is just going to be
262960	270080	a plenty of empty benches. But I do think that we are actually very likely to get visited by aliens,
270960	275840	alien intelligence quite soon. But I think we are going to be building that alien intelligence.
276720	287040	So we're going to give birth to an intelligent alien civilization. Unlike anything human,
287040	292720	the evolution here on Earth was able to create in terms of the path, the biological path it took.
292720	300000	Yeah, and it's going to be much more alien than a cat or even the most exotic
300000	306160	animal on the planet right now. Because it will not have been created through the usual
306160	311840	Darwinian competition where it necessarily cares about self preservation, the afraid of death,
313120	320000	any of those things. The space of alien minds that you can build is just so much faster than
320080	326640	what evolution will give you. And with that also comes great responsibility for us to make
326640	332960	sure that the kind of minds we create are those kind of minds that it's good to create minds that
333600	341760	will share our values and be good for humanity and life, and also create minds that don't suffer.
342720	349360	Do you try to visualize the full space of alien minds that AI could be? Do you try to consider
349360	356320	all the different kinds of intelligences, sort of generalizing what humans are able to do to the
356320	360400	full spectrum of what intelligent creatures entities could do?
360400	367520	I try, but I would say I fail. I mean, it's very difficult for human minds
367600	374800	to really grapple with something so completely alien, even for us, right? If we just try to
374800	382000	imagine how would it feel if we were completely indifferent towards death or individuality?
384000	391200	Even if you just imagine that, for example, you could just copy my knowledge of how to speak
391520	399600	Swedish. Boom, now you can speak Swedish. And you could copy any of my cool experiences, and
399600	404800	then you could delete the ones you didn't like in your own life, just like that. It would already
404800	411120	change quite a lot about how you feel as a human being, right? You probably spend less effort studying
411120	416160	things if you just copy them. And you might be less afraid of death, because if the plane you're on
416880	423600	starts to crash, you'd just be like, oh, shucks, I haven't backed my brain up for four hours.
424560	429280	So I'm going to lose all these wonderful experiences out of this flight.
431040	438080	We might also start feeling more compassionate, maybe with other people, if we can so readily
438080	444160	share each other's experiences and knowledge and feel more like a hive mind. It's very hard,
444160	452160	though. I really feel very humble about this, to grapple with it, how it might actually feel.
452960	459040	The one thing which is so obvious, though, which I think is just really worth reflecting on is,
459040	463600	because the mind space of possible intelligence is so different from ours,
464240	468080	it's very dangerous if we assume they're going to be like us or anything like us.
468240	477360	Well, the entirety of human written history has been through poetry, through novels,
477360	483440	been trying to describe through philosophy, trying to describe the human condition and what's
483440	487600	entailed in it. Like Jessica said, fear of death and all those kinds of things, what is love,
487600	493760	and all of that changes if you have a different kind of intelligence, all of it. The entirety,
493760	498320	all those poems, they're trying to sneak up to what the hell it means to be human,
498320	506160	all of that changes. How AI concerns and existential crises that AI experiences,
506160	513360	how that clashes with the human existential crisis, the human condition. It's hard to fathom,
513360	520320	hard to predict. It's hard, but it's fascinating to think about also. Even in the best case scenario,
520320	526960	where we don't lose control over the ever more powerful AI that we're building to other humans
526960	532800	whose goals we think are horrible and where we don't lose control to the machines and AI
534400	539520	provides the things that we want, even then you get into the questions, do you touch here?
540400	544960	Maybe it's the struggle that it's actually hard to do things. It's part of the things that give
544960	553040	this meaning as well. For example, I found it so shocking that this new Microsoft GPT-4
553600	558800	commercial that they put together has this woman talking about showing this demo,
559520	565520	how she's going to give a graduation speech to her beloved daughter and she asks GPT-4 to write it.
566720	572320	If it's frigging 200 words or so, if I realized that my parents couldn't be bothered struggling
572320	578320	a little bit to write 200 words and outsource that to their computer, I would feel really offended,
578320	586000	actually. I wonder if eliminating too much of this struggle from our existence,
588480	597680	do you think that would also take away a little bit of what means to be human? We can't even predict.
598240	606080	Somebody mentioned to me that they started using Chat GPT with a 3.5 and not 4.0
607920	614880	to write what they really feel to a person and they have a temper issue and they're
614880	620720	basically trying to get Chat GPT to rewrite it in a nicer way, to get the point across,
620800	627120	but rewrite it in a nicer way. We're even removing the inner asshole from our communication.
628720	634400	There's some positive aspects of that, but mostly it's just the transformation of how
634400	643280	humans communicate and it's scary because so much of our society is based on this glue of
643280	650640	communication. We're now using AI as the medium of communication that does the language for us.
651360	657120	So much of the emotion that's laden in human communication, so much of the intent
658240	662560	that's going to be handled by outsourced AI. How does that change everything? How does that
662560	667440	change the internal state of how we feel about other human beings? What makes us lonely? What
667440	672000	makes us excited? What makes us afraid? How we fall in love? All that kind of stuff.
673200	677600	For me personally, I have to confess the challenge is one of the things that really makes
677680	686880	my life feel meaningful. If I go hike a mountain with my wife, Maya, I don't want to just press
686880	690960	a button and be at the top. I want to struggle and come up there sweaty and feel like, wow,
690960	699680	we did this in the same way. I want to constantly work on myself to become a better person. If I
699680	707120	say something in anger that I regret, I want to go back and really work on myself rather than just
707680	712640	tell an AI from now on, always filter what I write so I don't have to work on myself,
713440	720240	because then I'm not growing. Yeah, but then again, it could be like with chess. An AI
721760	727680	wants it significantly, obviously supersedes the performance of humans. It will live in its own
727680	734160	world and provide maybe a flourishing civilizations for humans, but we humans will continue hiking
734240	739360	mountains and playing our games, even though AI is so much smarter, so much stronger, so much
739360	746560	superior in every single way, just like with chess. That's one possible hopeful trajectory here,
746560	763760	is that humans will continue to human and AI will just be a medium that enables the human
763760	771840	experience to flourish. Yeah, I would phrase that as rebranding ourselves from Homo sapiens
771840	779120	to Homo sentiens. Right now, sapiens, the ability to be intelligent, we've even put it in our
779120	786080	species name. We're branding ourselves as the smartest information processing
787040	792560	entity on the planet. That's clearly going to change if AI continues ahead.
794000	797840	So maybe we should focus on the experience instead, the subjective experience that we have
799200	805040	with Homo sentiens, and that's what's really valuable, the love, the connection, the other things.
807920	814800	Get off our high horses and get rid of this hubris that only we can do integrals.
815760	821920	So consciousness, the subjective experience is a fundamental value to what it means to be human.
821920	828720	Make that the priority. That feels like a hopeful direction to me, but that also requires more
829680	834960	compassion, not just towards other humans, because they happen to be the smartest
834960	839200	on the planet, but also towards all our other fellow creatures on this planet. I personally
839200	844240	feel right now, we're treating a lot of farm animals horribly, for example, and the excuse we're
844240	850240	using is, oh, they're not as smart as us. But if we admit that we're not that smart in the grand
850240	857200	scheme of things either in the post AI epoch, then surely we should value the subjective
857200	864640	experience of a cow also. Well, allow me to briefly look at the book, which at this point
864640	869600	is becoming more and more visionary that you've written, I guess, over five years ago, Life 3.0.
870560	878240	So first of all, 3.0. What's 1.0? What's 2.0? What's 3.0? And how's that vision sort of evolve?
878880	885040	The vision in the book evolved to today. Life 1.0 is really dumb, like bacteria,
885040	888960	and that it can't actually learn anything at all during the lifetime. The learning just comes from
888960	899280	this genetic process from one generation to the next. Life 2.0 is us and other animals which
899280	910720	have brains, which can learn during their lifetime a great deal. And you were born without being able
910720	915760	to speak English, and at some point you decided, hey, I want to upgrade my software. Let's install
915760	925600	an English speaking module. So you did? And Life 3.0 does not exist yet, can replace not only its
925600	933040	software the way we can, but also its hardware. And that's where we're heading towards at high
933040	942480	speed. We're already maybe 2.1 because we can put in an artificial knee, a pacemaker, etc, etc.
942480	951200	And if Neuralink and other companies succeed, we'll be Life 2.2, etc. But the companies trying
951200	956000	to build AGI or trying to make is, of course, full 3.0. And you can put that intelligence
956000	962640	in something that also has no biological basis whatsoever.
962640	969040	So less constraints and more capabilities, just like the leap from 1.0 to 2.0. There is,
969040	974160	nevertheless, you speaking so harshly about bacteria, so disrespectfully about bacteria,
974240	983760	there is still the same kind of magic there that permeates Life 2.0 and 3.0. It seems like maybe
983760	989920	the thing that's truly powerful about life intelligence and consciousness was already
989920	997920	there in 1.0. Is it possible? I think we should be humble and not be so quick to
998720	1003920	make everything binary and say either it's there or it's not. Clearly, there's a great
1003920	1009360	spectrum. And there is even a controversy about whether some unicellular organisms,
1009360	1014640	like amoebas, can maybe learn a little bit, you know, after all. So apologies if I offended
1014640	1020000	any bacteria here. It wasn't my intent. It was more that I wanted to talk up how cool it is to
1020000	1024480	actually have a brain where you can learn dramatically within your lifetime.
1024560	1029200	Typical human. And the higher up you get from 1.0 to 2.0 to 3.0,
1029200	1033840	the more you become the captain of your own ship, the master of your own destiny,
1033840	1040160	and the less you become a slave to whatever evolution gave you. By upgrading your software,
1040160	1043600	we can be so different from previous generations and even from our parents,
1044560	1052080	much more so than even a bacterium, no offense to them. And if you can also swap out your hardware,
1052080	1055760	take any physical form you want, of course, really, the sky is the limit.
1056720	1064160	Yeah. So it accelerates the rate at which you can perform the computation that determines your
1064160	1070400	destiny. Yeah. And I think it's worth commenting a bit on what you means in this context. Also,
1070400	1075040	if you swap things out a lot, right? This is controversial, but my
1075520	1088720	current understanding is that life is best thought of not as a bag of meat or even a bag of
1090080	1097920	elementary particles, but rather as a system which can process information and retain its own
1097920	1104320	complexity, even though nature is always trying to mess it up. So it's all about information
1104400	1111280	processing. And that makes it a lot like something like a wave in the ocean, which is not its
1111280	1116400	water molecules, right? The water molecules bob up and down, but the wave moves forward. It's an
1116400	1122640	information pattern. In the same way, you, Lex, you're not the same atoms as during the first
1123440	1130640	time you did with me. You've swapped out most of them, but it's still you. And the information
1130640	1140240	pattern is still there. And if you could swap out your arms and whatever, you can still have
1140240	1144960	this kind of continuity. It becomes much more sophisticated sort of way before in time where
1144960	1152400	the information lives on. I lost both of my parents since our last podcast. And it actually
1152400	1159120	gives me a lot of solace that this way of thinking about them, they haven't entirely died because
1160640	1166800	a lot of mommy and daddy's, sorry, I'm getting a little emotional here, but a lot of their values
1167680	1174240	and ideas and even jokes and so on, they haven't gone away, right? Some of them live on, I can
1174240	1179200	carry on some of them. And we also live on a lot of other, and a lot of other people. So in this
1179200	1187920	sense, even with life 2.0, we can to some extent already transcend our physical bodies and our
1187920	1196480	death. And particularly if you can share your own information, your own ideas with many others,
1196480	1206160	like you do in your podcast, then that's the closest immortality we can get with our bio bodies.
1206800	1213600	You carry a little bit of them in you in some sense. Do you miss them? You miss your mom and
1213600	1219680	dad? Of course. Of course. What did you learn about life from them if it can take a bit of a
1219680	1231280	tangent? I know so many things. For starters, my fascination for math and the physical mysteries
1231280	1237280	of our universe, I think you got a lot of that from my dad. But I think my obsession for really big
1237280	1245520	questions and consciousness and so on, that actually came mostly from my mom. And when I got
1245520	1260960	from both of them, which is very core part of really who I am, I think is this feeling comfortable with
1261200	1266640	not buying into what everybody else is saying.
1269680	1270720	Doing what I think is right.
1274560	1281680	They both very much did their own thing and sometimes they got flack for it and they did it
1281680	1287440	anyway. That's why you've always been an inspiration to me, that you're at the top of your field and
1287440	1296880	you're still willing to tackle the big questions in your own way. You're one of the people that
1296880	1302720	represents MIT best to me. You've always been an inspiration to that. So it's good to hear that
1302720	1308480	you got that from your mom and dad. Yeah, you're too kind. But yeah, I mean, the good reason to do
1308480	1314240	science is because you're really curious, you want to figure out the truth. If you think
1315120	1319680	this is how it is and everyone else says no, no, that's bullshit. And it's that way, you know,
1321760	1328960	you stick with what you think is true. And even if everybody else keeps thinking it's
1328960	1337360	bullshit, there's a certain, I always root for the underdog when I watch movies. And my dad once,
1337360	1342560	I, one time, for example, when I wrote one of my craziest papers ever, talking about our
1342560	1346320	universe ultimately being mathematical, which we're not going to get into today. I got this email
1346320	1350960	from a quite famous professor saying this is not only bullshit, but it's going to ruin your career.
1350960	1355360	You should stop doing this kind of stuff. I sent it to my dad. Do you know what he said?
1356640	1363520	He replied with a quote from Dante. Segui il tuo corso e la sedire la gente. Follow your own path
1364160	1372320	and let the people talk. Go dad. This is the kind of thing. He's dead, but that attitude
1372320	1380400	is not. How did losing them as a man, as a human being change you? How did it expand your thinking
1380400	1385600	about the world? How did it expand your thinking about, you know, this thing we're talking about,
1385600	1394240	which is humans creating another living sentient perhaps being? I think it
1394400	1404480	mainly did two things. One of them just going through all their stuff after they had passed
1404480	1408160	away and so on, just drove home to me. How important it is to ask ourselves,
1409920	1414320	why are we doing this things we do? Because it's inevitable that you look at some things they spent
1414320	1419280	an enormous time on. And you ask the, at hindsight, would they really have spent so much time on this?
1420240	1425680	Would they have done something that was more meaningful? So I've been looking more in my life
1425680	1435040	now and asking, you know, why am I doing what I'm doing? And I feel it should either be something I
1435040	1440800	really enjoy doing or it should be something that I find really, really meaningful because it helps
1441760	1451040	humanity. And if it's in none of those two categories, maybe I should spend less time on it,
1451040	1458000	you know. The other thing is dealing with death up in personal like this, it's actually made me
1458000	1465760	less afraid of even less afraid of other people telling me that I'm an idiot, you know,
1465760	1474560	which happens regularly and just let my life do my thing, you know. And
1476480	1480480	it made it a little bit easier for me to focus on what I feel is really important.
1480480	1485440	What about fear of your own death? Has it made it more real that this is
1487600	1493280	something that happens? Yeah, it's made it extremely real and I'm next in line in our
1493280	1501280	family now, right? Me and my younger brother. But they both handled it with such dignity.
1502160	1507760	That was a true inspiration also. They never complained about things and, you know, when
1507760	1511600	you're old and your body starts falling apart, it's more and more to complain about. They
1511600	1516640	looked at what could they still do that was meaningful. And they focused on that rather than
1516640	1523520	wasting time talking about or even thinking much about things they were disappointed in.
1524320	1529120	I think anyone can make themselves depressed if they start their morning by making a list of
1529120	1535760	grievances. Whereas if you start your day with a little meditation and just things you're grateful
1535760	1541840	for, you basically choose to be a happy person. Because you only have a finite number of days
1542480	1545840	to spend them being grateful. Yeah.
1548400	1555600	Well, you do happen to be working on a thing which seems to have potentially
1556400	1560640	some of the greatest impact on human civilization of anything humans have ever created,
1560640	1565520	which is artificial intelligence. This is on the both detailed technical level and in a
1565520	1572480	high philosophical level you work on. So you've mentioned to me that there's an open letter
1573520	1581120	that you're working on. It's actually going live in a few hours. So I've been having late
1581120	1588080	nights and early mornings. It's been very exciting, actually. In short, have you seen
1588080	1594880	Don't Look Up? the film? Yes, yes. I don't want to be the movie spoiler for anyone watching this
1594880	1601360	who hasn't seen it. But if you're watching this, you haven't seen it, watch it. Because we are
1601360	1608160	actually acting out. It's life-imitating art. Humanity is doing exactly that right now, except
1609120	1615200	it's an asteroid that we are building ourselves. Almost nobody is talking about it. People are
1615200	1619200	squabbling across the planet about all sorts of things which seem very minor compared to the
1619200	1624240	asteroid that's about to hit us, right? Most politicians don't even have the radar
1624240	1631280	this on the radar. They think maybe in 100 years or whatever. Right now we're at a fork on the road.
1631280	1637680	This is the most important fork humanity has reached in its over 100,000 years on this planet.
1637680	1644640	We're building effectively a new species that's smarter than us. It doesn't look so much like a
1644640	1650480	species yet because it's mostly not embodied in robots. But that's the technicality which will
1650480	1658400	soon be changed. And this arrival of artificial general intelligence that can do all our jobs
1658400	1665440	as well as us and probably shortly thereafter superintelligence which greatly exceeds our
1665440	1669840	cognitive abilities, it's going to either be the best thing ever to happen to humanity or the worst.
1669840	1675040	I'm really quite confident that there is not that much middle ground there.
1675120	1678640	But it would be fundamentally transformative to human civilization?
1678640	1685200	Of course. Utterly and totally. Again, we branded ourselves as homo sapiens because it seemed like
1685200	1690400	the basic thing. We're the king of the castle on this planet. We're the smart ones. If we can
1690400	1696880	control everything else, this could very easily change. We're certainly not going to be the smartest
1698240	1703920	on the planet very long unless AI progress just halts. And we can talk more about why I
1704640	1709440	think that's true because it's controversial. And then we can also talk about
1712480	1716560	reasons you might think it's going to be the best thing ever. And the reason you think it's going
1716560	1723520	to be the end of humanity, which is of course super controversial. But what I think we can,
1724400	1731920	anyone who's working on advanced AI can agree on is it's much like the film
1731920	1738800	don't look up and that it's just really comical how little serious public debate there is
1738800	1747600	about it given how huge it is. So what we're talking about is a development of currently
1747600	1757440	things like GPT-4 and the signs it's showing of rapid improvement that may in the near-term
1757440	1764320	lead to development of super intelligent AGI AI general AI systems and what kind of impact that
1764320	1771520	has on society. When that thing achieves general human level intelligence and then beyond that
1772400	1779760	general super human level intelligence. There's a lot of questions to explore here. So one you
1779760	1787280	mentioned halt. Is that the content of the letter? Is it to suggest that maybe we should
1787280	1791600	pause the development of these systems? Exactly. So this is very controversial
1792800	1798080	from when we talked the first time we talked about how I was involved in starting the Future
1798080	1803760	Life Institute and we worked very hard on 2014-2015 was the mainstream AI safety.
1804320	1808560	The idea that there even could be risks and that you could do things about them.
1809600	1813680	Before then a lot of people thought it was just really kooky to even talk about it and a lot of
1813680	1820160	AI researchers felt worried that this was too flaky and it could be bad for funding and that
1820160	1826400	the people who talked about it were just not didn't understand AI. I'm very very happy with
1827840	1833200	how that's gone and that now you know just completely mainstream you go on any AI conference
1833200	1837840	and people talk about AI safety and it's a nerdy technical field full of equations and
1841520	1848080	as it should be. But there's this other thing which has been quite taboo up until now
1849360	1855200	calling for slowdown. So what we've constantly been saying including myself I've been
1855280	1863040	biting my tongue a lot you know is that you know we don't need to slow down AI development we just
1863040	1870560	need to win this race the wisdom race between the growing power of the AI and the growing wisdom
1870560	1875360	with which we manage it and rather than trying to slow down AI let's just try to accelerate the
1875360	1881200	wisdom do all this technical work to figure out how you can actually ensure that your powerful AI
1881200	1888240	is going to do what you wanted to do and have society adapt also with incentives and regulations
1888240	1898720	so that these things get put to good use. Sadly that didn't pan out. The progress on technical AI
1898720	1907600	on capabilities has gone a lot faster than than many people thought back when we started this in
1907600	1916720	2014 turned out to be easier to build really advanced AI than we thought. And on the other side
1916720	1924240	it's gone much slower than we hoped with getting policy makers and others to actually
1925200	1931360	put incentives in place to steer this in the good direction. Maybe we should unpack it and
1931360	1935600	talk a little bit about each so why did it go faster than a lot of people thought.
1937600	1944240	In hindsight it's exactly like building flying machines. People spent a lot of time wondering
1944240	1949360	about how how do birds fly you know and that turned out to be really hard. Have you seen the
1949360	1954560	TED Talk with a flying bird? Like a flying robotic bird? Yeah flies around the audience
1955120	1959440	but it took a hundred years longer to figure out how to do that than for the Wright brothers to
1959440	1964240	build the first airplane because it turned out there was a much easier way to fly and evolution
1964240	1969520	picked a more complicated one because it had its hands tied. It could only build a machine that
1969520	1974960	could assemble itself which the Wright brothers didn't care about. They can only build a machine
1974960	1979680	that use only the most common atoms in the periodic table. Wright brothers didn't care about that.
1979680	1986880	They could use steel iron atoms and it had to be able to repair itself and it also had to be
1986880	1994000	incredibly fuel efficient. You know a lot of birds use less than half the fuel of a remote
1994000	1999440	control plane flying the same distance. For humans just throw a little more put a little more fuel
1999440	2004560	in a roof. There you go a hundred years earlier. That's exactly what's happening now with with
2004560	2010560	these large language models. The brain is incredibly complicated. Many people made the
2010560	2015440	mistake you're thinking we have to figure out how the brain does human level AI first before
2015440	2021200	we could build in a machine. That was completely wrong. You can take an incredibly simple
2021280	2027600	computational system called a transformer network and just train it to do something incredibly dumb.
2028560	2032080	Just read a gigantic amount of text and try to predict the next word.
2033200	2039360	And it turns out if you just throw a ton of compute at that and ton of data it gets to be
2040640	2044720	frighteningly good like GPT-4 which I've been playing with so much since it came out right.
2045680	2052800	And there's still some debate about whether that can get you all the way to full human level or not
2053520	2058080	but yeah we can come back to the details of that and how you might get the human level AI even if
2060160	2065200	large language models don't. Can you briefly if it's just a small tangent comment on your
2065200	2073680	feelings about GPT-4? Suggest that you're impressed by this rate of progress but where is it? Can GPT-4
2074640	2080800	reason? What are like the intuitions? What are human interpretable words you can assign to the
2080800	2087040	capabilities of GPT-4 that makes you so damn impressed with it? I'm both very excited about it
2087040	2093360	and terrified. It's an interesting mixture of emotions. All the best things in life include
2093360	2100560	those two somehow. Yeah I can absolutely reason. Anyone who hasn't played with it I highly recommend
2100560	2108320	doing that before dissing it. It can do quite remarkable reasoning and I've had to do a lot
2108320	2114640	of things which I realized I couldn't do that myself that well even and it obviously does it
2114640	2120400	dramatically faster than we do too when you watch a type and it's doing that well servicing a massive
2120400	2128640	number of other humans at the same time. At the same time it cannot reason as well as a human can
2129360	2136000	on some tasks. It's obviously a limitation from its architecture. We have in our heads what in
2136000	2140560	GeekSpeak is called a recurrent neural network. There are loops. Information can go from this
2140560	2144160	neuron to this neuron to this neuron and then then back to this one. You can like ruminate on
2144160	2151360	something for a while. You can self reflect a lot. These large language models they cannot
2151360	2156880	like GPT-4. It's a so-called transformer where it's just like a one-way street of information
2156880	2162000	basically and in GeekSpeak it's called a feed-forward neural network and it's only so deep
2163040	2170880	so it can only do logic that's that many steps and that deep. You can create problems which
2170880	2181680	will fail to solve for that reason. The fact that it can do so amazing things with this incredibly
2181680	2188480	simple architecture already is quite stunning and what we see in my lab at MIT when we look inside
2189600	2193120	large language models to try to figure out how they're doing it. That's the key core
2193120	2201440	focus of our research. It's called mechanistic interpretability in GeekSpeak. You have this
2201440	2205120	machine that does something smart. You try to reverse engineer and see how does it do it.
2206000	2211600	You think of it also as artificial neuroscience. That's exactly what neuroscientists do with
2211600	2215600	actual brains but here you have the advantage that you don't have to worry about measurement
2215600	2221520	errors. You can see what every neuron is doing all the time and a recurrent thing we see again
2221520	2228480	and again. There's been a number of beautiful papers quite recently by a lot of researchers.
2228480	2232800	Some of them here in this area is where when they figure out how something is done
2233760	2238240	you can say oh man that's such a dumb way of doing it and you immediately see how it can be
2238240	2243760	improved like for example there was a beautiful paper recently where they figured out how a large
2243760	2249680	language model stores certain facts like Eiffel Tower is in Paris and they figured out exactly
2249680	2255200	how it's stored and the proof that they understood it was they could edit it. They changed some of
2255200	2260400	the synapses in it and then they asked it where is the Eiffel Tower and they said it's in Rome
2261280	2266240	and then they asked you how do you get there? Oh how do you get there from Germany? Oh you take
2266240	2272080	this train and the Roma Termini train station and this and that and what might you see if you're
2272080	2278240	in front of it? Oh you might see the Colosseum. So they had edited it. So they literally moved
2278240	2285520	it to Rome. But the way it's storing this information it's incredibly dumb for any
2285520	2292000	fellow nerds listening to this. There was a big matrix and roughly speaking there are certain
2292000	2297280	row and column vectors which encode these things and they correspond very hand-wavely to principal
2297280	2303280	components and it would be much more efficient for a sparse matrix just store in the database.
2305440	2310080	But everything so far we've figured out how these things do are ways where you can see they can
2310080	2316640	easily be improved and the fact that this particular architecture has some roadblocks built into it
2316640	2324080	is in no way going to prevent crafty researchers from quickly finding workarounds and making
2325280	2332880	other kinds of architectures sort of go all the way. So in short it's turned out to be a lot
2333840	2339600	easier to build human close to human intelligence than we thought and that means our runway as a
2339600	2348880	species that get our shit together has shortened. And it seems like the scary thing about the
2348880	2353760	effectiveness of large language models. So Sam Altman every standard conversation with
2354880	2362320	and he really showed that the leap from GPT-3 to GPT-4 has to do with just a bunch of hacks.
2363040	2370160	A bunch of little explorations with smart researchers doing a few little fixes here
2370160	2375520	and there. It's not some fundamental leap and transformation in the architecture.
2375520	2380160	And more data and more compute. And more data and compute but he said the big leaps has to do
2380880	2386640	with not the data and the compute but just learning this new discipline just like you said.
2386640	2391680	So researchers are going to look at these architectures and there might be big leaps
2391680	2396000	where you realize why are we doing this in this dumb way. And all of a sudden this model is 10x
2396000	2402000	smarter and that can happen on any one day on any one Tuesday or Wednesday afternoon.
2402000	2406640	And then all of a sudden you have a system that's 10x smarter. It seems like it's such
2406640	2412480	a new discipline. It's such a new like we understand so little about why this thing works so damn well
2412480	2417280	that the linear improvement of compute or exponential but the steady improvement of
2417280	2421120	compute steady improvement of the data may not be the thing that even leads to the next
2421120	2424080	leap. It could be a surprise little hack that improves everything.
2424080	2429600	Or a lot of little leaps here and there because so much of this is out in the open also.
2431600	2435520	So many smart people are looking at this and trying to figure out little leaps here and there.
2435520	2441280	And it becomes a sort of collective race where a lot of people feel if I don't take the leap
2441280	2446080	someone else with. And it is actually very crucial for the other part of it. Why do we
2446080	2450960	want to slow this down? So again what this open letter is calling for is just pausing
2452160	2459120	all training of systems that are more powerful than GPT for for six months.
2460240	2468080	Give a chance for the labs to coordinate a bit on safety and for society to adapt.
2468080	2472640	Give the right incentives to the labs because you know you've interviewed a lot of these
2472640	2477760	people who lead these labs and you know just as well as I do that they're good people.
2477760	2482160	They're idealistic people. They're doing this first and foremost because they believe that
2482160	2489920	AI has a huge potential to help humanity. But at the same time they are trapped in this
2490800	2500800	horrible race to the bottom. Have you read Meditations on Moloch by Scott Alexander?
2501440	2506240	Yeah it's a beautiful essay on this poem by Ginsburg where he interprets it as being about
2506240	2514160	this monster. It's this game theory monster that pits people against each other in this
2515200	2519600	race to the bottom where everybody ultimately loses. The evil thing about this monster is
2519600	2523360	even though everybody sees it and understands they still can't get out of the race.
2524080	2530480	A good fraction of all the bad things that we humans do are caused by Moloch and I like
2531360	2538800	Scott Alexander's naming of the monster so we humans can think of it as a thing.
2540400	2544880	If you look at why do we have overfishing, why do we have more generally the tragedy
2544880	2551120	of the commons, why is it that so live, I don't know if you've had her on your podcast.
2551200	2558640	Yeah she's become a friend. Great she made this awesome point recently that beauty filters that
2558640	2565680	a lot of female influencers feel pressured to use are exactly Moloch in action again.
2566320	2572080	First nobody was using them and people saw them just the way they were and then some of them started
2572080	2578400	using it and becoming ever more plastic fantastic and then the other ones that weren't using it
2578400	2584400	started to realize that if they want to just keep their their market share they have to start
2584400	2590240	using it too and then you're in a situation where they're all using it and none of them has
2590240	2594960	any more market share or less than before so nobody gained anything. Everybody lost
2596720	2604160	and they have to keep becoming ever more plastic fantastic also right and but nobody can go back
2604160	2614000	to the old way because it's just too costly right. Moloch is everywhere and Moloch is not a new
2614000	2619280	arrival on the scene either. We humans have developed a lot of collaboration mechanisms
2619280	2624560	to help us fight back against Moloch through various kinds of constructive collaboration.
2625200	2630800	The Soviet Union and the United States did sign the number of arms control treaties
2631680	2638720	against Moloch who is trying to stoke them into unnecessarily risky nuclear arms races etc etc
2638720	2645280	and this is exactly what's happening on the AI front. This time it's a little bit geopolitics
2645280	2649680	but it's mostly money where there's just so much commercial pressure. If you take any of these
2650720	2657280	leaders of the top tech companies if they just say this is too risky I want to pause
2658080	2662080	for six months they're going to get a lot of pressure from shareholders and others.
2663440	2670400	They're like well you know if you pause but those guys don't pause. We don't want to get
2670400	2677040	our lunch eaten and shareholders even have the power to replace the executives in the worst case
2677040	2684880	right so we did this open letter because we want to help these idealistic tech executives to do
2685840	2691840	what their heart tells them by providing enough public pressure on the whole sector to just pause
2691840	2697280	so that they can all pause in a coordinated fashion and I think without the public pressure
2697280	2704560	none of them can do it alone push back against their shareholders no matter how good-hearted they
2704560	2714480	are. Moloch is a really powerful foe. So the idea is to for the major developers of AI systems
2714480	2723520	like this so we're talking about Microsoft, Google, Meta and anyone else? Open AI is very close with
2723520	2731200	Microsoft now of course and there are plenty of smaller players for example Anthropic which is
2731200	2735600	very impressive there's Conjecture there's many many many players I don't want to make a long
2735600	2743760	list so leave anyone out and for that reason it's so important that some coordination happens
2744480	2749920	that there's external pressure on all of them saying you all need to pause because then the
2749920	2754880	people the researchers in these organizations the leaders who want to slow down a little bit they
2754880	2761840	can say their shareholders you know everybody's slowing down because of this pressure and it's
2761840	2768720	the right thing to do. Have you seen in history their examples where it's possible to pause the
2769120	2774640	absolutely and even like human cloning for example you could make so much money on human cloning
2778320	2785360	why aren't we doing it? Because biologists thought hard about this and felt like this is
2786240	2791840	way too risky they got together in the 70s in the Selomar and decided even
2792800	2795920	to stop a lot more stuff also just editing the human germline
2797600	2805040	gene editing that goes into our offspring and decided let's not do this because
2805600	2811040	it's too unpredictable what it's going to lead to we could lose control over what happens to our
2811040	2818160	species so they paused there was a ton of money to be made there so it's very doable but you just
2818160	2824480	need you need a public awareness of what the risks are and the broader community coming in and saying
2825040	2831760	hey let's slow down and you know another common pushback I get today is we can't stop in the west
2832480	2840240	because China and in China undoubtedly they also get told we can't slow down because the west
2840240	2845440	because both sides think they're the good guy but look at human cloning you know
2846400	2850800	did China forge ahead with human cloning there's been exactly one human cloning that's actually
2850800	2856480	been done that I know of it was done by a Chinese guy do you know where he is now in jail
2857840	2864640	and you know who put him there who Chinese government not because westerners said China
2864640	2868000	looked this is no the Chinese government put him there because they also felt
2868960	2873920	they like control the Chinese government if anything maybe they are even more concerned
2873920	2879840	about having control then the western governments have no incentive of just losing control over
2879840	2885760	where everything is going and you can also see the Ernie bot that was released by I believe Baidu
2885760	2890720	recently they got a lot of pushback from the government and had to reign it in you know in a
2890720	2897440	big way I think once this basic message comes out that this isn't an arms race it's a suicide race
2897440	2905200	where everybody loses if anybody's AI goes out of control it really changes the whole dynamic
2905760	2912800	it's not it's I'll say this again because this is a very basic point I think a lot of people get
2912800	2921840	wrong because a lot of people dismiss the whole idea that AI can really get very superhuman
2921840	2926080	because they think there's something really magical about intelligence such that it can only exist
2926160	2930240	in human minds you know because they believe that they think it's kind of kind of get to just more or
2930240	2938640	less GPT-4 plus plus and then that's it they don't see it as a super as a suicide race they think
2938640	2943040	whoever gets that first they're going to control the world they're going to win but that's not how
2943040	2949040	it's going to be and we can talk again about the scientific arguments from why it's not going to
2949040	2955520	stop there but the way it's going to be is if anybody completely loses control and you know
2955520	2963120	you don't care if someone manages to take over the world who really doesn't share your goals
2964000	2967520	you probably don't really even care very much about what nationality they have you're not going
2967520	2974960	to like it much worse than today if you live in Orwellia in dystopia what do you care who
2974960	2982960	created it right and if it goes farther and we just lose control even to the machines
2982960	2992320	so that it's not us versus them it's us versus it what do you care who who created this this
2992320	2998400	unaligned entity which has goals different from humans ultimately and we get marginalized we get
2998400	3004960	made obsolete we get replaced that's why I what I mean when I say it's a suicide race
3006000	3011760	it's kind of like we're rushing towards this cliff but the closer the cliff we get the more
3011840	3017280	scenic the views are and the more money there is there and so we keep going but we have to also
3017280	3030000	stop at some point right quit while we're ahead and uh it's um it's a suicide race which cannot be
3030000	3038320	won but the way to really benefit from it is to continue developing awesome AI a little bit slower
3038960	3043520	so we make it safe make sure it does the things that humans want and create a condition where
3043520	3051680	everybody wins the technology has shown us that you know geopolitics and and politics in general
3051680	3056800	is not a zero-sum game at all so there is some rate of development that will lead
3058960	3064480	us as a human species to lose control of this thing and the hope you have is that there's some
3064480	3070240	lower level of development which will not which will not allow us to lose control this is an
3070240	3074240	interesting thought you have about losing control so what if you have somebody if you are somebody
3074240	3080240	like santa prachai or sam altman at the head of a company like this you're saying if they develop an
3080240	3087760	agi they too will lose control of it so no one person can maintain control no group of individuals
3087760	3093840	can maintain control if it's if it's created very very soon and is a big black box that we
3093920	3098400	don't understand like the large language models yeah then i'm very confident they're going to lose
3098400	3103680	control but this isn't just me saying it you know sam altman and them as the sabbis have both said
3104880	3109360	themselves acknowledge that you know there's really great risks with this and they they want to slow
3109360	3114960	down once they feel it gets it's scary it's but it's clear that they're stuck in this again malloc
3114960	3120480	is forcing them to go a little faster than they're comfortable with because of pressure from just
3120560	3126640	commercial pressures right to get a bit optimistic here you know of course this is a problem that
3126640	3135200	can be ultimately solved uh it's just to win this wisdom race it's clear that what we hope that
3135200	3140560	is going to happen hasn't happened the the capability progress has gone faster than a lot of people
3140560	3145680	thought then and the the progress in in the public sphere of policy making and so on has gone slower
3145680	3151120	than we thought even the technical ai safety has gone slower a lot of the technical safety research
3151120	3156400	was kind of banking on that um large language models and other poorly understood systems couldn't
3156400	3160560	get us all the way that you had to build more of a kind of intelligence that you could understand
3161120	3169120	maybe it could prove itself safe you know things like this and um i'm quite confident that this
3169120	3176560	can be done um so we can reap all the benefits but we cannot do it as quickly as uh this out of
3176560	3181920	control express train we are on now is going to get the agi that's why we need a little more time
3181920	3188960	i feel is there something to be said what like sam alman talked about which is while we're in the
3188960	3199040	pre agi stage to release often and as transparently as possible to learn a lot so as opposed to being
3199040	3205920	extremely cautious release a lot don't uh don't invest in a closed development where you focus on
3205920	3214000	ai safety while it's somewhat dumb quote unquote uh release as often as possible and as you start
3214000	3221360	to see signs of uh human level intelligence or superhuman level intelligence then you put a halt
3221360	3227200	on it well what a lot of safety researchers have been saying for many years is that the most dangerous
3227200	3233120	things you can do with an ai is first of all teach it to write code yeah because that's the
3233120	3237840	first step towards recursive self-improvement which can take it from agi to much higher levels
3238480	3246240	okay oops we've done that and uh another thing high risk is connected to the internet let it go
3246240	3252560	to websites download stuff on its own and talk to people oops we've done that already you know
3252640	3256800	eliezer yukowski you said you interviewed him recently right yes he had this tweet recently
3256800	3262480	which gave me one of the best laughs in a while where he's like hey people used to make fun of me
3262480	3268320	and say you're so stupid eliezer because you're saying you're saying uh you have to worry of
3268320	3273920	obviously developers once they get to like really strong ai the first thing you're gonna do is like
3273920	3279200	never connect it to the internet keep it in the box yeah where you know you can really study it
3279760	3286560	it's if so he had written it in the like in the meme forms was like then yeah and then that and then
3286560	3296640	now let's lol let's make a chatbot yeah and the third thing is steward russell yeah you know
3298560	3307440	amazing ai researcher he has argued for a while that we should never teach ai anything about humans
3308240	3312720	above all we should never let it learn about human psychology and how you manipulate humans
3313840	3317680	that's the most dangerous kind of knowledge you can give it yeah you can teach it all it needs to
3317680	3323040	know how to about how to cure cancer and stuff like that but don't let it read daniel kahneman's
3323040	3329840	book about cognitive biases and all that and then oops lol you know let's invent social media
3330640	3337760	I'll recommend our algorithms which do exactly that they they look get so good at knowing us
3338320	3343840	and pressing our buttons that we've we're starting to create a world now where we're just
3343840	3350000	having ever more hatred because they figured out that these algorithms not for out of evil but
3350000	3356480	just to make money on advertising that the best way to get more engagement the euphemism get people
3356480	3360960	glued to their little rectangles right is just to make them pissed off that's really interesting
3360960	3368400	that a large ai system that's doing the recommender system kind of tasks on social media is basically
3368400	3376080	just studying human beings because it's a bunch of us rats giving it signal nonstop signal it'll
3376080	3381200	show a thing and then we give signal and whether we spread that thing we like that thing that thing
3381200	3385600	increases our engagement gets us to return to the platform it has that on the scale of hundreds
3385600	3390400	of millions of people constantly so it's just learning and learning and learning and presumably
3390400	3395120	if the parameter the number of parameters in your network that's doing the learning and more
3395120	3402960	and to end the learning is the more it's able to just basically encode how to manipulate human
3402960	3408080	behavior how to control humans at scale exactly and that is not something I think is in humanity's
3408080	3413920	interest yes right now it's mainly letting some humans manipulate other humans for profit
3415840	3424480	and power which already caused a lot of damage and eventually that's a sort of skill that can
3424480	3430880	make ai's persuade humans to let them escape whatever safety precautions be it but you know
3430880	3436960	there was a really nice article in the New York Times recently by Yuval Noah Harari and
3437520	3443440	two co-authors including Tristan Harris from the social dilemma and they have this phrase in there
3443440	3452560	I love he said humanity's first contact with advanced AI was social media and we lost that one
3453680	3459760	we now live in a country where there's much more hate in the world where there's much more hate in
3459760	3465440	fact and in our democracy that we're having this conversation and people can't even agree on who
3465440	3470880	won the last election you know and we humans often point fingers at other humans and say it's
3470880	3478640	their fault but it's really malloc and these AI algorithms we got the algorithms and then malloc
3479760	3483600	pitted the social media companies are against each other so nobody could have a less creepy
3483600	3487680	algorithm because then they would lose out on revenue to the other company is there any way to
3487680	3493200	win that battle back just if we just linger on this one battle that we've lost in terms of social
3493200	3500800	media is it possible to redesign social media this very medium in which we use as a civilization
3500800	3505120	to communicate with each other to have these kinds of conversation to have discourse to try
3505120	3510480	to figure out how to solve the biggest problems in the world whether that's nuclear war or the
3510480	3516800	development of AGI is is it possible to do social media correctly I think it's not only possible
3516800	3521440	but it's it's necessary who are we kidding that we're going to be able to solve all these other
3521440	3526160	challenges if we can't even have a conversation with each other that's constructive the whole idea
3526160	3531040	the key idea of democracy is that you get a bunch of people together and they have a real
3531040	3535440	conversation the ones you try to foster on this podcast or you respectfully listen to people you
3535440	3540560	disagree with and you realize actually you know there are some things actually we some common ground
3540560	3547280	we have and let's it's yeah we both agree let's not have nuclear wars let's not do that and etc etc
3547760	3555120	we're kidding ourselves the thinking we can face off the second contact with
3555120	3559760	whatever more powerful AI that's happening now with these large language models if we can't even
3561200	3567200	have a functional conversation in the public space that's why I started the improve the news
3567200	3573680	project improve the news.org but I'm an optimist fundamentally in
3576400	3584400	and that there is a lot of intrinsic goodness in people and that what makes the difference
3585040	3591440	between someone doing good things for humanity and bad things is not some sort of fairy tale thing
3591440	3596080	that this person was born with the evil gene and this one was not born with a good gene no
3596080	3602800	I think it's whether we put whether people find themselves in situations that bring out the best
3602800	3610160	in them or the bring out the worst in them and I feel we're building an internet and a society that
3610800	3618160	brings out the worst but it doesn't have to be that way no it does not it's possible to create
3618160	3624000	incentives and also create incentives that make money uh they both make money and bring out the
3624000	3628000	best in people I mean in the long term it's not a good investment for anyone you know it's have a
3628000	3633440	nuclear war for example and you know is it a good investment for humanity if we just ultimately
3633440	3639360	replace all humans by machines and then we're so obsolete that eventually there's no humans left
3640400	3646560	well depends against how you do the math but I would say by any reasonable economic standard
3646560	3650720	if you look at the future income of humans and there aren't any you know that's not a good investment
3650800	3658480	moreover like why why can't we have a little bit of pride in our species damn it you know why should
3658480	3666240	we just build another species that gets rid of us if we were neanderthals would we really consider
3666240	3673200	it a smart move if if we had really advanced biotech to build homo sapiens you you know you
3673200	3679520	might say hey max you know yeah let's let's build build these homo sapiens they're going to be smarter
3679520	3684320	than us maybe they can help us defend this better against predators and help fix their
3684320	3690000	bar caves make them nicer we'll control them undoubtedly you know so then they build build
3690000	3697200	a couple a little baby girl a little baby boy they either and and then you have some some
3697200	3704400	wise old and the underthrall elder was like hmm I'm scared that uh we're opening a pandora's box
3704400	3711760	here and that we're going to get outsmarted by these super neanderthal intelligences
3712400	3716640	and there won't be any neanderthals left but then you have a bunch of others in the cave
3716640	3720800	right you are you such a luddite scare monger of course they're going to want to keep us around
3720800	3725360	because we are their creators and and why you know the smarter I think the smarter they get the
3725360	3729760	nicer they're going to get they're going to leave us they're going to they're going to want this around
3729840	3734720	and that's going to be fine and and besides look at these babies they're so cute it's clearly
3734720	3741200	they're totally harmless that's exact those babies are exactly GPT-4 yeah it's not I want to be clear
3741200	3749760	it's not GPT-4 that's terrifying it's the GPT-4 is a baby technology you know and Microsoft even
3749760	3757520	had a paper recently out uh with a title something like sparkles of AGI whatever basically saying
3757520	3765440	this is baby AI like these little neanderthal babies and it's going to grow up there's going to be
3765440	3770880	other systems from from the same company from other companies they'll be way more powerful and
3770880	3777600	but they're going to take all the things ideas from these babies and before we know it we're
3777600	3784080	going to be like those last neanderthals who are pretty disappointed and when they realized
3784160	3789280	they were getting replaced well this interesting point you make which is the programming it's
3789280	3795680	entirely possible that GPT-4 is already the kind of system that can change everything
3796800	3804400	by writing programs life three it's yeah it's because it's life 2.0 the systems I'm afraid of
3804400	3810880	are going to look nothing like a large language model and they're not but once it gets once
3810880	3816080	it or other people figure out a way of using this tech to make much better tech right it's just
3816080	3821920	constantly replacing its software and from everything we've seen about how how these work
3821920	3827360	under the hood they're like the minimum viable intelligence they do everything in a dumbest
3827360	3834800	way that still works sort of yeah and um so they are life 3.0 except when they replace their software
3835360	3838800	it's a lot faster than when you when when you decide to learn swedish
3840880	3851120	and moreover they think a lot faster than us too so when uh you know we don't think on how one
3852720	3860320	logical step every nanosecond or few or so the way they do and we can't also just suddenly scale
3860320	3868960	up our hardware massively in the cloud which is so limited right uh so they are they are also life
3870080	3876320	to have consumed become a little bit more like life 3.0 in that if they need more hardware hey
3876320	3880640	just rent it in the cloud you know how do you pay for it well with all the services you provide
3881040	3891200	and what we haven't seen yet which could change a lot is uh entire
3892880	3899840	software system so right now programming is done sort of in bits and pieces uh as as an assistant
3899840	3905680	tool to humans but i do a lot of programming and with the kind of stuff that gbt4 is able to do i
3905680	3911120	mean is replacing a lot what i'm able to do but i you still need a human in the loop to kind of
3912000	3917200	manage the design of things manage like what are the prompts that generate the kind of stuff to
3917200	3924400	to do some basic adjustment of the code to do some debugging but if it's possible to add on top of
3924400	3934000	gbt4 kind of uh feedback loop of of uh self-debugging improving the code and then you launch that
3934000	3938400	system onto the wild on the internet because everything is connected and have it do things
3938400	3943680	have it interact with humans and then get that feedback and now you have this giant ecosystem
3943680	3949760	yeah of humans that's one of the things that um yeah elon musk recently sort of tweeted as a case
3949760	3953920	why everyone needs to pay seven dollars or whatever for twitter to make sure they're real
3954480	3959200	they're make sure they're real we're now going to be living in a world where the the bots are
3959200	3965440	getting smarter and smarter and smarter to a degree where you can't uh you can't tell the
3965440	3971280	difference between a human and a bot that's right and now you can have uh bots outnumber humans by
3971280	3977280	a one million to one which is why he's making a case why you have to pay to prove you're human
3977280	3984080	which is one of the only mechanisms to which is depressing and i yeah i feel we have to remember
3984800	3989760	as individuals we should from time to time ask ourselves why are we doing what we're doing
3989760	3996720	all right then as a species we need to do that too so if we're building as as you say machines
3996720	4002720	that are outnumbering us and more and more outsmarting us and replacing us on the job
4002720	4009520	market not just for the dangerous and then boring tasks but also for writing poems and doing art
4009520	4014240	and things that a lot of people find really meaningful gotta ask ourselves why why are we
4014240	4022800	doing this uh we are the answer is malloc is tricking us into doing it and it's such a clever
4022800	4027120	trick that even though we see the trick we still have no choice but to fall for it right
4029440	4037120	come also the thing you said about you using uh co-pilot ai tools to program faster how many
4037200	4044400	what factor faster would you say you code now does it go twice as fast or i don't really uh
4044400	4049200	because it's a new tool yeah it's i don't know if speed is significantly improved
4050400	4058240	but it feels like i'm a year away from being uh five to ten times faster so if that's typical
4058240	4065280	for programmers then uh you're already seeing another kind of self recursive self-improvement
4065280	4071200	right because previously one like a major generation of improvement of the codes
4071200	4075200	would happen on the human r&d timescale and now if that's five times shorter
4076320	4080240	then it's going to take five times less time than otherwise would to develop the next level
4080240	4087200	of these tools and so on so this these these are the this is exactly the sort of beginning of an
4087200	4091920	of an intelligence explosion there can be humans in the loop a lot in the early stages and then
4091920	4096880	eventually humans are needed less and less and the machines can more kind of go along but you
4096880	4101520	what you weren't you said there is just an exact example of these sort of things another thing which
4101520	4109920	which um i was kind of lying on my psychiatrist imagining i'm on a psychiatrist couch here saying
4109920	4116000	what are my fears that people would do with um ai systems another so i mentioned three that i had
4116000	4122240	fears about many years ago that they would do uh namely uh teach it the code uh connected to the
4122240	4130960	internet and teach it to manipulate humans a fourth one is building an api where code can
4130960	4137760	control the super powerful thing right that's very unfortunate because one thing that systems like
4137760	4143440	gpt4 have going for them is that they are an oracle in the sense that they just answer questions
4144320	4149840	there is no robot connected to the gpt4 gpt4 can't go and do stock trading based on its thinking
4150560	4155520	it's not an agent and an intelligent agent is something that takes in information from the
4155520	4162640	world processes it to figure out what action to take based on its goals that it has and then
4163600	4170560	does something back on the world but once you have an api for example gpt4 nothing stops
4170560	4177920	joe schmo and a lot of other people from building real agents which just keep making calls somewhere
4177920	4184720	and some inner loop somewhere to these powerful oracle systems and which makes them themselves
4184720	4190000	much more powerful that's another kind of um unfortunate development which i think we would
4190000	4195520	have been better off uh delaying i don't want to pick on any particular companies i think
4195520	4204160	they're all under a lot of pressure to make money yeah and um again we the reason we're calling for
4204160	4209680	this pause is to give them all cover to do what they know is the right thing slow down a little bit
4209680	4216800	at this point but everything we've talked about i hope we'll can we'll make it clear to people
4216800	4223520	watching this you know why these sort of human level tools can cause a gradual acceleration
4223520	4229520	you keep using yesterday's technology to build tomorrow's technology yeah and when you do that
4229520	4233920	over and over again you naturally get an explosion you know that's the definition of an explosion
4233920	4244160	in science right like if you have um two people um they fall in love now you have four people and
4244160	4250480	then they can make more babies and now you have eight people and then then you have 16 32 64 etc
4250800	4257040	we call that a population explosion where it's just that each if it's instead free neutrons
4257840	4263280	in a nuclear reaction that if each one can make more than one then you get an exponential growth
4263280	4267520	in that we call it a nuclear explosion all explosions are like that an intelligence
4267520	4271200	explosion it's just exactly the same principle that some quantities some amount of intelligence
4271200	4276720	can make more intelligence than that and then repeat you always get the exponentials
4277680	4282240	what's your intuition why does you mention there's some technical reasons why it doesn't stop
4282240	4287440	at a certain point what's your intuition and uh do you have any intuition why it might stop
4288240	4290880	it's obviously going to stop when it bumps up against the laws of physics
4291600	4294960	there are some things you just can't do no matter how smart you are allegedly
4297200	4302240	and because we don't know the full laws of physics yeah like Seth Lloyd wrote a really cool
4302240	4308640	paper on the physical limits on computation for example if you make it put too much energy into
4308640	4313600	it and the finite space it'll turn into a black hole you can't move information around fast and
4313600	4321120	the speed of light stuff like that but uh it's hard to store way more than than a modest number
4321120	4327520	of bits per atom etc but you know those limits are just astronomically above like 30 orders of
4327520	4334960	magnitude above where we are now so bigger different bigger jump in intelligence than if
4334960	4345120	you go from a from an ant to a human I think of course what we want to do is have a controlled
4346080	4351200	thing a nuclear reactor you put moderators in to make sure exactly it doesn't blow up out of control
4351200	4359040	right when we do experiments with biology and cells and so on you know we also try to make
4359040	4366400	sure it doesn't get out of control um we can do this with AI too the thing is we haven't succeeded
4366400	4375200	yet and malloc is exactly doing the opposite just fueling just egging everybody on faster faster
4375200	4379600	faster or the other company is going to catch up with you or the other country is going to catch up
4379600	4387280	with you we do this we have to want to stop we have and and I don't believe in this just asking
4387920	4392880	people to look into their hearts and do the right thing it's easier for others to say that but like
4392880	4398880	if if you're in the situation where your company is going to get screwed if you
4401440	4405440	by other companies they're not stopping you know you're putting people in a very hard situation
4406000	4408800	the right thing to do is change the whole incentive structure instead
4409760	4414960	and and this is not an old maybe I should say one more thing about this because malloc
4415680	4422240	has been around as humanity's number one or number two enemy since the beginning of civilization
4422240	4428800	and we came up with some really cool countermeasures like first of all already over a hundred thousand
4428800	4434400	years ago evolution realized that it was very unhelpful that people kept killing each other all
4434400	4443120	the time yeah so it genetically gave us compassion and made it so that it like if you get too drunk
4443120	4450160	dudes getting into a pointless bar fight they might give each other black eyes but they have a lot of
4450160	4457920	inhibition towards just killing each other that's a and similarly if you find a baby lying on the
4457920	4462320	street when you go out for your morning jog tomorrow you're gonna stop and pick it up right
4462320	4468560	even though it may be a make you late for your next podcast so evolution gave us these genes
4468560	4474240	that make our own egoistic incentives more aligned with what's good for the greater group
4474240	4481040	we're part of right and then as we got a bit more sophisticated and developed language
4482240	4487600	we invented gossip which is also a fantastic anti malloc right because now
4488160	4496880	it it's really discourages liars moochers cheaters because it their own incentive now is not to do
4496880	4502000	this because word quickly gets around and then suddenly people aren't going to invite them to
4502000	4507040	their dinners anymore and or trust them and then when we got still more sophisticated and bigger
4507040	4513200	societies you know invented the legal system where even strangers who didn't couldn't rely on gossip
4514160	4517760	and and things like this would treat each other would have an incentive
4517760	4521360	now those guys in the bar fights even if they someone is so drunk that he
4522960	4528080	actually wants to kill the other guy he also has a little thought in the back of his head that you
4528080	4534320	know do i really want to spend the next 10 years eating like really crappy food in a small room
4536720	4540720	i'm just gonna chill out you know so and we we similarly have tried to give these incentives
4540800	4547200	to our corporations by having having regulation and all sorts of oversight so that their incentives
4547200	4554160	are aligned with the greater good we tried really hard and the big problem that we're failing now
4555600	4559120	is not that we haven't tried before but it's just that the tech is growing much
4560000	4563600	is developing much faster than the regulators been able to keep up right so
4564880	4570400	regulators it's kind of comical that european union right now is doing this AI act right which
4571040	4578560	and in the beginning they had a little opt-out exception that gpt4 would be completely excluded
4578560	4586560	from regulation brilliant idea what's the logic behind that some lobbyists pushed successfully
4586560	4591600	for this so we were actually quite involved with the future life institutes um mark brackel
4592160	4597520	mr uke anthony agir and others you know we're quite involved with um talking to very educating
4597520	4603760	various people involved in this process about these general purpose AI models coming and pointing
4603760	4608240	out that they would become the laughing stock if they didn't put it in so it the french started
4608240	4613360	pushing for it it got put in to the draft and it looked like all was good and then there was a huge
4614560	4619680	counterpush from lobbyists yeah there were more lobbyists sent in brussels from tech companies and
4619680	4625600	from oil companies for example and it looked like it might as we can maybe get taken out again
4626560	4633200	and now gpt4 happened and i think it's going to stay in but this just shows you know malloc
4633200	4640640	can be defeated but the the challenge you're facing is that the tech is generally much faster than
4640640	4648480	what the policy makers are and a lot of the policy makers also don't have a tech background so it's
4649200	4655200	you know we really need to work hard to educate them on how on what's taking place here so so
4655200	4660720	we're getting the situation where the first kind of non so you know i define artificial
4660720	4668320	intelligence just as non-biological intelligence all right and by that definition a company a
4668320	4672640	corporation is also an artificial intelligence because the corporation isn't it's humans it's a
4672640	4679840	system if its CEO decides the CEO of a tobacco company decides one morning the CEO he doesn't
4679840	4685200	want to sell cigarettes anymore they'll just put another CEO in there it's not enough to align
4685920	4692800	the incentives of individual people or align individual computers incentives to their owners
4692800	4697280	which is what technically iSafety research is about you also have to align the incentives of
4697280	4703040	corporations with a greater good and some corporations have gotten so big and so powerful
4703040	4711200	very quickly that in many cases their lobbyists instead align the regulators to what they want
4711200	4717840	rather than the early way around the classic regulatory capture all right is is the thing
4717840	4724240	that the slowdown hopes to achieve is give enough time to the regulators to catch up or enough time
4724240	4729440	to the companies themselves to breathe and understand how to do AI safety correctly i think
4729440	4735200	both and but i think that the vision path to success i see is first you give a breather
4735200	4740240	actually to to the people in these companies their leadership who wants to do the right thing and
4740240	4746080	they all have safety teams and so on on their companies give them a chance to get together
4746080	4754080	with the other companies and the outside pressure can also help catalyze that right and and work out
4754720	4762000	what is it that's what are the reasonable safety requirements one should put on future systems
4762000	4767600	before they get rolled out there are a lot of people also in academia and elsewhere outside
4767600	4773840	of these companies who can be brought into this and have a lot of very good ideas and then i think
4773840	4779840	it's very realistic that within six months you can get these people coming up so here's a white
4780480	4786240	paper here's where we all think is reasonable you know you didn't just because cars killed a
4786240	4790640	lot of people you didn't ban cars but they got together a bunch of people and decided you know
4790640	4796800	in order to be allowed to sell a car it has to have a seat belt in it there the analogous things
4796800	4807600	that you can start requiring a future AI systems so that they are are safe and once this heavy
4807600	4812560	lifting this intellectual work has been done by experts in the field which can be done quickly
4813360	4818560	i think it's going to be quite easy to get policymakers to to see yeah this is a good idea
4819200	4826800	and it's it's you know for the fight for the companies to fight mallock they want and i
4826800	4831040	believe sam altman has explicitly called for this they want the regulators to actually adopt it so
4831040	4834320	that their competition is going to abide by it too right you don't want
4837040	4841280	you don't want to be enacting all these principles then you abide by them and then
4841280	4848400	there's this one little company that doesn't sign on to it and then now they can gradually overtake
4848400	4855520	you then the companies will get be able to sleep secure knowing that everybody's playing by the
4855520	4862400	same rules so do you think it's possible to develop guardrails that keep the systems from
4864640	4871440	from basically damaging irreparably humanity while still enabling sort of the capitalist
4871440	4876800	fueled competition between companies as they develop how to best make money with this AI
4876800	4881280	you think there's a balancing that's possible absolutely i mean we've seen that in many other
4881280	4886720	sectors where you've had the free market produce quite good things without uh causing particular
4886720	4894720	harm um when the guardrails are there and they work you know capitalism is a very good way of
4894720	4899680	optimizing for just getting the same things on more efficiently it was but it was good you know
4899680	4907360	and like in hindsight i've never met anyone even even on parties way over on the right
4908080	4911840	in in in any country who who think it was a bad it thinks it was a terrible idea to
4912560	4918560	ban child labor for example yeah but it seems like this particular technology has gotten so
4918560	4926080	good so fast become powerful to a degree where you could see in the near term the ability to make
4926080	4931600	a lot of money yeah and to put guardrails develop guardrails quickly in that kind of context seems
4931600	4938800	to be tricky it's not uh similar to cars or child labor it seems like the opportunity to make a lot
4938800	4946640	of money here very quickly is right here before again it's there's this cliff yeah it gets quite
4946640	4951680	seated closer to the cliff there you go yeah the more the more go there more money there is more
4951680	4955520	gold in gets there on the ground you can pick up or whatever it's you want to drive there very fast
4956080	4960240	but it's not in anyone's incentive that we go over the cliff and it's not like everybody's in their
4960240	4965360	own car all the cars are connected together with a chain yeah so if anyone goes over they'll start
4965360	4969840	dragging others down the others down too and so ultimately it's in the selfish
4971840	4977680	interests also of the people in the companies to slow down when the when you can start seeing
4978240	4982800	the corners of the cliff there in front of you right and the problem is that even though the
4982800	4990320	people who are building the technology and the CEOs they really get it the shareholders and
4991040	4996720	these other market forces they are people who don't honestly understand that the cliff is there
4996720	5002000	they usually don't you have to get quite into the weeds to really appreciate how powerful this is
5002000	5008240	and how fast and a lot of people are even still stuck again in this idea that intelligence in this
5008240	5012960	carbon chauvinism as I like to call it that you can only have our level of intelligence in
5014000	5017920	humans that there's something magical about it whereas the people in the tech companies
5019440	5025200	who build this stuff they all realize that intelligence is information processing of a
5025200	5029840	certain kind and it really doesn't matter at all whether the information is processed by carbon
5029840	5038880	atoms in neurons in brains or by silicon atoms in some technology we build so you brought up
5038880	5045200	capitalism earlier and there are a lot of people who love capitalism and a lot of people who really
5045200	5055760	really don't and it's it struck me recently that the what's happening with capitalism here is exactly
5055760	5065200	analogous to the way in which super intelligence might wipe us out so you know I studied economics
5065200	5071840	for my undergrad in Stockholm school of economics yay well no no I tell me so I was very interested
5071840	5077600	in how how you could use market forces to just get stuff done more efficiently but give the right
5077600	5083360	incentives to market so that it wouldn't do really bad things so Dylan had Phil Manel who's a
5084160	5090160	professor and colleague of mine at MIT wrote this really interesting paper with some collaborators
5090160	5096640	recently where they proved mathematically that if you just take one goal that you just optimize for
5097760	5102560	on and on and on indefinitely that you think is gonna bring you in the right direction
5103280	5108400	what basically always happens is in the beginning it will make things better for you
5108400	5113920	but if you keep going at some point it's going to start making things worse for you again and then
5113920	5118720	gradually it's going to make it really really terrible so just as a simple the way I think of
5118720	5127040	the proof is suppose you want to go from here back to Austin for example and you're like okay yeah
5127040	5132640	let's just let's go south but you put in exactly the right sort of the right direction just optimize
5132640	5140560	that south as possible you get closer and closer to Austin but there's always some little error
5141280	5146320	so you you're not going exactly towards Austin but you get pretty close but eventually you start
5146320	5152080	going away again and eventually you're gonna be leaving the solar system yeah and they they proved
5152080	5157200	it's beautiful mathematical proof this happens generally and this is very important for AI
5157760	5164720	because for even though Stuart Russell has written a book and given a lot of talks on why
5164720	5169760	it's a bad idea to have AI just blindly optimize something that's what pretty much all our systems
5169760	5174000	do yeah we have something called a loss function then we're just minimizing or reward function
5174000	5184960	we're just minimize maximizing and um capitalism is exactly like that too we wanted to get stuff
5184960	5193040	done more efficiently that people wanted so introduce the free market things got done much
5193040	5201760	more efficiently than they did and and say communism right and it got better but then
5201760	5206880	it just kept optimizing it and kept optimizing and you got ever bigger companies and ever more
5206880	5214480	efficient information processing and I was also very much powered by it and eventually a lot of
5214480	5218480	people are beginning to feel wait we're kind of optimizing a bit too much like why did we just chop
5218480	5226480	down half the rain for us you know and why why did suddenly these regulators get captured by lobbyists
5226480	5233280	and so on it's just the same optimization that's been running for too long if you have an AI that
5233280	5237600	actually has power over the world and you just give it one goal and just like keep optimizing
5237600	5241760	that most likely everybody's gonna be like yay this is great in the beginning things are getting
5241760	5250320	better but um it's almost impossible to give it exactly the right direction to optimize in and then
5252080	5257680	eventually all hey breaks loose right nick boss drum and others are given examples that sound
5257680	5264320	quite silly like what if you just want to like tell it to cure cancer or something and that's all
5264320	5271680	you tell it maybe it's gonna decide to take over entire continents just so we can get more
5271760	5276160	super computer facilities in there and figure out a cure cancer backwards and then you're like
5276160	5283360	wait that's not what I wanted right and um the the the issue with capitalism and the issue with
5283360	5289760	the front of way I have kind of merged now because the malloc I talked about is exactly the capitalist
5289760	5298480	malloc that we have built an economy that has is optimized for only one thing profit right and
5298560	5302000	that worked great back when things were very inefficient and then now it's getting done better
5302560	5306640	and it worked great as long as the companies were small enough that they couldn't capture the
5306640	5315680	regulators but that's not true anymore but they keep optimizing and now we they realize that that
5315680	5319840	they can these companies can make even more profit by building ever more powerful AI even if it's
5319840	5330560	reckless but optimize more and more and more and more so this is malloc again showing up and I just
5330560	5338320	want to anyone here who has any concerns about about late stage capitalism having gone a little too
5338320	5345360	far you should worry about superintelligence because it's the same villain in both cases it's
5345440	5353520	malloc and optimizing one objective function aggressively blindly is going to take us there
5353520	5360640	yeah we have to pause from time to time and look into our hearts and ask why are we doing this is
5360640	5366480	this am I still going towards austin or have I gone too far you know maybe we should change direction
5367360	5372800	and that is the idea behind the halt for six months why six months it seems like a very short
5372800	5378960	period just can we just linger and explore different ideas here because this feels like a
5378960	5384720	really important moment in human history where pausing would actually have a significant positive
5384720	5393200	effect we said six months because we figured the number one pushback we were going to get
5393200	5401680	in the west was like but china and everybody knows there's no way that china is going to catch up
5402160	5406240	west on this in six months so it's that argument goes off the table and you can
5406240	5412480	forget about geopolitical competition and just focus on the real issue that's why we put this
5412480	5418480	that's really interesting but you've already made the case that even for china if you actually
5418480	5425680	want to take on that argument china too would not be bothered by a longer halt because they
5425680	5430640	don't want to lose control even more than the west doesn't that's what I think that's a really
5430640	5435360	interesting argument like I have to actually really think about that which the the kind of
5436000	5441680	thing people assume is if you develop an agi that open ai if they're the ones that do it for example
5442240	5449040	they're going to win but you're saying no they're everybody loses yeah it's going to get better
5449040	5453840	and better and better and then kaboom we all lose that's what's going to happen when lose and win
5453840	5461040	a define and a metric of basically quality of life for human civilization and for sam altman
5461920	5466640	I have to be people on my personal guess you know and people can quibble with this is that we're
5466640	5472080	just gonna there won't be any humans that's it that's what I mean by lose you know if you we can
5472080	5477840	see in history once you have some species or some group of people who aren't needed anymore
5478720	5486240	and doesn't usually work out so well for them right yeah there were a lot of horses for were used
5486240	5491680	for traffic in boston and then the car got invented and most of them got you know we don't
5491680	5503840	need to go there and uh if you look at humans you know right now we why did the labor movement
5503840	5512160	succeed and after the industrial revolution because it was needed even though we had a lot
5512160	5518400	of mollocks and there was child labor and so on you know the company still needed to have workers
5519520	5525280	and that's why strikes had power and so on if we get to the point where most humans aren't
5525280	5529040	needed anymore I think it's like it's quite naive to think that they're going to still be treated
5529040	5534880	well you know we say that yeah yeah everybody's equal and the government will always we'll always
5534880	5540320	protect them but if you look in practice groups that are very disenfranchised and don't have any
5540320	5550800	actual power usually get screwed and now in the beginning so industrial revolution we automated
5550800	5557840	away muscle work but that got went worked out pretty well eventually because we educated ourselves
5557840	5562640	and started working with our brains instead and got usually more interesting better paid jobs
5563520	5568560	but now we're beginning to replace brain work so we replaced a lot of boring stuff like we got the
5568560	5574560	pocket calculator so you don't have people adding multiplying numbers anymore at work fine there were
5574560	5582720	better jobs they could get but now gpt4 you know and the stable diffusion and techniques like this
5582720	5588880	they're really beginning to blow away some real some jobs that people really love having it was a
5588880	5595680	heartbreaking article just post just yesterday social media I saw about this guy who was doing
5595680	5601840	3d modeling for gaming and he and all of a sudden now they got this new software he just give
5601840	5610480	says prompts and he feels his whole job that he loved lost its meaning you know and I asked gpt4
5611040	5617120	rewrite twinkle twinkle little star in the style of Shakespeare I couldn't have done such a good
5617120	5623840	job it was really impressive you've seen a lot of art coming out here right so I'm all for
5625360	5631920	automating away the dangerous jobs and the boring jobs but I think you hear a lot some
5631920	5635200	arguments which are too glib sometimes people say well that's all that's going to happen we're
5635200	5641120	getting rid of the boring boring tedious dangerous jobs it's just not true there are a lot of really
5641120	5645600	interesting jobs that are being taken away now journalism is getting going to get crushed
5647040	5652960	coding is going to get crushed I predict the job market for programmers salaries are going to start
5652960	5659040	dropping you know if you said you can code five times faster you know then you need five times
5659040	5665040	fewer programmers maybe there will be more output also but you'll still end up using fewer program
5665040	5669600	needing fewer programmers than today and I love coding you know I think it's super cool
5671280	5676720	so we we need to stop and ask ourselves why again are we doing this as humans right
5678880	5685520	I feel that AI should be built by humanity for humanity and let's not forget that
5686160	5691360	it shouldn't be by malloc for malloc or what it really is now is kind of by humanity
5692000	5697840	for malloc which doesn't make any sense it's for us that we're doing it then and
5699280	5704800	make a lot more sense if we build develop figure out gradually safely how to make all this tech
5704800	5708000	and then we think about what are the kind of jobs that people really don't want to have
5708000	5714080	you know automate them all the way and then we ask what are the jobs that people really find
5714080	5724000	meaning in like maybe taking care of children in the daycare center maybe doing art etc etc and
5724000	5729120	even if it were possible to automate that way we don't need to do that right it's we built these
5729120	5736640	machines well it's possible that we redefine or rediscover what are the jobs that give us meaning
5736720	5740480	for me the thing it is really sad like I
5742000	5748880	half the time I'm excited half the time I'm crying as I'm generating code because
5749680	5758400	I kind of love programming it's an act of creation you have an idea you design it and then you
5758400	5762240	bring it to life and it does something especially if there's some intelligence to it does something
5762240	5766160	it doesn't even have to have intelligence bringing printing hello world on screen
5767120	5773440	you made a little machine and it comes to life and there's a bunch of tricks you learn along
5773440	5779920	the way because you've been doing it for for many many years and then to see AI be able to generate
5779920	5790800	all the tricks you thought were special I don't know it's very it it's scary it's almost painful
5790800	5798640	like a loss loss of innocence maybe like yeah maybe when I was younger I remember before I learned
5798640	5805440	that sugar is bad for you you should be on a diet I remember I enjoyed candy deeply in a way I just
5805440	5814480	can't anymore that I know is bad for me I enjoyed it unapologetically fully just intensely and I
5815200	5820800	lost that now I feel like a little bit of that is lost for me with program or being lost with
5820800	5828880	programming similar as it is for the 3d modeler no longer being able to really enjoy the art of
5828880	5833920	modeling 3d things for gaming I don't know I don't know what to make sense of that maybe I would
5833920	5838160	rediscover that the true magic of what it means to be human is connecting with other humans to
5838240	5845440	have conversations like this I don't know to uh to have sex to have to eat food to really intensify
5846160	5852080	the value from conscious experiences versus like creating other stuff you're pitching the rebranding
5852080	5857440	again from homo sapiens the homo sapiens the meaningful experiences and just to inject some
5857440	5862160	optimism in this year so we don't sound like a bunch of gloomers you know we can totally have
5862160	5866560	our cake and eat it you hear a lot of totally bullshit claims that we can't afford having more
5866640	5873440	teachers yeah have to cut the number of nurses you know that's just nonsense obviously with
5874160	5882720	anything even quite far short of agi we can dramatically improve grow the gdp and produce
5882720	5888240	this wealth of goods and services it's very easy to create a world where everybody is better off
5888240	5895120	than today including the richest people can be better off as well right it's not a zero-sum game
5895760	5900800	you know technology again you can have two countries like sweden and danmark had all these
5900800	5908720	ridiculous wars century after century and uh sometimes that sweden got a little better off
5908720	5911760	because it got a little bit bigger and then danmark got a little better off because sweden got a
5911760	5915840	little bit smaller and and but then we then technology came along and we both got just
5915840	5920080	dramatically wealthier without taking away from anyone else it was just a total win for everyone
5920960	5929280	and uh ai can do that on steroids if you can build safe agi if you can build super intelligence
5930800	5938000	basically all the limitations that cause harm today can be completely eliminated right it's a
5938000	5943680	wonderful you talk possibility and this is not sci-fi this is something which is clearly possible
5943680	5949760	according to the laws of physics and i we can talk about ways of making it safe also um but
5949760	5956240	unfortunately that'll only happen if we steer in that direction that's absolutely not the default
5956240	5964000	outcome that's why income inequality keeps going up that's why the life expectancy in the u.s has
5964000	5970160	been going down now i think it's four years in a row i just read a heartbreaking study from
5970160	5976720	the cdc about how something like one-third of all teenage girls in the u.s i've been thinking
5976720	5983920	about suicide you know like those are steps in the totally the wrong direction and and it's
5983920	5991280	important to keep our eyes on the prize here that we can we have the power now for the first time
5991280	5997840	in the history of our species to harness artificial intelligence to help us really flourish
5997840	6007680	and help bring out the best in our humanity rather than the worst of it to help us have
6007680	6013360	really fulfilling experiences that feel truly meaningful and and we you and i shouldn't sit
6013360	6016800	here and dictate the future generations what they will be let them figure it out but let's
6016800	6021920	give them a chance to live and and not foreclose all these possibilities for them by just messing
6021920	6026400	things up right now for that we have to solve the ai safety problem i just it'll be nice if
6026480	6033920	we can link on exploring that a little bit so one interesting way to enter that discussion is
6034720	6041520	you tweeted and elon replied you tweeted let's not just focus on whether gpt4 will do more harm or
6041520	6046560	good on the job market but also whether it's coding skills will hasten the arrival of super
6046560	6051120	intelligence that's something we've been talking about right so elon proposed one thing in their
6051120	6058800	reply saying maximum truth seeking is my best guess for ai safety can you maybe uh steelman the
6058800	6066640	case for this uh sense this objective function of truth and uh maybe make an argument against it
6066640	6072640	and in general what uh are your different ideas to start approaching the the solution to ai safety
6072640	6078000	i didn't see that reply actually oh interesting so but i really resonate with it because
6081760	6086560	ai is not evil it it caused people around the world to hate each other much more
6087680	6092560	but that's because we made it in a certain way it's a tool we can use it for great things and
6092560	6097840	bad things and we could just as well have ai systems and this is this is part of my vision
6097840	6105040	for success here truth seeking ai that really brings us together again you know why do people
6105040	6111040	hate each other so much between countries and within countries it's because they each have
6111040	6117600	totally different versions of the truth right if they all have the same truth that they trusted
6117600	6121600	for good reason because they could check it and verify it and not have to believe in some
6121600	6127120	self-proclaimed authority right they wouldn't be as nearly as much hate there'd be a lot more
6127120	6135360	understanding instead and this is i think something ai can help enormously with for example
6136880	6141280	a little baby step in this direction is this website called metaculous where
6143360	6147840	people bet and make predictions not for money but just for their own reputation
6148880	6152800	and it's kind of funny actually you treat the humans like you treat ai as you have a
6153760	6158400	lost function where they get penalized if they're super confident on something and then the opposite
6158400	6164640	happens yeah whereas if you're kind of humble and then you're like i think it's 51% chance this
6164640	6169200	is going to happen and then the other happens you don't get penalized much and and what you can see
6169200	6175760	is that some people are much better at predicting than others they've earned your trust right one
6175760	6179520	project that i'm working on right now is an outgrowth to improve the news foundation together
6179520	6184720	the metaculous folks is seeing if we can really scale this up a lot with more powerful ai because
6184720	6190720	i would love it i would love further to be like a really powerful truth seeking system where
6192160	6199360	that is trustworthy because it keeps being right about stuff and people who come to it and
6201840	6207520	maybe look at its latest trust ranking of different pundits and newspapers etc if they
6207520	6212320	want to know why some someone got a low score they can click on it and see all the predictions
6212320	6217440	that they actually made and how they turned out you know this is how we do it in science
6218080	6221680	you trust scientists like einstein who said something everybody thought was bullshit
6222320	6227200	and turned out to be right you get a lot for a trust point and he did it multiple times even
6227600	6235440	i think ai has the power to really heal a lot of the rifts we're seeing
6236640	6243760	by creating trust system it has to get away from this idea today with some fact-checking sites
6243760	6247520	which might themselves have an agenda and you just trust it because of its reputation
6249600	6254640	you want to have it so these sort of systems they earn their trust and that's completely
6254720	6262720	transparent this i think would actually help a lot that i think help heal the very dysfunctional
6263280	6268800	conversation that humanity has about how it's going to deal with all its biggest challenges
6268800	6277280	in the world today and then on the technical side you know another common sort of gloom
6278640	6281440	comment that yet from people are saying we're just screwed there's no hope
6282240	6286320	is well things like gpt4 are way too complicated for a human to ever understand
6287040	6290800	and prove that they can be trustworthy they're forgetting that ai can help us
6291520	6298800	prove that things work right and and there's this very fundamental fact that in math it's much
6299680	6304080	harder to come up with a proof than it is to verify that the proof is correct
6304720	6309680	you can actually write a little proof checking code it's quite short but you can as human
6309680	6314720	understand and then it can check most monstrously long proof ever generated even by a computer and
6314720	6327600	say yeah this is valid so so right now we we have um this uh this approach with virus checking
6327600	6332400	software that it looks to see if there's something if you should not trust it and if it can prove
6332400	6339040	to itself that you should not trust that code it warns you right what if you flip this around
6339920	6345280	and this is an idea i should give credit to steve i'm a hundred or four so that it will only run
6345280	6350480	the code if it can prove instead of not running it if it can prove that it's not trustworthy if
6350480	6354400	it will only run and if it can prove that it's trustworthy so it asks the code prove to me
6354400	6360320	that you're going to do what you say you're going to do and and it gives you this proof
6361520	6365040	and you a little proof tricker can check it now you can actually trust
6365920	6372560	an AI that's much more intelligent than you are right because you it's its problem to come
6372560	6376960	up with this proof that you could never have found that you should trust it so this is the
6376960	6383120	interesting point i i agree with you but this is where eliezer yakovsky might disagree with you
6383120	6389600	his claim not with you but with this idea is his claim is super intelligent AI
6390560	6394480	would be able to know how to lie to you with such a proof
6396000	6401040	how to lie to you and give me a proof that i'm gonna think is correct yeah but but it's not me
6401040	6408160	it's lying to you that's the trick my proof checker so yes so his general idea is a super
6408160	6416480	intelligent system can lie to a dumber proof checker so you're going to have as a system
6416480	6421440	because more and more intelligent there's going to be a threshold where a super intelligent system
6421440	6427120	would be able to effectively lie to a slightly dumber AGI system like there's a threat like he
6427120	6435040	really focuses on this weak AGI the strong AGI jump where the strong AGI can make all the weak
6435040	6442800	AGI's think that it's just one of them but it's no longer that and that leap is when it runs away
6442800	6449680	yeah i i don't buy that argument i think no matter how super intelligent an AI is it's never
6449680	6453040	going to be able to prove to me that they're only finitely many primes for example
6454880	6460880	and it just it just can't and and um it can try to snow me we're making up all sorts of new weird
6460880	6469120	rules of of deduction and that and say trust me you know the way your proof checker work is too
6469120	6475440	limited and we have this new hyper math and it's true but then i would i would just take the attitude
6475440	6480320	okay i'm going to forfeit some of these this supposedly super cool technologies i'm only
6480320	6484560	going to go with the ones that i can prove in my own trusted proof checker then i don't i think
6484560	6489760	it's fine there's still of course this is not something anyone has successfully implemented
6489760	6495600	at this point but i think it i just give it as an example of hope we don't have to do all the
6495600	6501120	work ourselves right this is exactly the sort of very boring and tedious tasks is perfect outsourced
6501120	6507440	to an AI and this is a way in which less powerful and less intelligent agents like us can actually
6508560	6514320	continue to control and trust more powerful ones so build a gi systems that help us defend against
6514320	6520320	other a gi systems well for starters begin with a simple problem of just making sure that the system
6520400	6526000	that you own or that's supposed to be loyal to you has to prove to itself that it's always
6526000	6530000	going to do the things that you actually wanted to do right and if it can't prove it maybe it's
6530000	6535440	still going to do it but you won't run it so you just forfeit some aspects of all the cool things
6535440	6540400	that i can do i i bet your dollars and donuts it can still do some incredibly cool stuff for you
6540400	6546400	yeah there are other things too that we shouldn't sweep under the rug like not every human agrees on
6546480	6551920	exactly where what direction we should go with humanity right yes and you've talked a lot about
6552480	6556400	geopolitical things on this on on your podcast to this effect you know but
6557600	6561920	i think that shouldn't distract us from the fact that there are actually a lot of things that
6561920	6568800	everybody in the world virtually agrees on that hey you know like having no humans on the planet
6568800	6576640	in a in a in a near future let's not do that right you look at something like the united
6576640	6584080	nation sustainable development goals some of them were quite ambitious and basically all the
6584080	6590560	countries agree us china russia ukraine you all agree so instead of quibbling about the little
6590560	6595600	things we don't agree on let's start with the things we do agree on and and and get them done
6596560	6603040	instead of being so distracted by all these things we disagree on that mollock wins because
6603760	6611920	frankly mollock going wild now it feels like a war on life playing out in front of our eyes
6611920	6620560	if you if you just look at it from space you know we're on this planet beautiful vibrant ecosystem
6620640	6626640	now we start chopping down big parts of it even though nobody most people thought that was a
6626640	6633840	bad idea always start doing ocean acidification wiping out all sorts of species oh now we have
6633840	6639920	all these close calls you almost had a nuclear war and we're replacing more and more of the biosphere
6639920	6646560	with non-living things we're also replacing in our social lives a lot of the things which
6647280	6652960	we're so valuable to humanity a lot of social interactions now are replaced by people staring
6652960	6659440	into their rectangles right and i i'm not a psychologist i'm out of my depth here but i
6659440	6664880	suspect that part of the reason why teen suicide and suicide in general in the u.s the
6664880	6671520	record-breaking levels is actually caused by again a so ai technologies and social media
6671520	6675600	that making people spend less time with with actually an actually just human interaction
6676240	6682160	we've all seen a bunch of good-looking people in restaurants staring into the rectangles
6682160	6688080	instead of looking into each other's eyes right that so that's also a part of the war on life
6688080	6699120	that that we're we're replacing so many really life-affirming things by technology we're we're
6699120	6704160	putting technology between us the technology that was supposed to connect us is actually
6704160	6710720	distancing us ourselves from each other and um and and then we're giving ever more power to
6710720	6716720	things which are not alive these large corporations are not living things right they're just maximizing
6716720	6725680	profit there i want to win them war on life i think we humans together with all our fellow
6725680	6732560	living things on this planet will be better off if we can remain in control over the non-living
6732560	6738480	things and make sure that they they work for us i really think it can be done can you just linger
6738480	6744800	on this um maybe high-level philosophical disagreement with eliezer yadkowski
6745440	6754640	i in this the hope you're stating so he is very sure he puts a very high probability
6755520	6762400	very close to one depending on the day he puts it at one uh that ai is going to kill humans
6763920	6770240	that there's just he does not see a trajectory which it doesn't end up with that conclusion
6770800	6774880	what uh what trajectory do you see that doesn't end up there and maybe can you
6775760	6780960	can you see the point he's making and and can you also see a way out
6783280	6791040	first of all i tremendously respect eliezer yadkowski and his his thinking second i do
6791840	6796880	share his view that there's a pretty large chance that we're not going to make it as humans there
6796880	6802000	won't be any humans on the planet and not the distant future and and that makes me very sad
6802000	6805520	you know we just had a little baby and i keep asking myself you know is um
6811520	6819120	how old is even gonna get you know and and um i asked myself it feels i i said to my wife recently
6819120	6825840	it feels a little bit like i was just diagnosed with some sort of um cancer which has some you know
6825840	6833600	risk of dying from and some risk of surviving you know uh except this is a kind of cancer which
6833600	6841200	would kill all of humanity so i completely take seriously his um his concerns i think um
6842800	6850560	but i don't absolutely don't think it's hopeless i think um there is a there is um first of all
6850560	6856480	a lot of momentum now for the first time actually since the many many years that have
6856480	6862240	passed since my since i and many others started warning about this i feel most people are getting
6862240	6872880	it now i i uh just talking to this guy in the gas station they were house the other day my
6873200	6882000	and he's like i think we're getting replaced and i think in it so that's positive that they're
6882000	6886480	finally we're finally seeing this reaction which is the first step towards solving the problem
6887040	6894720	uh second uh i really think that this this vision of only running ai's really if the stakes are
6894720	6899680	really high they can prove to us that they're safe it's really just virus checking in reverse
6899680	6907280	again i i think it's scientifically doable i don't think it's hopeless um we might have to
6907280	6912240	forfeit some of the technology that we could get if we were putting blind faith in our ai's
6912240	6916240	but we're still gonna get amazing stuff do you envision a process with a proof checker like
6916240	6923120	something like gpt4 gpt5 will go through a process no rigorous no no i think it's hopeless
6923120	6929680	that's like trying to prove fair about five spaghetti okay what i think well how the the
6929680	6934880	whole the vision i have for success is instead that you know just like we human beings were able
6934880	6940800	to look at our brains and and distill out the key knowledge Galileo when his dad threw him
6941440	6945040	an apple when he was a kid he was able to catch it because his brain could in this funny spaghetti
6945040	6949520	kind of way you know predict how parabolas are going to move his conaman system one right
6949520	6955840	but then he got older and it's like wait this is a parabola it's it's y equals x squared i can
6955840	6960000	distill this knowledge out and today you can easily program it into a computer and it can
6960640	6965600	simulate not just that but how to get tamars and so on right i envision a similar process
6965600	6971440	where we use the amazing learning power of neural networks to discover the knowledge in the first
6971440	6979120	place but we don't stop with a black box and and use that we then do a second round of ai
6979120	6983200	where we use automated systems to extract out the knowledge and see what is it look what are the
6983200	6989600	insights it's had okay and it's and then we we put that knowledge into a completely different
6989600	6995440	kind of architecture or programming language or whatever that's that's made in a way that
6995440	7001200	it can be both really efficient and also is more amenable to the very formal verification
7003040	7006800	that's that's my vision i'm not saying sitting here saying i'm confident
7006800	7011200	100 sure that it's gonna work you know but i don't think this chance is certainly not zero
7011200	7016480	either and it will certainly be possible to do for a lot of really cool ai applications
7017120	7021440	that we're not using now so we can have a lot of the fun that we're excited about
7022160	7028880	if we if we do this we're gonna need a little bit of time that's why it's good to pause and
7028880	7038720	put in place requirements one more thing also i i think you know someone might think well zero
7038720	7043600	percent chance we're gonna survive let's just give up right that's very dangerous
7046160	7051680	because there's no more guaranteed way it's a fail than to convince yourself that it's impossible
7052320	7059280	and not to try you know any if you you know when you're studying history and military history the
7059280	7065760	first thing you learn is that that's how you do psychological warfare you persuade the other side
7065760	7072880	that that's hopeless so they don't even fight and then of course you win right let's not do this
7072880	7077200	psychological warfare on ourselves and say there's a hundred percent probability we're all gonna we're
7077200	7084560	all screwed anyway it's sadly i i do get that a little bit sometimes from from uh
7084560	7088000	let's see some young people who are like so convinced that we're all screwed that they're like
7088000	7093600	i'm just gonna play game play computer games and do drugs and because we're screwed anyway right
7095200	7099920	it's important to keep the hope alive because it actually has a causal impact
7099920	7104720	and makes it makes it more likely that we're gonna succeed it seems like the people that actually
7104720	7110960	build solutions to a problem seemingly impossible to solve problems are the ones that believe yeah
7110960	7116240	they're the ones who are the optimists yeah and it's like uh it seems like there's some fundamental
7116240	7121840	law to the universe where fake it till you make it kind of works like believe yeah it's possible
7121840	7128960	and it becomes possible yeah was it henry ford who said that if you can if you tell yourself
7128960	7135200	that it's impossible it is so let's not make that mistake yeah and this is a big mistake
7135200	7139200	society is making you know i think all in all everybody's so gloomy and the media are also
7139200	7148000	very biased towards if it bleeds it leads and gloom and doom right so um most visions of the
7148000	7154240	future we have or or dystopian which really demotivates people we want to really really really
7154240	7159360	focus on the upside also to give people the willingness to fight for it and um
7162000	7167120	for ai you and i mostly talked about gloom here again but let's not remember not forget that you
7167120	7174800	know we have probably both lost someone we really cared about some disease that we were told were
7174800	7179520	was incurable well it's not there's no law of physics saying they had to die of that cancer
7179520	7185040	whatever of course you can cure it and there are so many other things where that we with
7185040	7191520	our human intelligence have also failed to solve on this planet which ai could also very much help
7191520	7197840	us with right so if we can get this right just be a little more chill and slow down a little bit
7197840	7205200	so we get it right it's mind blowing how awesome our future can be right we talked a lot about
7205200	7211120	stuff on earth can be great but even if you really get ambitious and look up into the skies right
7211120	7217360	there's no reason we have to be stuck on this planet for the rest of um the remain for billions
7217360	7224400	of years to come we totally understand now as laws of physics let life spread out into space
7225360	7229120	to other solar systems to other galaxies and flourish for billions of billions of years
7229840	7232560	and this to me is a very very hopeful vision
7234640	7240080	that really motivates me to to fight and coming back to the end something you talked about again
7240080	7245120	you know this the struggle how the human struggle is one of the things that also really gives meaning
7245120	7251680	to our lives if there's ever been an epic struggle this is it and isn't it even more epic if you're
7251680	7258160	the underdog if most people are telling you this is gonna fail it's impossible right and you persist
7259920	7265840	and you succeed right and that's what we can do together as a species on this one a lot of
7266480	7272160	pundits are ready to count this out both in the battle to keep AI safe and becoming a
7272160	7277920	multi-planetary species yeah and they're they're the same challenge if we can keep AI safe that's
7277920	7283760	how we're gonna get multi-planetary very efficiently i have some sort of technical questions about
7283760	7290080	how to get it right so one idea that i'm not even sure what the right answer is to is
7291200	7297840	should systems like GPT-4 be open sourced in whole or in part can make the can you see the
7297840	7307120	case for either i think the answer right now is no i think the answer early on was yes so we could
7307200	7315440	bring in all the wonderful great thought process of everybody on this but asking should we open
7315440	7319280	source GPT-4 now is just the same as if you say well is it good should we open source
7321760	7328240	how to build really small nuclear weapons should we open source how to make bio weapons
7329520	7335760	should we open source how to make a new virus that kills 90% of everybody who gets it of course we
7335760	7342640	shouldn't so it's already that powerful it's already that powerful that we have to respect
7342640	7348640	the power of the systems we've built the knowledge that you get
7350480	7356160	from open sourcing everything we do now might very well be powerful enough that people looking at
7356160	7362000	that can use it to build the things that you're really threatening again let's get it remember
7362000	7371200	open AI is GPT-4 is a baby AI baby sort of baby proto almost a little bit AGI according to what
7371200	7376240	Microsoft's recent paper said right it's not that that we're scared of what we're scared about
7376240	7382560	is people taking that who are who might be a lot less responsible than the company that made it
7383440	7392080	and just going to town with it that's why we want to it's it's an information hazard
7392080	7396880	there are many things which yeah are not open sourced right now in society for a very good reason
7397520	7405360	like how do you make certain kind of very powerful toxins out of stuff you can buy
7406080	7409280	and Home Depot you know you don't open source those things for a reason
7410240	7416960	and this is really no different so I'm saying that I have to say it feels in a bit weird in a
7416960	7422880	way a bit weird to say it because MIT is like the cradle of the open source movement and I love
7422880	7430880	open source in general power to the people let's say but there's always gonna be some stuff that
7430880	7436240	you don't open source and you know it's just like you don't open source so we have a three month old
7436240	7439920	baby right when he gets a little bit older we're not going to open source to him all the most
7439920	7447760	dangerous things he could do in the house yeah right but it does it's a weird feeling because
7447760	7453600	this is one of the first moments in history where there's a strong case to be made not to open
7453600	7460800	source software this is when the software has become too dangerous yeah but it's not the first
7460800	7463760	time that we didn't want to open source a technology technology yeah
7467040	7471840	is there something to be said about how to get the release of such systems right like GPT-4 and
7471840	7479680	GPT-5 so open AI went through a pretty rigorous effort for several months you could say it could
7479680	7484400	be longer but nevertheless it's longer than you would have expected of trying to test the system
7484400	7488160	to see like what are the ways it goes wrong to make it very difficult for people while
7489120	7496240	somewhat difficult for people to ask things how do I make a bomb for one dollar or how do I
7497280	7502560	say I hate a certain group on Twitter in a way that doesn't get me blocked from Twitter ban from
7502560	7511360	Twitter those kinds of questions yeah so you basically use the system to do harm yeah is there
7511360	7516480	something you could say about ideas you have it's just on looking having thought about this problem
7516560	7522080	of AI safety how to release such system how to test such systems when you have them inside the company
7524960	7525440	yeah so
7528880	7532720	a lot of people say that the two biggest risks from large language models are
7537600	7541920	it's spreading disinformation harmful information of various types
7542560	7548320	of threats and second being used for offensive cyber weapon
7550640	7554480	design I think those are not the two greatest threats they're very serious threats and it's
7554480	7560400	wonderful that people are trying to mitigate them a much bigger elephant in the room is how
7560400	7564640	is this going to disrupt the economy in a huge way obviously and maybe take away a lot of the
7564640	7570080	most meaningful jobs and an even bigger one is the one we spent so much time talking about here
7570080	7579200	that that this becomes the bootloader for the more powerful AI right code connected to the
7579200	7584400	internet manipulate humans yeah and before we know when we have something else which is not
7584400	7588320	at all a large language model that looks nothing like it but which is way more intelligent and
7588320	7595440	capable and has goals and that's the that's the elephant in the room and and obviously no matter
7595440	7601120	how hard any of these companies have tried they that's not something that's easy for them to verify
7601120	7605920	with large language models and the only way to be really lower that risk a lot would be
7606720	7612560	to not let for example train not never let it read any code not train on that and not put it
7612560	7621680	into an API and not not give it access to so much information about how to manipulate humans
7622240	7629600	so but that doesn't mean you still can't make a lot a ton of money on them you know we're
7629600	7637680	going to just watch now this coming year right Microsoft is rolling out the new office suite
7637680	7643120	where you go into Microsoft Word and give it a prompt that it writes the whole text for you
7643120	7647440	and then you edit it and then you're like oh give me a PowerPoint version of this and it makes it
7647440	7654480	and now take the spreadsheet and blah and you know all of those things I think are you can debate
7654480	7659600	the economic impact of it and whether society is prepared to deal with this disruption but those
7659600	7665200	are not the things which that's not the elephant of the room that keeps me awake at night for wiping
7665200	7671840	out humanity and I think that's the biggest misunderstanding we have a lot of people think
7671920	7677360	they were scared of like automatic spreadsheets that's not the case that's not what Eliezer
7677360	7685760	was freaked out about either is there in terms the actual mechanism of how AI might kill all humans
7686640	7692880	so something you've been outspoken about you've talked about a lot is it autonomous weapon systems
7693680	7701120	so the use of AI in war is that one of the things that still you carry a concern for
7701120	7705120	as these systems become more and more powerful and carry a concern for it not that all humans
7705120	7711760	are going to get killed by slaughterbots but rather just this express route into Orwellian dystopia
7711760	7716480	where it becomes much easier for very few to kill very many and therefore it becomes very easy for
7716480	7725200	very few to dominate very many right if you want to know how I could kill all people just ask yourself
7725200	7730800	we humans have driven a lot of species extinct how do we do it you know we were smarter than them
7731760	7737920	usually we didn't do it even systematically by going around one on one one after the other and
7737920	7742000	stepping on them or shooting them or anything like that we just like chopped down their habitat
7742000	7747920	because we needed it for something else in some cases we did it by putting more carbon dioxide
7747920	7753840	in the atmosphere because of some reason that those animals didn't even understand and now
7753840	7762320	they're gone right so if if you're in AI and you just want to figure something out then you decide
7762320	7769040	you know we just really need them this space here to build more compute facilities you know
7771680	7777200	if that's the only goal it has you know we are just the sort of accidental roadkill along the
7777200	7780960	way and you could totally imagine yeah maybe this oxygen is kind of annoying because it
7780960	7786160	caused more corrosion so let's get rid of the oxygen and good luck surviving after that you
7786160	7790320	know I I'm not particularly concerned that they would want to kill us just because
7792400	7796320	that would be like a goal in itself you know when we
7798400	7802400	driven number we've driven a number of the elephant species extinct right it wasn't
7802400	7810240	because we didn't like elephants what the basic problem is you just don't want to give
7811040	7815760	you don't want to seed control over your planet to some other more intelligent
7816320	7823600	entity that doesn't share your goals it's that simple so which brings us to another key challenge
7823600	7828480	which AI safety researchers have been grappling with for a long time like how do you make AI
7828880	7834320	first of all understand our goals and then adopt our goals and then retain them as they get smarter
7834320	7848800	right and all three of those are really hard right like a human child first they're just not
7848800	7856080	smart enough to understand our goals they can't even talk and then eventually they're teenagers
7856080	7862480	and understand our goals just fine but they don't share yeah but there's fortunately a
7862480	7865840	magic phase in the middle where they're smart enough to understand our goals and malleable
7865840	7870480	enough that we can hopefully with good parenting and teach them right from wrong and instead good
7870480	7877840	goal is still good goals in them right and so those are all tough challenges with computers
7877840	7881760	and then you know even if you teach your kids good goals when they're little they might outgrow
7881760	7887120	them too and that's a challenge for machines and they keep improving so these are a lot of hard
7887120	7895120	hard challenges we're up for but I don't think any of them are insurmountable the fundamental
7895120	7900160	reason why eliezer looked so depressed when I last saw him was because he felt it just wasn't
7900160	7906640	enough time oh that not that it was unsolvable correct it's just not enough time he was hoping
7906640	7911360	that humanity was going to take this threat more seriously so we would have more time yeah
7911360	7916000	and now we don't have more time that's why the open letter is calling for more time
7919760	7926720	but even with time the ai alignment problem seems to be really difficult oh yeah
7928080	7934080	but it's also the most worthy problem the most important problem for humanity to ever solve
7934080	7939840	because if we solve that one lex that aligned the eye can help us solve all the other problems
7940560	7946160	because it seems like it has to have constant humility about his goal constantly question the goal
7947520	7952800	because as you optimize towards a particular goal and you start to achieve it that's when
7952800	7957440	you have the unintended consequences all the things you mentioned about so how do you enforce and
7957440	7962720	code a constant humility as your ability become better and better and better and better
7962800	7965600	steward professor steward russell berkeley who's also one of the
7967840	7973920	driving forces behind this letter he uh has a whole research program about this
7975920	7981200	i think of it as ai humility exactly although he calls it inverse reinforcement learning
7981200	7986160	and other nerdy terms but it's about exactly that instead of telling the ai here's his goal go
7986160	7995600	optimize the the bejesus out of it you tell it okay do what i want you to do but i'm not going
7995600	8000240	to tell you right now what it is i want you to do you need to figure it out so then you give the
8000240	8004000	incentives to be very humble and keep asking you questions along the way is this what you really
8004000	8008480	meant is this what you wanted and oh this the other thing i tried didn't work seemed like it
8008480	8015440	didn't work out right should i try it differently what's nice about this is it's not just philosophical
8015440	8020320	mumbo jumbo it's theorems and technical work that with more time i think it can make a lot of
8020320	8026240	progress and there are a lot of brilliant people now working on ai safety we just not we just need
8026240	8031280	to give them a bit more time but also not that many relative to the scale of the problem no
8031280	8037760	exactly there there should be at least as just like every university worth its name has some
8037760	8043040	cancer research going on in its biology department right every university that's computer that does
8043040	8049600	computer science should have a real effort in this area and it's nowhere near that this is
8049600	8057120	something i hope is changing now thanks to the gpt4 right so i i think um if there's a silver lining
8057120	8062560	to um what's happening here even though i think many people would wish it would have been rolled
8062560	8070080	out more carefully is that this might be the wake-up call that humanity needed to really
8070560	8075440	stop the stop fantasizing about this being 100 years off and stop fantasizing about this being
8075440	8083920	completely controllable and predictable because it's so obvious it's it's not predictable you know
8083920	8093200	why is it that open that that i think it was gpt chat gpt tried to persuade um a journalist
8093760	8101600	a journalist where was the gpt4 to divorce his wife you know it was not because the
8101600	8108560	engineers had built it was like let's put this in here and and screw a little bit with people
8109520	8116160	they hadn't predicted at all they built the giant black box and trained to predict the next word
8116160	8120880	and got all these emergent properties and oops it did this you know um
8123840	8128720	i think this is a very powerful wake-up call and anyone watching this who's not scared
8129680	8135040	i would encourage them to just play a bit more with these these tools they're out there now like
8135040	8145120	gpt4 and um it's a wake-up call it's first step once you've woken up uh then gotta slow down a
8145120	8151520	little bit the risky stuff to give a chance to all everyone who's woken up to to catch up with
8151520	8158240	us on the safety front you know what's interesting is you know mit that's computer science but in
8158240	8163360	general but let's just even say computer science curriculum how does the computer science curriculum
8163360	8170880	change now you mentioned you mentioned programming yeah like why would you be when i was coming up
8170880	8177120	programming as a prestigious position like why would you be dedicating crazy amounts of
8177120	8181680	time to become an excellent programmer like the nature of programming is fundamentally changing
8181680	8189120	the nature of our entire education system is completely torn on its head i has anyone been
8189120	8194560	able to like load that in and like think about because it's really turning i mean some english
8194560	8199840	professors or english teachers are beginning to really freak out now yeah right they give an essay
8199840	8204880	assignment and they get back all these fantastic pros like this is the style of Hemingway and
8204880	8212160	and then they realize they have to completely rethink and even you know just like we stopped
8212160	8219680	teaching writing uh script is that what you're saying english yeah handwritten yeah yeah when
8219680	8223680	when everybody started typing you know like so much of what we teach our kids today
8224400	8237920	yeah i mean that's uh everything is changing and it's changing very it is changing very quickly
8237920	8242960	and so much of us understanding how to deal with the big problems of the world is through the
8242960	8248000	education system and if the education system is being turned on its head then what what's next
8248000	8253280	it feels like having these kinds of conversations is essential to try to figure it out and everything
8253280	8259040	is happening so rapidly uh i don't think there's even you're speaking of safety what the broad
8259040	8265680	ai safety defined i don't think most universities have courses on ai safety no it's a philosophy
8265680	8271760	and like i'm an educator myself so it pains me to see this say this but i feel our education
8271760	8278320	right now is that completely obsoleted by what's happening you know you put a kid into first grade
8279280	8283760	and then uh you envisioning like and then they're gonna come out of high school 12 years later
8284800	8288720	and you've already pre-planned now what they're gonna learn when you're not even sure if there's
8288720	8295280	going to be any world left to come out to like clearly you need to have a much more
8296240	8301680	opportunistic education system that keeps adapting itself very rapidly as society re-adapts
8302560	8308000	the the skills that were really useful when the curriculum was written i mean how many of those
8308080	8315280	skills are going to get you a job in 12 years i mean seriously if we just linger on the gpt4 system
8315280	8323840	a little bit you kind of hinted at it especially talking about the importance of consciousness
8323840	8333680	in uh in the human mind with home ascents do you think gpt4 is conscious i love this question
8334080	8340240	so let's define consciousness first because in my experience like 90% of all arguments
8340800	8344480	about consciousness are allowed to the two people arguing having totally different
8344480	8350480	definitions of what it is and they're just shouting past each other i define consciousness
8351840	8359360	as subjective experience right now i'm experiencing colors and sounds and emotions you know
8360160	8365760	but does a self-driving car experience anything that's the question about whether it's conscious
8365760	8373440	or not right other people think you should define consciousness differently fine by me but then maybe
8373440	8380880	use a different word for it or they i'm gonna use consciousness for this at least um so um
8382320	8389200	but if people hate the yeah so is gpt4 conscious does gpt4 have subjective experience
8390080	8394640	short answer i don't know because we still don't know what it is that gives this wonderful
8394640	8400400	subjective experience that is kind of the meaning of our life right because meaning itself the
8400400	8404960	feeling of meaning is a subjective experience joy is a subjective experience love is a subjective
8404960	8412400	experience we don't know what it is i've written some papers about this a lot of people have
8413360	8420160	the julio tononi professor has stuck his neck out the farthest and written down actually very
8420160	8426800	bold mathematical conjecture for what's the essence of conscious information processing
8426800	8433840	he might be wrong he might be right but we should test it uh he postulates that consciousness has
8433840	8440400	to do with loops in the information processing so our brain has loops information can go round
8440400	8445680	and round in computer science nerd speak you call it a recurrent neural network where some
8445680	8455680	of the output gets fed back in again and with his mathematical formalism if it's a feed forward
8455680	8460800	neural network where information only goes in one direction like from your eye retina into the
8460800	8464800	back of your brain for example that's not conscious so he would predict that your retina itself isn't
8464800	8473200	conscious of anything or a video camera now the interesting thing about gpt4 is it's also just
8473200	8480880	one way flow of information so if tononi is right and gpt4 is a very intelligent zombie
8480880	8489040	that can do all this smart stuff but isn't experiencing anything and this is both a relief
8489040	8492800	in that you don't have if it's true you know in that you don't have to feel guilty about turning
8492800	8499840	off gpt4 and wiping its memory whenever a new user comes along i wouldn't like if someone used that
8499840	8508560	to me neuralized me like in men in black but it's also creepy that you can have very high
8508560	8513040	intelligence perhaps then it's not conscious because if we get replaced by machines
8515760	8520160	otherwise it's sad enough that humanity isn't here anymore because i kind of like humanity
8521120	8526160	but at least if the machines were conscious it could be like well but there are descendants and
8526160	8531360	maybe we they have our values and there are children but if if tononi is right and it's all
8531360	8540480	these are all transformers that are not in the sense of the of hollywood but in the sense of these
8540480	8546320	one-way direction neural networks so they're all the zombies that's the ultimate zombie apocalypse
8546320	8550880	now we have this universe that goes on with great construction projects and stuff but there's no one
8550880	8558080	experiencing anything that would be like the ultimate depressing future so i i actually think
8559920	8565040	as we move forward to the building world last day i should do more research on figuring out what
8565040	8568960	kind of information processing actually has experience because i think that's what it's all
8568960	8575680	about and i completely don't buy the dismissal that some people some people will say well this
8575680	8581600	is all bullshit because consciousness equals intelligence right it's obviously not true you
8581600	8586160	can have a lot of conscious experience when you're not really accomplishing any goals at all you're
8586160	8593600	just reflecting on something and you can sometimes um have things doing things that are quite
8593600	8598560	intelligent probably without being being conscious but i also worry that we humans won't
8599440	8605360	we'll discriminate against the AI systems that clearly exhibit consciousness that we will not
8605360	8611680	allow AI systems to have consciousness we'll come up with theories about measuring consciousness
8611680	8617920	that will say this is a lesser being and this is why i worry about that because maybe we humans
8617920	8626240	will create something that is better than us humans in the in the way that we find beautiful
8627040	8633680	which is they they have a deeper subjective experience of reality not only are they smarter
8633680	8641280	but they feel deeper and we humans will hate them for it as we as human history is shown
8642000	8646960	they'll be the other we'll try to suppress it they'll create conflict they'll create war
8646960	8651760	all of this i i worry about this too are you saying that we humans sometimes come up with
8651760	8657280	self-serving arguments no we would never do that would be well that's the danger here is uh
8657840	8665760	even in this early stages we might create something beautiful yeah and uh we'll erase its memory i i uh
8666960	8674480	was horrified as a kid when someone started boiling uh boiling lobsters like oh my god that
8674480	8680080	that's so cruel and some grown up there that's back in sweden so it doesn't feel pain i'm like
8680080	8686400	how do you know that oh scientists have shown that and then there was a recent study where they
8686400	8690800	show that lobsters actually do feel pain when you boil them so they banned lobsters boiling in
8690800	8696000	switzerland now you have to kill them in a different way first so presumably that scientific research
8696560	8702640	boiled out to someone asked the lobsters has this hurt survey so we do the same thing with
8702640	8708080	cruelty to farm animals also all these self-serving arguments for why they're fine and yeah so we
8708080	8713680	should certainly be watchful i think step one is just be humble and acknowledge that consciousness
8713680	8718960	is not the same thing as intelligence and i believe that consciousness still is a form of
8718960	8723280	information processing where it's really information being aware of itself in a certain way and
8723280	8727440	let's study it and give ourselves a little bit of time and i think we will be able to figure out
8728080	8734400	actually what it is that causes consciousness and then we can make probably unconscious robots
8734400	8738800	that do the boring jobs that we would feel are immoral to get the machines but if you have a
8738800	8744800	companion robot taking care of your mom or something like that you would probably want it
8744800	8751040	to be conscious right so the emotions it seems to display aren't fake all these things can be
8752560	8758720	done in a good way if we give ourselves a little bit of time and don't run and take on this challenge
8759280	8765600	is there something you could say to the timeline that you think about about the development of AGI
8767040	8772560	depending on the day i'm sure that changes for you but when do you think there will be a really big
8772560	8779120	leap in intelligence where you definitively say we have built AGI do you think it's one year from
8779120	8788000	now five years from now 10 20 50 what's your gut say honestly
8791360	8795760	for the past decade i've deliberately given very long timelines because i didn't want to fuel some
8795760	8801600	kind of stupid malloc race yeah um but i think that cat has really left the bag now
8802560	8810640	and i think it might be very very close i don't think the microsoft paper is totally off when
8810640	8818160	they say that there are some glimmers of AGI it's not AGI yet it's not an agent there's a lot of
8818160	8827200	things it can't do but um i wouldn't bet very strongly against it happening very soon that's
8827200	8831840	why we decided to do this open letter because you know if there's ever been a time to pause
8832960	8841040	you know it's today there's a feeling like this GPT-4 is a big transition into waking everybody
8841040	8850000	out to the effectiveness of the system and so the next version will be big yeah and if that
8850000	8854800	next one isn't AGI maybe the next next one will and there are many companies trying to do these
8854800	8859920	things and the basic architecture of them is not some sort of super well-kept secret so
8860800	8866800	this is this is a time to a lot of people have said for many years that there will come a time
8866800	8871120	when we want to pause a little bit that time is now
8874240	8881680	you have spoken about and thought about nuclear war a lot over the past year we've
8881920	8891280	we've seemingly have come closest to the precipice of nuclear war than uh at least in my lifetime
8892720	8898320	yeah what do you learn about human nature from that it's our old friend Malak again
8899440	8907200	it's really scary to see it where America doesn't want there to be a nuclear war Russia
8907200	8911440	doesn't want there to be a global nuclear war either we know we both know that it's just
8911440	8916240	being others if we just try to do it it both sides try to launch first it's just another
8916240	8921440	suicide race right so why are we why is it the way you said that this is the closest we've come
8921440	8927040	since 1962 in fact i think we've come closer now than even the Cuban Missile Crisis it's
8927040	8934640	because of Malak you know you you have these other forces on one hand you have the west
8935600	8944160	saying that we have to drive Russia out of Ukraine it's a matter of pride and we've staked so much
8944160	8950800	on it that it would be seen as a huge loss of the credibility of the west if we don't drive
8950800	8960800	Russia out entirely of the Ukraine and on the other hand you have Russia who has and you have
8960800	8965680	the Russian leadership who knows that if they get completely driven out of Ukraine you know
8967200	8975200	it might it's not just going to be very humiliating for them but they might it often happens when
8975200	8980080	countries lose wars that things don't go so well for their leadership either like you remember when
8980080	8988480	Argentina invaded the Falkland Islands the the military junta that ordered that right people
8988480	8994560	were cheering on the streets at first when they took it and then when they got their butt kicked
8995200	9003120	by the British you know what happened to those guys they were out and I believe those were
9003120	9009440	still alive or in jail now right so so you know the Russian leadership is entirely cornered where
9009440	9017280	they know that just getting driven out of Ukraine is not an option and
9020640	9026720	so this to me is a typical example of Malik you you have these incentives of the two parties
9028000	9032000	where both of them are just driven to escalate more and more right if Russia starts losing in
9032000	9037520	the conventional warfare the only thing they can do is to back against the war is to keep
9037520	9043920	escalating and but and the west has put itself in the in the situation now we're sort of already
9043920	9048720	committed to the dry rush out so the only option the west has is to call Russia's bluff and keep
9048720	9054960	sending in more weapons this really bothers me because Malik can sometimes drive competing
9054960	9061200	parties to do something which is ultimately just really bad for both of them and you know
9061280	9068800	what makes me even more worried is not just that it's difficult to see an ending
9070160	9075200	a quick peaceful ending to this tragedy that doesn't involve some horrible escalation
9076240	9080640	but also that we understand more clearly now just how horrible it was going to be
9081440	9086800	there was an amazing paper that was published in Nature Food this August
9087360	9091440	by some of the top researchers who've been studying nuclear winter for a long time and what
9091440	9102400	they basically did was they combined climate models with food agricultural models so instead
9102400	9106080	of just saying yeah you know it gets really cold blah blah blah they figured out actually how
9106080	9111360	many people would die in the different different countries and it's uh it's pretty mind blowing
9111360	9114400	you know so basically what happens you know is that the thing that kills the most people is not
9115120	9122800	the explosions it's not the radioactivity it's not the EMP mayhem it's not the rampaging mobs
9123440	9128000	foraging food no it's the fact that you get so much smoke coming up from the burning cities
9128000	9137120	into the stratosphere that spreads around the earth from the jet streams so in typical models
9137120	9143840	you get like 10 years or so where it's just crazy cold and during the first year or
9143840	9152240	after the the war and their models the temperature drops in in Nebraska and in the Ukraine
9153120	9162000	bread baskets you know by like 20 Celsius or so if I remember no yeah 20 30 Celsius depending
9162000	9167040	on where you are 40 Celsius in some places which is you know 40 Fahrenheit to 80 Fahrenheit colder
9167040	9173600	than what it would normally be so you know I'm not good at farming but uh if it's snowing
9174640	9179280	if it drops below freezing pretty much most days in July and then like that's not good so
9179280	9183360	they worked out they put this into their farming models and what they found was really interesting
9184000	9187600	the countries that get the most hard hit are the ones in the northern hemisphere
9188960	9195360	so in in the US and and one model they had about 99 percent of all Americans starving to death
9196240	9200800	in Russia and China and Europe also about 99 98 percent starving to death
9202080	9206960	so you might be like oh it's kind of poetic justice that both the Russians and the Americans
9208080	9211360	99 percent of them have to pay for it because it was their bombs that did it but
9212000	9217280	you know that doesn't particularly cheer people up in Sweden or other random countries that have
9217280	9225280	nothing to do with it right and um it uh I think it hasn't entered the mainstream
9228400	9235600	not understanding very much just like how bad this is most people especially a lot of people
9235600	9239200	in decision-making positions still think of nuclear weapons as something that makes you powerful
9239200	9246560	uh scary powerful they don't think of it as something where uh yeah just
9248160	9251520	to within a percent or two you know we're all just just gonna starve to death
9252880	9262800	and um and starving to death is is um the worst way to die as holly molly is all all the
9262800	9268960	famines in history show the torture involved in that probably brings out the worst in people also
9269760	9276880	when when people are desperate like this it's not so some people I've heard some people say that
9278000	9281840	if that's what's gonna happen they'd rather be at round zero and just get vaporized you know
9283520	9290400	but uh so but I think people underestimate the risk of this because they they
9291360	9295760	they aren't afraid of malloc they think oh it's just gonna be because humans don't want this so
9295760	9299600	it's not gonna happen that's the whole point the malloc that things happen that nobody wanted
9300240	9304080	and that applies to nuclear weapons and that applies to agi
9306080	9311120	exactly and it applies to some of the things that people have gotten most upset with capitalism
9311120	9317280	for also right where everybody was just kind of trapped you know it it's not to see if some
9317280	9324960	company does something uh it causes a lot of harm and not that the ceo is a bad person
9325520	9330560	but she or he knew that you know that the other all the other companies were doing this too so
9330560	9341120	malloc is um as a formable foe I hope which someone makes good movies so we can see who
9341120	9346480	the real enemy is we don't because we're not fighting against each other uh malloc makes
9346480	9354320	us fight against each other that's more that's what malloc superpower is the hope here is any
9354320	9360400	kind of technology or the mechanism that lets us instead realize that we're fighting the wrong
9360400	9367520	enemy right now it's such a fascinating battle it's not us versus them it's us versus it yeah yeah
9368160	9374160	we are fighting malloc for human survival yeah we as a civilization have you seen the movie
9374240	9380960	needful things it's a steven king novel I love steven king and uh max von sudo of
9380960	9386160	Swedish actors playing the guys it's brilliant exactly I just thought I hadn't thought about
9386160	9392400	that until now but that's the closest I've seen to a a movie about malloc I don't want to spoil
9392400	9399360	the film for anyone who wants to watch it but basically it's about this guy who turns out to
9399360	9403440	you can interpret him as the devil or whatever but he doesn't actually ever go around or
9403440	9408560	and kill people or torture people will go burning coal or anything he makes everybody fight each
9408560	9413440	other it makes everybody hate fear each other hate each other and then kill each other so that
9413440	9421440	that's the movie about malloc you know love is the answer that seems to be um one of the ways to
9421440	9430960	fight malloc is by um compassion by seeing the common humanity yes yes and to not sound so we
9430960	9437680	don't sound like like uh was it kumbaya tree huggers here right we're not just saying love
9437680	9444960	and peace man we're trying to actually help people understand the true facts about the other side
9446720	9455440	and feel the compassion because the truth makes you more compassionate right
9456240	9465840	so I that's why I really like using AI for truth and for truth seeking technologies can
9468640	9476560	that can as a result you know get us more love than hate and and even if you can't get love you
9476560	9482000	know settle for settle for some understanding which already gives compassion if someone is like you
9482000	9488320	know I really disagree with you lex but I can see why you're where you're coming from you're not a
9490400	9494320	bad person who needs to be destroyed but I disagree with you and I'm happy to have an
9494320	9499600	argument about it you know that's a lot of progress compared to where we are 2023 in
9499600	9505760	the public space wouldn't you say if we solve the AI safety problem as we've talked about
9506480	9514240	and then you max tag mark who has been talking about this uh for many years get to sit down
9514240	9520640	with the agi with the early agi system on a beach with a drink uh what what what kind of what would
9520640	9526880	you ask her what kind of question would you ask what would you talk about something so much smarter
9526880	9533520	than you would be would you be afraid we're gonna get me with a really zinger of a question
9534080	9542720	would you be afraid to ask some questions yeah so I'm not afraid of the truth I'm very humble I
9542720	9548960	know I'm just a meat bag you with all these flaws you know but yeah I I have the I we talked a lot
9548960	9553680	about homo sentience I've really already tried that for a long time with myself just so that is
9553680	9558800	what's really valuable about being alive for me is that I have these meaningful experiences
9559680	9565520	it's not that I'm have what I'm good at this or good at that or whatever there's so much I suck at
9565520	9571440	and so you're not afraid for the system to show you just how dumb you are no no in fact my son
9571440	9577440	reminds me that you could find out how dumb you are in terms of physics how little how little
9577440	9586400	we humans understand I'm cool with that I think I think um so I can't waffle my way out of this
9586400	9592880	question it's a fair one I think given that I'm a really really curious person that's
9594160	9598160	really the defining part of who I am I'm so curious
9603440	9610800	uh I have some physics questions I love to love to understand I have some questions about
9611440	9615600	consciousness about the nature of reality I would just really really love to understand also
9617120	9620240	I could tell you one for example that I've been obsessing about a lot recently
9622960	9630000	so I believe that so suppose tenoni is right as opposed there are some information processing
9630000	9634400	systems that are conscious and some they're not suppose you can even make reasonably smart things
9634400	9640000	like gpt4 they're not conscious but you can also make them conscious here's the question that keeps
9640000	9648560	me naked night is it the case that the unconscious zombie systems that are really intelligent
9648560	9653920	are also really efficient so they're really inefficient so that when you try to make things
9653920	9660320	more efficient we still naturally be a pressure to do they become conscious I'm kind of hoping
9661120	9665680	that that's correct and I do you want me to give you a hand away the argument for it please you know
9665680	9673680	like in my lab again every time we look at how how these large language models do something we
9673680	9678640	see they do them in really dumb ways and you could you could make it make it better if if you uh
9680080	9685680	we have loops in our computer language for a reason the code would get way way longer if you
9685680	9694240	weren't allowed to use them it's more efficient to have the loops and in order to have self-reflection
9694240	9699520	whether it's conscious or not right even an operating system knows things about itself right
9701520	9709440	you need to have loops already right so I think this is I'm waving my hands a lot but I suspect that
9711840	9715440	the most efficient way of implementing a given level of intelligence
9715760	9724800	has loops in it the self-reflection can and will be conscious isn't that great news yes if it's
9724800	9729840	true it's wonderful because then we don't have to fear the ultimate zombie apocalypse and I think
9729840	9736640	if you look at our brains actually our brains are part zombie and part conscious
9737520	9748320	when I open my eyes I immediately take all these pixels that hit my on my retina right and like oh
9748320	9754000	that's Lex but I have no freaking clue of how I did that computation it's actually quite complicated
9754000	9760720	right it was only relatively recently we could even do it well with machines right you get a bunch
9760720	9764880	of information processing happening in my retina then it goes to the lateral genicular nucleus
9764880	9770640	my thalamus and the vision the area v1 v2 v4 and the fusiform face area here that Nancy can
9770640	9775520	wish her at MIT invented and blah blah blah blah and I have no freaking clue how that worked right
9776240	9782400	it feels to me subjectively like my conscious module just got a little email say
9783200	9794240	face facial processing uh fit task complete it's Lex yeah and I'm gonna just go with that right
9795040	9801760	so uh this fits perfectly with Tanone's model because this was all one way information processing
9802400	9810080	mainly and uh it turned out for that particular task that's all you needed and it probably was
9810080	9814880	kind of the most efficient way to do it but there were a lot of other things that we associated with
9814880	9820160	higher intelligence and planning and and so on and so forth where you kind of want to have loops and
9820160	9827360	be able to ruminate and self-reflect and introspect and so on where my hunch is that if you want to
9827360	9832320	fake that with a zombie system that just all goes one way you have to like unroll those loops and it
9832320	9837760	just really really long and it's much more inefficient so I'm actually hopeful that AI
9838560	9842560	if in the future we have all these very sublime and interesting machines that do cool things
9843360	9849840	and are lined with us that they will at least they will also have consciousness for the kind of
9849840	9856800	these things that we do that great intelligence is also correlated to great consciousness or a deep
9856800	9863360	kind of consciousness yes so that's a happy thought for me because the zombie apocalypse
9863360	9868400	really is my worst nightmare of all it would be like adding insult to injury not only did we get
9868400	9874560	replaced but we friggin replaced ourselves by zombies like how dumb can we be uh that's such a
9874560	9879840	beautiful vision and that's actually a provable one that's one that we humans can uh intuit and
9879840	9885200	prove that those two things are correlated as we start to understand what it means to be intelligent
9885200	9891600	and what it means to be conscious which these systems uh early AGI like systems will help us
9891600	9896480	understand and I just want to say one more thing which is super important most of my colleagues
9896480	9900080	when I started going on about consciousness tell me that it's all bullshit and I should stop talking
9900080	9906800	about it I hear a little inner voice from my father and from my mom saying keep talking about it
9906800	9915200	because I think they're wrong and and and the main way to convince people like that that they're
9915200	9920080	wrong if they say that consciousness is just equal to intelligence is to ask them what's wrong with
9920080	9928160	torture now why are you against torture if it's just about you know these these particles
9928160	9932720	moving this way around on that way and there is no such thing as subjective experience what's
9932720	9938160	wrong with torture I mean do you have a good comeback to that no it seems like suffering
9938720	9945120	suffering imposed on other humans is somehow deeply wrong in a way that intelligence doesn't
9945120	9951360	quite explain and if someone tells me well you know it's just an illusion consciousness
9953040	9959360	whatever you know I like to invite them the next time they're having surgery to do it without
9959360	9965680	anesthesia like what is anesthesia really doing if you have it you can have a local anesthesia
9965680	9969440	when you're awake I had that when they fixed my shoulder I was super entertaining uh
9970080	9976320	uh what was that that it did it just removed my subjective experience of pain it didn't change
9976320	9981120	anything about what was actually happening in my shoulder right so if someone says that's all
9981120	9988800	bullshit skip the anesthesia as my advice this is incredibly central it could be fundamental
9988800	9995120	to whatever this thing we have going on here it is fundamental because we're we what we feel
9995120	10001280	is so fundamental is suffering and joy and pleasure and meaning and
10003520	10009680	that's all those are all subjective experiences there and let's not those are the elephant in
10009680	10013280	the room that's what makes life worth living and that's what can make it horrible if it's just
10013280	10018800	those are suffering so let's not make the mistake of saying that that's all bullshit and let's not
10018880	10026160	make the mistake of uh not instilling the AI systems with that same thing that makes us
10027760	10033360	special yeah max it's a huge honor that you will sit down to me the first time
10034400	10039280	on the first episode of this podcast it's a huge honor to sit down again and talk about this what
10039280	10046160	I think is the most important topic the most important problem that we humans have to face
10046800	10053280	and hopefully solve yeah well the honor is all mine and I'm I'm so grateful to you for making
10053280	10057920	more people aware of this fact that humanity has reached the most important fork in the road
10057920	10064080	ever in its history and let's turn in the correct direction thanks for listening to this conversation
10064080	10069520	with max tag mark to support this podcast please check out our sponsors in the description and
10069520	10075680	now let me leave you with some words from frank harbert history is a constant race
10076320	10083360	between invention and catastrophe thank you for listening and hope to see you next time
