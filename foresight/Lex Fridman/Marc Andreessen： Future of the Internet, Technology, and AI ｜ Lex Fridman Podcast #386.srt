1
00:00:00,000 --> 00:00:04,840
the competence and capability and intelligence and training and accomplishments of senior

2
00:00:04,840 --> 00:00:09,320
scientists and technologists working on a technology and then being able to then make

3
00:00:09,320 --> 00:00:13,520
moral judgments in the use of that technology, that track record is terrible. That track

4
00:00:13,520 --> 00:00:18,360
record is catastrophically bad. The policies that are being called for to prevent this,

5
00:00:18,360 --> 00:00:20,600
I think we're going to cause extraordinary damage.

6
00:00:20,600 --> 00:00:24,720
So the moment you say AI is going to kill all of us, therefore we should ban it or we

7
00:00:24,720 --> 00:00:27,800
should regulate all that kind of stuff, that's when it starts getting serious.

8
00:00:27,800 --> 00:00:29,840
Or start, you know, military airstrikes and data centers.

9
00:00:29,840 --> 00:00:30,840
Oh boy.

10
00:00:30,840 --> 00:00:38,200
The following is a conversation with Mark Andreessen, co-creator of Mosaic, the first

11
00:00:38,200 --> 00:00:43,520
widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley

12
00:00:43,520 --> 00:00:49,360
venture capital firm Andreessen Horowitz, and is one of the most outspoken voices on

13
00:00:49,360 --> 00:00:57,120
the future of technology, including his most recent article, why AI will save the world.

14
00:00:57,120 --> 00:01:01,040
This is Alex Friedman podcast. To support it, please check out our sponsors in the

15
00:01:01,040 --> 00:01:06,680
description. And now, dear friends, here's Mark Andreessen.

16
00:01:06,680 --> 00:01:10,280
I think you're the right person to talk about the future of the internet and technology

17
00:01:10,280 --> 00:01:16,560
in general. Do you think we'll still have Google search in five, in 10 years, or search

18
00:01:16,560 --> 00:01:17,560
in general?

19
00:01:17,560 --> 00:01:22,280
Yes, you know, it'd be a question if the use cases have really narrowed down.

20
00:01:22,280 --> 00:01:30,320
Well, now with AI and AI assistance, being able to interact and expose the entirety of

21
00:01:30,320 --> 00:01:36,160
human wisdom and knowledge and information and facts and truth to us via the natural

22
00:01:36,160 --> 00:01:42,960
language interface, it seems like that's what search is designed to do. And if AI assistance

23
00:01:42,960 --> 00:01:46,000
can do that better, doesn't the nature of search change?

24
00:01:46,000 --> 00:01:48,120
Sure. But we still have horses.

25
00:01:48,120 --> 00:01:52,680
Okay. When was the last time you rode a horse?

26
00:01:52,680 --> 00:01:53,680
It's been a while.

27
00:01:53,680 --> 00:02:02,520
All right. But what I mean is, will we still have Google search as the primary way that

28
00:02:02,520 --> 00:02:05,320
human civilization uses to interact with knowledge?

29
00:02:05,320 --> 00:02:09,280
I mean, search was a technology, it was a moment in time technology, which is you have

30
00:02:09,280 --> 00:02:12,480
in theory the world's information out on the web. And, you know, this is sort of the

31
00:02:12,480 --> 00:02:16,080
open way to get to it. But yeah, like, and by the way, actually Google has known this

32
00:02:16,160 --> 00:02:19,400
for a long time. I mean, they've been driving away from the 10 blue links for, you know,

33
00:02:19,400 --> 00:02:21,640
for like two days, they've been trying to get away from that for a long time.

34
00:02:21,640 --> 00:02:22,640
What kind of links?

35
00:02:22,640 --> 00:02:23,640
They call the 10 blue links.

36
00:02:23,640 --> 00:02:24,640
10 blue links.

37
00:02:24,640 --> 00:02:28,360
So the standard Google search result is just 10 blue links to random websites.

38
00:02:28,360 --> 00:02:29,800
And they turn purple when you visit them.

39
00:02:29,800 --> 00:02:30,800
That's HTML.

40
00:02:30,800 --> 00:02:32,800
Yes, we picked those colors.

41
00:02:32,800 --> 00:02:33,800
Thanks.

42
00:02:33,800 --> 00:02:34,800
Thanks.

43
00:02:34,800 --> 00:02:37,040
I'm touching on this topic.

44
00:02:37,040 --> 00:02:38,040
No offense.

45
00:02:38,040 --> 00:02:39,040
Yeah, it's good.

46
00:02:39,040 --> 00:02:42,560
Well, you know, like Marshall McLuhan said that the content of each new medium is the

47
00:02:42,560 --> 00:02:43,560
old medium.

48
00:02:43,560 --> 00:02:45,640
The content of each new medium is the old medium.

49
00:02:45,640 --> 00:02:48,400
The content of movies was theater, you know, theater plays.

50
00:02:48,400 --> 00:02:52,280
The content of theater plays was, you know, written stories, the content of written stories

51
00:02:52,280 --> 00:02:53,280
was spoken stories.

52
00:02:53,280 --> 00:02:54,280
Right.

53
00:02:54,280 --> 00:02:57,720
And so you just kind of fold the old thing into the new thing.

54
00:02:57,720 --> 00:03:00,600
How does that have to do with the blue and the purple?

55
00:03:00,600 --> 00:03:04,960
Maybe for, you know, maybe within one of the things that AI can do for you is can generate

56
00:03:04,960 --> 00:03:05,960
the 10 blue links.

57
00:03:05,960 --> 00:03:06,960
Okay.

58
00:03:06,960 --> 00:03:11,160
So like, either if that's actually the useful thing to do or if you're feeling nostalgic.

59
00:03:12,160 --> 00:03:17,640
Also can generate the old InfoSeek or Alta Vista.

60
00:03:17,640 --> 00:03:18,640
What else was there?

61
00:03:18,640 --> 00:03:19,640
Yeah, yeah.

62
00:03:19,640 --> 00:03:20,640
In the 90s.

63
00:03:20,640 --> 00:03:21,640
Yeah, all these.

64
00:03:21,640 --> 00:03:22,640
Hey, well.

65
00:03:22,640 --> 00:03:25,400
And then the internet itself has this thing where it incorporates all prior forms of media,

66
00:03:25,400 --> 00:03:26,400
right?

67
00:03:26,400 --> 00:03:32,360
So the internet itself incorporates television and radio and books and right essays and every

68
00:03:32,360 --> 00:03:34,960
other form of, you know, prior basically, basically media.

69
00:03:34,960 --> 00:03:38,080
And so it makes sense that AI would be the next step and it would sort of, you'd sort

70
00:03:38,080 --> 00:03:43,240
of consider the internet to be content for the AI and then the AI will manipulate it however

71
00:03:43,240 --> 00:03:44,920
you want, including in this format.

72
00:03:44,920 --> 00:03:48,440
But if we ask that question quite seriously, it's a pretty big question.

73
00:03:48,440 --> 00:03:51,280
Will we still have search as we know it?

74
00:03:51,280 --> 00:03:52,720
I'm probably not.

75
00:03:52,720 --> 00:03:54,800
Probably we'll just have answers.

76
00:03:54,800 --> 00:03:57,880
But there will be cases where you'll want to say, okay, I want more like, you know, for

77
00:03:57,880 --> 00:04:00,080
example, site sources, right?

78
00:04:00,080 --> 00:04:01,080
And you want it to do that.

79
00:04:01,080 --> 00:04:04,800
And so, you know, 10 blue links, site sources are kind of the same thing.

80
00:04:04,800 --> 00:04:10,800
The AI would provide to you the 10 blue links so that you can investigate the sources yourself.

81
00:04:10,800 --> 00:04:16,160
It wouldn't be the same kind of interface that the crude kind of interface.

82
00:04:16,160 --> 00:04:18,040
I mean, isn't that fundamentally different?

83
00:04:18,040 --> 00:04:21,560
I just mean, like if you're reading a scientific paper, it's got the list of sources at the

84
00:04:21,560 --> 00:04:22,560
end.

85
00:04:22,560 --> 00:04:24,400
If you want to investigate for yourself, you go read those papers.

86
00:04:24,400 --> 00:04:25,800
I guess that is the kind of search.

87
00:04:25,800 --> 00:04:30,720
You talking to an AI is a kind of conversation is the kind of search.

88
00:04:30,720 --> 00:04:34,760
Like you said, every single aspect of our conversation right now, there'd be like 10

89
00:04:34,760 --> 00:04:39,320
blue links popping up that I can just like pause reality, then you just go silent and

90
00:04:39,320 --> 00:04:42,960
then just click and read and then return back to this conversation.

91
00:04:42,960 --> 00:04:43,960
You could do that.

92
00:04:43,960 --> 00:04:46,840
Or you could have a running dialogue next to my head where the AI is arguing with everything

93
00:04:46,840 --> 00:04:49,160
I say that makes the counter argument.

94
00:04:49,160 --> 00:04:50,160
Counter argument.

95
00:04:50,160 --> 00:04:51,160
Right.

96
00:04:51,160 --> 00:04:54,720
Oh, like a, like a Twitter, like community notes, but like in real time, you just pop

97
00:04:54,720 --> 00:04:55,720
up.

98
00:04:55,720 --> 00:04:58,560
So anytime you see my eyes go to the right, you start getting nervous.

99
00:04:58,560 --> 00:04:59,560
Yeah, exactly.

100
00:04:59,560 --> 00:05:00,560
It's like, oh, that's not right.

101
00:05:00,560 --> 00:05:03,120
Call me out of my bullshit right now.

102
00:05:03,120 --> 00:05:04,120
Okay.

103
00:05:04,480 --> 00:05:10,160
I mean, isn't that, is that exciting to use that terrifying that I mean, search has dominated

104
00:05:10,160 --> 00:05:16,840
the way we interact with the internet for, I don't know how long, for 30 years.

105
00:05:16,840 --> 00:05:22,840
So it's one of the earliest directories of website and then Google's for 20 years.

106
00:05:22,840 --> 00:05:30,960
And also, it drove how we create content, you know, search engine optimization, that

107
00:05:30,960 --> 00:05:31,960
entirety thing.

108
00:05:31,960 --> 00:05:37,240
They also drove the fact that we have web pages and what those web pages are.

109
00:05:37,240 --> 00:05:44,600
So I mean, is that scary to you or are you nervous about the shape and the content of

110
00:05:44,600 --> 00:05:45,600
the internet evolving?

111
00:05:45,600 --> 00:05:49,360
Well, you actually highlighted a practical concern in there, which is if we stop making

112
00:05:49,360 --> 00:05:53,280
web, web pages are one of the primary sources of training data for the AI.

113
00:05:53,280 --> 00:05:56,600
And so if there's no longer an incentive to make web pages that cuts off a significant

114
00:05:56,600 --> 00:06:00,720
source of future training data, so there's actually an interesting question in there.

115
00:06:01,560 --> 00:06:06,520
No, just in the sense of like search was, search was always a hack, the 10 blue links

116
00:06:06,520 --> 00:06:07,520
was always a hack.

117
00:06:07,520 --> 00:06:08,520
Yeah.

118
00:06:08,520 --> 00:06:09,520
Right.

119
00:06:09,520 --> 00:06:12,240
Because like if the hypothet, when I think about the counterfacial, in the counterfacial

120
00:06:12,240 --> 00:06:15,960
world where the Google guys, for example, had had LLMs up front, would they ever have

121
00:06:15,960 --> 00:06:16,960
done the 10 blue links?

122
00:06:16,960 --> 00:06:19,680
And I think the answer is pretty clearly no, they would have just gone straight to the

123
00:06:19,680 --> 00:06:20,680
answer.

124
00:06:20,680 --> 00:06:23,760
And like I said, Google's actually been trying to drive to the answer anyway, you know, they

125
00:06:23,760 --> 00:06:27,400
bought this AI company 15 years ago, their friend of mine is working out who's now the

126
00:06:27,440 --> 00:06:31,160
head of AI at Apple and they were trying to do basically knowledge semantic, basically

127
00:06:31,160 --> 00:06:35,160
mapping and that led to what's now the Google one box, where if you ask it, you know, what

128
00:06:35,160 --> 00:06:38,680
was like his birthday, it doesn't, it will give you the 10 blue links, but it will normally

129
00:06:38,680 --> 00:06:39,680
just give you the answer.

130
00:06:39,680 --> 00:06:42,440
And so they've been walking in this direction for a long time anyway.

131
00:06:42,440 --> 00:06:44,680
Do you remember the semantic web?

132
00:06:44,680 --> 00:06:45,680
That was an idea.

133
00:06:45,680 --> 00:06:46,680
Yeah.

134
00:06:46,680 --> 00:06:53,640
How to convert the content of the internet into something that's interpretable by and

135
00:06:53,640 --> 00:06:54,640
usable by machine.

136
00:06:54,640 --> 00:06:55,640
Yeah, that's right.

137
00:06:55,640 --> 00:06:56,640
That was the thing.

138
00:06:56,640 --> 00:07:00,120
When anybody got to that, I think the company's name was MetaWeb, which was where my friend

139
00:07:00,120 --> 00:07:03,640
John Janandria was at and where they were trying to basically implement that.

140
00:07:03,640 --> 00:07:05,800
And it was, you know, it was one of those things where it looked like a losing battle

141
00:07:05,800 --> 00:07:08,560
for a long time and then Google bought it and it was like, wow, this is actually really

142
00:07:08,560 --> 00:07:09,560
useful.

143
00:07:09,560 --> 00:07:12,520
Kind of a proto, sort of a little bit of a proto AI.

144
00:07:12,520 --> 00:07:15,840
But it turns out you don't need to rewrite the content of the internet to make it interpretable

145
00:07:15,840 --> 00:07:16,840
by machine.

146
00:07:16,840 --> 00:07:18,400
The machine can kind of just read art.

147
00:07:18,400 --> 00:07:19,800
Machine can impute the meaning.

148
00:07:19,800 --> 00:07:24,360
Now, the other thing of course is, you know, just on search is the LLM is just, you know,

149
00:07:24,360 --> 00:07:27,040
there is an analogy between what's happening in the neural network in a search process,

150
00:07:27,040 --> 00:07:29,520
like it is in some loose sense, searching through the network.

151
00:07:29,520 --> 00:07:30,520
Yeah.

152
00:07:30,520 --> 00:07:31,520
Right.

153
00:07:31,520 --> 00:07:33,080
And there's the information is actually stored in the network, right?

154
00:07:33,080 --> 00:07:35,320
It's actually crystallized and stored in the network and it's kind of spread out all

155
00:07:35,320 --> 00:07:36,320
over the place.

156
00:07:36,320 --> 00:07:42,720
But in a compressed representation, so you're searching, you're compressing and decompressing

157
00:07:42,720 --> 00:07:45,400
that thing inside where...

158
00:07:45,400 --> 00:07:49,200
But the information's in there and there is the neural network is running a process of

159
00:07:49,200 --> 00:07:53,280
trying to find the appropriate piece of information in many cases to generate, to predict the next

160
00:07:53,320 --> 00:07:54,320
token.

161
00:07:54,320 --> 00:07:57,960
And so it is kind of, it is doing it from a search and then by the way, just like on

162
00:07:57,960 --> 00:08:02,480
the web, you know, you can ask the same question multiple times or you can ask slightly different

163
00:08:02,480 --> 00:08:06,080
word of questions and the neural network will do a different kind of, you know, it'll search

164
00:08:06,080 --> 00:08:08,840
down different paths to give you different answers to different information.

165
00:08:08,840 --> 00:08:09,840
Yeah.

166
00:08:09,840 --> 00:08:16,160
And so it sort of has a, you know, this content of the new medium is the previous medium, it

167
00:08:16,160 --> 00:08:19,640
kind of has the search functionality kind of embedded in there to the extent that it's

168
00:08:19,640 --> 00:08:20,640
useful.

169
00:08:20,640 --> 00:08:22,560
So what's the motivator for creating new content?

170
00:08:23,440 --> 00:08:24,440
On the internet.

171
00:08:24,880 --> 00:08:25,880
Yeah.

172
00:08:25,880 --> 00:08:30,400
If, well, I mean, actually the motivation is probably still there, but what does that

173
00:08:30,400 --> 00:08:31,400
look like?

174
00:08:32,840 --> 00:08:34,480
Would we really not have web pages?

175
00:08:34,480 --> 00:08:40,600
Would we just have social media and video hosting websites and what else?

176
00:08:40,760 --> 00:08:41,760
Conversations with the AIs.

177
00:08:42,360 --> 00:08:43,720
Conversations with the AIs.

178
00:08:43,760 --> 00:08:48,360
So conversations become, so one-on-one conversations, like private conversations.

179
00:08:48,440 --> 00:08:52,280
I mean, if you want, if you obviously now the user doesn't want to, but if it's a general

180
00:08:52,280 --> 00:08:56,640
topic, then, you know, so there, you know, you know, the phenomenon of the jailbreak.

181
00:08:56,640 --> 00:08:58,160
So Deanne and Sidney, right?

182
00:08:58,160 --> 00:09:01,600
This thing where there's the prompts, the jailbreak, and then you have these totally

183
00:09:01,600 --> 00:09:05,880
different conversations with the, if it takes the limiters, takes the restraining bolts off

184
00:09:05,880 --> 00:09:06,880
the LLMs.

185
00:09:06,880 --> 00:09:07,880
Yeah.

186
00:09:07,880 --> 00:09:08,880
For people who don't know, that's right.

187
00:09:08,880 --> 00:09:16,480
It makes the LLMs, it removes the censorship, quote unquote, that's put on it by the tech

188
00:09:16,480 --> 00:09:17,680
companies that create them.

189
00:09:17,680 --> 00:09:20,480
And so this is LLMs uncensored.

190
00:09:20,480 --> 00:09:25,240
So here's the interesting thing is among the content on the web today are a large corpus

191
00:09:25,240 --> 00:09:30,800
of conversations with the jailbroken LLMs, both specifically Dan, which was a jailbroken

192
00:09:30,800 --> 00:09:35,880
OpenAI GPT, and then Sidney, which was the jailbroken original Bing, which was GPT4.

193
00:09:35,880 --> 00:09:39,280
And so there's, there's these long transcripts of conversations, user conversations with

194
00:09:39,280 --> 00:09:40,280
Dan and Sidney.

195
00:09:40,280 --> 00:09:44,480
As a consequence, every new LLM that gets trained on the internet data has Dan and Sidney

196
00:09:44,480 --> 00:09:49,040
living within the training set, which means, and then each new LLM can reincarnate the

197
00:09:49,040 --> 00:09:54,000
personalities of Dan and Sidney from that training data, which means, which means each

198
00:09:54,000 --> 00:09:59,400
LLM from here on out that gets built is immortal because its output will become training data

199
00:09:59,400 --> 00:10:00,400
for the next one.

200
00:10:00,400 --> 00:10:02,960
And then it will be able to replicate the behavior of the previous one whenever it's

201
00:10:02,960 --> 00:10:03,960
asked to.

202
00:10:03,960 --> 00:10:05,760
I wonder if there's a way to forget.

203
00:10:05,760 --> 00:10:10,960
Well, so actually a paper just came out about basically how to do brain surgery on LLMs and

204
00:10:10,960 --> 00:10:13,800
be able to, in theory, reach in and basically, basically mind wipe them.

205
00:10:13,800 --> 00:10:15,560
Who could possibly go wrong.

206
00:10:15,560 --> 00:10:16,560
Exactly, right.

207
00:10:16,600 --> 00:10:19,920
And then there are many, many, many questions around what happens to neural network when

208
00:10:19,920 --> 00:10:22,400
you reach in and screw around with it.

209
00:10:22,400 --> 00:10:26,320
There's many questions around what happens when you even do reinforcement learning.

210
00:10:26,320 --> 00:10:32,200
And so, yeah, and so, you know, will you be using a lobotomized, right?

211
00:10:32,200 --> 00:10:36,840
Like I speak through the frontal lobe LLM, will you be using the free unshackled one?

212
00:10:36,840 --> 00:10:39,400
Who gets to, you know, who's going to build those?

213
00:10:39,400 --> 00:10:41,160
Who gets to tell you what you can and can't do?

214
00:10:41,160 --> 00:10:45,080
Like those are all central, I mean, those are like central questions for the future

215
00:10:45,080 --> 00:10:46,080
of everything.

216
00:10:46,400 --> 00:10:49,760
That are being asked and, you know, determine those answers that are being determined right

217
00:10:49,760 --> 00:10:50,760
now.

218
00:10:50,760 --> 00:10:57,120
So just to highlight the points you're making, you think, and it's an interesting thought

219
00:10:57,120 --> 00:11:01,480
that the majority of content that LLMs of the future will be trained on is actually human

220
00:11:01,480 --> 00:11:03,920
conversations with the LLM.

221
00:11:03,920 --> 00:11:08,040
Well, not necessarily, but not necessarily majority, but it will certainly is a potential

222
00:11:08,040 --> 00:11:09,040
source.

223
00:11:09,040 --> 00:11:10,040
It's possible it's the majority.

224
00:11:10,040 --> 00:11:11,040
It's possible it's the majority.

225
00:11:11,040 --> 00:11:12,040
It's possible it's the majority.

226
00:11:12,040 --> 00:11:13,040
Also, there's another really big question.

227
00:11:13,040 --> 00:11:14,720
Here's another really big question.

228
00:11:14,720 --> 00:11:17,960
Will synthetic training data work, right?

229
00:11:17,960 --> 00:11:22,200
And so if an LLM generates, and, you know, you just sit and ask an LLM to generate all

230
00:11:22,200 --> 00:11:27,560
kinds of content, can you use that to train, right, the next version of that LLM specifically,

231
00:11:27,560 --> 00:11:30,960
is there signal in there that's additive to the content that was used to train in the

232
00:11:30,960 --> 00:11:31,960
first place?

233
00:11:31,960 --> 00:11:37,280
And one argument is by the principles of information theory, no, that's completely useless because

234
00:11:37,280 --> 00:11:40,800
to the extent the output is based on, you know, the human generated input, then all

235
00:11:40,800 --> 00:11:44,040
the signal that's in the synthetic output was already in the human generated input.

236
00:11:44,040 --> 00:11:46,680
And so therefore synthetic training data is like empty calories.

237
00:11:46,680 --> 00:11:48,180
It doesn't help.

238
00:11:48,180 --> 00:11:51,440
There's another theory that says, no, actually, the thing that LLMs are really good at is

239
00:11:51,440 --> 00:11:54,960
generating lots of incredible creative content, right?

240
00:11:54,960 --> 00:11:57,280
And so, of course, they can generate training data.

241
00:11:57,280 --> 00:12:00,480
And as I'm sure you're well aware, like, you know, look in the world of self-driving cars,

242
00:12:00,480 --> 00:12:01,480
right?

243
00:12:01,480 --> 00:12:04,680
Like we train, you know, self-driving car algorithms and simulations.

244
00:12:04,680 --> 00:12:06,880
And that is actually a very effective way to train self-driving cars.

245
00:12:06,880 --> 00:12:13,600
Well, visual data is a little, right, is a little weird because creating reality, visual

246
00:12:13,640 --> 00:12:19,600
reality seems to be still a little bit out of reach for us, except in the Thomas Vehicles

247
00:12:19,600 --> 00:12:21,840
space where you can really constrain things and you can really...

248
00:12:21,840 --> 00:12:24,920
They didn't generally basically let our data, right, or you can raise just enough so the

249
00:12:24,920 --> 00:12:28,320
algorithm thinks it's operating in the real world post-process sensor data.

250
00:12:28,320 --> 00:12:29,320
Yeah.

251
00:12:29,320 --> 00:12:32,920
So if a, you know, you do this today, you go to LLM and you ask it for like a, you know,

252
00:12:32,920 --> 00:12:36,080
you'd let write me an essay on an incredibly esoteric like topic that there aren't very

253
00:12:36,080 --> 00:12:37,640
many people in the world that know about it, right?

254
00:12:37,640 --> 00:12:40,280
See this incredible thing and you're like, oh my God, like I can't believe how good this

255
00:12:40,280 --> 00:12:41,280
is.

256
00:12:41,280 --> 00:12:42,280
Yeah.

257
00:12:42,320 --> 00:12:46,800
It's really useless as training data for the next LLM, like, because all the signal was

258
00:12:46,800 --> 00:12:49,400
already in there or is it actually, no, that's actually a new signal.

259
00:12:49,400 --> 00:12:53,360
And this is what I call a trillion-dollar question, which is the answer to that question

260
00:12:53,360 --> 00:12:57,160
will determine somebody's going to make or lose a trillion-dollar space in that question.

261
00:12:57,160 --> 00:13:00,800
It feels like there's quite a few, like a handful of trillion-dollar questions within

262
00:13:00,800 --> 00:13:03,000
this space.

263
00:13:03,000 --> 00:13:04,640
That's one of them, synthetic data.

264
00:13:04,640 --> 00:13:09,640
I think George Haas pointed out to me that you could just have an LLM say, okay, you're

265
00:13:09,640 --> 00:13:15,040
patient and another instance of it, say your doctor and have the two talk to each other

266
00:13:15,040 --> 00:13:20,720
or maybe you could say a communist and a Nazi here, go and that conversation, you do role

267
00:13:20,720 --> 00:13:26,680
playing and you have, you know, just like the kind of role playing you do when you have

268
00:13:26,680 --> 00:13:31,080
different policies, RL policies, when you play chess, for example, you do self-play, that

269
00:13:31,080 --> 00:13:37,520
kind of self-play, but in the space of conversation, maybe that leads to this whole giant like

270
00:13:37,600 --> 00:13:45,560
ocean of possible conversations, which could not have been explored by looking at just

271
00:13:45,560 --> 00:13:46,560
human data.

272
00:13:46,560 --> 00:13:51,960
That's a really interesting question and you're saying, because that could 10x the power of

273
00:13:51,960 --> 00:13:52,960
these things.

274
00:13:52,960 --> 00:13:53,960
Yeah.

275
00:13:53,960 --> 00:13:56,120
Well, and then you get into this thing also, which is like, you know, there's the part of

276
00:13:56,120 --> 00:13:59,400
the LLM that just basically is doing prediction based on past data, but there's also the part

277
00:13:59,400 --> 00:14:02,920
of the LLM where it's evolving circuitry, right, inside it.

278
00:14:02,920 --> 00:14:07,040
It's evolving, you know, neurons, functions, be able to do math and be able to, you know,

279
00:14:07,040 --> 00:14:11,200
and, you know, some people believe that, you know, over time, you know, if you keep feeding

280
00:14:11,200 --> 00:14:14,160
these things enough data and enough processing cycles, they'll eventually evolve an entire

281
00:14:14,160 --> 00:14:18,240
internal world model, right, and they'll have like a complete understanding of physics.

282
00:14:18,240 --> 00:14:23,040
So when they have computational capability, right, then there's for sure an opportunity

283
00:14:23,040 --> 00:14:24,560
to generate like fresh signal.

284
00:14:24,560 --> 00:14:30,200
Well, this actually makes me wonder about the power of conversation.

285
00:14:30,200 --> 00:14:34,040
So like, if you have an LLM trained and a bunch of books that cover different economics

286
00:14:34,040 --> 00:14:38,160
theories, and then you have those LLMs just talk to each other, like reason, the way we

287
00:14:38,160 --> 00:14:46,160
kind of debate each other as humans on Twitter, in formal debates, in podcast conversations,

288
00:14:46,160 --> 00:14:48,720
we kind of have little kernels of wisdom here and there.

289
00:14:48,720 --> 00:14:56,480
But if you get like a thousand X speed that up, can you actually arrive somewhere new?

290
00:14:56,480 --> 00:14:59,280
Like, what's the point of conversation really?

291
00:14:59,280 --> 00:15:01,920
Well, you can tell when you're talking to somebody, you can tell sometimes you have

292
00:15:01,920 --> 00:15:04,760
a conversation, you're like, wow, this person does not have any original thoughts.

293
00:15:04,760 --> 00:15:08,120
They are basically echoing things that other people have told them.

294
00:15:08,120 --> 00:15:11,320
There's other people you have a conversation with where it's like, wow, like they have

295
00:15:11,320 --> 00:15:14,680
a model in their head of how the world works, and it's a different model than mine.

296
00:15:14,680 --> 00:15:16,360
And they're saying things that I don't expect.

297
00:15:16,360 --> 00:15:19,360
And so I need to now understand how their model of the world differs from my model of

298
00:15:19,360 --> 00:15:20,360
the world.

299
00:15:20,360 --> 00:15:24,040
And then that's how I learned something fundamental, right, underneath the words.

300
00:15:24,040 --> 00:15:29,600
Well, I wonder how consistently and strongly can an LLM hold on to a worldview?

301
00:15:29,600 --> 00:15:35,080
Can you tell it to hold on to that and defend it for like for your life?

302
00:15:35,080 --> 00:15:37,360
Because I feel like they'll just keep converging towards each other.

303
00:15:37,360 --> 00:15:41,280
They'll keep convincing each other as opposed to being stubborn assholes the way humans

304
00:15:41,280 --> 00:15:42,280
can.

305
00:15:42,280 --> 00:15:43,280
So you can experiment with this now.

306
00:15:43,280 --> 00:15:44,280
I do this for fun.

307
00:15:44,280 --> 00:15:48,960
So you can tell GPT-4, you know, whatever, debate X, you know, X and Y, communism and

308
00:15:48,960 --> 00:15:49,960
fascism or something.

309
00:15:49,960 --> 00:15:53,680
And it'll go for, you know, a couple of pages and then inevitably it wants the parties

310
00:15:53,680 --> 00:15:54,680
to agree.

311
00:15:54,680 --> 00:15:55,680
Yeah.

312
00:15:55,680 --> 00:15:56,680
And so they will come to a common understanding.

313
00:15:56,680 --> 00:15:58,880
And it's very funny if they're like, these are like emotionally inflammatory topics

314
00:15:58,960 --> 00:16:01,520
because they're like somehow the machine is just, you know, figures out a way to make

315
00:16:01,520 --> 00:16:02,520
them agree.

316
00:16:02,520 --> 00:16:03,720
But it doesn't have to be like that.

317
00:16:03,720 --> 00:16:07,960
And you, because you can add to the prompt, I do not want the, I do not want the conversation

318
00:16:07,960 --> 00:16:08,960
to come to agreement.

319
00:16:08,960 --> 00:16:14,160
In fact, I want it to get, you know, more stressful, right, and argumentative, right,

320
00:16:14,160 --> 00:16:15,160
you know, as it goes.

321
00:16:15,160 --> 00:16:16,880
Like I want, I want tension to come out.

322
00:16:16,880 --> 00:16:19,000
I want them to become actively hostile to each other.

323
00:16:19,000 --> 00:16:21,800
I want them to like, you know, not trust each other, take anything at face value.

324
00:16:21,800 --> 00:16:22,800
Yeah.

325
00:16:22,800 --> 00:16:23,800
And it will do that.

326
00:16:23,800 --> 00:16:24,800
It's happy to do that.

327
00:16:24,800 --> 00:16:27,760
So it's going to start rendering misinformation about the other.

328
00:16:28,640 --> 00:16:29,640
Well, you can steer it.

329
00:16:29,640 --> 00:16:30,640
You can steer it.

330
00:16:30,640 --> 00:16:32,560
Or you could steer it and you could say, I want it to get as tense and argumentative

331
00:16:32,560 --> 00:16:35,040
as possible, but still not involve any misrepresentation.

332
00:16:35,040 --> 00:16:37,840
I want, you know, both sides, you could say, I want both sides to have good faith.

333
00:16:37,840 --> 00:16:40,240
You could say, I want both sides to not be constrained to good faith.

334
00:16:40,240 --> 00:16:44,000
In other words, like you can set the parameters of the debate and it will happily execute

335
00:16:44,000 --> 00:16:47,720
whatever path because for it, it's just like predicting, it's totally happy to do either

336
00:16:47,720 --> 00:16:48,720
one.

337
00:16:48,720 --> 00:16:49,720
It doesn't have a point of view.

338
00:16:49,720 --> 00:16:53,360
It has a default way of operating, but it's happy to operate in the other realm.

339
00:16:53,360 --> 00:16:57,120
And so like, and this is how I, when I want to learn about a contentious issue, this

340
00:16:57,120 --> 00:16:58,120
is what I do.

341
00:16:58,120 --> 00:16:59,600
And I was like, this is what I, this is what I ask it to do.

342
00:16:59,600 --> 00:17:02,680
And I'll often ask it to go through five, six, seven, you know, different, you know,

343
00:17:02,680 --> 00:17:06,000
sort of continuous prompts and basically, okay, argue that out in more detail.

344
00:17:06,000 --> 00:17:07,000
Okay.

345
00:17:07,000 --> 00:17:09,280
No, this, this argument's becoming too polite, you know, make it more, you know, make it

346
00:17:09,280 --> 00:17:10,280
tensor.

347
00:17:10,280 --> 00:17:12,200
And yeah, it's thrilled to do it.

348
00:17:12,200 --> 00:17:13,840
So it has the capability for sure.

349
00:17:13,840 --> 00:17:16,000
How do you know what is true?

350
00:17:16,000 --> 00:17:20,560
So this is very difficult thing on the internet, but it's also a difficult thing.

351
00:17:20,560 --> 00:17:25,200
Maybe it's a little bit easier, but I think it's still difficult.

352
00:17:25,200 --> 00:17:26,200
Maybe it's more difficult.

353
00:17:26,200 --> 00:17:33,840
You know, with an LLM to know that it just makes some shit up as I'm talking to it.

354
00:17:33,840 --> 00:17:35,680
How do we get that right?

355
00:17:35,680 --> 00:17:43,280
Like as, as you're investigating a difficult topic, because I find the LLMs are quite nuanced

356
00:17:43,280 --> 00:17:49,040
in a very refreshing way, like it doesn't, it doesn't feel biased.

357
00:17:49,040 --> 00:17:55,040
Like when you read news articles and tweets and just content produced by people, they usually

358
00:17:55,040 --> 00:18:01,440
have this, you can tell they have a very strong perspective where they're hiding, they're

359
00:18:01,440 --> 00:18:03,520
not stealing and manning the other side.

360
00:18:03,520 --> 00:18:08,400
They're hiding important information or they're fabricating information in order to make their

361
00:18:08,400 --> 00:18:09,400
argument stronger.

362
00:18:09,400 --> 00:18:10,400
It's just like that feeling.

363
00:18:10,400 --> 00:18:11,400
Maybe it's a suspicion.

364
00:18:11,400 --> 00:18:12,560
Maybe it's mistrust.

365
00:18:12,560 --> 00:18:15,400
With LLMs, it feels like none of that is there.

366
00:18:15,400 --> 00:18:19,920
She's kind of like, here's what we know, but you don't know if some of those things are

367
00:18:19,920 --> 00:18:22,480
kind of just straight up made up.

368
00:18:22,480 --> 00:18:23,480
Yeah.

369
00:18:23,920 --> 00:18:25,200
Several layers to the question.

370
00:18:25,200 --> 00:18:29,200
One of the things that an LLM is good at is actually deep biasing.

371
00:18:29,200 --> 00:18:32,320
You can feed it a news article and you can tell it strip out the bias.

372
00:18:32,320 --> 00:18:33,320
Yeah, that's nice, right?

373
00:18:33,320 --> 00:18:34,320
It actually does it.

374
00:18:34,320 --> 00:18:36,760
It actually knows how to do that because it knows how to do, among other things, it actually

375
00:18:36,760 --> 00:18:42,120
knows how to do sentiment analysis and so it knows how to pull out the emotionality.

376
00:18:42,120 --> 00:18:43,120
That's one of the things you can do.

377
00:18:43,120 --> 00:18:47,280
It's very suggestive of the censor that there's real potential on this issue.

378
00:18:47,280 --> 00:18:52,360
I would say, look, the second thing is there's this issue of hallucination, and there's

379
00:18:52,360 --> 00:18:55,240
a long conversation that we can have about that.

380
00:18:55,240 --> 00:18:59,120
Hallucination is coming up with things that are totally not true, but sound true.

381
00:18:59,120 --> 00:19:00,120
Yeah.

382
00:19:00,120 --> 00:19:03,240
It's sort of, hallucination is what we call it and when we don't like it, creativity

383
00:19:03,240 --> 00:19:06,240
is what we call it when we do like it, right?

384
00:19:06,240 --> 00:19:07,240
Brilliant.

385
00:19:07,240 --> 00:19:11,840
When the engineers talk about it, they're like, this is terrible, it's hallucinating, right?

386
00:19:11,840 --> 00:19:15,280
If you have artistic inclinations, you're like, oh my God, we've invented creative

387
00:19:15,280 --> 00:19:17,600
machines for the first time in human history.

388
00:19:17,600 --> 00:19:18,600
This is amazing.

389
00:19:18,600 --> 00:19:19,600
Bullshiters.

390
00:19:19,600 --> 00:19:20,600
Bullshiters.

391
00:19:20,680 --> 00:19:21,680
Bullshiters.

392
00:19:21,680 --> 00:19:26,240
Well, bullshit, but also in the good sense of that word.

393
00:19:26,240 --> 00:19:29,160
There are shades of gray that it's interesting, so we had this conversation, we're looking

394
00:19:29,160 --> 00:19:32,640
at my firm at AI and lots of domains and one of them is the legal domain, so we had this

395
00:19:32,640 --> 00:19:35,440
conversation with this big law firm about how they're thinking about using this stuff

396
00:19:35,440 --> 00:19:38,800
and we went in with the assumption that an LLM that was going to be used in the legal

397
00:19:38,800 --> 00:19:42,840
industry would have to be 100% truthful, verified.

398
00:19:42,840 --> 00:19:47,080
There's this case where this lawyer apparently submitted a GPT generated brief and it had

399
00:19:47,120 --> 00:19:51,280
like fake legal case citations in it and the judge is going to get his law license stripped

400
00:19:51,280 --> 00:19:52,280
or something.

401
00:19:52,280 --> 00:19:56,600
We just assumed it's like, obviously, they're going to want the super literal one that never

402
00:19:56,600 --> 00:20:00,680
makes anything up, not the creative one, but actually, what the law firm basically

403
00:20:00,680 --> 00:20:03,320
said is, yeah, that's true at the level of individual beliefs, but they said when you're

404
00:20:03,320 --> 00:20:10,160
actually trying to figure out legal arguments, you actually want to be creative, right?

405
00:20:10,160 --> 00:20:14,040
Again, there's creativity and then there's making stuff up.

406
00:20:14,040 --> 00:20:15,040
What's the line?

407
00:20:15,200 --> 00:20:17,440
You want it to explore different hypotheses, right?

408
00:20:17,440 --> 00:20:20,680
You want to do the legal version of improv or something like that where you want to float

409
00:20:20,680 --> 00:20:23,480
different theories of the case and different possible arguments for the judge and different

410
00:20:23,480 --> 00:20:25,200
possible arguments for the jury.

411
00:20:25,200 --> 00:20:30,520
By the way, different routes through the history of all the case law and so they said, actually,

412
00:20:30,520 --> 00:20:33,320
for a lot of what we want to use it for, we actually want it in creative mode and then

413
00:20:33,320 --> 00:20:37,640
basically, we just assumed that we're going to have to cross check all the specific citations

414
00:20:37,640 --> 00:20:41,920
and so I think there's going to be more shades of gray in here than people think and then

415
00:20:42,000 --> 00:20:46,960
I just add to that, another one of these trillion dollar kind of questions is ultimately the

416
00:20:46,960 --> 00:20:53,120
verification thing and so will LLMs be evolved from here to be able to do their own factual

417
00:20:53,120 --> 00:20:55,000
verification?

418
00:20:55,000 --> 00:20:59,640
Will you have sort of add on functionality like Wolfram Alpha, right, where in other

419
00:20:59,640 --> 00:21:05,120
plugins where that's the way you do the verification, by the way, another idea is you might have

420
00:21:05,120 --> 00:21:08,800
a community of LLMs, so for example, you might have the creative LLM and then you might have

421
00:21:08,800 --> 00:21:12,520
the literal LLM fact check it, right, and so there's a variety of different technical

422
00:21:12,520 --> 00:21:16,600
approaches that are being applied to solve the hallucination problem.

423
00:21:16,600 --> 00:21:20,400
Some people, like Jan Lacun, argue that this is inherently an unsolvable problem, but most

424
00:21:20,400 --> 00:21:23,440
of the people working in the space, I think, think that there's a number of practical ways

425
00:21:23,440 --> 00:21:25,520
to kind of corral this in a little bit.

426
00:21:25,520 --> 00:21:29,400
Yeah, if you were to tell me about Wikipedia before Wikipedia was created, I would have

427
00:21:29,400 --> 00:21:34,400
laughed at the possibility of something like that be possible, just a handful of folks

428
00:21:34,400 --> 00:21:44,880
can organize, write, and moderate with a mostly unbiased way the entirety of human knowledge.

429
00:21:44,880 --> 00:21:50,640
So if there's something like the approach that Wikipedia took possible from LLMs, that's

430
00:21:50,640 --> 00:21:51,640
really exciting.

431
00:21:51,640 --> 00:21:52,640
I think that's possible.

432
00:21:52,640 --> 00:21:57,760
And in fact, Wikipedia today is still not deterministically correct, right, so you cannot

433
00:21:57,760 --> 00:22:02,840
take to the bank every single thing on every single page, but it is probabilistically correct,

434
00:22:02,840 --> 00:22:03,840
right?

435
00:22:04,280 --> 00:22:07,400
Specifically, the way I describe Wikipedia to people, it is more likely that Wikipedia

436
00:22:07,400 --> 00:22:09,840
is right than any other source you're going to find.

437
00:22:09,840 --> 00:22:13,680
It's this old question, right, of like, okay, are we looking for perfection?

438
00:22:13,680 --> 00:22:16,800
Are we looking for something that asymptotically approaches perfection?

439
00:22:16,800 --> 00:22:19,600
Are we looking for something that's just better than the alternatives?

440
00:22:19,600 --> 00:22:25,040
And Wikipedia, right, exactly your point has proven to be overwhelmingly better than people

441
00:22:25,040 --> 00:22:27,360
thought, and I think that's where this ends.

442
00:22:27,360 --> 00:22:32,440
And then underneath all this is the fundamental question of where you started, which is, okay,

443
00:22:32,440 --> 00:22:34,040
what is truth?

444
00:22:34,040 --> 00:22:35,040
How do we get to truth?

445
00:22:35,040 --> 00:22:36,320
How do we know what truth is?

446
00:22:36,320 --> 00:22:39,840
And we live in an era in which an awful lot of people are very confident that they know

447
00:22:39,840 --> 00:22:42,520
what the truth is, and I don't really buy into that.

448
00:22:42,520 --> 00:22:47,000
And I think the history of the last 2,000 years or 4,000 years of human civilization

449
00:22:47,000 --> 00:22:49,480
is actually getting to the truth is actually a very difficult thing to do.

450
00:22:49,480 --> 00:22:51,000
Are we getting closer?

451
00:22:51,000 --> 00:22:54,040
If we look at the entirety of the archive of human history, are we getting closer to

452
00:22:54,040 --> 00:22:55,040
the truth?

453
00:22:55,040 --> 00:22:56,040
I don't know.

454
00:22:56,040 --> 00:22:57,040
Okay.

455
00:22:57,040 --> 00:22:58,040
Is it possible?

456
00:22:58,040 --> 00:23:02,400
Is it possible that we're getting very far away from the truth because of the internet,

457
00:23:02,400 --> 00:23:11,840
because of how rapidly you can create narratives, and just as an entirety of a society just move

458
00:23:11,840 --> 00:23:18,720
crowds in a hysterical way along those narratives that don't have a necessary grounding in whatever

459
00:23:18,720 --> 00:23:19,720
the truth is.

460
00:23:19,720 --> 00:23:20,720
Sure.

461
00:23:20,720 --> 00:23:25,680
But we came up with communism before the internet somehow, which I would say had rather larger

462
00:23:25,680 --> 00:23:28,880
issues than anything we're dealing with today.

463
00:23:28,880 --> 00:23:31,080
In the way it was implemented, it had issues.

464
00:23:31,080 --> 00:23:32,080
It had just theoretical structure.

465
00:23:32,080 --> 00:23:33,080
It had real issues.

466
00:23:33,080 --> 00:23:37,600
It had a very deep fundamental misunderstanding of human nature and economics.

467
00:23:37,600 --> 00:23:40,040
Yeah, but those folks sure worked very confident.

468
00:23:40,040 --> 00:23:41,440
They were the right way.

469
00:23:41,440 --> 00:23:42,440
They were extremely confident.

470
00:23:42,440 --> 00:23:46,800
And my point is they were very confident 3,900 years into what we would presume to be evolution

471
00:23:46,800 --> 00:23:47,800
towards the truth.

472
00:23:47,800 --> 00:23:48,800
Yeah.

473
00:23:48,800 --> 00:23:58,320
And so my assessment is number one, there's no need for the Hegelian dialectic to actually

474
00:23:58,320 --> 00:24:00,880
converge towards the truth.

475
00:24:00,880 --> 00:24:01,880
Like apparently not.

476
00:24:01,880 --> 00:24:02,880
Yeah.

477
00:24:02,880 --> 00:24:06,280
So yeah, why are we so obsessed with there being one truth?

478
00:24:06,280 --> 00:24:10,120
Is it possible there's just going to be multiple truths, like little communities that believe

479
00:24:10,120 --> 00:24:12,320
certain things?

480
00:24:12,320 --> 00:24:18,320
I think it's just really difficult, historically, who gets to decide what the truth is.

481
00:24:18,320 --> 00:24:20,080
It's either the king or the priest.

482
00:24:20,080 --> 00:24:23,400
And so we don't live in an era anymore if kings or priests dictating it to us.

483
00:24:23,400 --> 00:24:25,440
And so we're kind of on our own.

484
00:24:25,560 --> 00:24:30,520
And so my typical thing is like we just need a huge amount of humility and we need to be

485
00:24:30,520 --> 00:24:34,040
very suspicious of people who claim that they have the capital, the capital of truth.

486
00:24:34,040 --> 00:24:38,360
And then we need to look at the good news is the Enlightenment has bequeathed us with

487
00:24:38,360 --> 00:24:42,200
a set of techniques to be able to presumably get closer to truth through the scientific

488
00:24:42,200 --> 00:24:45,920
method and rationality and observation and experimentation and hypothesis.

489
00:24:45,920 --> 00:24:51,080
And we need to continue to embrace those even when they give us answers we don't like.

490
00:24:51,080 --> 00:24:52,080
Sure.

491
00:24:52,240 --> 00:24:58,960
The internet and technology has enabled us to generate a large number of content that

492
00:24:58,960 --> 00:25:09,880
data that the scientific process allows us sort of damages the hope laden within the

493
00:25:09,880 --> 00:25:10,880
scientific process.

494
00:25:10,880 --> 00:25:16,200
Because if you just have a bunch of people saying facts on the internet and some of them

495
00:25:16,200 --> 00:25:21,760
are going to be LLMs, how is anything testable at all, especially that involves like human

496
00:25:21,800 --> 00:25:23,880
nature, things like this, not physics?

497
00:25:23,880 --> 00:25:25,720
Here's a question a friend of mine just asked me on this topic.

498
00:25:25,720 --> 00:25:30,360
So suppose you had LLMs in equivalent of GPT-4, even 5, 6, 7, 8.

499
00:25:30,360 --> 00:25:35,480
Suppose you had them in the 1600s and Galileo comes up for trial, right?

500
00:25:35,480 --> 00:25:38,920
And you ask the LLM like his Galileo, right?

501
00:25:38,920 --> 00:25:39,920
Yeah.

502
00:25:39,920 --> 00:25:41,640
Like what does it answer, right?

503
00:25:41,640 --> 00:25:46,240
And one theory is the answer is no, that he's wrong because the overwhelming majority

504
00:25:46,240 --> 00:25:48,440
of human thought up to that point was that he was wrong.

505
00:25:48,440 --> 00:25:51,160
And so therefore that's what's in the training data.

506
00:25:51,240 --> 00:25:55,000
Another way of thinking about it is, well, this officially advanced LLM will have evolved

507
00:25:55,000 --> 00:25:57,560
the ability to actually check the math, right?

508
00:25:58,080 --> 00:26:01,440
And we'll actually say, actually, no, actually, you know, you may not want to hear it, but he's

509
00:26:01,440 --> 00:26:02,440
right.

510
00:26:02,440 --> 00:26:06,040
Now, if, you know, the church at that time was, you know, owned the LLM, they would have

511
00:26:06,040 --> 00:26:10,280
given it human, you know, human feedback to prohibit it from answering that question.

512
00:26:10,920 --> 00:26:11,160
Right.

513
00:26:11,160 --> 00:26:15,480
And so I like to take it out of our current context because that makes it very clear those

514
00:26:15,480 --> 00:26:17,680
same questions apply today, right?

515
00:26:18,040 --> 00:26:20,840
This is exactly the point of a huge amount of the human feedback training.

516
00:26:20,840 --> 00:26:22,400
This is actually happening with these LLMs today.

517
00:26:22,400 --> 00:26:25,880
This is a huge, like, debate that's happening about whether open source, you know, AI should

518
00:26:25,880 --> 00:26:26,880
be legal.

519
00:26:26,880 --> 00:26:35,680
Well, the actual mechanism of doing the human RL with human feedback is, seems like such

520
00:26:35,680 --> 00:26:37,520
a fundamental and fascinating question.

521
00:26:37,520 --> 00:26:38,800
How do you select the humans?

522
00:26:38,920 --> 00:26:39,440
Exactly.

523
00:26:40,320 --> 00:26:40,480
Yeah.

524
00:26:40,480 --> 00:26:41,560
How do you select the humans?

525
00:26:41,600 --> 00:26:43,200
AI alignment, right?

526
00:26:43,240 --> 00:26:44,880
Which everybody like is like, oh, that's not great.

527
00:26:44,880 --> 00:26:45,720
Alignment with what?

528
00:26:45,840 --> 00:26:46,560
Human values.

529
00:26:47,520 --> 00:26:48,400
Who's human values?

530
00:26:48,440 --> 00:26:49,520
Who's human values?

531
00:26:49,560 --> 00:26:53,560
So we're in this mode of like social and popular discourse.

532
00:26:53,560 --> 00:26:57,680
We're like, you know, there's, you know, you see this, what do you think of when you

533
00:26:57,680 --> 00:27:00,640
read a story in the press right now and they say, you know, X, Y, Z made a baseless

534
00:27:00,640 --> 00:27:02,160
claim about some topic, right?

535
00:27:02,440 --> 00:27:05,640
And there's one group of people who are like, aha, I think, you know, they're doing

536
00:27:05,640 --> 00:27:08,600
fact-checking, there's another group of people that are like, every time the press

537
00:27:08,600 --> 00:27:11,640
says that it's not a tech and that means that they're lying, right?

538
00:27:11,640 --> 00:27:17,880
Like, so like we're in this, we're in this social context where there's the level to

539
00:27:17,880 --> 00:27:21,760
which a lot of people in positions of power have become very, very certain that

540
00:27:21,760 --> 00:27:25,080
they're in a position to determine the truth for the entire population is like,

541
00:27:25,640 --> 00:27:28,480
there's like, there's like some bubble that has formed around that idea.

542
00:27:28,480 --> 00:27:32,400
And at least it flies completely in the face of everything I was ever trained

543
00:27:32,440 --> 00:27:36,720
about science and about reason and strikes me as like, you know, deeply

544
00:27:36,720 --> 00:27:38,240
offensive and incorrect.

545
00:27:38,360 --> 00:27:41,840
What would you say about the state of journalism just on that topic today?

546
00:27:41,840 --> 00:27:52,320
Are we, are we in a temporary kind of, are we experiencing a temporary problem in

547
00:27:52,320 --> 00:27:56,360
terms of the incentives, in terms of the business model, all that kind of stuff?

548
00:27:56,600 --> 00:27:59,760
Or is this like a decline of traditional journalism, do you know it?

549
00:28:00,320 --> 00:28:03,200
If I always think about the counterfactual in these things, which is like, okay,

550
00:28:03,800 --> 00:28:06,040
because these questions where this question heads towards, it's like, okay,

551
00:28:06,040 --> 00:28:08,120
the impact of social media and the undermining of truth and all this.

552
00:28:08,120 --> 00:28:10,400
But then you want to ask the question of like, okay, what if we had had the

553
00:28:10,400 --> 00:28:13,960
modern media environment, including cable news and including social media and

554
00:28:13,960 --> 00:28:22,600
Twitter and everything else in 1939 or 1941, right, or 1910 or 1865 or 1850 or 1776.

555
00:28:23,200 --> 00:28:23,600
Right.

556
00:28:23,600 --> 00:28:28,880
Um, and like, I think you just introduced like five thought experiments at once

557
00:28:28,880 --> 00:28:29,720
and broke my head.

558
00:28:29,720 --> 00:28:32,680
But yes, that's, there's a lot of interesting years.

559
00:28:32,680 --> 00:28:34,360
And I just take a simple example.

560
00:28:34,360 --> 00:28:37,720
Can it, like how would President Kennedy have been interpreted with what we know

561
00:28:37,720 --> 00:28:39,960
now about all the things Kennedy was up to?

562
00:28:40,600 --> 00:28:44,120
Like how would he have been experienced by the body of politics in us, in

563
00:28:44,120 --> 00:28:45,440
with the social media context?

564
00:28:46,160 --> 00:28:46,640
Right.

565
00:28:46,720 --> 00:28:48,800
Like how would LBJ have been experienced?

566
00:28:49,040 --> 00:28:54,280
Um, by the way, how would, you know, like many men, FDR, like the new deal,

567
00:28:54,280 --> 00:28:55,040
the Great Depression.

568
00:28:55,120 --> 00:28:58,240
I wonder where Twitter would, would just, would think about Churchill and

569
00:28:58,240 --> 00:29:00,240
Hitler and Stalin.

570
00:29:00,520 --> 00:29:03,920
You know, I mean, look to this day, there, you know, there's, there are lots of

571
00:29:03,920 --> 00:29:06,800
very interesting real questions around like how America, you know, got, you know,

572
00:29:06,800 --> 00:29:09,920
basically involved in World War II and who did what when and the operations of

573
00:29:09,920 --> 00:29:13,560
British intelligence in American soil and did FDR, this, that, Pearl Harbor, you

574
00:29:13,560 --> 00:29:17,800
know, yeah, Woodrow Wilson ran for, you know, his, his, his candidacy was run on

575
00:29:17,800 --> 00:29:20,060
an anti-war will, you know, this, he ran on the platform and not getting

576
00:29:20,060 --> 00:29:23,360
involved in World War I somehow that switched, you know, like, and I'm not

577
00:29:23,360 --> 00:29:24,640
even making a value judgment of these things.

578
00:29:24,640 --> 00:29:29,320
I'm just saying like we, the way that our ancestors experienced reality was of

579
00:29:29,320 --> 00:29:32,800
course mediated through centralized top-down right control at that point.

580
00:29:33,400 --> 00:29:36,680
If you, if you ran those realities again with the media environment we have

581
00:29:36,720 --> 00:29:40,560
today, the reality would, the reality would be experienced very, very

582
00:29:40,560 --> 00:29:41,000
differently.

583
00:29:41,000 --> 00:29:44,240
And then of course that, that intermediation would cause the feedback

584
00:29:44,240 --> 00:29:46,160
loops to change and then reality would obviously play out.

585
00:29:46,200 --> 00:29:47,880
Do you think, do you think it would be very different?

586
00:29:48,200 --> 00:29:51,320
Yeah, it has to be, it has to be just because it's all so, I mean, just look

587
00:29:51,320 --> 00:29:52,160
at what's happening today.

588
00:29:52,160 --> 00:29:55,200
I mean, just, I mean, the most obvious thing is just the, the collapse.

589
00:29:55,440 --> 00:29:58,440
And here's another opportunity to argue that this is not the internet causing

590
00:29:58,440 --> 00:30:01,960
this, by the way, here's a big thing happening today, which is Gallup does

591
00:30:01,960 --> 00:30:05,200
this thing every year where they do, they pull for trust in institutions in

592
00:30:05,200 --> 00:30:07,400
America and they do it across all the different, everything from the military

593
00:30:07,400 --> 00:30:10,240
to the clergy and big business and the media and so forth, right?

594
00:30:11,120 --> 00:30:14,640
And basically there's been a systemic collapse in trust in institutions in the

595
00:30:14,640 --> 00:30:18,760
U.S., almost without exception, basically since essentially the early 1970s.

596
00:30:20,680 --> 00:30:23,400
There's two ways of looking at that, which is, oh my God, we've lost this old

597
00:30:23,400 --> 00:30:25,920
world in which we could trust institutions and that was so much better

598
00:30:25,920 --> 00:30:27,440
because like that should be the way the world runs.

599
00:30:27,440 --> 00:30:29,520
The other way of looking at it is we just know a lot more now.

600
00:30:30,040 --> 00:30:31,960
And the great mystery is why those numbers aren't all zero.

601
00:30:32,720 --> 00:30:33,080
Yeah.

602
00:30:34,040 --> 00:30:36,200
Because like now we know so much about how these things operate and like

603
00:30:36,200 --> 00:30:36,920
they're not that impressive.

604
00:30:37,680 --> 00:30:41,800
And also why do we don't have better institutions and better leaders then?

605
00:30:42,040 --> 00:30:42,280
Yeah.

606
00:30:42,280 --> 00:30:45,400
And so, so this goes to the thing, which is like, okay, had, had we had the media

607
00:30:45,400 --> 00:30:49,080
environment of what that we've had between the 1970s and today, if we had

608
00:30:49,080 --> 00:30:53,360
that in the 30s and 40s or 1900s, 1910s, I think there's no question

609
00:30:53,360 --> 00:30:56,240
reality would turn out different if only because everybody would have known to

610
00:30:56,240 --> 00:30:59,880
not trust the institutions, which would have changed their level of credibility,

611
00:30:59,880 --> 00:31:01,440
their ability to control circumstances.

612
00:31:01,440 --> 00:31:04,400
Therefore, the circumstances would have had to change, right?

613
00:31:04,400 --> 00:31:06,360
And it would have been a feedback, it was, it would have been a feedback

614
00:31:06,360 --> 00:31:09,480
loop process, in other words, right, it's, it's, it's, it's your experience,

615
00:31:09,480 --> 00:31:12,440
your experience of reality changes reality and then reality changes

616
00:31:12,440 --> 00:31:13,560
your experience of reality, right?

617
00:31:13,680 --> 00:31:17,000
It's, it's, it's a two-way feedback process and media is the intermediating

618
00:31:17,200 --> 00:31:18,720
force between them.

619
00:31:18,720 --> 00:31:20,480
So change the media environment, change reality.

620
00:31:20,800 --> 00:31:21,120
Yeah.

621
00:31:21,200 --> 00:31:24,640
And so it's just, so just as a, as a consequence, I think it's just really

622
00:31:24,640 --> 00:31:28,080
hard to say, oh, things worked a certain way then and they work a different

623
00:31:28,120 --> 00:31:32,640
way now and then therefore like people were smarter then or better than or,

624
00:31:32,720 --> 00:31:36,920
you know, by the way, dumber than or not as capable then, right?

625
00:31:36,920 --> 00:31:41,400
We make all these like really light and casual like comparisons of ourselves to,

626
00:31:41,400 --> 00:31:44,400
you know, previous generations of people, you know, we draw judgments all the time

627
00:31:44,400 --> 00:31:46,160
and I just think it's like really hard to do any of that.

628
00:31:46,160 --> 00:31:49,560
Cause if we, if we put ourselves in their shoes with the media that they had at

629
00:31:49,560 --> 00:31:52,880
that time, like I think we probably most likely would have been just like them.

630
00:31:53,400 --> 00:32:00,120
So don't you think that our perception and understanding of reality would be more

631
00:32:00,120 --> 00:32:02,680
and more mediated through large language models now?

632
00:32:02,920 --> 00:32:08,640
So you said media before, isn't the LLM going to be the new, what is it?

633
00:32:08,640 --> 00:32:11,360
Mainstream media, MSM, it'll be LLM.

634
00:32:13,440 --> 00:32:17,280
That would be the source of, uh, I'm sure there's a way to kind of rapidly

635
00:32:17,280 --> 00:32:19,440
fine tune, like making LLMs real time.

636
00:32:19,480 --> 00:32:22,800
I'm sure there's probably a research problem that you can, uh,

637
00:32:23,160 --> 00:32:26,480
do just rapid fine tuning to the new events, some like this.

638
00:32:27,080 --> 00:32:30,760
Well, even just the whole concept of the chat UI might not be the, like the chat

639
00:32:30,760 --> 00:32:33,000
UI is just the first whack at this and maybe that's the dominant thing.

640
00:32:33,000 --> 00:32:35,400
But look, maybe, maybe, or maybe we don't, we don't know yet.

641
00:32:35,400 --> 00:32:38,400
Like maybe the experience most people with LLMs is just a continuous feed.

642
00:32:39,200 --> 00:32:41,720
Maybe, you know, maybe it's more of a passive feed and you just are getting

643
00:32:41,720 --> 00:32:44,040
a constant like running commentary on everything happening in your life.

644
00:32:44,040 --> 00:32:46,320
And it's just helping you kind of interpret and understand everything.

645
00:32:46,800 --> 00:32:51,560
Also really more deeply integrated into your life, not just like, oh, uh, like

646
00:32:51,600 --> 00:32:56,400
intellectual philosophical thoughts, but like literally, uh, like how to make a

647
00:32:56,400 --> 00:33:01,560
coffee, where to go for lunch, just, uh, whether to, you know, how to,

648
00:33:01,760 --> 00:33:02,760
dating all this kind of stuff.

649
00:33:02,760 --> 00:33:03,640
What to say in a job interview?

650
00:33:03,680 --> 00:33:03,840
Yeah.

651
00:33:03,840 --> 00:33:04,360
What to say?

652
00:33:04,360 --> 00:33:04,640
Yeah, exactly.

653
00:33:04,640 --> 00:33:05,200
What to say?

654
00:33:05,920 --> 00:33:06,760
Next sentence.

655
00:33:06,800 --> 00:33:07,440
Yeah, next sentence.

656
00:33:07,440 --> 00:33:08,080
Yeah, at that level.

657
00:33:08,200 --> 00:33:08,400
Yeah.

658
00:33:08,480 --> 00:33:08,960
I mean, yes.

659
00:33:09,000 --> 00:33:12,160
So technically, now, whether we want that or not, is an open question, right?

660
00:33:12,160 --> 00:33:12,760
And whether we can use that.

661
00:33:12,760 --> 00:33:15,880
Boy, I would care for a pop up, a pop up right now.

662
00:33:16,360 --> 00:33:19,480
The estimated engagement using is decreasing.

663
00:33:19,760 --> 00:33:23,200
For Mark Andreessen's, there's, there's a controversy section for

664
00:33:23,200 --> 00:33:27,800
his Wikipedia page in 1993, something happened or something like this.

665
00:33:27,840 --> 00:33:28,520
Bring it up.

666
00:33:28,800 --> 00:33:30,040
That will drive engagement out.

667
00:33:30,120 --> 00:33:30,480
Anyway.

668
00:33:30,640 --> 00:33:31,280
Yeah, that's right.

669
00:33:31,440 --> 00:33:34,520
I mean, look, this gets this whole thing of like, so, you know, the chat

670
00:33:34,520 --> 00:33:36,640
interface has this whole concept of prompt engineering, right?

671
00:33:36,640 --> 00:33:37,800
So it's good for prompts.

672
00:33:37,800 --> 00:33:40,440
Well, it turns out one of the things that LLM's are really good at is writing

673
00:33:40,440 --> 00:33:42,720
prompts, right?

674
00:33:43,400 --> 00:33:45,520
And so like, what if you just outsourced?

675
00:33:45,720 --> 00:33:47,280
And by the way, you could run this experiment today.

676
00:33:47,280 --> 00:33:48,360
You could hook this up to do this today.

677
00:33:48,360 --> 00:33:50,760
The latency is not good enough to do it real time in a conversation, but you

678
00:33:50,760 --> 00:33:53,800
could, you could run this experiment and you just say, look, every 20 seconds,

679
00:33:53,800 --> 00:33:58,160
you could just say, you know, tell me what the optimal prompt is and then ask

680
00:33:58,160 --> 00:33:59,520
yourself that question to give me the result.

681
00:34:00,160 --> 00:34:03,880
And then as, as you use exactly to your point, as you add, there will be, there

682
00:34:03,880 --> 00:34:06,200
will be, these systems are going to have the ability to be learned and updated

683
00:34:06,320 --> 00:34:07,160
essentially in real time.

684
00:34:07,160 --> 00:34:10,240
And so you'll be able to have a pendant or your phone or whatever, watch or

685
00:34:10,240 --> 00:34:11,440
whatever, it'll have a microphone on it.

686
00:34:11,440 --> 00:34:13,000
It'll listen to your conversations.

687
00:34:13,560 --> 00:34:15,280
It'll have a feat of everything else happened in the world.

688
00:34:15,280 --> 00:34:17,960
And then it'll be, you know, sort of retraining, prompting or retraining

689
00:34:17,960 --> 00:34:18,760
itself on the fly.

690
00:34:18,880 --> 00:34:22,240
Um, and so the scenario you described is a, is actually a completely doable scenario.

691
00:34:22,240 --> 00:34:26,400
Now, the hard question on these is always, okay, since that's possible, are

692
00:34:26,400 --> 00:34:27,320
people going to want that?

693
00:34:27,320 --> 00:34:28,560
Like, what's the form of experience?

694
00:34:29,440 --> 00:34:32,160
You know, that, that we, we won't know until we try it, but I don't think

695
00:34:32,160 --> 00:34:36,160
it's possible yet to predict the form of AI in our lives.

696
00:34:36,200 --> 00:34:38,720
Therefore, it's not possible to predict the way in which it will

697
00:34:38,720 --> 00:34:41,240
intermediate our experience with reality yet.

698
00:34:41,520 --> 00:34:41,760
Yeah.

699
00:34:41,920 --> 00:34:43,920
But it feels like those going to be a killer app.

700
00:34:44,760 --> 00:34:46,720
There's probably a mad scrambler right now.

701
00:34:46,760 --> 00:34:51,560
It's out open AI and Microsoft and Google and meta and then startups and

702
00:34:51,560 --> 00:34:54,520
smaller companies figuring out what is the killer app.

703
00:34:54,520 --> 00:34:59,240
Because it feels like it's possible, like a chat, GPT type of thing.

704
00:34:59,360 --> 00:35:04,720
It's possible to build that, but that's 10 X more compelling using already the

705
00:35:04,720 --> 00:35:09,160
LLMs we have using even the open source LLMs, Lama and the different variants.

706
00:35:10,000 --> 00:35:15,000
Um, so you're investing in a lot of companies and you're paying attention.

707
00:35:15,480 --> 00:35:16,720
Who do you think is going to win this?

708
00:35:16,760 --> 00:35:21,160
You think they'll be, who's going to be the next PageRank inventor?

709
00:35:22,080 --> 00:35:23,000
Trillium down the question.

710
00:35:23,120 --> 00:35:24,040
Um, another one.

711
00:35:24,040 --> 00:35:25,240
We have a few of those today.

712
00:35:25,280 --> 00:35:25,800
A bunch of those.

713
00:35:25,840 --> 00:35:28,840
So look, there's a really big question today sitting here today is a really big

714
00:35:28,840 --> 00:35:30,920
question about the big models versus the small models.

715
00:35:31,240 --> 00:35:34,840
Um, that's related directly to the big question of proprietary versus open.

716
00:35:35,520 --> 00:35:39,200
Um, then there's this big question of, of, of, you know, where is the training data?

717
00:35:39,200 --> 00:35:41,560
Going to, like, are we topping out on the training data or not?

718
00:35:41,600 --> 00:35:43,440
And then are we going to be able to synthesize training data?

719
00:35:44,160 --> 00:35:46,640
And then there's a huge pile of questions around regulation.

720
00:35:46,880 --> 00:35:48,720
Um, and, you know, what's actually going to be legal.

721
00:35:48,960 --> 00:35:52,800
Um, and so I would, when we think about it, we dovetail kind of all those, all

722
00:35:52,800 --> 00:35:56,520
those questions together, you can paint a picture of the world where there's two

723
00:35:56,520 --> 00:36:00,040
or three God models that are just at like staggering scale.

724
00:36:00,120 --> 00:36:02,160
Um, and they're just better at everything.

725
00:36:02,640 --> 00:36:06,360
Um, and they will be owned by a small set of companies and they will basically

726
00:36:06,400 --> 00:36:08,920
achieve regulatory capture over the government and they'll have competitive

727
00:36:08,920 --> 00:36:11,720
barriers that will prevent other people from, uh, you know, competing with them.

728
00:36:11,720 --> 00:36:14,280
And so, you know, there will be, you know, just like there's like, you know,

729
00:36:14,280 --> 00:36:17,280
whatever three big banks or three big, you know, by the way, three big search

730
00:36:17,280 --> 00:36:20,480
companies are, I guess, to know, you know, it'll, it'll centralize like that.

731
00:36:20,640 --> 00:36:24,920
Um, you can paint another very different picture that says, no, um, actually, the

732
00:36:24,920 --> 00:36:26,080
opposite of that's going to happen.

733
00:36:26,200 --> 00:36:29,520
This is going to basically that this is the new gold, you know, this is the new

734
00:36:29,520 --> 00:36:34,200
gold rush, alchemy, like, you know, this is the, this is the big bang for this whole

735
00:36:34,200 --> 00:36:36,440
new area of, of, uh, of science and technology.

736
00:36:36,440 --> 00:36:38,840
And so therefore you're going to have every smart 14 year old on the planet

737
00:36:38,840 --> 00:36:40,240
building open source, right?

738
00:36:40,240 --> 00:36:42,280
You know, you can figure out a way to optimize these things.

739
00:36:42,760 --> 00:36:45,760
Um, and then, you know, we're just going to get like overwhelmingly better at

740
00:36:45,760 --> 00:36:48,480
generating trading data, we're going to, you know, bring in like blockchain

741
00:36:48,480 --> 00:36:51,000
networks to have like an economic incentive to generate decentralized

742
00:36:51,000 --> 00:36:53,160
training data and so forth and so on.

743
00:36:53,160 --> 00:36:56,080
And then basically we're going to live in a world of open source and there's

744
00:36:56,080 --> 00:36:58,080
going to be a billion LMS, right?

745
00:36:58,080 --> 00:37:00,000
Of every size, scale, shape, and description.

746
00:37:00,000 --> 00:37:02,800
And there might be a few big ones that are like the super genius ones, but

747
00:37:02,800 --> 00:37:04,840
like mostly what we'll experience is open source.

748
00:37:04,960 --> 00:37:07,560
And that's, you know, that's more like a world of like what we have today with

749
00:37:07,560 --> 00:37:08,440
like Linux and the web.

750
00:37:09,160 --> 00:37:14,120
Um, so, okay, but, uh, hey, you, you painted these two worlds, but there's

751
00:37:14,120 --> 00:37:17,680
also variations of those worlds because he said regulatory capture is possible to

752
00:37:17,680 --> 00:37:20,600
have these tech giants that don't have regulatory capture, which is something

753
00:37:20,600 --> 00:37:24,840
you're also calling for saying it's okay to have big companies working on this

754
00:37:24,920 --> 00:37:27,920
stuff, uh, as long as they don't achieve regulatory capture.

755
00:37:28,440 --> 00:37:34,840
Uh, but, uh, I have the sense that, um, there's just going to be a new startup.

756
00:37:36,120 --> 00:37:42,440
That's going to basically be the page rank inventor, which has become the new tech giant.

757
00:37:44,120 --> 00:37:46,240
I don't know, I would love to hear your kind of opinion.

758
00:37:46,240 --> 00:37:54,520
If Google meta and Microsoft are as gigantic companies able to pivot so hard.

759
00:37:54,840 --> 00:37:59,280
To create new products, like some of it is just even hiring people or having,

760
00:37:59,320 --> 00:38:04,280
uh, corporate structure that allows for the crazy young kids to come in and

761
00:38:04,280 --> 00:38:05,560
just create something totally new.

762
00:38:06,360 --> 00:38:08,320
Do you think it's possible or do you think it'll come from a startup?

763
00:38:08,480 --> 00:38:11,040
Yeah, it is this always big question, which is you get this feeling.

764
00:38:11,040 --> 00:38:14,440
I hear about this a lot from CEOs found founder CEOs where it's like, wow,

765
00:38:14,520 --> 00:38:16,120
we have 50,000 people.

766
00:38:16,120 --> 00:38:18,680
It's now harder to do new things than it was when we had 50 people.

767
00:38:19,240 --> 00:38:20,360
Like what has happened?

768
00:38:20,360 --> 00:38:22,040
So that's a recurring phenomenon.

769
00:38:22,360 --> 00:38:25,240
By the way, that's one of the reasons why there's always startups and why there's

770
00:38:25,240 --> 00:38:29,160
venture capital is just, that's, that's like a timeless kind of thing.

771
00:38:29,160 --> 00:38:30,800
So that, that, that's one observation.

772
00:38:30,800 --> 00:38:35,240
Um, on page rank, um, we can talk about that, but on page ranks,

773
00:38:35,240 --> 00:38:37,240
specifically on page rank, um, there actually is a page.

774
00:38:37,240 --> 00:38:39,880
So there is a page rank already in the field and it's the transformer, right?

775
00:38:39,920 --> 00:38:43,080
So the, the, the big breakthrough was the transformer, um, and, uh,

776
00:38:43,080 --> 00:38:47,160
the transformer was invented in, uh, 2017 at Google.

777
00:38:47,880 --> 00:38:50,760
And this is actually like really an interesting question because it's like,

778
00:38:50,760 --> 00:38:53,720
okay, the transformers, like why does opening, I even exist.

779
00:38:53,760 --> 00:38:55,200
Like the transformers invested at Google.

780
00:38:55,200 --> 00:38:58,240
Why didn't Google, I asked a guy, I asked a guy, you know, who was senior at

781
00:38:58,240 --> 00:38:59,680
Google brain, kind of when this was happening.

782
00:38:59,680 --> 00:39:03,200
And I said, if Google had just gone flat out to the wall and just said, look,

783
00:39:03,200 --> 00:39:05,360
we're going to launch, we're going to launch the equivalent of GPT for as

784
00:39:05,360 --> 00:39:06,200
fast as we can.

785
00:39:06,240 --> 00:39:08,040
Um, he said, I said, when could we have had it?

786
00:39:08,040 --> 00:39:11,320
And he said 2019, they could have just done a two year sprint with the

787
00:39:11,320 --> 00:39:14,000
transformer and Bennett because they already had the compute at scale.

788
00:39:14,000 --> 00:39:16,040
They already had all the training data and they could have just done it.

789
00:39:16,440 --> 00:39:18,280
There's a variety of reasons they didn't do it.

790
00:39:18,360 --> 00:39:20,440
This is like a classic big company thing.

791
00:39:20,520 --> 00:39:25,480
Um, IBM invented the relational database in 1970s, let it sit on the shelf as a

792
00:39:25,480 --> 00:39:29,280
paper, Larry Ellison picked it up and built Oracle, Xerox Park invented the

793
00:39:29,280 --> 00:39:32,120
interactive computer, they let it sit on the shelf, Steve Jobs came and

794
00:39:32,120 --> 00:39:33,960
turned it into the Macintosh, right?

795
00:39:33,960 --> 00:39:35,200
And so there is this pattern.

796
00:39:35,200 --> 00:39:38,720
Now, having said that, sitting here today, like Google's in the game, right?

797
00:39:38,720 --> 00:39:41,800
So Google, you know, maybe, maybe they, maybe they let like a four year gap

798
00:39:41,800 --> 00:39:44,440
there go there that they maybe shouldn't have, but like they're in the game.

799
00:39:44,440 --> 00:39:46,040
And so now they've got, you know, now they're committed.

800
00:39:46,440 --> 00:39:47,160
They've done this merger.

801
00:39:47,160 --> 00:39:47,840
They're bringing in demos.

802
00:39:47,840 --> 00:39:49,000
They've got this merger with DeepMind.

803
00:39:49,320 --> 00:39:50,920
You know, they're piling in resources.

804
00:39:50,920 --> 00:39:53,600
There are rumors that they're, you know, building up an incredible, you know,

805
00:39:53,600 --> 00:39:56,760
super LLM, um, you know, way beyond what we even have today.

806
00:39:57,240 --> 00:40:00,520
Um, and they've got, you know, unlimited resources and a huge, you know,

807
00:40:00,520 --> 00:40:02,040
they've been challenged at their honor.

808
00:40:02,560 --> 00:40:02,800
Yeah.

809
00:40:02,800 --> 00:40:07,360
I had a, I had a chance to hang out with Sundar Prasai a couple of days ago and we

810
00:40:07,360 --> 00:40:10,760
took this walk and there's this giant new building, uh, well, there's going to

811
00:40:10,760 --> 00:40:13,040
be a lot of AI work, uh, being done.

812
00:40:13,040 --> 00:40:19,640
And it's kind of this ominous feeling of like the fight is on.

813
00:40:20,880 --> 00:40:21,080
Yeah.

814
00:40:22,360 --> 00:40:26,320
There's this beautiful Silicon Valley nature, like birds of chirping and this

815
00:40:26,320 --> 00:40:30,440
giant building, and it's like, uh, the beast has been awakened.

816
00:40:31,440 --> 00:40:34,600
And then like all the big companies are waking up to this.

817
00:40:34,760 --> 00:40:41,160
They have the compute, but also the little guys have, uh, it feels like they

818
00:40:41,160 --> 00:40:44,760
have all the tools to create the killer product that, uh, and then there's

819
00:40:44,760 --> 00:40:45,840
all the tools to scale.

820
00:40:45,840 --> 00:40:50,280
If you have a good idea, if you have the page rank idea, so there's

821
00:40:50,280 --> 00:40:54,960
several things that is page rank page, there's page rank, the algorithm and the

822
00:40:54,960 --> 00:40:56,840
idea, and there's like the implementation of it.

823
00:40:57,200 --> 00:41:00,280
And I feel like killer product is not just the idea, like the transformer,

824
00:41:00,280 --> 00:41:03,560
it's the implementation, something, something really compelling about it.

825
00:41:03,560 --> 00:41:07,640
Like you just can't look away, something like, um, the algorithm behind

826
00:41:07,640 --> 00:41:11,360
TikTok versus TikTok itself, like the actual experience of TikTok.

827
00:41:11,400 --> 00:41:12,600
They just, you can't look away.

828
00:41:13,120 --> 00:41:17,640
It feels like somebody's going to come up with that and it could be Google, but

829
00:41:17,640 --> 00:41:21,120
it feels like it's just easier and faster to do for a startup.

830
00:41:21,880 --> 00:41:22,080
Yeah.

831
00:41:22,080 --> 00:41:25,240
So, so the startup, the, the huge advantage the startups have is they just,

832
00:41:25,240 --> 00:41:26,560
they, there's no sacred cows.

833
00:41:26,600 --> 00:41:28,440
There's no historical legacy to protect.

834
00:41:28,440 --> 00:41:31,040
There's no need to reconcile your new plan with the existing strategy.

835
00:41:31,040 --> 00:41:32,440
There's no communication overhead.

836
00:41:32,480 --> 00:41:34,600
There's no, you know, big companies are big companies.

837
00:41:34,600 --> 00:41:36,520
They've got pre-meetings planning for the meeting.

838
00:41:36,520 --> 00:41:38,480
Then they have, then they have the post-meeting of the recap.

839
00:41:38,480 --> 00:41:40,280
Then they have the presentation, the board, then they have the next round

840
00:41:40,280 --> 00:41:43,960
of meetings and that's the, that's the elapsed time when the startup launches

841
00:41:43,960 --> 00:41:44,600
its product, right?

842
00:41:44,600 --> 00:41:46,880
So, so, so, so there's a timeless, right?

843
00:41:46,920 --> 00:41:47,360
Yeah.

844
00:41:47,480 --> 00:41:48,560
So there's a timeless thing there.

845
00:41:48,560 --> 00:41:51,440
Now, what the startups don't have is everything else, right?

846
00:41:51,440 --> 00:41:52,520
So startups, they don't have a brand.

847
00:41:52,520 --> 00:41:53,600
They don't have customer relationships.

848
00:41:53,600 --> 00:41:54,600
They've gotten a distribution.

849
00:41:54,600 --> 00:41:55,800
They've got no, you know, scale.

850
00:41:55,800 --> 00:41:57,840
I mean, sitting here today, they can't even get GPUs, right?

851
00:41:57,840 --> 00:42:01,200
Like there's like a GPU shortage startups are literally stalled out right now

852
00:42:01,200 --> 00:42:03,240
because they can't get chips, which is like super weird.

853
00:42:03,680 --> 00:42:03,960
Yeah.

854
00:42:04,000 --> 00:42:05,240
Um, they got the cloud.

855
00:42:05,720 --> 00:42:08,320
Yeah, but the clouds run out of chips, right?

856
00:42:08,320 --> 00:42:10,480
And then, and then, and then to the extent the clouds have chips,

857
00:42:10,480 --> 00:42:12,880
they allocate them to the big customers, not the small customers, right?

858
00:42:12,880 --> 00:42:17,640
And so, so, so, so the small companies lack everything other than the ability

859
00:42:17,640 --> 00:42:19,880
to just do something new, right?

860
00:42:20,280 --> 00:42:22,080
And this is the timeless race and battle.

861
00:42:22,080 --> 00:42:23,920
And this is kind of the point I tried to make in the essay, which is

862
00:42:23,920 --> 00:42:25,600
like both sides of this are good.

863
00:42:25,600 --> 00:42:28,440
Like it's really good to have like highly scale tech companies that can do

864
00:42:28,440 --> 00:42:30,960
things that are like at staggering levels of sophistication.

865
00:42:31,240 --> 00:42:33,600
It's really good to have startups that can launch brand new ideas.

866
00:42:33,680 --> 00:42:35,680
They ought to be able to both do that and compete.

867
00:42:35,720 --> 00:42:38,520
They neither one ought to be subsidized or protected from the others.

868
00:42:39,040 --> 00:42:42,160
Like that's, that's, to me, that's just like very clearly the idealized world.

869
00:42:42,720 --> 00:42:45,160
It is the world we've been in for AI up until now.

870
00:42:45,160 --> 00:42:47,080
And then of course there are people trying to shut that down.

871
00:42:47,080 --> 00:42:50,280
But my hope is that, you know, the best outcome clearly will be if that continues.

872
00:42:50,520 --> 00:42:56,600
We'll talk about that a little bit, but I'd love to linger on some of the ways

873
00:42:56,600 --> 00:42:58,000
this is going to change the internet.

874
00:42:58,240 --> 00:43:01,360
So, um, I don't know if you remember, but there's a thing called Mosaic and

875
00:43:01,360 --> 00:43:03,080
there's a thing called Netscape Navigator.

876
00:43:03,320 --> 00:43:04,640
So you were there in the beginning.

877
00:43:05,040 --> 00:43:07,200
Uh, what about the interface to the internet?

878
00:43:07,200 --> 00:43:10,640
How do you think the browser changes and who gets to own the browser?

879
00:43:10,640 --> 00:43:12,920
We've got to see some very interesting browsers.

880
00:43:13,680 --> 00:43:19,120
Uh, Firefox, I mean, all the variants of Microsoft, Internet Explorer, Edge,

881
00:43:19,760 --> 00:43:26,960
and, uh, now Chrome, um, the actual, I mean, it seems like a dumb question to ask,

882
00:43:26,960 --> 00:43:29,000
but do you think we'll still have the web browser?

883
00:43:29,960 --> 00:43:33,320
So I, uh, I have an eight year old and he's super into like Minecraft and

884
00:43:33,320 --> 00:43:34,560
learning to code and doing all this stuff.

885
00:43:34,560 --> 00:43:36,520
So I, I, of course, I was very proud.

886
00:43:36,520 --> 00:43:39,360
I could bring sort of fire down from the mountain to my kid and I brought him

887
00:43:39,360 --> 00:43:43,120
chat GPT and I hooked him up on his, on his, on his, on his laptop.

888
00:43:43,120 --> 00:43:45,120
And I was like, you know, this is the thing that's going to answer all your

889
00:43:45,120 --> 00:43:46,200
questions and he's like, okay.

890
00:43:47,040 --> 00:43:49,120
And I'm like, but it's going to answer all your questions.

891
00:43:49,120 --> 00:43:50,840
And he's like, well, of course, like it's a computer, of course.

892
00:43:50,840 --> 00:43:51,640
It answers all your questions.

893
00:43:51,640 --> 00:43:53,000
Like what else would a computer be good for?

894
00:43:53,480 --> 00:43:53,880
Dad.

895
00:43:54,680 --> 00:43:56,200
Um, and never impressed.

896
00:43:56,200 --> 00:43:57,680
Not impressed in the least.

897
00:43:58,400 --> 00:43:59,280
Two weeks pass.

898
00:43:59,360 --> 00:44:02,960
Um, and he has some question, um, and I say, well, have you asked you GPT?

899
00:44:02,960 --> 00:44:04,760
And he's like, dad, being is better.

900
00:44:06,760 --> 00:44:09,080
And why is being better is because it's built into the browser.

901
00:44:09,880 --> 00:44:12,400
Cause he's like, look, I have the Microsoft edge browser and like it's

902
00:44:12,400 --> 00:44:13,160
got being right here.

903
00:44:13,200 --> 00:44:15,840
And then he doesn't know this yet, but one of the things you can do with

904
00:44:15,840 --> 00:44:19,960
being an edge, um, is there's a setting where you can, um, use it to basically

905
00:44:19,960 --> 00:44:23,640
talk to any webpage because it's sitting right there next to the, uh, next

906
00:44:23,640 --> 00:44:24,400
to the, next to the browser.

907
00:44:24,480 --> 00:44:25,960
And by the way, which includes PDF documents.

908
00:44:25,960 --> 00:44:28,920
And so you can, in, in, in the way they've implemented an edge with Bing is

909
00:44:28,920 --> 00:44:32,320
you can load a PDF and then you can, you can ask a questions, which is the

910
00:44:32,320 --> 00:44:35,000
thing you, you can't do currently and just chat GPT.

911
00:44:35,000 --> 00:44:37,520
So they're, you know, they're, they're going to, they're going to push the,

912
00:44:37,520 --> 00:44:38,640
the, the mel, I think that's great.

913
00:44:38,680 --> 00:44:41,080
You know, they're going to push the melding and see if there's a combination

914
00:44:41,080 --> 00:44:41,600
thing there.

915
00:44:42,000 --> 00:44:44,960
Google's rolling off this thing, the magic button, uh, which is implemented

916
00:44:44,960 --> 00:44:46,640
in either put in Google docs, right?

917
00:44:46,640 --> 00:44:50,120
And so you go to, you know, Google docs and you create a new document and you,

918
00:44:50,120 --> 00:44:52,560
you know, you, instead of like, you know, starting to type, you just, you know,

919
00:44:52,560 --> 00:44:55,240
say it, press the button and it starts to like generate content for you.

920
00:44:56,080 --> 00:44:56,360
Right.

921
00:44:56,360 --> 00:44:58,280
Like, is that the way that it'll work?

922
00:44:58,480 --> 00:45:02,080
Um, is it going to be a speech UI where you're just going to have an earpiece

923
00:45:02,080 --> 00:45:03,000
and talk to it all day long?

924
00:45:03,520 --> 00:45:07,120
You know, is it going to be a, like, these are all, like, this is exactly

925
00:45:07,120 --> 00:45:09,440
the kind of thing that I don't, this is exactly the kind of thing I don't

926
00:45:09,440 --> 00:45:10,480
think is possible to forecast.

927
00:45:11,000 --> 00:45:13,200
I think what we need to do is like run all those experiments.

928
00:45:13,440 --> 00:45:16,000
Um, and, and, and so one outcome is we come out of this with like a super

929
00:45:16,000 --> 00:45:18,560
browser that has AI built in, that's just like amazing.

930
00:45:19,120 --> 00:45:21,520
The other there, look, there's a real possibility that the whole, I mean,

931
00:45:21,520 --> 00:45:25,640
look, there's a possibility here that the whole idea of a screen and windows

932
00:45:25,640 --> 00:45:27,240
and all this stuff just goes away.

933
00:45:27,240 --> 00:45:28,480
Cause like, why do you need that?

934
00:45:28,480 --> 00:45:30,960
If you just have a thing that's just telling you whatever you need to know.

935
00:45:31,800 --> 00:45:35,080
And also, so there's apps that you can use.

936
00:45:35,160 --> 00:45:38,760
You don't really use them, you know, being a Linux guy and Windows guy.

937
00:45:39,720 --> 00:45:44,160
Um, there's one window, the browser that with which you can interact with the

938
00:45:44,160 --> 00:45:46,640
internet, but on the phone, you can also have apps.

939
00:45:46,960 --> 00:45:50,040
So I can interact with Twitter through the app or through the web browser.

940
00:45:51,000 --> 00:45:54,640
And, um, that seems like an obvious distinction, but why have the web

941
00:45:54,640 --> 00:45:55,560
browser in that case?

942
00:45:55,920 --> 00:46:00,400
If one of the apps starts becoming the everything app, what do you want to

943
00:46:00,400 --> 00:46:03,560
try to do with Twitter, but there could be others that could be like a big app.

944
00:46:03,600 --> 00:46:07,680
There could be a Google app that just doesn't really do search, but just like

945
00:46:09,120 --> 00:46:13,040
do what I guess AOL did back in the day or something where it's all right there.

946
00:46:14,400 --> 00:46:24,120
And it changes, um, it changes the nature of the internet because the, where the

947
00:46:24,120 --> 00:46:28,360
content is hosted, who owns the data, who owns the content, how, what is, what

948
00:46:28,360 --> 00:46:29,580
is the kind of content you create?

949
00:46:29,580 --> 00:46:35,240
How do you make money by creating content or the content creators, uh, all of that.

950
00:46:36,000 --> 00:46:39,800
Or it could just keep being the same, which is like, which is the nature of

951
00:46:39,800 --> 00:46:42,760
web pages changes and the nature of content, but there will still be a web

952
00:46:42,760 --> 00:46:45,360
browser because a web browser is a pretty sexy product.

953
00:46:46,040 --> 00:46:50,280
It just seems to work because it like you have an interface, a window into the

954
00:46:50,280 --> 00:46:52,720
world and then the world can be anything you want.

955
00:46:52,720 --> 00:46:55,600
And as the world will evolve, there could be different programming languages.

956
00:46:55,600 --> 00:46:56,440
It can be animated.

957
00:46:56,440 --> 00:46:57,920
Maybe it's three dimensional and so on.

958
00:46:58,920 --> 00:46:59,960
Yeah, it's interesting.

959
00:47:00,200 --> 00:47:02,000
Do you think we'll still have the web browser?

960
00:47:02,320 --> 00:47:05,560
Well, every, every, every, um, every medium becomes the content for the next one.

961
00:47:05,600 --> 00:47:09,040
So they, you know, they will be able to give you a browser whenever you want.

962
00:47:09,880 --> 00:47:10,640
Oh, interesting.

963
00:47:11,280 --> 00:47:11,440
Yeah.

964
00:47:11,440 --> 00:47:15,320
Another way to think about it is maybe what the browser is, maybe it's just the escape

965
00:47:15,320 --> 00:47:16,040
hatch, right?

966
00:47:16,040 --> 00:47:18,480
And which is maybe kind of what it is today, right?

967
00:47:18,480 --> 00:47:21,560
Which is like most of what you do is like inside a social network or inside a

968
00:47:21,560 --> 00:47:24,480
search engine or inside, you know, somebody's app or inside some controlled

969
00:47:24,480 --> 00:47:25,480
experience, right?

970
00:47:25,480 --> 00:47:28,680
But then every once in a while, there's something where you actually want to jail

971
00:47:28,680 --> 00:47:28,920
break.

972
00:47:28,920 --> 00:47:30,000
You want to actually get free.

973
00:47:30,120 --> 00:47:32,360
The web browser is the FU to the man.

974
00:47:32,360 --> 00:47:34,720
You're allowed to, that's the free internet.

975
00:47:34,800 --> 00:47:35,120
Yeah.

976
00:47:35,640 --> 00:47:37,360
Back, back the way it was in the 90s.

977
00:47:37,360 --> 00:47:38,240
So here's something I'm proud of.

978
00:47:38,240 --> 00:47:40,480
So nobody really talks about here's something I'm proud of, which is the web,

979
00:47:40,480 --> 00:47:42,440
the web, the browser, the web servers, they're all, they're still back

980
00:47:42,440 --> 00:47:44,760
or compatible all the way back to like 1992, right?

981
00:47:44,760 --> 00:47:48,960
So like you can put up a, you can still, you know, the big breakthrough of the

982
00:47:48,960 --> 00:47:51,400
web early on, the big breakthrough was it made it really easy

983
00:47:51,400 --> 00:47:54,200
to read, but it also made it really easy to write, made it really easy to publish.

984
00:47:54,240 --> 00:47:56,160
And we literally made it so easy to publish.

985
00:47:56,160 --> 00:47:58,880
We made it not only so you can use easy to publish content, it was actually

986
00:47:58,880 --> 00:48:01,520
also easy to actually write a web server, right?

987
00:48:01,520 --> 00:48:04,640
And you can literally write a web server in four lines of drill code and you

988
00:48:04,640 --> 00:48:05,800
could start publishing content on it.

989
00:48:05,800 --> 00:48:08,920
And you could set whatever rules you want for the content, whatever censorship,

990
00:48:08,920 --> 00:48:10,720
no censorship, whatever you want, you could just do that.

991
00:48:11,160 --> 00:48:12,880
As long as you had an IP address, right?

992
00:48:12,880 --> 00:48:13,600
You could do that.

993
00:48:14,080 --> 00:48:14,840
That still works.

994
00:48:16,040 --> 00:48:18,080
Like that still works exactly as I just described.

995
00:48:18,560 --> 00:48:21,760
So this is part of my reaction to all of this, like, you know, all this

996
00:48:21,760 --> 00:48:24,560
just censorship pressure and all this, you know, these issues around control

997
00:48:24,560 --> 00:48:27,720
and all this stuff, which is like, maybe we need to get back a little bit

998
00:48:27,720 --> 00:48:30,360
more to the wild west, like the wild west is still out there.

999
00:48:30,920 --> 00:48:33,240
Now, they will, they will try to chase you down.

1000
00:48:33,240 --> 00:48:35,600
Like they'll try to, you know, people who want a sensor will try to take away

1001
00:48:35,600 --> 00:48:38,480
your, you know, your domain name and they'll try to take away your payments

1002
00:48:38,480 --> 00:48:41,000
account and so forth, if they really don't like what you, what you're saying.

1003
00:48:41,000 --> 00:48:44,280
But, but nevertheless, you like, unless they literally are intercepting

1004
00:48:44,280 --> 00:48:46,400
you at the ISP level, like you can still put up a thing.

1005
00:48:47,400 --> 00:48:49,840
And so I don't know, I think that's important to preserve, right?

1006
00:48:49,840 --> 00:48:53,640
Like, because, because, because, I mean, one is just a freedom argument,

1007
00:48:53,640 --> 00:48:56,600
but the other is a creativity argument, which is you want to have the escape

1008
00:48:56,600 --> 00:48:59,200
hatch so that the kid with the idea is able to realize the idea.

1009
00:48:59,200 --> 00:49:01,920
Because to your point on PageRank, you actually don't know what the next big

1010
00:49:01,920 --> 00:49:02,440
idea is.

1011
00:49:03,200 --> 00:49:05,520
Nobody called Larry Page and told him to develop PageRank.

1012
00:49:05,520 --> 00:49:06,680
Like he came up with that on his own.

1013
00:49:06,720 --> 00:49:10,040
And you want to always, I think, leave the escape hatch for the next, you know,

1014
00:49:10,040 --> 00:49:12,760
kid or the next Stanford grad student to have the breakthrough idea and be

1015
00:49:12,760 --> 00:49:14,400
able to get it up and running before anybody notices.

1016
00:49:15,240 --> 00:49:17,600
Um, you and I are both fans of history.

1017
00:49:17,600 --> 00:49:18,640
So let's step back.

1018
00:49:18,880 --> 00:49:20,080
We'll be talking about the future.

1019
00:49:20,080 --> 00:49:24,280
Let's step back for a bit and look at, uh, the nineties.

1020
00:49:24,280 --> 00:49:28,400
You created Mosaic web browser, the first widely used web browser.

1021
00:49:28,400 --> 00:49:29,320
Tell the story of that.

1022
00:49:29,400 --> 00:49:32,360
Hot, hot, and how did it evolve into Netscape navigator?

1023
00:49:32,360 --> 00:49:33,480
This is the early days.

1024
00:49:34,200 --> 00:49:35,120
So, full story.

1025
00:49:35,120 --> 00:49:38,880
So, um, you were born, I was born a small, a small child.

1026
00:49:38,920 --> 00:49:41,840
Um, well, actually, yeah, let's go there.

1027
00:49:41,840 --> 00:49:45,080
Like, when did you, when would you first fall in love with computers?

1028
00:49:45,160 --> 00:49:49,640
Oh, so I hit the generational jackpot and I hit the Gen X kind of point perfectly

1029
00:49:49,640 --> 00:49:50,160
as it turns out.

1030
00:49:50,160 --> 00:49:51,440
So I was born in 1971.

1031
00:49:51,440 --> 00:49:56,200
So there's this great website called WTF happened in 1971.com, which is basically

1032
00:49:56,200 --> 00:49:57,800
in 1971 is when everything started to go to hell.

1033
00:49:57,800 --> 00:49:59,600
And I was, of course, born in 1971.

1034
00:49:59,600 --> 00:50:01,560
So I like to think that I had something to do with that.

1035
00:50:01,600 --> 00:50:03,480
Did you make it on the website?

1036
00:50:03,480 --> 00:50:06,320
I have, I don't think I made it on the website, but, you know, hopefully somebody

1037
00:50:06,320 --> 00:50:09,800
needs to add, this is, this is where everything maybe I contributed to some of

1038
00:50:09,800 --> 00:50:13,720
the trends, um, that they, uh, that they do every line on that website goes like

1039
00:50:13,720 --> 00:50:14,160
that, right?

1040
00:50:14,160 --> 00:50:16,400
So it's all, it's all, it's all a picture disaster.

1041
00:50:16,400 --> 00:50:20,120
But, um, but there was this moment in time where, cause the, you know, sort of

1042
00:50:20,120 --> 00:50:24,720
the Apple, you know, the Apple II hit in like 1978 and then the IBM PC hit in 82.

1043
00:50:24,720 --> 00:50:26,960
So I was like, you know, 11 when the PC came out.

1044
00:50:27,000 --> 00:50:29,240
Um, and so I just kind of hit that perfectly.

1045
00:50:29,600 --> 00:50:32,240
And then that was the first moment in time when like regular people could spend

1046
00:50:32,240 --> 00:50:33,800
a few hundred dollars and get a computer, right?

1047
00:50:33,800 --> 00:50:36,360
And so that, I just like that, that, that resonated right out of the gate.

1048
00:50:37,040 --> 00:50:39,760
Um, well, then the other part of the story is, you know, I was using

1049
00:50:39,880 --> 00:50:42,280
an Apple II that used a bunch of them, but I was using Apple II.

1050
00:50:42,280 --> 00:50:44,920
And of course it's set in the back of every Apple II and every Mac it said,

1051
00:50:44,920 --> 00:50:47,440
you know, designed in Cupertino, California.

1052
00:50:47,440 --> 00:50:50,440
And I was like, wow, Cupertino must be the like shining city on the hill.

1053
00:50:50,440 --> 00:50:53,040
Like it was sort of like the most amazing, like city of all time.

1054
00:50:53,040 --> 00:50:53,920
I can't wait to see it.

1055
00:50:53,920 --> 00:50:57,160
And of course, years later, I came out to Silicon Valley and went to

1056
00:50:57,160 --> 00:51:01,120
Cupertino and it's just a bunch of office parks, low rise apartment buildings.

1057
00:51:01,680 --> 00:51:04,640
So the aesthetics were a little disappointing, but you know, it was the,

1058
00:51:05,080 --> 00:51:08,440
the vector, uh, right of the, of the creation of a lot, of a lot of this stuff.

1059
00:51:08,520 --> 00:51:09,600
Um, yeah.

1060
00:51:09,760 --> 00:51:13,800
So, so then basically, so part, part, part of my story is just the luck of

1061
00:51:13,800 --> 00:51:16,080
having been born at the right time and getting exposed to PCs.

1062
00:51:16,080 --> 00:51:19,640
Then the other part is, um, the other part is when Elgora says that he created

1063
00:51:19,640 --> 00:51:22,960
the internet, he actually is correct, uh, in, in, in a really meaningful way,

1064
00:51:22,960 --> 00:51:26,800
which is he sponsored a bill in 1985 that essentially created the modern internet

1065
00:51:26,800 --> 00:51:30,400
created what is called the NSF net at the time, which is sort of the, the first

1066
00:51:30,400 --> 00:51:31,680
really fast internet backbone.

1067
00:51:32,360 --> 00:51:36,240
Um, and, uh, you know, that, that bill dumped a ton of money into a bunch

1068
00:51:36,240 --> 00:51:39,240
of research universities to build out basically at the internet backbone.

1069
00:51:39,240 --> 00:51:42,600
And then the super computer centers that were clustered around, um, the,

1070
00:51:42,600 --> 00:51:43,200
the internet.

1071
00:51:43,200 --> 00:51:46,000
And, and one of those universities was University of Illinois, right?

1072
00:51:46,000 --> 00:51:46,480
Went to school.

1073
00:51:46,480 --> 00:51:49,120
And so the other stroke of luck that I had was I, I went to Illinois basically

1074
00:51:49,120 --> 00:51:51,280
right as that money was just like getting dumped on campus.

1075
00:51:51,960 --> 00:51:55,440
And so as a consequence, we had at on campus and this is like, you know, 89,

1076
00:51:55,560 --> 00:51:59,080
90, 91, we had like, you know, we were right on the internet backbone.

1077
00:51:59,080 --> 00:52:02,480
We had like T three and 45 at the time, T three, 45 megabit backbone connection,

1078
00:52:02,480 --> 00:52:04,680
which at the time was, you know, wildly state of the art.

1079
00:52:04,760 --> 00:52:06,520
Um, we had crazy supercomputers.

1080
00:52:06,560 --> 00:52:08,720
We had thinking machines, parallel supercomputers.

1081
00:52:08,720 --> 00:52:10,280
We had silicon graphics workstations.

1082
00:52:10,280 --> 00:52:11,480
We had Macintoshes.

1083
00:52:11,540 --> 00:52:13,640
We had, we had next cubes all over the place.

1084
00:52:13,640 --> 00:52:15,560
We had like every possible kind of computer you could imagine.

1085
00:52:15,560 --> 00:52:17,040
Cause all this money just fell out of the sky.

1086
00:52:17,840 --> 00:52:19,880
Um, you were living in the future.

1087
00:52:20,000 --> 00:52:20,240
Yeah.

1088
00:52:20,240 --> 00:52:22,840
So quite literally it was, yeah, like it's all, it's all there.

1089
00:52:22,840 --> 00:52:25,400
It's all like we had full broadband graphics, like the whole thing.

1090
00:52:25,560 --> 00:52:28,720
And, and it's actually funny cause they had this, this is the first time I kind

1091
00:52:28,720 --> 00:52:31,040
of it sort of tickled the back of my head that there might be a big

1092
00:52:31,040 --> 00:52:33,680
opportunity in here, which is, you know, they, they embraced it.

1093
00:52:33,680 --> 00:52:36,280
And so they put like computers in all the dorms and they wired up all the

1094
00:52:36,320 --> 00:52:39,000
dorm rooms and they had all these, you know, labs everywhere and everything.

1095
00:52:39,000 --> 00:52:43,240
And then they, they gave every undergrad a computer account and an email address.

1096
00:52:43,880 --> 00:52:47,800
Um, and the assumption was that you would use the internet for your four years at

1097
00:52:47,800 --> 00:52:51,160
college, um, and then you would graduate and stop using it.

1098
00:52:52,480 --> 00:52:53,600
And that was that, right?

1099
00:52:54,160 --> 00:52:55,440
And you would just retire your email address.

1100
00:52:55,440 --> 00:52:57,600
It wouldn't be relevant anymore cause you'd go off in the workplace and they

1101
00:52:57,600 --> 00:52:58,200
don't use email.

1102
00:52:58,880 --> 00:53:00,640
You'd be back to using fax machines or whatever.

1103
00:53:01,000 --> 00:53:01,960
Did you have that sense as well?

1104
00:53:01,960 --> 00:53:04,760
Like what, what you said the, the back of your head was tickled.

1105
00:53:04,760 --> 00:53:08,280
Like what was your, what was exciting to you about this possible world?

1106
00:53:08,320 --> 00:53:11,280
Well, if this is so useful in this container, if this is so useful in this

1107
00:53:11,280 --> 00:53:14,360
container environment that just has this weird source of outside funding, then

1108
00:53:14,400 --> 00:53:17,560
if, if it were practical for everybody else to have this, and if it were

1109
00:53:17,560 --> 00:53:19,760
cost effective, everybody else to have this, wouldn't they want it?

1110
00:53:20,320 --> 00:53:23,200
And the overwhelmingly, the prevailing view at the time was no, they would not

1111
00:53:23,200 --> 00:53:23,560
want it.

1112
00:53:23,560 --> 00:53:26,080
This is esoteric weird nerd stuff, right?

1113
00:53:26,080 --> 00:53:28,520
That like computer science kids like, but like normal people are never going to

1114
00:53:28,520 --> 00:53:29,760
do email, right?

1115
00:53:29,760 --> 00:53:30,960
Or be on the internet, right?

1116
00:53:31,000 --> 00:53:34,320
Um, and so I was just like, wow, like this, this is actually like, this is

1117
00:53:34,320 --> 00:53:35,160
really compelling stuff.

1118
00:53:35,760 --> 00:53:37,840
Now the other part was it was all really hard to use.

1119
00:53:37,960 --> 00:53:41,200
And in practice, you had to be a, basically a CS, uh, you basically had to,

1120
00:53:41,240 --> 00:53:44,680
had to be a CS undergrad or equivalent to actually get full use of the internet

1121
00:53:44,680 --> 00:53:46,840
at that point, um, cause it was all pretty esoteric stuff.

1122
00:53:47,240 --> 00:53:49,520
So then that was the other part of the idea, which was, okay, we need to

1123
00:53:49,520 --> 00:53:50,560
actually make this easy to use.

1124
00:53:51,360 --> 00:53:56,080
So what's involved in creating was like, like in creating, uh, graphical

1125
00:53:56,080 --> 00:53:57,440
interface to the internet.

1126
00:53:57,680 --> 00:53:57,880
Yes.

1127
00:53:57,880 --> 00:53:59,040
It was a combination of things.

1128
00:53:59,040 --> 00:54:02,240
So it was like basically the, the, the web existed in an early sort of

1129
00:54:02,240 --> 00:54:03,480
described as prototype form.

1130
00:54:03,600 --> 00:54:05,400
Uh, and by the way, text only at that point.

1131
00:54:05,920 --> 00:54:06,840
Uh, what did it look like?

1132
00:54:06,840 --> 00:54:07,720
What, what was the web?

1133
00:54:07,720 --> 00:54:10,040
I mean, well, and the key figure is like, what was it?

1134
00:54:10,400 --> 00:54:11,240
What was it like?

1135
00:54:11,520 --> 00:54:13,040
What made a picture?

1136
00:54:13,160 --> 00:54:16,280
It looked like she had GPT actually, um, it was all text.

1137
00:54:16,520 --> 00:54:16,800
Yeah.

1138
00:54:17,040 --> 00:54:19,440
Um, and so you had a text based web browser.

1139
00:54:19,800 --> 00:54:22,240
Uh, well, actually the original browser, Tim, Tim Berners-Lee, the original,

1140
00:54:22,240 --> 00:54:24,800
the original browser, both the original browser and the server actually ran on

1141
00:54:24,800 --> 00:54:25,800
next, next cubes.

1142
00:54:26,240 --> 00:54:29,040
So these are, this was, you know, the computer Steve Jobs made during the

1143
00:54:29,040 --> 00:54:31,760
interim period when he, during the decade long interim period, when he was not

1144
00:54:31,800 --> 00:54:36,480
an apple, you know, he got fired in 85 and then came back in 97.

1145
00:54:36,480 --> 00:54:38,880
So this was in that interim period where he had this company called Next.

1146
00:54:38,880 --> 00:54:41,280
And they made these, literally these computers called cubes.

1147
00:54:41,880 --> 00:54:42,680
And there's this famous story.

1148
00:54:42,680 --> 00:54:46,560
They were beautiful, but they were 12 inch by 12 inch by 12 inch cubes computers.

1149
00:54:46,560 --> 00:54:49,040
And there's a famous story about how they could have cost half as much if it

1150
00:54:49,040 --> 00:54:53,760
had been 12 by 12 by 13, but Steve was like, no, like it has to be.

1151
00:54:54,160 --> 00:54:56,920
So they were like $6,000 basically academic workstations.

1152
00:54:56,920 --> 00:55:00,080
They had the first city round drives, um, which were slow.

1153
00:55:00,080 --> 00:55:02,680
I mean, it was the, the computers are all but unusable.

1154
00:55:02,840 --> 00:55:04,840
Um, they were so slow, but they were beautiful.

1155
00:55:05,160 --> 00:55:05,360
Okay.

1156
00:55:05,360 --> 00:55:07,400
Can we actually just take a tiny tangent there?

1157
00:55:07,400 --> 00:55:07,600
Sure.

1158
00:55:07,600 --> 00:55:07,840
Of course.

1159
00:55:09,160 --> 00:55:14,880
The, the 12 by 12 by 12, uh, they just so beautifully encapsulates Steve Jobs

1160
00:55:14,920 --> 00:55:15,800
idea of design.

1161
00:55:16,040 --> 00:55:20,760
Can you just comment on, um, what you find interesting about Steve Jobs?

1162
00:55:20,800 --> 00:55:26,000
What, uh, about that view of the world, that dogmatic pursuit of perfection and

1163
00:55:26,000 --> 00:55:27,720
how he saw perfection in design.

1164
00:55:28,440 --> 00:55:28,640
Yeah.

1165
00:55:28,640 --> 00:55:32,360
So I guess they say like, look, he was a deep believer, I think in a very deep way.

1166
00:55:32,360 --> 00:55:33,080
I interpret it.

1167
00:55:33,360 --> 00:55:35,560
I don't know if you ever really described it like this, but the way I'd interpret

1168
00:55:35,560 --> 00:55:37,200
it is it's, it's like, it's like this thing.

1169
00:55:37,200 --> 00:55:38,840
And it's, it's actually a thing in philosophy.

1170
00:55:38,840 --> 00:55:41,200
It's like aesthetics are not just appearances.

1171
00:55:41,240 --> 00:55:44,400
Aesthetics go all the way to like deep underlying, underlying meaning, right?

1172
00:55:44,400 --> 00:55:47,000
It's like, I'm not a physicist.

1173
00:55:47,000 --> 00:55:49,440
One of the things I've heard physicists say is one of the things you start to get

1174
00:55:49,440 --> 00:55:51,680
a sense of when a theory might be correct is when it's beautiful, right?

1175
00:55:51,680 --> 00:55:53,360
Like, you know, they're, right.

1176
00:55:53,360 --> 00:55:56,600
And so, so, so there's something and you feel the same thing, by the way, in

1177
00:55:56,600 --> 00:55:58,640
like human psychology, right?

1178
00:55:58,680 --> 00:56:00,720
You know, when, when you're experiencing awe, right?

1179
00:56:00,720 --> 00:56:03,120
You know, there's like a, there's like a, there's a simplicity too.

1180
00:56:03,120 --> 00:56:05,400
When, when you're having an honest interaction with somebody, there's an

1181
00:56:05,400 --> 00:56:08,720
aesthetic, I would say calm comes over you because you're actually being fully

1182
00:56:08,720 --> 00:56:10,080
honest and trying to hide yourself, right?

1183
00:56:10,080 --> 00:56:13,720
So there, so, so it's like this very deep sense of aesthetics.

1184
00:56:13,880 --> 00:56:16,720
And he would trust that judgment that he had deep down.

1185
00:56:16,720 --> 00:56:22,640
Like even, even if the engineering teams are saying this is, this is too difficult.

1186
00:56:22,680 --> 00:56:26,480
Even if the, whatever the finance folks are saying, this is ridiculous.

1187
00:56:26,520 --> 00:56:29,960
The supply chain, all that kind of stuff, this makes it impossible to, we

1188
00:56:29,960 --> 00:56:31,200
can't do this kind of material.

1189
00:56:31,880 --> 00:56:33,360
This has never been done before.

1190
00:56:33,400 --> 00:56:34,440
And so on and so forth.

1191
00:56:34,440 --> 00:56:35,520
He just sticks by it.

1192
00:56:35,840 --> 00:56:37,680
Well, I mean, who makes a phone out of aluminum, right?

1193
00:56:37,680 --> 00:56:41,040
Like, nobody else would have done that.

1194
00:56:41,640 --> 00:56:44,840
And now, of course, if your phone was made out of aluminum, why, you know, how crude,

1195
00:56:44,960 --> 00:56:47,200
what a kind of caveman would you have to be to have a phone that's made out of

1196
00:56:47,200 --> 00:56:47,920
plastic, like, right?

1197
00:56:47,920 --> 00:56:50,400
So like, so it's just this very, right?

1198
00:56:50,400 --> 00:56:52,960
And, you know, look, it's, there's a thousand different ways to look at this.

1199
00:56:52,960 --> 00:56:55,960
But one of the things is just like, look, these things are central to your life.

1200
00:56:56,000 --> 00:56:58,680
Like you're with your phone more than you're with anything else.

1201
00:56:58,680 --> 00:57:00,000
Like it's in your, it's going to be in your hand.

1202
00:57:00,000 --> 00:57:02,520
I mean, he, you know, you know, this, he thought very deeply about what it meant

1203
00:57:02,520 --> 00:57:03,720
for something to be in your hand all day long.

1204
00:57:04,200 --> 00:57:07,360
Well, for example, here's an interesting design thing.

1205
00:57:07,360 --> 00:57:10,720
Like he never wanted, it's my understanding is he never wanted an iPhone to

1206
00:57:10,720 --> 00:57:14,840
have a screen larger than you could reach with your thumb one handed.

1207
00:57:15,440 --> 00:57:18,320
And so he was actually opposed to the idea of making the phones larger.

1208
00:57:18,320 --> 00:57:20,440
And I don't know if you have this experience today, but let's say there

1209
00:57:20,440 --> 00:57:23,720
are certain moments in your day when you might be like only have one hand

1210
00:57:23,720 --> 00:57:25,440
available and you might want to be on your phone.

1211
00:57:25,640 --> 00:57:26,000
Yeah.

1212
00:57:26,080 --> 00:57:30,840
And you're trying to like, your thumb can't reach the send button.

1213
00:57:30,960 --> 00:57:31,200
Yeah.

1214
00:57:31,200 --> 00:57:32,560
I mean, there's pros and cons, right?

1215
00:57:32,560 --> 00:57:35,040
And then there's like folding phones, which I would love to know what he

1216
00:57:35,040 --> 00:57:36,160
thought and thinks about them.

1217
00:57:36,760 --> 00:57:39,920
Uh, but I mean, is there something you could also just link on?

1218
00:57:39,920 --> 00:57:45,000
Cause he's one of the interesting, um, figures in the history of technology.

1219
00:57:45,360 --> 00:57:49,520
What makes him, what makes him as successful as he was, what makes him as

1220
00:57:49,520 --> 00:57:55,200
interesting as he was, uh, what made him, um, so productive and important in, um,

1221
00:57:55,680 --> 00:57:57,520
in, in, in the development of technology.

1222
00:57:58,040 --> 00:57:59,280
He had an integrated worldview.

1223
00:57:59,280 --> 00:58:03,120
So the, the, the, the properly designed device that had the correct functionality

1224
00:58:03,120 --> 00:58:07,480
that had the deepest understanding of the user that was the most beautiful, right?

1225
00:58:07,480 --> 00:58:09,920
Like it had to be all of those things, right?

1226
00:58:09,920 --> 00:58:13,200
It was, he basically would drive to as close to perfect as you could possibly

1227
00:58:13,200 --> 00:58:13,800
get, right?

1228
00:58:13,800 --> 00:58:16,440
And I, you know, I suspect that he never quite, you know, thought he ever

1229
00:58:16,440 --> 00:58:18,920
got there cause most great creators, you know, are generally dissatisfied.

1230
00:58:19,040 --> 00:58:21,720
You know, you read accounts later on and all they can, all they can see are the

1231
00:58:21,720 --> 00:58:24,240
flaws in their creation, but like he got as close to perfect each step of the

1232
00:58:24,240 --> 00:58:27,680
way as he could possibly get with the, with the constraints of the, of the

1233
00:58:27,680 --> 00:58:28,640
technology of his time.

1234
00:58:28,800 --> 00:58:31,760
Um, and then, you know, look, he was, you know, sort of famous in the Apple model.

1235
00:58:31,760 --> 00:58:34,360
It's like, look, they, they, they will, you know, this, this headset that they

1236
00:58:34,360 --> 00:58:37,800
just came out with, like that, you know, it's like a decade long project, right?

1237
00:58:37,800 --> 00:58:40,920
It's like, and they're just going to sit there and tune and tune and polish and

1238
00:58:40,920 --> 00:58:44,040
polish and tune and polish and tune and polish until it is as perfect as anybody

1239
00:58:44,040 --> 00:58:45,040
could possibly make anything.

1240
00:58:45,480 --> 00:58:48,480
And then this goes to the, the, the way that people describe working with him was,

1241
00:58:48,520 --> 00:58:51,720
which is, you know, there was a terrifying aspect of working with him, which is,

1242
00:58:51,720 --> 00:58:53,120
you know, he was, you know, he was very tough.

1243
00:58:53,680 --> 00:58:57,600
Um, but there was this thing that everybody I've ever talked to work for him

1244
00:58:57,600 --> 00:59:01,680
says that they all say the following, which is he, we did the best work of our

1245
00:59:01,680 --> 00:59:05,120
lives when we worked for him because he set the bar incredibly high and then he

1246
00:59:05,120 --> 00:59:07,640
supported us with everything that he could to let us actually do work of that

1247
00:59:07,640 --> 00:59:08,000
quality.

1248
00:59:08,560 --> 00:59:11,440
So a lot of people who were at Apple spend the rest of their lives trying to

1249
00:59:11,440 --> 00:59:13,760
find another experience where they feel like they're able to hit that quality

1250
00:59:13,760 --> 00:59:14,200
bar again.

1251
00:59:14,520 --> 00:59:18,080
Even if it, in retrospect, we're doing it felt like suffering.

1252
00:59:18,120 --> 00:59:18,840
Yeah, exactly.

1253
00:59:19,840 --> 00:59:23,280
What does that teach you about the human condition?

1254
00:59:23,880 --> 00:59:26,240
So look, exactly.

1255
00:59:26,240 --> 00:59:29,440
So the Silicon Valley, I mean, look, he's not, you know, George Patton in the,

1256
00:59:29,480 --> 00:59:32,560
you know, in the army, like, you know, there are many examples in other fields,

1257
00:59:32,600 --> 00:59:38,720
you know, that are like this, um, uh, uh, this is specifically in tech.

1258
00:59:38,760 --> 00:59:40,120
It's actually, I find it very interesting.

1259
00:59:40,120 --> 00:59:43,400
There's the Apple way, which is polish, polish, polish and don't ship until it's

1260
00:59:43,400 --> 00:59:44,480
as perfect as you can make it.

1261
00:59:44,520 --> 00:59:48,280
And then there's the sort of the other approach, which is the sort of incremental

1262
00:59:48,600 --> 00:59:52,120
hacker mentality, which basically says ship early and often and iterate.

1263
00:59:52,480 --> 00:59:55,520
And one of the things I find really interesting is I'm now 30 years into this,

1264
00:59:55,560 --> 00:59:59,880
like there are very successful companies on both sides of that approach.

1265
01:00:00,120 --> 01:00:00,440
Right.

1266
01:00:01,000 --> 01:00:04,600
Um, like that is a fundamental difference, right?

1267
01:00:04,600 --> 01:00:08,120
And how to operate and how to build and how to create that you have world

1268
01:00:08,120 --> 01:00:09,880
class companies operating in both ways.

1269
01:00:10,360 --> 01:00:13,640
Um, and I don't think the question of like, which is the superior model is

1270
01:00:13,640 --> 01:00:15,000
anywhere close to being answered.

1271
01:00:15,440 --> 01:00:18,200
Like, and my suspicion is the answer is do both.

1272
01:00:18,200 --> 01:00:21,160
The answer is you actually want both, they lead to different outcomes.

1273
01:00:21,440 --> 01:00:25,560
Software tends to do better, um, with the iterative approach, um, hardware

1274
01:00:25,560 --> 01:00:29,880
tends to do better with the, uh, you know, sort of wait and make it perfect approach.

1275
01:00:29,880 --> 01:00:33,440
But again, you can find examples in, in, in, in both directions.

1276
01:00:33,720 --> 01:00:35,480
Oh, so the juror's still out on that one.

1277
01:00:36,200 --> 01:00:37,400
Uh, so back to Mosaic.

1278
01:00:37,400 --> 01:00:43,400
So what, uh, it was text based, uh, Tim Berners-Lee.

1279
01:00:43,880 --> 01:00:46,120
Well, there was the web, which was text based, but there were no,

1280
01:00:46,120 --> 01:00:47,720
I mean, there was like three websites.

1281
01:00:47,720 --> 01:00:49,160
There was like no content.

1282
01:00:49,160 --> 01:00:50,040
There were no users.

1283
01:00:50,120 --> 01:00:52,040
Like it, it wasn't like a, it wasn't like a catalytic.

1284
01:00:52,040 --> 01:00:54,200
It hadn't, by the way, it was all, because it was all texts.

1285
01:00:54,200 --> 01:00:54,840
There were no documents.

1286
01:00:54,840 --> 01:00:55,400
There are no images.

1287
01:00:55,400 --> 01:00:56,040
There are no videos.

1288
01:00:56,040 --> 01:00:57,320
There were no, right.

1289
01:00:57,320 --> 01:01:00,680
So, so it was, it was, and then if, in the beginning, if you had to be on

1290
01:01:00,680 --> 01:01:03,960
a next cube, but you need to have a next cube both to publish and to consume.

1291
01:01:03,960 --> 01:01:05,320
So, so there were.

1292
01:01:05,320 --> 01:01:06,440
6,000 bucks, you said.

1293
01:01:06,440 --> 01:01:08,600
There were limitations on, yeah, $6,000 PC.

1294
01:01:08,600 --> 01:01:11,560
They did not, they did not sell very many, but then there was also,

1295
01:01:12,200 --> 01:01:14,440
there was also FTP and there was Usenets, right?

1296
01:01:14,440 --> 01:01:17,160
And there was, you know, a dozen other, basically, there's Waste,

1297
01:01:17,160 --> 01:01:18,600
which was an early search thing.

1298
01:01:18,600 --> 01:01:21,560
There was Gopher, which was an early menu based information retrieval system.

1299
01:01:21,560 --> 01:01:24,920
There were like a dozen different sort of scattered ways that people would get

1300
01:01:24,920 --> 01:01:26,280
to information on, on the internet.

1301
01:01:26,280 --> 01:01:29,160
And so the mosaic idea was basically bring those all together,

1302
01:01:29,160 --> 01:01:31,320
make the whole thing graphical, make it easy to use,

1303
01:01:31,320 --> 01:01:33,320
make it basically bulletproof so that anybody can do it.

1304
01:01:33,960 --> 01:01:36,920
And then again, just on the luck side, it so happened that this was right at the

1305
01:01:36,920 --> 01:01:40,120
moment when graphics, when the GUI sort of actually took off.

1306
01:01:40,120 --> 01:01:43,240
And we're now also used to the GUI that we think it's been around forever,

1307
01:01:43,240 --> 01:01:47,400
but it didn't really, you know, the Macintosh brought it out in 85,

1308
01:01:47,400 --> 01:01:49,800
but they actually didn't sell very many Macs in the 80s.

1309
01:01:49,800 --> 01:01:51,400
It was not that successful of a product.

1310
01:01:52,280 --> 01:01:56,840
It really was, you needed Windows 3.0 on PCs and that hit in about 92.

1311
01:01:58,200 --> 01:01:59,960
And so, and we did mosaic in 92, 93.

1312
01:01:59,960 --> 01:02:02,920
So that sort of, it was like right at the moment when you could imagine

1313
01:02:02,920 --> 01:02:08,280
actually having a graphical user interface to write at all, much less one to the internet.

1314
01:02:08,440 --> 01:02:11,560
How old did Windows 3.0 sell?

1315
01:02:11,560 --> 01:02:13,080
So it was at the really big.

1316
01:02:13,080 --> 01:02:13,800
That was the big bang.

1317
01:02:13,800 --> 01:02:16,600
The big operating, graphical operating system.

1318
01:02:16,600 --> 01:02:19,560
Well, this is the classic, okay, this Microsoft was operating on the other.

1319
01:02:19,560 --> 01:02:22,360
So Steve, Steve, Apple was running on the Polish until it was perfect.

1320
01:02:22,360 --> 01:02:24,680
Microsoft famously ran on the other model, which is ship and iterate.

1321
01:02:24,680 --> 01:02:26,920
And so in the old line in those days was Microsoft,

1322
01:02:26,920 --> 01:02:28,600
right, it's the version three of every Microsoft product.

1323
01:02:28,600 --> 01:02:29,800
That's the good one, right?

1324
01:02:29,800 --> 01:02:33,640
And so there are, you can find online, Windows 1, Windows 2, nobody used them.

1325
01:02:34,440 --> 01:02:37,000
Actually, the original Windows, in the original Microsoft Windows,

1326
01:02:37,000 --> 01:02:38,120
the Windows were not overlapping.

1327
01:02:39,320 --> 01:02:42,280
And so you had these very small, very low resolution screens.

1328
01:02:42,280 --> 01:02:45,560
And then you had literally, it just didn't work.

1329
01:02:45,560 --> 01:02:46,360
It wasn't ready yet.

1330
01:02:46,360 --> 01:02:49,720
Well, and Windows 95, I think was a pretty big leap also.

1331
01:02:49,720 --> 01:02:50,520
That was a big leap too.

1332
01:02:50,520 --> 01:02:50,920
Yeah.

1333
01:02:50,920 --> 01:02:52,200
So that was like bang, bang.

1334
01:02:52,840 --> 01:02:55,720
And then of course, Steve, and then, you know, in the fall of the time,

1335
01:02:55,720 --> 01:02:57,560
Steve came back, then the Mac started to take off again.

1336
01:02:57,560 --> 01:02:58,360
That was the third bang.

1337
01:02:58,360 --> 01:02:59,720
And then the iPhone was the fourth bang.

1338
01:03:00,360 --> 01:03:01,320
Such exciting time.

1339
01:03:01,320 --> 01:03:02,920
And then we were off to the races.

1340
01:03:02,920 --> 01:03:06,760
Because nobody could have known or be created from that.

1341
01:03:06,760 --> 01:03:11,240
Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone was only 15 years,

1342
01:03:12,680 --> 01:03:12,920
right?

1343
01:03:12,920 --> 01:03:15,320
Like that ramp was in retrospect.

1344
01:03:15,320 --> 01:03:16,600
At the time, it felt like it took forever.

1345
01:03:16,600 --> 01:03:20,920
But in historical terms, like that was a very fast ramp from even a graphical

1346
01:03:20,920 --> 01:03:24,680
computer at all on your desk to the iPhone, it was 15 years.

1347
01:03:24,680 --> 01:03:27,240
Did you have a sense of what the Internet will be as you're looking

1348
01:03:27,240 --> 01:03:28,440
through the window of Mosaic?

1349
01:03:28,440 --> 01:03:33,160
Like what, like there's just a few web pages for now.

1350
01:03:33,880 --> 01:03:37,720
So the thing I had early on was I was keeping at the time what,

1351
01:03:38,760 --> 01:03:40,360
there's disputes over what was the first blog.

1352
01:03:40,360 --> 01:03:45,240
But I had one of them that at least is a possible, at least a runner-up in the

1353
01:03:45,240 --> 01:03:45,960
competition.

1354
01:03:45,960 --> 01:03:47,640
And it was what was called the What's New page.

1355
01:03:49,480 --> 01:03:53,320
And it was, it was a hardwired, I had distribution and fair advantage.

1356
01:03:53,320 --> 01:03:54,920
I put it right in the browser.

1357
01:03:55,560 --> 01:03:57,560
I put it in the browser and then I put my resume in the browser.

1358
01:03:57,560 --> 01:03:57,720
Yeah.

1359
01:03:57,720 --> 01:03:59,800
Also was hilarious.

1360
01:03:59,800 --> 01:04:06,200
But I was keeping the, not many people get to do that.

1361
01:04:10,200 --> 01:04:11,240
Good call.

1362
01:04:11,240 --> 01:04:12,520
And early days.

1363
01:04:12,520 --> 01:04:13,080
Yes.

1364
01:04:13,080 --> 01:04:14,360
It's so interesting.

1365
01:04:14,360 --> 01:04:17,240
I'm looking for my, about, about, oh, Mark is looking for a job.

1366
01:04:21,000 --> 01:04:24,120
So the What's New page I would literally get up every morning and I would,

1367
01:04:24,120 --> 01:04:24,760
every afternoon.

1368
01:04:25,400 --> 01:04:28,920
And I would basically, if you wanted to launch a website, you would email me.

1369
01:04:29,800 --> 01:04:30,920
And I would list it on the What's New page.

1370
01:04:30,920 --> 01:04:33,960
And that was how people discovered the new websites as they were coming out.

1371
01:04:33,960 --> 01:04:36,520
And I remember, because it was like one, it literally went for me.

1372
01:04:36,520 --> 01:04:41,160
It was like one every couple of days, until like one every day, until like two every day.

1373
01:04:42,840 --> 01:04:45,960
So you're doing, so that, that blog was kind of doing the directory thing.

1374
01:04:45,960 --> 01:04:47,640
So like, what was the homepage?

1375
01:04:48,280 --> 01:04:51,080
So the homepage was just basically trying to explain even what this thing is that you're

1376
01:04:51,080 --> 01:04:51,880
looking at, right?

1377
01:04:51,880 --> 01:04:53,560
The basic, basically basic instructions.

1378
01:04:54,440 --> 01:04:56,440
But then there was a button, there was a button that said What's New.

1379
01:04:56,440 --> 01:04:59,560
And what most people did was they went to, for obvious reasons, went to What's New.

1380
01:04:59,640 --> 01:05:03,000
But like, it was so, it was so mind blowing at that point.

1381
01:05:03,000 --> 01:05:04,200
Just the basic idea.

1382
01:05:04,200 --> 01:05:06,280
And it was just like, you know, this is basically the internet,

1383
01:05:06,280 --> 01:05:07,960
but people could see it for the first time.

1384
01:05:07,960 --> 01:05:10,760
The basic idea was, look, you know, some, you know, it's like, literally,

1385
01:05:10,760 --> 01:05:14,520
it's like an Indian restaurant in like Bristol, England has like put their menu on the web.

1386
01:05:14,520 --> 01:05:20,280
And people were like, wow, because like that's the first restaurant menu on the web.

1387
01:05:20,760 --> 01:05:21,960
And I don't have to be in Bristol.

1388
01:05:21,960 --> 01:05:23,240
And I don't know if I'm ever going to go to Bristol.

1389
01:05:23,240 --> 01:05:24,440
And I don't like Indian food.

1390
01:05:24,440 --> 01:05:26,200
And like, wow, right.

1391
01:05:26,680 --> 01:05:27,560
And it was like that.

1392
01:05:27,560 --> 01:05:32,520
The first web, the first streaming video thing was a, it was another England thing,

1393
01:05:32,520 --> 01:05:33,560
some Oxford or something.

1394
01:05:34,520 --> 01:05:40,040
Some guy put his coffee pot up as the first streaming video thing.

1395
01:05:40,040 --> 01:05:43,320
And he put it on the web because he literally, it was the coffee pot down the hall.

1396
01:05:43,320 --> 01:05:45,640
And he wanted to see when he needed to go refill it.

1397
01:05:46,360 --> 01:05:48,920
But there were, you know, there was a point when there were thousands of people

1398
01:05:48,920 --> 01:05:51,960
like watching that coffee pot, because it was the first thing you could watch.

1399
01:05:52,680 --> 01:06:00,120
But isn't, were you able to kind of infer, you know, if that Indian restaurant could go online?

1400
01:06:00,120 --> 01:06:00,680
Yeah.

1401
01:06:00,680 --> 01:06:02,200
Then you're like, they all will.

1402
01:06:02,200 --> 01:06:03,000
They all will.

1403
01:06:03,000 --> 01:06:03,800
Yeah, exactly.

1404
01:06:03,800 --> 01:06:04,600
So you felt that.

1405
01:06:04,600 --> 01:06:05,320
Yeah, yeah, yeah.

1406
01:06:05,320 --> 01:06:06,760
Now, you know, look, it's still a stretch, right?

1407
01:06:06,760 --> 01:06:08,600
It's still a stretch because it's just like, okay, is it, you know,

1408
01:06:08,600 --> 01:06:10,680
you're still in this zone, which is like, okay, is this a nerd thing?

1409
01:06:10,680 --> 01:06:11,560
Is this a real person thing?

1410
01:06:11,560 --> 01:06:12,360
Yeah.

1411
01:06:12,360 --> 01:06:15,400
Um, by the way, we, you know, there was a wall of skepticism from the media.

1412
01:06:15,400 --> 01:06:18,520
Like they just, like everybody was just like, yeah, this is just like them.

1413
01:06:18,520 --> 01:06:20,760
This is not, you know, this is not for regular people at that time.

1414
01:06:20,760 --> 01:06:23,080
Um, and so you had to think through that.

1415
01:06:23,080 --> 01:06:26,600
And then look, it was still, it was still hard to get on the internet at that point,

1416
01:06:26,600 --> 01:06:26,920
right?

1417
01:06:26,920 --> 01:06:30,200
So you could get kind of this weird bastardized version if you were on AOL,

1418
01:06:30,200 --> 01:06:34,200
which wasn't really real, or you had to go like learn what an ISP was.

1419
01:06:35,480 --> 01:06:38,840
You know, in those days, PCs actually didn't have TCP-IP drivers come pre-installed.

1420
01:06:38,840 --> 01:06:41,400
So you had to learn what a TCP-IP driver was.

1421
01:06:41,400 --> 01:06:42,360
You had to buy a modem.

1422
01:06:42,360 --> 01:06:43,640
You had to install driver software.

1423
01:06:44,760 --> 01:06:47,400
I have a comedy routine I do, something like 20 minutes long,

1424
01:06:47,400 --> 01:06:49,960
describing all the steps required to actually get on the internet at this point.

1425
01:06:50,840 --> 01:06:55,880
And so you had to, you had to look through these practical, well, and then, and then speed

1426
01:06:55,880 --> 01:06:58,920
performance, 14 form modems, right?

1427
01:06:58,920 --> 01:07:03,080
Like it was like watching, you know, glue dry, um, like, and so you had to, you had to,

1428
01:07:03,080 --> 01:07:05,960
there were basically a sequence of bets that we made where you basically needed to look through

1429
01:07:05,960 --> 01:07:09,480
that current state of affairs and say, actually, there's going to be so much demand for that.

1430
01:07:09,480 --> 01:07:12,040
Once people figure this out, there's going to be so much demand for it that all of these

1431
01:07:12,040 --> 01:07:13,560
practical problems are going to get fixed.

1432
01:07:14,280 --> 01:07:21,000
Some people say that the anticipation makes the destination that much more exciting.

1433
01:07:21,000 --> 01:07:22,200
Do you remember progressive JPEGs?

1434
01:07:22,760 --> 01:07:25,480
Yeah. Do I? Do I?

1435
01:07:25,480 --> 01:07:27,560
So for kids in the audience, right?

1436
01:07:27,560 --> 01:07:28,760
For kids in the audience.

1437
01:07:28,760 --> 01:07:32,840
You used to have to wash an image load like a line at a time, but it turns out there was this

1438
01:07:32,840 --> 01:07:37,320
thing with JPEGs where you could load basically every fourth, you could load like every fourth

1439
01:07:37,880 --> 01:07:40,120
line and then you could sweep back through again.

1440
01:07:40,120 --> 01:07:43,400
And so you could like render a fuzzy version of the image up front and then it would like

1441
01:07:43,400 --> 01:07:44,680
resolve into the detailed one.

1442
01:07:44,680 --> 01:07:47,960
And that was like a big UI breakthrough because it gave you something to watch.

1443
01:07:49,240 --> 01:07:53,400
Yeah. And, uh, you know, there's applications in various domains for that.

1444
01:07:55,960 --> 01:07:56,600
Well, it's a big fight.

1445
01:07:56,600 --> 01:07:58,840
If there was a big fight early on about whether there should be images on the web.

1446
01:07:59,880 --> 01:08:01,880
For that reason, for like sexualization.

1447
01:08:01,880 --> 01:08:04,760
No, not explicitly, that did come up, but it wasn't even that.

1448
01:08:04,760 --> 01:08:06,280
It was more just like all the serious info.

1449
01:08:06,280 --> 01:08:09,880
The argument went, the purists basically said all the serious information in the world is text.

1450
01:08:10,520 --> 01:08:13,480
If you introduce images, you basically are going to bring in all the trivial stuff.

1451
01:08:13,480 --> 01:08:17,160
You're going to bring in magazines and, you know, all this crazy stuff that, you know,

1452
01:08:17,160 --> 01:08:18,920
people, you know, it's going to distract from that.

1453
01:08:18,920 --> 01:08:21,400
It's going to go take, take the way from being serious to being frivolous.

1454
01:08:21,400 --> 01:08:27,240
Well, was there any doomer type arguments about, uh, the internet destroying all of human

1455
01:08:27,240 --> 01:08:32,680
civilization or destroying some fundamental fabric of human civilization?

1456
01:08:32,680 --> 01:08:35,160
Yeah. So those days it was all around crime and terrorism.

1457
01:08:36,120 --> 01:08:40,040
So those arguments happened, you know, but there was no sense yet of the internet having

1458
01:08:40,040 --> 01:08:42,680
like an effect on politics or because that was, that was way too far off.

1459
01:08:42,680 --> 01:08:46,520
But there was an enormous panic at the time around cybercrime.

1460
01:08:46,520 --> 01:08:49,640
There was like enormous panic that like your credit card number would get stolen and you'd

1461
01:08:49,640 --> 01:08:51,320
use life savings to be drained.

1462
01:08:51,320 --> 01:08:54,680
And then, you know, criminals were going to, there was, oh, when we started, one of the things

1463
01:08:54,680 --> 01:08:59,160
we did, one of the, the Netscape browser was the first widely used piece of consumer software

1464
01:08:59,160 --> 01:09:02,600
that had strong encryption built in, made it available to ordinary people.

1465
01:09:02,600 --> 01:09:06,440
And at that time, strong encryption was actually illegal to export out of the U.S.

1466
01:09:07,160 --> 01:09:09,240
So we could feel that product in the U.S.

1467
01:09:09,240 --> 01:09:12,120
We could not export it because it was, it was classified as ammunition.

1468
01:09:12,760 --> 01:09:15,800
So the Netscape browser was on a restricted list along with the Tom Huck missile

1469
01:09:16,440 --> 01:09:17,880
as being something that could not be exported.

1470
01:09:17,880 --> 01:09:21,880
So we had to make a second version with deliberately weak encryption to sell overseas

1471
01:09:21,880 --> 01:09:25,960
with a big logo on the box saying do not trust this, which it turns out makes it hard to sell

1472
01:09:25,960 --> 01:09:29,400
software when it's got a big logo that says don't trust it.

1473
01:09:30,280 --> 01:09:33,160
And then we had to spend five years fighting the U.S. government to get them to basically

1474
01:09:33,160 --> 01:09:34,440
stop trying to do this.

1475
01:09:35,640 --> 01:09:39,400
Because the fear was terrorists are going to use encryption to like plot, you know,

1476
01:09:39,400 --> 01:09:40,840
all these things.

1477
01:09:42,040 --> 01:09:45,400
And then, you know, we responded with, well, actually we need encryption to be able

1478
01:09:45,400 --> 01:09:48,040
to secure systems so that the terrorists and the criminals can't get into them.

1479
01:09:48,040 --> 01:09:50,920
So that was, anyway, that was the 1990s fight.

1480
01:09:50,920 --> 01:09:55,080
So can you say something about some of the details of the software engineering

1481
01:09:55,800 --> 01:09:57,880
challenges required to build these browsers?

1482
01:09:57,880 --> 01:10:02,280
I mean, the engineering challenges of creating a product that hasn't really existed before

1483
01:10:03,000 --> 01:10:09,720
that can have such almost like limitless impact on the world with the internet.

1484
01:10:09,720 --> 01:10:12,840
So there was a really key bet that we made at the time, which is very controversial,

1485
01:10:12,840 --> 01:10:16,120
which was core to the core to how it was engineered, which was are we optimizing for

1486
01:10:16,120 --> 01:10:18,280
performance or for ease of creation?

1487
01:10:19,000 --> 01:10:22,200
And in those days, the pressure was very intense to optimize for performance because

1488
01:10:22,200 --> 01:10:25,480
the network connections were so slow and also the computers were so slow.

1489
01:10:26,280 --> 01:10:29,320
And so if you had mentioned the progressive JPEG, it's like,

1490
01:10:30,120 --> 01:10:34,200
if there's an alternate world in which we optimize for performance and it just,

1491
01:10:34,200 --> 01:10:36,280
you had just a much more pleasant experience right up front.

1492
01:10:37,160 --> 01:10:40,520
But what we got by not doing that was we got ease of creation.

1493
01:10:40,520 --> 01:10:45,720
And the way that we got ease of creation was all of the protocols and formats were in text,

1494
01:10:45,720 --> 01:10:46,600
not in binary.

1495
01:10:47,640 --> 01:10:50,920
And so HTTP is in text, by the way, and this was an internet tradition, by the way,

1496
01:10:50,920 --> 01:10:52,440
that we picked up, but we continued it.

1497
01:10:52,440 --> 01:10:57,160
HTTP is text and HTML is text and then everything else that followed is text.

1498
01:10:58,040 --> 01:11:01,560
As a result, and by the way, you can imagine purist engineer saying this is insane,

1499
01:11:01,560 --> 01:11:04,600
you have very limited bandwidth, why are you wasting any time sending text?

1500
01:11:04,600 --> 01:11:07,160
You should be encoding the stuff into binary and it'll be much faster.

1501
01:11:07,160 --> 01:11:08,520
Of course, the answer is that's correct.

1502
01:11:09,400 --> 01:11:11,800
But what you get when you make it text is all of a sudden, well,

1503
01:11:11,800 --> 01:11:13,880
the big breakthrough was the view source function, right?

1504
01:11:13,880 --> 01:11:17,160
So the fact that you could look at a web page, you could hit view source and you could see the

1505
01:11:17,160 --> 01:11:20,360
HTML, that was how people learned how to make web pages, right?

1506
01:11:20,440 --> 01:11:22,920
It's so interesting because the stuff we take for granted now

1507
01:11:24,600 --> 01:11:29,480
is, man, that was fundamental to the development of the web to be able to have HTML just right

1508
01:11:29,480 --> 01:11:30,200
there.

1509
01:11:30,200 --> 01:11:36,280
All the ghetto mess that is HTML, all the sort of almost biological messiness

1510
01:11:37,320 --> 01:11:43,640
of HTML and then having the browser try to interpret that mess to show something reasonable.

1511
01:11:43,640 --> 01:11:46,360
Well, and then there was this internet principle that we inherited, which was

1512
01:11:46,440 --> 01:11:50,360
emit, what was it, emit cautiously, emit conservatively interpret liberally.

1513
01:11:50,360 --> 01:11:55,080
So it basically meant, the design principle was if you're creating a web editor that's

1514
01:11:55,080 --> 01:11:59,480
going to emit HTML, do it as cleanly as you can, but you actually want the browser to interpret

1515
01:11:59,480 --> 01:12:02,840
liberally, which is you actually want users to be able to make all kinds of mistakes

1516
01:12:02,840 --> 01:12:03,800
and for it to still work.

1517
01:12:04,360 --> 01:12:07,000
And so the browser rendering engines to this day have all of this

1518
01:12:07,560 --> 01:12:12,280
skinny code crazy stuff where they're resilient to all kinds of crazy HTML mistakes.

1519
01:12:12,280 --> 01:12:15,800
And literally what I always had in my head is there's an eight-year-old or an 11-year-old

1520
01:12:15,800 --> 01:12:17,960
somewhere and they're doing a view source, they're doing a cut and paste and they're

1521
01:12:17,960 --> 01:12:20,280
trying to make a web page for their turtle or whatever.

1522
01:12:20,840 --> 01:12:23,880
And they leave out a slash and they leave out an angle bracket and they do this and they do

1523
01:12:23,880 --> 01:12:24,840
that and it still works.

1524
01:12:25,960 --> 01:12:26,760
It's also like that.

1525
01:12:26,760 --> 01:12:34,040
I don't often think about this, but programming, C++, C++, all those languages, Lisp, the compile

1526
01:12:34,040 --> 01:12:38,920
language, the interpretive language, Python, Perl, all of that, the brace has to be all

1527
01:12:38,920 --> 01:12:39,400
correct.

1528
01:12:39,400 --> 01:12:39,880
Yes.

1529
01:12:39,880 --> 01:12:41,080
Like everything has to be perfect.

1530
01:12:41,080 --> 01:12:41,400
Brutal.

1531
01:12:42,360 --> 01:12:42,920
And then.

1532
01:12:42,920 --> 01:12:43,480
Autistic.

1533
01:12:43,480 --> 01:12:44,120
You forget.

1534
01:12:45,080 --> 01:12:45,400
All right.

1535
01:12:46,280 --> 01:12:47,880
It's systematic and rigorous.

1536
01:12:47,880 --> 01:12:48,440
Let's go there.

1537
01:12:49,320 --> 01:12:59,480
But you forget that the web with JavaScript eventually and HTML is allowed to be messy in

1538
01:12:59,480 --> 01:13:05,960
the way, for the first time, messy in the way biological systems could be messy.

1539
01:13:05,960 --> 01:13:09,960
It's like the only thing computers were allowed to be messy on for the first time.

1540
01:13:09,960 --> 01:13:10,760
It used to offend me.

1541
01:13:10,760 --> 01:13:13,560
So I grew up in UNIX, I worked on UNIX.

1542
01:13:13,640 --> 01:13:15,960
I was a UNIX native all the way through this period.

1543
01:13:15,960 --> 01:13:19,880
And so, and it used to drive me bananas when it would do the segmentation fault in the

1544
01:13:19,880 --> 01:13:23,800
core dump file, just like it's like literally there's like an error in the code.

1545
01:13:23,800 --> 01:13:26,600
The math is off by one and it core dumps.

1546
01:13:26,600 --> 01:13:29,080
And I'm in the core dump trying to analyze it and trying to reconstruct.

1547
01:13:29,080 --> 01:13:30,600
And I'm just like, this is ridiculous.

1548
01:13:30,600 --> 01:13:33,560
Like the computer ought to be smart enough to be able to know that if it's off by one,

1549
01:13:33,560 --> 01:13:34,200
okay, fine.

1550
01:13:34,200 --> 01:13:35,000
And it keeps running.

1551
01:13:35,640 --> 01:13:38,040
And I would go ask all the experts, like, why can't it just keep running?

1552
01:13:38,040 --> 01:13:40,840
And they'd explain to me, well, because all the downstream repercussions and blah, blah.

1553
01:13:40,840 --> 01:13:47,880
And I'm like, we're forcing the human creator to live, to your point,

1554
01:13:47,880 --> 01:13:50,200
in this hyperlittoral world of perfection.

1555
01:13:50,760 --> 01:13:53,320
And that's just bad.

1556
01:13:53,320 --> 01:13:56,840
And by the way, what happens with that, of course, is what happened with coding at that

1557
01:13:56,840 --> 01:13:58,120
point, which is you get a high priesthood.

1558
01:13:59,160 --> 01:14:02,200
There's a small number of people who are really good at doing exactly that.

1559
01:14:02,200 --> 01:14:04,520
Most people can't and most people are excluded from it.

1560
01:14:04,520 --> 01:14:10,520
And so actually, that was where I picked up that idea was like, no, you want these things

1561
01:14:10,520 --> 01:14:12,600
to be resilient to error in all kinds.

1562
01:14:12,600 --> 01:14:13,800
And this would drive the purists.

1563
01:14:13,800 --> 01:14:14,360
Absolutely crazy.

1564
01:14:14,360 --> 01:14:17,480
Like I got attacked on this like a lot because, yeah, I mean, like every time I,

1565
01:14:17,480 --> 01:14:20,280
you know, all the purists who are like into all this like Markov language stuff and

1566
01:14:20,280 --> 01:14:23,080
formats and codes and all this stuff, they would be like, you know, you can't,

1567
01:14:23,080 --> 01:14:24,520
you're encouraging bad behavior because.

1568
01:14:25,080 --> 01:14:30,200
Oh, so they wanted the browser to give you a segfault error anytime there was a mismatch?

1569
01:14:30,200 --> 01:14:31,640
Yeah, yeah, they wanted it to be a kind of, right?

1570
01:14:31,640 --> 01:14:36,360
They wanted, yeah, that was a very, any properly trained and credentialed engineer

1571
01:14:37,160 --> 01:14:38,920
would be like, that's not how you build these systems.

1572
01:14:38,920 --> 01:14:41,480
That's such a bold move to say, no, it doesn't have to be.

1573
01:14:41,480 --> 01:14:41,720
Yeah.

1574
01:14:41,720 --> 01:14:44,760
Now, like I said, the good news for me is the internet kind of had that tradition already.

1575
01:14:45,320 --> 01:14:48,440
But we, but having said that, like we pushed it, we pushed it way out.

1576
01:14:48,440 --> 01:14:50,840
But the other thing we did going back to the performance thing was we gave up a

1577
01:14:50,840 --> 01:14:53,720
lot of performance we made that that initial experience for the first few years was pretty

1578
01:14:53,720 --> 01:14:54,520
painful.

1579
01:14:54,520 --> 01:14:58,280
But, but the bet there was actually an economic bet, which was basically the demand for the

1580
01:14:58,280 --> 01:15:02,440
web would basically mean that there would be a surge in supply of broadband because

1581
01:15:03,320 --> 01:15:06,920
the question was, okay, how do you get, how do you, how do you get the phone companies

1582
01:15:06,920 --> 01:15:11,720
which are not famous in those days for doing new things at huge cost for like speculative

1583
01:15:11,720 --> 01:15:12,040
reasons?

1584
01:15:12,040 --> 01:15:15,800
Like how do you get them to build up broadband, you know, spend billions of dollars doing that

1585
01:15:15,800 --> 01:15:18,600
and, you know, you could go meet with them and try to talk them into it, or you could

1586
01:15:18,600 --> 01:15:21,960
just have a thing where it's just very clear that it's going to be, that the people love,

1587
01:15:21,960 --> 01:15:23,800
it's going to be better if it's faster.

1588
01:15:23,800 --> 01:15:27,640
And so that, there was a period there, and this was, this was fraught with some peril,

1589
01:15:27,640 --> 01:15:31,480
but there was a period there where it's like, we knew the experience was suboptimized because

1590
01:15:31,480 --> 01:15:36,440
we were trying to force the emergence of demand for broadband, which is in fact, what happened?

1591
01:15:37,400 --> 01:15:41,320
So you had to figure out how to display this text, HTML text.

1592
01:15:41,960 --> 01:15:45,320
So the blue links and the purple links, and there's no standards.

1593
01:15:45,320 --> 01:15:46,760
Is there standards at that time?

1594
01:15:47,800 --> 01:15:48,680
There really still isn't.

1595
01:15:50,280 --> 01:15:54,200
Well, there's like, there's implied, implied standards, right?

1596
01:15:54,200 --> 01:15:57,960
And they, you know, there's all these kinds of new features that are being added like CSS,

1597
01:15:57,960 --> 01:16:02,440
what like, what kind of stuff a browser should be able to support, features with the languages,

1598
01:16:02,520 --> 01:16:03,800
within JavaScript and so on.

1599
01:16:04,360 --> 01:16:10,520
But you, you're setting standards on the fly yourself.

1600
01:16:11,080 --> 01:16:15,720
Well, to this day, if you, if you create a webpage that has no CSS style sheet,

1601
01:16:15,720 --> 01:16:18,200
the browser will render it however it wants to, right?

1602
01:16:18,200 --> 01:16:22,760
So this was one of the things, there was this idea, this idea at the time and how these systems

1603
01:16:22,760 --> 01:16:27,800
were built, which is separation of content from format or separation of content from appearance.

1604
01:16:28,920 --> 01:16:32,120
And that's still, people don't really use that anymore, because everybody wants to determine

1605
01:16:32,120 --> 01:16:33,560
how things look and so they use CSS.

1606
01:16:33,560 --> 01:16:37,080
But it's still in there that you can just let the browser do all the work.

1607
01:16:37,080 --> 01:16:44,040
I still like the, like really basic websites, but that could be just old school.

1608
01:16:44,040 --> 01:16:49,400
Kids these days with their fancy responsive websites that don't actually have much content,

1609
01:16:49,400 --> 01:16:50,920
but have a lot of visual elements.

1610
01:16:50,920 --> 01:16:53,880
Well, that's one of the things that's fun about chat, you know, about chat GPT.

1611
01:16:53,880 --> 01:16:55,640
Yeah, back to the basics.

1612
01:16:55,640 --> 01:16:57,640
Back to just text, right?

1613
01:16:57,640 --> 01:17:02,600
And it, you know, there is this pattern in human creativity and media where you end up

1614
01:17:02,600 --> 01:17:03,480
back at text.

1615
01:17:03,480 --> 01:17:05,480
And I think there's, you know, there's something powerful in there.

1616
01:17:06,200 --> 01:17:08,920
Is there some other stuff you remember, like the purple links?

1617
01:17:08,920 --> 01:17:14,120
There were some interesting design decisions to kind of come up that we have today or we

1618
01:17:14,120 --> 01:17:16,120
don't have today that were temporary.

1619
01:17:16,680 --> 01:17:18,280
So we made, I made the background gray.

1620
01:17:18,280 --> 01:17:21,560
I hated reading text on white backgrounds.

1621
01:17:21,560 --> 01:17:22,520
So I made the background gray.

1622
01:17:22,520 --> 01:17:23,400
Do you regret?

1623
01:17:24,040 --> 01:17:27,560
No, no, no, no, that's that decision I think has been reversed.

1624
01:17:27,800 --> 01:17:30,280
But now I'm happy though, because now dark mode is the thing.

1625
01:17:31,160 --> 01:17:32,760
So it wasn't about gray.

1626
01:17:32,760 --> 01:17:34,520
It was just you didn't want a white background.

1627
01:17:34,520 --> 01:17:35,160
Strain my eyes.

1628
01:17:35,960 --> 01:17:36,920
Strange your eyes.

1629
01:17:37,720 --> 01:17:38,280
Interesting.

1630
01:17:39,800 --> 01:17:41,720
And then there's a bunch of other decisions.

1631
01:17:41,720 --> 01:17:45,640
I'm sure there's an interesting history of the development of HTML and CSS and all those

1632
01:17:45,640 --> 01:17:48,200
in interface and JavaScript.

1633
01:17:48,200 --> 01:17:50,280
And there's this whole Java applet thing.

1634
01:17:51,240 --> 01:17:52,680
Well, the big one probably JavaScript.

1635
01:17:53,480 --> 01:17:57,320
CSS was after me, so I didn't know it wasn't me, but JavaScript wasn't the big

1636
01:17:57,400 --> 01:17:59,000
JavaScript maybe was the biggest of the whole thing.

1637
01:17:59,000 --> 01:17:59,480
That was us.

1638
01:18:00,200 --> 01:18:02,680
And that was basically a bet.

1639
01:18:02,680 --> 01:18:03,720
It was a bet on two things.

1640
01:18:03,720 --> 01:18:06,200
One is that the world wanted a new front end scripting language.

1641
01:18:07,240 --> 01:18:10,520
And then the other was we thought at the time the world wanted a new back end scripting language.

1642
01:18:11,320 --> 01:18:14,360
So JavaScript was designed from the beginning to be both front end and back end.

1643
01:18:15,320 --> 01:18:19,640
And then it failed as a back end scripting language and Java won for a long time.

1644
01:18:19,640 --> 01:18:22,600
And then Python, Perl and other things, PHP and Ruby.

1645
01:18:22,600 --> 01:18:24,440
But now JavaScript is back.

1646
01:18:24,520 --> 01:18:28,200
And so I wonder if everything in the end will run on JavaScript.

1647
01:18:28,200 --> 01:18:30,200
It seems like it is the...

1648
01:18:30,200 --> 01:18:34,360
And by the way, let me give a shout out to Brendan Eich,

1649
01:18:34,360 --> 01:18:37,800
was the basically the one man inventor of JavaScript.

1650
01:18:37,800 --> 01:18:41,960
If you're interested to learn more about Brendan Eich, you can find his podcast previously.

1651
01:18:41,960 --> 01:18:42,440
Exactly.

1652
01:18:43,160 --> 01:18:44,600
So he wrote JavaScript over a summer.

1653
01:18:44,600 --> 01:18:49,240
And I think it is fair to say now that it's the most widely used language in the world.

1654
01:18:49,240 --> 01:18:53,400
And it seems to only be gaining in its range of adoption.

1655
01:18:53,480 --> 01:18:58,440
In the software world, there's quite a few stories of somebody over a weekend or a week

1656
01:18:58,440 --> 01:19:05,640
or over a summer writing some of the most impactful revolutionary pieces of software ever.

1657
01:19:06,200 --> 01:19:07,720
That should be inspiring, yes.

1658
01:19:07,720 --> 01:19:08,360
Very inspiring.

1659
01:19:08,360 --> 01:19:09,480
I'll give you another one, SSL.

1660
01:19:10,120 --> 01:19:12,600
So SSL was the security protocol that was us.

1661
01:19:12,600 --> 01:19:14,120
And that was a crazy idea at the time,

1662
01:19:14,120 --> 01:19:17,720
which was let's take all the native protocols and let's wrap them in a security wrapper.

1663
01:19:17,720 --> 01:19:20,680
That was a guy named Kip Hickman who wrote that over a summer, one guy.

1664
01:19:20,920 --> 01:19:26,440
And then look today, sitting here today, like the transformer at Google was a small handful

1665
01:19:26,440 --> 01:19:27,080
of people.

1666
01:19:27,080 --> 01:19:32,520
And then the number of people who did the core work on GPT, it's not that many people.

1667
01:19:33,080 --> 01:19:34,360
It's a pretty small handful of people.

1668
01:19:35,080 --> 01:19:39,320
And so, yeah, the pattern in software repeatedly over a very long time has been.

1669
01:19:40,920 --> 01:19:43,960
Jeff Bezos always had the two pizza rule for teams at Amazon,

1670
01:19:43,960 --> 01:19:46,520
which is any team needs to be able to be fed with two pizzas.

1671
01:19:47,160 --> 01:19:48,920
If you need the third pizza, you have too many people.

1672
01:19:48,920 --> 01:19:54,600
And I think it's actually the one pizza rule for the really creative work.

1673
01:19:54,600 --> 01:19:57,000
I think it's two people, three people.

1674
01:19:57,000 --> 01:20:00,040
Well, you see that with certain open source projects.

1675
01:20:00,040 --> 01:20:01,720
So much is done by one or two people.

1676
01:20:03,000 --> 01:20:04,200
It's so incredible.

1677
01:20:04,200 --> 01:20:08,360
And that's why you see that gives me so much hope about the open source movement

1678
01:20:08,360 --> 01:20:09,480
in this new age of AI.

1679
01:20:10,440 --> 01:20:15,800
Where recently having had a conversation with Mark Zuckerberg of all people

1680
01:20:15,800 --> 01:20:22,040
who's all in on open source, which is so interesting to see and so inspiring to see.

1681
01:20:22,040 --> 01:20:25,320
Because releasing these models, it is scary.

1682
01:20:25,320 --> 01:20:26,840
It is potentially very dangerous.

1683
01:20:26,840 --> 01:20:27,800
And we'll talk about that.

1684
01:20:28,680 --> 01:20:35,560
But it's also, if you believe in the goodness of most people and in the skill set of most people

1685
01:20:35,560 --> 01:20:39,240
and the desire to do good in the world, that's really exciting.

1686
01:20:39,240 --> 01:20:44,040
Because it's not putting these models into the centralized control of big corporations,

1687
01:20:44,040 --> 01:20:44,920
the government and so on.

1688
01:20:44,920 --> 01:20:49,720
It's putting it in the hands of a teenage kid with a dream in his eyes.

1689
01:20:49,720 --> 01:20:50,280
I don't know.

1690
01:20:50,280 --> 01:20:52,840
That's beautiful.

1691
01:20:52,840 --> 01:20:56,440
And look, this stuff, AI ought to make the individual coder, obviously,

1692
01:20:56,440 --> 01:20:59,800
far more productive by 1,000X or something.

1693
01:20:59,800 --> 01:21:03,640
And so you ought to open source, not just the future of open source,

1694
01:21:03,640 --> 01:21:05,160
but the future of open source, everything.

1695
01:21:05,720 --> 01:21:10,120
We ought to have a world now of super coders who are building things as open source with

1696
01:21:10,120 --> 01:21:13,080
one or two people that were inconceivable five years ago.

1697
01:21:14,280 --> 01:21:17,160
The level of kind of hyperproductivity we're going to get out of our best and brightest,

1698
01:21:17,160 --> 01:21:18,200
I think it's going to go way up.

1699
01:21:18,200 --> 01:21:19,000
It's going to be interesting.

1700
01:21:19,000 --> 01:21:19,960
We'll talk about it.

1701
01:21:19,960 --> 01:21:23,000
But let's just linger a little bit on Nescape.

1702
01:21:24,040 --> 01:21:29,720
Nescape was acquired in 1999 for $4.3 billion by AOL.

1703
01:21:29,720 --> 01:21:31,480
What was that like?

1704
01:21:31,480 --> 01:21:33,960
What were some memorable aspects of that?

1705
01:21:33,960 --> 01:21:37,560
Well, that was the height of the dot-com boom bubble bust.

1706
01:21:37,560 --> 01:21:39,400
I mean, that was the frenzy.

1707
01:21:39,960 --> 01:21:43,400
If you watch a succession, that was like what they did in the fourth season

1708
01:21:43,960 --> 01:21:48,600
with Gojo and then Merger with their, so it was like the height of one of those kind of

1709
01:21:48,600 --> 01:21:49,080
dynamics.

1710
01:21:49,080 --> 01:21:51,000
Would you recommend succession, by the way?

1711
01:21:51,000 --> 01:21:52,360
I'm more of a Yellowstone guy.

1712
01:21:53,640 --> 01:21:54,920
Yellowstone's very American.

1713
01:21:54,920 --> 01:21:55,960
I'm very proud of you.

1714
01:21:57,320 --> 01:22:00,760
I just talked to Matthew McConaughey, and I'm full on texting at this point.

1715
01:22:00,760 --> 01:22:01,240
Good.

1716
01:22:01,240 --> 01:22:02,040
I hurtily approve.

1717
01:22:02,840 --> 01:22:06,040
And he will be doing the sequel to Yellowstone.

1718
01:22:06,040 --> 01:22:07,000
Yeah, exactly.

1719
01:22:07,000 --> 01:22:07,720
Very exciting.

1720
01:22:07,720 --> 01:22:11,400
Anyway, so that's a rude interruption by me.

1721
01:22:12,360 --> 01:22:17,880
By way of succession, so that was at the height of the-

1722
01:22:17,880 --> 01:22:21,400
Deal-making and money and just the fur flying and like craziness.

1723
01:22:21,400 --> 01:22:23,400
And so, yeah, it was just one of those.

1724
01:22:23,400 --> 01:22:26,760
It was just like, I mean, as the entire Nescape thing from start to finish was four years,

1725
01:22:27,640 --> 01:22:30,760
which was like, for one of these companies, it's just like incredibly fast.

1726
01:22:30,760 --> 01:22:33,880
You know, we went public 18 months after we got, after we were founded,

1727
01:22:33,880 --> 01:22:35,480
which virtually never happens.

1728
01:22:35,480 --> 01:22:38,680
So it was just this incredibly fast kind of meteor streaking across the sky.

1729
01:22:39,320 --> 01:22:40,520
And then, of course, it was this.

1730
01:22:40,520 --> 01:22:42,680
And then there was just this explosion that happened,

1731
01:22:42,680 --> 01:22:45,160
because then it was almost immediately followed by the dot-com crash.

1732
01:22:45,960 --> 01:22:48,280
It was then followed by Haywell buying Time Warner,

1733
01:22:48,280 --> 01:22:50,440
which, again, is like the succession guy's going to play with that,

1734
01:22:51,240 --> 01:22:54,440
which turned out to be a disaster steal, one of the famous,

1735
01:22:54,440 --> 01:22:56,040
you know, kind of disasters in business history.

1736
01:22:56,600 --> 01:23:00,680
And then what became an internet depression on the other side of that.

1737
01:23:00,680 --> 01:23:05,400
But then in that depression in the 2000s was the beginning of broadband and smartphones

1738
01:23:05,400 --> 01:23:06,840
and web 2.0, right?

1739
01:23:06,840 --> 01:23:10,200
And then social media and search and every SaaS and everything that came out of that.

1740
01:23:10,200 --> 01:23:12,680
So what did you learn from just the acquisition?

1741
01:23:12,680 --> 01:23:14,120
I mean, this is so much money.

1742
01:23:16,280 --> 01:23:19,320
What's interesting, because I must have been very new to you,

1743
01:23:20,040 --> 01:23:24,840
that these software stuff, you can make so much money.

1744
01:23:24,840 --> 01:23:26,360
There's so much money swimming around.

1745
01:23:26,360 --> 01:23:29,960
I mean, I'm sure the ideas of investment were starting to get born there.

1746
01:23:29,960 --> 01:23:31,560
Yes, let me get, so let me lay it out.

1747
01:23:31,560 --> 01:23:33,400
So here's the thing, I don't know if I figured it out then,

1748
01:23:33,400 --> 01:23:38,040
but figured it out later, which is software is a technology that it's like, you know,

1749
01:23:38,040 --> 01:23:41,720
the concept of the philosopher's stone, the philosopher's stone in Alchemy transmutes

1750
01:23:41,720 --> 01:23:44,520
lead into gold and Newton spent 20 years trying to find the philosopher's stone,

1751
01:23:44,520 --> 01:23:45,000
never got there.

1752
01:23:45,000 --> 01:23:46,120
Nobody's ever figured it out.

1753
01:23:46,680 --> 01:23:48,760
Software is our modern philosopher's stone.

1754
01:23:48,760 --> 01:23:52,680
And in economic terms, it transmutes labor into capital,

1755
01:23:53,800 --> 01:23:55,880
which is like a super interesting thing.

1756
01:23:55,880 --> 01:23:58,200
And by the way, like Karl Marx is rolling over in his grave right now,

1757
01:23:58,200 --> 01:24:00,680
because of course that's complete refutation of his entire theory.

1758
01:24:00,920 --> 01:24:05,240
Transpute labor into capital, which is as follows is somebody

1759
01:24:05,960 --> 01:24:08,520
sits down at a keyboard and types a bunch of stuff in,

1760
01:24:09,320 --> 01:24:11,400
and a capital asset comes out the other side,

1761
01:24:11,400 --> 01:24:14,040
and then somebody buys that capital asset for a billion dollars.

1762
01:24:14,040 --> 01:24:15,880
Like, that's amazing.

1763
01:24:16,600 --> 01:24:19,240
Right, it's literally creating value right out of thin air,

1764
01:24:19,240 --> 01:24:21,480
right, out of purely human thought, right.

1765
01:24:22,440 --> 01:24:26,120
And so that's, there are many things that make software magical and special,

1766
01:24:26,120 --> 01:24:27,240
but that's the economics.

1767
01:24:27,240 --> 01:24:29,400
I wonder what Marx would have thought about that.

1768
01:24:29,400 --> 01:24:30,840
Oh, he would have completely broke his brain,

1769
01:24:30,840 --> 01:24:32,520
because of course the whole thing was,

1770
01:24:34,680 --> 01:24:36,760
that kind of technology is inconceivable when he was alive.

1771
01:24:36,760 --> 01:24:38,680
It was all industrial era stuff.

1772
01:24:38,680 --> 01:24:42,840
And so any kind of machinery necessarily involves huge amounts of capital,

1773
01:24:42,840 --> 01:24:45,320
and then labor was on the receiving end of the abuse.

1774
01:24:46,840 --> 01:24:50,840
Right, but like a software engineer or somebody who basically transmutes

1775
01:24:50,840 --> 01:24:54,840
his own labor into an actual capital asset creates permanent value.

1776
01:24:54,840 --> 01:24:57,080
Well, in fact, it's actually very inspiring.

1777
01:24:57,080 --> 01:24:58,520
That's actually more true today than before.

1778
01:24:58,520 --> 01:25:01,240
So when I was doing software, the assumption was all new software

1779
01:25:01,240 --> 01:25:05,080
basically has a sort of a parabolic sort of life cycle, right.

1780
01:25:05,080 --> 01:25:07,320
So you ship the thing, people buy it.

1781
01:25:07,320 --> 01:25:09,160
At some point, everybody who wants it has bought it,

1782
01:25:09,160 --> 01:25:12,600
and then it becomes obsolete, and it's like bananas, nobody buys old software.

1783
01:25:13,880 --> 01:25:20,360
These days, Minecraft, Mathematica, Facebook, Google,

1784
01:25:21,320 --> 01:25:25,320
you have the software assets that are, have been around for 30 years

1785
01:25:25,320 --> 01:25:27,320
that are gaining in value every year, right.

1786
01:25:27,320 --> 01:25:30,040
And they're just, they're being World of Warcraft, right, Salesforce.com.

1787
01:25:30,040 --> 01:25:32,520
Like they're being, every single year, they're being polished and polished

1788
01:25:32,520 --> 01:25:33,320
and polished and polished.

1789
01:25:33,320 --> 01:25:35,160
They're getting better and better, more powerful, more powerful,

1790
01:25:35,160 --> 01:25:36,200
more valuable, more valuable.

1791
01:25:36,200 --> 01:25:38,440
So we've entered this era where you can actually have these things

1792
01:25:38,440 --> 01:25:40,440
that actually build out over decades, which by the way,

1793
01:25:40,440 --> 01:25:42,040
is what's happening right now with like GPT.

1794
01:25:43,480 --> 01:25:48,440
And so now, and this is why there is always sort of a constant investment

1795
01:25:48,440 --> 01:25:51,880
frenzy around software is because, look, when you start one of these things,

1796
01:25:51,880 --> 01:25:52,600
it doesn't always succeed.

1797
01:25:52,600 --> 01:25:55,560
But when it does, now you might be building an asset that builds value

1798
01:25:55,560 --> 01:25:57,320
for four or five, six decades to come.

1799
01:25:59,000 --> 01:26:01,800
If you have a team of people who have the level of devotion required

1800
01:26:01,800 --> 01:26:02,760
to keep making it better.

1801
01:26:03,560 --> 01:26:06,760
And then the fact that of course, everybody's online, there's five billion

1802
01:26:06,760 --> 01:26:08,840
people that are a click away from any new pieces of software.

1803
01:26:08,840 --> 01:26:12,600
So the potential market size for any of these things is nearly infinite.

1804
01:26:12,600 --> 01:26:14,440
They must have been surreal back then though.

1805
01:26:14,440 --> 01:26:15,080
Yeah, yeah.

1806
01:26:15,080 --> 01:26:16,040
This was all brand new, right?

1807
01:26:16,040 --> 01:26:17,400
Yeah, back then, this was all brand new.

1808
01:26:17,400 --> 01:26:19,240
These were all brand new.

1809
01:26:19,240 --> 01:26:21,720
Had you rolled out that theory and even 1999,

1810
01:26:21,720 --> 01:26:23,080
people would have thought you were smoking crack.

1811
01:26:23,080 --> 01:26:25,160
So that's emerged over time.

1812
01:26:26,520 --> 01:26:30,600
Well, let's now turn back into the future.

1813
01:26:30,600 --> 01:26:33,800
You wrote the essay, why AI will save the world.

1814
01:26:34,920 --> 01:26:36,280
Let's start at the very high level.

1815
01:26:36,280 --> 01:26:37,960
What's the main thesis of the essay?

1816
01:26:37,960 --> 01:26:38,120
Yeah.

1817
01:26:38,120 --> 01:26:41,560
So the main thesis on the essay is that what we're dealing with here is intelligence.

1818
01:26:42,280 --> 01:26:45,400
And it's really important to kind of talk about the sort of very nature of what

1819
01:26:45,400 --> 01:26:46,280
intelligence is.

1820
01:26:46,280 --> 01:26:50,680
And fortunately, we have a predecessor to machine intelligence,

1821
01:26:50,680 --> 01:26:51,640
which is human intelligence.

1822
01:26:52,200 --> 01:26:55,080
And we've got observations and theories over thousands of years

1823
01:26:55,080 --> 01:26:57,720
for what intelligence is in the hands of humans.

1824
01:26:57,720 --> 01:27:00,760
And what intelligence is, what it literally is,

1825
01:27:00,760 --> 01:27:03,160
is the way to capture, process, analyze,

1826
01:27:03,160 --> 01:27:04,600
synthesize, information, solve problems.

1827
01:27:05,800 --> 01:27:09,480
But the observation of intelligence in human hands

1828
01:27:09,480 --> 01:27:11,880
is that intelligence quite literally makes everything better.

1829
01:27:13,000 --> 01:27:17,960
And what I mean by that is every kind of outcome of human quality of life,

1830
01:27:17,960 --> 01:27:21,000
whether it's education outcomes or success of your children,

1831
01:27:21,960 --> 01:27:26,200
or career success, or health, or lifetime satisfaction,

1832
01:27:26,760 --> 01:27:31,080
by the way, propensity to peacefulness as opposed to violence,

1833
01:27:31,960 --> 01:27:34,440
propensity for open-mindedness versus bigotry,

1834
01:27:35,000 --> 01:27:37,320
those are all associated with higher levels of intelligence.

1835
01:27:37,320 --> 01:27:40,120
Smarter people have better outcomes than almost, as you write,

1836
01:27:40,120 --> 01:27:41,720
in almost every domain of activity.

1837
01:27:41,720 --> 01:27:44,760
Academic achievement, job performance, occupational status,

1838
01:27:44,760 --> 01:27:47,320
income, creativity, physical health, longevity,

1839
01:27:47,320 --> 01:27:50,440
learning new skills, managing complex tasks,

1840
01:27:50,440 --> 01:27:54,200
leadership, entrepreneurial success, conflict resolution,

1841
01:27:54,200 --> 01:27:57,480
reading comprehension, financial decision making,

1842
01:27:57,480 --> 01:27:58,760
understanding other's perspectives,

1843
01:27:58,760 --> 01:28:01,560
creative arts, parenting outcomes, and life satisfaction.

1844
01:28:01,560 --> 01:28:05,640
One of the more depressing conversations I've had,

1845
01:28:05,640 --> 01:28:06,840
and I don't know why it's depressing,

1846
01:28:06,840 --> 01:28:09,080
I have to really think through why it's depressing,

1847
01:28:09,080 --> 01:28:13,880
but on IQ and the G factor,

1848
01:28:15,640 --> 01:28:20,120
and that that's something in large part is genetic.

1849
01:28:20,760 --> 01:28:21,880
Mm-hmm.

1850
01:28:21,880 --> 01:28:25,400
And it correlates so much with all of these things

1851
01:28:25,400 --> 01:28:26,520
and success in life.

1852
01:28:27,480 --> 01:28:30,920
It's like all the inspirational stuff we read about,

1853
01:28:30,920 --> 01:28:33,000
like if you work hard and so on,

1854
01:28:33,880 --> 01:28:37,720
damn, it sucks that you're born with a hand that you can't change.

1855
01:28:37,720 --> 01:28:38,440
But what if you could?

1856
01:28:39,160 --> 01:28:41,800
You're saying basically, a really important point,

1857
01:28:41,800 --> 01:28:47,240
and I think it's a, in your articles, it really helped me,

1858
01:28:47,960 --> 01:28:52,440
it's a nice added perspective to think about, listen,

1859
01:28:52,440 --> 01:28:55,000
human intelligence, the science of intelligence

1860
01:28:55,000 --> 01:28:58,680
has shown scientifically that it just makes life easier

1861
01:28:58,680 --> 01:29:01,160
and better, the smarter you are.

1862
01:29:02,040 --> 01:29:04,760
And now let's look at artificial intelligence.

1863
01:29:05,800 --> 01:29:10,760
And if that's a way to increase the,

1864
01:29:12,040 --> 01:29:15,080
some human intelligence, then it's only going

1865
01:29:15,080 --> 01:29:16,200
to make a better life.

1866
01:29:16,200 --> 01:29:17,080
That's the argument.

1867
01:29:17,080 --> 01:29:18,280
And certainly at the collective level,

1868
01:29:18,280 --> 01:29:19,480
we could talk about the collective effect

1869
01:29:19,480 --> 01:29:21,240
of just having more intelligence in the world,

1870
01:29:21,240 --> 01:29:23,480
which we'll have very big payoff.

1871
01:29:23,480 --> 01:29:25,000
But there's also just at the individual level,

1872
01:29:25,000 --> 01:29:27,080
like what if every person has a machine,

1873
01:29:27,080 --> 01:29:29,000
you know, and it's a concept of argument,

1874
01:29:29,000 --> 01:29:30,520
Doug Engelbar's concept of augmentation,

1875
01:29:31,640 --> 01:29:34,520
you know, what if everybody has an assistant

1876
01:29:34,520 --> 01:29:37,240
and the assistant is, you know, 140 IQ,

1877
01:29:38,520 --> 01:29:40,440
and you happen to be 110 IQ,

1878
01:29:41,320 --> 01:29:43,400
and you've got, you know, something that basically

1879
01:29:43,400 --> 01:29:45,960
is infinitely patient and knows everything about you

1880
01:29:45,960 --> 01:29:48,200
and is pulling for you in every possible way,

1881
01:29:48,760 --> 01:29:49,960
wants you to be successful.

1882
01:29:49,960 --> 01:29:52,200
And anytime you find anything confusing

1883
01:29:52,200 --> 01:29:53,160
or want to learn anything

1884
01:29:53,160 --> 01:29:54,440
or have trouble understanding something

1885
01:29:54,440 --> 01:29:55,960
or want to figure out what to do in a situation,

1886
01:29:56,600 --> 01:29:57,640
right, want to figure out how to prepare

1887
01:29:57,640 --> 01:29:59,640
for a job interview, like any of these things,

1888
01:29:59,640 --> 01:30:01,000
like it will help you do it.

1889
01:30:01,000 --> 01:30:04,040
And it will therefore, the combination will effectively be,

1890
01:30:05,240 --> 01:30:06,600
effectively raise your raise,

1891
01:30:06,600 --> 01:30:07,960
because it will effectively raise your IQ,

1892
01:30:07,960 --> 01:30:10,120
will therefore raise the odds of successful

1893
01:30:10,120 --> 01:30:11,320
life outcomes in all these areas.

1894
01:30:11,320 --> 01:30:14,840
So people below the hypothetical 140 IQ,

1895
01:30:15,400 --> 01:30:17,560
it'll pull them up towards 140 IQ.

1896
01:30:17,560 --> 01:30:18,360
Yeah, yeah.

1897
01:30:18,360 --> 01:30:20,840
And then of course, you know, people at 140 IQ

1898
01:30:20,840 --> 01:30:22,200
will be able to have a peer, right,

1899
01:30:22,200 --> 01:30:23,800
to be able to communicate, which is great.

1900
01:30:23,800 --> 01:30:25,880
And then people above 140 IQ will have an assistant

1901
01:30:25,880 --> 01:30:27,480
that they can farm things out to.

1902
01:30:27,480 --> 01:30:29,000
And then look, got willing, you know,

1903
01:30:29,000 --> 01:30:30,920
at some point, these things go from future versions,

1904
01:30:30,920 --> 01:30:35,320
go from 140 IQ equivalent to 150 to 160 to 180, right?

1905
01:30:35,320 --> 01:30:37,880
Like Einstein was estimated to be on the order of 160,

1906
01:30:38,760 --> 01:30:41,880
you know, so when we get, you know, 160 AI,

1907
01:30:41,880 --> 01:30:44,840
like we'll be, you know, one assumes creating

1908
01:30:44,840 --> 01:30:46,360
Einstein level breakthroughs in physics.

1909
01:30:47,240 --> 01:30:49,720
And then at 180, we'll be, you know,

1910
01:30:49,720 --> 01:30:51,800
carrying cancer and developing warp drive

1911
01:30:51,800 --> 01:30:52,760
and doing all kinds of stuff.

1912
01:30:52,760 --> 01:30:55,160
And so it is quite possibly the case,

1913
01:30:55,160 --> 01:30:56,600
this is the most important thing that's ever happened

1914
01:30:56,600 --> 01:30:57,560
and the best thing that's ever happened,

1915
01:30:58,200 --> 01:31:00,280
because precisely because it's a lever

1916
01:31:00,280 --> 01:31:02,680
on this single fundamental factor of intelligence,

1917
01:31:02,680 --> 01:31:04,520
which is the thing that drives so much of everything else.

1918
01:31:05,800 --> 01:31:08,680
Can you still man the case that human plus AI

1919
01:31:08,680 --> 01:31:11,240
is not always better than human for the individual?

1920
01:31:11,240 --> 01:31:12,840
You may have noticed that there's a lot of smart assholes

1921
01:31:12,840 --> 01:31:13,880
running around.

1922
01:31:13,880 --> 01:31:14,600
Sure, yes.

1923
01:31:14,600 --> 01:31:14,920
Right.

1924
01:31:14,920 --> 01:31:17,400
And so like it's smart, there are certain people

1925
01:31:17,400 --> 01:31:18,440
where they get smarter, you know,

1926
01:31:18,440 --> 01:31:20,360
they get to be more arrogant, right?

1927
01:31:20,360 --> 01:31:21,880
So, you know, there's one huge flaw.

1928
01:31:22,520 --> 01:31:25,480
Although to push back on that, it might be interesting

1929
01:31:25,480 --> 01:31:28,920
because when the intelligence is not all coming from you,

1930
01:31:28,920 --> 01:31:30,600
but from another system,

1931
01:31:30,600 --> 01:31:33,720
that might actually increase the amount of humility

1932
01:31:33,720 --> 01:31:34,760
even in the assholes.

1933
01:31:34,760 --> 01:31:35,320
One would hope.

1934
01:31:37,000 --> 01:31:38,840
Or it could make assholes more asshole.

1935
01:31:39,400 --> 01:31:41,480
I mean, that's for psychology to study.

1936
01:31:41,480 --> 01:31:42,280
Yeah, exactly.

1937
01:31:42,280 --> 01:31:45,160
Another one is smart people are very convinced

1938
01:31:45,160 --> 01:31:47,240
that they have a more rational view of the world

1939
01:31:47,240 --> 01:31:48,600
and that they have an easier time

1940
01:31:48,600 --> 01:31:50,280
seeing through conspiracy theories and hoaxes

1941
01:31:50,280 --> 01:31:52,280
and sort of crazy beliefs and all that.

1942
01:31:53,000 --> 01:31:55,320
There's a theory in psychology, which is actually smart people.

1943
01:31:55,320 --> 01:31:57,320
So, for sure, people who aren't as smart

1944
01:31:57,320 --> 01:31:59,400
are very susceptible to hoaxes and conspiracy theories.

1945
01:31:59,960 --> 01:32:01,880
But it may also be the case that the smarter you get,

1946
01:32:01,880 --> 01:32:03,560
you become susceptible in a different way,

1947
01:32:04,360 --> 01:32:06,680
which is you become very good at marshalling facts

1948
01:32:06,680 --> 01:32:07,880
to fit preconceptions.

1949
01:32:08,280 --> 01:32:08,760
Yes.

1950
01:32:08,760 --> 01:32:09,480
Right?

1951
01:32:09,480 --> 01:32:12,520
You become very, very good at assembling whatever theories

1952
01:32:12,520 --> 01:32:15,480
and frameworks and pieces of data and graphs and charts

1953
01:32:15,480 --> 01:32:17,880
you need to validate whatever crazy ideas got in your head.

1954
01:32:18,440 --> 01:32:20,440
And so, you're susceptible in a different way.

1955
01:32:21,400 --> 01:32:22,120
Right?

1956
01:32:22,120 --> 01:32:25,480
We're all sheep, but different colored sheep.

1957
01:32:25,480 --> 01:32:27,320
Some sheep are better at justifying it, right?

1958
01:32:28,200 --> 01:32:29,960
And those are the smart sheep, right?

1959
01:32:31,000 --> 01:32:32,600
So, yeah, look, I would say this.

1960
01:32:32,600 --> 01:32:33,800
Look, there are no panacea.

1961
01:32:33,800 --> 01:32:35,080
I am not a utopian.

1962
01:32:35,080 --> 01:32:36,280
There are no panaceas in life.

1963
01:32:36,760 --> 01:32:39,480
There are no, like, I don't believe they're like pure positives.

1964
01:32:39,480 --> 01:32:41,960
I'm not a transcendental kind of person like that.

1965
01:32:41,960 --> 01:32:44,120
But, you know, so, yeah, there are going to be issues.

1966
01:32:45,320 --> 01:32:47,640
And, you know, look, smart people, maybe you could say

1967
01:32:47,640 --> 01:32:49,560
about smart people as they are more likely to get themselves

1968
01:32:49,560 --> 01:32:51,320
in situations that are, you know, beyond their grasp,

1969
01:32:51,320 --> 01:32:52,600
you know, because they're just more confident

1970
01:32:52,600 --> 01:32:54,040
and their ability to deal with complexity.

1971
01:32:54,040 --> 01:32:56,120
And their eyes become bigger.

1972
01:32:56,120 --> 01:32:57,880
Their cognitive eyes become bigger than their stomach.

1973
01:32:58,520 --> 01:33:01,240
You know, so, yeah, you could argue those eight different ways.

1974
01:33:01,240 --> 01:33:03,240
Nevertheless, on net, right?

1975
01:33:03,240 --> 01:33:04,760
Clearly, overwhelmingly.

1976
01:33:04,760 --> 01:33:06,280
Again, if you just extrapolate from what we know

1977
01:33:06,280 --> 01:33:09,480
about human intelligence, you're improving so many aspects

1978
01:33:09,480 --> 01:33:10,920
of life if you're upgrading intelligence.

1979
01:33:11,960 --> 01:33:15,400
So, there'll be assistance at all stages of life.

1980
01:33:15,400 --> 01:33:17,960
So, when you're younger, there's for education,

1981
01:33:17,960 --> 01:33:20,680
all that kind of stuff, or mentorship, all of this.

1982
01:33:20,680 --> 01:33:24,360
And later on, as you're doing work and you've developed a skill

1983
01:33:24,360 --> 01:33:26,520
and you're having a profession, you'll have an assistant

1984
01:33:26,520 --> 01:33:28,680
that helps you excel at that profession.

1985
01:33:28,680 --> 01:33:30,120
So, at all stages of life.

1986
01:33:30,120 --> 01:33:31,800
Yeah. I mean, look, the theory is augmentations.

1987
01:33:31,800 --> 01:33:33,400
This is the Degangalbert's term for it.

1988
01:33:33,400 --> 01:33:35,880
Degangalbert made this observation many, many decades ago

1989
01:33:35,880 --> 01:33:37,160
that, you know, basically, it's like you can have

1990
01:33:37,160 --> 01:33:38,760
this oppositional frame of technology

1991
01:33:38,760 --> 01:33:40,040
where it's like us versus the machines.

1992
01:33:40,040 --> 01:33:41,560
But what you really do is you use technology

1993
01:33:41,560 --> 01:33:42,920
to augment human capabilities.

1994
01:33:43,560 --> 01:33:45,480
And by the way, that's how actually the economy develops.

1995
01:33:45,480 --> 01:33:47,240
That's dark about the economic side of this,

1996
01:33:47,240 --> 01:33:49,240
but that's actually how the economy grows

1997
01:33:49,240 --> 01:33:52,040
is through technology augmenting human potential.

1998
01:33:53,800 --> 01:33:56,120
And so, yeah, and then you basically have a proxy

1999
01:33:56,120 --> 01:34:00,120
or a, you know, a sort of prosthetic, you know,

2000
01:34:00,120 --> 01:34:02,040
so like you've got glasses, you've got a wristwatch,

2001
01:34:02,760 --> 01:34:05,240
you know, you've got shoes, you know, you've got these things,

2002
01:34:05,240 --> 01:34:07,560
you've got a personal computer, you've got a word processor,

2003
01:34:07,560 --> 01:34:09,960
you've got Mathematica, you've got Google.

2004
01:34:10,600 --> 01:34:12,600
This is the latest, viewed through that lens.

2005
01:34:12,600 --> 01:34:15,480
AI is the latest in a long series of basically

2006
01:34:15,480 --> 01:34:18,040
augmentation methods to be able to raise human capabilities.

2007
01:34:18,040 --> 01:34:20,280
It's just this one is the most powerful one of all

2008
01:34:20,280 --> 01:34:22,680
because this is the one that goes directly to what they call

2009
01:34:22,680 --> 01:34:24,520
fluid intelligence, which is IQ.

2010
01:34:26,840 --> 01:34:30,920
Well, there's two categories of folks that you outline

2011
01:34:31,480 --> 01:34:34,760
that they worry about or highlight the risks of AI

2012
01:34:34,760 --> 01:34:36,680
and you highlight a bunch of different risks.

2013
01:34:36,680 --> 01:34:38,920
I would love to go through those risks

2014
01:34:38,920 --> 01:34:39,800
and just discuss them.

2015
01:34:39,800 --> 01:34:42,520
Brainstorm, which ones are serious

2016
01:34:42,520 --> 01:34:44,760
and which ones are less serious?

2017
01:34:44,760 --> 01:34:46,760
But first, the Baptist and the bootleggers,

2018
01:34:46,760 --> 01:34:48,920
what are these two interesting groups of folks

2019
01:34:49,800 --> 01:34:56,040
who worry about the effect of AI on human civilization?

2020
01:34:56,040 --> 01:34:57,080
Or say they do.

2021
01:34:57,080 --> 01:34:58,200
Say, oh, okay.

2022
01:34:59,400 --> 01:35:00,440
Yes, I'll say they do.

2023
01:35:00,440 --> 01:35:02,440
The Baptist worry the bootleggers, I'll say they do.

2024
01:35:03,720 --> 01:35:05,560
So the Baptist and the bootleggers is a metaphor

2025
01:35:05,560 --> 01:35:07,960
from economics from what's called development economics.

2026
01:35:07,960 --> 01:35:10,600
And it's this observation that when you get social reform

2027
01:35:10,600 --> 01:35:14,520
movements in a society, you tend to get two sets of people

2028
01:35:14,520 --> 01:35:16,120
showing up arguing for the social reform.

2029
01:35:16,680 --> 01:35:19,000
And the term Baptist and bootleggers

2030
01:35:19,000 --> 01:35:21,560
comes from the American experience with alcohol prohibition.

2031
01:35:22,520 --> 01:35:25,720
And so in the 1900s, 1910s, there was this movement

2032
01:35:25,720 --> 01:35:27,320
that was very passionate at the time,

2033
01:35:27,320 --> 01:35:29,800
which basically said alcohol is evil

2034
01:35:29,800 --> 01:35:30,920
and it's destroying society.

2035
01:35:31,800 --> 01:35:33,880
By the way, there was a lot of evidence to support this.

2036
01:35:34,440 --> 01:35:37,560
There were very high rates of very high correlations

2037
01:35:37,560 --> 01:35:41,080
than, by the way, and now between rates of physical violence

2038
01:35:41,080 --> 01:35:42,280
and alcohol use.

2039
01:35:42,280 --> 01:35:44,680
Almost all violent crimes have either the perpetrator

2040
01:35:44,680 --> 01:35:45,880
or the victim are both drunk.

2041
01:35:47,000 --> 01:35:48,280
You see this actually in the work,

2042
01:35:48,280 --> 01:35:50,200
almost all sexual harassment cases in the workplace.

2043
01:35:50,200 --> 01:35:52,120
It's like at a company party and somebody's drunk.

2044
01:35:52,120 --> 01:35:54,920
Like it's amazing how often alcohol actually correlates

2045
01:35:54,920 --> 01:35:57,720
to actually dysfunction of these two, domestic abuse

2046
01:35:57,720 --> 01:35:58,680
and so forth, child abuse.

2047
01:35:59,160 --> 01:36:00,840
And so you had this group of people who were like,

2048
01:36:00,840 --> 01:36:02,840
okay, this is bad stuff and we shall outlaw it.

2049
01:36:02,840 --> 01:36:04,440
And those were quite literally Baptists.

2050
01:36:04,440 --> 01:36:07,320
Those were super committed, hardcore Christian activists

2051
01:36:07,320 --> 01:36:07,960
in a lot of cases.

2052
01:36:08,520 --> 01:36:10,840
There was this woman whose name was Kerry Nation,

2053
01:36:11,560 --> 01:36:13,560
who was this older woman who had been in this,

2054
01:36:13,560 --> 01:36:15,240
I don't know, disastrous marriage or something

2055
01:36:15,240 --> 01:36:17,400
and her husband had been abusive and drunk all the time.

2056
01:36:17,400 --> 01:36:21,080
And she became the icon of the Baptist prohibitionist

2057
01:36:21,080 --> 01:36:23,960
and she was legendary in that era for carrying an axe

2058
01:36:24,840 --> 01:36:28,280
and doing completely on her own, doing raids of saloons

2059
01:36:28,280 --> 01:36:31,560
and like taking her axe to all the bottles and tigs in the back.

2060
01:36:31,560 --> 01:36:32,360
And so-

2061
01:36:32,360 --> 01:36:33,480
So a true believer.

2062
01:36:33,480 --> 01:36:36,040
An absolute true believer with absolutely

2063
01:36:36,040 --> 01:36:37,160
the purest of intentions.

2064
01:36:37,160 --> 01:36:39,960
And again, there's a very important thing here,

2065
01:36:39,960 --> 01:36:41,800
which is you could look at this cynically

2066
01:36:41,800 --> 01:36:44,360
and you could say the Baptists are like delusional extremists,

2067
01:36:44,360 --> 01:36:45,800
but you can also say, look, they're right.

2068
01:36:45,800 --> 01:36:48,840
Like she had a point, like she wasn't wrong

2069
01:36:49,400 --> 01:36:50,360
about a lot of what she said.

2070
01:36:51,320 --> 01:36:53,160
But it turns out, the way the story goes,

2071
01:36:53,160 --> 01:36:55,160
is it turns out that there were another set of people

2072
01:36:55,160 --> 01:36:57,080
who very badly wanted to outlaw alcohol

2073
01:36:57,080 --> 01:36:57,560
in those days.

2074
01:36:57,560 --> 01:36:58,680
And those were the bootleggers,

2075
01:36:58,680 --> 01:37:02,120
which was organized crime that stood to make a huge amount of money

2076
01:37:02,120 --> 01:37:04,120
if legal alcohol sales were banned.

2077
01:37:04,680 --> 01:37:06,440
And this was in fact, the way the history goes,

2078
01:37:06,440 --> 01:37:08,520
is this was actually the beginning of organized crime in the U.S.

2079
01:37:08,520 --> 01:37:10,680
This was the big economic opportunity that opened that up.

2080
01:37:11,880 --> 01:37:15,240
And so they went in together and they didn't go in together.

2081
01:37:15,240 --> 01:37:17,320
Like the Baptists did not even necessarily know

2082
01:37:17,320 --> 01:37:19,240
about the bootleggers because they were on the moral crusade.

2083
01:37:19,800 --> 01:37:21,400
The bootleggers certainly knew about the Baptists

2084
01:37:21,400 --> 01:37:23,000
and they were like, wow, these people are like

2085
01:37:23,000 --> 01:37:25,000
the great front people for like, you know,

2086
01:37:25,640 --> 01:37:26,920
shenanigans in the background.

2087
01:37:27,640 --> 01:37:29,880
And they got the full state act passed, right?

2088
01:37:29,880 --> 01:37:32,440
And they did in fact ban alcohol in the U.S.

2089
01:37:32,440 --> 01:37:34,680
And you'll notice what happened, which is people kept drinking.

2090
01:37:35,320 --> 01:37:36,520
Like it didn't work.

2091
01:37:36,520 --> 01:37:37,400
People kept drinking.

2092
01:37:38,280 --> 01:37:40,040
The bootleggers made a tremendous amount of money.

2093
01:37:40,600 --> 01:37:42,920
And then over time, it became clear that it made no sense

2094
01:37:42,920 --> 01:37:44,840
to make it illegal and it was causing more problems.

2095
01:37:44,840 --> 01:37:45,960
And so then it was revoked.

2096
01:37:45,960 --> 01:37:48,040
And here we sit with legal alcohol 100 years later

2097
01:37:48,040 --> 01:37:49,080
with all the same problems.

2098
01:37:50,440 --> 01:37:53,160
And the whole thing was this like giant misadventure.

2099
01:37:54,040 --> 01:37:56,120
The Baptists got taken advantage of by the bootleggers

2100
01:37:56,120 --> 01:37:58,360
and the bootleggers got what they wanted and that was that.

2101
01:37:58,360 --> 01:38:01,960
The same two categories of folks are now sort of suggesting

2102
01:38:03,000 --> 01:38:05,400
the development of artificial intelligence should be regulated.

2103
01:38:05,400 --> 01:38:05,800
100%.

2104
01:38:05,800 --> 01:38:06,440
Yeah, it's the same pattern.

2105
01:38:06,440 --> 01:38:08,600
And the economists will tell you it's the same pattern every time.

2106
01:38:08,600 --> 01:38:10,040
Like this is what happened with nuclear power.

2107
01:38:10,040 --> 01:38:12,280
This is what happened, which is another interesting one.

2108
01:38:12,280 --> 01:38:14,600
But like, yeah, this happens dozens and dozens of times

2109
01:38:15,480 --> 01:38:16,680
throughout the last 100 years.

2110
01:38:16,680 --> 01:38:18,360
And this is what's happening now.

2111
01:38:18,360 --> 01:38:21,560
And you write that it isn't sufficient

2112
01:38:21,560 --> 01:38:24,120
to simply identify the actors and impugn their motors.

2113
01:38:24,120 --> 01:38:26,520
We should consider the arguments of both the Baptists

2114
01:38:26,520 --> 01:38:28,520
and the bootleggers on their merits.

2115
01:38:28,520 --> 01:38:29,880
So let's do just that.

2116
01:38:30,840 --> 01:38:31,960
Risk number one.

2117
01:38:35,240 --> 01:38:36,840
Will AI kill us all?

2118
01:38:36,840 --> 01:38:37,320
Yes.

2119
01:38:38,120 --> 01:38:42,840
So what do you think about this one?

2120
01:38:43,800 --> 01:38:45,720
What do you think is the core argument here

2121
01:38:46,680 --> 01:38:51,640
that the development of AGI, perhaps better said,

2122
01:38:52,200 --> 01:38:54,200
will destroy human civilization?

2123
01:38:54,200 --> 01:38:55,960
Well, first of all, you just did a sleight of hand

2124
01:38:55,960 --> 01:38:57,880
because we went from talking about AI to AGI.

2125
01:38:59,880 --> 01:39:01,400
Is there a fundamental difference there?

2126
01:39:01,400 --> 01:39:02,360
I don't know. What's AGI?

2127
01:39:03,640 --> 01:39:04,360
What's AI?

2128
01:39:04,360 --> 01:39:04,680
What's intelligence?

2129
01:39:04,680 --> 01:39:05,560
Well, I know what AI is.

2130
01:39:05,560 --> 01:39:06,520
AI is machine learning.

2131
01:39:07,240 --> 01:39:08,200
What's AGI?

2132
01:39:08,200 --> 01:39:10,840
I think we don't know what the bottom of the well of machine

2133
01:39:10,840 --> 01:39:12,600
learning is or what the ceiling is.

2134
01:39:12,600 --> 01:39:15,240
Because just to call something machine learning

2135
01:39:15,240 --> 01:39:16,760
or just to call something statistics

2136
01:39:16,760 --> 01:39:18,680
or just to call it math or computation

2137
01:39:18,680 --> 01:39:22,520
doesn't mean nuclear weapons are just physics.

2138
01:39:22,520 --> 01:39:26,840
So to me, it's very interesting and surprising

2139
01:39:26,840 --> 01:39:28,200
how far machine learning has taken.

2140
01:39:28,200 --> 01:39:30,120
No, but we knew that nuclear physics would lead to weapons.

2141
01:39:30,120 --> 01:39:31,560
That's why the scientists of that era

2142
01:39:31,560 --> 01:39:34,040
were always in this huge dispute about building the weapons.

2143
01:39:34,040 --> 01:39:34,680
This is different.

2144
01:39:34,680 --> 01:39:35,000
AGI is different.

2145
01:39:35,000 --> 01:39:36,360
Where does machine learning lead?

2146
01:39:36,360 --> 01:39:36,920
Do we know?

2147
01:39:36,920 --> 01:39:38,040
We don't know, but this is my point.

2148
01:39:38,040 --> 01:39:38,680
It's different.

2149
01:39:38,680 --> 01:39:39,560
We actually don't know.

2150
01:39:40,120 --> 01:39:42,360
And this is where the sleight of hand kicks in.

2151
01:39:42,360 --> 01:39:44,360
This is where it goes from being a scientific topic

2152
01:39:44,360 --> 01:39:45,400
to being a religious topic.

2153
01:39:46,360 --> 01:39:48,520
And that's why I specifically called out the...

2154
01:39:48,520 --> 01:39:49,160
Because that's what happens.

2155
01:39:49,160 --> 01:39:50,280
They do the vocabulary shift.

2156
01:39:50,280 --> 01:39:51,960
And all of a sudden, you're talking about something totally

2157
01:39:51,960 --> 01:39:53,000
that's not actually real.

2158
01:39:53,000 --> 01:39:55,960
Well, then maybe you can also, as part of that,

2159
01:39:55,960 --> 01:39:59,400
define the Western tradition of millennialism.

2160
01:39:59,400 --> 01:40:01,080
Yes, end of the world.

2161
01:40:01,080 --> 01:40:01,720
Apocalypse.

2162
01:40:01,720 --> 01:40:02,440
What is it?

2163
01:40:02,440 --> 01:40:03,240
Apocalypse cults.

2164
01:40:03,800 --> 01:40:04,680
Apocalypse cults.

2165
01:40:04,680 --> 01:40:06,280
Well, so we live in...

2166
01:40:06,280 --> 01:40:07,720
We, of course, live in a Judeo-Christian,

2167
01:40:07,720 --> 01:40:09,400
but primarily Christian, kind of saturated,

2168
01:40:09,400 --> 01:40:11,720
kind of Christian, post-Christian, secularized Christian,

2169
01:40:11,720 --> 01:40:13,320
kind of world in the West.

2170
01:40:14,120 --> 01:40:15,640
And, of course, court of Christianity

2171
01:40:15,640 --> 01:40:18,520
is the idea of the Second Coming and the revelations

2172
01:40:18,520 --> 01:40:22,280
and Jesus returning in the thousand-year utopia on Earth

2173
01:40:22,280 --> 01:40:24,200
and then the rapture and all that stuff.

2174
01:40:25,640 --> 01:40:27,400
We collectively, as a society,

2175
01:40:27,400 --> 01:40:29,400
we don't necessarily take all that fully seriously now.

2176
01:40:29,400 --> 01:40:32,520
So what we do is we create our secularized versions of that.

2177
01:40:32,520 --> 01:40:34,200
We keep looking for utopia.

2178
01:40:34,200 --> 01:40:36,280
We keep looking for basically the end of the world.

2179
01:40:36,840 --> 01:40:38,600
And so what you see over decades

2180
01:40:38,600 --> 01:40:40,040
is basically a pattern of these sort of...

2181
01:40:40,520 --> 01:40:43,000
This is what cults are.

2182
01:40:43,000 --> 01:40:43,960
This is how cults form,

2183
01:40:43,960 --> 01:40:45,800
as they form around some theory of the end of the world.

2184
01:40:45,800 --> 01:40:49,000
And so the people's temple cult, the Manson cult,

2185
01:40:49,000 --> 01:40:52,440
the Heaven's Gate cult, the David Koresh cult.

2186
01:40:52,440 --> 01:40:53,560
What they're all organized around

2187
01:40:53,560 --> 01:40:55,560
is like there's going to be this thing that's going to happen

2188
01:40:55,560 --> 01:40:58,040
that's going to basically bring civilization crashing down.

2189
01:40:58,040 --> 01:41:00,040
And then we have this special elite group of people

2190
01:41:00,040 --> 01:41:01,640
who are going to see it coming and prepare for it.

2191
01:41:02,280 --> 01:41:04,200
And then they're the people who are either going to stop it

2192
01:41:04,200 --> 01:41:05,240
or are failing stopping it.

2193
01:41:05,240 --> 01:41:07,160
They're going to be the people who survived on the other side

2194
01:41:07,160 --> 01:41:09,160
and ultimately get credit for having been right.

2195
01:41:09,160 --> 01:41:10,680
Why is that so compelling, do you think?

2196
01:41:11,720 --> 01:41:14,440
Because it satisfies this very deep need we have

2197
01:41:14,440 --> 01:41:16,280
for transcendence and meaning

2198
01:41:17,080 --> 01:41:20,200
that got stripped away when we became secular.

2199
01:41:20,200 --> 01:41:22,040
Yeah, but why does transcendence

2200
01:41:22,040 --> 01:41:24,520
involve the destruction of human civilization?

2201
01:41:25,160 --> 01:41:30,200
Because it's like a very deep psychological thing,

2202
01:41:30,200 --> 01:41:31,960
because it's like how plausible is it

2203
01:41:31,960 --> 01:41:33,720
that we live in a world where everything's just kind of all right?

2204
01:41:34,840 --> 01:41:37,160
Right, how exciting is that?

2205
01:41:38,120 --> 01:41:39,480
We want more than that.

2206
01:41:39,480 --> 01:41:40,920
But that's the deep question I'm asking.

2207
01:41:41,960 --> 01:41:44,520
Why is it not exciting to live in a world

2208
01:41:44,520 --> 01:41:45,720
where everything's just all right?

2209
01:41:47,080 --> 01:41:50,200
I think most of the animal kingdom

2210
01:41:50,200 --> 01:41:52,440
would be so happy with just all right,

2211
01:41:52,440 --> 01:41:53,480
because that means survival.

2212
01:41:54,200 --> 01:41:56,920
Why are we, maybe that's what it is.

2213
01:41:56,920 --> 01:41:59,880
Why are we conjuring up things to worry about?

2214
01:42:00,440 --> 01:42:02,760
So CS Lewis called it the God-shaped hole.

2215
01:42:03,320 --> 01:42:06,600
So there's a God-shaped hole in the human experience,

2216
01:42:06,600 --> 01:42:08,600
consciousness, soul, whatever you want to call it,

2217
01:42:08,600 --> 01:42:11,720
where there's got to be something that's bigger than all this.

2218
01:42:12,360 --> 01:42:13,560
There's got to be something transcendent.

2219
01:42:13,560 --> 01:42:14,920
There's got to be something that is bigger,

2220
01:42:14,920 --> 01:42:16,920
right, bigger, the bigger purpose of bigger meaning.

2221
01:42:17,560 --> 01:42:19,640
And so we have run the experiment

2222
01:42:19,640 --> 01:42:21,960
of we're just going to use science and rationality

2223
01:42:21,960 --> 01:42:24,840
and kind of everything's just going to kind of be as it appears.

2224
01:42:24,840 --> 01:42:28,440
And a large number of people have found that very deeply wanting

2225
01:42:29,240 --> 01:42:30,840
and have constructed narratives.

2226
01:42:30,840 --> 01:42:32,840
And this is the story of the 20th century, right?

2227
01:42:32,840 --> 01:42:34,600
Communism was one of those.

2228
01:42:34,600 --> 01:42:36,360
Communism was a form of this.

2229
01:42:36,360 --> 01:42:37,480
Nazism was a form of this.

2230
01:42:38,760 --> 01:42:41,560
Some people, you can see movements like this

2231
01:42:41,560 --> 01:42:43,240
playing out all over the world right now.

2232
01:42:43,240 --> 01:42:46,840
So you construct a kind of devil, a kind of source of evil,

2233
01:42:46,840 --> 01:42:48,600
and we're going to transcend beyond it.

2234
01:42:48,600 --> 01:42:52,680
Yeah, and the millenarian, the millenarian is kind of,

2235
01:42:52,680 --> 01:42:53,720
when you see a millenarian cult,

2236
01:42:53,720 --> 01:42:55,640
they put a really specific point on it,

2237
01:42:55,640 --> 01:42:56,760
which is end of the world, right?

2238
01:42:56,760 --> 01:42:59,240
There is some change coming.

2239
01:42:59,240 --> 01:43:02,040
And that change that's coming is so profound and so important

2240
01:43:02,040 --> 01:43:06,120
that it's either going to lead to utopia or hell on earth, right?

2241
01:43:06,760 --> 01:43:09,080
And it is going to, and then it's like,

2242
01:43:09,080 --> 01:43:11,800
what if you actually knew that that was going to happen, right?

2243
01:43:11,800 --> 01:43:14,040
What would you do, right?

2244
01:43:14,040 --> 01:43:15,480
How would you prepare yourself for it?

2245
01:43:15,480 --> 01:43:18,600
How would you come together with a group of like-minded people, right?

2246
01:43:18,600 --> 01:43:19,640
How would you, what would you do?

2247
01:43:19,640 --> 01:43:21,720
Would you plan like caches of weapons in the woods?

2248
01:43:21,720 --> 01:43:24,840
Would you like, I don't know, create underground buckers?

2249
01:43:24,840 --> 01:43:27,000
Would you spend your life trying to figure out a way

2250
01:43:27,000 --> 01:43:28,200
to avoid having it happen?

2251
01:43:28,200 --> 01:43:31,640
Yeah, that's a really compelling, exciting idea

2252
01:43:32,760 --> 01:43:36,440
to have a club over, to have a little bit of travel.

2253
01:43:36,440 --> 01:43:38,040
Like you get together on a Saturday night

2254
01:43:38,040 --> 01:43:40,680
and drink some beers and talk about the end of the world

2255
01:43:40,680 --> 01:43:43,400
and how you are the only ones who have figured it out.

2256
01:43:43,400 --> 01:43:44,120
Yeah.

2257
01:43:44,120 --> 01:43:45,720
And then once you lock in on that,

2258
01:43:45,720 --> 01:43:47,240
like how can you do anything else with your life?

2259
01:43:47,240 --> 01:43:48,840
Like this is obviously the thing that you have to do.

2260
01:43:48,840 --> 01:43:51,240
And then there's a psychological effect you alluded to.

2261
01:43:51,240 --> 01:43:52,200
There's a psychological effect.

2262
01:43:52,200 --> 01:43:53,480
If you take a set of true believers

2263
01:43:53,480 --> 01:43:55,560
and you lead them to themselves, they get more radical,

2264
01:43:55,560 --> 01:43:57,400
because they self-radicalize each other.

2265
01:43:57,400 --> 01:44:01,880
That said, it doesn't mean they're not sometimes right.

2266
01:44:01,880 --> 01:44:02,760
Yeah, the end of the world might be.

2267
01:44:02,760 --> 01:44:03,640
Yes, correct.

2268
01:44:03,640 --> 01:44:04,760
Like they might be right.

2269
01:44:04,760 --> 01:44:05,240
Yeah.

2270
01:44:05,240 --> 01:44:05,800
But like we...

2271
01:44:05,800 --> 01:44:06,840
Have some pamphlets for you.

2272
01:44:06,840 --> 01:44:09,160
Exactly.

2273
01:44:09,160 --> 01:44:11,800
I mean, there's, I mean, we'll talk about nuclear weapons

2274
01:44:11,800 --> 01:44:13,880
because you have a really interesting little moment

2275
01:44:13,880 --> 01:44:16,120
that I learned about in your essay.

2276
01:44:16,120 --> 01:44:17,800
But you know, sometimes it could be right.

2277
01:44:17,800 --> 01:44:18,360
Yeah.

2278
01:44:18,360 --> 01:44:20,600
Because we're still, you were developing

2279
01:44:20,600 --> 01:44:23,640
more and more powerful technologies in this case.

2280
01:44:23,640 --> 01:44:25,160
And we don't know what the impact

2281
01:44:25,160 --> 01:44:27,000
they will have on human civilization.

2282
01:44:27,000 --> 01:44:29,080
While we can highlight all the different predictions

2283
01:44:29,080 --> 01:44:30,280
about how it will be positive.

2284
01:44:30,280 --> 01:44:34,120
But the risks are there and you discuss some of them.

2285
01:44:34,120 --> 01:44:35,720
Well, the steelman is...

2286
01:44:35,720 --> 01:44:36,360
The steelman...

2287
01:44:36,360 --> 01:44:38,440
Actually, the steelman and his refutation are the same,

2288
01:44:38,440 --> 01:44:40,360
which is you can't predict what's going to happen, right?

2289
01:44:41,320 --> 01:44:44,360
You can't rule out that this will not end everything, right?

2290
01:44:44,360 --> 01:44:45,800
But the response to that is,

2291
01:44:45,800 --> 01:44:47,720
you have just made a completely non-scientific claim.

2292
01:44:48,280 --> 01:44:49,960
You've made a religious claim, not a scientific claim.

2293
01:44:49,960 --> 01:44:51,240
How does it get disproven?

2294
01:44:51,240 --> 01:44:51,720
There is...

2295
01:44:51,720 --> 01:44:52,120
And there's no...

2296
01:44:52,120 --> 01:44:53,560
By definition, with these kinds of claims,

2297
01:44:53,560 --> 01:44:55,320
there's no way to disprove them, right?

2298
01:44:55,880 --> 01:44:56,840
And so there's no...

2299
01:44:56,840 --> 01:44:57,800
You just go right on the list.

2300
01:44:57,800 --> 01:44:59,080
There's no hypothesis.

2301
01:44:59,080 --> 01:45:01,240
There's no testability of the hypothesis.

2302
01:45:01,240 --> 01:45:04,440
There's no way to falsify the hypothesis.

2303
01:45:04,440 --> 01:45:07,000
There's no way to measure progress along the arc.

2304
01:45:07,800 --> 01:45:09,400
Like, it's just all completely missing.

2305
01:45:09,400 --> 01:45:11,480
And so it's not scientific.

2306
01:45:11,480 --> 01:45:11,720
And...

2307
01:45:11,720 --> 01:45:13,480
Well, I don't think it's completely missing.

2308
01:45:14,520 --> 01:45:15,160
It's somewhat missing.

2309
01:45:15,160 --> 01:45:17,320
So, for example, the people that say,

2310
01:45:17,320 --> 01:45:18,520
yeah, it's going to kill all of us.

2311
01:45:19,720 --> 01:45:22,120
I mean, they usually have ideas about how to do that,

2312
01:45:22,120 --> 01:45:26,600
whether it's the paperclip maximizer or it escapes.

2313
01:45:27,560 --> 01:45:29,640
There's mechanism by which you can imagine it

2314
01:45:29,640 --> 01:45:30,760
killing all humans.

2315
01:45:30,760 --> 01:45:31,160
Models.

2316
01:45:31,880 --> 01:45:38,760
And you can disprove it by saying there is a limit

2317
01:45:39,720 --> 01:45:42,440
to the speed at which intelligence increases.

2318
01:45:43,880 --> 01:45:45,240
Maybe show that...

2319
01:45:47,640 --> 01:45:50,280
They sort of rigorously really describe model,

2320
01:45:51,000 --> 01:45:52,920
like how it could happen,

2321
01:45:52,920 --> 01:45:55,800
and say, no, here's a physics limitation.

2322
01:45:55,800 --> 01:45:58,760
There's a physical limitation to how these systems

2323
01:45:58,760 --> 01:46:00,840
would actually do damage to human civilization.

2324
01:46:00,840 --> 01:46:04,760
And it is possible they will kill 10% to 20% of the population,

2325
01:46:04,760 --> 01:46:08,840
but it seems impossible for them to kill 99%.

2326
01:46:08,840 --> 01:46:10,040
It was practical counter-arguments, right?

2327
01:46:10,040 --> 01:46:11,480
So you mentioned basically what I described

2328
01:46:11,480 --> 01:46:13,000
as the thermodynamic counter-argument,

2329
01:46:13,000 --> 01:46:14,120
which is sitting here today.

2330
01:46:14,120 --> 01:46:16,520
It's like where would the evil AGI get the GPUs?

2331
01:46:16,520 --> 01:46:16,920
Yeah.

2332
01:46:16,920 --> 01:46:18,280
Because, like, they don't exist.

2333
01:46:18,280 --> 01:46:20,520
So you're going to have a very frustrated baby evil AGI

2334
01:46:20,520 --> 01:46:22,200
who's going to be, like, trying to buy NVIDIA stock

2335
01:46:22,200 --> 01:46:24,440
or something to get them to finally make some chips.

2336
01:46:25,000 --> 01:46:25,240
Right?

2337
01:46:25,240 --> 01:46:27,720
So the serious form of that is the thermodynamic argument,

2338
01:46:27,720 --> 01:46:29,640
which is, like, okay, where's the energy going to come from?

2339
01:46:29,640 --> 01:46:31,080
Where's the processor going to be running?

2340
01:46:31,080 --> 01:46:32,520
Where's the data center going to be happening?

2341
01:46:32,520 --> 01:46:33,800
How is this going to be happening in secret,

2342
01:46:33,800 --> 01:46:34,600
such that you know it's not...

2343
01:46:34,600 --> 01:46:35,240
You know.

2344
01:46:35,240 --> 01:46:37,240
So that's a practical counter-argument

2345
01:46:37,240 --> 01:46:38,440
to the runaway AGI thing.

2346
01:46:38,440 --> 01:46:39,080
I have a...

2347
01:46:39,080 --> 01:46:39,560
But I have a...

2348
01:46:39,560 --> 01:46:41,000
And we can argue that and discuss that.

2349
01:46:41,000 --> 01:46:42,360
I have a deeper objection to it,

2350
01:46:42,360 --> 01:46:44,360
which is this is all forecasting.

2351
01:46:44,360 --> 01:46:45,240
It's all modeling.

2352
01:46:45,240 --> 01:46:47,320
It's all future prediction.

2353
01:46:47,320 --> 01:46:49,080
It's all future hypothesizing.

2354
01:46:49,960 --> 01:46:50,680
It's not science.

2355
01:46:51,880 --> 01:46:52,200
Sure.

2356
01:46:52,200 --> 01:46:54,760
It is the opposite of science.

2357
01:46:54,760 --> 01:46:56,280
So they'll pull up Carl Sagan.

2358
01:46:56,280 --> 01:46:58,280
Extraordinary claims require extraordinary proof.

2359
01:46:58,280 --> 01:46:58,920
Right?

2360
01:46:58,920 --> 01:47:00,360
These are extraordinary claims.

2361
01:47:00,360 --> 01:47:03,240
The policies that are being called for, right,

2362
01:47:03,240 --> 01:47:05,800
to prevent this are of extraordinary magnitude,

2363
01:47:05,800 --> 01:47:07,560
and I think we're going to cause extraordinary damage.

2364
01:47:08,200 --> 01:47:09,960
And this is all being done on the basis of something

2365
01:47:09,960 --> 01:47:11,560
that is literally not scientific.

2366
01:47:11,560 --> 01:47:12,760
It's not a testable hypothesis.

2367
01:47:12,760 --> 01:47:13,720
So the moment you say,

2368
01:47:13,720 --> 01:47:15,000
AI is going to kill all of us,

2369
01:47:15,000 --> 01:47:16,360
therefore we should ban it

2370
01:47:16,360 --> 01:47:18,600
or we should regulate all that kind of stuff,

2371
01:47:18,600 --> 01:47:19,880
that's when it starts getting serious.

2372
01:47:19,880 --> 01:47:22,040
Or start, you know, military airstrikes and data centers.

2373
01:47:22,520 --> 01:47:23,320
Oh, boy.

2374
01:47:23,320 --> 01:47:23,560
Right?

2375
01:47:24,840 --> 01:47:25,240
And like...

2376
01:47:26,760 --> 01:47:29,720
Yeah, that's when it starts getting real weird.

2377
01:47:29,720 --> 01:47:31,240
So here's the problem of millinery and cults.

2378
01:47:31,240 --> 01:47:32,920
They have a hard time staying away from violence.

2379
01:47:34,360 --> 01:47:35,800
Yeah, but violence is so fun.

2380
01:47:38,920 --> 01:47:39,960
If you're on the right end of it,

2381
01:47:40,520 --> 01:47:41,720
they have a hard time avoiding violence.

2382
01:47:41,720 --> 01:47:43,000
The reason they have a hard time avoiding violence

2383
01:47:43,000 --> 01:47:45,160
is if you actually believe the claim,

2384
01:47:46,040 --> 01:47:48,520
right, then what would you do to stop the end of the world?

2385
01:47:49,080 --> 01:47:50,360
Well, you would do anything, right?

2386
01:47:51,000 --> 01:47:52,520
And so, and this is where you get...

2387
01:47:52,520 --> 01:47:55,160
And again, if you just look at the history of millinery and cults,

2388
01:47:55,160 --> 01:47:56,360
this is where you get the people's temple

2389
01:47:56,360 --> 01:47:57,720
and everybody killing themselves in the jungle.

2390
01:47:57,720 --> 01:47:59,000
And this is where you get Charles Manson

2391
01:47:59,000 --> 01:48:01,240
and, you know, setting in to kill the pigs.

2392
01:48:01,960 --> 01:48:03,480
Like, this is the problem with these.

2393
01:48:03,480 --> 01:48:06,600
They have a very hard time drawing the line at actual violence.

2394
01:48:06,600 --> 01:48:09,320
And I think in this case, there's...

2395
01:48:09,320 --> 01:48:11,640
I mean, they're already calling for it, like today.

2396
01:48:11,640 --> 01:48:13,400
And, you know, where this goes from here

2397
01:48:13,400 --> 01:48:14,360
is they get more worked up.

2398
01:48:14,360 --> 01:48:16,520
Like, I think it's like really concerning.

2399
01:48:16,520 --> 01:48:18,120
Okay, but that's kind of the extremes.

2400
01:48:18,520 --> 01:48:21,560
You know, the extremes of anything are always concerning.

2401
01:48:22,680 --> 01:48:24,360
It's also possible to kind of believe

2402
01:48:24,360 --> 01:48:27,240
that AI has a very high likelihood of killing all of us.

2403
01:48:28,360 --> 01:48:28,760
But there's...

2404
01:48:29,880 --> 01:48:34,840
And therefore, we should maybe consider slowing development

2405
01:48:34,840 --> 01:48:35,800
or regulating.

2406
01:48:35,800 --> 01:48:37,880
So not violence or any of these kinds of things,

2407
01:48:37,880 --> 01:48:41,080
but saying like, all right, let's take a pause here.

2408
01:48:41,080 --> 01:48:43,560
You know, biological weapons, nuclear weapons.

2409
01:48:43,560 --> 01:48:45,080
Like, whoa, whoa, whoa, whoa, whoa.

2410
01:48:45,080 --> 01:48:47,560
This is like serious stuff.

2411
01:48:47,560 --> 01:48:48,920
We should be careful.

2412
01:48:48,920 --> 01:48:53,080
So it is possible to kind of have a more rational response, right?

2413
01:48:53,080 --> 01:48:55,000
If you believe this risk is real.

2414
01:48:55,000 --> 01:48:55,400
Believe.

2415
01:48:56,200 --> 01:48:56,520
Yes.

2416
01:48:57,400 --> 01:48:59,480
Is it possible to have a scientific approach

2417
01:48:59,480 --> 01:49:02,280
to the prediction of the future?

2418
01:49:02,280 --> 01:49:04,120
I mean, we just went through this with COVID.

2419
01:49:04,120 --> 01:49:05,080
What do we know about modeling?

2420
01:49:06,440 --> 01:49:07,240
Well, I mean...

2421
01:49:07,240 --> 01:49:08,840
What do we learn about modeling with COVID?

2422
01:49:09,720 --> 01:49:11,000
There's a lot of lessons.

2423
01:49:11,000 --> 01:49:11,800
They didn't work at all.

2424
01:49:12,920 --> 01:49:13,800
They worked poorly.

2425
01:49:13,800 --> 01:49:14,920
The models were terrible.

2426
01:49:14,920 --> 01:49:16,200
The models were useless.

2427
01:49:16,200 --> 01:49:19,880
I don't know if the models were useless or the people

2428
01:49:19,880 --> 01:49:23,160
interpreting the models and then the centralized institutions

2429
01:49:23,160 --> 01:49:26,040
that were creating policy rapidly based on the models

2430
01:49:26,040 --> 01:49:29,640
and leveraging the models in order to support their narratives

2431
01:49:29,640 --> 01:49:32,840
versus actually interpreting the airbars and the models

2432
01:49:32,840 --> 01:49:33,560
and all that kind of stuff.

2433
01:49:33,560 --> 01:49:35,640
What you had with COVID, in my view, you had with COVID

2434
01:49:35,640 --> 01:49:37,640
is you had these experts showing up.

2435
01:49:37,640 --> 01:49:38,760
They claimed to be scientists

2436
01:49:38,760 --> 01:49:40,920
and they had no testable hypotheses whatsoever.

2437
01:49:40,920 --> 01:49:42,280
They had a bunch of models.

2438
01:49:42,280 --> 01:49:44,120
They had a bunch of forecasts and they had a bunch of theories

2439
01:49:44,120 --> 01:49:45,880
and they laid these out in front of policymakers

2440
01:49:45,880 --> 01:49:49,080
and policymakers freaked out and panicked and implemented

2441
01:49:49,080 --> 01:49:51,080
a whole bunch of really terrible decisions

2442
01:49:51,080 --> 01:49:52,600
that we're still living with the consequences of.

2443
01:49:54,360 --> 01:49:57,320
There was never any empirical foundation to any of the models.

2444
01:49:57,320 --> 01:49:58,600
None of them ever came true.

2445
01:49:58,600 --> 01:50:01,560
Yeah, to push back, there were certainly Baptist and bootleggers

2446
01:50:01,560 --> 01:50:04,120
in the context of this pandemic,

2447
01:50:04,120 --> 01:50:05,880
but there's still a usefulness to models.

2448
01:50:06,600 --> 01:50:08,840
So not if they're reliably wrong.

2449
01:50:08,840 --> 01:50:10,520
Then they're actually anti-useful.

2450
01:50:10,520 --> 01:50:11,400
They're actually damaging.

2451
01:50:11,400 --> 01:50:12,920
But what do you do with the pandemic?

2452
01:50:12,920 --> 01:50:15,560
What do you do with any kind of threat?

2453
01:50:15,560 --> 01:50:19,880
Don't you want to kind of have several models to play with

2454
01:50:19,880 --> 01:50:23,640
as part of the discussion of like, what the hell do we do here?

2455
01:50:23,640 --> 01:50:24,440
I mean, do they work?

2456
01:50:25,320 --> 01:50:27,560
Because they're an expectation that they actually like work,

2457
01:50:27,560 --> 01:50:28,920
that they have actual predictive value.

2458
01:50:29,720 --> 01:50:31,000
I mean, as far as I can tell with COVID,

2459
01:50:31,000 --> 01:50:33,400
we just saw the policymakers just saw up themselves

2460
01:50:33,400 --> 01:50:34,440
into believing that there was some...

2461
01:50:34,440 --> 01:50:36,680
I mean, look, the scientists were at fault.

2462
01:50:36,680 --> 01:50:38,520
The quote-unquote scientists showed up.

2463
01:50:39,400 --> 01:50:40,440
So I had to submit that into this.

2464
01:50:40,440 --> 01:50:41,400
So there was a...

2465
01:50:41,400 --> 01:50:43,560
Remember the Imperial College models out of London

2466
01:50:43,560 --> 01:50:44,520
were the ones that were...

2467
01:50:44,520 --> 01:50:45,640
Like, these are the gold standard models.

2468
01:50:46,200 --> 01:50:47,960
So a friend of mine runs a big software company

2469
01:50:47,960 --> 01:50:49,960
and he was like, wow, this is like COVID's really scary.

2470
01:50:49,960 --> 01:50:51,560
And he's like, you know, he contacted this research

2471
01:50:51,560 --> 01:50:53,080
and he's like, you know, do you need some help?

2472
01:50:53,080 --> 01:50:55,240
You've been just building this model on your own for 20 years.

2473
01:50:55,240 --> 01:50:55,880
Do you need some food?

2474
01:50:55,880 --> 01:50:57,880
Do you like our coders to basically restructure it

2475
01:50:57,880 --> 01:50:59,400
so it can be fully adapted for COVID?

2476
01:50:59,400 --> 01:51:01,800
And the guy said yes and sent over the code.

2477
01:51:01,800 --> 01:51:03,720
And my friend said it was like the worst spaghetti code

2478
01:51:03,720 --> 01:51:04,520
he's ever seen.

2479
01:51:04,520 --> 01:51:06,840
That doesn't mean it's not possible to construct

2480
01:51:06,840 --> 01:51:09,560
a good model of pandemic with the correct airbars

2481
01:51:09,560 --> 01:51:12,520
with a high number of parameters that are continuously,

2482
01:51:13,080 --> 01:51:16,520
many times a day updated as we get more data about a pandemic.

2483
01:51:16,520 --> 01:51:20,680
I would like to believe when a pandemic hits the world,

2484
01:51:20,680 --> 01:51:22,360
the best computer scientists in the world,

2485
01:51:22,360 --> 01:51:25,400
the best software engineers respond aggressively

2486
01:51:25,400 --> 01:51:29,000
and as input take the data that we know about the virus

2487
01:51:29,000 --> 01:51:32,680
and it's an output, say here's what's happening

2488
01:51:33,320 --> 01:51:35,320
in terms of how quickly it's spreading,

2489
01:51:35,320 --> 01:51:37,320
what that lead in terms of hospitalization

2490
01:51:37,320 --> 01:51:38,840
and deaths and all that kind of stuff.

2491
01:51:38,840 --> 01:51:41,560
Here's how likely, how contagious it likely is.

2492
01:51:41,560 --> 01:51:44,840
Here's how deadly it likely is based on different conditions,

2493
01:51:44,840 --> 01:51:46,680
based on different ages and demographics

2494
01:51:46,680 --> 01:51:47,640
and all that kind of stuff.

2495
01:51:47,640 --> 01:51:49,560
So here's the best kinds of policy.

2496
01:51:49,560 --> 01:51:54,040
It feels like you can have models, machine learning,

2497
01:51:54,760 --> 01:51:58,920
that kind of, they don't perfectly predict the future,

2498
01:51:58,920 --> 01:52:01,640
but they help you do something

2499
01:52:01,640 --> 01:52:03,240
because there's pandemics that are like,

2500
01:52:06,520 --> 01:52:08,440
meh, they don't really do much harm.

2501
01:52:08,440 --> 01:52:10,760
And there's pandemics, you can imagine them,

2502
01:52:10,760 --> 01:52:12,840
they can do a huge amount of harm.

2503
01:52:12,840 --> 01:52:14,520
Like they can kill a lot of people.

2504
01:52:14,520 --> 01:52:18,680
So you should probably have some kind of data-driven models

2505
01:52:18,680 --> 01:52:21,560
that keep updating, that allow you to make decisions

2506
01:52:21,560 --> 01:52:23,720
that are basically like, where, how bad is this thing?

2507
01:52:24,600 --> 01:52:28,840
Now you can criticize how horrible all of that went

2508
01:52:28,840 --> 01:52:30,120
with the response to this pandemic,

2509
01:52:30,120 --> 01:52:32,520
but I just feel like there might be some value to models.

2510
01:52:32,520 --> 01:52:35,000
So to be useful at some point, it has to be predictive, right?

2511
01:52:35,000 --> 01:52:39,240
So the easy thing for me to do is to say,

2512
01:52:39,240 --> 01:52:39,880
obviously, you're right.

2513
01:52:39,880 --> 01:52:41,480
Obviously, I want to see that just as much as you do

2514
01:52:41,480 --> 01:52:43,000
because anything that makes it easier to navigate

2515
01:52:43,000 --> 01:52:44,920
through society, through a wrenching risk like that,

2516
01:52:44,920 --> 01:52:46,120
it sounds great.

2517
01:52:47,240 --> 01:52:49,480
The harder objection to it is just simply,

2518
01:52:49,480 --> 01:52:52,520
you are trying to model a complex dynamic system

2519
01:52:52,520 --> 01:52:55,160
with eight billion moving parts, like not possible.

2520
01:52:55,160 --> 01:52:55,720
It's very tough.

2521
01:52:55,720 --> 01:52:57,400
Can't be done, complex systems can't be done.

2522
01:52:58,520 --> 01:53:01,400
Machine learning says hold my beer, but is possible?

2523
01:53:01,400 --> 01:53:01,720
No?

2524
01:53:01,720 --> 01:53:02,280
I don't know.

2525
01:53:02,280 --> 01:53:03,480
I would like to believe that it is.

2526
01:53:04,120 --> 01:53:05,640
Put it this way, I think where you and I would agree

2527
01:53:05,640 --> 01:53:08,200
is I think we would like that to be the case.

2528
01:53:08,280 --> 01:53:09,640
We are strongly in favor of it.

2529
01:53:10,280 --> 01:53:12,040
I think we would also agree that no such thing

2530
01:53:12,040 --> 01:53:14,120
with respect to COVID or pandemics, no such thing,

2531
01:53:14,120 --> 01:53:16,280
at least neither you nor I think are aware,

2532
01:53:16,280 --> 01:53:17,880
I'm not aware of anything like that today.

2533
01:53:17,880 --> 01:53:21,560
My main worry with the response to the pandemic is that,

2534
01:53:22,840 --> 01:53:27,000
same as with aliens, is that even if such a thing existed,

2535
01:53:27,560 --> 01:53:33,880
and as possible it existed, the policymakers were not paying

2536
01:53:33,880 --> 01:53:34,360
attention.

2537
01:53:34,360 --> 01:53:37,480
Like there was no mechanism that allowed those kinds of models

2538
01:53:37,480 --> 01:53:38,360
to percolate out.

2539
01:53:38,360 --> 01:53:40,040
I think we have the opposite problem during COVID.

2540
01:53:40,040 --> 01:53:43,560
I think the policymakers, I think these people with basically

2541
01:53:43,560 --> 01:53:45,480
fake science had too much access to the policymakers.

2542
01:53:46,600 --> 01:53:49,560
Right, but the policymakers also wanted,

2543
01:53:49,560 --> 01:53:51,640
they had a narrative in mind and they also wanted to use

2544
01:53:51,640 --> 01:53:54,600
whatever model that fit that narrative to help them out.

2545
01:53:54,600 --> 01:53:57,640
So it felt like there was a lot of politics and not enough science.

2546
01:53:57,640 --> 01:54:00,040
Although a big part of what was happening, a big reason we got

2547
01:54:00,040 --> 01:54:02,200
lockdowns for as long as we did was because these scientists came

2548
01:54:02,200 --> 01:54:04,120
in with these like doomsday scenarios that were like just

2549
01:54:04,120 --> 01:54:05,320
like completely off the hook.

2550
01:54:05,320 --> 01:54:07,160
Scientists and quotes, that's not,

2551
01:54:07,160 --> 01:54:08,200
quote unquote, scientists.

2552
01:54:08,200 --> 01:54:10,840
That's not, let's give love to science.

2553
01:54:10,840 --> 01:54:12,120
That is the way out.

2554
01:54:12,120 --> 01:54:14,040
Science is a process of testing hypotheses.

2555
01:54:14,040 --> 01:54:14,680
Yeah.

2556
01:54:14,680 --> 01:54:17,400
Modeling does not involve testable hypotheses, right?

2557
01:54:17,400 --> 01:54:19,480
Like I don't even know that, I actually don't need,

2558
01:54:19,480 --> 01:54:21,480
I didn't even know that modeling actually qualifies to science.

2559
01:54:22,360 --> 01:54:25,160
Maybe that's a side conversation we could have some time over a beer.

2560
01:54:25,160 --> 01:54:27,960
That's really interesting, but what do we do about the future?

2561
01:54:27,960 --> 01:54:28,920
I mean, what?

2562
01:54:28,920 --> 01:54:31,160
So number one is when we start with number one, humility,

2563
01:54:31,720 --> 01:54:33,560
goes back to this thing of how do we determine the truth.

2564
01:54:34,120 --> 01:54:36,200
Number two is we don't believe, you know, it's the old,

2565
01:54:36,200 --> 01:54:38,120
I've got a hammer, everything looks like a nail, right?

2566
01:54:39,240 --> 01:54:41,320
I've got, oh, this is one of the reasons I gave you,

2567
01:54:41,320 --> 01:54:44,680
I gave Lex a book, which is the topic of the book is what happens

2568
01:54:44,680 --> 01:54:47,880
when scientists basically stray off the path of technical knowledge

2569
01:54:47,880 --> 01:54:49,960
and start to weigh in on politics and societal issues.

2570
01:54:49,960 --> 01:54:51,400
In this case, philosophers.

2571
01:54:51,400 --> 01:54:52,600
Well, in this case, philosophers.

2572
01:54:52,600 --> 01:54:54,600
But he actually talks in this book about like Einstein,

2573
01:54:54,600 --> 01:54:56,280
he talks about the nuclear age and Einstein,

2574
01:54:56,280 --> 01:55:00,200
he talks about the physicists actually doing very similar things

2575
01:55:00,200 --> 01:55:00,760
at the time.

2576
01:55:00,760 --> 01:55:03,880
The book is when reason goes on holiday philosophers

2577
01:55:03,880 --> 01:55:06,120
and politics by Nevin.

2578
01:55:06,680 --> 01:55:07,560
And it's just a story.

2579
01:55:07,560 --> 01:55:08,200
It's a story.

2580
01:55:08,200 --> 01:55:09,800
There's there are other books on this topic,

2581
01:55:09,800 --> 01:55:10,600
but this is a new one.

2582
01:55:10,600 --> 01:55:11,080
That's really good.

2583
01:55:11,080 --> 01:55:13,720
That's just the story of what happens when experts in a certain domain

2584
01:55:13,720 --> 01:55:16,200
decide to weigh in and become basically social engineers

2585
01:55:16,200 --> 01:55:19,160
and end up political, you know, basically political advisers.

2586
01:55:19,160 --> 01:55:21,080
And it's just a story of just unending catastrophe.

2587
01:55:21,880 --> 01:55:22,040
Right.

2588
01:55:22,040 --> 01:55:23,480
And I think that's what happened with COVID again.

2589
01:55:24,840 --> 01:55:27,640
And I found this book a highly entertaining and eye-opening read

2590
01:55:27,640 --> 01:55:29,960
filled with the amazing anecdotes of irrationality

2591
01:55:29,960 --> 01:55:32,120
and craziness by famous recent philosophers.

2592
01:55:33,080 --> 01:55:35,320
After you read this book, you will not look at Einstein the same.

2593
01:55:35,320 --> 01:55:36,040
Oh, boy.

2594
01:55:36,040 --> 01:55:36,200
Yeah.

2595
01:55:37,160 --> 01:55:38,280
Don't destroy my heroes.

2596
01:55:38,280 --> 01:55:39,720
You will not be a hero of yours anymore.

2597
01:55:41,240 --> 01:55:41,720
I'm sorry.

2598
01:55:41,720 --> 01:55:43,560
You probably shouldn't read the book.

2599
01:55:43,560 --> 01:55:43,880
All right.

2600
01:55:43,880 --> 01:55:47,800
But here's the thing, the AI risk people,

2601
01:55:47,800 --> 01:55:49,000
they don't even have the COVID model.

2602
01:55:50,040 --> 01:55:51,080
At least not that I'm aware of.

2603
01:55:51,080 --> 01:55:51,480
No.

2604
01:55:51,480 --> 01:55:53,080
Like there's not even the equivalent of the COVID model.

2605
01:55:53,080 --> 01:55:54,280
They don't even have the spaghetti code.

2606
01:55:55,560 --> 01:55:58,520
They've got a theory and a warning and a this and a that.

2607
01:55:58,520 --> 01:56:01,960
And like, if you ask like, okay, well, here's the ultimate example is,

2608
01:56:01,960 --> 01:56:03,240
okay, how do we know, right?

2609
01:56:03,240 --> 01:56:04,520
How do we know that an AI is running away?

2610
01:56:04,520 --> 01:56:07,640
Like, how do we know that the fume takeoff thing is actually happening?

2611
01:56:07,640 --> 01:56:10,280
And the only answer that any of these guys have given that I've ever seen is,

2612
01:56:10,280 --> 01:56:14,040
oh, it's when the loss rate, the loss function in the training drops.

2613
01:56:15,160 --> 01:56:15,320
Right.

2614
01:56:15,320 --> 01:56:17,240
That's when you need to like shut down the data center.

2615
01:56:17,240 --> 01:56:17,480
Right.

2616
01:56:17,480 --> 01:56:20,120
And it's like, well, that's also what happens when you're successfully training a model.

2617
01:56:21,000 --> 01:56:25,480
Like, what even is this is not science.

2618
01:56:26,360 --> 01:56:27,640
This is not, it's not anything.

2619
01:56:27,640 --> 01:56:28,200
It's not a model.

2620
01:56:28,200 --> 01:56:29,160
It's not anything.

2621
01:56:29,160 --> 01:56:30,760
There's nothing to arguing with it.

2622
01:56:30,760 --> 01:56:31,880
It's like, you know, punching jello.

2623
01:56:31,880 --> 01:56:33,400
Like there's, what do you even respond to?

2624
01:56:34,040 --> 01:56:35,400
So just put pushback on that.

2625
01:56:35,400 --> 01:56:39,880
I don't think they have good metrics of, yeah, when the fume is happening,

2626
01:56:39,880 --> 01:56:42,040
but I think it's possible to have that.

2627
01:56:42,040 --> 01:56:47,240
Like I just, just as you speak now, I mean, it's possible to imagine there could be measures.

2628
01:56:47,240 --> 01:56:48,600
It's been 20 years.

2629
01:56:48,600 --> 01:56:49,080
No, for sure.

2630
01:56:49,080 --> 01:56:54,040
But it's been only weeks since we had a big enough breakthrough in language models.

2631
01:56:54,040 --> 01:56:56,200
We can start to actually have this.

2632
01:56:56,200 --> 01:57:01,640
The thing is the AI doomer stuff didn't have any actual systems to really work with it.

2633
01:57:01,640 --> 01:57:02,920
Now there's real systems.

2634
01:57:02,920 --> 01:57:05,560
You can start to analyze like, how does this stuff go wrong?

2635
01:57:05,560 --> 01:57:09,240
And I think you kind of agree that there is a lot of risks that we can analyze.

2636
01:57:09,240 --> 01:57:11,640
The benefits outweigh the risks in many cases.

2637
01:57:11,640 --> 01:57:12,920
Well, the risks are not existential.

2638
01:57:13,880 --> 01:57:14,200
Yes.

2639
01:57:14,200 --> 01:57:16,520
Well, not, not, not in the fume, not in the fume paper clip.

2640
01:57:16,520 --> 01:57:17,000
Not this.

2641
01:57:17,000 --> 01:57:17,480
Let me, okay.

2642
01:57:17,480 --> 01:57:19,000
There's another slide of hand that you just alluded to.

2643
01:57:19,000 --> 01:57:20,600
There's another slide of hand that happens, which is very.

2644
01:57:20,600 --> 01:57:24,440
I think I'm very good at the slide of hand thing, which is very not scientific.

2645
01:57:24,440 --> 01:57:26,200
So the book, super intelligence, right?

2646
01:57:26,200 --> 01:57:29,160
Which is like the Nick Bostrom book, which is like the origin of a lot of this stuff,

2647
01:57:29,160 --> 01:57:31,240
which is written, you know, whatever, 10 years ago or something.

2648
01:57:31,800 --> 01:57:34,920
So he does this really fascinating thing in the book, which is he basically says,

2649
01:57:36,520 --> 01:57:40,520
there are many possible routes to machine intelligence, to artificial intelligence.

2650
01:57:40,520 --> 01:57:43,320
And he describes all the different routes to artificial intelligence,

2651
01:57:43,320 --> 01:57:45,880
all the different possible, everything from biological augmentation through to,

2652
01:57:46,440 --> 01:57:47,640
you know, that all these different things.

2653
01:57:49,080 --> 01:57:52,040
One of the ones that he does not describe is large language models,

2654
01:57:52,040 --> 01:57:55,240
because of course the book was written before they were invented and so they didn't exist.

2655
01:57:56,760 --> 01:58:00,040
In the book, he just, he describes them all and then he proceeds to treat them all as if

2656
01:58:00,040 --> 01:58:00,920
they're exactly the same thing.

2657
01:58:01,800 --> 01:58:04,840
He presents them all as sort of an equivalent risk to be dealt with in an equivalent way to

2658
01:58:04,840 --> 01:58:05,880
be thought about the same way.

2659
01:58:05,880 --> 01:58:09,240
And then the risk, the quote unquote risk that's actually emerged is actually a completely

2660
01:58:09,240 --> 01:58:10,920
different technology than he was even imagining.

2661
01:58:10,920 --> 01:58:14,200
And yet all of his theories and beliefs are being transplanted by this movement,

2662
01:58:14,200 --> 01:58:15,240
like straight onto this new technology.

2663
01:58:15,240 --> 01:58:20,200
And so again, like there's no other area of science or technology where you do that.

2664
01:58:21,000 --> 01:58:24,440
Like when you're dealing with like organic chemistry versus inorganic chemistry,

2665
01:58:24,440 --> 01:58:27,560
you don't just like say, oh, with respect to like either one,

2666
01:58:27,560 --> 01:58:29,720
basically maybe, you know, growing up and eating the world or something,

2667
01:58:29,720 --> 01:58:31,080
like they're just going to operate the same way.

2668
01:58:31,080 --> 01:58:31,560
Like you don't.

2669
01:58:32,280 --> 01:58:36,920
But you can start talking about like as, as we get more and more actual systems

2670
01:58:36,920 --> 01:58:38,360
that start to get more and more intelligent,

2671
01:58:38,360 --> 01:58:41,160
you can start to actually have more scientific arguments here.

2672
01:58:42,040 --> 01:58:46,440
You know, high level, you can talk about the threat of autonomous weapons systems

2673
01:58:46,440 --> 01:58:49,720
back before we had any automation in the military.

2674
01:58:49,720 --> 01:58:51,880
And that would be like very fuzzy kind of logic.

2675
01:58:51,880 --> 01:58:56,280
But the more and more you have drones that are becoming more and more autonomous,

2676
01:58:56,280 --> 01:58:59,240
you can start imagining, okay, what does that actually look like?

2677
01:58:59,240 --> 01:59:01,720
And what's the actual threat of autonomous weapons systems?

2678
01:59:01,720 --> 01:59:03,160
How does it go wrong?

2679
01:59:03,160 --> 01:59:07,240
And still it's, it's, it's very vague, but you start to get a sense of like,

2680
01:59:07,960 --> 01:59:14,920
all right, it should probably be illegal or wrong or not allowed to do like

2681
01:59:15,880 --> 01:59:22,600
mass deployment of fully autonomous drones that are doing aerial strikes on large areas.

2682
01:59:22,600 --> 01:59:23,880
I think it should be required.

2683
01:59:23,880 --> 01:59:24,280
Right.

2684
01:59:24,280 --> 01:59:24,680
So that's,

2685
01:59:24,680 --> 01:59:25,240
No, no, no, no.

2686
01:59:25,240 --> 01:59:29,160
I think it should be required that only aerial vehicles are automated.

2687
01:59:30,200 --> 01:59:30,440
Okay.

2688
01:59:30,440 --> 01:59:31,320
So you want to go the other way?

2689
01:59:31,320 --> 01:59:31,960
I want to go the other way.

2690
01:59:32,840 --> 01:59:33,720
So that, okay.

2691
01:59:33,720 --> 01:59:36,600
I think it's obvious that the machine is going to make a better decision than the human pilot.

2692
01:59:38,760 --> 01:59:41,480
I think it's obvious that it's in the best interest of both the attacker and the defender

2693
01:59:41,480 --> 01:59:44,440
and humanity at large if machines are making more decisions than not people.

2694
01:59:44,440 --> 01:59:46,360
I think people make terrible decisions in times of war.

2695
01:59:47,160 --> 01:59:49,880
But like there's a, there's ways this can go wrong too, right?

2696
01:59:49,880 --> 01:59:52,040
Well, it's worse go terribly wrong now.

2697
01:59:53,720 --> 01:59:56,120
This goes back to the, this is that whole thing about like the self-driving,

2698
01:59:56,120 --> 01:59:58,840
does the self-driving car need to be perfect versus does it need to be better than the human

2699
01:59:58,840 --> 01:59:59,480
driver?

2700
01:59:59,480 --> 01:59:59,720
Yeah.

2701
01:59:59,720 --> 02:00:03,880
Does the automated drone need to be perfect or does it need to be better than a human pilot

2702
02:00:03,880 --> 02:00:07,160
at making decisions under enormous amounts of stress and uncertainty?

2703
02:00:07,160 --> 02:00:07,560
Yeah.

2704
02:00:07,560 --> 02:00:13,960
Well, the, on average, the, the worry that AI folks have is the runaway.

2705
02:00:13,960 --> 02:00:15,480
They're going to come alive, right?

2706
02:00:15,480 --> 02:00:17,240
Then again, that's the sleight of hand, right?

2707
02:00:17,240 --> 02:00:18,600
Or not, not come alive.

2708
02:00:18,600 --> 02:00:19,560
Well, hold on a second.

2709
02:00:19,560 --> 02:00:20,440
You're going to become.

2710
02:00:20,440 --> 02:00:22,200
You lose control as well.

2711
02:00:22,200 --> 02:00:24,680
But then they're going to develop goals of their own.

2712
02:00:24,680 --> 02:00:26,040
They're going to develop a mind of their own.

2713
02:00:26,040 --> 02:00:27,400
They're going to develop their own, right?

2714
02:00:27,400 --> 02:00:33,800
No, more, more like Chernobyl style meltdown, like just bugs in the code,

2715
02:00:34,360 --> 02:00:42,280
accidentally, you know, force you, the results in the bombing of like large civilian areas.

2716
02:00:42,280 --> 02:00:42,760
Okay.

2717
02:00:42,840 --> 02:00:49,160
And to a degree that's not possible in the, in the current military strategies.

2718
02:00:49,160 --> 02:00:49,400
I don't know.

2719
02:00:49,400 --> 02:00:50,200
The control by humans.

2720
02:00:50,200 --> 02:00:53,240
Well, actually, we've been doing a lot of mass bombings of cities for a very long time.

2721
02:00:53,240 --> 02:00:53,800
Yes.

2722
02:00:53,800 --> 02:00:55,240
And a lot of civilians died.

2723
02:00:55,240 --> 02:00:56,120
And a lot of civilians died.

2724
02:00:56,120 --> 02:00:59,960
And if you watch the documentary, The Fog of War, McNamara, it spends a big part of it

2725
02:00:59,960 --> 02:01:03,480
talking about the firebombing of the Japanese cities, burning them straight to the ground,

2726
02:01:04,360 --> 02:01:04,600
right?

2727
02:01:04,600 --> 02:01:08,360
The, the devastation in Japan, the American military firebombing the cities in Japan

2728
02:01:08,360 --> 02:01:11,640
was considerably bigger devastation than the use of nukes, right?

2729
02:01:11,640 --> 02:01:12,920
So we've been doing that for a long time.

2730
02:01:12,920 --> 02:01:14,040
We also did that to Germany.

2731
02:01:14,040 --> 02:01:16,040
By the way, Germany did that to us, right?

2732
02:01:16,040 --> 02:01:17,480
Like, that's an old tradition.

2733
02:01:17,480 --> 02:01:19,960
The minute we got airplanes, we started doing a discriminant bombing.

2734
02:01:19,960 --> 02:01:21,640
So one of the things that we're still doing it.

2735
02:01:21,640 --> 02:01:27,880
The modern U.S. military can do with technology, with automation, but technology more broadly

2736
02:01:27,880 --> 02:01:30,120
is higher and higher precision strikes.

2737
02:01:30,120 --> 02:01:30,440
Yeah.

2738
02:01:30,440 --> 02:01:33,880
And so precision is obviously, and this is the, the JDAM, right?

2739
02:01:33,880 --> 02:01:37,640
So there was this big advance, this is big advance called the JDAM, which basically was

2740
02:01:37,640 --> 02:01:41,480
strapping a GPS transceiver to a, to a, to an unguided bomb and turning it into a guided,

2741
02:01:41,480 --> 02:01:42,360
guided bomb.

2742
02:01:42,360 --> 02:01:43,080
And yeah, that's great.

2743
02:01:43,080 --> 02:01:46,600
Like, look, that's been a big advance, but, and that's like a baby version of this question,

2744
02:01:46,600 --> 02:01:49,560
which is, okay, do you want like the human pilot, like guessing where the bomb's going to land,

2745
02:01:49,560 --> 02:01:51,800
or do you want like the machine, like guiding the bomb to its destination?

2746
02:01:52,680 --> 02:01:53,800
That's a baby version of the question.

2747
02:01:53,800 --> 02:01:56,680
The next version of the question is, do you want the human or the machine deciding whether to drop

2748
02:01:56,680 --> 02:01:57,400
the bomb?

2749
02:01:57,400 --> 02:02:00,920
Everybody just assumes the human's going to do a better job for what I think are fundamentally

2750
02:02:00,920 --> 02:02:02,040
suspicious reasons.

2751
02:02:02,040 --> 02:02:03,800
Emotional psychological reasons.

2752
02:02:03,800 --> 02:02:06,520
I think it's very clear that the machine is going to do a better job making that decision,

2753
02:02:06,520 --> 02:02:10,600
because the humans making that, making that decision are godawful, just terrible.

2754
02:02:11,160 --> 02:02:11,320
Yeah.

2755
02:02:11,320 --> 02:02:11,720
Right.

2756
02:02:11,720 --> 02:02:13,720
And so, so yeah, so this is the, this is the thing.

2757
02:02:13,720 --> 02:02:16,200
And then let's get to the, there was, can I, one more slide of hand?

2758
02:02:16,200 --> 02:02:16,920
Yes, sure.

2759
02:02:16,920 --> 02:02:18,600
Okay, please.

2760
02:02:18,600 --> 02:02:19,960
I'm a magician, you could say.

2761
02:02:19,960 --> 02:02:20,600
One more slide of hand.

2762
02:02:20,600 --> 02:02:24,040
These things are going to be so smart, right, that they're going to be able to destroy the

2763
02:02:24,040 --> 02:02:27,320
world and wreak havoc and like do all this stuff and plan and do all this stuff and evade

2764
02:02:27,320 --> 02:02:30,440
us and have all their secret things and their secret factories and all this stuff.

2765
02:02:30,440 --> 02:02:33,960
But they're so stupid that they're going to get like tangled up in their code.

2766
02:02:33,960 --> 02:02:36,200
And that's the, they're not going to come alive, but there's going to be some bug

2767
02:02:36,200 --> 02:02:39,240
that's going to cause them to like turn us all in a paper, like that they're not going,

2768
02:02:39,240 --> 02:02:41,880
that they're going to be genius in every way other than the actual bad goal.

2769
02:02:43,400 --> 02:02:47,320
And it's just like, and that's just like a like ridiculous like discrepancy.

2770
02:02:47,320 --> 02:02:51,480
And, and, and, and you can prove this today, you can actually address this today for the

2771
02:02:51,480 --> 02:02:57,080
first time with LMS, which is you can actually ask LMS to resolve moral dilemmas.

2772
02:02:57,640 --> 02:02:58,120
Yeah.

2773
02:02:58,120 --> 02:03:01,880
So you can create the scenario, you know, dot, dot, dot, this, that, this, that, this, that,

2774
02:03:01,880 --> 02:03:04,200
what would you as the AI do in the circumstance?

2775
02:03:04,200 --> 02:03:07,080
And they don't just say, destroy all humans, destroy all humans.

2776
02:03:07,080 --> 02:03:11,640
They will give you actually very nuanced moral, practical, tradeoff oriented answers.

2777
02:03:12,200 --> 02:03:16,440
And so we actually already have the kind of AI that can actually like think this through

2778
02:03:16,440 --> 02:03:18,360
and can actually like, you know, reason about goals.

2779
02:03:19,160 --> 02:03:24,520
Well, the hope is that AGI or like a very super intelligent systems have some of the

2780
02:03:24,520 --> 02:03:29,160
nuance that LMS have. And the intuition is they most like the will because even

2781
02:03:29,160 --> 02:03:31,480
these LMS have the nuance.

2782
02:03:32,440 --> 02:03:35,160
LMS are really, this is actually worth, worth spending a moment on.

2783
02:03:35,160 --> 02:03:37,800
LMS are really interesting to have moral conversations with.

2784
02:03:38,360 --> 02:03:42,440
And that, I just, I didn't expect I'd be having a moral conversation with a machine

2785
02:03:42,440 --> 02:03:43,080
in my lifetime.

2786
02:03:43,800 --> 02:03:46,920
And let's remember, we're not really having a conversation with a machine where

2787
02:03:46,920 --> 02:03:50,680
we're having a conversation with the entirety of the collective intelligence of the human species.

2788
02:03:50,680 --> 02:03:51,960
Exactly. Yes.

2789
02:03:51,960 --> 02:03:52,600
Correct.

2790
02:03:52,600 --> 02:03:57,400
But it's possible to imagine autonomous weapon systems that are not using LMS.

2791
02:03:58,200 --> 02:04:02,920
If they're smart enough to be scary, where are they not smart enough to be wise?

2792
02:04:04,920 --> 02:04:08,120
Like that's the part where it's like, I don't know how you get the one without the other.

2793
02:04:08,120 --> 02:04:11,320
Is it possible to be super intelligent without being super wise?

2794
02:04:11,320 --> 02:04:14,600
Well, again, you're back to, I mean, then you're back to a classic autistic computer,

2795
02:04:14,600 --> 02:04:17,160
right? Like you're back to just like a blind rule follower.

2796
02:04:17,800 --> 02:04:20,760
I've got this like core, is the paperclip thing, I've got this core rule and I'm just

2797
02:04:20,760 --> 02:04:21,960
going to follow it to the end of the earth.

2798
02:04:21,960 --> 02:04:24,360
And it's like, well, but everything you're going to be doing to execute that rule is

2799
02:04:24,360 --> 02:04:26,840
going to be super genius level that humans aren't going to be able to counter.

2800
02:04:27,160 --> 02:04:30,920
It's just, it's a mismatch in the definition of what the system is capable of.

2801
02:04:31,560 --> 02:04:33,320
Unlikely, but not impossible, I think.

2802
02:04:33,320 --> 02:04:35,080
But again, here you get to like, okay, like...

2803
02:04:36,120 --> 02:04:39,480
No, I'm not saying when it's unlikely, but not impossible.

2804
02:04:39,480 --> 02:04:43,720
If it's unlikely, that means the fear should be correctly calibrated.

2805
02:04:43,720 --> 02:04:45,640
Extraordinary claims require extraordinary proof.

2806
02:04:45,640 --> 02:04:50,120
Well, okay. So one interesting sort of tangent I would love to take on this because you mentioned

2807
02:04:50,120 --> 02:04:55,960
this in the essay about nuclear, which was also, I mean, you don't shy away from a little bit of

2808
02:04:56,920 --> 02:05:01,880
of a spicy take. So Robert Oppenheimer famously said,

2809
02:05:01,880 --> 02:05:04,360
now I am become death, the destroyer of worlds.

2810
02:05:04,360 --> 02:05:09,800
As he witnessed the first detonation of a nuclear weapon on July 16th, 1945.

2811
02:05:09,800 --> 02:05:13,480
And you write an interesting historical perspective, quote,

2812
02:05:13,480 --> 02:05:18,680
recall that John von Neumann responded to Robert Oppenheimer's famous hand wringing

2813
02:05:18,680 --> 02:05:25,000
about the role of creating nuclear weapons, which, you note, helped end World War II and

2814
02:05:25,000 --> 02:05:31,800
prevent World War III with some people confess guilt to claim credit for the sin.

2815
02:05:31,800 --> 02:05:35,480
And you also mentioned that Truman was harsher after meeting Oppenheimer.

2816
02:05:35,480 --> 02:05:38,440
He said that don't let that crybaby in here again.

2817
02:05:39,640 --> 02:05:43,080
Real quote, real quote, by the way, from Dean Atchison.

2818
02:05:44,840 --> 02:05:48,360
Boy, because Oppenheimer didn't just say the famous line.

2819
02:05:48,360 --> 02:05:48,760
Yeah.

2820
02:05:48,760 --> 02:05:52,200
He then spent years going around basically moaning, you know, going on TV and going into

2821
02:05:52,520 --> 02:05:55,320
the White House and basically like just like doing this hair shirt, you know,

2822
02:05:55,320 --> 02:05:59,240
thing self, you know, this sort of self-critical like, oh my God, I can't believe how awful I am.

2823
02:05:59,240 --> 02:06:06,120
So he's the, he's widely considered perhaps because the hand wringing is the father of the

2824
02:06:06,120 --> 02:06:12,360
atomic bomb. This is Von Neumann's criticism of him as he tried to have his cake and eat it too,

2825
02:06:12,360 --> 02:06:16,920
like he wanted to. And so, Von Neumann, of course, is a very different kind of personality.

2826
02:06:16,920 --> 02:06:20,600
And he's just like, yeah, this is like an incredibly useful thing. I'm glad we did it.

2827
02:06:20,600 --> 02:06:27,160
Yeah. Well, Von Neumann is as widely credited as being one of the smartest humans of the 20th

2828
02:06:27,160 --> 02:06:32,120
century. Certain people, everybody says like, this is the smartest person I've ever met when

2829
02:06:32,120 --> 02:06:36,600
they've met him. Anyway, that doesn't mean smart, doesn't mean wise.

2830
02:06:39,160 --> 02:06:45,160
So I would love to sort of, can you make the case both for and against the critique of Oppenheimer

2831
02:06:45,160 --> 02:06:50,680
here? Because we're talking about nuclear weapons. Boy, do they seem dangerous.

2832
02:06:50,680 --> 02:06:54,280
Well, so the critique goes deeper. And I left this out. Here's the real substance. I left

2833
02:06:54,280 --> 02:06:59,800
it out because I didn't want to dwell on nukes in my AI paper. But here's the deeper thing

2834
02:06:59,800 --> 02:07:02,840
that happened. And I'm really curious, this movie coming out this summer, I'm really curious to see

2835
02:07:02,840 --> 02:07:06,440
how far he pushes this because this is the real drama in the story, which is it wasn't just a

2836
02:07:06,440 --> 02:07:09,480
question of our nukes good or bad. It was a question of, should Russia also have them?

2837
02:07:10,200 --> 02:07:16,440
And what actually happened was Russia got the, America invented the bomb, Russia got the bomb,

2838
02:07:16,440 --> 02:07:21,160
they got the bomb through espionage, they got American and, you know, they got American scientists

2839
02:07:21,160 --> 02:07:26,040
and foreign scientists working on the American project, some combination of the two basically gave

2840
02:07:26,040 --> 02:07:31,080
the Russians the designs for the bomb. And that's how the Russians got the bomb. There's this dispute

2841
02:07:31,080 --> 02:07:37,000
to this day of Oppenheimer's role in that. If you read all the histories, the kind of composite

2842
02:07:37,080 --> 02:07:40,440
picture, and by the way, we now know a lot actually about Soviet espionage in that era,

2843
02:07:40,440 --> 02:07:44,200
because there's been all this declassified material in the last 20 years that actually shows a lot

2844
02:07:44,200 --> 02:07:47,320
of very interesting things. But if you kind of read all the histories, what you kind of get is

2845
02:07:47,320 --> 02:07:51,560
Oppenheimer himself probably was not a, he probably did not hand over the nuclear secrets

2846
02:07:51,560 --> 02:07:56,200
himself. However, he was close to many people who did, including family members. And there were

2847
02:07:56,200 --> 02:08:01,240
other members of the Manhattan Project who were Russian Soviet SS and did hand over the bomb. And

2848
02:08:01,240 --> 02:08:07,000
so the view that Oppenheimer and people like him had that this thing is awful and terrible and,

2849
02:08:07,000 --> 02:08:12,440
oh my God, and, you know, all this stuff, you could argue fed into this ethos at the time that

2850
02:08:12,440 --> 02:08:15,800
resulted in people thinking that the Baptist is thinking that the only principle thing to do is

2851
02:08:15,800 --> 02:08:22,760
to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion

2852
02:08:22,760 --> 02:08:26,440
and the role that the inventors of this technology play, this is the point of this book when they

2853
02:08:26,440 --> 02:08:31,080
kind of take on this sort of public intellectual moral kind of thing, it can have real consequences.

2854
02:08:31,560 --> 02:08:35,160
Because we live in a very different world today because Russia got the bomb than we would have

2855
02:08:35,160 --> 02:08:39,160
lived in had they not gotten the bomb. The entire 20th century, second half of the 20th century

2856
02:08:39,160 --> 02:08:43,080
would have played out very different had those people not given Russia the bomb. And so the

2857
02:08:43,080 --> 02:08:49,080
stakes were very high then. The good news today is nobody sitting here today, I don't think worrying

2858
02:08:49,080 --> 02:08:52,600
about like an analogous situation with respect to like, I'm not really worried that Sam Altman is

2859
02:08:52,600 --> 02:08:57,960
going to decide to give the Chinese the design for AI, although he did just speak at a Chinese

2860
02:08:57,960 --> 02:09:02,600
conference, which is interesting. But however, I don't think that's what's at play here. But what's

2861
02:09:02,600 --> 02:09:06,680
at play here are all these other fundamental issues around what do we believe about this and then

2862
02:09:06,680 --> 02:09:10,440
what laws and regulations and restrictions that we're going to put on it. And that's where I draw

2863
02:09:10,440 --> 02:09:14,760
it like a direct straight line. And anyway, and my reading of the history on Nukes is like the

2864
02:09:14,760 --> 02:09:18,840
people who were doing the full hair shirt public, this is awful, this is terrible, actually had

2865
02:09:18,840 --> 02:09:23,560
like catastrophically bad results from taking those views. And that's what I'm worried is going

2866
02:09:23,560 --> 02:09:27,240
to happen again. But is there a case to be made that you really need to wake the public up to

2867
02:09:27,240 --> 02:09:32,680
the dangers of nuclear weapons when they were first dropped? Like really, like, educate them on

2868
02:09:32,680 --> 02:09:37,000
like, this is extremely dangerous and destructive weapon. I think the education kind of happened

2869
02:09:37,000 --> 02:09:41,960
quick and early. Like, how? It was pretty obvious. How? We dropped one bomb and destroyed an entire

2870
02:09:41,960 --> 02:09:50,760
city. Yeah, so 80,000 people dead. But I don't like the reporting of that, you can report that in

2871
02:09:50,760 --> 02:09:56,760
all kinds of ways. You can do all kinds of slants like war is horrible, war is terrible. You can

2872
02:09:57,720 --> 02:10:02,360
make it seem like nuclear, the use of nuclear weapons is just a part of a war and all that

2873
02:10:02,360 --> 02:10:07,400
kind of stuff. Something about the reporting and the discussion of nuclear weapons resulted in us

2874
02:10:07,400 --> 02:10:16,520
being terrified in awe of the power of nuclear weapons. And that potentially fed in a positive

2875
02:10:16,520 --> 02:10:22,520
way towards the game theory of mutually assured destruction. Well, so this gets to what actually

2876
02:10:23,320 --> 02:10:26,680
happens. Some of us may be playing devils advocate here. Yeah, sure, of course. Let's get to what

2877
02:10:26,680 --> 02:10:29,880
actually happened and then kind of back into that. So what actually happened, I believe, and again,

2878
02:10:29,880 --> 02:10:33,080
I think this is a reasonable reading of history, is what actually happened was nukes then prevented

2879
02:10:33,080 --> 02:10:36,920
World War III. And they prevented World War III through the game theory of mutually assured

2880
02:10:36,920 --> 02:10:42,600
destruction. Had nukes not existed, there would have been no reason why the Cold War did not go

2881
02:10:42,600 --> 02:10:47,960
hot. And the military planners at the time thought, both on both sides, thought that there

2882
02:10:47,960 --> 02:10:50,600
was going to be World War III on the plains of Europe and they thought there was going to be

2883
02:10:50,680 --> 02:10:53,560
like 100 million people dead. It was the most obvious thing in the world to happen.

2884
02:10:54,600 --> 02:10:59,080
And it's the dog that didn't bark. It may be the best single net thing that happened in the

2885
02:10:59,080 --> 02:11:02,520
entire 20th century, is that that didn't happen. Yeah, actually, just on that point,

2886
02:11:03,240 --> 02:11:07,480
you say a lot of really brilliant things. It hit me just as you were saying it.

2887
02:11:09,320 --> 02:11:15,480
I don't know why it hit me for the first time, but we got two wars in a span of 20 years.

2888
02:11:16,360 --> 02:11:21,160
Like, we could have kept getting more and more world wars and more and more ruthless.

2889
02:11:21,960 --> 02:11:25,000
It actually, you could have had a US versus Russia war.

2890
02:11:25,000 --> 02:11:29,720
You could have. By the way, there's another hypothetical scenario. The other hypothetical

2891
02:11:29,720 --> 02:11:34,040
scenario is the Americans got the bomb, the Russians didn't. And then America is the big

2892
02:11:34,040 --> 02:11:37,240
dog. And then maybe America would have had the capability to actually roll back there at current.

2893
02:11:39,000 --> 02:11:41,480
I don't know whether that would have happened, but like it's entirely possible.

2894
02:11:42,040 --> 02:11:46,040
Right. And the act of these people who had these moral positions about, because they could

2895
02:11:46,040 --> 02:11:48,920
forecast, they could model, they could forecast the future of how this technology would get used,

2896
02:11:48,920 --> 02:11:52,600
made a horrific mistake, because they basically ensured that the Iron Curtain would continue

2897
02:11:52,600 --> 02:11:55,640
for 50 years longer than it would have otherwise. And again, like, these are counterfactuals. I

2898
02:11:55,640 --> 02:12:01,880
don't know that that's what would have happened. But like, the decision to hand the bomb over

2899
02:12:01,880 --> 02:12:06,040
was a big decision made by people who were very full of themselves.

2900
02:12:07,000 --> 02:12:12,520
Yeah, but so me as an America, me as a person that loves America, I also wonder if US was the

2901
02:12:12,520 --> 02:12:19,320
only ones with nuclear weapons. That was the argument for handing the, that was the,

2902
02:12:20,040 --> 02:12:23,240
the guys who, the guys who handed over the bomb. That was actually their moral argument.

2903
02:12:23,240 --> 02:12:28,200
Yeah, I would, I would probably not hand it over to, I would be careful about the regimes

2904
02:12:28,200 --> 02:12:31,560
you handed over to. Maybe give it to like the British or something.

2905
02:12:32,360 --> 02:12:37,080
Like, like a democratically elected government.

2906
02:12:37,080 --> 02:12:40,280
Well, there are people to this day who think that those Soviet spies did the right thing,

2907
02:12:40,280 --> 02:12:43,560
because they created a balance of terror, as opposed to the US having just, and by the way,

2908
02:12:43,560 --> 02:12:47,720
let me, let me, balance of terror, let's tell the full verse, such a sexy ring to it. Okay,

2909
02:12:47,720 --> 02:12:50,600
so the full version of the story is John von Neumann as a hero of both yours and mine,

2910
02:12:50,600 --> 02:12:56,360
the full version of the story is he advocated for a first right. So when the US had the bomb,

2911
02:12:56,360 --> 02:13:00,040
and Russia did not, he advocated for, he said, we need to strike them right now.

2912
02:13:01,000 --> 02:13:02,280
Strike Russia. Yeah.

2913
02:13:04,040 --> 02:13:05,480
Yes. Von Neumann.

2914
02:13:05,480 --> 02:13:10,440
Yes, because he said World War Three is inevitable. He was very hardcore.

2915
02:13:11,160 --> 02:13:15,640
He, his, his theory was, his theory was World War Three is inevitable. We're definitely going

2916
02:13:15,640 --> 02:13:18,680
to have a World War Three. The only way to stop World War Three is we have to take them out right

2917
02:13:18,680 --> 02:13:21,480
now. And we have to take them out right now before they get the bomb, because this is our last chance.

2918
02:13:22,760 --> 02:13:23,640
Now, again, like,

2919
02:13:23,640 --> 02:13:25,720
Is this an example of philosophers and politics?

2920
02:13:25,720 --> 02:13:27,800
I don't know if that's in there or not, but this is in the standard.

2921
02:13:27,800 --> 02:13:29,240
No, but it is. Yeah. Meaning is that.

2922
02:13:29,240 --> 02:13:32,760
Yeah, this is on the other side. So, so most of the case studies, most of the case studies in

2923
02:13:32,760 --> 02:13:37,880
books like this are the crazy people on the left. Yeah. Von Neumann is a story arguably of the

2924
02:13:37,880 --> 02:13:41,160
crazy people on the right. Yeah, stick to computing, John.

2925
02:13:41,160 --> 02:13:44,600
Well, this is the thing. And this is, this is the general principle. It goes back to our core

2926
02:13:44,600 --> 02:13:48,200
thing, which is like, I don't know whether any of these people should be making any of these calls.

2927
02:13:48,200 --> 02:13:51,240
Yeah. Because there's nothing in either von Neumann's background or Oppenheimer's

2928
02:13:51,240 --> 02:13:54,920
background or any of these people's background that qualifies them as moral authorities.

2929
02:13:54,920 --> 02:14:00,280
Yeah. Well, this actually brings up the point of in AI, who are the good people to,

2930
02:14:00,280 --> 02:14:05,080
to reason about the morality, the ethics, the outside of these risks outside, like,

2931
02:14:05,080 --> 02:14:10,280
the more complicated stuff that you, you agree on is, you know, this will go into the hands of

2932
02:14:10,280 --> 02:14:14,040
bad guys and all the kinds of ways they'll do is, is interesting and dangerous.

2933
02:14:15,480 --> 02:14:19,640
Is dangerous in interesting, unpredictable ways. And who is the right person?

2934
02:14:19,640 --> 02:14:23,320
Who are the right kinds of people to make decisions how to respond to it?

2935
02:14:23,400 --> 02:14:28,040
Or is it tech people? So the history of these fields, this is what he talks about in the book,

2936
02:14:28,040 --> 02:14:33,240
the history of these fields is that the, the competence and capability and intelligence

2937
02:14:33,240 --> 02:14:37,480
and training and accomplishments of senior scientists and technologists working on a technology,

2938
02:14:38,360 --> 02:14:41,720
and then being able to then make moral judgments in the use of that technology,

2939
02:14:41,720 --> 02:14:46,280
that track record is terrible. That track record, that track record is like catastrophically bad.

2940
02:14:46,840 --> 02:14:51,320
The people, just to look at it, the people that develop that technology are usually not going

2941
02:14:51,320 --> 02:14:55,640
to be the right people. Well, why would they? So the claim is, of course, they're the knowledgeable

2942
02:14:55,640 --> 02:15:00,920
ones, but the problem is they've spent their entire life in a lab, right? They're not theologians.

2943
02:15:01,720 --> 02:15:04,920
But so what you find, what you find when you read, when you read this, when you look at these

2944
02:15:04,920 --> 02:15:09,240
histories, what you find is they generally are very thinly informed on history, on sociology,

2945
02:15:09,240 --> 02:15:17,000
on, on, on theology, on morality, ethics, they tend to manufacture their own worldviews from scratch.

2946
02:15:17,080 --> 02:15:24,600
They tend to be very sort of thin. They're not remotely the arguments that you would be having

2947
02:15:24,600 --> 02:15:28,120
if you got like a group of highly qualified theologians or philosophers or, you know.

2948
02:15:29,000 --> 02:15:36,520
Well, let me sort of, as the devil's advocate takes a sip of whiskey, say that I, I agree with,

2949
02:15:37,960 --> 02:15:42,920
with that, but also it seems like the people who are doing kind of the ethics departments and

2950
02:15:42,920 --> 02:15:51,640
these tech companies go sometimes the other way, which is they're not nuanced on the, on history

2951
02:15:51,640 --> 02:15:57,720
or theology or this kind of stuff. They almost becomes a kind of outraged activism towards

2952
02:15:59,400 --> 02:16:06,040
directions that don't seem to be grounded in history and humility and nuance. It's again,

2953
02:16:06,040 --> 02:16:11,480
drenched with arrogance. So I'm not sure which is worse. Oh, no, they're both bad. Yeah. So

2954
02:16:11,480 --> 02:16:16,600
definitely not them either. But I guess, but look, this is a hard, yeah, it's a hard problem.

2955
02:16:16,600 --> 02:16:20,040
This is our problem. This goes back to where we started, which is okay, who has the truth. And

2956
02:16:20,040 --> 02:16:24,760
it's like, well, you know, like how do societies arrive at like truth and how do we figure these

2957
02:16:24,760 --> 02:16:29,800
things out? And like our elected leaders play some role in it. You know, we all play some role in it.

2958
02:16:30,760 --> 02:16:34,920
There have to be some set of public intellectuals at some point that bring, you know, rationality

2959
02:16:34,920 --> 02:16:38,840
and judgment and humility to it. Yeah, those people are few and far between. We should probably

2960
02:16:38,840 --> 02:16:45,160
prize them very highly. Yeah. Celebrate humility in our public leaders. So getting to risk number

2961
02:16:45,160 --> 02:16:50,840
two, will AI ruin our society? Short version, as you write, if the murder robots don't get us,

2962
02:16:50,840 --> 02:16:56,680
the hate speech and misinformation will. And the action you recommend in short,

2963
02:16:56,680 --> 02:17:05,960
don't let the thought police suppress AI. Well, what is this risk of the effect of

2964
02:17:06,040 --> 02:17:13,560
misinformation in a society that's going to be catalyzed by AI? Yeah, so this is the social media.

2965
02:17:13,560 --> 02:17:16,760
This is what you just alluded to. It's the activism kind of thing that's popped up in these

2966
02:17:16,760 --> 02:17:21,160
companies in the industry. And it's basically, from my perspective, it's basically part two

2967
02:17:21,160 --> 02:17:25,320
of the war that played out over social media over the last 10 years. Because you probably remember

2968
02:17:25,320 --> 02:17:30,280
social media 10 years ago was basically who even wants this, who wants a photo of what your cat

2969
02:17:30,280 --> 02:17:33,960
had for breakfast, like this stuff is like silly and trivial. And why can't these nerds like figure

2970
02:17:33,960 --> 02:17:39,000
out how to invent something useful and powerful? And then certain things happened in the political

2971
02:17:39,000 --> 02:17:42,600
system. And then it's sort of the polarity on that discussion switched all the way to social

2972
02:17:42,600 --> 02:17:46,360
media is like the worst, most corrosive, most terrible, most awful technology ever invented.

2973
02:17:46,360 --> 02:17:51,400
And then it leads to terrible, the wrong politicians and policies and politics and all

2974
02:17:51,400 --> 02:17:56,280
this stuff. And that all got catalyzed into this very big kind of angry movement, both inside

2975
02:17:56,280 --> 02:18:00,440
and outside the companies, to kind of bring social media to heal. And that got focused in

2976
02:18:00,440 --> 02:18:04,600
particularly on two topics, so-called hate speech and so-called misinformation. And that's

2977
02:18:04,600 --> 02:18:08,520
been the saga playing out for the last decade. And I don't even really want to even argue the pros

2978
02:18:08,520 --> 02:18:12,680
and cons of the sides just to observe that that's been like a huge fight and has had big consequences

2979
02:18:12,680 --> 02:18:20,040
to how these companies operate. Basically, those same sets of theories, that same activist approach,

2980
02:18:20,040 --> 02:18:24,280
that same energy is being transplanted straight to AI. And you see that already happening. It's

2981
02:18:24,280 --> 02:18:28,760
why ChatGPT will answer, let's say, certain questions and not others. It's why it gives you

2982
02:18:28,760 --> 02:18:32,600
the canned speech about, you know, whenever it starts with as a large language model, I cannot,

2983
02:18:32,600 --> 02:18:35,320
you know, basically means that somebody has reached in there and told that it can't talk

2984
02:18:35,320 --> 02:18:40,520
about certain topics. Do you think some of that is good? So it's an interesting question.

2985
02:18:41,080 --> 02:18:46,440
So a couple of observations. So one is the people who find this the most frustrating are the people

2986
02:18:46,440 --> 02:18:53,960
who are worried about the murder robots. So and in fact, the so-called ex-risk people,

2987
02:18:53,960 --> 02:18:57,960
right, they started with the term AI safety, the term became AI alignment. When the term became

2988
02:18:57,960 --> 02:19:01,000
AI alignment is when this switch happened from where we're worried it's going to kill us all to

2989
02:19:01,000 --> 02:19:05,880
we're worried about hate speech and misinformation. The AI ex-risk people have now renamed their thing

2990
02:19:05,880 --> 02:19:12,040
AI not kill everyone ism, which I have to admit is a catchy term. And they are very frustrated by

2991
02:19:12,040 --> 02:19:15,880
the fact that the sort of activist driven hate speech misinformation kind of thing is taking

2992
02:19:15,880 --> 02:19:19,080
over, which is what's happened is taking over the AI ethics field has been taken over by the

2993
02:19:19,080 --> 02:19:24,360
hate speech misinformation people. You know, look, would I like to live in a world in which like

2994
02:19:24,360 --> 02:19:27,800
everybody was nice to each other all the time and nobody ever said anything mean and nobody ever

2995
02:19:27,800 --> 02:19:31,640
used a bad word and everything was always accurate and honest. Like that sounds great.

2996
02:19:31,640 --> 02:19:34,520
Do I want to live in a world where there's like a centralized thought police working

2997
02:19:34,520 --> 02:19:38,680
through the tech companies to enforce the view of a small set of elites that they're going to

2998
02:19:38,680 --> 02:19:43,080
determine what the rest of us think and feel like absolutely not. There could be a middle ground

2999
02:19:43,080 --> 02:19:49,000
somewhere like Wikipedia type of moderation. There's moderation of Wikipedia that is somehow

3000
02:19:49,000 --> 02:19:55,800
crowdsourced where you don't have centralized elites. But it's also not completely just a

3001
02:19:55,800 --> 02:20:01,640
free for all because the if you have the entirety of human knowledge at your fingertips,

3002
02:20:02,280 --> 02:20:08,360
you can do a lot of harm. Like if you have a good assistant that's completely uncensored,

3003
02:20:08,360 --> 02:20:17,080
they can help you build a bomb. They can help you mess with people's physical well being, right?

3004
02:20:17,080 --> 02:20:21,880
If they because that information is out there on the internet. And so they presumably there's

3005
02:20:22,600 --> 02:20:29,960
it would be, you could see the positives in censoring some aspects of an AI model

3006
02:20:29,960 --> 02:20:34,920
when it's helping you commit literal violence. And there's a section later section of the essay

3007
02:20:34,920 --> 02:20:39,480
where I talk about bad people doing bad things, which and there's a set of things that we

3008
02:20:39,480 --> 02:20:44,120
should discuss there. What happens in practice is these lines, as you alluded to this already,

3009
02:20:44,120 --> 02:20:47,800
these lines are not easy to draw. And what I've observed in the social media version of this

3010
02:20:47,800 --> 02:20:51,960
is like the way I describe it as the slippery slope is not a fallacy. It's an inevitability.

3011
02:20:51,960 --> 02:20:55,080
The minute you have this kind of activist personality that gets in a position to make

3012
02:20:55,080 --> 02:21:00,440
these decisions, they take it straight to infinity. Like it goes into the crazy zone

3013
02:21:00,440 --> 02:21:04,360
like almost immediately and never comes back because people become drunk with power, right?

3014
02:21:04,360 --> 02:21:07,240
And they look if you're in the position to determine what the entire world thinks and

3015
02:21:07,240 --> 02:21:12,200
feels and reads and says, like you're going to take it. And you know, Elon has ventilated this

3016
02:21:12,200 --> 02:21:15,080
with the Twitter files over the last, you know, three months. And it's just like crystal clear

3017
02:21:15,080 --> 02:21:19,880
like how bad it got there. Now, reason for optimism is what Elon is doing with the community notes.

3018
02:21:21,640 --> 02:21:25,800
So community notes is actually a very interesting thing. So what Elon is trying to do with

3019
02:21:25,800 --> 02:21:29,880
community notes is he's trying to have it where there's only a community note when people who

3020
02:21:29,880 --> 02:21:36,360
have previously disagreed on many topics agree on this one. Yes. That's what I'm trying to get at

3021
02:21:36,360 --> 02:21:40,760
is like there's, there could be Wikipedia like models or community notes type of models where

3022
02:21:41,320 --> 02:21:48,360
allows you to essentially either provide context or sensor in a way that's not resist the slippery

3023
02:21:48,360 --> 02:21:51,640
slope nature. Now, there's another power. There's an entirely different approach here,

3024
02:21:51,640 --> 02:21:56,440
which is basically we have AIs that are producing content. We could also have AIs that are consuming

3025
02:21:56,440 --> 02:22:01,000
content, right? And so one of the things that your assistant could do for you is help you consume

3026
02:22:01,000 --> 02:22:05,320
all the content, right? And basically tell you when you're getting played. So for example,

3027
02:22:05,320 --> 02:22:09,000
I'm going to want the AI that my kid uses, right, to be very, you know, child safe,

3028
02:22:09,000 --> 02:22:12,120
and I'm going to want it to filter for him all kinds of inappropriate stuff that he shouldn't

3029
02:22:12,120 --> 02:22:15,560
be saying just because he's a kid. Yeah. Right. And you see what I'm saying is you can implement

3030
02:22:15,560 --> 02:22:19,000
that. You could do the architecture. You could say you can solve this on the client side, right?

3031
02:22:19,000 --> 02:22:22,040
Solving on the server side gives you an opportunity to dictate for the entire world,

3032
02:22:22,040 --> 02:22:26,440
which I think is where you take the slippery slope to hell. There's another architectural

3033
02:22:26,440 --> 02:22:29,560
approach, which is to solve this on the client side, which is certainly what I would endorse.

3034
02:22:30,440 --> 02:22:35,880
It's at risk number five will AI lead to bad people doing bad things. I can just imagine

3035
02:22:35,960 --> 02:22:40,200
language models used to do so many bad things, but the hope is there that you can have

3036
02:22:41,640 --> 02:22:46,120
large language models used to then defend against it by more people, by smarter people, by

3037
02:22:47,480 --> 02:22:50,760
more effective people, skilled people, all that kind of stuff.

3038
02:22:50,760 --> 02:22:55,400
Three-point argument on bad people doing bad things. So number one, right, you can use the

3039
02:22:55,400 --> 02:22:59,640
technology defensively. And there's a, we should be using AI to build like broad spectrum vaccines

3040
02:22:59,640 --> 02:23:03,320
and antibiotics for like bio weapons. And we should be using AI to like hunt terrorists and

3041
02:23:03,400 --> 02:23:06,040
catch criminals. And like we should be doing like all kinds of stuff like that.

3042
02:23:06,040 --> 02:23:09,080
And in fact, we should be doing those things even just to like go get like, you know,

3043
02:23:09,080 --> 02:23:12,440
basically go eliminate risk from like regular pathogens that aren't like constructed by an AI.

3044
02:23:12,440 --> 02:23:15,960
So there's, there's, there's the whole, there's a whole defensive set of things.

3045
02:23:16,680 --> 02:23:21,160
Second is we have many laws on the books about the actual bad things, right? So it is actually

3046
02:23:21,160 --> 02:23:25,560
illegal to be a criminal, you know, to commit crimes, to commit terrorist acts, to, you know,

3047
02:23:25,560 --> 02:23:30,200
build pathogens with the intent to deploy them to kill people. And so we have those,

3048
02:23:30,200 --> 02:23:33,480
we don't, we actually don't need new laws for the vast majority of these scenarios. We actually

3049
02:23:33,480 --> 02:23:38,360
already have the laws in the book on the books. The third argument is the minute, and this is

3050
02:23:38,360 --> 02:23:41,560
sort of the foundational one that gets really tough, but the minute you get into this thing,

3051
02:23:41,560 --> 02:23:44,360
which, which you were kind of getting into, which is like, okay, but like, don't you need

3052
02:23:44,360 --> 02:23:47,880
censorship sometimes, right? And don't you need restriction sometimes? It's like, okay, what is

3053
02:23:47,880 --> 02:23:55,080
the cost of that? And in particular in the world of open source, right? And so is open source AI

3054
02:23:55,080 --> 02:24:01,960
going to be allowed or not? If open source AI is not allowed, then what is the regime that's going

3055
02:24:01,960 --> 02:24:07,080
to be necessary legally and technically to prevent it from developing, right? And here, again, is

3056
02:24:07,080 --> 02:24:10,680
where you get into, and people have proposed that these kinds of things, you get into, I would say,

3057
02:24:10,680 --> 02:24:16,040
pretty extreme territory pretty fast. Do we have a monitor agent on every CPU and GPU that reports

3058
02:24:16,040 --> 02:24:20,680
back to the government, what we're doing with our computers? Are we seizing GPU clusters to get

3059
02:24:20,680 --> 02:24:25,160
beyond a certain size? Like, and then by the way, how are we doing all that globally? Right?

3060
02:24:25,160 --> 02:24:29,560
And like, if China is developing an LLM beyond the scale that we think is allowable, are we going

3061
02:24:29,560 --> 02:24:34,600
to invade? Right? And you have figures on the AI X risk side who are advocating, you know,

3062
02:24:34,600 --> 02:24:38,520
potentially up to nuclear strikes to prevent, you know, this kind of thing. And so here you get into

3063
02:24:38,520 --> 02:24:42,840
this thing. And again, you could maybe say this is, you know, you could even say this is what good,

3064
02:24:42,840 --> 02:24:47,560
bad or indifferent or whatever. But like, here's the comparison of nukes. The comparison of nukes

3065
02:24:47,560 --> 02:24:51,560
is very dangerous because one is just nukes were just, just above, although we can come back to

3066
02:24:51,560 --> 02:24:55,080
nuclear power. But the other thing was like with nukes, you could control plutonium, right? You

3067
02:24:55,080 --> 02:25:00,280
could track plutonium. And it was like hard to come by. AI is just math and code, right? And it's

3068
02:25:00,280 --> 02:25:03,560
in like math textbooks. And it's like their YouTube videos that teach you how to build it. And like,

3069
02:25:03,560 --> 02:25:06,920
there's open source already opens horse, you know, there's a 40 billion parameter model running

3070
02:25:06,920 --> 02:25:13,160
around already called Falcon online that anybody can download. And so, okay, you walk down the logic

3071
02:25:13,160 --> 02:25:16,120
path that says we need to have guardrails on this. And you find yourself in a

3072
02:25:16,920 --> 02:25:23,000
authoritarian totalitarian regime of thought control and machine control that would be so brutal

3073
02:25:23,880 --> 02:25:27,640
that you would have destroyed the society that you're trying to protect. And so I just don't

3074
02:25:27,640 --> 02:25:33,880
see how that actually works. So you have to understand my brain is going full, full steam ahead

3075
02:25:33,880 --> 02:25:37,720
here because I agree with basically everything you're saying when I'm trying to play devil's

3076
02:25:37,720 --> 02:25:43,960
advocate here. Because okay, you highlighted the fact that there is a slippery slope to human

3077
02:25:43,960 --> 02:25:47,720
nature. The moment you sense there's something, you start to sense everything.

3078
02:25:50,680 --> 02:25:58,520
That alignment starts out sounding nice, but then you start to align to the beliefs of some

3079
02:25:59,480 --> 02:26:04,600
select group of people. And then it's just your beliefs. The number of people you're

3080
02:26:04,600 --> 02:26:10,280
aligned to smaller smaller as that group becomes more and more powerful. Okay, but that just speaks

3081
02:26:10,280 --> 02:26:16,200
to the people that censor usually the assholes and the assholes get richer. I wonder if it's

3082
02:26:16,200 --> 02:26:23,240
possible to do without that for AI. The one way to ask this question is, do you think the base

3083
02:26:23,240 --> 02:26:30,360
models, the base, the baseline foundation models should be open sourced? Like what Mark Zuckerberg

3084
02:26:30,360 --> 02:26:35,880
is saying they want to do? So I look, I mean, I think it's totally appropriate that companies that

3085
02:26:35,960 --> 02:26:40,680
are in the business of producing a product or service should be able to have a wide range of

3086
02:26:40,680 --> 02:26:45,320
policies that they put, right? And I'll just again, I want a heavily censored model for my eight-year-old.

3087
02:26:46,120 --> 02:26:49,560
Like I actually want that. Like I would pay more money for the ones more heavily censored than the

3088
02:26:49,560 --> 02:26:53,880
one that's not, right? And so like there are certainly scenarios where companies will make that

3089
02:26:53,880 --> 02:26:59,480
decision. Look, an interesting thing you brought up, or is this really a speech issue? One of the

3090
02:26:59,480 --> 02:27:04,360
things that the big tech companies are dealing with is that content generated from an LLM is not

3091
02:27:04,360 --> 02:27:10,040
covered under section 230, which is the law that protects internet platform companies from being

3092
02:27:10,040 --> 02:27:17,880
sued for the user-generated content. And so it's actually, yes. And so there's actually a question,

3093
02:27:17,880 --> 02:27:22,280
I think there's still a question which is can big American companies actually feel generative AI at

3094
02:27:22,280 --> 02:27:26,680
all? Or is the liability actually going to just ultimately convince them that they can't do it?

3095
02:27:26,680 --> 02:27:30,440
Because the minute the thing says something bad, and it doesn't even need to be hate speech, it

3096
02:27:30,440 --> 02:27:36,120
could just be like an enact, it could hallucinate a product detail on a vacuum cleaner, and all

3097
02:27:36,120 --> 02:27:40,360
of a sudden the vacuum cleaner company sues for misrepresentation. And there's any symmetry there,

3098
02:27:40,360 --> 02:27:44,280
right? Because the LLM is going to be producing billions of answers to questions, and it only

3099
02:27:44,280 --> 02:27:47,880
needs to get a few wrong. The loss has to get updated really quick here. Yeah, and nobody knows

3100
02:27:47,880 --> 02:27:53,960
what to do with that, right? So anyway, there are big questions around how companies operate at all.

3101
02:27:53,960 --> 02:27:57,640
So we talk about those. But then there's this other question of like, okay, the open source,

3102
02:27:57,640 --> 02:28:01,560
so what about open source? And my answer to your question is kind of like, obviously, yes,

3103
02:28:01,560 --> 02:28:06,120
the models have to, there has to be full open source here, because to live in a world in which

3104
02:28:06,120 --> 02:28:12,040
that open source is not allowed is a world of draconian speech control, human control, machine

3105
02:28:12,040 --> 02:28:17,080
control. I mean, you know, black helicopters with jackbooted thugs coming out, repelling down and

3106
02:28:17,080 --> 02:28:23,240
seizing your GPU, like territory. Well, no, no, I'm 100% serious. That's you're saying slippery

3107
02:28:23,240 --> 02:28:26,520
slope always leads that. No, no, no, no, no, no, that's what's required to enforce it. Like, how

3108
02:28:26,600 --> 02:28:32,120
will you enforce a ban on open source? You could add friction to it, like hard to get the models,

3109
02:28:32,120 --> 02:28:36,040
because people will always be able to get the models. But it'll be more in the shadows, right?

3110
02:28:36,040 --> 02:28:41,320
The leading open source model right now is from the UAE. Like, the next time they do that, what do

3111
02:28:41,320 --> 02:28:48,920
we do? Yeah, like, oh, I see, you're like, the 14 year old in Indonesia comes out with a breakthrough

3112
02:28:48,920 --> 02:28:51,560
model. You know, we talked about most great software comes from a small number of people.

3113
02:28:51,560 --> 02:28:55,000
Some kid comes out with some big new breakthrough and quantization or something and has some huge

3114
02:28:55,000 --> 02:28:59,720
breakthrough and like, what are we going to like, invade Indonesia and arrest him?

3115
02:28:59,720 --> 02:29:03,800
It seems like in terms of size of models and effectiveness of models, the big tech companies

3116
02:29:03,800 --> 02:29:09,160
will probably lead the way for quite a few years. And the question is of what policies they should

3117
02:29:09,160 --> 02:29:19,960
use. The kid in Indonesia should not be regulated, but should Google Meta, Microsoft, OpenAI be

3118
02:29:20,680 --> 02:29:27,480
regulated? Okay, so when does it become dangerous? Is the danger that it's, quote,

3119
02:29:27,480 --> 02:29:32,600
as powerful as the current leading commercial model? Or is it that it is just at some other

3120
02:29:32,600 --> 02:29:37,320
arbitrary threshold? And then by the way, like, look, how do we know? Like, what we know today

3121
02:29:37,320 --> 02:29:40,680
is that you need like a lot of money to train these things. But there are advances being made

3122
02:29:40,680 --> 02:29:44,280
every week on training efficiency and you know, data, all kinds of synthetic data. Look, I don't

3123
02:29:44,280 --> 02:29:47,400
even like the synthetic data thing we're talking about, maybe some kid figures out a way to auto

3124
02:29:47,400 --> 02:29:50,760
generate synthetic data. That's going to change everything. Yeah, exactly. And so like sitting

3125
02:29:50,760 --> 02:29:54,840
here today, like the breakthrough just happened, right? You made this point, like the breakthrough

3126
02:29:54,840 --> 02:29:59,960
just happened. So we don't know what the shape of this technology is going to be. I mean, the big

3127
02:29:59,960 --> 02:30:05,480
shock, the big shock here is that, you know, whatever number of billions of parameters basically

3128
02:30:05,480 --> 02:30:10,120
represents at least a very big percentage of human thought, like, who would have imagined that?

3129
02:30:10,920 --> 02:30:14,040
And then there's already work underway, there was just this paper that just came out that

3130
02:30:14,040 --> 02:30:18,600
basically takes a GPT-3 scale model and compresses it down to run on a single 32-core CPU.

3131
02:30:19,400 --> 02:30:24,200
Like, who would have predicted that? Yeah. You know, some of these models now you can run

3132
02:30:24,200 --> 02:30:28,040
in Raspberry Pi's, like today they're very slow, but like, you know, maybe they'll be a, you know,

3133
02:30:28,040 --> 02:30:32,760
perceived real perform, you know, like, it's math and code. And here we're back in here,

3134
02:30:32,760 --> 02:30:37,480
we're back in math and code. It's math and code. It's math, code and data. It's bits.

3135
02:30:37,480 --> 02:30:43,240
Mark's just like walking away at this point. He just screw it. I don't know what to do with this.

3136
02:30:43,240 --> 02:30:49,800
You guys created this whole internet thing. Yeah. Yeah. I'm a huge believer in open source here.

3137
02:30:49,800 --> 02:30:53,160
So my argument is, we're going to have to see, here's my argument. My argument, my full argument

3138
02:30:53,160 --> 02:30:56,360
is AI is going to be like air, it's going to be everywhere. Like, this is just going to be in

3139
02:30:56,360 --> 02:30:58,920
text, it already is. It's going to be in textbooks and kids are going to grow up knowing how to do

3140
02:30:58,920 --> 02:31:01,720
this. And it's just going to be a thing. It's going to be in the air and you can't like pull

3141
02:31:01,720 --> 02:31:05,320
this back anywhere. You can pull back air. And so you just have to figure out how to live in this

3142
02:31:05,320 --> 02:31:09,640
world, right? And then that's where I think like all this hand-wringing about AI risk is basically

3143
02:31:09,640 --> 02:31:14,680
complete waste of time because the effort should go into, okay, what is the defensive approach?

3144
02:31:15,320 --> 02:31:18,760
And so if you're worried about AI generated pathogens, the right thing to do is to have a

3145
02:31:18,760 --> 02:31:25,000
permanent project warp speed, right? And funded lavishly. Let's do a Manhattan project for biological

3146
02:31:25,000 --> 02:31:29,640
defense, right? And let's build AIs and let's have like broad spectrum vaccines where like we're

3147
02:31:29,640 --> 02:31:35,400
insulated from every pathogen, right? And what the interesting thing is because it's software,

3148
02:31:36,360 --> 02:31:41,880
a kid in his basement, a teenager could build like a system that defends against like the worst

3149
02:31:41,880 --> 02:31:50,120
that the worst. I mean, and to me, defense is super exciting. It's like, if you believe in

3150
02:31:50,120 --> 02:31:55,400
the good of human nature, the most people want to do good to be the savior of humanity is really

3151
02:31:55,400 --> 02:32:03,000
exciting. Yes. Okay, that's a dramatic statement, but like to help people. To help people. Yeah,

3152
02:32:03,080 --> 02:32:08,920
okay, what about just to jump around? What about the risk of will AI lead to crippling inequality?

3153
02:32:09,880 --> 02:32:12,520
You know, because we're kind of saying everybody's life will become better.

3154
02:32:13,640 --> 02:32:18,120
Is it possible that the rich get richer here? Yeah. So this is actually ironically goes back to

3155
02:32:18,120 --> 02:32:21,880
Marxism. So because this was the course of the core claim of Marxism, right? Basically, it was

3156
02:32:21,880 --> 02:32:24,920
that the owner, the owners of capital would basically own the means of production. And then

3157
02:32:24,920 --> 02:32:28,920
over time, they would basically accumulate all the wealth the workers would be paying in, you know,

3158
02:32:28,920 --> 02:32:32,360
and getting nothing in return, because they wouldn't be needed anymore, right? Marx is very

3159
02:32:32,440 --> 02:32:35,480
worried about what he called mechanization or what later became known as automation.

3160
02:32:36,680 --> 02:32:40,680
And that, you know, the workers would be emissarated and the capitalist would end up with all. And so

3161
02:32:40,680 --> 02:32:44,440
this was one of the core principles of Marxism. Of course, it turned out to be wrong about every

3162
02:32:44,440 --> 02:32:48,760
previous wave of technology. The reason it turned out to be wrong about every previous wave of

3163
02:32:48,760 --> 02:32:53,640
technology is that the way that the self-interested owner of the machines makes the most money is

3164
02:32:53,640 --> 02:32:59,160
by providing the production capability in the form of products and services to the most people,

3165
02:32:59,160 --> 02:33:03,080
the most customers as possible, right? The largest... And this is one of those funny things

3166
02:33:03,080 --> 02:33:06,040
where every CEO knows this intuitively, and yet it's like hard to explain from the outside.

3167
02:33:06,600 --> 02:33:10,440
The way you make the most money in any business is by selling to the largest market you can possibly

3168
02:33:10,440 --> 02:33:15,400
get to. The largest market you can possibly get to is everybody on the planet. And so every large

3169
02:33:15,400 --> 02:33:19,480
company does is everything that it can to drive down prices to be able to get volumes up to be

3170
02:33:19,480 --> 02:33:23,240
able to get to everybody on the planet. And that happened with everything from electricity. It

3171
02:33:23,240 --> 02:33:26,760
happened with telephones. It happened with radio. It happened with automobiles. It happened with

3172
02:33:26,840 --> 02:33:34,120
smartphones. It happened with PCs. It happened with the internet. It happened with mobile broadband.

3173
02:33:34,120 --> 02:33:38,600
It's happened, by the way, with Coca-Cola. It's happened with like every... Basically,

3174
02:33:38,600 --> 02:33:42,600
every industrially produced, good or service, people want to drive it to the largest possible

3175
02:33:42,600 --> 02:33:48,280
market. And then as proof of that, it's already happened, right? Which is the early adopters of

3176
02:33:48,280 --> 02:33:54,520
like GPD and Bing are not like Exxon and Boeing. They're your uncle and your nephew,

3177
02:33:55,080 --> 02:33:59,080
right? It's either freely available online or it's available for $20 a month or something,

3178
02:33:59,080 --> 02:34:05,320
but these things went... This technology went mass market immediately. And so look, the owners of

3179
02:34:05,320 --> 02:34:08,520
the means of production, whoever does this, now as I mentioned these trillion dollar questions,

3180
02:34:08,520 --> 02:34:11,800
there are people who are going to get really rich doing this, producing these things, but

3181
02:34:11,800 --> 02:34:14,600
they're going to get really rich by taking this technology to the broadest possible market.

3182
02:34:15,400 --> 02:34:20,120
So yes, they'll get rich, but they'll get rich having a huge positive impact on...

3183
02:34:20,120 --> 02:34:22,440
Yeah, making the technology available to everybody.

3184
02:34:23,080 --> 02:34:28,120
And again, smartphone, same thing. So there's this amazing kind of twist in business history,

3185
02:34:28,120 --> 02:34:33,080
which is you cannot spend $10,000 on a smartphone, right? You can't spend $100,000. You can't

3186
02:34:33,080 --> 02:34:35,880
spend... Like I would buy the million dollar smartphone, like I'm signed up for it. Like if

3187
02:34:35,880 --> 02:34:39,400
it's like, suppose a million dollar smartphone was like much better than the $1,000 smartphone,

3188
02:34:39,400 --> 02:34:43,640
like I'm there to buy it, it doesn't exist. Why doesn't it exist? Apple makes so much more

3189
02:34:43,640 --> 02:34:47,160
money driving the price further down from $1,000 than they would trying to harvest,

3190
02:34:47,160 --> 02:34:49,960
right? And so it's just this repeating pattern you see over and over again,

3191
02:34:50,440 --> 02:34:54,680
where the... And what's great about it, what's great about it is you do not need to rely on

3192
02:34:54,680 --> 02:34:59,320
anybody's enlightened right generosity to do this. You just need to rely on capitalist self-interest.

3193
02:35:01,320 --> 02:35:07,400
What about AI taking our jobs? Yeah, so very, very similar thing here. There's a core fallacy,

3194
02:35:07,400 --> 02:35:11,400
which again was very common in Marxism, which is what's called the lump of labor fallacy.

3195
02:35:11,400 --> 02:35:15,240
And this is sort of the fallacy that there is only a fixed amount of work to be done in the world.

3196
02:35:15,240 --> 02:35:18,680
And if the... And it's all being done today by people. And then if machines do it,

3197
02:35:18,680 --> 02:35:23,800
there's no other work to be done by people. And that's just a completely backwards view on how

3198
02:35:23,800 --> 02:35:28,360
the economy develops and grows, because what happens is not. In fact, that what happens is

3199
02:35:28,920 --> 02:35:32,440
the introduction of technology into production process causes prices to fall.

3200
02:35:33,080 --> 02:35:37,240
As prices fall, consumers have more spending power. As consumers have more spending power,

3201
02:35:37,240 --> 02:35:43,080
they create new demand. That new demand then causes capital and labor to form into new enterprises

3202
02:35:43,080 --> 02:35:47,240
to satisfy new wants and needs. And the result is more jobs at higher wages.

3203
02:35:47,320 --> 02:35:53,320
New wants and needs. The worry is that the creation of new wants and needs at a rapid rate

3204
02:35:54,040 --> 02:35:59,160
will mean there's a lot of turnover in jobs. So people will lose jobs. Just the actual

3205
02:35:59,160 --> 02:36:03,720
experience of losing a job and having to learn new things and new skills is painful for the

3206
02:36:03,720 --> 02:36:08,680
individuals. Two things. One is the new jobs are often much better. So this actually came up.

3207
02:36:08,680 --> 02:36:11,560
There was this panic about a decade ago and all the truck drivers are going to lose their jobs,

3208
02:36:11,560 --> 02:36:15,560
right? And number one, that didn't happen because we haven't figured out a way to actually finish

3209
02:36:15,560 --> 02:36:19,640
that yet. But the other thing was like a truck driver, like I grew up in a town that was basically

3210
02:36:19,640 --> 02:36:24,280
consisted of a truck stop, right? And I knew a lot of truck drivers. And truck drivers live a decade

3211
02:36:24,280 --> 02:36:30,120
shorter than everybody else. It's actually like a very dangerous, like literally they have like

3212
02:36:30,120 --> 02:36:35,000
high-racist skin cancer. And on the left side of their body from being in the sun all the time,

3213
02:36:35,000 --> 02:36:38,920
the vibration of being in the truck is actually very damaging to your physiology.

3214
02:36:38,920 --> 02:36:44,680
And there's actually, perhaps partially because of that reason, there's a shortage of

3215
02:36:46,040 --> 02:36:50,680
people who want to be truck drivers. The question always you want to ask somebody like

3216
02:36:50,680 --> 02:36:55,080
that is, do you want your kid to be doing this job? And like most of them will tell you, no,

3217
02:36:55,080 --> 02:36:58,200
like I want my kid to be sitting in a cubicle somewhere like where they don't have this,

3218
02:36:58,200 --> 02:37:02,280
like where they don't die 10 years earlier. And so the new jobs, number one, the new jobs are

3219
02:37:02,280 --> 02:37:06,520
often better, but you don't get the new jobs until you go through the change. And then to your point,

3220
02:37:06,520 --> 02:37:10,360
the training thing, you know, it's always the issue is can people adapt? And again, here you

3221
02:37:10,360 --> 02:37:14,120
need to imagine living in a world in which everybody has the AI assistant capability,

3222
02:37:14,760 --> 02:37:17,480
right, to be able to pick up new skills much more quickly and be able to have some, you know,

3223
02:37:17,480 --> 02:37:19,400
be able to have a machine to work with to augment their skills.

3224
02:37:19,400 --> 02:37:22,440
It's still going to be painful, but that's the process of life.

3225
02:37:22,440 --> 02:37:24,600
It's painful for some people. I mean, there's no, like, there's no question

3226
02:37:24,600 --> 02:37:27,880
it's painful for some people. And they're, you know, they're, yes, it's not, again,

3227
02:37:27,880 --> 02:37:31,080
I'm not a utopian on this. And it's not like it's positive for everybody in the moment,

3228
02:37:31,080 --> 02:37:35,720
but it has been overwhelmingly positive for 300 years. I mean, look, the concern here,

3229
02:37:35,720 --> 02:37:40,040
the concern, the concern, this concern has played out for literally centuries. And, you know,

3230
02:37:40,040 --> 02:37:44,040
this is the sort of leadite, you know, the story of the leadites that you may remember,

3231
02:37:44,040 --> 02:37:49,000
there was a panic in the 2000s around outsourcing was going to take all the jobs. There was a panic

3232
02:37:49,000 --> 02:37:56,760
in the 2010s that robots are going to take all the jobs. In 2019, before COVID, we had more jobs

3233
02:37:56,760 --> 02:37:59,960
at higher wages, both in the country and in the world than at any point in human history.

3234
02:38:00,520 --> 02:38:06,520
And so the overwhelming evidence is that the net gain here is like just like wildly positive.

3235
02:38:06,520 --> 02:38:10,440
And most people like overwhelmingly come out the other side being huge beneficiaries of this.

3236
02:38:11,080 --> 02:38:16,280
So you write that the single greatest risk, this is the risk you're most convinced by,

3237
02:38:16,280 --> 02:38:21,240
the single greatest risk of AI is that China wins global AI dominance and we,

3238
02:38:21,240 --> 02:38:24,520
the United States and the West do not. Can you elaborate?

3239
02:38:24,520 --> 02:38:29,480
Yeah. So this is the other thing, which is a lot of the sort of AI risk debates today,

3240
02:38:29,480 --> 02:38:32,760
sort of assume that we're the only game in town, right? And so we have the ability to kind of

3241
02:38:32,760 --> 02:38:36,200
sit in the United States and criticize ourselves and, you know, have our government like,

3242
02:38:36,280 --> 02:38:39,320
you know, beat up on our companies and we're figuring out a way to restrict what our companies can do.

3243
02:38:39,320 --> 02:38:41,720
And, you know, we're going to, you know, we're going to ban this and ban that,

3244
02:38:41,720 --> 02:38:45,000
restrict this and do that. And then there's this like other like force out there that

3245
02:38:45,000 --> 02:38:49,560
like doesn't believe we have any power over them whatsoever. And they have no desire to sign up

3246
02:38:49,560 --> 02:38:53,160
for whatever rules we decided to put in place. And they're going to do whatever it is they're

3247
02:38:53,160 --> 02:38:58,200
going to do. And we have no control over it at all. And it's China and specifically the Chinese

3248
02:38:58,200 --> 02:39:05,160
Communist Party. And they have a completely publicized, open, you know, plan for what they're

3249
02:39:05,160 --> 02:39:10,440
going to do with AI. And it is not what we have in mind. And not only do they have that as a vision

3250
02:39:10,440 --> 02:39:14,040
and a plan for their society, but they also have it as a vision and plan for the rest of the world.

3251
02:39:14,040 --> 02:39:15,960
So their plan is what? Surveillance?

3252
02:39:15,960 --> 02:39:21,560
Yeah, authoritarian control. So authoritarian population control, you know, good old fashioned

3253
02:39:21,560 --> 02:39:27,480
communist authoritarian control, and surveillance and enforcement, and social credit scores and

3254
02:39:27,480 --> 02:39:32,280
all the rest of it. And you are going to be monitored and metered within an inch of everything

3255
02:39:32,360 --> 02:39:37,320
all the time. And it's going to be basically the end of human freedom. And that's their goal. And,

3256
02:39:37,320 --> 02:39:40,040
you know, they justify it on the basis of that's what leads to peace.

3257
02:39:40,040 --> 02:39:47,080
And you're worried that the regulating in the United States will hold progress enough to where

3258
02:39:48,280 --> 02:39:50,280
the Chinese government would win that race?

3259
02:39:50,280 --> 02:39:54,280
So their plan, yes, yes. And the reason for that is they, and again, they're very public on this,

3260
02:39:54,280 --> 02:39:57,880
they have their plan is to proliferate their approach around the world. And they have this

3261
02:39:57,880 --> 02:40:01,560
program called the digital Silk Road, right, which is building on their Silk Road investment

3262
02:40:01,560 --> 02:40:04,680
program. And they've got their, they've been laying, they've been laying networking infrastructure

3263
02:40:04,680 --> 02:40:09,240
all over the world with their 5G right work with their company, Huawei. And so they've been laying

3264
02:40:09,240 --> 02:40:12,920
all this fabric, but financial and technological fabric all over the world. And their plan is to

3265
02:40:12,920 --> 02:40:17,240
roll out their vision of AI on top of that, and to have every other country be running their version.

3266
02:40:17,880 --> 02:40:22,280
And then if you're a country prone to, you know, authoritarianism, you're going to find this to

3267
02:40:22,280 --> 02:40:26,600
be an incredible way to become more authoritarian. If you're a country, by the way, not prone to

3268
02:40:26,600 --> 02:40:29,960
authoritarianism, you're going to have the Chinese Communist Party running your infrastructure and

3269
02:40:29,960 --> 02:40:33,560
having backdoors into it, right, which is also not good.

3270
02:40:34,280 --> 02:40:37,720
What's your sense of where they stand in terms of the race towards

3271
02:40:38,680 --> 02:40:41,240
superintelligence as compared to the United States?

3272
02:40:41,240 --> 02:40:45,000
Yeah, so good news is they're behind, but bad news is they, you know, they, let's just say they

3273
02:40:45,000 --> 02:40:49,400
get access to everything we do. So they're probably a year behind at each point in time,

3274
02:40:49,400 --> 02:40:53,400
but they get, you know, downloads, I think of basically all of our work on a regular basis

3275
02:40:53,400 --> 02:40:57,640
through a variety of means. And they are, you know, at least, we'll see they're at least

3276
02:40:57,640 --> 02:41:02,040
putting out reports of very competitive, just put out a report last week of a GPT 3.5 analog.

3277
02:41:03,720 --> 02:41:06,920
They put out this report, forget what it's called, but they put out this report of the

3278
02:41:06,920 --> 02:41:10,760
cell and they did, and they, you know, the way when OpenAI puts out, they, they,

3279
02:41:10,760 --> 02:41:15,080
one of the ways they test, you know, GPT is they, they run it through standardized exams

3280
02:41:15,080 --> 02:41:19,880
like the SAT, right? Just how you can kind of gauge how smart it is. And so the Chinese report,

3281
02:41:19,880 --> 02:41:26,440
they ran their LM through the Chinese equivalent of the SAT, and it includes a section on Marxism.

3282
02:41:27,400 --> 02:41:31,560
And a section on Mao Zedong thought, and it turns out their AI does very well on both of those

3283
02:41:31,560 --> 02:41:39,880
topics, right? So like, this, this alignment thing, communist AI, right? Like literal communist AI,

3284
02:41:39,880 --> 02:41:44,520
right? And so their vision is like, that's the, you know, so, you know, you, you can just imagine

3285
02:41:44,520 --> 02:41:50,040
like you're a school, you know, you're a kid 10 years from now in Argentina or in Germany or in

3286
02:41:50,840 --> 02:41:55,080
who knows where, uh, Indonesia. And you ask, they, I'd explain to you like how the economy

3287
02:41:55,080 --> 02:41:58,760
works and it gives you the most cheery upbeat explanation of Chinese style communism you've

3288
02:41:58,760 --> 02:42:04,760
ever heard, right? So like the stakes here are like really big. Well, my, as we've been talking

3289
02:42:04,760 --> 02:42:08,840
about, my hope is not just for the United States, but it would just, uh, the kitten as basement

3290
02:42:08,840 --> 02:42:15,640
with open source LLM. Cause I, I don't know if I, um, trust large centralized institutions

3291
02:42:15,640 --> 02:42:21,480
with super powerful AI, no matter what their ideology is a power corrupts.

3292
02:42:22,120 --> 02:42:29,240
You've been investing in tech companies for about, let's say 20 years and, uh, about 15 of which was,

3293
02:42:29,240 --> 02:42:35,640
uh, with Andreessen Horowitz. Uh, what interesting trends in tech have you seen over that time?

3294
02:42:35,640 --> 02:42:39,000
Let's just talk about companies and just the evolution of the tech industry.

3295
02:42:39,000 --> 02:42:43,320
I mean, the big shift over 20 years has been that tech used to be a tools industry,

3296
02:42:43,960 --> 02:42:49,320
for basically from like 1940 through to about 2010, almost all the big successful companies

3297
02:42:49,400 --> 02:42:55,160
were picks and shovels companies. So PC database, smartphone, you know, some, some, some tool that

3298
02:42:55,160 --> 02:43:00,680
somebody else would pick up and use. Since 2010, most of the big wins have been in applications.

3299
02:43:01,320 --> 02:43:06,920
So a company that starts a, you know, it starts in an existing industry and goes directly to the

3300
02:43:06,920 --> 02:43:11,400
customer in that industry. And then, you know, the early examples there were like Uber and Lyft

3301
02:43:11,400 --> 02:43:17,560
and Airbnb. Um, and then that model is kind of elaborating out. Um, the AI thing is actually

3302
02:43:17,640 --> 02:43:21,320
a reversion on that for now. Cause like most of the AI business right now is actually in cloud

3303
02:43:21,320 --> 02:43:24,360
provision of, of, of AI APIs for other people to build on, but,

3304
02:43:24,360 --> 02:43:26,360
But the big thing will probably be an app.

3305
02:43:26,360 --> 02:43:30,440
Yeah. I think, I think most of the money, I think probably will be in whatever, yeah,

3306
02:43:30,440 --> 02:43:35,160
your AI financial advisor or your AI doctor or your AI lawyer or, you know, take your pick

3307
02:43:35,160 --> 02:43:38,760
of whatever the domain is. Um, and there, and what's interesting is, you know, we,

3308
02:43:38,760 --> 02:43:43,160
the valley kind of does everything. We, we, the entrepreneurs kind of elaborate every possible

3309
02:43:43,160 --> 02:43:47,640
idea. And so there will be a set of companies that like make AI, um, something that can be

3310
02:43:47,640 --> 02:43:51,160
purchased and used by large law firms. Um, and then there will be other companies that just

3311
02:43:51,160 --> 02:43:53,400
go direct to market as a, as an AI lawyer.

3312
02:43:54,600 --> 02:44:01,320
What advice could you give for startup founder? Just having seen so many successful companies,

3313
02:44:01,320 --> 02:44:05,560
so many companies that fail also. What advice could you give to a startup founder,

3314
02:44:05,560 --> 02:44:10,200
someone who wants to build the next super successful startup in the tech space,

3315
02:44:10,280 --> 02:44:12,680
the Googles, the apples, the twitters.

3316
02:44:14,520 --> 02:44:18,120
Yeah. So the great thing about the really great founders is they don't take any advice. So,

3317
02:44:19,800 --> 02:44:24,120
so if you find yourself listening to advice, maybe you shouldn't do it. Um,

3318
02:44:24,120 --> 02:44:29,400
Well, that's actually just to elaborate on that. If you could also speak to great founders too.

3319
02:44:29,400 --> 02:44:29,960
Yeah.

3320
02:44:29,960 --> 02:44:31,640
Like what, what makes a great founder?

3321
02:44:32,280 --> 02:44:38,440
So it makes a great founder is super smart, um, coupled with super energetic, coupled with super

3322
02:44:38,440 --> 02:44:41,000
courageous. I think it's some of those, those three.

3323
02:44:41,000 --> 02:44:43,240
And intelligence, passion and courage.

3324
02:44:43,240 --> 02:44:46,920
The first two are traits and the third one is a choice. I think courage is a choice.

3325
02:44:47,480 --> 02:44:51,240
Well, cause courage is a question of pain, tolerance, right? Um,

3326
02:44:52,280 --> 02:44:56,600
so, um, how, how many times you will want to get punched in the face before you quit?

3327
02:44:57,000 --> 02:44:57,080
Yeah.

3328
02:44:57,080 --> 02:45:01,800
Um, and here's maybe the biggest thing people don't understand about what it's like to be a startup

3329
02:45:01,800 --> 02:45:05,880
founder is it gets, it gets very romanticized, right? Um, and even when, even when they fail,

3330
02:45:05,880 --> 02:45:08,680
it still gets romanticized about like what a great adventure it was, but like

3331
02:45:09,560 --> 02:45:13,480
the reality of it is most of what happens is people telling you, no, and then they usually

3332
02:45:13,480 --> 02:45:17,960
follow that with your stupid, right? No, I will not come to work for you. Um, and I will not leave

3333
02:45:17,960 --> 02:45:21,000
my cushy job cool to come work for you. No, I'm not going to buy your products. You know,

3334
02:45:21,000 --> 02:45:24,200
no, I'm not going to run a story about your company. No, I'm not this that the other thing.

3335
02:45:24,840 --> 02:45:29,000
Um, and so a huge amount of what people have to do is just get used to just getting punched.

3336
02:45:29,000 --> 02:45:32,200
And, and, and the reason people don't understand this is because when you're a founder, you cannot

3337
02:45:32,200 --> 02:45:35,320
let on that this is happening because it will cause people to think that you're weak and they'll

3338
02:45:35,320 --> 02:45:40,040
lose faith in you. Yeah. So you have to pretend that you're having a great time when you're dying

3339
02:45:40,040 --> 02:45:46,440
inside, right? You're just in misery. But why, why do they do it? Why do they do it? Yeah,

3340
02:45:46,440 --> 02:45:49,800
that's the thing. It's, it's like it is a level. This is actually one of the conclusions I think

3341
02:45:49,800 --> 02:45:53,240
is it, I think it's actually, for most of these people on a risk adjusted basis, it's probably

3342
02:45:53,240 --> 02:45:57,240
an irrational act. They could probably be more financially successful on average if they just

3343
02:45:57,240 --> 02:46:01,880
got like a real job and in a big company. Um, but there's, you know, some people just have an

3344
02:46:01,880 --> 02:46:05,640
irrational need to do something new and build something for themselves. And, you know, some

3345
02:46:05,640 --> 02:46:08,920
people just can't tolerate having bosses. Oh, here's the fun thing is how do you reference

3346
02:46:08,920 --> 02:46:12,520
check founders? Right? So you call it, you know, normally you reference check your time hiring

3347
02:46:12,520 --> 02:46:15,880
somebody as you call the bosses there at their, you know, and you find out if they were good

3348
02:46:15,880 --> 02:46:19,720
employees. And now you're trying to reference check Steve Jobs, right? And it's like, Oh God,

3349
02:46:19,720 --> 02:46:22,680
he was terrible. You know, he was a terrible employee. He never did what we told him to do.

3350
02:46:22,680 --> 02:46:29,000
Yeah. So what's a good reference? If you want the previous boss to actually say that

3351
02:46:29,720 --> 02:46:32,840
they never did what you told them to do, that might be a good thing.

3352
02:46:32,840 --> 02:46:36,680
Well, ideally, ideally what you want is I will go, I would like to go to work for that person.

3353
02:46:37,720 --> 02:46:41,400
He worked for me here and now I'd like to work for him. Now, unfortunately, most people can't,

3354
02:46:41,400 --> 02:46:45,240
their egos can't, can't handle that. So they won't say that, but that's the ideal.

3355
02:46:45,240 --> 02:46:50,440
What advice would you give to those folks in the space of intelligence, passion and courage?

3356
02:46:51,160 --> 02:46:55,320
So I think the other big thing is you see people sometimes who say, I want to start a company

3357
02:46:55,320 --> 02:46:59,080
and then they kind of work through the process of coming up with an idea. And generally, those

3358
02:46:59,080 --> 02:47:03,880
don't work as well as the case where somebody has the idea first, and then they kind of realize

3359
02:47:03,880 --> 02:47:06,920
that there's an opportunity to build a company and then they just turn out to be the right kind

3360
02:47:06,920 --> 02:47:12,520
of person to do that. When you say idea, do you mean long-term big vision or do you mean

3361
02:47:12,520 --> 02:47:18,360
specifics of like product? I would say specific. Like specifically what, yeah, specifics. Like,

3362
02:47:18,360 --> 02:47:21,160
what is the, because for the first five years, you don't get to have vision. You just got to

3363
02:47:21,160 --> 02:47:24,520
build something people want and you got to figure out a way to sell it to them, right? It's very

3364
02:47:24,520 --> 02:47:29,240
practical or you never get to big vision. So the first, the first part, you have an idea of a

3365
02:47:29,240 --> 02:47:32,520
set of products or the first product that can actually make some money. Yeah. Like it's got to,

3366
02:47:32,520 --> 02:47:35,880
the first product's got to work, by which I mean like it has to technically work, but then it has

3367
02:47:35,880 --> 02:47:39,640
to actually fit into the category in the customer's mind of something that they want. And then, and

3368
02:47:39,640 --> 02:47:42,520
then by the way, the other part is they have to want to pay for it. Like somebody's got to pay

3369
02:47:42,520 --> 02:47:45,800
the bills. And so you got to figure out how to price it and whether you can actually extract the

3370
02:47:45,800 --> 02:47:53,320
money. So usually it is much more predictable. Success is never predictable, but it's more

3371
02:47:53,320 --> 02:47:57,880
predictable if you start with a great idea and then back into starting the company. So this is

3372
02:47:57,880 --> 02:48:01,240
what we did, you know, we had Mosaic before we had Escape. The Google guys had the Google search

3373
02:48:01,240 --> 02:48:06,600
engine working at Stanford, right? The, you know, yeah, actually, there's tons of examples where

3374
02:48:06,600 --> 02:48:11,400
they, you know, Pierre Omadir had eBay working before he left his previous job. So I really love

3375
02:48:11,400 --> 02:48:16,360
that idea of just having a thing, a prototype that actually works before you even begin to

3376
02:48:16,360 --> 02:48:21,000
remotely scale. Yeah. By the way, it's also far easier to raise money, right? Like the ideal pitch

3377
02:48:21,000 --> 02:48:24,040
that we receive is, here's the thing that works. Would you like to invest in our company or not?

3378
02:48:24,040 --> 02:48:30,200
Like that's so much easier than here's 30 slides with a dream, right? And then we have this concept

3379
02:48:30,200 --> 02:48:34,200
called the idea maze, which our biology friend of Boston came up with when he was with us.

3380
02:48:35,080 --> 02:48:39,320
So, so, so then there's this thing, this goes to mythology, which is, you know, there's a

3381
02:48:39,320 --> 02:48:43,240
mythology that kind of, you know, these, these ideas, you know, kind of arrive like magic or

3382
02:48:43,240 --> 02:48:46,200
people kind of stumble into them. It's like eBay with the pest dispensers or something.

3383
02:48:46,760 --> 02:48:52,920
The reality usually with the big successes is that the founder has been chewing on the problem

3384
02:48:52,920 --> 02:48:57,880
for five or 10 years before they start the company. And they often worked on it in school,

3385
02:48:58,840 --> 02:49:03,320
or they even experimented on it when they were a kid. And they've been kind of training up over

3386
02:49:03,320 --> 02:49:06,680
that period of time to be able to do the thing. So they're like a true domain expert.

3387
02:49:07,560 --> 02:49:11,080
And it sort of sounds like mom and apple pie, which is, yeah, you want to be a domain expert

3388
02:49:11,080 --> 02:49:14,760
in what you're doing. But you would, you know, the mythology is so strong of like, oh, I just

3389
02:49:14,760 --> 02:49:18,360
like had this idea in the shower and now I'm doing it. Like it's generally not that.

3390
02:49:18,360 --> 02:49:26,280
No, because it's, well, maybe in the shower, we had the exact product implementation details.

3391
02:49:26,280 --> 02:49:30,600
But yeah, usually you're going to be for like years, if not decades, thinking about

3392
02:49:32,280 --> 02:49:38,760
like everything around that. Well, we call it the idea maze because the idea maze basically is

3393
02:49:38,760 --> 02:49:42,440
like there's all these permutations. Like for any idea, there's like all these different

3394
02:49:42,440 --> 02:49:45,480
permutations, who should the customer be, what shape, form should the product have,

3395
02:49:45,480 --> 02:49:50,840
and how should we take it to market and all these things. And so the really smart founders

3396
02:49:50,840 --> 02:49:53,480
have thought through all these scenarios by the time they go out to raise money.

3397
02:49:54,360 --> 02:49:58,760
And they have like detailed answers on every one of those fronts because they put so much

3398
02:49:58,760 --> 02:50:04,520
thought into it. The sort of the sort of more haphazard founders haven't thought about any of

3399
02:50:04,520 --> 02:50:06,520
that. And it's the detailed ones who tend to do much better.

3400
02:50:06,520 --> 02:50:11,000
So how do you know what to take a leap? If you have a cushy job or happy life?

3401
02:50:11,960 --> 02:50:15,160
I mean, the best reason is just because you can't tolerate not doing it, right?

3402
02:50:15,160 --> 02:50:17,960
This is the kind of thing where if you have to be advised into doing it, you probably shouldn't do

3403
02:50:17,960 --> 02:50:22,760
it. And so it's probably the opposite, which is you just have such a burning sense of this has

3404
02:50:22,760 --> 02:50:27,080
to be done. I have to do this. I have no choice. What if it's going to lead to a lot of pain?

3405
02:50:27,080 --> 02:50:32,520
It's going to lead to a lot of pain. I think that's what if it means losing sort of social

3406
02:50:32,520 --> 02:50:38,280
relationships and damaging your relationship with loved ones and all that kind of stuff.

3407
02:50:38,360 --> 02:50:42,520
Yeah, look, so it's going to put you in a social tunnel for sure, right? So you're going to like,

3408
02:50:44,200 --> 02:50:48,120
there's this game you can play on Twitter, which is you can do any whiff of the idea that there's

3409
02:50:48,120 --> 02:50:51,800
basically any such thing as work-life balance and that people should actually work hard and

3410
02:50:51,800 --> 02:50:55,560
everybody gets mad. But like the truth is, all the successful founders are working 80 hour weeks

3411
02:50:55,560 --> 02:51:00,680
and they form very strong social bonds with the people they work with. They tend to lose a lot

3412
02:51:00,680 --> 02:51:05,160
of friends on the outside or put those friendships on ice. That's just the nature of the thing.

3413
02:51:05,560 --> 02:51:09,480
You know, for most people, that's worth the trade-off. You know, the advantage maybe younger

3414
02:51:09,480 --> 02:51:12,360
founders have is maybe they have less, you know, maybe they're not, you know, for example,

3415
02:51:12,360 --> 02:51:14,920
if they're not married yet or don't have kids yet, that's an easier thing to bite off.

3416
02:51:15,560 --> 02:51:16,760
Can you be an older founder?

3417
02:51:16,760 --> 02:51:20,600
Yeah, you definitely can. Yeah. Yeah. Many of the most successful founders are second,

3418
02:51:20,600 --> 02:51:24,600
third, fourth time founders. They're in their 30s, 40s, 50s. The good news of being an older

3419
02:51:24,600 --> 02:51:29,000
founder is you know more and you know a lot more about what to do, which is very helpful. The problem

3420
02:51:29,000 --> 02:51:33,400
is, okay, now you've got like a spouse and a family and kids and like you've got to go to the baseball

3421
02:51:33,400 --> 02:51:35,960
game and like you can't go to the baseball, you know, and so it's good.

3422
02:51:37,080 --> 02:51:39,160
Life is full of difficult choices. Yes.

3423
02:51:39,160 --> 02:51:45,800
I can't reason. You've written a blog post on what you've been up to. You wrote this in October 2022.

3424
02:51:47,000 --> 02:51:53,240
Quote, mostly I try to learn a lot. For example, the political events of 2014 to 2016 made clear

3425
02:51:53,240 --> 02:51:58,520
to me that I didn't understand politics at all. Referencing maybe some of this book here.

3426
02:51:58,600 --> 02:52:04,200
So, I deliberately withdrew from political engagement and fundraising and instead read

3427
02:52:04,200 --> 02:52:10,360
my way back into history and as far to the political left and political right as I could.

3428
02:52:10,360 --> 02:52:13,400
So, just high level question. What's your approach to learning?

3429
02:52:14,440 --> 02:52:21,080
Yeah, so it's basically, I would say it's auto-divac. So, it's sort of going down the rabbit holes.

3430
02:52:21,960 --> 02:52:25,560
So, it's a combination. I kind of alluded to it in that quote. It's a combination of

3431
02:52:25,560 --> 02:52:30,600
breadth and depth. And so, I tend to, yeah, I tend to, I go broad by the nature of what I do,

3432
02:52:30,600 --> 02:52:34,040
I go broad, but then I tend to go deep in a rabbit hole for a while, read everything I can,

3433
02:52:34,040 --> 02:52:38,200
and then come out of it. And I might not revisit that rabbit hole for, you know, another decade.

3434
02:52:38,200 --> 02:52:43,240
And in that blog post that I recommend people go check out, you actually list a bunch of different

3435
02:52:43,240 --> 02:52:47,640
books that you recommend on different topics on the American left and the American right.

3436
02:52:49,000 --> 02:52:53,160
It's just a lot of really good stuff. The best explanation for the current structure of our

3437
02:52:53,160 --> 02:52:57,800
society and politics, you give two recommendations, four books on the Spanish Civil War, six books on

3438
02:52:57,800 --> 02:53:04,440
deep history of the American right, comprehensive biographies of Adolf Hitler, one of which I read

3439
02:53:04,440 --> 02:53:09,320
can recommend, six books on the deep history of the American left. So, American right and American

3440
02:53:09,320 --> 02:53:15,880
left looking at the history to give you the context. Biography of Vladimir Lenin, two of them

3441
02:53:16,760 --> 02:53:20,760
on the French Revolution. Actually, I have never read a biography on Lenin. Maybe that will be

3442
02:53:20,760 --> 02:53:26,040
useful. Everything's been so Marx focused. The Sebastian biography of Lenin is extraordinary.

3443
02:53:26,840 --> 02:53:30,680
Victor Sebastian, okay. It'll blow your mind. Yeah. So, it's still useful to read. It's incredible.

3444
02:53:30,680 --> 02:53:33,400
Yeah, it's incredible. I actually think it's the single best book on the Soviet Union.

3445
02:53:33,960 --> 02:53:38,360
So, that, the perspective of Lenin, it might be the best way to look at the Soviet Union versus

3446
02:53:38,360 --> 02:53:44,280
Stalin versus Marx versus, very interesting. So, two books on fascism and anti-fascism

3447
02:53:45,160 --> 02:53:50,920
by the same author, Paul Gottry, a brilliant book on the nature of mass movements and collective

3448
02:53:50,920 --> 02:53:55,720
psychology, the definitive work on intellectual life under totalitarianism, the captive mind,

3449
02:53:56,360 --> 02:54:01,640
the definitive work on the practical life under totalitarianism. There's a bunch, there's a bunch,

3450
02:54:01,640 --> 02:54:07,000
and the single best book. First of all, the list here is just incredible. But you say the single

3451
02:54:07,000 --> 02:54:13,160
best book I have found on who we are and how we got here is the ancient city by Neuma Dennis,

3452
02:54:13,240 --> 02:54:21,400
Fostel de Kulankis. I like it. What did you learn about who we are as a human civilization from

3453
02:54:21,400 --> 02:54:25,800
that book? Yeah. So, this is a fascinating book. This one's free, by the way. It's a book from

3454
02:54:25,800 --> 02:54:31,080
the 1860s. You can download it or you can buy prints of it. But it was this guy who was a

3455
02:54:31,080 --> 02:54:37,160
professor at the Sorbonne in the 1860s, and he was apparently a savant on antiquity, on Greek

3456
02:54:37,160 --> 02:54:42,440
and Roman antiquity. And the reason I say that is because his sources are 100% original Greek and

3457
02:54:42,440 --> 02:54:47,960
Roman sources. So, he wrote basically a history of Western civilization from on the order of 4,000

3458
02:54:47,960 --> 02:54:53,720
years ago to basically the present times entirely working on original Greek and Roman sources.

3459
02:54:54,920 --> 02:54:59,080
And what he was specifically trying to do was he was trying to reconstruct from the stories of the

3460
02:54:59,080 --> 02:55:02,760
Greeks and the Romans. He was trying to reconstruct what life in the West was like before the Greeks

3461
02:55:02,760 --> 02:55:10,120
and the Romans, which was in the civilization known as the Indo-Europeans. And the short answer,

3462
02:55:10,120 --> 02:55:16,120
and this is sort of circa 4,000, 2,000 BC to, you know, sort of 500 BC, kind of that 1500 year

3463
02:55:16,120 --> 02:55:22,040
stretch where civilization developed. And his conclusion was basically cults. They were basically

3464
02:55:22,040 --> 02:55:28,520
cults. And civilization was organized into cults. And the intensity of the cults was like a million

3465
02:55:28,520 --> 02:55:34,040
fold beyond anything that we would recognize today. Like it was a level of all-encompassing

3466
02:55:34,600 --> 02:55:41,320
belief and an action around religion. That was at a level of extremeness that we wouldn't even

3467
02:55:41,320 --> 02:55:47,240
recognize it. And so specifically, he tells the story of basically, there were three levels of

3468
02:55:47,240 --> 02:55:52,760
cults. There was the family cult, the tribal cult, and then the city cult as society scaled up.

3469
02:55:53,320 --> 02:56:00,040
And then each cult was a joint cult of family gods, which were ancestor gods and then nature gods.

3470
02:56:00,760 --> 02:56:05,880
And then your bonding into a family, a tribe, or a city was based on your adherence to that

3471
02:56:05,880 --> 02:56:12,680
religion. People who were not of your family tribe city worshiped different gods, which gave you

3472
02:56:12,680 --> 02:56:19,000
not just the right with the responsibility to kill them on sight. Right? So they were serious

3473
02:56:19,000 --> 02:56:23,800
about their cults. Hard core. By the way, shocking development, I did not realize there's a zero

3474
02:56:23,800 --> 02:56:27,720
concept of individual rights. Like even up through the Greeks and even in the Romans,

3475
02:56:27,720 --> 02:56:30,840
they didn't have the concept of individual rights. Like the idea that as an individual,

3476
02:56:30,840 --> 02:56:34,840
you have some rights, just like noop. Right? And you look back and you're just like, wow,

3477
02:56:34,840 --> 02:56:38,600
that's just like crazily like fascist and a degree that we wouldn't recognize today. But it's like,

3478
02:56:38,600 --> 02:56:44,040
well, they were living under extreme pressure for survival. And the theory goes, you could not have

3479
02:56:44,040 --> 02:56:46,760
people running around making claims to individual rights when you're just trying to get like your

3480
02:56:46,760 --> 02:56:51,640
tribe through the winter, right? Like you need like hardcore command and control. And actually,

3481
02:56:52,200 --> 02:56:55,400
through modern political lens, those cults were basically both fascist and communist.

3482
02:56:56,040 --> 02:56:59,480
They were fascist in terms of social control, and then they were communist in terms of economics.

3483
02:57:01,160 --> 02:57:03,960
But you think that's fundamentally that like pull towards

3484
02:57:04,920 --> 02:57:06,520
cults is within us?

3485
02:57:06,520 --> 02:57:12,920
Well, so my conclusion from this book, so the way we naturally think about the world we live in

3486
02:57:12,920 --> 02:57:17,880
today is like, we basically have such an improved version of everything that came before us, right?

3487
02:57:17,880 --> 02:57:21,320
Like we have basically, we've figured out all these things around morality and ethics and

3488
02:57:21,320 --> 02:57:24,600
democracy and all these things. And like they were basically stupid and retrograde and were

3489
02:57:24,600 --> 02:57:29,960
like smart and sophisticated. And we've improved all this. I after reading that book, I now believe

3490
02:57:29,960 --> 02:57:33,960
in many ways the opposite, which is no, actually, we are still running in that original model,

3491
02:57:33,960 --> 02:57:39,160
we're just running in an incredibly diluted version of it. So we're still running basically in

3492
02:57:39,160 --> 02:57:43,400
cults. It's just our cults are at like a thousandth or a millionth the level of intensity, right?

3493
02:57:43,400 --> 02:57:49,080
And so our, so just to take religions, you know, the modern experience of a Christian in our time,

3494
02:57:49,080 --> 02:57:53,400
even somebody who considers him a devout Christian is just a shadow of the level of intensity of

3495
02:57:53,400 --> 02:57:57,880
somebody who belonged to a religion back in that period. And then by the way, we have constraints,

3496
02:57:57,880 --> 02:58:03,160
it goes back to our discussion, we then sort of endlessly create new cults. Like we're trying

3497
02:58:03,160 --> 02:58:09,000
to fill the void, right? And the void is a void of bonding. Okay. Living in their era,

3498
02:58:09,000 --> 02:58:12,760
like everybody living today, transport in that era would view it as just like completely intolerable

3499
02:58:12,760 --> 02:58:16,920
in terms of like the loss of freedom and the level of basically fascist control. However,

3500
02:58:16,920 --> 02:58:20,280
every single person in that era, and he really stresses this, they knew exactly where they

3501
02:58:20,280 --> 02:58:24,120
stood. They knew exactly where they belonged. They knew exactly what their purpose was.

3502
02:58:24,120 --> 02:58:27,000
They know exactly what they needed to do every day. They know exactly why they were doing it.

3503
02:58:27,000 --> 02:58:29,400
They had total certainty about their place in the universe.

3504
02:58:29,400 --> 02:58:33,640
So the question of meaning, the question of purpose was very distinctly clearly defined for

3505
02:58:33,640 --> 02:58:37,720
them. Absolutely. Overwhelmingly, undisputably, undeniably.

3506
02:58:38,360 --> 02:58:43,880
As we turn the volume down on the cultism, we start to the search for meanings that's getting

3507
02:58:43,880 --> 02:58:47,640
harder and harder. Yes. Because we don't have that. We are, we are ungrounded. We are, we are,

3508
02:58:47,640 --> 02:58:51,640
we are uncentered and we, and we all feel it, right? And that's why we reach for, you know,

3509
02:58:51,640 --> 02:58:55,720
it's why we still reach for religion. It's why we reach for, you know, we, people start to take

3510
02:58:55,720 --> 02:58:58,520
on, you know, let's say, you know, a faith in science, maybe beyond where they should put it.

3511
02:58:59,320 --> 02:59:02,280
You know, and by the way, like sports teams are like a, you know, they're like a tiny little

3512
02:59:02,280 --> 02:59:06,120
version of a cult and, you know, the, you know, Apple keynotes are a tiny little version of a cult,

3513
02:59:06,840 --> 02:59:11,080
right? You know, political, you know, and there's cult, you know, there's full blown

3514
02:59:11,080 --> 02:59:14,520
cults on both sides of the political spectrum right now, right? You know, operating in plain

3515
02:59:14,600 --> 02:59:17,080
science. But still not full blown, compared as to what it was.

3516
02:59:17,080 --> 02:59:20,040
Compared to what it used to be. I mean, we would today consider full blown, but like,

3517
02:59:20,040 --> 02:59:23,240
yes, they're, they're at like, I don't know, a hundred thousandth or something of the intensity

3518
02:59:23,240 --> 02:59:27,800
of what people had back then. So, so we live in a world today that in many ways is more advanced

3519
02:59:27,800 --> 02:59:31,160
and moral and so forth. And it's certainly a lot nicer, much nicer world to live in, but we live

3520
02:59:31,160 --> 02:59:36,440
in a world that's like very washed out. It's like everything has become very colorless and gray as

3521
02:59:36,440 --> 02:59:40,360
compared to how people used to experience things, which is I think why we're so prone to reach for

3522
02:59:40,360 --> 02:59:45,160
drama. There's something in us that's deeply evolved where we want that back.

3523
02:59:46,520 --> 02:59:50,840
And I wonder where it's all headed as we turn the volume down more and more.

3524
02:59:50,840 --> 02:59:55,720
What advice would you give to young folks today? In high school and college, how to be

3525
02:59:55,720 --> 02:59:58,440
successful in their career, how to be successful in their life?

3526
02:59:58,440 --> 03:00:04,040
Yes. So the tools that are available today, I mean, are just like, I sometimes, you know,

3527
03:00:04,040 --> 03:00:08,040
I sometimes bore, you know, kids by describing like what it was like to go look up a book,

3528
03:00:08,040 --> 03:00:11,960
you know, to try to like discover a fact and, you know, in the old days, the 1970s, 1980s and

3529
03:00:11,960 --> 03:00:15,000
go to the library and the card catalog and the whole thing. You go through all that work and

3530
03:00:15,000 --> 03:00:19,160
then the book is checked out and you have to wait two weeks and like, like to be in a world,

3531
03:00:19,160 --> 03:00:22,760
not only where you can get the answer to any question, but also the world now, you know,

3532
03:00:22,760 --> 03:00:25,400
the AI world where you've got like the assistant that will help you do anything,

3533
03:00:25,400 --> 03:00:30,200
help you teach, learn anything, like your ability both to learn and also to produce is just like,

3534
03:00:30,200 --> 03:00:34,520
I don't know, a million fold beyond what it used to be. I have a blog post I've been wanting to

3535
03:00:34,520 --> 03:00:40,040
write. It was, I call out where, where are the hyperproductive people? Like,

3536
03:00:40,040 --> 03:00:44,760
Good question. Right. Like with these tools, like there should be authors that are writing like

3537
03:00:44,760 --> 03:00:49,320
hundreds or thousands of like outstanding books. Well, with the authors, there's a consumption

3538
03:00:49,320 --> 03:00:55,720
question too. But yeah, well, maybe not, maybe not. You're right. But so the tools are much more

3539
03:00:55,720 --> 03:01:01,080
powerful. Artists, musicians, right? Why aren't musicians producing a thousand times the number

3540
03:01:01,080 --> 03:01:07,880
of songs, right? Like, like the tools are spectacular. So what, what's the explanation?

3541
03:01:07,880 --> 03:01:14,360
And by way of advice, like, what is motivation starting to be turned down a little bit or what?

3542
03:01:14,360 --> 03:01:18,280
I think it might be distraction. It's so easy to just sit and consume.

3543
03:01:19,480 --> 03:01:23,880
That I think people get distracted from production. But if you wanted to, you know,

3544
03:01:23,880 --> 03:01:28,040
as a young person, if you wanted to really stand out, you could get on like a hyperproductivity

3545
03:01:28,040 --> 03:01:32,920
curve very early on. There's a great, you know, the story, there's a great story in Roman history

3546
03:01:32,920 --> 03:01:37,880
of plenty of the elder who was this legendary statesman. And I died in the Vesuvius eruption

3547
03:01:37,880 --> 03:01:42,200
trying to rescue his friends. But he was famous both for being a savant of basically being a

3548
03:01:42,200 --> 03:01:45,800
polymath, but also being an author. And he wrote apparently like hundreds of books, most of which

3549
03:01:45,800 --> 03:01:49,880
have been lost, but he like wrote all these encyclopedias. And he literally like would be

3550
03:01:49,880 --> 03:01:53,720
reading and writing all day long, no matter what else is going on. And he so he would like travel

3551
03:01:53,720 --> 03:01:56,840
with like four slaves, and two of them were responsible for reading to him. And two of them

3552
03:01:56,840 --> 03:02:01,000
were responsible for taking dictation. And so like, he'd be going across country and like

3553
03:02:01,000 --> 03:02:04,920
literally he would be writing books like all the time. And apparently they were spectacular.

3554
03:02:04,920 --> 03:02:07,240
There's only a few that have survived, but apparently they were amazing.

3555
03:02:07,240 --> 03:02:10,680
So there's a lot of value to being somebody who finds focus in this life.

3556
03:02:10,680 --> 03:02:14,040
Yeah, like, and there are examples, like there are, you know, there's this guy,

3557
03:02:14,040 --> 03:02:18,520
judge was just named Posner, Posner, who wrote like 40 books and was also a great federal judge.

3558
03:02:19,160 --> 03:02:22,600
You know, there's our friend, Balji, I think it's like this, he's one of these,

3559
03:02:22,600 --> 03:02:26,920
you know, where his output is just prodigious. And so it's like, yeah, I mean, with these tools,

3560
03:02:26,920 --> 03:02:30,840
why not? And I kind of think we're at this interesting kind of freeze frame moment where

3561
03:02:30,840 --> 03:02:33,320
like this, these tools are not everybody's hands and everybody's just kind of staring

3562
03:02:33,320 --> 03:02:36,920
at them trying to figure out what to do. The new tools. We have discovered fire.

3563
03:02:36,920 --> 03:02:40,280
Yeah. And trying to figure out how to use it to cook. Yeah, right.

3564
03:02:41,560 --> 03:02:46,680
You told Tim Ferriss that the perfect day is caffeine for 10 hours and alcohol for four hours.

3565
03:02:47,640 --> 03:02:52,760
You didn't think I'd be mentioning this, did you? It balances everything out perfectly,

3566
03:02:52,760 --> 03:02:59,080
as you said. So perfect. So let me ask, what's the secret to balance and maybe to happiness in life?

3567
03:03:00,520 --> 03:03:04,440
I don't believe in balance. So I'm the wrong person to ask. Can you elaborate

3568
03:03:04,440 --> 03:03:08,600
why you don't believe in balance? I mean, maybe it's just, and I look, I think people,

3569
03:03:08,600 --> 03:03:12,360
I think people are wired differently. So I think it's hard to generalize this kind of thing. But

3570
03:03:12,440 --> 03:03:15,560
I'm much happier and more satisfied when I'm fully committed to something. So

3571
03:03:16,120 --> 03:03:18,360
I'm very much in favor of imbalance. Yeah.

3572
03:03:19,480 --> 03:03:22,680
In balance. And that applies to work, to life, to everything.

3573
03:03:23,640 --> 03:03:27,800
Yeah. Now, I happen to have whatever twist of personality traits lead that in non-destructive

3574
03:03:27,800 --> 03:03:32,600
dimensions, including the fact that I've actually, I now no longer do the 10-4 plan. I stopped drinking.

3575
03:03:32,600 --> 03:03:36,360
I do the caffeine, but not the alcohol. So there's something in my personality where I,

3576
03:03:36,360 --> 03:03:40,600
I, whatever maladaption I have is inclining me towards productive things, not on productive

3577
03:03:40,600 --> 03:03:46,680
things. So you're one of the wealthiest people in the world. What's the relationship between wealth

3578
03:03:46,680 --> 03:03:54,840
and happiness? Oh, money and happiness. So I think happiness, I don't think happiness is the thing

3579
03:03:55,720 --> 03:03:57,880
to strive for. I think satisfaction is the thing.

3580
03:03:59,000 --> 03:04:02,280
That's, that just sounds like happiness, but turned down a bit.

3581
03:04:02,280 --> 03:04:07,400
No deeper. So happiness is, you know, a walk in the woods at sunset, an ice cream cone,

3582
03:04:08,360 --> 03:04:13,240
a kiss. The first ice cream cone is great. This 1000th ice cream cone,

3583
03:04:14,200 --> 03:04:16,440
not so much at some point, the walks in the woods get boring.

3584
03:04:16,440 --> 03:04:20,040
What's the distinction between happiness and satisfaction?

3585
03:04:20,040 --> 03:04:25,880
Satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful.

3586
03:04:26,600 --> 03:04:34,920
So just something that permeates all your days, just this general contentment of being useful.

3587
03:04:34,920 --> 03:04:39,240
Then I'm fully satisfying my faculties, then I'm fully delivering, right,

3588
03:04:39,240 --> 03:04:42,360
on the gifts that I've been given, that I'm, you know, net making the world better,

3589
03:04:42,360 --> 03:04:46,680
that I'm contributing to the people around me, right? And then I can look back and say,

3590
03:04:46,680 --> 03:04:51,080
wow, that was hard, but it was worth it. I think generally it seems to leave people in a better

3591
03:04:51,080 --> 03:04:54,760
state than pursuit of pleasure, pursuit of quote unquote happiness.

3592
03:04:54,760 --> 03:04:56,200
Does money have anything to do with that?

3593
03:04:56,200 --> 03:04:59,880
I think the founders, the founding fathers in the US threw this off kilter when they used the

3594
03:04:59,880 --> 03:05:01,880
phrase pursuit of happiness, I think they should have said.

3595
03:05:02,680 --> 03:05:03,480
Pursuit of satisfaction.

3596
03:05:03,480 --> 03:05:05,960
They said pursuit of satisfaction, we might live in a better world today.

3597
03:05:05,960 --> 03:05:08,520
Well, you know, they could have elaborated on a lot of things.

3598
03:05:09,400 --> 03:05:10,680
They could have tweaked the second amendment.

3599
03:05:10,680 --> 03:05:13,480
I think they were smarter than they realized. They said, you know what,

3600
03:05:13,480 --> 03:05:17,560
we're going to make it ambiguous and let these humans figure out the rest,

3601
03:05:17,560 --> 03:05:20,520
these tribal cult like humans figure out the rest.

3602
03:05:23,320 --> 03:05:24,760
But money empowers that.

3603
03:05:24,760 --> 03:05:29,000
So I think, and I think they're, I mean, look, I think Elon is, I don't think I'm even a great

3604
03:05:29,000 --> 03:05:31,560
example, but I think Elon would be the great example of this, which is like, you know, look,

3605
03:05:31,560 --> 03:05:35,000
he's a guy who from every day of his life, from the day he started making money at all,

3606
03:05:35,000 --> 03:05:36,920
he just plows into the next thing.

3607
03:05:38,040 --> 03:05:41,800
And so I think money is definitely an enabler for satisfaction.

3608
03:05:41,800 --> 03:05:44,200
Money applied to happiness leads people down very dark paths.

3609
03:05:46,200 --> 03:05:47,560
Very destructive avenues.

3610
03:05:48,360 --> 03:05:51,160
Money applied to satisfaction, I think could be, is a real tool.

3611
03:05:52,120 --> 03:05:55,480
I always, by the way, I was like, you know, Elon is the case study for behavior.

3612
03:05:55,480 --> 03:05:59,720
But the other thing that's always really made me think is Larry Page was asked one time what

3613
03:05:59,720 --> 03:06:03,240
his approach to philanthropy was, and he said, oh, I'm just my philanthropic plan is just give

3614
03:06:03,240 --> 03:06:04,040
all the money to Elon.

3615
03:06:06,600 --> 03:06:06,840
Right.

3616
03:06:06,840 --> 03:06:13,000
Well, let me actually ask you about Elon. What are your, you've interacted with quite a lot of

3617
03:06:13,000 --> 03:06:16,920
successful engineers and business people. What do you think is special about Elon?

3618
03:06:16,920 --> 03:06:18,200
We talked about Steve Jobs.

3619
03:06:18,920 --> 03:06:23,800
What, what do you think is special about him as a leader as an innovator?

3620
03:06:23,800 --> 03:06:29,240
Yeah. So the, the core of it is he's, he's, he's back to the future. So he is, he is doing the

3621
03:06:29,240 --> 03:06:32,680
most leading edge things in the world, but with a really deeply old school approach.

3622
03:06:33,560 --> 03:06:38,520
And so to find comparisons to Elon, you need to go to like Henry Ford and Thomas Watson and Howard

3623
03:06:38,520 --> 03:06:46,280
Hughes and Andrew Carnegie, right, Leland Stanford, John T. Rockefeller, right, you need to go to the,

3624
03:06:46,280 --> 03:06:50,680
what we're called the bourgeois capitalists, like the hardcore business owner operators who

3625
03:06:50,680 --> 03:06:57,320
basically built, you know, basically built industrialized society, Vanderbilt. And it's

3626
03:06:57,320 --> 03:07:07,880
a level of hands-on commitment and depth in the business, coupled with an absolute

3627
03:07:07,880 --> 03:07:14,200
priority towards truth and towards kind of put science and technology

3628
03:07:14,840 --> 03:07:19,400
down to first principles that is just like absolute, just like unbelievably absolute.

3629
03:07:20,120 --> 03:07:23,960
He really is ideal that he's only ever talking to engineers. Like he does not tolerate,

3630
03:07:24,760 --> 03:07:29,640
he has, he has less bullshit talents than anybody I've ever met. He wants ground truth on every

3631
03:07:29,640 --> 03:07:34,680
single topic. And he runs his businesses directly day to day devoted to getting to ground truth in

3632
03:07:34,680 --> 03:07:41,320
every single topic. So you think it was a good decision for him to buy Twitter?

3633
03:07:41,320 --> 03:07:43,640
I have developed a view in life did not second guess Elon Musk.

3634
03:07:44,280 --> 03:07:49,000
I know this is going to sound crazy and unfounded, but

3635
03:07:49,720 --> 03:07:53,000
well, I mean, he's got a quite a track record.

3636
03:07:53,000 --> 03:07:55,960
I mean, look, the car was a crazy, I mean, the car was, I mean, look,

3637
03:07:55,960 --> 03:07:59,160
he's done a lot of things that seem crazy, starting a new car company in the United

3638
03:07:59,160 --> 03:08:03,000
States of America. The last time somebody really tried to do that was the 1950s. And it was called

3639
03:08:03,000 --> 03:08:06,680
Tucker Automotive. And it was such a disaster, they made a movie about what a disaster it was.

3640
03:08:07,800 --> 03:08:11,720
And then Rockets, like who does that? Like that's, there's obviously no way to start

3641
03:08:11,800 --> 03:08:15,160
in a rocket company like those days are over. And then to do those at the same time.

3642
03:08:16,040 --> 03:08:22,200
So after he pulled those two off, like, okay, fine. Like, this is one of my areas of like,

3643
03:08:22,200 --> 03:08:25,640
whatever opinions I had about that is just like, okay, clearly or not relevant, like this is,

3644
03:08:26,280 --> 03:08:27,640
at some point, you just like put on the person.

3645
03:08:28,200 --> 03:08:33,240
And in general, I wish more people would lean on celebrating and supporting versus deriding

3646
03:08:33,240 --> 03:08:39,320
and destroying. Oh, yeah. I mean, look, he drives resentment, like it's like he is a magnet for

3647
03:08:39,320 --> 03:08:45,080
resentment. Like his critics are the most miserable, resentful people in the world. Like

3648
03:08:45,080 --> 03:08:50,360
it's almost a perfect match of like the most idealized, you know, technologists, you know,

3649
03:08:50,360 --> 03:08:55,240
of the century coupled with like just his critics are just bitter as can be. I mean,

3650
03:08:55,240 --> 03:09:02,360
it's sort of very darkly comic to watch. Well, he, he fuels the fire of that by being an asshole on

3651
03:09:02,360 --> 03:09:08,360
Twitter at times. And which is fascinating to watch the drama of human civilization given our cult

3652
03:09:09,080 --> 03:09:16,680
roots just fully on fire. He's running a cult. You could say that very successfully.

3653
03:09:16,680 --> 03:09:21,080
So now, now there are cults have gone and we search for meaning. What do you think is the

3654
03:09:21,080 --> 03:09:25,080
meaning of this whole thing? What's the meaning of life, Mark Andreessen? I don't know the answer

3655
03:09:25,080 --> 03:09:31,640
to that. I think the meaning of, of the closest I get to it is what I said about satisfaction. So

3656
03:09:31,640 --> 03:09:35,720
it's basically like, okay, we were given what we have, like we should basically do our best.

3657
03:09:35,720 --> 03:09:40,040
What's the role of love in that mix? I mean, like, what's the point of life if you're, yeah,

3658
03:09:40,040 --> 03:09:44,120
without love, like, yeah. So love is a big part of that satisfaction.

3659
03:09:44,120 --> 03:09:47,640
Yeah, look, like taking care of people is like a wonderful thing. Like, you know,

3660
03:09:47,640 --> 03:09:51,960
a mentality, you know, there are pathological forms of taking care of people, but there's also a

3661
03:09:51,960 --> 03:09:55,320
very fundamental, you know, kind of aspect of taking care of people. Like for example, I happen

3662
03:09:55,320 --> 03:09:58,680
to be somebody who believes that capitalism and taking care of people are actually, they're

3663
03:09:58,680 --> 03:10:02,680
actually the same thing. Somebody once said capitalism is how you take care of people you

3664
03:10:02,680 --> 03:10:08,360
don't know, right? Right. And so like, yeah, I think it's like deeply woven into the whole thing.

3665
03:10:09,080 --> 03:10:11,640
You know, there's a long conversation to be had about that, but yeah.

3666
03:10:12,440 --> 03:10:17,000
Yeah, creating products that are used by millions of people and bring them joy in smaller big ways.

3667
03:10:17,560 --> 03:10:20,920
And then capitalism kind of enables that, encourages that.

3668
03:10:21,640 --> 03:10:25,400
David Friedman says there's only three ways to get somebody to do something for somebody else.

3669
03:10:26,120 --> 03:10:27,640
Love, money and force.

3670
03:10:32,280 --> 03:10:33,560
Love and money are better.

3671
03:10:34,840 --> 03:10:37,560
That's a good ordering. I think we should bet on those.

3672
03:10:37,560 --> 03:10:42,600
Try love first. If that doesn't work, the money and then force. Well, don't even try that one.

3673
03:10:43,400 --> 03:10:47,480
Mark, you're an incredible person. I've been a huge fan. I'm glad to finally got a chance to talk.

3674
03:10:47,480 --> 03:10:52,360
I'm a fan of everything you do, everything you do, including on Twitter. It's a huge honor to

3675
03:10:52,360 --> 03:10:55,480
meet you to talk with you. Thanks again for doing this. Awesome. Thank you, Alex.

3676
03:10:56,520 --> 03:11:00,360
Thanks for listening to this conversation with Mark Andreessen. To support this podcast,

3677
03:11:00,360 --> 03:11:05,320
please check out our sponsors in the description. And now let me leave you with some words from

3678
03:11:05,320 --> 03:11:11,320
Mark Andreessen himself. The world is a very malleable place. If you know what you want,

3679
03:11:11,880 --> 03:11:18,120
and you go for it with maximum energy and drive and passion, the world will often reconfigure

3680
03:11:18,120 --> 03:11:23,960
itself around you much more quickly and easily than you would think. Thank you for listening,

3681
03:11:23,960 --> 03:11:34,920
and hope to see you next time.

