WEBVTT

00:00.000 --> 00:06.000
Today we have Josh Tenenbaum. He's a professor here at MIT, leading the Computational Cognitive

00:06.000 --> 00:12.000
Science Group. Among many other topics in cognition and intelligence, he is fascinated

00:12.000 --> 00:18.000
with the question of how human beings learn so much from so little, and how these insights

00:18.000 --> 00:23.480
can lead to build AI systems that are much more efficient learning from data. So please

00:23.480 --> 00:25.480
give Josh a warm welcome.

00:31.480 --> 00:37.480
Alright, thank you very much. Thanks for having me. Excited to be part of what looks like

00:37.480 --> 00:43.480
really quite a very impressive lineup, especially starting after today. And it's, I think, quite a

00:43.480 --> 00:47.480
great opportunity to get to see perspectives on artificial intelligence from many of the

00:47.480 --> 00:54.480
leaders in industry and other entities working on this great quest. So I'm going to talk to you

00:54.480 --> 00:58.480
about some of the work that we do in our group, but also I'm going to try to give a broader

00:58.480 --> 01:02.480
perspective, reflective of a number of MIT faculty, especially those who are affiliated with the

01:02.480 --> 01:07.480
Center for Brains, Minds, and Machines. So you can see up there on my affiliation, academically

01:07.480 --> 01:12.480
I'm part of Brain and Cognitive Science, or Course 9. I'm also part of CSAIL, but I'm also part of

01:12.480 --> 01:17.480
the Center for Brains, Minds, and Machines, which is an NSF-funded center, Science and Technology

01:17.480 --> 01:22.480
Center, which really stands for the bridge between the science and the engineering of intelligence. It

01:22.480 --> 01:26.480
literally straddles Vassar Street in that we have CSAIL and BCS members. We also have partners at

01:26.480 --> 01:31.480
Harvard and other academic institutions. And again, what we stand for, I want to try to convey

01:31.480 --> 01:35.480
some of the specific things we're doing in the center and where we want to go with a vision that

01:35.480 --> 01:41.480
really is about jointly pursuing the science, the basic science of how intelligence arises in the

01:41.480 --> 01:47.480
human mind and brain, and also the engineering enterprise of how to build something increasingly

01:47.480 --> 01:51.480
like human intelligence in machines. And we deeply believe that these two projects have something to do

01:51.480 --> 01:57.480
with each other and are best pursued jointly. Now it's a really exciting time to be doing anything

01:57.480 --> 02:02.480
related to intelligence or certainly to AI for all the reasons that, you know, brought you all here.

02:02.480 --> 02:07.480
I don't have to tell you this. We have all these ways in which AI is kind of finally here. We finally

02:07.480 --> 02:13.480
live in the era of something like real practical AI, or for those who've been around for a while and

02:13.480 --> 02:18.480
have seen some of the rises and falls, you know, AI is back in a big way. But from my perspective,

02:18.480 --> 02:24.480
and I think maybe this reflects, you know, why we distinguish what we might call AGI from AI, we don't

02:24.480 --> 02:29.480
really have any real AI, basically. We have what I like to call AI technologies, which are systems

02:29.480 --> 02:34.480
that do things we used to think that only humans could do. And now we have machines that do them,

02:34.480 --> 02:40.480
often quite well, maybe even better than any human who's ever lived, right? Like a machine that plays go.

02:40.480 --> 02:45.480
But none of these systems, I would say, are truly intelligent. None of them have anything like common sense.

02:45.480 --> 02:51.480
None of them have anything like the flexible general purpose intelligence that each of you might use to learn

02:51.480 --> 02:57.480
every one of these skills or tasks, right? Each of these systems had to be built by large teams of engineers,

02:57.480 --> 03:02.480
working together often for a number of years at often at great cost to somebody who's willing to pay for it.

03:02.480 --> 03:09.480
And each of them just does one thing. So AlphaGo might beat the world's best, but it can't drive to the match,

03:09.480 --> 03:16.480
or even tell you what go is. It can't even tell you that go is a game, because it doesn't even know what a game is, right?

03:16.480 --> 03:23.480
So what's missing? What is it that makes every one of your brains, maybe you can't beat, you know, the world's best in go,

03:23.480 --> 03:29.480
but any one of you can get behind the wheel of a car. I think of this because my daughter is going to turn 16 tomorrow.

03:29.480 --> 03:36.480
If she lived in California, she'd have a driver's license. It's a little bit down the line for us here in Massachusetts.

03:36.480 --> 03:41.480
But, you know, she didn't have to be specially engineered by billion-dollar startups.

03:41.480 --> 03:46.480
And, you know, she got really into chess recently, and now she's taught herself chess by playing just, you know,

03:46.480 --> 03:51.480
and a handful of games, basically. And she can do any one of these activities, and any one of us can.

03:51.480 --> 03:56.480
So what is it? What makes up the difference? Well, there's many things, right?

03:56.480 --> 04:04.480
I'll talk about the focus for us in our research, and a lot of us, again, in CBMM, is summarized here.

04:04.480 --> 04:13.480
What drives the successes right now in AI, especially in industry, okay, and all these AI technologies, is many, many things, many things.

04:13.480 --> 04:19.480
But what's where the progress has been made most recently, and what's getting most of the attention is, of course, deep learning,

04:19.480 --> 04:27.480
but other kinds of machine learning technologies, which essentially represent the maturation of a decades-long effort to solve the problem of pattern recognition.

04:27.480 --> 04:37.480
That means taking data and finding patterns in the data that tell you something you care about, like how to label a class or how to predict some other signal, okay?

04:37.480 --> 04:45.480
And pattern recognition is great. It's an important part of intelligence, and it's reasonable to say that deep learning as a technology

04:45.480 --> 04:52.480
has really made great strides on pattern recognition, and maybe even, you know, has come in close to solving the problems of pattern recognition.

04:52.480 --> 04:59.480
But intelligence is about many other things. Intelligence is about a lot more. In particular, it's about modeling the world,

04:59.480 --> 05:06.480
and think about all the activities that a human does to model the world that go beyond, just say, recognizing patterns in data,

05:06.480 --> 05:10.480
but actually trying to explain and understand what we see, for instance, okay?

05:10.480 --> 05:17.480
Or to be able to imagine things that we've never seen, that never seen, maybe even very different from anything we've ever seen,

05:17.480 --> 05:24.480
but what I want to see, and then to set those as goals, to make plans and solve problems needed to make those things real,

05:24.480 --> 05:29.480
or thinking about learning, again, that, you know, some kinds of learning can be thought of as pattern recognition

05:29.480 --> 05:33.480
if you're learning sufficient statistics or weights in a neural net that are used for those purposes.

05:33.480 --> 05:37.480
But many activities of learning are about building out new models, right?

05:37.480 --> 05:44.480
Either refining, reusing, improving old models, or actually building fundamentally new models as you've experienced more of the world.

05:44.480 --> 05:49.480
And then think about sharing our models, communicating our models to others, modeling their models, learning from them.

05:49.480 --> 05:57.480
All these activities of modeling, these are at the heart of human intelligence, and it requires a much broader set of tools.

05:58.480 --> 06:03.480
So I want to talk about the ways we're studying these activities of modeling the world and something in a pretty non-technical way

06:03.480 --> 06:07.480
about what are the kind of tools that allow us to capture these abilities.

06:07.480 --> 06:12.480
Now, I think it's, I want to be very honest up front and to say this is just the beginning of a story, right?

06:12.480 --> 06:17.480
When you look at deep learning successes, that itself is a story that goes back decades.

06:17.480 --> 06:19.480
I'll say a little bit about that history in a minute.

06:19.480 --> 06:24.480
But where we are now is just looking forward to a future when we might be able to capture these abilities,

06:25.480 --> 06:27.480
at a really mature engineering scale.

06:27.480 --> 06:34.480
And I would say we are far from being able to capture all the ways in which humans richly, flexibly, quickly build models of the world

06:34.480 --> 06:36.480
at the kind of scale that, say, Silicon Valley wants.

06:36.480 --> 06:42.480
Either big tech companies like Google, or Microsoft, or IBM, or Facebook, or small startups, right?

06:42.480 --> 06:44.480
We can get there.

06:44.480 --> 06:48.480
And I think what I want to talk to you about here is one route for trying to get there.

06:48.480 --> 06:51.480
And this is the route that CBMM stands for.

06:51.480 --> 06:55.480
The idea that by reverse engineering how intelligence works in the human mind and brain,

06:55.480 --> 06:59.480
that will give us a route to engineering these abilities in machines.

06:59.480 --> 07:03.480
When we say reverse engineering, we're talking about science, but doing science like engineers.

07:03.480 --> 07:05.480
This is our fundamental principle.

07:05.480 --> 07:08.480
That if we approach cognitive science and neuroscience like an engineer,

07:08.480 --> 07:12.480
where, say, the output of our science isn't just a description of the brain or the mind in words,

07:12.480 --> 07:16.480
but in the same terms that an engineer would use to build an intelligent system,

07:16.480 --> 07:20.480
then that will be both the basis for a much more rigorous and deeply insightful science,

07:20.480 --> 07:25.480
but also direct translation of those insights into engineering applications.

07:25.480 --> 07:30.480
Now, before I talk a little bit about history, what I mean by that is this.

07:30.480 --> 07:33.480
Again, if part of what brought you here is deep learning, and I know,

07:33.480 --> 07:36.480
even if you've never heard of deep learning before, which I'm sure is unlikely,

07:36.480 --> 07:42.480
you saw a good spectrum of that in the overview session last night.

07:42.480 --> 07:48.480
It's really interesting and important to look back on the history of where did techniques for deep learning come from,

07:48.480 --> 07:49.480
or reinforcement learning.

07:49.480 --> 07:54.480
Those are the two tools in the current machine learning arsenal that are getting the most attention.

07:54.480 --> 07:57.480
Things like back propagation or end-to-end stochastic gradient descent

07:57.480 --> 07:59.480
or temporal difference learning or Q-learning.

07:59.480 --> 08:01.480
Here's a few papers from the literature.

08:01.480 --> 08:03.480
Maybe some of you have read these original papers.

08:03.480 --> 08:07.480
Here's the original paper by Rumelhardt, Hinton, and colleagues

08:07.480 --> 08:11.480
in which they introduced the back propagation algorithm for training multi-layer perceptrons,

08:11.480 --> 08:12.480
multi-layer neural networks.

08:12.480 --> 08:14.480
Here's the original perceptron paper by Rosenblatt,

08:14.480 --> 08:17.480
which introduced the one-layer version of that architecture

08:17.480 --> 08:19.480
and the basic perceptron learning algorithm.

08:19.480 --> 08:23.480
Here's the first paper on the temporal difference learning method

08:23.480 --> 08:26.480
for reinforcement learning from Sutton and Bartow.

08:26.480 --> 08:30.480
Here's the original Boltzmann machine paper also by Hinton and colleagues,

08:30.480 --> 08:38.480
those of you who don't know that architecture think of a probabilistic undirected multi-layer perceptron.

08:38.480 --> 08:43.480
For example, before there were LSTMs, if you know about current-recurrent neural network architecture,

08:43.480 --> 08:47.480
earlier much simpler versions of the same idea were proposed by Jeff Elman

08:47.480 --> 08:49.480
and his simple-recurrent networks.

08:49.480 --> 08:51.480
The reason I want to put up the original papers here

08:51.480 --> 08:55.480
is for you to look at both when they were published and where they were published.

08:55.480 --> 09:00.480
If you look at the dates, you'll see papers going back to the 80s,

09:00.480 --> 09:03.480
but even the 60s or even the 1950s.

09:03.480 --> 09:05.480
Look at where they were published.

09:05.480 --> 09:07.480
Most of them were published in psychology journals.

09:07.480 --> 09:09.480
The journal Psychological Review, if you don't know,

09:09.480 --> 09:13.480
it is like the leading journal of theoretical psychology and mathematical psychology,

09:13.480 --> 09:17.480
or Cognitive Science, the journal of the Cognitive Science Society,

09:17.480 --> 09:21.480
or the back prop paper was published in Nature, which is a general interest science journal,

09:21.480 --> 09:25.480
but by people who are mostly affiliated with the Institute for Cognitive Science in San Diego.

09:25.480 --> 09:30.480
What you see here is already a long history of scientists thinking like engineers.

09:30.480 --> 09:33.480
These are people who are in psychology or cognitive science departments

09:33.480 --> 09:35.480
and publishing in those places,

09:35.480 --> 09:40.480
but by formalizing even very basic insights about how humans might learn

09:40.480 --> 09:44.480
or how brains might learn in the right kind of math,

09:44.480 --> 09:47.480
that led to, of course, progress on the science side,

09:47.480 --> 09:49.480
but it led to all the engineering that we see now.

09:49.480 --> 09:51.480
It wasn't sufficient.

09:51.480 --> 09:54.480
We needed, of course, lots of innovations and advances

09:54.480 --> 09:57.480
in computing, hardware, and software systems.

09:57.480 --> 10:00.480
But this is where the basic math came from

10:00.480 --> 10:03.480
and it came from doing science like an engineer.

10:03.480 --> 10:06.480
What we want to talk about in our vision is what does the future of this look like?

10:06.480 --> 10:08.480
If we were to look 50 years into the future,

10:08.480 --> 10:11.480
what would we be looking back on now over this time scale?

10:11.480 --> 10:15.480
Well, here's a long-term research roadmap that reflects some of my ambitions

10:15.480 --> 10:18.480
and some of our center's goals and many others, too.

10:18.480 --> 10:22.480
We'd like to be able to address basic questions, fundamental questions

10:22.480 --> 10:24.480
of what it is to be and to think like a human,

10:24.480 --> 10:29.480
questions, for example, of consciousness or meaning in language or real learning,

10:29.480 --> 10:33.480
questions like, you know, even beyond the individual, like questions of culture

10:33.480 --> 10:35.480
or creativity.

10:35.480 --> 10:37.480
Those are our big ideas up there.

10:37.480 --> 10:39.480
And for each of these, there are basic scientific questions, right?

10:39.480 --> 10:42.480
How do we become aware of the world and ourselves in it?

10:42.480 --> 10:45.480
It starts with perception, but it really turns into awareness,

10:45.480 --> 10:49.480
awareness of yourself and of the world and what we might call consciousness, right?

10:49.480 --> 10:51.480
Or how does a word start to have a meaning?

10:51.480 --> 10:54.480
What really is a meaning and how does a child grasp it?

10:54.480 --> 10:55.480
Or how do children actually learn?

10:55.480 --> 10:57.480
What do babies' brains actually start with?

10:57.480 --> 11:00.480
Are they blank slates or do they start with some kind of cognitive structure?

11:00.480 --> 11:02.480
And then what does real learning look like?

11:02.480 --> 11:05.480
These are just some of the questions that we're interested in working on.

11:05.480 --> 11:07.480
Or when we talk about culture, we mean,

11:07.480 --> 11:11.480
how do you learn all the things you didn't directly experience, right?

11:11.480 --> 11:15.480
But that's somehow you got from the accumulation of knowledge in society over many generations.

11:15.480 --> 11:18.480
Or how do you ever think of new ideas or answers to new questions?

11:18.480 --> 11:20.480
How do you think of the new questions themselves?

11:20.480 --> 11:21.480
How do you decide what to think about?

11:21.480 --> 11:24.480
These are all key activities of human intelligence

11:24.480 --> 11:27.480
when we talk about how we model the world, where our models come from,

11:27.480 --> 11:29.480
what we do with our models, this is what we're talking about.

11:29.480 --> 11:31.480
And if we could get machines that could do these things,

11:31.480 --> 11:36.480
well, again, on the bottom row, think of all the actual real engineering payoffs.

11:36.480 --> 11:40.480
Now, in our center, in both my own activities and a lot of what my group does these days

11:40.480 --> 11:44.480
and what a number of other colleagues in the center for brains, minds, and machines do,

11:44.480 --> 11:47.480
as well as very broadly people in BCS and CSAIL,

11:47.480 --> 11:50.480
one place where we work on the beginnings of these problems in the near term.

11:50.480 --> 11:53.480
This is the long term, like, think 50 years, okay?

11:53.480 --> 11:59.480
Maybe shorter, maybe longer, I don't know, but think well beyond 10 years, okay?

11:59.480 --> 12:03.480
But in the short term, 5 to 10 years, a lot of our focus is around visual intelligence.

12:03.480 --> 12:04.480
And there's many reasons for that.

12:04.480 --> 12:07.480
Again, we can build on the successes of deep networks

12:07.480 --> 12:09.480
and a lot of pattern recognition and machine vision.

12:09.480 --> 12:11.480
It's a good way to put these ideas into practice.

12:11.480 --> 12:15.480
When we look at the actual brain, the visual system in the brain,

12:15.480 --> 12:18.480
in the human and other mammalian brains, for example,

12:18.480 --> 12:21.480
is really very clearly the best understood part of the brain.

12:21.480 --> 12:26.480
And at a circuit level, it's the part of the brain that's most inspired current deep learning

12:26.480 --> 12:27.480
and neural network systems.

12:27.480 --> 12:32.480
But even there, there's things which we still don't really understand like engineers.

12:32.480 --> 12:35.480
So here's an example of a basic problem in visual intelligence

12:35.480 --> 12:39.480
that we and others in the center are trying to solve.

12:39.480 --> 12:44.480
Look around you and you feel like there's a whole world around you.

12:44.480 --> 12:45.480
And there is a whole world around you.

12:45.480 --> 12:47.480
You feel like your brain captures it.

12:47.480 --> 12:50.480
But what the actual sense data that's coming in through your eyes

12:50.480 --> 12:53.480
looks more like this photograph here where you can see there's a crowd scene

12:53.480 --> 12:57.480
but it's mostly blurry except for a small region of high resolution in the center.

12:57.480 --> 13:01.480
So that corresponds biologically to what part of the image is in your fovea.

13:01.480 --> 13:03.480
That's the central region of cells in the retina

13:03.480 --> 13:06.480
where you have really high resolution visual data.

13:06.480 --> 13:10.480
The size of your fovea is roughly like if you hold out your thumb at arm's length.

13:10.480 --> 13:12.480
It's a little bit bigger than that but not much bigger, right?

13:12.480 --> 13:16.480
Most of the image, in terms of the actual information coming in

13:16.480 --> 13:19.480
in a bottom-up sense to your brain is really quite blurry.

13:19.480 --> 13:23.480
But somehow by looking at just one part and then by saccading around

13:23.480 --> 13:26.480
or making a few eye movements, you get a few glimpses,

13:26.480 --> 13:29.480
each not much bigger than the size of your thumb at arm's length.

13:29.480 --> 13:32.480
Somehow you stitch that information together into what feels like

13:32.480 --> 13:35.480
and really is a rich representation of the whole world around you.

13:35.480 --> 13:38.480
And when I say around you, I mean literally around you.

13:38.480 --> 13:40.480
So here's another kind of demonstration.

13:40.480 --> 13:44.480
Without turning around, nobody's allowed to turn around.

13:44.480 --> 13:46.480
Ask yourself what's behind you.

13:46.480 --> 13:48.480
Now the answer is going to be different for different people

13:48.480 --> 13:50.480
depending on where you're sitting, right?

13:50.480 --> 13:53.480
For most of you, you might think, well, I think there's a person

13:53.480 --> 13:55.480
pretty close behind me, right?

13:55.480 --> 13:57.480
You know you're in a crowded auditorium, although you haven't seen that person.

13:57.480 --> 14:00.480
You know that they're there, right?

14:00.480 --> 14:03.480
For people in the very back row, you know there isn't a person behind you

14:03.480 --> 14:05.480
and you're conscious of being in the back row, right?

14:05.480 --> 14:07.480
You might be conscious that there's a wall right behind you.

14:07.480 --> 14:11.480
But now for the people who are in the room, not in the very back,

14:11.480 --> 14:13.480
think about how far behind you is the back,

14:13.480 --> 14:15.480
like where's the nearest wall behind you?

14:15.480 --> 14:18.480
Maybe we can call out, try a little demonstration.

14:18.480 --> 14:20.480
So I don't know. I'm pointing to someone there.

14:20.480 --> 14:23.480
Can you say something if you think I'm pointing at you?

14:23.480 --> 14:25.480
Well, I could have been pointing at you,

14:25.480 --> 14:27.480
but I'm pointing someone behind you, okay?

14:27.480 --> 14:29.480
I'll point to you. Yeah, I'm pointing to you.

14:29.480 --> 14:31.480
All right, so how far is the nearest wall...

14:31.480 --> 14:33.480
No, you can't turn around. You've blown your chance.

14:33.480 --> 14:36.480
Without turning around. Okay, so you were...

14:36.480 --> 14:38.480
Okay, do you see I'm pointing to you there with the tie?

14:38.480 --> 14:42.480
Okay, so without turning around, how far is the nearest wall behind you?

14:46.480 --> 14:48.480
Sorry, how far?

14:48.480 --> 14:51.480
Five meters. Okay, well, I mean, that might be about right.

14:51.480 --> 14:54.480
Other people can turn around.

14:54.480 --> 14:57.480
Now, how about you? How far is the nearest wall behind you?

14:57.480 --> 14:59.480
Ten meters.

14:59.480 --> 15:01.480
Ten meters, okay.

15:01.480 --> 15:03.480
That might be right, yeah.

15:03.480 --> 15:06.480
How about here? What do you think?

15:06.480 --> 15:08.480
20, okay.

15:08.480 --> 15:10.480
So yeah, since I didn't grow up in the metric system, I barely know.

15:10.480 --> 15:12.480
But yeah, I mean...

15:12.480 --> 15:14.480
The point is that like, you're...

15:14.480 --> 15:17.480
Each of you is surely not exactly right,

15:17.480 --> 15:19.480
but you're certainly within an order of magnitude,

15:19.480 --> 15:22.480
and I guess if we actually tried to measure, you know, you're probably...

15:22.480 --> 15:25.480
My guess is you're probably right within, you know, 50% or less,

15:25.480 --> 15:28.480
often, you know, maybe just 20% error.

15:28.480 --> 15:30.480
Okay, so how do you know this?

15:30.480 --> 15:32.480
I mean, even if it's not... What did you say, 20 meters?

15:32.480 --> 15:35.480
Even if it's not 20 meters, it's probably closer to 20 meters

15:35.480 --> 15:38.480
than it is to 5 or 10 meters, and then it is to 50 meters.

15:38.480 --> 15:41.480
So how did you know this? You haven't turned around in a while, right?

15:41.480 --> 15:45.480
So how do you know if your brain is tracking the whole world around you, right?

15:45.480 --> 15:48.480
And how many people are behind you?

15:48.480 --> 15:50.480
Yeah, like a few hundred, right?

15:50.480 --> 15:52.480
I mean, I don't know if it's 200 or 300 or...

15:52.480 --> 15:54.480
But it's not a thousand.

15:54.480 --> 15:56.480
I don't think so.

15:56.480 --> 15:58.480
And it's certainly not 10 or 20 or 50, right?

15:58.480 --> 16:02.480
So you track these things, and you use them to plan your actions.

16:02.480 --> 16:05.480
Okay, so again, think about how instantly, effortlessly,

16:05.480 --> 16:07.480
and very reliably, okay?

16:07.480 --> 16:09.480
Your brain computes all these things.

16:09.480 --> 16:11.480
There's a lot of people and objects around you,

16:11.480 --> 16:13.480
and it's not just approximations.

16:13.480 --> 16:16.480
Certainly, when we're talking about what's behind you in space,

16:16.480 --> 16:18.480
there's a lot of imprecision.

16:18.480 --> 16:20.480
But when it comes to reaching for things right in front of you,

16:20.480 --> 16:23.480
very precise shape and physical property estimates

16:23.480 --> 16:25.480
needed to pick up and manipulate objects.

16:25.480 --> 16:28.480
And then when it comes to people, it's not just the existence of the people,

16:28.480 --> 16:30.480
but something about what's in their head, right?

16:30.480 --> 16:32.480
You track whether someone's paying attention to you

16:32.480 --> 16:34.480
and you're talking to them, what they might want from you,

16:34.480 --> 16:36.480
what they might be thinking about you,

16:36.480 --> 16:38.480
what they might be thinking about other people, okay?

16:38.480 --> 16:40.480
So when we talk about visual intelligence,

16:40.480 --> 16:42.480
this is the whole stuff we're talking about.

16:42.480 --> 16:45.480
And you can start to see how it turns into basic questions, I think,

16:45.480 --> 16:49.480
of what we might call the beginnings of consciousness,

16:49.480 --> 16:52.480
or at least our awareness of our self in the world,

16:52.480 --> 16:55.480
and of ourselves as a self in the world,

16:55.480 --> 16:58.480
but also other aspects of higher level intelligence and cognition

16:58.480 --> 17:00.480
that are not just about perception, like symbols, right?

17:00.480 --> 17:04.480
To describe even to ourselves what's around us and where we are

17:04.480 --> 17:06.480
and what we can do with it.

17:06.480 --> 17:10.480
We can go beyond just what we would normally call the stuff of perception

17:10.480 --> 17:14.480
to say the thoughts in somebody's head and your own thoughts about that, okay?

17:14.480 --> 17:16.480
So what we've been doing in CBMM

17:16.480 --> 17:19.480
is trying to develop an architecture for visual intelligence.

17:19.480 --> 17:22.480
And I'm not going to go into any of the details of how this works,

17:22.480 --> 17:24.480
and this is just notional, this is just a picture,

17:24.480 --> 17:27.480
it's like a sketch from a grand proposal of what we say we want to do.

17:27.480 --> 17:30.480
But it's based on a lot of scientific understanding

17:30.480 --> 17:32.480
of how the brain works.

17:32.480 --> 17:35.480
There are different parts of the brain that correspond to these different modules

17:35.480 --> 17:38.480
of architecture, as well as some kind of emerging engineering way

17:38.480 --> 17:41.480
to try to capture at the software and maybe even hardware levels

17:41.480 --> 17:43.480
how these modules might work.

17:43.480 --> 17:46.480
So we talk about sort of an early module of visual or perceptual stream,

17:46.480 --> 17:50.480
which is like bottom-up visual or other perceptual input.

17:50.480 --> 17:53.480
That's the kind of thing that is pretty close to what we currently have

17:53.480 --> 17:55.480
in say deep convolutional neural networks.

17:55.480 --> 18:00.480
But then we talk about some kind of, the output of that isn't just pattern class labels,

18:00.480 --> 18:03.480
but what we call the cognitive core or core cognition.

18:03.480 --> 18:06.480
And an understanding of space and objects, their physics,

18:06.480 --> 18:09.480
other people, their minds, that's the real stuff of cognition

18:09.480 --> 18:12.480
that has to be the output of perception.

18:12.480 --> 18:16.480
But somehow we have to have, this is what we call the brain OS in this picture,

18:16.480 --> 18:19.480
we have to get there by stitching together the bottom-up inputs

18:19.480 --> 18:22.480
from a glimpse here, a glimpse here, a little bit here and there,

18:22.480 --> 18:25.480
and accessing prior knowledge that comes from our memory systems

18:25.480 --> 18:28.480
to tell us how to stitch these things together

18:28.480 --> 18:32.480
into the really core cognitive representations of what's out there in the world.

18:32.480 --> 18:35.480
And then if we're going to start to talk about it in language

18:35.480 --> 18:40.480
or to build plans on top of what we have seen and understood,

18:40.480 --> 18:44.480
that's where we talk about symbols coming into the picture.

18:44.480 --> 18:48.480
The building blocks of language and plans and so on.

18:48.480 --> 18:51.480
So now we might say, well, okay, this is an architecture

18:51.480 --> 18:54.480
that is brain-inspired and cognitively inspired

18:54.480 --> 18:57.480
and we're planning to turn into real engineering.

18:57.480 --> 18:59.480
And you can say, well, do we need that?

18:59.480 --> 19:02.480
Again, I know this is a question you considered in the first lecture.

19:02.480 --> 19:06.480
Maybe the engineering toolkit that's currently been making a lot of progress

19:06.480 --> 19:08.480
in, let's say, industry, maybe that's good enough.

19:08.480 --> 19:10.480
Maybe, you know, let's take deep learning

19:10.480 --> 19:14.480
but to stand for a broader set of modern pattern recognition-based

19:14.480 --> 19:16.480
and reinforcement learning-based tools

19:16.480 --> 19:20.480
and say, okay, well, maybe that can scale up to this.

19:20.480 --> 19:22.480
And you might, you know, maybe that's possible.

19:22.480 --> 19:24.480
I'm happy in the question period if people want to debate this.

19:24.480 --> 19:26.480
My sense is, no.

19:26.480 --> 19:29.480
I think that it's not, when I say no,

19:29.480 --> 19:32.480
I don't mean like it can't happen or it won't happen.

19:32.480 --> 19:36.480
What I mean is the highest value, the highest expected route right now

19:36.480 --> 19:39.480
is to take this more science-based reverse engineering approach.

19:39.480 --> 19:42.480
And that at least if you follow the current trajectory

19:42.480 --> 19:45.480
that industry incentives especially optimize for,

19:45.480 --> 19:48.480
it's not even really trying to take us to these things.

19:48.480 --> 19:51.480
So think about, for example, a case study of visual intelligence

19:51.480 --> 19:54.480
that is in some ways, as pattern recognition, very much of a success.

19:54.480 --> 19:56.480
It's again been mostly driven by industry.

19:56.480 --> 19:59.480
It's something that if you read in the news

19:59.480 --> 20:02.480
or even play around with in certain publicly available data sets

20:02.480 --> 20:04.480
feels like we've made great progress.

20:04.480 --> 20:06.480
And this is an aspect of visual intelligence

20:06.480 --> 20:09.480
which is sometimes called image captioning.

20:09.480 --> 20:12.480
It's mapping images to text.

20:12.480 --> 20:14.480
You know, basically there's been a bunch of systems.

20:14.480 --> 20:16.480
Here's a couple of press releases.

20:16.480 --> 20:18.480
I guess this one's about Google.

20:18.480 --> 20:21.480
Google's AI can now capture images almost as well as humans.

20:21.480 --> 20:23.480
Here's one's about Microsoft.

20:23.480 --> 20:26.480
A couple of years ago, I think there were something like

20:26.480 --> 20:29.480
eight papers all released on to archive around the same time

20:29.480 --> 20:32.480
from basically all the major industry computer vision groups

20:32.480 --> 20:35.480
as well as a couple of academic partners, okay,

20:35.480 --> 20:38.480
which all driven by basically the same data set

20:38.480 --> 20:41.480
produced by some Microsoft researchers and other collaborators,

20:41.480 --> 20:44.480
trained a combination of deep convolutional neural networks,

20:44.480 --> 20:47.480
you know, state-of-the-art visual pattern recognition

20:47.480 --> 20:50.480
with recurrent neural networks which had recently been developed

20:50.480 --> 20:53.480
for, you know, basically kinds of neural statistical language modeling,

20:53.480 --> 20:55.480
glued them together and produced a system

20:55.480 --> 20:59.480
which made very impressive results in a big training set

20:59.480 --> 21:02.480
and a held out test set where the goal was to take an image

21:02.480 --> 21:05.480
and write a sentence like a short sentence caption

21:05.480 --> 21:09.480
that would seem like the kind of way a human would describe that image.

21:09.480 --> 21:12.480
And these systems, you know, surpassed human level accuracy

21:12.480 --> 21:15.480
on the held out test set from a big training set.

21:15.480 --> 21:17.480
But what you can see when you really dig into these things

21:17.480 --> 21:20.480
is there's often a lot of what I would call data set overfitting.

21:20.480 --> 21:22.480
It's not overfitting to the training set,

21:22.480 --> 21:25.480
but it's overfitting to whatever are the particular characteristics

21:25.480 --> 21:28.480
of this data set, you know, wherever it came from,

21:28.480 --> 21:31.480
certain set of photographs and certain ways of captioning them, okay,

21:31.480 --> 21:34.480
which even a big data set, it's not about quantity,

21:34.480 --> 21:38.480
it's more about the quality, the nature of what people are doing, all right.

21:38.480 --> 21:41.480
So one way to test this system is to apply it

21:41.480 --> 21:44.480
to what seems like basically the same problem

21:44.480 --> 21:48.480
but not within a certain curated or built data set.

21:48.480 --> 21:51.480
And there's a convenient Twitter bot that lets you do this.

21:51.480 --> 21:53.480
So there's something called the PIC desk bot,

21:53.480 --> 21:57.480
which takes one of the state of the art industry AI captioning systems,

21:57.480 --> 21:59.480
a very good one. Again, this is not meant to,

21:59.480 --> 22:02.480
I'm not trying to critique these systems for what they're trying to do,

22:02.480 --> 22:04.480
I'm just trying to point out what they don't really even try to do.

22:04.480 --> 22:07.480
So this takes the Microsoft caption bot

22:07.480 --> 22:10.480
and just every couple of hours takes a random image from the web,

22:10.480 --> 22:13.480
captions it and uploads the results to Twitter.

22:13.480 --> 22:16.480
And a couple of months ago when I prepared the first version of this talk,

22:16.480 --> 22:19.480
I just took a few days in the life of this Twitter bot.

22:19.480 --> 22:22.480
I didn't take every single image, but I took, you know,

22:22.480 --> 22:25.480
most of the images in a way that was meant to be representative of the successes

22:25.480 --> 22:28.480
and the kinds of failures that such a system will make.

22:28.480 --> 22:30.480
So we can go through this and it's a little bit entertaining

22:30.480 --> 22:32.480
and I think quite informative.

22:32.480 --> 22:37.480
So here's just a somewhat random sample of a few days in the life

22:37.480 --> 22:39.480
of one of these caption bots.

22:39.480 --> 22:42.480
So here we have a picture of a person holding,

22:42.480 --> 22:44.480
fortunately my screen is very small here and I can't read up there,

22:44.480 --> 22:46.480
so maybe you'll have to tell me what it is.

22:46.480 --> 22:48.480
But a person holding a cell phone, I guess I'll just read along with you.

22:48.480 --> 22:50.480
So you have a person holding a cell phone.

22:50.480 --> 22:52.480
Well, it's not a person holding a cell phone, but it's kind of close.

22:52.480 --> 22:54.480
It's a person holding some kind of machine.

22:54.480 --> 22:58.480
I don't even know what that is, but it's some kind of musical instrument, right?

22:58.480 --> 23:01.480
So that's a mixed success or failure.

23:01.480 --> 23:05.480
Here's a pretty good one, a group of people on a field playing football.

23:05.480 --> 23:09.480
I would call that an A result, maybe even A plus.

23:09.480 --> 23:12.480
Here's a group of people standing on top of a mountain.

23:12.480 --> 23:15.480
So less good, there's a mountain, but as far as I can tell there's no people.

23:15.480 --> 23:18.480
But these systems like to see people because of both the combination,

23:18.480 --> 23:20.480
because in the data set they were trained on there's a lot of people

23:20.480 --> 23:22.480
and people often talk about people.

23:22.480 --> 23:27.480
And the fact that you can appreciate both what I said and why it's funny,

23:27.480 --> 23:32.480
there you did some of my cognitive activities that this system is not even trying to do.

23:32.480 --> 23:34.480
Here we've got a building with a cake.

23:34.480 --> 23:35.480
I'll go through these fast.

23:35.480 --> 23:38.480
Building with a cake, a large stone building with a clock tower.

23:38.480 --> 23:39.480
I think that's pretty good.

23:39.480 --> 23:40.480
I'd give that like a B plus.

23:40.480 --> 23:42.480
There's no clock, but it's plausibly right.

23:42.480 --> 23:44.480
There might be a clock in there.

23:44.480 --> 23:45.480
There's definitely something like that.

23:45.480 --> 23:47.480
Here's a truck parked on the side of a building.

23:47.480 --> 23:49.480
I don't know, maybe a B minus.

23:49.480 --> 23:52.480
There is a car on the side of a building, but it's not a truck

23:52.480 --> 23:55.480
and it doesn't seem like the main thing in the image.

23:55.480 --> 23:57.480
Here's a necklace made of bananas.

23:57.480 --> 24:01.480
Here's a large ship in the water.

24:01.480 --> 24:02.480
This is pretty good.

24:02.480 --> 24:05.480
I give this like an A minus or B plus because there is a ship in the water,

24:05.480 --> 24:06.480
but it's not very large.

24:06.480 --> 24:08.480
It's really more of like a tugboat or something.

24:08.480 --> 24:10.480
Here's a sign sitting on the grass.

24:10.480 --> 24:11.480
You know, in some sense, that's great.

24:11.480 --> 24:14.480
No, but in another sense, it's really missing what's actually

24:14.480 --> 24:17.480
interesting and important and meaningful to humans.

24:17.480 --> 24:22.480
Here's a garden is in the dirt.

24:22.480 --> 24:24.480
A pizza sitting on top of a building.

24:24.480 --> 24:26.480
A small house with a red brick building.

24:26.480 --> 24:28.480
That's pretty good, although a kind of weird way of saying it.

24:28.480 --> 24:30.480
A vintage photo of a pond.

24:30.480 --> 24:31.480
That's good.

24:31.480 --> 24:32.480
They like vintage photos.

24:32.480 --> 24:34.480
A group of people that are standing in the grass near a bridge.

24:34.480 --> 24:36.480
Again, there's two people and there's some grass and there's a bridge,

24:36.480 --> 24:39.480
but it's really not what's going on.

24:39.480 --> 24:42.480
A person in a yard, kind of.

24:42.480 --> 24:44.480
A group of people standing on top of a boat.

24:44.480 --> 24:45.480
There's a boat.

24:45.480 --> 24:46.480
There's a group of people.

24:46.480 --> 24:47.480
They're standing.

24:47.480 --> 24:51.480
But again, the sentence that you see is more based on a bias of what people

24:51.480 --> 24:55.480
have said in the past about images that are only vaguely like this.

24:55.480 --> 24:56.480
A clock tower is a little bit night.

24:56.480 --> 24:58.480
That's really, I think, pretty impressive.

24:58.480 --> 25:00.480
A large clock mounted to the side of a building.

25:00.480 --> 25:01.480
A little bit less so.

25:01.480 --> 25:03.480
A snow-covered feel, very good.

25:03.480 --> 25:05.480
A building with snow on the ground.

25:05.480 --> 25:06.480
A little bit less good.

25:06.480 --> 25:07.480
There's no snow.

25:07.480 --> 25:08.480
It's white.

25:08.480 --> 25:11.480
Some people who I don't know them, but I bet that's probably right

25:11.480 --> 25:14.480
because identifying faces and recognizing people who are famous

25:14.480 --> 25:16.480
because they won medals in the Olympics,

25:16.480 --> 25:20.480
probably I would trust current pattern recognition systems to get that.

25:20.480 --> 25:22.480
A painting of a vase in front of a mirror.

25:22.480 --> 25:23.480
Less good.

25:23.480 --> 25:26.480
Also a famous person there, but we didn't get him.

25:26.480 --> 25:28.480
A person walking in the rain.

25:28.480 --> 25:31.480
Again, there is sort of a person and there's some puddles.

25:31.480 --> 25:35.480
A group of stuffed animals.

25:35.480 --> 25:37.480
A car parked in a parking lot.

25:37.480 --> 25:38.480
That's good.

25:38.480 --> 25:40.480
A car parked in front of a building.

25:40.480 --> 25:41.480
Less good.

25:41.480 --> 25:43.480
A plate with a fork and knife.

25:43.480 --> 25:44.480
A clear blue sky.

25:44.480 --> 25:45.480
Okay.

25:45.480 --> 25:46.480
So you get the idea.

25:46.480 --> 25:49.480
Again, if you actually go and play with this system,

25:49.480 --> 25:53.480
partly because I think my friends at Microsoft told me they've improved it some.

25:53.480 --> 25:56.480
This is partly for entertainment values.

25:56.480 --> 25:58.480
I chose what also would be the funnier examples.

25:58.480 --> 26:00.480
So I want to be quite honest about this.

26:00.480 --> 26:04.480
I'm not trying to take away what are impressive AI technologies,

26:04.480 --> 26:09.480
but I think it's clear that there's a sense of understanding any one of these images,

26:09.480 --> 26:12.480
that it's important to see that even when it seems to be correct,

26:12.480 --> 26:15.480
if it can make the kind of errors that it makes,

26:15.480 --> 26:17.480
that even when it seems to be correct,

26:17.480 --> 26:19.480
it's probably not doing what you're doing

26:19.480 --> 26:22.480
and it's probably not even trying to scale towards the dimensions of intelligence

26:22.480 --> 26:25.480
that we think about when we're talking about human intelligence.

26:25.480 --> 26:27.480
Another way to put this,

26:27.480 --> 26:30.480
I want to show you a really insightful blog post from one of your other speakers.

26:30.480 --> 26:32.480
So in a couple of days,

26:32.480 --> 26:34.480
I'm not sure you're going to have Andre Carpathi,

26:34.480 --> 26:37.480
who's one of the leading people in deep learning.

26:37.480 --> 26:41.480
This is a really great blog post he wrote a couple of years ago,

26:41.480 --> 26:43.480
when he was, I think, still at Stanford.

26:43.480 --> 26:45.480
He got his PhD from Stanford.

26:45.480 --> 26:50.480
He worked at Google a little bit on some early big neural net AI projects there.

26:50.480 --> 26:52.480
He was at OpenAI.

26:52.480 --> 26:54.480
He was one of the founders of OpenAI,

26:54.480 --> 26:58.480
and recently he joined Tesla as their director of AI research.

26:58.480 --> 27:00.480
But about five years ago,

27:00.480 --> 27:04.480
he was looking at the state of computer vision from a human intelligence point of view

27:04.480 --> 27:06.480
and lamenting how far away we were.

27:06.480 --> 27:08.480
So this is the title of his blog post,

27:08.480 --> 27:10.480
the state of computer vision and AI.

27:10.480 --> 27:12.480
We are really, really far away.

27:12.480 --> 27:16.480
And he took this image, which was a sort of a famous image in its own right.

27:16.480 --> 27:19.480
It was a popular image of Obama back when he was president,

27:19.480 --> 27:22.480
kind of playing around as he liked to do when he was on tour.

27:22.480 --> 27:24.480
If you take a look at this, you can see,

27:24.480 --> 27:27.480
you probably all can recognize the previous president of the United States,

27:27.480 --> 27:30.480
but you can also get the sense of where he is and what's going on,

27:30.480 --> 27:32.480
and you might see people smiling,

27:32.480 --> 27:34.480
and you might get the sense that he's playing a joke on someone.

27:34.480 --> 27:36.480
Can you see that?

27:36.480 --> 27:39.480
So how do you know that he's playing a joke and what that joke is?

27:39.480 --> 27:42.480
Well, as Andre goes on to talk about in his blog post,

27:42.480 --> 27:47.480
if you think about all the things that you have to really deploy in your mind to understand that,

27:47.480 --> 27:49.480
it's a huge list.

27:49.480 --> 27:53.480
Of course, it starts with seeing people and objects and maybe doing some face recognition,

27:53.480 --> 27:55.480
but you have to do things like, for example,

27:55.480 --> 27:59.480
notice his foot on the scale and understand enough about how scales work,

27:59.480 --> 28:01.480
that when a foot presses down, it exerts force,

28:01.480 --> 28:04.480
that the scale is sensitive, doesn't just magically measure people's weight,

28:04.480 --> 28:06.480
but it does that somehow through force.

28:06.480 --> 28:09.480
You have to see who can see that he's doing that and who can't,

28:09.480 --> 28:11.480
who cannot see that he's doing that, right,

28:11.480 --> 28:13.480
in particular the person on the scale,

28:13.480 --> 28:15.480
and why some people can see that he's doing that

28:15.480 --> 28:17.480
and can see that some other people can't see it,

28:17.480 --> 28:19.480
and that makes it funny to them.

28:19.480 --> 28:23.480
And someday, we should have machines that can understand this,

28:23.480 --> 28:29.480
but hopefully you can see why the kind of architecture that I'm talking about

28:29.480 --> 28:34.480
would be the building blocks of the ingredients to be able to get them to do that.

28:34.480 --> 28:37.480
Now, again, I prepared a version of this talk a few months ago

28:37.480 --> 28:40.480
and I wrote to Andre and I said, I was going to use this

28:40.480 --> 28:44.480
and I was curious if he had any reflections on this

28:44.480 --> 28:47.480
and where he thought we were relative to five years ago,

28:47.480 --> 28:49.480
because certainly a lot of progress has been made.

28:49.480 --> 28:53.480
But he said, here's his email, I hope he doesn't mind me sharing it,

28:53.480 --> 28:55.480
but I mean, again, he's a very honest person

28:55.480 --> 28:58.480
and that's one of the many reasons why he's such an important person right now in AI.

28:58.480 --> 29:01.480
He's both very technically strong and honest about what we can do what we can't do,

29:01.480 --> 29:03.480
and as he says, what does he say?

29:03.480 --> 29:06.480
It's nice to hear from you, it's fun you should bring this up.

29:06.480 --> 29:09.480
I was also thinking about writing a return to this

29:09.480 --> 29:12.480
and in short, basically I don't believe we've made very much progress, right?

29:12.480 --> 29:16.480
He points out that in his long list of things that you need to understand the image,

29:16.480 --> 29:18.480
we have made progress on some, the ability to, again,

29:18.480 --> 29:22.480
detect people and do face recognition for well-known individuals, okay?

29:22.480 --> 29:24.480
But that's kind of about it, all right?

29:24.480 --> 29:28.480
And he wasn't particularly optimistic that the current route that's being pursued in industry

29:28.480 --> 29:34.480
is anywhere close to solving or even really trying to solve these larger questions.

29:34.480 --> 29:38.480
If we give this image to that caption bot,

29:38.480 --> 29:41.480
what we see is, again, represents the same point.

29:41.480 --> 29:43.480
Here's the caption bot, it says,

29:43.480 --> 29:46.480
I think it's a group of people standing next to a man in a suit and tie, right?

29:46.480 --> 29:48.480
So that's right, right?

29:48.480 --> 29:51.480
As far as it goes, it just doesn't go far enough,

29:51.480 --> 29:54.480
and the current ideas of build a data set,

29:54.480 --> 29:58.480
train a deep learning algorithm on it, and then repeat,

29:58.480 --> 30:03.480
aren't really even, I would venture, trying to get to what we're talking about.

30:03.480 --> 30:06.480
Or here's another, I'll just give you one other example of a couple of photographs

30:06.480 --> 30:10.480
from my recent vacation in a nice warm tropical locale,

30:10.480 --> 30:14.480
which I think illustrate ways in which, again, the gap where we have machines

30:14.480 --> 30:18.480
that can, say, beat the world's best at go,

30:18.480 --> 30:21.480
but can't even beat a child at tic-tac-toe.

30:21.480 --> 30:23.480
Now, what do I mean by that?

30:23.480 --> 30:26.480
Well, of course, we don't even need reinforcement learning or deep learning

30:26.480 --> 30:30.480
to build a machine that can win or tie, do optimally in tic-tac-toe.

30:30.480 --> 30:33.480
But think about this, this is a real tic-tac-toe game,

30:33.480 --> 30:36.480
which I saw on the grass outside my hotel, right?

30:36.480 --> 30:39.480
What do you have to do to look at this and recognize that it's a tic-tac-toe game?

30:39.480 --> 30:42.480
You have to see the objects, you have to see what's, you know, in some sense,

30:42.480 --> 30:46.480
there's a three-by-three grid, but it's only abstract, right?

30:46.480 --> 30:50.480
It's only delimited by these ropes or strings, okay?

30:50.480 --> 30:54.480
It's not actually a grid in any simple geometric sense, all right?

30:54.480 --> 30:58.480
But yet a child can look at that, and indeed, here's an actual child who was looking at it

30:58.480 --> 31:01.480
and recognize, oh, it's a game of tic-tac-toe, and even know what they need to do to win,

31:01.480 --> 31:04.480
namely put the X and complete it, and now they've got three in a row, right?

31:04.480 --> 31:07.480
That's literally child's play, okay?

31:07.480 --> 31:11.480
You show this sort of thing, though, to one of these, you know, image-understanding caption bots,

31:11.480 --> 31:14.480
and I think it's a close-up of a sign, okay?

31:14.480 --> 31:22.480
Again, saying that this is a close-up of a sign is not the same thing, I would venture,

31:22.480 --> 31:27.480
as a cognitive or computational activity that's going to give us what we need to say,

31:27.480 --> 31:30.480
recognize the objects, recognize it as a game to understand the goal

31:30.480 --> 31:32.480
and how to plan to achieve those goals.

31:32.480 --> 31:36.480
Whereas this kind of architecture is designed to try to do all of these things, ultimately, right?

31:36.480 --> 31:44.480
And I bring in these examples of games or jokes to really show where perception goes to cognition,

31:44.480 --> 31:47.480
you know, all the way up to symbols, right?

31:47.480 --> 31:52.480
So to get objects and forces and mental states, that's the cognitive core,

31:52.480 --> 31:58.480
but to be able to get goals and plans and what do I do or how do I talk about it, that symbols, okay?

31:58.480 --> 32:04.480
Here's another way into this, and it's one that also motivates, I think, a lot of really good work on the engineering side

32:04.480 --> 32:08.480
and a lot of our interest in the science side is think about robotics

32:08.480 --> 32:14.480
and think about what do you have to do to, you know, what does the brain have to be like to control the body?

32:14.480 --> 32:18.480
So again, you're going to hear from, shortly, I think maybe it's next week, from Mark Rayburt,

32:18.480 --> 32:24.480
who's one of the founders of Boston Dynamics, which is one of my favorite companies anywhere.

32:24.480 --> 32:31.480
They're, without doubt, the leading maker of humanoid robots, legged, locomoting robots in industry.

32:31.480 --> 32:37.480
They have all sorts of other really cool robots, robots like dogs, robots that have all, you know,

32:37.480 --> 32:40.480
I think you'll even get to see a live demonstration of one of these robots.

32:40.480 --> 32:43.480
It's really awesome, impressive stuff, okay?

32:43.480 --> 32:46.480
But what about the minds and brains of these robots?

32:46.480 --> 32:51.480
Well, again, if you ask Mark, ask them how much of human-like cognition do they have in their robots,

32:51.480 --> 32:54.480
and I think he would say very little.

32:54.480 --> 32:56.480
In fact, we have asked him that, and he would say very little.

32:56.480 --> 32:58.480
He has said very little.

32:58.480 --> 33:03.480
He's actually one of the advisors of our center, and I think in many ways we're very much on the same page.

33:03.480 --> 33:10.480
We both want to know how do you build the kind of intelligence that can control these bodies like the way a human does?

33:10.480 --> 33:13.480
All right, here's another example of an industry robotics effort.

33:13.480 --> 33:16.480
This is Google's arm farm, where, you know, they've got lots of robot arms,

33:16.480 --> 33:21.480
and they're trying to train them to pick up objects using various kinds of deep learning and reinforcement learning techniques.

33:21.480 --> 33:23.480
And I think it's one approach.

33:23.480 --> 33:29.480
I just think it's very, very different from the way humans learn to, say, control their body and manipulate objects.

33:29.480 --> 33:33.480
And you can see that in terms of things that go back to what you were saying when you were introducing me, right?

33:33.480 --> 33:35.480
Think about how quickly we learn things, right?

33:35.480 --> 33:40.480
Here you have these, the arm farm is trying to generate, you know, effectively, maybe if not infinite,

33:40.480 --> 33:46.480
but hundreds of thousands, millions of examples of reaches and pickups of objects, even with just a single gripper.

33:46.480 --> 33:53.480
And yet a child who in some ways can't control their body nearly as well as robots can be controlled at the low level

33:53.480 --> 33:55.480
is able to do so much more.

33:55.480 --> 34:01.480
So I'll show you two of my favorite videos from YouTube here, which motivate some of the research that we're doing.

34:01.480 --> 34:04.480
The one on the left is a one-and-a-half-year-old and the other one's a one-year-old.

34:04.480 --> 34:09.480
So just watch this one-and-a-half-year-old here doing a popular activity for many kids.

34:09.480 --> 34:11.480
Is it playing?

34:11.480 --> 34:13.480
Hmm.

34:15.480 --> 34:17.480
You see video up there?

34:18.480 --> 34:19.480
Okay, there we go.

34:19.480 --> 34:23.480
Okay, so he's doing this stacking cup activity.

34:23.480 --> 34:27.480
All right, he's stacking up cups to make a tall tower.

34:27.480 --> 34:28.480
He's got a stack of three.

34:28.480 --> 34:33.480
And what you can see for the first part of this video is it looks like he's trying to make a second stack

34:33.480 --> 34:35.480
that he's trying to pick up at once.

34:35.480 --> 34:39.480
Basically, he's trying to make a stack of two that'll go on the stack of three.

34:39.480 --> 34:44.480
And, you know, he's trying to debug his plan because it got a little bit stuck here.

34:44.480 --> 34:49.480
But, and think about, I mean, again, if you know anything about robots manipulating objects,

34:49.480 --> 34:53.480
even just what he just did, no robot can decide to do that and actually do it, right?

34:53.480 --> 34:55.480
At some point, he's almost got it.

34:55.480 --> 34:59.480
It's a little bit tricky, but at some point he's going to get that stack of two.

34:59.480 --> 35:02.480
He realizes he has to move that object out of the way.

35:02.480 --> 35:03.480
Look at what he just did.

35:03.480 --> 35:05.480
Move it out of the way, use two hands to pick it up.

35:05.480 --> 35:09.480
And now he's got a stack of two on a stack of three, and suddenly, you know, sub-goal completed.

35:09.480 --> 35:10.480
He's now got a stack of five.

35:10.480 --> 35:16.480
And he gives himself a hand because he knows he accomplished a key waypoint along the way to his final goal.

35:16.480 --> 35:19.480
That's a kind of early symbolic cognition, right?

35:19.480 --> 35:24.480
To understand that I'm trying to build a tall tower, but a tower is made up of little towers.

35:24.480 --> 35:28.480
And you can take a tower and put it on top of another tower or stack a stack on a stack,

35:28.480 --> 35:30.480
and you have a bigger stack, right?

35:30.480 --> 35:34.480
So think about how he goes from bottom-up perception to the objects that the physics needed

35:34.480 --> 35:39.480
to manipulate the objects to the ability to make even those early kinds of symbolic plans.

35:39.480 --> 35:40.480
At some point, he keeps doing this.

35:40.480 --> 35:43.480
He puts another stack on there.

35:43.480 --> 35:44.480
I'll just jump to the end.

35:44.480 --> 35:45.480
Oops, sorry.

35:45.480 --> 35:46.480
You missed.

35:46.480 --> 35:47.480
Sorry.

35:47.480 --> 35:51.480
He gets really excited and he gives himself another big hand, but falls over.

35:51.480 --> 35:52.480
Okay.

35:52.480 --> 35:57.480
Again, Boston Dynamics now has robots that could pick themselves up after that.

35:57.480 --> 35:59.480
That's really impressive, again.

35:59.480 --> 36:03.480
But all the other stuff to get to that point, we don't really know how to do in a robotic setting.

36:03.480 --> 36:04.480
Or think about this baby here.

36:04.480 --> 36:05.480
This is a younger baby.

36:05.480 --> 36:12.480
This is one of the Internet's very most popular videos because it features a baby and a cat.

36:12.480 --> 36:14.480
But the baby's doing something interesting.

36:14.480 --> 36:19.480
He's got the same cups, but he's decided, again, decided to try a new thing.

36:19.480 --> 36:20.480
So think about creativity.

36:20.480 --> 36:25.480
He's decided that his goal is to stack up cups on the back of a cat, I guess.

36:25.480 --> 36:27.480
He's asking, how many cups can I fit on the back of a cat?

36:27.480 --> 36:28.480
Well, three.

36:28.480 --> 36:31.480
Let's see, can I fit more?

36:31.480 --> 36:33.480
Let's try another one.

36:33.480 --> 36:34.480
Okay.

36:34.480 --> 36:35.480
Well, he can't fit more than three.

36:35.480 --> 36:36.480
It turns out.

36:36.480 --> 36:37.480
And then he, then it's not working.

36:37.480 --> 36:39.480
So he changes his goal.

36:39.480 --> 36:42.480
Now his goal appears to be to get the cups on the other side of the cat.

36:42.480 --> 36:45.480
Now watch that part when he reaches back behind him there.

36:45.480 --> 36:47.480
I'll just pause it there for a moment.

36:47.480 --> 36:51.480
So when he just reached back there, that's a particularly striking moment in the video.

36:51.480 --> 36:56.480
It shows a very strong form of what we call in cognitive science, object permanence.

36:56.480 --> 36:57.480
Okay.

36:57.480 --> 37:01.480
That's the idea that you represent objects as these permanent, enduring entities in the

37:01.480 --> 37:03.480
world, even when you can't see them.

37:03.480 --> 37:07.480
In this case, he hadn't seen or touched that object behind him for like at least a minute,

37:07.480 --> 37:08.480
right?

37:08.480 --> 37:09.480
Maybe much longer, I don't know.

37:09.480 --> 37:12.480
And yet he still knew it was there and he was able to incorporate it in his plan, right?

37:12.480 --> 37:15.480
There's a moment before that when he's about to reach for it, but then he sees this other

37:15.480 --> 37:16.480
one, right?

37:16.480 --> 37:19.480
And it's only when he's now exhausted all the other objects here that he can see.

37:19.480 --> 37:22.480
He's like, okay, now time to get this object and bring it into play, right?

37:22.480 --> 37:27.480
So think about what has to be going on in his brain for him to be able to do that, right?

37:27.480 --> 37:31.480
That's like the analog of you understanding what's behind you, okay?

37:31.480 --> 37:34.480
It's not that these things are impossible to capture machines far from it.

37:34.480 --> 37:37.480
It's just that like training a deep neural network or any kind of pattern recognition

37:37.480 --> 37:39.480
system we don't think is going to do it.

37:39.480 --> 37:43.480
But we think by reverse engineering how it works in the brain, we might be able to do

37:43.480 --> 37:44.480
it.

37:44.480 --> 37:45.480
I think we can do it, okay?

37:45.480 --> 37:47.480
It's not just humans that do this kind of activity.

37:47.480 --> 37:49.480
Here's a couple of, again, rather famous videos.

37:49.480 --> 37:51.480
You can watch all of these on YouTube.

37:51.480 --> 37:58.480
Crows are famous object manipulators and tool users, but also orangutans, other primates,

37:58.480 --> 37:59.480
rodents.

37:59.480 --> 38:02.480
We can watch, if we just, here, let me pause this one for a second.

38:02.480 --> 38:05.480
If we watch this orangutan here, he's got a bunch of big Legos.

38:05.480 --> 38:10.480
And over the course of this video, he's building up a stack of Legos.

38:10.480 --> 38:13.480
It's really quite impressive.

38:13.480 --> 38:16.480
He's just jumping to the end.

38:16.480 --> 38:21.480
There's actually some controversy out there of whether this video is a fake.

38:21.480 --> 38:25.480
But the controversy isn't about, you know, it's not like whether it was, I don't know,

38:25.480 --> 38:27.480
done with computer animation.

38:27.480 --> 38:31.480
Some people think the video was actually filmed backwards, that a human built up the stack

38:31.480 --> 38:34.480
and the orangutan just slowly disassembled it piece by piece.

38:34.480 --> 38:37.480
And it turns out it's remarkably hard to tell whether it's played forward or backwards in

38:37.480 --> 38:38.480
time.

38:38.480 --> 38:40.480
And people have argued over little details because, you know, it would be quite impressive

38:40.480 --> 38:45.480
if an orangutan actually was able to build up this really impressive stack of Legos.

38:45.480 --> 38:48.480
And I would submit that it would be almost as impressive if he disassembled it.

38:48.480 --> 38:50.480
Think about the activity.

38:50.480 --> 38:53.480
I mean, if I wanted to disassemble that, the easiest thing to do would just be to knock it over.

38:53.480 --> 38:55.480
That's really all most robots could do.

38:55.480 --> 38:59.480
But to piece by piece disassemble it, even if it's played backwards like this,

38:59.480 --> 39:03.480
that's still a really impressive act of symbolic planning on physical objects.

39:03.480 --> 39:08.480
Or here you've got this famous mouse, this you can find on the internet under the

39:08.480 --> 39:10.480
Mouse vs. Cracker video.

39:10.480 --> 39:15.480
And what you'll see here over the course of this video is a mouse valiantly and mostly

39:15.480 --> 39:20.480
hopelessly struggling with a cracker that they're hoping to bring back to their nest.

39:20.480 --> 39:23.480
I guess it's a very appealing big meal.

39:23.480 --> 39:28.480
And at some point after just trying to get it over the wall, at some point the mouse

39:28.480 --> 39:30.480
just gives up because it's just never going to happen.

39:30.480 --> 39:31.480
And he just goes away.

39:31.480 --> 39:37.480
Except that because even mouses can dream or mice can dream, at some point he decides,

39:37.480 --> 39:39.480
he's not going to come back for one more try.

39:39.480 --> 39:42.480
And he tries one more time and this time valiantly gets it over.

39:42.480 --> 39:44.480
Isn't that very impressive?

39:44.480 --> 39:45.480
Congratulations.

39:45.480 --> 39:46.480
You don't have to clap.

39:46.480 --> 39:49.480
You can clap for me at the end or clap for whoever later.

39:49.480 --> 39:52.480
But I want to applaud the mouse there every time I see that.

39:52.480 --> 39:56.480
But again, think what had to be going on in his brain to be able to do that.

39:56.480 --> 39:58.480
It's a crazy thing.

39:58.480 --> 40:01.480
And yet he formulated the goal and was able to achieve it.

40:01.480 --> 40:04.480
I'll just show one more video that is really more about science.

40:04.480 --> 40:07.480
These other ones are, you know, some of them actually were from scientific experiments.

40:07.480 --> 40:10.480
But this is one that motivates a lot of the science that I do.

40:10.480 --> 40:15.480
And to me it sets up kind of a grand cognitive science challenge for AI and robotics.

40:15.480 --> 40:20.480
It's from an experiment with humans, again 18-month-olds or one and a half-year-olds.

40:20.480 --> 40:23.480
So the kids in this experiment were the same age as the first baby I showed you,

40:23.480 --> 40:24.480
the one who did the stacking.

40:24.480 --> 40:29.480
And 18 months is really a very, very good age to study if you're interested in intelligence

40:29.480 --> 40:32.480
for reasons we can talk about later if you're interested.

40:32.480 --> 40:35.480
This is from a very famous experiment done by two psychologists,

40:35.480 --> 40:38.480
Felix Wernicke and Michael Tomasello.

40:38.480 --> 40:42.480
And it was studying the spontaneous helping behavior of young children.

40:42.480 --> 40:44.480
It also contrasted humans and chimps.

40:44.480 --> 40:48.480
And the punchline is that chimps sometimes do things that are kind of like what this human did,

40:48.480 --> 40:51.480
but not nearly as reliably or as flexibly.

40:51.480 --> 40:56.480
Okay, so not nearly in his, and I'll show you a particular kind of unusual situation

40:56.480 --> 41:00.480
where human kids had relatively little trouble figuring out kind of what to do

41:00.480 --> 41:02.480
or even whether they should do it.

41:02.480 --> 41:06.480
Whereas basically no chimp did what you're going to see humans sometimes doing here.

41:06.480 --> 41:11.480
So the experimenter in this movie, and I'll turn on the sound here if you can hear it.

41:11.480 --> 41:17.480
The experimenter is the tall guy and the participant is the little kid in the corner.

41:17.480 --> 41:21.480
There's sound but no words, right?

41:21.480 --> 41:25.480
And at some point he stops and then the kid just does whatever they want to do.

41:25.480 --> 41:26.480
So watch what he does.

41:26.480 --> 41:31.480
He goes over, he opens the cabinet, looks inside, then he steps back

41:31.480 --> 41:36.480
and he looks up at Felix and then looks down, okay, and then the action is completed.

41:36.480 --> 41:41.480
Now, I want you to watch it one more time and think about what's got to be going inside the kid's head

41:41.480 --> 41:45.480
to understand this, to understand, like, so it seems like what it looks like to us

41:45.480 --> 41:48.480
is the kid figured out that this guy needed help and helped him.

41:48.480 --> 41:50.480
And the paper is full of many other situations like this.

41:50.480 --> 41:52.480
This is just one, okay?

41:52.480 --> 41:54.480
But the key idea is that the situation is somewhat novel.

41:54.480 --> 41:57.480
People have seen people holding books and opening cabinets,

41:57.480 --> 42:01.480
but probably it's very rare to see this kind of situation exactly, right?

42:01.480 --> 42:04.480
It's different in some important details from what you might have seen before.

42:04.480 --> 42:06.480
And there's other ones in there that are really truly novel

42:06.480 --> 42:09.480
because they just made up a machine right there, okay?

42:09.480 --> 42:14.480
But somehow he has to understand causally from the way the guy is banging the books against the thing.

42:14.480 --> 42:18.480
It's sort of both a symbol, but it's also somehow he's got to understand

42:18.480 --> 42:22.480
what he can do and what he can't do and then what the kid can do to help.

42:22.480 --> 42:25.480
And I'll show this again, but really just watch,

42:25.480 --> 42:32.480
the main part I want you to see is, I'll just sort of skip ahead.

42:32.480 --> 42:35.480
So watch this part here.

42:35.480 --> 42:39.480
Let's say I'll just jump right when he, watch, right now he's about to look up.

42:39.480 --> 42:43.480
He looks up and makes eye contact and then his eyes look down.

42:43.480 --> 42:48.480
So again, he looks up, he looks up and then a saccade,

42:48.480 --> 42:52.480
a sudden rapid eye movement down, down to his hands, up, down, okay?

42:52.480 --> 42:55.480
So that's, again, that's this brain OS in action, right?

42:55.480 --> 43:01.480
He's making one glance, small glance, at the big guy's eyes to make eye contact,

43:01.480 --> 43:05.480
to see, to get a signal, did I understand what you wanted

43:05.480 --> 43:08.480
and did you register that joint attention?

43:08.480 --> 43:11.480
And then he makes a prediction about what the guy's going to do,

43:11.480 --> 43:13.480
so he looks right down, he doesn't just like look around randomly,

43:13.480 --> 43:18.480
he looks right down to the guy's hands to track the action that he expects to see happening

43:18.480 --> 43:22.480
if I did the right thing to help you, then I expect you're going to put the books there, okay?

43:22.480 --> 43:26.480
So you can see these things happening and we want to know what's going on inside the mind

43:26.480 --> 43:28.480
that guides all of that, all right?

43:28.480 --> 43:32.480
So that's this sort of big scientific agenda that we're working on over the next few years

43:32.480 --> 43:38.480
where we think some kind of human, understanding of human intelligence and scientific terms

43:38.480 --> 43:40.480
could lead to all sorts of AI payoffs.

43:40.480 --> 43:43.480
In particular, suppose we could build a robot that could do what this kid

43:43.480 --> 43:47.480
and many other kids in these experiments do to say help you out around the house

43:47.480 --> 43:51.480
without having to be programmed or even really instructed just to kind of get a sense,

43:51.480 --> 43:54.480
oh yeah, you need to hand with that, sure, let me help you out, okay?

43:54.480 --> 43:58.480
Even 18-month-olds will do that, sometimes not very reliably or effectively,

43:58.480 --> 44:01.480
sometimes they'll try to help and really do the opposite, right?

44:01.480 --> 44:07.480
But imagine if you could take the flexible understanding of humans, actions, goals and so on

44:07.480 --> 44:11.480
and make those reliable engineering technology, that would be very useful.

44:11.480 --> 44:15.480
And it would also be related to, say, machines that you could actually start to talk to

44:15.480 --> 44:18.480
and trust in some ways, right, that shared understanding.

44:18.480 --> 44:20.480
So how are we going to do this?

44:20.480 --> 44:23.480
Well, let me spend the rest of the time talking about how we try to do this, right?

44:23.480 --> 44:28.480
Some of the technology that we're building both in our group and more broadly

44:28.480 --> 44:31.480
to try to make these kinds of architectures real.

44:31.480 --> 44:36.480
And I'll talk about two or three technical ideas, again, not in any detail, all right?

44:36.480 --> 44:39.480
One is the idea of a probabilistic program.

44:39.480 --> 44:45.480
So this is a kind of, think of it as a computational abstraction

44:45.480 --> 44:49.480
that we can use to capture the common sense knowledge of this core cognition.

44:49.480 --> 44:53.480
So when I say we have an intuitive understanding of physical objects and people's goals,

44:53.480 --> 44:56.480
how do I build a model of that model you have in the head?

44:56.480 --> 44:59.480
Probabilistic programs, a little bit more technically,

44:59.480 --> 45:03.480
one way to understand them is as a generalization of Bayesian networks

45:03.480 --> 45:07.480
or other kinds of directed graphical models, if you know those, okay?

45:07.480 --> 45:11.480
But where instead of defining a probability model on a graph,

45:11.480 --> 45:13.480
you define it on a program,

45:13.480 --> 45:19.480
and thereby have access to a much more expressive toolkit of knowledge representation.

45:19.480 --> 45:24.480
So data structures, other kinds of algorithmic tools for representing knowledge, okay?

45:24.480 --> 45:27.480
But you still have access to the ability to do probabilistic inference,

45:27.480 --> 45:32.480
like in a graphical model, but also causal inference in a directed graphical model.

45:32.480 --> 45:35.480
So for those of you who know about graphical models, that might make some sense to you.

45:35.480 --> 45:38.480
But just more broadly, what this is, think of this as a toolkit

45:38.480 --> 45:41.480
that allows us to combine several of the best ideas,

45:41.480 --> 45:43.480
not just of the recent deep learning era,

45:43.480 --> 45:47.480
but if you look back over the whole scope of AI and as well as cognitive science,

45:47.480 --> 45:51.480
I think there's three or four ideas and more,

45:51.480 --> 45:53.480
but definitely like three ideas we can really put up there

45:53.480 --> 45:57.480
that have proven their worth and have risen and fallen in terms of,

45:57.480 --> 46:00.480
each of these had ideas when the mainstream of the field thought,

46:00.480 --> 46:04.480
this was totally the way to go and every other idea was obviously a waste of time,

46:04.480 --> 46:08.480
and also had its time when many people thought it was a waste of time, okay?

46:08.480 --> 46:13.480
And these three big ideas, I would say, are first of all the idea of symbolic representation,

46:13.480 --> 46:16.480
or symbolic languages for knowledge representation,

46:16.480 --> 46:20.480
probabilistic inference in generative models to capture uncertainty, ambiguity,

46:20.480 --> 46:25.480
learning from sparse data, and in their hierarchical setting, learning to learn, right?

46:25.480 --> 46:32.480
And then, of course, the recent developments with neural-inspired architectures for pattern recognition, okay?

46:32.480 --> 46:36.480
Each of these things, each of these ideas, symbolic languages, probabilistic inference,

46:36.480 --> 46:41.480
and neural networks have some distinctive strengths that are real weak points of the other approaches, right?

46:41.480 --> 46:44.480
So to take one example that I haven't really talked about here,

46:44.480 --> 46:49.480
people in the, but you mentioned as an outstanding challenge for neural networks,

46:49.480 --> 46:54.480
transfer learning, or learning to take knowledge across a number of previous tasks to transfer to others,

46:54.480 --> 46:58.480
is a real challenge and has always been a challenge in a neural net, okay?

46:58.480 --> 47:03.480
But it's something that's addressed very naturally and very scalably in, for example, a hierarchical Bayesian model.

47:03.480 --> 47:07.480
And if you look at some of the recent attempts, really interesting attempts within the deep learning world

47:07.480 --> 47:11.480
to try to get kinds of transfer learning and learning to learn, they're really cool, okay?

47:11.480 --> 47:16.480
But many of them are in some ways kind of reinventing within a neural network paradigm

47:16.480 --> 47:21.480
ideas that people, you know, maybe just 10 or 15 years ago developed in very sophisticated ways

47:21.480 --> 47:24.480
in, let's say, hierarchical Bayesian models, okay?

47:24.480 --> 47:29.480
And a lot of attempts to get sort of symbolic algorithm-like behavior in neural networks, again,

47:29.480 --> 47:34.480
are really, you know, they're very small steps towards something which is a very mature technology

47:34.480 --> 47:37.480
in computer systems and programming languages.

47:37.480 --> 47:41.480
Probabilistic programs, I'll just sort of advertise mostly,

47:41.480 --> 47:44.480
are a way to combine the strengths of all of these approaches,

47:44.480 --> 47:49.480
to have knowledge representations which are as expressive as anything that anybody ever did in the symbolic paradigm,

47:49.480 --> 47:54.480
that are as flexible at dealing with uncertainty and sparse data as anything in the probabilistic paradigm,

47:54.480 --> 47:59.480
but that also can support pattern recognition tools to be able to, for example,

47:59.480 --> 48:03.480
do very fast efficient inference in very complex scenarios.

48:03.480 --> 48:06.480
And there's a number of, that's the kind of conceptual framework.

48:06.480 --> 48:09.480
There's a number of actually implemented tools.

48:09.480 --> 48:14.480
I point to here on the slide a number of probabilistic programming languages which you can go explore.

48:14.480 --> 48:17.480
For example, there's one that was developed in our group a few years ago,

48:17.480 --> 48:21.480
almost 10 years ago, now called Church, which was the antecedent of some of these other languages

48:21.480 --> 48:23.480
built on a functional programming core.

48:23.480 --> 48:26.480
So Church is a probabilistic programming language built on the Lambda Calculus,

48:26.480 --> 48:28.480
or really in LISP, basically.

48:28.480 --> 48:34.480
But there are many other more modern tools, especially if you are interested in neural networks.

48:34.480 --> 48:38.480
There are tools like, for example, Pyro, or ProbTorch, or BayesFlow,

48:38.480 --> 48:42.480
that try to combine all these ideas in a, or for example, Gen here,

48:42.480 --> 48:46.480
which is a project of Vakash Mansingh's probabilistic computing group.

48:46.480 --> 48:51.480
These are all things which are just in the very beginning stages, very, very alpha.

48:51.480 --> 48:55.480
But you can find out more about them online or by writing to their creators.

48:55.480 --> 49:02.480
And I think this is a very exciting place where the convergence of a number of different AI tools are happening.

49:02.480 --> 49:08.480
And this will be absolutely necessary for making the kind of architecture that I'm talking about work.

49:08.480 --> 49:11.480
Another key idea, which we've been building on in our lab,

49:11.480 --> 49:15.480
and I think, again, many people are using some version of this idea,

49:15.480 --> 49:18.480
but maybe a little bit different from the way we're doing it,

49:18.480 --> 49:23.480
is what the version of this idea that I like to talk about is what I call the game engine in the head.

49:23.480 --> 49:27.480
So this is the idea that it's really what the programs are about.

49:27.480 --> 49:31.480
When I talk about probabilistic programs, I haven't said anything about what kind of programs we're using.

49:31.480 --> 49:35.480
We're just basically, these probabilistic programming languages at their best,

49:35.480 --> 49:39.480
and church, the language that was developed by Noah Goodman and Vakash and others,

49:39.480 --> 49:41.480
and Dan Roy and our group some 10 years ago,

49:41.480 --> 49:45.480
was intended to be a turing complete probabilistic programming language.

49:45.480 --> 49:51.480
So any probability model that was computable, or for whose conditional inferences are computable,

49:51.480 --> 49:53.480
you could represent in these languages.

49:53.480 --> 49:59.480
But that leaves completely open what kind of program I'm going to write to model the world.

49:59.480 --> 50:06.480
And I've been very inspired in the last few years by thinking about the kinds of programs that are in modern video game engines.

50:06.480 --> 50:09.480
So again, probably most of you are familiar with these,

50:09.480 --> 50:12.480
and increasingly they're playing a role in all sorts of ways in AI,

50:12.480 --> 50:18.480
but these are tools that were developed by the video game industry to allow a game designer to make a new game

50:18.480 --> 50:23.480
without having to do most of, in some sense, many, most of the hard technical work from scratch,

50:23.480 --> 50:27.480
but rather to focus on the characters, the world, the story,

50:27.480 --> 50:31.480
the things that are more interesting for designing a novel game.

50:31.480 --> 50:36.480
In particular, if we want a player to explore some new three-dimensional world,

50:36.480 --> 50:39.480
but to have them be able to interact with the world in real time

50:39.480 --> 50:46.480
and to render nice-looking graphics in real time in an interactive way as the player moves around and explores the world,

50:46.480 --> 50:51.480
or if you want to populate the world with non-player characters that will behave in an even vaguely intelligent way.

50:51.480 --> 50:56.480
Game engines give you tools for doing all of this without having to write all of graphics from scratch

50:56.480 --> 51:00.480
or all of physics, the rules of physics from scratch,

51:00.480 --> 51:05.480
so what are called game physics engines, and in some sense are a set of principles,

51:05.480 --> 51:08.480
but also hacks from Newtonian mechanics and other areas of physics

51:08.480 --> 51:13.480
that allow you to simulate plausible-looking physical interactions in very complex world,

51:13.480 --> 51:16.480
very approximately, but very fast.

51:16.480 --> 51:20.480
There's also what's called game AI, which are basically very simple planning models.

51:20.480 --> 51:26.480
So let's say I want to have an AI in the game that is like a guard that guards a base and a player is going to attack the space.

51:26.480 --> 51:29.480
So back in the old Atari days, like when I was a kid,

51:29.480 --> 51:34.480
the guards would just be random things that would fire missiles kind of randomly in random directions at random times,

51:34.480 --> 51:37.480
but let's say you want a guard to be a little intelligent,

51:37.480 --> 51:42.480
so to actually look around and, oh, and I see the player, and then to actually start shooting at you and to even maybe pursue you.

51:42.480 --> 51:45.480
So that requires putting a little AI in the game,

51:45.480 --> 51:49.480
and you do that by having basically simple agent models in the game.

51:49.480 --> 51:54.480
So what we think, and some of you might think this is crazy and some of you might think this is a very natural idea,

51:54.480 --> 51:56.480
I get both kinds of reactions.

51:56.480 --> 52:01.480
What we think is that these tools of fast-approximate renderers, physics engines,

52:01.480 --> 52:06.480
and sort of very simple kinds of AI planning are an interesting first approximation

52:06.480 --> 52:11.480
to the kinds of common-sense knowledge representations that evolution has built into our brains.

52:11.480 --> 52:16.480
So when we talk about the cognitive core or how do babies start,

52:16.480 --> 52:19.480
ways in which a baby's brain isn't a blank slate,

52:19.480 --> 52:24.480
one interesting idea is that it starts with something like these tools

52:24.480 --> 52:27.480
and then wrapped inside a framework for probabilistic inference,

52:27.480 --> 52:29.480
that's what we mean by probabilistic programs,

52:29.480 --> 52:33.480
that can support many activities of common-sense perception and thinking.

52:33.480 --> 52:38.480
So I'll just give you one example of what we call this intuitive physics engine.

52:38.480 --> 52:45.480
So this is work that we did in our groups that Pete Battaglia and Jess Hamrick started this work about five years ago now,

52:45.480 --> 52:48.480
where we showed people, in some sense,

52:48.480 --> 52:52.480
and this is also an illustration of a kind of experiment that you might do,

52:52.480 --> 52:55.480
when I keep talking about science, I'll show you now a couple of experiments.

52:55.480 --> 52:59.480
So we would show people simple physical scenes like these blocks world scenes

52:59.480 --> 53:01.480
and ask them to make a number of judgments.

53:01.480 --> 53:05.480
And the model we built does basically a little bit of probabilistic inference

53:05.480 --> 53:08.480
in a game-style physics engine, it perceives the physical state

53:08.480 --> 53:13.480
and imagines a few different possible ways the world could go over the next one or two seconds

53:13.480 --> 53:16.480
to answer questions like, will the stack of blocks fall?

53:16.480 --> 53:18.480
Or if they fall, how far will they fall?

53:18.480 --> 53:19.480
Or which way will they fall?

53:19.480 --> 53:25.480
Or what would happen if, say, one color of blocks or one material, like the green stuff,

53:25.480 --> 53:27.480
is ten times heavier than the gray stuff?

53:27.480 --> 53:29.480
Or vice versa, how will that change the direction of fall?

53:29.480 --> 53:34.480
Or look at those red and yellow blocks, some of which look like they should be falling but aren't.

53:34.480 --> 53:35.480
So why?

53:35.480 --> 53:41.480
Can you infer from the fact that they're not falling that one color block is much heavier than the other?

53:41.480 --> 53:44.480
Or let me show you a sort of a slightly weird task.

53:44.480 --> 53:46.480
It's like other behavioral experiments.

53:46.480 --> 53:51.480
Sometimes we do weird things so that we can test ways in which you use your knowledge

53:51.480 --> 53:54.480
that you didn't just learn from pattern recognition,

53:54.480 --> 53:57.480
but use it to do new kinds of tasks that you'd never seen before.

53:57.480 --> 54:01.480
So here's a task which many of you have maybe seen me talk about these things.

54:01.480 --> 54:05.480
So you might have seen this task, but probably only if you saw me give a talk around here before.

54:05.480 --> 54:09.480
We call this the red-yellow task, and again, we'll make this one interactive.

54:09.480 --> 54:14.480
So imagine that the blocks on the table are knocked hard enough to bump,

54:14.480 --> 54:17.480
the tables bumped hard enough to knock some of the blocks onto the floor.

54:17.480 --> 54:20.480
So you tell me, is it more likely to be red blocks or yellow blocks?

54:20.480 --> 54:21.480
What do you say?

54:21.480 --> 54:22.480
Red.

54:22.480 --> 54:23.480
Okay, good.

54:23.480 --> 54:25.480
How about here?

54:25.480 --> 54:26.480
Yellow.

54:26.480 --> 54:27.480
Yellow.

54:27.480 --> 54:28.480
Good.

54:28.480 --> 54:29.480
How about here?

54:29.480 --> 54:30.480
Uh-huh.

54:30.480 --> 54:31.480
Here?

54:31.480 --> 54:33.480
Here?

54:33.480 --> 54:35.480
Okay.

54:35.480 --> 54:38.480
Here?

54:38.480 --> 54:39.480
Here?

54:39.480 --> 54:40.480
Here?

54:40.480 --> 54:41.480
Okay.

54:41.480 --> 54:46.480
So you just experience for yourself what it's like to be a subject in one of these experiments.

54:46.480 --> 54:47.480
We just did the experiment here.

54:47.480 --> 54:50.480
The data is all captured on video, sort of, right?

54:50.480 --> 54:51.480
Okay.

54:51.480 --> 54:55.480
You could see that sometimes people were very quick, other times people were slower.

54:55.480 --> 54:59.480
Sometimes there was a lot of consensus, sometimes there was a little bit less consensus, right?

54:59.480 --> 55:01.480
That reflects uncertainty.

55:01.480 --> 55:04.480
So again, there's a long history of studying this scientifically.

55:05.480 --> 55:09.480
That, you know, you could, but you can see some, you can see the probabilistic inference at work.

55:09.480 --> 55:11.480
Probabilistic inference over what?

55:11.480 --> 55:18.480
Well, I would say one way to describe it is over one or a few short low precision simulations of the physics of these scenes.

55:18.480 --> 55:20.480
So here is what I mean by this.

55:20.480 --> 55:26.480
I'm going to show you a video of a game engine reconstruction of one of these scenes that simulates a small bump.

55:26.480 --> 55:27.480
So here's a small bump.

55:27.480 --> 55:29.480
Here's the same scene with the big bump.

55:29.480 --> 55:30.480
Okay.

55:30.480 --> 55:33.480
Now notice that at the micro level, different things happen.

55:33.480 --> 55:38.480
But at the cognitive or macro level that matters for common sense reasoning, the same thing happened.

55:38.480 --> 55:43.480
Namely, all the yellow blocks went over onto one side of the table and few or none of the red blocks did.

55:43.480 --> 55:45.480
So it didn't matter which of those simulations you ran in your head.

55:45.480 --> 55:47.480
You'd get the same answer in this case, right?

55:47.480 --> 55:49.480
This is one that's very easy and high confidence and quick.

55:49.480 --> 55:52.480
Also, you didn't have to run the simulation for very long.

55:52.480 --> 55:56.480
You only have to run it for a few time steps like that to see what's going to happen or similarly here.

55:56.480 --> 55:58.480
You only have to run it for a few time steps, okay?

55:58.480 --> 56:00.480
And it doesn't have to be even very accurate.

56:00.480 --> 56:06.480
Even a fair amount of imprecision will give you basically the same answer at the level that matters for common sense.

56:06.480 --> 56:08.480
So that's the kind of thing our model does.

56:08.480 --> 56:11.480
It runs a few low precision simulations for a few time steps.

56:11.480 --> 56:15.480
But if you take the average of what happens there and you compare that with people's judgments,

56:15.480 --> 56:17.480
you get results like what I show you here.

56:17.480 --> 56:21.480
This scatter plot shows on the y-axis the average judgments of people.

56:21.480 --> 56:23.480
On the x-axis, the average judgments of this model.

56:23.480 --> 56:24.480
And it does a pretty good job.

56:24.480 --> 56:32.480
It's not perfect, but the model basically captures people's graded sense of what's going on in this scene and many of these others, okay?

56:32.480 --> 56:35.480
And it doesn't do it with any learning, but I'll come back to that in a second.

56:35.480 --> 56:39.480
It just does it by probabilistic reasoning over a game physics simulation.

56:39.480 --> 56:45.480
Now we can use and we have used the same kind of technology to capture in very simple forms.

56:45.480 --> 56:47.480
Really just proofs of concept at this point.

56:47.480 --> 56:52.480
The kind of common sense physical scene understanding in a child playing with blocks or other objects.

56:52.480 --> 56:56.480
Or in what might go on in a young child's understanding of other people's actions.

56:56.480 --> 56:58.480
What we call the intuitive psychology engine.

56:58.480 --> 57:04.480
Where now the probabilistic programs are defined over these kind of very simple planning and perception programs.

57:04.480 --> 57:05.480
And I won't go into any details.

57:05.480 --> 57:09.480
I'll just point to a couple of papers that my group played a very small role in.

57:09.480 --> 57:12.480
But we provided some models, which together with some infant researchers,

57:12.480 --> 57:18.480
people working on both of these are experiments that were done with 10 or 12 month infants.

57:18.480 --> 57:20.480
So younger than even some of the babies I showed you before.

57:20.480 --> 57:23.480
But basically like that youngest baby, the one with the cat.

57:23.480 --> 57:26.480
Here's an example of showing simple physical scenes.

57:26.480 --> 57:33.480
These are moving objects to 12 month olds where they saw a few objects bouncing around inside a gumball machine.

57:33.480 --> 57:36.480
And after some point in time, the scene gets occluded.

57:36.480 --> 57:37.480
You'll see the scene is occluded.

57:37.480 --> 57:41.480
And then after another period of time, one of the objects will appear at the bottom.

57:41.480 --> 57:44.480
And the question is, is that the object you expected to see or not?

57:44.480 --> 57:46.480
Is it expected or surprising?

57:46.480 --> 57:50.480
The standard way you study what infants know is by what's called looking time methods.

57:50.480 --> 57:51.480
Just like an adult.

57:51.480 --> 57:54.480
If I show you something that's surprising, you might look longer.

57:54.480 --> 57:57.480
If you're bored, you'll look away.

57:57.480 --> 58:00.480
So you can do that same kind of thing with infants.

58:00.480 --> 58:02.480
And by measuring how long they look at a scene,

58:02.480 --> 58:06.480
you can measure whether you've shown them something surprising or not.

58:06.480 --> 58:09.480
There are literally hundreds of studies, if not more,

58:09.480 --> 58:12.480
using looking time measures to study what infants know.

58:12.480 --> 58:16.480
But only with this paper that we published a few years ago,

58:16.480 --> 58:19.480
did we have a quantitative model where we were able to show a relation

58:19.480 --> 58:21.480
between inverse probability in this case and surprise.

58:21.480 --> 58:26.480
So things which were objectively lower probability under one of these probabilistic physics simulations

58:26.480 --> 58:29.480
across a number of different manipulations of how fast the objects were,

58:29.480 --> 58:31.480
where they were, when the scene was occluded,

58:31.480 --> 58:34.480
how long the delay was, various physically relevant variables,

58:34.480 --> 58:36.480
how many objects there were of one type or another.

58:36.480 --> 58:39.480
Infants' expectations connected with this model.

58:39.480 --> 58:42.480
Or another paper that we published, that one was done,

58:42.480 --> 58:46.480
the experiments there were done by Erno Teglas and Luca Benotti's lab.

58:46.480 --> 58:51.480
Here is a study that was done just recently by Sherry Liu in Liz Spelke's lab

58:51.480 --> 58:54.480
there at Harvard, but they're partners with us in CBMM,

58:54.480 --> 58:56.480
which was about infants' understanding of goals.

58:56.480 --> 58:59.480
So this is more like, again, understanding of agents and intuitive psychology,

58:59.480 --> 59:02.480
where, again, in very simple cartoon scenes,

59:02.480 --> 59:05.480
you show an infant an agent that seems to be doing something,

59:05.480 --> 59:07.480
like an animated cartoon character.

59:07.480 --> 59:11.480
But it jumps over a wall, or it rolls up a hill, or it jumps over a gap.

59:11.480 --> 59:15.480
And the question is, basically, how much does the agent want the goal

59:15.480 --> 59:17.480
that it seems to be trying to achieve?

59:17.480 --> 59:21.480
And what this study showed, and the models here were done by Tomer Omen,

59:21.480 --> 59:26.480
was that infants appeared to be sensitive to the physical work done by the agent.

59:26.480 --> 59:31.480
The more work the agent did, in the sense of the integral of force applied over a path,

59:31.480 --> 59:36.480
the more the infants thought the agent wanted the goal.

59:36.480 --> 59:40.480
We think of this as representing what we've sometimes called the naive utility calculus.

59:40.480 --> 59:44.480
So the idea that there's a basic calculus of cost and benefit,

59:44.480 --> 59:48.480
you know, we take actions which are a little bit costly to achieve goal states

59:48.480 --> 59:51.480
which give us some reward, that's the most basic way,

59:51.480 --> 59:54.480
the oldest way to think about rational and intentional action.

59:54.480 --> 59:57.480
And it seems that even 10-month-olds understand some version of that,

59:57.480 --> 01:00:00.480
where the cost can be measured in physical terms.

01:00:00.480 --> 01:00:03.480
I see I'm running a little bit behind on time,

01:00:03.480 --> 01:00:05.480
and I wanted to leave some time for discussion.

01:00:05.480 --> 01:00:08.480
So I'll just go very quickly through a couple of other things,

01:00:08.480 --> 01:00:13.480
and happy to stay around at the end for discussion.

01:00:13.480 --> 01:00:17.480
What I showed you here was the science, where does the engineering go?

01:00:17.480 --> 01:00:20.480
So one thing you can do with this is say,

01:00:20.480 --> 01:00:23.480
build a machine system that can look not a little animated cartoon

01:00:23.480 --> 01:00:26.480
like these baby experiments, but a real person doing something,

01:00:26.480 --> 01:00:29.480
and again, combine physical costs and constraints of actions

01:00:29.480 --> 01:00:32.480
with some understanding of the agent's utilities,

01:00:32.480 --> 01:00:36.480
that's the math of planning, to figure out what they wanted.

01:00:36.480 --> 01:00:40.480
So look in this scene here, and see if you can judge which object

01:00:40.480 --> 01:00:42.480
the woman is reaching for.

01:00:42.480 --> 01:00:45.480
So you can see there's a grid of four by four objects,

01:00:45.480 --> 01:00:48.480
there's 16 objects here, and she's going to be reaching for one of them.

01:00:48.480 --> 01:00:50.480
It's going to play in slow motion,

01:00:50.480 --> 01:00:53.480
but raise your hand when you know which one she's reaching for.

01:00:53.480 --> 01:01:00.480
So just watch and raise your hand when you know which one she wants.

01:01:00.480 --> 01:01:02.480
So most of the hands are up by now.

01:01:02.480 --> 01:01:04.480
And notice, I was looking at your hands not here,

01:01:04.480 --> 01:01:06.480
but what happened is most of the hands were up

01:01:06.480 --> 01:01:10.480
at about the time when that gray, or the one that dashed line,

01:01:10.480 --> 01:01:11.480
shot up.

01:01:11.480 --> 01:01:15.480
That's not human data, you provided the data, this is our model.

01:01:15.480 --> 01:01:17.480
So our model is predicting, more or less,

01:01:17.480 --> 01:01:19.480
when you're able to say what her goal was.

01:01:19.480 --> 01:01:21.480
It's well before she actually touched the object.

01:01:21.480 --> 01:01:22.480
How does the model work?

01:01:22.480 --> 01:01:26.480
Again, I'll skip the details, but it does the same kind of thing

01:01:26.480 --> 01:01:28.480
that our models of those infants did.

01:01:28.480 --> 01:01:32.480
Namely, but in this case, it does it with a full body model from robotics.

01:01:32.480 --> 01:01:34.480
We use what's called the Mujoko physics engine,

01:01:34.480 --> 01:01:37.480
which is a standard tool in robotics for planning

01:01:37.480 --> 01:01:40.480
physically efficient reaches of, say, a humanoid robot.

01:01:40.480 --> 01:01:45.480
And we say, we can give this planner program a goal object as input.

01:01:45.480 --> 01:01:47.480
We can give it each of the possible goal objects as input

01:01:47.480 --> 01:01:49.480
and say, plan the most physically efficient action,

01:01:49.480 --> 01:01:52.480
so the one that uses the least energy to get to that object.

01:01:52.480 --> 01:01:54.480
And then we can do a Bayesian inference.

01:01:54.480 --> 01:01:56.480
This is the probabilistic inference part.

01:01:56.480 --> 01:01:59.480
This program is the Mujoko planner.

01:01:59.480 --> 01:02:02.480
But then we can say, I want to do Bayesian inference

01:02:02.480 --> 01:02:04.480
to work backwards from what I observed,

01:02:04.480 --> 01:02:06.480
which was the action, to the input to that program.

01:02:06.480 --> 01:02:08.480
What goal was provided as input to the planner?

01:02:08.480 --> 01:02:12.480
And here you can see the full array of four by four possible inputs

01:02:12.480 --> 01:02:14.480
and those bars that are moving up and down,

01:02:14.480 --> 01:02:16.480
that's the Bayesian posterior probability

01:02:16.480 --> 01:02:19.480
of how likely each of those was to be the goal.

01:02:19.480 --> 01:02:21.480
And what you can see is it converges on the right answer,

01:02:21.480 --> 01:02:24.480
at least, well, it turns out to be the ground truth right answer,

01:02:24.480 --> 01:02:26.480
but also the right answer according to what people think,

01:02:26.480 --> 01:02:29.480
with about the same kind of data that people took.

01:02:29.480 --> 01:02:31.480
Now, you might say, well, okay,

01:02:31.480 --> 01:02:33.480
sure, if I just wanted to build a system

01:02:33.480 --> 01:02:35.480
that could detect what somebody was reaching for,

01:02:35.480 --> 01:02:38.480
I could generate a training data set of this sort of scene

01:02:38.480 --> 01:02:41.480
and train something up to analyze patterns of motion.

01:02:41.480 --> 01:02:43.480
But again, because the engine in your head

01:02:43.480 --> 01:02:45.480
actually does something we think more like this,

01:02:45.480 --> 01:02:48.480
it does what we call inverse planning over a physics model.

01:02:48.480 --> 01:02:50.480
It can apply to much more interesting scenes

01:02:50.480 --> 01:02:52.480
that you haven't really seen much of before.

01:02:52.480 --> 01:02:54.480
So take the scene on the left, right,

01:02:54.480 --> 01:02:56.480
where again, you see somebody reaching for one

01:02:56.480 --> 01:02:58.480
of a four by four array of objects,

01:02:58.480 --> 01:03:00.480
but what you see is a strange kind of reach.

01:03:00.480 --> 01:03:02.480
Can you see why he's doing a strange reach?

01:03:02.480 --> 01:03:04.480
Up there, it's a little small,

01:03:04.480 --> 01:03:07.480
but you can see that he's reaching over something, right?

01:03:07.480 --> 01:03:09.480
It's actually a pane of glass, right?

01:03:09.480 --> 01:03:10.480
You see that?

01:03:10.480 --> 01:03:13.480
And then there's this other guy who's helping him,

01:03:13.480 --> 01:03:16.480
who sees what he wants and hands him the thing he wants.

01:03:16.480 --> 01:03:20.480
So how does the guy on the foreground see the other guy's goal?

01:03:20.480 --> 01:03:23.480
How does he infer his goal and know how to help him?

01:03:23.480 --> 01:03:25.480
And then how do we look at the two of them

01:03:25.480 --> 01:03:27.480
and figure out who's trying to help who?

01:03:27.480 --> 01:03:29.480
Or that in a scene like this one here,

01:03:29.480 --> 01:03:31.480
that it's not somebody trying to help somebody,

01:03:31.480 --> 01:03:33.480
but rather the opposite, okay?

01:03:33.480 --> 01:03:35.480
So here's a model on the left of how that might work, right?

01:03:35.480 --> 01:03:37.480
And we think this is the kind of model

01:03:37.480 --> 01:03:39.480
needed to tackle this sort of challenge here, right?

01:03:39.480 --> 01:03:41.480
Basically, it's a model,

01:03:41.480 --> 01:03:43.480
we take this model of planning,

01:03:43.480 --> 01:03:45.480
sort of maximal expected utility planning,

01:03:45.480 --> 01:03:47.480
which you can run backwards,

01:03:47.480 --> 01:03:49.480
but then we recursively nest these models inside each other.

01:03:49.480 --> 01:03:50.480
Okay?

01:03:50.480 --> 01:03:52.480
An agent is helping another agent.

01:03:52.480 --> 01:03:54.480
If this agent is acting apparently to us,

01:03:54.480 --> 01:03:57.480
seems to be maximizing an expected utility,

01:03:57.480 --> 01:04:00.480
that's a positive function of that agent's expectation

01:04:00.480 --> 01:04:02.480
about another agent's expected utility,

01:04:02.480 --> 01:04:04.480
and that's what it means to be a helper.

01:04:04.480 --> 01:04:06.480
Hindering is sort of the opposite

01:04:06.480 --> 01:04:09.480
if one seems to be trying to lower somebody else's utility, okay?

01:04:09.480 --> 01:04:11.480
And we've used these same kind of models

01:04:11.480 --> 01:04:13.480
to also describe infant's understanding

01:04:13.480 --> 01:04:15.480
of helping and hindering in a range of scenes.

01:04:15.480 --> 01:04:18.480
I'll just say one last word about learning

01:04:18.480 --> 01:04:20.480
that everybody wants to know about learning,

01:04:20.480 --> 01:04:22.480
and the key thing here,

01:04:22.480 --> 01:04:24.480
and it's definitely part of any picture of AGI,

01:04:24.480 --> 01:04:26.480
but the thought I want to leave you on

01:04:26.480 --> 01:04:28.480
is really about what learning is about, okay?

01:04:28.480 --> 01:04:31.480
It'll be just a few more slides, and then I'll stop, I promise.

01:04:31.480 --> 01:04:33.480
None of the models I showed you so far

01:04:33.480 --> 01:04:34.480
really did any learning.

01:04:34.480 --> 01:04:37.480
They certainly didn't do any task-specific learning, okay?

01:04:37.480 --> 01:04:39.480
We set up a probabilistic program, and then we let it do inference.

01:04:39.480 --> 01:04:41.480
Now, that's not to say that we don't think

01:04:41.480 --> 01:04:43.480
people learn to do these things, we do.

01:04:43.480 --> 01:04:46.480
But the real learning goes on when you're much younger, right?

01:04:46.480 --> 01:04:48.480
The other thing I showed you in basic form,

01:04:48.480 --> 01:04:50.480
even a one-year-old baby can do, okay?

01:04:50.480 --> 01:04:53.480
The basic learning goes on to support these kinds of abilities,

01:04:53.480 --> 01:04:55.480
not that there isn't learning beyond one year,

01:04:55.480 --> 01:04:57.480
but the basic way you learn to, say,

01:04:57.480 --> 01:04:59.480
solve these physics problems is what goes on

01:04:59.480 --> 01:05:03.480
in the brain of a child between zero and 12 months.

01:05:03.480 --> 01:05:05.480
So this is just an example of some phenomena

01:05:05.480 --> 01:05:08.480
that come from the literature on infant cognitive development.

01:05:08.480 --> 01:05:10.480
These are very rough timelines.

01:05:10.480 --> 01:05:12.480
You can take pictures of this if you like.

01:05:12.480 --> 01:05:15.480
This is always a popular slide because it really is quite inspiring, I think,

01:05:15.480 --> 01:05:17.480
and I can give you lots of literature pointers,

01:05:17.480 --> 01:05:20.480
but I'm summarizing in very broad strokes with big error bars

01:05:20.480 --> 01:05:24.480
what we've learned in the field of infant cognitive development

01:05:24.480 --> 01:05:27.480
about when and how kids seem to at least come

01:05:27.480 --> 01:05:30.480
to a certain understanding of basic aspects of physics.

01:05:30.480 --> 01:05:34.480
So if you really want to study how people learn to be intelligent,

01:05:34.480 --> 01:05:36.480
a lot of what you have to study are kids at this age.

01:05:36.480 --> 01:05:39.480
You have to study what's already in their brain at zero months

01:05:39.480 --> 01:05:41.480
and what they learn and how they learn

01:05:41.480 --> 01:05:43.480
between four, six, eight, ten, twelve, and so on,

01:05:43.480 --> 01:05:46.480
and all that beyond that, okay?

01:05:46.480 --> 01:05:49.480
Now, effectively what that amounts to, we think,

01:05:49.480 --> 01:05:52.480
is if what you're learning is something like, let's say,

01:05:52.480 --> 01:05:56.480
an intuitive game physics engine to capture these basic abilities,

01:05:56.480 --> 01:05:59.480
then what we need, if we're going to try to reverse engineer that,

01:05:59.480 --> 01:06:01.480
is what we might think of as a program learning program.

01:06:01.480 --> 01:06:03.480
If your knowledge is in the form of a program,

01:06:03.480 --> 01:06:06.480
then you have to have programs that build other programs, right?

01:06:06.480 --> 01:06:08.480
This is what I was talking about at the beginning,

01:06:08.480 --> 01:06:10.480
about learning as building models of the world.

01:06:10.480 --> 01:06:12.480
Or ultimately, if you think what we start off with

01:06:12.480 --> 01:06:15.480
is something like a game engine that can play any game,

01:06:15.480 --> 01:06:17.480
then what you have to learn is the program of the game

01:06:17.480 --> 01:06:18.480
that you're actually playing,

01:06:18.480 --> 01:06:21.480
or the many different games that you might be playing over your life.

01:06:21.480 --> 01:06:24.480
So think of learning as programming the game engine in your head

01:06:24.480 --> 01:06:27.480
to fit with your experience and to fit with the possible actions

01:06:27.480 --> 01:06:29.480
that you seem like you can take.

01:06:29.480 --> 01:06:32.480
Now, this is what you could call the hard problem of learning

01:06:32.480 --> 01:06:34.480
if you come to learning from, say, neural networks

01:06:34.480 --> 01:06:36.480
or other tools in machine learning, right?

01:06:36.480 --> 01:06:39.480
So what makes most of machine learning go right now,

01:06:39.480 --> 01:06:41.480
and certainly what makes neural networks so appealing,

01:06:41.480 --> 01:06:44.480
is that you can set up basically a big function approximator

01:06:44.480 --> 01:06:47.480
that can approximate many of the functions you might want to do

01:06:47.480 --> 01:06:49.480
in a certain application or task,

01:06:49.480 --> 01:06:51.480
but in a way that's end-to-end differentiable

01:06:51.480 --> 01:06:53.480
and with a meaningful cost function.

01:06:53.480 --> 01:06:55.480
So you can have one of these nice optimization landscapes.

01:06:55.480 --> 01:06:58.480
You can compute the gradients and basically just roll downhill

01:06:58.480 --> 01:07:00.480
until you get to an optimal solution.

01:07:00.480 --> 01:07:03.480
But if you're talking about learning as something like search

01:07:03.480 --> 01:07:05.480
in the space of programs,

01:07:05.480 --> 01:07:07.480
we don't know how to do anything like that yet.

01:07:07.480 --> 01:07:10.480
We don't know how to set this up as any kind of a nice optimization problem

01:07:10.480 --> 01:07:13.480
with any notion of smoothness or gradients, okay?

01:07:13.480 --> 01:07:16.480
Rather, what we need is instead of learning as, like,

01:07:16.480 --> 01:07:18.480
rolling downhill effectively, right,

01:07:18.480 --> 01:07:20.480
a process which just, if you're willing to wait long enough,

01:07:20.480 --> 01:07:24.480
you know, some, you know, simple algorithm we'll take care of,

01:07:24.480 --> 01:07:27.480
think of what we call the idea of learning as programming.

01:07:27.480 --> 01:07:30.480
There's a popular metaphor in cognitive development

01:07:30.480 --> 01:07:32.480
called the child as scientist,

01:07:32.480 --> 01:07:35.480
which emphasizes children as active theory builders

01:07:35.480 --> 01:07:39.480
and children's play as a kind of casual experimentation.

01:07:39.480 --> 01:07:41.480
But this is the algorithmic complement to that,

01:07:41.480 --> 01:07:43.480
what we call the child as coder,

01:07:43.480 --> 01:07:45.480
or around MIT we'll say the child as hacker,

01:07:45.480 --> 01:07:47.480
but the rest of the world, if you say child as hacker,

01:07:47.480 --> 01:07:49.480
they think of someone who breaks into your email

01:07:49.480 --> 01:07:51.480
and steals your credit card numbers,

01:07:51.480 --> 01:07:55.480
we all know that hacking is, you know, making your code more awesome, right?

01:07:55.480 --> 01:07:57.480
If your knowledge is some kind of code,

01:07:57.480 --> 01:07:59.480
or, you know, library of programs,

01:07:59.480 --> 01:08:02.480
then learning is all the ways that a child hacks on their code

01:08:02.480 --> 01:08:04.480
to make it more awesome.

01:08:04.480 --> 01:08:06.480
Awesome can mean more accurate,

01:08:06.480 --> 01:08:08.480
but it can also mean faster, more elegant,

01:08:08.480 --> 01:08:11.480
more transportable to other applications or their tasks,

01:08:11.480 --> 01:08:14.480
more explainable to others, maybe just more entertaining, okay?

01:08:14.480 --> 01:08:17.480
Children have all of those goals in learning,

01:08:17.480 --> 01:08:20.480
and the activities by which they make their code more awesome

01:08:20.480 --> 01:08:24.480
also correspond to many of the activities of coding, right?

01:08:24.480 --> 01:08:27.480
So think about all the ways on a day-to-day basis

01:08:27.480 --> 01:08:29.480
you might make your code more awesome, right?

01:08:29.480 --> 01:08:33.480
You might have a big library of existing functions

01:08:33.480 --> 01:08:35.480
with some parameters that you can tune on a data set.

01:08:35.480 --> 01:08:38.480
That's basically what you do with backprop or stochastic gradient descent

01:08:38.480 --> 01:08:40.480
in training a deep learning system.

01:08:40.480 --> 01:08:43.480
But think about all the ways in which you might actually modify the underlying function.

01:08:43.480 --> 01:08:46.480
So write new code, or take old code from some other thing

01:08:46.480 --> 01:08:49.480
and map it over here, or make a whole new library of code,

01:08:49.480 --> 01:08:51.480
or refactor your code to some other, you know,

01:08:51.480 --> 01:08:54.480
some other basis for it that will work more robustly

01:08:54.480 --> 01:08:58.480
and be more extensible, or transpiling, or compiling, right?

01:08:58.480 --> 01:09:00.480
Or even just commenting your code,

01:09:00.480 --> 01:09:02.480
or asking someone else for their code, okay?

01:09:02.480 --> 01:09:05.480
Again, these are all ways that we make our code more awesome

01:09:05.480 --> 01:09:08.480
and children's learning has analogues to all of these

01:09:08.480 --> 01:09:10.480
that we would want to understand as an engineer

01:09:10.480 --> 01:09:12.480
from an algorithmic point of view.

01:09:12.480 --> 01:09:15.480
So in our group, we've been working on various early steps towards this.

01:09:15.480 --> 01:09:19.480
And again, we don't have anything like program writing programs

01:09:19.480 --> 01:09:21.480
at the level of children's learning algorithms.

01:09:21.480 --> 01:09:23.480
But one example of something that we did in our group,

01:09:23.480 --> 01:09:25.480
which you might not have thought of being about this,

01:09:25.480 --> 01:09:27.480
but it's definitely the AI work we did

01:09:27.480 --> 01:09:31.480
that got the most attention in the last couple of years from our group.

01:09:31.480 --> 01:09:33.480
We had this paper that was in science,

01:09:33.480 --> 01:09:35.480
it was actually on the cover of science,

01:09:35.480 --> 01:09:38.480
sort of just hit the market at the right time, if you like,

01:09:38.480 --> 01:09:40.480
and it got about 100 times more publicity

01:09:40.480 --> 01:09:42.480
than anything else I've ever done,

01:09:42.480 --> 01:09:44.480
which is partly a testament to the really great work

01:09:44.480 --> 01:09:47.480
that Brendan Lake, who was the first author, did for his PhD here,

01:09:47.480 --> 01:09:50.480
but much more so just about the hunger for AI systems at the time

01:09:50.480 --> 01:09:52.480
when we published this in 2015.

01:09:52.480 --> 01:09:55.480
And we built a machine system that, the way we described it,

01:09:55.480 --> 01:09:58.480
was doing human-level concept learning for simple concepts,

01:09:58.480 --> 01:10:00.480
very simple visual concepts.

01:10:00.480 --> 01:10:03.480
These handwritten characters in many of the world's alphabets.

01:10:03.480 --> 01:10:05.480
For those of you who know the famous MNIST dataset,

01:10:05.480 --> 01:10:08.480
the dataset of handwritten digits 0 through 9,

01:10:08.480 --> 01:10:11.480
sorry, that drove so much good research in deep learning

01:10:11.480 --> 01:10:13.480
and pattern recognition,

01:10:13.480 --> 01:10:16.480
it did that not because Jan Lacoon, who put that together,

01:10:16.480 --> 01:10:19.480
or Jeff Hinton, who did a lot of work on deep learning with MNIST,

01:10:19.480 --> 01:10:21.480
they were interested fundamentally in character recognition,

01:10:21.480 --> 01:10:24.480
that they saw that as a very simple test bed

01:10:24.480 --> 01:10:26.480
for developing more general ideas.

01:10:26.480 --> 01:10:28.480
And similarly, we did this work on getting machines

01:10:28.480 --> 01:10:32.480
to do a kind of one-shot learning of generative models,

01:10:32.480 --> 01:10:34.480
also to develop more general ideas.

01:10:34.480 --> 01:10:38.480
We saw this as learning very simple, little mini probabilistic programs.

01:10:38.480 --> 01:10:40.480
In this case, what are those programs?

01:10:40.480 --> 01:10:42.480
They're the programs you use to draw a character.

01:10:42.480 --> 01:10:45.480
So ask yourself, how can you look at any one of these characters

01:10:45.480 --> 01:10:47.480
and see, in a sense, how somebody might draw it?

01:10:47.480 --> 01:10:51.480
The way we tested this in our system was this little visual-turing test

01:10:51.480 --> 01:10:54.480
where we showed people one character in a novel alphabet

01:10:54.480 --> 01:10:56.480
and we said, draw another one.

01:10:56.480 --> 01:10:58.480
We paired nine people, like say on the left,

01:10:58.480 --> 01:11:01.480
and nine samples from our machine, say on the right,

01:11:01.480 --> 01:11:03.480
and we asked other people,

01:11:03.480 --> 01:11:05.480
could you tell which was the human drawing another example,

01:11:05.480 --> 01:11:08.480
or imagining another example, and which was the machine?

01:11:08.480 --> 01:11:09.480
And people couldn't tell.

01:11:09.480 --> 01:11:11.480
When I said ones on the left and ones on the right,

01:11:11.480 --> 01:11:12.480
I don't actually remember,

01:11:12.480 --> 01:11:14.480
and on different ones you can see if you can tell.

01:11:14.480 --> 01:11:15.480
It's very hard to tell.

01:11:15.480 --> 01:11:17.480
Can you tell for each one of these characters

01:11:17.480 --> 01:11:21.480
which new set of examples were drawn by a human versus a machine?

01:11:21.480 --> 01:11:24.480
Here's the right answer, and probably you couldn't tell.

01:11:24.480 --> 01:11:28.480
The way we did this was by assembling a simple kind of program learning program.

01:11:28.480 --> 01:11:30.480
So we basically said, when you draw a character,

01:11:30.480 --> 01:11:33.480
you're assembling strokes and substrokes with goals and sub-goals

01:11:33.480 --> 01:11:35.480
that produce ink on the page,

01:11:35.480 --> 01:11:38.480
and when you see a character, you're working backwards to figure out

01:11:38.480 --> 01:11:41.480
what was the program, the most efficient program that did that.

01:11:41.480 --> 01:11:44.480
So you're basically inverting a probabilistic program,

01:11:44.480 --> 01:11:46.480
doing Bayesian inference to the program,

01:11:46.480 --> 01:11:48.480
most likely to have generated what you saw.

01:11:48.480 --> 01:11:50.480
This is one small step, we think,

01:11:50.480 --> 01:11:52.480
towards being able to learn programs,

01:11:52.480 --> 01:11:55.480
to being able to learn something ultimately like a whole game engine program.

01:11:55.480 --> 01:11:59.480
The last thing I'll leave you with is just a pointer to sort of work in action.

01:11:59.480 --> 01:12:02.480
So this is some work being done by a current PhD student

01:12:02.480 --> 01:12:03.480
who works partly with me,

01:12:03.480 --> 01:12:05.480
but also with Armando, Solar Lasama, and CSAIL.

01:12:05.480 --> 01:12:06.480
This is Kevin Ellis.

01:12:06.480 --> 01:12:09.480
It's an example of what's now, I think, again,

01:12:09.480 --> 01:12:11.480
an emerging, exciting area in AI

01:12:11.480 --> 01:12:13.480
well beyond anything that we're doing,

01:12:13.480 --> 01:12:16.480
is combining techniques from where Armando comes from,

01:12:16.480 --> 01:12:18.480
which is the world of programming languages,

01:12:18.480 --> 01:12:20.480
not machine learning or AI,

01:12:20.480 --> 01:12:22.480
but tools from programming languages

01:12:22.480 --> 01:12:25.480
which can be used to automatically synthesize code.

01:12:25.480 --> 01:12:27.480
With the machine learning toolkit,

01:12:27.480 --> 01:12:30.480
in this case a kind of Bayesian minimum description-length idea,

01:12:30.480 --> 01:12:32.480
to be able to make, again,

01:12:32.480 --> 01:12:35.480
what is really one small step towards machines that can learn programs

01:12:35.480 --> 01:12:39.480
by basically trying to efficiently find the shortest, simplest program

01:12:39.480 --> 01:12:41.480
which can capture some data set.

01:12:41.480 --> 01:12:43.480
So we think by combining these kinds of tools,

01:12:43.480 --> 01:12:46.480
in this case, let's say, from Bayesian inference over programs

01:12:46.480 --> 01:12:48.480
with a number of tools that have been developed

01:12:48.480 --> 01:12:50.480
in other areas of computer science

01:12:50.480 --> 01:12:52.480
that don't look anything or haven't been considered

01:12:52.480 --> 01:12:55.480
to be machine learning or AI, like programming languages.

01:12:55.480 --> 01:12:58.480
It's one of the many ways that, going forward,

01:12:58.480 --> 01:13:01.480
we're going to be able to build smarter, more human-like machines.

01:13:01.480 --> 01:13:05.480
So just to end then, what I've tried to tell you here is talk,

01:13:05.480 --> 01:13:08.480
first of all, identify the ways in which human intelligence

01:13:08.480 --> 01:13:10.480
goes beyond pattern recognition

01:13:10.480 --> 01:13:12.480
to really all these activities of modeling the world.

01:13:12.480 --> 01:13:14.480
To give you a sense of some of the domains

01:13:14.480 --> 01:13:16.480
where we can start to study this,

01:13:16.480 --> 01:13:19.480
like common sense scene understanding, for example,

01:13:19.480 --> 01:13:23.480
or something like one-shot learning, for example,

01:13:23.480 --> 01:13:25.480
like what we were just doing there,

01:13:25.480 --> 01:13:28.480
or learning as programming the engine in your head.

01:13:28.480 --> 01:13:31.480
And to give you a sense of some of the technical tools,

01:13:31.480 --> 01:13:34.480
probabilistic programs, program synthesis, game engines,

01:13:34.480 --> 01:13:37.480
for example, as well as a little bit of deep learning,

01:13:37.480 --> 01:13:41.480
that bringing together, we're starting to be able to make these things real.

01:13:41.480 --> 01:13:44.480
Now, that's the science agenda and the reverse engineering agenda,

01:13:44.480 --> 01:13:47.480
but think about, for those of you who are interested in technology,

01:13:47.480 --> 01:13:50.480
what are the many big AI frontiers that this opens up?

01:13:50.480 --> 01:13:53.480
So the one I'm most excited about is this idea

01:13:53.480 --> 01:13:56.480
which I've highlighted here in our big research agenda.

01:13:56.480 --> 01:13:58.480
This is the one I'm most excited about to work on

01:13:58.480 --> 01:14:00.480
for the rest of my career, honestly.

01:14:00.480 --> 01:14:04.480
But it's really what is the oldest and maybe the best dream

01:14:04.480 --> 01:14:08.480
of AI researchers of how to build a human-like intelligence system,

01:14:08.480 --> 01:14:10.480
a real AGI system.

01:14:10.480 --> 01:14:13.480
It's the idea that Turing proposed when he proposed the Turing test,

01:14:13.480 --> 01:14:15.480
and Minsky proposed this at different times in his life.

01:14:15.480 --> 01:14:17.480
Or many people have proposed this, right?

01:14:17.480 --> 01:14:19.480
Which is to build a system that grows into intelligence

01:14:19.480 --> 01:14:22.480
the way a human does, that starts like a baby and learns like a child.

01:14:22.480 --> 01:14:26.480
And I've tried to show you how we're starting to be able to understand those things.

01:14:26.480 --> 01:14:30.480
What a baby's mind starts with, how children actually learn.

01:14:30.480 --> 01:14:32.480
And looking forward, we might imagine

01:14:32.480 --> 01:14:34.480
that someday we'll be able to build machines that can do this.

01:14:34.480 --> 01:14:37.480
I think we can actually start working on this right now.

01:14:37.480 --> 01:14:40.480
And that's something that we're doing in our group.

01:14:40.480 --> 01:14:43.480
So if that kind of thing excites you, then I encourage you to work on it,

01:14:43.480 --> 01:14:44.480
maybe even with us.

01:14:44.480 --> 01:14:47.480
Or if any one of these other activities of human intelligence excite you,

01:14:47.480 --> 01:14:51.480
I think taking the kind of science-based reverse engineering approach

01:14:51.480 --> 01:14:55.480
that we're doing and then trying to put that into engineering practice,

01:14:55.480 --> 01:14:58.480
this is not just a possible route,

01:14:58.480 --> 01:15:01.480
but I think it's quite possibly the most valuable route

01:15:01.480 --> 01:15:04.480
that you could work on right now to try to actually achieve

01:15:04.480 --> 01:15:07.480
at least some kind of artificial general intelligence,

01:15:07.480 --> 01:15:10.480
especially the kind of intelligence, AI system,

01:15:10.480 --> 01:15:13.480
that's going to live in a human world and interact with humans.

01:15:13.480 --> 01:15:16.480
There's many kinds of AI systems that could live in worlds of data

01:15:16.480 --> 01:15:18.480
that none of us can understand or will ever live in ourselves.

01:15:18.480 --> 01:15:21.480
But if you want to build machines that can live in our world

01:15:21.480 --> 01:15:24.480
and interact with us the way we are used to interacting with other people,

01:15:24.480 --> 01:15:27.480
then I think this is a route that you should consider.

01:15:27.480 --> 01:15:28.480
Okay, thank you.

01:15:28.480 --> 01:15:39.480
Hi there.

01:15:39.480 --> 01:15:43.480
So, early in the talk you expressed some skepticism about whether or not

01:15:43.480 --> 01:15:46.480
industry would get us to understanding human level intelligence.

01:15:46.480 --> 01:15:49.480
It seems that there's a couple of trends that favour industry.

01:15:49.480 --> 01:15:52.480
One is that industry is better than that academia at accumulating resources

01:15:52.480 --> 01:15:54.480
and plowing them back into the topic.

01:15:54.480 --> 01:15:57.480
And it seems at the moment we've got a bit of brain drain going on

01:15:57.480 --> 01:16:01.480
from academia into industry, and that seems like an ongoing trend.

01:16:01.480 --> 01:16:05.480
If you look at something like learning to fly or learning to fly into space,

01:16:05.480 --> 01:16:10.480
then it looks like the story is one of industry kind of taking over the field

01:16:10.480 --> 01:16:13.480
and going off on its own a little bit.

01:16:13.480 --> 01:16:17.480
Academics still have a role, but industry kind of dominates,

01:16:17.480 --> 01:16:19.480
so is industry going to overtake the field, do you think?

01:16:19.480 --> 01:16:21.480
Well, that's a really good question,

01:16:21.480 --> 01:16:23.480
and it's got several good questions packed into one there, right?

01:16:23.480 --> 01:16:26.480
I didn't mean to say, this wasn't meant to say,

01:16:26.480 --> 01:16:29.480
go academia, bad industry, right?

01:16:29.480 --> 01:16:34.480
What I tried to say was the approaches that are currently getting

01:16:34.480 --> 01:16:36.480
the most attention in industry, and they're really,

01:16:36.480 --> 01:16:39.480
because they're really the most valuable ones right now for the short term,

01:16:39.480 --> 01:16:42.480
you know, any industry is really focused on what it can do,

01:16:42.480 --> 01:16:45.480
what are the value propositions on basically a two-year timescale at most?

01:16:45.480 --> 01:16:48.480
I mean, if you ask, say, Google researchers to take the most prominent example,

01:16:48.480 --> 01:16:51.480
that's pretty much what they'll all tell you, okay?

01:16:51.480 --> 01:16:56.480
Maybe things that might pay off initially in two years,

01:16:56.480 --> 01:16:58.480
but maybe take five years or more to really develop,

01:16:58.480 --> 01:17:01.480
but if you can't show that it's going to do something practical for us

01:17:01.480 --> 01:17:03.480
in two years in a way that matters for our bottom line,

01:17:03.480 --> 01:17:06.480
then it's not really worth doing, okay?

01:17:06.480 --> 01:17:10.480
So what I'm talking about is the technologies which right now

01:17:10.480 --> 01:17:13.480
industry sees as meeting that specification,

01:17:13.480 --> 01:17:18.480
and what I'm saying is right now, I think that's not where the root is

01:17:18.480 --> 01:17:22.480
to something like human-like, but not the most valuable promising root

01:17:22.480 --> 01:17:24.480
to human-like kinds of AI systems, all right?

01:17:24.480 --> 01:17:27.480
But I hope that like in the case as you said, you know,

01:17:27.480 --> 01:17:30.480
the basic research that we're doing now will be successful enough

01:17:30.480 --> 01:17:34.480
that it will get the attention of industry when the time is right.

01:17:34.480 --> 01:17:37.480
So, you know, I hope at some point, you know,

01:17:37.480 --> 01:17:41.480
it won't, at least the engineering side will have to be done in industry,

01:17:41.480 --> 01:17:46.480
not just in academia, but you're also pointing to issues of like brain drain

01:17:46.480 --> 01:17:49.480
and other things like that, that I think these are real issues confronting our community.

01:17:49.480 --> 01:17:53.480
I think everybody knows this, and I'm sure this will come up multiple times here,

01:17:53.480 --> 01:17:57.480
which is, you know, I think we have to find ways to, even now,

01:17:57.480 --> 01:18:02.480
to combine the best of the ideas, the energy, and the resources of academia and industry

01:18:02.480 --> 01:18:06.480
if we want to keep doing basically something interesting, right?

01:18:06.480 --> 01:18:10.480
If we just want to redefine AI to be, well, whatever people currently call AI

01:18:10.480 --> 01:18:14.480
but scaled up, well, then fine, forget about it.

01:18:14.480 --> 01:18:17.480
Or if we just want to say, let me and people like me do what we're doing

01:18:17.480 --> 01:18:21.480
at what industry we consider a snail's pace on toy problems, okay, fine.

01:18:21.480 --> 01:18:26.480
But if we want to, you know, if I want to take what I'm doing to the level

01:18:26.480 --> 01:18:30.480
that will really be, you know, paying off the level that industry can appreciate

01:18:30.480 --> 01:18:34.480
or just that really has technological impact on a broad scale, right?

01:18:34.480 --> 01:18:38.480
Or I think if industry wants to take what it's doing and really build machines

01:18:38.480 --> 01:18:42.480
that are actually intelligent, right, or machine learning that actually learns like a person,

01:18:42.480 --> 01:18:46.480
then I think we need each other now and not just in some point in the future.

01:18:46.480 --> 01:18:50.480
So this is a general challenge for MIT and for everywhere and for Google.

01:18:50.480 --> 01:18:54.480
I mean, we just spent a few days talking to Google about exactly this issue.

01:18:54.480 --> 01:18:57.480
In fact, this was a talk I prepared partly for that purpose.

01:18:57.480 --> 01:19:01.480
So we wanted to raise those issues and it's just, I mean, really, I don't know what,

01:19:01.480 --> 01:19:05.480
I mean, rather I can think of some solutions to that problem

01:19:05.480 --> 01:19:08.480
of what you could call brain drain from the academic point of view

01:19:08.480 --> 01:19:12.480
or what you could call just narrowing in into certain local minima in the industry point of view.

01:19:12.480 --> 01:19:17.480
But they will require the leadership of both academic institutions like MIT

01:19:17.480 --> 01:19:20.480
and companies like Google being creative about how they might work together

01:19:20.480 --> 01:19:22.480
in ways that are a little bit outside of their comfort zone.

01:19:22.480 --> 01:19:27.480
I hope that will start to happen, including at MIT and at many other universities

01:19:27.480 --> 01:19:29.480
and at companies like Google and many others.

01:19:29.480 --> 01:19:33.480
And I think we need it to happen for the health of all parties concerned.

01:19:33.480 --> 01:19:34.480
Okay, thank you very much.

01:19:34.480 --> 01:19:36.480
Thanks.

01:19:36.480 --> 01:19:42.480
I'm curious about sort of the premise that you gave that one of the big gaps missing

01:19:42.480 --> 01:19:49.480
at determining intelligence is the fact that we need to teach machines how to recognize models.

01:19:49.480 --> 01:19:57.480
And I'm curious as to what you think sort of non-goal-oriented cognitive activity comes into play.

01:19:57.480 --> 01:20:03.480
There are things like feelings and emotions and why you don't think

01:20:03.480 --> 01:20:09.480
that might not necessarily be like the most important question.

01:20:09.480 --> 01:20:14.480
The only reason emotions didn't appear on my slide is because there's a few reasons,

01:20:14.480 --> 01:20:16.480
but the slide is only so big.

01:20:16.480 --> 01:20:19.480
I wanted the font to be big, readable for such an important slide.

01:20:19.480 --> 01:20:24.480
I have versions of my slide in which I do talk about that.

01:20:24.480 --> 01:20:27.480
It's not that I think feelings or emotions aren't important.

01:20:27.480 --> 01:20:28.480
I think they are important.

01:20:28.480 --> 01:20:32.480
And I used to not have many insights about what to do about them,

01:20:32.480 --> 01:20:36.480
but actually partly based on some of my colleagues here at MIT, BCS,

01:20:36.480 --> 01:20:41.480
Laura Shultz and Rebecca Sacks, two of my cognitive colleagues who I work closely with,

01:20:41.480 --> 01:20:45.480
they've been starting to do research on how people understand emotions,

01:20:45.480 --> 01:20:49.480
both their own and others, and we've been starting to work with them on computational models.

01:20:49.480 --> 01:20:52.480
So that's actually something I'm actively interested in and even working on.

01:20:52.480 --> 01:20:55.480
But I would say, and again, for those of you who study emotion or know about this,

01:20:55.480 --> 01:20:57.480
actually you're going to have Lisa coming in, right?

01:20:57.480 --> 01:21:00.480
So she's going to basically say a version of the same thing, I think.

01:21:00.480 --> 01:21:03.480
The deepest way to understand, she's one of the world's experts on this,

01:21:03.480 --> 01:21:07.480
the deepest way to understand emotion is very much based on our mental models

01:21:07.480 --> 01:21:10.480
of ourselves, of the situation we're in and of other people, right?

01:21:10.480 --> 01:21:16.480
Think about, for example, all of the different, I mean, if you think about,

01:21:16.480 --> 01:21:19.480
I mean, again, Lisa will talk all about this, but if you think about emotion,

01:21:19.480 --> 01:21:22.480
it's just a very small set of what are sometimes called basic emotions,

01:21:22.480 --> 01:21:29.480
like being happy or angry or sad or, you know, those are small number of them, right?

01:21:29.480 --> 01:21:32.480
There's usually only a few, right?

01:21:32.480 --> 01:21:36.480
You might not say, you might see that as somehow like very basic things

01:21:36.480 --> 01:21:38.480
that are opposed to some kind of cognitive activity.

01:21:38.480 --> 01:21:42.480
But think about all the different words we have for emotion, right?

01:21:42.480 --> 01:21:47.480
For example, think about a famous cognitive emotion like regret.

01:21:47.480 --> 01:21:50.480
What does it mean to feel regret or frustration, right?

01:21:50.480 --> 01:21:55.480
To know both for yourself when you're not just feeling kind of down or negative,

01:21:55.480 --> 01:21:57.480
but you're feeling regret.

01:21:57.480 --> 01:22:01.480
That means something like I have to feel like there's a situation that came out

01:22:01.480 --> 01:22:06.480
differently from how I hoped and I realized I could have done something differently, right?

01:22:06.480 --> 01:22:09.480
So that means you have to be able to understand, you have to have a model.

01:22:09.480 --> 01:22:12.480
You have to be able to do a kind of counterfactual reasoning and to think,

01:22:12.480 --> 01:22:15.480
oh, if only I had acted a different way, then I can predict that the world would have

01:22:15.480 --> 01:22:17.480
come out differently and that's the situation I wanted,

01:22:17.480 --> 01:22:20.480
but instead it came out this other way, right?

01:22:20.480 --> 01:22:24.480
Or think about frustration, again, that requires something like understanding,

01:22:24.480 --> 01:22:27.480
okay, I've tried a bunch of times, I thought this would work,

01:22:27.480 --> 01:22:30.480
but it doesn't seem to be working, maybe I'm ready to give up.

01:22:30.480 --> 01:22:33.480
Those are all, those are very important human emotions.

01:22:33.480 --> 01:22:36.480
We have to understand ourselves, we need that,

01:22:36.480 --> 01:22:38.480
to understand other people, to understand communication,

01:22:38.480 --> 01:22:42.480
but those are all filtered through the kinds of models of action

01:22:42.480 --> 01:22:44.480
that I was, just the ones I was talking about here

01:22:44.480 --> 01:22:46.480
with these say cost-benefit analyses of action.

01:22:46.480 --> 01:22:49.480
So what I'm, so I'm just trying to say I think this is very basic stuff

01:22:49.480 --> 01:22:52.480
that will be the basis for building, I think,

01:22:52.480 --> 01:22:56.480
better engineering-style models of the full spectrum of human emotion

01:22:56.480 --> 01:22:59.480
beyond just like, well, I'm feeling good or bad or scared, okay?

01:22:59.480 --> 01:23:02.480
And I think when you see Lisa, she will, in her own way,

01:23:02.480 --> 01:23:04.480
say something very similar.

01:23:04.480 --> 01:23:06.480
Interesting, thanks.

01:23:06.480 --> 01:23:07.480
Yeah.

01:23:07.480 --> 01:23:09.480
Thanks, Josh, for your nice talk.

01:23:09.480 --> 01:23:13.480
So all is about human cognition and try to build a model to mimic those cognition,

01:23:13.480 --> 01:23:16.480
but you don't, how much could help you to understand

01:23:16.480 --> 01:23:18.480
how the circuit implement those things?

01:23:18.480 --> 01:23:20.480
I mean, like the circuit's in the brain.

01:23:20.480 --> 01:23:21.480
Yeah, and what's the...

01:23:21.480 --> 01:23:23.480
Is that what you work on by any chance?

01:23:23.480 --> 01:23:24.480
Sorry, what?

01:23:24.480 --> 01:23:25.480
Is that what you work on by any chance?

01:23:25.480 --> 01:23:26.480
Yeah, I know.

01:23:26.480 --> 01:23:28.480
I'm kidding, yeah, yeah.

01:23:28.480 --> 01:23:30.480
So in the center for brains, minds, and machines,

01:23:30.480 --> 01:23:32.480
as well as in brain and cognitive science, yeah,

01:23:32.480 --> 01:23:36.480
I have a number of colleagues who study the actual hardware basis

01:23:36.480 --> 01:23:38.480
of this stuff in the brain, and that includes

01:23:38.480 --> 01:23:40.480
like the large-scale architecture of the brain,

01:23:40.480 --> 01:23:42.480
say like what Nancy Camuscher, Rebecca Sacks,

01:23:42.480 --> 01:23:45.480
study with functional brain imaging, or the more detailed circuitry,

01:23:45.480 --> 01:23:48.480
usually requires recording from, say, non-human brains, right,

01:23:48.480 --> 01:23:51.480
at the level of individual neurons and connections between neurons.

01:23:51.480 --> 01:23:53.480
All right, so I'm very interested in those things,

01:23:53.480 --> 01:23:56.480
although it's not mostly what I work on, right?

01:23:56.480 --> 01:23:59.480
But I would say, you know, again, like in many other areas of science,

01:23:59.480 --> 01:24:02.480
certainly in neuroscience, the kind of work I'm talking about here

01:24:02.480 --> 01:24:04.480
in a sort of classic reductionist program

01:24:04.480 --> 01:24:07.480
sets the target for what we might look for.

01:24:07.480 --> 01:24:09.480
Like if I just want to go...

01:24:09.480 --> 01:24:10.480
I mean, I would...

01:24:10.480 --> 01:24:14.480
What I would assert, right, in my working conjecture

01:24:14.480 --> 01:24:17.480
is that if you do the kind of work that I'm talking about here,

01:24:17.480 --> 01:24:19.480
it gives you the right targets

01:24:19.480 --> 01:24:21.480
or gives you a candidate set of targets to look for,

01:24:21.480 --> 01:24:24.480
what are the neural circuits computing, right?

01:24:24.480 --> 01:24:29.480
Whereas if you just go in and just, say, start poking around in the brain

01:24:29.480 --> 01:24:31.480
or have some idea that what you're going to try to do

01:24:31.480 --> 01:24:33.480
is find the neural circuits which underlie behavior,

01:24:33.480 --> 01:24:38.480
without a sense of the computations needed to produce those behaviors,

01:24:38.480 --> 01:24:42.480
I think it's going to be very difficult to know what to look for

01:24:42.480 --> 01:24:45.480
and to know when you've found even viable answers.

01:24:45.480 --> 01:24:50.480
So I think that's the standard kind of reductionist program.

01:24:50.480 --> 01:24:52.480
But it's not...

01:24:52.480 --> 01:24:57.480
I also think it's not one that is divorced from the study of neural circuits.

01:24:57.480 --> 01:25:00.480
It's also one, if you look at the broad picture of reverse engineering,

01:25:00.480 --> 01:25:05.480
it's one where neural circuits and understanding the circuits in the brain

01:25:05.480 --> 01:25:08.480
play an absolutely critical role, okay?

01:25:08.480 --> 01:25:13.480
I would say when you look at the brain at the hardware level as an engineer,

01:25:13.480 --> 01:25:15.480
I'm mostly looking at the software level, right?

01:25:15.480 --> 01:25:18.480
But when you look at the hardware level, there are some remarkable properties.

01:25:18.480 --> 01:25:21.480
One remarkable property, again, is how much parallelism there is

01:25:21.480 --> 01:25:24.480
and in many ways how fast the computations are, okay?

01:25:24.480 --> 01:25:28.480
Neurons are slow, but the computations of intelligence are very fast.

01:25:28.480 --> 01:25:32.480
So how do we get elements that are, in some sense, quite slow in their time constant

01:25:32.480 --> 01:25:34.480
to produce such intelligent behavior so quickly?

01:25:34.480 --> 01:25:37.480
That's a great mystery and I think if we understood that,

01:25:37.480 --> 01:25:40.480
we would have payoff for building all sorts of, you know,

01:25:40.480 --> 01:25:43.480
basically application embedded circuits, okay?

01:25:43.480 --> 01:25:46.480
But also, maybe most important is the power consumption

01:25:46.480 --> 01:25:49.480
and again, many people have noted this, right?

01:25:49.480 --> 01:25:52.480
If you look at the power consumption, the power that the brain consumes,

01:25:52.480 --> 01:25:54.480
like, what did I eat today, okay?

01:25:54.480 --> 01:25:56.480
Almost nothing.

01:25:56.480 --> 01:25:59.480
My daughter, who's, again, she's doing an internship here,

01:25:59.480 --> 01:26:01.480
literally yesterday, all she ate was a burrito.

01:26:01.480 --> 01:26:05.480
And yet, she wrote 300 lines of code for her internship project

01:26:05.480 --> 01:26:08.480
on a really cool computational linguistics project.

01:26:08.480 --> 01:26:13.480
So somehow she turned a burrito into, you know, a model of child language acquisition, okay?

01:26:13.480 --> 01:26:16.480
But how did she do that or how do any of us do this, right?

01:26:16.480 --> 01:26:18.480
Where if you look at the power that we consume, when we simulate

01:26:18.480 --> 01:26:22.480
even a very, very small chunk of cortex on our conventional hardware,

01:26:22.480 --> 01:26:24.480
or we do any kind of machine learning thing,

01:26:24.480 --> 01:26:28.480
we have systems which are very, very, very, very far from the power

01:26:28.480 --> 01:26:30.480
of the human brain computationally,

01:26:30.480 --> 01:26:33.480
but in terms of physical energy consumed,

01:26:33.480 --> 01:26:36.480
way past what any individual brain is doing.

01:26:36.480 --> 01:26:41.480
So how do we get circuitry of any sort, biological or just any physical circuits

01:26:41.480 --> 01:26:44.480
to be as smart as we are with as little energy as we are?

01:26:44.480 --> 01:26:49.480
This is a huge problem for basically every area of engineering, right?

01:26:49.480 --> 01:26:54.480
If you want to have any kind of robot, the power consumption is a key bottleneck.

01:26:54.480 --> 01:26:56.480
Same for self-driving cars.

01:26:56.480 --> 01:27:01.480
If we want to build AI without contributing to global warming and climate change,

01:27:01.480 --> 01:27:05.480
let alone use AI to solve climate change, we really need to address these issues,

01:27:05.480 --> 01:27:09.480
and the brain is a huge guide there, right?

01:27:09.480 --> 01:27:11.480
I think there are some people who are really starting to think about this.

01:27:11.480 --> 01:27:15.480
How can we say, for example, build somehow brain-inspired computers

01:27:15.480 --> 01:27:18.480
which are very, very low power but maybe only approximate?

01:27:18.480 --> 01:27:20.480
So I'm thinking here of Joe Bates.

01:27:20.480 --> 01:27:22.480
I don't know if you know Joe.

01:27:22.480 --> 01:27:25.480
He's been around MIT and other places for quite a while.

01:27:25.480 --> 01:27:27.480
Can I tell them about your company?

01:27:27.480 --> 01:27:31.480
So Joe has a startup in Kendall Square called Singular Computing,

01:27:31.480 --> 01:27:35.480
and they have some very interesting ideas including some actual implemented technology

01:27:35.480 --> 01:27:39.480
for low power approximate computing in a sort of a brain-like way

01:27:39.480 --> 01:27:44.480
that might lead to possibly even like the ability to build something, this is Joe's dream,

01:27:44.480 --> 01:27:47.480
to build something that's about the size of this table but that has a billion cores,

01:27:47.480 --> 01:27:51.480
a billion cores and runs on a reasonable kind of power consumption.

01:27:51.480 --> 01:27:53.480
I would love to have such a machine.

01:27:53.480 --> 01:27:57.480
If somebody wants to help Joe build it, I think he'd love to talk to you.

01:27:57.480 --> 01:28:00.480
But it's one of a number of ideas.

01:28:00.480 --> 01:28:02.480
I mean Google X, people are working on similar things.

01:28:02.480 --> 01:28:06.480
Probably most of the major chip companies are also inspired by this idea.

01:28:06.480 --> 01:28:09.480
And I think even if you didn't think you were interested in the brain,

01:28:09.480 --> 01:28:12.480
if you want to build the kind of AI we're talking about

01:28:12.480 --> 01:28:14.480
and run it on physical hardware of any sort

01:28:14.480 --> 01:28:18.480
and understanding how the brain's circuits compute what they do,

01:28:18.480 --> 01:28:22.480
what I'm talking about with as little power as they do,

01:28:22.480 --> 01:28:24.480
I don't know any better place to look.

01:28:24.480 --> 01:28:28.480
It seems like a lot of the improvements in AI have been driven by increasing

01:28:28.480 --> 01:28:30.480
like computational power.

01:28:30.480 --> 01:28:32.480
How far would you say...

01:28:32.480 --> 01:28:34.480
I mean like GPUs or CPUs.

01:28:34.480 --> 01:28:40.480
How far would you say we are from hardware that could run a general artificial intelligence?

01:28:40.480 --> 01:28:42.480
Of the kind that I'm talking about.

01:28:42.480 --> 01:28:45.480
Yeah I don't know, I'll start with a billion cores and then we'll see.

01:28:45.480 --> 01:28:48.480
I mean I think we're...

01:28:48.480 --> 01:28:51.480
I think there's no way to answer that question in a way that's software independent.

01:28:51.480 --> 01:28:53.480
I don't know how to do that, right?

01:28:53.480 --> 01:28:57.480
But I think that it's...

01:28:57.480 --> 01:29:00.480
And I don't know, like when you say how far are we,

01:29:00.480 --> 01:29:03.480
you mean how far am I with the resources I have right now?

01:29:03.480 --> 01:29:07.480
How far am I if Google decides to put all of its resources at my disposal,

01:29:07.480 --> 01:29:10.480
like they might if I were working at DeepMind?

01:29:10.480 --> 01:29:12.480
I don't know the answer to that question.

01:29:12.480 --> 01:29:16.480
But I think what we can say is this.

01:29:16.480 --> 01:29:20.480
Individual neurons, I mean again this goes back to another reason to study neural circuits.

01:29:20.480 --> 01:29:24.480
If you look at what we currently call neural networks in the AI side,

01:29:24.480 --> 01:29:28.480
the model of a neuron is this very very simple thing, right?

01:29:28.480 --> 01:29:30.480
Individual neurons are not only much more complex,

01:29:30.480 --> 01:29:32.480
but have a lot more computational power.

01:29:32.480 --> 01:29:35.480
It's not clear how they use it or whether they use it.

01:29:35.480 --> 01:29:39.480
But I think it's just as likely that a neuron is something like a real you, right?

01:29:39.480 --> 01:29:41.480
Is that a neuron is something like a computer.

01:29:41.480 --> 01:29:46.480
Like one neuron in your brain is more like a CPU node, okay?

01:29:46.480 --> 01:29:47.480
Maybe.

01:29:47.480 --> 01:29:50.480
And thus the ten billion or trillion, you know,

01:29:50.480 --> 01:29:53.480
the large number of neurons in your brain,

01:29:53.480 --> 01:29:55.480
I think it's like ten billion cortical,

01:29:55.480 --> 01:29:57.480
pyramidal neurons or something,

01:29:57.480 --> 01:29:59.480
might be like ten billion cores, okay?

01:29:59.480 --> 01:30:02.480
For example, that's at least as plausible I think to me as any other estimate.

01:30:02.480 --> 01:30:08.480
So I think we're definitely on the underside with very big error bars.

01:30:08.480 --> 01:30:10.480
So I completely agree that,

01:30:10.480 --> 01:30:12.480
or if this is what you might be suggesting,

01:30:12.480 --> 01:30:14.480
and going back to my answer to your question,

01:30:14.480 --> 01:30:16.480
I don't think we're going to get to what I'm talking about,

01:30:16.480 --> 01:30:18.480
anything like a real brain scale

01:30:18.480 --> 01:30:21.480
without major innovations on the hardware side.

01:30:21.480 --> 01:30:24.480
And you know, it's interesting that what drove those innovations

01:30:24.480 --> 01:30:27.480
that support current AI was mostly not AI.

01:30:27.480 --> 01:30:29.480
It was the video game industry.

01:30:29.480 --> 01:30:32.480
When I point to the video game engine in your head,

01:30:32.480 --> 01:30:36.480
that's a similar thing that was driven by the video game industry on the software side.

01:30:36.480 --> 01:30:40.480
I think we should all play as many video games as we can

01:30:40.480 --> 01:30:42.480
and contribute to the growth of the video game industry.

01:30:43.480 --> 01:30:46.480
Because, no, because I mean, you can see this in very,

01:30:46.480 --> 01:30:48.480
there are companies out there, for example,

01:30:48.480 --> 01:30:50.480
there's a company called Improbable,

01:30:50.480 --> 01:30:52.480
which is a London company,

01:30:52.480 --> 01:30:55.480
London-based startup, a pretty sizable startup at this point,

01:30:55.480 --> 01:30:58.480
which is building something that they call Spatial OS,

01:30:58.480 --> 01:31:00.480
which it's not a hardware idea,

01:31:00.480 --> 01:31:02.480
but it's a kind of software idea

01:31:02.480 --> 01:31:04.480
for very, very big distributed computing environments

01:31:04.480 --> 01:31:06.480
to run much, much more complex,

01:31:06.480 --> 01:31:08.480
realistic simulations of the world

01:31:08.480 --> 01:31:11.480
for much more interesting, immersive, permanent video games.

01:31:11.480 --> 01:31:13.480
I think that's one thing that might, hopefully,

01:31:13.480 --> 01:31:16.480
that will lead to more fun new kinds of games,

01:31:16.480 --> 01:31:19.480
but that's one example of where we might look to that industry

01:31:19.480 --> 01:31:22.480
to drive some of the, you know, just computer systems,

01:31:22.480 --> 01:31:24.480
really hardware and software systems

01:31:24.480 --> 01:31:28.480
that will take our game to the next level.

01:31:29.480 --> 01:31:33.480
Josh, understanding on the algorithmic level or cognitive level

01:31:33.480 --> 01:31:36.480
is just to understanding the meaning of learning

01:31:36.480 --> 01:31:38.480
would be how to predict,

01:31:38.480 --> 01:31:40.480
but on the circuit level it's different.

01:31:40.480 --> 01:31:42.480
On the circuit level?

01:31:42.480 --> 01:31:44.480
Well, of course it's different, right?

01:31:44.480 --> 01:31:47.480
But already, I think you made a mistake there, honestly.

01:31:47.480 --> 01:31:50.480
Like, you said the cognitive level is learning how to predict,

01:31:50.480 --> 01:31:52.480
but I'm not sure what you mean by that.

01:31:52.480 --> 01:31:54.480
There's many things you could mean,

01:31:54.480 --> 01:31:56.480
and what our cognitive science is about

01:31:56.480 --> 01:31:58.480
is learning which of those versions,

01:31:58.480 --> 01:32:00.480
like, I don't think it's learning how to predict.

01:32:00.480 --> 01:32:03.480
I think it's learning what you need to know to plan actions

01:32:03.480 --> 01:32:05.480
and to, you know, all those things.

01:32:05.480 --> 01:32:07.480
Like, it's not just about predicting.

01:32:07.480 --> 01:32:09.480
Because there are things we can imagine

01:32:09.480 --> 01:32:11.480
that make the world different.

01:32:11.480 --> 01:32:13.480
So generalization, sorry, you're not predicting.

01:32:13.480 --> 01:32:15.480
When your model could generalize.

01:32:15.480 --> 01:32:17.480
But especially in the transfer learning that you are interested in.

01:32:17.480 --> 01:32:19.480
A few hundred of neurons in prefrontal cortex

01:32:19.480 --> 01:32:21.480
they could generalize a lot.

01:32:21.480 --> 01:32:25.480
But not kind of a Bayesian model could do that.

01:32:25.480 --> 01:32:28.480
You said, but a Bayesian model won't do that?

01:32:28.480 --> 01:32:30.480
Or they don't do it the way a Bayesian model does?

01:32:30.480 --> 01:32:33.480
For sure, because that's in the abstract level.

01:32:33.480 --> 01:32:35.480
Well, I mean, how do you really know?

01:32:35.480 --> 01:32:38.480
Like, and what does it mean to say that some neurons do it?

01:32:38.480 --> 01:32:40.480
So maybe another way to put this is to say,

01:32:40.480 --> 01:32:42.480
look, we have a certain math that we use

01:32:42.480 --> 01:32:44.480
to capture these, you could call it abstract

01:32:44.480 --> 01:32:46.480
or I call it software level abstractions.

01:32:46.480 --> 01:32:49.480
I mean, all engineering is based on some kind of abstraction.

01:32:49.480 --> 01:32:51.480
But you might have a circuit level abstraction,

01:32:51.480 --> 01:32:53.480
a certain kind of hardware level

01:32:53.480 --> 01:32:55.480
that you're interested in describing the brain at.

01:32:55.480 --> 01:32:57.480
And I'm mostly working out or starting from

01:32:57.480 --> 01:32:59.480
a more software level of abstraction.

01:32:59.480 --> 01:33:01.480
They're all abstractions. We're not talking about molecules here.

01:33:01.480 --> 01:33:03.480
We're talking about some abstract notion

01:33:03.480 --> 01:33:06.480
of maybe a circuit or of a program.

01:33:06.480 --> 01:33:08.480
Now, it's a really interesting question.

01:33:08.480 --> 01:33:10.480
If I look at some circuits, how do I know

01:33:10.480 --> 01:33:12.480
what program they're implementing?

01:33:12.480 --> 01:33:14.480
If I look at the circuits in this machine,

01:33:14.480 --> 01:33:16.480
could I tell what program they're implementing?

01:33:16.480 --> 01:33:18.480
Well, maybe, but certainly it would be a lot easier

01:33:18.480 --> 01:33:20.480
if I knew something about what programs they might be implementing

01:33:20.480 --> 01:33:22.480
before I start to look at the circuitry.

01:33:22.480 --> 01:33:24.480
If I just looked at the circuitry without

01:33:24.480 --> 01:33:26.480
knowing what a program was or what programs

01:33:26.480 --> 01:33:28.480
the thing might be doing or what kind of

01:33:28.480 --> 01:33:30.480
programming components would be

01:33:30.480 --> 01:33:32.480
mappable to circuits in different ways,

01:33:32.480 --> 01:33:34.480
I don't even know how I'd begin to answer that question.

01:33:34.480 --> 01:33:36.480
So I think we've made some progress

01:33:36.480 --> 01:33:38.480
at understanding what neurons

01:33:38.480 --> 01:33:40.480
are doing in certain low-level parts

01:33:40.480 --> 01:33:42.480
of sensory system and certain parts

01:33:42.480 --> 01:33:44.480
of the motor system, like primary motor cortex,

01:33:44.480 --> 01:33:46.480
like basically the parts of the neurons

01:33:46.480 --> 01:33:48.480
that are closest to the inputs and outputs of the brain,

01:33:48.480 --> 01:33:50.480
where we don't...

01:33:50.480 --> 01:33:52.480
Well, you could say we don't need

01:33:52.480 --> 01:33:54.480
the kind of software abstractions

01:33:54.480 --> 01:33:56.480
that I'm talking about or where

01:33:56.480 --> 01:33:58.480
we sort of agree on what those things already are

01:33:58.480 --> 01:34:00.480
so we can make enough progress

01:34:00.480 --> 01:34:02.480
on knowing what to look for and how to know

01:34:02.480 --> 01:34:04.480
when we found it.

01:34:04.480 --> 01:34:06.480
But if you want to talk about flexible planning,

01:34:06.480 --> 01:34:08.480
things that are more like cognition that

01:34:08.480 --> 01:34:10.480
go on in prefrontal cortex,

01:34:10.480 --> 01:34:12.480
at this point I don't think

01:34:12.480 --> 01:34:14.480
that just by recording from those neurons

01:34:14.480 --> 01:34:16.480
we're going to be able to answer those questions

01:34:16.480 --> 01:34:18.480
in a meaningful engineering way.

01:34:18.480 --> 01:34:20.480
A way that any engineer, software, hardware, whatever,

01:34:20.480 --> 01:34:22.480
could really say, yeah, okay, I get it.

01:34:22.480 --> 01:34:24.480
I get those insights in a way that I can engineer with.

01:34:24.480 --> 01:34:26.480
And that's what my goal is.

01:34:26.480 --> 01:34:28.480
So that's my goal to do at the software level,

01:34:28.480 --> 01:34:30.480
the hardware level, or the entire systems level

01:34:30.480 --> 01:34:32.480
connecting them.

01:34:32.480 --> 01:34:34.480
And I think that we can do that

01:34:34.480 --> 01:34:36.480
by taking what we're doing and bringing into contact

01:34:36.480 --> 01:34:38.480
with people studying neural circuits.

01:34:38.480 --> 01:34:40.480
But I don't think you can leave this level out

01:34:40.480 --> 01:34:42.480
and just go straight to the neural circuits.

01:34:42.480 --> 01:34:44.480
And I think the more progress we make,

01:34:44.480 --> 01:34:46.480
the more we can help people who are

01:34:46.480 --> 01:34:48.480
studying at the neural circuit level.

01:34:48.480 --> 01:34:50.480
And they can help us address these other engineering questions

01:34:50.480 --> 01:34:52.480
that we don't really have access to,

01:34:52.480 --> 01:34:54.480
like the power issue or the speed issue.

01:34:54.480 --> 01:34:56.480
Okay, thanks.

01:34:56.480 --> 01:34:58.480
That was great.

01:34:58.480 --> 01:35:00.480
Thank you.

01:35:00.480 --> 01:35:02.480
Thank you.

01:35:02.480 --> 01:35:04.480
Thank you.

