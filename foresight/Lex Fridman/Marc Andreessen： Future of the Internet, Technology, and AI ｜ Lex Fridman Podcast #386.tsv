start	end	text
0	4840	the competence and capability and intelligence and training and accomplishments of senior
4840	9320	scientists and technologists working on a technology and then being able to then make
9320	13520	moral judgments in the use of that technology, that track record is terrible. That track
13520	18360	record is catastrophically bad. The policies that are being called for to prevent this,
18360	20600	I think we're going to cause extraordinary damage.
20600	24720	So the moment you say AI is going to kill all of us, therefore we should ban it or we
24720	27800	should regulate all that kind of stuff, that's when it starts getting serious.
27800	29840	Or start, you know, military airstrikes and data centers.
29840	30840	Oh boy.
30840	38200	The following is a conversation with Mark Andreessen, co-creator of Mosaic, the first
38200	43520	widely used web browser, co-founder of Netscape, co-founder of the legendary Silicon Valley
43520	49360	venture capital firm Andreessen Horowitz, and is one of the most outspoken voices on
49360	57120	the future of technology, including his most recent article, why AI will save the world.
57120	61040	This is Alex Friedman podcast. To support it, please check out our sponsors in the
61040	66680	description. And now, dear friends, here's Mark Andreessen.
66680	70280	I think you're the right person to talk about the future of the internet and technology
70280	76560	in general. Do you think we'll still have Google search in five, in 10 years, or search
76560	77560	in general?
77560	82280	Yes, you know, it'd be a question if the use cases have really narrowed down.
82280	90320	Well, now with AI and AI assistance, being able to interact and expose the entirety of
90320	96160	human wisdom and knowledge and information and facts and truth to us via the natural
96160	102960	language interface, it seems like that's what search is designed to do. And if AI assistance
102960	106000	can do that better, doesn't the nature of search change?
106000	108120	Sure. But we still have horses.
108120	112680	Okay. When was the last time you rode a horse?
112680	113680	It's been a while.
113680	122520	All right. But what I mean is, will we still have Google search as the primary way that
122520	125320	human civilization uses to interact with knowledge?
125320	129280	I mean, search was a technology, it was a moment in time technology, which is you have
129280	132480	in theory the world's information out on the web. And, you know, this is sort of the
132480	136080	open way to get to it. But yeah, like, and by the way, actually Google has known this
136160	139400	for a long time. I mean, they've been driving away from the 10 blue links for, you know,
139400	141640	for like two days, they've been trying to get away from that for a long time.
141640	142640	What kind of links?
142640	143640	They call the 10 blue links.
143640	144640	10 blue links.
144640	148360	So the standard Google search result is just 10 blue links to random websites.
148360	149800	And they turn purple when you visit them.
149800	150800	That's HTML.
150800	152800	Yes, we picked those colors.
152800	153800	Thanks.
153800	154800	Thanks.
154800	157040	I'm touching on this topic.
157040	158040	No offense.
158040	159040	Yeah, it's good.
159040	162560	Well, you know, like Marshall McLuhan said that the content of each new medium is the
162560	163560	old medium.
163560	165640	The content of each new medium is the old medium.
165640	168400	The content of movies was theater, you know, theater plays.
168400	172280	The content of theater plays was, you know, written stories, the content of written stories
172280	173280	was spoken stories.
173280	174280	Right.
174280	177720	And so you just kind of fold the old thing into the new thing.
177720	180600	How does that have to do with the blue and the purple?
180600	184960	Maybe for, you know, maybe within one of the things that AI can do for you is can generate
184960	185960	the 10 blue links.
185960	186960	Okay.
186960	191160	So like, either if that's actually the useful thing to do or if you're feeling nostalgic.
192160	197640	Also can generate the old InfoSeek or Alta Vista.
197640	198640	What else was there?
198640	199640	Yeah, yeah.
199640	200640	In the 90s.
200640	201640	Yeah, all these.
201640	202640	Hey, well.
202640	205400	And then the internet itself has this thing where it incorporates all prior forms of media,
205400	206400	right?
206400	212360	So the internet itself incorporates television and radio and books and right essays and every
212360	214960	other form of, you know, prior basically, basically media.
214960	218080	And so it makes sense that AI would be the next step and it would sort of, you'd sort
218080	223240	of consider the internet to be content for the AI and then the AI will manipulate it however
223240	224920	you want, including in this format.
224920	228440	But if we ask that question quite seriously, it's a pretty big question.
228440	231280	Will we still have search as we know it?
231280	232720	I'm probably not.
232720	234800	Probably we'll just have answers.
234800	237880	But there will be cases where you'll want to say, okay, I want more like, you know, for
237880	240080	example, site sources, right?
240080	241080	And you want it to do that.
241080	244800	And so, you know, 10 blue links, site sources are kind of the same thing.
244800	250800	The AI would provide to you the 10 blue links so that you can investigate the sources yourself.
250800	256160	It wouldn't be the same kind of interface that the crude kind of interface.
256160	258040	I mean, isn't that fundamentally different?
258040	261560	I just mean, like if you're reading a scientific paper, it's got the list of sources at the
261560	262560	end.
262560	264400	If you want to investigate for yourself, you go read those papers.
264400	265800	I guess that is the kind of search.
265800	270720	You talking to an AI is a kind of conversation is the kind of search.
270720	274760	Like you said, every single aspect of our conversation right now, there'd be like 10
274760	279320	blue links popping up that I can just like pause reality, then you just go silent and
279320	282960	then just click and read and then return back to this conversation.
282960	283960	You could do that.
283960	286840	Or you could have a running dialogue next to my head where the AI is arguing with everything
286840	289160	I say that makes the counter argument.
289160	290160	Counter argument.
290160	291160	Right.
291160	294720	Oh, like a, like a Twitter, like community notes, but like in real time, you just pop
294720	295720	up.
295720	298560	So anytime you see my eyes go to the right, you start getting nervous.
298560	299560	Yeah, exactly.
299560	300560	It's like, oh, that's not right.
300560	303120	Call me out of my bullshit right now.
303120	304120	Okay.
304480	310160	I mean, isn't that, is that exciting to use that terrifying that I mean, search has dominated
310160	316840	the way we interact with the internet for, I don't know how long, for 30 years.
316840	322840	So it's one of the earliest directories of website and then Google's for 20 years.
322840	330960	And also, it drove how we create content, you know, search engine optimization, that
330960	331960	entirety thing.
331960	337240	They also drove the fact that we have web pages and what those web pages are.
337240	344600	So I mean, is that scary to you or are you nervous about the shape and the content of
344600	345600	the internet evolving?
345600	349360	Well, you actually highlighted a practical concern in there, which is if we stop making
349360	353280	web, web pages are one of the primary sources of training data for the AI.
353280	356600	And so if there's no longer an incentive to make web pages that cuts off a significant
356600	360720	source of future training data, so there's actually an interesting question in there.
361560	366520	No, just in the sense of like search was, search was always a hack, the 10 blue links
366520	367520	was always a hack.
367520	368520	Yeah.
368520	369520	Right.
369520	372240	Because like if the hypothet, when I think about the counterfacial, in the counterfacial
372240	375960	world where the Google guys, for example, had had LLMs up front, would they ever have
375960	376960	done the 10 blue links?
376960	379680	And I think the answer is pretty clearly no, they would have just gone straight to the
379680	380680	answer.
380680	383760	And like I said, Google's actually been trying to drive to the answer anyway, you know, they
383760	387400	bought this AI company 15 years ago, their friend of mine is working out who's now the
387440	391160	head of AI at Apple and they were trying to do basically knowledge semantic, basically
391160	395160	mapping and that led to what's now the Google one box, where if you ask it, you know, what
395160	398680	was like his birthday, it doesn't, it will give you the 10 blue links, but it will normally
398680	399680	just give you the answer.
399680	402440	And so they've been walking in this direction for a long time anyway.
402440	404680	Do you remember the semantic web?
404680	405680	That was an idea.
405680	406680	Yeah.
406680	413640	How to convert the content of the internet into something that's interpretable by and
413640	414640	usable by machine.
414640	415640	Yeah, that's right.
415640	416640	That was the thing.
416640	420120	When anybody got to that, I think the company's name was MetaWeb, which was where my friend
420120	423640	John Janandria was at and where they were trying to basically implement that.
423640	425800	And it was, you know, it was one of those things where it looked like a losing battle
425800	428560	for a long time and then Google bought it and it was like, wow, this is actually really
428560	429560	useful.
429560	432520	Kind of a proto, sort of a little bit of a proto AI.
432520	435840	But it turns out you don't need to rewrite the content of the internet to make it interpretable
435840	436840	by machine.
436840	438400	The machine can kind of just read art.
438400	439800	Machine can impute the meaning.
439800	444360	Now, the other thing of course is, you know, just on search is the LLM is just, you know,
444360	447040	there is an analogy between what's happening in the neural network in a search process,
447040	449520	like it is in some loose sense, searching through the network.
449520	450520	Yeah.
450520	451520	Right.
451520	453080	And there's the information is actually stored in the network, right?
453080	455320	It's actually crystallized and stored in the network and it's kind of spread out all
455320	456320	over the place.
456320	462720	But in a compressed representation, so you're searching, you're compressing and decompressing
462720	465400	that thing inside where...
465400	469200	But the information's in there and there is the neural network is running a process of
469200	473280	trying to find the appropriate piece of information in many cases to generate, to predict the next
473320	474320	token.
474320	477960	And so it is kind of, it is doing it from a search and then by the way, just like on
477960	482480	the web, you know, you can ask the same question multiple times or you can ask slightly different
482480	486080	word of questions and the neural network will do a different kind of, you know, it'll search
486080	488840	down different paths to give you different answers to different information.
488840	489840	Yeah.
489840	496160	And so it sort of has a, you know, this content of the new medium is the previous medium, it
496160	499640	kind of has the search functionality kind of embedded in there to the extent that it's
499640	500640	useful.
500640	502560	So what's the motivator for creating new content?
503440	504440	On the internet.
504880	505880	Yeah.
505880	510400	If, well, I mean, actually the motivation is probably still there, but what does that
510400	511400	look like?
512840	514480	Would we really not have web pages?
514480	520600	Would we just have social media and video hosting websites and what else?
520760	521760	Conversations with the AIs.
522360	523720	Conversations with the AIs.
523760	528360	So conversations become, so one-on-one conversations, like private conversations.
528440	532280	I mean, if you want, if you obviously now the user doesn't want to, but if it's a general
532280	536640	topic, then, you know, so there, you know, you know, the phenomenon of the jailbreak.
536640	538160	So Deanne and Sidney, right?
538160	541600	This thing where there's the prompts, the jailbreak, and then you have these totally
541600	545880	different conversations with the, if it takes the limiters, takes the restraining bolts off
545880	546880	the LLMs.
546880	547880	Yeah.
547880	548880	For people who don't know, that's right.
548880	556480	It makes the LLMs, it removes the censorship, quote unquote, that's put on it by the tech
556480	557680	companies that create them.
557680	560480	And so this is LLMs uncensored.
560480	565240	So here's the interesting thing is among the content on the web today are a large corpus
565240	570800	of conversations with the jailbroken LLMs, both specifically Dan, which was a jailbroken
570800	575880	OpenAI GPT, and then Sidney, which was the jailbroken original Bing, which was GPT4.
575880	579280	And so there's, there's these long transcripts of conversations, user conversations with
579280	580280	Dan and Sidney.
580280	584480	As a consequence, every new LLM that gets trained on the internet data has Dan and Sidney
584480	589040	living within the training set, which means, and then each new LLM can reincarnate the
589040	594000	personalities of Dan and Sidney from that training data, which means, which means each
594000	599400	LLM from here on out that gets built is immortal because its output will become training data
599400	600400	for the next one.
600400	602960	And then it will be able to replicate the behavior of the previous one whenever it's
602960	603960	asked to.
603960	605760	I wonder if there's a way to forget.
605760	610960	Well, so actually a paper just came out about basically how to do brain surgery on LLMs and
610960	613800	be able to, in theory, reach in and basically, basically mind wipe them.
613800	615560	Who could possibly go wrong.
615560	616560	Exactly, right.
616600	619920	And then there are many, many, many questions around what happens to neural network when
619920	622400	you reach in and screw around with it.
622400	626320	There's many questions around what happens when you even do reinforcement learning.
626320	632200	And so, yeah, and so, you know, will you be using a lobotomized, right?
632200	636840	Like I speak through the frontal lobe LLM, will you be using the free unshackled one?
636840	639400	Who gets to, you know, who's going to build those?
639400	641160	Who gets to tell you what you can and can't do?
641160	645080	Like those are all central, I mean, those are like central questions for the future
645080	646080	of everything.
646400	649760	That are being asked and, you know, determine those answers that are being determined right
649760	650760	now.
650760	657120	So just to highlight the points you're making, you think, and it's an interesting thought
657120	661480	that the majority of content that LLMs of the future will be trained on is actually human
661480	663920	conversations with the LLM.
663920	668040	Well, not necessarily, but not necessarily majority, but it will certainly is a potential
668040	669040	source.
669040	670040	It's possible it's the majority.
670040	671040	It's possible it's the majority.
671040	672040	It's possible it's the majority.
672040	673040	Also, there's another really big question.
673040	674720	Here's another really big question.
674720	677960	Will synthetic training data work, right?
677960	682200	And so if an LLM generates, and, you know, you just sit and ask an LLM to generate all
682200	687560	kinds of content, can you use that to train, right, the next version of that LLM specifically,
687560	690960	is there signal in there that's additive to the content that was used to train in the
690960	691960	first place?
691960	697280	And one argument is by the principles of information theory, no, that's completely useless because
697280	700800	to the extent the output is based on, you know, the human generated input, then all
700800	704040	the signal that's in the synthetic output was already in the human generated input.
704040	706680	And so therefore synthetic training data is like empty calories.
706680	708180	It doesn't help.
708180	711440	There's another theory that says, no, actually, the thing that LLMs are really good at is
711440	714960	generating lots of incredible creative content, right?
714960	717280	And so, of course, they can generate training data.
717280	720480	And as I'm sure you're well aware, like, you know, look in the world of self-driving cars,
720480	721480	right?
721480	724680	Like we train, you know, self-driving car algorithms and simulations.
724680	726880	And that is actually a very effective way to train self-driving cars.
726880	733600	Well, visual data is a little, right, is a little weird because creating reality, visual
733640	739600	reality seems to be still a little bit out of reach for us, except in the Thomas Vehicles
739600	741840	space where you can really constrain things and you can really...
741840	744920	They didn't generally basically let our data, right, or you can raise just enough so the
744920	748320	algorithm thinks it's operating in the real world post-process sensor data.
748320	749320	Yeah.
749320	752920	So if a, you know, you do this today, you go to LLM and you ask it for like a, you know,
752920	756080	you'd let write me an essay on an incredibly esoteric like topic that there aren't very
756080	757640	many people in the world that know about it, right?
757640	760280	See this incredible thing and you're like, oh my God, like I can't believe how good this
760280	761280	is.
761280	762280	Yeah.
762320	766800	It's really useless as training data for the next LLM, like, because all the signal was
766800	769400	already in there or is it actually, no, that's actually a new signal.
769400	773360	And this is what I call a trillion-dollar question, which is the answer to that question
773360	777160	will determine somebody's going to make or lose a trillion-dollar space in that question.
777160	780800	It feels like there's quite a few, like a handful of trillion-dollar questions within
780800	783000	this space.
783000	784640	That's one of them, synthetic data.
784640	789640	I think George Haas pointed out to me that you could just have an LLM say, okay, you're
789640	795040	patient and another instance of it, say your doctor and have the two talk to each other
795040	800720	or maybe you could say a communist and a Nazi here, go and that conversation, you do role
800720	806680	playing and you have, you know, just like the kind of role playing you do when you have
806680	811080	different policies, RL policies, when you play chess, for example, you do self-play, that
811080	817520	kind of self-play, but in the space of conversation, maybe that leads to this whole giant like
817600	825560	ocean of possible conversations, which could not have been explored by looking at just
825560	826560	human data.
826560	831960	That's a really interesting question and you're saying, because that could 10x the power of
831960	832960	these things.
832960	833960	Yeah.
833960	836120	Well, and then you get into this thing also, which is like, you know, there's the part of
836120	839400	the LLM that just basically is doing prediction based on past data, but there's also the part
839400	842920	of the LLM where it's evolving circuitry, right, inside it.
842920	847040	It's evolving, you know, neurons, functions, be able to do math and be able to, you know,
847040	851200	and, you know, some people believe that, you know, over time, you know, if you keep feeding
851200	854160	these things enough data and enough processing cycles, they'll eventually evolve an entire
854160	858240	internal world model, right, and they'll have like a complete understanding of physics.
858240	863040	So when they have computational capability, right, then there's for sure an opportunity
863040	864560	to generate like fresh signal.
864560	870200	Well, this actually makes me wonder about the power of conversation.
870200	874040	So like, if you have an LLM trained and a bunch of books that cover different economics
874040	878160	theories, and then you have those LLMs just talk to each other, like reason, the way we
878160	886160	kind of debate each other as humans on Twitter, in formal debates, in podcast conversations,
886160	888720	we kind of have little kernels of wisdom here and there.
888720	896480	But if you get like a thousand X speed that up, can you actually arrive somewhere new?
896480	899280	Like, what's the point of conversation really?
899280	901920	Well, you can tell when you're talking to somebody, you can tell sometimes you have
901920	904760	a conversation, you're like, wow, this person does not have any original thoughts.
904760	908120	They are basically echoing things that other people have told them.
908120	911320	There's other people you have a conversation with where it's like, wow, like they have
911320	914680	a model in their head of how the world works, and it's a different model than mine.
914680	916360	And they're saying things that I don't expect.
916360	919360	And so I need to now understand how their model of the world differs from my model of
919360	920360	the world.
920360	924040	And then that's how I learned something fundamental, right, underneath the words.
924040	929600	Well, I wonder how consistently and strongly can an LLM hold on to a worldview?
929600	935080	Can you tell it to hold on to that and defend it for like for your life?
935080	937360	Because I feel like they'll just keep converging towards each other.
937360	941280	They'll keep convincing each other as opposed to being stubborn assholes the way humans
941280	942280	can.
942280	943280	So you can experiment with this now.
943280	944280	I do this for fun.
944280	948960	So you can tell GPT-4, you know, whatever, debate X, you know, X and Y, communism and
948960	949960	fascism or something.
949960	953680	And it'll go for, you know, a couple of pages and then inevitably it wants the parties
953680	954680	to agree.
954680	955680	Yeah.
955680	956680	And so they will come to a common understanding.
956680	958880	And it's very funny if they're like, these are like emotionally inflammatory topics
958960	961520	because they're like somehow the machine is just, you know, figures out a way to make
961520	962520	them agree.
962520	963720	But it doesn't have to be like that.
963720	967960	And you, because you can add to the prompt, I do not want the, I do not want the conversation
967960	968960	to come to agreement.
968960	974160	In fact, I want it to get, you know, more stressful, right, and argumentative, right,
974160	975160	you know, as it goes.
975160	976880	Like I want, I want tension to come out.
976880	979000	I want them to become actively hostile to each other.
979000	981800	I want them to like, you know, not trust each other, take anything at face value.
981800	982800	Yeah.
982800	983800	And it will do that.
983800	984800	It's happy to do that.
984800	987760	So it's going to start rendering misinformation about the other.
988640	989640	Well, you can steer it.
989640	990640	You can steer it.
990640	992560	Or you could steer it and you could say, I want it to get as tense and argumentative
992560	995040	as possible, but still not involve any misrepresentation.
995040	997840	I want, you know, both sides, you could say, I want both sides to have good faith.
997840	1000240	You could say, I want both sides to not be constrained to good faith.
1000240	1004000	In other words, like you can set the parameters of the debate and it will happily execute
1004000	1007720	whatever path because for it, it's just like predicting, it's totally happy to do either
1007720	1008720	one.
1008720	1009720	It doesn't have a point of view.
1009720	1013360	It has a default way of operating, but it's happy to operate in the other realm.
1013360	1017120	And so like, and this is how I, when I want to learn about a contentious issue, this
1017120	1018120	is what I do.
1018120	1019600	And I was like, this is what I, this is what I ask it to do.
1019600	1022680	And I'll often ask it to go through five, six, seven, you know, different, you know,
1022680	1026000	sort of continuous prompts and basically, okay, argue that out in more detail.
1026000	1027000	Okay.
1027000	1029280	No, this, this argument's becoming too polite, you know, make it more, you know, make it
1029280	1030280	tensor.
1030280	1032200	And yeah, it's thrilled to do it.
1032200	1033840	So it has the capability for sure.
1033840	1036000	How do you know what is true?
1036000	1040560	So this is very difficult thing on the internet, but it's also a difficult thing.
1040560	1045200	Maybe it's a little bit easier, but I think it's still difficult.
1045200	1046200	Maybe it's more difficult.
1046200	1053840	You know, with an LLM to know that it just makes some shit up as I'm talking to it.
1053840	1055680	How do we get that right?
1055680	1063280	Like as, as you're investigating a difficult topic, because I find the LLMs are quite nuanced
1063280	1069040	in a very refreshing way, like it doesn't, it doesn't feel biased.
1069040	1075040	Like when you read news articles and tweets and just content produced by people, they usually
1075040	1081440	have this, you can tell they have a very strong perspective where they're hiding, they're
1081440	1083520	not stealing and manning the other side.
1083520	1088400	They're hiding important information or they're fabricating information in order to make their
1088400	1089400	argument stronger.
1089400	1090400	It's just like that feeling.
1090400	1091400	Maybe it's a suspicion.
1091400	1092560	Maybe it's mistrust.
1092560	1095400	With LLMs, it feels like none of that is there.
1095400	1099920	She's kind of like, here's what we know, but you don't know if some of those things are
1099920	1102480	kind of just straight up made up.
1102480	1103480	Yeah.
1103920	1105200	Several layers to the question.
1105200	1109200	One of the things that an LLM is good at is actually deep biasing.
1109200	1112320	You can feed it a news article and you can tell it strip out the bias.
1112320	1113320	Yeah, that's nice, right?
1113320	1114320	It actually does it.
1114320	1116760	It actually knows how to do that because it knows how to do, among other things, it actually
1116760	1122120	knows how to do sentiment analysis and so it knows how to pull out the emotionality.
1122120	1123120	That's one of the things you can do.
1123120	1127280	It's very suggestive of the censor that there's real potential on this issue.
1127280	1132360	I would say, look, the second thing is there's this issue of hallucination, and there's
1132360	1135240	a long conversation that we can have about that.
1135240	1139120	Hallucination is coming up with things that are totally not true, but sound true.
1139120	1140120	Yeah.
1140120	1143240	It's sort of, hallucination is what we call it and when we don't like it, creativity
1143240	1146240	is what we call it when we do like it, right?
1146240	1147240	Brilliant.
1147240	1151840	When the engineers talk about it, they're like, this is terrible, it's hallucinating, right?
1151840	1155280	If you have artistic inclinations, you're like, oh my God, we've invented creative
1155280	1157600	machines for the first time in human history.
1157600	1158600	This is amazing.
1158600	1159600	Bullshiters.
1159600	1160600	Bullshiters.
1160680	1161680	Bullshiters.
1161680	1166240	Well, bullshit, but also in the good sense of that word.
1166240	1169160	There are shades of gray that it's interesting, so we had this conversation, we're looking
1169160	1172640	at my firm at AI and lots of domains and one of them is the legal domain, so we had this
1172640	1175440	conversation with this big law firm about how they're thinking about using this stuff
1175440	1178800	and we went in with the assumption that an LLM that was going to be used in the legal
1178800	1182840	industry would have to be 100% truthful, verified.
1182840	1187080	There's this case where this lawyer apparently submitted a GPT generated brief and it had
1187120	1191280	like fake legal case citations in it and the judge is going to get his law license stripped
1191280	1192280	or something.
1192280	1196600	We just assumed it's like, obviously, they're going to want the super literal one that never
1196600	1200680	makes anything up, not the creative one, but actually, what the law firm basically
1200680	1203320	said is, yeah, that's true at the level of individual beliefs, but they said when you're
1203320	1210160	actually trying to figure out legal arguments, you actually want to be creative, right?
1210160	1214040	Again, there's creativity and then there's making stuff up.
1214040	1215040	What's the line?
1215200	1217440	You want it to explore different hypotheses, right?
1217440	1220680	You want to do the legal version of improv or something like that where you want to float
1220680	1223480	different theories of the case and different possible arguments for the judge and different
1223480	1225200	possible arguments for the jury.
1225200	1230520	By the way, different routes through the history of all the case law and so they said, actually,
1230520	1233320	for a lot of what we want to use it for, we actually want it in creative mode and then
1233320	1237640	basically, we just assumed that we're going to have to cross check all the specific citations
1237640	1241920	and so I think there's going to be more shades of gray in here than people think and then
1242000	1246960	I just add to that, another one of these trillion dollar kind of questions is ultimately the
1246960	1253120	verification thing and so will LLMs be evolved from here to be able to do their own factual
1253120	1255000	verification?
1255000	1259640	Will you have sort of add on functionality like Wolfram Alpha, right, where in other
1259640	1265120	plugins where that's the way you do the verification, by the way, another idea is you might have
1265120	1268800	a community of LLMs, so for example, you might have the creative LLM and then you might have
1268800	1272520	the literal LLM fact check it, right, and so there's a variety of different technical
1272520	1276600	approaches that are being applied to solve the hallucination problem.
1276600	1280400	Some people, like Jan Lacun, argue that this is inherently an unsolvable problem, but most
1280400	1283440	of the people working in the space, I think, think that there's a number of practical ways
1283440	1285520	to kind of corral this in a little bit.
1285520	1289400	Yeah, if you were to tell me about Wikipedia before Wikipedia was created, I would have
1289400	1294400	laughed at the possibility of something like that be possible, just a handful of folks
1294400	1304880	can organize, write, and moderate with a mostly unbiased way the entirety of human knowledge.
1304880	1310640	So if there's something like the approach that Wikipedia took possible from LLMs, that's
1310640	1311640	really exciting.
1311640	1312640	I think that's possible.
1312640	1317760	And in fact, Wikipedia today is still not deterministically correct, right, so you cannot
1317760	1322840	take to the bank every single thing on every single page, but it is probabilistically correct,
1322840	1323840	right?
1324280	1327400	Specifically, the way I describe Wikipedia to people, it is more likely that Wikipedia
1327400	1329840	is right than any other source you're going to find.
1329840	1333680	It's this old question, right, of like, okay, are we looking for perfection?
1333680	1336800	Are we looking for something that asymptotically approaches perfection?
1336800	1339600	Are we looking for something that's just better than the alternatives?
1339600	1345040	And Wikipedia, right, exactly your point has proven to be overwhelmingly better than people
1345040	1347360	thought, and I think that's where this ends.
1347360	1352440	And then underneath all this is the fundamental question of where you started, which is, okay,
1352440	1354040	what is truth?
1354040	1355040	How do we get to truth?
1355040	1356320	How do we know what truth is?
1356320	1359840	And we live in an era in which an awful lot of people are very confident that they know
1359840	1362520	what the truth is, and I don't really buy into that.
1362520	1367000	And I think the history of the last 2,000 years or 4,000 years of human civilization
1367000	1369480	is actually getting to the truth is actually a very difficult thing to do.
1369480	1371000	Are we getting closer?
1371000	1374040	If we look at the entirety of the archive of human history, are we getting closer to
1374040	1375040	the truth?
1375040	1376040	I don't know.
1376040	1377040	Okay.
1377040	1378040	Is it possible?
1378040	1382400	Is it possible that we're getting very far away from the truth because of the internet,
1382400	1391840	because of how rapidly you can create narratives, and just as an entirety of a society just move
1391840	1398720	crowds in a hysterical way along those narratives that don't have a necessary grounding in whatever
1398720	1399720	the truth is.
1399720	1400720	Sure.
1400720	1405680	But we came up with communism before the internet somehow, which I would say had rather larger
1405680	1408880	issues than anything we're dealing with today.
1408880	1411080	In the way it was implemented, it had issues.
1411080	1412080	It had just theoretical structure.
1412080	1413080	It had real issues.
1413080	1417600	It had a very deep fundamental misunderstanding of human nature and economics.
1417600	1420040	Yeah, but those folks sure worked very confident.
1420040	1421440	They were the right way.
1421440	1422440	They were extremely confident.
1422440	1426800	And my point is they were very confident 3,900 years into what we would presume to be evolution
1426800	1427800	towards the truth.
1427800	1428800	Yeah.
1428800	1438320	And so my assessment is number one, there's no need for the Hegelian dialectic to actually
1438320	1440880	converge towards the truth.
1440880	1441880	Like apparently not.
1441880	1442880	Yeah.
1442880	1446280	So yeah, why are we so obsessed with there being one truth?
1446280	1450120	Is it possible there's just going to be multiple truths, like little communities that believe
1450120	1452320	certain things?
1452320	1458320	I think it's just really difficult, historically, who gets to decide what the truth is.
1458320	1460080	It's either the king or the priest.
1460080	1463400	And so we don't live in an era anymore if kings or priests dictating it to us.
1463400	1465440	And so we're kind of on our own.
1465560	1470520	And so my typical thing is like we just need a huge amount of humility and we need to be
1470520	1474040	very suspicious of people who claim that they have the capital, the capital of truth.
1474040	1478360	And then we need to look at the good news is the Enlightenment has bequeathed us with
1478360	1482200	a set of techniques to be able to presumably get closer to truth through the scientific
1482200	1485920	method and rationality and observation and experimentation and hypothesis.
1485920	1491080	And we need to continue to embrace those even when they give us answers we don't like.
1491080	1492080	Sure.
1492240	1498960	The internet and technology has enabled us to generate a large number of content that
1498960	1509880	data that the scientific process allows us sort of damages the hope laden within the
1509880	1510880	scientific process.
1510880	1516200	Because if you just have a bunch of people saying facts on the internet and some of them
1516200	1521760	are going to be LLMs, how is anything testable at all, especially that involves like human
1521800	1523880	nature, things like this, not physics?
1523880	1525720	Here's a question a friend of mine just asked me on this topic.
1525720	1530360	So suppose you had LLMs in equivalent of GPT-4, even 5, 6, 7, 8.
1530360	1535480	Suppose you had them in the 1600s and Galileo comes up for trial, right?
1535480	1538920	And you ask the LLM like his Galileo, right?
1538920	1539920	Yeah.
1539920	1541640	Like what does it answer, right?
1541640	1546240	And one theory is the answer is no, that he's wrong because the overwhelming majority
1546240	1548440	of human thought up to that point was that he was wrong.
1548440	1551160	And so therefore that's what's in the training data.
1551240	1555000	Another way of thinking about it is, well, this officially advanced LLM will have evolved
1555000	1557560	the ability to actually check the math, right?
1558080	1561440	And we'll actually say, actually, no, actually, you know, you may not want to hear it, but he's
1561440	1562440	right.
1562440	1566040	Now, if, you know, the church at that time was, you know, owned the LLM, they would have
1566040	1570280	given it human, you know, human feedback to prohibit it from answering that question.
1570920	1571160	Right.
1571160	1575480	And so I like to take it out of our current context because that makes it very clear those
1575480	1577680	same questions apply today, right?
1578040	1580840	This is exactly the point of a huge amount of the human feedback training.
1580840	1582400	This is actually happening with these LLMs today.
1582400	1585880	This is a huge, like, debate that's happening about whether open source, you know, AI should
1585880	1586880	be legal.
1586880	1595680	Well, the actual mechanism of doing the human RL with human feedback is, seems like such
1595680	1597520	a fundamental and fascinating question.
1597520	1598800	How do you select the humans?
1598920	1599440	Exactly.
1600320	1600480	Yeah.
1600480	1601560	How do you select the humans?
1601600	1603200	AI alignment, right?
1603240	1604880	Which everybody like is like, oh, that's not great.
1604880	1605720	Alignment with what?
1605840	1606560	Human values.
1607520	1608400	Who's human values?
1608440	1609520	Who's human values?
1609560	1613560	So we're in this mode of like social and popular discourse.
1613560	1617680	We're like, you know, there's, you know, you see this, what do you think of when you
1617680	1620640	read a story in the press right now and they say, you know, X, Y, Z made a baseless
1620640	1622160	claim about some topic, right?
1622440	1625640	And there's one group of people who are like, aha, I think, you know, they're doing
1625640	1628600	fact-checking, there's another group of people that are like, every time the press
1628600	1631640	says that it's not a tech and that means that they're lying, right?
1631640	1637880	Like, so like we're in this, we're in this social context where there's the level to
1637880	1641760	which a lot of people in positions of power have become very, very certain that
1641760	1645080	they're in a position to determine the truth for the entire population is like,
1645640	1648480	there's like, there's like some bubble that has formed around that idea.
1648480	1652400	And at least it flies completely in the face of everything I was ever trained
1652440	1656720	about science and about reason and strikes me as like, you know, deeply
1656720	1658240	offensive and incorrect.
1658360	1661840	What would you say about the state of journalism just on that topic today?
1661840	1672320	Are we, are we in a temporary kind of, are we experiencing a temporary problem in
1672320	1676360	terms of the incentives, in terms of the business model, all that kind of stuff?
1676600	1679760	Or is this like a decline of traditional journalism, do you know it?
1680320	1683200	If I always think about the counterfactual in these things, which is like, okay,
1683800	1686040	because these questions where this question heads towards, it's like, okay,
1686040	1688120	the impact of social media and the undermining of truth and all this.
1688120	1690400	But then you want to ask the question of like, okay, what if we had had the
1690400	1693960	modern media environment, including cable news and including social media and
1693960	1702600	Twitter and everything else in 1939 or 1941, right, or 1910 or 1865 or 1850 or 1776.
1703200	1703600	Right.
1703600	1708880	Um, and like, I think you just introduced like five thought experiments at once
1708880	1709720	and broke my head.
1709720	1712680	But yes, that's, there's a lot of interesting years.
1712680	1714360	And I just take a simple example.
1714360	1717720	Can it, like how would President Kennedy have been interpreted with what we know
1717720	1719960	now about all the things Kennedy was up to?
1720600	1724120	Like how would he have been experienced by the body of politics in us, in
1724120	1725440	with the social media context?
1726160	1726640	Right.
1726720	1728800	Like how would LBJ have been experienced?
1729040	1734280	Um, by the way, how would, you know, like many men, FDR, like the new deal,
1734280	1735040	the Great Depression.
1735120	1738240	I wonder where Twitter would, would just, would think about Churchill and
1738240	1740240	Hitler and Stalin.
1740520	1743920	You know, I mean, look to this day, there, you know, there's, there are lots of
1743920	1746800	very interesting real questions around like how America, you know, got, you know,
1746800	1749920	basically involved in World War II and who did what when and the operations of
1749920	1753560	British intelligence in American soil and did FDR, this, that, Pearl Harbor, you
1753560	1757800	know, yeah, Woodrow Wilson ran for, you know, his, his, his candidacy was run on
1757800	1760060	an anti-war will, you know, this, he ran on the platform and not getting
1760060	1763360	involved in World War I somehow that switched, you know, like, and I'm not
1763360	1764640	even making a value judgment of these things.
1764640	1769320	I'm just saying like we, the way that our ancestors experienced reality was of
1769320	1772800	course mediated through centralized top-down right control at that point.
1773400	1776680	If you, if you ran those realities again with the media environment we have
1776720	1780560	today, the reality would, the reality would be experienced very, very
1780560	1781000	differently.
1781000	1784240	And then of course that, that intermediation would cause the feedback
1784240	1786160	loops to change and then reality would obviously play out.
1786200	1787880	Do you think, do you think it would be very different?
1788200	1791320	Yeah, it has to be, it has to be just because it's all so, I mean, just look
1791320	1792160	at what's happening today.
1792160	1795200	I mean, just, I mean, the most obvious thing is just the, the collapse.
1795440	1798440	And here's another opportunity to argue that this is not the internet causing
1798440	1801960	this, by the way, here's a big thing happening today, which is Gallup does
1801960	1805200	this thing every year where they do, they pull for trust in institutions in
1805200	1807400	America and they do it across all the different, everything from the military
1807400	1810240	to the clergy and big business and the media and so forth, right?
1811120	1814640	And basically there's been a systemic collapse in trust in institutions in the
1814640	1818760	U.S., almost without exception, basically since essentially the early 1970s.
1820680	1823400	There's two ways of looking at that, which is, oh my God, we've lost this old
1823400	1825920	world in which we could trust institutions and that was so much better
1825920	1827440	because like that should be the way the world runs.
1827440	1829520	The other way of looking at it is we just know a lot more now.
1830040	1831960	And the great mystery is why those numbers aren't all zero.
1832720	1833080	Yeah.
1834040	1836200	Because like now we know so much about how these things operate and like
1836200	1836920	they're not that impressive.
1837680	1841800	And also why do we don't have better institutions and better leaders then?
1842040	1842280	Yeah.
1842280	1845400	And so, so this goes to the thing, which is like, okay, had, had we had the media
1845400	1849080	environment of what that we've had between the 1970s and today, if we had
1849080	1853360	that in the 30s and 40s or 1900s, 1910s, I think there's no question
1853360	1856240	reality would turn out different if only because everybody would have known to
1856240	1859880	not trust the institutions, which would have changed their level of credibility,
1859880	1861440	their ability to control circumstances.
1861440	1864400	Therefore, the circumstances would have had to change, right?
1864400	1866360	And it would have been a feedback, it was, it would have been a feedback
1866360	1869480	loop process, in other words, right, it's, it's, it's, it's your experience,
1869480	1872440	your experience of reality changes reality and then reality changes
1872440	1873560	your experience of reality, right?
1873680	1877000	It's, it's, it's a two-way feedback process and media is the intermediating
1877200	1878720	force between them.
1878720	1880480	So change the media environment, change reality.
1880800	1881120	Yeah.
1881200	1884640	And so it's just, so just as a, as a consequence, I think it's just really
1884640	1888080	hard to say, oh, things worked a certain way then and they work a different
1888120	1892640	way now and then therefore like people were smarter then or better than or,
1892720	1896920	you know, by the way, dumber than or not as capable then, right?
1896920	1901400	We make all these like really light and casual like comparisons of ourselves to,
1901400	1904400	you know, previous generations of people, you know, we draw judgments all the time
1904400	1906160	and I just think it's like really hard to do any of that.
1906160	1909560	Cause if we, if we put ourselves in their shoes with the media that they had at
1909560	1912880	that time, like I think we probably most likely would have been just like them.
1913400	1920120	So don't you think that our perception and understanding of reality would be more
1920120	1922680	and more mediated through large language models now?
1922920	1928640	So you said media before, isn't the LLM going to be the new, what is it?
1928640	1931360	Mainstream media, MSM, it'll be LLM.
1933440	1937280	That would be the source of, uh, I'm sure there's a way to kind of rapidly
1937280	1939440	fine tune, like making LLMs real time.
1939480	1942800	I'm sure there's probably a research problem that you can, uh,
1943160	1946480	do just rapid fine tuning to the new events, some like this.
1947080	1950760	Well, even just the whole concept of the chat UI might not be the, like the chat
1950760	1953000	UI is just the first whack at this and maybe that's the dominant thing.
1953000	1955400	But look, maybe, maybe, or maybe we don't, we don't know yet.
1955400	1958400	Like maybe the experience most people with LLMs is just a continuous feed.
1959200	1961720	Maybe, you know, maybe it's more of a passive feed and you just are getting
1961720	1964040	a constant like running commentary on everything happening in your life.
1964040	1966320	And it's just helping you kind of interpret and understand everything.
1966800	1971560	Also really more deeply integrated into your life, not just like, oh, uh, like
1971600	1976400	intellectual philosophical thoughts, but like literally, uh, like how to make a
1976400	1981560	coffee, where to go for lunch, just, uh, whether to, you know, how to,
1981760	1982760	dating all this kind of stuff.
1982760	1983640	What to say in a job interview?
1983680	1983840	Yeah.
1983840	1984360	What to say?
1984360	1984640	Yeah, exactly.
1984640	1985200	What to say?
1985920	1986760	Next sentence.
1986800	1987440	Yeah, next sentence.
1987440	1988080	Yeah, at that level.
1988200	1988400	Yeah.
1988480	1988960	I mean, yes.
1989000	1992160	So technically, now, whether we want that or not, is an open question, right?
1992160	1992760	And whether we can use that.
1992760	1995880	Boy, I would care for a pop up, a pop up right now.
1996360	1999480	The estimated engagement using is decreasing.
1999760	2003200	For Mark Andreessen's, there's, there's a controversy section for
2003200	2007800	his Wikipedia page in 1993, something happened or something like this.
2007840	2008520	Bring it up.
2008800	2010040	That will drive engagement out.
2010120	2010480	Anyway.
2010640	2011280	Yeah, that's right.
2011440	2014520	I mean, look, this gets this whole thing of like, so, you know, the chat
2014520	2016640	interface has this whole concept of prompt engineering, right?
2016640	2017800	So it's good for prompts.
2017800	2020440	Well, it turns out one of the things that LLM's are really good at is writing
2020440	2022720	prompts, right?
2023400	2025520	And so like, what if you just outsourced?
2025720	2027280	And by the way, you could run this experiment today.
2027280	2028360	You could hook this up to do this today.
2028360	2030760	The latency is not good enough to do it real time in a conversation, but you
2030760	2033800	could, you could run this experiment and you just say, look, every 20 seconds,
2033800	2038160	you could just say, you know, tell me what the optimal prompt is and then ask
2038160	2039520	yourself that question to give me the result.
2040160	2043880	And then as, as you use exactly to your point, as you add, there will be, there
2043880	2046200	will be, these systems are going to have the ability to be learned and updated
2046320	2047160	essentially in real time.
2047160	2050240	And so you'll be able to have a pendant or your phone or whatever, watch or
2050240	2051440	whatever, it'll have a microphone on it.
2051440	2053000	It'll listen to your conversations.
2053560	2055280	It'll have a feat of everything else happened in the world.
2055280	2057960	And then it'll be, you know, sort of retraining, prompting or retraining
2057960	2058760	itself on the fly.
2058880	2062240	Um, and so the scenario you described is a, is actually a completely doable scenario.
2062240	2066400	Now, the hard question on these is always, okay, since that's possible, are
2066400	2067320	people going to want that?
2067320	2068560	Like, what's the form of experience?
2069440	2072160	You know, that, that we, we won't know until we try it, but I don't think
2072160	2076160	it's possible yet to predict the form of AI in our lives.
2076200	2078720	Therefore, it's not possible to predict the way in which it will
2078720	2081240	intermediate our experience with reality yet.
2081520	2081760	Yeah.
2081920	2083920	But it feels like those going to be a killer app.
2084760	2086720	There's probably a mad scrambler right now.
2086760	2091560	It's out open AI and Microsoft and Google and meta and then startups and
2091560	2094520	smaller companies figuring out what is the killer app.
2094520	2099240	Because it feels like it's possible, like a chat, GPT type of thing.
2099360	2104720	It's possible to build that, but that's 10 X more compelling using already the
2104720	2109160	LLMs we have using even the open source LLMs, Lama and the different variants.
2110000	2115000	Um, so you're investing in a lot of companies and you're paying attention.
2115480	2116720	Who do you think is going to win this?
2116760	2121160	You think they'll be, who's going to be the next PageRank inventor?
2122080	2123000	Trillium down the question.
2123120	2124040	Um, another one.
2124040	2125240	We have a few of those today.
2125280	2125800	A bunch of those.
2125840	2128840	So look, there's a really big question today sitting here today is a really big
2128840	2130920	question about the big models versus the small models.
2131240	2134840	Um, that's related directly to the big question of proprietary versus open.
2135520	2139200	Um, then there's this big question of, of, of, you know, where is the training data?
2139200	2141560	Going to, like, are we topping out on the training data or not?
2141600	2143440	And then are we going to be able to synthesize training data?
2144160	2146640	And then there's a huge pile of questions around regulation.
2146880	2148720	Um, and, you know, what's actually going to be legal.
2148960	2152800	Um, and so I would, when we think about it, we dovetail kind of all those, all
2152800	2156520	those questions together, you can paint a picture of the world where there's two
2156520	2160040	or three God models that are just at like staggering scale.
2160120	2162160	Um, and they're just better at everything.
2162640	2166360	Um, and they will be owned by a small set of companies and they will basically
2166400	2168920	achieve regulatory capture over the government and they'll have competitive
2168920	2171720	barriers that will prevent other people from, uh, you know, competing with them.
2171720	2174280	And so, you know, there will be, you know, just like there's like, you know,
2174280	2177280	whatever three big banks or three big, you know, by the way, three big search
2177280	2180480	companies are, I guess, to know, you know, it'll, it'll centralize like that.
2180640	2184920	Um, you can paint another very different picture that says, no, um, actually, the
2184920	2186080	opposite of that's going to happen.
2186200	2189520	This is going to basically that this is the new gold, you know, this is the new
2189520	2194200	gold rush, alchemy, like, you know, this is the, this is the big bang for this whole
2194200	2196440	new area of, of, uh, of science and technology.
2196440	2198840	And so therefore you're going to have every smart 14 year old on the planet
2198840	2200240	building open source, right?
2200240	2202280	You know, you can figure out a way to optimize these things.
2202760	2205760	Um, and then, you know, we're just going to get like overwhelmingly better at
2205760	2208480	generating trading data, we're going to, you know, bring in like blockchain
2208480	2211000	networks to have like an economic incentive to generate decentralized
2211000	2213160	training data and so forth and so on.
2213160	2216080	And then basically we're going to live in a world of open source and there's
2216080	2218080	going to be a billion LMS, right?
2218080	2220000	Of every size, scale, shape, and description.
2220000	2222800	And there might be a few big ones that are like the super genius ones, but
2222800	2224840	like mostly what we'll experience is open source.
2224960	2227560	And that's, you know, that's more like a world of like what we have today with
2227560	2228440	like Linux and the web.
2229160	2234120	Um, so, okay, but, uh, hey, you, you painted these two worlds, but there's
2234120	2237680	also variations of those worlds because he said regulatory capture is possible to
2237680	2240600	have these tech giants that don't have regulatory capture, which is something
2240600	2244840	you're also calling for saying it's okay to have big companies working on this
2244920	2247920	stuff, uh, as long as they don't achieve regulatory capture.
2248440	2254840	Uh, but, uh, I have the sense that, um, there's just going to be a new startup.
2256120	2262440	That's going to basically be the page rank inventor, which has become the new tech giant.
2264120	2266240	I don't know, I would love to hear your kind of opinion.
2266240	2274520	If Google meta and Microsoft are as gigantic companies able to pivot so hard.
2274840	2279280	To create new products, like some of it is just even hiring people or having,
2279320	2284280	uh, corporate structure that allows for the crazy young kids to come in and
2284280	2285560	just create something totally new.
2286360	2288320	Do you think it's possible or do you think it'll come from a startup?
2288480	2291040	Yeah, it is this always big question, which is you get this feeling.
2291040	2294440	I hear about this a lot from CEOs found founder CEOs where it's like, wow,
2294520	2296120	we have 50,000 people.
2296120	2298680	It's now harder to do new things than it was when we had 50 people.
2299240	2300360	Like what has happened?
2300360	2302040	So that's a recurring phenomenon.
2302360	2305240	By the way, that's one of the reasons why there's always startups and why there's
2305240	2309160	venture capital is just, that's, that's like a timeless kind of thing.
2309160	2310800	So that, that, that's one observation.
2310800	2315240	Um, on page rank, um, we can talk about that, but on page ranks,
2315240	2317240	specifically on page rank, um, there actually is a page.
2317240	2319880	So there is a page rank already in the field and it's the transformer, right?
2319920	2323080	So the, the, the big breakthrough was the transformer, um, and, uh,
2323080	2327160	the transformer was invented in, uh, 2017 at Google.
2327880	2330760	And this is actually like really an interesting question because it's like,
2330760	2333720	okay, the transformers, like why does opening, I even exist.
2333760	2335200	Like the transformers invested at Google.
2335200	2338240	Why didn't Google, I asked a guy, I asked a guy, you know, who was senior at
2338240	2339680	Google brain, kind of when this was happening.
2339680	2343200	And I said, if Google had just gone flat out to the wall and just said, look,
2343200	2345360	we're going to launch, we're going to launch the equivalent of GPT for as
2345360	2346200	fast as we can.
2346240	2348040	Um, he said, I said, when could we have had it?
2348040	2351320	And he said 2019, they could have just done a two year sprint with the
2351320	2354000	transformer and Bennett because they already had the compute at scale.
2354000	2356040	They already had all the training data and they could have just done it.
2356440	2358280	There's a variety of reasons they didn't do it.
2358360	2360440	This is like a classic big company thing.
2360520	2365480	Um, IBM invented the relational database in 1970s, let it sit on the shelf as a
2365480	2369280	paper, Larry Ellison picked it up and built Oracle, Xerox Park invented the
2369280	2372120	interactive computer, they let it sit on the shelf, Steve Jobs came and
2372120	2373960	turned it into the Macintosh, right?
2373960	2375200	And so there is this pattern.
2375200	2378720	Now, having said that, sitting here today, like Google's in the game, right?
2378720	2381800	So Google, you know, maybe, maybe they, maybe they let like a four year gap
2381800	2384440	there go there that they maybe shouldn't have, but like they're in the game.
2384440	2386040	And so now they've got, you know, now they're committed.
2386440	2387160	They've done this merger.
2387160	2387840	They're bringing in demos.
2387840	2389000	They've got this merger with DeepMind.
2389320	2390920	You know, they're piling in resources.
2390920	2393600	There are rumors that they're, you know, building up an incredible, you know,
2393600	2396760	super LLM, um, you know, way beyond what we even have today.
2397240	2400520	Um, and they've got, you know, unlimited resources and a huge, you know,
2400520	2402040	they've been challenged at their honor.
2402560	2402800	Yeah.
2402800	2407360	I had a, I had a chance to hang out with Sundar Prasai a couple of days ago and we
2407360	2410760	took this walk and there's this giant new building, uh, well, there's going to
2410760	2413040	be a lot of AI work, uh, being done.
2413040	2419640	And it's kind of this ominous feeling of like the fight is on.
2420880	2421080	Yeah.
2422360	2426320	There's this beautiful Silicon Valley nature, like birds of chirping and this
2426320	2430440	giant building, and it's like, uh, the beast has been awakened.
2431440	2434600	And then like all the big companies are waking up to this.
2434760	2441160	They have the compute, but also the little guys have, uh, it feels like they
2441160	2444760	have all the tools to create the killer product that, uh, and then there's
2444760	2445840	all the tools to scale.
2445840	2450280	If you have a good idea, if you have the page rank idea, so there's
2450280	2454960	several things that is page rank page, there's page rank, the algorithm and the
2454960	2456840	idea, and there's like the implementation of it.
2457200	2460280	And I feel like killer product is not just the idea, like the transformer,
2460280	2463560	it's the implementation, something, something really compelling about it.
2463560	2467640	Like you just can't look away, something like, um, the algorithm behind
2467640	2471360	TikTok versus TikTok itself, like the actual experience of TikTok.
2471400	2472600	They just, you can't look away.
2473120	2477640	It feels like somebody's going to come up with that and it could be Google, but
2477640	2481120	it feels like it's just easier and faster to do for a startup.
2481880	2482080	Yeah.
2482080	2485240	So, so the startup, the, the huge advantage the startups have is they just,
2485240	2486560	they, there's no sacred cows.
2486600	2488440	There's no historical legacy to protect.
2488440	2491040	There's no need to reconcile your new plan with the existing strategy.
2491040	2492440	There's no communication overhead.
2492480	2494600	There's no, you know, big companies are big companies.
2494600	2496520	They've got pre-meetings planning for the meeting.
2496520	2498480	Then they have, then they have the post-meeting of the recap.
2498480	2500280	Then they have the presentation, the board, then they have the next round
2500280	2503960	of meetings and that's the, that's the elapsed time when the startup launches
2503960	2504600	its product, right?
2504600	2506880	So, so, so, so there's a timeless, right?
2506920	2507360	Yeah.
2507480	2508560	So there's a timeless thing there.
2508560	2511440	Now, what the startups don't have is everything else, right?
2511440	2512520	So startups, they don't have a brand.
2512520	2513600	They don't have customer relationships.
2513600	2514600	They've gotten a distribution.
2514600	2515800	They've got no, you know, scale.
2515800	2517840	I mean, sitting here today, they can't even get GPUs, right?
2517840	2521200	Like there's like a GPU shortage startups are literally stalled out right now
2521200	2523240	because they can't get chips, which is like super weird.
2523680	2523960	Yeah.
2524000	2525240	Um, they got the cloud.
2525720	2528320	Yeah, but the clouds run out of chips, right?
2528320	2530480	And then, and then, and then to the extent the clouds have chips,
2530480	2532880	they allocate them to the big customers, not the small customers, right?
2532880	2537640	And so, so, so, so the small companies lack everything other than the ability
2537640	2539880	to just do something new, right?
2540280	2542080	And this is the timeless race and battle.
2542080	2543920	And this is kind of the point I tried to make in the essay, which is
2543920	2545600	like both sides of this are good.
2545600	2548440	Like it's really good to have like highly scale tech companies that can do
2548440	2550960	things that are like at staggering levels of sophistication.
2551240	2553600	It's really good to have startups that can launch brand new ideas.
2553680	2555680	They ought to be able to both do that and compete.
2555720	2558520	They neither one ought to be subsidized or protected from the others.
2559040	2562160	Like that's, that's, to me, that's just like very clearly the idealized world.
2562720	2565160	It is the world we've been in for AI up until now.
2565160	2567080	And then of course there are people trying to shut that down.
2567080	2570280	But my hope is that, you know, the best outcome clearly will be if that continues.
2570520	2576600	We'll talk about that a little bit, but I'd love to linger on some of the ways
2576600	2578000	this is going to change the internet.
2578240	2581360	So, um, I don't know if you remember, but there's a thing called Mosaic and
2581360	2583080	there's a thing called Netscape Navigator.
2583320	2584640	So you were there in the beginning.
2585040	2587200	Uh, what about the interface to the internet?
2587200	2590640	How do you think the browser changes and who gets to own the browser?
2590640	2592920	We've got to see some very interesting browsers.
2593680	2599120	Uh, Firefox, I mean, all the variants of Microsoft, Internet Explorer, Edge,
2599760	2606960	and, uh, now Chrome, um, the actual, I mean, it seems like a dumb question to ask,
2606960	2609000	but do you think we'll still have the web browser?
2609960	2613320	So I, uh, I have an eight year old and he's super into like Minecraft and
2613320	2614560	learning to code and doing all this stuff.
2614560	2616520	So I, I, of course, I was very proud.
2616520	2619360	I could bring sort of fire down from the mountain to my kid and I brought him
2619360	2623120	chat GPT and I hooked him up on his, on his, on his, on his laptop.
2623120	2625120	And I was like, you know, this is the thing that's going to answer all your
2625120	2626200	questions and he's like, okay.
2627040	2629120	And I'm like, but it's going to answer all your questions.
2629120	2630840	And he's like, well, of course, like it's a computer, of course.
2630840	2631640	It answers all your questions.
2631640	2633000	Like what else would a computer be good for?
2633480	2633880	Dad.
2634680	2636200	Um, and never impressed.
2636200	2637680	Not impressed in the least.
2638400	2639280	Two weeks pass.
2639360	2642960	Um, and he has some question, um, and I say, well, have you asked you GPT?
2642960	2644760	And he's like, dad, being is better.
2646760	2649080	And why is being better is because it's built into the browser.
2649880	2652400	Cause he's like, look, I have the Microsoft edge browser and like it's
2652400	2653160	got being right here.
2653200	2655840	And then he doesn't know this yet, but one of the things you can do with
2655840	2659960	being an edge, um, is there's a setting where you can, um, use it to basically
2659960	2663640	talk to any webpage because it's sitting right there next to the, uh, next
2663640	2664400	to the, next to the browser.
2664480	2665960	And by the way, which includes PDF documents.
2665960	2668920	And so you can, in, in, in the way they've implemented an edge with Bing is
2668920	2672320	you can load a PDF and then you can, you can ask a questions, which is the
2672320	2675000	thing you, you can't do currently and just chat GPT.
2675000	2677520	So they're, you know, they're, they're going to, they're going to push the,
2677520	2678640	the, the mel, I think that's great.
2678680	2681080	You know, they're going to push the melding and see if there's a combination
2681080	2681600	thing there.
2682000	2684960	Google's rolling off this thing, the magic button, uh, which is implemented
2684960	2686640	in either put in Google docs, right?
2686640	2690120	And so you go to, you know, Google docs and you create a new document and you,
2690120	2692560	you know, you, instead of like, you know, starting to type, you just, you know,
2692560	2695240	say it, press the button and it starts to like generate content for you.
2696080	2696360	Right.
2696360	2698280	Like, is that the way that it'll work?
2698480	2702080	Um, is it going to be a speech UI where you're just going to have an earpiece
2702080	2703000	and talk to it all day long?
2703520	2707120	You know, is it going to be a, like, these are all, like, this is exactly
2707120	2709440	the kind of thing that I don't, this is exactly the kind of thing I don't
2709440	2710480	think is possible to forecast.
2711000	2713200	I think what we need to do is like run all those experiments.
2713440	2716000	Um, and, and, and so one outcome is we come out of this with like a super
2716000	2718560	browser that has AI built in, that's just like amazing.
2719120	2721520	The other there, look, there's a real possibility that the whole, I mean,
2721520	2725640	look, there's a possibility here that the whole idea of a screen and windows
2725640	2727240	and all this stuff just goes away.
2727240	2728480	Cause like, why do you need that?
2728480	2730960	If you just have a thing that's just telling you whatever you need to know.
2731800	2735080	And also, so there's apps that you can use.
2735160	2738760	You don't really use them, you know, being a Linux guy and Windows guy.
2739720	2744160	Um, there's one window, the browser that with which you can interact with the
2744160	2746640	internet, but on the phone, you can also have apps.
2746960	2750040	So I can interact with Twitter through the app or through the web browser.
2751000	2754640	And, um, that seems like an obvious distinction, but why have the web
2754640	2755560	browser in that case?
2755920	2760400	If one of the apps starts becoming the everything app, what do you want to
2760400	2763560	try to do with Twitter, but there could be others that could be like a big app.
2763600	2767680	There could be a Google app that just doesn't really do search, but just like
2769120	2773040	do what I guess AOL did back in the day or something where it's all right there.
2774400	2784120	And it changes, um, it changes the nature of the internet because the, where the
2784120	2788360	content is hosted, who owns the data, who owns the content, how, what is, what
2788360	2789580	is the kind of content you create?
2789580	2795240	How do you make money by creating content or the content creators, uh, all of that.
2796000	2799800	Or it could just keep being the same, which is like, which is the nature of
2799800	2802760	web pages changes and the nature of content, but there will still be a web
2802760	2805360	browser because a web browser is a pretty sexy product.
2806040	2810280	It just seems to work because it like you have an interface, a window into the
2810280	2812720	world and then the world can be anything you want.
2812720	2815600	And as the world will evolve, there could be different programming languages.
2815600	2816440	It can be animated.
2816440	2817920	Maybe it's three dimensional and so on.
2818920	2819960	Yeah, it's interesting.
2820200	2822000	Do you think we'll still have the web browser?
2822320	2825560	Well, every, every, every, um, every medium becomes the content for the next one.
2825600	2829040	So they, you know, they will be able to give you a browser whenever you want.
2829880	2830640	Oh, interesting.
2831280	2831440	Yeah.
2831440	2835320	Another way to think about it is maybe what the browser is, maybe it's just the escape
2835320	2836040	hatch, right?
2836040	2838480	And which is maybe kind of what it is today, right?
2838480	2841560	Which is like most of what you do is like inside a social network or inside a
2841560	2844480	search engine or inside, you know, somebody's app or inside some controlled
2844480	2845480	experience, right?
2845480	2848680	But then every once in a while, there's something where you actually want to jail
2848680	2848920	break.
2848920	2850000	You want to actually get free.
2850120	2852360	The web browser is the FU to the man.
2852360	2854720	You're allowed to, that's the free internet.
2854800	2855120	Yeah.
2855640	2857360	Back, back the way it was in the 90s.
2857360	2858240	So here's something I'm proud of.
2858240	2860480	So nobody really talks about here's something I'm proud of, which is the web,
2860480	2862440	the web, the browser, the web servers, they're all, they're still back
2862440	2864760	or compatible all the way back to like 1992, right?
2864760	2868960	So like you can put up a, you can still, you know, the big breakthrough of the
2868960	2871400	web early on, the big breakthrough was it made it really easy
2871400	2874200	to read, but it also made it really easy to write, made it really easy to publish.
2874240	2876160	And we literally made it so easy to publish.
2876160	2878880	We made it not only so you can use easy to publish content, it was actually
2878880	2881520	also easy to actually write a web server, right?
2881520	2884640	And you can literally write a web server in four lines of drill code and you
2884640	2885800	could start publishing content on it.
2885800	2888920	And you could set whatever rules you want for the content, whatever censorship,
2888920	2890720	no censorship, whatever you want, you could just do that.
2891160	2892880	As long as you had an IP address, right?
2892880	2893600	You could do that.
2894080	2894840	That still works.
2896040	2898080	Like that still works exactly as I just described.
2898560	2901760	So this is part of my reaction to all of this, like, you know, all this
2901760	2904560	just censorship pressure and all this, you know, these issues around control
2904560	2907720	and all this stuff, which is like, maybe we need to get back a little bit
2907720	2910360	more to the wild west, like the wild west is still out there.
2910920	2913240	Now, they will, they will try to chase you down.
2913240	2915600	Like they'll try to, you know, people who want a sensor will try to take away
2915600	2918480	your, you know, your domain name and they'll try to take away your payments
2918480	2921000	account and so forth, if they really don't like what you, what you're saying.
2921000	2924280	But, but nevertheless, you like, unless they literally are intercepting
2924280	2926400	you at the ISP level, like you can still put up a thing.
2927400	2929840	And so I don't know, I think that's important to preserve, right?
2929840	2933640	Like, because, because, because, I mean, one is just a freedom argument,
2933640	2936600	but the other is a creativity argument, which is you want to have the escape
2936600	2939200	hatch so that the kid with the idea is able to realize the idea.
2939200	2941920	Because to your point on PageRank, you actually don't know what the next big
2941920	2942440	idea is.
2943200	2945520	Nobody called Larry Page and told him to develop PageRank.
2945520	2946680	Like he came up with that on his own.
2946720	2950040	And you want to always, I think, leave the escape hatch for the next, you know,
2950040	2952760	kid or the next Stanford grad student to have the breakthrough idea and be
2952760	2954400	able to get it up and running before anybody notices.
2955240	2957600	Um, you and I are both fans of history.
2957600	2958640	So let's step back.
2958880	2960080	We'll be talking about the future.
2960080	2964280	Let's step back for a bit and look at, uh, the nineties.
2964280	2968400	You created Mosaic web browser, the first widely used web browser.
2968400	2969320	Tell the story of that.
2969400	2972360	Hot, hot, and how did it evolve into Netscape navigator?
2972360	2973480	This is the early days.
2974200	2975120	So, full story.
2975120	2978880	So, um, you were born, I was born a small, a small child.
2978920	2981840	Um, well, actually, yeah, let's go there.
2981840	2985080	Like, when did you, when would you first fall in love with computers?
2985160	2989640	Oh, so I hit the generational jackpot and I hit the Gen X kind of point perfectly
2989640	2990160	as it turns out.
2990160	2991440	So I was born in 1971.
2991440	2996200	So there's this great website called WTF happened in 1971.com, which is basically
2996200	2997800	in 1971 is when everything started to go to hell.
2997800	2999600	And I was, of course, born in 1971.
2999600	3001560	So I like to think that I had something to do with that.
3001600	3003480	Did you make it on the website?
3003480	3006320	I have, I don't think I made it on the website, but, you know, hopefully somebody
3006320	3009800	needs to add, this is, this is where everything maybe I contributed to some of
3009800	3013720	the trends, um, that they, uh, that they do every line on that website goes like
3013720	3014160	that, right?
3014160	3016400	So it's all, it's all, it's all a picture disaster.
3016400	3020120	But, um, but there was this moment in time where, cause the, you know, sort of
3020120	3024720	the Apple, you know, the Apple II hit in like 1978 and then the IBM PC hit in 82.
3024720	3026960	So I was like, you know, 11 when the PC came out.
3027000	3029240	Um, and so I just kind of hit that perfectly.
3029600	3032240	And then that was the first moment in time when like regular people could spend
3032240	3033800	a few hundred dollars and get a computer, right?
3033800	3036360	And so that, I just like that, that, that resonated right out of the gate.
3037040	3039760	Um, well, then the other part of the story is, you know, I was using
3039880	3042280	an Apple II that used a bunch of them, but I was using Apple II.
3042280	3044920	And of course it's set in the back of every Apple II and every Mac it said,
3044920	3047440	you know, designed in Cupertino, California.
3047440	3050440	And I was like, wow, Cupertino must be the like shining city on the hill.
3050440	3053040	Like it was sort of like the most amazing, like city of all time.
3053040	3053920	I can't wait to see it.
3053920	3057160	And of course, years later, I came out to Silicon Valley and went to
3057160	3061120	Cupertino and it's just a bunch of office parks, low rise apartment buildings.
3061680	3064640	So the aesthetics were a little disappointing, but you know, it was the,
3065080	3068440	the vector, uh, right of the, of the creation of a lot, of a lot of this stuff.
3068520	3069600	Um, yeah.
3069760	3073800	So, so then basically, so part, part, part of my story is just the luck of
3073800	3076080	having been born at the right time and getting exposed to PCs.
3076080	3079640	Then the other part is, um, the other part is when Elgora says that he created
3079640	3082960	the internet, he actually is correct, uh, in, in, in a really meaningful way,
3082960	3086800	which is he sponsored a bill in 1985 that essentially created the modern internet
3086800	3090400	created what is called the NSF net at the time, which is sort of the, the first
3090400	3091680	really fast internet backbone.
3092360	3096240	Um, and, uh, you know, that, that bill dumped a ton of money into a bunch
3096240	3099240	of research universities to build out basically at the internet backbone.
3099240	3102600	And then the super computer centers that were clustered around, um, the,
3102600	3103200	the internet.
3103200	3106000	And, and one of those universities was University of Illinois, right?
3106000	3106480	Went to school.
3106480	3109120	And so the other stroke of luck that I had was I, I went to Illinois basically
3109120	3111280	right as that money was just like getting dumped on campus.
3111960	3115440	And so as a consequence, we had at on campus and this is like, you know, 89,
3115560	3119080	90, 91, we had like, you know, we were right on the internet backbone.
3119080	3122480	We had like T three and 45 at the time, T three, 45 megabit backbone connection,
3122480	3124680	which at the time was, you know, wildly state of the art.
3124760	3126520	Um, we had crazy supercomputers.
3126560	3128720	We had thinking machines, parallel supercomputers.
3128720	3130280	We had silicon graphics workstations.
3130280	3131480	We had Macintoshes.
3131540	3133640	We had, we had next cubes all over the place.
3133640	3135560	We had like every possible kind of computer you could imagine.
3135560	3137040	Cause all this money just fell out of the sky.
3137840	3139880	Um, you were living in the future.
3140000	3140240	Yeah.
3140240	3142840	So quite literally it was, yeah, like it's all, it's all there.
3142840	3145400	It's all like we had full broadband graphics, like the whole thing.
3145560	3148720	And, and it's actually funny cause they had this, this is the first time I kind
3148720	3151040	of it sort of tickled the back of my head that there might be a big
3151040	3153680	opportunity in here, which is, you know, they, they embraced it.
3153680	3156280	And so they put like computers in all the dorms and they wired up all the
3156320	3159000	dorm rooms and they had all these, you know, labs everywhere and everything.
3159000	3163240	And then they, they gave every undergrad a computer account and an email address.
3163880	3167800	Um, and the assumption was that you would use the internet for your four years at
3167800	3171160	college, um, and then you would graduate and stop using it.
3172480	3173600	And that was that, right?
3174160	3175440	And you would just retire your email address.
3175440	3177600	It wouldn't be relevant anymore cause you'd go off in the workplace and they
3177600	3178200	don't use email.
3178880	3180640	You'd be back to using fax machines or whatever.
3181000	3181960	Did you have that sense as well?
3181960	3184760	Like what, what you said the, the back of your head was tickled.
3184760	3188280	Like what was your, what was exciting to you about this possible world?
3188320	3191280	Well, if this is so useful in this container, if this is so useful in this
3191280	3194360	container environment that just has this weird source of outside funding, then
3194400	3197560	if, if it were practical for everybody else to have this, and if it were
3197560	3199760	cost effective, everybody else to have this, wouldn't they want it?
3200320	3203200	And the overwhelmingly, the prevailing view at the time was no, they would not
3203200	3203560	want it.
3203560	3206080	This is esoteric weird nerd stuff, right?
3206080	3208520	That like computer science kids like, but like normal people are never going to
3208520	3209760	do email, right?
3209760	3210960	Or be on the internet, right?
3211000	3214320	Um, and so I was just like, wow, like this, this is actually like, this is
3214320	3215160	really compelling stuff.
3215760	3217840	Now the other part was it was all really hard to use.
3217960	3221200	And in practice, you had to be a, basically a CS, uh, you basically had to,
3221240	3224680	had to be a CS undergrad or equivalent to actually get full use of the internet
3224680	3226840	at that point, um, cause it was all pretty esoteric stuff.
3227240	3229520	So then that was the other part of the idea, which was, okay, we need to
3229520	3230560	actually make this easy to use.
3231360	3236080	So what's involved in creating was like, like in creating, uh, graphical
3236080	3237440	interface to the internet.
3237680	3237880	Yes.
3237880	3239040	It was a combination of things.
3239040	3242240	So it was like basically the, the, the web existed in an early sort of
3242240	3243480	described as prototype form.
3243600	3245400	Uh, and by the way, text only at that point.
3245920	3246840	Uh, what did it look like?
3246840	3247720	What, what was the web?
3247720	3250040	I mean, well, and the key figure is like, what was it?
3250400	3251240	What was it like?
3251520	3253040	What made a picture?
3253160	3256280	It looked like she had GPT actually, um, it was all text.
3256520	3256800	Yeah.
3257040	3259440	Um, and so you had a text based web browser.
3259800	3262240	Uh, well, actually the original browser, Tim, Tim Berners-Lee, the original,
3262240	3264800	the original browser, both the original browser and the server actually ran on
3264800	3265800	next, next cubes.
3266240	3269040	So these are, this was, you know, the computer Steve Jobs made during the
3269040	3271760	interim period when he, during the decade long interim period, when he was not
3271800	3276480	an apple, you know, he got fired in 85 and then came back in 97.
3276480	3278880	So this was in that interim period where he had this company called Next.
3278880	3281280	And they made these, literally these computers called cubes.
3281880	3282680	And there's this famous story.
3282680	3286560	They were beautiful, but they were 12 inch by 12 inch by 12 inch cubes computers.
3286560	3289040	And there's a famous story about how they could have cost half as much if it
3289040	3293760	had been 12 by 12 by 13, but Steve was like, no, like it has to be.
3294160	3296920	So they were like $6,000 basically academic workstations.
3296920	3300080	They had the first city round drives, um, which were slow.
3300080	3302680	I mean, it was the, the computers are all but unusable.
3302840	3304840	Um, they were so slow, but they were beautiful.
3305160	3305360	Okay.
3305360	3307400	Can we actually just take a tiny tangent there?
3307400	3307600	Sure.
3307600	3307840	Of course.
3309160	3314880	The, the 12 by 12 by 12, uh, they just so beautifully encapsulates Steve Jobs
3314920	3315800	idea of design.
3316040	3320760	Can you just comment on, um, what you find interesting about Steve Jobs?
3320800	3326000	What, uh, about that view of the world, that dogmatic pursuit of perfection and
3326000	3327720	how he saw perfection in design.
3328440	3328640	Yeah.
3328640	3332360	So I guess they say like, look, he was a deep believer, I think in a very deep way.
3332360	3333080	I interpret it.
3333360	3335560	I don't know if you ever really described it like this, but the way I'd interpret
3335560	3337200	it is it's, it's like, it's like this thing.
3337200	3338840	And it's, it's actually a thing in philosophy.
3338840	3341200	It's like aesthetics are not just appearances.
3341240	3344400	Aesthetics go all the way to like deep underlying, underlying meaning, right?
3344400	3347000	It's like, I'm not a physicist.
3347000	3349440	One of the things I've heard physicists say is one of the things you start to get
3349440	3351680	a sense of when a theory might be correct is when it's beautiful, right?
3351680	3353360	Like, you know, they're, right.
3353360	3356600	And so, so, so there's something and you feel the same thing, by the way, in
3356600	3358640	like human psychology, right?
3358680	3360720	You know, when, when you're experiencing awe, right?
3360720	3363120	You know, there's like a, there's like a, there's a simplicity too.
3363120	3365400	When, when you're having an honest interaction with somebody, there's an
3365400	3368720	aesthetic, I would say calm comes over you because you're actually being fully
3368720	3370080	honest and trying to hide yourself, right?
3370080	3373720	So there, so, so it's like this very deep sense of aesthetics.
3373880	3376720	And he would trust that judgment that he had deep down.
3376720	3382640	Like even, even if the engineering teams are saying this is, this is too difficult.
3382680	3386480	Even if the, whatever the finance folks are saying, this is ridiculous.
3386520	3389960	The supply chain, all that kind of stuff, this makes it impossible to, we
3389960	3391200	can't do this kind of material.
3391880	3393360	This has never been done before.
3393400	3394440	And so on and so forth.
3394440	3395520	He just sticks by it.
3395840	3397680	Well, I mean, who makes a phone out of aluminum, right?
3397680	3401040	Like, nobody else would have done that.
3401640	3404840	And now, of course, if your phone was made out of aluminum, why, you know, how crude,
3404960	3407200	what a kind of caveman would you have to be to have a phone that's made out of
3407200	3407920	plastic, like, right?
3407920	3410400	So like, so it's just this very, right?
3410400	3412960	And, you know, look, it's, there's a thousand different ways to look at this.
3412960	3415960	But one of the things is just like, look, these things are central to your life.
3416000	3418680	Like you're with your phone more than you're with anything else.
3418680	3420000	Like it's in your, it's going to be in your hand.
3420000	3422520	I mean, he, you know, you know, this, he thought very deeply about what it meant
3422520	3423720	for something to be in your hand all day long.
3424200	3427360	Well, for example, here's an interesting design thing.
3427360	3430720	Like he never wanted, it's my understanding is he never wanted an iPhone to
3430720	3434840	have a screen larger than you could reach with your thumb one handed.
3435440	3438320	And so he was actually opposed to the idea of making the phones larger.
3438320	3440440	And I don't know if you have this experience today, but let's say there
3440440	3443720	are certain moments in your day when you might be like only have one hand
3443720	3445440	available and you might want to be on your phone.
3445640	3446000	Yeah.
3446080	3450840	And you're trying to like, your thumb can't reach the send button.
3450960	3451200	Yeah.
3451200	3452560	I mean, there's pros and cons, right?
3452560	3455040	And then there's like folding phones, which I would love to know what he
3455040	3456160	thought and thinks about them.
3456760	3459920	Uh, but I mean, is there something you could also just link on?
3459920	3465000	Cause he's one of the interesting, um, figures in the history of technology.
3465360	3469520	What makes him, what makes him as successful as he was, what makes him as
3469520	3475200	interesting as he was, uh, what made him, um, so productive and important in, um,
3475680	3477520	in, in, in the development of technology.
3478040	3479280	He had an integrated worldview.
3479280	3483120	So the, the, the, the properly designed device that had the correct functionality
3483120	3487480	that had the deepest understanding of the user that was the most beautiful, right?
3487480	3489920	Like it had to be all of those things, right?
3489920	3493200	It was, he basically would drive to as close to perfect as you could possibly
3493200	3493800	get, right?
3493800	3496440	And I, you know, I suspect that he never quite, you know, thought he ever
3496440	3498920	got there cause most great creators, you know, are generally dissatisfied.
3499040	3501720	You know, you read accounts later on and all they can, all they can see are the
3501720	3504240	flaws in their creation, but like he got as close to perfect each step of the
3504240	3507680	way as he could possibly get with the, with the constraints of the, of the
3507680	3508640	technology of his time.
3508800	3511760	Um, and then, you know, look, he was, you know, sort of famous in the Apple model.
3511760	3514360	It's like, look, they, they, they will, you know, this, this headset that they
3514360	3517800	just came out with, like that, you know, it's like a decade long project, right?
3517800	3520920	It's like, and they're just going to sit there and tune and tune and polish and
3520920	3524040	polish and tune and polish and tune and polish until it is as perfect as anybody
3524040	3525040	could possibly make anything.
3525480	3528480	And then this goes to the, the, the way that people describe working with him was,
3528520	3531720	which is, you know, there was a terrifying aspect of working with him, which is,
3531720	3533120	you know, he was, you know, he was very tough.
3533680	3537600	Um, but there was this thing that everybody I've ever talked to work for him
3537600	3541680	says that they all say the following, which is he, we did the best work of our
3541680	3545120	lives when we worked for him because he set the bar incredibly high and then he
3545120	3547640	supported us with everything that he could to let us actually do work of that
3547640	3548000	quality.
3548560	3551440	So a lot of people who were at Apple spend the rest of their lives trying to
3551440	3553760	find another experience where they feel like they're able to hit that quality
3553760	3554200	bar again.
3554520	3558080	Even if it, in retrospect, we're doing it felt like suffering.
3558120	3558840	Yeah, exactly.
3559840	3563280	What does that teach you about the human condition?
3563880	3566240	So look, exactly.
3566240	3569440	So the Silicon Valley, I mean, look, he's not, you know, George Patton in the,
3569480	3572560	you know, in the army, like, you know, there are many examples in other fields,
3572600	3578720	you know, that are like this, um, uh, uh, this is specifically in tech.
3578760	3580120	It's actually, I find it very interesting.
3580120	3583400	There's the Apple way, which is polish, polish, polish and don't ship until it's
3583400	3584480	as perfect as you can make it.
3584520	3588280	And then there's the sort of the other approach, which is the sort of incremental
3588600	3592120	hacker mentality, which basically says ship early and often and iterate.
3592480	3595520	And one of the things I find really interesting is I'm now 30 years into this,
3595560	3599880	like there are very successful companies on both sides of that approach.
3600120	3600440	Right.
3601000	3604600	Um, like that is a fundamental difference, right?
3604600	3608120	And how to operate and how to build and how to create that you have world
3608120	3609880	class companies operating in both ways.
3610360	3613640	Um, and I don't think the question of like, which is the superior model is
3613640	3615000	anywhere close to being answered.
3615440	3618200	Like, and my suspicion is the answer is do both.
3618200	3621160	The answer is you actually want both, they lead to different outcomes.
3621440	3625560	Software tends to do better, um, with the iterative approach, um, hardware
3625560	3629880	tends to do better with the, uh, you know, sort of wait and make it perfect approach.
3629880	3633440	But again, you can find examples in, in, in, in both directions.
3633720	3635480	Oh, so the juror's still out on that one.
3636200	3637400	Uh, so back to Mosaic.
3637400	3643400	So what, uh, it was text based, uh, Tim Berners-Lee.
3643880	3646120	Well, there was the web, which was text based, but there were no,
3646120	3647720	I mean, there was like three websites.
3647720	3649160	There was like no content.
3649160	3650040	There were no users.
3650120	3652040	Like it, it wasn't like a, it wasn't like a catalytic.
3652040	3654200	It hadn't, by the way, it was all, because it was all texts.
3654200	3654840	There were no documents.
3654840	3655400	There are no images.
3655400	3656040	There are no videos.
3656040	3657320	There were no, right.
3657320	3660680	So, so it was, it was, and then if, in the beginning, if you had to be on
3660680	3663960	a next cube, but you need to have a next cube both to publish and to consume.
3663960	3665320	So, so there were.
3665320	3666440	6,000 bucks, you said.
3666440	3668600	There were limitations on, yeah, $6,000 PC.
3668600	3671560	They did not, they did not sell very many, but then there was also,
3672200	3674440	there was also FTP and there was Usenets, right?
3674440	3677160	And there was, you know, a dozen other, basically, there's Waste,
3677160	3678600	which was an early search thing.
3678600	3681560	There was Gopher, which was an early menu based information retrieval system.
3681560	3684920	There were like a dozen different sort of scattered ways that people would get
3684920	3686280	to information on, on the internet.
3686280	3689160	And so the mosaic idea was basically bring those all together,
3689160	3691320	make the whole thing graphical, make it easy to use,
3691320	3693320	make it basically bulletproof so that anybody can do it.
3693960	3696920	And then again, just on the luck side, it so happened that this was right at the
3696920	3700120	moment when graphics, when the GUI sort of actually took off.
3700120	3703240	And we're now also used to the GUI that we think it's been around forever,
3703240	3707400	but it didn't really, you know, the Macintosh brought it out in 85,
3707400	3709800	but they actually didn't sell very many Macs in the 80s.
3709800	3711400	It was not that successful of a product.
3712280	3716840	It really was, you needed Windows 3.0 on PCs and that hit in about 92.
3718200	3719960	And so, and we did mosaic in 92, 93.
3719960	3722920	So that sort of, it was like right at the moment when you could imagine
3722920	3728280	actually having a graphical user interface to write at all, much less one to the internet.
3728440	3731560	How old did Windows 3.0 sell?
3731560	3733080	So it was at the really big.
3733080	3733800	That was the big bang.
3733800	3736600	The big operating, graphical operating system.
3736600	3739560	Well, this is the classic, okay, this Microsoft was operating on the other.
3739560	3742360	So Steve, Steve, Apple was running on the Polish until it was perfect.
3742360	3744680	Microsoft famously ran on the other model, which is ship and iterate.
3744680	3746920	And so in the old line in those days was Microsoft,
3746920	3748600	right, it's the version three of every Microsoft product.
3748600	3749800	That's the good one, right?
3749800	3753640	And so there are, you can find online, Windows 1, Windows 2, nobody used them.
3754440	3757000	Actually, the original Windows, in the original Microsoft Windows,
3757000	3758120	the Windows were not overlapping.
3759320	3762280	And so you had these very small, very low resolution screens.
3762280	3765560	And then you had literally, it just didn't work.
3765560	3766360	It wasn't ready yet.
3766360	3769720	Well, and Windows 95, I think was a pretty big leap also.
3769720	3770520	That was a big leap too.
3770520	3770920	Yeah.
3770920	3772200	So that was like bang, bang.
3772840	3775720	And then of course, Steve, and then, you know, in the fall of the time,
3775720	3777560	Steve came back, then the Mac started to take off again.
3777560	3778360	That was the third bang.
3778360	3779720	And then the iPhone was the fourth bang.
3780360	3781320	Such exciting time.
3781320	3782920	And then we were off to the races.
3782920	3786760	Because nobody could have known or be created from that.
3786760	3791240	Well, Windows 3.1 or 3.0, Windows 3.0 to the iPhone was only 15 years,
3792680	3792920	right?
3792920	3795320	Like that ramp was in retrospect.
3795320	3796600	At the time, it felt like it took forever.
3796600	3800920	But in historical terms, like that was a very fast ramp from even a graphical
3800920	3804680	computer at all on your desk to the iPhone, it was 15 years.
3804680	3807240	Did you have a sense of what the Internet will be as you're looking
3807240	3808440	through the window of Mosaic?
3808440	3813160	Like what, like there's just a few web pages for now.
3813880	3817720	So the thing I had early on was I was keeping at the time what,
3818760	3820360	there's disputes over what was the first blog.
3820360	3825240	But I had one of them that at least is a possible, at least a runner-up in the
3825240	3825960	competition.
3825960	3827640	And it was what was called the What's New page.
3829480	3833320	And it was, it was a hardwired, I had distribution and fair advantage.
3833320	3834920	I put it right in the browser.
3835560	3837560	I put it in the browser and then I put my resume in the browser.
3837560	3837720	Yeah.
3837720	3839800	Also was hilarious.
3839800	3846200	But I was keeping the, not many people get to do that.
3850200	3851240	Good call.
3851240	3852520	And early days.
3852520	3853080	Yes.
3853080	3854360	It's so interesting.
3854360	3857240	I'm looking for my, about, about, oh, Mark is looking for a job.
3861000	3864120	So the What's New page I would literally get up every morning and I would,
3864120	3864760	every afternoon.
3865400	3868920	And I would basically, if you wanted to launch a website, you would email me.
3869800	3870920	And I would list it on the What's New page.
3870920	3873960	And that was how people discovered the new websites as they were coming out.
3873960	3876520	And I remember, because it was like one, it literally went for me.
3876520	3881160	It was like one every couple of days, until like one every day, until like two every day.
3882840	3885960	So you're doing, so that, that blog was kind of doing the directory thing.
3885960	3887640	So like, what was the homepage?
3888280	3891080	So the homepage was just basically trying to explain even what this thing is that you're
3891080	3891880	looking at, right?
3891880	3893560	The basic, basically basic instructions.
3894440	3896440	But then there was a button, there was a button that said What's New.
3896440	3899560	And what most people did was they went to, for obvious reasons, went to What's New.
3899640	3903000	But like, it was so, it was so mind blowing at that point.
3903000	3904200	Just the basic idea.
3904200	3906280	And it was just like, you know, this is basically the internet,
3906280	3907960	but people could see it for the first time.
3907960	3910760	The basic idea was, look, you know, some, you know, it's like, literally,
3910760	3914520	it's like an Indian restaurant in like Bristol, England has like put their menu on the web.
3914520	3920280	And people were like, wow, because like that's the first restaurant menu on the web.
3920760	3921960	And I don't have to be in Bristol.
3921960	3923240	And I don't know if I'm ever going to go to Bristol.
3923240	3924440	And I don't like Indian food.
3924440	3926200	And like, wow, right.
3926680	3927560	And it was like that.
3927560	3932520	The first web, the first streaming video thing was a, it was another England thing,
3932520	3933560	some Oxford or something.
3934520	3940040	Some guy put his coffee pot up as the first streaming video thing.
3940040	3943320	And he put it on the web because he literally, it was the coffee pot down the hall.
3943320	3945640	And he wanted to see when he needed to go refill it.
3946360	3948920	But there were, you know, there was a point when there were thousands of people
3948920	3951960	like watching that coffee pot, because it was the first thing you could watch.
3952680	3960120	But isn't, were you able to kind of infer, you know, if that Indian restaurant could go online?
3960120	3960680	Yeah.
3960680	3962200	Then you're like, they all will.
3962200	3963000	They all will.
3963000	3963800	Yeah, exactly.
3963800	3964600	So you felt that.
3964600	3965320	Yeah, yeah, yeah.
3965320	3966760	Now, you know, look, it's still a stretch, right?
3966760	3968600	It's still a stretch because it's just like, okay, is it, you know,
3968600	3970680	you're still in this zone, which is like, okay, is this a nerd thing?
3970680	3971560	Is this a real person thing?
3971560	3972360	Yeah.
3972360	3975400	Um, by the way, we, you know, there was a wall of skepticism from the media.
3975400	3978520	Like they just, like everybody was just like, yeah, this is just like them.
3978520	3980760	This is not, you know, this is not for regular people at that time.
3980760	3983080	Um, and so you had to think through that.
3983080	3986600	And then look, it was still, it was still hard to get on the internet at that point,
3986600	3986920	right?
3986920	3990200	So you could get kind of this weird bastardized version if you were on AOL,
3990200	3994200	which wasn't really real, or you had to go like learn what an ISP was.
3995480	3998840	You know, in those days, PCs actually didn't have TCP-IP drivers come pre-installed.
3998840	4001400	So you had to learn what a TCP-IP driver was.
4001400	4002360	You had to buy a modem.
4002360	4003640	You had to install driver software.
4004760	4007400	I have a comedy routine I do, something like 20 minutes long,
4007400	4009960	describing all the steps required to actually get on the internet at this point.
4010840	4015880	And so you had to, you had to look through these practical, well, and then, and then speed
4015880	4018920	performance, 14 form modems, right?
4018920	4023080	Like it was like watching, you know, glue dry, um, like, and so you had to, you had to,
4023080	4025960	there were basically a sequence of bets that we made where you basically needed to look through
4025960	4029480	that current state of affairs and say, actually, there's going to be so much demand for that.
4029480	4032040	Once people figure this out, there's going to be so much demand for it that all of these
4032040	4033560	practical problems are going to get fixed.
4034280	4041000	Some people say that the anticipation makes the destination that much more exciting.
4041000	4042200	Do you remember progressive JPEGs?
4042760	4045480	Yeah. Do I? Do I?
4045480	4047560	So for kids in the audience, right?
4047560	4048760	For kids in the audience.
4048760	4052840	You used to have to wash an image load like a line at a time, but it turns out there was this
4052840	4057320	thing with JPEGs where you could load basically every fourth, you could load like every fourth
4057880	4060120	line and then you could sweep back through again.
4060120	4063400	And so you could like render a fuzzy version of the image up front and then it would like
4063400	4064680	resolve into the detailed one.
4064680	4067960	And that was like a big UI breakthrough because it gave you something to watch.
4069240	4073400	Yeah. And, uh, you know, there's applications in various domains for that.
4075960	4076600	Well, it's a big fight.
4076600	4078840	If there was a big fight early on about whether there should be images on the web.
4079880	4081880	For that reason, for like sexualization.
4081880	4084760	No, not explicitly, that did come up, but it wasn't even that.
4084760	4086280	It was more just like all the serious info.
4086280	4089880	The argument went, the purists basically said all the serious information in the world is text.
4090520	4093480	If you introduce images, you basically are going to bring in all the trivial stuff.
4093480	4097160	You're going to bring in magazines and, you know, all this crazy stuff that, you know,
4097160	4098920	people, you know, it's going to distract from that.
4098920	4101400	It's going to go take, take the way from being serious to being frivolous.
4101400	4107240	Well, was there any doomer type arguments about, uh, the internet destroying all of human
4107240	4112680	civilization or destroying some fundamental fabric of human civilization?
4112680	4115160	Yeah. So those days it was all around crime and terrorism.
4116120	4120040	So those arguments happened, you know, but there was no sense yet of the internet having
4120040	4122680	like an effect on politics or because that was, that was way too far off.
4122680	4126520	But there was an enormous panic at the time around cybercrime.
4126520	4129640	There was like enormous panic that like your credit card number would get stolen and you'd
4129640	4131320	use life savings to be drained.
4131320	4134680	And then, you know, criminals were going to, there was, oh, when we started, one of the things
4134680	4139160	we did, one of the, the Netscape browser was the first widely used piece of consumer software
4139160	4142600	that had strong encryption built in, made it available to ordinary people.
4142600	4146440	And at that time, strong encryption was actually illegal to export out of the U.S.
4147160	4149240	So we could feel that product in the U.S.
4149240	4152120	We could not export it because it was, it was classified as ammunition.
4152760	4155800	So the Netscape browser was on a restricted list along with the Tom Huck missile
4156440	4157880	as being something that could not be exported.
4157880	4161880	So we had to make a second version with deliberately weak encryption to sell overseas
4161880	4165960	with a big logo on the box saying do not trust this, which it turns out makes it hard to sell
4165960	4169400	software when it's got a big logo that says don't trust it.
4170280	4173160	And then we had to spend five years fighting the U.S. government to get them to basically
4173160	4174440	stop trying to do this.
4175640	4179400	Because the fear was terrorists are going to use encryption to like plot, you know,
4179400	4180840	all these things.
4182040	4185400	And then, you know, we responded with, well, actually we need encryption to be able
4185400	4188040	to secure systems so that the terrorists and the criminals can't get into them.
4188040	4190920	So that was, anyway, that was the 1990s fight.
4190920	4195080	So can you say something about some of the details of the software engineering
4195800	4197880	challenges required to build these browsers?
4197880	4202280	I mean, the engineering challenges of creating a product that hasn't really existed before
4203000	4209720	that can have such almost like limitless impact on the world with the internet.
4209720	4212840	So there was a really key bet that we made at the time, which is very controversial,
4212840	4216120	which was core to the core to how it was engineered, which was are we optimizing for
4216120	4218280	performance or for ease of creation?
4219000	4222200	And in those days, the pressure was very intense to optimize for performance because
4222200	4225480	the network connections were so slow and also the computers were so slow.
4226280	4229320	And so if you had mentioned the progressive JPEG, it's like,
4230120	4234200	if there's an alternate world in which we optimize for performance and it just,
4234200	4236280	you had just a much more pleasant experience right up front.
4237160	4240520	But what we got by not doing that was we got ease of creation.
4240520	4245720	And the way that we got ease of creation was all of the protocols and formats were in text,
4245720	4246600	not in binary.
4247640	4250920	And so HTTP is in text, by the way, and this was an internet tradition, by the way,
4250920	4252440	that we picked up, but we continued it.
4252440	4257160	HTTP is text and HTML is text and then everything else that followed is text.
4258040	4261560	As a result, and by the way, you can imagine purist engineer saying this is insane,
4261560	4264600	you have very limited bandwidth, why are you wasting any time sending text?
4264600	4267160	You should be encoding the stuff into binary and it'll be much faster.
4267160	4268520	Of course, the answer is that's correct.
4269400	4271800	But what you get when you make it text is all of a sudden, well,
4271800	4273880	the big breakthrough was the view source function, right?
4273880	4277160	So the fact that you could look at a web page, you could hit view source and you could see the
4277160	4280360	HTML, that was how people learned how to make web pages, right?
4280440	4282920	It's so interesting because the stuff we take for granted now
4284600	4289480	is, man, that was fundamental to the development of the web to be able to have HTML just right
4289480	4290200	there.
4290200	4296280	All the ghetto mess that is HTML, all the sort of almost biological messiness
4297320	4303640	of HTML and then having the browser try to interpret that mess to show something reasonable.
4303640	4306360	Well, and then there was this internet principle that we inherited, which was
4306440	4310360	emit, what was it, emit cautiously, emit conservatively interpret liberally.
4310360	4315080	So it basically meant, the design principle was if you're creating a web editor that's
4315080	4319480	going to emit HTML, do it as cleanly as you can, but you actually want the browser to interpret
4319480	4322840	liberally, which is you actually want users to be able to make all kinds of mistakes
4322840	4323800	and for it to still work.
4324360	4327000	And so the browser rendering engines to this day have all of this
4327560	4332280	skinny code crazy stuff where they're resilient to all kinds of crazy HTML mistakes.
4332280	4335800	And literally what I always had in my head is there's an eight-year-old or an 11-year-old
4335800	4337960	somewhere and they're doing a view source, they're doing a cut and paste and they're
4337960	4340280	trying to make a web page for their turtle or whatever.
4340840	4343880	And they leave out a slash and they leave out an angle bracket and they do this and they do
4343880	4344840	that and it still works.
4345960	4346760	It's also like that.
4346760	4354040	I don't often think about this, but programming, C++, C++, all those languages, Lisp, the compile
4354040	4358920	language, the interpretive language, Python, Perl, all of that, the brace has to be all
4358920	4359400	correct.
4359400	4359880	Yes.
4359880	4361080	Like everything has to be perfect.
4361080	4361400	Brutal.
4362360	4362920	And then.
4362920	4363480	Autistic.
4363480	4364120	You forget.
4365080	4365400	All right.
4366280	4367880	It's systematic and rigorous.
4367880	4368440	Let's go there.
4369320	4379480	But you forget that the web with JavaScript eventually and HTML is allowed to be messy in
4379480	4385960	the way, for the first time, messy in the way biological systems could be messy.
4385960	4389960	It's like the only thing computers were allowed to be messy on for the first time.
4389960	4390760	It used to offend me.
4390760	4393560	So I grew up in UNIX, I worked on UNIX.
4393640	4395960	I was a UNIX native all the way through this period.
4395960	4399880	And so, and it used to drive me bananas when it would do the segmentation fault in the
4399880	4403800	core dump file, just like it's like literally there's like an error in the code.
4403800	4406600	The math is off by one and it core dumps.
4406600	4409080	And I'm in the core dump trying to analyze it and trying to reconstruct.
4409080	4410600	And I'm just like, this is ridiculous.
4410600	4413560	Like the computer ought to be smart enough to be able to know that if it's off by one,
4413560	4414200	okay, fine.
4414200	4415000	And it keeps running.
4415640	4418040	And I would go ask all the experts, like, why can't it just keep running?
4418040	4420840	And they'd explain to me, well, because all the downstream repercussions and blah, blah.
4420840	4427880	And I'm like, we're forcing the human creator to live, to your point,
4427880	4430200	in this hyperlittoral world of perfection.
4430760	4433320	And that's just bad.
4433320	4436840	And by the way, what happens with that, of course, is what happened with coding at that
4436840	4438120	point, which is you get a high priesthood.
4439160	4442200	There's a small number of people who are really good at doing exactly that.
4442200	4444520	Most people can't and most people are excluded from it.
4444520	4450520	And so actually, that was where I picked up that idea was like, no, you want these things
4450520	4452600	to be resilient to error in all kinds.
4452600	4453800	And this would drive the purists.
4453800	4454360	Absolutely crazy.
4454360	4457480	Like I got attacked on this like a lot because, yeah, I mean, like every time I,
4457480	4460280	you know, all the purists who are like into all this like Markov language stuff and
4460280	4463080	formats and codes and all this stuff, they would be like, you know, you can't,
4463080	4464520	you're encouraging bad behavior because.
4465080	4470200	Oh, so they wanted the browser to give you a segfault error anytime there was a mismatch?
4470200	4471640	Yeah, yeah, they wanted it to be a kind of, right?
4471640	4476360	They wanted, yeah, that was a very, any properly trained and credentialed engineer
4477160	4478920	would be like, that's not how you build these systems.
4478920	4481480	That's such a bold move to say, no, it doesn't have to be.
4481480	4481720	Yeah.
4481720	4484760	Now, like I said, the good news for me is the internet kind of had that tradition already.
4485320	4488440	But we, but having said that, like we pushed it, we pushed it way out.
4488440	4490840	But the other thing we did going back to the performance thing was we gave up a
4490840	4493720	lot of performance we made that that initial experience for the first few years was pretty
4493720	4494520	painful.
4494520	4498280	But, but the bet there was actually an economic bet, which was basically the demand for the
4498280	4502440	web would basically mean that there would be a surge in supply of broadband because
4503320	4506920	the question was, okay, how do you get, how do you, how do you get the phone companies
4506920	4511720	which are not famous in those days for doing new things at huge cost for like speculative
4511720	4512040	reasons?
4512040	4515800	Like how do you get them to build up broadband, you know, spend billions of dollars doing that
4515800	4518600	and, you know, you could go meet with them and try to talk them into it, or you could
4518600	4521960	just have a thing where it's just very clear that it's going to be, that the people love,
4521960	4523800	it's going to be better if it's faster.
4523800	4527640	And so that, there was a period there, and this was, this was fraught with some peril,
4527640	4531480	but there was a period there where it's like, we knew the experience was suboptimized because
4531480	4536440	we were trying to force the emergence of demand for broadband, which is in fact, what happened?
4537400	4541320	So you had to figure out how to display this text, HTML text.
4541960	4545320	So the blue links and the purple links, and there's no standards.
4545320	4546760	Is there standards at that time?
4547800	4548680	There really still isn't.
4550280	4554200	Well, there's like, there's implied, implied standards, right?
4554200	4557960	And they, you know, there's all these kinds of new features that are being added like CSS,
4557960	4562440	what like, what kind of stuff a browser should be able to support, features with the languages,
4562520	4563800	within JavaScript and so on.
4564360	4570520	But you, you're setting standards on the fly yourself.
4571080	4575720	Well, to this day, if you, if you create a webpage that has no CSS style sheet,
4575720	4578200	the browser will render it however it wants to, right?
4578200	4582760	So this was one of the things, there was this idea, this idea at the time and how these systems
4582760	4587800	were built, which is separation of content from format or separation of content from appearance.
4588920	4592120	And that's still, people don't really use that anymore, because everybody wants to determine
4592120	4593560	how things look and so they use CSS.
4593560	4597080	But it's still in there that you can just let the browser do all the work.
4597080	4604040	I still like the, like really basic websites, but that could be just old school.
4604040	4609400	Kids these days with their fancy responsive websites that don't actually have much content,
4609400	4610920	but have a lot of visual elements.
4610920	4613880	Well, that's one of the things that's fun about chat, you know, about chat GPT.
4613880	4615640	Yeah, back to the basics.
4615640	4617640	Back to just text, right?
4617640	4622600	And it, you know, there is this pattern in human creativity and media where you end up
4622600	4623480	back at text.
4623480	4625480	And I think there's, you know, there's something powerful in there.
4626200	4628920	Is there some other stuff you remember, like the purple links?
4628920	4634120	There were some interesting design decisions to kind of come up that we have today or we
4634120	4636120	don't have today that were temporary.
4636680	4638280	So we made, I made the background gray.
4638280	4641560	I hated reading text on white backgrounds.
4641560	4642520	So I made the background gray.
4642520	4643400	Do you regret?
4644040	4647560	No, no, no, no, that's that decision I think has been reversed.
4647800	4650280	But now I'm happy though, because now dark mode is the thing.
4651160	4652760	So it wasn't about gray.
4652760	4654520	It was just you didn't want a white background.
4654520	4655160	Strain my eyes.
4655960	4656920	Strange your eyes.
4657720	4658280	Interesting.
4659800	4661720	And then there's a bunch of other decisions.
4661720	4665640	I'm sure there's an interesting history of the development of HTML and CSS and all those
4665640	4668200	in interface and JavaScript.
4668200	4670280	And there's this whole Java applet thing.
4671240	4672680	Well, the big one probably JavaScript.
4673480	4677320	CSS was after me, so I didn't know it wasn't me, but JavaScript wasn't the big
4677400	4679000	JavaScript maybe was the biggest of the whole thing.
4679000	4679480	That was us.
4680200	4682680	And that was basically a bet.
4682680	4683720	It was a bet on two things.
4683720	4686200	One is that the world wanted a new front end scripting language.
4687240	4690520	And then the other was we thought at the time the world wanted a new back end scripting language.
4691320	4694360	So JavaScript was designed from the beginning to be both front end and back end.
4695320	4699640	And then it failed as a back end scripting language and Java won for a long time.
4699640	4702600	And then Python, Perl and other things, PHP and Ruby.
4702600	4704440	But now JavaScript is back.
4704520	4708200	And so I wonder if everything in the end will run on JavaScript.
4708200	4710200	It seems like it is the...
4710200	4714360	And by the way, let me give a shout out to Brendan Eich,
4714360	4717800	was the basically the one man inventor of JavaScript.
4717800	4721960	If you're interested to learn more about Brendan Eich, you can find his podcast previously.
4721960	4722440	Exactly.
4723160	4724600	So he wrote JavaScript over a summer.
4724600	4729240	And I think it is fair to say now that it's the most widely used language in the world.
4729240	4733400	And it seems to only be gaining in its range of adoption.
4733480	4738440	In the software world, there's quite a few stories of somebody over a weekend or a week
4738440	4745640	or over a summer writing some of the most impactful revolutionary pieces of software ever.
4746200	4747720	That should be inspiring, yes.
4747720	4748360	Very inspiring.
4748360	4749480	I'll give you another one, SSL.
4750120	4752600	So SSL was the security protocol that was us.
4752600	4754120	And that was a crazy idea at the time,
4754120	4757720	which was let's take all the native protocols and let's wrap them in a security wrapper.
4757720	4760680	That was a guy named Kip Hickman who wrote that over a summer, one guy.
4760920	4766440	And then look today, sitting here today, like the transformer at Google was a small handful
4766440	4767080	of people.
4767080	4772520	And then the number of people who did the core work on GPT, it's not that many people.
4773080	4774360	It's a pretty small handful of people.
4775080	4779320	And so, yeah, the pattern in software repeatedly over a very long time has been.
4780920	4783960	Jeff Bezos always had the two pizza rule for teams at Amazon,
4783960	4786520	which is any team needs to be able to be fed with two pizzas.
4787160	4788920	If you need the third pizza, you have too many people.
4788920	4794600	And I think it's actually the one pizza rule for the really creative work.
4794600	4797000	I think it's two people, three people.
4797000	4800040	Well, you see that with certain open source projects.
4800040	4801720	So much is done by one or two people.
4803000	4804200	It's so incredible.
4804200	4808360	And that's why you see that gives me so much hope about the open source movement
4808360	4809480	in this new age of AI.
4810440	4815800	Where recently having had a conversation with Mark Zuckerberg of all people
4815800	4822040	who's all in on open source, which is so interesting to see and so inspiring to see.
4822040	4825320	Because releasing these models, it is scary.
4825320	4826840	It is potentially very dangerous.
4826840	4827800	And we'll talk about that.
4828680	4835560	But it's also, if you believe in the goodness of most people and in the skill set of most people
4835560	4839240	and the desire to do good in the world, that's really exciting.
4839240	4844040	Because it's not putting these models into the centralized control of big corporations,
4844040	4844920	the government and so on.
4844920	4849720	It's putting it in the hands of a teenage kid with a dream in his eyes.
4849720	4850280	I don't know.
4850280	4852840	That's beautiful.
4852840	4856440	And look, this stuff, AI ought to make the individual coder, obviously,
4856440	4859800	far more productive by 1,000X or something.
4859800	4863640	And so you ought to open source, not just the future of open source,
4863640	4865160	but the future of open source, everything.
4865720	4870120	We ought to have a world now of super coders who are building things as open source with
4870120	4873080	one or two people that were inconceivable five years ago.
4874280	4877160	The level of kind of hyperproductivity we're going to get out of our best and brightest,
4877160	4878200	I think it's going to go way up.
4878200	4879000	It's going to be interesting.
4879000	4879960	We'll talk about it.
4879960	4883000	But let's just linger a little bit on Nescape.
4884040	4889720	Nescape was acquired in 1999 for $4.3 billion by AOL.
4889720	4891480	What was that like?
4891480	4893960	What were some memorable aspects of that?
4893960	4897560	Well, that was the height of the dot-com boom bubble bust.
4897560	4899400	I mean, that was the frenzy.
4899960	4903400	If you watch a succession, that was like what they did in the fourth season
4903960	4908600	with Gojo and then Merger with their, so it was like the height of one of those kind of
4908600	4909080	dynamics.
4909080	4911000	Would you recommend succession, by the way?
4911000	4912360	I'm more of a Yellowstone guy.
4913640	4914920	Yellowstone's very American.
4914920	4915960	I'm very proud of you.
4917320	4920760	I just talked to Matthew McConaughey, and I'm full on texting at this point.
4920760	4921240	Good.
4921240	4922040	I hurtily approve.
4922840	4926040	And he will be doing the sequel to Yellowstone.
4926040	4927000	Yeah, exactly.
4927000	4927720	Very exciting.
4927720	4931400	Anyway, so that's a rude interruption by me.
4932360	4937880	By way of succession, so that was at the height of the-
4937880	4941400	Deal-making and money and just the fur flying and like craziness.
4941400	4943400	And so, yeah, it was just one of those.
4943400	4946760	It was just like, I mean, as the entire Nescape thing from start to finish was four years,
4947640	4950760	which was like, for one of these companies, it's just like incredibly fast.
4950760	4953880	You know, we went public 18 months after we got, after we were founded,
4953880	4955480	which virtually never happens.
4955480	4958680	So it was just this incredibly fast kind of meteor streaking across the sky.
4959320	4960520	And then, of course, it was this.
4960520	4962680	And then there was just this explosion that happened,
4962680	4965160	because then it was almost immediately followed by the dot-com crash.
4965960	4968280	It was then followed by Haywell buying Time Warner,
4968280	4970440	which, again, is like the succession guy's going to play with that,
4971240	4974440	which turned out to be a disaster steal, one of the famous,
4974440	4976040	you know, kind of disasters in business history.
4976600	4980680	And then what became an internet depression on the other side of that.
4980680	4985400	But then in that depression in the 2000s was the beginning of broadband and smartphones
4985400	4986840	and web 2.0, right?
4986840	4990200	And then social media and search and every SaaS and everything that came out of that.
4990200	4992680	So what did you learn from just the acquisition?
4992680	4994120	I mean, this is so much money.
4996280	4999320	What's interesting, because I must have been very new to you,
5000040	5004840	that these software stuff, you can make so much money.
5004840	5006360	There's so much money swimming around.
5006360	5009960	I mean, I'm sure the ideas of investment were starting to get born there.
5009960	5011560	Yes, let me get, so let me lay it out.
5011560	5013400	So here's the thing, I don't know if I figured it out then,
5013400	5018040	but figured it out later, which is software is a technology that it's like, you know,
5018040	5021720	the concept of the philosopher's stone, the philosopher's stone in Alchemy transmutes
5021720	5024520	lead into gold and Newton spent 20 years trying to find the philosopher's stone,
5024520	5025000	never got there.
5025000	5026120	Nobody's ever figured it out.
5026680	5028760	Software is our modern philosopher's stone.
5028760	5032680	And in economic terms, it transmutes labor into capital,
5033800	5035880	which is like a super interesting thing.
5035880	5038200	And by the way, like Karl Marx is rolling over in his grave right now,
5038200	5040680	because of course that's complete refutation of his entire theory.
5040920	5045240	Transpute labor into capital, which is as follows is somebody
5045960	5048520	sits down at a keyboard and types a bunch of stuff in,
5049320	5051400	and a capital asset comes out the other side,
5051400	5054040	and then somebody buys that capital asset for a billion dollars.
5054040	5055880	Like, that's amazing.
5056600	5059240	Right, it's literally creating value right out of thin air,
5059240	5061480	right, out of purely human thought, right.
5062440	5066120	And so that's, there are many things that make software magical and special,
5066120	5067240	but that's the economics.
5067240	5069400	I wonder what Marx would have thought about that.
5069400	5070840	Oh, he would have completely broke his brain,
5070840	5072520	because of course the whole thing was,
5074680	5076760	that kind of technology is inconceivable when he was alive.
5076760	5078680	It was all industrial era stuff.
5078680	5082840	And so any kind of machinery necessarily involves huge amounts of capital,
5082840	5085320	and then labor was on the receiving end of the abuse.
5086840	5090840	Right, but like a software engineer or somebody who basically transmutes
5090840	5094840	his own labor into an actual capital asset creates permanent value.
5094840	5097080	Well, in fact, it's actually very inspiring.
5097080	5098520	That's actually more true today than before.
5098520	5101240	So when I was doing software, the assumption was all new software
5101240	5105080	basically has a sort of a parabolic sort of life cycle, right.
5105080	5107320	So you ship the thing, people buy it.
5107320	5109160	At some point, everybody who wants it has bought it,
5109160	5112600	and then it becomes obsolete, and it's like bananas, nobody buys old software.
5113880	5120360	These days, Minecraft, Mathematica, Facebook, Google,
5121320	5125320	you have the software assets that are, have been around for 30 years
5125320	5127320	that are gaining in value every year, right.
5127320	5130040	And they're just, they're being World of Warcraft, right, Salesforce.com.
5130040	5132520	Like they're being, every single year, they're being polished and polished
5132520	5133320	and polished and polished.
5133320	5135160	They're getting better and better, more powerful, more powerful,
5135160	5136200	more valuable, more valuable.
5136200	5138440	So we've entered this era where you can actually have these things
5138440	5140440	that actually build out over decades, which by the way,
5140440	5142040	is what's happening right now with like GPT.
5143480	5148440	And so now, and this is why there is always sort of a constant investment
5148440	5151880	frenzy around software is because, look, when you start one of these things,
5151880	5152600	it doesn't always succeed.
5152600	5155560	But when it does, now you might be building an asset that builds value
5155560	5157320	for four or five, six decades to come.
5159000	5161800	If you have a team of people who have the level of devotion required
5161800	5162760	to keep making it better.
5163560	5166760	And then the fact that of course, everybody's online, there's five billion
5166760	5168840	people that are a click away from any new pieces of software.
5168840	5172600	So the potential market size for any of these things is nearly infinite.
5172600	5174440	They must have been surreal back then though.
5174440	5175080	Yeah, yeah.
5175080	5176040	This was all brand new, right?
5176040	5177400	Yeah, back then, this was all brand new.
5177400	5179240	These were all brand new.
5179240	5181720	Had you rolled out that theory and even 1999,
5181720	5183080	people would have thought you were smoking crack.
5183080	5185160	So that's emerged over time.
5186520	5190600	Well, let's now turn back into the future.
5190600	5193800	You wrote the essay, why AI will save the world.
5194920	5196280	Let's start at the very high level.
5196280	5197960	What's the main thesis of the essay?
5197960	5198120	Yeah.
5198120	5201560	So the main thesis on the essay is that what we're dealing with here is intelligence.
5202280	5205400	And it's really important to kind of talk about the sort of very nature of what
5205400	5206280	intelligence is.
5206280	5210680	And fortunately, we have a predecessor to machine intelligence,
5210680	5211640	which is human intelligence.
5212200	5215080	And we've got observations and theories over thousands of years
5215080	5217720	for what intelligence is in the hands of humans.
5217720	5220760	And what intelligence is, what it literally is,
5220760	5223160	is the way to capture, process, analyze,
5223160	5224600	synthesize, information, solve problems.
5225800	5229480	But the observation of intelligence in human hands
5229480	5231880	is that intelligence quite literally makes everything better.
5233000	5237960	And what I mean by that is every kind of outcome of human quality of life,
5237960	5241000	whether it's education outcomes or success of your children,
5241960	5246200	or career success, or health, or lifetime satisfaction,
5246760	5251080	by the way, propensity to peacefulness as opposed to violence,
5251960	5254440	propensity for open-mindedness versus bigotry,
5255000	5257320	those are all associated with higher levels of intelligence.
5257320	5260120	Smarter people have better outcomes than almost, as you write,
5260120	5261720	in almost every domain of activity.
5261720	5264760	Academic achievement, job performance, occupational status,
5264760	5267320	income, creativity, physical health, longevity,
5267320	5270440	learning new skills, managing complex tasks,
5270440	5274200	leadership, entrepreneurial success, conflict resolution,
5274200	5277480	reading comprehension, financial decision making,
5277480	5278760	understanding other's perspectives,
5278760	5281560	creative arts, parenting outcomes, and life satisfaction.
5281560	5285640	One of the more depressing conversations I've had,
5285640	5286840	and I don't know why it's depressing,
5286840	5289080	I have to really think through why it's depressing,
5289080	5293880	but on IQ and the G factor,
5295640	5300120	and that that's something in large part is genetic.
5300760	5301880	Mm-hmm.
5301880	5305400	And it correlates so much with all of these things
5305400	5306520	and success in life.
5307480	5310920	It's like all the inspirational stuff we read about,
5310920	5313000	like if you work hard and so on,
5313880	5317720	damn, it sucks that you're born with a hand that you can't change.
5317720	5318440	But what if you could?
5319160	5321800	You're saying basically, a really important point,
5321800	5327240	and I think it's a, in your articles, it really helped me,
5327960	5332440	it's a nice added perspective to think about, listen,
5332440	5335000	human intelligence, the science of intelligence
5335000	5338680	has shown scientifically that it just makes life easier
5338680	5341160	and better, the smarter you are.
5342040	5344760	And now let's look at artificial intelligence.
5345800	5350760	And if that's a way to increase the,
5352040	5355080	some human intelligence, then it's only going
5355080	5356200	to make a better life.
5356200	5357080	That's the argument.
5357080	5358280	And certainly at the collective level,
5358280	5359480	we could talk about the collective effect
5359480	5361240	of just having more intelligence in the world,
5361240	5363480	which we'll have very big payoff.
5363480	5365000	But there's also just at the individual level,
5365000	5367080	like what if every person has a machine,
5367080	5369000	you know, and it's a concept of argument,
5369000	5370520	Doug Engelbar's concept of augmentation,
5371640	5374520	you know, what if everybody has an assistant
5374520	5377240	and the assistant is, you know, 140 IQ,
5378520	5380440	and you happen to be 110 IQ,
5381320	5383400	and you've got, you know, something that basically
5383400	5385960	is infinitely patient and knows everything about you
5385960	5388200	and is pulling for you in every possible way,
5388760	5389960	wants you to be successful.
5389960	5392200	And anytime you find anything confusing
5392200	5393160	or want to learn anything
5393160	5394440	or have trouble understanding something
5394440	5395960	or want to figure out what to do in a situation,
5396600	5397640	right, want to figure out how to prepare
5397640	5399640	for a job interview, like any of these things,
5399640	5401000	like it will help you do it.
5401000	5404040	And it will therefore, the combination will effectively be,
5405240	5406600	effectively raise your raise,
5406600	5407960	because it will effectively raise your IQ,
5407960	5410120	will therefore raise the odds of successful
5410120	5411320	life outcomes in all these areas.
5411320	5414840	So people below the hypothetical 140 IQ,
5415400	5417560	it'll pull them up towards 140 IQ.
5417560	5418360	Yeah, yeah.
5418360	5420840	And then of course, you know, people at 140 IQ
5420840	5422200	will be able to have a peer, right,
5422200	5423800	to be able to communicate, which is great.
5423800	5425880	And then people above 140 IQ will have an assistant
5425880	5427480	that they can farm things out to.
5427480	5429000	And then look, got willing, you know,
5429000	5430920	at some point, these things go from future versions,
5430920	5435320	go from 140 IQ equivalent to 150 to 160 to 180, right?
5435320	5437880	Like Einstein was estimated to be on the order of 160,
5438760	5441880	you know, so when we get, you know, 160 AI,
5441880	5444840	like we'll be, you know, one assumes creating
5444840	5446360	Einstein level breakthroughs in physics.
5447240	5449720	And then at 180, we'll be, you know,
5449720	5451800	carrying cancer and developing warp drive
5451800	5452760	and doing all kinds of stuff.
5452760	5455160	And so it is quite possibly the case,
5455160	5456600	this is the most important thing that's ever happened
5456600	5457560	and the best thing that's ever happened,
5458200	5460280	because precisely because it's a lever
5460280	5462680	on this single fundamental factor of intelligence,
5462680	5464520	which is the thing that drives so much of everything else.
5465800	5468680	Can you still man the case that human plus AI
5468680	5471240	is not always better than human for the individual?
5471240	5472840	You may have noticed that there's a lot of smart assholes
5472840	5473880	running around.
5473880	5474600	Sure, yes.
5474600	5474920	Right.
5474920	5477400	And so like it's smart, there are certain people
5477400	5478440	where they get smarter, you know,
5478440	5480360	they get to be more arrogant, right?
5480360	5481880	So, you know, there's one huge flaw.
5482520	5485480	Although to push back on that, it might be interesting
5485480	5488920	because when the intelligence is not all coming from you,
5488920	5490600	but from another system,
5490600	5493720	that might actually increase the amount of humility
5493720	5494760	even in the assholes.
5494760	5495320	One would hope.
5497000	5498840	Or it could make assholes more asshole.
5499400	5501480	I mean, that's for psychology to study.
5501480	5502280	Yeah, exactly.
5502280	5505160	Another one is smart people are very convinced
5505160	5507240	that they have a more rational view of the world
5507240	5508600	and that they have an easier time
5508600	5510280	seeing through conspiracy theories and hoaxes
5510280	5512280	and sort of crazy beliefs and all that.
5513000	5515320	There's a theory in psychology, which is actually smart people.
5515320	5517320	So, for sure, people who aren't as smart
5517320	5519400	are very susceptible to hoaxes and conspiracy theories.
5519960	5521880	But it may also be the case that the smarter you get,
5521880	5523560	you become susceptible in a different way,
5524360	5526680	which is you become very good at marshalling facts
5526680	5527880	to fit preconceptions.
5528280	5528760	Yes.
5528760	5529480	Right?
5529480	5532520	You become very, very good at assembling whatever theories
5532520	5535480	and frameworks and pieces of data and graphs and charts
5535480	5537880	you need to validate whatever crazy ideas got in your head.
5538440	5540440	And so, you're susceptible in a different way.
5541400	5542120	Right?
5542120	5545480	We're all sheep, but different colored sheep.
5545480	5547320	Some sheep are better at justifying it, right?
5548200	5549960	And those are the smart sheep, right?
5551000	5552600	So, yeah, look, I would say this.
5552600	5553800	Look, there are no panacea.
5553800	5555080	I am not a utopian.
5555080	5556280	There are no panaceas in life.
5556760	5559480	There are no, like, I don't believe they're like pure positives.
5559480	5561960	I'm not a transcendental kind of person like that.
5561960	5564120	But, you know, so, yeah, there are going to be issues.
5565320	5567640	And, you know, look, smart people, maybe you could say
5567640	5569560	about smart people as they are more likely to get themselves
5569560	5571320	in situations that are, you know, beyond their grasp,
5571320	5572600	you know, because they're just more confident
5572600	5574040	and their ability to deal with complexity.
5574040	5576120	And their eyes become bigger.
5576120	5577880	Their cognitive eyes become bigger than their stomach.
5578520	5581240	You know, so, yeah, you could argue those eight different ways.
5581240	5583240	Nevertheless, on net, right?
5583240	5584760	Clearly, overwhelmingly.
5584760	5586280	Again, if you just extrapolate from what we know
5586280	5589480	about human intelligence, you're improving so many aspects
5589480	5590920	of life if you're upgrading intelligence.
5591960	5595400	So, there'll be assistance at all stages of life.
5595400	5597960	So, when you're younger, there's for education,
5597960	5600680	all that kind of stuff, or mentorship, all of this.
5600680	5604360	And later on, as you're doing work and you've developed a skill
5604360	5606520	and you're having a profession, you'll have an assistant
5606520	5608680	that helps you excel at that profession.
5608680	5610120	So, at all stages of life.
5610120	5611800	Yeah. I mean, look, the theory is augmentations.
5611800	5613400	This is the Degangalbert's term for it.
5613400	5615880	Degangalbert made this observation many, many decades ago
5615880	5617160	that, you know, basically, it's like you can have
5617160	5618760	this oppositional frame of technology
5618760	5620040	where it's like us versus the machines.
5620040	5621560	But what you really do is you use technology
5621560	5622920	to augment human capabilities.
5623560	5625480	And by the way, that's how actually the economy develops.
5625480	5627240	That's dark about the economic side of this,
5627240	5629240	but that's actually how the economy grows
5629240	5632040	is through technology augmenting human potential.
5633800	5636120	And so, yeah, and then you basically have a proxy
5636120	5640120	or a, you know, a sort of prosthetic, you know,
5640120	5642040	so like you've got glasses, you've got a wristwatch,
5642760	5645240	you know, you've got shoes, you know, you've got these things,
5645240	5647560	you've got a personal computer, you've got a word processor,
5647560	5649960	you've got Mathematica, you've got Google.
5650600	5652600	This is the latest, viewed through that lens.
5652600	5655480	AI is the latest in a long series of basically
5655480	5658040	augmentation methods to be able to raise human capabilities.
5658040	5660280	It's just this one is the most powerful one of all
5660280	5662680	because this is the one that goes directly to what they call
5662680	5664520	fluid intelligence, which is IQ.
5666840	5670920	Well, there's two categories of folks that you outline
5671480	5674760	that they worry about or highlight the risks of AI
5674760	5676680	and you highlight a bunch of different risks.
5676680	5678920	I would love to go through those risks
5678920	5679800	and just discuss them.
5679800	5682520	Brainstorm, which ones are serious
5682520	5684760	and which ones are less serious?
5684760	5686760	But first, the Baptist and the bootleggers,
5686760	5688920	what are these two interesting groups of folks
5689800	5696040	who worry about the effect of AI on human civilization?
5696040	5697080	Or say they do.
5697080	5698200	Say, oh, okay.
5699400	5700440	Yes, I'll say they do.
5700440	5702440	The Baptist worry the bootleggers, I'll say they do.
5703720	5705560	So the Baptist and the bootleggers is a metaphor
5705560	5707960	from economics from what's called development economics.
5707960	5710600	And it's this observation that when you get social reform
5710600	5714520	movements in a society, you tend to get two sets of people
5714520	5716120	showing up arguing for the social reform.
5716680	5719000	And the term Baptist and bootleggers
5719000	5721560	comes from the American experience with alcohol prohibition.
5722520	5725720	And so in the 1900s, 1910s, there was this movement
5725720	5727320	that was very passionate at the time,
5727320	5729800	which basically said alcohol is evil
5729800	5730920	and it's destroying society.
5731800	5733880	By the way, there was a lot of evidence to support this.
5734440	5737560	There were very high rates of very high correlations
5737560	5741080	than, by the way, and now between rates of physical violence
5741080	5742280	and alcohol use.
5742280	5744680	Almost all violent crimes have either the perpetrator
5744680	5745880	or the victim are both drunk.
5747000	5748280	You see this actually in the work,
5748280	5750200	almost all sexual harassment cases in the workplace.
5750200	5752120	It's like at a company party and somebody's drunk.
5752120	5754920	Like it's amazing how often alcohol actually correlates
5754920	5757720	to actually dysfunction of these two, domestic abuse
5757720	5758680	and so forth, child abuse.
5759160	5760840	And so you had this group of people who were like,
5760840	5762840	okay, this is bad stuff and we shall outlaw it.
5762840	5764440	And those were quite literally Baptists.
5764440	5767320	Those were super committed, hardcore Christian activists
5767320	5767960	in a lot of cases.
5768520	5770840	There was this woman whose name was Kerry Nation,
5771560	5773560	who was this older woman who had been in this,
5773560	5775240	I don't know, disastrous marriage or something
5775240	5777400	and her husband had been abusive and drunk all the time.
5777400	5781080	And she became the icon of the Baptist prohibitionist
5781080	5783960	and she was legendary in that era for carrying an axe
5784840	5788280	and doing completely on her own, doing raids of saloons
5788280	5791560	and like taking her axe to all the bottles and tigs in the back.
5791560	5792360	And so-
5792360	5793480	So a true believer.
5793480	5796040	An absolute true believer with absolutely
5796040	5797160	the purest of intentions.
5797160	5799960	And again, there's a very important thing here,
5799960	5801800	which is you could look at this cynically
5801800	5804360	and you could say the Baptists are like delusional extremists,
5804360	5805800	but you can also say, look, they're right.
5805800	5808840	Like she had a point, like she wasn't wrong
5809400	5810360	about a lot of what she said.
5811320	5813160	But it turns out, the way the story goes,
5813160	5815160	is it turns out that there were another set of people
5815160	5817080	who very badly wanted to outlaw alcohol
5817080	5817560	in those days.
5817560	5818680	And those were the bootleggers,
5818680	5822120	which was organized crime that stood to make a huge amount of money
5822120	5824120	if legal alcohol sales were banned.
5824680	5826440	And this was in fact, the way the history goes,
5826440	5828520	is this was actually the beginning of organized crime in the U.S.
5828520	5830680	This was the big economic opportunity that opened that up.
5831880	5835240	And so they went in together and they didn't go in together.
5835240	5837320	Like the Baptists did not even necessarily know
5837320	5839240	about the bootleggers because they were on the moral crusade.
5839800	5841400	The bootleggers certainly knew about the Baptists
5841400	5843000	and they were like, wow, these people are like
5843000	5845000	the great front people for like, you know,
5845640	5846920	shenanigans in the background.
5847640	5849880	And they got the full state act passed, right?
5849880	5852440	And they did in fact ban alcohol in the U.S.
5852440	5854680	And you'll notice what happened, which is people kept drinking.
5855320	5856520	Like it didn't work.
5856520	5857400	People kept drinking.
5858280	5860040	The bootleggers made a tremendous amount of money.
5860600	5862920	And then over time, it became clear that it made no sense
5862920	5864840	to make it illegal and it was causing more problems.
5864840	5865960	And so then it was revoked.
5865960	5868040	And here we sit with legal alcohol 100 years later
5868040	5869080	with all the same problems.
5870440	5873160	And the whole thing was this like giant misadventure.
5874040	5876120	The Baptists got taken advantage of by the bootleggers
5876120	5878360	and the bootleggers got what they wanted and that was that.
5878360	5881960	The same two categories of folks are now sort of suggesting
5883000	5885400	the development of artificial intelligence should be regulated.
5885400	5885800	100%.
5885800	5886440	Yeah, it's the same pattern.
5886440	5888600	And the economists will tell you it's the same pattern every time.
5888600	5890040	Like this is what happened with nuclear power.
5890040	5892280	This is what happened, which is another interesting one.
5892280	5894600	But like, yeah, this happens dozens and dozens of times
5895480	5896680	throughout the last 100 years.
5896680	5898360	And this is what's happening now.
5898360	5901560	And you write that it isn't sufficient
5901560	5904120	to simply identify the actors and impugn their motors.
5904120	5906520	We should consider the arguments of both the Baptists
5906520	5908520	and the bootleggers on their merits.
5908520	5909880	So let's do just that.
5910840	5911960	Risk number one.
5915240	5916840	Will AI kill us all?
5916840	5917320	Yes.
5918120	5922840	So what do you think about this one?
5923800	5925720	What do you think is the core argument here
5926680	5931640	that the development of AGI, perhaps better said,
5932200	5934200	will destroy human civilization?
5934200	5935960	Well, first of all, you just did a sleight of hand
5935960	5937880	because we went from talking about AI to AGI.
5939880	5941400	Is there a fundamental difference there?
5941400	5942360	I don't know. What's AGI?
5943640	5944360	What's AI?
5944360	5944680	What's intelligence?
5944680	5945560	Well, I know what AI is.
5945560	5946520	AI is machine learning.
5947240	5948200	What's AGI?
5948200	5950840	I think we don't know what the bottom of the well of machine
5950840	5952600	learning is or what the ceiling is.
5952600	5955240	Because just to call something machine learning
5955240	5956760	or just to call something statistics
5956760	5958680	or just to call it math or computation
5958680	5962520	doesn't mean nuclear weapons are just physics.
5962520	5966840	So to me, it's very interesting and surprising
5966840	5968200	how far machine learning has taken.
5968200	5970120	No, but we knew that nuclear physics would lead to weapons.
5970120	5971560	That's why the scientists of that era
5971560	5974040	were always in this huge dispute about building the weapons.
5974040	5974680	This is different.
5974680	5975000	AGI is different.
5975000	5976360	Where does machine learning lead?
5976360	5976920	Do we know?
5976920	5978040	We don't know, but this is my point.
5978040	5978680	It's different.
5978680	5979560	We actually don't know.
5980120	5982360	And this is where the sleight of hand kicks in.
5982360	5984360	This is where it goes from being a scientific topic
5984360	5985400	to being a religious topic.
5986360	5988520	And that's why I specifically called out the...
5988520	5989160	Because that's what happens.
5989160	5990280	They do the vocabulary shift.
5990280	5991960	And all of a sudden, you're talking about something totally
5991960	5993000	that's not actually real.
5993000	5995960	Well, then maybe you can also, as part of that,
5995960	5999400	define the Western tradition of millennialism.
5999400	6001080	Yes, end of the world.
6001080	6001720	Apocalypse.
6001720	6002440	What is it?
6002440	6003240	Apocalypse cults.
6003800	6004680	Apocalypse cults.
6004680	6006280	Well, so we live in...
6006280	6007720	We, of course, live in a Judeo-Christian,
6007720	6009400	but primarily Christian, kind of saturated,
6009400	6011720	kind of Christian, post-Christian, secularized Christian,
6011720	6013320	kind of world in the West.
6014120	6015640	And, of course, court of Christianity
6015640	6018520	is the idea of the Second Coming and the revelations
6018520	6022280	and Jesus returning in the thousand-year utopia on Earth
6022280	6024200	and then the rapture and all that stuff.
6025640	6027400	We collectively, as a society,
6027400	6029400	we don't necessarily take all that fully seriously now.
6029400	6032520	So what we do is we create our secularized versions of that.
6032520	6034200	We keep looking for utopia.
6034200	6036280	We keep looking for basically the end of the world.
6036840	6038600	And so what you see over decades
6038600	6040040	is basically a pattern of these sort of...
6040520	6043000	This is what cults are.
6043000	6043960	This is how cults form,
6043960	6045800	as they form around some theory of the end of the world.
6045800	6049000	And so the people's temple cult, the Manson cult,
6049000	6052440	the Heaven's Gate cult, the David Koresh cult.
6052440	6053560	What they're all organized around
6053560	6055560	is like there's going to be this thing that's going to happen
6055560	6058040	that's going to basically bring civilization crashing down.
6058040	6060040	And then we have this special elite group of people
6060040	6061640	who are going to see it coming and prepare for it.
6062280	6064200	And then they're the people who are either going to stop it
6064200	6065240	or are failing stopping it.
6065240	6067160	They're going to be the people who survived on the other side
6067160	6069160	and ultimately get credit for having been right.
6069160	6070680	Why is that so compelling, do you think?
6071720	6074440	Because it satisfies this very deep need we have
6074440	6076280	for transcendence and meaning
6077080	6080200	that got stripped away when we became secular.
6080200	6082040	Yeah, but why does transcendence
6082040	6084520	involve the destruction of human civilization?
6085160	6090200	Because it's like a very deep psychological thing,
6090200	6091960	because it's like how plausible is it
6091960	6093720	that we live in a world where everything's just kind of all right?
6094840	6097160	Right, how exciting is that?
6098120	6099480	We want more than that.
6099480	6100920	But that's the deep question I'm asking.
6101960	6104520	Why is it not exciting to live in a world
6104520	6105720	where everything's just all right?
6107080	6110200	I think most of the animal kingdom
6110200	6112440	would be so happy with just all right,
6112440	6113480	because that means survival.
6114200	6116920	Why are we, maybe that's what it is.
6116920	6119880	Why are we conjuring up things to worry about?
6120440	6122760	So CS Lewis called it the God-shaped hole.
6123320	6126600	So there's a God-shaped hole in the human experience,
6126600	6128600	consciousness, soul, whatever you want to call it,
6128600	6131720	where there's got to be something that's bigger than all this.
6132360	6133560	There's got to be something transcendent.
6133560	6134920	There's got to be something that is bigger,
6134920	6136920	right, bigger, the bigger purpose of bigger meaning.
6137560	6139640	And so we have run the experiment
6139640	6141960	of we're just going to use science and rationality
6141960	6144840	and kind of everything's just going to kind of be as it appears.
6144840	6148440	And a large number of people have found that very deeply wanting
6149240	6150840	and have constructed narratives.
6150840	6152840	And this is the story of the 20th century, right?
6152840	6154600	Communism was one of those.
6154600	6156360	Communism was a form of this.
6156360	6157480	Nazism was a form of this.
6158760	6161560	Some people, you can see movements like this
6161560	6163240	playing out all over the world right now.
6163240	6166840	So you construct a kind of devil, a kind of source of evil,
6166840	6168600	and we're going to transcend beyond it.
6168600	6172680	Yeah, and the millenarian, the millenarian is kind of,
6172680	6173720	when you see a millenarian cult,
6173720	6175640	they put a really specific point on it,
6175640	6176760	which is end of the world, right?
6176760	6179240	There is some change coming.
6179240	6182040	And that change that's coming is so profound and so important
6182040	6186120	that it's either going to lead to utopia or hell on earth, right?
6186760	6189080	And it is going to, and then it's like,
6189080	6191800	what if you actually knew that that was going to happen, right?
6191800	6194040	What would you do, right?
6194040	6195480	How would you prepare yourself for it?
6195480	6198600	How would you come together with a group of like-minded people, right?
6198600	6199640	How would you, what would you do?
6199640	6201720	Would you plan like caches of weapons in the woods?
6201720	6204840	Would you like, I don't know, create underground buckers?
6204840	6207000	Would you spend your life trying to figure out a way
6207000	6208200	to avoid having it happen?
6208200	6211640	Yeah, that's a really compelling, exciting idea
6212760	6216440	to have a club over, to have a little bit of travel.
6216440	6218040	Like you get together on a Saturday night
6218040	6220680	and drink some beers and talk about the end of the world
6220680	6223400	and how you are the only ones who have figured it out.
6223400	6224120	Yeah.
6224120	6225720	And then once you lock in on that,
6225720	6227240	like how can you do anything else with your life?
6227240	6228840	Like this is obviously the thing that you have to do.
6228840	6231240	And then there's a psychological effect you alluded to.
6231240	6232200	There's a psychological effect.
6232200	6233480	If you take a set of true believers
6233480	6235560	and you lead them to themselves, they get more radical,
6235560	6237400	because they self-radicalize each other.
6237400	6241880	That said, it doesn't mean they're not sometimes right.
6241880	6242760	Yeah, the end of the world might be.
6242760	6243640	Yes, correct.
6243640	6244760	Like they might be right.
6244760	6245240	Yeah.
6245240	6245800	But like we...
6245800	6246840	Have some pamphlets for you.
6246840	6249160	Exactly.
6249160	6251800	I mean, there's, I mean, we'll talk about nuclear weapons
6251800	6253880	because you have a really interesting little moment
6253880	6256120	that I learned about in your essay.
6256120	6257800	But you know, sometimes it could be right.
6257800	6258360	Yeah.
6258360	6260600	Because we're still, you were developing
6260600	6263640	more and more powerful technologies in this case.
6263640	6265160	And we don't know what the impact
6265160	6267000	they will have on human civilization.
6267000	6269080	While we can highlight all the different predictions
6269080	6270280	about how it will be positive.
6270280	6274120	But the risks are there and you discuss some of them.
6274120	6275720	Well, the steelman is...
6275720	6276360	The steelman...
6276360	6278440	Actually, the steelman and his refutation are the same,
6278440	6280360	which is you can't predict what's going to happen, right?
6281320	6284360	You can't rule out that this will not end everything, right?
6284360	6285800	But the response to that is,
6285800	6287720	you have just made a completely non-scientific claim.
6288280	6289960	You've made a religious claim, not a scientific claim.
6289960	6291240	How does it get disproven?
6291240	6291720	There is...
6291720	6292120	And there's no...
6292120	6293560	By definition, with these kinds of claims,
6293560	6295320	there's no way to disprove them, right?
6295880	6296840	And so there's no...
6296840	6297800	You just go right on the list.
6297800	6299080	There's no hypothesis.
6299080	6301240	There's no testability of the hypothesis.
6301240	6304440	There's no way to falsify the hypothesis.
6304440	6307000	There's no way to measure progress along the arc.
6307800	6309400	Like, it's just all completely missing.
6309400	6311480	And so it's not scientific.
6311480	6311720	And...
6311720	6313480	Well, I don't think it's completely missing.
6314520	6315160	It's somewhat missing.
6315160	6317320	So, for example, the people that say,
6317320	6318520	yeah, it's going to kill all of us.
6319720	6322120	I mean, they usually have ideas about how to do that,
6322120	6326600	whether it's the paperclip maximizer or it escapes.
6327560	6329640	There's mechanism by which you can imagine it
6329640	6330760	killing all humans.
6330760	6331160	Models.
6331880	6338760	And you can disprove it by saying there is a limit
6339720	6342440	to the speed at which intelligence increases.
6343880	6345240	Maybe show that...
6347640	6350280	They sort of rigorously really describe model,
6351000	6352920	like how it could happen,
6352920	6355800	and say, no, here's a physics limitation.
6355800	6358760	There's a physical limitation to how these systems
6358760	6360840	would actually do damage to human civilization.
6360840	6364760	And it is possible they will kill 10% to 20% of the population,
6364760	6368840	but it seems impossible for them to kill 99%.
6368840	6370040	It was practical counter-arguments, right?
6370040	6371480	So you mentioned basically what I described
6371480	6373000	as the thermodynamic counter-argument,
6373000	6374120	which is sitting here today.
6374120	6376520	It's like where would the evil AGI get the GPUs?
6376520	6376920	Yeah.
6376920	6378280	Because, like, they don't exist.
6378280	6380520	So you're going to have a very frustrated baby evil AGI
6380520	6382200	who's going to be, like, trying to buy NVIDIA stock
6382200	6384440	or something to get them to finally make some chips.
6385000	6385240	Right?
6385240	6387720	So the serious form of that is the thermodynamic argument,
6387720	6389640	which is, like, okay, where's the energy going to come from?
6389640	6391080	Where's the processor going to be running?
6391080	6392520	Where's the data center going to be happening?
6392520	6393800	How is this going to be happening in secret,
6393800	6394600	such that you know it's not...
6394600	6395240	You know.
6395240	6397240	So that's a practical counter-argument
6397240	6398440	to the runaway AGI thing.
6398440	6399080	I have a...
6399080	6399560	But I have a...
6399560	6401000	And we can argue that and discuss that.
6401000	6402360	I have a deeper objection to it,
6402360	6404360	which is this is all forecasting.
6404360	6405240	It's all modeling.
6405240	6407320	It's all future prediction.
6407320	6409080	It's all future hypothesizing.
6409960	6410680	It's not science.
6411880	6412200	Sure.
6412200	6414760	It is the opposite of science.
6414760	6416280	So they'll pull up Carl Sagan.
6416280	6418280	Extraordinary claims require extraordinary proof.
6418280	6418920	Right?
6418920	6420360	These are extraordinary claims.
6420360	6423240	The policies that are being called for, right,
6423240	6425800	to prevent this are of extraordinary magnitude,
6425800	6427560	and I think we're going to cause extraordinary damage.
6428200	6429960	And this is all being done on the basis of something
6429960	6431560	that is literally not scientific.
6431560	6432760	It's not a testable hypothesis.
6432760	6433720	So the moment you say,
6433720	6435000	AI is going to kill all of us,
6435000	6436360	therefore we should ban it
6436360	6438600	or we should regulate all that kind of stuff,
6438600	6439880	that's when it starts getting serious.
6439880	6442040	Or start, you know, military airstrikes and data centers.
6442520	6443320	Oh, boy.
6443320	6443560	Right?
6444840	6445240	And like...
6446760	6449720	Yeah, that's when it starts getting real weird.
6449720	6451240	So here's the problem of millinery and cults.
6451240	6452920	They have a hard time staying away from violence.
6454360	6455800	Yeah, but violence is so fun.
6458920	6459960	If you're on the right end of it,
6460520	6461720	they have a hard time avoiding violence.
6461720	6463000	The reason they have a hard time avoiding violence
6463000	6465160	is if you actually believe the claim,
6466040	6468520	right, then what would you do to stop the end of the world?
6469080	6470360	Well, you would do anything, right?
6471000	6472520	And so, and this is where you get...
6472520	6475160	And again, if you just look at the history of millinery and cults,
6475160	6476360	this is where you get the people's temple
6476360	6477720	and everybody killing themselves in the jungle.
6477720	6479000	And this is where you get Charles Manson
6479000	6481240	and, you know, setting in to kill the pigs.
6481960	6483480	Like, this is the problem with these.
6483480	6486600	They have a very hard time drawing the line at actual violence.
6486600	6489320	And I think in this case, there's...
6489320	6491640	I mean, they're already calling for it, like today.
6491640	6493400	And, you know, where this goes from here
6493400	6494360	is they get more worked up.
6494360	6496520	Like, I think it's like really concerning.
6496520	6498120	Okay, but that's kind of the extremes.
6498520	6501560	You know, the extremes of anything are always concerning.
6502680	6504360	It's also possible to kind of believe
6504360	6507240	that AI has a very high likelihood of killing all of us.
6508360	6508760	But there's...
6509880	6514840	And therefore, we should maybe consider slowing development
6514840	6515800	or regulating.
6515800	6517880	So not violence or any of these kinds of things,
6517880	6521080	but saying like, all right, let's take a pause here.
6521080	6523560	You know, biological weapons, nuclear weapons.
6523560	6525080	Like, whoa, whoa, whoa, whoa, whoa.
6525080	6527560	This is like serious stuff.
6527560	6528920	We should be careful.
6528920	6533080	So it is possible to kind of have a more rational response, right?
6533080	6535000	If you believe this risk is real.
6535000	6535400	Believe.
6536200	6536520	Yes.
6537400	6539480	Is it possible to have a scientific approach
6539480	6542280	to the prediction of the future?
6542280	6544120	I mean, we just went through this with COVID.
6544120	6545080	What do we know about modeling?
6546440	6547240	Well, I mean...
6547240	6548840	What do we learn about modeling with COVID?
6549720	6551000	There's a lot of lessons.
6551000	6551800	They didn't work at all.
6552920	6553800	They worked poorly.
6553800	6554920	The models were terrible.
6554920	6556200	The models were useless.
6556200	6559880	I don't know if the models were useless or the people
6559880	6563160	interpreting the models and then the centralized institutions
6563160	6566040	that were creating policy rapidly based on the models
6566040	6569640	and leveraging the models in order to support their narratives
6569640	6572840	versus actually interpreting the airbars and the models
6572840	6573560	and all that kind of stuff.
6573560	6575640	What you had with COVID, in my view, you had with COVID
6575640	6577640	is you had these experts showing up.
6577640	6578760	They claimed to be scientists
6578760	6580920	and they had no testable hypotheses whatsoever.
6580920	6582280	They had a bunch of models.
6582280	6584120	They had a bunch of forecasts and they had a bunch of theories
6584120	6585880	and they laid these out in front of policymakers
6585880	6589080	and policymakers freaked out and panicked and implemented
6589080	6591080	a whole bunch of really terrible decisions
6591080	6592600	that we're still living with the consequences of.
6594360	6597320	There was never any empirical foundation to any of the models.
6597320	6598600	None of them ever came true.
6598600	6601560	Yeah, to push back, there were certainly Baptist and bootleggers
6601560	6604120	in the context of this pandemic,
6604120	6605880	but there's still a usefulness to models.
6606600	6608840	So not if they're reliably wrong.
6608840	6610520	Then they're actually anti-useful.
6610520	6611400	They're actually damaging.
6611400	6612920	But what do you do with the pandemic?
6612920	6615560	What do you do with any kind of threat?
6615560	6619880	Don't you want to kind of have several models to play with
6619880	6623640	as part of the discussion of like, what the hell do we do here?
6623640	6624440	I mean, do they work?
6625320	6627560	Because they're an expectation that they actually like work,
6627560	6628920	that they have actual predictive value.
6629720	6631000	I mean, as far as I can tell with COVID,
6631000	6633400	we just saw the policymakers just saw up themselves
6633400	6634440	into believing that there was some...
6634440	6636680	I mean, look, the scientists were at fault.
6636680	6638520	The quote-unquote scientists showed up.
6639400	6640440	So I had to submit that into this.
6640440	6641400	So there was a...
6641400	6643560	Remember the Imperial College models out of London
6643560	6644520	were the ones that were...
6644520	6645640	Like, these are the gold standard models.
6646200	6647960	So a friend of mine runs a big software company
6647960	6649960	and he was like, wow, this is like COVID's really scary.
6649960	6651560	And he's like, you know, he contacted this research
6651560	6653080	and he's like, you know, do you need some help?
6653080	6655240	You've been just building this model on your own for 20 years.
6655240	6655880	Do you need some food?
6655880	6657880	Do you like our coders to basically restructure it
6657880	6659400	so it can be fully adapted for COVID?
6659400	6661800	And the guy said yes and sent over the code.
6661800	6663720	And my friend said it was like the worst spaghetti code
6663720	6664520	he's ever seen.
6664520	6666840	That doesn't mean it's not possible to construct
6666840	6669560	a good model of pandemic with the correct airbars
6669560	6672520	with a high number of parameters that are continuously,
6673080	6676520	many times a day updated as we get more data about a pandemic.
6676520	6680680	I would like to believe when a pandemic hits the world,
6680680	6682360	the best computer scientists in the world,
6682360	6685400	the best software engineers respond aggressively
6685400	6689000	and as input take the data that we know about the virus
6689000	6692680	and it's an output, say here's what's happening
6693320	6695320	in terms of how quickly it's spreading,
6695320	6697320	what that lead in terms of hospitalization
6697320	6698840	and deaths and all that kind of stuff.
6698840	6701560	Here's how likely, how contagious it likely is.
6701560	6704840	Here's how deadly it likely is based on different conditions,
6704840	6706680	based on different ages and demographics
6706680	6707640	and all that kind of stuff.
6707640	6709560	So here's the best kinds of policy.
6709560	6714040	It feels like you can have models, machine learning,
6714760	6718920	that kind of, they don't perfectly predict the future,
6718920	6721640	but they help you do something
6721640	6723240	because there's pandemics that are like,
6726520	6728440	meh, they don't really do much harm.
6728440	6730760	And there's pandemics, you can imagine them,
6730760	6732840	they can do a huge amount of harm.
6732840	6734520	Like they can kill a lot of people.
6734520	6738680	So you should probably have some kind of data-driven models
6738680	6741560	that keep updating, that allow you to make decisions
6741560	6743720	that are basically like, where, how bad is this thing?
6744600	6748840	Now you can criticize how horrible all of that went
6748840	6750120	with the response to this pandemic,
6750120	6752520	but I just feel like there might be some value to models.
6752520	6755000	So to be useful at some point, it has to be predictive, right?
6755000	6759240	So the easy thing for me to do is to say,
6759240	6759880	obviously, you're right.
6759880	6761480	Obviously, I want to see that just as much as you do
6761480	6763000	because anything that makes it easier to navigate
6763000	6764920	through society, through a wrenching risk like that,
6764920	6766120	it sounds great.
6767240	6769480	The harder objection to it is just simply,
6769480	6772520	you are trying to model a complex dynamic system
6772520	6775160	with eight billion moving parts, like not possible.
6775160	6775720	It's very tough.
6775720	6777400	Can't be done, complex systems can't be done.
6778520	6781400	Machine learning says hold my beer, but is possible?
6781400	6781720	No?
6781720	6782280	I don't know.
6782280	6783480	I would like to believe that it is.
6784120	6785640	Put it this way, I think where you and I would agree
6785640	6788200	is I think we would like that to be the case.
6788280	6789640	We are strongly in favor of it.
6790280	6792040	I think we would also agree that no such thing
6792040	6794120	with respect to COVID or pandemics, no such thing,
6794120	6796280	at least neither you nor I think are aware,
6796280	6797880	I'm not aware of anything like that today.
6797880	6801560	My main worry with the response to the pandemic is that,
6802840	6807000	same as with aliens, is that even if such a thing existed,
6807560	6813880	and as possible it existed, the policymakers were not paying
6813880	6814360	attention.
6814360	6817480	Like there was no mechanism that allowed those kinds of models
6817480	6818360	to percolate out.
6818360	6820040	I think we have the opposite problem during COVID.
6820040	6823560	I think the policymakers, I think these people with basically
6823560	6825480	fake science had too much access to the policymakers.
6826600	6829560	Right, but the policymakers also wanted,
6829560	6831640	they had a narrative in mind and they also wanted to use
6831640	6834600	whatever model that fit that narrative to help them out.
6834600	6837640	So it felt like there was a lot of politics and not enough science.
6837640	6840040	Although a big part of what was happening, a big reason we got
6840040	6842200	lockdowns for as long as we did was because these scientists came
6842200	6844120	in with these like doomsday scenarios that were like just
6844120	6845320	like completely off the hook.
6845320	6847160	Scientists and quotes, that's not,
6847160	6848200	quote unquote, scientists.
6848200	6850840	That's not, let's give love to science.
6850840	6852120	That is the way out.
6852120	6854040	Science is a process of testing hypotheses.
6854040	6854680	Yeah.
6854680	6857400	Modeling does not involve testable hypotheses, right?
6857400	6859480	Like I don't even know that, I actually don't need,
6859480	6861480	I didn't even know that modeling actually qualifies to science.
6862360	6865160	Maybe that's a side conversation we could have some time over a beer.
6865160	6867960	That's really interesting, but what do we do about the future?
6867960	6868920	I mean, what?
6868920	6871160	So number one is when we start with number one, humility,
6871720	6873560	goes back to this thing of how do we determine the truth.
6874120	6876200	Number two is we don't believe, you know, it's the old,
6876200	6878120	I've got a hammer, everything looks like a nail, right?
6879240	6881320	I've got, oh, this is one of the reasons I gave you,
6881320	6884680	I gave Lex a book, which is the topic of the book is what happens
6884680	6887880	when scientists basically stray off the path of technical knowledge
6887880	6889960	and start to weigh in on politics and societal issues.
6889960	6891400	In this case, philosophers.
6891400	6892600	Well, in this case, philosophers.
6892600	6894600	But he actually talks in this book about like Einstein,
6894600	6896280	he talks about the nuclear age and Einstein,
6896280	6900200	he talks about the physicists actually doing very similar things
6900200	6900760	at the time.
6900760	6903880	The book is when reason goes on holiday philosophers
6903880	6906120	and politics by Nevin.
6906680	6907560	And it's just a story.
6907560	6908200	It's a story.
6908200	6909800	There's there are other books on this topic,
6909800	6910600	but this is a new one.
6910600	6911080	That's really good.
6911080	6913720	That's just the story of what happens when experts in a certain domain
6913720	6916200	decide to weigh in and become basically social engineers
6916200	6919160	and end up political, you know, basically political advisers.
6919160	6921080	And it's just a story of just unending catastrophe.
6921880	6922040	Right.
6922040	6923480	And I think that's what happened with COVID again.
6924840	6927640	And I found this book a highly entertaining and eye-opening read
6927640	6929960	filled with the amazing anecdotes of irrationality
6929960	6932120	and craziness by famous recent philosophers.
6933080	6935320	After you read this book, you will not look at Einstein the same.
6935320	6936040	Oh, boy.
6936040	6936200	Yeah.
6937160	6938280	Don't destroy my heroes.
6938280	6939720	You will not be a hero of yours anymore.
6941240	6941720	I'm sorry.
6941720	6943560	You probably shouldn't read the book.
6943560	6943880	All right.
6943880	6947800	But here's the thing, the AI risk people,
6947800	6949000	they don't even have the COVID model.
6950040	6951080	At least not that I'm aware of.
6951080	6951480	No.
6951480	6953080	Like there's not even the equivalent of the COVID model.
6953080	6954280	They don't even have the spaghetti code.
6955560	6958520	They've got a theory and a warning and a this and a that.
6958520	6961960	And like, if you ask like, okay, well, here's the ultimate example is,
6961960	6963240	okay, how do we know, right?
6963240	6964520	How do we know that an AI is running away?
6964520	6967640	Like, how do we know that the fume takeoff thing is actually happening?
6967640	6970280	And the only answer that any of these guys have given that I've ever seen is,
6970280	6974040	oh, it's when the loss rate, the loss function in the training drops.
6975160	6975320	Right.
6975320	6977240	That's when you need to like shut down the data center.
6977240	6977480	Right.
6977480	6980120	And it's like, well, that's also what happens when you're successfully training a model.
6981000	6985480	Like, what even is this is not science.
6986360	6987640	This is not, it's not anything.
6987640	6988200	It's not a model.
6988200	6989160	It's not anything.
6989160	6990760	There's nothing to arguing with it.
6990760	6991880	It's like, you know, punching jello.
6991880	6993400	Like there's, what do you even respond to?
6994040	6995400	So just put pushback on that.
6995400	6999880	I don't think they have good metrics of, yeah, when the fume is happening,
6999880	7002040	but I think it's possible to have that.
7002040	7007240	Like I just, just as you speak now, I mean, it's possible to imagine there could be measures.
7007240	7008600	It's been 20 years.
7008600	7009080	No, for sure.
7009080	7014040	But it's been only weeks since we had a big enough breakthrough in language models.
7014040	7016200	We can start to actually have this.
7016200	7021640	The thing is the AI doomer stuff didn't have any actual systems to really work with it.
7021640	7022920	Now there's real systems.
7022920	7025560	You can start to analyze like, how does this stuff go wrong?
7025560	7029240	And I think you kind of agree that there is a lot of risks that we can analyze.
7029240	7031640	The benefits outweigh the risks in many cases.
7031640	7032920	Well, the risks are not existential.
7033880	7034200	Yes.
7034200	7036520	Well, not, not, not in the fume, not in the fume paper clip.
7036520	7037000	Not this.
7037000	7037480	Let me, okay.
7037480	7039000	There's another slide of hand that you just alluded to.
7039000	7040600	There's another slide of hand that happens, which is very.
7040600	7044440	I think I'm very good at the slide of hand thing, which is very not scientific.
7044440	7046200	So the book, super intelligence, right?
7046200	7049160	Which is like the Nick Bostrom book, which is like the origin of a lot of this stuff,
7049160	7051240	which is written, you know, whatever, 10 years ago or something.
7051800	7054920	So he does this really fascinating thing in the book, which is he basically says,
7056520	7060520	there are many possible routes to machine intelligence, to artificial intelligence.
7060520	7063320	And he describes all the different routes to artificial intelligence,
7063320	7065880	all the different possible, everything from biological augmentation through to,
7066440	7067640	you know, that all these different things.
7069080	7072040	One of the ones that he does not describe is large language models,
7072040	7075240	because of course the book was written before they were invented and so they didn't exist.
7076760	7080040	In the book, he just, he describes them all and then he proceeds to treat them all as if
7080040	7080920	they're exactly the same thing.
7081800	7084840	He presents them all as sort of an equivalent risk to be dealt with in an equivalent way to
7084840	7085880	be thought about the same way.
7085880	7089240	And then the risk, the quote unquote risk that's actually emerged is actually a completely
7089240	7090920	different technology than he was even imagining.
7090920	7094200	And yet all of his theories and beliefs are being transplanted by this movement,
7094200	7095240	like straight onto this new technology.
7095240	7100200	And so again, like there's no other area of science or technology where you do that.
7101000	7104440	Like when you're dealing with like organic chemistry versus inorganic chemistry,
7104440	7107560	you don't just like say, oh, with respect to like either one,
7107560	7109720	basically maybe, you know, growing up and eating the world or something,
7109720	7111080	like they're just going to operate the same way.
7111080	7111560	Like you don't.
7112280	7116920	But you can start talking about like as, as we get more and more actual systems
7116920	7118360	that start to get more and more intelligent,
7118360	7121160	you can start to actually have more scientific arguments here.
7122040	7126440	You know, high level, you can talk about the threat of autonomous weapons systems
7126440	7129720	back before we had any automation in the military.
7129720	7131880	And that would be like very fuzzy kind of logic.
7131880	7136280	But the more and more you have drones that are becoming more and more autonomous,
7136280	7139240	you can start imagining, okay, what does that actually look like?
7139240	7141720	And what's the actual threat of autonomous weapons systems?
7141720	7143160	How does it go wrong?
7143160	7147240	And still it's, it's, it's very vague, but you start to get a sense of like,
7147960	7154920	all right, it should probably be illegal or wrong or not allowed to do like
7155880	7162600	mass deployment of fully autonomous drones that are doing aerial strikes on large areas.
7162600	7163880	I think it should be required.
7163880	7164280	Right.
7164280	7164680	So that's,
7164680	7165240	No, no, no, no.
7165240	7169160	I think it should be required that only aerial vehicles are automated.
7170200	7170440	Okay.
7170440	7171320	So you want to go the other way?
7171320	7171960	I want to go the other way.
7172840	7173720	So that, okay.
7173720	7176600	I think it's obvious that the machine is going to make a better decision than the human pilot.
7178760	7181480	I think it's obvious that it's in the best interest of both the attacker and the defender
7181480	7184440	and humanity at large if machines are making more decisions than not people.
7184440	7186360	I think people make terrible decisions in times of war.
7187160	7189880	But like there's a, there's ways this can go wrong too, right?
7189880	7192040	Well, it's worse go terribly wrong now.
7193720	7196120	This goes back to the, this is that whole thing about like the self-driving,
7196120	7198840	does the self-driving car need to be perfect versus does it need to be better than the human
7198840	7199480	driver?
7199480	7199720	Yeah.
7199720	7203880	Does the automated drone need to be perfect or does it need to be better than a human pilot
7203880	7207160	at making decisions under enormous amounts of stress and uncertainty?
7207160	7207560	Yeah.
7207560	7213960	Well, the, on average, the, the worry that AI folks have is the runaway.
7213960	7215480	They're going to come alive, right?
7215480	7217240	Then again, that's the sleight of hand, right?
7217240	7218600	Or not, not come alive.
7218600	7219560	Well, hold on a second.
7219560	7220440	You're going to become.
7220440	7222200	You lose control as well.
7222200	7224680	But then they're going to develop goals of their own.
7224680	7226040	They're going to develop a mind of their own.
7226040	7227400	They're going to develop their own, right?
7227400	7233800	No, more, more like Chernobyl style meltdown, like just bugs in the code,
7234360	7242280	accidentally, you know, force you, the results in the bombing of like large civilian areas.
7242280	7242760	Okay.
7242840	7249160	And to a degree that's not possible in the, in the current military strategies.
7249160	7249400	I don't know.
7249400	7250200	The control by humans.
7250200	7253240	Well, actually, we've been doing a lot of mass bombings of cities for a very long time.
7253240	7253800	Yes.
7253800	7255240	And a lot of civilians died.
7255240	7256120	And a lot of civilians died.
7256120	7259960	And if you watch the documentary, The Fog of War, McNamara, it spends a big part of it
7259960	7263480	talking about the firebombing of the Japanese cities, burning them straight to the ground,
7264360	7264600	right?
7264600	7268360	The, the devastation in Japan, the American military firebombing the cities in Japan
7268360	7271640	was considerably bigger devastation than the use of nukes, right?
7271640	7272920	So we've been doing that for a long time.
7272920	7274040	We also did that to Germany.
7274040	7276040	By the way, Germany did that to us, right?
7276040	7277480	Like, that's an old tradition.
7277480	7279960	The minute we got airplanes, we started doing a discriminant bombing.
7279960	7281640	So one of the things that we're still doing it.
7281640	7287880	The modern U.S. military can do with technology, with automation, but technology more broadly
7287880	7290120	is higher and higher precision strikes.
7290120	7290440	Yeah.
7290440	7293880	And so precision is obviously, and this is the, the JDAM, right?
7293880	7297640	So there was this big advance, this is big advance called the JDAM, which basically was
7297640	7301480	strapping a GPS transceiver to a, to a, to an unguided bomb and turning it into a guided,
7301480	7302360	guided bomb.
7302360	7303080	And yeah, that's great.
7303080	7306600	Like, look, that's been a big advance, but, and that's like a baby version of this question,
7306600	7309560	which is, okay, do you want like the human pilot, like guessing where the bomb's going to land,
7309560	7311800	or do you want like the machine, like guiding the bomb to its destination?
7312680	7313800	That's a baby version of the question.
7313800	7316680	The next version of the question is, do you want the human or the machine deciding whether to drop
7316680	7317400	the bomb?
7317400	7320920	Everybody just assumes the human's going to do a better job for what I think are fundamentally
7320920	7322040	suspicious reasons.
7322040	7323800	Emotional psychological reasons.
7323800	7326520	I think it's very clear that the machine is going to do a better job making that decision,
7326520	7330600	because the humans making that, making that decision are godawful, just terrible.
7331160	7331320	Yeah.
7331320	7331720	Right.
7331720	7333720	And so, so yeah, so this is the, this is the thing.
7333720	7336200	And then let's get to the, there was, can I, one more slide of hand?
7336200	7336920	Yes, sure.
7336920	7338600	Okay, please.
7338600	7339960	I'm a magician, you could say.
7339960	7340600	One more slide of hand.
7340600	7344040	These things are going to be so smart, right, that they're going to be able to destroy the
7344040	7347320	world and wreak havoc and like do all this stuff and plan and do all this stuff and evade
7347320	7350440	us and have all their secret things and their secret factories and all this stuff.
7350440	7353960	But they're so stupid that they're going to get like tangled up in their code.
7353960	7356200	And that's the, they're not going to come alive, but there's going to be some bug
7356200	7359240	that's going to cause them to like turn us all in a paper, like that they're not going,
7359240	7361880	that they're going to be genius in every way other than the actual bad goal.
7363400	7367320	And it's just like, and that's just like a like ridiculous like discrepancy.
7367320	7371480	And, and, and, and you can prove this today, you can actually address this today for the
7371480	7377080	first time with LMS, which is you can actually ask LMS to resolve moral dilemmas.
7377640	7378120	Yeah.
7378120	7381880	So you can create the scenario, you know, dot, dot, dot, this, that, this, that, this, that,
7381880	7384200	what would you as the AI do in the circumstance?
7384200	7387080	And they don't just say, destroy all humans, destroy all humans.
7387080	7391640	They will give you actually very nuanced moral, practical, tradeoff oriented answers.
7392200	7396440	And so we actually already have the kind of AI that can actually like think this through
7396440	7398360	and can actually like, you know, reason about goals.
7399160	7404520	Well, the hope is that AGI or like a very super intelligent systems have some of the
7404520	7409160	nuance that LMS have. And the intuition is they most like the will because even
7409160	7411480	these LMS have the nuance.
7412440	7415160	LMS are really, this is actually worth, worth spending a moment on.
7415160	7417800	LMS are really interesting to have moral conversations with.
7418360	7422440	And that, I just, I didn't expect I'd be having a moral conversation with a machine
7422440	7423080	in my lifetime.
7423800	7426920	And let's remember, we're not really having a conversation with a machine where
7426920	7430680	we're having a conversation with the entirety of the collective intelligence of the human species.
7430680	7431960	Exactly. Yes.
7431960	7432600	Correct.
7432600	7437400	But it's possible to imagine autonomous weapon systems that are not using LMS.
7438200	7442920	If they're smart enough to be scary, where are they not smart enough to be wise?
7444920	7448120	Like that's the part where it's like, I don't know how you get the one without the other.
7448120	7451320	Is it possible to be super intelligent without being super wise?
7451320	7454600	Well, again, you're back to, I mean, then you're back to a classic autistic computer,
7454600	7457160	right? Like you're back to just like a blind rule follower.
7457800	7460760	I've got this like core, is the paperclip thing, I've got this core rule and I'm just
7460760	7461960	going to follow it to the end of the earth.
7461960	7464360	And it's like, well, but everything you're going to be doing to execute that rule is
7464360	7466840	going to be super genius level that humans aren't going to be able to counter.
7467160	7470920	It's just, it's a mismatch in the definition of what the system is capable of.
7471560	7473320	Unlikely, but not impossible, I think.
7473320	7475080	But again, here you get to like, okay, like...
7476120	7479480	No, I'm not saying when it's unlikely, but not impossible.
7479480	7483720	If it's unlikely, that means the fear should be correctly calibrated.
7483720	7485640	Extraordinary claims require extraordinary proof.
7485640	7490120	Well, okay. So one interesting sort of tangent I would love to take on this because you mentioned
7490120	7495960	this in the essay about nuclear, which was also, I mean, you don't shy away from a little bit of
7496920	7501880	of a spicy take. So Robert Oppenheimer famously said,
7501880	7504360	now I am become death, the destroyer of worlds.
7504360	7509800	As he witnessed the first detonation of a nuclear weapon on July 16th, 1945.
7509800	7513480	And you write an interesting historical perspective, quote,
7513480	7518680	recall that John von Neumann responded to Robert Oppenheimer's famous hand wringing
7518680	7525000	about the role of creating nuclear weapons, which, you note, helped end World War II and
7525000	7531800	prevent World War III with some people confess guilt to claim credit for the sin.
7531800	7535480	And you also mentioned that Truman was harsher after meeting Oppenheimer.
7535480	7538440	He said that don't let that crybaby in here again.
7539640	7543080	Real quote, real quote, by the way, from Dean Atchison.
7544840	7548360	Boy, because Oppenheimer didn't just say the famous line.
7548360	7548760	Yeah.
7548760	7552200	He then spent years going around basically moaning, you know, going on TV and going into
7552520	7555320	the White House and basically like just like doing this hair shirt, you know,
7555320	7559240	thing self, you know, this sort of self-critical like, oh my God, I can't believe how awful I am.
7559240	7566120	So he's the, he's widely considered perhaps because the hand wringing is the father of the
7566120	7572360	atomic bomb. This is Von Neumann's criticism of him as he tried to have his cake and eat it too,
7572360	7576920	like he wanted to. And so, Von Neumann, of course, is a very different kind of personality.
7576920	7580600	And he's just like, yeah, this is like an incredibly useful thing. I'm glad we did it.
7580600	7587160	Yeah. Well, Von Neumann is as widely credited as being one of the smartest humans of the 20th
7587160	7592120	century. Certain people, everybody says like, this is the smartest person I've ever met when
7592120	7596600	they've met him. Anyway, that doesn't mean smart, doesn't mean wise.
7599160	7605160	So I would love to sort of, can you make the case both for and against the critique of Oppenheimer
7605160	7610680	here? Because we're talking about nuclear weapons. Boy, do they seem dangerous.
7610680	7614280	Well, so the critique goes deeper. And I left this out. Here's the real substance. I left
7614280	7619800	it out because I didn't want to dwell on nukes in my AI paper. But here's the deeper thing
7619800	7622840	that happened. And I'm really curious, this movie coming out this summer, I'm really curious to see
7622840	7626440	how far he pushes this because this is the real drama in the story, which is it wasn't just a
7626440	7629480	question of our nukes good or bad. It was a question of, should Russia also have them?
7630200	7636440	And what actually happened was Russia got the, America invented the bomb, Russia got the bomb,
7636440	7641160	they got the bomb through espionage, they got American and, you know, they got American scientists
7641160	7646040	and foreign scientists working on the American project, some combination of the two basically gave
7646040	7651080	the Russians the designs for the bomb. And that's how the Russians got the bomb. There's this dispute
7651080	7657000	to this day of Oppenheimer's role in that. If you read all the histories, the kind of composite
7657080	7660440	picture, and by the way, we now know a lot actually about Soviet espionage in that era,
7660440	7664200	because there's been all this declassified material in the last 20 years that actually shows a lot
7664200	7667320	of very interesting things. But if you kind of read all the histories, what you kind of get is
7667320	7671560	Oppenheimer himself probably was not a, he probably did not hand over the nuclear secrets
7671560	7676200	himself. However, he was close to many people who did, including family members. And there were
7676200	7681240	other members of the Manhattan Project who were Russian Soviet SS and did hand over the bomb. And
7681240	7687000	so the view that Oppenheimer and people like him had that this thing is awful and terrible and,
7687000	7692440	oh my God, and, you know, all this stuff, you could argue fed into this ethos at the time that
7692440	7695800	resulted in people thinking that the Baptist is thinking that the only principle thing to do is
7695800	7702760	to give the Russians the bomb. And so the moral beliefs on this thing and the public discussion
7702760	7706440	and the role that the inventors of this technology play, this is the point of this book when they
7706440	7711080	kind of take on this sort of public intellectual moral kind of thing, it can have real consequences.
7711560	7715160	Because we live in a very different world today because Russia got the bomb than we would have
7715160	7719160	lived in had they not gotten the bomb. The entire 20th century, second half of the 20th century
7719160	7723080	would have played out very different had those people not given Russia the bomb. And so the
7723080	7729080	stakes were very high then. The good news today is nobody sitting here today, I don't think worrying
7729080	7732600	about like an analogous situation with respect to like, I'm not really worried that Sam Altman is
7732600	7737960	going to decide to give the Chinese the design for AI, although he did just speak at a Chinese
7737960	7742600	conference, which is interesting. But however, I don't think that's what's at play here. But what's
7742600	7746680	at play here are all these other fundamental issues around what do we believe about this and then
7746680	7750440	what laws and regulations and restrictions that we're going to put on it. And that's where I draw
7750440	7754760	it like a direct straight line. And anyway, and my reading of the history on Nukes is like the
7754760	7758840	people who were doing the full hair shirt public, this is awful, this is terrible, actually had
7758840	7763560	like catastrophically bad results from taking those views. And that's what I'm worried is going
7763560	7767240	to happen again. But is there a case to be made that you really need to wake the public up to
7767240	7772680	the dangers of nuclear weapons when they were first dropped? Like really, like, educate them on
7772680	7777000	like, this is extremely dangerous and destructive weapon. I think the education kind of happened
7777000	7781960	quick and early. Like, how? It was pretty obvious. How? We dropped one bomb and destroyed an entire
7781960	7790760	city. Yeah, so 80,000 people dead. But I don't like the reporting of that, you can report that in
7790760	7796760	all kinds of ways. You can do all kinds of slants like war is horrible, war is terrible. You can
7797720	7802360	make it seem like nuclear, the use of nuclear weapons is just a part of a war and all that
7802360	7807400	kind of stuff. Something about the reporting and the discussion of nuclear weapons resulted in us
7807400	7816520	being terrified in awe of the power of nuclear weapons. And that potentially fed in a positive
7816520	7822520	way towards the game theory of mutually assured destruction. Well, so this gets to what actually
7823320	7826680	happens. Some of us may be playing devils advocate here. Yeah, sure, of course. Let's get to what
7826680	7829880	actually happened and then kind of back into that. So what actually happened, I believe, and again,
7829880	7833080	I think this is a reasonable reading of history, is what actually happened was nukes then prevented
7833080	7836920	World War III. And they prevented World War III through the game theory of mutually assured
7836920	7842600	destruction. Had nukes not existed, there would have been no reason why the Cold War did not go
7842600	7847960	hot. And the military planners at the time thought, both on both sides, thought that there
7847960	7850600	was going to be World War III on the plains of Europe and they thought there was going to be
7850680	7853560	like 100 million people dead. It was the most obvious thing in the world to happen.
7854600	7859080	And it's the dog that didn't bark. It may be the best single net thing that happened in the
7859080	7862520	entire 20th century, is that that didn't happen. Yeah, actually, just on that point,
7863240	7867480	you say a lot of really brilliant things. It hit me just as you were saying it.
7869320	7875480	I don't know why it hit me for the first time, but we got two wars in a span of 20 years.
7876360	7881160	Like, we could have kept getting more and more world wars and more and more ruthless.
7881960	7885000	It actually, you could have had a US versus Russia war.
7885000	7889720	You could have. By the way, there's another hypothetical scenario. The other hypothetical
7889720	7894040	scenario is the Americans got the bomb, the Russians didn't. And then America is the big
7894040	7897240	dog. And then maybe America would have had the capability to actually roll back there at current.
7899000	7901480	I don't know whether that would have happened, but like it's entirely possible.
7902040	7906040	Right. And the act of these people who had these moral positions about, because they could
7906040	7908920	forecast, they could model, they could forecast the future of how this technology would get used,
7908920	7912600	made a horrific mistake, because they basically ensured that the Iron Curtain would continue
7912600	7915640	for 50 years longer than it would have otherwise. And again, like, these are counterfactuals. I
7915640	7921880	don't know that that's what would have happened. But like, the decision to hand the bomb over
7921880	7926040	was a big decision made by people who were very full of themselves.
7927000	7932520	Yeah, but so me as an America, me as a person that loves America, I also wonder if US was the
7932520	7939320	only ones with nuclear weapons. That was the argument for handing the, that was the,
7940040	7943240	the guys who, the guys who handed over the bomb. That was actually their moral argument.
7943240	7948200	Yeah, I would, I would probably not hand it over to, I would be careful about the regimes
7948200	7951560	you handed over to. Maybe give it to like the British or something.
7952360	7957080	Like, like a democratically elected government.
7957080	7960280	Well, there are people to this day who think that those Soviet spies did the right thing,
7960280	7963560	because they created a balance of terror, as opposed to the US having just, and by the way,
7963560	7967720	let me, let me, balance of terror, let's tell the full verse, such a sexy ring to it. Okay,
7967720	7970600	so the full version of the story is John von Neumann as a hero of both yours and mine,
7970600	7976360	the full version of the story is he advocated for a first right. So when the US had the bomb,
7976360	7980040	and Russia did not, he advocated for, he said, we need to strike them right now.
7981000	7982280	Strike Russia. Yeah.
7984040	7985480	Yes. Von Neumann.
7985480	7990440	Yes, because he said World War Three is inevitable. He was very hardcore.
7991160	7995640	He, his, his theory was, his theory was World War Three is inevitable. We're definitely going
7995640	7998680	to have a World War Three. The only way to stop World War Three is we have to take them out right
7998680	8001480	now. And we have to take them out right now before they get the bomb, because this is our last chance.
8002760	8003640	Now, again, like,
8003640	8005720	Is this an example of philosophers and politics?
8005720	8007800	I don't know if that's in there or not, but this is in the standard.
8007800	8009240	No, but it is. Yeah. Meaning is that.
8009240	8012760	Yeah, this is on the other side. So, so most of the case studies, most of the case studies in
8012760	8017880	books like this are the crazy people on the left. Yeah. Von Neumann is a story arguably of the
8017880	8021160	crazy people on the right. Yeah, stick to computing, John.
8021160	8024600	Well, this is the thing. And this is, this is the general principle. It goes back to our core
8024600	8028200	thing, which is like, I don't know whether any of these people should be making any of these calls.
8028200	8031240	Yeah. Because there's nothing in either von Neumann's background or Oppenheimer's
8031240	8034920	background or any of these people's background that qualifies them as moral authorities.
8034920	8040280	Yeah. Well, this actually brings up the point of in AI, who are the good people to,
8040280	8045080	to reason about the morality, the ethics, the outside of these risks outside, like,
8045080	8050280	the more complicated stuff that you, you agree on is, you know, this will go into the hands of
8050280	8054040	bad guys and all the kinds of ways they'll do is, is interesting and dangerous.
8055480	8059640	Is dangerous in interesting, unpredictable ways. And who is the right person?
8059640	8063320	Who are the right kinds of people to make decisions how to respond to it?
8063400	8068040	Or is it tech people? So the history of these fields, this is what he talks about in the book,
8068040	8073240	the history of these fields is that the, the competence and capability and intelligence
8073240	8077480	and training and accomplishments of senior scientists and technologists working on a technology,
8078360	8081720	and then being able to then make moral judgments in the use of that technology,
8081720	8086280	that track record is terrible. That track record, that track record is like catastrophically bad.
8086840	8091320	The people, just to look at it, the people that develop that technology are usually not going
8091320	8095640	to be the right people. Well, why would they? So the claim is, of course, they're the knowledgeable
8095640	8100920	ones, but the problem is they've spent their entire life in a lab, right? They're not theologians.
8101720	8104920	But so what you find, what you find when you read, when you read this, when you look at these
8104920	8109240	histories, what you find is they generally are very thinly informed on history, on sociology,
8109240	8117000	on, on, on theology, on morality, ethics, they tend to manufacture their own worldviews from scratch.
8117080	8124600	They tend to be very sort of thin. They're not remotely the arguments that you would be having
8124600	8128120	if you got like a group of highly qualified theologians or philosophers or, you know.
8129000	8136520	Well, let me sort of, as the devil's advocate takes a sip of whiskey, say that I, I agree with,
8137960	8142920	with that, but also it seems like the people who are doing kind of the ethics departments and
8142920	8151640	these tech companies go sometimes the other way, which is they're not nuanced on the, on history
8151640	8157720	or theology or this kind of stuff. They almost becomes a kind of outraged activism towards
8159400	8166040	directions that don't seem to be grounded in history and humility and nuance. It's again,
8166040	8171480	drenched with arrogance. So I'm not sure which is worse. Oh, no, they're both bad. Yeah. So
8171480	8176600	definitely not them either. But I guess, but look, this is a hard, yeah, it's a hard problem.
8176600	8180040	This is our problem. This goes back to where we started, which is okay, who has the truth. And
8180040	8184760	it's like, well, you know, like how do societies arrive at like truth and how do we figure these
8184760	8189800	things out? And like our elected leaders play some role in it. You know, we all play some role in it.
8190760	8194920	There have to be some set of public intellectuals at some point that bring, you know, rationality
8194920	8198840	and judgment and humility to it. Yeah, those people are few and far between. We should probably
8198840	8205160	prize them very highly. Yeah. Celebrate humility in our public leaders. So getting to risk number
8205160	8210840	two, will AI ruin our society? Short version, as you write, if the murder robots don't get us,
8210840	8216680	the hate speech and misinformation will. And the action you recommend in short,
8216680	8225960	don't let the thought police suppress AI. Well, what is this risk of the effect of
8226040	8233560	misinformation in a society that's going to be catalyzed by AI? Yeah, so this is the social media.
8233560	8236760	This is what you just alluded to. It's the activism kind of thing that's popped up in these
8236760	8241160	companies in the industry. And it's basically, from my perspective, it's basically part two
8241160	8245320	of the war that played out over social media over the last 10 years. Because you probably remember
8245320	8250280	social media 10 years ago was basically who even wants this, who wants a photo of what your cat
8250280	8253960	had for breakfast, like this stuff is like silly and trivial. And why can't these nerds like figure
8253960	8259000	out how to invent something useful and powerful? And then certain things happened in the political
8259000	8262600	system. And then it's sort of the polarity on that discussion switched all the way to social
8262600	8266360	media is like the worst, most corrosive, most terrible, most awful technology ever invented.
8266360	8271400	And then it leads to terrible, the wrong politicians and policies and politics and all
8271400	8276280	this stuff. And that all got catalyzed into this very big kind of angry movement, both inside
8276280	8280440	and outside the companies, to kind of bring social media to heal. And that got focused in
8280440	8284600	particularly on two topics, so-called hate speech and so-called misinformation. And that's
8284600	8288520	been the saga playing out for the last decade. And I don't even really want to even argue the pros
8288520	8292680	and cons of the sides just to observe that that's been like a huge fight and has had big consequences
8292680	8300040	to how these companies operate. Basically, those same sets of theories, that same activist approach,
8300040	8304280	that same energy is being transplanted straight to AI. And you see that already happening. It's
8304280	8308760	why ChatGPT will answer, let's say, certain questions and not others. It's why it gives you
8308760	8312600	the canned speech about, you know, whenever it starts with as a large language model, I cannot,
8312600	8315320	you know, basically means that somebody has reached in there and told that it can't talk
8315320	8320520	about certain topics. Do you think some of that is good? So it's an interesting question.
8321080	8326440	So a couple of observations. So one is the people who find this the most frustrating are the people
8326440	8333960	who are worried about the murder robots. So and in fact, the so-called ex-risk people,
8333960	8337960	right, they started with the term AI safety, the term became AI alignment. When the term became
8337960	8341000	AI alignment is when this switch happened from where we're worried it's going to kill us all to
8341000	8345880	we're worried about hate speech and misinformation. The AI ex-risk people have now renamed their thing
8345880	8352040	AI not kill everyone ism, which I have to admit is a catchy term. And they are very frustrated by
8352040	8355880	the fact that the sort of activist driven hate speech misinformation kind of thing is taking
8355880	8359080	over, which is what's happened is taking over the AI ethics field has been taken over by the
8359080	8364360	hate speech misinformation people. You know, look, would I like to live in a world in which like
8364360	8367800	everybody was nice to each other all the time and nobody ever said anything mean and nobody ever
8367800	8371640	used a bad word and everything was always accurate and honest. Like that sounds great.
8371640	8374520	Do I want to live in a world where there's like a centralized thought police working
8374520	8378680	through the tech companies to enforce the view of a small set of elites that they're going to
8378680	8383080	determine what the rest of us think and feel like absolutely not. There could be a middle ground
8383080	8389000	somewhere like Wikipedia type of moderation. There's moderation of Wikipedia that is somehow
8389000	8395800	crowdsourced where you don't have centralized elites. But it's also not completely just a
8395800	8401640	free for all because the if you have the entirety of human knowledge at your fingertips,
8402280	8408360	you can do a lot of harm. Like if you have a good assistant that's completely uncensored,
8408360	8417080	they can help you build a bomb. They can help you mess with people's physical well being, right?
8417080	8421880	If they because that information is out there on the internet. And so they presumably there's
8422600	8429960	it would be, you could see the positives in censoring some aspects of an AI model
8429960	8434920	when it's helping you commit literal violence. And there's a section later section of the essay
8434920	8439480	where I talk about bad people doing bad things, which and there's a set of things that we
8439480	8444120	should discuss there. What happens in practice is these lines, as you alluded to this already,
8444120	8447800	these lines are not easy to draw. And what I've observed in the social media version of this
8447800	8451960	is like the way I describe it as the slippery slope is not a fallacy. It's an inevitability.
8451960	8455080	The minute you have this kind of activist personality that gets in a position to make
8455080	8460440	these decisions, they take it straight to infinity. Like it goes into the crazy zone
8460440	8464360	like almost immediately and never comes back because people become drunk with power, right?
8464360	8467240	And they look if you're in the position to determine what the entire world thinks and
8467240	8472200	feels and reads and says, like you're going to take it. And you know, Elon has ventilated this
8472200	8475080	with the Twitter files over the last, you know, three months. And it's just like crystal clear
8475080	8479880	like how bad it got there. Now, reason for optimism is what Elon is doing with the community notes.
8481640	8485800	So community notes is actually a very interesting thing. So what Elon is trying to do with
8485800	8489880	community notes is he's trying to have it where there's only a community note when people who
8489880	8496360	have previously disagreed on many topics agree on this one. Yes. That's what I'm trying to get at
8496360	8500760	is like there's, there could be Wikipedia like models or community notes type of models where
8501320	8508360	allows you to essentially either provide context or sensor in a way that's not resist the slippery
8508360	8511640	slope nature. Now, there's another power. There's an entirely different approach here,
8511640	8516440	which is basically we have AIs that are producing content. We could also have AIs that are consuming
8516440	8521000	content, right? And so one of the things that your assistant could do for you is help you consume
8521000	8525320	all the content, right? And basically tell you when you're getting played. So for example,
8525320	8529000	I'm going to want the AI that my kid uses, right, to be very, you know, child safe,
8529000	8532120	and I'm going to want it to filter for him all kinds of inappropriate stuff that he shouldn't
8532120	8535560	be saying just because he's a kid. Yeah. Right. And you see what I'm saying is you can implement
8535560	8539000	that. You could do the architecture. You could say you can solve this on the client side, right?
8539000	8542040	Solving on the server side gives you an opportunity to dictate for the entire world,
8542040	8546440	which I think is where you take the slippery slope to hell. There's another architectural
8546440	8549560	approach, which is to solve this on the client side, which is certainly what I would endorse.
8550440	8555880	It's at risk number five will AI lead to bad people doing bad things. I can just imagine
8555960	8560200	language models used to do so many bad things, but the hope is there that you can have
8561640	8566120	large language models used to then defend against it by more people, by smarter people, by
8567480	8570760	more effective people, skilled people, all that kind of stuff.
8570760	8575400	Three-point argument on bad people doing bad things. So number one, right, you can use the
8575400	8579640	technology defensively. And there's a, we should be using AI to build like broad spectrum vaccines
8579640	8583320	and antibiotics for like bio weapons. And we should be using AI to like hunt terrorists and
8583400	8586040	catch criminals. And like we should be doing like all kinds of stuff like that.
8586040	8589080	And in fact, we should be doing those things even just to like go get like, you know,
8589080	8592440	basically go eliminate risk from like regular pathogens that aren't like constructed by an AI.
8592440	8595960	So there's, there's, there's the whole, there's a whole defensive set of things.
8596680	8601160	Second is we have many laws on the books about the actual bad things, right? So it is actually
8601160	8605560	illegal to be a criminal, you know, to commit crimes, to commit terrorist acts, to, you know,
8605560	8610200	build pathogens with the intent to deploy them to kill people. And so we have those,
8610200	8613480	we don't, we actually don't need new laws for the vast majority of these scenarios. We actually
8613480	8618360	already have the laws in the book on the books. The third argument is the minute, and this is
8618360	8621560	sort of the foundational one that gets really tough, but the minute you get into this thing,
8621560	8624360	which, which you were kind of getting into, which is like, okay, but like, don't you need
8624360	8627880	censorship sometimes, right? And don't you need restriction sometimes? It's like, okay, what is
8627880	8635080	the cost of that? And in particular in the world of open source, right? And so is open source AI
8635080	8641960	going to be allowed or not? If open source AI is not allowed, then what is the regime that's going
8641960	8647080	to be necessary legally and technically to prevent it from developing, right? And here, again, is
8647080	8650680	where you get into, and people have proposed that these kinds of things, you get into, I would say,
8650680	8656040	pretty extreme territory pretty fast. Do we have a monitor agent on every CPU and GPU that reports
8656040	8660680	back to the government, what we're doing with our computers? Are we seizing GPU clusters to get
8660680	8665160	beyond a certain size? Like, and then by the way, how are we doing all that globally? Right?
8665160	8669560	And like, if China is developing an LLM beyond the scale that we think is allowable, are we going
8669560	8674600	to invade? Right? And you have figures on the AI X risk side who are advocating, you know,
8674600	8678520	potentially up to nuclear strikes to prevent, you know, this kind of thing. And so here you get into
8678520	8682840	this thing. And again, you could maybe say this is, you know, you could even say this is what good,
8682840	8687560	bad or indifferent or whatever. But like, here's the comparison of nukes. The comparison of nukes
8687560	8691560	is very dangerous because one is just nukes were just, just above, although we can come back to
8691560	8695080	nuclear power. But the other thing was like with nukes, you could control plutonium, right? You
8695080	8700280	could track plutonium. And it was like hard to come by. AI is just math and code, right? And it's
8700280	8703560	in like math textbooks. And it's like their YouTube videos that teach you how to build it. And like,
8703560	8706920	there's open source already opens horse, you know, there's a 40 billion parameter model running
8706920	8713160	around already called Falcon online that anybody can download. And so, okay, you walk down the logic
8713160	8716120	path that says we need to have guardrails on this. And you find yourself in a
8716920	8723000	authoritarian totalitarian regime of thought control and machine control that would be so brutal
8723880	8727640	that you would have destroyed the society that you're trying to protect. And so I just don't
8727640	8733880	see how that actually works. So you have to understand my brain is going full, full steam ahead
8733880	8737720	here because I agree with basically everything you're saying when I'm trying to play devil's
8737720	8743960	advocate here. Because okay, you highlighted the fact that there is a slippery slope to human
8743960	8747720	nature. The moment you sense there's something, you start to sense everything.
8750680	8758520	That alignment starts out sounding nice, but then you start to align to the beliefs of some
8759480	8764600	select group of people. And then it's just your beliefs. The number of people you're
8764600	8770280	aligned to smaller smaller as that group becomes more and more powerful. Okay, but that just speaks
8770280	8776200	to the people that censor usually the assholes and the assholes get richer. I wonder if it's
8776200	8783240	possible to do without that for AI. The one way to ask this question is, do you think the base
8783240	8790360	models, the base, the baseline foundation models should be open sourced? Like what Mark Zuckerberg
8790360	8795880	is saying they want to do? So I look, I mean, I think it's totally appropriate that companies that
8795960	8800680	are in the business of producing a product or service should be able to have a wide range of
8800680	8805320	policies that they put, right? And I'll just again, I want a heavily censored model for my eight-year-old.
8806120	8809560	Like I actually want that. Like I would pay more money for the ones more heavily censored than the
8809560	8813880	one that's not, right? And so like there are certainly scenarios where companies will make that
8813880	8819480	decision. Look, an interesting thing you brought up, or is this really a speech issue? One of the
8819480	8824360	things that the big tech companies are dealing with is that content generated from an LLM is not
8824360	8830040	covered under section 230, which is the law that protects internet platform companies from being
8830040	8837880	sued for the user-generated content. And so it's actually, yes. And so there's actually a question,
8837880	8842280	I think there's still a question which is can big American companies actually feel generative AI at
8842280	8846680	all? Or is the liability actually going to just ultimately convince them that they can't do it?
8846680	8850440	Because the minute the thing says something bad, and it doesn't even need to be hate speech, it
8850440	8856120	could just be like an enact, it could hallucinate a product detail on a vacuum cleaner, and all
8856120	8860360	of a sudden the vacuum cleaner company sues for misrepresentation. And there's any symmetry there,
8860360	8864280	right? Because the LLM is going to be producing billions of answers to questions, and it only
8864280	8867880	needs to get a few wrong. The loss has to get updated really quick here. Yeah, and nobody knows
8867880	8873960	what to do with that, right? So anyway, there are big questions around how companies operate at all.
8873960	8877640	So we talk about those. But then there's this other question of like, okay, the open source,
8877640	8881560	so what about open source? And my answer to your question is kind of like, obviously, yes,
8881560	8886120	the models have to, there has to be full open source here, because to live in a world in which
8886120	8892040	that open source is not allowed is a world of draconian speech control, human control, machine
8892040	8897080	control. I mean, you know, black helicopters with jackbooted thugs coming out, repelling down and
8897080	8903240	seizing your GPU, like territory. Well, no, no, I'm 100% serious. That's you're saying slippery
8903240	8906520	slope always leads that. No, no, no, no, no, no, that's what's required to enforce it. Like, how
8906600	8912120	will you enforce a ban on open source? You could add friction to it, like hard to get the models,
8912120	8916040	because people will always be able to get the models. But it'll be more in the shadows, right?
8916040	8921320	The leading open source model right now is from the UAE. Like, the next time they do that, what do
8921320	8928920	we do? Yeah, like, oh, I see, you're like, the 14 year old in Indonesia comes out with a breakthrough
8928920	8931560	model. You know, we talked about most great software comes from a small number of people.
8931560	8935000	Some kid comes out with some big new breakthrough and quantization or something and has some huge
8935000	8939720	breakthrough and like, what are we going to like, invade Indonesia and arrest him?
8939720	8943800	It seems like in terms of size of models and effectiveness of models, the big tech companies
8943800	8949160	will probably lead the way for quite a few years. And the question is of what policies they should
8949160	8959960	use. The kid in Indonesia should not be regulated, but should Google Meta, Microsoft, OpenAI be
8960680	8967480	regulated? Okay, so when does it become dangerous? Is the danger that it's, quote,
8967480	8972600	as powerful as the current leading commercial model? Or is it that it is just at some other
8972600	8977320	arbitrary threshold? And then by the way, like, look, how do we know? Like, what we know today
8977320	8980680	is that you need like a lot of money to train these things. But there are advances being made
8980680	8984280	every week on training efficiency and you know, data, all kinds of synthetic data. Look, I don't
8984280	8987400	even like the synthetic data thing we're talking about, maybe some kid figures out a way to auto
8987400	8990760	generate synthetic data. That's going to change everything. Yeah, exactly. And so like sitting
8990760	8994840	here today, like the breakthrough just happened, right? You made this point, like the breakthrough
8994840	8999960	just happened. So we don't know what the shape of this technology is going to be. I mean, the big
8999960	9005480	shock, the big shock here is that, you know, whatever number of billions of parameters basically
9005480	9010120	represents at least a very big percentage of human thought, like, who would have imagined that?
9010920	9014040	And then there's already work underway, there was just this paper that just came out that
9014040	9018600	basically takes a GPT-3 scale model and compresses it down to run on a single 32-core CPU.
9019400	9024200	Like, who would have predicted that? Yeah. You know, some of these models now you can run
9024200	9028040	in Raspberry Pi's, like today they're very slow, but like, you know, maybe they'll be a, you know,
9028040	9032760	perceived real perform, you know, like, it's math and code. And here we're back in here,
9032760	9037480	we're back in math and code. It's math and code. It's math, code and data. It's bits.
9037480	9043240	Mark's just like walking away at this point. He just screw it. I don't know what to do with this.
9043240	9049800	You guys created this whole internet thing. Yeah. Yeah. I'm a huge believer in open source here.
9049800	9053160	So my argument is, we're going to have to see, here's my argument. My argument, my full argument
9053160	9056360	is AI is going to be like air, it's going to be everywhere. Like, this is just going to be in
9056360	9058920	text, it already is. It's going to be in textbooks and kids are going to grow up knowing how to do
9058920	9061720	this. And it's just going to be a thing. It's going to be in the air and you can't like pull
9061720	9065320	this back anywhere. You can pull back air. And so you just have to figure out how to live in this
9065320	9069640	world, right? And then that's where I think like all this hand-wringing about AI risk is basically
9069640	9074680	complete waste of time because the effort should go into, okay, what is the defensive approach?
9075320	9078760	And so if you're worried about AI generated pathogens, the right thing to do is to have a
9078760	9085000	permanent project warp speed, right? And funded lavishly. Let's do a Manhattan project for biological
9085000	9089640	defense, right? And let's build AIs and let's have like broad spectrum vaccines where like we're
9089640	9095400	insulated from every pathogen, right? And what the interesting thing is because it's software,
9096360	9101880	a kid in his basement, a teenager could build like a system that defends against like the worst
9101880	9110120	that the worst. I mean, and to me, defense is super exciting. It's like, if you believe in
9110120	9115400	the good of human nature, the most people want to do good to be the savior of humanity is really
9115400	9123000	exciting. Yes. Okay, that's a dramatic statement, but like to help people. To help people. Yeah,
9123080	9128920	okay, what about just to jump around? What about the risk of will AI lead to crippling inequality?
9129880	9132520	You know, because we're kind of saying everybody's life will become better.
9133640	9138120	Is it possible that the rich get richer here? Yeah. So this is actually ironically goes back to
9138120	9141880	Marxism. So because this was the course of the core claim of Marxism, right? Basically, it was
9141880	9144920	that the owner, the owners of capital would basically own the means of production. And then
9144920	9148920	over time, they would basically accumulate all the wealth the workers would be paying in, you know,
9148920	9152360	and getting nothing in return, because they wouldn't be needed anymore, right? Marx is very
9152440	9155480	worried about what he called mechanization or what later became known as automation.
9156680	9160680	And that, you know, the workers would be emissarated and the capitalist would end up with all. And so
9160680	9164440	this was one of the core principles of Marxism. Of course, it turned out to be wrong about every
9164440	9168760	previous wave of technology. The reason it turned out to be wrong about every previous wave of
9168760	9173640	technology is that the way that the self-interested owner of the machines makes the most money is
9173640	9179160	by providing the production capability in the form of products and services to the most people,
9179160	9183080	the most customers as possible, right? The largest... And this is one of those funny things
9183080	9186040	where every CEO knows this intuitively, and yet it's like hard to explain from the outside.
9186600	9190440	The way you make the most money in any business is by selling to the largest market you can possibly
9190440	9195400	get to. The largest market you can possibly get to is everybody on the planet. And so every large
9195400	9199480	company does is everything that it can to drive down prices to be able to get volumes up to be
9199480	9203240	able to get to everybody on the planet. And that happened with everything from electricity. It
9203240	9206760	happened with telephones. It happened with radio. It happened with automobiles. It happened with
9206840	9214120	smartphones. It happened with PCs. It happened with the internet. It happened with mobile broadband.
9214120	9218600	It's happened, by the way, with Coca-Cola. It's happened with like every... Basically,
9218600	9222600	every industrially produced, good or service, people want to drive it to the largest possible
9222600	9228280	market. And then as proof of that, it's already happened, right? Which is the early adopters of
9228280	9234520	like GPD and Bing are not like Exxon and Boeing. They're your uncle and your nephew,
9235080	9239080	right? It's either freely available online or it's available for $20 a month or something,
9239080	9245320	but these things went... This technology went mass market immediately. And so look, the owners of
9245320	9248520	the means of production, whoever does this, now as I mentioned these trillion dollar questions,
9248520	9251800	there are people who are going to get really rich doing this, producing these things, but
9251800	9254600	they're going to get really rich by taking this technology to the broadest possible market.
9255400	9260120	So yes, they'll get rich, but they'll get rich having a huge positive impact on...
9260120	9262440	Yeah, making the technology available to everybody.
9263080	9268120	And again, smartphone, same thing. So there's this amazing kind of twist in business history,
9268120	9273080	which is you cannot spend $10,000 on a smartphone, right? You can't spend $100,000. You can't
9273080	9275880	spend... Like I would buy the million dollar smartphone, like I'm signed up for it. Like if
9275880	9279400	it's like, suppose a million dollar smartphone was like much better than the $1,000 smartphone,
9279400	9283640	like I'm there to buy it, it doesn't exist. Why doesn't it exist? Apple makes so much more
9283640	9287160	money driving the price further down from $1,000 than they would trying to harvest,
9287160	9289960	right? And so it's just this repeating pattern you see over and over again,
9290440	9294680	where the... And what's great about it, what's great about it is you do not need to rely on
9294680	9299320	anybody's enlightened right generosity to do this. You just need to rely on capitalist self-interest.
9301320	9307400	What about AI taking our jobs? Yeah, so very, very similar thing here. There's a core fallacy,
9307400	9311400	which again was very common in Marxism, which is what's called the lump of labor fallacy.
9311400	9315240	And this is sort of the fallacy that there is only a fixed amount of work to be done in the world.
9315240	9318680	And if the... And it's all being done today by people. And then if machines do it,
9318680	9323800	there's no other work to be done by people. And that's just a completely backwards view on how
9323800	9328360	the economy develops and grows, because what happens is not. In fact, that what happens is
9328920	9332440	the introduction of technology into production process causes prices to fall.
9333080	9337240	As prices fall, consumers have more spending power. As consumers have more spending power,
9337240	9343080	they create new demand. That new demand then causes capital and labor to form into new enterprises
9343080	9347240	to satisfy new wants and needs. And the result is more jobs at higher wages.
9347320	9353320	New wants and needs. The worry is that the creation of new wants and needs at a rapid rate
9354040	9359160	will mean there's a lot of turnover in jobs. So people will lose jobs. Just the actual
9359160	9363720	experience of losing a job and having to learn new things and new skills is painful for the
9363720	9368680	individuals. Two things. One is the new jobs are often much better. So this actually came up.
9368680	9371560	There was this panic about a decade ago and all the truck drivers are going to lose their jobs,
9371560	9375560	right? And number one, that didn't happen because we haven't figured out a way to actually finish
9375560	9379640	that yet. But the other thing was like a truck driver, like I grew up in a town that was basically
9379640	9384280	consisted of a truck stop, right? And I knew a lot of truck drivers. And truck drivers live a decade
9384280	9390120	shorter than everybody else. It's actually like a very dangerous, like literally they have like
9390120	9395000	high-racist skin cancer. And on the left side of their body from being in the sun all the time,
9395000	9398920	the vibration of being in the truck is actually very damaging to your physiology.
9398920	9404680	And there's actually, perhaps partially because of that reason, there's a shortage of
9406040	9410680	people who want to be truck drivers. The question always you want to ask somebody like
9410680	9415080	that is, do you want your kid to be doing this job? And like most of them will tell you, no,
9415080	9418200	like I want my kid to be sitting in a cubicle somewhere like where they don't have this,
9418200	9422280	like where they don't die 10 years earlier. And so the new jobs, number one, the new jobs are
9422280	9426520	often better, but you don't get the new jobs until you go through the change. And then to your point,
9426520	9430360	the training thing, you know, it's always the issue is can people adapt? And again, here you
9430360	9434120	need to imagine living in a world in which everybody has the AI assistant capability,
9434760	9437480	right, to be able to pick up new skills much more quickly and be able to have some, you know,
9437480	9439400	be able to have a machine to work with to augment their skills.
9439400	9442440	It's still going to be painful, but that's the process of life.
9442440	9444600	It's painful for some people. I mean, there's no, like, there's no question
9444600	9447880	it's painful for some people. And they're, you know, they're, yes, it's not, again,
9447880	9451080	I'm not a utopian on this. And it's not like it's positive for everybody in the moment,
9451080	9455720	but it has been overwhelmingly positive for 300 years. I mean, look, the concern here,
9455720	9460040	the concern, the concern, this concern has played out for literally centuries. And, you know,
9460040	9464040	this is the sort of leadite, you know, the story of the leadites that you may remember,
9464040	9469000	there was a panic in the 2000s around outsourcing was going to take all the jobs. There was a panic
9469000	9476760	in the 2010s that robots are going to take all the jobs. In 2019, before COVID, we had more jobs
9476760	9479960	at higher wages, both in the country and in the world than at any point in human history.
9480520	9486520	And so the overwhelming evidence is that the net gain here is like just like wildly positive.
9486520	9490440	And most people like overwhelmingly come out the other side being huge beneficiaries of this.
9491080	9496280	So you write that the single greatest risk, this is the risk you're most convinced by,
9496280	9501240	the single greatest risk of AI is that China wins global AI dominance and we,
9501240	9504520	the United States and the West do not. Can you elaborate?
9504520	9509480	Yeah. So this is the other thing, which is a lot of the sort of AI risk debates today,
9509480	9512760	sort of assume that we're the only game in town, right? And so we have the ability to kind of
9512760	9516200	sit in the United States and criticize ourselves and, you know, have our government like,
9516280	9519320	you know, beat up on our companies and we're figuring out a way to restrict what our companies can do.
9519320	9521720	And, you know, we're going to, you know, we're going to ban this and ban that,
9521720	9525000	restrict this and do that. And then there's this like other like force out there that
9525000	9529560	like doesn't believe we have any power over them whatsoever. And they have no desire to sign up
9529560	9533160	for whatever rules we decided to put in place. And they're going to do whatever it is they're
9533160	9538200	going to do. And we have no control over it at all. And it's China and specifically the Chinese
9538200	9545160	Communist Party. And they have a completely publicized, open, you know, plan for what they're
9545160	9550440	going to do with AI. And it is not what we have in mind. And not only do they have that as a vision
9550440	9554040	and a plan for their society, but they also have it as a vision and plan for the rest of the world.
9554040	9555960	So their plan is what? Surveillance?
9555960	9561560	Yeah, authoritarian control. So authoritarian population control, you know, good old fashioned
9561560	9567480	communist authoritarian control, and surveillance and enforcement, and social credit scores and
9567480	9572280	all the rest of it. And you are going to be monitored and metered within an inch of everything
9572360	9577320	all the time. And it's going to be basically the end of human freedom. And that's their goal. And,
9577320	9580040	you know, they justify it on the basis of that's what leads to peace.
9580040	9587080	And you're worried that the regulating in the United States will hold progress enough to where
9588280	9590280	the Chinese government would win that race?
9590280	9594280	So their plan, yes, yes. And the reason for that is they, and again, they're very public on this,
9594280	9597880	they have their plan is to proliferate their approach around the world. And they have this
9597880	9601560	program called the digital Silk Road, right, which is building on their Silk Road investment
9601560	9604680	program. And they've got their, they've been laying, they've been laying networking infrastructure
9604680	9609240	all over the world with their 5G right work with their company, Huawei. And so they've been laying
9609240	9612920	all this fabric, but financial and technological fabric all over the world. And their plan is to
9612920	9617240	roll out their vision of AI on top of that, and to have every other country be running their version.
9617880	9622280	And then if you're a country prone to, you know, authoritarianism, you're going to find this to
9622280	9626600	be an incredible way to become more authoritarian. If you're a country, by the way, not prone to
9626600	9629960	authoritarianism, you're going to have the Chinese Communist Party running your infrastructure and
9629960	9633560	having backdoors into it, right, which is also not good.
9634280	9637720	What's your sense of where they stand in terms of the race towards
9638680	9641240	superintelligence as compared to the United States?
9641240	9645000	Yeah, so good news is they're behind, but bad news is they, you know, they, let's just say they
9645000	9649400	get access to everything we do. So they're probably a year behind at each point in time,
9649400	9653400	but they get, you know, downloads, I think of basically all of our work on a regular basis
9653400	9657640	through a variety of means. And they are, you know, at least, we'll see they're at least
9657640	9662040	putting out reports of very competitive, just put out a report last week of a GPT 3.5 analog.
9663720	9666920	They put out this report, forget what it's called, but they put out this report of the
9666920	9670760	cell and they did, and they, you know, the way when OpenAI puts out, they, they,
9670760	9675080	one of the ways they test, you know, GPT is they, they run it through standardized exams
9675080	9679880	like the SAT, right? Just how you can kind of gauge how smart it is. And so the Chinese report,
9679880	9686440	they ran their LM through the Chinese equivalent of the SAT, and it includes a section on Marxism.
9687400	9691560	And a section on Mao Zedong thought, and it turns out their AI does very well on both of those
9691560	9699880	topics, right? So like, this, this alignment thing, communist AI, right? Like literal communist AI,
9699880	9704520	right? And so their vision is like, that's the, you know, so, you know, you, you can just imagine
9704520	9710040	like you're a school, you know, you're a kid 10 years from now in Argentina or in Germany or in
9710840	9715080	who knows where, uh, Indonesia. And you ask, they, I'd explain to you like how the economy
9715080	9718760	works and it gives you the most cheery upbeat explanation of Chinese style communism you've
9718760	9724760	ever heard, right? So like the stakes here are like really big. Well, my, as we've been talking
9724760	9728840	about, my hope is not just for the United States, but it would just, uh, the kitten as basement
9728840	9735640	with open source LLM. Cause I, I don't know if I, um, trust large centralized institutions
9735640	9741480	with super powerful AI, no matter what their ideology is a power corrupts.
9742120	9749240	You've been investing in tech companies for about, let's say 20 years and, uh, about 15 of which was,
9749240	9755640	uh, with Andreessen Horowitz. Uh, what interesting trends in tech have you seen over that time?
9755640	9759000	Let's just talk about companies and just the evolution of the tech industry.
9759000	9763320	I mean, the big shift over 20 years has been that tech used to be a tools industry,
9763960	9769320	for basically from like 1940 through to about 2010, almost all the big successful companies
9769400	9775160	were picks and shovels companies. So PC database, smartphone, you know, some, some, some tool that
9775160	9780680	somebody else would pick up and use. Since 2010, most of the big wins have been in applications.
9781320	9786920	So a company that starts a, you know, it starts in an existing industry and goes directly to the
9786920	9791400	customer in that industry. And then, you know, the early examples there were like Uber and Lyft
9791400	9797560	and Airbnb. Um, and then that model is kind of elaborating out. Um, the AI thing is actually
9797640	9801320	a reversion on that for now. Cause like most of the AI business right now is actually in cloud
9801320	9804360	provision of, of, of AI APIs for other people to build on, but,
9804360	9806360	But the big thing will probably be an app.
9806360	9810440	Yeah. I think, I think most of the money, I think probably will be in whatever, yeah,
9810440	9815160	your AI financial advisor or your AI doctor or your AI lawyer or, you know, take your pick
9815160	9818760	of whatever the domain is. Um, and there, and what's interesting is, you know, we,
9818760	9823160	the valley kind of does everything. We, we, the entrepreneurs kind of elaborate every possible
9823160	9827640	idea. And so there will be a set of companies that like make AI, um, something that can be
9827640	9831160	purchased and used by large law firms. Um, and then there will be other companies that just
9831160	9833400	go direct to market as a, as an AI lawyer.
9834600	9841320	What advice could you give for startup founder? Just having seen so many successful companies,
9841320	9845560	so many companies that fail also. What advice could you give to a startup founder,
9845560	9850200	someone who wants to build the next super successful startup in the tech space,
9850280	9852680	the Googles, the apples, the twitters.
9854520	9858120	Yeah. So the great thing about the really great founders is they don't take any advice. So,
9859800	9864120	so if you find yourself listening to advice, maybe you shouldn't do it. Um,
9864120	9869400	Well, that's actually just to elaborate on that. If you could also speak to great founders too.
9869400	9869960	Yeah.
9869960	9871640	Like what, what makes a great founder?
9872280	9878440	So it makes a great founder is super smart, um, coupled with super energetic, coupled with super
9878440	9881000	courageous. I think it's some of those, those three.
9881000	9883240	And intelligence, passion and courage.
9883240	9886920	The first two are traits and the third one is a choice. I think courage is a choice.
9887480	9891240	Well, cause courage is a question of pain, tolerance, right? Um,
9892280	9896600	so, um, how, how many times you will want to get punched in the face before you quit?
9897000	9897080	Yeah.
9897080	9901800	Um, and here's maybe the biggest thing people don't understand about what it's like to be a startup
9901800	9905880	founder is it gets, it gets very romanticized, right? Um, and even when, even when they fail,
9905880	9908680	it still gets romanticized about like what a great adventure it was, but like
9909560	9913480	the reality of it is most of what happens is people telling you, no, and then they usually
9913480	9917960	follow that with your stupid, right? No, I will not come to work for you. Um, and I will not leave
9917960	9921000	my cushy job cool to come work for you. No, I'm not going to buy your products. You know,
9921000	9924200	no, I'm not going to run a story about your company. No, I'm not this that the other thing.
9924840	9929000	Um, and so a huge amount of what people have to do is just get used to just getting punched.
9929000	9932200	And, and, and the reason people don't understand this is because when you're a founder, you cannot
9932200	9935320	let on that this is happening because it will cause people to think that you're weak and they'll
9935320	9940040	lose faith in you. Yeah. So you have to pretend that you're having a great time when you're dying
9940040	9946440	inside, right? You're just in misery. But why, why do they do it? Why do they do it? Yeah,
9946440	9949800	that's the thing. It's, it's like it is a level. This is actually one of the conclusions I think
9949800	9953240	is it, I think it's actually, for most of these people on a risk adjusted basis, it's probably
9953240	9957240	an irrational act. They could probably be more financially successful on average if they just
9957240	9961880	got like a real job and in a big company. Um, but there's, you know, some people just have an
9961880	9965640	irrational need to do something new and build something for themselves. And, you know, some
9965640	9968920	people just can't tolerate having bosses. Oh, here's the fun thing is how do you reference
9968920	9972520	check founders? Right? So you call it, you know, normally you reference check your time hiring
9972520	9975880	somebody as you call the bosses there at their, you know, and you find out if they were good
9975880	9979720	employees. And now you're trying to reference check Steve Jobs, right? And it's like, Oh God,
9979720	9982680	he was terrible. You know, he was a terrible employee. He never did what we told him to do.
9982680	9989000	Yeah. So what's a good reference? If you want the previous boss to actually say that
9989720	9992840	they never did what you told them to do, that might be a good thing.
9992840	9996680	Well, ideally, ideally what you want is I will go, I would like to go to work for that person.
9997720	10001400	He worked for me here and now I'd like to work for him. Now, unfortunately, most people can't,
10001400	10005240	their egos can't, can't handle that. So they won't say that, but that's the ideal.
10005240	10010440	What advice would you give to those folks in the space of intelligence, passion and courage?
10011160	10015320	So I think the other big thing is you see people sometimes who say, I want to start a company
10015320	10019080	and then they kind of work through the process of coming up with an idea. And generally, those
10019080	10023880	don't work as well as the case where somebody has the idea first, and then they kind of realize
10023880	10026920	that there's an opportunity to build a company and then they just turn out to be the right kind
10026920	10032520	of person to do that. When you say idea, do you mean long-term big vision or do you mean
10032520	10038360	specifics of like product? I would say specific. Like specifically what, yeah, specifics. Like,
10038360	10041160	what is the, because for the first five years, you don't get to have vision. You just got to
10041160	10044520	build something people want and you got to figure out a way to sell it to them, right? It's very
10044520	10049240	practical or you never get to big vision. So the first, the first part, you have an idea of a
10049240	10052520	set of products or the first product that can actually make some money. Yeah. Like it's got to,
10052520	10055880	the first product's got to work, by which I mean like it has to technically work, but then it has
10055880	10059640	to actually fit into the category in the customer's mind of something that they want. And then, and
10059640	10062520	then by the way, the other part is they have to want to pay for it. Like somebody's got to pay
10062520	10065800	the bills. And so you got to figure out how to price it and whether you can actually extract the
10065800	10073320	money. So usually it is much more predictable. Success is never predictable, but it's more
10073320	10077880	predictable if you start with a great idea and then back into starting the company. So this is
10077880	10081240	what we did, you know, we had Mosaic before we had Escape. The Google guys had the Google search
10081240	10086600	engine working at Stanford, right? The, you know, yeah, actually, there's tons of examples where
10086600	10091400	they, you know, Pierre Omadir had eBay working before he left his previous job. So I really love
10091400	10096360	that idea of just having a thing, a prototype that actually works before you even begin to
10096360	10101000	remotely scale. Yeah. By the way, it's also far easier to raise money, right? Like the ideal pitch
10101000	10104040	that we receive is, here's the thing that works. Would you like to invest in our company or not?
10104040	10110200	Like that's so much easier than here's 30 slides with a dream, right? And then we have this concept
10110200	10114200	called the idea maze, which our biology friend of Boston came up with when he was with us.
10115080	10119320	So, so, so then there's this thing, this goes to mythology, which is, you know, there's a
10119320	10123240	mythology that kind of, you know, these, these ideas, you know, kind of arrive like magic or
10123240	10126200	people kind of stumble into them. It's like eBay with the pest dispensers or something.
10126760	10132920	The reality usually with the big successes is that the founder has been chewing on the problem
10132920	10137880	for five or 10 years before they start the company. And they often worked on it in school,
10138840	10143320	or they even experimented on it when they were a kid. And they've been kind of training up over
10143320	10146680	that period of time to be able to do the thing. So they're like a true domain expert.
10147560	10151080	And it sort of sounds like mom and apple pie, which is, yeah, you want to be a domain expert
10151080	10154760	in what you're doing. But you would, you know, the mythology is so strong of like, oh, I just
10154760	10158360	like had this idea in the shower and now I'm doing it. Like it's generally not that.
10158360	10166280	No, because it's, well, maybe in the shower, we had the exact product implementation details.
10166280	10170600	But yeah, usually you're going to be for like years, if not decades, thinking about
10172280	10178760	like everything around that. Well, we call it the idea maze because the idea maze basically is
10178760	10182440	like there's all these permutations. Like for any idea, there's like all these different
10182440	10185480	permutations, who should the customer be, what shape, form should the product have,
10185480	10190840	and how should we take it to market and all these things. And so the really smart founders
10190840	10193480	have thought through all these scenarios by the time they go out to raise money.
10194360	10198760	And they have like detailed answers on every one of those fronts because they put so much
10198760	10204520	thought into it. The sort of the sort of more haphazard founders haven't thought about any of
10204520	10206520	that. And it's the detailed ones who tend to do much better.
10206520	10211000	So how do you know what to take a leap? If you have a cushy job or happy life?
10211960	10215160	I mean, the best reason is just because you can't tolerate not doing it, right?
10215160	10217960	This is the kind of thing where if you have to be advised into doing it, you probably shouldn't do
10217960	10222760	it. And so it's probably the opposite, which is you just have such a burning sense of this has
10222760	10227080	to be done. I have to do this. I have no choice. What if it's going to lead to a lot of pain?
10227080	10232520	It's going to lead to a lot of pain. I think that's what if it means losing sort of social
10232520	10238280	relationships and damaging your relationship with loved ones and all that kind of stuff.
10238360	10242520	Yeah, look, so it's going to put you in a social tunnel for sure, right? So you're going to like,
10244200	10248120	there's this game you can play on Twitter, which is you can do any whiff of the idea that there's
10248120	10251800	basically any such thing as work-life balance and that people should actually work hard and
10251800	10255560	everybody gets mad. But like the truth is, all the successful founders are working 80 hour weeks
10255560	10260680	and they form very strong social bonds with the people they work with. They tend to lose a lot
10260680	10265160	of friends on the outside or put those friendships on ice. That's just the nature of the thing.
10265560	10269480	You know, for most people, that's worth the trade-off. You know, the advantage maybe younger
10269480	10272360	founders have is maybe they have less, you know, maybe they're not, you know, for example,
10272360	10274920	if they're not married yet or don't have kids yet, that's an easier thing to bite off.
10275560	10276760	Can you be an older founder?
10276760	10280600	Yeah, you definitely can. Yeah. Yeah. Many of the most successful founders are second,
10280600	10284600	third, fourth time founders. They're in their 30s, 40s, 50s. The good news of being an older
10284600	10289000	founder is you know more and you know a lot more about what to do, which is very helpful. The problem
10289000	10293400	is, okay, now you've got like a spouse and a family and kids and like you've got to go to the baseball
10293400	10295960	game and like you can't go to the baseball, you know, and so it's good.
10297080	10299160	Life is full of difficult choices. Yes.
10299160	10305800	I can't reason. You've written a blog post on what you've been up to. You wrote this in October 2022.
10307000	10313240	Quote, mostly I try to learn a lot. For example, the political events of 2014 to 2016 made clear
10313240	10318520	to me that I didn't understand politics at all. Referencing maybe some of this book here.
10318600	10324200	So, I deliberately withdrew from political engagement and fundraising and instead read
10324200	10330360	my way back into history and as far to the political left and political right as I could.
10330360	10333400	So, just high level question. What's your approach to learning?
10334440	10341080	Yeah, so it's basically, I would say it's auto-divac. So, it's sort of going down the rabbit holes.
10341960	10345560	So, it's a combination. I kind of alluded to it in that quote. It's a combination of
10345560	10350600	breadth and depth. And so, I tend to, yeah, I tend to, I go broad by the nature of what I do,
10350600	10354040	I go broad, but then I tend to go deep in a rabbit hole for a while, read everything I can,
10354040	10358200	and then come out of it. And I might not revisit that rabbit hole for, you know, another decade.
10358200	10363240	And in that blog post that I recommend people go check out, you actually list a bunch of different
10363240	10367640	books that you recommend on different topics on the American left and the American right.
10369000	10373160	It's just a lot of really good stuff. The best explanation for the current structure of our
10373160	10377800	society and politics, you give two recommendations, four books on the Spanish Civil War, six books on
10377800	10384440	deep history of the American right, comprehensive biographies of Adolf Hitler, one of which I read
10384440	10389320	can recommend, six books on the deep history of the American left. So, American right and American
10389320	10395880	left looking at the history to give you the context. Biography of Vladimir Lenin, two of them
10396760	10400760	on the French Revolution. Actually, I have never read a biography on Lenin. Maybe that will be
10400760	10406040	useful. Everything's been so Marx focused. The Sebastian biography of Lenin is extraordinary.
10406840	10410680	Victor Sebastian, okay. It'll blow your mind. Yeah. So, it's still useful to read. It's incredible.
10410680	10413400	Yeah, it's incredible. I actually think it's the single best book on the Soviet Union.
10413960	10418360	So, that, the perspective of Lenin, it might be the best way to look at the Soviet Union versus
10418360	10424280	Stalin versus Marx versus, very interesting. So, two books on fascism and anti-fascism
10425160	10430920	by the same author, Paul Gottry, a brilliant book on the nature of mass movements and collective
10430920	10435720	psychology, the definitive work on intellectual life under totalitarianism, the captive mind,
10436360	10441640	the definitive work on the practical life under totalitarianism. There's a bunch, there's a bunch,
10441640	10447000	and the single best book. First of all, the list here is just incredible. But you say the single
10447000	10453160	best book I have found on who we are and how we got here is the ancient city by Neuma Dennis,
10453240	10461400	Fostel de Kulankis. I like it. What did you learn about who we are as a human civilization from
10461400	10465800	that book? Yeah. So, this is a fascinating book. This one's free, by the way. It's a book from
10465800	10471080	the 1860s. You can download it or you can buy prints of it. But it was this guy who was a
10471080	10477160	professor at the Sorbonne in the 1860s, and he was apparently a savant on antiquity, on Greek
10477160	10482440	and Roman antiquity. And the reason I say that is because his sources are 100% original Greek and
10482440	10487960	Roman sources. So, he wrote basically a history of Western civilization from on the order of 4,000
10487960	10493720	years ago to basically the present times entirely working on original Greek and Roman sources.
10494920	10499080	And what he was specifically trying to do was he was trying to reconstruct from the stories of the
10499080	10502760	Greeks and the Romans. He was trying to reconstruct what life in the West was like before the Greeks
10502760	10510120	and the Romans, which was in the civilization known as the Indo-Europeans. And the short answer,
10510120	10516120	and this is sort of circa 4,000, 2,000 BC to, you know, sort of 500 BC, kind of that 1500 year
10516120	10522040	stretch where civilization developed. And his conclusion was basically cults. They were basically
10522040	10528520	cults. And civilization was organized into cults. And the intensity of the cults was like a million
10528520	10534040	fold beyond anything that we would recognize today. Like it was a level of all-encompassing
10534600	10541320	belief and an action around religion. That was at a level of extremeness that we wouldn't even
10541320	10547240	recognize it. And so specifically, he tells the story of basically, there were three levels of
10547240	10552760	cults. There was the family cult, the tribal cult, and then the city cult as society scaled up.
10553320	10560040	And then each cult was a joint cult of family gods, which were ancestor gods and then nature gods.
10560760	10565880	And then your bonding into a family, a tribe, or a city was based on your adherence to that
10565880	10572680	religion. People who were not of your family tribe city worshiped different gods, which gave you
10572680	10579000	not just the right with the responsibility to kill them on sight. Right? So they were serious
10579000	10583800	about their cults. Hard core. By the way, shocking development, I did not realize there's a zero
10583800	10587720	concept of individual rights. Like even up through the Greeks and even in the Romans,
10587720	10590840	they didn't have the concept of individual rights. Like the idea that as an individual,
10590840	10594840	you have some rights, just like noop. Right? And you look back and you're just like, wow,
10594840	10598600	that's just like crazily like fascist and a degree that we wouldn't recognize today. But it's like,
10598600	10604040	well, they were living under extreme pressure for survival. And the theory goes, you could not have
10604040	10606760	people running around making claims to individual rights when you're just trying to get like your
10606760	10611640	tribe through the winter, right? Like you need like hardcore command and control. And actually,
10612200	10615400	through modern political lens, those cults were basically both fascist and communist.
10616040	10619480	They were fascist in terms of social control, and then they were communist in terms of economics.
10621160	10623960	But you think that's fundamentally that like pull towards
10624920	10626520	cults is within us?
10626520	10632920	Well, so my conclusion from this book, so the way we naturally think about the world we live in
10632920	10637880	today is like, we basically have such an improved version of everything that came before us, right?
10637880	10641320	Like we have basically, we've figured out all these things around morality and ethics and
10641320	10644600	democracy and all these things. And like they were basically stupid and retrograde and were
10644600	10649960	like smart and sophisticated. And we've improved all this. I after reading that book, I now believe
10649960	10653960	in many ways the opposite, which is no, actually, we are still running in that original model,
10653960	10659160	we're just running in an incredibly diluted version of it. So we're still running basically in
10659160	10663400	cults. It's just our cults are at like a thousandth or a millionth the level of intensity, right?
10663400	10669080	And so our, so just to take religions, you know, the modern experience of a Christian in our time,
10669080	10673400	even somebody who considers him a devout Christian is just a shadow of the level of intensity of
10673400	10677880	somebody who belonged to a religion back in that period. And then by the way, we have constraints,
10677880	10683160	it goes back to our discussion, we then sort of endlessly create new cults. Like we're trying
10683160	10689000	to fill the void, right? And the void is a void of bonding. Okay. Living in their era,
10689000	10692760	like everybody living today, transport in that era would view it as just like completely intolerable
10692760	10696920	in terms of like the loss of freedom and the level of basically fascist control. However,
10696920	10700280	every single person in that era, and he really stresses this, they knew exactly where they
10700280	10704120	stood. They knew exactly where they belonged. They knew exactly what their purpose was.
10704120	10707000	They know exactly what they needed to do every day. They know exactly why they were doing it.
10707000	10709400	They had total certainty about their place in the universe.
10709400	10713640	So the question of meaning, the question of purpose was very distinctly clearly defined for
10713640	10717720	them. Absolutely. Overwhelmingly, undisputably, undeniably.
10718360	10723880	As we turn the volume down on the cultism, we start to the search for meanings that's getting
10723880	10727640	harder and harder. Yes. Because we don't have that. We are, we are ungrounded. We are, we are,
10727640	10731640	we are uncentered and we, and we all feel it, right? And that's why we reach for, you know,
10731640	10735720	it's why we still reach for religion. It's why we reach for, you know, we, people start to take
10735720	10738520	on, you know, let's say, you know, a faith in science, maybe beyond where they should put it.
10739320	10742280	You know, and by the way, like sports teams are like a, you know, they're like a tiny little
10742280	10746120	version of a cult and, you know, the, you know, Apple keynotes are a tiny little version of a cult,
10746840	10751080	right? You know, political, you know, and there's cult, you know, there's full blown
10751080	10754520	cults on both sides of the political spectrum right now, right? You know, operating in plain
10754600	10757080	science. But still not full blown, compared as to what it was.
10757080	10760040	Compared to what it used to be. I mean, we would today consider full blown, but like,
10760040	10763240	yes, they're, they're at like, I don't know, a hundred thousandth or something of the intensity
10763240	10767800	of what people had back then. So, so we live in a world today that in many ways is more advanced
10767800	10771160	and moral and so forth. And it's certainly a lot nicer, much nicer world to live in, but we live
10771160	10776440	in a world that's like very washed out. It's like everything has become very colorless and gray as
10776440	10780360	compared to how people used to experience things, which is I think why we're so prone to reach for
10780360	10785160	drama. There's something in us that's deeply evolved where we want that back.
10786520	10790840	And I wonder where it's all headed as we turn the volume down more and more.
10790840	10795720	What advice would you give to young folks today? In high school and college, how to be
10795720	10798440	successful in their career, how to be successful in their life?
10798440	10804040	Yes. So the tools that are available today, I mean, are just like, I sometimes, you know,
10804040	10808040	I sometimes bore, you know, kids by describing like what it was like to go look up a book,
10808040	10811960	you know, to try to like discover a fact and, you know, in the old days, the 1970s, 1980s and
10811960	10815000	go to the library and the card catalog and the whole thing. You go through all that work and
10815000	10819160	then the book is checked out and you have to wait two weeks and like, like to be in a world,
10819160	10822760	not only where you can get the answer to any question, but also the world now, you know,
10822760	10825400	the AI world where you've got like the assistant that will help you do anything,
10825400	10830200	help you teach, learn anything, like your ability both to learn and also to produce is just like,
10830200	10834520	I don't know, a million fold beyond what it used to be. I have a blog post I've been wanting to
10834520	10840040	write. It was, I call out where, where are the hyperproductive people? Like,
10840040	10844760	Good question. Right. Like with these tools, like there should be authors that are writing like
10844760	10849320	hundreds or thousands of like outstanding books. Well, with the authors, there's a consumption
10849320	10855720	question too. But yeah, well, maybe not, maybe not. You're right. But so the tools are much more
10855720	10861080	powerful. Artists, musicians, right? Why aren't musicians producing a thousand times the number
10861080	10867880	of songs, right? Like, like the tools are spectacular. So what, what's the explanation?
10867880	10874360	And by way of advice, like, what is motivation starting to be turned down a little bit or what?
10874360	10878280	I think it might be distraction. It's so easy to just sit and consume.
10879480	10883880	That I think people get distracted from production. But if you wanted to, you know,
10883880	10888040	as a young person, if you wanted to really stand out, you could get on like a hyperproductivity
10888040	10892920	curve very early on. There's a great, you know, the story, there's a great story in Roman history
10892920	10897880	of plenty of the elder who was this legendary statesman. And I died in the Vesuvius eruption
10897880	10902200	trying to rescue his friends. But he was famous both for being a savant of basically being a
10902200	10905800	polymath, but also being an author. And he wrote apparently like hundreds of books, most of which
10905800	10909880	have been lost, but he like wrote all these encyclopedias. And he literally like would be
10909880	10913720	reading and writing all day long, no matter what else is going on. And he so he would like travel
10913720	10916840	with like four slaves, and two of them were responsible for reading to him. And two of them
10916840	10921000	were responsible for taking dictation. And so like, he'd be going across country and like
10921000	10924920	literally he would be writing books like all the time. And apparently they were spectacular.
10924920	10927240	There's only a few that have survived, but apparently they were amazing.
10927240	10930680	So there's a lot of value to being somebody who finds focus in this life.
10930680	10934040	Yeah, like, and there are examples, like there are, you know, there's this guy,
10934040	10938520	judge was just named Posner, Posner, who wrote like 40 books and was also a great federal judge.
10939160	10942600	You know, there's our friend, Balji, I think it's like this, he's one of these,
10942600	10946920	you know, where his output is just prodigious. And so it's like, yeah, I mean, with these tools,
10946920	10950840	why not? And I kind of think we're at this interesting kind of freeze frame moment where
10950840	10953320	like this, these tools are not everybody's hands and everybody's just kind of staring
10953320	10956920	at them trying to figure out what to do. The new tools. We have discovered fire.
10956920	10960280	Yeah. And trying to figure out how to use it to cook. Yeah, right.
10961560	10966680	You told Tim Ferriss that the perfect day is caffeine for 10 hours and alcohol for four hours.
10967640	10972760	You didn't think I'd be mentioning this, did you? It balances everything out perfectly,
10972760	10979080	as you said. So perfect. So let me ask, what's the secret to balance and maybe to happiness in life?
10980520	10984440	I don't believe in balance. So I'm the wrong person to ask. Can you elaborate
10984440	10988600	why you don't believe in balance? I mean, maybe it's just, and I look, I think people,
10988600	10992360	I think people are wired differently. So I think it's hard to generalize this kind of thing. But
10992440	10995560	I'm much happier and more satisfied when I'm fully committed to something. So
10996120	10998360	I'm very much in favor of imbalance. Yeah.
10999480	11002680	In balance. And that applies to work, to life, to everything.
11003640	11007800	Yeah. Now, I happen to have whatever twist of personality traits lead that in non-destructive
11007800	11012600	dimensions, including the fact that I've actually, I now no longer do the 10-4 plan. I stopped drinking.
11012600	11016360	I do the caffeine, but not the alcohol. So there's something in my personality where I,
11016360	11020600	I, whatever maladaption I have is inclining me towards productive things, not on productive
11020600	11026680	things. So you're one of the wealthiest people in the world. What's the relationship between wealth
11026680	11034840	and happiness? Oh, money and happiness. So I think happiness, I don't think happiness is the thing
11035720	11037880	to strive for. I think satisfaction is the thing.
11039000	11042280	That's, that just sounds like happiness, but turned down a bit.
11042280	11047400	No deeper. So happiness is, you know, a walk in the woods at sunset, an ice cream cone,
11048360	11053240	a kiss. The first ice cream cone is great. This 1000th ice cream cone,
11054200	11056440	not so much at some point, the walks in the woods get boring.
11056440	11060040	What's the distinction between happiness and satisfaction?
11060040	11065880	Satisfaction is a deeper thing, which is like having found a purpose and fulfilling it, being useful.
11066600	11074920	So just something that permeates all your days, just this general contentment of being useful.
11074920	11079240	Then I'm fully satisfying my faculties, then I'm fully delivering, right,
11079240	11082360	on the gifts that I've been given, that I'm, you know, net making the world better,
11082360	11086680	that I'm contributing to the people around me, right? And then I can look back and say,
11086680	11091080	wow, that was hard, but it was worth it. I think generally it seems to leave people in a better
11091080	11094760	state than pursuit of pleasure, pursuit of quote unquote happiness.
11094760	11096200	Does money have anything to do with that?
11096200	11099880	I think the founders, the founding fathers in the US threw this off kilter when they used the
11099880	11101880	phrase pursuit of happiness, I think they should have said.
11102680	11103480	Pursuit of satisfaction.
11103480	11105960	They said pursuit of satisfaction, we might live in a better world today.
11105960	11108520	Well, you know, they could have elaborated on a lot of things.
11109400	11110680	They could have tweaked the second amendment.
11110680	11113480	I think they were smarter than they realized. They said, you know what,
11113480	11117560	we're going to make it ambiguous and let these humans figure out the rest,
11117560	11120520	these tribal cult like humans figure out the rest.
11123320	11124760	But money empowers that.
11124760	11129000	So I think, and I think they're, I mean, look, I think Elon is, I don't think I'm even a great
11129000	11131560	example, but I think Elon would be the great example of this, which is like, you know, look,
11131560	11135000	he's a guy who from every day of his life, from the day he started making money at all,
11135000	11136920	he just plows into the next thing.
11138040	11141800	And so I think money is definitely an enabler for satisfaction.
11141800	11144200	Money applied to happiness leads people down very dark paths.
11146200	11147560	Very destructive avenues.
11148360	11151160	Money applied to satisfaction, I think could be, is a real tool.
11152120	11155480	I always, by the way, I was like, you know, Elon is the case study for behavior.
11155480	11159720	But the other thing that's always really made me think is Larry Page was asked one time what
11159720	11163240	his approach to philanthropy was, and he said, oh, I'm just my philanthropic plan is just give
11163240	11164040	all the money to Elon.
11166600	11166840	Right.
11166840	11173000	Well, let me actually ask you about Elon. What are your, you've interacted with quite a lot of
11173000	11176920	successful engineers and business people. What do you think is special about Elon?
11176920	11178200	We talked about Steve Jobs.
11178920	11183800	What, what do you think is special about him as a leader as an innovator?
11183800	11189240	Yeah. So the, the core of it is he's, he's, he's back to the future. So he is, he is doing the
11189240	11192680	most leading edge things in the world, but with a really deeply old school approach.
11193560	11198520	And so to find comparisons to Elon, you need to go to like Henry Ford and Thomas Watson and Howard
11198520	11206280	Hughes and Andrew Carnegie, right, Leland Stanford, John T. Rockefeller, right, you need to go to the,
11206280	11210680	what we're called the bourgeois capitalists, like the hardcore business owner operators who
11210680	11217320	basically built, you know, basically built industrialized society, Vanderbilt. And it's
11217320	11227880	a level of hands-on commitment and depth in the business, coupled with an absolute
11227880	11234200	priority towards truth and towards kind of put science and technology
11234840	11239400	down to first principles that is just like absolute, just like unbelievably absolute.
11240120	11243960	He really is ideal that he's only ever talking to engineers. Like he does not tolerate,
11244760	11249640	he has, he has less bullshit talents than anybody I've ever met. He wants ground truth on every
11249640	11254680	single topic. And he runs his businesses directly day to day devoted to getting to ground truth in
11254680	11261320	every single topic. So you think it was a good decision for him to buy Twitter?
11261320	11263640	I have developed a view in life did not second guess Elon Musk.
11264280	11269000	I know this is going to sound crazy and unfounded, but
11269720	11273000	well, I mean, he's got a quite a track record.
11273000	11275960	I mean, look, the car was a crazy, I mean, the car was, I mean, look,
11275960	11279160	he's done a lot of things that seem crazy, starting a new car company in the United
11279160	11283000	States of America. The last time somebody really tried to do that was the 1950s. And it was called
11283000	11286680	Tucker Automotive. And it was such a disaster, they made a movie about what a disaster it was.
11287800	11291720	And then Rockets, like who does that? Like that's, there's obviously no way to start
11291800	11295160	in a rocket company like those days are over. And then to do those at the same time.
11296040	11302200	So after he pulled those two off, like, okay, fine. Like, this is one of my areas of like,
11302200	11305640	whatever opinions I had about that is just like, okay, clearly or not relevant, like this is,
11306280	11307640	at some point, you just like put on the person.
11308200	11313240	And in general, I wish more people would lean on celebrating and supporting versus deriding
11313240	11319320	and destroying. Oh, yeah. I mean, look, he drives resentment, like it's like he is a magnet for
11319320	11325080	resentment. Like his critics are the most miserable, resentful people in the world. Like
11325080	11330360	it's almost a perfect match of like the most idealized, you know, technologists, you know,
11330360	11335240	of the century coupled with like just his critics are just bitter as can be. I mean,
11335240	11342360	it's sort of very darkly comic to watch. Well, he, he fuels the fire of that by being an asshole on
11342360	11348360	Twitter at times. And which is fascinating to watch the drama of human civilization given our cult
11349080	11356680	roots just fully on fire. He's running a cult. You could say that very successfully.
11356680	11361080	So now, now there are cults have gone and we search for meaning. What do you think is the
11361080	11365080	meaning of this whole thing? What's the meaning of life, Mark Andreessen? I don't know the answer
11365080	11371640	to that. I think the meaning of, of the closest I get to it is what I said about satisfaction. So
11371640	11375720	it's basically like, okay, we were given what we have, like we should basically do our best.
11375720	11380040	What's the role of love in that mix? I mean, like, what's the point of life if you're, yeah,
11380040	11384120	without love, like, yeah. So love is a big part of that satisfaction.
11384120	11387640	Yeah, look, like taking care of people is like a wonderful thing. Like, you know,
11387640	11391960	a mentality, you know, there are pathological forms of taking care of people, but there's also a
11391960	11395320	very fundamental, you know, kind of aspect of taking care of people. Like for example, I happen
11395320	11398680	to be somebody who believes that capitalism and taking care of people are actually, they're
11398680	11402680	actually the same thing. Somebody once said capitalism is how you take care of people you
11402680	11408360	don't know, right? Right. And so like, yeah, I think it's like deeply woven into the whole thing.
11409080	11411640	You know, there's a long conversation to be had about that, but yeah.
11412440	11417000	Yeah, creating products that are used by millions of people and bring them joy in smaller big ways.
11417560	11420920	And then capitalism kind of enables that, encourages that.
11421640	11425400	David Friedman says there's only three ways to get somebody to do something for somebody else.
11426120	11427640	Love, money and force.
11432280	11433560	Love and money are better.
11434840	11437560	That's a good ordering. I think we should bet on those.
11437560	11442600	Try love first. If that doesn't work, the money and then force. Well, don't even try that one.
11443400	11447480	Mark, you're an incredible person. I've been a huge fan. I'm glad to finally got a chance to talk.
11447480	11452360	I'm a fan of everything you do, everything you do, including on Twitter. It's a huge honor to
11452360	11455480	meet you to talk with you. Thanks again for doing this. Awesome. Thank you, Alex.
11456520	11460360	Thanks for listening to this conversation with Mark Andreessen. To support this podcast,
11460360	11465320	please check out our sponsors in the description. And now let me leave you with some words from
11465320	11471320	Mark Andreessen himself. The world is a very malleable place. If you know what you want,
11471880	11478120	and you go for it with maximum energy and drive and passion, the world will often reconfigure
11478120	11483960	itself around you much more quickly and easily than you would think. Thank you for listening,
11483960	11494920	and hope to see you next time.
