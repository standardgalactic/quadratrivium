start	end	text
0	2320	The following is a conversation with Jeff Hawkins.
2320	7040	He's the founder of the Redwood Center for Theoretical Neuroscience in 2002 and New
7040	13280	Menta in 2005. In his 2004 book titled On Intelligence and in the research before and
13280	18640	after, he and his team have worked to reverse engineer the New York Cortex and propose artificial
18640	22960	intelligence architectures approaches and ideas that are inspired by the human brain.
23600	28880	These ideas include hierarchical temporal memory, HTM from 2004, and new work,
28880	36080	the 1000s brain theory of intelligence from 2017, 18, and 19. Jeff's ideas have been an inspiration
36080	40400	to many who have looked for progress beyond the current machine learning approaches,
40400	46160	but they have also received criticism for lacking a body of empirical evidence supporting the models.
46160	50720	This is always a challenge when seeking more than small incremental steps forward in AI.
51360	56480	Jeff is a brilliant mind and many of the ideas he has developed and aggregated from neuroscience
56480	61360	are worth understanding and thinking about. There are limits to deep learning as it is
61360	67040	currently defined. Forward progress in AI is shrouded in mystery. My hope is that conversations
67040	73280	like this can help provide an inspiring spark for new ideas. This is the Artificial Intelligence
73280	78560	Podcast. If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter
78560	85040	at Lex Friedman spelled F-R-I-D. And now here's my conversation with Jeff Hawkins.
109520	115360	I also firmly believe that we will not be able to create fully intelligent machines until we
115360	120560	understand how the human brain works. So I don't see those as separate problems. I think there's
120560	124240	limits to what can be done with machine intelligence if you don't understand the principles by which
124240	130000	the brain works. And so I actually believe that studying the brain is actually the fastest way
130000	135520	to get to machine intelligence. And within that, let me ask the impossible question. How do you
135520	138720	not define but at least think about what it means to be intelligent?
139360	144400	So I didn't try to answer that question first. We said let's just talk about how the brain works
144400	149120	and let's figure out how certain parts of the brain, mostly the neocortex, but some other parts too,
149680	154480	the parts of the brain most associated with intelligence, and let's discover the principles
154480	159920	by how they work. Because intelligence isn't just like some mechanism and it's not just some
159920	166880	capabilities. It's like, okay, we don't even know where to begin on this stuff. And so now that we've
166880	171520	made a lot of progress on this, after we've made a lot of progress on how the neocortex works,
171520	176080	and we can talk about that, I now have a very good idea what's going to be required to make
176080	182000	intelligent machines. I can tell you today, some of the things are going to be necessary, I believe,
182000	186400	to create intelligent machines. Well, so we'll get there. We'll get to the neocortex and some of
186400	191360	the theories of how the whole thing works. And you're saying, as we understand more and more
192560	197280	about the neocortex, about our own human mind, we'll be able to start to more specifically
197280	201760	define what it means to be intelligent. It's not useful to really talk about that until...
201760	206880	I don't know if it's not useful. Look, there's a long history of AI, as you know. And there's
206880	214480	been different approaches taken to it. And who knows, maybe they're all useful. So the good
214480	219600	old fashioned AI, the expert systems, the current convolutional neural networks, they all have their
219600	225520	utility. They all have a value in the world. But I would think almost everyone agree that none of
225520	232960	them are really intelligent in a sort of a deep way that humans are. And so it's just the question
232960	237760	is how do you get from where those systems were or are today to where a lot of people think we're
237760	244880	going to go? And there's a big, big gap there, a huge gap. And I think the quickest way of bridging
244880	250320	that gap is to figure out how the brain does that. And then we can sit back and look and say, oh,
250320	255360	what are these principles that the brain works on are necessary and which ones are not? Clearly,
255360	258640	we don't have to build this in... and intelligent machines aren't going to be built out of
260480	265760	organic living cells. But there's a lot of stuff that goes on the brain that's going to be necessary.
265760	271840	So let me ask maybe, before we get into the fun details, let me ask maybe a depressing or
271840	277360	a difficult question. Do you think it's possible that we will never be able to understand how our
277360	284960	brain works? That maybe there's aspects to the human mind, like we ourselves cannot introspectively
284960	289840	get to the core, that there's a wall you eventually hit? Yeah, I don't believe that's the case.
290720	294400	I have never believed that's the case. There's not been a single thing we've ever,
294400	298640	human have ever put their minds to. We've said, oh, we reached the wall. We can't go any further.
298640	303440	People keep saying that. People used to believe that about life, you know, Alain Vaital, right?
303440	306400	There's like, what's the difference between living matter and non-living matter? Something
306400	312720	special you never understand. We no longer think that. So there's no historical evidence that
312720	317440	suggests this is the case. And I just never even consider that's a possibility. I would also say
317520	323760	I would also say today we understand so much about the near cortex. We've made tremendous
323760	330880	progress in the last few years that I no longer think of as an open question. The answers are
330880	336000	very clear to me. The pieces we know we don't know are clear to me, but the framework is all there.
336000	339840	And it's like, oh, okay, we're going to be able to do this. This is not a problem anymore.
339840	343440	It just takes time and effort. But there's no mystery, big mystery anymore.
344000	352240	So then let's get into it for people like myself who are not very well versed in the human brain,
352800	358160	except my own. Can you describe to me at the highest level, what are the different parts
358160	364000	of the human brain and then zooming in on the near cortex, the parts of the near cortex and so on,
364000	369680	a quick overview? Yeah, sure. The human brain, we can divide it roughly into two parts.
370640	376400	There's the old parts, lots of pieces, and then there's the new part. The new part is the near
376400	382080	cortex. It's new because it didn't exist before mammals. The only mammals have a near cortex
382080	387200	and in humans, in primates, it's very large. In the human brain, the near cortex occupies about
387200	393520	70 to 75% of the volume of the brain. It's huge. And the old parts of the brain are,
394720	398640	there's lots of pieces there. There's a spinal cord, and there's the brain stem,
398640	401360	and the cerebellum, and the different parts of the basal ganglion and so on.
401920	406240	In the old parts of the brain, you have the autonomic regulation like breathing and heart rate.
406240	410560	You have basic behaviors. So like walking and running are controlled by the old parts of the
410560	414160	brain. All the emotional centers of the brain are in the old part of the brain. So when you
414160	417200	feel anger or hungry, lust or things like that, those are all in the old parts of the brain.
419040	424480	And we associate with the near cortex all the things we think about as sort of high level
424480	432880	perception and cognitive functions, anything from seeing and hearing and touching things to
432880	438160	language to mathematics and engineering and science and so on. Those are all associated with the
438160	444080	near cortex. And they're certainly correlated. Our abilities in those regards are correlated with
444080	449120	the relative size of our near cortex compared to other mammals. So that's like the rough
449200	455440	division. And you obviously can't understand the near cortex completely isolated, but you
455440	459600	can understand a lot of it with just a few interfaces to the old parts of the brain.
460320	467120	And so it gives you a system to study. The other remarkable thing about the near cortex
467920	474720	compared to the old parts of the brain is the near cortex is extremely uniform. It's not visibly or
474720	481200	anatomically or it's very it's like a I always like to say it's like the size of a dinner napkin
481200	486240	about two and a half millimeters thick. And it looks remarkably the same everywhere. Everywhere
486240	490400	you look in that two and a half millimeters is this detailed architecture. And it looks
490400	494640	remarkably the same everywhere. And that's across species, a mouse versus a cat and a dog and a
494640	498720	human. Where if you look at the old parts of the brain, there's lots of little pieces do specific
498720	503200	things. So it's like the old parts of a brain involved like this is the part that controls
503200	506000	heart rate. And this is the part that controls this and this is this kind of thing. And that's
506000	510960	this kind of thing. And these evolve for eons of a long, long time. And they have those specific
510960	514320	functions. And all of a sudden mammals come along and they got this thing called the near
514320	519280	cortex. And it got large by just replicating the same thing over and over and over again.
519280	527760	This is like, wow, this is incredible. So all the evidence we have. And this is an idea that was
527760	534880	first articulated in a very cogent and beautiful argument by a guy named Vernon Malcastle in 1978,
534880	543760	I think it was, that the neocortex all works on the same principle. So language, hearing,
543760	548320	touch, vision, engineering, all these things are basically underlying or all built in the same
548320	552960	computational substrate. They're really all the same problem. So the low level of the building
552960	557440	blocks all look similar. Yeah. And they're not even that low level. We're not talking about like
557440	561520	neurons. We're talking about this very complex circuit that exists throughout the neocortex is
562160	567200	remarkably similar. It is, it's like, yes, you see variations of it here and there, more of the cell
567200	574080	left and left and so on. But what Malcastle argued was it says, you know, if you take a section on
574080	580160	neocortex, why is one a visual area and one is a auditory area? Or why is, and his answer was
581040	584320	it's because one is connected to eyes and one is connected to ears.
585280	590320	Literally, you mean just it's most closest in terms of number of connections to the sensor?
590320	595200	Literally, if you took the optic nerve and attached it to a different part of the neocortex,
595200	600320	that part would become a visual region. This actually, this experiment was actually done by
600320	606640	McGonkiss Sir in developing, I think it was lemurs, I can't remember what it was, some animal.
606640	610480	And, and there's a lot of evidence to this. You know, if you take a blind person, a person is
610480	616000	born blind at birth. They, they're born with a visual neocortex. It doesn't
616000	621440	may not get any input from the eyes, because of some congenital defect or something. And
622480	629200	that region becomes, does something else. It picks up another task. So, and it's, it's,
629200	633760	so it's this, it's this very complex thing. It's not like, oh, they're all built on neurons. No,
633760	639520	they're all built in this very complex circuit. And, and somehow that circuit underlies everything.
640160	647040	And so this is the, it's called the common cortical algorithm, if you will. Some scientists just find
647040	651440	it hard to believe. And they just say, I can't believe that's true. But the evidence is overwhelming
651440	656320	in this case. And so a large part of what it means to figure out how the brain creates intelligence
656320	662240	and what is intelligence in the brain is to understand what that circuit does. If you can
662240	668000	figure out what that circuit does, as amazing as it is, then you can, then you, then you understand
668000	673760	what all these other cognitive functions are. So if you were to sort of put neural cortex outside
673760	678400	of your book on intelligence, if you look, if you wrote a giant tome, a textbook on the neural
678400	685760	cortex, and you look maybe a couple centuries from now, how much of what we know now would still
685760	690720	be accurate two centuries from now. So how close are we in terms of understanding? So I'm going to,
690720	695840	I have to speak from my own particular experience here. So I run a small research lab here. It's
696560	699680	it's like any other research lab on the sort of the principal investigator, there's actually
699680	704320	two of us and there's a bunch of other people. And this is what we do. We started the neural
704320	710160	cortex, and we published our results and so on. So about three years ago, we had a real
710160	713920	breakthrough in this, in this field. It's a tremendous breakthrough. We started, we've now
713920	720320	published, I think, three papers on it. And so I have, I have a pretty good understanding of all
720320	726800	the pieces and what we're missing. I would say that almost all the empirical data we've collected
726800	730960	about the brain, which is enormous, if you don't know the neuroscience literature, it's just
730960	740960	incredibly big. And it's, for the most part, all correct. It's facts and experimental results and
740960	746720	measurements and all kinds of stuff. But none of that has been really assimilated into a theoretical
746720	753440	framework. It's, it's data without, it's in the, in the language of Thomas Coon, the historian,
753440	758080	it would be a sort of a pre-paradigm science, lots of data, but no way to fit it in together.
758080	760800	I think almost all of that's correct. There's going to be some mistakes in there.
762080	767200	And for the most part, there aren't really good cogent theories about how to put it together.
767200	771120	It's not like we have two or three competing good theories, which ones are right and which ones
771120	774640	are wrong. It's like, yeah, people just like scratching their heads, throwing things, you know,
774640	777520	some people have given up on trying to like figure out what the whole thing does.
777520	784160	In fact, there's very, very few labs that we do that focus really on theory and all this
784160	788880	unassimilated data and trying to explain it. So it's not like we haven't, we've got it wrong.
788880	794160	It's just that we haven't got it at all. So it's really, I would say, pretty early days
795040	800160	in terms of understanding the fundamental theories, forces of the way our mind works.
800160	803520	I don't think so. I would have said that's true five years ago.
805280	810480	So as I said, we had some really big breakthroughs on this recently, and we started publishing papers
810480	816720	on this. So we'll get to that. So I don't think it's, you know, I'm an optimist,
816720	820320	and from where I sit today, most people would disagree with this, but from where I sit today,
820320	827200	from what I know, it's not super early days anymore. The way these things go is it's not a
827200	830720	linear path, right? You don't just start accumulating and get better and better and better.
830720	834320	No, you got all the stuff you've collected. None of it makes sense. All these different
834320	837520	things we just started around. And then you're going to have some breaking points all of a sudden,
837520	843520	oh my God, now we got it right. That's how it goes in science. And I personally feel like we
843520	847440	passed that little thing about a couple of years ago, all that big thing a couple of years ago.
847440	852480	So we can talk about that. Time will tell if I'm right. But I feel very confident about it.
852560	858560	That's the moment to say it on tape like this. At least very optimistic. So let's,
858560	865840	before those few years ago, let's take a step back to HTM, the hierarchical temporal memory theory,
865840	869360	which you first proposed on intelligence and went through a few different generations. Can
869360	874480	you describe what it is, how it evolved through the three generations, since you first put it
874480	882240	on paper? Yeah. So one of the things that neuroscientists just sort of missed for many,
882240	887600	many years, and especially people who are thinking about theory, was the nature of time in the brain.
889040	893520	Brains process information through time. The information coming into the brain is constantly
893520	899280	changing. The patterns from my speech right now, if you're listening to it at normal speed,
900000	904000	would be changing on your ears about every 10 milliseconds or so you'd have it change.
904000	908160	This constant flow, when you look at the world, your eyes are moving constantly,
908160	912480	three to five times a second, and the inputs completely. If I were to touch something like
912480	917600	a coffee cup, as I move my fingers, the inputs change. So this idea that the brain works on
917600	922720	time changing patterns is almost completely or was almost completely missing from a lot of
922720	926480	the basic theories like fears of vision and so on. It's like, oh, no, we're going to put this
926480	930880	image in front of you and flash it and say, what is it? convolutional neural networks work that
930880	936560	way today, right? Classified this picture. But that's not what vision is like. Vision is this
936560	941680	sort of crazy time-based pattern that's going all over the place and so is touch and so is hearing.
941680	946000	So the first part of a hierarchical temporal memory was the temporal part. It's to say,
946800	950320	you won't understand the brain, nor will you understand intelligent machines unless you're
950320	955920	dealing with time-based patterns. The second thing was the memory component of it was, is to say
956640	964160	that we aren't just processing input. We learn a model of the world. And the memory stands for
964160	968080	that model. We have to, the point of the brain, the part of the neocortex, it learns a model of
968080	973760	the world. We have to store things that are experiences in a form that leads to a model of
973760	977280	the world. So we can move around the world. We can pick things up and do things and navigate and
977280	980960	know how it's going on. So that's, that's what the memory referred to. And many people just,
980960	986160	they were thinking about like certain processes without memory at all. They're just like processing
986160	992160	things. And then finally, the hierarchical component was a reflection to that the neocortex,
992160	997520	although it's just a uniform sheet of cells, different parts of it project to other parts,
997520	1003200	which project to other parts. And there is a sort of rough hierarchy in terms of that. So
1003840	1007760	the hierarchical temporal memory is just saying, look, we should be thinking about the brain
1007760	1016160	as time-based, you know, model memory-based and hierarchical processing. And, and that was a
1016160	1021680	placeholder for a bunch of components that we would then plug into that. We still believe all
1021680	1027120	those things I just said, but we now know so much more that I'm stopping to use the word
1027120	1031440	hierarchical temporal memory yet because it's insufficient to capture the stuff we know. So
1031440	1036800	again, it's not incorrect, but it's, I now know more and I would rather describe it more accurately.
1036800	1042880	Yeah. So you're basically, we could think of HTM as emphasizing that there's three aspects
1043520	1047680	of intelligence that are important to think about whatever the, whatever the eventual theory
1047680	1053760	converges to. So in terms of time, how do you think of nature of time across different timescales?
1053760	1059600	So you mentioned things changing, sensory inputs changing every 10, 20 minutes. What about
1059600	1063840	every few minutes, every few months and years? Well, if you think about a neuroscience
1064320	1071440	problem, the brain problem, neurons themselves can stay active for certain parts of time.
1071440	1074160	They can, they're parts of the brain where they stay active for minutes, you know,
1074160	1082320	so you could hold a certain perception or activity for a certain part of time, but not,
1082320	1088960	most of them don't last that long. And so if you think about your thoughts or the activity neurons,
1088960	1092880	if you're going to want to involve something that happened a long time ago, even just this morning,
1092880	1097600	for example, the neurons haven't been active throughout that time. So you have to store that.
1097600	1102320	So if I ask you, what did you have for breakfast today? That is memory. That is,
1102320	1105840	you've built that into your model of the world now, you remember that. And that memory is in the
1107120	1114640	synapses, it's basically in the formation of synapses. And so it's, you're sliding into what,
1114640	1118560	you know, used to different timescales. There's timescales of which we are like
1118560	1122080	understanding my language and moving about and seeing things rapidly and over time. That's the
1122080	1126640	timescales of activities of neurons. But if you want to get in longer timescales, then it's more
1126640	1130960	memory. And we have to invoke those memories to say, Oh, yes, well, now I can remember what
1130960	1137440	I had for breakfast because I stored that someplace. I may forget it tomorrow, but I'd store it for
1137440	1147040	now. So this memory also need to have the hierarchical aspect of reality is not just about
1147040	1149600	concepts, it's also about time. Do you think of it that way?
1149600	1154560	Uh, yeah, time is infused in everything. It's like, you really can't separate it out.
1155440	1160800	If I ask you, what is the, what is your, you know, how's the brain learn a model of this coffee cup
1160800	1165440	here? I have a coffee cup and then I met the coffee cup. I said, well, time is not an inherent
1165440	1169520	property of this, of this, of the model I have with this cup, whether it's a visual model or
1169520	1174800	tactile model. I can sense it through time, but the model itself doesn't really have much time.
1174800	1178000	If I asked you, if I said, well, what is the model of my cell phone?
1178880	1183360	My brain has learned a model of the cell phones. If you have a smartphone like this.
1183360	1187920	And I said, well, this has time aspects to it. I have expectations when I turn it on,
1187920	1191200	what's going to happen, what water, how long it's going to take to do certain things.
1191760	1196400	If I bring up an app, what sequences. And so I have instant, it's like melodies in the world,
1196400	1201440	you know, melody has a sense of time. So many things in the world move and act, and there's
1201440	1210160	a sense of time related to them. Some don't, but most things do actually. So it's sort of infused
1210160	1214560	throughout the models of the world. You build a model of the world, you're learning the structure
1214560	1219440	of the objects in the world, and you're also learning how those things change through time.
1220640	1226080	Okay, so it's, it really is just a fourth dimension that's infused deeply. And you have
1226080	1231120	to make sure that your models of an intelligence incorporated. So
1233040	1236960	like you mentioned, the state of neuroscience is deeply empirical, a lot of data collection.
1237680	1242960	It's, you know, that's, that's where it is. You mentioned Thomas Kuhn, right?
1242960	1250400	Yeah. And then you're proposing a theory of intelligence, and which is really the next step,
1250400	1259520	the really important step to take. But why, why is HTM or what we'll talk about soon,
1260880	1268480	the right theory? So is it more in this? Is it backed by intuition? Is it backed by
1269360	1274960	evidence? Is it backed by a mixture of both? Is it kind of closer to where string theory is in physics,
1275520	1282240	where there's mathematical components which show that, you know what, it seems that this,
1282880	1288480	it fits together too well for it not to be true, which is what where string theory is. Is that
1288480	1293040	where it's a mix of all those things, although definitely where we are right now, it's definitely
1293040	1298320	much more on the empirical side than let's say string theory. The way this goes about, we're
1298320	1302320	theorists, right? So we look at all this data and we're trying to come up with some sort of model
1302400	1308480	that explains it, basically. And there's a, unlike string theory, there's this vast more
1308480	1316560	amounts of empirical data here that I think that most physicists deal with. And so our challenge
1316560	1323360	is to sort through that and figure out what kind of constructs would explain this. And when we have
1323360	1329200	an idea, you come up with a theory of some sort, you have lots of ways of testing it. First of all,
1329920	1336560	I am, you know, there are 100 years of assimilated, undesimulated empirical data from neuroscience.
1336560	1341840	So we go back and read papers and we say, oh, did someone find this already? We can predict X,
1341840	1347680	Y, and Z. And maybe no one's even talked about it since 1972 or something, but we go back and
1347680	1353440	find that. And we say, oh, either it can support the theory or it can invalidate the theory. And
1353440	1356880	we say, okay, we have to start over again. Oh, no, it's support. Let's keep going with that one.
1357840	1365280	So the way I kind of view it, when we do our work, we come up, we look at all this empirical data,
1365280	1368640	and it's what I call it as a set of constraints. We're not interested in something that's
1368640	1372720	biologically inspired. We're trying to figure out how the actual brain works. So every piece
1372720	1376880	of empirical data is a constraint on a theory. In theory, if you have the correct theory,
1376880	1382960	it needs to explain every pin, right? So we have this huge number of constraints on the problem,
1383040	1387120	which initially makes it very, very difficult. If you don't have many constraints,
1387120	1390080	you can make up stuff all the day. You can say, oh, here's an answer. How you can do this,
1390080	1393680	you can do that, you can do this. But if you consider all biology as a set of constraints,
1393680	1397200	all neuroscience, a set of constraints, and even if you're working in one little part of
1397200	1401280	the Neocortex, for example, there are hundreds and hundreds of constraints, these are empirical
1401280	1406880	constraints, that it's very, very difficult initially to come up with a theoretical framework
1406880	1412880	for that. But when you do, and it solves all those constraints at once, you have a high confidence
1412880	1418560	that you got something close to correct. It's just mathematically almost impossible not to be.
1419120	1426800	So that's the curse and the advantage of what we have. The curse is we have to meet all these
1426800	1433120	constraints, which is really hard. But when you do meet them, then you have a great confidence
1433120	1438640	that you've discovered something. In addition, then we work with scientific labs. So we'll say,
1438640	1443200	oh, there's something we can't find, we can predict something, but we can't find it anywhere in the
1443200	1448640	literature. So we will then, we have people we collaborated with, we'll say, sometimes they'll
1448640	1452480	say, you know what, I have some collected data, which I didn't publish. But we can go back and
1452480	1456880	look at it and see if we can find that, which is much easier than designing a new experiment,
1456880	1461920	you know, new neuroscience experiments take a long time, years. So although some people
1461920	1468720	are doing that now too. So, but between all of these things, I think it's a reasonable,
1470080	1474400	actually a very, very good approach. We are blessed with the fact that we can test our theories
1474960	1479280	out to Yang Yang here, because there's so much on a similar data. And we can also falsify our
1479280	1484080	theories very easily, which we do often. So it's kind of reminiscent to whenever, whenever that
1484080	1490720	was with Copernicus, you know, when you figure out that the sun's at the center of the solar
1490720	1496240	system as opposed to Earth, the pieces just fall into place. Yeah, I think that's the general
1497200	1503840	nature of the Ha moments is in Copernicus, it could be, you could say the same thing about Darwin.
1505120	1511360	You could say the same thing about, you know, about the double helix, that people have been
1511360	1514480	working on a problem for so long and have all this data and they can't make sense of it,
1514480	1519280	they can't make sense of it. But when the answer comes to you and everything falls into place,
1519280	1526400	it's like, oh, my gosh, that's it. That's got to be right. I asked both Jim Watson and Francis
1526400	1533440	Crick about this. I asked them, you know, when you were working on trying to discover the structure
1533440	1540720	of the double helix, and when you came up with the sort of the structure that ended up being correct,
1542400	1546320	but it was sort of a guess, you know, it wasn't really verified yet. I said,
1546320	1551360	did you know that it was right? And they both said, absolutely. So we absolutely knew it was
1551360	1555520	right. And it doesn't matter if other people didn't believe it or not, we knew it was right,
1555520	1560000	they'd get around to thinking it and agree with it eventually anyway. And that's the kind of thing
1560000	1565760	you hear a lot with scientists who really are studying a difficult problem. And I feel that way
1565760	1571520	too about our work. Have you talked to Crick or Watson about the problem you're trying to solve,
1572000	1579840	the, of finding the DNA of the brain? Yeah. In fact, Francis Crick was very interested in this,
1579840	1585280	in the latter part of his life. And in fact, I got interested in brains by reading an essay he wrote
1585280	1592000	in 1979 called Thinking About the Brain. And that was when I decided I'm going to leave my
1592000	1596640	profession of computers and engineering and become a neuroscientist, just reading that one essay from
1596640	1603600	Francis Crick. I got to meet him later in life. I got to, I spoke at the Salk Institute and he
1603600	1609600	was in the audience and then I had a tea with him afterwards. You know, he was interested in a
1609600	1617520	different problem. He was focused on consciousness. Oh, the easy problem, right? Well, I think it's
1617520	1624240	the red herring. And so we weren't really overlapping a lot there. Jim Watson, who's still alive,
1625200	1630080	is also interested in this problem. And he was, when he was director of the Coltsman Harbor
1630080	1635840	laboratories, he was really sort of behind moving in the direction of neuroscience there.
1636480	1641920	And so he had a personal interest in this field. And I have met with him numerous times.
1643520	1649200	And in fact, the last time was about a little bit over a year ago, I gave a talk at Coltsman
1649280	1657920	Harbor Labs about the progress we were making in our work. And it was a lot of fun because
1659360	1662240	he said, well, you wouldn't be coming here unless you had something important to say,
1662240	1668960	so I'm going to go attend your talk. So he sat in the very front row. Next to him was the director
1668960	1672480	of the lab, Bruce Stillman. So these guys are in the front row of this auditorium, right? So
1672480	1675600	nobody else in the auditorium wants to sit in the front row because there's Jim Watson and there's
1675600	1685040	the director. And I gave a talk and then I had dinner with Jim afterwards. But there's a great
1685040	1689840	picture of my colleague, Subitai Amantik, where I'm up there sort of explaining the basics of
1689840	1694560	this new framework we have. And Jim Watson is on the edge of his chair. He's literally on the edge
1694560	1700480	of his chair, like intently staring up at the screen. And when he discovered the structure of
1700480	1706560	DNA, the first public talk he gave was at Coltsman Harbor Labs. And there's a picture,
1706560	1710720	there's a famous picture of Jim Watson standing at the whiteboard with a overrated thing pointing
1710720	1714400	at something, holding at the double helix with his pointer. And it actually looks a lot like the
1714400	1717760	picture of me. So there was a sort of funny, there's an area talking about the brain and there's Jim
1717760	1721520	Watson staring up intently at it. And of course, there was, you know, whatever, 60 years earlier,
1721520	1726000	he was standing, you know, pointing at the double helix. And it's one of the great discoveries in
1726000	1732320	all of, you know, whatever, by all the science, all science DNA. So it's just the funny that
1732320	1737280	there's echoes of that in your presentation. Do you think in terms of evolutionary timeline and
1737280	1747360	history, the development of the neocortex was a big leap? Or is it just a small step? So like,
1747360	1752720	if we ran the whole thing over again, from the from the birth of human of life on earth, how
1752720	1756320	likely would we develop the mechanism of the neocortex? Okay, well, those are two separate
1756320	1762240	questions. One is, was it a big leap? And one was how likely it is. Okay, they're not necessarily
1762240	1767200	related. Maybe correlated. And we don't really have enough data to make a judgment about that.
1768000	1772560	I would say definitely was a big leap. And I can tell you why I think I don't think it was just
1772560	1778320	another incremental step. I'll get that moment. I don't really have any idea how likely it is.
1778320	1784080	If we look at evolution, we have one data point, which is earth, right? Life formed on earth billions
1784080	1788880	of years ago, whether it was introduced here, or it created here, or someone introduced it,
1788880	1794160	we don't really know, but it was here early. It took a long, long time to get to multicellular life.
1795040	1802640	And then from multicellular life, it took a long, long time to get the neocortex. And we've only
1802640	1809040	had the neocortex for a few hundred thousand years. So that's like nothing. Okay, so is it
1809040	1813760	likely? Well, certainly isn't something that happened right away on earth. And there were
1813760	1817520	multiple steps to get there. So I would say it's probably not going to something that would happen
1817520	1822400	instantaneously on other planets that might have life. It might take several billion years on average.
1823040	1826880	Is it likely? I don't know, but you'd have to survive for several billion years to find out.
1827840	1836560	Probably. Is it a big leap? Yeah, I think it is a qualitative difference in all other evolutionary
1836560	1842800	steps. I can try to describe that if you'd like. Sure. In which way? Yeah, I can tell you how.
1843760	1849600	Pretty much, let's start with a little preface. Many of the things that humans are able to do
1850320	1860400	do not have obvious survival advantages precedent. We could create music. Is there
1860400	1865280	a really survival advantage to that? Maybe, maybe not. What about mathematics? Is there a real
1865280	1870480	survival advantage to mathematics? You can stretch it. You can try to figure these things out.
1872080	1877920	But mostly evolutionary history, everything had immediate survival advantages to write.
1878640	1886240	So I'll tell you a story, which I like. It may not be true. But the story goes as follows.
1889040	1892320	Organisms have been evolving since the beginning of life here on Earth,
1893680	1897680	adding this sort of complexity onto that and this sort of complexity onto that. And the brain itself
1898240	1903360	is evolved this way. In fact, there's an old part, an older part, an older, older part to the
1903360	1907360	brain that kind of just keeps calming on new things and we keep adding capabilities. And we
1907360	1913840	got to the neocortex. Initially, it had a very clear survival advantage in that it produced better
1913840	1919200	vision and better hearing and better touch and maybe, you know, say, so on. But what I think
1919200	1925440	happens is that evolution took a mechanism, and this is in our recent theory, but it took a
1925440	1930320	mechanism that evolved a long time ago for navigating in the world, for knowing where you are.
1930320	1934320	These are the so-called grid cells and place cells of an old part of the brain.
1935120	1942160	And it took that mechanism for building maps of the world and knowing where you are on those
1942160	1947920	maps and how to navigate those maps and turns it into a sort of a slimmed down, idealized version
1947920	1953120	of it. And that idealized version could now apply to building maps of other things, maps of
1953680	1959200	coffee cups and maps of phones, maps of, you know, concepts, concepts, yes, and not just almost,
1959200	1964400	exactly. And so you, and it just started replicating this stuff, right? You just think
1964400	1970480	more and more and more. So we went from being sort of dedicated purpose neural hardware to solve
1970480	1976000	certain problems that are important to survival to a general purpose neural hardware that could
1976000	1983360	be applied to all problems. And now it's escaped the orbit of survival. It's, we are now able to
1983360	1992240	apply it to things which we find enjoyment, you know, but aren't really clearly survival
1992240	1998000	characteristics. And that it seems to only have happened in humans to the large extent.
1999120	2004960	And so that's what's going on where we sort of have, we've sort of escaped the gravity of
2004960	2010960	evolutionary pressure in some sense in the New York cortex. And it now does things which
2011520	2016000	that are really interesting, discovering models of the universe, which may not really help us,
2016000	2020800	doesn't matter. How does it help us surviving knowing that there might be multiverses or
2020800	2025440	that there might be, you know, the age of the universe or how do, you know, various stellar
2025440	2030320	things occur? It doesn't really help us survive at all. But we enjoy it. And that's what happened.
2030320	2037200	Or at least not in the obvious way, perhaps it is required. If you look at the entire
2037200	2041280	universe in an evolutionary way, it's required for us to do interplanetary travel and therefore
2041280	2044320	survive past our own fun. But you know, let's not get too quick.
2044320	2050000	Yeah, but you know, evolution works at one time frame and survival, if you think of survival
2050000	2055600	of the phenotype, survival of the individual, what you're talking about there is spans well
2055600	2062400	beyond that. So there's no genetic, I'm not transferring any genetic traits to my children
2063280	2065760	that are going to help them survive better on Mars.
2065760	2071200	Right. Totally different mechanism. So let's get into the new, as you've mentioned,
2071200	2075280	this idea, I don't know if you have a nice name, 1000.
2075280	2077200	I would call it the thousand brain theory of intelligence.
2077200	2084000	I like it. So can you talk about the this idea of spatial view of concepts and so on?
2084000	2088480	Yeah. So can I just describe sort of the there's an underlying core discovery,
2089200	2092320	which then everything comes from that that's a very simple.
2093120	2099520	This is really what happened. We were deep into problems about understanding how we build models
2099520	2105280	of stuff in the world and how we make predictions about things. And I was holding a coffee cup just
2105280	2110960	like this in my hand. And I had my finger was touching the side, my index finger. And then I
2110960	2116560	moved it to the top. And I was going to feel the rim at the top of the cup. And I asked myself
2116560	2121280	a very simple question. I said, well, first of all, let's say I know that my brain predicts
2121280	2125040	what it's going to feel before it touches it. You can just think about it and imagine it.
2125920	2129200	And so we know that the brain's making predictions all the time. So the question is,
2129200	2132880	what does it take to predict that? Right. And there's a very interesting answer.
2133440	2136720	First of all, it says the brain has to know it's touching a coffee cup. It has to have
2136720	2142320	a model of a coffee cup and needs to know where the finger currently is on the cup relative to
2142320	2146320	the cup. Because when I make a movement, it needs to know where it's going to be on the cup
2146320	2152000	after the movement is completed relative to the cup. And then it can make a prediction about
2152000	2156320	what it's going to sense. So this told me that the neocortex, which is making this prediction,
2156320	2160960	needs to know that it's sensing it's touching a cup. And it needs to know the location of
2160960	2165040	my finger relative to that cup in a reference frame of the cup. It doesn't matter where the
2165040	2169200	cup is relative to my body. It doesn't matter its orientation. None of that matters. It's
2169200	2173040	where my finger is relative to the cup, which tells me then that the neocortex
2173600	2178400	has a reference frame that's anchored to the cup. Because otherwise, I wouldn't be able to
2178400	2182480	say the location and I wouldn't be able to predict my new location. And then we quickly,
2182480	2186560	very instantly, you can say, well, every part of my skin could touch this cup. And therefore,
2186560	2189760	every part of my skin is making predictions and every part of my skin must have a reference frame
2190800	2198240	that it's using to make predictions. So the big idea is that throughout the neocortex,
2198320	2207520	everything is being stored and referenced in reference frames. You can think of them like
2207520	2211760	XYZ reference frames, but they're not like that. We know a lot about the neural mechanisms for
2211760	2216640	this. But the brain thinks in reference frames. And as an engineer, if you're an engineer,
2216640	2221200	this is not surprising. You'd say, if I were to build a CAD model of the coffee cup, well,
2221200	2224800	I would bring it up in some CAD software and I would assign some reference frame and say,
2224880	2229920	this features at this location and so on. But the idea that this is occurring throughout
2229920	2238640	the neocortex everywhere, it was a novel idea. And then a zillion things fell into place after
2238640	2242960	that. A zillion. So now we think about the neocortex as processing information quite
2242960	2246480	differently than we used to do it. We used to think about the neocortex as processing
2246480	2250880	sensory data and extracting features from that sensory data and then extracting features from
2250880	2255680	the features, very much like a deep learning network does today. But that's not how the brain
2255680	2261360	works at all. The brain works by assigning everything, every input, everything to reference
2261360	2266160	frames. And there are thousands, hundreds of thousands of them active at once in your neocortex.
2267520	2271600	It's a surprising thing to think about. But once you sort of internalize this, you understand
2271600	2279680	that it explains almost all the mysteries we've had about the structure. So one of the consequences
2280240	2285600	of that is that every small part of the neocortex, say a millimeter square and there's 150,000 of
2285600	2290240	those. So it's about 150,000 square millimeters. If you take every little square millimeter of the
2290240	2295120	cortex, it's got some input coming into it and it's going to have reference frames where it's
2295120	2301440	assigning that input to and each square millimeter can learn complete models of objects. So what do
2301440	2305840	I mean by that? If I'm touching the coffee cup, well, if I just touch it in one place, I can't
2305840	2310320	learn what this coffee cup is because I'm just feeling one part. But if I move it around the cup
2310880	2314800	and touch it in different areas, I can build up a complete model of the cup because I'm now
2314800	2318560	filling in that three dimensional map, which is the coffee cup, I can say, oh, what am I feeling
2318560	2321760	at all these different locations? That's the basic idea. It's more complicated than that.
2322880	2328000	But so through time, and we talked about time earlier, through time, even a single column,
2328000	2331520	which is only looking at or a single part of the cortex, it's only looking at a small part of the
2331520	2336960	world can build up a complete model of an object. And so if you think about the part of the brain,
2336960	2341440	which is getting input from all my fingers, so there's spread across the top of your head here,
2341440	2346160	this is the somatosensory cortex, there's columns associated with all the different areas of my
2346160	2352720	skin. And what we believe is happening is that all of them are building models of this cup,
2352720	2357680	every one of them, or things, not all building all, not every column or every part of the
2357680	2363760	cortex builds models of everything. But they're all building models of something. And so you have,
2363760	2369040	so when I touch this cup with my hand, there are multiple models of the cup being invoked.
2369040	2373280	If I look at it with my eyes, there are again many models of the cup being invoked because each part
2373280	2379360	of the visual system, the brain doesn't process an image, that's a misleading idea. It's just like
2379360	2382320	your fingers touching the cup, so different parts of my retina are looking at different parts of the
2382320	2388160	cup. And thousands and thousands of models of the cup are being invoked at once. And they're all
2388160	2390960	voting with each other trying to figure out what's going on. So that's why we call it the
2390960	2395520	Thousand Brains Theory of Intelligence because there isn't one model of a cup. There are thousands
2395520	2399200	of models of this cup. There are thousands of models of your cell phone and about cameras and
2399200	2403680	microphones and so on. It's a distributed modeling system, which is very different than
2403680	2408160	what people have thought about it. So that's a really compelling and interesting idea. I have two
2408160	2412560	first questions. So one, on the ensemble part of everything coming together, you have these
2412560	2418800	Thousand Brains, how do you know which one has done the best job of forming the cup?
2418800	2423840	Great question. Let me try to explain. There's a problem that's known in neuroscience called the
2423840	2429280	sensor fusion problem. And so the idea is something like, oh, the image comes from the eye. There's
2429280	2434480	a picture on the retina and it gets projected to the neocortex. Oh, by now it's all sped out all
2434480	2438960	over the place and it's kind of squirrely and distorted and pieces are all over the, you know,
2438960	2443600	it doesn't look like a picture anymore. When does it all come back together again? Right?
2443600	2448560	Or you might say, well, yes, but I also, I also have sounds or touches associated with the cup.
2448560	2452560	So I'm seeing the cup and touching the cup. How do they get combined together again?
2452560	2456480	So this is called the sensor fusion problem as if all these disparate parts have to be brought
2456480	2462400	together into one model someplace. That's the wrong idea. The right idea is that you get all
2462400	2466560	these guys voting. There's auditory models of the cup, there's visual models of the cup,
2466560	2471840	there's tactile models of the cup. In the vision system, there might be ones that are more focused
2471840	2475360	on black and white, one's version on color. It doesn't really matter. There's just thousands and
2475360	2480560	thousands of models of this cup and they vote. They don't actually come together in one spot.
2480560	2485360	Just literally think of it this way. Imagine you have each column about the size of a little
2485360	2489840	piece of spaghetti, okay? Like a two and a half millimeters tall and about a millimeter in white.
2489840	2494480	They're not physical like, but you can think of them that way. And each one's trying to guess
2494480	2498400	what this thing is we're touching. Now they can, they can do a pretty good job if they're allowed
2498400	2503360	to move over time. So I can reach my hand into a black box and move my finger around an object
2503360	2508160	and if I touch enough space, it's like, okay, I know what it is. But often we don't do that.
2508160	2511520	Often I can just reach and grab something with my hand all at once and I get it. Or
2511520	2515760	if I had to look through the world through a straw, so I'm only invoking one little column,
2515760	2518880	I can only see part of something because I have to move the straw around. But if I open my eyes
2518880	2523120	to see the whole thing at once. So what we think is going on is all these little pieces of spaghetti
2523120	2527200	if you have all these little columns in the cortex or all trying to guess what it is that they're
2527200	2532320	sensing. They'll do a better guess if they have time and can move over time. So if I move my eyes
2532320	2537760	or move my fingers, but if they don't, they have a, they have a poor guess. It's a, it's a probabilistic
2537760	2542000	guess of what they might be touching. Now imagine they can post their probability
2542880	2546400	at the top of little piece of spaghetti, each one of them says, I think, and it's not really a
2546400	2550800	probability distribution. It's more like a set of possibilities in the brain. It doesn't work
2550800	2555360	as a probability distribution. It works as more like what we call a union. You could say, and one
2555360	2560800	column says, I think it could be a coffee cup soda can or a water bottle. And another column says,
2560800	2567840	I think it could be a coffee cup or, you know, telephone or camera or whatever. Right. And all
2567840	2571680	these guys are saying what they think it might be. And there's these long range connections in
2571760	2577600	certain layers in the cortex. So there's some layers in the, some cells types in each column
2577600	2582720	send the projections across the brain. And that's the voting occurs. And so there's a simple
2582720	2587040	associative memory mechanism. We've, we've described this in a recent paper and we've modeled this
2588560	2594160	that says they can all quickly settle on the only or the one best answer for all of them.
2594800	2599040	If there is a single best answer, they all vote and say, yep, it's got to be the coffee cup.
2599040	2602800	And at that point, they all know it's a coffee cup. And at that point, everyone acts as if it's
2602800	2605920	the coffee cup. They know it's a coffee, even though I've only seen one little piece of this
2605920	2610080	world. I know it's a coffee cup I'm touching or I'm seeing or whatever. And so you can think of
2610080	2613920	all these columns are looking at different parts and different places, different sensory input,
2613920	2617840	different locations. They're all different. But this layer that's doing the voting,
2619040	2623280	that's, it solidifies. It's just like it crystallizes and says, oh, we all know what we're
2623280	2627520	doing. And so you don't bring these models together in one model, you just vote and there's a
2627520	2632480	crystallization of the vote. Great. That's at least a compelling way to think about
2634240	2642000	about the way you form a model of the world. Now, you talk about a coffee cup. Do you see this
2642000	2646160	as far as I understand that you were proposing this as well, that this extends to much more than
2646160	2654080	coffee cups? Yeah, it does. Or at least the physical world that expands to the world of concepts.
2654080	2659040	Yeah, it does. And well, the first, the primary phase of evidence for that is that
2659040	2663760	the regions of the neocortex that are associated with language or high-level thought or mathematics
2663760	2667520	or things like that, they look like the regions of the neocortex that process vision and hearing
2667520	2674080	and touch. They don't look any different or they look only marginally different. And so one would
2674080	2679280	say, well, if Vernon Mountcastle, who proposed that all the parts of the neocortex do the same
2679280	2683840	thing, if he's right, then the parts that are doing language or mathematics or physics
2684480	2687600	are working on the same principle. They must be working on the principle of reference frames.
2688480	2695040	So that's a little odd thought. But of course, we had no prior idea how these things happen,
2695040	2701840	so let's go with that. And in our recent paper, we talked a little bit about that. I've been
2701840	2706960	working on it more since. I have better ideas about it now. I'm sitting here very confident
2707040	2710240	that that's what's happening. And I can give you some examples to help you think about that.
2711120	2714240	It's not that we understand it completely, but I understand it better than I've described it
2714240	2722800	in any paper so far. But we did put that idea out there. It's a good place to start. And the
2722800	2726720	evidence would suggest it's how it's happening. And then we can start tackling that problem one
2726720	2730080	piece at a time. What does it mean to do high-level thought? What does it mean to do language? How
2730080	2737040	would that fit into a reference framework? I don't know if you could tell me if there's a
2737040	2742320	connection, but there's an app called Anki that helps you remember different concepts.
2742320	2747920	And they talk about a memory palace that helps you remember completely random concepts by
2748880	2752320	trying to put them in a physical space in your mind and putting them next to each other.
2752320	2756560	It's called the method of loci. For some reason, that seems to work really well.
2757520	2760480	Now that's a very narrow kind of application of just remembering some facts.
2760480	2763040	But that's a very, very telling one.
2763040	2769040	Yes, exactly. So this seems like you're describing a mechanism why this seems to work.
2769040	2773680	Yeah. So basically the way what we think is going on is all things you know,
2773680	2779600	all concepts, all ideas, words, everything, you know, are stored in reference frames.
2780400	2786480	And so if you want to remember something, you have to basically navigate through a reference
2786480	2790320	frame the same way a rat navigates to a maven, the same way my finger rat navigates to this
2790320	2796000	coffee cup. You are moving through some space. And so if you have a random list of things you
2796000	2800480	would ask to remember, by assigning them to a reference frame, you've already know very well
2800480	2805040	to see your house, right? And the idea of the method of loci is you can say, okay, in my lobby,
2805040	2808160	I'm going to put this thing. And then the bedroom, I put this one. I go down the hall,
2808160	2812240	I put this thing. And then you want to recall those facts or recall those things. You just walk
2812240	2816560	mentally, you walk through your house. You're mentally moving through a reference frame
2816560	2820640	that you already had. And that tells you there's two things that are really important about that.
2820640	2825520	It tells us the brain prefers to store things in reference frames. And that the method of
2825520	2831520	recalling things or thinking, if you will, is to move mentally through those reference frames.
2831520	2834720	You could move physically through some reference frames, like I could physically move through
2834720	2837840	the reference frame of this coffee cup. I can also mentally move through the reference frame
2837840	2843920	of the coffee cup, imagining me touching it. But I can also mentally move my house. And so now we
2843920	2850000	can ask yourself, are all concepts stored this way? There was some recent research using human
2850000	2854960	subjects in fMRI. And I'm going to apologize for not knowing the name of the scientists who did
2854960	2861840	this. But what they did is they put humans in this fMRI machine, which is one of these imaging
2861840	2867680	machines. And they gave the humans tasks to think about birds. So they had different types of birds
2867680	2872560	and birds that looked big and small and long necks and long legs, things like that. And what they
2872560	2879040	could tell from the fMRI was a very clever experiment. Get to tell when humans were thinking
2879040	2885520	about the birds, that the birds, the knowledge of birds was arranged in a reference frame,
2885520	2890240	similar to the ones that are used when you navigate in a room. These are called grid cells.
2890240	2894320	And there are grid cell-like patterns of activity in the neocortex when they do this.
2895280	2900880	So it's a very clever experiment. And what it basically says is that even when you're thinking
2900880	2904560	about something abstract, and you're not really thinking about it as a reference frame,
2904560	2908160	it tells us the brain is actually using a reference frame. And it's using the same neural
2908160	2912800	mechanisms. These grid cells are the basic same neural mechanisms that we propose that grid cells,
2912800	2917280	which exist in the old part of the brain, the entirionic cortex, that that mechanism
2917280	2922560	is now similar mechanism is used throughout the neocortex. It's the same nature of preserve this
2922560	2927680	interesting way of creating reference frames. And so now they have empirical evidence that
2927680	2932160	when you think about concepts like birds, that you're using reference frames that are built on
2932160	2936880	grid cells. So that's similar to the method of loci, but in this case, the birds are related so
2936880	2940320	that makes they create their own reference frame, which is consistent with bird space.
2941040	2944720	And when you think about something, you go through that, you can make the same example.
2944720	2949920	Let's take a math mathematics. Let's say you want to prove a conjecture. What is a conjecture?
2949920	2956080	Conjecture is a statement you believe to be true, but you haven't proven it. And so it might be an
2956080	2961120	equation. I want to show that this is equal to that. And you have some places you start with,
2961120	2965040	you say, well, I know this is true, and I know this is true. And I think that maybe to get to
2965040	2969840	the final proof, I need to go through some intermediate results. But I believe it's happening
2970640	2976960	is literally these equations or these points are assigned to a reference frame, a mathematical
2976960	2981200	reference frame. And when you do mathematical operations, a simple one might be multiply or
2981200	2985280	divide, but you might be able to transform or something else, that is like a movement in the
2985280	2991680	reference frame of the math. And so you're literally trying to discover a path from one location to
2991680	2998160	another location in a space of mathematics. And if you can get to these intermediate results,
2998160	3002000	then you know your map is pretty good. And you know you're using the right operations.
3002960	3008000	Much of what we think about is solving hard problems, is designing the correct reference
3008000	3012400	frame for that problem, figuring out how to organize the information, and what behaviors
3012400	3018880	I want to use in that space to get me there. Yeah, so if you dig in an idea of this reference
3018880	3023920	frame, whether it's the math, you start a set of axioms to try to get to proving the conjecture.
3024640	3029840	Can you try to describe, maybe take a step back, how you think of the reference frame in that
3029840	3037120	context? Is it the reference frame that the axioms are happy in? Is it the reference frame
3037120	3043040	that might contain everything? Is it a changing thing? So you have many, many reference frames.
3043040	3046480	I mean, in fact, the way the theory, the 1000 brain theory of intelligence says that every
3046480	3050480	single thing in the world has its own reference frame. So every word has its own reference
3050480	3055680	frames. And we can talk about this, the mathematics work out, this is no problem for neurons to do
3055680	3059920	this. But how many reference frames does the coffee cup have? Well, it's on a table. Remember,
3060720	3066880	let's say you ask how many reference frames could the column in my finger that's touching the coffee
3066880	3070800	cup have? Because there are many, many copy, there are many, many models of coffee cups. So
3070800	3074320	the coffee, there is no one model of coffee cup, there are many models of coffee cup. And you
3074320	3078560	could say, well, how many different things can my finger learn? Is this is the question you want
3078560	3083520	to ask? Imagine, I say every concept, every idea, everything you've ever know about that you can
3083520	3089200	say, I know that thing, it has a reference frame associated with it. And what we do when we build
3089200	3094320	composite objects, we can we assign reference frames to points, another reference frame. So
3094320	3099440	my coffee cup has multiple components to it. It's got a limb, it's got a cylinder, it's got a handle.
3100640	3104720	And those things that have their own reference frames, and they're assigned to a master reference
3104720	3108640	frame, which is called this cup. And now I have this mental logo on it. Well, that's something
3108640	3112400	that exists elsewhere in the world. It's its own thing. So it has its own reference frame. So we
3112400	3118640	now have to say, how can I assign the mental logo reference frame onto the cylinder or onto the coffee
3118640	3125520	cup? So it's all, we talked about this in the paper that came out in December of this last year.
3126800	3130480	The idea of how you can assign reference frames to reference frames, how neurons could do this.
3130480	3135600	So my question is, even though you mentioned reference frames a lot, I almost feel it's really
3135600	3141440	useful to dig into how you think of what a reference frame is. I mean, it was already helpful for me
3141440	3147760	to understand that you think of reference frames as something there is a lot of. Okay, so let's just
3147760	3152320	say that we're going to have some neurons in the brain, not many actually, 10,000, 20,000 are going
3152320	3156160	to create a whole bunch of reference frames. What does it mean? What is a reference frame?
3157120	3161120	First of all, these reference frames are different than the ones you might have
3161120	3165520	be used to. We know lots of reference things. For example, we know the Cartesian coordinates,
3165520	3170800	x, y, z, that's a type of reference frame. We know longitude and latitude, that's a different type
3170800	3179040	of reference frame. If I look at a printed map, you might have columns a through m and rows,
3179040	3182640	you know, one through 20, that's a different type of reference frame. It's kind of a Cartesian
3182720	3187840	reference frame. The interesting thing about the reference frames in the brain, we know this
3187840	3192160	because these have been established through neuroscience, studying the entorhonic cortex.
3192160	3196800	So I'm not speculating here. Okay, this is known neuroscience in an old part of the brain. The
3196800	3203760	way these cells create reference frames, they have no origin. So what is more like you have a point,
3204320	3210720	a point in some space, and you given a particular movement, you can then tell what the next point
3210720	3216800	should be. And you can then tell what the next point would be and so on. You can use this to
3218160	3223360	calculate how to get from one point to another. So how do I get from my house to my home, or how
3223360	3228400	do I get my finger from the side of my cup to the top of the cup? How do I get from the axioms to
3229600	3236080	the conjecture? So it's a different type of reference frame. And if you want, I can describe
3236080	3239600	in more detail, I can paint a picture how you might want to think about that.
3239600	3247040	It's really helpful to think it's something you can move through. Yeah. But is it helpful to think
3247040	3252160	of it as spatial in some sense, or is there something? No, it's definitely spatial. It's spatial
3252160	3256400	in a mathematical sense. How many dimensions? Can it be a crazy number of dimensions? Well,
3256400	3259520	that's an interesting question. In the old part of the brain, the entorhonic cortex,
3260160	3264640	they studied rats. And initially, it looks like, oh, this is just two dimensional. It's like the
3264640	3269120	rat is in some box and a maze or whatever. And they know whether the rat is using these two
3269120	3274800	dimensional reference frames and know where it is in the maze. We said, okay, what about bats?
3275360	3280000	That's a mammal. And they fly in three dimensional space. How do they do that? They seem to know
3280000	3285840	where they are. So this is a current area of active research. And it seems like somehow the
3285840	3292240	neurons in the entorhonic cortex can learn three dimensional space. We just, two members of our
3292240	3298880	team, along with Ilef Fett from MIT, just released a paper this literally last week,
3299440	3305840	it's on bioarchive, where they show that you can, if you, the way these things work, and I won't get
3305840	3311920	unless you want to, I won't get into the detail, but grid cells can represent any n dimensional
3311920	3318000	space. It's not inherently limited. You can think of it this way. If you had two dimensional,
3318000	3321600	the way it works is you had a bunch of two dimensional slices. That's the way these things
3321600	3326160	work. There's a whole bunch of two dimensional models. And you can just, you can slice up any
3326160	3330880	n dimensional space and with two dimensional projections. So, and you could have one dimensional
3330880	3334640	models. It does. So there's, there's nothing inherent about the mathematics about the way
3334640	3340000	the neurons do this, which, which constrained the dimensionality of the space, which I think was
3340000	3344880	important. So obviously, I have a three dimensional map of this cup, maybe it's even more than that.
3344880	3349520	I don't know. But it's clearly three dimensional map of the cup. I don't just have a projection of
3349520	3353920	the cup. But when I think about birds or when I think about mathematics, perhaps it's more than
3353920	3362000	three dimensions. Who knows? So in terms of each individual column building up more and more
3362000	3368320	information over time, do you think that mechanism is well understood in your mind? You've proposed
3368320	3374640	a lot of architectures there. Is that a key piece or is it, is the big piece, the thousand brain
3375360	3378880	theory of intelligence, the ensemble of it all? Well, I think they're both big. I mean,
3378960	3382960	clearly the concept as a theorist, the concept that's most exciting, right?
3382960	3386240	A high level concept. A high level concept. This is a totally new way of thinking about
3386240	3390640	how the near characteristics work. So that is appealing. It has all these ramifications.
3390640	3394880	And with that, as a framework for how the brain works, you can make all kinds of predictions
3394880	3398400	and solve all kinds of problems. Now we're trying to work through many of these details right now.
3398400	3402560	Okay. How do the neurons actually do this? Well, it turns out, if you think about grid cells and
3402560	3406240	place cells in the old parts of the brain, there's a lot that's known about them, but there's still
3406240	3410000	some mysteries. There's a lot of debate about exactly the details, how these work and what are
3410000	3414000	the signs. And we have that still, that same level of detail, that same level of concern.
3414000	3420480	What we spend here, most of our time doing is trying to make a very good list of the things
3420480	3425520	we don't understand yet. That's the key part here. What are the constraints? It's not like,
3425520	3429760	oh, this seems to work. We're done. Now it's like, okay, it kind of works, but these are other things
3429760	3434880	we know what has to do and it's not doing those yet. I would say we're well on the way here.
3434960	3442240	We're not done yet. There's a lot of trickiness to this system, but the basic principles about how
3442240	3446080	different layers in the neocortex are doing much of this, we understand,
3447120	3449840	but there's some fundamental parts that we don't understand as well.
3449840	3455680	So what would you say is one of the harder open problems or one of the ones that have been bothering
3455680	3460800	you, keeping you up at night the most? Oh, well, right now, this is a detailed thing that wouldn't
3460800	3464960	apply to most people, okay. But you want me to answer that question? Yeah, please.
3466080	3470880	We've talked about, as if, oh, to predict what you're going to sense on this coffee cup, I need
3470880	3474960	to know where my finger is going to be on the coffee cup. That is true, but it's insufficient.
3476240	3480240	Think about my finger touches the edge of the coffee cup. My finger can touch it at different
3480240	3487520	orientations. I can rotate my finger around here, and that doesn't change. I can make that prediction
3488160	3492160	and somehow, so it's not just the location. There's an orientation component of this as well.
3493200	3496880	This is known in the old part of the brain too. There's things called head direction cells, which
3496880	3501520	way the rat is facing. It's the same kind of basic idea. So if my finger were a rat,
3502160	3505040	you know, in three dimensions, I have a three-dimensional orientation,
3505600	3508640	and I have a three-dimensional location. If I was a rat, I would have a,
3508640	3511920	I think of it as a two-dimensional location, a two-dimensional orientation, a one-dimensional
3512000	3518720	orientation, like just which way is it facing. So how the two components work together, how it is
3518720	3528480	that I combine orientation, the orientation of my sensor, as well as the location, is a tricky
3528480	3535680	problem, and I think I've made progress on it. So at a bigger version of that, the perspective is
3535680	3541360	super interesting, but super specific. Yeah, I warned you. No, no, that's really good, but
3542080	3547840	there's a more general version of that. Do you think context matters? The fact that we are
3548400	3556160	in a building in North America, that we, in the day and age where we have mugs, I mean,
3557120	3560640	there's all this extra information that you bring to the table about
3561360	3564560	everything else in the room that's outside of just the coffee cup. Of course it is.
3564560	3570240	How does it get connected, do you think? Yeah, and that is another really interesting question.
3570240	3575280	I'm going to throw that under the rubric or the name of attentional problems. First of all,
3575280	3579920	we have this model. I have many, many models. And also the question, does it matter because...
3579920	3584640	Well, it matters for certain things. Of course it does. Maybe what we think of that as a coffee
3584640	3588320	cup in another part of the world is viewed as something completely different, or maybe our
3588320	3592480	logo, which is very benign in this part of the world, it means something very different in
3592480	3599200	another part of the world. So those things do matter. I think the way to think about this
3599200	3603120	the following, or one way to think about it, is we have all these models of the world.
3604640	3608800	And we model everything. And as I said earlier, I kind of snuck it in there.
3608800	3614880	Our models are actually, we build composite structure. So every object is composed of other
3614880	3618960	objects, which are composed of other objects, and they become members of other objects. So this
3618960	3623840	room has chairs and a table and a room and walls and so on. Now we can just arrange these things
3623840	3630560	in a certain way. You go, oh, that's in the romantic conference room. And what we do is,
3630560	3636240	when we go around the world and we experience the world, by walking into a room, for example,
3636240	3639200	the first thing I do is say, oh, I'm in this room. Do I recognize the room?
3639200	3644000	Then I can say, oh, look, there's a table here. And by attending to the table,
3644000	3647120	I'm then assigning this table in the context of the room. Then I say, oh, on the table,
3647120	3650960	there's a coffee cup. Oh, and on the table, there's a logo. And in the logo,
3650960	3654320	there's the word Nemento. On the look in the logo, there's the letter E. On the look,
3654320	3659600	it has an unusual surf. And it doesn't actually, but pretend that there's a surf.
3659600	3666240	So the point is your attention is kind of drilling deep in and out of these nested structures.
3667360	3671040	And I can pop back up and I can pop back down. I can pop back up and I can pop back down. So
3671600	3675520	when I attend to the coffee cup, I haven't lost the context of everything else,
3676080	3678720	but it's sort of, there's this sort of nested structure.
3678720	3681840	So the attention filters the reference frame formation
3682880	3684240	for that particular period of time?
3684240	3688240	Yes. It basically, moment to moment, you attend the subcomponents,
3688240	3690240	and then you can attend the subcomponents to subcomponents.
3690240	3691360	You can move up and down that.
3691360	3693360	You can move up and down that. We do that all the time. You're not even,
3694080	3697120	now that I'm aware of it, I'm very conscious of it. But until,
3698160	3701120	but most people don't even think about this, you know, you just walk in the room and you
3701120	3704480	don't say, oh, I looked at the chair and I looked at the board and looked at that word on the board
3704480	3706960	and I looked over here. What's going on? Right?
3706960	3709920	So what percentage of your day are you deeply aware of this?
3709920	3712720	And what part can you actually relax and just be Jeff?
3712720	3714320	Me personally, like my personal day.
3714320	3714480	Yeah.
3715360	3717680	Unfortunately, I'm afflicted with too much of the former.
3721200	3722640	Unfortunately, they are unfortunate.
3722640	3724400	Yeah. So you don't think it's useful?
3724400	3725760	Oh, it is useful. Totally useful.
3726640	3729120	I think about this stuff almost all the time.
3729120	3733760	And one of my primary ways of thinking is when I'm asleep at night,
3733760	3738080	I always wake up in the middle of the night and then I stay awake for at least an hour with my
3738080	3742480	eyes shut in sort of a half-sleep state thinking about these things. I come up with answers to
3742480	3746720	problems very often in that sort of half-sleeping state. I think about on my bike ride, I think
3746720	3751520	about on walks. I'm just constantly thinking about this. I have to almost schedule time
3752400	3757120	to not think about this stuff because it's very, it's mentally taxing.
3758320	3762000	When you're thinking about this stuff, are you thinking introspectively, like almost
3762080	3765520	taking a step outside of yourself and trying to figure out what is your mind doing right now?
3765520	3770640	I do that all the time, but that's not all I do. I'm constantly observing myself.
3770640	3775120	So as soon as I started thinking about grid cells, for example, and getting into that,
3775120	3778240	I started saying, oh, well, grid cells can have my place of sense in the world.
3778240	3781440	That's where you know where you are. And it's interesting, we always have a sense of where
3781440	3785840	we are unless we're lost. And so I started at night when I got up to go to the bathroom,
3785840	3789200	I would start trying to do it completely with my eyes closed all the time and I would test my
3789200	3794320	sense of grid cells. I would walk five feet and say, okay, I think I'm here. Am I really there?
3794320	3797840	What's my error? And then I would calculate my error again and see how the errors can accumulate.
3797840	3800320	So even something as simple as getting up in the middle of the night to go to the bathroom,
3800320	3805040	I'm testing these theories out. It's kind of fun. I mean, the coffee cup is an example of that too.
3805600	3811600	So I think I find that these sort of everyday introspections are actually quite helpful.
3812800	3818080	It doesn't mean you can ignore the science. I mean, I spend hours every day reading ridiculously
3818080	3823840	complex papers. That's not nearly as much fun, but you have to sort of build up those constraints
3824480	3828880	and the knowledge about the field and who's doing what and what exactly they think is happening here.
3828880	3832160	And then you can sit back and say, okay, let's try to have pieces all together.
3833280	3838320	Let's come up with some, you know, I'm very, in this group here, people, they know they do this,
3838320	3841280	I do this all the time. I come in with these introspective ideas and say, well,
3841280	3843920	did we ever thought about this? Now watch, well, let's all do this together.
3844880	3850240	And it's helpful. It's not, as long as you don't, if all you did was that,
3850240	3854880	then you're just making up stuff, right? But if you're constraining it by the reality of
3854880	3860800	the neuroscience, then it's really helpful. So let's talk a little bit about deep learning and
3860800	3868880	the successes in the applied space of neural networks and ideas of training model on data
3868960	3873120	and these simple computational units and you're on artificial neurons
3873920	3881280	that would back propagation of statistical ways of being able to generalize from the training
3881280	3887440	set onto data that's similar to that training set. So where do you think are the limitations of
3887440	3892240	those approaches? What do you think are its strengths relative to your major efforts of
3892800	3897680	constructing a theory of human intelligence? Yeah. Well, I'm not an expert in this field.
3897760	3902000	I'm somewhat knowledgeable. So some of it is in just your intuition. What are your...
3902000	3907200	Well, I have a little bit more than intuition, but I just want to say one of the things that
3907200	3910560	you asked me, do I spend all my time thinking about neuroscience? I do. That's to the exclusion
3910560	3914560	of thinking about things like convolutional neural networks. But I try to stay current.
3915200	3919760	So look, I think it's great the progress they've made. It's fantastic. And as I mentioned earlier,
3919760	3926160	it's very highly useful for many things. The models that we have today are actually derived
3926160	3930480	from a lot of neuroscience principles. They are distributed processing systems and distributed
3930480	3935760	memory systems. And that's how the brain works. They use things that we might call them neurons,
3935760	3939680	but they're really not neurons at all. So we can just... They're not really neurons. So they're
3939680	3947040	distributed processing systems. And nature of hierarchy that came also from neuroscience.
3947040	3951440	And so there's a lot of things, the learning rules basically, not backprop, but other heavy
3951440	3955920	and tight learning. I'd be curious to say they're not neurons at all. Can you describe in which
3955920	3960880	way? I mean, some of it is obvious, but I'd be curious if you have specific ways in which
3960880	3965440	you think are the biggest differences. Yeah, we had a paper in 2016 called Why Neurons of
3965440	3971280	Thousands of Synapses. And if you read that paper, you'll know what I'm talking about here.
3971280	3977440	A real neuron in the brain is a complex thing. Let's just start with the synapses on it, which is
3977440	3983280	a connection between neurons. Real neurons can everywhere from five to 30,000 synapses on them.
3984240	3989680	The ones near the cell body, the ones that are close to the soma or the cell body,
3990400	3994560	those are like the ones that people model in artificial neurons. There's a few hundred of
3994560	4001520	those, maybe they can affect the cell, they can make the cell become active. 95% of the synapses
4002160	4005920	can't do that. They're too far away. So if you activate one of those synapses,
4005920	4008800	it just doesn't affect the cell body enough to make any difference.
4008800	4010000	Any one of them individually.
4010000	4012320	Any one of them individually, or even if you do a mass of them.
4014000	4022160	What real neurons do is the following. If you activate or you get 10 to 20 of them
4023440	4026640	active at the same time, meaning they're all receiving an input at the same time,
4026640	4031200	and those 10 to 20 synapses or 40 synapses within a very short distance on the dendro,
4031200	4035280	like 40 microns, a very small area. So if you activate a bunch of these right next to each
4035280	4040480	other at some distant place, what happens is it creates what's called the dendritic spike.
4041120	4046640	And then dendritic spike travels through the dendroids and can reach the soma or the cell body.
4047680	4053440	Now, when it gets there, it changes the voltage, which is sort of like going to make the cell fire,
4053440	4058320	but never enough to make the cell fire. It's sort of what we call it, we depolarize the cell,
4058320	4061920	you raise the voltage a little bit, but not enough to do anything. It's like, well,
4061920	4068800	good as that. And then it goes back down again. So we proposed a theory, which I'm very confident
4069280	4076240	basics are, is that what's happening there is those 95% of the synapses are recognizing dozens
4076240	4081120	to hundreds of unique patterns. They can write, you know, about the 10 and 20 synapses at a time,
4082000	4086880	and they're acting like predictions. So the neuron actually is a predictive engine on its own.
4087600	4091600	It can fire when it gets enough what they call proximal input from those ones near the cell
4091600	4096560	fire, but it can get ready to fire from dozens to hundreds of patterns that it recognizes from
4096560	4103360	the other guys. And the advantage of this to the neuron is that when it actually does produce a spike
4103360	4108480	in action potential, it does so slightly sooner than it would have otherwise. And so what could
4108480	4113120	it slightly sooner? Well, the slightly sooner part is it, there's it all the neurons in the,
4113120	4116560	the excitatory neurons in the brain are surrounded by these inhibitory neurons,
4116560	4122480	and they're very fast, the inhibitory neurons, these basket cells. And if I get my spike out
4122480	4127280	a little bit sooner than someone else, I inhibit all my neighbors around me. Right. And what you
4127280	4131440	end up with is a different representation. You end up with a representation that matches your
4131440	4135920	prediction. It's a, it's a sparser representation, meaning the few are non interactive, but it's
4135920	4142560	much more specific. And so we showed how networks of these neurons can do very sophisticated temporal
4142560	4149840	prediction, basically. So, so this summarizes real neurons in the brain are time based prediction
4149920	4157280	engines. And, and they, and there's no concept of this at all, in artificial, what we call point
4157280	4161040	neurons. I don't think you can mail the brain without them. I don't think you can build intelligence
4161040	4165920	about it, because it's the, it's the, it's where large part of the time comes from. It's, it's,
4165920	4171120	these are predictive models. And the time is, is there's a prior and a, you know, a prediction
4171120	4176560	and an action. And it's inherent through every neuron in the neocortex. So, so I would say that
4176560	4181520	point neurons sort of model a piece of that and not very well at that either. But, you know, like,
4181520	4189120	like for example, synapses are very unreliable. And you cannot assign any precision to them.
4189840	4195200	So even one digit of precision is not possible. So the way real neurons work is they don't add
4195200	4199920	these, they don't change these weights accurately, like artificial neural networks do. They basically
4199920	4205760	form new synapses. And so what you're trying to always do is, is detect the presence of some 10
4205760	4211680	to 20 active synapses at the same time, as opposed, and there's, they're almost binary. It's like,
4211680	4215520	because you can't really represent anything much finer than that. So these are the kind of
4216160	4219840	and I think that's actually another essential component, because the brain works on sparse
4219840	4224880	patterns. And all that, all that mechanism is based on sparse patterns. And I don't actually
4224880	4230000	think you could build our real brains or machine intelligence without incorporating some of those
4230000	4235280	ideas. It's hard to even think about the complexity that emerges from the fact that the timing of the
4235280	4243120	firing matters in the brain, the fact that you form new, new synapses. And I mean, everything
4243120	4247120	you just mentioned in the past few minutes, trust me, if you spend time on it, you can get your mind
4247120	4252320	around it. It's not like it's no longer a mystery to me. No, but, but sorry, as a function in a
4252320	4258000	mathematical way, it's, can you get it start getting an intuition about what gets it excited,
4258000	4263760	what not, and what kind of representation it's not as easy as there's many other types of neural
4263760	4270560	networks that are more amenable to pure analysis. You know, especially very simple networks, you
4270560	4274400	know, oh, I have four neurons and they're doing this. Can we, you know, describe them mathematically
4274400	4279280	what they're doing type of thing. Even the complexity of convolutional neural networks today,
4279280	4285360	it's sort of a mystery. They can't really describe the whole system. And so it's different. My colleague,
4285360	4292880	Subitain Ahmad, he did a nice paper on this. You can get all the stuff on our website if you're
4292880	4297920	interested in talking about some of the mathematical properties of sparse representations. And so we
4297920	4304560	can't, what we can do is we can show mathematically, for example, why 10 to 20 synapses to recognize a
4304560	4308800	pattern is the correct number is the right number you'd want to use. And by the way, that matches
4308800	4317520	biology, we can show mathematically some of these concepts about the show why the brain is so robust
4318480	4322960	to noise and error and fall out and so on. We can show that mathematically as well as empirically
4322960	4329760	in simulations. But the system can't be analyzed completely. Any complex system can't. And so
4329760	4337760	that's out of the realm. But there is there is mathematical benefits and intuitions that can
4337760	4342080	be derived from mathematics. And we try to do that as well. Most most of our papers have a section
4342080	4347440	about that. So I think it's refreshing and useful for me to be talking to you about deep
4347440	4354480	neural networks, because your intuition basically says that we can't achieve anything like intelligence
4354480	4357760	with artificial neural networks. Well, not in the current form. Not in the current form. I'm sure
4357760	4362800	we can do it in the ultimate form, sure. So let me dig into it and see what your thoughts are there
4362800	4367440	a little bit. So I'm not sure if you read this little blog post called Bitter Lesson by Rich
4367520	4373280	Sutton. Recently, he's a reinforcement learning pioneer. I'm not sure if you're familiar with him.
4373280	4379440	His basic idea is that all the stuff we've done in AI in the past 70 years, he's one of the old
4379440	4388960	school guys. The biggest lesson learned is that all the tricky things we've done don't, you know,
4388960	4393600	they benefit in the short term. But in the long term, what wins out is a simple general method
4394400	4399760	that just relies on Moore's law on computation getting faster and faster.
4399760	4402720	This is what he's saying. This is what has worked up to now.
4403600	4409920	What has worked up to now, that if you're trying to build a system, if we're talking about,
4409920	4413360	he's not concerned about intelligence. He's concerned about a system that works
4414400	4420160	in terms of making predictions on applied, narrow AI problems. That's what the discussion is about.
4421120	4428480	That you just try to go as general as possible and wait years or decades for the computation
4428480	4433200	to make it actually. Is he saying that as a criticism or is he saying this is a prescription
4433200	4437920	of what we ought to be doing? Well, it's very difficult. He's saying this is what has worked
4437920	4440880	and yes, a prescription, but it's a difficult prescription because it says
4441520	4447280	all the fun things you guys are trying to do, we are trying to do, he's part of the community,
4447280	4453200	is saying it's only going to be short-term gains. This all leads up to a question, I guess,
4453840	4459200	on artificial neural networks and maybe our own biological neural networks is,
4460320	4465280	do you think if we just scale things up significantly, so take these dumb artificial
4465840	4473120	neurons, the point neurons, I like that term, if we just have a lot more of them,
4473120	4478080	do you think some of the elements that we see in the brain may start emerging?
4478080	4484960	No, I don't think so. We can do bigger problems of the same type. It's been pointed out by many
4484960	4488720	people that today's convolutional neural networks aren't really much different than the ones we had
4488720	4493680	quite a while ago. They're bigger and train more and we have more labeled data and so on,
4496320	4501120	but I don't think you can get to the kind of things I know the brain can do and that we think
4501120	4507120	about as intelligence by just scaling it up. It's a good description of what's happened in
4507120	4511680	the past, what's happened recently with the reemergence of artificial neural networks.
4512480	4516160	It may be a good prescription for what's going to happen in the short term,
4517520	4521920	but I don't think that's the path. I've said that earlier, there's an alternate path. I should
4521920	4528000	mention to you, by the way, that we've made sufficient progress on the whole cortical theory
4528000	4536880	in the last few years that last year, we decided to start actively pursuing how we get these ideas
4536880	4542480	embedded into machine learning. That's again being led by my colleague, Subhathayamad,
4543040	4546080	and because he's more of a machine learning guy, I'm more of an neuroscience guy.
4549200	4556320	Now, I wouldn't say our focus, but it is now an equal focus here because we need to
4556320	4563280	proselytize what we've learned, and we need to show how it's beneficial to the machine
4563280	4567600	learning. We have a plan in place right now. In fact, we just did our first paper on this.
4567600	4572480	I can tell you about that, but one of the reasons I want to talk to you is because I'm trying to
4573200	4577120	get more people in the machine learning community to say, I need to learn about this stuff,
4577120	4580880	and maybe we should just think about this a bit more about what we've learned about the brain,
4580880	4584640	and what are those team members meant to have? What have they done? Is that useful for us?
4585120	4589760	Yeah, so is there elements of all the cortical theory that things we've been talking about
4589760	4593360	that may be useful in the short term? Yes, in the short term, yes.
4593360	4599200	This is the, sorry to interrupt, but the open question is it certainly feels from my perspective
4599200	4604160	that in the long term, some of the ideas we've been talking about will be extremely useful.
4604160	4608480	The question is whether in the short term. Well, this is always what I would call the
4608480	4614720	entrepreneur's dilemma. So you have this long term vision. Oh, we're going to all be driving
4614720	4618160	electric cars or we're all going to have computers or we're all going to whatever.
4618960	4623120	And, and you're at some point in time and you say, I can see that long term vision. I'm sure
4623120	4626320	it's going to happen. How do I get there without killing myself, you know, without going out of
4626320	4631760	business? That's the challenge. That's the dilemma. That's the really difficult thing to do. So we're
4631760	4636240	facing that right now. So ideally what you'd want to do is find some steps along the way that you
4636240	4639840	can get there incrementally. You don't have to like throw it all out and start over again.
4640400	4647520	The first thing that we've done is we focus on the sparse representations. So just, just in case
4647520	4651520	you don't know what that means or some of the listeners don't know what that means. In the
4651520	4656880	brain, if I have like 10,000 neurons, what you would see is maybe 2% of them active at a time.
4656880	4662560	You don't see 50%, you don't think 30%, you might see 2%. And it's always like that.
4662560	4666480	For any set of sensory inputs. It doesn't matter if anything, it doesn't matter with any part of the
4666480	4674080	brain. But which neurons differs? Which neurons are active? Yeah, so let me put this, let's say I
4674080	4678000	take 10,000 neurons that are representing something. They're sitting there in a block together. It's a
4678000	4681600	teeny little block in a neuron, 10,000 neurons. And they're representing a location, they're
4681600	4684560	representing a cop, they're representing the input from my sensors. I don't know, it doesn't
4684560	4690000	matter. It's representing something. The way the representations occur, it's always a sparse
4690000	4694320	representation, meaning it's a population code. So which 200 cells are active tells me what's
4694320	4699280	going on. It's not individual cells aren't that important at all. It's the population code that
4699280	4705680	matters. And when you have sparse population codes, then all kinds of beautiful properties come out
4705680	4710160	of them. So the brain uses sparse population codes that we've written and described these
4710160	4717360	benefits in some of our papers. So they give this tremendous robustness to the systems.
4717360	4721920	Your brains are incredibly robust. Neurons are dying all the time and spasming and synapses
4721920	4729120	falling apart and, you know, all the time and it keeps working. So what Subitai and Louise,
4729120	4735760	one of our other engineers here have done, I've shown that they're introducing sparseness into
4735760	4738480	convolutional neural networks. Now other people are thinking along these lines, but we're going
4738480	4744000	about it in a more principled way, I think. And we're showing that if you enforce sparseness
4744000	4751920	throughout these convolutional neural networks, in both which neurons are active and the connections
4751920	4757120	between them, that you get some very desirable properties. So one of the current hot topics in
4757920	4762640	deep learning right now are these adversarial examples. So, you know, I can give me any deep
4762640	4766800	learning network and I can give you a picture that looks perfect and you're going to call it,
4766800	4772400	you know, you're going to say the monkey is, you know, an airplane. So that's a problem.
4772400	4776480	And DARPA just announced some big thing. They're trying to, you know, have some contests for this.
4776480	4782000	But if you enforce sparse representations here, many of these problems go away. They're much more
4782000	4787440	robust and they're not easy to fool. So we've already shown some of those results,
4788240	4796160	just literally in January or February, just like last month we did that. And you can, I think it's
4796160	4802320	on bioarchive right now or on iCry, you can read about it. But so that's like a baby step.
4802320	4805920	Okay. That's a take something from the brain. We know, we know about sparseness. We know why
4805920	4809440	it's important. We know what it gives the brain. So let's try to enforce that onto this.
4809440	4813360	What's your intuition why sparsity leads to robustness? Because it feels like it would be
4813360	4822880	less robust. Why would you feel the rest robust to you? So it just feels like if the fewer neurons
4822960	4827840	are involved, the more fragile the representation. Yeah, but I didn't say there was lots of
4827840	4834400	funerals. I said, let's say 200. That's a lot. There's still a lot. So here's an intuition for it.
4835120	4840640	This is a bit technical. So for, you know, for engineers, machine learning people,
4840640	4845600	this would be easy, but all the listeners, maybe not. If you're trying to classify something,
4845600	4849600	you're trying to divide some very high dimensional space into different pieces,
4849600	4853520	A and B, and you're trying to create some point where you say all these points in this
4853520	4856320	high dimensional space are A and all these points inside dimensional space are B.
4857520	4863280	And if you have points that are close to that line, it's not very robust. It works for all
4863280	4868240	the points you know about, but it's, it's not very robust because you just move a little bit and
4868240	4874160	you've crossed over the line. When you have sparse representations, imagine I pick, I have,
4874160	4880240	I'm going to pick 200 cells active out of, out of 10,000. Okay. So I have 200 cells active.
4880240	4884880	Now let's say I pick randomly another, a different representation, 200. The overlap
4884880	4890800	between those is going to be very small, just a few. I can pick millions of samples randomly
4891440	4898880	of 200 neurons and not one of them will overlap more than just a few. So one way to think about
4898880	4903360	is if I want to fool one of these representations to look like one of those other representations,
4903360	4908160	I can't move just one cell or two cells or three cells or four cells. I have to move a hundred cells
4909120	4916080	and that makes them robust. In terms of further, so you mentioned sparsity.
4916080	4921040	Won't it be the next thing? Yeah. Okay. So we have, we picked one. We don't know if it's going
4921040	4925040	to work well yet. So again, we're trying to come up incremental ways of moving from
4925760	4931360	brain theory to add pieces to machine learning, current machine learning world and one step at
4931360	4936000	a time. So the next thing we're going to try to do is sort of incorporate some of the ideas of
4937520	4943360	the 1000 brains theory that you have many, many models and that are voting. Now that idea is not
4943360	4948960	new. There's a mixture of models has been around for a long time, but the way the brain does it is
4948960	4955680	a little different and the way it votes is different and the kind of way it represents
4955680	4960960	uncertainty is different. So we're just starting this work, but we're going to try to see if we
4961360	4965280	sort of incorporate some of the principles of voting or principles of 1000 brain theory,
4966000	4972640	like lots of simple models that talk to each other in a very certain way.
4973840	4980480	And can we build more machines and systems that learn faster and also, well, mostly
4981920	4989600	are multimodal and robust to multimodal type of issues. So one of the challenges there
4989600	4995520	is the machine learning computer vision community has certain sets of benchmarks.
4995520	5000960	So it's a test based on which they compete. And I would argue, especially from your perspective,
5001920	5008720	that those benchmarks aren't that useful for testing the aspects that the brain is good at
5008720	5013360	or intelligent. They're not really testing intelligence. They're very fine. And it's been
5013360	5019760	extremely useful for developing specific mathematical models, but it's not useful in the
5019760	5024960	long term for creating intelligence. So you think you also have a role in proposing better
5026080	5030080	tests? Yeah, this is a very, you've identified a very serious problem.
5031360	5035520	First of all, the test that they have or the test that they want, not the test of the other
5035520	5042720	things that we're trying to do, right? You know, what are the so on? The second thing is,
5042720	5049280	sometimes these to be competitive in these tests, you have to have huge data sets and huge computing
5049280	5055440	power. And so, you know, and we don't have that here. We don't have it as well as other big teams
5055440	5062320	that big companies do. So there's numerous issues there. You know, we come out of, you know,
5062320	5066000	we're our approach to this is all based on in some sense, you might argue elegance,
5066000	5068800	we're coming at it from like a theoretical base that we think, Oh, my God, this is so,
5068800	5071680	this is so clearly elegant. This is how brains work. This is what intelligence is.
5071680	5074880	But the machine learning world has gotten in this phase where they think it doesn't matter.
5075440	5079360	Doesn't matter what you think, as long as you do, you know, 0.1% better on this benchmark,
5079360	5085280	that's what that's all that matters. And that's a problem. You know, we have to figure out how
5085280	5088640	to get around that. That's that's a challenge for us. That's that's one of the challenges that
5088640	5095120	we have to deal with. So I agree, you've identified a big issue. It's difficult for those reasons.
5095840	5100720	But, you know, part of the reasons I'm talking to you here today is I hope I'm going to get some
5100720	5105040	machine learning people to say, read those papers. Those might be some interesting ideas. I'm tired.
5105040	5107600	I'm tired of doing this 0.1% improvement stuff, you know,
5108400	5112960	well, that's what that's why I'm here as well, because I think machine learning now as a community
5112960	5120800	is a place where the next step is needs to be orthogonal to what has received success in the
5120800	5126080	past. You see other leaders saying this, machine learning leaders, you know, Jeff Hinton, with
5126080	5129760	his capsules idea. Many people have gotten up saying, you know, we're going to hit road,
5130880	5136080	maybe we should look at the brain, you know, things like that. So hopefully that thinking
5136080	5141120	will occur organically. And then then we're in a nice position for people to come and look at our
5141120	5144960	work and say, well, what can we learn from these guys? Yeah, MIT is just launching a
5144960	5150160	billion dollar computing college that's centered around this idea. So on this idea of what?
5150880	5155600	Well, the idea that, you know, the humanities, psychology and neuroscience have to work all
5155600	5161360	together to get to build the S. Yeah. I mean, Stanford just did this human center today. I
5161360	5167920	said, yeah, I'm a little disappointed in these initiatives because, you know, they're, they're
5167920	5173440	focusing on sort of the human side of it. And it could very easily slip into how humans interact
5173440	5179360	with intelligent machines, which is nothing wrong with that. But that's not, that is orthogonal
5179360	5183120	to what we're trying to do. We're trying to say, like, what is the essence of intelligence? I don't
5183120	5187040	care. In fact, I want to build intelligent machines that aren't emotional, that don't
5187040	5191760	smile at you, that, you know, that aren't trying to tuck you in at night.
5191760	5196880	Yeah, there is that pattern that you, when you talk about understanding humans is important
5196880	5201440	for understanding intelligence, that you start slipping into topics of ethics or,
5202720	5206880	yeah, like you said, the interactive elements as opposed to, no, no, no, we have to zoom in on
5206880	5212720	the brain, study, study what the human brain, the baby, the, let's study what a brain does.
5212720	5217920	Does. And then we can decide which parts of that we want to recreate in some system. But
5217920	5221120	until you have that theory about what the brain does, what's the point? You know, it's just,
5221120	5224560	you're going to be wasting time, I think. Right. Just to break it down on the artificial
5224560	5229040	neural networks side, maybe you can speak to this on the, on the biologic neural networks side,
5229040	5235040	the process of learning versus the process of inference. Maybe you can explain to me,
5236400	5240000	what is there a difference between, you know, in artificial neural networks, there's a
5240000	5244240	difference between the learning stage and the inference stage. Do you see the brain as something
5244240	5249840	different? One of the, one of the big distinctions that people often say, I don't know how correct
5249840	5254240	it is, is artificial neural networks need a lot of data, they're very inefficient learning.
5254800	5260160	Do you see that as a correct distinction from the, the biology of the human brain,
5260160	5264160	that the human brain is very efficient? Or is that just something we deceive ourselves with?
5264160	5267440	No, it is efficient, obviously. We can learn new things almost instantly.
5267440	5272240	And so what elements do you think? Yeah, I can talk about that. You brought up two issues there.
5272240	5277200	So remember I talked early about the constraints we, we always feel, well, one of those constraints
5277200	5282720	is the fact that brains are continually learning. That's not something we said, oh, we can add that
5282720	5289520	later. That's something that was upfront, had to be there from the start, made our problems
5290080	5296160	harder. But we showed, going back to the 2016 paper on sequence memory, we showed how that
5296160	5302560	happens, how the brains infer and learn at the same time. And our models do that. They're not
5302560	5309680	two separate phases or two separate sets of time. I think that's a big, big problem in AI,
5309760	5316480	at least for many applications, not for all. So I can talk about that. There are some that gets
5316480	5321600	detailed. There are some parts of the neocortex in the brain where actually what's going on,
5321600	5326160	there's these, there's these, with these cycles, they're like cycles of activity in the brain.
5326800	5332320	And there's very strong evidence that you're doing more of inference on one part of the phase and
5332320	5335680	more of learning on the other part of the phase. So the brain can actually sort of separate different
5335680	5340720	populations of cells are going back and forth like this. But in general, I would say that's an
5340720	5346880	important problem. We have all of our networks that we've come up with do both. They're learning,
5346880	5351840	continuous learning networks. And you mentioned benchmarks earlier. Well, there are no benchmarks
5351840	5357920	about that. Exactly. So we have to like, we get in our little soapbox and say, hey, by the way,
5357920	5363840	this is important and here's the mechanism for doing that. But until you can prove it to someone
5363840	5367600	in some, you know, commercial system or something, it's a little harder. So yeah, one of the things
5367600	5375200	I had to linger on that is in some ways to learn the concept of a coffee cup. You only need this one
5375200	5380080	coffee cup and maybe some time alone in a room with it. Well, the first thing is I, when I imagine
5380080	5384320	I reach my hand into a black box and I'm reaching, I'm trying to touch something. I don't know up
5384320	5389920	front if it's something I already know, or if it's a new thing. And I have to, I'm doing both at the
5389920	5394720	same time. I don't say, oh, let's see if it's a new thing. Oh, let's see if it's an old thing. I
5394720	5400880	don't do that. As I go, my brain says, oh, it's new or it's not new. And if it's new, I start learning
5401600	5405840	what it is. So and by the way, it starts learning from the get go, even if we're going to recognize
5405840	5410960	it. So they're not separate problems. And so that's the thing. The other thing you mentioned
5410960	5416320	was the fast learning. So I was just talking about continuous learning, but there's also fast
5416320	5420080	learning. Literally, I can show you this coffee cup. And I say, here's a new coffee cup. It's
5420080	5425040	got the logo on it. Take a look at it. Done. You're done. You can predict what it's going to look
5425040	5432720	like, you know, in different positions. So I can talk about that too. In the brain, the way learning
5432720	5437440	occurs. I mentioned this earlier, but I mentioned again, the way learning occurs, I imagine I have
5437440	5443520	a section of a dendrite of a neuron. And I want to learn, I'm going to learn something new. I'm
5443520	5447600	just doesn't matter what it is, I'm just going to learn something new. I need to recognize a new
5447600	5453600	pattern. So what I'm going to do is I'm going to form new synapses. New synapses, we're going to
5453600	5460480	rewire the brain onto that section of the dendrite. Once I've done that, everything else that neuron
5460480	5466320	has learned is not affected by it. That's because it's isolated to that small section of the dendrite.
5466320	5471200	They're not all being added together, like a point neuron. So if I learned something new on this
5471200	5474800	segment here, it doesn't change any of the learning that occur anywhere else in that neuron.
5474800	5479440	So I can add something without affecting previous learning. And I can do it quickly.
5480880	5484400	Now let's talk, we can talk about the quickness, how it's done in real neurons. You might say,
5484400	5489840	well, doesn't it take time to form synapses? Yes, it can take maybe an hour to form a new synapse.
5490800	5496000	We can form memories quicker than that. And I can explain that happens too, if you want. But
5496720	5501280	it's getting a bit neuroscience-y. That's great. But is there an understanding
5501280	5506160	of these mechanisms at every level? So from the short-term memories and the forming
5507520	5512400	many connections. So this idea of synaptogenesis, the growth of new synapses, that's well
5512400	5516720	described, as well understood. And that's an essential part of learning. That is learning.
5516720	5524560	That is learning. Going back many, many years, people was
5525520	5531120	what's his name, the psychologist proposed, Heb, Donald Heb. He proposed that learning was the
5531120	5537200	modification of the strength of a connection between two neurons. People interpreted that as
5537200	5541680	the modification of the strength of a synapse. He didn't say that. He just said there's a
5541680	5546400	modification between the effect of one neuron and another. So synaptogenesis is totally consistent
5546400	5550800	with Donald Heb said. But anyway, there's these mechanisms, the growth of new synapse,
5550800	5553840	you can go online, you can watch a video of a synapse growing in real time.
5553840	5558960	It's literally, you can see this little thing going. It's pretty impressive. So those
5558960	5563520	mechanisms are known. Now, there's another thing that we've speculated and we've written about,
5563520	5569120	which is consistent with no neuroscience, but it's less proven. And this is the idea,
5569120	5573920	how do I form a memory really, really quickly? Like instantaneous. If it takes an hour to
5573920	5580080	grow a synapse, like that's not instantaneous. So there are types of synapses called silent
5580080	5585040	synapses. They look like a synapse, but they don't do anything. They're just sitting there. It's
5585040	5590560	like if an action potential comes in, it doesn't release any neurotransmitter. Some parts of the
5590560	5595200	brain have more of these than others. For example, the hippocampus has a lot of them, which is where
5595200	5602000	we associate most short-term memory with. So what we speculated, again, in that 2016 paper,
5602000	5608160	we proposed that the way we form very quick memories, very short-term memories, or quick
5608160	5615200	memories, is that we convert silent synapses into active synapses. It's like saying a synapse has
5615200	5621520	a zero weight and a one weight. But the long-term memory has to be formed by synaptogenesis. So
5621520	5625600	you can remember something really quickly by just flipping a bunch of these guys from silent to active.
5626080	5632080	It's not from 0.1 to 0.15. It doesn't do anything until it releases transmitter.
5632080	5635520	And if I do that over a bunch of these, I've got a very quick short-term memory.
5636240	5640880	So I guess the lesson behind this is that most neural networks today are fully connected.
5641760	5645920	Every neuron connects every other neuron from layer to layer. That's not correct in the brain.
5645920	5650880	We don't want that. We actually don't want that. It's bad. You want a very sparse connectivity so
5650880	5656640	that any neuron connects to some subset of the neurons in the other layer. And it does so on a
5656640	5663360	dendrite by dendrite segment basis. So it's a very parcelated out type of thing. And that then
5663360	5667360	learning is not adjusting all these weights, but learning is just saying, okay, connect to these
5667360	5673360	10 cells here right now. In that process, you know, with artificial neural networks, it's a very
5673360	5679840	simple process of back propagation that adjusts the weights. The process of synaptogenesis.
5679840	5684160	Synaptogenesis. Synaptogenesis. It's even easier. It's even easier. It's even easier.
5684160	5689840	Back propagation requires something that really can't happen in brains. This back propagation
5689840	5693360	of this error signal. They really can't happen. People are trying to make it happen in brains,
5693360	5698080	but it doesn't happen in brain. This is pure Hebbian learning. Well, synaptogenesis is pure
5698080	5702080	Hebbian learning. It's basically saying there's a population of cells over here that are active
5702080	5706320	right now. And there's a population of cells over here active right now. How do I form connections
5706320	5712480	between those active cells? And it's literally saying this guy became active. These 100 neurons
5712480	5717440	here became active before this neuron became active. So form connections to those ones. That's it.
5717440	5722800	There's no propagation of error. Nothing. All the networks we do, all the models we have work on
5723520	5731520	almost completely on Hebbian learning, but in on dendritic segments and multiple synaptes at the
5731520	5737360	same time. So now let's turn the question that you already answered and maybe you can answer it again.
5738640	5743760	If you look at the history of artificial intelligence, where do you think we stand? How
5743840	5748800	far are we from solving intelligence? You said you were very optimistic. Can you elaborate on that?
5748800	5754960	Yeah. It's always the crazy question to ask because no one can predict the future.
5755520	5761440	So I'll tell you a story. I used to run a different neuroscience institute called the
5761440	5766240	Redburn Neuroscience Institute. And we would hold these symposiums and we'd get like 35 scientists
5766240	5770640	from around the world to come together. And I used to ask them all the same question. I would say,
5770640	5774320	well, how long do you think it'll be before we understand how the New York Cortex works?
5774320	5777040	And everyone went around the room and they introduced the name and they have to answer
5777040	5784640	that question. So I got, the typical answer was 50 to 100 years. Some people would say 500 years.
5784640	5790160	Some people said never. I said, why are you, why are you a neuroscience? It's a good pay.
5792560	5797840	It's interesting. So, you know, but it doesn't work like that. As I mentioned earlier, these are
5798720	5802560	step functions. Things happen and then bingo, they happen. You can't predict that.
5803520	5808720	I feel I've already passed a step function. So if I can do my job correctly over the next five years,
5810720	5814960	then meaning I can proselytize these ideas, I can convince other people they're right,
5816000	5820400	we can show that other people or other machine learning people should pay attention to these
5820400	5825840	ideas, then we're definitely in an under 20 year time frame. If I can do those things,
5825920	5829680	if I, if I'm not successful in that, and this is the last time anyone talks to me
5829680	5834640	and no one reads our papers and, you know, I'm wrong or something like that, then,
5834640	5841520	then I don't know. But it's, it's not 50 years. It's, it, you know, it'll, it'll, you know,
5841520	5845280	the same thing about electric cars, how quickly are they going to populate the world? It probably
5845280	5850240	takes about a 20 year span. It'll be something like that. But I think if I can do what I said,
5850240	5855760	we're starting it. And of course, there could be other use of step functions. It could be
5857440	5861760	everybody gives up on your ideas for 20 years, and then all of a sudden somebody picks it up
5861760	5865520	again. Wait, that guy was onto something. Yeah. So that would be a, that would be a failure on
5865520	5870560	my part, right? You know, think about Charles Babbage, you know, Charles Babbage, he's the guy
5870560	5878320	who invented the computer back in the 18 something 1800s. And everyone forgot about it until, you
5878320	5882080	know, 100 years later and say, Hey, this guy figured this stuff out a long time ago. Yeah.
5882080	5885680	You know, but he was ahead of his time. Yeah. I don't think, you know, like, as I said,
5886320	5891040	I recognize this is part of any entrepreneur's challenge. I use entrepreneur broadly in this
5891040	5893920	case. I'm not meaning like I'm building a business trying to sell something. I mean,
5893920	5900000	like I'm trying to sell ideas. And this is the challenge as to how you get people to pay attention
5900000	5905360	to you. How do you get them to give you positive or negative feedback? How do you get to people
5905360	5909520	act differently based on your ideas? So, you know, we'll see how well we do on that.
5910080	5914800	So, you know, that there's a lot of hype behind artificial intelligence currently. Do you,
5916240	5922400	as, as you look to spread the ideas that are of New York cortical theory of the things you're
5922400	5927120	working on, do you think there's some possibility we'll hit an AI winter once again?
5927120	5929440	Yeah, it's certainly a possibility. No question about it.
5930160	5936400	Yeah. Well, I guess, do I worry about it? I haven't decided yet if that's good or bad for
5936400	5943680	my mission. That's true. That's very true because it's almost like you need the winter to refresh
5943680	5948880	the pallet. Yeah. So, it's like, I want, here's what you want to have it is you want like,
5949520	5956000	to the extent that everyone is so thrilled about the current state of machine learning and AI and
5956080	5961040	they don't imagine they need anything else. It makes my job harder. Right. If, if everything
5961040	5965600	crashed completely and every student left the field and there was no money for anybody to do
5965600	5969200	anything and it became an embarrassment to talk about machine intelligence and AI, that wouldn't
5969200	5973840	be good for us either. You want, you want sort of the soft landing approach, right? You want enough
5973840	5978880	people, the senior people in AI and machine learning and say, you know, we need other approaches. We
5978880	5982960	really need other approaches. Damn, we need other approaches. Maybe we should look to the brain.
5982960	5986800	Okay, let's look to the brain. Who's got some brain ideas? Okay, let's, let's start a little
5986800	5991200	project on the side here, trying to do a brain idea related stuff. That's the ideal outcome we
5991200	5996320	would want. So, I don't want a total winter and yet I don't want it to be sunny all the time either.
5997520	6001760	So, what do you think it takes to build a system with human level intelligence
6002960	6008560	where once demonstrated, you would be very impressed? So, does it have to have a body?
6008560	6013360	Does it have to have the, the, the C word we used before consciousness
6015600	6021280	as, as, as an entirety as a holistic sense? First of all, I don't think the goal is to create a
6021280	6025840	machine that is human level intelligence. I think it's a false goal. Back to Turing, I think it was
6025840	6030160	a false statement. We want to understand what intelligence is and then we can build intelligent
6030160	6035120	machines of all different scales, all different capabilities. You know, a dog is intelligent.
6035120	6039040	I don't need, you know, that'd be pretty good to have a dog, you know, but what about something
6039040	6044320	that doesn't look like an animal at all in different spaces. So, my thinking about this is that we
6044320	6049600	want to define what intelligence is, agree upon what makes an intelligent system. We can then say,
6049600	6053680	okay, we're now going to build systems that work on those principles or some subset of them,
6054240	6060160	and we can apply them to all different types of problems. And the, the kind, the idea, it's
6060160	6065680	like computing. We don't ask, if I take a little, you know, little one chip computer, I don't say,
6065680	6069600	well, that's not a computer because it's not as powerful as this, you know, big server over here.
6069600	6072800	No, no, because we know that what the principles are computing on, and I can apply those principles
6072800	6076800	to a small problem or into a big problem. And same intelligence needs to get there. We have to say,
6076800	6080000	these are the principles. I can make a small one, a big one, I can make them distributed, I can
6080000	6084000	put them on different sensors. They don't have to be human like at all. Now you did bring up a very
6084000	6089120	interesting question about embodiment. Does it have to have a body? It has to have some concept
6089200	6094080	of movement. It has to be able to move through these reference frames I talked about earlier.
6094080	6097680	I, whether it's physically moving, like I need, if I'm going to have an AI that
6097680	6101360	understands coffee cups, it's going to have to pick up the coffee cup and touch it and look at it
6101360	6107360	with its, with its eyes and hands or something equivalent to that. If I have a mathematical AI,
6108160	6113440	maybe it needs to move through mathematical spaces. I could have a virtual AI that lives
6114320	6120880	in the internet and its movements are traversing links and digging into files, but it's got a
6120880	6127600	location that it's traveling through some space. You can't have an AI that just takes some flash
6127600	6134720	thing input, you know, we call it flash inference. Here's a pattern, done. No, it's movement time,
6134720	6138080	movement pattern, movement pattern, movement pattern, attention, digging, building, building
6138080	6143040	structure, just figuring out the model of the world. So some sort of embodiment, whether it's
6143040	6147920	physical or not, has to be part of it. So self-awareness in the way to be able to answer
6147920	6151440	where am I? You're bringing up self-awareness, it's a different topic, self-awareness.
6151440	6159040	No, the very narrow definition, meaning knowing a sense of self enough to know where am I in this
6159040	6164640	space? Yeah, basically the system, the system needs to know its location or each component of the
6164640	6170720	system needs to know where it is in the world at that point in time. So self-awareness and
6170720	6176320	consciousness, do you think, one, from the perspective of neuroscience and neocortex,
6176320	6182240	these are interesting topics, solvable topics, do you have any ideas of why the heck it is that
6182240	6185520	we have a subjective experience at all? Yeah, I have a lot of questions.
6185520	6188320	And is it useful or is it just a side effect of us?
6188320	6193760	It's interesting to think about. I don't think it's useful as a means to figure out how to
6193760	6201280	build intelligent machines. It's something that systems do, and we can talk about what it is,
6201840	6205440	that are like, well, if I build a system like this, then it would be self-aware. Or
6206400	6209920	if I build it like this, it wouldn't be self-aware. So that's a choice I can have.
6209920	6216000	It's not like, oh my god, it's self-aware. I heard an interview recently with this
6216000	6219600	philosopher from Yale, I can't remember his name, I apologize for that. But he was talking about,
6219600	6223120	well, if these computers were self-aware, then it would be a crime done, plug them. And I'm like,
6223120	6230160	oh, come on. I unplug myself every night, I go to sleep. Is that a crime? I plug myself in again
6230160	6235920	in the morning, and there I am. So people get kind of bent out of shape about this.
6235920	6242160	I have very definite, very detailed understanding or opinions about what it means to be conscious
6242160	6247120	and what it means to be self-aware. I don't think it's that interesting a problem. You talk to
6247120	6252160	Kristoff Koch, he thinks that's the only problem. I didn't actually listen to your interview with
6252240	6255680	him, but I know him, and I know that's the thing he cares about.
6255680	6259200	He also thinks intelligence and consciousness have disjoint. So I mean, it's not,
6259200	6263600	you don't have to have one or the other. I disagree with that. I just totally disagree with that.
6264400	6268160	So where's your thoughts and consciousness? Where does it emerge from? Because it is...
6268160	6272000	So then we have to break it down to the two parts, okay? Because consciousness isn't one thing,
6272000	6275440	that's part of the problem with that term. It means different things to different people,
6275440	6280000	and there's different components of it. There is a concept of self-awareness, okay?
6280800	6287360	That can be very easily explained. You have a model of your own body, the neocortex models
6287360	6294000	things in the world, and it also models your own body. And then it has a memory. It can remember
6294000	6298000	what you've done, okay? So it can remember what you did this morning, can remember what you had
6298000	6304480	for breakfast, and so on. And so I can say to you, okay, Lex, were you conscious this morning when
6304560	6310160	you had your bagel? And you'd say, yes, I was conscious. Now what if I could take your brain
6310160	6314960	and revert all the synapses back to the state they were this morning? And then I said to you,
6314960	6318640	Lex, were you conscious when you ate the bagel? And he said, no, I wasn't conscious. I said,
6318640	6323200	here's a video of eating the bagel. And he said, I wasn't there. I have no... That's not possible
6323200	6327360	because I must have been unconscious at that time. So we can just make this one-to-one correlation
6327360	6331920	between memory of your body's trajectory through the world over some period of time,
6331920	6336080	a memory of it. And the ability to recall that memory is what you would call conscious. I was
6336080	6342160	conscious of that. It's a self-awareness. And any system that can recall, memorize what it's
6342160	6348560	done recently, and bring that back and invoke it again would say, yeah, I'm aware. I remember what
6348560	6353680	I did. All right, I got it. That's an easy one, although some people think that's a hard one.
6354640	6359520	The more challenging part of consciousness is this is one that's sometimes used by the word
6359520	6368240	qualia, which is, why does an object seem red? Or what is pain? And why does pain feel like
6368240	6373200	something? Why do I feel redness? Or why do I feel a little painless in a way? And then I could say,
6373200	6376800	well, why does sight seems different than hearing? That's the same problem. It's really,
6377360	6382080	these are all just neurons. And so how is it that why does looking at you feel different than
6382960	6386720	hearing you? It feels different, but there's just neurons in my head. They're all doing the same
6386800	6392160	thing. So that's an interesting question. The best treatise I've read about this is by a guy named
6392160	6398800	O'Regan. O'Regan, he wrote a book called Why Red Doesn't Sound Like a Bell. It's a little,
6400400	6406640	it's not a trade book, easy to read, but it, and it's an interesting question. Take something like
6406640	6411520	color. Color really doesn't exist in the world. It's not a property of the world. Property of the
6411520	6417920	world that exists is light frequency. And that gets turned into we have certain cells in the retina
6417920	6421280	that respond to different frequencies, different than others. And so when they enter the brain,
6421280	6426720	you just have a bunch of axons that are firing at different rates. And from that, we perceive color.
6426720	6430800	But there is no color in the brain. I mean, there's, there's no color coming in on those synapses.
6430800	6436160	It's just a correlation between some, some, some axons and some property of frequency.
6436720	6440560	And that isn't even color itself. Frequency doesn't have a color. It's just a,
6441280	6446400	it's just what it is. So then the question is, well, why does it even appear to have a color at all?
6447840	6451440	Just as you're describing it, there seems to be a connection to these, those ideas of reference
6451440	6460080	frames. I mean, it just feels like consciousness having the subject, assigning the feeling of red
6460880	6467920	to the actual color or to the wavelength is useful for intelligence.
6467920	6471920	Yeah, I think that's a good way of putting it. It's useful as a predictive mechanism or useful
6471920	6476720	as a generalization idea. It's a way of grouping things together to say it's useful to have a model
6476720	6484800	like this. Think about the, the, the well-known syndrome that people who've lost a limb experience
6484800	6492720	called phantom limbs. And what they claim is they can have their arms removed, but they feel
6492720	6497680	the arm that not only feel it, they know it's there. They, it's there. I can, I know it's there.
6497680	6501040	They'll swear to you that it's there and then they can feel pain in their arm and they'll
6501040	6504880	feel it in their finger and if they move their, they move their non-existent arm behind their
6504880	6510720	back, then they feel the pain behind their back. So this whole idea that your arm exists is a model
6510720	6517520	of your brain. It may or may not really exist. And just like, but it's useful to have a model
6517520	6521520	of something that sort of correlates to things in the world so you can make predictions about what
6521520	6525200	would happen when those things occur. It's a little bit of a fuzzy, but I think you're getting
6525200	6530960	quite towards the answer there. It's, it's useful for the model of to, to express things certain
6530960	6534880	ways that we can then map them into these reference frames and make predictions about them.
6535680	6538560	I need to spend more time on this topic. It doesn't bother me.
6538800	6544640	Do you really need to spend more time? It does feel special that we have subjective experience,
6544640	6549040	but I'm yet to know why. I'm just, I'm just personally curious. It's not,
6549040	6553200	it's not necessary for the work we're doing here. I don't think I need to solve that problem to
6553200	6558000	build intelligent machines at all, not at all. But there is sort of the silly notion that you
6558000	6564000	described briefly that doesn't seem so silly to us humans is, you know, if you're successful
6564000	6571120	building intelligent machines, it feels wrong to then turn them off. Because if you're able to
6571120	6577920	build a lot of them, it feels wrong to then be able to, you know, to turn off the,
6578560	6582960	Well, why, but just let's, let's break that down a bit. As humans, why do we fear death?
6583760	6587920	There's, there's two reasons we fear death. Well, first of all, I'll say when you're
6587920	6593040	dead, it doesn't matter. Oh, okay. You're dead. So why do we fear death? We fear death for two
6593040	6598960	reasons. One is because we are our program genetically to fear death. That's a, that's a
6598960	6605360	survival and prop beginning of the genes thing. And we also are programmed to feel sad when people
6605360	6609360	we know die. We don't feel sad for someone we don't know dies. There's people dying right now,
6609360	6612080	they're only scared to say, I'm feel bad about them because I don't know them. But I knew them,
6612080	6618960	I'd feel really bad. So again, this, these are old brain genetically embedded things that we
6619040	6625200	fear death. There's outside of those, those uncomfortable feelings. There's nothing else
6625200	6630080	to worry about. Wait, wait, hold on a second. Do you know the denial of death by Becker?
6631040	6633760	You know, there's a thought that death is,
6636320	6644560	you know, our whole conception of our world model kind of assumes immortality. And then death is
6644560	6649600	this terror that underlies it all. So like, well, some people's world model, not mine.
6650320	6654400	But okay, so what, what Becker would say is that you're just living in an illusion,
6654400	6659520	you've constructed illusion for yourself, because it's such a terrible terror. The fact that
6660080	6664240	what's the illusion, the illusion that death doesn't matter, you're still not coming to grips
6664240	6669760	with the illusion of what that death is going to happen. Oh, like it's not going to happen.
6670640	6674160	You're actually operating. You haven't, even though you said you've accepted it,
6674160	6677680	you haven't really accepted the notion of death is what he was saying. So it sounds like,
6679520	6681600	it sounds like you disagree with that notion. I mean,
6681600	6684240	Yeah, yeah, totally. I, like, I,
6684240	6687120	So death is not that such an important. Every night, every night I go to bed,
6687120	6690640	it's like dying. With little deaths. It's full of death. And if I didn't wake up,
6691600	6695280	it wouldn't matter to me. Only if I knew that was going to happen would it be bothersome. But I
6695280	6699360	didn't know it was going to happen. How would I know? Then I would worry about my wife.
6699440	6704000	So imagine, imagine I was a loner and I lived in Alaska and I lived them
6704000	6707760	out there and there was no animals. Nobody knew I existed. I was just eating these roots all the
6707760	6715920	time and nobody knew I was there. And one day I didn't wake up. What pain in the world would there
6715920	6721920	exist? Well, so most people that think about this problem would say that you're just deeply enlightened
6721920	6729840	or are completely delusional. But I would say, I would say that's a very enlightened
6730400	6735760	way to see the world is that that's the rational one. Well, I think it's rational. That's right.
6735760	6743440	But the fact is we don't, I mean, we really don't have an understanding of why the heck it is we're
6743440	6747840	born and why we die and what happens after we die. Well, maybe there isn't a reason, maybe there is.
6747840	6752240	So I'm interested in those big problems too, right? You know, you interviewed Max Tagmark,
6752240	6755280	you know, and there's people like that, right? I'm interested in those big problems as well.
6755280	6761600	And in fact, when I was young, I made a list of the biggest problems I could think of. First,
6761600	6767040	why does anything exist? Second, why did we have the laws of physics that we have? Third,
6767680	6773040	is life inevitable? And why is it here? Fourth, is intelligence inevitable? And why is it here?
6773040	6777120	I stopped there because I figured if you can make a truly intelligent system,
6777840	6780560	we'll be, that'll be the quickest way to answer the first three questions.
6783200	6788640	I'm serious. And so I said, my mission, you know, you asked me earlier, my first mission is
6788640	6792160	to understand the brain, but I felt that is the shortest way to get to true machine intelligence.
6792160	6795920	And I want to get to true machine intelligence because even if it doesn't occur in my lifetime,
6795920	6799600	other people will benefit from it because I think it'll occur in my lifetime, but you know,
6799600	6806720	20 years, you never know. And but that will be the quickest way for us to, you know, we can make
6806720	6812880	super mathematicians, we can make super space explorers, we can make super physicists brains
6812880	6818640	that do these things, and that can run experiments that we can't run, we don't have the abilities
6818640	6822560	to manipulate things and so on. But we can build and tell the machines to do all those things.
6822560	6827360	And with the ultimate goal of finding out the answers to the other questions.
6827600	6834800	Let me ask you another depressing and difficult question, which is, once we achieve that goal,
6836080	6842960	do you, of creating, no, of understanding intelligence, do you think we would be happier
6842960	6846720	and more fulfilled as a species? Understanding intelligence or understanding the answers to
6846720	6852880	the big questions? Understanding intelligence. Totally. Totally. It would be a far more fun
6852880	6858080	place to live. You think so? Oh, yeah, why not? I mean, you know, just put aside this, you know,
6858080	6864880	terminator nonsense and, and, and, and just think about, you can think about the, we can talk about
6864880	6870160	the risk of AI if you want. I'd love to. So let's talk about. But I think the world is far better
6870160	6874080	knowing things. We're always better than no things. Do you think it's better? Is it a better place to
6874080	6878880	live in that I know that our planet is one of many in the solar system and the solar system is one
6878880	6883280	of many of the galaxies? I think it's a more, I, I dread, I used to, I sometimes think like,
6883280	6887360	God, what would it be like 300 years ago? I'd be looking up the sky. I can't understand anything.
6887360	6891120	Oh my God, I'd be like going to bed every night going, what's going on here? Well, I mean, in
6891120	6896720	some sense, I agree with you, but I'm not exactly sure. So I'm also a scientist. So I have, I share
6896720	6903360	your views, but I'm not, we're, we're like rolling down the hill together. What's down the hill?
6903360	6907520	I feel like we're climbing a hill. Whatever. We're getting, we're getting closer to enlightenment.
6907760	6913200	Whatever. We're climbing, we're getting pulled up a hill by our curiosity.
6913200	6916960	We are putting, our polarity is pulling, we're pulling ourselves up the hill by our curiosity.
6916960	6923120	Yeah. Sisyphus are doing the same thing with the rock. Yeah. But okay, our happiness aside,
6923120	6930480	do you have concerns about, you know, you talk about Sam Harris, Elon Musk, of existential threats
6930480	6934240	of intelligence systems? No, I'm not worried about existential threats at all. There are,
6934240	6937680	there are some things we really do need to worry about. Even today's AI, we have things we have
6937680	6942640	to worry about. We have to worry about privacy and about how impacts false beliefs in the world.
6942640	6947680	And, and we have real problems that, and things to worry about with today's AI.
6948480	6952000	And that will continue as we create more intelligent systems. There's no question, you
6952000	6957440	know, the whole issue about, you know, making intelligent armaments and weapons is something
6957440	6961760	that really we have to think about carefully. I don't think of those as existential threats.
6961760	6966400	I think those are the kind of threats we always face, and we'll have to face them here and, and,
6967280	6972320	and we'll have to deal with them. The, we can, we could talk about what people think are the
6972320	6977360	existential threats. But when I hear people talking about them, they all sound hollow to me.
6977360	6981360	They're, they're based on ideas, they're based on people who really have no idea what intelligence
6981360	6987440	is. And, and if they knew what intelligence was, they wouldn't say those things. So those are not
6987440	6992640	experts in the field, you know. So yeah, so there's two, right? There's, so one is like
6992640	7003040	super intelligent. So a system that becomes far, far superior in reasoning ability than us humans.
7003040	7009840	How is that an existential threat? Then, so there's a lot of ways in which it could be. One way is
7010640	7016000	us humans are actually irrational, inefficient, and get in the way of,
7017520	7023600	of not happiness, but whatever the objective function is of maximizing that objective function.
7023600	7024880	Yeah. Yeah. Super intelligent.
7024880	7026480	There's a paperclip problem and things like that.
7026480	7029360	But so the paperclip problem, but with a super intelligent.
7029360	7033840	Yeah. Yeah. Yeah. So we already faced this threat in some sense.
7034480	7040400	They're called bacteria. These are organisms in the world that would like to turn everything into
7040400	7045680	bacteria. And they're constantly morphing. They're constantly changing to evade our protections.
7046320	7053200	And in the past, they have killed huge swaths of populations of humans on this planet.
7053200	7057440	So if you want to worry about something that's going to multiply endlessly, we have it.
7058320	7063200	And I'm far more worried in that regard, I'm far more worried that some scientists in a laboratory
7063200	7068400	will create a super virus or a super bacteria that we cannot control. That is a more existential
7068400	7073840	threat. Putting an intelligence thing on top of it actually seems to make it less existential
7073840	7078480	to me. It's like, it limits its power. It limits where it can go. It limits the number of things
7078480	7083920	it can do in many ways. A bacteria is something you can't even see. So that's only one of those
7083920	7089680	problems. Yes, exactly. So the other one, just in your intuition about intelligence, when you
7089680	7095600	think about the intelligence of us humans, do you think of that as something, if you look at
7095600	7102000	intelligence on a spectrum from zero to us humans, do you think you can scale that to something far
7102000	7105280	superior? Yeah, all the mechanisms we've been talking about. Let me, I want to make another
7105280	7111440	point here, Alex, before I get there. Sure. Intelligence is the neocortex. It is not the
7111440	7118240	entire brain. If I, the goal is not to make a human. The goal is not to make an emotional system.
7118240	7122640	The goal is not to make a system that wants to have sex and reproduce. Why would I build that?
7122640	7125840	If I want to have a system that wants to reproduce and have sex, make bacteria,
7125840	7130960	make computer viruses. Those are bad things. Don't do that. Those are really bad. Don't do
7130960	7136880	those things. Regulate those. But if I just say I want an intelligent system, why doesn't have to
7136880	7141520	have any of the human-like emotions? Why does it even care if it lives? Why does it even care
7141520	7146240	if it has food? It doesn't care about those things. It's just, you know, it's just in a trance
7146240	7151600	thinking about mathematics or it's out there just trying to build the space, you know, for it on
7151600	7158080	Mars. It's a, we, that's a choice we make. Don't make human-like things. Don't make replicating
7158080	7161600	things. Don't make things that have emotions. Just stick to the neocortex. So that's, that's a
7161600	7166480	view actually that I share, but not everybody shares in the sense that you have faith and
7166480	7173520	optimism about us as engineers of systems, humans as builders of systems to, to, to not put in
7174080	7178960	stupid, not stupid. So this is why, this is why I mentioned the bacteria one. Because you might
7178960	7183280	say, well, some person's going to do that. Well, some person today could create a bacteria that's
7183280	7189840	resistant to all the non-antibacterial agents. So we already have that threat. We already know
7189840	7195920	this is going on. It's not a new threat. So just accept that and then we have to deal with it,
7195920	7201120	right? Yeah. So my point is nothing to do with intelligence. It, intelligence is a separate
7201200	7205200	component that you might apply to a system that wants to reproduce and do stupid things.
7205840	7209600	Let's not do that. Yeah. In fact, it is a mystery why people haven't done that yet.
7210400	7216480	My, my dad as a physicist believes that the reason, for example, nuclear weapons haven't
7217120	7223280	proliferated amongst evil people. So one, one belief that I share is that there's not that many
7223280	7230400	evil people in the world that would, that, that would use back to whether it's bacteria,
7230400	7236720	nuclear weapons, or maybe the future AI systems to do bad. So the fraction is small. And the second
7236720	7242560	is that it's actually really hard, technically. So the, the intersection between evil and
7242560	7247600	competent is small in terms. And by the way, to really annihilate humanity, you'd have to have,
7248240	7252160	you know, sort of the, the nuclear winter phenomenon, which is not one person shooting,
7252160	7257600	you know, or even 10 bombs, you'd have to have some automated system that, you know, detonates
7257600	7262400	a million bombs or 10, whatever many thousands we have. So it's extreme evil combined with extreme
7262400	7267200	competence and just building some stupid system that would automatically, you know, Dr. Strangelup
7267200	7273760	type of thing, you know, I mean, look, we could have some nuclear bomb go off in some major city
7273760	7277520	in the world. Like, I think that's actually quite likely even in my lifetime. I don't think that's
7277680	7283840	unlike the thing. And it'll be a tragedy. But it won't be an existential threat. And it's the same
7283840	7291600	as, you know, the virus of 1917, whenever it was, you know, the influenza, these bad things can happen
7291600	7296960	and the plague and so on. We can't always prevent it. We always, to always try, but we can't. But
7296960	7300320	they're not existential threats until we combine all those crazy things together.
7301040	7306240	So on the, on the spectrum of intelligence from zero to human, do you have a sense of
7306960	7312880	whether it's possible to create several orders of magnitude or at least double that
7313520	7315840	of human intelligence, talking about New York context?
7315840	7320080	I think it's the wrong thing to say double the intelligence. Break it down into different
7320080	7324880	components. Can I make something that's a million times faster than a human brain? Yes,
7324880	7331200	I can do that. Could I make something that is, has a lot more storage than human brain? Yes,
7331200	7334720	I could do that. More common, more copies come. Can I make something that attaches to
7334800	7339280	different sensors than human brain? Yes, I can do that. Could I make something that's distributed?
7339280	7342960	So these people, yeah, we talked earlier about the important New York Cortex voting,
7342960	7346160	they don't have to be co-located. Like, you know, they can be all around the places. I could do that,
7346160	7353360	too. Those are the levers I have, but is it more intelligent? What depends what I train
7353360	7359360	in on? What is it doing? Well, so here's the thing. So let's say larger in New York Cortex
7359360	7368480	and or whatever size that allows for higher and higher hierarchies to form. We're talking about
7368480	7371920	reference frames and concepts. So I could, could I have something that's a super physicist or
7371920	7377120	a super mathematician? Yes. And the question is, once you have a super physicist, will they be
7377120	7383040	able to understand something? Do you have a sense that it will be orders like us compared to ants?
7383040	7391200	Could we ever understand it? Yeah. Most people cannot understand general relativity.
7391840	7395760	It's a really hard thing to get. I mean, you can paint it in a fuzzy picture,
7395760	7401280	stretchy space, you know? Yeah. But the field equations to do that in the deep intuitions
7401280	7407520	are really, really hard. And I've tried, I'm unable to do it. Like, easy to get, you know,
7407520	7410240	it's easy to get special relative, but general relative, man, that's too much.
7412320	7416800	And so we already live with this to some extent. The vast majority of people can't understand
7416800	7421200	actually what the vast majority of other people actually know. We're just either we don't have
7421200	7425600	the effort to or we can't or we don't have time or just not smart enough, whatever. So,
7426800	7430800	but we have ways of communicating. Einstein has spoken in a way that I can understand.
7431520	7437120	He's given me analogies that are useful. I can use those analogies from my own work and think
7437120	7443440	about, you know, concepts that are similar. It's not stupid. It's not like he's exist in some
7443440	7448320	of the plane. There's no connection to my plane in the world here. So that will occur. It already
7448320	7452800	has occurred. That's my point that this story is it already has occurred. We live it every day.
7454400	7458320	One could argue that with we create machine intelligence that think a million times faster
7458320	7462240	than us that it'll be so far, we can't make the connections. But, you know, at the moment,
7463200	7467280	everything that seems really, really hard to figure out in the world when you actually figure
7467280	7472320	it out is not that hard. You know, we can almost everyone can understand the multiverses. Almost
7472320	7475920	everyone can understand quantum physics. Almost everyone can understand these basic things,
7475920	7480960	even though hardly any people could figure those things out. Yeah, but really understand. So,
7482160	7485360	only a few people really don't understand. You need to only understand the
7486320	7490800	the projections, the sprinkles of the useful insights from that. That was my example of
7490800	7495200	Einstein, right? His general theory of relativity is one thing that very, very, very few people
7495200	7499680	can get. And what if we just said those other few people are also artificial intelligences?
7500480	7505120	How bad is that? In some sense, they are, right? Yeah, they say already. I mean, Einstein wasn't
7505120	7509360	a really normal person. He had a lot of weird quirks. And so the other people who work with him.
7509360	7513440	So, you know, maybe they already were sort of this astral plane of intelligence that
7514160	7519120	we live with it already. It's not a problem. It's still useful and, you know.
7520080	7524000	So, do you think we are the only intelligent life out there in the universe?
7524800	7530240	I would say that intelligent life has and will exist elsewhere in the universe. I'll say that.
7531360	7535440	There is a question about contemporaneous intelligence life, which is hard to even answer
7535440	7540720	when we think about relativity and the nature of space time. We can't say what exactly is this
7540720	7548000	time someplace else in the world. But I think it's, you know, I do worry a lot about the filter
7548000	7555120	idea, which is that perhaps intelligent species don't last very long. And so we haven't been around
7555120	7559760	very long. And as a technological species, we've been around for almost nothing, right? You know,
7559760	7565360	what 200 years or something like that. And we don't have any data, a good data point on whether
7565360	7570640	it's likely that we'll survive or not. So, do I think that there have been intelligent
7570640	7574880	life elsewhere in the universe? Almost certainly, of course. In the past and the future, yes.
7576320	7581040	Does it survive for a long time? I don't know. This is another reason I'm excited about our work,
7581040	7587120	is our work meaning the general world of AI. I think we can build intelligent machines
7588640	7595040	that outlast us. And, you know, they don't have to be tied to earth. They don't have to,
7595040	7599600	you know, I'm not saying they're recreating, you know, you know, aliens. I'm just saying
7600800	7604960	if I asked myself, and this might be a good point to end on here. If I asked myself, you know,
7604960	7609760	what's special about our species? We're not particularly interesting physically. We're not,
7609760	7613840	we don't fly. We're not good swimmers. We're not very fast. We're not very strong, you know.
7613840	7617760	It's our brain. That's the only thing. And we are the only species on this planet that's built
7617760	7622320	the model of the world that extends beyond what we can actually sense. We're the only people who
7622320	7627280	know about the far side of the moon and the other universes and other galaxies and other stars and
7628400	7633200	what happens in the atom. That knowledge doesn't exist anywhere else. It's only in our heads.
7633760	7638240	Cats don't do it. Dogs don't do it. Monkeys don't do it. That is what we've created that's unique.
7638240	7644400	Not our genes. It's knowledge. And if I ask me, what is the legacy of humanity? What should our
7644400	7648880	legacy be? It should be knowledge. We should preserve our knowledge in a way that it can exist
7648880	7653680	beyond us. And I think the best way of doing that, in fact, you have to do it, is that it has to go
7653680	7659200	along with intelligent machines to understand that knowledge. That's a very broad idea,
7659840	7664320	but we should be thinking, I call it a state planning for humanity. We should be thinking
7664320	7669760	about what we want to leave behind when as a species we're no longer here. And that'll happen
7669760	7674400	sometime. Sooner or later, it's going to happen. And understanding intelligence and creating
7674400	7679840	intelligence gives us a better chance to prolong. It does give us a better chance to prolong life,
7680400	7685360	yes. It gives us a chance to live on other planets. But even beyond that, I mean, our
7685360	7690560	solar system will disappear one day. It's given enough time. So I don't know. I doubt we will
7690560	7696320	ever be able to travel to other things, but we could tell the stars, but we could send intelligent
7696320	7704320	machines to do that. So you have an optimistic, a hopeful view of our knowledge of the
7704320	7709120	echoes of human civilization living through the intelligent systems we create.
7709120	7712160	Oh, totally. Well, I think the intelligent systems are greater in some sense, the
7712720	7718480	vessel for bringing them beyond Earth or making them last beyond humans themselves.
7719040	7723600	So... And how do you feel about that? That they won't be human, quote unquote.
7723600	7727920	Okay, it's not. But human, what is human? Our species are changing all the time.
7728560	7733840	Human today is not the same as human just 50 years ago. It's, what is human? Do we care about
7733840	7738320	our genetics? Why is that important? As I point out, our genetics are no more interesting than
7738320	7742880	a bacterium's genetics. It's no more interesting than a monkey's genetics. What we have, what's
7742880	7748640	unique and what's valuable is our knowledge, what we've learned about the world. And that
7748640	7753280	is the rare thing. That's the thing we want to preserve. We care about our genes.
7755360	7759520	It's the knowledge. It's the knowledge. That's a really good place to end. Thank you so much
7759520	7761520	for talking to me. Oh, it was fun.
