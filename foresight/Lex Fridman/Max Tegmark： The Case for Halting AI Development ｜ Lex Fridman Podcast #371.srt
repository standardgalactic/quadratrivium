1
00:00:00,000 --> 00:00:04,320
A lot of people have said for many years that there will come a time when we want to pause a little bit.

2
00:00:06,240 --> 00:00:07,120
That time is now.

3
00:00:10,800 --> 00:00:15,440
The following is a conversation with Max Tegmark, his third time in the podcast.

4
00:00:15,440 --> 00:00:20,000
In fact, his first appearance was episode number one of this very podcast.

5
00:00:20,560 --> 00:00:26,240
He is a physicist and artificial intelligence researcher at MIT, co-founder of FutureLeft

6
00:00:27,200 --> 00:00:32,720
Institute and author of Life 3.0, being human in the age of artificial intelligence.

7
00:00:33,440 --> 00:00:39,120
Most recently, he's a key figure in spearheading the open letter calling for a six-month pause

8
00:00:39,120 --> 00:00:44,720
on giant AI experiments like training GPT-4. The letter reads,

9
00:00:45,440 --> 00:00:51,200
We're calling for a pause on training of models larger than GPT-4 for six months.

10
00:00:51,840 --> 00:00:55,840
This does not imply a pause or ban on all AI research and development,

11
00:00:55,840 --> 00:00:59,120
or the use of systems that have already been placed on the market.

12
00:00:59,760 --> 00:01:06,000
Our call is specific and addresses a very small pool of actors who possess this capability.

13
00:01:06,800 --> 00:01:13,280
The letter has been signed by over 50,000 individuals, including 1,800 CEOs and over 1,500

14
00:01:13,280 --> 00:01:19,760
professors. Signatories include Joshua Benio, Steele Russell, Elon Musk, Steve Wozniak,

15
00:01:19,760 --> 00:01:26,320
Yuval Noah Harari, Andrew Yang, and many others. This is a defining moment in the history of human

16
00:01:26,320 --> 00:01:34,240
civilization, where the balance of power between human and AI begins to shift. And Max's mind

17
00:01:34,240 --> 00:01:40,480
and his voice is one of the most valuable and powerful in a time like this. His support,

18
00:01:40,480 --> 00:01:45,760
his wisdom, his friendship has been a gift I'm forever deeply grateful for.

19
00:01:46,640 --> 00:01:51,760
This is Alex Friedman podcast. To support it, please check out our sponsors in the description.

20
00:01:51,760 --> 00:01:59,200
And now, dear friends, here's Max Tagmark. You were the first ever guest on this podcast,

21
00:01:59,200 --> 00:02:05,120
episode number one. So first of all, Max, I just have to say thank you for giving me a chance.

22
00:02:05,120 --> 00:02:08,960
Thank you for starting this journey. It's been an incredible journey. Just thank you for

23
00:02:09,920 --> 00:02:14,960
sitting down with me and just acting like I'm somebody who matters, that I'm somebody who's

24
00:02:14,960 --> 00:02:19,360
interesting to talk to. And thank you for doing it. That meant a lot.

25
00:02:20,160 --> 00:02:26,400
Thanks to you for putting your heart and soul into this. I know when you delve into controversial

26
00:02:26,400 --> 00:02:32,240
topics, it's inevitable to get hit by what Hamlet talks about, the slings and arrows and stuff.

27
00:02:32,240 --> 00:02:37,920
And I really admire this. It's in an era where YouTube videos are too long and now it has to

28
00:02:37,920 --> 00:02:44,640
be like a 20-second TikTok clip. It's just so refreshing to see you going exactly against

29
00:02:44,640 --> 00:02:49,680
all of the advice and doing these really long-form things. And the people appreciate it.

30
00:02:49,680 --> 00:02:54,800
Reality is nuanced. And thanks for sharing it that way.

31
00:02:55,760 --> 00:03:00,320
So let me ask you again, the first question I've ever asked on this podcast, episode number one,

32
00:03:00,880 --> 00:03:05,680
talking to you. Do you think there's intelligent life out there in the universe?

33
00:03:05,680 --> 00:03:12,240
Let's revisit that question. Do you have any updates? What's your view when you look out to the stars?

34
00:03:12,240 --> 00:03:18,880
So when we look out to the stars, if you define our universe the way most astrophysicists do,

35
00:03:18,880 --> 00:03:23,760
not as all of space, but the spherical region of space that we can see with our telescopes,

36
00:03:23,760 --> 00:03:29,520
from which light has the time to reach us since our big man. I'm in the minority. I

37
00:03:31,360 --> 00:03:36,800
estimate that we are the only life in this spherical volume that has

38
00:03:36,960 --> 00:03:43,360
invented internet, radios, gotten our level of tech. And if that's true,

39
00:03:44,720 --> 00:03:51,280
then it puts a lot of responsibility on us to not mess this one up. Because if it's true,

40
00:03:51,280 --> 00:03:59,360
it means that life is quite rare and we are stewards of this one spark of advanced consciousness,

41
00:03:59,360 --> 00:04:05,520
which if we nurture it and help it grow, eventually life can spread from here out

42
00:04:05,520 --> 00:04:09,680
into much of our universe. And we can have this just amazing future. Whereas if we instead

43
00:04:10,640 --> 00:04:15,200
are reckless with the technology we build and just snuff it out due to the stupidity,

44
00:04:16,160 --> 00:04:22,960
or infighting, then maybe the rest of cosmic history in our universe is just going to be

45
00:04:22,960 --> 00:04:30,080
a plenty of empty benches. But I do think that we are actually very likely to get visited by aliens,

46
00:04:30,960 --> 00:04:35,840
alien intelligence quite soon. But I think we are going to be building that alien intelligence.

47
00:04:36,720 --> 00:04:47,040
So we're going to give birth to an intelligent alien civilization. Unlike anything human,

48
00:04:47,040 --> 00:04:52,720
the evolution here on Earth was able to create in terms of the path, the biological path it took.

49
00:04:52,720 --> 00:05:00,000
Yeah, and it's going to be much more alien than a cat or even the most exotic

50
00:05:00,000 --> 00:05:06,160
animal on the planet right now. Because it will not have been created through the usual

51
00:05:06,160 --> 00:05:11,840
Darwinian competition where it necessarily cares about self preservation, the afraid of death,

52
00:05:13,120 --> 00:05:20,000
any of those things. The space of alien minds that you can build is just so much faster than

53
00:05:20,080 --> 00:05:26,640
what evolution will give you. And with that also comes great responsibility for us to make

54
00:05:26,640 --> 00:05:32,960
sure that the kind of minds we create are those kind of minds that it's good to create minds that

55
00:05:33,600 --> 00:05:41,760
will share our values and be good for humanity and life, and also create minds that don't suffer.

56
00:05:42,720 --> 00:05:49,360
Do you try to visualize the full space of alien minds that AI could be? Do you try to consider

57
00:05:49,360 --> 00:05:56,320
all the different kinds of intelligences, sort of generalizing what humans are able to do to the

58
00:05:56,320 --> 00:06:00,400
full spectrum of what intelligent creatures entities could do?

59
00:06:00,400 --> 00:06:07,520
I try, but I would say I fail. I mean, it's very difficult for human minds

60
00:06:07,600 --> 00:06:14,800
to really grapple with something so completely alien, even for us, right? If we just try to

61
00:06:14,800 --> 00:06:22,000
imagine how would it feel if we were completely indifferent towards death or individuality?

62
00:06:24,000 --> 00:06:31,200
Even if you just imagine that, for example, you could just copy my knowledge of how to speak

63
00:06:31,520 --> 00:06:39,600
Swedish. Boom, now you can speak Swedish. And you could copy any of my cool experiences, and

64
00:06:39,600 --> 00:06:44,800
then you could delete the ones you didn't like in your own life, just like that. It would already

65
00:06:44,800 --> 00:06:51,120
change quite a lot about how you feel as a human being, right? You probably spend less effort studying

66
00:06:51,120 --> 00:06:56,160
things if you just copy them. And you might be less afraid of death, because if the plane you're on

67
00:06:56,880 --> 00:07:03,600
starts to crash, you'd just be like, oh, shucks, I haven't backed my brain up for four hours.

68
00:07:04,560 --> 00:07:09,280
So I'm going to lose all these wonderful experiences out of this flight.

69
00:07:11,040 --> 00:07:18,080
We might also start feeling more compassionate, maybe with other people, if we can so readily

70
00:07:18,080 --> 00:07:24,160
share each other's experiences and knowledge and feel more like a hive mind. It's very hard,

71
00:07:24,160 --> 00:07:32,160
though. I really feel very humble about this, to grapple with it, how it might actually feel.

72
00:07:32,960 --> 00:07:39,040
The one thing which is so obvious, though, which I think is just really worth reflecting on is,

73
00:07:39,040 --> 00:07:43,600
because the mind space of possible intelligence is so different from ours,

74
00:07:44,240 --> 00:07:48,080
it's very dangerous if we assume they're going to be like us or anything like us.

75
00:07:48,240 --> 00:07:57,360
Well, the entirety of human written history has been through poetry, through novels,

76
00:07:57,360 --> 00:08:03,440
been trying to describe through philosophy, trying to describe the human condition and what's

77
00:08:03,440 --> 00:08:07,600
entailed in it. Like Jessica said, fear of death and all those kinds of things, what is love,

78
00:08:07,600 --> 00:08:13,760
and all of that changes if you have a different kind of intelligence, all of it. The entirety,

79
00:08:13,760 --> 00:08:18,320
all those poems, they're trying to sneak up to what the hell it means to be human,

80
00:08:18,320 --> 00:08:26,160
all of that changes. How AI concerns and existential crises that AI experiences,

81
00:08:26,160 --> 00:08:33,360
how that clashes with the human existential crisis, the human condition. It's hard to fathom,

82
00:08:33,360 --> 00:08:40,320
hard to predict. It's hard, but it's fascinating to think about also. Even in the best case scenario,

83
00:08:40,320 --> 00:08:46,960
where we don't lose control over the ever more powerful AI that we're building to other humans

84
00:08:46,960 --> 00:08:52,800
whose goals we think are horrible and where we don't lose control to the machines and AI

85
00:08:54,400 --> 00:08:59,520
provides the things that we want, even then you get into the questions, do you touch here?

86
00:09:00,400 --> 00:09:04,960
Maybe it's the struggle that it's actually hard to do things. It's part of the things that give

87
00:09:04,960 --> 00:09:13,040
this meaning as well. For example, I found it so shocking that this new Microsoft GPT-4

88
00:09:13,600 --> 00:09:18,800
commercial that they put together has this woman talking about showing this demo,

89
00:09:19,520 --> 00:09:25,520
how she's going to give a graduation speech to her beloved daughter and she asks GPT-4 to write it.

90
00:09:26,720 --> 00:09:32,320
If it's frigging 200 words or so, if I realized that my parents couldn't be bothered struggling

91
00:09:32,320 --> 00:09:38,320
a little bit to write 200 words and outsource that to their computer, I would feel really offended,

92
00:09:38,320 --> 00:09:46,000
actually. I wonder if eliminating too much of this struggle from our existence,

93
00:09:48,480 --> 00:09:57,680
do you think that would also take away a little bit of what means to be human? We can't even predict.

94
00:09:58,240 --> 00:10:06,080
Somebody mentioned to me that they started using Chat GPT with a 3.5 and not 4.0

95
00:10:07,920 --> 00:10:14,880
to write what they really feel to a person and they have a temper issue and they're

96
00:10:14,880 --> 00:10:20,720
basically trying to get Chat GPT to rewrite it in a nicer way, to get the point across,

97
00:10:20,800 --> 00:10:27,120
but rewrite it in a nicer way. We're even removing the inner asshole from our communication.

98
00:10:28,720 --> 00:10:34,400
There's some positive aspects of that, but mostly it's just the transformation of how

99
00:10:34,400 --> 00:10:43,280
humans communicate and it's scary because so much of our society is based on this glue of

100
00:10:43,280 --> 00:10:50,640
communication. We're now using AI as the medium of communication that does the language for us.

101
00:10:51,360 --> 00:10:57,120
So much of the emotion that's laden in human communication, so much of the intent

102
00:10:58,240 --> 00:11:02,560
that's going to be handled by outsourced AI. How does that change everything? How does that

103
00:11:02,560 --> 00:11:07,440
change the internal state of how we feel about other human beings? What makes us lonely? What

104
00:11:07,440 --> 00:11:12,000
makes us excited? What makes us afraid? How we fall in love? All that kind of stuff.

105
00:11:13,200 --> 00:11:17,600
For me personally, I have to confess the challenge is one of the things that really makes

106
00:11:17,680 --> 00:11:26,880
my life feel meaningful. If I go hike a mountain with my wife, Maya, I don't want to just press

107
00:11:26,880 --> 00:11:30,960
a button and be at the top. I want to struggle and come up there sweaty and feel like, wow,

108
00:11:30,960 --> 00:11:39,680
we did this in the same way. I want to constantly work on myself to become a better person. If I

109
00:11:39,680 --> 00:11:47,120
say something in anger that I regret, I want to go back and really work on myself rather than just

110
00:11:47,680 --> 00:11:52,640
tell an AI from now on, always filter what I write so I don't have to work on myself,

111
00:11:53,440 --> 00:12:00,240
because then I'm not growing. Yeah, but then again, it could be like with chess. An AI

112
00:12:01,760 --> 00:12:07,680
wants it significantly, obviously supersedes the performance of humans. It will live in its own

113
00:12:07,680 --> 00:12:14,160
world and provide maybe a flourishing civilizations for humans, but we humans will continue hiking

114
00:12:14,240 --> 00:12:19,360
mountains and playing our games, even though AI is so much smarter, so much stronger, so much

115
00:12:19,360 --> 00:12:26,560
superior in every single way, just like with chess. That's one possible hopeful trajectory here,

116
00:12:26,560 --> 00:12:43,760
is that humans will continue to human and AI will just be a medium that enables the human

117
00:12:43,760 --> 00:12:51,840
experience to flourish. Yeah, I would phrase that as rebranding ourselves from Homo sapiens

118
00:12:51,840 --> 00:12:59,120
to Homo sentiens. Right now, sapiens, the ability to be intelligent, we've even put it in our

119
00:12:59,120 --> 00:13:06,080
species name. We're branding ourselves as the smartest information processing

120
00:13:07,040 --> 00:13:12,560
entity on the planet. That's clearly going to change if AI continues ahead.

121
00:13:14,000 --> 00:13:17,840
So maybe we should focus on the experience instead, the subjective experience that we have

122
00:13:19,200 --> 00:13:25,040
with Homo sentiens, and that's what's really valuable, the love, the connection, the other things.

123
00:13:27,920 --> 00:13:34,800
Get off our high horses and get rid of this hubris that only we can do integrals.

124
00:13:35,760 --> 00:13:41,920
So consciousness, the subjective experience is a fundamental value to what it means to be human.

125
00:13:41,920 --> 00:13:48,720
Make that the priority. That feels like a hopeful direction to me, but that also requires more

126
00:13:49,680 --> 00:13:54,960
compassion, not just towards other humans, because they happen to be the smartest

127
00:13:54,960 --> 00:13:59,200
on the planet, but also towards all our other fellow creatures on this planet. I personally

128
00:13:59,200 --> 00:14:04,240
feel right now, we're treating a lot of farm animals horribly, for example, and the excuse we're

129
00:14:04,240 --> 00:14:10,240
using is, oh, they're not as smart as us. But if we admit that we're not that smart in the grand

130
00:14:10,240 --> 00:14:17,200
scheme of things either in the post AI epoch, then surely we should value the subjective

131
00:14:17,200 --> 00:14:24,640
experience of a cow also. Well, allow me to briefly look at the book, which at this point

132
00:14:24,640 --> 00:14:29,600
is becoming more and more visionary that you've written, I guess, over five years ago, Life 3.0.

133
00:14:30,560 --> 00:14:38,240
So first of all, 3.0. What's 1.0? What's 2.0? What's 3.0? And how's that vision sort of evolve?

134
00:14:38,880 --> 00:14:45,040
The vision in the book evolved to today. Life 1.0 is really dumb, like bacteria,

135
00:14:45,040 --> 00:14:48,960
and that it can't actually learn anything at all during the lifetime. The learning just comes from

136
00:14:48,960 --> 00:14:59,280
this genetic process from one generation to the next. Life 2.0 is us and other animals which

137
00:14:59,280 --> 00:15:10,720
have brains, which can learn during their lifetime a great deal. And you were born without being able

138
00:15:10,720 --> 00:15:15,760
to speak English, and at some point you decided, hey, I want to upgrade my software. Let's install

139
00:15:15,760 --> 00:15:25,600
an English speaking module. So you did? And Life 3.0 does not exist yet, can replace not only its

140
00:15:25,600 --> 00:15:33,040
software the way we can, but also its hardware. And that's where we're heading towards at high

141
00:15:33,040 --> 00:15:42,480
speed. We're already maybe 2.1 because we can put in an artificial knee, a pacemaker, etc, etc.

142
00:15:42,480 --> 00:15:51,200
And if Neuralink and other companies succeed, we'll be Life 2.2, etc. But the companies trying

143
00:15:51,200 --> 00:15:56,000
to build AGI or trying to make is, of course, full 3.0. And you can put that intelligence

144
00:15:56,000 --> 00:16:02,640
in something that also has no biological basis whatsoever.

145
00:16:02,640 --> 00:16:09,040
So less constraints and more capabilities, just like the leap from 1.0 to 2.0. There is,

146
00:16:09,040 --> 00:16:14,160
nevertheless, you speaking so harshly about bacteria, so disrespectfully about bacteria,

147
00:16:14,240 --> 00:16:23,760
there is still the same kind of magic there that permeates Life 2.0 and 3.0. It seems like maybe

148
00:16:23,760 --> 00:16:29,920
the thing that's truly powerful about life intelligence and consciousness was already

149
00:16:29,920 --> 00:16:37,920
there in 1.0. Is it possible? I think we should be humble and not be so quick to

150
00:16:38,720 --> 00:16:43,920
make everything binary and say either it's there or it's not. Clearly, there's a great

151
00:16:43,920 --> 00:16:49,360
spectrum. And there is even a controversy about whether some unicellular organisms,

152
00:16:49,360 --> 00:16:54,640
like amoebas, can maybe learn a little bit, you know, after all. So apologies if I offended

153
00:16:54,640 --> 00:17:00,000
any bacteria here. It wasn't my intent. It was more that I wanted to talk up how cool it is to

154
00:17:00,000 --> 00:17:04,480
actually have a brain where you can learn dramatically within your lifetime.

155
00:17:04,560 --> 00:17:09,200
Typical human. And the higher up you get from 1.0 to 2.0 to 3.0,

156
00:17:09,200 --> 00:17:13,840
the more you become the captain of your own ship, the master of your own destiny,

157
00:17:13,840 --> 00:17:20,160
and the less you become a slave to whatever evolution gave you. By upgrading your software,

158
00:17:20,160 --> 00:17:23,600
we can be so different from previous generations and even from our parents,

159
00:17:24,560 --> 00:17:32,080
much more so than even a bacterium, no offense to them. And if you can also swap out your hardware,

160
00:17:32,080 --> 00:17:35,760
take any physical form you want, of course, really, the sky is the limit.

161
00:17:36,720 --> 00:17:44,160
Yeah. So it accelerates the rate at which you can perform the computation that determines your

162
00:17:44,160 --> 00:17:50,400
destiny. Yeah. And I think it's worth commenting a bit on what you means in this context. Also,

163
00:17:50,400 --> 00:17:55,040
if you swap things out a lot, right? This is controversial, but my

164
00:17:55,520 --> 00:18:08,720
current understanding is that life is best thought of not as a bag of meat or even a bag of

165
00:18:10,080 --> 00:18:17,920
elementary particles, but rather as a system which can process information and retain its own

166
00:18:17,920 --> 00:18:24,320
complexity, even though nature is always trying to mess it up. So it's all about information

167
00:18:24,400 --> 00:18:31,280
processing. And that makes it a lot like something like a wave in the ocean, which is not its

168
00:18:31,280 --> 00:18:36,400
water molecules, right? The water molecules bob up and down, but the wave moves forward. It's an

169
00:18:36,400 --> 00:18:42,640
information pattern. In the same way, you, Lex, you're not the same atoms as during the first

170
00:18:43,440 --> 00:18:50,640
time you did with me. You've swapped out most of them, but it's still you. And the information

171
00:18:50,640 --> 00:19:00,240
pattern is still there. And if you could swap out your arms and whatever, you can still have

172
00:19:00,240 --> 00:19:04,960
this kind of continuity. It becomes much more sophisticated sort of way before in time where

173
00:19:04,960 --> 00:19:12,400
the information lives on. I lost both of my parents since our last podcast. And it actually

174
00:19:12,400 --> 00:19:19,120
gives me a lot of solace that this way of thinking about them, they haven't entirely died because

175
00:19:20,640 --> 00:19:26,800
a lot of mommy and daddy's, sorry, I'm getting a little emotional here, but a lot of their values

176
00:19:27,680 --> 00:19:34,240
and ideas and even jokes and so on, they haven't gone away, right? Some of them live on, I can

177
00:19:34,240 --> 00:19:39,200
carry on some of them. And we also live on a lot of other, and a lot of other people. So in this

178
00:19:39,200 --> 00:19:47,920
sense, even with life 2.0, we can to some extent already transcend our physical bodies and our

179
00:19:47,920 --> 00:19:56,480
death. And particularly if you can share your own information, your own ideas with many others,

180
00:19:56,480 --> 00:20:06,160
like you do in your podcast, then that's the closest immortality we can get with our bio bodies.

181
00:20:06,800 --> 00:20:13,600
You carry a little bit of them in you in some sense. Do you miss them? You miss your mom and

182
00:20:13,600 --> 00:20:19,680
dad? Of course. Of course. What did you learn about life from them if it can take a bit of a

183
00:20:19,680 --> 00:20:31,280
tangent? I know so many things. For starters, my fascination for math and the physical mysteries

184
00:20:31,280 --> 00:20:37,280
of our universe, I think you got a lot of that from my dad. But I think my obsession for really big

185
00:20:37,280 --> 00:20:45,520
questions and consciousness and so on, that actually came mostly from my mom. And when I got

186
00:20:45,520 --> 00:21:00,960
from both of them, which is very core part of really who I am, I think is this feeling comfortable with

187
00:21:01,200 --> 00:21:06,640
not buying into what everybody else is saying.

188
00:21:09,680 --> 00:21:10,720
Doing what I think is right.

189
00:21:14,560 --> 00:21:21,680
They both very much did their own thing and sometimes they got flack for it and they did it

190
00:21:21,680 --> 00:21:27,440
anyway. That's why you've always been an inspiration to me, that you're at the top of your field and

191
00:21:27,440 --> 00:21:36,880
you're still willing to tackle the big questions in your own way. You're one of the people that

192
00:21:36,880 --> 00:21:42,720
represents MIT best to me. You've always been an inspiration to that. So it's good to hear that

193
00:21:42,720 --> 00:21:48,480
you got that from your mom and dad. Yeah, you're too kind. But yeah, I mean, the good reason to do

194
00:21:48,480 --> 00:21:54,240
science is because you're really curious, you want to figure out the truth. If you think

195
00:21:55,120 --> 00:21:59,680
this is how it is and everyone else says no, no, that's bullshit. And it's that way, you know,

196
00:22:01,760 --> 00:22:08,960
you stick with what you think is true. And even if everybody else keeps thinking it's

197
00:22:08,960 --> 00:22:17,360
bullshit, there's a certain, I always root for the underdog when I watch movies. And my dad once,

198
00:22:17,360 --> 00:22:22,560
I, one time, for example, when I wrote one of my craziest papers ever, talking about our

199
00:22:22,560 --> 00:22:26,320
universe ultimately being mathematical, which we're not going to get into today. I got this email

200
00:22:26,320 --> 00:22:30,960
from a quite famous professor saying this is not only bullshit, but it's going to ruin your career.

201
00:22:30,960 --> 00:22:35,360
You should stop doing this kind of stuff. I sent it to my dad. Do you know what he said?

202
00:22:36,640 --> 00:22:43,520
He replied with a quote from Dante. Segui il tuo corso e la sedire la gente. Follow your own path

203
00:22:44,160 --> 00:22:52,320
and let the people talk. Go dad. This is the kind of thing. He's dead, but that attitude

204
00:22:52,320 --> 00:23:00,400
is not. How did losing them as a man, as a human being change you? How did it expand your thinking

205
00:23:00,400 --> 00:23:05,600
about the world? How did it expand your thinking about, you know, this thing we're talking about,

206
00:23:05,600 --> 00:23:14,240
which is humans creating another living sentient perhaps being? I think it

207
00:23:14,400 --> 00:23:24,480
mainly did two things. One of them just going through all their stuff after they had passed

208
00:23:24,480 --> 00:23:28,160
away and so on, just drove home to me. How important it is to ask ourselves,

209
00:23:29,920 --> 00:23:34,320
why are we doing this things we do? Because it's inevitable that you look at some things they spent

210
00:23:34,320 --> 00:23:39,280
an enormous time on. And you ask the, at hindsight, would they really have spent so much time on this?

211
00:23:40,240 --> 00:23:45,680
Would they have done something that was more meaningful? So I've been looking more in my life

212
00:23:45,680 --> 00:23:55,040
now and asking, you know, why am I doing what I'm doing? And I feel it should either be something I

213
00:23:55,040 --> 00:24:00,800
really enjoy doing or it should be something that I find really, really meaningful because it helps

214
00:24:01,760 --> 00:24:11,040
humanity. And if it's in none of those two categories, maybe I should spend less time on it,

215
00:24:11,040 --> 00:24:18,000
you know. The other thing is dealing with death up in personal like this, it's actually made me

216
00:24:18,000 --> 00:24:25,760
less afraid of even less afraid of other people telling me that I'm an idiot, you know,

217
00:24:25,760 --> 00:24:34,560
which happens regularly and just let my life do my thing, you know. And

218
00:24:36,480 --> 00:24:40,480
it made it a little bit easier for me to focus on what I feel is really important.

219
00:24:40,480 --> 00:24:45,440
What about fear of your own death? Has it made it more real that this is

220
00:24:47,600 --> 00:24:53,280
something that happens? Yeah, it's made it extremely real and I'm next in line in our

221
00:24:53,280 --> 00:25:01,280
family now, right? Me and my younger brother. But they both handled it with such dignity.

222
00:25:02,160 --> 00:25:07,760
That was a true inspiration also. They never complained about things and, you know, when

223
00:25:07,760 --> 00:25:11,600
you're old and your body starts falling apart, it's more and more to complain about. They

224
00:25:11,600 --> 00:25:16,640
looked at what could they still do that was meaningful. And they focused on that rather than

225
00:25:16,640 --> 00:25:23,520
wasting time talking about or even thinking much about things they were disappointed in.

226
00:25:24,320 --> 00:25:29,120
I think anyone can make themselves depressed if they start their morning by making a list of

227
00:25:29,120 --> 00:25:35,760
grievances. Whereas if you start your day with a little meditation and just things you're grateful

228
00:25:35,760 --> 00:25:41,840
for, you basically choose to be a happy person. Because you only have a finite number of days

229
00:25:42,480 --> 00:25:45,840
to spend them being grateful. Yeah.

230
00:25:48,400 --> 00:25:55,600
Well, you do happen to be working on a thing which seems to have potentially

231
00:25:56,400 --> 00:26:00,640
some of the greatest impact on human civilization of anything humans have ever created,

232
00:26:00,640 --> 00:26:05,520
which is artificial intelligence. This is on the both detailed technical level and in a

233
00:26:05,520 --> 00:26:12,480
high philosophical level you work on. So you've mentioned to me that there's an open letter

234
00:26:13,520 --> 00:26:21,120
that you're working on. It's actually going live in a few hours. So I've been having late

235
00:26:21,120 --> 00:26:28,080
nights and early mornings. It's been very exciting, actually. In short, have you seen

236
00:26:28,080 --> 00:26:34,880
Don't Look Up? the film? Yes, yes. I don't want to be the movie spoiler for anyone watching this

237
00:26:34,880 --> 00:26:41,360
who hasn't seen it. But if you're watching this, you haven't seen it, watch it. Because we are

238
00:26:41,360 --> 00:26:48,160
actually acting out. It's life-imitating art. Humanity is doing exactly that right now, except

239
00:26:49,120 --> 00:26:55,200
it's an asteroid that we are building ourselves. Almost nobody is talking about it. People are

240
00:26:55,200 --> 00:26:59,200
squabbling across the planet about all sorts of things which seem very minor compared to the

241
00:26:59,200 --> 00:27:04,240
asteroid that's about to hit us, right? Most politicians don't even have the radar

242
00:27:04,240 --> 00:27:11,280
this on the radar. They think maybe in 100 years or whatever. Right now we're at a fork on the road.

243
00:27:11,280 --> 00:27:17,680
This is the most important fork humanity has reached in its over 100,000 years on this planet.

244
00:27:17,680 --> 00:27:24,640
We're building effectively a new species that's smarter than us. It doesn't look so much like a

245
00:27:24,640 --> 00:27:30,480
species yet because it's mostly not embodied in robots. But that's the technicality which will

246
00:27:30,480 --> 00:27:38,400
soon be changed. And this arrival of artificial general intelligence that can do all our jobs

247
00:27:38,400 --> 00:27:45,440
as well as us and probably shortly thereafter superintelligence which greatly exceeds our

248
00:27:45,440 --> 00:27:49,840
cognitive abilities, it's going to either be the best thing ever to happen to humanity or the worst.

249
00:27:49,840 --> 00:27:55,040
I'm really quite confident that there is not that much middle ground there.

250
00:27:55,120 --> 00:27:58,640
But it would be fundamentally transformative to human civilization?

251
00:27:58,640 --> 00:28:05,200
Of course. Utterly and totally. Again, we branded ourselves as homo sapiens because it seemed like

252
00:28:05,200 --> 00:28:10,400
the basic thing. We're the king of the castle on this planet. We're the smart ones. If we can

253
00:28:10,400 --> 00:28:16,880
control everything else, this could very easily change. We're certainly not going to be the smartest

254
00:28:18,240 --> 00:28:23,920
on the planet very long unless AI progress just halts. And we can talk more about why I

255
00:28:24,640 --> 00:28:29,440
think that's true because it's controversial. And then we can also talk about

256
00:28:32,480 --> 00:28:36,560
reasons you might think it's going to be the best thing ever. And the reason you think it's going

257
00:28:36,560 --> 00:28:43,520
to be the end of humanity, which is of course super controversial. But what I think we can,

258
00:28:44,400 --> 00:28:51,920
anyone who's working on advanced AI can agree on is it's much like the film

259
00:28:51,920 --> 00:28:58,800
don't look up and that it's just really comical how little serious public debate there is

260
00:28:58,800 --> 00:29:07,600
about it given how huge it is. So what we're talking about is a development of currently

261
00:29:07,600 --> 00:29:17,440
things like GPT-4 and the signs it's showing of rapid improvement that may in the near-term

262
00:29:17,440 --> 00:29:24,320
lead to development of super intelligent AGI AI general AI systems and what kind of impact that

263
00:29:24,320 --> 00:29:31,520
has on society. When that thing achieves general human level intelligence and then beyond that

264
00:29:32,400 --> 00:29:39,760
general super human level intelligence. There's a lot of questions to explore here. So one you

265
00:29:39,760 --> 00:29:47,280
mentioned halt. Is that the content of the letter? Is it to suggest that maybe we should

266
00:29:47,280 --> 00:29:51,600
pause the development of these systems? Exactly. So this is very controversial

267
00:29:52,800 --> 00:29:58,080
from when we talked the first time we talked about how I was involved in starting the Future

268
00:29:58,080 --> 00:30:03,760
Life Institute and we worked very hard on 2014-2015 was the mainstream AI safety.

269
00:30:04,320 --> 00:30:08,560
The idea that there even could be risks and that you could do things about them.

270
00:30:09,600 --> 00:30:13,680
Before then a lot of people thought it was just really kooky to even talk about it and a lot of

271
00:30:13,680 --> 00:30:20,160
AI researchers felt worried that this was too flaky and it could be bad for funding and that

272
00:30:20,160 --> 00:30:26,400
the people who talked about it were just not didn't understand AI. I'm very very happy with

273
00:30:27,840 --> 00:30:33,200
how that's gone and that now you know just completely mainstream you go on any AI conference

274
00:30:33,200 --> 00:30:37,840
and people talk about AI safety and it's a nerdy technical field full of equations and

275
00:30:41,520 --> 00:30:48,080
as it should be. But there's this other thing which has been quite taboo up until now

276
00:30:49,360 --> 00:30:55,200
calling for slowdown. So what we've constantly been saying including myself I've been

277
00:30:55,280 --> 00:31:03,040
biting my tongue a lot you know is that you know we don't need to slow down AI development we just

278
00:31:03,040 --> 00:31:10,560
need to win this race the wisdom race between the growing power of the AI and the growing wisdom

279
00:31:10,560 --> 00:31:15,360
with which we manage it and rather than trying to slow down AI let's just try to accelerate the

280
00:31:15,360 --> 00:31:21,200
wisdom do all this technical work to figure out how you can actually ensure that your powerful AI

281
00:31:21,200 --> 00:31:28,240
is going to do what you wanted to do and have society adapt also with incentives and regulations

282
00:31:28,240 --> 00:31:38,720
so that these things get put to good use. Sadly that didn't pan out. The progress on technical AI

283
00:31:38,720 --> 00:31:47,600
on capabilities has gone a lot faster than than many people thought back when we started this in

284
00:31:47,600 --> 00:31:56,720
2014 turned out to be easier to build really advanced AI than we thought. And on the other side

285
00:31:56,720 --> 00:32:04,240
it's gone much slower than we hoped with getting policy makers and others to actually

286
00:32:05,200 --> 00:32:11,360
put incentives in place to steer this in the good direction. Maybe we should unpack it and

287
00:32:11,360 --> 00:32:15,600
talk a little bit about each so why did it go faster than a lot of people thought.

288
00:32:17,600 --> 00:32:24,240
In hindsight it's exactly like building flying machines. People spent a lot of time wondering

289
00:32:24,240 --> 00:32:29,360
about how how do birds fly you know and that turned out to be really hard. Have you seen the

290
00:32:29,360 --> 00:32:34,560
TED Talk with a flying bird? Like a flying robotic bird? Yeah flies around the audience

291
00:32:35,120 --> 00:32:39,440
but it took a hundred years longer to figure out how to do that than for the Wright brothers to

292
00:32:39,440 --> 00:32:44,240
build the first airplane because it turned out there was a much easier way to fly and evolution

293
00:32:44,240 --> 00:32:49,520
picked a more complicated one because it had its hands tied. It could only build a machine that

294
00:32:49,520 --> 00:32:54,960
could assemble itself which the Wright brothers didn't care about. They can only build a machine

295
00:32:54,960 --> 00:32:59,680
that use only the most common atoms in the periodic table. Wright brothers didn't care about that.

296
00:32:59,680 --> 00:33:06,880
They could use steel iron atoms and it had to be able to repair itself and it also had to be

297
00:33:06,880 --> 00:33:14,000
incredibly fuel efficient. You know a lot of birds use less than half the fuel of a remote

298
00:33:14,000 --> 00:33:19,440
control plane flying the same distance. For humans just throw a little more put a little more fuel

299
00:33:19,440 --> 00:33:24,560
in a roof. There you go a hundred years earlier. That's exactly what's happening now with with

300
00:33:24,560 --> 00:33:30,560
these large language models. The brain is incredibly complicated. Many people made the

301
00:33:30,560 --> 00:33:35,440
mistake you're thinking we have to figure out how the brain does human level AI first before

302
00:33:35,440 --> 00:33:41,200
we could build in a machine. That was completely wrong. You can take an incredibly simple

303
00:33:41,280 --> 00:33:47,600
computational system called a transformer network and just train it to do something incredibly dumb.

304
00:33:48,560 --> 00:33:52,080
Just read a gigantic amount of text and try to predict the next word.

305
00:33:53,200 --> 00:33:59,360
And it turns out if you just throw a ton of compute at that and ton of data it gets to be

306
00:34:00,640 --> 00:34:04,720
frighteningly good like GPT-4 which I've been playing with so much since it came out right.

307
00:34:05,680 --> 00:34:12,800
And there's still some debate about whether that can get you all the way to full human level or not

308
00:34:13,520 --> 00:34:18,080
but yeah we can come back to the details of that and how you might get the human level AI even if

309
00:34:20,160 --> 00:34:25,200
large language models don't. Can you briefly if it's just a small tangent comment on your

310
00:34:25,200 --> 00:34:33,680
feelings about GPT-4? Suggest that you're impressed by this rate of progress but where is it? Can GPT-4

311
00:34:34,640 --> 00:34:40,800
reason? What are like the intuitions? What are human interpretable words you can assign to the

312
00:34:40,800 --> 00:34:47,040
capabilities of GPT-4 that makes you so damn impressed with it? I'm both very excited about it

313
00:34:47,040 --> 00:34:53,360
and terrified. It's an interesting mixture of emotions. All the best things in life include

314
00:34:53,360 --> 00:35:00,560
those two somehow. Yeah I can absolutely reason. Anyone who hasn't played with it I highly recommend

315
00:35:00,560 --> 00:35:08,320
doing that before dissing it. It can do quite remarkable reasoning and I've had to do a lot

316
00:35:08,320 --> 00:35:14,640
of things which I realized I couldn't do that myself that well even and it obviously does it

317
00:35:14,640 --> 00:35:20,400
dramatically faster than we do too when you watch a type and it's doing that well servicing a massive

318
00:35:20,400 --> 00:35:28,640
number of other humans at the same time. At the same time it cannot reason as well as a human can

319
00:35:29,360 --> 00:35:36,000
on some tasks. It's obviously a limitation from its architecture. We have in our heads what in

320
00:35:36,000 --> 00:35:40,560
GeekSpeak is called a recurrent neural network. There are loops. Information can go from this

321
00:35:40,560 --> 00:35:44,160
neuron to this neuron to this neuron and then then back to this one. You can like ruminate on

322
00:35:44,160 --> 00:35:51,360
something for a while. You can self reflect a lot. These large language models they cannot

323
00:35:51,360 --> 00:35:56,880
like GPT-4. It's a so-called transformer where it's just like a one-way street of information

324
00:35:56,880 --> 00:36:02,000
basically and in GeekSpeak it's called a feed-forward neural network and it's only so deep

325
00:36:03,040 --> 00:36:10,880
so it can only do logic that's that many steps and that deep. You can create problems which

326
00:36:10,880 --> 00:36:21,680
will fail to solve for that reason. The fact that it can do so amazing things with this incredibly

327
00:36:21,680 --> 00:36:28,480
simple architecture already is quite stunning and what we see in my lab at MIT when we look inside

328
00:36:29,600 --> 00:36:33,120
large language models to try to figure out how they're doing it. That's the key core

329
00:36:33,120 --> 00:36:41,440
focus of our research. It's called mechanistic interpretability in GeekSpeak. You have this

330
00:36:41,440 --> 00:36:45,120
machine that does something smart. You try to reverse engineer and see how does it do it.

331
00:36:46,000 --> 00:36:51,600
You think of it also as artificial neuroscience. That's exactly what neuroscientists do with

332
00:36:51,600 --> 00:36:55,600
actual brains but here you have the advantage that you don't have to worry about measurement

333
00:36:55,600 --> 00:37:01,520
errors. You can see what every neuron is doing all the time and a recurrent thing we see again

334
00:37:01,520 --> 00:37:08,480
and again. There's been a number of beautiful papers quite recently by a lot of researchers.

335
00:37:08,480 --> 00:37:12,800
Some of them here in this area is where when they figure out how something is done

336
00:37:13,760 --> 00:37:18,240
you can say oh man that's such a dumb way of doing it and you immediately see how it can be

337
00:37:18,240 --> 00:37:23,760
improved like for example there was a beautiful paper recently where they figured out how a large

338
00:37:23,760 --> 00:37:29,680
language model stores certain facts like Eiffel Tower is in Paris and they figured out exactly

339
00:37:29,680 --> 00:37:35,200
how it's stored and the proof that they understood it was they could edit it. They changed some of

340
00:37:35,200 --> 00:37:40,400
the synapses in it and then they asked it where is the Eiffel Tower and they said it's in Rome

341
00:37:41,280 --> 00:37:46,240
and then they asked you how do you get there? Oh how do you get there from Germany? Oh you take

342
00:37:46,240 --> 00:37:52,080
this train and the Roma Termini train station and this and that and what might you see if you're

343
00:37:52,080 --> 00:37:58,240
in front of it? Oh you might see the Colosseum. So they had edited it. So they literally moved

344
00:37:58,240 --> 00:38:05,520
it to Rome. But the way it's storing this information it's incredibly dumb for any

345
00:38:05,520 --> 00:38:12,000
fellow nerds listening to this. There was a big matrix and roughly speaking there are certain

346
00:38:12,000 --> 00:38:17,280
row and column vectors which encode these things and they correspond very hand-wavely to principal

347
00:38:17,280 --> 00:38:23,280
components and it would be much more efficient for a sparse matrix just store in the database.

348
00:38:25,440 --> 00:38:30,080
But everything so far we've figured out how these things do are ways where you can see they can

349
00:38:30,080 --> 00:38:36,640
easily be improved and the fact that this particular architecture has some roadblocks built into it

350
00:38:36,640 --> 00:38:44,080
is in no way going to prevent crafty researchers from quickly finding workarounds and making

351
00:38:45,280 --> 00:38:52,880
other kinds of architectures sort of go all the way. So in short it's turned out to be a lot

352
00:38:53,840 --> 00:38:59,600
easier to build human close to human intelligence than we thought and that means our runway as a

353
00:38:59,600 --> 00:39:08,880
species that get our shit together has shortened. And it seems like the scary thing about the

354
00:39:08,880 --> 00:39:13,760
effectiveness of large language models. So Sam Altman every standard conversation with

355
00:39:14,880 --> 00:39:22,320
and he really showed that the leap from GPT-3 to GPT-4 has to do with just a bunch of hacks.

356
00:39:23,040 --> 00:39:30,160
A bunch of little explorations with smart researchers doing a few little fixes here

357
00:39:30,160 --> 00:39:35,520
and there. It's not some fundamental leap and transformation in the architecture.

358
00:39:35,520 --> 00:39:40,160
And more data and more compute. And more data and compute but he said the big leaps has to do

359
00:39:40,880 --> 00:39:46,640
with not the data and the compute but just learning this new discipline just like you said.

360
00:39:46,640 --> 00:39:51,680
So researchers are going to look at these architectures and there might be big leaps

361
00:39:51,680 --> 00:39:56,000
where you realize why are we doing this in this dumb way. And all of a sudden this model is 10x

362
00:39:56,000 --> 00:40:02,000
smarter and that can happen on any one day on any one Tuesday or Wednesday afternoon.

363
00:40:02,000 --> 00:40:06,640
And then all of a sudden you have a system that's 10x smarter. It seems like it's such

364
00:40:06,640 --> 00:40:12,480
a new discipline. It's such a new like we understand so little about why this thing works so damn well

365
00:40:12,480 --> 00:40:17,280
that the linear improvement of compute or exponential but the steady improvement of

366
00:40:17,280 --> 00:40:21,120
compute steady improvement of the data may not be the thing that even leads to the next

367
00:40:21,120 --> 00:40:24,080
leap. It could be a surprise little hack that improves everything.

368
00:40:24,080 --> 00:40:29,600
Or a lot of little leaps here and there because so much of this is out in the open also.

369
00:40:31,600 --> 00:40:35,520
So many smart people are looking at this and trying to figure out little leaps here and there.

370
00:40:35,520 --> 00:40:41,280
And it becomes a sort of collective race where a lot of people feel if I don't take the leap

371
00:40:41,280 --> 00:40:46,080
someone else with. And it is actually very crucial for the other part of it. Why do we

372
00:40:46,080 --> 00:40:50,960
want to slow this down? So again what this open letter is calling for is just pausing

373
00:40:52,160 --> 00:40:59,120
all training of systems that are more powerful than GPT for for six months.

374
00:41:00,240 --> 00:41:08,080
Give a chance for the labs to coordinate a bit on safety and for society to adapt.

375
00:41:08,080 --> 00:41:12,640
Give the right incentives to the labs because you know you've interviewed a lot of these

376
00:41:12,640 --> 00:41:17,760
people who lead these labs and you know just as well as I do that they're good people.

377
00:41:17,760 --> 00:41:22,160
They're idealistic people. They're doing this first and foremost because they believe that

378
00:41:22,160 --> 00:41:29,920
AI has a huge potential to help humanity. But at the same time they are trapped in this

379
00:41:30,800 --> 00:41:40,800
horrible race to the bottom. Have you read Meditations on Moloch by Scott Alexander?

380
00:41:41,440 --> 00:41:46,240
Yeah it's a beautiful essay on this poem by Ginsburg where he interprets it as being about

381
00:41:46,240 --> 00:41:54,160
this monster. It's this game theory monster that pits people against each other in this

382
00:41:55,200 --> 00:41:59,600
race to the bottom where everybody ultimately loses. The evil thing about this monster is

383
00:41:59,600 --> 00:42:03,360
even though everybody sees it and understands they still can't get out of the race.

384
00:42:04,080 --> 00:42:10,480
A good fraction of all the bad things that we humans do are caused by Moloch and I like

385
00:42:11,360 --> 00:42:18,800
Scott Alexander's naming of the monster so we humans can think of it as a thing.

386
00:42:20,400 --> 00:42:24,880
If you look at why do we have overfishing, why do we have more generally the tragedy

387
00:42:24,880 --> 00:42:31,120
of the commons, why is it that so live, I don't know if you've had her on your podcast.

388
00:42:31,200 --> 00:42:38,640
Yeah she's become a friend. Great she made this awesome point recently that beauty filters that

389
00:42:38,640 --> 00:42:45,680
a lot of female influencers feel pressured to use are exactly Moloch in action again.

390
00:42:46,320 --> 00:42:52,080
First nobody was using them and people saw them just the way they were and then some of them started

391
00:42:52,080 --> 00:42:58,400
using it and becoming ever more plastic fantastic and then the other ones that weren't using it

392
00:42:58,400 --> 00:43:04,400
started to realize that if they want to just keep their their market share they have to start

393
00:43:04,400 --> 00:43:10,240
using it too and then you're in a situation where they're all using it and none of them has

394
00:43:10,240 --> 00:43:14,960
any more market share or less than before so nobody gained anything. Everybody lost

395
00:43:16,720 --> 00:43:24,160
and they have to keep becoming ever more plastic fantastic also right and but nobody can go back

396
00:43:24,160 --> 00:43:34,000
to the old way because it's just too costly right. Moloch is everywhere and Moloch is not a new

397
00:43:34,000 --> 00:43:39,280
arrival on the scene either. We humans have developed a lot of collaboration mechanisms

398
00:43:39,280 --> 00:43:44,560
to help us fight back against Moloch through various kinds of constructive collaboration.

399
00:43:45,200 --> 00:43:50,800
The Soviet Union and the United States did sign the number of arms control treaties

400
00:43:51,680 --> 00:43:58,720
against Moloch who is trying to stoke them into unnecessarily risky nuclear arms races etc etc

401
00:43:58,720 --> 00:44:05,280
and this is exactly what's happening on the AI front. This time it's a little bit geopolitics

402
00:44:05,280 --> 00:44:09,680
but it's mostly money where there's just so much commercial pressure. If you take any of these

403
00:44:10,720 --> 00:44:17,280
leaders of the top tech companies if they just say this is too risky I want to pause

404
00:44:18,080 --> 00:44:22,080
for six months they're going to get a lot of pressure from shareholders and others.

405
00:44:23,440 --> 00:44:30,400
They're like well you know if you pause but those guys don't pause. We don't want to get

406
00:44:30,400 --> 00:44:37,040
our lunch eaten and shareholders even have the power to replace the executives in the worst case

407
00:44:37,040 --> 00:44:44,880
right so we did this open letter because we want to help these idealistic tech executives to do

408
00:44:45,840 --> 00:44:51,840
what their heart tells them by providing enough public pressure on the whole sector to just pause

409
00:44:51,840 --> 00:44:57,280
so that they can all pause in a coordinated fashion and I think without the public pressure

410
00:44:57,280 --> 00:45:04,560
none of them can do it alone push back against their shareholders no matter how good-hearted they

411
00:45:04,560 --> 00:45:14,480
are. Moloch is a really powerful foe. So the idea is to for the major developers of AI systems

412
00:45:14,480 --> 00:45:23,520
like this so we're talking about Microsoft, Google, Meta and anyone else? Open AI is very close with

413
00:45:23,520 --> 00:45:31,200
Microsoft now of course and there are plenty of smaller players for example Anthropic which is

414
00:45:31,200 --> 00:45:35,600
very impressive there's Conjecture there's many many many players I don't want to make a long

415
00:45:35,600 --> 00:45:43,760
list so leave anyone out and for that reason it's so important that some coordination happens

416
00:45:44,480 --> 00:45:49,920
that there's external pressure on all of them saying you all need to pause because then the

417
00:45:49,920 --> 00:45:54,880
people the researchers in these organizations the leaders who want to slow down a little bit they

418
00:45:54,880 --> 00:46:01,840
can say their shareholders you know everybody's slowing down because of this pressure and it's

419
00:46:01,840 --> 00:46:08,720
the right thing to do. Have you seen in history their examples where it's possible to pause the

420
00:46:09,120 --> 00:46:14,640
absolutely and even like human cloning for example you could make so much money on human cloning

421
00:46:18,320 --> 00:46:25,360
why aren't we doing it? Because biologists thought hard about this and felt like this is

422
00:46:26,240 --> 00:46:31,840
way too risky they got together in the 70s in the Selomar and decided even

423
00:46:32,800 --> 00:46:35,920
to stop a lot more stuff also just editing the human germline

424
00:46:37,600 --> 00:46:45,040
gene editing that goes into our offspring and decided let's not do this because

425
00:46:45,600 --> 00:46:51,040
it's too unpredictable what it's going to lead to we could lose control over what happens to our

426
00:46:51,040 --> 00:46:58,160
species so they paused there was a ton of money to be made there so it's very doable but you just

427
00:46:58,160 --> 00:47:04,480
need you need a public awareness of what the risks are and the broader community coming in and saying

428
00:47:05,040 --> 00:47:11,760
hey let's slow down and you know another common pushback I get today is we can't stop in the west

429
00:47:12,480 --> 00:47:20,240
because China and in China undoubtedly they also get told we can't slow down because the west

430
00:47:20,240 --> 00:47:25,440
because both sides think they're the good guy but look at human cloning you know

431
00:47:26,400 --> 00:47:30,800
did China forge ahead with human cloning there's been exactly one human cloning that's actually

432
00:47:30,800 --> 00:47:36,480
been done that I know of it was done by a Chinese guy do you know where he is now in jail

433
00:47:37,840 --> 00:47:44,640
and you know who put him there who Chinese government not because westerners said China

434
00:47:44,640 --> 00:47:48,000
looked this is no the Chinese government put him there because they also felt

435
00:47:48,960 --> 00:47:53,920
they like control the Chinese government if anything maybe they are even more concerned

436
00:47:53,920 --> 00:47:59,840
about having control then the western governments have no incentive of just losing control over

437
00:47:59,840 --> 00:48:05,760
where everything is going and you can also see the Ernie bot that was released by I believe Baidu

438
00:48:05,760 --> 00:48:10,720
recently they got a lot of pushback from the government and had to reign it in you know in a

439
00:48:10,720 --> 00:48:17,440
big way I think once this basic message comes out that this isn't an arms race it's a suicide race

440
00:48:17,440 --> 00:48:25,200
where everybody loses if anybody's AI goes out of control it really changes the whole dynamic

441
00:48:25,760 --> 00:48:32,800
it's not it's I'll say this again because this is a very basic point I think a lot of people get

442
00:48:32,800 --> 00:48:41,840
wrong because a lot of people dismiss the whole idea that AI can really get very superhuman

443
00:48:41,840 --> 00:48:46,080
because they think there's something really magical about intelligence such that it can only exist

444
00:48:46,160 --> 00:48:50,240
in human minds you know because they believe that they think it's kind of kind of get to just more or

445
00:48:50,240 --> 00:48:58,640
less GPT-4 plus plus and then that's it they don't see it as a super as a suicide race they think

446
00:48:58,640 --> 00:49:03,040
whoever gets that first they're going to control the world they're going to win but that's not how

447
00:49:03,040 --> 00:49:09,040
it's going to be and we can talk again about the scientific arguments from why it's not going to

448
00:49:09,040 --> 00:49:15,520
stop there but the way it's going to be is if anybody completely loses control and you know

449
00:49:15,520 --> 00:49:23,120
you don't care if someone manages to take over the world who really doesn't share your goals

450
00:49:24,000 --> 00:49:27,520
you probably don't really even care very much about what nationality they have you're not going

451
00:49:27,520 --> 00:49:34,960
to like it much worse than today if you live in Orwellia in dystopia what do you care who

452
00:49:34,960 --> 00:49:42,960
created it right and if it goes farther and we just lose control even to the machines

453
00:49:42,960 --> 00:49:52,320
so that it's not us versus them it's us versus it what do you care who who created this this

454
00:49:52,320 --> 00:49:58,400
unaligned entity which has goals different from humans ultimately and we get marginalized we get

455
00:49:58,400 --> 00:50:04,960
made obsolete we get replaced that's why I what I mean when I say it's a suicide race

456
00:50:06,000 --> 00:50:11,760
it's kind of like we're rushing towards this cliff but the closer the cliff we get the more

457
00:50:11,840 --> 00:50:17,280
scenic the views are and the more money there is there and so we keep going but we have to also

458
00:50:17,280 --> 00:50:30,000
stop at some point right quit while we're ahead and uh it's um it's a suicide race which cannot be

459
00:50:30,000 --> 00:50:38,320
won but the way to really benefit from it is to continue developing awesome AI a little bit slower

460
00:50:38,960 --> 00:50:43,520
so we make it safe make sure it does the things that humans want and create a condition where

461
00:50:43,520 --> 00:50:51,680
everybody wins the technology has shown us that you know geopolitics and and politics in general

462
00:50:51,680 --> 00:50:56,800
is not a zero-sum game at all so there is some rate of development that will lead

463
00:50:58,960 --> 00:51:04,480
us as a human species to lose control of this thing and the hope you have is that there's some

464
00:51:04,480 --> 00:51:10,240
lower level of development which will not which will not allow us to lose control this is an

465
00:51:10,240 --> 00:51:14,240
interesting thought you have about losing control so what if you have somebody if you are somebody

466
00:51:14,240 --> 00:51:20,240
like santa prachai or sam altman at the head of a company like this you're saying if they develop an

467
00:51:20,240 --> 00:51:27,760
agi they too will lose control of it so no one person can maintain control no group of individuals

468
00:51:27,760 --> 00:51:33,840
can maintain control if it's if it's created very very soon and is a big black box that we

469
00:51:33,920 --> 00:51:38,400
don't understand like the large language models yeah then i'm very confident they're going to lose

470
00:51:38,400 --> 00:51:43,680
control but this isn't just me saying it you know sam altman and them as the sabbis have both said

471
00:51:44,880 --> 00:51:49,360
themselves acknowledge that you know there's really great risks with this and they they want to slow

472
00:51:49,360 --> 00:51:54,960
down once they feel it gets it's scary it's but it's clear that they're stuck in this again malloc

473
00:51:54,960 --> 00:52:00,480
is forcing them to go a little faster than they're comfortable with because of pressure from just

474
00:52:00,560 --> 00:52:06,640
commercial pressures right to get a bit optimistic here you know of course this is a problem that

475
00:52:06,640 --> 00:52:15,200
can be ultimately solved uh it's just to win this wisdom race it's clear that what we hope that

476
00:52:15,200 --> 00:52:20,560
is going to happen hasn't happened the the capability progress has gone faster than a lot of people

477
00:52:20,560 --> 00:52:25,680
thought then and the the progress in in the public sphere of policy making and so on has gone slower

478
00:52:25,680 --> 00:52:31,120
than we thought even the technical ai safety has gone slower a lot of the technical safety research

479
00:52:31,120 --> 00:52:36,400
was kind of banking on that um large language models and other poorly understood systems couldn't

480
00:52:36,400 --> 00:52:40,560
get us all the way that you had to build more of a kind of intelligence that you could understand

481
00:52:41,120 --> 00:52:49,120
maybe it could prove itself safe you know things like this and um i'm quite confident that this

482
00:52:49,120 --> 00:52:56,560
can be done um so we can reap all the benefits but we cannot do it as quickly as uh this out of

483
00:52:56,560 --> 00:53:01,920
control express train we are on now is going to get the agi that's why we need a little more time

484
00:53:01,920 --> 00:53:08,960
i feel is there something to be said what like sam alman talked about which is while we're in the

485
00:53:08,960 --> 00:53:19,040
pre agi stage to release often and as transparently as possible to learn a lot so as opposed to being

486
00:53:19,040 --> 00:53:25,920
extremely cautious release a lot don't uh don't invest in a closed development where you focus on

487
00:53:25,920 --> 00:53:34,000
ai safety while it's somewhat dumb quote unquote uh release as often as possible and as you start

488
00:53:34,000 --> 00:53:41,360
to see signs of uh human level intelligence or superhuman level intelligence then you put a halt

489
00:53:41,360 --> 00:53:47,200
on it well what a lot of safety researchers have been saying for many years is that the most dangerous

490
00:53:47,200 --> 00:53:53,120
things you can do with an ai is first of all teach it to write code yeah because that's the

491
00:53:53,120 --> 00:53:57,840
first step towards recursive self-improvement which can take it from agi to much higher levels

492
00:53:58,480 --> 00:54:06,240
okay oops we've done that and uh another thing high risk is connected to the internet let it go

493
00:54:06,240 --> 00:54:12,560
to websites download stuff on its own and talk to people oops we've done that already you know

494
00:54:12,640 --> 00:54:16,800
eliezer yukowski you said you interviewed him recently right yes he had this tweet recently

495
00:54:16,800 --> 00:54:22,480
which gave me one of the best laughs in a while where he's like hey people used to make fun of me

496
00:54:22,480 --> 00:54:28,320
and say you're so stupid eliezer because you're saying you're saying uh you have to worry of

497
00:54:28,320 --> 00:54:33,920
obviously developers once they get to like really strong ai the first thing you're gonna do is like

498
00:54:33,920 --> 00:54:39,200
never connect it to the internet keep it in the box yeah where you know you can really study it

499
00:54:39,760 --> 00:54:46,560
it's if so he had written it in the like in the meme forms was like then yeah and then that and then

500
00:54:46,560 --> 00:54:56,640
now let's lol let's make a chatbot yeah and the third thing is steward russell yeah you know

501
00:54:58,560 --> 00:55:07,440
amazing ai researcher he has argued for a while that we should never teach ai anything about humans

502
00:55:08,240 --> 00:55:12,720
above all we should never let it learn about human psychology and how you manipulate humans

503
00:55:13,840 --> 00:55:17,680
that's the most dangerous kind of knowledge you can give it yeah you can teach it all it needs to

504
00:55:17,680 --> 00:55:23,040
know how to about how to cure cancer and stuff like that but don't let it read daniel kahneman's

505
00:55:23,040 --> 00:55:29,840
book about cognitive biases and all that and then oops lol you know let's invent social media

506
00:55:30,640 --> 00:55:37,760
I'll recommend our algorithms which do exactly that they they look get so good at knowing us

507
00:55:38,320 --> 00:55:43,840
and pressing our buttons that we've we're starting to create a world now where we're just

508
00:55:43,840 --> 00:55:50,000
having ever more hatred because they figured out that these algorithms not for out of evil but

509
00:55:50,000 --> 00:55:56,480
just to make money on advertising that the best way to get more engagement the euphemism get people

510
00:55:56,480 --> 00:56:00,960
glued to their little rectangles right is just to make them pissed off that's really interesting

511
00:56:00,960 --> 00:56:08,400
that a large ai system that's doing the recommender system kind of tasks on social media is basically

512
00:56:08,400 --> 00:56:16,080
just studying human beings because it's a bunch of us rats giving it signal nonstop signal it'll

513
00:56:16,080 --> 00:56:21,200
show a thing and then we give signal and whether we spread that thing we like that thing that thing

514
00:56:21,200 --> 00:56:25,600
increases our engagement gets us to return to the platform it has that on the scale of hundreds

515
00:56:25,600 --> 00:56:30,400
of millions of people constantly so it's just learning and learning and learning and presumably

516
00:56:30,400 --> 00:56:35,120
if the parameter the number of parameters in your network that's doing the learning and more

517
00:56:35,120 --> 00:56:42,960
and to end the learning is the more it's able to just basically encode how to manipulate human

518
00:56:42,960 --> 00:56:48,080
behavior how to control humans at scale exactly and that is not something I think is in humanity's

519
00:56:48,080 --> 00:56:53,920
interest yes right now it's mainly letting some humans manipulate other humans for profit

520
00:56:55,840 --> 00:57:04,480
and power which already caused a lot of damage and eventually that's a sort of skill that can

521
00:57:04,480 --> 00:57:10,880
make ai's persuade humans to let them escape whatever safety precautions be it but you know

522
00:57:10,880 --> 00:57:16,960
there was a really nice article in the New York Times recently by Yuval Noah Harari and

523
00:57:17,520 --> 00:57:23,440
two co-authors including Tristan Harris from the social dilemma and they have this phrase in there

524
00:57:23,440 --> 00:57:32,560
I love he said humanity's first contact with advanced AI was social media and we lost that one

525
00:57:33,680 --> 00:57:39,760
we now live in a country where there's much more hate in the world where there's much more hate in

526
00:57:39,760 --> 00:57:45,440
fact and in our democracy that we're having this conversation and people can't even agree on who

527
00:57:45,440 --> 00:57:50,880
won the last election you know and we humans often point fingers at other humans and say it's

528
00:57:50,880 --> 00:57:58,640
their fault but it's really malloc and these AI algorithms we got the algorithms and then malloc

529
00:57:59,760 --> 00:58:03,600
pitted the social media companies are against each other so nobody could have a less creepy

530
00:58:03,600 --> 00:58:07,680
algorithm because then they would lose out on revenue to the other company is there any way to

531
00:58:07,680 --> 00:58:13,200
win that battle back just if we just linger on this one battle that we've lost in terms of social

532
00:58:13,200 --> 00:58:20,800
media is it possible to redesign social media this very medium in which we use as a civilization

533
00:58:20,800 --> 00:58:25,120
to communicate with each other to have these kinds of conversation to have discourse to try

534
00:58:25,120 --> 00:58:30,480
to figure out how to solve the biggest problems in the world whether that's nuclear war or the

535
00:58:30,480 --> 00:58:36,800
development of AGI is is it possible to do social media correctly I think it's not only possible

536
00:58:36,800 --> 00:58:41,440
but it's it's necessary who are we kidding that we're going to be able to solve all these other

537
00:58:41,440 --> 00:58:46,160
challenges if we can't even have a conversation with each other that's constructive the whole idea

538
00:58:46,160 --> 00:58:51,040
the key idea of democracy is that you get a bunch of people together and they have a real

539
00:58:51,040 --> 00:58:55,440
conversation the ones you try to foster on this podcast or you respectfully listen to people you

540
00:58:55,440 --> 00:59:00,560
disagree with and you realize actually you know there are some things actually we some common ground

541
00:59:00,560 --> 00:59:07,280
we have and let's it's yeah we both agree let's not have nuclear wars let's not do that and etc etc

542
00:59:07,760 --> 00:59:15,120
we're kidding ourselves the thinking we can face off the second contact with

543
00:59:15,120 --> 00:59:19,760
whatever more powerful AI that's happening now with these large language models if we can't even

544
00:59:21,200 --> 00:59:27,200
have a functional conversation in the public space that's why I started the improve the news

545
00:59:27,200 --> 00:59:33,680
project improve the news.org but I'm an optimist fundamentally in

546
00:59:36,400 --> 00:59:44,400
and that there is a lot of intrinsic goodness in people and that what makes the difference

547
00:59:45,040 --> 00:59:51,440
between someone doing good things for humanity and bad things is not some sort of fairy tale thing

548
00:59:51,440 --> 00:59:56,080
that this person was born with the evil gene and this one was not born with a good gene no

549
00:59:56,080 --> 01:00:02,800
I think it's whether we put whether people find themselves in situations that bring out the best

550
01:00:02,800 --> 01:00:10,160
in them or the bring out the worst in them and I feel we're building an internet and a society that

551
01:00:10,800 --> 01:00:18,160
brings out the worst but it doesn't have to be that way no it does not it's possible to create

552
01:00:18,160 --> 01:00:24,000
incentives and also create incentives that make money uh they both make money and bring out the

553
01:00:24,000 --> 01:00:28,000
best in people I mean in the long term it's not a good investment for anyone you know it's have a

554
01:00:28,000 --> 01:00:33,440
nuclear war for example and you know is it a good investment for humanity if we just ultimately

555
01:00:33,440 --> 01:00:39,360
replace all humans by machines and then we're so obsolete that eventually there's no humans left

556
01:00:40,400 --> 01:00:46,560
well depends against how you do the math but I would say by any reasonable economic standard

557
01:00:46,560 --> 01:00:50,720
if you look at the future income of humans and there aren't any you know that's not a good investment

558
01:00:50,800 --> 01:00:58,480
moreover like why why can't we have a little bit of pride in our species damn it you know why should

559
01:00:58,480 --> 01:01:06,240
we just build another species that gets rid of us if we were neanderthals would we really consider

560
01:01:06,240 --> 01:01:13,200
it a smart move if if we had really advanced biotech to build homo sapiens you you know you

561
01:01:13,200 --> 01:01:19,520
might say hey max you know yeah let's let's build build these homo sapiens they're going to be smarter

562
01:01:19,520 --> 01:01:24,320
than us maybe they can help us defend this better against predators and help fix their

563
01:01:24,320 --> 01:01:30,000
bar caves make them nicer we'll control them undoubtedly you know so then they build build

564
01:01:30,000 --> 01:01:37,200
a couple a little baby girl a little baby boy they either and and then you have some some

565
01:01:37,200 --> 01:01:44,400
wise old and the underthrall elder was like hmm I'm scared that uh we're opening a pandora's box

566
01:01:44,400 --> 01:01:51,760
here and that we're going to get outsmarted by these super neanderthal intelligences

567
01:01:52,400 --> 01:01:56,640
and there won't be any neanderthals left but then you have a bunch of others in the cave

568
01:01:56,640 --> 01:02:00,800
right you are you such a luddite scare monger of course they're going to want to keep us around

569
01:02:00,800 --> 01:02:05,360
because we are their creators and and why you know the smarter I think the smarter they get the

570
01:02:05,360 --> 01:02:09,760
nicer they're going to get they're going to leave us they're going to they're going to want this around

571
01:02:09,840 --> 01:02:14,720
and that's going to be fine and and besides look at these babies they're so cute it's clearly

572
01:02:14,720 --> 01:02:21,200
they're totally harmless that's exact those babies are exactly GPT-4 yeah it's not I want to be clear

573
01:02:21,200 --> 01:02:29,760
it's not GPT-4 that's terrifying it's the GPT-4 is a baby technology you know and Microsoft even

574
01:02:29,760 --> 01:02:37,520
had a paper recently out uh with a title something like sparkles of AGI whatever basically saying

575
01:02:37,520 --> 01:02:45,440
this is baby AI like these little neanderthal babies and it's going to grow up there's going to be

576
01:02:45,440 --> 01:02:50,880
other systems from from the same company from other companies they'll be way more powerful and

577
01:02:50,880 --> 01:02:57,600
but they're going to take all the things ideas from these babies and before we know it we're

578
01:02:57,600 --> 01:03:04,080
going to be like those last neanderthals who are pretty disappointed and when they realized

579
01:03:04,160 --> 01:03:09,280
they were getting replaced well this interesting point you make which is the programming it's

580
01:03:09,280 --> 01:03:15,680
entirely possible that GPT-4 is already the kind of system that can change everything

581
01:03:16,800 --> 01:03:24,400
by writing programs life three it's yeah it's because it's life 2.0 the systems I'm afraid of

582
01:03:24,400 --> 01:03:30,880
are going to look nothing like a large language model and they're not but once it gets once

583
01:03:30,880 --> 01:03:36,080
it or other people figure out a way of using this tech to make much better tech right it's just

584
01:03:36,080 --> 01:03:41,920
constantly replacing its software and from everything we've seen about how how these work

585
01:03:41,920 --> 01:03:47,360
under the hood they're like the minimum viable intelligence they do everything in a dumbest

586
01:03:47,360 --> 01:03:54,800
way that still works sort of yeah and um so they are life 3.0 except when they replace their software

587
01:03:55,360 --> 01:03:58,800
it's a lot faster than when you when when you decide to learn swedish

588
01:04:00,880 --> 01:04:11,120
and moreover they think a lot faster than us too so when uh you know we don't think on how one

589
01:04:12,720 --> 01:04:20,320
logical step every nanosecond or few or so the way they do and we can't also just suddenly scale

590
01:04:20,320 --> 01:04:28,960
up our hardware massively in the cloud which is so limited right uh so they are they are also life

591
01:04:30,080 --> 01:04:36,320
to have consumed become a little bit more like life 3.0 in that if they need more hardware hey

592
01:04:36,320 --> 01:04:40,640
just rent it in the cloud you know how do you pay for it well with all the services you provide

593
01:04:41,040 --> 01:04:51,200
and what we haven't seen yet which could change a lot is uh entire

594
01:04:52,880 --> 01:04:59,840
software system so right now programming is done sort of in bits and pieces uh as as an assistant

595
01:04:59,840 --> 01:05:05,680
tool to humans but i do a lot of programming and with the kind of stuff that gbt4 is able to do i

596
01:05:05,680 --> 01:05:11,120
mean is replacing a lot what i'm able to do but i you still need a human in the loop to kind of

597
01:05:12,000 --> 01:05:17,200
manage the design of things manage like what are the prompts that generate the kind of stuff to

598
01:05:17,200 --> 01:05:24,400
to do some basic adjustment of the code to do some debugging but if it's possible to add on top of

599
01:05:24,400 --> 01:05:34,000
gbt4 kind of uh feedback loop of of uh self-debugging improving the code and then you launch that

600
01:05:34,000 --> 01:05:38,400
system onto the wild on the internet because everything is connected and have it do things

601
01:05:38,400 --> 01:05:43,680
have it interact with humans and then get that feedback and now you have this giant ecosystem

602
01:05:43,680 --> 01:05:49,760
yeah of humans that's one of the things that um yeah elon musk recently sort of tweeted as a case

603
01:05:49,760 --> 01:05:53,920
why everyone needs to pay seven dollars or whatever for twitter to make sure they're real

604
01:05:54,480 --> 01:05:59,200
they're make sure they're real we're now going to be living in a world where the the bots are

605
01:05:59,200 --> 01:06:05,440
getting smarter and smarter and smarter to a degree where you can't uh you can't tell the

606
01:06:05,440 --> 01:06:11,280
difference between a human and a bot that's right and now you can have uh bots outnumber humans by

607
01:06:11,280 --> 01:06:17,280
a one million to one which is why he's making a case why you have to pay to prove you're human

608
01:06:17,280 --> 01:06:24,080
which is one of the only mechanisms to which is depressing and i yeah i feel we have to remember

609
01:06:24,800 --> 01:06:29,760
as individuals we should from time to time ask ourselves why are we doing what we're doing

610
01:06:29,760 --> 01:06:36,720
all right then as a species we need to do that too so if we're building as as you say machines

611
01:06:36,720 --> 01:06:42,720
that are outnumbering us and more and more outsmarting us and replacing us on the job

612
01:06:42,720 --> 01:06:49,520
market not just for the dangerous and then boring tasks but also for writing poems and doing art

613
01:06:49,520 --> 01:06:54,240
and things that a lot of people find really meaningful gotta ask ourselves why why are we

614
01:06:54,240 --> 01:07:02,800
doing this uh we are the answer is malloc is tricking us into doing it and it's such a clever

615
01:07:02,800 --> 01:07:07,120
trick that even though we see the trick we still have no choice but to fall for it right

616
01:07:09,440 --> 01:07:17,120
come also the thing you said about you using uh co-pilot ai tools to program faster how many

617
01:07:17,200 --> 01:07:24,400
what factor faster would you say you code now does it go twice as fast or i don't really uh

618
01:07:24,400 --> 01:07:29,200
because it's a new tool yeah it's i don't know if speed is significantly improved

619
01:07:30,400 --> 01:07:38,240
but it feels like i'm a year away from being uh five to ten times faster so if that's typical

620
01:07:38,240 --> 01:07:45,280
for programmers then uh you're already seeing another kind of self recursive self-improvement

621
01:07:45,280 --> 01:07:51,200
right because previously one like a major generation of improvement of the codes

622
01:07:51,200 --> 01:07:55,200
would happen on the human r&d timescale and now if that's five times shorter

623
01:07:56,320 --> 01:08:00,240
then it's going to take five times less time than otherwise would to develop the next level

624
01:08:00,240 --> 01:08:07,200
of these tools and so on so this these these are the this is exactly the sort of beginning of an

625
01:08:07,200 --> 01:08:11,920
of an intelligence explosion there can be humans in the loop a lot in the early stages and then

626
01:08:11,920 --> 01:08:16,880
eventually humans are needed less and less and the machines can more kind of go along but you

627
01:08:16,880 --> 01:08:21,520
what you weren't you said there is just an exact example of these sort of things another thing which

628
01:08:21,520 --> 01:08:29,920
which um i was kind of lying on my psychiatrist imagining i'm on a psychiatrist couch here saying

629
01:08:29,920 --> 01:08:36,000
what are my fears that people would do with um ai systems another so i mentioned three that i had

630
01:08:36,000 --> 01:08:42,240
fears about many years ago that they would do uh namely uh teach it the code uh connected to the

631
01:08:42,240 --> 01:08:50,960
internet and teach it to manipulate humans a fourth one is building an api where code can

632
01:08:50,960 --> 01:08:57,760
control the super powerful thing right that's very unfortunate because one thing that systems like

633
01:08:57,760 --> 01:09:03,440
gpt4 have going for them is that they are an oracle in the sense that they just answer questions

634
01:09:04,320 --> 01:09:09,840
there is no robot connected to the gpt4 gpt4 can't go and do stock trading based on its thinking

635
01:09:10,560 --> 01:09:15,520
it's not an agent and an intelligent agent is something that takes in information from the

636
01:09:15,520 --> 01:09:22,640
world processes it to figure out what action to take based on its goals that it has and then

637
01:09:23,600 --> 01:09:30,560
does something back on the world but once you have an api for example gpt4 nothing stops

638
01:09:30,560 --> 01:09:37,920
joe schmo and a lot of other people from building real agents which just keep making calls somewhere

639
01:09:37,920 --> 01:09:44,720
and some inner loop somewhere to these powerful oracle systems and which makes them themselves

640
01:09:44,720 --> 01:09:50,000
much more powerful that's another kind of um unfortunate development which i think we would

641
01:09:50,000 --> 01:09:55,520
have been better off uh delaying i don't want to pick on any particular companies i think

642
01:09:55,520 --> 01:10:04,160
they're all under a lot of pressure to make money yeah and um again we the reason we're calling for

643
01:10:04,160 --> 01:10:09,680
this pause is to give them all cover to do what they know is the right thing slow down a little bit

644
01:10:09,680 --> 01:10:16,800
at this point but everything we've talked about i hope we'll can we'll make it clear to people

645
01:10:16,800 --> 01:10:23,520
watching this you know why these sort of human level tools can cause a gradual acceleration

646
01:10:23,520 --> 01:10:29,520
you keep using yesterday's technology to build tomorrow's technology yeah and when you do that

647
01:10:29,520 --> 01:10:33,920
over and over again you naturally get an explosion you know that's the definition of an explosion

648
01:10:33,920 --> 01:10:44,160
in science right like if you have um two people um they fall in love now you have four people and

649
01:10:44,160 --> 01:10:50,480
then they can make more babies and now you have eight people and then then you have 16 32 64 etc

650
01:10:50,800 --> 01:10:57,040
we call that a population explosion where it's just that each if it's instead free neutrons

651
01:10:57,840 --> 01:11:03,280
in a nuclear reaction that if each one can make more than one then you get an exponential growth

652
01:11:03,280 --> 01:11:07,520
in that we call it a nuclear explosion all explosions are like that an intelligence

653
01:11:07,520 --> 01:11:11,200
explosion it's just exactly the same principle that some quantities some amount of intelligence

654
01:11:11,200 --> 01:11:16,720
can make more intelligence than that and then repeat you always get the exponentials

655
01:11:17,680 --> 01:11:22,240
what's your intuition why does you mention there's some technical reasons why it doesn't stop

656
01:11:22,240 --> 01:11:27,440
at a certain point what's your intuition and uh do you have any intuition why it might stop

657
01:11:28,240 --> 01:11:30,880
it's obviously going to stop when it bumps up against the laws of physics

658
01:11:31,600 --> 01:11:34,960
there are some things you just can't do no matter how smart you are allegedly

659
01:11:37,200 --> 01:11:42,240
and because we don't know the full laws of physics yeah like Seth Lloyd wrote a really cool

660
01:11:42,240 --> 01:11:48,640
paper on the physical limits on computation for example if you make it put too much energy into

661
01:11:48,640 --> 01:11:53,600
it and the finite space it'll turn into a black hole you can't move information around fast and

662
01:11:53,600 --> 01:12:01,120
the speed of light stuff like that but uh it's hard to store way more than than a modest number

663
01:12:01,120 --> 01:12:07,520
of bits per atom etc but you know those limits are just astronomically above like 30 orders of

664
01:12:07,520 --> 01:12:14,960
magnitude above where we are now so bigger different bigger jump in intelligence than if

665
01:12:14,960 --> 01:12:25,120
you go from a from an ant to a human I think of course what we want to do is have a controlled

666
01:12:26,080 --> 01:12:31,200
thing a nuclear reactor you put moderators in to make sure exactly it doesn't blow up out of control

667
01:12:31,200 --> 01:12:39,040
right when we do experiments with biology and cells and so on you know we also try to make

668
01:12:39,040 --> 01:12:46,400
sure it doesn't get out of control um we can do this with AI too the thing is we haven't succeeded

669
01:12:46,400 --> 01:12:55,200
yet and malloc is exactly doing the opposite just fueling just egging everybody on faster faster

670
01:12:55,200 --> 01:12:59,600
faster or the other company is going to catch up with you or the other country is going to catch up

671
01:12:59,600 --> 01:13:07,280
with you we do this we have to want to stop we have and and I don't believe in this just asking

672
01:13:07,920 --> 01:13:12,880
people to look into their hearts and do the right thing it's easier for others to say that but like

673
01:13:12,880 --> 01:13:18,880
if if you're in the situation where your company is going to get screwed if you

674
01:13:21,440 --> 01:13:25,440
by other companies they're not stopping you know you're putting people in a very hard situation

675
01:13:26,000 --> 01:13:28,800
the right thing to do is change the whole incentive structure instead

676
01:13:29,760 --> 01:13:34,960
and and this is not an old maybe I should say one more thing about this because malloc

677
01:13:35,680 --> 01:13:42,240
has been around as humanity's number one or number two enemy since the beginning of civilization

678
01:13:42,240 --> 01:13:48,800
and we came up with some really cool countermeasures like first of all already over a hundred thousand

679
01:13:48,800 --> 01:13:54,400
years ago evolution realized that it was very unhelpful that people kept killing each other all

680
01:13:54,400 --> 01:14:03,120
the time yeah so it genetically gave us compassion and made it so that it like if you get too drunk

681
01:14:03,120 --> 01:14:10,160
dudes getting into a pointless bar fight they might give each other black eyes but they have a lot of

682
01:14:10,160 --> 01:14:17,920
inhibition towards just killing each other that's a and similarly if you find a baby lying on the

683
01:14:17,920 --> 01:14:22,320
street when you go out for your morning jog tomorrow you're gonna stop and pick it up right

684
01:14:22,320 --> 01:14:28,560
even though it may be a make you late for your next podcast so evolution gave us these genes

685
01:14:28,560 --> 01:14:34,240
that make our own egoistic incentives more aligned with what's good for the greater group

686
01:14:34,240 --> 01:14:41,040
we're part of right and then as we got a bit more sophisticated and developed language

687
01:14:42,240 --> 01:14:47,600
we invented gossip which is also a fantastic anti malloc right because now

688
01:14:48,160 --> 01:14:56,880
it it's really discourages liars moochers cheaters because it their own incentive now is not to do

689
01:14:56,880 --> 01:15:02,000
this because word quickly gets around and then suddenly people aren't going to invite them to

690
01:15:02,000 --> 01:15:07,040
their dinners anymore and or trust them and then when we got still more sophisticated and bigger

691
01:15:07,040 --> 01:15:13,200
societies you know invented the legal system where even strangers who didn't couldn't rely on gossip

692
01:15:14,160 --> 01:15:17,760
and and things like this would treat each other would have an incentive

693
01:15:17,760 --> 01:15:21,360
now those guys in the bar fights even if they someone is so drunk that he

694
01:15:22,960 --> 01:15:28,080
actually wants to kill the other guy he also has a little thought in the back of his head that you

695
01:15:28,080 --> 01:15:34,320
know do i really want to spend the next 10 years eating like really crappy food in a small room

696
01:15:36,720 --> 01:15:40,720
i'm just gonna chill out you know so and we we similarly have tried to give these incentives

697
01:15:40,800 --> 01:15:47,200
to our corporations by having having regulation and all sorts of oversight so that their incentives

698
01:15:47,200 --> 01:15:54,160
are aligned with the greater good we tried really hard and the big problem that we're failing now

699
01:15:55,600 --> 01:15:59,120
is not that we haven't tried before but it's just that the tech is growing much

700
01:16:00,000 --> 01:16:03,600
is developing much faster than the regulators been able to keep up right so

701
01:16:04,880 --> 01:16:10,400
regulators it's kind of comical that european union right now is doing this AI act right which

702
01:16:11,040 --> 01:16:18,560
and in the beginning they had a little opt-out exception that gpt4 would be completely excluded

703
01:16:18,560 --> 01:16:26,560
from regulation brilliant idea what's the logic behind that some lobbyists pushed successfully

704
01:16:26,560 --> 01:16:31,600
for this so we were actually quite involved with the future life institutes um mark brackel

705
01:16:32,160 --> 01:16:37,520
mr uke anthony agir and others you know we're quite involved with um talking to very educating

706
01:16:37,520 --> 01:16:43,760
various people involved in this process about these general purpose AI models coming and pointing

707
01:16:43,760 --> 01:16:48,240
out that they would become the laughing stock if they didn't put it in so it the french started

708
01:16:48,240 --> 01:16:53,360
pushing for it it got put in to the draft and it looked like all was good and then there was a huge

709
01:16:54,560 --> 01:16:59,680
counterpush from lobbyists yeah there were more lobbyists sent in brussels from tech companies and

710
01:16:59,680 --> 01:17:05,600
from oil companies for example and it looked like it might as we can maybe get taken out again

711
01:17:06,560 --> 01:17:13,200
and now gpt4 happened and i think it's going to stay in but this just shows you know malloc

712
01:17:13,200 --> 01:17:20,640
can be defeated but the the challenge you're facing is that the tech is generally much faster than

713
01:17:20,640 --> 01:17:28,480
what the policy makers are and a lot of the policy makers also don't have a tech background so it's

714
01:17:29,200 --> 01:17:35,200
you know we really need to work hard to educate them on how on what's taking place here so so

715
01:17:35,200 --> 01:17:40,720
we're getting the situation where the first kind of non so you know i define artificial

716
01:17:40,720 --> 01:17:48,320
intelligence just as non-biological intelligence all right and by that definition a company a

717
01:17:48,320 --> 01:17:52,640
corporation is also an artificial intelligence because the corporation isn't it's humans it's a

718
01:17:52,640 --> 01:17:59,840
system if its CEO decides the CEO of a tobacco company decides one morning the CEO he doesn't

719
01:17:59,840 --> 01:18:05,200
want to sell cigarettes anymore they'll just put another CEO in there it's not enough to align

720
01:18:05,920 --> 01:18:12,800
the incentives of individual people or align individual computers incentives to their owners

721
01:18:12,800 --> 01:18:17,280
which is what technically iSafety research is about you also have to align the incentives of

722
01:18:17,280 --> 01:18:23,040
corporations with a greater good and some corporations have gotten so big and so powerful

723
01:18:23,040 --> 01:18:31,200
very quickly that in many cases their lobbyists instead align the regulators to what they want

724
01:18:31,200 --> 01:18:37,840
rather than the early way around the classic regulatory capture all right is is the thing

725
01:18:37,840 --> 01:18:44,240
that the slowdown hopes to achieve is give enough time to the regulators to catch up or enough time

726
01:18:44,240 --> 01:18:49,440
to the companies themselves to breathe and understand how to do AI safety correctly i think

727
01:18:49,440 --> 01:18:55,200
both and but i think that the vision path to success i see is first you give a breather

728
01:18:55,200 --> 01:19:00,240
actually to to the people in these companies their leadership who wants to do the right thing and

729
01:19:00,240 --> 01:19:06,080
they all have safety teams and so on on their companies give them a chance to get together

730
01:19:06,080 --> 01:19:14,080
with the other companies and the outside pressure can also help catalyze that right and and work out

731
01:19:14,720 --> 01:19:22,000
what is it that's what are the reasonable safety requirements one should put on future systems

732
01:19:22,000 --> 01:19:27,600
before they get rolled out there are a lot of people also in academia and elsewhere outside

733
01:19:27,600 --> 01:19:33,840
of these companies who can be brought into this and have a lot of very good ideas and then i think

734
01:19:33,840 --> 01:19:39,840
it's very realistic that within six months you can get these people coming up so here's a white

735
01:19:40,480 --> 01:19:46,240
paper here's where we all think is reasonable you know you didn't just because cars killed a

736
01:19:46,240 --> 01:19:50,640
lot of people you didn't ban cars but they got together a bunch of people and decided you know

737
01:19:50,640 --> 01:19:56,800
in order to be allowed to sell a car it has to have a seat belt in it there the analogous things

738
01:19:56,800 --> 01:20:07,600
that you can start requiring a future AI systems so that they are are safe and once this heavy

739
01:20:07,600 --> 01:20:12,560
lifting this intellectual work has been done by experts in the field which can be done quickly

740
01:20:13,360 --> 01:20:18,560
i think it's going to be quite easy to get policymakers to to see yeah this is a good idea

741
01:20:19,200 --> 01:20:26,800
and it's it's you know for the fight for the companies to fight mallock they want and i

742
01:20:26,800 --> 01:20:31,040
believe sam altman has explicitly called for this they want the regulators to actually adopt it so

743
01:20:31,040 --> 01:20:34,320
that their competition is going to abide by it too right you don't want

744
01:20:37,040 --> 01:20:41,280
you don't want to be enacting all these principles then you abide by them and then

745
01:20:41,280 --> 01:20:48,400
there's this one little company that doesn't sign on to it and then now they can gradually overtake

746
01:20:48,400 --> 01:20:55,520
you then the companies will get be able to sleep secure knowing that everybody's playing by the

747
01:20:55,520 --> 01:21:02,400
same rules so do you think it's possible to develop guardrails that keep the systems from

748
01:21:04,640 --> 01:21:11,440
from basically damaging irreparably humanity while still enabling sort of the capitalist

749
01:21:11,440 --> 01:21:16,800
fueled competition between companies as they develop how to best make money with this AI

750
01:21:16,800 --> 01:21:21,280
you think there's a balancing that's possible absolutely i mean we've seen that in many other

751
01:21:21,280 --> 01:21:26,720
sectors where you've had the free market produce quite good things without uh causing particular

752
01:21:26,720 --> 01:21:34,720
harm um when the guardrails are there and they work you know capitalism is a very good way of

753
01:21:34,720 --> 01:21:39,680
optimizing for just getting the same things on more efficiently it was but it was good you know

754
01:21:39,680 --> 01:21:47,360
and like in hindsight i've never met anyone even even on parties way over on the right

755
01:21:48,080 --> 01:21:51,840
in in in any country who who think it was a bad it thinks it was a terrible idea to

756
01:21:52,560 --> 01:21:58,560
ban child labor for example yeah but it seems like this particular technology has gotten so

757
01:21:58,560 --> 01:22:06,080
good so fast become powerful to a degree where you could see in the near term the ability to make

758
01:22:06,080 --> 01:22:11,600
a lot of money yeah and to put guardrails develop guardrails quickly in that kind of context seems

759
01:22:11,600 --> 01:22:18,800
to be tricky it's not uh similar to cars or child labor it seems like the opportunity to make a lot

760
01:22:18,800 --> 01:22:26,640
of money here very quickly is right here before again it's there's this cliff yeah it gets quite

761
01:22:26,640 --> 01:22:31,680
seated closer to the cliff there you go yeah the more the more go there more money there is more

762
01:22:31,680 --> 01:22:35,520
gold in gets there on the ground you can pick up or whatever it's you want to drive there very fast

763
01:22:36,080 --> 01:22:40,240
but it's not in anyone's incentive that we go over the cliff and it's not like everybody's in their

764
01:22:40,240 --> 01:22:45,360
own car all the cars are connected together with a chain yeah so if anyone goes over they'll start

765
01:22:45,360 --> 01:22:49,840
dragging others down the others down too and so ultimately it's in the selfish

766
01:22:51,840 --> 01:22:57,680
interests also of the people in the companies to slow down when the when you can start seeing

767
01:22:58,240 --> 01:23:02,800
the corners of the cliff there in front of you right and the problem is that even though the

768
01:23:02,800 --> 01:23:10,320
people who are building the technology and the CEOs they really get it the shareholders and

769
01:23:11,040 --> 01:23:16,720
these other market forces they are people who don't honestly understand that the cliff is there

770
01:23:16,720 --> 01:23:22,000
they usually don't you have to get quite into the weeds to really appreciate how powerful this is

771
01:23:22,000 --> 01:23:28,240
and how fast and a lot of people are even still stuck again in this idea that intelligence in this

772
01:23:28,240 --> 01:23:32,960
carbon chauvinism as I like to call it that you can only have our level of intelligence in

773
01:23:34,000 --> 01:23:37,920
humans that there's something magical about it whereas the people in the tech companies

774
01:23:39,440 --> 01:23:45,200
who build this stuff they all realize that intelligence is information processing of a

775
01:23:45,200 --> 01:23:49,840
certain kind and it really doesn't matter at all whether the information is processed by carbon

776
01:23:49,840 --> 01:23:58,880
atoms in neurons in brains or by silicon atoms in some technology we build so you brought up

777
01:23:58,880 --> 01:24:05,200
capitalism earlier and there are a lot of people who love capitalism and a lot of people who really

778
01:24:05,200 --> 01:24:15,760
really don't and it's it struck me recently that the what's happening with capitalism here is exactly

779
01:24:15,760 --> 01:24:25,200
analogous to the way in which super intelligence might wipe us out so you know I studied economics

780
01:24:25,200 --> 01:24:31,840
for my undergrad in Stockholm school of economics yay well no no I tell me so I was very interested

781
01:24:31,840 --> 01:24:37,600
in how how you could use market forces to just get stuff done more efficiently but give the right

782
01:24:37,600 --> 01:24:43,360
incentives to market so that it wouldn't do really bad things so Dylan had Phil Manel who's a

783
01:24:44,160 --> 01:24:50,160
professor and colleague of mine at MIT wrote this really interesting paper with some collaborators

784
01:24:50,160 --> 01:24:56,640
recently where they proved mathematically that if you just take one goal that you just optimize for

785
01:24:57,760 --> 01:25:02,560
on and on and on indefinitely that you think is gonna bring you in the right direction

786
01:25:03,280 --> 01:25:08,400
what basically always happens is in the beginning it will make things better for you

787
01:25:08,400 --> 01:25:13,920
but if you keep going at some point it's going to start making things worse for you again and then

788
01:25:13,920 --> 01:25:18,720
gradually it's going to make it really really terrible so just as a simple the way I think of

789
01:25:18,720 --> 01:25:27,040
the proof is suppose you want to go from here back to Austin for example and you're like okay yeah

790
01:25:27,040 --> 01:25:32,640
let's just let's go south but you put in exactly the right sort of the right direction just optimize

791
01:25:32,640 --> 01:25:40,560
that south as possible you get closer and closer to Austin but there's always some little error

792
01:25:41,280 --> 01:25:46,320
so you you're not going exactly towards Austin but you get pretty close but eventually you start

793
01:25:46,320 --> 01:25:52,080
going away again and eventually you're gonna be leaving the solar system yeah and they they proved

794
01:25:52,080 --> 01:25:57,200
it's beautiful mathematical proof this happens generally and this is very important for AI

795
01:25:57,760 --> 01:26:04,720
because for even though Stuart Russell has written a book and given a lot of talks on why

796
01:26:04,720 --> 01:26:09,760
it's a bad idea to have AI just blindly optimize something that's what pretty much all our systems

797
01:26:09,760 --> 01:26:14,000
do yeah we have something called a loss function then we're just minimizing or reward function

798
01:26:14,000 --> 01:26:24,960
we're just minimize maximizing and um capitalism is exactly like that too we wanted to get stuff

799
01:26:24,960 --> 01:26:33,040
done more efficiently that people wanted so introduce the free market things got done much

800
01:26:33,040 --> 01:26:41,760
more efficiently than they did and and say communism right and it got better but then

801
01:26:41,760 --> 01:26:46,880
it just kept optimizing it and kept optimizing and you got ever bigger companies and ever more

802
01:26:46,880 --> 01:26:54,480
efficient information processing and I was also very much powered by it and eventually a lot of

803
01:26:54,480 --> 01:26:58,480
people are beginning to feel wait we're kind of optimizing a bit too much like why did we just chop

804
01:26:58,480 --> 01:27:06,480
down half the rain for us you know and why why did suddenly these regulators get captured by lobbyists

805
01:27:06,480 --> 01:27:13,280
and so on it's just the same optimization that's been running for too long if you have an AI that

806
01:27:13,280 --> 01:27:17,600
actually has power over the world and you just give it one goal and just like keep optimizing

807
01:27:17,600 --> 01:27:21,760
that most likely everybody's gonna be like yay this is great in the beginning things are getting

808
01:27:21,760 --> 01:27:30,320
better but um it's almost impossible to give it exactly the right direction to optimize in and then

809
01:27:32,080 --> 01:27:37,680
eventually all hey breaks loose right nick boss drum and others are given examples that sound

810
01:27:37,680 --> 01:27:44,320
quite silly like what if you just want to like tell it to cure cancer or something and that's all

811
01:27:44,320 --> 01:27:51,680
you tell it maybe it's gonna decide to take over entire continents just so we can get more

812
01:27:51,760 --> 01:27:56,160
super computer facilities in there and figure out a cure cancer backwards and then you're like

813
01:27:56,160 --> 01:28:03,360
wait that's not what I wanted right and um the the the issue with capitalism and the issue with

814
01:28:03,360 --> 01:28:09,760
the front of way I have kind of merged now because the malloc I talked about is exactly the capitalist

815
01:28:09,760 --> 01:28:18,480
malloc that we have built an economy that has is optimized for only one thing profit right and

816
01:28:18,560 --> 01:28:22,000
that worked great back when things were very inefficient and then now it's getting done better

817
01:28:22,560 --> 01:28:26,640
and it worked great as long as the companies were small enough that they couldn't capture the

818
01:28:26,640 --> 01:28:35,680
regulators but that's not true anymore but they keep optimizing and now we they realize that that

819
01:28:35,680 --> 01:28:39,840
they can these companies can make even more profit by building ever more powerful AI even if it's

820
01:28:39,840 --> 01:28:50,560
reckless but optimize more and more and more and more so this is malloc again showing up and I just

821
01:28:50,560 --> 01:28:58,320
want to anyone here who has any concerns about about late stage capitalism having gone a little too

822
01:28:58,320 --> 01:29:05,360
far you should worry about superintelligence because it's the same villain in both cases it's

823
01:29:05,440 --> 01:29:13,520
malloc and optimizing one objective function aggressively blindly is going to take us there

824
01:29:13,520 --> 01:29:20,640
yeah we have to pause from time to time and look into our hearts and ask why are we doing this is

825
01:29:20,640 --> 01:29:26,480
this am I still going towards austin or have I gone too far you know maybe we should change direction

826
01:29:27,360 --> 01:29:32,800
and that is the idea behind the halt for six months why six months it seems like a very short

827
01:29:32,800 --> 01:29:38,960
period just can we just linger and explore different ideas here because this feels like a

828
01:29:38,960 --> 01:29:44,720
really important moment in human history where pausing would actually have a significant positive

829
01:29:44,720 --> 01:29:53,200
effect we said six months because we figured the number one pushback we were going to get

830
01:29:53,200 --> 01:30:01,680
in the west was like but china and everybody knows there's no way that china is going to catch up

831
01:30:02,160 --> 01:30:06,240
west on this in six months so it's that argument goes off the table and you can

832
01:30:06,240 --> 01:30:12,480
forget about geopolitical competition and just focus on the real issue that's why we put this

833
01:30:12,480 --> 01:30:18,480
that's really interesting but you've already made the case that even for china if you actually

834
01:30:18,480 --> 01:30:25,680
want to take on that argument china too would not be bothered by a longer halt because they

835
01:30:25,680 --> 01:30:30,640
don't want to lose control even more than the west doesn't that's what I think that's a really

836
01:30:30,640 --> 01:30:35,360
interesting argument like I have to actually really think about that which the the kind of

837
01:30:36,000 --> 01:30:41,680
thing people assume is if you develop an agi that open ai if they're the ones that do it for example

838
01:30:42,240 --> 01:30:49,040
they're going to win but you're saying no they're everybody loses yeah it's going to get better

839
01:30:49,040 --> 01:30:53,840
and better and better and then kaboom we all lose that's what's going to happen when lose and win

840
01:30:53,840 --> 01:31:01,040
a define and a metric of basically quality of life for human civilization and for sam altman

841
01:31:01,920 --> 01:31:06,640
I have to be people on my personal guess you know and people can quibble with this is that we're

842
01:31:06,640 --> 01:31:12,080
just gonna there won't be any humans that's it that's what I mean by lose you know if you we can

843
01:31:12,080 --> 01:31:17,840
see in history once you have some species or some group of people who aren't needed anymore

844
01:31:18,720 --> 01:31:26,240
and doesn't usually work out so well for them right yeah there were a lot of horses for were used

845
01:31:26,240 --> 01:31:31,680
for traffic in boston and then the car got invented and most of them got you know we don't

846
01:31:31,680 --> 01:31:43,840
need to go there and uh if you look at humans you know right now we why did the labor movement

847
01:31:43,840 --> 01:31:52,160
succeed and after the industrial revolution because it was needed even though we had a lot

848
01:31:52,160 --> 01:31:58,400
of mollocks and there was child labor and so on you know the company still needed to have workers

849
01:31:59,520 --> 01:32:05,280
and that's why strikes had power and so on if we get to the point where most humans aren't

850
01:32:05,280 --> 01:32:09,040
needed anymore I think it's like it's quite naive to think that they're going to still be treated

851
01:32:09,040 --> 01:32:14,880
well you know we say that yeah yeah everybody's equal and the government will always we'll always

852
01:32:14,880 --> 01:32:20,320
protect them but if you look in practice groups that are very disenfranchised and don't have any

853
01:32:20,320 --> 01:32:30,800
actual power usually get screwed and now in the beginning so industrial revolution we automated

854
01:32:30,800 --> 01:32:37,840
away muscle work but that got went worked out pretty well eventually because we educated ourselves

855
01:32:37,840 --> 01:32:42,640
and started working with our brains instead and got usually more interesting better paid jobs

856
01:32:43,520 --> 01:32:48,560
but now we're beginning to replace brain work so we replaced a lot of boring stuff like we got the

857
01:32:48,560 --> 01:32:54,560
pocket calculator so you don't have people adding multiplying numbers anymore at work fine there were

858
01:32:54,560 --> 01:33:02,720
better jobs they could get but now gpt4 you know and the stable diffusion and techniques like this

859
01:33:02,720 --> 01:33:08,880
they're really beginning to blow away some real some jobs that people really love having it was a

860
01:33:08,880 --> 01:33:15,680
heartbreaking article just post just yesterday social media I saw about this guy who was doing

861
01:33:15,680 --> 01:33:21,840
3d modeling for gaming and he and all of a sudden now they got this new software he just give

862
01:33:21,840 --> 01:33:30,480
says prompts and he feels his whole job that he loved lost its meaning you know and I asked gpt4

863
01:33:31,040 --> 01:33:37,120
rewrite twinkle twinkle little star in the style of Shakespeare I couldn't have done such a good

864
01:33:37,120 --> 01:33:43,840
job it was really impressive you've seen a lot of art coming out here right so I'm all for

865
01:33:45,360 --> 01:33:51,920
automating away the dangerous jobs and the boring jobs but I think you hear a lot some

866
01:33:51,920 --> 01:33:55,200
arguments which are too glib sometimes people say well that's all that's going to happen we're

867
01:33:55,200 --> 01:34:01,120
getting rid of the boring boring tedious dangerous jobs it's just not true there are a lot of really

868
01:34:01,120 --> 01:34:05,600
interesting jobs that are being taken away now journalism is getting going to get crushed

869
01:34:07,040 --> 01:34:12,960
coding is going to get crushed I predict the job market for programmers salaries are going to start

870
01:34:12,960 --> 01:34:19,040
dropping you know if you said you can code five times faster you know then you need five times

871
01:34:19,040 --> 01:34:25,040
fewer programmers maybe there will be more output also but you'll still end up using fewer program

872
01:34:25,040 --> 01:34:29,600
needing fewer programmers than today and I love coding you know I think it's super cool

873
01:34:31,280 --> 01:34:36,720
so we we need to stop and ask ourselves why again are we doing this as humans right

874
01:34:38,880 --> 01:34:45,520
I feel that AI should be built by humanity for humanity and let's not forget that

875
01:34:46,160 --> 01:34:51,360
it shouldn't be by malloc for malloc or what it really is now is kind of by humanity

876
01:34:52,000 --> 01:34:57,840
for malloc which doesn't make any sense it's for us that we're doing it then and

877
01:34:59,280 --> 01:35:04,800
make a lot more sense if we build develop figure out gradually safely how to make all this tech

878
01:35:04,800 --> 01:35:08,000
and then we think about what are the kind of jobs that people really don't want to have

879
01:35:08,000 --> 01:35:14,080
you know automate them all the way and then we ask what are the jobs that people really find

880
01:35:14,080 --> 01:35:24,000
meaning in like maybe taking care of children in the daycare center maybe doing art etc etc and

881
01:35:24,000 --> 01:35:29,120
even if it were possible to automate that way we don't need to do that right it's we built these

882
01:35:29,120 --> 01:35:36,640
machines well it's possible that we redefine or rediscover what are the jobs that give us meaning

883
01:35:36,720 --> 01:35:40,480
for me the thing it is really sad like I

884
01:35:42,000 --> 01:35:48,880
half the time I'm excited half the time I'm crying as I'm generating code because

885
01:35:49,680 --> 01:35:58,400
I kind of love programming it's an act of creation you have an idea you design it and then you

886
01:35:58,400 --> 01:36:02,240
bring it to life and it does something especially if there's some intelligence to it does something

887
01:36:02,240 --> 01:36:06,160
it doesn't even have to have intelligence bringing printing hello world on screen

888
01:36:07,120 --> 01:36:13,440
you made a little machine and it comes to life and there's a bunch of tricks you learn along

889
01:36:13,440 --> 01:36:19,920
the way because you've been doing it for for many many years and then to see AI be able to generate

890
01:36:19,920 --> 01:36:30,800
all the tricks you thought were special I don't know it's very it it's scary it's almost painful

891
01:36:30,800 --> 01:36:38,640
like a loss loss of innocence maybe like yeah maybe when I was younger I remember before I learned

892
01:36:38,640 --> 01:36:45,440
that sugar is bad for you you should be on a diet I remember I enjoyed candy deeply in a way I just

893
01:36:45,440 --> 01:36:54,480
can't anymore that I know is bad for me I enjoyed it unapologetically fully just intensely and I

894
01:36:55,200 --> 01:37:00,800
lost that now I feel like a little bit of that is lost for me with program or being lost with

895
01:37:00,800 --> 01:37:08,880
programming similar as it is for the 3d modeler no longer being able to really enjoy the art of

896
01:37:08,880 --> 01:37:13,920
modeling 3d things for gaming I don't know I don't know what to make sense of that maybe I would

897
01:37:13,920 --> 01:37:18,160
rediscover that the true magic of what it means to be human is connecting with other humans to

898
01:37:18,240 --> 01:37:25,440
have conversations like this I don't know to uh to have sex to have to eat food to really intensify

899
01:37:26,160 --> 01:37:32,080
the value from conscious experiences versus like creating other stuff you're pitching the rebranding

900
01:37:32,080 --> 01:37:37,440
again from homo sapiens the homo sapiens the meaningful experiences and just to inject some

901
01:37:37,440 --> 01:37:42,160
optimism in this year so we don't sound like a bunch of gloomers you know we can totally have

902
01:37:42,160 --> 01:37:46,560
our cake and eat it you hear a lot of totally bullshit claims that we can't afford having more

903
01:37:46,640 --> 01:37:53,440
teachers yeah have to cut the number of nurses you know that's just nonsense obviously with

904
01:37:54,160 --> 01:38:02,720
anything even quite far short of agi we can dramatically improve grow the gdp and produce

905
01:38:02,720 --> 01:38:08,240
this wealth of goods and services it's very easy to create a world where everybody is better off

906
01:38:08,240 --> 01:38:15,120
than today including the richest people can be better off as well right it's not a zero-sum game

907
01:38:15,760 --> 01:38:20,800
you know technology again you can have two countries like sweden and danmark had all these

908
01:38:20,800 --> 01:38:28,720
ridiculous wars century after century and uh sometimes that sweden got a little better off

909
01:38:28,720 --> 01:38:31,760
because it got a little bit bigger and then danmark got a little better off because sweden got a

910
01:38:31,760 --> 01:38:35,840
little bit smaller and and but then we then technology came along and we both got just

911
01:38:35,840 --> 01:38:40,080
dramatically wealthier without taking away from anyone else it was just a total win for everyone

912
01:38:40,960 --> 01:38:49,280
and uh ai can do that on steroids if you can build safe agi if you can build super intelligence

913
01:38:50,800 --> 01:38:58,000
basically all the limitations that cause harm today can be completely eliminated right it's a

914
01:38:58,000 --> 01:39:03,680
wonderful you talk possibility and this is not sci-fi this is something which is clearly possible

915
01:39:03,680 --> 01:39:09,760
according to the laws of physics and i we can talk about ways of making it safe also um but

916
01:39:09,760 --> 01:39:16,240
unfortunately that'll only happen if we steer in that direction that's absolutely not the default

917
01:39:16,240 --> 01:39:24,000
outcome that's why income inequality keeps going up that's why the life expectancy in the u.s has

918
01:39:24,000 --> 01:39:30,160
been going down now i think it's four years in a row i just read a heartbreaking study from

919
01:39:30,160 --> 01:39:36,720
the cdc about how something like one-third of all teenage girls in the u.s i've been thinking

920
01:39:36,720 --> 01:39:43,920
about suicide you know like those are steps in the totally the wrong direction and and it's

921
01:39:43,920 --> 01:39:51,280
important to keep our eyes on the prize here that we can we have the power now for the first time

922
01:39:51,280 --> 01:39:57,840
in the history of our species to harness artificial intelligence to help us really flourish

923
01:39:57,840 --> 01:40:07,680
and help bring out the best in our humanity rather than the worst of it to help us have

924
01:40:07,680 --> 01:40:13,360
really fulfilling experiences that feel truly meaningful and and we you and i shouldn't sit

925
01:40:13,360 --> 01:40:16,800
here and dictate the future generations what they will be let them figure it out but let's

926
01:40:16,800 --> 01:40:21,920
give them a chance to live and and not foreclose all these possibilities for them by just messing

927
01:40:21,920 --> 01:40:26,400
things up right now for that we have to solve the ai safety problem i just it'll be nice if

928
01:40:26,480 --> 01:40:33,920
we can link on exploring that a little bit so one interesting way to enter that discussion is

929
01:40:34,720 --> 01:40:41,520
you tweeted and elon replied you tweeted let's not just focus on whether gpt4 will do more harm or

930
01:40:41,520 --> 01:40:46,560
good on the job market but also whether it's coding skills will hasten the arrival of super

931
01:40:46,560 --> 01:40:51,120
intelligence that's something we've been talking about right so elon proposed one thing in their

932
01:40:51,120 --> 01:40:58,800
reply saying maximum truth seeking is my best guess for ai safety can you maybe uh steelman the

933
01:40:58,800 --> 01:41:06,640
case for this uh sense this objective function of truth and uh maybe make an argument against it

934
01:41:06,640 --> 01:41:12,640
and in general what uh are your different ideas to start approaching the the solution to ai safety

935
01:41:12,640 --> 01:41:18,000
i didn't see that reply actually oh interesting so but i really resonate with it because

936
01:41:21,760 --> 01:41:26,560
ai is not evil it it caused people around the world to hate each other much more

937
01:41:27,680 --> 01:41:32,560
but that's because we made it in a certain way it's a tool we can use it for great things and

938
01:41:32,560 --> 01:41:37,840
bad things and we could just as well have ai systems and this is this is part of my vision

939
01:41:37,840 --> 01:41:45,040
for success here truth seeking ai that really brings us together again you know why do people

940
01:41:45,040 --> 01:41:51,040
hate each other so much between countries and within countries it's because they each have

941
01:41:51,040 --> 01:41:57,600
totally different versions of the truth right if they all have the same truth that they trusted

942
01:41:57,600 --> 01:42:01,600
for good reason because they could check it and verify it and not have to believe in some

943
01:42:01,600 --> 01:42:07,120
self-proclaimed authority right they wouldn't be as nearly as much hate there'd be a lot more

944
01:42:07,120 --> 01:42:15,360
understanding instead and this is i think something ai can help enormously with for example

945
01:42:16,880 --> 01:42:21,280
a little baby step in this direction is this website called metaculous where

946
01:42:23,360 --> 01:42:27,840
people bet and make predictions not for money but just for their own reputation

947
01:42:28,880 --> 01:42:32,800
and it's kind of funny actually you treat the humans like you treat ai as you have a

948
01:42:33,760 --> 01:42:38,400
lost function where they get penalized if they're super confident on something and then the opposite

949
01:42:38,400 --> 01:42:44,640
happens yeah whereas if you're kind of humble and then you're like i think it's 51% chance this

950
01:42:44,640 --> 01:42:49,200
is going to happen and then the other happens you don't get penalized much and and what you can see

951
01:42:49,200 --> 01:42:55,760
is that some people are much better at predicting than others they've earned your trust right one

952
01:42:55,760 --> 01:42:59,520
project that i'm working on right now is an outgrowth to improve the news foundation together

953
01:42:59,520 --> 01:43:04,720
the metaculous folks is seeing if we can really scale this up a lot with more powerful ai because

954
01:43:04,720 --> 01:43:10,720
i would love it i would love further to be like a really powerful truth seeking system where

955
01:43:12,160 --> 01:43:19,360
that is trustworthy because it keeps being right about stuff and people who come to it and

956
01:43:21,840 --> 01:43:27,520
maybe look at its latest trust ranking of different pundits and newspapers etc if they

957
01:43:27,520 --> 01:43:32,320
want to know why some someone got a low score they can click on it and see all the predictions

958
01:43:32,320 --> 01:43:37,440
that they actually made and how they turned out you know this is how we do it in science

959
01:43:38,080 --> 01:43:41,680
you trust scientists like einstein who said something everybody thought was bullshit

960
01:43:42,320 --> 01:43:47,200
and turned out to be right you get a lot for a trust point and he did it multiple times even

961
01:43:47,600 --> 01:43:55,440
i think ai has the power to really heal a lot of the rifts we're seeing

962
01:43:56,640 --> 01:44:03,760
by creating trust system it has to get away from this idea today with some fact-checking sites

963
01:44:03,760 --> 01:44:07,520
which might themselves have an agenda and you just trust it because of its reputation

964
01:44:09,600 --> 01:44:14,640
you want to have it so these sort of systems they earn their trust and that's completely

965
01:44:14,720 --> 01:44:22,720
transparent this i think would actually help a lot that i think help heal the very dysfunctional

966
01:44:23,280 --> 01:44:28,800
conversation that humanity has about how it's going to deal with all its biggest challenges

967
01:44:28,800 --> 01:44:37,280
in the world today and then on the technical side you know another common sort of gloom

968
01:44:38,640 --> 01:44:41,440
comment that yet from people are saying we're just screwed there's no hope

969
01:44:42,240 --> 01:44:46,320
is well things like gpt4 are way too complicated for a human to ever understand

970
01:44:47,040 --> 01:44:50,800
and prove that they can be trustworthy they're forgetting that ai can help us

971
01:44:51,520 --> 01:44:58,800
prove that things work right and and there's this very fundamental fact that in math it's much

972
01:44:59,680 --> 01:45:04,080
harder to come up with a proof than it is to verify that the proof is correct

973
01:45:04,720 --> 01:45:09,680
you can actually write a little proof checking code it's quite short but you can as human

974
01:45:09,680 --> 01:45:14,720
understand and then it can check most monstrously long proof ever generated even by a computer and

975
01:45:14,720 --> 01:45:27,600
say yeah this is valid so so right now we we have um this uh this approach with virus checking

976
01:45:27,600 --> 01:45:32,400
software that it looks to see if there's something if you should not trust it and if it can prove

977
01:45:32,400 --> 01:45:39,040
to itself that you should not trust that code it warns you right what if you flip this around

978
01:45:39,920 --> 01:45:45,280
and this is an idea i should give credit to steve i'm a hundred or four so that it will only run

979
01:45:45,280 --> 01:45:50,480
the code if it can prove instead of not running it if it can prove that it's not trustworthy if

980
01:45:50,480 --> 01:45:54,400
it will only run and if it can prove that it's trustworthy so it asks the code prove to me

981
01:45:54,400 --> 01:46:00,320
that you're going to do what you say you're going to do and and it gives you this proof

982
01:46:01,520 --> 01:46:05,040
and you a little proof tricker can check it now you can actually trust

983
01:46:05,920 --> 01:46:12,560
an AI that's much more intelligent than you are right because you it's its problem to come

984
01:46:12,560 --> 01:46:16,960
up with this proof that you could never have found that you should trust it so this is the

985
01:46:16,960 --> 01:46:23,120
interesting point i i agree with you but this is where eliezer yakovsky might disagree with you

986
01:46:23,120 --> 01:46:29,600
his claim not with you but with this idea is his claim is super intelligent AI

987
01:46:30,560 --> 01:46:34,480
would be able to know how to lie to you with such a proof

988
01:46:36,000 --> 01:46:41,040
how to lie to you and give me a proof that i'm gonna think is correct yeah but but it's not me

989
01:46:41,040 --> 01:46:48,160
it's lying to you that's the trick my proof checker so yes so his general idea is a super

990
01:46:48,160 --> 01:46:56,480
intelligent system can lie to a dumber proof checker so you're going to have as a system

991
01:46:56,480 --> 01:47:01,440
because more and more intelligent there's going to be a threshold where a super intelligent system

992
01:47:01,440 --> 01:47:07,120
would be able to effectively lie to a slightly dumber AGI system like there's a threat like he

993
01:47:07,120 --> 01:47:15,040
really focuses on this weak AGI the strong AGI jump where the strong AGI can make all the weak

994
01:47:15,040 --> 01:47:22,800
AGI's think that it's just one of them but it's no longer that and that leap is when it runs away

995
01:47:22,800 --> 01:47:29,680
yeah i i don't buy that argument i think no matter how super intelligent an AI is it's never

996
01:47:29,680 --> 01:47:33,040
going to be able to prove to me that they're only finitely many primes for example

997
01:47:34,880 --> 01:47:40,880
and it just it just can't and and um it can try to snow me we're making up all sorts of new weird

998
01:47:40,880 --> 01:47:49,120
rules of of deduction and that and say trust me you know the way your proof checker work is too

999
01:47:49,120 --> 01:47:55,440
limited and we have this new hyper math and it's true but then i would i would just take the attitude

1000
01:47:55,440 --> 01:48:00,320
okay i'm going to forfeit some of these this supposedly super cool technologies i'm only

1001
01:48:00,320 --> 01:48:04,560
going to go with the ones that i can prove in my own trusted proof checker then i don't i think

1002
01:48:04,560 --> 01:48:09,760
it's fine there's still of course this is not something anyone has successfully implemented

1003
01:48:09,760 --> 01:48:15,600
at this point but i think it i just give it as an example of hope we don't have to do all the

1004
01:48:15,600 --> 01:48:21,120
work ourselves right this is exactly the sort of very boring and tedious tasks is perfect outsourced

1005
01:48:21,120 --> 01:48:27,440
to an AI and this is a way in which less powerful and less intelligent agents like us can actually

1006
01:48:28,560 --> 01:48:34,320
continue to control and trust more powerful ones so build a gi systems that help us defend against

1007
01:48:34,320 --> 01:48:40,320
other a gi systems well for starters begin with a simple problem of just making sure that the system

1008
01:48:40,400 --> 01:48:46,000
that you own or that's supposed to be loyal to you has to prove to itself that it's always

1009
01:48:46,000 --> 01:48:50,000
going to do the things that you actually wanted to do right and if it can't prove it maybe it's

1010
01:48:50,000 --> 01:48:55,440
still going to do it but you won't run it so you just forfeit some aspects of all the cool things

1011
01:48:55,440 --> 01:49:00,400
that i can do i i bet your dollars and donuts it can still do some incredibly cool stuff for you

1012
01:49:00,400 --> 01:49:06,400
yeah there are other things too that we shouldn't sweep under the rug like not every human agrees on

1013
01:49:06,480 --> 01:49:11,920
exactly where what direction we should go with humanity right yes and you've talked a lot about

1014
01:49:12,480 --> 01:49:16,400
geopolitical things on this on on your podcast to this effect you know but

1015
01:49:17,600 --> 01:49:21,920
i think that shouldn't distract us from the fact that there are actually a lot of things that

1016
01:49:21,920 --> 01:49:28,800
everybody in the world virtually agrees on that hey you know like having no humans on the planet

1017
01:49:28,800 --> 01:49:36,640
in a in a in a near future let's not do that right you look at something like the united

1018
01:49:36,640 --> 01:49:44,080
nation sustainable development goals some of them were quite ambitious and basically all the

1019
01:49:44,080 --> 01:49:50,560
countries agree us china russia ukraine you all agree so instead of quibbling about the little

1020
01:49:50,560 --> 01:49:55,600
things we don't agree on let's start with the things we do agree on and and and get them done

1021
01:49:56,560 --> 01:50:03,040
instead of being so distracted by all these things we disagree on that mollock wins because

1022
01:50:03,760 --> 01:50:11,920
frankly mollock going wild now it feels like a war on life playing out in front of our eyes

1023
01:50:11,920 --> 01:50:20,560
if you if you just look at it from space you know we're on this planet beautiful vibrant ecosystem

1024
01:50:20,640 --> 01:50:26,640
now we start chopping down big parts of it even though nobody most people thought that was a

1025
01:50:26,640 --> 01:50:33,840
bad idea always start doing ocean acidification wiping out all sorts of species oh now we have

1026
01:50:33,840 --> 01:50:39,920
all these close calls you almost had a nuclear war and we're replacing more and more of the biosphere

1027
01:50:39,920 --> 01:50:46,560
with non-living things we're also replacing in our social lives a lot of the things which

1028
01:50:47,280 --> 01:50:52,960
we're so valuable to humanity a lot of social interactions now are replaced by people staring

1029
01:50:52,960 --> 01:50:59,440
into their rectangles right and i i'm not a psychologist i'm out of my depth here but i

1030
01:50:59,440 --> 01:51:04,880
suspect that part of the reason why teen suicide and suicide in general in the u.s the

1031
01:51:04,880 --> 01:51:11,520
record-breaking levels is actually caused by again a so ai technologies and social media

1032
01:51:11,520 --> 01:51:15,600
that making people spend less time with with actually an actually just human interaction

1033
01:51:16,240 --> 01:51:22,160
we've all seen a bunch of good-looking people in restaurants staring into the rectangles

1034
01:51:22,160 --> 01:51:28,080
instead of looking into each other's eyes right that so that's also a part of the war on life

1035
01:51:28,080 --> 01:51:39,120
that that we're we're replacing so many really life-affirming things by technology we're we're

1036
01:51:39,120 --> 01:51:44,160
putting technology between us the technology that was supposed to connect us is actually

1037
01:51:44,160 --> 01:51:50,720
distancing us ourselves from each other and um and and then we're giving ever more power to

1038
01:51:50,720 --> 01:51:56,720
things which are not alive these large corporations are not living things right they're just maximizing

1039
01:51:56,720 --> 01:52:05,680
profit there i want to win them war on life i think we humans together with all our fellow

1040
01:52:05,680 --> 01:52:12,560
living things on this planet will be better off if we can remain in control over the non-living

1041
01:52:12,560 --> 01:52:18,480
things and make sure that they they work for us i really think it can be done can you just linger

1042
01:52:18,480 --> 01:52:24,800
on this um maybe high-level philosophical disagreement with eliezer yadkowski

1043
01:52:25,440 --> 01:52:34,640
i in this the hope you're stating so he is very sure he puts a very high probability

1044
01:52:35,520 --> 01:52:42,400
very close to one depending on the day he puts it at one uh that ai is going to kill humans

1045
01:52:43,920 --> 01:52:50,240
that there's just he does not see a trajectory which it doesn't end up with that conclusion

1046
01:52:50,800 --> 01:52:54,880
what uh what trajectory do you see that doesn't end up there and maybe can you

1047
01:52:55,760 --> 01:53:00,960
can you see the point he's making and and can you also see a way out

1048
01:53:03,280 --> 01:53:11,040
first of all i tremendously respect eliezer yadkowski and his his thinking second i do

1049
01:53:11,840 --> 01:53:16,880
share his view that there's a pretty large chance that we're not going to make it as humans there

1050
01:53:16,880 --> 01:53:22,000
won't be any humans on the planet and not the distant future and and that makes me very sad

1051
01:53:22,000 --> 01:53:25,520
you know we just had a little baby and i keep asking myself you know is um

1052
01:53:31,520 --> 01:53:39,120
how old is even gonna get you know and and um i asked myself it feels i i said to my wife recently

1053
01:53:39,120 --> 01:53:45,840
it feels a little bit like i was just diagnosed with some sort of um cancer which has some you know

1054
01:53:45,840 --> 01:53:53,600
risk of dying from and some risk of surviving you know uh except this is a kind of cancer which

1055
01:53:53,600 --> 01:54:01,200
would kill all of humanity so i completely take seriously his um his concerns i think um

1056
01:54:02,800 --> 01:54:10,560
but i don't absolutely don't think it's hopeless i think um there is a there is um first of all

1057
01:54:10,560 --> 01:54:16,480
a lot of momentum now for the first time actually since the many many years that have

1058
01:54:16,480 --> 01:54:22,240
passed since my since i and many others started warning about this i feel most people are getting

1059
01:54:22,240 --> 01:54:32,880
it now i i uh just talking to this guy in the gas station they were house the other day my

1060
01:54:33,200 --> 01:54:42,000
and he's like i think we're getting replaced and i think in it so that's positive that they're

1061
01:54:42,000 --> 01:54:46,480
finally we're finally seeing this reaction which is the first step towards solving the problem

1062
01:54:47,040 --> 01:54:54,720
uh second uh i really think that this this vision of only running ai's really if the stakes are

1063
01:54:54,720 --> 01:54:59,680
really high they can prove to us that they're safe it's really just virus checking in reverse

1064
01:54:59,680 --> 01:55:07,280
again i i think it's scientifically doable i don't think it's hopeless um we might have to

1065
01:55:07,280 --> 01:55:12,240
forfeit some of the technology that we could get if we were putting blind faith in our ai's

1066
01:55:12,240 --> 01:55:16,240
but we're still gonna get amazing stuff do you envision a process with a proof checker like

1067
01:55:16,240 --> 01:55:23,120
something like gpt4 gpt5 will go through a process no rigorous no no i think it's hopeless

1068
01:55:23,120 --> 01:55:29,680
that's like trying to prove fair about five spaghetti okay what i think well how the the

1069
01:55:29,680 --> 01:55:34,880
whole the vision i have for success is instead that you know just like we human beings were able

1070
01:55:34,880 --> 01:55:40,800
to look at our brains and and distill out the key knowledge Galileo when his dad threw him

1071
01:55:41,440 --> 01:55:45,040
an apple when he was a kid he was able to catch it because his brain could in this funny spaghetti

1072
01:55:45,040 --> 01:55:49,520
kind of way you know predict how parabolas are going to move his conaman system one right

1073
01:55:49,520 --> 01:55:55,840
but then he got older and it's like wait this is a parabola it's it's y equals x squared i can

1074
01:55:55,840 --> 01:56:00,000
distill this knowledge out and today you can easily program it into a computer and it can

1075
01:56:00,640 --> 01:56:05,600
simulate not just that but how to get tamars and so on right i envision a similar process

1076
01:56:05,600 --> 01:56:11,440
where we use the amazing learning power of neural networks to discover the knowledge in the first

1077
01:56:11,440 --> 01:56:19,120
place but we don't stop with a black box and and use that we then do a second round of ai

1078
01:56:19,120 --> 01:56:23,200
where we use automated systems to extract out the knowledge and see what is it look what are the

1079
01:56:23,200 --> 01:56:29,600
insights it's had okay and it's and then we we put that knowledge into a completely different

1080
01:56:29,600 --> 01:56:35,440
kind of architecture or programming language or whatever that's that's made in a way that

1081
01:56:35,440 --> 01:56:41,200
it can be both really efficient and also is more amenable to the very formal verification

1082
01:56:43,040 --> 01:56:46,800
that's that's my vision i'm not saying sitting here saying i'm confident

1083
01:56:46,800 --> 01:56:51,200
100 sure that it's gonna work you know but i don't think this chance is certainly not zero

1084
01:56:51,200 --> 01:56:56,480
either and it will certainly be possible to do for a lot of really cool ai applications

1085
01:56:57,120 --> 01:57:01,440
that we're not using now so we can have a lot of the fun that we're excited about

1086
01:57:02,160 --> 01:57:08,880
if we if we do this we're gonna need a little bit of time that's why it's good to pause and

1087
01:57:08,880 --> 01:57:18,720
put in place requirements one more thing also i i think you know someone might think well zero

1088
01:57:18,720 --> 01:57:23,600
percent chance we're gonna survive let's just give up right that's very dangerous

1089
01:57:26,160 --> 01:57:31,680
because there's no more guaranteed way it's a fail than to convince yourself that it's impossible

1090
01:57:32,320 --> 01:57:39,280
and not to try you know any if you you know when you're studying history and military history the

1091
01:57:39,280 --> 01:57:45,760
first thing you learn is that that's how you do psychological warfare you persuade the other side

1092
01:57:45,760 --> 01:57:52,880
that that's hopeless so they don't even fight and then of course you win right let's not do this

1093
01:57:52,880 --> 01:57:57,200
psychological warfare on ourselves and say there's a hundred percent probability we're all gonna we're

1094
01:57:57,200 --> 01:58:04,560
all screwed anyway it's sadly i i do get that a little bit sometimes from from uh

1095
01:58:04,560 --> 01:58:08,000
let's see some young people who are like so convinced that we're all screwed that they're like

1096
01:58:08,000 --> 01:58:13,600
i'm just gonna play game play computer games and do drugs and because we're screwed anyway right

1097
01:58:15,200 --> 01:58:19,920
it's important to keep the hope alive because it actually has a causal impact

1098
01:58:19,920 --> 01:58:24,720
and makes it makes it more likely that we're gonna succeed it seems like the people that actually

1099
01:58:24,720 --> 01:58:30,960
build solutions to a problem seemingly impossible to solve problems are the ones that believe yeah

1100
01:58:30,960 --> 01:58:36,240
they're the ones who are the optimists yeah and it's like uh it seems like there's some fundamental

1101
01:58:36,240 --> 01:58:41,840
law to the universe where fake it till you make it kind of works like believe yeah it's possible

1102
01:58:41,840 --> 01:58:48,960
and it becomes possible yeah was it henry ford who said that if you can if you tell yourself

1103
01:58:48,960 --> 01:58:55,200
that it's impossible it is so let's not make that mistake yeah and this is a big mistake

1104
01:58:55,200 --> 01:58:59,200
society is making you know i think all in all everybody's so gloomy and the media are also

1105
01:58:59,200 --> 01:59:08,000
very biased towards if it bleeds it leads and gloom and doom right so um most visions of the

1106
01:59:08,000 --> 01:59:14,240
future we have or or dystopian which really demotivates people we want to really really really

1107
01:59:14,240 --> 01:59:19,360
focus on the upside also to give people the willingness to fight for it and um

1108
01:59:22,000 --> 01:59:27,120
for ai you and i mostly talked about gloom here again but let's not remember not forget that you

1109
01:59:27,120 --> 01:59:34,800
know we have probably both lost someone we really cared about some disease that we were told were

1110
01:59:34,800 --> 01:59:39,520
was incurable well it's not there's no law of physics saying they had to die of that cancer

1111
01:59:39,520 --> 01:59:45,040
whatever of course you can cure it and there are so many other things where that we with

1112
01:59:45,040 --> 01:59:51,520
our human intelligence have also failed to solve on this planet which ai could also very much help

1113
01:59:51,520 --> 01:59:57,840
us with right so if we can get this right just be a little more chill and slow down a little bit

1114
01:59:57,840 --> 02:00:05,200
so we get it right it's mind blowing how awesome our future can be right we talked a lot about

1115
02:00:05,200 --> 02:00:11,120
stuff on earth can be great but even if you really get ambitious and look up into the skies right

1116
02:00:11,120 --> 02:00:17,360
there's no reason we have to be stuck on this planet for the rest of um the remain for billions

1117
02:00:17,360 --> 02:00:24,400
of years to come we totally understand now as laws of physics let life spread out into space

1118
02:00:25,360 --> 02:00:29,120
to other solar systems to other galaxies and flourish for billions of billions of years

1119
02:00:29,840 --> 02:00:32,560
and this to me is a very very hopeful vision

1120
02:00:34,640 --> 02:00:40,080
that really motivates me to to fight and coming back to the end something you talked about again

1121
02:00:40,080 --> 02:00:45,120
you know this the struggle how the human struggle is one of the things that also really gives meaning

1122
02:00:45,120 --> 02:00:51,680
to our lives if there's ever been an epic struggle this is it and isn't it even more epic if you're

1123
02:00:51,680 --> 02:00:58,160
the underdog if most people are telling you this is gonna fail it's impossible right and you persist

1124
02:00:59,920 --> 02:01:05,840
and you succeed right and that's what we can do together as a species on this one a lot of

1125
02:01:06,480 --> 02:01:12,160
pundits are ready to count this out both in the battle to keep AI safe and becoming a

1126
02:01:12,160 --> 02:01:17,920
multi-planetary species yeah and they're they're the same challenge if we can keep AI safe that's

1127
02:01:17,920 --> 02:01:23,760
how we're gonna get multi-planetary very efficiently i have some sort of technical questions about

1128
02:01:23,760 --> 02:01:30,080
how to get it right so one idea that i'm not even sure what the right answer is to is

1129
02:01:31,200 --> 02:01:37,840
should systems like GPT-4 be open sourced in whole or in part can make the can you see the

1130
02:01:37,840 --> 02:01:47,120
case for either i think the answer right now is no i think the answer early on was yes so we could

1131
02:01:47,200 --> 02:01:55,440
bring in all the wonderful great thought process of everybody on this but asking should we open

1132
02:01:55,440 --> 02:01:59,280
source GPT-4 now is just the same as if you say well is it good should we open source

1133
02:02:01,760 --> 02:02:08,240
how to build really small nuclear weapons should we open source how to make bio weapons

1134
02:02:09,520 --> 02:02:15,760
should we open source how to make a new virus that kills 90% of everybody who gets it of course we

1135
02:02:15,760 --> 02:02:22,640
shouldn't so it's already that powerful it's already that powerful that we have to respect

1136
02:02:22,640 --> 02:02:28,640
the power of the systems we've built the knowledge that you get

1137
02:02:30,480 --> 02:02:36,160
from open sourcing everything we do now might very well be powerful enough that people looking at

1138
02:02:36,160 --> 02:02:42,000
that can use it to build the things that you're really threatening again let's get it remember

1139
02:02:42,000 --> 02:02:51,200
open AI is GPT-4 is a baby AI baby sort of baby proto almost a little bit AGI according to what

1140
02:02:51,200 --> 02:02:56,240
Microsoft's recent paper said right it's not that that we're scared of what we're scared about

1141
02:02:56,240 --> 02:03:02,560
is people taking that who are who might be a lot less responsible than the company that made it

1142
02:03:03,440 --> 02:03:12,080
and just going to town with it that's why we want to it's it's an information hazard

1143
02:03:12,080 --> 02:03:16,880
there are many things which yeah are not open sourced right now in society for a very good reason

1144
02:03:17,520 --> 02:03:25,360
like how do you make certain kind of very powerful toxins out of stuff you can buy

1145
02:03:26,080 --> 02:03:29,280
and Home Depot you know you don't open source those things for a reason

1146
02:03:30,240 --> 02:03:36,960
and this is really no different so I'm saying that I have to say it feels in a bit weird in a

1147
02:03:36,960 --> 02:03:42,880
way a bit weird to say it because MIT is like the cradle of the open source movement and I love

1148
02:03:42,880 --> 02:03:50,880
open source in general power to the people let's say but there's always gonna be some stuff that

1149
02:03:50,880 --> 02:03:56,240
you don't open source and you know it's just like you don't open source so we have a three month old

1150
02:03:56,240 --> 02:03:59,920
baby right when he gets a little bit older we're not going to open source to him all the most

1151
02:03:59,920 --> 02:04:07,760
dangerous things he could do in the house yeah right but it does it's a weird feeling because

1152
02:04:07,760 --> 02:04:13,600
this is one of the first moments in history where there's a strong case to be made not to open

1153
02:04:13,600 --> 02:04:20,800
source software this is when the software has become too dangerous yeah but it's not the first

1154
02:04:20,800 --> 02:04:23,760
time that we didn't want to open source a technology technology yeah

1155
02:04:27,040 --> 02:04:31,840
is there something to be said about how to get the release of such systems right like GPT-4 and

1156
02:04:31,840 --> 02:04:39,680
GPT-5 so open AI went through a pretty rigorous effort for several months you could say it could

1157
02:04:39,680 --> 02:04:44,400
be longer but nevertheless it's longer than you would have expected of trying to test the system

1158
02:04:44,400 --> 02:04:48,160
to see like what are the ways it goes wrong to make it very difficult for people while

1159
02:04:49,120 --> 02:04:56,240
somewhat difficult for people to ask things how do I make a bomb for one dollar or how do I

1160
02:04:57,280 --> 02:05:02,560
say I hate a certain group on Twitter in a way that doesn't get me blocked from Twitter ban from

1161
02:05:02,560 --> 02:05:11,360
Twitter those kinds of questions yeah so you basically use the system to do harm yeah is there

1162
02:05:11,360 --> 02:05:16,480
something you could say about ideas you have it's just on looking having thought about this problem

1163
02:05:16,560 --> 02:05:22,080
of AI safety how to release such system how to test such systems when you have them inside the company

1164
02:05:24,960 --> 02:05:25,440
yeah so

1165
02:05:28,880 --> 02:05:32,720
a lot of people say that the two biggest risks from large language models are

1166
02:05:37,600 --> 02:05:41,920
it's spreading disinformation harmful information of various types

1167
02:05:42,560 --> 02:05:48,320
of threats and second being used for offensive cyber weapon

1168
02:05:50,640 --> 02:05:54,480
design I think those are not the two greatest threats they're very serious threats and it's

1169
02:05:54,480 --> 02:06:00,400
wonderful that people are trying to mitigate them a much bigger elephant in the room is how

1170
02:06:00,400 --> 02:06:04,640
is this going to disrupt the economy in a huge way obviously and maybe take away a lot of the

1171
02:06:04,640 --> 02:06:10,080
most meaningful jobs and an even bigger one is the one we spent so much time talking about here

1172
02:06:10,080 --> 02:06:19,200
that that this becomes the bootloader for the more powerful AI right code connected to the

1173
02:06:19,200 --> 02:06:24,400
internet manipulate humans yeah and before we know when we have something else which is not

1174
02:06:24,400 --> 02:06:28,320
at all a large language model that looks nothing like it but which is way more intelligent and

1175
02:06:28,320 --> 02:06:35,440
capable and has goals and that's the that's the elephant in the room and and obviously no matter

1176
02:06:35,440 --> 02:06:41,120
how hard any of these companies have tried they that's not something that's easy for them to verify

1177
02:06:41,120 --> 02:06:45,920
with large language models and the only way to be really lower that risk a lot would be

1178
02:06:46,720 --> 02:06:52,560
to not let for example train not never let it read any code not train on that and not put it

1179
02:06:52,560 --> 02:07:01,680
into an API and not not give it access to so much information about how to manipulate humans

1180
02:07:02,240 --> 02:07:09,600
so but that doesn't mean you still can't make a lot a ton of money on them you know we're

1181
02:07:09,600 --> 02:07:17,680
going to just watch now this coming year right Microsoft is rolling out the new office suite

1182
02:07:17,680 --> 02:07:23,120
where you go into Microsoft Word and give it a prompt that it writes the whole text for you

1183
02:07:23,120 --> 02:07:27,440
and then you edit it and then you're like oh give me a PowerPoint version of this and it makes it

1184
02:07:27,440 --> 02:07:34,480
and now take the spreadsheet and blah and you know all of those things I think are you can debate

1185
02:07:34,480 --> 02:07:39,600
the economic impact of it and whether society is prepared to deal with this disruption but those

1186
02:07:39,600 --> 02:07:45,200
are not the things which that's not the elephant of the room that keeps me awake at night for wiping

1187
02:07:45,200 --> 02:07:51,840
out humanity and I think that's the biggest misunderstanding we have a lot of people think

1188
02:07:51,920 --> 02:07:57,360
they were scared of like automatic spreadsheets that's not the case that's not what Eliezer

1189
02:07:57,360 --> 02:08:05,760
was freaked out about either is there in terms the actual mechanism of how AI might kill all humans

1190
02:08:06,640 --> 02:08:12,880
so something you've been outspoken about you've talked about a lot is it autonomous weapon systems

1191
02:08:13,680 --> 02:08:21,120
so the use of AI in war is that one of the things that still you carry a concern for

1192
02:08:21,120 --> 02:08:25,120
as these systems become more and more powerful and carry a concern for it not that all humans

1193
02:08:25,120 --> 02:08:31,760
are going to get killed by slaughterbots but rather just this express route into Orwellian dystopia

1194
02:08:31,760 --> 02:08:36,480
where it becomes much easier for very few to kill very many and therefore it becomes very easy for

1195
02:08:36,480 --> 02:08:45,200
very few to dominate very many right if you want to know how I could kill all people just ask yourself

1196
02:08:45,200 --> 02:08:50,800
we humans have driven a lot of species extinct how do we do it you know we were smarter than them

1197
02:08:51,760 --> 02:08:57,920
usually we didn't do it even systematically by going around one on one one after the other and

1198
02:08:57,920 --> 02:09:02,000
stepping on them or shooting them or anything like that we just like chopped down their habitat

1199
02:09:02,000 --> 02:09:07,920
because we needed it for something else in some cases we did it by putting more carbon dioxide

1200
02:09:07,920 --> 02:09:13,840
in the atmosphere because of some reason that those animals didn't even understand and now

1201
02:09:13,840 --> 02:09:22,320
they're gone right so if if you're in AI and you just want to figure something out then you decide

1202
02:09:22,320 --> 02:09:29,040
you know we just really need them this space here to build more compute facilities you know

1203
02:09:31,680 --> 02:09:37,200
if that's the only goal it has you know we are just the sort of accidental roadkill along the

1204
02:09:37,200 --> 02:09:40,960
way and you could totally imagine yeah maybe this oxygen is kind of annoying because it

1205
02:09:40,960 --> 02:09:46,160
caused more corrosion so let's get rid of the oxygen and good luck surviving after that you

1206
02:09:46,160 --> 02:09:50,320
know I I'm not particularly concerned that they would want to kill us just because

1207
02:09:52,400 --> 02:09:56,320
that would be like a goal in itself you know when we

1208
02:09:58,400 --> 02:10:02,400
driven number we've driven a number of the elephant species extinct right it wasn't

1209
02:10:02,400 --> 02:10:10,240
because we didn't like elephants what the basic problem is you just don't want to give

1210
02:10:11,040 --> 02:10:15,760
you don't want to seed control over your planet to some other more intelligent

1211
02:10:16,320 --> 02:10:23,600
entity that doesn't share your goals it's that simple so which brings us to another key challenge

1212
02:10:23,600 --> 02:10:28,480
which AI safety researchers have been grappling with for a long time like how do you make AI

1213
02:10:28,880 --> 02:10:34,320
first of all understand our goals and then adopt our goals and then retain them as they get smarter

1214
02:10:34,320 --> 02:10:48,800
right and all three of those are really hard right like a human child first they're just not

1215
02:10:48,800 --> 02:10:56,080
smart enough to understand our goals they can't even talk and then eventually they're teenagers

1216
02:10:56,080 --> 02:11:02,480
and understand our goals just fine but they don't share yeah but there's fortunately a

1217
02:11:02,480 --> 02:11:05,840
magic phase in the middle where they're smart enough to understand our goals and malleable

1218
02:11:05,840 --> 02:11:10,480
enough that we can hopefully with good parenting and teach them right from wrong and instead good

1219
02:11:10,480 --> 02:11:17,840
goal is still good goals in them right and so those are all tough challenges with computers

1220
02:11:17,840 --> 02:11:21,760
and then you know even if you teach your kids good goals when they're little they might outgrow

1221
02:11:21,760 --> 02:11:27,120
them too and that's a challenge for machines and they keep improving so these are a lot of hard

1222
02:11:27,120 --> 02:11:35,120
hard challenges we're up for but I don't think any of them are insurmountable the fundamental

1223
02:11:35,120 --> 02:11:40,160
reason why eliezer looked so depressed when I last saw him was because he felt it just wasn't

1224
02:11:40,160 --> 02:11:46,640
enough time oh that not that it was unsolvable correct it's just not enough time he was hoping

1225
02:11:46,640 --> 02:11:51,360
that humanity was going to take this threat more seriously so we would have more time yeah

1226
02:11:51,360 --> 02:11:56,000
and now we don't have more time that's why the open letter is calling for more time

1227
02:11:59,760 --> 02:12:06,720
but even with time the ai alignment problem seems to be really difficult oh yeah

1228
02:12:08,080 --> 02:12:14,080
but it's also the most worthy problem the most important problem for humanity to ever solve

1229
02:12:14,080 --> 02:12:19,840
because if we solve that one lex that aligned the eye can help us solve all the other problems

1230
02:12:20,560 --> 02:12:26,160
because it seems like it has to have constant humility about his goal constantly question the goal

1231
02:12:27,520 --> 02:12:32,800
because as you optimize towards a particular goal and you start to achieve it that's when

1232
02:12:32,800 --> 02:12:37,440
you have the unintended consequences all the things you mentioned about so how do you enforce and

1233
02:12:37,440 --> 02:12:42,720
code a constant humility as your ability become better and better and better and better

1234
02:12:42,800 --> 02:12:45,600
steward professor steward russell berkeley who's also one of the

1235
02:12:47,840 --> 02:12:53,920
driving forces behind this letter he uh has a whole research program about this

1236
02:12:55,920 --> 02:13:01,200
i think of it as ai humility exactly although he calls it inverse reinforcement learning

1237
02:13:01,200 --> 02:13:06,160
and other nerdy terms but it's about exactly that instead of telling the ai here's his goal go

1238
02:13:06,160 --> 02:13:15,600
optimize the the bejesus out of it you tell it okay do what i want you to do but i'm not going

1239
02:13:15,600 --> 02:13:20,240
to tell you right now what it is i want you to do you need to figure it out so then you give the

1240
02:13:20,240 --> 02:13:24,000
incentives to be very humble and keep asking you questions along the way is this what you really

1241
02:13:24,000 --> 02:13:28,480
meant is this what you wanted and oh this the other thing i tried didn't work seemed like it

1242
02:13:28,480 --> 02:13:35,440
didn't work out right should i try it differently what's nice about this is it's not just philosophical

1243
02:13:35,440 --> 02:13:40,320
mumbo jumbo it's theorems and technical work that with more time i think it can make a lot of

1244
02:13:40,320 --> 02:13:46,240
progress and there are a lot of brilliant people now working on ai safety we just not we just need

1245
02:13:46,240 --> 02:13:51,280
to give them a bit more time but also not that many relative to the scale of the problem no

1246
02:13:51,280 --> 02:13:57,760
exactly there there should be at least as just like every university worth its name has some

1247
02:13:57,760 --> 02:14:03,040
cancer research going on in its biology department right every university that's computer that does

1248
02:14:03,040 --> 02:14:09,600
computer science should have a real effort in this area and it's nowhere near that this is

1249
02:14:09,600 --> 02:14:17,120
something i hope is changing now thanks to the gpt4 right so i i think um if there's a silver lining

1250
02:14:17,120 --> 02:14:22,560
to um what's happening here even though i think many people would wish it would have been rolled

1251
02:14:22,560 --> 02:14:30,080
out more carefully is that this might be the wake-up call that humanity needed to really

1252
02:14:30,560 --> 02:14:35,440
stop the stop fantasizing about this being 100 years off and stop fantasizing about this being

1253
02:14:35,440 --> 02:14:43,920
completely controllable and predictable because it's so obvious it's it's not predictable you know

1254
02:14:43,920 --> 02:14:53,200
why is it that open that that i think it was gpt chat gpt tried to persuade um a journalist

1255
02:14:53,760 --> 02:15:01,600
a journalist where was the gpt4 to divorce his wife you know it was not because the

1256
02:15:01,600 --> 02:15:08,560
engineers had built it was like let's put this in here and and screw a little bit with people

1257
02:15:09,520 --> 02:15:16,160
they hadn't predicted at all they built the giant black box and trained to predict the next word

1258
02:15:16,160 --> 02:15:20,880
and got all these emergent properties and oops it did this you know um

1259
02:15:23,840 --> 02:15:28,720
i think this is a very powerful wake-up call and anyone watching this who's not scared

1260
02:15:29,680 --> 02:15:35,040
i would encourage them to just play a bit more with these these tools they're out there now like

1261
02:15:35,040 --> 02:15:45,120
gpt4 and um it's a wake-up call it's first step once you've woken up uh then gotta slow down a

1262
02:15:45,120 --> 02:15:51,520
little bit the risky stuff to give a chance to all everyone who's woken up to to catch up with

1263
02:15:51,520 --> 02:15:58,240
us on the safety front you know what's interesting is you know mit that's computer science but in

1264
02:15:58,240 --> 02:16:03,360
general but let's just even say computer science curriculum how does the computer science curriculum

1265
02:16:03,360 --> 02:16:10,880
change now you mentioned you mentioned programming yeah like why would you be when i was coming up

1266
02:16:10,880 --> 02:16:17,120
programming as a prestigious position like why would you be dedicating crazy amounts of

1267
02:16:17,120 --> 02:16:21,680
time to become an excellent programmer like the nature of programming is fundamentally changing

1268
02:16:21,680 --> 02:16:29,120
the nature of our entire education system is completely torn on its head i has anyone been

1269
02:16:29,120 --> 02:16:34,560
able to like load that in and like think about because it's really turning i mean some english

1270
02:16:34,560 --> 02:16:39,840
professors or english teachers are beginning to really freak out now yeah right they give an essay

1271
02:16:39,840 --> 02:16:44,880
assignment and they get back all these fantastic pros like this is the style of Hemingway and

1272
02:16:44,880 --> 02:16:52,160
and then they realize they have to completely rethink and even you know just like we stopped

1273
02:16:52,160 --> 02:16:59,680
teaching writing uh script is that what you're saying english yeah handwritten yeah yeah when

1274
02:16:59,680 --> 02:17:03,680
when everybody started typing you know like so much of what we teach our kids today

1275
02:17:04,400 --> 02:17:17,920
yeah i mean that's uh everything is changing and it's changing very it is changing very quickly

1276
02:17:17,920 --> 02:17:22,960
and so much of us understanding how to deal with the big problems of the world is through the

1277
02:17:22,960 --> 02:17:28,000
education system and if the education system is being turned on its head then what what's next

1278
02:17:28,000 --> 02:17:33,280
it feels like having these kinds of conversations is essential to try to figure it out and everything

1279
02:17:33,280 --> 02:17:39,040
is happening so rapidly uh i don't think there's even you're speaking of safety what the broad

1280
02:17:39,040 --> 02:17:45,680
ai safety defined i don't think most universities have courses on ai safety no it's a philosophy

1281
02:17:45,680 --> 02:17:51,760
and like i'm an educator myself so it pains me to see this say this but i feel our education

1282
02:17:51,760 --> 02:17:58,320
right now is that completely obsoleted by what's happening you know you put a kid into first grade

1283
02:17:59,280 --> 02:18:03,760
and then uh you envisioning like and then they're gonna come out of high school 12 years later

1284
02:18:04,800 --> 02:18:08,720
and you've already pre-planned now what they're gonna learn when you're not even sure if there's

1285
02:18:08,720 --> 02:18:15,280
going to be any world left to come out to like clearly you need to have a much more

1286
02:18:16,240 --> 02:18:21,680
opportunistic education system that keeps adapting itself very rapidly as society re-adapts

1287
02:18:22,560 --> 02:18:28,000
the the skills that were really useful when the curriculum was written i mean how many of those

1288
02:18:28,080 --> 02:18:35,280
skills are going to get you a job in 12 years i mean seriously if we just linger on the gpt4 system

1289
02:18:35,280 --> 02:18:43,840
a little bit you kind of hinted at it especially talking about the importance of consciousness

1290
02:18:43,840 --> 02:18:53,680
in uh in the human mind with home ascents do you think gpt4 is conscious i love this question

1291
02:18:54,080 --> 02:19:00,240
so let's define consciousness first because in my experience like 90% of all arguments

1292
02:19:00,800 --> 02:19:04,480
about consciousness are allowed to the two people arguing having totally different

1293
02:19:04,480 --> 02:19:10,480
definitions of what it is and they're just shouting past each other i define consciousness

1294
02:19:11,840 --> 02:19:19,360
as subjective experience right now i'm experiencing colors and sounds and emotions you know

1295
02:19:20,160 --> 02:19:25,760
but does a self-driving car experience anything that's the question about whether it's conscious

1296
02:19:25,760 --> 02:19:33,440
or not right other people think you should define consciousness differently fine by me but then maybe

1297
02:19:33,440 --> 02:19:40,880
use a different word for it or they i'm gonna use consciousness for this at least um so um

1298
02:19:42,320 --> 02:19:49,200
but if people hate the yeah so is gpt4 conscious does gpt4 have subjective experience

1299
02:19:50,080 --> 02:19:54,640
short answer i don't know because we still don't know what it is that gives this wonderful

1300
02:19:54,640 --> 02:20:00,400
subjective experience that is kind of the meaning of our life right because meaning itself the

1301
02:20:00,400 --> 02:20:04,960
feeling of meaning is a subjective experience joy is a subjective experience love is a subjective

1302
02:20:04,960 --> 02:20:12,400
experience we don't know what it is i've written some papers about this a lot of people have

1303
02:20:13,360 --> 02:20:20,160
the julio tononi professor has stuck his neck out the farthest and written down actually very

1304
02:20:20,160 --> 02:20:26,800
bold mathematical conjecture for what's the essence of conscious information processing

1305
02:20:26,800 --> 02:20:33,840
he might be wrong he might be right but we should test it uh he postulates that consciousness has

1306
02:20:33,840 --> 02:20:40,400
to do with loops in the information processing so our brain has loops information can go round

1307
02:20:40,400 --> 02:20:45,680
and round in computer science nerd speak you call it a recurrent neural network where some

1308
02:20:45,680 --> 02:20:55,680
of the output gets fed back in again and with his mathematical formalism if it's a feed forward

1309
02:20:55,680 --> 02:21:00,800
neural network where information only goes in one direction like from your eye retina into the

1310
02:21:00,800 --> 02:21:04,800
back of your brain for example that's not conscious so he would predict that your retina itself isn't

1311
02:21:04,800 --> 02:21:13,200
conscious of anything or a video camera now the interesting thing about gpt4 is it's also just

1312
02:21:13,200 --> 02:21:20,880
one way flow of information so if tononi is right and gpt4 is a very intelligent zombie

1313
02:21:20,880 --> 02:21:29,040
that can do all this smart stuff but isn't experiencing anything and this is both a relief

1314
02:21:29,040 --> 02:21:32,800
in that you don't have if it's true you know in that you don't have to feel guilty about turning

1315
02:21:32,800 --> 02:21:39,840
off gpt4 and wiping its memory whenever a new user comes along i wouldn't like if someone used that

1316
02:21:39,840 --> 02:21:48,560
to me neuralized me like in men in black but it's also creepy that you can have very high

1317
02:21:48,560 --> 02:21:53,040
intelligence perhaps then it's not conscious because if we get replaced by machines

1318
02:21:55,760 --> 02:22:00,160
otherwise it's sad enough that humanity isn't here anymore because i kind of like humanity

1319
02:22:01,120 --> 02:22:06,160
but at least if the machines were conscious it could be like well but there are descendants and

1320
02:22:06,160 --> 02:22:11,360
maybe we they have our values and there are children but if if tononi is right and it's all

1321
02:22:11,360 --> 02:22:20,480
these are all transformers that are not in the sense of the of hollywood but in the sense of these

1322
02:22:20,480 --> 02:22:26,320
one-way direction neural networks so they're all the zombies that's the ultimate zombie apocalypse

1323
02:22:26,320 --> 02:22:30,880
now we have this universe that goes on with great construction projects and stuff but there's no one

1324
02:22:30,880 --> 02:22:38,080
experiencing anything that would be like the ultimate depressing future so i i actually think

1325
02:22:39,920 --> 02:22:45,040
as we move forward to the building world last day i should do more research on figuring out what

1326
02:22:45,040 --> 02:22:48,960
kind of information processing actually has experience because i think that's what it's all

1327
02:22:48,960 --> 02:22:55,680
about and i completely don't buy the dismissal that some people some people will say well this

1328
02:22:55,680 --> 02:23:01,600
is all bullshit because consciousness equals intelligence right it's obviously not true you

1329
02:23:01,600 --> 02:23:06,160
can have a lot of conscious experience when you're not really accomplishing any goals at all you're

1330
02:23:06,160 --> 02:23:13,600
just reflecting on something and you can sometimes um have things doing things that are quite

1331
02:23:13,600 --> 02:23:18,560
intelligent probably without being being conscious but i also worry that we humans won't

1332
02:23:19,440 --> 02:23:25,360
we'll discriminate against the AI systems that clearly exhibit consciousness that we will not

1333
02:23:25,360 --> 02:23:31,680
allow AI systems to have consciousness we'll come up with theories about measuring consciousness

1334
02:23:31,680 --> 02:23:37,920
that will say this is a lesser being and this is why i worry about that because maybe we humans

1335
02:23:37,920 --> 02:23:46,240
will create something that is better than us humans in the in the way that we find beautiful

1336
02:23:47,040 --> 02:23:53,680
which is they they have a deeper subjective experience of reality not only are they smarter

1337
02:23:53,680 --> 02:24:01,280
but they feel deeper and we humans will hate them for it as we as human history is shown

1338
02:24:02,000 --> 02:24:06,960
they'll be the other we'll try to suppress it they'll create conflict they'll create war

1339
02:24:06,960 --> 02:24:11,760
all of this i i worry about this too are you saying that we humans sometimes come up with

1340
02:24:11,760 --> 02:24:17,280
self-serving arguments no we would never do that would be well that's the danger here is uh

1341
02:24:17,840 --> 02:24:25,760
even in this early stages we might create something beautiful yeah and uh we'll erase its memory i i uh

1342
02:24:26,960 --> 02:24:34,480
was horrified as a kid when someone started boiling uh boiling lobsters like oh my god that

1343
02:24:34,480 --> 02:24:40,080
that's so cruel and some grown up there that's back in sweden so it doesn't feel pain i'm like

1344
02:24:40,080 --> 02:24:46,400
how do you know that oh scientists have shown that and then there was a recent study where they

1345
02:24:46,400 --> 02:24:50,800
show that lobsters actually do feel pain when you boil them so they banned lobsters boiling in

1346
02:24:50,800 --> 02:24:56,000
switzerland now you have to kill them in a different way first so presumably that scientific research

1347
02:24:56,560 --> 02:25:02,640
boiled out to someone asked the lobsters has this hurt survey so we do the same thing with

1348
02:25:02,640 --> 02:25:08,080
cruelty to farm animals also all these self-serving arguments for why they're fine and yeah so we

1349
02:25:08,080 --> 02:25:13,680
should certainly be watchful i think step one is just be humble and acknowledge that consciousness

1350
02:25:13,680 --> 02:25:18,960
is not the same thing as intelligence and i believe that consciousness still is a form of

1351
02:25:18,960 --> 02:25:23,280
information processing where it's really information being aware of itself in a certain way and

1352
02:25:23,280 --> 02:25:27,440
let's study it and give ourselves a little bit of time and i think we will be able to figure out

1353
02:25:28,080 --> 02:25:34,400
actually what it is that causes consciousness and then we can make probably unconscious robots

1354
02:25:34,400 --> 02:25:38,800
that do the boring jobs that we would feel are immoral to get the machines but if you have a

1355
02:25:38,800 --> 02:25:44,800
companion robot taking care of your mom or something like that you would probably want it

1356
02:25:44,800 --> 02:25:51,040
to be conscious right so the emotions it seems to display aren't fake all these things can be

1357
02:25:52,560 --> 02:25:58,720
done in a good way if we give ourselves a little bit of time and don't run and take on this challenge

1358
02:25:59,280 --> 02:26:05,600
is there something you could say to the timeline that you think about about the development of AGI

1359
02:26:07,040 --> 02:26:12,560
depending on the day i'm sure that changes for you but when do you think there will be a really big

1360
02:26:12,560 --> 02:26:19,120
leap in intelligence where you definitively say we have built AGI do you think it's one year from

1361
02:26:19,120 --> 02:26:28,000
now five years from now 10 20 50 what's your gut say honestly

1362
02:26:31,360 --> 02:26:35,760
for the past decade i've deliberately given very long timelines because i didn't want to fuel some

1363
02:26:35,760 --> 02:26:41,600
kind of stupid malloc race yeah um but i think that cat has really left the bag now

1364
02:26:42,560 --> 02:26:50,640
and i think it might be very very close i don't think the microsoft paper is totally off when

1365
02:26:50,640 --> 02:26:58,160
they say that there are some glimmers of AGI it's not AGI yet it's not an agent there's a lot of

1366
02:26:58,160 --> 02:27:07,200
things it can't do but um i wouldn't bet very strongly against it happening very soon that's

1367
02:27:07,200 --> 02:27:11,840
why we decided to do this open letter because you know if there's ever been a time to pause

1368
02:27:12,960 --> 02:27:21,040
you know it's today there's a feeling like this GPT-4 is a big transition into waking everybody

1369
02:27:21,040 --> 02:27:30,000
out to the effectiveness of the system and so the next version will be big yeah and if that

1370
02:27:30,000 --> 02:27:34,800
next one isn't AGI maybe the next next one will and there are many companies trying to do these

1371
02:27:34,800 --> 02:27:39,920
things and the basic architecture of them is not some sort of super well-kept secret so

1372
02:27:40,800 --> 02:27:46,800
this is this is a time to a lot of people have said for many years that there will come a time

1373
02:27:46,800 --> 02:27:51,120
when we want to pause a little bit that time is now

1374
02:27:54,240 --> 02:28:01,680
you have spoken about and thought about nuclear war a lot over the past year we've

1375
02:28:01,920 --> 02:28:11,280
we've seemingly have come closest to the precipice of nuclear war than uh at least in my lifetime

1376
02:28:12,720 --> 02:28:18,320
yeah what do you learn about human nature from that it's our old friend Malak again

1377
02:28:19,440 --> 02:28:27,200
it's really scary to see it where America doesn't want there to be a nuclear war Russia

1378
02:28:27,200 --> 02:28:31,440
doesn't want there to be a global nuclear war either we know we both know that it's just

1379
02:28:31,440 --> 02:28:36,240
being others if we just try to do it it both sides try to launch first it's just another

1380
02:28:36,240 --> 02:28:41,440
suicide race right so why are we why is it the way you said that this is the closest we've come

1381
02:28:41,440 --> 02:28:47,040
since 1962 in fact i think we've come closer now than even the Cuban Missile Crisis it's

1382
02:28:47,040 --> 02:28:54,640
because of Malak you know you you have these other forces on one hand you have the west

1383
02:28:55,600 --> 02:29:04,160
saying that we have to drive Russia out of Ukraine it's a matter of pride and we've staked so much

1384
02:29:04,160 --> 02:29:10,800
on it that it would be seen as a huge loss of the credibility of the west if we don't drive

1385
02:29:10,800 --> 02:29:20,800
Russia out entirely of the Ukraine and on the other hand you have Russia who has and you have

1386
02:29:20,800 --> 02:29:25,680
the Russian leadership who knows that if they get completely driven out of Ukraine you know

1387
02:29:27,200 --> 02:29:35,200
it might it's not just going to be very humiliating for them but they might it often happens when

1388
02:29:35,200 --> 02:29:40,080
countries lose wars that things don't go so well for their leadership either like you remember when

1389
02:29:40,080 --> 02:29:48,480
Argentina invaded the Falkland Islands the the military junta that ordered that right people

1390
02:29:48,480 --> 02:29:54,560
were cheering on the streets at first when they took it and then when they got their butt kicked

1391
02:29:55,200 --> 02:30:03,120
by the British you know what happened to those guys they were out and I believe those were

1392
02:30:03,120 --> 02:30:09,440
still alive or in jail now right so so you know the Russian leadership is entirely cornered where

1393
02:30:09,440 --> 02:30:17,280
they know that just getting driven out of Ukraine is not an option and

1394
02:30:20,640 --> 02:30:26,720
so this to me is a typical example of Malik you you have these incentives of the two parties

1395
02:30:28,000 --> 02:30:32,000
where both of them are just driven to escalate more and more right if Russia starts losing in

1396
02:30:32,000 --> 02:30:37,520
the conventional warfare the only thing they can do is to back against the war is to keep

1397
02:30:37,520 --> 02:30:43,920
escalating and but and the west has put itself in the in the situation now we're sort of already

1398
02:30:43,920 --> 02:30:48,720
committed to the dry rush out so the only option the west has is to call Russia's bluff and keep

1399
02:30:48,720 --> 02:30:54,960
sending in more weapons this really bothers me because Malik can sometimes drive competing

1400
02:30:54,960 --> 02:31:01,200
parties to do something which is ultimately just really bad for both of them and you know

1401
02:31:01,280 --> 02:31:08,800
what makes me even more worried is not just that it's difficult to see an ending

1402
02:31:10,160 --> 02:31:15,200
a quick peaceful ending to this tragedy that doesn't involve some horrible escalation

1403
02:31:16,240 --> 02:31:20,640
but also that we understand more clearly now just how horrible it was going to be

1404
02:31:21,440 --> 02:31:26,800
there was an amazing paper that was published in Nature Food this August

1405
02:31:27,360 --> 02:31:31,440
by some of the top researchers who've been studying nuclear winter for a long time and what

1406
02:31:31,440 --> 02:31:42,400
they basically did was they combined climate models with food agricultural models so instead

1407
02:31:42,400 --> 02:31:46,080
of just saying yeah you know it gets really cold blah blah blah they figured out actually how

1408
02:31:46,080 --> 02:31:51,360
many people would die in the different different countries and it's uh it's pretty mind blowing

1409
02:31:51,360 --> 02:31:54,400
you know so basically what happens you know is that the thing that kills the most people is not

1410
02:31:55,120 --> 02:32:02,800
the explosions it's not the radioactivity it's not the EMP mayhem it's not the rampaging mobs

1411
02:32:03,440 --> 02:32:08,000
foraging food no it's the fact that you get so much smoke coming up from the burning cities

1412
02:32:08,000 --> 02:32:17,120
into the stratosphere that spreads around the earth from the jet streams so in typical models

1413
02:32:17,120 --> 02:32:23,840
you get like 10 years or so where it's just crazy cold and during the first year or

1414
02:32:23,840 --> 02:32:32,240
after the the war and their models the temperature drops in in Nebraska and in the Ukraine

1415
02:32:33,120 --> 02:32:42,000
bread baskets you know by like 20 Celsius or so if I remember no yeah 20 30 Celsius depending

1416
02:32:42,000 --> 02:32:47,040
on where you are 40 Celsius in some places which is you know 40 Fahrenheit to 80 Fahrenheit colder

1417
02:32:47,040 --> 02:32:53,600
than what it would normally be so you know I'm not good at farming but uh if it's snowing

1418
02:32:54,640 --> 02:32:59,280
if it drops below freezing pretty much most days in July and then like that's not good so

1419
02:32:59,280 --> 02:33:03,360
they worked out they put this into their farming models and what they found was really interesting

1420
02:33:04,000 --> 02:33:07,600
the countries that get the most hard hit are the ones in the northern hemisphere

1421
02:33:08,960 --> 02:33:15,360
so in in the US and and one model they had about 99 percent of all Americans starving to death

1422
02:33:16,240 --> 02:33:20,800
in Russia and China and Europe also about 99 98 percent starving to death

1423
02:33:22,080 --> 02:33:26,960
so you might be like oh it's kind of poetic justice that both the Russians and the Americans

1424
02:33:28,080 --> 02:33:31,360
99 percent of them have to pay for it because it was their bombs that did it but

1425
02:33:32,000 --> 02:33:37,280
you know that doesn't particularly cheer people up in Sweden or other random countries that have

1426
02:33:37,280 --> 02:33:45,280
nothing to do with it right and um it uh I think it hasn't entered the mainstream

1427
02:33:48,400 --> 02:33:55,600
not understanding very much just like how bad this is most people especially a lot of people

1428
02:33:55,600 --> 02:33:59,200
in decision-making positions still think of nuclear weapons as something that makes you powerful

1429
02:33:59,200 --> 02:34:06,560
uh scary powerful they don't think of it as something where uh yeah just

1430
02:34:08,160 --> 02:34:11,520
to within a percent or two you know we're all just just gonna starve to death

1431
02:34:12,880 --> 02:34:22,800
and um and starving to death is is um the worst way to die as holly molly is all all the

1432
02:34:22,800 --> 02:34:28,960
famines in history show the torture involved in that probably brings out the worst in people also

1433
02:34:29,760 --> 02:34:36,880
when when people are desperate like this it's not so some people I've heard some people say that

1434
02:34:38,000 --> 02:34:41,840
if that's what's gonna happen they'd rather be at round zero and just get vaporized you know

1435
02:34:43,520 --> 02:34:50,400
but uh so but I think people underestimate the risk of this because they they

1436
02:34:51,360 --> 02:34:55,760
they aren't afraid of malloc they think oh it's just gonna be because humans don't want this so

1437
02:34:55,760 --> 02:34:59,600
it's not gonna happen that's the whole point the malloc that things happen that nobody wanted

1438
02:35:00,240 --> 02:35:04,080
and that applies to nuclear weapons and that applies to agi

1439
02:35:06,080 --> 02:35:11,120
exactly and it applies to some of the things that people have gotten most upset with capitalism

1440
02:35:11,120 --> 02:35:17,280
for also right where everybody was just kind of trapped you know it it's not to see if some

1441
02:35:17,280 --> 02:35:24,960
company does something uh it causes a lot of harm and not that the ceo is a bad person

1442
02:35:25,520 --> 02:35:30,560
but she or he knew that you know that the other all the other companies were doing this too so

1443
02:35:30,560 --> 02:35:41,120
malloc is um as a formable foe I hope which someone makes good movies so we can see who

1444
02:35:41,120 --> 02:35:46,480
the real enemy is we don't because we're not fighting against each other uh malloc makes

1445
02:35:46,480 --> 02:35:54,320
us fight against each other that's more that's what malloc superpower is the hope here is any

1446
02:35:54,320 --> 02:36:00,400
kind of technology or the mechanism that lets us instead realize that we're fighting the wrong

1447
02:36:00,400 --> 02:36:07,520
enemy right now it's such a fascinating battle it's not us versus them it's us versus it yeah yeah

1448
02:36:08,160 --> 02:36:14,160
we are fighting malloc for human survival yeah we as a civilization have you seen the movie

1449
02:36:14,240 --> 02:36:20,960
needful things it's a steven king novel I love steven king and uh max von sudo of

1450
02:36:20,960 --> 02:36:26,160
Swedish actors playing the guys it's brilliant exactly I just thought I hadn't thought about

1451
02:36:26,160 --> 02:36:32,400
that until now but that's the closest I've seen to a a movie about malloc I don't want to spoil

1452
02:36:32,400 --> 02:36:39,360
the film for anyone who wants to watch it but basically it's about this guy who turns out to

1453
02:36:39,360 --> 02:36:43,440
you can interpret him as the devil or whatever but he doesn't actually ever go around or

1454
02:36:43,440 --> 02:36:48,560
and kill people or torture people will go burning coal or anything he makes everybody fight each

1455
02:36:48,560 --> 02:36:53,440
other it makes everybody hate fear each other hate each other and then kill each other so that

1456
02:36:53,440 --> 02:37:01,440
that's the movie about malloc you know love is the answer that seems to be um one of the ways to

1457
02:37:01,440 --> 02:37:10,960
fight malloc is by um compassion by seeing the common humanity yes yes and to not sound so we

1458
02:37:10,960 --> 02:37:17,680
don't sound like like uh was it kumbaya tree huggers here right we're not just saying love

1459
02:37:17,680 --> 02:37:24,960
and peace man we're trying to actually help people understand the true facts about the other side

1460
02:37:26,720 --> 02:37:35,440
and feel the compassion because the truth makes you more compassionate right

1461
02:37:36,240 --> 02:37:45,840
so I that's why I really like using AI for truth and for truth seeking technologies can

1462
02:37:48,640 --> 02:37:56,560
that can as a result you know get us more love than hate and and even if you can't get love you

1463
02:37:56,560 --> 02:38:02,000
know settle for settle for some understanding which already gives compassion if someone is like you

1464
02:38:02,000 --> 02:38:08,320
know I really disagree with you lex but I can see why you're where you're coming from you're not a

1465
02:38:10,400 --> 02:38:14,320
bad person who needs to be destroyed but I disagree with you and I'm happy to have an

1466
02:38:14,320 --> 02:38:19,600
argument about it you know that's a lot of progress compared to where we are 2023 in

1467
02:38:19,600 --> 02:38:25,760
the public space wouldn't you say if we solve the AI safety problem as we've talked about

1468
02:38:26,480 --> 02:38:34,240
and then you max tag mark who has been talking about this uh for many years get to sit down

1469
02:38:34,240 --> 02:38:40,640
with the agi with the early agi system on a beach with a drink uh what what what kind of what would

1470
02:38:40,640 --> 02:38:46,880
you ask her what kind of question would you ask what would you talk about something so much smarter

1471
02:38:46,880 --> 02:38:53,520
than you would be would you be afraid we're gonna get me with a really zinger of a question

1472
02:38:54,080 --> 02:39:02,720
would you be afraid to ask some questions yeah so I'm not afraid of the truth I'm very humble I

1473
02:39:02,720 --> 02:39:08,960
know I'm just a meat bag you with all these flaws you know but yeah I I have the I we talked a lot

1474
02:39:08,960 --> 02:39:13,680
about homo sentience I've really already tried that for a long time with myself just so that is

1475
02:39:13,680 --> 02:39:18,800
what's really valuable about being alive for me is that I have these meaningful experiences

1476
02:39:19,680 --> 02:39:25,520
it's not that I'm have what I'm good at this or good at that or whatever there's so much I suck at

1477
02:39:25,520 --> 02:39:31,440
and so you're not afraid for the system to show you just how dumb you are no no in fact my son

1478
02:39:31,440 --> 02:39:37,440
reminds me that you could find out how dumb you are in terms of physics how little how little

1479
02:39:37,440 --> 02:39:46,400
we humans understand I'm cool with that I think I think um so I can't waffle my way out of this

1480
02:39:46,400 --> 02:39:52,880
question it's a fair one I think given that I'm a really really curious person that's

1481
02:39:54,160 --> 02:39:58,160
really the defining part of who I am I'm so curious

1482
02:40:03,440 --> 02:40:10,800
uh I have some physics questions I love to love to understand I have some questions about

1483
02:40:11,440 --> 02:40:15,600
consciousness about the nature of reality I would just really really love to understand also

1484
02:40:17,120 --> 02:40:20,240
I could tell you one for example that I've been obsessing about a lot recently

1485
02:40:22,960 --> 02:40:30,000
so I believe that so suppose tenoni is right as opposed there are some information processing

1486
02:40:30,000 --> 02:40:34,400
systems that are conscious and some they're not suppose you can even make reasonably smart things

1487
02:40:34,400 --> 02:40:40,000
like gpt4 they're not conscious but you can also make them conscious here's the question that keeps

1488
02:40:40,000 --> 02:40:48,560
me naked night is it the case that the unconscious zombie systems that are really intelligent

1489
02:40:48,560 --> 02:40:53,920
are also really efficient so they're really inefficient so that when you try to make things

1490
02:40:53,920 --> 02:41:00,320
more efficient we still naturally be a pressure to do they become conscious I'm kind of hoping

1491
02:41:01,120 --> 02:41:05,680
that that's correct and I do you want me to give you a hand away the argument for it please you know

1492
02:41:05,680 --> 02:41:13,680
like in my lab again every time we look at how how these large language models do something we

1493
02:41:13,680 --> 02:41:18,640
see they do them in really dumb ways and you could you could make it make it better if if you uh

1494
02:41:20,080 --> 02:41:25,680
we have loops in our computer language for a reason the code would get way way longer if you

1495
02:41:25,680 --> 02:41:34,240
weren't allowed to use them it's more efficient to have the loops and in order to have self-reflection

1496
02:41:34,240 --> 02:41:39,520
whether it's conscious or not right even an operating system knows things about itself right

1497
02:41:41,520 --> 02:41:49,440
you need to have loops already right so I think this is I'm waving my hands a lot but I suspect that

1498
02:41:51,840 --> 02:41:55,440
the most efficient way of implementing a given level of intelligence

1499
02:41:55,760 --> 02:42:04,800
has loops in it the self-reflection can and will be conscious isn't that great news yes if it's

1500
02:42:04,800 --> 02:42:09,840
true it's wonderful because then we don't have to fear the ultimate zombie apocalypse and I think

1501
02:42:09,840 --> 02:42:16,640
if you look at our brains actually our brains are part zombie and part conscious

1502
02:42:17,520 --> 02:42:28,320
when I open my eyes I immediately take all these pixels that hit my on my retina right and like oh

1503
02:42:28,320 --> 02:42:34,000
that's Lex but I have no freaking clue of how I did that computation it's actually quite complicated

1504
02:42:34,000 --> 02:42:40,720
right it was only relatively recently we could even do it well with machines right you get a bunch

1505
02:42:40,720 --> 02:42:44,880
of information processing happening in my retina then it goes to the lateral genicular nucleus

1506
02:42:44,880 --> 02:42:50,640
my thalamus and the vision the area v1 v2 v4 and the fusiform face area here that Nancy can

1507
02:42:50,640 --> 02:42:55,520
wish her at MIT invented and blah blah blah blah and I have no freaking clue how that worked right

1508
02:42:56,240 --> 02:43:02,400
it feels to me subjectively like my conscious module just got a little email say

1509
02:43:03,200 --> 02:43:14,240
face facial processing uh fit task complete it's Lex yeah and I'm gonna just go with that right

1510
02:43:15,040 --> 02:43:21,760
so uh this fits perfectly with Tanone's model because this was all one way information processing

1511
02:43:22,400 --> 02:43:30,080
mainly and uh it turned out for that particular task that's all you needed and it probably was

1512
02:43:30,080 --> 02:43:34,880
kind of the most efficient way to do it but there were a lot of other things that we associated with

1513
02:43:34,880 --> 02:43:40,160
higher intelligence and planning and and so on and so forth where you kind of want to have loops and

1514
02:43:40,160 --> 02:43:47,360
be able to ruminate and self-reflect and introspect and so on where my hunch is that if you want to

1515
02:43:47,360 --> 02:43:52,320
fake that with a zombie system that just all goes one way you have to like unroll those loops and it

1516
02:43:52,320 --> 02:43:57,760
just really really long and it's much more inefficient so I'm actually hopeful that AI

1517
02:43:58,560 --> 02:44:02,560
if in the future we have all these very sublime and interesting machines that do cool things

1518
02:44:03,360 --> 02:44:09,840
and are lined with us that they will at least they will also have consciousness for the kind of

1519
02:44:09,840 --> 02:44:16,800
these things that we do that great intelligence is also correlated to great consciousness or a deep

1520
02:44:16,800 --> 02:44:23,360
kind of consciousness yes so that's a happy thought for me because the zombie apocalypse

1521
02:44:23,360 --> 02:44:28,400
really is my worst nightmare of all it would be like adding insult to injury not only did we get

1522
02:44:28,400 --> 02:44:34,560
replaced but we friggin replaced ourselves by zombies like how dumb can we be uh that's such a

1523
02:44:34,560 --> 02:44:39,840
beautiful vision and that's actually a provable one that's one that we humans can uh intuit and

1524
02:44:39,840 --> 02:44:45,200
prove that those two things are correlated as we start to understand what it means to be intelligent

1525
02:44:45,200 --> 02:44:51,600
and what it means to be conscious which these systems uh early AGI like systems will help us

1526
02:44:51,600 --> 02:44:56,480
understand and I just want to say one more thing which is super important most of my colleagues

1527
02:44:56,480 --> 02:45:00,080
when I started going on about consciousness tell me that it's all bullshit and I should stop talking

1528
02:45:00,080 --> 02:45:06,800
about it I hear a little inner voice from my father and from my mom saying keep talking about it

1529
02:45:06,800 --> 02:45:15,200
because I think they're wrong and and and the main way to convince people like that that they're

1530
02:45:15,200 --> 02:45:20,080
wrong if they say that consciousness is just equal to intelligence is to ask them what's wrong with

1531
02:45:20,080 --> 02:45:28,160
torture now why are you against torture if it's just about you know these these particles

1532
02:45:28,160 --> 02:45:32,720
moving this way around on that way and there is no such thing as subjective experience what's

1533
02:45:32,720 --> 02:45:38,160
wrong with torture I mean do you have a good comeback to that no it seems like suffering

1534
02:45:38,720 --> 02:45:45,120
suffering imposed on other humans is somehow deeply wrong in a way that intelligence doesn't

1535
02:45:45,120 --> 02:45:51,360
quite explain and if someone tells me well you know it's just an illusion consciousness

1536
02:45:53,040 --> 02:45:59,360
whatever you know I like to invite them the next time they're having surgery to do it without

1537
02:45:59,360 --> 02:46:05,680
anesthesia like what is anesthesia really doing if you have it you can have a local anesthesia

1538
02:46:05,680 --> 02:46:09,440
when you're awake I had that when they fixed my shoulder I was super entertaining uh

1539
02:46:10,080 --> 02:46:16,320
uh what was that that it did it just removed my subjective experience of pain it didn't change

1540
02:46:16,320 --> 02:46:21,120
anything about what was actually happening in my shoulder right so if someone says that's all

1541
02:46:21,120 --> 02:46:28,800
bullshit skip the anesthesia as my advice this is incredibly central it could be fundamental

1542
02:46:28,800 --> 02:46:35,120
to whatever this thing we have going on here it is fundamental because we're we what we feel

1543
02:46:35,120 --> 02:46:41,280
is so fundamental is suffering and joy and pleasure and meaning and

1544
02:46:43,520 --> 02:46:49,680
that's all those are all subjective experiences there and let's not those are the elephant in

1545
02:46:49,680 --> 02:46:53,280
the room that's what makes life worth living and that's what can make it horrible if it's just

1546
02:46:53,280 --> 02:46:58,800
those are suffering so let's not make the mistake of saying that that's all bullshit and let's not

1547
02:46:58,880 --> 02:47:06,160
make the mistake of uh not instilling the AI systems with that same thing that makes us

1548
02:47:07,760 --> 02:47:13,360
special yeah max it's a huge honor that you will sit down to me the first time

1549
02:47:14,400 --> 02:47:19,280
on the first episode of this podcast it's a huge honor to sit down again and talk about this what

1550
02:47:19,280 --> 02:47:26,160
I think is the most important topic the most important problem that we humans have to face

1551
02:47:26,800 --> 02:47:33,280
and hopefully solve yeah well the honor is all mine and I'm I'm so grateful to you for making

1552
02:47:33,280 --> 02:47:37,920
more people aware of this fact that humanity has reached the most important fork in the road

1553
02:47:37,920 --> 02:47:44,080
ever in its history and let's turn in the correct direction thanks for listening to this conversation

1554
02:47:44,080 --> 02:47:49,520
with max tag mark to support this podcast please check out our sponsors in the description and

1555
02:47:49,520 --> 02:47:55,680
now let me leave you with some words from frank harbert history is a constant race

1556
02:47:56,320 --> 02:48:03,360
between invention and catastrophe thank you for listening and hope to see you next time

