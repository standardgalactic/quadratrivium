start	end	text
0	5760	The following is a conversation with Marcus Hutter, senior research scientist at Google DeepMind.
6640	11680	Throughout his career of research, including with JÃ¶rgen Schmidhuber and Shane Legg,
11680	16320	he has proposed a lot of interesting ideas in and around the field of artificial general
16320	23760	intelligence, including the development of IEXI, spelled AIXI model, which is the mathematical
23760	30560	approach to AGI that incorporates ideas of Komogorov complexity, Solominov induction,
30560	38320	and reinforcement learning. In 2006, Marcus launched the 50,000-euro Hutter prize for
38320	43840	lossless compression of human knowledge. The idea behind this prize is that the ability to
43840	52080	compress well is closely related to intelligence. This, to me, is a profound idea. Specifically,
52160	57040	if you can compress the first 100 megabytes or one gigabyte of Wikipedia better than your
57040	63600	predecessors, your compressor likely has to also be smarter. The intention of this prize
63600	68160	is to encourage the development of intelligent compressors as a path to AGI.
69520	75440	In conjunction with his podcast release just a few days ago, Marcus announced a 10x increase
75440	83120	in several aspects of this prize, including the money, to 500,000 euros. The better your
83120	88160	compressor works relative to the previous winners, the higher fraction of that prize money is awarded
88160	96400	to you. You can learn more about it if you google simply Hutter prize. I'm a big fan of benchmarks
96400	101920	for developing AI systems, and the Hutter prize may indeed be one that will spark some good ideas
101920	108560	for approaches that will make progress on the path of developing AGI systems. This is the
108560	113760	Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it 5 stars on Apple
113760	119200	Podcasts, support it on Patreon, or simply connect with me on Twitter at Lex Freedman,
119200	126400	spelled F-R-I-D-M-A-N. As usual, I'll do one or two minutes of ads now and never any ads in the
126400	131280	middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the
131360	137680	listening experience. This show is presented by Cash App, the number one finance app in the App Store.
137680	144480	When you get it, use code LEX Podcast. Cash App lets you send money to friends, buy Bitcoin,
144480	150000	and invest in the stock market with as little as $1. Broker services are provided by Cash App
150000	157040	Investing, a subsidiary of Square, and member SIPC. Since Cash App allows you to send and receive
157040	162560	money digitally, peer-to-peer, and security in all digital transactions very important,
162560	168960	let me mention the PCI Data Security Standard that Cash App is compliant with. I'm a big fan of
168960	176080	standards for safety and security. PCI DSS is a good example of that, where a bunch of competitors
176080	181600	got together and agreed that there needs to be a global standard around the security of transactions.
182480	189600	Now, we just need to do the same for autonomous vehicles and AI systems in general. So again,
189600	194880	if you get Cash App from the App Store or Google Play and use the code LEX Podcast,
194880	201120	you'll get $10 and Cash App will also donate $10 to FIRST, one of my favorite organizations
201120	206480	that is helping to advance robotics and STEM education for young people around the world.
207440	211360	And now, here's my conversation with Marcus Hodder.
212400	216880	Do you think of the universe as a computer or maybe an information processing system?
216880	222320	Let's go with a big question first. Okay, I have a big question first. I think it's a very
222320	229360	interesting hypothesis or idea, and I have a background in physics, so I know a little bit
229360	234320	about physical theories, the standard model of particle physics and general relativity theory,
234400	238480	and they are amazing and describe virtually everything in the universe, and they're all in
238480	243520	a sense, computable theories. I mean, they're very hard to compute, and it's very elegant,
243520	248400	simple theories which describe virtually everything in the universe. So there's a strong
249040	256960	indication that somehow the universe is computable, but it's a plausible hypothesis.
257680	262160	So what do you think, just like you said, general relativity, quantum field theory,
262160	268160	what do you think that the laws of physics are so nice and beautiful and simple and compressible?
269040	276000	Do you think our universe was designed is naturally this way? Are we just focusing on the
276000	282720	parts that are especially compressible? Are human minds just enjoying something about that simplicity
282720	286640	and, in fact, there's other things that are not so compressible?
286640	292000	No, I strongly believe and I'm pretty convinced that the universe is inherently beautiful
292000	296880	elegant and simple and described by these equations, and we're not just picking that.
297440	304160	I mean, if there were some phenomena which cannot be neatly described, scientists would try that,
304160	307680	right? And, you know, there's biology which is more messy, but we understand that it's an
307680	312320	emergent phenomena, and, you know, it's complex systems, but they still follow the same rules,
312320	316640	right? Of quantum and electrodynamics, all of chemistry follows that, and we know that. I mean,
316640	320720	we cannot compute everything because we have limited computational resources. No, I think it's
320720	325440	not a bias of the humans, but it's objectively simple. I mean, of course, you never know, you
325440	330480	know, maybe there's some corners very far out in the universe or super, super tiny below the
330480	339520	nucleus of atoms or, well, parallel universes where which are not nice and simple, but there's no
339520	344240	evidence for that, and we should apply Occam's razor and, you know, choose the simple streak
344240	347920	consistent with it, but also it's a little bit self-referential.
347920	354480	So maybe a quick pause. What is Occam's razor? So Occam's razor says that you should not multiply
354480	361760	entities beyond necessity, which sort of if you translate it to proper English means, and, you
361760	366880	know, in the scientific context means that if you have two theories or hypotheses or models which
366880	372400	equally well describe the phenomenon you're studying or the data, you should choose the more
372400	380400	simple one. So that's just a principle or sort of that's not like a provable law, perhaps, perhaps
380400	387040	we'll kind of discuss it and think about it, but what's the intuition of why the simpler answer
388000	394960	is the one that is likely to be more correct descriptor of whatever we're talking about?
394960	400400	I believe that Occam's razor is probably the most important principle in science. I mean,
400400	407280	of course, we need logical deduction and we do experimental design, but science is about finding
407840	413600	understanding the world, finding models of the world, and we can come up with crazy complex models
413600	419040	which, you know, explain everything but predict nothing, but the simple model seemed to have
419040	426960	predictive power, and it's a valid question why. And the two answers to that, you can just accept
427120	432000	that as the principle of science, and we use this principle and it seems to be successful.
432720	438480	We don't know why, but it just happens to be. Or you can try, you know, find another principle
438480	445440	which explains Occam's razor. And if we start with assumption that the world is governed by
445440	453600	simple rules, then there's a bias to our simplicity, and applying Occam's razor
454160	458880	is the mechanism to finding these rules. And actually, in a more quantitative sense,
458880	462720	and we come back to that later in case of somnambular deduction, you can rigorously prove
462720	468160	that you assume that the world is simple, then Occam's razor is the best you can do in a certain
468160	475520	sense. So, I apologize for the romanticized question, but why do you think outside of its
475520	480000	effectiveness, why do we do you think we find simplicity so appealing as human beings? Why
480000	487120	does it just, why does E equals MC squared seem so beautiful to us humans?
488240	494240	I guess mostly, in general, many things can be explained by an evolutionary argument.
494880	499280	And, you know, there's some artifacts in humans which, you know, are just artifacts and not an
499280	508240	evolutionary necessary. But with this beauty and simplicity, it's, I believe, at least the core
508800	515840	is about, like science, finding regularities in the world, understanding the world,
515840	522400	which is necessary for survival, right? You know, if I look at a bush, right, and I just see noise,
522400	527200	and there is a tiger, right, and eats me, then I'm dead. But if I try to find a pattern, and
527200	534800	we know that humans are prone to find more patterns in data than they are, you know, like, you know,
534880	541040	Mars face and all these things, but these bias towards finding patterns, even if they are not,
541040	545040	but I mean, it's best, of course, if they are, yeah, helps us for survival.
546480	551600	Yeah, that's fascinating. I haven't thought really about the, I thought I just loved science, but
552800	560320	indeed, from in terms of just for survival purposes, there is an evolutionary argument for why we find
561280	568080	the work of Einstein so beautiful. Maybe a quick small tangent, could you describe what
568080	576880	Solomon of induction is? Yeah, so that's a theory which I claim, and Resolominov sort of claimed
576880	582800	a long time ago, that this solves the big philosophical problem of induction. And I believe
582800	589360	the claim is essentially true. And what it does is the following. So, okay, for the
590320	598720	picky listener, induction can be interpreted narrowly and wildly narrow means inferring models from data.
600560	605920	And widely means also then using these models for doing predictions or predictions also part of
605920	610640	the induction. So I'm a little sloppy sort of with the terminology and maybe that comes from
611200	614400	Resolominov, you know, being sloppy, maybe I shouldn't say that.
615920	619920	He can't complain anymore. So let me explain a little bit this theory.
620880	626880	In simple terms, so assume we have a data sequence, make it very simple, the simplest one say 111111 and
626880	632400	you see if 100 ones, what do you think comes next? The natural answer, I'm going to speed up a little
632400	638720	bit, the natural answer is of course, you know, one. Okay, and the question is why? Okay, well,
638720	643760	we see a pattern there. Yeah, okay, there's a one and we repeat it. And why should it suddenly
643760	648480	after 100 ones be different? So what we're looking for is simple explanations or models
649120	654480	for the data we have. And now the question is a model has to be presented in a certain language
655360	660720	in which language to be used. In science, we want formal languages, and we can use mathematics,
660720	666320	or we can use programs on a computer. So abstractly on a Turing machine, for instance,
666320	671600	or can be a general purpose computer. So and there are of course, lots of models of you can
671600	676560	say maybe it's 101 and then 100 zeros and 100 ones, that's a model, right? But they're simpler
676560	682640	models, there's a model print one loop. Now that also explains the data. And if you push
682640	688480	that to the extreme, you are looking for the shortest program, which if you run this program
688480	694720	reproduces the data you have, it will not stop, it will continue naturally. And this you take
694720	699760	for your prediction. And on the sequence of ones, it's very plausible, right, that print one loop
699760	705760	is the shortest program, we can give some more complex examples like 12345. What comes next,
705760	711840	the short program is again, you know, counter. And so that is, roughly speaking, how Solomar's
711840	718400	induction works. The extra twist is that it can also deal with noisy data. So if you have, for
718400	723040	instance, a coin flip, say a biased coin, which comes up head with 60% probability,
724480	729200	then it will predict, it will learn and figure this out. And after a while, it predict or the next
730240	734160	coin flip will be head with probability 60%. So it's the stochastic version of that.
734720	738640	But the goal is the dream is always the search for the short program.
738640	743920	Yes. Yeah. Well, in Solomar of induction, precisely what you do is, so you combine. So
743920	749520	looking for the shortest program is like applying opax razor, like looking for the simplest theory.
749520	754640	There's also Epicoros principle, which says, if you have multiple hypotheses, which equally well
754640	759280	describe your data, don't discard any of them, keep all of them around, you never know. And you
759280	764240	can put that together and say, okay, I have a bias towards simplicity, but I don't rule out the
764240	771280	larger models. And technically, what we do is we weigh the shorter models higher and the longer
771280	780000	models lower. And you use a Bayesian techniques, you have a prior, and which is precisely two to
780000	785280	the minus the complexity of the program. And you weigh all this hypothesis and takes this mixture,
785280	789760	and then you get also this stochasticity in. Yeah, like many of your ideas, that's just a
789760	795920	beautiful idea of weighing based on the simplicity of the program. I love that. That seems to me
795920	803200	maybe a very human-centric concept seems to be a very appealing way of discovering good programs
803200	810720	in this world. You've used the term compression quite a bit. I think it's a beautiful idea,
810720	817200	sort of, we just talked about simplicity, and maybe science or just all of our intellectual
817200	823600	pursuits is basically the attempt to compress the complexity all around us into something simple.
823600	832320	So what does this word mean to you, compression? I essentially have already explained it. So
832320	840160	it compression means, for me, finding short programs for the data or the phenomenon at hand.
840160	845840	You could interpret it more widely as finding simple theories, which can be mathematical theories,
845840	852400	or maybe even informal, like just in words, compression means finding short descriptions,
852400	861440	explanations, programs for the data. Do you see science as a kind of our human attempt at compression?
862240	866320	So we're speaking more generally, because when you say programs, you're kind of zooming in on a
866320	870720	particular, sort of, almost like a computer science, artificial intelligence focus. But
870720	875920	do you see all of human endeavor as a kind of compression? Well, at least all of science,
875920	881440	I see as an endeavor of compression, not all of humanity, maybe. And, well, there are also some
881440	886800	other aspects of science, like experimental design, right? I mean, we create experiments
886800	892480	specifically to get extra knowledge. And this is, that isn't part of the decision-making process.
893200	899200	But once we have the data to understand the data is essentially compression. So I don't see any
899200	907520	difference between compression, understanding, and prediction. So we're jumping around topics a
907520	913760	little bit, but returning back to simplicity, a fascinating concept of comagra of complexity.
914320	921200	So in your sense, do most objects in our mathematical universe have high comagra of
921200	925840	complexity? And maybe what is, first of all, what is comagra of complexity?
925840	933760	Okay, comagra of complexity is a notion of simplicity or complexity. And it takes the
933760	939600	compression view to the extreme. So I explained before that if you have some data sequence,
939600	945360	just think about a file on a computer and best sort of, you know, just a string of bits. And
946320	952160	if you, and we have data compressors, like we compress big files into, say, zip files with
952160	957920	certain compressors. And you can also produce self-extracting RKFs. That means as an executable,
957920	963120	if you run it, it reproduces your original file without needing an extra decompressor. It's just
963120	968880	a decompressor plus the RKF together in one. And now there are better and worse compressors. And
968880	974320	you can ask, what is the ultimate compressor? So what is the shortest possible self-extracting
974320	980480	RKF you could produce for a certain data set, which reproduces the data set. And the length of
980480	987120	this is called the comagra of complexity. And arguably, that is the information content in
987120	991200	the data set. I mean, if the data set is very redundant or very boring, you can compress it
991200	996480	very well. So the information content should be low. And, you know, it is low according to this
996480	1001920	definition. Does the length of the shortest program that summarizes the data? Yes. Yeah.
1001920	1007840	And what's your sense of our universe when we think about the different
1010000	1016320	objects in our universe that we try concepts or whatever at every level? Do they have higher
1016320	1022560	or low comagra of complexity? So what's the hope? Do we have a lot of hope in being able to summarize
1023280	1028720	much of our world? That's a tricky and difficult question. So
1029360	1034000	as I said before, I believe that the whole universe, based on the evidence we have,
1034000	1039920	is very simple. So it has a very short description, the whole. Sorry to linger on that.
1039920	1045920	The whole universe, what does that mean? Do you mean at the very basic fundamental level in order
1045920	1052160	to create the universe? Yes. Yeah. So you need a very short program when you run it. To get the
1052160	1056880	thing going. To get the thing going, and then it will reproduce our universe. There's a problem
1057840	1064560	with noise. We can come back to that later, possibly. Is noise a problem or is it a bug or a
1064560	1072400	feature? I would say it makes our life as a scientist really, really much harder. I mean,
1072400	1077200	think about it without noise. We wouldn't need all of the statistics. But that maybe we wouldn't
1077200	1083200	feel like there's a free will. Maybe we need that for the... This is an illusion that noise can give
1083280	1088960	you free will. At least in that way, it's a feature. But also, if you don't have noise,
1088960	1095120	you have chaotic phenomena, which are effectively like noise. So we can't get away with statistics
1095120	1099520	even then. I mean, think about rolling a dice and forget about quantum mechanics and you know
1099520	1104960	exactly how you throw it. But I mean, it's still so hard to compute the trajectory that, effectively,
1104960	1111360	it is best to model it as coming out with a number, with probability one over six.
1111920	1119280	But from this sort of philosophical Kolmogorov complexity perspective, if we didn't have noise,
1119840	1126640	then arguably you could describe the whole universe as standard model plus
1126640	1130800	generativity. I mean, we don't have a theory of everything yet, but sort of assuming we are
1130800	1135600	close to it or have it here, plus the initial conditions, which may hopefully be simple. And
1135600	1140640	then you just run it and then you would reproduce the universe. But that's spoiled by noise or by
1140720	1148320	chaotic systems or by initial conditions, which may be complex. So now if we don't take the whole
1148320	1155760	universe with just a subset, just take planet Earth. Planet Earth cannot be compressed into
1155760	1160960	a couple of equations. This is a hugely complex system. So interesting. So when you look at the
1160960	1166640	window, the whole thing might be simple, but when you just take a small window, then it may become
1166640	1173200	complex. And that may be counterintuitive. But there's a very nice analogy, the book, the library
1173200	1178160	of all books. So imagine you have a normal library with interesting books, and you go there, great.
1178160	1184160	Lots of information and huge, quite complex. So now I create a library which contains all
1184160	1190080	possible books, say, of 500 pages. So the first book just has AAA over all the pages. The next book
1190080	1195200	AAA and ends with B. And so on. I create this library of all books. I can write a super short
1195200	1200080	program which creates this library. So this library which has all books has zero information
1200080	1204640	content. And you take a subset of this library and suddenly you have a lot of information in there.
1205200	1209760	So that's fascinating. I think one of the most beautiful object, mathematical objects that
1209760	1214160	at least today seems to be understudied or under talked about is cellular automata.
1215280	1220000	What lessons do you draw from sort of the game of life for cellular automata, where you start with
1220000	1226160	the simple rules, just like you're describing with the universe, and somehow complexity emerges.
1226160	1233200	Do you feel like you have an intuitive grasp on the behavior, the fascinating behavior of such
1233200	1238640	systems, where some, like you said, some chaotic behavior could happen, some complexity could
1238640	1244640	emerge, some, it could die out in some very rigid structures. Do you have a sense about
1245360	1250800	cellular automata that somehow transfers maybe to the bigger questions of our universe?
1250800	1255200	Yeah, the cellular automata and especially the converse game of life is really great because
1255200	1259360	this rule is so simple. You can explain it to every child and even by hand you can simulate a
1259360	1265920	little bit. And you see this beautiful patterns emerge and people have proven that it's even
1265920	1270480	touring complete. You cannot just use a computer to simulate game of life, but you can also use
1270480	1279520	game of life to simulate any computer. That is truly amazing. And it's the prime example probably to
1280240	1286480	demonstrate that very simple rules can lead to very rich phenomena. And people sometimes,
1286480	1291600	you know, how can, how is chemistry and biology so rich? I mean, this can't be based on simple
1291600	1298320	rules. But no, we know quantum electrodynamics describes all of chemistry. And we come later
1298400	1302880	back to that. I claim intelligence can be explained or described in one single equation,
1302880	1309200	this very rich phenomenon. You asked also about whether, you know, I understand this
1309200	1316720	phenomenon and it's probably not. And this is saying you never understand really things,
1316720	1324720	you just get used to them. And I think pretty used to cellular automata. So you believe that
1324720	1329760	you understand now why this phenomenon happens. But I give you a different example. I didn't play
1329760	1335360	too much this converse game of life, but a little bit more with fractals and with the
1335360	1339760	Mandelbrot set. And you need beautiful, you know, patterns, just look Mandelbrot set.
1340960	1344640	And well, when the computers were really slow and I just had a black and white
1344640	1348480	monitor and programmed my own programs on an assembler too.
1349440	1356960	Wow. Wow, you're legit to get these fractals on the screen. And it was mesmerized and much
1356960	1361520	later. So I returned to this, you know, every couple of years. And then I tried to understand
1361520	1368400	what is going on. And you can understand a little bit. So I tried to derive the locations,
1368400	1377280	you know, there are these circles and the apple shape. And then you have smaller Mandelbrot sets
1377280	1383360	recursively in this set. And there's a way to mathematically by solving high order polynomials
1383360	1389440	to figure out where these centers are and what size they are approximately. And by sort of
1389440	1398000	mathematically approaching this problem, you slowly get a feeling of why things are like they are.
1398000	1404800	And that sort of isn't, you know, first step to understanding why this rich phenomenon.
1404800	1408800	Do you think it's possible? What's your intuition? Do you think it's possible to reverse engineer
1408800	1415520	and find the short program that generated the these fractals, sort of by what looking at the
1415520	1422320	fractals? Well, in principle, yes. Yeah. So I mean, in principle, what you can do is you take,
1422320	1426400	you know, any data set, you know, you take these fractals, or you take whatever your data set,
1426400	1433280	whatever you have. So a picture of Conveys game of life. And you run through all programs, you
1433280	1437200	take a program of size one, two, three, four, and all these programs around them all in parallel in
1437200	1442960	so called dovetailing fashion, give them computational resources, first one 50%, second one half
1442960	1449120	resources and so on and let them run, wait until they hold, give an output, compare it to your data.
1449120	1453600	And if some of these programs produce the correct data, then you stop and then you have already
1453600	1458320	some program, it may be a long program, because it's faster. And then you continue and you get
1458320	1463040	shorter and shorter programs until you eventually find the shortest program. The interesting thing
1463040	1467360	you can ever know whether it's a shortest program, because there could be an even shorter program,
1467360	1474080	which is just even slower. And you just have to wait here. But asymptotically, and actually
1474080	1479200	after the final time, you have the shortest program. So this is a theoretical, but completely
1479200	1487840	impractical way of finding the underlying structure in every data set. And that was a
1487840	1491840	lot more of induction does and Convogorov complexity. In practice, of course, we have to
1491840	1500640	approach the problem more intelligently. And then if you take resource limitations into account,
1500640	1506240	there's once the field of pseudo random numbers, yeah, and these are random numbers. So these are
1506240	1512800	deterministic sequences, but no algorithm which is fast, fast means runs in polynomial time can
1512800	1518560	detect that it's actually deterministic. So we can produce interesting, I mean, random numbers,
1518560	1523360	maybe not that interesting, but just an example, we can produce complex looking data.
1524240	1528800	And we can then prove that no fast algorithm can detect the underlying pattern.
1531600	1534080	Which is unfortunately,
1537200	1541280	that's a big challenge for our search for simple programs in the space of artificial intelligence,
1541280	1546000	perhaps. Yes, it definitely is wanted vision intelligence. And it's quite surprising that
1546000	1553280	it's, I can't say easy. I mean, physicists worked really hard to find these theories, but apparently
1553280	1557840	it was possible for human minds to find these simple rules in the universe. It could have
1557840	1562720	been different, right? It could have been different. It's, it's, it's awe inspiring.
1564560	1572320	So let me ask another absurdly big question. What is intelligence in your view?
1573120	1575040	So I have, of course, a definition.
1576960	1580800	I wasn't sure what you're going to say, because you could have just as easy said, I have no clue.
1581360	1589920	Which many people would say, but I'm not modest in this question. So the, the informal version,
1591600	1596960	which I worked out together with Shane, like who co-founded the mind is that intelligence
1596960	1601760	measures and agents ability to perform well in a wide range of environments.
1603120	1609120	So that doesn't sound very impressive. And what it, these words have been very carefully chosen.
1609760	1616800	And there is a mathematical theory behind that. And we come back to that later. And if you look at
1616800	1622800	this, this definition by itself, it seems like, yeah, okay, but it seems a lot of things are
1622800	1630320	missing. But if you think it's true, then you realize that most, and I claim all of the other
1630320	1634640	traits, at least of rational intelligence, which we usually associate with intelligence,
1634640	1640880	are emergent phenomena from this definition, like, you know, creativity, memorization, planning,
1640880	1646560	knowledge. You all need that in order to perform well in a wide range of environments.
1647520	1650160	So you don't have to explicitly mention that in a definition.
1650160	1655200	Interesting. So yeah, so the consciousness, abstract reasoning, all these kinds of things
1655200	1662000	are just emergent phenomena that help you in towards, can you say the definition again?
1662000	1665520	So multiple environments. Did you mention the word goals?
1666240	1670080	No, but we have an alternative definition instead of performing well, you can just replace it by
1670080	1675280	goals. So intelligence measures and agents ability to achieve goals in a wide range of
1675280	1678080	environments. That's more or less equal. But interesting, because in there,
1678080	1682640	there's an injection of the word goals. So we want to specify there, there should be a goal.
1683200	1686720	Yeah, but perform well is sort of, what does it mean? It's the same problem.
1688000	1692160	There's a little bit gray area, but it's much closer to something that could be formalized.
1694240	1698880	In your view, are humans, where do humans fit into that definition? Are they
1699600	1706960	general intelligence systems that are able to perform in like, how good are they at fulfilling
1706960	1710480	that definition at performing well in multiple environments?
1711280	1715680	Yeah, that's a big question. I mean, the humans are performing best among all
1716880	1719920	species on Earth. Species we know, we know of. Yeah.
1720720	1725760	Depends. You could say that trees and plants are doing a better job. They'll probably outlast us.
1726400	1730400	So yeah, but they are in a much more narrow environment, right? I mean, you just, you know,
1730400	1734720	have a little bit of air pollution and these trees die and we can adapt, right? We build houses,
1734800	1741120	we build filters, we do geo-engineering. So the multiple environment part.
1741120	1745120	Yeah, that is very important, yes. So that distinguish narrow intelligence from wide
1745120	1753040	intelligence, also in the AI research. So let me ask the alenturing question, can machines
1753840	1760400	think? Can machines be intelligent? So in your view, I have to kind of ask, the answer is probably
1760400	1767200	yes, but I want to kind of hear what your thoughts on it. Can machines be made to fulfill this
1767200	1773680	definition of intelligence, to achieve intelligence? Well, we are sort of getting there and, you know,
1773680	1779040	on a small scale, we are already there. The wide range of environments are missing,
1779040	1784480	but we have self-driving cars, we have programs to play go and chess, we have speech recognition.
1784480	1788240	So it's pretty amazing, but you can, you know, these are narrow environments.
1788480	1794160	But if you look at AlphaZero, that was also developed by DeepMind. I mean,
1794160	1799920	got famous with AlphaGo and then came AlphaZero a year later. That was truly amazing. So
1799920	1805760	on reinforcement learning algorithm, which is able just by self-play to play chess
1807040	1811760	and then also go. And I mean, yes, they're both games, but they're quite different games. And,
1811760	1816640	you know, this, you didn't, don't feed them the rules of the game. And the most remarkable thing,
1816640	1821760	which is still a mystery to me that usually for any decent chess program, I don't know much about
1821760	1829120	Go, you need opening books and end game tables and so on, too. And nothing in there, nothing was
1829120	1833440	put in there. Especially with AlphaZero, the self-play mechanism, starting from scratch,
1833440	1842000	being able to learn actually new strategies. It really discovered, you know, all these
1842000	1848000	famous openings within four hours by itself. What I was really happy about, I'm a terrible
1848000	1853040	chess player, but I like Queen Gambi. And AlphaZero figured out that this is the best opening.
1854720	1861920	Finally, somebody proved you correct. So yes, to answer your question, yes,
1861920	1869120	I believe that general intelligence is possible. And it also depends how you define it. Do you say
1869120	1875280	AGI with general intelligence, artificial intelligence, only refers to if you achieve
1875280	1879840	human level or a sub-human level, but quite broad? Is it also general intelligence,
1879840	1885040	so we have to distinguish or it's only super human intelligence, general artificial intelligence?
1885040	1889760	Is there a test in your mind, like the Turing test for natural language or some other test
1889760	1894400	that would impress the heck out of you, that would kind of cross the line of
1895360	1901600	your sense of intelligence within the framework that you said? Well, the Turing test has been
1901600	1906800	criticized a lot, but I think it's not as bad as some people think. And some people think it's too
1906800	1915120	strong. So it tests not just for a system to be intelligent, but it also has to fake human
1915120	1920000	deception. Disception, right? Which is, you know, much harder. And on the other hand,
1920000	1927040	they say it's too weak because it's just maybe fakes, you know, emotions or intelligent behavior.
1927680	1933920	It's not real. But I don't think that's the problem or big problem. So if you would pass the Turing
1933920	1942560	test, so a conversation or a terminal with a bot for an hour, or maybe a day or so, and you can
1942560	1947120	fool a human into, you know, not knowing whether this is a human or not, that it's the Turing test,
1947600	1952320	I would be truly impressed. And we have this annual competition, the Lubna
1953360	1957600	prize. And I mean, it started with Eliza, that was the first conversational program.
1958240	1963200	And what is it called the Japanese Mitsuko or so, that's the winner of the last, you know,
1963200	1968880	couple of years. And well, it's impressive. Yeah, it's quite impressive. And then Google has
1968880	1975680	developed Mina, right? Just recently, that's an open domain conversational bot. Just a couple of
1975680	1981520	weeks ago, I think. Yeah, I kind of like the metric that sort of the Alexa price has proposed.
1981520	1985360	I mean, maybe it's obvious to you, it wasn't to me of setting sort of a length
1986480	1991600	of a conversation. Like, you want the bot to be sufficiently interesting that you'd want to keep
1991600	1999440	talking to it for like 20 minutes. And that's a surprisingly effective and aggregate metric.
1999440	2007680	Because really, like, nobody has the patience to be able to talk to a bot that's not interesting
2007680	2013440	and intelligent and witty and is able to go into different tangents, jump domains, be able to,
2014000	2018240	you know, say something interesting to maintain your attention. Maybe many humans will also fail
2018240	2025280	this test. Unfortunately, we set just like with autonomous vehicles with chatbots,
2025280	2029920	we also set a bar that's way too hard to reach. I said, you know, the Turing test is not as bad
2029920	2036560	as some people believe. But what is really not useful about the Turing test, it gives us no
2036560	2041920	guidance how to develop these systems in the first place. Of course, you know, we can develop them
2041920	2046800	by trial and error and, you know, do whatever and then run the test and see whether it works or not.
2046800	2056880	But a mathematical definition of intelligence gives us, you know, an objective which we can then
2056880	2063200	analyze by, you know, theoretical tools or computational and, you know, maybe even prove
2063200	2070480	how close we are. And we will come back to that later with the ISE model. So I mentioned the
2070480	2076720	compression, right? So in natural language processing, they have achieved amazing results.
2076720	2080800	And one way to test this, of course, you know, take the system, you train it, and then you,
2080800	2088000	you know, see how well it performs on the task. But a lot of performance measurement is done by
2088000	2093280	so-called perplexity, which is essentially the same as complexity or compression length.
2093280	2097520	So the NLP community develops new systems, and then they measure the compression length,
2097520	2103440	and then they have ranking and leaks, because there's a strong correlation between
2103440	2108400	compressing well, and then the systems performing well at the task at hand. It's not perfect,
2108400	2113360	but it's good enough for them as an intermediate aim.
2114560	2119840	So you mean measure, so this is kind of almost returning to the common growth complexity. So
2119840	2125120	you're saying good compression usually means good intelligence. Yes.
2126880	2134800	So you mentioned you're one of the, one of the only people who dared boldly to try to
2134800	2141600	formalize the idea of artificial general intelligence, to have a mathematical framework
2141600	2150560	for intelligence, just like as we mentioned, termed IEXI, A-I-X-I. So let me ask the basic
2150560	2158560	question, what is IEXI? Okay, so let me first say what it stands for, because what it stands for,
2158560	2162720	actually, that's probably the more basic question. The first question is usually how
2163520	2168080	how it's pronounced, but finally I put it on the website, how it's pronounced, and you figured it out.
2169040	2175360	Yeah. The name comes from AI, artificial intelligence, and the XI is the Greek letter
2175360	2183200	XI, which are used for Solomonov's distribution for quite stupid reasons, which I'm not willing to
2183200	2190640	repeat here in front of camera. So it does happen to be more or less arbitrary, I chose the XI,
2191440	2197760	but it also has nice other interpretations. So there are actions and perceptions in this
2197760	2204960	model, where an agent has actions and perceptions, and over time, so this is A-I-X-I, so there's
2204960	2210560	an action at time I, and then followed by a perception at time I. We'll go with that. I'll
2210560	2216720	edit out the first part. I'm just kidding. I have some more interpretations. So at some point,
2216720	2224000	maybe five years ago or 10 years ago, I discovered in Barcelona, it was on a big church,
2224560	2231200	there was a stone engraved, some text, and the word Aixi appeared there a couple of times.
2232720	2239280	I was very surprised and happy about that, and I looked it up, so this is Catalan language,
2239280	2244080	and it means with some interpretation, that's it, that's the right thing to do. Yeah, Heureka.
2244720	2252000	Oh, so it's almost like destined, somehow came to you in a dream.
2252000	2256480	And similar, there's a Chinese word, Aixi, also written like Aixi, if you transcribe that to
2256480	2262480	Pingen. And the final one is that is A-I, crossed with induction, because that is, and that's going
2262480	2268240	more to the content now. So good old fashioned A-I is more about planning a known deterministic
2268240	2273760	world, and induction is more about often IID data and inferring models, and essentially,
2273760	2278640	what this Aixi model does is combining these two. And I actually also recently,
2278640	2285920	I think heard that in Japanese, A-I means love. So if you can combine X-I somehow with that,
2286560	2292240	I think we can, there might be some interesting ideas there. So Aixi, let's then take the next
2292240	2299440	step. Can you maybe talk at the big level of what is this mathematical framework?
2299680	2306480	So it consists essentially of two parts. One is the learning and induction and prediction part.
2306480	2311760	And the other one is the planning part. So let's come first to the learning induction
2311760	2318480	prediction part, which essentially I explained already before. So what we need for any agent
2319200	2324640	to act well is that it can somehow predict what happens. I mean, if you have no idea what your
2324640	2330640	actions do, how can you decide which acts are good or not? So you need to have some model of
2330640	2337840	what your actions effect. So what you do is you have some experience, you build models like scientists
2337840	2342480	of your experience, then you hope these models are roughly correct, and then you use these models
2342480	2347520	for prediction. And a model is, sorry to interrupt, and a model is based on your perception of the
2347520	2354160	world, how your actions will affect that world. That's not the important part.
2354640	2358400	It is technically important, but at this stage, we can just think about predicting, say,
2359120	2363920	stock market data, whether data or IQ sequences, one, two, three, four, five, what comes next.
2363920	2370160	Yeah. So of course, our actions affect what we're doing, but I'll come back to that in a second.
2370160	2376400	So, and I'll keep just interrupting. So just to draw a line between prediction and planning.
2377440	2381840	What do you mean by prediction in this way? It's trying to predict the
2382720	2388000	environment without your long-term action in the environment. What is prediction?
2389440	2392320	Okay. If you want to put the actions in now, okay, then let's put in them now.
2394720	2400160	We don't have to put them now. Scratch it. Don't question. Okay. So the simplest form of prediction
2400160	2406160	is that you just have data that you passively observe, and you want to predict what happens
2406160	2413280	without, you know, interfering. As I said, weather forecasting, stock market, IQ sequences, or just
2414880	2420000	anything. Okay. And Solominov's theory of induction based on compression. So you look for the shortest
2420000	2424320	program which describes your data sequence. And then you take this program, run it,
2424320	2429040	which reproduces your data sequence by definition, and then you let it continue running,
2429040	2434240	and then it will produce some predictions. And you can rigorously prove that for any
2434240	2441280	prediction task, this is essentially the best possible predictor. Of course, if there's a prediction
2441280	2447120	task, or a task which is unpredictable, like, you know, your fair coin flips, yeah, I cannot
2447120	2450880	predict the next fair coin, but what Solominov does is says, okay, next head is probably 50%.
2451520	2455120	It's the best you can do. So if something is unpredictable, Solominov will also not
2455120	2460560	magically predict it. But if there is some pattern and predictability, then Solominov
2460640	2466000	induction will figure that out, eventually, and not just eventually, but rather quickly,
2466000	2473760	and you can have proof convergence rates, whatever your data is. So there's pure magic in a sense.
2474640	2478160	What's the catch? Well, the catch is that it's not computable. And we come back to that later.
2478160	2482640	You cannot just implement it in even this Google resources here, and run it and, you know, predict
2482640	2487040	the stock market and become rich. I mean, if it's raised Solominov already, you know, tried it at the
2487040	2492720	time. But so the basic task is you're in the environment, and you're interacting with an
2492720	2497520	environment to try to learn a model of that environment. And the model is in the space of
2497520	2501200	these all these programs. And your goal is to get a bunch of programs that are simple.
2501200	2505280	And so let's, let's go to the actions now. But actually, good that you asked usually,
2505280	2509600	I skipped this part, although there is also a minor contribution, which I did. So the action part,
2509600	2513360	but they usually sort of just jump to the decision part. So let me explain the action part now.
2513360	2521040	Thanks for asking. So you have to modify it a little bit by now not just predicting a sequence
2521040	2527680	which just comes to you, but you have an observation, then you act somehow. And then you want to predict
2527680	2533920	the next observation based on the past observation and your action. Then you take the next action.
2534560	2538960	You don't care about predicting it because you're doing it. And then you get the next observation.
2538960	2542720	And you want, well, before you get it, you want to predict it again based on your past
2542720	2549360	action and observation sequence. You just condition extra on your actions. There's an
2549360	2553040	interesting alternative that you also try to predict your own actions.
2555520	2560320	If you want. In the past or the future. Your future actions. That's interesting.
2561840	2567360	Wait, let me wrap. I think my brain is broke. We should maybe discuss that later
2567360	2571200	after I've explained the ICSE model. That's an interesting variation. But that is a really
2571200	2575440	interesting variation. And a quick comment. I don't know if you want to insert that in here.
2575440	2581120	But you're looking at that in terms of observations, you're looking at the entire big
2581120	2585600	history, the long history of the observations. Exactly. That's very important, the whole history
2585600	2590800	from birth sort of of the agent. And we can come back to that. Also why this is important here.
2590800	2595760	Often, you know, in RL, you have MDPs, macro decision processes, which are much more limiting.
2595760	2601520	Okay, so now we can predict conditioned on actions. So even if the influence environment.
2601520	2606080	But prediction is not all we want to do, right? We also want to act really in the world.
2606800	2612080	And the question is how to choose the actions. And we don't want to greedily choose the actions.
2613280	2617520	You know, just, you know, what is best in the next time step. And we first I should say, you
2617520	2621840	know, what is, you know, how do we measure performance? So we measure performance by giving
2621840	2627040	the agent reward. That's the so called reinforcement learning framework. So every time step,
2627040	2631120	you can give it a positive reward or negative reward, or maybe no reward, it could be a very
2631120	2635360	scarce, right? Like if you play chess, just at the end of the game, you give plus one for winning
2635360	2639840	or minus one for losing. So in the IXI framework, that's completely sufficient. So occasionally,
2639840	2645120	you give a reward signal, and you ask the agent to maximize reward, but not greedily sort of,
2645120	2648640	you know, the next one, next one, because that's very bad in the long run, if you're greedy.
2648880	2654000	So, but over the lifetime of the agent. So let's assume the agent lives for M
2654000	2658320	time steps as they dice in sort of 100 years sharp. That's just, you know, the simplest model
2658320	2665120	to explain. So it looks at the future reward sum and ask what is my action sequence, or actually
2665120	2670880	more precisely my policy, which leads in expectation, because I don't know the world,
2672080	2677200	to the maximum reward sum. Let me give you an analogy. In chess, for instance,
2678160	2683360	we know how to play optimally in theory, it's just a mini max strategy. I play the move which
2683360	2688560	seems best to me under the assumption that the opponent plays the move which is best for him,
2688560	2694480	so best, so worst for me, under the assumption that he I play again, the best move. And then you
2694480	2699120	have this expecting max three to the end of the game. And then you back propagate and then you
2699120	2702960	get the best possible move. So that is the optimal strategy, which for Neumann already
2702960	2710640	figured out a long time ago, for playing adversarial games, luckily, or maybe unluckily,
2710640	2716400	for the theory, it becomes harder that world is not always adversarial. So it can be,
2716400	2721280	if the other humans even cooperative here, or nature is usually I mean, the dead nature is
2721280	2727520	stochastic, you know, things just happen randomly, or don't care about you. So what you have to
2727520	2732000	take into account is the noise, you know, and not necessarily adversariality. So you replace
2732000	2737360	the minimum on the opponent's side by an expectation, which is general enough to include
2737360	2743040	also adversarial cases. So now instead of a mini max strategy of an expecting max strategy.
2743760	2746960	So far so good. So that is well known. It's called sequential decision theory.
2747920	2753200	But the question is, on which probability distribution do you base that if I have the
2753200	2757840	true probability distribution, like say I play beggining, right, there's dice,
2757840	2761680	and there's certain randomness involved, yeah, I can calculate probabilities and feed it in the
2761680	2765760	expecting max or the sequential decision tree, come up with the optimal decision if I have
2765760	2770800	enough compute. But in the for the real world, we don't know that, you know, what is the probability
2771520	2777040	the driver in front of me breaks. I don't know. Yeah, so depends on all kinds of things. And
2777680	2782800	especially new situations, I don't know. So this is this unknown thing about prediction. And there's
2782800	2787760	where Solomanov comes in. So what you do is in sequential decision tree, you just replace the
2787760	2794000	true distribution, which we don't know, by this universal distribution, I didn't explicitly
2794000	2799040	talk about it, but this is used for universal prediction, and plug it into the sequential
2799040	2804000	decision tree mechanism. And then you get the best of both worlds. You have a long term planning
2804000	2809760	agent. But it doesn't need to know anything about the world, because the Solomanov induction part
2810720	2818480	learns. Can you explicitly try to describe the universal distribution and how Solomanov induction
2818480	2823760	plays a role here? I'm trying to understand. So what he does it. So in the simplest case,
2823760	2828000	I said, take the shortest program describing your data, run it, have a prediction which would be
2828000	2834080	deterministic. Yes. Okay. But you should not just take the shortest program, but also consider the
2834080	2842320	longer ones, but keep it lower a priori probability. So in the Bayesian framework, you say a priori,
2842320	2850240	any distribution, which is a model, or a stochastic program has a certain a priori
2850240	2854480	probability, which is two to the minus and y two to the minus length, you know, I could explain
2854480	2861280	length of this program. So longer programs are punished, a priori. And then you multiply it
2861280	2868080	with the so called likelihood function. Yeah, which is as the name suggests, is how likely
2868080	2874160	is this model given the data at hand. So if you have a very wrong model, it's very unlikely
2874160	2878800	that this model is true. And so it is very small numbers. So even if the model is simple, it gets
2878800	2884480	penalized by that. And what you do is then you take just the sum, but this is the average over it.
2884480	2890480	And this gives you a probability distribution. So universal distribution or Solomanov distribution.
2890480	2894320	So it's weighed by the simplicity of the program and likelihood. Yes.
2895200	2902480	It's kind of a nice idea. Yeah. So okay. And then you said there's your planning
2902480	2908480	n or m or forgot the letter steps into the future. So how difficult is that problem? What's
2908480	2912720	involved there? Okay, basic optimization problem? What are we talking about? Yeah, so you have a
2912720	2918640	planning problem up to horizon m. And that's exponential time in the horizon m, which is,
2918640	2923440	I mean, it's computable, but in fact, intractable. I mean, even for chess, it's already intractable
2923440	2928080	to do that exactly. And, you know, for though, but it could be also discounted kind of framework
2928080	2934560	where so, so having a hard horizon, you know, at 100 years, it's just for simplicity of
2934560	2940000	discussing the model. And also sometimes the master simple. But there are lots of variations.
2940000	2947120	Actually, quite interesting parameter is it's, there's nothing really problematic about it.
2947120	2950960	But it's very interesting. So for instance, you think, no, let's, let's, let's, let's let the
2950960	2956240	parameter m tend to infinity, right? You want an agent, which lives forever, right? If you do it
2956240	2960560	now, you have two problems. First, the mathematics breaks down because you have an infinite reward
2960560	2966240	sum, which may give infinity and getting reward 0.1 in the time step is infinity and giving reward
2966240	2972640	one every time step is infinity. So equally good. Not really what we want. Other problem is that
2973600	2979120	if you have an infinite life, you can be lazy for as long as you want for 10 years and then catch
2979120	2984960	up with the same expected reward. And, you know, think about yourself or, you know, or maybe,
2984960	2991280	you know, some friends or so, if they knew they lived forever, you know, why work hard now, you
2991280	2995440	know, just enjoy your life, you know, and then catch up later. So that's another problem with
2995440	3000560	the infinite horizon. And you mentioned, yes, we can go to discounting. But then the standard
3000640	3005920	discounting is so-called geometric discounting. So a dollar today is about worth as much as,
3005920	3011520	you know, $1.05 tomorrow. So if you do the so-called geometric discounting, you have introduced an
3011520	3018320	effective horizon. So the agent is now motivated to look ahead a certain amount of time effectively.
3018320	3025120	It's like a moving horizon. And for any fixed effective horizon, there is a problem
3025920	3030400	to solve which requires larger horizons. So if I look ahead, you know, five time steps,
3030400	3035520	I'm a terrible chess player, right? I'll need to look ahead longer. If I play go, I probably
3035520	3041280	have to look ahead even longer. So for every problem, no, for every horizon, there is a problem
3041280	3047280	which this horizon cannot solve. But I introduced the so-called near harmonic horizon, which goes
3047280	3052640	down with one over t rather than exponentially t, which produces an agent which effectively looks
3052640	3057280	into the future proportional to each age. So if it's five years old, it plans for five years.
3057280	3062000	If it's 100 years old, it then plans for 100 years. And it's a little bit similar to humans,
3062000	3066240	too, right? I mean, children don't plan ahead very long, but then we get adults, we play ahead
3066240	3070160	more longer. Maybe when we get very old, I mean, we know that we don't live forever,
3070160	3076960	you know, maybe then our horizon shrinks again. So that's really interesting. So adjusting the
3076960	3081520	horizon, what is there some mathematical benefit of that? Or is it just a nice
3082880	3087120	I mean, intuitively, empirically, it will probably be a good idea to sort of push the
3087120	3093760	horizon back to extend the horizon as you experience more of the world. But is there
3093760	3096480	some mathematical conclusions here that are beneficial?
3097040	3104880	With Salomon's prediction part, we have extremely strong finite time, finite data results. So you
3104880	3109200	have so and so much data, then you lose so and so much. So the deterioration is really great.
3109280	3115920	With the IKC model with the planning part, many results are only asymptotic, which, well, this is
3116720	3121680	what is asymptotic means you can prove, for instance, that in the long run, if the agent,
3121680	3126320	you know, acts long enough, then, you know, it performs optimal or some nice thing happens.
3126320	3131200	So but you don't know how fast it converges. Yeah, so it may converge fast, but we're just
3131200	3137200	not able to prove it because a difficult problem. Or maybe there's a bug in the in the in the model
3137200	3142720	so that it's really that slow. Yeah. So so that is what asymptotic means sort of eventually,
3142720	3150800	but we don't know how fast. And if I give the agent a fixed horizon M, yeah, then I cannot prove
3150800	3156240	asymptotic results, right? So I mean, sort of if it dies in 100 years, then in 100 years is over,
3156240	3161760	I cannot say eventually. So this is the advantage of the discounting that I can prove asymptotic
3161760	3171040	results. So just to clarify, so so I okay, I made I've built up a model with now in the moment of
3171920	3177520	have this way of looking several steps ahead. How do I pick what action I will take?
3178800	3183840	It's like with a playing chess, right, you do this mini max. In this case, here do you expect the
3183840	3191440	max based on the solometer of distribution, you propagate back. And then, while an action falls
3191440	3196720	out, the action which maximizes the future expected reward on the solometer of distribution,
3196720	3200880	and then you just take this action, and then repeat. And then you get a new observation,
3200880	3205760	and you feed it in this action observation, then you repeat and the reward so on. Yeah. So you wrote
3205760	3211520	to you. And then maybe you can even predict your own action. I love the idea. But okay, this big
3211520	3219840	framework. What is it? I mean, it's kind of a beautiful mathematical framework to think about
3219840	3226160	artificial general intelligence. What can you, what does it help you into it about
3227600	3235280	how to build such systems or maybe from another perspective? What does it help us in understanding
3235280	3243280	AGI? So when I started in the field, I was always interested in two things. One was, you know, AGI.
3244240	3250800	The name didn't exist then, what called general AI or strong AI. And the physics of everything.
3250800	3254640	So I switched back and forth between computer science and physics quite often.
3254640	3256560	You said the theory of everything. The theory of everything.
3257360	3261120	It was basically the biggest problems before all humanity.
3263360	3269520	Yeah, I can explain if you wanted some later time, you know, why I'm interested in these two
3269520	3277040	questions. Can I ask you, in a small tangent, if, if, if one to be, it was one to be solved,
3277040	3282720	which one would you, if one, if you were, if an apple fell in your head, and there was a brilliant
3282720	3288480	insight and you could arrive at the solution to one, would it be AGI or the theory of everything?
3289120	3293120	Definitely AGI, because once the AGI problem is solved, they can ask the AGI to solve the
3293120	3301120	other problem for me. Yeah, brilliantly put. Okay. So, so as you were saying about it.
3301120	3306640	Okay. So, and the reason why it didn't settle, I mean, this thought about, you know,
3307280	3311120	once you have solved AGI, it solves all kinds of other, not just the theory of every problem,
3311120	3316400	but all kinds of use, more useful problems to humanity is very appealing to many people. And,
3316400	3323920	you know, I had this thought also, but I was quite disappointed with the state of the art
3323920	3328800	of the field of AI. There was some theory, you know, about logical reasoning, but I was never
3328800	3333680	convinced that this will fly. And then there was this more, more heuristic approaches with neural
3333680	3340080	networks, and I didn't like these heuristics. So, and also I didn't have any good idea myself.
3342160	3345360	So that's the reason why I toggled back and forth quite some while and even worked
3345360	3349600	so four and a half years in a company developing software or something completely unrelated.
3349600	3356960	But then I had this idea about the ICSE model. And so what it gives you, it gives you a gold
3356960	3363760	standard. So I have proven that this is the most intelligent agents, which anybody could
3364960	3370560	build built in quotation mark, because it's just mathematical and you need infinite compute.
3371040	3377040	But this is the limit. And this is completely specified. It's not just a framework and, you know,
3378400	3383440	every year, tens of frameworks are developed, which is just skeletons, and then pieces are
3383440	3387280	missing. And usually these missing pieces, you know, turn out to be really, really difficult.
3387280	3393680	And so this is completely and uniquely defined. And we can analyze that mathematically. And
3395120	3399440	we've also developed some approximations, I can talk about that a little bit later,
3400160	3404160	that would be sort of the top down approach, like, say, for Neumann's minimax theory,
3404160	3409520	that's the theoretical optimal play of games. And now we need to approximate it, put heuristics
3409520	3413040	in prune, the tree, blah, blah, blah, and so on. So we can do that also with the ICSE model,
3413040	3420720	but for generally I, it can also inspire those. And most of most researchers go bottom upright,
3420720	3425840	they have the systems that try to make it more general, more intelligent. It can inspire in which
3425840	3431840	direction to go. What do you mean by that? So if you have some choice to make, right, so how should
3431840	3438160	they evaluate my system if I can't do cross validation? How should they do my learning if
3438160	3443120	my standard regularization doesn't work well? Yeah. So the answer is always this, we have a system
3443120	3448480	which does everything that's ICSE. It's just, you know, completely in the ivory tower, completely
3448480	3453520	useless from a practical point of view. But you can look at it and see, ah, yeah, maybe, you know,
3453520	3457520	I can take some aspects and, you know, instead of Kolmogorov complexity, they just take some
3457520	3463040	compressors which has been developed so far. And for the planning, well, we have UCT which has also,
3463040	3471600	you know, been used in Go. And it, at least it's inspired me a lot to have this formal
3473280	3477680	definition. And if you look at other fields, you know, like, I always come back to physics
3477680	3481440	because I have a physics background, think about the phenomenon of energy that was
3481440	3485120	long time a mysterious concept. And at some point, it was completely formalized.
3486160	3491280	And that really helped a lot. And you can point out a lot of these things which were
3491280	3495280	first mysterious and vague, and then they have been rigorously formalized.
3495280	3500400	Speed and acceleration has been confused, right, until it was formally defined. There was a time
3500400	3505840	like this. And people, you know, often, you know, don't have any background, you know, still confuse
3505840	3513120	it. So, and this IXI model or the intelligence definitions, which is sort of the dual to it,
3513120	3518320	we come back to that later, formalizes the notion of intelligence uniquely and rigorously.
3518880	3522480	So in a sense, it serves as kind of the light at the end of the tunnel.
3523040	3528320	Yes, yeah. So I mean, there's a million questions I could ask her. So maybe
3529440	3534640	the kind of, okay, let's feel around in the dark a little bit. So there's been here a deep mind,
3534640	3538000	but in general, been a lot of breakthrough ideas, just like we've been saying around
3538000	3543600	reinforcement learning. So how do you see the progress in reinforcement learning is different?
3544320	3551120	Like, which subset of IXI does it occupy the current, like you said, maybe
3551920	3555600	the Markov assumption is made quite often in reinforcement learning.
3556400	3562960	The, there's other assumptions made in order to make the system work. What do you see as the
3562960	3565760	difference connection between reinforcement learning and IXI?
3566640	3572480	And so the major difference is that essentially all other approaches,
3573360	3577600	they make stronger assumptions. So in reinforcement learning, the Markov assumption
3578240	3583280	is that the next state or next observation only depends on the on the previous observation
3583280	3588000	and not the whole history, which makes, of course, the mathematics much easier rather than dealing
3588000	3593360	with histories. Of course, they profit from it also, because then you have algorithms that run
3593360	3599520	on current computers and do something practically useful. But for generally, I all the assumptions
3599520	3607040	which are made by other approaches, we know already now they are limiting. So, for instance,
3607840	3611520	usually you need a gothicity assumption in the MDP frameworks in order to learn.
3611520	3616080	A gothicity essentially means that you can recover from your mistakes and that they are
3616080	3621680	not traps in the environment. And if you make this assumption, then essentially you can go back
3621680	3628640	to a previous state, go there a couple of times and then learn what, what statistics and what
3628640	3633840	this state is like. And then in the long run perform well in this state. But there are no
3633840	3640480	fundamental problems. But in real life, we know, there can be one single action, one second of
3640480	3646880	being inattentive while driving a car fast, you know, can ruin the rest of my life, I can become
3646880	3651440	quadruplegic or whatever. So, and there's no recovery anymore. So, the real world is not
3651440	3655520	ergodic, I always say, you know, there are traps and there are situations where you're not recovered
3655520	3666480	from. And very little theory has been developed for this case. What about what do you see in the
3666560	3675200	context of IHC as the role of exploration, sort of, you mentioned, you know, in the real world
3675200	3680240	and get into trouble when we make the wrong decisions and really pay for it. But exploration
3680240	3684960	seems to be fundamentally important for learning about this world, for gaining new knowledge.
3685600	3691360	So, is exploration baked in? Another way to ask it, what are the parameters
3692240	3698240	of this of IHC that can be controlled? Yeah, I say the good thing is that there are no
3698240	3704240	parameters to control. Some other people try knobs to control and you can do that. I mean,
3704240	3711280	you can modify IHC so that you have some knobs to play with if you want to. But the exploration
3711280	3718640	is directly baked in. And that comes from the Bayesian learning and the long term planning.
3718640	3728560	So these together already imply exploration. You can nicely and explicitly prove that for
3729360	3737920	simple problems like so-called banded problems, where you say to give a real-world example,
3737920	3742160	say you have two medical treatments, A and B, you don't know the effectiveness, you try A a
3742160	3746640	little bit, B a little bit, but you don't want to harm too many patients. So you have to sort of
3747600	3754240	trade off exploring. And at some point you want to explore and you can do the mathematics and
3754240	3760240	figure out the optimal strategy. It's called Bayesian agents, they're also non-Bayesian agents.
3761040	3766560	But it shows that this Bayesian framework by taking a prior over possible worlds,
3767280	3771360	doing the Bayesian mixture, then the Bayes optimal decision with long term planning that is important,
3772240	3778960	automatically implies exploration also to the proper extent, not too much exploration
3778960	3784560	and not too little. It is very simple settings. In the IHC model, I was also able to prove that
3784560	3788800	it is a self-optimizing theorem or asymptotic optimality theorems, although they're only asymptotic,
3788800	3793120	not finite time bounds. So it seems like the long term planning is a really important,
3793120	3798880	the long term part of the planning is really important. And also, maybe a quick tangent,
3798880	3803840	how important do you think is removing the Markov assumption and looking at the full
3803840	3810080	history? So intuitively, of course, it's important, but is it like fundamentally
3810080	3815440	transformative to the entirety of the problem? What's your sense of it? Because we all,
3815440	3819840	we make that assumption quite often, just throwing away the past.
3819840	3827360	Now, I think it's absolutely crucial. The question is whether there's a way to deal with it in a
3827360	3835440	more heuristic and still sufficiently well way. So I have to come up with an example in the fly,
3835440	3842000	but you have some key event in your life a long time ago, in some city or something,
3842000	3846720	you realize it's a really dangerous street or whatever. And you want to remember that forever,
3847920	3852000	in case you come back there. Kind of a selective kind of memory. So you remember
3852800	3856640	all the important events in the past, but somehow selecting the important
3857360	3862080	They're very hard. Yeah. And I'm not concerned about just storing the whole history. Just
3862080	3867360	you can calculate human life, say 30 or 100 years doesn't matter, right?
3868720	3874000	How much data comes in through the vision system and the auditory system, you compress it a little
3874000	3879840	bit, in this case, lossily and store it. We are soon in the means of just storing it.
3880400	3886000	But you still need to the selection for the planning part and the compression for the
3886000	3891680	understanding part. The raw storage I'm really not concerned about. And I think we should just
3891680	3898320	store if you develop an agent, preferably just store all the interaction history.
3899360	3904800	And then you build, of course, models on top of it and you compress it and you are selective,
3904880	3911440	but occasionally, you go back to the old data and reanalyze it based on your new experience
3911440	3916240	you have. You know, sometimes you are in school, you learn all these things you think is totally
3916240	3921040	useless. And you know, much later you read us, oh, they were not, you know, so useless as you
3921040	3927120	thought. I'm looking at you linear algebra. Right. So maybe let me ask about objective
3927120	3934640	functions because that rewards, it seems to be an important part. The rewards are kind of
3934640	3945120	given to the system. For a lot of people, the specification of the objective function
3946320	3952080	is a key part of intelligence. The agent itself figuring out what is important.
3952960	3960160	What do you think about that? Is it possible within IACC framework to yourself discover
3960240	3966800	the reward based on which you should operate? Okay, that will be a long answer.
3968800	3974400	So, and that is a very interesting question. And I'm asked a lot about this question,
3974400	3982960	where do the rewards come from? And that depends. So, and I give you now a couple of answers. So
3982960	3989760	if we want to build agents, now let's start simple. So let's assume we want to build an agent
3989760	3995120	based on the IACC model, which performs a particular task. Let's start with something
3995120	3998960	super simple, like, I mean, super simple, like playing chess or go or something.
3999760	4003360	Then you just, you know, the reward is, you know, winning the game is plus one,
4003360	4008240	losing the game is minus one, done. You apply this agent, if you have enough compute,
4008240	4013360	you let itself play, and it will learn the rules of the game will play perfect chess after some
4013360	4023440	while problem solved. Okay, so if you have more complicated problems, then you may believe that
4023440	4029600	you have the right reward, but it's not. So a nice cute example is elevator control that is also in
4029600	4036080	Rich Sutton's book, which is a great book, by the way. So you control the elevator, and you think,
4036080	4040320	well, maybe the reward should be coupled to how long people wait in front of the elevator,
4040400	4045520	you know, long wait is bad. You program it and you do it. And what happens is the elevator
4045520	4053040	eagerly picks up all the people, but never drops them off. So then you realize, maybe the time in
4053040	4058800	the elevator also counts. So you minimize the sum. Yeah. And the elevator does that, but never picks
4058800	4062880	up the people in the 10th floor and the top floor, because in expectation, it's not worth it. Just
4062960	4072000	let them stay. So even in apparently simple problems, you can make mistakes. And that's
4073600	4079600	what in more serious context, say, AGI safety researchers consider. So now let's go back to
4080240	4085280	general agents. So assume we want to build an agent, which is generally useful to humans.
4085280	4091040	Yes, we have a household robot, and it should do all kinds of tasks. So in this case,
4091840	4096800	the human should give the reward on the fly. I mean, maybe it's pre trained in the factory and
4096800	4100560	that there's some sort of internal reward for, you know, the battery level or whatever. But
4101600	4105120	so it, you know, it does the dishes badly, you know, you punish the robot, you does it good,
4105120	4109040	you reward the robot and then train it to a new task, like a child, right? So
4110000	4115520	you need the human in the loop. If you want a system, which is useful to the human. And as long
4115520	4122160	as this agent stays sub human level, that should work reasonably well. And apart from, you know,
4122160	4126560	these examples, it becomes critical if they become, you know, on a human level, it's like
4126560	4131440	with children, small children, you have reasonably well under control, they become older. The reward
4131440	4139760	technique doesn't work so well anymore. So then finally, so this would be agents, which are just,
4139760	4144880	you could say slaves to the humans. Yeah. So if you are more ambitious and just say we want to
4144880	4150960	build a new spacious of intelligent beings, we put them on a new planet and we want them to
4150960	4158080	develop this planet or whatever. So we don't give them any reward. So what could we do? And
4158080	4162800	you could try to, you know, come up with some reward functions like, you know, it should maintain
4162800	4170160	itself the robot, it should maybe multiply build more robots, right? And, you know, maybe
4171120	4174640	for all kinds of things that you find useful, but that's pretty hard, right? You know,
4174640	4178720	what does self maintenance mean? You know, what does it mean to build a copy? Should it be exact
4178720	4185120	copy or an approximate copy? And so that's really hard. But Laurent or so, also at DeepMind,
4186000	4192000	developed a beautiful model. So it just took the ICSE model and coupled the rewards
4192720	4199040	to information gain. So he said the reward is proportional to how much the agent had
4199120	4203840	learned about the world. And you can rigorously formally uniquely define that in terms of
4203840	4209200	our cattle diversions. Okay. So if you put that in, you get a completely autonomous agent.
4209760	4213440	And actually, interestingly, for this agent, we can prove much stronger result than for the
4213440	4219040	general agent, which is also nice. And if you let this agent lose, it will be in a sense the
4219040	4224800	optimal scientist is absolutely curious to learn as much as possible about the world. And of course,
4224800	4228800	it will also have a lot of instrumental goals, right? In order to learn, it needs to at least
4228800	4233920	survive, right? That that agent is not good for anything. So it needs to have self preservation.
4233920	4241040	And if it builds small helpers acquiring more information, it will do that. Yeah, if exploration,
4241040	4246240	space exploration or whatever is necessary, right, to gathering information and develop it. So it has
4246240	4252240	a lot of instrumental goals following on this information gain. And this agent is completely
4252240	4258240	autonomous of us. No rewards necessary anymore. Yeah, of course, you could find a way to gain
4258320	4265680	the concept of information and get stuck in that library that you mentioned beforehand
4265680	4272000	with a with a very large number of books. The first agent had this problem. It would get stuck
4272000	4277760	in front of an old TV screen, which has just said white noise. Yeah, white noise. But the second
4277760	4284560	version can deal with at least stochasticity. Well, yeah, what about curiosity, this kind of word
4285360	4291920	curiosity, creativity? Is that kind of the reward function being of getting new information?
4291920	4301280	Is that similar to idea of kind of injecting exploration for its own sake inside the reward
4301280	4306240	function? Do you find this at all appealing, interesting? I think that's a nice definition.
4306240	4311600	Curiosity is the reward. Sorry, curiosity is exploration for its own sake.
4314720	4320960	Yeah, I would accept that. But most curiosity, while in humans and especially in children,
4320960	4326320	yeah, is not just for its own sake, but for actually learning about the environment and for
4326320	4334720	behaving better. So I would, I think most curiosity is tied in the end to what's performing better.
4334800	4341200	Well, okay, so if intelligent systems need to have this reward function, let me, you're an
4341200	4349600	intelligent system, currently passing the torrent test quite effectively. What's the reward function
4350800	4356480	of our human intelligence existence? What's the reward function that Marcus Hutter is operating
4356480	4363840	under? Okay, to the first question, the biological reward function is to survive and to spread.
4364480	4369680	And very few humans sort of are able to overcome this biological reward function.
4370880	4377280	But we live in a very nice world where we have lots of spare time and can still survive and
4377280	4383280	spread. So we can develop arbitrary other interests, which is quite interesting.
4383280	4389600	On top of that? On top of that, yeah. But the survival and spreading sort of is, I would say,
4390320	4394080	the goal or the reward function of humans that the core one.
4395120	4399040	I like how you avoided answering the second question, which a good intelligence system would.
4399600	4404160	So my, your own meaning of life and the reward function?
4404160	4409200	My own meaning of life and reward function is to find an AGI to build it.
4411040	4417280	Beautifully put. Okay, let's dissect the X even further. So one of the assumptions is kind of
4417280	4425920	infinity keeps creeping up everywhere, which, what are your thoughts on kind of bounded
4425920	4431920	rationality and sort of the nature of our existence and intelligence systems is that we're operating
4431920	4438720	always under constraints, under, you know, limited time, limited resources. How does that, how do
4438720	4445040	you think about that within the IXE framework, within trying to create an AGI system that operates
4445040	4450000	under these constraints? Yeah, that is one of the criticisms about IXE that it ignores
4450000	4456160	computational completely. And some people believe that intelligence is inherently tied to what's
4457040	4461600	bounded resources. What do you think on this one point? Do you think it's,
4462400	4465280	do you think the bounded resources are fundamental to intelligence?
4467760	4474560	I would say that an intelligence notion which ignores computational limits is extremely useful
4475440	4480560	a good intelligence notion, which includes these resources would be even more useful,
4480560	4487520	but we don't have that yet. And so look at other fields outside of computer science,
4488400	4494800	computational aspects never play a fundamental role. You develop biological models for cells,
4494800	4499120	something in physics, these theories, I mean, become more and more crazy and harder and harder
4499120	4503520	to compute. Well, in the end, of course, we need to do something with this model, but there's more
4503600	4510400	nuisance than a feature. And I'm sometimes wondering if artificial intelligence would not
4510400	4515040	sit in a computer science department, but in a philosophy department, then this computational
4515040	4520000	focus would be probably significantly less. I mean, think about the induction problem is more
4520000	4525040	in the philosophy department. There's virtually no paper who cares about, you know, how long it
4525040	4530480	takes to compute the answer that is completely secondary. Of course, once we have figured out
4530480	4536080	the first problem, so intelligence without computational resources, then
4537360	4542320	the next and very good question is, could we improve it by including computational resources,
4542320	4547600	but nobody was able to do that so far in an even halfway satisfactory manner?
4549040	4553680	I like that that's in the long run, the right department to belong to is philosophy.
4554640	4561840	That's actually quite a deep idea of or even to at least to think about big picture
4561840	4567440	philosophical questions, big picture questions, even in the computer science department. But
4567440	4573840	you've mentioned approximation, sort of, there's a lot of infinity, a lot of huge resources needed.
4573840	4578960	Are there approximations to IHC that within the IHC framework that are useful?
4579680	4587840	Yeah, we have to develop a couple of approximations. And what we do there is that the
4587840	4593520	Solomov induction part, which was, you know, find the shortest program describing your data,
4593520	4598880	which just replaces by standard data compressors, right? And the better compressors get,
4598880	4603520	you know, the better this part will become. We focus on a particular compressor called
4603520	4609600	Context Rewaiting, which is pretty amazing, not so well known. It has beautiful theoretical
4609600	4613840	properties also works reasonably well in practice. So we use that for the approximation
4613840	4620560	of the induction and the learning and the prediction part. And for the planning part,
4621600	4628000	we essentially just took the ideas from a computer go from 2006. It was Java,
4628000	4635760	Cyprus, Bari, also now a DeepMind, who developed the so called UCT algorithm, upper confidence
4635760	4640400	bound for trees algorithm, on top of the Monte Carlo tree search. So we approximate this planning
4640400	4652240	part by sampling. And it's successful on some small toy problems. We don't want to lose the
4652240	4655840	generality, right? And that's sort of the handicap, right? If you want to be general,
4656080	4661840	you have to give up something. So but this single agent was able to play, you know, small games
4661840	4671920	like Coon poker and tic-tac-toe and, and even Pac-Man. And the same architecture, no change.
4671920	4677520	The agent doesn't know the rules of the game, really nothing at all by self or by a player
4677520	4683680	with these environments. So you're gonna Schmidt, who were proposed something called
4683680	4688160	Ghetto Machines, which is a self improving program that rewrites its own code.
4690800	4695040	Sort of mathematically or philosophically, what's the relationship in your eyes,
4695040	4698320	if you're familiar with it between AXI and the Ghetto Machines?
4698320	4701120	Yeah, familiar with it. He developed it while I was in his lab.
4702160	4710000	Yeah. So the Ghetto Machine, explain briefly. You give it a task. It could be a simple task as,
4710000	4713920	you know, finding prime factors in numbers, right? You can formally write it down. There's
4713920	4719200	a very slow algorithm to do that. Just all try all the factors. Yeah. Or play chess, right?
4719200	4723920	Optimally, you write the algorithm to minimax to the end of the game. So you write down what the
4723920	4729840	Ghetto Machine should do. Then it will take part of its resources to run this program.
4730640	4736880	And other part of the sources to improve this program. And when it finds an improved version,
4736880	4744000	which provably computes the same answer. So that's the key part. It needs to prove by itself
4744000	4750000	that this change of program still satisfies the original specification. And if it does so,
4750000	4754160	then it replaces the original program by the improved program. And by definition,
4754160	4759120	it does the same job, but just faster. Okay. And then, you know, it proves over it and over it.
4759120	4766640	And it's developed in a way that all parts of this Ghetto Machine can self improve.
4766640	4773840	But it stays provably consistent with the original specification. So from this perspective,
4773840	4779440	it has nothing to do with IxE. But if you would now put IxE as the starting axioms in,
4780560	4786320	it would run IxE. But, you know, that takes forever. But then if it finds a provable
4787200	4792160	speed up of IxE, it would replace it by this and this and this and maybe eventually it comes
4792160	4798960	up with a model which is still the IxE model. It cannot be, I mean, just for the knowledgeable
4798960	4804400	reader, IxE is incomputable. And that can prove that therefore there cannot be a computable
4805360	4811120	exact algorithm of computers. There needs to be some approximations. And this is not dealt
4811120	4814400	with the Ghetto Machine. So you have to do something about it. But there's the IxE TL model,
4814400	4819120	which is finally computable, which we could put in. Which part of IxE is non computable?
4819120	4822160	The Solomonov induction part. The induction. Okay, so.
4822160	4829120	But there's ways of getting computable approximations of the IxE model. So then it's at least
4829120	4834800	computable. It is still way beyond any resources anybody will ever have. But then the Ghetto Machine
4834800	4841280	could sort of improve it further and further in an exact way. So is this theoretically possible that
4842160	4850880	the Ghetto Machine process could improve? Isn't IxE already optimal?
4851760	4860640	It is optimal in terms of the reward collected over its interaction cycles. But it takes infinite
4860640	4868080	time to produce one action. And the world continues whether you want it or not. So the model is
4868080	4872800	assuming it had an oracle, which solved this problem, and then in the next 100 milliseconds
4872800	4879280	or the reaction time you need gives the answer, then IxE is optimal. It's optimal in sense of
4879280	4885280	data, also from learning efficiency and data efficiency, but not in terms of computation
4885280	4890160	time. And then the Ghetto Machine in theory, but probably not provably could make it go faster.
4890880	4899280	Yes. Okay. Interesting. Those two components are super interesting. The perfect intelligence
4899280	4907920	combined with self-improvement. Sort of provable self-improvement in sense you're always getting
4907920	4913760	the correct answer and you're improving. Beautiful ideas. Okay, so you've also mentioned that
4914480	4921360	different kinds of things in the chase of solving this reward, sort of optimizing for the goal,
4923040	4928560	interesting human things could emerge. So is there a place for consciousness within IxE?
4930800	4937360	Where does, maybe you can comment, because I suppose we humans are just another instantiation
4937360	4943360	by IxE agents and we seem to have consciousness. You say humans are an instantiation of an IxE agent?
4943360	4948240	Yes. Oh, that would be amazing. But I think that's not true even for the smartest and most
4948240	4954240	rational humans. I think maybe we are very crude approximations. Interesting. I mean, I tend to
4954240	4963760	believe, again, I'm Russian, so I tend to believe our flaws are part of the optimal. So we tend to
4963760	4969920	laugh off and criticize our flaws and I tend to think that that's actually close to an optimal
4969920	4974960	behavior. Well, some flaws, if you think more carefully about it, are actually not flaws here,
4974960	4981840	but I think there are still enough flaws. I don't know. It's unclear. As a student of history,
4981840	4988880	I think all the suffering that we've endured as a civilization, it's possible that that's the
4988880	4995680	optimal amount of suffering we need to endure to minimize long-term suffering. That's your Russian
4995760	5001760	background. That's the Russian. Whether humans are or not instantiations of an IxE agent,
5001760	5007440	do you think there's consciousness is something that could emerge in a computational form of
5007440	5012720	framework like IxE? Let me also ask you a question. Do you think I'm conscious?
5016720	5024160	That's a good question. That tie is confusing me, but I think so.
5024240	5026880	You think that makes me unconscious because it strangles me?
5027520	5031280	If an agent were to solve the imitation game posed by Turing, I think that would be
5031280	5037440	dressed similarly to you. Because there's a kind of flamboyant, interesting,
5038960	5045440	complex behavior pattern that sells that you're human and you're conscious. But why do you ask?
5046080	5047440	Was it a yes or was it a no?
5047840	5052000	Yes, I think you're conscious, yes.
5052640	5059600	So, and you explain somehow why, but you infer that from my behavior. You can never be sure
5059600	5066720	about that. And I think the same thing will happen with any intelligent agent we develop
5066720	5073200	if it behaves in a way sufficiently close to humans. Or maybe if not humans, maybe a dog
5073200	5079920	is also sometimes a little bit self-conscious. So, if it behaves in a way where we attribute
5079920	5084320	typically consciousness, we would attribute consciousness to these intelligent systems
5084320	5089840	and IxE probably in particular. That, of course, doesn't answer the question whether it's really
5089840	5095920	conscious. And that's the big hard problem of consciousness. Maybe I'm a zombie. I mean,
5095920	5098400	not the movie zombie, but the philosophical zombie.
5099360	5106480	It's to you, the display of consciousness close enough to consciousness from a perspective of AGI
5106480	5111200	that the distinction of the heart problem of consciousness is not an interesting one.
5111200	5114720	I think we don't have to worry about the consciousness problem, especially the heart
5114720	5122480	problem for developing AGI. I think we progress. At some point, we have solved all the technical
5122560	5126720	problems and this system will behave intelligent and then super intelligent and
5127760	5133040	this consciousness will emerge. I mean, definitely it will display behavior which we
5133040	5139040	will interpret as conscious. And then it's a philosophical question. Did this consciousness
5139040	5144960	really emerge? Or is it a zombie which just fakes everything? We still don't have to figure that
5144960	5149360	out. Although it may be interesting, at least from a philosophical point of view. It's very
5149360	5153840	interesting, but it may also be sort of practically interesting. You know, there's some people
5153840	5157920	saying, if it's just faking consciousness and feelings, then we don't need to be concerned
5157920	5162720	about rights. But if it's real conscious and has feelings, then we need to be concerned.
5165840	5171920	I can't wait till the day where AI systems exhibit consciousness because it'll truly
5171920	5175600	be some of the hardest ethical questions of what we do with that.
5175600	5182560	It is rather easy to build systems which people ascribe consciousness. And I give you an analogy.
5182560	5186080	I mean, remember, maybe it was before you were born, the Tamagotchi.
5188640	5189520	How dare you, sir.
5190880	5193200	Why that's so... You're young, right?
5193200	5197600	Yes, it's good to think. Thank you. Thank you very much. But I was also in the Soviet Union. We
5197600	5201120	didn't have... We didn't have any of those fun things.
5201120	5203840	But you have heard about this Tamagotchi, which was, you know, really,
5203920	5210720	really primitive. Actually, for the time it was... And you could raise this. And kids got so
5210720	5217360	attached to it and didn't want to let it die. And probably if we would have asked the children,
5217360	5220320	do you think this Tamagotchi is conscious? They would have said yes.
5221520	5226800	I think that's kind of a beautiful thing, actually, because that consciousness, ascribing
5226800	5233120	consciousness seems to create a deeper connection, which is a powerful thing. But we have to be
5233120	5238880	careful on the ethics side of that. Well, let me ask about the AGI community broadly. You kind of
5238880	5246320	represent some of the most serious work on AGI, at least earlier. And DeepMind represents
5247120	5254000	serious work on AGI these days. But why, in your sense, is the AGI community so small,
5254000	5260240	or has been so small, until maybe DeepMind came along? Like, why aren't more people
5260240	5267120	seriously working on human level and superhuman level intelligence from a formal perspective?
5268160	5274080	Okay, from a formal perspective, that's sort of, you know, an extra point. So I think there are
5274080	5278400	a couple of reasons. I mean, AI came in waves, right? You know, AI winters and AI summers,
5278400	5287200	and then there were big promises, which were not fulfilled. And people got disappointed. But
5288080	5294240	narrow AI, solving particular problems, which seemed to require intelligence, was
5295200	5300480	always to some extent successful, and there were improvements, small steps. And if you build
5300480	5306480	something which is, you know, useful for society or industrial useful, then there's a lot of funding.
5306480	5314480	So I guess it was in parts the money, which drives people to develop specific system solving
5314480	5320640	specific tasks. But you would think that, you know, at least in university, you should be able to do
5321280	5326800	ivory tower research. And that was probably better a long time ago. But even nowadays,
5326800	5332960	there's quite some pressure of doing applied research or translational research. And, you
5332960	5340480	know, it's harder to get grants as a theorist. So that also drives people away. It's maybe also
5340480	5344960	harder, attacking the general intelligence problem. So I think enough people, I mean,
5344960	5351920	maybe a small number, we're still interested in, in formalizing intelligence and, and thinking of
5351920	5359200	general intelligence. But, you know, not much came up, right? Or not not much great stuff came up.
5359760	5366000	So what do you think we talked about the formal big light at the end of the tunnel,
5366000	5369440	but from the engineering perspective, what do you think it takes to build an AI system?
5370240	5375680	Is that, and I don't know if that's a stupid question or a distinct question from everything
5375680	5380880	we've been talking about AIXE. But what do you see as the steps that are necessary to take
5380880	5386160	to start to try to build something? So you want a blueprint now, and then you go off and do it?
5386160	5390320	That's the whole point of this conversation, trying to squeeze that in there. Now, is there,
5390320	5395360	I mean, what's your intuition? Is it is in the robotic space or something that has a body and
5395360	5399920	tries to explore the world? Is in the reinforcement learning space, like the efforts of Alpha
5399920	5405360	Zero and Alpha Star, they're kind of exploring how you can solve it through in the, in the simulation
5405360	5411760	in the gaming world. Is there stuff in sort of the, all the transformer work in natural
5411760	5416640	English processing, sort of maybe attacking the open domain dialogue? Like what, what,
5416640	5424800	where do you see the promising pathways? Let me pick the embodiment maybe. So
5425600	5437600	embodiment is important, yes and no. I don't believe that we need a physical robot
5438560	5445440	walking or rolling around interacting with the real world in order to achieve AGI. And
5447680	5452800	I think it's more of a distraction probably than helpful. It's sort of confusing the body
5452800	5459440	with the mind. For industrial applications or near term applications, of course, we need
5459440	5466400	robots for all kinds of things, but for solving the big problem, at least at this stage, I think
5466400	5473520	it's not necessary. But the answer is also yes, that I think the most promising approach is that
5473520	5480000	you have an agent, and that can be a virtual agent in a computer interacting with an environment,
5480080	5483840	possibly, you know, a 3D simulated environment like in many computer games.
5485280	5493040	And, and you train and learn the agent. Even if you don't intend to later put it sort of, you know,
5493040	5498480	this algorithm in a, in a robot brain and leave it forever in the virtual reality,
5498480	5503440	getting experience in a, although it's just simulated 3D world,
5504240	5513520	is possibly, and as I possibly important to understand things on a similar level as humans do,
5515040	5519920	especially if the agent or primarily if the agent wants, needs to interact with the humans,
5519920	5524160	right? You know, if you talk about objects on top of each other in space and flying and cars and
5524160	5530800	so on, and the agent has no experience with even virtual 3D worlds, it's probably hard to grasp.
5531120	5537840	So if we develop an abstract agent, say we take the mathematical path, and we just want to build
5537840	5542400	an agent which can prove theorems and becomes a better and better mathematician, then this agent
5542400	5547920	needs to be able to reason in very abstract spaces, and then maybe sort of putting it into
5547920	5553280	3D environments, simulated world is even harmful, it should sort of, you put it in, I don't know,
5553280	5558400	an environment which it creates itself or so. It seems like you have an interesting,
5558400	5563680	rich complex trajectory through life in terms of your journey of ideas. So it's interesting to
5563680	5572640	ask what books, technical fiction, philosophical books, ideas, people had a transformative effect.
5572640	5577920	Books are most interesting because maybe people could also read those books and see if they could
5577920	5585440	be inspired as well. Yeah, luckily I asked books and not singular book, it's very hard and I try
5585440	5595920	to pin down one book, and I can do that at the end. So the books which were most transformative
5595920	5606320	for me or which I can most highly recommend to people interested in AI, I would always start
5606320	5613600	with Russell and Norbic, Artificial Intelligence and Modern Approach, that's the AI Bible, it's
5613600	5620400	an amazing book, it's very broad, it covers all approaches to AI and even if you focus on one
5620400	5624560	approach, I think that is the minimum you should know about the other approaches out there,
5624560	5628320	so that should be your first book. Fourth edition should be coming out soon.
5628320	5634000	Oh, okay, interesting. There's a deep learning chapter now as there must be, written by Ian
5634000	5640880	Goodfellow, okay. And then the next book I would recommend, The Reinforcement Learning Book by
5640880	5648960	Sutton and Bartow. There's a beautiful book, if there's any problem with the book, it makes RL
5650400	5656640	feel and look much easier than it actually is. It's very gentle book, it's very nice to read the
5656640	5663280	exercises, you can very quickly get some RL systems to run, very toy problems, but it's a lot of fun
5663280	5670800	and in a couple of days you feel you know what RL is about, but it's much harder than
5670800	5680320	the book. Come on now, it's an awesome book. Yeah, no, no, it is, yeah. And maybe, I mean,
5680320	5683680	there's so many books out there, if you like the information theoretic approach, then there's
5683680	5690720	Kolmogorff Complexity by Aline Vitani, but probably, you know, some short article is enough,
5690720	5697920	you don't need to read the whole book, but it's a great book. And if you have to mention one
5697920	5704240	all-time favorite book, it's a different flavor, that's a book which is used in the international
5704240	5710880	baccalaureate for high school students in several countries. That's from Nikolas Altjen,
5710880	5717520	Theory of Knowledge, second edition, or first, not the third, please. The third one they put,
5717520	5726480	they took out all the fun. Okay, so this asks all the interesting, or to me, interesting
5726480	5730000	philosophical questions about how we acquire knowledge from all perspectives, you know,
5730000	5736800	from math, from art, from physics, and ask how can we know anything? And the book is called
5736800	5740800	Theory of Knowledge. From which, is this almost like a philosophical exploration of
5741680	5745040	how we get knowledge from anything? Yes, yeah, I mean, can religion tell us, you know,
5745040	5748800	about something about the world? Can science tell us something about the world? Can mathematics,
5748800	5755120	or is it just playing with symbols? And you know, it's open-ended questions, and I mean,
5755120	5758960	it's for high school students, so they have the resources from Hitchhiker's Guide to the Galaxy
5758960	5765360	and from Star Wars and the Chicken Cross the Road, yeah. And it's fun to read, but it's also quite
5765360	5772720	deep. If you could live one day of your life over again, because it made you truly happy,
5772720	5778080	or maybe like we said with the books, it was truly transformative, what day, what moment would you
5778080	5783840	choose? Does something pop into your mind? Does it need to be a day in the past, or can it be a
5783840	5790720	day in the future? Well, space-time is an emerging phenomena, so it's all the same anyway. Okay.
5791920	5797600	Okay, from the past. You're really going to say from the future, I love it. No, I will tell you
5797600	5802960	from the future. Okay, from the past. So from the past, I would say when I discovered my AXI model,
5803680	5808000	I mean, it was not in one day, but it was one moment where I realized
5808720	5815440	comagor of complexity. I didn't even know that it existed, but I discovered sort of this compression
5815440	5820640	idea myself, but immediately I knew I can't be the first one, but I had this idea. And then I
5820640	5825680	knew about sequential decisionary, and I knew if I put it together, this is the right thing.
5826320	5832320	And yeah, still when I think back about this moment, I'm super excited about it.
5832400	5837680	Was there any more details in context that moment? Did an apple fall in your head?
5840000	5846080	So like if you look at Ian Goodfellow talking about gans, there was beer involved. Is there
5847200	5853120	some more context of what sparked your thought, or was it just? No, it was much more mundane. So I
5853120	5857360	worked in this company. So in this sense, the four and a half years was not completely wasted.
5857600	5867360	So and I worked on an image interpolation problem. And I developed quite neat new
5867360	5872160	interpolation techniques, and they got patented. And then, you know, which happens quite often,
5872160	5875840	I got sort of overboard and thought about, you know, yeah, that's pretty good, but it's not the
5875840	5881200	best. So what is the best possible way of doing interpolation? And then I thought, yeah, you
5881200	5886480	want the simplest picture, which is if you coarse-grain it, recovers your original picture.
5886480	5892960	And then I thought about the simplicity concept more in quantitative terms. And yeah, then
5892960	5898960	everything developed. And somehow the full beautiful mix of also being a physicist and
5898960	5905120	thinking about the big picture of it, then led you to probably beg with ice. Yeah. So as a physicist,
5905120	5909280	I was probably trained not to always think in computational terms, you know, just ignore that
5909280	5913120	and think about the fundamental properties which you want to have.
5913920	5919040	So what about if you could really live one day in the future? What would that be?
5919760	5925040	When I solve the AGI problem. In practice, in practice. So in theory,
5925040	5930000	I have solved it with the IHC model, but in practice. And then I asked the first question.
5930640	5934160	What would be the first question? What's the meaning of life?
5934400	5939600	I don't think there's a better way to end it. Thank you so much for talking today. It's
5939600	5943360	a huge honor to finally meet you. Yeah, thank you too. It was a pleasure of mine side too.
5944560	5948160	Thanks for listening to this conversation with Marcus Hutter. And thank you to our
5948160	5954560	presenting sponsor, Cash App. Download it, use code LEX Podcast. You'll get $10 and $10 will go
5954560	5959440	to first, an organization that inspires and educates young minds to become science and
5959440	5964960	technology innovators of tomorrow. If you enjoy this podcast, subscribe on YouTube,
5964960	5970320	give it five stars on Apple Podcasts, support on Patreon, or simply connect with me on Twitter
5970320	5977120	at Lex Friedman. And now let me leave you with some words of wisdom from Albert Einstein.
5977840	5985600	The measure of intelligence is the ability to change. Thank you for listening and hope to see you
5985600	5986880	next time.
