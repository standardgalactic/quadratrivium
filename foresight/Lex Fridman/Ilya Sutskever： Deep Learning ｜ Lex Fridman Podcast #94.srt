1
00:00:00,000 --> 00:00:05,360
The following is a conversation with Ilya Satskeva, co-founder and chief scientist of Open AI,

2
00:00:06,080 --> 00:00:12,800
one of the most cited computer scientists in history with over 165,000 citations,

3
00:00:13,440 --> 00:00:19,040
and to me, one of the most brilliant and insightful minds ever in the field of deep learning.

4
00:00:19,920 --> 00:00:24,240
There are very few people in this world who I would rather talk to and brainstorm with about

5
00:00:24,240 --> 00:00:31,920
deep learning, intelligence, and life in general than Ilya, on and off the mic. This was an honor

6
00:00:31,920 --> 00:00:37,120
and a pleasure. This conversation was recorded before the outbreak of the pandemic,

7
00:00:37,120 --> 00:00:41,360
for everyone feeling the medical, psychological, and financial burden of this crisis,

8
00:00:41,360 --> 00:00:46,160
I'm sending love your way. Stay strong, we're in this together, we'll beat this thing.

9
00:00:47,120 --> 00:00:51,680
This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,

10
00:00:51,680 --> 00:00:56,240
review it with Five Stars and Apple Podcasts, support on Patreon, or simply connect with me on

11
00:00:56,240 --> 00:01:02,880
Twitter at Lex Freedman's belt F-R-I-D-M-A-N. As usual, I'll do a few minutes of ads now

12
00:01:02,880 --> 00:01:06,480
and never any ads in the middle that can break the flow of the conversation.

13
00:01:06,480 --> 00:01:09,840
I hope that works for you and doesn't hurt the listening experience.

14
00:01:10,880 --> 00:01:15,600
This show is presented by Cash App, the number one finance app in the App Store.

15
00:01:15,600 --> 00:01:20,880
When you get it, use code LexPodcast. Cash App lets you send money to friends,

16
00:01:20,880 --> 00:01:26,720
buy Bitcoin, invest in the stock market with as little as $1. Since Cash App allows you to buy

17
00:01:26,720 --> 00:01:32,400
Bitcoin, let me mention that cryptocurrency in the context of the history of money is fascinating.

18
00:01:32,960 --> 00:01:36,080
I recommend Ascent of Money as a great book on this history.

19
00:01:36,720 --> 00:01:42,000
Both the book and audiobook are great. Debits and credits on ledgers started around

20
00:01:42,000 --> 00:01:48,880
30,000 years ago. The US Dollar created over 200 years ago, and Bitcoin, the first decentralized

21
00:01:48,880 --> 00:01:54,880
cryptocurrency released just over 10 years ago. Given that history, cryptocurrency is still very

22
00:01:54,880 --> 00:02:00,720
much in its early days of development, but is still aiming to and just might redefine the nature of

23
00:02:00,720 --> 00:02:07,440
money. Again, if you get Cash App from the App Store or Google Play and use the code LexPodcast,

24
00:02:08,000 --> 00:02:14,160
you get $10 and Cash App will also donate $10 to FIRST, an organization that is helping advance

25
00:02:14,160 --> 00:02:20,560
robotics and STEM education for young people around the world. And now, here's my conversation

26
00:02:20,560 --> 00:02:27,680
with Ilya Satskeva. You were one of the three authors with Alex Koshchevsky, Jeff Hinton,

27
00:02:27,680 --> 00:02:35,200
of the famed AlexNet paper that is arguably the paper that marked the big catalytic moment that

28
00:02:35,200 --> 00:02:39,920
launched the deep learning revolution. At that time, take us back to that time. What was your

29
00:02:39,920 --> 00:02:44,880
intuition about neural networks, about the representation of power of neural networks?

30
00:02:45,920 --> 00:02:52,000
And maybe you could mention how did that evolve over the next few years, up to today, over the

31
00:02:52,000 --> 00:02:58,240
10 years? Yeah, I can answer that question. At some point in about 2010 or 2011,

32
00:02:59,920 --> 00:03:08,080
I connected two facts in my mind. Basically, the realization was this. At some point,

33
00:03:08,080 --> 00:03:13,520
we realized that we can train very large, I shouldn't say very tiny by today's standards, but

34
00:03:14,320 --> 00:03:19,200
large and deep neural networks end to end with back propagation. At some point,

35
00:03:20,640 --> 00:03:26,320
different people obtained this result. I obtained this result. The first moment in which I realized

36
00:03:26,320 --> 00:03:32,160
that deep neural networks are powerful was when James Martens invented the Hessian Free Optimizer

37
00:03:32,160 --> 00:03:38,080
in 2010. And he trained a 10 layer neural network end to end without pre-training

38
00:03:39,600 --> 00:03:45,120
from scratch. And when that happened, I thought this is it. Because if you can train a big neural

39
00:03:45,120 --> 00:03:50,160
network, a big neural network can represent very complicated function. Because if you have a neural

40
00:03:50,160 --> 00:03:58,240
network with 10 layers, it's as though you allow the human brain to run for some number of milliseconds,

41
00:03:58,320 --> 00:04:04,640
neuron firings are slow. And so in maybe 100 milliseconds, your neurons only fire 10 times.

42
00:04:04,640 --> 00:04:09,440
So it's also kind of like 10 layers. And in 100 milliseconds, you can perfectly recognize any

43
00:04:09,440 --> 00:04:14,800
object. So I thought, so I already had the idea then that we need to train a very big neural network

44
00:04:16,080 --> 00:04:20,560
on lots of supervised data. And then it must succeed, because we can find the best neural

45
00:04:20,560 --> 00:04:24,800
network. And then there's also theory that if you have more data than parameters, you won't

46
00:04:24,800 --> 00:04:28,880
overfit. Today, we know that actually, this theory is very incomplete, and you won't overfit even

47
00:04:28,880 --> 00:04:32,640
you have less data than parameters. But definitely, if you have more data than parameters, you won't

48
00:04:32,640 --> 00:04:38,640
overfit. So the fact that neural networks were heavily overparameterized, wasn't discouraging to

49
00:04:38,640 --> 00:04:43,920
you. So you were thinking about the theory that the number of parameters, the fact there's a huge

50
00:04:43,920 --> 00:04:47,360
number of parameters is okay, it's going to be okay. I mean, there was some evidence before that

51
00:04:47,360 --> 00:04:51,680
it was okay, but the theory was most the theory was that if you had a big data set and a big

52
00:04:51,680 --> 00:04:56,640
neural net, it was going to work. The overparameterization just didn't really figure much as a

53
00:04:56,640 --> 00:04:59,440
problem. I thought, well, with images, you just go and add some data augmentation, and it's going

54
00:04:59,440 --> 00:05:04,560
to be okay. So where was any doubt coming from? The main doubt was can we train a bigger, really

55
00:05:04,560 --> 00:05:08,160
have enough compute to train a big enough neural net with back propagation, back propagation,

56
00:05:08,160 --> 00:05:12,320
I thought was would work. This image wasn't clear would was whether there would be enough compute

57
00:05:12,320 --> 00:05:16,720
to get a very convincing result. And then at some point, Alex Krzewski wrote these insanely fast

58
00:05:16,800 --> 00:05:21,120
CUDA kernels for training convolutional neural nets. Net was bam, let's do this. Let's get

59
00:05:21,120 --> 00:05:26,160
image net and it's going to be the greatest thing. Was your intuition, most of your intuition from

60
00:05:26,160 --> 00:05:32,560
empirical results by you and by others? So like just actually demonstrating that a piece of program

61
00:05:32,560 --> 00:05:39,200
can train a 10 layer neural network? Or was there some pen and paper or marker and white board

62
00:05:39,200 --> 00:05:45,360
thinking intuition? Because you just connected a 10 layer large neural network to the brain.

63
00:05:45,360 --> 00:05:49,760
So you just mentioned the brain. So in your intuition about neural networks, does the human

64
00:05:49,760 --> 00:05:56,080
brain come into play as a intuition builder? Definitely. I mean, you know, you got to be

65
00:05:56,080 --> 00:06:00,480
precise with these analogies between neural artificial neural networks in the brain. But

66
00:06:01,120 --> 00:06:06,480
there's no question that the brain is a huge source of intuition and inspiration for deep

67
00:06:06,480 --> 00:06:12,320
learning researchers since all the way from Rosenblatt in the 60s. Like, if you look at the

68
00:06:12,320 --> 00:06:16,640
whole idea of a neural network is directly inspired by the brain. You had people like

69
00:06:16,640 --> 00:06:22,800
McCallum and Pitts who were saying, hey, you got these neurons in the brain. And hey, we recently

70
00:06:22,800 --> 00:06:26,480
learned about the computer and automata. Can we use some ideas from the computer and automata to

71
00:06:26,480 --> 00:06:31,920
design some kind of computational object that's going to be simple, computational and kind of

72
00:06:31,920 --> 00:06:36,320
like the brain and invented the neuron. So they were inspired by it back then. Then you had the

73
00:06:36,320 --> 00:06:40,960
convolutional neural network from Fukushima. And then later young Lacan, who said, Hey, if you

74
00:06:40,960 --> 00:06:45,280
limit the receptive fields of a neural network, it's going to be especially suitable for images,

75
00:06:45,280 --> 00:06:51,040
as it turned out to be true. So there was a very small number of examples where analogies to the

76
00:06:51,040 --> 00:06:56,240
brain were successful. And I thought, well, probably an artificial neuron is not that different from

77
00:06:56,240 --> 00:07:01,600
the brain if it's cleaned hard enough. So let's just assume it is and roll with it. So we're now

78
00:07:01,600 --> 00:07:08,480
at a time where deep learning is very successful. So let us squint less and say, let's open our

79
00:07:08,560 --> 00:07:14,080
eyes and say, what do you use an interesting difference between the human brain? Now, I know

80
00:07:14,080 --> 00:07:19,600
you're probably not an expert, neither in your scientists and your biologists, but loosely speaking,

81
00:07:19,600 --> 00:07:23,120
what's the difference between the human brain and artificial neural networks? That's interesting

82
00:07:23,120 --> 00:07:28,720
to you for the next decade or two. That's a good question to ask. What is an interesting difference

83
00:07:28,720 --> 00:07:34,000
between the neural between the brain and our artificial neural networks? So I feel like today,

84
00:07:34,880 --> 00:07:40,000
artificial neural networks, so we all agree that there are certain dimensions in which the human

85
00:07:40,000 --> 00:07:45,280
brain vastly outperforms our models. But I also think that there are some ways in which artificial

86
00:07:45,280 --> 00:07:51,440
neural networks have a number of very important advantages over the brain. Looking at the advantages

87
00:07:51,440 --> 00:07:56,720
versus disadvantages is a good way to figure out what is the important difference. So the brain

88
00:07:57,360 --> 00:08:01,760
uses spikes, which may or may not be important. Yes, that's a really interesting question. Do you

89
00:08:01,760 --> 00:08:07,200
think it's important or not? That's one big architectural difference between artificial

90
00:08:07,200 --> 00:08:13,600
neural networks. It's hard to tell, but my prior is not very high. And I can say why. There are

91
00:08:13,600 --> 00:08:17,920
people who are interested in spiking neural networks. And basically, what they figured out is

92
00:08:17,920 --> 00:08:23,280
that they need to simulate the non-spiking neural networks in spikes. And that's how they're going

93
00:08:23,280 --> 00:08:27,200
to make them work. If you don't simulate the non-spiking neural networks in spikes, it's not

94
00:08:27,200 --> 00:08:30,560
going to work because the question is, why should it work? And that connects to questions around

95
00:08:30,560 --> 00:08:36,800
back propagation and questions around deep learning. You've got this giant neural network.

96
00:08:36,800 --> 00:08:39,840
Why should it work at all? Why should that learning rule work at all?

97
00:08:43,120 --> 00:08:47,200
It's not a self-evident question, especially if you, let's say, if you were just starting in the

98
00:08:47,200 --> 00:08:52,640
field and you read the very early papers, you can say, hey, people are saying, let's build neural

99
00:08:52,640 --> 00:08:56,640
networks. That's a great idea because the brain is a neural network, so it would be useful to

100
00:08:56,640 --> 00:09:01,360
build neural networks. Now, let's figure out how to train them. It should be possible to train

101
00:09:01,360 --> 00:09:09,200
them probably, but how? And so the big idea is the cost function. That's the big idea. The cost

102
00:09:09,200 --> 00:09:14,720
function is a way of measuring the performance of the system according to some measure.

103
00:09:14,720 --> 00:09:21,280
By the way, that is a big, actually, let me think. Is that one, a difficult idea to arrive at? And

104
00:09:21,280 --> 00:09:29,040
how big of an idea is that? That there's a single cost function? Sorry, let me take a pause. Is

105
00:09:29,040 --> 00:09:35,760
supervised learning a difficult concept to come to? I don't know. All concepts are very easy in

106
00:09:35,760 --> 00:09:40,720
retrospect. Yeah, that's what it seems trivial now. Because the reason I asked that, and we'll

107
00:09:40,720 --> 00:09:47,040
talk about it, is there other things? Is there things that don't necessarily have a cost function,

108
00:09:47,120 --> 00:09:51,680
maybe have many cost functions, or maybe have dynamic cost functions, or maybe

109
00:09:51,680 --> 00:09:55,840
a totally different kind of architectures? Because we have to think like that in order to

110
00:09:55,840 --> 00:10:01,040
arrive at something new, right? So the good examples of things which don't have clear cost

111
00:10:01,040 --> 00:10:07,200
functions are GANs. And again, you have a game. So instead of thinking of a cost function,

112
00:10:08,080 --> 00:10:12,000
where you want to optimize, where you know that you have an algorithm gradient descent,

113
00:10:12,000 --> 00:10:16,240
which will optimize the cost function. And then you can reason about the behavior of your system

114
00:10:16,240 --> 00:10:21,680
in terms of what it optimizes. With GAN, you say, I have a game, and I'll reason about the behavior

115
00:10:21,680 --> 00:10:25,600
of the system in terms of the equilibrium of the game. But it's all about coming up with these

116
00:10:25,600 --> 00:10:30,640
mathematical objects that help us reason about the behavior of our system. Right, that's really

117
00:10:30,640 --> 00:10:35,600
interesting. Yeah, so GAN is the only one. It's kind of a, the cost function is emergent from the

118
00:10:35,600 --> 00:10:40,160
comparison. I don't know if it has a cost function. I don't know if it's meaningful to talk about the

119
00:10:40,160 --> 00:10:44,240
cost function of a GAN. It's kind of like the cost function of biological evolution or the cost

120
00:10:44,240 --> 00:10:51,920
function of the economy. It's, you can talk about regions to which it will go towards, but I don't

121
00:10:51,920 --> 00:10:59,440
think, I don't think the cost function analogy is the most useful. So evolution doesn't, that's

122
00:10:59,440 --> 00:11:04,240
really interesting. So if evolution doesn't really have a cost function, like a cost function based

123
00:11:04,240 --> 00:11:11,920
on it's something akin to our mathematical conception of a cost function, then do you think

124
00:11:11,920 --> 00:11:17,440
cost functions in deep learning are holding us back? Yeah, I, so you just kind of mentioned

125
00:11:17,440 --> 00:11:23,440
that cost function is a, is a nice first profound idea. Do you think that's a good idea? Do you

126
00:11:23,440 --> 00:11:30,240
think it's an idea will go past? So self play starts to touch on that a little bit in reinforcement

127
00:11:30,240 --> 00:11:35,840
learning systems. That's right. Self play and also ideas around exploration where you're trying to

128
00:11:35,840 --> 00:11:40,800
take action. That's, that's surprise a predictor. I'm a big fan of cost functions. I think cost

129
00:11:40,800 --> 00:11:44,560
functions are great and they serve us really well. And I think that whenever we can do things with

130
00:11:44,560 --> 00:11:50,240
cost functions, we should. And you know, maybe there is a chance that we will come up with some

131
00:11:50,240 --> 00:11:55,440
yet another profound way of looking at things that will involve cost functions in a less central way.

132
00:11:55,440 --> 00:11:57,280
But I don't know, I think cost functions are, I mean,

133
00:11:59,840 --> 00:12:04,640
I would not bet against against cost functions. Is there other things about the brain

134
00:12:05,360 --> 00:12:10,240
that pop into your mind that might be different and interesting for us to consider

135
00:12:10,960 --> 00:12:14,880
in designing artificial neural networks? So we talked about spiking a little bit.

136
00:12:16,080 --> 00:12:19,280
I mean, one, one thing which may potentially be useful, I think people,

137
00:12:19,280 --> 00:12:22,320
neuroscientists figured out something about the learning rule of the brain or

138
00:12:22,880 --> 00:12:26,480
talking about spike time independent plasticity. And it would be nice if some people were to

139
00:12:26,480 --> 00:12:31,200
study that in simulation. Wait, sorry, spike time independent plasticity. Yeah, that's

140
00:12:31,680 --> 00:12:37,360
STD. It's a particular learning rule that uses spike timing to figure out how to determine how

141
00:12:37,360 --> 00:12:43,280
to update the synapses. So it's kind of like, if a synapse fires into the neuron before the neuron

142
00:12:43,280 --> 00:12:48,320
fires, then it's strengthened the synapse. And if the synapse fires into the neurons shortly

143
00:12:48,320 --> 00:12:52,960
after the neuron fired, then it becomes the synapse something along this line. I'm 90%

144
00:12:52,960 --> 00:12:58,400
sure it's right. So if I said something wrong here, don't don't get too angry.

145
00:12:59,280 --> 00:13:03,440
But you sounded brilliant while saying it. But the timing, that's one thing that's missing.

146
00:13:04,160 --> 00:13:10,000
The temporal dynamics is not captured. I think that's like a fundamental property of the brain

147
00:13:10,000 --> 00:13:14,480
is the timing of the signals. Well, you're recording your networks.

148
00:13:15,360 --> 00:13:21,920
But you think of that as, I mean, that's a very crude simplified, what's that called?

149
00:13:22,640 --> 00:13:29,600
There's a clock, I guess, to recurrent neural networks. This seems like the brain is the

150
00:13:29,600 --> 00:13:35,440
general, the continuous version of that, the generalization where all possible timings are

151
00:13:35,440 --> 00:13:41,200
possible. And then within those timings, this contains some information. You think recurrent

152
00:13:41,200 --> 00:13:47,520
neural networks, the recurrence in recurrent neural networks can capture the same kind of

153
00:13:47,600 --> 00:13:55,360
phenomena as the timing that seems to be important for the brain in the firing of neurons in the

154
00:13:55,360 --> 00:14:01,920
brain? I mean, I think recurrent neural networks are amazing and they can do,

155
00:14:02,560 --> 00:14:08,000
I think they can do anything we'd want them to, we'd want a system to do. Right now,

156
00:14:08,000 --> 00:14:12,000
recurrent neural networks have been superseded by transformers, but maybe one day they'll make

157
00:14:12,000 --> 00:14:18,240
a comeback, maybe they'll be back, we'll see. Let me in a small tangent say, do you think they'll

158
00:14:18,240 --> 00:14:24,240
be back? So so much of the breakthroughs recently that we'll talk about on natural language processing

159
00:14:24,240 --> 00:14:31,120
and language modeling has been with transformers that don't emphasize recurrence. Do you think

160
00:14:31,120 --> 00:14:35,920
recurrence will make a comeback? Well, some kind of recurrence, I think very likely.

161
00:14:36,800 --> 00:14:42,960
Recurrent neural networks, as they're typically thought of for processing sequences, I think it's

162
00:14:42,960 --> 00:14:49,120
also possible. What is, to you, a recurrent neural network? In general speaking, I guess,

163
00:14:49,120 --> 00:14:53,520
what is a recurrent neural network? You have a neural network which maintains a high dimensional

164
00:14:53,520 --> 00:14:59,200
hidden state. And then when an observation arrives, it updates its high dimensional hidden state

165
00:14:59,760 --> 00:15:06,960
through its connections in some way. So do you think, you know, that's what like expert systems

166
00:15:06,960 --> 00:15:17,040
did, right? Symbolic AI, the knowledge based, growing a knowledge base is maintaining a hidden state,

167
00:15:17,040 --> 00:15:21,040
which is its knowledge base and is growing it by sequential processing. Do you think of it more

168
00:15:21,120 --> 00:15:29,920
generally in that way? Or is it simply, is it the more constrained form of a hidden state with

169
00:15:29,920 --> 00:15:34,960
certain kind of gating units that we think of as today with LSTMs and that? I mean, the hidden

170
00:15:34,960 --> 00:15:39,280
state is technically what you described there, the hidden state that goes inside the LSTM or

171
00:15:39,280 --> 00:15:43,600
there are an N or something like this. But then what should be contained, you know, if you want to

172
00:15:43,600 --> 00:15:50,000
make the expert system and analogy, I'm not, I mean, you could say that the knowledge is stored

173
00:15:50,080 --> 00:15:55,120
in the connections and then the short term processing is done in the hidden state.

174
00:15:56,240 --> 00:16:02,800
Yes. Could you say that? So sort of, do you think there's a future of building large scale

175
00:16:03,440 --> 00:16:06,080
knowledge bases within the neural networks? Definitely.

176
00:16:08,960 --> 00:16:14,160
So we're going to pause on that confidence because I want to explore that. Well, let me zoom back out

177
00:16:14,160 --> 00:16:21,360
and ask back to the history of ImageNet. Neural networks have been around for many decades as

178
00:16:21,360 --> 00:16:27,280
you mentioned. What do you think were the key ideas that led to their success, that ImageNet moment

179
00:16:27,280 --> 00:16:34,240
and beyond the success in the past 10 years? Okay. So the question is to make sure I didn't

180
00:16:34,240 --> 00:16:39,280
miss anything, the key ideas that led to the success of deep learning over the past 10 years.

181
00:16:39,360 --> 00:16:44,800
Exactly. Even though the fundamental thing behind deep learning has been around for much longer.

182
00:16:45,360 --> 00:16:55,600
So the key idea about deep learning, or rather the key fact about deep learning before deep

183
00:16:55,600 --> 00:17:02,480
learning started to be successful is that it was underestimated. People who worked in machine

184
00:17:02,480 --> 00:17:08,080
learning simply didn't think that neural networks could do much. People didn't believe that large

185
00:17:08,080 --> 00:17:13,840
neural networks could be trained. People thought that, well, there was lots of, there was a lot of

186
00:17:13,840 --> 00:17:18,720
debate going on in machine learning about what are the right methods and so on. And people were

187
00:17:18,720 --> 00:17:23,760
arguing because there were no, there were no, there was no way to get hard facts. And by that,

188
00:17:23,760 --> 00:17:28,480
I mean, there were no benchmarks which were truly hard, that if you do really well on them, then

189
00:17:28,480 --> 00:17:36,640
you can say, look, here's my system. That's when you switch from, that's when this field becomes

190
00:17:36,640 --> 00:17:40,480
a little bit more of an engineering field. So in terms of deep learning to answer the question

191
00:17:40,480 --> 00:17:47,040
directly, the ideas were all there. The thing that was missing was a lot of supervised data and a lot

192
00:17:47,040 --> 00:17:53,200
of compute. Once you have a lot of supervised data and a lot of compute, then there is a third

193
00:17:53,200 --> 00:17:57,680
thing which is needed as well. And that is conviction, conviction that if you take

194
00:17:58,320 --> 00:18:03,360
the right stuff, which already exists, and apply and mixed with a lot of data and a lot of compute,

195
00:18:03,440 --> 00:18:09,440
that it will in fact work. And so that was the missing piece. It was you had the, you needed

196
00:18:09,440 --> 00:18:15,040
the data, you needed the compute which showed up in terms of GPUs, and you needed the conviction

197
00:18:15,040 --> 00:18:21,920
to realize that you need to mix them together. So that's really interesting. So I guess the

198
00:18:21,920 --> 00:18:28,880
presence of compute and the presence supervised data allowed the empirical evidence to do the

199
00:18:28,960 --> 00:18:33,120
convincing of the majority of the computer science community. So I guess there's a

200
00:18:33,120 --> 00:18:42,720
key moment with Jitendra Malik and Alex, Alyosha Efros, who were very skeptical, right? And then

201
00:18:42,720 --> 00:18:48,160
there's a Jeffrey Hinton that was the opposite of skeptical. And there was a convincing moment.

202
00:18:48,160 --> 00:18:53,840
And I think ImageNet served as that moment. And that represented this kind of, or the big

203
00:18:53,840 --> 00:19:00,480
pillars of computer vision community, kind of the wizards got together. And then all of a sudden

204
00:19:00,480 --> 00:19:06,240
there was a shift. And it's not enough for the ideas to all be there and the computer to be there.

205
00:19:06,240 --> 00:19:13,040
It's for it to convince the cynicism that existed that that's interesting that people just didn't

206
00:19:13,040 --> 00:19:20,560
believe for a couple of decades. Yeah, well, but it's more than that. It's kind of when put this

207
00:19:20,560 --> 00:19:25,440
way, it sounds like, well, you know, those silly people who didn't believe what were they missing.

208
00:19:25,440 --> 00:19:30,080
But in reality, things were confusing because neural networks really did not work on anything.

209
00:19:30,080 --> 00:19:35,520
And they were not the best method on pretty much anything as well. And it was pretty rational to

210
00:19:35,520 --> 00:19:41,680
say, yeah, this stuff doesn't have any traction. And that's why you need to have these very hard

211
00:19:41,680 --> 00:19:47,280
tasks which are which produce undeniable evidence. And that's how we make progress. And that's why

212
00:19:47,280 --> 00:19:51,600
the field is making progress today, because we have these hard benchmarks, which represent true

213
00:19:51,600 --> 00:19:59,840
progress. And so, and this is why we were able to avoid endless debate. So incredibly, you've

214
00:19:59,840 --> 00:20:05,920
contributed some of the biggest recent ideas in AI in computer vision, language, natural

215
00:20:05,920 --> 00:20:12,080
language processing, reinforcement learning, sort of everything in between, maybe not GANs.

216
00:20:12,800 --> 00:20:17,840
Is there, there may not be a topic you haven't touched. And of course, the fundamental science

217
00:20:17,840 --> 00:20:25,600
of deep learning. What is the difference to you between vision, language, and as in reinforcement

218
00:20:25,600 --> 00:20:30,400
learning action, as learning problems? And what are the commonalities? Do you see them as all

219
00:20:30,400 --> 00:20:36,320
interconnected? Are they fundamentally different domains that require different approaches?

220
00:20:36,880 --> 00:20:42,640
Okay, that's a good question. Machine learning is a field with a lot of unity, a huge amount of

221
00:20:42,640 --> 00:20:49,680
unity. In fact, what do you mean by unity, like overlap of ideas? overlap of ideas overlap of

222
00:20:49,680 --> 00:20:54,320
principles. In fact, there's only one or two or three principles, which are very, very simple.

223
00:20:54,320 --> 00:20:59,840
And then they apply in almost the same way, in almost the same way to the different modalities

224
00:20:59,840 --> 00:21:04,720
through the different problems. And that's why today, when someone writes a paper on improving

225
00:21:04,720 --> 00:21:09,200
optimization of deep learning and vision, it improves the different NLP applications,

226
00:21:09,200 --> 00:21:13,200
and it improves the different reinforcement learning applications. Reinforcement learning.

227
00:21:13,200 --> 00:21:20,000
So I would say that computer vision and NLP are very similar to each other. Today, they differ in

228
00:21:20,000 --> 00:21:24,640
that they have slightly different architectures. We use transformers in NLP, and we use convolutional

229
00:21:24,640 --> 00:21:29,760
neural networks in vision. But it's also possible that one day this will change and everything

230
00:21:29,760 --> 00:21:33,840
will be unified with a single architecture. Because if you go back a few years ago in

231
00:21:33,840 --> 00:21:39,760
natural language processing, there were a huge, huge number of architectures for every

232
00:21:39,760 --> 00:21:46,320
different tiny problem had its own architecture. Today, there's just one transformer for all

233
00:21:46,320 --> 00:21:50,560
those different tasks. And if you go back in time even more, you had even more and more

234
00:21:50,560 --> 00:21:56,320
fragmentation and every little problem in AI had its own little subspecialization and sub,

235
00:21:56,320 --> 00:22:00,800
you know, little set of collection of skills, people who would know how to engineer the features.

236
00:22:00,880 --> 00:22:05,040
Now it's all been subsumed by deep learning. We have this unification. And so I expect

237
00:22:05,680 --> 00:22:09,520
vision to become unified with natural language as well. Or rather, I shouldn't say expect,

238
00:22:09,520 --> 00:22:13,360
I think it's possible. I don't want to be too sure, because I think on the commercial

239
00:22:13,360 --> 00:22:17,280
neural network, it is very computationally efficient. Arell is different. Arell doesn't

240
00:22:17,280 --> 00:22:21,360
require slightly different techniques, because you really do need to take action. You really

241
00:22:21,360 --> 00:22:26,400
need to do something about exploration, your variance is much higher. But I think there

242
00:22:26,400 --> 00:22:30,240
is a lot of unity even there. And I would expect, for example, that at some point, there will be

243
00:22:30,240 --> 00:22:36,080
some broader unification between Arell and supervised learning, where somehow the Arell

244
00:22:36,080 --> 00:22:40,640
will be making decisions to make the supervised learning go better. And it will be, I imagine

245
00:22:40,640 --> 00:22:45,120
one big black box and you just throw every, you know, you shovel, shovel things into it. And it

246
00:22:45,120 --> 00:22:49,840
just figures out what to do with whatever you shovel in it. I mean, reinforcement learning has

247
00:22:49,840 --> 00:22:57,600
some aspects of language and vision combined, almost, there's elements of a long term memory

248
00:22:57,600 --> 00:23:02,240
that you should be utilizing, and there's elements of a really rich sensory space.

249
00:23:02,960 --> 00:23:09,040
So it seems like the, it's like the union of the two or something like that. I'd say something

250
00:23:09,040 --> 00:23:14,720
slightly different. I'd say that reinforcement learning is neither, but it naturally interfaces

251
00:23:14,720 --> 00:23:19,600
and integrates with the two of them. Do you think action is fundamentally different? So yeah,

252
00:23:19,600 --> 00:23:26,800
what is interesting about, what is unique about policy of learning to act? Well, so one example,

253
00:23:26,800 --> 00:23:32,480
for instance, is that when you learn to act, you are fundamentally in a non stationary world.

254
00:23:33,120 --> 00:23:40,480
Because as your actions change, the things you see start changing. You, you experience the world

255
00:23:40,480 --> 00:23:45,040
in a different way. And this is not the case for the more traditional static problem where you have

256
00:23:45,040 --> 00:23:50,320
a some distribution and you just apply a model to that distribution. You think it's a fundamentally

257
00:23:50,320 --> 00:23:55,520
different problem or is it just a more difficult general, it's a generalization of the problem

258
00:23:55,520 --> 00:24:00,160
of understanding. I mean, it's, it's, it's a question of definitions almost. There is a huge

259
00:24:00,160 --> 00:24:04,800
amount of commonality for sure. You take gradients, you try, you take gradients, we try to approximate

260
00:24:04,800 --> 00:24:08,560
gradients in both cases. In some case, in the case of reinforcement learning, you have

261
00:24:08,560 --> 00:24:13,760
some tools to reduce the variance of the gradients. You do that. There's lots of commonality,

262
00:24:13,760 --> 00:24:18,320
use the same neural net in both cases. You compute the gradient, you apply atom in both cases.

263
00:24:18,400 --> 00:24:27,520
So, I mean, there's lots in common for sure, but there are some small differences which are not

264
00:24:27,520 --> 00:24:31,360
completely insignificant. It's really just a matter of your, of your point of view, what

265
00:24:31,360 --> 00:24:37,120
frame of reference you, what, how much do you want to zoom in or out as you look at these problems?

266
00:24:37,120 --> 00:24:42,720
Which problem do you think is harder? So people like no Chomsky believe that language is fundamental

267
00:24:42,800 --> 00:24:49,200
to everything. So it underlies everything. Do you think language understanding is harder than

268
00:24:49,200 --> 00:24:54,640
visual scene understanding or vice versa? I think that asking if a problem is hard is

269
00:24:54,640 --> 00:24:58,480
slightly wrong. I think the question is a little bit wrong, and I want to explain why.

270
00:24:59,440 --> 00:25:02,160
So what does it mean for a problem to be hard?

271
00:25:04,240 --> 00:25:10,560
Okay, the non-interesting, dumb answer to that is there's a, there's a benchmark,

272
00:25:10,560 --> 00:25:16,720
and there's a human level performance on that benchmark. And how is the effort required to

273
00:25:16,720 --> 00:25:22,000
reach the human level benchmark? So from the perspective of how much until we get to human

274
00:25:22,000 --> 00:25:28,720
level on a very good benchmark? Yeah, like some, I understand what you mean by that.

275
00:25:28,720 --> 00:25:33,040
So what I was going to say that a lot of it depends on, you know, once you solve a problem,

276
00:25:33,040 --> 00:25:37,680
it stops being hard. And that's, that's always true. And so, but if something is hard or not,

277
00:25:37,680 --> 00:25:42,800
depends on what our tools can do today. So you know, you say today, through human level,

278
00:25:43,600 --> 00:25:49,040
language understanding and visual perception are hard in the sense that there is no way of

279
00:25:49,040 --> 00:25:53,040
solving the problem completely in the next three months. Right. So I agree with that statement.

280
00:25:53,840 --> 00:25:57,440
Beyond that, I'm just, I'll be my, my guess would be as good as yours. I don't know.

281
00:25:57,440 --> 00:26:02,720
Oh, okay. So you don't have a fundamental intuition about how hard language understanding is.

282
00:26:02,720 --> 00:26:07,040
I think I, I know I changed my mind. I'd say language is probably going to be harder. I mean,

283
00:26:07,040 --> 00:26:12,880
it depends on how you define it. Like if you mean absolute top notch 100% language understanding,

284
00:26:12,880 --> 00:26:19,200
I'll go with language. So, but then if I show you a piece of paper with letters on it, is that

285
00:26:19,920 --> 00:26:24,960
you see what I mean? So you have a vision system, you say it's the best human level vision system.

286
00:26:24,960 --> 00:26:30,560
I show you, I open a book and I show you letters. Will it understand how these letters form into

287
00:26:30,560 --> 00:26:34,560
word and sentences and meaning is this part of the vision problem? Where does vision end and

288
00:26:34,560 --> 00:26:39,280
language begin? Yeah. So Chomsky would say it starts at language. So vision is just a little

289
00:26:39,280 --> 00:26:46,880
example of the kind of structure and, you know, fundamental hierarchy of ideas that's already

290
00:26:46,880 --> 00:26:55,360
represented in our brain. Somehow that's represented through language. But where does vision stop and

291
00:26:55,360 --> 00:27:09,280
language begin? That's a really interesting question. So one possibility is that it's

292
00:27:09,280 --> 00:27:16,240
impossible to achieve really deep understanding in either images or language without basically

293
00:27:16,240 --> 00:27:22,000
using the same kind of system. So you're going to get the other for free. I think, I think it's

294
00:27:22,000 --> 00:27:26,560
pretty likely that yes, if we can get one, our machine learning is probably that good that we

295
00:27:26,560 --> 00:27:34,080
can get the other. But it's not 100. I'm not 100% sure. And also, I think a lot of it really does

296
00:27:34,080 --> 00:27:42,400
depend on your definitions. Definitions of like perfect vision. Because reading is vision, but

297
00:27:42,400 --> 00:27:49,520
should it count? Yeah, to me, my definition is if a system looked at an image and then a system

298
00:27:50,480 --> 00:27:57,040
looked at a piece of text, and then told me something about that. And I was really impressed.

299
00:27:58,240 --> 00:28:02,080
That's relative. You'll be impressed for half an hour. And then you're going to say, well,

300
00:28:02,080 --> 00:28:06,160
I mean, all the systems do that. But here's the thing they don't do. Yeah, but I don't have that

301
00:28:06,160 --> 00:28:13,200
with humans. Humans continue to impress me. Is that true? Well, the ones, okay, so I'm a fan of

302
00:28:13,200 --> 00:28:18,960
monogamy. So I like the idea of marrying somebody being with them for several decades. So I believe

303
00:28:18,960 --> 00:28:25,040
in the fact that, yes, it's possible to have somebody continuously giving you pleasurable,

304
00:28:25,760 --> 00:28:31,120
interesting, witty, new ideas, friends. Yeah, I think so. They continue to surprise you.

305
00:28:32,000 --> 00:28:44,480
The surprise, it's that injection of randomness seems to be a nice source of continued

306
00:28:45,120 --> 00:28:54,800
inspiration, like the wit, the humor. I think, yeah, that would be, it's a very subjective test,

307
00:28:54,800 --> 00:29:00,640
but I think if you have enough humans in the room. Yeah, I understand what you mean. Yeah,

308
00:29:00,640 --> 00:29:03,840
I feel like I misunderstood what you meant by impressing you. I thought you meant to

309
00:29:03,840 --> 00:29:10,800
impress you with its intelligence, with how valid understands an image. I thought you meant

310
00:29:10,800 --> 00:29:13,840
something like, I'm going to show you a really complicated image and it's going to get it right,

311
00:29:13,920 --> 00:29:19,440
and you're going to say, wow, that's really cool, the systems of January 2020 have not been doing

312
00:29:19,440 --> 00:29:26,000
that. Yeah, I think it all boils down to the reason people click like on stuff on the internet,

313
00:29:26,000 --> 00:29:34,320
which is like it makes them laugh. So it's like humor or wit or insight. I'm sure we'll get that

314
00:29:34,320 --> 00:29:41,920
as well. So forgive the romanticized question, but looking back to you, what is the most beautiful

315
00:29:41,920 --> 00:29:47,600
or surprising idea in deep learning or AI in general you've come across? So I think the most

316
00:29:47,600 --> 00:29:52,560
beautiful thing about deep learning is that it actually works. And I mean it because you got

317
00:29:52,560 --> 00:29:56,240
these ideas, you got a little neural network, you got the back propagation algorithm.

318
00:29:58,800 --> 00:30:02,480
And then you got some theories as to, you know, this is kind of like the brain. So maybe if you

319
00:30:02,480 --> 00:30:06,320
make it large, if you make the neural network large and you train a lot of data, then it will

320
00:30:07,840 --> 00:30:11,280
do the same function that the brain does. And it turns out to be true. That's crazy.

321
00:30:12,400 --> 00:30:15,920
And now we just train these neural networks and you make them larger and they keep getting better.

322
00:30:16,560 --> 00:30:21,280
And I find it unbelievable. I find it unbelievable that this whole AI stuff with neural networks

323
00:30:21,280 --> 00:30:27,840
works. Have you built up an intuition of why are there a little bits and pieces of intuitions

324
00:30:27,840 --> 00:30:33,760
of insights of why this whole thing works? I mean, some definitely. Well, we know that

325
00:30:33,840 --> 00:30:40,800
optimization, we now have good, you know, we've had lots of empirical, you know,

326
00:30:40,800 --> 00:30:44,960
huge amounts of empirical reasons to believe that optimization should work on all most

327
00:30:44,960 --> 00:30:50,720
problems we care about. Do you have insights of what, so you just said empirical evidence.

328
00:30:50,720 --> 00:31:00,320
Is most of your sort of empirical evidence kind of convinces you, it's like evolution is empirical,

329
00:31:00,320 --> 00:31:04,480
it shows you that look, this evolutionary process seems to be a good way to design

330
00:31:05,760 --> 00:31:11,760
organisms that survive in their environment. But it doesn't really get you to the insights of how

331
00:31:12,640 --> 00:31:17,520
the whole thing works. I think it's a good analogy is physics. You know how you say, hey,

332
00:31:17,520 --> 00:31:21,600
let's do some physics calculation and come up with some new physics theory and make some prediction.

333
00:31:21,600 --> 00:31:25,920
But then you got around the experiment. You know, you got around the experiment, it's important.

334
00:31:25,920 --> 00:31:30,080
So it's a bit the same here, except that maybe sometimes the experiment came before

335
00:31:30,080 --> 00:31:34,480
the theory. But it still is the case, you know, you have some data and you come up with some

336
00:31:34,480 --> 00:31:37,840
prediction, you say, yeah, let's make a big neural network, let's train it, and it's going to work

337
00:31:38,400 --> 00:31:41,840
much better than anything before it. And it will in fact continue to get better as you make it

338
00:31:41,840 --> 00:31:46,880
larger. And it turns out to be true. That's, that's amazing when a theory is validated like this,

339
00:31:46,880 --> 00:31:50,560
you know, it's not a mathematical theory, it's more of a biological theory almost.

340
00:31:51,600 --> 00:31:55,840
So I think there are not terrible analogies between deep learning and biology. I would say

341
00:31:55,840 --> 00:31:59,360
it's like the geometric mean of biology and physics, that's deep learning.

342
00:32:00,080 --> 00:32:05,360
The geometric mean of biology and physics. I think I'm going to need a few hours to wrap

343
00:32:05,360 --> 00:32:15,360
my head around that. Because just to find the geometric, just to find the set of what biology

344
00:32:15,360 --> 00:32:20,240
represents. Well, biology, in biology, things are really complicated. The theories are really,

345
00:32:20,240 --> 00:32:24,320
really, it's really hard to have good predictive theory. And if in physics, the theories are too

346
00:32:24,320 --> 00:32:28,560
good. In theory, in physics, people make these super precise theories, which make these amazing

347
00:32:28,560 --> 00:32:33,440
predictions. And in machine learning, we're kind of in between. Kind of in between. But it'd be

348
00:32:33,440 --> 00:32:38,400
nice if machine learning somehow helped us discover the unification of the two as opposed to server

349
00:32:38,400 --> 00:32:45,680
the in between. But you're right, that's, you're kind of trying to juggle both. So do you think

350
00:32:45,680 --> 00:32:49,440
there are still beautiful and mysterious properties in your networks that are yet to be

351
00:32:49,440 --> 00:32:54,000
discovered? Definitely. I think that we are still massively underestimating deep learning.

352
00:32:55,280 --> 00:33:01,360
What do you think it will look like? Like what if I knew I would have done it? So,

353
00:33:02,560 --> 00:33:06,240
but if you look at all the progress from the past 10 years, I would say most of it,

354
00:33:06,960 --> 00:33:12,080
I would say there have been a few cases where some were things that felt like really new ideas

355
00:33:12,080 --> 00:33:17,120
showed up. But by and large, it was every year, we thought, okay, deep learning goes this far.

356
00:33:17,120 --> 00:33:21,680
Nope, it actually goes further. And then the next year, okay, now you know, this is this is

357
00:33:21,680 --> 00:33:25,440
big deep learning, we are really done. Nope, it goes further, it just keeps going further each

358
00:33:25,440 --> 00:33:30,240
year. So that means that we keep underestimating, we keep not understanding it as surprising properties

359
00:33:30,240 --> 00:33:35,120
all the time. You think it's getting harder and harder to make progress, need to make progress?

360
00:33:35,840 --> 00:33:39,840
It depends on what we mean. I think the field will continue to make very robust progress

361
00:33:39,840 --> 00:33:44,320
for quite a while. I think for individual researchers, especially people who are doing

362
00:33:45,040 --> 00:33:49,120
research, it can be harder because there is a very large number of researchers right now.

363
00:33:50,000 --> 00:33:54,080
I think that if you have a lot of compute, then you can make a lot of very interesting

364
00:33:54,080 --> 00:34:00,160
discoveries, but then you have to deal with the challenge of managing a huge computer,

365
00:34:00,160 --> 00:34:03,200
a huge class, a huge computer cluster to run your experiments. It's a little bit harder.

366
00:34:03,200 --> 00:34:07,760
So I'm asking all these questions that nobody knows the answer to, but you're one of the smartest

367
00:34:07,760 --> 00:34:12,800
people I know, so I'm going to keep asking. So let's imagine all the breakthroughs that happen

368
00:34:12,800 --> 00:34:18,000
in the next 30 years in deep learning. Do you think most of those breakthroughs can be done by

369
00:34:18,000 --> 00:34:28,320
one person with one computer? In the space of breakthroughs, do you think compute and

370
00:34:28,320 --> 00:34:35,760
large efforts will be necessary? I mean, I can't be sure. When you say one computer, you mean how

371
00:34:35,760 --> 00:34:45,840
large? You're clever. I mean, one GPU. I see. I think it's pretty unlikely.

372
00:34:46,800 --> 00:34:53,040
I think it's pretty unlikely. I think that the stack of deep learning is starting to be quite

373
00:34:53,040 --> 00:35:01,440
deep. If you look at it, you've got all the way from the ideas, the systems to build the datasets,

374
00:35:02,000 --> 00:35:07,360
the distributed programming, the building the actual cluster, the GPU programming,

375
00:35:08,080 --> 00:35:13,120
putting it all together. So the stack is getting really deep, and I think it can be quite hard

376
00:35:13,120 --> 00:35:18,320
for a single person to become to be world-class in every single layer of the stack. What about

377
00:35:19,920 --> 00:35:25,840
Vladimir Vapnik really insists on is taking MNIST and trying to learn from very few examples,

378
00:35:25,840 --> 00:35:32,000
so being able to learn more efficiently. Do you think there'll be breakthroughs in that space

379
00:35:32,000 --> 00:35:38,320
that may not need a huge compute? I think there will be a large number of breakthroughs in general

380
00:35:38,320 --> 00:35:42,960
that will not need a huge amount of compute. So maybe I should clarify that. I think that

381
00:35:42,960 --> 00:35:48,240
some breakthroughs will require a lot of compute, and I think building systems which actually do

382
00:35:48,240 --> 00:35:53,440
things will require a huge amount of compute. That one is pretty obvious. If you want to do X,

383
00:35:53,440 --> 00:35:58,240
and X requires a huge neural net, you've got to get a huge neural net, but I think there will be

384
00:35:58,240 --> 00:36:03,760
lots of, I think there is lots of room for very important work being done by small groups and

385
00:36:03,760 --> 00:36:10,480
individuals. Can you maybe sort of on the topic of the science of deep learning, talk about one

386
00:36:10,480 --> 00:36:17,200
of the recent papers that you've released, the Deep Double Descent, where bigger models and more

387
00:36:17,200 --> 00:36:22,720
data hurt. I think it's a really interesting paper. Can you describe the main idea? Yeah,

388
00:36:22,720 --> 00:36:28,400
definitely. So what happened is that some over the years, some small number of researchers

389
00:36:28,400 --> 00:36:32,080
noticed that it is kind of weird that when you make the neural net work larger, it works better,

390
00:36:32,080 --> 00:36:35,760
and it seems to go in contradiction with statistical ideas. And then some people made

391
00:36:35,760 --> 00:36:40,080
an analysis showing that actually you got this double descent bump. And what we've done was to

392
00:36:40,080 --> 00:36:45,600
show that double descent occurs for pretty much all practical deep learning systems.

393
00:36:46,320 --> 00:36:54,480
And that it'll be also, so can you step back? What's the X axis and the Y axis of a double

394
00:36:54,480 --> 00:37:03,280
descent plot? Okay, great. So you can look, you can do things like you can take your neural

395
00:37:03,280 --> 00:37:08,720
network, and you can start increasing its size slowly, while keeping your data set fixed.

396
00:37:10,000 --> 00:37:16,800
So if you increase the size of the neural network slowly, and if you don't do early stopping,

397
00:37:16,800 --> 00:37:23,360
that's a pretty important detail. Then when the neural network is really small, you make it larger,

398
00:37:23,440 --> 00:37:27,520
you get a very rapid increase in performance. Then you continue to make it larger. And at some

399
00:37:27,520 --> 00:37:34,000
point performance, you'll get worse. And it gets and it gets the worst exactly at the point at

400
00:37:34,000 --> 00:37:39,280
which it achieves zero training error, precisely zero training loss. And then as you make it

401
00:37:39,280 --> 00:37:43,600
larger, it starts to get better again. And it's kind of counterintuitive because you'd expect

402
00:37:43,600 --> 00:37:51,280
deep learning phenomena to be monotonic. And it's hard to be sure what it means, but it also occurs

403
00:37:51,360 --> 00:37:55,360
in the case of linear classifiers. And the intuition basically boils down to the following.

404
00:37:56,960 --> 00:38:04,000
When you, when you have a lot, when you have a large data set, and a small model, then small,

405
00:38:04,000 --> 00:38:11,440
tiny random. So basically, what is overfitting? Overfitting is when your model is somehow very

406
00:38:11,440 --> 00:38:17,200
sensitive to the small random, unimportant stuff in your data set in the training data in the

407
00:38:17,200 --> 00:38:22,400
training data set precisely. So if you have a small model, and you have a big data set,

408
00:38:23,280 --> 00:38:27,360
and there may be some random thing, you know, some training cases are randomly in the data set,

409
00:38:27,360 --> 00:38:31,680
and others may not be there. But the small model, but the small model is kind of insensitive to

410
00:38:31,680 --> 00:38:36,960
this randomness, because it's the same, you there is pretty much no uncertainty about the model

411
00:38:36,960 --> 00:38:42,400
when the data set is large. So okay, so at the very basic level, to me, it is the most surprising

412
00:38:42,400 --> 00:38:53,280
thing that neural networks don't overfit every time very quickly. Before ever being able to learn

413
00:38:53,280 --> 00:38:59,280
anything, the huge number of parameters. So here is so there is one way Okay, so maybe so let me try

414
00:38:59,280 --> 00:39:03,520
to give the explanation and maybe that will be that will work. So you got a huge neural network,

415
00:39:03,520 --> 00:39:08,640
let's suppose you got a your you have a huge neural network, you have a huge number of parameters.

416
00:39:09,600 --> 00:39:13,840
And now let's pretend everything is linear, which is not let's just pretend. Then there is this big

417
00:39:13,840 --> 00:39:21,040
subspace, where a neural network achieves zero error. And SGT is going to find approximately

418
00:39:21,040 --> 00:39:25,120
that's right, approximately the point with the smallest norm in that subspace.

419
00:39:27,040 --> 00:39:33,600
And that can also be proven to be insensitive to the small randomness in the data, when the

420
00:39:33,600 --> 00:39:38,480
dimensionality is high. But when the dimensionality of the data is equal to the dimensionality of

421
00:39:38,480 --> 00:39:44,400
the model, then there is a one to one correspondence between all the data sets and the models. So

422
00:39:44,400 --> 00:39:47,600
small changes in the data set actually lead to large changes in the model and that's why

423
00:39:47,600 --> 00:39:52,880
performance gets worse. So this is the best explanation more or less. So then it would be

424
00:39:52,880 --> 00:39:58,960
good for the model to have more parameters sort of to be bigger than the data. That's right,

425
00:39:58,960 --> 00:40:02,720
but only if you don't early stop. If you introduce early stop in your regularization,

426
00:40:02,720 --> 00:40:07,280
you can make a double as a descent pump almost completely disappear. What is early stop early

427
00:40:07,280 --> 00:40:12,480
stopping is when you train your model, and you monitor your validation performance.

428
00:40:13,440 --> 00:40:16,560
And then if at some point validation performance starts to get worse, you say, okay, let's stop

429
00:40:16,560 --> 00:40:23,040
training. We are good. We are good. We are good enough. So the magic happens after that moment.

430
00:40:23,040 --> 00:40:26,560
So you don't want to do the early stopping. Well, if you don't do the early stopping,

431
00:40:26,560 --> 00:40:31,920
you get this very, you get the very pronounced double descent. Do you have any intuition why

432
00:40:31,920 --> 00:40:36,960
this happens? Double descent or sorry, are you stopping? No, the double descent. So the

433
00:40:36,960 --> 00:40:43,680
well, yeah, so I try it. Let's see the intuition is basically is this that when the data set has

434
00:40:43,680 --> 00:40:49,680
as many degrees of freedom as the model, then there is a one to one correspondence between them.

435
00:40:49,680 --> 00:40:55,520
And so small changes to the data set lead to noticeable changes in the model. So your model

436
00:40:55,520 --> 00:41:01,360
is very sensitive to all the randomness. It is unable to discard it. Whereas it turns out that

437
00:41:01,360 --> 00:41:07,120
when you have a lot more data than parameters, or a lot more parameters than data, the resulting

438
00:41:07,120 --> 00:41:14,160
solution will be insensitive to small changes in the data set. So it's able to nicely put discard

439
00:41:14,160 --> 00:41:19,840
the small changes, the randomness. Exactly. The spurious correlations which you don't want.

440
00:41:20,480 --> 00:41:24,480
Jeff Hinton suggested we need to throw back propagation. We already kind of talked about

441
00:41:24,480 --> 00:41:28,880
this a little bit, but he suggested we need to throw away back propagation and start over.

442
00:41:29,680 --> 00:41:31,920
I mean, of course, some of that is a little bit

443
00:41:33,760 --> 00:41:38,640
wit and humor. But what do you think? What could be an alternative method of training neural

444
00:41:38,640 --> 00:41:43,520
networks? Well, the thing that he said precisely is that to the extent that you can't find back

445
00:41:43,520 --> 00:41:48,960
propagation in the brain, it's worth seeing if we can learn something from how the brain learns.

446
00:41:48,960 --> 00:41:53,440
But back propagation is very useful and we should keep using it. Oh, you're saying that

447
00:41:53,440 --> 00:41:58,000
once we discover the mechanism of learning in the brain or any aspects of that mechanism,

448
00:41:58,000 --> 00:42:02,160
we should also try to implement that in your networks. If it turns out that you can't find

449
00:42:02,160 --> 00:42:05,920
back propagation in the brain. If we can't find back propagation in the brain.

450
00:42:08,000 --> 00:42:14,960
Well, so I guess your answer to that is back propagation is pretty damn useful. So why are

451
00:42:14,960 --> 00:42:18,960
we complaining? I mean, I personally am a big fan of back propagation. I think it's a great

452
00:42:18,960 --> 00:42:25,280
algorithm because it solves an extremely fundamental problem which is finding a neural

453
00:42:25,280 --> 00:42:31,200
circuit subject to some constraints. I don't see that problem going away. So that's why I

454
00:42:32,560 --> 00:42:36,480
really, I think it's pretty unlikely that we'll have anything which is going to be

455
00:42:37,280 --> 00:42:41,200
dramatically different. It could happen. But I wouldn't bet on it right now.

456
00:42:43,200 --> 00:42:51,040
So let me ask a sort of big picture question. Do you think neural networks can be made to reason?

457
00:42:51,600 --> 00:42:56,560
Why not? Well, if you look, for example, at AlphaGo or AlphaZero,

458
00:42:58,160 --> 00:43:04,320
the neural network of AlphaZero plays Go, which we all agree is a game that requires reasoning,

459
00:43:05,120 --> 00:43:11,040
better than 99.9% of all humans. Just the neural network without the search, just the neural network

460
00:43:11,040 --> 00:43:16,320
itself. Doesn't that give us an existence proof that neural networks can reason?

461
00:43:16,560 --> 00:43:21,920
To push back and disagree a little bit, we all agree that Go is reasoning.

462
00:43:23,200 --> 00:43:28,640
I think I agree. I don't think it's a trivial. So obviously, reasoning like intelligence is

463
00:43:29,920 --> 00:43:36,640
a loose gray area term a little bit. Maybe you disagree with that. But yes, I think it has some

464
00:43:36,640 --> 00:43:44,560
of the same elements of reasoning. Reasoning is almost akin to search. There's a sequential element

465
00:43:45,200 --> 00:43:54,160
of stepwise consideration of possibilities and sort of building on top of those possibilities

466
00:43:54,160 --> 00:43:59,760
in a sequential manner until you arrive at some insight. So yeah, I guess playing Go is kind of

467
00:43:59,760 --> 00:44:04,640
like that. And when you have a single neural network doing that without search, that's kind of like

468
00:44:04,640 --> 00:44:10,080
that. So there's an existence proof in a particular constrained environment that a process akin to

469
00:44:10,880 --> 00:44:18,720
what many people call reasoning exists. But more general kind of reasoning. So off the board.

470
00:44:18,720 --> 00:44:22,720
There is one other existence proof. Oh boy, which one? Us humans?

471
00:44:22,720 --> 00:44:32,240
Yes. Okay. All right. So do you think the architecture that will allow neural

472
00:44:32,240 --> 00:44:37,920
networks to reason will look similar to the neural network architectures we have today?

473
00:44:38,720 --> 00:44:43,360
I think it will. I think, well, I don't want to make two overly definitive statements.

474
00:44:43,920 --> 00:44:49,440
I think it's definitely possible that the neural networks that will produce the reasoning breakthroughs

475
00:44:49,440 --> 00:44:54,480
of the future will be very similar to the architecture that exists today, maybe a little

476
00:44:54,480 --> 00:45:02,800
bit more recurrent, maybe a little bit deeper. But these neural nets are so insanely powerful.

477
00:45:02,880 --> 00:45:08,400
Why wouldn't they be able to learn to reason? Humans can reason. So why can't neural networks?

478
00:45:09,200 --> 00:45:14,480
So do you think the kind of stuff we've seen neural networks do is a kind of just weak reasoning?

479
00:45:14,480 --> 00:45:18,400
So it's not a fundamentally different process. Again, this is stuff we don't nobody knows the

480
00:45:18,400 --> 00:45:24,240
answer to. So when it comes to our neural networks, I would think which I would say

481
00:45:24,240 --> 00:45:30,720
is that neural networks are capable of reasoning. But if you train a neural network on a task which

482
00:45:30,720 --> 00:45:35,520
doesn't require reasoning, it's not going to reason. This is a well-known effect where the

483
00:45:35,520 --> 00:45:43,040
neural network will solve exactly the problem that you pose in front of it in the easiest way possible.

484
00:45:44,400 --> 00:45:53,120
Right. That takes us to one of the brilliant ways you describe neural networks, which is

485
00:45:54,080 --> 00:45:57,040
you've referred to neural networks as the search for small circuits

486
00:45:57,920 --> 00:46:03,040
and maybe general intelligence as the search for small programs,

487
00:46:04,400 --> 00:46:08,640
which I found is a metaphor very compelling. Can you elaborate on that difference?

488
00:46:09,200 --> 00:46:17,520
Yeah. So the thing which I said precisely was that if you can find the shortest program that

489
00:46:17,520 --> 00:46:23,520
outputs the data in your disposal, then you will be able to use it to make the best prediction

490
00:46:23,520 --> 00:46:29,680
possible. And that's a theoretical statement which can be proved mathematically. Now,

491
00:46:29,680 --> 00:46:34,800
you can also prove mathematically that it is that finding the shortest program which generates

492
00:46:34,800 --> 00:46:41,600
some data is not a computable operation. No finite amount of compute can do this.

493
00:46:42,560 --> 00:46:48,720
So then with neural networks, neural networks are the next best thing that actually works in

494
00:46:48,720 --> 00:46:54,560
practice. We are not able to find the best, the shortest program which generates our data,

495
00:46:55,600 --> 00:47:00,880
but we are able to find a small, but now that statement should be amended,

496
00:47:01,440 --> 00:47:04,480
even a large circuit which fits our data in some way.

497
00:47:05,120 --> 00:47:09,840
Well, I think what you meant by the small circuit is the smallest needed circuit.

498
00:47:09,920 --> 00:47:14,640
Well, the thing which I would change now, back then I really haven't fully internalized

499
00:47:14,640 --> 00:47:19,600
the overparameterized results. The things we know about overparameterized neural

500
00:47:19,600 --> 00:47:26,640
nets, now I would phrase it as a large circuit whose weights contain a small amount of information,

501
00:47:27,600 --> 00:47:31,680
which I think is what's going on. If you imagine the training process of a neural network as you

502
00:47:31,680 --> 00:47:39,680
slowly transmit entropy from the data set to the parameters, then somehow the amount of information

503
00:47:39,680 --> 00:47:44,560
in the weights ends up being not very large, which would explain whether generalized so well.

504
00:47:45,200 --> 00:47:51,840
So that's the large circuit might be one that's helpful for the generalization.

505
00:47:51,840 --> 00:47:52,640
Yeah, something like this.

506
00:47:54,720 --> 00:48:02,320
But do you see it important to be able to try to learn something like programs?

507
00:48:02,320 --> 00:48:08,960
I mean, if we can, definitely. I think the answer is kind of yes, if we can do it.

508
00:48:08,960 --> 00:48:13,760
We should do things that we can do it. It's the reason we are pushing on deep learning.

509
00:48:15,200 --> 00:48:20,000
The fundamental reason, the root cause is that we are able to train them.

510
00:48:21,360 --> 00:48:26,720
So in other words, training comes first. We've got our pillar, which is the training pillar.

511
00:48:27,440 --> 00:48:31,040
And now we are trying to contort our neural networks around the training pillar. We got

512
00:48:31,040 --> 00:48:36,800
to stay trainable. This is an invariant we cannot violate. And so

513
00:48:37,040 --> 00:48:41,520
being trainable means starting from scratch, knowing nothing, you can actually pretty quickly

514
00:48:41,520 --> 00:48:47,520
converge towards knowing a lot or even slowly. But it means that given the resources at your

515
00:48:47,520 --> 00:48:54,240
disposal, you can train the neural net and get it to achieve useful performance.

516
00:48:54,240 --> 00:48:57,920
Yeah, that's a pillar we can't move away from. That's right. Because if you can, whereas if you

517
00:48:57,920 --> 00:49:03,040
say, Hey, let's find the shortest program, we can't do that. So it doesn't matter how useful

518
00:49:04,000 --> 00:49:07,600
that would be. We can do it. So we want.

519
00:49:07,600 --> 00:49:11,520
So do you think you kind of mentioned that the neural networks are good at finding small

520
00:49:11,520 --> 00:49:18,560
circuits or large circuits? Do you think then the matter of finding small programs is just the data?

521
00:49:18,560 --> 00:49:19,040
No.

522
00:49:19,040 --> 00:49:28,240
So the, sorry, not the size or the quality, the type of data sort of ask giving it programs.

523
00:49:28,880 --> 00:49:34,880
Well, I think the thing is that right now, finding there are no good precedents of people

524
00:49:34,880 --> 00:49:41,920
successfully finding programs really well. And so the way you'd find programs is you'd train a

525
00:49:41,920 --> 00:49:47,440
deep neural network to do it basically. But which is the right way to go about it.

526
00:49:48,000 --> 00:49:54,000
But there's not good illustrations that hasn't been done yet. But in principle, it should be possible.

527
00:49:54,080 --> 00:50:01,200
Can you elaborate a little bit? What's your insight in principle? And put another way,

528
00:50:01,200 --> 00:50:07,520
you don't see why it's not possible. Well, it's kind of like more, it's more a statement of,

529
00:50:09,360 --> 00:50:13,440
I think that it's, I think that it's unwise to bet against deep learning. And

530
00:50:14,960 --> 00:50:18,880
if it's a fun, if it's a cognitive function that humans seem to be able to do, then

531
00:50:19,200 --> 00:50:24,320
it doesn't take too long for some deep neural net to pop up that can do it too.

532
00:50:25,680 --> 00:50:32,880
Yeah, I'm there with you. I can, I've stopped betting against neural networks at this point

533
00:50:32,880 --> 00:50:38,320
because they continue to surprise us. What about long term memory? Can neural networks have long

534
00:50:38,320 --> 00:50:45,360
term memory or something like knowledge basis? So being able to aggregate important information

535
00:50:45,440 --> 00:50:53,200
over long periods of time, that would then serve as useful sort of representations of

536
00:50:54,000 --> 00:51:00,480
state that you can make decisions by so have a long term context based on what you make in the

537
00:51:00,480 --> 00:51:07,600
decision. So in some sense, the parameters already do that. The parameters are an aggregation of the

538
00:51:07,600 --> 00:51:12,080
day of the neural of the entirety of the neural net experience. And so they count as the long

539
00:51:12,560 --> 00:51:19,040
form, long term knowledge. And people have trained various neural nets to act as knowledge basis

540
00:51:19,040 --> 00:51:23,200
and, you know, investigated with invest, people have investigated language models as knowledge

541
00:51:23,200 --> 00:51:29,760
basis. So there is work, there is work there. Yeah, but in some sense, do you think in every sense,

542
00:51:29,760 --> 00:51:37,200
do you think there's a, it's all just a matter of coming up with a better mechanism of forgetting

543
00:51:37,200 --> 00:51:42,400
the useless stuff and remembering the useful stuff? Because right now, I mean, there's not been

544
00:51:42,400 --> 00:51:47,920
mechanisms that do remember really long term information. What do you mean by that precisely?

545
00:51:48,880 --> 00:51:52,000
Precisely. I like the word precisely. So

546
00:51:54,720 --> 00:51:59,840
I'm thinking of the kind of compression of information the knowledge basis represent.

547
00:52:00,400 --> 00:52:07,120
Sort of creating a, now I apologize for my sort of human centric thinking about

548
00:52:07,680 --> 00:52:13,440
what knowledge is because neural networks aren't interpretable necessarily with the kind of

549
00:52:13,440 --> 00:52:19,680
knowledge they have discovered. But a good example for me is knowledge basis being able to build up

550
00:52:19,680 --> 00:52:25,840
over time something like the knowledge that Wikipedia represents. It's a really compressed,

551
00:52:25,840 --> 00:52:33,600
structured knowledge base. Obviously, not the actual Wikipedia or the language,

552
00:52:34,160 --> 00:52:39,200
but like a semantic web, the dream that semantic web represented. So it's a really nice compressed

553
00:52:39,200 --> 00:52:46,320
knowledge base or something akin to that in the noninterpretable sense as neural networks would

554
00:52:46,320 --> 00:52:50,080
have. Well, the neural networks would be noninterpretable if you look at their rates, but

555
00:52:50,080 --> 00:52:53,680
their outputs should be very interpretable. Okay, so yeah, how do you, how do you make

556
00:52:54,480 --> 00:52:57,360
very smart neural networks like language models interpretable?

557
00:52:58,000 --> 00:53:02,000
Well, you ask them to generate some text and the text will generally be interpretable.

558
00:53:02,000 --> 00:53:07,840
Do you find that the epitome of interpretability, like can you do better? Like can you, because

559
00:53:07,840 --> 00:53:13,440
you can't, okay, I'd like to know what does it know and what doesn't know. I would like the neural

560
00:53:13,440 --> 00:53:19,200
network to come up with examples where it's completely dumb and examples where it's completely

561
00:53:19,200 --> 00:53:24,960
brilliant. And the only way I know how to do that now is to generate a lot of examples and use my

562
00:53:24,960 --> 00:53:31,040
human judgment. But it would be nice if the neural network had some self-awareness about

563
00:53:31,600 --> 00:53:36,880
100%. I'm a big believer in self-awareness. And I think that I think, I think

564
00:53:38,720 --> 00:53:43,600
neural net self-awareness will allow for things like the capabilities, like the ones you describe,

565
00:53:43,600 --> 00:53:48,240
like for them to know what they know and what they don't know. And for them to know where to

566
00:53:48,240 --> 00:53:52,160
invest, to increase their skills most optimally. And to your question of interpretability,

567
00:53:52,160 --> 00:53:56,320
there are actually two answers to that question. One answer is, you know, we have the neural net,

568
00:53:56,320 --> 00:54:00,560
so we can analyze the neurons and we can try to understand what the different neurons and

569
00:54:00,560 --> 00:54:04,960
different layers mean. And you can actually do that. And OpenAI has done some work on that.

570
00:54:05,760 --> 00:54:11,360
But there is a different answer, which is that I would say that's the human centric answer where

571
00:54:11,360 --> 00:54:18,080
you say, you know, you look at a human being, you can't read. How do you know what a human

572
00:54:18,080 --> 00:54:20,800
being is thinking? You ask them, you say, Hey, what do you think about this? What do you think

573
00:54:20,800 --> 00:54:26,320
about that? And you get some answers. The answers you get are sticky. In the sense, you already

574
00:54:26,320 --> 00:54:33,360
have a mental model. You already have a mental model of that human being. You already have an

575
00:54:33,360 --> 00:54:40,240
understanding of like a big conception of what it of that human being, how they think, what they know,

576
00:54:40,240 --> 00:54:46,880
how they see the world, and then everything you ask, you're adding on to that. And that stickiness

577
00:54:47,520 --> 00:54:53,600
seems to be, that's one of the really interesting qualities of the human being is that information

578
00:54:53,600 --> 00:54:59,520
is sticky. You don't, you seem to remember the useful stuff, aggregate it well, and forget most

579
00:54:59,520 --> 00:55:05,040
of the information that's not useful, that process. But that's also pretty similar to the

580
00:55:05,040 --> 00:55:09,600
process that neural networks do is just that neural networks are much crappier at this time.

581
00:55:10,480 --> 00:55:15,600
It doesn't seem to be fundamentally that different. But just stick on reasoning for a little longer.

582
00:55:17,120 --> 00:55:24,560
He said, why not? Why can't I reason? What's a good impressive feat benchmark to you of reasoning

583
00:55:27,360 --> 00:55:31,600
that you'll be impressed by if neural networks were able to do? Is that something you already have

584
00:55:31,600 --> 00:55:38,640
in mind? Well, I think writing, writing really good code. I think proving really hard theorems,

585
00:55:39,360 --> 00:55:42,800
solving open ended problems without of the box solutions.

586
00:55:45,840 --> 00:55:51,600
And sort of theorem type mathematical problems. Yeah, I think those ones are a very natural

587
00:55:51,600 --> 00:55:55,680
example as well. You know, if you can prove an unproven theorem, then it's hard to argue

588
00:55:55,680 --> 00:56:00,880
don't reason. And so by the way, and this comes back to the point about the hard results, you know,

589
00:56:00,880 --> 00:56:06,000
if you got a hard, if you have machine learning, deep learning as a field is very fortunate,

590
00:56:06,000 --> 00:56:12,160
because we have the ability to sometimes produce these unambiguous results. And when they happen,

591
00:56:12,160 --> 00:56:17,040
the debate changes, the conversation changes, it's a converse, you have the ability to produce

592
00:56:17,040 --> 00:56:22,080
conversation changing results conversation. And then of course, just like you said, people kind

593
00:56:22,080 --> 00:56:26,240
of take that for granted, say that wasn't actually a hard problem. Well, I mean, at some point,

594
00:56:26,240 --> 00:56:32,880
we'll probably run out of hard problems. Yeah, that whole mortality thing is kind of kind of a

595
00:56:32,880 --> 00:56:38,320
sticky problem that we haven't quite figured out. Maybe we'll solve that one. I think one of the

596
00:56:38,320 --> 00:56:42,880
fascinating things in your entire body of work, but also the work at OpenAI recently,

597
00:56:42,880 --> 00:56:48,480
one of the conversation changers has been in the world of language models. Can you briefly kind of

598
00:56:49,280 --> 00:56:53,760
try to describe the recent history of using neural networks in the domain of language and text?

599
00:56:54,480 --> 00:57:00,160
Well, there's been lots of history. I think I think the Elman network was was a small,

600
00:57:00,240 --> 00:57:05,200
tiny recurrent neural network applied to language back in the 80s. So the history is really,

601
00:57:06,400 --> 00:57:13,280
you know, fairly long, at least. And the thing that started the thing that changed the trajectory

602
00:57:13,280 --> 00:57:17,920
of neural networks and language is the thing that changed the trajectory of all deep learning and

603
00:57:17,920 --> 00:57:23,680
that's data and compute. So suddenly you move from small language models, which learn a little bit.

604
00:57:24,240 --> 00:57:28,960
And with language models, in particular, you can, there's a very clear explanation for why

605
00:57:29,040 --> 00:57:33,200
they need to be large to be good, because they're trying to predict the next word.

606
00:57:34,480 --> 00:57:41,440
So when you don't know anything, you'll notice very, very broad strokes, surface level patterns,

607
00:57:41,440 --> 00:57:46,400
like sometimes there are characters and there is space between those characters,

608
00:57:46,400 --> 00:57:50,560
you'll notice this pattern. And you'll notice that sometimes there is a comma and then the

609
00:57:50,560 --> 00:57:54,880
next character is a capital letter, you'll notice that pattern. Eventually, you may start to notice

610
00:57:54,880 --> 00:57:59,280
that there are certain words occur often, you may notice that spellings are a thing,

611
00:57:59,280 --> 00:58:04,960
you may notice syntax. And when you get really good at all these, you start to notice the semantics,

612
00:58:05,760 --> 00:58:10,480
you start to notice the facts. But for that to happen, the language model needs to be larger.

613
00:58:11,360 --> 00:58:16,320
So that's, let's linger on that, because that's where you and Noam Chomsky disagree.

614
00:58:16,320 --> 00:58:25,600
So you think we're actually taking incremental steps, a sort of larger network, larger compute

615
00:58:25,600 --> 00:58:34,640
will be able to get to the semantics, be able to understand language without what Noam likes to

616
00:58:34,640 --> 00:58:42,000
sort of think of as a fundamental understandings of the structure of language, like imposing

617
00:58:42,080 --> 00:58:47,920
your theory of language onto the learning mechanism. So you're saying the learning,

618
00:58:47,920 --> 00:58:52,400
you can learn from raw data, the mechanism that underlies language?

619
00:58:53,360 --> 00:58:58,480
Well, I think it's pretty likely. But I also want to say that I don't really

620
00:59:00,400 --> 00:59:07,280
know precisely what Chomsky means when he talks about him. You said something about imposing

621
00:59:07,280 --> 00:59:12,800
your structure on language. I'm not 100% sure what he means. But empirically, it seems that when

622
00:59:12,800 --> 00:59:16,560
you inspect those larger language models, they exhibit signs of understanding the semantics,

623
00:59:16,560 --> 00:59:20,400
whereas the smaller language models do not. We've seen that a few years ago when we

624
00:59:20,400 --> 00:59:26,000
did work on the sentiment neuron, we trained a small, you know, smallish LSTM to predict the

625
00:59:26,000 --> 00:59:31,920
next character in Amazon reviews. And we noticed that when you increase the size of the LSTM from

626
00:59:31,920 --> 00:59:37,760
500 LSTM cells to 4,000 LSTM cells, then one of the neurons starts to represent the sentiment

627
00:59:38,480 --> 00:59:44,720
of the article, of sorry, of their view. Now, why is that sentiment is a pretty semantic

628
00:59:44,720 --> 00:59:48,640
attribute? It's not a syntactic attribute. And for people who might not know, I don't know if

629
00:59:48,640 --> 00:59:52,400
that's a standard term, but sentiment is whether it's a positive or negative review. That's right.

630
00:59:52,400 --> 00:59:57,680
Like, is the person happy with something or is the person unhappy with something? And so here we had

631
00:59:57,680 --> 01:00:02,800
very clear evidence that a small neural net does not capture sentiment while a large neural net

632
01:00:02,800 --> 01:00:08,720
does. And why is that? Well, our theory is that at some point, you run out of syntax to models,

633
01:00:08,720 --> 01:00:15,760
you start to gotta focus on something else. And with size, you quickly run out of syntax to model,

634
01:00:15,760 --> 01:00:20,560
and then you really start to focus on the semantics is would be the idea. That's right. And so I don't

635
01:00:20,560 --> 01:00:24,720
want to imply that our models have complete semantic understanding, because that's not true.

636
01:00:25,280 --> 01:00:30,720
But they definitely are showing signs of semantic understanding, partial semantic understanding,

637
01:00:30,720 --> 01:00:38,000
but the smaller models do not show that those signs. Can you take a step back and say, what is GPT2,

638
01:00:38,000 --> 01:00:43,040
which is one of the big language models that was the conversation changer in the past couple of

639
01:00:43,040 --> 01:00:51,360
years? Yes. So GPT2 is a transformer with one and a half billion parameters that was trained on

640
01:00:51,440 --> 01:01:00,080
up on about 40 billion tokens of text, which were obtained from web pages that were linked to

641
01:01:00,080 --> 01:01:04,720
from Reddit articles with more than three uploads. And what's the transformer? The transformer,

642
01:01:04,720 --> 01:01:08,960
it's the most important advance in neural network architectures in recent history.

643
01:01:09,680 --> 01:01:14,080
What is attention maybe to because I think that's an interesting idea, not necessarily sort of

644
01:01:14,080 --> 01:01:20,480
technically speaking, but the idea of attention versus maybe what recurrent neural networks

645
01:01:20,480 --> 01:01:25,760
represent. Yeah. So the thing is, the transformer is a combination of multiple ideas simultaneously

646
01:01:25,760 --> 01:01:31,760
of which attention is one. Do you think attention is the key? No, it's a key, but it's not the key.

647
01:01:32,320 --> 01:01:37,680
The transformer is successful because it is the simultaneous combination of multiple ideas. And

648
01:01:37,680 --> 01:01:43,040
if you were to remove either idea, it would be much less successful. So the transformer uses a

649
01:01:43,040 --> 01:01:47,280
lot of attention, but attention exists for a few years. So that can't be the main innovation.

650
01:01:48,240 --> 01:01:54,880
The transformer is designed in such a way that it runs really fast on the GPU.

651
01:01:56,000 --> 01:02:00,240
And that makes a huge amount of difference. This is one thing. The second thing is that

652
01:02:00,240 --> 01:02:06,320
transformer is not recurrent. And that is really important too, because it is more shallow and

653
01:02:06,320 --> 01:02:12,400
therefore much easier to optimize. So in other words, uses attention. It is, it is a really great

654
01:02:12,480 --> 01:02:17,760
fit to the GPU. And it is not recurrent. So therefore, less deep and easier to optimize.

655
01:02:17,760 --> 01:02:22,720
And the combination of those factors make it successful. So now it makes it makes great

656
01:02:22,720 --> 01:02:27,440
use of your GPU. It allows you to achieve better results for the same amount of compute.

657
01:02:28,560 --> 01:02:33,520
And that's why it's successful. Were you surprised how well transformers worked?

658
01:02:34,160 --> 01:02:41,280
And GPT2 worked? So you worked on language. You've had a lot of great ideas before transformers came

659
01:02:41,280 --> 01:02:46,320
about in language. So you got to see the whole set of revolutions before and after. Were you

660
01:02:46,320 --> 01:02:52,240
surprised? Yeah, a little. A little? Yeah. I mean, it's hard. It's hard to remember because

661
01:02:52,800 --> 01:02:57,680
you adapt really quickly. But it definitely was surprising. It definitely was. In fact, I'll,

662
01:02:57,680 --> 01:03:03,360
you know what, I'll, I'll retract my statement. It was, it was pretty amazing. It was just amazing

663
01:03:03,360 --> 01:03:08,160
to see, generate this text of this. And you know, you got to keep in mind that we've seen,

664
01:03:08,160 --> 01:03:12,480
at that time, we've seen all this progress in GANs, in improving the, you know, the samples

665
01:03:12,480 --> 01:03:17,120
produced by GANs were just amazing. You have these realistic faces, but text hasn't really moved that

666
01:03:17,120 --> 01:03:24,080
much. And suddenly we moved from, you know, whatever GANs were in 2015, to the best, most

667
01:03:24,080 --> 01:03:29,120
amazing GANs in one step. And I was really stunning. Even though theory predicted, yeah,

668
01:03:29,120 --> 01:03:32,800
you train a big language model, of course, you should get this, but then to see it with your own

669
01:03:32,800 --> 01:03:39,840
eyes. It's something else. And yet we adapt really quickly. And now there's a sort of

670
01:03:41,520 --> 01:03:49,200
some cognitive scientist, right? Articles saying that GPT2 models don't truly understand language.

671
01:03:49,200 --> 01:03:54,880
So we adapt quickly to how amazing the fact that they're able to model the language so well is.

672
01:03:55,600 --> 01:04:01,440
So what do you think is the bar? For what? For impressing us that it.

673
01:04:02,320 --> 01:04:06,080
I don't know. Do you think that bar will continuously be moved?

674
01:04:06,080 --> 01:04:12,160
Definitely. I think when you start to see really dramatic economic impact, that's when I think

675
01:04:12,160 --> 01:04:16,800
that's in some sense the next barrier. Because right now, if you think about the work in AI,

676
01:04:16,800 --> 01:04:21,680
it's really confusing. It's really hard to know what to make of all these advances.

677
01:04:22,400 --> 01:04:27,440
It's kind of like, okay, you got an advance and now you can do more things and you got another

678
01:04:28,160 --> 01:04:35,360
improvement and you got another cool demo. At some point, I think people who are outside of AI,

679
01:04:36,160 --> 01:04:38,640
they can no longer distinguish this progress anymore.

680
01:04:38,640 --> 01:04:42,160
So we were talking offline about translating Russian to English and

681
01:04:42,160 --> 01:04:46,400
how there's a lot of brilliant work in Russian that the rest of the world doesn't know about.

682
01:04:46,400 --> 01:04:52,080
That's true for Chinese. That's true for a lot of scientists and just artistic work in general.

683
01:04:52,080 --> 01:04:56,960
Do you think translation is the place where we're going to see sort of economic big impact?

684
01:04:57,280 --> 01:05:02,240
I don't know. I think there is a huge number of applications. First of all, I want to point out

685
01:05:02,240 --> 01:05:07,600
that translation already today is huge. I think billions of people interact with

686
01:05:08,800 --> 01:05:12,960
big chunks of the internet primarily through translation. So translation is already huge

687
01:05:12,960 --> 01:05:19,760
and it's hugely positive too. I think self-driving is going to be hugely impactful.

688
01:05:20,080 --> 01:05:26,880
And it's unknown exactly when it happens, but again, I would not bet against deep learning.

689
01:05:27,840 --> 01:05:31,200
So there's deep learning in general, but you think deep learning for self-driving?

690
01:05:31,760 --> 01:05:35,040
Yes, deep learning for self-driving. But I was talking about sort of language models.

691
01:05:36,880 --> 01:05:38,000
Be your duff a little bit.

692
01:05:38,000 --> 01:05:41,040
Just to check. You're not seeing a connection between driving and language.

693
01:05:41,040 --> 01:05:44,080
No, no. Okay. I'd rather both use neural nets.

694
01:05:44,080 --> 01:05:47,600
That would be a poetic connection. I think there might be some, like you said,

695
01:05:47,600 --> 01:05:54,320
there might be some kind of unification towards a kind of multitask transformers

696
01:05:54,320 --> 01:06:00,720
that can take on both language and vision tasks. That'd be an interesting unification.

697
01:06:01,440 --> 01:06:03,600
Let's see. What can I ask about GPT two more?

698
01:06:04,880 --> 01:06:09,840
It's simple. It's not much to ask. You take a transform, you make it bigger,

699
01:06:09,840 --> 01:06:12,560
give it more data, and suddenly it does all those amazing things.

700
01:06:12,560 --> 01:06:16,800
Yeah. One of the beautiful things is that GPT, the transformers are fundamentally

701
01:06:16,800 --> 01:06:25,920
simple to explain, to train. Do you think bigger will continue to show better results in language?

702
01:06:26,960 --> 01:06:27,440
Probably.

703
01:06:28,240 --> 01:06:31,360
Sort of like what are the next steps with GPT two? Do you think?

704
01:06:31,360 --> 01:06:38,000
I mean, I think for sure seeing what larger versions can do is one direction. Also,

705
01:06:39,760 --> 01:06:42,640
I mean, there are many questions. There's one question which I'm curious about,

706
01:06:42,640 --> 01:06:46,480
and that's the following. Right now, GPT two, so we feed it all this data from the

707
01:06:46,480 --> 01:06:50,400
internet, which means that it needs to memorize all those random facts about everything in the

708
01:06:50,400 --> 01:06:59,680
internet. It would be nice if the model could somehow use its own intelligence to decide

709
01:06:59,680 --> 01:07:04,240
what data it wants to start, accept, and what data it wants to reject, just like people.

710
01:07:04,240 --> 01:07:09,760
People don't learn all data indiscriminately. We are super selective about what we learn,

711
01:07:09,760 --> 01:07:13,040
and I think this kind of active learning I think would be very nice to have.

712
01:07:14,160 --> 01:07:21,040
Yeah. Listen, I love active learning. So let me ask, does the selection of data,

713
01:07:21,040 --> 01:07:25,920
can you just elaborate that a little bit more? Do you think the selection of data is,

714
01:07:28,000 --> 01:07:33,680
like I have this kind of sense that the optimization of how you select data,

715
01:07:33,760 --> 01:07:39,600
so the active learning process is going to be a place for a lot of breakthroughs

716
01:07:40,720 --> 01:07:44,960
even in the near future, because there hasn't been many breakthroughs there that are public.

717
01:07:44,960 --> 01:07:49,200
I feel like there might be private breakthroughs that companies keep to themselves,

718
01:07:49,200 --> 01:07:52,880
because the fundamental problem has to be solved if you want to solve self-driving,

719
01:07:52,880 --> 01:07:57,760
if you want to solve a particular task. What do you think about the space in general?

720
01:07:57,760 --> 01:08:02,400
Yeah, so I think that for something like active learning, or in fact for any kind of capability,

721
01:08:02,400 --> 01:08:07,680
like active learning, the thing that it really needs is a problem. It needs a problem that requires it.

722
01:08:09,280 --> 01:08:12,880
It's very hard to do research about the capability if you don't have a task,

723
01:08:12,880 --> 01:08:16,640
because then what's going to happen is you will come up with an artificial task,

724
01:08:16,640 --> 01:08:19,520
get good results, but not really convince anyone.

725
01:08:20,560 --> 01:08:27,520
Right. We're now past the stage where getting a result on MNIST,

726
01:08:27,520 --> 01:08:30,720
some clever formulation of MNIST will convince people.

727
01:08:30,720 --> 01:08:35,280
That's right. In fact, you could quite easily come up with a simple active learning scheme on MNIST

728
01:08:35,280 --> 01:08:44,880
and get a 10x speed up, but then so what? I think that active learning will naturally arise

729
01:08:45,440 --> 01:08:51,600
as problems that require it to pop up. That's my take on it.

730
01:08:52,720 --> 01:08:58,800
There's another interesting thing that OpenAS brought up with GPT2, which is when you create a

731
01:08:58,800 --> 01:09:04,560
powerful artificial intelligence system, and it was unclear what kind of detrimental,

732
01:09:04,560 --> 01:09:11,360
once you release GPT2, what kind of detrimental effect it'll have. Because if you have a model

733
01:09:11,360 --> 01:09:18,320
that can generate pretty realistic text, you can start to imagine that it would be used by bots in

734
01:09:19,360 --> 01:09:24,400
some way that we can't even imagine. There's this nervousness about what it's possible to do.

735
01:09:24,960 --> 01:09:30,000
You did a really brave and I think profound thing, which just started a conversation about this.

736
01:09:30,000 --> 01:09:38,400
How do we release powerful artificial intelligence models to the public? If we do it all, how do we

737
01:09:38,400 --> 01:09:45,440
privately discuss with other even competitors about how we manage the use of the systems and

738
01:09:45,440 --> 01:09:50,560
so on? From this whole experience, you've released a report on it, but in general,

739
01:09:50,560 --> 01:09:56,480
are there any insights that you've gathered from just thinking about this, about how you release

740
01:09:56,480 --> 01:10:03,760
models like this? I think that my take on this is that the field of AI has been in a state of

741
01:10:03,760 --> 01:10:10,240
childhood, and now it's exiting that state and it's entering a state of maturity. What that means

742
01:10:10,240 --> 01:10:15,600
is that AI is very successful and also very impactful, and its impact is not only large,

743
01:10:15,680 --> 01:10:23,840
but it's also growing. For that reason, it seems wise to start thinking about the impact of our

744
01:10:23,840 --> 01:10:28,560
systems before releasing them maybe a little bit too soon rather than a little bit too late.

745
01:10:29,760 --> 01:10:34,240
With the case of GPT-2, like I mentioned earlier, the results really were stunning,

746
01:10:35,040 --> 01:10:41,440
and it seemed plausible. It didn't seem certain. It seemed plausible that something like GPT-2

747
01:10:41,440 --> 01:10:48,800
could easily use to reduce the cost of disinformation. There was a question of what's

748
01:10:48,800 --> 01:10:52,880
the best way to release it, and a staged release seemed logical. A small model was released,

749
01:10:53,680 --> 01:10:59,840
and there was time to see the... Many people use these models in lots of cool ways. There've been

750
01:10:59,840 --> 01:11:06,080
lots of really cool applications. There haven't been any negative applications we know of,

751
01:11:06,080 --> 01:11:09,840
and so eventually it was released, but also other people replicated similar models.

752
01:11:09,840 --> 01:11:17,680
That's an interesting question though that we know of. In your view, staged release is at least

753
01:11:17,680 --> 01:11:25,360
part of the answer to the question of how do we... What do we do once we create a system like this?

754
01:11:25,920 --> 01:11:31,840
It's part of the answer, yes. Is there any other insights? Say you don't want to release the model

755
01:11:31,840 --> 01:11:37,440
at all because it's useful to you for whatever the business is. Well, there are plenty of people

756
01:11:37,440 --> 01:11:43,920
who don't release models already. Right, of course, but is there some moral ethical responsibility

757
01:11:44,480 --> 01:11:51,840
when you have a very powerful model to communicate? Just as you said, when you had GPT-2, it was

758
01:11:51,840 --> 01:11:55,200
unclear how much it could be used for misinformation. It's an open question,

759
01:11:56,240 --> 01:12:00,880
and getting an answer to that might require that you talk to other really smart people that are

760
01:12:01,600 --> 01:12:08,720
outside of your particular group. Have you... Please tell me there's some optimistic pathway for

761
01:12:09,280 --> 01:12:12,320
people across the world to collaborate on these kinds of cases,

762
01:12:14,560 --> 01:12:19,440
or is it still really difficult from one company to talk to another company?

763
01:12:19,440 --> 01:12:26,720
So it's definitely possible. It's definitely possible to discuss these kind of models with

764
01:12:26,720 --> 01:12:31,280
colleagues elsewhere and to get their take on what to do.

765
01:12:32,080 --> 01:12:34,000
How hard is it though? I mean...

766
01:12:36,320 --> 01:12:41,920
Do you see that happening? I think that's a place where it's important to gradually build trust

767
01:12:41,920 --> 01:12:47,840
between companies, because ultimately, all the AI developers are building technology,

768
01:12:47,840 --> 01:12:51,440
which is going to be increasingly more powerful, and so it's...

769
01:12:52,160 --> 01:12:57,040
The way to think about it is that ultimately we're only together.

770
01:12:58,640 --> 01:13:06,400
Yeah, it's... I tend to believe in the better angels of our nature, but I do hope that

771
01:13:09,040 --> 01:13:12,720
when you build a really powerful AI system in a particular domain,

772
01:13:12,720 --> 01:13:18,160
that you also think about the potential negative consequences of... Yeah.

773
01:13:22,240 --> 01:13:27,360
It's an interesting and scary possibility that there will be a race for AI development that

774
01:13:27,360 --> 01:13:34,400
would push people to close that development and not share ideas with others. I don't love this.

775
01:13:34,400 --> 01:13:39,440
I've been in a pure academic for 10 years. I really like sharing ideas and it's fun, it's exciting.

776
01:13:41,440 --> 01:13:45,440
What do you think it takes to... Let's talk about AGI a little bit. What do you think it takes to

777
01:13:45,440 --> 01:13:50,080
build a system of human level intelligence? We talked about reasoning, we talked about

778
01:13:50,080 --> 01:13:55,760
long-term memory, but in general, what does it take, do you think? Well, I can't be sure,

779
01:13:57,360 --> 01:14:05,520
but I think the deep learning plus maybe another small idea. Do you think self-play will be involved?

780
01:14:05,520 --> 01:14:11,040
Sort of like you've spoken about the powerful mechanism of self-play where systems learn by

781
01:14:12,480 --> 01:14:19,360
sort of exploring the world in a competitive setting against other entities that are similarly

782
01:14:19,360 --> 01:14:24,480
skilled as them and so incrementally improving this way? Do you think self-play will be a component

783
01:14:24,480 --> 01:14:31,200
of building an AGI system? Yeah, so what I would say to build AGI, I think it's going to be

784
01:14:32,480 --> 01:14:39,680
deep learning plus some ideas and I think self-play will be one of those ideas. I think that that is a

785
01:14:39,680 --> 01:14:50,320
very... Self-play has this amazing property that it can surprise us in truly novel ways. For example,

786
01:14:52,080 --> 01:14:59,280
like we, I mean, pretty much every self-play system, both are Dota bot. I don't know if

787
01:14:59,280 --> 01:15:05,200
OpenAI had a release about multi-agents where you had two little agents who were playing hide and

788
01:15:06,080 --> 01:15:11,520
seek and of course also AlphaZero. They all produce surprising behaviors. They all produce

789
01:15:11,520 --> 01:15:17,280
behaviors that we didn't expect. They are creative solutions to problems and that seems like an

790
01:15:17,280 --> 01:15:23,840
important part of AGI that our systems don't exhibit routinely right now. And so that's why I

791
01:15:23,840 --> 01:15:27,520
like this area, I like this direction because of its ability to surprises.

792
01:15:27,520 --> 01:15:32,560
To surprises and an AGI system would surprise us fundamentally. Yes, but and to be precise,

793
01:15:33,280 --> 01:15:38,640
not just a random surprise, but to find a surprising solution to a problem that's also useful.

794
01:15:39,920 --> 01:15:46,960
Now, a lot of the self-play mechanisms have been used in the game context or at least in the

795
01:15:46,960 --> 01:15:57,040
simulation context. How far along the path to AGI do you think will be done in simulation? How much

796
01:15:57,040 --> 01:16:04,960
faith promise do you have in simulation versus having to have a system that operates in the real

797
01:16:04,960 --> 01:16:11,680
world, whether it's the real world of digital real-world data or real-world like actual physical

798
01:16:11,680 --> 01:16:17,360
world with robotics? I don't think it's in either war. I think simulation is a tool and it helps.

799
01:16:17,360 --> 01:16:23,840
It has certain strengths and certain weaknesses and we should use it. Yeah, but okay, I understand

800
01:16:24,400 --> 01:16:34,320
that that's true. But one of the criticisms of self-play, one of the criticisms of reinforcement

801
01:16:34,320 --> 01:16:42,800
learning is one of the its current power, its current results while amazing have been demonstrated

802
01:16:42,800 --> 01:16:47,200
in a simulated environments or very constrained physical environments. Do you think it's possible

803
01:16:47,200 --> 01:16:53,280
to escape them? Escape the simulated environments and be able to learn in non-simulated environments?

804
01:16:53,280 --> 01:17:01,040
Or do you think it's possible to also just simulate in a photorealistic and physics realistic way

805
01:17:01,040 --> 01:17:06,080
the real world in a way that we can solve real problems with self-play in simulation?

806
01:17:07,440 --> 01:17:12,480
I think that transfer from simulation to the real world is definitely possible and has been

807
01:17:12,480 --> 01:17:17,600
exhibited many times by many different groups. It's been especially successful in vision.

808
01:17:18,560 --> 01:17:24,240
Also OpenAI in the summer has demonstrated a robot hand which was trained entirely in simulation

809
01:17:25,120 --> 01:17:28,160
in a certain way that allowed for sim to real transfer to occur.

810
01:17:29,680 --> 01:17:34,480
Is this for the Rubik's Cube? Yes, that's right. I wasn't aware that was trained in simulation.

811
01:17:34,480 --> 01:17:40,000
It was trained in simulation entirely. Really? So it wasn't in the physical that the hand wasn't

812
01:17:40,000 --> 01:17:46,240
trained? No, 100% of the training was done in simulation and the policy that was learned in

813
01:17:46,240 --> 01:17:51,360
simulation was trained to be very adaptive. So adaptive that when you transfer it, it could very

814
01:17:51,360 --> 01:17:57,760
quickly adapt to the physical world. So the kind of perturbations with the giraffe or whatever the

815
01:17:57,760 --> 01:18:05,200
heck it was, were those part of the simulation? Well, the simulation was generally, so the simulation

816
01:18:05,200 --> 01:18:10,160
was trained to be robust to many different things, but not the kind of perturbations we've had in the

817
01:18:10,160 --> 01:18:16,960
video. So it's never been trained with a glove. It's never been trained with a stuffed giraffe.

818
01:18:16,960 --> 01:18:20,800
So in theory, these are novel perturbations? Correct. It's not in theory in practice.

819
01:18:21,680 --> 01:18:28,320
That those are novel perturbations? Well, that's okay. That's a clean, small-scale,

820
01:18:28,320 --> 01:18:32,000
but clean example of a transfer from the simulated world to the physical world.

821
01:18:32,000 --> 01:18:36,960
Yeah, and I will also say that I expect the transfer capabilities of deep learning to increase in

822
01:18:36,960 --> 01:18:42,000
general and the better the transfer capabilities are, the more useful simulation will become

823
01:18:43,520 --> 01:18:47,760
because then you could take, you could experience something in simulation

824
01:18:48,400 --> 01:18:52,720
and then learn a moral of the story which you could then carry with you to the real world, right?

825
01:18:53,360 --> 01:19:01,040
As humans do all the time and they play computer games. So let me ask sort of an embodied question,

826
01:19:01,760 --> 01:19:08,400
staying on AGI for a sec. Do you think AGI system, we need to have a body? We need to have some of

827
01:19:08,400 --> 01:19:16,560
those human elements of self-awareness, consciousness, sort of fear of mortality, sort of self-preservation

828
01:19:16,560 --> 01:19:22,320
in the physical space which comes with having a body? I think having a body will be useful.

829
01:19:22,320 --> 01:19:26,720
I don't think it's necessary, but I think it's very useful to have a body for sure because you can

830
01:19:27,200 --> 01:19:31,600
learn a whole new, you can learn things which cannot be learned without a body,

831
01:19:32,400 --> 01:19:36,000
but at the same time, I think that you can, if you don't have a body, you could

832
01:19:36,000 --> 01:19:40,880
compensate for it and still succeed. You think so? Yes. Well, there is evidence for this. For

833
01:19:40,880 --> 01:19:46,720
example, there are many people who were born deaf and blind and they were able to compensate for

834
01:19:46,720 --> 01:19:52,560
the lack of modalities. I'm thinking about Helen Keller specifically. So even if you're not able

835
01:19:52,560 --> 01:19:58,320
to physically interact with the world and if you're not able to, I actually was getting at,

836
01:19:59,520 --> 01:20:05,280
maybe let me ask on the more particular, I'm not sure if it's connected to having a body or not,

837
01:20:05,280 --> 01:20:11,200
but the idea of consciousness and a more constrained version of that is self-awareness.

838
01:20:11,200 --> 01:20:17,760
Do you think an AGI system should have consciousness? We can't define whatever the

839
01:20:17,760 --> 01:20:22,880
heck you think consciousness is. Yeah. Hard question to answer, given how hard it is to define it.

840
01:20:24,640 --> 01:20:28,240
Do you think it's useful to think about? I mean, it's definitely interesting,

841
01:20:28,240 --> 01:20:32,880
it's fascinating. I think it's definitely possible that our systems will be conscious.

842
01:20:33,760 --> 01:20:37,680
Do you think that's an emergent thing that just comes from, do you think consciousness could emerge

843
01:20:37,680 --> 01:20:42,400
from the representation that's stored within your networks? So like that it naturally just

844
01:20:42,400 --> 01:20:47,040
emerges when you become more and more, you're able to represent more and more over the world?

845
01:20:47,040 --> 01:20:52,720
Well, I'd say I'd make the following argument, which is humans are conscious,

846
01:20:53,680 --> 01:20:56,960
and if you believe that artificial neural nets are sufficiently

847
01:20:57,840 --> 01:21:03,200
similar to the brain, then there should at least exist artificial neural nets you should be conscious

848
01:21:03,200 --> 01:21:11,520
to. You're leaning on that existence proof pretty heavily. Okay. But that's the best answer I can

849
01:21:11,600 --> 01:21:19,120
give. No, I know. I know. There's still an open question if there's not some magic in the brain

850
01:21:19,120 --> 01:21:27,040
that we're not. I mean, I don't mean a non-materialistic magic, but that the brain might be a lot

851
01:21:27,040 --> 01:21:31,440
more complicated and interesting than we give it credit for. If that's the case, then it should show

852
01:21:31,440 --> 01:21:37,280
up and at some point, we will find out that we can't continue to make progress. But I think

853
01:21:37,280 --> 01:21:41,920
it's unlikely. So we talk about consciousness, but let me talk about another poorly defined

854
01:21:41,920 --> 01:21:48,240
concept of intelligence. Again, we've talked about reasoning. We've talked about memory. What do

855
01:21:48,240 --> 01:21:54,480
you think is a good test of intelligence for you? Are you impressed by the test that Alan Turing

856
01:21:54,480 --> 01:22:01,200
formulated with the imitation game with natural language? Is there something in your mind that

857
01:22:01,280 --> 01:22:09,920
you would be deeply impressed by if a system was able to do? I mean, lots of things. There's a

858
01:22:09,920 --> 01:22:16,800
certain frontier of capabilities today. And there exist things outside of that frontier,

859
01:22:16,800 --> 01:22:23,200
and I would be impressed by any such thing. For example, I would be impressed by a deep learning

860
01:22:23,200 --> 01:22:30,080
system which solves a very pedestrian task like machine translation or computer vision task or

861
01:22:30,080 --> 01:22:37,600
something which never makes a mistake a human wouldn't make under any circumstances. I think

862
01:22:37,600 --> 01:22:41,680
that is something which have not yet been demonstrated and I would find it very impressive.

863
01:22:42,640 --> 01:22:46,480
Yes, so right now, they make mistakes in different, they might be more accurate than human beings,

864
01:22:46,480 --> 01:22:53,360
but they still make a different set of mistakes. So I would guess that a lot of the skepticism

865
01:22:53,360 --> 01:22:57,280
that some people have about deep learning is when they look at their mistakes and they say,

866
01:22:57,280 --> 01:23:02,160
well, those mistakes, they make no sense. If you understood the concept, you wouldn't make that

867
01:23:02,160 --> 01:23:11,680
mistake. And I think that changing that would inspire me. That would be, yes, this is progress.

868
01:23:12,400 --> 01:23:18,640
Yeah, that's a really nice way to put it. But I also just don't like that human instinct to

869
01:23:19,680 --> 01:23:24,320
criticize a model is not intelligent. That's the same instinct as we do when we criticize

870
01:23:24,320 --> 01:23:34,240
any group of creatures as the other. Because it's very possible that GPT2 is much smarter

871
01:23:34,240 --> 01:23:39,280
than human beings at many things. That's definitely true. It has a lot more breadth of knowledge.

872
01:23:39,280 --> 01:23:44,640
Yes, breadth of knowledge and even perhaps depth on certain topics.

873
01:23:46,000 --> 01:23:50,240
It's kind of hard to judge what depth means, but there's definitely a sense in which

874
01:23:51,120 --> 01:23:53,920
humans don't make mistakes that these models do.

875
01:23:54,480 --> 01:23:59,280
Yes, the same is applied to autonomous vehicles. The same is probably going to continue being

876
01:23:59,280 --> 01:24:07,360
applied to a lot of artificial intelligence systems. In the 21st century, the process of

877
01:24:07,360 --> 01:24:15,280
analyzing the progress of AI is the search for one case where the system fails in a big way

878
01:24:15,280 --> 01:24:21,520
where humans would not. And then many people writing articles about it. And then broadly,

879
01:24:22,720 --> 01:24:28,720
the public generally gets convinced that the system is not intelligent. And we pacify ourselves

880
01:24:28,720 --> 01:24:33,520
by thinking it's not intelligent because of this one anecdotal case. And this seems to continue

881
01:24:33,520 --> 01:24:38,400
happening. Yeah, I mean, there is truth to that. Although I'm sure that plenty of people are also

882
01:24:38,400 --> 01:24:42,160
extremely impressed by the systems that exist today. But I think this connects to the earlier

883
01:24:42,160 --> 01:24:49,520
point we discussed that it's just confusing to judge progress in AI. And you have a new robot

884
01:24:49,520 --> 01:24:55,440
demonstrating something. How impressed should you be? And I think that people will start to be

885
01:24:55,440 --> 01:25:01,520
impressed once AI starts to really move the needle on the GDP. So you're one of the people that

886
01:25:01,520 --> 01:25:08,000
might be able to create an AI system here, not you, but you and open AI. If you do create an

887
01:25:08,000 --> 01:25:16,000
AI system, and you get to spend sort of the evening with it, him, her, what would you talk about,

888
01:25:16,000 --> 01:25:20,800
do you think? The very first time? First time? Well, the first time I would just

889
01:25:21,760 --> 01:25:26,320
would just ask all kinds of questions and try to make it to get it to make a mistake. And that would

890
01:25:26,320 --> 01:25:33,920
be amazed that it doesn't make mistakes and just keep keep asking broad. Okay, what kind of questions

891
01:25:33,920 --> 01:25:41,120
do you think? Would they be factual or would they be personal, emotional, psychological? What do you

892
01:25:41,120 --> 01:25:51,520
think? All of the above. Would you ask for advice? Definitely. I mean, why would I limit myself

893
01:25:51,520 --> 01:25:57,440
talking to a system like this? Now, again, let me emphasize the fact that you truly are one of the

894
01:25:57,440 --> 01:26:05,120
people that might be in the room where this happens. So let me ask a sort of a profound question

895
01:26:05,120 --> 01:26:11,520
about, I've just talked to a Stalin historian. I've been talking to a lot of people who are

896
01:26:11,520 --> 01:26:18,560
studying power. Abraham Lincoln said, nearly all men can stand adversity. But if you want to test

897
01:26:18,560 --> 01:26:26,400
a man's character, give him power. I would say the power of the 21st century, maybe the 22nd,

898
01:26:26,400 --> 01:26:32,640
but hopefully the 21st would be the creation of an AGI system and the people who have control,

899
01:26:33,360 --> 01:26:40,000
direct possession and control the AGI system. So what do you think after spending that evening

900
01:26:41,120 --> 01:26:45,120
having a discussion with the AGI system? What do you think you would do?

901
01:26:45,520 --> 01:26:56,240
Well, the ideal world I'd like to imagine is one where humanity are like the board,

902
01:26:56,960 --> 01:27:03,760
the board members of a company where the AGI is the CEO. So it would be,

903
01:27:05,520 --> 01:27:09,840
I would like the picture of which I would imagine is you have some kind of different

904
01:27:10,720 --> 01:27:17,520
entities, different countries or cities, and the people that leave their vote for what the AGI

905
01:27:17,520 --> 01:27:22,160
that represents them should do and the AGI that represents them goes and does it. I think a picture

906
01:27:22,160 --> 01:27:28,560
like that, I find very appealing. You could have multiple AGI, you would have an AGI for a city,

907
01:27:28,560 --> 01:27:35,920
for a country, and it would be trying to in effect take the democratic process to the next level.

908
01:27:35,920 --> 01:27:40,080
And the board can almost fire the CEO? Essentially, press the reset button,

909
01:27:40,080 --> 01:27:47,200
say, re-randomize the parameters. But let me sort of, that's actually, okay, that's a beautiful

910
01:27:47,200 --> 01:27:54,560
vision, I think, as long as it's possible to press the reset button. Do you think it will always be

911
01:27:54,560 --> 01:28:00,080
possible to press the reset button? So I think that it's definitely really possible to build.

912
01:28:00,880 --> 01:28:11,600
So you're talking, so the question that I really understand from you is, will humans or humans

913
01:28:11,600 --> 01:28:16,720
people have control over the AI systems that they build? Yes. And my answer is, it's definitely

914
01:28:16,720 --> 01:28:22,720
possible to build AI systems which will want to be controlled by their humans. Wow, that's part of

915
01:28:22,720 --> 01:28:32,080
their, so it's not that just they can't help but be controlled, but that's the one of the

916
01:28:32,080 --> 01:28:37,360
objectives of their existence is to be controlled. In the same way that human parents

917
01:28:39,760 --> 01:28:45,280
generally want to help their children, they want their children to succeed. It's not a burden for

918
01:28:45,280 --> 01:28:51,600
them. They are excited to help the children to feed them and to dress them and to take care of them.

919
01:28:52,560 --> 01:28:59,360
And I believe with high conviction that the same will be possible for an AGI. It will be possible

920
01:28:59,360 --> 01:29:05,360
to program an AGI to design it in such a way that it will have a similar deep drive that it will be

921
01:29:05,360 --> 01:29:12,800
delighted to fulfill and the drive will be to help humans flourish. But let me take a step back

922
01:29:12,800 --> 01:29:16,880
to that moment where you create the AGI system. I think this is a really crucial moment.

923
01:29:17,760 --> 01:29:27,360
And between that moment and the Democratic Board members with the AGI at the head,

924
01:29:28,800 --> 01:29:36,160
there has to be a relinquishing of power. So as George Washington, despite all the bad things he

925
01:29:36,160 --> 01:29:40,960
did, one of the big things he did is he relinquished power. He first of all didn't want to be president

926
01:29:41,920 --> 01:29:47,760
and even when he became president, he didn't keep just serving as most dictators do for indefinitely.

927
01:29:49,120 --> 01:29:57,120
Do you see yourself being able to relinquish control over an AGI system, given how much

928
01:29:57,120 --> 01:30:03,040
power you can have over the world, at first financial, just make a lot of money and then

929
01:30:03,040 --> 01:30:09,280
control by having possession as a AGI system? I'd find it trivial to do that. I'd find it

930
01:30:09,360 --> 01:30:14,320
trivial to relinquish this kind of power. I mean, you know, the kind of scenario you are

931
01:30:14,320 --> 01:30:21,200
describing sounds terrifying to me. That's all. I would absolutely not want to be in that position.

932
01:30:22,400 --> 01:30:28,640
Do you think you represent the majority or the minority of people in the AGI community?

933
01:30:29,280 --> 01:30:35,600
Well, I mean, it's an open question, an important one. Are most people good is another way to ask it.

934
01:30:36,480 --> 01:30:39,600
So I don't know if most people are good. But

935
01:30:42,160 --> 01:30:45,760
I think that when it really counts, people can be better than we think.

936
01:30:46,880 --> 01:30:52,560
That's beautifully put. Yeah. Are there specific mechanisms you can think of of aligning AGI

937
01:30:52,560 --> 01:30:58,320
in values to human values? Is that do you think about these problems of continued alignment

938
01:30:58,320 --> 01:31:03,440
as we develop the systems? Yeah, definitely. In some sense,

939
01:31:04,400 --> 01:31:09,120
the kind of question which you are asking is so if you have to translate the question to today's

940
01:31:09,120 --> 01:31:18,400
terms, it would be a question about how to get an RL agent that's optimizing a value function

941
01:31:18,400 --> 01:31:23,920
which itself is learned. And if you look at humans, humans are like that because the reward

942
01:31:23,920 --> 01:31:29,920
function, the value function of humans is not external, it is internal. That's right. And

943
01:31:30,080 --> 01:31:38,080
and there are definite ideas of how to train a value function, basically an objective,

944
01:31:38,960 --> 01:31:44,240
an as objective as possible perception system that will be trained separately

945
01:31:45,680 --> 01:31:53,120
to recognize, to internalize human judgments on different situations. And then that component

946
01:31:53,120 --> 01:31:58,880
wouldn't be integrated as the value as the base value function for some more capable RL system.

947
01:31:58,880 --> 01:32:02,960
You could imagine a process like this. I'm not saying this is the process. I'm saying this is

948
01:32:02,960 --> 01:32:11,120
an example of the kind of thing you could do. So on that topic of the objective functions of

949
01:32:11,120 --> 01:32:16,720
human existence, what do you think is the objective function that's implicit in human

950
01:32:16,720 --> 01:32:32,480
existence? What's the meaning of life? Oh, I think the question is wrong in some way. I think that

951
01:32:32,480 --> 01:32:36,640
the question implies that there is an objective answer which is an external answer, you know,

952
01:32:36,640 --> 01:32:44,320
your meaning of life is X. I think what's going on is that we exist and that's amazing. And we

953
01:32:44,400 --> 01:32:48,960
should try to make the most of it and try to maximize our own value and enjoyment of

954
01:32:49,760 --> 01:32:56,160
a very short time while we do exist. It's funny because action does require an objective function.

955
01:32:56,160 --> 01:33:01,840
It's definitely theirs in some form, but it's difficult to make it explicit and maybe impossible

956
01:33:01,840 --> 01:33:07,120
to make it explicit. I guess is what you're getting at. And that's an interesting fact of an RL

957
01:33:07,120 --> 01:33:13,360
environment. Well, what I was making a slightly different point is that humans want things and

958
01:33:13,360 --> 01:33:19,760
their wants create the drives that cause them to, you know, our wants are our objective functions,

959
01:33:19,760 --> 01:33:24,160
our individual objective functions. We can later decide that we want to change,

960
01:33:24,160 --> 01:33:27,120
that what we wanted before is no longer good and we want something else.

961
01:33:27,120 --> 01:33:32,000
Yeah, but they're so dynamic, there's got to be some underlying sort of Freud,

962
01:33:32,000 --> 01:33:37,840
there's things like sexual stuff, there's people who think it's the fear of death and there's also

963
01:33:38,800 --> 01:33:42,720
the desire for knowledge and, you know, all these kinds of things, procreation,

964
01:33:43,760 --> 01:33:48,000
the sort of all the evolutionary arguments, it seems to be, there might be some kind of

965
01:33:48,000 --> 01:33:54,880
fundamental objective function from which everything else emerges. But it seems like

966
01:33:54,880 --> 01:33:58,160
it's very difficult to make it explicit. I think that probably is an evolutionary

967
01:33:58,160 --> 01:34:01,600
objective function, which is to survive and procreate and make your students succeed.

968
01:34:02,480 --> 01:34:07,360
That would be my guess, but it doesn't give an answer to the question of what's the meaning of

969
01:34:07,360 --> 01:34:14,560
life. I think you can see how humans are part of this big process, this ancient process we are,

970
01:34:16,640 --> 01:34:24,160
we exist on a small planet. And that's it. So given that we exist, try to make the most of it

971
01:34:24,160 --> 01:34:30,960
and try to enjoy more and suffer less as much as we can. Let me ask two silly questions about life.

972
01:34:31,680 --> 01:34:39,360
One, do you have regrets? Moments that if you went back, you would do differently. And two,

973
01:34:40,000 --> 01:34:43,360
are there moments that you're especially proud of that made you truly happy?

974
01:34:44,720 --> 01:34:52,080
So I can answer both questions. Of course, there's a huge number of choices and decisions that have

975
01:34:52,080 --> 01:34:56,880
made that with the benefit of hindsight, I wouldn't have made them. And I do experience some regret,

976
01:34:56,880 --> 01:35:01,680
but I tried to take solace in the knowledge that at the time I did the best I could.

977
01:35:02,720 --> 01:35:07,360
And in terms of things that I'm proud of, I'm very fortunate to have done things I'm proud of

978
01:35:08,640 --> 01:35:13,200
and they made me happy for some time, but I don't think that that is the source of happiness.

979
01:35:14,480 --> 01:35:19,280
So your academic accomplishments, all the papers, you're one of the most cited people in the world.

980
01:35:19,840 --> 01:35:23,440
All the breakthroughs I mentioned in computer vision and language and so on

981
01:35:23,440 --> 01:35:29,440
is what is the source of happiness and pride for you?

982
01:35:29,440 --> 01:35:33,920
I mean, all those things are a source of pride for sure. I'm very grateful for having done all

983
01:35:33,920 --> 01:35:40,160
those things. And it was very fun to do them. But happiness comes, you know, you can, happiness,

984
01:35:40,160 --> 01:35:46,160
well, my current view is that happiness comes from our to a very large degree from the way we

985
01:35:46,160 --> 01:35:51,600
look at things. You know, you can have a simple meal and be quite happy as a result or you can

986
01:35:51,600 --> 01:35:57,360
talk to someone and be happy as a result as well. Or conversely, you can have a meal and be

987
01:35:57,360 --> 01:36:02,240
disappointed that the meal wasn't a better meal. So I think a lot of happiness comes from that.

988
01:36:02,240 --> 01:36:08,000
But I'm not sure. I don't want to be too confident. Being humble in the face of the uncertainty seems

989
01:36:08,000 --> 01:36:13,840
to be also a part of this whole happiness thing. Well, I don't think there's a better way to end

990
01:36:13,840 --> 01:36:19,840
it than meaning of life and discussions of happiness. So Ilya, thank you so much. You've

991
01:36:19,840 --> 01:36:25,600
given me a few incredible ideas. You've given the world many incredible ideas. I really appreciate it.

992
01:36:25,600 --> 01:36:29,360
And thanks for talking today. Yeah, thanks for stopping by. I really enjoyed it.

993
01:36:30,400 --> 01:36:34,080
Thanks for listening to this conversation with Ilya Setskever. And thank you to our

994
01:36:34,080 --> 01:36:39,440
presenting sponsor, Cash App. Please consider supporting the podcast by downloading Cash App

995
01:36:39,440 --> 01:36:45,280
and using the code Lex podcast. If you enjoy this podcast, subscribe on YouTube,

996
01:36:45,280 --> 01:36:50,400
review it with five stars and Apple podcast, support on Patreon or simply connect with me

997
01:36:50,400 --> 01:36:58,240
on Twitter at Lex Friedman. And now let me leave you with some words from Alan Turing on machine

998
01:36:58,240 --> 01:37:04,960
learning. Instead of trying to produce a program to simulate the adult mind, why not rather try to

999
01:37:04,960 --> 01:37:11,280
produce one which simulates the child? If this were then subjected to an appropriate course of

1000
01:37:11,280 --> 01:37:27,200
education, one would obtain the adult brain. Thank you for listening and hope to see you next time.

