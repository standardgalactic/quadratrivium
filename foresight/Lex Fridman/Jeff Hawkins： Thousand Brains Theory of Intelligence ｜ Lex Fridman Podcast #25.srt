1
00:00:00,000 --> 00:00:02,320
The following is a conversation with Jeff Hawkins.

2
00:00:02,320 --> 00:00:07,040
He's the founder of the Redwood Center for Theoretical Neuroscience in 2002 and New

3
00:00:07,040 --> 00:00:13,280
Menta in 2005. In his 2004 book titled On Intelligence and in the research before and

4
00:00:13,280 --> 00:00:18,640
after, he and his team have worked to reverse engineer the New York Cortex and propose artificial

5
00:00:18,640 --> 00:00:22,960
intelligence architectures approaches and ideas that are inspired by the human brain.

6
00:00:23,600 --> 00:00:28,880
These ideas include hierarchical temporal memory, HTM from 2004, and new work,

7
00:00:28,880 --> 00:00:36,080
the 1000s brain theory of intelligence from 2017, 18, and 19. Jeff's ideas have been an inspiration

8
00:00:36,080 --> 00:00:40,400
to many who have looked for progress beyond the current machine learning approaches,

9
00:00:40,400 --> 00:00:46,160
but they have also received criticism for lacking a body of empirical evidence supporting the models.

10
00:00:46,160 --> 00:00:50,720
This is always a challenge when seeking more than small incremental steps forward in AI.

11
00:00:51,360 --> 00:00:56,480
Jeff is a brilliant mind and many of the ideas he has developed and aggregated from neuroscience

12
00:00:56,480 --> 00:01:01,360
are worth understanding and thinking about. There are limits to deep learning as it is

13
00:01:01,360 --> 00:01:07,040
currently defined. Forward progress in AI is shrouded in mystery. My hope is that conversations

14
00:01:07,040 --> 00:01:13,280
like this can help provide an inspiring spark for new ideas. This is the Artificial Intelligence

15
00:01:13,280 --> 00:01:18,560
Podcast. If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter

16
00:01:18,560 --> 00:01:25,040
at Lex Friedman spelled F-R-I-D. And now here's my conversation with Jeff Hawkins.

17
00:01:49,520 --> 00:01:55,360
I also firmly believe that we will not be able to create fully intelligent machines until we

18
00:01:55,360 --> 00:02:00,560
understand how the human brain works. So I don't see those as separate problems. I think there's

19
00:02:00,560 --> 00:02:04,240
limits to what can be done with machine intelligence if you don't understand the principles by which

20
00:02:04,240 --> 00:02:10,000
the brain works. And so I actually believe that studying the brain is actually the fastest way

21
00:02:10,000 --> 00:02:15,520
to get to machine intelligence. And within that, let me ask the impossible question. How do you

22
00:02:15,520 --> 00:02:18,720
not define but at least think about what it means to be intelligent?

23
00:02:19,360 --> 00:02:24,400
So I didn't try to answer that question first. We said let's just talk about how the brain works

24
00:02:24,400 --> 00:02:29,120
and let's figure out how certain parts of the brain, mostly the neocortex, but some other parts too,

25
00:02:29,680 --> 00:02:34,480
the parts of the brain most associated with intelligence, and let's discover the principles

26
00:02:34,480 --> 00:02:39,920
by how they work. Because intelligence isn't just like some mechanism and it's not just some

27
00:02:39,920 --> 00:02:46,880
capabilities. It's like, okay, we don't even know where to begin on this stuff. And so now that we've

28
00:02:46,880 --> 00:02:51,520
made a lot of progress on this, after we've made a lot of progress on how the neocortex works,

29
00:02:51,520 --> 00:02:56,080
and we can talk about that, I now have a very good idea what's going to be required to make

30
00:02:56,080 --> 00:03:02,000
intelligent machines. I can tell you today, some of the things are going to be necessary, I believe,

31
00:03:02,000 --> 00:03:06,400
to create intelligent machines. Well, so we'll get there. We'll get to the neocortex and some of

32
00:03:06,400 --> 00:03:11,360
the theories of how the whole thing works. And you're saying, as we understand more and more

33
00:03:12,560 --> 00:03:17,280
about the neocortex, about our own human mind, we'll be able to start to more specifically

34
00:03:17,280 --> 00:03:21,760
define what it means to be intelligent. It's not useful to really talk about that until...

35
00:03:21,760 --> 00:03:26,880
I don't know if it's not useful. Look, there's a long history of AI, as you know. And there's

36
00:03:26,880 --> 00:03:34,480
been different approaches taken to it. And who knows, maybe they're all useful. So the good

37
00:03:34,480 --> 00:03:39,600
old fashioned AI, the expert systems, the current convolutional neural networks, they all have their

38
00:03:39,600 --> 00:03:45,520
utility. They all have a value in the world. But I would think almost everyone agree that none of

39
00:03:45,520 --> 00:03:52,960
them are really intelligent in a sort of a deep way that humans are. And so it's just the question

40
00:03:52,960 --> 00:03:57,760
is how do you get from where those systems were or are today to where a lot of people think we're

41
00:03:57,760 --> 00:04:04,880
going to go? And there's a big, big gap there, a huge gap. And I think the quickest way of bridging

42
00:04:04,880 --> 00:04:10,320
that gap is to figure out how the brain does that. And then we can sit back and look and say, oh,

43
00:04:10,320 --> 00:04:15,360
what are these principles that the brain works on are necessary and which ones are not? Clearly,

44
00:04:15,360 --> 00:04:18,640
we don't have to build this in... and intelligent machines aren't going to be built out of

45
00:04:20,480 --> 00:04:25,760
organic living cells. But there's a lot of stuff that goes on the brain that's going to be necessary.

46
00:04:25,760 --> 00:04:31,840
So let me ask maybe, before we get into the fun details, let me ask maybe a depressing or

47
00:04:31,840 --> 00:04:37,360
a difficult question. Do you think it's possible that we will never be able to understand how our

48
00:04:37,360 --> 00:04:44,960
brain works? That maybe there's aspects to the human mind, like we ourselves cannot introspectively

49
00:04:44,960 --> 00:04:49,840
get to the core, that there's a wall you eventually hit? Yeah, I don't believe that's the case.

50
00:04:50,720 --> 00:04:54,400
I have never believed that's the case. There's not been a single thing we've ever,

51
00:04:54,400 --> 00:04:58,640
human have ever put their minds to. We've said, oh, we reached the wall. We can't go any further.

52
00:04:58,640 --> 00:05:03,440
People keep saying that. People used to believe that about life, you know, Alain Vaital, right?

53
00:05:03,440 --> 00:05:06,400
There's like, what's the difference between living matter and non-living matter? Something

54
00:05:06,400 --> 00:05:12,720
special you never understand. We no longer think that. So there's no historical evidence that

55
00:05:12,720 --> 00:05:17,440
suggests this is the case. And I just never even consider that's a possibility. I would also say

56
00:05:17,520 --> 00:05:23,760
I would also say today we understand so much about the near cortex. We've made tremendous

57
00:05:23,760 --> 00:05:30,880
progress in the last few years that I no longer think of as an open question. The answers are

58
00:05:30,880 --> 00:05:36,000
very clear to me. The pieces we know we don't know are clear to me, but the framework is all there.

59
00:05:36,000 --> 00:05:39,840
And it's like, oh, okay, we're going to be able to do this. This is not a problem anymore.

60
00:05:39,840 --> 00:05:43,440
It just takes time and effort. But there's no mystery, big mystery anymore.

61
00:05:44,000 --> 00:05:52,240
So then let's get into it for people like myself who are not very well versed in the human brain,

62
00:05:52,800 --> 00:05:58,160
except my own. Can you describe to me at the highest level, what are the different parts

63
00:05:58,160 --> 00:06:04,000
of the human brain and then zooming in on the near cortex, the parts of the near cortex and so on,

64
00:06:04,000 --> 00:06:09,680
a quick overview? Yeah, sure. The human brain, we can divide it roughly into two parts.

65
00:06:10,640 --> 00:06:16,400
There's the old parts, lots of pieces, and then there's the new part. The new part is the near

66
00:06:16,400 --> 00:06:22,080
cortex. It's new because it didn't exist before mammals. The only mammals have a near cortex

67
00:06:22,080 --> 00:06:27,200
and in humans, in primates, it's very large. In the human brain, the near cortex occupies about

68
00:06:27,200 --> 00:06:33,520
70 to 75% of the volume of the brain. It's huge. And the old parts of the brain are,

69
00:06:34,720 --> 00:06:38,640
there's lots of pieces there. There's a spinal cord, and there's the brain stem,

70
00:06:38,640 --> 00:06:41,360
and the cerebellum, and the different parts of the basal ganglion and so on.

71
00:06:41,920 --> 00:06:46,240
In the old parts of the brain, you have the autonomic regulation like breathing and heart rate.

72
00:06:46,240 --> 00:06:50,560
You have basic behaviors. So like walking and running are controlled by the old parts of the

73
00:06:50,560 --> 00:06:54,160
brain. All the emotional centers of the brain are in the old part of the brain. So when you

74
00:06:54,160 --> 00:06:57,200
feel anger or hungry, lust or things like that, those are all in the old parts of the brain.

75
00:06:59,040 --> 00:07:04,480
And we associate with the near cortex all the things we think about as sort of high level

76
00:07:04,480 --> 00:07:12,880
perception and cognitive functions, anything from seeing and hearing and touching things to

77
00:07:12,880 --> 00:07:18,160
language to mathematics and engineering and science and so on. Those are all associated with the

78
00:07:18,160 --> 00:07:24,080
near cortex. And they're certainly correlated. Our abilities in those regards are correlated with

79
00:07:24,080 --> 00:07:29,120
the relative size of our near cortex compared to other mammals. So that's like the rough

80
00:07:29,200 --> 00:07:35,440
division. And you obviously can't understand the near cortex completely isolated, but you

81
00:07:35,440 --> 00:07:39,600
can understand a lot of it with just a few interfaces to the old parts of the brain.

82
00:07:40,320 --> 00:07:47,120
And so it gives you a system to study. The other remarkable thing about the near cortex

83
00:07:47,920 --> 00:07:54,720
compared to the old parts of the brain is the near cortex is extremely uniform. It's not visibly or

84
00:07:54,720 --> 00:08:01,200
anatomically or it's very it's like a I always like to say it's like the size of a dinner napkin

85
00:08:01,200 --> 00:08:06,240
about two and a half millimeters thick. And it looks remarkably the same everywhere. Everywhere

86
00:08:06,240 --> 00:08:10,400
you look in that two and a half millimeters is this detailed architecture. And it looks

87
00:08:10,400 --> 00:08:14,640
remarkably the same everywhere. And that's across species, a mouse versus a cat and a dog and a

88
00:08:14,640 --> 00:08:18,720
human. Where if you look at the old parts of the brain, there's lots of little pieces do specific

89
00:08:18,720 --> 00:08:23,200
things. So it's like the old parts of a brain involved like this is the part that controls

90
00:08:23,200 --> 00:08:26,000
heart rate. And this is the part that controls this and this is this kind of thing. And that's

91
00:08:26,000 --> 00:08:30,960
this kind of thing. And these evolve for eons of a long, long time. And they have those specific

92
00:08:30,960 --> 00:08:34,320
functions. And all of a sudden mammals come along and they got this thing called the near

93
00:08:34,320 --> 00:08:39,280
cortex. And it got large by just replicating the same thing over and over and over again.

94
00:08:39,280 --> 00:08:47,760
This is like, wow, this is incredible. So all the evidence we have. And this is an idea that was

95
00:08:47,760 --> 00:08:54,880
first articulated in a very cogent and beautiful argument by a guy named Vernon Malcastle in 1978,

96
00:08:54,880 --> 00:09:03,760
I think it was, that the neocortex all works on the same principle. So language, hearing,

97
00:09:03,760 --> 00:09:08,320
touch, vision, engineering, all these things are basically underlying or all built in the same

98
00:09:08,320 --> 00:09:12,960
computational substrate. They're really all the same problem. So the low level of the building

99
00:09:12,960 --> 00:09:17,440
blocks all look similar. Yeah. And they're not even that low level. We're not talking about like

100
00:09:17,440 --> 00:09:21,520
neurons. We're talking about this very complex circuit that exists throughout the neocortex is

101
00:09:22,160 --> 00:09:27,200
remarkably similar. It is, it's like, yes, you see variations of it here and there, more of the cell

102
00:09:27,200 --> 00:09:34,080
left and left and so on. But what Malcastle argued was it says, you know, if you take a section on

103
00:09:34,080 --> 00:09:40,160
neocortex, why is one a visual area and one is a auditory area? Or why is, and his answer was

104
00:09:41,040 --> 00:09:44,320
it's because one is connected to eyes and one is connected to ears.

105
00:09:45,280 --> 00:09:50,320
Literally, you mean just it's most closest in terms of number of connections to the sensor?

106
00:09:50,320 --> 00:09:55,200
Literally, if you took the optic nerve and attached it to a different part of the neocortex,

107
00:09:55,200 --> 00:10:00,320
that part would become a visual region. This actually, this experiment was actually done by

108
00:10:00,320 --> 00:10:06,640
McGonkiss Sir in developing, I think it was lemurs, I can't remember what it was, some animal.

109
00:10:06,640 --> 00:10:10,480
And, and there's a lot of evidence to this. You know, if you take a blind person, a person is

110
00:10:10,480 --> 00:10:16,000
born blind at birth. They, they're born with a visual neocortex. It doesn't

111
00:10:16,000 --> 00:10:21,440
may not get any input from the eyes, because of some congenital defect or something. And

112
00:10:22,480 --> 00:10:29,200
that region becomes, does something else. It picks up another task. So, and it's, it's,

113
00:10:29,200 --> 00:10:33,760
so it's this, it's this very complex thing. It's not like, oh, they're all built on neurons. No,

114
00:10:33,760 --> 00:10:39,520
they're all built in this very complex circuit. And, and somehow that circuit underlies everything.

115
00:10:40,160 --> 00:10:47,040
And so this is the, it's called the common cortical algorithm, if you will. Some scientists just find

116
00:10:47,040 --> 00:10:51,440
it hard to believe. And they just say, I can't believe that's true. But the evidence is overwhelming

117
00:10:51,440 --> 00:10:56,320
in this case. And so a large part of what it means to figure out how the brain creates intelligence

118
00:10:56,320 --> 00:11:02,240
and what is intelligence in the brain is to understand what that circuit does. If you can

119
00:11:02,240 --> 00:11:08,000
figure out what that circuit does, as amazing as it is, then you can, then you, then you understand

120
00:11:08,000 --> 00:11:13,760
what all these other cognitive functions are. So if you were to sort of put neural cortex outside

121
00:11:13,760 --> 00:11:18,400
of your book on intelligence, if you look, if you wrote a giant tome, a textbook on the neural

122
00:11:18,400 --> 00:11:25,760
cortex, and you look maybe a couple centuries from now, how much of what we know now would still

123
00:11:25,760 --> 00:11:30,720
be accurate two centuries from now. So how close are we in terms of understanding? So I'm going to,

124
00:11:30,720 --> 00:11:35,840
I have to speak from my own particular experience here. So I run a small research lab here. It's

125
00:11:36,560 --> 00:11:39,680
it's like any other research lab on the sort of the principal investigator, there's actually

126
00:11:39,680 --> 00:11:44,320
two of us and there's a bunch of other people. And this is what we do. We started the neural

127
00:11:44,320 --> 00:11:50,160
cortex, and we published our results and so on. So about three years ago, we had a real

128
00:11:50,160 --> 00:11:53,920
breakthrough in this, in this field. It's a tremendous breakthrough. We started, we've now

129
00:11:53,920 --> 00:12:00,320
published, I think, three papers on it. And so I have, I have a pretty good understanding of all

130
00:12:00,320 --> 00:12:06,800
the pieces and what we're missing. I would say that almost all the empirical data we've collected

131
00:12:06,800 --> 00:12:10,960
about the brain, which is enormous, if you don't know the neuroscience literature, it's just

132
00:12:10,960 --> 00:12:20,960
incredibly big. And it's, for the most part, all correct. It's facts and experimental results and

133
00:12:20,960 --> 00:12:26,720
measurements and all kinds of stuff. But none of that has been really assimilated into a theoretical

134
00:12:26,720 --> 00:12:33,440
framework. It's, it's data without, it's in the, in the language of Thomas Coon, the historian,

135
00:12:33,440 --> 00:12:38,080
it would be a sort of a pre-paradigm science, lots of data, but no way to fit it in together.

136
00:12:38,080 --> 00:12:40,800
I think almost all of that's correct. There's going to be some mistakes in there.

137
00:12:42,080 --> 00:12:47,200
And for the most part, there aren't really good cogent theories about how to put it together.

138
00:12:47,200 --> 00:12:51,120
It's not like we have two or three competing good theories, which ones are right and which ones

139
00:12:51,120 --> 00:12:54,640
are wrong. It's like, yeah, people just like scratching their heads, throwing things, you know,

140
00:12:54,640 --> 00:12:57,520
some people have given up on trying to like figure out what the whole thing does.

141
00:12:57,520 --> 00:13:04,160
In fact, there's very, very few labs that we do that focus really on theory and all this

142
00:13:04,160 --> 00:13:08,880
unassimilated data and trying to explain it. So it's not like we haven't, we've got it wrong.

143
00:13:08,880 --> 00:13:14,160
It's just that we haven't got it at all. So it's really, I would say, pretty early days

144
00:13:15,040 --> 00:13:20,160
in terms of understanding the fundamental theories, forces of the way our mind works.

145
00:13:20,160 --> 00:13:23,520
I don't think so. I would have said that's true five years ago.

146
00:13:25,280 --> 00:13:30,480
So as I said, we had some really big breakthroughs on this recently, and we started publishing papers

147
00:13:30,480 --> 00:13:36,720
on this. So we'll get to that. So I don't think it's, you know, I'm an optimist,

148
00:13:36,720 --> 00:13:40,320
and from where I sit today, most people would disagree with this, but from where I sit today,

149
00:13:40,320 --> 00:13:47,200
from what I know, it's not super early days anymore. The way these things go is it's not a

150
00:13:47,200 --> 00:13:50,720
linear path, right? You don't just start accumulating and get better and better and better.

151
00:13:50,720 --> 00:13:54,320
No, you got all the stuff you've collected. None of it makes sense. All these different

152
00:13:54,320 --> 00:13:57,520
things we just started around. And then you're going to have some breaking points all of a sudden,

153
00:13:57,520 --> 00:14:03,520
oh my God, now we got it right. That's how it goes in science. And I personally feel like we

154
00:14:03,520 --> 00:14:07,440
passed that little thing about a couple of years ago, all that big thing a couple of years ago.

155
00:14:07,440 --> 00:14:12,480
So we can talk about that. Time will tell if I'm right. But I feel very confident about it.

156
00:14:12,560 --> 00:14:18,560
That's the moment to say it on tape like this. At least very optimistic. So let's,

157
00:14:18,560 --> 00:14:25,840
before those few years ago, let's take a step back to HTM, the hierarchical temporal memory theory,

158
00:14:25,840 --> 00:14:29,360
which you first proposed on intelligence and went through a few different generations. Can

159
00:14:29,360 --> 00:14:34,480
you describe what it is, how it evolved through the three generations, since you first put it

160
00:14:34,480 --> 00:14:42,240
on paper? Yeah. So one of the things that neuroscientists just sort of missed for many,

161
00:14:42,240 --> 00:14:47,600
many years, and especially people who are thinking about theory, was the nature of time in the brain.

162
00:14:49,040 --> 00:14:53,520
Brains process information through time. The information coming into the brain is constantly

163
00:14:53,520 --> 00:14:59,280
changing. The patterns from my speech right now, if you're listening to it at normal speed,

164
00:15:00,000 --> 00:15:04,000
would be changing on your ears about every 10 milliseconds or so you'd have it change.

165
00:15:04,000 --> 00:15:08,160
This constant flow, when you look at the world, your eyes are moving constantly,

166
00:15:08,160 --> 00:15:12,480
three to five times a second, and the inputs completely. If I were to touch something like

167
00:15:12,480 --> 00:15:17,600
a coffee cup, as I move my fingers, the inputs change. So this idea that the brain works on

168
00:15:17,600 --> 00:15:22,720
time changing patterns is almost completely or was almost completely missing from a lot of

169
00:15:22,720 --> 00:15:26,480
the basic theories like fears of vision and so on. It's like, oh, no, we're going to put this

170
00:15:26,480 --> 00:15:30,880
image in front of you and flash it and say, what is it? convolutional neural networks work that

171
00:15:30,880 --> 00:15:36,560
way today, right? Classified this picture. But that's not what vision is like. Vision is this

172
00:15:36,560 --> 00:15:41,680
sort of crazy time-based pattern that's going all over the place and so is touch and so is hearing.

173
00:15:41,680 --> 00:15:46,000
So the first part of a hierarchical temporal memory was the temporal part. It's to say,

174
00:15:46,800 --> 00:15:50,320
you won't understand the brain, nor will you understand intelligent machines unless you're

175
00:15:50,320 --> 00:15:55,920
dealing with time-based patterns. The second thing was the memory component of it was, is to say

176
00:15:56,640 --> 00:16:04,160
that we aren't just processing input. We learn a model of the world. And the memory stands for

177
00:16:04,160 --> 00:16:08,080
that model. We have to, the point of the brain, the part of the neocortex, it learns a model of

178
00:16:08,080 --> 00:16:13,760
the world. We have to store things that are experiences in a form that leads to a model of

179
00:16:13,760 --> 00:16:17,280
the world. So we can move around the world. We can pick things up and do things and navigate and

180
00:16:17,280 --> 00:16:20,960
know how it's going on. So that's, that's what the memory referred to. And many people just,

181
00:16:20,960 --> 00:16:26,160
they were thinking about like certain processes without memory at all. They're just like processing

182
00:16:26,160 --> 00:16:32,160
things. And then finally, the hierarchical component was a reflection to that the neocortex,

183
00:16:32,160 --> 00:16:37,520
although it's just a uniform sheet of cells, different parts of it project to other parts,

184
00:16:37,520 --> 00:16:43,200
which project to other parts. And there is a sort of rough hierarchy in terms of that. So

185
00:16:43,840 --> 00:16:47,760
the hierarchical temporal memory is just saying, look, we should be thinking about the brain

186
00:16:47,760 --> 00:16:56,160
as time-based, you know, model memory-based and hierarchical processing. And, and that was a

187
00:16:56,160 --> 00:17:01,680
placeholder for a bunch of components that we would then plug into that. We still believe all

188
00:17:01,680 --> 00:17:07,120
those things I just said, but we now know so much more that I'm stopping to use the word

189
00:17:07,120 --> 00:17:11,440
hierarchical temporal memory yet because it's insufficient to capture the stuff we know. So

190
00:17:11,440 --> 00:17:16,800
again, it's not incorrect, but it's, I now know more and I would rather describe it more accurately.

191
00:17:16,800 --> 00:17:22,880
Yeah. So you're basically, we could think of HTM as emphasizing that there's three aspects

192
00:17:23,520 --> 00:17:27,680
of intelligence that are important to think about whatever the, whatever the eventual theory

193
00:17:27,680 --> 00:17:33,760
converges to. So in terms of time, how do you think of nature of time across different timescales?

194
00:17:33,760 --> 00:17:39,600
So you mentioned things changing, sensory inputs changing every 10, 20 minutes. What about

195
00:17:39,600 --> 00:17:43,840
every few minutes, every few months and years? Well, if you think about a neuroscience

196
00:17:44,320 --> 00:17:51,440
problem, the brain problem, neurons themselves can stay active for certain parts of time.

197
00:17:51,440 --> 00:17:54,160
They can, they're parts of the brain where they stay active for minutes, you know,

198
00:17:54,160 --> 00:18:02,320
so you could hold a certain perception or activity for a certain part of time, but not,

199
00:18:02,320 --> 00:18:08,960
most of them don't last that long. And so if you think about your thoughts or the activity neurons,

200
00:18:08,960 --> 00:18:12,880
if you're going to want to involve something that happened a long time ago, even just this morning,

201
00:18:12,880 --> 00:18:17,600
for example, the neurons haven't been active throughout that time. So you have to store that.

202
00:18:17,600 --> 00:18:22,320
So if I ask you, what did you have for breakfast today? That is memory. That is,

203
00:18:22,320 --> 00:18:25,840
you've built that into your model of the world now, you remember that. And that memory is in the

204
00:18:27,120 --> 00:18:34,640
synapses, it's basically in the formation of synapses. And so it's, you're sliding into what,

205
00:18:34,640 --> 00:18:38,560
you know, used to different timescales. There's timescales of which we are like

206
00:18:38,560 --> 00:18:42,080
understanding my language and moving about and seeing things rapidly and over time. That's the

207
00:18:42,080 --> 00:18:46,640
timescales of activities of neurons. But if you want to get in longer timescales, then it's more

208
00:18:46,640 --> 00:18:50,960
memory. And we have to invoke those memories to say, Oh, yes, well, now I can remember what

209
00:18:50,960 --> 00:18:57,440
I had for breakfast because I stored that someplace. I may forget it tomorrow, but I'd store it for

210
00:18:57,440 --> 00:19:07,040
now. So this memory also need to have the hierarchical aspect of reality is not just about

211
00:19:07,040 --> 00:19:09,600
concepts, it's also about time. Do you think of it that way?

212
00:19:09,600 --> 00:19:14,560
Uh, yeah, time is infused in everything. It's like, you really can't separate it out.

213
00:19:15,440 --> 00:19:20,800
If I ask you, what is the, what is your, you know, how's the brain learn a model of this coffee cup

214
00:19:20,800 --> 00:19:25,440
here? I have a coffee cup and then I met the coffee cup. I said, well, time is not an inherent

215
00:19:25,440 --> 00:19:29,520
property of this, of this, of the model I have with this cup, whether it's a visual model or

216
00:19:29,520 --> 00:19:34,800
tactile model. I can sense it through time, but the model itself doesn't really have much time.

217
00:19:34,800 --> 00:19:38,000
If I asked you, if I said, well, what is the model of my cell phone?

218
00:19:38,880 --> 00:19:43,360
My brain has learned a model of the cell phones. If you have a smartphone like this.

219
00:19:43,360 --> 00:19:47,920
And I said, well, this has time aspects to it. I have expectations when I turn it on,

220
00:19:47,920 --> 00:19:51,200
what's going to happen, what water, how long it's going to take to do certain things.

221
00:19:51,760 --> 00:19:56,400
If I bring up an app, what sequences. And so I have instant, it's like melodies in the world,

222
00:19:56,400 --> 00:20:01,440
you know, melody has a sense of time. So many things in the world move and act, and there's

223
00:20:01,440 --> 00:20:10,160
a sense of time related to them. Some don't, but most things do actually. So it's sort of infused

224
00:20:10,160 --> 00:20:14,560
throughout the models of the world. You build a model of the world, you're learning the structure

225
00:20:14,560 --> 00:20:19,440
of the objects in the world, and you're also learning how those things change through time.

226
00:20:20,640 --> 00:20:26,080
Okay, so it's, it really is just a fourth dimension that's infused deeply. And you have

227
00:20:26,080 --> 00:20:31,120
to make sure that your models of an intelligence incorporated. So

228
00:20:33,040 --> 00:20:36,960
like you mentioned, the state of neuroscience is deeply empirical, a lot of data collection.

229
00:20:37,680 --> 00:20:42,960
It's, you know, that's, that's where it is. You mentioned Thomas Kuhn, right?

230
00:20:42,960 --> 00:20:50,400
Yeah. And then you're proposing a theory of intelligence, and which is really the next step,

231
00:20:50,400 --> 00:20:59,520
the really important step to take. But why, why is HTM or what we'll talk about soon,

232
00:21:00,880 --> 00:21:08,480
the right theory? So is it more in this? Is it backed by intuition? Is it backed by

233
00:21:09,360 --> 00:21:14,960
evidence? Is it backed by a mixture of both? Is it kind of closer to where string theory is in physics,

234
00:21:15,520 --> 00:21:22,240
where there's mathematical components which show that, you know what, it seems that this,

235
00:21:22,880 --> 00:21:28,480
it fits together too well for it not to be true, which is what where string theory is. Is that

236
00:21:28,480 --> 00:21:33,040
where it's a mix of all those things, although definitely where we are right now, it's definitely

237
00:21:33,040 --> 00:21:38,320
much more on the empirical side than let's say string theory. The way this goes about, we're

238
00:21:38,320 --> 00:21:42,320
theorists, right? So we look at all this data and we're trying to come up with some sort of model

239
00:21:42,400 --> 00:21:48,480
that explains it, basically. And there's a, unlike string theory, there's this vast more

240
00:21:48,480 --> 00:21:56,560
amounts of empirical data here that I think that most physicists deal with. And so our challenge

241
00:21:56,560 --> 00:22:03,360
is to sort through that and figure out what kind of constructs would explain this. And when we have

242
00:22:03,360 --> 00:22:09,200
an idea, you come up with a theory of some sort, you have lots of ways of testing it. First of all,

243
00:22:09,920 --> 00:22:16,560
I am, you know, there are 100 years of assimilated, undesimulated empirical data from neuroscience.

244
00:22:16,560 --> 00:22:21,840
So we go back and read papers and we say, oh, did someone find this already? We can predict X,

245
00:22:21,840 --> 00:22:27,680
Y, and Z. And maybe no one's even talked about it since 1972 or something, but we go back and

246
00:22:27,680 --> 00:22:33,440
find that. And we say, oh, either it can support the theory or it can invalidate the theory. And

247
00:22:33,440 --> 00:22:36,880
we say, okay, we have to start over again. Oh, no, it's support. Let's keep going with that one.

248
00:22:37,840 --> 00:22:45,280
So the way I kind of view it, when we do our work, we come up, we look at all this empirical data,

249
00:22:45,280 --> 00:22:48,640
and it's what I call it as a set of constraints. We're not interested in something that's

250
00:22:48,640 --> 00:22:52,720
biologically inspired. We're trying to figure out how the actual brain works. So every piece

251
00:22:52,720 --> 00:22:56,880
of empirical data is a constraint on a theory. In theory, if you have the correct theory,

252
00:22:56,880 --> 00:23:02,960
it needs to explain every pin, right? So we have this huge number of constraints on the problem,

253
00:23:03,040 --> 00:23:07,120
which initially makes it very, very difficult. If you don't have many constraints,

254
00:23:07,120 --> 00:23:10,080
you can make up stuff all the day. You can say, oh, here's an answer. How you can do this,

255
00:23:10,080 --> 00:23:13,680
you can do that, you can do this. But if you consider all biology as a set of constraints,

256
00:23:13,680 --> 00:23:17,200
all neuroscience, a set of constraints, and even if you're working in one little part of

257
00:23:17,200 --> 00:23:21,280
the Neocortex, for example, there are hundreds and hundreds of constraints, these are empirical

258
00:23:21,280 --> 00:23:26,880
constraints, that it's very, very difficult initially to come up with a theoretical framework

259
00:23:26,880 --> 00:23:32,880
for that. But when you do, and it solves all those constraints at once, you have a high confidence

260
00:23:32,880 --> 00:23:38,560
that you got something close to correct. It's just mathematically almost impossible not to be.

261
00:23:39,120 --> 00:23:46,800
So that's the curse and the advantage of what we have. The curse is we have to meet all these

262
00:23:46,800 --> 00:23:53,120
constraints, which is really hard. But when you do meet them, then you have a great confidence

263
00:23:53,120 --> 00:23:58,640
that you've discovered something. In addition, then we work with scientific labs. So we'll say,

264
00:23:58,640 --> 00:24:03,200
oh, there's something we can't find, we can predict something, but we can't find it anywhere in the

265
00:24:03,200 --> 00:24:08,640
literature. So we will then, we have people we collaborated with, we'll say, sometimes they'll

266
00:24:08,640 --> 00:24:12,480
say, you know what, I have some collected data, which I didn't publish. But we can go back and

267
00:24:12,480 --> 00:24:16,880
look at it and see if we can find that, which is much easier than designing a new experiment,

268
00:24:16,880 --> 00:24:21,920
you know, new neuroscience experiments take a long time, years. So although some people

269
00:24:21,920 --> 00:24:28,720
are doing that now too. So, but between all of these things, I think it's a reasonable,

270
00:24:30,080 --> 00:24:34,400
actually a very, very good approach. We are blessed with the fact that we can test our theories

271
00:24:34,960 --> 00:24:39,280
out to Yang Yang here, because there's so much on a similar data. And we can also falsify our

272
00:24:39,280 --> 00:24:44,080
theories very easily, which we do often. So it's kind of reminiscent to whenever, whenever that

273
00:24:44,080 --> 00:24:50,720
was with Copernicus, you know, when you figure out that the sun's at the center of the solar

274
00:24:50,720 --> 00:24:56,240
system as opposed to Earth, the pieces just fall into place. Yeah, I think that's the general

275
00:24:57,200 --> 00:25:03,840
nature of the Ha moments is in Copernicus, it could be, you could say the same thing about Darwin.

276
00:25:05,120 --> 00:25:11,360
You could say the same thing about, you know, about the double helix, that people have been

277
00:25:11,360 --> 00:25:14,480
working on a problem for so long and have all this data and they can't make sense of it,

278
00:25:14,480 --> 00:25:19,280
they can't make sense of it. But when the answer comes to you and everything falls into place,

279
00:25:19,280 --> 00:25:26,400
it's like, oh, my gosh, that's it. That's got to be right. I asked both Jim Watson and Francis

280
00:25:26,400 --> 00:25:33,440
Crick about this. I asked them, you know, when you were working on trying to discover the structure

281
00:25:33,440 --> 00:25:40,720
of the double helix, and when you came up with the sort of the structure that ended up being correct,

282
00:25:42,400 --> 00:25:46,320
but it was sort of a guess, you know, it wasn't really verified yet. I said,

283
00:25:46,320 --> 00:25:51,360
did you know that it was right? And they both said, absolutely. So we absolutely knew it was

284
00:25:51,360 --> 00:25:55,520
right. And it doesn't matter if other people didn't believe it or not, we knew it was right,

285
00:25:55,520 --> 00:26:00,000
they'd get around to thinking it and agree with it eventually anyway. And that's the kind of thing

286
00:26:00,000 --> 00:26:05,760
you hear a lot with scientists who really are studying a difficult problem. And I feel that way

287
00:26:05,760 --> 00:26:11,520
too about our work. Have you talked to Crick or Watson about the problem you're trying to solve,

288
00:26:12,000 --> 00:26:19,840
the, of finding the DNA of the brain? Yeah. In fact, Francis Crick was very interested in this,

289
00:26:19,840 --> 00:26:25,280
in the latter part of his life. And in fact, I got interested in brains by reading an essay he wrote

290
00:26:25,280 --> 00:26:32,000
in 1979 called Thinking About the Brain. And that was when I decided I'm going to leave my

291
00:26:32,000 --> 00:26:36,640
profession of computers and engineering and become a neuroscientist, just reading that one essay from

292
00:26:36,640 --> 00:26:43,600
Francis Crick. I got to meet him later in life. I got to, I spoke at the Salk Institute and he

293
00:26:43,600 --> 00:26:49,600
was in the audience and then I had a tea with him afterwards. You know, he was interested in a

294
00:26:49,600 --> 00:26:57,520
different problem. He was focused on consciousness. Oh, the easy problem, right? Well, I think it's

295
00:26:57,520 --> 00:27:04,240
the red herring. And so we weren't really overlapping a lot there. Jim Watson, who's still alive,

296
00:27:05,200 --> 00:27:10,080
is also interested in this problem. And he was, when he was director of the Coltsman Harbor

297
00:27:10,080 --> 00:27:15,840
laboratories, he was really sort of behind moving in the direction of neuroscience there.

298
00:27:16,480 --> 00:27:21,920
And so he had a personal interest in this field. And I have met with him numerous times.

299
00:27:23,520 --> 00:27:29,200
And in fact, the last time was about a little bit over a year ago, I gave a talk at Coltsman

300
00:27:29,280 --> 00:27:37,920
Harbor Labs about the progress we were making in our work. And it was a lot of fun because

301
00:27:39,360 --> 00:27:42,240
he said, well, you wouldn't be coming here unless you had something important to say,

302
00:27:42,240 --> 00:27:48,960
so I'm going to go attend your talk. So he sat in the very front row. Next to him was the director

303
00:27:48,960 --> 00:27:52,480
of the lab, Bruce Stillman. So these guys are in the front row of this auditorium, right? So

304
00:27:52,480 --> 00:27:55,600
nobody else in the auditorium wants to sit in the front row because there's Jim Watson and there's

305
00:27:55,600 --> 00:28:05,040
the director. And I gave a talk and then I had dinner with Jim afterwards. But there's a great

306
00:28:05,040 --> 00:28:09,840
picture of my colleague, Subitai Amantik, where I'm up there sort of explaining the basics of

307
00:28:09,840 --> 00:28:14,560
this new framework we have. And Jim Watson is on the edge of his chair. He's literally on the edge

308
00:28:14,560 --> 00:28:20,480
of his chair, like intently staring up at the screen. And when he discovered the structure of

309
00:28:20,480 --> 00:28:26,560
DNA, the first public talk he gave was at Coltsman Harbor Labs. And there's a picture,

310
00:28:26,560 --> 00:28:30,720
there's a famous picture of Jim Watson standing at the whiteboard with a overrated thing pointing

311
00:28:30,720 --> 00:28:34,400
at something, holding at the double helix with his pointer. And it actually looks a lot like the

312
00:28:34,400 --> 00:28:37,760
picture of me. So there was a sort of funny, there's an area talking about the brain and there's Jim

313
00:28:37,760 --> 00:28:41,520
Watson staring up intently at it. And of course, there was, you know, whatever, 60 years earlier,

314
00:28:41,520 --> 00:28:46,000
he was standing, you know, pointing at the double helix. And it's one of the great discoveries in

315
00:28:46,000 --> 00:28:52,320
all of, you know, whatever, by all the science, all science DNA. So it's just the funny that

316
00:28:52,320 --> 00:28:57,280
there's echoes of that in your presentation. Do you think in terms of evolutionary timeline and

317
00:28:57,280 --> 00:29:07,360
history, the development of the neocortex was a big leap? Or is it just a small step? So like,

318
00:29:07,360 --> 00:29:12,720
if we ran the whole thing over again, from the from the birth of human of life on earth, how

319
00:29:12,720 --> 00:29:16,320
likely would we develop the mechanism of the neocortex? Okay, well, those are two separate

320
00:29:16,320 --> 00:29:22,240
questions. One is, was it a big leap? And one was how likely it is. Okay, they're not necessarily

321
00:29:22,240 --> 00:29:27,200
related. Maybe correlated. And we don't really have enough data to make a judgment about that.

322
00:29:28,000 --> 00:29:32,560
I would say definitely was a big leap. And I can tell you why I think I don't think it was just

323
00:29:32,560 --> 00:29:38,320
another incremental step. I'll get that moment. I don't really have any idea how likely it is.

324
00:29:38,320 --> 00:29:44,080
If we look at evolution, we have one data point, which is earth, right? Life formed on earth billions

325
00:29:44,080 --> 00:29:48,880
of years ago, whether it was introduced here, or it created here, or someone introduced it,

326
00:29:48,880 --> 00:29:54,160
we don't really know, but it was here early. It took a long, long time to get to multicellular life.

327
00:29:55,040 --> 00:30:02,640
And then from multicellular life, it took a long, long time to get the neocortex. And we've only

328
00:30:02,640 --> 00:30:09,040
had the neocortex for a few hundred thousand years. So that's like nothing. Okay, so is it

329
00:30:09,040 --> 00:30:13,760
likely? Well, certainly isn't something that happened right away on earth. And there were

330
00:30:13,760 --> 00:30:17,520
multiple steps to get there. So I would say it's probably not going to something that would happen

331
00:30:17,520 --> 00:30:22,400
instantaneously on other planets that might have life. It might take several billion years on average.

332
00:30:23,040 --> 00:30:26,880
Is it likely? I don't know, but you'd have to survive for several billion years to find out.

333
00:30:27,840 --> 00:30:36,560
Probably. Is it a big leap? Yeah, I think it is a qualitative difference in all other evolutionary

334
00:30:36,560 --> 00:30:42,800
steps. I can try to describe that if you'd like. Sure. In which way? Yeah, I can tell you how.

335
00:30:43,760 --> 00:30:49,600
Pretty much, let's start with a little preface. Many of the things that humans are able to do

336
00:30:50,320 --> 00:31:00,400
do not have obvious survival advantages precedent. We could create music. Is there

337
00:31:00,400 --> 00:31:05,280
a really survival advantage to that? Maybe, maybe not. What about mathematics? Is there a real

338
00:31:05,280 --> 00:31:10,480
survival advantage to mathematics? You can stretch it. You can try to figure these things out.

339
00:31:12,080 --> 00:31:17,920
But mostly evolutionary history, everything had immediate survival advantages to write.

340
00:31:18,640 --> 00:31:26,240
So I'll tell you a story, which I like. It may not be true. But the story goes as follows.

341
00:31:29,040 --> 00:31:32,320
Organisms have been evolving since the beginning of life here on Earth,

342
00:31:33,680 --> 00:31:37,680
adding this sort of complexity onto that and this sort of complexity onto that. And the brain itself

343
00:31:38,240 --> 00:31:43,360
is evolved this way. In fact, there's an old part, an older part, an older, older part to the

344
00:31:43,360 --> 00:31:47,360
brain that kind of just keeps calming on new things and we keep adding capabilities. And we

345
00:31:47,360 --> 00:31:53,840
got to the neocortex. Initially, it had a very clear survival advantage in that it produced better

346
00:31:53,840 --> 00:31:59,200
vision and better hearing and better touch and maybe, you know, say, so on. But what I think

347
00:31:59,200 --> 00:32:05,440
happens is that evolution took a mechanism, and this is in our recent theory, but it took a

348
00:32:05,440 --> 00:32:10,320
mechanism that evolved a long time ago for navigating in the world, for knowing where you are.

349
00:32:10,320 --> 00:32:14,320
These are the so-called grid cells and place cells of an old part of the brain.

350
00:32:15,120 --> 00:32:22,160
And it took that mechanism for building maps of the world and knowing where you are on those

351
00:32:22,160 --> 00:32:27,920
maps and how to navigate those maps and turns it into a sort of a slimmed down, idealized version

352
00:32:27,920 --> 00:32:33,120
of it. And that idealized version could now apply to building maps of other things, maps of

353
00:32:33,680 --> 00:32:39,200
coffee cups and maps of phones, maps of, you know, concepts, concepts, yes, and not just almost,

354
00:32:39,200 --> 00:32:44,400
exactly. And so you, and it just started replicating this stuff, right? You just think

355
00:32:44,400 --> 00:32:50,480
more and more and more. So we went from being sort of dedicated purpose neural hardware to solve

356
00:32:50,480 --> 00:32:56,000
certain problems that are important to survival to a general purpose neural hardware that could

357
00:32:56,000 --> 00:33:03,360
be applied to all problems. And now it's escaped the orbit of survival. It's, we are now able to

358
00:33:03,360 --> 00:33:12,240
apply it to things which we find enjoyment, you know, but aren't really clearly survival

359
00:33:12,240 --> 00:33:18,000
characteristics. And that it seems to only have happened in humans to the large extent.

360
00:33:19,120 --> 00:33:24,960
And so that's what's going on where we sort of have, we've sort of escaped the gravity of

361
00:33:24,960 --> 00:33:30,960
evolutionary pressure in some sense in the New York cortex. And it now does things which

362
00:33:31,520 --> 00:33:36,000
that are really interesting, discovering models of the universe, which may not really help us,

363
00:33:36,000 --> 00:33:40,800
doesn't matter. How does it help us surviving knowing that there might be multiverses or

364
00:33:40,800 --> 00:33:45,440
that there might be, you know, the age of the universe or how do, you know, various stellar

365
00:33:45,440 --> 00:33:50,320
things occur? It doesn't really help us survive at all. But we enjoy it. And that's what happened.

366
00:33:50,320 --> 00:33:57,200
Or at least not in the obvious way, perhaps it is required. If you look at the entire

367
00:33:57,200 --> 00:34:01,280
universe in an evolutionary way, it's required for us to do interplanetary travel and therefore

368
00:34:01,280 --> 00:34:04,320
survive past our own fun. But you know, let's not get too quick.

369
00:34:04,320 --> 00:34:10,000
Yeah, but you know, evolution works at one time frame and survival, if you think of survival

370
00:34:10,000 --> 00:34:15,600
of the phenotype, survival of the individual, what you're talking about there is spans well

371
00:34:15,600 --> 00:34:22,400
beyond that. So there's no genetic, I'm not transferring any genetic traits to my children

372
00:34:23,280 --> 00:34:25,760
that are going to help them survive better on Mars.

373
00:34:25,760 --> 00:34:31,200
Right. Totally different mechanism. So let's get into the new, as you've mentioned,

374
00:34:31,200 --> 00:34:35,280
this idea, I don't know if you have a nice name, 1000.

375
00:34:35,280 --> 00:34:37,200
I would call it the thousand brain theory of intelligence.

376
00:34:37,200 --> 00:34:44,000
I like it. So can you talk about the this idea of spatial view of concepts and so on?

377
00:34:44,000 --> 00:34:48,480
Yeah. So can I just describe sort of the there's an underlying core discovery,

378
00:34:49,200 --> 00:34:52,320
which then everything comes from that that's a very simple.

379
00:34:53,120 --> 00:34:59,520
This is really what happened. We were deep into problems about understanding how we build models

380
00:34:59,520 --> 00:35:05,280
of stuff in the world and how we make predictions about things. And I was holding a coffee cup just

381
00:35:05,280 --> 00:35:10,960
like this in my hand. And I had my finger was touching the side, my index finger. And then I

382
00:35:10,960 --> 00:35:16,560
moved it to the top. And I was going to feel the rim at the top of the cup. And I asked myself

383
00:35:16,560 --> 00:35:21,280
a very simple question. I said, well, first of all, let's say I know that my brain predicts

384
00:35:21,280 --> 00:35:25,040
what it's going to feel before it touches it. You can just think about it and imagine it.

385
00:35:25,920 --> 00:35:29,200
And so we know that the brain's making predictions all the time. So the question is,

386
00:35:29,200 --> 00:35:32,880
what does it take to predict that? Right. And there's a very interesting answer.

387
00:35:33,440 --> 00:35:36,720
First of all, it says the brain has to know it's touching a coffee cup. It has to have

388
00:35:36,720 --> 00:35:42,320
a model of a coffee cup and needs to know where the finger currently is on the cup relative to

389
00:35:42,320 --> 00:35:46,320
the cup. Because when I make a movement, it needs to know where it's going to be on the cup

390
00:35:46,320 --> 00:35:52,000
after the movement is completed relative to the cup. And then it can make a prediction about

391
00:35:52,000 --> 00:35:56,320
what it's going to sense. So this told me that the neocortex, which is making this prediction,

392
00:35:56,320 --> 00:36:00,960
needs to know that it's sensing it's touching a cup. And it needs to know the location of

393
00:36:00,960 --> 00:36:05,040
my finger relative to that cup in a reference frame of the cup. It doesn't matter where the

394
00:36:05,040 --> 00:36:09,200
cup is relative to my body. It doesn't matter its orientation. None of that matters. It's

395
00:36:09,200 --> 00:36:13,040
where my finger is relative to the cup, which tells me then that the neocortex

396
00:36:13,600 --> 00:36:18,400
has a reference frame that's anchored to the cup. Because otherwise, I wouldn't be able to

397
00:36:18,400 --> 00:36:22,480
say the location and I wouldn't be able to predict my new location. And then we quickly,

398
00:36:22,480 --> 00:36:26,560
very instantly, you can say, well, every part of my skin could touch this cup. And therefore,

399
00:36:26,560 --> 00:36:29,760
every part of my skin is making predictions and every part of my skin must have a reference frame

400
00:36:30,800 --> 00:36:38,240
that it's using to make predictions. So the big idea is that throughout the neocortex,

401
00:36:38,320 --> 00:36:47,520
everything is being stored and referenced in reference frames. You can think of them like

402
00:36:47,520 --> 00:36:51,760
XYZ reference frames, but they're not like that. We know a lot about the neural mechanisms for

403
00:36:51,760 --> 00:36:56,640
this. But the brain thinks in reference frames. And as an engineer, if you're an engineer,

404
00:36:56,640 --> 00:37:01,200
this is not surprising. You'd say, if I were to build a CAD model of the coffee cup, well,

405
00:37:01,200 --> 00:37:04,800
I would bring it up in some CAD software and I would assign some reference frame and say,

406
00:37:04,880 --> 00:37:09,920
this features at this location and so on. But the idea that this is occurring throughout

407
00:37:09,920 --> 00:37:18,640
the neocortex everywhere, it was a novel idea. And then a zillion things fell into place after

408
00:37:18,640 --> 00:37:22,960
that. A zillion. So now we think about the neocortex as processing information quite

409
00:37:22,960 --> 00:37:26,480
differently than we used to do it. We used to think about the neocortex as processing

410
00:37:26,480 --> 00:37:30,880
sensory data and extracting features from that sensory data and then extracting features from

411
00:37:30,880 --> 00:37:35,680
the features, very much like a deep learning network does today. But that's not how the brain

412
00:37:35,680 --> 00:37:41,360
works at all. The brain works by assigning everything, every input, everything to reference

413
00:37:41,360 --> 00:37:46,160
frames. And there are thousands, hundreds of thousands of them active at once in your neocortex.

414
00:37:47,520 --> 00:37:51,600
It's a surprising thing to think about. But once you sort of internalize this, you understand

415
00:37:51,600 --> 00:37:59,680
that it explains almost all the mysteries we've had about the structure. So one of the consequences

416
00:38:00,240 --> 00:38:05,600
of that is that every small part of the neocortex, say a millimeter square and there's 150,000 of

417
00:38:05,600 --> 00:38:10,240
those. So it's about 150,000 square millimeters. If you take every little square millimeter of the

418
00:38:10,240 --> 00:38:15,120
cortex, it's got some input coming into it and it's going to have reference frames where it's

419
00:38:15,120 --> 00:38:21,440
assigning that input to and each square millimeter can learn complete models of objects. So what do

420
00:38:21,440 --> 00:38:25,840
I mean by that? If I'm touching the coffee cup, well, if I just touch it in one place, I can't

421
00:38:25,840 --> 00:38:30,320
learn what this coffee cup is because I'm just feeling one part. But if I move it around the cup

422
00:38:30,880 --> 00:38:34,800
and touch it in different areas, I can build up a complete model of the cup because I'm now

423
00:38:34,800 --> 00:38:38,560
filling in that three dimensional map, which is the coffee cup, I can say, oh, what am I feeling

424
00:38:38,560 --> 00:38:41,760
at all these different locations? That's the basic idea. It's more complicated than that.

425
00:38:42,880 --> 00:38:48,000
But so through time, and we talked about time earlier, through time, even a single column,

426
00:38:48,000 --> 00:38:51,520
which is only looking at or a single part of the cortex, it's only looking at a small part of the

427
00:38:51,520 --> 00:38:56,960
world can build up a complete model of an object. And so if you think about the part of the brain,

428
00:38:56,960 --> 00:39:01,440
which is getting input from all my fingers, so there's spread across the top of your head here,

429
00:39:01,440 --> 00:39:06,160
this is the somatosensory cortex, there's columns associated with all the different areas of my

430
00:39:06,160 --> 00:39:12,720
skin. And what we believe is happening is that all of them are building models of this cup,

431
00:39:12,720 --> 00:39:17,680
every one of them, or things, not all building all, not every column or every part of the

432
00:39:17,680 --> 00:39:23,760
cortex builds models of everything. But they're all building models of something. And so you have,

433
00:39:23,760 --> 00:39:29,040
so when I touch this cup with my hand, there are multiple models of the cup being invoked.

434
00:39:29,040 --> 00:39:33,280
If I look at it with my eyes, there are again many models of the cup being invoked because each part

435
00:39:33,280 --> 00:39:39,360
of the visual system, the brain doesn't process an image, that's a misleading idea. It's just like

436
00:39:39,360 --> 00:39:42,320
your fingers touching the cup, so different parts of my retina are looking at different parts of the

437
00:39:42,320 --> 00:39:48,160
cup. And thousands and thousands of models of the cup are being invoked at once. And they're all

438
00:39:48,160 --> 00:39:50,960
voting with each other trying to figure out what's going on. So that's why we call it the

439
00:39:50,960 --> 00:39:55,520
Thousand Brains Theory of Intelligence because there isn't one model of a cup. There are thousands

440
00:39:55,520 --> 00:39:59,200
of models of this cup. There are thousands of models of your cell phone and about cameras and

441
00:39:59,200 --> 00:40:03,680
microphones and so on. It's a distributed modeling system, which is very different than

442
00:40:03,680 --> 00:40:08,160
what people have thought about it. So that's a really compelling and interesting idea. I have two

443
00:40:08,160 --> 00:40:12,560
first questions. So one, on the ensemble part of everything coming together, you have these

444
00:40:12,560 --> 00:40:18,800
Thousand Brains, how do you know which one has done the best job of forming the cup?

445
00:40:18,800 --> 00:40:23,840
Great question. Let me try to explain. There's a problem that's known in neuroscience called the

446
00:40:23,840 --> 00:40:29,280
sensor fusion problem. And so the idea is something like, oh, the image comes from the eye. There's

447
00:40:29,280 --> 00:40:34,480
a picture on the retina and it gets projected to the neocortex. Oh, by now it's all sped out all

448
00:40:34,480 --> 00:40:38,960
over the place and it's kind of squirrely and distorted and pieces are all over the, you know,

449
00:40:38,960 --> 00:40:43,600
it doesn't look like a picture anymore. When does it all come back together again? Right?

450
00:40:43,600 --> 00:40:48,560
Or you might say, well, yes, but I also, I also have sounds or touches associated with the cup.

451
00:40:48,560 --> 00:40:52,560
So I'm seeing the cup and touching the cup. How do they get combined together again?

452
00:40:52,560 --> 00:40:56,480
So this is called the sensor fusion problem as if all these disparate parts have to be brought

453
00:40:56,480 --> 00:41:02,400
together into one model someplace. That's the wrong idea. The right idea is that you get all

454
00:41:02,400 --> 00:41:06,560
these guys voting. There's auditory models of the cup, there's visual models of the cup,

455
00:41:06,560 --> 00:41:11,840
there's tactile models of the cup. In the vision system, there might be ones that are more focused

456
00:41:11,840 --> 00:41:15,360
on black and white, one's version on color. It doesn't really matter. There's just thousands and

457
00:41:15,360 --> 00:41:20,560
thousands of models of this cup and they vote. They don't actually come together in one spot.

458
00:41:20,560 --> 00:41:25,360
Just literally think of it this way. Imagine you have each column about the size of a little

459
00:41:25,360 --> 00:41:29,840
piece of spaghetti, okay? Like a two and a half millimeters tall and about a millimeter in white.

460
00:41:29,840 --> 00:41:34,480
They're not physical like, but you can think of them that way. And each one's trying to guess

461
00:41:34,480 --> 00:41:38,400
what this thing is we're touching. Now they can, they can do a pretty good job if they're allowed

462
00:41:38,400 --> 00:41:43,360
to move over time. So I can reach my hand into a black box and move my finger around an object

463
00:41:43,360 --> 00:41:48,160
and if I touch enough space, it's like, okay, I know what it is. But often we don't do that.

464
00:41:48,160 --> 00:41:51,520
Often I can just reach and grab something with my hand all at once and I get it. Or

465
00:41:51,520 --> 00:41:55,760
if I had to look through the world through a straw, so I'm only invoking one little column,

466
00:41:55,760 --> 00:41:58,880
I can only see part of something because I have to move the straw around. But if I open my eyes

467
00:41:58,880 --> 00:42:03,120
to see the whole thing at once. So what we think is going on is all these little pieces of spaghetti

468
00:42:03,120 --> 00:42:07,200
if you have all these little columns in the cortex or all trying to guess what it is that they're

469
00:42:07,200 --> 00:42:12,320
sensing. They'll do a better guess if they have time and can move over time. So if I move my eyes

470
00:42:12,320 --> 00:42:17,760
or move my fingers, but if they don't, they have a, they have a poor guess. It's a, it's a probabilistic

471
00:42:17,760 --> 00:42:22,000
guess of what they might be touching. Now imagine they can post their probability

472
00:42:22,880 --> 00:42:26,400
at the top of little piece of spaghetti, each one of them says, I think, and it's not really a

473
00:42:26,400 --> 00:42:30,800
probability distribution. It's more like a set of possibilities in the brain. It doesn't work

474
00:42:30,800 --> 00:42:35,360
as a probability distribution. It works as more like what we call a union. You could say, and one

475
00:42:35,360 --> 00:42:40,800
column says, I think it could be a coffee cup soda can or a water bottle. And another column says,

476
00:42:40,800 --> 00:42:47,840
I think it could be a coffee cup or, you know, telephone or camera or whatever. Right. And all

477
00:42:47,840 --> 00:42:51,680
these guys are saying what they think it might be. And there's these long range connections in

478
00:42:51,760 --> 00:42:57,600
certain layers in the cortex. So there's some layers in the, some cells types in each column

479
00:42:57,600 --> 00:43:02,720
send the projections across the brain. And that's the voting occurs. And so there's a simple

480
00:43:02,720 --> 00:43:07,040
associative memory mechanism. We've, we've described this in a recent paper and we've modeled this

481
00:43:08,560 --> 00:43:14,160
that says they can all quickly settle on the only or the one best answer for all of them.

482
00:43:14,800 --> 00:43:19,040
If there is a single best answer, they all vote and say, yep, it's got to be the coffee cup.

483
00:43:19,040 --> 00:43:22,800
And at that point, they all know it's a coffee cup. And at that point, everyone acts as if it's

484
00:43:22,800 --> 00:43:25,920
the coffee cup. They know it's a coffee, even though I've only seen one little piece of this

485
00:43:25,920 --> 00:43:30,080
world. I know it's a coffee cup I'm touching or I'm seeing or whatever. And so you can think of

486
00:43:30,080 --> 00:43:33,920
all these columns are looking at different parts and different places, different sensory input,

487
00:43:33,920 --> 00:43:37,840
different locations. They're all different. But this layer that's doing the voting,

488
00:43:39,040 --> 00:43:43,280
that's, it solidifies. It's just like it crystallizes and says, oh, we all know what we're

489
00:43:43,280 --> 00:43:47,520
doing. And so you don't bring these models together in one model, you just vote and there's a

490
00:43:47,520 --> 00:43:52,480
crystallization of the vote. Great. That's at least a compelling way to think about

491
00:43:54,240 --> 00:44:02,000
about the way you form a model of the world. Now, you talk about a coffee cup. Do you see this

492
00:44:02,000 --> 00:44:06,160
as far as I understand that you were proposing this as well, that this extends to much more than

493
00:44:06,160 --> 00:44:14,080
coffee cups? Yeah, it does. Or at least the physical world that expands to the world of concepts.

494
00:44:14,080 --> 00:44:19,040
Yeah, it does. And well, the first, the primary phase of evidence for that is that

495
00:44:19,040 --> 00:44:23,760
the regions of the neocortex that are associated with language or high-level thought or mathematics

496
00:44:23,760 --> 00:44:27,520
or things like that, they look like the regions of the neocortex that process vision and hearing

497
00:44:27,520 --> 00:44:34,080
and touch. They don't look any different or they look only marginally different. And so one would

498
00:44:34,080 --> 00:44:39,280
say, well, if Vernon Mountcastle, who proposed that all the parts of the neocortex do the same

499
00:44:39,280 --> 00:44:43,840
thing, if he's right, then the parts that are doing language or mathematics or physics

500
00:44:44,480 --> 00:44:47,600
are working on the same principle. They must be working on the principle of reference frames.

501
00:44:48,480 --> 00:44:55,040
So that's a little odd thought. But of course, we had no prior idea how these things happen,

502
00:44:55,040 --> 00:45:01,840
so let's go with that. And in our recent paper, we talked a little bit about that. I've been

503
00:45:01,840 --> 00:45:06,960
working on it more since. I have better ideas about it now. I'm sitting here very confident

504
00:45:07,040 --> 00:45:10,240
that that's what's happening. And I can give you some examples to help you think about that.

505
00:45:11,120 --> 00:45:14,240
It's not that we understand it completely, but I understand it better than I've described it

506
00:45:14,240 --> 00:45:22,800
in any paper so far. But we did put that idea out there. It's a good place to start. And the

507
00:45:22,800 --> 00:45:26,720
evidence would suggest it's how it's happening. And then we can start tackling that problem one

508
00:45:26,720 --> 00:45:30,080
piece at a time. What does it mean to do high-level thought? What does it mean to do language? How

509
00:45:30,080 --> 00:45:37,040
would that fit into a reference framework? I don't know if you could tell me if there's a

510
00:45:37,040 --> 00:45:42,320
connection, but there's an app called Anki that helps you remember different concepts.

511
00:45:42,320 --> 00:45:47,920
And they talk about a memory palace that helps you remember completely random concepts by

512
00:45:48,880 --> 00:45:52,320
trying to put them in a physical space in your mind and putting them next to each other.

513
00:45:52,320 --> 00:45:56,560
It's called the method of loci. For some reason, that seems to work really well.

514
00:45:57,520 --> 00:46:00,480
Now that's a very narrow kind of application of just remembering some facts.

515
00:46:00,480 --> 00:46:03,040
But that's a very, very telling one.

516
00:46:03,040 --> 00:46:09,040
Yes, exactly. So this seems like you're describing a mechanism why this seems to work.

517
00:46:09,040 --> 00:46:13,680
Yeah. So basically the way what we think is going on is all things you know,

518
00:46:13,680 --> 00:46:19,600
all concepts, all ideas, words, everything, you know, are stored in reference frames.

519
00:46:20,400 --> 00:46:26,480
And so if you want to remember something, you have to basically navigate through a reference

520
00:46:26,480 --> 00:46:30,320
frame the same way a rat navigates to a maven, the same way my finger rat navigates to this

521
00:46:30,320 --> 00:46:36,000
coffee cup. You are moving through some space. And so if you have a random list of things you

522
00:46:36,000 --> 00:46:40,480
would ask to remember, by assigning them to a reference frame, you've already know very well

523
00:46:40,480 --> 00:46:45,040
to see your house, right? And the idea of the method of loci is you can say, okay, in my lobby,

524
00:46:45,040 --> 00:46:48,160
I'm going to put this thing. And then the bedroom, I put this one. I go down the hall,

525
00:46:48,160 --> 00:46:52,240
I put this thing. And then you want to recall those facts or recall those things. You just walk

526
00:46:52,240 --> 00:46:56,560
mentally, you walk through your house. You're mentally moving through a reference frame

527
00:46:56,560 --> 00:47:00,640
that you already had. And that tells you there's two things that are really important about that.

528
00:47:00,640 --> 00:47:05,520
It tells us the brain prefers to store things in reference frames. And that the method of

529
00:47:05,520 --> 00:47:11,520
recalling things or thinking, if you will, is to move mentally through those reference frames.

530
00:47:11,520 --> 00:47:14,720
You could move physically through some reference frames, like I could physically move through

531
00:47:14,720 --> 00:47:17,840
the reference frame of this coffee cup. I can also mentally move through the reference frame

532
00:47:17,840 --> 00:47:23,920
of the coffee cup, imagining me touching it. But I can also mentally move my house. And so now we

533
00:47:23,920 --> 00:47:30,000
can ask yourself, are all concepts stored this way? There was some recent research using human

534
00:47:30,000 --> 00:47:34,960
subjects in fMRI. And I'm going to apologize for not knowing the name of the scientists who did

535
00:47:34,960 --> 00:47:41,840
this. But what they did is they put humans in this fMRI machine, which is one of these imaging

536
00:47:41,840 --> 00:47:47,680
machines. And they gave the humans tasks to think about birds. So they had different types of birds

537
00:47:47,680 --> 00:47:52,560
and birds that looked big and small and long necks and long legs, things like that. And what they

538
00:47:52,560 --> 00:47:59,040
could tell from the fMRI was a very clever experiment. Get to tell when humans were thinking

539
00:47:59,040 --> 00:48:05,520
about the birds, that the birds, the knowledge of birds was arranged in a reference frame,

540
00:48:05,520 --> 00:48:10,240
similar to the ones that are used when you navigate in a room. These are called grid cells.

541
00:48:10,240 --> 00:48:14,320
And there are grid cell-like patterns of activity in the neocortex when they do this.

542
00:48:15,280 --> 00:48:20,880
So it's a very clever experiment. And what it basically says is that even when you're thinking

543
00:48:20,880 --> 00:48:24,560
about something abstract, and you're not really thinking about it as a reference frame,

544
00:48:24,560 --> 00:48:28,160
it tells us the brain is actually using a reference frame. And it's using the same neural

545
00:48:28,160 --> 00:48:32,800
mechanisms. These grid cells are the basic same neural mechanisms that we propose that grid cells,

546
00:48:32,800 --> 00:48:37,280
which exist in the old part of the brain, the entirionic cortex, that that mechanism

547
00:48:37,280 --> 00:48:42,560
is now similar mechanism is used throughout the neocortex. It's the same nature of preserve this

548
00:48:42,560 --> 00:48:47,680
interesting way of creating reference frames. And so now they have empirical evidence that

549
00:48:47,680 --> 00:48:52,160
when you think about concepts like birds, that you're using reference frames that are built on

550
00:48:52,160 --> 00:48:56,880
grid cells. So that's similar to the method of loci, but in this case, the birds are related so

551
00:48:56,880 --> 00:49:00,320
that makes they create their own reference frame, which is consistent with bird space.

552
00:49:01,040 --> 00:49:04,720
And when you think about something, you go through that, you can make the same example.

553
00:49:04,720 --> 00:49:09,920
Let's take a math mathematics. Let's say you want to prove a conjecture. What is a conjecture?

554
00:49:09,920 --> 00:49:16,080
Conjecture is a statement you believe to be true, but you haven't proven it. And so it might be an

555
00:49:16,080 --> 00:49:21,120
equation. I want to show that this is equal to that. And you have some places you start with,

556
00:49:21,120 --> 00:49:25,040
you say, well, I know this is true, and I know this is true. And I think that maybe to get to

557
00:49:25,040 --> 00:49:29,840
the final proof, I need to go through some intermediate results. But I believe it's happening

558
00:49:30,640 --> 00:49:36,960
is literally these equations or these points are assigned to a reference frame, a mathematical

559
00:49:36,960 --> 00:49:41,200
reference frame. And when you do mathematical operations, a simple one might be multiply or

560
00:49:41,200 --> 00:49:45,280
divide, but you might be able to transform or something else, that is like a movement in the

561
00:49:45,280 --> 00:49:51,680
reference frame of the math. And so you're literally trying to discover a path from one location to

562
00:49:51,680 --> 00:49:58,160
another location in a space of mathematics. And if you can get to these intermediate results,

563
00:49:58,160 --> 00:50:02,000
then you know your map is pretty good. And you know you're using the right operations.

564
00:50:02,960 --> 00:50:08,000
Much of what we think about is solving hard problems, is designing the correct reference

565
00:50:08,000 --> 00:50:12,400
frame for that problem, figuring out how to organize the information, and what behaviors

566
00:50:12,400 --> 00:50:18,880
I want to use in that space to get me there. Yeah, so if you dig in an idea of this reference

567
00:50:18,880 --> 00:50:23,920
frame, whether it's the math, you start a set of axioms to try to get to proving the conjecture.

568
00:50:24,640 --> 00:50:29,840
Can you try to describe, maybe take a step back, how you think of the reference frame in that

569
00:50:29,840 --> 00:50:37,120
context? Is it the reference frame that the axioms are happy in? Is it the reference frame

570
00:50:37,120 --> 00:50:43,040
that might contain everything? Is it a changing thing? So you have many, many reference frames.

571
00:50:43,040 --> 00:50:46,480
I mean, in fact, the way the theory, the 1000 brain theory of intelligence says that every

572
00:50:46,480 --> 00:50:50,480
single thing in the world has its own reference frame. So every word has its own reference

573
00:50:50,480 --> 00:50:55,680
frames. And we can talk about this, the mathematics work out, this is no problem for neurons to do

574
00:50:55,680 --> 00:50:59,920
this. But how many reference frames does the coffee cup have? Well, it's on a table. Remember,

575
00:51:00,720 --> 00:51:06,880
let's say you ask how many reference frames could the column in my finger that's touching the coffee

576
00:51:06,880 --> 00:51:10,800
cup have? Because there are many, many copy, there are many, many models of coffee cups. So

577
00:51:10,800 --> 00:51:14,320
the coffee, there is no one model of coffee cup, there are many models of coffee cup. And you

578
00:51:14,320 --> 00:51:18,560
could say, well, how many different things can my finger learn? Is this is the question you want

579
00:51:18,560 --> 00:51:23,520
to ask? Imagine, I say every concept, every idea, everything you've ever know about that you can

580
00:51:23,520 --> 00:51:29,200
say, I know that thing, it has a reference frame associated with it. And what we do when we build

581
00:51:29,200 --> 00:51:34,320
composite objects, we can we assign reference frames to points, another reference frame. So

582
00:51:34,320 --> 00:51:39,440
my coffee cup has multiple components to it. It's got a limb, it's got a cylinder, it's got a handle.

583
00:51:40,640 --> 00:51:44,720
And those things that have their own reference frames, and they're assigned to a master reference

584
00:51:44,720 --> 00:51:48,640
frame, which is called this cup. And now I have this mental logo on it. Well, that's something

585
00:51:48,640 --> 00:51:52,400
that exists elsewhere in the world. It's its own thing. So it has its own reference frame. So we

586
00:51:52,400 --> 00:51:58,640
now have to say, how can I assign the mental logo reference frame onto the cylinder or onto the coffee

587
00:51:58,640 --> 00:52:05,520
cup? So it's all, we talked about this in the paper that came out in December of this last year.

588
00:52:06,800 --> 00:52:10,480
The idea of how you can assign reference frames to reference frames, how neurons could do this.

589
00:52:10,480 --> 00:52:15,600
So my question is, even though you mentioned reference frames a lot, I almost feel it's really

590
00:52:15,600 --> 00:52:21,440
useful to dig into how you think of what a reference frame is. I mean, it was already helpful for me

591
00:52:21,440 --> 00:52:27,760
to understand that you think of reference frames as something there is a lot of. Okay, so let's just

592
00:52:27,760 --> 00:52:32,320
say that we're going to have some neurons in the brain, not many actually, 10,000, 20,000 are going

593
00:52:32,320 --> 00:52:36,160
to create a whole bunch of reference frames. What does it mean? What is a reference frame?

594
00:52:37,120 --> 00:52:41,120
First of all, these reference frames are different than the ones you might have

595
00:52:41,120 --> 00:52:45,520
be used to. We know lots of reference things. For example, we know the Cartesian coordinates,

596
00:52:45,520 --> 00:52:50,800
x, y, z, that's a type of reference frame. We know longitude and latitude, that's a different type

597
00:52:50,800 --> 00:52:59,040
of reference frame. If I look at a printed map, you might have columns a through m and rows,

598
00:52:59,040 --> 00:53:02,640
you know, one through 20, that's a different type of reference frame. It's kind of a Cartesian

599
00:53:02,720 --> 00:53:07,840
reference frame. The interesting thing about the reference frames in the brain, we know this

600
00:53:07,840 --> 00:53:12,160
because these have been established through neuroscience, studying the entorhonic cortex.

601
00:53:12,160 --> 00:53:16,800
So I'm not speculating here. Okay, this is known neuroscience in an old part of the brain. The

602
00:53:16,800 --> 00:53:23,760
way these cells create reference frames, they have no origin. So what is more like you have a point,

603
00:53:24,320 --> 00:53:30,720
a point in some space, and you given a particular movement, you can then tell what the next point

604
00:53:30,720 --> 00:53:36,800
should be. And you can then tell what the next point would be and so on. You can use this to

605
00:53:38,160 --> 00:53:43,360
calculate how to get from one point to another. So how do I get from my house to my home, or how

606
00:53:43,360 --> 00:53:48,400
do I get my finger from the side of my cup to the top of the cup? How do I get from the axioms to

607
00:53:49,600 --> 00:53:56,080
the conjecture? So it's a different type of reference frame. And if you want, I can describe

608
00:53:56,080 --> 00:53:59,600
in more detail, I can paint a picture how you might want to think about that.

609
00:53:59,600 --> 00:54:07,040
It's really helpful to think it's something you can move through. Yeah. But is it helpful to think

610
00:54:07,040 --> 00:54:12,160
of it as spatial in some sense, or is there something? No, it's definitely spatial. It's spatial

611
00:54:12,160 --> 00:54:16,400
in a mathematical sense. How many dimensions? Can it be a crazy number of dimensions? Well,

612
00:54:16,400 --> 00:54:19,520
that's an interesting question. In the old part of the brain, the entorhonic cortex,

613
00:54:20,160 --> 00:54:24,640
they studied rats. And initially, it looks like, oh, this is just two dimensional. It's like the

614
00:54:24,640 --> 00:54:29,120
rat is in some box and a maze or whatever. And they know whether the rat is using these two

615
00:54:29,120 --> 00:54:34,800
dimensional reference frames and know where it is in the maze. We said, okay, what about bats?

616
00:54:35,360 --> 00:54:40,000
That's a mammal. And they fly in three dimensional space. How do they do that? They seem to know

617
00:54:40,000 --> 00:54:45,840
where they are. So this is a current area of active research. And it seems like somehow the

618
00:54:45,840 --> 00:54:52,240
neurons in the entorhonic cortex can learn three dimensional space. We just, two members of our

619
00:54:52,240 --> 00:54:58,880
team, along with Ilef Fett from MIT, just released a paper this literally last week,

620
00:54:59,440 --> 00:55:05,840
it's on bioarchive, where they show that you can, if you, the way these things work, and I won't get

621
00:55:05,840 --> 00:55:11,920
unless you want to, I won't get into the detail, but grid cells can represent any n dimensional

622
00:55:11,920 --> 00:55:18,000
space. It's not inherently limited. You can think of it this way. If you had two dimensional,

623
00:55:18,000 --> 00:55:21,600
the way it works is you had a bunch of two dimensional slices. That's the way these things

624
00:55:21,600 --> 00:55:26,160
work. There's a whole bunch of two dimensional models. And you can just, you can slice up any

625
00:55:26,160 --> 00:55:30,880
n dimensional space and with two dimensional projections. So, and you could have one dimensional

626
00:55:30,880 --> 00:55:34,640
models. It does. So there's, there's nothing inherent about the mathematics about the way

627
00:55:34,640 --> 00:55:40,000
the neurons do this, which, which constrained the dimensionality of the space, which I think was

628
00:55:40,000 --> 00:55:44,880
important. So obviously, I have a three dimensional map of this cup, maybe it's even more than that.

629
00:55:44,880 --> 00:55:49,520
I don't know. But it's clearly three dimensional map of the cup. I don't just have a projection of

630
00:55:49,520 --> 00:55:53,920
the cup. But when I think about birds or when I think about mathematics, perhaps it's more than

631
00:55:53,920 --> 00:56:02,000
three dimensions. Who knows? So in terms of each individual column building up more and more

632
00:56:02,000 --> 00:56:08,320
information over time, do you think that mechanism is well understood in your mind? You've proposed

633
00:56:08,320 --> 00:56:14,640
a lot of architectures there. Is that a key piece or is it, is the big piece, the thousand brain

634
00:56:15,360 --> 00:56:18,880
theory of intelligence, the ensemble of it all? Well, I think they're both big. I mean,

635
00:56:18,960 --> 00:56:22,960
clearly the concept as a theorist, the concept that's most exciting, right?

636
00:56:22,960 --> 00:56:26,240
A high level concept. A high level concept. This is a totally new way of thinking about

637
00:56:26,240 --> 00:56:30,640
how the near characteristics work. So that is appealing. It has all these ramifications.

638
00:56:30,640 --> 00:56:34,880
And with that, as a framework for how the brain works, you can make all kinds of predictions

639
00:56:34,880 --> 00:56:38,400
and solve all kinds of problems. Now we're trying to work through many of these details right now.

640
00:56:38,400 --> 00:56:42,560
Okay. How do the neurons actually do this? Well, it turns out, if you think about grid cells and

641
00:56:42,560 --> 00:56:46,240
place cells in the old parts of the brain, there's a lot that's known about them, but there's still

642
00:56:46,240 --> 00:56:50,000
some mysteries. There's a lot of debate about exactly the details, how these work and what are

643
00:56:50,000 --> 00:56:54,000
the signs. And we have that still, that same level of detail, that same level of concern.

644
00:56:54,000 --> 00:57:00,480
What we spend here, most of our time doing is trying to make a very good list of the things

645
00:57:00,480 --> 00:57:05,520
we don't understand yet. That's the key part here. What are the constraints? It's not like,

646
00:57:05,520 --> 00:57:09,760
oh, this seems to work. We're done. Now it's like, okay, it kind of works, but these are other things

647
00:57:09,760 --> 00:57:14,880
we know what has to do and it's not doing those yet. I would say we're well on the way here.

648
00:57:14,960 --> 00:57:22,240
We're not done yet. There's a lot of trickiness to this system, but the basic principles about how

649
00:57:22,240 --> 00:57:26,080
different layers in the neocortex are doing much of this, we understand,

650
00:57:27,120 --> 00:57:29,840
but there's some fundamental parts that we don't understand as well.

651
00:57:29,840 --> 00:57:35,680
So what would you say is one of the harder open problems or one of the ones that have been bothering

652
00:57:35,680 --> 00:57:40,800
you, keeping you up at night the most? Oh, well, right now, this is a detailed thing that wouldn't

653
00:57:40,800 --> 00:57:44,960
apply to most people, okay. But you want me to answer that question? Yeah, please.

654
00:57:46,080 --> 00:57:50,880
We've talked about, as if, oh, to predict what you're going to sense on this coffee cup, I need

655
00:57:50,880 --> 00:57:54,960
to know where my finger is going to be on the coffee cup. That is true, but it's insufficient.

656
00:57:56,240 --> 00:58:00,240
Think about my finger touches the edge of the coffee cup. My finger can touch it at different

657
00:58:00,240 --> 00:58:07,520
orientations. I can rotate my finger around here, and that doesn't change. I can make that prediction

658
00:58:08,160 --> 00:58:12,160
and somehow, so it's not just the location. There's an orientation component of this as well.

659
00:58:13,200 --> 00:58:16,880
This is known in the old part of the brain too. There's things called head direction cells, which

660
00:58:16,880 --> 00:58:21,520
way the rat is facing. It's the same kind of basic idea. So if my finger were a rat,

661
00:58:22,160 --> 00:58:25,040
you know, in three dimensions, I have a three-dimensional orientation,

662
00:58:25,600 --> 00:58:28,640
and I have a three-dimensional location. If I was a rat, I would have a,

663
00:58:28,640 --> 00:58:31,920
I think of it as a two-dimensional location, a two-dimensional orientation, a one-dimensional

664
00:58:32,000 --> 00:58:38,720
orientation, like just which way is it facing. So how the two components work together, how it is

665
00:58:38,720 --> 00:58:48,480
that I combine orientation, the orientation of my sensor, as well as the location, is a tricky

666
00:58:48,480 --> 00:58:55,680
problem, and I think I've made progress on it. So at a bigger version of that, the perspective is

667
00:58:55,680 --> 00:59:01,360
super interesting, but super specific. Yeah, I warned you. No, no, that's really good, but

668
00:59:02,080 --> 00:59:07,840
there's a more general version of that. Do you think context matters? The fact that we are

669
00:59:08,400 --> 00:59:16,160
in a building in North America, that we, in the day and age where we have mugs, I mean,

670
00:59:17,120 --> 00:59:20,640
there's all this extra information that you bring to the table about

671
00:59:21,360 --> 00:59:24,560
everything else in the room that's outside of just the coffee cup. Of course it is.

672
00:59:24,560 --> 00:59:30,240
How does it get connected, do you think? Yeah, and that is another really interesting question.

673
00:59:30,240 --> 00:59:35,280
I'm going to throw that under the rubric or the name of attentional problems. First of all,

674
00:59:35,280 --> 00:59:39,920
we have this model. I have many, many models. And also the question, does it matter because...

675
00:59:39,920 --> 00:59:44,640
Well, it matters for certain things. Of course it does. Maybe what we think of that as a coffee

676
00:59:44,640 --> 00:59:48,320
cup in another part of the world is viewed as something completely different, or maybe our

677
00:59:48,320 --> 00:59:52,480
logo, which is very benign in this part of the world, it means something very different in

678
00:59:52,480 --> 00:59:59,200
another part of the world. So those things do matter. I think the way to think about this

679
00:59:59,200 --> 01:00:03,120
the following, or one way to think about it, is we have all these models of the world.

680
01:00:04,640 --> 01:00:08,800
And we model everything. And as I said earlier, I kind of snuck it in there.

681
01:00:08,800 --> 01:00:14,880
Our models are actually, we build composite structure. So every object is composed of other

682
01:00:14,880 --> 01:00:18,960
objects, which are composed of other objects, and they become members of other objects. So this

683
01:00:18,960 --> 01:00:23,840
room has chairs and a table and a room and walls and so on. Now we can just arrange these things

684
01:00:23,840 --> 01:00:30,560
in a certain way. You go, oh, that's in the romantic conference room. And what we do is,

685
01:00:30,560 --> 01:00:36,240
when we go around the world and we experience the world, by walking into a room, for example,

686
01:00:36,240 --> 01:00:39,200
the first thing I do is say, oh, I'm in this room. Do I recognize the room?

687
01:00:39,200 --> 01:00:44,000
Then I can say, oh, look, there's a table here. And by attending to the table,

688
01:00:44,000 --> 01:00:47,120
I'm then assigning this table in the context of the room. Then I say, oh, on the table,

689
01:00:47,120 --> 01:00:50,960
there's a coffee cup. Oh, and on the table, there's a logo. And in the logo,

690
01:00:50,960 --> 01:00:54,320
there's the word Nemento. On the look in the logo, there's the letter E. On the look,

691
01:00:54,320 --> 01:00:59,600
it has an unusual surf. And it doesn't actually, but pretend that there's a surf.

692
01:00:59,600 --> 01:01:06,240
So the point is your attention is kind of drilling deep in and out of these nested structures.

693
01:01:07,360 --> 01:01:11,040
And I can pop back up and I can pop back down. I can pop back up and I can pop back down. So

694
01:01:11,600 --> 01:01:15,520
when I attend to the coffee cup, I haven't lost the context of everything else,

695
01:01:16,080 --> 01:01:18,720
but it's sort of, there's this sort of nested structure.

696
01:01:18,720 --> 01:01:21,840
So the attention filters the reference frame formation

697
01:01:22,880 --> 01:01:24,240
for that particular period of time?

698
01:01:24,240 --> 01:01:28,240
Yes. It basically, moment to moment, you attend the subcomponents,

699
01:01:28,240 --> 01:01:30,240
and then you can attend the subcomponents to subcomponents.

700
01:01:30,240 --> 01:01:31,360
You can move up and down that.

701
01:01:31,360 --> 01:01:33,360
You can move up and down that. We do that all the time. You're not even,

702
01:01:34,080 --> 01:01:37,120
now that I'm aware of it, I'm very conscious of it. But until,

703
01:01:38,160 --> 01:01:41,120
but most people don't even think about this, you know, you just walk in the room and you

704
01:01:41,120 --> 01:01:44,480
don't say, oh, I looked at the chair and I looked at the board and looked at that word on the board

705
01:01:44,480 --> 01:01:46,960
and I looked over here. What's going on? Right?

706
01:01:46,960 --> 01:01:49,920
So what percentage of your day are you deeply aware of this?

707
01:01:49,920 --> 01:01:52,720
And what part can you actually relax and just be Jeff?

708
01:01:52,720 --> 01:01:54,320
Me personally, like my personal day.

709
01:01:54,320 --> 01:01:54,480
Yeah.

710
01:01:55,360 --> 01:01:57,680
Unfortunately, I'm afflicted with too much of the former.

711
01:02:01,200 --> 01:02:02,640
Unfortunately, they are unfortunate.

712
01:02:02,640 --> 01:02:04,400
Yeah. So you don't think it's useful?

713
01:02:04,400 --> 01:02:05,760
Oh, it is useful. Totally useful.

714
01:02:06,640 --> 01:02:09,120
I think about this stuff almost all the time.

715
01:02:09,120 --> 01:02:13,760
And one of my primary ways of thinking is when I'm asleep at night,

716
01:02:13,760 --> 01:02:18,080
I always wake up in the middle of the night and then I stay awake for at least an hour with my

717
01:02:18,080 --> 01:02:22,480
eyes shut in sort of a half-sleep state thinking about these things. I come up with answers to

718
01:02:22,480 --> 01:02:26,720
problems very often in that sort of half-sleeping state. I think about on my bike ride, I think

719
01:02:26,720 --> 01:02:31,520
about on walks. I'm just constantly thinking about this. I have to almost schedule time

720
01:02:32,400 --> 01:02:37,120
to not think about this stuff because it's very, it's mentally taxing.

721
01:02:38,320 --> 01:02:42,000
When you're thinking about this stuff, are you thinking introspectively, like almost

722
01:02:42,080 --> 01:02:45,520
taking a step outside of yourself and trying to figure out what is your mind doing right now?

723
01:02:45,520 --> 01:02:50,640
I do that all the time, but that's not all I do. I'm constantly observing myself.

724
01:02:50,640 --> 01:02:55,120
So as soon as I started thinking about grid cells, for example, and getting into that,

725
01:02:55,120 --> 01:02:58,240
I started saying, oh, well, grid cells can have my place of sense in the world.

726
01:02:58,240 --> 01:03:01,440
That's where you know where you are. And it's interesting, we always have a sense of where

727
01:03:01,440 --> 01:03:05,840
we are unless we're lost. And so I started at night when I got up to go to the bathroom,

728
01:03:05,840 --> 01:03:09,200
I would start trying to do it completely with my eyes closed all the time and I would test my

729
01:03:09,200 --> 01:03:14,320
sense of grid cells. I would walk five feet and say, okay, I think I'm here. Am I really there?

730
01:03:14,320 --> 01:03:17,840
What's my error? And then I would calculate my error again and see how the errors can accumulate.

731
01:03:17,840 --> 01:03:20,320
So even something as simple as getting up in the middle of the night to go to the bathroom,

732
01:03:20,320 --> 01:03:25,040
I'm testing these theories out. It's kind of fun. I mean, the coffee cup is an example of that too.

733
01:03:25,600 --> 01:03:31,600
So I think I find that these sort of everyday introspections are actually quite helpful.

734
01:03:32,800 --> 01:03:38,080
It doesn't mean you can ignore the science. I mean, I spend hours every day reading ridiculously

735
01:03:38,080 --> 01:03:43,840
complex papers. That's not nearly as much fun, but you have to sort of build up those constraints

736
01:03:44,480 --> 01:03:48,880
and the knowledge about the field and who's doing what and what exactly they think is happening here.

737
01:03:48,880 --> 01:03:52,160
And then you can sit back and say, okay, let's try to have pieces all together.

738
01:03:53,280 --> 01:03:58,320
Let's come up with some, you know, I'm very, in this group here, people, they know they do this,

739
01:03:58,320 --> 01:04:01,280
I do this all the time. I come in with these introspective ideas and say, well,

740
01:04:01,280 --> 01:04:03,920
did we ever thought about this? Now watch, well, let's all do this together.

741
01:04:04,880 --> 01:04:10,240
And it's helpful. It's not, as long as you don't, if all you did was that,

742
01:04:10,240 --> 01:04:14,880
then you're just making up stuff, right? But if you're constraining it by the reality of

743
01:04:14,880 --> 01:04:20,800
the neuroscience, then it's really helpful. So let's talk a little bit about deep learning and

744
01:04:20,800 --> 01:04:28,880
the successes in the applied space of neural networks and ideas of training model on data

745
01:04:28,960 --> 01:04:33,120
and these simple computational units and you're on artificial neurons

746
01:04:33,920 --> 01:04:41,280
that would back propagation of statistical ways of being able to generalize from the training

747
01:04:41,280 --> 01:04:47,440
set onto data that's similar to that training set. So where do you think are the limitations of

748
01:04:47,440 --> 01:04:52,240
those approaches? What do you think are its strengths relative to your major efforts of

749
01:04:52,800 --> 01:04:57,680
constructing a theory of human intelligence? Yeah. Well, I'm not an expert in this field.

750
01:04:57,760 --> 01:05:02,000
I'm somewhat knowledgeable. So some of it is in just your intuition. What are your...

751
01:05:02,000 --> 01:05:07,200
Well, I have a little bit more than intuition, but I just want to say one of the things that

752
01:05:07,200 --> 01:05:10,560
you asked me, do I spend all my time thinking about neuroscience? I do. That's to the exclusion

753
01:05:10,560 --> 01:05:14,560
of thinking about things like convolutional neural networks. But I try to stay current.

754
01:05:15,200 --> 01:05:19,760
So look, I think it's great the progress they've made. It's fantastic. And as I mentioned earlier,

755
01:05:19,760 --> 01:05:26,160
it's very highly useful for many things. The models that we have today are actually derived

756
01:05:26,160 --> 01:05:30,480
from a lot of neuroscience principles. They are distributed processing systems and distributed

757
01:05:30,480 --> 01:05:35,760
memory systems. And that's how the brain works. They use things that we might call them neurons,

758
01:05:35,760 --> 01:05:39,680
but they're really not neurons at all. So we can just... They're not really neurons. So they're

759
01:05:39,680 --> 01:05:47,040
distributed processing systems. And nature of hierarchy that came also from neuroscience.

760
01:05:47,040 --> 01:05:51,440
And so there's a lot of things, the learning rules basically, not backprop, but other heavy

761
01:05:51,440 --> 01:05:55,920
and tight learning. I'd be curious to say they're not neurons at all. Can you describe in which

762
01:05:55,920 --> 01:06:00,880
way? I mean, some of it is obvious, but I'd be curious if you have specific ways in which

763
01:06:00,880 --> 01:06:05,440
you think are the biggest differences. Yeah, we had a paper in 2016 called Why Neurons of

764
01:06:05,440 --> 01:06:11,280
Thousands of Synapses. And if you read that paper, you'll know what I'm talking about here.

765
01:06:11,280 --> 01:06:17,440
A real neuron in the brain is a complex thing. Let's just start with the synapses on it, which is

766
01:06:17,440 --> 01:06:23,280
a connection between neurons. Real neurons can everywhere from five to 30,000 synapses on them.

767
01:06:24,240 --> 01:06:29,680
The ones near the cell body, the ones that are close to the soma or the cell body,

768
01:06:30,400 --> 01:06:34,560
those are like the ones that people model in artificial neurons. There's a few hundred of

769
01:06:34,560 --> 01:06:41,520
those, maybe they can affect the cell, they can make the cell become active. 95% of the synapses

770
01:06:42,160 --> 01:06:45,920
can't do that. They're too far away. So if you activate one of those synapses,

771
01:06:45,920 --> 01:06:48,800
it just doesn't affect the cell body enough to make any difference.

772
01:06:48,800 --> 01:06:50,000
Any one of them individually.

773
01:06:50,000 --> 01:06:52,320
Any one of them individually, or even if you do a mass of them.

774
01:06:54,000 --> 01:07:02,160
What real neurons do is the following. If you activate or you get 10 to 20 of them

775
01:07:03,440 --> 01:07:06,640
active at the same time, meaning they're all receiving an input at the same time,

776
01:07:06,640 --> 01:07:11,200
and those 10 to 20 synapses or 40 synapses within a very short distance on the dendro,

777
01:07:11,200 --> 01:07:15,280
like 40 microns, a very small area. So if you activate a bunch of these right next to each

778
01:07:15,280 --> 01:07:20,480
other at some distant place, what happens is it creates what's called the dendritic spike.

779
01:07:21,120 --> 01:07:26,640
And then dendritic spike travels through the dendroids and can reach the soma or the cell body.

780
01:07:27,680 --> 01:07:33,440
Now, when it gets there, it changes the voltage, which is sort of like going to make the cell fire,

781
01:07:33,440 --> 01:07:38,320
but never enough to make the cell fire. It's sort of what we call it, we depolarize the cell,

782
01:07:38,320 --> 01:07:41,920
you raise the voltage a little bit, but not enough to do anything. It's like, well,

783
01:07:41,920 --> 01:07:48,800
good as that. And then it goes back down again. So we proposed a theory, which I'm very confident

784
01:07:49,280 --> 01:07:56,240
basics are, is that what's happening there is those 95% of the synapses are recognizing dozens

785
01:07:56,240 --> 01:08:01,120
to hundreds of unique patterns. They can write, you know, about the 10 and 20 synapses at a time,

786
01:08:02,000 --> 01:08:06,880
and they're acting like predictions. So the neuron actually is a predictive engine on its own.

787
01:08:07,600 --> 01:08:11,600
It can fire when it gets enough what they call proximal input from those ones near the cell

788
01:08:11,600 --> 01:08:16,560
fire, but it can get ready to fire from dozens to hundreds of patterns that it recognizes from

789
01:08:16,560 --> 01:08:23,360
the other guys. And the advantage of this to the neuron is that when it actually does produce a spike

790
01:08:23,360 --> 01:08:28,480
in action potential, it does so slightly sooner than it would have otherwise. And so what could

791
01:08:28,480 --> 01:08:33,120
it slightly sooner? Well, the slightly sooner part is it, there's it all the neurons in the,

792
01:08:33,120 --> 01:08:36,560
the excitatory neurons in the brain are surrounded by these inhibitory neurons,

793
01:08:36,560 --> 01:08:42,480
and they're very fast, the inhibitory neurons, these basket cells. And if I get my spike out

794
01:08:42,480 --> 01:08:47,280
a little bit sooner than someone else, I inhibit all my neighbors around me. Right. And what you

795
01:08:47,280 --> 01:08:51,440
end up with is a different representation. You end up with a representation that matches your

796
01:08:51,440 --> 01:08:55,920
prediction. It's a, it's a sparser representation, meaning the few are non interactive, but it's

797
01:08:55,920 --> 01:09:02,560
much more specific. And so we showed how networks of these neurons can do very sophisticated temporal

798
01:09:02,560 --> 01:09:09,840
prediction, basically. So, so this summarizes real neurons in the brain are time based prediction

799
01:09:09,920 --> 01:09:17,280
engines. And, and they, and there's no concept of this at all, in artificial, what we call point

800
01:09:17,280 --> 01:09:21,040
neurons. I don't think you can mail the brain without them. I don't think you can build intelligence

801
01:09:21,040 --> 01:09:25,920
about it, because it's the, it's the, it's where large part of the time comes from. It's, it's,

802
01:09:25,920 --> 01:09:31,120
these are predictive models. And the time is, is there's a prior and a, you know, a prediction

803
01:09:31,120 --> 01:09:36,560
and an action. And it's inherent through every neuron in the neocortex. So, so I would say that

804
01:09:36,560 --> 01:09:41,520
point neurons sort of model a piece of that and not very well at that either. But, you know, like,

805
01:09:41,520 --> 01:09:49,120
like for example, synapses are very unreliable. And you cannot assign any precision to them.

806
01:09:49,840 --> 01:09:55,200
So even one digit of precision is not possible. So the way real neurons work is they don't add

807
01:09:55,200 --> 01:09:59,920
these, they don't change these weights accurately, like artificial neural networks do. They basically

808
01:09:59,920 --> 01:10:05,760
form new synapses. And so what you're trying to always do is, is detect the presence of some 10

809
01:10:05,760 --> 01:10:11,680
to 20 active synapses at the same time, as opposed, and there's, they're almost binary. It's like,

810
01:10:11,680 --> 01:10:15,520
because you can't really represent anything much finer than that. So these are the kind of

811
01:10:16,160 --> 01:10:19,840
and I think that's actually another essential component, because the brain works on sparse

812
01:10:19,840 --> 01:10:24,880
patterns. And all that, all that mechanism is based on sparse patterns. And I don't actually

813
01:10:24,880 --> 01:10:30,000
think you could build our real brains or machine intelligence without incorporating some of those

814
01:10:30,000 --> 01:10:35,280
ideas. It's hard to even think about the complexity that emerges from the fact that the timing of the

815
01:10:35,280 --> 01:10:43,120
firing matters in the brain, the fact that you form new, new synapses. And I mean, everything

816
01:10:43,120 --> 01:10:47,120
you just mentioned in the past few minutes, trust me, if you spend time on it, you can get your mind

817
01:10:47,120 --> 01:10:52,320
around it. It's not like it's no longer a mystery to me. No, but, but sorry, as a function in a

818
01:10:52,320 --> 01:10:58,000
mathematical way, it's, can you get it start getting an intuition about what gets it excited,

819
01:10:58,000 --> 01:11:03,760
what not, and what kind of representation it's not as easy as there's many other types of neural

820
01:11:03,760 --> 01:11:10,560
networks that are more amenable to pure analysis. You know, especially very simple networks, you

821
01:11:10,560 --> 01:11:14,400
know, oh, I have four neurons and they're doing this. Can we, you know, describe them mathematically

822
01:11:14,400 --> 01:11:19,280
what they're doing type of thing. Even the complexity of convolutional neural networks today,

823
01:11:19,280 --> 01:11:25,360
it's sort of a mystery. They can't really describe the whole system. And so it's different. My colleague,

824
01:11:25,360 --> 01:11:32,880
Subitain Ahmad, he did a nice paper on this. You can get all the stuff on our website if you're

825
01:11:32,880 --> 01:11:37,920
interested in talking about some of the mathematical properties of sparse representations. And so we

826
01:11:37,920 --> 01:11:44,560
can't, what we can do is we can show mathematically, for example, why 10 to 20 synapses to recognize a

827
01:11:44,560 --> 01:11:48,800
pattern is the correct number is the right number you'd want to use. And by the way, that matches

828
01:11:48,800 --> 01:11:57,520
biology, we can show mathematically some of these concepts about the show why the brain is so robust

829
01:11:58,480 --> 01:12:02,960
to noise and error and fall out and so on. We can show that mathematically as well as empirically

830
01:12:02,960 --> 01:12:09,760
in simulations. But the system can't be analyzed completely. Any complex system can't. And so

831
01:12:09,760 --> 01:12:17,760
that's out of the realm. But there is there is mathematical benefits and intuitions that can

832
01:12:17,760 --> 01:12:22,080
be derived from mathematics. And we try to do that as well. Most most of our papers have a section

833
01:12:22,080 --> 01:12:27,440
about that. So I think it's refreshing and useful for me to be talking to you about deep

834
01:12:27,440 --> 01:12:34,480
neural networks, because your intuition basically says that we can't achieve anything like intelligence

835
01:12:34,480 --> 01:12:37,760
with artificial neural networks. Well, not in the current form. Not in the current form. I'm sure

836
01:12:37,760 --> 01:12:42,800
we can do it in the ultimate form, sure. So let me dig into it and see what your thoughts are there

837
01:12:42,800 --> 01:12:47,440
a little bit. So I'm not sure if you read this little blog post called Bitter Lesson by Rich

838
01:12:47,520 --> 01:12:53,280
Sutton. Recently, he's a reinforcement learning pioneer. I'm not sure if you're familiar with him.

839
01:12:53,280 --> 01:12:59,440
His basic idea is that all the stuff we've done in AI in the past 70 years, he's one of the old

840
01:12:59,440 --> 01:13:08,960
school guys. The biggest lesson learned is that all the tricky things we've done don't, you know,

841
01:13:08,960 --> 01:13:13,600
they benefit in the short term. But in the long term, what wins out is a simple general method

842
01:13:14,400 --> 01:13:19,760
that just relies on Moore's law on computation getting faster and faster.

843
01:13:19,760 --> 01:13:22,720
This is what he's saying. This is what has worked up to now.

844
01:13:23,600 --> 01:13:29,920
What has worked up to now, that if you're trying to build a system, if we're talking about,

845
01:13:29,920 --> 01:13:33,360
he's not concerned about intelligence. He's concerned about a system that works

846
01:13:34,400 --> 01:13:40,160
in terms of making predictions on applied, narrow AI problems. That's what the discussion is about.

847
01:13:41,120 --> 01:13:48,480
That you just try to go as general as possible and wait years or decades for the computation

848
01:13:48,480 --> 01:13:53,200
to make it actually. Is he saying that as a criticism or is he saying this is a prescription

849
01:13:53,200 --> 01:13:57,920
of what we ought to be doing? Well, it's very difficult. He's saying this is what has worked

850
01:13:57,920 --> 01:14:00,880
and yes, a prescription, but it's a difficult prescription because it says

851
01:14:01,520 --> 01:14:07,280
all the fun things you guys are trying to do, we are trying to do, he's part of the community,

852
01:14:07,280 --> 01:14:13,200
is saying it's only going to be short-term gains. This all leads up to a question, I guess,

853
01:14:13,840 --> 01:14:19,200
on artificial neural networks and maybe our own biological neural networks is,

854
01:14:20,320 --> 01:14:25,280
do you think if we just scale things up significantly, so take these dumb artificial

855
01:14:25,840 --> 01:14:33,120
neurons, the point neurons, I like that term, if we just have a lot more of them,

856
01:14:33,120 --> 01:14:38,080
do you think some of the elements that we see in the brain may start emerging?

857
01:14:38,080 --> 01:14:44,960
No, I don't think so. We can do bigger problems of the same type. It's been pointed out by many

858
01:14:44,960 --> 01:14:48,720
people that today's convolutional neural networks aren't really much different than the ones we had

859
01:14:48,720 --> 01:14:53,680
quite a while ago. They're bigger and train more and we have more labeled data and so on,

860
01:14:56,320 --> 01:15:01,120
but I don't think you can get to the kind of things I know the brain can do and that we think

861
01:15:01,120 --> 01:15:07,120
about as intelligence by just scaling it up. It's a good description of what's happened in

862
01:15:07,120 --> 01:15:11,680
the past, what's happened recently with the reemergence of artificial neural networks.

863
01:15:12,480 --> 01:15:16,160
It may be a good prescription for what's going to happen in the short term,

864
01:15:17,520 --> 01:15:21,920
but I don't think that's the path. I've said that earlier, there's an alternate path. I should

865
01:15:21,920 --> 01:15:28,000
mention to you, by the way, that we've made sufficient progress on the whole cortical theory

866
01:15:28,000 --> 01:15:36,880
in the last few years that last year, we decided to start actively pursuing how we get these ideas

867
01:15:36,880 --> 01:15:42,480
embedded into machine learning. That's again being led by my colleague, Subhathayamad,

868
01:15:43,040 --> 01:15:46,080
and because he's more of a machine learning guy, I'm more of an neuroscience guy.

869
01:15:49,200 --> 01:15:56,320
Now, I wouldn't say our focus, but it is now an equal focus here because we need to

870
01:15:56,320 --> 01:16:03,280
proselytize what we've learned, and we need to show how it's beneficial to the machine

871
01:16:03,280 --> 01:16:07,600
learning. We have a plan in place right now. In fact, we just did our first paper on this.

872
01:16:07,600 --> 01:16:12,480
I can tell you about that, but one of the reasons I want to talk to you is because I'm trying to

873
01:16:13,200 --> 01:16:17,120
get more people in the machine learning community to say, I need to learn about this stuff,

874
01:16:17,120 --> 01:16:20,880
and maybe we should just think about this a bit more about what we've learned about the brain,

875
01:16:20,880 --> 01:16:24,640
and what are those team members meant to have? What have they done? Is that useful for us?

876
01:16:25,120 --> 01:16:29,760
Yeah, so is there elements of all the cortical theory that things we've been talking about

877
01:16:29,760 --> 01:16:33,360
that may be useful in the short term? Yes, in the short term, yes.

878
01:16:33,360 --> 01:16:39,200
This is the, sorry to interrupt, but the open question is it certainly feels from my perspective

879
01:16:39,200 --> 01:16:44,160
that in the long term, some of the ideas we've been talking about will be extremely useful.

880
01:16:44,160 --> 01:16:48,480
The question is whether in the short term. Well, this is always what I would call the

881
01:16:48,480 --> 01:16:54,720
entrepreneur's dilemma. So you have this long term vision. Oh, we're going to all be driving

882
01:16:54,720 --> 01:16:58,160
electric cars or we're all going to have computers or we're all going to whatever.

883
01:16:58,960 --> 01:17:03,120
And, and you're at some point in time and you say, I can see that long term vision. I'm sure

884
01:17:03,120 --> 01:17:06,320
it's going to happen. How do I get there without killing myself, you know, without going out of

885
01:17:06,320 --> 01:17:11,760
business? That's the challenge. That's the dilemma. That's the really difficult thing to do. So we're

886
01:17:11,760 --> 01:17:16,240
facing that right now. So ideally what you'd want to do is find some steps along the way that you

887
01:17:16,240 --> 01:17:19,840
can get there incrementally. You don't have to like throw it all out and start over again.

888
01:17:20,400 --> 01:17:27,520
The first thing that we've done is we focus on the sparse representations. So just, just in case

889
01:17:27,520 --> 01:17:31,520
you don't know what that means or some of the listeners don't know what that means. In the

890
01:17:31,520 --> 01:17:36,880
brain, if I have like 10,000 neurons, what you would see is maybe 2% of them active at a time.

891
01:17:36,880 --> 01:17:42,560
You don't see 50%, you don't think 30%, you might see 2%. And it's always like that.

892
01:17:42,560 --> 01:17:46,480
For any set of sensory inputs. It doesn't matter if anything, it doesn't matter with any part of the

893
01:17:46,480 --> 01:17:54,080
brain. But which neurons differs? Which neurons are active? Yeah, so let me put this, let's say I

894
01:17:54,080 --> 01:17:58,000
take 10,000 neurons that are representing something. They're sitting there in a block together. It's a

895
01:17:58,000 --> 01:18:01,600
teeny little block in a neuron, 10,000 neurons. And they're representing a location, they're

896
01:18:01,600 --> 01:18:04,560
representing a cop, they're representing the input from my sensors. I don't know, it doesn't

897
01:18:04,560 --> 01:18:10,000
matter. It's representing something. The way the representations occur, it's always a sparse

898
01:18:10,000 --> 01:18:14,320
representation, meaning it's a population code. So which 200 cells are active tells me what's

899
01:18:14,320 --> 01:18:19,280
going on. It's not individual cells aren't that important at all. It's the population code that

900
01:18:19,280 --> 01:18:25,680
matters. And when you have sparse population codes, then all kinds of beautiful properties come out

901
01:18:25,680 --> 01:18:30,160
of them. So the brain uses sparse population codes that we've written and described these

902
01:18:30,160 --> 01:18:37,360
benefits in some of our papers. So they give this tremendous robustness to the systems.

903
01:18:37,360 --> 01:18:41,920
Your brains are incredibly robust. Neurons are dying all the time and spasming and synapses

904
01:18:41,920 --> 01:18:49,120
falling apart and, you know, all the time and it keeps working. So what Subitai and Louise,

905
01:18:49,120 --> 01:18:55,760
one of our other engineers here have done, I've shown that they're introducing sparseness into

906
01:18:55,760 --> 01:18:58,480
convolutional neural networks. Now other people are thinking along these lines, but we're going

907
01:18:58,480 --> 01:19:04,000
about it in a more principled way, I think. And we're showing that if you enforce sparseness

908
01:19:04,000 --> 01:19:11,920
throughout these convolutional neural networks, in both which neurons are active and the connections

909
01:19:11,920 --> 01:19:17,120
between them, that you get some very desirable properties. So one of the current hot topics in

910
01:19:17,920 --> 01:19:22,640
deep learning right now are these adversarial examples. So, you know, I can give me any deep

911
01:19:22,640 --> 01:19:26,800
learning network and I can give you a picture that looks perfect and you're going to call it,

912
01:19:26,800 --> 01:19:32,400
you know, you're going to say the monkey is, you know, an airplane. So that's a problem.

913
01:19:32,400 --> 01:19:36,480
And DARPA just announced some big thing. They're trying to, you know, have some contests for this.

914
01:19:36,480 --> 01:19:42,000
But if you enforce sparse representations here, many of these problems go away. They're much more

915
01:19:42,000 --> 01:19:47,440
robust and they're not easy to fool. So we've already shown some of those results,

916
01:19:48,240 --> 01:19:56,160
just literally in January or February, just like last month we did that. And you can, I think it's

917
01:19:56,160 --> 01:20:02,320
on bioarchive right now or on iCry, you can read about it. But so that's like a baby step.

918
01:20:02,320 --> 01:20:05,920
Okay. That's a take something from the brain. We know, we know about sparseness. We know why

919
01:20:05,920 --> 01:20:09,440
it's important. We know what it gives the brain. So let's try to enforce that onto this.

920
01:20:09,440 --> 01:20:13,360
What's your intuition why sparsity leads to robustness? Because it feels like it would be

921
01:20:13,360 --> 01:20:22,880
less robust. Why would you feel the rest robust to you? So it just feels like if the fewer neurons

922
01:20:22,960 --> 01:20:27,840
are involved, the more fragile the representation. Yeah, but I didn't say there was lots of

923
01:20:27,840 --> 01:20:34,400
funerals. I said, let's say 200. That's a lot. There's still a lot. So here's an intuition for it.

924
01:20:35,120 --> 01:20:40,640
This is a bit technical. So for, you know, for engineers, machine learning people,

925
01:20:40,640 --> 01:20:45,600
this would be easy, but all the listeners, maybe not. If you're trying to classify something,

926
01:20:45,600 --> 01:20:49,600
you're trying to divide some very high dimensional space into different pieces,

927
01:20:49,600 --> 01:20:53,520
A and B, and you're trying to create some point where you say all these points in this

928
01:20:53,520 --> 01:20:56,320
high dimensional space are A and all these points inside dimensional space are B.

929
01:20:57,520 --> 01:21:03,280
And if you have points that are close to that line, it's not very robust. It works for all

930
01:21:03,280 --> 01:21:08,240
the points you know about, but it's, it's not very robust because you just move a little bit and

931
01:21:08,240 --> 01:21:14,160
you've crossed over the line. When you have sparse representations, imagine I pick, I have,

932
01:21:14,160 --> 01:21:20,240
I'm going to pick 200 cells active out of, out of 10,000. Okay. So I have 200 cells active.

933
01:21:20,240 --> 01:21:24,880
Now let's say I pick randomly another, a different representation, 200. The overlap

934
01:21:24,880 --> 01:21:30,800
between those is going to be very small, just a few. I can pick millions of samples randomly

935
01:21:31,440 --> 01:21:38,880
of 200 neurons and not one of them will overlap more than just a few. So one way to think about

936
01:21:38,880 --> 01:21:43,360
is if I want to fool one of these representations to look like one of those other representations,

937
01:21:43,360 --> 01:21:48,160
I can't move just one cell or two cells or three cells or four cells. I have to move a hundred cells

938
01:21:49,120 --> 01:21:56,080
and that makes them robust. In terms of further, so you mentioned sparsity.

939
01:21:56,080 --> 01:22:01,040
Won't it be the next thing? Yeah. Okay. So we have, we picked one. We don't know if it's going

940
01:22:01,040 --> 01:22:05,040
to work well yet. So again, we're trying to come up incremental ways of moving from

941
01:22:05,760 --> 01:22:11,360
brain theory to add pieces to machine learning, current machine learning world and one step at

942
01:22:11,360 --> 01:22:16,000
a time. So the next thing we're going to try to do is sort of incorporate some of the ideas of

943
01:22:17,520 --> 01:22:23,360
the 1000 brains theory that you have many, many models and that are voting. Now that idea is not

944
01:22:23,360 --> 01:22:28,960
new. There's a mixture of models has been around for a long time, but the way the brain does it is

945
01:22:28,960 --> 01:22:35,680
a little different and the way it votes is different and the kind of way it represents

946
01:22:35,680 --> 01:22:40,960
uncertainty is different. So we're just starting this work, but we're going to try to see if we

947
01:22:41,360 --> 01:22:45,280
sort of incorporate some of the principles of voting or principles of 1000 brain theory,

948
01:22:46,000 --> 01:22:52,640
like lots of simple models that talk to each other in a very certain way.

949
01:22:53,840 --> 01:23:00,480
And can we build more machines and systems that learn faster and also, well, mostly

950
01:23:01,920 --> 01:23:09,600
are multimodal and robust to multimodal type of issues. So one of the challenges there

951
01:23:09,600 --> 01:23:15,520
is the machine learning computer vision community has certain sets of benchmarks.

952
01:23:15,520 --> 01:23:20,960
So it's a test based on which they compete. And I would argue, especially from your perspective,

953
01:23:21,920 --> 01:23:28,720
that those benchmarks aren't that useful for testing the aspects that the brain is good at

954
01:23:28,720 --> 01:23:33,360
or intelligent. They're not really testing intelligence. They're very fine. And it's been

955
01:23:33,360 --> 01:23:39,760
extremely useful for developing specific mathematical models, but it's not useful in the

956
01:23:39,760 --> 01:23:44,960
long term for creating intelligence. So you think you also have a role in proposing better

957
01:23:46,080 --> 01:23:50,080
tests? Yeah, this is a very, you've identified a very serious problem.

958
01:23:51,360 --> 01:23:55,520
First of all, the test that they have or the test that they want, not the test of the other

959
01:23:55,520 --> 01:24:02,720
things that we're trying to do, right? You know, what are the so on? The second thing is,

960
01:24:02,720 --> 01:24:09,280
sometimes these to be competitive in these tests, you have to have huge data sets and huge computing

961
01:24:09,280 --> 01:24:15,440
power. And so, you know, and we don't have that here. We don't have it as well as other big teams

962
01:24:15,440 --> 01:24:22,320
that big companies do. So there's numerous issues there. You know, we come out of, you know,

963
01:24:22,320 --> 01:24:26,000
we're our approach to this is all based on in some sense, you might argue elegance,

964
01:24:26,000 --> 01:24:28,800
we're coming at it from like a theoretical base that we think, Oh, my God, this is so,

965
01:24:28,800 --> 01:24:31,680
this is so clearly elegant. This is how brains work. This is what intelligence is.

966
01:24:31,680 --> 01:24:34,880
But the machine learning world has gotten in this phase where they think it doesn't matter.

967
01:24:35,440 --> 01:24:39,360
Doesn't matter what you think, as long as you do, you know, 0.1% better on this benchmark,

968
01:24:39,360 --> 01:24:45,280
that's what that's all that matters. And that's a problem. You know, we have to figure out how

969
01:24:45,280 --> 01:24:48,640
to get around that. That's that's a challenge for us. That's that's one of the challenges that

970
01:24:48,640 --> 01:24:55,120
we have to deal with. So I agree, you've identified a big issue. It's difficult for those reasons.

971
01:24:55,840 --> 01:25:00,720
But, you know, part of the reasons I'm talking to you here today is I hope I'm going to get some

972
01:25:00,720 --> 01:25:05,040
machine learning people to say, read those papers. Those might be some interesting ideas. I'm tired.

973
01:25:05,040 --> 01:25:07,600
I'm tired of doing this 0.1% improvement stuff, you know,

974
01:25:08,400 --> 01:25:12,960
well, that's what that's why I'm here as well, because I think machine learning now as a community

975
01:25:12,960 --> 01:25:20,800
is a place where the next step is needs to be orthogonal to what has received success in the

976
01:25:20,800 --> 01:25:26,080
past. You see other leaders saying this, machine learning leaders, you know, Jeff Hinton, with

977
01:25:26,080 --> 01:25:29,760
his capsules idea. Many people have gotten up saying, you know, we're going to hit road,

978
01:25:30,880 --> 01:25:36,080
maybe we should look at the brain, you know, things like that. So hopefully that thinking

979
01:25:36,080 --> 01:25:41,120
will occur organically. And then then we're in a nice position for people to come and look at our

980
01:25:41,120 --> 01:25:44,960
work and say, well, what can we learn from these guys? Yeah, MIT is just launching a

981
01:25:44,960 --> 01:25:50,160
billion dollar computing college that's centered around this idea. So on this idea of what?

982
01:25:50,880 --> 01:25:55,600
Well, the idea that, you know, the humanities, psychology and neuroscience have to work all

983
01:25:55,600 --> 01:26:01,360
together to get to build the S. Yeah. I mean, Stanford just did this human center today. I

984
01:26:01,360 --> 01:26:07,920
said, yeah, I'm a little disappointed in these initiatives because, you know, they're, they're

985
01:26:07,920 --> 01:26:13,440
focusing on sort of the human side of it. And it could very easily slip into how humans interact

986
01:26:13,440 --> 01:26:19,360
with intelligent machines, which is nothing wrong with that. But that's not, that is orthogonal

987
01:26:19,360 --> 01:26:23,120
to what we're trying to do. We're trying to say, like, what is the essence of intelligence? I don't

988
01:26:23,120 --> 01:26:27,040
care. In fact, I want to build intelligent machines that aren't emotional, that don't

989
01:26:27,040 --> 01:26:31,760
smile at you, that, you know, that aren't trying to tuck you in at night.

990
01:26:31,760 --> 01:26:36,880
Yeah, there is that pattern that you, when you talk about understanding humans is important

991
01:26:36,880 --> 01:26:41,440
for understanding intelligence, that you start slipping into topics of ethics or,

992
01:26:42,720 --> 01:26:46,880
yeah, like you said, the interactive elements as opposed to, no, no, no, we have to zoom in on

993
01:26:46,880 --> 01:26:52,720
the brain, study, study what the human brain, the baby, the, let's study what a brain does.

994
01:26:52,720 --> 01:26:57,920
Does. And then we can decide which parts of that we want to recreate in some system. But

995
01:26:57,920 --> 01:27:01,120
until you have that theory about what the brain does, what's the point? You know, it's just,

996
01:27:01,120 --> 01:27:04,560
you're going to be wasting time, I think. Right. Just to break it down on the artificial

997
01:27:04,560 --> 01:27:09,040
neural networks side, maybe you can speak to this on the, on the biologic neural networks side,

998
01:27:09,040 --> 01:27:15,040
the process of learning versus the process of inference. Maybe you can explain to me,

999
01:27:16,400 --> 01:27:20,000
what is there a difference between, you know, in artificial neural networks, there's a

1000
01:27:20,000 --> 01:27:24,240
difference between the learning stage and the inference stage. Do you see the brain as something

1001
01:27:24,240 --> 01:27:29,840
different? One of the, one of the big distinctions that people often say, I don't know how correct

1002
01:27:29,840 --> 01:27:34,240
it is, is artificial neural networks need a lot of data, they're very inefficient learning.

1003
01:27:34,800 --> 01:27:40,160
Do you see that as a correct distinction from the, the biology of the human brain,

1004
01:27:40,160 --> 01:27:44,160
that the human brain is very efficient? Or is that just something we deceive ourselves with?

1005
01:27:44,160 --> 01:27:47,440
No, it is efficient, obviously. We can learn new things almost instantly.

1006
01:27:47,440 --> 01:27:52,240
And so what elements do you think? Yeah, I can talk about that. You brought up two issues there.

1007
01:27:52,240 --> 01:27:57,200
So remember I talked early about the constraints we, we always feel, well, one of those constraints

1008
01:27:57,200 --> 01:28:02,720
is the fact that brains are continually learning. That's not something we said, oh, we can add that

1009
01:28:02,720 --> 01:28:09,520
later. That's something that was upfront, had to be there from the start, made our problems

1010
01:28:10,080 --> 01:28:16,160
harder. But we showed, going back to the 2016 paper on sequence memory, we showed how that

1011
01:28:16,160 --> 01:28:22,560
happens, how the brains infer and learn at the same time. And our models do that. They're not

1012
01:28:22,560 --> 01:28:29,680
two separate phases or two separate sets of time. I think that's a big, big problem in AI,

1013
01:28:29,760 --> 01:28:36,480
at least for many applications, not for all. So I can talk about that. There are some that gets

1014
01:28:36,480 --> 01:28:41,600
detailed. There are some parts of the neocortex in the brain where actually what's going on,

1015
01:28:41,600 --> 01:28:46,160
there's these, there's these, with these cycles, they're like cycles of activity in the brain.

1016
01:28:46,800 --> 01:28:52,320
And there's very strong evidence that you're doing more of inference on one part of the phase and

1017
01:28:52,320 --> 01:28:55,680
more of learning on the other part of the phase. So the brain can actually sort of separate different

1018
01:28:55,680 --> 01:29:00,720
populations of cells are going back and forth like this. But in general, I would say that's an

1019
01:29:00,720 --> 01:29:06,880
important problem. We have all of our networks that we've come up with do both. They're learning,

1020
01:29:06,880 --> 01:29:11,840
continuous learning networks. And you mentioned benchmarks earlier. Well, there are no benchmarks

1021
01:29:11,840 --> 01:29:17,920
about that. Exactly. So we have to like, we get in our little soapbox and say, hey, by the way,

1022
01:29:17,920 --> 01:29:23,840
this is important and here's the mechanism for doing that. But until you can prove it to someone

1023
01:29:23,840 --> 01:29:27,600
in some, you know, commercial system or something, it's a little harder. So yeah, one of the things

1024
01:29:27,600 --> 01:29:35,200
I had to linger on that is in some ways to learn the concept of a coffee cup. You only need this one

1025
01:29:35,200 --> 01:29:40,080
coffee cup and maybe some time alone in a room with it. Well, the first thing is I, when I imagine

1026
01:29:40,080 --> 01:29:44,320
I reach my hand into a black box and I'm reaching, I'm trying to touch something. I don't know up

1027
01:29:44,320 --> 01:29:49,920
front if it's something I already know, or if it's a new thing. And I have to, I'm doing both at the

1028
01:29:49,920 --> 01:29:54,720
same time. I don't say, oh, let's see if it's a new thing. Oh, let's see if it's an old thing. I

1029
01:29:54,720 --> 01:30:00,880
don't do that. As I go, my brain says, oh, it's new or it's not new. And if it's new, I start learning

1030
01:30:01,600 --> 01:30:05,840
what it is. So and by the way, it starts learning from the get go, even if we're going to recognize

1031
01:30:05,840 --> 01:30:10,960
it. So they're not separate problems. And so that's the thing. The other thing you mentioned

1032
01:30:10,960 --> 01:30:16,320
was the fast learning. So I was just talking about continuous learning, but there's also fast

1033
01:30:16,320 --> 01:30:20,080
learning. Literally, I can show you this coffee cup. And I say, here's a new coffee cup. It's

1034
01:30:20,080 --> 01:30:25,040
got the logo on it. Take a look at it. Done. You're done. You can predict what it's going to look

1035
01:30:25,040 --> 01:30:32,720
like, you know, in different positions. So I can talk about that too. In the brain, the way learning

1036
01:30:32,720 --> 01:30:37,440
occurs. I mentioned this earlier, but I mentioned again, the way learning occurs, I imagine I have

1037
01:30:37,440 --> 01:30:43,520
a section of a dendrite of a neuron. And I want to learn, I'm going to learn something new. I'm

1038
01:30:43,520 --> 01:30:47,600
just doesn't matter what it is, I'm just going to learn something new. I need to recognize a new

1039
01:30:47,600 --> 01:30:53,600
pattern. So what I'm going to do is I'm going to form new synapses. New synapses, we're going to

1040
01:30:53,600 --> 01:31:00,480
rewire the brain onto that section of the dendrite. Once I've done that, everything else that neuron

1041
01:31:00,480 --> 01:31:06,320
has learned is not affected by it. That's because it's isolated to that small section of the dendrite.

1042
01:31:06,320 --> 01:31:11,200
They're not all being added together, like a point neuron. So if I learned something new on this

1043
01:31:11,200 --> 01:31:14,800
segment here, it doesn't change any of the learning that occur anywhere else in that neuron.

1044
01:31:14,800 --> 01:31:19,440
So I can add something without affecting previous learning. And I can do it quickly.

1045
01:31:20,880 --> 01:31:24,400
Now let's talk, we can talk about the quickness, how it's done in real neurons. You might say,

1046
01:31:24,400 --> 01:31:29,840
well, doesn't it take time to form synapses? Yes, it can take maybe an hour to form a new synapse.

1047
01:31:30,800 --> 01:31:36,000
We can form memories quicker than that. And I can explain that happens too, if you want. But

1048
01:31:36,720 --> 01:31:41,280
it's getting a bit neuroscience-y. That's great. But is there an understanding

1049
01:31:41,280 --> 01:31:46,160
of these mechanisms at every level? So from the short-term memories and the forming

1050
01:31:47,520 --> 01:31:52,400
many connections. So this idea of synaptogenesis, the growth of new synapses, that's well

1051
01:31:52,400 --> 01:31:56,720
described, as well understood. And that's an essential part of learning. That is learning.

1052
01:31:56,720 --> 01:32:04,560
That is learning. Going back many, many years, people was

1053
01:32:05,520 --> 01:32:11,120
what's his name, the psychologist proposed, Heb, Donald Heb. He proposed that learning was the

1054
01:32:11,120 --> 01:32:17,200
modification of the strength of a connection between two neurons. People interpreted that as

1055
01:32:17,200 --> 01:32:21,680
the modification of the strength of a synapse. He didn't say that. He just said there's a

1056
01:32:21,680 --> 01:32:26,400
modification between the effect of one neuron and another. So synaptogenesis is totally consistent

1057
01:32:26,400 --> 01:32:30,800
with Donald Heb said. But anyway, there's these mechanisms, the growth of new synapse,

1058
01:32:30,800 --> 01:32:33,840
you can go online, you can watch a video of a synapse growing in real time.

1059
01:32:33,840 --> 01:32:38,960
It's literally, you can see this little thing going. It's pretty impressive. So those

1060
01:32:38,960 --> 01:32:43,520
mechanisms are known. Now, there's another thing that we've speculated and we've written about,

1061
01:32:43,520 --> 01:32:49,120
which is consistent with no neuroscience, but it's less proven. And this is the idea,

1062
01:32:49,120 --> 01:32:53,920
how do I form a memory really, really quickly? Like instantaneous. If it takes an hour to

1063
01:32:53,920 --> 01:33:00,080
grow a synapse, like that's not instantaneous. So there are types of synapses called silent

1064
01:33:00,080 --> 01:33:05,040
synapses. They look like a synapse, but they don't do anything. They're just sitting there. It's

1065
01:33:05,040 --> 01:33:10,560
like if an action potential comes in, it doesn't release any neurotransmitter. Some parts of the

1066
01:33:10,560 --> 01:33:15,200
brain have more of these than others. For example, the hippocampus has a lot of them, which is where

1067
01:33:15,200 --> 01:33:22,000
we associate most short-term memory with. So what we speculated, again, in that 2016 paper,

1068
01:33:22,000 --> 01:33:28,160
we proposed that the way we form very quick memories, very short-term memories, or quick

1069
01:33:28,160 --> 01:33:35,200
memories, is that we convert silent synapses into active synapses. It's like saying a synapse has

1070
01:33:35,200 --> 01:33:41,520
a zero weight and a one weight. But the long-term memory has to be formed by synaptogenesis. So

1071
01:33:41,520 --> 01:33:45,600
you can remember something really quickly by just flipping a bunch of these guys from silent to active.

1072
01:33:46,080 --> 01:33:52,080
It's not from 0.1 to 0.15. It doesn't do anything until it releases transmitter.

1073
01:33:52,080 --> 01:33:55,520
And if I do that over a bunch of these, I've got a very quick short-term memory.

1074
01:33:56,240 --> 01:34:00,880
So I guess the lesson behind this is that most neural networks today are fully connected.

1075
01:34:01,760 --> 01:34:05,920
Every neuron connects every other neuron from layer to layer. That's not correct in the brain.

1076
01:34:05,920 --> 01:34:10,880
We don't want that. We actually don't want that. It's bad. You want a very sparse connectivity so

1077
01:34:10,880 --> 01:34:16,640
that any neuron connects to some subset of the neurons in the other layer. And it does so on a

1078
01:34:16,640 --> 01:34:23,360
dendrite by dendrite segment basis. So it's a very parcelated out type of thing. And that then

1079
01:34:23,360 --> 01:34:27,360
learning is not adjusting all these weights, but learning is just saying, okay, connect to these

1080
01:34:27,360 --> 01:34:33,360
10 cells here right now. In that process, you know, with artificial neural networks, it's a very

1081
01:34:33,360 --> 01:34:39,840
simple process of back propagation that adjusts the weights. The process of synaptogenesis.

1082
01:34:39,840 --> 01:34:44,160
Synaptogenesis. Synaptogenesis. It's even easier. It's even easier. It's even easier.

1083
01:34:44,160 --> 01:34:49,840
Back propagation requires something that really can't happen in brains. This back propagation

1084
01:34:49,840 --> 01:34:53,360
of this error signal. They really can't happen. People are trying to make it happen in brains,

1085
01:34:53,360 --> 01:34:58,080
but it doesn't happen in brain. This is pure Hebbian learning. Well, synaptogenesis is pure

1086
01:34:58,080 --> 01:35:02,080
Hebbian learning. It's basically saying there's a population of cells over here that are active

1087
01:35:02,080 --> 01:35:06,320
right now. And there's a population of cells over here active right now. How do I form connections

1088
01:35:06,320 --> 01:35:12,480
between those active cells? And it's literally saying this guy became active. These 100 neurons

1089
01:35:12,480 --> 01:35:17,440
here became active before this neuron became active. So form connections to those ones. That's it.

1090
01:35:17,440 --> 01:35:22,800
There's no propagation of error. Nothing. All the networks we do, all the models we have work on

1091
01:35:23,520 --> 01:35:31,520
almost completely on Hebbian learning, but in on dendritic segments and multiple synaptes at the

1092
01:35:31,520 --> 01:35:37,360
same time. So now let's turn the question that you already answered and maybe you can answer it again.

1093
01:35:38,640 --> 01:35:43,760
If you look at the history of artificial intelligence, where do you think we stand? How

1094
01:35:43,840 --> 01:35:48,800
far are we from solving intelligence? You said you were very optimistic. Can you elaborate on that?

1095
01:35:48,800 --> 01:35:54,960
Yeah. It's always the crazy question to ask because no one can predict the future.

1096
01:35:55,520 --> 01:36:01,440
So I'll tell you a story. I used to run a different neuroscience institute called the

1097
01:36:01,440 --> 01:36:06,240
Redburn Neuroscience Institute. And we would hold these symposiums and we'd get like 35 scientists

1098
01:36:06,240 --> 01:36:10,640
from around the world to come together. And I used to ask them all the same question. I would say,

1099
01:36:10,640 --> 01:36:14,320
well, how long do you think it'll be before we understand how the New York Cortex works?

1100
01:36:14,320 --> 01:36:17,040
And everyone went around the room and they introduced the name and they have to answer

1101
01:36:17,040 --> 01:36:24,640
that question. So I got, the typical answer was 50 to 100 years. Some people would say 500 years.

1102
01:36:24,640 --> 01:36:30,160
Some people said never. I said, why are you, why are you a neuroscience? It's a good pay.

1103
01:36:32,560 --> 01:36:37,840
It's interesting. So, you know, but it doesn't work like that. As I mentioned earlier, these are

1104
01:36:38,720 --> 01:36:42,560
step functions. Things happen and then bingo, they happen. You can't predict that.

1105
01:36:43,520 --> 01:36:48,720
I feel I've already passed a step function. So if I can do my job correctly over the next five years,

1106
01:36:50,720 --> 01:36:54,960
then meaning I can proselytize these ideas, I can convince other people they're right,

1107
01:36:56,000 --> 01:37:00,400
we can show that other people or other machine learning people should pay attention to these

1108
01:37:00,400 --> 01:37:05,840
ideas, then we're definitely in an under 20 year time frame. If I can do those things,

1109
01:37:05,920 --> 01:37:09,680
if I, if I'm not successful in that, and this is the last time anyone talks to me

1110
01:37:09,680 --> 01:37:14,640
and no one reads our papers and, you know, I'm wrong or something like that, then,

1111
01:37:14,640 --> 01:37:21,520
then I don't know. But it's, it's not 50 years. It's, it, you know, it'll, it'll, you know,

1112
01:37:21,520 --> 01:37:25,280
the same thing about electric cars, how quickly are they going to populate the world? It probably

1113
01:37:25,280 --> 01:37:30,240
takes about a 20 year span. It'll be something like that. But I think if I can do what I said,

1114
01:37:30,240 --> 01:37:35,760
we're starting it. And of course, there could be other use of step functions. It could be

1115
01:37:37,440 --> 01:37:41,760
everybody gives up on your ideas for 20 years, and then all of a sudden somebody picks it up

1116
01:37:41,760 --> 01:37:45,520
again. Wait, that guy was onto something. Yeah. So that would be a, that would be a failure on

1117
01:37:45,520 --> 01:37:50,560
my part, right? You know, think about Charles Babbage, you know, Charles Babbage, he's the guy

1118
01:37:50,560 --> 01:37:58,320
who invented the computer back in the 18 something 1800s. And everyone forgot about it until, you

1119
01:37:58,320 --> 01:38:02,080
know, 100 years later and say, Hey, this guy figured this stuff out a long time ago. Yeah.

1120
01:38:02,080 --> 01:38:05,680
You know, but he was ahead of his time. Yeah. I don't think, you know, like, as I said,

1121
01:38:06,320 --> 01:38:11,040
I recognize this is part of any entrepreneur's challenge. I use entrepreneur broadly in this

1122
01:38:11,040 --> 01:38:13,920
case. I'm not meaning like I'm building a business trying to sell something. I mean,

1123
01:38:13,920 --> 01:38:20,000
like I'm trying to sell ideas. And this is the challenge as to how you get people to pay attention

1124
01:38:20,000 --> 01:38:25,360
to you. How do you get them to give you positive or negative feedback? How do you get to people

1125
01:38:25,360 --> 01:38:29,520
act differently based on your ideas? So, you know, we'll see how well we do on that.

1126
01:38:30,080 --> 01:38:34,800
So, you know, that there's a lot of hype behind artificial intelligence currently. Do you,

1127
01:38:36,240 --> 01:38:42,400
as, as you look to spread the ideas that are of New York cortical theory of the things you're

1128
01:38:42,400 --> 01:38:47,120
working on, do you think there's some possibility we'll hit an AI winter once again?

1129
01:38:47,120 --> 01:38:49,440
Yeah, it's certainly a possibility. No question about it.

1130
01:38:50,160 --> 01:38:56,400
Yeah. Well, I guess, do I worry about it? I haven't decided yet if that's good or bad for

1131
01:38:56,400 --> 01:39:03,680
my mission. That's true. That's very true because it's almost like you need the winter to refresh

1132
01:39:03,680 --> 01:39:08,880
the pallet. Yeah. So, it's like, I want, here's what you want to have it is you want like,

1133
01:39:09,520 --> 01:39:16,000
to the extent that everyone is so thrilled about the current state of machine learning and AI and

1134
01:39:16,080 --> 01:39:21,040
they don't imagine they need anything else. It makes my job harder. Right. If, if everything

1135
01:39:21,040 --> 01:39:25,600
crashed completely and every student left the field and there was no money for anybody to do

1136
01:39:25,600 --> 01:39:29,200
anything and it became an embarrassment to talk about machine intelligence and AI, that wouldn't

1137
01:39:29,200 --> 01:39:33,840
be good for us either. You want, you want sort of the soft landing approach, right? You want enough

1138
01:39:33,840 --> 01:39:38,880
people, the senior people in AI and machine learning and say, you know, we need other approaches. We

1139
01:39:38,880 --> 01:39:42,960
really need other approaches. Damn, we need other approaches. Maybe we should look to the brain.

1140
01:39:42,960 --> 01:39:46,800
Okay, let's look to the brain. Who's got some brain ideas? Okay, let's, let's start a little

1141
01:39:46,800 --> 01:39:51,200
project on the side here, trying to do a brain idea related stuff. That's the ideal outcome we

1142
01:39:51,200 --> 01:39:56,320
would want. So, I don't want a total winter and yet I don't want it to be sunny all the time either.

1143
01:39:57,520 --> 01:40:01,760
So, what do you think it takes to build a system with human level intelligence

1144
01:40:02,960 --> 01:40:08,560
where once demonstrated, you would be very impressed? So, does it have to have a body?

1145
01:40:08,560 --> 01:40:13,360
Does it have to have the, the, the C word we used before consciousness

1146
01:40:15,600 --> 01:40:21,280
as, as, as an entirety as a holistic sense? First of all, I don't think the goal is to create a

1147
01:40:21,280 --> 01:40:25,840
machine that is human level intelligence. I think it's a false goal. Back to Turing, I think it was

1148
01:40:25,840 --> 01:40:30,160
a false statement. We want to understand what intelligence is and then we can build intelligent

1149
01:40:30,160 --> 01:40:35,120
machines of all different scales, all different capabilities. You know, a dog is intelligent.

1150
01:40:35,120 --> 01:40:39,040
I don't need, you know, that'd be pretty good to have a dog, you know, but what about something

1151
01:40:39,040 --> 01:40:44,320
that doesn't look like an animal at all in different spaces. So, my thinking about this is that we

1152
01:40:44,320 --> 01:40:49,600
want to define what intelligence is, agree upon what makes an intelligent system. We can then say,

1153
01:40:49,600 --> 01:40:53,680
okay, we're now going to build systems that work on those principles or some subset of them,

1154
01:40:54,240 --> 01:41:00,160
and we can apply them to all different types of problems. And the, the kind, the idea, it's

1155
01:41:00,160 --> 01:41:05,680
like computing. We don't ask, if I take a little, you know, little one chip computer, I don't say,

1156
01:41:05,680 --> 01:41:09,600
well, that's not a computer because it's not as powerful as this, you know, big server over here.

1157
01:41:09,600 --> 01:41:12,800
No, no, because we know that what the principles are computing on, and I can apply those principles

1158
01:41:12,800 --> 01:41:16,800
to a small problem or into a big problem. And same intelligence needs to get there. We have to say,

1159
01:41:16,800 --> 01:41:20,000
these are the principles. I can make a small one, a big one, I can make them distributed, I can

1160
01:41:20,000 --> 01:41:24,000
put them on different sensors. They don't have to be human like at all. Now you did bring up a very

1161
01:41:24,000 --> 01:41:29,120
interesting question about embodiment. Does it have to have a body? It has to have some concept

1162
01:41:29,200 --> 01:41:34,080
of movement. It has to be able to move through these reference frames I talked about earlier.

1163
01:41:34,080 --> 01:41:37,680
I, whether it's physically moving, like I need, if I'm going to have an AI that

1164
01:41:37,680 --> 01:41:41,360
understands coffee cups, it's going to have to pick up the coffee cup and touch it and look at it

1165
01:41:41,360 --> 01:41:47,360
with its, with its eyes and hands or something equivalent to that. If I have a mathematical AI,

1166
01:41:48,160 --> 01:41:53,440
maybe it needs to move through mathematical spaces. I could have a virtual AI that lives

1167
01:41:54,320 --> 01:42:00,880
in the internet and its movements are traversing links and digging into files, but it's got a

1168
01:42:00,880 --> 01:42:07,600
location that it's traveling through some space. You can't have an AI that just takes some flash

1169
01:42:07,600 --> 01:42:14,720
thing input, you know, we call it flash inference. Here's a pattern, done. No, it's movement time,

1170
01:42:14,720 --> 01:42:18,080
movement pattern, movement pattern, movement pattern, attention, digging, building, building

1171
01:42:18,080 --> 01:42:23,040
structure, just figuring out the model of the world. So some sort of embodiment, whether it's

1172
01:42:23,040 --> 01:42:27,920
physical or not, has to be part of it. So self-awareness in the way to be able to answer

1173
01:42:27,920 --> 01:42:31,440
where am I? You're bringing up self-awareness, it's a different topic, self-awareness.

1174
01:42:31,440 --> 01:42:39,040
No, the very narrow definition, meaning knowing a sense of self enough to know where am I in this

1175
01:42:39,040 --> 01:42:44,640
space? Yeah, basically the system, the system needs to know its location or each component of the

1176
01:42:44,640 --> 01:42:50,720
system needs to know where it is in the world at that point in time. So self-awareness and

1177
01:42:50,720 --> 01:42:56,320
consciousness, do you think, one, from the perspective of neuroscience and neocortex,

1178
01:42:56,320 --> 01:43:02,240
these are interesting topics, solvable topics, do you have any ideas of why the heck it is that

1179
01:43:02,240 --> 01:43:05,520
we have a subjective experience at all? Yeah, I have a lot of questions.

1180
01:43:05,520 --> 01:43:08,320
And is it useful or is it just a side effect of us?

1181
01:43:08,320 --> 01:43:13,760
It's interesting to think about. I don't think it's useful as a means to figure out how to

1182
01:43:13,760 --> 01:43:21,280
build intelligent machines. It's something that systems do, and we can talk about what it is,

1183
01:43:21,840 --> 01:43:25,440
that are like, well, if I build a system like this, then it would be self-aware. Or

1184
01:43:26,400 --> 01:43:29,920
if I build it like this, it wouldn't be self-aware. So that's a choice I can have.

1185
01:43:29,920 --> 01:43:36,000
It's not like, oh my god, it's self-aware. I heard an interview recently with this

1186
01:43:36,000 --> 01:43:39,600
philosopher from Yale, I can't remember his name, I apologize for that. But he was talking about,

1187
01:43:39,600 --> 01:43:43,120
well, if these computers were self-aware, then it would be a crime done, plug them. And I'm like,

1188
01:43:43,120 --> 01:43:50,160
oh, come on. I unplug myself every night, I go to sleep. Is that a crime? I plug myself in again

1189
01:43:50,160 --> 01:43:55,920
in the morning, and there I am. So people get kind of bent out of shape about this.

1190
01:43:55,920 --> 01:44:02,160
I have very definite, very detailed understanding or opinions about what it means to be conscious

1191
01:44:02,160 --> 01:44:07,120
and what it means to be self-aware. I don't think it's that interesting a problem. You talk to

1192
01:44:07,120 --> 01:44:12,160
Kristoff Koch, he thinks that's the only problem. I didn't actually listen to your interview with

1193
01:44:12,240 --> 01:44:15,680
him, but I know him, and I know that's the thing he cares about.

1194
01:44:15,680 --> 01:44:19,200
He also thinks intelligence and consciousness have disjoint. So I mean, it's not,

1195
01:44:19,200 --> 01:44:23,600
you don't have to have one or the other. I disagree with that. I just totally disagree with that.

1196
01:44:24,400 --> 01:44:28,160
So where's your thoughts and consciousness? Where does it emerge from? Because it is...

1197
01:44:28,160 --> 01:44:32,000
So then we have to break it down to the two parts, okay? Because consciousness isn't one thing,

1198
01:44:32,000 --> 01:44:35,440
that's part of the problem with that term. It means different things to different people,

1199
01:44:35,440 --> 01:44:40,000
and there's different components of it. There is a concept of self-awareness, okay?

1200
01:44:40,800 --> 01:44:47,360
That can be very easily explained. You have a model of your own body, the neocortex models

1201
01:44:47,360 --> 01:44:54,000
things in the world, and it also models your own body. And then it has a memory. It can remember

1202
01:44:54,000 --> 01:44:58,000
what you've done, okay? So it can remember what you did this morning, can remember what you had

1203
01:44:58,000 --> 01:45:04,480
for breakfast, and so on. And so I can say to you, okay, Lex, were you conscious this morning when

1204
01:45:04,560 --> 01:45:10,160
you had your bagel? And you'd say, yes, I was conscious. Now what if I could take your brain

1205
01:45:10,160 --> 01:45:14,960
and revert all the synapses back to the state they were this morning? And then I said to you,

1206
01:45:14,960 --> 01:45:18,640
Lex, were you conscious when you ate the bagel? And he said, no, I wasn't conscious. I said,

1207
01:45:18,640 --> 01:45:23,200
here's a video of eating the bagel. And he said, I wasn't there. I have no... That's not possible

1208
01:45:23,200 --> 01:45:27,360
because I must have been unconscious at that time. So we can just make this one-to-one correlation

1209
01:45:27,360 --> 01:45:31,920
between memory of your body's trajectory through the world over some period of time,

1210
01:45:31,920 --> 01:45:36,080
a memory of it. And the ability to recall that memory is what you would call conscious. I was

1211
01:45:36,080 --> 01:45:42,160
conscious of that. It's a self-awareness. And any system that can recall, memorize what it's

1212
01:45:42,160 --> 01:45:48,560
done recently, and bring that back and invoke it again would say, yeah, I'm aware. I remember what

1213
01:45:48,560 --> 01:45:53,680
I did. All right, I got it. That's an easy one, although some people think that's a hard one.

1214
01:45:54,640 --> 01:45:59,520
The more challenging part of consciousness is this is one that's sometimes used by the word

1215
01:45:59,520 --> 01:46:08,240
qualia, which is, why does an object seem red? Or what is pain? And why does pain feel like

1216
01:46:08,240 --> 01:46:13,200
something? Why do I feel redness? Or why do I feel a little painless in a way? And then I could say,

1217
01:46:13,200 --> 01:46:16,800
well, why does sight seems different than hearing? That's the same problem. It's really,

1218
01:46:17,360 --> 01:46:22,080
these are all just neurons. And so how is it that why does looking at you feel different than

1219
01:46:22,960 --> 01:46:26,720
hearing you? It feels different, but there's just neurons in my head. They're all doing the same

1220
01:46:26,800 --> 01:46:32,160
thing. So that's an interesting question. The best treatise I've read about this is by a guy named

1221
01:46:32,160 --> 01:46:38,800
O'Regan. O'Regan, he wrote a book called Why Red Doesn't Sound Like a Bell. It's a little,

1222
01:46:40,400 --> 01:46:46,640
it's not a trade book, easy to read, but it, and it's an interesting question. Take something like

1223
01:46:46,640 --> 01:46:51,520
color. Color really doesn't exist in the world. It's not a property of the world. Property of the

1224
01:46:51,520 --> 01:46:57,920
world that exists is light frequency. And that gets turned into we have certain cells in the retina

1225
01:46:57,920 --> 01:47:01,280
that respond to different frequencies, different than others. And so when they enter the brain,

1226
01:47:01,280 --> 01:47:06,720
you just have a bunch of axons that are firing at different rates. And from that, we perceive color.

1227
01:47:06,720 --> 01:47:10,800
But there is no color in the brain. I mean, there's, there's no color coming in on those synapses.

1228
01:47:10,800 --> 01:47:16,160
It's just a correlation between some, some, some axons and some property of frequency.

1229
01:47:16,720 --> 01:47:20,560
And that isn't even color itself. Frequency doesn't have a color. It's just a,

1230
01:47:21,280 --> 01:47:26,400
it's just what it is. So then the question is, well, why does it even appear to have a color at all?

1231
01:47:27,840 --> 01:47:31,440
Just as you're describing it, there seems to be a connection to these, those ideas of reference

1232
01:47:31,440 --> 01:47:40,080
frames. I mean, it just feels like consciousness having the subject, assigning the feeling of red

1233
01:47:40,880 --> 01:47:47,920
to the actual color or to the wavelength is useful for intelligence.

1234
01:47:47,920 --> 01:47:51,920
Yeah, I think that's a good way of putting it. It's useful as a predictive mechanism or useful

1235
01:47:51,920 --> 01:47:56,720
as a generalization idea. It's a way of grouping things together to say it's useful to have a model

1236
01:47:56,720 --> 01:48:04,800
like this. Think about the, the, the well-known syndrome that people who've lost a limb experience

1237
01:48:04,800 --> 01:48:12,720
called phantom limbs. And what they claim is they can have their arms removed, but they feel

1238
01:48:12,720 --> 01:48:17,680
the arm that not only feel it, they know it's there. They, it's there. I can, I know it's there.

1239
01:48:17,680 --> 01:48:21,040
They'll swear to you that it's there and then they can feel pain in their arm and they'll

1240
01:48:21,040 --> 01:48:24,880
feel it in their finger and if they move their, they move their non-existent arm behind their

1241
01:48:24,880 --> 01:48:30,720
back, then they feel the pain behind their back. So this whole idea that your arm exists is a model

1242
01:48:30,720 --> 01:48:37,520
of your brain. It may or may not really exist. And just like, but it's useful to have a model

1243
01:48:37,520 --> 01:48:41,520
of something that sort of correlates to things in the world so you can make predictions about what

1244
01:48:41,520 --> 01:48:45,200
would happen when those things occur. It's a little bit of a fuzzy, but I think you're getting

1245
01:48:45,200 --> 01:48:50,960
quite towards the answer there. It's, it's useful for the model of to, to express things certain

1246
01:48:50,960 --> 01:48:54,880
ways that we can then map them into these reference frames and make predictions about them.

1247
01:48:55,680 --> 01:48:58,560
I need to spend more time on this topic. It doesn't bother me.

1248
01:48:58,800 --> 01:49:04,640
Do you really need to spend more time? It does feel special that we have subjective experience,

1249
01:49:04,640 --> 01:49:09,040
but I'm yet to know why. I'm just, I'm just personally curious. It's not,

1250
01:49:09,040 --> 01:49:13,200
it's not necessary for the work we're doing here. I don't think I need to solve that problem to

1251
01:49:13,200 --> 01:49:18,000
build intelligent machines at all, not at all. But there is sort of the silly notion that you

1252
01:49:18,000 --> 01:49:24,000
described briefly that doesn't seem so silly to us humans is, you know, if you're successful

1253
01:49:24,000 --> 01:49:31,120
building intelligent machines, it feels wrong to then turn them off. Because if you're able to

1254
01:49:31,120 --> 01:49:37,920
build a lot of them, it feels wrong to then be able to, you know, to turn off the,

1255
01:49:38,560 --> 01:49:42,960
Well, why, but just let's, let's break that down a bit. As humans, why do we fear death?

1256
01:49:43,760 --> 01:49:47,920
There's, there's two reasons we fear death. Well, first of all, I'll say when you're

1257
01:49:47,920 --> 01:49:53,040
dead, it doesn't matter. Oh, okay. You're dead. So why do we fear death? We fear death for two

1258
01:49:53,040 --> 01:49:58,960
reasons. One is because we are our program genetically to fear death. That's a, that's a

1259
01:49:58,960 --> 01:50:05,360
survival and prop beginning of the genes thing. And we also are programmed to feel sad when people

1260
01:50:05,360 --> 01:50:09,360
we know die. We don't feel sad for someone we don't know dies. There's people dying right now,

1261
01:50:09,360 --> 01:50:12,080
they're only scared to say, I'm feel bad about them because I don't know them. But I knew them,

1262
01:50:12,080 --> 01:50:18,960
I'd feel really bad. So again, this, these are old brain genetically embedded things that we

1263
01:50:19,040 --> 01:50:25,200
fear death. There's outside of those, those uncomfortable feelings. There's nothing else

1264
01:50:25,200 --> 01:50:30,080
to worry about. Wait, wait, hold on a second. Do you know the denial of death by Becker?

1265
01:50:31,040 --> 01:50:33,760
You know, there's a thought that death is,

1266
01:50:36,320 --> 01:50:44,560
you know, our whole conception of our world model kind of assumes immortality. And then death is

1267
01:50:44,560 --> 01:50:49,600
this terror that underlies it all. So like, well, some people's world model, not mine.

1268
01:50:50,320 --> 01:50:54,400
But okay, so what, what Becker would say is that you're just living in an illusion,

1269
01:50:54,400 --> 01:50:59,520
you've constructed illusion for yourself, because it's such a terrible terror. The fact that

1270
01:51:00,080 --> 01:51:04,240
what's the illusion, the illusion that death doesn't matter, you're still not coming to grips

1271
01:51:04,240 --> 01:51:09,760
with the illusion of what that death is going to happen. Oh, like it's not going to happen.

1272
01:51:10,640 --> 01:51:14,160
You're actually operating. You haven't, even though you said you've accepted it,

1273
01:51:14,160 --> 01:51:17,680
you haven't really accepted the notion of death is what he was saying. So it sounds like,

1274
01:51:19,520 --> 01:51:21,600
it sounds like you disagree with that notion. I mean,

1275
01:51:21,600 --> 01:51:24,240
Yeah, yeah, totally. I, like, I,

1276
01:51:24,240 --> 01:51:27,120
So death is not that such an important. Every night, every night I go to bed,

1277
01:51:27,120 --> 01:51:30,640
it's like dying. With little deaths. It's full of death. And if I didn't wake up,

1278
01:51:31,600 --> 01:51:35,280
it wouldn't matter to me. Only if I knew that was going to happen would it be bothersome. But I

1279
01:51:35,280 --> 01:51:39,360
didn't know it was going to happen. How would I know? Then I would worry about my wife.

1280
01:51:39,440 --> 01:51:44,000
So imagine, imagine I was a loner and I lived in Alaska and I lived them

1281
01:51:44,000 --> 01:51:47,760
out there and there was no animals. Nobody knew I existed. I was just eating these roots all the

1282
01:51:47,760 --> 01:51:55,920
time and nobody knew I was there. And one day I didn't wake up. What pain in the world would there

1283
01:51:55,920 --> 01:52:01,920
exist? Well, so most people that think about this problem would say that you're just deeply enlightened

1284
01:52:01,920 --> 01:52:09,840
or are completely delusional. But I would say, I would say that's a very enlightened

1285
01:52:10,400 --> 01:52:15,760
way to see the world is that that's the rational one. Well, I think it's rational. That's right.

1286
01:52:15,760 --> 01:52:23,440
But the fact is we don't, I mean, we really don't have an understanding of why the heck it is we're

1287
01:52:23,440 --> 01:52:27,840
born and why we die and what happens after we die. Well, maybe there isn't a reason, maybe there is.

1288
01:52:27,840 --> 01:52:32,240
So I'm interested in those big problems too, right? You know, you interviewed Max Tagmark,

1289
01:52:32,240 --> 01:52:35,280
you know, and there's people like that, right? I'm interested in those big problems as well.

1290
01:52:35,280 --> 01:52:41,600
And in fact, when I was young, I made a list of the biggest problems I could think of. First,

1291
01:52:41,600 --> 01:52:47,040
why does anything exist? Second, why did we have the laws of physics that we have? Third,

1292
01:52:47,680 --> 01:52:53,040
is life inevitable? And why is it here? Fourth, is intelligence inevitable? And why is it here?

1293
01:52:53,040 --> 01:52:57,120
I stopped there because I figured if you can make a truly intelligent system,

1294
01:52:57,840 --> 01:53:00,560
we'll be, that'll be the quickest way to answer the first three questions.

1295
01:53:03,200 --> 01:53:08,640
I'm serious. And so I said, my mission, you know, you asked me earlier, my first mission is

1296
01:53:08,640 --> 01:53:12,160
to understand the brain, but I felt that is the shortest way to get to true machine intelligence.

1297
01:53:12,160 --> 01:53:15,920
And I want to get to true machine intelligence because even if it doesn't occur in my lifetime,

1298
01:53:15,920 --> 01:53:19,600
other people will benefit from it because I think it'll occur in my lifetime, but you know,

1299
01:53:19,600 --> 01:53:26,720
20 years, you never know. And but that will be the quickest way for us to, you know, we can make

1300
01:53:26,720 --> 01:53:32,880
super mathematicians, we can make super space explorers, we can make super physicists brains

1301
01:53:32,880 --> 01:53:38,640
that do these things, and that can run experiments that we can't run, we don't have the abilities

1302
01:53:38,640 --> 01:53:42,560
to manipulate things and so on. But we can build and tell the machines to do all those things.

1303
01:53:42,560 --> 01:53:47,360
And with the ultimate goal of finding out the answers to the other questions.

1304
01:53:47,600 --> 01:53:54,800
Let me ask you another depressing and difficult question, which is, once we achieve that goal,

1305
01:53:56,080 --> 01:54:02,960
do you, of creating, no, of understanding intelligence, do you think we would be happier

1306
01:54:02,960 --> 01:54:06,720
and more fulfilled as a species? Understanding intelligence or understanding the answers to

1307
01:54:06,720 --> 01:54:12,880
the big questions? Understanding intelligence. Totally. Totally. It would be a far more fun

1308
01:54:12,880 --> 01:54:18,080
place to live. You think so? Oh, yeah, why not? I mean, you know, just put aside this, you know,

1309
01:54:18,080 --> 01:54:24,880
terminator nonsense and, and, and, and just think about, you can think about the, we can talk about

1310
01:54:24,880 --> 01:54:30,160
the risk of AI if you want. I'd love to. So let's talk about. But I think the world is far better

1311
01:54:30,160 --> 01:54:34,080
knowing things. We're always better than no things. Do you think it's better? Is it a better place to

1312
01:54:34,080 --> 01:54:38,880
live in that I know that our planet is one of many in the solar system and the solar system is one

1313
01:54:38,880 --> 01:54:43,280
of many of the galaxies? I think it's a more, I, I dread, I used to, I sometimes think like,

1314
01:54:43,280 --> 01:54:47,360
God, what would it be like 300 years ago? I'd be looking up the sky. I can't understand anything.

1315
01:54:47,360 --> 01:54:51,120
Oh my God, I'd be like going to bed every night going, what's going on here? Well, I mean, in

1316
01:54:51,120 --> 01:54:56,720
some sense, I agree with you, but I'm not exactly sure. So I'm also a scientist. So I have, I share

1317
01:54:56,720 --> 01:55:03,360
your views, but I'm not, we're, we're like rolling down the hill together. What's down the hill?

1318
01:55:03,360 --> 01:55:07,520
I feel like we're climbing a hill. Whatever. We're getting, we're getting closer to enlightenment.

1319
01:55:07,760 --> 01:55:13,200
Whatever. We're climbing, we're getting pulled up a hill by our curiosity.

1320
01:55:13,200 --> 01:55:16,960
We are putting, our polarity is pulling, we're pulling ourselves up the hill by our curiosity.

1321
01:55:16,960 --> 01:55:23,120
Yeah. Sisyphus are doing the same thing with the rock. Yeah. But okay, our happiness aside,

1322
01:55:23,120 --> 01:55:30,480
do you have concerns about, you know, you talk about Sam Harris, Elon Musk, of existential threats

1323
01:55:30,480 --> 01:55:34,240
of intelligence systems? No, I'm not worried about existential threats at all. There are,

1324
01:55:34,240 --> 01:55:37,680
there are some things we really do need to worry about. Even today's AI, we have things we have

1325
01:55:37,680 --> 01:55:42,640
to worry about. We have to worry about privacy and about how impacts false beliefs in the world.

1326
01:55:42,640 --> 01:55:47,680
And, and we have real problems that, and things to worry about with today's AI.

1327
01:55:48,480 --> 01:55:52,000
And that will continue as we create more intelligent systems. There's no question, you

1328
01:55:52,000 --> 01:55:57,440
know, the whole issue about, you know, making intelligent armaments and weapons is something

1329
01:55:57,440 --> 01:56:01,760
that really we have to think about carefully. I don't think of those as existential threats.

1330
01:56:01,760 --> 01:56:06,400
I think those are the kind of threats we always face, and we'll have to face them here and, and,

1331
01:56:07,280 --> 01:56:12,320
and we'll have to deal with them. The, we can, we could talk about what people think are the

1332
01:56:12,320 --> 01:56:17,360
existential threats. But when I hear people talking about them, they all sound hollow to me.

1333
01:56:17,360 --> 01:56:21,360
They're, they're based on ideas, they're based on people who really have no idea what intelligence

1334
01:56:21,360 --> 01:56:27,440
is. And, and if they knew what intelligence was, they wouldn't say those things. So those are not

1335
01:56:27,440 --> 01:56:32,640
experts in the field, you know. So yeah, so there's two, right? There's, so one is like

1336
01:56:32,640 --> 01:56:43,040
super intelligent. So a system that becomes far, far superior in reasoning ability than us humans.

1337
01:56:43,040 --> 01:56:49,840
How is that an existential threat? Then, so there's a lot of ways in which it could be. One way is

1338
01:56:50,640 --> 01:56:56,000
us humans are actually irrational, inefficient, and get in the way of,

1339
01:56:57,520 --> 01:57:03,600
of not happiness, but whatever the objective function is of maximizing that objective function.

1340
01:57:03,600 --> 01:57:04,880
Yeah. Yeah. Super intelligent.

1341
01:57:04,880 --> 01:57:06,480
There's a paperclip problem and things like that.

1342
01:57:06,480 --> 01:57:09,360
But so the paperclip problem, but with a super intelligent.

1343
01:57:09,360 --> 01:57:13,840
Yeah. Yeah. Yeah. So we already faced this threat in some sense.

1344
01:57:14,480 --> 01:57:20,400
They're called bacteria. These are organisms in the world that would like to turn everything into

1345
01:57:20,400 --> 01:57:25,680
bacteria. And they're constantly morphing. They're constantly changing to evade our protections.

1346
01:57:26,320 --> 01:57:33,200
And in the past, they have killed huge swaths of populations of humans on this planet.

1347
01:57:33,200 --> 01:57:37,440
So if you want to worry about something that's going to multiply endlessly, we have it.

1348
01:57:38,320 --> 01:57:43,200
And I'm far more worried in that regard, I'm far more worried that some scientists in a laboratory

1349
01:57:43,200 --> 01:57:48,400
will create a super virus or a super bacteria that we cannot control. That is a more existential

1350
01:57:48,400 --> 01:57:53,840
threat. Putting an intelligence thing on top of it actually seems to make it less existential

1351
01:57:53,840 --> 01:57:58,480
to me. It's like, it limits its power. It limits where it can go. It limits the number of things

1352
01:57:58,480 --> 01:58:03,920
it can do in many ways. A bacteria is something you can't even see. So that's only one of those

1353
01:58:03,920 --> 01:58:09,680
problems. Yes, exactly. So the other one, just in your intuition about intelligence, when you

1354
01:58:09,680 --> 01:58:15,600
think about the intelligence of us humans, do you think of that as something, if you look at

1355
01:58:15,600 --> 01:58:22,000
intelligence on a spectrum from zero to us humans, do you think you can scale that to something far

1356
01:58:22,000 --> 01:58:25,280
superior? Yeah, all the mechanisms we've been talking about. Let me, I want to make another

1357
01:58:25,280 --> 01:58:31,440
point here, Alex, before I get there. Sure. Intelligence is the neocortex. It is not the

1358
01:58:31,440 --> 01:58:38,240
entire brain. If I, the goal is not to make a human. The goal is not to make an emotional system.

1359
01:58:38,240 --> 01:58:42,640
The goal is not to make a system that wants to have sex and reproduce. Why would I build that?

1360
01:58:42,640 --> 01:58:45,840
If I want to have a system that wants to reproduce and have sex, make bacteria,

1361
01:58:45,840 --> 01:58:50,960
make computer viruses. Those are bad things. Don't do that. Those are really bad. Don't do

1362
01:58:50,960 --> 01:58:56,880
those things. Regulate those. But if I just say I want an intelligent system, why doesn't have to

1363
01:58:56,880 --> 01:59:01,520
have any of the human-like emotions? Why does it even care if it lives? Why does it even care

1364
01:59:01,520 --> 01:59:06,240
if it has food? It doesn't care about those things. It's just, you know, it's just in a trance

1365
01:59:06,240 --> 01:59:11,600
thinking about mathematics or it's out there just trying to build the space, you know, for it on

1366
01:59:11,600 --> 01:59:18,080
Mars. It's a, we, that's a choice we make. Don't make human-like things. Don't make replicating

1367
01:59:18,080 --> 01:59:21,600
things. Don't make things that have emotions. Just stick to the neocortex. So that's, that's a

1368
01:59:21,600 --> 01:59:26,480
view actually that I share, but not everybody shares in the sense that you have faith and

1369
01:59:26,480 --> 01:59:33,520
optimism about us as engineers of systems, humans as builders of systems to, to, to not put in

1370
01:59:34,080 --> 01:59:38,960
stupid, not stupid. So this is why, this is why I mentioned the bacteria one. Because you might

1371
01:59:38,960 --> 01:59:43,280
say, well, some person's going to do that. Well, some person today could create a bacteria that's

1372
01:59:43,280 --> 01:59:49,840
resistant to all the non-antibacterial agents. So we already have that threat. We already know

1373
01:59:49,840 --> 01:59:55,920
this is going on. It's not a new threat. So just accept that and then we have to deal with it,

1374
01:59:55,920 --> 02:00:01,120
right? Yeah. So my point is nothing to do with intelligence. It, intelligence is a separate

1375
02:00:01,200 --> 02:00:05,200
component that you might apply to a system that wants to reproduce and do stupid things.

1376
02:00:05,840 --> 02:00:09,600
Let's not do that. Yeah. In fact, it is a mystery why people haven't done that yet.

1377
02:00:10,400 --> 02:00:16,480
My, my dad as a physicist believes that the reason, for example, nuclear weapons haven't

1378
02:00:17,120 --> 02:00:23,280
proliferated amongst evil people. So one, one belief that I share is that there's not that many

1379
02:00:23,280 --> 02:00:30,400
evil people in the world that would, that, that would use back to whether it's bacteria,

1380
02:00:30,400 --> 02:00:36,720
nuclear weapons, or maybe the future AI systems to do bad. So the fraction is small. And the second

1381
02:00:36,720 --> 02:00:42,560
is that it's actually really hard, technically. So the, the intersection between evil and

1382
02:00:42,560 --> 02:00:47,600
competent is small in terms. And by the way, to really annihilate humanity, you'd have to have,

1383
02:00:48,240 --> 02:00:52,160
you know, sort of the, the nuclear winter phenomenon, which is not one person shooting,

1384
02:00:52,160 --> 02:00:57,600
you know, or even 10 bombs, you'd have to have some automated system that, you know, detonates

1385
02:00:57,600 --> 02:01:02,400
a million bombs or 10, whatever many thousands we have. So it's extreme evil combined with extreme

1386
02:01:02,400 --> 02:01:07,200
competence and just building some stupid system that would automatically, you know, Dr. Strangelup

1387
02:01:07,200 --> 02:01:13,760
type of thing, you know, I mean, look, we could have some nuclear bomb go off in some major city

1388
02:01:13,760 --> 02:01:17,520
in the world. Like, I think that's actually quite likely even in my lifetime. I don't think that's

1389
02:01:17,680 --> 02:01:23,840
unlike the thing. And it'll be a tragedy. But it won't be an existential threat. And it's the same

1390
02:01:23,840 --> 02:01:31,600
as, you know, the virus of 1917, whenever it was, you know, the influenza, these bad things can happen

1391
02:01:31,600 --> 02:01:36,960
and the plague and so on. We can't always prevent it. We always, to always try, but we can't. But

1392
02:01:36,960 --> 02:01:40,320
they're not existential threats until we combine all those crazy things together.

1393
02:01:41,040 --> 02:01:46,240
So on the, on the spectrum of intelligence from zero to human, do you have a sense of

1394
02:01:46,960 --> 02:01:52,880
whether it's possible to create several orders of magnitude or at least double that

1395
02:01:53,520 --> 02:01:55,840
of human intelligence, talking about New York context?

1396
02:01:55,840 --> 02:02:00,080
I think it's the wrong thing to say double the intelligence. Break it down into different

1397
02:02:00,080 --> 02:02:04,880
components. Can I make something that's a million times faster than a human brain? Yes,

1398
02:02:04,880 --> 02:02:11,200
I can do that. Could I make something that is, has a lot more storage than human brain? Yes,

1399
02:02:11,200 --> 02:02:14,720
I could do that. More common, more copies come. Can I make something that attaches to

1400
02:02:14,800 --> 02:02:19,280
different sensors than human brain? Yes, I can do that. Could I make something that's distributed?

1401
02:02:19,280 --> 02:02:22,960
So these people, yeah, we talked earlier about the important New York Cortex voting,

1402
02:02:22,960 --> 02:02:26,160
they don't have to be co-located. Like, you know, they can be all around the places. I could do that,

1403
02:02:26,160 --> 02:02:33,360
too. Those are the levers I have, but is it more intelligent? What depends what I train

1404
02:02:33,360 --> 02:02:39,360
in on? What is it doing? Well, so here's the thing. So let's say larger in New York Cortex

1405
02:02:39,360 --> 02:02:48,480
and or whatever size that allows for higher and higher hierarchies to form. We're talking about

1406
02:02:48,480 --> 02:02:51,920
reference frames and concepts. So I could, could I have something that's a super physicist or

1407
02:02:51,920 --> 02:02:57,120
a super mathematician? Yes. And the question is, once you have a super physicist, will they be

1408
02:02:57,120 --> 02:03:03,040
able to understand something? Do you have a sense that it will be orders like us compared to ants?

1409
02:03:03,040 --> 02:03:11,200
Could we ever understand it? Yeah. Most people cannot understand general relativity.

1410
02:03:11,840 --> 02:03:15,760
It's a really hard thing to get. I mean, you can paint it in a fuzzy picture,

1411
02:03:15,760 --> 02:03:21,280
stretchy space, you know? Yeah. But the field equations to do that in the deep intuitions

1412
02:03:21,280 --> 02:03:27,520
are really, really hard. And I've tried, I'm unable to do it. Like, easy to get, you know,

1413
02:03:27,520 --> 02:03:30,240
it's easy to get special relative, but general relative, man, that's too much.

1414
02:03:32,320 --> 02:03:36,800
And so we already live with this to some extent. The vast majority of people can't understand

1415
02:03:36,800 --> 02:03:41,200
actually what the vast majority of other people actually know. We're just either we don't have

1416
02:03:41,200 --> 02:03:45,600
the effort to or we can't or we don't have time or just not smart enough, whatever. So,

1417
02:03:46,800 --> 02:03:50,800
but we have ways of communicating. Einstein has spoken in a way that I can understand.

1418
02:03:51,520 --> 02:03:57,120
He's given me analogies that are useful. I can use those analogies from my own work and think

1419
02:03:57,120 --> 02:04:03,440
about, you know, concepts that are similar. It's not stupid. It's not like he's exist in some

1420
02:04:03,440 --> 02:04:08,320
of the plane. There's no connection to my plane in the world here. So that will occur. It already

1421
02:04:08,320 --> 02:04:12,800
has occurred. That's my point that this story is it already has occurred. We live it every day.

1422
02:04:14,400 --> 02:04:18,320
One could argue that with we create machine intelligence that think a million times faster

1423
02:04:18,320 --> 02:04:22,240
than us that it'll be so far, we can't make the connections. But, you know, at the moment,

1424
02:04:23,200 --> 02:04:27,280
everything that seems really, really hard to figure out in the world when you actually figure

1425
02:04:27,280 --> 02:04:32,320
it out is not that hard. You know, we can almost everyone can understand the multiverses. Almost

1426
02:04:32,320 --> 02:04:35,920
everyone can understand quantum physics. Almost everyone can understand these basic things,

1427
02:04:35,920 --> 02:04:40,960
even though hardly any people could figure those things out. Yeah, but really understand. So,

1428
02:04:42,160 --> 02:04:45,360
only a few people really don't understand. You need to only understand the

1429
02:04:46,320 --> 02:04:50,800
the projections, the sprinkles of the useful insights from that. That was my example of

1430
02:04:50,800 --> 02:04:55,200
Einstein, right? His general theory of relativity is one thing that very, very, very few people

1431
02:04:55,200 --> 02:04:59,680
can get. And what if we just said those other few people are also artificial intelligences?

1432
02:05:00,480 --> 02:05:05,120
How bad is that? In some sense, they are, right? Yeah, they say already. I mean, Einstein wasn't

1433
02:05:05,120 --> 02:05:09,360
a really normal person. He had a lot of weird quirks. And so the other people who work with him.

1434
02:05:09,360 --> 02:05:13,440
So, you know, maybe they already were sort of this astral plane of intelligence that

1435
02:05:14,160 --> 02:05:19,120
we live with it already. It's not a problem. It's still useful and, you know.

1436
02:05:20,080 --> 02:05:24,000
So, do you think we are the only intelligent life out there in the universe?

1437
02:05:24,800 --> 02:05:30,240
I would say that intelligent life has and will exist elsewhere in the universe. I'll say that.

1438
02:05:31,360 --> 02:05:35,440
There is a question about contemporaneous intelligence life, which is hard to even answer

1439
02:05:35,440 --> 02:05:40,720
when we think about relativity and the nature of space time. We can't say what exactly is this

1440
02:05:40,720 --> 02:05:48,000
time someplace else in the world. But I think it's, you know, I do worry a lot about the filter

1441
02:05:48,000 --> 02:05:55,120
idea, which is that perhaps intelligent species don't last very long. And so we haven't been around

1442
02:05:55,120 --> 02:05:59,760
very long. And as a technological species, we've been around for almost nothing, right? You know,

1443
02:05:59,760 --> 02:06:05,360
what 200 years or something like that. And we don't have any data, a good data point on whether

1444
02:06:05,360 --> 02:06:10,640
it's likely that we'll survive or not. So, do I think that there have been intelligent

1445
02:06:10,640 --> 02:06:14,880
life elsewhere in the universe? Almost certainly, of course. In the past and the future, yes.

1446
02:06:16,320 --> 02:06:21,040
Does it survive for a long time? I don't know. This is another reason I'm excited about our work,

1447
02:06:21,040 --> 02:06:27,120
is our work meaning the general world of AI. I think we can build intelligent machines

1448
02:06:28,640 --> 02:06:35,040
that outlast us. And, you know, they don't have to be tied to earth. They don't have to,

1449
02:06:35,040 --> 02:06:39,600
you know, I'm not saying they're recreating, you know, you know, aliens. I'm just saying

1450
02:06:40,800 --> 02:06:44,960
if I asked myself, and this might be a good point to end on here. If I asked myself, you know,

1451
02:06:44,960 --> 02:06:49,760
what's special about our species? We're not particularly interesting physically. We're not,

1452
02:06:49,760 --> 02:06:53,840
we don't fly. We're not good swimmers. We're not very fast. We're not very strong, you know.

1453
02:06:53,840 --> 02:06:57,760
It's our brain. That's the only thing. And we are the only species on this planet that's built

1454
02:06:57,760 --> 02:07:02,320
the model of the world that extends beyond what we can actually sense. We're the only people who

1455
02:07:02,320 --> 02:07:07,280
know about the far side of the moon and the other universes and other galaxies and other stars and

1456
02:07:08,400 --> 02:07:13,200
what happens in the atom. That knowledge doesn't exist anywhere else. It's only in our heads.

1457
02:07:13,760 --> 02:07:18,240
Cats don't do it. Dogs don't do it. Monkeys don't do it. That is what we've created that's unique.

1458
02:07:18,240 --> 02:07:24,400
Not our genes. It's knowledge. And if I ask me, what is the legacy of humanity? What should our

1459
02:07:24,400 --> 02:07:28,880
legacy be? It should be knowledge. We should preserve our knowledge in a way that it can exist

1460
02:07:28,880 --> 02:07:33,680
beyond us. And I think the best way of doing that, in fact, you have to do it, is that it has to go

1461
02:07:33,680 --> 02:07:39,200
along with intelligent machines to understand that knowledge. That's a very broad idea,

1462
02:07:39,840 --> 02:07:44,320
but we should be thinking, I call it a state planning for humanity. We should be thinking

1463
02:07:44,320 --> 02:07:49,760
about what we want to leave behind when as a species we're no longer here. And that'll happen

1464
02:07:49,760 --> 02:07:54,400
sometime. Sooner or later, it's going to happen. And understanding intelligence and creating

1465
02:07:54,400 --> 02:07:59,840
intelligence gives us a better chance to prolong. It does give us a better chance to prolong life,

1466
02:08:00,400 --> 02:08:05,360
yes. It gives us a chance to live on other planets. But even beyond that, I mean, our

1467
02:08:05,360 --> 02:08:10,560
solar system will disappear one day. It's given enough time. So I don't know. I doubt we will

1468
02:08:10,560 --> 02:08:16,320
ever be able to travel to other things, but we could tell the stars, but we could send intelligent

1469
02:08:16,320 --> 02:08:24,320
machines to do that. So you have an optimistic, a hopeful view of our knowledge of the

1470
02:08:24,320 --> 02:08:29,120
echoes of human civilization living through the intelligent systems we create.

1471
02:08:29,120 --> 02:08:32,160
Oh, totally. Well, I think the intelligent systems are greater in some sense, the

1472
02:08:32,720 --> 02:08:38,480
vessel for bringing them beyond Earth or making them last beyond humans themselves.

1473
02:08:39,040 --> 02:08:43,600
So... And how do you feel about that? That they won't be human, quote unquote.

1474
02:08:43,600 --> 02:08:47,920
Okay, it's not. But human, what is human? Our species are changing all the time.

1475
02:08:48,560 --> 02:08:53,840
Human today is not the same as human just 50 years ago. It's, what is human? Do we care about

1476
02:08:53,840 --> 02:08:58,320
our genetics? Why is that important? As I point out, our genetics are no more interesting than

1477
02:08:58,320 --> 02:09:02,880
a bacterium's genetics. It's no more interesting than a monkey's genetics. What we have, what's

1478
02:09:02,880 --> 02:09:08,640
unique and what's valuable is our knowledge, what we've learned about the world. And that

1479
02:09:08,640 --> 02:09:13,280
is the rare thing. That's the thing we want to preserve. We care about our genes.

1480
02:09:15,360 --> 02:09:19,520
It's the knowledge. It's the knowledge. That's a really good place to end. Thank you so much

1481
02:09:19,520 --> 02:09:21,520
for talking to me. Oh, it was fun.

