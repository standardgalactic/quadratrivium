start	end	text
0	2720	The following is a conversation with Gary Marcus.
2720	8160	He's a professor emeritus at NYU, founder of robust AI and geometric intelligence.
8160	12800	The latter is a machine learning company that was acquired by Uber in 2016.
13440	18160	He's the author of several books on natural and artificial intelligence,
18160	22560	including his new book, Rebooting AI, Building Machines We Can Trust.
23440	28800	Gary has been a critical voice highlighting the limits of deep learning and AI in general
28800	35680	and discussing the challenges before our AI community that must be solved in order to achieve
35680	41360	artificial general intelligence. As I'm having these conversations, I try to find paths toward
41360	46640	insight towards new ideas. I try to have no ego in the process. It gets in the way.
47520	53760	I'll often continuously try on several hats, several roles. One, for example, is the role of
53760	59520	a three-year-old who understands very little about anything and asks big what and why questions.
60240	64800	The other might be a role of a devil's advocate who presents counter-ideas with the goal of
64800	71120	arriving at greater understanding through debate. Hopefully, both are useful, interesting,
71120	76880	and even entertaining at times. I ask for your patience as I learn to have better conversations.
77760	82960	This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube,
82960	88480	give it five stars on iTunes, support it on Patreon, or simply connect with me on Twitter
88480	96000	at Lex Freedman, spelled F-R-I-D-M-A-N. And now, here's my conversation with Gary Marcus.
97200	102960	Do you think human civilization will one day have to face an AI-driven technological singularity
102960	109040	that will, in a societal way, modify our place in the food chain of intelligent living beings
109040	113760	on this planet? I think our place in the food chain has already changed.
114720	118560	So there are lots of things people used to do by hand that they do with machine.
119120	123120	If you think of a singularity as one single moment, which is, I guess, what it suggests,
123120	127280	I don't know if it'll be like that. But I think that there's a lot of gradual change,
127280	131600	and AI is getting better and better. I'm here to tell you why I think it's not nearly as good
131600	137280	as people think. But the overall trend is clear. Maybe Ray Kurzweil thinks it's an exponential,
137280	141360	and I think it's linear. In some cases, it's close to zero right now, but it's all going to
141360	147760	happen. I mean, we are going to get to human-level intelligence or whatever you want. Artificial
147760	152160	general intelligence at some point. And that's certainly going to change our place in the food
152160	156160	chain because a lot of the tedious things that we do now, we're going to have machines do,
156160	160640	and a lot of the dangerous things that we do now, we're going to have machines do. I think our whole
160640	165760	lives are going to change from people finding their meaning through their work, through people
165760	173200	finding their meaning through creative expression. So the singularity will be a very gradual,
173920	179200	in fact, removing the meaning of the word singularity. It'll be a very gradual transformation
179200	184720	in your view. I think that it'll be somewhere in between, and I guess it depends what you mean
184720	188720	by gradual and sudden. I don't think it's going to be one day. I think it's important to realize
188720	194720	that intelligence is a multi-dimensional variable. So people write this stuff as if
195760	202880	IQ was one number, and the day that you hit 262 or whatever, you displace the human beings.
202880	206640	And really, there's lots of facets to intelligence. So there's verbal intelligence,
206640	211440	and there's motor intelligence, and there's mathematical intelligence, and so forth.
212080	217440	Machines, in their mathematical intelligence, far exceed most people already in their ability
217440	221680	to play games. They far exceed most people already. In their ability to understand language,
221680	226240	they lag behind my five-year-old, far behind my five-year-old. So there are some facets of
226240	231360	intelligence, the machines of graphs, and some that they haven't. And we have a lot of work left
231360	237680	to do to get them to, say, understand natural language or to understand how to flexibly approach
237680	244320	some kind of novel MacGyver problem-solving kind of situation. And I don't know that all of these
244320	249280	things will come once. I think there are certain vital prerequisites that we're missing now.
249840	253360	For example, machines don't really have common sense now. So they don't
253360	258000	understand that bottles contain water, and that people drink water to quench their thirst,
258000	262080	and that they don't want to dehydrate. They don't know these basic facts about human beings. And
262080	267040	I think that that's a great limiting step for many things. It's a great limiting step for reading,
267040	271440	for example, because stories depend on things like, oh my god, that person's running out of water.
271440	276400	That's why they did this thing. Or if they only had water, they could put out the fire.
276960	283360	So you watch a movie, and you're knowledge about how things work matter. And so a computer can't
283360	286560	understand that movie if it doesn't have that background knowledge. Same thing if you read a
286560	293200	book. And so there are lots of places where if we had a good machine interpretable set of common
293200	299280	sense, many things would accelerate relatively quickly. But I don't think even that is a single
299280	305120	point. There's many different aspects of knowledge. And we might, for example, find that we make a lot
305120	310400	of progress on physical reasoning, getting machines to understand, for example, how keys fit into
310400	318320	locks or that kind of stuff, or how this gadget here works, and so forth. And so machines might do
318320	323920	that long before they do really good psychological reasoning, because it's easier to get labeled
323920	330160	data or to do direct experimentation on a microphone stand than it is to do direct
330160	334480	experimentation on human beings to understand the levers that guide them.
334720	339600	That's a really interesting point, actually, whether it's easier to gain common sense knowledge
339600	344240	or psychological knowledge. I would say that common sense knowledge includes both physical
344240	348080	knowledge and psychological knowledge. And the argument I was making.
348080	351520	Physical versus psychological. Yeah, physical versus psychological. And the argument I was
351520	355200	making is physical knowledge might be more accessible, because you could have a robot,
355200	360240	for example, lift a bottle, try putting a bottle cap on it, see that it falls off if it does this,
360320	363760	and see that it could turn it upside down. And so the robot could do some experimentation.
364640	370640	We do some of our psychological reasoning by looking at our own minds. So I can sort of guess
370640	374640	how you might react to something based on how I think I would react to it. And robots don't have
374640	379680	that intuition. And they also can't do experiments on people in the same way, or we'll probably
379680	386880	shut them down. So if we wanted to have robots figure out how I respond to pain by pinching
386880	390880	me in different ways, that's probably not going to make it past the human subjects board,
390880	395200	and companies are going to get sued or whatever. So there's certain kinds of practical
395200	401040	experience that are limited or off limits to robots. That's a really interesting point.
401040	410480	What is more difficult to gain a grounding in? Because to play devil's advocate, I would say
410480	418320	that human behavior is easier expressed in data and digital form. And so when you look at Facebook
418320	423920	algorithms, they get to observe human behavior. So you get to study and manipulate even a human
423920	430480	behavior in a way that you perhaps cannot study or manipulate the physical world. So it's true
430480	437120	why you said pain is like physical pain. But that's again the physical world. Emotional pain
437120	443360	might be much easier to experiment with, perhaps unethical, but nevertheless, some would argue
443360	450800	it's already going on. I think that you're right, for example, that Facebook does a lot of experimentation
450800	458320	in psychological reasoning. In fact, Zuckerberg talked about AI at a talk that he gave NIPPS.
458320	462160	I wasn't there, but the conference has been renamed NeurOps, but he used to be called NIPPS when he
462160	466720	gave the talk. And he talked about Facebook basically having a gigantic theory of mind.
467840	472240	I think it is certainly possible. Facebook does some of that. I think they have a really good
472240	476320	idea of how to addict people to things. They understand what draws people back to things.
476320	479840	And I think they exploit it in ways that I'm not very comfortable with. But even so,
480800	486160	I think that there are only some slices of human experience that they can access through
486160	489120	the kind of interface they have. And of course, they're doing all kinds of VR stuff. And maybe
489120	494000	that'll change and they'll expand their data. And I'm sure that that's part of their goal.
494720	503200	So, it is an interesting question. I think love, fear, insecurity, all of the things that
504160	508800	I would say some of the deepest things about human nature and the human mind could be
508800	513520	exported to digital form. You're actually the first person just now that brought up.
513520	521120	I wonder what is more difficult because I think folks who are the slow and we'll talk a lot about
521120	525920	deep learning, but the people who are thinking beyond deep learning are thinking about the physical
525920	531680	world. You're starting to think about robotics in the home robotics. How do we make robots manipulate
531680	535840	objects which requires an understanding of the physical world and then requires common sense
535840	540800	reasoning? And that has felt to be like the next step for common sense reasoning. But
540800	544640	you've not brought up the idea that there's also the emotional part. And it's interesting
544640	550000	whether that's hard or easy. I think some parts of it are and some aren't. So, my company that I
550000	557040	recently founded with Brod Brooks from MIT for many years and so forth, we're interested in both.
557040	561120	We're interested in physical reasoning and psychological reasoning, among many other things.
563360	567680	There are pieces of each of these that are accessible. So, if you want a robot to figure
567680	572320	out whether it can fit under a table, that's a relatively accessible piece of physical reasoning.
573440	576880	If you know the height of the table and you know the height of the robot, it's not that hard.
576880	581360	If you wanted to do physical reasoning about Jenga, it gets a little bit more complicated.
581360	586800	And you have to have higher resolution data in order to do it. With psychological reasoning,
586800	591280	it's not that hard to know, for example, that people have goals and they like to act on those
591280	596960	goals. But it's really hard to know exactly what those goals are. But ideas of frustration. I mean,
596960	601360	you could argue it's extremely difficult to understand the sources of human frustration
601360	607200	as they're playing Jenga with you or not. Yeah, I mean, that is very accessible.
607920	612960	There's some things that are going to be obvious and some not. So, I don't think anybody really
612960	618960	can do this well yet, but I think it's not inconceivable to imagine machines in the not so
618960	624960	distant future being able to understand that if people lose in a game that they don't like that.
625600	629920	Right. You know, that's not such a hard thing to program and it's pretty consistent across people.
629920	634480	Most people don't enjoy losing and so, you know, that makes it relatively easy to code.
634480	638320	On the other hand, if you wanted to capture everything about frustration, well, people
638320	642160	get frustrated for a lot of different reasons. They might get sexually frustrated,
642160	646320	they might get frustrated, they can get their promotion at work, all kinds of different things.
647040	651520	And the more you expand the scope, the harder it is for anything like the existing techniques to
651520	657280	really do that. So, I'm talking to Gary Kasparov next week and he seemed pretty frustrated with
657280	661200	this game against Deep Blue. So, yeah, well, I'm frustrated with my game against him last year
661200	665920	because I played him. I had two excuses. I'll give you my excuses up front. It won't mitigate the
665920	672240	outcome. I was jet lagged and I hadn't played in 25 or 30 years, but the outcome is he completely
672240	678960	destroyed me and it wasn't even close. Have you ever been beaten in any board game by a machine?
679600	687120	I have. I actually played the predecessor to Deep Blue. Deep thought, I believe it was
687120	695200	called and that too crushed me. And after that, you realized it's over for us.
695200	698000	Well, there's no point in my playing Deep Blue. I mean, it's a waste of Deep Blue's
699120	703040	computation. I mean, I played Kasparov because we both gave lectures this
704080	708080	same event and he was playing 30 people. I forgot to mention that not only did he crush me, but he
708080	714800	crushed 29 other people at the same time. I mean, but the actual philosophical and emotional
714800	720880	experience of being beaten by a machine, I imagine, is to you who thinks about these things
721440	727600	may be a profound experience or no, it was a simple mathematical experience.
727600	732560	Yeah, I think a game like chess, particularly where it's, you know, you have perfect information,
732560	737600	it's two player closed end and there's more computation for the computer. It's no surprise
737600	744720	the machine. I mean, I'm not sad when a computer calculates a cube root faster
744800	748720	than me. Like, I know I can't win that game. I'm not going to try.
748720	754400	Well, with a system like AlphaGo or AlphaZero, do you see a little bit more magic in a system
754400	759040	like that, even though it's simply playing a board game, but because there's a strong learning
759040	762880	component? You know, I find you should mention that in the context of this conversation because
762880	767040	Kasparov and I are working on an article that's going to be called AI is not magic.
768160	771440	And you know, neither one of us thinks that it's magic. And part of the point of this
771440	776000	article is that AI is actually a grab bag of different techniques and some of them have,
776000	782720	or they each have their own unique strengths and weaknesses. So, you know, you read media accounts
782720	788400	and it's like, ooh, AI, it must be magical or can solve any problem. Well, no, some problems
788400	793280	are really accessible like chess and go and other problems like reading are completely outside the
794000	798400	current technology. And it's not like you can take the technology that drives AlphaGo
798400	803360	and apply it to reading and get anywhere. You know, DeepMind has tried that a bit. They have
803360	808080	all kinds of resources. You know, they built AlphaGo and they have, you know, they, I wrote a piece
808080	813280	recently that they lost, and you can argue about the word lost, but they spent $530 million more
813280	817840	than they made last year. So, you know, they're making huge investments. They have a large budget
817840	824800	and they have applied the same kinds of techniques to reading or to language. It's just much less
824800	829200	productive there because it's a fundamentally different kind of problem. Chess and go and so
829200	833840	forth are closed-in problems. The rules haven't changed in 2,500 years. There's only so many
833840	837840	moves you can make. You can talk about the ex-financial as you look at the combinations of
837840	842400	moves, but fundamentally, you know, the Go board has 361 squares. That's it. That's the only, you
842400	848000	know, those intersections are the only places that you can place your stone. Whereas when you're
848000	853360	reading, the next sentence could be anything. You know, it's completely up to the writer what
853360	857600	they're going to do next. That's fascinating that you think this way. You're clearly a brilliant
857600	862000	mind who points out the emperor has no clothes, but so I'll play the role of a person who says-
862000	864160	You're gonna put clothes on the emperor? Good luck with it.
864160	870080	Romanticizes the notion of the emperor, period, suggesting that clothes don't even matter.
870080	877280	Okay, so that's really interesting that you're talking about language. So there's the physical
877280	882240	world of being able to move about the world, making an omelet and coffee and so on. There's
882240	888880	language where you first understand what's being written and then maybe even more complicated than
888880	894480	that having a natural dialogue. And then there's the game of Go and chess. I would argue that
895040	901360	language is much closer to Go than it is to the physical world. Like it is still very constrained.
901360	906640	When you say the possibility of the number of sentences that could come, it is huge, but it
906640	912800	nevertheless is much more constrained. It feels maybe I'm wrong than the possibilities that the
912800	917760	physical world brings us. There's something to what you say in some ways in which I disagree. So
919120	924720	one interesting thing about language is that it abstracts away. This bottle, I don't know if it'll
924720	929680	be in the field of view, is on this table. And I use the word on here. And I can use the word
929760	937600	on here, maybe not here. But that one word encompasses in analog space a sort of infinite
937600	944560	number of possibilities. So there is a way in which language filters down the variation of the world.
945680	951200	And there's other ways. So we have a grammar and more or less, you have to follow the rules of
951200	955360	that grammar. You can break them a little bit, but by and large, we follow the rules of grammar.
955360	959360	And so that's a constraint on language. So there are ways in which language is a constrained system.
959440	964880	On the other hand, there are many arguments that say there's an infinite number of possible sentences,
964880	969600	and you can establish that by just stacking them up. So I think there's water on the table. You
969600	973280	think that I think there's water on the table. Your mother thinks that you think that I think
973280	977520	the water is on the table. Your brother thinks that maybe your mom is wrong to think that you
977520	983520	think that I think. We can make sentences of infinite length or we can stack up adjectives.
983520	987840	This is a very silly example, a very, very silly example, a very, very, very, very, very, very,
988480	992480	example and so forth. There are good arguments that there's an infinite range of sentences. In
992480	997680	any case, it's vast by any reasonable measure. And for example, almost anything in the physical
997680	1003200	world we can talk about in the language world. And interestingly, many of the sentences that we
1003200	1008240	understand, we can only understand if we have a very rich model of the physical world. So I don't
1008240	1012960	ultimately want to adjudicate the debate that I think you just set up, but I find it interesting.
1013920	1019040	You know, maybe the physical world is even more complicated than language. I think that's fair,
1019040	1023040	but you think that language is really, really complicated. It's hard.
1023040	1026000	It's really, really hard. Well, it's really, really hard for machines,
1026000	1030400	for linguists, people trying to understand it. It's not that hard for children and that's part of
1030400	1035280	what's driven my whole career. I was a student of Stephen Pinkers and we were trying to figure out
1035280	1040400	why kids could learn language when machines couldn't. I think we're going to get into language,
1040480	1045040	we're going to get into communication intelligence and neural networks and so on. But let me
1046640	1055200	return to the high level of the futuristic for a brief moment. So you've written in your book,
1055840	1061280	in your new book, it will be arrogant to suppose that we could forecast where AI will be,
1061280	1066720	where the impact it will have in a thousand years or even 500 years. So let me ask you to be arrogant.
1067440	1072960	What do AI systems with or without physical bodies look like a hundred years from now?
1075440	1080000	You can't predict, but if you were to philosophize and imagine, do.
1080000	1083920	Can I first justify the arrogance before you try to push me beyond it?
1083920	1084240	Sure.
1085760	1089600	I mean, there are examples. Like, you know, people figured out how electricity worked.
1089600	1093600	They had no idea that that was going to lead to cell phones, right? I mean,
1093600	1099360	things can move awfully fast once new technologies are perfected. Even when they made transistors,
1099360	1102480	they weren't really thinking that cell phones would lead to social networking.
1103360	1108640	There are nevertheless predictions of the future, which are statistically unlikely to come to be,
1108640	1110720	but nevertheless, is the best. You're asking me to be wrong.
1111360	1113920	Asking you to be... In which way would I like to be wrong?
1113920	1119680	Pick the least unlikely to be wrong thing, even though it's most very likely to be wrong.
1119680	1122080	I mean, here's some things that we can safely predict, I suppose.
1122880	1125920	We can predict that AI will be faster than it is now.
1127120	1132800	It will be cheaper than it is now. It will be better in the sense of being more general
1132800	1141600	and applicable in more places. It will be pervasive. I mean, these are easy predictions.
1141600	1146000	I'm sort of modeling them in my head on Jeff Bezos' famous predictions. He says,
1146000	1148880	I can't predict the future, not in every way. I'm paraphrasing,
1149680	1153120	but I can predict that people will never want to pay more money for their stuff.
1153120	1155120	They're never going to want it to take longer to get there.
1156480	1159440	You can't predict everything, but you can predict something. Sure, of course,
1159440	1165680	it's going to be faster and better. We can't really predict is the full scope of
1166720	1171760	where AI will be in a certain period. I mean, I think it's safe to say that,
1171760	1177200	although I'm very skeptical about current AI, that it's possible to do much better.
1177760	1182000	There's no in-principled argument that says AI is an insolvable problem,
1182000	1185040	that there's magic inside our brains that will never be captured. I mean,
1185040	1187600	I've heard people make those kind of arguments. I don't think they're very good.
1188960	1196400	So AI is going to come and probably 500 years is plenty to get there. And then once it's here,
1196400	1200640	it really will change everything. So when you say AI is going to come,
1200640	1205920	are you talking about human level intelligence? I like the term general
1205920	1210640	intelligence. So I don't think that the ultimate AI, if there is such a thing,
1210640	1214240	is going to look just like humans. I think it's going to do some things that humans do
1214800	1221040	better than current machines like reason flexibly and understand language and so forth.
1221040	1225200	But this doesn't mean they have to be identical to humans. So for example, humans have terrible
1225200	1231440	memory and they suffer from what some people call motivated reasoning. So they like arguments that
1231440	1236160	seem to support them and they dismiss arguments that they don't like. There's no reason that a
1236160	1243840	machine should ever do that. So you see that those limitations of memory as a bug, not a feature?
1243840	1248320	Absolutely. I'll say two things about that. One is I was on a panel with Danny Kahneman,
1248320	1252720	the Nobel Prize winner last night, and we were talking about this stuff. And I think what we
1252720	1258800	converged on is that humans are a low bar to exceed. They may be outside of our skill right now,
1259760	1265920	as AI programmers, but eventually AI will exceed it. So we're not talking about human level AI.
1265920	1269680	We're talking about general intelligence that can do all kinds of different things and do it
1269680	1273600	without some of the flaws that human beings have. The other thing I'll say is I wrote a whole book
1273600	1278880	actually about the flaws of humans. It's actually a nice book end to the counterpoint to the current
1278880	1284160	book. So I wrote a book called Cluj, which was about the limits of the human mind. The current
1284160	1287840	book is kind of about those few things that humans do a lot better than machines.
1288640	1294080	Do you think it's possible that the flaws of the human mind, the limits of memory, our mortality,
1294960	1304720	our bias is a strength, not a weakness. That is the thing that enables from which motivation
1305520	1310080	springs and meaning springs. I've heard a lot of arguments like this. I've never found them
1310080	1316640	that convincing. I think that there's a lot of making lemonade out of lemons. So we, for example,
1316640	1321680	do a lot of free association, where one idea just leads to the next and they're not really that well
1321680	1327040	connected. And we enjoy that and we make poetry out of it and we make movies with free associations
1327040	1332720	and it's fun and whatever. I don't think that's really a virtue of the system. I think that
1333600	1337520	the limitations in human reasoning actually get us in a lot of trouble. For example,
1337600	1341760	politically, we can't see eye to eye because we have the motivational reasoning I was talking
1341760	1346560	about and something related called confirmation bias. So we have all of these problems that
1346560	1351040	actually make for a rougher society because we can't get along because we can't interpret the
1351040	1358080	data in shared ways. And then we do some nice stuff with that. So my free associations are
1358080	1362720	different from yours and you're kind of amused by them and that's great and hence poetry. So
1363600	1368400	there are lots of ways in which we take a lousy situation and make it good. Another example
1368400	1373280	would be our memories are terrible. So we play games like Concentration where you flip over
1373280	1376960	two cards, try to find a pair. Can you imagine a computer playing that? A computer is like,
1376960	1380080	this is the dullest game in the world. I know where all the cards are. I see it once. I know
1380080	1385920	where it is. What are you even talking about? So we make a fun game out of having this terrible
1385920	1393440	memory. So we are imperfect in discovering and optimizing some kind of utility function,
1393440	1397600	but you think in general, there is a utility function. There's an objective function that's
1397600	1403200	better than others. I didn't say that. But see the presumption, when you say...
1404720	1410000	I think you could design a better memory system. You could argue about utility functions and
1411040	1414960	how you want to think about that. But objectively, it would be really nice to do
1414960	1419680	some of the following things. To get rid of memories that are no longer useful.
1421120	1425280	Objectively, that would just be good. And we're not that good at it. So when you park in the same
1425280	1429120	lot every day, you confuse where you parked today with where you parked yesterday with where you
1429120	1433120	parked the day before and so forth. So you blur together a series of memories. There's just no
1433120	1437920	way that that's optimal. I mean, I've heard all kinds of wacky arguments, people trying to defend
1437920	1441600	that. But in the end of the day, I don't think any of them hold water. Or trauma memories of
1441600	1446640	traumatic events would be possibly a very nice feature to have to get rid of those.
1446640	1451280	It'd be great if you could just be like, I'm going to wipe this sector. I'm done with that.
1451920	1455680	I didn't have fun last night. I don't want to think about it anymore. Bye-bye.
1455680	1457040	I'm gone. But we can't.
1457760	1462560	Do you think it's possible to build a system? So you said human-level intelligence is a weird
1462560	1468080	concept. I'm saying I prefer general intelligence. I mean, human-level intelligence is a real thing.
1468160	1471920	And you could try to make a machine that matches people or something like that.
1471920	1477280	I'm saying that per se shouldn't be the objective, but rather that we should learn from humans the
1477280	1481280	things they do well and incorporate that into our AI, just as we incorporate the things that
1481280	1486240	machines do well that people do terribly. So I mean, it's great that AI systems can do all
1486240	1490800	this brute force computation that people can't. And one of the reasons I work on this stuff
1490800	1494400	is because I would like to see machines solve problems that people can't,
1494400	1502080	that in order to be solved would combine the strengths of machines to do all this computation
1502080	1506560	with the ability, let's say, of people to read. So I'd like machines that can read the entire
1506560	1511600	medical literature in a day, 7,000 new papers or whatever the numbers comes out every day.
1511600	1517360	There's no way for any doctor or whatever to read them all. The machine that could read would be a
1517360	1522640	brilliant thing. And that would be strengths of brute force computation combined with kind of
1522720	1526720	subtlety and understanding medicine that, you know, a good doctor or scientist has.
1526720	1529600	So if we can linger a little bit on the idea of general intelligence.
1529600	1534400	So Jan Lacoon believes that human intelligence isn't general at all. It's very narrow.
1535520	1540960	How do you think? I don't think that makes sense. We have lots of narrow intelligences for specific
1540960	1548160	problems. But the fact is like anybody can walk into, let's say a Hollywood movie and reason
1548160	1554080	about the content of almost anything that goes on there. So you can reason about what happens
1554080	1562160	in a bank robbery or what happens when someone is infertile and wants to go to IVF to try to have
1562160	1569280	a child. The list is essentially endless. And not everybody understands every scene in the
1569280	1574960	movie, but there's a huge range of things that pretty much any ordinary adult can understand.
1574960	1582080	His argument is that actually the set of things seems large to us humans because we're very limited
1582800	1587200	in considering the kind of possibilities of experience as they're possible.
1587200	1591920	But in fact, the amount of experience that are possible is infinitely larger.
1592480	1597440	Well, I mean, if you want to make an argument that humans are constrained in what they can
1597440	1604400	understand, I've no issue with that. I think that's right. But it's still not the same thing at all
1604400	1609760	as saying, here's a system that can play go. It's been trained on five million games.
1609760	1614080	And then I say, can it play on a rectangular board rather than a square board? And you say,
1614080	1618960	well, if I retrain it from scratch on another five million games, I can't. That's really,
1618960	1624480	really narrow. And that's where we are. We don't have even a system that could play go.
1625040	1631120	And then without further retraining play on a rectangular board, which any human could do
1631120	1636800	with very little problem. So that's what I mean by narrow. And so it's just wordplay to say
1636800	1642480	semantics. Then it's just words. Then yeah, you mean general in a sense that you can do all kinds
1642480	1648960	of go board shapes flexibly? Well, I mean, that would be like a first step in the right direction.
1648960	1654800	Obviously, that's not what it really meaning. You're kidding. What I mean by general is that you
1654880	1661200	could transfer the knowledge you learn in one domain to another. So if you learn about bank
1661200	1666720	robberies in movies and there's chase scenes, then you can understand that amazing scene in
1666720	1672560	Breaking Bad when Walter White has a car chase scene with only one person. He's the only one in it.
1672560	1677360	And you can reflect on how that car chase scene is like all the other car chase scenes you've
1677360	1683040	ever seen and totally different and why that's cool. And the fact that the number of domains you
1683120	1686960	can do that with is finite doesn't make it less general. So the idea of general is you can just
1686960	1691280	do it on a lot of transfer across a lot of domains. Yeah, I mean, I'm not saying humans are infinitely
1691280	1696240	general or that humans are perfect. I just said a minute ago, it's a low bar, but it's just it's
1696240	1701360	a low bar. But you know, right now, like the bar is here and we're there and eventually we'll get
1701360	1707760	way past it. So speaking of low bars, you've highlighted in your new book as well, but a couple
1707840	1712960	years ago wrote a paper titled Deep Learning a Critical Appraisal that lists 10 challenges
1712960	1720480	faced by current deep learning systems. So let me summarize them as data efficiency, transfer
1720480	1727040	learning, hierarchical knowledge, open-ended inference, explainability, integrating prior
1727040	1733440	knowledge, causal reasoning, modeling on a stable world, robustness, adversarial examples, and so
1733520	1739280	on. And then my favorite problem is reliability and engineering of real world systems. So
1740000	1743280	whatever people can read the paper, they should definitely read the paper, should definitely
1743280	1749600	read your book. But which of these challenges is solved in your view has the biggest impact on the
1749600	1756240	AI community? It's a very good question. And I'm going to be evasive because I think that
1756240	1762000	they go together a lot. So some of them might be solved independently of others. But I think a
1762000	1768320	good solution to AI starts by having real what I would call cognitive models of what's going on.
1768320	1773600	So right now we have an approach that's dominant where you take statistical approximations of
1773600	1779040	things, but you don't really understand them. So you know that bottles are correlated in your data
1779040	1784080	with bottle caps, but you don't understand that there's a thread on the bottle cap that fits with
1784080	1788560	the thread on the bottle and that that's tightens in if I tighten enough that there's a seal and
1788560	1793360	the water can come out. Like there's no machine that understands that. And having a good cognitive
1793360	1797680	model of that kind of everyday phenomena is what we call common sense. And if you had that,
1797680	1803600	then a lot of these other things start to fall into at least a little bit better place. Right now
1803600	1807600	you're like learning correlations between pixels when you play a video game or something like that.
1807600	1811200	And it doesn't work very well. It works when the video game is just the way that you
1811200	1814800	studied it. And then you alter the video game in small ways, like you move the paddle and break
1814800	1819360	out a few pixels and the system falls apart because it doesn't understand, it doesn't have
1819360	1824080	a representation of a paddle, a ball, a wall, a set of bricks and so forth. And so it's reasoning
1824080	1831440	at the wrong level. So the idea of common sense is full of mystery. You've worked on it, but it's
1831440	1837360	nevertheless full of mystery, full of promise. What does common sense mean? What does knowledge
1837360	1841920	mean? So the way you've been discussing it now is very intuitive. It makes a lot of sense that
1841920	1844960	that is something we should have and that's something deep learning systems don't have.
1845520	1852560	But the argument could be that we're oversimplifying it because we're oversimplifying the notion of
1852560	1858480	common sense because that's how it feels like we as humans at the cognitive level approach
1858480	1865120	problems. So a lot of people aren't actually going to read my book. But if they did read the book,
1865120	1869920	one of the things that might come as a surprise to them is that we actually say a common sense
1869920	1875280	is really hard and really complicated. So my critics know that I like common sense, but
1876320	1880880	that chapter actually starts by us beating up not on deep learning, but kind of on our own
1880880	1886640	home team as it will. So Ernie and I are first and foremost people that believe in at least some
1886640	1891360	of what good old fashioned AI tried to do. So we believe in symbols and logic and programming.
1892400	1898720	Things like that are important. And we go through why even those tools that we hold fairly dear
1898720	1904160	aren't really enough. So we talk about why common sense is actually many things. And some of them
1904160	1910320	fit really well with those classical sets of tools. So things like taxonomy. So I know that a bottle
1910320	1915760	is an object or it's a vessel, let's say, and I know a vessel is an object and objects are
1915760	1923280	material things in the physical world. So I can make some inferences. If I know that vessels need to
1923280	1930000	not have holes in them, then I can infer that in order to carry their contents, then I can infer
1930000	1934560	that a bottle shouldn't have a hole in order to carry its contents. So you can do hierarchical
1934560	1939200	inference and so forth. And we say that's great, but it's only a tiny piece of what you need
1940160	1945120	for common sense. We give lots of examples that don't fit into that. So another one that we talk
1945120	1949600	about is a cheese grater. You've got holes in a cheese grater. You've got a handle on top. You
1949600	1955040	can build a model in the game engine sense of a model so that you could have a little cartoon
1955040	1960400	character flying around through the holes of the grater. But we don't have a system yet. Taxonomy
1960400	1964480	doesn't help us that much. It really understands why the handle is on top and what you do with the
1964480	1970000	handle or why all of those circles are sharp or how you'd hold the cheese with respect to the
1970000	1975360	grater in order to make it actually work. Do you think these ideas are just abstractions that could
1975360	1981760	emerge on a system like a very large deep neural network? I'm a skeptic that that kind of emergence
1981760	1987360	per se can work. So I think that deep learning might play a role in the systems that do what I
1987360	1993840	want systems to do, but it won't do it by itself. I've never seen a deep learning system really
1993840	1999440	extract an abstract concept. What they do, principal reasons for that stemming from how
1999440	2005040	back propagation works, how the architectures are set up. One example is deep learning people
2005120	2011360	actually all build in something called convolution, which Jan Lacoon is famous for,
2011360	2016320	which is an abstraction. They don't have their systems learn this. So the abstraction is an
2016320	2020880	object that looks the same if it appears in different places. And what Lacoon figured out and
2020880	2025360	why, you know, essentially why he was a co-winner of the Thuring Ward was that if you programmed
2025360	2031680	this in innately, then your system would be a whole lot more efficient. In principle, this
2031680	2037040	should be learnable, but people don't have systems that kind of reify things that make them
2037040	2042560	more abstract. And so what you really wind up with if you don't program that in advance is a system
2042560	2046880	that kind of realizes that this is the same thing as this, but then I take your little clock there
2046880	2050400	and I move it over and it doesn't realize that the same thing applies to the clock.
2050400	2055280	So the really nice thing you're right that convolution is just one of the things that's
2055280	2060320	like it's an innate feature that's programmed by the human expert, but we need more of those
2060320	2066560	not less. So the, but the nice feature is it feels like that requires coming up with that brilliant
2067200	2075360	idea can get your Thuring Award, but it requires less effort than encoding and something we'll
2075360	2081920	talk about the expert system. So encoding a lot of knowledge by hand. So it feels like one,
2081920	2086320	there's a huge amount of limitations, which you clearly outline with deep learning,
2086320	2090160	but the nice feature deep learning, whatever it is able to accomplish, it does it,
2091680	2094720	it does a lot of stuff automatically without human intervention.
2094720	2099040	Well, and that's part of why people love it, right? But I always think of this quote from
2099040	2104720	Bertrand Russell, which is it has all the advantages of theft over honest toil. It's
2104720	2110880	really hard to program into a machine a notion of causality or, you know, even how a bottle
2110880	2116560	works or what containers are. Ernie Davis and I wrote a, I don't know, 45 page academic paper
2116560	2120960	trying just to understand what a container is, which I don't think anybody ever read the paper,
2120960	2126560	but it's a very detailed analysis of all the things. Well, not even all the some of the things
2126560	2131040	you need to do in order to understand a container. It would be a whole lot nice. And, you know,
2131040	2134720	I'm a co-author on the paper, I made it a little bit better, but Ernie did the hard work for that
2134720	2140480	particular paper. And he took him like three months to get the logical statements correct.
2140560	2145520	And maybe that's not the right way to do it. It's a way to do it. But on that way of doing it,
2146320	2150400	it's really hard work to do something as simple as understanding containers.
2150400	2154800	And nobody wants to do that hard work. Even Ernie didn't want to do that hard work.
2155520	2159520	Everybody would rather just like feed their system in with a bunch of videos with a bunch of
2159520	2164880	containers and have the systems infer how containers work. It would be like so much
2164880	2168880	less effort. Let the machine do the work. And so I understand the impulse. I understand why
2168880	2174480	people want to do that. I just don't think that it works. I've never seen anybody build a system
2174480	2180960	that in a robust way can actually watch videos and predict exactly, you know, which containers would
2180960	2184960	leak and which ones wouldn't or something like. And I know someone's going to go out and do that
2184960	2191120	since I said it and I look forward to seeing it. But getting these things to work robustly is really,
2191120	2196800	really hard. So Yann LeCun, who was my colleague at NYU for many years,
2197600	2203040	thinks that the hard work should go into defining an unsupervised learning algorithm
2203040	2208400	that will watch videos, use the next frame basically in order to tell it what's going on.
2208400	2211680	And he thinks that's the royal road and he's willing to put in the work in
2211680	2217840	devising that algorithm. Then he wants the machine to do the rest. And again, I understand the impulse.
2217840	2223280	My intuition based on years of watching this stuff and making predictions 20 years ago that
2223280	2227040	still hold even though there's a lot more computation and so forth, is that we actually
2227040	2231200	have to do a different kind of hard work, which is more like building a design specification
2231200	2235680	for what we want the system to do, doing hard engineering work to figure out how we do things
2235680	2241600	like what Yann did for convolution in order to figure out how to encode complex knowledge
2241600	2246880	into the systems. The current systems don't have that much knowledge other than convolution,
2246880	2252480	which is again this, you know, objects being in different places and having the same perception,
2252480	2259600	I guess I'll say, same appearance. People don't want to do that work. They don't see how to naturally
2259600	2265440	fit one with the other. I think that's, yes, absolutely. But also on the expert system side,
2265440	2269760	there's a temptation to go too far the other way. So we're just having an expert sort of sit down
2269760	2275280	and encode the description, the framework for what a container is, and then having the system
2275280	2280400	reason the rest. From my view, like one really exciting possibility is of active learning where
2280400	2286320	it's continuous interaction between a human and machine. As the machine, there's kind of deep
2286320	2292720	learning type extraction of information from data patterns and so on. But humans also guiding
2292720	2300960	the learning procedures, guiding both the process and the framework of how the machine
2300960	2304720	learns whatever the task is. I was with you with almost everything you said except the phrase
2304720	2310880	deep learning. What I think you really want there is a new form of machine learning. So
2310880	2315360	let's remember deep learning is a particular way of doing machine learning. Most often it's done
2315360	2320560	with supervised data for perceptual categories. There are other things you can do with deep learning.
2321680	2326480	Some of them quite technical, but the standard use of deep learning is I have a lot of examples
2326480	2331280	and I have labels for them. So here are pictures. This one's the Eiffel Tower. This one's the Sears
2331280	2335120	Tower. This one's the Empire State Building. This one's a cat. This one's a pig and so forth.
2335120	2340400	You just get millions of examples, millions of labels. And deep learning is extremely good at
2340400	2346080	that. It's better than any other solution that anybody has devised. But it is not good at representing
2346080	2353600	abstract knowledge. It's not good at representing things like bottles contain liquid and have tops
2353600	2357760	to them and so forth. It's not very good at learning or representing that kind of knowledge.
2357760	2362880	It is an example of having a machine learn something, but it's a machine that learns a
2362880	2367200	particular kind of thing, which is object classification. It's not a particularly good
2367200	2372080	algorithm for learning about the abstractions that govern our world. There may be such a thing.
2372960	2376800	Part of what we counsel in the book is maybe people should be working on devising such things.
2376800	2383600	So one possibility, I wonder what you think about it, is deep neural networks do form
2383600	2389280	abstractions, but they're not accessible to us humans in terms of we can't.
2389280	2394720	There's some truth in that. So is it possible that either current or future neural networks
2394720	2402080	form very high level abstractions, which are as powerful as our human abstractions of common
2402080	2407600	sense, we just can't get a hold of them. And so the problem is essentially what we need to make
2407600	2412480	them explainable. This is an astute question, but I think the answer is at least partly no.
2413120	2417520	One of the kinds of classical neural network architecture is what we call an auto-associator.
2417520	2422960	It just tries to take an input, goes through a set of hidden layers, and comes out with an output.
2422960	2426880	And it's supposed to learn essentially the identity function, that your input is the same as your
2426880	2430560	output. So you think of those binary numbers, you've got like the 1, the 2, the 4, the 8,
2430560	2435920	the 16, and so forth. And so if you want to input 24, you turn on the 16, you turn on the 8. It's
2436000	2444640	like binary 1, 1, and bunch of zeros. So I did some experiments in 1998 with the precursors of
2445280	2451600	contemporary deep learning. And what I showed was you could train these networks on all the even
2451600	2456240	numbers and they would never generalize to the odd number. A lot of people thought that I was,
2456240	2461600	I don't know, an idiot or faking the experiment or wasn't true or whatever, but it is true that
2461600	2466320	with this class of networks that we had in that day, that they would never ever make this
2466320	2471360	generalization. And it's not that the networks were stupid, it's that they see the world in a
2471360	2476880	different way than we do. They were basically concerned, what is the probability that the
2476880	2482000	rightmost output node is going to be 1? And as far as they were concerned, in everything they'd
2482000	2487760	ever been trained on, it was a 0. That node had never been turned on. And so they figured,
2487760	2491120	why turn it on now? Whereas a person would look at the same problem and say, well,
2491120	2495520	it's obvious, we're just doing the thing that corresponds. The Latin for it is mutatus mutatus,
2495520	2502720	will change what needs to be changed. And we do this, this is what algebra is. So I can do f of x
2502720	2507360	equals y plus two. And I can do it for a couple of values, I can tell you if y is three, then x is
2507360	2510960	five, and if y is four, x is six. And now I can do it with some totally different number,
2510960	2513920	like a million, then you can say, well, obviously it's a million and two, because you have
2513920	2519520	an algebraic operation that you're applying to a variable. And deep learning systems kind of
2519520	2524800	emulate that, but they don't actually do it. The particular example, you could
2526560	2530400	fudge a solution to that particular problem. The general form of that problem remains
2530400	2534240	that what they learn is really correlations between different input and output nodes.
2534240	2539200	They're complex correlations with multiple nodes involved and so forth. But ultimately,
2539200	2544000	they're correlative. They're not structured over these operations over variables. Now, someday,
2544000	2547840	people may do a new form of deep learning that incorporates that stuff. And I think it will
2547840	2552160	help a lot. And there's some tentative work on things like differentiable programming right now
2552160	2556800	that fall into that category. But the sort of classic stuff like people use for ImageNet
2557920	2561120	doesn't have it. And you have people like Hinton going around saying,
2561120	2565680	symbol manipulation like what Marcus, what I advocate is like the gasoline engine,
2565680	2569680	it's obsolete, we should just use this cool electric power that we've got with the deep
2569680	2575840	learning. And that's really destructive, because we really do need to have the gasoline engine stuff
2575840	2582560	that represents, I don't think it's a good analogy, but we really do need to have the stuff that
2582560	2588160	represents symbols. And Hinton as well would say that we do need to throw out everything
2588160	2595520	and start over. Hinton said that to Axios. And I had a friend who interviewed him and
2595520	2599200	tried to pin him down on what exactly we need to throw. And he was very evasive.
2599760	2604400	Well, of course, because we can't, if he knew that he'd throw it out himself. But I mean,
2604400	2607520	you can't have it both ways. You can't be like, I don't know what to throw out,
2607520	2612800	but I am going to throw out the symbols. And not just the symbols, but the variables and
2612800	2616720	the operations over variables. Don't forget the operations over variables, the stuff that I'm
2616720	2623120	endorsing, and which John McCarthy did when he founded AI, that stuff is the stuff that we build
2623120	2627680	most computers out of. There are people now who say, we don't need computer programmers anymore.
2628720	2632320	Not quite looking at the statistics of how much computer programmers actually get paid right now.
2632880	2637680	We need lots of computer programs. And most of them, they do a little bit of machine learning,
2637680	2642480	but they still do a lot of code, right? Code where it's like, if the value of x is greater
2642480	2646720	than the value of y, then do this kind of thing, like conditionals and comparing operations over
2646720	2650880	variables. There's this fantasy you can machine learn anything. There's some things you would
2650880	2655920	never want to machine learn. I would not use a phone operating system that was machine learned.
2655920	2659680	Like you made a bunch of phone calls and you recorded which packets were transmitted and
2659680	2666640	you just machine learned it. It would be insane. Or to build a web browser by taking logs of key
2666640	2672000	strokes and images, screenshots, and then trying to learn the relation between them. Nobody would
2672000	2676800	ever, no rational person would ever try to build a browser that way. They would use symbol
2676800	2681280	manipulation, the stuff that I think AI needs to avail itself of in addition to deep learning.
2682000	2688000	Can you describe what your view of symbol manipulation in its early days? Can you
2688000	2693520	describe expert systems and where do you think they hit a wall or a set of challenges?
2695520	2699600	First, I just want to clarify. I'm not endorsing expert systems per se. You've been
2699600	2703040	kind of contrasting them. There is a contrast, but that's not the thing that I'm endorsing.
2704080	2709360	So expert systems try to capture things like medical knowledge with a large set of rules.
2709360	2714240	So if the patient has this symptom and this other symptom, then it is likely that they have this
2714240	2718800	disease. So there are logical rules and they were symbol manipulating rules of just the sort
2718800	2725040	that I'm talking about. They encode a set of knowledge that the experts have put in.
2725040	2731040	Very explicitly so. So you'd have somebody interview an expert and then try to turn that stuff into
2731040	2737520	rules. At some level, I'm arguing for rules, but the difference is those guys did in the 80s
2737520	2743120	who was almost entirely rules, almost entirely handwritten with no machine learning. What a lot
2743120	2748160	of people are doing now is almost entirely one species of machine learning with no rules.
2748160	2752240	And what I'm counseling is actually a hybrid. I'm saying that both of these things have their
2752240	2756960	advantage. So if you're talking about perceptual classification, how do I recognize a bottle?
2756960	2760320	Deep learning is the best tool we've got right now. If you're talking about making
2760320	2764640	inferences about what a bottle does, something closer to the expert systems is probably still
2765680	2770480	the best available alternative. And probably we want something that is better able to handle
2770560	2774880	quantitative and statistical information than those classical systems typically were.
2774880	2779360	So we need new technologies that are going to draw some of the strengths of both the expert
2779360	2783280	systems and the deep learning, but are going to find new ways to synthesize them.
2783280	2790400	How hard do you think it is to add knowledge at the low level? So mind human intellects
2790400	2795680	to add extra information to symbol manipulating systems.
2796320	2801520	In some domains, it's not that hard, but it's often really hard, partly because a lot of the
2802960	2808320	things that are important, people wouldn't bother to tell you. So if you pay someone on
2808320	2814240	Amazon Mechanical Turk to tell you stuff about bottles, they probably won't even bother to tell
2814240	2820960	you some of the basic level stuff that's just so obvious to a human being and yet so hard to capture
2821040	2828800	in machines. They're going to tell you more exotic things, and they're all well and good,
2828800	2836400	but they're not getting to the root of the problem. So untutored humans aren't very good at knowing,
2836400	2843440	and why should they be, what kind of knowledge the computer system developers actually need.
2843440	2848800	I don't think that that's an irremediable problem. I think it's historically been a problem. People
2849040	2853120	had crowd sourcing efforts, and they don't work that well. There's one at MIT. We're
2853120	2858640	recording this at MIT called Virtual Home, and we talk about this in the book.
2859440	2863920	Find the exact example there, but people were asked to do things like describe an exercise
2863920	2869440	routine. And the things that the people describe it are very low level and don't really capture
2869440	2874640	what's going on. So they're like, go to the room with the television and the weights,
2874640	2880880	turn on the television, press the remote to turn on the television, lift weight, put weight down.
2881360	2886720	It's like very micro level, and it's not telling you what an exercise routine is really about,
2886720	2891120	which is like, I want to fit a certain number of exercises in a certain time period. I want to
2891120	2896000	emphasize these muscles. You want some kind of abstract description. The fact that you happen
2896000	2901360	to press the remote control in this room when you watch this television isn't really the essence of
2902080	2906080	the exercise routine. But if you just ask people what did they do, then they give you
2906080	2912480	this fine grain. And so it takes a little level of expertise about how the AI works in order to
2913120	2917520	craft the right kind of knowledge. So there's this ocean of knowledge that we all operate on.
2917520	2923120	Some of it may not even be conscious, or at least we're not able to communicate it effectively.
2923120	2927360	Yeah, most of it we would recognize if somebody said it if it was true or not,
2927360	2930560	but we wouldn't think to say that it's true or not. It's a really interesting
2931280	2936640	mathematical property. This ocean has the property that every piece of knowledge in it,
2936640	2944080	we will recognize it as true if we're told, but we're unlikely to retrieve it in the reverse.
2944080	2949760	So that interesting property, I would say there's a huge ocean of that knowledge.
2950480	2955120	What's your intuition? Is it accessible to AI systems somehow? Can we?
2956080	2961360	Yeah, I mean, most of it is not, I'll give you an asterisk on this in a second, but most of it is
2961360	2967200	not ever been encoded in machine interpretable form. And so, I mean, if you say accessible,
2967200	2972240	there's two meanings of that. One is like, could you build it into a machine? Yes.
2972240	2978240	The other is like, is there some database that we could go, you know, download and stick into our
2978240	2983120	machine? But the first thing, no. Could we? What's your intuition? I think we could. I think it
2983120	2990720	hasn't been done right. The closest, and this is the asterisk, is the CYC psych system. Try to do
2990720	2996640	this. A lot of logicians worked for Doug Lennon for 30 years on this project. I think they stuck
2996640	3001120	too closely to logic, didn't represent enough about probabilities, tried to hand code it.
3001120	3005840	There are various issues, and it hasn't been that successful. That is the closest
3006800	3013360	existing system to trying to encode this. Why do you think there's not more excitement
3013360	3019520	slash money behind this idea currently? There was. People view that project as a failure. I think that
3019520	3025440	they confused the failure of a specific instance that was conceived 30 years ago for the failure
3025440	3031360	of an approach, which they don't do for deep learning. So, in 2010, people had the same
3031440	3036880	attitude towards deep learning. They're like, this stuff doesn't really work. And, you know,
3036880	3041440	all these other algorithms work better and so forth. And then certain key technical advances
3041440	3046320	were made, but mostly it was the advent of graphics processing units that changed that.
3046320	3051360	It wasn't even anything foundational in the techniques. And there were some new tricks, but
3052880	3057120	mostly it was just more compute and more data, things like ImageNet that didn't exist before,
3057760	3062160	that allowed deep learning. And it could be to work. It could be that, you know,
3062160	3067600	psych just needs a few more things or something like psych. But the widespread view is that
3067600	3072800	that just doesn't work. And people are reasoning from a single example. They don't do that with
3072800	3078240	deep learning. They don't say nothing that existed in 2010. And there were many, many efforts in
3078240	3083760	deep learning was really worth anything. Right? I mean, really, there's no model from 2010
3083840	3087760	in deep learning. So, this is the deep learning that has any commercial value
3087760	3092880	whatsoever at this point, right? They're all failures. But that doesn't mean that there wasn't
3092880	3098720	anything there. I have a friend, I was getting to know him, and he said, I had a company too,
3098720	3103440	I was talking about, I had a new company. He said, I had a company too, and it failed. And I said,
3103440	3108240	well, what did you do? And he said, deep learning. And the problem was he did it in 1986 or something
3108240	3113120	like that. And we didn't have the tools then or 1990. We didn't have the tools then, not the
3113120	3117600	algorithms. His algorithms weren't that different from other algorithms, but he didn't have the GPUs
3117600	3122800	to run it fast enough. He didn't have the data. And so it failed. It could be that
3124240	3131440	simple manipulation per se with modern amounts of data and compute and maybe some advance in
3131440	3138640	compute for that kind of compute might be great. My perspective on it is not that we want to
3138640	3142160	resuscitate that stuff per se, but we want to borrow lessons from it, bring together with
3142160	3147120	other things that we've learned. And it might have an ImageNet moment where it will spark the
3147120	3151920	world's imagination, and it'll be an explosion of simple manipulation efforts. Yeah, I think that
3151920	3159520	people at AI2, Paul Allen's AI Institute, are trying to build datasets that, well, they're not
3159520	3163520	doing it for quite the reason that you say, but they're trying to build datasets that at least
3163520	3168080	spark interest in common sense reasoning. To create benchmarks. Benchmarks for common sense.
3168080	3171920	That's a large part of what the AI2.org is working on right now.
3171920	3176800	So speaking of compute, Rich Sutton wrote a blog post titled Bitter Lesson. I don't know if you've
3176800	3181440	read it, but he said that the biggest lesson that can be read from 70 years of AI research
3181440	3185520	is that general methods that leverage computation are ultimately the most effective.
3186320	3193280	The most effective of what? So they have been most effective for perceptual classification
3193360	3199360	problems, and for some reinforcement learning problems, he works on reinforcement learning.
3199360	3204000	Well, no, let me push back on that. You're actually absolutely right. But I would also
3204000	3211440	say they've been most effective generally, because everything we've done up to the world.
3211440	3217040	Would you argue against that? To me, deep learning is the first thing that has been
3217520	3227120	successful at anything in AI. And you're pointing out that this success is very limited, folks,
3227120	3231600	but has there been something truly successful before deep learning?
3231600	3238240	Sure. I want to make a larger point, but on the narrower point, classical AI
3239520	3246400	is used, for example, in doing navigation instructions. It's very successful. Everybody
3246400	3250640	on the planet uses it now, multiple times a day. That's a measure of success.
3253360	3258640	I don't think classical AI was wildly successful, but there are cases like that that is used all
3258640	3265680	the time nobody even notices them, because they're so pervasive. So there are some successes for
3265680	3270000	classical AI. I think deep learning has been more successful, but my usual line about this,
3270000	3274640	and I didn't invent it, but I like it a lot, is just because you can build a better ladder,
3274720	3279600	it doesn't mean you can build a ladder to the moon. So the bitter lesson is if you have a
3279600	3285120	perceptual classification problem, throwing a lot of data at it is better than anything else.
3285760	3291840	But that has not given us any material progress in natural language understanding,
3291840	3297120	common sense reasoning like a robot would need to navigate a home. Problems like that,
3297120	3302160	there's no actual progress there. So flip side of that, if we remove data from the picture,
3302160	3311440	another bitter lesson is that you just have a very simple algorithm and you wait for compute
3311440	3314880	to scale. This doesn't have to be learning, it doesn't have to be deep learning, it doesn't
3314880	3319840	have to be data driven, but just wait for the compute. So my question for you, do you think
3319840	3325600	compute can unlock some of the things with either deep learning or simple manipulation that?
3325680	3332800	Sure, but I'll put a proviso on that. The more compute's always better. Nobody's going to argue
3332800	3337440	with more compute, it's like having more money. There's diminishing returns on more money.
3337440	3341280	Exactly, there's diminishing returns on more money, but nobody's going to argue if you want
3341280	3345120	to give them more money, right? Except maybe the people who signed the giving pledge and some of
3345120	3350160	them have a problem, they've promised to give away more money than they're able to. But the rest of
3350160	3354480	us, if you want to give me more money, fine. Say more money, more problems, but okay.
3354480	3361200	That's true too. What I would say to you is your brain uses 20 watts and it does a lot of things
3361200	3366080	that deep learning doesn't do or that simple manipulation doesn't do, that AI just hasn't
3366080	3372560	figured out how to do. So it's an existence proof that you don't need server resources that are
3372560	3378880	Google scale in order to have an intelligence. I built, with a lot of help from my wife,
3378960	3383920	two intelligences that are 20 watts each and far exceed anything that anybody else
3385040	3391760	has built at a silicon. Speaking of those two robots, what have you learned about AI from
3391760	3396560	having- Well, they're not robots, but- Sorry, intelligent agents.
3396560	3400960	There's two intelligent agents. I've learned a lot by watching my two intelligent agents.
3402720	3407600	I think that what's fundamentally interesting, well, one of the many things that's fundamentally
3407600	3411280	interesting about them is the way that they set their own problems to solve.
3411920	3416320	So my two kids are a year and a half apart. They're both five and six and a half.
3416320	3421280	They play together all the time and they're constantly creating new challenges. That's what
3421280	3426720	they do is they make up games and they're like, well, what if this or what if that or what if I
3426720	3431920	had this superpower or what if you could walk through this wall. They're doing these what if
3431920	3439200	scenarios all the time and that's how they learn something about the world and grow their minds
3439200	3444480	and machines don't really do that. So that's interesting. You've talked about this. You've
3444480	3451840	written about it. You've thought about it. Nature versus nurture. So what innate knowledge do you
3451840	3458160	think we're born with and what do we learn along the way in those early months and years?
3458160	3464720	Can I just say how much I like that question? You phrased it just right and almost nobody ever does,
3465680	3470240	which is what is the innate knowledge and what's learned along the way. So many people
3470240	3476080	dichotomize it and they think it's nature versus nurture. When it is obviously, it has to be nature
3476080	3481520	and nurture. They have to work together. You can't learn the stuff along the way unless you have some
3481520	3485280	innate stuff. But just because you have the innate stuff doesn't mean you don't learn anything.
3485680	3491360	And so many people get that wrong, including in the field. People think if I work in machine
3491360	3496080	learning, the learning side, I must not be allowed to work on the innate side where
3496080	3502080	that will be cheating. Exactly. People have said that to me and it's just absurd. So thank you.
3503280	3507600	But you could break that apart more. I've talked to folks who studied the development of the brain
3508240	3515680	and the growth of the brain in the first few days, in the first few months, in the womb,
3515680	3522720	all of that, you know, is that innate? So that process of development from a stem cell to the
3522720	3530400	growth of the central nervous system and so on to the information that's encoded through the long
3530400	3536320	arc of evolution. So all of that comes into play and it's unclear. It's not just whether it's a
3536320	3543200	dichotomy or not. It's where most or where the knowledge is encoded. So what's your intuition
3543840	3550560	about the innate knowledge, the power of it, what's contained in it, what can we learn from it?
3551200	3554560	One of my earlier books was actually trying to understand the biology of this. The book was
3554560	3560160	called The Birth of the Mind. Like how is it the genes even build innate knowledge? And from the
3560160	3564480	perspective of the conversation we're having today, there's actually two questions. One is what
3564480	3570800	innate knowledge or mechanisms or what have you? People or other animals might be endowed with.
3570800	3575680	I always like showing this video of a baby ibex climbing down a mountain. That baby ibex, you
3575680	3579760	know, a few hours after his birth, knows how to climb down a mountain. That means that it knows,
3579760	3585920	not consciously, something about its own body and physics and 3D geometry and all of this kind of
3585920	3591200	stuff. So there's one question about it. Like what does biology give its creatures? You know,
3591280	3595440	what has evolved in our brains? How is that represented in our brains? The question I
3595440	3599200	thought about in the book The Birth of the Mind. And then there's a question of what AI should have.
3599200	3606960	And they don't have to be the same. But I would say that it's a pretty interesting
3606960	3611040	set of things that we are equipped with that allows us to do a lot of interesting things. So
3611040	3615120	I would argue or guess based on my reading of the developmental psychology literature,
3615120	3621840	which I've also participated in, that children are born with a notion of space,
3621840	3629200	time, other agents, places, and also this kind of mental algebra that I was describing before.
3630160	3635760	No certain of causation, if I didn't just say that. So at least those kinds of things. They're
3635760	3641120	like frameworks for learning the other things. So are they disjoint in your view or is it just
3641200	3647520	somehow all connected? You've talked a lot about language. Is it all kind of connected in some
3647520	3654000	mesh that's language like? If understanding concepts all together? I don't think we know
3654000	3658960	for people how they're represented in machines just don't really do this yet. So I think it's an
3658960	3664880	interesting open question both for science and for engineering. Some of it has to be at least
3665520	3670880	interrelated in the way that like the interfaces of a software package have to be able to talk to
3670880	3679200	one another. So the systems that represent space and time can't be totally disjoint because a lot
3679200	3683120	of the things that we reason about are the relations between space and time and cause. So
3684240	3688640	I put this on and I have expectations about what's going to happen with the bottle cap on top of the
3688640	3696000	bottle and those span space and time. If the cap is over here, I get a different outcome. If
3696800	3701120	the timing is different, if I put this here after I move that, then I get a different outcome
3701760	3706640	that relates to causality. So obviously these mechanisms, whatever they are,
3707760	3713440	can certainly communicate with each other. So I think evolution had a significant role to play
3713440	3719360	in the development of this whole collage. How efficient do you think is evolution? Oh, it's
3719360	3729920	terribly inefficient except that it's inefficient except that once it gets a good idea, it runs with
3729920	3741120	it. So it took I guess a billion years, roughly a billion years to evolve to a vertebrate
3741440	3750160	brain plan. Once that vertebrate plan evolved, it spread everywhere. So fish have it and dogs
3750160	3755360	have it and we have it. We have adaptations of it and specializations of it and the same thing
3755360	3761520	with a primate brain plan. So monkeys have it and apes have it and we have it. So there are
3761520	3767280	additional innovations like color vision and those spread really rapidly. So it takes evolution a
3767280	3774480	long time to get a good idea and being anthropomorphic and not literal here. But once it has that
3774480	3779440	idea, so to speak, which caches out into one set of genes or in the genome, those genes spread
3779440	3784000	very rapidly and they're like subroutines or libraries, I guess the word people might use
3784000	3788720	nowadays or be more familiar with. They're libraries that can get used over and over again.
3788720	3793680	So once you have the library for building something with multiple digits, you can use it for a hand
3793680	3798000	but you can also use it for a foot. You just kind of reuse the library with slightly different
3798000	3803520	parameters. Evolution does a lot of that, which means that the speed over time picks up. So
3803520	3809200	evolution can happen faster because you have bigger and bigger libraries. And what I think has
3809920	3817200	happened in attempts at evolutionary computation is that people start with libraries that are
3818080	3825520	very, very minimal, like almost nothing and then progress is slow and it's hard for someone to get
3825520	3830560	a good PhD thesis out of it and then give up. If we had richer libraries to begin with, if you were
3830560	3836640	evolving from systems that hadn't originate structure to begin with, then things might speed up.
3836640	3841600	Or more PhD students, if the evolution process is indeed in a meta way,
3841840	3846080	runs away with good ideas, you need to have a lot of ideas,
3846640	3850320	pool of ideas in order for it to discover one that you can run away with. And
3850320	3853120	PhD students representing individual ideas as well.
3853120	3856160	Yeah. I mean, you could throw a billion PhD students at it.
3856160	3858720	Yeah. The monkeys are typewriters with Shakespeare. Yep.
3860000	3864480	Well, I mean, those aren't cumulative, right? That's just random. And part of the point that
3864480	3871040	I'm making is that evolution is cumulative. So if you have a billion monkeys independently,
3871040	3874720	you don't really get anywhere. But if you have a billion monkeys, I think Dawkins made this
3874720	3878640	point originally, or probably other people. Dawkins made it very nice in either selfish
3878640	3884800	gene or blind watchmaker. If there is some sort of fitness function that can drive you toward
3884800	3889440	something, I guess that's Dawkins' point. And my point, which is a little variation on that,
3889440	3895520	is that if the evolution is cumulative of the related points, then you can start going faster.
3895520	3899360	Do you think something like the process of evolution is required to build the intelligence
3899360	3905760	systems? Not logically. So all the stuff that evolution did, a good engineer might be able to
3905760	3913680	do. So for example, evolution made quadrupeds, which distribute the load across a horizontal
3913680	3918240	surface. A good engineer could come up with that idea. I mean, sometimes good engineers come up
3918240	3923600	with ideas by looking at biology. There's lots of ways to get your ideas. Part of what I'm suggesting
3923600	3929440	is we should look at biology a lot more. We should look at the biology of thought and the
3929440	3935840	understanding and the biology by which creatures intuitively reason about physics or other agents
3935840	3940400	or how do dogs reason about people? They're actually pretty good at it. If we could understand,
3941760	3947520	at my college we joked, dognition. If we could understand dognition well and how it was implemented,
3947600	3955680	that might help us with our AI. Do you think it's possible that the kind of timescale that
3955680	3960400	evolution took is the kind of timescale that will be needed to build intelligence systems,
3960400	3963760	or can we significantly accelerate that process inside a computer?
3965520	3969440	I think the way that we accelerate that process is we borrow from biology.
3970560	3975600	Not slavishly, but I think we look at how biology has solved problems and we say,
3975600	3981040	does that inspire any engineering solutions here? Try to mimic biological systems and then
3981040	3986960	therefore have a shortcut. There's a field called biomimicry. People do that for material
3986960	3994400	science all the time. We should be doing the analog of that for AI. The analog for that for AI
3994400	3998640	is to look at cognitive science or the cognitive sciences, which is psychology,
3998640	4002720	maybe neuroscience, linguistics, and so forth. Look to those for insight.
4003280	4005680	What do you think is a good test of intelligence in your view?
4006480	4012080	I don't think there's one good test. In fact, I try to organize a movement towards something
4012080	4017120	called a Turing Olympics. My hope is that Francois is actually going to take, Francois Chalet,
4017120	4021840	is going to take over this. I think he's interested and I just don't have place in my
4021840	4027840	busy life at this moment. The notion is that there'll be many tests and not just one because
4027920	4034000	intelligence is multifaceted. There can't really be a single measure of it because it isn't a single
4034000	4039840	thing. Just at the crudest level, the SAT is a verbal component and a math component because
4039840	4043920	they're not identical. Howard Gardner has talked about multiple intelligence like
4043920	4048880	kinesthetic intelligence and verbal intelligence and so forth. There are a lot of things that go
4048880	4054640	into intelligence and people can get good at one or the other. In some sense, every expert has
4054720	4059120	developed a very specific kind of intelligence and then there are people that are generalists.
4059920	4064080	I think of myself as a generalist with respect to cognitive science, which doesn't mean I know
4064080	4068240	anything about quantum mechanics, but I know a lot about the different facets of the mind.
4069920	4073760	There's a kind of intelligence to thinking about intelligence. I like to think that I have some
4073760	4079120	of that, but social intelligence, I'm just okay. There are people that are much better at that than
4079120	4086160	I am. Sure, but what would be really impressive to you? I think the idea of a touring Olympics is
4086160	4091920	really interesting, especially if somebody like Francois is running it, but to you in general,
4093360	4097600	not as a benchmark, but if you saw an AI system being able to accomplish something
4098400	4103680	that would impress the heck out of you, what would that thing be? Would it be natural language
4103680	4109840	conversation? For me personally, I would like to see a kind of comprehension that relates
4109840	4116080	to what you just said. I wrote a piece in The New Yorker in I think 2015, right after Eugene
4116080	4126000	Gustman, which was a software package, won a version of the touring test. The way you win
4126000	4131040	the touring test, it's called win it, is the touring test is you fool a person into thinking
4132000	4138320	that a machine is a person. You're evasive, you pretend to have limitations, so you don't have
4138320	4144000	to answer certain questions and so forth. This particular system pretended to be a 13-year-old
4144000	4148640	boy from Odessa who didn't understand English and was kind of sarcastic and wouldn't answer your
4148640	4153440	questions and so forth. Judges got fooled into thinking briefly with a very little exposure
4153440	4157360	as a 13-year-old boy, and it docked to all the questions the touring was actually interested
4157360	4161600	in, which is like, how do you make the machine actually intelligent? That test itself is not
4161600	4167040	that good. In The New Yorker, I proposed an alternative, I guess. The one that I proposed
4167040	4171680	there was a comprehension test. I must like Breaking Bad because I've already given you
4171680	4176640	one Breaking Bad example. In that article, I have one as well, which was something like,
4176640	4180880	if Walter White, you should be able to watch an episode of Breaking Bad or maybe you have to
4180880	4184800	watch the whole series to be able to answer the question and say, if Walter White took a hit
4184800	4189920	out on Jesse, why did he do that? If you could answer kind of arbitrary questions about characters
4189920	4194320	motivations, I would be really impressed with that. I mean, you build software to do that.
4195200	4200640	They could watch a film or they're different versions. Ultimately, I wrote this up with
4200640	4205280	Praveen Paratosh in a special issue of AI magazine that basically was about the Touring
4205280	4209600	Olympics. There were like 14 tests proposed. The one that I was pushing was a comprehension
4209600	4214080	challenge and Praveen, who's at Google, was trying to figure out how we would actually run it. We
4214080	4218960	wrote a paper together. You could have a text version too. You could have an auditory podcast
4218960	4224640	version. You could have a written version. But the point is that you win at this test if you can do,
4224640	4230160	let's say, human level or better than humans at answering kind of arbitrary questions. Why did this
4230160	4234640	person pick up the stone? What were they thinking when they picked up the stone? Were they trying to
4234640	4239360	knock down glass? I mean, ideally, these wouldn't be multiple choice either because multiple choice
4239360	4245760	is pretty easily gamed. If you could have relatively open-ended questions and you can answer why people
4245760	4250880	are doing this stuff, I would be very impressed. Of course, humans can do this. If you watch a
4250880	4257600	well-constructed movie and somebody picks up a rock, everybody watching the movie knows why they
4257600	4263520	picked up the rock. They all know, oh my gosh, he's going to hit this character or whatever.
4263520	4268720	We have an example in the book about when a whole bunch of people say, I am Spartacus.
4268720	4278240	You know this famous scene? The viewers understand, first of all, everybody or everybody minus one
4278240	4282000	has to be lying. They can't all be Spartacus. We have enough common sense knowledge to know
4282000	4286560	they couldn't all have the same name. We know that they're lying and we can infer why they're
4286560	4291040	lying. They're lying to protect someone and to protect things they believe in. You get a machine
4291040	4296240	that can do that. They can say, this is why these guys all got up and said, I am Spartacus.
4296880	4301360	I will sit down and say AI has really achieved a lot. Thank you.
4301360	4306320	Without cheating any part of the system. Yeah. If you do it, there are lots of ways you can
4306320	4311120	cheat. You could build a Spartacus machine that works on that film. That's not what I'm talking
4311120	4315680	about. You can do this with essentially arbitrary films from a large set.
4315680	4320560	Even beyond films because it's possible such a system would discover that the number of narrative
4320560	4326400	arcs in film is limited to like 93. Well, there's a famous thing about the classic seven plots or
4326400	4330720	whatever. I don't care if you want to build in the system, boy meets girl, boy loses girl,
4330720	4334400	boy finds girl. That's fine. I don't mind having some headstone. Any knowledge?
4336240	4340400	I mean, you could build it in an Ailey or you could have your system watch a lot of films again.
4340400	4346000	If you can do this at all, but with a wide range of films, not just one film in one genre,
4346960	4350080	but even if you could do it for all Westerns, I'd be reasonably impressed.
4351920	4357520	So in terms of being impressed just for the fun of it, because you've put so many interesting
4357520	4364480	ideas out there in your book, challenging the community for further steps, is it possible
4364480	4371360	on the deep learning front that you're wrong about its limitations? That deep learning will
4371360	4376160	unlock. Yanlacoon next year will publish a paper that achieves this comprehension.
4376800	4383360	So do you think that way often as a scientist, do you consider that your intuition that deep
4383360	4390880	learning could actually run away with it? I'm worried about rebranding as a kind of political
4390880	4396320	thing. So I mean, what's going to happen, I think, is that deep learning is going to start to encompass
4396320	4400880	simple manipulation. So I think Hinton's just wrong. Hinton says we don't want hybrids. I think
4400880	4404800	people will work towards hybrids and they will relabel their hybrids as deep learning. We've
4404800	4410000	already seen some of that. So AlphaGo is often described as a deep learning system, but it's
4410000	4413760	more correctly described as a system that has deep learning, but also Monte Carlo tree search,
4413760	4418960	which is a classical AI technique. And people will start to blur the lines in the way that IBM
4418960	4422000	blurred Watson. First, Watson meant this particular system, and then it was just
4422000	4425680	anything that IBM built in their cognitive division. But purely, let me ask for sure,
4425920	4431920	that's a branding question, and that's a giant mess. I mean, purely a single neural network
4431920	4435440	being able to accomplish reasonable comprehension. I don't step at night
4435440	4439920	worrying that that's going to happen. And I'll just give you two examples. One is
4440560	4446720	a guy at DeepMind thought he had finally outfoxed me at Xergy Lord, I think is his Twitter handle.
4447920	4453840	And he specifically made an example. Marcus said that such and such, he fed it into GP2,
4454800	4459280	which is the AI system that is so smart that OpenAI couldn't release it because it would
4459280	4465440	destroy the world, right? You remember that a few months ago. So he feeds it into GPT2.
4466000	4469600	And my example was something like a rose is a rose, a tulip is a tulip,
4470160	4474080	a lily is a blank. And he got it to actually do that, which was a little bit impressive. And I
4474080	4479280	wrote back and I said, that's impressive. But can I ask you a few questions? I said, was that just
4479280	4483840	one example? Can it do it generally? And can it do it with novel words, which is part of what I
4483840	4491120	was talking about in 1998, when I first raised the example. So a DAX is a DAX, right? And he
4491120	4494720	sheepishly wrote back about 20 minutes later and the answer was, well, it had some problems with
4494720	4501840	those. So I made some predictions 21 years ago that still hold in the world of computer science.
4501840	4508320	That's amazing, right? Because there's a thousand or a million times more memory and computations,
4508400	4514320	million times, do million times more operations per second, spread across a cluster,
4515360	4524080	and there's been advances in replacing sigmoids with other functions and so forth. There's all
4524080	4528240	kinds of advances, but the fundamental architecture hasn't changed and the fundamental limit hasn't
4528240	4533040	changed. And what I said then is kind of still true. And then here's a second example. I recently
4533040	4540080	had a piece in WIRED that's adapted from the book. And the book was when to press before GP2 came out.
4540080	4546480	But we describe this children's story and all the inferences that you make in this story about a boy
4546480	4553920	finding a lost wallet. And for fun, in the WIRED piece, we ran it through GP2. GPT2 at something
4553920	4558720	called TalkToTransformer.com, and your viewers can try this experiment themselves, go to the WIRED
4558800	4565280	piece that has the link and has the story. And the system made perfectly fluent text that was
4565280	4571840	totally inconsistent with the conceptual underpinnings of the story. And this is what, again,
4571840	4576880	I predicted in 1998. And for that matter, Chomsky Miller made the same prediction in 1963. I was
4576880	4583040	just updating their claim for a slightly new text. So those particular architectures that
4583040	4588400	don't have any bills to acknowledge, they're basically just a bunch of layers doing correlational
4588480	4593440	stuff. They're not going to solve these problems. So 20 years ago, you said the emperor has no
4593440	4598720	clothes. Today, the emperor still has no clothes. The lighting's better, though. The lighting is
4598720	4603920	better. And I think you yourself are also, I mean, and we found out some things to do with naked
4603920	4608800	emperors. I mean, it's not like stuff is worthless. I mean, they're not really naked. It's more like
4608800	4613280	they're in their briefs and everybody thinks that. And so like, I mean, they are great at speech
4613280	4617680	recognition. But the problems that I said were hard, because I didn't literally say the emperor
4617680	4622240	has no clothes. I said, this is a set of problems that humans are really good at. And it wasn't
4622240	4627760	couched as AI. It was couched as cognitive science. But I said, if you want to build a neural model
4627760	4632080	of how humans do certain class of things, you're going to have to change the architecture. And
4632080	4638400	I stand by those claims. And I think people should understand you're quite entertaining in your
4638400	4644320	cynicism, but you're also very optimistic and a dreamer about the future of AI, too. So you're
4644320	4650640	both is just there's a famous saying about being people overselling technology in the short run
4650640	4658560	and underselling it in the long run. And so I actually end the book or Ernie Davis and I end
4658560	4663520	our book with an optimistic chapter, which kind of killed Ernie because he's even more pessimistic
4663520	4669280	than I am. He describes me as a contrarian and him as a pessimist. But I persuaded that we should
4669280	4675280	end the book with a look at what would happen if AI really did incorporate, for example,
4675280	4679680	the common sense reasoning and the nativism and so forth, the things that we counseled for. And
4679680	4685680	we wrote it and it's an optimistic chapter that AI suitably reconstructed so that we could trust
4685680	4691760	it, which we can't now could really be world changing. So on that point, if you look at the
4691760	4697600	future trajectories of AI, people have worries about negative effects of AI, whether it's
4698160	4705280	at the large existential scale or smaller short term scale of negative impact on society. So
4705280	4711760	you write about trustworthy AI. How can we build AI systems that align with our values that make
4711760	4715840	for a better world that we can interact with that we can trust? The first thing we have to do is to
4715840	4722800	replace deep learning with deep understanding. So you can't have alignment with a system that
4722800	4727200	traffics only in correlations and doesn't understand concepts like bottles or harm.
4728880	4735360	Asimov talked about these famous laws. The first one was first do no harm. And you can quibble
4735360	4738800	about the details of Asimov's laws, but we have to if we're going to build real robots in the
4738800	4742880	real world have something like that. That means we have to program in a notion that's at least
4742880	4747120	something like harm. That means we have to have these more abstract ideas that deep learning is
4747120	4752320	not particularly good at. They have to be in the mix somewhere. You could do statistical analysis
4752320	4756080	about probabilities of given harms or whatever, but you have to know what a harm is in the same
4756080	4759360	way that you have to understand that a bottle isn't just a collection of pixels.
4763280	4768320	You're implying that you need to also be able to communicate that to humans. So the AI systems
4768320	4775360	would be able to prove to humans that they understand that they know what harm means.
4775360	4778800	I might run it in the reverse direction, but roughly speaking, I agree with you. So
4779520	4784880	we probably need to have committees of wise people, ethicists, and so forth,
4785600	4789840	think about what these rules ought to be, and we should just leave it to software engineers.
4789840	4794000	It shouldn't just be software engineers, and it shouldn't just be people who
4794720	4799840	own large mega corporations that are good at technology. Ethicists and so forth should be
4799840	4805280	involved. But there should be some assembly of wise people, as I was putting it,
4806080	4811280	that tries to figure out what the rules ought to be, and those have to get translated into code.
4812400	4815920	You can argue code or neural networks or something. They have to be
4817360	4822240	translated into something that machines can work with, and that means there has to be a way of
4822240	4826560	working the translation, and right now we don't. We don't have a way. So let's say you and I were
4826560	4830720	the committee, and we decide that Asimov's first law is actually right, and let's say it's not just
4830720	4835920	two white guys, which would be unfortunate, and then we have a broad representative sample of the
4835920	4841360	world or however we want to do this, and the committee decides eventually, okay, Asimov's
4841360	4844800	first law is actually pretty good. There are these exceptions to it. We want to program in
4844800	4848800	these exceptions, but let's start with just the first one, and then we'll get to the exceptions.
4848800	4854080	First one is first do no harm. Well, somebody has to now actually turn that into a computer
4854080	4858960	program or a neural network or something, and one way of taking the whole book, the whole
4859040	4863520	argument that I'm making is that we just don't have to do that yet, and we're fooling ourselves
4863520	4869200	if we think that we can build trustworthy AI. If we can't even specify in any kind of,
4869200	4874000	you know, we can't do it in Python and we can't do it in TensorFlow, we're fooling ourselves
4874000	4878960	and thinking that we can make trustworthy AI if we can't translate harm into something that we
4878960	4884320	can execute, and if we can't, then we should be thinking really hard, how could we ever do such
4884320	4889200	a thing? Because if we're going to use AI in the ways that we want to use it to make job interviews
4889200	4892880	or to do surveillance, not that I personally want to do that, or whatever, I mean, if we're going to
4892880	4898960	use AI in ways that have practical impact on people's lives or medicine, it's got to be able
4898960	4904160	to understand stuff like that. So one of the things your book highlights is that, you know,
4904880	4910560	a lot of people in the deep learning community, but also the general public, politicians, just
4910560	4917280	people in all general groups and walks of life have different levels of misunderstanding of AI.
4917280	4926800	So when you talk about committees, what's your advice to our society? How do we grow? How do we
4926800	4932640	learn about AI such that such committees could emerge, where large groups of people could have
4933440	4937760	a productive discourse about how to build successful AI systems?
4937760	4942880	Part of the reason we wrote the book was to try to inform those committees. So part of the reason
4942880	4947040	we wrote the book was to inspire a future generation of students to solve what we think are the
4947040	4951120	important problems. So a lot of the book is trying to pinpoint what we think are the hard problems
4951120	4958160	where we think effort would most be rewarded. And part of it is to try to train people who
4958160	4963360	talk about AI, but aren't experts in the field to understand what's realistic and what's not.
4963360	4966080	One of my favorite parts in the book is the six questions you should ask
4966880	4971040	anytime you read a media account. So number one is if somebody talks about something,
4971040	4975440	look for the demo. If there's no demo, don't believe it. The demo that you can try. If you
4975440	4980240	can't try it at home, maybe it doesn't really work that well yet. So we don't have this example
4980240	4986560	in the book. But if Sundar Pinchai says, we have this thing that allows it to sound like human
4986560	4992000	beings in conversation, you should ask, can I try it? And you should ask how general it is. And it
4992000	4996560	turns out at that time, I'm alluding to Google Duplex, when it was announced, it only worked on
4996560	5001200	calling hairdressers, restaurants, and finding opening hours. That's not very general. That's
5001200	5006960	narrow AI. And I'm not going to ask your thoughts about Sophia. But yeah, I understand that's a
5006960	5012160	really good question to ask of any kind of high-top idea. So Sophia has very good material written
5012160	5017360	for her, but she doesn't understand the things that she's saying. So a while ago, you've written a
5017360	5022400	book on the science of learning, which I think is fascinating. But the learning case studies of
5022400	5028640	playing guitar, guitar zero, I love guitar myself, I've been playing my whole life. So let me ask
5028640	5036080	a very important question. What is your favorite song, rock song to listen to or try to play?
5036080	5039920	Well, those would be different. But I'll say that my favorite rock song to listen to is probably
5039920	5044000	all along the Watchtower, the Jimi Hendrix version. Jimi Hendrix version feels magic to me.
5044720	5048560	I've actually recently learned that I love that song. I've been trying to put it on YouTube,
5048560	5053280	myself singing. Singing is the scary part. If you could party with a rock star for a weekend,
5053280	5060080	living or dead, who would you choose? And pick their mind, it's not necessarily about the partying.
5061040	5068240	Thanks for the clarification. I guess John Lennon is such an intriguing person. I mean,
5068480	5074640	a troubled person, but an intriguing one. So beautiful. Well, Imagine is one of my favorite
5074640	5078480	songs. So also one of my favorite songs. That's a beautiful way to end it. Gary,
5078480	5080560	thank you so much for talking to me. Thanks so much for having me.
