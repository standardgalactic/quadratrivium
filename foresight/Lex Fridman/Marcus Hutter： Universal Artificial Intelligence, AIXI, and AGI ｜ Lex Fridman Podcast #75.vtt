WEBVTT

00:00.000 --> 00:05.760
The following is a conversation with Marcus Hutter, senior research scientist at Google DeepMind.

00:06.640 --> 00:11.680
Throughout his career of research, including with JÃ¶rgen Schmidhuber and Shane Legg,

00:11.680 --> 00:16.320
he has proposed a lot of interesting ideas in and around the field of artificial general

00:16.320 --> 00:23.760
intelligence, including the development of IEXI, spelled AIXI model, which is the mathematical

00:23.760 --> 00:30.560
approach to AGI that incorporates ideas of Komogorov complexity, Solominov induction,

00:30.560 --> 00:38.320
and reinforcement learning. In 2006, Marcus launched the 50,000-euro Hutter prize for

00:38.320 --> 00:43.840
lossless compression of human knowledge. The idea behind this prize is that the ability to

00:43.840 --> 00:52.080
compress well is closely related to intelligence. This, to me, is a profound idea. Specifically,

00:52.160 --> 00:57.040
if you can compress the first 100 megabytes or one gigabyte of Wikipedia better than your

00:57.040 --> 01:03.600
predecessors, your compressor likely has to also be smarter. The intention of this prize

01:03.600 --> 01:08.160
is to encourage the development of intelligent compressors as a path to AGI.

01:09.520 --> 01:15.440
In conjunction with his podcast release just a few days ago, Marcus announced a 10x increase

01:15.440 --> 01:23.120
in several aspects of this prize, including the money, to 500,000 euros. The better your

01:23.120 --> 01:28.160
compressor works relative to the previous winners, the higher fraction of that prize money is awarded

01:28.160 --> 01:36.400
to you. You can learn more about it if you google simply Hutter prize. I'm a big fan of benchmarks

01:36.400 --> 01:41.920
for developing AI systems, and the Hutter prize may indeed be one that will spark some good ideas

01:41.920 --> 01:48.560
for approaches that will make progress on the path of developing AGI systems. This is the

01:48.560 --> 01:53.760
Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it 5 stars on Apple

01:53.760 --> 01:59.200
Podcasts, support it on Patreon, or simply connect with me on Twitter at Lex Freedman,

01:59.200 --> 02:06.400
spelled F-R-I-D-M-A-N. As usual, I'll do one or two minutes of ads now and never any ads in the

02:06.400 --> 02:11.280
middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the

02:11.360 --> 02:17.680
listening experience. This show is presented by Cash App, the number one finance app in the App Store.

02:17.680 --> 02:24.480
When you get it, use code LEX Podcast. Cash App lets you send money to friends, buy Bitcoin,

02:24.480 --> 02:30.000
and invest in the stock market with as little as $1. Broker services are provided by Cash App

02:30.000 --> 02:37.040
Investing, a subsidiary of Square, and member SIPC. Since Cash App allows you to send and receive

02:37.040 --> 02:42.560
money digitally, peer-to-peer, and security in all digital transactions very important,

02:42.560 --> 02:48.960
let me mention the PCI Data Security Standard that Cash App is compliant with. I'm a big fan of

02:48.960 --> 02:56.080
standards for safety and security. PCI DSS is a good example of that, where a bunch of competitors

02:56.080 --> 03:01.600
got together and agreed that there needs to be a global standard around the security of transactions.

03:02.480 --> 03:09.600
Now, we just need to do the same for autonomous vehicles and AI systems in general. So again,

03:09.600 --> 03:14.880
if you get Cash App from the App Store or Google Play and use the code LEX Podcast,

03:14.880 --> 03:21.120
you'll get $10 and Cash App will also donate $10 to FIRST, one of my favorite organizations

03:21.120 --> 03:26.480
that is helping to advance robotics and STEM education for young people around the world.

03:27.440 --> 03:31.360
And now, here's my conversation with Marcus Hodder.

03:32.400 --> 03:36.880
Do you think of the universe as a computer or maybe an information processing system?

03:36.880 --> 03:42.320
Let's go with a big question first. Okay, I have a big question first. I think it's a very

03:42.320 --> 03:49.360
interesting hypothesis or idea, and I have a background in physics, so I know a little bit

03:49.360 --> 03:54.320
about physical theories, the standard model of particle physics and general relativity theory,

03:54.400 --> 03:58.480
and they are amazing and describe virtually everything in the universe, and they're all in

03:58.480 --> 04:03.520
a sense, computable theories. I mean, they're very hard to compute, and it's very elegant,

04:03.520 --> 04:08.400
simple theories which describe virtually everything in the universe. So there's a strong

04:09.040 --> 04:16.960
indication that somehow the universe is computable, but it's a plausible hypothesis.

04:17.680 --> 04:22.160
So what do you think, just like you said, general relativity, quantum field theory,

04:22.160 --> 04:28.160
what do you think that the laws of physics are so nice and beautiful and simple and compressible?

04:29.040 --> 04:36.000
Do you think our universe was designed is naturally this way? Are we just focusing on the

04:36.000 --> 04:42.720
parts that are especially compressible? Are human minds just enjoying something about that simplicity

04:42.720 --> 04:46.640
and, in fact, there's other things that are not so compressible?

04:46.640 --> 04:52.000
No, I strongly believe and I'm pretty convinced that the universe is inherently beautiful

04:52.000 --> 04:56.880
elegant and simple and described by these equations, and we're not just picking that.

04:57.440 --> 05:04.160
I mean, if there were some phenomena which cannot be neatly described, scientists would try that,

05:04.160 --> 05:07.680
right? And, you know, there's biology which is more messy, but we understand that it's an

05:07.680 --> 05:12.320
emergent phenomena, and, you know, it's complex systems, but they still follow the same rules,

05:12.320 --> 05:16.640
right? Of quantum and electrodynamics, all of chemistry follows that, and we know that. I mean,

05:16.640 --> 05:20.720
we cannot compute everything because we have limited computational resources. No, I think it's

05:20.720 --> 05:25.440
not a bias of the humans, but it's objectively simple. I mean, of course, you never know, you

05:25.440 --> 05:30.480
know, maybe there's some corners very far out in the universe or super, super tiny below the

05:30.480 --> 05:39.520
nucleus of atoms or, well, parallel universes where which are not nice and simple, but there's no

05:39.520 --> 05:44.240
evidence for that, and we should apply Occam's razor and, you know, choose the simple streak

05:44.240 --> 05:47.920
consistent with it, but also it's a little bit self-referential.

05:47.920 --> 05:54.480
So maybe a quick pause. What is Occam's razor? So Occam's razor says that you should not multiply

05:54.480 --> 06:01.760
entities beyond necessity, which sort of if you translate it to proper English means, and, you

06:01.760 --> 06:06.880
know, in the scientific context means that if you have two theories or hypotheses or models which

06:06.880 --> 06:12.400
equally well describe the phenomenon you're studying or the data, you should choose the more

06:12.400 --> 06:20.400
simple one. So that's just a principle or sort of that's not like a provable law, perhaps, perhaps

06:20.400 --> 06:27.040
we'll kind of discuss it and think about it, but what's the intuition of why the simpler answer

06:28.000 --> 06:34.960
is the one that is likely to be more correct descriptor of whatever we're talking about?

06:34.960 --> 06:40.400
I believe that Occam's razor is probably the most important principle in science. I mean,

06:40.400 --> 06:47.280
of course, we need logical deduction and we do experimental design, but science is about finding

06:47.840 --> 06:53.600
understanding the world, finding models of the world, and we can come up with crazy complex models

06:53.600 --> 06:59.040
which, you know, explain everything but predict nothing, but the simple model seemed to have

06:59.040 --> 07:06.960
predictive power, and it's a valid question why. And the two answers to that, you can just accept

07:07.120 --> 07:12.000
that as the principle of science, and we use this principle and it seems to be successful.

07:12.720 --> 07:18.480
We don't know why, but it just happens to be. Or you can try, you know, find another principle

07:18.480 --> 07:25.440
which explains Occam's razor. And if we start with assumption that the world is governed by

07:25.440 --> 07:33.600
simple rules, then there's a bias to our simplicity, and applying Occam's razor

07:34.160 --> 07:38.880
is the mechanism to finding these rules. And actually, in a more quantitative sense,

07:38.880 --> 07:42.720
and we come back to that later in case of somnambular deduction, you can rigorously prove

07:42.720 --> 07:48.160
that you assume that the world is simple, then Occam's razor is the best you can do in a certain

07:48.160 --> 07:55.520
sense. So, I apologize for the romanticized question, but why do you think outside of its

07:55.520 --> 08:00.000
effectiveness, why do we do you think we find simplicity so appealing as human beings? Why

08:00.000 --> 08:07.120
does it just, why does E equals MC squared seem so beautiful to us humans?

08:08.240 --> 08:14.240
I guess mostly, in general, many things can be explained by an evolutionary argument.

08:14.880 --> 08:19.280
And, you know, there's some artifacts in humans which, you know, are just artifacts and not an

08:19.280 --> 08:28.240
evolutionary necessary. But with this beauty and simplicity, it's, I believe, at least the core

08:28.800 --> 08:35.840
is about, like science, finding regularities in the world, understanding the world,

08:35.840 --> 08:42.400
which is necessary for survival, right? You know, if I look at a bush, right, and I just see noise,

08:42.400 --> 08:47.200
and there is a tiger, right, and eats me, then I'm dead. But if I try to find a pattern, and

08:47.200 --> 08:54.800
we know that humans are prone to find more patterns in data than they are, you know, like, you know,

08:54.880 --> 09:01.040
Mars face and all these things, but these bias towards finding patterns, even if they are not,

09:01.040 --> 09:05.040
but I mean, it's best, of course, if they are, yeah, helps us for survival.

09:06.480 --> 09:11.600
Yeah, that's fascinating. I haven't thought really about the, I thought I just loved science, but

09:12.800 --> 09:20.320
indeed, from in terms of just for survival purposes, there is an evolutionary argument for why we find

09:21.280 --> 09:28.080
the work of Einstein so beautiful. Maybe a quick small tangent, could you describe what

09:28.080 --> 09:36.880
Solomon of induction is? Yeah, so that's a theory which I claim, and Resolominov sort of claimed

09:36.880 --> 09:42.800
a long time ago, that this solves the big philosophical problem of induction. And I believe

09:42.800 --> 09:49.360
the claim is essentially true. And what it does is the following. So, okay, for the

09:50.320 --> 09:58.720
picky listener, induction can be interpreted narrowly and wildly narrow means inferring models from data.

10:00.560 --> 10:05.920
And widely means also then using these models for doing predictions or predictions also part of

10:05.920 --> 10:10.640
the induction. So I'm a little sloppy sort of with the terminology and maybe that comes from

10:11.200 --> 10:14.400
Resolominov, you know, being sloppy, maybe I shouldn't say that.

10:15.920 --> 10:19.920
He can't complain anymore. So let me explain a little bit this theory.

10:20.880 --> 10:26.880
In simple terms, so assume we have a data sequence, make it very simple, the simplest one say 111111 and

10:26.880 --> 10:32.400
you see if 100 ones, what do you think comes next? The natural answer, I'm going to speed up a little

10:32.400 --> 10:38.720
bit, the natural answer is of course, you know, one. Okay, and the question is why? Okay, well,

10:38.720 --> 10:43.760
we see a pattern there. Yeah, okay, there's a one and we repeat it. And why should it suddenly

10:43.760 --> 10:48.480
after 100 ones be different? So what we're looking for is simple explanations or models

10:49.120 --> 10:54.480
for the data we have. And now the question is a model has to be presented in a certain language

10:55.360 --> 11:00.720
in which language to be used. In science, we want formal languages, and we can use mathematics,

11:00.720 --> 11:06.320
or we can use programs on a computer. So abstractly on a Turing machine, for instance,

11:06.320 --> 11:11.600
or can be a general purpose computer. So and there are of course, lots of models of you can

11:11.600 --> 11:16.560
say maybe it's 101 and then 100 zeros and 100 ones, that's a model, right? But they're simpler

11:16.560 --> 11:22.640
models, there's a model print one loop. Now that also explains the data. And if you push

11:22.640 --> 11:28.480
that to the extreme, you are looking for the shortest program, which if you run this program

11:28.480 --> 11:34.720
reproduces the data you have, it will not stop, it will continue naturally. And this you take

11:34.720 --> 11:39.760
for your prediction. And on the sequence of ones, it's very plausible, right, that print one loop

11:39.760 --> 11:45.760
is the shortest program, we can give some more complex examples like 12345. What comes next,

11:45.760 --> 11:51.840
the short program is again, you know, counter. And so that is, roughly speaking, how Solomar's

11:51.840 --> 11:58.400
induction works. The extra twist is that it can also deal with noisy data. So if you have, for

11:58.400 --> 12:03.040
instance, a coin flip, say a biased coin, which comes up head with 60% probability,

12:04.480 --> 12:09.200
then it will predict, it will learn and figure this out. And after a while, it predict or the next

12:10.240 --> 12:14.160
coin flip will be head with probability 60%. So it's the stochastic version of that.

12:14.720 --> 12:18.640
But the goal is the dream is always the search for the short program.

12:18.640 --> 12:23.920
Yes. Yeah. Well, in Solomar of induction, precisely what you do is, so you combine. So

12:23.920 --> 12:29.520
looking for the shortest program is like applying opax razor, like looking for the simplest theory.

12:29.520 --> 12:34.640
There's also Epicoros principle, which says, if you have multiple hypotheses, which equally well

12:34.640 --> 12:39.280
describe your data, don't discard any of them, keep all of them around, you never know. And you

12:39.280 --> 12:44.240
can put that together and say, okay, I have a bias towards simplicity, but I don't rule out the

12:44.240 --> 12:51.280
larger models. And technically, what we do is we weigh the shorter models higher and the longer

12:51.280 --> 13:00.000
models lower. And you use a Bayesian techniques, you have a prior, and which is precisely two to

13:00.000 --> 13:05.280
the minus the complexity of the program. And you weigh all this hypothesis and takes this mixture,

13:05.280 --> 13:09.760
and then you get also this stochasticity in. Yeah, like many of your ideas, that's just a

13:09.760 --> 13:15.920
beautiful idea of weighing based on the simplicity of the program. I love that. That seems to me

13:15.920 --> 13:23.200
maybe a very human-centric concept seems to be a very appealing way of discovering good programs

13:23.200 --> 13:30.720
in this world. You've used the term compression quite a bit. I think it's a beautiful idea,

13:30.720 --> 13:37.200
sort of, we just talked about simplicity, and maybe science or just all of our intellectual

13:37.200 --> 13:43.600
pursuits is basically the attempt to compress the complexity all around us into something simple.

13:43.600 --> 13:52.320
So what does this word mean to you, compression? I essentially have already explained it. So

13:52.320 --> 14:00.160
it compression means, for me, finding short programs for the data or the phenomenon at hand.

14:00.160 --> 14:05.840
You could interpret it more widely as finding simple theories, which can be mathematical theories,

14:05.840 --> 14:12.400
or maybe even informal, like just in words, compression means finding short descriptions,

14:12.400 --> 14:21.440
explanations, programs for the data. Do you see science as a kind of our human attempt at compression?

14:22.240 --> 14:26.320
So we're speaking more generally, because when you say programs, you're kind of zooming in on a

14:26.320 --> 14:30.720
particular, sort of, almost like a computer science, artificial intelligence focus. But

14:30.720 --> 14:35.920
do you see all of human endeavor as a kind of compression? Well, at least all of science,

14:35.920 --> 14:41.440
I see as an endeavor of compression, not all of humanity, maybe. And, well, there are also some

14:41.440 --> 14:46.800
other aspects of science, like experimental design, right? I mean, we create experiments

14:46.800 --> 14:52.480
specifically to get extra knowledge. And this is, that isn't part of the decision-making process.

14:53.200 --> 14:59.200
But once we have the data to understand the data is essentially compression. So I don't see any

14:59.200 --> 15:07.520
difference between compression, understanding, and prediction. So we're jumping around topics a

15:07.520 --> 15:13.760
little bit, but returning back to simplicity, a fascinating concept of comagra of complexity.

15:14.320 --> 15:21.200
So in your sense, do most objects in our mathematical universe have high comagra of

15:21.200 --> 15:25.840
complexity? And maybe what is, first of all, what is comagra of complexity?

15:25.840 --> 15:33.760
Okay, comagra of complexity is a notion of simplicity or complexity. And it takes the

15:33.760 --> 15:39.600
compression view to the extreme. So I explained before that if you have some data sequence,

15:39.600 --> 15:45.360
just think about a file on a computer and best sort of, you know, just a string of bits. And

15:46.320 --> 15:52.160
if you, and we have data compressors, like we compress big files into, say, zip files with

15:52.160 --> 15:57.920
certain compressors. And you can also produce self-extracting RKFs. That means as an executable,

15:57.920 --> 16:03.120
if you run it, it reproduces your original file without needing an extra decompressor. It's just

16:03.120 --> 16:08.880
a decompressor plus the RKF together in one. And now there are better and worse compressors. And

16:08.880 --> 16:14.320
you can ask, what is the ultimate compressor? So what is the shortest possible self-extracting

16:14.320 --> 16:20.480
RKF you could produce for a certain data set, which reproduces the data set. And the length of

16:20.480 --> 16:27.120
this is called the comagra of complexity. And arguably, that is the information content in

16:27.120 --> 16:31.200
the data set. I mean, if the data set is very redundant or very boring, you can compress it

16:31.200 --> 16:36.480
very well. So the information content should be low. And, you know, it is low according to this

16:36.480 --> 16:41.920
definition. Does the length of the shortest program that summarizes the data? Yes. Yeah.

16:41.920 --> 16:47.840
And what's your sense of our universe when we think about the different

16:50.000 --> 16:56.320
objects in our universe that we try concepts or whatever at every level? Do they have higher

16:56.320 --> 17:02.560
or low comagra of complexity? So what's the hope? Do we have a lot of hope in being able to summarize

17:03.280 --> 17:08.720
much of our world? That's a tricky and difficult question. So

17:09.360 --> 17:14.000
as I said before, I believe that the whole universe, based on the evidence we have,

17:14.000 --> 17:19.920
is very simple. So it has a very short description, the whole. Sorry to linger on that.

17:19.920 --> 17:25.920
The whole universe, what does that mean? Do you mean at the very basic fundamental level in order

17:25.920 --> 17:32.160
to create the universe? Yes. Yeah. So you need a very short program when you run it. To get the

17:32.160 --> 17:36.880
thing going. To get the thing going, and then it will reproduce our universe. There's a problem

17:37.840 --> 17:44.560
with noise. We can come back to that later, possibly. Is noise a problem or is it a bug or a

17:44.560 --> 17:52.400
feature? I would say it makes our life as a scientist really, really much harder. I mean,

17:52.400 --> 17:57.200
think about it without noise. We wouldn't need all of the statistics. But that maybe we wouldn't

17:57.200 --> 18:03.200
feel like there's a free will. Maybe we need that for the... This is an illusion that noise can give

18:03.280 --> 18:08.960
you free will. At least in that way, it's a feature. But also, if you don't have noise,

18:08.960 --> 18:15.120
you have chaotic phenomena, which are effectively like noise. So we can't get away with statistics

18:15.120 --> 18:19.520
even then. I mean, think about rolling a dice and forget about quantum mechanics and you know

18:19.520 --> 18:24.960
exactly how you throw it. But I mean, it's still so hard to compute the trajectory that, effectively,

18:24.960 --> 18:31.360
it is best to model it as coming out with a number, with probability one over six.

18:31.920 --> 18:39.280
But from this sort of philosophical Kolmogorov complexity perspective, if we didn't have noise,

18:39.840 --> 18:46.640
then arguably you could describe the whole universe as standard model plus

18:46.640 --> 18:50.800
generativity. I mean, we don't have a theory of everything yet, but sort of assuming we are

18:50.800 --> 18:55.600
close to it or have it here, plus the initial conditions, which may hopefully be simple. And

18:55.600 --> 19:00.640
then you just run it and then you would reproduce the universe. But that's spoiled by noise or by

19:00.720 --> 19:08.320
chaotic systems or by initial conditions, which may be complex. So now if we don't take the whole

19:08.320 --> 19:15.760
universe with just a subset, just take planet Earth. Planet Earth cannot be compressed into

19:15.760 --> 19:20.960
a couple of equations. This is a hugely complex system. So interesting. So when you look at the

19:20.960 --> 19:26.640
window, the whole thing might be simple, but when you just take a small window, then it may become

19:26.640 --> 19:33.200
complex. And that may be counterintuitive. But there's a very nice analogy, the book, the library

19:33.200 --> 19:38.160
of all books. So imagine you have a normal library with interesting books, and you go there, great.

19:38.160 --> 19:44.160
Lots of information and huge, quite complex. So now I create a library which contains all

19:44.160 --> 19:50.080
possible books, say, of 500 pages. So the first book just has AAA over all the pages. The next book

19:50.080 --> 19:55.200
AAA and ends with B. And so on. I create this library of all books. I can write a super short

19:55.200 --> 20:00.080
program which creates this library. So this library which has all books has zero information

20:00.080 --> 20:04.640
content. And you take a subset of this library and suddenly you have a lot of information in there.

20:05.200 --> 20:09.760
So that's fascinating. I think one of the most beautiful object, mathematical objects that

20:09.760 --> 20:14.160
at least today seems to be understudied or under talked about is cellular automata.

20:15.280 --> 20:20.000
What lessons do you draw from sort of the game of life for cellular automata, where you start with

20:20.000 --> 20:26.160
the simple rules, just like you're describing with the universe, and somehow complexity emerges.

20:26.160 --> 20:33.200
Do you feel like you have an intuitive grasp on the behavior, the fascinating behavior of such

20:33.200 --> 20:38.640
systems, where some, like you said, some chaotic behavior could happen, some complexity could

20:38.640 --> 20:44.640
emerge, some, it could die out in some very rigid structures. Do you have a sense about

20:45.360 --> 20:50.800
cellular automata that somehow transfers maybe to the bigger questions of our universe?

20:50.800 --> 20:55.200
Yeah, the cellular automata and especially the converse game of life is really great because

20:55.200 --> 20:59.360
this rule is so simple. You can explain it to every child and even by hand you can simulate a

20:59.360 --> 21:05.920
little bit. And you see this beautiful patterns emerge and people have proven that it's even

21:05.920 --> 21:10.480
touring complete. You cannot just use a computer to simulate game of life, but you can also use

21:10.480 --> 21:19.520
game of life to simulate any computer. That is truly amazing. And it's the prime example probably to

21:20.240 --> 21:26.480
demonstrate that very simple rules can lead to very rich phenomena. And people sometimes,

21:26.480 --> 21:31.600
you know, how can, how is chemistry and biology so rich? I mean, this can't be based on simple

21:31.600 --> 21:38.320
rules. But no, we know quantum electrodynamics describes all of chemistry. And we come later

21:38.400 --> 21:42.880
back to that. I claim intelligence can be explained or described in one single equation,

21:42.880 --> 21:49.200
this very rich phenomenon. You asked also about whether, you know, I understand this

21:49.200 --> 21:56.720
phenomenon and it's probably not. And this is saying you never understand really things,

21:56.720 --> 22:04.720
you just get used to them. And I think pretty used to cellular automata. So you believe that

22:04.720 --> 22:09.760
you understand now why this phenomenon happens. But I give you a different example. I didn't play

22:09.760 --> 22:15.360
too much this converse game of life, but a little bit more with fractals and with the

22:15.360 --> 22:19.760
Mandelbrot set. And you need beautiful, you know, patterns, just look Mandelbrot set.

22:20.960 --> 22:24.640
And well, when the computers were really slow and I just had a black and white

22:24.640 --> 22:28.480
monitor and programmed my own programs on an assembler too.

22:29.440 --> 22:36.960
Wow. Wow, you're legit to get these fractals on the screen. And it was mesmerized and much

22:36.960 --> 22:41.520
later. So I returned to this, you know, every couple of years. And then I tried to understand

22:41.520 --> 22:48.400
what is going on. And you can understand a little bit. So I tried to derive the locations,

22:48.400 --> 22:57.280
you know, there are these circles and the apple shape. And then you have smaller Mandelbrot sets

22:57.280 --> 23:03.360
recursively in this set. And there's a way to mathematically by solving high order polynomials

23:03.360 --> 23:09.440
to figure out where these centers are and what size they are approximately. And by sort of

23:09.440 --> 23:18.000
mathematically approaching this problem, you slowly get a feeling of why things are like they are.

23:18.000 --> 23:24.800
And that sort of isn't, you know, first step to understanding why this rich phenomenon.

23:24.800 --> 23:28.800
Do you think it's possible? What's your intuition? Do you think it's possible to reverse engineer

23:28.800 --> 23:35.520
and find the short program that generated the these fractals, sort of by what looking at the

23:35.520 --> 23:42.320
fractals? Well, in principle, yes. Yeah. So I mean, in principle, what you can do is you take,

23:42.320 --> 23:46.400
you know, any data set, you know, you take these fractals, or you take whatever your data set,

23:46.400 --> 23:53.280
whatever you have. So a picture of Conveys game of life. And you run through all programs, you

23:53.280 --> 23:57.200
take a program of size one, two, three, four, and all these programs around them all in parallel in

23:57.200 --> 24:02.960
so called dovetailing fashion, give them computational resources, first one 50%, second one half

24:02.960 --> 24:09.120
resources and so on and let them run, wait until they hold, give an output, compare it to your data.

24:09.120 --> 24:13.600
And if some of these programs produce the correct data, then you stop and then you have already

24:13.600 --> 24:18.320
some program, it may be a long program, because it's faster. And then you continue and you get

24:18.320 --> 24:23.040
shorter and shorter programs until you eventually find the shortest program. The interesting thing

24:23.040 --> 24:27.360
you can ever know whether it's a shortest program, because there could be an even shorter program,

24:27.360 --> 24:34.080
which is just even slower. And you just have to wait here. But asymptotically, and actually

24:34.080 --> 24:39.200
after the final time, you have the shortest program. So this is a theoretical, but completely

24:39.200 --> 24:47.840
impractical way of finding the underlying structure in every data set. And that was a

24:47.840 --> 24:51.840
lot more of induction does and Convogorov complexity. In practice, of course, we have to

24:51.840 --> 25:00.640
approach the problem more intelligently. And then if you take resource limitations into account,

25:00.640 --> 25:06.240
there's once the field of pseudo random numbers, yeah, and these are random numbers. So these are

25:06.240 --> 25:12.800
deterministic sequences, but no algorithm which is fast, fast means runs in polynomial time can

25:12.800 --> 25:18.560
detect that it's actually deterministic. So we can produce interesting, I mean, random numbers,

25:18.560 --> 25:23.360
maybe not that interesting, but just an example, we can produce complex looking data.

25:24.240 --> 25:28.800
And we can then prove that no fast algorithm can detect the underlying pattern.

25:31.600 --> 25:34.080
Which is unfortunately,

25:37.200 --> 25:41.280
that's a big challenge for our search for simple programs in the space of artificial intelligence,

25:41.280 --> 25:46.000
perhaps. Yes, it definitely is wanted vision intelligence. And it's quite surprising that

25:46.000 --> 25:53.280
it's, I can't say easy. I mean, physicists worked really hard to find these theories, but apparently

25:53.280 --> 25:57.840
it was possible for human minds to find these simple rules in the universe. It could have

25:57.840 --> 26:02.720
been different, right? It could have been different. It's, it's, it's awe inspiring.

26:04.560 --> 26:12.320
So let me ask another absurdly big question. What is intelligence in your view?

26:13.120 --> 26:15.040
So I have, of course, a definition.

26:16.960 --> 26:20.800
I wasn't sure what you're going to say, because you could have just as easy said, I have no clue.

26:21.360 --> 26:29.920
Which many people would say, but I'm not modest in this question. So the, the informal version,

26:31.600 --> 26:36.960
which I worked out together with Shane, like who co-founded the mind is that intelligence

26:36.960 --> 26:41.760
measures and agents ability to perform well in a wide range of environments.

26:43.120 --> 26:49.120
So that doesn't sound very impressive. And what it, these words have been very carefully chosen.

26:49.760 --> 26:56.800
And there is a mathematical theory behind that. And we come back to that later. And if you look at

26:56.800 --> 27:02.800
this, this definition by itself, it seems like, yeah, okay, but it seems a lot of things are

27:02.800 --> 27:10.320
missing. But if you think it's true, then you realize that most, and I claim all of the other

27:10.320 --> 27:14.640
traits, at least of rational intelligence, which we usually associate with intelligence,

27:14.640 --> 27:20.880
are emergent phenomena from this definition, like, you know, creativity, memorization, planning,

27:20.880 --> 27:26.560
knowledge. You all need that in order to perform well in a wide range of environments.

27:27.520 --> 27:30.160
So you don't have to explicitly mention that in a definition.

27:30.160 --> 27:35.200
Interesting. So yeah, so the consciousness, abstract reasoning, all these kinds of things

27:35.200 --> 27:42.000
are just emergent phenomena that help you in towards, can you say the definition again?

27:42.000 --> 27:45.520
So multiple environments. Did you mention the word goals?

27:46.240 --> 27:50.080
No, but we have an alternative definition instead of performing well, you can just replace it by

27:50.080 --> 27:55.280
goals. So intelligence measures and agents ability to achieve goals in a wide range of

27:55.280 --> 27:58.080
environments. That's more or less equal. But interesting, because in there,

27:58.080 --> 28:02.640
there's an injection of the word goals. So we want to specify there, there should be a goal.

28:03.200 --> 28:06.720
Yeah, but perform well is sort of, what does it mean? It's the same problem.

28:08.000 --> 28:12.160
There's a little bit gray area, but it's much closer to something that could be formalized.

28:14.240 --> 28:18.880
In your view, are humans, where do humans fit into that definition? Are they

28:19.600 --> 28:26.960
general intelligence systems that are able to perform in like, how good are they at fulfilling

28:26.960 --> 28:30.480
that definition at performing well in multiple environments?

28:31.280 --> 28:35.680
Yeah, that's a big question. I mean, the humans are performing best among all

28:36.880 --> 28:39.920
species on Earth. Species we know, we know of. Yeah.

28:40.720 --> 28:45.760
Depends. You could say that trees and plants are doing a better job. They'll probably outlast us.

28:46.400 --> 28:50.400
So yeah, but they are in a much more narrow environment, right? I mean, you just, you know,

28:50.400 --> 28:54.720
have a little bit of air pollution and these trees die and we can adapt, right? We build houses,

28:54.800 --> 29:01.120
we build filters, we do geo-engineering. So the multiple environment part.

29:01.120 --> 29:05.120
Yeah, that is very important, yes. So that distinguish narrow intelligence from wide

29:05.120 --> 29:13.040
intelligence, also in the AI research. So let me ask the alenturing question, can machines

29:13.840 --> 29:20.400
think? Can machines be intelligent? So in your view, I have to kind of ask, the answer is probably

29:20.400 --> 29:27.200
yes, but I want to kind of hear what your thoughts on it. Can machines be made to fulfill this

29:27.200 --> 29:33.680
definition of intelligence, to achieve intelligence? Well, we are sort of getting there and, you know,

29:33.680 --> 29:39.040
on a small scale, we are already there. The wide range of environments are missing,

29:39.040 --> 29:44.480
but we have self-driving cars, we have programs to play go and chess, we have speech recognition.

29:44.480 --> 29:48.240
So it's pretty amazing, but you can, you know, these are narrow environments.

29:48.480 --> 29:54.160
But if you look at AlphaZero, that was also developed by DeepMind. I mean,

29:54.160 --> 29:59.920
got famous with AlphaGo and then came AlphaZero a year later. That was truly amazing. So

29:59.920 --> 30:05.760
on reinforcement learning algorithm, which is able just by self-play to play chess

30:07.040 --> 30:11.760
and then also go. And I mean, yes, they're both games, but they're quite different games. And,

30:11.760 --> 30:16.640
you know, this, you didn't, don't feed them the rules of the game. And the most remarkable thing,

30:16.640 --> 30:21.760
which is still a mystery to me that usually for any decent chess program, I don't know much about

30:21.760 --> 30:29.120
Go, you need opening books and end game tables and so on, too. And nothing in there, nothing was

30:29.120 --> 30:33.440
put in there. Especially with AlphaZero, the self-play mechanism, starting from scratch,

30:33.440 --> 30:42.000
being able to learn actually new strategies. It really discovered, you know, all these

30:42.000 --> 30:48.000
famous openings within four hours by itself. What I was really happy about, I'm a terrible

30:48.000 --> 30:53.040
chess player, but I like Queen Gambi. And AlphaZero figured out that this is the best opening.

30:54.720 --> 31:01.920
Finally, somebody proved you correct. So yes, to answer your question, yes,

31:01.920 --> 31:09.120
I believe that general intelligence is possible. And it also depends how you define it. Do you say

31:09.120 --> 31:15.280
AGI with general intelligence, artificial intelligence, only refers to if you achieve

31:15.280 --> 31:19.840
human level or a sub-human level, but quite broad? Is it also general intelligence,

31:19.840 --> 31:25.040
so we have to distinguish or it's only super human intelligence, general artificial intelligence?

31:25.040 --> 31:29.760
Is there a test in your mind, like the Turing test for natural language or some other test

31:29.760 --> 31:34.400
that would impress the heck out of you, that would kind of cross the line of

31:35.360 --> 31:41.600
your sense of intelligence within the framework that you said? Well, the Turing test has been

31:41.600 --> 31:46.800
criticized a lot, but I think it's not as bad as some people think. And some people think it's too

31:46.800 --> 31:55.120
strong. So it tests not just for a system to be intelligent, but it also has to fake human

31:55.120 --> 32:00.000
deception. Disception, right? Which is, you know, much harder. And on the other hand,

32:00.000 --> 32:07.040
they say it's too weak because it's just maybe fakes, you know, emotions or intelligent behavior.

32:07.680 --> 32:13.920
It's not real. But I don't think that's the problem or big problem. So if you would pass the Turing

32:13.920 --> 32:22.560
test, so a conversation or a terminal with a bot for an hour, or maybe a day or so, and you can

32:22.560 --> 32:27.120
fool a human into, you know, not knowing whether this is a human or not, that it's the Turing test,

32:27.600 --> 32:32.320
I would be truly impressed. And we have this annual competition, the Lubna

32:33.360 --> 32:37.600
prize. And I mean, it started with Eliza, that was the first conversational program.

32:38.240 --> 32:43.200
And what is it called the Japanese Mitsuko or so, that's the winner of the last, you know,

32:43.200 --> 32:48.880
couple of years. And well, it's impressive. Yeah, it's quite impressive. And then Google has

32:48.880 --> 32:55.680
developed Mina, right? Just recently, that's an open domain conversational bot. Just a couple of

32:55.680 --> 33:01.520
weeks ago, I think. Yeah, I kind of like the metric that sort of the Alexa price has proposed.

33:01.520 --> 33:05.360
I mean, maybe it's obvious to you, it wasn't to me of setting sort of a length

33:06.480 --> 33:11.600
of a conversation. Like, you want the bot to be sufficiently interesting that you'd want to keep

33:11.600 --> 33:19.440
talking to it for like 20 minutes. And that's a surprisingly effective and aggregate metric.

33:19.440 --> 33:27.680
Because really, like, nobody has the patience to be able to talk to a bot that's not interesting

33:27.680 --> 33:33.440
and intelligent and witty and is able to go into different tangents, jump domains, be able to,

33:34.000 --> 33:38.240
you know, say something interesting to maintain your attention. Maybe many humans will also fail

33:38.240 --> 33:45.280
this test. Unfortunately, we set just like with autonomous vehicles with chatbots,

33:45.280 --> 33:49.920
we also set a bar that's way too hard to reach. I said, you know, the Turing test is not as bad

33:49.920 --> 33:56.560
as some people believe. But what is really not useful about the Turing test, it gives us no

33:56.560 --> 34:01.920
guidance how to develop these systems in the first place. Of course, you know, we can develop them

34:01.920 --> 34:06.800
by trial and error and, you know, do whatever and then run the test and see whether it works or not.

34:06.800 --> 34:16.880
But a mathematical definition of intelligence gives us, you know, an objective which we can then

34:16.880 --> 34:23.200
analyze by, you know, theoretical tools or computational and, you know, maybe even prove

34:23.200 --> 34:30.480
how close we are. And we will come back to that later with the ISE model. So I mentioned the

34:30.480 --> 34:36.720
compression, right? So in natural language processing, they have achieved amazing results.

34:36.720 --> 34:40.800
And one way to test this, of course, you know, take the system, you train it, and then you,

34:40.800 --> 34:48.000
you know, see how well it performs on the task. But a lot of performance measurement is done by

34:48.000 --> 34:53.280
so-called perplexity, which is essentially the same as complexity or compression length.

34:53.280 --> 34:57.520
So the NLP community develops new systems, and then they measure the compression length,

34:57.520 --> 35:03.440
and then they have ranking and leaks, because there's a strong correlation between

35:03.440 --> 35:08.400
compressing well, and then the systems performing well at the task at hand. It's not perfect,

35:08.400 --> 35:13.360
but it's good enough for them as an intermediate aim.

35:14.560 --> 35:19.840
So you mean measure, so this is kind of almost returning to the common growth complexity. So

35:19.840 --> 35:25.120
you're saying good compression usually means good intelligence. Yes.

35:26.880 --> 35:34.800
So you mentioned you're one of the, one of the only people who dared boldly to try to

35:34.800 --> 35:41.600
formalize the idea of artificial general intelligence, to have a mathematical framework

35:41.600 --> 35:50.560
for intelligence, just like as we mentioned, termed IEXI, A-I-X-I. So let me ask the basic

35:50.560 --> 35:58.560
question, what is IEXI? Okay, so let me first say what it stands for, because what it stands for,

35:58.560 --> 36:02.720
actually, that's probably the more basic question. The first question is usually how

36:03.520 --> 36:08.080
how it's pronounced, but finally I put it on the website, how it's pronounced, and you figured it out.

36:09.040 --> 36:15.360
Yeah. The name comes from AI, artificial intelligence, and the XI is the Greek letter

36:15.360 --> 36:23.200
XI, which are used for Solomonov's distribution for quite stupid reasons, which I'm not willing to

36:23.200 --> 36:30.640
repeat here in front of camera. So it does happen to be more or less arbitrary, I chose the XI,

36:31.440 --> 36:37.760
but it also has nice other interpretations. So there are actions and perceptions in this

36:37.760 --> 36:44.960
model, where an agent has actions and perceptions, and over time, so this is A-I-X-I, so there's

36:44.960 --> 36:50.560
an action at time I, and then followed by a perception at time I. We'll go with that. I'll

36:50.560 --> 36:56.720
edit out the first part. I'm just kidding. I have some more interpretations. So at some point,

36:56.720 --> 37:04.000
maybe five years ago or 10 years ago, I discovered in Barcelona, it was on a big church,

37:04.560 --> 37:11.200
there was a stone engraved, some text, and the word Aixi appeared there a couple of times.

37:12.720 --> 37:19.280
I was very surprised and happy about that, and I looked it up, so this is Catalan language,

37:19.280 --> 37:24.080
and it means with some interpretation, that's it, that's the right thing to do. Yeah, Heureka.

37:24.720 --> 37:32.000
Oh, so it's almost like destined, somehow came to you in a dream.

37:32.000 --> 37:36.480
And similar, there's a Chinese word, Aixi, also written like Aixi, if you transcribe that to

37:36.480 --> 37:42.480
Pingen. And the final one is that is A-I, crossed with induction, because that is, and that's going

37:42.480 --> 37:48.240
more to the content now. So good old fashioned A-I is more about planning a known deterministic

37:48.240 --> 37:53.760
world, and induction is more about often IID data and inferring models, and essentially,

37:53.760 --> 37:58.640
what this Aixi model does is combining these two. And I actually also recently,

37:58.640 --> 38:05.920
I think heard that in Japanese, A-I means love. So if you can combine X-I somehow with that,

38:06.560 --> 38:12.240
I think we can, there might be some interesting ideas there. So Aixi, let's then take the next

38:12.240 --> 38:19.440
step. Can you maybe talk at the big level of what is this mathematical framework?

38:19.680 --> 38:26.480
So it consists essentially of two parts. One is the learning and induction and prediction part.

38:26.480 --> 38:31.760
And the other one is the planning part. So let's come first to the learning induction

38:31.760 --> 38:38.480
prediction part, which essentially I explained already before. So what we need for any agent

38:39.200 --> 38:44.640
to act well is that it can somehow predict what happens. I mean, if you have no idea what your

38:44.640 --> 38:50.640
actions do, how can you decide which acts are good or not? So you need to have some model of

38:50.640 --> 38:57.840
what your actions effect. So what you do is you have some experience, you build models like scientists

38:57.840 --> 39:02.480
of your experience, then you hope these models are roughly correct, and then you use these models

39:02.480 --> 39:07.520
for prediction. And a model is, sorry to interrupt, and a model is based on your perception of the

39:07.520 --> 39:14.160
world, how your actions will affect that world. That's not the important part.

39:14.640 --> 39:18.400
It is technically important, but at this stage, we can just think about predicting, say,

39:19.120 --> 39:23.920
stock market data, whether data or IQ sequences, one, two, three, four, five, what comes next.

39:23.920 --> 39:30.160
Yeah. So of course, our actions affect what we're doing, but I'll come back to that in a second.

39:30.160 --> 39:36.400
So, and I'll keep just interrupting. So just to draw a line between prediction and planning.

39:37.440 --> 39:41.840
What do you mean by prediction in this way? It's trying to predict the

39:42.720 --> 39:48.000
environment without your long-term action in the environment. What is prediction?

39:49.440 --> 39:52.320
Okay. If you want to put the actions in now, okay, then let's put in them now.

39:54.720 --> 40:00.160
We don't have to put them now. Scratch it. Don't question. Okay. So the simplest form of prediction

40:00.160 --> 40:06.160
is that you just have data that you passively observe, and you want to predict what happens

40:06.160 --> 40:13.280
without, you know, interfering. As I said, weather forecasting, stock market, IQ sequences, or just

40:14.880 --> 40:20.000
anything. Okay. And Solominov's theory of induction based on compression. So you look for the shortest

40:20.000 --> 40:24.320
program which describes your data sequence. And then you take this program, run it,

40:24.320 --> 40:29.040
which reproduces your data sequence by definition, and then you let it continue running,

40:29.040 --> 40:34.240
and then it will produce some predictions. And you can rigorously prove that for any

40:34.240 --> 40:41.280
prediction task, this is essentially the best possible predictor. Of course, if there's a prediction

40:41.280 --> 40:47.120
task, or a task which is unpredictable, like, you know, your fair coin flips, yeah, I cannot

40:47.120 --> 40:50.880
predict the next fair coin, but what Solominov does is says, okay, next head is probably 50%.

40:51.520 --> 40:55.120
It's the best you can do. So if something is unpredictable, Solominov will also not

40:55.120 --> 41:00.560
magically predict it. But if there is some pattern and predictability, then Solominov

41:00.640 --> 41:06.000
induction will figure that out, eventually, and not just eventually, but rather quickly,

41:06.000 --> 41:13.760
and you can have proof convergence rates, whatever your data is. So there's pure magic in a sense.

41:14.640 --> 41:18.160
What's the catch? Well, the catch is that it's not computable. And we come back to that later.

41:18.160 --> 41:22.640
You cannot just implement it in even this Google resources here, and run it and, you know, predict

41:22.640 --> 41:27.040
the stock market and become rich. I mean, if it's raised Solominov already, you know, tried it at the

41:27.040 --> 41:32.720
time. But so the basic task is you're in the environment, and you're interacting with an

41:32.720 --> 41:37.520
environment to try to learn a model of that environment. And the model is in the space of

41:37.520 --> 41:41.200
these all these programs. And your goal is to get a bunch of programs that are simple.

41:41.200 --> 41:45.280
And so let's, let's go to the actions now. But actually, good that you asked usually,

41:45.280 --> 41:49.600
I skipped this part, although there is also a minor contribution, which I did. So the action part,

41:49.600 --> 41:53.360
but they usually sort of just jump to the decision part. So let me explain the action part now.

41:53.360 --> 42:01.040
Thanks for asking. So you have to modify it a little bit by now not just predicting a sequence

42:01.040 --> 42:07.680
which just comes to you, but you have an observation, then you act somehow. And then you want to predict

42:07.680 --> 42:13.920
the next observation based on the past observation and your action. Then you take the next action.

42:14.560 --> 42:18.960
You don't care about predicting it because you're doing it. And then you get the next observation.

42:18.960 --> 42:22.720
And you want, well, before you get it, you want to predict it again based on your past

42:22.720 --> 42:29.360
action and observation sequence. You just condition extra on your actions. There's an

42:29.360 --> 42:33.040
interesting alternative that you also try to predict your own actions.

42:35.520 --> 42:40.320
If you want. In the past or the future. Your future actions. That's interesting.

42:41.840 --> 42:47.360
Wait, let me wrap. I think my brain is broke. We should maybe discuss that later

42:47.360 --> 42:51.200
after I've explained the ICSE model. That's an interesting variation. But that is a really

42:51.200 --> 42:55.440
interesting variation. And a quick comment. I don't know if you want to insert that in here.

42:55.440 --> 43:01.120
But you're looking at that in terms of observations, you're looking at the entire big

43:01.120 --> 43:05.600
history, the long history of the observations. Exactly. That's very important, the whole history

43:05.600 --> 43:10.800
from birth sort of of the agent. And we can come back to that. Also why this is important here.

43:10.800 --> 43:15.760
Often, you know, in RL, you have MDPs, macro decision processes, which are much more limiting.

43:15.760 --> 43:21.520
Okay, so now we can predict conditioned on actions. So even if the influence environment.

43:21.520 --> 43:26.080
But prediction is not all we want to do, right? We also want to act really in the world.

43:26.800 --> 43:32.080
And the question is how to choose the actions. And we don't want to greedily choose the actions.

43:33.280 --> 43:37.520
You know, just, you know, what is best in the next time step. And we first I should say, you

43:37.520 --> 43:41.840
know, what is, you know, how do we measure performance? So we measure performance by giving

43:41.840 --> 43:47.040
the agent reward. That's the so called reinforcement learning framework. So every time step,

43:47.040 --> 43:51.120
you can give it a positive reward or negative reward, or maybe no reward, it could be a very

43:51.120 --> 43:55.360
scarce, right? Like if you play chess, just at the end of the game, you give plus one for winning

43:55.360 --> 43:59.840
or minus one for losing. So in the IXI framework, that's completely sufficient. So occasionally,

43:59.840 --> 44:05.120
you give a reward signal, and you ask the agent to maximize reward, but not greedily sort of,

44:05.120 --> 44:08.640
you know, the next one, next one, because that's very bad in the long run, if you're greedy.

44:08.880 --> 44:14.000
So, but over the lifetime of the agent. So let's assume the agent lives for M

44:14.000 --> 44:18.320
time steps as they dice in sort of 100 years sharp. That's just, you know, the simplest model

44:18.320 --> 44:25.120
to explain. So it looks at the future reward sum and ask what is my action sequence, or actually

44:25.120 --> 44:30.880
more precisely my policy, which leads in expectation, because I don't know the world,

44:32.080 --> 44:37.200
to the maximum reward sum. Let me give you an analogy. In chess, for instance,

44:38.160 --> 44:43.360
we know how to play optimally in theory, it's just a mini max strategy. I play the move which

44:43.360 --> 44:48.560
seems best to me under the assumption that the opponent plays the move which is best for him,

44:48.560 --> 44:54.480
so best, so worst for me, under the assumption that he I play again, the best move. And then you

44:54.480 --> 44:59.120
have this expecting max three to the end of the game. And then you back propagate and then you

44:59.120 --> 45:02.960
get the best possible move. So that is the optimal strategy, which for Neumann already

45:02.960 --> 45:10.640
figured out a long time ago, for playing adversarial games, luckily, or maybe unluckily,

45:10.640 --> 45:16.400
for the theory, it becomes harder that world is not always adversarial. So it can be,

45:16.400 --> 45:21.280
if the other humans even cooperative here, or nature is usually I mean, the dead nature is

45:21.280 --> 45:27.520
stochastic, you know, things just happen randomly, or don't care about you. So what you have to

45:27.520 --> 45:32.000
take into account is the noise, you know, and not necessarily adversariality. So you replace

45:32.000 --> 45:37.360
the minimum on the opponent's side by an expectation, which is general enough to include

45:37.360 --> 45:43.040
also adversarial cases. So now instead of a mini max strategy of an expecting max strategy.

45:43.760 --> 45:46.960
So far so good. So that is well known. It's called sequential decision theory.

45:47.920 --> 45:53.200
But the question is, on which probability distribution do you base that if I have the

45:53.200 --> 45:57.840
true probability distribution, like say I play beggining, right, there's dice,

45:57.840 --> 46:01.680
and there's certain randomness involved, yeah, I can calculate probabilities and feed it in the

46:01.680 --> 46:05.760
expecting max or the sequential decision tree, come up with the optimal decision if I have

46:05.760 --> 46:10.800
enough compute. But in the for the real world, we don't know that, you know, what is the probability

46:11.520 --> 46:17.040
the driver in front of me breaks. I don't know. Yeah, so depends on all kinds of things. And

46:17.680 --> 46:22.800
especially new situations, I don't know. So this is this unknown thing about prediction. And there's

46:22.800 --> 46:27.760
where Solomanov comes in. So what you do is in sequential decision tree, you just replace the

46:27.760 --> 46:34.000
true distribution, which we don't know, by this universal distribution, I didn't explicitly

46:34.000 --> 46:39.040
talk about it, but this is used for universal prediction, and plug it into the sequential

46:39.040 --> 46:44.000
decision tree mechanism. And then you get the best of both worlds. You have a long term planning

46:44.000 --> 46:49.760
agent. But it doesn't need to know anything about the world, because the Solomanov induction part

46:50.720 --> 46:58.480
learns. Can you explicitly try to describe the universal distribution and how Solomanov induction

46:58.480 --> 47:03.760
plays a role here? I'm trying to understand. So what he does it. So in the simplest case,

47:03.760 --> 47:08.000
I said, take the shortest program describing your data, run it, have a prediction which would be

47:08.000 --> 47:14.080
deterministic. Yes. Okay. But you should not just take the shortest program, but also consider the

47:14.080 --> 47:22.320
longer ones, but keep it lower a priori probability. So in the Bayesian framework, you say a priori,

47:22.320 --> 47:30.240
any distribution, which is a model, or a stochastic program has a certain a priori

47:30.240 --> 47:34.480
probability, which is two to the minus and y two to the minus length, you know, I could explain

47:34.480 --> 47:41.280
length of this program. So longer programs are punished, a priori. And then you multiply it

47:41.280 --> 47:48.080
with the so called likelihood function. Yeah, which is as the name suggests, is how likely

47:48.080 --> 47:54.160
is this model given the data at hand. So if you have a very wrong model, it's very unlikely

47:54.160 --> 47:58.800
that this model is true. And so it is very small numbers. So even if the model is simple, it gets

47:58.800 --> 48:04.480
penalized by that. And what you do is then you take just the sum, but this is the average over it.

48:04.480 --> 48:10.480
And this gives you a probability distribution. So universal distribution or Solomanov distribution.

48:10.480 --> 48:14.320
So it's weighed by the simplicity of the program and likelihood. Yes.

48:15.200 --> 48:22.480
It's kind of a nice idea. Yeah. So okay. And then you said there's your planning

48:22.480 --> 48:28.480
n or m or forgot the letter steps into the future. So how difficult is that problem? What's

48:28.480 --> 48:32.720
involved there? Okay, basic optimization problem? What are we talking about? Yeah, so you have a

48:32.720 --> 48:38.640
planning problem up to horizon m. And that's exponential time in the horizon m, which is,

48:38.640 --> 48:43.440
I mean, it's computable, but in fact, intractable. I mean, even for chess, it's already intractable

48:43.440 --> 48:48.080
to do that exactly. And, you know, for though, but it could be also discounted kind of framework

48:48.080 --> 48:54.560
where so, so having a hard horizon, you know, at 100 years, it's just for simplicity of

48:54.560 --> 49:00.000
discussing the model. And also sometimes the master simple. But there are lots of variations.

49:00.000 --> 49:07.120
Actually, quite interesting parameter is it's, there's nothing really problematic about it.

49:07.120 --> 49:10.960
But it's very interesting. So for instance, you think, no, let's, let's, let's, let's let the

49:10.960 --> 49:16.240
parameter m tend to infinity, right? You want an agent, which lives forever, right? If you do it

49:16.240 --> 49:20.560
now, you have two problems. First, the mathematics breaks down because you have an infinite reward

49:20.560 --> 49:26.240
sum, which may give infinity and getting reward 0.1 in the time step is infinity and giving reward

49:26.240 --> 49:32.640
one every time step is infinity. So equally good. Not really what we want. Other problem is that

49:33.600 --> 49:39.120
if you have an infinite life, you can be lazy for as long as you want for 10 years and then catch

49:39.120 --> 49:44.960
up with the same expected reward. And, you know, think about yourself or, you know, or maybe,

49:44.960 --> 49:51.280
you know, some friends or so, if they knew they lived forever, you know, why work hard now, you

49:51.280 --> 49:55.440
know, just enjoy your life, you know, and then catch up later. So that's another problem with

49:55.440 --> 50:00.560
the infinite horizon. And you mentioned, yes, we can go to discounting. But then the standard

50:00.640 --> 50:05.920
discounting is so-called geometric discounting. So a dollar today is about worth as much as,

50:05.920 --> 50:11.520
you know, $1.05 tomorrow. So if you do the so-called geometric discounting, you have introduced an

50:11.520 --> 50:18.320
effective horizon. So the agent is now motivated to look ahead a certain amount of time effectively.

50:18.320 --> 50:25.120
It's like a moving horizon. And for any fixed effective horizon, there is a problem

50:25.920 --> 50:30.400
to solve which requires larger horizons. So if I look ahead, you know, five time steps,

50:30.400 --> 50:35.520
I'm a terrible chess player, right? I'll need to look ahead longer. If I play go, I probably

50:35.520 --> 50:41.280
have to look ahead even longer. So for every problem, no, for every horizon, there is a problem

50:41.280 --> 50:47.280
which this horizon cannot solve. But I introduced the so-called near harmonic horizon, which goes

50:47.280 --> 50:52.640
down with one over t rather than exponentially t, which produces an agent which effectively looks

50:52.640 --> 50:57.280
into the future proportional to each age. So if it's five years old, it plans for five years.

50:57.280 --> 51:02.000
If it's 100 years old, it then plans for 100 years. And it's a little bit similar to humans,

51:02.000 --> 51:06.240
too, right? I mean, children don't plan ahead very long, but then we get adults, we play ahead

51:06.240 --> 51:10.160
more longer. Maybe when we get very old, I mean, we know that we don't live forever,

51:10.160 --> 51:16.960
you know, maybe then our horizon shrinks again. So that's really interesting. So adjusting the

51:16.960 --> 51:21.520
horizon, what is there some mathematical benefit of that? Or is it just a nice

51:22.880 --> 51:27.120
I mean, intuitively, empirically, it will probably be a good idea to sort of push the

51:27.120 --> 51:33.760
horizon back to extend the horizon as you experience more of the world. But is there

51:33.760 --> 51:36.480
some mathematical conclusions here that are beneficial?

51:37.040 --> 51:44.880
With Salomon's prediction part, we have extremely strong finite time, finite data results. So you

51:44.880 --> 51:49.200
have so and so much data, then you lose so and so much. So the deterioration is really great.

51:49.280 --> 51:55.920
With the IKC model with the planning part, many results are only asymptotic, which, well, this is

51:56.720 --> 52:01.680
what is asymptotic means you can prove, for instance, that in the long run, if the agent,

52:01.680 --> 52:06.320
you know, acts long enough, then, you know, it performs optimal or some nice thing happens.

52:06.320 --> 52:11.200
So but you don't know how fast it converges. Yeah, so it may converge fast, but we're just

52:11.200 --> 52:17.200
not able to prove it because a difficult problem. Or maybe there's a bug in the in the in the model

52:17.200 --> 52:22.720
so that it's really that slow. Yeah. So so that is what asymptotic means sort of eventually,

52:22.720 --> 52:30.800
but we don't know how fast. And if I give the agent a fixed horizon M, yeah, then I cannot prove

52:30.800 --> 52:36.240
asymptotic results, right? So I mean, sort of if it dies in 100 years, then in 100 years is over,

52:36.240 --> 52:41.760
I cannot say eventually. So this is the advantage of the discounting that I can prove asymptotic

52:41.760 --> 52:51.040
results. So just to clarify, so so I okay, I made I've built up a model with now in the moment of

52:51.920 --> 52:57.520
have this way of looking several steps ahead. How do I pick what action I will take?

52:58.800 --> 53:03.840
It's like with a playing chess, right, you do this mini max. In this case, here do you expect the

53:03.840 --> 53:11.440
max based on the solometer of distribution, you propagate back. And then, while an action falls

53:11.440 --> 53:16.720
out, the action which maximizes the future expected reward on the solometer of distribution,

53:16.720 --> 53:20.880
and then you just take this action, and then repeat. And then you get a new observation,

53:20.880 --> 53:25.760
and you feed it in this action observation, then you repeat and the reward so on. Yeah. So you wrote

53:25.760 --> 53:31.520
to you. And then maybe you can even predict your own action. I love the idea. But okay, this big

53:31.520 --> 53:39.840
framework. What is it? I mean, it's kind of a beautiful mathematical framework to think about

53:39.840 --> 53:46.160
artificial general intelligence. What can you, what does it help you into it about

53:47.600 --> 53:55.280
how to build such systems or maybe from another perspective? What does it help us in understanding

53:55.280 --> 54:03.280
AGI? So when I started in the field, I was always interested in two things. One was, you know, AGI.

54:04.240 --> 54:10.800
The name didn't exist then, what called general AI or strong AI. And the physics of everything.

54:10.800 --> 54:14.640
So I switched back and forth between computer science and physics quite often.

54:14.640 --> 54:16.560
You said the theory of everything. The theory of everything.

54:17.360 --> 54:21.120
It was basically the biggest problems before all humanity.

54:23.360 --> 54:29.520
Yeah, I can explain if you wanted some later time, you know, why I'm interested in these two

54:29.520 --> 54:37.040
questions. Can I ask you, in a small tangent, if, if, if one to be, it was one to be solved,

54:37.040 --> 54:42.720
which one would you, if one, if you were, if an apple fell in your head, and there was a brilliant

54:42.720 --> 54:48.480
insight and you could arrive at the solution to one, would it be AGI or the theory of everything?

54:49.120 --> 54:53.120
Definitely AGI, because once the AGI problem is solved, they can ask the AGI to solve the

54:53.120 --> 55:01.120
other problem for me. Yeah, brilliantly put. Okay. So, so as you were saying about it.

55:01.120 --> 55:06.640
Okay. So, and the reason why it didn't settle, I mean, this thought about, you know,

55:07.280 --> 55:11.120
once you have solved AGI, it solves all kinds of other, not just the theory of every problem,

55:11.120 --> 55:16.400
but all kinds of use, more useful problems to humanity is very appealing to many people. And,

55:16.400 --> 55:23.920
you know, I had this thought also, but I was quite disappointed with the state of the art

55:23.920 --> 55:28.800
of the field of AI. There was some theory, you know, about logical reasoning, but I was never

55:28.800 --> 55:33.680
convinced that this will fly. And then there was this more, more heuristic approaches with neural

55:33.680 --> 55:40.080
networks, and I didn't like these heuristics. So, and also I didn't have any good idea myself.

55:42.160 --> 55:45.360
So that's the reason why I toggled back and forth quite some while and even worked

55:45.360 --> 55:49.600
so four and a half years in a company developing software or something completely unrelated.

55:49.600 --> 55:56.960
But then I had this idea about the ICSE model. And so what it gives you, it gives you a gold

55:56.960 --> 56:03.760
standard. So I have proven that this is the most intelligent agents, which anybody could

56:04.960 --> 56:10.560
build built in quotation mark, because it's just mathematical and you need infinite compute.

56:11.040 --> 56:17.040
But this is the limit. And this is completely specified. It's not just a framework and, you know,

56:18.400 --> 56:23.440
every year, tens of frameworks are developed, which is just skeletons, and then pieces are

56:23.440 --> 56:27.280
missing. And usually these missing pieces, you know, turn out to be really, really difficult.

56:27.280 --> 56:33.680
And so this is completely and uniquely defined. And we can analyze that mathematically. And

56:35.120 --> 56:39.440
we've also developed some approximations, I can talk about that a little bit later,

56:40.160 --> 56:44.160
that would be sort of the top down approach, like, say, for Neumann's minimax theory,

56:44.160 --> 56:49.520
that's the theoretical optimal play of games. And now we need to approximate it, put heuristics

56:49.520 --> 56:53.040
in prune, the tree, blah, blah, blah, and so on. So we can do that also with the ICSE model,

56:53.040 --> 57:00.720
but for generally I, it can also inspire those. And most of most researchers go bottom upright,

57:00.720 --> 57:05.840
they have the systems that try to make it more general, more intelligent. It can inspire in which

57:05.840 --> 57:11.840
direction to go. What do you mean by that? So if you have some choice to make, right, so how should

57:11.840 --> 57:18.160
they evaluate my system if I can't do cross validation? How should they do my learning if

57:18.160 --> 57:23.120
my standard regularization doesn't work well? Yeah. So the answer is always this, we have a system

57:23.120 --> 57:28.480
which does everything that's ICSE. It's just, you know, completely in the ivory tower, completely

57:28.480 --> 57:33.520
useless from a practical point of view. But you can look at it and see, ah, yeah, maybe, you know,

57:33.520 --> 57:37.520
I can take some aspects and, you know, instead of Kolmogorov complexity, they just take some

57:37.520 --> 57:43.040
compressors which has been developed so far. And for the planning, well, we have UCT which has also,

57:43.040 --> 57:51.600
you know, been used in Go. And it, at least it's inspired me a lot to have this formal

57:53.280 --> 57:57.680
definition. And if you look at other fields, you know, like, I always come back to physics

57:57.680 --> 58:01.440
because I have a physics background, think about the phenomenon of energy that was

58:01.440 --> 58:05.120
long time a mysterious concept. And at some point, it was completely formalized.

58:06.160 --> 58:11.280
And that really helped a lot. And you can point out a lot of these things which were

58:11.280 --> 58:15.280
first mysterious and vague, and then they have been rigorously formalized.

58:15.280 --> 58:20.400
Speed and acceleration has been confused, right, until it was formally defined. There was a time

58:20.400 --> 58:25.840
like this. And people, you know, often, you know, don't have any background, you know, still confuse

58:25.840 --> 58:33.120
it. So, and this IXI model or the intelligence definitions, which is sort of the dual to it,

58:33.120 --> 58:38.320
we come back to that later, formalizes the notion of intelligence uniquely and rigorously.

58:38.880 --> 58:42.480
So in a sense, it serves as kind of the light at the end of the tunnel.

58:43.040 --> 58:48.320
Yes, yeah. So I mean, there's a million questions I could ask her. So maybe

58:49.440 --> 58:54.640
the kind of, okay, let's feel around in the dark a little bit. So there's been here a deep mind,

58:54.640 --> 58:58.000
but in general, been a lot of breakthrough ideas, just like we've been saying around

58:58.000 --> 59:03.600
reinforcement learning. So how do you see the progress in reinforcement learning is different?

59:04.320 --> 59:11.120
Like, which subset of IXI does it occupy the current, like you said, maybe

59:11.920 --> 59:15.600
the Markov assumption is made quite often in reinforcement learning.

59:16.400 --> 59:22.960
The, there's other assumptions made in order to make the system work. What do you see as the

59:22.960 --> 59:25.760
difference connection between reinforcement learning and IXI?

59:26.640 --> 59:32.480
And so the major difference is that essentially all other approaches,

59:33.360 --> 59:37.600
they make stronger assumptions. So in reinforcement learning, the Markov assumption

59:38.240 --> 59:43.280
is that the next state or next observation only depends on the on the previous observation

59:43.280 --> 59:48.000
and not the whole history, which makes, of course, the mathematics much easier rather than dealing

59:48.000 --> 59:53.360
with histories. Of course, they profit from it also, because then you have algorithms that run

59:53.360 --> 59:59.520
on current computers and do something practically useful. But for generally, I all the assumptions

59:59.520 --> 01:00:07.040
which are made by other approaches, we know already now they are limiting. So, for instance,

01:00:07.840 --> 01:00:11.520
usually you need a gothicity assumption in the MDP frameworks in order to learn.

01:00:11.520 --> 01:00:16.080
A gothicity essentially means that you can recover from your mistakes and that they are

01:00:16.080 --> 01:00:21.680
not traps in the environment. And if you make this assumption, then essentially you can go back

01:00:21.680 --> 01:00:28.640
to a previous state, go there a couple of times and then learn what, what statistics and what

01:00:28.640 --> 01:00:33.840
this state is like. And then in the long run perform well in this state. But there are no

01:00:33.840 --> 01:00:40.480
fundamental problems. But in real life, we know, there can be one single action, one second of

01:00:40.480 --> 01:00:46.880
being inattentive while driving a car fast, you know, can ruin the rest of my life, I can become

01:00:46.880 --> 01:00:51.440
quadruplegic or whatever. So, and there's no recovery anymore. So, the real world is not

01:00:51.440 --> 01:00:55.520
ergodic, I always say, you know, there are traps and there are situations where you're not recovered

01:00:55.520 --> 01:01:06.480
from. And very little theory has been developed for this case. What about what do you see in the

01:01:06.560 --> 01:01:15.200
context of IHC as the role of exploration, sort of, you mentioned, you know, in the real world

01:01:15.200 --> 01:01:20.240
and get into trouble when we make the wrong decisions and really pay for it. But exploration

01:01:20.240 --> 01:01:24.960
seems to be fundamentally important for learning about this world, for gaining new knowledge.

01:01:25.600 --> 01:01:31.360
So, is exploration baked in? Another way to ask it, what are the parameters

01:01:32.240 --> 01:01:38.240
of this of IHC that can be controlled? Yeah, I say the good thing is that there are no

01:01:38.240 --> 01:01:44.240
parameters to control. Some other people try knobs to control and you can do that. I mean,

01:01:44.240 --> 01:01:51.280
you can modify IHC so that you have some knobs to play with if you want to. But the exploration

01:01:51.280 --> 01:01:58.640
is directly baked in. And that comes from the Bayesian learning and the long term planning.

01:01:58.640 --> 01:02:08.560
So these together already imply exploration. You can nicely and explicitly prove that for

01:02:09.360 --> 01:02:17.920
simple problems like so-called banded problems, where you say to give a real-world example,

01:02:17.920 --> 01:02:22.160
say you have two medical treatments, A and B, you don't know the effectiveness, you try A a

01:02:22.160 --> 01:02:26.640
little bit, B a little bit, but you don't want to harm too many patients. So you have to sort of

01:02:27.600 --> 01:02:34.240
trade off exploring. And at some point you want to explore and you can do the mathematics and

01:02:34.240 --> 01:02:40.240
figure out the optimal strategy. It's called Bayesian agents, they're also non-Bayesian agents.

01:02:41.040 --> 01:02:46.560
But it shows that this Bayesian framework by taking a prior over possible worlds,

01:02:47.280 --> 01:02:51.360
doing the Bayesian mixture, then the Bayes optimal decision with long term planning that is important,

01:02:52.240 --> 01:02:58.960
automatically implies exploration also to the proper extent, not too much exploration

01:02:58.960 --> 01:03:04.560
and not too little. It is very simple settings. In the IHC model, I was also able to prove that

01:03:04.560 --> 01:03:08.800
it is a self-optimizing theorem or asymptotic optimality theorems, although they're only asymptotic,

01:03:08.800 --> 01:03:13.120
not finite time bounds. So it seems like the long term planning is a really important,

01:03:13.120 --> 01:03:18.880
the long term part of the planning is really important. And also, maybe a quick tangent,

01:03:18.880 --> 01:03:23.840
how important do you think is removing the Markov assumption and looking at the full

01:03:23.840 --> 01:03:30.080
history? So intuitively, of course, it's important, but is it like fundamentally

01:03:30.080 --> 01:03:35.440
transformative to the entirety of the problem? What's your sense of it? Because we all,

01:03:35.440 --> 01:03:39.840
we make that assumption quite often, just throwing away the past.

01:03:39.840 --> 01:03:47.360
Now, I think it's absolutely crucial. The question is whether there's a way to deal with it in a

01:03:47.360 --> 01:03:55.440
more heuristic and still sufficiently well way. So I have to come up with an example in the fly,

01:03:55.440 --> 01:04:02.000
but you have some key event in your life a long time ago, in some city or something,

01:04:02.000 --> 01:04:06.720
you realize it's a really dangerous street or whatever. And you want to remember that forever,

01:04:07.920 --> 01:04:12.000
in case you come back there. Kind of a selective kind of memory. So you remember

01:04:12.800 --> 01:04:16.640
all the important events in the past, but somehow selecting the important

01:04:17.360 --> 01:04:22.080
They're very hard. Yeah. And I'm not concerned about just storing the whole history. Just

01:04:22.080 --> 01:04:27.360
you can calculate human life, say 30 or 100 years doesn't matter, right?

01:04:28.720 --> 01:04:34.000
How much data comes in through the vision system and the auditory system, you compress it a little

01:04:34.000 --> 01:04:39.840
bit, in this case, lossily and store it. We are soon in the means of just storing it.

01:04:40.400 --> 01:04:46.000
But you still need to the selection for the planning part and the compression for the

01:04:46.000 --> 01:04:51.680
understanding part. The raw storage I'm really not concerned about. And I think we should just

01:04:51.680 --> 01:04:58.320
store if you develop an agent, preferably just store all the interaction history.

01:04:59.360 --> 01:05:04.800
And then you build, of course, models on top of it and you compress it and you are selective,

01:05:04.880 --> 01:05:11.440
but occasionally, you go back to the old data and reanalyze it based on your new experience

01:05:11.440 --> 01:05:16.240
you have. You know, sometimes you are in school, you learn all these things you think is totally

01:05:16.240 --> 01:05:21.040
useless. And you know, much later you read us, oh, they were not, you know, so useless as you

01:05:21.040 --> 01:05:27.120
thought. I'm looking at you linear algebra. Right. So maybe let me ask about objective

01:05:27.120 --> 01:05:34.640
functions because that rewards, it seems to be an important part. The rewards are kind of

01:05:34.640 --> 01:05:45.120
given to the system. For a lot of people, the specification of the objective function

01:05:46.320 --> 01:05:52.080
is a key part of intelligence. The agent itself figuring out what is important.

01:05:52.960 --> 01:06:00.160
What do you think about that? Is it possible within IACC framework to yourself discover

01:06:00.240 --> 01:06:06.800
the reward based on which you should operate? Okay, that will be a long answer.

01:06:08.800 --> 01:06:14.400
So, and that is a very interesting question. And I'm asked a lot about this question,

01:06:14.400 --> 01:06:22.960
where do the rewards come from? And that depends. So, and I give you now a couple of answers. So

01:06:22.960 --> 01:06:29.760
if we want to build agents, now let's start simple. So let's assume we want to build an agent

01:06:29.760 --> 01:06:35.120
based on the IACC model, which performs a particular task. Let's start with something

01:06:35.120 --> 01:06:38.960
super simple, like, I mean, super simple, like playing chess or go or something.

01:06:39.760 --> 01:06:43.360
Then you just, you know, the reward is, you know, winning the game is plus one,

01:06:43.360 --> 01:06:48.240
losing the game is minus one, done. You apply this agent, if you have enough compute,

01:06:48.240 --> 01:06:53.360
you let itself play, and it will learn the rules of the game will play perfect chess after some

01:06:53.360 --> 01:07:03.440
while problem solved. Okay, so if you have more complicated problems, then you may believe that

01:07:03.440 --> 01:07:09.600
you have the right reward, but it's not. So a nice cute example is elevator control that is also in

01:07:09.600 --> 01:07:16.080
Rich Sutton's book, which is a great book, by the way. So you control the elevator, and you think,

01:07:16.080 --> 01:07:20.320
well, maybe the reward should be coupled to how long people wait in front of the elevator,

01:07:20.400 --> 01:07:25.520
you know, long wait is bad. You program it and you do it. And what happens is the elevator

01:07:25.520 --> 01:07:33.040
eagerly picks up all the people, but never drops them off. So then you realize, maybe the time in

01:07:33.040 --> 01:07:38.800
the elevator also counts. So you minimize the sum. Yeah. And the elevator does that, but never picks

01:07:38.800 --> 01:07:42.880
up the people in the 10th floor and the top floor, because in expectation, it's not worth it. Just

01:07:42.960 --> 01:07:52.000
let them stay. So even in apparently simple problems, you can make mistakes. And that's

01:07:53.600 --> 01:07:59.600
what in more serious context, say, AGI safety researchers consider. So now let's go back to

01:08:00.240 --> 01:08:05.280
general agents. So assume we want to build an agent, which is generally useful to humans.

01:08:05.280 --> 01:08:11.040
Yes, we have a household robot, and it should do all kinds of tasks. So in this case,

01:08:11.840 --> 01:08:16.800
the human should give the reward on the fly. I mean, maybe it's pre trained in the factory and

01:08:16.800 --> 01:08:20.560
that there's some sort of internal reward for, you know, the battery level or whatever. But

01:08:21.600 --> 01:08:25.120
so it, you know, it does the dishes badly, you know, you punish the robot, you does it good,

01:08:25.120 --> 01:08:29.040
you reward the robot and then train it to a new task, like a child, right? So

01:08:30.000 --> 01:08:35.520
you need the human in the loop. If you want a system, which is useful to the human. And as long

01:08:35.520 --> 01:08:42.160
as this agent stays sub human level, that should work reasonably well. And apart from, you know,

01:08:42.160 --> 01:08:46.560
these examples, it becomes critical if they become, you know, on a human level, it's like

01:08:46.560 --> 01:08:51.440
with children, small children, you have reasonably well under control, they become older. The reward

01:08:51.440 --> 01:08:59.760
technique doesn't work so well anymore. So then finally, so this would be agents, which are just,

01:08:59.760 --> 01:09:04.880
you could say slaves to the humans. Yeah. So if you are more ambitious and just say we want to

01:09:04.880 --> 01:09:10.960
build a new spacious of intelligent beings, we put them on a new planet and we want them to

01:09:10.960 --> 01:09:18.080
develop this planet or whatever. So we don't give them any reward. So what could we do? And

01:09:18.080 --> 01:09:22.800
you could try to, you know, come up with some reward functions like, you know, it should maintain

01:09:22.800 --> 01:09:30.160
itself the robot, it should maybe multiply build more robots, right? And, you know, maybe

01:09:31.120 --> 01:09:34.640
for all kinds of things that you find useful, but that's pretty hard, right? You know,

01:09:34.640 --> 01:09:38.720
what does self maintenance mean? You know, what does it mean to build a copy? Should it be exact

01:09:38.720 --> 01:09:45.120
copy or an approximate copy? And so that's really hard. But Laurent or so, also at DeepMind,

01:09:46.000 --> 01:09:52.000
developed a beautiful model. So it just took the ICSE model and coupled the rewards

01:09:52.720 --> 01:09:59.040
to information gain. So he said the reward is proportional to how much the agent had

01:09:59.120 --> 01:10:03.840
learned about the world. And you can rigorously formally uniquely define that in terms of

01:10:03.840 --> 01:10:09.200
our cattle diversions. Okay. So if you put that in, you get a completely autonomous agent.

01:10:09.760 --> 01:10:13.440
And actually, interestingly, for this agent, we can prove much stronger result than for the

01:10:13.440 --> 01:10:19.040
general agent, which is also nice. And if you let this agent lose, it will be in a sense the

01:10:19.040 --> 01:10:24.800
optimal scientist is absolutely curious to learn as much as possible about the world. And of course,

01:10:24.800 --> 01:10:28.800
it will also have a lot of instrumental goals, right? In order to learn, it needs to at least

01:10:28.800 --> 01:10:33.920
survive, right? That that agent is not good for anything. So it needs to have self preservation.

01:10:33.920 --> 01:10:41.040
And if it builds small helpers acquiring more information, it will do that. Yeah, if exploration,

01:10:41.040 --> 01:10:46.240
space exploration or whatever is necessary, right, to gathering information and develop it. So it has

01:10:46.240 --> 01:10:52.240
a lot of instrumental goals following on this information gain. And this agent is completely

01:10:52.240 --> 01:10:58.240
autonomous of us. No rewards necessary anymore. Yeah, of course, you could find a way to gain

01:10:58.320 --> 01:11:05.680
the concept of information and get stuck in that library that you mentioned beforehand

01:11:05.680 --> 01:11:12.000
with a with a very large number of books. The first agent had this problem. It would get stuck

01:11:12.000 --> 01:11:17.760
in front of an old TV screen, which has just said white noise. Yeah, white noise. But the second

01:11:17.760 --> 01:11:24.560
version can deal with at least stochasticity. Well, yeah, what about curiosity, this kind of word

01:11:25.360 --> 01:11:31.920
curiosity, creativity? Is that kind of the reward function being of getting new information?

01:11:31.920 --> 01:11:41.280
Is that similar to idea of kind of injecting exploration for its own sake inside the reward

01:11:41.280 --> 01:11:46.240
function? Do you find this at all appealing, interesting? I think that's a nice definition.

01:11:46.240 --> 01:11:51.600
Curiosity is the reward. Sorry, curiosity is exploration for its own sake.

01:11:54.720 --> 01:12:00.960
Yeah, I would accept that. But most curiosity, while in humans and especially in children,

01:12:00.960 --> 01:12:06.320
yeah, is not just for its own sake, but for actually learning about the environment and for

01:12:06.320 --> 01:12:14.720
behaving better. So I would, I think most curiosity is tied in the end to what's performing better.

01:12:14.800 --> 01:12:21.200
Well, okay, so if intelligent systems need to have this reward function, let me, you're an

01:12:21.200 --> 01:12:29.600
intelligent system, currently passing the torrent test quite effectively. What's the reward function

01:12:30.800 --> 01:12:36.480
of our human intelligence existence? What's the reward function that Marcus Hutter is operating

01:12:36.480 --> 01:12:43.840
under? Okay, to the first question, the biological reward function is to survive and to spread.

01:12:44.480 --> 01:12:49.680
And very few humans sort of are able to overcome this biological reward function.

01:12:50.880 --> 01:12:57.280
But we live in a very nice world where we have lots of spare time and can still survive and

01:12:57.280 --> 01:13:03.280
spread. So we can develop arbitrary other interests, which is quite interesting.

01:13:03.280 --> 01:13:09.600
On top of that? On top of that, yeah. But the survival and spreading sort of is, I would say,

01:13:10.320 --> 01:13:14.080
the goal or the reward function of humans that the core one.

01:13:15.120 --> 01:13:19.040
I like how you avoided answering the second question, which a good intelligence system would.

01:13:19.600 --> 01:13:24.160
So my, your own meaning of life and the reward function?

01:13:24.160 --> 01:13:29.200
My own meaning of life and reward function is to find an AGI to build it.

01:13:31.040 --> 01:13:37.280
Beautifully put. Okay, let's dissect the X even further. So one of the assumptions is kind of

01:13:37.280 --> 01:13:45.920
infinity keeps creeping up everywhere, which, what are your thoughts on kind of bounded

01:13:45.920 --> 01:13:51.920
rationality and sort of the nature of our existence and intelligence systems is that we're operating

01:13:51.920 --> 01:13:58.720
always under constraints, under, you know, limited time, limited resources. How does that, how do

01:13:58.720 --> 01:14:05.040
you think about that within the IXE framework, within trying to create an AGI system that operates

01:14:05.040 --> 01:14:10.000
under these constraints? Yeah, that is one of the criticisms about IXE that it ignores

01:14:10.000 --> 01:14:16.160
computational completely. And some people believe that intelligence is inherently tied to what's

01:14:17.040 --> 01:14:21.600
bounded resources. What do you think on this one point? Do you think it's,

01:14:22.400 --> 01:14:25.280
do you think the bounded resources are fundamental to intelligence?

01:14:27.760 --> 01:14:34.560
I would say that an intelligence notion which ignores computational limits is extremely useful

01:14:35.440 --> 01:14:40.560
a good intelligence notion, which includes these resources would be even more useful,

01:14:40.560 --> 01:14:47.520
but we don't have that yet. And so look at other fields outside of computer science,

01:14:48.400 --> 01:14:54.800
computational aspects never play a fundamental role. You develop biological models for cells,

01:14:54.800 --> 01:14:59.120
something in physics, these theories, I mean, become more and more crazy and harder and harder

01:14:59.120 --> 01:15:03.520
to compute. Well, in the end, of course, we need to do something with this model, but there's more

01:15:03.600 --> 01:15:10.400
nuisance than a feature. And I'm sometimes wondering if artificial intelligence would not

01:15:10.400 --> 01:15:15.040
sit in a computer science department, but in a philosophy department, then this computational

01:15:15.040 --> 01:15:20.000
focus would be probably significantly less. I mean, think about the induction problem is more

01:15:20.000 --> 01:15:25.040
in the philosophy department. There's virtually no paper who cares about, you know, how long it

01:15:25.040 --> 01:15:30.480
takes to compute the answer that is completely secondary. Of course, once we have figured out

01:15:30.480 --> 01:15:36.080
the first problem, so intelligence without computational resources, then

01:15:37.360 --> 01:15:42.320
the next and very good question is, could we improve it by including computational resources,

01:15:42.320 --> 01:15:47.600
but nobody was able to do that so far in an even halfway satisfactory manner?

01:15:49.040 --> 01:15:53.680
I like that that's in the long run, the right department to belong to is philosophy.

01:15:54.640 --> 01:16:01.840
That's actually quite a deep idea of or even to at least to think about big picture

01:16:01.840 --> 01:16:07.440
philosophical questions, big picture questions, even in the computer science department. But

01:16:07.440 --> 01:16:13.840
you've mentioned approximation, sort of, there's a lot of infinity, a lot of huge resources needed.

01:16:13.840 --> 01:16:18.960
Are there approximations to IHC that within the IHC framework that are useful?

01:16:19.680 --> 01:16:27.840
Yeah, we have to develop a couple of approximations. And what we do there is that the

01:16:27.840 --> 01:16:33.520
Solomov induction part, which was, you know, find the shortest program describing your data,

01:16:33.520 --> 01:16:38.880
which just replaces by standard data compressors, right? And the better compressors get,

01:16:38.880 --> 01:16:43.520
you know, the better this part will become. We focus on a particular compressor called

01:16:43.520 --> 01:16:49.600
Context Rewaiting, which is pretty amazing, not so well known. It has beautiful theoretical

01:16:49.600 --> 01:16:53.840
properties also works reasonably well in practice. So we use that for the approximation

01:16:53.840 --> 01:17:00.560
of the induction and the learning and the prediction part. And for the planning part,

01:17:01.600 --> 01:17:08.000
we essentially just took the ideas from a computer go from 2006. It was Java,

01:17:08.000 --> 01:17:15.760
Cyprus, Bari, also now a DeepMind, who developed the so called UCT algorithm, upper confidence

01:17:15.760 --> 01:17:20.400
bound for trees algorithm, on top of the Monte Carlo tree search. So we approximate this planning

01:17:20.400 --> 01:17:32.240
part by sampling. And it's successful on some small toy problems. We don't want to lose the

01:17:32.240 --> 01:17:35.840
generality, right? And that's sort of the handicap, right? If you want to be general,

01:17:36.080 --> 01:17:41.840
you have to give up something. So but this single agent was able to play, you know, small games

01:17:41.840 --> 01:17:51.920
like Coon poker and tic-tac-toe and, and even Pac-Man. And the same architecture, no change.

01:17:51.920 --> 01:17:57.520
The agent doesn't know the rules of the game, really nothing at all by self or by a player

01:17:57.520 --> 01:18:03.680
with these environments. So you're gonna Schmidt, who were proposed something called

01:18:03.680 --> 01:18:08.160
Ghetto Machines, which is a self improving program that rewrites its own code.

01:18:10.800 --> 01:18:15.040
Sort of mathematically or philosophically, what's the relationship in your eyes,

01:18:15.040 --> 01:18:18.320
if you're familiar with it between AXI and the Ghetto Machines?

01:18:18.320 --> 01:18:21.120
Yeah, familiar with it. He developed it while I was in his lab.

01:18:22.160 --> 01:18:30.000
Yeah. So the Ghetto Machine, explain briefly. You give it a task. It could be a simple task as,

01:18:30.000 --> 01:18:33.920
you know, finding prime factors in numbers, right? You can formally write it down. There's

01:18:33.920 --> 01:18:39.200
a very slow algorithm to do that. Just all try all the factors. Yeah. Or play chess, right?

01:18:39.200 --> 01:18:43.920
Optimally, you write the algorithm to minimax to the end of the game. So you write down what the

01:18:43.920 --> 01:18:49.840
Ghetto Machine should do. Then it will take part of its resources to run this program.

01:18:50.640 --> 01:18:56.880
And other part of the sources to improve this program. And when it finds an improved version,

01:18:56.880 --> 01:19:04.000
which provably computes the same answer. So that's the key part. It needs to prove by itself

01:19:04.000 --> 01:19:10.000
that this change of program still satisfies the original specification. And if it does so,

01:19:10.000 --> 01:19:14.160
then it replaces the original program by the improved program. And by definition,

01:19:14.160 --> 01:19:19.120
it does the same job, but just faster. Okay. And then, you know, it proves over it and over it.

01:19:19.120 --> 01:19:26.640
And it's developed in a way that all parts of this Ghetto Machine can self improve.

01:19:26.640 --> 01:19:33.840
But it stays provably consistent with the original specification. So from this perspective,

01:19:33.840 --> 01:19:39.440
it has nothing to do with IxE. But if you would now put IxE as the starting axioms in,

01:19:40.560 --> 01:19:46.320
it would run IxE. But, you know, that takes forever. But then if it finds a provable

01:19:47.200 --> 01:19:52.160
speed up of IxE, it would replace it by this and this and this and maybe eventually it comes

01:19:52.160 --> 01:19:58.960
up with a model which is still the IxE model. It cannot be, I mean, just for the knowledgeable

01:19:58.960 --> 01:20:04.400
reader, IxE is incomputable. And that can prove that therefore there cannot be a computable

01:20:05.360 --> 01:20:11.120
exact algorithm of computers. There needs to be some approximations. And this is not dealt

01:20:11.120 --> 01:20:14.400
with the Ghetto Machine. So you have to do something about it. But there's the IxE TL model,

01:20:14.400 --> 01:20:19.120
which is finally computable, which we could put in. Which part of IxE is non computable?

01:20:19.120 --> 01:20:22.160
The Solomonov induction part. The induction. Okay, so.

01:20:22.160 --> 01:20:29.120
But there's ways of getting computable approximations of the IxE model. So then it's at least

01:20:29.120 --> 01:20:34.800
computable. It is still way beyond any resources anybody will ever have. But then the Ghetto Machine

01:20:34.800 --> 01:20:41.280
could sort of improve it further and further in an exact way. So is this theoretically possible that

01:20:42.160 --> 01:20:50.880
the Ghetto Machine process could improve? Isn't IxE already optimal?

01:20:51.760 --> 01:21:00.640
It is optimal in terms of the reward collected over its interaction cycles. But it takes infinite

01:21:00.640 --> 01:21:08.080
time to produce one action. And the world continues whether you want it or not. So the model is

01:21:08.080 --> 01:21:12.800
assuming it had an oracle, which solved this problem, and then in the next 100 milliseconds

01:21:12.800 --> 01:21:19.280
or the reaction time you need gives the answer, then IxE is optimal. It's optimal in sense of

01:21:19.280 --> 01:21:25.280
data, also from learning efficiency and data efficiency, but not in terms of computation

01:21:25.280 --> 01:21:30.160
time. And then the Ghetto Machine in theory, but probably not provably could make it go faster.

01:21:30.880 --> 01:21:39.280
Yes. Okay. Interesting. Those two components are super interesting. The perfect intelligence

01:21:39.280 --> 01:21:47.920
combined with self-improvement. Sort of provable self-improvement in sense you're always getting

01:21:47.920 --> 01:21:53.760
the correct answer and you're improving. Beautiful ideas. Okay, so you've also mentioned that

01:21:54.480 --> 01:22:01.360
different kinds of things in the chase of solving this reward, sort of optimizing for the goal,

01:22:03.040 --> 01:22:08.560
interesting human things could emerge. So is there a place for consciousness within IxE?

01:22:10.800 --> 01:22:17.360
Where does, maybe you can comment, because I suppose we humans are just another instantiation

01:22:17.360 --> 01:22:23.360
by IxE agents and we seem to have consciousness. You say humans are an instantiation of an IxE agent?

01:22:23.360 --> 01:22:28.240
Yes. Oh, that would be amazing. But I think that's not true even for the smartest and most

01:22:28.240 --> 01:22:34.240
rational humans. I think maybe we are very crude approximations. Interesting. I mean, I tend to

01:22:34.240 --> 01:22:43.760
believe, again, I'm Russian, so I tend to believe our flaws are part of the optimal. So we tend to

01:22:43.760 --> 01:22:49.920
laugh off and criticize our flaws and I tend to think that that's actually close to an optimal

01:22:49.920 --> 01:22:54.960
behavior. Well, some flaws, if you think more carefully about it, are actually not flaws here,

01:22:54.960 --> 01:23:01.840
but I think there are still enough flaws. I don't know. It's unclear. As a student of history,

01:23:01.840 --> 01:23:08.880
I think all the suffering that we've endured as a civilization, it's possible that that's the

01:23:08.880 --> 01:23:15.680
optimal amount of suffering we need to endure to minimize long-term suffering. That's your Russian

01:23:15.760 --> 01:23:21.760
background. That's the Russian. Whether humans are or not instantiations of an IxE agent,

01:23:21.760 --> 01:23:27.440
do you think there's consciousness is something that could emerge in a computational form of

01:23:27.440 --> 01:23:32.720
framework like IxE? Let me also ask you a question. Do you think I'm conscious?

01:23:36.720 --> 01:23:44.160
That's a good question. That tie is confusing me, but I think so.

01:23:44.240 --> 01:23:46.880
You think that makes me unconscious because it strangles me?

01:23:47.520 --> 01:23:51.280
If an agent were to solve the imitation game posed by Turing, I think that would be

01:23:51.280 --> 01:23:57.440
dressed similarly to you. Because there's a kind of flamboyant, interesting,

01:23:58.960 --> 01:24:05.440
complex behavior pattern that sells that you're human and you're conscious. But why do you ask?

01:24:06.080 --> 01:24:07.440
Was it a yes or was it a no?

01:24:07.840 --> 01:24:12.000
Yes, I think you're conscious, yes.

01:24:12.640 --> 01:24:19.600
So, and you explain somehow why, but you infer that from my behavior. You can never be sure

01:24:19.600 --> 01:24:26.720
about that. And I think the same thing will happen with any intelligent agent we develop

01:24:26.720 --> 01:24:33.200
if it behaves in a way sufficiently close to humans. Or maybe if not humans, maybe a dog

01:24:33.200 --> 01:24:39.920
is also sometimes a little bit self-conscious. So, if it behaves in a way where we attribute

01:24:39.920 --> 01:24:44.320
typically consciousness, we would attribute consciousness to these intelligent systems

01:24:44.320 --> 01:24:49.840
and IxE probably in particular. That, of course, doesn't answer the question whether it's really

01:24:49.840 --> 01:24:55.920
conscious. And that's the big hard problem of consciousness. Maybe I'm a zombie. I mean,

01:24:55.920 --> 01:24:58.400
not the movie zombie, but the philosophical zombie.

01:24:59.360 --> 01:25:06.480
It's to you, the display of consciousness close enough to consciousness from a perspective of AGI

01:25:06.480 --> 01:25:11.200
that the distinction of the heart problem of consciousness is not an interesting one.

01:25:11.200 --> 01:25:14.720
I think we don't have to worry about the consciousness problem, especially the heart

01:25:14.720 --> 01:25:22.480
problem for developing AGI. I think we progress. At some point, we have solved all the technical

01:25:22.560 --> 01:25:26.720
problems and this system will behave intelligent and then super intelligent and

01:25:27.760 --> 01:25:33.040
this consciousness will emerge. I mean, definitely it will display behavior which we

01:25:33.040 --> 01:25:39.040
will interpret as conscious. And then it's a philosophical question. Did this consciousness

01:25:39.040 --> 01:25:44.960
really emerge? Or is it a zombie which just fakes everything? We still don't have to figure that

01:25:44.960 --> 01:25:49.360
out. Although it may be interesting, at least from a philosophical point of view. It's very

01:25:49.360 --> 01:25:53.840
interesting, but it may also be sort of practically interesting. You know, there's some people

01:25:53.840 --> 01:25:57.920
saying, if it's just faking consciousness and feelings, then we don't need to be concerned

01:25:57.920 --> 01:26:02.720
about rights. But if it's real conscious and has feelings, then we need to be concerned.

01:26:05.840 --> 01:26:11.920
I can't wait till the day where AI systems exhibit consciousness because it'll truly

01:26:11.920 --> 01:26:15.600
be some of the hardest ethical questions of what we do with that.

01:26:15.600 --> 01:26:22.560
It is rather easy to build systems which people ascribe consciousness. And I give you an analogy.

01:26:22.560 --> 01:26:26.080
I mean, remember, maybe it was before you were born, the Tamagotchi.

01:26:28.640 --> 01:26:29.520
How dare you, sir.

01:26:30.880 --> 01:26:33.200
Why that's so... You're young, right?

01:26:33.200 --> 01:26:37.600
Yes, it's good to think. Thank you. Thank you very much. But I was also in the Soviet Union. We

01:26:37.600 --> 01:26:41.120
didn't have... We didn't have any of those fun things.

01:26:41.120 --> 01:26:43.840
But you have heard about this Tamagotchi, which was, you know, really,

01:26:43.920 --> 01:26:50.720
really primitive. Actually, for the time it was... And you could raise this. And kids got so

01:26:50.720 --> 01:26:57.360
attached to it and didn't want to let it die. And probably if we would have asked the children,

01:26:57.360 --> 01:27:00.320
do you think this Tamagotchi is conscious? They would have said yes.

01:27:01.520 --> 01:27:06.800
I think that's kind of a beautiful thing, actually, because that consciousness, ascribing

01:27:06.800 --> 01:27:13.120
consciousness seems to create a deeper connection, which is a powerful thing. But we have to be

01:27:13.120 --> 01:27:18.880
careful on the ethics side of that. Well, let me ask about the AGI community broadly. You kind of

01:27:18.880 --> 01:27:26.320
represent some of the most serious work on AGI, at least earlier. And DeepMind represents

01:27:27.120 --> 01:27:34.000
serious work on AGI these days. But why, in your sense, is the AGI community so small,

01:27:34.000 --> 01:27:40.240
or has been so small, until maybe DeepMind came along? Like, why aren't more people

01:27:40.240 --> 01:27:47.120
seriously working on human level and superhuman level intelligence from a formal perspective?

01:27:48.160 --> 01:27:54.080
Okay, from a formal perspective, that's sort of, you know, an extra point. So I think there are

01:27:54.080 --> 01:27:58.400
a couple of reasons. I mean, AI came in waves, right? You know, AI winters and AI summers,

01:27:58.400 --> 01:28:07.200
and then there were big promises, which were not fulfilled. And people got disappointed. But

01:28:08.080 --> 01:28:14.240
narrow AI, solving particular problems, which seemed to require intelligence, was

01:28:15.200 --> 01:28:20.480
always to some extent successful, and there were improvements, small steps. And if you build

01:28:20.480 --> 01:28:26.480
something which is, you know, useful for society or industrial useful, then there's a lot of funding.

01:28:26.480 --> 01:28:34.480
So I guess it was in parts the money, which drives people to develop specific system solving

01:28:34.480 --> 01:28:40.640
specific tasks. But you would think that, you know, at least in university, you should be able to do

01:28:41.280 --> 01:28:46.800
ivory tower research. And that was probably better a long time ago. But even nowadays,

01:28:46.800 --> 01:28:52.960
there's quite some pressure of doing applied research or translational research. And, you

01:28:52.960 --> 01:29:00.480
know, it's harder to get grants as a theorist. So that also drives people away. It's maybe also

01:29:00.480 --> 01:29:04.960
harder, attacking the general intelligence problem. So I think enough people, I mean,

01:29:04.960 --> 01:29:11.920
maybe a small number, we're still interested in, in formalizing intelligence and, and thinking of

01:29:11.920 --> 01:29:19.200
general intelligence. But, you know, not much came up, right? Or not not much great stuff came up.

01:29:19.760 --> 01:29:26.000
So what do you think we talked about the formal big light at the end of the tunnel,

01:29:26.000 --> 01:29:29.440
but from the engineering perspective, what do you think it takes to build an AI system?

01:29:30.240 --> 01:29:35.680
Is that, and I don't know if that's a stupid question or a distinct question from everything

01:29:35.680 --> 01:29:40.880
we've been talking about AIXE. But what do you see as the steps that are necessary to take

01:29:40.880 --> 01:29:46.160
to start to try to build something? So you want a blueprint now, and then you go off and do it?

01:29:46.160 --> 01:29:50.320
That's the whole point of this conversation, trying to squeeze that in there. Now, is there,

01:29:50.320 --> 01:29:55.360
I mean, what's your intuition? Is it is in the robotic space or something that has a body and

01:29:55.360 --> 01:29:59.920
tries to explore the world? Is in the reinforcement learning space, like the efforts of Alpha

01:29:59.920 --> 01:30:05.360
Zero and Alpha Star, they're kind of exploring how you can solve it through in the, in the simulation

01:30:05.360 --> 01:30:11.760
in the gaming world. Is there stuff in sort of the, all the transformer work in natural

01:30:11.760 --> 01:30:16.640
English processing, sort of maybe attacking the open domain dialogue? Like what, what,

01:30:16.640 --> 01:30:24.800
where do you see the promising pathways? Let me pick the embodiment maybe. So

01:30:25.600 --> 01:30:37.600
embodiment is important, yes and no. I don't believe that we need a physical robot

01:30:38.560 --> 01:30:45.440
walking or rolling around interacting with the real world in order to achieve AGI. And

01:30:47.680 --> 01:30:52.800
I think it's more of a distraction probably than helpful. It's sort of confusing the body

01:30:52.800 --> 01:30:59.440
with the mind. For industrial applications or near term applications, of course, we need

01:30:59.440 --> 01:31:06.400
robots for all kinds of things, but for solving the big problem, at least at this stage, I think

01:31:06.400 --> 01:31:13.520
it's not necessary. But the answer is also yes, that I think the most promising approach is that

01:31:13.520 --> 01:31:20.000
you have an agent, and that can be a virtual agent in a computer interacting with an environment,

01:31:20.080 --> 01:31:23.840
possibly, you know, a 3D simulated environment like in many computer games.

01:31:25.280 --> 01:31:33.040
And, and you train and learn the agent. Even if you don't intend to later put it sort of, you know,

01:31:33.040 --> 01:31:38.480
this algorithm in a, in a robot brain and leave it forever in the virtual reality,

01:31:38.480 --> 01:31:43.440
getting experience in a, although it's just simulated 3D world,

01:31:44.240 --> 01:31:53.520
is possibly, and as I possibly important to understand things on a similar level as humans do,

01:31:55.040 --> 01:31:59.920
especially if the agent or primarily if the agent wants, needs to interact with the humans,

01:31:59.920 --> 01:32:04.160
right? You know, if you talk about objects on top of each other in space and flying and cars and

01:32:04.160 --> 01:32:10.800
so on, and the agent has no experience with even virtual 3D worlds, it's probably hard to grasp.

01:32:11.120 --> 01:32:17.840
So if we develop an abstract agent, say we take the mathematical path, and we just want to build

01:32:17.840 --> 01:32:22.400
an agent which can prove theorems and becomes a better and better mathematician, then this agent

01:32:22.400 --> 01:32:27.920
needs to be able to reason in very abstract spaces, and then maybe sort of putting it into

01:32:27.920 --> 01:32:33.280
3D environments, simulated world is even harmful, it should sort of, you put it in, I don't know,

01:32:33.280 --> 01:32:38.400
an environment which it creates itself or so. It seems like you have an interesting,

01:32:38.400 --> 01:32:43.680
rich complex trajectory through life in terms of your journey of ideas. So it's interesting to

01:32:43.680 --> 01:32:52.640
ask what books, technical fiction, philosophical books, ideas, people had a transformative effect.

01:32:52.640 --> 01:32:57.920
Books are most interesting because maybe people could also read those books and see if they could

01:32:57.920 --> 01:33:05.440
be inspired as well. Yeah, luckily I asked books and not singular book, it's very hard and I try

01:33:05.440 --> 01:33:15.920
to pin down one book, and I can do that at the end. So the books which were most transformative

01:33:15.920 --> 01:33:26.320
for me or which I can most highly recommend to people interested in AI, I would always start

01:33:26.320 --> 01:33:33.600
with Russell and Norbic, Artificial Intelligence and Modern Approach, that's the AI Bible, it's

01:33:33.600 --> 01:33:40.400
an amazing book, it's very broad, it covers all approaches to AI and even if you focus on one

01:33:40.400 --> 01:33:44.560
approach, I think that is the minimum you should know about the other approaches out there,

01:33:44.560 --> 01:33:48.320
so that should be your first book. Fourth edition should be coming out soon.

01:33:48.320 --> 01:33:54.000
Oh, okay, interesting. There's a deep learning chapter now as there must be, written by Ian

01:33:54.000 --> 01:34:00.880
Goodfellow, okay. And then the next book I would recommend, The Reinforcement Learning Book by

01:34:00.880 --> 01:34:08.960
Sutton and Bartow. There's a beautiful book, if there's any problem with the book, it makes RL

01:34:10.400 --> 01:34:16.640
feel and look much easier than it actually is. It's very gentle book, it's very nice to read the

01:34:16.640 --> 01:34:23.280
exercises, you can very quickly get some RL systems to run, very toy problems, but it's a lot of fun

01:34:23.280 --> 01:34:30.800
and in a couple of days you feel you know what RL is about, but it's much harder than

01:34:30.800 --> 01:34:40.320
the book. Come on now, it's an awesome book. Yeah, no, no, it is, yeah. And maybe, I mean,

01:34:40.320 --> 01:34:43.680
there's so many books out there, if you like the information theoretic approach, then there's

01:34:43.680 --> 01:34:50.720
Kolmogorff Complexity by Aline Vitani, but probably, you know, some short article is enough,

01:34:50.720 --> 01:34:57.920
you don't need to read the whole book, but it's a great book. And if you have to mention one

01:34:57.920 --> 01:35:04.240
all-time favorite book, it's a different flavor, that's a book which is used in the international

01:35:04.240 --> 01:35:10.880
baccalaureate for high school students in several countries. That's from Nikolas Altjen,

01:35:10.880 --> 01:35:17.520
Theory of Knowledge, second edition, or first, not the third, please. The third one they put,

01:35:17.520 --> 01:35:26.480
they took out all the fun. Okay, so this asks all the interesting, or to me, interesting

01:35:26.480 --> 01:35:30.000
philosophical questions about how we acquire knowledge from all perspectives, you know,

01:35:30.000 --> 01:35:36.800
from math, from art, from physics, and ask how can we know anything? And the book is called

01:35:36.800 --> 01:35:40.800
Theory of Knowledge. From which, is this almost like a philosophical exploration of

01:35:41.680 --> 01:35:45.040
how we get knowledge from anything? Yes, yeah, I mean, can religion tell us, you know,

01:35:45.040 --> 01:35:48.800
about something about the world? Can science tell us something about the world? Can mathematics,

01:35:48.800 --> 01:35:55.120
or is it just playing with symbols? And you know, it's open-ended questions, and I mean,

01:35:55.120 --> 01:35:58.960
it's for high school students, so they have the resources from Hitchhiker's Guide to the Galaxy

01:35:58.960 --> 01:36:05.360
and from Star Wars and the Chicken Cross the Road, yeah. And it's fun to read, but it's also quite

01:36:05.360 --> 01:36:12.720
deep. If you could live one day of your life over again, because it made you truly happy,

01:36:12.720 --> 01:36:18.080
or maybe like we said with the books, it was truly transformative, what day, what moment would you

01:36:18.080 --> 01:36:23.840
choose? Does something pop into your mind? Does it need to be a day in the past, or can it be a

01:36:23.840 --> 01:36:30.720
day in the future? Well, space-time is an emerging phenomena, so it's all the same anyway. Okay.

01:36:31.920 --> 01:36:37.600
Okay, from the past. You're really going to say from the future, I love it. No, I will tell you

01:36:37.600 --> 01:36:42.960
from the future. Okay, from the past. So from the past, I would say when I discovered my AXI model,

01:36:43.680 --> 01:36:48.000
I mean, it was not in one day, but it was one moment where I realized

01:36:48.720 --> 01:36:55.440
comagor of complexity. I didn't even know that it existed, but I discovered sort of this compression

01:36:55.440 --> 01:37:00.640
idea myself, but immediately I knew I can't be the first one, but I had this idea. And then I

01:37:00.640 --> 01:37:05.680
knew about sequential decisionary, and I knew if I put it together, this is the right thing.

01:37:06.320 --> 01:37:12.320
And yeah, still when I think back about this moment, I'm super excited about it.

01:37:12.400 --> 01:37:17.680
Was there any more details in context that moment? Did an apple fall in your head?

01:37:20.000 --> 01:37:26.080
So like if you look at Ian Goodfellow talking about gans, there was beer involved. Is there

01:37:27.200 --> 01:37:33.120
some more context of what sparked your thought, or was it just? No, it was much more mundane. So I

01:37:33.120 --> 01:37:37.360
worked in this company. So in this sense, the four and a half years was not completely wasted.

01:37:37.600 --> 01:37:47.360
So and I worked on an image interpolation problem. And I developed quite neat new

01:37:47.360 --> 01:37:52.160
interpolation techniques, and they got patented. And then, you know, which happens quite often,

01:37:52.160 --> 01:37:55.840
I got sort of overboard and thought about, you know, yeah, that's pretty good, but it's not the

01:37:55.840 --> 01:38:01.200
best. So what is the best possible way of doing interpolation? And then I thought, yeah, you

01:38:01.200 --> 01:38:06.480
want the simplest picture, which is if you coarse-grain it, recovers your original picture.

01:38:06.480 --> 01:38:12.960
And then I thought about the simplicity concept more in quantitative terms. And yeah, then

01:38:12.960 --> 01:38:18.960
everything developed. And somehow the full beautiful mix of also being a physicist and

01:38:18.960 --> 01:38:25.120
thinking about the big picture of it, then led you to probably beg with ice. Yeah. So as a physicist,

01:38:25.120 --> 01:38:29.280
I was probably trained not to always think in computational terms, you know, just ignore that

01:38:29.280 --> 01:38:33.120
and think about the fundamental properties which you want to have.

01:38:33.920 --> 01:38:39.040
So what about if you could really live one day in the future? What would that be?

01:38:39.760 --> 01:38:45.040
When I solve the AGI problem. In practice, in practice. So in theory,

01:38:45.040 --> 01:38:50.000
I have solved it with the IHC model, but in practice. And then I asked the first question.

01:38:50.640 --> 01:38:54.160
What would be the first question? What's the meaning of life?

01:38:54.400 --> 01:38:59.600
I don't think there's a better way to end it. Thank you so much for talking today. It's

01:38:59.600 --> 01:39:03.360
a huge honor to finally meet you. Yeah, thank you too. It was a pleasure of mine side too.

01:39:04.560 --> 01:39:08.160
Thanks for listening to this conversation with Marcus Hutter. And thank you to our

01:39:08.160 --> 01:39:14.560
presenting sponsor, Cash App. Download it, use code LEX Podcast. You'll get $10 and $10 will go

01:39:14.560 --> 01:39:19.440
to first, an organization that inspires and educates young minds to become science and

01:39:19.440 --> 01:39:24.960
technology innovators of tomorrow. If you enjoy this podcast, subscribe on YouTube,

01:39:24.960 --> 01:39:30.320
give it five stars on Apple Podcasts, support on Patreon, or simply connect with me on Twitter

01:39:30.320 --> 01:39:37.120
at Lex Friedman. And now let me leave you with some words of wisdom from Albert Einstein.

01:39:37.840 --> 01:39:45.600
The measure of intelligence is the ability to change. Thank you for listening and hope to see you

01:39:45.600 --> 01:39:46.880
next time.

