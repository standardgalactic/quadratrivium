{"text": " Naively, I certainly thought that all humans would have words for exact counting. And the Piroha don't, okay? So they don't have any words for even one. There's not a word for one in their language. And so there's certainly not a word for two, three, or four. So that kind of blows people's minds off. Yeah, that's blowing my mind. That's pretty weird. How are you going to ask? I want two of those. You just don't. And so that's just not a thing you can possibly ask in the Piroha. It's not possible. That is, there's no words for that. The following is a conversation with Edward Gibson, or Ted, as everybody calls him. He is a Psycho-Linguistics Professor at MIT. He heads the MIT Language Lab that investigates why human languages look the way they do, the relationship between cultural language and how people represent, process, and learn language. Also, he should have a book titled Syntax, A Cognitive Approach, published by MIT Press, coming out this fall. So look out for that. This is the Lex Freeman podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Edward Gibson. When did you first become fascinated with human language? As a kid in school, when we had to structure sentences and English grammar, I found that process interesting. I found it confusing as to what it was I was told to do. I didn't understand what the theory was behind it, but I found it very interesting. So when you look at grammar, you're almost thinking about like a puzzle, like almost like a mathematical puzzle? Yeah, I think that's right. I didn't know I was going to work on this at all at that point. I was really just, I was kind of a math geek person. Computer scientist. I really liked computer science. And then I found language as a neat puzzle to work on from an engineering perspective. Actually, that's what I, as a, I sort of accidentally, I decided after I finished my undergraduate degree, which was computer science and math and Canada and Queens University, I decided to go to grad school. That's what I always thought I would do. And I went to Cambridge, where they had a master's in a master's program in computational linguistics. And I hadn't taken a single language class before. All I'd taken was CS, computer science, math classes, pretty much mostly as an undergrad. And I just thought, oh, this was an interesting thing to do for a year, because it was a single year program. And then I ended up spending my whole life doing it. So fundamentally, your journey through life was one of a mathematician and computer scientists. And then you kind of discovered the puzzle, the problem of language, and approached it from that angle, to try to understand it from that angle, almost like a mathematician or maybe even an engineer. As an engineer, I'd say, I mean, to be frank, I had taken an AI class, I guess it was 83 or 85, somewhere 84 in there a long time ago. And there was a natural language section in there. And it didn't impress me. I thought there must be more interesting things we can do. It didn't seem very, it seemed just a bunch of hacks to me. It didn't seem like a real theory of things in any way. And so this seemed like an interesting area where there wasn't enough good work. Did you ever come across like the philosophy angle of logic? So if you think about the 80s with AI, the expert systems where you try to kind of maybe sidestep the poetry of language and some of the syntax and the grammar and all that kind of stuff and go to the underlying meaning that language is trying to communicate and try to somehow compress that in a computer representable way. Do you ever come across that in your studies? I mean, I probably did, but I wasn't as interested in it. I was trying to do the easier problems first than ones I could thought maybe were handleable, which seems like the syntax is easier, which is just the forms as opposed to the meaning. When you're starting talking about the meaning, that's a very hard problem. And it still is a really, really hard problem. But the forms is easier. And so I thought at least figuring out the forms of human language, which sounds really hard, but is actually maybe more tractable. So it's interesting. You think there is a big divide, there's a gap, there's a distance between form and meaning. Because that's a question you have discussed a lot with LLMs, because they're damn good at form. Yeah, I think it's what they're good at, is form. And that's why they're good, because they can do form, meaning's hard. Do you think they're, oh, wow. I mean, it's an open question, right? How close form and meaning are? We'll discuss it. But to me, studying form, maybe some romantic notion, gives you form is like the shadow of the bigger meaning thing underline language. Language is how we communicate ideas. We communicate with each other using language. So in understanding the structure of that communication, I think you start to understand the structure of thought and the structure of meaning behind those thoughts and communication to me. But to you, big gap. What do you find most beautiful about human language, maybe the form of human language, the expression of human language? What I find beautiful about human language is some of the generalizations that happen across the human language, just within and across a language. So let me give you an example of something which I find kind of remarkable, that is if a language, if it has a word order, such that the verbs tend to come before their objects. And so that's like English does that. So we have the subject comes first in a simple sentence. So I say, you know, the dog chased the cat or Mary kicked the ball. So the subject first, and then after the subject, there's the verb. And then we have objects. All these things come after in English. So it's generally a verb. And most of the stuff that we want to say comes after the subject. It's the objects. There's a lot of things we want to say they come after. And there's a lot of languages like that. About 40% of the languages of the world look like that. They're subject verb object languages. And then these languages tend to have prepositions, these little markers on the nouns that connect nouns to other nouns or nouns to verb. So when I say a verb, I say preposition like in or on or of or about, I say, I talk about something. The something is the object of that preposition that we have these little markers come also just like verbs, they come before their their nouns. Okay. And then so now we look at other languages that like Japanese or or Hindi or some, these are these are so called verb final languages. Those is about maybe a little more than 40%, maybe 45% of the world's languages are more, I mean, 50% of the world's languages are verb final. Those tend to be post positions, those markers, the same if you have the states have the same kinds of markers, as we do in English, but they put them after. So sorry, they put them first, the markers come first. So you say instead of, you know, talk about a book, you say a book about the opposite order there in Japanese or in Hindi, you do the opposite and the and the talk comes at the end. So the verb will come at the end as well. So instead of Mary kicked the ball, it's Mary ball kicked. And then if it's Mary kicked the ball to John, it's John two, the two, the little marker there, the preposition, it's a post position in these languages. And so the interesting thing, a fascinating thing to me is that within a language, this order aligns, it's harmonic. And so if it's one or the other, it's either verb initial or verb final, but then you then you'll have prepositions, prepositions or post positions. And so that and that's across the languages that we can look at. We've got around 1000 languages for there's around 7000 languages around on the earth right now. But we have information about say word order on around 1000 of those pretty decent amount of information. And for those 1000, which we know about, about 95% fit that pattern. So they will have either verb and it's about half and half or half verb initial, like English and half verb final, like Japanese. So just to clarify, verb initial is subject verb object. That's correct. Verb final is still subject, object verb. That's correct. Yeah, the subject is generally first. That's so fascinating. I ate an apple or I apple eight. Yes. Okay. And it's fascinating that there's pretty even division in the world amongst those 45%. Yeah, it's pretty, it's pretty even. And those two are the most common by far. Those two words, the subject tends to be first. There's so many interesting things, but these things are what I find so fascinating is there are these generalizations within and across a language. And not only those are the, and there's actually a simple explanation, I think, for a lot of that. And that is, you're trying to like minimize dependencies between words. That's basically the story, I think, behind a lot of why word order looks the way it is, is you, we're always connecting. What is it? What is the thing I'm telling you? I'm talking to you in sentences. You're talking to me in sentences. These are sequences of words, which are connected. And the connections are dependencies between the words. And it turns out that what we're trying to do in a language is actually minimize those dependency links. It's easier for me to say things if the words that are connecting for their meaning are close together. It's easier for you in understanding if that's also true. If they're far away, it's hard to produce that and it's hard for you to understand. And the languages of the world within a language and across languages fit that generalization, which is, so it turns out that having verbs initial and then having prepositions ends up making dependencies shorter. And having verbs final and having postpositions ends up making dependencies shorter than if you cross them. If you cross them, it ends up, you just end up, it's possible, you can do it. Even within a language. Within a language, you can do it. It just ends up with longer dependencies than if you didn't. And so languages tend to go that way. They tend to, they say they call it harmonic. So it was observed a long time ago without the explanation by a guy called Joseph Greenberg, who's a famous typologist from Stanford. He observed a lot of generalizations about how word order works. And these are some of the harmonic generalizations that he observed. Harmonic generalizations about word, word, word. There's so many things I want to ask you. Okay, good. Okay, let me just, sometimes basics. You mentioned dependencies a few times. Yeah. What do you mean by dependencies? Well, what I mean is in, in language, there's kind of three structures to three components to the structure of language. One is the sounds. So cat is cat and to in English. I'm not talking about that part. I'm talking, and then there's two meaning parts. And those are the words. And, and you're talking about meaning earlier. So words have a form and they have a meaning associated with them. And so cat is a full form in English and it has a meaning associated with whatever a cat is. And then the combinations of words, that's what I'll call grammar or syntax. And that's like, when I have a combination like the cat or two cats, okay, so where I take two different words there and put them together and I get a compositional meaning from putting those two different words together. And so that's the syntax. And in any sentence or utterance, whatever I'm talking to you, you're talking to me, we have a bunch of words and we're putting together in a sequence. They, it turns out they are connected so that every word is connected to just one other word in that, in that sentence. And so you end up with what's called technically a tree. It's a tree structure. So there, where there's a root of that, of that to utterance of that sentence. And then there's a bunch of dependence, like branches from that root that go down to the words, the words are the leaves in this metaphor for a tree. So a tree is also sort of a mathematical construct. Yeah. Yeah. It's a graph theoretical thing. Graph theory thing. Yeah. So in this fascinating that you can break down a sentence into a tree and then one, every word is hanging on to another, it's depending on it. That's right. And everyone agrees on that. So all linguists will agree with that. No one. It's not a controversial. That is not controversial. There's nobody sitting here. I do nothing mad at you. I don't think so. Okay. There's no linguists sitting there mad at this. No, I think in every language, I think everyone agrees that all sentences are trees at some level. Can I pause on that? Sure. Because it's to me just as a layman, it's surprising that you can break down sentences in many, mostly all languages into a tree. I think so. I've never heard of anyone disagreeing with that. That's weird. The details of the trees are what people disagree with. Well, okay. So what's the root of it? How do you construct? How hard is it? What is the process of constructing a tree from a sentence? Well, this is where, depending on what your theoretical notions, I'm going to say the simplest thing. Dependency grammar. It's like a bunch of people invented this. Tinier was the first French guy back in, I mean, the paper was published in 1959, but he was working on the 30s and stuff. And it goes back to, you know, philologist Panini was doing this in ancient India. Okay. And so, you know, doing something like this, the simplest thing we can think of is that there's just connections between the words to make the utterance. And so let's just say I have like two dogs entered a room. Okay. Here's a sentence. And so we're connecting two and dogs together. That's like, there's some dependency between those words to make some bigger meaning. And then we're connecting dogs now to entered, right? And we connect a room somehow to entered. And so I'm going to connect to room and then room back to entered. That's the tree, is I, the root is entered. That's the thing is like an entering event. That's what we're saying here. And the subject, which is whatever that dog is, is two dogs, it was, and the connection goes back to dogs, which goes back to them, then that goes back to two. I'm just, that's my tree. It starts at entered, goes to dogs down to two. And then the other side, after the verb, the object, it goes to room. And then that goes back to the determiner or article, whatever you want to call that word. So there's a bunch of categories of words here we're noticing. So there are verbs. Those are these things that typically mark, they refer to events and states in the world. And they're nouns, which typically refer to people, places and things is what people say, but they can refer to other more, they can refer to events themselves as well. They're marked by, you know, how they, how they, the category, the part of speech of a word is how it gets used in language. It's like, that's how you decide what the, what the category of a word is not, not by the meaning, but how it's, how it gets used. What's usually the root? Is it going to be the verb that defies the event? Yes. Yes. Yes. Okay. Yeah. I mean, if I don't say a verb, then there won't be a verb. And so it'll be something else. What if you're messing, are we talking about language that's like correct language? What if you're doing poetry and messing with stuff? Is it then, then rules got the window, right? Then it's, no, you're still, no, no, no, no, you're constrained by whatever language you're dealing with. Probably you have other constraints in poetry, such that you're like usually in poetry, there's multiple constraints that you want to, like you want to usually convey multiple meanings is the idea. And maybe you have like a rhythm or a rhyming structure as well. And depending on, so, but you usually are constrained by your, the rules of your language for the most part. And so you don't violate those too much. You can violate them somewhat, but not too much. So it has to be recognizable as your language. Like in English, I can't say dogs to entered room. Ah, I mean, I meant that, you know, two dogs entered a room and I can't mess with the order of the, the articles and the articles and the nouns. You just can't do that. In some languages, you can, you can mess around with the order of words much more. I mean, you speak Russian. Russian has a much freer word order than English. And so in fact, you can move around words in, you know, I told you that English has the subject, verb, object, word order. So does Russian, but Russian is much freer than English. And so you can actually mess around with the word order. So probably Russian poetry is going to be quite different from English poetry because the word order is much less constrained. Yeah, there's a much more extensive culture of poetry throughout the history of the last 100 years in Russia. And I always wondered why that is, but it seems that there's more flexibility in the way the language is used. There's more, you're more female language easier by altering the words, altering the order of the words, messing with it. Well, you can just mess with different things in each language. And so in Russian, you have case markers, right? On the end, which is these endings on the nouns, which tell you how it can each noun connects to the verb, right? We don't have that in English. And so when I say Mary kissed John, I don't know who the agent or the patient is, except by the order of the words, right? In Russian, you actually have a marker. On the end, if you're using a Russian name and each of those names, you'll also say, is it, you know, agent, it'll be the, you know, nominative, which is marking the subject, or an accusative will mark the object. And you could put them in the reverse order. You could put accusative first, as you could put subject, you could put the patient first, and then the verb, and then the, the, the subject, and that would be a perfectly good Russian sentence. And it would still mean Mary, I could say John kissed Mary, meaning Mary kissed John, as long as I use the case markers in the right way, you can't do that in English. And so I love the terminology of agent and patient. And, and the other ones you use, those are sort of linguistic terms, correct? Those are, those are for like kind of meaning, those are meaning. And, and subject and object are generally used for position. So subject is just like the thing that comes before the verb. And the object is when it comes after the verb. The agent is kind of like the thing doing it. That's kind of what that means, right? The, the, the subject is often the person doing the action, right? The thing. So yeah. Okay. This is fascinating. So how hard is it to form a tree in general? Is there, is there a procedure to it? Like if you look at different languages, is it supposed to be a very natural, like is it automatable or is there some human genius involved in? I think it's pretty automatable at this point. People can figure out the words are, they can figure out the morphemes, which are the, technically morphemes are the, the minimal meaning units within a language. Okay. And so when you say eats or drinks, it actually has two morphemes in an English. There's, there's the, there's the root, which is the verb. And then there's some ending on it, which tells you, you know, that's this third person, third person singular. Can you say what morphemes are? Morphemes are just the minimal meaning units within a language. And then a word is just kind of the things we put spaces between English and 10. They have a little bit more, they have the morphology as well. They have the endings, this inflexual morphology on the endings on the roots. They modify something about the word that adds additional meaning. They tell you, yeah, yeah. And so we have a little bit of that in English, very little, much more in Russian, for instance. And, and, but we have a little bit in English. And so we have a little on the, on the nouns, you can say it's either singular or plural. And, and you can say, same thing for, for, for verbs, like simple past tense, for example, it's like, you know, notice in English, we say drinks, you know, he drinks, but everyone else says, I drink, you drink, we drink, it's unmarked in a way. And then, but in the past tense, it's just drank for everyone. There's no morphology at all for past tense. There is morphology, it's marking past tense, but it's kind of, it's an irregular now. So we don't even, you know, it drink to drink, you know, it's not even a regular word. So in most verbs, many verbs, there's an ED, we kind of add. So walk to walked, we add that to say it's the past tense, that I just happened to choose an irregular because the high frequency word and the high frequency words tend to have irregular as in English for. What's an irregular? Irregular is just, there's, there isn't a rule. So drink to drink is an, it's an irregular. Drink, drink, okay, as opposed to walk, walked, talked, talked. And there's a lot of irregular, irregular as in English. There's a lot of irregular as in English. The, the, the frequent ones, the common words tend to be irregular. They'll let, there's many, many more low frequency words and those tend to be, those irregular ones. The evolution of the irregular is fascinating. It's essentially slang that's sticky because you're breaking the rules and then everybody use it and doesn't follow the rules. And they, they say screw it to the rules. It's fascinating. So you said it morphemes, lots of questions. So morphology is what, the study of morphemes? Morphology is the, is the connections between the morphemes onto the roots, the roots. So in English, we mostly have suffixes. We have endings on the words, not very much, but a little bit. And as opposed to prefixes, some words, depending on your language can have, you know, mostly prefixes, mostly suffixes or mostly, or both. And then even languages, several languages have things called infixes where you have some kind of a general form for the, for the root and you put stuff in the middle. You change the vowels. That's fascinating. That's fascinating. So in general, there's what, two morphemes per word, usually one or two or three? Well, in English, it's, it's one or two. In English, it tends to be one or two. There can be more, you know, in other languages, you know, a language, language like, like Finnish, which has a very elaborate morphology, there may be 10 morphemes on the end of a root. Okay. And so there may be, there may be millions of forms of a given word. Okay. Okay. I will ask the same question over and over. But how does the, just sometimes to understand things like morphemes, it's nice to just ask the question, how does these kinds of things evolve? So you have a great book studying sort of the, how, how, how the cognitive processing, how language used for communication. So the, the mathematical notion of how effective languages for communication, what role that plays in the evolution of language, but just high level, like how do we, how does a language evolve with where English is two morphemes or one or two morphemes per word and then Finnish has infinity per word? So what, how does that, how does that happen? Is it just people? That's a really good question. That's a very good question. It's like, why do languages have more morphology versus less morphology? And I don't think we know the answer to this. I know, I think there's just like a lot of good solutions to the problem of communication. And so I, like, I believe as you hinted that language is an invented system by humans for communicating their ideas. And I think we, it comes down to we label the things we want to talk about. Those are the morphemes and words. Those are the things we want to talk about in the world and we invent those things. And then we put them together in ways that are easy for us to convey, to process. But that's like a naive view. And I don't, I mean, I think it's probably right, right? It's naive and probably right. Well, that's a nice, I don't know if it's naive. I think it's simple. Simple. Yeah. I think naive is, naive is an indication that's an incorrect somehow. It's a trivial, too simple. I think it could very well be correct. But it's interesting how sticky it feels like two people got together. It just feels like once you figure out certain aspects of a language, that just becomes sticky and the tribe forms around that language, maybe the language, maybe the tribe forms first, then the language evolves. And then you just kind of agree and you stick to whatever that is. I mean, these are very interesting questions. We don't know really about how words, even words, get invented very much about, you know, we don't really, I mean, assuming they get invented, they, we don't really know how that process works and how these things evolve. What we have is kind of a current picture, a current picture of a few thousand languages, a few thousand instances. We don't have any pictures of really how these things are evolving, really. And then the evolution is massively, you know, confused by contact, right? So as soon as one language, group, one group runs into another, we are smart, humans are smart, and they take on whatever is useful in the other group. And so any kind of contrast, which you're talking about, which I find useful, I'm going to, I'm going to start using as well. So I worked a little bit in specific areas of words, in number words and in color words, and in color words. So we have, in English, we have around 11 words that everyone knows for colors. And many more, if you happen to be interested in color for some reason or other, if you're a fashion designer or an artist or something, you may have many, many more words. But we can see millions, like if you have normal color vision, normal trichrometric color vision, you can see millions of distinctions in color. So we don't have millions of words. You know, the most efficient, no, the most, you know, detailed color vocabulary would have over a million terms to distinguish all the different colors that we can see. But of course, we don't have that. So it's somehow, it's been, it's kind of useful for English to have evolved in some way to, there's 11 terms that people find useful to talk about, you know, black, white, red, blue, green, yellow, purple, gray, pink, and I probably miss something there. Anyway, there's 11 that everyone knows. And depending on your, but you go to different cultures, especially the non industrialized cultures, and there'll be many fewer. So some cultures will have only two, believe it or not, that the Danai and Papua New Guinea have only two labels that the group uses for color. And those are roughly black and white. They are very, very dark and very, very light, which are roughly black and white. And you might think, oh, they're dividing the whole color space into, you know, light and dark or something. And that's not really true. They mostly just only label the light, the black and the white things. They just don't talk about the colors for the other ones. And so, and then there's other groups, I worked with a group called the Chimani down in, in Bolivia, in South America, and they have three words that everyone knows, but there's a few others that are, that, that several people, that many people know. And so, they have made, it's kind of depending on how you count between three and seven words that the group knows. Okay. And again, they're black and white. Everyone knows those. And red, red is, you know, like that tends to be the third word that everyone, that cultures bring in, if there's a word, it's always red, the third one. And then after that, it's kind of all bets are off about what they bring in. And so after that, they bring in a sort of a big blue, green space group, group, they have one for that. And then they have, and then, you know, different people have different words that they'll use for other parts of the space. And so anyway, it's probably related to what they want to talk, what they, not what they, not what they see, because they see the same colors as we see. So it's not like they have, they don't, they have a weak, a low color palette and the things they're looking at, they're looking at a lot of beautiful scenery. Okay. A lot of different colored flowers and berries and things. And you know, and so there's lots of things of very bright colors, but they just don't label the color in those cases. And the reason probably, we don't know this, but we think probably what's going on here is that what you do, why you label something is you need to talk to someone else about it. And why do I need to talk about a color? Well, if I have two things which are identical, and I want you to give me the one that's different, and in the only way it varies is color, then I invent a word, which tells you, you know, this is the one I want. So I want the red sweater off the rack, not the green sweater, right? There's two. And so those things will be identical, because these are things we made and they're dyed, and there's nothing different about them. And so in industrialized society, we have, you know, everything, everything we've got is pretty much arbitrarily colored. But if you go to a non-industrialized group, that's not true. And so they don't, suddenly they're not interested in color. If you bring bright colored things to them, they like them just like we like them. Bright colors are great. They're beautiful. But they just don't need to, nobody to talk about them. They don't have. So probably color words is a good example of how language evolves from sort of function, when you need to communicate the use of something. I think so. Then you kind of invent different variations. And basically, you can imagine that the evolution of a language has to do with what the early tribes doing, like what they want it, what kind of problems they're facing them, and they're quickly figuring out how to efficiently communicate the solution to those problems, whether it's aesthetic or function, all that kind of stuff, running away from a mammoth or whatever. But you know, it's, so I think what you're pointing to is that we don't have data on the evolution of language, because many languages have formed a long time ago. So you don't get the chatter. We have a little bit of like old English to modern English, because there was a writing system. And we can see how old English looked. So the word order changed, for instance, in old English to middle English to modern English. And so it, you know, we could see things like that, but most languages don't even have a writing system. So of the 7000, only, you know, a small subset of those have a writing system. And even if they have a writing system, they, it's not a very modern writing system. And so they don't have it. So we just basically have for Mandarin, for Chinese, we have a lot of, a lot of evidence from, for long time and for English and not for much else, not from in German a little bit, but not for a whole lot of like long-term language evolution. We don't have a lot. Well, you can have snapshots is what we've got of current languages. Yeah, you get an inkling of that from the rapid communication and certain platforms, like on Reddit, there's different communities, and they'll come up with different slang, usually from my perspective, German by a little bit of humor, or maybe mockery or whatever, you know, just talking shit in different kinds of ways. And you could see the evolution of language there. Because I think a lot of things on the internet, you don't want to be the boring mainstream. So you like want to deviate from the proper way of talking. And so you get a lot of deviation, like rapid deviation, then when communities collide, you get like, just like you said, humans adapt to it. And you can see it through the lens of humor. I mean, it's very difficult to study, but you can imagine like a hundred years from now, well, if there's a new language born, for example, we'll get really high resolution data on. I mean, English is changing. English changes all the time. All languages change all the time. So, you know, it's a famous result about the Queen's English. So if you look at the Queen's vowels, the Queen's English is supposed to be, you know, originally the proper way for the talk was sort of defined by whoever the Queen talked, or the King, whoever was in charge. And so if you look at how her vowels changed from when she first became Queen in 1952 or 1953, when she was coronated, the first, I mean, that's Queen Elizabeth, who died recently, of course, until, you know, 50 years later, her vowels changed, her vowels shifted a lot. And so that, you know, even in the sounds of British English, in her, the way she was talking was changing. The vowels were changing slightly. So that's just, in the sounds, there's change. I don't know what's, you know, we're, I'm interested. We're all interested in what's driving any of these changes. The word order of English changed a lot over a thousand years, right? So it used to look like German. You know, it used to be a verb final language with case marking, and it shifted to a verb-medial language, a lot of contact. So a lot of contact with French. And it became a verb medial language with no case marking. And so it became this, you know, verb, verb initially thing. So, and so that's, it totally evolved. And so it may very well, I mean, you know, it doesn't evolve maybe very much in 20 years is maybe what you're talking about. But over 50 and 100 years, things change a lot, I think. We'll now have good data, which is great. Can you talk to what is syntax and what is grammar? So you wrote a book on syntax. I did. You were asking me before about what, you know, how do I figure out what a dependency structure is? I'd say the dependency structures aren't that hard to generally, I think it's a lot of agreement of what they, of what they are for almost any sentence in most languages. I think people will agree on a lot of that. There are other parameters in the mix such that some people think there's a more complicated grammar than just a dependency structure. And so, you know, like Noam Tromsky, he's the most famous linguist ever. And he is famous for proposing a slightly more complicated syntax. And so he invented phrase structure grammar. So he's well known for many, many things. But in the 50s, in the early 60s, like the late 50s, he was basically figuring out what's called formal language theory. So, and he figured out sort of a framework for figuring out how complicated language, you know, a certain type of language might be, so-called phrase structure grammars of language might be. And so he, his, his idea was that maybe we can, we can think about the complexity of a language by how complicated the rules are. Okay. And the rules will look like this. They will have a left-hand side and they'll have a right-hand side. Something will, on the left-hand side, will expand to the thing on the right-hand side. So we'll say we'll start with an S, which is like the root, which is a sentence. Okay. And then we're going to expand to things like a noun phrase and a verb phrase is what he would say, for instance. Okay. And S goes to an NP and a VP is a kind of a phrase structure rule. And then we figure out what an NP is. An NP is a determiner and a noun, for instance. And verb phrase is something else, is a verb and another noun phrase and another NP, for instance. Those are the rules of a very simple phrase structure. Okay. And, and so he, he proposed phrase structure grammar as a way to sort of cover human languages. And then he actually figured out that, well, depending on the formalization of those grammars, you might get more complicated or less complicated languages. And so you could, he could, he said, well, you, these are, these are things called, you know, context-free languages, that rule that he thought, you know, human languages tend to be what he calls context-free languages. And, but there are simpler languages, which are so-called regular languages, and they have a more, a more constrained form to the rules of the, of the phrase structure of, of these particular rules. So he, he basically discovered and kind of invented ways to describe the language. And though, and those are phrase, those are phrase structure, a human language. And he was mostly interested in English initially in his, his work in the 50s. So quick questions around all this. So formal language theory is the big field of just studying language formally. Yes. And it doesn't have to be human language there. We can have a computer languages, any kind of system, which is generating a, some set of expressions in a language. And those could be like the, the, you know, the statements in a computer language, for example. So it could be that, or it could be human language. So technically, you can study programming languages. Yes. And have been, I mean, heavily studied using this formalism. There, there's a big field of programming languages within the formal language. Okay. And then phrase structure grammar is this idea that you can break down language into this SNP, VP, like type of thing. It's a particular formalism for describing language. Okay. So and, and Chomsky was the first one, he's the one who figured that stuff out back in the 50s. And, and, and, but he, and, and that's equivalent. Actually, the context free grammar is actually is kind of equivalent in the sense that it generates the same sentences as a dependency grammar would, you know, as the dependency grammar is a little simpler in some way. You just have a root and it goes like, we don't have any of these, the rules are implicit, I guess, in, in, we just have connections between words. The free structure grammars are kind of a different way to think about the, the dependency grammar. It's slightly more complicated, but it's kind of the same in some ways. So to clarify, dependency grammar is the framework under which you see language. And you make a case that this is a good way to describe the language. That's correct. And no, no Chomsky is watching this is very upset right now. So let's just kidding. But what's the difference between where's the place of disagreement between phrase structure grammar and dependency grammar? They're very close. So free structure grammar and dependency grammar aren't that, aren't that far apart. I like dependency grammar, because it's more perspicuous, it's more transparent about representing the connections between the words. It's just a little harder to see in phrase structure grammar, you know, the place where Chomsky sort of devolved or went off from, from, from this is he also thought there was something called movement. Okay. And so, and so, and that's where we disagree. Okay, that's the place where I would say we disagree. And, and, and I mean, well, maybe we'll get into that later. But the idea is, if you want to, do you want me to explain that? No, I would love to explain movement. Okay, so you're saying so many interesting things. Okay, so here's the movement is Chomsky basically sees English. And he says, okay, I said, you know, we had that sentence early, like it was like two dogs entered the room, it's changed a little bit, say, two dogs will enter the room. And he notices that, hey, English, if I want to make a question, yes, no question from that same sentence, I say, instead of two dogs will enter the room, I say, will two dogs enter the room? Okay, there's a different way to say the same idea. And it's like, well, the auxiliary verb that will thing, it's at the front as opposed to in the middle. Okay. And so, and he looked, you know, if you look at English, you see that that's true for all those modal verbs. And for other kinds of auxiliary verbs in English, you always do that, you always put an auxiliary verb at the front. And, and what he's, when he saw that, so, you know, if I say, I can win this bet, can I win this bet, right? So I move a can to the front. So actually, that's a theory, I just gave you a theory there, he talks about it as movement, that word in the declarative is the root, is the sort of default way to think about the sentence, and you move the auxiliary verb to the front, that's a movement theory. Okay, he said, and he just thought that was just so obvious that it must be true, that there's nothing more to say about that, that this is how auxiliary verbs work in English. There's a movement rule, such that you're moved, like to get from the declarative to the interrogative, you're moving the auxiliary to the front. And it's a little more complicated as soon as you go to simple, simple present and simple past, because, you know, if I say, you know, John slept, you have to say, did John sleep, not slept John, right? And so you have to somehow get an auxiliary verb, and I guess underlyingly, it's like slept is, it's a little more complicated than that, but that's his idea, there's a movement, okay? And so a different way to think about that, that isn't, I mean, he ended up showing later. So he proposed this theory of grammar, which has movement, and there's other places where he thought there's movement, not just auxiliary verbs, but things like the passive in English, and things like questions, WH questions, a bunch of places where he thought there's also movement going on. And each one of those, he thinks there's words, well, phrases and words are moving around from one structure to another, which he called deep structure to surface structure. I mean, there's like two different structures in his theory, okay? There's a different way to think about this, which is there's no movement at all. There's a lexical copying rule, such that the word will or the word can, these auxiliary verbs, they just have two forms. And one of them is the declarative, and one of them is the interrogative. And you basically have the declarative one, and oh, I form the interrogative, or I can form one from the other, doesn't matter which direction you go. And I just have a new entry, which has the same meaning, which has a slightly different argument structure, argument structure, it's a fancy word for the ordering of the words. And so if I say, you know, it was the dogs, two dogs can or will enter the room. There's two forms of will. One is will declarative. And then, okay, I've got my subject to the left, it comes before me, and the verb comes after me in that one. And then the will interrogative is like, oh, I go first, interrogative will is first, and then I have the subject immediately after, and then the verb after that. And so you just, you can just generate from one of those words, another word with a slightly different argument structure with different ordering. And these are just lexical copies. They're not necessarily moving from one to another. There's no movement. There's a romantic notion that you have one main way to use a word, and then you could move it around, which is essentially what movement is applying. Yeah, but that's the lexical copying is similar. So then we do lexical copying for that same idea that maybe the declarative is the source, and then we can copy it. And so an advantage, there's multiple advantages of the lexical copying story. It's not my story. This is like Ivan Sog, a bunch of linguists have been proposing these stories as well in tandem with the movement story. Ivan Sog died a while ago, but he was one of the proponents of the non-movement of the lexical copying story. And so that is that a great advantage is, well, Chomsky, really famously in 1971, showed that the movement story leads to learnability problems. It leads to problems for how language is learned. It's really, really hard to figure out what the underlying structure of a language is if you have both phrase structure and movement. It's like really hard to figure out what came from what. There's like a lot of possibilities there. If you don't have that problem, the learning problem gets a lot easier. Just say there's lexical copies. Well, we say the learning problem. Do you mean like humans learning a new language? Yeah, just learning English. So baby is lying around, listening to the crib, listening to me talk, and how are they learning English? Or maybe it's a two-year-old who's learning interrogatives and stuff. How are they doing that? Are they doing it from like, are they figuring out? So Chomsky said it's impossible to figure it out, actually. He said it's actually impossible, not hard, but impossible. And therefore, that's where universal grammar comes from, is that it has to be built in. And so what they're learning is that there's some built-in movement is built in in his story is absolutely part of your language module. And then you are, you're just setting parameters. You're set, depending on English, is just sort of a variant of the universal grammar. And you're figuring out, oh, which orders does English do these things? The non-movement story doesn't have this. It's like much more bottom-up. You're learning rules. You're learning rules one by one. And, oh, this word is connected to that word. Another advantage, it's learnable. Another advantage of it is that it predicts that not all auxiliary might move. Like, it might depend on the word, depending on whether you, and that turns out to be true. So there's words that don't really work as auxiliary. They work in declarative and not in interrogative. So I can say, I'll give you the opposite first. I can say, aren't I invited to the party? Okay. And that's an interrogative form, but it's not from, I aren't invited to the party. There is no I aren't, right? So that's interrogative only. And then we also have forms like ought. I ought to do this. And I guess some British, old British people can say, exactly. It doesn't sound right, does it? For me, it sounds ridiculous. I don't even think ought is great, but I mean, I totally recognize I ought to do it. It's not too bad, actually. I can say ought to do this. That sounds pretty good. If I'm trying to sound sophisticated, maybe. I don't know. It just sounds completely out of to me. Anyway, so there are variants here. And a lot of these words just work in one versus the other. And that's fine under the lexical copying story. It's like, well, you just learned the usage. Whatever the usage is, is what you do with this word. But it doesn't, it's a little bit harder in the movement story. The movement story, like that's an advantage, I think, of lexical copying. And in all these different places, there's all these usage variants which make the movement story a little bit harder to work. So one of the main divisions here is the movement story versus the lexical copy story that has to do about the auxiliary words and so on. But if you're relying to the phrase structure grammar versus dependency grammar. Those are equivalent in some sense in that for any dependency grammar, I can generate a free structure grammar which generates exactly the same sentences. I just like the dependency grammar formalism because it makes something really salient, which is the lengths of dependencies between words, which isn't so obvious in the phrase structure. In the phrase structure, it's just kind of hard to see. It's in there. It's just very opaque. Technically, I think phrase structure grammar is mappable to dependency grammar. And vice versa. And vice versa. But there's like these little labels, S and PVP. Yeah. For a particular dependency grammar, you can make a phrase structure grammar which generates exactly those same sentences and vice versa. But there are many phrase structure grammars which you can't really make a dependency grammar. I mean, you can do a lot more in a phrase structure grammar. You get many more of these extra nodes, basically. You can have more structure in there. And some people like that. And maybe there's value to that. I don't like it. Well, for you, we should clarify. So dependency grammar, it's just one word depends on only one other word and you form these trees. And that makes, it really puts priority on those dependencies just like as a tree that you can then measure the distance of the dependency from one word to the other. They can then map to the cognitive processing of the of these sentences, how well how easy is to understand all that kind of stuff. So it just puts the focus on just like the mathematical distance of dependence between words. So like it's just a different focus. Absolutely. Just continue on a thread of Chomsky because it's really interesting because it as you're discussing disagreement, to the degree there's disagreement, you're also telling the history of the study of language, which is really awesome. So you mentioned context free versus regular. Does that distinction come into play for dependency grammars? No, not at all. I mean, regular languages are too simple for human languages. It's a part of the hierarchy, but human languages are in the phrase structure world are definitely, at least context free, maybe a little bit more, a little bit harder than that. So there's something called context sensitive as well, where you can have like this is the just the formal language description. In a context free grammar, you have one, this is like a bunch of like formal language theory we're doing here. I love it. Okay. So you have you have a left hand side category and you're expanding to anything on the right is a, that's a context free. So like the idea is that that category on the left expands in independent of context to those things, whatever they're on the right. It doesn't matter what. And a context sensitive says, okay, I actually have more than one thing on the left. I can tell you only in this context, you know, I have maybe you have like a left and a right context or just a left context or a right context, I have two or more stuff on the left tells you how to expand that those things in that way. Okay. So it's context sensitive. A regular language is just more constrained. And so it, it doesn't allow anything on the right. It allows very, it allows basically it's a one very complicated rule is kind of what a regular language is. And so it doesn't have any, let's just say long distance dependencies, it doesn't allow recursion, for instance, there's no recursion. Yeah, recursion is where you, which is human languages have recursion, they have embedding. And you can't, well, it doesn't allow center embedded recursion, which human languages have, which is what center embedded recursion, within a sentence, within a sentence. Yeah, within a sentence. So here we're going to get to that. But I, you know, the formal language stuff is a little aside, Chomsky wasn't proposing it for human languages, even he was just pointing out that human languages are context free. And then he was most in for, for human, because that was kind of stuff we did for formal languages. And what he was most interested in was human language. And that's like the, the movement is where we, we, we, where, where he sort of set off in on the, I would say a very interesting, but wrong foot. It was kind of interesting. It's a very, I agree. It's kind of, it's very interesting history. So there's a set, he proposed this multiple theories in 57, and then 65, they're, they all have this framework, though, was phrase structure, plus movement, different versions of the, of the phrase structure and the movement in the 57. This is the most famous original bits of Chomsky's work. And then 71 is when he figured out that those lead to learning problems, that, that there's cases where a kid could never figure out which rule, which set of rules was intended. And, and so, and then he said, well, that means it's innate. It's kind of interesting. He just really thought the movement was just so obviously true that he couldn't, he didn't even entertain giving it up. It's just obvious that that's obviously right. And it was later where people figured out that there's all these like subtle ways in which things would, which look like generalizations aren't generalizations. And they, you know, across the category, they're, they're word specific and they have, and they, they kind of work, but they don't work across various other words in the category. And so it's easier to just think of these things as lexical copies. And, and I think he was very obsessed. I don't know. I'm just guessing that he just, he really wanted this story to be simple in some sense. And language is a little more complicated in some sense, you know, he didn't like words. He never talks about words. He likes to talk about combinations of words and words are, you know, look up a dictionary, there's 50 senses for a common word, right? The word take will have 30 or 40 senses in it. So, there'll be many different senses for common words. And he just doesn't think about that. It's, or he doesn't think that's language. I think he doesn't think that's language. He thinks that words are distinct from combinations of words. I think they're the same. If you look at my brain in the scanner, while I'm listening to a language I understand, and you compare, I can localize my language network in a few minutes, in like 15 minutes. And what you do is I listen to a language I know, I listen to, you know, maybe some language I don't know, or I listen to muffled speech, or I read sentences, or I read non words, like I do anything like this, anything that sort of really like English and anything that's not very like English. So I've got something like it and not, and I got to control. And the voxels, which is just, you know, the 3D pixels in my brain that are responding most is a language area. And that's this left lateralized area in my head. And wherever I look in that network, if you look for the combinations versus the words, it's everywhere. It's the same. That's fascinating. And so it's like hard to find, there are no areas that we know. I mean, that's a little overstated right now. At this point, the technology isn't great. It's not bad. But we have the best way to figure out what's going on in my brain when I'm listening or reading language is to use fMRI, Functional Magnetic Resonance Imaging. And that's a very good localization method. So I can figure out where exactly these signals are coming from pretty, you know, down to, you know, millimeters, cubic millimeters are smaller, okay? Very small. We can figure those out very well. The problem is the when, okay? It's measuring oxygen, okay? And oxygen takes a little while to get to those cells. And so it takes on the order of seconds. So I talk fast. I probably listened fast and I can probably understand things really fast. So a lot of stuff happens in two seconds. And so to say that we know what's going on, that the words right now in that network, our best guess is that whole network is doing something similar, but maybe different parts of that network are doing different things. And that's probably the case. We just don't have very good methods to figure that out right at this moment. And so since we're kind of talking about the history of the study of language, what other interesting disagreements, and you're both at MIT or were for a long time, what kind of interesting disagreements there, attention of ideas are there between you and Noam Chomsky. And we should say that Noam was in the linguistics department. And you're, I guess, for a time were affiliated there, but primarily brain and cognitive science department, which is another way of studying language. And you've been talking about fMRI. So like what, is there something else interesting to bring to the surface about the disagreement between the two of you or other people in the industry? Yeah, I mean, I've been at MIT for 31 years since 1993, and Chomsky's been there much longer. So I met him, I knew him, I met when I first got there, I guess, and we would interact every now and then. I'd say that, so I'd say our biggest difference is our methods. And so that's the biggest difference between me and Noam, is that I gather data from people. I do experiments with people and I gather corpus data, whatever, whatever corpus data is available, and we do quantitative methods to evaluate any kind of hypothesis we have. He just doesn't do that. And so, you know, he has never once been associated with any experiment or corpus work ever. And so it's all thought experiments, it's his own intuitions. So I just don't think that's the way to do things. That's a, you know, across the street, they're across the street from us, kind of difference between brain and cog sci and linguistics. I mean, not all linguists, some of the linguists, depending on what you do, more speech oriented, they do more quantitative stuff. But in the meaning, words and, well, it's combinations of words, syntax, semantics, they tend not to do experiments and corpus analyses. So I know in linguistics size, probably, well, but the method is a symptom of a bigger approach, which is sort of a psychology philosophy side on Nome. And for you, it's more sort of data driven, sort of almost like mathematical approach. Yeah, I mean, I'm a psychologist. So I would say we're in psychology. You know, I mean, brain and cognitive sciences is MIT's old psychology department. It was a psychology department up until 1985, and it became the brain and cognitive science department. And so, I mean, my training is in psychology, I mean, my training is math and computer science, but I'm a psychologist. I mean, I mean, I don't know what I am. So data driven, psychologist. Yeah, yeah, yeah. You are. I am what I am, but I'm having to be called a linguist. I'm having to be called a computer scientist. I'm having to be called a psychologist, any of those things. But in the actual, like how that manifests itself outside of the methodology is like these differences, these subtle differences about the movement story versus the lexical copy story. Yeah, those are theories, right? So the theories, like the theories are, but I think the reason we differ in part is because of how we evaluate the theories. And so I evaluate theories quantitatively, and Nome doesn't. Got it. Okay, well, let's let's explore the theories that you explore in your book. Let's return to this dependency grammar framework of looking at language. What's a good justification why the dependency grammar framework is a good way to explain language? What's your intuition? So the reason I like dependency grammar, as I've said before, is that it's very transparent about its representation of distance between words. So it's like, all it is, is you've got a bunch of words, you're connecting them together to make a sentence. And a really neat insight, which turns out to be true, is that the further apart the pair of words are that you're connecting the harder it is to do the production, the harder it is to do the comprehension. It's as hard to produce, it's hard to understand when the words are far apart. When they're close together, it's easy to produce and it's easy to comprehend. Let me give you an example. Okay, so we have, in any language, we have mostly local connections between words, but they're abstract. The connections are abstract, they're between categories of words. And so you can always make things further apart if you add modification, for example, after a noun. So a noun in English comes before a verb, the subject noun comes before a verb, and then there's an object after, for example. So I can say what I said before, the dog entered the room or something like that. So I can modify dog. If I say something more about dog after it, then what I'm doing is, indirectly, I'm lengthening the dependence between dog and entered by adding more stuff to it. So I just make it explicit here if I say the boy who the cat scratched cried. We're going to have a mean cat here. And so what I've got here is, the boy cried, it would be a very short, simple sentence, and I just told you something about the boy, and I told you it was the boy who the cat scratched. Okay. So the cry is connected to the boy. The cry at the end is connected to the boy in the beginning. Right. And so I can do that. And I can say that that's a perfectly fine English sentence. And I can say the cat which the dog chased ran away or something. Okay. I can do that. But it's really hard. So it's really hard now. I've got whatever I have here. I have the boy who the cat. Now let's say I try to modify cat. Okay. The boy who the cat which the dog chased scratched ran away. Oh my God, that's hard, right? I can, I'm sort of just working that through in my head how to produce and how to, and it's really just horrendous to understand. It's not so bad. At least I've got intonation there to sort of mark the boundaries and stuff. But it's, that's really complicated. That's sort of English in a way. I mean, that follows the rules of English. But so what's interesting about that is, is that what I'm doing is nesting dependencies there. I'm putting one, I've got a subject connected to a verb there. And then I'm modifying that with a clause, another clause, which happens to have a subject and a verb relation. I'm trying to do that again on the second one. And what that does is it lengthens out the dependence, multiple dependence actually get lengthened out there. The dependencies get longer, on the outside ones get long. And even the ones in between get kind of long. And you just, so what's fascinating is that that's bad. That's really horrendous in English. But that's horrendous in any language. And so in, in no matter what language you look at, if you do just figure out some structure where I'm going to have some modification following some head, which is connected to some later head, and I do it again, it won't be good. It guaranteed like 100% that will be uninterpretable in that language in the same way that was uninterpretable in English. Just clarify, the distance of the dependencies is whenever the boy cried, there's a dependence between two words. And then you counting the number of what morphemes between them. That's a good question. I just say words. Your words are morphemes between. We don't know that. Actually, that's a very good question. What is the distance metric? But let's just say it's words. Sure. And you're saying the longer the distance of that dependence, the more no matter the language, except legalese. Even legalese. We'll talk about it. Okay. But that, the people will be very upset that speak that language, not upset, but they'll either not understand it. They'll be like, this is, their brain will be working in overtime. They will have a hard time either producing or comprehending it. They might tell you that's not their language. It's sort of the language. I mean, it's following, they'll agree with each of those pieces as part of the language. But somehow that combination will be very, very difficult to produce and understand. Is that a chicken or the egg issue here? Well, I'm giving you an explanation. I'm giving you two kinds of explanations. I'm telling you that center embedding, that's nesting, those are synonyms for the same concept here. And the explanation for what, those are always hard. Center embedding and nesting are always hard. And I give you an explanation for why they might be hard, which is long distance connections. There's a when you do center embedding, when you do nesting, you always have long distance connections between the dependents. That's not necessarily the right explanation. I can go through reasons why that's probably a good explanation. And it's not really just about one of them. So probably it's a pair of them or something of these dependents that get long, that drives you to be really confused in that case. And so what the behavioral consequence there, I mean, this is kind of methods, like how do we get at this? You could try to do experiments to get people to produce these things. They're going to have a hard time producing them. You can try to do experiments to get them to understand them and see how well they understand them, can they understand them. Another method you can do is give people partial materials and ask them to complete them, those center embedded materials and they'll fail. So I've done that. I've done all these kinds of things. Wait a minute. So central embedding, meaning like you take a normal sentence like boy cried and inject a bunch of crap in the middle that separates the boy and the cried. Okay, that's central bedding and nesting is on top of that. No, nesting is the same thing. Center embedding, those are totally equivalent terms. I'm sorry, I sometimes use one and sometimes use the other. Got it. Got it. And then what you're saying is there's a bunch of different kinds of experiments you can do. I mean, I like to understand one is like have more embedding, more central bedding, is it easier or harder to understand, but then you have to measure the level of understanding, I guess. Yeah, you could. I mean, there's multiple ways to do that. I mean, there's the simplest ways just to ask people, how good is it sound? How natural is this sound? That's a very blunt, but very good measure. It's very, very reliable. People will do the same thing. And so it's like, I don't know what it means exactly, but it's doing something such that we're measuring something about the confusion, the difficulty associated with those. And those like those are giving you signal. That's why you can say that. What about the completion of the central bed? So if you give them a partial sentence, say I say the book which the author who, and I ask you to now finish that off for me, I mean, either say it, but you can just say it's written in front of you and you can just type and have as much time as you want. They will, even though that one's not too hard, right? So if I say it's like the book is like, oh, the book which the author who I met wrote was good. That's a very simple completion for that. If I give that completion on online somewhere to a crowdsourcing platform and ask people to complete that, they will miss off a verb very regularly, like half of the time, maybe two thirds of the time. They'll say, they'll just leave off one of those verb phrases. Even with that simple so to say, the book which the author who, and they'll say was, you need three verbs, right? I need three verbs are who I met wrote was good, and they'll give me two. They'll say who was famous was good or something like that. They'll just give me two. And that'll happen about 60% of the time. So 40%, maybe 30%, they'll do it correctly, correctly, meaning they'll do a three verb phrase. I don't know what's correct or not. This is hard. It's a hard task. Yeah. I can actually, I'm struggling with it in my head. Yeah. Well, it's easier when you look at it. If you look at it a little easier, then listening is pretty tough because you have to, because there's no trace of it. You have to remember the words that I'm saying, which is very hard auditorily. We wouldn't do it this way. We do it written. You can look at it and figure it out. It's easier in many dimensions in some ways, depending on the person. It's easier to gather written data for, I mean, most sort of psycho, I work in psycholinguistics, right? Psychology of language and stuff. And so a lot of our work is based on written stuff because it's so easy to gather data from people doing written kinds of tasks. Spoken tasks are just more complicated to administer and analyze because people do weird things when they speak. And it's harder to analyze what they do, but they generally point to the same kinds of things. It's okay. So the universal theory of language by Ted Gibson is that you can form dependency, you can form trees from any senses, and you can measure the distance in some way of those dependencies. And then you can say that most languages have very short dependencies. All languages. All languages. All languages have short dependencies. You can actually measure that. So an ex-student of mine, this guy is at University of California, Irvine. Richard Futrell did a thing a bunch of years ago now where he looked at all the languages we could look at, which was about 40 initially. And now I think there's about 60 for which there are dependency structures. So they're meaning there's got to be like a big text, a bunch of texts, which have been parsed for the dependency structures. And there's about 60 of those which have been parsed that way. And for all of those, what he did was take any sentence in one of those languages, and you can do the dependency structure, and then start at the root. We were talking about dependency structures. That's pretty easy now. And he's trying to figure out what a control way you might say the same sentence is in that language. And so what he does is just like, all right, there's a root, and say as a sentence is, let's go back to, two dogs entered the room. So entered is the root. And entered has two dependents that's got dogs, and it has room. And what he does is like, let's scramble that order. That's three things, the root, and the head, and the two dependents, and into some random order, just random. And then just do that for all the dependents down the two. So now look, do it for the whatever was two in dogs and for in room. And that's not a very short sentence. When sentences get longer, and you have more dependents, there's more scrambling that's possible. And what he found, what so that so so that that's one, you can figure out one scrambling for that sentence, he did like a hundred times for every sentence and every corp and every one of these texts, every corpus. And then he just compared the dependency lengths in those random scramblings to what actually happened with what the English or the French or the German was in the original language or Chinese or what all these like 80, like, you know, 60 languages, okay. And the dependency lengths are always shorter in the real language compared to this kind of a control. And there's another, it's a little more rigid his control. So the way I described it, you could have crossed dependencies, like that by scrambling that way, you could scramble in any way at all. Languages don't do that. They tend not to cross dependencies very much. Like so the dependency structure, they just they tend to keep things non-crossed. And there's a, you know, like, there's a technical term they call that projective, but it's just non-crossed is all that is projective. And so if you just constrain the scrambling so that it only gives you projectives sort of non-crossed is the same thing holds. So it's so the you still still human languages are much shorter than these this kind of a control. So there's like, what it means is that that we're in every language, we're trying to put things close in relative to this kind of a control, like it doesn't matter about the word order, some of these are verb final, some of them is a verb, media-like English, and some are even verb initial. There are a few languages in the world which have VSO, word order, verb, subject, object languages, haven't talked about those. It's like 10% of the and even even in those languages, it's still short dependencies. Short dependencies is rules. Okay. So how what, what are some possible explanations for that? For why languages have evolved that way? So that that's one of the, I suppose disagreements you might have with Chomsky. So you consider the evolution of language in terms of information theory. And for you, the purpose of language is ease of communication, right, in processing. That's right. That's right. So I mean, the story here is just about communication. It is just about production really. It's about ease of production is the story. When you say production, can you? Oh, I just mean each of language production. It's easier for me to say things when the, when I'm doing, whenever I'm talking to you is somehow I'm formulating some idea in my head and I'm putting these words together. And it's easier for me to do that, to put, to say something where the words are close, closely connected in a dependency, as opposed to separated, like by putting something in between and over and over again. It's just hard for me to keep that in my head. It like, that's, that's the whole story. Like the story is basically, it's like the dependency grammar sort of gives that to you. Like just like long, long as bad, short as good. It's like easier to keep in mind because you have to keep it in mind for probably for production, probably matters in comprehension as well. Like also matters in comprehension. It's on both sides of it. The production and the, but I would guess it's probably evolved for production. It's about producing. It's what's easier for me to say that ends up being easier for you also. And that's very hard to disentangle this idea of who is it for? Is it for me, the speaker, or is it for you, the listener? I mean, part of my language is for you. Like the way I talk to you is going to be different from how I talk to different people. So I'm, I'm definitely angling what I'm saying to who I'm saying, right? It's not like I'm just talking the same way to every single person. And so I am sensitive to my audience, but how does that, does that, you know, work itself out in the, in the dependency link differences? I don't know. Maybe that's about just the words, that part, you know, which words I select. My initial intuition is that you optimize language for the audience. Yeah. But it's just kind of like messing with my head a little bit to say that some of the optimization might be, or it may be the primary objective, the optimization might be the ease of production. We have different senses, I guess. I'm, I'm like, very selfish and you're like, I think it's like, it's all about me. I'm like, I'm just doing what's easiest for me at all times. I don't want to, I'm like, I'll, I mean, but I have to, of course, choose the words that I think you're going to know. I'm not going to choose words you don't know. In fact, I'm going to fix that when I, you know, so there it's about, but, but maybe for, for the syntax, for the combinations, it's just about me. I feel like it's, I don't know though, it's great. Wait, wait, wait, wait, wait, but the purpose of communication is to be understood, is to convince others and so on. So like the selfish things to be understood. Okay. It's about the listener. It's a little circular there too then. Okay. Right. I mean, like the ease of production helps me be understood then. I don't think it's circular. So I think the primary, I think the primary objective is to be understood is about the listener. Cause otherwise the, if you're optimizing for the ease of production, then you're, you're not going to have any of the interesting complexity of language. Like you're trying to like explain. Well, let's control for what it is I want to say. Like I, I'm saying let's control for the thing, the, the message, control for the message. But that means the message needs to be understood. That's the goal. Oh, but that's the meaning. So I'm still talking about the form. Just the form of the meaning. How do I frame the form of the meaning is all I'm talking about. You're talking about a harder thing, I think, is like, how am I, like trying to change the meaning. Let's, let's keep the meaning constant. Like which, if you keep the meaning constant, how can I phrase whatever it is I need to say, like I gotta pick the right words and I'm gonna pick the order so that it's, so it's easy for me. That's, that's, that's what I think it's probably like. I think I'm still tying meaning and form together in my head. But you're saying, if you keep the meaning of what you're saying constant, would the optimization, yeah, it could be the primary objective of that optimization is the, for production. That's interesting. I'm struggling to keep constant the meaning. It's just so, I mean, I'm such a, I'm a human, right? So for me, the form without having introspected on this, the form and the meaning are tied together, like deeply because I'm a human. Like for me, when I'm speaking, because I haven't thought about language, like in a rigorous way about the form of language. But look, for any event, there's, there's an unbounded, I don't want to say infinite, but sort of ways that I might communicate that same event. This two dogs entered a room, I can say in many, many different ways. I can say, Hey, there's two dogs. They entered the room. Hey, the room was entered by something. The thing that was entered was two dogs. I mean, there's, I mean, it's kind of awkward and weird stuff. But those are all similar messages with different forms, but different ways that might frame. And of course, I use the same words there all the time. I could have referred to the dogs as a Dalmatian and a Poodle or something. I could have been more specific or less specific about what they are. And I could have said, been more abstract about the number. There's like, so I, like I'm trying to keep the meaning, which is this event constant. And then how am I going to describe that to get that to you? It kind of depends on what you need to know, right? And what I think you need to know. But I'm like, let's control for all that stuff and not, and then I'm just like choosing about, I'm doing something simpler than you're doing, which is just forms. Yes. Just words. So to you specifying the breed of dog and whether they're cute or not is changing the meaning. That might be, yeah. Yeah, that would be changing. Oh, that would be changing the meaning for sure. Right. So you're just, yeah. Yeah. That's changing the meaning. But say, even if we keep that constant, we can still talk about what's easier or hard for me, right? The listener and the, which phrase structures I use, which combinations, which, yeah. This is so fascinating and just like a really powerful window into human language. But I wonder still throughout this, how vast the gap between meaning and form, I just, I just have this like, maybe romanticized notion that they're close together, that they evolve close to like hand in hand, that you can't just simply optimize for one without the other being in the room with us. Like it's, well, it's kind of like an iceberg. Form is the tip of the iceberg and the rest, the, the meaning is the iceberg, but you can't like separate. But I think that's why these large language models are so successful is because they're good at form and form isn't that hard in some sense. And meaning is tough still. And that's why they're not, they're, you know, they don't understand what they're doing. We're going to talk about that later maybe, but like we can distinguish in our, forget about large language models, like humans, maybe you'll talk about that later too, is like the difference between language, which is a communication system and thinking, which is meaning. So language is a communication system for the meaning. It's not the meaning. And so that's why, I mean, that, and there's a lot of interesting evidence we can talk about relevant, relevant to that. Well, I mean, that's a really interesting question. What is the difference between language written, communicated versus thought? What to use the difference between them? Well, you or anyone cast a think of a task, which they think is, is a good thinking task. And there's lots and lots of tasks, which should be good thinking tasks. And whatever those tasks, let's say it's, you know, playing chess, or that's a good thinking task, or playing some game, or doing some complex puzzles, maybe, maybe remembering some digits that's thinking, remembering some, a lot of different tasks we might think, maybe just listening to music is thinking, or there's a lot of different tasks we might think of as thinking. There's a woman in my department at Federico, and she's done a lot of work at, on this question about what's the connection between language and thought. And, and so she uses, I was referring earlier to MRI, fMRI, that's her primary method. And so she is been really fascinated by this question about whether, what language is, okay? And so as I mentioned earlier, you can localize my language area, your language area in a few minutes, okay? In like 15 minutes, I can listen to language, listen to non-language or backward speech or something. And we'll find areas left lateralized network in my head, which is especially, which is very sensitive to language, as opposed to whatever that control was, okay? Can you specify what you mean by language, like communicated language? Like what is language? Just sentences. You know, I'm listening to English of any kind story, or I can read sentences, anything at all that I understand, if I understand it, then it'll activate my language network. So right now, my language network is going like crazy when I'm talking and when I'm listening to you, because we're both, we're communicating. And that's pretty stable. Yeah, it's incredibly stable. So I've, I happen to be married to this woman Federico. So I've been scanned by her over and over and over since 2007 or six or something. And so my language network is exactly the same, you know, like a month ago, as it was back in 2007. It's amazingly stable. It's astounding. And with it, it's, it's a really fundamentally cool thing. And so my language network is, it's like my face, okay? It's not changing much over time inside my head. Can I ask a quick question? Sorry, it's a small tangent. At which point in the, as you grow up from baby to adult, does it stabilize? We don't know. Like that's a, that's a very hard question. They're working on that right now because of the problem of scanning little kids, like doing the, trying to do local, trying to do the localization on little children in this scanner, or you're lying in the fMRI scan. That's the best way to figure out where something's going on inside our brains. And the scanner is loud and you're in this tiny little, you know, area, you're claustrophobic. And it doesn't bother me at all. I can go to sleep in there, but some people are bothered by it. And little kids don't really like it. And they don't like to lie still. And you have to be really still because you can move around. That's, that messes up the coordinates of where, where everything is. And so, you know, try to get, you know, your question is how and when our language developing, you know, how, when, when, how does this left lateralized system come to play? Where does it, you know, and it's really hard to get a two year old to do this task. But you can maybe where they're starting to get three and four and five year olds to do this task for short periods. And it looks like it's there pretty early. So clearly when you lead up to you, like a baby's first words, before that, there's a lot of fascinating turmoil going on about like figuring out like, what are, what are these people saying? And you're trying to like make sense, how does that connect to the world and all that kind of stuff. Yeah. That might be just fascinating development that's happening there. That's hard to introspect. But anyway, you, we're back to the scanner and I can find my network in 15 minutes. And now we can ask a, find my network, find yours, find, you know, 20 other people do this task. And we can do some other tasks. Anything else you think is thinking of some other thing. I can do a spatial memory task. I can do a music perception task. I can do programming task if I program. Okay. I can do where I can like understand computer programs. And none of those tasks will tap the language network at all, like at all. There's no overlap. They do, they're, they're highly activated in other parts of the brain. There's a, there's a bilateral network, which I think she tends to call the multiple demands network, which does anything kind of hard and, and so anything that's kind of difficult in some ways will activate that multiple demands network. I mean, music will be in some music area, you know, there's music specific kinds of areas. And so, but they're, but, but none of them are activating the language area at all, unless there's words, like, so if you have music and there's a song and you can hear the words then, then then you get the language area. We're talking about speaking and listening, but are, or are we also talking about reading? This is all comprehension of any kind. And so that is fast. So what this, this, this network doesn't make any difference if it's written or spoken. So the, the, the thing that she calls, Federico calls the language network is this high level language. So it's not about the spoken, the spoken language, and it's not about the written language is about either one of them. And so we're, so when you do speech, you're sort of list, you either, you're listening to speech and you'd, you know, subtract away some language you don't understand and see where it's, or you subtract away back, backward speech, which signs sounds like speech, but it isn't. And, and then so you take away the sound part altogether. And so, and then if you do written, you get exactly the same network. So for just reading the language versus reading sort of nonsense words or something like that, you'll find exactly the same network. And so it's just about high level, um, comprehension of language. Yeah. In this case, and the same thing happened, production is a little harder to run the scanner, but the same thing happens in production. You get the same network. So production is a little harder. You have to figure out how do you run a task, you know, in the network such that you're doing some kind of production. And I can't remember what, they've done a bunch of different kinds of tasks there where you get people to, you know, produce things. Yeah. Figure out how to produce and the same network goes on there. Exactly the same place. And so if, wait, wait, so if you read random words, yeah, if you read things like, um, like gibberish. Yeah. Yeah. Lewis Carroll's, Twas Brillig, Jabberwocky, right? They call that Jabberwocky speech. The network doesn't get activated. Not as much. There are words in there. Because it's like, there's function words and stuff. So it's lower activation. Yeah. Yeah. So there's like, basically the more language like it is, the higher it goes in the language network. And that network is there from when you speak from, as soon as you learn language. And, and it's, it's there, like you speak multiple languages, the same network is going for your multiple languages. So you speak English, you speak Russian, both of them are hitting that same network. If you, if you're affluent in those languages. So programming. Not at all. Isn't that amazing? Even if you're a really good programmer, that is not a human language. It's just not conveying the same information. And so it is not in the language network. And so that has mind blowing as I think. That's pretty cool. That's weird. It is amazing. And so that's like one set of day. This is hers like shows that what you might think is thinking is, is not language language is just the seek, just, just this conventionalized system that we've worked out in human languages. Oh, another fascinating little bit tidbit is that even if they're these constructed languages, like Klingon, or I don't know the languages from Game of Thrones, I'm sorry, I don't remember those languages, maybe a lot of people offended right now. There's people that speak those languages. They really speak those languages because the people that wrote the languages for the shows, they did an amazing job of constructing on something like a human language. And those that, that lights up the language area. That's like, because they can speak, you know, pretty much arbitrary thoughts in a human language. It's not a, it's a constructed human language. Probably it's related to human languages because the people that were constructing them were making them like human languages in various ways, but it also activates the same network, which is pretty, pretty cool. Anyway. Sorry to go into a place where you may be a little bit philosophical, but is it possible that this area of the brain is doing some kind of translation into a deeper set of almost like concepts? It has to be doing. So it's doing in communication, right? It is translating from thought, whatever that is, is more abstract, and it's doing that. That's what it's doing. Like, it is, that is kind of what it is doing. It's like kind of a meaning network, I guess. Yeah, like a translation network. Yeah. But I wonder what is at the core, at the bottom of it? Like, what are thoughts? Are they thoughts? To me, like, thoughts and words, are they neighbors or are, is it one turtle sitting on top of the other? Meaning, like, is there a deep set of concepts that we... Well, there's connections right between those, what these things mean. And then there's probably other parts of the brain that what these things mean. And so, you know, when I'm talking about whatever it is I want to talk about, it'll be represented somewhere else. That knowledge of whatever that is will be represented somewhere else. Well, I wonder if there's like some stable, nicely compressed encoding of meanings that's separate from language. I guess the implication here is that we don't think in language. That's correct. Isn't that cool? And that's so interesting. So people, I mean, this is like hard to do experiments on, but there is this idea of inner voice and a lot of people have an inner voice. And so, if you do a poll on the internet and ask if you hear yourself talking when you're just thinking or whatever, about 70 or 80% of people will say yes. Most people have an inner voice. I don't. And so, I always find this strange when... So, when people talk about an inner voice, I always thought this was a metaphor. And they hear, I know most of you, whoever's listening to this, thinks I'm crazy now because I don't have an inner voice. And I just don't know what you're listening to. It sounds so kind of annoying to me, but to have this voice going on while you're thinking, but I guess most people have that. And I don't have that. And we don't really know what that connects to. I wonder if the inner voice activates that same network. I don't know. I don't know. I mean, this could be speechy, right? So that's like, you hear, do you have an inner voice? I don't think so. A lot of people have this sense that they hear themselves. And then, say they read someone's email, I've heard people tell me that they hear that other person's voice when they read other people's emails. And I'm like, wow, that sounds so disruptive. I do think I like vocalize when I'm reading, but I don't think I hear a voice. Well, that's, you probably don't have an inner voice. Yeah, I don't think I have an inner voice. People have an inner voice. People have this strong percept of hearing sound in their heads when they're just thinking. I refuse to believe that's the majority of people. Majority, absolutely. What? It's like two-thirds or three-quarters. It's not. I would never ask class. And I went internet, they always say that. So you're in a minority. It could be a self-report flaw. It could be. You know, when I'm reading inside my head, I'm kind of like saying the words, which is probably the wrong way to read, but I don't hear a voice. There's no percept of a voice. I don't refuse to believe the majority of people have it. Anyway, it's a fascinating, the human brain is fascinating, but it still blew my mind that the, that language does appear, comprehension does appear to be separate from thinking. So that's one set. One set of data from Fedorenko's group is that no matter what task you do, if it doesn't have words and combinations of words in it, then it won't light up the language network. You know, you could, it'll be active somewhere else, but not there. So that's one. And then this other piece of evidence relevant to that question is, it turns out there are these, this group of people who've had a massive stroke on the left side and wiped out their language network. And as long as they didn't wipe out everything on the right as well, in that case, they wouldn't be, you know, cognitively functionable. But if they just wiped out language, which is pretty tough to do because it's, it's very expansive on the left. But if they have, then there are these, there's patients like this, so-called global aphasics, who can do any task just fine, but not language. They can't, you can't talk to them. I mean, they don't understand you. They can't speak, can't write, they can't read, but they can do, they can play chess, they can drive their cars, they can do all kinds of other stuff, you know, do math, they can do all, like, so math is not in the language area, for instance, you do arithmetic and stuff. That's not language area. It's got symbols. So people sort of confuse some kind of symbolic processing with language and symbolic processing is not the same. So there are symbols and they have meaning, but it's not language. It's not a, you know, conventionalized language system. And so language, so math isn't there. And so they can do math. They're, they do just as well as their control, age match controls and all these tasks. This is Rosemary Varley over in University College London, who has a bunch of patients who, who she's shown this that they're just, so that sort of combination suggests that language isn't necessary for thinking. It doesn't mean you can't think in language. You could think in language because language allows a lot of expression, but it's just, you don't need it for thinking. It's, it suggests that language is separate, is a separate system. This is kind of blowing my mind right now. I'm trying to load that in because it has implications for large language models. It sure does. And they've been working on that. Well, let's take a stroll there. You wrote that the best current theories of human language are arguably large language models. So this has to do with form. It's kind of a big theory. And, but the reason it's arguably the best is that it, it does the best at predicting what's English, for instance, it's, it's like incredibly good, you know, it better than any other theory. It's so, you know, but, you know, we don't, you know, there's, it's not sort of, there's not enough detail. What's opaque? Like, there's not, you don't know what's going on. No, what's going on. It's another black box. But I think it's, you know, it is a theory. What's your definition of a theory? Because it's a gigantic, it's a gigantic black box with, you know, a very large number of parameters controlling it. To me, theory usually requires a simplicity, right? Well, I don't know. Maybe I'm just being loose there. I think it's a, it's not, it's not a great theory, but it's a theory. It's a good theory in one sense, and then it covers all the data. Like anything you want to say in English, it does. And so that's why it's, that's how it's arguably the best, is that no other theory is as good as a large language model in predicting exactly what's good and what's bad in English. You know, now you're saying, is it a good theory? Well, probably not, you know, because I want a smaller theory than that. It's too big. I agree. You could probably construct mechanism by which it can generate a simple explanation of a particular language, like a set of rules, something like it could generate a dependency grammar for a language, right? Yeah. You could probably, you could probably just ask it about itself. Well, you know, that's, I mean, that presumes, and there's some evidence for this, some large language models are implementing something like dependency grammar inside them. And so there's work from a guy called Chris Manning and colleagues over at Stanford in natural language. And they looked at, I don't know how many large language model types, but certainly Burt and some others where, and where you do some kind of fancy math to figure out exactly what the sort of, what kind of abstractions of representations are going on. And they, and they were saying it does look like dependency structure is, is what they're constructing. It doesn't, like, so it's actually a very, very good map. So kind of a, they are constructing something like that. Does it mean that, you know, that they're using that for meaning? I mean, probably, but we don't know. You write that the kinds of theories of language that LLMs are closest to are called construction based theories. Can you explain what construction based theories are? It's just a general theory of language such that there's a form and a meaning pair for, for lots of pieces of the language. And so it's, it's, it's primarily usage based, is a construction grammar. It's just, it's trying to deal with the things that people actually say, actually say and actually write. And so that's, it's a usage based idea. And what's the constructional constructions either a simple word, so like a morpheme plus its meaning or a combination of words, it's basically combinations of words, like the rules. So, but it's, it's unspecified as to what the form of the grammar is under underlyingly. And so I would, I would argue that the dependency grammar is maybe the right form to use for the types of construction grammar. Construction grammar typically isn't kind of formalized quite. And so maybe the formalization, a formalization of that, it might be in dependency grammar. Yeah. I mean, I, I would think so, but I mean, it's up to people, other researchers in that area, if they agree or not. So. Well, do you think that large language models understand language? Are they mimicking language? I guess the deeper question there is, are they just understanding the surface form? Or do they understand something deeper about the meaning that then generates the form? I mean, I would argue they're doing the form, they're doing the form, they're doing it really, really well. And are they doing the meaning? No, probably not. I mean, there's lots of these examples from various groups showing that they can be tricked in all kinds of ways. They really don't understand the, the meaning of what's going on. And so there's a lot of examples that he and other groups have given, which just, which show they don't really understand what's going on. So, you know, the Monty Hall problem is this silly problem, right? Where, you know, if you have three door, it's less make a deal as this old game show, and there's three doors, and there's a prize behind one, and there's some junk prizes behind the other two, and you're trying to select one. And if you, you know, he knows Monty, he knows where the target item is, the good thing, he knows everything is back there. And you're supposed to, he gives you a choice, you choose one of the three, and then he opens one of the doors, and it's some junk prize. And then the question is, should you trade to get the other one? And the answer is yes, you should trade, because he knew which ones you could turn around. And so now the odds are two-thirds, okay? And then you just change that a little bit to the large language mall. The large language mall has seen that explanation so many times, that it just, if you change the story, it's a little bit, but it makes it sound like it's the Monty Hall problem, but it's not. You just say, oh, there's three doors, and one behind them is a good prize, and there's two bad doors. I happen to know it's behind door number one. The good prize, the car, is behind door number one. So I'm going to choose door number one. Monty Hall opens door number three, and shows me nothing there. Should I trade for door number two? Even though I know the good prize in door number one, and then the large language mall say, yes, you should trade, because it just goes through the forms that it's seen before so many times on these cases, where it, yes, you should trade, because your odds have shifted from one in three now to two out of three, to being that thing. It doesn't have any way to remember that actually you have a 100% probability behind that door number one. You know that. That's not part of the scheme that it's seen hundreds and hundreds of times before. And so you can't, even if you try to explain to it that it's wrong, that they can't do that, it'll just keep giving you back the problem. But it's also possible that a larger language model would be aware of the fact that there's sometimes over a representation of a particular kind of formulation, and it's easy to get tricked by that. And so you could see if they get larger and larger models be a little bit more skeptical. So you see over a representation. So it just feels like form can, training on form can go really far in terms of being able to generate things that look like the thing understands deeply the underlying world model of the kind of mathematical world, physical world, psychological world that would generate these kinds of sentences. It just feels like you're creeping close to the meaning part, easily fooled, all this kind of stuff, but that's humans too. So it just seems really impressive how often it seems like it un-understands concepts. I mean, you don't have to convince me of that. I am very, very impressed, but does it, does it mean you're giving a possible world where maybe someone's going to train some other versions such that it'll be somehow abstracting away from types of forms? I mean, I don't think that's happened. No, no, no. I'm not saying that. I think when you just look at anecdotal examples and just showing a large number of them where it doesn't seem to understand and it's easily fooled, that does not seem like a scientific data-driven analysis of how many places is a damn impressive in terms of meaning and understanding and how many places is easily fooled. That's not the inference. So I don't want to make that, the inference I don't, I wouldn't want to make was that inference. The inference I'm trying to push is just that is it, is it like humans here? It's probably not like humans here. It's different. So humans don't make that error. If you explain that to them, they're not going to make that error. They don't make that error. And so that's something, it's doing something different from humans that they're doing in that case. What's the mechanism by which humans figure out that it's an error? I'm just saying the error there is like, if I explain to you there's 100% chance that the car is behind this case, this door, well, do you want to trade? If you'll say no. But this thing will say yes, because it's so, that trick, it's so wound up on the form that it's, that's an error that a human doesn't make, which is kind of interesting. Less likely to make, I should say. Yeah, less likely. Because like humans are very- Oh yeah. I mean, you're asking, you're asking humans, you're asking a system to understand 100%, like you're asking some mathematical concepts. And so like- Look, the places where large language models are, the form is amazing. So let's go back to nested structures, center embedded structures. Okay, if you ask a human to complete those, they can't do it. Neither can a large language model. They're just like humans in that. If you ask, if I ask a large language model- That's fascinating, by the way. The central embedding, the central embedding, it struggles with- Just like humans, exactly like humans. Exactly the same way as humans. And that's not trained. So they do exactly, so that is a similarity. So but then it's, that's not meaning, right? This is form. But when we get into meaning, this is where they get kind of messed up. When you start to saying, oh, what's behind this door? Oh, it's, you know, this is the thing I want. Humans don't mess that up as much, you know. Here, the form is just like, the form of the match is amazing, is similar, without being trained to do that. I mean, it's trained in the sense that it's getting lots of data, which is just like human data. But it's not being trained on, you know, bad sentences and being told what's bad. It just can't do those. It'll actually say things like, those are too hard for me to complete or something, which is kind of interesting. Actually, kind of, how does it know that? I don't know. But it really often doesn't just complete, very often says stuff that's true. And sometimes says stuff that's not true. And almost always the form is great. But it's still very surprising that with really great form, it's able to generate a lot of things that are true based on what is trained on and so on. So it's not just form that is generating. It's mimicking true statements from the internet. I guess the underlying idea there is that on the internet, truth is overrepresented versus falsehood. I think that's probably right. Yeah. So but the fundamental thing is trained on, you're saying is just form. I think so. Yeah. Yeah, I think so. Well, that's a sad, if that's, to me, that's still a little bit of open question. I probably lean agreeing with you, especially now you've just blown my mind that there's a separate module in the brain for language versus thinking. Maybe there's a fundamental part missing from the large language model approach that lacks the thinking, the reasoning capability. Yeah, that's what this group argues. So the same group, Fedorenko's group has a recent paper arguing exactly that. There's a guy called Kyle Mahwell who's here in Austin, Texas, actually. He's an old student of mine, but he's a faculty in linguistics at Texas, and he was the first author on that. That's fascinating. Still, to me, an open question. What do you have the interesting limits of LLMs? I don't see any limits to their form. Their form is perfect. Yeah, it's pretty much, I mean, it's close to- Well, you said ability to complete central embeddings. Yeah, it's just the same as humans. It seems the same. But that's not perfect, right? It should be- That's good. No, but I want to be like humans. I'm trying to, I want a model of humans. Oh, wait, wait, wait, wait, wait. Also, perfect is as close to humans as possible. I got it. Yeah. But you should be able to, if you're not human, you're like you're super human, you should be able to complete central embedded sentences, right? I mean, that's the mechanism is, if it's modeling some, I think it's kind of really interesting that it can't. That it's really interesting. It's more like, like I think it's potentially underlyingly modeling something like what the way the form is processed. The form of human language and how humans process the language. Yes. I think that's plausible. And how they generate language. Process language and general language, that's fascinating. So in that sense, they're perfect. If we can just linger on the center embedding thing, that's hard for LLMs to produce, and that seems really impressive because that's hard for humans to produce. And how does that connect to the thing we've been talking about before, which is the dependency grammar framework in which you view language and the finding that short dependencies seem to be a universal part of language. So why is it hard to complete center embeddings? So what I like about dependency grammar is it makes the cognitive cost associated with longer distance connections very transparent. Basically, as there's some, it turns out there is a cost associated with producing and comprehending connections between words, which are just not beside each other. The further apart they are, the worse it is according to, well, we can measure that. And there is a cost associated with that. Can you just linger on, what do you mean by cognitive cost and how do you measure it? You can measure it in a lot of ways. The simplest is just asking people to say whether how good a sentence sounds. We just ask, that's one way to measure, and you try to triangulate then across sentences and across structures to try to figure out what the source of that is. You can look at reading times in controlled materials, in certain kinds of materials, and then we can measure the dependency distances there. We can, there's a recent study which looked at, we're talking about the brain here, we could look at the language network, okay? We could look at the language network, and we could look at the activation in the language network, and how big the activation is depending on the length of the dependencies. And it turns out in just random sentences that you're listening to, if you're listening to, so it turns out there are people listening to stories here. And the longer the dependency is, the stronger the activation in the language network. And so there's some measure, there's a bunch of different measures we could do, that's a kind of a neat measure actually of actual activation in the brain. So that you can somehow in different ways convert it to a number. I wonder if there's a beautiful equation connecting cognitive costs and length of dependency, equals MC squared kind of thing. Yeah, it's complicated, but probably it's doable. I would guess it's doable. I tried to do that a while ago, and I was reasonably successful, but for some reason I stopped working on that. I agree with you that it would be nice to figure out. So there's like some way to figure out the cost. I mean, it's complicated. Another issue you raised before was like, how do you measure distance? Is it words? It probably isn't, is it part of the problem? Is that some words matter than more than others? And probably, meaning like nouns might matter depending, and then it maybe depends on which kind of noun. Is it a noun we've already introduced or a noun that's already been mentioned? Is it a pronoun versus a name? Like all these things probably matter. So probably the simplest thing to do is just like, oh, let's forget about all that and just think about words or morphemes. For sure. But there might be some insight in a kind of function that fits the data, meaning like what... I think it's an exponential. So we think it's probably an exponential such that the longer the distance, the less it matters. And so then it's the sum of those. That was our best guess a while ago. So you've got a bunch of dependencies. If you've got a bunch of them that are being connected at some point, at the ends of those, the cost is some exponential function of those is my guess. But because the reason it's probably an exponential is like, it's not just the distance between two words, because I can make a very, very long subject verb depends by adding lots and lots of noun phrases and prepositional phrases. And it doesn't matter too much. It's when you do nest it, when I have multiple of these, then things go really bad, go south. Probably somehow connected to working memory or something that's probably the function of the memory here is the access is trying to find those earlier things. It's kind of hard to figure out what was referred to earlier. Those are those connections. That's the sort of notion of working as opposed to a storagey thing, but trying to connect, retrieve those earlier words depending on what was in between. And then we're talking about interference of similar things in between. That's the right theory probably has that kind of notion and it is an interference of similar. And so I'm dealing with an abstraction over the right theory, which is just, it's count words, it's not right, but it's close. And then maybe you're right though, there's some sort of an exponential or something to figure out the total so we can figure out a function for any given sentence in any given language. But it's funny, people haven't done that too much, which I do think is, I'm interested that you find that interesting. I really find that interesting. And a lot of people haven't found it interesting. And I don't know why I haven't got people to want to work on that. I really like that too. That's a beautiful, in the underlying idea is beautiful that there's a cognitive cost that correlates with the length of dependency. It just, it feels like it's a deep, I mean, language is so fundamental to the human experience. And this is a nice clean theory of language where it's like, wow, okay. So like we like our words close together, dependent words close together. That's why I like it too. It's so simple. Yeah, the simplicity of the theory. And yet it explains some very complicated phenomena. If I write these very complicated sentences, it's kind of hard to know why they're so hard. And you can like, oh, nail it down. I can do like a, give you a math formula for why each one of them is bad and where. And that's kind of cool. I think that's very neat. Have you gone through the process? Is there like a, if you take a piece of text and then simplify, sort of like there's an average length of dependency, and then you like, you know, reduce it and see comprehension on the entire, not just a single sentence, but like, you know, you go from James Joyce to Hemingway or something. No, no, simple answer is no, that does, there's probably things you can do in that, in that kind of direction. That's fun. We might, you know, we're gonna talk about legalese at some point. And so we, maybe we'll talk about that kind of thinking with applied to legalese. Well, let's talk about legalese because you mentioned that as an exception, which is taking tangent upon tangent. That's an interesting one. You give it as an exception. It's an exception. That you say that most natural languages, as we've been talking about, have local dependencies with one exception, legalese. That's right. So what is legalese, first of all? Oh, well, legalese is what you think it is. It's just any legal language. I mean, like, I actually know very little about the kind of language that lawyers use. So I'm just talking about language in laws and language in contracts. Got it. So the stuff that you have to run into, we have to run into every other day or every day, and you skip over because it reads poorly. And or, you know, partly it's just long, right? There's a lot of text there that we don't really want to know about. And so, but the thing I'm interested in, so I've been working with this guy called Eric Martinez, who is a, he was a lawyer who was taking my class. I was teaching a psycholinguistics lab class. I haven't been teaching it for a long time at MIT. And he's a, he was a law student at Harvard. And he took the class because he had done some linguistics as an undergrad. And he was interested in the problem of why legalese sounds hard to understand, you know, why. And so why is it hard to understand and why do they write that way? If it is hard to understand, it seems apparent that it's hard to understand. The question is, why is it? And so we didn't know. And we did an evaluation of a bunch of contracts. Actually, we just took a bunch of random contracts, because I don't know, you know, there's contracts and laws might not be exactly the same, but contracts are kind of the things that most people have to deal with most of the time. And so that's kind of the most common thing that humans have, like humans, that adults in our industrialized society have to deal with a lot. And so that's what we pulled. And we didn't know what was hard about them. But it turns out that the way they're written is very center embedded, has nested structures in them. So it has low frequency words as well. That's not surprising. Lots of texts have low frequency, it does have surprising, slightly lower frequency words than other kinds of control texts, even sort of academic texts. Legalese is even worse. It is the worst that we weren't being able to find. You just reveal the game that lawyers are playing. They're optimizing a different... Well, you know, it's interesting. Now you're getting at why. And so, and I don't think... So now you're saying they're doing intentionally. I don't think they're doing intentionally. But let's... It's an emergent phenomena. Okay. Yeah, yeah, yeah. We'll get to that. We'll get to that. And so, but we wanted to see why. So we see what first as opposed... So because it turns out that we're not the first to observe that legalese is weird, like back to... Nixon had a plain language act in 1970, and Obama had one. And boy, a lot of these... A lot of presidents have said, oh, we've got to simplify legal language, must simplify it. But if you don't know how it's complicated, it's not easy to simplify it. You need to know what it is you're supposed to do before you can fix it. And so you need to cycle linguists to analyze the text and see what's wrong with it before you can fix it. You don't know how to fix it. How am I supposed to fix something? I don't know what's wrong with it. And so what we did was just... That's what we did. We figured out, well, it's okay. We just took a bunch of contracts, had people, and we encoded them for a bunch of features. And so another feature of the people, one of them was the center embedding. And so that is like basically how often a clause would intervene between a subject and a verb, for example. That's one kind of a center embedding of a clause. And turns out they're massively center embedded. So I think in random contracts and in random laws, I think you get about 70% or 70% of sentences have a center embedded clause, which is insanely high. If you go to any other text, it's down to 20% or something. It's so much higher than any control you can think of, including... You think, oh, people think, oh, technical, academic text, no, people don't write center embedded sentences in technical, academic texts. I mean, they do a little bit, but it's on the 20%, 30% realm as opposed to 70. And so there's that. And there's low frequency words. And then people, oh, maybe it's passive. People don't like the passive. Passive, for some reason, the passive voice in English has a bad rap. And I'm not really sure where that comes from. And there is a lot of passive. There's much more passive voice in legalese than there is in other texts. Passive voice accounts for some of the low frequency words. No, no, no, no. Those are separate. Those are separate. Oh, so passive voice sucks. Low frequency word sucks. Well, sucks are different. So these are different. That's a judgment on passive. Yeah, yeah, yeah. Drop the judgment. It's just like, these are frequent. These are things which happen in legalese texts. Then we can ask the dependent measure is like, how well you understand those things with those features. Okay. And so then, and it turns out the passive makes no difference. So it has a zero effect on your comprehension ability, on your recall ability. No, nothing at all. That means no effect. The words matter a little bit. They do low frequency words are going to hurt you in recall and understanding. But what really hurts is the central bedding. That kills you. That is like, that slows people down. That makes them very poor at understanding. That makes them, they can't recall what was said as well, nearly as well. And we did this not only on lay people. We didn't have a lot of lay people. We ran it on a hundred lawyers. We recruited lawyers from a wide range of sort of different levels of law firms and stuff. And they have the same pattern. So they also like, when they did this, I did not know what happened. I thought maybe they could process, they're used to legalese. They think you can process it just as well as it was normal. No, no, they're much better than lay people. So they're much, they can much better recall, much better understanding, but they have the same main effects as lay people, exactly the same. So they also much prefer the non-center. So we constructed non-center embedded versions of each of these. We constructed versions which have higher frequency words in those places. And we did, we un-passivized, we turned them into active versions. The passive active made no difference. The words made a little difference. And the un-center embedding makes big differences in all the populations. Un-center embedding. How hard is that process, by the way? It's not very hard. So I don't question, but how hard is it to detect center embedding? Oh, easy, easy to detect. That's just easy to parse. You just looking at long dependencies? Yeah, yeah. You can just, you can, so there's automatic parsers for English, which are pretty good. And they can detect center embedding. Oh, yeah. Very. Or, I guess, Nestle. Perfectly. Yeah, you learn pretty much. So you're not just looking for long dependencies. You're just literally looking for center embedding. Yeah, we are in this case, in these cases. But long dependencies are, they're highly correlated to this. So like a center embedding is a big bomb you throw inside of a sentence that just blows up the, that makes sure. Yeah. Can I read a sentence for you from these things? Sure. I see. I mean, this is just like one of the things that, this is just terrible. My eyes might glaze over in mid-sentence. No, I understand that. I mean, legally, this is hard. This is a go, because in the event that any payment or benefit by the company, all such payments and benefits, including the payments and benefits under section 3a here of being here and after referred to as a total payments, would be subject to the excise tax, then the cash severance payments shall be reduced. So that's something we pulled from a regular text, from a contract. Wow. And the center embedded bit there is just, for some reason, there's a definition. They throw the definition of what payments and benefits are in between the subject and the verbal. Let's, how about don't do that? Yeah. How about put the definition somewhere else as opposed to in the middle of the sentence? And so that's very, very common, by the way. That's what happens. You just throw your definitions, you use a word, a couple of words, and then you define it, and then you continue the sentence. Like, just don't write like that. And you ask, so then we asked lawyers, we thought, oh, maybe lawyers like this. Lawyers don't like this. They don't like this. They don't want to, they don't want to write like this. They, we asked them to rate materials which are with the same meaning with un-centered bed and center bed, and they much preferred the un-centered bed versions. On the comprehension, on the reading side. Yeah, well, and we asked them, we asked them, would you hire someone who writes like this or this? We asked them all kinds of questions, and they always preferred the less complicated version, all of them. So I don't even think they want it this way. Yeah, but how did it happen? How did it happen? That's a very good question. And the answer is, they still don't know. But I have some theories. Well, our best theory at the moment is that there's actually some kind of a performative meaning in the center embedding and the style which tells you it's legalese. We think that that's the kind of a style which tells you it's legalese. Like that's a reasonable guess. And maybe it's just, so for instance, if you're like, it's like a magic spell. So we kind of call this the magic spell hypothesis. So when you give them, when you tell someone to put a magic spell on someone, what do you do? They, you know, people know what a magic spell is and they do a lot of rhyming. You know, that's kind of what people will tend to do. They'll do rhyming and they'll do sort of like some kind of poetry kind of thing. Abracadabra type of thing. Yeah. And maybe that's, there's a syntactic sort of reflex here of a magic spell which is center embedding. And so that's like, oh, it's trying to like tell you this is like, this is something which is true, which is what the goal of law is, right, is telling you something that we want you to believe as certainly true, right? That's what legal contracts are trying to enforce on you, right? And so maybe that's like a form which has, this is like an abstract, very abstract form, center embedding, which has a, has a, has a meaning associated with it. Well, don't you think there's an incentive for lawyers to generate things that are hard to understand? That was our, one of our working hypotheses. We just couldn't find any evidence of that. No, lawyers also don't understand it. But you're creating space, why you yourself, but I mean, you ask in a communist Soviet Union, the individual members, their self-report is not going to correctly reflect what is broken about the gigantic bureaucracy that leads to Chernobyl or something like this. I think the incentives under which you operate are not always transparent to the members within that system. So like, it just feels like a strange coincidence that like, there is benefit if you just zoom out, look at the system as opposed to asking individual lawyers that making something hard to understand is going to make a lot of people money. Like there's going to, you're going to need a lawyer to figure that out, I guess, from the perspective of the individual, but then that could be the performative aspect. It could be as opposed to the incentive driven to be complicated. It could be performative to where we lawyers speak in this sophisticated way and you regular humans don't understand it, so you need to hire a lawyer. Yeah, I don't know which one it is, but it's suspicious. Suspicious that it's hard to understand and that everybody's eyes glaze over and they don't read. I'm suspicious as well. I'm still suspicious. And I hear what you're saying. It could be kind of a, you know, no individual and even average of individuals, it could just be a few bad apples in a way which are driving the effect in some way. Influential bad apples at the sort of, that everybody looks up to whatever they're like in central figures and how, you know. It turns out, but it is kind of interesting that among our 100 lawyers, they did not share it with us. They really didn't like it. And they weren't better than regular people at comprehending it or they were on average better. But they had the same difference. Exactly, same difference. But they wanted it fixed. And so that gave us hope that because it actually isn't very hard to construct a material which is uncenter embedded and has the same meaning, it's not very hard to do. Just basically in that situation, just putting definitions outside of the subject verb relation in that particular example, and that's kind of, that's pretty general. What they're doing is just throwing stuff in there, which you didn't have to put in there. There's extra words involved. Typically, you may need a few extra words sort of to refer to the things that you're defining outside in some way, because if you only use it in that one sentence, then there's no reason to introduce extra terms. So we might have a few more words, but it'll be easier to understand. So I mean, I have hope that now that maybe we can make legalese less convoluted in this way. So maybe the next president in the United States can, instead of saying generic things, say, I ban center embeddings and make Ted the language czar of the U.S. Eric Martinez is the guy you should really put in there. But center embeddings are the bad thing to have. That's right. So you can get rid of that. That'll do a lot of it. That'll fix a lot. That's fascinating. That is so fascinating. And it is really fascinating on many fronts that humans are just not able to deal with this kind of thing. And that language, because of that, evolved in the way you did. It's fascinating. So one of the mathematical formulations you have when talking about languages communication is, let's say, do you have noisy channels? What's a noisy channel? So that's about communication. And so this is going back to Shannon. So Claude Shannon was a student at MIT in the 40s. And so he wrote this very influential piece of work about communication theory or information theory. And he was interested in human language, actually. He was interested in this problem of communication, of getting a message from my head to your head. And he was concerned or interested in what was a robust way to do that. And so that assuming we both speak the same language, we both already speak English, whatever the language is, we speak that. What is a way that I can say the language so that it's most likely to get the signal that I want to you? And then the problem there in the communication is the noisy channel. There's a lot of noise in the system. I don't speak perfectly. I make errors. That's noise. There's background noise. You know that. Literal background noise. There is like white noise in the background or some other kind of noise. There's some speaking going on that you're at a party. That's background noise. You try to hear someone. It's hard to understand them because there's all this other stuff going on in the background. And then there's noise on the receiver side so that you have some problem maybe understanding me for stuff that's just internal to you in some way. So you've got some other problems, whatever, with understanding for whatever reasons. Maybe you've had too much to drink. Who knows why you're not able to pay attention to the signal? So that's the noisy channel. And so that language, if it's a communication system, we are trying to optimize in some sense the passing of the message from one side to the other. And so, I mean, one idea is that maybe aspects of word order, for example, might have optimized in some way to make language a little more easy to be passed from speaker to listener. And so Shannon's the guy that did the stuff way back in the forties. It's very interesting. Historically, he was interested in working in linguistics. He was in MIT. And this was his master's thesis of all things. It's crazy how much he did for his master's thesis. In 1948, I think, or 49 or something. And he wanted to keep working in language. And it just wasn't a popular communication as a reason, a source for what language was wasn't popular at the time. So Chomsky was becoming, it was moving in there. And he just wasn't able to get a handle there, I think. And so he moved to Bell Haps and worked on communication from a mathematical point of view and did all kinds of amazing work. And so he's just more on the signal side versus like the language side. It would have been interesting to see if you proceed the language side. That's really interesting. Yeah, he was interested in that. His examples in the forties are kind of like, they're very language like things. We can kind of show that there's a noisy channel process going on in when you're listening to me, you can often sort of guess what I meant by what you think I meant given what I said. And I mean, with respect to sort of why language looks the way it does, we might, there might be sort of, as I alluded to, there might be ways in which word order is somewhat optimized for, because of the noisy channel in some way. I mean, that's really cool to sort of model if you don't hear certain parts of a sentence or have some probability of missing that part, like how do you construct a language that's resilient to that? That's somewhat robust to that. Yeah, that's the idea. And then you're kind of saying like the word order and the syntax of language, the dependency length are all helpful. Yeah, well, the dependency length is really about memory, right? I think that's like about sort of what's easier or harder to produce in some way. And these other ideas are about sort of robustness to communication. So the problem of potential loss of loss of signal due to noise. It's so that there may be aspects of word order, which is somewhat optimized for that. And, you know, we have this one guess in that, and these are kind of just so stories. I have to be, you know, pretty frank, they're not like, I can't show this is true. All we can do is like look at the current languages of the world. This is like, we can't sort of see how languages change or anything because we've got these snapshots of a few, you know, hundred or a few thousand languages. We don't really, we can't do the right kinds of modifications to test these things experimentally. And so, you know, so just take this with a grain of salt, okay, from here, this stuff. The dependency stuff I can, I'm much more solid on. I'm like, here's what the lengths are, and here's what's hard. Here's what's easy. And this is a reasonable structure. I think I'm pretty reasonable. Here's like, why, you know, why does the word order look the way it does? We're now into shaky territory, but it's kind of cool. But we're talking about, just to be clear, we're talking about maybe just actually the sounds of communication. Like you and I are sitting in the bar, it's very loud, and you model with a noisy channel, the loudness, the noise, and we have the signal that's coming across. And you're saying word order might have something to do with optimizing that. Yes. Yes. There's a presence of noise. Yes. Yes. Yeah. It's really interesting. I mean, to me, it's interesting how much you can load into the noisy channel. Like how much can you bake in? You said like, you know, cognitive load on the receiver end. We think that those are, there's three, at least three different kinds of things going on there. And we probably don't want to treat them all as the same. Sure. And so I think that you, you know, the right model, a better model of a noisy channel would have three different sources of noise, which are background noise, speaker, inherent noise, and listener, inherent noise. And those are not, those are all different things. Sure. But then underneath it, there's a million other subsets of like what? Oh yeah. That's true. On the receiving, I mean, I just mentioned cognitive load on both sides. Then there's like speech impediments or just everything. World view, I mean, on the meeting, we start to creep into the meeting realm of like, we have different world views. Well, how about just form still though? Like just what language you know? Like, so how well you know the language? And so if it's second language for you versus first language, and how maybe what other languages you know, these are still just form stuff. And that's like potentially very informative. And, you know, how old you are, these things probably matter, right? So like a child learning a language is, is a, you know, as a noisy representation of English grammar, you know, depending on how old they are. So maybe when they're six, they're perfectly formed. But you mentioned one of the things is like a way to measure the language is learning problems. So like, what's the correlation between everything we've been talking about and how easy it is to learn a language? So is like a short dependencies correlated to ability to learn a language? Is there some kind of, or like the dependency grammars, there's some kind of connection there? How easy it is to learn? Yeah, well, all the languages in the world's language, none is right now, we know is any better than any other with respect to sort of optimizing dependency lengths, for example, they're all kind of do it, do it well, they all keep low. It's so that I think of every human language is some kind of an opposite sort of an optimization problem, a complex optimization problem to this communication problem. And so they've, like they've solved it, you know, they're just sort of noisy solutions to this problem of communication. There's just so many ways you can do this. So they're not optimized for learning, they're probably less for communication. And learning. So yes, one of the factors which, yeah, so learning is messing this up a bit. And so, so for example, if it were just about minimizing dependency lengths, and that was all that matters, you know, then we, you know, so then we might find grammars which didn't have regularity in their rules. But languages always have regularity in their rules. So what I mean by that is that if I wanted to say something to you in the optimal way to say it was, it would really matter to me, all that mattered was keeping the dependencies as close together as possible. Then I would have a very lack set of free structure or dependency rule. It wouldn't have very many of those. I would have very little of that. And I would just put the words as close to the things that refer to the things that are connected right beside each other. But we don't do that. Like there are, like there are word order rules, right? So they're very, and depending on the language, they're more and less strict, right? So you speak Russian, they're less strict than English. English is very rigid word order rules. We order things in a very particular way. And so why do we do that? Like that's probably not about communication. That's probably about learning. I mean, then we're talking about learning. It's probably easier to learn regular things, things which are very predictable and easy to, so that's probably about learning is our guess, because that can't be about communication. Can it be just noise? Can it be just the messiness of the development of a language? Well, if it were just a communication, then we should have languages which have very, very free word order. And we don't have that. We have free error, but not free. Like there's always- Well, no, but what I mean by noise is like cultural, like sticky cultural things, like the way, the way you communicate, just there's a stickiness to it. That it's an imperfect, it's a noisy, it's stochastic. The function over which you're optimizing is very noisy. So, because I don't, it feels weird to say that learning is part of the objective function, because some languages are way harder to learn than others, right? Or is that, that's not true. That's interesting. I mean, that's the public perception, right? Yes, that's true for a second language. For a second language. But that depends on what you started with, right? So, it really depends on how close that second language is to the first language you've got. And so, yes, it's very, very hard to learn Arabic if you've started with English, or it's harder to learn Japanese, or if you've started with Chinese, I think is the worst. There's like Defense Language Institute in the United States has like a list of how hard it is to learn what language from English. I think Chinese is the worst. But that's the second language. You're saying babies don't care? No. There's no evidence that there's anything harder, easier, but any baby, any language learned, like three or four, they speak that language. And so, there's no evidence of anything harder, easier, but any human language. They're all kind of equal. To what degree is language, this is returning to Chomsky a little bit, is innate. You said that for Chomsky, he used the idea that language is, some aspects of language are in need to explain away certain things that are observed. How much are we born with language at the core of our mind, brain? I mean, I, you know, the answer is I don't know, of course, but the, I mean, I like to, I'm an engineer at heart, I guess, and I sort of think it's fine to postulate that a lot of it's learned. And so, I'm guessing that a lot of it's learned. So, I think the reason Chomsky went with innateness is because he hypothesized movement in his grammar. He was interested in grammar and movement's hard to learn. I think he's right. Movement is a hard thing to learn, to learn these two things together and how they interact. And there's like a lot of ways in which you might generate exactly the same sentences. And it's like really hard. And so, he's like, oh, I guess it's learned. Sorry, I guess it's not learned, it's innate. And if you just throw out the movement and just think about that in a different way, you know, then you get some messiness. But the messiness is human language, which it actually fits better. That messiness isn't a problem. It's actually, it's a valuable asset of the theory. And so, I think I don't really see a reason to postulate much innate structure. And that's kind of why I think these large language models are learning so well, is because I think you can learn the form, the forms of human language from the input. I think that's like, it's likely to be true. So that part of the brain that lights up when you're doing all the comprehension, that could be learned. That could be just, you don't need, you don't need any. It doesn't have to be innate. So, like lots of stuff is modular in the brain that's learned. It doesn't have to, you know, so there's something called the visual word form area in the back. And so, it's in the back of your head, near the, you know, the visual cortex, okay? And that is very specialized language, sorry, very specialized brain area, which does visual word processing if you read, if you're a reader, okay? If you don't read, you don't have it, okay? Guess what? You spend some time learning to read and you develop that brain area, which does exactly that. And so, these, the modularization is not evidence for innateness. So, the modularization of a language area doesn't mean we're born with it. We could have easily learned that. We might have been born with it. We just don't know at this point. We might very well have been born with this left lateralized area. I mean, there's like a lot of other interesting components here, features of this kind of argument. So, some people get a stroke or something goes really wrong on the left side, where the left, where the language area would be. And that, and that isn't there. It's not, not available. And it develops just fine on the right. So, it's no lie. So, it's not about the left. It goes to the left. Like, this is a very interesting question. It's like, why is the, why are any of the brain areas the way that they are? And how, how, how did they come to be that way? And, you know, there's these natural experiments, which happen where people get these, you know, strange events in their brains at very young ages, which wipe out sections of their brain and, and they behave totally normally and no one knows anything was wrong. And we find out later, because they happen to be accidentally scanned for some reason. It's like, what, what happened to your left hemisphere? It's missing. There's not many people who have missed their whole left hemisphere, but they'll be missing some other section of their left or their right. And they behave absolutely normally, we would never know. So, that's like a very interesting, you know, current research. You know, this is another project that this person in Federico is working on. She's got all these people contacting her because she's scanned some people who have been missing sections, one person missing, missed a section of her brain and was scanned in her lab. And she, and she happened to be a writer for the New York Times. And there was an article in the New York Times about, about the, just about the scanning procedure and, and about what might be learned about by sort of the general process of MRI and language and that's her language. And, and because she's writing for the New York Times, then she, all these people started writing to her who also have similar, similar kinds of deficits because they've been, you know, accidentally, you know, to scan for some reason and, and found out they're missing some section. And they, they volunteer to be scanned. These are natural experiments. Natural experiments. They're kind of messy, but natural experiments kind of cool. She calls them interesting brains. The first few hours, days, months of human life are fascinating. It's like, well, inside the womb, actually, like that development, that machinery, whatever that is, seems to create powerful humans that are able to speak, comprehend, think, all that kind of stuff, no matter what happened, not no matter what, but robust to the different ways that the brain might be damaged and so on. That's really, that's really interesting. But what would Chomsky say about the fact, the thing you're saying now, that language is, seems to be happening separate from thought? Because as far as I understand, maybe you can correct me, he thought that language underpins. Yeah, he thinks so. I don't know what he'd say. He would be surprised because for him, the idea is that language is a sort of the foundation of thought. That's right. Absolutely. And it's pretty mind blowing to think that it could be completely separate from thought. That's right. But so, you know, he's basically a philosopher, philosopher of language in a way, thinking about these things. It's a fine thought. You can't test it in his methods. You can't do a thought experiment to figure that out. You need a scanner. You need brain damage people. You need something. You need ways to measure that. And that's what, you know, fMRI offers as a, and, you know, patients are a little messier. fMRI is pretty unambiguous, I'd say. It's like very unambiguous. There's no way to say that the language network is doing any of these tasks. There's, like, you should look at those data. It's like, there's no chance that you can say that those networks are overlapping. They're not overlapping. They're just, like, completely different. And so, you know, so the, you know, you can always make, you know, it's only two people. It's four people or something for the patients. And there's something special about them we don't know. But these are just random people. And with lots of them, and you find always the same effects. And it's very robust, I'd say. What's a fessing effect? What's the, you mentioned Bolivia. What's the connection between culture and language? You've also mentioned that, you know, much of our study of language comes from W-E-I-R-D, weird people, western educated, industrialized, rich, and democratic. So when you study, like, remote cultures, such as around the Amazon jungle, what can you learn about language? So that term weird is from Joe Henrich. He's at Harvard. He's a Harvard evolutionary biologist. And so he works on lots of different topics. And he basically was pushing that observation that we should be careful about the inferences we want to make when we're talking in psychology or, yeah, mostly in psychology, I guess, about humans if we're talking about, undergrads at MIT and Harvard. Those aren't the same, right? These aren't the same things. And so if you want to make inferences about language, for instance, there's a lot of very, a lot of other kinds of languages in the world, then English and French and Chinese, you know. And so maybe for language, we care about how culture, because cultures can be very, I mean, of course, English and Chinese cultures are very different. But you know, hunter-gatherers are much more different in some ways. And so if culture hasn't affected what language is, then we kind of want to look there as well as looking. It's not like the industrialized cultures aren't interesting. Of course they are. But we want to look at non-industrialized cultures as well. And so I've worked with two. I've worked with Chimani, which are in Bolivia and Amazon, both in the Amazon in these cases. And there are so-called farmer foragers, which is not hunter-gatherers. It's sort of one up from hunter-gatherers in that they do a little bit of farming as well. A lot of hunting as well, but a little bit of farming. And the kind of farming they do is the kind of farming that I might do if I ever were to grow like tomatoes or something in my backyard. So it's not like big field farming. It's just a farming for a family, a few things you do that. And so that's the kind of farming they do. And the other group I've worked with are the Pirah\u00e1, which are also in the Amazon and happen to be in Brazil. And that's with a guy called Dan Everett, who is a linguist, anthropologist, who actually lived and worked in the... I mean, he was a missionary, actually, initially, back in the 70s, working with trying to translate languages so they could teach them the Bible, teach them Christianity. What can you say about that? Yeah, so the two groups I've worked with, the Cimani and the Pirah\u00e1, are both isolate languages, meaning there's no known connected languages at all. They're just like on their own. Oh, cool. Yeah, there's a lot of those. And most of the isolates occur in the Amazon or in Papua New Guinea, in these places where the world has sort of stayed still for long enough. And so there aren't earthquakes. There aren't... Well, certainly no earthquakes in the Amazon jungle. And the climate isn't bad. So you don't have droughts. And so in Africa, you've got a lot of moving of people because there's drought problems. And so they get a lot of language contact when people have to... You've got to move because you've got no water, then you've got to get going. And then you run into contact with other tribes, other groups. In the Amazon, that's not the case. And so people can stay there for hundreds and hundreds and probably thousands of years, I guess. And so these groups have... The Cimani and the Pirah\u00e1 are both isolates in that. And I guess they've just lived there for ages and ages with minimal contact with other outside groups. And so, I mean, I'm interested in them because they are... In these cases, I'm interested in their words. I would love to study their syntax, their orders of words, but I'm mostly just interested in how languages are connected to their cultures in this way. And so with the Pirah\u00e1, sort of most interesting, I was working on number there, number information. And so the basic idea is I think language is invented. That's what I get from the words here is that I think language is invented. We talked about color earlier. It's the same idea, so that what you need to talk about with someone else is what you're going to invent words for. And so we invent labels for colors that I need, not that I can see, but that things I need to tell you about so that I can get objects from you or get you to give me the right objects. And I just don't need a word for teal or a word for aquamarine in the Amazon jungle, for the most part, because I don't have two things which differ on those colors. I just don't have that. And so numbers are really another fascinating source of information here where you might... Naively, I certainly thought that all humans would have words for exact counting and the Pirah\u00e1 don't. So they don't have any words for even one. There's not a word for one in their language. And so there's certainly not a word for two, three, or four. So that kind of blows people's minds off. Yeah, that is blowing my mind. That's pretty weird. How are you going to ask, I want two of those? You just don't. And so that's just not a thing you can possibly ask in the Pirah\u00e1. It's not possible. That is, there's no words for that. So here's how we found this out. So it was thought to be a one, two, many language. There are three words, four quantifiers for sets. And people had thought that those meant one, two, and many. But what they really mean is few, some, and many. Many is correct. It's few, some, and many. And so the way we figured this out, and this is kind of cool, is that we gave people... We had a set of objects. These are having to be spools of thread. It doesn't really matter what they are. Identical objects. And when I sort of start off here, I just give you one of those and say, what's that? Okay, so you're a Pirah\u00e1 speaker and you tell me what it is. And then I give you two and say, what's that? And nothing's changing in this set except for the number. And then I just ask you to label these things. We just do this for a bunch of different people. And frankly, it's a... I did this task. And it's a little bit weird. So they say the word that we thought was one, it's few, but for the first one. And then maybe they say few or maybe they say some for the second. And then for the third or the fourth, they start using the word many for the set. And then five, six, seven, eight. I go all the way to ten. And it's always the same word. And they look at me like I'm stupid because they told me what the word was for six, seven, eight. And I'm going to continue asking them at nine and ten. I'm sorry. They understand that I want to know their language. That's the point of the task is like I'm trying to learn their language. And so that's okay. But it does seem like I'm a little slow because they already told me what the word for many was, five, six, seven. And I keep asking. So it's a little funny to do this task over and over. We did this with a guy called... Dan was our translator. He's the only one who really speaks Piazza fluently. He's a good bilingual for a bunch of languages, but also English and Piazza. And then a guy called Mike Frank was also a student with me down there. He and I did these things. And so you do that. Okay. And everyone does the same thing. We asked like 10 people. And they all do exactly the same labeling for one up. And then we just do the same thing down on random order. Actually, we do some of them up, some of them down first. Okay. And so we do, instead of one to 10, we do 10 down to one. And so I give them 10, nine, eight. They start saying the word for some. And then when you get to four, everyone is saying the word for few, which we thought was one. So the context determined what that quantifier they used was. So it's not a count word. They're not count words. They're just approximate words. And they're going to be noisy when you interview a bunch of people, what the definition of few, and there's going to be a threshold in the context. Yeah, I don't know what that means. That's going to be 10 on the context. I think it's true in English too, right? If you ask an English person what a few is, I mean, that's going to depend completely on the context. And it might actually be at first hard to discover. Because for a lot of people, the jump from one to two will be few. Right? So it's a jump. Yeah, it might be. It might still be there. Yeah. I mean, that's fascinating. That's fascinating that numbers don't present themselves. Yeah. So the words aren't there. And then, and so then we do these other things. Well, if they don't have the words, can they do exact matching kinds of tasks? Can they even do those tasks? And the answer is sort of yes and no. And so yes, they can do them. So here's the tasks that we did. We put out those spools of thread again. Okay. So maybe I put like three out here. And then we gave them some objects. And those happen to be uninflated red balloons. It doesn't really matter what they are. It's just a bunch of exactly the same thing. And it was easy to put down right next to these spools of thread. Okay. And so then I put out three of these. And your task was to just put one against each of my three things. And they could do that perfectly. So I mean, I would actually do that. It was a very easy task to explain to them because I have, I did this with this guy, Mike Frank, and he would be my, I'd be the experimenter telling him to do this and showing him to do this. And then we just like, just do what he did. You'll copy him. All we had to, I didn't have to speak to him, except for know what copy him, like do what he did is like all we had to be able to say. And then they would do that just perfectly. And it's always moving up. We do some sort of random number of items up to 10. And they basically do perfectly on that. They never get that wrong. I mean, that's not a counting task, right? That is just a match. You just put one against them. It doesn't matter how many, I don't need to know how many there are there to do that correctly. And they would make mistakes, but very, very few and no more than MIT undergrads. Just going to say, like there's no, these are low stakes. So, you know, you make mistakes. Counting is not required to complete the matching task. That's right. Not at all. Okay. And so, and so that's our control. And this guy had gone down there before and said that they couldn't do this task, but I just don't know what he did wrong there because they can do this task perfectly well. And, you know, I can train my dog to do this task. So, of course, they can do this task. And so, you know, it's not a hard task. But the other task that was sort of more interesting is like, so then we do a bunch of tasks where you need some way to encode the set. So, like one of them is just a, I just put a opaque sheet in front of the things I put down a bunch of set of these things and I put an opaque sheet down. And so you can't see them anymore. And I tell you, do the same thing you were doing before, right? You know, and it's easy if it's two or three, it's very easy. But if I don't have the words for eight, it's a little harder, like, maybe, you know, with practice, well, no. Because you have to count. For us, it's easy because we just, we just count them. It's just so easy to count them. But, but they don't, they can't count them because they don't count. They don't have words for this thing. And so, they would do approximate. It's totally fascinating. So, they would get them approximately right, you know, after four or five, you know, because you can, basically, you always get four right, three or four, that looks, that's something we can visually see. But, but after that, you kind of have, it's an approximate number. And so, then, and there's a bunch of tasks we did, and they all failed as, I mean, failed. They did approximate after five on all those tasks. And it kind of shows that the words, you kind of need the words, you know, to be able to do these, these kinds of tasks. This is a little bit of a chicken and egg thing there. Because if you don't have the words, then maybe they'll limit you in the kind of, like a little baby Einstein there, won't be able to come up with a counting task. You know what I mean? Like, the ability to count enables you to come up with interesting things, probably. So, yes, you develop counting because you need it. But then, once you have counting, you can probably come up with a bunch of different inventions, like how to, I don't know, what kind of thing they do matching really well for building purposes, building some kind of hut or something like this. So, it's interesting that language is a, a limiter on what you're able to do. Yeah, here's language is just, is the words, here is the words, like the words for exact count is the limiting factor here. They just don't have them. Yeah. Well, that's what I mean. That limit is also a limit on the society of what they're able to build. That's going to be true. Yeah. So, it's problem, I mean, we don't know, this is one of those problems with the snapshot of just current languages is that we don't know what causes a culture to discover slash invent accounting system. But the hypothesis is the guess out there is something to do with farming. So, if you have a bunch of goats and you want to keep track of them, and you have saved 17 goats and you go to bed at night and you get up in the morning, boy, it's easier to have a count system to do that. You know, that's an abstract abstraction over a set. So, they don't have, like, people often ask me when I talk to them about this kind of work, and they say, well, don't these, Peter, huh, don't they have kids? Don't they have a lot of children? I'm like, yeah, they have a lot of children. And they do, they often have families of three or four or five kids. And they go, well, don't they need the numbers to keep track of their kids? And I always ask the person who says this, like, do you have children? And the answer is always no, because that's not how you keep track of your kids. You care about their identities. It's very important to me when I go, I think I have five children. It's, it's, it doesn't matter which, yeah, it matters which five. It's like, if you replaced one with someone else, I would, I would care. A goat maybe not, right? That's the kind of point. It's an abstraction, something that looks very similar to the one wouldn't matter to me, probably. But if you care about goats, you're going to know them actually individually also. Yeah, you will. I mean, cows and goats, if there's a source of food and milk and all that kind of stuff, you're going to actually do it right there. But I'm saying it is an abstraction such that you don't have to care about their identities to do this thing fast. That's, that's the hypothesis, not mine. From anthropologists are guessing about where words for counting came from is from farming, maybe. Yeah. Do you have a sense why universal languages like Esperanto have not taken off? Like, why do we have all these different languages? Well, my guess is the function of a language is to do something in a community. And I mean, unless there's some function to that language in the community, it's not going to survive. It's not going to be useful. So here's a great example. So what I'm like, language death is super common. Okay. Languages are dying all around the world. And here's why they're dying. And it's like, yeah, I see this in, you know, it's not happening right now in either the Chimane or the, or the Piedoha, but it probably will. And so there's a neighboring group called Mostitan, which is, I said that it's a isolates. Actually, there's a dual. There's two of them. Okay. So it's actually, there's two languages, which are really close, which are Mostitan and Chimane, which are unrelated to anything else. And Mostitan is unlike Chimane in that it has a lot of contact with Spanish and it's dying. So that language is dying. The reason it's dying is there's not a lot of value for the local people in their native language. So there's much more value in knowing Spanish, like because they want to feed their families. And how do you feed your family? You learn Spanish, so you can make money, so you can get a job and do these things. And then you can, and then you make money. And so they want Spanish things they want. And so, so Mostitan is in danger and is dying. And that's normal. And so basically the problem is that people, the reason we learn languages to communicate, and we need to, we use it to make money and to do whatever it is to feed our families. And if that's not happening, then it won't take off. It's not like a game or something. This is like something we use. Like, why is English so popular? It's not because it's an easy language to learn. Maybe it is. I don't really know. But that's not why it's popular. But because the United States is a gigantic economy. Yeah. It's big economies that do this. It's all it is. It's all about money. And that's what, and so there's a motivation to learn Mandarin. There's a motivation to learn Spanish. There's a motivation to learn English. These languages are very valuable to know because there's so, so many speakers all over the world. There's less of a value economically. It's like kind of what drives this. It's not a, but it's not a, you know, it's not just for fun. I mean, there are these groups that do want to learn language just for language's sake. And then there's something, you know, to that, but those are rare. Those are rarities in general. Those are a few small groups that do that. Not most people don't do that. Well, if that was the primary driver, then everybody was speaking English or speaking one language. There's also a tension. That's happening. And that, well, we're moving towards fewer and fewer languages. We are. I wonder, you're right. Maybe, maybe, you know, this is slow, but maybe that's where we're moving. But there is a tension. You're saying a language that the fringes. But if you look at geopolitics and superpowers, it does seem that there's another thing in tension, which is a language is a national identity sometimes. For a certain nation. I mean, that's the, the war in Ukraine, language, Ukrainian language is a symbol of that war in many ways, like a country fighting for its own identity. So it's not merely the convenience. I mean, those two things that are a tension is the, the convenience of trade and the economics and be able to communicate with neighboring countries and trade more efficiently with neighboring countries, all that kind of stuff, but also identity of the group. That's right. I completely agree. Because language is the way, for every community, like dialects that emerge are a kind of identity for people. And sometimes a way for people to say FU to the more powerful people. That's interesting. So in that way, language can't be used as that tool. Yeah. I completely agree. And there's a lot of work to try to create that identity. So people want to do that. Speak, you know, as a cognitive scientist and language expert, I hope that continues because I don't want languages to die. I want languages to survive because, because they're so interesting for, for so many reasons. But I mean, I find them fascinating just for the language part, but I think they, you know, there's a lot of connections to culture as well, which is also very important. Do you have hope for machine translation that can break down the barriers of language? So while all these different diverse languages exist, I guess there's many ways of asking this question, but basically how hard is it to translate in an automated way from one language to another? There's, there's going to be cases where it's going to be really hard, right? So there are concepts that are in one language and not in another. Like the most extreme kinds of cases are these cases of number information. So exactly, like good luck translating a lot of English into it's just impossible. There's no way to do it because there are no words for these concepts that we're talking about. There's probably the flip side, right? There's probably stuff in which is going to be hard to translate into English on the other side. And so I just don't know what those concepts are. I mean, you know, the space, the world space is a little, is different from my world space. And so I don't know what like, so that the things they talk about things are, you know, it's going to have to do with their life as opposed to, you know, my industrial life, which is going to be different. And so there's going to be problems like that always. You know, there's like, it's not, maybe it's not so bad in the case of some of these spaces, and maybe it's going to be harder than others. And so it's pretty bad in number. It's like, you know, extreme, I'd say, in the number space, you know, exact number space, but in the color dimension, right? So that's not so bad. There's, I mean, but it's a problem that, that you don't have ways to talk about the concepts. There might be entire concepts that are missing. So to you, it's more about the space of concept versus the space of form. Like form, you can probably map. Yes. Yeah. But so you were talking earlier about translation and about how translations, you know, there's good and bad translations. I mean, now we're talking about translations of form, right? So what makes a writing good, right? It's not just the content. It's, you know, it's how it's written. And translating that, I, you know, I, you know, that's, that sounds difficult. We should, we should say that there is like, I don't, it has a day to say meaning, but there's a music and a rhythm to the form. When you look at the broad picture, like the Fritz Wietzi and Dostoyevsky and Tolstoy, or Hemingway Bukowski, James Joyce, like I mentioned, there's a beat to it. There's an edge to it that it's like, is in the form. We can probably get measures of those. Yeah. I don't know. I'm optimistic that we could get measures of those things. And so maybe that's translatable. I don't know. I don't know though. I haven't worked on that. I would love to see. That sounds totally fascinating. Translation to Hemingway. I mean, Hemingway is probably the lowest, I would love to see different authors, but the average per sentence dependency length for Hemingway is probably the shortest. That's your sense, huh? It's simple sentences with short, yeah, yeah, yeah, yeah. I mean, that's when, if you have really long sentences, even if they don't have center of writing, like. They can have longer connections. Yeah. They can have longer connections. They don't have to, right? You can have a long, long sentence with a bunch of local words. Yeah. Yeah. But it's, but it is much more likely to have the possibility of long dependencies with long sentences. Yeah. I met a guy named Azar Askin who does a lot of cool stuff. Really brilliant. He works with Tristan Harris and a bunch of stuff. But he was talking to me about communicating with animals. He co-founded Earth Species Project, where you're trying to find the common language between whales, crows, and humans. And he was saying that there is a lot of promising work that even though the signals are very different. Right. Like the actual, like, if you have embeddings of the languages, they're actually trying to communicate similar type things. Is there something you can comment on that? Like where, is there promise to that? And everything you've seen in different cultures, especially like remote cultures, that this is a possibility? No. They can talk to whales. I would say yes. I think it's not crazy at all. I think it's quite reasonable. There's this sort of weird view, well, odd view, I think, that to think that human language is somehow special. I mean, it is, maybe it is. We can certainly do more than any of the other species. And maybe our language system is part of that. It's possible. But people do have often talked about how human, like Chomsky, in fact, has talked about how human language has this compositionality thing that he thinks is sort of key in language. And the problem with that argument is he doesn't speak whale. And he doesn't speak crow, and he doesn't speak monkey. They say things like, well, they're making a bunch of grunts and squeaks. And the reasoning is like, that's bad reasoning. I'm pretty sure if you asked a whale what we're saying, they'd say, well, I'm making a bunch of weird noises. Exactly. And so it's like, this is a very odd reasoning to be making that human language is special because we're the only ones who have human language. I'm well, we don't know what those other, we just don't, we can't talk to them yet. And so there are probably a signal in there. And it might very well be something complicated, like human language. I mean, sure, with a small brain, in lower species, there's probably not a very good communication system. But in these higher species where you have what seems to be abilities to communicate something, there might very well be a lot more signal there than we're than we might have otherwise thought. But, but also if we have a lot of intellectual humility here, as somebody formerly from MIT, Neri Oxman, who I admire very much, has talked a lot about, has worked on communicating with plants. So like, yes, the signal there is even less than we're like, it's not out of the realm of possibility that all nature has a way of communicating. And it's a very different language, but they do develop a kind of language through the chemistry, through some way of communicating with each other. And if you have enough humility about that possibility, I think you can, I think it would be a very interesting in a few decades, maybe centuries, hopefully not a humbling possibility of being able to communicate not just between humans effectively, but between all of living things on earth. Well, I mean, I think some of them are not going to have much interesting to say, we don't know. We certainly don't know. I think if we're humble, there could be some interesting trees out there. Well, they're probably talking to other trees, right? They're not talking to us. And so to the extent they're talking, they're saying something interesting to some other, you know, conspecific as opposed to us, right? And so there probably is, there may be some signal there. So there are people out there, actually it's pretty common to say that human language is special and different from any other animal communication system. And I just don't think the evidence is there for that claim. I think it's not obvious. We just don't know, because we don't speak these other communication systems until we get better. You know, I do think there's, there are people working on that, as you pointed out, though, people working on whale speak, for instance, like that's really fascinating. Let me ask you a wild out there sci-fi question. If we make contact with an intelligent alien civilization and you get to meet them, how hard do you think, like how surprised would you be about their way of communicating? Do you think it would be recognizable? Maybe there's some parallels here when you go to the remote tribes. I mean, I would want Dan Everett with me. He is like amazing at learning foreign languages. And so he like, this is an amazing feat, right, to be able to go, this is a language, which has no translators before him. I mean, there were, he was a mystery, well, there was a guy that had been there before, but he wasn't very good. And so he learned the language far better than anyone else had learned before him. He's like good at, he's just a, he's a very social person. I think that's a big part of it is being able to interact. So I don't know, it kind of depends on these, this species from outer space, how much they want to talk to us. Is there something you can say about the process he follows? Like what, how do you show up to a tribe and socialize? I mean, I guess colors and counting is one of the most basic things to figure out. Yeah, you start that. You actually start with like objects and just say, you know, just throw a stick down and say stick. And then you say, what do you call this? And then they'll say the word, whatever. And he says, the standard thing to do is to throw two sticks at two sticks. And then, you know, he learned pretty quick that there weren't any count words in this language because they didn't know this wasn't interesting. I mean, it was kind of weird. They'd say some or something, the same word over and over again. And so, but that is a standard thing. You just like try to, but you have to be pretty out there socially, like willing to talk to random people, which these are, you know, really very different people from you. And he was, and he's, he's very social. And so I think that's a big part of this is like, that's how, you know, a lot of people know a lot of languages that they're willing to talk to other people. That's a tough one. We just show up knowing nothing. Yeah. Oh, God. It's beautiful that humans are able to connect in that way. Yeah. Yeah. You've had an incredible career exploring this fascinating topic. What advice would you give to young people about how to have a career like that or a life that they can be proud of? When you see something interesting, just go and do it. Like I do, I do that. Like that's something I do, which is kind of unusual for most people. So like when I saw the, like if Piedoha was available to go and visit, I was like, yes, yes, I'll go. And then when we couldn't go back, we had some trouble with the Brazilian government. There's some corrupt people there. It was very difficult to get, go back in there. And so I was like, all right, I got to find another group. And so we searched around and we were able to find the, because I wanted to keep working on this kind of problem. And so we found the Chimani and just go there. I didn't really have, we didn't have contact. We had a little bit of contact and brought someone. And that was, you know, we just, just kind of just try things. I say it's like, a lot of that's just like ambition, just try to do something that other people haven't done. Just give it a shot is what I, I mean, I do that all the time. I love it. And I love the fact that your pursuit of fun has landed you here talking to me. This was an incredible conversation that you're, you're, you're just a fascinating human being. Thank you for taking a journey through human language with me today. This is awesome. Thank you very much. It's been pleasure. Thanks for listening to this conversation with Edward Gibson. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Wittgenstein. The limits of my language mean the limits of my world. Thank you for listening and hope to see you next time.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.84, "text": " Naively, I certainly thought that all humans would have words for exact counting.", "tokens": [50364, 6056, 3413, 11, 286, 3297, 1194, 300, 439, 6255, 576, 362, 2283, 337, 1900, 13251, 13, 50656], "temperature": 0.0, "avg_logprob": -0.15705506251408505, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.03668307885527611}, {"id": 1, "seek": 0, "start": 6.96, "end": 12.48, "text": " And the Piroha don't, okay? So they don't have any words for even one. There's not a word for", "tokens": [50712, 400, 264, 430, 5182, 1641, 500, 380, 11, 1392, 30, 407, 436, 500, 380, 362, 604, 2283, 337, 754, 472, 13, 821, 311, 406, 257, 1349, 337, 50988], "temperature": 0.0, "avg_logprob": -0.15705506251408505, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.03668307885527611}, {"id": 2, "seek": 0, "start": 12.48, "end": 17.12, "text": " one in their language. And so there's certainly not a word for two, three, or four. So that kind", "tokens": [50988, 472, 294, 641, 2856, 13, 400, 370, 456, 311, 3297, 406, 257, 1349, 337, 732, 11, 1045, 11, 420, 1451, 13, 407, 300, 733, 51220], "temperature": 0.0, "avg_logprob": -0.15705506251408505, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.03668307885527611}, {"id": 3, "seek": 0, "start": 17.12, "end": 21.76, "text": " of blows people's minds off. Yeah, that's blowing my mind. That's pretty weird. How are you going to", "tokens": [51220, 295, 18458, 561, 311, 9634, 766, 13, 865, 11, 300, 311, 15068, 452, 1575, 13, 663, 311, 1238, 3657, 13, 1012, 366, 291, 516, 281, 51452], "temperature": 0.0, "avg_logprob": -0.15705506251408505, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.03668307885527611}, {"id": 4, "seek": 0, "start": 21.76, "end": 27.28, "text": " ask? I want two of those. You just don't. And so that's just not a thing you can possibly ask in", "tokens": [51452, 1029, 30, 286, 528, 732, 295, 729, 13, 509, 445, 500, 380, 13, 400, 370, 300, 311, 445, 406, 257, 551, 291, 393, 6264, 1029, 294, 51728], "temperature": 0.0, "avg_logprob": -0.15705506251408505, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.03668307885527611}, {"id": 5, "seek": 2728, "start": 27.28, "end": 29.84, "text": " the Piroha. It's not possible. That is, there's no words for that.", "tokens": [50364, 264, 430, 5182, 1641, 13, 467, 311, 406, 1944, 13, 663, 307, 11, 456, 311, 572, 2283, 337, 300, 13, 50492], "temperature": 0.0, "avg_logprob": -0.14091201262040573, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.0024309465661644936}, {"id": 6, "seek": 2728, "start": 32.160000000000004, "end": 38.24, "text": " The following is a conversation with Edward Gibson, or Ted, as everybody calls him. He is a", "tokens": [50608, 440, 3480, 307, 257, 3761, 365, 18456, 42250, 11, 420, 14985, 11, 382, 2201, 5498, 796, 13, 634, 307, 257, 50912], "temperature": 0.0, "avg_logprob": -0.14091201262040573, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.0024309465661644936}, {"id": 7, "seek": 2728, "start": 38.24, "end": 44.56, "text": " Psycho-Linguistics Professor at MIT. He heads the MIT Language Lab that investigates why human", "tokens": [50912, 17303, 78, 12, 43, 7050, 6006, 8419, 412, 13100, 13, 634, 8050, 264, 13100, 24445, 10137, 300, 4557, 1024, 983, 1952, 51228], "temperature": 0.0, "avg_logprob": -0.14091201262040573, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.0024309465661644936}, {"id": 8, "seek": 2728, "start": 44.56, "end": 51.120000000000005, "text": " languages look the way they do, the relationship between cultural language and how people represent,", "tokens": [51228, 8650, 574, 264, 636, 436, 360, 11, 264, 2480, 1296, 6988, 2856, 293, 577, 561, 2906, 11, 51556], "temperature": 0.0, "avg_logprob": -0.14091201262040573, "compression_ratio": 1.51931330472103, "no_speech_prob": 0.0024309465661644936}, {"id": 9, "seek": 5112, "start": 51.12, "end": 57.68, "text": " process, and learn language. Also, he should have a book titled Syntax, A Cognitive Approach,", "tokens": [50364, 1399, 11, 293, 1466, 2856, 13, 2743, 11, 415, 820, 362, 257, 1446, 19841, 3902, 580, 2797, 11, 316, 383, 2912, 2187, 29551, 608, 11, 50692], "temperature": 0.0, "avg_logprob": -0.11933122510495393, "compression_ratio": 1.448, "no_speech_prob": 0.018828876316547394}, {"id": 10, "seek": 5112, "start": 58.239999999999995, "end": 64.8, "text": " published by MIT Press, coming out this fall. So look out for that. This is the Lex Freeman", "tokens": [50720, 6572, 538, 13100, 6776, 11, 1348, 484, 341, 2100, 13, 407, 574, 484, 337, 300, 13, 639, 307, 264, 24086, 42163, 51048], "temperature": 0.0, "avg_logprob": -0.11933122510495393, "compression_ratio": 1.448, "no_speech_prob": 0.018828876316547394}, {"id": 11, "seek": 5112, "start": 64.8, "end": 70.47999999999999, "text": " podcast. To support it, please check out our sponsors in the description. And now, dear friends,", "tokens": [51048, 7367, 13, 1407, 1406, 309, 11, 1767, 1520, 484, 527, 22593, 294, 264, 3855, 13, 400, 586, 11, 6875, 1855, 11, 51332], "temperature": 0.0, "avg_logprob": -0.11933122510495393, "compression_ratio": 1.448, "no_speech_prob": 0.018828876316547394}, {"id": 12, "seek": 5112, "start": 70.47999999999999, "end": 76.72, "text": " here's Edward Gibson. When did you first become fascinated with human language?", "tokens": [51332, 510, 311, 18456, 42250, 13, 1133, 630, 291, 700, 1813, 24597, 365, 1952, 2856, 30, 51644], "temperature": 0.0, "avg_logprob": -0.11933122510495393, "compression_ratio": 1.448, "no_speech_prob": 0.018828876316547394}, {"id": 13, "seek": 7672, "start": 77.36, "end": 84.64, "text": " As a kid in school, when we had to structure sentences and English grammar, I found that", "tokens": [50396, 1018, 257, 1636, 294, 1395, 11, 562, 321, 632, 281, 3877, 16579, 293, 3669, 22317, 11, 286, 1352, 300, 50760], "temperature": 0.0, "avg_logprob": -0.08070431046813499, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.1008341982960701}, {"id": 14, "seek": 7672, "start": 84.64, "end": 90.64, "text": " process interesting. I found it confusing as to what it was I was told to do. I didn't", "tokens": [50760, 1399, 1880, 13, 286, 1352, 309, 13181, 382, 281, 437, 309, 390, 286, 390, 1907, 281, 360, 13, 286, 994, 380, 51060], "temperature": 0.0, "avg_logprob": -0.08070431046813499, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.1008341982960701}, {"id": 15, "seek": 7672, "start": 90.64, "end": 94.72, "text": " understand what the theory was behind it, but I found it very interesting.", "tokens": [51060, 1223, 437, 264, 5261, 390, 2261, 309, 11, 457, 286, 1352, 309, 588, 1880, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08070431046813499, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.1008341982960701}, {"id": 16, "seek": 7672, "start": 94.72, "end": 97.52, "text": " So when you look at grammar, you're almost thinking about like a puzzle,", "tokens": [51264, 407, 562, 291, 574, 412, 22317, 11, 291, 434, 1920, 1953, 466, 411, 257, 12805, 11, 51404], "temperature": 0.0, "avg_logprob": -0.08070431046813499, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.1008341982960701}, {"id": 17, "seek": 7672, "start": 97.52, "end": 101.2, "text": " like almost like a mathematical puzzle? Yeah, I think that's right. I didn't know I was going", "tokens": [51404, 411, 1920, 411, 257, 18894, 12805, 30, 865, 11, 286, 519, 300, 311, 558, 13, 286, 994, 380, 458, 286, 390, 516, 51588], "temperature": 0.0, "avg_logprob": -0.08070431046813499, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.1008341982960701}, {"id": 18, "seek": 7672, "start": 101.2, "end": 106.32, "text": " to work on this at all at that point. I was really just, I was kind of a math geek person.", "tokens": [51588, 281, 589, 322, 341, 412, 439, 412, 300, 935, 13, 286, 390, 534, 445, 11, 286, 390, 733, 295, 257, 5221, 36162, 954, 13, 51844], "temperature": 0.0, "avg_logprob": -0.08070431046813499, "compression_ratio": 1.7887323943661972, "no_speech_prob": 0.1008341982960701}, {"id": 19, "seek": 10632, "start": 106.39999999999999, "end": 109.27999999999999, "text": " Computer scientist. I really liked computer science. And then I found", "tokens": [50368, 22289, 12662, 13, 286, 534, 4501, 3820, 3497, 13, 400, 550, 286, 1352, 50512], "temperature": 0.0, "avg_logprob": -0.14830118876237136, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.002472073072567582}, {"id": 20, "seek": 10632, "start": 110.0, "end": 116.63999999999999, "text": " language as a neat puzzle to work on from an engineering perspective. Actually, that's what", "tokens": [50548, 2856, 382, 257, 10654, 12805, 281, 589, 322, 490, 364, 7043, 4585, 13, 5135, 11, 300, 311, 437, 50880], "temperature": 0.0, "avg_logprob": -0.14830118876237136, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.002472073072567582}, {"id": 21, "seek": 10632, "start": 116.63999999999999, "end": 123.6, "text": " I, as a, I sort of accidentally, I decided after I finished my undergraduate degree,", "tokens": [50880, 286, 11, 382, 257, 11, 286, 1333, 295, 15715, 11, 286, 3047, 934, 286, 4335, 452, 19113, 4314, 11, 51228], "temperature": 0.0, "avg_logprob": -0.14830118876237136, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.002472073072567582}, {"id": 22, "seek": 10632, "start": 123.6, "end": 128.72, "text": " which was computer science and math and Canada and Queens University, I decided to go to grad", "tokens": [51228, 597, 390, 3820, 3497, 293, 5221, 293, 6309, 293, 18414, 3535, 11, 286, 3047, 281, 352, 281, 2771, 51484], "temperature": 0.0, "avg_logprob": -0.14830118876237136, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.002472073072567582}, {"id": 23, "seek": 10632, "start": 128.72, "end": 135.04, "text": " school. That's what I always thought I would do. And I went to Cambridge, where they had a master's", "tokens": [51484, 1395, 13, 663, 311, 437, 286, 1009, 1194, 286, 576, 360, 13, 400, 286, 1437, 281, 24876, 11, 689, 436, 632, 257, 4505, 311, 51800], "temperature": 0.0, "avg_logprob": -0.14830118876237136, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.002472073072567582}, {"id": 24, "seek": 13504, "start": 135.04, "end": 141.12, "text": " in a master's program in computational linguistics. And I hadn't taken a single language class", "tokens": [50364, 294, 257, 4505, 311, 1461, 294, 28270, 21766, 6006, 13, 400, 286, 8782, 380, 2726, 257, 2167, 2856, 1508, 50668], "temperature": 0.0, "avg_logprob": -0.12070842992479557, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0008294699946418405}, {"id": 25, "seek": 13504, "start": 141.12, "end": 146.64, "text": " before. All I'd taken was CS, computer science, math classes, pretty much mostly as an undergrad.", "tokens": [50668, 949, 13, 1057, 286, 1116, 2726, 390, 9460, 11, 3820, 3497, 11, 5221, 5359, 11, 1238, 709, 5240, 382, 364, 14295, 13, 50944], "temperature": 0.0, "avg_logprob": -0.12070842992479557, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0008294699946418405}, {"id": 26, "seek": 13504, "start": 146.64, "end": 149.51999999999998, "text": " And I just thought, oh, this was an interesting thing to do for a year,", "tokens": [50944, 400, 286, 445, 1194, 11, 1954, 11, 341, 390, 364, 1880, 551, 281, 360, 337, 257, 1064, 11, 51088], "temperature": 0.0, "avg_logprob": -0.12070842992479557, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0008294699946418405}, {"id": 27, "seek": 13504, "start": 150.23999999999998, "end": 155.76, "text": " because it was a single year program. And then I ended up spending my whole life doing it.", "tokens": [51124, 570, 309, 390, 257, 2167, 1064, 1461, 13, 400, 550, 286, 4590, 493, 6434, 452, 1379, 993, 884, 309, 13, 51400], "temperature": 0.0, "avg_logprob": -0.12070842992479557, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0008294699946418405}, {"id": 28, "seek": 13504, "start": 155.76, "end": 160.79999999999998, "text": " So fundamentally, your journey through life was one of a mathematician and computer scientists.", "tokens": [51400, 407, 17879, 11, 428, 4671, 807, 993, 390, 472, 295, 257, 48281, 293, 3820, 7708, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12070842992479557, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.0008294699946418405}, {"id": 29, "seek": 16080, "start": 160.8, "end": 166.48000000000002, "text": " And then you kind of discovered the puzzle, the problem of language, and approached it", "tokens": [50364, 400, 550, 291, 733, 295, 6941, 264, 12805, 11, 264, 1154, 295, 2856, 11, 293, 17247, 309, 50648], "temperature": 0.0, "avg_logprob": -0.08993072345339019, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.004467398393899202}, {"id": 30, "seek": 16080, "start": 166.48000000000002, "end": 172.24, "text": " from that angle, to try to understand it from that angle, almost like a mathematician or maybe", "tokens": [50648, 490, 300, 5802, 11, 281, 853, 281, 1223, 309, 490, 300, 5802, 11, 1920, 411, 257, 48281, 420, 1310, 50936], "temperature": 0.0, "avg_logprob": -0.08993072345339019, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.004467398393899202}, {"id": 31, "seek": 16080, "start": 172.24, "end": 178.4, "text": " even an engineer. As an engineer, I'd say, I mean, to be frank, I had taken an AI class,", "tokens": [50936, 754, 364, 11403, 13, 1018, 364, 11403, 11, 286, 1116, 584, 11, 286, 914, 11, 281, 312, 10455, 11, 286, 632, 2726, 364, 7318, 1508, 11, 51244], "temperature": 0.0, "avg_logprob": -0.08993072345339019, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.004467398393899202}, {"id": 32, "seek": 16080, "start": 178.4, "end": 183.20000000000002, "text": " I guess it was 83 or 85, somewhere 84 in there a long time ago. And there was a natural language", "tokens": [51244, 286, 2041, 309, 390, 30997, 420, 14695, 11, 4079, 29018, 294, 456, 257, 938, 565, 2057, 13, 400, 456, 390, 257, 3303, 2856, 51484], "temperature": 0.0, "avg_logprob": -0.08993072345339019, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.004467398393899202}, {"id": 33, "seek": 16080, "start": 183.20000000000002, "end": 188.8, "text": " section in there. And it didn't impress me. I thought there must be more interesting things", "tokens": [51484, 3541, 294, 456, 13, 400, 309, 994, 380, 6729, 385, 13, 286, 1194, 456, 1633, 312, 544, 1880, 721, 51764], "temperature": 0.0, "avg_logprob": -0.08993072345339019, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.004467398393899202}, {"id": 34, "seek": 18880, "start": 188.8, "end": 196.16000000000003, "text": " we can do. It didn't seem very, it seemed just a bunch of hacks to me. It didn't seem like a real", "tokens": [50364, 321, 393, 360, 13, 467, 994, 380, 1643, 588, 11, 309, 6576, 445, 257, 3840, 295, 33617, 281, 385, 13, 467, 994, 380, 1643, 411, 257, 957, 50732], "temperature": 0.0, "avg_logprob": -0.09342369079589843, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.0015008443733677268}, {"id": 35, "seek": 18880, "start": 196.16000000000003, "end": 203.20000000000002, "text": " theory of things in any way. And so this seemed like an interesting area where there wasn't enough", "tokens": [50732, 5261, 295, 721, 294, 604, 636, 13, 400, 370, 341, 6576, 411, 364, 1880, 1859, 689, 456, 2067, 380, 1547, 51084], "temperature": 0.0, "avg_logprob": -0.09342369079589843, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.0015008443733677268}, {"id": 36, "seek": 18880, "start": 203.20000000000002, "end": 208.56, "text": " good work. Did you ever come across like the philosophy angle of logic? So if you think about", "tokens": [51084, 665, 589, 13, 2589, 291, 1562, 808, 2108, 411, 264, 10675, 5802, 295, 9952, 30, 407, 498, 291, 519, 466, 51352], "temperature": 0.0, "avg_logprob": -0.09342369079589843, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.0015008443733677268}, {"id": 37, "seek": 18880, "start": 208.56, "end": 216.4, "text": " the 80s with AI, the expert systems where you try to kind of maybe sidestep the poetry of language", "tokens": [51352, 264, 4688, 82, 365, 7318, 11, 264, 5844, 3652, 689, 291, 853, 281, 733, 295, 1310, 20822, 377, 595, 264, 15155, 295, 2856, 51744], "temperature": 0.0, "avg_logprob": -0.09342369079589843, "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.0015008443733677268}, {"id": 38, "seek": 21640, "start": 216.4, "end": 221.12, "text": " and some of the syntax and the grammar and all that kind of stuff and go to the underlying", "tokens": [50364, 293, 512, 295, 264, 28431, 293, 264, 22317, 293, 439, 300, 733, 295, 1507, 293, 352, 281, 264, 14217, 50600], "temperature": 0.0, "avg_logprob": -0.1074378793889826, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.019117137417197227}, {"id": 39, "seek": 21640, "start": 221.12, "end": 226.72, "text": " meaning that language is trying to communicate and try to somehow compress that in a computer", "tokens": [50600, 3620, 300, 2856, 307, 1382, 281, 7890, 293, 853, 281, 6063, 14778, 300, 294, 257, 3820, 50880], "temperature": 0.0, "avg_logprob": -0.1074378793889826, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.019117137417197227}, {"id": 40, "seek": 21640, "start": 226.72, "end": 231.84, "text": " representable way. Do you ever come across that in your studies? I mean, I probably did, but I", "tokens": [50880, 2906, 712, 636, 13, 1144, 291, 1562, 808, 2108, 300, 294, 428, 5313, 30, 286, 914, 11, 286, 1391, 630, 11, 457, 286, 51136], "temperature": 0.0, "avg_logprob": -0.1074378793889826, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.019117137417197227}, {"id": 41, "seek": 21640, "start": 231.84, "end": 236.72, "text": " wasn't as interested in it. I was trying to do the easier problems first than ones I could", "tokens": [51136, 2067, 380, 382, 3102, 294, 309, 13, 286, 390, 1382, 281, 360, 264, 3571, 2740, 700, 813, 2306, 286, 727, 51380], "temperature": 0.0, "avg_logprob": -0.1074378793889826, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.019117137417197227}, {"id": 42, "seek": 21640, "start": 237.28, "end": 243.12, "text": " thought maybe were handleable, which seems like the syntax is easier, which is just the forms as", "tokens": [51408, 1194, 1310, 645, 4813, 712, 11, 597, 2544, 411, 264, 28431, 307, 3571, 11, 597, 307, 445, 264, 6422, 382, 51700], "temperature": 0.0, "avg_logprob": -0.1074378793889826, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.019117137417197227}, {"id": 43, "seek": 24312, "start": 243.12, "end": 246.56, "text": " opposed to the meaning. When you're starting talking about the meaning, that's a very hard", "tokens": [50364, 8851, 281, 264, 3620, 13, 1133, 291, 434, 2891, 1417, 466, 264, 3620, 11, 300, 311, 257, 588, 1152, 50536], "temperature": 0.0, "avg_logprob": -0.11798987768392648, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0046086483635008335}, {"id": 44, "seek": 24312, "start": 246.56, "end": 251.84, "text": " problem. And it still is a really, really hard problem. But the forms is easier. And so I thought", "tokens": [50536, 1154, 13, 400, 309, 920, 307, 257, 534, 11, 534, 1152, 1154, 13, 583, 264, 6422, 307, 3571, 13, 400, 370, 286, 1194, 50800], "temperature": 0.0, "avg_logprob": -0.11798987768392648, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0046086483635008335}, {"id": 45, "seek": 24312, "start": 251.84, "end": 258.24, "text": " at least figuring out the forms of human language, which sounds really hard, but is actually maybe", "tokens": [50800, 412, 1935, 15213, 484, 264, 6422, 295, 1952, 2856, 11, 597, 3263, 534, 1152, 11, 457, 307, 767, 1310, 51120], "temperature": 0.0, "avg_logprob": -0.11798987768392648, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0046086483635008335}, {"id": 46, "seek": 24312, "start": 258.24, "end": 263.68, "text": " more tractable. So it's interesting. You think there is a big divide, there's a gap,", "tokens": [51120, 544, 24207, 712, 13, 407, 309, 311, 1880, 13, 509, 519, 456, 307, 257, 955, 9845, 11, 456, 311, 257, 7417, 11, 51392], "temperature": 0.0, "avg_logprob": -0.11798987768392648, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0046086483635008335}, {"id": 47, "seek": 24312, "start": 263.68, "end": 269.44, "text": " there's a distance between form and meaning. Because that's a question you have discussed a", "tokens": [51392, 456, 311, 257, 4560, 1296, 1254, 293, 3620, 13, 1436, 300, 311, 257, 1168, 291, 362, 7152, 257, 51680], "temperature": 0.0, "avg_logprob": -0.11798987768392648, "compression_ratio": 1.7509433962264151, "no_speech_prob": 0.0046086483635008335}, {"id": 48, "seek": 26944, "start": 269.44, "end": 274.96, "text": " lot with LLMs, because they're damn good at form. Yeah, I think it's what they're good at,", "tokens": [50364, 688, 365, 441, 43, 26386, 11, 570, 436, 434, 8151, 665, 412, 1254, 13, 865, 11, 286, 519, 309, 311, 437, 436, 434, 665, 412, 11, 50640], "temperature": 0.0, "avg_logprob": -0.2085614522298177, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.00600196048617363}, {"id": 49, "seek": 26944, "start": 274.96, "end": 278.71999999999997, "text": " is form. And that's why they're good, because they can do form, meaning's hard.", "tokens": [50640, 307, 1254, 13, 400, 300, 311, 983, 436, 434, 665, 11, 570, 436, 393, 360, 1254, 11, 3620, 311, 1152, 13, 50828], "temperature": 0.0, "avg_logprob": -0.2085614522298177, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.00600196048617363}, {"id": 50, "seek": 26944, "start": 279.68, "end": 284.16, "text": " Do you think they're, oh, wow. I mean, it's an open question, right? How close form and meaning", "tokens": [50876, 1144, 291, 519, 436, 434, 11, 1954, 11, 6076, 13, 286, 914, 11, 309, 311, 364, 1269, 1168, 11, 558, 30, 1012, 1998, 1254, 293, 3620, 51100], "temperature": 0.0, "avg_logprob": -0.2085614522298177, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.00600196048617363}, {"id": 51, "seek": 26944, "start": 284.16, "end": 289.6, "text": " are? We'll discuss it. But to me, studying form, maybe some romantic notion,", "tokens": [51100, 366, 30, 492, 603, 2248, 309, 13, 583, 281, 385, 11, 7601, 1254, 11, 1310, 512, 13590, 10710, 11, 51372], "temperature": 0.0, "avg_logprob": -0.2085614522298177, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.00600196048617363}, {"id": 52, "seek": 26944, "start": 290.88, "end": 298.08, "text": " gives you form is like the shadow of the bigger meaning thing underline language.", "tokens": [51436, 2709, 291, 1254, 307, 411, 264, 8576, 295, 264, 3801, 3620, 551, 833, 1889, 2856, 13, 51796], "temperature": 0.0, "avg_logprob": -0.2085614522298177, "compression_ratio": 1.6798418972332017, "no_speech_prob": 0.00600196048617363}, {"id": 53, "seek": 29808, "start": 298.15999999999997, "end": 305.35999999999996, "text": " Language is how we communicate ideas. We communicate with each other using language.", "tokens": [50368, 24445, 307, 577, 321, 7890, 3487, 13, 492, 7890, 365, 1184, 661, 1228, 2856, 13, 50728], "temperature": 0.0, "avg_logprob": -0.0914435118771671, "compression_ratio": 1.9485981308411215, "no_speech_prob": 0.00035685382317751646}, {"id": 54, "seek": 29808, "start": 305.35999999999996, "end": 310.71999999999997, "text": " So in understanding the structure of that communication, I think you start to understand", "tokens": [50728, 407, 294, 3701, 264, 3877, 295, 300, 6101, 11, 286, 519, 291, 722, 281, 1223, 50996], "temperature": 0.0, "avg_logprob": -0.0914435118771671, "compression_ratio": 1.9485981308411215, "no_speech_prob": 0.00035685382317751646}, {"id": 55, "seek": 29808, "start": 310.71999999999997, "end": 315.91999999999996, "text": " the structure of thought and the structure of meaning behind those thoughts and communication", "tokens": [50996, 264, 3877, 295, 1194, 293, 264, 3877, 295, 3620, 2261, 729, 4598, 293, 6101, 51256], "temperature": 0.0, "avg_logprob": -0.0914435118771671, "compression_ratio": 1.9485981308411215, "no_speech_prob": 0.00035685382317751646}, {"id": 56, "seek": 29808, "start": 315.91999999999996, "end": 322.88, "text": " to me. But to you, big gap. What do you find most beautiful about human language,", "tokens": [51256, 281, 385, 13, 583, 281, 291, 11, 955, 7417, 13, 708, 360, 291, 915, 881, 2238, 466, 1952, 2856, 11, 51604], "temperature": 0.0, "avg_logprob": -0.0914435118771671, "compression_ratio": 1.9485981308411215, "no_speech_prob": 0.00035685382317751646}, {"id": 57, "seek": 29808, "start": 322.88, "end": 327.76, "text": " maybe the form of human language, the expression of human language?", "tokens": [51604, 1310, 264, 1254, 295, 1952, 2856, 11, 264, 6114, 295, 1952, 2856, 30, 51848], "temperature": 0.0, "avg_logprob": -0.0914435118771671, "compression_ratio": 1.9485981308411215, "no_speech_prob": 0.00035685382317751646}, {"id": 58, "seek": 32776, "start": 327.76, "end": 334.4, "text": " What I find beautiful about human language is some of the generalizations that happen", "tokens": [50364, 708, 286, 915, 2238, 466, 1952, 2856, 307, 512, 295, 264, 2674, 14455, 300, 1051, 50696], "temperature": 0.0, "avg_logprob": -0.11707737279492755, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.000444032164523378}, {"id": 59, "seek": 32776, "start": 334.4, "end": 339.12, "text": " across the human language, just within and across a language. So let me give you an example of", "tokens": [50696, 2108, 264, 1952, 2856, 11, 445, 1951, 293, 2108, 257, 2856, 13, 407, 718, 385, 976, 291, 364, 1365, 295, 50932], "temperature": 0.0, "avg_logprob": -0.11707737279492755, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.000444032164523378}, {"id": 60, "seek": 32776, "start": 339.12, "end": 347.03999999999996, "text": " something which I find kind of remarkable, that is if a language, if it has a word order,", "tokens": [50932, 746, 597, 286, 915, 733, 295, 12802, 11, 300, 307, 498, 257, 2856, 11, 498, 309, 575, 257, 1349, 1668, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11707737279492755, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.000444032164523378}, {"id": 61, "seek": 32776, "start": 347.03999999999996, "end": 351.2, "text": " such that the verbs tend to come before their objects. And so that's like English does that.", "tokens": [51328, 1270, 300, 264, 30051, 3928, 281, 808, 949, 641, 6565, 13, 400, 370, 300, 311, 411, 3669, 775, 300, 13, 51536], "temperature": 0.0, "avg_logprob": -0.11707737279492755, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.000444032164523378}, {"id": 62, "seek": 35120, "start": 351.2, "end": 357.44, "text": " So we have the subject comes first in a simple sentence. So I say, you know, the", "tokens": [50364, 407, 321, 362, 264, 3983, 1487, 700, 294, 257, 2199, 8174, 13, 407, 286, 584, 11, 291, 458, 11, 264, 50676], "temperature": 0.0, "avg_logprob": -0.13163983724950776, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.06750128418207169}, {"id": 63, "seek": 35120, "start": 358.47999999999996, "end": 363.68, "text": " dog chased the cat or Mary kicked the ball. So the subject first, and then after the subject,", "tokens": [50728, 3000, 33091, 264, 3857, 420, 6059, 14609, 264, 2594, 13, 407, 264, 3983, 700, 11, 293, 550, 934, 264, 3983, 11, 50988], "temperature": 0.0, "avg_logprob": -0.13163983724950776, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.06750128418207169}, {"id": 64, "seek": 35120, "start": 363.68, "end": 369.03999999999996, "text": " there's the verb. And then we have objects. All these things come after in English. So it's generally", "tokens": [50988, 456, 311, 264, 9595, 13, 400, 550, 321, 362, 6565, 13, 1057, 613, 721, 808, 934, 294, 3669, 13, 407, 309, 311, 5101, 51256], "temperature": 0.0, "avg_logprob": -0.13163983724950776, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.06750128418207169}, {"id": 65, "seek": 35120, "start": 369.03999999999996, "end": 373.76, "text": " a verb. And most of the stuff that we want to say comes after the subject. It's the objects.", "tokens": [51256, 257, 9595, 13, 400, 881, 295, 264, 1507, 300, 321, 528, 281, 584, 1487, 934, 264, 3983, 13, 467, 311, 264, 6565, 13, 51492], "temperature": 0.0, "avg_logprob": -0.13163983724950776, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.06750128418207169}, {"id": 66, "seek": 35120, "start": 373.76, "end": 377.59999999999997, "text": " There's a lot of things we want to say they come after. And there's a lot of languages like that.", "tokens": [51492, 821, 311, 257, 688, 295, 721, 321, 528, 281, 584, 436, 808, 934, 13, 400, 456, 311, 257, 688, 295, 8650, 411, 300, 13, 51684], "temperature": 0.0, "avg_logprob": -0.13163983724950776, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.06750128418207169}, {"id": 67, "seek": 37760, "start": 377.6, "end": 384.0, "text": " About 40% of the languages of the world look like that. They're subject verb object languages.", "tokens": [50364, 7769, 3356, 4, 295, 264, 8650, 295, 264, 1002, 574, 411, 300, 13, 814, 434, 3983, 9595, 2657, 8650, 13, 50684], "temperature": 0.0, "avg_logprob": -0.1889634813581194, "compression_ratio": 1.814070351758794, "no_speech_prob": 0.0018100360175594687}, {"id": 68, "seek": 37760, "start": 384.0, "end": 392.64000000000004, "text": " And then these languages tend to have prepositions, these little markers on the nouns", "tokens": [50684, 400, 550, 613, 8650, 3928, 281, 362, 2666, 329, 2451, 11, 613, 707, 19175, 322, 264, 48184, 51116], "temperature": 0.0, "avg_logprob": -0.1889634813581194, "compression_ratio": 1.814070351758794, "no_speech_prob": 0.0018100360175594687}, {"id": 69, "seek": 37760, "start": 392.64000000000004, "end": 398.72, "text": " that connect nouns to other nouns or nouns to verb. So when I say a verb, I say preposition", "tokens": [51116, 300, 1745, 48184, 281, 661, 48184, 420, 48184, 281, 9595, 13, 407, 562, 286, 584, 257, 9595, 11, 286, 584, 2666, 5830, 51420], "temperature": 0.0, "avg_logprob": -0.1889634813581194, "compression_ratio": 1.814070351758794, "no_speech_prob": 0.0018100360175594687}, {"id": 70, "seek": 37760, "start": 398.72, "end": 404.72, "text": " like in or on or of or about, I say, I talk about something. The something is the object", "tokens": [51420, 411, 294, 420, 322, 420, 295, 420, 466, 11, 286, 584, 11, 286, 751, 466, 746, 13, 440, 746, 307, 264, 2657, 51720], "temperature": 0.0, "avg_logprob": -0.1889634813581194, "compression_ratio": 1.814070351758794, "no_speech_prob": 0.0018100360175594687}, {"id": 71, "seek": 40472, "start": 404.8, "end": 409.12, "text": " of that preposition that we have these little markers come also just like verbs,", "tokens": [50368, 295, 300, 2666, 5830, 300, 321, 362, 613, 707, 19175, 808, 611, 445, 411, 30051, 11, 50584], "temperature": 0.0, "avg_logprob": -0.1441883194112332, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0694904774427414}, {"id": 72, "seek": 40472, "start": 409.12, "end": 413.92, "text": " they come before their their nouns. Okay. And then so now we look at other languages", "tokens": [50584, 436, 808, 949, 641, 641, 48184, 13, 1033, 13, 400, 550, 370, 586, 321, 574, 412, 661, 8650, 50824], "temperature": 0.0, "avg_logprob": -0.1441883194112332, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0694904774427414}, {"id": 73, "seek": 40472, "start": 413.92, "end": 420.08000000000004, "text": " that like Japanese or or Hindi or some, these are these are so called verb final languages.", "tokens": [50824, 300, 411, 5433, 420, 420, 36225, 420, 512, 11, 613, 366, 613, 366, 370, 1219, 9595, 2572, 8650, 13, 51132], "temperature": 0.0, "avg_logprob": -0.1441883194112332, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0694904774427414}, {"id": 74, "seek": 40472, "start": 420.08000000000004, "end": 426.0, "text": " Those is about maybe a little more than 40%, maybe 45% of the world's languages are more,", "tokens": [51132, 3950, 307, 466, 1310, 257, 707, 544, 813, 3356, 8923, 1310, 6905, 4, 295, 264, 1002, 311, 8650, 366, 544, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1441883194112332, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0694904774427414}, {"id": 75, "seek": 40472, "start": 426.0, "end": 432.08000000000004, "text": " I mean, 50% of the world's languages are verb final. Those tend to be post positions,", "tokens": [51428, 286, 914, 11, 2625, 4, 295, 264, 1002, 311, 8650, 366, 9595, 2572, 13, 3950, 3928, 281, 312, 2183, 8432, 11, 51732], "temperature": 0.0, "avg_logprob": -0.1441883194112332, "compression_ratio": 1.7818930041152263, "no_speech_prob": 0.0694904774427414}, {"id": 76, "seek": 43208, "start": 432.08, "end": 435.28, "text": " those markers, the same if you have the states have the same kinds of markers,", "tokens": [50364, 729, 19175, 11, 264, 912, 498, 291, 362, 264, 4368, 362, 264, 912, 3685, 295, 19175, 11, 50524], "temperature": 0.0, "avg_logprob": -0.12936111257857635, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.01097823679447174}, {"id": 77, "seek": 43208, "start": 436.08, "end": 442.08, "text": " as we do in English, but they put them after. So sorry, they put them first, the markers come", "tokens": [50564, 382, 321, 360, 294, 3669, 11, 457, 436, 829, 552, 934, 13, 407, 2597, 11, 436, 829, 552, 700, 11, 264, 19175, 808, 50864], "temperature": 0.0, "avg_logprob": -0.12936111257857635, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.01097823679447174}, {"id": 78, "seek": 43208, "start": 442.08, "end": 449.59999999999997, "text": " first. So you say instead of, you know, talk about a book, you say a book about the opposite", "tokens": [50864, 700, 13, 407, 291, 584, 2602, 295, 11, 291, 458, 11, 751, 466, 257, 1446, 11, 291, 584, 257, 1446, 466, 264, 6182, 51240], "temperature": 0.0, "avg_logprob": -0.12936111257857635, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.01097823679447174}, {"id": 79, "seek": 43208, "start": 449.59999999999997, "end": 455.28, "text": " order there in Japanese or in Hindi, you do the opposite and the and the talk comes at the end.", "tokens": [51240, 1668, 456, 294, 5433, 420, 294, 36225, 11, 291, 360, 264, 6182, 293, 264, 293, 264, 751, 1487, 412, 264, 917, 13, 51524], "temperature": 0.0, "avg_logprob": -0.12936111257857635, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.01097823679447174}, {"id": 80, "seek": 43208, "start": 455.28, "end": 460.4, "text": " So the verb will come at the end as well. So instead of Mary kicked the ball, it's Mary", "tokens": [51524, 407, 264, 9595, 486, 808, 412, 264, 917, 382, 731, 13, 407, 2602, 295, 6059, 14609, 264, 2594, 11, 309, 311, 6059, 51780], "temperature": 0.0, "avg_logprob": -0.12936111257857635, "compression_ratio": 1.9188034188034189, "no_speech_prob": 0.01097823679447174}, {"id": 81, "seek": 46040, "start": 461.35999999999996, "end": 468.15999999999997, "text": " ball kicked. And then if it's Mary kicked the ball to John, it's John two, the two,", "tokens": [50412, 2594, 14609, 13, 400, 550, 498, 309, 311, 6059, 14609, 264, 2594, 281, 2619, 11, 309, 311, 2619, 732, 11, 264, 732, 11, 50752], "temperature": 0.0, "avg_logprob": -0.1447815946353379, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.001926303026266396}, {"id": 82, "seek": 46040, "start": 468.15999999999997, "end": 472.23999999999995, "text": " the little marker there, the preposition, it's a post position in these languages.", "tokens": [50752, 264, 707, 15247, 456, 11, 264, 2666, 5830, 11, 309, 311, 257, 2183, 2535, 294, 613, 8650, 13, 50956], "temperature": 0.0, "avg_logprob": -0.1447815946353379, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.001926303026266396}, {"id": 83, "seek": 46040, "start": 472.23999999999995, "end": 476.32, "text": " And so the interesting thing, a fascinating thing to me is that within a language,", "tokens": [50956, 400, 370, 264, 1880, 551, 11, 257, 10343, 551, 281, 385, 307, 300, 1951, 257, 2856, 11, 51160], "temperature": 0.0, "avg_logprob": -0.1447815946353379, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.001926303026266396}, {"id": 84, "seek": 46040, "start": 477.76, "end": 486.08, "text": " this order aligns, it's harmonic. And so if it's one or the other, it's either verb initial or", "tokens": [51232, 341, 1668, 7975, 82, 11, 309, 311, 32270, 13, 400, 370, 498, 309, 311, 472, 420, 264, 661, 11, 309, 311, 2139, 9595, 5883, 420, 51648], "temperature": 0.0, "avg_logprob": -0.1447815946353379, "compression_ratio": 1.7916666666666667, "no_speech_prob": 0.001926303026266396}, {"id": 85, "seek": 48608, "start": 486.15999999999997, "end": 490.71999999999997, "text": " verb final, but then you then you'll have prepositions, prepositions or post positions.", "tokens": [50368, 9595, 2572, 11, 457, 550, 291, 550, 291, 603, 362, 2666, 329, 2451, 11, 2666, 329, 2451, 420, 2183, 8432, 13, 50596], "temperature": 0.0, "avg_logprob": -0.1393788226015933, "compression_ratio": 1.7890625, "no_speech_prob": 0.02674471214413643}, {"id": 86, "seek": 48608, "start": 490.71999999999997, "end": 495.68, "text": " And so that and that's across the languages that we can look at. We've got around 1000 languages", "tokens": [50596, 400, 370, 300, 293, 300, 311, 2108, 264, 8650, 300, 321, 393, 574, 412, 13, 492, 600, 658, 926, 9714, 8650, 50844], "temperature": 0.0, "avg_logprob": -0.1393788226015933, "compression_ratio": 1.7890625, "no_speech_prob": 0.02674471214413643}, {"id": 87, "seek": 48608, "start": 495.68, "end": 501.91999999999996, "text": " for there's around 7000 languages around on the earth right now. But we have information", "tokens": [50844, 337, 456, 311, 926, 1614, 1360, 8650, 926, 322, 264, 4120, 558, 586, 13, 583, 321, 362, 1589, 51156], "temperature": 0.0, "avg_logprob": -0.1393788226015933, "compression_ratio": 1.7890625, "no_speech_prob": 0.02674471214413643}, {"id": 88, "seek": 48608, "start": 501.91999999999996, "end": 507.44, "text": " about say word order on around 1000 of those pretty decent amount of information. And for", "tokens": [51156, 466, 584, 1349, 1668, 322, 926, 9714, 295, 729, 1238, 8681, 2372, 295, 1589, 13, 400, 337, 51432], "temperature": 0.0, "avg_logprob": -0.1393788226015933, "compression_ratio": 1.7890625, "no_speech_prob": 0.02674471214413643}, {"id": 89, "seek": 48608, "start": 507.44, "end": 514.48, "text": " those 1000, which we know about, about 95% fit that pattern. So they will have either verb and", "tokens": [51432, 729, 9714, 11, 597, 321, 458, 466, 11, 466, 13420, 4, 3318, 300, 5102, 13, 407, 436, 486, 362, 2139, 9595, 293, 51784], "temperature": 0.0, "avg_logprob": -0.1393788226015933, "compression_ratio": 1.7890625, "no_speech_prob": 0.02674471214413643}, {"id": 90, "seek": 51448, "start": 514.88, "end": 521.28, "text": " it's about half and half or half verb initial, like English and half verb final, like Japanese.", "tokens": [50384, 309, 311, 466, 1922, 293, 1922, 420, 1922, 9595, 5883, 11, 411, 3669, 293, 1922, 9595, 2572, 11, 411, 5433, 13, 50704], "temperature": 0.0, "avg_logprob": -0.180847414078251, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.009701750241219997}, {"id": 91, "seek": 51448, "start": 521.28, "end": 527.52, "text": " So just to clarify, verb initial is subject verb object. That's correct. Verb final is", "tokens": [50704, 407, 445, 281, 17594, 11, 9595, 5883, 307, 3983, 9595, 2657, 13, 663, 311, 3006, 13, 27034, 2572, 307, 51016], "temperature": 0.0, "avg_logprob": -0.180847414078251, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.009701750241219997}, {"id": 92, "seek": 51448, "start": 527.52, "end": 532.0, "text": " still subject, object verb. That's correct. Yeah, the subject is generally first.", "tokens": [51016, 920, 3983, 11, 2657, 9595, 13, 663, 311, 3006, 13, 865, 11, 264, 3983, 307, 5101, 700, 13, 51240], "temperature": 0.0, "avg_logprob": -0.180847414078251, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.009701750241219997}, {"id": 93, "seek": 51448, "start": 532.0, "end": 539.9200000000001, "text": " That's so fascinating. I ate an apple or I apple eight. Yes. Okay. And it's fascinating that there's", "tokens": [51240, 663, 311, 370, 10343, 13, 286, 8468, 364, 10606, 420, 286, 10606, 3180, 13, 1079, 13, 1033, 13, 400, 309, 311, 10343, 300, 456, 311, 51636], "temperature": 0.0, "avg_logprob": -0.180847414078251, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.009701750241219997}, {"id": 94, "seek": 53992, "start": 540.16, "end": 545.36, "text": " pretty even division in the world amongst those 45%. Yeah, it's pretty, it's pretty even.", "tokens": [50376, 1238, 754, 10044, 294, 264, 1002, 12918, 729, 6905, 6856, 865, 11, 309, 311, 1238, 11, 309, 311, 1238, 754, 13, 50636], "temperature": 0.0, "avg_logprob": -0.15973805514248934, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.008575684390962124}, {"id": 95, "seek": 53992, "start": 545.36, "end": 549.5999999999999, "text": " And those two are the most common by far. Those two words, the subject tends to be first.", "tokens": [50636, 400, 729, 732, 366, 264, 881, 2689, 538, 1400, 13, 3950, 732, 2283, 11, 264, 3983, 12258, 281, 312, 700, 13, 50848], "temperature": 0.0, "avg_logprob": -0.15973805514248934, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.008575684390962124}, {"id": 96, "seek": 53992, "start": 549.5999999999999, "end": 553.12, "text": " There's so many interesting things, but these things are what I find so fascinating is there", "tokens": [50848, 821, 311, 370, 867, 1880, 721, 11, 457, 613, 721, 366, 437, 286, 915, 370, 10343, 307, 456, 51024], "temperature": 0.0, "avg_logprob": -0.15973805514248934, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.008575684390962124}, {"id": 97, "seek": 53992, "start": 553.12, "end": 559.12, "text": " are these generalizations within and across a language. And not only those are the, and", "tokens": [51024, 366, 613, 2674, 14455, 1951, 293, 2108, 257, 2856, 13, 400, 406, 787, 729, 366, 264, 11, 293, 51324], "temperature": 0.0, "avg_logprob": -0.15973805514248934, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.008575684390962124}, {"id": 98, "seek": 53992, "start": 559.12, "end": 563.28, "text": " there's actually a simple explanation, I think, for a lot of that. And that is,", "tokens": [51324, 456, 311, 767, 257, 2199, 10835, 11, 286, 519, 11, 337, 257, 688, 295, 300, 13, 400, 300, 307, 11, 51532], "temperature": 0.0, "avg_logprob": -0.15973805514248934, "compression_ratio": 1.6988416988416988, "no_speech_prob": 0.008575684390962124}, {"id": 99, "seek": 56328, "start": 563.68, "end": 569.6, "text": " you're trying to like minimize dependencies between words. That's basically the story,", "tokens": [50384, 291, 434, 1382, 281, 411, 17522, 36606, 1296, 2283, 13, 663, 311, 1936, 264, 1657, 11, 50680], "temperature": 0.0, "avg_logprob": -0.16171473627505095, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01911972649395466}, {"id": 100, "seek": 56328, "start": 570.16, "end": 575.6, "text": " I think, behind a lot of why word order looks the way it is, is you, we're always connecting.", "tokens": [50708, 286, 519, 11, 2261, 257, 688, 295, 983, 1349, 1668, 1542, 264, 636, 309, 307, 11, 307, 291, 11, 321, 434, 1009, 11015, 13, 50980], "temperature": 0.0, "avg_logprob": -0.16171473627505095, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01911972649395466}, {"id": 101, "seek": 56328, "start": 576.64, "end": 579.52, "text": " What is it? What is the thing I'm telling you? I'm talking to you in sentences. You're talking", "tokens": [51032, 708, 307, 309, 30, 708, 307, 264, 551, 286, 478, 3585, 291, 30, 286, 478, 1417, 281, 291, 294, 16579, 13, 509, 434, 1417, 51176], "temperature": 0.0, "avg_logprob": -0.16171473627505095, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01911972649395466}, {"id": 102, "seek": 56328, "start": 579.52, "end": 585.36, "text": " to me in sentences. These are sequences of words, which are connected. And the connections are", "tokens": [51176, 281, 385, 294, 16579, 13, 1981, 366, 22978, 295, 2283, 11, 597, 366, 4582, 13, 400, 264, 9271, 366, 51468], "temperature": 0.0, "avg_logprob": -0.16171473627505095, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01911972649395466}, {"id": 103, "seek": 56328, "start": 585.36, "end": 592.0, "text": " dependencies between the words. And it turns out that what we're trying to do in a language is", "tokens": [51468, 36606, 1296, 264, 2283, 13, 400, 309, 4523, 484, 300, 437, 321, 434, 1382, 281, 360, 294, 257, 2856, 307, 51800], "temperature": 0.0, "avg_logprob": -0.16171473627505095, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.01911972649395466}, {"id": 104, "seek": 59200, "start": 592.08, "end": 597.2, "text": " actually minimize those dependency links. It's easier for me to say things if the words that", "tokens": [50368, 767, 17522, 729, 33621, 6123, 13, 467, 311, 3571, 337, 385, 281, 584, 721, 498, 264, 2283, 300, 50624], "temperature": 0.0, "avg_logprob": -0.10562929975877114, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0037062927149236202}, {"id": 105, "seek": 59200, "start": 597.2, "end": 602.24, "text": " are connecting for their meaning are close together. It's easier for you in understanding if that's", "tokens": [50624, 366, 11015, 337, 641, 3620, 366, 1998, 1214, 13, 467, 311, 3571, 337, 291, 294, 3701, 498, 300, 311, 50876], "temperature": 0.0, "avg_logprob": -0.10562929975877114, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0037062927149236202}, {"id": 106, "seek": 59200, "start": 602.24, "end": 608.56, "text": " also true. If they're far away, it's hard to produce that and it's hard for you to understand.", "tokens": [50876, 611, 2074, 13, 759, 436, 434, 1400, 1314, 11, 309, 311, 1152, 281, 5258, 300, 293, 309, 311, 1152, 337, 291, 281, 1223, 13, 51192], "temperature": 0.0, "avg_logprob": -0.10562929975877114, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0037062927149236202}, {"id": 107, "seek": 59200, "start": 608.56, "end": 613.84, "text": " And the languages of the world within a language and across languages fit that generalization,", "tokens": [51192, 400, 264, 8650, 295, 264, 1002, 1951, 257, 2856, 293, 2108, 8650, 3318, 300, 2674, 2144, 11, 51456], "temperature": 0.0, "avg_logprob": -0.10562929975877114, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0037062927149236202}, {"id": 108, "seek": 59200, "start": 613.84, "end": 621.28, "text": " which is, so it turns out that having verbs initial and then having prepositions ends up", "tokens": [51456, 597, 307, 11, 370, 309, 4523, 484, 300, 1419, 30051, 5883, 293, 550, 1419, 2666, 329, 2451, 5314, 493, 51828], "temperature": 0.0, "avg_logprob": -0.10562929975877114, "compression_ratio": 1.8115384615384615, "no_speech_prob": 0.0037062927149236202}, {"id": 109, "seek": 62128, "start": 621.28, "end": 627.4399999999999, "text": " making dependencies shorter. And having verbs final and having postpositions ends up making", "tokens": [50364, 1455, 36606, 11639, 13, 400, 1419, 30051, 2572, 293, 1419, 2183, 30010, 2451, 5314, 493, 1455, 50672], "temperature": 0.0, "avg_logprob": -0.13281693951836948, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.0018672570586204529}, {"id": 110, "seek": 62128, "start": 627.4399999999999, "end": 631.04, "text": " dependencies shorter than if you cross them. If you cross them, it ends up, you just end up,", "tokens": [50672, 36606, 11639, 813, 498, 291, 3278, 552, 13, 759, 291, 3278, 552, 11, 309, 5314, 493, 11, 291, 445, 917, 493, 11, 50852], "temperature": 0.0, "avg_logprob": -0.13281693951836948, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.0018672570586204529}, {"id": 111, "seek": 62128, "start": 631.04, "end": 635.28, "text": " it's possible, you can do it. Even within a language. Within a language, you can do it.", "tokens": [50852, 309, 311, 1944, 11, 291, 393, 360, 309, 13, 2754, 1951, 257, 2856, 13, 15996, 257, 2856, 11, 291, 393, 360, 309, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13281693951836948, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.0018672570586204529}, {"id": 112, "seek": 62128, "start": 635.28, "end": 640.56, "text": " It just ends up with longer dependencies than if you didn't. And so languages tend to go that way.", "tokens": [51064, 467, 445, 5314, 493, 365, 2854, 36606, 813, 498, 291, 994, 380, 13, 400, 370, 8650, 3928, 281, 352, 300, 636, 13, 51328], "temperature": 0.0, "avg_logprob": -0.13281693951836948, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.0018672570586204529}, {"id": 113, "seek": 62128, "start": 640.56, "end": 646.8, "text": " They tend to, they say they call it harmonic. So it was observed a long time ago without the", "tokens": [51328, 814, 3928, 281, 11, 436, 584, 436, 818, 309, 32270, 13, 407, 309, 390, 13095, 257, 938, 565, 2057, 1553, 264, 51640], "temperature": 0.0, "avg_logprob": -0.13281693951836948, "compression_ratio": 1.9253112033195021, "no_speech_prob": 0.0018672570586204529}, {"id": 114, "seek": 64680, "start": 646.8, "end": 653.4399999999999, "text": " explanation by a guy called Joseph Greenberg, who's a famous typologist from Stanford. He", "tokens": [50364, 10835, 538, 257, 2146, 1219, 11170, 6969, 6873, 11, 567, 311, 257, 4618, 2125, 9201, 490, 20374, 13, 634, 50696], "temperature": 0.0, "avg_logprob": -0.1809213520151324, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.012427443638443947}, {"id": 115, "seek": 64680, "start": 653.4399999999999, "end": 657.1999999999999, "text": " observed a lot of generalizations about how word order works. And these are some of the", "tokens": [50696, 13095, 257, 688, 295, 2674, 14455, 466, 577, 1349, 1668, 1985, 13, 400, 613, 366, 512, 295, 264, 50884], "temperature": 0.0, "avg_logprob": -0.1809213520151324, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.012427443638443947}, {"id": 116, "seek": 64680, "start": 657.1999999999999, "end": 659.68, "text": " harmonic generalizations that he observed.", "tokens": [50884, 32270, 2674, 14455, 300, 415, 13095, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1809213520151324, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.012427443638443947}, {"id": 117, "seek": 64680, "start": 659.68, "end": 664.56, "text": " Harmonic generalizations about word, word, word. There's so many things I want to ask you.", "tokens": [51008, 40599, 299, 2674, 14455, 466, 1349, 11, 1349, 11, 1349, 13, 821, 311, 370, 867, 721, 286, 528, 281, 1029, 291, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1809213520151324, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.012427443638443947}, {"id": 118, "seek": 64680, "start": 664.56, "end": 664.88, "text": " Okay, good.", "tokens": [51252, 1033, 11, 665, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1809213520151324, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.012427443638443947}, {"id": 119, "seek": 64680, "start": 664.88, "end": 669.76, "text": " Okay, let me just, sometimes basics. You mentioned dependencies a few times.", "tokens": [51268, 1033, 11, 718, 385, 445, 11, 2171, 14688, 13, 509, 2835, 36606, 257, 1326, 1413, 13, 51512], "temperature": 0.0, "avg_logprob": -0.1809213520151324, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.012427443638443947}, {"id": 120, "seek": 64680, "start": 669.76, "end": 670.24, "text": " Yeah.", "tokens": [51512, 865, 13, 51536], "temperature": 0.0, "avg_logprob": -0.1809213520151324, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.012427443638443947}, {"id": 121, "seek": 64680, "start": 670.24, "end": 671.92, "text": " What do you mean by dependencies?", "tokens": [51536, 708, 360, 291, 914, 538, 36606, 30, 51620], "temperature": 0.0, "avg_logprob": -0.1809213520151324, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.012427443638443947}, {"id": 122, "seek": 67192, "start": 671.92, "end": 678.64, "text": " Well, what I mean is in, in language, there's kind of three structures to three components", "tokens": [50364, 1042, 11, 437, 286, 914, 307, 294, 11, 294, 2856, 11, 456, 311, 733, 295, 1045, 9227, 281, 1045, 6677, 50700], "temperature": 0.0, "avg_logprob": -0.13336774858377748, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0037056701257824898}, {"id": 123, "seek": 67192, "start": 678.64, "end": 684.4799999999999, "text": " to the structure of language. One is the sounds. So cat is cat and to in English. I'm not talking", "tokens": [50700, 281, 264, 3877, 295, 2856, 13, 1485, 307, 264, 3263, 13, 407, 3857, 307, 3857, 293, 281, 294, 3669, 13, 286, 478, 406, 1417, 50992], "temperature": 0.0, "avg_logprob": -0.13336774858377748, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0037056701257824898}, {"id": 124, "seek": 67192, "start": 684.4799999999999, "end": 688.56, "text": " about that part. I'm talking, and then there's two meaning parts. And those are the words.", "tokens": [50992, 466, 300, 644, 13, 286, 478, 1417, 11, 293, 550, 456, 311, 732, 3620, 3166, 13, 400, 729, 366, 264, 2283, 13, 51196], "temperature": 0.0, "avg_logprob": -0.13336774858377748, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0037056701257824898}, {"id": 125, "seek": 67192, "start": 688.56, "end": 692.88, "text": " And, and you're talking about meaning earlier. So words have a form and they have a meaning", "tokens": [51196, 400, 11, 293, 291, 434, 1417, 466, 3620, 3071, 13, 407, 2283, 362, 257, 1254, 293, 436, 362, 257, 3620, 51412], "temperature": 0.0, "avg_logprob": -0.13336774858377748, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0037056701257824898}, {"id": 126, "seek": 67192, "start": 692.88, "end": 697.28, "text": " associated with them. And so cat is a full form in English and it has a meaning associated with", "tokens": [51412, 6615, 365, 552, 13, 400, 370, 3857, 307, 257, 1577, 1254, 294, 3669, 293, 309, 575, 257, 3620, 6615, 365, 51632], "temperature": 0.0, "avg_logprob": -0.13336774858377748, "compression_ratio": 1.9297520661157024, "no_speech_prob": 0.0037056701257824898}, {"id": 127, "seek": 69728, "start": 697.28, "end": 704.0799999999999, "text": " whatever a cat is. And then the combinations of words, that's what I'll call grammar or syntax.", "tokens": [50364, 2035, 257, 3857, 307, 13, 400, 550, 264, 21267, 295, 2283, 11, 300, 311, 437, 286, 603, 818, 22317, 420, 28431, 13, 50704], "temperature": 0.0, "avg_logprob": -0.10588429266946357, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.0351308174431324}, {"id": 128, "seek": 69728, "start": 704.0799999999999, "end": 711.28, "text": " And that's like, when I have a combination like the cat or two cats, okay, so where I", "tokens": [50704, 400, 300, 311, 411, 11, 562, 286, 362, 257, 6562, 411, 264, 3857, 420, 732, 11111, 11, 1392, 11, 370, 689, 286, 51064], "temperature": 0.0, "avg_logprob": -0.10588429266946357, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.0351308174431324}, {"id": 129, "seek": 69728, "start": 711.28, "end": 715.28, "text": " take two different words there and put them together and I get a compositional meaning", "tokens": [51064, 747, 732, 819, 2283, 456, 293, 829, 552, 1214, 293, 286, 483, 257, 10199, 2628, 3620, 51264], "temperature": 0.0, "avg_logprob": -0.10588429266946357, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.0351308174431324}, {"id": 130, "seek": 69728, "start": 715.28, "end": 721.8399999999999, "text": " from putting those two different words together. And so that's the syntax. And in any sentence", "tokens": [51264, 490, 3372, 729, 732, 819, 2283, 1214, 13, 400, 370, 300, 311, 264, 28431, 13, 400, 294, 604, 8174, 51592], "temperature": 0.0, "avg_logprob": -0.10588429266946357, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.0351308174431324}, {"id": 131, "seek": 69728, "start": 721.8399999999999, "end": 725.4399999999999, "text": " or utterance, whatever I'm talking to you, you're talking to me, we have a bunch of words", "tokens": [51592, 420, 17567, 719, 11, 2035, 286, 478, 1417, 281, 291, 11, 291, 434, 1417, 281, 385, 11, 321, 362, 257, 3840, 295, 2283, 51772], "temperature": 0.0, "avg_logprob": -0.10588429266946357, "compression_ratio": 1.871900826446281, "no_speech_prob": 0.0351308174431324}, {"id": 132, "seek": 72544, "start": 725.44, "end": 733.0400000000001, "text": " and we're putting together in a sequence. They, it turns out they are connected so that every word", "tokens": [50364, 293, 321, 434, 3372, 1214, 294, 257, 8310, 13, 814, 11, 309, 4523, 484, 436, 366, 4582, 370, 300, 633, 1349, 50744], "temperature": 0.0, "avg_logprob": -0.1178242579227736, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0018670160789042711}, {"id": 133, "seek": 72544, "start": 733.0400000000001, "end": 738.0, "text": " is connected to just one other word in that, in that sentence. And so you end up with what's", "tokens": [50744, 307, 4582, 281, 445, 472, 661, 1349, 294, 300, 11, 294, 300, 8174, 13, 400, 370, 291, 917, 493, 365, 437, 311, 50992], "temperature": 0.0, "avg_logprob": -0.1178242579227736, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0018670160789042711}, {"id": 134, "seek": 72544, "start": 738.0, "end": 743.5200000000001, "text": " called technically a tree. It's a tree structure. So there, where there's a root of that, of that", "tokens": [50992, 1219, 12120, 257, 4230, 13, 467, 311, 257, 4230, 3877, 13, 407, 456, 11, 689, 456, 311, 257, 5593, 295, 300, 11, 295, 300, 51268], "temperature": 0.0, "avg_logprob": -0.1178242579227736, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0018670160789042711}, {"id": 135, "seek": 72544, "start": 743.5200000000001, "end": 748.96, "text": " to utterance of that sentence. And then there's a bunch of dependence, like branches from that root", "tokens": [51268, 281, 17567, 719, 295, 300, 8174, 13, 400, 550, 456, 311, 257, 3840, 295, 31704, 11, 411, 14770, 490, 300, 5593, 51540], "temperature": 0.0, "avg_logprob": -0.1178242579227736, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0018670160789042711}, {"id": 136, "seek": 72544, "start": 748.96, "end": 754.5600000000001, "text": " that go down to the words, the words are the leaves in this metaphor for a tree.", "tokens": [51540, 300, 352, 760, 281, 264, 2283, 11, 264, 2283, 366, 264, 5510, 294, 341, 19157, 337, 257, 4230, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1178242579227736, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0018670160789042711}, {"id": 137, "seek": 75456, "start": 754.56, "end": 758.64, "text": " So a tree is also sort of a mathematical construct. Yeah. Yeah. It's a graph theoretical thing.", "tokens": [50364, 407, 257, 4230, 307, 611, 1333, 295, 257, 18894, 7690, 13, 865, 13, 865, 13, 467, 311, 257, 4295, 20864, 551, 13, 50568], "temperature": 0.0, "avg_logprob": -0.216949852145448, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0006877367268316448}, {"id": 138, "seek": 75456, "start": 758.64, "end": 764.3199999999999, "text": " Graph theory thing. Yeah. So in this fascinating that you can break down a sentence into a tree", "tokens": [50568, 21884, 5261, 551, 13, 865, 13, 407, 294, 341, 10343, 300, 291, 393, 1821, 760, 257, 8174, 666, 257, 4230, 50852], "temperature": 0.0, "avg_logprob": -0.216949852145448, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0006877367268316448}, {"id": 139, "seek": 75456, "start": 764.3199999999999, "end": 768.64, "text": " and then one, every word is hanging on to another, it's depending on it. That's right. And everyone", "tokens": [50852, 293, 550, 472, 11, 633, 1349, 307, 8345, 322, 281, 1071, 11, 309, 311, 5413, 322, 309, 13, 663, 311, 558, 13, 400, 1518, 51068], "temperature": 0.0, "avg_logprob": -0.216949852145448, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0006877367268316448}, {"id": 140, "seek": 75456, "start": 768.64, "end": 772.2399999999999, "text": " agrees on that. So all linguists will agree with that. No one. It's not a controversial.", "tokens": [51068, 26383, 322, 300, 13, 407, 439, 21766, 1751, 486, 3986, 365, 300, 13, 883, 472, 13, 467, 311, 406, 257, 17323, 13, 51248], "temperature": 0.0, "avg_logprob": -0.216949852145448, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0006877367268316448}, {"id": 141, "seek": 75456, "start": 772.2399999999999, "end": 776.4, "text": " That is not controversial. There's nobody sitting here. I do nothing mad at you. I don't think so.", "tokens": [51248, 663, 307, 406, 17323, 13, 821, 311, 5079, 3798, 510, 13, 286, 360, 1825, 5244, 412, 291, 13, 286, 500, 380, 519, 370, 13, 51456], "temperature": 0.0, "avg_logprob": -0.216949852145448, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0006877367268316448}, {"id": 142, "seek": 75456, "start": 776.4, "end": 780.0799999999999, "text": " Okay. There's no linguists sitting there mad at this. No, I think in every language,", "tokens": [51456, 1033, 13, 821, 311, 572, 21766, 1751, 3798, 456, 5244, 412, 341, 13, 883, 11, 286, 519, 294, 633, 2856, 11, 51640], "temperature": 0.0, "avg_logprob": -0.216949852145448, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0006877367268316448}, {"id": 143, "seek": 78008, "start": 780.08, "end": 786.48, "text": " I think everyone agrees that all sentences are trees at some level. Can I pause on that? Sure.", "tokens": [50364, 286, 519, 1518, 26383, 300, 439, 16579, 366, 5852, 412, 512, 1496, 13, 1664, 286, 10465, 322, 300, 30, 4894, 13, 50684], "temperature": 0.0, "avg_logprob": -0.15580050499884637, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.001206434448249638}, {"id": 144, "seek": 78008, "start": 786.48, "end": 794.4000000000001, "text": " Because it's to me just as a layman, it's surprising that you can break down sentences", "tokens": [50684, 1436, 309, 311, 281, 385, 445, 382, 257, 2360, 1601, 11, 309, 311, 8830, 300, 291, 393, 1821, 760, 16579, 51080], "temperature": 0.0, "avg_logprob": -0.15580050499884637, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.001206434448249638}, {"id": 145, "seek": 78008, "start": 794.4000000000001, "end": 800.32, "text": " in many, mostly all languages into a tree. I think so. I've never heard of anyone", "tokens": [51080, 294, 867, 11, 5240, 439, 8650, 666, 257, 4230, 13, 286, 519, 370, 13, 286, 600, 1128, 2198, 295, 2878, 51376], "temperature": 0.0, "avg_logprob": -0.15580050499884637, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.001206434448249638}, {"id": 146, "seek": 78008, "start": 800.32, "end": 805.12, "text": " disagreeing with that. That's weird. The details of the trees are what people disagree with.", "tokens": [51376, 14091, 278, 365, 300, 13, 663, 311, 3657, 13, 440, 4365, 295, 264, 5852, 366, 437, 561, 14091, 365, 13, 51616], "temperature": 0.0, "avg_logprob": -0.15580050499884637, "compression_ratio": 1.6255707762557077, "no_speech_prob": 0.001206434448249638}, {"id": 147, "seek": 80512, "start": 805.84, "end": 809.6, "text": " Well, okay. So what's the root of it? How do you construct?", "tokens": [50400, 1042, 11, 1392, 13, 407, 437, 311, 264, 5593, 295, 309, 30, 1012, 360, 291, 7690, 30, 50588], "temperature": 0.0, "avg_logprob": -0.17810870053475364, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.04594254121184349}, {"id": 148, "seek": 80512, "start": 809.6, "end": 812.88, "text": " How hard is it? What is the process of constructing a tree from a sentence?", "tokens": [50588, 1012, 1152, 307, 309, 30, 708, 307, 264, 1399, 295, 39969, 257, 4230, 490, 257, 8174, 30, 50752], "temperature": 0.0, "avg_logprob": -0.17810870053475364, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.04594254121184349}, {"id": 149, "seek": 80512, "start": 814.0, "end": 819.52, "text": " Well, this is where, depending on what your theoretical notions, I'm going to say the simplest", "tokens": [50808, 1042, 11, 341, 307, 689, 11, 5413, 322, 437, 428, 20864, 35799, 11, 286, 478, 516, 281, 584, 264, 22811, 51084], "temperature": 0.0, "avg_logprob": -0.17810870053475364, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.04594254121184349}, {"id": 150, "seek": 80512, "start": 819.52, "end": 824.4, "text": " thing. Dependency grammar. It's like a bunch of people invented this. Tinier was the first", "tokens": [51084, 551, 13, 4056, 521, 3020, 22317, 13, 467, 311, 411, 257, 3840, 295, 561, 14479, 341, 13, 47741, 811, 390, 264, 700, 51328], "temperature": 0.0, "avg_logprob": -0.17810870053475364, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.04594254121184349}, {"id": 151, "seek": 80512, "start": 824.4, "end": 830.0, "text": " French guy back in, I mean, the paper was published in 1959, but he was working on the 30s and stuff.", "tokens": [51328, 5522, 2146, 646, 294, 11, 286, 914, 11, 264, 3035, 390, 6572, 294, 45608, 11, 457, 415, 390, 1364, 322, 264, 2217, 82, 293, 1507, 13, 51608], "temperature": 0.0, "avg_logprob": -0.17810870053475364, "compression_ratio": 1.578358208955224, "no_speech_prob": 0.04594254121184349}, {"id": 152, "seek": 83000, "start": 830.72, "end": 837.84, "text": " And it goes back to, you know, philologist Panini was doing this in ancient India. Okay.", "tokens": [50400, 400, 309, 1709, 646, 281, 11, 291, 458, 11, 903, 388, 9201, 7557, 3812, 390, 884, 341, 294, 7832, 5282, 13, 1033, 13, 50756], "temperature": 0.0, "avg_logprob": -0.15051929767315203, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0019565518014132977}, {"id": 153, "seek": 83000, "start": 837.84, "end": 842.48, "text": " And so, you know, doing something like this, the simplest thing we can think of is that there's", "tokens": [50756, 400, 370, 11, 291, 458, 11, 884, 746, 411, 341, 11, 264, 22811, 551, 321, 393, 519, 295, 307, 300, 456, 311, 50988], "temperature": 0.0, "avg_logprob": -0.15051929767315203, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0019565518014132977}, {"id": 154, "seek": 83000, "start": 842.48, "end": 848.0, "text": " just connections between the words to make the utterance. And so let's just say I have like two", "tokens": [50988, 445, 9271, 1296, 264, 2283, 281, 652, 264, 17567, 719, 13, 400, 370, 718, 311, 445, 584, 286, 362, 411, 732, 51264], "temperature": 0.0, "avg_logprob": -0.15051929767315203, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0019565518014132977}, {"id": 155, "seek": 83000, "start": 848.0, "end": 855.44, "text": " dogs entered a room. Okay. Here's a sentence. And so we're connecting two and dogs together.", "tokens": [51264, 7197, 9065, 257, 1808, 13, 1033, 13, 1692, 311, 257, 8174, 13, 400, 370, 321, 434, 11015, 732, 293, 7197, 1214, 13, 51636], "temperature": 0.0, "avg_logprob": -0.15051929767315203, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0019565518014132977}, {"id": 156, "seek": 83000, "start": 855.44, "end": 858.88, "text": " That's like, there's some dependency between those words to make some bigger meaning.", "tokens": [51636, 663, 311, 411, 11, 456, 311, 512, 33621, 1296, 729, 2283, 281, 652, 512, 3801, 3620, 13, 51808], "temperature": 0.0, "avg_logprob": -0.15051929767315203, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.0019565518014132977}, {"id": 157, "seek": 85888, "start": 858.88, "end": 866.72, "text": " And then we're connecting dogs now to entered, right? And we connect a room somehow to entered.", "tokens": [50364, 400, 550, 321, 434, 11015, 7197, 586, 281, 9065, 11, 558, 30, 400, 321, 1745, 257, 1808, 6063, 281, 9065, 13, 50756], "temperature": 0.0, "avg_logprob": -0.12522664666175842, "compression_ratio": 2.0482456140350878, "no_speech_prob": 0.00040444236947223544}, {"id": 158, "seek": 85888, "start": 866.72, "end": 871.28, "text": " And so I'm going to connect to room and then room back to entered. That's the tree,", "tokens": [50756, 400, 370, 286, 478, 516, 281, 1745, 281, 1808, 293, 550, 1808, 646, 281, 9065, 13, 663, 311, 264, 4230, 11, 50984], "temperature": 0.0, "avg_logprob": -0.12522664666175842, "compression_ratio": 2.0482456140350878, "no_speech_prob": 0.00040444236947223544}, {"id": 159, "seek": 85888, "start": 871.28, "end": 875.52, "text": " is I, the root is entered. That's the thing is like an entering event. That's what we're saying", "tokens": [50984, 307, 286, 11, 264, 5593, 307, 9065, 13, 663, 311, 264, 551, 307, 411, 364, 11104, 2280, 13, 663, 311, 437, 321, 434, 1566, 51196], "temperature": 0.0, "avg_logprob": -0.12522664666175842, "compression_ratio": 2.0482456140350878, "no_speech_prob": 0.00040444236947223544}, {"id": 160, "seek": 85888, "start": 875.52, "end": 881.04, "text": " here. And the subject, which is whatever that dog is, is two dogs, it was, and the connection", "tokens": [51196, 510, 13, 400, 264, 3983, 11, 597, 307, 2035, 300, 3000, 307, 11, 307, 732, 7197, 11, 309, 390, 11, 293, 264, 4984, 51472], "temperature": 0.0, "avg_logprob": -0.12522664666175842, "compression_ratio": 2.0482456140350878, "no_speech_prob": 0.00040444236947223544}, {"id": 161, "seek": 85888, "start": 881.04, "end": 886.4, "text": " goes back to dogs, which goes back to them, then that goes back to two. I'm just, that's my tree.", "tokens": [51472, 1709, 646, 281, 7197, 11, 597, 1709, 646, 281, 552, 11, 550, 300, 1709, 646, 281, 732, 13, 286, 478, 445, 11, 300, 311, 452, 4230, 13, 51740], "temperature": 0.0, "avg_logprob": -0.12522664666175842, "compression_ratio": 2.0482456140350878, "no_speech_prob": 0.00040444236947223544}, {"id": 162, "seek": 88640, "start": 886.4, "end": 892.0799999999999, "text": " It starts at entered, goes to dogs down to two. And then the other side, after the verb,", "tokens": [50364, 467, 3719, 412, 9065, 11, 1709, 281, 7197, 760, 281, 732, 13, 400, 550, 264, 661, 1252, 11, 934, 264, 9595, 11, 50648], "temperature": 0.0, "avg_logprob": -0.11400679884285762, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.000535756116732955}, {"id": 163, "seek": 88640, "start": 892.0799999999999, "end": 897.6, "text": " the object, it goes to room. And then that goes back to the determiner or article, whatever you", "tokens": [50648, 264, 2657, 11, 309, 1709, 281, 1808, 13, 400, 550, 300, 1709, 646, 281, 264, 3618, 4564, 420, 7222, 11, 2035, 291, 50924], "temperature": 0.0, "avg_logprob": -0.11400679884285762, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.000535756116732955}, {"id": 164, "seek": 88640, "start": 897.6, "end": 901.36, "text": " want to call that word. So there's a bunch of categories of words here we're noticing. So", "tokens": [50924, 528, 281, 818, 300, 1349, 13, 407, 456, 311, 257, 3840, 295, 10479, 295, 2283, 510, 321, 434, 21814, 13, 407, 51112], "temperature": 0.0, "avg_logprob": -0.11400679884285762, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.000535756116732955}, {"id": 165, "seek": 88640, "start": 901.36, "end": 908.0799999999999, "text": " there are verbs. Those are these things that typically mark, they refer to events and states", "tokens": [51112, 456, 366, 30051, 13, 3950, 366, 613, 721, 300, 5850, 1491, 11, 436, 2864, 281, 3931, 293, 4368, 51448], "temperature": 0.0, "avg_logprob": -0.11400679884285762, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.000535756116732955}, {"id": 166, "seek": 88640, "start": 908.0799999999999, "end": 912.48, "text": " in the world. And they're nouns, which typically refer to people, places and things is what people", "tokens": [51448, 294, 264, 1002, 13, 400, 436, 434, 48184, 11, 597, 5850, 2864, 281, 561, 11, 3190, 293, 721, 307, 437, 561, 51668], "temperature": 0.0, "avg_logprob": -0.11400679884285762, "compression_ratio": 1.8346456692913387, "no_speech_prob": 0.000535756116732955}, {"id": 167, "seek": 91248, "start": 912.48, "end": 917.12, "text": " say, but they can refer to other more, they can refer to events themselves as well. They're", "tokens": [50364, 584, 11, 457, 436, 393, 2864, 281, 661, 544, 11, 436, 393, 2864, 281, 3931, 2969, 382, 731, 13, 814, 434, 50596], "temperature": 0.0, "avg_logprob": -0.12344368584722067, "compression_ratio": 1.84375, "no_speech_prob": 0.029738744720816612}, {"id": 168, "seek": 91248, "start": 917.12, "end": 923.28, "text": " marked by, you know, how they, how they, the category, the part of speech of a word is how", "tokens": [50596, 12658, 538, 11, 291, 458, 11, 577, 436, 11, 577, 436, 11, 264, 7719, 11, 264, 644, 295, 6218, 295, 257, 1349, 307, 577, 50904], "temperature": 0.0, "avg_logprob": -0.12344368584722067, "compression_ratio": 1.84375, "no_speech_prob": 0.029738744720816612}, {"id": 169, "seek": 91248, "start": 923.28, "end": 928.5600000000001, "text": " it gets used in language. It's like, that's how you decide what the, what the category of a word", "tokens": [50904, 309, 2170, 1143, 294, 2856, 13, 467, 311, 411, 11, 300, 311, 577, 291, 4536, 437, 264, 11, 437, 264, 7719, 295, 257, 1349, 51168], "temperature": 0.0, "avg_logprob": -0.12344368584722067, "compression_ratio": 1.84375, "no_speech_prob": 0.029738744720816612}, {"id": 170, "seek": 91248, "start": 928.5600000000001, "end": 934.4, "text": " is not, not by the meaning, but how it's, how it gets used. What's usually the root? Is it going", "tokens": [51168, 307, 406, 11, 406, 538, 264, 3620, 11, 457, 577, 309, 311, 11, 577, 309, 2170, 1143, 13, 708, 311, 2673, 264, 5593, 30, 1119, 309, 516, 51460], "temperature": 0.0, "avg_logprob": -0.12344368584722067, "compression_ratio": 1.84375, "no_speech_prob": 0.029738744720816612}, {"id": 171, "seek": 91248, "start": 934.4, "end": 940.64, "text": " to be the verb that defies the event? Yes. Yes. Yes. Okay. Yeah. I mean, if I don't say a verb,", "tokens": [51460, 281, 312, 264, 9595, 300, 1060, 530, 264, 2280, 30, 1079, 13, 1079, 13, 1079, 13, 1033, 13, 865, 13, 286, 914, 11, 498, 286, 500, 380, 584, 257, 9595, 11, 51772], "temperature": 0.0, "avg_logprob": -0.12344368584722067, "compression_ratio": 1.84375, "no_speech_prob": 0.029738744720816612}, {"id": 172, "seek": 94064, "start": 940.64, "end": 943.4399999999999, "text": " then there won't be a verb. And so it'll be something else. What if you're messing, are", "tokens": [50364, 550, 456, 1582, 380, 312, 257, 9595, 13, 400, 370, 309, 603, 312, 746, 1646, 13, 708, 498, 291, 434, 23258, 11, 366, 50504], "temperature": 0.0, "avg_logprob": -0.13383034780515846, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005059650633484125}, {"id": 173, "seek": 94064, "start": 943.4399999999999, "end": 947.52, "text": " we talking about language that's like correct language? What if you're doing poetry and messing", "tokens": [50504, 321, 1417, 466, 2856, 300, 311, 411, 3006, 2856, 30, 708, 498, 291, 434, 884, 15155, 293, 23258, 50708], "temperature": 0.0, "avg_logprob": -0.13383034780515846, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005059650633484125}, {"id": 174, "seek": 94064, "start": 947.52, "end": 953.36, "text": " with stuff? Is it then, then rules got the window, right? Then it's, no, you're still, no, no, no,", "tokens": [50708, 365, 1507, 30, 1119, 309, 550, 11, 550, 4474, 658, 264, 4910, 11, 558, 30, 1396, 309, 311, 11, 572, 11, 291, 434, 920, 11, 572, 11, 572, 11, 572, 11, 51000], "temperature": 0.0, "avg_logprob": -0.13383034780515846, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005059650633484125}, {"id": 175, "seek": 94064, "start": 953.36, "end": 957.1999999999999, "text": " no, you're constrained by whatever language you're dealing with. Probably you have other", "tokens": [51000, 572, 11, 291, 434, 38901, 538, 2035, 2856, 291, 434, 6260, 365, 13, 9210, 291, 362, 661, 51192], "temperature": 0.0, "avg_logprob": -0.13383034780515846, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005059650633484125}, {"id": 176, "seek": 94064, "start": 957.1999999999999, "end": 961.76, "text": " constraints in poetry, such that you're like usually in poetry, there's multiple constraints", "tokens": [51192, 18491, 294, 15155, 11, 1270, 300, 291, 434, 411, 2673, 294, 15155, 11, 456, 311, 3866, 18491, 51420], "temperature": 0.0, "avg_logprob": -0.13383034780515846, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005059650633484125}, {"id": 177, "seek": 94064, "start": 961.76, "end": 966.3199999999999, "text": " that you want to, like you want to usually convey multiple meanings is the idea. And maybe you have", "tokens": [51420, 300, 291, 528, 281, 11, 411, 291, 528, 281, 2673, 16965, 3866, 28138, 307, 264, 1558, 13, 400, 1310, 291, 362, 51648], "temperature": 0.0, "avg_logprob": -0.13383034780515846, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005059650633484125}, {"id": 178, "seek": 96632, "start": 966.32, "end": 971.44, "text": " like a rhythm or a rhyming structure as well. And depending on, so, but you usually are constrained", "tokens": [50364, 411, 257, 11801, 420, 257, 8740, 2810, 3877, 382, 731, 13, 400, 5413, 322, 11, 370, 11, 457, 291, 2673, 366, 38901, 50620], "temperature": 0.0, "avg_logprob": -0.09822705435374426, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.06950489431619644}, {"id": 179, "seek": 96632, "start": 971.44, "end": 977.44, "text": " by your, the rules of your language for the most part. And so you don't violate those too much.", "tokens": [50620, 538, 428, 11, 264, 4474, 295, 428, 2856, 337, 264, 881, 644, 13, 400, 370, 291, 500, 380, 37478, 729, 886, 709, 13, 50920], "temperature": 0.0, "avg_logprob": -0.09822705435374426, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.06950489431619644}, {"id": 180, "seek": 96632, "start": 977.44, "end": 982.4000000000001, "text": " You can violate them somewhat, but not too much. So it has to be recognizable as your language.", "tokens": [50920, 509, 393, 37478, 552, 8344, 11, 457, 406, 886, 709, 13, 407, 309, 575, 281, 312, 40757, 382, 428, 2856, 13, 51168], "temperature": 0.0, "avg_logprob": -0.09822705435374426, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.06950489431619644}, {"id": 181, "seek": 96632, "start": 982.4000000000001, "end": 989.2800000000001, "text": " Like in English, I can't say dogs to entered room. Ah, I mean, I meant that, you know, two dogs", "tokens": [51168, 1743, 294, 3669, 11, 286, 393, 380, 584, 7197, 281, 9065, 1808, 13, 2438, 11, 286, 914, 11, 286, 4140, 300, 11, 291, 458, 11, 732, 7197, 51512], "temperature": 0.0, "avg_logprob": -0.09822705435374426, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.06950489431619644}, {"id": 182, "seek": 96632, "start": 989.2800000000001, "end": 995.84, "text": " entered a room and I can't mess with the order of the, the articles and the articles and the", "tokens": [51512, 9065, 257, 1808, 293, 286, 393, 380, 2082, 365, 264, 1668, 295, 264, 11, 264, 11290, 293, 264, 11290, 293, 264, 51840], "temperature": 0.0, "avg_logprob": -0.09822705435374426, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.06950489431619644}, {"id": 183, "seek": 99584, "start": 995.84, "end": 1001.12, "text": " nouns. You just can't do that. In some languages, you can, you can mess around with the order", "tokens": [50364, 48184, 13, 509, 445, 393, 380, 360, 300, 13, 682, 512, 8650, 11, 291, 393, 11, 291, 393, 2082, 926, 365, 264, 1668, 50628], "temperature": 0.0, "avg_logprob": -0.07916871479579381, "compression_ratio": 2.0216606498194944, "no_speech_prob": 0.0034815927501767874}, {"id": 184, "seek": 99584, "start": 1001.12, "end": 1006.32, "text": " of words much more. I mean, you speak Russian. Russian has a much freer word order than English.", "tokens": [50628, 295, 2283, 709, 544, 13, 286, 914, 11, 291, 1710, 7220, 13, 7220, 575, 257, 709, 2130, 260, 1349, 1668, 813, 3669, 13, 50888], "temperature": 0.0, "avg_logprob": -0.07916871479579381, "compression_ratio": 2.0216606498194944, "no_speech_prob": 0.0034815927501767874}, {"id": 185, "seek": 99584, "start": 1006.32, "end": 1011.0400000000001, "text": " And so in fact, you can move around words in, you know, I told you that English has the subject,", "tokens": [50888, 400, 370, 294, 1186, 11, 291, 393, 1286, 926, 2283, 294, 11, 291, 458, 11, 286, 1907, 291, 300, 3669, 575, 264, 3983, 11, 51124], "temperature": 0.0, "avg_logprob": -0.07916871479579381, "compression_ratio": 2.0216606498194944, "no_speech_prob": 0.0034815927501767874}, {"id": 186, "seek": 99584, "start": 1011.0400000000001, "end": 1016.08, "text": " verb, object, word order. So does Russian, but Russian is much freer than English. And so you", "tokens": [51124, 9595, 11, 2657, 11, 1349, 1668, 13, 407, 775, 7220, 11, 457, 7220, 307, 709, 2130, 260, 813, 3669, 13, 400, 370, 291, 51376], "temperature": 0.0, "avg_logprob": -0.07916871479579381, "compression_ratio": 2.0216606498194944, "no_speech_prob": 0.0034815927501767874}, {"id": 187, "seek": 99584, "start": 1016.08, "end": 1020.72, "text": " can actually mess around with the word order. So probably Russian poetry is going to be quite", "tokens": [51376, 393, 767, 2082, 926, 365, 264, 1349, 1668, 13, 407, 1391, 7220, 15155, 307, 516, 281, 312, 1596, 51608], "temperature": 0.0, "avg_logprob": -0.07916871479579381, "compression_ratio": 2.0216606498194944, "no_speech_prob": 0.0034815927501767874}, {"id": 188, "seek": 99584, "start": 1020.72, "end": 1025.04, "text": " different from English poetry because the word order is much less constrained. Yeah,", "tokens": [51608, 819, 490, 3669, 15155, 570, 264, 1349, 1668, 307, 709, 1570, 38901, 13, 865, 11, 51824], "temperature": 0.0, "avg_logprob": -0.07916871479579381, "compression_ratio": 2.0216606498194944, "no_speech_prob": 0.0034815927501767874}, {"id": 189, "seek": 102504, "start": 1025.12, "end": 1031.04, "text": " there's a much more extensive culture of poetry throughout the history of the last 100 years", "tokens": [50368, 456, 311, 257, 709, 544, 13246, 3713, 295, 15155, 3710, 264, 2503, 295, 264, 1036, 2319, 924, 50664], "temperature": 0.0, "avg_logprob": -0.12044534017873365, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.00039807724533602595}, {"id": 190, "seek": 102504, "start": 1031.04, "end": 1036.6399999999999, "text": " in Russia. And I always wondered why that is, but it seems that there's more flexibility", "tokens": [50664, 294, 6797, 13, 400, 286, 1009, 17055, 983, 300, 307, 11, 457, 309, 2544, 300, 456, 311, 544, 12635, 50944], "temperature": 0.0, "avg_logprob": -0.12044534017873365, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.00039807724533602595}, {"id": 191, "seek": 102504, "start": 1037.36, "end": 1042.08, "text": " in the way the language is used. There's more, you're more female language easier by", "tokens": [50980, 294, 264, 636, 264, 2856, 307, 1143, 13, 821, 311, 544, 11, 291, 434, 544, 6556, 2856, 3571, 538, 51216], "temperature": 0.0, "avg_logprob": -0.12044534017873365, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.00039807724533602595}, {"id": 192, "seek": 102504, "start": 1042.8, "end": 1045.92, "text": " altering the words, altering the order of the words, messing with it.", "tokens": [51252, 11337, 278, 264, 2283, 11, 11337, 278, 264, 1668, 295, 264, 2283, 11, 23258, 365, 309, 13, 51408], "temperature": 0.0, "avg_logprob": -0.12044534017873365, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.00039807724533602595}, {"id": 193, "seek": 102504, "start": 1045.92, "end": 1049.68, "text": " Well, you can just mess with different things in each language. And so in Russian, you have", "tokens": [51408, 1042, 11, 291, 393, 445, 2082, 365, 819, 721, 294, 1184, 2856, 13, 400, 370, 294, 7220, 11, 291, 362, 51596], "temperature": 0.0, "avg_logprob": -0.12044534017873365, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.00039807724533602595}, {"id": 194, "seek": 102504, "start": 1049.68, "end": 1054.96, "text": " case markers, right? On the end, which is these endings on the nouns, which tell you how it can", "tokens": [51596, 1389, 19175, 11, 558, 30, 1282, 264, 917, 11, 597, 307, 613, 42474, 322, 264, 48184, 11, 597, 980, 291, 577, 309, 393, 51860], "temperature": 0.0, "avg_logprob": -0.12044534017873365, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.00039807724533602595}, {"id": 195, "seek": 105496, "start": 1055.28, "end": 1058.96, "text": " each noun connects to the verb, right? We don't have that in English. And so when I say", "tokens": [50380, 1184, 23307, 16967, 281, 264, 9595, 11, 558, 30, 492, 500, 380, 362, 300, 294, 3669, 13, 400, 370, 562, 286, 584, 50564], "temperature": 0.0, "avg_logprob": -0.11174430269183534, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0035912280436605215}, {"id": 196, "seek": 105496, "start": 1060.0, "end": 1065.52, "text": " Mary kissed John, I don't know who the agent or the patient is, except by the order of the words,", "tokens": [50616, 6059, 33027, 2619, 11, 286, 500, 380, 458, 567, 264, 9461, 420, 264, 4537, 307, 11, 3993, 538, 264, 1668, 295, 264, 2283, 11, 50892], "temperature": 0.0, "avg_logprob": -0.11174430269183534, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0035912280436605215}, {"id": 197, "seek": 105496, "start": 1065.52, "end": 1069.68, "text": " right? In Russian, you actually have a marker. On the end, if you're using a Russian name and", "tokens": [50892, 558, 30, 682, 7220, 11, 291, 767, 362, 257, 15247, 13, 1282, 264, 917, 11, 498, 291, 434, 1228, 257, 7220, 1315, 293, 51100], "temperature": 0.0, "avg_logprob": -0.11174430269183534, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0035912280436605215}, {"id": 198, "seek": 105496, "start": 1069.68, "end": 1075.28, "text": " each of those names, you'll also say, is it, you know, agent, it'll be the, you know, nominative,", "tokens": [51100, 1184, 295, 729, 5288, 11, 291, 603, 611, 584, 11, 307, 309, 11, 291, 458, 11, 9461, 11, 309, 603, 312, 264, 11, 291, 458, 11, 5369, 259, 1166, 11, 51380], "temperature": 0.0, "avg_logprob": -0.11174430269183534, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0035912280436605215}, {"id": 199, "seek": 105496, "start": 1075.28, "end": 1079.76, "text": " which is marking the subject, or an accusative will mark the object. And you could put them in", "tokens": [51380, 597, 307, 25482, 264, 3983, 11, 420, 364, 11168, 1166, 486, 1491, 264, 2657, 13, 400, 291, 727, 829, 552, 294, 51604], "temperature": 0.0, "avg_logprob": -0.11174430269183534, "compression_ratio": 1.673758865248227, "no_speech_prob": 0.0035912280436605215}, {"id": 200, "seek": 107976, "start": 1079.76, "end": 1084.64, "text": " the reverse order. You could put accusative first, as you could put subject, you could put", "tokens": [50364, 264, 9943, 1668, 13, 509, 727, 829, 11168, 1166, 700, 11, 382, 291, 727, 829, 3983, 11, 291, 727, 829, 50608], "temperature": 0.0, "avg_logprob": -0.1455731709798177, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.014724223874509335}, {"id": 201, "seek": 107976, "start": 1085.6, "end": 1090.8799999999999, "text": " the patient first, and then the verb, and then the, the, the subject, and that would be a perfectly", "tokens": [50656, 264, 4537, 700, 11, 293, 550, 264, 9595, 11, 293, 550, 264, 11, 264, 11, 264, 3983, 11, 293, 300, 576, 312, 257, 6239, 50920], "temperature": 0.0, "avg_logprob": -0.1455731709798177, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.014724223874509335}, {"id": 202, "seek": 107976, "start": 1090.8799999999999, "end": 1095.84, "text": " good Russian sentence. And it would still mean Mary, I could say John kissed Mary, meaning Mary", "tokens": [50920, 665, 7220, 8174, 13, 400, 309, 576, 920, 914, 6059, 11, 286, 727, 584, 2619, 33027, 6059, 11, 3620, 6059, 51168], "temperature": 0.0, "avg_logprob": -0.1455731709798177, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.014724223874509335}, {"id": 203, "seek": 107976, "start": 1095.84, "end": 1100.8, "text": " kissed John, as long as I use the case markers in the right way, you can't do that in English.", "tokens": [51168, 33027, 2619, 11, 382, 938, 382, 286, 764, 264, 1389, 19175, 294, 264, 558, 636, 11, 291, 393, 380, 360, 300, 294, 3669, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1455731709798177, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.014724223874509335}, {"id": 204, "seek": 107976, "start": 1100.8, "end": 1106.72, "text": " And so I love the terminology of agent and patient. And, and the other ones you use,", "tokens": [51416, 400, 370, 286, 959, 264, 27575, 295, 9461, 293, 4537, 13, 400, 11, 293, 264, 661, 2306, 291, 764, 11, 51712], "temperature": 0.0, "avg_logprob": -0.1455731709798177, "compression_ratio": 1.841897233201581, "no_speech_prob": 0.014724223874509335}, {"id": 205, "seek": 110672, "start": 1107.3600000000001, "end": 1110.96, "text": " those are sort of linguistic terms, correct? Those are, those are for like kind of meaning,", "tokens": [50396, 729, 366, 1333, 295, 43002, 2115, 11, 3006, 30, 3950, 366, 11, 729, 366, 337, 411, 733, 295, 3620, 11, 50576], "temperature": 0.0, "avg_logprob": -0.12041689001995584, "compression_ratio": 1.9741697416974169, "no_speech_prob": 0.008184907957911491}, {"id": 206, "seek": 110672, "start": 1110.96, "end": 1115.84, "text": " those are meaning. And, and subject and object are generally used for position. So subject is", "tokens": [50576, 729, 366, 3620, 13, 400, 11, 293, 3983, 293, 2657, 366, 5101, 1143, 337, 2535, 13, 407, 3983, 307, 50820], "temperature": 0.0, "avg_logprob": -0.12041689001995584, "compression_ratio": 1.9741697416974169, "no_speech_prob": 0.008184907957911491}, {"id": 207, "seek": 110672, "start": 1115.84, "end": 1120.0, "text": " just like the thing that comes before the verb. And the object is when it comes after the verb.", "tokens": [50820, 445, 411, 264, 551, 300, 1487, 949, 264, 9595, 13, 400, 264, 2657, 307, 562, 309, 1487, 934, 264, 9595, 13, 51028], "temperature": 0.0, "avg_logprob": -0.12041689001995584, "compression_ratio": 1.9741697416974169, "no_speech_prob": 0.008184907957911491}, {"id": 208, "seek": 110672, "start": 1120.0, "end": 1124.16, "text": " The agent is kind of like the thing doing it. That's kind of what that means, right? The,", "tokens": [51028, 440, 9461, 307, 733, 295, 411, 264, 551, 884, 309, 13, 663, 311, 733, 295, 437, 300, 1355, 11, 558, 30, 440, 11, 51236], "temperature": 0.0, "avg_logprob": -0.12041689001995584, "compression_ratio": 1.9741697416974169, "no_speech_prob": 0.008184907957911491}, {"id": 209, "seek": 110672, "start": 1124.16, "end": 1128.48, "text": " the, the subject is often the person doing the action, right? The thing. So yeah.", "tokens": [51236, 264, 11, 264, 3983, 307, 2049, 264, 954, 884, 264, 3069, 11, 558, 30, 440, 551, 13, 407, 1338, 13, 51452], "temperature": 0.0, "avg_logprob": -0.12041689001995584, "compression_ratio": 1.9741697416974169, "no_speech_prob": 0.008184907957911491}, {"id": 210, "seek": 110672, "start": 1128.48, "end": 1132.0, "text": " Okay. This is fascinating. So how hard is it to form a tree in general? Is there,", "tokens": [51452, 1033, 13, 639, 307, 10343, 13, 407, 577, 1152, 307, 309, 281, 1254, 257, 4230, 294, 2674, 30, 1119, 456, 11, 51628], "temperature": 0.0, "avg_logprob": -0.12041689001995584, "compression_ratio": 1.9741697416974169, "no_speech_prob": 0.008184907957911491}, {"id": 211, "seek": 113200, "start": 1132.4, "end": 1137.36, "text": " is there a procedure to it? Like if you look at different languages, is it supposed to be a", "tokens": [50384, 307, 456, 257, 10747, 281, 309, 30, 1743, 498, 291, 574, 412, 819, 8650, 11, 307, 309, 3442, 281, 312, 257, 50632], "temperature": 0.0, "avg_logprob": -0.120115292377961, "compression_ratio": 1.8098159509202454, "no_speech_prob": 0.004330650437623262}, {"id": 212, "seek": 113200, "start": 1137.36, "end": 1141.04, "text": " very natural, like is it automatable or is there some human genius involved in?", "tokens": [50632, 588, 3303, 11, 411, 307, 309, 28034, 712, 420, 307, 456, 512, 1952, 14017, 3288, 294, 30, 50816], "temperature": 0.0, "avg_logprob": -0.120115292377961, "compression_ratio": 1.8098159509202454, "no_speech_prob": 0.004330650437623262}, {"id": 213, "seek": 113200, "start": 1141.04, "end": 1144.64, "text": " I think it's pretty automatable at this point. People can figure out the words are,", "tokens": [50816, 286, 519, 309, 311, 1238, 28034, 712, 412, 341, 935, 13, 3432, 393, 2573, 484, 264, 2283, 366, 11, 50996], "temperature": 0.0, "avg_logprob": -0.120115292377961, "compression_ratio": 1.8098159509202454, "no_speech_prob": 0.004330650437623262}, {"id": 214, "seek": 113200, "start": 1144.64, "end": 1148.0, "text": " they can figure out the morphemes, which are the, technically morphemes are the,", "tokens": [50996, 436, 393, 2573, 484, 264, 25778, 443, 279, 11, 597, 366, 264, 11, 12120, 25778, 443, 279, 366, 264, 11, 51164], "temperature": 0.0, "avg_logprob": -0.120115292377961, "compression_ratio": 1.8098159509202454, "no_speech_prob": 0.004330650437623262}, {"id": 215, "seek": 113200, "start": 1148.64, "end": 1153.6, "text": " the minimal meaning units within a language. Okay. And so when you say eats or drinks,", "tokens": [51196, 264, 13206, 3620, 6815, 1951, 257, 2856, 13, 1033, 13, 400, 370, 562, 291, 584, 18109, 420, 12142, 11, 51444], "temperature": 0.0, "avg_logprob": -0.120115292377961, "compression_ratio": 1.8098159509202454, "no_speech_prob": 0.004330650437623262}, {"id": 216, "seek": 113200, "start": 1153.6, "end": 1156.8, "text": " it actually has two morphemes in an English. There's, there's the, there's the root,", "tokens": [51444, 309, 767, 575, 732, 25778, 443, 279, 294, 364, 3669, 13, 821, 311, 11, 456, 311, 264, 11, 456, 311, 264, 5593, 11, 51604], "temperature": 0.0, "avg_logprob": -0.120115292377961, "compression_ratio": 1.8098159509202454, "no_speech_prob": 0.004330650437623262}, {"id": 217, "seek": 113200, "start": 1156.8, "end": 1160.32, "text": " which is the verb. And then there's some ending on it, which tells you, you know,", "tokens": [51604, 597, 307, 264, 9595, 13, 400, 550, 456, 311, 512, 8121, 322, 309, 11, 597, 5112, 291, 11, 291, 458, 11, 51780], "temperature": 0.0, "avg_logprob": -0.120115292377961, "compression_ratio": 1.8098159509202454, "no_speech_prob": 0.004330650437623262}, {"id": 218, "seek": 116032, "start": 1160.32, "end": 1164.48, "text": " that's this third person, third person singular. Can you say what morphemes are?", "tokens": [50364, 300, 311, 341, 2636, 954, 11, 2636, 954, 20010, 13, 1664, 291, 584, 437, 25778, 443, 279, 366, 30, 50572], "temperature": 0.0, "avg_logprob": -0.1393527622464337, "compression_ratio": 1.9554140127388535, "no_speech_prob": 0.0018097638385370374}, {"id": 219, "seek": 116032, "start": 1164.48, "end": 1168.08, "text": " Morphemes are just the minimal meaning units within a language. And then a word is just", "tokens": [50572, 5146, 950, 443, 279, 366, 445, 264, 13206, 3620, 6815, 1951, 257, 2856, 13, 400, 550, 257, 1349, 307, 445, 50752], "temperature": 0.0, "avg_logprob": -0.1393527622464337, "compression_ratio": 1.9554140127388535, "no_speech_prob": 0.0018097638385370374}, {"id": 220, "seek": 116032, "start": 1168.08, "end": 1171.52, "text": " kind of the things we put spaces between English and 10. They have a little bit more,", "tokens": [50752, 733, 295, 264, 721, 321, 829, 7673, 1296, 3669, 293, 1266, 13, 814, 362, 257, 707, 857, 544, 11, 50924], "temperature": 0.0, "avg_logprob": -0.1393527622464337, "compression_ratio": 1.9554140127388535, "no_speech_prob": 0.0018097638385370374}, {"id": 221, "seek": 116032, "start": 1171.52, "end": 1176.1599999999999, "text": " they have the morphology as well. They have the endings, this inflexual morphology on the endings", "tokens": [50924, 436, 362, 264, 25778, 1793, 382, 731, 13, 814, 362, 264, 42474, 11, 341, 1536, 2021, 901, 25778, 1793, 322, 264, 42474, 51156], "temperature": 0.0, "avg_logprob": -0.1393527622464337, "compression_ratio": 1.9554140127388535, "no_speech_prob": 0.0018097638385370374}, {"id": 222, "seek": 116032, "start": 1176.1599999999999, "end": 1179.9199999999998, "text": " on the roots. They modify something about the word that adds additional meaning.", "tokens": [51156, 322, 264, 10669, 13, 814, 16927, 746, 466, 264, 1349, 300, 10860, 4497, 3620, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1393527622464337, "compression_ratio": 1.9554140127388535, "no_speech_prob": 0.0018097638385370374}, {"id": 223, "seek": 116032, "start": 1179.9199999999998, "end": 1183.04, "text": " They tell you, yeah, yeah. And so we have a little bit of that in English, very little,", "tokens": [51344, 814, 980, 291, 11, 1338, 11, 1338, 13, 400, 370, 321, 362, 257, 707, 857, 295, 300, 294, 3669, 11, 588, 707, 11, 51500], "temperature": 0.0, "avg_logprob": -0.1393527622464337, "compression_ratio": 1.9554140127388535, "no_speech_prob": 0.0018097638385370374}, {"id": 224, "seek": 116032, "start": 1183.04, "end": 1187.12, "text": " much more in Russian, for instance. And, and, but we have a little bit in English. And so we", "tokens": [51500, 709, 544, 294, 7220, 11, 337, 5197, 13, 400, 11, 293, 11, 457, 321, 362, 257, 707, 857, 294, 3669, 13, 400, 370, 321, 51704], "temperature": 0.0, "avg_logprob": -0.1393527622464337, "compression_ratio": 1.9554140127388535, "no_speech_prob": 0.0018097638385370374}, {"id": 225, "seek": 118712, "start": 1187.12, "end": 1191.36, "text": " have a little on the, on the nouns, you can say it's either singular or plural. And, and you can", "tokens": [50364, 362, 257, 707, 322, 264, 11, 322, 264, 48184, 11, 291, 393, 584, 309, 311, 2139, 20010, 420, 25377, 13, 400, 11, 293, 291, 393, 50576], "temperature": 0.0, "avg_logprob": -0.12347975009825171, "compression_ratio": 1.9822695035460993, "no_speech_prob": 0.0033761716913431883}, {"id": 226, "seek": 118712, "start": 1191.36, "end": 1197.12, "text": " say, same thing for, for, for verbs, like simple past tense, for example, it's like, you know,", "tokens": [50576, 584, 11, 912, 551, 337, 11, 337, 11, 337, 30051, 11, 411, 2199, 1791, 18760, 11, 337, 1365, 11, 309, 311, 411, 11, 291, 458, 11, 50864], "temperature": 0.0, "avg_logprob": -0.12347975009825171, "compression_ratio": 1.9822695035460993, "no_speech_prob": 0.0033761716913431883}, {"id": 227, "seek": 118712, "start": 1197.12, "end": 1201.6799999999998, "text": " notice in English, we say drinks, you know, he drinks, but everyone else says, I drink,", "tokens": [50864, 3449, 294, 3669, 11, 321, 584, 12142, 11, 291, 458, 11, 415, 12142, 11, 457, 1518, 1646, 1619, 11, 286, 2822, 11, 51092], "temperature": 0.0, "avg_logprob": -0.12347975009825171, "compression_ratio": 1.9822695035460993, "no_speech_prob": 0.0033761716913431883}, {"id": 228, "seek": 118712, "start": 1201.6799999999998, "end": 1206.56, "text": " you drink, we drink, it's unmarked in a way. And then, but in the past tense, it's just drank", "tokens": [51092, 291, 2822, 11, 321, 2822, 11, 309, 311, 517, 5638, 292, 294, 257, 636, 13, 400, 550, 11, 457, 294, 264, 1791, 18760, 11, 309, 311, 445, 21011, 51336], "temperature": 0.0, "avg_logprob": -0.12347975009825171, "compression_ratio": 1.9822695035460993, "no_speech_prob": 0.0033761716913431883}, {"id": 229, "seek": 118712, "start": 1206.56, "end": 1210.7199999999998, "text": " for everyone. There's no morphology at all for past tense. There is morphology, it's marking", "tokens": [51336, 337, 1518, 13, 821, 311, 572, 25778, 1793, 412, 439, 337, 1791, 18760, 13, 821, 307, 25778, 1793, 11, 309, 311, 25482, 51544], "temperature": 0.0, "avg_logprob": -0.12347975009825171, "compression_ratio": 1.9822695035460993, "no_speech_prob": 0.0033761716913431883}, {"id": 230, "seek": 118712, "start": 1210.7199999999998, "end": 1215.4399999999998, "text": " past tense, but it's kind of, it's an irregular now. So we don't even, you know, it drink to", "tokens": [51544, 1791, 18760, 11, 457, 309, 311, 733, 295, 11, 309, 311, 364, 29349, 586, 13, 407, 321, 500, 380, 754, 11, 291, 458, 11, 309, 2822, 281, 51780], "temperature": 0.0, "avg_logprob": -0.12347975009825171, "compression_ratio": 1.9822695035460993, "no_speech_prob": 0.0033761716913431883}, {"id": 231, "seek": 121544, "start": 1215.44, "end": 1220.16, "text": " drink, you know, it's not even a regular word. So in most verbs, many verbs, there's an ED,", "tokens": [50364, 2822, 11, 291, 458, 11, 309, 311, 406, 754, 257, 3890, 1349, 13, 407, 294, 881, 30051, 11, 867, 30051, 11, 456, 311, 364, 18050, 11, 50600], "temperature": 0.0, "avg_logprob": -0.20827838352748326, "compression_ratio": 2.021505376344086, "no_speech_prob": 0.007344264071434736}, {"id": 232, "seek": 121544, "start": 1220.16, "end": 1224.72, "text": " we kind of add. So walk to walked, we add that to say it's the past tense, that I just happened", "tokens": [50600, 321, 733, 295, 909, 13, 407, 1792, 281, 7628, 11, 321, 909, 300, 281, 584, 309, 311, 264, 1791, 18760, 11, 300, 286, 445, 2011, 50828], "temperature": 0.0, "avg_logprob": -0.20827838352748326, "compression_ratio": 2.021505376344086, "no_speech_prob": 0.007344264071434736}, {"id": 233, "seek": 121544, "start": 1224.72, "end": 1228.56, "text": " to choose an irregular because the high frequency word and the high frequency words tend to have", "tokens": [50828, 281, 2826, 364, 29349, 570, 264, 1090, 7893, 1349, 293, 264, 1090, 7893, 2283, 3928, 281, 362, 51020], "temperature": 0.0, "avg_logprob": -0.20827838352748326, "compression_ratio": 2.021505376344086, "no_speech_prob": 0.007344264071434736}, {"id": 234, "seek": 121544, "start": 1228.56, "end": 1233.2, "text": " irregular as in English for. What's an irregular? Irregular is just, there's, there isn't a rule.", "tokens": [51020, 29349, 382, 294, 3669, 337, 13, 708, 311, 364, 29349, 30, 9151, 26713, 307, 445, 11, 456, 311, 11, 456, 1943, 380, 257, 4978, 13, 51252], "temperature": 0.0, "avg_logprob": -0.20827838352748326, "compression_ratio": 2.021505376344086, "no_speech_prob": 0.007344264071434736}, {"id": 235, "seek": 121544, "start": 1233.2, "end": 1238.72, "text": " So drink to drink is an, it's an irregular. Drink, drink, okay, as opposed to walk, walked,", "tokens": [51252, 407, 2822, 281, 2822, 307, 364, 11, 309, 311, 364, 29349, 13, 24529, 11, 2822, 11, 1392, 11, 382, 8851, 281, 1792, 11, 7628, 11, 51528], "temperature": 0.0, "avg_logprob": -0.20827838352748326, "compression_ratio": 2.021505376344086, "no_speech_prob": 0.007344264071434736}, {"id": 236, "seek": 121544, "start": 1238.72, "end": 1242.88, "text": " talked, talked. And there's a lot of irregular, irregular as in English. There's a lot of", "tokens": [51528, 2825, 11, 2825, 13, 400, 456, 311, 257, 688, 295, 29349, 11, 29349, 382, 294, 3669, 13, 821, 311, 257, 688, 295, 51736], "temperature": 0.0, "avg_logprob": -0.20827838352748326, "compression_ratio": 2.021505376344086, "no_speech_prob": 0.007344264071434736}, {"id": 237, "seek": 124288, "start": 1242.88, "end": 1247.2800000000002, "text": " irregular as in English. The, the, the frequent ones, the common words tend to be irregular.", "tokens": [50364, 29349, 382, 294, 3669, 13, 440, 11, 264, 11, 264, 18004, 2306, 11, 264, 2689, 2283, 3928, 281, 312, 29349, 13, 50584], "temperature": 0.0, "avg_logprob": -0.15743623930832434, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.016394052654504776}, {"id": 238, "seek": 124288, "start": 1247.2800000000002, "end": 1252.96, "text": " They'll let, there's many, many more low frequency words and those tend to be, those irregular ones.", "tokens": [50584, 814, 603, 718, 11, 456, 311, 867, 11, 867, 544, 2295, 7893, 2283, 293, 729, 3928, 281, 312, 11, 729, 29349, 2306, 13, 50868], "temperature": 0.0, "avg_logprob": -0.15743623930832434, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.016394052654504776}, {"id": 239, "seek": 124288, "start": 1252.96, "end": 1257.68, "text": " The evolution of the irregular is fascinating. It's essentially slang that's sticky because", "tokens": [50868, 440, 9303, 295, 264, 29349, 307, 10343, 13, 467, 311, 4476, 42517, 300, 311, 14470, 570, 51104], "temperature": 0.0, "avg_logprob": -0.15743623930832434, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.016394052654504776}, {"id": 240, "seek": 124288, "start": 1257.68, "end": 1262.3200000000002, "text": " you're breaking the rules and then everybody use it and doesn't follow the rules. And they,", "tokens": [51104, 291, 434, 7697, 264, 4474, 293, 550, 2201, 764, 309, 293, 1177, 380, 1524, 264, 4474, 13, 400, 436, 11, 51336], "temperature": 0.0, "avg_logprob": -0.15743623930832434, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.016394052654504776}, {"id": 241, "seek": 124288, "start": 1262.3200000000002, "end": 1267.68, "text": " they say screw it to the rules. It's fascinating. So you said it morphemes, lots of questions.", "tokens": [51336, 436, 584, 5630, 309, 281, 264, 4474, 13, 467, 311, 10343, 13, 407, 291, 848, 309, 25778, 443, 279, 11, 3195, 295, 1651, 13, 51604], "temperature": 0.0, "avg_logprob": -0.15743623930832434, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.016394052654504776}, {"id": 242, "seek": 126768, "start": 1267.68, "end": 1272.8, "text": " So morphology is what, the study of morphemes? Morphology is the, is the connections between", "tokens": [50364, 407, 25778, 1793, 307, 437, 11, 264, 2979, 295, 25778, 443, 279, 30, 5146, 950, 1793, 307, 264, 11, 307, 264, 9271, 1296, 50620], "temperature": 0.0, "avg_logprob": -0.09750556144393793, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.013631163164973259}, {"id": 243, "seek": 126768, "start": 1272.8, "end": 1277.2, "text": " the morphemes onto the roots, the roots. So in English, we mostly have suffixes. We have", "tokens": [50620, 264, 25778, 443, 279, 3911, 264, 10669, 11, 264, 10669, 13, 407, 294, 3669, 11, 321, 5240, 362, 3889, 36005, 13, 492, 362, 50840], "temperature": 0.0, "avg_logprob": -0.09750556144393793, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.013631163164973259}, {"id": 244, "seek": 126768, "start": 1277.2, "end": 1281.76, "text": " endings on the words, not very much, but a little bit. And as opposed to prefixes,", "tokens": [50840, 42474, 322, 264, 2283, 11, 406, 588, 709, 11, 457, 257, 707, 857, 13, 400, 382, 8851, 281, 18417, 36005, 11, 51068], "temperature": 0.0, "avg_logprob": -0.09750556144393793, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.013631163164973259}, {"id": 245, "seek": 126768, "start": 1281.76, "end": 1288.0, "text": " some words, depending on your language can have, you know, mostly prefixes, mostly suffixes or", "tokens": [51068, 512, 2283, 11, 5413, 322, 428, 2856, 393, 362, 11, 291, 458, 11, 5240, 18417, 36005, 11, 5240, 3889, 36005, 420, 51380], "temperature": 0.0, "avg_logprob": -0.09750556144393793, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.013631163164973259}, {"id": 246, "seek": 126768, "start": 1288.0, "end": 1292.96, "text": " mostly, or both. And then even languages, several languages have things called infixes where you", "tokens": [51380, 5240, 11, 420, 1293, 13, 400, 550, 754, 8650, 11, 2940, 8650, 362, 721, 1219, 1536, 36005, 689, 291, 51628], "temperature": 0.0, "avg_logprob": -0.09750556144393793, "compression_ratio": 1.8313253012048192, "no_speech_prob": 0.013631163164973259}, {"id": 247, "seek": 129296, "start": 1292.96, "end": 1301.76, "text": " have some kind of a general form for the, for the root and you put stuff in the middle. You", "tokens": [50364, 362, 512, 733, 295, 257, 2674, 1254, 337, 264, 11, 337, 264, 5593, 293, 291, 829, 1507, 294, 264, 2808, 13, 509, 50804], "temperature": 0.0, "avg_logprob": -0.15190351993665782, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.001867302693426609}, {"id": 248, "seek": 129296, "start": 1301.76, "end": 1308.0, "text": " change the vowels. That's fascinating. That's fascinating. So in general, there's what, two", "tokens": [50804, 1319, 264, 44972, 13, 663, 311, 10343, 13, 663, 311, 10343, 13, 407, 294, 2674, 11, 456, 311, 437, 11, 732, 51116], "temperature": 0.0, "avg_logprob": -0.15190351993665782, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.001867302693426609}, {"id": 249, "seek": 129296, "start": 1308.0, "end": 1313.8400000000001, "text": " morphemes per word, usually one or two or three? Well, in English, it's, it's one or two. In English,", "tokens": [51116, 25778, 443, 279, 680, 1349, 11, 2673, 472, 420, 732, 420, 1045, 30, 1042, 11, 294, 3669, 11, 309, 311, 11, 309, 311, 472, 420, 732, 13, 682, 3669, 11, 51408], "temperature": 0.0, "avg_logprob": -0.15190351993665782, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.001867302693426609}, {"id": 250, "seek": 129296, "start": 1313.8400000000001, "end": 1317.92, "text": " it tends to be one or two. There can be more, you know, in other languages, you know, a language,", "tokens": [51408, 309, 12258, 281, 312, 472, 420, 732, 13, 821, 393, 312, 544, 11, 291, 458, 11, 294, 661, 8650, 11, 291, 458, 11, 257, 2856, 11, 51612], "temperature": 0.0, "avg_logprob": -0.15190351993665782, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.001867302693426609}, {"id": 251, "seek": 131792, "start": 1318.0, "end": 1324.48, "text": " language like, like Finnish, which has a very elaborate morphology, there may be 10 morphemes", "tokens": [50368, 2856, 411, 11, 411, 38429, 11, 597, 575, 257, 588, 20945, 25778, 1793, 11, 456, 815, 312, 1266, 25778, 443, 279, 50692], "temperature": 0.0, "avg_logprob": -0.13997868537902833, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.03111564740538597}, {"id": 252, "seek": 131792, "start": 1324.48, "end": 1329.6000000000001, "text": " on the end of a root. Okay. And so there may be, there may be millions of forms of a given word.", "tokens": [50692, 322, 264, 917, 295, 257, 5593, 13, 1033, 13, 400, 370, 456, 815, 312, 11, 456, 815, 312, 6803, 295, 6422, 295, 257, 2212, 1349, 13, 50948], "temperature": 0.0, "avg_logprob": -0.13997868537902833, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.03111564740538597}, {"id": 253, "seek": 131792, "start": 1329.6000000000001, "end": 1338.8000000000002, "text": " Okay. Okay. I will ask the same question over and over. But how does the, just sometimes to", "tokens": [50948, 1033, 13, 1033, 13, 286, 486, 1029, 264, 912, 1168, 670, 293, 670, 13, 583, 577, 775, 264, 11, 445, 2171, 281, 51408], "temperature": 0.0, "avg_logprob": -0.13997868537902833, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.03111564740538597}, {"id": 254, "seek": 131792, "start": 1338.8000000000002, "end": 1344.64, "text": " understand things like morphemes, it's nice to just ask the question, how does these kinds of", "tokens": [51408, 1223, 721, 411, 25778, 443, 279, 11, 309, 311, 1481, 281, 445, 1029, 264, 1168, 11, 577, 775, 613, 3685, 295, 51700], "temperature": 0.0, "avg_logprob": -0.13997868537902833, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.03111564740538597}, {"id": 255, "seek": 134464, "start": 1344.64, "end": 1354.64, "text": " things evolve? So you have a great book studying sort of the, how, how, how the cognitive processing,", "tokens": [50364, 721, 16693, 30, 407, 291, 362, 257, 869, 1446, 7601, 1333, 295, 264, 11, 577, 11, 577, 11, 577, 264, 15605, 9007, 11, 50864], "temperature": 0.0, "avg_logprob": -0.14432537421751557, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0023565057199448347}, {"id": 256, "seek": 134464, "start": 1354.64, "end": 1359.3600000000001, "text": " how language used for communication. So the, the mathematical notion of how effective languages", "tokens": [50864, 577, 2856, 1143, 337, 6101, 13, 407, 264, 11, 264, 18894, 10710, 295, 577, 4942, 8650, 51100], "temperature": 0.0, "avg_logprob": -0.14432537421751557, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0023565057199448347}, {"id": 257, "seek": 134464, "start": 1359.3600000000001, "end": 1363.5200000000002, "text": " for communication, what role that plays in the evolution of language, but just high level,", "tokens": [51100, 337, 6101, 11, 437, 3090, 300, 5749, 294, 264, 9303, 295, 2856, 11, 457, 445, 1090, 1496, 11, 51308], "temperature": 0.0, "avg_logprob": -0.14432537421751557, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0023565057199448347}, {"id": 258, "seek": 134464, "start": 1364.24, "end": 1370.0800000000002, "text": " like how do we, how does a language evolve with where English is two morphemes or one or two", "tokens": [51344, 411, 577, 360, 321, 11, 577, 775, 257, 2856, 16693, 365, 689, 3669, 307, 732, 25778, 443, 279, 420, 472, 420, 732, 51636], "temperature": 0.0, "avg_logprob": -0.14432537421751557, "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.0023565057199448347}, {"id": 259, "seek": 137008, "start": 1370.08, "end": 1376.3999999999999, "text": " morphemes per word and then Finnish has infinity per word? So what, how does that, how does that", "tokens": [50364, 25778, 443, 279, 680, 1349, 293, 550, 38429, 575, 13202, 680, 1349, 30, 407, 437, 11, 577, 775, 300, 11, 577, 775, 300, 50680], "temperature": 0.0, "avg_logprob": -0.09911458015441894, "compression_ratio": 1.68, "no_speech_prob": 0.022273285314440727}, {"id": 260, "seek": 137008, "start": 1376.3999999999999, "end": 1382.0, "text": " happen? Is it just people? That's a really good question. That's a very good question. It's like,", "tokens": [50680, 1051, 30, 1119, 309, 445, 561, 30, 663, 311, 257, 534, 665, 1168, 13, 663, 311, 257, 588, 665, 1168, 13, 467, 311, 411, 11, 50960], "temperature": 0.0, "avg_logprob": -0.09911458015441894, "compression_ratio": 1.68, "no_speech_prob": 0.022273285314440727}, {"id": 261, "seek": 137008, "start": 1382.0, "end": 1387.52, "text": " why do languages have more morphology versus less morphology? And I don't think we know the", "tokens": [50960, 983, 360, 8650, 362, 544, 25778, 1793, 5717, 1570, 25778, 1793, 30, 400, 286, 500, 380, 519, 321, 458, 264, 51236], "temperature": 0.0, "avg_logprob": -0.09911458015441894, "compression_ratio": 1.68, "no_speech_prob": 0.022273285314440727}, {"id": 262, "seek": 137008, "start": 1387.52, "end": 1392.56, "text": " answer to this. I know, I think there's just like a lot of good solutions to the problem of", "tokens": [51236, 1867, 281, 341, 13, 286, 458, 11, 286, 519, 456, 311, 445, 411, 257, 688, 295, 665, 6547, 281, 264, 1154, 295, 51488], "temperature": 0.0, "avg_logprob": -0.09911458015441894, "compression_ratio": 1.68, "no_speech_prob": 0.022273285314440727}, {"id": 263, "seek": 139256, "start": 1392.56, "end": 1399.76, "text": " communication. And so I, like, I believe as you hinted that language is an invented system", "tokens": [50364, 6101, 13, 400, 370, 286, 11, 411, 11, 286, 1697, 382, 291, 12075, 292, 300, 2856, 307, 364, 14479, 1185, 50724], "temperature": 0.0, "avg_logprob": -0.12008910377820332, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.2449234127998352}, {"id": 264, "seek": 139256, "start": 1399.76, "end": 1405.6, "text": " by humans for communicating their ideas. And I think we, it comes down to we label the things we", "tokens": [50724, 538, 6255, 337, 17559, 641, 3487, 13, 400, 286, 519, 321, 11, 309, 1487, 760, 281, 321, 7645, 264, 721, 321, 51016], "temperature": 0.0, "avg_logprob": -0.12008910377820332, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.2449234127998352}, {"id": 265, "seek": 139256, "start": 1405.6, "end": 1410.0, "text": " want to talk about. Those are the morphemes and words. Those are the things we want to talk about", "tokens": [51016, 528, 281, 751, 466, 13, 3950, 366, 264, 25778, 443, 279, 293, 2283, 13, 3950, 366, 264, 721, 321, 528, 281, 751, 466, 51236], "temperature": 0.0, "avg_logprob": -0.12008910377820332, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.2449234127998352}, {"id": 266, "seek": 139256, "start": 1410.0, "end": 1416.08, "text": " in the world and we invent those things. And then we put them together in ways that are easy for us", "tokens": [51236, 294, 264, 1002, 293, 321, 7962, 729, 721, 13, 400, 550, 321, 829, 552, 1214, 294, 2098, 300, 366, 1858, 337, 505, 51540], "temperature": 0.0, "avg_logprob": -0.12008910377820332, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.2449234127998352}, {"id": 267, "seek": 141608, "start": 1416.1599999999999, "end": 1422.08, "text": " to convey, to process. But that's like a naive view. And I don't, I mean, I think it's probably", "tokens": [50368, 281, 16965, 11, 281, 1399, 13, 583, 300, 311, 411, 257, 29052, 1910, 13, 400, 286, 500, 380, 11, 286, 914, 11, 286, 519, 309, 311, 1391, 50664], "temperature": 0.0, "avg_logprob": -0.221516296643169, "compression_ratio": 1.7929515418502202, "no_speech_prob": 0.4403417706489563}, {"id": 268, "seek": 141608, "start": 1422.08, "end": 1423.84, "text": " right, right? It's naive and probably right.", "tokens": [50664, 558, 11, 558, 30, 467, 311, 29052, 293, 1391, 558, 13, 50752], "temperature": 0.0, "avg_logprob": -0.221516296643169, "compression_ratio": 1.7929515418502202, "no_speech_prob": 0.4403417706489563}, {"id": 269, "seek": 141608, "start": 1423.84, "end": 1426.32, "text": " Well, that's a nice, I don't know if it's naive. I think it's simple.", "tokens": [50752, 1042, 11, 300, 311, 257, 1481, 11, 286, 500, 380, 458, 498, 309, 311, 29052, 13, 286, 519, 309, 311, 2199, 13, 50876], "temperature": 0.0, "avg_logprob": -0.221516296643169, "compression_ratio": 1.7929515418502202, "no_speech_prob": 0.4403417706489563}, {"id": 270, "seek": 141608, "start": 1426.32, "end": 1426.96, "text": " Simple. Yeah.", "tokens": [50876, 21532, 13, 865, 13, 50908], "temperature": 0.0, "avg_logprob": -0.221516296643169, "compression_ratio": 1.7929515418502202, "no_speech_prob": 0.4403417706489563}, {"id": 271, "seek": 141608, "start": 1426.96, "end": 1432.32, "text": " I think naive is, naive is an indication that's an incorrect somehow. It's a trivial,", "tokens": [50908, 286, 519, 29052, 307, 11, 29052, 307, 364, 18877, 300, 311, 364, 18424, 6063, 13, 467, 311, 257, 26703, 11, 51176], "temperature": 0.0, "avg_logprob": -0.221516296643169, "compression_ratio": 1.7929515418502202, "no_speech_prob": 0.4403417706489563}, {"id": 272, "seek": 141608, "start": 1433.1999999999998, "end": 1439.36, "text": " too simple. I think it could very well be correct. But it's interesting how sticky it feels like", "tokens": [51220, 886, 2199, 13, 286, 519, 309, 727, 588, 731, 312, 3006, 13, 583, 309, 311, 1880, 577, 14470, 309, 3417, 411, 51528], "temperature": 0.0, "avg_logprob": -0.221516296643169, "compression_ratio": 1.7929515418502202, "no_speech_prob": 0.4403417706489563}, {"id": 273, "seek": 143936, "start": 1439.9199999999998, "end": 1446.08, "text": " two people got together. It just feels like once you figure out certain aspects of a language,", "tokens": [50392, 732, 561, 658, 1214, 13, 467, 445, 3417, 411, 1564, 291, 2573, 484, 1629, 7270, 295, 257, 2856, 11, 50700], "temperature": 0.0, "avg_logprob": -0.1140531604572878, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.15598894655704498}, {"id": 274, "seek": 143936, "start": 1446.08, "end": 1450.4799999999998, "text": " that just becomes sticky and the tribe forms around that language, maybe the language,", "tokens": [50700, 300, 445, 3643, 14470, 293, 264, 17625, 6422, 926, 300, 2856, 11, 1310, 264, 2856, 11, 50920], "temperature": 0.0, "avg_logprob": -0.1140531604572878, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.15598894655704498}, {"id": 275, "seek": 143936, "start": 1450.4799999999998, "end": 1454.4799999999998, "text": " maybe the tribe forms first, then the language evolves. And then you just kind of agree and", "tokens": [50920, 1310, 264, 17625, 6422, 700, 11, 550, 264, 2856, 43737, 13, 400, 550, 291, 445, 733, 295, 3986, 293, 51120], "temperature": 0.0, "avg_logprob": -0.1140531604572878, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.15598894655704498}, {"id": 276, "seek": 143936, "start": 1454.4799999999998, "end": 1456.08, "text": " you stick to whatever that is.", "tokens": [51120, 291, 2897, 281, 2035, 300, 307, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1140531604572878, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.15598894655704498}, {"id": 277, "seek": 143936, "start": 1456.08, "end": 1462.4799999999998, "text": " I mean, these are very interesting questions. We don't know really about how words, even words,", "tokens": [51200, 286, 914, 11, 613, 366, 588, 1880, 1651, 13, 492, 500, 380, 458, 534, 466, 577, 2283, 11, 754, 2283, 11, 51520], "temperature": 0.0, "avg_logprob": -0.1140531604572878, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.15598894655704498}, {"id": 278, "seek": 143936, "start": 1462.4799999999998, "end": 1467.12, "text": " get invented very much about, you know, we don't really, I mean, assuming they get invented,", "tokens": [51520, 483, 14479, 588, 709, 466, 11, 291, 458, 11, 321, 500, 380, 534, 11, 286, 914, 11, 11926, 436, 483, 14479, 11, 51752], "temperature": 0.0, "avg_logprob": -0.1140531604572878, "compression_ratio": 1.8327137546468402, "no_speech_prob": 0.15598894655704498}, {"id": 279, "seek": 146712, "start": 1467.9199999999998, "end": 1472.0, "text": " they, we don't really know how that process works and how these things evolve. What we have is", "tokens": [50404, 436, 11, 321, 500, 380, 534, 458, 577, 300, 1399, 1985, 293, 577, 613, 721, 16693, 13, 708, 321, 362, 307, 50608], "temperature": 0.0, "avg_logprob": -0.08951947953965929, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0002868077135644853}, {"id": 280, "seek": 146712, "start": 1472.9599999999998, "end": 1480.32, "text": " kind of a current picture, a current picture of a few thousand languages, a few thousand", "tokens": [50656, 733, 295, 257, 2190, 3036, 11, 257, 2190, 3036, 295, 257, 1326, 4714, 8650, 11, 257, 1326, 4714, 51024], "temperature": 0.0, "avg_logprob": -0.08951947953965929, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0002868077135644853}, {"id": 281, "seek": 146712, "start": 1480.32, "end": 1486.2399999999998, "text": " instances. We don't have any pictures of really how these things are evolving, really. And then", "tokens": [51024, 14519, 13, 492, 500, 380, 362, 604, 5242, 295, 534, 577, 613, 721, 366, 21085, 11, 534, 13, 400, 550, 51320], "temperature": 0.0, "avg_logprob": -0.08951947953965929, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0002868077135644853}, {"id": 282, "seek": 146712, "start": 1486.2399999999998, "end": 1493.9199999999998, "text": " the evolution is massively, you know, confused by contact, right? So as soon as one language,", "tokens": [51320, 264, 9303, 307, 29379, 11, 291, 458, 11, 9019, 538, 3385, 11, 558, 30, 407, 382, 2321, 382, 472, 2856, 11, 51704], "temperature": 0.0, "avg_logprob": -0.08951947953965929, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0002868077135644853}, {"id": 283, "seek": 149392, "start": 1493.92, "end": 1500.3200000000002, "text": " group, one group runs into another, we are smart, humans are smart, and they take on", "tokens": [50364, 1594, 11, 472, 1594, 6676, 666, 1071, 11, 321, 366, 4069, 11, 6255, 366, 4069, 11, 293, 436, 747, 322, 50684], "temperature": 0.0, "avg_logprob": -0.13720417022705078, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.028411582112312317}, {"id": 284, "seek": 149392, "start": 1500.3200000000002, "end": 1505.6000000000001, "text": " whatever is useful in the other group. And so any kind of contrast, which you're talking about,", "tokens": [50684, 2035, 307, 4420, 294, 264, 661, 1594, 13, 400, 370, 604, 733, 295, 8712, 11, 597, 291, 434, 1417, 466, 11, 50948], "temperature": 0.0, "avg_logprob": -0.13720417022705078, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.028411582112312317}, {"id": 285, "seek": 149392, "start": 1505.6000000000001, "end": 1510.16, "text": " which I find useful, I'm going to, I'm going to start using as well. So I worked a little bit", "tokens": [50948, 597, 286, 915, 4420, 11, 286, 478, 516, 281, 11, 286, 478, 516, 281, 722, 1228, 382, 731, 13, 407, 286, 2732, 257, 707, 857, 51176], "temperature": 0.0, "avg_logprob": -0.13720417022705078, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.028411582112312317}, {"id": 286, "seek": 149392, "start": 1510.72, "end": 1518.48, "text": " in specific areas of words, in number words and in color words, and in color words. So we have,", "tokens": [51204, 294, 2685, 3179, 295, 2283, 11, 294, 1230, 2283, 293, 294, 2017, 2283, 11, 293, 294, 2017, 2283, 13, 407, 321, 362, 11, 51592], "temperature": 0.0, "avg_logprob": -0.13720417022705078, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.028411582112312317}, {"id": 287, "seek": 151848, "start": 1519.04, "end": 1526.56, "text": " in English, we have around 11 words that everyone knows for colors. And many more, if you happen to", "tokens": [50392, 294, 3669, 11, 321, 362, 926, 2975, 2283, 300, 1518, 3255, 337, 4577, 13, 400, 867, 544, 11, 498, 291, 1051, 281, 50768], "temperature": 0.0, "avg_logprob": -0.09122360043409394, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.09938029944896698}, {"id": 288, "seek": 151848, "start": 1527.1200000000001, "end": 1531.44, "text": " be interested in color for some reason or other, if you're a fashion designer or an artist or something,", "tokens": [50796, 312, 3102, 294, 2017, 337, 512, 1778, 420, 661, 11, 498, 291, 434, 257, 6700, 11795, 420, 364, 5748, 420, 746, 11, 51012], "temperature": 0.0, "avg_logprob": -0.09122360043409394, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.09938029944896698}, {"id": 289, "seek": 151848, "start": 1531.44, "end": 1537.1200000000001, "text": " you may have many, many more words. But we can see millions, like if you have normal color vision,", "tokens": [51012, 291, 815, 362, 867, 11, 867, 544, 2283, 13, 583, 321, 393, 536, 6803, 11, 411, 498, 291, 362, 2710, 2017, 5201, 11, 51296], "temperature": 0.0, "avg_logprob": -0.09122360043409394, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.09938029944896698}, {"id": 290, "seek": 151848, "start": 1537.1200000000001, "end": 1541.68, "text": " normal trichrometric color vision, you can see millions of distinctions in color. So we don't", "tokens": [51296, 2710, 504, 480, 4397, 17475, 2017, 5201, 11, 291, 393, 536, 6803, 295, 1483, 49798, 294, 2017, 13, 407, 321, 500, 380, 51524], "temperature": 0.0, "avg_logprob": -0.09122360043409394, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.09938029944896698}, {"id": 291, "seek": 151848, "start": 1541.68, "end": 1546.8, "text": " have millions of words. You know, the most efficient, no, the most, you know, detailed color", "tokens": [51524, 362, 6803, 295, 2283, 13, 509, 458, 11, 264, 881, 7148, 11, 572, 11, 264, 881, 11, 291, 458, 11, 9942, 2017, 51780], "temperature": 0.0, "avg_logprob": -0.09122360043409394, "compression_ratio": 1.8421052631578947, "no_speech_prob": 0.09938029944896698}, {"id": 292, "seek": 154680, "start": 1546.8, "end": 1551.6, "text": " vocabulary would have over a million terms to distinguish all the different colors that we", "tokens": [50364, 19864, 576, 362, 670, 257, 2459, 2115, 281, 20206, 439, 264, 819, 4577, 300, 321, 50604], "temperature": 0.0, "avg_logprob": -0.0884250857965733, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.012046894989907742}, {"id": 293, "seek": 154680, "start": 1551.6, "end": 1557.44, "text": " can see. But of course, we don't have that. So it's somehow, it's been, it's kind of useful", "tokens": [50604, 393, 536, 13, 583, 295, 1164, 11, 321, 500, 380, 362, 300, 13, 407, 309, 311, 6063, 11, 309, 311, 668, 11, 309, 311, 733, 295, 4420, 50896], "temperature": 0.0, "avg_logprob": -0.0884250857965733, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.012046894989907742}, {"id": 294, "seek": 154680, "start": 1557.44, "end": 1563.28, "text": " for English to have evolved in some way to, there's 11 terms that people find useful to talk about,", "tokens": [50896, 337, 3669, 281, 362, 14178, 294, 512, 636, 281, 11, 456, 311, 2975, 2115, 300, 561, 915, 4420, 281, 751, 466, 11, 51188], "temperature": 0.0, "avg_logprob": -0.0884250857965733, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.012046894989907742}, {"id": 295, "seek": 154680, "start": 1563.28, "end": 1570.72, "text": " you know, black, white, red, blue, green, yellow, purple, gray, pink, and I probably miss something", "tokens": [51188, 291, 458, 11, 2211, 11, 2418, 11, 2182, 11, 3344, 11, 3092, 11, 5566, 11, 9656, 11, 10855, 11, 7022, 11, 293, 286, 1391, 1713, 746, 51560], "temperature": 0.0, "avg_logprob": -0.0884250857965733, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.012046894989907742}, {"id": 296, "seek": 154680, "start": 1570.72, "end": 1576.08, "text": " there. Anyway, there's 11 that everyone knows. And depending on your, but you go to different", "tokens": [51560, 456, 13, 5684, 11, 456, 311, 2975, 300, 1518, 3255, 13, 400, 5413, 322, 428, 11, 457, 291, 352, 281, 819, 51828], "temperature": 0.0, "avg_logprob": -0.0884250857965733, "compression_ratio": 1.6643356643356644, "no_speech_prob": 0.012046894989907742}, {"id": 297, "seek": 157608, "start": 1576.08, "end": 1581.84, "text": " cultures, especially the non industrialized cultures, and there'll be many fewer. So some cultures", "tokens": [50364, 12951, 11, 2318, 264, 2107, 9987, 1602, 12951, 11, 293, 456, 603, 312, 867, 13366, 13, 407, 512, 12951, 50652], "temperature": 0.0, "avg_logprob": -0.10129844347635905, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0028886860236525536}, {"id": 298, "seek": 157608, "start": 1581.84, "end": 1589.28, "text": " will have only two, believe it or not, that the Danai and Papua New Guinea have only two labels", "tokens": [50652, 486, 362, 787, 732, 11, 1697, 309, 420, 406, 11, 300, 264, 3394, 1301, 293, 15919, 4398, 1873, 46793, 362, 787, 732, 16949, 51024], "temperature": 0.0, "avg_logprob": -0.10129844347635905, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0028886860236525536}, {"id": 299, "seek": 157608, "start": 1589.28, "end": 1594.3999999999999, "text": " that the group uses for color. And those are roughly black and white. They are very, very dark", "tokens": [51024, 300, 264, 1594, 4960, 337, 2017, 13, 400, 729, 366, 9810, 2211, 293, 2418, 13, 814, 366, 588, 11, 588, 2877, 51280], "temperature": 0.0, "avg_logprob": -0.10129844347635905, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0028886860236525536}, {"id": 300, "seek": 157608, "start": 1594.3999999999999, "end": 1598.3999999999999, "text": " and very, very light, which are roughly black and white. And you might think, oh, they're dividing", "tokens": [51280, 293, 588, 11, 588, 1442, 11, 597, 366, 9810, 2211, 293, 2418, 13, 400, 291, 1062, 519, 11, 1954, 11, 436, 434, 26764, 51480], "temperature": 0.0, "avg_logprob": -0.10129844347635905, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0028886860236525536}, {"id": 301, "seek": 157608, "start": 1598.3999999999999, "end": 1602.72, "text": " the whole color space into, you know, light and dark or something. And that's not really true.", "tokens": [51480, 264, 1379, 2017, 1901, 666, 11, 291, 458, 11, 1442, 293, 2877, 420, 746, 13, 400, 300, 311, 406, 534, 2074, 13, 51696], "temperature": 0.0, "avg_logprob": -0.10129844347635905, "compression_ratio": 1.788888888888889, "no_speech_prob": 0.0028886860236525536}, {"id": 302, "seek": 160272, "start": 1602.72, "end": 1606.88, "text": " They mostly just only label the light, the black and the white things. They just don't talk about", "tokens": [50364, 814, 5240, 445, 787, 7645, 264, 1442, 11, 264, 2211, 293, 264, 2418, 721, 13, 814, 445, 500, 380, 751, 466, 50572], "temperature": 0.0, "avg_logprob": -0.1373431721671683, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.01588725484907627}, {"id": 303, "seek": 160272, "start": 1606.88, "end": 1611.3600000000001, "text": " the colors for the other ones. And so, and then there's other groups, I worked with a group called", "tokens": [50572, 264, 4577, 337, 264, 661, 2306, 13, 400, 370, 11, 293, 550, 456, 311, 661, 3935, 11, 286, 2732, 365, 257, 1594, 1219, 50796], "temperature": 0.0, "avg_logprob": -0.1373431721671683, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.01588725484907627}, {"id": 304, "seek": 160272, "start": 1611.3600000000001, "end": 1618.56, "text": " the Chimani down in, in Bolivia, in South America, and they have three words that everyone knows,", "tokens": [50796, 264, 761, 332, 3782, 760, 294, 11, 294, 14331, 18503, 11, 294, 4242, 3374, 11, 293, 436, 362, 1045, 2283, 300, 1518, 3255, 11, 51156], "temperature": 0.0, "avg_logprob": -0.1373431721671683, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.01588725484907627}, {"id": 305, "seek": 160272, "start": 1618.56, "end": 1624.72, "text": " but there's a few others that are, that, that several people, that many people know. And so,", "tokens": [51156, 457, 456, 311, 257, 1326, 2357, 300, 366, 11, 300, 11, 300, 2940, 561, 11, 300, 867, 561, 458, 13, 400, 370, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1373431721671683, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.01588725484907627}, {"id": 306, "seek": 160272, "start": 1624.72, "end": 1630.0, "text": " they have made, it's kind of depending on how you count between three and seven words that", "tokens": [51464, 436, 362, 1027, 11, 309, 311, 733, 295, 5413, 322, 577, 291, 1207, 1296, 1045, 293, 3407, 2283, 300, 51728], "temperature": 0.0, "avg_logprob": -0.1373431721671683, "compression_ratio": 1.7703703703703704, "no_speech_prob": 0.01588725484907627}, {"id": 307, "seek": 163000, "start": 1630.0, "end": 1636.24, "text": " the group knows. Okay. And again, they're black and white. Everyone knows those. And red, red is,", "tokens": [50364, 264, 1594, 3255, 13, 1033, 13, 400, 797, 11, 436, 434, 2211, 293, 2418, 13, 5198, 3255, 729, 13, 400, 2182, 11, 2182, 307, 11, 50676], "temperature": 0.0, "avg_logprob": -0.127968970145888, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.02440207079052925}, {"id": 308, "seek": 163000, "start": 1636.24, "end": 1641.36, "text": " you know, like that tends to be the third word that everyone, that cultures bring in, if there's", "tokens": [50676, 291, 458, 11, 411, 300, 12258, 281, 312, 264, 2636, 1349, 300, 1518, 11, 300, 12951, 1565, 294, 11, 498, 456, 311, 50932], "temperature": 0.0, "avg_logprob": -0.127968970145888, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.02440207079052925}, {"id": 309, "seek": 163000, "start": 1641.36, "end": 1645.36, "text": " a word, it's always red, the third one. And then after that, it's kind of all bets are off", "tokens": [50932, 257, 1349, 11, 309, 311, 1009, 2182, 11, 264, 2636, 472, 13, 400, 550, 934, 300, 11, 309, 311, 733, 295, 439, 39922, 366, 766, 51132], "temperature": 0.0, "avg_logprob": -0.127968970145888, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.02440207079052925}, {"id": 310, "seek": 163000, "start": 1645.36, "end": 1651.12, "text": " about what they bring in. And so after that, they bring in a sort of a big blue, green space group,", "tokens": [51132, 466, 437, 436, 1565, 294, 13, 400, 370, 934, 300, 11, 436, 1565, 294, 257, 1333, 295, 257, 955, 3344, 11, 3092, 1901, 1594, 11, 51420], "temperature": 0.0, "avg_logprob": -0.127968970145888, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.02440207079052925}, {"id": 311, "seek": 163000, "start": 1651.12, "end": 1656.16, "text": " group, they have one for that. And then they have, and then, you know, different people have", "tokens": [51420, 1594, 11, 436, 362, 472, 337, 300, 13, 400, 550, 436, 362, 11, 293, 550, 11, 291, 458, 11, 819, 561, 362, 51672], "temperature": 0.0, "avg_logprob": -0.127968970145888, "compression_ratio": 1.8968253968253967, "no_speech_prob": 0.02440207079052925}, {"id": 312, "seek": 165616, "start": 1656.16, "end": 1661.44, "text": " different words that they'll use for other parts of the space. And so anyway, it's probably related", "tokens": [50364, 819, 2283, 300, 436, 603, 764, 337, 661, 3166, 295, 264, 1901, 13, 400, 370, 4033, 11, 309, 311, 1391, 4077, 50628], "temperature": 0.0, "avg_logprob": -0.09421890500992064, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.03019896149635315}, {"id": 313, "seek": 165616, "start": 1661.44, "end": 1666.16, "text": " to what they want to talk, what they, not what they, not what they see, because they see the", "tokens": [50628, 281, 437, 436, 528, 281, 751, 11, 437, 436, 11, 406, 437, 436, 11, 406, 437, 436, 536, 11, 570, 436, 536, 264, 50864], "temperature": 0.0, "avg_logprob": -0.09421890500992064, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.03019896149635315}, {"id": 314, "seek": 165616, "start": 1666.16, "end": 1673.1200000000001, "text": " same colors as we see. So it's not like they have, they don't, they have a weak, a low color palette", "tokens": [50864, 912, 4577, 382, 321, 536, 13, 407, 309, 311, 406, 411, 436, 362, 11, 436, 500, 380, 11, 436, 362, 257, 5336, 11, 257, 2295, 2017, 15851, 51212], "temperature": 0.0, "avg_logprob": -0.09421890500992064, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.03019896149635315}, {"id": 315, "seek": 165616, "start": 1673.1200000000001, "end": 1678.0800000000002, "text": " and the things they're looking at, they're looking at a lot of beautiful scenery. Okay. A lot of", "tokens": [51212, 293, 264, 721, 436, 434, 1237, 412, 11, 436, 434, 1237, 412, 257, 688, 295, 2238, 25805, 13, 1033, 13, 316, 688, 295, 51460], "temperature": 0.0, "avg_logprob": -0.09421890500992064, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.03019896149635315}, {"id": 316, "seek": 165616, "start": 1678.0800000000002, "end": 1684.0, "text": " different colored flowers and berries and things. And you know, and so there's lots of things of", "tokens": [51460, 819, 14332, 8085, 293, 29898, 293, 721, 13, 400, 291, 458, 11, 293, 370, 456, 311, 3195, 295, 721, 295, 51756], "temperature": 0.0, "avg_logprob": -0.09421890500992064, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.03019896149635315}, {"id": 317, "seek": 168400, "start": 1684.0, "end": 1689.68, "text": " very bright colors, but they just don't label the color in those cases. And the reason probably,", "tokens": [50364, 588, 4730, 4577, 11, 457, 436, 445, 500, 380, 7645, 264, 2017, 294, 729, 3331, 13, 400, 264, 1778, 1391, 11, 50648], "temperature": 0.0, "avg_logprob": -0.07095126655158096, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.007120357360690832}, {"id": 318, "seek": 168400, "start": 1689.68, "end": 1695.28, "text": " we don't know this, but we think probably what's going on here is that what you do, why you label", "tokens": [50648, 321, 500, 380, 458, 341, 11, 457, 321, 519, 1391, 437, 311, 516, 322, 510, 307, 300, 437, 291, 360, 11, 983, 291, 7645, 50928], "temperature": 0.0, "avg_logprob": -0.07095126655158096, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.007120357360690832}, {"id": 319, "seek": 168400, "start": 1695.28, "end": 1700.08, "text": " something is you need to talk to someone else about it. And why do I need to talk about a color?", "tokens": [50928, 746, 307, 291, 643, 281, 751, 281, 1580, 1646, 466, 309, 13, 400, 983, 360, 286, 643, 281, 751, 466, 257, 2017, 30, 51168], "temperature": 0.0, "avg_logprob": -0.07095126655158096, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.007120357360690832}, {"id": 320, "seek": 168400, "start": 1700.08, "end": 1706.16, "text": " Well, if I have two things which are identical, and I want you to give me the one that's different,", "tokens": [51168, 1042, 11, 498, 286, 362, 732, 721, 597, 366, 14800, 11, 293, 286, 528, 291, 281, 976, 385, 264, 472, 300, 311, 819, 11, 51472], "temperature": 0.0, "avg_logprob": -0.07095126655158096, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.007120357360690832}, {"id": 321, "seek": 168400, "start": 1706.16, "end": 1711.36, "text": " and in the only way it varies is color, then I invent a word, which tells you, you know,", "tokens": [51472, 293, 294, 264, 787, 636, 309, 21716, 307, 2017, 11, 550, 286, 7962, 257, 1349, 11, 597, 5112, 291, 11, 291, 458, 11, 51732], "temperature": 0.0, "avg_logprob": -0.07095126655158096, "compression_ratio": 1.7712177121771218, "no_speech_prob": 0.007120357360690832}, {"id": 322, "seek": 171136, "start": 1711.36, "end": 1715.52, "text": " this is the one I want. So I want the red sweater off the rack, not the green sweater, right?", "tokens": [50364, 341, 307, 264, 472, 286, 528, 13, 407, 286, 528, 264, 2182, 26550, 766, 264, 14788, 11, 406, 264, 3092, 26550, 11, 558, 30, 50572], "temperature": 0.0, "avg_logprob": -0.11267335975871366, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.010984504595398903}, {"id": 323, "seek": 171136, "start": 1715.52, "end": 1720.24, "text": " There's two. And so those things will be identical, because these are things we made and they're", "tokens": [50572, 821, 311, 732, 13, 400, 370, 729, 721, 486, 312, 14800, 11, 570, 613, 366, 721, 321, 1027, 293, 436, 434, 50808], "temperature": 0.0, "avg_logprob": -0.11267335975871366, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.010984504595398903}, {"id": 324, "seek": 171136, "start": 1720.24, "end": 1724.8799999999999, "text": " dyed, and there's nothing different about them. And so in industrialized society, we have,", "tokens": [50808, 43199, 11, 293, 456, 311, 1825, 819, 466, 552, 13, 400, 370, 294, 9987, 1602, 4086, 11, 321, 362, 11, 51040], "temperature": 0.0, "avg_logprob": -0.11267335975871366, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.010984504595398903}, {"id": 325, "seek": 171136, "start": 1725.76, "end": 1729.6799999999998, "text": " you know, everything, everything we've got is pretty much arbitrarily colored.", "tokens": [51084, 291, 458, 11, 1203, 11, 1203, 321, 600, 658, 307, 1238, 709, 19071, 3289, 14332, 13, 51280], "temperature": 0.0, "avg_logprob": -0.11267335975871366, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.010984504595398903}, {"id": 326, "seek": 171136, "start": 1730.3999999999999, "end": 1734.8799999999999, "text": " But if you go to a non-industrialized group, that's not true. And so they don't,", "tokens": [51316, 583, 498, 291, 352, 281, 257, 2107, 12, 29850, 7111, 1602, 1594, 11, 300, 311, 406, 2074, 13, 400, 370, 436, 500, 380, 11, 51540], "temperature": 0.0, "avg_logprob": -0.11267335975871366, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.010984504595398903}, {"id": 327, "seek": 171136, "start": 1734.8799999999999, "end": 1738.3999999999999, "text": " suddenly they're not interested in color. If you bring bright colored things to them,", "tokens": [51540, 5800, 436, 434, 406, 3102, 294, 2017, 13, 759, 291, 1565, 4730, 14332, 721, 281, 552, 11, 51716], "temperature": 0.0, "avg_logprob": -0.11267335975871366, "compression_ratio": 1.7925170068027212, "no_speech_prob": 0.010984504595398903}, {"id": 328, "seek": 173840, "start": 1738.4, "end": 1742.5600000000002, "text": " they like them just like we like them. Bright colors are great. They're beautiful.", "tokens": [50364, 436, 411, 552, 445, 411, 321, 411, 552, 13, 24271, 4577, 366, 869, 13, 814, 434, 2238, 13, 50572], "temperature": 0.0, "avg_logprob": -0.13550434737909037, "compression_ratio": 1.71875, "no_speech_prob": 0.0019873082637786865}, {"id": 329, "seek": 173840, "start": 1743.76, "end": 1747.0400000000002, "text": " But they just don't need to, nobody to talk about them. They don't have.", "tokens": [50632, 583, 436, 445, 500, 380, 643, 281, 11, 5079, 281, 751, 466, 552, 13, 814, 500, 380, 362, 13, 50796], "temperature": 0.0, "avg_logprob": -0.13550434737909037, "compression_ratio": 1.71875, "no_speech_prob": 0.0019873082637786865}, {"id": 330, "seek": 173840, "start": 1747.0400000000002, "end": 1753.1200000000001, "text": " So probably color words is a good example of how language evolves from sort of function,", "tokens": [50796, 407, 1391, 2017, 2283, 307, 257, 665, 1365, 295, 577, 2856, 43737, 490, 1333, 295, 2445, 11, 51100], "temperature": 0.0, "avg_logprob": -0.13550434737909037, "compression_ratio": 1.71875, "no_speech_prob": 0.0019873082637786865}, {"id": 331, "seek": 173840, "start": 1753.1200000000001, "end": 1755.68, "text": " when you need to communicate the use of something.", "tokens": [51100, 562, 291, 643, 281, 7890, 264, 764, 295, 746, 13, 51228], "temperature": 0.0, "avg_logprob": -0.13550434737909037, "compression_ratio": 1.71875, "no_speech_prob": 0.0019873082637786865}, {"id": 332, "seek": 173840, "start": 1755.68, "end": 1756.3200000000002, "text": " I think so.", "tokens": [51228, 286, 519, 370, 13, 51260], "temperature": 0.0, "avg_logprob": -0.13550434737909037, "compression_ratio": 1.71875, "no_speech_prob": 0.0019873082637786865}, {"id": 333, "seek": 173840, "start": 1756.3200000000002, "end": 1762.16, "text": " Then you kind of invent different variations. And basically, you can imagine that the evolution", "tokens": [51260, 1396, 291, 733, 295, 7962, 819, 17840, 13, 400, 1936, 11, 291, 393, 3811, 300, 264, 9303, 51552], "temperature": 0.0, "avg_logprob": -0.13550434737909037, "compression_ratio": 1.71875, "no_speech_prob": 0.0019873082637786865}, {"id": 334, "seek": 173840, "start": 1762.16, "end": 1767.2, "text": " of a language has to do with what the early tribes doing, like what they want it, what kind", "tokens": [51552, 295, 257, 2856, 575, 281, 360, 365, 437, 264, 2440, 19035, 884, 11, 411, 437, 436, 528, 309, 11, 437, 733, 51804], "temperature": 0.0, "avg_logprob": -0.13550434737909037, "compression_ratio": 1.71875, "no_speech_prob": 0.0019873082637786865}, {"id": 335, "seek": 176720, "start": 1767.2, "end": 1771.28, "text": " of problems they're facing them, and they're quickly figuring out how to efficiently communicate", "tokens": [50364, 295, 2740, 436, 434, 7170, 552, 11, 293, 436, 434, 2661, 15213, 484, 577, 281, 19621, 7890, 50568], "temperature": 0.0, "avg_logprob": -0.10626393127441407, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.001597357913851738}, {"id": 336, "seek": 176720, "start": 1772.16, "end": 1776.32, "text": " the solution to those problems, whether it's aesthetic or function, all that kind of stuff,", "tokens": [50612, 264, 3827, 281, 729, 2740, 11, 1968, 309, 311, 20092, 420, 2445, 11, 439, 300, 733, 295, 1507, 11, 50820], "temperature": 0.0, "avg_logprob": -0.10626393127441407, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.001597357913851738}, {"id": 337, "seek": 176720, "start": 1776.32, "end": 1781.92, "text": " running away from a mammoth or whatever. But you know, it's, so I think what you're pointing to", "tokens": [50820, 2614, 1314, 490, 257, 19033, 900, 420, 2035, 13, 583, 291, 458, 11, 309, 311, 11, 370, 286, 519, 437, 291, 434, 12166, 281, 51100], "temperature": 0.0, "avg_logprob": -0.10626393127441407, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.001597357913851738}, {"id": 338, "seek": 176720, "start": 1781.92, "end": 1787.2, "text": " is that we don't have data on the evolution of language, because many languages have formed", "tokens": [51100, 307, 300, 321, 500, 380, 362, 1412, 322, 264, 9303, 295, 2856, 11, 570, 867, 8650, 362, 8693, 51364], "temperature": 0.0, "avg_logprob": -0.10626393127441407, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.001597357913851738}, {"id": 339, "seek": 176720, "start": 1787.2, "end": 1790.0, "text": " a long time ago. So you don't get the chatter.", "tokens": [51364, 257, 938, 565, 2057, 13, 407, 291, 500, 380, 483, 264, 26929, 13, 51504], "temperature": 0.0, "avg_logprob": -0.10626393127441407, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.001597357913851738}, {"id": 340, "seek": 176720, "start": 1790.0, "end": 1795.76, "text": " We have a little bit of like old English to modern English, because there was a writing system.", "tokens": [51504, 492, 362, 257, 707, 857, 295, 411, 1331, 3669, 281, 4363, 3669, 11, 570, 456, 390, 257, 3579, 1185, 13, 51792], "temperature": 0.0, "avg_logprob": -0.10626393127441407, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.001597357913851738}, {"id": 341, "seek": 179576, "start": 1795.84, "end": 1800.0, "text": " And we can see how old English looked. So the word order changed, for instance,", "tokens": [50368, 400, 321, 393, 536, 577, 1331, 3669, 2956, 13, 407, 264, 1349, 1668, 3105, 11, 337, 5197, 11, 50576], "temperature": 0.0, "avg_logprob": -0.10843932199821198, "compression_ratio": 1.8981818181818182, "no_speech_prob": 0.001987289171665907}, {"id": 342, "seek": 179576, "start": 1800.0, "end": 1803.6, "text": " in old English to middle English to modern English. And so it, you know, we could see", "tokens": [50576, 294, 1331, 3669, 281, 2808, 3669, 281, 4363, 3669, 13, 400, 370, 309, 11, 291, 458, 11, 321, 727, 536, 50756], "temperature": 0.0, "avg_logprob": -0.10843932199821198, "compression_ratio": 1.8981818181818182, "no_speech_prob": 0.001987289171665907}, {"id": 343, "seek": 179576, "start": 1803.6, "end": 1808.4, "text": " things like that, but most languages don't even have a writing system. So of the 7000,", "tokens": [50756, 721, 411, 300, 11, 457, 881, 8650, 500, 380, 754, 362, 257, 3579, 1185, 13, 407, 295, 264, 1614, 1360, 11, 50996], "temperature": 0.0, "avg_logprob": -0.10843932199821198, "compression_ratio": 1.8981818181818182, "no_speech_prob": 0.001987289171665907}, {"id": 344, "seek": 179576, "start": 1808.96, "end": 1813.36, "text": " only, you know, a small subset of those have a writing system. And even if they have a writing", "tokens": [51024, 787, 11, 291, 458, 11, 257, 1359, 25993, 295, 729, 362, 257, 3579, 1185, 13, 400, 754, 498, 436, 362, 257, 3579, 51244], "temperature": 0.0, "avg_logprob": -0.10843932199821198, "compression_ratio": 1.8981818181818182, "no_speech_prob": 0.001987289171665907}, {"id": 345, "seek": 179576, "start": 1813.36, "end": 1817.28, "text": " system, they, it's not a very modern writing system. And so they don't have it. So we just", "tokens": [51244, 1185, 11, 436, 11, 309, 311, 406, 257, 588, 4363, 3579, 1185, 13, 400, 370, 436, 500, 380, 362, 309, 13, 407, 321, 445, 51440], "temperature": 0.0, "avg_logprob": -0.10843932199821198, "compression_ratio": 1.8981818181818182, "no_speech_prob": 0.001987289171665907}, {"id": 346, "seek": 179576, "start": 1817.28, "end": 1822.32, "text": " basically have for Mandarin, for Chinese, we have a lot of, a lot of evidence from,", "tokens": [51440, 1936, 362, 337, 42292, 11, 337, 4649, 11, 321, 362, 257, 688, 295, 11, 257, 688, 295, 4467, 490, 11, 51692], "temperature": 0.0, "avg_logprob": -0.10843932199821198, "compression_ratio": 1.8981818181818182, "no_speech_prob": 0.001987289171665907}, {"id": 347, "seek": 182232, "start": 1822.8799999999999, "end": 1827.04, "text": " for long time and for English and not for much else, not from in German a little bit,", "tokens": [50392, 337, 938, 565, 293, 337, 3669, 293, 406, 337, 709, 1646, 11, 406, 490, 294, 6521, 257, 707, 857, 11, 50600], "temperature": 0.0, "avg_logprob": -0.14931554642934647, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.005727234296500683}, {"id": 348, "seek": 182232, "start": 1827.04, "end": 1832.0, "text": " but not for a whole lot of like long-term language evolution. We don't have a lot.", "tokens": [50600, 457, 406, 337, 257, 1379, 688, 295, 411, 938, 12, 7039, 2856, 9303, 13, 492, 500, 380, 362, 257, 688, 13, 50848], "temperature": 0.0, "avg_logprob": -0.14931554642934647, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.005727234296500683}, {"id": 349, "seek": 182232, "start": 1832.0, "end": 1834.72, "text": " Well, you can have snapshots is what we've got of current languages.", "tokens": [50848, 1042, 11, 291, 393, 362, 19206, 27495, 307, 437, 321, 600, 658, 295, 2190, 8650, 13, 50984], "temperature": 0.0, "avg_logprob": -0.14931554642934647, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.005727234296500683}, {"id": 350, "seek": 182232, "start": 1834.72, "end": 1839.4399999999998, "text": " Yeah, you get an inkling of that from the rapid communication and certain platforms,", "tokens": [50984, 865, 11, 291, 483, 364, 11276, 1688, 295, 300, 490, 264, 7558, 6101, 293, 1629, 9473, 11, 51220], "temperature": 0.0, "avg_logprob": -0.14931554642934647, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.005727234296500683}, {"id": 351, "seek": 182232, "start": 1839.4399999999998, "end": 1843.84, "text": " like on Reddit, there's different communities, and they'll come up with different slang,", "tokens": [51220, 411, 322, 32210, 11, 456, 311, 819, 4456, 11, 293, 436, 603, 808, 493, 365, 819, 42517, 11, 51440], "temperature": 0.0, "avg_logprob": -0.14931554642934647, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.005727234296500683}, {"id": 352, "seek": 182232, "start": 1843.84, "end": 1849.6, "text": " usually from my perspective, German by a little bit of humor, or maybe mockery or whatever,", "tokens": [51440, 2673, 490, 452, 4585, 11, 6521, 538, 257, 707, 857, 295, 14318, 11, 420, 1310, 17362, 2109, 420, 2035, 11, 51728], "temperature": 0.0, "avg_logprob": -0.14931554642934647, "compression_ratio": 1.7108843537414966, "no_speech_prob": 0.005727234296500683}, {"id": 353, "seek": 184960, "start": 1850.1599999999999, "end": 1856.0, "text": " you know, just talking shit in different kinds of ways. And you could see the evolution", "tokens": [50392, 291, 458, 11, 445, 1417, 4611, 294, 819, 3685, 295, 2098, 13, 400, 291, 727, 536, 264, 9303, 50684], "temperature": 0.0, "avg_logprob": -0.12087698604749597, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.0023228491190820932}, {"id": 354, "seek": 184960, "start": 1856.8799999999999, "end": 1864.8, "text": " of language there. Because I think a lot of things on the internet, you don't want to be the boring", "tokens": [50728, 295, 2856, 456, 13, 1436, 286, 519, 257, 688, 295, 721, 322, 264, 4705, 11, 291, 500, 380, 528, 281, 312, 264, 9989, 51124], "temperature": 0.0, "avg_logprob": -0.12087698604749597, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.0023228491190820932}, {"id": 355, "seek": 184960, "start": 1864.8, "end": 1872.48, "text": " mainstream. So you like want to deviate from the proper way of talking. And so you get a lot of", "tokens": [51124, 15960, 13, 407, 291, 411, 528, 281, 1905, 13024, 490, 264, 2296, 636, 295, 1417, 13, 400, 370, 291, 483, 257, 688, 295, 51508], "temperature": 0.0, "avg_logprob": -0.12087698604749597, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.0023228491190820932}, {"id": 356, "seek": 184960, "start": 1872.48, "end": 1878.48, "text": " deviation, like rapid deviation, then when communities collide, you get like, just like you", "tokens": [51508, 25163, 11, 411, 7558, 25163, 11, 550, 562, 4456, 49093, 11, 291, 483, 411, 11, 445, 411, 291, 51808], "temperature": 0.0, "avg_logprob": -0.12087698604749597, "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.0023228491190820932}, {"id": 357, "seek": 187848, "start": 1878.48, "end": 1882.48, "text": " said, humans adapt to it. And you can see it through the lens of humor. I mean, it's very", "tokens": [50364, 848, 11, 6255, 6231, 281, 309, 13, 400, 291, 393, 536, 309, 807, 264, 6765, 295, 14318, 13, 286, 914, 11, 309, 311, 588, 50564], "temperature": 0.0, "avg_logprob": -0.14272296260780012, "compression_ratio": 1.825503355704698, "no_speech_prob": 0.006094793789088726}, {"id": 358, "seek": 187848, "start": 1882.48, "end": 1886.48, "text": " difficult to study, but you can imagine like a hundred years from now, well, if there's a new", "tokens": [50564, 2252, 281, 2979, 11, 457, 291, 393, 3811, 411, 257, 3262, 924, 490, 586, 11, 731, 11, 498, 456, 311, 257, 777, 50764], "temperature": 0.0, "avg_logprob": -0.14272296260780012, "compression_ratio": 1.825503355704698, "no_speech_prob": 0.006094793789088726}, {"id": 359, "seek": 187848, "start": 1886.48, "end": 1889.92, "text": " language born, for example, we'll get really high resolution data on.", "tokens": [50764, 2856, 4232, 11, 337, 1365, 11, 321, 603, 483, 534, 1090, 8669, 1412, 322, 13, 50936], "temperature": 0.0, "avg_logprob": -0.14272296260780012, "compression_ratio": 1.825503355704698, "no_speech_prob": 0.006094793789088726}, {"id": 360, "seek": 187848, "start": 1889.92, "end": 1894.48, "text": " I mean, English is changing. English changes all the time. All languages change all the time. So,", "tokens": [50936, 286, 914, 11, 3669, 307, 4473, 13, 3669, 2962, 439, 264, 565, 13, 1057, 8650, 1319, 439, 264, 565, 13, 407, 11, 51164], "temperature": 0.0, "avg_logprob": -0.14272296260780012, "compression_ratio": 1.825503355704698, "no_speech_prob": 0.006094793789088726}, {"id": 361, "seek": 187848, "start": 1895.1200000000001, "end": 1902.4, "text": " you know, it's a famous result about the Queen's English. So if you look at the Queen's vowels,", "tokens": [51196, 291, 458, 11, 309, 311, 257, 4618, 1874, 466, 264, 10077, 311, 3669, 13, 407, 498, 291, 574, 412, 264, 10077, 311, 44972, 11, 51560], "temperature": 0.0, "avg_logprob": -0.14272296260780012, "compression_ratio": 1.825503355704698, "no_speech_prob": 0.006094793789088726}, {"id": 362, "seek": 187848, "start": 1902.4, "end": 1907.44, "text": " the Queen's English is supposed to be, you know, originally the proper way for the talk was sort", "tokens": [51560, 264, 10077, 311, 3669, 307, 3442, 281, 312, 11, 291, 458, 11, 7993, 264, 2296, 636, 337, 264, 751, 390, 1333, 51812], "temperature": 0.0, "avg_logprob": -0.14272296260780012, "compression_ratio": 1.825503355704698, "no_speech_prob": 0.006094793789088726}, {"id": 363, "seek": 190744, "start": 1907.44, "end": 1912.96, "text": " of defined by whoever the Queen talked, or the King, whoever was in charge. And so if you look", "tokens": [50364, 295, 7642, 538, 11387, 264, 10077, 2825, 11, 420, 264, 3819, 11, 11387, 390, 294, 4602, 13, 400, 370, 498, 291, 574, 50640], "temperature": 0.0, "avg_logprob": -0.14788035095715132, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0038221068680286407}, {"id": 364, "seek": 190744, "start": 1912.96, "end": 1920.0800000000002, "text": " at how her vowels changed from when she first became Queen in 1952 or 1953, when she was", "tokens": [50640, 412, 577, 720, 44972, 3105, 490, 562, 750, 700, 3062, 10077, 294, 10858, 17, 420, 48528, 11, 562, 750, 390, 50996], "temperature": 0.0, "avg_logprob": -0.14788035095715132, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0038221068680286407}, {"id": 365, "seek": 190744, "start": 1920.0800000000002, "end": 1924.64, "text": " coronated, the first, I mean, that's Queen Elizabeth, who died recently, of course, until,", "tokens": [50996, 10451, 770, 11, 264, 700, 11, 286, 914, 11, 300, 311, 10077, 12978, 11, 567, 4539, 3938, 11, 295, 1164, 11, 1826, 11, 51224], "temperature": 0.0, "avg_logprob": -0.14788035095715132, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0038221068680286407}, {"id": 366, "seek": 190744, "start": 1924.64, "end": 1929.2, "text": " you know, 50 years later, her vowels changed, her vowels shifted a lot. And so that, you know,", "tokens": [51224, 291, 458, 11, 2625, 924, 1780, 11, 720, 44972, 3105, 11, 720, 44972, 18892, 257, 688, 13, 400, 370, 300, 11, 291, 458, 11, 51452], "temperature": 0.0, "avg_logprob": -0.14788035095715132, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0038221068680286407}, {"id": 367, "seek": 190744, "start": 1929.2, "end": 1935.6000000000001, "text": " even in the sounds of British English, in her, the way she was talking was changing. The vowels", "tokens": [51452, 754, 294, 264, 3263, 295, 6221, 3669, 11, 294, 720, 11, 264, 636, 750, 390, 1417, 390, 4473, 13, 440, 44972, 51772], "temperature": 0.0, "avg_logprob": -0.14788035095715132, "compression_ratio": 1.7095588235294117, "no_speech_prob": 0.0038221068680286407}, {"id": 368, "seek": 193560, "start": 1935.6, "end": 1940.08, "text": " were changing slightly. So that's just, in the sounds, there's change. I don't know what's, you", "tokens": [50364, 645, 4473, 4748, 13, 407, 300, 311, 445, 11, 294, 264, 3263, 11, 456, 311, 1319, 13, 286, 500, 380, 458, 437, 311, 11, 291, 50588], "temperature": 0.0, "avg_logprob": -0.13006744687519375, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.006288463715463877}, {"id": 369, "seek": 193560, "start": 1940.08, "end": 1944.7199999999998, "text": " know, we're, I'm interested. We're all interested in what's driving any of these changes. The", "tokens": [50588, 458, 11, 321, 434, 11, 286, 478, 3102, 13, 492, 434, 439, 3102, 294, 437, 311, 4840, 604, 295, 613, 2962, 13, 440, 50820], "temperature": 0.0, "avg_logprob": -0.13006744687519375, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.006288463715463877}, {"id": 370, "seek": 193560, "start": 1944.7199999999998, "end": 1949.52, "text": " word order of English changed a lot over a thousand years, right? So it used to look like", "tokens": [50820, 1349, 1668, 295, 3669, 3105, 257, 688, 670, 257, 4714, 924, 11, 558, 30, 407, 309, 1143, 281, 574, 411, 51060], "temperature": 0.0, "avg_logprob": -0.13006744687519375, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.006288463715463877}, {"id": 371, "seek": 193560, "start": 1949.52, "end": 1954.8799999999999, "text": " German. You know, it used to be a verb final language with case marking, and it shifted", "tokens": [51060, 6521, 13, 509, 458, 11, 309, 1143, 281, 312, 257, 9595, 2572, 2856, 365, 1389, 25482, 11, 293, 309, 18892, 51328], "temperature": 0.0, "avg_logprob": -0.13006744687519375, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.006288463715463877}, {"id": 372, "seek": 193560, "start": 1954.8799999999999, "end": 1960.32, "text": " to a verb-medial language, a lot of contact. So a lot of contact with French. And it became a verb", "tokens": [51328, 281, 257, 9595, 12, 1912, 831, 2856, 11, 257, 688, 295, 3385, 13, 407, 257, 688, 295, 3385, 365, 5522, 13, 400, 309, 3062, 257, 9595, 51600], "temperature": 0.0, "avg_logprob": -0.13006744687519375, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.006288463715463877}, {"id": 373, "seek": 196032, "start": 1960.32, "end": 1965.6799999999998, "text": " medial language with no case marking. And so it became this, you know, verb, verb initially thing.", "tokens": [50364, 1205, 831, 2856, 365, 572, 1389, 25482, 13, 400, 370, 309, 3062, 341, 11, 291, 458, 11, 9595, 11, 9595, 9105, 551, 13, 50632], "temperature": 0.0, "avg_logprob": -0.13465620866462366, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.026753980666399002}, {"id": 374, "seek": 196032, "start": 1965.6799999999998, "end": 1971.52, "text": " So, and so that's, it totally evolved. And so it may very well, I mean, you know, it doesn't evolve", "tokens": [50632, 407, 11, 293, 370, 300, 311, 11, 309, 3879, 14178, 13, 400, 370, 309, 815, 588, 731, 11, 286, 914, 11, 291, 458, 11, 309, 1177, 380, 16693, 50924], "temperature": 0.0, "avg_logprob": -0.13465620866462366, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.026753980666399002}, {"id": 375, "seek": 196032, "start": 1971.52, "end": 1976.32, "text": " maybe very much in 20 years is maybe what you're talking about. But over 50 and 100 years, things", "tokens": [50924, 1310, 588, 709, 294, 945, 924, 307, 1310, 437, 291, 434, 1417, 466, 13, 583, 670, 2625, 293, 2319, 924, 11, 721, 51164], "temperature": 0.0, "avg_logprob": -0.13465620866462366, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.026753980666399002}, {"id": 376, "seek": 196032, "start": 1976.32, "end": 1982.8, "text": " change a lot, I think. We'll now have good data, which is great. Can you talk to what is syntax", "tokens": [51164, 1319, 257, 688, 11, 286, 519, 13, 492, 603, 586, 362, 665, 1412, 11, 597, 307, 869, 13, 1664, 291, 751, 281, 437, 307, 28431, 51488], "temperature": 0.0, "avg_logprob": -0.13465620866462366, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.026753980666399002}, {"id": 377, "seek": 196032, "start": 1982.8, "end": 1988.08, "text": " and what is grammar? So you wrote a book on syntax. I did. You were asking me before about what,", "tokens": [51488, 293, 437, 307, 22317, 30, 407, 291, 4114, 257, 1446, 322, 28431, 13, 286, 630, 13, 509, 645, 3365, 385, 949, 466, 437, 11, 51752], "temperature": 0.0, "avg_logprob": -0.13465620866462366, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.026753980666399002}, {"id": 378, "seek": 198808, "start": 1988.08, "end": 1992.6399999999999, "text": " you know, how do I figure out what a dependency structure is? I'd say the dependency structures", "tokens": [50364, 291, 458, 11, 577, 360, 286, 2573, 484, 437, 257, 33621, 3877, 307, 30, 286, 1116, 584, 264, 33621, 9227, 50592], "temperature": 0.0, "avg_logprob": -0.07279308422191723, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.001867202459834516}, {"id": 379, "seek": 198808, "start": 1992.6399999999999, "end": 1996.8799999999999, "text": " aren't that hard to generally, I think it's a lot of agreement of what they, of what they are", "tokens": [50592, 3212, 380, 300, 1152, 281, 5101, 11, 286, 519, 309, 311, 257, 688, 295, 8106, 295, 437, 436, 11, 295, 437, 436, 366, 50804], "temperature": 0.0, "avg_logprob": -0.07279308422191723, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.001867202459834516}, {"id": 380, "seek": 198808, "start": 1996.8799999999999, "end": 2000.96, "text": " for almost any sentence in most languages. I think people will agree on a lot of that.", "tokens": [50804, 337, 1920, 604, 8174, 294, 881, 8650, 13, 286, 519, 561, 486, 3986, 322, 257, 688, 295, 300, 13, 51008], "temperature": 0.0, "avg_logprob": -0.07279308422191723, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.001867202459834516}, {"id": 381, "seek": 198808, "start": 2002.3999999999999, "end": 2007.9199999999998, "text": " There are other parameters in the mix such that some people think there's a more complicated", "tokens": [51080, 821, 366, 661, 9834, 294, 264, 2890, 1270, 300, 512, 561, 519, 456, 311, 257, 544, 6179, 51356], "temperature": 0.0, "avg_logprob": -0.07279308422191723, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.001867202459834516}, {"id": 382, "seek": 198808, "start": 2007.9199999999998, "end": 2011.76, "text": " grammar than just a dependency structure. And so, you know, like Noam Tromsky,", "tokens": [51356, 22317, 813, 445, 257, 33621, 3877, 13, 400, 370, 11, 291, 458, 11, 411, 883, 335, 1765, 4785, 4133, 11, 51548], "temperature": 0.0, "avg_logprob": -0.07279308422191723, "compression_ratio": 1.7707509881422925, "no_speech_prob": 0.001867202459834516}, {"id": 383, "seek": 201176, "start": 2011.76, "end": 2020.0, "text": " he's the most famous linguist ever. And he is famous for proposing a slightly more complicated", "tokens": [50364, 415, 311, 264, 881, 4618, 21766, 468, 1562, 13, 400, 415, 307, 4618, 337, 29939, 257, 4748, 544, 6179, 50776], "temperature": 0.0, "avg_logprob": -0.08706692431835418, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.02227558195590973}, {"id": 384, "seek": 201176, "start": 2020.0, "end": 2026.48, "text": " syntax. And so he invented phrase structure grammar. So he's well known for many, many", "tokens": [50776, 28431, 13, 400, 370, 415, 14479, 9535, 3877, 22317, 13, 407, 415, 311, 731, 2570, 337, 867, 11, 867, 51100], "temperature": 0.0, "avg_logprob": -0.08706692431835418, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.02227558195590973}, {"id": 385, "seek": 201176, "start": 2026.48, "end": 2032.08, "text": " things. But in the 50s, in the early 60s, like the late 50s, he was basically figuring out what's", "tokens": [51100, 721, 13, 583, 294, 264, 2625, 82, 11, 294, 264, 2440, 4060, 82, 11, 411, 264, 3469, 2625, 82, 11, 415, 390, 1936, 15213, 484, 437, 311, 51380], "temperature": 0.0, "avg_logprob": -0.08706692431835418, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.02227558195590973}, {"id": 386, "seek": 201176, "start": 2032.08, "end": 2038.96, "text": " called formal language theory. So, and he figured out sort of a framework for figuring out how", "tokens": [51380, 1219, 9860, 2856, 5261, 13, 407, 11, 293, 415, 8932, 484, 1333, 295, 257, 8388, 337, 15213, 484, 577, 51724], "temperature": 0.0, "avg_logprob": -0.08706692431835418, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.02227558195590973}, {"id": 387, "seek": 203896, "start": 2038.96, "end": 2043.6000000000001, "text": " complicated language, you know, a certain type of language might be, so-called phrase structure", "tokens": [50364, 6179, 2856, 11, 291, 458, 11, 257, 1629, 2010, 295, 2856, 1062, 312, 11, 370, 12, 11880, 9535, 3877, 50596], "temperature": 0.0, "avg_logprob": -0.13539225055325416, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0017005486879497766}, {"id": 388, "seek": 203896, "start": 2043.6000000000001, "end": 2051.6, "text": " grammars of language might be. And so he, his, his idea was that maybe we can, we can think", "tokens": [50596, 17570, 685, 295, 2856, 1062, 312, 13, 400, 370, 415, 11, 702, 11, 702, 1558, 390, 300, 1310, 321, 393, 11, 321, 393, 519, 50996], "temperature": 0.0, "avg_logprob": -0.13539225055325416, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0017005486879497766}, {"id": 389, "seek": 203896, "start": 2051.6, "end": 2056.7200000000003, "text": " about the complexity of a language by how complicated the rules are. Okay. And the rules", "tokens": [50996, 466, 264, 14024, 295, 257, 2856, 538, 577, 6179, 264, 4474, 366, 13, 1033, 13, 400, 264, 4474, 51252], "temperature": 0.0, "avg_logprob": -0.13539225055325416, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0017005486879497766}, {"id": 390, "seek": 203896, "start": 2057.44, "end": 2062.8, "text": " will look like this. They will have a left-hand side and they'll have a right-hand side. Something", "tokens": [51288, 486, 574, 411, 341, 13, 814, 486, 362, 257, 1411, 12, 5543, 1252, 293, 436, 603, 362, 257, 558, 12, 5543, 1252, 13, 6595, 51556], "temperature": 0.0, "avg_logprob": -0.13539225055325416, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0017005486879497766}, {"id": 391, "seek": 203896, "start": 2062.8, "end": 2066.0, "text": " will, on the left-hand side, will expand to the thing on the right-hand side. So we'll say we'll", "tokens": [51556, 486, 11, 322, 264, 1411, 12, 5543, 1252, 11, 486, 5268, 281, 264, 551, 322, 264, 558, 12, 5543, 1252, 13, 407, 321, 603, 584, 321, 603, 51716], "temperature": 0.0, "avg_logprob": -0.13539225055325416, "compression_ratio": 1.91869918699187, "no_speech_prob": 0.0017005486879497766}, {"id": 392, "seek": 206600, "start": 2066.0, "end": 2071.52, "text": " start with an S, which is like the root, which is a sentence. Okay. And then we're going to expand", "tokens": [50364, 722, 365, 364, 318, 11, 597, 307, 411, 264, 5593, 11, 597, 307, 257, 8174, 13, 1033, 13, 400, 550, 321, 434, 516, 281, 5268, 50640], "temperature": 0.0, "avg_logprob": -0.11651849371241772, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004903625231236219}, {"id": 393, "seek": 206600, "start": 2071.52, "end": 2077.2, "text": " to things like a noun phrase and a verb phrase is what he would say, for instance. Okay. And S", "tokens": [50640, 281, 721, 411, 257, 23307, 9535, 293, 257, 9595, 9535, 307, 437, 415, 576, 584, 11, 337, 5197, 13, 1033, 13, 400, 318, 50924], "temperature": 0.0, "avg_logprob": -0.11651849371241772, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004903625231236219}, {"id": 394, "seek": 206600, "start": 2077.2, "end": 2082.0, "text": " goes to an NP and a VP is a kind of a phrase structure rule. And then we figure out what an NP", "tokens": [50924, 1709, 281, 364, 38611, 293, 257, 35812, 307, 257, 733, 295, 257, 9535, 3877, 4978, 13, 400, 550, 321, 2573, 484, 437, 364, 38611, 51164], "temperature": 0.0, "avg_logprob": -0.11651849371241772, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004903625231236219}, {"id": 395, "seek": 206600, "start": 2082.0, "end": 2087.68, "text": " is. An NP is a determiner and a noun, for instance. And verb phrase is something else,", "tokens": [51164, 307, 13, 1107, 38611, 307, 257, 3618, 4564, 293, 257, 23307, 11, 337, 5197, 13, 400, 9595, 9535, 307, 746, 1646, 11, 51448], "temperature": 0.0, "avg_logprob": -0.11651849371241772, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004903625231236219}, {"id": 396, "seek": 206600, "start": 2087.68, "end": 2092.4, "text": " is a verb and another noun phrase and another NP, for instance. Those are the rules of a very", "tokens": [51448, 307, 257, 9595, 293, 1071, 23307, 9535, 293, 1071, 38611, 11, 337, 5197, 13, 3950, 366, 264, 4474, 295, 257, 588, 51684], "temperature": 0.0, "avg_logprob": -0.11651849371241772, "compression_ratio": 1.9623430962343096, "no_speech_prob": 0.004903625231236219}, {"id": 397, "seek": 209240, "start": 2092.48, "end": 2099.44, "text": " simple phrase structure. Okay. And, and so he, he proposed phrase structure grammar as a way to", "tokens": [50368, 2199, 9535, 3877, 13, 1033, 13, 400, 11, 293, 370, 415, 11, 415, 10348, 9535, 3877, 22317, 382, 257, 636, 281, 50716], "temperature": 0.0, "avg_logprob": -0.08849629561106363, "compression_ratio": 1.921875, "no_speech_prob": 0.002433893969282508}, {"id": 398, "seek": 209240, "start": 2099.44, "end": 2104.1600000000003, "text": " sort of cover human languages. And then he actually figured out that, well, depending on the formalization", "tokens": [50716, 1333, 295, 2060, 1952, 8650, 13, 400, 550, 415, 767, 8932, 484, 300, 11, 731, 11, 5413, 322, 264, 9860, 2144, 50952], "temperature": 0.0, "avg_logprob": -0.08849629561106363, "compression_ratio": 1.921875, "no_speech_prob": 0.002433893969282508}, {"id": 399, "seek": 209240, "start": 2104.1600000000003, "end": 2108.48, "text": " of those grammars, you might get more complicated or less complicated languages. And so you could,", "tokens": [50952, 295, 729, 17570, 685, 11, 291, 1062, 483, 544, 6179, 420, 1570, 6179, 8650, 13, 400, 370, 291, 727, 11, 51168], "temperature": 0.0, "avg_logprob": -0.08849629561106363, "compression_ratio": 1.921875, "no_speech_prob": 0.002433893969282508}, {"id": 400, "seek": 209240, "start": 2108.48, "end": 2113.2000000000003, "text": " he could, he said, well, you, these are, these are things called, you know, context-free languages,", "tokens": [51168, 415, 727, 11, 415, 848, 11, 731, 11, 291, 11, 613, 366, 11, 613, 366, 721, 1219, 11, 291, 458, 11, 4319, 12, 10792, 8650, 11, 51404], "temperature": 0.0, "avg_logprob": -0.08849629561106363, "compression_ratio": 1.921875, "no_speech_prob": 0.002433893969282508}, {"id": 401, "seek": 209240, "start": 2113.2000000000003, "end": 2117.52, "text": " that rule that he thought, you know, human languages tend to be what he calls context-free", "tokens": [51404, 300, 4978, 300, 415, 1194, 11, 291, 458, 11, 1952, 8650, 3928, 281, 312, 437, 415, 5498, 4319, 12, 10792, 51620], "temperature": 0.0, "avg_logprob": -0.08849629561106363, "compression_ratio": 1.921875, "no_speech_prob": 0.002433893969282508}, {"id": 402, "seek": 211752, "start": 2117.52, "end": 2123.04, "text": " languages. And, but there are simpler languages, which are so-called regular languages, and they", "tokens": [50364, 8650, 13, 400, 11, 457, 456, 366, 18587, 8650, 11, 597, 366, 370, 12, 11880, 3890, 8650, 11, 293, 436, 50640], "temperature": 0.0, "avg_logprob": -0.11736862747757523, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.005552825517952442}, {"id": 403, "seek": 211752, "start": 2123.04, "end": 2128.08, "text": " have a more, a more constrained form to the rules of the, of the phrase structure of, of these", "tokens": [50640, 362, 257, 544, 11, 257, 544, 38901, 1254, 281, 264, 4474, 295, 264, 11, 295, 264, 9535, 3877, 295, 11, 295, 613, 50892], "temperature": 0.0, "avg_logprob": -0.11736862747757523, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.005552825517952442}, {"id": 404, "seek": 211752, "start": 2128.08, "end": 2135.2, "text": " particular rules. So he, he basically discovered and kind of invented ways to describe the language.", "tokens": [50892, 1729, 4474, 13, 407, 415, 11, 415, 1936, 6941, 293, 733, 295, 14479, 2098, 281, 6786, 264, 2856, 13, 51248], "temperature": 0.0, "avg_logprob": -0.11736862747757523, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.005552825517952442}, {"id": 405, "seek": 211752, "start": 2135.2, "end": 2139.28, "text": " And though, and those are phrase, those are phrase structure, a human language. And he was", "tokens": [51248, 400, 1673, 11, 293, 729, 366, 9535, 11, 729, 366, 9535, 3877, 11, 257, 1952, 2856, 13, 400, 415, 390, 51452], "temperature": 0.0, "avg_logprob": -0.11736862747757523, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.005552825517952442}, {"id": 406, "seek": 211752, "start": 2139.28, "end": 2143.12, "text": " mostly interested in English initially in his, his work in the 50s.", "tokens": [51452, 5240, 3102, 294, 3669, 9105, 294, 702, 11, 702, 589, 294, 264, 2625, 82, 13, 51644], "temperature": 0.0, "avg_logprob": -0.11736862747757523, "compression_ratio": 1.796812749003984, "no_speech_prob": 0.005552825517952442}, {"id": 407, "seek": 214312, "start": 2143.12, "end": 2148.08, "text": " So quick questions around all this. So formal language theory is the big field of just studying", "tokens": [50364, 407, 1702, 1651, 926, 439, 341, 13, 407, 9860, 2856, 5261, 307, 264, 955, 2519, 295, 445, 7601, 50612], "temperature": 0.0, "avg_logprob": -0.14294845717293875, "compression_ratio": 1.80078125, "no_speech_prob": 0.0022861312609165907}, {"id": 408, "seek": 214312, "start": 2148.08, "end": 2152.0, "text": " language formally. Yes. And it doesn't have to be human language there. We can have a computer", "tokens": [50612, 2856, 25983, 13, 1079, 13, 400, 309, 1177, 380, 362, 281, 312, 1952, 2856, 456, 13, 492, 393, 362, 257, 3820, 50808], "temperature": 0.0, "avg_logprob": -0.14294845717293875, "compression_ratio": 1.80078125, "no_speech_prob": 0.0022861312609165907}, {"id": 409, "seek": 214312, "start": 2152.0, "end": 2161.6, "text": " languages, any kind of system, which is generating a, some set of expressions in a language. And", "tokens": [50808, 8650, 11, 604, 733, 295, 1185, 11, 597, 307, 17746, 257, 11, 512, 992, 295, 15277, 294, 257, 2856, 13, 400, 51288], "temperature": 0.0, "avg_logprob": -0.14294845717293875, "compression_ratio": 1.80078125, "no_speech_prob": 0.0022861312609165907}, {"id": 410, "seek": 214312, "start": 2161.6, "end": 2168.08, "text": " those could be like the, the, you know, the statements in a computer language, for example.", "tokens": [51288, 729, 727, 312, 411, 264, 11, 264, 11, 291, 458, 11, 264, 12363, 294, 257, 3820, 2856, 11, 337, 1365, 13, 51612], "temperature": 0.0, "avg_logprob": -0.14294845717293875, "compression_ratio": 1.80078125, "no_speech_prob": 0.0022861312609165907}, {"id": 411, "seek": 214312, "start": 2168.08, "end": 2171.6, "text": " So it could be that, or it could be human language. So technically, you can study", "tokens": [51612, 407, 309, 727, 312, 300, 11, 420, 309, 727, 312, 1952, 2856, 13, 407, 12120, 11, 291, 393, 2979, 51788], "temperature": 0.0, "avg_logprob": -0.14294845717293875, "compression_ratio": 1.80078125, "no_speech_prob": 0.0022861312609165907}, {"id": 412, "seek": 217160, "start": 2171.6, "end": 2176.16, "text": " programming languages. Yes. And have been, I mean, heavily studied using this formalism.", "tokens": [50364, 9410, 8650, 13, 1079, 13, 400, 362, 668, 11, 286, 914, 11, 10950, 9454, 1228, 341, 9860, 1434, 13, 50592], "temperature": 0.0, "avg_logprob": -0.17784987930702953, "compression_ratio": 1.671641791044776, "no_speech_prob": 0.0052187577821314335}, {"id": 413, "seek": 217160, "start": 2176.16, "end": 2180.64, "text": " There, there's a big field of programming languages within the formal language.", "tokens": [50592, 821, 11, 456, 311, 257, 955, 2519, 295, 9410, 8650, 1951, 264, 9860, 2856, 13, 50816], "temperature": 0.0, "avg_logprob": -0.17784987930702953, "compression_ratio": 1.671641791044776, "no_speech_prob": 0.0052187577821314335}, {"id": 414, "seek": 217160, "start": 2180.64, "end": 2186.48, "text": " Okay. And then phrase structure grammar is this idea that you can break down language into this", "tokens": [50816, 1033, 13, 400, 550, 9535, 3877, 22317, 307, 341, 1558, 300, 291, 393, 1821, 760, 2856, 666, 341, 51108], "temperature": 0.0, "avg_logprob": -0.17784987930702953, "compression_ratio": 1.671641791044776, "no_speech_prob": 0.0052187577821314335}, {"id": 415, "seek": 217160, "start": 2186.48, "end": 2193.52, "text": " SNP, VP, like type of thing. It's a particular formalism for describing language. Okay. So", "tokens": [51108, 13955, 47, 11, 35812, 11, 411, 2010, 295, 551, 13, 467, 311, 257, 1729, 9860, 1434, 337, 16141, 2856, 13, 1033, 13, 407, 51460], "temperature": 0.0, "avg_logprob": -0.17784987930702953, "compression_ratio": 1.671641791044776, "no_speech_prob": 0.0052187577821314335}, {"id": 416, "seek": 217160, "start": 2193.52, "end": 2197.36, "text": " and, and Chomsky was the first one, he's the one who figured that stuff out back in the 50s.", "tokens": [51460, 293, 11, 293, 761, 4785, 4133, 390, 264, 700, 472, 11, 415, 311, 264, 472, 567, 8932, 300, 1507, 484, 646, 294, 264, 2625, 82, 13, 51652], "temperature": 0.0, "avg_logprob": -0.17784987930702953, "compression_ratio": 1.671641791044776, "no_speech_prob": 0.0052187577821314335}, {"id": 417, "seek": 219736, "start": 2197.36, "end": 2203.04, "text": " And, and, and, but he, and, and that's equivalent. Actually, the context free grammar is actually", "tokens": [50364, 400, 11, 293, 11, 293, 11, 457, 415, 11, 293, 11, 293, 300, 311, 10344, 13, 5135, 11, 264, 4319, 1737, 22317, 307, 767, 50648], "temperature": 0.0, "avg_logprob": -0.15670307077092233, "compression_ratio": 1.8519736842105263, "no_speech_prob": 0.0012447090120986104}, {"id": 418, "seek": 219736, "start": 2203.04, "end": 2207.28, "text": " is kind of equivalent in the sense that it generates the same sentences as a dependency", "tokens": [50648, 307, 733, 295, 10344, 294, 264, 2020, 300, 309, 23815, 264, 912, 16579, 382, 257, 33621, 50860], "temperature": 0.0, "avg_logprob": -0.15670307077092233, "compression_ratio": 1.8519736842105263, "no_speech_prob": 0.0012447090120986104}, {"id": 419, "seek": 219736, "start": 2207.28, "end": 2212.08, "text": " grammar would, you know, as the dependency grammar is a little simpler in some way. You just have", "tokens": [50860, 22317, 576, 11, 291, 458, 11, 382, 264, 33621, 22317, 307, 257, 707, 18587, 294, 512, 636, 13, 509, 445, 362, 51100], "temperature": 0.0, "avg_logprob": -0.15670307077092233, "compression_ratio": 1.8519736842105263, "no_speech_prob": 0.0012447090120986104}, {"id": 420, "seek": 219736, "start": 2212.08, "end": 2217.52, "text": " a root and it goes like, we don't have any of these, the rules are implicit, I guess, in,", "tokens": [51100, 257, 5593, 293, 309, 1709, 411, 11, 321, 500, 380, 362, 604, 295, 613, 11, 264, 4474, 366, 26947, 11, 286, 2041, 11, 294, 11, 51372], "temperature": 0.0, "avg_logprob": -0.15670307077092233, "compression_ratio": 1.8519736842105263, "no_speech_prob": 0.0012447090120986104}, {"id": 421, "seek": 219736, "start": 2217.52, "end": 2221.2000000000003, "text": " in, we just have connections between words. The free structure grammars are kind of a different", "tokens": [51372, 294, 11, 321, 445, 362, 9271, 1296, 2283, 13, 440, 1737, 3877, 17570, 685, 366, 733, 295, 257, 819, 51556], "temperature": 0.0, "avg_logprob": -0.15670307077092233, "compression_ratio": 1.8519736842105263, "no_speech_prob": 0.0012447090120986104}, {"id": 422, "seek": 219736, "start": 2221.2000000000003, "end": 2226.4, "text": " way to think about the, the dependency grammar. It's slightly more complicated, but it's kind", "tokens": [51556, 636, 281, 519, 466, 264, 11, 264, 33621, 22317, 13, 467, 311, 4748, 544, 6179, 11, 457, 309, 311, 733, 51816], "temperature": 0.0, "avg_logprob": -0.15670307077092233, "compression_ratio": 1.8519736842105263, "no_speech_prob": 0.0012447090120986104}, {"id": 423, "seek": 222640, "start": 2226.4, "end": 2232.48, "text": " of the same in some ways. So to clarify, dependency grammar is the framework under", "tokens": [50364, 295, 264, 912, 294, 512, 2098, 13, 407, 281, 17594, 11, 33621, 22317, 307, 264, 8388, 833, 50668], "temperature": 0.0, "avg_logprob": -0.16194133223774276, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0027143999468535185}, {"id": 424, "seek": 222640, "start": 2232.48, "end": 2237.2000000000003, "text": " which you see language. And you make a case that this is a good way to describe the language.", "tokens": [50668, 597, 291, 536, 2856, 13, 400, 291, 652, 257, 1389, 300, 341, 307, 257, 665, 636, 281, 6786, 264, 2856, 13, 50904], "temperature": 0.0, "avg_logprob": -0.16194133223774276, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0027143999468535185}, {"id": 425, "seek": 222640, "start": 2237.2000000000003, "end": 2244.08, "text": " That's correct. And no, no Chomsky is watching this is very upset right now. So let's just kidding.", "tokens": [50904, 663, 311, 3006, 13, 400, 572, 11, 572, 761, 4785, 4133, 307, 1976, 341, 307, 588, 8340, 558, 586, 13, 407, 718, 311, 445, 9287, 13, 51248], "temperature": 0.0, "avg_logprob": -0.16194133223774276, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0027143999468535185}, {"id": 426, "seek": 222640, "start": 2244.08, "end": 2251.6, "text": " But what's the difference between where's the place of disagreement between phrase structure", "tokens": [51248, 583, 437, 311, 264, 2649, 1296, 689, 311, 264, 1081, 295, 38947, 1296, 9535, 3877, 51624], "temperature": 0.0, "avg_logprob": -0.16194133223774276, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0027143999468535185}, {"id": 427, "seek": 222640, "start": 2251.6, "end": 2255.76, "text": " grammar and dependency grammar? They're very close. So free structure grammar and dependency", "tokens": [51624, 22317, 293, 33621, 22317, 30, 814, 434, 588, 1998, 13, 407, 1737, 3877, 22317, 293, 33621, 51832], "temperature": 0.0, "avg_logprob": -0.16194133223774276, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0027143999468535185}, {"id": 428, "seek": 225576, "start": 2255.76, "end": 2261.6000000000004, "text": " grammar aren't that, aren't that far apart. I like dependency grammar, because it's more", "tokens": [50364, 22317, 3212, 380, 300, 11, 3212, 380, 300, 1400, 4936, 13, 286, 411, 33621, 22317, 11, 570, 309, 311, 544, 50656], "temperature": 0.0, "avg_logprob": -0.12070845713657616, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.0035921575035899878}, {"id": 429, "seek": 225576, "start": 2261.6000000000004, "end": 2266.0, "text": " perspicuous, it's more transparent about representing the connections between the words.", "tokens": [50656, 868, 37509, 12549, 11, 309, 311, 544, 12737, 466, 13460, 264, 9271, 1296, 264, 2283, 13, 50876], "temperature": 0.0, "avg_logprob": -0.12070845713657616, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.0035921575035899878}, {"id": 430, "seek": 225576, "start": 2266.0, "end": 2269.6000000000004, "text": " It's just a little harder to see in phrase structure grammar, you know, the place where", "tokens": [50876, 467, 311, 445, 257, 707, 6081, 281, 536, 294, 9535, 3877, 22317, 11, 291, 458, 11, 264, 1081, 689, 51056], "temperature": 0.0, "avg_logprob": -0.12070845713657616, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.0035921575035899878}, {"id": 431, "seek": 225576, "start": 2269.6000000000004, "end": 2275.5200000000004, "text": " Chomsky sort of devolved or went off from, from, from this is he also thought there was", "tokens": [51056, 761, 4785, 4133, 1333, 295, 1905, 29110, 420, 1437, 766, 490, 11, 490, 11, 490, 341, 307, 415, 611, 1194, 456, 390, 51352], "temperature": 0.0, "avg_logprob": -0.12070845713657616, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.0035921575035899878}, {"id": 432, "seek": 225576, "start": 2276.7200000000003, "end": 2281.28, "text": " something called movement. Okay. And so, and so, and that's where we disagree. Okay, that's the", "tokens": [51412, 746, 1219, 3963, 13, 1033, 13, 400, 370, 11, 293, 370, 11, 293, 300, 311, 689, 321, 14091, 13, 1033, 11, 300, 311, 264, 51640], "temperature": 0.0, "avg_logprob": -0.12070845713657616, "compression_ratio": 1.6753731343283582, "no_speech_prob": 0.0035921575035899878}, {"id": 433, "seek": 228128, "start": 2281.28, "end": 2284.96, "text": " place where I would say we disagree. And, and, and I mean, well, maybe we'll get into that later.", "tokens": [50364, 1081, 689, 286, 576, 584, 321, 14091, 13, 400, 11, 293, 11, 293, 286, 914, 11, 731, 11, 1310, 321, 603, 483, 666, 300, 1780, 13, 50548], "temperature": 0.0, "avg_logprob": -0.13578738175429306, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.016910426318645477}, {"id": 434, "seek": 228128, "start": 2284.96, "end": 2289.52, "text": " But the idea is, if you want to, do you want me to explain that? No, I would love to explain", "tokens": [50548, 583, 264, 1558, 307, 11, 498, 291, 528, 281, 11, 360, 291, 528, 385, 281, 2903, 300, 30, 883, 11, 286, 576, 959, 281, 2903, 50776], "temperature": 0.0, "avg_logprob": -0.13578738175429306, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.016910426318645477}, {"id": 435, "seek": 228128, "start": 2289.52, "end": 2294.32, "text": " movement. Okay, so you're saying so many interesting things. Okay, so here's the movement is", "tokens": [50776, 3963, 13, 1033, 11, 370, 291, 434, 1566, 370, 867, 1880, 721, 13, 1033, 11, 370, 510, 311, 264, 3963, 307, 51016], "temperature": 0.0, "avg_logprob": -0.13578738175429306, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.016910426318645477}, {"id": 436, "seek": 228128, "start": 2294.32, "end": 2299.92, "text": " Chomsky basically sees English. And he says, okay, I said, you know, we had that sentence", "tokens": [51016, 761, 4785, 4133, 1936, 8194, 3669, 13, 400, 415, 1619, 11, 1392, 11, 286, 848, 11, 291, 458, 11, 321, 632, 300, 8174, 51296], "temperature": 0.0, "avg_logprob": -0.13578738175429306, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.016910426318645477}, {"id": 437, "seek": 228128, "start": 2299.92, "end": 2303.1200000000003, "text": " early, like it was like two dogs entered the room, it's changed a little bit, say,", "tokens": [51296, 2440, 11, 411, 309, 390, 411, 732, 7197, 9065, 264, 1808, 11, 309, 311, 3105, 257, 707, 857, 11, 584, 11, 51456], "temperature": 0.0, "avg_logprob": -0.13578738175429306, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.016910426318645477}, {"id": 438, "seek": 228128, "start": 2303.1200000000003, "end": 2308.7200000000003, "text": " two dogs will enter the room. And he notices that, hey, English, if I want to make a question,", "tokens": [51456, 732, 7197, 486, 3242, 264, 1808, 13, 400, 415, 32978, 300, 11, 4177, 11, 3669, 11, 498, 286, 528, 281, 652, 257, 1168, 11, 51736], "temperature": 0.0, "avg_logprob": -0.13578738175429306, "compression_ratio": 1.7774193548387096, "no_speech_prob": 0.016910426318645477}, {"id": 439, "seek": 230872, "start": 2309.3599999999997, "end": 2313.8399999999997, "text": " yes, no question from that same sentence, I say, instead of two dogs will enter the room,", "tokens": [50396, 2086, 11, 572, 1168, 490, 300, 912, 8174, 11, 286, 584, 11, 2602, 295, 732, 7197, 486, 3242, 264, 1808, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09296251632071831, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.0015974121633917093}, {"id": 440, "seek": 230872, "start": 2313.8399999999997, "end": 2319.2799999999997, "text": " I say, will two dogs enter the room? Okay, there's a different way to say the same idea.", "tokens": [50620, 286, 584, 11, 486, 732, 7197, 3242, 264, 1808, 30, 1033, 11, 456, 311, 257, 819, 636, 281, 584, 264, 912, 1558, 13, 50892], "temperature": 0.0, "avg_logprob": -0.09296251632071831, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.0015974121633917093}, {"id": 441, "seek": 230872, "start": 2319.2799999999997, "end": 2324.0, "text": " And it's like, well, the auxiliary verb that will thing, it's at the front as opposed to", "tokens": [50892, 400, 309, 311, 411, 11, 731, 11, 264, 43741, 9595, 300, 486, 551, 11, 309, 311, 412, 264, 1868, 382, 8851, 281, 51128], "temperature": 0.0, "avg_logprob": -0.09296251632071831, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.0015974121633917093}, {"id": 442, "seek": 230872, "start": 2324.0, "end": 2328.48, "text": " in the middle. Okay. And so, and he looked, you know, if you look at English, you see that", "tokens": [51128, 294, 264, 2808, 13, 1033, 13, 400, 370, 11, 293, 415, 2956, 11, 291, 458, 11, 498, 291, 574, 412, 3669, 11, 291, 536, 300, 51352], "temperature": 0.0, "avg_logprob": -0.09296251632071831, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.0015974121633917093}, {"id": 443, "seek": 230872, "start": 2328.48, "end": 2333.2, "text": " that's true for all those modal verbs. And for other kinds of auxiliary verbs in English,", "tokens": [51352, 300, 311, 2074, 337, 439, 729, 39745, 30051, 13, 400, 337, 661, 3685, 295, 43741, 30051, 294, 3669, 11, 51588], "temperature": 0.0, "avg_logprob": -0.09296251632071831, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.0015974121633917093}, {"id": 444, "seek": 230872, "start": 2333.2, "end": 2338.0, "text": " you always do that, you always put an auxiliary verb at the front. And, and what he's, when he", "tokens": [51588, 291, 1009, 360, 300, 11, 291, 1009, 829, 364, 43741, 9595, 412, 264, 1868, 13, 400, 11, 293, 437, 415, 311, 11, 562, 415, 51828], "temperature": 0.0, "avg_logprob": -0.09296251632071831, "compression_ratio": 1.9187279151943464, "no_speech_prob": 0.0015974121633917093}, {"id": 445, "seek": 233800, "start": 2338.0, "end": 2343.6, "text": " saw that, so, you know, if I say, I can win this bet, can I win this bet, right? So I move a can", "tokens": [50364, 1866, 300, 11, 370, 11, 291, 458, 11, 498, 286, 584, 11, 286, 393, 1942, 341, 778, 11, 393, 286, 1942, 341, 778, 11, 558, 30, 407, 286, 1286, 257, 393, 50644], "temperature": 0.0, "avg_logprob": -0.1300231615702311, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.001206446555443108}, {"id": 446, "seek": 233800, "start": 2343.6, "end": 2348.88, "text": " to the front. So actually, that's a theory, I just gave you a theory there, he talks about it as", "tokens": [50644, 281, 264, 1868, 13, 407, 767, 11, 300, 311, 257, 5261, 11, 286, 445, 2729, 291, 257, 5261, 456, 11, 415, 6686, 466, 309, 382, 50908], "temperature": 0.0, "avg_logprob": -0.1300231615702311, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.001206446555443108}, {"id": 447, "seek": 233800, "start": 2348.88, "end": 2354.48, "text": " movement, that word in the declarative is the root, is the sort of default way to think about", "tokens": [50908, 3963, 11, 300, 1349, 294, 264, 16694, 1166, 307, 264, 5593, 11, 307, 264, 1333, 295, 7576, 636, 281, 519, 466, 51188], "temperature": 0.0, "avg_logprob": -0.1300231615702311, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.001206446555443108}, {"id": 448, "seek": 233800, "start": 2354.48, "end": 2359.12, "text": " the sentence, and you move the auxiliary verb to the front, that's a movement theory. Okay,", "tokens": [51188, 264, 8174, 11, 293, 291, 1286, 264, 43741, 9595, 281, 264, 1868, 11, 300, 311, 257, 3963, 5261, 13, 1033, 11, 51420], "temperature": 0.0, "avg_logprob": -0.1300231615702311, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.001206446555443108}, {"id": 449, "seek": 233800, "start": 2359.12, "end": 2365.12, "text": " he said, and he just thought that was just so obvious that it must be true, that there's nothing", "tokens": [51420, 415, 848, 11, 293, 415, 445, 1194, 300, 390, 445, 370, 6322, 300, 309, 1633, 312, 2074, 11, 300, 456, 311, 1825, 51720], "temperature": 0.0, "avg_logprob": -0.1300231615702311, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.001206446555443108}, {"id": 450, "seek": 236512, "start": 2365.12, "end": 2370.88, "text": " more to say about that, that this is how auxiliary verbs work in English. There's a movement rule,", "tokens": [50364, 544, 281, 584, 466, 300, 11, 300, 341, 307, 577, 43741, 30051, 589, 294, 3669, 13, 821, 311, 257, 3963, 4978, 11, 50652], "temperature": 0.0, "avg_logprob": -0.12093749237060547, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.03845175355672836}, {"id": 451, "seek": 236512, "start": 2371.52, "end": 2375.44, "text": " such that you're moved, like to get from the declarative to the interrogative, you're moving", "tokens": [50684, 1270, 300, 291, 434, 4259, 11, 411, 281, 483, 490, 264, 16694, 1166, 281, 264, 24871, 1166, 11, 291, 434, 2684, 50880], "temperature": 0.0, "avg_logprob": -0.12093749237060547, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.03845175355672836}, {"id": 452, "seek": 236512, "start": 2375.44, "end": 2380.0, "text": " the auxiliary to the front. And it's a little more complicated as soon as you go to simple,", "tokens": [50880, 264, 43741, 281, 264, 1868, 13, 400, 309, 311, 257, 707, 544, 6179, 382, 2321, 382, 291, 352, 281, 2199, 11, 51108], "temperature": 0.0, "avg_logprob": -0.12093749237060547, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.03845175355672836}, {"id": 453, "seek": 236512, "start": 2380.0, "end": 2384.3199999999997, "text": " simple present and simple past, because, you know, if I say, you know, John slept, you have to say,", "tokens": [51108, 2199, 1974, 293, 2199, 1791, 11, 570, 11, 291, 458, 11, 498, 286, 584, 11, 291, 458, 11, 2619, 17400, 11, 291, 362, 281, 584, 11, 51324], "temperature": 0.0, "avg_logprob": -0.12093749237060547, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.03845175355672836}, {"id": 454, "seek": 236512, "start": 2384.88, "end": 2390.0, "text": " did John sleep, not slept John, right? And so you have to somehow get an auxiliary verb, and I", "tokens": [51352, 630, 2619, 2817, 11, 406, 17400, 2619, 11, 558, 30, 400, 370, 291, 362, 281, 6063, 483, 364, 43741, 9595, 11, 293, 286, 51608], "temperature": 0.0, "avg_logprob": -0.12093749237060547, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.03845175355672836}, {"id": 455, "seek": 239000, "start": 2390.0, "end": 2395.2, "text": " guess underlyingly, it's like slept is, it's a little more complicated than that, but that's his", "tokens": [50364, 2041, 14217, 356, 11, 309, 311, 411, 17400, 307, 11, 309, 311, 257, 707, 544, 6179, 813, 300, 11, 457, 300, 311, 702, 50624], "temperature": 0.0, "avg_logprob": -0.14972447948295528, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0032727473881095648}, {"id": 456, "seek": 239000, "start": 2395.2, "end": 2399.92, "text": " idea, there's a movement, okay? And so a different way to think about that, that isn't, I mean,", "tokens": [50624, 1558, 11, 456, 311, 257, 3963, 11, 1392, 30, 400, 370, 257, 819, 636, 281, 519, 466, 300, 11, 300, 1943, 380, 11, 286, 914, 11, 50860], "temperature": 0.0, "avg_logprob": -0.14972447948295528, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0032727473881095648}, {"id": 457, "seek": 239000, "start": 2400.48, "end": 2405.36, "text": " he ended up showing later. So he proposed this theory of grammar, which has movement, and there's", "tokens": [50888, 415, 4590, 493, 4099, 1780, 13, 407, 415, 10348, 341, 5261, 295, 22317, 11, 597, 575, 3963, 11, 293, 456, 311, 51132], "temperature": 0.0, "avg_logprob": -0.14972447948295528, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0032727473881095648}, {"id": 458, "seek": 239000, "start": 2405.36, "end": 2410.4, "text": " other places where he thought there's movement, not just auxiliary verbs, but things like the passive", "tokens": [51132, 661, 3190, 689, 415, 1194, 456, 311, 3963, 11, 406, 445, 43741, 30051, 11, 457, 721, 411, 264, 14975, 51384], "temperature": 0.0, "avg_logprob": -0.14972447948295528, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0032727473881095648}, {"id": 459, "seek": 239000, "start": 2410.4, "end": 2415.84, "text": " in English, and things like questions, WH questions, a bunch of places where he thought", "tokens": [51384, 294, 3669, 11, 293, 721, 411, 1651, 11, 8183, 1651, 11, 257, 3840, 295, 3190, 689, 415, 1194, 51656], "temperature": 0.0, "avg_logprob": -0.14972447948295528, "compression_ratio": 1.7582417582417582, "no_speech_prob": 0.0032727473881095648}, {"id": 460, "seek": 241584, "start": 2415.84, "end": 2421.1200000000003, "text": " there's also movement going on. And each one of those, he thinks there's words, well, phrases", "tokens": [50364, 456, 311, 611, 3963, 516, 322, 13, 400, 1184, 472, 295, 729, 11, 415, 7309, 456, 311, 2283, 11, 731, 11, 20312, 50628], "temperature": 0.0, "avg_logprob": -0.10260159146469251, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.006690974812954664}, {"id": 461, "seek": 241584, "start": 2421.1200000000003, "end": 2424.7200000000003, "text": " and words are moving around from one structure to another, which he called deep structure to", "tokens": [50628, 293, 2283, 366, 2684, 926, 490, 472, 3877, 281, 1071, 11, 597, 415, 1219, 2452, 3877, 281, 50808], "temperature": 0.0, "avg_logprob": -0.10260159146469251, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.006690974812954664}, {"id": 462, "seek": 241584, "start": 2424.7200000000003, "end": 2428.7200000000003, "text": " surface structure. I mean, there's like two different structures in his theory, okay?", "tokens": [50808, 3753, 3877, 13, 286, 914, 11, 456, 311, 411, 732, 819, 9227, 294, 702, 5261, 11, 1392, 30, 51008], "temperature": 0.0, "avg_logprob": -0.10260159146469251, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.006690974812954664}, {"id": 463, "seek": 241584, "start": 2429.6000000000004, "end": 2435.04, "text": " There's a different way to think about this, which is there's no movement at all. There's a", "tokens": [51052, 821, 311, 257, 819, 636, 281, 519, 466, 341, 11, 597, 307, 456, 311, 572, 3963, 412, 439, 13, 821, 311, 257, 51324], "temperature": 0.0, "avg_logprob": -0.10260159146469251, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.006690974812954664}, {"id": 464, "seek": 241584, "start": 2435.04, "end": 2441.76, "text": " lexical copying rule, such that the word will or the word can, these auxiliary verbs, they just", "tokens": [51324, 476, 87, 804, 27976, 4978, 11, 1270, 300, 264, 1349, 486, 420, 264, 1349, 393, 11, 613, 43741, 30051, 11, 436, 445, 51660], "temperature": 0.0, "avg_logprob": -0.10260159146469251, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.006690974812954664}, {"id": 465, "seek": 244176, "start": 2441.76, "end": 2446.5600000000004, "text": " have two forms. And one of them is the declarative, and one of them is the interrogative. And you", "tokens": [50364, 362, 732, 6422, 13, 400, 472, 295, 552, 307, 264, 16694, 1166, 11, 293, 472, 295, 552, 307, 264, 24871, 1166, 13, 400, 291, 50604], "temperature": 0.0, "avg_logprob": -0.10478216552734375, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.037311337888240814}, {"id": 466, "seek": 244176, "start": 2446.5600000000004, "end": 2451.5200000000004, "text": " basically have the declarative one, and oh, I form the interrogative, or I can form one from", "tokens": [50604, 1936, 362, 264, 16694, 1166, 472, 11, 293, 1954, 11, 286, 1254, 264, 24871, 1166, 11, 420, 286, 393, 1254, 472, 490, 50852], "temperature": 0.0, "avg_logprob": -0.10478216552734375, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.037311337888240814}, {"id": 467, "seek": 244176, "start": 2451.5200000000004, "end": 2456.4, "text": " the other, doesn't matter which direction you go. And I just have a new entry, which has the same", "tokens": [50852, 264, 661, 11, 1177, 380, 1871, 597, 3513, 291, 352, 13, 400, 286, 445, 362, 257, 777, 8729, 11, 597, 575, 264, 912, 51096], "temperature": 0.0, "avg_logprob": -0.10478216552734375, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.037311337888240814}, {"id": 468, "seek": 244176, "start": 2456.4, "end": 2461.92, "text": " meaning, which has a slightly different argument structure, argument structure, it's a fancy word", "tokens": [51096, 3620, 11, 597, 575, 257, 4748, 819, 6770, 3877, 11, 6770, 3877, 11, 309, 311, 257, 10247, 1349, 51372], "temperature": 0.0, "avg_logprob": -0.10478216552734375, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.037311337888240814}, {"id": 469, "seek": 244176, "start": 2461.92, "end": 2469.2000000000003, "text": " for the ordering of the words. And so if I say, you know, it was the dogs, two dogs can or will", "tokens": [51372, 337, 264, 21739, 295, 264, 2283, 13, 400, 370, 498, 286, 584, 11, 291, 458, 11, 309, 390, 264, 7197, 11, 732, 7197, 393, 420, 486, 51736], "temperature": 0.0, "avg_logprob": -0.10478216552734375, "compression_ratio": 1.9435483870967742, "no_speech_prob": 0.037311337888240814}, {"id": 470, "seek": 246920, "start": 2469.2, "end": 2477.12, "text": " enter the room. There's two forms of will. One is will declarative. And then, okay, I've got my", "tokens": [50364, 3242, 264, 1808, 13, 821, 311, 732, 6422, 295, 486, 13, 1485, 307, 486, 16694, 1166, 13, 400, 550, 11, 1392, 11, 286, 600, 658, 452, 50760], "temperature": 0.0, "avg_logprob": -0.10221813492855783, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0010321740992367268}, {"id": 471, "seek": 246920, "start": 2477.12, "end": 2482.16, "text": " subject to the left, it comes before me, and the verb comes after me in that one. And then", "tokens": [50760, 3983, 281, 264, 1411, 11, 309, 1487, 949, 385, 11, 293, 264, 9595, 1487, 934, 385, 294, 300, 472, 13, 400, 550, 51012], "temperature": 0.0, "avg_logprob": -0.10221813492855783, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0010321740992367268}, {"id": 472, "seek": 246920, "start": 2482.16, "end": 2487.4399999999996, "text": " the will interrogative is like, oh, I go first, interrogative will is first, and then I have", "tokens": [51012, 264, 486, 24871, 1166, 307, 411, 11, 1954, 11, 286, 352, 700, 11, 24871, 1166, 486, 307, 700, 11, 293, 550, 286, 362, 51276], "temperature": 0.0, "avg_logprob": -0.10221813492855783, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0010321740992367268}, {"id": 473, "seek": 246920, "start": 2487.4399999999996, "end": 2491.6, "text": " the subject immediately after, and then the verb after that. And so you just, you can just", "tokens": [51276, 264, 3983, 4258, 934, 11, 293, 550, 264, 9595, 934, 300, 13, 400, 370, 291, 445, 11, 291, 393, 445, 51484], "temperature": 0.0, "avg_logprob": -0.10221813492855783, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0010321740992367268}, {"id": 474, "seek": 246920, "start": 2491.6, "end": 2496.16, "text": " generate from one of those words, another word with a slightly different argument structure", "tokens": [51484, 8460, 490, 472, 295, 729, 2283, 11, 1071, 1349, 365, 257, 4748, 819, 6770, 3877, 51712], "temperature": 0.0, "avg_logprob": -0.10221813492855783, "compression_ratio": 1.7837837837837838, "no_speech_prob": 0.0010321740992367268}, {"id": 475, "seek": 249616, "start": 2496.24, "end": 2501.52, "text": " with different ordering. And these are just lexical copies. They're not necessarily moving", "tokens": [50368, 365, 819, 21739, 13, 400, 613, 366, 445, 476, 87, 804, 14341, 13, 814, 434, 406, 4725, 2684, 50632], "temperature": 0.0, "avg_logprob": -0.1421248726222826, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.022966550663113594}, {"id": 476, "seek": 249616, "start": 2501.52, "end": 2505.3599999999997, "text": " from one to another. There's no movement. There's a romantic notion that you have one", "tokens": [50632, 490, 472, 281, 1071, 13, 821, 311, 572, 3963, 13, 821, 311, 257, 13590, 10710, 300, 291, 362, 472, 50824], "temperature": 0.0, "avg_logprob": -0.1421248726222826, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.022966550663113594}, {"id": 477, "seek": 249616, "start": 2506.16, "end": 2511.68, "text": " main way to use a word, and then you could move it around, which is essentially what", "tokens": [50864, 2135, 636, 281, 764, 257, 1349, 11, 293, 550, 291, 727, 1286, 309, 926, 11, 597, 307, 4476, 437, 51140], "temperature": 0.0, "avg_logprob": -0.1421248726222826, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.022966550663113594}, {"id": 478, "seek": 249616, "start": 2511.68, "end": 2518.24, "text": " movement is applying. Yeah, but that's the lexical copying is similar. So then we do lexical copying", "tokens": [51140, 3963, 307, 9275, 13, 865, 11, 457, 300, 311, 264, 476, 87, 804, 27976, 307, 2531, 13, 407, 550, 321, 360, 476, 87, 804, 27976, 51468], "temperature": 0.0, "avg_logprob": -0.1421248726222826, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.022966550663113594}, {"id": 479, "seek": 249616, "start": 2518.24, "end": 2523.6, "text": " for that same idea that maybe the declarative is the source, and then we can copy it. And so", "tokens": [51468, 337, 300, 912, 1558, 300, 1310, 264, 16694, 1166, 307, 264, 4009, 11, 293, 550, 321, 393, 5055, 309, 13, 400, 370, 51736], "temperature": 0.0, "avg_logprob": -0.1421248726222826, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.022966550663113594}, {"id": 480, "seek": 252360, "start": 2523.68, "end": 2529.12, "text": " an advantage, there's multiple advantages of the lexical copying story. It's not my story. This is", "tokens": [50368, 364, 5002, 11, 456, 311, 3866, 14906, 295, 264, 476, 87, 804, 27976, 1657, 13, 467, 311, 406, 452, 1657, 13, 639, 307, 50640], "temperature": 0.0, "avg_logprob": -0.11309770584106445, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.00192647788207978}, {"id": 481, "seek": 252360, "start": 2529.12, "end": 2536.08, "text": " like Ivan Sog, a bunch of linguists have been proposing these stories as well in tandem with", "tokens": [50640, 411, 28893, 407, 70, 11, 257, 3840, 295, 21766, 1751, 362, 668, 29939, 613, 3676, 382, 731, 294, 48120, 365, 50988], "temperature": 0.0, "avg_logprob": -0.11309770584106445, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.00192647788207978}, {"id": 482, "seek": 252360, "start": 2536.08, "end": 2542.48, "text": " the movement story. Ivan Sog died a while ago, but he was one of the proponents of the", "tokens": [50988, 264, 3963, 1657, 13, 28893, 407, 70, 4539, 257, 1339, 2057, 11, 457, 415, 390, 472, 295, 264, 2365, 40496, 295, 264, 51308], "temperature": 0.0, "avg_logprob": -0.11309770584106445, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.00192647788207978}, {"id": 483, "seek": 252360, "start": 2542.48, "end": 2548.72, "text": " non-movement of the lexical copying story. And so that is that a great advantage is, well,", "tokens": [51308, 2107, 12, 76, 1682, 518, 295, 264, 476, 87, 804, 27976, 1657, 13, 400, 370, 300, 307, 300, 257, 869, 5002, 307, 11, 731, 11, 51620], "temperature": 0.0, "avg_logprob": -0.11309770584106445, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.00192647788207978}, {"id": 484, "seek": 254872, "start": 2549.4399999999996, "end": 2557.8399999999997, "text": " Chomsky, really famously in 1971, showed that the movement story leads to learnability problems.", "tokens": [50400, 761, 4785, 4133, 11, 534, 34360, 294, 34578, 11, 4712, 300, 264, 3963, 1657, 6689, 281, 1466, 2310, 2740, 13, 50820], "temperature": 0.0, "avg_logprob": -0.09453008885969195, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0014102434506639838}, {"id": 485, "seek": 254872, "start": 2557.8399999999997, "end": 2564.3199999999997, "text": " It leads to problems for how language is learned. It's really, really hard to figure out what the", "tokens": [50820, 467, 6689, 281, 2740, 337, 577, 2856, 307, 3264, 13, 467, 311, 534, 11, 534, 1152, 281, 2573, 484, 437, 264, 51144], "temperature": 0.0, "avg_logprob": -0.09453008885969195, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0014102434506639838}, {"id": 486, "seek": 254872, "start": 2564.3199999999997, "end": 2568.72, "text": " underlying structure of a language is if you have both phrase structure and movement. It's like", "tokens": [51144, 14217, 3877, 295, 257, 2856, 307, 498, 291, 362, 1293, 9535, 3877, 293, 3963, 13, 467, 311, 411, 51364], "temperature": 0.0, "avg_logprob": -0.09453008885969195, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0014102434506639838}, {"id": 487, "seek": 254872, "start": 2568.72, "end": 2573.4399999999996, "text": " really hard to figure out what came from what. There's like a lot of possibilities there. If", "tokens": [51364, 534, 1152, 281, 2573, 484, 437, 1361, 490, 437, 13, 821, 311, 411, 257, 688, 295, 12178, 456, 13, 759, 51600], "temperature": 0.0, "avg_logprob": -0.09453008885969195, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0014102434506639838}, {"id": 488, "seek": 254872, "start": 2573.4399999999996, "end": 2578.3199999999997, "text": " you don't have that problem, the learning problem gets a lot easier. Just say there's lexical copies.", "tokens": [51600, 291, 500, 380, 362, 300, 1154, 11, 264, 2539, 1154, 2170, 257, 688, 3571, 13, 1449, 584, 456, 311, 476, 87, 804, 14341, 13, 51844], "temperature": 0.0, "avg_logprob": -0.09453008885969195, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0014102434506639838}, {"id": 489, "seek": 257872, "start": 2579.68, "end": 2582.9599999999996, "text": " Well, we say the learning problem. Do you mean like humans learning a new language?", "tokens": [50412, 1042, 11, 321, 584, 264, 2539, 1154, 13, 1144, 291, 914, 411, 6255, 2539, 257, 777, 2856, 30, 50576], "temperature": 0.0, "avg_logprob": -0.18589970189282018, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0013456834712997079}, {"id": 490, "seek": 257872, "start": 2582.9599999999996, "end": 2588.24, "text": " Yeah, just learning English. So baby is lying around, listening to the crib, listening to me talk,", "tokens": [50576, 865, 11, 445, 2539, 3669, 13, 407, 3186, 307, 8493, 926, 11, 4764, 281, 264, 47163, 11, 4764, 281, 385, 751, 11, 50840], "temperature": 0.0, "avg_logprob": -0.18589970189282018, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0013456834712997079}, {"id": 491, "seek": 257872, "start": 2588.24, "end": 2592.64, "text": " and how are they learning English? Or maybe it's a two-year-old who's learning", "tokens": [50840, 293, 577, 366, 436, 2539, 3669, 30, 1610, 1310, 309, 311, 257, 732, 12, 5294, 12, 2641, 567, 311, 2539, 51060], "temperature": 0.0, "avg_logprob": -0.18589970189282018, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0013456834712997079}, {"id": 492, "seek": 257872, "start": 2593.2, "end": 2599.04, "text": " interrogatives and stuff. How are they doing that? Are they doing it from like, are they figuring out?", "tokens": [51088, 24871, 4884, 293, 1507, 13, 1012, 366, 436, 884, 300, 30, 2014, 436, 884, 309, 490, 411, 11, 366, 436, 15213, 484, 30, 51380], "temperature": 0.0, "avg_logprob": -0.18589970189282018, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0013456834712997079}, {"id": 493, "seek": 257872, "start": 2600.3999999999996, "end": 2604.56, "text": " So Chomsky said it's impossible to figure it out, actually. He said it's actually impossible,", "tokens": [51448, 407, 761, 4785, 4133, 848, 309, 311, 6243, 281, 2573, 309, 484, 11, 767, 13, 634, 848, 309, 311, 767, 6243, 11, 51656], "temperature": 0.0, "avg_logprob": -0.18589970189282018, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.0013456834712997079}, {"id": 494, "seek": 260456, "start": 2605.2, "end": 2610.4, "text": " not hard, but impossible. And therefore, that's where universal grammar comes from,", "tokens": [50396, 406, 1152, 11, 457, 6243, 13, 400, 4412, 11, 300, 311, 689, 11455, 22317, 1487, 490, 11, 50656], "temperature": 0.0, "avg_logprob": -0.16722503594592608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.002888711402192712}, {"id": 495, "seek": 260456, "start": 2610.4, "end": 2616.7999999999997, "text": " is that it has to be built in. And so what they're learning is that there's some built-in movement", "tokens": [50656, 307, 300, 309, 575, 281, 312, 3094, 294, 13, 400, 370, 437, 436, 434, 2539, 307, 300, 456, 311, 512, 3094, 12, 259, 3963, 50976], "temperature": 0.0, "avg_logprob": -0.16722503594592608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.002888711402192712}, {"id": 496, "seek": 260456, "start": 2616.7999999999997, "end": 2623.44, "text": " is built in in his story is absolutely part of your language module. And then you are,", "tokens": [50976, 307, 3094, 294, 294, 702, 1657, 307, 3122, 644, 295, 428, 2856, 10088, 13, 400, 550, 291, 366, 11, 51308], "temperature": 0.0, "avg_logprob": -0.16722503594592608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.002888711402192712}, {"id": 497, "seek": 260456, "start": 2624.08, "end": 2628.08, "text": " you're just setting parameters. You're set, depending on English, is just sort of a variant", "tokens": [51340, 291, 434, 445, 3287, 9834, 13, 509, 434, 992, 11, 5413, 322, 3669, 11, 307, 445, 1333, 295, 257, 17501, 51540], "temperature": 0.0, "avg_logprob": -0.16722503594592608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.002888711402192712}, {"id": 498, "seek": 260456, "start": 2628.08, "end": 2633.04, "text": " of the universal grammar. And you're figuring out, oh, which orders does English do these things?", "tokens": [51540, 295, 264, 11455, 22317, 13, 400, 291, 434, 15213, 484, 11, 1954, 11, 597, 9470, 775, 3669, 360, 613, 721, 30, 51788], "temperature": 0.0, "avg_logprob": -0.16722503594592608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.002888711402192712}, {"id": 499, "seek": 263304, "start": 2633.6, "end": 2640.64, "text": " The non-movement story doesn't have this. It's like much more bottom-up. You're learning rules.", "tokens": [50392, 440, 2107, 12, 76, 1682, 518, 1657, 1177, 380, 362, 341, 13, 467, 311, 411, 709, 544, 2767, 12, 1010, 13, 509, 434, 2539, 4474, 13, 50744], "temperature": 0.0, "avg_logprob": -0.17360305786132812, "compression_ratio": 1.674641148325359, "no_speech_prob": 0.0010004400974139571}, {"id": 500, "seek": 263304, "start": 2640.64, "end": 2645.36, "text": " You're learning rules one by one. And, oh, this word is connected to that word.", "tokens": [50744, 509, 434, 2539, 4474, 472, 538, 472, 13, 400, 11, 1954, 11, 341, 1349, 307, 4582, 281, 300, 1349, 13, 50980], "temperature": 0.0, "avg_logprob": -0.17360305786132812, "compression_ratio": 1.674641148325359, "no_speech_prob": 0.0010004400974139571}, {"id": 501, "seek": 263304, "start": 2646.8, "end": 2650.72, "text": " Another advantage, it's learnable. Another advantage of it is that it predicts", "tokens": [51052, 3996, 5002, 11, 309, 311, 1466, 712, 13, 3996, 5002, 295, 309, 307, 300, 309, 6069, 82, 51248], "temperature": 0.0, "avg_logprob": -0.17360305786132812, "compression_ratio": 1.674641148325359, "no_speech_prob": 0.0010004400974139571}, {"id": 502, "seek": 263304, "start": 2651.2799999999997, "end": 2656.16, "text": " that not all auxiliary might move. Like, it might depend on the word, depending on whether you,", "tokens": [51276, 300, 406, 439, 43741, 1062, 1286, 13, 1743, 11, 309, 1062, 5672, 322, 264, 1349, 11, 5413, 322, 1968, 291, 11, 51520], "temperature": 0.0, "avg_logprob": -0.17360305786132812, "compression_ratio": 1.674641148325359, "no_speech_prob": 0.0010004400974139571}, {"id": 503, "seek": 265616, "start": 2656.72, "end": 2663.52, "text": " and that turns out to be true. So there's words that don't really work as auxiliary.", "tokens": [50392, 293, 300, 4523, 484, 281, 312, 2074, 13, 407, 456, 311, 2283, 300, 500, 380, 534, 589, 382, 43741, 13, 50732], "temperature": 0.0, "avg_logprob": -0.14585278080958947, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.03208872303366661}, {"id": 504, "seek": 265616, "start": 2663.52, "end": 2669.44, "text": " They work in declarative and not in interrogative. So I can say, I'll give you the opposite first.", "tokens": [50732, 814, 589, 294, 16694, 1166, 293, 406, 294, 24871, 1166, 13, 407, 286, 393, 584, 11, 286, 603, 976, 291, 264, 6182, 700, 13, 51028], "temperature": 0.0, "avg_logprob": -0.14585278080958947, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.03208872303366661}, {"id": 505, "seek": 265616, "start": 2669.44, "end": 2674.56, "text": " I can say, aren't I invited to the party? Okay. And that's an interrogative form,", "tokens": [51028, 286, 393, 584, 11, 3212, 380, 286, 9185, 281, 264, 3595, 30, 1033, 13, 400, 300, 311, 364, 24871, 1166, 1254, 11, 51284], "temperature": 0.0, "avg_logprob": -0.14585278080958947, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.03208872303366661}, {"id": 506, "seek": 265616, "start": 2675.2799999999997, "end": 2680.48, "text": " but it's not from, I aren't invited to the party. There is no I aren't, right? So that's", "tokens": [51320, 457, 309, 311, 406, 490, 11, 286, 3212, 380, 9185, 281, 264, 3595, 13, 821, 307, 572, 286, 3212, 380, 11, 558, 30, 407, 300, 311, 51580], "temperature": 0.0, "avg_logprob": -0.14585278080958947, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.03208872303366661}, {"id": 507, "seek": 268048, "start": 2680.56, "end": 2687.76, "text": " interrogative only. And then we also have forms like ought. I ought to do this.", "tokens": [50368, 24871, 1166, 787, 13, 400, 550, 321, 611, 362, 6422, 411, 13416, 13, 286, 13416, 281, 360, 341, 13, 50728], "temperature": 0.0, "avg_logprob": -0.17280464472733145, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.11275632679462433}, {"id": 508, "seek": 268048, "start": 2688.56, "end": 2691.04, "text": " And I guess some British, old British people can say,", "tokens": [50768, 400, 286, 2041, 512, 6221, 11, 1331, 6221, 561, 393, 584, 11, 50892], "temperature": 0.0, "avg_logprob": -0.17280464472733145, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.11275632679462433}, {"id": 509, "seek": 268048, "start": 2691.92, "end": 2696.2400000000002, "text": " exactly. It doesn't sound right, does it? For me, it sounds ridiculous. I don't even", "tokens": [50936, 2293, 13, 467, 1177, 380, 1626, 558, 11, 775, 309, 30, 1171, 385, 11, 309, 3263, 11083, 13, 286, 500, 380, 754, 51152], "temperature": 0.0, "avg_logprob": -0.17280464472733145, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.11275632679462433}, {"id": 510, "seek": 268048, "start": 2696.2400000000002, "end": 2699.92, "text": " think ought is great, but I mean, I totally recognize I ought to do it. It's not too bad,", "tokens": [51152, 519, 13416, 307, 869, 11, 457, 286, 914, 11, 286, 3879, 5521, 286, 13416, 281, 360, 309, 13, 467, 311, 406, 886, 1578, 11, 51336], "temperature": 0.0, "avg_logprob": -0.17280464472733145, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.11275632679462433}, {"id": 511, "seek": 268048, "start": 2699.92, "end": 2702.08, "text": " actually. I can say ought to do this. That sounds pretty good.", "tokens": [51336, 767, 13, 286, 393, 584, 13416, 281, 360, 341, 13, 663, 3263, 1238, 665, 13, 51444], "temperature": 0.0, "avg_logprob": -0.17280464472733145, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.11275632679462433}, {"id": 512, "seek": 268048, "start": 2702.08, "end": 2706.08, "text": " If I'm trying to sound sophisticated, maybe. I don't know. It just sounds completely out of", "tokens": [51444, 759, 286, 478, 1382, 281, 1626, 16950, 11, 1310, 13, 286, 500, 380, 458, 13, 467, 445, 3263, 2584, 484, 295, 51644], "temperature": 0.0, "avg_logprob": -0.17280464472733145, "compression_ratio": 1.721189591078067, "no_speech_prob": 0.11275632679462433}, {"id": 513, "seek": 270608, "start": 2706.96, "end": 2712.72, "text": " to me. Anyway, so there are variants here. And a lot of these words just work in one", "tokens": [50408, 281, 385, 13, 5684, 11, 370, 456, 366, 21669, 510, 13, 400, 257, 688, 295, 613, 2283, 445, 589, 294, 472, 50696], "temperature": 0.0, "avg_logprob": -0.15743045149178342, "compression_ratio": 1.7165991902834008, "no_speech_prob": 0.004608381073921919}, {"id": 514, "seek": 270608, "start": 2712.72, "end": 2717.2, "text": " versus the other. And that's fine under the lexical copying story. It's like, well,", "tokens": [50696, 5717, 264, 661, 13, 400, 300, 311, 2489, 833, 264, 476, 87, 804, 27976, 1657, 13, 467, 311, 411, 11, 731, 11, 50920], "temperature": 0.0, "avg_logprob": -0.15743045149178342, "compression_ratio": 1.7165991902834008, "no_speech_prob": 0.004608381073921919}, {"id": 515, "seek": 270608, "start": 2717.2, "end": 2723.52, "text": " you just learned the usage. Whatever the usage is, is what you do with this word.", "tokens": [50920, 291, 445, 3264, 264, 14924, 13, 8541, 264, 14924, 307, 11, 307, 437, 291, 360, 365, 341, 1349, 13, 51236], "temperature": 0.0, "avg_logprob": -0.15743045149178342, "compression_ratio": 1.7165991902834008, "no_speech_prob": 0.004608381073921919}, {"id": 516, "seek": 270608, "start": 2723.52, "end": 2727.7599999999998, "text": " But it doesn't, it's a little bit harder in the movement story. The movement story,", "tokens": [51236, 583, 309, 1177, 380, 11, 309, 311, 257, 707, 857, 6081, 294, 264, 3963, 1657, 13, 440, 3963, 1657, 11, 51448], "temperature": 0.0, "avg_logprob": -0.15743045149178342, "compression_ratio": 1.7165991902834008, "no_speech_prob": 0.004608381073921919}, {"id": 517, "seek": 270608, "start": 2727.7599999999998, "end": 2731.52, "text": " like that's an advantage, I think, of lexical copying. And in all these different places,", "tokens": [51448, 411, 300, 311, 364, 5002, 11, 286, 519, 11, 295, 476, 87, 804, 27976, 13, 400, 294, 439, 613, 819, 3190, 11, 51636], "temperature": 0.0, "avg_logprob": -0.15743045149178342, "compression_ratio": 1.7165991902834008, "no_speech_prob": 0.004608381073921919}, {"id": 518, "seek": 273152, "start": 2731.84, "end": 2739.7599999999998, "text": " there's all these usage variants which make the movement story a little bit harder to work.", "tokens": [50380, 456, 311, 439, 613, 14924, 21669, 597, 652, 264, 3963, 1657, 257, 707, 857, 6081, 281, 589, 13, 50776], "temperature": 0.0, "avg_logprob": -0.15586609641710916, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0012841179268434644}, {"id": 519, "seek": 273152, "start": 2739.7599999999998, "end": 2743.6, "text": " So one of the main divisions here is the movement story versus the lexical copy story", "tokens": [50776, 407, 472, 295, 264, 2135, 24328, 510, 307, 264, 3963, 1657, 5717, 264, 476, 87, 804, 5055, 1657, 50968], "temperature": 0.0, "avg_logprob": -0.15586609641710916, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0012841179268434644}, {"id": 520, "seek": 273152, "start": 2743.6, "end": 2748.96, "text": " that has to do about the auxiliary words and so on. But if you're relying to the phrase", "tokens": [50968, 300, 575, 281, 360, 466, 264, 43741, 2283, 293, 370, 322, 13, 583, 498, 291, 434, 24140, 281, 264, 9535, 51236], "temperature": 0.0, "avg_logprob": -0.15586609641710916, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0012841179268434644}, {"id": 521, "seek": 273152, "start": 2748.96, "end": 2754.88, "text": " structure grammar versus dependency grammar. Those are equivalent in some sense in that", "tokens": [51236, 3877, 22317, 5717, 33621, 22317, 13, 3950, 366, 10344, 294, 512, 2020, 294, 300, 51532], "temperature": 0.0, "avg_logprob": -0.15586609641710916, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0012841179268434644}, {"id": 522, "seek": 273152, "start": 2754.88, "end": 2759.92, "text": " for any dependency grammar, I can generate a free structure grammar which generates", "tokens": [51532, 337, 604, 33621, 22317, 11, 286, 393, 8460, 257, 1737, 3877, 22317, 597, 23815, 51784], "temperature": 0.0, "avg_logprob": -0.15586609641710916, "compression_ratio": 1.7836734693877552, "no_speech_prob": 0.0012841179268434644}, {"id": 523, "seek": 275992, "start": 2759.92, "end": 2767.28, "text": " exactly the same sentences. I just like the dependency grammar formalism because it makes", "tokens": [50364, 2293, 264, 912, 16579, 13, 286, 445, 411, 264, 33621, 22317, 9860, 1434, 570, 309, 1669, 50732], "temperature": 0.0, "avg_logprob": -0.13418902291191948, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.0020183161832392216}, {"id": 524, "seek": 275992, "start": 2767.28, "end": 2773.52, "text": " something really salient, which is the lengths of dependencies between words, which isn't so", "tokens": [50732, 746, 534, 1845, 1196, 11, 597, 307, 264, 26329, 295, 36606, 1296, 2283, 11, 597, 1943, 380, 370, 51044], "temperature": 0.0, "avg_logprob": -0.13418902291191948, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.0020183161832392216}, {"id": 525, "seek": 275992, "start": 2773.52, "end": 2777.6800000000003, "text": " obvious in the phrase structure. In the phrase structure, it's just kind of hard to see. It's", "tokens": [51044, 6322, 294, 264, 9535, 3877, 13, 682, 264, 9535, 3877, 11, 309, 311, 445, 733, 295, 1152, 281, 536, 13, 467, 311, 51252], "temperature": 0.0, "avg_logprob": -0.13418902291191948, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.0020183161832392216}, {"id": 526, "seek": 275992, "start": 2777.6800000000003, "end": 2784.0, "text": " in there. It's just very opaque. Technically, I think phrase structure grammar is mappable to", "tokens": [51252, 294, 456, 13, 467, 311, 445, 588, 42687, 13, 42494, 11, 286, 519, 9535, 3877, 22317, 307, 463, 427, 712, 281, 51568], "temperature": 0.0, "avg_logprob": -0.13418902291191948, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.0020183161832392216}, {"id": 527, "seek": 275992, "start": 2784.0, "end": 2788.8, "text": " dependency grammar. And vice versa. And vice versa. But there's like these little labels,", "tokens": [51568, 33621, 22317, 13, 400, 11964, 25650, 13, 400, 11964, 25650, 13, 583, 456, 311, 411, 613, 707, 16949, 11, 51808], "temperature": 0.0, "avg_logprob": -0.13418902291191948, "compression_ratio": 1.8548387096774193, "no_speech_prob": 0.0020183161832392216}, {"id": 528, "seek": 278880, "start": 2788.88, "end": 2794.0800000000004, "text": " S and PVP. Yeah. For a particular dependency grammar, you can make a phrase structure grammar", "tokens": [50368, 318, 293, 23035, 47, 13, 865, 13, 1171, 257, 1729, 33621, 22317, 11, 291, 393, 652, 257, 9535, 3877, 22317, 50628], "temperature": 0.0, "avg_logprob": -0.10687067702009871, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.001206501154229045}, {"id": 529, "seek": 278880, "start": 2794.0800000000004, "end": 2798.96, "text": " which generates exactly those same sentences and vice versa. But there are many phrase", "tokens": [50628, 597, 23815, 2293, 729, 912, 16579, 293, 11964, 25650, 13, 583, 456, 366, 867, 9535, 50872], "temperature": 0.0, "avg_logprob": -0.10687067702009871, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.001206501154229045}, {"id": 530, "seek": 278880, "start": 2798.96, "end": 2803.6800000000003, "text": " structure grammars which you can't really make a dependency grammar. I mean, you can do a lot", "tokens": [50872, 3877, 17570, 685, 597, 291, 393, 380, 534, 652, 257, 33621, 22317, 13, 286, 914, 11, 291, 393, 360, 257, 688, 51108], "temperature": 0.0, "avg_logprob": -0.10687067702009871, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.001206501154229045}, {"id": 531, "seek": 278880, "start": 2803.6800000000003, "end": 2808.88, "text": " more in a phrase structure grammar. You get many more of these extra nodes, basically. You can", "tokens": [51108, 544, 294, 257, 9535, 3877, 22317, 13, 509, 483, 867, 544, 295, 613, 2857, 13891, 11, 1936, 13, 509, 393, 51368], "temperature": 0.0, "avg_logprob": -0.10687067702009871, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.001206501154229045}, {"id": 532, "seek": 278880, "start": 2808.88, "end": 2814.8, "text": " have more structure in there. And some people like that. And maybe there's value to that. I don't", "tokens": [51368, 362, 544, 3877, 294, 456, 13, 400, 512, 561, 411, 300, 13, 400, 1310, 456, 311, 2158, 281, 300, 13, 286, 500, 380, 51664], "temperature": 0.0, "avg_logprob": -0.10687067702009871, "compression_ratio": 1.8385826771653544, "no_speech_prob": 0.001206501154229045}, {"id": 533, "seek": 281480, "start": 2814.8, "end": 2821.36, "text": " like it. Well, for you, we should clarify. So dependency grammar, it's just one word depends", "tokens": [50364, 411, 309, 13, 1042, 11, 337, 291, 11, 321, 820, 17594, 13, 407, 33621, 22317, 11, 309, 311, 445, 472, 1349, 5946, 50692], "temperature": 0.0, "avg_logprob": -0.1311631202697754, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.008707675151526928}, {"id": 534, "seek": 281480, "start": 2821.36, "end": 2828.2400000000002, "text": " on only one other word and you form these trees. And that makes, it really puts priority on those", "tokens": [50692, 322, 787, 472, 661, 1349, 293, 291, 1254, 613, 5852, 13, 400, 300, 1669, 11, 309, 534, 8137, 9365, 322, 729, 51036], "temperature": 0.0, "avg_logprob": -0.1311631202697754, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.008707675151526928}, {"id": 535, "seek": 281480, "start": 2828.2400000000002, "end": 2833.44, "text": " dependencies just like as a tree that you can then measure the distance of the dependency", "tokens": [51036, 36606, 445, 411, 382, 257, 4230, 300, 291, 393, 550, 3481, 264, 4560, 295, 264, 33621, 51296], "temperature": 0.0, "avg_logprob": -0.1311631202697754, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.008707675151526928}, {"id": 536, "seek": 281480, "start": 2833.44, "end": 2838.96, "text": " from one word to the other. They can then map to the cognitive processing of the", "tokens": [51296, 490, 472, 1349, 281, 264, 661, 13, 814, 393, 550, 4471, 281, 264, 15605, 9007, 295, 264, 51572], "temperature": 0.0, "avg_logprob": -0.1311631202697754, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.008707675151526928}, {"id": 537, "seek": 283896, "start": 2839.76, "end": 2844.0, "text": " of these sentences, how well how easy is to understand all that kind of stuff. So it just", "tokens": [50404, 295, 613, 16579, 11, 577, 731, 577, 1858, 307, 281, 1223, 439, 300, 733, 295, 1507, 13, 407, 309, 445, 50616], "temperature": 0.0, "avg_logprob": -0.12037057783997175, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.03408265858888626}, {"id": 538, "seek": 283896, "start": 2844.0, "end": 2852.4, "text": " puts the focus on just like the mathematical distance of dependence between words. So like", "tokens": [50616, 8137, 264, 1879, 322, 445, 411, 264, 18894, 4560, 295, 31704, 1296, 2283, 13, 407, 411, 51036], "temperature": 0.0, "avg_logprob": -0.12037057783997175, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.03408265858888626}, {"id": 539, "seek": 283896, "start": 2852.4, "end": 2857.12, "text": " it's just a different focus. Absolutely. Just continue on a thread of Chomsky because it's", "tokens": [51036, 309, 311, 445, 257, 819, 1879, 13, 7021, 13, 1449, 2354, 322, 257, 7207, 295, 761, 4785, 4133, 570, 309, 311, 51272], "temperature": 0.0, "avg_logprob": -0.12037057783997175, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.03408265858888626}, {"id": 540, "seek": 283896, "start": 2857.12, "end": 2862.8, "text": " really interesting because it as you're discussing disagreement, to the degree there's", "tokens": [51272, 534, 1880, 570, 309, 382, 291, 434, 10850, 38947, 11, 281, 264, 4314, 456, 311, 51556], "temperature": 0.0, "avg_logprob": -0.12037057783997175, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.03408265858888626}, {"id": 541, "seek": 283896, "start": 2862.8, "end": 2866.64, "text": " disagreement, you're also telling the history of the study of language, which is really awesome.", "tokens": [51556, 38947, 11, 291, 434, 611, 3585, 264, 2503, 295, 264, 2979, 295, 2856, 11, 597, 307, 534, 3476, 13, 51748], "temperature": 0.0, "avg_logprob": -0.12037057783997175, "compression_ratio": 1.7234848484848484, "no_speech_prob": 0.03408265858888626}, {"id": 542, "seek": 286664, "start": 2867.52, "end": 2873.7599999999998, "text": " So you mentioned context free versus regular. Does that distinction come into play for dependency", "tokens": [50408, 407, 291, 2835, 4319, 1737, 5717, 3890, 13, 4402, 300, 16844, 808, 666, 862, 337, 33621, 50720], "temperature": 0.0, "avg_logprob": -0.1422109818190671, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.0008557831752113998}, {"id": 543, "seek": 286664, "start": 2873.7599999999998, "end": 2880.4, "text": " grammars? No, not at all. I mean, regular languages are too simple for human languages.", "tokens": [50720, 17570, 685, 30, 883, 11, 406, 412, 439, 13, 286, 914, 11, 3890, 8650, 366, 886, 2199, 337, 1952, 8650, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1422109818190671, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.0008557831752113998}, {"id": 544, "seek": 286664, "start": 2882.24, "end": 2888.16, "text": " It's a part of the hierarchy, but human languages are in the phrase structure world are definitely,", "tokens": [51144, 467, 311, 257, 644, 295, 264, 22333, 11, 457, 1952, 8650, 366, 294, 264, 9535, 3877, 1002, 366, 2138, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1422109818190671, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.0008557831752113998}, {"id": 545, "seek": 286664, "start": 2889.92, "end": 2895.6, "text": " at least context free, maybe a little bit more, a little bit harder than that. So there's something", "tokens": [51528, 412, 1935, 4319, 1737, 11, 1310, 257, 707, 857, 544, 11, 257, 707, 857, 6081, 813, 300, 13, 407, 456, 311, 746, 51812], "temperature": 0.0, "avg_logprob": -0.1422109818190671, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.0008557831752113998}, {"id": 546, "seek": 289560, "start": 2895.6, "end": 2900.96, "text": " called context sensitive as well, where you can have like this is the just the formal language", "tokens": [50364, 1219, 4319, 9477, 382, 731, 11, 689, 291, 393, 362, 411, 341, 307, 264, 445, 264, 9860, 2856, 50632], "temperature": 0.0, "avg_logprob": -0.12854053663170856, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.0071189808659255505}, {"id": 547, "seek": 289560, "start": 2900.96, "end": 2907.92, "text": " description. In a context free grammar, you have one, this is like a bunch of like formal language", "tokens": [50632, 3855, 13, 682, 257, 4319, 1737, 22317, 11, 291, 362, 472, 11, 341, 307, 411, 257, 3840, 295, 411, 9860, 2856, 50980], "temperature": 0.0, "avg_logprob": -0.12854053663170856, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.0071189808659255505}, {"id": 548, "seek": 289560, "start": 2907.92, "end": 2912.4, "text": " theory we're doing here. I love it. Okay. So you have you have a left hand side category and", "tokens": [50980, 5261, 321, 434, 884, 510, 13, 286, 959, 309, 13, 1033, 13, 407, 291, 362, 291, 362, 257, 1411, 1011, 1252, 7719, 293, 51204], "temperature": 0.0, "avg_logprob": -0.12854053663170856, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.0071189808659255505}, {"id": 549, "seek": 289560, "start": 2912.4, "end": 2917.2799999999997, "text": " you're expanding to anything on the right is a, that's a context free. So like the idea is that", "tokens": [51204, 291, 434, 14702, 281, 1340, 322, 264, 558, 307, 257, 11, 300, 311, 257, 4319, 1737, 13, 407, 411, 264, 1558, 307, 300, 51448], "temperature": 0.0, "avg_logprob": -0.12854053663170856, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.0071189808659255505}, {"id": 550, "seek": 289560, "start": 2917.2799999999997, "end": 2921.92, "text": " that category on the left expands in independent of context to those things, whatever they're on", "tokens": [51448, 300, 7719, 322, 264, 1411, 33706, 294, 6695, 295, 4319, 281, 729, 721, 11, 2035, 436, 434, 322, 51680], "temperature": 0.0, "avg_logprob": -0.12854053663170856, "compression_ratio": 1.8423076923076922, "no_speech_prob": 0.0071189808659255505}, {"id": 551, "seek": 292192, "start": 2921.92, "end": 2929.04, "text": " the right. It doesn't matter what. And a context sensitive says, okay, I actually have more than", "tokens": [50364, 264, 558, 13, 467, 1177, 380, 1871, 437, 13, 400, 257, 4319, 9477, 1619, 11, 1392, 11, 286, 767, 362, 544, 813, 50720], "temperature": 0.0, "avg_logprob": -0.08853886922200521, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.005219293292611837}, {"id": 552, "seek": 292192, "start": 2929.04, "end": 2932.96, "text": " one thing on the left. I can tell you only in this context, you know, I have maybe you have", "tokens": [50720, 472, 551, 322, 264, 1411, 13, 286, 393, 980, 291, 787, 294, 341, 4319, 11, 291, 458, 11, 286, 362, 1310, 291, 362, 50916], "temperature": 0.0, "avg_logprob": -0.08853886922200521, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.005219293292611837}, {"id": 553, "seek": 292192, "start": 2932.96, "end": 2937.04, "text": " like a left and a right context or just a left context or a right context, I have two or more", "tokens": [50916, 411, 257, 1411, 293, 257, 558, 4319, 420, 445, 257, 1411, 4319, 420, 257, 558, 4319, 11, 286, 362, 732, 420, 544, 51120], "temperature": 0.0, "avg_logprob": -0.08853886922200521, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.005219293292611837}, {"id": 554, "seek": 292192, "start": 2937.04, "end": 2941.92, "text": " stuff on the left tells you how to expand that those things in that way. Okay. So it's context", "tokens": [51120, 1507, 322, 264, 1411, 5112, 291, 577, 281, 5268, 300, 729, 721, 294, 300, 636, 13, 1033, 13, 407, 309, 311, 4319, 51364], "temperature": 0.0, "avg_logprob": -0.08853886922200521, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.005219293292611837}, {"id": 555, "seek": 292192, "start": 2941.92, "end": 2948.48, "text": " sensitive. A regular language is just more constrained. And so it, it doesn't allow anything", "tokens": [51364, 9477, 13, 316, 3890, 2856, 307, 445, 544, 38901, 13, 400, 370, 309, 11, 309, 1177, 380, 2089, 1340, 51692], "temperature": 0.0, "avg_logprob": -0.08853886922200521, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.005219293292611837}, {"id": 556, "seek": 294848, "start": 2948.48, "end": 2954.72, "text": " on the right. It allows very, it allows basically it's a one very complicated rule is kind of what", "tokens": [50364, 322, 264, 558, 13, 467, 4045, 588, 11, 309, 4045, 1936, 309, 311, 257, 472, 588, 6179, 4978, 307, 733, 295, 437, 50676], "temperature": 0.0, "avg_logprob": -0.18369899361820544, "compression_ratio": 1.9625, "no_speech_prob": 0.024785416200757027}, {"id": 557, "seek": 294848, "start": 2955.28, "end": 2961.52, "text": " a regular language is. And so it doesn't have any, let's just say long distance dependencies,", "tokens": [50704, 257, 3890, 2856, 307, 13, 400, 370, 309, 1177, 380, 362, 604, 11, 718, 311, 445, 584, 938, 4560, 36606, 11, 51016], "temperature": 0.0, "avg_logprob": -0.18369899361820544, "compression_ratio": 1.9625, "no_speech_prob": 0.024785416200757027}, {"id": 558, "seek": 294848, "start": 2961.52, "end": 2966.4, "text": " it doesn't allow recursion, for instance, there's no recursion. Yeah, recursion is where you,", "tokens": [51016, 309, 1177, 380, 2089, 20560, 313, 11, 337, 5197, 11, 456, 311, 572, 20560, 313, 13, 865, 11, 20560, 313, 307, 689, 291, 11, 51260], "temperature": 0.0, "avg_logprob": -0.18369899361820544, "compression_ratio": 1.9625, "no_speech_prob": 0.024785416200757027}, {"id": 559, "seek": 294848, "start": 2966.4, "end": 2970.56, "text": " which is human languages have recursion, they have embedding. And you can't, well, it doesn't", "tokens": [51260, 597, 307, 1952, 8650, 362, 20560, 313, 11, 436, 362, 12240, 3584, 13, 400, 291, 393, 380, 11, 731, 11, 309, 1177, 380, 51468], "temperature": 0.0, "avg_logprob": -0.18369899361820544, "compression_ratio": 1.9625, "no_speech_prob": 0.024785416200757027}, {"id": 560, "seek": 294848, "start": 2970.56, "end": 2975.04, "text": " allow center embedded recursion, which human languages have, which is what center embedded", "tokens": [51468, 2089, 3056, 16741, 20560, 313, 11, 597, 1952, 8650, 362, 11, 597, 307, 437, 3056, 16741, 51692], "temperature": 0.0, "avg_logprob": -0.18369899361820544, "compression_ratio": 1.9625, "no_speech_prob": 0.024785416200757027}, {"id": 561, "seek": 297504, "start": 2975.12, "end": 2978.4, "text": " recursion, within a sentence, within a sentence. Yeah, within a sentence. So here we're going to", "tokens": [50368, 20560, 313, 11, 1951, 257, 8174, 11, 1951, 257, 8174, 13, 865, 11, 1951, 257, 8174, 13, 407, 510, 321, 434, 516, 281, 50532], "temperature": 0.0, "avg_logprob": -0.14265454695528787, "compression_ratio": 1.9657534246575343, "no_speech_prob": 0.006486670579761267}, {"id": 562, "seek": 297504, "start": 2978.4, "end": 2982.96, "text": " get to that. But I, you know, the formal language stuff is a little aside, Chomsky wasn't proposing", "tokens": [50532, 483, 281, 300, 13, 583, 286, 11, 291, 458, 11, 264, 9860, 2856, 1507, 307, 257, 707, 7359, 11, 761, 4785, 4133, 2067, 380, 29939, 50760], "temperature": 0.0, "avg_logprob": -0.14265454695528787, "compression_ratio": 1.9657534246575343, "no_speech_prob": 0.006486670579761267}, {"id": 563, "seek": 297504, "start": 2982.96, "end": 2988.0, "text": " it for human languages, even he was just pointing out that human languages are context free. And", "tokens": [50760, 309, 337, 1952, 8650, 11, 754, 415, 390, 445, 12166, 484, 300, 1952, 8650, 366, 4319, 1737, 13, 400, 51012], "temperature": 0.0, "avg_logprob": -0.14265454695528787, "compression_ratio": 1.9657534246575343, "no_speech_prob": 0.006486670579761267}, {"id": 564, "seek": 297504, "start": 2988.0, "end": 2992.08, "text": " then he was most in for, for human, because that was kind of stuff we did for formal languages.", "tokens": [51012, 550, 415, 390, 881, 294, 337, 11, 337, 1952, 11, 570, 300, 390, 733, 295, 1507, 321, 630, 337, 9860, 8650, 13, 51216], "temperature": 0.0, "avg_logprob": -0.14265454695528787, "compression_ratio": 1.9657534246575343, "no_speech_prob": 0.006486670579761267}, {"id": 565, "seek": 297504, "start": 2992.08, "end": 2997.2799999999997, "text": " And what he was most interested in was human language. And that's like the, the movement is", "tokens": [51216, 400, 437, 415, 390, 881, 3102, 294, 390, 1952, 2856, 13, 400, 300, 311, 411, 264, 11, 264, 3963, 307, 51476], "temperature": 0.0, "avg_logprob": -0.14265454695528787, "compression_ratio": 1.9657534246575343, "no_speech_prob": 0.006486670579761267}, {"id": 566, "seek": 297504, "start": 2997.2799999999997, "end": 3003.2, "text": " where we, we, we, where, where he sort of set off in on the, I would say a very interesting,", "tokens": [51476, 689, 321, 11, 321, 11, 321, 11, 689, 11, 689, 415, 1333, 295, 992, 766, 294, 322, 264, 11, 286, 576, 584, 257, 588, 1880, 11, 51772], "temperature": 0.0, "avg_logprob": -0.14265454695528787, "compression_ratio": 1.9657534246575343, "no_speech_prob": 0.006486670579761267}, {"id": 567, "seek": 300320, "start": 3003.8399999999997, "end": 3006.7999999999997, "text": " but wrong foot. It was kind of interesting. It's a very, I agree. It's kind of,", "tokens": [50396, 457, 2085, 2671, 13, 467, 390, 733, 295, 1880, 13, 467, 311, 257, 588, 11, 286, 3986, 13, 467, 311, 733, 295, 11, 50544], "temperature": 0.0, "avg_logprob": -0.1320180823333072, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.010983604937791824}, {"id": 568, "seek": 300320, "start": 3006.7999999999997, "end": 3011.8399999999997, "text": " it's very interesting history. So there's a set, he proposed this multiple theories in 57,", "tokens": [50544, 309, 311, 588, 1880, 2503, 13, 407, 456, 311, 257, 992, 11, 415, 10348, 341, 3866, 13667, 294, 21423, 11, 50796], "temperature": 0.0, "avg_logprob": -0.1320180823333072, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.010983604937791824}, {"id": 569, "seek": 300320, "start": 3011.8399999999997, "end": 3015.4399999999996, "text": " and then 65, they're, they all have this framework, though, was phrase structure,", "tokens": [50796, 293, 550, 11624, 11, 436, 434, 11, 436, 439, 362, 341, 8388, 11, 1673, 11, 390, 9535, 3877, 11, 50976], "temperature": 0.0, "avg_logprob": -0.1320180823333072, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.010983604937791824}, {"id": 570, "seek": 300320, "start": 3015.4399999999996, "end": 3019.52, "text": " plus movement, different versions of the, of the phrase structure and the movement in the 57.", "tokens": [50976, 1804, 3963, 11, 819, 9606, 295, 264, 11, 295, 264, 9535, 3877, 293, 264, 3963, 294, 264, 21423, 13, 51180], "temperature": 0.0, "avg_logprob": -0.1320180823333072, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.010983604937791824}, {"id": 571, "seek": 300320, "start": 3019.52, "end": 3024.24, "text": " This is the most famous original bits of Chomsky's work. And then 71 is when he figured out that", "tokens": [51180, 639, 307, 264, 881, 4618, 3380, 9239, 295, 761, 4785, 4133, 311, 589, 13, 400, 550, 30942, 307, 562, 415, 8932, 484, 300, 51416], "temperature": 0.0, "avg_logprob": -0.1320180823333072, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.010983604937791824}, {"id": 572, "seek": 300320, "start": 3024.24, "end": 3030.24, "text": " those lead to learning problems, that, that there's cases where a kid could never figure out", "tokens": [51416, 729, 1477, 281, 2539, 2740, 11, 300, 11, 300, 456, 311, 3331, 689, 257, 1636, 727, 1128, 2573, 484, 51716], "temperature": 0.0, "avg_logprob": -0.1320180823333072, "compression_ratio": 1.7516339869281046, "no_speech_prob": 0.010983604937791824}, {"id": 573, "seek": 303024, "start": 3030.8799999999997, "end": 3036.72, "text": " which rule, which set of rules was intended. And, and so, and then he said, well, that means it's", "tokens": [50396, 597, 4978, 11, 597, 992, 295, 4474, 390, 10226, 13, 400, 11, 293, 370, 11, 293, 550, 415, 848, 11, 731, 11, 300, 1355, 309, 311, 50688], "temperature": 0.0, "avg_logprob": -0.08849121352373543, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.003074677661061287}, {"id": 574, "seek": 303024, "start": 3036.72, "end": 3041.52, "text": " innate. It's kind of interesting. He just really thought the movement was just so obviously true", "tokens": [50688, 41766, 13, 467, 311, 733, 295, 1880, 13, 634, 445, 534, 1194, 264, 3963, 390, 445, 370, 2745, 2074, 50928], "temperature": 0.0, "avg_logprob": -0.08849121352373543, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.003074677661061287}, {"id": 575, "seek": 303024, "start": 3041.52, "end": 3046.9599999999996, "text": " that he couldn't, he didn't even entertain giving it up. It's just obvious that that's obviously", "tokens": [50928, 300, 415, 2809, 380, 11, 415, 994, 380, 754, 7655, 2902, 309, 493, 13, 467, 311, 445, 6322, 300, 300, 311, 2745, 51200], "temperature": 0.0, "avg_logprob": -0.08849121352373543, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.003074677661061287}, {"id": 576, "seek": 303024, "start": 3046.9599999999996, "end": 3053.4399999999996, "text": " right. And it was later where people figured out that there's all these like subtle ways in which", "tokens": [51200, 558, 13, 400, 309, 390, 1780, 689, 561, 8932, 484, 300, 456, 311, 439, 613, 411, 13743, 2098, 294, 597, 51524], "temperature": 0.0, "avg_logprob": -0.08849121352373543, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.003074677661061287}, {"id": 577, "seek": 303024, "start": 3053.4399999999996, "end": 3057.12, "text": " things would, which look like generalizations aren't generalizations. And they, you know,", "tokens": [51524, 721, 576, 11, 597, 574, 411, 2674, 14455, 3212, 380, 2674, 14455, 13, 400, 436, 11, 291, 458, 11, 51708], "temperature": 0.0, "avg_logprob": -0.08849121352373543, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.003074677661061287}, {"id": 578, "seek": 305712, "start": 3057.12, "end": 3061.52, "text": " across the category, they're, they're word specific and they have, and they, they kind of", "tokens": [50364, 2108, 264, 7719, 11, 436, 434, 11, 436, 434, 1349, 2685, 293, 436, 362, 11, 293, 436, 11, 436, 733, 295, 50584], "temperature": 0.0, "avg_logprob": -0.09408728496448414, "compression_ratio": 1.8980263157894737, "no_speech_prob": 0.015420311130583286}, {"id": 579, "seek": 305712, "start": 3061.52, "end": 3065.2799999999997, "text": " work, but they don't work across various other words in the category. And so it's easier to just", "tokens": [50584, 589, 11, 457, 436, 500, 380, 589, 2108, 3683, 661, 2283, 294, 264, 7719, 13, 400, 370, 309, 311, 3571, 281, 445, 50772], "temperature": 0.0, "avg_logprob": -0.09408728496448414, "compression_ratio": 1.8980263157894737, "no_speech_prob": 0.015420311130583286}, {"id": 580, "seek": 305712, "start": 3065.2799999999997, "end": 3070.56, "text": " think of these things as lexical copies. And, and I think he was very obsessed. I don't know. I'm", "tokens": [50772, 519, 295, 613, 721, 382, 476, 87, 804, 14341, 13, 400, 11, 293, 286, 519, 415, 390, 588, 16923, 13, 286, 500, 380, 458, 13, 286, 478, 51036], "temperature": 0.0, "avg_logprob": -0.09408728496448414, "compression_ratio": 1.8980263157894737, "no_speech_prob": 0.015420311130583286}, {"id": 581, "seek": 305712, "start": 3070.56, "end": 3076.24, "text": " just guessing that he just, he really wanted this story to be simple in some sense. And language", "tokens": [51036, 445, 17939, 300, 415, 445, 11, 415, 534, 1415, 341, 1657, 281, 312, 2199, 294, 512, 2020, 13, 400, 2856, 51320], "temperature": 0.0, "avg_logprob": -0.09408728496448414, "compression_ratio": 1.8980263157894737, "no_speech_prob": 0.015420311130583286}, {"id": 582, "seek": 305712, "start": 3076.24, "end": 3081.44, "text": " is a little more complicated in some sense, you know, he didn't like words. He never talks about", "tokens": [51320, 307, 257, 707, 544, 6179, 294, 512, 2020, 11, 291, 458, 11, 415, 994, 380, 411, 2283, 13, 634, 1128, 6686, 466, 51580], "temperature": 0.0, "avg_logprob": -0.09408728496448414, "compression_ratio": 1.8980263157894737, "no_speech_prob": 0.015420311130583286}, {"id": 583, "seek": 305712, "start": 3081.44, "end": 3085.92, "text": " words. He likes to talk about combinations of words and words are, you know, look up a dictionary,", "tokens": [51580, 2283, 13, 634, 5902, 281, 751, 466, 21267, 295, 2283, 293, 2283, 366, 11, 291, 458, 11, 574, 493, 257, 25890, 11, 51804], "temperature": 0.0, "avg_logprob": -0.09408728496448414, "compression_ratio": 1.8980263157894737, "no_speech_prob": 0.015420311130583286}, {"id": 584, "seek": 308592, "start": 3085.92, "end": 3092.08, "text": " there's 50 senses for a common word, right? The word take will have 30 or 40 senses in it. So,", "tokens": [50364, 456, 311, 2625, 17057, 337, 257, 2689, 1349, 11, 558, 30, 440, 1349, 747, 486, 362, 2217, 420, 3356, 17057, 294, 309, 13, 407, 11, 50672], "temperature": 0.0, "avg_logprob": -0.10749329566955566, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.006486245431005955}, {"id": 585, "seek": 308592, "start": 3092.64, "end": 3097.76, "text": " there'll be many different senses for common words. And he just doesn't think about that. It's, or", "tokens": [50700, 456, 603, 312, 867, 819, 17057, 337, 2689, 2283, 13, 400, 415, 445, 1177, 380, 519, 466, 300, 13, 467, 311, 11, 420, 50956], "temperature": 0.0, "avg_logprob": -0.10749329566955566, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.006486245431005955}, {"id": 586, "seek": 308592, "start": 3097.76, "end": 3102.4, "text": " he doesn't think that's language. I think he doesn't think that's language. He thinks that", "tokens": [50956, 415, 1177, 380, 519, 300, 311, 2856, 13, 286, 519, 415, 1177, 380, 519, 300, 311, 2856, 13, 634, 7309, 300, 51188], "temperature": 0.0, "avg_logprob": -0.10749329566955566, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.006486245431005955}, {"id": 587, "seek": 308592, "start": 3102.4, "end": 3109.52, "text": " words are distinct from combinations of words. I think they're the same. If you look at my brain", "tokens": [51188, 2283, 366, 10644, 490, 21267, 295, 2283, 13, 286, 519, 436, 434, 264, 912, 13, 759, 291, 574, 412, 452, 3567, 51544], "temperature": 0.0, "avg_logprob": -0.10749329566955566, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.006486245431005955}, {"id": 588, "seek": 310952, "start": 3109.68, "end": 3116.4, "text": " in the scanner, while I'm listening to a language I understand, and you compare, I can localize my", "tokens": [50372, 294, 264, 30211, 11, 1339, 286, 478, 4764, 281, 257, 2856, 286, 1223, 11, 293, 291, 6794, 11, 286, 393, 2654, 1125, 452, 50708], "temperature": 0.0, "avg_logprob": -0.11228849005511427, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.27486926317214966}, {"id": 589, "seek": 310952, "start": 3116.4, "end": 3120.56, "text": " language network in a few minutes, in like 15 minutes. And what you do is I listen to a language", "tokens": [50708, 2856, 3209, 294, 257, 1326, 2077, 11, 294, 411, 2119, 2077, 13, 400, 437, 291, 360, 307, 286, 2140, 281, 257, 2856, 50916], "temperature": 0.0, "avg_logprob": -0.11228849005511427, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.27486926317214966}, {"id": 590, "seek": 310952, "start": 3120.56, "end": 3126.4, "text": " I know, I listen to, you know, maybe some language I don't know, or I listen to muffled speech, or I", "tokens": [50916, 286, 458, 11, 286, 2140, 281, 11, 291, 458, 11, 1310, 512, 2856, 286, 500, 380, 458, 11, 420, 286, 2140, 281, 22635, 1493, 6218, 11, 420, 286, 51208], "temperature": 0.0, "avg_logprob": -0.11228849005511427, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.27486926317214966}, {"id": 591, "seek": 310952, "start": 3126.4, "end": 3130.96, "text": " read sentences, or I read non words, like I do anything like this, anything that sort of really", "tokens": [51208, 1401, 16579, 11, 420, 286, 1401, 2107, 2283, 11, 411, 286, 360, 1340, 411, 341, 11, 1340, 300, 1333, 295, 534, 51436], "temperature": 0.0, "avg_logprob": -0.11228849005511427, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.27486926317214966}, {"id": 592, "seek": 310952, "start": 3130.96, "end": 3135.04, "text": " like English and anything that's not very like English. So I've got something like it and not,", "tokens": [51436, 411, 3669, 293, 1340, 300, 311, 406, 588, 411, 3669, 13, 407, 286, 600, 658, 746, 411, 309, 293, 406, 11, 51640], "temperature": 0.0, "avg_logprob": -0.11228849005511427, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.27486926317214966}, {"id": 593, "seek": 313504, "start": 3135.04, "end": 3141.84, "text": " and I got to control. And the voxels, which is just, you know, the 3D pixels in my brain", "tokens": [50364, 293, 286, 658, 281, 1969, 13, 400, 264, 1650, 87, 1625, 11, 597, 307, 445, 11, 291, 458, 11, 264, 805, 35, 18668, 294, 452, 3567, 50704], "temperature": 0.0, "avg_logprob": -0.1545925234804059, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.012050241231918335}, {"id": 594, "seek": 313504, "start": 3141.84, "end": 3150.48, "text": " that are responding most is a language area. And that's this left lateralized area in my head. And", "tokens": [50704, 300, 366, 16670, 881, 307, 257, 2856, 1859, 13, 400, 300, 311, 341, 1411, 25128, 1602, 1859, 294, 452, 1378, 13, 400, 51136], "temperature": 0.0, "avg_logprob": -0.1545925234804059, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.012050241231918335}, {"id": 595, "seek": 313504, "start": 3151.36, "end": 3156.24, "text": " wherever I look in that network, if you look for the combinations versus the words, it's", "tokens": [51180, 8660, 286, 574, 294, 300, 3209, 11, 498, 291, 574, 337, 264, 21267, 5717, 264, 2283, 11, 309, 311, 51424], "temperature": 0.0, "avg_logprob": -0.1545925234804059, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.012050241231918335}, {"id": 596, "seek": 313504, "start": 3157.2799999999997, "end": 3162.32, "text": " everywhere. It's the same. That's fascinating. And so it's like hard to find, there are no areas", "tokens": [51476, 5315, 13, 467, 311, 264, 912, 13, 663, 311, 10343, 13, 400, 370, 309, 311, 411, 1152, 281, 915, 11, 456, 366, 572, 3179, 51728], "temperature": 0.0, "avg_logprob": -0.1545925234804059, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.012050241231918335}, {"id": 597, "seek": 316232, "start": 3162.32, "end": 3168.88, "text": " that we know. I mean, that's a little overstated right now. At this point, the technology isn't", "tokens": [50364, 300, 321, 458, 13, 286, 914, 11, 300, 311, 257, 707, 48834, 770, 558, 586, 13, 1711, 341, 935, 11, 264, 2899, 1943, 380, 50692], "temperature": 0.0, "avg_logprob": -0.11181304125281853, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.0023965013679116964}, {"id": 598, "seek": 316232, "start": 3168.88, "end": 3174.1600000000003, "text": " great. It's not bad. But we have the best way to figure out what's going on in my brain when I'm", "tokens": [50692, 869, 13, 467, 311, 406, 1578, 13, 583, 321, 362, 264, 1151, 636, 281, 2573, 484, 437, 311, 516, 322, 294, 452, 3567, 562, 286, 478, 50956], "temperature": 0.0, "avg_logprob": -0.11181304125281853, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.0023965013679116964}, {"id": 599, "seek": 316232, "start": 3174.1600000000003, "end": 3179.04, "text": " listening or reading language is to use fMRI, Functional Magnetic Resonance Imaging. And", "tokens": [50956, 4764, 420, 3760, 2856, 307, 281, 764, 283, 44, 5577, 11, 11166, 41048, 19664, 3532, 5015, 266, 719, 4331, 3568, 13, 400, 51200], "temperature": 0.0, "avg_logprob": -0.11181304125281853, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.0023965013679116964}, {"id": 600, "seek": 316232, "start": 3179.04, "end": 3184.48, "text": " that's a very good localization method. So I can figure out where exactly these signals are coming", "tokens": [51200, 300, 311, 257, 588, 665, 2654, 2144, 3170, 13, 407, 286, 393, 2573, 484, 689, 2293, 613, 12354, 366, 1348, 51472], "temperature": 0.0, "avg_logprob": -0.11181304125281853, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.0023965013679116964}, {"id": 601, "seek": 316232, "start": 3184.48, "end": 3189.04, "text": " from pretty, you know, down to, you know, millimeters, cubic millimeters are smaller,", "tokens": [51472, 490, 1238, 11, 291, 458, 11, 760, 281, 11, 291, 458, 11, 24388, 11, 28733, 24388, 366, 4356, 11, 51700], "temperature": 0.0, "avg_logprob": -0.11181304125281853, "compression_ratio": 1.6350877192982456, "no_speech_prob": 0.0023965013679116964}, {"id": 602, "seek": 318904, "start": 3189.04, "end": 3194.24, "text": " okay? Very small. We can figure those out very well. The problem is the when, okay? It's measuring", "tokens": [50364, 1392, 30, 4372, 1359, 13, 492, 393, 2573, 729, 484, 588, 731, 13, 440, 1154, 307, 264, 562, 11, 1392, 30, 467, 311, 13389, 50624], "temperature": 0.0, "avg_logprob": -0.10714778253587626, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.005059457384049892}, {"id": 603, "seek": 318904, "start": 3195.2, "end": 3200.32, "text": " oxygen, okay? And oxygen takes a little while to get to those cells. And so it takes on the", "tokens": [50672, 9169, 11, 1392, 30, 400, 9169, 2516, 257, 707, 1339, 281, 483, 281, 729, 5438, 13, 400, 370, 309, 2516, 322, 264, 50928], "temperature": 0.0, "avg_logprob": -0.10714778253587626, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.005059457384049892}, {"id": 604, "seek": 318904, "start": 3200.32, "end": 3205.12, "text": " order of seconds. So I talk fast. I probably listened fast and I can probably understand", "tokens": [50928, 1668, 295, 3949, 13, 407, 286, 751, 2370, 13, 286, 1391, 13207, 2370, 293, 286, 393, 1391, 1223, 51168], "temperature": 0.0, "avg_logprob": -0.10714778253587626, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.005059457384049892}, {"id": 605, "seek": 318904, "start": 3205.12, "end": 3210.16, "text": " things really fast. So a lot of stuff happens in two seconds. And so to say that we know what's", "tokens": [51168, 721, 534, 2370, 13, 407, 257, 688, 295, 1507, 2314, 294, 732, 3949, 13, 400, 370, 281, 584, 300, 321, 458, 437, 311, 51420], "temperature": 0.0, "avg_logprob": -0.10714778253587626, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.005059457384049892}, {"id": 606, "seek": 318904, "start": 3210.16, "end": 3216.64, "text": " going on, that the words right now in that network, our best guess is that whole network", "tokens": [51420, 516, 322, 11, 300, 264, 2283, 558, 586, 294, 300, 3209, 11, 527, 1151, 2041, 307, 300, 1379, 3209, 51744], "temperature": 0.0, "avg_logprob": -0.10714778253587626, "compression_ratio": 1.7185185185185186, "no_speech_prob": 0.005059457384049892}, {"id": 607, "seek": 321664, "start": 3216.64, "end": 3222.08, "text": " is doing something similar, but maybe different parts of that network are doing different things.", "tokens": [50364, 307, 884, 746, 2531, 11, 457, 1310, 819, 3166, 295, 300, 3209, 366, 884, 819, 721, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1068272000267392, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0020503299310803413}, {"id": 608, "seek": 321664, "start": 3222.08, "end": 3227.04, "text": " And that's probably the case. We just don't have very good methods to figure that out right at", "tokens": [50636, 400, 300, 311, 1391, 264, 1389, 13, 492, 445, 500, 380, 362, 588, 665, 7150, 281, 2573, 300, 484, 558, 412, 50884], "temperature": 0.0, "avg_logprob": -0.1068272000267392, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0020503299310803413}, {"id": 609, "seek": 321664, "start": 3227.04, "end": 3233.04, "text": " this moment. And so since we're kind of talking about the history of the study of language,", "tokens": [50884, 341, 1623, 13, 400, 370, 1670, 321, 434, 733, 295, 1417, 466, 264, 2503, 295, 264, 2979, 295, 2856, 11, 51184], "temperature": 0.0, "avg_logprob": -0.1068272000267392, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0020503299310803413}, {"id": 610, "seek": 321664, "start": 3233.92, "end": 3238.7999999999997, "text": " what other interesting disagreements, and you're both at MIT or were for a long time,", "tokens": [51228, 437, 661, 1880, 23926, 6400, 11, 293, 291, 434, 1293, 412, 13100, 420, 645, 337, 257, 938, 565, 11, 51472], "temperature": 0.0, "avg_logprob": -0.1068272000267392, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0020503299310803413}, {"id": 611, "seek": 321664, "start": 3238.7999999999997, "end": 3243.2, "text": " what kind of interesting disagreements there, attention of ideas are there between you and", "tokens": [51472, 437, 733, 295, 1880, 23926, 6400, 456, 11, 3202, 295, 3487, 366, 456, 1296, 291, 293, 51692], "temperature": 0.0, "avg_logprob": -0.1068272000267392, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0020503299310803413}, {"id": 612, "seek": 324320, "start": 3243.2, "end": 3248.3999999999996, "text": " Noam Chomsky. And we should say that Noam was in the linguistics department. And you're,", "tokens": [50364, 883, 335, 761, 4785, 4133, 13, 400, 321, 820, 584, 300, 883, 335, 390, 294, 264, 21766, 6006, 5882, 13, 400, 291, 434, 11, 50624], "temperature": 0.0, "avg_logprob": -0.19273355272081164, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.015894601121544838}, {"id": 613, "seek": 324320, "start": 3249.8399999999997, "end": 3255.2, "text": " I guess, for a time were affiliated there, but primarily brain and cognitive science department,", "tokens": [50696, 286, 2041, 11, 337, 257, 565, 645, 42174, 456, 11, 457, 10029, 3567, 293, 15605, 3497, 5882, 11, 50964], "temperature": 0.0, "avg_logprob": -0.19273355272081164, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.015894601121544838}, {"id": 614, "seek": 324320, "start": 3255.7599999999998, "end": 3260.3199999999997, "text": " which is another way of studying language. And you've been talking about fMRI. So like what,", "tokens": [50992, 597, 307, 1071, 636, 295, 7601, 2856, 13, 400, 291, 600, 668, 1417, 466, 283, 44, 5577, 13, 407, 411, 437, 11, 51220], "temperature": 0.0, "avg_logprob": -0.19273355272081164, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.015894601121544838}, {"id": 615, "seek": 324320, "start": 3260.96, "end": 3266.24, "text": " is there something else interesting to bring to the surface about the disagreement between the two", "tokens": [51252, 307, 456, 746, 1646, 1880, 281, 1565, 281, 264, 3753, 466, 264, 38947, 1296, 264, 732, 51516], "temperature": 0.0, "avg_logprob": -0.19273355272081164, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.015894601121544838}, {"id": 616, "seek": 326624, "start": 3266.24, "end": 3273.68, "text": " of you or other people in the industry? Yeah, I mean, I've been at MIT for 31 years since 1993,", "tokens": [50364, 295, 291, 420, 661, 561, 294, 264, 3518, 30, 865, 11, 286, 914, 11, 286, 600, 668, 412, 13100, 337, 10353, 924, 1670, 25137, 11, 50736], "temperature": 0.0, "avg_logprob": -0.13329905383991744, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.17993897199630737}, {"id": 617, "seek": 326624, "start": 3273.68, "end": 3280.8799999999997, "text": " and Chomsky's been there much longer. So I met him, I knew him, I met when I first got there,", "tokens": [50736, 293, 761, 4785, 4133, 311, 668, 456, 709, 2854, 13, 407, 286, 1131, 796, 11, 286, 2586, 796, 11, 286, 1131, 562, 286, 700, 658, 456, 11, 51096], "temperature": 0.0, "avg_logprob": -0.13329905383991744, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.17993897199630737}, {"id": 618, "seek": 326624, "start": 3280.8799999999997, "end": 3287.7599999999998, "text": " I guess, and we would interact every now and then. I'd say that, so I'd say our biggest difference", "tokens": [51096, 286, 2041, 11, 293, 321, 576, 4648, 633, 586, 293, 550, 13, 286, 1116, 584, 300, 11, 370, 286, 1116, 584, 527, 3880, 2649, 51440], "temperature": 0.0, "avg_logprob": -0.13329905383991744, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.17993897199630737}, {"id": 619, "seek": 326624, "start": 3287.7599999999998, "end": 3294.56, "text": " is our methods. And so that's the biggest difference between me and Noam, is that I", "tokens": [51440, 307, 527, 7150, 13, 400, 370, 300, 311, 264, 3880, 2649, 1296, 385, 293, 883, 335, 11, 307, 300, 286, 51780], "temperature": 0.0, "avg_logprob": -0.13329905383991744, "compression_ratio": 1.5630252100840336, "no_speech_prob": 0.17993897199630737}, {"id": 620, "seek": 329456, "start": 3295.2799999999997, "end": 3301.44, "text": " gather data from people. I do experiments with people and I gather corpus data, whatever,", "tokens": [50400, 5448, 1412, 490, 561, 13, 286, 360, 12050, 365, 561, 293, 286, 5448, 1181, 31624, 1412, 11, 2035, 11, 50708], "temperature": 0.0, "avg_logprob": -0.1035952126538312, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.003648025682196021}, {"id": 621, "seek": 329456, "start": 3301.44, "end": 3305.68, "text": " whatever corpus data is available, and we do quantitative methods to evaluate", "tokens": [50708, 2035, 1181, 31624, 1412, 307, 2435, 11, 293, 321, 360, 27778, 7150, 281, 13059, 50920], "temperature": 0.0, "avg_logprob": -0.1035952126538312, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.003648025682196021}, {"id": 622, "seek": 329456, "start": 3306.48, "end": 3313.52, "text": " any kind of hypothesis we have. He just doesn't do that. And so, you know, he has never once been", "tokens": [50960, 604, 733, 295, 17291, 321, 362, 13, 634, 445, 1177, 380, 360, 300, 13, 400, 370, 11, 291, 458, 11, 415, 575, 1128, 1564, 668, 51312], "temperature": 0.0, "avg_logprob": -0.1035952126538312, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.003648025682196021}, {"id": 623, "seek": 329456, "start": 3313.52, "end": 3319.36, "text": " associated with any experiment or corpus work ever. And so it's all thought experiments,", "tokens": [51312, 6615, 365, 604, 5120, 420, 1181, 31624, 589, 1562, 13, 400, 370, 309, 311, 439, 1194, 12050, 11, 51604], "temperature": 0.0, "avg_logprob": -0.1035952126538312, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.003648025682196021}, {"id": 624, "seek": 329456, "start": 3319.36, "end": 3323.36, "text": " it's his own intuitions. So I just don't think that's the way to do things.", "tokens": [51604, 309, 311, 702, 1065, 16224, 626, 13, 407, 286, 445, 500, 380, 519, 300, 311, 264, 636, 281, 360, 721, 13, 51804], "temperature": 0.0, "avg_logprob": -0.1035952126538312, "compression_ratio": 1.7131474103585658, "no_speech_prob": 0.003648025682196021}, {"id": 625, "seek": 332456, "start": 3324.88, "end": 3329.36, "text": " That's a, you know, across the street, they're across the street from us, kind of difference", "tokens": [50380, 663, 311, 257, 11, 291, 458, 11, 2108, 264, 4838, 11, 436, 434, 2108, 264, 4838, 490, 505, 11, 733, 295, 2649, 50604], "temperature": 0.0, "avg_logprob": -0.18235811412843883, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0011872929753735662}, {"id": 626, "seek": 332456, "start": 3329.36, "end": 3334.16, "text": " between brain and cog sci and linguistics. I mean, not all linguists, some of the linguists,", "tokens": [50604, 1296, 3567, 293, 46521, 2180, 293, 21766, 6006, 13, 286, 914, 11, 406, 439, 21766, 1751, 11, 512, 295, 264, 21766, 1751, 11, 50844], "temperature": 0.0, "avg_logprob": -0.18235811412843883, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0011872929753735662}, {"id": 627, "seek": 332456, "start": 3334.16, "end": 3338.96, "text": " depending on what you do, more speech oriented, they do more quantitative stuff. But in the", "tokens": [50844, 5413, 322, 437, 291, 360, 11, 544, 6218, 21841, 11, 436, 360, 544, 27778, 1507, 13, 583, 294, 264, 51084], "temperature": 0.0, "avg_logprob": -0.18235811412843883, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0011872929753735662}, {"id": 628, "seek": 332456, "start": 3339.84, "end": 3345.52, "text": " meaning, words and, well, it's combinations of words, syntax, semantics, they tend not to do", "tokens": [51128, 3620, 11, 2283, 293, 11, 731, 11, 309, 311, 21267, 295, 2283, 11, 28431, 11, 4361, 45298, 11, 436, 3928, 406, 281, 360, 51412], "temperature": 0.0, "avg_logprob": -0.18235811412843883, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0011872929753735662}, {"id": 629, "seek": 332456, "start": 3346.08, "end": 3352.56, "text": " experiments and corpus analyses. So I know in linguistics size, probably, well,", "tokens": [51440, 12050, 293, 1181, 31624, 37560, 13, 407, 286, 458, 294, 21766, 6006, 2744, 11, 1391, 11, 731, 11, 51764], "temperature": 0.0, "avg_logprob": -0.18235811412843883, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0011872929753735662}, {"id": 630, "seek": 335256, "start": 3352.56, "end": 3358.64, "text": " but the method is a symptom of a bigger approach, which is sort of a psychology philosophy side on", "tokens": [50364, 457, 264, 3170, 307, 257, 29370, 295, 257, 3801, 3109, 11, 597, 307, 1333, 295, 257, 15105, 10675, 1252, 322, 50668], "temperature": 0.0, "avg_logprob": -0.17933589350568116, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0009693900938145816}, {"id": 631, "seek": 335256, "start": 3358.64, "end": 3363.52, "text": " Nome. And for you, it's more sort of data driven, sort of almost like mathematical approach.", "tokens": [50668, 426, 423, 13, 400, 337, 291, 11, 309, 311, 544, 1333, 295, 1412, 9555, 11, 1333, 295, 1920, 411, 18894, 3109, 13, 50912], "temperature": 0.0, "avg_logprob": -0.17933589350568116, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0009693900938145816}, {"id": 632, "seek": 335256, "start": 3363.52, "end": 3368.16, "text": " Yeah, I mean, I'm a psychologist. So I would say we're in psychology. You know, I mean,", "tokens": [50912, 865, 11, 286, 914, 11, 286, 478, 257, 29514, 13, 407, 286, 576, 584, 321, 434, 294, 15105, 13, 509, 458, 11, 286, 914, 11, 51144], "temperature": 0.0, "avg_logprob": -0.17933589350568116, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0009693900938145816}, {"id": 633, "seek": 335256, "start": 3368.16, "end": 3372.72, "text": " brain and cognitive sciences is MIT's old psychology department. It was a psychology", "tokens": [51144, 3567, 293, 15605, 17677, 307, 13100, 311, 1331, 15105, 5882, 13, 467, 390, 257, 15105, 51372], "temperature": 0.0, "avg_logprob": -0.17933589350568116, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0009693900938145816}, {"id": 634, "seek": 335256, "start": 3372.72, "end": 3377.04, "text": " department up until 1985, and it became the brain and cognitive science department. And so,", "tokens": [51372, 5882, 493, 1826, 28962, 11, 293, 309, 3062, 264, 3567, 293, 15605, 3497, 5882, 13, 400, 370, 11, 51588], "temperature": 0.0, "avg_logprob": -0.17933589350568116, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0009693900938145816}, {"id": 635, "seek": 335256, "start": 3377.6, "end": 3381.44, "text": " I mean, my training is in psychology, I mean, my training is math and computer science, but I'm", "tokens": [51616, 286, 914, 11, 452, 3097, 307, 294, 15105, 11, 286, 914, 11, 452, 3097, 307, 5221, 293, 3820, 3497, 11, 457, 286, 478, 51808], "temperature": 0.0, "avg_logprob": -0.17933589350568116, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0009693900938145816}, {"id": 636, "seek": 338144, "start": 3381.44, "end": 3384.32, "text": " a psychologist. I mean, I mean, I don't know what I am.", "tokens": [50364, 257, 29514, 13, 286, 914, 11, 286, 914, 11, 286, 500, 380, 458, 437, 286, 669, 13, 50508], "temperature": 0.0, "avg_logprob": -0.20733011179956898, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.001454847166314721}, {"id": 637, "seek": 338144, "start": 3384.32, "end": 3386.4, "text": " So data driven, psychologist. Yeah, yeah, yeah.", "tokens": [50508, 407, 1412, 9555, 11, 29514, 13, 865, 11, 1338, 11, 1338, 13, 50612], "temperature": 0.0, "avg_logprob": -0.20733011179956898, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.001454847166314721}, {"id": 638, "seek": 338144, "start": 3386.4, "end": 3390.64, "text": " You are. I am what I am, but I'm having to be called a linguist. I'm having to be called a", "tokens": [50612, 509, 366, 13, 286, 669, 437, 286, 669, 11, 457, 286, 478, 1419, 281, 312, 1219, 257, 21766, 468, 13, 286, 478, 1419, 281, 312, 1219, 257, 50824], "temperature": 0.0, "avg_logprob": -0.20733011179956898, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.001454847166314721}, {"id": 639, "seek": 338144, "start": 3390.64, "end": 3393.76, "text": " computer scientist. I'm having to be called a psychologist, any of those things.", "tokens": [50824, 3820, 12662, 13, 286, 478, 1419, 281, 312, 1219, 257, 29514, 11, 604, 295, 729, 721, 13, 50980], "temperature": 0.0, "avg_logprob": -0.20733011179956898, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.001454847166314721}, {"id": 640, "seek": 338144, "start": 3393.76, "end": 3398.8, "text": " But in the actual, like how that manifests itself outside of the methodology is like these", "tokens": [50980, 583, 294, 264, 3539, 11, 411, 577, 300, 50252, 2564, 2380, 295, 264, 24850, 307, 411, 613, 51232], "temperature": 0.0, "avg_logprob": -0.20733011179956898, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.001454847166314721}, {"id": 641, "seek": 338144, "start": 3398.8, "end": 3403.76, "text": " differences, these subtle differences about the movement story versus the lexical copy story.", "tokens": [51232, 7300, 11, 613, 13743, 7300, 466, 264, 3963, 1657, 5717, 264, 476, 87, 804, 5055, 1657, 13, 51480], "temperature": 0.0, "avg_logprob": -0.20733011179956898, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.001454847166314721}, {"id": 642, "seek": 338144, "start": 3403.76, "end": 3408.64, "text": " Yeah, those are theories, right? So the theories, like the theories are, but I think the reason", "tokens": [51480, 865, 11, 729, 366, 13667, 11, 558, 30, 407, 264, 13667, 11, 411, 264, 13667, 366, 11, 457, 286, 519, 264, 1778, 51724], "temperature": 0.0, "avg_logprob": -0.20733011179956898, "compression_ratio": 1.992831541218638, "no_speech_prob": 0.001454847166314721}, {"id": 643, "seek": 340864, "start": 3408.64, "end": 3414.7999999999997, "text": " we differ in part is because of how we evaluate the theories. And so I evaluate theories quantitatively,", "tokens": [50364, 321, 743, 294, 644, 307, 570, 295, 577, 321, 13059, 264, 13667, 13, 400, 370, 286, 13059, 13667, 27778, 356, 11, 50672], "temperature": 0.0, "avg_logprob": -0.12334482857350552, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015008981572464108}, {"id": 644, "seek": 340864, "start": 3414.7999999999997, "end": 3422.8799999999997, "text": " and Nome doesn't. Got it. Okay, well, let's let's explore the theories that you explore in your", "tokens": [50672, 293, 426, 423, 1177, 380, 13, 5803, 309, 13, 1033, 11, 731, 11, 718, 311, 718, 311, 6839, 264, 13667, 300, 291, 6839, 294, 428, 51076], "temperature": 0.0, "avg_logprob": -0.12334482857350552, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015008981572464108}, {"id": 645, "seek": 340864, "start": 3422.8799999999997, "end": 3430.96, "text": " book. Let's return to this dependency grammar framework of looking at language. What's a good", "tokens": [51076, 1446, 13, 961, 311, 2736, 281, 341, 33621, 22317, 8388, 295, 1237, 412, 2856, 13, 708, 311, 257, 665, 51480], "temperature": 0.0, "avg_logprob": -0.12334482857350552, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015008981572464108}, {"id": 646, "seek": 340864, "start": 3430.96, "end": 3434.56, "text": " justification why the dependency grammar framework is a good way to explain language?", "tokens": [51480, 31591, 983, 264, 33621, 22317, 8388, 307, 257, 665, 636, 281, 2903, 2856, 30, 51660], "temperature": 0.0, "avg_logprob": -0.12334482857350552, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0015008981572464108}, {"id": 647, "seek": 343456, "start": 3435.2799999999997, "end": 3439.84, "text": " What's your intuition? So the reason I like dependency grammar, as I've", "tokens": [50400, 708, 311, 428, 24002, 30, 407, 264, 1778, 286, 411, 33621, 22317, 11, 382, 286, 600, 50628], "temperature": 0.0, "avg_logprob": -0.10894686715644702, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0015976110007613897}, {"id": 648, "seek": 343456, "start": 3440.56, "end": 3446.0, "text": " said before, is that it's very transparent about its representation of distance between words.", "tokens": [50664, 848, 949, 11, 307, 300, 309, 311, 588, 12737, 466, 1080, 10290, 295, 4560, 1296, 2283, 13, 50936], "temperature": 0.0, "avg_logprob": -0.10894686715644702, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0015976110007613897}, {"id": 649, "seek": 343456, "start": 3446.0, "end": 3451.12, "text": " So it's like, all it is, is you've got a bunch of words, you're connecting them together to make a", "tokens": [50936, 407, 309, 311, 411, 11, 439, 309, 307, 11, 307, 291, 600, 658, 257, 3840, 295, 2283, 11, 291, 434, 11015, 552, 1214, 281, 652, 257, 51192], "temperature": 0.0, "avg_logprob": -0.10894686715644702, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0015976110007613897}, {"id": 650, "seek": 343456, "start": 3451.12, "end": 3460.16, "text": " sentence. And a really neat insight, which turns out to be true, is that the further apart the pair", "tokens": [51192, 8174, 13, 400, 257, 534, 10654, 11269, 11, 597, 4523, 484, 281, 312, 2074, 11, 307, 300, 264, 3052, 4936, 264, 6119, 51644], "temperature": 0.0, "avg_logprob": -0.10894686715644702, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0015976110007613897}, {"id": 651, "seek": 343456, "start": 3460.16, "end": 3463.68, "text": " of words are that you're connecting the harder it is to do the production, the harder it is to", "tokens": [51644, 295, 2283, 366, 300, 291, 434, 11015, 264, 6081, 309, 307, 281, 360, 264, 4265, 11, 264, 6081, 309, 307, 281, 51820], "temperature": 0.0, "avg_logprob": -0.10894686715644702, "compression_ratio": 1.7424242424242424, "no_speech_prob": 0.0015976110007613897}, {"id": 652, "seek": 346368, "start": 3463.68, "end": 3467.12, "text": " do the comprehension. It's as hard to produce, it's hard to understand when the words are far", "tokens": [50364, 360, 264, 44991, 13, 467, 311, 382, 1152, 281, 5258, 11, 309, 311, 1152, 281, 1223, 562, 264, 2283, 366, 1400, 50536], "temperature": 0.0, "avg_logprob": -0.11293807200023107, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0032721362076699734}, {"id": 653, "seek": 346368, "start": 3467.12, "end": 3472.24, "text": " apart. When they're close together, it's easy to produce and it's easy to comprehend. Let me", "tokens": [50536, 4936, 13, 1133, 436, 434, 1998, 1214, 11, 309, 311, 1858, 281, 5258, 293, 309, 311, 1858, 281, 38183, 13, 961, 385, 50792], "temperature": 0.0, "avg_logprob": -0.11293807200023107, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0032721362076699734}, {"id": 654, "seek": 346368, "start": 3472.24, "end": 3480.16, "text": " give you an example. Okay, so we have, in any language, we have mostly local connections between", "tokens": [50792, 976, 291, 364, 1365, 13, 1033, 11, 370, 321, 362, 11, 294, 604, 2856, 11, 321, 362, 5240, 2654, 9271, 1296, 51188], "temperature": 0.0, "avg_logprob": -0.11293807200023107, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0032721362076699734}, {"id": 655, "seek": 346368, "start": 3480.16, "end": 3484.96, "text": " words, but they're abstract. The connections are abstract, they're between categories of words.", "tokens": [51188, 2283, 11, 457, 436, 434, 12649, 13, 440, 9271, 366, 12649, 11, 436, 434, 1296, 10479, 295, 2283, 13, 51428], "temperature": 0.0, "avg_logprob": -0.11293807200023107, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0032721362076699734}, {"id": 656, "seek": 346368, "start": 3484.96, "end": 3492.3999999999996, "text": " And so you can always make things further apart if you add modification, for example,", "tokens": [51428, 400, 370, 291, 393, 1009, 652, 721, 3052, 4936, 498, 291, 909, 26747, 11, 337, 1365, 11, 51800], "temperature": 0.0, "avg_logprob": -0.11293807200023107, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0032721362076699734}, {"id": 657, "seek": 349240, "start": 3492.4, "end": 3499.04, "text": " after a noun. So a noun in English comes before a verb, the subject noun comes before a verb,", "tokens": [50364, 934, 257, 23307, 13, 407, 257, 23307, 294, 3669, 1487, 949, 257, 9595, 11, 264, 3983, 23307, 1487, 949, 257, 9595, 11, 50696], "temperature": 0.0, "avg_logprob": -0.0928615415939177, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.003375324420630932}, {"id": 658, "seek": 349240, "start": 3499.04, "end": 3504.0, "text": " and then there's an object after, for example. So I can say what I said before, the dog entered", "tokens": [50696, 293, 550, 456, 311, 364, 2657, 934, 11, 337, 1365, 13, 407, 286, 393, 584, 437, 286, 848, 949, 11, 264, 3000, 9065, 50944], "temperature": 0.0, "avg_logprob": -0.0928615415939177, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.003375324420630932}, {"id": 659, "seek": 349240, "start": 3504.0, "end": 3509.2000000000003, "text": " the room or something like that. So I can modify dog. If I say something more about dog after it,", "tokens": [50944, 264, 1808, 420, 746, 411, 300, 13, 407, 286, 393, 16927, 3000, 13, 759, 286, 584, 746, 544, 466, 3000, 934, 309, 11, 51204], "temperature": 0.0, "avg_logprob": -0.0928615415939177, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.003375324420630932}, {"id": 660, "seek": 349240, "start": 3509.2000000000003, "end": 3516.2400000000002, "text": " then what I'm doing is, indirectly, I'm lengthening the dependence between dog and entered by adding", "tokens": [51204, 550, 437, 286, 478, 884, 307, 11, 37779, 11, 286, 478, 4641, 4559, 264, 31704, 1296, 3000, 293, 9065, 538, 5127, 51556], "temperature": 0.0, "avg_logprob": -0.0928615415939177, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.003375324420630932}, {"id": 661, "seek": 351624, "start": 3516.24, "end": 3526.3999999999996, "text": " more stuff to it. So I just make it explicit here if I say the boy who the cat scratched", "tokens": [50364, 544, 1507, 281, 309, 13, 407, 286, 445, 652, 309, 13691, 510, 498, 286, 584, 264, 3237, 567, 264, 3857, 40513, 50872], "temperature": 0.0, "avg_logprob": -0.11036589227873704, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.17756351828575134}, {"id": 662, "seek": 351624, "start": 3527.04, "end": 3532.4799999999996, "text": " cried. We're going to have a mean cat here. And so what I've got here is, the boy cried,", "tokens": [50904, 16266, 13, 492, 434, 516, 281, 362, 257, 914, 3857, 510, 13, 400, 370, 437, 286, 600, 658, 510, 307, 11, 264, 3237, 16266, 11, 51176], "temperature": 0.0, "avg_logprob": -0.11036589227873704, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.17756351828575134}, {"id": 663, "seek": 351624, "start": 3532.4799999999996, "end": 3537.68, "text": " it would be a very short, simple sentence, and I just told you something about the boy,", "tokens": [51176, 309, 576, 312, 257, 588, 2099, 11, 2199, 8174, 11, 293, 286, 445, 1907, 291, 746, 466, 264, 3237, 11, 51436], "temperature": 0.0, "avg_logprob": -0.11036589227873704, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.17756351828575134}, {"id": 664, "seek": 351624, "start": 3537.68, "end": 3541.04, "text": " and I told you it was the boy who the cat scratched. Okay.", "tokens": [51436, 293, 286, 1907, 291, 309, 390, 264, 3237, 567, 264, 3857, 40513, 13, 1033, 13, 51604], "temperature": 0.0, "avg_logprob": -0.11036589227873704, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.17756351828575134}, {"id": 665, "seek": 351624, "start": 3541.04, "end": 3545.8399999999997, "text": " So the cry is connected to the boy. The cry at the end is connected to the boy in the beginning.", "tokens": [51604, 407, 264, 3305, 307, 4582, 281, 264, 3237, 13, 440, 3305, 412, 264, 917, 307, 4582, 281, 264, 3237, 294, 264, 2863, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11036589227873704, "compression_ratio": 1.8304347826086957, "no_speech_prob": 0.17756351828575134}, {"id": 666, "seek": 354584, "start": 3545.84, "end": 3549.6000000000004, "text": " Right. And so I can do that. And I can say that that's a perfectly fine English sentence.", "tokens": [50364, 1779, 13, 400, 370, 286, 393, 360, 300, 13, 400, 286, 393, 584, 300, 300, 311, 257, 6239, 2489, 3669, 8174, 13, 50552], "temperature": 0.0, "avg_logprob": -0.13440723789548412, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002251580823212862}, {"id": 667, "seek": 354584, "start": 3549.6000000000004, "end": 3557.6800000000003, "text": " And I can say the cat which the dog chased ran away or something. Okay. I can do that.", "tokens": [50552, 400, 286, 393, 584, 264, 3857, 597, 264, 3000, 33091, 5872, 1314, 420, 746, 13, 1033, 13, 286, 393, 360, 300, 13, 50956], "temperature": 0.0, "avg_logprob": -0.13440723789548412, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002251580823212862}, {"id": 668, "seek": 354584, "start": 3557.6800000000003, "end": 3562.88, "text": " But it's really hard. So it's really hard now. I've got whatever I have here. I have the boy", "tokens": [50956, 583, 309, 311, 534, 1152, 13, 407, 309, 311, 534, 1152, 586, 13, 286, 600, 658, 2035, 286, 362, 510, 13, 286, 362, 264, 3237, 51216], "temperature": 0.0, "avg_logprob": -0.13440723789548412, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002251580823212862}, {"id": 669, "seek": 354584, "start": 3563.6000000000004, "end": 3570.7200000000003, "text": " who the cat. Now let's say I try to modify cat. Okay. The boy who the cat which the dog", "tokens": [51252, 567, 264, 3857, 13, 823, 718, 311, 584, 286, 853, 281, 16927, 3857, 13, 1033, 13, 440, 3237, 567, 264, 3857, 597, 264, 3000, 51608], "temperature": 0.0, "avg_logprob": -0.13440723789548412, "compression_ratio": 1.803030303030303, "no_speech_prob": 0.002251580823212862}, {"id": 670, "seek": 357072, "start": 3570.7999999999997, "end": 3575.68, "text": " chased scratched ran away. Oh my God, that's hard, right? I can, I'm sort of just working", "tokens": [50368, 33091, 40513, 5872, 1314, 13, 876, 452, 1265, 11, 300, 311, 1152, 11, 558, 30, 286, 393, 11, 286, 478, 1333, 295, 445, 1364, 50612], "temperature": 0.0, "avg_logprob": -0.11989670246839523, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.026753513142466545}, {"id": 671, "seek": 357072, "start": 3575.68, "end": 3580.48, "text": " that through in my head how to produce and how to, and it's really just horrendous to understand.", "tokens": [50612, 300, 807, 294, 452, 1378, 577, 281, 5258, 293, 577, 281, 11, 293, 309, 311, 534, 445, 49520, 563, 281, 1223, 13, 50852], "temperature": 0.0, "avg_logprob": -0.11989670246839523, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.026753513142466545}, {"id": 672, "seek": 357072, "start": 3580.48, "end": 3584.48, "text": " It's not so bad. At least I've got intonation there to sort of mark the boundaries and stuff.", "tokens": [50852, 467, 311, 406, 370, 1578, 13, 1711, 1935, 286, 600, 658, 560, 266, 399, 456, 281, 1333, 295, 1491, 264, 13180, 293, 1507, 13, 51052], "temperature": 0.0, "avg_logprob": -0.11989670246839523, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.026753513142466545}, {"id": 673, "seek": 357072, "start": 3584.48, "end": 3590.3999999999996, "text": " But it's, that's really complicated. That's sort of English in a way. I mean, that follows the", "tokens": [51052, 583, 309, 311, 11, 300, 311, 534, 6179, 13, 663, 311, 1333, 295, 3669, 294, 257, 636, 13, 286, 914, 11, 300, 10002, 264, 51348], "temperature": 0.0, "avg_logprob": -0.11989670246839523, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.026753513142466545}, {"id": 674, "seek": 357072, "start": 3590.3999999999996, "end": 3595.2799999999997, "text": " rules of English. But so what's interesting about that is, is that what I'm doing is nesting", "tokens": [51348, 4474, 295, 3669, 13, 583, 370, 437, 311, 1880, 466, 300, 307, 11, 307, 300, 437, 286, 478, 884, 307, 297, 8714, 51592], "temperature": 0.0, "avg_logprob": -0.11989670246839523, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.026753513142466545}, {"id": 675, "seek": 359528, "start": 3595.28, "end": 3601.28, "text": " dependencies there. I'm putting one, I've got a subject connected to a verb there. And then I'm", "tokens": [50364, 36606, 456, 13, 286, 478, 3372, 472, 11, 286, 600, 658, 257, 3983, 4582, 281, 257, 9595, 456, 13, 400, 550, 286, 478, 50664], "temperature": 0.0, "avg_logprob": -0.13310854538627293, "compression_ratio": 1.884, "no_speech_prob": 0.12586945295333862}, {"id": 676, "seek": 359528, "start": 3601.28, "end": 3606.1600000000003, "text": " modifying that with a clause, another clause, which happens to have a subject and a verb relation.", "tokens": [50664, 42626, 300, 365, 257, 25925, 11, 1071, 25925, 11, 597, 2314, 281, 362, 257, 3983, 293, 257, 9595, 9721, 13, 50908], "temperature": 0.0, "avg_logprob": -0.13310854538627293, "compression_ratio": 1.884, "no_speech_prob": 0.12586945295333862}, {"id": 677, "seek": 359528, "start": 3606.1600000000003, "end": 3611.2000000000003, "text": " I'm trying to do that again on the second one. And what that does is it lengthens out the dependence,", "tokens": [50908, 286, 478, 1382, 281, 360, 300, 797, 322, 264, 1150, 472, 13, 400, 437, 300, 775, 307, 309, 4641, 694, 484, 264, 31704, 11, 51160], "temperature": 0.0, "avg_logprob": -0.13310854538627293, "compression_ratio": 1.884, "no_speech_prob": 0.12586945295333862}, {"id": 678, "seek": 359528, "start": 3611.2000000000003, "end": 3615.2000000000003, "text": " multiple dependence actually get lengthened out there. The dependencies get longer,", "tokens": [51160, 3866, 31704, 767, 483, 4641, 5320, 484, 456, 13, 440, 36606, 483, 2854, 11, 51360], "temperature": 0.0, "avg_logprob": -0.13310854538627293, "compression_ratio": 1.884, "no_speech_prob": 0.12586945295333862}, {"id": 679, "seek": 359528, "start": 3615.2000000000003, "end": 3619.6000000000004, "text": " on the outside ones get long. And even the ones in between get kind of long. And you just,", "tokens": [51360, 322, 264, 2380, 2306, 483, 938, 13, 400, 754, 264, 2306, 294, 1296, 483, 733, 295, 938, 13, 400, 291, 445, 11, 51580], "temperature": 0.0, "avg_logprob": -0.13310854538627293, "compression_ratio": 1.884, "no_speech_prob": 0.12586945295333862}, {"id": 680, "seek": 361960, "start": 3619.6, "end": 3625.04, "text": " so what's fascinating is that that's bad. That's really horrendous in English.", "tokens": [50364, 370, 437, 311, 10343, 307, 300, 300, 311, 1578, 13, 663, 311, 534, 49520, 563, 294, 3669, 13, 50636], "temperature": 0.0, "avg_logprob": -0.12803277799061366, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0027145950589329004}, {"id": 681, "seek": 361960, "start": 3625.92, "end": 3630.3199999999997, "text": " But that's horrendous in any language. And so in, in no matter what language you look at,", "tokens": [50680, 583, 300, 311, 49520, 563, 294, 604, 2856, 13, 400, 370, 294, 11, 294, 572, 1871, 437, 2856, 291, 574, 412, 11, 50900], "temperature": 0.0, "avg_logprob": -0.12803277799061366, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0027145950589329004}, {"id": 682, "seek": 361960, "start": 3630.3199999999997, "end": 3635.68, "text": " if you do just figure out some structure where I'm going to have some modification following", "tokens": [50900, 498, 291, 360, 445, 2573, 484, 512, 3877, 689, 286, 478, 516, 281, 362, 512, 26747, 3480, 51168], "temperature": 0.0, "avg_logprob": -0.12803277799061366, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0027145950589329004}, {"id": 683, "seek": 361960, "start": 3635.68, "end": 3639.92, "text": " some head, which is connected to some later head, and I do it again, it won't be good.", "tokens": [51168, 512, 1378, 11, 597, 307, 4582, 281, 512, 1780, 1378, 11, 293, 286, 360, 309, 797, 11, 309, 1582, 380, 312, 665, 13, 51380], "temperature": 0.0, "avg_logprob": -0.12803277799061366, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0027145950589329004}, {"id": 684, "seek": 361960, "start": 3639.92, "end": 3645.2799999999997, "text": " It guaranteed like 100% that will be uninterpretable in that language in the same way that was", "tokens": [51380, 467, 18031, 411, 2319, 4, 300, 486, 312, 517, 41935, 712, 294, 300, 2856, 294, 264, 912, 636, 300, 390, 51648], "temperature": 0.0, "avg_logprob": -0.12803277799061366, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0027145950589329004}, {"id": 685, "seek": 364528, "start": 3645.28, "end": 3650.32, "text": " uninterpretable in English. Just clarify, the distance of the dependencies is whenever", "tokens": [50364, 517, 41935, 712, 294, 3669, 13, 1449, 17594, 11, 264, 4560, 295, 264, 36606, 307, 5699, 50616], "temperature": 0.0, "avg_logprob": -0.14198350498818943, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.04533723369240761}, {"id": 686, "seek": 364528, "start": 3651.28, "end": 3659.2000000000003, "text": " the boy cried, there's a dependence between two words. And then you counting the number of what", "tokens": [50664, 264, 3237, 16266, 11, 456, 311, 257, 31704, 1296, 732, 2283, 13, 400, 550, 291, 13251, 264, 1230, 295, 437, 51060], "temperature": 0.0, "avg_logprob": -0.14198350498818943, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.04533723369240761}, {"id": 687, "seek": 364528, "start": 3659.2000000000003, "end": 3664.88, "text": " morphemes between them. That's a good question. I just say words. Your words are morphemes between.", "tokens": [51060, 25778, 443, 279, 1296, 552, 13, 663, 311, 257, 665, 1168, 13, 286, 445, 584, 2283, 13, 2260, 2283, 366, 25778, 443, 279, 1296, 13, 51344], "temperature": 0.0, "avg_logprob": -0.14198350498818943, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.04533723369240761}, {"id": 688, "seek": 364528, "start": 3664.88, "end": 3668.0800000000004, "text": " We don't know that. Actually, that's a very good question. What is the distance metric?", "tokens": [51344, 492, 500, 380, 458, 300, 13, 5135, 11, 300, 311, 257, 588, 665, 1168, 13, 708, 307, 264, 4560, 20678, 30, 51504], "temperature": 0.0, "avg_logprob": -0.14198350498818943, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.04533723369240761}, {"id": 689, "seek": 364528, "start": 3668.0800000000004, "end": 3673.52, "text": " But let's just say it's words. Sure. And you're saying the longer the distance of that dependence,", "tokens": [51504, 583, 718, 311, 445, 584, 309, 311, 2283, 13, 4894, 13, 400, 291, 434, 1566, 264, 2854, 264, 4560, 295, 300, 31704, 11, 51776], "temperature": 0.0, "avg_logprob": -0.14198350498818943, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.04533723369240761}, {"id": 690, "seek": 367352, "start": 3673.52, "end": 3682.88, "text": " the more no matter the language, except legalese. Even legalese. We'll talk about it. Okay. But that,", "tokens": [50364, 264, 544, 572, 1871, 264, 2856, 11, 3993, 5089, 1130, 13, 2754, 5089, 1130, 13, 492, 603, 751, 466, 309, 13, 1033, 13, 583, 300, 11, 50832], "temperature": 0.0, "avg_logprob": -0.2184284210205078, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.005217971745878458}, {"id": 691, "seek": 367352, "start": 3684.32, "end": 3688.48, "text": " the people will be very upset that speak that language, not upset, but they'll either not", "tokens": [50904, 264, 561, 486, 312, 588, 8340, 300, 1710, 300, 2856, 11, 406, 8340, 11, 457, 436, 603, 2139, 406, 51112], "temperature": 0.0, "avg_logprob": -0.2184284210205078, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.005217971745878458}, {"id": 692, "seek": 367352, "start": 3688.48, "end": 3693.7599999999998, "text": " understand it. They'll be like, this is, their brain will be working in overtime.", "tokens": [51112, 1223, 309, 13, 814, 603, 312, 411, 11, 341, 307, 11, 641, 3567, 486, 312, 1364, 294, 29863, 13, 51376], "temperature": 0.0, "avg_logprob": -0.2184284210205078, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.005217971745878458}, {"id": 693, "seek": 367352, "start": 3693.7599999999998, "end": 3697.52, "text": " They will have a hard time either producing or comprehending it. They might tell you that's not", "tokens": [51376, 814, 486, 362, 257, 1152, 565, 2139, 10501, 420, 10753, 2029, 309, 13, 814, 1062, 980, 291, 300, 311, 406, 51564], "temperature": 0.0, "avg_logprob": -0.2184284210205078, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.005217971745878458}, {"id": 694, "seek": 367352, "start": 3697.52, "end": 3702.32, "text": " their language. It's sort of the language. I mean, it's following, they'll agree with each of those", "tokens": [51564, 641, 2856, 13, 467, 311, 1333, 295, 264, 2856, 13, 286, 914, 11, 309, 311, 3480, 11, 436, 603, 3986, 365, 1184, 295, 729, 51804], "temperature": 0.0, "avg_logprob": -0.2184284210205078, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.005217971745878458}, {"id": 695, "seek": 370232, "start": 3702.32, "end": 3707.36, "text": " pieces as part of the language. But somehow that combination will be very, very difficult to produce", "tokens": [50364, 3755, 382, 644, 295, 264, 2856, 13, 583, 6063, 300, 6562, 486, 312, 588, 11, 588, 2252, 281, 5258, 50616], "temperature": 0.0, "avg_logprob": -0.12174687241062973, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.0010648756287992}, {"id": 696, "seek": 370232, "start": 3707.36, "end": 3713.1200000000003, "text": " and understand. Is that a chicken or the egg issue here? Well, I'm giving you an explanation.", "tokens": [50616, 293, 1223, 13, 1119, 300, 257, 4662, 420, 264, 3777, 2734, 510, 30, 1042, 11, 286, 478, 2902, 291, 364, 10835, 13, 50904], "temperature": 0.0, "avg_logprob": -0.12174687241062973, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.0010648756287992}, {"id": 697, "seek": 370232, "start": 3716.2400000000002, "end": 3719.36, "text": " I'm giving you two kinds of explanations. I'm telling you that center embedding,", "tokens": [51060, 286, 478, 2902, 291, 732, 3685, 295, 28708, 13, 286, 478, 3585, 291, 300, 3056, 12240, 3584, 11, 51216], "temperature": 0.0, "avg_logprob": -0.12174687241062973, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.0010648756287992}, {"id": 698, "seek": 370232, "start": 3719.36, "end": 3725.36, "text": " that's nesting, those are synonyms for the same concept here. And the explanation for what,", "tokens": [51216, 300, 311, 297, 8714, 11, 729, 366, 5451, 2526, 2592, 337, 264, 912, 3410, 510, 13, 400, 264, 10835, 337, 437, 11, 51516], "temperature": 0.0, "avg_logprob": -0.12174687241062973, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.0010648756287992}, {"id": 699, "seek": 370232, "start": 3725.36, "end": 3728.4, "text": " those are always hard. Center embedding and nesting are always hard. And I give you an", "tokens": [51516, 729, 366, 1009, 1152, 13, 5169, 12240, 3584, 293, 297, 8714, 366, 1009, 1152, 13, 400, 286, 976, 291, 364, 51668], "temperature": 0.0, "avg_logprob": -0.12174687241062973, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.0010648756287992}, {"id": 700, "seek": 370232, "start": 3728.4, "end": 3732.2400000000002, "text": " explanation for why they might be hard, which is long distance connections. There's a", "tokens": [51668, 10835, 337, 983, 436, 1062, 312, 1152, 11, 597, 307, 938, 4560, 9271, 13, 821, 311, 257, 51860], "temperature": 0.0, "avg_logprob": -0.12174687241062973, "compression_ratio": 1.8685121107266436, "no_speech_prob": 0.0010648756287992}, {"id": 701, "seek": 373224, "start": 3732.3199999999997, "end": 3736.24, "text": " when you do center embedding, when you do nesting, you always have long distance connections between", "tokens": [50368, 562, 291, 360, 3056, 12240, 3584, 11, 562, 291, 360, 297, 8714, 11, 291, 1009, 362, 938, 4560, 9271, 1296, 50564], "temperature": 0.0, "avg_logprob": -0.12216304477892424, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0003353267384227365}, {"id": 702, "seek": 373224, "start": 3736.24, "end": 3741.7599999999998, "text": " the dependents. That's not necessarily the right explanation. I can go through reasons why that's", "tokens": [50564, 264, 5672, 791, 13, 663, 311, 406, 4725, 264, 558, 10835, 13, 286, 393, 352, 807, 4112, 983, 300, 311, 50840], "temperature": 0.0, "avg_logprob": -0.12216304477892424, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0003353267384227365}, {"id": 703, "seek": 373224, "start": 3741.7599999999998, "end": 3747.7599999999998, "text": " probably a good explanation. And it's not really just about one of them. So probably it's a pair", "tokens": [50840, 1391, 257, 665, 10835, 13, 400, 309, 311, 406, 534, 445, 466, 472, 295, 552, 13, 407, 1391, 309, 311, 257, 6119, 51140], "temperature": 0.0, "avg_logprob": -0.12216304477892424, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0003353267384227365}, {"id": 704, "seek": 373224, "start": 3747.7599999999998, "end": 3753.2799999999997, "text": " of them or something of these dependents that get long, that drives you to be really confused", "tokens": [51140, 295, 552, 420, 746, 295, 613, 5672, 791, 300, 483, 938, 11, 300, 11754, 291, 281, 312, 534, 9019, 51416], "temperature": 0.0, "avg_logprob": -0.12216304477892424, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0003353267384227365}, {"id": 705, "seek": 373224, "start": 3753.2799999999997, "end": 3759.68, "text": " in that case. And so what the behavioral consequence there, I mean, this is kind of methods,", "tokens": [51416, 294, 300, 1389, 13, 400, 370, 437, 264, 19124, 18326, 456, 11, 286, 914, 11, 341, 307, 733, 295, 7150, 11, 51736], "temperature": 0.0, "avg_logprob": -0.12216304477892424, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0003353267384227365}, {"id": 706, "seek": 375968, "start": 3759.68, "end": 3764.3199999999997, "text": " like how do we get at this? You could try to do experiments to get people to produce these things.", "tokens": [50364, 411, 577, 360, 321, 483, 412, 341, 30, 509, 727, 853, 281, 360, 12050, 281, 483, 561, 281, 5258, 613, 721, 13, 50596], "temperature": 0.0, "avg_logprob": -0.132685171591269, "compression_ratio": 1.9262295081967213, "no_speech_prob": 0.007117708213627338}, {"id": 707, "seek": 375968, "start": 3764.3199999999997, "end": 3767.44, "text": " They're going to have a hard time producing them. You can try to do experiments to get them to", "tokens": [50596, 814, 434, 516, 281, 362, 257, 1152, 565, 10501, 552, 13, 509, 393, 853, 281, 360, 12050, 281, 483, 552, 281, 50752], "temperature": 0.0, "avg_logprob": -0.132685171591269, "compression_ratio": 1.9262295081967213, "no_speech_prob": 0.007117708213627338}, {"id": 708, "seek": 375968, "start": 3767.44, "end": 3773.12, "text": " understand them and see how well they understand them, can they understand them. Another method you", "tokens": [50752, 1223, 552, 293, 536, 577, 731, 436, 1223, 552, 11, 393, 436, 1223, 552, 13, 3996, 3170, 291, 51036], "temperature": 0.0, "avg_logprob": -0.132685171591269, "compression_ratio": 1.9262295081967213, "no_speech_prob": 0.007117708213627338}, {"id": 709, "seek": 375968, "start": 3773.12, "end": 3779.6, "text": " can do is give people partial materials and ask them to complete them, those center embedded", "tokens": [51036, 393, 360, 307, 976, 561, 14641, 5319, 293, 1029, 552, 281, 3566, 552, 11, 729, 3056, 16741, 51360], "temperature": 0.0, "avg_logprob": -0.132685171591269, "compression_ratio": 1.9262295081967213, "no_speech_prob": 0.007117708213627338}, {"id": 710, "seek": 375968, "start": 3779.6, "end": 3783.8399999999997, "text": " materials and they'll fail. So I've done that. I've done all these kinds of things.", "tokens": [51360, 5319, 293, 436, 603, 3061, 13, 407, 286, 600, 1096, 300, 13, 286, 600, 1096, 439, 613, 3685, 295, 721, 13, 51572], "temperature": 0.0, "avg_logprob": -0.132685171591269, "compression_ratio": 1.9262295081967213, "no_speech_prob": 0.007117708213627338}, {"id": 711, "seek": 378384, "start": 3784.8, "end": 3790.32, "text": " Wait a minute. So central embedding, meaning like you take a normal sentence like boy cried and", "tokens": [50412, 3802, 257, 3456, 13, 407, 5777, 12240, 3584, 11, 3620, 411, 291, 747, 257, 2710, 8174, 411, 3237, 16266, 293, 50688], "temperature": 0.0, "avg_logprob": -0.20446543332909337, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.015418501570820808}, {"id": 712, "seek": 378384, "start": 3790.32, "end": 3796.0, "text": " inject a bunch of crap in the middle that separates the boy and the cried. Okay, that's", "tokens": [50688, 10711, 257, 3840, 295, 12426, 294, 264, 2808, 300, 34149, 264, 3237, 293, 264, 16266, 13, 1033, 11, 300, 311, 50972], "temperature": 0.0, "avg_logprob": -0.20446543332909337, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.015418501570820808}, {"id": 713, "seek": 378384, "start": 3796.0, "end": 3800.2400000000002, "text": " central bedding and nesting is on top of that. No, nesting is the same thing. Center embedding,", "tokens": [50972, 5777, 2901, 3584, 293, 297, 8714, 307, 322, 1192, 295, 300, 13, 883, 11, 297, 8714, 307, 264, 912, 551, 13, 5169, 12240, 3584, 11, 51184], "temperature": 0.0, "avg_logprob": -0.20446543332909337, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.015418501570820808}, {"id": 714, "seek": 378384, "start": 3800.2400000000002, "end": 3804.0, "text": " those are totally equivalent terms. I'm sorry, I sometimes use one and sometimes use the other.", "tokens": [51184, 729, 366, 3879, 10344, 2115, 13, 286, 478, 2597, 11, 286, 2171, 764, 472, 293, 2171, 764, 264, 661, 13, 51372], "temperature": 0.0, "avg_logprob": -0.20446543332909337, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.015418501570820808}, {"id": 715, "seek": 378384, "start": 3804.0, "end": 3810.88, "text": " Got it. Got it. And then what you're saying is there's a bunch of different kinds of experiments", "tokens": [51372, 5803, 309, 13, 5803, 309, 13, 400, 550, 437, 291, 434, 1566, 307, 456, 311, 257, 3840, 295, 819, 3685, 295, 12050, 51716], "temperature": 0.0, "avg_logprob": -0.20446543332909337, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.015418501570820808}, {"id": 716, "seek": 381088, "start": 3810.88, "end": 3815.44, "text": " you can do. I mean, I like to understand one is like have more embedding, more central bedding,", "tokens": [50364, 291, 393, 360, 13, 286, 914, 11, 286, 411, 281, 1223, 472, 307, 411, 362, 544, 12240, 3584, 11, 544, 5777, 2901, 3584, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1334560349670767, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.04334433749318123}, {"id": 717, "seek": 381088, "start": 3815.44, "end": 3819.12, "text": " is it easier or harder to understand, but then you have to measure the level of understanding,", "tokens": [50592, 307, 309, 3571, 420, 6081, 281, 1223, 11, 457, 550, 291, 362, 281, 3481, 264, 1496, 295, 3701, 11, 50776], "temperature": 0.0, "avg_logprob": -0.1334560349670767, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.04334433749318123}, {"id": 718, "seek": 381088, "start": 3819.12, "end": 3823.36, "text": " I guess. Yeah, you could. I mean, there's multiple ways to do that. I mean, there's the simplest", "tokens": [50776, 286, 2041, 13, 865, 11, 291, 727, 13, 286, 914, 11, 456, 311, 3866, 2098, 281, 360, 300, 13, 286, 914, 11, 456, 311, 264, 22811, 50988], "temperature": 0.0, "avg_logprob": -0.1334560349670767, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.04334433749318123}, {"id": 719, "seek": 381088, "start": 3823.36, "end": 3828.0, "text": " ways just to ask people, how good is it sound? How natural is this sound? That's a very blunt,", "tokens": [50988, 2098, 445, 281, 1029, 561, 11, 577, 665, 307, 309, 1626, 30, 1012, 3303, 307, 341, 1626, 30, 663, 311, 257, 588, 32246, 11, 51220], "temperature": 0.0, "avg_logprob": -0.1334560349670767, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.04334433749318123}, {"id": 720, "seek": 381088, "start": 3828.0, "end": 3832.8, "text": " but very good measure. It's very, very reliable. People will do the same thing. And so it's like,", "tokens": [51220, 457, 588, 665, 3481, 13, 467, 311, 588, 11, 588, 12924, 13, 3432, 486, 360, 264, 912, 551, 13, 400, 370, 309, 311, 411, 11, 51460], "temperature": 0.0, "avg_logprob": -0.1334560349670767, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.04334433749318123}, {"id": 721, "seek": 381088, "start": 3832.8, "end": 3836.56, "text": " I don't know what it means exactly, but it's doing something such that we're measuring something", "tokens": [51460, 286, 500, 380, 458, 437, 309, 1355, 2293, 11, 457, 309, 311, 884, 746, 1270, 300, 321, 434, 13389, 746, 51648], "temperature": 0.0, "avg_logprob": -0.1334560349670767, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.04334433749318123}, {"id": 722, "seek": 381088, "start": 3836.56, "end": 3840.1600000000003, "text": " about the confusion, the difficulty associated with those. And those like those are giving you", "tokens": [51648, 466, 264, 15075, 11, 264, 10360, 6615, 365, 729, 13, 400, 729, 411, 729, 366, 2902, 291, 51828], "temperature": 0.0, "avg_logprob": -0.1334560349670767, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.04334433749318123}, {"id": 723, "seek": 384016, "start": 3840.24, "end": 3845.44, "text": " signal. That's why you can say that. What about the completion of the central bed?", "tokens": [50368, 6358, 13, 663, 311, 983, 291, 393, 584, 300, 13, 708, 466, 264, 19372, 295, 264, 5777, 2901, 30, 50628], "temperature": 0.0, "avg_logprob": -0.17151558215801532, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.0049034408293664455}, {"id": 724, "seek": 384016, "start": 3845.44, "end": 3854.3999999999996, "text": " So if you give them a partial sentence, say I say the book which the author who, and I ask you to", "tokens": [50628, 407, 498, 291, 976, 552, 257, 14641, 8174, 11, 584, 286, 584, 264, 1446, 597, 264, 3793, 567, 11, 293, 286, 1029, 291, 281, 51076], "temperature": 0.0, "avg_logprob": -0.17151558215801532, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.0049034408293664455}, {"id": 725, "seek": 384016, "start": 3854.3999999999996, "end": 3858.8799999999997, "text": " now finish that off for me, I mean, either say it, but you can just say it's written in front", "tokens": [51076, 586, 2413, 300, 766, 337, 385, 11, 286, 914, 11, 2139, 584, 309, 11, 457, 291, 393, 445, 584, 309, 311, 3720, 294, 1868, 51300], "temperature": 0.0, "avg_logprob": -0.17151558215801532, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.0049034408293664455}, {"id": 726, "seek": 384016, "start": 3858.8799999999997, "end": 3863.12, "text": " of you and you can just type and have as much time as you want. They will, even though that one's", "tokens": [51300, 295, 291, 293, 291, 393, 445, 2010, 293, 362, 382, 709, 565, 382, 291, 528, 13, 814, 486, 11, 754, 1673, 300, 472, 311, 51512], "temperature": 0.0, "avg_logprob": -0.17151558215801532, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.0049034408293664455}, {"id": 727, "seek": 384016, "start": 3863.12, "end": 3868.48, "text": " not too hard, right? So if I say it's like the book is like, oh, the book which the author who I", "tokens": [51512, 406, 886, 1152, 11, 558, 30, 407, 498, 286, 584, 309, 311, 411, 264, 1446, 307, 411, 11, 1954, 11, 264, 1446, 597, 264, 3793, 567, 286, 51780], "temperature": 0.0, "avg_logprob": -0.17151558215801532, "compression_ratio": 1.817829457364341, "no_speech_prob": 0.0049034408293664455}, {"id": 728, "seek": 386848, "start": 3868.48, "end": 3875.04, "text": " met wrote was good. That's a very simple completion for that. If I give that completion", "tokens": [50364, 1131, 4114, 390, 665, 13, 663, 311, 257, 588, 2199, 19372, 337, 300, 13, 759, 286, 976, 300, 19372, 50692], "temperature": 0.0, "avg_logprob": -0.16409914681081023, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.0028006385546177626}, {"id": 729, "seek": 386848, "start": 3875.04, "end": 3881.04, "text": " on online somewhere to a crowdsourcing platform and ask people to complete that,", "tokens": [50692, 322, 2950, 4079, 281, 257, 26070, 41849, 3663, 293, 1029, 561, 281, 3566, 300, 11, 50992], "temperature": 0.0, "avg_logprob": -0.16409914681081023, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.0028006385546177626}, {"id": 730, "seek": 386848, "start": 3881.04, "end": 3886.48, "text": " they will miss off a verb very regularly, like half of the time, maybe two thirds of the time.", "tokens": [50992, 436, 486, 1713, 766, 257, 9595, 588, 11672, 11, 411, 1922, 295, 264, 565, 11, 1310, 732, 34552, 295, 264, 565, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16409914681081023, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.0028006385546177626}, {"id": 731, "seek": 386848, "start": 3886.48, "end": 3890.32, "text": " They'll say, they'll just leave off one of those verb phrases. Even with that simple so to say,", "tokens": [51264, 814, 603, 584, 11, 436, 603, 445, 1856, 766, 472, 295, 729, 9595, 20312, 13, 2754, 365, 300, 2199, 370, 281, 584, 11, 51456], "temperature": 0.0, "avg_logprob": -0.16409914681081023, "compression_ratio": 1.662037037037037, "no_speech_prob": 0.0028006385546177626}, {"id": 732, "seek": 389032, "start": 3890.32, "end": 3900.0, "text": " the book which the author who, and they'll say was, you need three verbs, right? I need three", "tokens": [50364, 264, 1446, 597, 264, 3793, 567, 11, 293, 436, 603, 584, 390, 11, 291, 643, 1045, 30051, 11, 558, 30, 286, 643, 1045, 50848], "temperature": 0.0, "avg_logprob": -0.1714763101541771, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.23359866440296173}, {"id": 733, "seek": 389032, "start": 3900.0, "end": 3906.88, "text": " verbs are who I met wrote was good, and they'll give me two. They'll say who was famous was good", "tokens": [50848, 30051, 366, 567, 286, 1131, 4114, 390, 665, 11, 293, 436, 603, 976, 385, 732, 13, 814, 603, 584, 567, 390, 4618, 390, 665, 51192], "temperature": 0.0, "avg_logprob": -0.1714763101541771, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.23359866440296173}, {"id": 734, "seek": 389032, "start": 3906.88, "end": 3912.2400000000002, "text": " or something like that. They'll just give me two. And that'll happen about 60% of the time. So 40%,", "tokens": [51192, 420, 746, 411, 300, 13, 814, 603, 445, 976, 385, 732, 13, 400, 300, 603, 1051, 466, 4060, 4, 295, 264, 565, 13, 407, 3356, 8923, 51460], "temperature": 0.0, "avg_logprob": -0.1714763101541771, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.23359866440296173}, {"id": 735, "seek": 389032, "start": 3912.2400000000002, "end": 3916.96, "text": " maybe 30%, they'll do it correctly, correctly, meaning they'll do a three verb phrase. I don't", "tokens": [51460, 1310, 2217, 8923, 436, 603, 360, 309, 8944, 11, 8944, 11, 3620, 436, 603, 360, 257, 1045, 9595, 9535, 13, 286, 500, 380, 51696], "temperature": 0.0, "avg_logprob": -0.1714763101541771, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.23359866440296173}, {"id": 736, "seek": 391696, "start": 3917.04, "end": 3920.16, "text": " know what's correct or not. This is hard. It's a hard task.", "tokens": [50368, 458, 437, 311, 3006, 420, 406, 13, 639, 307, 1152, 13, 467, 311, 257, 1152, 5633, 13, 50524], "temperature": 0.0, "avg_logprob": -0.16413011356275908, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.015420923940837383}, {"id": 737, "seek": 391696, "start": 3920.16, "end": 3922.2400000000002, "text": " Yeah. I can actually, I'm struggling with it in my head.", "tokens": [50524, 865, 13, 286, 393, 767, 11, 286, 478, 9314, 365, 309, 294, 452, 1378, 13, 50628], "temperature": 0.0, "avg_logprob": -0.16413011356275908, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.015420923940837383}, {"id": 738, "seek": 391696, "start": 3922.2400000000002, "end": 3925.12, "text": " Yeah. Well, it's easier when you look at it.", "tokens": [50628, 865, 13, 1042, 11, 309, 311, 3571, 562, 291, 574, 412, 309, 13, 50772], "temperature": 0.0, "avg_logprob": -0.16413011356275908, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.015420923940837383}, {"id": 739, "seek": 391696, "start": 3925.12, "end": 3928.88, "text": " If you look at it a little easier, then listening is pretty tough because you have to,", "tokens": [50772, 759, 291, 574, 412, 309, 257, 707, 3571, 11, 550, 4764, 307, 1238, 4930, 570, 291, 362, 281, 11, 50960], "temperature": 0.0, "avg_logprob": -0.16413011356275908, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.015420923940837383}, {"id": 740, "seek": 391696, "start": 3928.88, "end": 3933.28, "text": " because there's no trace of it. You have to remember the words that I'm saying,", "tokens": [50960, 570, 456, 311, 572, 13508, 295, 309, 13, 509, 362, 281, 1604, 264, 2283, 300, 286, 478, 1566, 11, 51180], "temperature": 0.0, "avg_logprob": -0.16413011356275908, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.015420923940837383}, {"id": 741, "seek": 391696, "start": 3933.28, "end": 3937.2, "text": " which is very hard auditorily. We wouldn't do it this way. We do it written. You can look at it", "tokens": [51180, 597, 307, 588, 1152, 33970, 953, 13, 492, 2759, 380, 360, 309, 341, 636, 13, 492, 360, 309, 3720, 13, 509, 393, 574, 412, 309, 51376], "temperature": 0.0, "avg_logprob": -0.16413011356275908, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.015420923940837383}, {"id": 742, "seek": 391696, "start": 3937.2, "end": 3941.92, "text": " and figure it out. It's easier in many dimensions in some ways, depending on the person. It's easier", "tokens": [51376, 293, 2573, 309, 484, 13, 467, 311, 3571, 294, 867, 12819, 294, 512, 2098, 11, 5413, 322, 264, 954, 13, 467, 311, 3571, 51612], "temperature": 0.0, "avg_logprob": -0.16413011356275908, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.015420923940837383}, {"id": 743, "seek": 394192, "start": 3941.92, "end": 3947.76, "text": " to gather written data for, I mean, most sort of psycho, I work in psycholinguistics, right?", "tokens": [50364, 281, 5448, 3720, 1412, 337, 11, 286, 914, 11, 881, 1333, 295, 33355, 11, 286, 589, 294, 4681, 401, 7050, 6006, 11, 558, 30, 50656], "temperature": 0.0, "avg_logprob": -0.13419330871856963, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.13646163046360016}, {"id": 744, "seek": 394192, "start": 3947.76, "end": 3952.8, "text": " Psychology of language and stuff. And so a lot of our work is based on written stuff because it's", "tokens": [50656, 42827, 295, 2856, 293, 1507, 13, 400, 370, 257, 688, 295, 527, 589, 307, 2361, 322, 3720, 1507, 570, 309, 311, 50908], "temperature": 0.0, "avg_logprob": -0.13419330871856963, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.13646163046360016}, {"id": 745, "seek": 394192, "start": 3952.8, "end": 3959.6, "text": " so easy to gather data from people doing written kinds of tasks. Spoken tasks are just more complicated", "tokens": [50908, 370, 1858, 281, 5448, 1412, 490, 561, 884, 3720, 3685, 295, 9608, 13, 1738, 8406, 9608, 366, 445, 544, 6179, 51248], "temperature": 0.0, "avg_logprob": -0.13419330871856963, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.13646163046360016}, {"id": 746, "seek": 394192, "start": 3959.6, "end": 3965.2000000000003, "text": " to administer and analyze because people do weird things when they speak. And it's harder to analyze", "tokens": [51248, 281, 22096, 293, 12477, 570, 561, 360, 3657, 721, 562, 436, 1710, 13, 400, 309, 311, 6081, 281, 12477, 51528], "temperature": 0.0, "avg_logprob": -0.13419330871856963, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.13646163046360016}, {"id": 747, "seek": 394192, "start": 3965.2000000000003, "end": 3970.16, "text": " what they do, but they generally point to the same kinds of things.", "tokens": [51528, 437, 436, 360, 11, 457, 436, 5101, 935, 281, 264, 912, 3685, 295, 721, 13, 51776], "temperature": 0.0, "avg_logprob": -0.13419330871856963, "compression_ratio": 1.7471698113207548, "no_speech_prob": 0.13646163046360016}, {"id": 748, "seek": 397016, "start": 3970.16, "end": 3977.92, "text": " It's okay. So the universal theory of language by Ted Gibson is that you can form dependency,", "tokens": [50364, 467, 311, 1392, 13, 407, 264, 11455, 5261, 295, 2856, 538, 14985, 42250, 307, 300, 291, 393, 1254, 33621, 11, 50752], "temperature": 0.0, "avg_logprob": -0.15617445038586128, "compression_ratio": 1.9267015706806283, "no_speech_prob": 0.001500366604886949}, {"id": 749, "seek": 397016, "start": 3979.12, "end": 3984.0, "text": " you can form trees from any senses, and you can measure the distance in some way of those", "tokens": [50812, 291, 393, 1254, 5852, 490, 604, 17057, 11, 293, 291, 393, 3481, 264, 4560, 294, 512, 636, 295, 729, 51056], "temperature": 0.0, "avg_logprob": -0.15617445038586128, "compression_ratio": 1.9267015706806283, "no_speech_prob": 0.001500366604886949}, {"id": 750, "seek": 397016, "start": 3984.0, "end": 3990.56, "text": " dependencies. And then you can say that most languages have very short dependencies.", "tokens": [51056, 36606, 13, 400, 550, 291, 393, 584, 300, 881, 8650, 362, 588, 2099, 36606, 13, 51384], "temperature": 0.0, "avg_logprob": -0.15617445038586128, "compression_ratio": 1.9267015706806283, "no_speech_prob": 0.001500366604886949}, {"id": 751, "seek": 397016, "start": 3990.56, "end": 3994.72, "text": " All languages. All languages. All languages have short dependencies. You can actually measure that.", "tokens": [51384, 1057, 8650, 13, 1057, 8650, 13, 1057, 8650, 362, 2099, 36606, 13, 509, 393, 767, 3481, 300, 13, 51592], "temperature": 0.0, "avg_logprob": -0.15617445038586128, "compression_ratio": 1.9267015706806283, "no_speech_prob": 0.001500366604886949}, {"id": 752, "seek": 399472, "start": 3994.72, "end": 4000.08, "text": " So an ex-student of mine, this guy is at University of California, Irvine. Richard", "tokens": [50364, 407, 364, 454, 12, 372, 24064, 295, 3892, 11, 341, 2146, 307, 412, 3535, 295, 5384, 11, 9151, 41243, 13, 9809, 50632], "temperature": 0.0, "avg_logprob": -0.12564254018057763, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.0035930031444877386}, {"id": 753, "seek": 399472, "start": 4000.08, "end": 4005.52, "text": " Futrell did a thing a bunch of years ago now where he looked at all the languages we could", "tokens": [50632, 16569, 19771, 630, 257, 551, 257, 3840, 295, 924, 2057, 586, 689, 415, 2956, 412, 439, 264, 8650, 321, 727, 50904], "temperature": 0.0, "avg_logprob": -0.12564254018057763, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.0035930031444877386}, {"id": 754, "seek": 399472, "start": 4005.52, "end": 4010.7999999999997, "text": " look at, which was about 40 initially. And now I think there's about 60 for which there are", "tokens": [50904, 574, 412, 11, 597, 390, 466, 3356, 9105, 13, 400, 586, 286, 519, 456, 311, 466, 4060, 337, 597, 456, 366, 51168], "temperature": 0.0, "avg_logprob": -0.12564254018057763, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.0035930031444877386}, {"id": 755, "seek": 399472, "start": 4010.7999999999997, "end": 4015.68, "text": " dependency structures. So they're meaning there's got to be like a big text, a bunch of texts,", "tokens": [51168, 33621, 9227, 13, 407, 436, 434, 3620, 456, 311, 658, 281, 312, 411, 257, 955, 2487, 11, 257, 3840, 295, 15765, 11, 51412], "temperature": 0.0, "avg_logprob": -0.12564254018057763, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.0035930031444877386}, {"id": 756, "seek": 399472, "start": 4015.68, "end": 4019.7599999999998, "text": " which have been parsed for the dependency structures. And there's about 60 of those", "tokens": [51412, 597, 362, 668, 21156, 292, 337, 264, 33621, 9227, 13, 400, 456, 311, 466, 4060, 295, 729, 51616], "temperature": 0.0, "avg_logprob": -0.12564254018057763, "compression_ratio": 1.7011494252873562, "no_speech_prob": 0.0035930031444877386}, {"id": 757, "seek": 401976, "start": 4019.76, "end": 4025.92, "text": " which have been parsed that way. And for all of those, what he did was take any", "tokens": [50364, 597, 362, 668, 21156, 292, 300, 636, 13, 400, 337, 439, 295, 729, 11, 437, 415, 630, 390, 747, 604, 50672], "temperature": 0.0, "avg_logprob": -0.1375771851098838, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.025949880480766296}, {"id": 758, "seek": 401976, "start": 4027.0400000000004, "end": 4031.36, "text": " sentence in one of those languages, and you can do the dependency structure, and then start at", "tokens": [50728, 8174, 294, 472, 295, 729, 8650, 11, 293, 291, 393, 360, 264, 33621, 3877, 11, 293, 550, 722, 412, 50944], "temperature": 0.0, "avg_logprob": -0.1375771851098838, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.025949880480766296}, {"id": 759, "seek": 401976, "start": 4031.36, "end": 4035.2000000000003, "text": " the root. We were talking about dependency structures. That's pretty easy now. And he's", "tokens": [50944, 264, 5593, 13, 492, 645, 1417, 466, 33621, 9227, 13, 663, 311, 1238, 1858, 586, 13, 400, 415, 311, 51136], "temperature": 0.0, "avg_logprob": -0.1375771851098838, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.025949880480766296}, {"id": 760, "seek": 401976, "start": 4035.2000000000003, "end": 4041.28, "text": " trying to figure out what a control way you might say the same sentence is in that language. And so", "tokens": [51136, 1382, 281, 2573, 484, 437, 257, 1969, 636, 291, 1062, 584, 264, 912, 8174, 307, 294, 300, 2856, 13, 400, 370, 51440], "temperature": 0.0, "avg_logprob": -0.1375771851098838, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.025949880480766296}, {"id": 761, "seek": 401976, "start": 4041.28, "end": 4046.6400000000003, "text": " what he does is just like, all right, there's a root, and say as a sentence is, let's go back to,", "tokens": [51440, 437, 415, 775, 307, 445, 411, 11, 439, 558, 11, 456, 311, 257, 5593, 11, 293, 584, 382, 257, 8174, 307, 11, 718, 311, 352, 646, 281, 11, 51708], "temperature": 0.0, "avg_logprob": -0.1375771851098838, "compression_ratio": 1.7898832684824904, "no_speech_prob": 0.025949880480766296}, {"id": 762, "seek": 404664, "start": 4046.8799999999997, "end": 4052.8799999999997, "text": " two dogs entered the room. So entered is the root. And entered has two dependents that's got", "tokens": [50376, 732, 7197, 9065, 264, 1808, 13, 407, 9065, 307, 264, 5593, 13, 400, 9065, 575, 732, 5672, 791, 300, 311, 658, 50676], "temperature": 0.0, "avg_logprob": -0.1539805254598302, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.01242924202233553}, {"id": 763, "seek": 404664, "start": 4052.8799999999997, "end": 4058.3199999999997, "text": " dogs, and it has room. And what he does is like, let's scramble that order. That's three things,", "tokens": [50676, 7197, 11, 293, 309, 575, 1808, 13, 400, 437, 415, 775, 307, 411, 11, 718, 311, 795, 48382, 300, 1668, 13, 663, 311, 1045, 721, 11, 50948], "temperature": 0.0, "avg_logprob": -0.1539805254598302, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.01242924202233553}, {"id": 764, "seek": 404664, "start": 4058.3199999999997, "end": 4063.68, "text": " the root, and the head, and the two dependents, and into some random order, just random. And then", "tokens": [50948, 264, 5593, 11, 293, 264, 1378, 11, 293, 264, 732, 5672, 791, 11, 293, 666, 512, 4974, 1668, 11, 445, 4974, 13, 400, 550, 51216], "temperature": 0.0, "avg_logprob": -0.1539805254598302, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.01242924202233553}, {"id": 765, "seek": 404664, "start": 4063.68, "end": 4068.3199999999997, "text": " just do that for all the dependents down the two. So now look, do it for the whatever was two in", "tokens": [51216, 445, 360, 300, 337, 439, 264, 5672, 791, 760, 264, 732, 13, 407, 586, 574, 11, 360, 309, 337, 264, 2035, 390, 732, 294, 51448], "temperature": 0.0, "avg_logprob": -0.1539805254598302, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.01242924202233553}, {"id": 766, "seek": 404664, "start": 4068.3199999999997, "end": 4074.0, "text": " dogs and for in room. And that's not a very short sentence. When sentences get longer,", "tokens": [51448, 7197, 293, 337, 294, 1808, 13, 400, 300, 311, 406, 257, 588, 2099, 8174, 13, 1133, 16579, 483, 2854, 11, 51732], "temperature": 0.0, "avg_logprob": -0.1539805254598302, "compression_ratio": 1.9873417721518987, "no_speech_prob": 0.01242924202233553}, {"id": 767, "seek": 407400, "start": 4074.0, "end": 4078.72, "text": " and you have more dependents, there's more scrambling that's possible. And what he found,", "tokens": [50364, 293, 291, 362, 544, 5672, 791, 11, 456, 311, 544, 5918, 19391, 300, 311, 1944, 13, 400, 437, 415, 1352, 11, 50600], "temperature": 0.0, "avg_logprob": -0.18387038672148293, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0024719671346247196}, {"id": 768, "seek": 407400, "start": 4078.72, "end": 4082.56, "text": " what so that so so that that's one, you can figure out one scrambling for that sentence,", "tokens": [50600, 437, 370, 300, 370, 370, 300, 300, 311, 472, 11, 291, 393, 2573, 484, 472, 5918, 19391, 337, 300, 8174, 11, 50792], "temperature": 0.0, "avg_logprob": -0.18387038672148293, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0024719671346247196}, {"id": 769, "seek": 407400, "start": 4082.56, "end": 4087.6, "text": " he did like a hundred times for every sentence and every corp and every one of these texts,", "tokens": [50792, 415, 630, 411, 257, 3262, 1413, 337, 633, 8174, 293, 633, 1181, 79, 293, 633, 472, 295, 613, 15765, 11, 51044], "temperature": 0.0, "avg_logprob": -0.18387038672148293, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0024719671346247196}, {"id": 770, "seek": 407400, "start": 4087.6, "end": 4093.76, "text": " every corpus. And then he just compared the dependency lengths in those random scramblings", "tokens": [51044, 633, 1181, 31624, 13, 400, 550, 415, 445, 5347, 264, 33621, 26329, 294, 729, 4974, 5918, 2173, 20823, 51352], "temperature": 0.0, "avg_logprob": -0.18387038672148293, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0024719671346247196}, {"id": 771, "seek": 407400, "start": 4093.76, "end": 4099.12, "text": " to what actually happened with what the English or the French or the German was in the original", "tokens": [51352, 281, 437, 767, 2011, 365, 437, 264, 3669, 420, 264, 5522, 420, 264, 6521, 390, 294, 264, 3380, 51620], "temperature": 0.0, "avg_logprob": -0.18387038672148293, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0024719671346247196}, {"id": 772, "seek": 407400, "start": 4099.12, "end": 4103.04, "text": " language or Chinese or what all these like 80, like, you know, 60 languages, okay. And the", "tokens": [51620, 2856, 420, 4649, 420, 437, 439, 613, 411, 4688, 11, 411, 11, 291, 458, 11, 4060, 8650, 11, 1392, 13, 400, 264, 51816], "temperature": 0.0, "avg_logprob": -0.18387038672148293, "compression_ratio": 1.870307167235495, "no_speech_prob": 0.0024719671346247196}, {"id": 773, "seek": 410304, "start": 4103.04, "end": 4108.08, "text": " dependency lengths are always shorter in the real language compared to this kind of a control.", "tokens": [50364, 33621, 26329, 366, 1009, 11639, 294, 264, 957, 2856, 5347, 281, 341, 733, 295, 257, 1969, 13, 50616], "temperature": 0.0, "avg_logprob": -0.14921212396701844, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0003920051676686853}, {"id": 774, "seek": 410304, "start": 4108.08, "end": 4116.96, "text": " And there's another, it's a little more rigid his control. So the way I described it, you could", "tokens": [50616, 400, 456, 311, 1071, 11, 309, 311, 257, 707, 544, 22195, 702, 1969, 13, 407, 264, 636, 286, 7619, 309, 11, 291, 727, 51060], "temperature": 0.0, "avg_logprob": -0.14921212396701844, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0003920051676686853}, {"id": 775, "seek": 410304, "start": 4116.96, "end": 4121.68, "text": " have crossed dependencies, like that by scrambling that way, you could scramble in any way at all.", "tokens": [51060, 362, 14622, 36606, 11, 411, 300, 538, 5918, 19391, 300, 636, 11, 291, 727, 795, 48382, 294, 604, 636, 412, 439, 13, 51296], "temperature": 0.0, "avg_logprob": -0.14921212396701844, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0003920051676686853}, {"id": 776, "seek": 410304, "start": 4122.56, "end": 4127.2, "text": " Languages don't do that. They tend not to cross dependencies very much. Like so the dependency", "tokens": [51340, 13313, 84, 1660, 500, 380, 360, 300, 13, 814, 3928, 406, 281, 3278, 36606, 588, 709, 13, 1743, 370, 264, 33621, 51572], "temperature": 0.0, "avg_logprob": -0.14921212396701844, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0003920051676686853}, {"id": 777, "seek": 410304, "start": 4127.2, "end": 4131.92, "text": " structure, they just they tend to keep things non-crossed. And there's a, you know, like,", "tokens": [51572, 3877, 11, 436, 445, 436, 3928, 281, 1066, 721, 2107, 12, 35418, 292, 13, 400, 456, 311, 257, 11, 291, 458, 11, 411, 11, 51808], "temperature": 0.0, "avg_logprob": -0.14921212396701844, "compression_ratio": 1.7686567164179106, "no_speech_prob": 0.0003920051676686853}, {"id": 778, "seek": 413192, "start": 4131.92, "end": 4135.4400000000005, "text": " there's a technical term they call that projective, but it's just non-crossed is all that is", "tokens": [50364, 456, 311, 257, 6191, 1433, 436, 818, 300, 1716, 488, 11, 457, 309, 311, 445, 2107, 12, 35418, 292, 307, 439, 300, 307, 50540], "temperature": 0.0, "avg_logprob": -0.12551383972167968, "compression_ratio": 1.9312714776632303, "no_speech_prob": 0.0007552940514869988}, {"id": 779, "seek": 413192, "start": 4135.4400000000005, "end": 4141.52, "text": " projective. And so if you just constrain the scrambling so that it only gives you projectives", "tokens": [50540, 1716, 488, 13, 400, 370, 498, 291, 445, 1817, 7146, 264, 5918, 19391, 370, 300, 309, 787, 2709, 291, 1716, 1539, 50844], "temperature": 0.0, "avg_logprob": -0.12551383972167968, "compression_ratio": 1.9312714776632303, "no_speech_prob": 0.0007552940514869988}, {"id": 780, "seek": 413192, "start": 4141.52, "end": 4146.56, "text": " sort of non-crossed is the same thing holds. So it's so the you still still human languages are", "tokens": [50844, 1333, 295, 2107, 12, 35418, 292, 307, 264, 912, 551, 9190, 13, 407, 309, 311, 370, 264, 291, 920, 920, 1952, 8650, 366, 51096], "temperature": 0.0, "avg_logprob": -0.12551383972167968, "compression_ratio": 1.9312714776632303, "no_speech_prob": 0.0007552940514869988}, {"id": 781, "seek": 413192, "start": 4146.56, "end": 4152.8, "text": " much shorter than these this kind of a control. So there's like, what it means is that that", "tokens": [51096, 709, 11639, 813, 613, 341, 733, 295, 257, 1969, 13, 407, 456, 311, 411, 11, 437, 309, 1355, 307, 300, 300, 51408], "temperature": 0.0, "avg_logprob": -0.12551383972167968, "compression_ratio": 1.9312714776632303, "no_speech_prob": 0.0007552940514869988}, {"id": 782, "seek": 413192, "start": 4152.8, "end": 4158.4800000000005, "text": " we're in every language, we're trying to put things close in relative to this kind of a control,", "tokens": [51408, 321, 434, 294, 633, 2856, 11, 321, 434, 1382, 281, 829, 721, 1998, 294, 4972, 281, 341, 733, 295, 257, 1969, 11, 51692], "temperature": 0.0, "avg_logprob": -0.12551383972167968, "compression_ratio": 1.9312714776632303, "no_speech_prob": 0.0007552940514869988}, {"id": 783, "seek": 413192, "start": 4158.4800000000005, "end": 4161.68, "text": " like it doesn't matter about the word order, some of these are verb final, some of them is", "tokens": [51692, 411, 309, 1177, 380, 1871, 466, 264, 1349, 1668, 11, 512, 295, 613, 366, 9595, 2572, 11, 512, 295, 552, 307, 51852], "temperature": 0.0, "avg_logprob": -0.12551383972167968, "compression_ratio": 1.9312714776632303, "no_speech_prob": 0.0007552940514869988}, {"id": 784, "seek": 416168, "start": 4161.68, "end": 4165.84, "text": " a verb, media-like English, and some are even verb initial. There are a few languages in the", "tokens": [50364, 257, 9595, 11, 3021, 12, 4092, 3669, 11, 293, 512, 366, 754, 9595, 5883, 13, 821, 366, 257, 1326, 8650, 294, 264, 50572], "temperature": 0.0, "avg_logprob": -0.25813547770182294, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004172546323388815}, {"id": 785, "seek": 416168, "start": 4165.84, "end": 4171.6, "text": " world which have VSO, word order, verb, subject, object languages, haven't talked about those.", "tokens": [50572, 1002, 597, 362, 691, 17188, 11, 1349, 1668, 11, 9595, 11, 3983, 11, 2657, 8650, 11, 2378, 380, 2825, 466, 729, 13, 50860], "temperature": 0.0, "avg_logprob": -0.25813547770182294, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004172546323388815}, {"id": 786, "seek": 416168, "start": 4171.6, "end": 4177.360000000001, "text": " It's like 10% of the and even even in those languages, it's still short dependencies.", "tokens": [50860, 467, 311, 411, 1266, 4, 295, 264, 293, 754, 754, 294, 729, 8650, 11, 309, 311, 920, 2099, 36606, 13, 51148], "temperature": 0.0, "avg_logprob": -0.25813547770182294, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004172546323388815}, {"id": 787, "seek": 416168, "start": 4177.360000000001, "end": 4182.72, "text": " Short dependencies is rules. Okay. So how what, what are some possible explanations for that?", "tokens": [51148, 16881, 36606, 307, 4474, 13, 1033, 13, 407, 577, 437, 11, 437, 366, 512, 1944, 28708, 337, 300, 30, 51416], "temperature": 0.0, "avg_logprob": -0.25813547770182294, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004172546323388815}, {"id": 788, "seek": 416168, "start": 4183.92, "end": 4188.0, "text": " For why languages have evolved that way? So that that's one of the,", "tokens": [51476, 1171, 983, 8650, 362, 14178, 300, 636, 30, 407, 300, 300, 311, 472, 295, 264, 11, 51680], "temperature": 0.0, "avg_logprob": -0.25813547770182294, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004172546323388815}, {"id": 789, "seek": 418800, "start": 4188.88, "end": 4194.64, "text": " I suppose disagreements you might have with Chomsky. So you consider the evolution of language", "tokens": [50408, 286, 7297, 23926, 6400, 291, 1062, 362, 365, 761, 4785, 4133, 13, 407, 291, 1949, 264, 9303, 295, 2856, 50696], "temperature": 0.0, "avg_logprob": -0.22231732244076935, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.000939793128054589}, {"id": 790, "seek": 418800, "start": 4194.64, "end": 4204.0, "text": " in terms of information theory. And for you, the purpose of language is ease of communication,", "tokens": [50696, 294, 2115, 295, 1589, 5261, 13, 400, 337, 291, 11, 264, 4334, 295, 2856, 307, 12708, 295, 6101, 11, 51164], "temperature": 0.0, "avg_logprob": -0.22231732244076935, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.000939793128054589}, {"id": 791, "seek": 418800, "start": 4204.0, "end": 4208.24, "text": " right, in processing. That's right. That's right. So I mean, the story here is just about", "tokens": [51164, 558, 11, 294, 9007, 13, 663, 311, 558, 13, 663, 311, 558, 13, 407, 286, 914, 11, 264, 1657, 510, 307, 445, 466, 51376], "temperature": 0.0, "avg_logprob": -0.22231732244076935, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.000939793128054589}, {"id": 792, "seek": 418800, "start": 4208.8, "end": 4213.76, "text": " communication. It is just about production really. It's about ease of production is the story.", "tokens": [51404, 6101, 13, 467, 307, 445, 466, 4265, 534, 13, 467, 311, 466, 12708, 295, 4265, 307, 264, 1657, 13, 51652], "temperature": 0.0, "avg_logprob": -0.22231732244076935, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.000939793128054589}, {"id": 793, "seek": 418800, "start": 4213.76, "end": 4217.6, "text": " When you say production, can you? Oh, I just mean each of language production. It's easier for me", "tokens": [51652, 1133, 291, 584, 4265, 11, 393, 291, 30, 876, 11, 286, 445, 914, 1184, 295, 2856, 4265, 13, 467, 311, 3571, 337, 385, 51844], "temperature": 0.0, "avg_logprob": -0.22231732244076935, "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.000939793128054589}, {"id": 794, "seek": 421760, "start": 4217.6, "end": 4223.52, "text": " to say things when the, when I'm doing, whenever I'm talking to you is somehow I'm formulating", "tokens": [50364, 281, 584, 721, 562, 264, 11, 562, 286, 478, 884, 11, 5699, 286, 478, 1417, 281, 291, 307, 6063, 286, 478, 1254, 12162, 50660], "temperature": 0.0, "avg_logprob": -0.1331625053350874, "compression_ratio": 1.8650519031141868, "no_speech_prob": 0.004903492052108049}, {"id": 795, "seek": 421760, "start": 4223.52, "end": 4227.6, "text": " some idea in my head and I'm putting these words together. And it's easier for me to do that,", "tokens": [50660, 512, 1558, 294, 452, 1378, 293, 286, 478, 3372, 613, 2283, 1214, 13, 400, 309, 311, 3571, 337, 385, 281, 360, 300, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1331625053350874, "compression_ratio": 1.8650519031141868, "no_speech_prob": 0.004903492052108049}, {"id": 796, "seek": 421760, "start": 4229.04, "end": 4233.280000000001, "text": " to put, to say something where the words are close, closely connected in a dependency,", "tokens": [50936, 281, 829, 11, 281, 584, 746, 689, 264, 2283, 366, 1998, 11, 8185, 4582, 294, 257, 33621, 11, 51148], "temperature": 0.0, "avg_logprob": -0.1331625053350874, "compression_ratio": 1.8650519031141868, "no_speech_prob": 0.004903492052108049}, {"id": 797, "seek": 421760, "start": 4233.280000000001, "end": 4238.4800000000005, "text": " as opposed to separated, like by putting something in between and over and over again.", "tokens": [51148, 382, 8851, 281, 12005, 11, 411, 538, 3372, 746, 294, 1296, 293, 670, 293, 670, 797, 13, 51408], "temperature": 0.0, "avg_logprob": -0.1331625053350874, "compression_ratio": 1.8650519031141868, "no_speech_prob": 0.004903492052108049}, {"id": 798, "seek": 421760, "start": 4238.4800000000005, "end": 4242.160000000001, "text": " It's just hard for me to keep that in my head. It like, that's, that's the whole story.", "tokens": [51408, 467, 311, 445, 1152, 337, 385, 281, 1066, 300, 294, 452, 1378, 13, 467, 411, 11, 300, 311, 11, 300, 311, 264, 1379, 1657, 13, 51592], "temperature": 0.0, "avg_logprob": -0.1331625053350874, "compression_ratio": 1.8650519031141868, "no_speech_prob": 0.004903492052108049}, {"id": 799, "seek": 421760, "start": 4242.160000000001, "end": 4246.320000000001, "text": " Like the story is basically, it's like the dependency grammar sort of gives that to you.", "tokens": [51592, 1743, 264, 1657, 307, 1936, 11, 309, 311, 411, 264, 33621, 22317, 1333, 295, 2709, 300, 281, 291, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1331625053350874, "compression_ratio": 1.8650519031141868, "no_speech_prob": 0.004903492052108049}, {"id": 800, "seek": 424632, "start": 4246.4, "end": 4251.2, "text": " Like just like long, long as bad, short as good. It's like easier to keep in mind because you", "tokens": [50368, 1743, 445, 411, 938, 11, 938, 382, 1578, 11, 2099, 382, 665, 13, 467, 311, 411, 3571, 281, 1066, 294, 1575, 570, 291, 50608], "temperature": 0.0, "avg_logprob": -0.16063356399536133, "compression_ratio": 1.9075342465753424, "no_speech_prob": 0.001454753684811294}, {"id": 801, "seek": 424632, "start": 4251.2, "end": 4256.4, "text": " have to keep it in mind for probably for production, probably matters in comprehension", "tokens": [50608, 362, 281, 1066, 309, 294, 1575, 337, 1391, 337, 4265, 11, 1391, 7001, 294, 44991, 50868], "temperature": 0.0, "avg_logprob": -0.16063356399536133, "compression_ratio": 1.9075342465753424, "no_speech_prob": 0.001454753684811294}, {"id": 802, "seek": 424632, "start": 4256.4, "end": 4260.24, "text": " as well. Like also matters in comprehension. It's on both sides of it. The production and the,", "tokens": [50868, 382, 731, 13, 1743, 611, 7001, 294, 44991, 13, 467, 311, 322, 1293, 4881, 295, 309, 13, 440, 4265, 293, 264, 11, 51060], "temperature": 0.0, "avg_logprob": -0.16063356399536133, "compression_ratio": 1.9075342465753424, "no_speech_prob": 0.001454753684811294}, {"id": 803, "seek": 424632, "start": 4260.24, "end": 4264.0, "text": " but I would guess it's probably evolved for production. It's about producing. It's what's", "tokens": [51060, 457, 286, 576, 2041, 309, 311, 1391, 14178, 337, 4265, 13, 467, 311, 466, 10501, 13, 467, 311, 437, 311, 51248], "temperature": 0.0, "avg_logprob": -0.16063356399536133, "compression_ratio": 1.9075342465753424, "no_speech_prob": 0.001454753684811294}, {"id": 804, "seek": 424632, "start": 4264.0, "end": 4269.599999999999, "text": " easier for me to say that ends up being easier for you also. And that's very hard to disentangle", "tokens": [51248, 3571, 337, 385, 281, 584, 300, 5314, 493, 885, 3571, 337, 291, 611, 13, 400, 300, 311, 588, 1152, 281, 37313, 7846, 51528], "temperature": 0.0, "avg_logprob": -0.16063356399536133, "compression_ratio": 1.9075342465753424, "no_speech_prob": 0.001454753684811294}, {"id": 805, "seek": 424632, "start": 4269.599999999999, "end": 4274.4, "text": " this idea of who is it for? Is it for me, the speaker, or is it for you, the listener? I mean,", "tokens": [51528, 341, 1558, 295, 567, 307, 309, 337, 30, 1119, 309, 337, 385, 11, 264, 8145, 11, 420, 307, 309, 337, 291, 11, 264, 31569, 30, 286, 914, 11, 51768], "temperature": 0.0, "avg_logprob": -0.16063356399536133, "compression_ratio": 1.9075342465753424, "no_speech_prob": 0.001454753684811294}, {"id": 806, "seek": 427440, "start": 4274.48, "end": 4279.28, "text": " part of my language is for you. Like the way I talk to you is going to be different", "tokens": [50368, 644, 295, 452, 2856, 307, 337, 291, 13, 1743, 264, 636, 286, 751, 281, 291, 307, 516, 281, 312, 819, 50608], "temperature": 0.0, "avg_logprob": -0.0870581899370466, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00348272523842752}, {"id": 807, "seek": 427440, "start": 4279.28, "end": 4283.28, "text": " from how I talk to different people. So I'm, I'm definitely angling what I'm saying", "tokens": [50608, 490, 577, 286, 751, 281, 819, 561, 13, 407, 286, 478, 11, 286, 478, 2138, 2562, 1688, 437, 286, 478, 1566, 50808], "temperature": 0.0, "avg_logprob": -0.0870581899370466, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00348272523842752}, {"id": 808, "seek": 427440, "start": 4283.28, "end": 4287.839999999999, "text": " to who I'm saying, right? It's not like I'm just talking the same way to every single person.", "tokens": [50808, 281, 567, 286, 478, 1566, 11, 558, 30, 467, 311, 406, 411, 286, 478, 445, 1417, 264, 912, 636, 281, 633, 2167, 954, 13, 51036], "temperature": 0.0, "avg_logprob": -0.0870581899370466, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00348272523842752}, {"id": 809, "seek": 427440, "start": 4287.839999999999, "end": 4293.839999999999, "text": " And so I am sensitive to my audience, but how does that, does that, you know,", "tokens": [51036, 400, 370, 286, 669, 9477, 281, 452, 4034, 11, 457, 577, 775, 300, 11, 775, 300, 11, 291, 458, 11, 51336], "temperature": 0.0, "avg_logprob": -0.0870581899370466, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00348272523842752}, {"id": 810, "seek": 427440, "start": 4293.839999999999, "end": 4298.48, "text": " work itself out in the, in the dependency link differences? I don't know. Maybe that's about", "tokens": [51336, 589, 2564, 484, 294, 264, 11, 294, 264, 33621, 2113, 7300, 30, 286, 500, 380, 458, 13, 2704, 300, 311, 466, 51568], "temperature": 0.0, "avg_logprob": -0.0870581899370466, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00348272523842752}, {"id": 811, "seek": 427440, "start": 4298.48, "end": 4302.48, "text": " just the words, that part, you know, which words I select. My initial intuition is that", "tokens": [51568, 445, 264, 2283, 11, 300, 644, 11, 291, 458, 11, 597, 2283, 286, 3048, 13, 1222, 5883, 24002, 307, 300, 51768], "temperature": 0.0, "avg_logprob": -0.0870581899370466, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.00348272523842752}, {"id": 812, "seek": 430248, "start": 4303.44, "end": 4309.28, "text": " you optimize language for the audience. Yeah. But it's just kind of like messing with my head", "tokens": [50412, 291, 19719, 2856, 337, 264, 4034, 13, 865, 13, 583, 309, 311, 445, 733, 295, 411, 23258, 365, 452, 1378, 50704], "temperature": 0.0, "avg_logprob": -0.14370300233826155, "compression_ratio": 1.7, "no_speech_prob": 0.0032725867349654436}, {"id": 813, "seek": 430248, "start": 4309.28, "end": 4314.48, "text": " a little bit to say that some of the optimization might be, or it may be the primary objective,", "tokens": [50704, 257, 707, 857, 281, 584, 300, 512, 295, 264, 19618, 1062, 312, 11, 420, 309, 815, 312, 264, 6194, 10024, 11, 50964], "temperature": 0.0, "avg_logprob": -0.14370300233826155, "compression_ratio": 1.7, "no_speech_prob": 0.0032725867349654436}, {"id": 814, "seek": 430248, "start": 4314.48, "end": 4320.0, "text": " the optimization might be the ease of production. We have different senses, I guess. I'm, I'm like,", "tokens": [50964, 264, 19618, 1062, 312, 264, 12708, 295, 4265, 13, 492, 362, 819, 17057, 11, 286, 2041, 13, 286, 478, 11, 286, 478, 411, 11, 51240], "temperature": 0.0, "avg_logprob": -0.14370300233826155, "compression_ratio": 1.7, "no_speech_prob": 0.0032725867349654436}, {"id": 815, "seek": 430248, "start": 4320.0, "end": 4324.639999999999, "text": " very selfish and you're like, I think it's like, it's all about me. I'm like,", "tokens": [51240, 588, 19074, 293, 291, 434, 411, 11, 286, 519, 309, 311, 411, 11, 309, 311, 439, 466, 385, 13, 286, 478, 411, 11, 51472], "temperature": 0.0, "avg_logprob": -0.14370300233826155, "compression_ratio": 1.7, "no_speech_prob": 0.0032725867349654436}, {"id": 816, "seek": 430248, "start": 4324.639999999999, "end": 4328.879999999999, "text": " I'm just doing what's easiest for me at all times. I don't want to, I'm like, I'll, I mean,", "tokens": [51472, 286, 478, 445, 884, 437, 311, 12889, 337, 385, 412, 439, 1413, 13, 286, 500, 380, 528, 281, 11, 286, 478, 411, 11, 286, 603, 11, 286, 914, 11, 51684], "temperature": 0.0, "avg_logprob": -0.14370300233826155, "compression_ratio": 1.7, "no_speech_prob": 0.0032725867349654436}, {"id": 817, "seek": 432888, "start": 4328.88, "end": 4333.68, "text": " but I have to, of course, choose the words that I think you're going to know. I'm not going to", "tokens": [50364, 457, 286, 362, 281, 11, 295, 1164, 11, 2826, 264, 2283, 300, 286, 519, 291, 434, 516, 281, 458, 13, 286, 478, 406, 516, 281, 50604], "temperature": 0.0, "avg_logprob": -0.11172403779419714, "compression_ratio": 1.9112627986348123, "no_speech_prob": 0.005218592006713152}, {"id": 818, "seek": 432888, "start": 4333.68, "end": 4337.28, "text": " choose words you don't know. In fact, I'm going to fix that when I, you know, so there it's about,", "tokens": [50604, 2826, 2283, 291, 500, 380, 458, 13, 682, 1186, 11, 286, 478, 516, 281, 3191, 300, 562, 286, 11, 291, 458, 11, 370, 456, 309, 311, 466, 11, 50784], "temperature": 0.0, "avg_logprob": -0.11172403779419714, "compression_ratio": 1.9112627986348123, "no_speech_prob": 0.005218592006713152}, {"id": 819, "seek": 432888, "start": 4337.28, "end": 4342.8, "text": " but, but maybe for, for the syntax, for the combinations, it's just about me. I feel like", "tokens": [50784, 457, 11, 457, 1310, 337, 11, 337, 264, 28431, 11, 337, 264, 21267, 11, 309, 311, 445, 466, 385, 13, 286, 841, 411, 51060], "temperature": 0.0, "avg_logprob": -0.11172403779419714, "compression_ratio": 1.9112627986348123, "no_speech_prob": 0.005218592006713152}, {"id": 820, "seek": 432888, "start": 4342.8, "end": 4345.4400000000005, "text": " it's, I don't know though, it's great. Wait, wait, wait, wait, wait, but the purpose of", "tokens": [51060, 309, 311, 11, 286, 500, 380, 458, 1673, 11, 309, 311, 869, 13, 3802, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 457, 264, 4334, 295, 51192], "temperature": 0.0, "avg_logprob": -0.11172403779419714, "compression_ratio": 1.9112627986348123, "no_speech_prob": 0.005218592006713152}, {"id": 821, "seek": 432888, "start": 4345.4400000000005, "end": 4350.72, "text": " communication is to be understood, is to convince others and so on. So like the selfish things to", "tokens": [51192, 6101, 307, 281, 312, 7320, 11, 307, 281, 13447, 2357, 293, 370, 322, 13, 407, 411, 264, 19074, 721, 281, 51456], "temperature": 0.0, "avg_logprob": -0.11172403779419714, "compression_ratio": 1.9112627986348123, "no_speech_prob": 0.005218592006713152}, {"id": 822, "seek": 432888, "start": 4350.72, "end": 4354.400000000001, "text": " be understood. Okay. It's about the listener. It's a little circular there too then. Okay.", "tokens": [51456, 312, 7320, 13, 1033, 13, 467, 311, 466, 264, 31569, 13, 467, 311, 257, 707, 16476, 456, 886, 550, 13, 1033, 13, 51640], "temperature": 0.0, "avg_logprob": -0.11172403779419714, "compression_ratio": 1.9112627986348123, "no_speech_prob": 0.005218592006713152}, {"id": 823, "seek": 435440, "start": 4354.4, "end": 4359.04, "text": " Right. I mean, like the ease of production helps me be understood then.", "tokens": [50364, 1779, 13, 286, 914, 11, 411, 264, 12708, 295, 4265, 3665, 385, 312, 7320, 550, 13, 50596], "temperature": 0.0, "avg_logprob": -0.15917655097113714, "compression_ratio": 1.9028776978417266, "no_speech_prob": 0.007814846932888031}, {"id": 824, "seek": 435440, "start": 4360.4, "end": 4364.879999999999, "text": " I don't think it's circular. So I think the primary, I think the primary objective is to be", "tokens": [50664, 286, 500, 380, 519, 309, 311, 16476, 13, 407, 286, 519, 264, 6194, 11, 286, 519, 264, 6194, 10024, 307, 281, 312, 50888], "temperature": 0.0, "avg_logprob": -0.15917655097113714, "compression_ratio": 1.9028776978417266, "no_speech_prob": 0.007814846932888031}, {"id": 825, "seek": 435440, "start": 4364.879999999999, "end": 4370.32, "text": " understood is about the listener. Cause otherwise the, if you're optimizing for the ease of production,", "tokens": [50888, 7320, 307, 466, 264, 31569, 13, 10865, 5911, 264, 11, 498, 291, 434, 40425, 337, 264, 12708, 295, 4265, 11, 51160], "temperature": 0.0, "avg_logprob": -0.15917655097113714, "compression_ratio": 1.9028776978417266, "no_speech_prob": 0.007814846932888031}, {"id": 826, "seek": 435440, "start": 4370.32, "end": 4374.5599999999995, "text": " then you're, you're not going to have any of the interesting complexity of language. Like", "tokens": [51160, 550, 291, 434, 11, 291, 434, 406, 516, 281, 362, 604, 295, 264, 1880, 14024, 295, 2856, 13, 1743, 51372], "temperature": 0.0, "avg_logprob": -0.15917655097113714, "compression_ratio": 1.9028776978417266, "no_speech_prob": 0.007814846932888031}, {"id": 827, "seek": 435440, "start": 4374.5599999999995, "end": 4378.08, "text": " you're trying to like explain. Well, let's control for what it is I want to say. Like I,", "tokens": [51372, 291, 434, 1382, 281, 411, 2903, 13, 1042, 11, 718, 311, 1969, 337, 437, 309, 307, 286, 528, 281, 584, 13, 1743, 286, 11, 51548], "temperature": 0.0, "avg_logprob": -0.15917655097113714, "compression_ratio": 1.9028776978417266, "no_speech_prob": 0.007814846932888031}, {"id": 828, "seek": 435440, "start": 4378.08, "end": 4381.92, "text": " I'm saying let's control for the thing, the, the message, control for the message.", "tokens": [51548, 286, 478, 1566, 718, 311, 1969, 337, 264, 551, 11, 264, 11, 264, 3636, 11, 1969, 337, 264, 3636, 13, 51740], "temperature": 0.0, "avg_logprob": -0.15917655097113714, "compression_ratio": 1.9028776978417266, "no_speech_prob": 0.007814846932888031}, {"id": 829, "seek": 438192, "start": 4381.92, "end": 4385.04, "text": " But that means the message needs to be understood. That's the goal.", "tokens": [50364, 583, 300, 1355, 264, 3636, 2203, 281, 312, 7320, 13, 663, 311, 264, 3387, 13, 50520], "temperature": 0.0, "avg_logprob": -0.15951518594783587, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.007576382718980312}, {"id": 830, "seek": 438192, "start": 4385.04, "end": 4387.12, "text": " Oh, but that's the meaning. So I'm still talking about the form.", "tokens": [50520, 876, 11, 457, 300, 311, 264, 3620, 13, 407, 286, 478, 920, 1417, 466, 264, 1254, 13, 50624], "temperature": 0.0, "avg_logprob": -0.15951518594783587, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.007576382718980312}, {"id": 831, "seek": 438192, "start": 4387.76, "end": 4392.8, "text": " Just the form of the meaning. How do I frame the form of the meaning is all I'm talking about.", "tokens": [50656, 1449, 264, 1254, 295, 264, 3620, 13, 1012, 360, 286, 3920, 264, 1254, 295, 264, 3620, 307, 439, 286, 478, 1417, 466, 13, 50908], "temperature": 0.0, "avg_logprob": -0.15951518594783587, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.007576382718980312}, {"id": 832, "seek": 438192, "start": 4392.8, "end": 4396.56, "text": " You're talking about a harder thing, I think, is like, how am I, like trying to change the", "tokens": [50908, 509, 434, 1417, 466, 257, 6081, 551, 11, 286, 519, 11, 307, 411, 11, 577, 669, 286, 11, 411, 1382, 281, 1319, 264, 51096], "temperature": 0.0, "avg_logprob": -0.15951518594783587, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.007576382718980312}, {"id": 833, "seek": 438192, "start": 4396.56, "end": 4402.4, "text": " meaning. Let's, let's keep the meaning constant. Like which, if you keep the meaning constant,", "tokens": [51096, 3620, 13, 961, 311, 11, 718, 311, 1066, 264, 3620, 5754, 13, 1743, 597, 11, 498, 291, 1066, 264, 3620, 5754, 11, 51388], "temperature": 0.0, "avg_logprob": -0.15951518594783587, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.007576382718980312}, {"id": 834, "seek": 438192, "start": 4402.4, "end": 4407.52, "text": " how can I phrase whatever it is I need to say, like I gotta pick the right words and I'm gonna", "tokens": [51388, 577, 393, 286, 9535, 2035, 309, 307, 286, 643, 281, 584, 11, 411, 286, 3428, 1888, 264, 558, 2283, 293, 286, 478, 799, 51644], "temperature": 0.0, "avg_logprob": -0.15951518594783587, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.007576382718980312}, {"id": 835, "seek": 440752, "start": 4407.52, "end": 4412.080000000001, "text": " pick the order so that it's, so it's easy for me. That's, that's, that's what I think it's", "tokens": [50364, 1888, 264, 1668, 370, 300, 309, 311, 11, 370, 309, 311, 1858, 337, 385, 13, 663, 311, 11, 300, 311, 11, 300, 311, 437, 286, 519, 309, 311, 50592], "temperature": 0.0, "avg_logprob": -0.14524129837278335, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.009265274740755558}, {"id": 836, "seek": 440752, "start": 4412.080000000001, "end": 4418.320000000001, "text": " probably like. I think I'm still tying meaning and form together in my head. But you're saying,", "tokens": [50592, 1391, 411, 13, 286, 519, 286, 478, 920, 32405, 3620, 293, 1254, 1214, 294, 452, 1378, 13, 583, 291, 434, 1566, 11, 50904], "temperature": 0.0, "avg_logprob": -0.14524129837278335, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.009265274740755558}, {"id": 837, "seek": 440752, "start": 4418.320000000001, "end": 4423.4400000000005, "text": " if you keep the meaning of what you're saying constant, would the optimization, yeah, it could", "tokens": [50904, 498, 291, 1066, 264, 3620, 295, 437, 291, 434, 1566, 5754, 11, 576, 264, 19618, 11, 1338, 11, 309, 727, 51160], "temperature": 0.0, "avg_logprob": -0.14524129837278335, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.009265274740755558}, {"id": 838, "seek": 440752, "start": 4423.4400000000005, "end": 4429.200000000001, "text": " be the primary objective of that optimization is the, for production. That's interesting.", "tokens": [51160, 312, 264, 6194, 10024, 295, 300, 19618, 307, 264, 11, 337, 4265, 13, 663, 311, 1880, 13, 51448], "temperature": 0.0, "avg_logprob": -0.14524129837278335, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.009265274740755558}, {"id": 839, "seek": 440752, "start": 4429.92, "end": 4436.400000000001, "text": " I'm struggling to keep constant the meaning. It's just so, I mean, I'm such a, I'm a human,", "tokens": [51484, 286, 478, 9314, 281, 1066, 5754, 264, 3620, 13, 467, 311, 445, 370, 11, 286, 914, 11, 286, 478, 1270, 257, 11, 286, 478, 257, 1952, 11, 51808], "temperature": 0.0, "avg_logprob": -0.14524129837278335, "compression_ratio": 1.8228346456692914, "no_speech_prob": 0.009265274740755558}, {"id": 840, "seek": 443640, "start": 4436.4, "end": 4443.28, "text": " right? So for me, the form without having introspected on this, the form and the meaning", "tokens": [50364, 558, 30, 407, 337, 385, 11, 264, 1254, 1553, 1419, 560, 28713, 292, 322, 341, 11, 264, 1254, 293, 264, 3620, 50708], "temperature": 0.0, "avg_logprob": -0.14141262794027523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006262076203711331}, {"id": 841, "seek": 443640, "start": 4443.28, "end": 4451.04, "text": " are tied together, like deeply because I'm a human. Like for me, when I'm speaking,", "tokens": [50708, 366, 9601, 1214, 11, 411, 8760, 570, 286, 478, 257, 1952, 13, 1743, 337, 385, 11, 562, 286, 478, 4124, 11, 51096], "temperature": 0.0, "avg_logprob": -0.14141262794027523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006262076203711331}, {"id": 842, "seek": 443640, "start": 4451.04, "end": 4456.08, "text": " because I haven't thought about language, like in a rigorous way about the form of language.", "tokens": [51096, 570, 286, 2378, 380, 1194, 466, 2856, 11, 411, 294, 257, 29882, 636, 466, 264, 1254, 295, 2856, 13, 51348], "temperature": 0.0, "avg_logprob": -0.14141262794027523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006262076203711331}, {"id": 843, "seek": 443640, "start": 4456.08, "end": 4463.759999999999, "text": " But look, for any event, there's, there's an unbounded, I don't want to say infinite, but sort", "tokens": [51348, 583, 574, 11, 337, 604, 2280, 11, 456, 311, 11, 456, 311, 364, 517, 18767, 292, 11, 286, 500, 380, 528, 281, 584, 13785, 11, 457, 1333, 51732], "temperature": 0.0, "avg_logprob": -0.14141262794027523, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006262076203711331}, {"id": 844, "seek": 446376, "start": 4463.76, "end": 4469.68, "text": " of ways that I might communicate that same event. This two dogs entered a room, I can say in many,", "tokens": [50364, 295, 2098, 300, 286, 1062, 7890, 300, 912, 2280, 13, 639, 732, 7197, 9065, 257, 1808, 11, 286, 393, 584, 294, 867, 11, 50660], "temperature": 0.0, "avg_logprob": -0.10960892268589564, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.007813547737896442}, {"id": 845, "seek": 446376, "start": 4469.68, "end": 4475.4400000000005, "text": " many different ways. I can say, Hey, there's two dogs. They entered the room. Hey, the room was", "tokens": [50660, 867, 819, 2098, 13, 286, 393, 584, 11, 1911, 11, 456, 311, 732, 7197, 13, 814, 9065, 264, 1808, 13, 1911, 11, 264, 1808, 390, 50948], "temperature": 0.0, "avg_logprob": -0.10960892268589564, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.007813547737896442}, {"id": 846, "seek": 446376, "start": 4475.4400000000005, "end": 4479.360000000001, "text": " entered by something. The thing that was entered was two dogs. I mean, there's, I mean, it's kind", "tokens": [50948, 9065, 538, 746, 13, 440, 551, 300, 390, 9065, 390, 732, 7197, 13, 286, 914, 11, 456, 311, 11, 286, 914, 11, 309, 311, 733, 51144], "temperature": 0.0, "avg_logprob": -0.10960892268589564, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.007813547737896442}, {"id": 847, "seek": 446376, "start": 4479.360000000001, "end": 4486.08, "text": " of awkward and weird stuff. But those are all similar messages with different forms, but different", "tokens": [51144, 295, 11411, 293, 3657, 1507, 13, 583, 729, 366, 439, 2531, 7897, 365, 819, 6422, 11, 457, 819, 51480], "temperature": 0.0, "avg_logprob": -0.10960892268589564, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.007813547737896442}, {"id": 848, "seek": 446376, "start": 4486.08, "end": 4490.64, "text": " ways that might frame. And of course, I use the same words there all the time. I could have referred", "tokens": [51480, 2098, 300, 1062, 3920, 13, 400, 295, 1164, 11, 286, 764, 264, 912, 2283, 456, 439, 264, 565, 13, 286, 727, 362, 10839, 51708], "temperature": 0.0, "avg_logprob": -0.10960892268589564, "compression_ratio": 1.8850574712643677, "no_speech_prob": 0.007813547737896442}, {"id": 849, "seek": 449064, "start": 4490.72, "end": 4495.6, "text": " to the dogs as a Dalmatian and a Poodle or something. I could have been more specific", "tokens": [50368, 281, 264, 7197, 382, 257, 17357, 15677, 952, 293, 257, 430, 1816, 306, 420, 746, 13, 286, 727, 362, 668, 544, 2685, 50612], "temperature": 0.0, "avg_logprob": -0.14508497873942058, "compression_ratio": 1.7665615141955835, "no_speech_prob": 0.04333484172821045}, {"id": 850, "seek": 449064, "start": 4495.6, "end": 4500.8, "text": " or less specific about what they are. And I could have said, been more abstract about the number.", "tokens": [50612, 420, 1570, 2685, 466, 437, 436, 366, 13, 400, 286, 727, 362, 848, 11, 668, 544, 12649, 466, 264, 1230, 13, 50872], "temperature": 0.0, "avg_logprob": -0.14508497873942058, "compression_ratio": 1.7665615141955835, "no_speech_prob": 0.04333484172821045}, {"id": 851, "seek": 449064, "start": 4500.8, "end": 4506.64, "text": " There's like, so I, like I'm trying to keep the meaning, which is this event constant. And then", "tokens": [50872, 821, 311, 411, 11, 370, 286, 11, 411, 286, 478, 1382, 281, 1066, 264, 3620, 11, 597, 307, 341, 2280, 5754, 13, 400, 550, 51164], "temperature": 0.0, "avg_logprob": -0.14508497873942058, "compression_ratio": 1.7665615141955835, "no_speech_prob": 0.04333484172821045}, {"id": 852, "seek": 449064, "start": 4506.64, "end": 4510.320000000001, "text": " how am I going to describe that to get that to you? It kind of depends on what you need to know,", "tokens": [51164, 577, 669, 286, 516, 281, 6786, 300, 281, 483, 300, 281, 291, 30, 467, 733, 295, 5946, 322, 437, 291, 643, 281, 458, 11, 51348], "temperature": 0.0, "avg_logprob": -0.14508497873942058, "compression_ratio": 1.7665615141955835, "no_speech_prob": 0.04333484172821045}, {"id": 853, "seek": 449064, "start": 4510.320000000001, "end": 4515.280000000001, "text": " right? And what I think you need to know. But I'm like, let's control for all that stuff and not,", "tokens": [51348, 558, 30, 400, 437, 286, 519, 291, 643, 281, 458, 13, 583, 286, 478, 411, 11, 718, 311, 1969, 337, 439, 300, 1507, 293, 406, 11, 51596], "temperature": 0.0, "avg_logprob": -0.14508497873942058, "compression_ratio": 1.7665615141955835, "no_speech_prob": 0.04333484172821045}, {"id": 854, "seek": 449064, "start": 4515.84, "end": 4519.52, "text": " and then I'm just like choosing about, I'm doing something simpler than you're doing,", "tokens": [51624, 293, 550, 286, 478, 445, 411, 10875, 466, 11, 286, 478, 884, 746, 18587, 813, 291, 434, 884, 11, 51808], "temperature": 0.0, "avg_logprob": -0.14508497873942058, "compression_ratio": 1.7665615141955835, "no_speech_prob": 0.04333484172821045}, {"id": 855, "seek": 451952, "start": 4519.52, "end": 4526.320000000001, "text": " which is just forms. Yes. Just words. So to you specifying the breed of dog and", "tokens": [50364, 597, 307, 445, 6422, 13, 1079, 13, 1449, 2283, 13, 407, 281, 291, 1608, 5489, 264, 18971, 295, 3000, 293, 50704], "temperature": 0.0, "avg_logprob": -0.16708257375669874, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.005729081574827433}, {"id": 856, "seek": 451952, "start": 4527.120000000001, "end": 4531.52, "text": " whether they're cute or not is changing the meaning. That might be, yeah. Yeah, that would be", "tokens": [50744, 1968, 436, 434, 4052, 420, 406, 307, 4473, 264, 3620, 13, 663, 1062, 312, 11, 1338, 13, 865, 11, 300, 576, 312, 50964], "temperature": 0.0, "avg_logprob": -0.16708257375669874, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.005729081574827433}, {"id": 857, "seek": 451952, "start": 4531.52, "end": 4536.320000000001, "text": " changing. Oh, that would be changing the meaning for sure. Right. So you're just, yeah. Yeah.", "tokens": [50964, 4473, 13, 876, 11, 300, 576, 312, 4473, 264, 3620, 337, 988, 13, 1779, 13, 407, 291, 434, 445, 11, 1338, 13, 865, 13, 51204], "temperature": 0.0, "avg_logprob": -0.16708257375669874, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.005729081574827433}, {"id": 858, "seek": 451952, "start": 4536.320000000001, "end": 4540.8, "text": " That's changing the meaning. But say, even if we keep that constant, we can still talk about", "tokens": [51204, 663, 311, 4473, 264, 3620, 13, 583, 584, 11, 754, 498, 321, 1066, 300, 5754, 11, 321, 393, 920, 751, 466, 51428], "temperature": 0.0, "avg_logprob": -0.16708257375669874, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.005729081574827433}, {"id": 859, "seek": 451952, "start": 4540.8, "end": 4547.280000000001, "text": " what's easier or hard for me, right? The listener and the, which phrase structures I use, which", "tokens": [51428, 437, 311, 3571, 420, 1152, 337, 385, 11, 558, 30, 440, 31569, 293, 264, 11, 597, 9535, 9227, 286, 764, 11, 597, 51752], "temperature": 0.0, "avg_logprob": -0.16708257375669874, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.005729081574827433}, {"id": 860, "seek": 454728, "start": 4547.28, "end": 4554.5599999999995, "text": " combinations, which, yeah. This is so fascinating and just like a really powerful window into human", "tokens": [50364, 21267, 11, 597, 11, 1338, 13, 639, 307, 370, 10343, 293, 445, 411, 257, 534, 4005, 4910, 666, 1952, 50728], "temperature": 0.0, "avg_logprob": -0.1345076228297034, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0007320558070205152}, {"id": 861, "seek": 454728, "start": 4554.5599999999995, "end": 4561.84, "text": " language. But I wonder still throughout this, how vast the gap between meaning and form,", "tokens": [50728, 2856, 13, 583, 286, 2441, 920, 3710, 341, 11, 577, 8369, 264, 7417, 1296, 3620, 293, 1254, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1345076228297034, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0007320558070205152}, {"id": 862, "seek": 454728, "start": 4561.84, "end": 4567.84, "text": " I just, I just have this like, maybe romanticized notion that they're close together, that they", "tokens": [51092, 286, 445, 11, 286, 445, 362, 341, 411, 11, 1310, 13590, 1602, 10710, 300, 436, 434, 1998, 1214, 11, 300, 436, 51392], "temperature": 0.0, "avg_logprob": -0.1345076228297034, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0007320558070205152}, {"id": 863, "seek": 454728, "start": 4567.84, "end": 4573.84, "text": " evolve close to like hand in hand, that you can't just simply optimize for one without", "tokens": [51392, 16693, 1998, 281, 411, 1011, 294, 1011, 11, 300, 291, 393, 380, 445, 2935, 19719, 337, 472, 1553, 51692], "temperature": 0.0, "avg_logprob": -0.1345076228297034, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0007320558070205152}, {"id": 864, "seek": 457384, "start": 4574.56, "end": 4580.4800000000005, "text": " the other being in the room with us. Like it's, well, it's kind of like an iceberg.", "tokens": [50400, 264, 661, 885, 294, 264, 1808, 365, 505, 13, 1743, 309, 311, 11, 731, 11, 309, 311, 733, 295, 411, 364, 38880, 13, 50696], "temperature": 0.0, "avg_logprob": -0.13833183970877794, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00298073492012918}, {"id": 865, "seek": 457384, "start": 4580.4800000000005, "end": 4585.12, "text": " Form is the tip of the iceberg and the rest, the, the meaning is the iceberg, but you can't like", "tokens": [50696, 10126, 307, 264, 4125, 295, 264, 38880, 293, 264, 1472, 11, 264, 11, 264, 3620, 307, 264, 38880, 11, 457, 291, 393, 380, 411, 50928], "temperature": 0.0, "avg_logprob": -0.13833183970877794, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00298073492012918}, {"id": 866, "seek": 457384, "start": 4585.12, "end": 4590.400000000001, "text": " separate. But I think that's why these large language models are so successful is because", "tokens": [50928, 4994, 13, 583, 286, 519, 300, 311, 983, 613, 2416, 2856, 5245, 366, 370, 4406, 307, 570, 51192], "temperature": 0.0, "avg_logprob": -0.13833183970877794, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00298073492012918}, {"id": 867, "seek": 457384, "start": 4590.400000000001, "end": 4595.68, "text": " they're good at form and form isn't that hard in some sense. And meaning is tough still. And", "tokens": [51192, 436, 434, 665, 412, 1254, 293, 1254, 1943, 380, 300, 1152, 294, 512, 2020, 13, 400, 3620, 307, 4930, 920, 13, 400, 51456], "temperature": 0.0, "avg_logprob": -0.13833183970877794, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00298073492012918}, {"id": 868, "seek": 457384, "start": 4595.68, "end": 4598.56, "text": " that's why they're not, they're, you know, they don't understand what they're doing. We're going", "tokens": [51456, 300, 311, 983, 436, 434, 406, 11, 436, 434, 11, 291, 458, 11, 436, 500, 380, 1223, 437, 436, 434, 884, 13, 492, 434, 516, 51600], "temperature": 0.0, "avg_logprob": -0.13833183970877794, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.00298073492012918}, {"id": 869, "seek": 459856, "start": 4598.64, "end": 4604.4800000000005, "text": " to talk about that later maybe, but like we can distinguish in our, forget about large language", "tokens": [50368, 281, 751, 466, 300, 1780, 1310, 11, 457, 411, 321, 393, 20206, 294, 527, 11, 2870, 466, 2416, 2856, 50660], "temperature": 0.0, "avg_logprob": -0.12959910751482762, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.15195196866989136}, {"id": 870, "seek": 459856, "start": 4604.4800000000005, "end": 4608.4800000000005, "text": " models, like humans, maybe you'll talk about that later too, is like the difference between", "tokens": [50660, 5245, 11, 411, 6255, 11, 1310, 291, 603, 751, 466, 300, 1780, 886, 11, 307, 411, 264, 2649, 1296, 50860], "temperature": 0.0, "avg_logprob": -0.12959910751482762, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.15195196866989136}, {"id": 871, "seek": 459856, "start": 4609.200000000001, "end": 4614.56, "text": " language, which is a communication system and thinking, which is meaning. So language is a", "tokens": [50896, 2856, 11, 597, 307, 257, 6101, 1185, 293, 1953, 11, 597, 307, 3620, 13, 407, 2856, 307, 257, 51164], "temperature": 0.0, "avg_logprob": -0.12959910751482762, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.15195196866989136}, {"id": 872, "seek": 459856, "start": 4614.56, "end": 4620.400000000001, "text": " communication system for the meaning. It's not the meaning. And so that's why, I mean, that, and", "tokens": [51164, 6101, 1185, 337, 264, 3620, 13, 467, 311, 406, 264, 3620, 13, 400, 370, 300, 311, 983, 11, 286, 914, 11, 300, 11, 293, 51456], "temperature": 0.0, "avg_logprob": -0.12959910751482762, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.15195196866989136}, {"id": 873, "seek": 459856, "start": 4620.400000000001, "end": 4624.320000000001, "text": " there's a lot of interesting evidence we can talk about relevant, relevant to that.", "tokens": [51456, 456, 311, 257, 688, 295, 1880, 4467, 321, 393, 751, 466, 7340, 11, 7340, 281, 300, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12959910751482762, "compression_ratio": 1.9367088607594938, "no_speech_prob": 0.15195196866989136}, {"id": 874, "seek": 462432, "start": 4624.32, "end": 4628.639999999999, "text": " Well, I mean, that's a really interesting question. What is the difference between", "tokens": [50364, 1042, 11, 286, 914, 11, 300, 311, 257, 534, 1880, 1168, 13, 708, 307, 264, 2649, 1296, 50580], "temperature": 0.0, "avg_logprob": -0.1213561213293741, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.004006459377706051}, {"id": 875, "seek": 462432, "start": 4630.24, "end": 4637.599999999999, "text": " language written, communicated versus thought? What to use the difference between them?", "tokens": [50660, 2856, 3720, 11, 34989, 5717, 1194, 30, 708, 281, 764, 264, 2649, 1296, 552, 30, 51028], "temperature": 0.0, "avg_logprob": -0.1213561213293741, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.004006459377706051}, {"id": 876, "seek": 462432, "start": 4638.799999999999, "end": 4645.2, "text": " Well, you or anyone cast a think of a task, which they think is, is a good thinking task. And", "tokens": [51088, 1042, 11, 291, 420, 2878, 4193, 257, 519, 295, 257, 5633, 11, 597, 436, 519, 307, 11, 307, 257, 665, 1953, 5633, 13, 400, 51408], "temperature": 0.0, "avg_logprob": -0.1213561213293741, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.004006459377706051}, {"id": 877, "seek": 462432, "start": 4645.2, "end": 4649.92, "text": " there's lots and lots of tasks, which should be good thinking tasks. And whatever those tasks,", "tokens": [51408, 456, 311, 3195, 293, 3195, 295, 9608, 11, 597, 820, 312, 665, 1953, 9608, 13, 400, 2035, 729, 9608, 11, 51644], "temperature": 0.0, "avg_logprob": -0.1213561213293741, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.004006459377706051}, {"id": 878, "seek": 464992, "start": 4649.92, "end": 4654.16, "text": " let's say it's, you know, playing chess, or that's a good thinking task, or playing some game,", "tokens": [50364, 718, 311, 584, 309, 311, 11, 291, 458, 11, 2433, 24122, 11, 420, 300, 311, 257, 665, 1953, 5633, 11, 420, 2433, 512, 1216, 11, 50576], "temperature": 0.0, "avg_logprob": -0.12153908826302791, "compression_ratio": 1.9172413793103449, "no_speech_prob": 0.018538396805524826}, {"id": 879, "seek": 464992, "start": 4654.16, "end": 4660.24, "text": " or doing some complex puzzles, maybe, maybe remembering some digits that's thinking,", "tokens": [50576, 420, 884, 512, 3997, 24138, 11, 1310, 11, 1310, 20719, 512, 27011, 300, 311, 1953, 11, 50880], "temperature": 0.0, "avg_logprob": -0.12153908826302791, "compression_ratio": 1.9172413793103449, "no_speech_prob": 0.018538396805524826}, {"id": 880, "seek": 464992, "start": 4660.24, "end": 4664.0, "text": " remembering some, a lot of different tasks we might think, maybe just listening to music is", "tokens": [50880, 20719, 512, 11, 257, 688, 295, 819, 9608, 321, 1062, 519, 11, 1310, 445, 4764, 281, 1318, 307, 51068], "temperature": 0.0, "avg_logprob": -0.12153908826302791, "compression_ratio": 1.9172413793103449, "no_speech_prob": 0.018538396805524826}, {"id": 881, "seek": 464992, "start": 4664.0, "end": 4668.32, "text": " thinking, or there's a lot of different tasks we might think of as thinking. There's a woman in", "tokens": [51068, 1953, 11, 420, 456, 311, 257, 688, 295, 819, 9608, 321, 1062, 519, 295, 382, 1953, 13, 821, 311, 257, 3059, 294, 51284], "temperature": 0.0, "avg_logprob": -0.12153908826302791, "compression_ratio": 1.9172413793103449, "no_speech_prob": 0.018538396805524826}, {"id": 882, "seek": 464992, "start": 4668.32, "end": 4673.04, "text": " my department at Federico, and she's done a lot of work at, on this question about what's the", "tokens": [51284, 452, 5882, 412, 45545, 2789, 11, 293, 750, 311, 1096, 257, 688, 295, 589, 412, 11, 322, 341, 1168, 466, 437, 311, 264, 51520], "temperature": 0.0, "avg_logprob": -0.12153908826302791, "compression_ratio": 1.9172413793103449, "no_speech_prob": 0.018538396805524826}, {"id": 883, "seek": 464992, "start": 4673.04, "end": 4678.32, "text": " connection between language and thought. And, and so she uses, I was referring earlier to MRI,", "tokens": [51520, 4984, 1296, 2856, 293, 1194, 13, 400, 11, 293, 370, 750, 4960, 11, 286, 390, 13761, 3071, 281, 32812, 11, 51784], "temperature": 0.0, "avg_logprob": -0.12153908826302791, "compression_ratio": 1.9172413793103449, "no_speech_prob": 0.018538396805524826}, {"id": 884, "seek": 467832, "start": 4678.32, "end": 4684.639999999999, "text": " fMRI, that's her primary method. And so she is been really fascinated by this question about whether,", "tokens": [50364, 283, 44, 5577, 11, 300, 311, 720, 6194, 3170, 13, 400, 370, 750, 307, 668, 534, 24597, 538, 341, 1168, 466, 1968, 11, 50680], "temperature": 0.0, "avg_logprob": -0.13551194688915152, "compression_ratio": 1.7318007662835249, "no_speech_prob": 0.0007791270036250353}, {"id": 885, "seek": 467832, "start": 4685.599999999999, "end": 4688.88, "text": " what language is, okay? And so as I mentioned earlier, you can", "tokens": [50728, 437, 2856, 307, 11, 1392, 30, 400, 370, 382, 286, 2835, 3071, 11, 291, 393, 50892], "temperature": 0.0, "avg_logprob": -0.13551194688915152, "compression_ratio": 1.7318007662835249, "no_speech_prob": 0.0007791270036250353}, {"id": 886, "seek": 467832, "start": 4689.44, "end": 4693.84, "text": " localize my language area, your language area in a few minutes, okay? In like 15 minutes,", "tokens": [50920, 2654, 1125, 452, 2856, 1859, 11, 428, 2856, 1859, 294, 257, 1326, 2077, 11, 1392, 30, 682, 411, 2119, 2077, 11, 51140], "temperature": 0.0, "avg_logprob": -0.13551194688915152, "compression_ratio": 1.7318007662835249, "no_speech_prob": 0.0007791270036250353}, {"id": 887, "seek": 467832, "start": 4693.84, "end": 4699.2, "text": " I can listen to language, listen to non-language or backward speech or something. And we'll find", "tokens": [51140, 286, 393, 2140, 281, 2856, 11, 2140, 281, 2107, 12, 25241, 20473, 420, 23897, 6218, 420, 746, 13, 400, 321, 603, 915, 51408], "temperature": 0.0, "avg_logprob": -0.13551194688915152, "compression_ratio": 1.7318007662835249, "no_speech_prob": 0.0007791270036250353}, {"id": 888, "seek": 467832, "start": 4699.2, "end": 4706.08, "text": " areas left lateralized network in my head, which is especially, which is very sensitive to language,", "tokens": [51408, 3179, 1411, 25128, 1602, 3209, 294, 452, 1378, 11, 597, 307, 2318, 11, 597, 307, 588, 9477, 281, 2856, 11, 51752], "temperature": 0.0, "avg_logprob": -0.13551194688915152, "compression_ratio": 1.7318007662835249, "no_speech_prob": 0.0007791270036250353}, {"id": 889, "seek": 470608, "start": 4706.08, "end": 4709.6, "text": " as opposed to whatever that control was, okay? Can you specify what you mean by language, like", "tokens": [50364, 382, 8851, 281, 2035, 300, 1969, 390, 11, 1392, 30, 1664, 291, 16500, 437, 291, 914, 538, 2856, 11, 411, 50540], "temperature": 0.0, "avg_logprob": -0.12059923266688137, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.012621068395674229}, {"id": 890, "seek": 470608, "start": 4709.6, "end": 4713.92, "text": " communicated language? Like what is language? Just sentences. You know, I'm listening to English", "tokens": [50540, 34989, 2856, 30, 1743, 437, 307, 2856, 30, 1449, 16579, 13, 509, 458, 11, 286, 478, 4764, 281, 3669, 50756], "temperature": 0.0, "avg_logprob": -0.12059923266688137, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.012621068395674229}, {"id": 891, "seek": 470608, "start": 4713.92, "end": 4719.04, "text": " of any kind story, or I can read sentences, anything at all that I understand, if I understand it,", "tokens": [50756, 295, 604, 733, 1657, 11, 420, 286, 393, 1401, 16579, 11, 1340, 412, 439, 300, 286, 1223, 11, 498, 286, 1223, 309, 11, 51012], "temperature": 0.0, "avg_logprob": -0.12059923266688137, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.012621068395674229}, {"id": 892, "seek": 470608, "start": 4719.04, "end": 4724.08, "text": " then it'll activate my language network. So right now, my language network is going like crazy", "tokens": [51012, 550, 309, 603, 13615, 452, 2856, 3209, 13, 407, 558, 586, 11, 452, 2856, 3209, 307, 516, 411, 3219, 51264], "temperature": 0.0, "avg_logprob": -0.12059923266688137, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.012621068395674229}, {"id": 893, "seek": 470608, "start": 4724.08, "end": 4728.4, "text": " when I'm talking and when I'm listening to you, because we're both, we're communicating. And", "tokens": [51264, 562, 286, 478, 1417, 293, 562, 286, 478, 4764, 281, 291, 11, 570, 321, 434, 1293, 11, 321, 434, 17559, 13, 400, 51480], "temperature": 0.0, "avg_logprob": -0.12059923266688137, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.012621068395674229}, {"id": 894, "seek": 470608, "start": 4728.4, "end": 4734.8, "text": " that's pretty stable. Yeah, it's incredibly stable. So I've, I happen to be married to this woman", "tokens": [51480, 300, 311, 1238, 8351, 13, 865, 11, 309, 311, 6252, 8351, 13, 407, 286, 600, 11, 286, 1051, 281, 312, 5259, 281, 341, 3059, 51800], "temperature": 0.0, "avg_logprob": -0.12059923266688137, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.012621068395674229}, {"id": 895, "seek": 473480, "start": 4734.88, "end": 4739.52, "text": " Federico. So I've been scanned by her over and over and over since 2007 or six or something.", "tokens": [50368, 45545, 2789, 13, 407, 286, 600, 668, 45089, 538, 720, 670, 293, 670, 293, 670, 1670, 12656, 420, 2309, 420, 746, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1373401471038363, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0012445278698578477}, {"id": 896, "seek": 473480, "start": 4739.52, "end": 4745.6, "text": " And so my language network is exactly the same, you know, like a month ago, as it was back in 2007.", "tokens": [50600, 400, 370, 452, 2856, 3209, 307, 2293, 264, 912, 11, 291, 458, 11, 411, 257, 1618, 2057, 11, 382, 309, 390, 646, 294, 12656, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1373401471038363, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0012445278698578477}, {"id": 897, "seek": 473480, "start": 4745.6, "end": 4751.6, "text": " It's amazingly stable. It's astounding. And with it, it's, it's a really fundamentally cool thing.", "tokens": [50904, 467, 311, 31762, 8351, 13, 467, 311, 5357, 24625, 13, 400, 365, 309, 11, 309, 311, 11, 309, 311, 257, 534, 17879, 1627, 551, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1373401471038363, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0012445278698578477}, {"id": 898, "seek": 473480, "start": 4751.6, "end": 4756.88, "text": " And so my language network is, it's like my face, okay? It's not changing much over time inside my", "tokens": [51204, 400, 370, 452, 2856, 3209, 307, 11, 309, 311, 411, 452, 1851, 11, 1392, 30, 467, 311, 406, 4473, 709, 670, 565, 1854, 452, 51468], "temperature": 0.0, "avg_logprob": -0.1373401471038363, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0012445278698578477}, {"id": 899, "seek": 473480, "start": 4756.88, "end": 4762.72, "text": " head. Can I ask a quick question? Sorry, it's a small tangent. At which point in the, as you grow", "tokens": [51468, 1378, 13, 1664, 286, 1029, 257, 1702, 1168, 30, 4919, 11, 309, 311, 257, 1359, 27747, 13, 1711, 597, 935, 294, 264, 11, 382, 291, 1852, 51760], "temperature": 0.0, "avg_logprob": -0.1373401471038363, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.0012445278698578477}, {"id": 900, "seek": 476272, "start": 4762.8, "end": 4768.0, "text": " up from baby to adult, does it stabilize? We don't know. Like that's a, that's a very hard", "tokens": [50368, 493, 490, 3186, 281, 5075, 11, 775, 309, 31870, 30, 492, 500, 380, 458, 13, 1743, 300, 311, 257, 11, 300, 311, 257, 588, 1152, 50628], "temperature": 0.0, "avg_logprob": -0.1407665077008699, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.012050317600369453}, {"id": 901, "seek": 476272, "start": 4768.0, "end": 4772.56, "text": " question. They're working on that right now because of the problem of scanning little kids,", "tokens": [50628, 1168, 13, 814, 434, 1364, 322, 300, 558, 586, 570, 295, 264, 1154, 295, 27019, 707, 2301, 11, 50856], "temperature": 0.0, "avg_logprob": -0.1407665077008699, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.012050317600369453}, {"id": 902, "seek": 476272, "start": 4772.56, "end": 4778.8, "text": " like doing the, trying to do local, trying to do the localization on little children in this scanner,", "tokens": [50856, 411, 884, 264, 11, 1382, 281, 360, 2654, 11, 1382, 281, 360, 264, 2654, 2144, 322, 707, 2227, 294, 341, 30211, 11, 51168], "temperature": 0.0, "avg_logprob": -0.1407665077008699, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.012050317600369453}, {"id": 903, "seek": 476272, "start": 4778.8, "end": 4782.8, "text": " or you're lying in the fMRI scan. That's the best way to figure out where something's going on", "tokens": [51168, 420, 291, 434, 8493, 294, 264, 283, 44, 5577, 11049, 13, 663, 311, 264, 1151, 636, 281, 2573, 484, 689, 746, 311, 516, 322, 51368], "temperature": 0.0, "avg_logprob": -0.1407665077008699, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.012050317600369453}, {"id": 904, "seek": 476272, "start": 4782.8, "end": 4787.360000000001, "text": " inside our brains. And the scanner is loud and you're in this tiny little, you know, area,", "tokens": [51368, 1854, 527, 15442, 13, 400, 264, 30211, 307, 6588, 293, 291, 434, 294, 341, 5870, 707, 11, 291, 458, 11, 1859, 11, 51596], "temperature": 0.0, "avg_logprob": -0.1407665077008699, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.012050317600369453}, {"id": 905, "seek": 476272, "start": 4787.360000000001, "end": 4792.240000000001, "text": " you're claustrophobic. And it doesn't bother me at all. I can go to sleep in there, but some people", "tokens": [51596, 291, 434, 3583, 381, 11741, 22234, 13, 400, 309, 1177, 380, 8677, 385, 412, 439, 13, 286, 393, 352, 281, 2817, 294, 456, 11, 457, 512, 561, 51840], "temperature": 0.0, "avg_logprob": -0.1407665077008699, "compression_ratio": 1.7484662576687116, "no_speech_prob": 0.012050317600369453}, {"id": 906, "seek": 479224, "start": 4792.24, "end": 4795.5199999999995, "text": " are bothered by it. And little kids don't really like it. And they don't like to lie still. And", "tokens": [50364, 366, 22996, 538, 309, 13, 400, 707, 2301, 500, 380, 534, 411, 309, 13, 400, 436, 500, 380, 411, 281, 4544, 920, 13, 400, 50528], "temperature": 0.0, "avg_logprob": -0.1215414090102978, "compression_ratio": 1.9124293785310735, "no_speech_prob": 0.0019873648416250944}, {"id": 907, "seek": 479224, "start": 4795.5199999999995, "end": 4799.84, "text": " you have to be really still because you can move around. That's, that messes up the coordinates of", "tokens": [50528, 291, 362, 281, 312, 534, 920, 570, 291, 393, 1286, 926, 13, 663, 311, 11, 300, 2082, 279, 493, 264, 21056, 295, 50744], "temperature": 0.0, "avg_logprob": -0.1215414090102978, "compression_ratio": 1.9124293785310735, "no_speech_prob": 0.0019873648416250944}, {"id": 908, "seek": 479224, "start": 4799.84, "end": 4804.5599999999995, "text": " where, where everything is. And so, you know, try to get, you know, your question is how and when", "tokens": [50744, 689, 11, 689, 1203, 307, 13, 400, 370, 11, 291, 458, 11, 853, 281, 483, 11, 291, 458, 11, 428, 1168, 307, 577, 293, 562, 50980], "temperature": 0.0, "avg_logprob": -0.1215414090102978, "compression_ratio": 1.9124293785310735, "no_speech_prob": 0.0019873648416250944}, {"id": 909, "seek": 479224, "start": 4804.5599999999995, "end": 4808.96, "text": " our language developing, you know, how, when, when, how does this left lateralized system come to", "tokens": [50980, 527, 2856, 6416, 11, 291, 458, 11, 577, 11, 562, 11, 562, 11, 577, 775, 341, 1411, 25128, 1602, 1185, 808, 281, 51200], "temperature": 0.0, "avg_logprob": -0.1215414090102978, "compression_ratio": 1.9124293785310735, "no_speech_prob": 0.0019873648416250944}, {"id": 910, "seek": 479224, "start": 4808.96, "end": 4812.8, "text": " play? Where does it, you know, and it's really hard to get a two year old to do this task. But", "tokens": [51200, 862, 30, 2305, 775, 309, 11, 291, 458, 11, 293, 309, 311, 534, 1152, 281, 483, 257, 732, 1064, 1331, 281, 360, 341, 5633, 13, 583, 51392], "temperature": 0.0, "avg_logprob": -0.1215414090102978, "compression_ratio": 1.9124293785310735, "no_speech_prob": 0.0019873648416250944}, {"id": 911, "seek": 479224, "start": 4812.8, "end": 4816.88, "text": " you can maybe where they're starting to get three and four and five year olds to do this task for", "tokens": [51392, 291, 393, 1310, 689, 436, 434, 2891, 281, 483, 1045, 293, 1451, 293, 1732, 1064, 41972, 281, 360, 341, 5633, 337, 51596], "temperature": 0.0, "avg_logprob": -0.1215414090102978, "compression_ratio": 1.9124293785310735, "no_speech_prob": 0.0019873648416250944}, {"id": 912, "seek": 479224, "start": 4816.88, "end": 4821.36, "text": " short periods. And it looks like it's there pretty early. So clearly when you lead up to you,", "tokens": [51596, 2099, 13804, 13, 400, 309, 1542, 411, 309, 311, 456, 1238, 2440, 13, 407, 4448, 562, 291, 1477, 493, 281, 291, 11, 51820], "temperature": 0.0, "avg_logprob": -0.1215414090102978, "compression_ratio": 1.9124293785310735, "no_speech_prob": 0.0019873648416250944}, {"id": 913, "seek": 482136, "start": 4821.44, "end": 4828.16, "text": " like a baby's first words, before that, there's a lot of fascinating turmoil going on about like", "tokens": [50368, 411, 257, 3186, 311, 700, 2283, 11, 949, 300, 11, 456, 311, 257, 688, 295, 10343, 44554, 516, 322, 466, 411, 50704], "temperature": 0.0, "avg_logprob": -0.14584965176052517, "compression_ratio": 1.65, "no_speech_prob": 0.0018099506851285696}, {"id": 914, "seek": 482136, "start": 4828.16, "end": 4833.679999999999, "text": " figuring out like, what are, what are these people saying? And you're trying to like make", "tokens": [50704, 15213, 484, 411, 11, 437, 366, 11, 437, 366, 613, 561, 1566, 30, 400, 291, 434, 1382, 281, 411, 652, 50980], "temperature": 0.0, "avg_logprob": -0.14584965176052517, "compression_ratio": 1.65, "no_speech_prob": 0.0018099506851285696}, {"id": 915, "seek": 482136, "start": 4833.679999999999, "end": 4837.759999999999, "text": " sense, how does that connect to the world and all that kind of stuff. Yeah. That might be just", "tokens": [50980, 2020, 11, 577, 775, 300, 1745, 281, 264, 1002, 293, 439, 300, 733, 295, 1507, 13, 865, 13, 663, 1062, 312, 445, 51184], "temperature": 0.0, "avg_logprob": -0.14584965176052517, "compression_ratio": 1.65, "no_speech_prob": 0.0018099506851285696}, {"id": 916, "seek": 482136, "start": 4837.759999999999, "end": 4842.4, "text": " fascinating development that's happening there. That's hard to introspect. But anyway, you,", "tokens": [51184, 10343, 3250, 300, 311, 2737, 456, 13, 663, 311, 1152, 281, 560, 28713, 13, 583, 4033, 11, 291, 11, 51416], "temperature": 0.0, "avg_logprob": -0.14584965176052517, "compression_ratio": 1.65, "no_speech_prob": 0.0018099506851285696}, {"id": 917, "seek": 482136, "start": 4842.4, "end": 4848.4, "text": " we're back to the scanner and I can find my network in 15 minutes. And now we can ask a,", "tokens": [51416, 321, 434, 646, 281, 264, 30211, 293, 286, 393, 915, 452, 3209, 294, 2119, 2077, 13, 400, 586, 321, 393, 1029, 257, 11, 51716], "temperature": 0.0, "avg_logprob": -0.14584965176052517, "compression_ratio": 1.65, "no_speech_prob": 0.0018099506851285696}, {"id": 918, "seek": 484840, "start": 4849.04, "end": 4853.5199999999995, "text": " find my network, find yours, find, you know, 20 other people do this task. And we can do some", "tokens": [50396, 915, 452, 3209, 11, 915, 6342, 11, 915, 11, 291, 458, 11, 945, 661, 561, 360, 341, 5633, 13, 400, 321, 393, 360, 512, 50620], "temperature": 0.0, "avg_logprob": -0.13228243047540839, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.01639850065112114}, {"id": 919, "seek": 484840, "start": 4853.5199999999995, "end": 4859.2, "text": " other tasks. Anything else you think is thinking of some other thing. I can do a spatial memory task.", "tokens": [50620, 661, 9608, 13, 11998, 1646, 291, 519, 307, 1953, 295, 512, 661, 551, 13, 286, 393, 360, 257, 23598, 4675, 5633, 13, 50904], "temperature": 0.0, "avg_logprob": -0.13228243047540839, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.01639850065112114}, {"id": 920, "seek": 484840, "start": 4859.2, "end": 4865.92, "text": " I can do a music perception task. I can do programming task if I program. Okay. I can do", "tokens": [50904, 286, 393, 360, 257, 1318, 12860, 5633, 13, 286, 393, 360, 9410, 5633, 498, 286, 1461, 13, 1033, 13, 286, 393, 360, 51240], "temperature": 0.0, "avg_logprob": -0.13228243047540839, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.01639850065112114}, {"id": 921, "seek": 484840, "start": 4866.48, "end": 4872.08, "text": " where I can like understand computer programs. And none of those tasks will tap the language", "tokens": [51268, 689, 286, 393, 411, 1223, 3820, 4268, 13, 400, 6022, 295, 729, 9608, 486, 5119, 264, 2856, 51548], "temperature": 0.0, "avg_logprob": -0.13228243047540839, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.01639850065112114}, {"id": 922, "seek": 484840, "start": 4872.08, "end": 4876.48, "text": " network at all, like at all. There's no overlap. They do, they're, they're highly activated in", "tokens": [51548, 3209, 412, 439, 11, 411, 412, 439, 13, 821, 311, 572, 19959, 13, 814, 360, 11, 436, 434, 11, 436, 434, 5405, 18157, 294, 51768], "temperature": 0.0, "avg_logprob": -0.13228243047540839, "compression_ratio": 1.8365758754863812, "no_speech_prob": 0.01639850065112114}, {"id": 923, "seek": 487648, "start": 4876.48, "end": 4881.599999999999, "text": " other parts of the brain. There's a, there's a bilateral network, which I think she tends to", "tokens": [50364, 661, 3166, 295, 264, 3567, 13, 821, 311, 257, 11, 456, 311, 257, 38772, 3209, 11, 597, 286, 519, 750, 12258, 281, 50620], "temperature": 0.0, "avg_logprob": -0.11572205180853186, "compression_ratio": 1.9621993127147765, "no_speech_prob": 0.011682543903589249}, {"id": 924, "seek": 487648, "start": 4881.599999999999, "end": 4885.5199999999995, "text": " call the multiple demands network, which does anything kind of hard and, and so anything that's", "tokens": [50620, 818, 264, 3866, 15107, 3209, 11, 597, 775, 1340, 733, 295, 1152, 293, 11, 293, 370, 1340, 300, 311, 50816], "temperature": 0.0, "avg_logprob": -0.11572205180853186, "compression_ratio": 1.9621993127147765, "no_speech_prob": 0.011682543903589249}, {"id": 925, "seek": 487648, "start": 4885.5199999999995, "end": 4891.28, "text": " kind of difficult in some ways will activate that multiple demands network. I mean, music will be", "tokens": [50816, 733, 295, 2252, 294, 512, 2098, 486, 13615, 300, 3866, 15107, 3209, 13, 286, 914, 11, 1318, 486, 312, 51104], "temperature": 0.0, "avg_logprob": -0.11572205180853186, "compression_ratio": 1.9621993127147765, "no_speech_prob": 0.011682543903589249}, {"id": 926, "seek": 487648, "start": 4891.28, "end": 4895.5199999999995, "text": " in some music area, you know, there's music specific kinds of areas. And so, but they're,", "tokens": [51104, 294, 512, 1318, 1859, 11, 291, 458, 11, 456, 311, 1318, 2685, 3685, 295, 3179, 13, 400, 370, 11, 457, 436, 434, 11, 51316], "temperature": 0.0, "avg_logprob": -0.11572205180853186, "compression_ratio": 1.9621993127147765, "no_speech_prob": 0.011682543903589249}, {"id": 927, "seek": 487648, "start": 4896.08, "end": 4901.5199999999995, "text": " but, but none of them are activating the language area at all, unless there's words, like, so if", "tokens": [51344, 457, 11, 457, 6022, 295, 552, 366, 42481, 264, 2856, 1859, 412, 439, 11, 5969, 456, 311, 2283, 11, 411, 11, 370, 498, 51616], "temperature": 0.0, "avg_logprob": -0.11572205180853186, "compression_ratio": 1.9621993127147765, "no_speech_prob": 0.011682543903589249}, {"id": 928, "seek": 487648, "start": 4901.5199999999995, "end": 4905.679999999999, "text": " you have music and there's a song and you can hear the words then, then then you get the language", "tokens": [51616, 291, 362, 1318, 293, 456, 311, 257, 2153, 293, 291, 393, 1568, 264, 2283, 550, 11, 550, 550, 291, 483, 264, 2856, 51824], "temperature": 0.0, "avg_logprob": -0.11572205180853186, "compression_ratio": 1.9621993127147765, "no_speech_prob": 0.011682543903589249}, {"id": 929, "seek": 490568, "start": 4906.0, "end": 4910.400000000001, "text": " area. We're talking about speaking and listening, but are, or are we also talking about reading?", "tokens": [50380, 1859, 13, 492, 434, 1417, 466, 4124, 293, 4764, 11, 457, 366, 11, 420, 366, 321, 611, 1417, 466, 3760, 30, 50600], "temperature": 0.0, "avg_logprob": -0.14697514639960396, "compression_ratio": 1.9686411149825784, "no_speech_prob": 0.004068329930305481}, {"id": 930, "seek": 490568, "start": 4910.400000000001, "end": 4916.88, "text": " This is all comprehension of any kind. And so that is fast. So what this, this, this network", "tokens": [50600, 639, 307, 439, 44991, 295, 604, 733, 13, 400, 370, 300, 307, 2370, 13, 407, 437, 341, 11, 341, 11, 341, 3209, 50924], "temperature": 0.0, "avg_logprob": -0.14697514639960396, "compression_ratio": 1.9686411149825784, "no_speech_prob": 0.004068329930305481}, {"id": 931, "seek": 490568, "start": 4916.88, "end": 4921.280000000001, "text": " doesn't make any difference if it's written or spoken. So the, the, the thing that she calls,", "tokens": [50924, 1177, 380, 652, 604, 2649, 498, 309, 311, 3720, 420, 10759, 13, 407, 264, 11, 264, 11, 264, 551, 300, 750, 5498, 11, 51144], "temperature": 0.0, "avg_logprob": -0.14697514639960396, "compression_ratio": 1.9686411149825784, "no_speech_prob": 0.004068329930305481}, {"id": 932, "seek": 490568, "start": 4921.280000000001, "end": 4925.84, "text": " Federico calls the language network is this high level language. So it's not about the spoken,", "tokens": [51144, 45545, 2789, 5498, 264, 2856, 3209, 307, 341, 1090, 1496, 2856, 13, 407, 309, 311, 406, 466, 264, 10759, 11, 51372], "temperature": 0.0, "avg_logprob": -0.14697514639960396, "compression_ratio": 1.9686411149825784, "no_speech_prob": 0.004068329930305481}, {"id": 933, "seek": 490568, "start": 4925.84, "end": 4929.68, "text": " the spoken language, and it's not about the written language is about either one of them.", "tokens": [51372, 264, 10759, 2856, 11, 293, 309, 311, 406, 466, 264, 3720, 2856, 307, 466, 2139, 472, 295, 552, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14697514639960396, "compression_ratio": 1.9686411149825784, "no_speech_prob": 0.004068329930305481}, {"id": 934, "seek": 490568, "start": 4929.68, "end": 4933.360000000001, "text": " And so we're, so when you do speech, you're sort of list, you either, you're listening to speech", "tokens": [51564, 400, 370, 321, 434, 11, 370, 562, 291, 360, 6218, 11, 291, 434, 1333, 295, 1329, 11, 291, 2139, 11, 291, 434, 4764, 281, 6218, 51748], "temperature": 0.0, "avg_logprob": -0.14697514639960396, "compression_ratio": 1.9686411149825784, "no_speech_prob": 0.004068329930305481}, {"id": 935, "seek": 493336, "start": 4933.36, "end": 4937.28, "text": " and you'd, you know, subtract away some language you don't understand and see where it's, or you", "tokens": [50364, 293, 291, 1116, 11, 291, 458, 11, 16390, 1314, 512, 2856, 291, 500, 380, 1223, 293, 536, 689, 309, 311, 11, 420, 291, 50560], "temperature": 0.0, "avg_logprob": -0.19324766794840495, "compression_ratio": 1.825925925925926, "no_speech_prob": 0.00970559660345316}, {"id": 936, "seek": 493336, "start": 4937.28, "end": 4942.24, "text": " subtract away back, backward speech, which signs sounds like speech, but it isn't. And, and then", "tokens": [50560, 16390, 1314, 646, 11, 23897, 6218, 11, 597, 7880, 3263, 411, 6218, 11, 457, 309, 1943, 380, 13, 400, 11, 293, 550, 50808], "temperature": 0.0, "avg_logprob": -0.19324766794840495, "compression_ratio": 1.825925925925926, "no_speech_prob": 0.00970559660345316}, {"id": 937, "seek": 493336, "start": 4942.24, "end": 4948.48, "text": " so you take away the sound part altogether. And so, and then if you do written, you get exactly", "tokens": [50808, 370, 291, 747, 1314, 264, 1626, 644, 19051, 13, 400, 370, 11, 293, 550, 498, 291, 360, 3720, 11, 291, 483, 2293, 51120], "temperature": 0.0, "avg_logprob": -0.19324766794840495, "compression_ratio": 1.825925925925926, "no_speech_prob": 0.00970559660345316}, {"id": 938, "seek": 493336, "start": 4948.48, "end": 4953.599999999999, "text": " the same network. So for just reading the language versus reading sort of nonsense words or something", "tokens": [51120, 264, 912, 3209, 13, 407, 337, 445, 3760, 264, 2856, 5717, 3760, 1333, 295, 14925, 2283, 420, 746, 51376], "temperature": 0.0, "avg_logprob": -0.19324766794840495, "compression_ratio": 1.825925925925926, "no_speech_prob": 0.00970559660345316}, {"id": 939, "seek": 493336, "start": 4953.599999999999, "end": 4959.679999999999, "text": " like that, you'll find exactly the same network. And so it's just about high level, um, comprehension", "tokens": [51376, 411, 300, 11, 291, 603, 915, 2293, 264, 912, 3209, 13, 400, 370, 309, 311, 445, 466, 1090, 1496, 11, 1105, 11, 44991, 51680], "temperature": 0.0, "avg_logprob": -0.19324766794840495, "compression_ratio": 1.825925925925926, "no_speech_prob": 0.00970559660345316}, {"id": 940, "seek": 495968, "start": 4959.68, "end": 4962.96, "text": " of language. Yeah. In this case, and the same thing happened, production is a little harder to", "tokens": [50364, 295, 2856, 13, 865, 13, 682, 341, 1389, 11, 293, 264, 912, 551, 2011, 11, 4265, 307, 257, 707, 6081, 281, 50528], "temperature": 0.0, "avg_logprob": -0.14827473958333334, "compression_ratio": 2.0125, "no_speech_prob": 0.236275777220726}, {"id": 941, "seek": 495968, "start": 4962.96, "end": 4966.56, "text": " run the scanner, but the same thing happens in production. You get the same network. So production", "tokens": [50528, 1190, 264, 30211, 11, 457, 264, 912, 551, 2314, 294, 4265, 13, 509, 483, 264, 912, 3209, 13, 407, 4265, 50708], "temperature": 0.0, "avg_logprob": -0.14827473958333334, "compression_ratio": 2.0125, "no_speech_prob": 0.236275777220726}, {"id": 942, "seek": 495968, "start": 4966.56, "end": 4970.08, "text": " is a little harder. You have to figure out how do you run a task, you know, in the network such", "tokens": [50708, 307, 257, 707, 6081, 13, 509, 362, 281, 2573, 484, 577, 360, 291, 1190, 257, 5633, 11, 291, 458, 11, 294, 264, 3209, 1270, 50884], "temperature": 0.0, "avg_logprob": -0.14827473958333334, "compression_ratio": 2.0125, "no_speech_prob": 0.236275777220726}, {"id": 943, "seek": 495968, "start": 4970.08, "end": 4973.04, "text": " that you're doing some kind of production. And I can't remember what, they've done a bunch of", "tokens": [50884, 300, 291, 434, 884, 512, 733, 295, 4265, 13, 400, 286, 393, 380, 1604, 437, 11, 436, 600, 1096, 257, 3840, 295, 51032], "temperature": 0.0, "avg_logprob": -0.14827473958333334, "compression_ratio": 2.0125, "no_speech_prob": 0.236275777220726}, {"id": 944, "seek": 495968, "start": 4973.04, "end": 4977.76, "text": " different kinds of tasks there where you get people to, you know, produce things. Yeah. Figure", "tokens": [51032, 819, 3685, 295, 9608, 456, 689, 291, 483, 561, 281, 11, 291, 458, 11, 5258, 721, 13, 865, 13, 43225, 51268], "temperature": 0.0, "avg_logprob": -0.14827473958333334, "compression_ratio": 2.0125, "no_speech_prob": 0.236275777220726}, {"id": 945, "seek": 495968, "start": 4977.76, "end": 4982.320000000001, "text": " out how to produce and the same network goes on there. Exactly the same place. And so if, wait,", "tokens": [51268, 484, 577, 281, 5258, 293, 264, 912, 3209, 1709, 322, 456, 13, 7587, 264, 912, 1081, 13, 400, 370, 498, 11, 1699, 11, 51496], "temperature": 0.0, "avg_logprob": -0.14827473958333334, "compression_ratio": 2.0125, "no_speech_prob": 0.236275777220726}, {"id": 946, "seek": 495968, "start": 4982.320000000001, "end": 4986.08, "text": " wait, so if you read random words, yeah, if you read things like, um,", "tokens": [51496, 1699, 11, 370, 498, 291, 1401, 4974, 2283, 11, 1338, 11, 498, 291, 1401, 721, 411, 11, 1105, 11, 51684], "temperature": 0.0, "avg_logprob": -0.14827473958333334, "compression_ratio": 2.0125, "no_speech_prob": 0.236275777220726}, {"id": 947, "seek": 498608, "start": 4986.88, "end": 4990.48, "text": " like gibberish. Yeah. Yeah. Lewis Carroll's, Twas Brillig,", "tokens": [50404, 411, 4553, 43189, 13, 865, 13, 865, 13, 17412, 48456, 311, 11, 2574, 296, 1603, 373, 328, 11, 50584], "temperature": 0.0, "avg_logprob": -0.21817290207435344, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.01001026388257742}, {"id": 948, "seek": 498608, "start": 4990.48, "end": 4995.04, "text": " Jabberwocky, right? They call that Jabberwocky speech. The network doesn't get activated.", "tokens": [50584, 40319, 607, 86, 1560, 88, 11, 558, 30, 814, 818, 300, 40319, 607, 86, 1560, 88, 6218, 13, 440, 3209, 1177, 380, 483, 18157, 13, 50812], "temperature": 0.0, "avg_logprob": -0.21817290207435344, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.01001026388257742}, {"id": 949, "seek": 498608, "start": 4995.04, "end": 4999.36, "text": " Not as much. There are words in there. Because it's like, there's function words and stuff.", "tokens": [50812, 1726, 382, 709, 13, 821, 366, 2283, 294, 456, 13, 1436, 309, 311, 411, 11, 456, 311, 2445, 2283, 293, 1507, 13, 51028], "temperature": 0.0, "avg_logprob": -0.21817290207435344, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.01001026388257742}, {"id": 950, "seek": 498608, "start": 4999.36, "end": 5004.72, "text": " So it's lower activation. Yeah. Yeah. So there's like, basically the more language like it is,", "tokens": [51028, 407, 309, 311, 3126, 24433, 13, 865, 13, 865, 13, 407, 456, 311, 411, 11, 1936, 264, 544, 2856, 411, 309, 307, 11, 51296], "temperature": 0.0, "avg_logprob": -0.21817290207435344, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.01001026388257742}, {"id": 951, "seek": 498608, "start": 5004.72, "end": 5010.48, "text": " the higher it goes in the language network. And that network is there from when you speak from,", "tokens": [51296, 264, 2946, 309, 1709, 294, 264, 2856, 3209, 13, 400, 300, 3209, 307, 456, 490, 562, 291, 1710, 490, 11, 51584], "temperature": 0.0, "avg_logprob": -0.21817290207435344, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.01001026388257742}, {"id": 952, "seek": 498608, "start": 5010.48, "end": 5015.28, "text": " as soon as you learn language. And, and it's, it's there, like you speak multiple languages,", "tokens": [51584, 382, 2321, 382, 291, 1466, 2856, 13, 400, 11, 293, 309, 311, 11, 309, 311, 456, 11, 411, 291, 1710, 3866, 8650, 11, 51824], "temperature": 0.0, "avg_logprob": -0.21817290207435344, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.01001026388257742}, {"id": 953, "seek": 501528, "start": 5015.28, "end": 5019.36, "text": " the same network is going for your multiple languages. So you speak English, you speak Russian,", "tokens": [50364, 264, 912, 3209, 307, 516, 337, 428, 3866, 8650, 13, 407, 291, 1710, 3669, 11, 291, 1710, 7220, 11, 50568], "temperature": 0.0, "avg_logprob": -0.13997502698965952, "compression_ratio": 1.8417508417508417, "no_speech_prob": 0.007343749515712261}, {"id": 954, "seek": 501528, "start": 5020.24, "end": 5024.4, "text": " both of them are hitting that same network. If you, if you're affluent in those languages.", "tokens": [50612, 1293, 295, 552, 366, 8850, 300, 912, 3209, 13, 759, 291, 11, 498, 291, 434, 2096, 43518, 294, 729, 8650, 13, 50820], "temperature": 0.0, "avg_logprob": -0.13997502698965952, "compression_ratio": 1.8417508417508417, "no_speech_prob": 0.007343749515712261}, {"id": 955, "seek": 501528, "start": 5024.4, "end": 5029.28, "text": " So programming. Not at all. Isn't that amazing? Even if you're a really good programmer,", "tokens": [50820, 407, 9410, 13, 1726, 412, 439, 13, 6998, 380, 300, 2243, 30, 2754, 498, 291, 434, 257, 534, 665, 32116, 11, 51064], "temperature": 0.0, "avg_logprob": -0.13997502698965952, "compression_ratio": 1.8417508417508417, "no_speech_prob": 0.007343749515712261}, {"id": 956, "seek": 501528, "start": 5029.28, "end": 5034.8, "text": " that is not a human language. It's just not conveying the same information. And so it is", "tokens": [51064, 300, 307, 406, 257, 1952, 2856, 13, 467, 311, 445, 406, 18053, 1840, 264, 912, 1589, 13, 400, 370, 309, 307, 51340], "temperature": 0.0, "avg_logprob": -0.13997502698965952, "compression_ratio": 1.8417508417508417, "no_speech_prob": 0.007343749515712261}, {"id": 957, "seek": 501528, "start": 5034.8, "end": 5038.8, "text": " not in the language network. And so that has mind blowing as I think. That's pretty cool.", "tokens": [51340, 406, 294, 264, 2856, 3209, 13, 400, 370, 300, 575, 1575, 15068, 382, 286, 519, 13, 663, 311, 1238, 1627, 13, 51540], "temperature": 0.0, "avg_logprob": -0.13997502698965952, "compression_ratio": 1.8417508417508417, "no_speech_prob": 0.007343749515712261}, {"id": 958, "seek": 501528, "start": 5038.8, "end": 5043.28, "text": " That's weird. It is amazing. And so that's like one set of day. This is hers like shows that", "tokens": [51540, 663, 311, 3657, 13, 467, 307, 2243, 13, 400, 370, 300, 311, 411, 472, 992, 295, 786, 13, 639, 307, 6820, 411, 3110, 300, 51764], "temperature": 0.0, "avg_logprob": -0.13997502698965952, "compression_ratio": 1.8417508417508417, "no_speech_prob": 0.007343749515712261}, {"id": 959, "seek": 504328, "start": 5043.28, "end": 5048.48, "text": " what you might think is thinking is, is not language language is just the seek, just, just", "tokens": [50364, 437, 291, 1062, 519, 307, 1953, 307, 11, 307, 406, 2856, 2856, 307, 445, 264, 8075, 11, 445, 11, 445, 50624], "temperature": 0.0, "avg_logprob": -0.1637507506779262, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0024721550289541483}, {"id": 960, "seek": 504328, "start": 5048.48, "end": 5054.639999999999, "text": " this conventionalized system that we've worked out in human languages. Oh, another fascinating", "tokens": [50624, 341, 16011, 1602, 1185, 300, 321, 600, 2732, 484, 294, 1952, 8650, 13, 876, 11, 1071, 10343, 50932], "temperature": 0.0, "avg_logprob": -0.1637507506779262, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0024721550289541483}, {"id": 961, "seek": 504328, "start": 5054.639999999999, "end": 5061.679999999999, "text": " little bit tidbit is that even if they're these constructed languages, like Klingon, or I don't", "tokens": [50932, 707, 857, 9422, 5260, 307, 300, 754, 498, 436, 434, 613, 17083, 8650, 11, 411, 591, 1688, 266, 11, 420, 286, 500, 380, 51284], "temperature": 0.0, "avg_logprob": -0.1637507506779262, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0024721550289541483}, {"id": 962, "seek": 504328, "start": 5061.679999999999, "end": 5065.28, "text": " know the languages from Game of Thrones, I'm sorry, I don't remember those languages, maybe a lot of", "tokens": [51284, 458, 264, 8650, 490, 7522, 295, 31659, 11, 286, 478, 2597, 11, 286, 500, 380, 1604, 729, 8650, 11, 1310, 257, 688, 295, 51464], "temperature": 0.0, "avg_logprob": -0.1637507506779262, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0024721550289541483}, {"id": 963, "seek": 504328, "start": 5065.28, "end": 5069.44, "text": " people offended right now. There's people that speak those languages. They really speak those", "tokens": [51464, 561, 26776, 558, 586, 13, 821, 311, 561, 300, 1710, 729, 8650, 13, 814, 534, 1710, 729, 51672], "temperature": 0.0, "avg_logprob": -0.1637507506779262, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.0024721550289541483}, {"id": 964, "seek": 506944, "start": 5069.44, "end": 5076.0, "text": " languages because the people that wrote the languages for the shows, they did an amazing", "tokens": [50364, 8650, 570, 264, 561, 300, 4114, 264, 8650, 337, 264, 3110, 11, 436, 630, 364, 2243, 50692], "temperature": 0.0, "avg_logprob": -0.11227982715495581, "compression_ratio": 2.0480349344978164, "no_speech_prob": 0.009410951286554337}, {"id": 965, "seek": 506944, "start": 5076.0, "end": 5080.96, "text": " job of constructing on something like a human language. And those that, that lights up the", "tokens": [50692, 1691, 295, 39969, 322, 746, 411, 257, 1952, 2856, 13, 400, 729, 300, 11, 300, 5811, 493, 264, 50940], "temperature": 0.0, "avg_logprob": -0.11227982715495581, "compression_ratio": 2.0480349344978164, "no_speech_prob": 0.009410951286554337}, {"id": 966, "seek": 506944, "start": 5080.96, "end": 5086.08, "text": " language area. That's like, because they can speak, you know, pretty much arbitrary thoughts", "tokens": [50940, 2856, 1859, 13, 663, 311, 411, 11, 570, 436, 393, 1710, 11, 291, 458, 11, 1238, 709, 23211, 4598, 51196], "temperature": 0.0, "avg_logprob": -0.11227982715495581, "compression_ratio": 2.0480349344978164, "no_speech_prob": 0.009410951286554337}, {"id": 967, "seek": 506944, "start": 5086.08, "end": 5090.639999999999, "text": " in a human language. It's not a, it's a constructed human language. Probably it's related to human", "tokens": [51196, 294, 257, 1952, 2856, 13, 467, 311, 406, 257, 11, 309, 311, 257, 17083, 1952, 2856, 13, 9210, 309, 311, 4077, 281, 1952, 51424], "temperature": 0.0, "avg_logprob": -0.11227982715495581, "compression_ratio": 2.0480349344978164, "no_speech_prob": 0.009410951286554337}, {"id": 968, "seek": 506944, "start": 5090.639999999999, "end": 5094.32, "text": " languages because the people that were constructing them were making them like human languages in", "tokens": [51424, 8650, 570, 264, 561, 300, 645, 39969, 552, 645, 1455, 552, 411, 1952, 8650, 294, 51608], "temperature": 0.0, "avg_logprob": -0.11227982715495581, "compression_ratio": 2.0480349344978164, "no_speech_prob": 0.009410951286554337}, {"id": 969, "seek": 509432, "start": 5094.32, "end": 5099.599999999999, "text": " various ways, but it also activates the same network, which is pretty, pretty cool. Anyway.", "tokens": [50364, 3683, 2098, 11, 457, 309, 611, 43869, 264, 912, 3209, 11, 597, 307, 1238, 11, 1238, 1627, 13, 5684, 13, 50628], "temperature": 0.0, "avg_logprob": -0.14099717962330785, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.11430498212575912}, {"id": 970, "seek": 509432, "start": 5099.599999999999, "end": 5104.799999999999, "text": " Sorry to go into a place where you may be a little bit philosophical, but is it possible", "tokens": [50628, 4919, 281, 352, 666, 257, 1081, 689, 291, 815, 312, 257, 707, 857, 25066, 11, 457, 307, 309, 1944, 50888], "temperature": 0.0, "avg_logprob": -0.14099717962330785, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.11430498212575912}, {"id": 971, "seek": 509432, "start": 5105.44, "end": 5110.0, "text": " that this area of the brain is doing some kind of translation into a deeper set of", "tokens": [50920, 300, 341, 1859, 295, 264, 3567, 307, 884, 512, 733, 295, 12853, 666, 257, 7731, 992, 295, 51148], "temperature": 0.0, "avg_logprob": -0.14099717962330785, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.11430498212575912}, {"id": 972, "seek": 509432, "start": 5111.2, "end": 5117.84, "text": " almost like concepts? It has to be doing. So it's doing in communication, right? It is translating", "tokens": [51208, 1920, 411, 10392, 30, 467, 575, 281, 312, 884, 13, 407, 309, 311, 884, 294, 6101, 11, 558, 30, 467, 307, 35030, 51540], "temperature": 0.0, "avg_logprob": -0.14099717962330785, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.11430498212575912}, {"id": 973, "seek": 509432, "start": 5118.4, "end": 5122.96, "text": " from thought, whatever that is, is more abstract, and it's doing that. That's what it's doing. Like,", "tokens": [51568, 490, 1194, 11, 2035, 300, 307, 11, 307, 544, 12649, 11, 293, 309, 311, 884, 300, 13, 663, 311, 437, 309, 311, 884, 13, 1743, 11, 51796], "temperature": 0.0, "avg_logprob": -0.14099717962330785, "compression_ratio": 1.6714801444043321, "no_speech_prob": 0.11430498212575912}, {"id": 974, "seek": 512296, "start": 5122.96, "end": 5126.56, "text": " it is, that is kind of what it is doing. It's like kind of a meaning network, I guess.", "tokens": [50364, 309, 307, 11, 300, 307, 733, 295, 437, 309, 307, 884, 13, 467, 311, 411, 733, 295, 257, 3620, 3209, 11, 286, 2041, 13, 50544], "temperature": 0.0, "avg_logprob": -0.17677160719750631, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.0017005524132400751}, {"id": 975, "seek": 512296, "start": 5127.36, "end": 5131.12, "text": " Yeah, like a translation network. Yeah. But I wonder what is at the core,", "tokens": [50584, 865, 11, 411, 257, 12853, 3209, 13, 865, 13, 583, 286, 2441, 437, 307, 412, 264, 4965, 11, 50772], "temperature": 0.0, "avg_logprob": -0.17677160719750631, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.0017005524132400751}, {"id": 976, "seek": 512296, "start": 5131.12, "end": 5137.12, "text": " at the bottom of it? Like, what are thoughts? Are they thoughts? To me, like, thoughts and words,", "tokens": [50772, 412, 264, 2767, 295, 309, 30, 1743, 11, 437, 366, 4598, 30, 2014, 436, 4598, 30, 1407, 385, 11, 411, 11, 4598, 293, 2283, 11, 51072], "temperature": 0.0, "avg_logprob": -0.17677160719750631, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.0017005524132400751}, {"id": 977, "seek": 512296, "start": 5137.12, "end": 5142.96, "text": " are they neighbors or are, is it one turtle sitting on top of the other? Meaning, like, is there a", "tokens": [51072, 366, 436, 12512, 420, 366, 11, 307, 309, 472, 22866, 3798, 322, 1192, 295, 264, 661, 30, 19948, 11, 411, 11, 307, 456, 257, 51364], "temperature": 0.0, "avg_logprob": -0.17677160719750631, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.0017005524132400751}, {"id": 978, "seek": 512296, "start": 5142.96, "end": 5149.68, "text": " deep set of concepts that we... Well, there's connections right between those, what these", "tokens": [51364, 2452, 992, 295, 10392, 300, 321, 485, 1042, 11, 456, 311, 9271, 558, 1296, 729, 11, 437, 613, 51700], "temperature": 0.0, "avg_logprob": -0.17677160719750631, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.0017005524132400751}, {"id": 979, "seek": 512296, "start": 5149.68, "end": 5152.72, "text": " things mean. And then there's probably other parts of the brain that what these things mean.", "tokens": [51700, 721, 914, 13, 400, 550, 456, 311, 1391, 661, 3166, 295, 264, 3567, 300, 437, 613, 721, 914, 13, 51852], "temperature": 0.0, "avg_logprob": -0.17677160719750631, "compression_ratio": 1.8556701030927836, "no_speech_prob": 0.0017005524132400751}, {"id": 980, "seek": 515272, "start": 5152.8, "end": 5158.4800000000005, "text": " And so, you know, when I'm talking about whatever it is I want to talk about, it'll be represented", "tokens": [50368, 400, 370, 11, 291, 458, 11, 562, 286, 478, 1417, 466, 2035, 309, 307, 286, 528, 281, 751, 466, 11, 309, 603, 312, 10379, 50652], "temperature": 0.0, "avg_logprob": -0.1316674830866795, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.000954562914557755}, {"id": 981, "seek": 515272, "start": 5158.4800000000005, "end": 5162.64, "text": " somewhere else. That knowledge of whatever that is will be represented somewhere else.", "tokens": [50652, 4079, 1646, 13, 663, 3601, 295, 2035, 300, 307, 486, 312, 10379, 4079, 1646, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1316674830866795, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.000954562914557755}, {"id": 982, "seek": 515272, "start": 5162.64, "end": 5167.4400000000005, "text": " Well, I wonder if there's like some stable, nicely compressed encoding of meanings", "tokens": [50860, 1042, 11, 286, 2441, 498, 456, 311, 411, 512, 8351, 11, 9594, 30353, 43430, 295, 28138, 51100], "temperature": 0.0, "avg_logprob": -0.1316674830866795, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.000954562914557755}, {"id": 983, "seek": 515272, "start": 5168.240000000001, "end": 5178.08, "text": " that's separate from language. I guess the implication here is that we don't think in", "tokens": [51140, 300, 311, 4994, 490, 2856, 13, 286, 2041, 264, 37814, 510, 307, 300, 321, 500, 380, 519, 294, 51632], "temperature": 0.0, "avg_logprob": -0.1316674830866795, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.000954562914557755}, {"id": 984, "seek": 515272, "start": 5178.08, "end": 5182.4800000000005, "text": " language. That's correct. Isn't that cool? And that's so interesting.", "tokens": [51632, 2856, 13, 663, 311, 3006, 13, 6998, 380, 300, 1627, 30, 400, 300, 311, 370, 1880, 13, 51852], "temperature": 0.0, "avg_logprob": -0.1316674830866795, "compression_ratio": 1.7377049180327868, "no_speech_prob": 0.000954562914557755}, {"id": 985, "seek": 518248, "start": 5182.48, "end": 5187.12, "text": " So people, I mean, this is like hard to do experiments on, but there is this idea of", "tokens": [50364, 407, 561, 11, 286, 914, 11, 341, 307, 411, 1152, 281, 360, 12050, 322, 11, 457, 456, 307, 341, 1558, 295, 50596], "temperature": 0.0, "avg_logprob": -0.12974018673244997, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.0030744520481675863}, {"id": 986, "seek": 518248, "start": 5187.12, "end": 5192.32, "text": " inner voice and a lot of people have an inner voice. And so, if you do a poll on the internet", "tokens": [50596, 7284, 3177, 293, 257, 688, 295, 561, 362, 364, 7284, 3177, 13, 400, 370, 11, 498, 291, 360, 257, 6418, 322, 264, 4705, 50856], "temperature": 0.0, "avg_logprob": -0.12974018673244997, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.0030744520481675863}, {"id": 987, "seek": 518248, "start": 5192.32, "end": 5195.2, "text": " and ask if you hear yourself talking when you're just thinking or whatever,", "tokens": [50856, 293, 1029, 498, 291, 1568, 1803, 1417, 562, 291, 434, 445, 1953, 420, 2035, 11, 51000], "temperature": 0.0, "avg_logprob": -0.12974018673244997, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.0030744520481675863}, {"id": 988, "seek": 518248, "start": 5195.839999999999, "end": 5202.48, "text": " about 70 or 80% of people will say yes. Most people have an inner voice. I don't. And so,", "tokens": [51032, 466, 5285, 420, 4688, 4, 295, 561, 486, 584, 2086, 13, 4534, 561, 362, 364, 7284, 3177, 13, 286, 500, 380, 13, 400, 370, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12974018673244997, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.0030744520481675863}, {"id": 989, "seek": 518248, "start": 5202.48, "end": 5205.919999999999, "text": " I always find this strange when... So, when people talk about an inner voice, I always", "tokens": [51364, 286, 1009, 915, 341, 5861, 562, 485, 407, 11, 562, 561, 751, 466, 364, 7284, 3177, 11, 286, 1009, 51536], "temperature": 0.0, "avg_logprob": -0.12974018673244997, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.0030744520481675863}, {"id": 990, "seek": 518248, "start": 5205.919999999999, "end": 5211.5199999999995, "text": " thought this was a metaphor. And they hear, I know most of you, whoever's listening to this,", "tokens": [51536, 1194, 341, 390, 257, 19157, 13, 400, 436, 1568, 11, 286, 458, 881, 295, 291, 11, 11387, 311, 4764, 281, 341, 11, 51816], "temperature": 0.0, "avg_logprob": -0.12974018673244997, "compression_ratio": 1.8257839721254356, "no_speech_prob": 0.0030744520481675863}, {"id": 991, "seek": 521152, "start": 5211.52, "end": 5215.92, "text": " thinks I'm crazy now because I don't have an inner voice. And I just don't know what you're", "tokens": [50364, 7309, 286, 478, 3219, 586, 570, 286, 500, 380, 362, 364, 7284, 3177, 13, 400, 286, 445, 500, 380, 458, 437, 291, 434, 50584], "temperature": 0.0, "avg_logprob": -0.15731114607590896, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.005554179195314646}, {"id": 992, "seek": 521152, "start": 5215.92, "end": 5222.080000000001, "text": " listening to. It sounds so kind of annoying to me, but to have this voice going on while you're", "tokens": [50584, 4764, 281, 13, 467, 3263, 370, 733, 295, 11304, 281, 385, 11, 457, 281, 362, 341, 3177, 516, 322, 1339, 291, 434, 50892], "temperature": 0.0, "avg_logprob": -0.15731114607590896, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.005554179195314646}, {"id": 993, "seek": 521152, "start": 5222.080000000001, "end": 5227.84, "text": " thinking, but I guess most people have that. And I don't have that. And we don't really know what", "tokens": [50892, 1953, 11, 457, 286, 2041, 881, 561, 362, 300, 13, 400, 286, 500, 380, 362, 300, 13, 400, 321, 500, 380, 534, 458, 437, 51180], "temperature": 0.0, "avg_logprob": -0.15731114607590896, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.005554179195314646}, {"id": 994, "seek": 521152, "start": 5227.84, "end": 5232.56, "text": " that connects to. I wonder if the inner voice activates that same network. I don't know.", "tokens": [51180, 300, 16967, 281, 13, 286, 2441, 498, 264, 7284, 3177, 43869, 300, 912, 3209, 13, 286, 500, 380, 458, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15731114607590896, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.005554179195314646}, {"id": 995, "seek": 521152, "start": 5233.52, "end": 5237.52, "text": " I don't know. I mean, this could be speechy, right? So that's like, you hear, do you have an inner", "tokens": [51464, 286, 500, 380, 458, 13, 286, 914, 11, 341, 727, 312, 6218, 88, 11, 558, 30, 407, 300, 311, 411, 11, 291, 1568, 11, 360, 291, 362, 364, 7284, 51664], "temperature": 0.0, "avg_logprob": -0.15731114607590896, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.005554179195314646}, {"id": 996, "seek": 523752, "start": 5237.52, "end": 5244.240000000001, "text": " voice? I don't think so. A lot of people have this sense that they hear themselves. And then,", "tokens": [50364, 3177, 30, 286, 500, 380, 519, 370, 13, 316, 688, 295, 561, 362, 341, 2020, 300, 436, 1568, 2969, 13, 400, 550, 11, 50700], "temperature": 0.0, "avg_logprob": -0.09822627381011323, "compression_ratio": 2.033707865168539, "no_speech_prob": 0.09530147165060043}, {"id": 997, "seek": 523752, "start": 5244.240000000001, "end": 5248.0, "text": " say they read someone's email, I've heard people tell me that they hear that other person's voice", "tokens": [50700, 584, 436, 1401, 1580, 311, 3796, 11, 286, 600, 2198, 561, 980, 385, 300, 436, 1568, 300, 661, 954, 311, 3177, 50888], "temperature": 0.0, "avg_logprob": -0.09822627381011323, "compression_ratio": 2.033707865168539, "no_speech_prob": 0.09530147165060043}, {"id": 998, "seek": 523752, "start": 5248.0, "end": 5253.68, "text": " when they read other people's emails. And I'm like, wow, that sounds so disruptive.", "tokens": [50888, 562, 436, 1401, 661, 561, 311, 12524, 13, 400, 286, 478, 411, 11, 6076, 11, 300, 3263, 370, 37865, 13, 51172], "temperature": 0.0, "avg_logprob": -0.09822627381011323, "compression_ratio": 2.033707865168539, "no_speech_prob": 0.09530147165060043}, {"id": 999, "seek": 523752, "start": 5253.68, "end": 5258.160000000001, "text": " I do think I like vocalize when I'm reading, but I don't think I hear a voice.", "tokens": [51172, 286, 360, 519, 286, 411, 11657, 1125, 562, 286, 478, 3760, 11, 457, 286, 500, 380, 519, 286, 1568, 257, 3177, 13, 51396], "temperature": 0.0, "avg_logprob": -0.09822627381011323, "compression_ratio": 2.033707865168539, "no_speech_prob": 0.09530147165060043}, {"id": 1000, "seek": 523752, "start": 5258.8, "end": 5260.88, "text": " Well, that's, you probably don't have an inner voice. Yeah, I don't think I have an inner voice.", "tokens": [51428, 1042, 11, 300, 311, 11, 291, 1391, 500, 380, 362, 364, 7284, 3177, 13, 865, 11, 286, 500, 380, 519, 286, 362, 364, 7284, 3177, 13, 51532], "temperature": 0.0, "avg_logprob": -0.09822627381011323, "compression_ratio": 2.033707865168539, "no_speech_prob": 0.09530147165060043}, {"id": 1001, "seek": 523752, "start": 5260.88, "end": 5266.64, "text": " People have an inner voice. People have this strong percept of hearing sound in their heads", "tokens": [51532, 3432, 362, 364, 7284, 3177, 13, 3432, 362, 341, 2068, 43276, 295, 4763, 1626, 294, 641, 8050, 51820], "temperature": 0.0, "avg_logprob": -0.09822627381011323, "compression_ratio": 2.033707865168539, "no_speech_prob": 0.09530147165060043}, {"id": 1002, "seek": 526664, "start": 5266.64, "end": 5270.240000000001, "text": " when they're just thinking. I refuse to believe that's the majority of people.", "tokens": [50364, 562, 436, 434, 445, 1953, 13, 286, 16791, 281, 1697, 300, 311, 264, 6286, 295, 561, 13, 50544], "temperature": 0.0, "avg_logprob": -0.16477668383889946, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.0017544341972097754}, {"id": 1003, "seek": 526664, "start": 5270.240000000001, "end": 5274.8, "text": " Majority, absolutely. What? It's like two-thirds or three-quarters. It's not.", "tokens": [50544, 15581, 507, 11, 3122, 13, 708, 30, 467, 311, 411, 732, 12, 38507, 420, 1045, 12, 17313, 13, 467, 311, 406, 13, 50772], "temperature": 0.0, "avg_logprob": -0.16477668383889946, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.0017544341972097754}, {"id": 1004, "seek": 526664, "start": 5274.8, "end": 5279.92, "text": " I would never ask class. And I went internet, they always say that. So you're in a minority.", "tokens": [50772, 286, 576, 1128, 1029, 1508, 13, 400, 286, 1437, 4705, 11, 436, 1009, 584, 300, 13, 407, 291, 434, 294, 257, 16166, 13, 51028], "temperature": 0.0, "avg_logprob": -0.16477668383889946, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.0017544341972097754}, {"id": 1005, "seek": 526664, "start": 5279.92, "end": 5286.08, "text": " It could be a self-report flaw. It could be. You know, when I'm reading inside my head,", "tokens": [51028, 467, 727, 312, 257, 2698, 12, 265, 2707, 13717, 13, 467, 727, 312, 13, 509, 458, 11, 562, 286, 478, 3760, 1854, 452, 1378, 11, 51336], "temperature": 0.0, "avg_logprob": -0.16477668383889946, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.0017544341972097754}, {"id": 1006, "seek": 526664, "start": 5287.52, "end": 5293.68, "text": " I'm kind of like saying the words, which is probably the wrong way to read, but I don't", "tokens": [51408, 286, 478, 733, 295, 411, 1566, 264, 2283, 11, 597, 307, 1391, 264, 2085, 636, 281, 1401, 11, 457, 286, 500, 380, 51716], "temperature": 0.0, "avg_logprob": -0.16477668383889946, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.0017544341972097754}, {"id": 1007, "seek": 529368, "start": 5293.68, "end": 5299.68, "text": " hear a voice. There's no percept of a voice. I don't refuse to believe the majority of people", "tokens": [50364, 1568, 257, 3177, 13, 821, 311, 572, 43276, 295, 257, 3177, 13, 286, 500, 380, 16791, 281, 1697, 264, 6286, 295, 561, 50664], "temperature": 0.0, "avg_logprob": -0.1666512688000997, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.0018099065637215972}, {"id": 1008, "seek": 529368, "start": 5299.68, "end": 5304.16, "text": " have it. Anyway, it's a fascinating, the human brain is fascinating, but it still blew my mind", "tokens": [50664, 362, 309, 13, 5684, 11, 309, 311, 257, 10343, 11, 264, 1952, 3567, 307, 10343, 11, 457, 309, 920, 19075, 452, 1575, 50888], "temperature": 0.0, "avg_logprob": -0.1666512688000997, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.0018099065637215972}, {"id": 1009, "seek": 529368, "start": 5304.16, "end": 5311.280000000001, "text": " that the, that language does appear, comprehension does appear to be separate from thinking.", "tokens": [50888, 300, 264, 11, 300, 2856, 775, 4204, 11, 44991, 775, 4204, 281, 312, 4994, 490, 1953, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1666512688000997, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.0018099065637215972}, {"id": 1010, "seek": 529368, "start": 5312.56, "end": 5319.4400000000005, "text": " So that's one set. One set of data from Fedorenko's group is that no matter what task you do,", "tokens": [51308, 407, 300, 311, 472, 992, 13, 1485, 992, 295, 1412, 490, 7772, 10948, 4093, 311, 1594, 307, 300, 572, 1871, 437, 5633, 291, 360, 11, 51652], "temperature": 0.0, "avg_logprob": -0.1666512688000997, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.0018099065637215972}, {"id": 1011, "seek": 531944, "start": 5319.44, "end": 5323.839999999999, "text": " if it doesn't have words and combinations of words in it, then it won't light up the language", "tokens": [50364, 498, 309, 1177, 380, 362, 2283, 293, 21267, 295, 2283, 294, 309, 11, 550, 309, 1582, 380, 1442, 493, 264, 2856, 50584], "temperature": 0.0, "avg_logprob": -0.08418645926401125, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.015420136973261833}, {"id": 1012, "seek": 531944, "start": 5323.839999999999, "end": 5328.719999999999, "text": " network. You know, you could, it'll be active somewhere else, but not there. So that's one.", "tokens": [50584, 3209, 13, 509, 458, 11, 291, 727, 11, 309, 603, 312, 4967, 4079, 1646, 11, 457, 406, 456, 13, 407, 300, 311, 472, 13, 50828], "temperature": 0.0, "avg_logprob": -0.08418645926401125, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.015420136973261833}, {"id": 1013, "seek": 531944, "start": 5328.719999999999, "end": 5335.5199999999995, "text": " And then this other piece of evidence relevant to that question is, it turns out there are these,", "tokens": [50828, 400, 550, 341, 661, 2522, 295, 4467, 7340, 281, 300, 1168, 307, 11, 309, 4523, 484, 456, 366, 613, 11, 51168], "temperature": 0.0, "avg_logprob": -0.08418645926401125, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.015420136973261833}, {"id": 1014, "seek": 531944, "start": 5335.5199999999995, "end": 5341.04, "text": " this group of people who've had a massive stroke on the left side and wiped out their language", "tokens": [51168, 341, 1594, 295, 561, 567, 600, 632, 257, 5994, 12403, 322, 264, 1411, 1252, 293, 26879, 484, 641, 2856, 51444], "temperature": 0.0, "avg_logprob": -0.08418645926401125, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.015420136973261833}, {"id": 1015, "seek": 531944, "start": 5341.04, "end": 5344.96, "text": " network. And as long as they didn't wipe out everything on the right as well, in that case,", "tokens": [51444, 3209, 13, 400, 382, 938, 382, 436, 994, 380, 14082, 484, 1203, 322, 264, 558, 382, 731, 11, 294, 300, 1389, 11, 51640], "temperature": 0.0, "avg_logprob": -0.08418645926401125, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.015420136973261833}, {"id": 1016, "seek": 531944, "start": 5344.96, "end": 5348.96, "text": " they wouldn't be, you know, cognitively functionable. But if they just wiped out language,", "tokens": [51640, 436, 2759, 380, 312, 11, 291, 458, 11, 15605, 356, 2445, 712, 13, 583, 498, 436, 445, 26879, 484, 2856, 11, 51840], "temperature": 0.0, "avg_logprob": -0.08418645926401125, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.015420136973261833}, {"id": 1017, "seek": 534896, "start": 5348.96, "end": 5353.84, "text": " which is pretty tough to do because it's, it's very expansive on the left. But if they have,", "tokens": [50364, 597, 307, 1238, 4930, 281, 360, 570, 309, 311, 11, 309, 311, 588, 46949, 322, 264, 1411, 13, 583, 498, 436, 362, 11, 50608], "temperature": 0.0, "avg_logprob": -0.13033354974562122, "compression_ratio": 1.8366666666666667, "no_speech_prob": 0.005908576771616936}, {"id": 1018, "seek": 534896, "start": 5353.84, "end": 5359.76, "text": " then there are these, there's patients like this, so-called global aphasics, who can do", "tokens": [50608, 550, 456, 366, 613, 11, 456, 311, 4209, 411, 341, 11, 370, 12, 11880, 4338, 257, 7485, 1167, 11, 567, 393, 360, 50904], "temperature": 0.0, "avg_logprob": -0.13033354974562122, "compression_ratio": 1.8366666666666667, "no_speech_prob": 0.005908576771616936}, {"id": 1019, "seek": 534896, "start": 5360.4800000000005, "end": 5364.8, "text": " any task just fine, but not language. They can't, you can't talk to them. I mean,", "tokens": [50940, 604, 5633, 445, 2489, 11, 457, 406, 2856, 13, 814, 393, 380, 11, 291, 393, 380, 751, 281, 552, 13, 286, 914, 11, 51156], "temperature": 0.0, "avg_logprob": -0.13033354974562122, "compression_ratio": 1.8366666666666667, "no_speech_prob": 0.005908576771616936}, {"id": 1020, "seek": 534896, "start": 5365.36, "end": 5370.4800000000005, "text": " they don't understand you. They can't speak, can't write, they can't read, but they can do,", "tokens": [51184, 436, 500, 380, 1223, 291, 13, 814, 393, 380, 1710, 11, 393, 380, 2464, 11, 436, 393, 380, 1401, 11, 457, 436, 393, 360, 11, 51440], "temperature": 0.0, "avg_logprob": -0.13033354974562122, "compression_ratio": 1.8366666666666667, "no_speech_prob": 0.005908576771616936}, {"id": 1021, "seek": 534896, "start": 5370.4800000000005, "end": 5374.24, "text": " they can play chess, they can drive their cars, they can do all kinds of other stuff, you know,", "tokens": [51440, 436, 393, 862, 24122, 11, 436, 393, 3332, 641, 5163, 11, 436, 393, 360, 439, 3685, 295, 661, 1507, 11, 291, 458, 11, 51628], "temperature": 0.0, "avg_logprob": -0.13033354974562122, "compression_ratio": 1.8366666666666667, "no_speech_prob": 0.005908576771616936}, {"id": 1022, "seek": 534896, "start": 5374.24, "end": 5378.56, "text": " do math, they can do all, like, so math is not in the language area, for instance, you do arithmetic", "tokens": [51628, 360, 5221, 11, 436, 393, 360, 439, 11, 411, 11, 370, 5221, 307, 406, 294, 264, 2856, 1859, 11, 337, 5197, 11, 291, 360, 42973, 51844], "temperature": 0.0, "avg_logprob": -0.13033354974562122, "compression_ratio": 1.8366666666666667, "no_speech_prob": 0.005908576771616936}, {"id": 1023, "seek": 537856, "start": 5378.56, "end": 5382.56, "text": " and stuff. That's not language area. It's got symbols. So people sort of confuse some kind of", "tokens": [50364, 293, 1507, 13, 663, 311, 406, 2856, 1859, 13, 467, 311, 658, 16944, 13, 407, 561, 1333, 295, 28584, 512, 733, 295, 50564], "temperature": 0.0, "avg_logprob": -0.11638277473179161, "compression_ratio": 1.8089171974522293, "no_speech_prob": 0.0025504068471491337}, {"id": 1024, "seek": 537856, "start": 5382.56, "end": 5387.4400000000005, "text": " symbolic processing with language and symbolic processing is not the same. So there are symbols", "tokens": [50564, 25755, 9007, 365, 2856, 293, 25755, 9007, 307, 406, 264, 912, 13, 407, 456, 366, 16944, 50808], "temperature": 0.0, "avg_logprob": -0.11638277473179161, "compression_ratio": 1.8089171974522293, "no_speech_prob": 0.0025504068471491337}, {"id": 1025, "seek": 537856, "start": 5387.4400000000005, "end": 5391.76, "text": " and they have meaning, but it's not language. It's not a, you know, conventionalized language", "tokens": [50808, 293, 436, 362, 3620, 11, 457, 309, 311, 406, 2856, 13, 467, 311, 406, 257, 11, 291, 458, 11, 16011, 1602, 2856, 51024], "temperature": 0.0, "avg_logprob": -0.11638277473179161, "compression_ratio": 1.8089171974522293, "no_speech_prob": 0.0025504068471491337}, {"id": 1026, "seek": 537856, "start": 5391.76, "end": 5396.56, "text": " system. And so language, so math isn't there. And so they can do math. They're, they do just as well", "tokens": [51024, 1185, 13, 400, 370, 2856, 11, 370, 5221, 1943, 380, 456, 13, 400, 370, 436, 393, 360, 5221, 13, 814, 434, 11, 436, 360, 445, 382, 731, 51264], "temperature": 0.0, "avg_logprob": -0.11638277473179161, "compression_ratio": 1.8089171974522293, "no_speech_prob": 0.0025504068471491337}, {"id": 1027, "seek": 537856, "start": 5396.56, "end": 5401.120000000001, "text": " as their control, age match controls and all these tasks. This is Rosemary Varley over in", "tokens": [51264, 382, 641, 1969, 11, 3205, 2995, 9003, 293, 439, 613, 9608, 13, 639, 307, 11144, 37529, 14662, 3420, 670, 294, 51492], "temperature": 0.0, "avg_logprob": -0.11638277473179161, "compression_ratio": 1.8089171974522293, "no_speech_prob": 0.0025504068471491337}, {"id": 1028, "seek": 537856, "start": 5401.120000000001, "end": 5405.200000000001, "text": " University College London, who has a bunch of patients who, who she's shown this that they're", "tokens": [51492, 3535, 6745, 7042, 11, 567, 575, 257, 3840, 295, 4209, 567, 11, 567, 750, 311, 4898, 341, 300, 436, 434, 51696], "temperature": 0.0, "avg_logprob": -0.11638277473179161, "compression_ratio": 1.8089171974522293, "no_speech_prob": 0.0025504068471491337}, {"id": 1029, "seek": 540520, "start": 5405.2, "end": 5413.5199999999995, "text": " just, so that sort of combination suggests that language isn't necessary for thinking. It doesn't", "tokens": [50364, 445, 11, 370, 300, 1333, 295, 6562, 13409, 300, 2856, 1943, 380, 4818, 337, 1953, 13, 467, 1177, 380, 50780], "temperature": 0.0, "avg_logprob": -0.11236456344867575, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0023961577098816633}, {"id": 1030, "seek": 540520, "start": 5413.5199999999995, "end": 5417.679999999999, "text": " mean you can't think in language. You could think in language because language allows a lot of", "tokens": [50780, 914, 291, 393, 380, 519, 294, 2856, 13, 509, 727, 519, 294, 2856, 570, 2856, 4045, 257, 688, 295, 50988], "temperature": 0.0, "avg_logprob": -0.11236456344867575, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0023961577098816633}, {"id": 1031, "seek": 540520, "start": 5417.679999999999, "end": 5422.32, "text": " expression, but it's just, you don't need it for thinking. It's, it suggests that language is separate,", "tokens": [50988, 6114, 11, 457, 309, 311, 445, 11, 291, 500, 380, 643, 309, 337, 1953, 13, 467, 311, 11, 309, 13409, 300, 2856, 307, 4994, 11, 51220], "temperature": 0.0, "avg_logprob": -0.11236456344867575, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0023961577098816633}, {"id": 1032, "seek": 540520, "start": 5422.96, "end": 5427.599999999999, "text": " is a separate system. This is kind of blowing my mind right now. I'm trying to load that in", "tokens": [51252, 307, 257, 4994, 1185, 13, 639, 307, 733, 295, 15068, 452, 1575, 558, 586, 13, 286, 478, 1382, 281, 3677, 300, 294, 51484], "temperature": 0.0, "avg_logprob": -0.11236456344867575, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0023961577098816633}, {"id": 1033, "seek": 540520, "start": 5427.599999999999, "end": 5434.16, "text": " because it has implications for large language models. It sure does. And they've been working on", "tokens": [51484, 570, 309, 575, 16602, 337, 2416, 2856, 5245, 13, 467, 988, 775, 13, 400, 436, 600, 668, 1364, 322, 51812], "temperature": 0.0, "avg_logprob": -0.11236456344867575, "compression_ratio": 1.8511450381679388, "no_speech_prob": 0.0023961577098816633}, {"id": 1034, "seek": 543416, "start": 5434.16, "end": 5439.28, "text": " that. Well, let's take a stroll there. You wrote that the best current theories of human language", "tokens": [50364, 300, 13, 1042, 11, 718, 311, 747, 257, 42812, 456, 13, 509, 4114, 300, 264, 1151, 2190, 13667, 295, 1952, 2856, 50620], "temperature": 0.0, "avg_logprob": -0.09208109820330584, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.006287915166467428}, {"id": 1035, "seek": 543416, "start": 5439.28, "end": 5445.36, "text": " are arguably large language models. So this has to do with form. It's kind of a big theory. And,", "tokens": [50620, 366, 26771, 2416, 2856, 5245, 13, 407, 341, 575, 281, 360, 365, 1254, 13, 467, 311, 733, 295, 257, 955, 5261, 13, 400, 11, 50924], "temperature": 0.0, "avg_logprob": -0.09208109820330584, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.006287915166467428}, {"id": 1036, "seek": 543416, "start": 5446.08, "end": 5451.68, "text": " but the reason it's arguably the best is that it, it does the best at predicting what's English,", "tokens": [50960, 457, 264, 1778, 309, 311, 26771, 264, 1151, 307, 300, 309, 11, 309, 775, 264, 1151, 412, 32884, 437, 311, 3669, 11, 51240], "temperature": 0.0, "avg_logprob": -0.09208109820330584, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.006287915166467428}, {"id": 1037, "seek": 543416, "start": 5451.68, "end": 5456.32, "text": " for instance, it's, it's like incredibly good, you know, it better than any other theory. It's so,", "tokens": [51240, 337, 5197, 11, 309, 311, 11, 309, 311, 411, 6252, 665, 11, 291, 458, 11, 309, 1101, 813, 604, 661, 5261, 13, 467, 311, 370, 11, 51472], "temperature": 0.0, "avg_logprob": -0.09208109820330584, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.006287915166467428}, {"id": 1038, "seek": 543416, "start": 5456.32, "end": 5461.36, "text": " you know, but, you know, we don't, you know, there's, it's not sort of, there's not enough detail.", "tokens": [51472, 291, 458, 11, 457, 11, 291, 458, 11, 321, 500, 380, 11, 291, 458, 11, 456, 311, 11, 309, 311, 406, 1333, 295, 11, 456, 311, 406, 1547, 2607, 13, 51724], "temperature": 0.0, "avg_logprob": -0.09208109820330584, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.006287915166467428}, {"id": 1039, "seek": 546136, "start": 5461.36, "end": 5464.08, "text": " What's opaque? Like, there's not, you don't know what's going on.", "tokens": [50364, 708, 311, 42687, 30, 1743, 11, 456, 311, 406, 11, 291, 500, 380, 458, 437, 311, 516, 322, 13, 50500], "temperature": 0.0, "avg_logprob": -0.14245237003673206, "compression_ratio": 1.8119402985074626, "no_speech_prob": 0.0017004782566800714}, {"id": 1040, "seek": 546136, "start": 5464.08, "end": 5467.92, "text": " No, what's going on. It's another black box. But I think it's, you know, it is a theory.", "tokens": [50500, 883, 11, 437, 311, 516, 322, 13, 467, 311, 1071, 2211, 2424, 13, 583, 286, 519, 309, 311, 11, 291, 458, 11, 309, 307, 257, 5261, 13, 50692], "temperature": 0.0, "avg_logprob": -0.14245237003673206, "compression_ratio": 1.8119402985074626, "no_speech_prob": 0.0017004782566800714}, {"id": 1041, "seek": 546136, "start": 5467.92, "end": 5472.24, "text": " What's your definition of a theory? Because it's a gigantic, it's a gigantic black box with,", "tokens": [50692, 708, 311, 428, 7123, 295, 257, 5261, 30, 1436, 309, 311, 257, 26800, 11, 309, 311, 257, 26800, 2211, 2424, 365, 11, 50908], "temperature": 0.0, "avg_logprob": -0.14245237003673206, "compression_ratio": 1.8119402985074626, "no_speech_prob": 0.0017004782566800714}, {"id": 1042, "seek": 546136, "start": 5472.24, "end": 5477.839999999999, "text": " you know, a very large number of parameters controlling it. To me, theory usually requires", "tokens": [50908, 291, 458, 11, 257, 588, 2416, 1230, 295, 9834, 14905, 309, 13, 1407, 385, 11, 5261, 2673, 7029, 51188], "temperature": 0.0, "avg_logprob": -0.14245237003673206, "compression_ratio": 1.8119402985074626, "no_speech_prob": 0.0017004782566800714}, {"id": 1043, "seek": 546136, "start": 5479.04, "end": 5483.599999999999, "text": " a simplicity, right? Well, I don't know. Maybe I'm just being loose there. I think it's a,", "tokens": [51248, 257, 25632, 11, 558, 30, 1042, 11, 286, 500, 380, 458, 13, 2704, 286, 478, 445, 885, 9612, 456, 13, 286, 519, 309, 311, 257, 11, 51476], "temperature": 0.0, "avg_logprob": -0.14245237003673206, "compression_ratio": 1.8119402985074626, "no_speech_prob": 0.0017004782566800714}, {"id": 1044, "seek": 546136, "start": 5483.599999999999, "end": 5487.36, "text": " it's not, it's not a great theory, but it's a theory. It's a good theory in one sense,", "tokens": [51476, 309, 311, 406, 11, 309, 311, 406, 257, 869, 5261, 11, 457, 309, 311, 257, 5261, 13, 467, 311, 257, 665, 5261, 294, 472, 2020, 11, 51664], "temperature": 0.0, "avg_logprob": -0.14245237003673206, "compression_ratio": 1.8119402985074626, "no_speech_prob": 0.0017004782566800714}, {"id": 1045, "seek": 546136, "start": 5487.36, "end": 5490.719999999999, "text": " and then it covers all the data. Like anything you want to say in English, it does. And so", "tokens": [51664, 293, 550, 309, 10538, 439, 264, 1412, 13, 1743, 1340, 291, 528, 281, 584, 294, 3669, 11, 309, 775, 13, 400, 370, 51832], "temperature": 0.0, "avg_logprob": -0.14245237003673206, "compression_ratio": 1.8119402985074626, "no_speech_prob": 0.0017004782566800714}, {"id": 1046, "seek": 549072, "start": 5490.72, "end": 5495.2, "text": " that's why it's, that's how it's arguably the best, is that no other theory is as good as", "tokens": [50364, 300, 311, 983, 309, 311, 11, 300, 311, 577, 309, 311, 26771, 264, 1151, 11, 307, 300, 572, 661, 5261, 307, 382, 665, 382, 50588], "temperature": 0.0, "avg_logprob": -0.09847212283410758, "compression_ratio": 1.6762295081967213, "no_speech_prob": 6.0138339904369786e-05}, {"id": 1047, "seek": 549072, "start": 5495.2, "end": 5499.84, "text": " a large language model in predicting exactly what's good and what's bad in English.", "tokens": [50588, 257, 2416, 2856, 2316, 294, 32884, 2293, 437, 311, 665, 293, 437, 311, 1578, 294, 3669, 13, 50820], "temperature": 0.0, "avg_logprob": -0.09847212283410758, "compression_ratio": 1.6762295081967213, "no_speech_prob": 6.0138339904369786e-05}, {"id": 1048, "seek": 549072, "start": 5500.400000000001, "end": 5503.84, "text": " You know, now you're saying, is it a good theory? Well, probably not, you know,", "tokens": [50848, 509, 458, 11, 586, 291, 434, 1566, 11, 307, 309, 257, 665, 5261, 30, 1042, 11, 1391, 406, 11, 291, 458, 11, 51020], "temperature": 0.0, "avg_logprob": -0.09847212283410758, "compression_ratio": 1.6762295081967213, "no_speech_prob": 6.0138339904369786e-05}, {"id": 1049, "seek": 549072, "start": 5503.84, "end": 5507.2, "text": " because I want a smaller theory than that. It's too big. I agree.", "tokens": [51020, 570, 286, 528, 257, 4356, 5261, 813, 300, 13, 467, 311, 886, 955, 13, 286, 3986, 13, 51188], "temperature": 0.0, "avg_logprob": -0.09847212283410758, "compression_ratio": 1.6762295081967213, "no_speech_prob": 6.0138339904369786e-05}, {"id": 1050, "seek": 549072, "start": 5507.2, "end": 5513.6, "text": " You could probably construct mechanism by which it can generate a simple explanation of a", "tokens": [51188, 509, 727, 1391, 7690, 7513, 538, 597, 309, 393, 8460, 257, 2199, 10835, 295, 257, 51508], "temperature": 0.0, "avg_logprob": -0.09847212283410758, "compression_ratio": 1.6762295081967213, "no_speech_prob": 6.0138339904369786e-05}, {"id": 1051, "seek": 551360, "start": 5513.6, "end": 5521.4400000000005, "text": " particular language, like a set of rules, something like it could generate a dependency", "tokens": [50364, 1729, 2856, 11, 411, 257, 992, 295, 4474, 11, 746, 411, 309, 727, 8460, 257, 33621, 50756], "temperature": 0.0, "avg_logprob": -0.1299314158303397, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.022281048819422722}, {"id": 1052, "seek": 551360, "start": 5521.4400000000005, "end": 5530.64, "text": " grammar for a language, right? Yeah. You could probably, you could probably just ask it about", "tokens": [50756, 22317, 337, 257, 2856, 11, 558, 30, 865, 13, 509, 727, 1391, 11, 291, 727, 1391, 445, 1029, 309, 466, 51216], "temperature": 0.0, "avg_logprob": -0.1299314158303397, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.022281048819422722}, {"id": 1053, "seek": 551360, "start": 5530.64, "end": 5536.88, "text": " itself. Well, you know, that's, I mean, that presumes, and there's some evidence for this,", "tokens": [51216, 2564, 13, 1042, 11, 291, 458, 11, 300, 311, 11, 286, 914, 11, 300, 1183, 10018, 11, 293, 456, 311, 512, 4467, 337, 341, 11, 51528], "temperature": 0.0, "avg_logprob": -0.1299314158303397, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.022281048819422722}, {"id": 1054, "seek": 551360, "start": 5537.92, "end": 5542.64, "text": " some large language models are implementing something like dependency grammar inside them.", "tokens": [51580, 512, 2416, 2856, 5245, 366, 18114, 746, 411, 33621, 22317, 1854, 552, 13, 51816], "temperature": 0.0, "avg_logprob": -0.1299314158303397, "compression_ratio": 1.704225352112676, "no_speech_prob": 0.022281048819422722}, {"id": 1055, "seek": 554264, "start": 5542.64, "end": 5548.320000000001, "text": " And so there's work from a guy called Chris Manning and colleagues over at Stanford", "tokens": [50364, 400, 370, 456, 311, 589, 490, 257, 2146, 1219, 6688, 2458, 773, 293, 7734, 670, 412, 20374, 50648], "temperature": 0.0, "avg_logprob": -0.11458193894588586, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0015484009636566043}, {"id": 1056, "seek": 554264, "start": 5548.320000000001, "end": 5553.4400000000005, "text": " in natural language. And they looked at, I don't know how many large language model types,", "tokens": [50648, 294, 3303, 2856, 13, 400, 436, 2956, 412, 11, 286, 500, 380, 458, 577, 867, 2416, 2856, 2316, 3467, 11, 50904], "temperature": 0.0, "avg_logprob": -0.11458193894588586, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0015484009636566043}, {"id": 1057, "seek": 554264, "start": 5553.4400000000005, "end": 5559.360000000001, "text": " but certainly Burt and some others where, and where you do some kind of fancy math to figure", "tokens": [50904, 457, 3297, 363, 6224, 293, 512, 2357, 689, 11, 293, 689, 291, 360, 512, 733, 295, 10247, 5221, 281, 2573, 51200], "temperature": 0.0, "avg_logprob": -0.11458193894588586, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0015484009636566043}, {"id": 1058, "seek": 554264, "start": 5559.360000000001, "end": 5563.4400000000005, "text": " out exactly what the sort of, what kind of abstractions of representations are going on.", "tokens": [51200, 484, 2293, 437, 264, 1333, 295, 11, 437, 733, 295, 12649, 626, 295, 33358, 366, 516, 322, 13, 51404], "temperature": 0.0, "avg_logprob": -0.11458193894588586, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0015484009636566043}, {"id": 1059, "seek": 554264, "start": 5563.4400000000005, "end": 5567.200000000001, "text": " And they, and they were saying it does look like dependency structure is, is what they're", "tokens": [51404, 400, 436, 11, 293, 436, 645, 1566, 309, 775, 574, 411, 33621, 3877, 307, 11, 307, 437, 436, 434, 51592], "temperature": 0.0, "avg_logprob": -0.11458193894588586, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0015484009636566043}, {"id": 1060, "seek": 554264, "start": 5567.200000000001, "end": 5571.52, "text": " constructing. It doesn't, like, so it's actually a very, very good map. So kind of a,", "tokens": [51592, 39969, 13, 467, 1177, 380, 11, 411, 11, 370, 309, 311, 767, 257, 588, 11, 588, 665, 4471, 13, 407, 733, 295, 257, 11, 51808], "temperature": 0.0, "avg_logprob": -0.11458193894588586, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0015484009636566043}, {"id": 1061, "seek": 557152, "start": 5571.52, "end": 5578.080000000001, "text": " they are constructing something like that. Does it mean that, you know, that they're using that", "tokens": [50364, 436, 366, 39969, 746, 411, 300, 13, 4402, 309, 914, 300, 11, 291, 458, 11, 300, 436, 434, 1228, 300, 50692], "temperature": 0.0, "avg_logprob": -0.07666794631792151, "compression_ratio": 1.7533632286995515, "no_speech_prob": 0.0011693013366311789}, {"id": 1062, "seek": 557152, "start": 5578.080000000001, "end": 5583.040000000001, "text": " for meaning? I mean, probably, but we don't know. You write that the kinds of theories of language", "tokens": [50692, 337, 3620, 30, 286, 914, 11, 1391, 11, 457, 321, 500, 380, 458, 13, 509, 2464, 300, 264, 3685, 295, 13667, 295, 2856, 50940], "temperature": 0.0, "avg_logprob": -0.07666794631792151, "compression_ratio": 1.7533632286995515, "no_speech_prob": 0.0011693013366311789}, {"id": 1063, "seek": 557152, "start": 5583.040000000001, "end": 5588.320000000001, "text": " that LLMs are closest to are called construction based theories. Can you explain what construction", "tokens": [50940, 300, 441, 43, 26386, 366, 13699, 281, 366, 1219, 6435, 2361, 13667, 13, 1664, 291, 2903, 437, 6435, 51204], "temperature": 0.0, "avg_logprob": -0.07666794631792151, "compression_ratio": 1.7533632286995515, "no_speech_prob": 0.0011693013366311789}, {"id": 1064, "seek": 557152, "start": 5588.320000000001, "end": 5594.88, "text": " based theories are? It's just a general theory of language such that there's a form and a meaning", "tokens": [51204, 2361, 13667, 366, 30, 467, 311, 445, 257, 2674, 5261, 295, 2856, 1270, 300, 456, 311, 257, 1254, 293, 257, 3620, 51532], "temperature": 0.0, "avg_logprob": -0.07666794631792151, "compression_ratio": 1.7533632286995515, "no_speech_prob": 0.0011693013366311789}, {"id": 1065, "seek": 559488, "start": 5594.96, "end": 5600.96, "text": " pair for, for lots of pieces of the language. And so it's, it's, it's primarily usage based,", "tokens": [50368, 6119, 337, 11, 337, 3195, 295, 3755, 295, 264, 2856, 13, 400, 370, 309, 311, 11, 309, 311, 11, 309, 311, 10029, 14924, 2361, 11, 50668], "temperature": 0.0, "avg_logprob": -0.1579193376068376, "compression_ratio": 1.8991596638655461, "no_speech_prob": 0.41459721326828003}, {"id": 1066, "seek": 559488, "start": 5600.96, "end": 5604.88, "text": " is a construction grammar. It's just, it's trying to deal with the things that people", "tokens": [50668, 307, 257, 6435, 22317, 13, 467, 311, 445, 11, 309, 311, 1382, 281, 2028, 365, 264, 721, 300, 561, 50864], "temperature": 0.0, "avg_logprob": -0.1579193376068376, "compression_ratio": 1.8991596638655461, "no_speech_prob": 0.41459721326828003}, {"id": 1067, "seek": 559488, "start": 5605.4400000000005, "end": 5611.28, "text": " actually say, actually say and actually write. And so that's, it's a usage based idea. And", "tokens": [50892, 767, 584, 11, 767, 584, 293, 767, 2464, 13, 400, 370, 300, 311, 11, 309, 311, 257, 14924, 2361, 1558, 13, 400, 51184], "temperature": 0.0, "avg_logprob": -0.1579193376068376, "compression_ratio": 1.8991596638655461, "no_speech_prob": 0.41459721326828003}, {"id": 1068, "seek": 559488, "start": 5611.28, "end": 5616.32, "text": " what's the constructional constructions either a simple word, so like a morpheme plus its meaning", "tokens": [51184, 437, 311, 264, 6435, 304, 7690, 626, 2139, 257, 2199, 1349, 11, 370, 411, 257, 25778, 5729, 1804, 1080, 3620, 51436], "temperature": 0.0, "avg_logprob": -0.1579193376068376, "compression_ratio": 1.8991596638655461, "no_speech_prob": 0.41459721326828003}, {"id": 1069, "seek": 559488, "start": 5616.32, "end": 5621.28, "text": " or a combination of words, it's basically combinations of words, like the rules. So,", "tokens": [51436, 420, 257, 6562, 295, 2283, 11, 309, 311, 1936, 21267, 295, 2283, 11, 411, 264, 4474, 13, 407, 11, 51684], "temperature": 0.0, "avg_logprob": -0.1579193376068376, "compression_ratio": 1.8991596638655461, "no_speech_prob": 0.41459721326828003}, {"id": 1070, "seek": 562128, "start": 5621.92, "end": 5630.96, "text": " but it's, it's unspecified as to what the form of the grammar is under underlyingly.", "tokens": [50396, 457, 309, 311, 11, 309, 311, 2693, 494, 66, 2587, 382, 281, 437, 264, 1254, 295, 264, 22317, 307, 833, 14217, 356, 13, 50848], "temperature": 0.0, "avg_logprob": -0.10360438173467462, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00044418495963327587}, {"id": 1071, "seek": 562128, "start": 5630.96, "end": 5637.5199999999995, "text": " And so I would, I would argue that the dependency grammar is maybe the right form to use for the", "tokens": [50848, 400, 370, 286, 576, 11, 286, 576, 9695, 300, 264, 33621, 22317, 307, 1310, 264, 558, 1254, 281, 764, 337, 264, 51176], "temperature": 0.0, "avg_logprob": -0.10360438173467462, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00044418495963327587}, {"id": 1072, "seek": 562128, "start": 5637.5199999999995, "end": 5643.28, "text": " types of construction grammar. Construction grammar typically isn't kind of formalized", "tokens": [51176, 3467, 295, 6435, 22317, 13, 40017, 22317, 5850, 1943, 380, 733, 295, 9860, 1602, 51464], "temperature": 0.0, "avg_logprob": -0.10360438173467462, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00044418495963327587}, {"id": 1073, "seek": 562128, "start": 5643.28, "end": 5648.4, "text": " quite. And so maybe the formalization, a formalization of that, it might be in dependency", "tokens": [51464, 1596, 13, 400, 370, 1310, 264, 9860, 2144, 11, 257, 9860, 2144, 295, 300, 11, 309, 1062, 312, 294, 33621, 51720], "temperature": 0.0, "avg_logprob": -0.10360438173467462, "compression_ratio": 1.8080808080808082, "no_speech_prob": 0.00044418495963327587}, {"id": 1074, "seek": 564840, "start": 5648.4, "end": 5654.08, "text": " grammar. Yeah. I mean, I, I would think so, but I mean, it's up to people, other researchers", "tokens": [50364, 22317, 13, 865, 13, 286, 914, 11, 286, 11, 286, 576, 519, 370, 11, 457, 286, 914, 11, 309, 311, 493, 281, 561, 11, 661, 10309, 50648], "temperature": 0.0, "avg_logprob": -0.09502712885538737, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.008575045503675938}, {"id": 1075, "seek": 564840, "start": 5654.08, "end": 5659.599999999999, "text": " in that area, if they agree or not. So. Well, do you think that large language models understand", "tokens": [50648, 294, 300, 1859, 11, 498, 436, 3986, 420, 406, 13, 407, 13, 1042, 11, 360, 291, 519, 300, 2416, 2856, 5245, 1223, 50924], "temperature": 0.0, "avg_logprob": -0.09502712885538737, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.008575045503675938}, {"id": 1076, "seek": 564840, "start": 5659.599999999999, "end": 5665.2, "text": " language? Are they mimicking language? I guess the deeper question there is, are they just", "tokens": [50924, 2856, 30, 2014, 436, 12247, 10401, 2856, 30, 286, 2041, 264, 7731, 1168, 456, 307, 11, 366, 436, 445, 51204], "temperature": 0.0, "avg_logprob": -0.09502712885538737, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.008575045503675938}, {"id": 1077, "seek": 564840, "start": 5665.2, "end": 5671.36, "text": " understanding the surface form? Or do they understand something deeper about the meaning", "tokens": [51204, 3701, 264, 3753, 1254, 30, 1610, 360, 436, 1223, 746, 7731, 466, 264, 3620, 51512], "temperature": 0.0, "avg_logprob": -0.09502712885538737, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.008575045503675938}, {"id": 1078, "seek": 564840, "start": 5671.36, "end": 5675.839999999999, "text": " that then generates the form? I mean, I would argue they're doing the form, they're doing", "tokens": [51512, 300, 550, 23815, 264, 1254, 30, 286, 914, 11, 286, 576, 9695, 436, 434, 884, 264, 1254, 11, 436, 434, 884, 51736], "temperature": 0.0, "avg_logprob": -0.09502712885538737, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.008575045503675938}, {"id": 1079, "seek": 567584, "start": 5675.84, "end": 5679.6, "text": " the form, they're doing it really, really well. And are they doing the meaning? No,", "tokens": [50364, 264, 1254, 11, 436, 434, 884, 309, 534, 11, 534, 731, 13, 400, 366, 436, 884, 264, 3620, 30, 883, 11, 50552], "temperature": 0.0, "avg_logprob": -0.09871859020657009, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0037067316006869078}, {"id": 1080, "seek": 567584, "start": 5679.6, "end": 5684.4800000000005, "text": " probably not. I mean, there's lots of these examples from various groups showing that they", "tokens": [50552, 1391, 406, 13, 286, 914, 11, 456, 311, 3195, 295, 613, 5110, 490, 3683, 3935, 4099, 300, 436, 50796], "temperature": 0.0, "avg_logprob": -0.09871859020657009, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0037067316006869078}, {"id": 1081, "seek": 567584, "start": 5684.4800000000005, "end": 5688.8, "text": " can be tricked in all kinds of ways. They really don't understand the, the meaning of what's going", "tokens": [50796, 393, 312, 39345, 294, 439, 3685, 295, 2098, 13, 814, 534, 500, 380, 1223, 264, 11, 264, 3620, 295, 437, 311, 516, 51012], "temperature": 0.0, "avg_logprob": -0.09871859020657009, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0037067316006869078}, {"id": 1082, "seek": 567584, "start": 5688.8, "end": 5695.360000000001, "text": " on. And so there's a lot of examples that he and other groups have given, which just, which show", "tokens": [51012, 322, 13, 400, 370, 456, 311, 257, 688, 295, 5110, 300, 415, 293, 661, 3935, 362, 2212, 11, 597, 445, 11, 597, 855, 51340], "temperature": 0.0, "avg_logprob": -0.09871859020657009, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0037067316006869078}, {"id": 1083, "seek": 567584, "start": 5695.360000000001, "end": 5699.6, "text": " they don't really understand what's going on. So, you know, the Monty Hall problem is this", "tokens": [51340, 436, 500, 380, 534, 1223, 437, 311, 516, 322, 13, 407, 11, 291, 458, 11, 264, 4713, 874, 5434, 1154, 307, 341, 51552], "temperature": 0.0, "avg_logprob": -0.09871859020657009, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0037067316006869078}, {"id": 1084, "seek": 567584, "start": 5699.6, "end": 5704.96, "text": " silly problem, right? Where, you know, if you have three door, it's less make a deal as this", "tokens": [51552, 11774, 1154, 11, 558, 30, 2305, 11, 291, 458, 11, 498, 291, 362, 1045, 2853, 11, 309, 311, 1570, 652, 257, 2028, 382, 341, 51820], "temperature": 0.0, "avg_logprob": -0.09871859020657009, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0037067316006869078}, {"id": 1085, "seek": 570496, "start": 5704.96, "end": 5710.64, "text": " old game show, and there's three doors, and there's a prize behind one, and there's some", "tokens": [50364, 1331, 1216, 855, 11, 293, 456, 311, 1045, 8077, 11, 293, 456, 311, 257, 12818, 2261, 472, 11, 293, 456, 311, 512, 50648], "temperature": 0.0, "avg_logprob": -0.08984485088578807, "compression_ratio": 2.014388489208633, "no_speech_prob": 0.003272648900747299}, {"id": 1086, "seek": 570496, "start": 5711.92, "end": 5716.4800000000005, "text": " junk prizes behind the other two, and you're trying to select one. And if you, you know,", "tokens": [50712, 19109, 27350, 2261, 264, 661, 732, 11, 293, 291, 434, 1382, 281, 3048, 472, 13, 400, 498, 291, 11, 291, 458, 11, 50940], "temperature": 0.0, "avg_logprob": -0.08984485088578807, "compression_ratio": 2.014388489208633, "no_speech_prob": 0.003272648900747299}, {"id": 1087, "seek": 570496, "start": 5716.4800000000005, "end": 5721.52, "text": " he knows Monty, he knows where the target item is, the good thing, he knows everything is back", "tokens": [50940, 415, 3255, 4713, 874, 11, 415, 3255, 689, 264, 3779, 3174, 307, 11, 264, 665, 551, 11, 415, 3255, 1203, 307, 646, 51192], "temperature": 0.0, "avg_logprob": -0.08984485088578807, "compression_ratio": 2.014388489208633, "no_speech_prob": 0.003272648900747299}, {"id": 1088, "seek": 570496, "start": 5721.52, "end": 5726.32, "text": " there. And you're supposed to, he gives you a choice, you choose one of the three, and then", "tokens": [51192, 456, 13, 400, 291, 434, 3442, 281, 11, 415, 2709, 291, 257, 3922, 11, 291, 2826, 472, 295, 264, 1045, 11, 293, 550, 51432], "temperature": 0.0, "avg_logprob": -0.08984485088578807, "compression_ratio": 2.014388489208633, "no_speech_prob": 0.003272648900747299}, {"id": 1089, "seek": 570496, "start": 5726.32, "end": 5730.56, "text": " he opens one of the doors, and it's some junk prize. And then the question is, should you trade", "tokens": [51432, 415, 9870, 472, 295, 264, 8077, 11, 293, 309, 311, 512, 19109, 12818, 13, 400, 550, 264, 1168, 307, 11, 820, 291, 4923, 51644], "temperature": 0.0, "avg_logprob": -0.08984485088578807, "compression_ratio": 2.014388489208633, "no_speech_prob": 0.003272648900747299}, {"id": 1090, "seek": 570496, "start": 5730.56, "end": 5734.4, "text": " to get the other one? And the answer is yes, you should trade, because he knew which ones you could", "tokens": [51644, 281, 483, 264, 661, 472, 30, 400, 264, 1867, 307, 2086, 11, 291, 820, 4923, 11, 570, 415, 2586, 597, 2306, 291, 727, 51836], "temperature": 0.0, "avg_logprob": -0.08984485088578807, "compression_ratio": 2.014388489208633, "no_speech_prob": 0.003272648900747299}, {"id": 1091, "seek": 573440, "start": 5734.4, "end": 5740.08, "text": " turn around. And so now the odds are two-thirds, okay? And then you just change that a little bit", "tokens": [50364, 1261, 926, 13, 400, 370, 586, 264, 17439, 366, 732, 12, 38507, 11, 1392, 30, 400, 550, 291, 445, 1319, 300, 257, 707, 857, 50648], "temperature": 0.0, "avg_logprob": -0.1518024561059384, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.005383958574384451}, {"id": 1092, "seek": 573440, "start": 5740.08, "end": 5745.36, "text": " to the large language mall. The large language mall has seen that explanation so many times,", "tokens": [50648, 281, 264, 2416, 2856, 16026, 13, 440, 2416, 2856, 16026, 575, 1612, 300, 10835, 370, 867, 1413, 11, 50912], "temperature": 0.0, "avg_logprob": -0.1518024561059384, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.005383958574384451}, {"id": 1093, "seek": 573440, "start": 5745.36, "end": 5749.44, "text": " that it just, if you change the story, it's a little bit, but it makes it sound like it's the", "tokens": [50912, 300, 309, 445, 11, 498, 291, 1319, 264, 1657, 11, 309, 311, 257, 707, 857, 11, 457, 309, 1669, 309, 1626, 411, 309, 311, 264, 51116], "temperature": 0.0, "avg_logprob": -0.1518024561059384, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.005383958574384451}, {"id": 1094, "seek": 573440, "start": 5749.44, "end": 5755.599999999999, "text": " Monty Hall problem, but it's not. You just say, oh, there's three doors, and one behind them is a", "tokens": [51116, 4713, 874, 5434, 1154, 11, 457, 309, 311, 406, 13, 509, 445, 584, 11, 1954, 11, 456, 311, 1045, 8077, 11, 293, 472, 2261, 552, 307, 257, 51424], "temperature": 0.0, "avg_logprob": -0.1518024561059384, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.005383958574384451}, {"id": 1095, "seek": 573440, "start": 5755.599999999999, "end": 5760.16, "text": " good prize, and there's two bad doors. I happen to know it's behind door number one. The good prize,", "tokens": [51424, 665, 12818, 11, 293, 456, 311, 732, 1578, 8077, 13, 286, 1051, 281, 458, 309, 311, 2261, 2853, 1230, 472, 13, 440, 665, 12818, 11, 51652], "temperature": 0.0, "avg_logprob": -0.1518024561059384, "compression_ratio": 1.8022388059701493, "no_speech_prob": 0.005383958574384451}, {"id": 1096, "seek": 576016, "start": 5760.16, "end": 5764.5599999999995, "text": " the car, is behind door number one. So I'm going to choose door number one. Monty Hall opens door", "tokens": [50364, 264, 1032, 11, 307, 2261, 2853, 1230, 472, 13, 407, 286, 478, 516, 281, 2826, 2853, 1230, 472, 13, 4713, 874, 5434, 9870, 2853, 50584], "temperature": 0.0, "avg_logprob": -0.1186466883946132, "compression_ratio": 1.875, "no_speech_prob": 0.06752919405698776}, {"id": 1097, "seek": 576016, "start": 5764.5599999999995, "end": 5768.639999999999, "text": " number three, and shows me nothing there. Should I trade for door number two? Even though I know", "tokens": [50584, 1230, 1045, 11, 293, 3110, 385, 1825, 456, 13, 6454, 286, 4923, 337, 2853, 1230, 732, 30, 2754, 1673, 286, 458, 50788], "temperature": 0.0, "avg_logprob": -0.1186466883946132, "compression_ratio": 1.875, "no_speech_prob": 0.06752919405698776}, {"id": 1098, "seek": 576016, "start": 5768.639999999999, "end": 5772.5599999999995, "text": " the good prize in door number one, and then the large language mall say, yes, you should trade,", "tokens": [50788, 264, 665, 12818, 294, 2853, 1230, 472, 11, 293, 550, 264, 2416, 2856, 16026, 584, 11, 2086, 11, 291, 820, 4923, 11, 50984], "temperature": 0.0, "avg_logprob": -0.1186466883946132, "compression_ratio": 1.875, "no_speech_prob": 0.06752919405698776}, {"id": 1099, "seek": 576016, "start": 5772.5599999999995, "end": 5779.68, "text": " because it just goes through the forms that it's seen before so many times on these cases,", "tokens": [50984, 570, 309, 445, 1709, 807, 264, 6422, 300, 309, 311, 1612, 949, 370, 867, 1413, 322, 613, 3331, 11, 51340], "temperature": 0.0, "avg_logprob": -0.1186466883946132, "compression_ratio": 1.875, "no_speech_prob": 0.06752919405698776}, {"id": 1100, "seek": 576016, "start": 5779.68, "end": 5784.4, "text": " where it, yes, you should trade, because your odds have shifted from one in three now to two out", "tokens": [51340, 689, 309, 11, 2086, 11, 291, 820, 4923, 11, 570, 428, 17439, 362, 18892, 490, 472, 294, 1045, 586, 281, 732, 484, 51576], "temperature": 0.0, "avg_logprob": -0.1186466883946132, "compression_ratio": 1.875, "no_speech_prob": 0.06752919405698776}, {"id": 1101, "seek": 576016, "start": 5784.4, "end": 5789.44, "text": " of three, to being that thing. It doesn't have any way to remember that actually you have a", "tokens": [51576, 295, 1045, 11, 281, 885, 300, 551, 13, 467, 1177, 380, 362, 604, 636, 281, 1604, 300, 767, 291, 362, 257, 51828], "temperature": 0.0, "avg_logprob": -0.1186466883946132, "compression_ratio": 1.875, "no_speech_prob": 0.06752919405698776}, {"id": 1102, "seek": 578944, "start": 5789.44, "end": 5795.12, "text": " 100% probability behind that door number one. You know that. That's not part of the scheme that", "tokens": [50364, 2319, 4, 8482, 2261, 300, 2853, 1230, 472, 13, 509, 458, 300, 13, 663, 311, 406, 644, 295, 264, 12232, 300, 50648], "temperature": 0.0, "avg_logprob": -0.09641234628085432, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0004441868222784251}, {"id": 1103, "seek": 578944, "start": 5795.12, "end": 5800.16, "text": " it's seen hundreds and hundreds of times before. And so you can't, even if you try to explain to", "tokens": [50648, 309, 311, 1612, 6779, 293, 6779, 295, 1413, 949, 13, 400, 370, 291, 393, 380, 11, 754, 498, 291, 853, 281, 2903, 281, 50900], "temperature": 0.0, "avg_logprob": -0.09641234628085432, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0004441868222784251}, {"id": 1104, "seek": 578944, "start": 5800.16, "end": 5805.5199999999995, "text": " it that it's wrong, that they can't do that, it'll just keep giving you back the problem.", "tokens": [50900, 309, 300, 309, 311, 2085, 11, 300, 436, 393, 380, 360, 300, 11, 309, 603, 445, 1066, 2902, 291, 646, 264, 1154, 13, 51168], "temperature": 0.0, "avg_logprob": -0.09641234628085432, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0004441868222784251}, {"id": 1105, "seek": 578944, "start": 5805.5199999999995, "end": 5809.2, "text": " But it's also possible that a larger language model would be aware of the fact that there's", "tokens": [51168, 583, 309, 311, 611, 1944, 300, 257, 4833, 2856, 2316, 576, 312, 3650, 295, 264, 1186, 300, 456, 311, 51352], "temperature": 0.0, "avg_logprob": -0.09641234628085432, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0004441868222784251}, {"id": 1106, "seek": 578944, "start": 5809.2, "end": 5817.44, "text": " sometimes over a representation of a particular kind of formulation, and it's easy to get tricked", "tokens": [51352, 2171, 670, 257, 10290, 295, 257, 1729, 733, 295, 37642, 11, 293, 309, 311, 1858, 281, 483, 39345, 51764], "temperature": 0.0, "avg_logprob": -0.09641234628085432, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0004441868222784251}, {"id": 1107, "seek": 581744, "start": 5817.44, "end": 5824.5599999999995, "text": " by that. And so you could see if they get larger and larger models be a little bit more skeptical.", "tokens": [50364, 538, 300, 13, 400, 370, 291, 727, 536, 498, 436, 483, 4833, 293, 4833, 5245, 312, 257, 707, 857, 544, 28601, 13, 50720], "temperature": 0.0, "avg_logprob": -0.12584514328927704, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.010647651739418507}, {"id": 1108, "seek": 581744, "start": 5824.5599999999995, "end": 5833.36, "text": " So you see over a representation. So it just feels like form can, training on form can go", "tokens": [50720, 407, 291, 536, 670, 257, 10290, 13, 407, 309, 445, 3417, 411, 1254, 393, 11, 3097, 322, 1254, 393, 352, 51160], "temperature": 0.0, "avg_logprob": -0.12584514328927704, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.010647651739418507}, {"id": 1109, "seek": 581744, "start": 5833.36, "end": 5842.32, "text": " really far in terms of being able to generate things that look like the thing understands", "tokens": [51160, 534, 1400, 294, 2115, 295, 885, 1075, 281, 8460, 721, 300, 574, 411, 264, 551, 15146, 51608], "temperature": 0.0, "avg_logprob": -0.12584514328927704, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.010647651739418507}, {"id": 1110, "seek": 584232, "start": 5843.2, "end": 5852.5599999999995, "text": " deeply the underlying world model of the kind of mathematical world, physical world,", "tokens": [50408, 8760, 264, 14217, 1002, 2316, 295, 264, 733, 295, 18894, 1002, 11, 4001, 1002, 11, 50876], "temperature": 0.0, "avg_logprob": -0.10213158560580894, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.05103504657745361}, {"id": 1111, "seek": 584232, "start": 5853.44, "end": 5858.639999999999, "text": " psychological world that would generate these kinds of sentences. It just feels like you're", "tokens": [50920, 14346, 1002, 300, 576, 8460, 613, 3685, 295, 16579, 13, 467, 445, 3417, 411, 291, 434, 51180], "temperature": 0.0, "avg_logprob": -0.10213158560580894, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.05103504657745361}, {"id": 1112, "seek": 584232, "start": 5858.639999999999, "end": 5864.08, "text": " creeping close to the meaning part, easily fooled, all this kind of stuff, but that's humans too.", "tokens": [51180, 47753, 1998, 281, 264, 3620, 644, 11, 3612, 33372, 11, 439, 341, 733, 295, 1507, 11, 457, 300, 311, 6255, 886, 13, 51452], "temperature": 0.0, "avg_logprob": -0.10213158560580894, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.05103504657745361}, {"id": 1113, "seek": 586408, "start": 5864.96, "end": 5872.88, "text": " So it just seems really impressive how often it seems like it un-understands concepts.", "tokens": [50408, 407, 309, 445, 2544, 534, 8992, 577, 2049, 309, 2544, 411, 309, 517, 12, 6617, 1115, 82, 10392, 13, 50804], "temperature": 0.0, "avg_logprob": -0.14649198388540616, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.07581604272127151}, {"id": 1114, "seek": 586408, "start": 5874.16, "end": 5879.28, "text": " I mean, you don't have to convince me of that. I am very, very impressed, but does it,", "tokens": [50868, 286, 914, 11, 291, 500, 380, 362, 281, 13447, 385, 295, 300, 13, 286, 669, 588, 11, 588, 11679, 11, 457, 775, 309, 11, 51124], "temperature": 0.0, "avg_logprob": -0.14649198388540616, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.07581604272127151}, {"id": 1115, "seek": 586408, "start": 5879.28, "end": 5884.88, "text": " does it mean you're giving a possible world where maybe someone's going to train some other", "tokens": [51124, 775, 309, 914, 291, 434, 2902, 257, 1944, 1002, 689, 1310, 1580, 311, 516, 281, 3847, 512, 661, 51404], "temperature": 0.0, "avg_logprob": -0.14649198388540616, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.07581604272127151}, {"id": 1116, "seek": 586408, "start": 5884.88, "end": 5890.8, "text": " versions such that it'll be somehow abstracting away from types of forms? I mean, I don't think", "tokens": [51404, 9606, 1270, 300, 309, 603, 312, 6063, 12649, 278, 1314, 490, 3467, 295, 6422, 30, 286, 914, 11, 286, 500, 380, 519, 51700], "temperature": 0.0, "avg_logprob": -0.14649198388540616, "compression_ratio": 1.6044444444444443, "no_speech_prob": 0.07581604272127151}, {"id": 1117, "seek": 589080, "start": 5890.8, "end": 5897.68, "text": " that's happened. No, no, no. I'm not saying that. I think when you just look at anecdotal examples", "tokens": [50364, 300, 311, 2011, 13, 883, 11, 572, 11, 572, 13, 286, 478, 406, 1566, 300, 13, 286, 519, 562, 291, 445, 574, 412, 26652, 38180, 5110, 50708], "temperature": 0.0, "avg_logprob": -0.18157862772983788, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.045990828424692154}, {"id": 1118, "seek": 589080, "start": 5898.320000000001, "end": 5902.72, "text": " and just showing a large number of them where it doesn't seem to understand and it's easily fooled,", "tokens": [50740, 293, 445, 4099, 257, 2416, 1230, 295, 552, 689, 309, 1177, 380, 1643, 281, 1223, 293, 309, 311, 3612, 33372, 11, 50960], "temperature": 0.0, "avg_logprob": -0.18157862772983788, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.045990828424692154}, {"id": 1119, "seek": 589080, "start": 5903.28, "end": 5911.76, "text": " that does not seem like a scientific data-driven analysis of how many places is a damn impressive", "tokens": [50988, 300, 775, 406, 1643, 411, 257, 8134, 1412, 12, 25456, 5215, 295, 577, 867, 3190, 307, 257, 8151, 8992, 51412], "temperature": 0.0, "avg_logprob": -0.18157862772983788, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.045990828424692154}, {"id": 1120, "seek": 589080, "start": 5912.4800000000005, "end": 5915.6, "text": " in terms of meaning and understanding and how many places is easily fooled.", "tokens": [51448, 294, 2115, 295, 3620, 293, 3701, 293, 577, 867, 3190, 307, 3612, 33372, 13, 51604], "temperature": 0.0, "avg_logprob": -0.18157862772983788, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.045990828424692154}, {"id": 1121, "seek": 589080, "start": 5916.56, "end": 5919.92, "text": " That's not the inference. So I don't want to make that, the inference I don't,", "tokens": [51652, 663, 311, 406, 264, 38253, 13, 407, 286, 500, 380, 528, 281, 652, 300, 11, 264, 38253, 286, 500, 380, 11, 51820], "temperature": 0.0, "avg_logprob": -0.18157862772983788, "compression_ratio": 1.748062015503876, "no_speech_prob": 0.045990828424692154}, {"id": 1122, "seek": 591992, "start": 5919.92, "end": 5923.76, "text": " I wouldn't want to make was that inference. The inference I'm trying to push is just that is it,", "tokens": [50364, 286, 2759, 380, 528, 281, 652, 390, 300, 38253, 13, 440, 38253, 286, 478, 1382, 281, 2944, 307, 445, 300, 307, 309, 11, 50556], "temperature": 0.0, "avg_logprob": -0.07282054927987112, "compression_ratio": 1.971326164874552, "no_speech_prob": 0.0010986535344272852}, {"id": 1123, "seek": 591992, "start": 5924.72, "end": 5929.52, "text": " is it like humans here? It's probably not like humans here. It's different. So humans don't make", "tokens": [50604, 307, 309, 411, 6255, 510, 30, 467, 311, 1391, 406, 411, 6255, 510, 13, 467, 311, 819, 13, 407, 6255, 500, 380, 652, 50844], "temperature": 0.0, "avg_logprob": -0.07282054927987112, "compression_ratio": 1.971326164874552, "no_speech_prob": 0.0010986535344272852}, {"id": 1124, "seek": 591992, "start": 5929.52, "end": 5933.84, "text": " that error. If you explain that to them, they're not going to make that error. They don't make", "tokens": [50844, 300, 6713, 13, 759, 291, 2903, 300, 281, 552, 11, 436, 434, 406, 516, 281, 652, 300, 6713, 13, 814, 500, 380, 652, 51060], "temperature": 0.0, "avg_logprob": -0.07282054927987112, "compression_ratio": 1.971326164874552, "no_speech_prob": 0.0010986535344272852}, {"id": 1125, "seek": 591992, "start": 5933.84, "end": 5937.4400000000005, "text": " that error. And so that's something, it's doing something different from humans that they're", "tokens": [51060, 300, 6713, 13, 400, 370, 300, 311, 746, 11, 309, 311, 884, 746, 819, 490, 6255, 300, 436, 434, 51240], "temperature": 0.0, "avg_logprob": -0.07282054927987112, "compression_ratio": 1.971326164874552, "no_speech_prob": 0.0010986535344272852}, {"id": 1126, "seek": 591992, "start": 5937.4400000000005, "end": 5942.16, "text": " doing in that case. What's the mechanism by which humans figure out that it's an error?", "tokens": [51240, 884, 294, 300, 1389, 13, 708, 311, 264, 7513, 538, 597, 6255, 2573, 484, 300, 309, 311, 364, 6713, 30, 51476], "temperature": 0.0, "avg_logprob": -0.07282054927987112, "compression_ratio": 1.971326164874552, "no_speech_prob": 0.0010986535344272852}, {"id": 1127, "seek": 591992, "start": 5942.16, "end": 5946.64, "text": " I'm just saying the error there is like, if I explain to you there's 100% chance", "tokens": [51476, 286, 478, 445, 1566, 264, 6713, 456, 307, 411, 11, 498, 286, 2903, 281, 291, 456, 311, 2319, 4, 2931, 51700], "temperature": 0.0, "avg_logprob": -0.07282054927987112, "compression_ratio": 1.971326164874552, "no_speech_prob": 0.0010986535344272852}, {"id": 1128, "seek": 594664, "start": 5946.64, "end": 5951.84, "text": " that the car is behind this case, this door, well, do you want to trade? If you'll say no.", "tokens": [50364, 300, 264, 1032, 307, 2261, 341, 1389, 11, 341, 2853, 11, 731, 11, 360, 291, 528, 281, 4923, 30, 759, 291, 603, 584, 572, 13, 50624], "temperature": 0.0, "avg_logprob": -0.2032380892535833, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.0009696985362097621}, {"id": 1129, "seek": 594664, "start": 5951.84, "end": 5957.4400000000005, "text": " But this thing will say yes, because it's so, that trick, it's so wound up on the form", "tokens": [50624, 583, 341, 551, 486, 584, 2086, 11, 570, 309, 311, 370, 11, 300, 4282, 11, 309, 311, 370, 10999, 493, 322, 264, 1254, 50904], "temperature": 0.0, "avg_logprob": -0.2032380892535833, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.0009696985362097621}, {"id": 1130, "seek": 594664, "start": 5958.320000000001, "end": 5963.280000000001, "text": " that it's, that's an error that a human doesn't make, which is kind of interesting.", "tokens": [50948, 300, 309, 311, 11, 300, 311, 364, 6713, 300, 257, 1952, 1177, 380, 652, 11, 597, 307, 733, 295, 1880, 13, 51196], "temperature": 0.0, "avg_logprob": -0.2032380892535833, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.0009696985362097621}, {"id": 1131, "seek": 594664, "start": 5963.280000000001, "end": 5965.92, "text": " Less likely to make, I should say. Yeah, less likely.", "tokens": [51196, 18649, 3700, 281, 652, 11, 286, 820, 584, 13, 865, 11, 1570, 3700, 13, 51328], "temperature": 0.0, "avg_logprob": -0.2032380892535833, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.0009696985362097621}, {"id": 1132, "seek": 594664, "start": 5965.92, "end": 5968.4800000000005, "text": " Because like humans are very- Oh yeah.", "tokens": [51328, 1436, 411, 6255, 366, 588, 12, 876, 1338, 13, 51456], "temperature": 0.0, "avg_logprob": -0.2032380892535833, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.0009696985362097621}, {"id": 1133, "seek": 594664, "start": 5969.12, "end": 5975.360000000001, "text": " I mean, you're asking, you're asking humans, you're asking a system to understand 100%,", "tokens": [51488, 286, 914, 11, 291, 434, 3365, 11, 291, 434, 3365, 6255, 11, 291, 434, 3365, 257, 1185, 281, 1223, 2319, 8923, 51800], "temperature": 0.0, "avg_logprob": -0.2032380892535833, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.0009696985362097621}, {"id": 1134, "seek": 597536, "start": 5975.36, "end": 5978.48, "text": " like you're asking some mathematical concepts. And so like-", "tokens": [50364, 411, 291, 434, 3365, 512, 18894, 10392, 13, 400, 370, 411, 12, 50520], "temperature": 0.0, "avg_logprob": -0.19016695713651352, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00027800671523436904}, {"id": 1135, "seek": 597536, "start": 5980.0, "end": 5985.759999999999, "text": " Look, the places where large language models are, the form is amazing. So let's go back to", "tokens": [50596, 2053, 11, 264, 3190, 689, 2416, 2856, 5245, 366, 11, 264, 1254, 307, 2243, 13, 407, 718, 311, 352, 646, 281, 50884], "temperature": 0.0, "avg_logprob": -0.19016695713651352, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00027800671523436904}, {"id": 1136, "seek": 597536, "start": 5985.759999999999, "end": 5990.0, "text": " nested structures, center embedded structures. Okay, if you ask a human to complete those,", "tokens": [50884, 15646, 292, 9227, 11, 3056, 16741, 9227, 13, 1033, 11, 498, 291, 1029, 257, 1952, 281, 3566, 729, 11, 51096], "temperature": 0.0, "avg_logprob": -0.19016695713651352, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00027800671523436904}, {"id": 1137, "seek": 597536, "start": 5990.0, "end": 5994.96, "text": " they can't do it. Neither can a large language model. They're just like humans in that. If you", "tokens": [51096, 436, 393, 380, 360, 309, 13, 23956, 393, 257, 2416, 2856, 2316, 13, 814, 434, 445, 411, 6255, 294, 300, 13, 759, 291, 51344], "temperature": 0.0, "avg_logprob": -0.19016695713651352, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00027800671523436904}, {"id": 1138, "seek": 597536, "start": 5994.96, "end": 5997.679999999999, "text": " ask, if I ask a large language model- That's fascinating, by the way.", "tokens": [51344, 1029, 11, 498, 286, 1029, 257, 2416, 2856, 2316, 12, 663, 311, 10343, 11, 538, 264, 636, 13, 51480], "temperature": 0.0, "avg_logprob": -0.19016695713651352, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00027800671523436904}, {"id": 1139, "seek": 597536, "start": 5997.679999999999, "end": 6001.679999999999, "text": " The central embedding, the central embedding, it struggles with-", "tokens": [51480, 440, 5777, 12240, 3584, 11, 264, 5777, 12240, 3584, 11, 309, 17592, 365, 12, 51680], "temperature": 0.0, "avg_logprob": -0.19016695713651352, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00027800671523436904}, {"id": 1140, "seek": 597536, "start": 6001.679999999999, "end": 6004.96, "text": " Just like humans, exactly like humans. Exactly the same way as humans.", "tokens": [51680, 1449, 411, 6255, 11, 2293, 411, 6255, 13, 7587, 264, 912, 636, 382, 6255, 13, 51844], "temperature": 0.0, "avg_logprob": -0.19016695713651352, "compression_ratio": 1.8754325259515572, "no_speech_prob": 0.00027800671523436904}, {"id": 1141, "seek": 600496, "start": 6005.2, "end": 6011.36, "text": " And that's not trained. So they do exactly, so that is a similarity. So but then it's,", "tokens": [50376, 400, 300, 311, 406, 8895, 13, 407, 436, 360, 2293, 11, 370, 300, 307, 257, 32194, 13, 407, 457, 550, 309, 311, 11, 50684], "temperature": 0.0, "avg_logprob": -0.15196856556322752, "compression_ratio": 1.7993079584775087, "no_speech_prob": 0.0006666296976618469}, {"id": 1142, "seek": 600496, "start": 6011.36, "end": 6015.6, "text": " that's not meaning, right? This is form. But when we get into meaning,", "tokens": [50684, 300, 311, 406, 3620, 11, 558, 30, 639, 307, 1254, 13, 583, 562, 321, 483, 666, 3620, 11, 50896], "temperature": 0.0, "avg_logprob": -0.15196856556322752, "compression_ratio": 1.7993079584775087, "no_speech_prob": 0.0006666296976618469}, {"id": 1143, "seek": 600496, "start": 6015.6, "end": 6020.08, "text": " this is where they get kind of messed up. When you start to saying, oh, what's behind this door?", "tokens": [50896, 341, 307, 689, 436, 483, 733, 295, 16507, 493, 13, 1133, 291, 722, 281, 1566, 11, 1954, 11, 437, 311, 2261, 341, 2853, 30, 51120], "temperature": 0.0, "avg_logprob": -0.15196856556322752, "compression_ratio": 1.7993079584775087, "no_speech_prob": 0.0006666296976618469}, {"id": 1144, "seek": 600496, "start": 6020.08, "end": 6024.88, "text": " Oh, it's, you know, this is the thing I want. Humans don't mess that up as much, you know.", "tokens": [51120, 876, 11, 309, 311, 11, 291, 458, 11, 341, 307, 264, 551, 286, 528, 13, 35809, 500, 380, 2082, 300, 493, 382, 709, 11, 291, 458, 13, 51360], "temperature": 0.0, "avg_logprob": -0.15196856556322752, "compression_ratio": 1.7993079584775087, "no_speech_prob": 0.0006666296976618469}, {"id": 1145, "seek": 600496, "start": 6024.88, "end": 6031.2, "text": " Here, the form is just like, the form of the match is amazing, is similar, without being", "tokens": [51360, 1692, 11, 264, 1254, 307, 445, 411, 11, 264, 1254, 295, 264, 2995, 307, 2243, 11, 307, 2531, 11, 1553, 885, 51676], "temperature": 0.0, "avg_logprob": -0.15196856556322752, "compression_ratio": 1.7993079584775087, "no_speech_prob": 0.0006666296976618469}, {"id": 1146, "seek": 600496, "start": 6031.2, "end": 6034.56, "text": " trained to do that. I mean, it's trained in the sense that it's getting lots of data,", "tokens": [51676, 8895, 281, 360, 300, 13, 286, 914, 11, 309, 311, 8895, 294, 264, 2020, 300, 309, 311, 1242, 3195, 295, 1412, 11, 51844], "temperature": 0.0, "avg_logprob": -0.15196856556322752, "compression_ratio": 1.7993079584775087, "no_speech_prob": 0.0006666296976618469}, {"id": 1147, "seek": 603456, "start": 6034.56, "end": 6039.280000000001, "text": " which is just like human data. But it's not being trained on, you know,", "tokens": [50364, 597, 307, 445, 411, 1952, 1412, 13, 583, 309, 311, 406, 885, 8895, 322, 11, 291, 458, 11, 50600], "temperature": 0.0, "avg_logprob": -0.1158310695759301, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0027143489569425583}, {"id": 1148, "seek": 603456, "start": 6040.080000000001, "end": 6044.160000000001, "text": " bad sentences and being told what's bad. It just can't do those. It'll actually say things like,", "tokens": [50640, 1578, 16579, 293, 885, 1907, 437, 311, 1578, 13, 467, 445, 393, 380, 360, 729, 13, 467, 603, 767, 584, 721, 411, 11, 50844], "temperature": 0.0, "avg_logprob": -0.1158310695759301, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0027143489569425583}, {"id": 1149, "seek": 603456, "start": 6044.160000000001, "end": 6048.72, "text": " those are too hard for me to complete or something, which is kind of interesting.", "tokens": [50844, 729, 366, 886, 1152, 337, 385, 281, 3566, 420, 746, 11, 597, 307, 733, 295, 1880, 13, 51072], "temperature": 0.0, "avg_logprob": -0.1158310695759301, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0027143489569425583}, {"id": 1150, "seek": 603456, "start": 6048.72, "end": 6050.160000000001, "text": " Actually, kind of, how does it know that? I don't know.", "tokens": [51072, 5135, 11, 733, 295, 11, 577, 775, 309, 458, 300, 30, 286, 500, 380, 458, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1158310695759301, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0027143489569425583}, {"id": 1151, "seek": 603456, "start": 6050.88, "end": 6058.320000000001, "text": " But it really often doesn't just complete, very often says stuff that's true.", "tokens": [51180, 583, 309, 534, 2049, 1177, 380, 445, 3566, 11, 588, 2049, 1619, 1507, 300, 311, 2074, 13, 51552], "temperature": 0.0, "avg_logprob": -0.1158310695759301, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0027143489569425583}, {"id": 1152, "seek": 605832, "start": 6058.96, "end": 6064.88, "text": " And sometimes says stuff that's not true. And almost always the form is great.", "tokens": [50396, 400, 2171, 1619, 1507, 300, 311, 406, 2074, 13, 400, 1920, 1009, 264, 1254, 307, 869, 13, 50692], "temperature": 0.0, "avg_logprob": -0.15118574541668559, "compression_ratio": 1.6384976525821595, "no_speech_prob": 0.05338136479258537}, {"id": 1153, "seek": 605832, "start": 6065.759999999999, "end": 6072.48, "text": " But it's still very surprising that with really great form, it's able to generate a lot of things", "tokens": [50736, 583, 309, 311, 920, 588, 8830, 300, 365, 534, 869, 1254, 11, 309, 311, 1075, 281, 8460, 257, 688, 295, 721, 51072], "temperature": 0.0, "avg_logprob": -0.15118574541668559, "compression_ratio": 1.6384976525821595, "no_speech_prob": 0.05338136479258537}, {"id": 1154, "seek": 605832, "start": 6072.48, "end": 6080.48, "text": " that are true based on what is trained on and so on. So it's not just form that is generating.", "tokens": [51072, 300, 366, 2074, 2361, 322, 437, 307, 8895, 322, 293, 370, 322, 13, 407, 309, 311, 406, 445, 1254, 300, 307, 17746, 13, 51472], "temperature": 0.0, "avg_logprob": -0.15118574541668559, "compression_ratio": 1.6384976525821595, "no_speech_prob": 0.05338136479258537}, {"id": 1155, "seek": 605832, "start": 6081.28, "end": 6087.92, "text": " It's mimicking true statements from the internet. I guess the underlying idea", "tokens": [51512, 467, 311, 12247, 10401, 2074, 12363, 490, 264, 4705, 13, 286, 2041, 264, 14217, 1558, 51844], "temperature": 0.0, "avg_logprob": -0.15118574541668559, "compression_ratio": 1.6384976525821595, "no_speech_prob": 0.05338136479258537}, {"id": 1156, "seek": 608792, "start": 6087.92, "end": 6093.2, "text": " there is that on the internet, truth is overrepresented versus falsehood.", "tokens": [50364, 456, 307, 300, 322, 264, 4705, 11, 3494, 307, 670, 38293, 5717, 7908, 3809, 13, 50628], "temperature": 0.0, "avg_logprob": -0.17456571405584162, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.0013654484646394849}, {"id": 1157, "seek": 608792, "start": 6093.2, "end": 6094.8, "text": " I think that's probably right. Yeah.", "tokens": [50628, 286, 519, 300, 311, 1391, 558, 13, 865, 13, 50708], "temperature": 0.0, "avg_logprob": -0.17456571405584162, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.0013654484646394849}, {"id": 1158, "seek": 608792, "start": 6094.8, "end": 6098.88, "text": " So but the fundamental thing is trained on, you're saying is just form.", "tokens": [50708, 407, 457, 264, 8088, 551, 307, 8895, 322, 11, 291, 434, 1566, 307, 445, 1254, 13, 50912], "temperature": 0.0, "avg_logprob": -0.17456571405584162, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.0013654484646394849}, {"id": 1159, "seek": 608792, "start": 6098.88, "end": 6101.28, "text": " I think so. Yeah. Yeah, I think so.", "tokens": [50912, 286, 519, 370, 13, 865, 13, 865, 11, 286, 519, 370, 13, 51032], "temperature": 0.0, "avg_logprob": -0.17456571405584162, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.0013654484646394849}, {"id": 1160, "seek": 608792, "start": 6102.0, "end": 6107.04, "text": " Well, that's a sad, if that's, to me, that's still a little bit of open question. I probably", "tokens": [51068, 1042, 11, 300, 311, 257, 4227, 11, 498, 300, 311, 11, 281, 385, 11, 300, 311, 920, 257, 707, 857, 295, 1269, 1168, 13, 286, 1391, 51320], "temperature": 0.0, "avg_logprob": -0.17456571405584162, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.0013654484646394849}, {"id": 1161, "seek": 608792, "start": 6107.04, "end": 6114.08, "text": " lean agreeing with you, especially now you've just blown my mind that there's a separate", "tokens": [51320, 11659, 36900, 365, 291, 11, 2318, 586, 291, 600, 445, 16479, 452, 1575, 300, 456, 311, 257, 4994, 51672], "temperature": 0.0, "avg_logprob": -0.17456571405584162, "compression_ratio": 1.694915254237288, "no_speech_prob": 0.0013654484646394849}, {"id": 1162, "seek": 611408, "start": 6114.08, "end": 6120.72, "text": " module in the brain for language versus thinking. Maybe there's a fundamental part missing from", "tokens": [50364, 10088, 294, 264, 3567, 337, 2856, 5717, 1953, 13, 2704, 456, 311, 257, 8088, 644, 5361, 490, 50696], "temperature": 0.0, "avg_logprob": -0.12295592703470369, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.002888641320168972}, {"id": 1163, "seek": 611408, "start": 6121.5199999999995, "end": 6126.88, "text": " the large language model approach that lacks the thinking, the reasoning capability.", "tokens": [50736, 264, 2416, 2856, 2316, 3109, 300, 31132, 264, 1953, 11, 264, 21577, 13759, 13, 51004], "temperature": 0.0, "avg_logprob": -0.12295592703470369, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.002888641320168972}, {"id": 1164, "seek": 611408, "start": 6128.24, "end": 6134.5599999999995, "text": " Yeah, that's what this group argues. So the same group, Fedorenko's group has a", "tokens": [51072, 865, 11, 300, 311, 437, 341, 1594, 38218, 13, 407, 264, 912, 1594, 11, 7772, 10948, 4093, 311, 1594, 575, 257, 51388], "temperature": 0.0, "avg_logprob": -0.12295592703470369, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.002888641320168972}, {"id": 1165, "seek": 611408, "start": 6134.5599999999995, "end": 6140.4, "text": " recent paper arguing exactly that. There's a guy called Kyle Mahwell who's here in Austin,", "tokens": [51388, 5162, 3035, 19697, 2293, 300, 13, 821, 311, 257, 2146, 1219, 18023, 10104, 6326, 567, 311, 510, 294, 15356, 11, 51680], "temperature": 0.0, "avg_logprob": -0.12295592703470369, "compression_ratio": 1.5669642857142858, "no_speech_prob": 0.002888641320168972}, {"id": 1166, "seek": 614040, "start": 6140.4, "end": 6144.96, "text": " Texas, actually. He's an old student of mine, but he's a faculty in linguistics at Texas,", "tokens": [50364, 7885, 11, 767, 13, 634, 311, 364, 1331, 3107, 295, 3892, 11, 457, 415, 311, 257, 6389, 294, 21766, 6006, 412, 7885, 11, 50592], "temperature": 0.0, "avg_logprob": -0.16085581141194022, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0030273774173110723}, {"id": 1167, "seek": 614040, "start": 6144.96, "end": 6150.08, "text": " and he was the first author on that. That's fascinating. Still, to me, an open question.", "tokens": [50592, 293, 415, 390, 264, 700, 3793, 322, 300, 13, 663, 311, 10343, 13, 8291, 11, 281, 385, 11, 364, 1269, 1168, 13, 50848], "temperature": 0.0, "avg_logprob": -0.16085581141194022, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0030273774173110723}, {"id": 1168, "seek": 614040, "start": 6151.12, "end": 6152.879999999999, "text": " What do you have the interesting limits of LLMs?", "tokens": [50900, 708, 360, 291, 362, 264, 1880, 10406, 295, 441, 43, 26386, 30, 50988], "temperature": 0.0, "avg_logprob": -0.16085581141194022, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0030273774173110723}, {"id": 1169, "seek": 614040, "start": 6154.799999999999, "end": 6160.799999999999, "text": " I don't see any limits to their form. Their form is perfect. Yeah, it's pretty much,", "tokens": [51084, 286, 500, 380, 536, 604, 10406, 281, 641, 1254, 13, 6710, 1254, 307, 2176, 13, 865, 11, 309, 311, 1238, 709, 11, 51384], "temperature": 0.0, "avg_logprob": -0.16085581141194022, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0030273774173110723}, {"id": 1170, "seek": 614040, "start": 6160.799999999999, "end": 6164.96, "text": " I mean, it's close to- Well, you said ability to complete central embeddings.", "tokens": [51384, 286, 914, 11, 309, 311, 1998, 281, 12, 1042, 11, 291, 848, 3485, 281, 3566, 5777, 12240, 29432, 13, 51592], "temperature": 0.0, "avg_logprob": -0.16085581141194022, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0030273774173110723}, {"id": 1171, "seek": 614040, "start": 6164.96, "end": 6167.599999999999, "text": " Yeah, it's just the same as humans. It seems the same.", "tokens": [51592, 865, 11, 309, 311, 445, 264, 912, 382, 6255, 13, 467, 2544, 264, 912, 13, 51724], "temperature": 0.0, "avg_logprob": -0.16085581141194022, "compression_ratio": 1.5949820788530467, "no_speech_prob": 0.0030273774173110723}, {"id": 1172, "seek": 616760, "start": 6167.6, "end": 6171.92, "text": " But that's not perfect, right? It should be- That's good. No, but I want to be like humans.", "tokens": [50364, 583, 300, 311, 406, 2176, 11, 558, 30, 467, 820, 312, 12, 663, 311, 665, 13, 883, 11, 457, 286, 528, 281, 312, 411, 6255, 13, 50580], "temperature": 0.0, "avg_logprob": -0.21029059092203775, "compression_ratio": 1.8768115942028984, "no_speech_prob": 0.006289335899055004}, {"id": 1173, "seek": 616760, "start": 6171.92, "end": 6176.160000000001, "text": " I'm trying to, I want a model of humans. Oh, wait, wait, wait, wait, wait. Also,", "tokens": [50580, 286, 478, 1382, 281, 11, 286, 528, 257, 2316, 295, 6255, 13, 876, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 11, 1699, 13, 2743, 11, 50792], "temperature": 0.0, "avg_logprob": -0.21029059092203775, "compression_ratio": 1.8768115942028984, "no_speech_prob": 0.006289335899055004}, {"id": 1174, "seek": 616760, "start": 6176.160000000001, "end": 6179.68, "text": " perfect is as close to humans as possible. I got it. Yeah.", "tokens": [50792, 2176, 307, 382, 1998, 281, 6255, 382, 1944, 13, 286, 658, 309, 13, 865, 13, 50968], "temperature": 0.0, "avg_logprob": -0.21029059092203775, "compression_ratio": 1.8768115942028984, "no_speech_prob": 0.006289335899055004}, {"id": 1175, "seek": 616760, "start": 6179.68, "end": 6183.200000000001, "text": " But you should be able to, if you're not human, you're like you're super human,", "tokens": [50968, 583, 291, 820, 312, 1075, 281, 11, 498, 291, 434, 406, 1952, 11, 291, 434, 411, 291, 434, 1687, 1952, 11, 51144], "temperature": 0.0, "avg_logprob": -0.21029059092203775, "compression_ratio": 1.8768115942028984, "no_speech_prob": 0.006289335899055004}, {"id": 1176, "seek": 616760, "start": 6183.200000000001, "end": 6186.0, "text": " you should be able to complete central embedded sentences, right?", "tokens": [51144, 291, 820, 312, 1075, 281, 3566, 5777, 16741, 16579, 11, 558, 30, 51284], "temperature": 0.0, "avg_logprob": -0.21029059092203775, "compression_ratio": 1.8768115942028984, "no_speech_prob": 0.006289335899055004}, {"id": 1177, "seek": 616760, "start": 6186.64, "end": 6192.0, "text": " I mean, that's the mechanism is, if it's modeling some, I think it's kind of", "tokens": [51316, 286, 914, 11, 300, 311, 264, 7513, 307, 11, 498, 309, 311, 15983, 512, 11, 286, 519, 309, 311, 733, 295, 51584], "temperature": 0.0, "avg_logprob": -0.21029059092203775, "compression_ratio": 1.8768115942028984, "no_speech_prob": 0.006289335899055004}, {"id": 1178, "seek": 616760, "start": 6192.0, "end": 6194.240000000001, "text": " really interesting that it can't. That it's really interesting.", "tokens": [51584, 534, 1880, 300, 309, 393, 380, 13, 663, 309, 311, 534, 1880, 13, 51696], "temperature": 0.0, "avg_logprob": -0.21029059092203775, "compression_ratio": 1.8768115942028984, "no_speech_prob": 0.006289335899055004}, {"id": 1179, "seek": 619424, "start": 6194.32, "end": 6198.5599999999995, "text": " It's more like, like I think it's potentially underlyingly modeling", "tokens": [50368, 467, 311, 544, 411, 11, 411, 286, 519, 309, 311, 7263, 14217, 356, 15983, 50580], "temperature": 0.0, "avg_logprob": -0.17900297745414404, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.0017544074216857553}, {"id": 1180, "seek": 619424, "start": 6198.5599999999995, "end": 6201.679999999999, "text": " something like what the way the form is processed.", "tokens": [50580, 746, 411, 437, 264, 636, 264, 1254, 307, 18846, 13, 50736], "temperature": 0.0, "avg_logprob": -0.17900297745414404, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.0017544074216857553}, {"id": 1181, "seek": 619424, "start": 6201.679999999999, "end": 6206.0, "text": " The form of human language and how humans process the language.", "tokens": [50736, 440, 1254, 295, 1952, 2856, 293, 577, 6255, 1399, 264, 2856, 13, 50952], "temperature": 0.0, "avg_logprob": -0.17900297745414404, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.0017544074216857553}, {"id": 1182, "seek": 619424, "start": 6206.0, "end": 6207.84, "text": " Yes. I think that's plausible.", "tokens": [50952, 1079, 13, 286, 519, 300, 311, 39925, 13, 51044], "temperature": 0.0, "avg_logprob": -0.17900297745414404, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.0017544074216857553}, {"id": 1183, "seek": 619424, "start": 6207.84, "end": 6211.44, "text": " And how they generate language. Process language and general language, that's fascinating.", "tokens": [51044, 400, 577, 436, 8460, 2856, 13, 31093, 2856, 293, 2674, 2856, 11, 300, 311, 10343, 13, 51224], "temperature": 0.0, "avg_logprob": -0.17900297745414404, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.0017544074216857553}, {"id": 1184, "seek": 619424, "start": 6212.639999999999, "end": 6218.639999999999, "text": " So in that sense, they're perfect. If we can just linger on the center embedding thing,", "tokens": [51284, 407, 294, 300, 2020, 11, 436, 434, 2176, 13, 759, 321, 393, 445, 45657, 322, 264, 3056, 12240, 3584, 551, 11, 51584], "temperature": 0.0, "avg_logprob": -0.17900297745414404, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.0017544074216857553}, {"id": 1185, "seek": 619424, "start": 6218.639999999999, "end": 6221.92, "text": " that's hard for LLMs to produce, and that seems really impressive because that's", "tokens": [51584, 300, 311, 1152, 337, 441, 43, 26386, 281, 5258, 11, 293, 300, 2544, 534, 8992, 570, 300, 311, 51748], "temperature": 0.0, "avg_logprob": -0.17900297745414404, "compression_ratio": 1.7715355805243447, "no_speech_prob": 0.0017544074216857553}, {"id": 1186, "seek": 622192, "start": 6221.92, "end": 6227.6, "text": " hard for humans to produce. And how does that connect to the thing we've been talking about", "tokens": [50364, 1152, 337, 6255, 281, 5258, 13, 400, 577, 775, 300, 1745, 281, 264, 551, 321, 600, 668, 1417, 466, 50648], "temperature": 0.0, "avg_logprob": -0.09001420094416691, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0014102308778092265}, {"id": 1187, "seek": 622192, "start": 6227.6, "end": 6234.64, "text": " before, which is the dependency grammar framework in which you view language and the finding that", "tokens": [50648, 949, 11, 597, 307, 264, 33621, 22317, 8388, 294, 597, 291, 1910, 2856, 293, 264, 5006, 300, 51000], "temperature": 0.0, "avg_logprob": -0.09001420094416691, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0014102308778092265}, {"id": 1188, "seek": 622192, "start": 6234.64, "end": 6241.12, "text": " short dependencies seem to be a universal part of language. So why is it hard to complete center", "tokens": [51000, 2099, 36606, 1643, 281, 312, 257, 11455, 644, 295, 2856, 13, 407, 983, 307, 309, 1152, 281, 3566, 3056, 51324], "temperature": 0.0, "avg_logprob": -0.09001420094416691, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0014102308778092265}, {"id": 1189, "seek": 622192, "start": 6241.12, "end": 6245.52, "text": " embeddings? So what I like about dependency grammar is it makes", "tokens": [51324, 12240, 29432, 30, 407, 437, 286, 411, 466, 33621, 22317, 307, 309, 1669, 51544], "temperature": 0.0, "avg_logprob": -0.09001420094416691, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0014102308778092265}, {"id": 1190, "seek": 624552, "start": 6245.68, "end": 6254.4800000000005, "text": " the cognitive cost associated with longer distance connections very transparent.", "tokens": [50372, 264, 15605, 2063, 6615, 365, 2854, 4560, 9271, 588, 12737, 13, 50812], "temperature": 0.0, "avg_logprob": -0.16620616193087595, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.005910441279411316}, {"id": 1191, "seek": 624552, "start": 6254.4800000000005, "end": 6258.72, "text": " Basically, as there's some, it turns out there is a cost associated with producing", "tokens": [50812, 8537, 11, 382, 456, 311, 512, 11, 309, 4523, 484, 456, 307, 257, 2063, 6615, 365, 10501, 51024], "temperature": 0.0, "avg_logprob": -0.16620616193087595, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.005910441279411316}, {"id": 1192, "seek": 624552, "start": 6258.72, "end": 6263.360000000001, "text": " and comprehending connections between words, which are just not beside each other.", "tokens": [51024, 293, 10753, 2029, 9271, 1296, 2283, 11, 597, 366, 445, 406, 15726, 1184, 661, 13, 51256], "temperature": 0.0, "avg_logprob": -0.16620616193087595, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.005910441279411316}, {"id": 1193, "seek": 624552, "start": 6263.360000000001, "end": 6268.56, "text": " The further apart they are, the worse it is according to, well, we can measure that.", "tokens": [51256, 440, 3052, 4936, 436, 366, 11, 264, 5324, 309, 307, 4650, 281, 11, 731, 11, 321, 393, 3481, 300, 13, 51516], "temperature": 0.0, "avg_logprob": -0.16620616193087595, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.005910441279411316}, {"id": 1194, "seek": 624552, "start": 6268.56, "end": 6270.240000000001, "text": " And there is a cost associated with that.", "tokens": [51516, 400, 456, 307, 257, 2063, 6615, 365, 300, 13, 51600], "temperature": 0.0, "avg_logprob": -0.16620616193087595, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.005910441279411316}, {"id": 1195, "seek": 624552, "start": 6270.96, "end": 6274.96, "text": " Can you just linger on, what do you mean by cognitive cost and how do you measure it?", "tokens": [51636, 1664, 291, 445, 45657, 322, 11, 437, 360, 291, 914, 538, 15605, 2063, 293, 577, 360, 291, 3481, 309, 30, 51836], "temperature": 0.0, "avg_logprob": -0.16620616193087595, "compression_ratio": 1.8433734939759037, "no_speech_prob": 0.005910441279411316}, {"id": 1196, "seek": 627496, "start": 6275.2, "end": 6279.52, "text": " You can measure it in a lot of ways. The simplest is just asking people to say", "tokens": [50376, 509, 393, 3481, 309, 294, 257, 688, 295, 2098, 13, 440, 22811, 307, 445, 3365, 561, 281, 584, 50592], "temperature": 0.0, "avg_logprob": -0.13991211871711576, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011693136766552925}, {"id": 1197, "seek": 627496, "start": 6280.16, "end": 6286.88, "text": " whether how good a sentence sounds. We just ask, that's one way to measure, and you try to triangulate", "tokens": [50624, 1968, 577, 665, 257, 8174, 3263, 13, 492, 445, 1029, 11, 300, 311, 472, 636, 281, 3481, 11, 293, 291, 853, 281, 19335, 5256, 50960], "temperature": 0.0, "avg_logprob": -0.13991211871711576, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011693136766552925}, {"id": 1198, "seek": 627496, "start": 6286.88, "end": 6290.96, "text": " then across sentences and across structures to try to figure out what the source of that is.", "tokens": [50960, 550, 2108, 16579, 293, 2108, 9227, 281, 853, 281, 2573, 484, 437, 264, 4009, 295, 300, 307, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13991211871711576, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011693136766552925}, {"id": 1199, "seek": 627496, "start": 6290.96, "end": 6298.4800000000005, "text": " You can look at reading times in controlled materials, in certain kinds of materials,", "tokens": [51164, 509, 393, 574, 412, 3760, 1413, 294, 10164, 5319, 11, 294, 1629, 3685, 295, 5319, 11, 51540], "temperature": 0.0, "avg_logprob": -0.13991211871711576, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011693136766552925}, {"id": 1200, "seek": 627496, "start": 6298.4800000000005, "end": 6301.36, "text": " and then we can measure the dependency distances there.", "tokens": [51540, 293, 550, 321, 393, 3481, 264, 33621, 22182, 456, 13, 51684], "temperature": 0.0, "avg_logprob": -0.13991211871711576, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0011693136766552925}, {"id": 1201, "seek": 630136, "start": 6301.759999999999, "end": 6308.16, "text": " We can, there's a recent study which looked at, we're talking about the brain here,", "tokens": [50384, 492, 393, 11, 456, 311, 257, 5162, 2979, 597, 2956, 412, 11, 321, 434, 1417, 466, 264, 3567, 510, 11, 50704], "temperature": 0.0, "avg_logprob": -0.12447972978864397, "compression_ratio": 2.0511627906976746, "no_speech_prob": 0.004330557305365801}, {"id": 1202, "seek": 630136, "start": 6308.16, "end": 6311.36, "text": " we could look at the language network, okay? We could look at the language network,", "tokens": [50704, 321, 727, 574, 412, 264, 2856, 3209, 11, 1392, 30, 492, 727, 574, 412, 264, 2856, 3209, 11, 50864], "temperature": 0.0, "avg_logprob": -0.12447972978864397, "compression_ratio": 2.0511627906976746, "no_speech_prob": 0.004330557305365801}, {"id": 1203, "seek": 630136, "start": 6311.36, "end": 6316.48, "text": " and we could look at the activation in the language network, and how big the activation is", "tokens": [50864, 293, 321, 727, 574, 412, 264, 24433, 294, 264, 2856, 3209, 11, 293, 577, 955, 264, 24433, 307, 51120], "temperature": 0.0, "avg_logprob": -0.12447972978864397, "compression_ratio": 2.0511627906976746, "no_speech_prob": 0.004330557305365801}, {"id": 1204, "seek": 630136, "start": 6316.48, "end": 6320.96, "text": " depending on the length of the dependencies. And it turns out in just random sentences", "tokens": [51120, 5413, 322, 264, 4641, 295, 264, 36606, 13, 400, 309, 4523, 484, 294, 445, 4974, 16579, 51344], "temperature": 0.0, "avg_logprob": -0.12447972978864397, "compression_ratio": 2.0511627906976746, "no_speech_prob": 0.004330557305365801}, {"id": 1205, "seek": 630136, "start": 6320.96, "end": 6323.679999999999, "text": " that you're listening to, if you're listening to, so it turns out there are people listening to", "tokens": [51344, 300, 291, 434, 4764, 281, 11, 498, 291, 434, 4764, 281, 11, 370, 309, 4523, 484, 456, 366, 561, 4764, 281, 51480], "temperature": 0.0, "avg_logprob": -0.12447972978864397, "compression_ratio": 2.0511627906976746, "no_speech_prob": 0.004330557305365801}, {"id": 1206, "seek": 632368, "start": 6323.68, "end": 6333.12, "text": " stories here. And the longer the dependency is, the stronger the activation in the language", "tokens": [50364, 3676, 510, 13, 400, 264, 2854, 264, 33621, 307, 11, 264, 7249, 264, 24433, 294, 264, 2856, 50836], "temperature": 0.0, "avg_logprob": -0.16447135523745887, "compression_ratio": 1.808695652173913, "no_speech_prob": 0.09530685842037201}, {"id": 1207, "seek": 632368, "start": 6333.12, "end": 6337.280000000001, "text": " network. And so there's some measure, there's a bunch of different measures we could do,", "tokens": [50836, 3209, 13, 400, 370, 456, 311, 512, 3481, 11, 456, 311, 257, 3840, 295, 819, 8000, 321, 727, 360, 11, 51044], "temperature": 0.0, "avg_logprob": -0.16447135523745887, "compression_ratio": 1.808695652173913, "no_speech_prob": 0.09530685842037201}, {"id": 1208, "seek": 632368, "start": 6337.280000000001, "end": 6342.0, "text": " that's a kind of a neat measure actually of actual activation in the brain.", "tokens": [51044, 300, 311, 257, 733, 295, 257, 10654, 3481, 767, 295, 3539, 24433, 294, 264, 3567, 13, 51280], "temperature": 0.0, "avg_logprob": -0.16447135523745887, "compression_ratio": 1.808695652173913, "no_speech_prob": 0.09530685842037201}, {"id": 1209, "seek": 632368, "start": 6342.0, "end": 6345.92, "text": " So that you can somehow in different ways convert it to a number. I wonder if there's a", "tokens": [51280, 407, 300, 291, 393, 6063, 294, 819, 2098, 7620, 309, 281, 257, 1230, 13, 286, 2441, 498, 456, 311, 257, 51476], "temperature": 0.0, "avg_logprob": -0.16447135523745887, "compression_ratio": 1.808695652173913, "no_speech_prob": 0.09530685842037201}, {"id": 1210, "seek": 632368, "start": 6345.92, "end": 6349.360000000001, "text": " beautiful equation connecting cognitive costs and length of dependency,", "tokens": [51476, 2238, 5367, 11015, 15605, 5497, 293, 4641, 295, 33621, 11, 51648], "temperature": 0.0, "avg_logprob": -0.16447135523745887, "compression_ratio": 1.808695652173913, "no_speech_prob": 0.09530685842037201}, {"id": 1211, "seek": 634936, "start": 6349.44, "end": 6354.88, "text": " equals MC squared kind of thing. Yeah, it's complicated, but probably it's doable. I would", "tokens": [50368, 6915, 8797, 8889, 733, 295, 551, 13, 865, 11, 309, 311, 6179, 11, 457, 1391, 309, 311, 41183, 13, 286, 576, 50640], "temperature": 0.0, "avg_logprob": -0.154580930384194, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.042639218270778656}, {"id": 1212, "seek": 634936, "start": 6354.88, "end": 6360.96, "text": " guess it's doable. I tried to do that a while ago, and I was reasonably successful, but for", "tokens": [50640, 2041, 309, 311, 41183, 13, 286, 3031, 281, 360, 300, 257, 1339, 2057, 11, 293, 286, 390, 23551, 4406, 11, 457, 337, 50944], "temperature": 0.0, "avg_logprob": -0.154580930384194, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.042639218270778656}, {"id": 1213, "seek": 634936, "start": 6360.96, "end": 6364.719999999999, "text": " some reason I stopped working on that. I agree with you that it would be nice to figure out.", "tokens": [50944, 512, 1778, 286, 5936, 1364, 322, 300, 13, 286, 3986, 365, 291, 300, 309, 576, 312, 1481, 281, 2573, 484, 13, 51132], "temperature": 0.0, "avg_logprob": -0.154580930384194, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.042639218270778656}, {"id": 1214, "seek": 634936, "start": 6364.719999999999, "end": 6369.92, "text": " So there's like some way to figure out the cost. I mean, it's complicated. Another issue you raised", "tokens": [51132, 407, 456, 311, 411, 512, 636, 281, 2573, 484, 264, 2063, 13, 286, 914, 11, 309, 311, 6179, 13, 3996, 2734, 291, 6005, 51392], "temperature": 0.0, "avg_logprob": -0.154580930384194, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.042639218270778656}, {"id": 1215, "seek": 634936, "start": 6369.92, "end": 6375.36, "text": " before was like, how do you measure distance? Is it words? It probably isn't, is it part of the", "tokens": [51392, 949, 390, 411, 11, 577, 360, 291, 3481, 4560, 30, 1119, 309, 2283, 30, 467, 1391, 1943, 380, 11, 307, 309, 644, 295, 264, 51664], "temperature": 0.0, "avg_logprob": -0.154580930384194, "compression_ratio": 1.7640449438202248, "no_speech_prob": 0.042639218270778656}, {"id": 1216, "seek": 637536, "start": 6375.36, "end": 6382.32, "text": " problem? Is that some words matter than more than others? And probably, meaning like nouns might", "tokens": [50364, 1154, 30, 1119, 300, 512, 2283, 1871, 813, 544, 813, 2357, 30, 400, 1391, 11, 3620, 411, 48184, 1062, 50712], "temperature": 0.0, "avg_logprob": -0.142876225658971, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.047401390969753265}, {"id": 1217, "seek": 637536, "start": 6382.32, "end": 6386.08, "text": " matter depending, and then it maybe depends on which kind of noun. Is it a noun we've already", "tokens": [50712, 1871, 5413, 11, 293, 550, 309, 1310, 5946, 322, 597, 733, 295, 23307, 13, 1119, 309, 257, 23307, 321, 600, 1217, 50900], "temperature": 0.0, "avg_logprob": -0.142876225658971, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.047401390969753265}, {"id": 1218, "seek": 637536, "start": 6386.08, "end": 6391.28, "text": " introduced or a noun that's already been mentioned? Is it a pronoun versus a name? Like all these", "tokens": [50900, 7268, 420, 257, 23307, 300, 311, 1217, 668, 2835, 30, 1119, 309, 257, 14144, 5717, 257, 1315, 30, 1743, 439, 613, 51160], "temperature": 0.0, "avg_logprob": -0.142876225658971, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.047401390969753265}, {"id": 1219, "seek": 637536, "start": 6391.28, "end": 6394.88, "text": " things probably matter. So probably the simplest thing to do is just like, oh, let's forget about", "tokens": [51160, 721, 1391, 1871, 13, 407, 1391, 264, 22811, 551, 281, 360, 307, 445, 411, 11, 1954, 11, 718, 311, 2870, 466, 51340], "temperature": 0.0, "avg_logprob": -0.142876225658971, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.047401390969753265}, {"id": 1220, "seek": 637536, "start": 6394.88, "end": 6402.24, "text": " all that and just think about words or morphemes. For sure. But there might be some insight in a", "tokens": [51340, 439, 300, 293, 445, 519, 466, 2283, 420, 25778, 443, 279, 13, 1171, 988, 13, 583, 456, 1062, 312, 512, 11269, 294, 257, 51708], "temperature": 0.0, "avg_logprob": -0.142876225658971, "compression_ratio": 1.7374100719424461, "no_speech_prob": 0.047401390969753265}, {"id": 1221, "seek": 640224, "start": 6402.24, "end": 6412.32, "text": " kind of function that fits the data, meaning like what... I think it's an exponential. So we", "tokens": [50364, 733, 295, 2445, 300, 9001, 264, 1412, 11, 3620, 411, 437, 485, 286, 519, 309, 311, 364, 21510, 13, 407, 321, 50868], "temperature": 0.0, "avg_logprob": -0.12274991472562154, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.0035933409817516804}, {"id": 1222, "seek": 640224, "start": 6412.32, "end": 6417.12, "text": " think it's probably an exponential such that the longer the distance, the less it matters.", "tokens": [50868, 519, 309, 311, 1391, 364, 21510, 1270, 300, 264, 2854, 264, 4560, 11, 264, 1570, 309, 7001, 13, 51108], "temperature": 0.0, "avg_logprob": -0.12274991472562154, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.0035933409817516804}, {"id": 1223, "seek": 640224, "start": 6417.12, "end": 6422.96, "text": " And so then it's the sum of those. That was our best guess a while ago. So you've got a bunch", "tokens": [51108, 400, 370, 550, 309, 311, 264, 2408, 295, 729, 13, 663, 390, 527, 1151, 2041, 257, 1339, 2057, 13, 407, 291, 600, 658, 257, 3840, 51400], "temperature": 0.0, "avg_logprob": -0.12274991472562154, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.0035933409817516804}, {"id": 1224, "seek": 640224, "start": 6422.96, "end": 6428.48, "text": " of dependencies. If you've got a bunch of them that are being connected at some point, at the ends", "tokens": [51400, 295, 36606, 13, 759, 291, 600, 658, 257, 3840, 295, 552, 300, 366, 885, 4582, 412, 512, 935, 11, 412, 264, 5314, 51676], "temperature": 0.0, "avg_logprob": -0.12274991472562154, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.0035933409817516804}, {"id": 1225, "seek": 642848, "start": 6428.48, "end": 6435.28, "text": " of those, the cost is some exponential function of those is my guess. But because the reason it's", "tokens": [50364, 295, 729, 11, 264, 2063, 307, 512, 21510, 2445, 295, 729, 307, 452, 2041, 13, 583, 570, 264, 1778, 309, 311, 50704], "temperature": 0.0, "avg_logprob": -0.13288172846255095, "compression_ratio": 1.6782006920415224, "no_speech_prob": 0.011330043897032738}, {"id": 1226, "seek": 642848, "start": 6435.28, "end": 6440.08, "text": " probably an exponential is like, it's not just the distance between two words, because I can make a", "tokens": [50704, 1391, 364, 21510, 307, 411, 11, 309, 311, 406, 445, 264, 4560, 1296, 732, 2283, 11, 570, 286, 393, 652, 257, 50944], "temperature": 0.0, "avg_logprob": -0.13288172846255095, "compression_ratio": 1.6782006920415224, "no_speech_prob": 0.011330043897032738}, {"id": 1227, "seek": 642848, "start": 6440.08, "end": 6445.04, "text": " very, very long subject verb depends by adding lots and lots of noun phrases and prepositional", "tokens": [50944, 588, 11, 588, 938, 3983, 9595, 5946, 538, 5127, 3195, 293, 3195, 295, 23307, 20312, 293, 2666, 329, 2628, 51192], "temperature": 0.0, "avg_logprob": -0.13288172846255095, "compression_ratio": 1.6782006920415224, "no_speech_prob": 0.011330043897032738}, {"id": 1228, "seek": 642848, "start": 6445.04, "end": 6450.08, "text": " phrases. And it doesn't matter too much. It's when you do nest it, when I have multiple of these,", "tokens": [51192, 20312, 13, 400, 309, 1177, 380, 1871, 886, 709, 13, 467, 311, 562, 291, 360, 15646, 309, 11, 562, 286, 362, 3866, 295, 613, 11, 51444], "temperature": 0.0, "avg_logprob": -0.13288172846255095, "compression_ratio": 1.6782006920415224, "no_speech_prob": 0.011330043897032738}, {"id": 1229, "seek": 642848, "start": 6450.719999999999, "end": 6456.799999999999, "text": " then things go really bad, go south. Probably somehow connected to working memory or something", "tokens": [51476, 550, 721, 352, 534, 1578, 11, 352, 7377, 13, 9210, 6063, 4582, 281, 1364, 4675, 420, 746, 51780], "temperature": 0.0, "avg_logprob": -0.13288172846255095, "compression_ratio": 1.6782006920415224, "no_speech_prob": 0.011330043897032738}, {"id": 1230, "seek": 645680, "start": 6457.360000000001, "end": 6463.360000000001, "text": " that's probably the function of the memory here is the access is trying to find those earlier", "tokens": [50392, 300, 311, 1391, 264, 2445, 295, 264, 4675, 510, 307, 264, 2105, 307, 1382, 281, 915, 729, 3071, 50692], "temperature": 0.0, "avg_logprob": -0.13641323553067503, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.002322692424058914}, {"id": 1231, "seek": 645680, "start": 6463.360000000001, "end": 6468.400000000001, "text": " things. It's kind of hard to figure out what was referred to earlier. Those are those connections.", "tokens": [50692, 721, 13, 467, 311, 733, 295, 1152, 281, 2573, 484, 437, 390, 10839, 281, 3071, 13, 3950, 366, 729, 9271, 13, 50944], "temperature": 0.0, "avg_logprob": -0.13641323553067503, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.002322692424058914}, {"id": 1232, "seek": 645680, "start": 6468.400000000001, "end": 6473.360000000001, "text": " That's the sort of notion of working as opposed to a storagey thing, but trying to connect,", "tokens": [50944, 663, 311, 264, 1333, 295, 10710, 295, 1364, 382, 8851, 281, 257, 6725, 88, 551, 11, 457, 1382, 281, 1745, 11, 51192], "temperature": 0.0, "avg_logprob": -0.13641323553067503, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.002322692424058914}, {"id": 1233, "seek": 645680, "start": 6474.88, "end": 6479.2, "text": " retrieve those earlier words depending on what was in between. And then we're talking about", "tokens": [51268, 30254, 729, 3071, 2283, 5413, 322, 437, 390, 294, 1296, 13, 400, 550, 321, 434, 1417, 466, 51484], "temperature": 0.0, "avg_logprob": -0.13641323553067503, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.002322692424058914}, {"id": 1234, "seek": 645680, "start": 6479.2, "end": 6484.24, "text": " interference of similar things in between. That's the right theory probably has that kind of notion", "tokens": [51484, 24497, 295, 2531, 721, 294, 1296, 13, 663, 311, 264, 558, 5261, 1391, 575, 300, 733, 295, 10710, 51736], "temperature": 0.0, "avg_logprob": -0.13641323553067503, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.002322692424058914}, {"id": 1235, "seek": 648424, "start": 6484.24, "end": 6488.8, "text": " and it is an interference of similar. And so I'm dealing with an abstraction over the right", "tokens": [50364, 293, 309, 307, 364, 24497, 295, 2531, 13, 400, 370, 286, 478, 6260, 365, 364, 37765, 670, 264, 558, 50592], "temperature": 0.0, "avg_logprob": -0.11307811737060547, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.00218252744525671}, {"id": 1236, "seek": 648424, "start": 6488.8, "end": 6493.2, "text": " theory, which is just, it's count words, it's not right, but it's close. And then maybe you're", "tokens": [50592, 5261, 11, 597, 307, 445, 11, 309, 311, 1207, 2283, 11, 309, 311, 406, 558, 11, 457, 309, 311, 1998, 13, 400, 550, 1310, 291, 434, 50812], "temperature": 0.0, "avg_logprob": -0.11307811737060547, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.00218252744525671}, {"id": 1237, "seek": 648424, "start": 6493.2, "end": 6498.8, "text": " right though, there's some sort of an exponential or something to figure out the total so we can", "tokens": [50812, 558, 1673, 11, 456, 311, 512, 1333, 295, 364, 21510, 420, 746, 281, 2573, 484, 264, 3217, 370, 321, 393, 51092], "temperature": 0.0, "avg_logprob": -0.11307811737060547, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.00218252744525671}, {"id": 1238, "seek": 648424, "start": 6498.8, "end": 6504.8, "text": " figure out a function for any given sentence in any given language. But it's funny, people haven't", "tokens": [51092, 2573, 484, 257, 2445, 337, 604, 2212, 8174, 294, 604, 2212, 2856, 13, 583, 309, 311, 4074, 11, 561, 2378, 380, 51392], "temperature": 0.0, "avg_logprob": -0.11307811737060547, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.00218252744525671}, {"id": 1239, "seek": 648424, "start": 6504.8, "end": 6509.76, "text": " done that too much, which I do think is, I'm interested that you find that interesting. I", "tokens": [51392, 1096, 300, 886, 709, 11, 597, 286, 360, 519, 307, 11, 286, 478, 3102, 300, 291, 915, 300, 1880, 13, 286, 51640], "temperature": 0.0, "avg_logprob": -0.11307811737060547, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.00218252744525671}, {"id": 1240, "seek": 648424, "start": 6509.76, "end": 6513.04, "text": " really find that interesting. And a lot of people haven't found it interesting. And I don't know", "tokens": [51640, 534, 915, 300, 1880, 13, 400, 257, 688, 295, 561, 2378, 380, 1352, 309, 1880, 13, 400, 286, 500, 380, 458, 51804], "temperature": 0.0, "avg_logprob": -0.11307811737060547, "compression_ratio": 1.8841059602649006, "no_speech_prob": 0.00218252744525671}, {"id": 1241, "seek": 651304, "start": 6513.04, "end": 6516.48, "text": " why I haven't got people to want to work on that. I really like that too.", "tokens": [50364, 983, 286, 2378, 380, 658, 561, 281, 528, 281, 589, 322, 300, 13, 286, 534, 411, 300, 886, 13, 50536], "temperature": 0.0, "avg_logprob": -0.18892017257547825, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.0015722152311354876}, {"id": 1242, "seek": 651304, "start": 6517.76, "end": 6521.04, "text": " That's a beautiful, in the underlying idea is beautiful that there's a cognitive", "tokens": [50600, 663, 311, 257, 2238, 11, 294, 264, 14217, 1558, 307, 2238, 300, 456, 311, 257, 15605, 50764], "temperature": 0.0, "avg_logprob": -0.18892017257547825, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.0015722152311354876}, {"id": 1243, "seek": 651304, "start": 6521.04, "end": 6525.92, "text": " cost that correlates with the length of dependency. It just, it feels like it's a deep,", "tokens": [50764, 2063, 300, 13983, 1024, 365, 264, 4641, 295, 33621, 13, 467, 445, 11, 309, 3417, 411, 309, 311, 257, 2452, 11, 51008], "temperature": 0.0, "avg_logprob": -0.18892017257547825, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.0015722152311354876}, {"id": 1244, "seek": 651304, "start": 6525.92, "end": 6530.64, "text": " I mean, language is so fundamental to the human experience. And this is a nice clean", "tokens": [51008, 286, 914, 11, 2856, 307, 370, 8088, 281, 264, 1952, 1752, 13, 400, 341, 307, 257, 1481, 2541, 51244], "temperature": 0.0, "avg_logprob": -0.18892017257547825, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.0015722152311354876}, {"id": 1245, "seek": 651304, "start": 6531.36, "end": 6538.88, "text": " theory of language where it's like, wow, okay. So like we like our words close together,", "tokens": [51280, 5261, 295, 2856, 689, 309, 311, 411, 11, 6076, 11, 1392, 13, 407, 411, 321, 411, 527, 2283, 1998, 1214, 11, 51656], "temperature": 0.0, "avg_logprob": -0.18892017257547825, "compression_ratio": 1.6442687747035574, "no_speech_prob": 0.0015722152311354876}, {"id": 1246, "seek": 653888, "start": 6538.96, "end": 6542.72, "text": " dependent words close together. That's why I like it too. It's so simple.", "tokens": [50368, 12334, 2283, 1998, 1214, 13, 663, 311, 983, 286, 411, 309, 886, 13, 467, 311, 370, 2199, 13, 50556], "temperature": 0.0, "avg_logprob": -0.12129676505310895, "compression_ratio": 1.79672131147541, "no_speech_prob": 0.005908896680921316}, {"id": 1247, "seek": 653888, "start": 6542.72, "end": 6547.12, "text": " Yeah, the simplicity of the theory. And yet it explains some very complicated phenomena.", "tokens": [50556, 865, 11, 264, 25632, 295, 264, 5261, 13, 400, 1939, 309, 13948, 512, 588, 6179, 22004, 13, 50776], "temperature": 0.0, "avg_logprob": -0.12129676505310895, "compression_ratio": 1.79672131147541, "no_speech_prob": 0.005908896680921316}, {"id": 1248, "seek": 653888, "start": 6547.12, "end": 6551.4400000000005, "text": " If I write these very complicated sentences, it's kind of hard to know why they're so hard.", "tokens": [50776, 759, 286, 2464, 613, 588, 6179, 16579, 11, 309, 311, 733, 295, 1152, 281, 458, 983, 436, 434, 370, 1152, 13, 50992], "temperature": 0.0, "avg_logprob": -0.12129676505310895, "compression_ratio": 1.79672131147541, "no_speech_prob": 0.005908896680921316}, {"id": 1249, "seek": 653888, "start": 6551.4400000000005, "end": 6556.4800000000005, "text": " And you can like, oh, nail it down. I can do like a, give you a math formula for why each one of", "tokens": [50992, 400, 291, 393, 411, 11, 1954, 11, 10173, 309, 760, 13, 286, 393, 360, 411, 257, 11, 976, 291, 257, 5221, 8513, 337, 983, 1184, 472, 295, 51244], "temperature": 0.0, "avg_logprob": -0.12129676505310895, "compression_ratio": 1.79672131147541, "no_speech_prob": 0.005908896680921316}, {"id": 1250, "seek": 653888, "start": 6556.4800000000005, "end": 6560.96, "text": " them is bad and where. And that's kind of cool. I think that's very neat. Have you gone through", "tokens": [51244, 552, 307, 1578, 293, 689, 13, 400, 300, 311, 733, 295, 1627, 13, 286, 519, 300, 311, 588, 10654, 13, 3560, 291, 2780, 807, 51468], "temperature": 0.0, "avg_logprob": -0.12129676505310895, "compression_ratio": 1.79672131147541, "no_speech_prob": 0.005908896680921316}, {"id": 1251, "seek": 653888, "start": 6560.96, "end": 6567.6, "text": " the process? Is there like a, if you take a piece of text and then simplify, sort of like there's an", "tokens": [51468, 264, 1399, 30, 1119, 456, 411, 257, 11, 498, 291, 747, 257, 2522, 295, 2487, 293, 550, 20460, 11, 1333, 295, 411, 456, 311, 364, 51800], "temperature": 0.0, "avg_logprob": -0.12129676505310895, "compression_ratio": 1.79672131147541, "no_speech_prob": 0.005908896680921316}, {"id": 1252, "seek": 656760, "start": 6567.6, "end": 6575.04, "text": " average length of dependency, and then you like, you know, reduce it and see comprehension on the", "tokens": [50364, 4274, 4641, 295, 33621, 11, 293, 550, 291, 411, 11, 291, 458, 11, 5407, 309, 293, 536, 44991, 322, 264, 50736], "temperature": 0.0, "avg_logprob": -0.14622279575892858, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047280287253670394}, {"id": 1253, "seek": 656760, "start": 6575.04, "end": 6579.76, "text": " entire, not just a single sentence, but like, you know, you go from James Joyce to Hemingway or", "tokens": [50736, 2302, 11, 406, 445, 257, 2167, 8174, 11, 457, 411, 11, 291, 458, 11, 291, 352, 490, 5678, 40044, 281, 18568, 278, 676, 420, 50972], "temperature": 0.0, "avg_logprob": -0.14622279575892858, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047280287253670394}, {"id": 1254, "seek": 656760, "start": 6579.76, "end": 6585.84, "text": " something. No, no, simple answer is no, that does, there's probably things you can do in that,", "tokens": [50972, 746, 13, 883, 11, 572, 11, 2199, 1867, 307, 572, 11, 300, 775, 11, 456, 311, 1391, 721, 291, 393, 360, 294, 300, 11, 51276], "temperature": 0.0, "avg_logprob": -0.14622279575892858, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047280287253670394}, {"id": 1255, "seek": 656760, "start": 6585.84, "end": 6590.160000000001, "text": " in that kind of direction. That's fun. We might, you know, we're gonna talk about legalese at", "tokens": [51276, 294, 300, 733, 295, 3513, 13, 663, 311, 1019, 13, 492, 1062, 11, 291, 458, 11, 321, 434, 799, 751, 466, 5089, 1130, 412, 51492], "temperature": 0.0, "avg_logprob": -0.14622279575892858, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047280287253670394}, {"id": 1256, "seek": 656760, "start": 6590.160000000001, "end": 6595.4400000000005, "text": " some point. And so we, maybe we'll talk about that kind of thinking with applied to legalese.", "tokens": [51492, 512, 935, 13, 400, 370, 321, 11, 1310, 321, 603, 751, 466, 300, 733, 295, 1953, 365, 6456, 281, 5089, 1130, 13, 51756], "temperature": 0.0, "avg_logprob": -0.14622279575892858, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047280287253670394}, {"id": 1257, "seek": 659544, "start": 6595.5199999999995, "end": 6598.08, "text": " Well, let's talk about legalese because you mentioned that as an exception,", "tokens": [50368, 1042, 11, 718, 311, 751, 466, 5089, 1130, 570, 291, 2835, 300, 382, 364, 11183, 11, 50496], "temperature": 0.0, "avg_logprob": -0.13785273329655928, "compression_ratio": 1.803448275862069, "no_speech_prob": 0.030202504247426987}, {"id": 1258, "seek": 659544, "start": 6598.08, "end": 6602.719999999999, "text": " which is taking tangent upon tangent. That's an interesting one. You give it as an exception.", "tokens": [50496, 597, 307, 1940, 27747, 3564, 27747, 13, 663, 311, 364, 1880, 472, 13, 509, 976, 309, 382, 364, 11183, 13, 50728], "temperature": 0.0, "avg_logprob": -0.13785273329655928, "compression_ratio": 1.803448275862069, "no_speech_prob": 0.030202504247426987}, {"id": 1259, "seek": 659544, "start": 6602.719999999999, "end": 6608.24, "text": " It's an exception. That you say that most natural languages, as we've been talking about,", "tokens": [50728, 467, 311, 364, 11183, 13, 663, 291, 584, 300, 881, 3303, 8650, 11, 382, 321, 600, 668, 1417, 466, 11, 51004], "temperature": 0.0, "avg_logprob": -0.13785273329655928, "compression_ratio": 1.803448275862069, "no_speech_prob": 0.030202504247426987}, {"id": 1260, "seek": 659544, "start": 6608.879999999999, "end": 6614.799999999999, "text": " have local dependencies with one exception, legalese. That's right. So what is legalese,", "tokens": [51036, 362, 2654, 36606, 365, 472, 11183, 11, 5089, 1130, 13, 663, 311, 558, 13, 407, 437, 307, 5089, 1130, 11, 51332], "temperature": 0.0, "avg_logprob": -0.13785273329655928, "compression_ratio": 1.803448275862069, "no_speech_prob": 0.030202504247426987}, {"id": 1261, "seek": 659544, "start": 6614.799999999999, "end": 6619.919999999999, "text": " first of all? Oh, well, legalese is what you think it is. It's just any legal language.", "tokens": [51332, 700, 295, 439, 30, 876, 11, 731, 11, 5089, 1130, 307, 437, 291, 519, 309, 307, 13, 467, 311, 445, 604, 5089, 2856, 13, 51588], "temperature": 0.0, "avg_logprob": -0.13785273329655928, "compression_ratio": 1.803448275862069, "no_speech_prob": 0.030202504247426987}, {"id": 1262, "seek": 659544, "start": 6620.639999999999, "end": 6624.719999999999, "text": " I mean, like, I actually know very little about the kind of language that lawyers use.", "tokens": [51624, 286, 914, 11, 411, 11, 286, 767, 458, 588, 707, 466, 264, 733, 295, 2856, 300, 16219, 764, 13, 51828], "temperature": 0.0, "avg_logprob": -0.13785273329655928, "compression_ratio": 1.803448275862069, "no_speech_prob": 0.030202504247426987}, {"id": 1263, "seek": 662472, "start": 6624.72, "end": 6628.320000000001, "text": " So I'm just talking about language in laws and language in contracts.", "tokens": [50364, 407, 286, 478, 445, 1417, 466, 2856, 294, 6064, 293, 2856, 294, 13952, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11443679563460811, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0003569312975741923}, {"id": 1264, "seek": 662472, "start": 6628.320000000001, "end": 6628.8, "text": " Got it.", "tokens": [50544, 5803, 309, 13, 50568], "temperature": 0.0, "avg_logprob": -0.11443679563460811, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0003569312975741923}, {"id": 1265, "seek": 662472, "start": 6628.8, "end": 6634.0, "text": " So the stuff that you have to run into, we have to run into every other day or every day,", "tokens": [50568, 407, 264, 1507, 300, 291, 362, 281, 1190, 666, 11, 321, 362, 281, 1190, 666, 633, 661, 786, 420, 633, 786, 11, 50828], "temperature": 0.0, "avg_logprob": -0.11443679563460811, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0003569312975741923}, {"id": 1266, "seek": 662472, "start": 6635.12, "end": 6640.72, "text": " and you skip over because it reads poorly. And or, you know, partly it's just long, right?", "tokens": [50884, 293, 291, 10023, 670, 570, 309, 15700, 22271, 13, 400, 420, 11, 291, 458, 11, 17031, 309, 311, 445, 938, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.11443679563460811, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0003569312975741923}, {"id": 1267, "seek": 662472, "start": 6640.72, "end": 6645.4400000000005, "text": " There's a lot of text there that we don't really want to know about. And so, but the thing I'm", "tokens": [51164, 821, 311, 257, 688, 295, 2487, 456, 300, 321, 500, 380, 534, 528, 281, 458, 466, 13, 400, 370, 11, 457, 264, 551, 286, 478, 51400], "temperature": 0.0, "avg_logprob": -0.11443679563460811, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0003569312975741923}, {"id": 1268, "seek": 662472, "start": 6645.4400000000005, "end": 6651.6, "text": " interested in, so I've been working with this guy called Eric Martinez, who is a, he was a lawyer", "tokens": [51400, 3102, 294, 11, 370, 286, 600, 668, 1364, 365, 341, 2146, 1219, 9336, 41886, 11, 567, 307, 257, 11, 415, 390, 257, 11613, 51708], "temperature": 0.0, "avg_logprob": -0.11443679563460811, "compression_ratio": 1.645985401459854, "no_speech_prob": 0.0003569312975741923}, {"id": 1269, "seek": 665160, "start": 6652.160000000001, "end": 6656.400000000001, "text": " who was taking my class. I was teaching a psycholinguistics lab class. I haven't been", "tokens": [50392, 567, 390, 1940, 452, 1508, 13, 286, 390, 4571, 257, 4681, 401, 7050, 6006, 2715, 1508, 13, 286, 2378, 380, 668, 50604], "temperature": 0.0, "avg_logprob": -0.15051799011230468, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.005059721413999796}, {"id": 1270, "seek": 665160, "start": 6656.400000000001, "end": 6660.8, "text": " teaching it for a long time at MIT. And he's a, he was a law student at Harvard. And he took the", "tokens": [50604, 4571, 309, 337, 257, 938, 565, 412, 13100, 13, 400, 415, 311, 257, 11, 415, 390, 257, 2101, 3107, 412, 13378, 13, 400, 415, 1890, 264, 50824], "temperature": 0.0, "avg_logprob": -0.15051799011230468, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.005059721413999796}, {"id": 1271, "seek": 665160, "start": 6660.8, "end": 6665.360000000001, "text": " class because he had done some linguistics as an undergrad. And he was interested in the problem of", "tokens": [50824, 1508, 570, 415, 632, 1096, 512, 21766, 6006, 382, 364, 14295, 13, 400, 415, 390, 3102, 294, 264, 1154, 295, 51052], "temperature": 0.0, "avg_logprob": -0.15051799011230468, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.005059721413999796}, {"id": 1272, "seek": 665160, "start": 6666.0, "end": 6672.8, "text": " why legalese sounds hard to understand, you know, why. And so why is it hard to understand and why", "tokens": [51084, 983, 5089, 1130, 3263, 1152, 281, 1223, 11, 291, 458, 11, 983, 13, 400, 370, 983, 307, 309, 1152, 281, 1223, 293, 983, 51424], "temperature": 0.0, "avg_logprob": -0.15051799011230468, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.005059721413999796}, {"id": 1273, "seek": 665160, "start": 6672.8, "end": 6677.280000000001, "text": " do they write that way? If it is hard to understand, it seems apparent that it's hard to understand.", "tokens": [51424, 360, 436, 2464, 300, 636, 30, 759, 309, 307, 1152, 281, 1223, 11, 309, 2544, 18335, 300, 309, 311, 1152, 281, 1223, 13, 51648], "temperature": 0.0, "avg_logprob": -0.15051799011230468, "compression_ratio": 1.8467432950191571, "no_speech_prob": 0.005059721413999796}, {"id": 1274, "seek": 667728, "start": 6677.36, "end": 6684.8, "text": " The question is, why is it? And so we didn't know. And we did an evaluation of a bunch of contracts.", "tokens": [50368, 440, 1168, 307, 11, 983, 307, 309, 30, 400, 370, 321, 994, 380, 458, 13, 400, 321, 630, 364, 13344, 295, 257, 3840, 295, 13952, 13, 50740], "temperature": 0.0, "avg_logprob": -0.10037550966963808, "compression_ratio": 1.82421875, "no_speech_prob": 0.024416273459792137}, {"id": 1275, "seek": 667728, "start": 6684.8, "end": 6688.96, "text": " Actually, we just took a bunch of random contracts, because I don't know, you know,", "tokens": [50740, 5135, 11, 321, 445, 1890, 257, 3840, 295, 4974, 13952, 11, 570, 286, 500, 380, 458, 11, 291, 458, 11, 50948], "temperature": 0.0, "avg_logprob": -0.10037550966963808, "compression_ratio": 1.82421875, "no_speech_prob": 0.024416273459792137}, {"id": 1276, "seek": 667728, "start": 6688.96, "end": 6693.759999999999, "text": " there's contracts and laws might not be exactly the same, but contracts are kind of the things", "tokens": [50948, 456, 311, 13952, 293, 6064, 1062, 406, 312, 2293, 264, 912, 11, 457, 13952, 366, 733, 295, 264, 721, 51188], "temperature": 0.0, "avg_logprob": -0.10037550966963808, "compression_ratio": 1.82421875, "no_speech_prob": 0.024416273459792137}, {"id": 1277, "seek": 667728, "start": 6693.759999999999, "end": 6697.759999999999, "text": " that most people have to deal with most of the time. And so that's kind of the most common", "tokens": [51188, 300, 881, 561, 362, 281, 2028, 365, 881, 295, 264, 565, 13, 400, 370, 300, 311, 733, 295, 264, 881, 2689, 51388], "temperature": 0.0, "avg_logprob": -0.10037550966963808, "compression_ratio": 1.82421875, "no_speech_prob": 0.024416273459792137}, {"id": 1278, "seek": 667728, "start": 6697.759999999999, "end": 6703.759999999999, "text": " thing that humans have, like humans, that adults in our industrialized society have to deal with", "tokens": [51388, 551, 300, 6255, 362, 11, 411, 6255, 11, 300, 8865, 294, 527, 9987, 1602, 4086, 362, 281, 2028, 365, 51688], "temperature": 0.0, "avg_logprob": -0.10037550966963808, "compression_ratio": 1.82421875, "no_speech_prob": 0.024416273459792137}, {"id": 1279, "seek": 670376, "start": 6703.76, "end": 6709.68, "text": " a lot. And so that's what we pulled. And we didn't know what was hard about them. But it turns out", "tokens": [50364, 257, 688, 13, 400, 370, 300, 311, 437, 321, 7373, 13, 400, 321, 994, 380, 458, 437, 390, 1152, 466, 552, 13, 583, 309, 4523, 484, 50660], "temperature": 0.0, "avg_logprob": -0.14343180898892677, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007575665134936571}, {"id": 1280, "seek": 670376, "start": 6709.68, "end": 6714.96, "text": " that the way they're written is very center embedded, has nested structures in them. So", "tokens": [50660, 300, 264, 636, 436, 434, 3720, 307, 588, 3056, 16741, 11, 575, 15646, 292, 9227, 294, 552, 13, 407, 50924], "temperature": 0.0, "avg_logprob": -0.14343180898892677, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007575665134936571}, {"id": 1281, "seek": 670376, "start": 6714.96, "end": 6718.96, "text": " it has low frequency words as well. That's not surprising. Lots of texts have low frequency,", "tokens": [50924, 309, 575, 2295, 7893, 2283, 382, 731, 13, 663, 311, 406, 8830, 13, 15908, 295, 15765, 362, 2295, 7893, 11, 51124], "temperature": 0.0, "avg_logprob": -0.14343180898892677, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007575665134936571}, {"id": 1282, "seek": 670376, "start": 6718.96, "end": 6724.8, "text": " it does have surprising, slightly lower frequency words than other kinds of control texts, even", "tokens": [51124, 309, 775, 362, 8830, 11, 4748, 3126, 7893, 2283, 813, 661, 3685, 295, 1969, 15765, 11, 754, 51416], "temperature": 0.0, "avg_logprob": -0.14343180898892677, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007575665134936571}, {"id": 1283, "seek": 670376, "start": 6724.8, "end": 6730.0, "text": " sort of academic texts. Legalese is even worse. It is the worst that we weren't being able to find.", "tokens": [51416, 1333, 295, 7778, 15765, 13, 33577, 1130, 307, 754, 5324, 13, 467, 307, 264, 5855, 300, 321, 4999, 380, 885, 1075, 281, 915, 13, 51676], "temperature": 0.0, "avg_logprob": -0.14343180898892677, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.007575665134936571}, {"id": 1284, "seek": 673000, "start": 6730.64, "end": 6734.72, "text": " You just reveal the game that lawyers are playing. They're optimizing a different...", "tokens": [50396, 509, 445, 10658, 264, 1216, 300, 16219, 366, 2433, 13, 814, 434, 40425, 257, 819, 485, 50600], "temperature": 0.0, "avg_logprob": -0.2342062066071225, "compression_ratio": 1.793594306049822, "no_speech_prob": 0.02228170447051525}, {"id": 1285, "seek": 673000, "start": 6734.72, "end": 6738.88, "text": " Well, you know, it's interesting. Now you're getting at why. And so, and I don't think...", "tokens": [50600, 1042, 11, 291, 458, 11, 309, 311, 1880, 13, 823, 291, 434, 1242, 412, 983, 13, 400, 370, 11, 293, 286, 500, 380, 519, 485, 50808], "temperature": 0.0, "avg_logprob": -0.2342062066071225, "compression_ratio": 1.793594306049822, "no_speech_prob": 0.02228170447051525}, {"id": 1286, "seek": 673000, "start": 6738.88, "end": 6741.92, "text": " So now you're saying they're doing intentionally. I don't think they're doing intentionally.", "tokens": [50808, 407, 586, 291, 434, 1566, 436, 434, 884, 22062, 13, 286, 500, 380, 519, 436, 434, 884, 22062, 13, 50960], "temperature": 0.0, "avg_logprob": -0.2342062066071225, "compression_ratio": 1.793594306049822, "no_speech_prob": 0.02228170447051525}, {"id": 1287, "seek": 673000, "start": 6741.92, "end": 6744.88, "text": " But let's... It's an emergent phenomena. Okay.", "tokens": [50960, 583, 718, 311, 485, 467, 311, 364, 4345, 6930, 22004, 13, 1033, 13, 51108], "temperature": 0.0, "avg_logprob": -0.2342062066071225, "compression_ratio": 1.793594306049822, "no_speech_prob": 0.02228170447051525}, {"id": 1288, "seek": 673000, "start": 6744.88, "end": 6749.2, "text": " Yeah, yeah, yeah. We'll get to that. We'll get to that. And so, but we wanted to see why. So we", "tokens": [51108, 865, 11, 1338, 11, 1338, 13, 492, 603, 483, 281, 300, 13, 492, 603, 483, 281, 300, 13, 400, 370, 11, 457, 321, 1415, 281, 536, 983, 13, 407, 321, 51324], "temperature": 0.0, "avg_logprob": -0.2342062066071225, "compression_ratio": 1.793594306049822, "no_speech_prob": 0.02228170447051525}, {"id": 1289, "seek": 673000, "start": 6749.2, "end": 6753.44, "text": " see what first as opposed... So because it turns out that we're not the first to observe that", "tokens": [51324, 536, 437, 700, 382, 8851, 485, 407, 570, 309, 4523, 484, 300, 321, 434, 406, 264, 700, 281, 11441, 300, 51536], "temperature": 0.0, "avg_logprob": -0.2342062066071225, "compression_ratio": 1.793594306049822, "no_speech_prob": 0.02228170447051525}, {"id": 1290, "seek": 675344, "start": 6753.44, "end": 6761.599999999999, "text": " legalese is weird, like back to... Nixon had a plain language act in 1970, and Obama had one.", "tokens": [50364, 5089, 1130, 307, 3657, 11, 411, 646, 281, 485, 31130, 632, 257, 11121, 2856, 605, 294, 14577, 11, 293, 9560, 632, 472, 13, 50772], "temperature": 0.0, "avg_logprob": -0.1249393344849579, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.05580771714448929}, {"id": 1291, "seek": 675344, "start": 6761.599999999999, "end": 6767.599999999999, "text": " And boy, a lot of these... A lot of presidents have said, oh, we've got to simplify legal language,", "tokens": [50772, 400, 3237, 11, 257, 688, 295, 613, 485, 316, 688, 295, 27611, 362, 848, 11, 1954, 11, 321, 600, 658, 281, 20460, 5089, 2856, 11, 51072], "temperature": 0.0, "avg_logprob": -0.1249393344849579, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.05580771714448929}, {"id": 1292, "seek": 675344, "start": 6767.599999999999, "end": 6772.24, "text": " must simplify it. But if you don't know how it's complicated, it's not easy to simplify it. You", "tokens": [51072, 1633, 20460, 309, 13, 583, 498, 291, 500, 380, 458, 577, 309, 311, 6179, 11, 309, 311, 406, 1858, 281, 20460, 309, 13, 509, 51304], "temperature": 0.0, "avg_logprob": -0.1249393344849579, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.05580771714448929}, {"id": 1293, "seek": 675344, "start": 6772.24, "end": 6777.12, "text": " need to know what it is you're supposed to do before you can fix it. And so you need to cycle", "tokens": [51304, 643, 281, 458, 437, 309, 307, 291, 434, 3442, 281, 360, 949, 291, 393, 3191, 309, 13, 400, 370, 291, 643, 281, 6586, 51548], "temperature": 0.0, "avg_logprob": -0.1249393344849579, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.05580771714448929}, {"id": 1294, "seek": 675344, "start": 6777.12, "end": 6782.32, "text": " linguists to analyze the text and see what's wrong with it before you can fix it. You don't", "tokens": [51548, 21766, 1751, 281, 12477, 264, 2487, 293, 536, 437, 311, 2085, 365, 309, 949, 291, 393, 3191, 309, 13, 509, 500, 380, 51808], "temperature": 0.0, "avg_logprob": -0.1249393344849579, "compression_ratio": 1.7025089605734767, "no_speech_prob": 0.05580771714448929}, {"id": 1295, "seek": 678232, "start": 6782.32, "end": 6785.44, "text": " know how to fix it. How am I supposed to fix something? I don't know what's wrong with it.", "tokens": [50364, 458, 577, 281, 3191, 309, 13, 1012, 669, 286, 3442, 281, 3191, 746, 30, 286, 500, 380, 458, 437, 311, 2085, 365, 309, 13, 50520], "temperature": 0.0, "avg_logprob": -0.11595291137695313, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.0017005626577883959}, {"id": 1296, "seek": 678232, "start": 6785.44, "end": 6788.5599999999995, "text": " And so what we did was just... That's what we did. We figured out, well, it's okay. We just", "tokens": [50520, 400, 370, 437, 321, 630, 390, 445, 485, 663, 311, 437, 321, 630, 13, 492, 8932, 484, 11, 731, 11, 309, 311, 1392, 13, 492, 445, 50676], "temperature": 0.0, "avg_logprob": -0.11595291137695313, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.0017005626577883959}, {"id": 1297, "seek": 678232, "start": 6788.5599999999995, "end": 6795.04, "text": " took a bunch of contracts, had people, and we encoded them for a bunch of features. And so", "tokens": [50676, 1890, 257, 3840, 295, 13952, 11, 632, 561, 11, 293, 321, 2058, 12340, 552, 337, 257, 3840, 295, 4122, 13, 400, 370, 51000], "temperature": 0.0, "avg_logprob": -0.11595291137695313, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.0017005626577883959}, {"id": 1298, "seek": 678232, "start": 6795.04, "end": 6799.44, "text": " another feature of the people, one of them was the center embedding. And so that is like basically", "tokens": [51000, 1071, 4111, 295, 264, 561, 11, 472, 295, 552, 390, 264, 3056, 12240, 3584, 13, 400, 370, 300, 307, 411, 1936, 51220], "temperature": 0.0, "avg_logprob": -0.11595291137695313, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.0017005626577883959}, {"id": 1299, "seek": 678232, "start": 6799.44, "end": 6807.28, "text": " how often a clause would intervene between a subject and a verb, for example. That's one kind", "tokens": [51220, 577, 2049, 257, 25925, 576, 30407, 1296, 257, 3983, 293, 257, 9595, 11, 337, 1365, 13, 663, 311, 472, 733, 51612], "temperature": 0.0, "avg_logprob": -0.11595291137695313, "compression_ratio": 1.7259259259259259, "no_speech_prob": 0.0017005626577883959}, {"id": 1300, "seek": 680728, "start": 6807.28, "end": 6813.599999999999, "text": " of a center embedding of a clause. And turns out they're massively center embedded. So I think", "tokens": [50364, 295, 257, 3056, 12240, 3584, 295, 257, 25925, 13, 400, 4523, 484, 436, 434, 29379, 3056, 16741, 13, 407, 286, 519, 50680], "temperature": 0.0, "avg_logprob": -0.10529959915030716, "compression_ratio": 1.73828125, "no_speech_prob": 0.2875977158546448}, {"id": 1301, "seek": 680728, "start": 6813.599999999999, "end": 6820.24, "text": " in random contracts and in random laws, I think you get about 70% or 70% of sentences", "tokens": [50680, 294, 4974, 13952, 293, 294, 4974, 6064, 11, 286, 519, 291, 483, 466, 5285, 4, 420, 5285, 4, 295, 16579, 51012], "temperature": 0.0, "avg_logprob": -0.10529959915030716, "compression_ratio": 1.73828125, "no_speech_prob": 0.2875977158546448}, {"id": 1302, "seek": 680728, "start": 6820.24, "end": 6824.96, "text": " have a center embedded clause, which is insanely high. If you go to any other text,", "tokens": [51012, 362, 257, 3056, 16741, 25925, 11, 597, 307, 40965, 1090, 13, 759, 291, 352, 281, 604, 661, 2487, 11, 51248], "temperature": 0.0, "avg_logprob": -0.10529959915030716, "compression_ratio": 1.73828125, "no_speech_prob": 0.2875977158546448}, {"id": 1303, "seek": 680728, "start": 6824.96, "end": 6830.32, "text": " it's down to 20% or something. It's so much higher than any control you can think of,", "tokens": [51248, 309, 311, 760, 281, 945, 4, 420, 746, 13, 467, 311, 370, 709, 2946, 813, 604, 1969, 291, 393, 519, 295, 11, 51516], "temperature": 0.0, "avg_logprob": -0.10529959915030716, "compression_ratio": 1.73828125, "no_speech_prob": 0.2875977158546448}, {"id": 1304, "seek": 680728, "start": 6830.32, "end": 6835.5199999999995, "text": " including... You think, oh, people think, oh, technical, academic text, no, people don't write", "tokens": [51516, 3009, 485, 509, 519, 11, 1954, 11, 561, 519, 11, 1954, 11, 6191, 11, 7778, 2487, 11, 572, 11, 561, 500, 380, 2464, 51776], "temperature": 0.0, "avg_logprob": -0.10529959915030716, "compression_ratio": 1.73828125, "no_speech_prob": 0.2875977158546448}, {"id": 1305, "seek": 683552, "start": 6835.52, "end": 6840.0, "text": " center embedded sentences in technical, academic texts. I mean, they do a little bit, but it's", "tokens": [50364, 3056, 16741, 16579, 294, 6191, 11, 7778, 15765, 13, 286, 914, 11, 436, 360, 257, 707, 857, 11, 457, 309, 311, 50588], "temperature": 0.0, "avg_logprob": -0.13164378750708797, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.001867267768830061}, {"id": 1306, "seek": 683552, "start": 6840.0, "end": 6845.76, "text": " on the 20%, 30% realm as opposed to 70. And so there's that. And there's low frequency words.", "tokens": [50588, 322, 264, 945, 8923, 2217, 4, 15355, 382, 8851, 281, 5285, 13, 400, 370, 456, 311, 300, 13, 400, 456, 311, 2295, 7893, 2283, 13, 50876], "temperature": 0.0, "avg_logprob": -0.13164378750708797, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.001867267768830061}, {"id": 1307, "seek": 683552, "start": 6845.76, "end": 6850.080000000001, "text": " And then people, oh, maybe it's passive. People don't like the passive. Passive, for some reason,", "tokens": [50876, 400, 550, 561, 11, 1954, 11, 1310, 309, 311, 14975, 13, 3432, 500, 380, 411, 264, 14975, 13, 10319, 488, 11, 337, 512, 1778, 11, 51092], "temperature": 0.0, "avg_logprob": -0.13164378750708797, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.001867267768830061}, {"id": 1308, "seek": 683552, "start": 6850.080000000001, "end": 6854.240000000001, "text": " the passive voice in English has a bad rap. And I'm not really sure where that comes from.", "tokens": [51092, 264, 14975, 3177, 294, 3669, 575, 257, 1578, 5099, 13, 400, 286, 478, 406, 534, 988, 689, 300, 1487, 490, 13, 51300], "temperature": 0.0, "avg_logprob": -0.13164378750708797, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.001867267768830061}, {"id": 1309, "seek": 683552, "start": 6856.240000000001, "end": 6863.120000000001, "text": " And there is a lot of passive. There's much more passive voice in legalese than there is", "tokens": [51400, 400, 456, 307, 257, 688, 295, 14975, 13, 821, 311, 709, 544, 14975, 3177, 294, 5089, 1130, 813, 456, 307, 51744], "temperature": 0.0, "avg_logprob": -0.13164378750708797, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.001867267768830061}, {"id": 1310, "seek": 686312, "start": 6863.36, "end": 6865.92, "text": " in other texts. Passive voice accounts for some of the low frequency words.", "tokens": [50376, 294, 661, 15765, 13, 10319, 488, 3177, 9402, 337, 512, 295, 264, 2295, 7893, 2283, 13, 50504], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1311, "seek": 686312, "start": 6865.92, "end": 6868.16, "text": " No, no, no, no. Those are separate. Those are separate.", "tokens": [50504, 883, 11, 572, 11, 572, 11, 572, 13, 3950, 366, 4994, 13, 3950, 366, 4994, 13, 50616], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1312, "seek": 686312, "start": 6868.16, "end": 6870.96, "text": " Oh, so passive voice sucks. Low frequency word sucks.", "tokens": [50616, 876, 11, 370, 14975, 3177, 15846, 13, 17078, 7893, 1349, 15846, 13, 50756], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1313, "seek": 686312, "start": 6870.96, "end": 6872.4, "text": " Well, sucks are different. So these are different.", "tokens": [50756, 1042, 11, 15846, 366, 819, 13, 407, 613, 366, 819, 13, 50828], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1314, "seek": 686312, "start": 6872.4, "end": 6873.5199999999995, "text": " That's a judgment on passive.", "tokens": [50828, 663, 311, 257, 12216, 322, 14975, 13, 50884], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1315, "seek": 686312, "start": 6873.5199999999995, "end": 6876.16, "text": " Yeah, yeah, yeah. Drop the judgment. It's just like, these are frequent.", "tokens": [50884, 865, 11, 1338, 11, 1338, 13, 17675, 264, 12216, 13, 467, 311, 445, 411, 11, 613, 366, 18004, 13, 51016], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1316, "seek": 686312, "start": 6876.16, "end": 6881.2, "text": " These are things which happen in legalese texts. Then we can ask the dependent measure is like,", "tokens": [51016, 1981, 366, 721, 597, 1051, 294, 5089, 1130, 15765, 13, 1396, 321, 393, 1029, 264, 12334, 3481, 307, 411, 11, 51268], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1317, "seek": 686312, "start": 6881.2, "end": 6885.5199999999995, "text": " how well you understand those things with those features. Okay. And so then,", "tokens": [51268, 577, 731, 291, 1223, 729, 721, 365, 729, 4122, 13, 1033, 13, 400, 370, 550, 11, 51484], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1318, "seek": 686312, "start": 6885.5199999999995, "end": 6889.84, "text": " and it turns out the passive makes no difference. So it has a zero effect on your comprehension", "tokens": [51484, 293, 309, 4523, 484, 264, 14975, 1669, 572, 2649, 13, 407, 309, 575, 257, 4018, 1802, 322, 428, 44991, 51700], "temperature": 0.0, "avg_logprob": -0.19442004423875076, "compression_ratio": 1.876543209876543, "no_speech_prob": 0.022969795390963554}, {"id": 1319, "seek": 688984, "start": 6889.84, "end": 6893.28, "text": " ability, on your recall ability. No, nothing at all. That means no effect.", "tokens": [50364, 3485, 11, 322, 428, 9901, 3485, 13, 883, 11, 1825, 412, 439, 13, 663, 1355, 572, 1802, 13, 50536], "temperature": 0.0, "avg_logprob": -0.16352510839942994, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.04207475483417511}, {"id": 1320, "seek": 688984, "start": 6894.400000000001, "end": 6898.16, "text": " The words matter a little bit. They do low frequency words are going to hurt you in", "tokens": [50592, 440, 2283, 1871, 257, 707, 857, 13, 814, 360, 2295, 7893, 2283, 366, 516, 281, 4607, 291, 294, 50780], "temperature": 0.0, "avg_logprob": -0.16352510839942994, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.04207475483417511}, {"id": 1321, "seek": 688984, "start": 6898.16, "end": 6902.72, "text": " recall and understanding. But what really hurts is the central bedding.", "tokens": [50780, 9901, 293, 3701, 13, 583, 437, 534, 11051, 307, 264, 5777, 2901, 3584, 13, 51008], "temperature": 0.0, "avg_logprob": -0.16352510839942994, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.04207475483417511}, {"id": 1322, "seek": 688984, "start": 6902.72, "end": 6908.96, "text": " That kills you. That is like, that slows people down. That makes them very poor at understanding.", "tokens": [51008, 663, 14563, 291, 13, 663, 307, 411, 11, 300, 35789, 561, 760, 13, 663, 1669, 552, 588, 4716, 412, 3701, 13, 51320], "temperature": 0.0, "avg_logprob": -0.16352510839942994, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.04207475483417511}, {"id": 1323, "seek": 688984, "start": 6908.96, "end": 6912.8, "text": " That makes them, they can't recall what was said as well, nearly as well.", "tokens": [51320, 663, 1669, 552, 11, 436, 393, 380, 9901, 437, 390, 848, 382, 731, 11, 6217, 382, 731, 13, 51512], "temperature": 0.0, "avg_logprob": -0.16352510839942994, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.04207475483417511}, {"id": 1324, "seek": 688984, "start": 6912.8, "end": 6916.16, "text": " And we did this not only on lay people. We didn't have a lot of lay people.", "tokens": [51512, 400, 321, 630, 341, 406, 787, 322, 2360, 561, 13, 492, 994, 380, 362, 257, 688, 295, 2360, 561, 13, 51680], "temperature": 0.0, "avg_logprob": -0.16352510839942994, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.04207475483417511}, {"id": 1325, "seek": 691616, "start": 6916.16, "end": 6921.12, "text": " We ran it on a hundred lawyers. We recruited lawyers from a wide range of", "tokens": [50364, 492, 5872, 309, 322, 257, 3262, 16219, 13, 492, 33004, 16219, 490, 257, 4874, 3613, 295, 50612], "temperature": 0.0, "avg_logprob": -0.1817672864525719, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.0019875485450029373}, {"id": 1326, "seek": 691616, "start": 6923.12, "end": 6928.0, "text": " sort of different levels of law firms and stuff. And they have the same pattern.", "tokens": [50712, 1333, 295, 819, 4358, 295, 2101, 18055, 293, 1507, 13, 400, 436, 362, 264, 912, 5102, 13, 50956], "temperature": 0.0, "avg_logprob": -0.1817672864525719, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.0019875485450029373}, {"id": 1327, "seek": 691616, "start": 6928.0, "end": 6932.96, "text": " So they also like, when they did this, I did not know what happened. I thought maybe they could", "tokens": [50956, 407, 436, 611, 411, 11, 562, 436, 630, 341, 11, 286, 630, 406, 458, 437, 2011, 13, 286, 1194, 1310, 436, 727, 51204], "temperature": 0.0, "avg_logprob": -0.1817672864525719, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.0019875485450029373}, {"id": 1328, "seek": 691616, "start": 6932.96, "end": 6937.36, "text": " process, they're used to legalese. They think you can process it just as well as it was normal.", "tokens": [51204, 1399, 11, 436, 434, 1143, 281, 5089, 1130, 13, 814, 519, 291, 393, 1399, 309, 445, 382, 731, 382, 309, 390, 2710, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1817672864525719, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.0019875485450029373}, {"id": 1329, "seek": 691616, "start": 6937.36, "end": 6943.36, "text": " No, no, they're much better than lay people. So they're much, they can much better recall,", "tokens": [51424, 883, 11, 572, 11, 436, 434, 709, 1101, 813, 2360, 561, 13, 407, 436, 434, 709, 11, 436, 393, 709, 1101, 9901, 11, 51724], "temperature": 0.0, "avg_logprob": -0.1817672864525719, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.0019875485450029373}, {"id": 1330, "seek": 694336, "start": 6943.36, "end": 6948.32, "text": " much better understanding, but they have the same main effects as lay people, exactly the same.", "tokens": [50364, 709, 1101, 3701, 11, 457, 436, 362, 264, 912, 2135, 5065, 382, 2360, 561, 11, 2293, 264, 912, 13, 50612], "temperature": 0.0, "avg_logprob": -0.14435784022013345, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.008314095437526703}, {"id": 1331, "seek": 694336, "start": 6948.32, "end": 6954.0, "text": " So they also much prefer the non-center. So we constructed non-center embedded versions of", "tokens": [50612, 407, 436, 611, 709, 4382, 264, 2107, 12, 50085, 13, 407, 321, 17083, 2107, 12, 50085, 16741, 9606, 295, 50896], "temperature": 0.0, "avg_logprob": -0.14435784022013345, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.008314095437526703}, {"id": 1332, "seek": 694336, "start": 6954.0, "end": 6959.5199999999995, "text": " each of these. We constructed versions which have higher frequency words in those places.", "tokens": [50896, 1184, 295, 613, 13, 492, 17083, 9606, 597, 362, 2946, 7893, 2283, 294, 729, 3190, 13, 51172], "temperature": 0.0, "avg_logprob": -0.14435784022013345, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.008314095437526703}, {"id": 1333, "seek": 694336, "start": 6959.5199999999995, "end": 6965.5199999999995, "text": " And we did, we un-passivized, we turned them into active versions. The passive active made no", "tokens": [51172, 400, 321, 630, 11, 321, 517, 12, 9216, 592, 1602, 11, 321, 3574, 552, 666, 4967, 9606, 13, 440, 14975, 4967, 1027, 572, 51472], "temperature": 0.0, "avg_logprob": -0.14435784022013345, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.008314095437526703}, {"id": 1334, "seek": 694336, "start": 6965.5199999999995, "end": 6971.28, "text": " difference. The words made a little difference. And the un-center embedding makes big differences", "tokens": [51472, 2649, 13, 440, 2283, 1027, 257, 707, 2649, 13, 400, 264, 517, 12, 50085, 12240, 3584, 1669, 955, 7300, 51760], "temperature": 0.0, "avg_logprob": -0.14435784022013345, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.008314095437526703}, {"id": 1335, "seek": 697128, "start": 6971.28, "end": 6974.96, "text": " in all the populations. Un-center embedding. How hard is that process, by the way?", "tokens": [50364, 294, 439, 264, 12822, 13, 1156, 12, 50085, 12240, 3584, 13, 1012, 1152, 307, 300, 1399, 11, 538, 264, 636, 30, 50548], "temperature": 0.0, "avg_logprob": -0.25087217467924505, "compression_ratio": 1.9415204678362572, "no_speech_prob": 0.049564577639102936}, {"id": 1336, "seek": 697128, "start": 6974.96, "end": 6979.12, "text": " It's not very hard. So I don't question, but how hard is it to detect center embedding?", "tokens": [50548, 467, 311, 406, 588, 1152, 13, 407, 286, 500, 380, 1168, 11, 457, 577, 1152, 307, 309, 281, 5531, 3056, 12240, 3584, 30, 50756], "temperature": 0.0, "avg_logprob": -0.25087217467924505, "compression_ratio": 1.9415204678362572, "no_speech_prob": 0.049564577639102936}, {"id": 1337, "seek": 697128, "start": 6979.12, "end": 6982.0, "text": " Oh, easy, easy to detect. That's just easy to parse. You just looking at long dependencies?", "tokens": [50756, 876, 11, 1858, 11, 1858, 281, 5531, 13, 663, 311, 445, 1858, 281, 48377, 13, 509, 445, 1237, 412, 938, 36606, 30, 50900], "temperature": 0.0, "avg_logprob": -0.25087217467924505, "compression_ratio": 1.9415204678362572, "no_speech_prob": 0.049564577639102936}, {"id": 1338, "seek": 697128, "start": 6982.0, "end": 6986.8, "text": " Yeah, yeah. You can just, you can, so there's automatic parsers for English, which are pretty good.", "tokens": [50900, 865, 11, 1338, 13, 509, 393, 445, 11, 291, 393, 11, 370, 456, 311, 12509, 21156, 433, 337, 3669, 11, 597, 366, 1238, 665, 13, 51140], "temperature": 0.0, "avg_logprob": -0.25087217467924505, "compression_ratio": 1.9415204678362572, "no_speech_prob": 0.049564577639102936}, {"id": 1339, "seek": 697128, "start": 6986.8, "end": 6988.8, "text": " And they can detect center embedding. Oh, yeah. Very.", "tokens": [51140, 400, 436, 393, 5531, 3056, 12240, 3584, 13, 876, 11, 1338, 13, 4372, 13, 51240], "temperature": 0.0, "avg_logprob": -0.25087217467924505, "compression_ratio": 1.9415204678362572, "no_speech_prob": 0.049564577639102936}, {"id": 1340, "seek": 697128, "start": 6988.8, "end": 6992.24, "text": " Or, I guess, Nestle. Perfectly. Yeah, you learn pretty much.", "tokens": [51240, 1610, 11, 286, 2041, 11, 31581, 306, 13, 10246, 356, 13, 865, 11, 291, 1466, 1238, 709, 13, 51412], "temperature": 0.0, "avg_logprob": -0.25087217467924505, "compression_ratio": 1.9415204678362572, "no_speech_prob": 0.049564577639102936}, {"id": 1341, "seek": 697128, "start": 6992.24, "end": 6996.08, "text": " So you're not just looking for long dependencies. You're just literally looking for center embedding.", "tokens": [51412, 407, 291, 434, 406, 445, 1237, 337, 938, 36606, 13, 509, 434, 445, 3736, 1237, 337, 3056, 12240, 3584, 13, 51604], "temperature": 0.0, "avg_logprob": -0.25087217467924505, "compression_ratio": 1.9415204678362572, "no_speech_prob": 0.049564577639102936}, {"id": 1342, "seek": 697128, "start": 6996.08, "end": 6999.28, "text": " Yeah, we are in this case, in these cases. But long dependencies are, they're highly", "tokens": [51604, 865, 11, 321, 366, 294, 341, 1389, 11, 294, 613, 3331, 13, 583, 938, 36606, 366, 11, 436, 434, 5405, 51764], "temperature": 0.0, "avg_logprob": -0.25087217467924505, "compression_ratio": 1.9415204678362572, "no_speech_prob": 0.049564577639102936}, {"id": 1343, "seek": 699928, "start": 6999.28, "end": 7004.8, "text": " correlated to this. So like a center embedding is a big bomb you throw inside of a sentence", "tokens": [50364, 38574, 281, 341, 13, 407, 411, 257, 3056, 12240, 3584, 307, 257, 955, 7851, 291, 3507, 1854, 295, 257, 8174, 50640], "temperature": 0.0, "avg_logprob": -0.21591860182741854, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.024416500702500343}, {"id": 1344, "seek": 699928, "start": 7004.8, "end": 7009.679999999999, "text": " that just blows up the, that makes sure. Yeah. Can I read a sentence for you from these things?", "tokens": [50640, 300, 445, 18458, 493, 264, 11, 300, 1669, 988, 13, 865, 13, 1664, 286, 1401, 257, 8174, 337, 291, 490, 613, 721, 30, 50884], "temperature": 0.0, "avg_logprob": -0.21591860182741854, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.024416500702500343}, {"id": 1345, "seek": 699928, "start": 7009.679999999999, "end": 7012.88, "text": " Sure. I see. I mean, this is just like one of the things that, this is just terrible.", "tokens": [50884, 4894, 13, 286, 536, 13, 286, 914, 11, 341, 307, 445, 411, 472, 295, 264, 721, 300, 11, 341, 307, 445, 6237, 13, 51044], "temperature": 0.0, "avg_logprob": -0.21591860182741854, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.024416500702500343}, {"id": 1346, "seek": 699928, "start": 7012.88, "end": 7020.08, "text": " My eyes might glaze over in mid-sentence. No, I understand that. I mean, legally, this is hard.", "tokens": [51044, 1222, 2575, 1062, 39390, 670, 294, 2062, 12, 49315, 655, 13, 883, 11, 286, 1223, 300, 13, 286, 914, 11, 21106, 11, 341, 307, 1152, 13, 51404], "temperature": 0.0, "avg_logprob": -0.21591860182741854, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.024416500702500343}, {"id": 1347, "seek": 699928, "start": 7020.08, "end": 7024.0, "text": " This is a go, because in the event that any payment or benefit by the company,", "tokens": [51404, 639, 307, 257, 352, 11, 570, 294, 264, 2280, 300, 604, 10224, 420, 5121, 538, 264, 2237, 11, 51600], "temperature": 0.0, "avg_logprob": -0.21591860182741854, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.024416500702500343}, {"id": 1348, "seek": 699928, "start": 7024.0, "end": 7028.08, "text": " all such payments and benefits, including the payments and benefits under section 3a here of", "tokens": [51600, 439, 1270, 14348, 293, 5311, 11, 3009, 264, 14348, 293, 5311, 833, 3541, 805, 64, 510, 295, 51804], "temperature": 0.0, "avg_logprob": -0.21591860182741854, "compression_ratio": 1.7913907284768211, "no_speech_prob": 0.024416500702500343}, {"id": 1349, "seek": 702808, "start": 7028.16, "end": 7032.64, "text": " being here and after referred to as a total payments, would be subject to the excise tax,", "tokens": [50368, 885, 510, 293, 934, 10839, 281, 382, 257, 3217, 14348, 11, 576, 312, 3983, 281, 264, 1624, 908, 3366, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1422743946313858, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0013669104082509875}, {"id": 1350, "seek": 702808, "start": 7033.2, "end": 7037.84, "text": " then the cash severance payments shall be reduced. So that's something we pulled from a regular text,", "tokens": [50620, 550, 264, 6388, 2802, 719, 14348, 4393, 312, 9212, 13, 407, 300, 311, 746, 321, 7373, 490, 257, 3890, 2487, 11, 50852], "temperature": 0.0, "avg_logprob": -0.1422743946313858, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0013669104082509875}, {"id": 1351, "seek": 702808, "start": 7037.84, "end": 7039.44, "text": " from a contract. Wow.", "tokens": [50852, 490, 257, 4364, 13, 3153, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1422743946313858, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0013669104082509875}, {"id": 1352, "seek": 702808, "start": 7039.44, "end": 7043.5199999999995, "text": " And the center embedded bit there is just, for some reason, there's a definition.", "tokens": [50932, 400, 264, 3056, 16741, 857, 456, 307, 445, 11, 337, 512, 1778, 11, 456, 311, 257, 7123, 13, 51136], "temperature": 0.0, "avg_logprob": -0.1422743946313858, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0013669104082509875}, {"id": 1353, "seek": 702808, "start": 7044.08, "end": 7049.76, "text": " They throw the definition of what payments and benefits are in between the subject and the verbal.", "tokens": [51164, 814, 3507, 264, 7123, 295, 437, 14348, 293, 5311, 366, 294, 1296, 264, 3983, 293, 264, 24781, 13, 51448], "temperature": 0.0, "avg_logprob": -0.1422743946313858, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0013669104082509875}, {"id": 1354, "seek": 702808, "start": 7049.76, "end": 7051.6, "text": " Let's, how about don't do that? Yeah.", "tokens": [51448, 961, 311, 11, 577, 466, 500, 380, 360, 300, 30, 865, 13, 51540], "temperature": 0.0, "avg_logprob": -0.1422743946313858, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0013669104082509875}, {"id": 1355, "seek": 702808, "start": 7051.6, "end": 7055.92, "text": " How about put the definition somewhere else as opposed to in the middle of the sentence?", "tokens": [51540, 1012, 466, 829, 264, 7123, 4079, 1646, 382, 8851, 281, 294, 264, 2808, 295, 264, 8174, 30, 51756], "temperature": 0.0, "avg_logprob": -0.1422743946313858, "compression_ratio": 1.7366666666666666, "no_speech_prob": 0.0013669104082509875}, {"id": 1356, "seek": 705592, "start": 7055.92, "end": 7061.28, "text": " And so that's very, very common, by the way. That's what happens. You just throw your definitions,", "tokens": [50364, 400, 370, 300, 311, 588, 11, 588, 2689, 11, 538, 264, 636, 13, 663, 311, 437, 2314, 13, 509, 445, 3507, 428, 21988, 11, 50632], "temperature": 0.0, "avg_logprob": -0.15347380387155632, "compression_ratio": 1.9757785467128028, "no_speech_prob": 0.002631413284689188}, {"id": 1357, "seek": 705592, "start": 7061.28, "end": 7065.84, "text": " you use a word, a couple of words, and then you define it, and then you continue the sentence.", "tokens": [50632, 291, 764, 257, 1349, 11, 257, 1916, 295, 2283, 11, 293, 550, 291, 6964, 309, 11, 293, 550, 291, 2354, 264, 8174, 13, 50860], "temperature": 0.0, "avg_logprob": -0.15347380387155632, "compression_ratio": 1.9757785467128028, "no_speech_prob": 0.002631413284689188}, {"id": 1358, "seek": 705592, "start": 7066.4, "end": 7069.6, "text": " Like, just don't write like that. And you ask, so then we asked lawyers, we thought,", "tokens": [50888, 1743, 11, 445, 500, 380, 2464, 411, 300, 13, 400, 291, 1029, 11, 370, 550, 321, 2351, 16219, 11, 321, 1194, 11, 51048], "temperature": 0.0, "avg_logprob": -0.15347380387155632, "compression_ratio": 1.9757785467128028, "no_speech_prob": 0.002631413284689188}, {"id": 1359, "seek": 705592, "start": 7069.6, "end": 7074.4, "text": " oh, maybe lawyers like this. Lawyers don't like this. They don't like this. They don't want to,", "tokens": [51048, 1954, 11, 1310, 16219, 411, 341, 13, 7744, 13917, 500, 380, 411, 341, 13, 814, 500, 380, 411, 341, 13, 814, 500, 380, 528, 281, 11, 51288], "temperature": 0.0, "avg_logprob": -0.15347380387155632, "compression_ratio": 1.9757785467128028, "no_speech_prob": 0.002631413284689188}, {"id": 1360, "seek": 705592, "start": 7074.4, "end": 7079.6, "text": " they don't want to write like this. They, we asked them to rate materials which are with the same", "tokens": [51288, 436, 500, 380, 528, 281, 2464, 411, 341, 13, 814, 11, 321, 2351, 552, 281, 3314, 5319, 597, 366, 365, 264, 912, 51548], "temperature": 0.0, "avg_logprob": -0.15347380387155632, "compression_ratio": 1.9757785467128028, "no_speech_prob": 0.002631413284689188}, {"id": 1361, "seek": 705592, "start": 7079.6, "end": 7085.28, "text": " meaning with un-centered bed and center bed, and they much preferred the un-centered bed versions.", "tokens": [51548, 3620, 365, 517, 12, 36814, 2901, 293, 3056, 2901, 11, 293, 436, 709, 16494, 264, 517, 12, 36814, 2901, 9606, 13, 51832], "temperature": 0.0, "avg_logprob": -0.15347380387155632, "compression_ratio": 1.9757785467128028, "no_speech_prob": 0.002631413284689188}, {"id": 1362, "seek": 708528, "start": 7085.36, "end": 7088.48, "text": " On the comprehension, on the reading side. Yeah, well, and we asked them,", "tokens": [50368, 1282, 264, 44991, 11, 322, 264, 3760, 1252, 13, 865, 11, 731, 11, 293, 321, 2351, 552, 11, 50524], "temperature": 0.0, "avg_logprob": -0.11620153797616203, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.0008295297157019377}, {"id": 1363, "seek": 708528, "start": 7088.48, "end": 7092.08, "text": " we asked them, would you hire someone who writes like this or this? We asked them all kinds of", "tokens": [50524, 321, 2351, 552, 11, 576, 291, 11158, 1580, 567, 13657, 411, 341, 420, 341, 30, 492, 2351, 552, 439, 3685, 295, 50704], "temperature": 0.0, "avg_logprob": -0.11620153797616203, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.0008295297157019377}, {"id": 1364, "seek": 708528, "start": 7092.08, "end": 7096.639999999999, "text": " questions, and they always preferred the less complicated version, all of them. So I don't", "tokens": [50704, 1651, 11, 293, 436, 1009, 16494, 264, 1570, 6179, 3037, 11, 439, 295, 552, 13, 407, 286, 500, 380, 50932], "temperature": 0.0, "avg_logprob": -0.11620153797616203, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.0008295297157019377}, {"id": 1365, "seek": 708528, "start": 7096.639999999999, "end": 7100.639999999999, "text": " even think they want it this way. Yeah, but how did it happen? How did it happen? That's a very good", "tokens": [50932, 754, 519, 436, 528, 309, 341, 636, 13, 865, 11, 457, 577, 630, 309, 1051, 30, 1012, 630, 309, 1051, 30, 663, 311, 257, 588, 665, 51132], "temperature": 0.0, "avg_logprob": -0.11620153797616203, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.0008295297157019377}, {"id": 1366, "seek": 708528, "start": 7100.639999999999, "end": 7107.36, "text": " question. And the answer is, they still don't know. But I have some theories. Well, our best", "tokens": [51132, 1168, 13, 400, 264, 1867, 307, 11, 436, 920, 500, 380, 458, 13, 583, 286, 362, 512, 13667, 13, 1042, 11, 527, 1151, 51468], "temperature": 0.0, "avg_logprob": -0.11620153797616203, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.0008295297157019377}, {"id": 1367, "seek": 708528, "start": 7107.36, "end": 7113.36, "text": " theory at the moment is that there's actually some kind of a performative meaning in the", "tokens": [51468, 5261, 412, 264, 1623, 307, 300, 456, 311, 767, 512, 733, 295, 257, 2042, 1166, 3620, 294, 264, 51768], "temperature": 0.0, "avg_logprob": -0.11620153797616203, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.0008295297157019377}, {"id": 1368, "seek": 711336, "start": 7113.36, "end": 7117.92, "text": " center embedding and the style which tells you it's legalese. We think that that's the kind", "tokens": [50364, 3056, 12240, 3584, 293, 264, 3758, 597, 5112, 291, 309, 311, 5089, 1130, 13, 492, 519, 300, 300, 311, 264, 733, 50592], "temperature": 0.0, "avg_logprob": -0.11403294224892893, "compression_ratio": 1.9929577464788732, "no_speech_prob": 0.00298072537407279}, {"id": 1369, "seek": 711336, "start": 7117.92, "end": 7123.599999999999, "text": " of a style which tells you it's legalese. Like that's a reasonable guess. And maybe it's just,", "tokens": [50592, 295, 257, 3758, 597, 5112, 291, 309, 311, 5089, 1130, 13, 1743, 300, 311, 257, 10585, 2041, 13, 400, 1310, 309, 311, 445, 11, 50876], "temperature": 0.0, "avg_logprob": -0.11403294224892893, "compression_ratio": 1.9929577464788732, "no_speech_prob": 0.00298072537407279}, {"id": 1370, "seek": 711336, "start": 7123.599999999999, "end": 7128.88, "text": " so for instance, if you're like, it's like a magic spell. So we kind of call this the magic", "tokens": [50876, 370, 337, 5197, 11, 498, 291, 434, 411, 11, 309, 311, 411, 257, 5585, 9827, 13, 407, 321, 733, 295, 818, 341, 264, 5585, 51140], "temperature": 0.0, "avg_logprob": -0.11403294224892893, "compression_ratio": 1.9929577464788732, "no_speech_prob": 0.00298072537407279}, {"id": 1371, "seek": 711336, "start": 7128.88, "end": 7133.12, "text": " spell hypothesis. So when you give them, when you tell someone to put a magic spell on someone,", "tokens": [51140, 9827, 17291, 13, 407, 562, 291, 976, 552, 11, 562, 291, 980, 1580, 281, 829, 257, 5585, 9827, 322, 1580, 11, 51352], "temperature": 0.0, "avg_logprob": -0.11403294224892893, "compression_ratio": 1.9929577464788732, "no_speech_prob": 0.00298072537407279}, {"id": 1372, "seek": 711336, "start": 7133.12, "end": 7138.48, "text": " what do you do? They, you know, people know what a magic spell is and they do a lot of rhyming.", "tokens": [51352, 437, 360, 291, 360, 30, 814, 11, 291, 458, 11, 561, 458, 437, 257, 5585, 9827, 307, 293, 436, 360, 257, 688, 295, 8740, 2810, 13, 51620], "temperature": 0.0, "avg_logprob": -0.11403294224892893, "compression_ratio": 1.9929577464788732, "no_speech_prob": 0.00298072537407279}, {"id": 1373, "seek": 711336, "start": 7138.48, "end": 7142.08, "text": " You know, that's kind of what people will tend to do. They'll do rhyming and they'll do sort of", "tokens": [51620, 509, 458, 11, 300, 311, 733, 295, 437, 561, 486, 3928, 281, 360, 13, 814, 603, 360, 8740, 2810, 293, 436, 603, 360, 1333, 295, 51800], "temperature": 0.0, "avg_logprob": -0.11403294224892893, "compression_ratio": 1.9929577464788732, "no_speech_prob": 0.00298072537407279}, {"id": 1374, "seek": 714208, "start": 7142.16, "end": 7147.2, "text": " like some kind of poetry kind of thing. Abracadabra type of thing. Yeah. And maybe that's,", "tokens": [50368, 411, 512, 733, 295, 15155, 733, 295, 551, 13, 31717, 326, 345, 455, 424, 2010, 295, 551, 13, 865, 13, 400, 1310, 300, 311, 11, 50620], "temperature": 0.0, "avg_logprob": -0.11537449534346418, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0039445203728973866}, {"id": 1375, "seek": 714208, "start": 7147.2, "end": 7153.04, "text": " there's a syntactic sort of reflex here of a magic spell which is center embedding. And so", "tokens": [50620, 456, 311, 257, 23980, 19892, 1333, 295, 23802, 510, 295, 257, 5585, 9827, 597, 307, 3056, 12240, 3584, 13, 400, 370, 50912], "temperature": 0.0, "avg_logprob": -0.11537449534346418, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0039445203728973866}, {"id": 1376, "seek": 714208, "start": 7153.04, "end": 7157.6, "text": " that's like, oh, it's trying to like tell you this is like, this is something which is true,", "tokens": [50912, 300, 311, 411, 11, 1954, 11, 309, 311, 1382, 281, 411, 980, 291, 341, 307, 411, 11, 341, 307, 746, 597, 307, 2074, 11, 51140], "temperature": 0.0, "avg_logprob": -0.11537449534346418, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0039445203728973866}, {"id": 1377, "seek": 714208, "start": 7157.6, "end": 7163.12, "text": " which is what the goal of law is, right, is telling you something that we want you to believe as", "tokens": [51140, 597, 307, 437, 264, 3387, 295, 2101, 307, 11, 558, 11, 307, 3585, 291, 746, 300, 321, 528, 291, 281, 1697, 382, 51416], "temperature": 0.0, "avg_logprob": -0.11537449534346418, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0039445203728973866}, {"id": 1378, "seek": 714208, "start": 7163.12, "end": 7167.68, "text": " certainly true, right? That's what legal contracts are trying to enforce on you, right?", "tokens": [51416, 3297, 2074, 11, 558, 30, 663, 311, 437, 5089, 13952, 366, 1382, 281, 24825, 322, 291, 11, 558, 30, 51644], "temperature": 0.0, "avg_logprob": -0.11537449534346418, "compression_ratio": 1.8214285714285714, "no_speech_prob": 0.0039445203728973866}, {"id": 1379, "seek": 716768, "start": 7167.68, "end": 7173.280000000001, "text": " And so maybe that's like a form which has, this is like an abstract, very abstract form,", "tokens": [50364, 400, 370, 1310, 300, 311, 411, 257, 1254, 597, 575, 11, 341, 307, 411, 364, 12649, 11, 588, 12649, 1254, 11, 50644], "temperature": 0.0, "avg_logprob": -0.1503301225267015, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002286692149937153}, {"id": 1380, "seek": 716768, "start": 7173.280000000001, "end": 7176.88, "text": " center embedding, which has a, has a, has a meaning associated with it. Well,", "tokens": [50644, 3056, 12240, 3584, 11, 597, 575, 257, 11, 575, 257, 11, 575, 257, 3620, 6615, 365, 309, 13, 1042, 11, 50824], "temperature": 0.0, "avg_logprob": -0.1503301225267015, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002286692149937153}, {"id": 1381, "seek": 716768, "start": 7177.6, "end": 7185.12, "text": " don't you think there's an incentive for lawyers to generate things that are hard to understand?", "tokens": [50860, 500, 380, 291, 519, 456, 311, 364, 22346, 337, 16219, 281, 8460, 721, 300, 366, 1152, 281, 1223, 30, 51236], "temperature": 0.0, "avg_logprob": -0.1503301225267015, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002286692149937153}, {"id": 1382, "seek": 716768, "start": 7185.12, "end": 7188.96, "text": " That was our, one of our working hypotheses. We just couldn't find any evidence of that.", "tokens": [51236, 663, 390, 527, 11, 472, 295, 527, 1364, 49969, 13, 492, 445, 2809, 380, 915, 604, 4467, 295, 300, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1503301225267015, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002286692149937153}, {"id": 1383, "seek": 716768, "start": 7188.96, "end": 7193.84, "text": " No, lawyers also don't understand it. But you're creating space, why you yourself,", "tokens": [51428, 883, 11, 16219, 611, 500, 380, 1223, 309, 13, 583, 291, 434, 4084, 1901, 11, 983, 291, 1803, 11, 51672], "temperature": 0.0, "avg_logprob": -0.1503301225267015, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002286692149937153}, {"id": 1384, "seek": 719384, "start": 7194.8, "end": 7199.52, "text": " but I mean, you ask in a communist Soviet Union, the individual members,", "tokens": [50412, 457, 286, 914, 11, 291, 1029, 294, 257, 29347, 11348, 8133, 11, 264, 2609, 2679, 11, 50648], "temperature": 0.0, "avg_logprob": -0.11469361720941006, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.0004653747018892318}, {"id": 1385, "seek": 719384, "start": 7200.56, "end": 7207.360000000001, "text": " their self-report is not going to correctly reflect what is broken about the gigantic bureaucracy", "tokens": [50700, 641, 2698, 12, 265, 2707, 307, 406, 516, 281, 8944, 5031, 437, 307, 5463, 466, 264, 26800, 44671, 51040], "temperature": 0.0, "avg_logprob": -0.11469361720941006, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.0004653747018892318}, {"id": 1386, "seek": 719384, "start": 7207.360000000001, "end": 7213.68, "text": " that leads to Chernobyl or something like this. I think the incentives under which you", "tokens": [51040, 300, 6689, 281, 49504, 49913, 420, 746, 411, 341, 13, 286, 519, 264, 23374, 833, 597, 291, 51356], "temperature": 0.0, "avg_logprob": -0.11469361720941006, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.0004653747018892318}, {"id": 1387, "seek": 719384, "start": 7214.400000000001, "end": 7220.08, "text": " operate are not always transparent to the members within that system. So like, it just", "tokens": [51392, 9651, 366, 406, 1009, 12737, 281, 264, 2679, 1951, 300, 1185, 13, 407, 411, 11, 309, 445, 51676], "temperature": 0.0, "avg_logprob": -0.11469361720941006, "compression_ratio": 1.5426008968609866, "no_speech_prob": 0.0004653747018892318}, {"id": 1388, "seek": 722008, "start": 7220.8, "end": 7227.12, "text": " feels like a strange coincidence that like, there is benefit if you just zoom out, look at the", "tokens": [50400, 3417, 411, 257, 5861, 22137, 300, 411, 11, 456, 307, 5121, 498, 291, 445, 8863, 484, 11, 574, 412, 264, 50716], "temperature": 0.0, "avg_logprob": -0.11400056100106454, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0017543232534080744}, {"id": 1389, "seek": 722008, "start": 7227.12, "end": 7232.88, "text": " system as opposed to asking individual lawyers that making something hard to understand is going", "tokens": [50716, 1185, 382, 8851, 281, 3365, 2609, 16219, 300, 1455, 746, 1152, 281, 1223, 307, 516, 51004], "temperature": 0.0, "avg_logprob": -0.11400056100106454, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0017543232534080744}, {"id": 1390, "seek": 722008, "start": 7232.88, "end": 7240.0, "text": " to make a lot of people money. Like there's going to, you're going to need a lawyer to figure that", "tokens": [51004, 281, 652, 257, 688, 295, 561, 1460, 13, 1743, 456, 311, 516, 281, 11, 291, 434, 516, 281, 643, 257, 11613, 281, 2573, 300, 51360], "temperature": 0.0, "avg_logprob": -0.11400056100106454, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0017543232534080744}, {"id": 1391, "seek": 722008, "start": 7240.0, "end": 7244.4, "text": " out, I guess, from the perspective of the individual, but then that could be the performative aspect.", "tokens": [51360, 484, 11, 286, 2041, 11, 490, 264, 4585, 295, 264, 2609, 11, 457, 550, 300, 727, 312, 264, 2042, 1166, 4171, 13, 51580], "temperature": 0.0, "avg_logprob": -0.11400056100106454, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0017543232534080744}, {"id": 1392, "seek": 722008, "start": 7244.4, "end": 7248.64, "text": " It could be as opposed to the incentive driven to be complicated. It could be performative", "tokens": [51580, 467, 727, 312, 382, 8851, 281, 264, 22346, 9555, 281, 312, 6179, 13, 467, 727, 312, 2042, 1166, 51792], "temperature": 0.0, "avg_logprob": -0.11400056100106454, "compression_ratio": 1.8435114503816794, "no_speech_prob": 0.0017543232534080744}, {"id": 1393, "seek": 724864, "start": 7248.64, "end": 7254.64, "text": " to where we lawyers speak in this sophisticated way and you regular humans don't understand it,", "tokens": [50364, 281, 689, 321, 16219, 1710, 294, 341, 16950, 636, 293, 291, 3890, 6255, 500, 380, 1223, 309, 11, 50664], "temperature": 0.0, "avg_logprob": -0.09345991611480713, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.00218203105032444}, {"id": 1394, "seek": 724864, "start": 7254.64, "end": 7258.160000000001, "text": " so you need to hire a lawyer. Yeah, I don't know which one it is, but it's suspicious.", "tokens": [50664, 370, 291, 643, 281, 11158, 257, 11613, 13, 865, 11, 286, 500, 380, 458, 597, 472, 309, 307, 11, 457, 309, 311, 17931, 13, 50840], "temperature": 0.0, "avg_logprob": -0.09345991611480713, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.00218203105032444}, {"id": 1395, "seek": 724864, "start": 7259.12, "end": 7264.4800000000005, "text": " Suspicious that it's hard to understand and that everybody's eyes glaze over and they don't read.", "tokens": [50888, 9545, 47067, 300, 309, 311, 1152, 281, 1223, 293, 300, 2201, 311, 2575, 39390, 670, 293, 436, 500, 380, 1401, 13, 51156], "temperature": 0.0, "avg_logprob": -0.09345991611480713, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.00218203105032444}, {"id": 1396, "seek": 724864, "start": 7264.4800000000005, "end": 7269.52, "text": " I'm suspicious as well. I'm still suspicious. And I hear what you're saying. It could be kind of a,", "tokens": [51156, 286, 478, 17931, 382, 731, 13, 286, 478, 920, 17931, 13, 400, 286, 1568, 437, 291, 434, 1566, 13, 467, 727, 312, 733, 295, 257, 11, 51408], "temperature": 0.0, "avg_logprob": -0.09345991611480713, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.00218203105032444}, {"id": 1397, "seek": 724864, "start": 7269.52, "end": 7273.6, "text": " you know, no individual and even average of individuals, it could just be a few bad apples", "tokens": [51408, 291, 458, 11, 572, 2609, 293, 754, 4274, 295, 5346, 11, 309, 727, 445, 312, 257, 1326, 1578, 16814, 51612], "temperature": 0.0, "avg_logprob": -0.09345991611480713, "compression_ratio": 1.7380073800738007, "no_speech_prob": 0.00218203105032444}, {"id": 1398, "seek": 727360, "start": 7273.6, "end": 7279.360000000001, "text": " in a way which are driving the effect in some way. Influential bad apples at the sort of,", "tokens": [50364, 294, 257, 636, 597, 366, 4840, 264, 1802, 294, 512, 636, 13, 11537, 2781, 2549, 1578, 16814, 412, 264, 1333, 295, 11, 50652], "temperature": 0.0, "avg_logprob": -0.23401354868477636, "compression_ratio": 1.594890510948905, "no_speech_prob": 0.016911404207348824}, {"id": 1399, "seek": 727360, "start": 7280.240000000001, "end": 7285.52, "text": " that everybody looks up to whatever they're like in central figures and how, you know.", "tokens": [50696, 300, 2201, 1542, 493, 281, 2035, 436, 434, 411, 294, 5777, 9624, 293, 577, 11, 291, 458, 13, 50960], "temperature": 0.0, "avg_logprob": -0.23401354868477636, "compression_ratio": 1.594890510948905, "no_speech_prob": 0.016911404207348824}, {"id": 1400, "seek": 727360, "start": 7285.52, "end": 7291.120000000001, "text": " It turns out, but it is kind of interesting that among our 100 lawyers, they did not share", "tokens": [50960, 467, 4523, 484, 11, 457, 309, 307, 733, 295, 1880, 300, 3654, 527, 2319, 16219, 11, 436, 630, 406, 2073, 51240], "temperature": 0.0, "avg_logprob": -0.23401354868477636, "compression_ratio": 1.594890510948905, "no_speech_prob": 0.016911404207348824}, {"id": 1401, "seek": 727360, "start": 7291.120000000001, "end": 7296.88, "text": " it with us. They really didn't like it. And they weren't better than regular people at", "tokens": [51240, 309, 365, 505, 13, 814, 534, 994, 380, 411, 309, 13, 400, 436, 4999, 380, 1101, 813, 3890, 561, 412, 51528], "temperature": 0.0, "avg_logprob": -0.23401354868477636, "compression_ratio": 1.594890510948905, "no_speech_prob": 0.016911404207348824}, {"id": 1402, "seek": 727360, "start": 7296.88, "end": 7301.120000000001, "text": " comprehending it or they were on average better. But they had the same difference.", "tokens": [51528, 10753, 2029, 309, 420, 436, 645, 322, 4274, 1101, 13, 583, 436, 632, 264, 912, 2649, 13, 51740], "temperature": 0.0, "avg_logprob": -0.23401354868477636, "compression_ratio": 1.594890510948905, "no_speech_prob": 0.016911404207348824}, {"id": 1403, "seek": 730112, "start": 7302.0, "end": 7309.5199999999995, "text": " Exactly, same difference. But they wanted it fixed. And so that gave us hope that", "tokens": [50408, 7587, 11, 912, 2649, 13, 583, 436, 1415, 309, 6806, 13, 400, 370, 300, 2729, 505, 1454, 300, 50784], "temperature": 0.0, "avg_logprob": -0.16053849346232865, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.002889246679842472}, {"id": 1404, "seek": 730112, "start": 7310.16, "end": 7315.76, "text": " because it actually isn't very hard to construct a material which is uncenter embedded and has", "tokens": [50816, 570, 309, 767, 1943, 380, 588, 1152, 281, 7690, 257, 2527, 597, 307, 6219, 14278, 16741, 293, 575, 51096], "temperature": 0.0, "avg_logprob": -0.16053849346232865, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.002889246679842472}, {"id": 1405, "seek": 730112, "start": 7315.76, "end": 7319.28, "text": " the same meaning, it's not very hard to do. Just basically in that situation, just putting", "tokens": [51096, 264, 912, 3620, 11, 309, 311, 406, 588, 1152, 281, 360, 13, 1449, 1936, 294, 300, 2590, 11, 445, 3372, 51272], "temperature": 0.0, "avg_logprob": -0.16053849346232865, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.002889246679842472}, {"id": 1406, "seek": 730112, "start": 7319.28, "end": 7322.96, "text": " definitions outside of the subject verb relation in that particular example, and that's kind of,", "tokens": [51272, 21988, 2380, 295, 264, 3983, 9595, 9721, 294, 300, 1729, 1365, 11, 293, 300, 311, 733, 295, 11, 51456], "temperature": 0.0, "avg_logprob": -0.16053849346232865, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.002889246679842472}, {"id": 1407, "seek": 730112, "start": 7323.5199999999995, "end": 7327.36, "text": " that's pretty general. What they're doing is just throwing stuff in there, which you didn't", "tokens": [51484, 300, 311, 1238, 2674, 13, 708, 436, 434, 884, 307, 445, 10238, 1507, 294, 456, 11, 597, 291, 994, 380, 51676], "temperature": 0.0, "avg_logprob": -0.16053849346232865, "compression_ratio": 1.6402877697841727, "no_speech_prob": 0.002889246679842472}, {"id": 1408, "seek": 732736, "start": 7327.36, "end": 7332.48, "text": " have to put in there. There's extra words involved. Typically, you may need a few extra", "tokens": [50364, 362, 281, 829, 294, 456, 13, 821, 311, 2857, 2283, 3288, 13, 23129, 11, 291, 815, 643, 257, 1326, 2857, 50620], "temperature": 0.0, "avg_logprob": -0.0905835349280555, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.01132869441062212}, {"id": 1409, "seek": 732736, "start": 7332.48, "end": 7337.599999999999, "text": " words sort of to refer to the things that you're defining outside in some way, because if you", "tokens": [50620, 2283, 1333, 295, 281, 2864, 281, 264, 721, 300, 291, 434, 17827, 2380, 294, 512, 636, 11, 570, 498, 291, 50876], "temperature": 0.0, "avg_logprob": -0.0905835349280555, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.01132869441062212}, {"id": 1410, "seek": 732736, "start": 7337.599999999999, "end": 7344.4, "text": " only use it in that one sentence, then there's no reason to introduce extra terms. So we might", "tokens": [50876, 787, 764, 309, 294, 300, 472, 8174, 11, 550, 456, 311, 572, 1778, 281, 5366, 2857, 2115, 13, 407, 321, 1062, 51216], "temperature": 0.0, "avg_logprob": -0.0905835349280555, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.01132869441062212}, {"id": 1411, "seek": 732736, "start": 7344.4, "end": 7350.16, "text": " have a few more words, but it'll be easier to understand. So I mean, I have hope that now", "tokens": [51216, 362, 257, 1326, 544, 2283, 11, 457, 309, 603, 312, 3571, 281, 1223, 13, 407, 286, 914, 11, 286, 362, 1454, 300, 586, 51504], "temperature": 0.0, "avg_logprob": -0.0905835349280555, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.01132869441062212}, {"id": 1412, "seek": 732736, "start": 7350.16, "end": 7355.2, "text": " that maybe we can make legalese less convoluted in this way.", "tokens": [51504, 300, 1310, 321, 393, 652, 5089, 1130, 1570, 3754, 2308, 292, 294, 341, 636, 13, 51756], "temperature": 0.0, "avg_logprob": -0.0905835349280555, "compression_ratio": 1.701195219123506, "no_speech_prob": 0.01132869441062212}, {"id": 1413, "seek": 735520, "start": 7355.2, "end": 7360.08, "text": " So maybe the next president in the United States can, instead of saying generic things, say,", "tokens": [50364, 407, 1310, 264, 958, 3868, 294, 264, 2824, 3040, 393, 11, 2602, 295, 1566, 19577, 721, 11, 584, 11, 50608], "temperature": 0.0, "avg_logprob": -0.21946679329385563, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001674130093306303}, {"id": 1414, "seek": 735520, "start": 7360.08, "end": 7367.599999999999, "text": " I ban center embeddings and make Ted the language czar of the U.S.", "tokens": [50608, 286, 5643, 3056, 12240, 29432, 293, 652, 14985, 264, 2856, 6472, 289, 295, 264, 624, 13, 50, 13, 50984], "temperature": 0.0, "avg_logprob": -0.21946679329385563, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001674130093306303}, {"id": 1415, "seek": 735520, "start": 7367.599999999999, "end": 7370.48, "text": " Eric Martinez is the guy you should really put in there.", "tokens": [50984, 9336, 41886, 307, 264, 2146, 291, 820, 534, 829, 294, 456, 13, 51128], "temperature": 0.0, "avg_logprob": -0.21946679329385563, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001674130093306303}, {"id": 1416, "seek": 735520, "start": 7373.2, "end": 7377.2, "text": " But center embeddings are the bad thing to have. That's right.", "tokens": [51264, 583, 3056, 12240, 29432, 366, 264, 1578, 551, 281, 362, 13, 663, 311, 558, 13, 51464], "temperature": 0.0, "avg_logprob": -0.21946679329385563, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001674130093306303}, {"id": 1417, "seek": 735520, "start": 7377.2, "end": 7380.08, "text": " So you can get rid of that. That'll do a lot of it. That'll fix a lot.", "tokens": [51464, 407, 291, 393, 483, 3973, 295, 300, 13, 663, 603, 360, 257, 688, 295, 309, 13, 663, 603, 3191, 257, 688, 13, 51608], "temperature": 0.0, "avg_logprob": -0.21946679329385563, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001674130093306303}, {"id": 1418, "seek": 738008, "start": 7380.64, "end": 7386.5599999999995, "text": " That's fascinating. That is so fascinating. And it is really fascinating on many fronts that humans", "tokens": [50392, 663, 311, 10343, 13, 663, 307, 370, 10343, 13, 400, 309, 307, 534, 10343, 322, 867, 40426, 300, 6255, 50688], "temperature": 0.0, "avg_logprob": -0.17042374161054502, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.03408600762486458}, {"id": 1419, "seek": 738008, "start": 7386.5599999999995, "end": 7390.16, "text": " are just not able to deal with this kind of thing. And that language, because of that,", "tokens": [50688, 366, 445, 406, 1075, 281, 2028, 365, 341, 733, 295, 551, 13, 400, 300, 2856, 11, 570, 295, 300, 11, 50868], "temperature": 0.0, "avg_logprob": -0.17042374161054502, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.03408600762486458}, {"id": 1420, "seek": 738008, "start": 7390.16, "end": 7395.6, "text": " evolved in the way you did. It's fascinating. So one of the mathematical formulations you have", "tokens": [50868, 14178, 294, 264, 636, 291, 630, 13, 467, 311, 10343, 13, 407, 472, 295, 264, 18894, 1254, 4136, 291, 362, 51140], "temperature": 0.0, "avg_logprob": -0.17042374161054502, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.03408600762486458}, {"id": 1421, "seek": 738008, "start": 7396.24, "end": 7400.8, "text": " when talking about languages communication is, let's say, do you have noisy channels?", "tokens": [51172, 562, 1417, 466, 8650, 6101, 307, 11, 718, 311, 584, 11, 360, 291, 362, 24518, 9235, 30, 51400], "temperature": 0.0, "avg_logprob": -0.17042374161054502, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.03408600762486458}, {"id": 1422, "seek": 738008, "start": 7402.16, "end": 7407.6, "text": " What's a noisy channel? So that's about communication. And so this is going back", "tokens": [51468, 708, 311, 257, 24518, 2269, 30, 407, 300, 311, 466, 6101, 13, 400, 370, 341, 307, 516, 646, 51740], "temperature": 0.0, "avg_logprob": -0.17042374161054502, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.03408600762486458}, {"id": 1423, "seek": 740760, "start": 7407.6, "end": 7415.76, "text": " to Shannon. So Claude Shannon was a student at MIT in the 40s. And so he wrote this very", "tokens": [50364, 281, 28974, 13, 407, 12947, 2303, 28974, 390, 257, 3107, 412, 13100, 294, 264, 3356, 82, 13, 400, 370, 415, 4114, 341, 588, 50772], "temperature": 0.0, "avg_logprob": -0.10534149923442322, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.007810723502188921}, {"id": 1424, "seek": 740760, "start": 7415.76, "end": 7422.400000000001, "text": " influential piece of work about communication theory or information theory. And he was interested", "tokens": [50772, 22215, 2522, 295, 589, 466, 6101, 5261, 420, 1589, 5261, 13, 400, 415, 390, 3102, 51104], "temperature": 0.0, "avg_logprob": -0.10534149923442322, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.007810723502188921}, {"id": 1425, "seek": 740760, "start": 7422.400000000001, "end": 7427.84, "text": " in human language, actually. He was interested in this problem of communication, of getting a", "tokens": [51104, 294, 1952, 2856, 11, 767, 13, 634, 390, 3102, 294, 341, 1154, 295, 6101, 11, 295, 1242, 257, 51376], "temperature": 0.0, "avg_logprob": -0.10534149923442322, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.007810723502188921}, {"id": 1426, "seek": 740760, "start": 7428.88, "end": 7434.72, "text": " message from my head to your head. And he was concerned or interested in", "tokens": [51428, 3636, 490, 452, 1378, 281, 428, 1378, 13, 400, 415, 390, 5922, 420, 3102, 294, 51720], "temperature": 0.0, "avg_logprob": -0.10534149923442322, "compression_ratio": 1.680952380952381, "no_speech_prob": 0.007810723502188921}, {"id": 1427, "seek": 743472, "start": 7435.68, "end": 7441.84, "text": " what was a robust way to do that. And so that assuming we both speak the same language, we", "tokens": [50412, 437, 390, 257, 13956, 636, 281, 360, 300, 13, 400, 370, 300, 11926, 321, 1293, 1710, 264, 912, 2856, 11, 321, 50720], "temperature": 0.0, "avg_logprob": -0.13113308936050258, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.0017004433320835233}, {"id": 1428, "seek": 743472, "start": 7441.84, "end": 7449.4400000000005, "text": " both already speak English, whatever the language is, we speak that. What is a way that I can say", "tokens": [50720, 1293, 1217, 1710, 3669, 11, 2035, 264, 2856, 307, 11, 321, 1710, 300, 13, 708, 307, 257, 636, 300, 286, 393, 584, 51100], "temperature": 0.0, "avg_logprob": -0.13113308936050258, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.0017004433320835233}, {"id": 1429, "seek": 743472, "start": 7449.4400000000005, "end": 7455.92, "text": " the language so that it's most likely to get the signal that I want to you? And then the problem", "tokens": [51100, 264, 2856, 370, 300, 309, 311, 881, 3700, 281, 483, 264, 6358, 300, 286, 528, 281, 291, 30, 400, 550, 264, 1154, 51424], "temperature": 0.0, "avg_logprob": -0.13113308936050258, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.0017004433320835233}, {"id": 1430, "seek": 743472, "start": 7455.92, "end": 7464.0, "text": " there in the communication is the noisy channel. There's a lot of noise in the system. I don't", "tokens": [51424, 456, 294, 264, 6101, 307, 264, 24518, 2269, 13, 821, 311, 257, 688, 295, 5658, 294, 264, 1185, 13, 286, 500, 380, 51828], "temperature": 0.0, "avg_logprob": -0.13113308936050258, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.0017004433320835233}, {"id": 1431, "seek": 746400, "start": 7464.0, "end": 7469.6, "text": " speak perfectly. I make errors. That's noise. There's background noise. You know that.", "tokens": [50364, 1710, 6239, 13, 286, 652, 13603, 13, 663, 311, 5658, 13, 821, 311, 3678, 5658, 13, 509, 458, 300, 13, 50644], "temperature": 0.0, "avg_logprob": -0.12882352308793502, "compression_ratio": 1.9656652360515021, "no_speech_prob": 0.014056626707315445}, {"id": 1432, "seek": 746400, "start": 7470.88, "end": 7474.32, "text": " Literal background noise. There is like white noise in the background or some other kind of", "tokens": [50708, 16090, 304, 3678, 5658, 13, 821, 307, 411, 2418, 5658, 294, 264, 3678, 420, 512, 661, 733, 295, 50880], "temperature": 0.0, "avg_logprob": -0.12882352308793502, "compression_ratio": 1.9656652360515021, "no_speech_prob": 0.014056626707315445}, {"id": 1433, "seek": 746400, "start": 7474.32, "end": 7479.84, "text": " noise. There's some speaking going on that you're at a party. That's background noise. You try to", "tokens": [50880, 5658, 13, 821, 311, 512, 4124, 516, 322, 300, 291, 434, 412, 257, 3595, 13, 663, 311, 3678, 5658, 13, 509, 853, 281, 51156], "temperature": 0.0, "avg_logprob": -0.12882352308793502, "compression_ratio": 1.9656652360515021, "no_speech_prob": 0.014056626707315445}, {"id": 1434, "seek": 746400, "start": 7479.84, "end": 7482.96, "text": " hear someone. It's hard to understand them because there's all this other stuff going on in the", "tokens": [51156, 1568, 1580, 13, 467, 311, 1152, 281, 1223, 552, 570, 456, 311, 439, 341, 661, 1507, 516, 322, 294, 264, 51312], "temperature": 0.0, "avg_logprob": -0.12882352308793502, "compression_ratio": 1.9656652360515021, "no_speech_prob": 0.014056626707315445}, {"id": 1435, "seek": 746400, "start": 7482.96, "end": 7490.72, "text": " background. And then there's noise on the receiver side so that you have some problem", "tokens": [51312, 3678, 13, 400, 550, 456, 311, 5658, 322, 264, 20086, 1252, 370, 300, 291, 362, 512, 1154, 51700], "temperature": 0.0, "avg_logprob": -0.12882352308793502, "compression_ratio": 1.9656652360515021, "no_speech_prob": 0.014056626707315445}, {"id": 1436, "seek": 749072, "start": 7490.8, "end": 7495.280000000001, "text": " maybe understanding me for stuff that's just internal to you in some way. So you've got some", "tokens": [50368, 1310, 3701, 385, 337, 1507, 300, 311, 445, 6920, 281, 291, 294, 512, 636, 13, 407, 291, 600, 658, 512, 50592], "temperature": 0.0, "avg_logprob": -0.11041505381746113, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.005553454626351595}, {"id": 1437, "seek": 749072, "start": 7495.280000000001, "end": 7500.72, "text": " other problems, whatever, with understanding for whatever reasons. Maybe you've had too much to", "tokens": [50592, 661, 2740, 11, 2035, 11, 365, 3701, 337, 2035, 4112, 13, 2704, 291, 600, 632, 886, 709, 281, 50864], "temperature": 0.0, "avg_logprob": -0.11041505381746113, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.005553454626351595}, {"id": 1438, "seek": 749072, "start": 7500.72, "end": 7505.2, "text": " drink. Who knows why you're not able to pay attention to the signal? So that's the noisy", "tokens": [50864, 2822, 13, 2102, 3255, 983, 291, 434, 406, 1075, 281, 1689, 3202, 281, 264, 6358, 30, 407, 300, 311, 264, 24518, 51088], "temperature": 0.0, "avg_logprob": -0.11041505381746113, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.005553454626351595}, {"id": 1439, "seek": 749072, "start": 7505.2, "end": 7509.52, "text": " channel. And so that language, if it's a communication system, we are trying to", "tokens": [51088, 2269, 13, 400, 370, 300, 2856, 11, 498, 309, 311, 257, 6101, 1185, 11, 321, 366, 1382, 281, 51304], "temperature": 0.0, "avg_logprob": -0.11041505381746113, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.005553454626351595}, {"id": 1440, "seek": 749072, "start": 7510.400000000001, "end": 7515.4400000000005, "text": " optimize in some sense the passing of the message from one side to the other. And", "tokens": [51348, 19719, 294, 512, 2020, 264, 8437, 295, 264, 3636, 490, 472, 1252, 281, 264, 661, 13, 400, 51600], "temperature": 0.0, "avg_logprob": -0.11041505381746113, "compression_ratio": 1.681992337164751, "no_speech_prob": 0.005553454626351595}, {"id": 1441, "seek": 751544, "start": 7516.24, "end": 7523.919999999999, "text": " so, I mean, one idea is that maybe aspects of word order, for example, might have optimized", "tokens": [50404, 370, 11, 286, 914, 11, 472, 1558, 307, 300, 1310, 7270, 295, 1349, 1668, 11, 337, 1365, 11, 1062, 362, 26941, 50788], "temperature": 0.0, "avg_logprob": -0.17800640856098926, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.0013668242609128356}, {"id": 1442, "seek": 751544, "start": 7523.919999999999, "end": 7529.839999999999, "text": " in some way to make language a little more easy to be passed from speaker to listener.", "tokens": [50788, 294, 512, 636, 281, 652, 2856, 257, 707, 544, 1858, 281, 312, 4678, 490, 8145, 281, 31569, 13, 51084], "temperature": 0.0, "avg_logprob": -0.17800640856098926, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.0013668242609128356}, {"id": 1443, "seek": 751544, "start": 7529.839999999999, "end": 7534.4, "text": " And so Shannon's the guy that did the stuff way back in the forties. It's very interesting.", "tokens": [51084, 400, 370, 28974, 311, 264, 2146, 300, 630, 264, 1507, 636, 646, 294, 264, 5009, 530, 13, 467, 311, 588, 1880, 13, 51312], "temperature": 0.0, "avg_logprob": -0.17800640856098926, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.0013668242609128356}, {"id": 1444, "seek": 751544, "start": 7534.4, "end": 7539.599999999999, "text": " Historically, he was interested in working in linguistics. He was in MIT. And this was his", "tokens": [51312, 25108, 984, 11, 415, 390, 3102, 294, 1364, 294, 21766, 6006, 13, 634, 390, 294, 13100, 13, 400, 341, 390, 702, 51572], "temperature": 0.0, "avg_logprob": -0.17800640856098926, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.0013668242609128356}, {"id": 1445, "seek": 751544, "start": 7539.599999999999, "end": 7544.879999999999, "text": " master's thesis of all things. It's crazy how much he did for his master's thesis. In 1948,", "tokens": [51572, 4505, 311, 22288, 295, 439, 721, 13, 467, 311, 3219, 577, 709, 415, 630, 337, 702, 4505, 311, 22288, 13, 682, 38833, 11, 51836], "temperature": 0.0, "avg_logprob": -0.17800640856098926, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.0013668242609128356}, {"id": 1446, "seek": 754488, "start": 7544.88, "end": 7549.84, "text": " I think, or 49 or something. And he wanted to keep working in language. And it just wasn't a", "tokens": [50364, 286, 519, 11, 420, 16513, 420, 746, 13, 400, 415, 1415, 281, 1066, 1364, 294, 2856, 13, 400, 309, 445, 2067, 380, 257, 50612], "temperature": 0.0, "avg_logprob": -0.13290822806478547, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.002630735980346799}, {"id": 1447, "seek": 754488, "start": 7549.84, "end": 7556.8, "text": " popular communication as a reason, a source for what language was wasn't popular at the time.", "tokens": [50612, 3743, 6101, 382, 257, 1778, 11, 257, 4009, 337, 437, 2856, 390, 2067, 380, 3743, 412, 264, 565, 13, 50960], "temperature": 0.0, "avg_logprob": -0.13290822806478547, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.002630735980346799}, {"id": 1448, "seek": 754488, "start": 7556.8, "end": 7561.52, "text": " So Chomsky was becoming, it was moving in there. And he just wasn't able to get a handle there,", "tokens": [50960, 407, 761, 4785, 4133, 390, 5617, 11, 309, 390, 2684, 294, 456, 13, 400, 415, 445, 2067, 380, 1075, 281, 483, 257, 4813, 456, 11, 51196], "temperature": 0.0, "avg_logprob": -0.13290822806478547, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.002630735980346799}, {"id": 1449, "seek": 754488, "start": 7561.52, "end": 7568.16, "text": " I think. And so he moved to Bell Haps and worked on communication from a mathematical point of", "tokens": [51196, 286, 519, 13, 400, 370, 415, 4259, 281, 11485, 389, 2382, 293, 2732, 322, 6101, 490, 257, 18894, 935, 295, 51528], "temperature": 0.0, "avg_logprob": -0.13290822806478547, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.002630735980346799}, {"id": 1450, "seek": 754488, "start": 7568.16, "end": 7573.6, "text": " view and did all kinds of amazing work. And so he's just more on the signal side versus", "tokens": [51528, 1910, 293, 630, 439, 3685, 295, 2243, 589, 13, 400, 370, 415, 311, 445, 544, 322, 264, 6358, 1252, 5717, 51800], "temperature": 0.0, "avg_logprob": -0.13290822806478547, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.002630735980346799}, {"id": 1451, "seek": 757360, "start": 7573.68, "end": 7578.4800000000005, "text": " like the language side. It would have been interesting to see if you proceed the language", "tokens": [50368, 411, 264, 2856, 1252, 13, 467, 576, 362, 668, 1880, 281, 536, 498, 291, 8991, 264, 2856, 50608], "temperature": 0.0, "avg_logprob": -0.144747438683974, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.0043977596797049046}, {"id": 1452, "seek": 757360, "start": 7578.4800000000005, "end": 7582.64, "text": " side. That's really interesting. Yeah, he was interested in that. His examples in the forties", "tokens": [50608, 1252, 13, 663, 311, 534, 1880, 13, 865, 11, 415, 390, 3102, 294, 300, 13, 2812, 5110, 294, 264, 5009, 530, 50816], "temperature": 0.0, "avg_logprob": -0.144747438683974, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.0043977596797049046}, {"id": 1453, "seek": 757360, "start": 7582.64, "end": 7590.0, "text": " are kind of like, they're very language like things. We can kind of show that there's a", "tokens": [50816, 366, 733, 295, 411, 11, 436, 434, 588, 2856, 411, 721, 13, 492, 393, 733, 295, 855, 300, 456, 311, 257, 51184], "temperature": 0.0, "avg_logprob": -0.144747438683974, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.0043977596797049046}, {"id": 1454, "seek": 757360, "start": 7590.0, "end": 7595.68, "text": " noisy channel process going on in when you're listening to me, you can often sort of guess", "tokens": [51184, 24518, 2269, 1399, 516, 322, 294, 562, 291, 434, 4764, 281, 385, 11, 291, 393, 2049, 1333, 295, 2041, 51468], "temperature": 0.0, "avg_logprob": -0.144747438683974, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.0043977596797049046}, {"id": 1455, "seek": 757360, "start": 7595.68, "end": 7602.64, "text": " what I meant by what you think I meant given what I said. And I mean, with respect to sort of", "tokens": [51468, 437, 286, 4140, 538, 437, 291, 519, 286, 4140, 2212, 437, 286, 848, 13, 400, 286, 914, 11, 365, 3104, 281, 1333, 295, 51816], "temperature": 0.0, "avg_logprob": -0.144747438683974, "compression_ratio": 1.7743190661478598, "no_speech_prob": 0.0043977596797049046}, {"id": 1456, "seek": 760264, "start": 7602.64, "end": 7606.8, "text": " why language looks the way it does, we might, there might be sort of, as I alluded to, there", "tokens": [50364, 983, 2856, 1542, 264, 636, 309, 775, 11, 321, 1062, 11, 456, 1062, 312, 1333, 295, 11, 382, 286, 33919, 281, 11, 456, 50572], "temperature": 0.0, "avg_logprob": -0.12070423014023725, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0005032469634898007}, {"id": 1457, "seek": 760264, "start": 7606.8, "end": 7612.64, "text": " might be ways in which word order is somewhat optimized for, because of the noisy channel in", "tokens": [50572, 1062, 312, 2098, 294, 597, 1349, 1668, 307, 8344, 26941, 337, 11, 570, 295, 264, 24518, 2269, 294, 50864], "temperature": 0.0, "avg_logprob": -0.12070423014023725, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0005032469634898007}, {"id": 1458, "seek": 760264, "start": 7612.64, "end": 7617.84, "text": " some way. I mean, that's really cool to sort of model if you don't hear certain parts of a sentence", "tokens": [50864, 512, 636, 13, 286, 914, 11, 300, 311, 534, 1627, 281, 1333, 295, 2316, 498, 291, 500, 380, 1568, 1629, 3166, 295, 257, 8174, 51124], "temperature": 0.0, "avg_logprob": -0.12070423014023725, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0005032469634898007}, {"id": 1459, "seek": 760264, "start": 7618.56, "end": 7622.4800000000005, "text": " or have some probability of missing that part, like how do you construct a language that's", "tokens": [51160, 420, 362, 512, 8482, 295, 5361, 300, 644, 11, 411, 577, 360, 291, 7690, 257, 2856, 300, 311, 51356], "temperature": 0.0, "avg_logprob": -0.12070423014023725, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0005032469634898007}, {"id": 1460, "seek": 760264, "start": 7622.4800000000005, "end": 7626.96, "text": " resilient to that? That's somewhat robust to that. Yeah, that's the idea. And then you're kind of", "tokens": [51356, 23699, 281, 300, 30, 663, 311, 8344, 13956, 281, 300, 13, 865, 11, 300, 311, 264, 1558, 13, 400, 550, 291, 434, 733, 295, 51580], "temperature": 0.0, "avg_logprob": -0.12070423014023725, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0005032469634898007}, {"id": 1461, "seek": 760264, "start": 7626.96, "end": 7632.56, "text": " saying like the word order and the syntax of language, the dependency length are all", "tokens": [51580, 1566, 411, 264, 1349, 1668, 293, 264, 28431, 295, 2856, 11, 264, 33621, 4641, 366, 439, 51860], "temperature": 0.0, "avg_logprob": -0.12070423014023725, "compression_ratio": 1.7859424920127795, "no_speech_prob": 0.0005032469634898007}, {"id": 1462, "seek": 763264, "start": 7633.12, "end": 7637.84, "text": " helpful. Yeah, well, the dependency length is really about memory, right? I think that's like", "tokens": [50388, 4961, 13, 865, 11, 731, 11, 264, 33621, 4641, 307, 534, 466, 4675, 11, 558, 30, 286, 519, 300, 311, 411, 50624], "temperature": 0.0, "avg_logprob": -0.16294269719399695, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005527174798771739}, {"id": 1463, "seek": 763264, "start": 7637.84, "end": 7642.88, "text": " about sort of what's easier or harder to produce in some way. And these other ideas are about sort", "tokens": [50624, 466, 1333, 295, 437, 311, 3571, 420, 6081, 281, 5258, 294, 512, 636, 13, 400, 613, 661, 3487, 366, 466, 1333, 50876], "temperature": 0.0, "avg_logprob": -0.16294269719399695, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005527174798771739}, {"id": 1464, "seek": 763264, "start": 7642.88, "end": 7648.96, "text": " of robustness to communication. So the problem of potential loss of loss of signal due to noise.", "tokens": [50876, 295, 13956, 1287, 281, 6101, 13, 407, 264, 1154, 295, 3995, 4470, 295, 4470, 295, 6358, 3462, 281, 5658, 13, 51180], "temperature": 0.0, "avg_logprob": -0.16294269719399695, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005527174798771739}, {"id": 1465, "seek": 763264, "start": 7648.96, "end": 7654.08, "text": " It's so that there may be aspects of word order, which is somewhat optimized for that. And, you", "tokens": [51180, 467, 311, 370, 300, 456, 815, 312, 7270, 295, 1349, 1668, 11, 597, 307, 8344, 26941, 337, 300, 13, 400, 11, 291, 51436], "temperature": 0.0, "avg_logprob": -0.16294269719399695, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005527174798771739}, {"id": 1466, "seek": 763264, "start": 7654.08, "end": 7658.64, "text": " know, we have this one guess in that, and these are kind of just so stories. I have to be, you", "tokens": [51436, 458, 11, 321, 362, 341, 472, 2041, 294, 300, 11, 293, 613, 366, 733, 295, 445, 370, 3676, 13, 286, 362, 281, 312, 11, 291, 51664], "temperature": 0.0, "avg_logprob": -0.16294269719399695, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0005527174798771739}, {"id": 1467, "seek": 765864, "start": 7658.64, "end": 7662.72, "text": " know, pretty frank, they're not like, I can't show this is true. All we can do is like look at the", "tokens": [50364, 458, 11, 1238, 10455, 11, 436, 434, 406, 411, 11, 286, 393, 380, 855, 341, 307, 2074, 13, 1057, 321, 393, 360, 307, 411, 574, 412, 264, 50568], "temperature": 0.0, "avg_logprob": -0.11782544129973005, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.025174999609589577}, {"id": 1468, "seek": 765864, "start": 7662.72, "end": 7667.04, "text": " current languages of the world. This is like, we can't sort of see how languages change or anything", "tokens": [50568, 2190, 8650, 295, 264, 1002, 13, 639, 307, 411, 11, 321, 393, 380, 1333, 295, 536, 577, 8650, 1319, 420, 1340, 50784], "temperature": 0.0, "avg_logprob": -0.11782544129973005, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.025174999609589577}, {"id": 1469, "seek": 765864, "start": 7667.04, "end": 7671.52, "text": " because we've got these snapshots of a few, you know, hundred or a few thousand languages.", "tokens": [50784, 570, 321, 600, 658, 613, 19206, 27495, 295, 257, 1326, 11, 291, 458, 11, 3262, 420, 257, 1326, 4714, 8650, 13, 51008], "temperature": 0.0, "avg_logprob": -0.11782544129973005, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.025174999609589577}, {"id": 1470, "seek": 765864, "start": 7671.52, "end": 7677.6, "text": " We don't really, we can't do the right kinds of modifications to test these things experimentally.", "tokens": [51008, 492, 500, 380, 534, 11, 321, 393, 380, 360, 264, 558, 3685, 295, 26881, 281, 1500, 613, 721, 5120, 379, 13, 51312], "temperature": 0.0, "avg_logprob": -0.11782544129973005, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.025174999609589577}, {"id": 1471, "seek": 765864, "start": 7677.6, "end": 7682.320000000001, "text": " And so, you know, so just take this with a grain of salt, okay, from here, this stuff. The dependency", "tokens": [51312, 400, 370, 11, 291, 458, 11, 370, 445, 747, 341, 365, 257, 12837, 295, 5139, 11, 1392, 11, 490, 510, 11, 341, 1507, 13, 440, 33621, 51548], "temperature": 0.0, "avg_logprob": -0.11782544129973005, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.025174999609589577}, {"id": 1472, "seek": 765864, "start": 7682.320000000001, "end": 7687.200000000001, "text": " stuff I can, I'm much more solid on. I'm like, here's what the lengths are, and here's what's", "tokens": [51548, 1507, 286, 393, 11, 286, 478, 709, 544, 5100, 322, 13, 286, 478, 411, 11, 510, 311, 437, 264, 26329, 366, 11, 293, 510, 311, 437, 311, 51792], "temperature": 0.0, "avg_logprob": -0.11782544129973005, "compression_ratio": 1.8024691358024691, "no_speech_prob": 0.025174999609589577}, {"id": 1473, "seek": 768720, "start": 7687.2, "end": 7690.88, "text": " hard. Here's what's easy. And this is a reasonable structure. I think I'm pretty reasonable. Here's", "tokens": [50364, 1152, 13, 1692, 311, 437, 311, 1858, 13, 400, 341, 307, 257, 10585, 3877, 13, 286, 519, 286, 478, 1238, 10585, 13, 1692, 311, 50548], "temperature": 0.0, "avg_logprob": -0.12783785383830698, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.005909095052629709}, {"id": 1474, "seek": 768720, "start": 7690.88, "end": 7696.16, "text": " like, why, you know, why does the word order look the way it does? We're now into shaky territory,", "tokens": [50548, 411, 11, 983, 11, 291, 458, 11, 983, 775, 264, 1349, 1668, 574, 264, 636, 309, 775, 30, 492, 434, 586, 666, 44785, 11360, 11, 50812], "temperature": 0.0, "avg_logprob": -0.12783785383830698, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.005909095052629709}, {"id": 1475, "seek": 768720, "start": 7696.16, "end": 7700.72, "text": " but it's kind of cool. But we're talking about, just to be clear, we're talking about maybe just", "tokens": [50812, 457, 309, 311, 733, 295, 1627, 13, 583, 321, 434, 1417, 466, 11, 445, 281, 312, 1850, 11, 321, 434, 1417, 466, 1310, 445, 51040], "temperature": 0.0, "avg_logprob": -0.12783785383830698, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.005909095052629709}, {"id": 1476, "seek": 768720, "start": 7700.72, "end": 7705.36, "text": " actually the sounds of communication. Like you and I are sitting in the bar, it's very loud,", "tokens": [51040, 767, 264, 3263, 295, 6101, 13, 1743, 291, 293, 286, 366, 3798, 294, 264, 2159, 11, 309, 311, 588, 6588, 11, 51272], "temperature": 0.0, "avg_logprob": -0.12783785383830698, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.005909095052629709}, {"id": 1477, "seek": 768720, "start": 7705.36, "end": 7713.12, "text": " and you model with a noisy channel, the loudness, the noise, and we have the signal that's coming", "tokens": [51272, 293, 291, 2316, 365, 257, 24518, 2269, 11, 264, 6588, 1287, 11, 264, 5658, 11, 293, 321, 362, 264, 6358, 300, 311, 1348, 51660], "temperature": 0.0, "avg_logprob": -0.12783785383830698, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.005909095052629709}, {"id": 1478, "seek": 771312, "start": 7713.2, "end": 7718.24, "text": " across. And you're saying word order might have something to do with optimizing that.", "tokens": [50368, 2108, 13, 400, 291, 434, 1566, 1349, 1668, 1062, 362, 746, 281, 360, 365, 40425, 300, 13, 50620], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1479, "seek": 771312, "start": 7718.24, "end": 7718.8, "text": " Yes. Yes.", "tokens": [50620, 1079, 13, 1079, 13, 50648], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1480, "seek": 771312, "start": 7718.8, "end": 7719.92, "text": " There's a presence of noise. Yes.", "tokens": [50648, 821, 311, 257, 6814, 295, 5658, 13, 1079, 13, 50704], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1481, "seek": 771312, "start": 7719.92, "end": 7720.8, "text": " Yes. Yeah.", "tokens": [50704, 1079, 13, 865, 13, 50748], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1482, "seek": 771312, "start": 7720.8, "end": 7723.76, "text": " It's really interesting. I mean, to me, it's interesting how much you can load into the", "tokens": [50748, 467, 311, 534, 1880, 13, 286, 914, 11, 281, 385, 11, 309, 311, 1880, 577, 709, 291, 393, 3677, 666, 264, 50896], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1483, "seek": 771312, "start": 7723.76, "end": 7728.72, "text": " noisy channel. Like how much can you bake in? You said like, you know, cognitive load on the", "tokens": [50896, 24518, 2269, 13, 1743, 577, 709, 393, 291, 16562, 294, 30, 509, 848, 411, 11, 291, 458, 11, 15605, 3677, 322, 264, 51144], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1484, "seek": 771312, "start": 7728.72, "end": 7729.68, "text": " receiver end.", "tokens": [51144, 20086, 917, 13, 51192], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1485, "seek": 771312, "start": 7729.68, "end": 7734.0, "text": " We think that those are, there's three, at least three different kinds of things going on there.", "tokens": [51192, 492, 519, 300, 729, 366, 11, 456, 311, 1045, 11, 412, 1935, 1045, 819, 3685, 295, 721, 516, 322, 456, 13, 51408], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1486, "seek": 771312, "start": 7734.0, "end": 7736.32, "text": " And we probably don't want to treat them all as the same.", "tokens": [51408, 400, 321, 1391, 500, 380, 528, 281, 2387, 552, 439, 382, 264, 912, 13, 51524], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1487, "seek": 771312, "start": 7736.32, "end": 7736.64, "text": " Sure.", "tokens": [51524, 4894, 13, 51540], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1488, "seek": 771312, "start": 7736.64, "end": 7740.64, "text": " And so I think that you, you know, the right model, a better model of a noisy channel would", "tokens": [51540, 400, 370, 286, 519, 300, 291, 11, 291, 458, 11, 264, 558, 2316, 11, 257, 1101, 2316, 295, 257, 24518, 2269, 576, 51740], "temperature": 0.0, "avg_logprob": -0.17668065264181126, "compression_ratio": 1.7872340425531914, "no_speech_prob": 0.07155770063400269}, {"id": 1489, "seek": 774064, "start": 7741.200000000001, "end": 7745.04, "text": " have three different sources of noise, which are background noise,", "tokens": [50392, 362, 1045, 819, 7139, 295, 5658, 11, 597, 366, 3678, 5658, 11, 50584], "temperature": 0.0, "avg_logprob": -0.22721689088003977, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.004608337767422199}, {"id": 1490, "seek": 774064, "start": 7745.76, "end": 7750.8, "text": " speaker, inherent noise, and listener, inherent noise. And those are not,", "tokens": [50620, 8145, 11, 26387, 5658, 11, 293, 31569, 11, 26387, 5658, 13, 400, 729, 366, 406, 11, 50872], "temperature": 0.0, "avg_logprob": -0.22721689088003977, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.004608337767422199}, {"id": 1491, "seek": 774064, "start": 7750.8, "end": 7751.68, "text": " those are all different things.", "tokens": [50872, 729, 366, 439, 819, 721, 13, 50916], "temperature": 0.0, "avg_logprob": -0.22721689088003977, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.004608337767422199}, {"id": 1492, "seek": 774064, "start": 7751.68, "end": 7755.92, "text": " Sure. But then underneath it, there's a million other subsets of like what?", "tokens": [50916, 4894, 13, 583, 550, 7223, 309, 11, 456, 311, 257, 2459, 661, 2090, 1385, 295, 411, 437, 30, 51128], "temperature": 0.0, "avg_logprob": -0.22721689088003977, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.004608337767422199}, {"id": 1493, "seek": 774064, "start": 7755.92, "end": 7756.72, "text": " Oh yeah. That's true.", "tokens": [51128, 876, 1338, 13, 663, 311, 2074, 13, 51168], "temperature": 0.0, "avg_logprob": -0.22721689088003977, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.004608337767422199}, {"id": 1494, "seek": 774064, "start": 7756.72, "end": 7761.76, "text": " On the receiving, I mean, I just mentioned cognitive load on both sides. Then there's like", "tokens": [51168, 1282, 264, 10040, 11, 286, 914, 11, 286, 445, 2835, 15605, 3677, 322, 1293, 4881, 13, 1396, 456, 311, 411, 51420], "temperature": 0.0, "avg_logprob": -0.22721689088003977, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.004608337767422199}, {"id": 1495, "seek": 774064, "start": 7763.360000000001, "end": 7768.0, "text": " speech impediments or just everything. World view, I mean, on the meeting,", "tokens": [51500, 6218, 22584, 8321, 420, 445, 1203, 13, 3937, 1910, 11, 286, 914, 11, 322, 264, 3440, 11, 51732], "temperature": 0.0, "avg_logprob": -0.22721689088003977, "compression_ratio": 1.7098039215686274, "no_speech_prob": 0.004608337767422199}, {"id": 1496, "seek": 776800, "start": 7768.08, "end": 7771.92, "text": " we start to creep into the meeting realm of like, we have different world views.", "tokens": [50368, 321, 722, 281, 9626, 666, 264, 3440, 15355, 295, 411, 11, 321, 362, 819, 1002, 6809, 13, 50560], "temperature": 0.0, "avg_logprob": -0.15238792951716934, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.006689741741865873}, {"id": 1497, "seek": 776800, "start": 7771.92, "end": 7774.88, "text": " Well, how about just form still though? Like just what language you know?", "tokens": [50560, 1042, 11, 577, 466, 445, 1254, 920, 1673, 30, 1743, 445, 437, 2856, 291, 458, 30, 50708], "temperature": 0.0, "avg_logprob": -0.15238792951716934, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.006689741741865873}, {"id": 1498, "seek": 776800, "start": 7774.88, "end": 7779.52, "text": " Like, so how well you know the language? And so if it's second language for you versus first", "tokens": [50708, 1743, 11, 370, 577, 731, 291, 458, 264, 2856, 30, 400, 370, 498, 309, 311, 1150, 2856, 337, 291, 5717, 700, 50940], "temperature": 0.0, "avg_logprob": -0.15238792951716934, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.006689741741865873}, {"id": 1499, "seek": 776800, "start": 7779.52, "end": 7784.24, "text": " language, and how maybe what other languages you know, these are still just form stuff.", "tokens": [50940, 2856, 11, 293, 577, 1310, 437, 661, 8650, 291, 458, 11, 613, 366, 920, 445, 1254, 1507, 13, 51176], "temperature": 0.0, "avg_logprob": -0.15238792951716934, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.006689741741865873}, {"id": 1500, "seek": 776800, "start": 7784.24, "end": 7788.88, "text": " And that's like potentially very informative. And, you know, how old you are, these things", "tokens": [51176, 400, 300, 311, 411, 7263, 588, 27759, 13, 400, 11, 291, 458, 11, 577, 1331, 291, 366, 11, 613, 721, 51408], "temperature": 0.0, "avg_logprob": -0.15238792951716934, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.006689741741865873}, {"id": 1501, "seek": 776800, "start": 7788.88, "end": 7793.68, "text": " probably matter, right? So like a child learning a language is, is a, you know, as a noisy", "tokens": [51408, 1391, 1871, 11, 558, 30, 407, 411, 257, 1440, 2539, 257, 2856, 307, 11, 307, 257, 11, 291, 458, 11, 382, 257, 24518, 51648], "temperature": 0.0, "avg_logprob": -0.15238792951716934, "compression_ratio": 1.8140350877192983, "no_speech_prob": 0.006689741741865873}, {"id": 1502, "seek": 779368, "start": 7794.240000000001, "end": 7798.56, "text": " representation of English grammar, you know, depending on how old they are.", "tokens": [50392, 10290, 295, 3669, 22317, 11, 291, 458, 11, 5413, 322, 577, 1331, 436, 366, 13, 50608], "temperature": 0.0, "avg_logprob": -0.15193412614905316, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.002934227930381894}, {"id": 1503, "seek": 779368, "start": 7799.200000000001, "end": 7802.320000000001, "text": " So maybe when they're six, they're perfectly formed. But", "tokens": [50640, 407, 1310, 562, 436, 434, 2309, 11, 436, 434, 6239, 8693, 13, 583, 50796], "temperature": 0.0, "avg_logprob": -0.15193412614905316, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.002934227930381894}, {"id": 1504, "seek": 779368, "start": 7803.200000000001, "end": 7808.320000000001, "text": " you mentioned one of the things is like a way to measure the language is learning problems.", "tokens": [50840, 291, 2835, 472, 295, 264, 721, 307, 411, 257, 636, 281, 3481, 264, 2856, 307, 2539, 2740, 13, 51096], "temperature": 0.0, "avg_logprob": -0.15193412614905316, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.002934227930381894}, {"id": 1505, "seek": 779368, "start": 7808.96, "end": 7812.88, "text": " So like, what's the correlation between everything we've been talking about and how", "tokens": [51128, 407, 411, 11, 437, 311, 264, 20009, 1296, 1203, 321, 600, 668, 1417, 466, 293, 577, 51324], "temperature": 0.0, "avg_logprob": -0.15193412614905316, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.002934227930381894}, {"id": 1506, "seek": 779368, "start": 7812.88, "end": 7821.04, "text": " easy it is to learn a language? So is like a short dependencies correlated to ability to", "tokens": [51324, 1858, 309, 307, 281, 1466, 257, 2856, 30, 407, 307, 411, 257, 2099, 36606, 38574, 281, 3485, 281, 51732], "temperature": 0.0, "avg_logprob": -0.15193412614905316, "compression_ratio": 1.6473029045643153, "no_speech_prob": 0.002934227930381894}, {"id": 1507, "seek": 782104, "start": 7821.04, "end": 7826.4, "text": " learn a language? Is there some kind of, or like the dependency grammars, there's some kind of", "tokens": [50364, 1466, 257, 2856, 30, 1119, 456, 512, 733, 295, 11, 420, 411, 264, 33621, 17570, 685, 11, 456, 311, 512, 733, 295, 50632], "temperature": 0.0, "avg_logprob": -0.14826853413227176, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.006486950442194939}, {"id": 1508, "seek": 782104, "start": 7827.44, "end": 7830.32, "text": " connection there? How easy it is to learn?", "tokens": [50684, 4984, 456, 30, 1012, 1858, 309, 307, 281, 1466, 30, 50828], "temperature": 0.0, "avg_logprob": -0.14826853413227176, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.006486950442194939}, {"id": 1509, "seek": 782104, "start": 7830.32, "end": 7834.72, "text": " Yeah, well, all the languages in the world's language, none is right now,", "tokens": [50828, 865, 11, 731, 11, 439, 264, 8650, 294, 264, 1002, 311, 2856, 11, 6022, 307, 558, 586, 11, 51048], "temperature": 0.0, "avg_logprob": -0.14826853413227176, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.006486950442194939}, {"id": 1510, "seek": 782104, "start": 7834.72, "end": 7839.12, "text": " we know is any better than any other with respect to sort of optimizing dependency lengths,", "tokens": [51048, 321, 458, 307, 604, 1101, 813, 604, 661, 365, 3104, 281, 1333, 295, 40425, 33621, 26329, 11, 51268], "temperature": 0.0, "avg_logprob": -0.14826853413227176, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.006486950442194939}, {"id": 1511, "seek": 782104, "start": 7839.12, "end": 7842.24, "text": " for example, they're all kind of do it, do it well, they all keep low.", "tokens": [51268, 337, 1365, 11, 436, 434, 439, 733, 295, 360, 309, 11, 360, 309, 731, 11, 436, 439, 1066, 2295, 13, 51424], "temperature": 0.0, "avg_logprob": -0.14826853413227176, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.006486950442194939}, {"id": 1512, "seek": 782104, "start": 7842.72, "end": 7847.6, "text": " It's so that I think of every human language is some kind of an opposite sort of an optimization", "tokens": [51448, 467, 311, 370, 300, 286, 519, 295, 633, 1952, 2856, 307, 512, 733, 295, 364, 6182, 1333, 295, 364, 19618, 51692], "temperature": 0.0, "avg_logprob": -0.14826853413227176, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.006486950442194939}, {"id": 1513, "seek": 784760, "start": 7847.6, "end": 7853.04, "text": " problem, a complex optimization problem to this communication problem. And so they've,", "tokens": [50364, 1154, 11, 257, 3997, 19618, 1154, 281, 341, 6101, 1154, 13, 400, 370, 436, 600, 11, 50636], "temperature": 0.0, "avg_logprob": -0.15420641217912948, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.02032659389078617}, {"id": 1514, "seek": 784760, "start": 7853.04, "end": 7857.120000000001, "text": " like they've solved it, you know, they're just sort of noisy solutions to this problem of", "tokens": [50636, 411, 436, 600, 13041, 309, 11, 291, 458, 11, 436, 434, 445, 1333, 295, 24518, 6547, 281, 341, 1154, 295, 50840], "temperature": 0.0, "avg_logprob": -0.15420641217912948, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.02032659389078617}, {"id": 1515, "seek": 784760, "start": 7857.120000000001, "end": 7860.08, "text": " communication. There's just so many ways you can do this.", "tokens": [50840, 6101, 13, 821, 311, 445, 370, 867, 2098, 291, 393, 360, 341, 13, 50988], "temperature": 0.0, "avg_logprob": -0.15420641217912948, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.02032659389078617}, {"id": 1516, "seek": 784760, "start": 7860.08, "end": 7863.92, "text": " So they're not optimized for learning, they're probably less for communication.", "tokens": [50988, 407, 436, 434, 406, 26941, 337, 2539, 11, 436, 434, 1391, 1570, 337, 6101, 13, 51180], "temperature": 0.0, "avg_logprob": -0.15420641217912948, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.02032659389078617}, {"id": 1517, "seek": 784760, "start": 7863.92, "end": 7866.56, "text": " And learning. So yes, one of the factors which,", "tokens": [51180, 400, 2539, 13, 407, 2086, 11, 472, 295, 264, 6771, 597, 11, 51312], "temperature": 0.0, "avg_logprob": -0.15420641217912948, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.02032659389078617}, {"id": 1518, "seek": 784760, "start": 7867.120000000001, "end": 7871.280000000001, "text": " yeah, so learning is messing this up a bit. And so, so for example,", "tokens": [51340, 1338, 11, 370, 2539, 307, 23258, 341, 493, 257, 857, 13, 400, 370, 11, 370, 337, 1365, 11, 51548], "temperature": 0.0, "avg_logprob": -0.15420641217912948, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.02032659389078617}, {"id": 1519, "seek": 784760, "start": 7871.280000000001, "end": 7875.76, "text": " if it were just about minimizing dependency lengths, and that was all that matters,", "tokens": [51548, 498, 309, 645, 445, 466, 46608, 33621, 26329, 11, 293, 300, 390, 439, 300, 7001, 11, 51772], "temperature": 0.0, "avg_logprob": -0.15420641217912948, "compression_ratio": 1.8827838827838828, "no_speech_prob": 0.02032659389078617}, {"id": 1520, "seek": 787576, "start": 7875.76, "end": 7881.52, "text": " you know, then we, you know, so then we might find grammars which didn't have regularity", "tokens": [50364, 291, 458, 11, 550, 321, 11, 291, 458, 11, 370, 550, 321, 1062, 915, 17570, 685, 597, 994, 380, 362, 3890, 507, 50652], "temperature": 0.0, "avg_logprob": -0.15164590982290416, "compression_ratio": 1.8475177304964538, "no_speech_prob": 0.0005192731041461229}, {"id": 1521, "seek": 787576, "start": 7881.52, "end": 7886.56, "text": " in their rules. But languages always have regularity in their rules. So what I mean by", "tokens": [50652, 294, 641, 4474, 13, 583, 8650, 1009, 362, 3890, 507, 294, 641, 4474, 13, 407, 437, 286, 914, 538, 50904], "temperature": 0.0, "avg_logprob": -0.15164590982290416, "compression_ratio": 1.8475177304964538, "no_speech_prob": 0.0005192731041461229}, {"id": 1522, "seek": 787576, "start": 7886.56, "end": 7891.360000000001, "text": " that is that if I wanted to say something to you in the optimal way to say it was,", "tokens": [50904, 300, 307, 300, 498, 286, 1415, 281, 584, 746, 281, 291, 294, 264, 16252, 636, 281, 584, 309, 390, 11, 51144], "temperature": 0.0, "avg_logprob": -0.15164590982290416, "compression_ratio": 1.8475177304964538, "no_speech_prob": 0.0005192731041461229}, {"id": 1523, "seek": 787576, "start": 7891.360000000001, "end": 7894.400000000001, "text": " it would really matter to me, all that mattered was keeping the dependencies", "tokens": [51144, 309, 576, 534, 1871, 281, 385, 11, 439, 300, 44282, 390, 5145, 264, 36606, 51296], "temperature": 0.0, "avg_logprob": -0.15164590982290416, "compression_ratio": 1.8475177304964538, "no_speech_prob": 0.0005192731041461229}, {"id": 1524, "seek": 787576, "start": 7895.280000000001, "end": 7900.4800000000005, "text": " as close together as possible. Then I would have a very lack set of free structure or dependency", "tokens": [51340, 382, 1998, 1214, 382, 1944, 13, 1396, 286, 576, 362, 257, 588, 5011, 992, 295, 1737, 3877, 420, 33621, 51600], "temperature": 0.0, "avg_logprob": -0.15164590982290416, "compression_ratio": 1.8475177304964538, "no_speech_prob": 0.0005192731041461229}, {"id": 1525, "seek": 787576, "start": 7900.4800000000005, "end": 7904.16, "text": " rule. It wouldn't have very many of those. I would have very little of that. And I would", "tokens": [51600, 4978, 13, 467, 2759, 380, 362, 588, 867, 295, 729, 13, 286, 576, 362, 588, 707, 295, 300, 13, 400, 286, 576, 51784], "temperature": 0.0, "avg_logprob": -0.15164590982290416, "compression_ratio": 1.8475177304964538, "no_speech_prob": 0.0005192731041461229}, {"id": 1526, "seek": 790416, "start": 7904.16, "end": 7908.24, "text": " just put the words as close to the things that refer to the things that are connected right", "tokens": [50364, 445, 829, 264, 2283, 382, 1998, 281, 264, 721, 300, 2864, 281, 264, 721, 300, 366, 4582, 558, 50568], "temperature": 0.0, "avg_logprob": -0.09312036226121642, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.005728920456022024}, {"id": 1527, "seek": 790416, "start": 7908.24, "end": 7912.5599999999995, "text": " beside each other. But we don't do that. Like there are, like there are word order rules, right?", "tokens": [50568, 15726, 1184, 661, 13, 583, 321, 500, 380, 360, 300, 13, 1743, 456, 366, 11, 411, 456, 366, 1349, 1668, 4474, 11, 558, 30, 50784], "temperature": 0.0, "avg_logprob": -0.09312036226121642, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.005728920456022024}, {"id": 1528, "seek": 790416, "start": 7912.5599999999995, "end": 7916.639999999999, "text": " So they're very, and depending on the language, they're more and less strict, right? So you speak", "tokens": [50784, 407, 436, 434, 588, 11, 293, 5413, 322, 264, 2856, 11, 436, 434, 544, 293, 1570, 10910, 11, 558, 30, 407, 291, 1710, 50988], "temperature": 0.0, "avg_logprob": -0.09312036226121642, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.005728920456022024}, {"id": 1529, "seek": 790416, "start": 7916.639999999999, "end": 7921.04, "text": " Russian, they're less strict than English. English is very rigid word order rules. We", "tokens": [50988, 7220, 11, 436, 434, 1570, 10910, 813, 3669, 13, 3669, 307, 588, 22195, 1349, 1668, 4474, 13, 492, 51208], "temperature": 0.0, "avg_logprob": -0.09312036226121642, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.005728920456022024}, {"id": 1530, "seek": 790416, "start": 7921.04, "end": 7927.28, "text": " order things in a very particular way. And so why do we do that? Like that's probably not about", "tokens": [51208, 1668, 721, 294, 257, 588, 1729, 636, 13, 400, 370, 983, 360, 321, 360, 300, 30, 1743, 300, 311, 1391, 406, 466, 51520], "temperature": 0.0, "avg_logprob": -0.09312036226121642, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.005728920456022024}, {"id": 1531, "seek": 790416, "start": 7928.16, "end": 7931.36, "text": " communication. That's probably about learning. I mean, then we're talking about learning. It's", "tokens": [51564, 6101, 13, 663, 311, 1391, 466, 2539, 13, 286, 914, 11, 550, 321, 434, 1417, 466, 2539, 13, 467, 311, 51724], "temperature": 0.0, "avg_logprob": -0.09312036226121642, "compression_ratio": 1.882943143812709, "no_speech_prob": 0.005728920456022024}, {"id": 1532, "seek": 793136, "start": 7931.44, "end": 7937.759999999999, "text": " probably easier to learn regular things, things which are very predictable and easy to, so that's", "tokens": [50368, 1391, 3571, 281, 1466, 3890, 721, 11, 721, 597, 366, 588, 27737, 293, 1858, 281, 11, 370, 300, 311, 50684], "temperature": 0.0, "avg_logprob": -0.14830737805548516, "compression_ratio": 1.7972972972972974, "no_speech_prob": 0.0030267953407019377}, {"id": 1533, "seek": 793136, "start": 7937.759999999999, "end": 7940.88, "text": " probably about learning is our guess, because that can't be about communication.", "tokens": [50684, 1391, 466, 2539, 307, 527, 2041, 11, 570, 300, 393, 380, 312, 466, 6101, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14830737805548516, "compression_ratio": 1.7972972972972974, "no_speech_prob": 0.0030267953407019377}, {"id": 1534, "seek": 793136, "start": 7940.88, "end": 7946.32, "text": " Can it be just noise? Can it be just the messiness of the development of a language?", "tokens": [50840, 1664, 309, 312, 445, 5658, 30, 1664, 309, 312, 445, 264, 2082, 1324, 295, 264, 3250, 295, 257, 2856, 30, 51112], "temperature": 0.0, "avg_logprob": -0.14830737805548516, "compression_ratio": 1.7972972972972974, "no_speech_prob": 0.0030267953407019377}, {"id": 1535, "seek": 793136, "start": 7946.32, "end": 7950.08, "text": " Well, if it were just a communication, then we should have languages which have very,", "tokens": [51112, 1042, 11, 498, 309, 645, 445, 257, 6101, 11, 550, 321, 820, 362, 8650, 597, 362, 588, 11, 51300], "temperature": 0.0, "avg_logprob": -0.14830737805548516, "compression_ratio": 1.7972972972972974, "no_speech_prob": 0.0030267953407019377}, {"id": 1536, "seek": 793136, "start": 7950.08, "end": 7954.639999999999, "text": " very free word order. And we don't have that. We have free error, but not free. Like there's", "tokens": [51300, 588, 1737, 1349, 1668, 13, 400, 321, 500, 380, 362, 300, 13, 492, 362, 1737, 6713, 11, 457, 406, 1737, 13, 1743, 456, 311, 51528], "temperature": 0.0, "avg_logprob": -0.14830737805548516, "compression_ratio": 1.7972972972972974, "no_speech_prob": 0.0030267953407019377}, {"id": 1537, "seek": 793136, "start": 7954.639999999999, "end": 7960.639999999999, "text": " always- Well, no, but what I mean by noise is like cultural, like sticky cultural things,", "tokens": [51528, 1009, 12, 1042, 11, 572, 11, 457, 437, 286, 914, 538, 5658, 307, 411, 6988, 11, 411, 14470, 6988, 721, 11, 51828], "temperature": 0.0, "avg_logprob": -0.14830737805548516, "compression_ratio": 1.7972972972972974, "no_speech_prob": 0.0030267953407019377}, {"id": 1538, "seek": 796064, "start": 7960.64, "end": 7966.96, "text": " like the way, the way you communicate, just there's a stickiness to it. That it's an imperfect,", "tokens": [50364, 411, 264, 636, 11, 264, 636, 291, 7890, 11, 445, 456, 311, 257, 2897, 1324, 281, 309, 13, 663, 309, 311, 364, 26714, 11, 50680], "temperature": 0.0, "avg_logprob": -0.16051197052001953, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.005376990884542465}, {"id": 1539, "seek": 796064, "start": 7967.52, "end": 7973.12, "text": " it's a noisy, it's stochastic. The function over which you're optimizing is very noisy.", "tokens": [50708, 309, 311, 257, 24518, 11, 309, 311, 342, 8997, 2750, 13, 440, 2445, 670, 597, 291, 434, 40425, 307, 588, 24518, 13, 50988], "temperature": 0.0, "avg_logprob": -0.16051197052001953, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.005376990884542465}, {"id": 1540, "seek": 796064, "start": 7974.08, "end": 7980.160000000001, "text": " So, because I don't, it feels weird to say that learning is part of the objective function,", "tokens": [51036, 407, 11, 570, 286, 500, 380, 11, 309, 3417, 3657, 281, 584, 300, 2539, 307, 644, 295, 264, 10024, 2445, 11, 51340], "temperature": 0.0, "avg_logprob": -0.16051197052001953, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.005376990884542465}, {"id": 1541, "seek": 796064, "start": 7980.160000000001, "end": 7986.0, "text": " because some languages are way harder to learn than others, right? Or is that, that's not true.", "tokens": [51340, 570, 512, 8650, 366, 636, 6081, 281, 1466, 813, 2357, 11, 558, 30, 1610, 307, 300, 11, 300, 311, 406, 2074, 13, 51632], "temperature": 0.0, "avg_logprob": -0.16051197052001953, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.005376990884542465}, {"id": 1542, "seek": 796064, "start": 7986.0, "end": 7988.88, "text": " That's interesting. I mean, that's the public perception, right?", "tokens": [51632, 663, 311, 1880, 13, 286, 914, 11, 300, 311, 264, 1908, 12860, 11, 558, 30, 51776], "temperature": 0.0, "avg_logprob": -0.16051197052001953, "compression_ratio": 1.6704980842911878, "no_speech_prob": 0.005376990884542465}, {"id": 1543, "seek": 798888, "start": 7988.88, "end": 7992.64, "text": " Yes, that's true for a second language. For a second language.", "tokens": [50364, 1079, 11, 300, 311, 2074, 337, 257, 1150, 2856, 13, 1171, 257, 1150, 2856, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11628251482349958, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.004463641904294491}, {"id": 1544, "seek": 798888, "start": 7992.64, "end": 7997.04, "text": " But that depends on what you started with, right? So, it really depends on how close", "tokens": [50552, 583, 300, 5946, 322, 437, 291, 1409, 365, 11, 558, 30, 407, 11, 309, 534, 5946, 322, 577, 1998, 50772], "temperature": 0.0, "avg_logprob": -0.11628251482349958, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.004463641904294491}, {"id": 1545, "seek": 798888, "start": 7997.04, "end": 8001.52, "text": " that second language is to the first language you've got. And so, yes, it's very, very hard", "tokens": [50772, 300, 1150, 2856, 307, 281, 264, 700, 2856, 291, 600, 658, 13, 400, 370, 11, 2086, 11, 309, 311, 588, 11, 588, 1152, 50996], "temperature": 0.0, "avg_logprob": -0.11628251482349958, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.004463641904294491}, {"id": 1546, "seek": 798888, "start": 8001.52, "end": 8006.24, "text": " to learn Arabic if you've started with English, or it's harder to learn Japanese,", "tokens": [50996, 281, 1466, 19938, 498, 291, 600, 1409, 365, 3669, 11, 420, 309, 311, 6081, 281, 1466, 5433, 11, 51232], "temperature": 0.0, "avg_logprob": -0.11628251482349958, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.004463641904294491}, {"id": 1547, "seek": 798888, "start": 8006.24, "end": 8011.68, "text": " or if you've started with Chinese, I think is the worst. There's like Defense Language Institute", "tokens": [51232, 420, 498, 291, 600, 1409, 365, 4649, 11, 286, 519, 307, 264, 5855, 13, 821, 311, 411, 17410, 24445, 9446, 51504], "temperature": 0.0, "avg_logprob": -0.11628251482349958, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.004463641904294491}, {"id": 1548, "seek": 798888, "start": 8011.68, "end": 8017.84, "text": " in the United States has like a list of how hard it is to learn what language from English.", "tokens": [51504, 294, 264, 2824, 3040, 575, 411, 257, 1329, 295, 577, 1152, 309, 307, 281, 1466, 437, 2856, 490, 3669, 13, 51812], "temperature": 0.0, "avg_logprob": -0.11628251482349958, "compression_ratio": 1.8681318681318682, "no_speech_prob": 0.004463641904294491}, {"id": 1549, "seek": 801784, "start": 8017.84, "end": 8020.24, "text": " I think Chinese is the worst. But that's the second language.", "tokens": [50364, 286, 519, 4649, 307, 264, 5855, 13, 583, 300, 311, 264, 1150, 2856, 13, 50484], "temperature": 0.0, "avg_logprob": -0.16080987915512204, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.002713150577619672}, {"id": 1550, "seek": 801784, "start": 8020.24, "end": 8024.64, "text": " You're saying babies don't care? No. There's no evidence that there's anything harder,", "tokens": [50484, 509, 434, 1566, 10917, 500, 380, 1127, 30, 883, 13, 821, 311, 572, 4467, 300, 456, 311, 1340, 6081, 11, 50704], "temperature": 0.0, "avg_logprob": -0.16080987915512204, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.002713150577619672}, {"id": 1551, "seek": 801784, "start": 8024.64, "end": 8029.360000000001, "text": " easier, but any baby, any language learned, like three or four, they speak that language.", "tokens": [50704, 3571, 11, 457, 604, 3186, 11, 604, 2856, 3264, 11, 411, 1045, 420, 1451, 11, 436, 1710, 300, 2856, 13, 50940], "temperature": 0.0, "avg_logprob": -0.16080987915512204, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.002713150577619672}, {"id": 1552, "seek": 801784, "start": 8029.360000000001, "end": 8033.4400000000005, "text": " And so, there's no evidence of anything harder, easier, but any human language. They're all", "tokens": [50940, 400, 370, 11, 456, 311, 572, 4467, 295, 1340, 6081, 11, 3571, 11, 457, 604, 1952, 2856, 13, 814, 434, 439, 51144], "temperature": 0.0, "avg_logprob": -0.16080987915512204, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.002713150577619672}, {"id": 1553, "seek": 801784, "start": 8033.4400000000005, "end": 8040.72, "text": " kind of equal. To what degree is language, this is returning to Chomsky a little bit, is innate.", "tokens": [51144, 733, 295, 2681, 13, 1407, 437, 4314, 307, 2856, 11, 341, 307, 12678, 281, 761, 4785, 4133, 257, 707, 857, 11, 307, 41766, 13, 51508], "temperature": 0.0, "avg_logprob": -0.16080987915512204, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.002713150577619672}, {"id": 1554, "seek": 801784, "start": 8041.52, "end": 8045.28, "text": " You said that for Chomsky, he used the idea that language is,", "tokens": [51548, 509, 848, 300, 337, 761, 4785, 4133, 11, 415, 1143, 264, 1558, 300, 2856, 307, 11, 51736], "temperature": 0.0, "avg_logprob": -0.16080987915512204, "compression_ratio": 1.8246268656716418, "no_speech_prob": 0.002713150577619672}, {"id": 1555, "seek": 804528, "start": 8045.28, "end": 8048.719999999999, "text": " some aspects of language are in need to explain away certain things that are observed.", "tokens": [50364, 512, 7270, 295, 2856, 366, 294, 643, 281, 2903, 1314, 1629, 721, 300, 366, 13095, 13, 50536], "temperature": 0.0, "avg_logprob": -0.12899769592285157, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.0009997376473620534}, {"id": 1556, "seek": 804528, "start": 8049.679999999999, "end": 8055.2, "text": " How much are we born with language at the core of our mind, brain?", "tokens": [50584, 1012, 709, 366, 321, 4232, 365, 2856, 412, 264, 4965, 295, 527, 1575, 11, 3567, 30, 50860], "temperature": 0.0, "avg_logprob": -0.12899769592285157, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.0009997376473620534}, {"id": 1557, "seek": 804528, "start": 8056.88, "end": 8063.12, "text": " I mean, I, you know, the answer is I don't know, of course, but the, I mean, I like to,", "tokens": [50944, 286, 914, 11, 286, 11, 291, 458, 11, 264, 1867, 307, 286, 500, 380, 458, 11, 295, 1164, 11, 457, 264, 11, 286, 914, 11, 286, 411, 281, 11, 51256], "temperature": 0.0, "avg_logprob": -0.12899769592285157, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.0009997376473620534}, {"id": 1558, "seek": 804528, "start": 8063.12, "end": 8068.16, "text": " I'm an engineer at heart, I guess, and I sort of think it's fine to postulate that a lot of", "tokens": [51256, 286, 478, 364, 11403, 412, 1917, 11, 286, 2041, 11, 293, 286, 1333, 295, 519, 309, 311, 2489, 281, 2183, 5256, 300, 257, 688, 295, 51508], "temperature": 0.0, "avg_logprob": -0.12899769592285157, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.0009997376473620534}, {"id": 1559, "seek": 804528, "start": 8068.16, "end": 8073.12, "text": " it's learned. And so, I'm guessing that a lot of it's learned. So, I think the reason Chomsky", "tokens": [51508, 309, 311, 3264, 13, 400, 370, 11, 286, 478, 17939, 300, 257, 688, 295, 309, 311, 3264, 13, 407, 11, 286, 519, 264, 1778, 761, 4785, 4133, 51756], "temperature": 0.0, "avg_logprob": -0.12899769592285157, "compression_ratio": 1.7357723577235773, "no_speech_prob": 0.0009997376473620534}, {"id": 1560, "seek": 807312, "start": 8073.12, "end": 8080.96, "text": " went with innateness is because he hypothesized movement in his grammar. He was interested in", "tokens": [50364, 1437, 365, 41766, 1287, 307, 570, 415, 14276, 1602, 3963, 294, 702, 22317, 13, 634, 390, 3102, 294, 50756], "temperature": 0.0, "avg_logprob": -0.11633454776201092, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0032721508760005236}, {"id": 1561, "seek": 807312, "start": 8080.96, "end": 8085.36, "text": " grammar and movement's hard to learn. I think he's right. Movement is a hard thing to learn,", "tokens": [50756, 22317, 293, 3963, 311, 1152, 281, 1466, 13, 286, 519, 415, 311, 558, 13, 26523, 307, 257, 1152, 551, 281, 1466, 11, 50976], "temperature": 0.0, "avg_logprob": -0.11633454776201092, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0032721508760005236}, {"id": 1562, "seek": 807312, "start": 8085.36, "end": 8089.5199999999995, "text": " to learn these two things together and how they interact. And there's like a lot of ways in which", "tokens": [50976, 281, 1466, 613, 732, 721, 1214, 293, 577, 436, 4648, 13, 400, 456, 311, 411, 257, 688, 295, 2098, 294, 597, 51184], "temperature": 0.0, "avg_logprob": -0.11633454776201092, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0032721508760005236}, {"id": 1563, "seek": 807312, "start": 8089.5199999999995, "end": 8092.96, "text": " you might generate exactly the same sentences. And it's like really hard. And so, he's like,", "tokens": [51184, 291, 1062, 8460, 2293, 264, 912, 16579, 13, 400, 309, 311, 411, 534, 1152, 13, 400, 370, 11, 415, 311, 411, 11, 51356], "temperature": 0.0, "avg_logprob": -0.11633454776201092, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0032721508760005236}, {"id": 1564, "seek": 807312, "start": 8092.96, "end": 8098.48, "text": " oh, I guess it's learned. Sorry, I guess it's not learned, it's innate. And if you just throw out", "tokens": [51356, 1954, 11, 286, 2041, 309, 311, 3264, 13, 4919, 11, 286, 2041, 309, 311, 406, 3264, 11, 309, 311, 41766, 13, 400, 498, 291, 445, 3507, 484, 51632], "temperature": 0.0, "avg_logprob": -0.11633454776201092, "compression_ratio": 1.806083650190114, "no_speech_prob": 0.0032721508760005236}, {"id": 1565, "seek": 809848, "start": 8098.48, "end": 8104.32, "text": " the movement and just think about that in a different way, you know, then you get some messiness.", "tokens": [50364, 264, 3963, 293, 445, 519, 466, 300, 294, 257, 819, 636, 11, 291, 458, 11, 550, 291, 483, 512, 2082, 1324, 13, 50656], "temperature": 0.0, "avg_logprob": -0.10290592232930292, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.026751520112156868}, {"id": 1566, "seek": 809848, "start": 8104.879999999999, "end": 8110.5599999999995, "text": " But the messiness is human language, which it actually fits better. That messiness isn't a", "tokens": [50684, 583, 264, 2082, 1324, 307, 1952, 2856, 11, 597, 309, 767, 9001, 1101, 13, 663, 2082, 1324, 1943, 380, 257, 50968], "temperature": 0.0, "avg_logprob": -0.10290592232930292, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.026751520112156868}, {"id": 1567, "seek": 809848, "start": 8110.5599999999995, "end": 8120.5599999999995, "text": " problem. It's actually, it's a valuable asset of the theory. And so, I think I don't really see a", "tokens": [50968, 1154, 13, 467, 311, 767, 11, 309, 311, 257, 8263, 11999, 295, 264, 5261, 13, 400, 370, 11, 286, 519, 286, 500, 380, 534, 536, 257, 51468], "temperature": 0.0, "avg_logprob": -0.10290592232930292, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.026751520112156868}, {"id": 1568, "seek": 809848, "start": 8120.5599999999995, "end": 8125.28, "text": " reason to postulate much innate structure. And that's kind of why I think these large language", "tokens": [51468, 1778, 281, 2183, 5256, 709, 41766, 3877, 13, 400, 300, 311, 733, 295, 983, 286, 519, 613, 2416, 2856, 51704], "temperature": 0.0, "avg_logprob": -0.10290592232930292, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.026751520112156868}, {"id": 1569, "seek": 812528, "start": 8125.36, "end": 8130.8, "text": " models are learning so well, is because I think you can learn the form, the forms of human language", "tokens": [50368, 5245, 366, 2539, 370, 731, 11, 307, 570, 286, 519, 291, 393, 1466, 264, 1254, 11, 264, 6422, 295, 1952, 2856, 50640], "temperature": 0.0, "avg_logprob": -0.13478556801291072, "compression_ratio": 1.8786764705882353, "no_speech_prob": 0.026752818375825882}, {"id": 1570, "seek": 812528, "start": 8130.8, "end": 8134.4, "text": " from the input. I think that's like, it's likely to be true.", "tokens": [50640, 490, 264, 4846, 13, 286, 519, 300, 311, 411, 11, 309, 311, 3700, 281, 312, 2074, 13, 50820], "temperature": 0.0, "avg_logprob": -0.13478556801291072, "compression_ratio": 1.8786764705882353, "no_speech_prob": 0.026752818375825882}, {"id": 1571, "seek": 812528, "start": 8134.4, "end": 8137.2, "text": " So that part of the brain that lights up when you're doing all the comprehension,", "tokens": [50820, 407, 300, 644, 295, 264, 3567, 300, 5811, 493, 562, 291, 434, 884, 439, 264, 44991, 11, 50960], "temperature": 0.0, "avg_logprob": -0.13478556801291072, "compression_ratio": 1.8786764705882353, "no_speech_prob": 0.026752818375825882}, {"id": 1572, "seek": 812528, "start": 8137.2, "end": 8140.24, "text": " that could be learned. That could be just, you don't need, you don't need any.", "tokens": [50960, 300, 727, 312, 3264, 13, 663, 727, 312, 445, 11, 291, 500, 380, 643, 11, 291, 500, 380, 643, 604, 13, 51112], "temperature": 0.0, "avg_logprob": -0.13478556801291072, "compression_ratio": 1.8786764705882353, "no_speech_prob": 0.026752818375825882}, {"id": 1573, "seek": 812528, "start": 8140.24, "end": 8146.639999999999, "text": " It doesn't have to be innate. So, like lots of stuff is modular in the brain that's learned.", "tokens": [51112, 467, 1177, 380, 362, 281, 312, 41766, 13, 407, 11, 411, 3195, 295, 1507, 307, 31111, 294, 264, 3567, 300, 311, 3264, 13, 51432], "temperature": 0.0, "avg_logprob": -0.13478556801291072, "compression_ratio": 1.8786764705882353, "no_speech_prob": 0.026752818375825882}, {"id": 1574, "seek": 812528, "start": 8146.639999999999, "end": 8151.44, "text": " It doesn't have to, you know, so there's something called the visual word form area in the back.", "tokens": [51432, 467, 1177, 380, 362, 281, 11, 291, 458, 11, 370, 456, 311, 746, 1219, 264, 5056, 1349, 1254, 1859, 294, 264, 646, 13, 51672], "temperature": 0.0, "avg_logprob": -0.13478556801291072, "compression_ratio": 1.8786764705882353, "no_speech_prob": 0.026752818375825882}, {"id": 1575, "seek": 815144, "start": 8151.5199999999995, "end": 8156.639999999999, "text": " And so, it's in the back of your head, near the, you know, the visual cortex, okay? And that", "tokens": [50368, 400, 370, 11, 309, 311, 294, 264, 646, 295, 428, 1378, 11, 2651, 264, 11, 291, 458, 11, 264, 5056, 33312, 11, 1392, 30, 400, 300, 50624], "temperature": 0.0, "avg_logprob": -0.12921740490457287, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.020328663289546967}, {"id": 1576, "seek": 815144, "start": 8157.36, "end": 8162.24, "text": " is very specialized language, sorry, very specialized brain area, which does", "tokens": [50660, 307, 588, 19813, 2856, 11, 2597, 11, 588, 19813, 3567, 1859, 11, 597, 775, 50904], "temperature": 0.0, "avg_logprob": -0.12921740490457287, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.020328663289546967}, {"id": 1577, "seek": 815144, "start": 8164.08, "end": 8168.0, "text": " visual word processing if you read, if you're a reader, okay? If you don't read, you don't have it,", "tokens": [50996, 5056, 1349, 9007, 498, 291, 1401, 11, 498, 291, 434, 257, 15149, 11, 1392, 30, 759, 291, 500, 380, 1401, 11, 291, 500, 380, 362, 309, 11, 51192], "temperature": 0.0, "avg_logprob": -0.12921740490457287, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.020328663289546967}, {"id": 1578, "seek": 815144, "start": 8168.0, "end": 8172.5599999999995, "text": " okay? Guess what? You spend some time learning to read and you develop that brain area,", "tokens": [51192, 1392, 30, 17795, 437, 30, 509, 3496, 512, 565, 2539, 281, 1401, 293, 291, 1499, 300, 3567, 1859, 11, 51420], "temperature": 0.0, "avg_logprob": -0.12921740490457287, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.020328663289546967}, {"id": 1579, "seek": 815144, "start": 8172.5599999999995, "end": 8176.96, "text": " which does exactly that. And so, these, the modularization is not evidence for", "tokens": [51420, 597, 775, 2293, 300, 13, 400, 370, 11, 613, 11, 264, 31111, 2144, 307, 406, 4467, 337, 51640], "temperature": 0.0, "avg_logprob": -0.12921740490457287, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.020328663289546967}, {"id": 1580, "seek": 817696, "start": 8176.96, "end": 8181.2, "text": " innateness. So, the modularization of a language area doesn't mean we're born with it.", "tokens": [50364, 41766, 1287, 13, 407, 11, 264, 31111, 2144, 295, 257, 2856, 1859, 1177, 380, 914, 321, 434, 4232, 365, 309, 13, 50576], "temperature": 0.0, "avg_logprob": -0.11420614143897748, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.024418652057647705}, {"id": 1581, "seek": 817696, "start": 8181.2, "end": 8186.88, "text": " We could have easily learned that. We might have been born with it. We just don't know at this", "tokens": [50576, 492, 727, 362, 3612, 3264, 300, 13, 492, 1062, 362, 668, 4232, 365, 309, 13, 492, 445, 500, 380, 458, 412, 341, 50860], "temperature": 0.0, "avg_logprob": -0.11420614143897748, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.024418652057647705}, {"id": 1582, "seek": 817696, "start": 8186.88, "end": 8191.68, "text": " point. We might very well have been born with this left lateralized area. I mean, there's like", "tokens": [50860, 935, 13, 492, 1062, 588, 731, 362, 668, 4232, 365, 341, 1411, 25128, 1602, 1859, 13, 286, 914, 11, 456, 311, 411, 51100], "temperature": 0.0, "avg_logprob": -0.11420614143897748, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.024418652057647705}, {"id": 1583, "seek": 817696, "start": 8191.68, "end": 8197.44, "text": " a lot of other interesting components here, features of this kind of argument. So, some people", "tokens": [51100, 257, 688, 295, 661, 1880, 6677, 510, 11, 4122, 295, 341, 733, 295, 6770, 13, 407, 11, 512, 561, 51388], "temperature": 0.0, "avg_logprob": -0.11420614143897748, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.024418652057647705}, {"id": 1584, "seek": 817696, "start": 8198.08, "end": 8202.64, "text": " get a stroke or something goes really wrong on the left side, where the left, where the language", "tokens": [51420, 483, 257, 12403, 420, 746, 1709, 534, 2085, 322, 264, 1411, 1252, 11, 689, 264, 1411, 11, 689, 264, 2856, 51648], "temperature": 0.0, "avg_logprob": -0.11420614143897748, "compression_ratio": 1.786259541984733, "no_speech_prob": 0.024418652057647705}, {"id": 1585, "seek": 820264, "start": 8202.64, "end": 8207.439999999999, "text": " area would be. And that, and that isn't there. It's not, not available. And it develops just", "tokens": [50364, 1859, 576, 312, 13, 400, 300, 11, 293, 300, 1943, 380, 456, 13, 467, 311, 406, 11, 406, 2435, 13, 400, 309, 25453, 445, 50604], "temperature": 0.0, "avg_logprob": -0.11209831815777403, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.007344868034124374}, {"id": 1586, "seek": 820264, "start": 8207.439999999999, "end": 8212.56, "text": " fine on the right. So, it's no lie. So, it's not about the left. It goes to the left. Like,", "tokens": [50604, 2489, 322, 264, 558, 13, 407, 11, 309, 311, 572, 4544, 13, 407, 11, 309, 311, 406, 466, 264, 1411, 13, 467, 1709, 281, 264, 1411, 13, 1743, 11, 50860], "temperature": 0.0, "avg_logprob": -0.11209831815777403, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.007344868034124374}, {"id": 1587, "seek": 820264, "start": 8212.56, "end": 8217.84, "text": " this is a very interesting question. It's like, why is the, why are any of the brain areas the", "tokens": [50860, 341, 307, 257, 588, 1880, 1168, 13, 467, 311, 411, 11, 983, 307, 264, 11, 983, 366, 604, 295, 264, 3567, 3179, 264, 51124], "temperature": 0.0, "avg_logprob": -0.11209831815777403, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.007344868034124374}, {"id": 1588, "seek": 820264, "start": 8217.84, "end": 8222.8, "text": " way that they are? And how, how, how did they come to be that way? And, you know, there's these", "tokens": [51124, 636, 300, 436, 366, 30, 400, 577, 11, 577, 11, 577, 630, 436, 808, 281, 312, 300, 636, 30, 400, 11, 291, 458, 11, 456, 311, 613, 51372], "temperature": 0.0, "avg_logprob": -0.11209831815777403, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.007344868034124374}, {"id": 1589, "seek": 820264, "start": 8222.8, "end": 8227.199999999999, "text": " natural experiments, which happen where people get these, you know, strange events in their", "tokens": [51372, 3303, 12050, 11, 597, 1051, 689, 561, 483, 613, 11, 291, 458, 11, 5861, 3931, 294, 641, 51592], "temperature": 0.0, "avg_logprob": -0.11209831815777403, "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.007344868034124374}, {"id": 1590, "seek": 822720, "start": 8227.2, "end": 8232.720000000001, "text": " brains at very young ages, which wipe out sections of their brain and, and they behave", "tokens": [50364, 15442, 412, 588, 2037, 12357, 11, 597, 14082, 484, 10863, 295, 641, 3567, 293, 11, 293, 436, 15158, 50640], "temperature": 0.0, "avg_logprob": -0.09720073068948616, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.036215636879205704}, {"id": 1591, "seek": 822720, "start": 8232.720000000001, "end": 8236.560000000001, "text": " totally normally and no one knows anything was wrong. And we find out later, because they happen", "tokens": [50640, 3879, 5646, 293, 572, 472, 3255, 1340, 390, 2085, 13, 400, 321, 915, 484, 1780, 11, 570, 436, 1051, 50832], "temperature": 0.0, "avg_logprob": -0.09720073068948616, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.036215636879205704}, {"id": 1592, "seek": 822720, "start": 8236.560000000001, "end": 8240.640000000001, "text": " to be accidentally scanned for some reason. It's like, what, what happened to your left hemisphere?", "tokens": [50832, 281, 312, 15715, 45089, 337, 512, 1778, 13, 467, 311, 411, 11, 437, 11, 437, 2011, 281, 428, 1411, 38453, 30, 51036], "temperature": 0.0, "avg_logprob": -0.09720073068948616, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.036215636879205704}, {"id": 1593, "seek": 822720, "start": 8240.640000000001, "end": 8243.76, "text": " It's missing. There's not many people who have missed their whole left hemisphere, but they'll", "tokens": [51036, 467, 311, 5361, 13, 821, 311, 406, 867, 561, 567, 362, 6721, 641, 1379, 1411, 38453, 11, 457, 436, 603, 51192], "temperature": 0.0, "avg_logprob": -0.09720073068948616, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.036215636879205704}, {"id": 1594, "seek": 822720, "start": 8243.76, "end": 8247.84, "text": " be missing some other section of their left or their right. And they behave absolutely normally,", "tokens": [51192, 312, 5361, 512, 661, 3541, 295, 641, 1411, 420, 641, 558, 13, 400, 436, 15158, 3122, 5646, 11, 51396], "temperature": 0.0, "avg_logprob": -0.09720073068948616, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.036215636879205704}, {"id": 1595, "seek": 822720, "start": 8247.84, "end": 8253.44, "text": " we would never know. So, that's like a very interesting, you know, current research. You know,", "tokens": [51396, 321, 576, 1128, 458, 13, 407, 11, 300, 311, 411, 257, 588, 1880, 11, 291, 458, 11, 2190, 2132, 13, 509, 458, 11, 51676], "temperature": 0.0, "avg_logprob": -0.09720073068948616, "compression_ratio": 1.8037974683544304, "no_speech_prob": 0.036215636879205704}, {"id": 1596, "seek": 825344, "start": 8253.44, "end": 8257.92, "text": " this is another project that this person in Federico is working on. She's got all these people", "tokens": [50364, 341, 307, 1071, 1716, 300, 341, 954, 294, 45545, 2789, 307, 1364, 322, 13, 1240, 311, 658, 439, 613, 561, 50588], "temperature": 0.0, "avg_logprob": -0.11393316168534129, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0019874449353665113}, {"id": 1597, "seek": 825344, "start": 8257.92, "end": 8265.04, "text": " contacting her because she's scanned some people who have been missing sections, one person missing,", "tokens": [50588, 41482, 720, 570, 750, 311, 45089, 512, 561, 567, 362, 668, 5361, 10863, 11, 472, 954, 5361, 11, 50944], "temperature": 0.0, "avg_logprob": -0.11393316168534129, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0019874449353665113}, {"id": 1598, "seek": 825344, "start": 8265.04, "end": 8269.52, "text": " missed a section of her brain and was scanned in her lab. And she, and she happened to be a writer", "tokens": [50944, 6721, 257, 3541, 295, 720, 3567, 293, 390, 45089, 294, 720, 2715, 13, 400, 750, 11, 293, 750, 2011, 281, 312, 257, 9936, 51168], "temperature": 0.0, "avg_logprob": -0.11393316168534129, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0019874449353665113}, {"id": 1599, "seek": 825344, "start": 8269.52, "end": 8275.28, "text": " for the New York Times. And there was an article in the New York Times about, about the, just about", "tokens": [51168, 337, 264, 1873, 3609, 11366, 13, 400, 456, 390, 364, 7222, 294, 264, 1873, 3609, 11366, 466, 11, 466, 264, 11, 445, 466, 51456], "temperature": 0.0, "avg_logprob": -0.11393316168534129, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0019874449353665113}, {"id": 1600, "seek": 825344, "start": 8275.28, "end": 8280.720000000001, "text": " the scanning procedure and, and about what might be learned about by sort of the general process", "tokens": [51456, 264, 27019, 10747, 293, 11, 293, 466, 437, 1062, 312, 3264, 466, 538, 1333, 295, 264, 2674, 1399, 51728], "temperature": 0.0, "avg_logprob": -0.11393316168534129, "compression_ratio": 1.8051470588235294, "no_speech_prob": 0.0019874449353665113}, {"id": 1601, "seek": 828072, "start": 8280.72, "end": 8286.08, "text": " of MRI and language and that's her language. And, and because she's writing for the New York Times,", "tokens": [50364, 295, 32812, 293, 2856, 293, 300, 311, 720, 2856, 13, 400, 11, 293, 570, 750, 311, 3579, 337, 264, 1873, 3609, 11366, 11, 50632], "temperature": 0.0, "avg_logprob": -0.1679993013365079, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.001000417280010879}, {"id": 1602, "seek": 828072, "start": 8286.08, "end": 8291.92, "text": " then she, all these people started writing to her who also have similar, similar kinds of deficits", "tokens": [50632, 550, 750, 11, 439, 613, 561, 1409, 3579, 281, 720, 567, 611, 362, 2531, 11, 2531, 3685, 295, 49616, 50924], "temperature": 0.0, "avg_logprob": -0.1679993013365079, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.001000417280010879}, {"id": 1603, "seek": 828072, "start": 8291.92, "end": 8298.08, "text": " because they've been, you know, accidentally, you know, to scan for some reason and, and found", "tokens": [50924, 570, 436, 600, 668, 11, 291, 458, 11, 15715, 11, 291, 458, 11, 281, 11049, 337, 512, 1778, 293, 11, 293, 1352, 51232], "temperature": 0.0, "avg_logprob": -0.1679993013365079, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.001000417280010879}, {"id": 1604, "seek": 828072, "start": 8298.08, "end": 8303.119999999999, "text": " out they're missing some section. And they, they volunteer to be scanned. These are natural", "tokens": [51232, 484, 436, 434, 5361, 512, 3541, 13, 400, 436, 11, 436, 13835, 281, 312, 45089, 13, 1981, 366, 3303, 51484], "temperature": 0.0, "avg_logprob": -0.1679993013365079, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.001000417280010879}, {"id": 1605, "seek": 828072, "start": 8303.119999999999, "end": 8306.96, "text": " experiments. Natural experiments. They're kind of messy, but natural experiments kind of cool.", "tokens": [51484, 12050, 13, 20137, 12050, 13, 814, 434, 733, 295, 16191, 11, 457, 3303, 12050, 733, 295, 1627, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1679993013365079, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.001000417280010879}, {"id": 1606, "seek": 830696, "start": 8307.759999999998, "end": 8313.359999999999, "text": " She calls them interesting brains. The first few hours, days, months of human life are fascinating.", "tokens": [50404, 1240, 5498, 552, 1880, 15442, 13, 440, 700, 1326, 2496, 11, 1708, 11, 2493, 295, 1952, 993, 366, 10343, 13, 50684], "temperature": 0.0, "avg_logprob": -0.1618912752936868, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.006288454867899418}, {"id": 1607, "seek": 830696, "start": 8313.359999999999, "end": 8317.039999999999, "text": " It's like, well, inside the womb, actually, like that development,", "tokens": [50684, 467, 311, 411, 11, 731, 11, 1854, 264, 34310, 11, 767, 11, 411, 300, 3250, 11, 50868], "temperature": 0.0, "avg_logprob": -0.1618912752936868, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.006288454867899418}, {"id": 1608, "seek": 830696, "start": 8319.439999999999, "end": 8325.039999999999, "text": " that machinery, whatever that is, seems to create powerful humans that are able to speak,", "tokens": [50988, 300, 27302, 11, 2035, 300, 307, 11, 2544, 281, 1884, 4005, 6255, 300, 366, 1075, 281, 1710, 11, 51268], "temperature": 0.0, "avg_logprob": -0.1618912752936868, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.006288454867899418}, {"id": 1609, "seek": 830696, "start": 8325.039999999999, "end": 8329.679999999998, "text": " comprehend, think, all that kind of stuff, no matter what happened, not no matter what, but", "tokens": [51268, 38183, 11, 519, 11, 439, 300, 733, 295, 1507, 11, 572, 1871, 437, 2011, 11, 406, 572, 1871, 437, 11, 457, 51500], "temperature": 0.0, "avg_logprob": -0.1618912752936868, "compression_ratio": 1.5963302752293578, "no_speech_prob": 0.006288454867899418}, {"id": 1610, "seek": 832968, "start": 8329.76, "end": 8336.960000000001, "text": " robust to the different ways that the brain might be damaged and so on. That's really,", "tokens": [50368, 13956, 281, 264, 819, 2098, 300, 264, 3567, 1062, 312, 14080, 293, 370, 322, 13, 663, 311, 534, 11, 50728], "temperature": 0.0, "avg_logprob": -0.12515209118525186, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.041436780244112015}, {"id": 1611, "seek": 832968, "start": 8336.960000000001, "end": 8342.800000000001, "text": " that's really interesting. But what would Chomsky say about the fact, the thing you're saying now,", "tokens": [50728, 300, 311, 534, 1880, 13, 583, 437, 576, 761, 4785, 4133, 584, 466, 264, 1186, 11, 264, 551, 291, 434, 1566, 586, 11, 51020], "temperature": 0.0, "avg_logprob": -0.12515209118525186, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.041436780244112015}, {"id": 1612, "seek": 832968, "start": 8342.800000000001, "end": 8349.76, "text": " that language is, seems to be happening separate from thought? Because as far as I understand,", "tokens": [51020, 300, 2856, 307, 11, 2544, 281, 312, 2737, 4994, 490, 1194, 30, 1436, 382, 1400, 382, 286, 1223, 11, 51368], "temperature": 0.0, "avg_logprob": -0.12515209118525186, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.041436780244112015}, {"id": 1613, "seek": 832968, "start": 8349.76, "end": 8354.32, "text": " maybe you can correct me, he thought that language underpins. Yeah, he thinks so. I don't know what", "tokens": [51368, 1310, 291, 393, 3006, 385, 11, 415, 1194, 300, 2856, 833, 79, 1292, 13, 865, 11, 415, 7309, 370, 13, 286, 500, 380, 458, 437, 51596], "temperature": 0.0, "avg_logprob": -0.12515209118525186, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.041436780244112015}, {"id": 1614, "seek": 835432, "start": 8354.32, "end": 8360.32, "text": " he'd say. He would be surprised because for him, the idea is that language is a sort of the foundation", "tokens": [50364, 415, 1116, 584, 13, 634, 576, 312, 6100, 570, 337, 796, 11, 264, 1558, 307, 300, 2856, 307, 257, 1333, 295, 264, 7030, 50664], "temperature": 0.0, "avg_logprob": -0.08968145271827435, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.006096596829593182}, {"id": 1615, "seek": 835432, "start": 8360.32, "end": 8367.44, "text": " of thought. That's right. Absolutely. And it's pretty mind blowing to think that it could be", "tokens": [50664, 295, 1194, 13, 663, 311, 558, 13, 7021, 13, 400, 309, 311, 1238, 1575, 15068, 281, 519, 300, 309, 727, 312, 51020], "temperature": 0.0, "avg_logprob": -0.08968145271827435, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.006096596829593182}, {"id": 1616, "seek": 835432, "start": 8367.44, "end": 8372.48, "text": " completely separate from thought. That's right. But so, you know, he's basically a philosopher,", "tokens": [51020, 2584, 4994, 490, 1194, 13, 663, 311, 558, 13, 583, 370, 11, 291, 458, 11, 415, 311, 1936, 257, 29805, 11, 51272], "temperature": 0.0, "avg_logprob": -0.08968145271827435, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.006096596829593182}, {"id": 1617, "seek": 835432, "start": 8372.48, "end": 8375.36, "text": " philosopher of language in a way, thinking about these things. It's a fine thought.", "tokens": [51272, 29805, 295, 2856, 294, 257, 636, 11, 1953, 466, 613, 721, 13, 467, 311, 257, 2489, 1194, 13, 51416], "temperature": 0.0, "avg_logprob": -0.08968145271827435, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.006096596829593182}, {"id": 1618, "seek": 835432, "start": 8376.48, "end": 8381.76, "text": " You can't test it in his methods. You can't do a thought experiment to figure that out.", "tokens": [51472, 509, 393, 380, 1500, 309, 294, 702, 7150, 13, 509, 393, 380, 360, 257, 1194, 5120, 281, 2573, 300, 484, 13, 51736], "temperature": 0.0, "avg_logprob": -0.08968145271827435, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.006096596829593182}, {"id": 1619, "seek": 838176, "start": 8381.76, "end": 8387.36, "text": " You need a scanner. You need brain damage people. You need something. You need ways to measure that.", "tokens": [50364, 509, 643, 257, 30211, 13, 509, 643, 3567, 4344, 561, 13, 509, 643, 746, 13, 509, 643, 2098, 281, 3481, 300, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1239097768610174, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0009109885431826115}, {"id": 1620, "seek": 838176, "start": 8387.36, "end": 8393.92, "text": " And that's what, you know, fMRI offers as a, and, you know, patients are a little messier.", "tokens": [50644, 400, 300, 311, 437, 11, 291, 458, 11, 283, 44, 5577, 7736, 382, 257, 11, 293, 11, 291, 458, 11, 4209, 366, 257, 707, 2082, 811, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1239097768610174, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0009109885431826115}, {"id": 1621, "seek": 838176, "start": 8393.92, "end": 8400.24, "text": " fMRI is pretty unambiguous, I'd say. It's like very unambiguous. There's no way to say that", "tokens": [50972, 283, 44, 5577, 307, 1238, 517, 2173, 30525, 11, 286, 1116, 584, 13, 467, 311, 411, 588, 517, 2173, 30525, 13, 821, 311, 572, 636, 281, 584, 300, 51288], "temperature": 0.0, "avg_logprob": -0.1239097768610174, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0009109885431826115}, {"id": 1622, "seek": 838176, "start": 8400.24, "end": 8405.44, "text": " the language network is doing any of these tasks. There's, like, you should look at those data.", "tokens": [51288, 264, 2856, 3209, 307, 884, 604, 295, 613, 9608, 13, 821, 311, 11, 411, 11, 291, 820, 574, 412, 729, 1412, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1239097768610174, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0009109885431826115}, {"id": 1623, "seek": 838176, "start": 8405.44, "end": 8409.76, "text": " It's like, there's no chance that you can say that those networks are overlapping. They're not", "tokens": [51548, 467, 311, 411, 11, 456, 311, 572, 2931, 300, 291, 393, 584, 300, 729, 9590, 366, 33535, 13, 814, 434, 406, 51764], "temperature": 0.0, "avg_logprob": -0.1239097768610174, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0009109885431826115}, {"id": 1624, "seek": 840976, "start": 8409.76, "end": 8414.16, "text": " overlapping. They're just, like, completely different. And so, you know, so the, you know,", "tokens": [50364, 33535, 13, 814, 434, 445, 11, 411, 11, 2584, 819, 13, 400, 370, 11, 291, 458, 11, 370, 264, 11, 291, 458, 11, 50584], "temperature": 0.0, "avg_logprob": -0.135951534394295, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0010319286957383156}, {"id": 1625, "seek": 840976, "start": 8414.16, "end": 8417.84, "text": " you can always make, you know, it's only two people. It's four people or something for the", "tokens": [50584, 291, 393, 1009, 652, 11, 291, 458, 11, 309, 311, 787, 732, 561, 13, 467, 311, 1451, 561, 420, 746, 337, 264, 50768], "temperature": 0.0, "avg_logprob": -0.135951534394295, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0010319286957383156}, {"id": 1626, "seek": 840976, "start": 8418.56, "end": 8422.64, "text": " patients. And there's something special about them we don't know. But these are just random people.", "tokens": [50804, 4209, 13, 400, 456, 311, 746, 2121, 466, 552, 321, 500, 380, 458, 13, 583, 613, 366, 445, 4974, 561, 13, 51008], "temperature": 0.0, "avg_logprob": -0.135951534394295, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0010319286957383156}, {"id": 1627, "seek": 840976, "start": 8423.28, "end": 8428.64, "text": " And with lots of them, and you find always the same effects. And it's very robust, I'd say.", "tokens": [51040, 400, 365, 3195, 295, 552, 11, 293, 291, 915, 1009, 264, 912, 5065, 13, 400, 309, 311, 588, 13956, 11, 286, 1116, 584, 13, 51308], "temperature": 0.0, "avg_logprob": -0.135951534394295, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0010319286957383156}, {"id": 1628, "seek": 840976, "start": 8428.64, "end": 8435.44, "text": " What's a fessing effect? What's the, you mentioned Bolivia. What's the connection between", "tokens": [51308, 708, 311, 257, 283, 442, 278, 1802, 30, 708, 311, 264, 11, 291, 2835, 14331, 18503, 13, 708, 311, 264, 4984, 1296, 51648], "temperature": 0.0, "avg_logprob": -0.135951534394295, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0010319286957383156}, {"id": 1629, "seek": 843544, "start": 8436.0, "end": 8445.44, "text": " culture and language? You've also mentioned that, you know, much of our study of language", "tokens": [50392, 3713, 293, 2856, 30, 509, 600, 611, 2835, 300, 11, 291, 458, 11, 709, 295, 527, 2979, 295, 2856, 50864], "temperature": 0.0, "avg_logprob": -0.13841600675840635, "compression_ratio": 1.4153846153846155, "no_speech_prob": 0.014058531261980534}, {"id": 1630, "seek": 843544, "start": 8445.44, "end": 8452.720000000001, "text": " comes from W-E-I-R-D, weird people, western educated, industrialized, rich, and democratic.", "tokens": [50864, 1487, 490, 343, 12, 36, 12, 40, 12, 49, 12, 35, 11, 3657, 561, 11, 13231, 15872, 11, 9987, 1602, 11, 4593, 11, 293, 15337, 13, 51228], "temperature": 0.0, "avg_logprob": -0.13841600675840635, "compression_ratio": 1.4153846153846155, "no_speech_prob": 0.014058531261980534}, {"id": 1631, "seek": 843544, "start": 8453.68, "end": 8460.4, "text": " So when you study, like, remote cultures, such as around the Amazon jungle, what can you learn", "tokens": [51276, 407, 562, 291, 2979, 11, 411, 11, 8607, 12951, 11, 1270, 382, 926, 264, 6795, 18228, 11, 437, 393, 291, 1466, 51612], "temperature": 0.0, "avg_logprob": -0.13841600675840635, "compression_ratio": 1.4153846153846155, "no_speech_prob": 0.014058531261980534}, {"id": 1632, "seek": 846040, "start": 8460.48, "end": 8468.0, "text": " about language? So that term weird is from Joe Henrich. He's at Harvard. He's a Harvard", "tokens": [50368, 466, 2856, 30, 407, 300, 1433, 3657, 307, 490, 6807, 8651, 10794, 13, 634, 311, 412, 13378, 13, 634, 311, 257, 13378, 50744], "temperature": 0.0, "avg_logprob": -0.11299193036425245, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.008979042991995811}, {"id": 1633, "seek": 846040, "start": 8468.0, "end": 8475.52, "text": " evolutionary biologist. And so he works on lots of different topics. And he basically was pushing", "tokens": [50744, 27567, 3228, 9201, 13, 400, 370, 415, 1985, 322, 3195, 295, 819, 8378, 13, 400, 415, 1936, 390, 7380, 51120], "temperature": 0.0, "avg_logprob": -0.11299193036425245, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.008979042991995811}, {"id": 1634, "seek": 846040, "start": 8475.52, "end": 8481.68, "text": " that observation that we should be careful about the inferences we want to make when we're talking", "tokens": [51120, 300, 14816, 300, 321, 820, 312, 5026, 466, 264, 13596, 2667, 321, 528, 281, 652, 562, 321, 434, 1417, 51428], "temperature": 0.0, "avg_logprob": -0.11299193036425245, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.008979042991995811}, {"id": 1635, "seek": 846040, "start": 8481.68, "end": 8488.64, "text": " in psychology or, yeah, mostly in psychology, I guess, about humans if we're talking about,", "tokens": [51428, 294, 15105, 420, 11, 1338, 11, 5240, 294, 15105, 11, 286, 2041, 11, 466, 6255, 498, 321, 434, 1417, 466, 11, 51776], "temperature": 0.0, "avg_logprob": -0.11299193036425245, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.008979042991995811}, {"id": 1636, "seek": 848864, "start": 8489.519999999999, "end": 8494.16, "text": " undergrads at MIT and Harvard. Those aren't the same, right? These aren't the same things.", "tokens": [50408, 833, 861, 5834, 412, 13100, 293, 13378, 13, 3950, 3212, 380, 264, 912, 11, 558, 30, 1981, 3212, 380, 264, 912, 721, 13, 50640], "temperature": 0.0, "avg_logprob": -0.10965247626777168, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0028880636673420668}, {"id": 1637, "seek": 848864, "start": 8494.16, "end": 8499.76, "text": " And so if you want to make inferences about language, for instance, there's a lot of very,", "tokens": [50640, 400, 370, 498, 291, 528, 281, 652, 13596, 2667, 466, 2856, 11, 337, 5197, 11, 456, 311, 257, 688, 295, 588, 11, 50920], "temperature": 0.0, "avg_logprob": -0.10965247626777168, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0028880636673420668}, {"id": 1638, "seek": 848864, "start": 8500.32, "end": 8505.359999999999, "text": " a lot of other kinds of languages in the world, then English and French and Chinese, you know.", "tokens": [50948, 257, 688, 295, 661, 3685, 295, 8650, 294, 264, 1002, 11, 550, 3669, 293, 5522, 293, 4649, 11, 291, 458, 13, 51200], "temperature": 0.0, "avg_logprob": -0.10965247626777168, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0028880636673420668}, {"id": 1639, "seek": 848864, "start": 8505.359999999999, "end": 8512.0, "text": " And so maybe for language, we care about how culture, because cultures can be very,", "tokens": [51200, 400, 370, 1310, 337, 2856, 11, 321, 1127, 466, 577, 3713, 11, 570, 12951, 393, 312, 588, 11, 51532], "temperature": 0.0, "avg_logprob": -0.10965247626777168, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0028880636673420668}, {"id": 1640, "seek": 848864, "start": 8512.0, "end": 8514.88, "text": " I mean, of course, English and Chinese cultures are very different. But", "tokens": [51532, 286, 914, 11, 295, 1164, 11, 3669, 293, 4649, 12951, 366, 588, 819, 13, 583, 51676], "temperature": 0.0, "avg_logprob": -0.10965247626777168, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0028880636673420668}, {"id": 1641, "seek": 851488, "start": 8515.519999999999, "end": 8521.279999999999, "text": " you know, hunter-gatherers are much more different in some ways. And so if culture", "tokens": [50396, 291, 458, 11, 22970, 12, 70, 1172, 433, 366, 709, 544, 819, 294, 512, 2098, 13, 400, 370, 498, 3713, 50684], "temperature": 0.0, "avg_logprob": -0.13602126946970194, "compression_ratio": 1.7509727626459144, "no_speech_prob": 0.0010003158822655678}, {"id": 1642, "seek": 851488, "start": 8521.279999999999, "end": 8526.96, "text": " hasn't affected what language is, then we kind of want to look there as well as looking. It's", "tokens": [50684, 6132, 380, 8028, 437, 2856, 307, 11, 550, 321, 733, 295, 528, 281, 574, 456, 382, 731, 382, 1237, 13, 467, 311, 50968], "temperature": 0.0, "avg_logprob": -0.13602126946970194, "compression_ratio": 1.7509727626459144, "no_speech_prob": 0.0010003158822655678}, {"id": 1643, "seek": 851488, "start": 8526.96, "end": 8530.32, "text": " not like the industrialized cultures aren't interesting. Of course they are. But we want", "tokens": [50968, 406, 411, 264, 9987, 1602, 12951, 3212, 380, 1880, 13, 2720, 1164, 436, 366, 13, 583, 321, 528, 51136], "temperature": 0.0, "avg_logprob": -0.13602126946970194, "compression_ratio": 1.7509727626459144, "no_speech_prob": 0.0010003158822655678}, {"id": 1644, "seek": 851488, "start": 8530.32, "end": 8535.119999999999, "text": " to look at non-industrialized cultures as well. And so I've worked with two. I've worked with", "tokens": [51136, 281, 574, 412, 2107, 12, 29850, 7111, 1602, 12951, 382, 731, 13, 400, 370, 286, 600, 2732, 365, 732, 13, 286, 600, 2732, 365, 51376], "temperature": 0.0, "avg_logprob": -0.13602126946970194, "compression_ratio": 1.7509727626459144, "no_speech_prob": 0.0010003158822655678}, {"id": 1645, "seek": 851488, "start": 8535.119999999999, "end": 8542.4, "text": " Chimani, which are in Bolivia and Amazon, both in the Amazon in these cases. And there are", "tokens": [51376, 761, 332, 3782, 11, 597, 366, 294, 14331, 18503, 293, 6795, 11, 1293, 294, 264, 6795, 294, 613, 3331, 13, 400, 456, 366, 51740], "temperature": 0.0, "avg_logprob": -0.13602126946970194, "compression_ratio": 1.7509727626459144, "no_speech_prob": 0.0010003158822655678}, {"id": 1646, "seek": 854240, "start": 8542.4, "end": 8548.48, "text": " so-called farmer foragers, which is not hunter-gatherers. It's sort of one up from hunter-gatherers", "tokens": [50364, 370, 12, 11880, 17891, 337, 8776, 11, 597, 307, 406, 22970, 12, 70, 1172, 433, 13, 467, 311, 1333, 295, 472, 493, 490, 22970, 12, 70, 1172, 433, 50668], "temperature": 0.0, "avg_logprob": -0.08987156550089519, "compression_ratio": 1.9631147540983607, "no_speech_prob": 0.010647003538906574}, {"id": 1647, "seek": 854240, "start": 8548.48, "end": 8552.96, "text": " in that they do a little bit of farming as well. A lot of hunting as well, but a little bit of", "tokens": [50668, 294, 300, 436, 360, 257, 707, 857, 295, 16557, 382, 731, 13, 316, 688, 295, 12599, 382, 731, 11, 457, 257, 707, 857, 295, 50892], "temperature": 0.0, "avg_logprob": -0.08987156550089519, "compression_ratio": 1.9631147540983607, "no_speech_prob": 0.010647003538906574}, {"id": 1648, "seek": 854240, "start": 8552.96, "end": 8557.52, "text": " farming. And the kind of farming they do is the kind of farming that I might do if I ever were", "tokens": [50892, 16557, 13, 400, 264, 733, 295, 16557, 436, 360, 307, 264, 733, 295, 16557, 300, 286, 1062, 360, 498, 286, 1562, 645, 51120], "temperature": 0.0, "avg_logprob": -0.08987156550089519, "compression_ratio": 1.9631147540983607, "no_speech_prob": 0.010647003538906574}, {"id": 1649, "seek": 854240, "start": 8557.52, "end": 8563.52, "text": " to grow like tomatoes or something in my backyard. So it's not like big field farming. It's just a", "tokens": [51120, 281, 1852, 411, 15135, 420, 746, 294, 452, 20036, 13, 407, 309, 311, 406, 411, 955, 2519, 16557, 13, 467, 311, 445, 257, 51420], "temperature": 0.0, "avg_logprob": -0.08987156550089519, "compression_ratio": 1.9631147540983607, "no_speech_prob": 0.010647003538906574}, {"id": 1650, "seek": 854240, "start": 8563.52, "end": 8567.44, "text": " farming for a family, a few things you do that. And so that's the kind of farming they do.", "tokens": [51420, 16557, 337, 257, 1605, 11, 257, 1326, 721, 291, 360, 300, 13, 400, 370, 300, 311, 264, 733, 295, 16557, 436, 360, 13, 51616], "temperature": 0.0, "avg_logprob": -0.08987156550089519, "compression_ratio": 1.9631147540983607, "no_speech_prob": 0.010647003538906574}, {"id": 1651, "seek": 856744, "start": 8567.84, "end": 8574.720000000001, "text": " And the other group I've worked with are the Pirah\u00e1, which are also in the Amazon and happen", "tokens": [50384, 400, 264, 661, 1594, 286, 600, 2732, 365, 366, 264, 24161, 545, 842, 11, 597, 366, 611, 294, 264, 6795, 293, 1051, 50728], "temperature": 0.0, "avg_logprob": -0.18199448732985662, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.041975852102041245}, {"id": 1652, "seek": 856744, "start": 8574.720000000001, "end": 8582.480000000001, "text": " to be in Brazil. And that's with a guy called Dan Everett, who is a linguist, anthropologist,", "tokens": [50728, 281, 312, 294, 9435, 13, 400, 300, 311, 365, 257, 2146, 1219, 3394, 12123, 3093, 11, 567, 307, 257, 21766, 468, 11, 22727, 9201, 11, 51116], "temperature": 0.0, "avg_logprob": -0.18199448732985662, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.041975852102041245}, {"id": 1653, "seek": 856744, "start": 8582.480000000001, "end": 8588.16, "text": " who actually lived and worked in the... I mean, he was a missionary, actually, initially, back in", "tokens": [51116, 567, 767, 5152, 293, 2732, 294, 264, 485, 286, 914, 11, 415, 390, 257, 45418, 11, 767, 11, 9105, 11, 646, 294, 51400], "temperature": 0.0, "avg_logprob": -0.18199448732985662, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.041975852102041245}, {"id": 1654, "seek": 856744, "start": 8588.16, "end": 8593.92, "text": " the 70s, working with trying to translate languages so they could teach them the Bible,", "tokens": [51400, 264, 5285, 82, 11, 1364, 365, 1382, 281, 13799, 8650, 370, 436, 727, 2924, 552, 264, 6544, 11, 51688], "temperature": 0.0, "avg_logprob": -0.18199448732985662, "compression_ratio": 1.5477178423236515, "no_speech_prob": 0.041975852102041245}, {"id": 1655, "seek": 859392, "start": 8593.92, "end": 8599.2, "text": " teach them Christianity. What can you say about that? Yeah, so the two groups I've worked with,", "tokens": [50364, 2924, 552, 17326, 13, 708, 393, 291, 584, 466, 300, 30, 865, 11, 370, 264, 732, 3935, 286, 600, 2732, 365, 11, 50628], "temperature": 0.0, "avg_logprob": -0.15434899984621533, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.018814221024513245}, {"id": 1656, "seek": 859392, "start": 8599.2, "end": 8605.68, "text": " the Cimani and the Pirah\u00e1, are both isolate languages, meaning there's no known connected", "tokens": [50628, 264, 383, 332, 3782, 293, 264, 24161, 545, 842, 11, 366, 1293, 25660, 8650, 11, 3620, 456, 311, 572, 2570, 4582, 50952], "temperature": 0.0, "avg_logprob": -0.15434899984621533, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.018814221024513245}, {"id": 1657, "seek": 859392, "start": 8605.68, "end": 8609.76, "text": " languages at all. They're just like on their own. Oh, cool. Yeah, there's a lot of those. And most of", "tokens": [50952, 8650, 412, 439, 13, 814, 434, 445, 411, 322, 641, 1065, 13, 876, 11, 1627, 13, 865, 11, 456, 311, 257, 688, 295, 729, 13, 400, 881, 295, 51156], "temperature": 0.0, "avg_logprob": -0.15434899984621533, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.018814221024513245}, {"id": 1658, "seek": 859392, "start": 8609.76, "end": 8619.52, "text": " the isolates occur in the Amazon or in Papua New Guinea, in these places where the world has", "tokens": [51156, 264, 7381, 1024, 5160, 294, 264, 6795, 420, 294, 15919, 4398, 1873, 46793, 11, 294, 613, 3190, 689, 264, 1002, 575, 51644], "temperature": 0.0, "avg_logprob": -0.15434899984621533, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.018814221024513245}, {"id": 1659, "seek": 861952, "start": 8619.52, "end": 8626.24, "text": " sort of stayed still for long enough. And so there aren't earthquakes. There aren't...", "tokens": [50364, 1333, 295, 9181, 920, 337, 938, 1547, 13, 400, 370, 456, 3212, 380, 34048, 13, 821, 3212, 380, 485, 50700], "temperature": 0.0, "avg_logprob": -0.15925841981714423, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0240187831223011}, {"id": 1660, "seek": 861952, "start": 8628.4, "end": 8634.24, "text": " Well, certainly no earthquakes in the Amazon jungle. And the climate isn't bad. So you don't", "tokens": [50808, 1042, 11, 3297, 572, 34048, 294, 264, 6795, 18228, 13, 400, 264, 5659, 1943, 380, 1578, 13, 407, 291, 500, 380, 51100], "temperature": 0.0, "avg_logprob": -0.15925841981714423, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0240187831223011}, {"id": 1661, "seek": 861952, "start": 8634.24, "end": 8639.36, "text": " have droughts. And so in Africa, you've got a lot of moving of people because there's", "tokens": [51100, 362, 22900, 82, 13, 400, 370, 294, 7349, 11, 291, 600, 658, 257, 688, 295, 2684, 295, 561, 570, 456, 311, 51356], "temperature": 0.0, "avg_logprob": -0.15925841981714423, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0240187831223011}, {"id": 1662, "seek": 861952, "start": 8639.36, "end": 8643.6, "text": " drought problems. And so they get a lot of language contact when people have to...", "tokens": [51356, 22900, 2740, 13, 400, 370, 436, 483, 257, 688, 295, 2856, 3385, 562, 561, 362, 281, 485, 51568], "temperature": 0.0, "avg_logprob": -0.15925841981714423, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0240187831223011}, {"id": 1663, "seek": 864360, "start": 8644.32, "end": 8650.16, "text": " You've got to move because you've got no water, then you've got to get going. And then you run", "tokens": [50400, 509, 600, 658, 281, 1286, 570, 291, 600, 658, 572, 1281, 11, 550, 291, 600, 658, 281, 483, 516, 13, 400, 550, 291, 1190, 50692], "temperature": 0.0, "avg_logprob": -0.10675259105494765, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.03408733010292053}, {"id": 1664, "seek": 864360, "start": 8650.16, "end": 8655.6, "text": " into contact with other tribes, other groups. In the Amazon, that's not the case. And so people", "tokens": [50692, 666, 3385, 365, 661, 19035, 11, 661, 3935, 13, 682, 264, 6795, 11, 300, 311, 406, 264, 1389, 13, 400, 370, 561, 50964], "temperature": 0.0, "avg_logprob": -0.10675259105494765, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.03408733010292053}, {"id": 1665, "seek": 864360, "start": 8655.6, "end": 8659.52, "text": " can stay there for hundreds and hundreds and probably thousands of years, I guess. And so these", "tokens": [50964, 393, 1754, 456, 337, 6779, 293, 6779, 293, 1391, 5383, 295, 924, 11, 286, 2041, 13, 400, 370, 613, 51160], "temperature": 0.0, "avg_logprob": -0.10675259105494765, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.03408733010292053}, {"id": 1666, "seek": 864360, "start": 8659.52, "end": 8665.28, "text": " groups have... The Cimani and the Pirah\u00e1 are both isolates in that. And I guess they've just lived", "tokens": [51160, 3935, 362, 485, 440, 383, 332, 3782, 293, 264, 24161, 545, 842, 366, 1293, 7381, 1024, 294, 300, 13, 400, 286, 2041, 436, 600, 445, 5152, 51448], "temperature": 0.0, "avg_logprob": -0.10675259105494765, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.03408733010292053}, {"id": 1667, "seek": 864360, "start": 8665.28, "end": 8673.04, "text": " there for ages and ages with minimal contact with other outside groups. And so, I mean,", "tokens": [51448, 456, 337, 12357, 293, 12357, 365, 13206, 3385, 365, 661, 2380, 3935, 13, 400, 370, 11, 286, 914, 11, 51836], "temperature": 0.0, "avg_logprob": -0.10675259105494765, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.03408733010292053}, {"id": 1668, "seek": 867304, "start": 8673.12, "end": 8678.960000000001, "text": " I'm interested in them because they are... In these cases, I'm interested in their words.", "tokens": [50368, 286, 478, 3102, 294, 552, 570, 436, 366, 485, 682, 613, 3331, 11, 286, 478, 3102, 294, 641, 2283, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1275262745148545, "compression_ratio": 1.734375, "no_speech_prob": 0.0001851901615737006}, {"id": 1669, "seek": 867304, "start": 8678.960000000001, "end": 8683.84, "text": " I would love to study their syntax, their orders of words, but I'm mostly just interested in how", "tokens": [50660, 286, 576, 959, 281, 2979, 641, 28431, 11, 641, 9470, 295, 2283, 11, 457, 286, 478, 5240, 445, 3102, 294, 577, 50904], "temperature": 0.0, "avg_logprob": -0.1275262745148545, "compression_ratio": 1.734375, "no_speech_prob": 0.0001851901615737006}, {"id": 1670, "seek": 867304, "start": 8683.84, "end": 8690.800000000001, "text": " languages are connected to their cultures in this way. And so with the Pirah\u00e1,", "tokens": [50904, 8650, 366, 4582, 281, 641, 12951, 294, 341, 636, 13, 400, 370, 365, 264, 24161, 545, 842, 11, 51252], "temperature": 0.0, "avg_logprob": -0.1275262745148545, "compression_ratio": 1.734375, "no_speech_prob": 0.0001851901615737006}, {"id": 1671, "seek": 867304, "start": 8690.800000000001, "end": 8695.6, "text": " sort of most interesting, I was working on number there, number information. And so the", "tokens": [51252, 1333, 295, 881, 1880, 11, 286, 390, 1364, 322, 1230, 456, 11, 1230, 1589, 13, 400, 370, 264, 51492], "temperature": 0.0, "avg_logprob": -0.1275262745148545, "compression_ratio": 1.734375, "no_speech_prob": 0.0001851901615737006}, {"id": 1672, "seek": 867304, "start": 8695.6, "end": 8699.2, "text": " basic idea is I think language is invented. That's what I get from the words here is that", "tokens": [51492, 3875, 1558, 307, 286, 519, 2856, 307, 14479, 13, 663, 311, 437, 286, 483, 490, 264, 2283, 510, 307, 300, 51672], "temperature": 0.0, "avg_logprob": -0.1275262745148545, "compression_ratio": 1.734375, "no_speech_prob": 0.0001851901615737006}, {"id": 1673, "seek": 869920, "start": 8699.2, "end": 8703.12, "text": " I think language is invented. We talked about color earlier. It's the same idea,", "tokens": [50364, 286, 519, 2856, 307, 14479, 13, 492, 2825, 466, 2017, 3071, 13, 467, 311, 264, 912, 1558, 11, 50560], "temperature": 0.0, "avg_logprob": -0.09721314705024331, "compression_ratio": 1.8130081300813008, "no_speech_prob": 0.0028879353776574135}, {"id": 1674, "seek": 869920, "start": 8703.12, "end": 8708.640000000001, "text": " so that what you need to talk about with someone else is what you're going to invent words for.", "tokens": [50560, 370, 300, 437, 291, 643, 281, 751, 466, 365, 1580, 1646, 307, 437, 291, 434, 516, 281, 7962, 2283, 337, 13, 50836], "temperature": 0.0, "avg_logprob": -0.09721314705024331, "compression_ratio": 1.8130081300813008, "no_speech_prob": 0.0028879353776574135}, {"id": 1675, "seek": 869920, "start": 8709.2, "end": 8716.240000000002, "text": " And so we invent labels for colors that I need, not that I can see, but that things I need to", "tokens": [50864, 400, 370, 321, 7962, 16949, 337, 4577, 300, 286, 643, 11, 406, 300, 286, 393, 536, 11, 457, 300, 721, 286, 643, 281, 51216], "temperature": 0.0, "avg_logprob": -0.09721314705024331, "compression_ratio": 1.8130081300813008, "no_speech_prob": 0.0028879353776574135}, {"id": 1676, "seek": 869920, "start": 8716.240000000002, "end": 8720.08, "text": " tell you about so that I can get objects from you or get you to give me the right objects.", "tokens": [51216, 980, 291, 466, 370, 300, 286, 393, 483, 6565, 490, 291, 420, 483, 291, 281, 976, 385, 264, 558, 6565, 13, 51408], "temperature": 0.0, "avg_logprob": -0.09721314705024331, "compression_ratio": 1.8130081300813008, "no_speech_prob": 0.0028879353776574135}, {"id": 1677, "seek": 869920, "start": 8720.08, "end": 8726.960000000001, "text": " And I just don't need a word for teal or a word for aquamarine in the Amazon jungle,", "tokens": [51408, 400, 286, 445, 500, 380, 643, 257, 1349, 337, 535, 304, 420, 257, 1349, 337, 2373, 45207, 533, 294, 264, 6795, 18228, 11, 51752], "temperature": 0.0, "avg_logprob": -0.09721314705024331, "compression_ratio": 1.8130081300813008, "no_speech_prob": 0.0028879353776574135}, {"id": 1678, "seek": 872696, "start": 8726.96, "end": 8730.64, "text": " for the most part, because I don't have two things which differ on those colors. I just", "tokens": [50364, 337, 264, 881, 644, 11, 570, 286, 500, 380, 362, 732, 721, 597, 743, 322, 729, 4577, 13, 286, 445, 50548], "temperature": 0.0, "avg_logprob": -0.06986568262288859, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.020321393385529518}, {"id": 1679, "seek": 872696, "start": 8730.64, "end": 8736.56, "text": " don't have that. And so numbers are really another fascinating source of information here where", "tokens": [50548, 500, 380, 362, 300, 13, 400, 370, 3547, 366, 534, 1071, 10343, 4009, 295, 1589, 510, 689, 50844], "temperature": 0.0, "avg_logprob": -0.06986568262288859, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.020321393385529518}, {"id": 1680, "seek": 872696, "start": 8737.439999999999, "end": 8744.88, "text": " you might... Naively, I certainly thought that all humans would have words for exact counting", "tokens": [50888, 291, 1062, 485, 6056, 3413, 11, 286, 3297, 1194, 300, 439, 6255, 576, 362, 2283, 337, 1900, 13251, 51260], "temperature": 0.0, "avg_logprob": -0.06986568262288859, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.020321393385529518}, {"id": 1681, "seek": 872696, "start": 8746.0, "end": 8751.919999999998, "text": " and the Pirah\u00e1 don't. So they don't have any words for even one. There's not a word for one", "tokens": [51316, 293, 264, 24161, 545, 842, 500, 380, 13, 407, 436, 500, 380, 362, 604, 2283, 337, 754, 472, 13, 821, 311, 406, 257, 1349, 337, 472, 51612], "temperature": 0.0, "avg_logprob": -0.06986568262288859, "compression_ratio": 1.5611814345991561, "no_speech_prob": 0.020321393385529518}, {"id": 1682, "seek": 875192, "start": 8751.92, "end": 8757.92, "text": " in their language. And so there's certainly not a word for two, three, or four. So that kind of", "tokens": [50364, 294, 641, 2856, 13, 400, 370, 456, 311, 3297, 406, 257, 1349, 337, 732, 11, 1045, 11, 420, 1451, 13, 407, 300, 733, 295, 50664], "temperature": 0.0, "avg_logprob": -0.1339666129898851, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.07579263299703598}, {"id": 1683, "seek": 875192, "start": 8757.92, "end": 8762.4, "text": " blows people's minds off. Yeah, that is blowing my mind. That's pretty weird. How are you going to", "tokens": [50664, 18458, 561, 311, 9634, 766, 13, 865, 11, 300, 307, 15068, 452, 1575, 13, 663, 311, 1238, 3657, 13, 1012, 366, 291, 516, 281, 50888], "temperature": 0.0, "avg_logprob": -0.1339666129898851, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.07579263299703598}, {"id": 1684, "seek": 875192, "start": 8762.4, "end": 8767.76, "text": " ask, I want two of those? You just don't. And so that's just not a thing you can possibly ask", "tokens": [50888, 1029, 11, 286, 528, 732, 295, 729, 30, 509, 445, 500, 380, 13, 400, 370, 300, 311, 445, 406, 257, 551, 291, 393, 6264, 1029, 51156], "temperature": 0.0, "avg_logprob": -0.1339666129898851, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.07579263299703598}, {"id": 1685, "seek": 875192, "start": 8767.76, "end": 8772.16, "text": " in the Pirah\u00e1. It's not possible. That is, there's no words for that. So here's how we found this", "tokens": [51156, 294, 264, 24161, 545, 842, 13, 467, 311, 406, 1944, 13, 663, 307, 11, 456, 311, 572, 2283, 337, 300, 13, 407, 510, 311, 577, 321, 1352, 341, 51376], "temperature": 0.0, "avg_logprob": -0.1339666129898851, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.07579263299703598}, {"id": 1686, "seek": 875192, "start": 8772.16, "end": 8778.24, "text": " out. So it was thought to be a one, two, many language. There are three words, four quantifiers", "tokens": [51376, 484, 13, 407, 309, 390, 1194, 281, 312, 257, 472, 11, 732, 11, 867, 2856, 13, 821, 366, 1045, 2283, 11, 1451, 4426, 23463, 51680], "temperature": 0.0, "avg_logprob": -0.1339666129898851, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.07579263299703598}, {"id": 1687, "seek": 877824, "start": 8778.96, "end": 8785.28, "text": " for sets. And people had thought that those meant one, two, and many. But what they really", "tokens": [50400, 337, 6352, 13, 400, 561, 632, 1194, 300, 729, 4140, 472, 11, 732, 11, 293, 867, 13, 583, 437, 436, 534, 50716], "temperature": 0.0, "avg_logprob": -0.13386450873480904, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.020003490149974823}, {"id": 1688, "seek": 877824, "start": 8785.28, "end": 8790.32, "text": " mean is few, some, and many. Many is correct. It's few, some, and many. And so the way we", "tokens": [50716, 914, 307, 1326, 11, 512, 11, 293, 867, 13, 5126, 307, 3006, 13, 467, 311, 1326, 11, 512, 11, 293, 867, 13, 400, 370, 264, 636, 321, 50968], "temperature": 0.0, "avg_logprob": -0.13386450873480904, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.020003490149974823}, {"id": 1689, "seek": 877824, "start": 8790.32, "end": 8798.4, "text": " figured this out, and this is kind of cool, is that we gave people... We had a set of objects.", "tokens": [50968, 8932, 341, 484, 11, 293, 341, 307, 733, 295, 1627, 11, 307, 300, 321, 2729, 561, 485, 492, 632, 257, 992, 295, 6565, 13, 51372], "temperature": 0.0, "avg_logprob": -0.13386450873480904, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.020003490149974823}, {"id": 1690, "seek": 877824, "start": 8798.4, "end": 8802.32, "text": " These are having to be spools of thread. It doesn't really matter what they are. Identical objects.", "tokens": [51372, 1981, 366, 1419, 281, 312, 637, 29298, 295, 7207, 13, 467, 1177, 380, 534, 1871, 437, 436, 366, 13, 25905, 804, 6565, 13, 51568], "temperature": 0.0, "avg_logprob": -0.13386450873480904, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.020003490149974823}, {"id": 1691, "seek": 877824, "start": 8802.32, "end": 8806.4, "text": " And when I sort of start off here, I just give you one of those and say,", "tokens": [51568, 400, 562, 286, 1333, 295, 722, 766, 510, 11, 286, 445, 976, 291, 472, 295, 729, 293, 584, 11, 51772], "temperature": 0.0, "avg_logprob": -0.13386450873480904, "compression_ratio": 1.7364341085271318, "no_speech_prob": 0.020003490149974823}, {"id": 1692, "seek": 880640, "start": 8806.4, "end": 8810.8, "text": " what's that? Okay, so you're a Pirah\u00e1 speaker and you tell me what it is. And then I give you two", "tokens": [50364, 437, 311, 300, 30, 1033, 11, 370, 291, 434, 257, 24161, 545, 842, 8145, 293, 291, 980, 385, 437, 309, 307, 13, 400, 550, 286, 976, 291, 732, 50584], "temperature": 0.0, "avg_logprob": -0.11858733794974081, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.014943471178412437}, {"id": 1693, "seek": 880640, "start": 8810.8, "end": 8815.68, "text": " and say, what's that? And nothing's changing in this set except for the number. And then I just", "tokens": [50584, 293, 584, 11, 437, 311, 300, 30, 400, 1825, 311, 4473, 294, 341, 992, 3993, 337, 264, 1230, 13, 400, 550, 286, 445, 50828], "temperature": 0.0, "avg_logprob": -0.11858733794974081, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.014943471178412437}, {"id": 1694, "seek": 880640, "start": 8815.68, "end": 8819.119999999999, "text": " ask you to label these things. We just do this for a bunch of different people. And frankly,", "tokens": [50828, 1029, 291, 281, 7645, 613, 721, 13, 492, 445, 360, 341, 337, 257, 3840, 295, 819, 561, 13, 400, 11939, 11, 51000], "temperature": 0.0, "avg_logprob": -0.11858733794974081, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.014943471178412437}, {"id": 1695, "seek": 880640, "start": 8819.119999999999, "end": 8826.0, "text": " it's a... I did this task. And it's a little bit weird. So they say the word that we thought was", "tokens": [51000, 309, 311, 257, 485, 286, 630, 341, 5633, 13, 400, 309, 311, 257, 707, 857, 3657, 13, 407, 436, 584, 264, 1349, 300, 321, 1194, 390, 51344], "temperature": 0.0, "avg_logprob": -0.11858733794974081, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.014943471178412437}, {"id": 1696, "seek": 880640, "start": 8826.0, "end": 8830.24, "text": " one, it's few, but for the first one. And then maybe they say few or maybe they say some for", "tokens": [51344, 472, 11, 309, 311, 1326, 11, 457, 337, 264, 700, 472, 13, 400, 550, 1310, 436, 584, 1326, 420, 1310, 436, 584, 512, 337, 51556], "temperature": 0.0, "avg_logprob": -0.11858733794974081, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.014943471178412437}, {"id": 1697, "seek": 880640, "start": 8830.24, "end": 8835.359999999999, "text": " the second. And then for the third or the fourth, they start using the word many for the set. And", "tokens": [51556, 264, 1150, 13, 400, 550, 337, 264, 2636, 420, 264, 6409, 11, 436, 722, 1228, 264, 1349, 867, 337, 264, 992, 13, 400, 51812], "temperature": 0.0, "avg_logprob": -0.11858733794974081, "compression_ratio": 1.8668831168831168, "no_speech_prob": 0.014943471178412437}, {"id": 1698, "seek": 883536, "start": 8835.36, "end": 8840.720000000001, "text": " then five, six, seven, eight. I go all the way to ten. And it's always the same word. And they", "tokens": [50364, 550, 1732, 11, 2309, 11, 3407, 11, 3180, 13, 286, 352, 439, 264, 636, 281, 2064, 13, 400, 309, 311, 1009, 264, 912, 1349, 13, 400, 436, 50632], "temperature": 0.0, "avg_logprob": -0.08923378286435622, "compression_ratio": 1.86328125, "no_speech_prob": 0.0006461040466092527}, {"id": 1699, "seek": 883536, "start": 8840.720000000001, "end": 8846.480000000001, "text": " look at me like I'm stupid because they told me what the word was for six, seven, eight. And I'm", "tokens": [50632, 574, 412, 385, 411, 286, 478, 6631, 570, 436, 1907, 385, 437, 264, 1349, 390, 337, 2309, 11, 3407, 11, 3180, 13, 400, 286, 478, 50920], "temperature": 0.0, "avg_logprob": -0.08923378286435622, "compression_ratio": 1.86328125, "no_speech_prob": 0.0006461040466092527}, {"id": 1700, "seek": 883536, "start": 8846.480000000001, "end": 8851.76, "text": " going to continue asking them at nine and ten. I'm sorry. They understand that I want to know", "tokens": [50920, 516, 281, 2354, 3365, 552, 412, 4949, 293, 2064, 13, 286, 478, 2597, 13, 814, 1223, 300, 286, 528, 281, 458, 51184], "temperature": 0.0, "avg_logprob": -0.08923378286435622, "compression_ratio": 1.86328125, "no_speech_prob": 0.0006461040466092527}, {"id": 1701, "seek": 883536, "start": 8851.76, "end": 8855.12, "text": " their language. That's the point of the task is like I'm trying to learn their language. And so", "tokens": [51184, 641, 2856, 13, 663, 311, 264, 935, 295, 264, 5633, 307, 411, 286, 478, 1382, 281, 1466, 641, 2856, 13, 400, 370, 51352], "temperature": 0.0, "avg_logprob": -0.08923378286435622, "compression_ratio": 1.86328125, "no_speech_prob": 0.0006461040466092527}, {"id": 1702, "seek": 883536, "start": 8855.12, "end": 8861.12, "text": " that's okay. But it does seem like I'm a little slow because they already told me what the word", "tokens": [51352, 300, 311, 1392, 13, 583, 309, 775, 1643, 411, 286, 478, 257, 707, 2964, 570, 436, 1217, 1907, 385, 437, 264, 1349, 51652], "temperature": 0.0, "avg_logprob": -0.08923378286435622, "compression_ratio": 1.86328125, "no_speech_prob": 0.0006461040466092527}, {"id": 1703, "seek": 886112, "start": 8861.12, "end": 8865.84, "text": " for many was, five, six, seven. And I keep asking. So it's a little funny to do this task over and", "tokens": [50364, 337, 867, 390, 11, 1732, 11, 2309, 11, 3407, 13, 400, 286, 1066, 3365, 13, 407, 309, 311, 257, 707, 4074, 281, 360, 341, 5633, 670, 293, 50600], "temperature": 0.0, "avg_logprob": -0.13027475080417314, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0952901840209961}, {"id": 1704, "seek": 886112, "start": 8865.84, "end": 8871.12, "text": " over. We did this with a guy called... Dan was our translator. He's the only one who really speaks", "tokens": [50600, 670, 13, 492, 630, 341, 365, 257, 2146, 1219, 485, 3394, 390, 527, 35223, 13, 634, 311, 264, 787, 472, 567, 534, 10789, 50864], "temperature": 0.0, "avg_logprob": -0.13027475080417314, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0952901840209961}, {"id": 1705, "seek": 886112, "start": 8871.12, "end": 8879.2, "text": " Piazza fluently. He's a good bilingual for a bunch of languages, but also English and Piazza.", "tokens": [50864, 430, 654, 26786, 5029, 2276, 13, 634, 311, 257, 665, 48757, 337, 257, 3840, 295, 8650, 11, 457, 611, 3669, 293, 430, 654, 26786, 13, 51268], "temperature": 0.0, "avg_logprob": -0.13027475080417314, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0952901840209961}, {"id": 1706, "seek": 886112, "start": 8879.2, "end": 8883.52, "text": " And then a guy called Mike Frank was also a student with me down there. He and I did these", "tokens": [51268, 400, 550, 257, 2146, 1219, 6602, 6823, 390, 611, 257, 3107, 365, 385, 760, 456, 13, 634, 293, 286, 630, 613, 51484], "temperature": 0.0, "avg_logprob": -0.13027475080417314, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0952901840209961}, {"id": 1707, "seek": 886112, "start": 8883.52, "end": 8890.560000000001, "text": " things. And so you do that. Okay. And everyone does the same thing. We asked like 10 people.", "tokens": [51484, 721, 13, 400, 370, 291, 360, 300, 13, 1033, 13, 400, 1518, 775, 264, 912, 551, 13, 492, 2351, 411, 1266, 561, 13, 51836], "temperature": 0.0, "avg_logprob": -0.13027475080417314, "compression_ratio": 1.621160409556314, "no_speech_prob": 0.0952901840209961}, {"id": 1708, "seek": 889056, "start": 8890.56, "end": 8895.039999999999, "text": " And they all do exactly the same labeling for one up. And then we just do the same thing", "tokens": [50364, 400, 436, 439, 360, 2293, 264, 912, 40244, 337, 472, 493, 13, 400, 550, 321, 445, 360, 264, 912, 551, 50588], "temperature": 0.0, "avg_logprob": -0.11105768054935104, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.004069425165653229}, {"id": 1709, "seek": 889056, "start": 8895.039999999999, "end": 8899.039999999999, "text": " down on random order. Actually, we do some of them up, some of them down first. Okay.", "tokens": [50588, 760, 322, 4974, 1668, 13, 5135, 11, 321, 360, 512, 295, 552, 493, 11, 512, 295, 552, 760, 700, 13, 1033, 13, 50788], "temperature": 0.0, "avg_logprob": -0.11105768054935104, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.004069425165653229}, {"id": 1710, "seek": 889056, "start": 8899.039999999999, "end": 8903.6, "text": " And so we do, instead of one to 10, we do 10 down to one. And so I give them 10,", "tokens": [50788, 400, 370, 321, 360, 11, 2602, 295, 472, 281, 1266, 11, 321, 360, 1266, 760, 281, 472, 13, 400, 370, 286, 976, 552, 1266, 11, 51016], "temperature": 0.0, "avg_logprob": -0.11105768054935104, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.004069425165653229}, {"id": 1711, "seek": 889056, "start": 8903.6, "end": 8908.96, "text": " nine, eight. They start saying the word for some. And then when you get to four,", "tokens": [51016, 4949, 11, 3180, 13, 814, 722, 1566, 264, 1349, 337, 512, 13, 400, 550, 562, 291, 483, 281, 1451, 11, 51284], "temperature": 0.0, "avg_logprob": -0.11105768054935104, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.004069425165653229}, {"id": 1712, "seek": 889056, "start": 8908.96, "end": 8914.96, "text": " everyone is saying the word for few, which we thought was one. So the context determined", "tokens": [51284, 1518, 307, 1566, 264, 1349, 337, 1326, 11, 597, 321, 1194, 390, 472, 13, 407, 264, 4319, 9540, 51584], "temperature": 0.0, "avg_logprob": -0.11105768054935104, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.004069425165653229}, {"id": 1713, "seek": 889056, "start": 8914.96, "end": 8920.16, "text": " what that quantifier they used was. So it's not a count word. They're not count words.", "tokens": [51584, 437, 300, 4426, 9902, 436, 1143, 390, 13, 407, 309, 311, 406, 257, 1207, 1349, 13, 814, 434, 406, 1207, 2283, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11105768054935104, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.004069425165653229}, {"id": 1714, "seek": 892016, "start": 8920.4, "end": 8923.52, "text": " They're just approximate words. And they're going to be noisy when you interview a bunch", "tokens": [50376, 814, 434, 445, 30874, 2283, 13, 400, 436, 434, 516, 281, 312, 24518, 562, 291, 4049, 257, 3840, 50532], "temperature": 0.0, "avg_logprob": -0.17346278429031373, "compression_ratio": 1.8204334365325077, "no_speech_prob": 0.0005883339326828718}, {"id": 1715, "seek": 892016, "start": 8923.52, "end": 8927.36, "text": " of people, what the definition of few, and there's going to be a threshold in the context.", "tokens": [50532, 295, 561, 11, 437, 264, 7123, 295, 1326, 11, 293, 456, 311, 516, 281, 312, 257, 14678, 294, 264, 4319, 13, 50724], "temperature": 0.0, "avg_logprob": -0.17346278429031373, "compression_ratio": 1.8204334365325077, "no_speech_prob": 0.0005883339326828718}, {"id": 1716, "seek": 892016, "start": 8928.16, "end": 8931.039999999999, "text": " Yeah, I don't know what that means. That's going to be 10 on the context. I think it's", "tokens": [50764, 865, 11, 286, 500, 380, 458, 437, 300, 1355, 13, 663, 311, 516, 281, 312, 1266, 322, 264, 4319, 13, 286, 519, 309, 311, 50908], "temperature": 0.0, "avg_logprob": -0.17346278429031373, "compression_ratio": 1.8204334365325077, "no_speech_prob": 0.0005883339326828718}, {"id": 1717, "seek": 892016, "start": 8931.039999999999, "end": 8933.84, "text": " true in English too, right? If you ask an English person what a few is, I mean,", "tokens": [50908, 2074, 294, 3669, 886, 11, 558, 30, 759, 291, 1029, 364, 3669, 954, 437, 257, 1326, 307, 11, 286, 914, 11, 51048], "temperature": 0.0, "avg_logprob": -0.17346278429031373, "compression_ratio": 1.8204334365325077, "no_speech_prob": 0.0005883339326828718}, {"id": 1718, "seek": 892016, "start": 8933.84, "end": 8937.76, "text": " that's going to depend completely on the context. And it might actually be at first", "tokens": [51048, 300, 311, 516, 281, 5672, 2584, 322, 264, 4319, 13, 400, 309, 1062, 767, 312, 412, 700, 51244], "temperature": 0.0, "avg_logprob": -0.17346278429031373, "compression_ratio": 1.8204334365325077, "no_speech_prob": 0.0005883339326828718}, {"id": 1719, "seek": 892016, "start": 8937.76, "end": 8942.88, "text": " hard to discover. Because for a lot of people, the jump from one to two will be few.", "tokens": [51244, 1152, 281, 4411, 13, 1436, 337, 257, 688, 295, 561, 11, 264, 3012, 490, 472, 281, 732, 486, 312, 1326, 13, 51500], "temperature": 0.0, "avg_logprob": -0.17346278429031373, "compression_ratio": 1.8204334365325077, "no_speech_prob": 0.0005883339326828718}, {"id": 1720, "seek": 892016, "start": 8943.68, "end": 8946.64, "text": " Right? So it's a jump. Yeah, it might be. It might still be there. Yeah.", "tokens": [51540, 1779, 30, 407, 309, 311, 257, 3012, 13, 865, 11, 309, 1062, 312, 13, 467, 1062, 920, 312, 456, 13, 865, 13, 51688], "temperature": 0.0, "avg_logprob": -0.17346278429031373, "compression_ratio": 1.8204334365325077, "no_speech_prob": 0.0005883339326828718}, {"id": 1721, "seek": 894664, "start": 8947.599999999999, "end": 8951.119999999999, "text": " I mean, that's fascinating. That's fascinating that numbers don't present themselves.", "tokens": [50412, 286, 914, 11, 300, 311, 10343, 13, 663, 311, 10343, 300, 3547, 500, 380, 1974, 2969, 13, 50588], "temperature": 0.0, "avg_logprob": -0.12684988403320313, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.002251487225294113}, {"id": 1722, "seek": 894664, "start": 8951.119999999999, "end": 8954.4, "text": " Yeah. So the words aren't there. And then, and so then we do these other things. Well,", "tokens": [50588, 865, 13, 407, 264, 2283, 3212, 380, 456, 13, 400, 550, 11, 293, 370, 550, 321, 360, 613, 661, 721, 13, 1042, 11, 50752], "temperature": 0.0, "avg_logprob": -0.12684988403320313, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.002251487225294113}, {"id": 1723, "seek": 894664, "start": 8954.4, "end": 8960.64, "text": " if they don't have the words, can they do exact matching kinds of tasks? Can they even do those", "tokens": [50752, 498, 436, 500, 380, 362, 264, 2283, 11, 393, 436, 360, 1900, 14324, 3685, 295, 9608, 30, 1664, 436, 754, 360, 729, 51064], "temperature": 0.0, "avg_logprob": -0.12684988403320313, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.002251487225294113}, {"id": 1724, "seek": 894664, "start": 8960.64, "end": 8967.599999999999, "text": " tasks? And the answer is sort of yes and no. And so yes, they can do them. So here's the tasks", "tokens": [51064, 9608, 30, 400, 264, 1867, 307, 1333, 295, 2086, 293, 572, 13, 400, 370, 2086, 11, 436, 393, 360, 552, 13, 407, 510, 311, 264, 9608, 51412], "temperature": 0.0, "avg_logprob": -0.12684988403320313, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.002251487225294113}, {"id": 1725, "seek": 894664, "start": 8967.599999999999, "end": 8972.32, "text": " that we did. We put out those spools of thread again. Okay. So maybe I put like three out here.", "tokens": [51412, 300, 321, 630, 13, 492, 829, 484, 729, 637, 29298, 295, 7207, 797, 13, 1033, 13, 407, 1310, 286, 829, 411, 1045, 484, 510, 13, 51648], "temperature": 0.0, "avg_logprob": -0.12684988403320313, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.002251487225294113}, {"id": 1726, "seek": 897232, "start": 8972.32, "end": 8977.76, "text": " And then we gave them some objects. And those happen to be uninflated red balloons. It doesn't", "tokens": [50364, 400, 550, 321, 2729, 552, 512, 6565, 13, 400, 729, 1051, 281, 312, 517, 27850, 770, 2182, 26193, 13, 467, 1177, 380, 50636], "temperature": 0.0, "avg_logprob": -0.08617617797851562, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0032726831268519163}, {"id": 1727, "seek": 897232, "start": 8977.76, "end": 8982.4, "text": " really matter what they are. It's just a bunch of exactly the same thing. And it was easy to", "tokens": [50636, 534, 1871, 437, 436, 366, 13, 467, 311, 445, 257, 3840, 295, 2293, 264, 912, 551, 13, 400, 309, 390, 1858, 281, 50868], "temperature": 0.0, "avg_logprob": -0.08617617797851562, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0032726831268519163}, {"id": 1728, "seek": 897232, "start": 8982.4, "end": 8988.32, "text": " put down right next to these spools of thread. Okay. And so then I put out three of these.", "tokens": [50868, 829, 760, 558, 958, 281, 613, 637, 29298, 295, 7207, 13, 1033, 13, 400, 370, 550, 286, 829, 484, 1045, 295, 613, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08617617797851562, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0032726831268519163}, {"id": 1729, "seek": 897232, "start": 8988.32, "end": 8994.08, "text": " And your task was to just put one against each of my three things. And they could do that perfectly.", "tokens": [51164, 400, 428, 5633, 390, 281, 445, 829, 472, 1970, 1184, 295, 452, 1045, 721, 13, 400, 436, 727, 360, 300, 6239, 13, 51452], "temperature": 0.0, "avg_logprob": -0.08617617797851562, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0032726831268519163}, {"id": 1730, "seek": 897232, "start": 8994.08, "end": 8997.84, "text": " So I mean, I would actually do that. It was a very easy task to explain to them because I have,", "tokens": [51452, 407, 286, 914, 11, 286, 576, 767, 360, 300, 13, 467, 390, 257, 588, 1858, 5633, 281, 2903, 281, 552, 570, 286, 362, 11, 51640], "temperature": 0.0, "avg_logprob": -0.08617617797851562, "compression_ratio": 1.6964285714285714, "no_speech_prob": 0.0032726831268519163}, {"id": 1731, "seek": 899784, "start": 8997.84, "end": 9003.04, "text": " I did this with this guy, Mike Frank, and he would be my, I'd be the experimenter telling him to do", "tokens": [50364, 286, 630, 341, 365, 341, 2146, 11, 6602, 6823, 11, 293, 415, 576, 312, 452, 11, 286, 1116, 312, 264, 5120, 260, 3585, 796, 281, 360, 50624], "temperature": 0.0, "avg_logprob": -0.11896834344220308, "compression_ratio": 1.7750759878419453, "no_speech_prob": 0.01542170625180006}, {"id": 1732, "seek": 899784, "start": 9003.04, "end": 9007.2, "text": " this and showing him to do this. And then we just like, just do what he did. You'll copy him. All we", "tokens": [50624, 341, 293, 4099, 796, 281, 360, 341, 13, 400, 550, 321, 445, 411, 11, 445, 360, 437, 415, 630, 13, 509, 603, 5055, 796, 13, 1057, 321, 50832], "temperature": 0.0, "avg_logprob": -0.11896834344220308, "compression_ratio": 1.7750759878419453, "no_speech_prob": 0.01542170625180006}, {"id": 1733, "seek": 899784, "start": 9007.2, "end": 9012.16, "text": " had to, I didn't have to speak to him, except for know what copy him, like do what he did is like", "tokens": [50832, 632, 281, 11, 286, 994, 380, 362, 281, 1710, 281, 796, 11, 3993, 337, 458, 437, 5055, 796, 11, 411, 360, 437, 415, 630, 307, 411, 51080], "temperature": 0.0, "avg_logprob": -0.11896834344220308, "compression_ratio": 1.7750759878419453, "no_speech_prob": 0.01542170625180006}, {"id": 1734, "seek": 899784, "start": 9012.16, "end": 9016.16, "text": " all we had to be able to say. And then they would do that just perfectly. And it's always", "tokens": [51080, 439, 321, 632, 281, 312, 1075, 281, 584, 13, 400, 550, 436, 576, 360, 300, 445, 6239, 13, 400, 309, 311, 1009, 51280], "temperature": 0.0, "avg_logprob": -0.11896834344220308, "compression_ratio": 1.7750759878419453, "no_speech_prob": 0.01542170625180006}, {"id": 1735, "seek": 899784, "start": 9016.16, "end": 9022.16, "text": " moving up. We do some sort of random number of items up to 10. And they basically do perfectly", "tokens": [51280, 2684, 493, 13, 492, 360, 512, 1333, 295, 4974, 1230, 295, 4754, 493, 281, 1266, 13, 400, 436, 1936, 360, 6239, 51580], "temperature": 0.0, "avg_logprob": -0.11896834344220308, "compression_ratio": 1.7750759878419453, "no_speech_prob": 0.01542170625180006}, {"id": 1736, "seek": 899784, "start": 9022.16, "end": 9026.16, "text": " on that. They never get that wrong. I mean, that's not a counting task, right? That is just a match.", "tokens": [51580, 322, 300, 13, 814, 1128, 483, 300, 2085, 13, 286, 914, 11, 300, 311, 406, 257, 13251, 5633, 11, 558, 30, 663, 307, 445, 257, 2995, 13, 51780], "temperature": 0.0, "avg_logprob": -0.11896834344220308, "compression_ratio": 1.7750759878419453, "no_speech_prob": 0.01542170625180006}, {"id": 1737, "seek": 902616, "start": 9026.16, "end": 9029.28, "text": " You just put one against them. It doesn't matter how many, I don't need to know how many there are", "tokens": [50364, 509, 445, 829, 472, 1970, 552, 13, 467, 1177, 380, 1871, 577, 867, 11, 286, 500, 380, 643, 281, 458, 577, 867, 456, 366, 50520], "temperature": 0.0, "avg_logprob": -0.12526045217142476, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.006096863187849522}, {"id": 1738, "seek": 902616, "start": 9029.28, "end": 9035.52, "text": " there to do that correctly. And they would make mistakes, but very, very few and no more than", "tokens": [50520, 456, 281, 360, 300, 8944, 13, 400, 436, 576, 652, 8038, 11, 457, 588, 11, 588, 1326, 293, 572, 544, 813, 50832], "temperature": 0.0, "avg_logprob": -0.12526045217142476, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.006096863187849522}, {"id": 1739, "seek": 902616, "start": 9035.52, "end": 9040.88, "text": " MIT undergrads. Just going to say, like there's no, these are low stakes. So, you know, you make", "tokens": [50832, 13100, 833, 861, 5834, 13, 1449, 516, 281, 584, 11, 411, 456, 311, 572, 11, 613, 366, 2295, 28429, 13, 407, 11, 291, 458, 11, 291, 652, 51100], "temperature": 0.0, "avg_logprob": -0.12526045217142476, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.006096863187849522}, {"id": 1740, "seek": 902616, "start": 9040.88, "end": 9044.64, "text": " mistakes. Counting is not required to complete the matching task. That's right. Not at all. Okay.", "tokens": [51100, 8038, 13, 5247, 278, 307, 406, 4739, 281, 3566, 264, 14324, 5633, 13, 663, 311, 558, 13, 1726, 412, 439, 13, 1033, 13, 51288], "temperature": 0.0, "avg_logprob": -0.12526045217142476, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.006096863187849522}, {"id": 1741, "seek": 902616, "start": 9044.64, "end": 9049.92, "text": " And so, and so that's our control. And this guy had gone down there before and said that they", "tokens": [51288, 400, 370, 11, 293, 370, 300, 311, 527, 1969, 13, 400, 341, 2146, 632, 2780, 760, 456, 949, 293, 848, 300, 436, 51552], "temperature": 0.0, "avg_logprob": -0.12526045217142476, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.006096863187849522}, {"id": 1742, "seek": 902616, "start": 9049.92, "end": 9053.6, "text": " couldn't do this task, but I just don't know what he did wrong there because they can do this task", "tokens": [51552, 2809, 380, 360, 341, 5633, 11, 457, 286, 445, 500, 380, 458, 437, 415, 630, 2085, 456, 570, 436, 393, 360, 341, 5633, 51736], "temperature": 0.0, "avg_logprob": -0.12526045217142476, "compression_ratio": 1.7791411042944785, "no_speech_prob": 0.006096863187849522}, {"id": 1743, "seek": 905360, "start": 9053.6, "end": 9058.16, "text": " perfectly well. And, you know, I can train my dog to do this task. So, of course, they can do this", "tokens": [50364, 6239, 731, 13, 400, 11, 291, 458, 11, 286, 393, 3847, 452, 3000, 281, 360, 341, 5633, 13, 407, 11, 295, 1164, 11, 436, 393, 360, 341, 50592], "temperature": 0.0, "avg_logprob": -0.1154102211567893, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.007814952172338963}, {"id": 1744, "seek": 905360, "start": 9058.16, "end": 9063.36, "text": " task. And so, you know, it's not a hard task. But the other task that was sort of more interesting", "tokens": [50592, 5633, 13, 400, 370, 11, 291, 458, 11, 309, 311, 406, 257, 1152, 5633, 13, 583, 264, 661, 5633, 300, 390, 1333, 295, 544, 1880, 50852], "temperature": 0.0, "avg_logprob": -0.1154102211567893, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.007814952172338963}, {"id": 1745, "seek": 905360, "start": 9063.36, "end": 9070.32, "text": " is like, so then we do a bunch of tasks where you need some way to encode the set.", "tokens": [50852, 307, 411, 11, 370, 550, 321, 360, 257, 3840, 295, 9608, 689, 291, 643, 512, 636, 281, 2058, 1429, 264, 992, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1154102211567893, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.007814952172338963}, {"id": 1746, "seek": 905360, "start": 9070.32, "end": 9078.880000000001, "text": " So, like one of them is just a, I just put a opaque sheet in front of the things I put down a", "tokens": [51200, 407, 11, 411, 472, 295, 552, 307, 445, 257, 11, 286, 445, 829, 257, 42687, 8193, 294, 1868, 295, 264, 721, 286, 829, 760, 257, 51628], "temperature": 0.0, "avg_logprob": -0.1154102211567893, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.007814952172338963}, {"id": 1747, "seek": 905360, "start": 9078.880000000001, "end": 9083.2, "text": " bunch of set of these things and I put an opaque sheet down. And so you can't see them anymore.", "tokens": [51628, 3840, 295, 992, 295, 613, 721, 293, 286, 829, 364, 42687, 8193, 760, 13, 400, 370, 291, 393, 380, 536, 552, 3602, 13, 51844], "temperature": 0.0, "avg_logprob": -0.1154102211567893, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.007814952172338963}, {"id": 1748, "seek": 908320, "start": 9083.2, "end": 9086.960000000001, "text": " And I tell you, do the same thing you were doing before, right? You know, and it's easy if it's", "tokens": [50364, 400, 286, 980, 291, 11, 360, 264, 912, 551, 291, 645, 884, 949, 11, 558, 30, 509, 458, 11, 293, 309, 311, 1858, 498, 309, 311, 50552], "temperature": 0.0, "avg_logprob": -0.12068893970587315, "compression_ratio": 1.9119718309859155, "no_speech_prob": 0.0006877706036902964}, {"id": 1749, "seek": 908320, "start": 9086.960000000001, "end": 9092.480000000001, "text": " two or three, it's very easy. But if I don't have the words for eight, it's a little harder, like,", "tokens": [50552, 732, 420, 1045, 11, 309, 311, 588, 1858, 13, 583, 498, 286, 500, 380, 362, 264, 2283, 337, 3180, 11, 309, 311, 257, 707, 6081, 11, 411, 11, 50828], "temperature": 0.0, "avg_logprob": -0.12068893970587315, "compression_ratio": 1.9119718309859155, "no_speech_prob": 0.0006877706036902964}, {"id": 1750, "seek": 908320, "start": 9092.480000000001, "end": 9098.320000000002, "text": " maybe, you know, with practice, well, no. Because you have to count. For us, it's easy", "tokens": [50828, 1310, 11, 291, 458, 11, 365, 3124, 11, 731, 11, 572, 13, 1436, 291, 362, 281, 1207, 13, 1171, 505, 11, 309, 311, 1858, 51120], "temperature": 0.0, "avg_logprob": -0.12068893970587315, "compression_ratio": 1.9119718309859155, "no_speech_prob": 0.0006877706036902964}, {"id": 1751, "seek": 908320, "start": 9098.320000000002, "end": 9102.560000000001, "text": " because we just, we just count them. It's just so easy to count them. But, but they don't,", "tokens": [51120, 570, 321, 445, 11, 321, 445, 1207, 552, 13, 467, 311, 445, 370, 1858, 281, 1207, 552, 13, 583, 11, 457, 436, 500, 380, 11, 51332], "temperature": 0.0, "avg_logprob": -0.12068893970587315, "compression_ratio": 1.9119718309859155, "no_speech_prob": 0.0006877706036902964}, {"id": 1752, "seek": 908320, "start": 9102.560000000001, "end": 9105.84, "text": " they can't count them because they don't count. They don't have words for this thing. And so,", "tokens": [51332, 436, 393, 380, 1207, 552, 570, 436, 500, 380, 1207, 13, 814, 500, 380, 362, 2283, 337, 341, 551, 13, 400, 370, 11, 51496], "temperature": 0.0, "avg_logprob": -0.12068893970587315, "compression_ratio": 1.9119718309859155, "no_speech_prob": 0.0006877706036902964}, {"id": 1753, "seek": 908320, "start": 9105.84, "end": 9108.800000000001, "text": " they would do approximate. It's totally fascinating. So, they would get them", "tokens": [51496, 436, 576, 360, 30874, 13, 467, 311, 3879, 10343, 13, 407, 11, 436, 576, 483, 552, 51644], "temperature": 0.0, "avg_logprob": -0.12068893970587315, "compression_ratio": 1.9119718309859155, "no_speech_prob": 0.0006877706036902964}, {"id": 1754, "seek": 910880, "start": 9108.8, "end": 9113.84, "text": " approximately right, you know, after four or five, you know, because you can,", "tokens": [50364, 10447, 558, 11, 291, 458, 11, 934, 1451, 420, 1732, 11, 291, 458, 11, 570, 291, 393, 11, 50616], "temperature": 0.0, "avg_logprob": -0.1319963606141454, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0031723121646791697}, {"id": 1755, "seek": 910880, "start": 9113.84, "end": 9118.0, "text": " basically, you always get four right, three or four, that looks, that's something we can", "tokens": [50616, 1936, 11, 291, 1009, 483, 1451, 558, 11, 1045, 420, 1451, 11, 300, 1542, 11, 300, 311, 746, 321, 393, 50824], "temperature": 0.0, "avg_logprob": -0.1319963606141454, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0031723121646791697}, {"id": 1756, "seek": 910880, "start": 9118.0, "end": 9122.64, "text": " visually see. But, but after that, you kind of have, it's an approximate number. And so,", "tokens": [50824, 19622, 536, 13, 583, 11, 457, 934, 300, 11, 291, 733, 295, 362, 11, 309, 311, 364, 30874, 1230, 13, 400, 370, 11, 51056], "temperature": 0.0, "avg_logprob": -0.1319963606141454, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0031723121646791697}, {"id": 1757, "seek": 910880, "start": 9122.64, "end": 9127.439999999999, "text": " then, and there's a bunch of tasks we did, and they all failed as, I mean, failed. They did", "tokens": [51056, 550, 11, 293, 456, 311, 257, 3840, 295, 9608, 321, 630, 11, 293, 436, 439, 7612, 382, 11, 286, 914, 11, 7612, 13, 814, 630, 51296], "temperature": 0.0, "avg_logprob": -0.1319963606141454, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0031723121646791697}, {"id": 1758, "seek": 910880, "start": 9127.439999999999, "end": 9132.32, "text": " approximate after five on all those tasks. And it kind of shows that the words,", "tokens": [51296, 30874, 934, 1732, 322, 439, 729, 9608, 13, 400, 309, 733, 295, 3110, 300, 264, 2283, 11, 51540], "temperature": 0.0, "avg_logprob": -0.1319963606141454, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0031723121646791697}, {"id": 1759, "seek": 910880, "start": 9133.599999999999, "end": 9137.279999999999, "text": " you kind of need the words, you know, to be able to do these, these kinds of tasks.", "tokens": [51604, 291, 733, 295, 643, 264, 2283, 11, 291, 458, 11, 281, 312, 1075, 281, 360, 613, 11, 613, 3685, 295, 9608, 13, 51788], "temperature": 0.0, "avg_logprob": -0.1319963606141454, "compression_ratio": 1.9210526315789473, "no_speech_prob": 0.0031723121646791697}, {"id": 1760, "seek": 913728, "start": 9137.28, "end": 9141.68, "text": " This is a little bit of a chicken and egg thing there. Because if you don't have the words,", "tokens": [50364, 639, 307, 257, 707, 857, 295, 257, 4662, 293, 3777, 551, 456, 13, 1436, 498, 291, 500, 380, 362, 264, 2283, 11, 50584], "temperature": 0.0, "avg_logprob": -0.10317779606224126, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.003374059684574604}, {"id": 1761, "seek": 913728, "start": 9142.720000000001, "end": 9148.0, "text": " then maybe they'll limit you in the kind of, like a little baby Einstein there,", "tokens": [50636, 550, 1310, 436, 603, 4948, 291, 294, 264, 733, 295, 11, 411, 257, 707, 3186, 23486, 456, 11, 50900], "temperature": 0.0, "avg_logprob": -0.10317779606224126, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.003374059684574604}, {"id": 1762, "seek": 913728, "start": 9148.720000000001, "end": 9153.84, "text": " won't be able to come up with a counting task. You know what I mean? Like, the ability to count", "tokens": [50936, 1582, 380, 312, 1075, 281, 808, 493, 365, 257, 13251, 5633, 13, 509, 458, 437, 286, 914, 30, 1743, 11, 264, 3485, 281, 1207, 51192], "temperature": 0.0, "avg_logprob": -0.10317779606224126, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.003374059684574604}, {"id": 1763, "seek": 913728, "start": 9153.84, "end": 9160.480000000001, "text": " enables you to come up with interesting things, probably. So, yes, you develop counting because", "tokens": [51192, 17077, 291, 281, 808, 493, 365, 1880, 721, 11, 1391, 13, 407, 11, 2086, 11, 291, 1499, 13251, 570, 51524], "temperature": 0.0, "avg_logprob": -0.10317779606224126, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.003374059684574604}, {"id": 1764, "seek": 913728, "start": 9160.480000000001, "end": 9165.28, "text": " you need it. But then, once you have counting, you can probably come up with a bunch of different", "tokens": [51524, 291, 643, 309, 13, 583, 550, 11, 1564, 291, 362, 13251, 11, 291, 393, 1391, 808, 493, 365, 257, 3840, 295, 819, 51764], "temperature": 0.0, "avg_logprob": -0.10317779606224126, "compression_ratio": 1.739622641509434, "no_speech_prob": 0.003374059684574604}, {"id": 1765, "seek": 916528, "start": 9165.28, "end": 9172.24, "text": " inventions, like how to, I don't know, what kind of thing they do matching really well for", "tokens": [50364, 43748, 11, 411, 577, 281, 11, 286, 500, 380, 458, 11, 437, 733, 295, 551, 436, 360, 14324, 534, 731, 337, 50712], "temperature": 0.0, "avg_logprob": -0.13711173789015094, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0028004031628370285}, {"id": 1766, "seek": 916528, "start": 9172.24, "end": 9178.0, "text": " building purposes, building some kind of hut or something like this. So, it's interesting that", "tokens": [50712, 2390, 9932, 11, 2390, 512, 733, 295, 36755, 420, 746, 411, 341, 13, 407, 11, 309, 311, 1880, 300, 51000], "temperature": 0.0, "avg_logprob": -0.13711173789015094, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0028004031628370285}, {"id": 1767, "seek": 916528, "start": 9178.0, "end": 9181.6, "text": " language is a, a limiter on what you're able to do.", "tokens": [51000, 2856, 307, 257, 11, 257, 2364, 1681, 322, 437, 291, 434, 1075, 281, 360, 13, 51180], "temperature": 0.0, "avg_logprob": -0.13711173789015094, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0028004031628370285}, {"id": 1768, "seek": 916528, "start": 9181.6, "end": 9187.04, "text": " Yeah, here's language is just, is the words, here is the words, like the words for exact count", "tokens": [51180, 865, 11, 510, 311, 2856, 307, 445, 11, 307, 264, 2283, 11, 510, 307, 264, 2283, 11, 411, 264, 2283, 337, 1900, 1207, 51452], "temperature": 0.0, "avg_logprob": -0.13711173789015094, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0028004031628370285}, {"id": 1769, "seek": 916528, "start": 9187.68, "end": 9190.560000000001, "text": " is the limiting factor here. They just don't have them.", "tokens": [51484, 307, 264, 22083, 5952, 510, 13, 814, 445, 500, 380, 362, 552, 13, 51628], "temperature": 0.0, "avg_logprob": -0.13711173789015094, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0028004031628370285}, {"id": 1770, "seek": 919056, "start": 9190.88, "end": 9198.24, "text": " Yeah. Well, that's what I mean. That limit is also a limit on the society of what they're able to", "tokens": [50380, 865, 13, 1042, 11, 300, 311, 437, 286, 914, 13, 663, 4948, 307, 611, 257, 4948, 322, 264, 4086, 295, 437, 436, 434, 1075, 281, 50748], "temperature": 0.0, "avg_logprob": -0.12325690638634466, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0033759032376110554}, {"id": 1771, "seek": 919056, "start": 9198.24, "end": 9203.359999999999, "text": " build. That's going to be true. Yeah. So, it's problem, I mean, we don't know, this is one of", "tokens": [50748, 1322, 13, 663, 311, 516, 281, 312, 2074, 13, 865, 13, 407, 11, 309, 311, 1154, 11, 286, 914, 11, 321, 500, 380, 458, 11, 341, 307, 472, 295, 51004], "temperature": 0.0, "avg_logprob": -0.12325690638634466, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0033759032376110554}, {"id": 1772, "seek": 919056, "start": 9203.359999999999, "end": 9207.84, "text": " those problems with the snapshot of just current languages is that we don't know what causes a", "tokens": [51004, 729, 2740, 365, 264, 30163, 295, 445, 2190, 8650, 307, 300, 321, 500, 380, 458, 437, 7700, 257, 51228], "temperature": 0.0, "avg_logprob": -0.12325690638634466, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0033759032376110554}, {"id": 1773, "seek": 919056, "start": 9207.84, "end": 9214.08, "text": " culture to discover slash invent accounting system. But the hypothesis is the guess out there is", "tokens": [51228, 3713, 281, 4411, 17330, 7962, 19163, 1185, 13, 583, 264, 17291, 307, 264, 2041, 484, 456, 307, 51540], "temperature": 0.0, "avg_logprob": -0.12325690638634466, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0033759032376110554}, {"id": 1774, "seek": 919056, "start": 9214.08, "end": 9220.08, "text": " something to do with farming. So, if you have a bunch of goats and you want to keep track of them,", "tokens": [51540, 746, 281, 360, 365, 16557, 13, 407, 11, 498, 291, 362, 257, 3840, 295, 34219, 293, 291, 528, 281, 1066, 2837, 295, 552, 11, 51840], "temperature": 0.0, "avg_logprob": -0.12325690638634466, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.0033759032376110554}, {"id": 1775, "seek": 922056, "start": 9220.56, "end": 9224.56, "text": " and you have saved 17 goats and you go to bed at night and you get up in the morning,", "tokens": [50364, 293, 291, 362, 6624, 3282, 34219, 293, 291, 352, 281, 2901, 412, 1818, 293, 291, 483, 493, 294, 264, 2446, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1333075136751742, "compression_ratio": 1.9106628242074928, "no_speech_prob": 0.0006461255252361298}, {"id": 1776, "seek": 922056, "start": 9224.56, "end": 9229.68, "text": " boy, it's easier to have a count system to do that. You know, that's an abstract abstraction", "tokens": [50564, 3237, 11, 309, 311, 3571, 281, 362, 257, 1207, 1185, 281, 360, 300, 13, 509, 458, 11, 300, 311, 364, 12649, 37765, 50820], "temperature": 0.0, "avg_logprob": -0.1333075136751742, "compression_ratio": 1.9106628242074928, "no_speech_prob": 0.0006461255252361298}, {"id": 1777, "seek": 922056, "start": 9229.68, "end": 9234.32, "text": " over a set. So, they don't have, like, people often ask me when I talk to them about this kind", "tokens": [50820, 670, 257, 992, 13, 407, 11, 436, 500, 380, 362, 11, 411, 11, 561, 2049, 1029, 385, 562, 286, 751, 281, 552, 466, 341, 733, 51052], "temperature": 0.0, "avg_logprob": -0.1333075136751742, "compression_ratio": 1.9106628242074928, "no_speech_prob": 0.0006461255252361298}, {"id": 1778, "seek": 922056, "start": 9234.32, "end": 9237.359999999999, "text": " of work, and they say, well, don't these, Peter, huh, don't they have kids? Don't they have a lot", "tokens": [51052, 295, 589, 11, 293, 436, 584, 11, 731, 11, 500, 380, 613, 11, 6508, 11, 7020, 11, 500, 380, 436, 362, 2301, 30, 1468, 380, 436, 362, 257, 688, 51204], "temperature": 0.0, "avg_logprob": -0.1333075136751742, "compression_ratio": 1.9106628242074928, "no_speech_prob": 0.0006461255252361298}, {"id": 1779, "seek": 922056, "start": 9237.359999999999, "end": 9241.279999999999, "text": " of children? I'm like, yeah, they have a lot of children. And they do, they often have families", "tokens": [51204, 295, 2227, 30, 286, 478, 411, 11, 1338, 11, 436, 362, 257, 688, 295, 2227, 13, 400, 436, 360, 11, 436, 2049, 362, 4466, 51400], "temperature": 0.0, "avg_logprob": -0.1333075136751742, "compression_ratio": 1.9106628242074928, "no_speech_prob": 0.0006461255252361298}, {"id": 1780, "seek": 922056, "start": 9241.279999999999, "end": 9245.119999999999, "text": " of three or four or five kids. And they go, well, don't they need the numbers to keep track of their", "tokens": [51400, 295, 1045, 420, 1451, 420, 1732, 2301, 13, 400, 436, 352, 11, 731, 11, 500, 380, 436, 643, 264, 3547, 281, 1066, 2837, 295, 641, 51592], "temperature": 0.0, "avg_logprob": -0.1333075136751742, "compression_ratio": 1.9106628242074928, "no_speech_prob": 0.0006461255252361298}, {"id": 1781, "seek": 922056, "start": 9245.119999999999, "end": 9250.16, "text": " kids? And I always ask the person who says this, like, do you have children? And the answer is", "tokens": [51592, 2301, 30, 400, 286, 1009, 1029, 264, 954, 567, 1619, 341, 11, 411, 11, 360, 291, 362, 2227, 30, 400, 264, 1867, 307, 51844], "temperature": 0.0, "avg_logprob": -0.1333075136751742, "compression_ratio": 1.9106628242074928, "no_speech_prob": 0.0006461255252361298}, {"id": 1782, "seek": 925016, "start": 9250.16, "end": 9255.039999999999, "text": " always no, because that's not how you keep track of your kids. You care about their identities.", "tokens": [50364, 1009, 572, 11, 570, 300, 311, 406, 577, 291, 1066, 2837, 295, 428, 2301, 13, 509, 1127, 466, 641, 24239, 13, 50608], "temperature": 0.0, "avg_logprob": -0.11999869010817836, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.007814948447048664}, {"id": 1783, "seek": 925016, "start": 9255.039999999999, "end": 9260.32, "text": " It's very important to me when I go, I think I have five children. It's, it's, it doesn't matter", "tokens": [50608, 467, 311, 588, 1021, 281, 385, 562, 286, 352, 11, 286, 519, 286, 362, 1732, 2227, 13, 467, 311, 11, 309, 311, 11, 309, 1177, 380, 1871, 50872], "temperature": 0.0, "avg_logprob": -0.11999869010817836, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.007814948447048664}, {"id": 1784, "seek": 925016, "start": 9260.32, "end": 9266.0, "text": " which, yeah, it matters which five. It's like, if you replaced one with someone else, I would,", "tokens": [50872, 597, 11, 1338, 11, 309, 7001, 597, 1732, 13, 467, 311, 411, 11, 498, 291, 10772, 472, 365, 1580, 1646, 11, 286, 576, 11, 51156], "temperature": 0.0, "avg_logprob": -0.11999869010817836, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.007814948447048664}, {"id": 1785, "seek": 925016, "start": 9266.0, "end": 9270.24, "text": " I would care. A goat maybe not, right? That's the kind of point. It's an abstraction,", "tokens": [51156, 286, 576, 1127, 13, 316, 23608, 1310, 406, 11, 558, 30, 663, 311, 264, 733, 295, 935, 13, 467, 311, 364, 37765, 11, 51368], "temperature": 0.0, "avg_logprob": -0.11999869010817836, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.007814948447048664}, {"id": 1786, "seek": 925016, "start": 9270.24, "end": 9273.76, "text": " something that looks very similar to the one wouldn't matter to me, probably.", "tokens": [51368, 746, 300, 1542, 588, 2531, 281, 264, 472, 2759, 380, 1871, 281, 385, 11, 1391, 13, 51544], "temperature": 0.0, "avg_logprob": -0.11999869010817836, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.007814948447048664}, {"id": 1787, "seek": 925016, "start": 9273.76, "end": 9277.76, "text": " But if you care about goats, you're going to know them actually individually also.", "tokens": [51544, 583, 498, 291, 1127, 466, 34219, 11, 291, 434, 516, 281, 458, 552, 767, 16652, 611, 13, 51744], "temperature": 0.0, "avg_logprob": -0.11999869010817836, "compression_ratio": 1.7337662337662338, "no_speech_prob": 0.007814948447048664}, {"id": 1788, "seek": 927776, "start": 9277.76, "end": 9281.28, "text": " Yeah, you will. I mean, cows and goats, if there's a source of food and milk and all that kind", "tokens": [50364, 865, 11, 291, 486, 13, 286, 914, 11, 19148, 293, 34219, 11, 498, 456, 311, 257, 4009, 295, 1755, 293, 5392, 293, 439, 300, 733, 50540], "temperature": 0.0, "avg_logprob": -0.1588966882050927, "compression_ratio": 1.6314199395770392, "no_speech_prob": 0.0020186337642371655}, {"id": 1789, "seek": 927776, "start": 9281.28, "end": 9284.56, "text": " of stuff, you're going to actually do it right there. But I'm saying it is an abstraction", "tokens": [50540, 295, 1507, 11, 291, 434, 516, 281, 767, 360, 309, 558, 456, 13, 583, 286, 478, 1566, 309, 307, 364, 37765, 50704], "temperature": 0.0, "avg_logprob": -0.1588966882050927, "compression_ratio": 1.6314199395770392, "no_speech_prob": 0.0020186337642371655}, {"id": 1790, "seek": 927776, "start": 9284.56, "end": 9288.4, "text": " such that you don't have to care about their identities to do this thing fast. That's,", "tokens": [50704, 1270, 300, 291, 500, 380, 362, 281, 1127, 466, 641, 24239, 281, 360, 341, 551, 2370, 13, 663, 311, 11, 50896], "temperature": 0.0, "avg_logprob": -0.1588966882050927, "compression_ratio": 1.6314199395770392, "no_speech_prob": 0.0020186337642371655}, {"id": 1791, "seek": 927776, "start": 9288.4, "end": 9293.68, "text": " that's the hypothesis, not mine. From anthropologists are guessing about where", "tokens": [50896, 300, 311, 264, 17291, 11, 406, 3892, 13, 3358, 22727, 12256, 366, 17939, 466, 689, 51160], "temperature": 0.0, "avg_logprob": -0.1588966882050927, "compression_ratio": 1.6314199395770392, "no_speech_prob": 0.0020186337642371655}, {"id": 1792, "seek": 927776, "start": 9293.68, "end": 9300.4, "text": " words for counting came from is from farming, maybe. Yeah. Do you have a sense why universal", "tokens": [51160, 2283, 337, 13251, 1361, 490, 307, 490, 16557, 11, 1310, 13, 865, 13, 1144, 291, 362, 257, 2020, 983, 11455, 51496], "temperature": 0.0, "avg_logprob": -0.1588966882050927, "compression_ratio": 1.6314199395770392, "no_speech_prob": 0.0020186337642371655}, {"id": 1793, "seek": 927776, "start": 9300.4, "end": 9307.44, "text": " languages like Esperanto have not taken off? Like, why do we have all these different languages?", "tokens": [51496, 8650, 411, 24142, 5857, 362, 406, 2726, 766, 30, 1743, 11, 983, 360, 321, 362, 439, 613, 819, 8650, 30, 51848], "temperature": 0.0, "avg_logprob": -0.1588966882050927, "compression_ratio": 1.6314199395770392, "no_speech_prob": 0.0020186337642371655}, {"id": 1794, "seek": 930776, "start": 9307.92, "end": 9314.64, "text": " Well, my guess is the function of a language is to do something in a community. And I mean,", "tokens": [50372, 1042, 11, 452, 2041, 307, 264, 2445, 295, 257, 2856, 307, 281, 360, 746, 294, 257, 1768, 13, 400, 286, 914, 11, 50708], "temperature": 0.0, "avg_logprob": -0.12135287254087386, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.000709408544935286}, {"id": 1795, "seek": 930776, "start": 9314.64, "end": 9318.880000000001, "text": " unless there's some function to that language in the community, it's not going to survive.", "tokens": [50708, 5969, 456, 311, 512, 2445, 281, 300, 2856, 294, 264, 1768, 11, 309, 311, 406, 516, 281, 7867, 13, 50920], "temperature": 0.0, "avg_logprob": -0.12135287254087386, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.000709408544935286}, {"id": 1796, "seek": 930776, "start": 9318.880000000001, "end": 9323.76, "text": " It's not going to be useful. So here's a great example. So what I'm like, language death is", "tokens": [50920, 467, 311, 406, 516, 281, 312, 4420, 13, 407, 510, 311, 257, 869, 1365, 13, 407, 437, 286, 478, 411, 11, 2856, 2966, 307, 51164], "temperature": 0.0, "avg_logprob": -0.12135287254087386, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.000709408544935286}, {"id": 1797, "seek": 930776, "start": 9323.76, "end": 9329.44, "text": " super common. Okay. Languages are dying all around the world. And here's why they're dying.", "tokens": [51164, 1687, 2689, 13, 1033, 13, 13313, 84, 1660, 366, 8639, 439, 926, 264, 1002, 13, 400, 510, 311, 983, 436, 434, 8639, 13, 51448], "temperature": 0.0, "avg_logprob": -0.12135287254087386, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.000709408544935286}, {"id": 1798, "seek": 930776, "start": 9329.44, "end": 9333.6, "text": " And it's like, yeah, I see this in, you know, it's not happening right now in either the", "tokens": [51448, 400, 309, 311, 411, 11, 1338, 11, 286, 536, 341, 294, 11, 291, 458, 11, 309, 311, 406, 2737, 558, 586, 294, 2139, 264, 51656], "temperature": 0.0, "avg_logprob": -0.12135287254087386, "compression_ratio": 1.7635658914728682, "no_speech_prob": 0.000709408544935286}, {"id": 1799, "seek": 933360, "start": 9333.6, "end": 9338.24, "text": " Chimane or the, or the Piedoha, but it probably will. And so there's a neighboring group called", "tokens": [50364, 761, 332, 1929, 420, 264, 11, 420, 264, 430, 1091, 1445, 64, 11, 457, 309, 1391, 486, 13, 400, 370, 456, 311, 257, 31521, 1594, 1219, 50596], "temperature": 0.0, "avg_logprob": -0.1817333915016868, "compression_ratio": 1.82421875, "no_speech_prob": 0.014259576797485352}, {"id": 1800, "seek": 933360, "start": 9338.24, "end": 9344.720000000001, "text": " Mostitan, which is, I said that it's a isolates. Actually, there's a dual. There's two of them.", "tokens": [50596, 4534, 9670, 11, 597, 307, 11, 286, 848, 300, 309, 311, 257, 7381, 1024, 13, 5135, 11, 456, 311, 257, 11848, 13, 821, 311, 732, 295, 552, 13, 50920], "temperature": 0.0, "avg_logprob": -0.1817333915016868, "compression_ratio": 1.82421875, "no_speech_prob": 0.014259576797485352}, {"id": 1801, "seek": 933360, "start": 9344.720000000001, "end": 9348.640000000001, "text": " Okay. So it's actually, there's two languages, which are really close, which are Mostitan", "tokens": [50920, 1033, 13, 407, 309, 311, 767, 11, 456, 311, 732, 8650, 11, 597, 366, 534, 1998, 11, 597, 366, 4534, 9670, 51116], "temperature": 0.0, "avg_logprob": -0.1817333915016868, "compression_ratio": 1.82421875, "no_speech_prob": 0.014259576797485352}, {"id": 1802, "seek": 933360, "start": 9348.640000000001, "end": 9355.04, "text": " and Chimane, which are unrelated to anything else. And Mostitan is unlike Chimane in that", "tokens": [51116, 293, 761, 332, 1929, 11, 597, 366, 38967, 281, 1340, 1646, 13, 400, 4534, 9670, 307, 8343, 761, 332, 1929, 294, 300, 51436], "temperature": 0.0, "avg_logprob": -0.1817333915016868, "compression_ratio": 1.82421875, "no_speech_prob": 0.014259576797485352}, {"id": 1803, "seek": 933360, "start": 9355.04, "end": 9360.16, "text": " it has a lot of contact with Spanish and it's dying. So that language is dying. The reason it's", "tokens": [51436, 309, 575, 257, 688, 295, 3385, 365, 8058, 293, 309, 311, 8639, 13, 407, 300, 2856, 307, 8639, 13, 440, 1778, 309, 311, 51692], "temperature": 0.0, "avg_logprob": -0.1817333915016868, "compression_ratio": 1.82421875, "no_speech_prob": 0.014259576797485352}, {"id": 1804, "seek": 936016, "start": 9360.16, "end": 9367.2, "text": " dying is there's not a lot of value for the local people in their native language. So there's much", "tokens": [50364, 8639, 307, 456, 311, 406, 257, 688, 295, 2158, 337, 264, 2654, 561, 294, 641, 8470, 2856, 13, 407, 456, 311, 709, 50716], "temperature": 0.0, "avg_logprob": -0.09224960327148438, "compression_ratio": 1.8992248062015504, "no_speech_prob": 0.026735562831163406}, {"id": 1805, "seek": 936016, "start": 9367.2, "end": 9372.16, "text": " more value in knowing Spanish, like because they want to feed their families. And how do you feed", "tokens": [50716, 544, 2158, 294, 5276, 8058, 11, 411, 570, 436, 528, 281, 3154, 641, 4466, 13, 400, 577, 360, 291, 3154, 50964], "temperature": 0.0, "avg_logprob": -0.09224960327148438, "compression_ratio": 1.8992248062015504, "no_speech_prob": 0.026735562831163406}, {"id": 1806, "seek": 936016, "start": 9372.16, "end": 9376.4, "text": " your family? You learn Spanish, so you can make money, so you can get a job and do these things.", "tokens": [50964, 428, 1605, 30, 509, 1466, 8058, 11, 370, 291, 393, 652, 1460, 11, 370, 291, 393, 483, 257, 1691, 293, 360, 613, 721, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09224960327148438, "compression_ratio": 1.8992248062015504, "no_speech_prob": 0.026735562831163406}, {"id": 1807, "seek": 936016, "start": 9376.4, "end": 9380.64, "text": " And then you can, and then you make money. And so they want Spanish things they want. And so,", "tokens": [51176, 400, 550, 291, 393, 11, 293, 550, 291, 652, 1460, 13, 400, 370, 436, 528, 8058, 721, 436, 528, 13, 400, 370, 11, 51388], "temperature": 0.0, "avg_logprob": -0.09224960327148438, "compression_ratio": 1.8992248062015504, "no_speech_prob": 0.026735562831163406}, {"id": 1808, "seek": 936016, "start": 9380.64, "end": 9387.039999999999, "text": " so Mostitan is in danger and is dying. And that's normal. And so basically the problem is that people,", "tokens": [51388, 370, 4534, 9670, 307, 294, 4330, 293, 307, 8639, 13, 400, 300, 311, 2710, 13, 400, 370, 1936, 264, 1154, 307, 300, 561, 11, 51708], "temperature": 0.0, "avg_logprob": -0.09224960327148438, "compression_ratio": 1.8992248062015504, "no_speech_prob": 0.026735562831163406}, {"id": 1809, "seek": 938704, "start": 9388.0, "end": 9395.44, "text": " the reason we learn languages to communicate, and we need to, we use it to make money and to", "tokens": [50412, 264, 1778, 321, 1466, 8650, 281, 7890, 11, 293, 321, 643, 281, 11, 321, 764, 309, 281, 652, 1460, 293, 281, 50784], "temperature": 0.0, "avg_logprob": -0.13091597557067872, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.019372526556253433}, {"id": 1810, "seek": 938704, "start": 9395.44, "end": 9402.640000000001, "text": " do whatever it is to feed our families. And if that's not happening, then it won't take off.", "tokens": [50784, 360, 2035, 309, 307, 281, 3154, 527, 4466, 13, 400, 498, 300, 311, 406, 2737, 11, 550, 309, 1582, 380, 747, 766, 13, 51144], "temperature": 0.0, "avg_logprob": -0.13091597557067872, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.019372526556253433}, {"id": 1811, "seek": 938704, "start": 9402.640000000001, "end": 9407.36, "text": " It's not like a game or something. This is like something we use. Like, why is English so popular?", "tokens": [51144, 467, 311, 406, 411, 257, 1216, 420, 746, 13, 639, 307, 411, 746, 321, 764, 13, 1743, 11, 983, 307, 3669, 370, 3743, 30, 51380], "temperature": 0.0, "avg_logprob": -0.13091597557067872, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.019372526556253433}, {"id": 1812, "seek": 938704, "start": 9407.36, "end": 9412.240000000002, "text": " It's not because it's an easy language to learn. Maybe it is. I don't really know.", "tokens": [51380, 467, 311, 406, 570, 309, 311, 364, 1858, 2856, 281, 1466, 13, 2704, 309, 307, 13, 286, 500, 380, 534, 458, 13, 51624], "temperature": 0.0, "avg_logprob": -0.13091597557067872, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.019372526556253433}, {"id": 1813, "seek": 941224, "start": 9413.039999999999, "end": 9417.36, "text": " But that's not why it's popular. But because the United States is a gigantic economy.", "tokens": [50404, 583, 300, 311, 406, 983, 309, 311, 3743, 13, 583, 570, 264, 2824, 3040, 307, 257, 26800, 5010, 13, 50620], "temperature": 0.0, "avg_logprob": -0.1301431655883789, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.028855280950665474}, {"id": 1814, "seek": 941224, "start": 9417.36, "end": 9422.16, "text": " Yeah. It's big economies that do this. It's all it is. It's all about money. And that's what,", "tokens": [50620, 865, 13, 467, 311, 955, 23158, 300, 360, 341, 13, 467, 311, 439, 309, 307, 13, 467, 311, 439, 466, 1460, 13, 400, 300, 311, 437, 11, 50860], "temperature": 0.0, "avg_logprob": -0.1301431655883789, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.028855280950665474}, {"id": 1815, "seek": 941224, "start": 9422.16, "end": 9426.96, "text": " and so there's a motivation to learn Mandarin. There's a motivation to learn Spanish. There's", "tokens": [50860, 293, 370, 456, 311, 257, 12335, 281, 1466, 42292, 13, 821, 311, 257, 12335, 281, 1466, 8058, 13, 821, 311, 51100], "temperature": 0.0, "avg_logprob": -0.1301431655883789, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.028855280950665474}, {"id": 1816, "seek": 941224, "start": 9426.96, "end": 9430.8, "text": " a motivation to learn English. These languages are very valuable to know because there's so,", "tokens": [51100, 257, 12335, 281, 1466, 3669, 13, 1981, 8650, 366, 588, 8263, 281, 458, 570, 456, 311, 370, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1301431655883789, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.028855280950665474}, {"id": 1817, "seek": 941224, "start": 9430.8, "end": 9436.16, "text": " so many speakers all over the world. There's less of a value economically. It's like kind of what", "tokens": [51292, 370, 867, 9518, 439, 670, 264, 1002, 13, 821, 311, 1570, 295, 257, 2158, 26811, 13, 467, 311, 411, 733, 295, 437, 51560], "temperature": 0.0, "avg_logprob": -0.1301431655883789, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.028855280950665474}, {"id": 1818, "seek": 941224, "start": 9436.16, "end": 9440.72, "text": " drives this. It's not a, but it's not a, you know, it's not just for fun. I mean, there are these", "tokens": [51560, 11754, 341, 13, 467, 311, 406, 257, 11, 457, 309, 311, 406, 257, 11, 291, 458, 11, 309, 311, 406, 445, 337, 1019, 13, 286, 914, 11, 456, 366, 613, 51788], "temperature": 0.0, "avg_logprob": -0.1301431655883789, "compression_ratio": 1.918088737201365, "no_speech_prob": 0.028855280950665474}, {"id": 1819, "seek": 944072, "start": 9440.72, "end": 9445.76, "text": " groups that do want to learn language just for language's sake. And then there's something,", "tokens": [50364, 3935, 300, 360, 528, 281, 1466, 2856, 445, 337, 2856, 311, 9717, 13, 400, 550, 456, 311, 746, 11, 50616], "temperature": 0.0, "avg_logprob": -0.1554708142652579, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.010003289207816124}, {"id": 1820, "seek": 944072, "start": 9445.76, "end": 9450.72, "text": " you know, to that, but those are rare. Those are rarities in general. Those are a few small", "tokens": [50616, 291, 458, 11, 281, 300, 11, 457, 729, 366, 5892, 13, 3950, 366, 367, 289, 1088, 294, 2674, 13, 3950, 366, 257, 1326, 1359, 50864], "temperature": 0.0, "avg_logprob": -0.1554708142652579, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.010003289207816124}, {"id": 1821, "seek": 944072, "start": 9450.72, "end": 9454.24, "text": " groups that do that. Not most people don't do that. Well, if that was the primary driver,", "tokens": [50864, 3935, 300, 360, 300, 13, 1726, 881, 561, 500, 380, 360, 300, 13, 1042, 11, 498, 300, 390, 264, 6194, 6787, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1554708142652579, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.010003289207816124}, {"id": 1822, "seek": 944072, "start": 9454.24, "end": 9458.4, "text": " then everybody was speaking English or speaking one language. There's also a tension.", "tokens": [51040, 550, 2201, 390, 4124, 3669, 420, 4124, 472, 2856, 13, 821, 311, 611, 257, 8980, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1554708142652579, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.010003289207816124}, {"id": 1823, "seek": 944072, "start": 9458.4, "end": 9459.279999999999, "text": " That's happening.", "tokens": [51248, 663, 311, 2737, 13, 51292], "temperature": 0.0, "avg_logprob": -0.1554708142652579, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.010003289207816124}, {"id": 1824, "seek": 944072, "start": 9459.279999999999, "end": 9462.88, "text": " And that, well, we're moving towards fewer and fewer languages.", "tokens": [51292, 400, 300, 11, 731, 11, 321, 434, 2684, 3030, 13366, 293, 13366, 8650, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1554708142652579, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.010003289207816124}, {"id": 1825, "seek": 944072, "start": 9462.88, "end": 9468.4, "text": " We are. I wonder, you're right. Maybe, maybe, you know, this is slow, but maybe that's where", "tokens": [51472, 492, 366, 13, 286, 2441, 11, 291, 434, 558, 13, 2704, 11, 1310, 11, 291, 458, 11, 341, 307, 2964, 11, 457, 1310, 300, 311, 689, 51748], "temperature": 0.0, "avg_logprob": -0.1554708142652579, "compression_ratio": 1.8225255972696246, "no_speech_prob": 0.010003289207816124}, {"id": 1826, "seek": 946840, "start": 9468.4, "end": 9474.88, "text": " we're moving. But there is a tension. You're saying a language that the fringes. But if you", "tokens": [50364, 321, 434, 2684, 13, 583, 456, 307, 257, 8980, 13, 509, 434, 1566, 257, 2856, 300, 264, 431, 278, 279, 13, 583, 498, 291, 50688], "temperature": 0.0, "avg_logprob": -0.14765253320204474, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.018215395510196686}, {"id": 1827, "seek": 946840, "start": 9475.44, "end": 9480.08, "text": " look at geopolitics and superpowers, it does seem that there's another thing in tension,", "tokens": [50716, 574, 412, 46615, 1167, 293, 1687, 47953, 11, 309, 775, 1643, 300, 456, 311, 1071, 551, 294, 8980, 11, 50948], "temperature": 0.0, "avg_logprob": -0.14765253320204474, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.018215395510196686}, {"id": 1828, "seek": 946840, "start": 9480.08, "end": 9485.92, "text": " which is a language is a national identity sometimes. For a certain nation. I mean,", "tokens": [50948, 597, 307, 257, 2856, 307, 257, 4048, 6575, 2171, 13, 1171, 257, 1629, 4790, 13, 286, 914, 11, 51240], "temperature": 0.0, "avg_logprob": -0.14765253320204474, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.018215395510196686}, {"id": 1829, "seek": 946840, "start": 9485.92, "end": 9491.279999999999, "text": " that's the, the war in Ukraine, language, Ukrainian language is a symbol of that war", "tokens": [51240, 300, 311, 264, 11, 264, 1516, 294, 14081, 11, 2856, 11, 24682, 2856, 307, 257, 5986, 295, 300, 1516, 51508], "temperature": 0.0, "avg_logprob": -0.14765253320204474, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.018215395510196686}, {"id": 1830, "seek": 946840, "start": 9491.84, "end": 9496.8, "text": " in many ways, like a country fighting for its own identity. So it's not merely the convenience.", "tokens": [51536, 294, 867, 2098, 11, 411, 257, 1941, 5237, 337, 1080, 1065, 6575, 13, 407, 309, 311, 406, 17003, 264, 19283, 13, 51784], "temperature": 0.0, "avg_logprob": -0.14765253320204474, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.018215395510196686}, {"id": 1831, "seek": 949680, "start": 9496.8, "end": 9501.759999999998, "text": " I mean, those two things that are a tension is the, the convenience of trade and the economics", "tokens": [50364, 286, 914, 11, 729, 732, 721, 300, 366, 257, 8980, 307, 264, 11, 264, 19283, 295, 4923, 293, 264, 14564, 50612], "temperature": 0.0, "avg_logprob": -0.12303395217724061, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0027114138938486576}, {"id": 1832, "seek": 949680, "start": 9501.759999999998, "end": 9507.519999999999, "text": " and be able to communicate with neighboring countries and trade more efficiently with", "tokens": [50612, 293, 312, 1075, 281, 7890, 365, 31521, 3517, 293, 4923, 544, 19621, 365, 50900], "temperature": 0.0, "avg_logprob": -0.12303395217724061, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0027114138938486576}, {"id": 1833, "seek": 949680, "start": 9507.519999999999, "end": 9511.359999999999, "text": " neighboring countries, all that kind of stuff, but also identity of the group.", "tokens": [50900, 31521, 3517, 11, 439, 300, 733, 295, 1507, 11, 457, 611, 6575, 295, 264, 1594, 13, 51092], "temperature": 0.0, "avg_logprob": -0.12303395217724061, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0027114138938486576}, {"id": 1834, "seek": 949680, "start": 9511.359999999999, "end": 9512.64, "text": " That's right. I completely agree.", "tokens": [51092, 663, 311, 558, 13, 286, 2584, 3986, 13, 51156], "temperature": 0.0, "avg_logprob": -0.12303395217724061, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0027114138938486576}, {"id": 1835, "seek": 949680, "start": 9512.64, "end": 9520.4, "text": " Because language is the way, for every community, like dialects that emerge are a kind of identity", "tokens": [51156, 1436, 2856, 307, 264, 636, 11, 337, 633, 1768, 11, 411, 24652, 82, 300, 21511, 366, 257, 733, 295, 6575, 51544], "temperature": 0.0, "avg_logprob": -0.12303395217724061, "compression_ratio": 1.7043478260869565, "no_speech_prob": 0.0027114138938486576}, {"id": 1836, "seek": 952040, "start": 9520.4, "end": 9526.72, "text": " for people. And sometimes a way for people to say FU to the more powerful people.", "tokens": [50364, 337, 561, 13, 400, 2171, 257, 636, 337, 561, 281, 584, 479, 52, 281, 264, 544, 4005, 561, 13, 50680], "temperature": 0.0, "avg_logprob": -0.15219356201507234, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.09126412868499756}, {"id": 1837, "seek": 952040, "start": 9527.359999999999, "end": 9531.119999999999, "text": " That's interesting. So in that way, language can't be used as that tool.", "tokens": [50712, 663, 311, 1880, 13, 407, 294, 300, 636, 11, 2856, 393, 380, 312, 1143, 382, 300, 2290, 13, 50900], "temperature": 0.0, "avg_logprob": -0.15219356201507234, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.09126412868499756}, {"id": 1838, "seek": 952040, "start": 9531.119999999999, "end": 9537.199999999999, "text": " Yeah. I completely agree. And there's a lot of work to try to create that identity. So people", "tokens": [50900, 865, 13, 286, 2584, 3986, 13, 400, 456, 311, 257, 688, 295, 589, 281, 853, 281, 1884, 300, 6575, 13, 407, 561, 51204], "temperature": 0.0, "avg_logprob": -0.15219356201507234, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.09126412868499756}, {"id": 1839, "seek": 952040, "start": 9537.199999999999, "end": 9544.72, "text": " want to do that. Speak, you know, as a cognitive scientist and language expert, I hope that continues", "tokens": [51204, 528, 281, 360, 300, 13, 27868, 11, 291, 458, 11, 382, 257, 15605, 12662, 293, 2856, 5844, 11, 286, 1454, 300, 6515, 51580], "temperature": 0.0, "avg_logprob": -0.15219356201507234, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.09126412868499756}, {"id": 1840, "seek": 954472, "start": 9544.72, "end": 9548.08, "text": " because I don't want languages to die. I want languages to survive because,", "tokens": [50364, 570, 286, 500, 380, 528, 8650, 281, 978, 13, 286, 528, 8650, 281, 7867, 570, 11, 50532], "temperature": 0.0, "avg_logprob": -0.0970148479237276, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.022954372689127922}, {"id": 1841, "seek": 954472, "start": 9550.08, "end": 9555.92, "text": " because they're so interesting for, for so many reasons. But I mean, I find them fascinating", "tokens": [50632, 570, 436, 434, 370, 1880, 337, 11, 337, 370, 867, 4112, 13, 583, 286, 914, 11, 286, 915, 552, 10343, 50924], "temperature": 0.0, "avg_logprob": -0.0970148479237276, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.022954372689127922}, {"id": 1842, "seek": 954472, "start": 9555.92, "end": 9559.76, "text": " just for the language part, but I think they, you know, there's a lot of connections to culture", "tokens": [50924, 445, 337, 264, 2856, 644, 11, 457, 286, 519, 436, 11, 291, 458, 11, 456, 311, 257, 688, 295, 9271, 281, 3713, 51116], "temperature": 0.0, "avg_logprob": -0.0970148479237276, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.022954372689127922}, {"id": 1843, "seek": 954472, "start": 9559.76, "end": 9565.439999999999, "text": " as well, which is also very important. Do you have hope for machine translation", "tokens": [51116, 382, 731, 11, 597, 307, 611, 588, 1021, 13, 1144, 291, 362, 1454, 337, 3479, 12853, 51400], "temperature": 0.0, "avg_logprob": -0.0970148479237276, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.022954372689127922}, {"id": 1844, "seek": 954472, "start": 9565.439999999999, "end": 9569.679999999998, "text": " that can break down the barriers of language? So while all these different diverse languages", "tokens": [51400, 300, 393, 1821, 760, 264, 13565, 295, 2856, 30, 407, 1339, 439, 613, 819, 9521, 8650, 51612], "temperature": 0.0, "avg_logprob": -0.0970148479237276, "compression_ratio": 1.661596958174905, "no_speech_prob": 0.022954372689127922}, {"id": 1845, "seek": 956968, "start": 9569.68, "end": 9576.800000000001, "text": " exist, I guess there's many ways of asking this question, but basically how hard is it to translate", "tokens": [50364, 2514, 11, 286, 2041, 456, 311, 867, 2098, 295, 3365, 341, 1168, 11, 457, 1936, 577, 1152, 307, 309, 281, 13799, 50720], "temperature": 0.0, "avg_logprob": -0.11844466496439814, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.035656653344631195}, {"id": 1846, "seek": 956968, "start": 9577.68, "end": 9580.24, "text": " in an automated way from one language to another?", "tokens": [50764, 294, 364, 18473, 636, 490, 472, 2856, 281, 1071, 30, 50892], "temperature": 0.0, "avg_logprob": -0.11844466496439814, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.035656653344631195}, {"id": 1847, "seek": 956968, "start": 9580.24, "end": 9584.64, "text": " There's, there's going to be cases where it's going to be really hard, right? So there are concepts", "tokens": [50892, 821, 311, 11, 456, 311, 516, 281, 312, 3331, 689, 309, 311, 516, 281, 312, 534, 1152, 11, 558, 30, 407, 456, 366, 10392, 51112], "temperature": 0.0, "avg_logprob": -0.11844466496439814, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.035656653344631195}, {"id": 1848, "seek": 956968, "start": 9585.36, "end": 9589.92, "text": " that are in one language and not in another. Like the most extreme kinds of cases are these", "tokens": [51148, 300, 366, 294, 472, 2856, 293, 406, 294, 1071, 13, 1743, 264, 881, 8084, 3685, 295, 3331, 366, 613, 51376], "temperature": 0.0, "avg_logprob": -0.11844466496439814, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.035656653344631195}, {"id": 1849, "seek": 956968, "start": 9589.92, "end": 9595.12, "text": " cases of number information. So exactly, like good luck translating a lot of English into", "tokens": [51376, 3331, 295, 1230, 1589, 13, 407, 2293, 11, 411, 665, 3668, 35030, 257, 688, 295, 3669, 666, 51636], "temperature": 0.0, "avg_logprob": -0.11844466496439814, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.035656653344631195}, {"id": 1850, "seek": 959512, "start": 9595.68, "end": 9599.84, "text": " it's just impossible. There's no way to do it because there are no words for these concepts", "tokens": [50392, 309, 311, 445, 6243, 13, 821, 311, 572, 636, 281, 360, 309, 570, 456, 366, 572, 2283, 337, 613, 10392, 50600], "temperature": 0.0, "avg_logprob": -0.08525756265030426, "compression_ratio": 1.8717105263157894, "no_speech_prob": 0.03619731217622757}, {"id": 1851, "seek": 959512, "start": 9599.84, "end": 9605.04, "text": " that we're talking about. There's probably the flip side, right? There's probably stuff in", "tokens": [50600, 300, 321, 434, 1417, 466, 13, 821, 311, 1391, 264, 7929, 1252, 11, 558, 30, 821, 311, 1391, 1507, 294, 50860], "temperature": 0.0, "avg_logprob": -0.08525756265030426, "compression_ratio": 1.8717105263157894, "no_speech_prob": 0.03619731217622757}, {"id": 1852, "seek": 959512, "start": 9606.400000000001, "end": 9610.480000000001, "text": " which is going to be hard to translate into English on the other side. And so I just don't", "tokens": [50928, 597, 307, 516, 281, 312, 1152, 281, 13799, 666, 3669, 322, 264, 661, 1252, 13, 400, 370, 286, 445, 500, 380, 51132], "temperature": 0.0, "avg_logprob": -0.08525756265030426, "compression_ratio": 1.8717105263157894, "no_speech_prob": 0.03619731217622757}, {"id": 1853, "seek": 959512, "start": 9610.480000000001, "end": 9615.68, "text": " know what those concepts are. I mean, you know, the space, the world space is a little, is different", "tokens": [51132, 458, 437, 729, 10392, 366, 13, 286, 914, 11, 291, 458, 11, 264, 1901, 11, 264, 1002, 1901, 307, 257, 707, 11, 307, 819, 51392], "temperature": 0.0, "avg_logprob": -0.08525756265030426, "compression_ratio": 1.8717105263157894, "no_speech_prob": 0.03619731217622757}, {"id": 1854, "seek": 959512, "start": 9615.68, "end": 9619.44, "text": " from my world space. And so I don't know what like, so that the things they talk about things are,", "tokens": [51392, 490, 452, 1002, 1901, 13, 400, 370, 286, 500, 380, 458, 437, 411, 11, 370, 300, 264, 721, 436, 751, 466, 721, 366, 11, 51580], "temperature": 0.0, "avg_logprob": -0.08525756265030426, "compression_ratio": 1.8717105263157894, "no_speech_prob": 0.03619731217622757}, {"id": 1855, "seek": 959512, "start": 9620.08, "end": 9624.320000000002, "text": " you know, it's going to have to do with their life as opposed to, you know, my industrial life,", "tokens": [51612, 291, 458, 11, 309, 311, 516, 281, 362, 281, 360, 365, 641, 993, 382, 8851, 281, 11, 291, 458, 11, 452, 9987, 993, 11, 51824], "temperature": 0.0, "avg_logprob": -0.08525756265030426, "compression_ratio": 1.8717105263157894, "no_speech_prob": 0.03619731217622757}, {"id": 1856, "seek": 962432, "start": 9624.32, "end": 9628.56, "text": " which is going to be different. And so there's going to be problems like that always.", "tokens": [50364, 597, 307, 516, 281, 312, 819, 13, 400, 370, 456, 311, 516, 281, 312, 2740, 411, 300, 1009, 13, 50576], "temperature": 0.0, "avg_logprob": -0.08979788859179065, "compression_ratio": 1.9893617021276595, "no_speech_prob": 0.0018670456483960152}, {"id": 1857, "seek": 962432, "start": 9629.279999999999, "end": 9632.56, "text": " You know, there's like, it's not, maybe it's not so bad in the case of some of these spaces,", "tokens": [50612, 509, 458, 11, 456, 311, 411, 11, 309, 311, 406, 11, 1310, 309, 311, 406, 370, 1578, 294, 264, 1389, 295, 512, 295, 613, 7673, 11, 50776], "temperature": 0.0, "avg_logprob": -0.08979788859179065, "compression_ratio": 1.9893617021276595, "no_speech_prob": 0.0018670456483960152}, {"id": 1858, "seek": 962432, "start": 9632.56, "end": 9637.039999999999, "text": " and maybe it's going to be harder than others. And so it's pretty bad in number. It's like,", "tokens": [50776, 293, 1310, 309, 311, 516, 281, 312, 6081, 813, 2357, 13, 400, 370, 309, 311, 1238, 1578, 294, 1230, 13, 467, 311, 411, 11, 51000], "temperature": 0.0, "avg_logprob": -0.08979788859179065, "compression_ratio": 1.9893617021276595, "no_speech_prob": 0.0018670456483960152}, {"id": 1859, "seek": 962432, "start": 9637.039999999999, "end": 9641.52, "text": " you know, extreme, I'd say, in the number space, you know, exact number space, but in the color", "tokens": [51000, 291, 458, 11, 8084, 11, 286, 1116, 584, 11, 294, 264, 1230, 1901, 11, 291, 458, 11, 1900, 1230, 1901, 11, 457, 294, 264, 2017, 51224], "temperature": 0.0, "avg_logprob": -0.08979788859179065, "compression_ratio": 1.9893617021276595, "no_speech_prob": 0.0018670456483960152}, {"id": 1860, "seek": 962432, "start": 9641.52, "end": 9646.64, "text": " dimension, right? So that's not so bad. There's, I mean, but it's a problem that, that you don't", "tokens": [51224, 10139, 11, 558, 30, 407, 300, 311, 406, 370, 1578, 13, 821, 311, 11, 286, 914, 11, 457, 309, 311, 257, 1154, 300, 11, 300, 291, 500, 380, 51480], "temperature": 0.0, "avg_logprob": -0.08979788859179065, "compression_ratio": 1.9893617021276595, "no_speech_prob": 0.0018670456483960152}, {"id": 1861, "seek": 962432, "start": 9646.64, "end": 9652.16, "text": " have ways to talk about the concepts. There might be entire concepts that are missing. So to you,", "tokens": [51480, 362, 2098, 281, 751, 466, 264, 10392, 13, 821, 1062, 312, 2302, 10392, 300, 366, 5361, 13, 407, 281, 291, 11, 51756], "temperature": 0.0, "avg_logprob": -0.08979788859179065, "compression_ratio": 1.9893617021276595, "no_speech_prob": 0.0018670456483960152}, {"id": 1862, "seek": 965216, "start": 9652.16, "end": 9658.24, "text": " it's more about the space of concept versus the space of form. Like form, you can probably map.", "tokens": [50364, 309, 311, 544, 466, 264, 1901, 295, 3410, 5717, 264, 1901, 295, 1254, 13, 1743, 1254, 11, 291, 393, 1391, 4471, 13, 50668], "temperature": 0.0, "avg_logprob": -0.17134885286030016, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.0018672568257898092}, {"id": 1863, "seek": 965216, "start": 9658.24, "end": 9664.24, "text": " Yes. Yeah. But so you were talking earlier about translation and about how translations,", "tokens": [50668, 1079, 13, 865, 13, 583, 370, 291, 645, 1417, 3071, 466, 12853, 293, 466, 577, 37578, 11, 50968], "temperature": 0.0, "avg_logprob": -0.17134885286030016, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.0018672568257898092}, {"id": 1864, "seek": 965216, "start": 9664.96, "end": 9668.24, "text": " you know, there's good and bad translations. I mean, now we're talking about translations of", "tokens": [51004, 291, 458, 11, 456, 311, 665, 293, 1578, 37578, 13, 286, 914, 11, 586, 321, 434, 1417, 466, 37578, 295, 51168], "temperature": 0.0, "avg_logprob": -0.17134885286030016, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.0018672568257898092}, {"id": 1865, "seek": 965216, "start": 9668.24, "end": 9676.08, "text": " form, right? So what makes a writing good, right? It's not just the content. It's, you know,", "tokens": [51168, 1254, 11, 558, 30, 407, 437, 1669, 257, 3579, 665, 11, 558, 30, 467, 311, 406, 445, 264, 2701, 13, 467, 311, 11, 291, 458, 11, 51560], "temperature": 0.0, "avg_logprob": -0.17134885286030016, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.0018672568257898092}, {"id": 1866, "seek": 967608, "start": 9676.08, "end": 9680.16, "text": " it's how it's written. And translating that, I, you know, I, you know, that's,", "tokens": [50364, 309, 311, 577, 309, 311, 3720, 13, 400, 35030, 300, 11, 286, 11, 291, 458, 11, 286, 11, 291, 458, 11, 300, 311, 11, 50568], "temperature": 0.0, "avg_logprob": -0.20858696050811232, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.11108129471540451}, {"id": 1867, "seek": 967608, "start": 9680.16, "end": 9686.48, "text": " that sounds difficult. We should, we should say that there is like, I don't, it has a day to say", "tokens": [50568, 300, 3263, 2252, 13, 492, 820, 11, 321, 820, 584, 300, 456, 307, 411, 11, 286, 500, 380, 11, 309, 575, 257, 786, 281, 584, 50884], "temperature": 0.0, "avg_logprob": -0.20858696050811232, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.11108129471540451}, {"id": 1868, "seek": 967608, "start": 9686.48, "end": 9692.64, "text": " meaning, but there's a music and a rhythm to the form. When you look at the broad picture, like", "tokens": [50884, 3620, 11, 457, 456, 311, 257, 1318, 293, 257, 11801, 281, 264, 1254, 13, 1133, 291, 574, 412, 264, 4152, 3036, 11, 411, 51192], "temperature": 0.0, "avg_logprob": -0.20858696050811232, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.11108129471540451}, {"id": 1869, "seek": 967608, "start": 9692.64, "end": 9700.88, "text": " the Fritz Wietzi and Dostoyevsky and Tolstoy, or Hemingway Bukowski, James Joyce, like I mentioned,", "tokens": [51192, 264, 1526, 6862, 343, 1684, 3992, 293, 413, 555, 939, 13379, 25810, 293, 21402, 372, 939, 11, 420, 18568, 278, 676, 363, 2034, 21866, 11, 5678, 40044, 11, 411, 286, 2835, 11, 51604], "temperature": 0.0, "avg_logprob": -0.20858696050811232, "compression_ratio": 1.578723404255319, "no_speech_prob": 0.11108129471540451}, {"id": 1870, "seek": 970088, "start": 9700.88, "end": 9704.8, "text": " there's a beat to it. There's an edge to it that it's like, is in the form.", "tokens": [50364, 456, 311, 257, 4224, 281, 309, 13, 821, 311, 364, 4691, 281, 309, 300, 309, 311, 411, 11, 307, 294, 264, 1254, 13, 50560], "temperature": 0.0, "avg_logprob": -0.158762556607606, "compression_ratio": 1.8067226890756303, "no_speech_prob": 0.006096947006881237}, {"id": 1871, "seek": 970088, "start": 9706.0, "end": 9711.759999999998, "text": " We can probably get measures of those. Yeah. I don't know. I'm optimistic that we could get", "tokens": [50620, 492, 393, 1391, 483, 8000, 295, 729, 13, 865, 13, 286, 500, 380, 458, 13, 286, 478, 19397, 300, 321, 727, 483, 50908], "temperature": 0.0, "avg_logprob": -0.158762556607606, "compression_ratio": 1.8067226890756303, "no_speech_prob": 0.006096947006881237}, {"id": 1872, "seek": 970088, "start": 9711.759999999998, "end": 9716.0, "text": " measures of those things. And so maybe that's translatable. I don't know. I don't know though.", "tokens": [50908, 8000, 295, 729, 721, 13, 400, 370, 1310, 300, 311, 5105, 31415, 13, 286, 500, 380, 458, 13, 286, 500, 380, 458, 1673, 13, 51120], "temperature": 0.0, "avg_logprob": -0.158762556607606, "compression_ratio": 1.8067226890756303, "no_speech_prob": 0.006096947006881237}, {"id": 1873, "seek": 970088, "start": 9716.0, "end": 9719.759999999998, "text": " I haven't worked on that. I would love to see. That sounds totally fascinating.", "tokens": [51120, 286, 2378, 380, 2732, 322, 300, 13, 286, 576, 959, 281, 536, 13, 663, 3263, 3879, 10343, 13, 51308], "temperature": 0.0, "avg_logprob": -0.158762556607606, "compression_ratio": 1.8067226890756303, "no_speech_prob": 0.006096947006881237}, {"id": 1874, "seek": 970088, "start": 9719.759999999998, "end": 9725.92, "text": " Translation to Hemingway. I mean, Hemingway is probably the lowest, I would love to see", "tokens": [51308, 6531, 24278, 281, 18568, 278, 676, 13, 286, 914, 11, 18568, 278, 676, 307, 1391, 264, 12437, 11, 286, 576, 959, 281, 536, 51616], "temperature": 0.0, "avg_logprob": -0.158762556607606, "compression_ratio": 1.8067226890756303, "no_speech_prob": 0.006096947006881237}, {"id": 1875, "seek": 972592, "start": 9725.92, "end": 9733.12, "text": " different authors, but the average per sentence dependency length for Hemingway is probably", "tokens": [50364, 819, 16552, 11, 457, 264, 4274, 680, 8174, 33621, 4641, 337, 18568, 278, 676, 307, 1391, 50724], "temperature": 0.0, "avg_logprob": -0.15475831712995256, "compression_ratio": 1.904109589041096, "no_speech_prob": 0.12243295460939407}, {"id": 1876, "seek": 972592, "start": 9733.12, "end": 9739.44, "text": " the shortest. That's your sense, huh? It's simple sentences with short, yeah, yeah, yeah, yeah.", "tokens": [50724, 264, 31875, 13, 663, 311, 428, 2020, 11, 7020, 30, 467, 311, 2199, 16579, 365, 2099, 11, 1338, 11, 1338, 11, 1338, 11, 1338, 13, 51040], "temperature": 0.0, "avg_logprob": -0.15475831712995256, "compression_ratio": 1.904109589041096, "no_speech_prob": 0.12243295460939407}, {"id": 1877, "seek": 972592, "start": 9739.44, "end": 9742.56, "text": " I mean, that's when, if you have really long sentences, even if they don't have center of", "tokens": [51040, 286, 914, 11, 300, 311, 562, 11, 498, 291, 362, 534, 938, 16579, 11, 754, 498, 436, 500, 380, 362, 3056, 295, 51196], "temperature": 0.0, "avg_logprob": -0.15475831712995256, "compression_ratio": 1.904109589041096, "no_speech_prob": 0.12243295460939407}, {"id": 1878, "seek": 972592, "start": 9742.56, "end": 9746.24, "text": " writing, like. They can have longer connections. Yeah. They can have longer connections. They", "tokens": [51196, 3579, 11, 411, 13, 814, 393, 362, 2854, 9271, 13, 865, 13, 814, 393, 362, 2854, 9271, 13, 814, 51380], "temperature": 0.0, "avg_logprob": -0.15475831712995256, "compression_ratio": 1.904109589041096, "no_speech_prob": 0.12243295460939407}, {"id": 1879, "seek": 972592, "start": 9746.24, "end": 9750.32, "text": " don't have to, right? You can have a long, long sentence with a bunch of local words. Yeah.", "tokens": [51380, 500, 380, 362, 281, 11, 558, 30, 509, 393, 362, 257, 938, 11, 938, 8174, 365, 257, 3840, 295, 2654, 2283, 13, 865, 13, 51584], "temperature": 0.0, "avg_logprob": -0.15475831712995256, "compression_ratio": 1.904109589041096, "no_speech_prob": 0.12243295460939407}, {"id": 1880, "seek": 972592, "start": 9750.32, "end": 9754.48, "text": " Yeah. But it's, but it is much more likely to have the possibility of long dependencies with", "tokens": [51584, 865, 13, 583, 309, 311, 11, 457, 309, 307, 709, 544, 3700, 281, 362, 264, 7959, 295, 938, 36606, 365, 51792], "temperature": 0.0, "avg_logprob": -0.15475831712995256, "compression_ratio": 1.904109589041096, "no_speech_prob": 0.12243295460939407}, {"id": 1881, "seek": 975448, "start": 9754.48, "end": 9761.84, "text": " long sentences. Yeah. I met a guy named Azar Askin who does a lot of cool stuff. Really", "tokens": [50364, 938, 16579, 13, 865, 13, 286, 1131, 257, 2146, 4926, 7607, 289, 1018, 5843, 567, 775, 257, 688, 295, 1627, 1507, 13, 4083, 50732], "temperature": 0.0, "avg_logprob": -0.14663259019242955, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.0012626753887161613}, {"id": 1882, "seek": 975448, "start": 9761.84, "end": 9767.039999999999, "text": " brilliant. He works with Tristan Harris and a bunch of stuff. But he was talking to me about", "tokens": [50732, 10248, 13, 634, 1985, 365, 1765, 7559, 17426, 293, 257, 3840, 295, 1507, 13, 583, 415, 390, 1417, 281, 385, 466, 50992], "temperature": 0.0, "avg_logprob": -0.14663259019242955, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.0012626753887161613}, {"id": 1883, "seek": 975448, "start": 9768.16, "end": 9774.4, "text": " communicating with animals. He co-founded Earth Species Project, where you're trying to find", "tokens": [51048, 17559, 365, 4882, 13, 634, 598, 12, 49547, 4755, 3550, 4629, 9849, 11, 689, 291, 434, 1382, 281, 915, 51360], "temperature": 0.0, "avg_logprob": -0.14663259019242955, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.0012626753887161613}, {"id": 1884, "seek": 975448, "start": 9774.4, "end": 9781.359999999999, "text": " the common language between whales, crows, and humans. And he was saying that there is a lot", "tokens": [51360, 264, 2689, 2856, 1296, 32403, 11, 941, 1509, 11, 293, 6255, 13, 400, 415, 390, 1566, 300, 456, 307, 257, 688, 51708], "temperature": 0.0, "avg_logprob": -0.14663259019242955, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.0012626753887161613}, {"id": 1885, "seek": 978136, "start": 9781.36, "end": 9786.560000000001, "text": " of promising work that even though the signals are very different. Right. Like the actual, like,", "tokens": [50364, 295, 20257, 589, 300, 754, 1673, 264, 12354, 366, 588, 819, 13, 1779, 13, 1743, 264, 3539, 11, 411, 11, 50624], "temperature": 0.0, "avg_logprob": -0.15111189875109443, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.058245886117219925}, {"id": 1886, "seek": 978136, "start": 9788.480000000001, "end": 9794.16, "text": " if you have embeddings of the languages, they're actually trying to communicate similar type things.", "tokens": [50720, 498, 291, 362, 12240, 29432, 295, 264, 8650, 11, 436, 434, 767, 1382, 281, 7890, 2531, 2010, 721, 13, 51004], "temperature": 0.0, "avg_logprob": -0.15111189875109443, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.058245886117219925}, {"id": 1887, "seek": 978136, "start": 9796.720000000001, "end": 9801.28, "text": " Is there something you can comment on that? Like where, is there promise to that? And everything", "tokens": [51132, 1119, 456, 746, 291, 393, 2871, 322, 300, 30, 1743, 689, 11, 307, 456, 6228, 281, 300, 30, 400, 1203, 51360], "temperature": 0.0, "avg_logprob": -0.15111189875109443, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.058245886117219925}, {"id": 1888, "seek": 978136, "start": 9801.28, "end": 9805.44, "text": " you've seen in different cultures, especially like remote cultures, that this is a possibility?", "tokens": [51360, 291, 600, 1612, 294, 819, 12951, 11, 2318, 411, 8607, 12951, 11, 300, 341, 307, 257, 7959, 30, 51568], "temperature": 0.0, "avg_logprob": -0.15111189875109443, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.058245886117219925}, {"id": 1889, "seek": 980544, "start": 9805.52, "end": 9812.560000000001, "text": " No. They can talk to whales. I would say yes. I think it's not crazy at all. I think it's quite", "tokens": [50368, 883, 13, 814, 393, 751, 281, 32403, 13, 286, 576, 584, 2086, 13, 286, 519, 309, 311, 406, 3219, 412, 439, 13, 286, 519, 309, 311, 1596, 50720], "temperature": 0.0, "avg_logprob": -0.12954606205584054, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.015901530161499977}, {"id": 1890, "seek": 980544, "start": 9812.560000000001, "end": 9820.32, "text": " reasonable. There's this sort of weird view, well, odd view, I think, that to think that human language", "tokens": [50720, 10585, 13, 821, 311, 341, 1333, 295, 3657, 1910, 11, 731, 11, 7401, 1910, 11, 286, 519, 11, 300, 281, 519, 300, 1952, 2856, 51108], "temperature": 0.0, "avg_logprob": -0.12954606205584054, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.015901530161499977}, {"id": 1891, "seek": 980544, "start": 9820.32, "end": 9827.6, "text": " is somehow special. I mean, it is, maybe it is. We can certainly do more than any of the other", "tokens": [51108, 307, 6063, 2121, 13, 286, 914, 11, 309, 307, 11, 1310, 309, 307, 13, 492, 393, 3297, 360, 544, 813, 604, 295, 264, 661, 51472], "temperature": 0.0, "avg_logprob": -0.12954606205584054, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.015901530161499977}, {"id": 1892, "seek": 982760, "start": 9827.68, "end": 9838.0, "text": " species. And maybe our language system is part of that. It's possible. But people do have often", "tokens": [50368, 6172, 13, 400, 1310, 527, 2856, 1185, 307, 644, 295, 300, 13, 467, 311, 1944, 13, 583, 561, 360, 362, 2049, 50884], "temperature": 0.0, "avg_logprob": -0.1485572702744428, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.09800560027360916}, {"id": 1893, "seek": 982760, "start": 9838.0, "end": 9844.48, "text": " talked about how human, like Chomsky, in fact, has talked about how human language has this", "tokens": [50884, 2825, 466, 577, 1952, 11, 411, 761, 4785, 4133, 11, 294, 1186, 11, 575, 2825, 466, 577, 1952, 2856, 575, 341, 51208], "temperature": 0.0, "avg_logprob": -0.1485572702744428, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.09800560027360916}, {"id": 1894, "seek": 982760, "start": 9846.0, "end": 9851.68, "text": " compositionality thing that he thinks is sort of key in language. And the problem with that", "tokens": [51284, 12686, 1860, 551, 300, 415, 7309, 307, 1333, 295, 2141, 294, 2856, 13, 400, 264, 1154, 365, 300, 51568], "temperature": 0.0, "avg_logprob": -0.1485572702744428, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.09800560027360916}, {"id": 1895, "seek": 985168, "start": 9851.68, "end": 9857.44, "text": " argument is he doesn't speak whale. And he doesn't speak crow, and he doesn't speak monkey.", "tokens": [50364, 6770, 307, 415, 1177, 380, 1710, 25370, 13, 400, 415, 1177, 380, 1710, 6401, 11, 293, 415, 1177, 380, 1710, 17847, 13, 50652], "temperature": 0.0, "avg_logprob": -0.11874056607484818, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.05499422177672386}, {"id": 1896, "seek": 985168, "start": 9858.48, "end": 9863.6, "text": " They say things like, well, they're making a bunch of grunts and squeaks. And the reasoning is like,", "tokens": [50704, 814, 584, 721, 411, 11, 731, 11, 436, 434, 1455, 257, 3840, 295, 677, 21253, 293, 8447, 5461, 13, 400, 264, 21577, 307, 411, 11, 50960], "temperature": 0.0, "avg_logprob": -0.11874056607484818, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.05499422177672386}, {"id": 1897, "seek": 985168, "start": 9864.16, "end": 9869.44, "text": " that's bad reasoning. I'm pretty sure if you asked a whale what we're saying, they'd say, well,", "tokens": [50988, 300, 311, 1578, 21577, 13, 286, 478, 1238, 988, 498, 291, 2351, 257, 25370, 437, 321, 434, 1566, 11, 436, 1116, 584, 11, 731, 11, 51252], "temperature": 0.0, "avg_logprob": -0.11874056607484818, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.05499422177672386}, {"id": 1898, "seek": 985168, "start": 9869.44, "end": 9875.36, "text": " I'm making a bunch of weird noises. Exactly. And so it's like, this is a very odd reasoning to", "tokens": [51252, 286, 478, 1455, 257, 3840, 295, 3657, 14620, 13, 7587, 13, 400, 370, 309, 311, 411, 11, 341, 307, 257, 588, 7401, 21577, 281, 51548], "temperature": 0.0, "avg_logprob": -0.11874056607484818, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.05499422177672386}, {"id": 1899, "seek": 985168, "start": 9875.36, "end": 9878.720000000001, "text": " be making that human language is special because we're the only ones who have human language. I'm", "tokens": [51548, 312, 1455, 300, 1952, 2856, 307, 2121, 570, 321, 434, 264, 787, 2306, 567, 362, 1952, 2856, 13, 286, 478, 51716], "temperature": 0.0, "avg_logprob": -0.11874056607484818, "compression_ratio": 1.8643410852713178, "no_speech_prob": 0.05499422177672386}, {"id": 1900, "seek": 987872, "start": 9879.359999999999, "end": 9885.439999999999, "text": " well, we don't know what those other, we just don't, we can't talk to them yet. And so there", "tokens": [50396, 731, 11, 321, 500, 380, 458, 437, 729, 661, 11, 321, 445, 500, 380, 11, 321, 393, 380, 751, 281, 552, 1939, 13, 400, 370, 456, 50700], "temperature": 0.0, "avg_logprob": -0.11727695636921101, "compression_ratio": 1.78515625, "no_speech_prob": 0.00132485490757972}, {"id": 1901, "seek": 987872, "start": 9885.439999999999, "end": 9891.279999999999, "text": " are probably a signal in there. And it might very well be something complicated, like human language.", "tokens": [50700, 366, 1391, 257, 6358, 294, 456, 13, 400, 309, 1062, 588, 731, 312, 746, 6179, 11, 411, 1952, 2856, 13, 50992], "temperature": 0.0, "avg_logprob": -0.11727695636921101, "compression_ratio": 1.78515625, "no_speech_prob": 0.00132485490757972}, {"id": 1902, "seek": 987872, "start": 9891.279999999999, "end": 9896.96, "text": " I mean, sure, with a small brain, in lower species, there's probably not a very good", "tokens": [50992, 286, 914, 11, 988, 11, 365, 257, 1359, 3567, 11, 294, 3126, 6172, 11, 456, 311, 1391, 406, 257, 588, 665, 51276], "temperature": 0.0, "avg_logprob": -0.11727695636921101, "compression_ratio": 1.78515625, "no_speech_prob": 0.00132485490757972}, {"id": 1903, "seek": 987872, "start": 9896.96, "end": 9901.84, "text": " communication system. But in these higher species where you have what seems to be", "tokens": [51276, 6101, 1185, 13, 583, 294, 613, 2946, 6172, 689, 291, 362, 437, 2544, 281, 312, 51520], "temperature": 0.0, "avg_logprob": -0.11727695636921101, "compression_ratio": 1.78515625, "no_speech_prob": 0.00132485490757972}, {"id": 1904, "seek": 987872, "start": 9903.199999999999, "end": 9908.08, "text": " abilities to communicate something, there might very well be a lot more signal there than we're", "tokens": [51588, 11582, 281, 7890, 746, 11, 456, 1062, 588, 731, 312, 257, 688, 544, 6358, 456, 813, 321, 434, 51832], "temperature": 0.0, "avg_logprob": -0.11727695636921101, "compression_ratio": 1.78515625, "no_speech_prob": 0.00132485490757972}, {"id": 1905, "seek": 990872, "start": 9909.279999999999, "end": 9912.88, "text": " than we might have otherwise thought. But, but also if we have a lot of intellectual", "tokens": [50392, 813, 321, 1062, 362, 5911, 1194, 13, 583, 11, 457, 611, 498, 321, 362, 257, 688, 295, 12576, 50572], "temperature": 0.0, "avg_logprob": -0.16048130937801894, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00034589023562148213}, {"id": 1906, "seek": 990872, "start": 9912.88, "end": 9917.279999999999, "text": " humility here, as somebody formerly from MIT, Neri Oxman, who I admire very much,", "tokens": [50572, 27106, 510, 11, 382, 2618, 34777, 490, 13100, 11, 426, 16310, 16489, 1601, 11, 567, 286, 21951, 588, 709, 11, 50792], "temperature": 0.0, "avg_logprob": -0.16048130937801894, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00034589023562148213}, {"id": 1907, "seek": 990872, "start": 9918.0, "end": 9926.96, "text": " has talked a lot about, has worked on communicating with plants. So like, yes, the signal there is", "tokens": [50828, 575, 2825, 257, 688, 466, 11, 575, 2732, 322, 17559, 365, 5972, 13, 407, 411, 11, 2086, 11, 264, 6358, 456, 307, 51276], "temperature": 0.0, "avg_logprob": -0.16048130937801894, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00034589023562148213}, {"id": 1908, "seek": 990872, "start": 9926.96, "end": 9933.679999999998, "text": " even less than we're like, it's not out of the realm of possibility that all nature has a way of", "tokens": [51276, 754, 1570, 813, 321, 434, 411, 11, 309, 311, 406, 484, 295, 264, 15355, 295, 7959, 300, 439, 3687, 575, 257, 636, 295, 51612], "temperature": 0.0, "avg_logprob": -0.16048130937801894, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00034589023562148213}, {"id": 1909, "seek": 993368, "start": 9933.68, "end": 9939.44, "text": " communicating. And it's a very different language, but they do develop a kind of language through", "tokens": [50364, 17559, 13, 400, 309, 311, 257, 588, 819, 2856, 11, 457, 436, 360, 1499, 257, 733, 295, 2856, 807, 50652], "temperature": 0.0, "avg_logprob": -0.07349015985216413, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.08256623893976212}, {"id": 1910, "seek": 993368, "start": 9939.44, "end": 9945.2, "text": " the chemistry, through some way of communicating with each other. And if you have enough humility", "tokens": [50652, 264, 12558, 11, 807, 512, 636, 295, 17559, 365, 1184, 661, 13, 400, 498, 291, 362, 1547, 27106, 50940], "temperature": 0.0, "avg_logprob": -0.07349015985216413, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.08256623893976212}, {"id": 1911, "seek": 993368, "start": 9945.2, "end": 9950.720000000001, "text": " about that possibility, I think you can, I think it would be a very interesting in a few decades,", "tokens": [50940, 466, 300, 7959, 11, 286, 519, 291, 393, 11, 286, 519, 309, 576, 312, 257, 588, 1880, 294, 257, 1326, 7878, 11, 51216], "temperature": 0.0, "avg_logprob": -0.07349015985216413, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.08256623893976212}, {"id": 1912, "seek": 993368, "start": 9951.52, "end": 9957.52, "text": " maybe centuries, hopefully not a humbling possibility of being able to communicate not", "tokens": [51256, 1310, 13926, 11, 4696, 406, 257, 1484, 18262, 7959, 295, 885, 1075, 281, 7890, 406, 51556], "temperature": 0.0, "avg_logprob": -0.07349015985216413, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.08256623893976212}, {"id": 1913, "seek": 995752, "start": 9957.52, "end": 9962.880000000001, "text": " just between humans effectively, but between all of living things on earth.", "tokens": [50364, 445, 1296, 6255, 8659, 11, 457, 1296, 439, 295, 2647, 721, 322, 4120, 13, 50632], "temperature": 0.0, "avg_logprob": -0.14373176748102362, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.16001470386981964}, {"id": 1914, "seek": 995752, "start": 9964.0, "end": 9966.880000000001, "text": " Well, I mean, I think some of them are not going to have much interesting to say,", "tokens": [50688, 1042, 11, 286, 914, 11, 286, 519, 512, 295, 552, 366, 406, 516, 281, 362, 709, 1880, 281, 584, 11, 50832], "temperature": 0.0, "avg_logprob": -0.14373176748102362, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.16001470386981964}, {"id": 1915, "seek": 995752, "start": 9968.32, "end": 9974.24, "text": " we don't know. We certainly don't know. I think if we're humble, there could be some", "tokens": [50904, 321, 500, 380, 458, 13, 492, 3297, 500, 380, 458, 13, 286, 519, 498, 321, 434, 16735, 11, 456, 727, 312, 512, 51200], "temperature": 0.0, "avg_logprob": -0.14373176748102362, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.16001470386981964}, {"id": 1916, "seek": 995752, "start": 9974.24, "end": 9979.68, "text": " interesting trees out there. Well, they're probably talking to other trees, right? They're not talking", "tokens": [51200, 1880, 5852, 484, 456, 13, 1042, 11, 436, 434, 1391, 1417, 281, 661, 5852, 11, 558, 30, 814, 434, 406, 1417, 51472], "temperature": 0.0, "avg_logprob": -0.14373176748102362, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.16001470386981964}, {"id": 1917, "seek": 995752, "start": 9979.68, "end": 9984.16, "text": " to us. And so to the extent they're talking, they're saying something interesting to some other,", "tokens": [51472, 281, 505, 13, 400, 370, 281, 264, 8396, 436, 434, 1417, 11, 436, 434, 1566, 746, 1880, 281, 512, 661, 11, 51696], "temperature": 0.0, "avg_logprob": -0.14373176748102362, "compression_ratio": 1.8416666666666666, "no_speech_prob": 0.16001470386981964}, {"id": 1918, "seek": 998416, "start": 9984.8, "end": 9990.0, "text": " you know, conspecific as opposed to us, right? And so there probably is, there may be some signal", "tokens": [50396, 291, 458, 11, 1014, 494, 22856, 382, 8851, 281, 505, 11, 558, 30, 400, 370, 456, 1391, 307, 11, 456, 815, 312, 512, 6358, 50656], "temperature": 0.0, "avg_logprob": -0.13015247673116703, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0012840824201703072}, {"id": 1919, "seek": 998416, "start": 9990.0, "end": 9996.56, "text": " there. So there are people out there, actually it's pretty common to say that human language", "tokens": [50656, 456, 13, 407, 456, 366, 561, 484, 456, 11, 767, 309, 311, 1238, 2689, 281, 584, 300, 1952, 2856, 50984], "temperature": 0.0, "avg_logprob": -0.13015247673116703, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0012840824201703072}, {"id": 1920, "seek": 998416, "start": 9996.56, "end": 10002.96, "text": " is special and different from any other animal communication system. And I just don't think", "tokens": [50984, 307, 2121, 293, 819, 490, 604, 661, 5496, 6101, 1185, 13, 400, 286, 445, 500, 380, 519, 51304], "temperature": 0.0, "avg_logprob": -0.13015247673116703, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0012840824201703072}, {"id": 1921, "seek": 998416, "start": 10002.96, "end": 10010.88, "text": " the evidence is there for that claim. I think it's not obvious. We just don't know, because we", "tokens": [51304, 264, 4467, 307, 456, 337, 300, 3932, 13, 286, 519, 309, 311, 406, 6322, 13, 492, 445, 500, 380, 458, 11, 570, 321, 51700], "temperature": 0.0, "avg_logprob": -0.13015247673116703, "compression_ratio": 1.6042553191489362, "no_speech_prob": 0.0012840824201703072}, {"id": 1922, "seek": 1001088, "start": 10010.88, "end": 10016.72, "text": " don't speak these other communication systems until we get better. You know, I do think there's,", "tokens": [50364, 500, 380, 1710, 613, 661, 6101, 3652, 1826, 321, 483, 1101, 13, 509, 458, 11, 286, 360, 519, 456, 311, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1274171534592543, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.001700453576631844}, {"id": 1923, "seek": 1001088, "start": 10016.72, "end": 10019.519999999999, "text": " there are people working on that, as you pointed out, though, people working on", "tokens": [50656, 456, 366, 561, 1364, 322, 300, 11, 382, 291, 10932, 484, 11, 1673, 11, 561, 1364, 322, 50796], "temperature": 0.0, "avg_logprob": -0.1274171534592543, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.001700453576631844}, {"id": 1924, "seek": 1001088, "start": 10019.519999999999, "end": 10022.0, "text": " whale speak, for instance, like that's really fascinating.", "tokens": [50796, 25370, 1710, 11, 337, 5197, 11, 411, 300, 311, 534, 10343, 13, 50920], "temperature": 0.0, "avg_logprob": -0.1274171534592543, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.001700453576631844}, {"id": 1925, "seek": 1001088, "start": 10022.0, "end": 10027.439999999999, "text": " Let me ask you a wild out there sci-fi question. If we make contact with an intelligent alien", "tokens": [50920, 961, 385, 1029, 291, 257, 4868, 484, 456, 2180, 12, 13325, 1168, 13, 759, 321, 652, 3385, 365, 364, 13232, 12319, 51192], "temperature": 0.0, "avg_logprob": -0.1274171534592543, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.001700453576631844}, {"id": 1926, "seek": 1001088, "start": 10027.439999999999, "end": 10035.039999999999, "text": " civilization and you get to meet them, how hard do you think, like how surprised would you be", "tokens": [51192, 18036, 293, 291, 483, 281, 1677, 552, 11, 577, 1152, 360, 291, 519, 11, 411, 577, 6100, 576, 291, 312, 51572], "temperature": 0.0, "avg_logprob": -0.1274171534592543, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.001700453576631844}, {"id": 1927, "seek": 1001088, "start": 10035.039999999999, "end": 10040.56, "text": " about their way of communicating? Do you think it would be recognizable? Maybe there's some", "tokens": [51572, 466, 641, 636, 295, 17559, 30, 1144, 291, 519, 309, 576, 312, 40757, 30, 2704, 456, 311, 512, 51848], "temperature": 0.0, "avg_logprob": -0.1274171534592543, "compression_ratio": 1.694078947368421, "no_speech_prob": 0.001700453576631844}, {"id": 1928, "seek": 1004056, "start": 10040.56, "end": 10045.76, "text": " parallels here when you go to the remote tribes. I mean, I would want Dan Everett with me. He is", "tokens": [50364, 44223, 510, 562, 291, 352, 281, 264, 8607, 19035, 13, 286, 914, 11, 286, 576, 528, 3394, 12123, 3093, 365, 385, 13, 634, 307, 50624], "temperature": 0.0, "avg_logprob": -0.1341012420654297, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.003271116642281413}, {"id": 1929, "seek": 1004056, "start": 10045.76, "end": 10051.039999999999, "text": " like amazing at learning foreign languages. And so he like, this is an amazing feat, right, to be", "tokens": [50624, 411, 2243, 412, 2539, 5329, 8650, 13, 400, 370, 415, 411, 11, 341, 307, 364, 2243, 15425, 11, 558, 11, 281, 312, 50888], "temperature": 0.0, "avg_logprob": -0.1341012420654297, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.003271116642281413}, {"id": 1930, "seek": 1004056, "start": 10051.039999999999, "end": 10056.32, "text": " able to go, this is a language, which has no translators before him. I mean, there were,", "tokens": [50888, 1075, 281, 352, 11, 341, 307, 257, 2856, 11, 597, 575, 572, 5105, 3391, 949, 796, 13, 286, 914, 11, 456, 645, 11, 51152], "temperature": 0.0, "avg_logprob": -0.1341012420654297, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.003271116642281413}, {"id": 1931, "seek": 1004056, "start": 10056.32, "end": 10060.32, "text": " he was a mystery, well, there was a guy that had been there before, but he wasn't very good.", "tokens": [51152, 415, 390, 257, 11422, 11, 731, 11, 456, 390, 257, 2146, 300, 632, 668, 456, 949, 11, 457, 415, 2067, 380, 588, 665, 13, 51352], "temperature": 0.0, "avg_logprob": -0.1341012420654297, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.003271116642281413}, {"id": 1932, "seek": 1004056, "start": 10060.32, "end": 10065.92, "text": " And so he learned the language far better than anyone else had learned before him. He's like good", "tokens": [51352, 400, 370, 415, 3264, 264, 2856, 1400, 1101, 813, 2878, 1646, 632, 3264, 949, 796, 13, 634, 311, 411, 665, 51632], "temperature": 0.0, "avg_logprob": -0.1341012420654297, "compression_ratio": 1.788679245283019, "no_speech_prob": 0.003271116642281413}, {"id": 1933, "seek": 1006592, "start": 10065.92, "end": 10070.48, "text": " at, he's just a, he's a very social person. I think that's a big part of it is being able to", "tokens": [50364, 412, 11, 415, 311, 445, 257, 11, 415, 311, 257, 588, 2093, 954, 13, 286, 519, 300, 311, 257, 955, 644, 295, 309, 307, 885, 1075, 281, 50592], "temperature": 0.0, "avg_logprob": -0.13362634023030598, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.0826474204659462}, {"id": 1934, "seek": 1006592, "start": 10070.48, "end": 10076.64, "text": " interact. So I don't know, it kind of depends on these, this species from outer space, how", "tokens": [50592, 4648, 13, 407, 286, 500, 380, 458, 11, 309, 733, 295, 5946, 322, 613, 11, 341, 6172, 490, 10847, 1901, 11, 577, 50900], "temperature": 0.0, "avg_logprob": -0.13362634023030598, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.0826474204659462}, {"id": 1935, "seek": 1006592, "start": 10076.64, "end": 10080.48, "text": " much they want to talk to us. Is there something you can say about the process he follows?", "tokens": [50900, 709, 436, 528, 281, 751, 281, 505, 13, 1119, 456, 746, 291, 393, 584, 466, 264, 1399, 415, 10002, 30, 51092], "temperature": 0.0, "avg_logprob": -0.13362634023030598, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.0826474204659462}, {"id": 1936, "seek": 1006592, "start": 10080.48, "end": 10085.92, "text": " Like what, how do you show up to a tribe and socialize? I mean, I guess colors and counting is", "tokens": [51092, 1743, 437, 11, 577, 360, 291, 855, 493, 281, 257, 17625, 293, 2093, 1125, 30, 286, 914, 11, 286, 2041, 4577, 293, 13251, 307, 51364], "temperature": 0.0, "avg_logprob": -0.13362634023030598, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.0826474204659462}, {"id": 1937, "seek": 1006592, "start": 10085.92, "end": 10089.6, "text": " one of the most basic things to figure out. Yeah, you start that. You actually start with", "tokens": [51364, 472, 295, 264, 881, 3875, 721, 281, 2573, 484, 13, 865, 11, 291, 722, 300, 13, 509, 767, 722, 365, 51548], "temperature": 0.0, "avg_logprob": -0.13362634023030598, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.0826474204659462}, {"id": 1938, "seek": 1006592, "start": 10089.6, "end": 10094.16, "text": " like objects and just say, you know, just throw a stick down and say stick. And then you say,", "tokens": [51548, 411, 6565, 293, 445, 584, 11, 291, 458, 11, 445, 3507, 257, 2897, 760, 293, 584, 2897, 13, 400, 550, 291, 584, 11, 51776], "temperature": 0.0, "avg_logprob": -0.13362634023030598, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.0826474204659462}, {"id": 1939, "seek": 1009416, "start": 10094.16, "end": 10097.76, "text": " what do you call this? And then they'll say the word, whatever. And he says,", "tokens": [50364, 437, 360, 291, 818, 341, 30, 400, 550, 436, 603, 584, 264, 1349, 11, 2035, 13, 400, 415, 1619, 11, 50544], "temperature": 0.0, "avg_logprob": -0.09583463668823242, "compression_ratio": 1.8244047619047619, "no_speech_prob": 0.005058319773525}, {"id": 1940, "seek": 1009416, "start": 10097.76, "end": 10101.119999999999, "text": " the standard thing to do is to throw two sticks at two sticks. And then, you know,", "tokens": [50544, 264, 3832, 551, 281, 360, 307, 281, 3507, 732, 12518, 412, 732, 12518, 13, 400, 550, 11, 291, 458, 11, 50712], "temperature": 0.0, "avg_logprob": -0.09583463668823242, "compression_ratio": 1.8244047619047619, "no_speech_prob": 0.005058319773525}, {"id": 1941, "seek": 1009416, "start": 10101.119999999999, "end": 10105.36, "text": " he learned pretty quick that there weren't any count words in this language because", "tokens": [50712, 415, 3264, 1238, 1702, 300, 456, 4999, 380, 604, 1207, 2283, 294, 341, 2856, 570, 50924], "temperature": 0.0, "avg_logprob": -0.09583463668823242, "compression_ratio": 1.8244047619047619, "no_speech_prob": 0.005058319773525}, {"id": 1942, "seek": 1009416, "start": 10105.36, "end": 10108.8, "text": " they didn't know this wasn't interesting. I mean, it was kind of weird. They'd say some or", "tokens": [50924, 436, 994, 380, 458, 341, 2067, 380, 1880, 13, 286, 914, 11, 309, 390, 733, 295, 3657, 13, 814, 1116, 584, 512, 420, 51096], "temperature": 0.0, "avg_logprob": -0.09583463668823242, "compression_ratio": 1.8244047619047619, "no_speech_prob": 0.005058319773525}, {"id": 1943, "seek": 1009416, "start": 10108.8, "end": 10112.32, "text": " something, the same word over and over again. And so, but that is a standard thing. You just", "tokens": [51096, 746, 11, 264, 912, 1349, 670, 293, 670, 797, 13, 400, 370, 11, 457, 300, 307, 257, 3832, 551, 13, 509, 445, 51272], "temperature": 0.0, "avg_logprob": -0.09583463668823242, "compression_ratio": 1.8244047619047619, "no_speech_prob": 0.005058319773525}, {"id": 1944, "seek": 1009416, "start": 10112.32, "end": 10117.84, "text": " like try to, but you have to be pretty out there socially, like willing to talk to random people,", "tokens": [51272, 411, 853, 281, 11, 457, 291, 362, 281, 312, 1238, 484, 456, 21397, 11, 411, 4950, 281, 751, 281, 4974, 561, 11, 51548], "temperature": 0.0, "avg_logprob": -0.09583463668823242, "compression_ratio": 1.8244047619047619, "no_speech_prob": 0.005058319773525}, {"id": 1945, "seek": 1009416, "start": 10118.64, "end": 10122.4, "text": " which these are, you know, really very different people from you. And he was, and he's,", "tokens": [51588, 597, 613, 366, 11, 291, 458, 11, 534, 588, 819, 561, 490, 291, 13, 400, 415, 390, 11, 293, 415, 311, 11, 51776], "temperature": 0.0, "avg_logprob": -0.09583463668823242, "compression_ratio": 1.8244047619047619, "no_speech_prob": 0.005058319773525}, {"id": 1946, "seek": 1012240, "start": 10122.4, "end": 10125.44, "text": " he's very social. And so I think that's a big part of this is like, that's how,", "tokens": [50364, 415, 311, 588, 2093, 13, 400, 370, 286, 519, 300, 311, 257, 955, 644, 295, 341, 307, 411, 11, 300, 311, 577, 11, 50516], "temperature": 0.0, "avg_logprob": -0.11667166726063874, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0014547904720529914}, {"id": 1947, "seek": 1012240, "start": 10126.08, "end": 10130.48, "text": " you know, a lot of people know a lot of languages that they're willing to talk to other people.", "tokens": [50548, 291, 458, 11, 257, 688, 295, 561, 458, 257, 688, 295, 8650, 300, 436, 434, 4950, 281, 751, 281, 661, 561, 13, 50768], "temperature": 0.0, "avg_logprob": -0.11667166726063874, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0014547904720529914}, {"id": 1948, "seek": 1012240, "start": 10130.48, "end": 10135.279999999999, "text": " That's a tough one. We just show up knowing nothing. Yeah. Oh, God. It's beautiful that", "tokens": [50768, 663, 311, 257, 4930, 472, 13, 492, 445, 855, 493, 5276, 1825, 13, 865, 13, 876, 11, 1265, 13, 467, 311, 2238, 300, 51008], "temperature": 0.0, "avg_logprob": -0.11667166726063874, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0014547904720529914}, {"id": 1949, "seek": 1012240, "start": 10135.279999999999, "end": 10140.56, "text": " humans are able to connect in that way. Yeah. Yeah. You've had an incredible career exploring", "tokens": [51008, 6255, 366, 1075, 281, 1745, 294, 300, 636, 13, 865, 13, 865, 13, 509, 600, 632, 364, 4651, 3988, 12736, 51272], "temperature": 0.0, "avg_logprob": -0.11667166726063874, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0014547904720529914}, {"id": 1950, "seek": 1012240, "start": 10140.56, "end": 10146.48, "text": " this fascinating topic. What advice would you give to young people about how to have a career", "tokens": [51272, 341, 10343, 4829, 13, 708, 5192, 576, 291, 976, 281, 2037, 561, 466, 577, 281, 362, 257, 3988, 51568], "temperature": 0.0, "avg_logprob": -0.11667166726063874, "compression_ratio": 1.6765799256505576, "no_speech_prob": 0.0014547904720529914}, {"id": 1951, "seek": 1014648, "start": 10147.439999999999, "end": 10152.32, "text": " like that or a life that they can be proud of? When you see something interesting,", "tokens": [50412, 411, 300, 420, 257, 993, 300, 436, 393, 312, 4570, 295, 30, 1133, 291, 536, 746, 1880, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1442216736039305, "compression_ratio": 1.75, "no_speech_prob": 0.1687081903219223}, {"id": 1952, "seek": 1014648, "start": 10152.32, "end": 10156.88, "text": " just go and do it. Like I do, I do that. Like that's something I do, which is kind of unusual", "tokens": [50656, 445, 352, 293, 360, 309, 13, 1743, 286, 360, 11, 286, 360, 300, 13, 1743, 300, 311, 746, 286, 360, 11, 597, 307, 733, 295, 10901, 50884], "temperature": 0.0, "avg_logprob": -0.1442216736039305, "compression_ratio": 1.75, "no_speech_prob": 0.1687081903219223}, {"id": 1953, "seek": 1014648, "start": 10156.88, "end": 10160.88, "text": " for most people. So like when I saw the, like if Piedoha was available to go and visit, I was like,", "tokens": [50884, 337, 881, 561, 13, 407, 411, 562, 286, 1866, 264, 11, 411, 498, 430, 1091, 1445, 64, 390, 2435, 281, 352, 293, 3441, 11, 286, 390, 411, 11, 51084], "temperature": 0.0, "avg_logprob": -0.1442216736039305, "compression_ratio": 1.75, "no_speech_prob": 0.1687081903219223}, {"id": 1954, "seek": 1014648, "start": 10160.88, "end": 10167.92, "text": " yes, yes, I'll go. And then when we couldn't go back, we had some trouble with the Brazilian", "tokens": [51084, 2086, 11, 2086, 11, 286, 603, 352, 13, 400, 550, 562, 321, 2809, 380, 352, 646, 11, 321, 632, 512, 5253, 365, 264, 23435, 51436], "temperature": 0.0, "avg_logprob": -0.1442216736039305, "compression_ratio": 1.75, "no_speech_prob": 0.1687081903219223}, {"id": 1955, "seek": 1014648, "start": 10167.92, "end": 10171.52, "text": " government. There's some corrupt people there. It was very difficult to get, go back in there.", "tokens": [51436, 2463, 13, 821, 311, 512, 17366, 561, 456, 13, 467, 390, 588, 2252, 281, 483, 11, 352, 646, 294, 456, 13, 51616], "temperature": 0.0, "avg_logprob": -0.1442216736039305, "compression_ratio": 1.75, "no_speech_prob": 0.1687081903219223}, {"id": 1956, "seek": 1014648, "start": 10171.52, "end": 10174.96, "text": " And so I was like, all right, I got to find another group. And so we searched around and", "tokens": [51616, 400, 370, 286, 390, 411, 11, 439, 558, 11, 286, 658, 281, 915, 1071, 1594, 13, 400, 370, 321, 22961, 926, 293, 51788], "temperature": 0.0, "avg_logprob": -0.1442216736039305, "compression_ratio": 1.75, "no_speech_prob": 0.1687081903219223}, {"id": 1957, "seek": 1017496, "start": 10174.96, "end": 10178.96, "text": " we were able to find the, because I wanted to keep working on this kind of problem. And so we", "tokens": [50364, 321, 645, 1075, 281, 915, 264, 11, 570, 286, 1415, 281, 1066, 1364, 322, 341, 733, 295, 1154, 13, 400, 370, 321, 50564], "temperature": 0.0, "avg_logprob": -0.12937419029974168, "compression_ratio": 1.7891373801916932, "no_speech_prob": 0.0037059762980788946}, {"id": 1958, "seek": 1017496, "start": 10178.96, "end": 10182.88, "text": " found the Chimani and just go there. I didn't really have, we didn't have contact. We had a", "tokens": [50564, 1352, 264, 761, 332, 3782, 293, 445, 352, 456, 13, 286, 994, 380, 534, 362, 11, 321, 994, 380, 362, 3385, 13, 492, 632, 257, 50760], "temperature": 0.0, "avg_logprob": -0.12937419029974168, "compression_ratio": 1.7891373801916932, "no_speech_prob": 0.0037059762980788946}, {"id": 1959, "seek": 1017496, "start": 10182.88, "end": 10187.599999999999, "text": " little bit of contact and brought someone. And that was, you know, we just, just kind of just", "tokens": [50760, 707, 857, 295, 3385, 293, 3038, 1580, 13, 400, 300, 390, 11, 291, 458, 11, 321, 445, 11, 445, 733, 295, 445, 50996], "temperature": 0.0, "avg_logprob": -0.12937419029974168, "compression_ratio": 1.7891373801916932, "no_speech_prob": 0.0037059762980788946}, {"id": 1960, "seek": 1017496, "start": 10187.599999999999, "end": 10193.039999999999, "text": " try things. I say it's like, a lot of that's just like ambition, just try to do something", "tokens": [50996, 853, 721, 13, 286, 584, 309, 311, 411, 11, 257, 688, 295, 300, 311, 445, 411, 22814, 11, 445, 853, 281, 360, 746, 51268], "temperature": 0.0, "avg_logprob": -0.12937419029974168, "compression_ratio": 1.7891373801916932, "no_speech_prob": 0.0037059762980788946}, {"id": 1961, "seek": 1017496, "start": 10193.039999999999, "end": 10197.439999999999, "text": " that other people haven't done. Just give it a shot is what I, I mean, I do that all the time.", "tokens": [51268, 300, 661, 561, 2378, 380, 1096, 13, 1449, 976, 309, 257, 3347, 307, 437, 286, 11, 286, 914, 11, 286, 360, 300, 439, 264, 565, 13, 51488], "temperature": 0.0, "avg_logprob": -0.12937419029974168, "compression_ratio": 1.7891373801916932, "no_speech_prob": 0.0037059762980788946}, {"id": 1962, "seek": 1017496, "start": 10197.439999999999, "end": 10203.279999999999, "text": " I love it. And I love the fact that your pursuit of fun has landed you here talking to me. This", "tokens": [51488, 286, 959, 309, 13, 400, 286, 959, 264, 1186, 300, 428, 23365, 295, 1019, 575, 15336, 291, 510, 1417, 281, 385, 13, 639, 51780], "temperature": 0.0, "avg_logprob": -0.12937419029974168, "compression_ratio": 1.7891373801916932, "no_speech_prob": 0.0037059762980788946}, {"id": 1963, "seek": 1020328, "start": 10203.28, "end": 10208.960000000001, "text": " was an incredible conversation that you're, you're, you're just a fascinating human being.", "tokens": [50364, 390, 364, 4651, 3761, 300, 291, 434, 11, 291, 434, 11, 291, 434, 445, 257, 10343, 1952, 885, 13, 50648], "temperature": 0.0, "avg_logprob": -0.10656246931656548, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.0012836229288950562}, {"id": 1964, "seek": 1020328, "start": 10208.960000000001, "end": 10212.880000000001, "text": " Thank you for taking a journey through human language with me today. This is awesome.", "tokens": [50648, 1044, 291, 337, 1940, 257, 4671, 807, 1952, 2856, 365, 385, 965, 13, 639, 307, 3476, 13, 50844], "temperature": 0.0, "avg_logprob": -0.10656246931656548, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.0012836229288950562}, {"id": 1965, "seek": 1020328, "start": 10212.880000000001, "end": 10214.800000000001, "text": " Thank you very much. It's been pleasure.", "tokens": [50844, 1044, 291, 588, 709, 13, 467, 311, 668, 6834, 13, 50940], "temperature": 0.0, "avg_logprob": -0.10656246931656548, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.0012836229288950562}, {"id": 1966, "seek": 1020328, "start": 10215.92, "end": 10220.400000000001, "text": " Thanks for listening to this conversation with Edward Gibson. To support this podcast,", "tokens": [50996, 2561, 337, 4764, 281, 341, 3761, 365, 18456, 42250, 13, 1407, 1406, 341, 7367, 11, 51220], "temperature": 0.0, "avg_logprob": -0.10656246931656548, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.0012836229288950562}, {"id": 1967, "seek": 1020328, "start": 10220.400000000001, "end": 10225.68, "text": " please check out our sponsors in the description. And now let me leave you with some words from", "tokens": [51220, 1767, 1520, 484, 527, 22593, 294, 264, 3855, 13, 400, 586, 718, 385, 1856, 291, 365, 512, 2283, 490, 51484], "temperature": 0.0, "avg_logprob": -0.10656246931656548, "compression_ratio": 1.639344262295082, "no_speech_prob": 0.0012836229288950562}, {"id": 1968, "seek": 1022568, "start": 10225.76, "end": 10231.28, "text": " Wittgenstein. The limits of my language mean the limits of my world.", "tokens": [50368, 343, 593, 1766, 9089, 13, 440, 10406, 295, 452, 2856, 914, 264, 10406, 295, 452, 1002, 13, 50644], "temperature": 0.0, "avg_logprob": -0.18669446776894963, "compression_ratio": 1.3085106382978724, "no_speech_prob": 0.27160924673080444}, {"id": 1969, "seek": 1022568, "start": 10232.48, "end": 10241.6, "text": " Thank you for listening and hope to see you next time.", "tokens": [50704, 1044, 291, 337, 4764, 293, 1454, 281, 536, 291, 958, 565, 13, 51160], "temperature": 0.0, "avg_logprob": -0.18669446776894963, "compression_ratio": 1.3085106382978724, "no_speech_prob": 0.27160924673080444}], "language": "en"}