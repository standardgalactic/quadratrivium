start	end	text
0	7360	I see the danger of this concentration of power through proprietary AI systems as a much bigger
7360	13840	danger than everything else. What works against this is people who think that for reasons of
13840	20000	security we should keep AI systems under lock and key because it's too dangerous to put it in the
20000	27280	hands of everybody. That would lead to a very bad future in which all of our information
27280	32240	diet is controlled by a small number of companies through proprietary systems.
32240	38720	I believe that people are fundamentally good and so if AI, especially open source AI, can
40160	44160	make them smarter, it just empowers the goodness in humans.
44160	48080	So I share that feeling. Okay, I think people are fundamentally good.
50080	54960	And in fact a lot of doomers are doomers because they don't think that people are fundamentally
55280	62880	good. The following is a conversation with Yan Lukun, his third time on this podcast.
62880	69920	He is the chief AI scientist at Meta, professor at NYU, touring award winner, and one of the
69920	76240	seminal figures in the history of artificial intelligence. He and Meta AI have been big
76240	82560	proponents of open sourcing AI development and have been walking the walk by open sourcing many
82560	90960	of their biggest models, including Lama 2 and eventually Lama 3. Also, Yan has been an outspoken
90960	97520	critic of those people in the AI community who warned about the looming danger and existential
97520	106240	threat of AGI. He believes the AGI will be created one day, but it will be good. It will not escape
106240	114320	human control, nor will it dominate and kill all humans. At this moment of rapid AI development,
114320	121440	this happens to be somewhat a controversial position. And so it's been fun seeing Yan get
121440	127680	into a lot of intense and fascinating discussions online, as we do in this very conversation.
128560	132640	This is the lecture room and podcast that supported. Please check out our sponsors in the
132640	140240	description. And now, dear friends, here's Yan Likun. You've had some strong statements,
141200	145760	technical statements about the future of artificial intelligence recently throughout
145760	152160	your career, actually, but recently as well. You've said that autoregressive LLMs are
154080	159840	not the way we're going to make progress towards superhuman intelligence. These are the large
159920	166000	language models like GPT-4 like Lama 2 and 3 soon and so on. How do they work and why are they not
166000	170880	going to take us all the way? For a number of reasons. The first is that there is a number of
170880	177920	characteristics of intelligent behavior. For example, the capacity to understand the world,
178720	187920	understand the physical world, the ability to remember and retrieve things, persistent memory,
188800	194400	the ability to reason and the ability to plan. Those are four essential characteristics of
194400	204080	intelligent systems or entities, humans, animals. LLMs can do none of those. Or they can only do
204080	209840	them in a very primitive way. And they don't really understand the physical world. They don't
209840	214640	really have persistent memory. They can't really reason and they certainly can't plan. And so,
215600	222720	if you expect a system to become intelligent just without having the possibility of doing those
222720	230080	things, you're making a mistake. That is not to say that autoregressive LLMs are not useful.
230640	238000	They're certainly useful. That they're not interesting, that we can't build a whole ecosystem
238560	245040	of applications around them. Of course, we can. But as a path towards human level intelligence,
245760	251680	they are missing essential components. And then there is another tidbit or fact that I think is
252320	259120	very interesting. Those LLMs are trained on enormous amounts of text. Basically, the entirety of
259120	265920	all publicly available texts on the internet. That's typically on the order of 10 to the 13
265920	271200	tokens. Each token is typically two bytes. So that's two 10 to the 13 bytes as training data.
271760	276960	It would take you or me 170,000 years to just read through this at eight hours a day.
278560	282560	So it seems like an enormous amount of knowledge that those systems can accumulate.
286000	289360	But then you realize it's really not that much data. If you talk to
290320	295760	a developmental psychologist and they tell you a four-year-old has been awake for 16,000 hours
296400	306720	in his or her life. And the amount of information that has reached the visual cortex of that child
306720	314240	in four years is about 10 to the 15 bytes. And you can compute this by estimating that the
315200	321280	optical nerve carry about 20 megabytes per second, roughly. And so 10 to the 15 bytes
321280	328320	for a four-year-old versus two times 10 to the 13 bytes for 170,000 years worth of reading.
329280	336800	What I tell you is that through sensory input, we see a lot more information than we do through
336800	342880	language. And that despite our intuition, most of what we learn and most of our knowledge
343840	348720	is through our observation and interaction with the real world, not through language.
349360	353840	Everything that we learn in the first few years of life and certainly everything that
353840	359440	animals learn has nothing to do with language. So it would be good to maybe push against some
359440	365840	of the intuition behind what you're saying. So it is true there's several orders of magnitude,
365840	372240	more data coming into the human mind, how much faster and the human mind is able to learn very
372240	377200	quickly from that filter of the data very quickly. Somebody might argue your comparison
377200	383600	between sensory data versus language. That language is already very compressed. It already
383600	388320	contains a lot more information than the bytes it takes to store them if you compare it to visual
388320	393760	data. So there's a lot of wisdom in language. There's words and the way we stitch them together,
393760	401520	it already contains a lot of information. So is it possible that language alone already has
402160	409440	enough wisdom and knowledge in there to be able to from that language construct a
410080	414560	world model and understanding of the world and understanding of the physical world
414560	421040	that you're saying all limbs lack? So it's a big debate among philosophers and also cognitive
421040	427360	scientists like whether intelligence needs to be grounded in reality. I'm clearly in the camp that
428080	434800	yes, intelligence cannot appear without some grounding in some reality. It doesn't need to be
435920	441200	physical reality. It could be simulated, but the environment is just much richer than what you
441200	446880	can express in language. Language is a very approximate representation of our perceives
447520	453120	and our mental models. There's a lot of tasks that we accomplish where we manipulate
454080	460560	a mental model of the situation at hand, and that has nothing to do with language.
460560	465520	Everything that's physical, mechanical, whatever, when we build something, when we
465520	472240	accomplish a task, a model task of grabbing something, etc., we plan for action sequences and
472240	480160	we do this by essentially imagining the outcome of a sequence of actions that we might imagine.
481120	486640	And that requires mental models that don't have much to do with language. And that's,
486640	493680	I would argue, most of our knowledge is derived from that interaction with the physical world.
493680	498400	So a lot of my colleagues who are more interested in things like computer vision
499120	506240	are really on that camp that AI needs to be embodied, essentially. And then other people
506240	513840	coming from the NLP side or maybe some other motivation don't necessarily agree with that.
514800	521760	And philosophers are split as well. And the complexity of the world is hard to
524000	532720	imagine. It's hard to represent all the complexities that we take completely for granted
532720	536720	in the real world that we don't even imagine require intelligence. This is the old Moravec
536720	543120	paradox from the pioneer of robotics, Hans Moravec. We said, how is it that with computers,
543120	548240	it seems to be easy to do high-level complex tasks like playing chess and solving integrals
548240	552960	and doing things like that? Whereas the thing we take for granted that we do every day,
554160	559600	like, I don't know, learning to drive a car or grabbing an object, we can do with computers.
563200	569680	We have LLMs that can pass the bar exam, so they must be smart. But then
570880	576880	they can't learn to drive in 20 hours like any 17-year-old. They can't learn to clear up the
576880	581840	dinner table and fill up the dishwasher like any 10-year-old can learn in one shot.
583440	591200	Why is that? What are we missing? What type of learning or reasoning architecture or whatever
591200	599920	are we missing that basically prevent us from having LLMs in cars and domestic robots?
600800	608160	Can a large language model construct a world model that does know how to drive and does know
608160	612640	how to fill a dishwasher but just doesn't know how to deal with visual data at this time? So
613680	616240	it can operate in a space of concepts.
617040	621600	So yeah, that's what a lot of people are working on. So the answer, the short answer is no.
622320	632560	And the more complex answer is you can use all kinds of tricks to get an LLM to basically digest
633840	640320	visual representations of images or video or audio for that matter.
641120	648240	And a classical way of doing this is you train a vision system in some way.
649200	652720	And we have a number of ways to train vision systems, either supervised, semi-supervised,
652720	660000	self-supervised, all kinds of different ways. That will turn any image into a high-level
660000	665840	representation, basically a list of tokens that are really similar to the kind of tokens that
665920	676240	typical LLM takes as an input. And then you just feed that to the LLM in addition to the text.
676960	683440	And you just expect the LLM to kind of, you know, during training to kind of be able to
684080	688000	use those representations to help make decisions. I mean, it's been working
688000	692720	along those lines for quite a long time. And now you see those systems, right? I mean,
692720	698320	there are LLMs that have some vision extension, but they're basically hacks in the sense that
699440	703600	those things are not like trained end-to-end to handle, to really understand the world.
703600	708160	They're not trained with video, for example. They don't really understand intuitive physics,
708800	713760	at least not at the moment. So you don't think there's something special to you about intuitive
713760	718560	physics, about sort of common sense reasoning about the physical space, about physical reality?
718880	722640	That to you is a giant leap that LLMs are just not able to do.
722640	727600	We're not going to be able to do this with the type of LLMs that we are working with today.
727600	730640	And there's a number of reasons for this. But the main reason is,
732240	736640	the way LLMs are trained is that you take a piece of text, you
737840	741680	remove some of the words in that text, you mask them, you replace them by
741680	745840	blank markers, and you train a genetic neural net to predict the words that are missing.
746720	751520	And if you build this neural net in a particular way, so that it can only look at
752720	754960	words that are to the left of the one it's trying to predict,
755920	759840	then what you have is a system that basically is trying to predict the next word in a text.
759840	765600	So then you can feed it a text, a prompt, and you can ask it to predict the next word.
765600	770240	It can never predict the next word exactly. And so what it's going to do is produce
771120	774800	a probability distribution over all the possible words in your dictionary.
774800	778160	In fact, it doesn't predict words, it predicts tokens that are kind of subword units.
778880	784240	And so it's easy to handle the uncertainty in the prediction there because there's only a finite
784240	789200	number of possible words in the dictionary. And you can just compute a distribution over them.
790880	796720	Then what the system does is that it picks a word from that distribution.
796720	800320	Of course, there's a higher chance of picking words that have a higher probability
800320	804160	within that distribution. So you sample from that distribution to actually produce a word.
805120	810240	And then you shift that word into the input. And so that allows the system not to predict
810240	815600	the second word. And once you do this, you shift it into the input, etc. That's called
815600	821280	autoregressive prediction, which is why those LLMs should be called autoregressive LLMs.
823200	830400	But we just call them LLMs. And there is a difference between this kind of process
830400	836480	and a process by which before producing a word, when you talk, when you and I talk,
836480	841840	you and I are bilingual, we think about what we're going to say, and it's relatively independent
841840	846800	of the language in which we're going to say it. When we talk about, I don't know,
846800	850080	let's say a mathematical concept or something, the kind of thinking that we're doing,
850640	857040	and the answer that we're planning to produce is not linked to whether we're going to say it in
857040	862400	French, Russian, or English. Chomsky just rolled his eyes, but I understand. So you're saying
862400	870080	that there's a bigger abstraction that goes before language, maps onto language.
870080	873760	Right. It's certainly true for a lot of thinking that we do.
873760	878080	Is that obvious that we don't? You're saying your thinking is
878080	880800	same in French as it is in English? Yeah, pretty much.
881760	887600	Pretty much? Or is this like, how flexible are you? Like, if there's a probability distribution?
888880	894000	Well, it depends what kind of thinking, right? If it's just, if it's like producing puns,
894000	896080	I get much better in French than English about that.
896720	901760	No, but is there an abstract representation of puns? Like, is your humor an abstract?
901760	905360	But like when you tweet, and your tweets are sometimes a little bit spicy,
905920	911040	what's, is there an abstract representation in your brain of a tweet before it maps onto English?
911600	918560	There is an abstract representation of imagining the reaction of a reader to that text.
918560	921760	Well, you start with laughter and then figure out how to make that happen.
921760	927120	Or a figure out like a reaction you want to cause and then figure out how to say it,
927120	930640	right? So that it causes that reaction. But that's like really close to language.
930640	936560	But think about like a mathematical concept or, you know, imagining, you know,
936560	939280	something you want to build out of wood or something like that, right?
939840	943280	The kind of thinking you're doing is absolutely nothing to do with language, really.
943280	947520	Like it's not like you have necessarily like an internal monologue in any particular language.
947520	953680	You're, you know, imagining mental models of the thing, right? I mean, if I ask you to,
953680	958320	like imagine what this water bottle will look like if I rotate it 90 degrees,
958720	965600	that has nothing to do with language. And so, so clearly there is, you know,
965600	972160	a more abstract level of representation in which we do most of our thinking and we plan
972960	981440	what we're going to say if the output is, you know, uttered words as opposed to an output being,
982400	989120	you know, muscle actions, right? We plan our answer before we produce it.
989120	992640	And LLMs don't do that. They just produce one word after the other,
993760	1000560	instinctively, if you want. It's like, it's a bit like the, you know, subconscious actions where
1000560	1005440	you don't, like you're distracted, you're doing something, you're completely concentrated and
1005440	1009520	someone comes to you and, you know, ask you a question and you kind of answer the question.
1009520	1013120	You don't have time to think about the answer, but the answer is easy, so you don't need to pay
1013120	1018800	attention. You sort of respond automatically. That's kind of what an LLM does, right? It doesn't
1018800	1024320	think about its answer really. It retrieves it because it's accumulated a lot of knowledge,
1024320	1030800	so it can retrieve some, some things, but it's going to just spit out one token after the other
1030800	1037120	without planning the answer. But you're making it sound just one token after the other,
1037120	1047280	one token at a time generation is bound to be simplistic. But if the world model is sufficiently
1047280	1054880	sophisticated, that one token at a time, the, the most likely thing it generates is a sequence of
1054880	1062640	tokens is going to be a deeply profound thing. Okay, but then that assumes that those systems
1062640	1068080	actually possess a neutral world model. So it really goes to the, I think the fundamental question
1068080	1076720	is, can you build a really complete world model, not complete, but a one that has a deep
1076720	1082800	understanding of the world? Yeah. So can you build this, first of all, by prediction?
1083440	1090560	Right. And the answer is probably yes. Can you predict, can you build it by predicting words?
1090560	1098160	And the answer is most probably no, because language is very poor in terms of weak or low
1098160	1103760	bandwidth, if you want, there's just not enough information there. So building world models means
1104960	1113840	observing the world and understanding why the world is evolving the way, the way it is. And then
1114720	1121840	the, the extra component of a world model is something that can predict how the world is
1121840	1126880	going to evolve as a consequence of an action you might take, right? So what model really is,
1126880	1130240	here is my idea of the state of the world at time t, here is an action I might take.
1130880	1137520	What is the predicted state of the world at time t plus one. Now that state of the world doesn't,
1137520	1142240	that does not need to represent everything about the world. It just needs to represent
1142240	1147600	enough that's relevant for this planning of the action, but not necessarily all the details.
1148240	1153760	Now here is the problem. You're not going to be able to do this with generative models.
1154720	1158960	So a generative model has trained on video, and we've tried to do this for 10 years. You take a
1158960	1165120	video, show a system a piece of video, and then ask it to predict the reminder of the video.
1165680	1171520	Basically, predict what's going to happen. One frame at a time, do the same thing as sort of
1171520	1176240	the auto aggressive LLMs do, but for video. Right. Either one frame at a time or a group of
1176240	1185840	friends at a time. But yeah, a large video model if you want. The idea of doing this has been floating
1185840	1191920	around for a long time. And at fair, some of my colleagues and I have been trying to do this for
1191920	1198720	about 10 years. And you can't really do the same trick as with LLMs because
1199680	1205760	you know LLMs, as I said, you can't predict exactly which word is going to follow a
1205760	1210720	sequence of words, but you can predict the distribution over words. Now, if you go to video,
1211440	1215120	what you would have to do is predict the distribution over all possible frames
1215120	1221600	in a video. And we don't really know how to do that properly. We do not know how to represent
1221600	1225520	distributions over high dimensional continuous spaces in ways that are useful.
1226080	1234480	And that's, there lies the main issue. And the reason we can do this is because the world is
1234480	1241360	incredibly more complicated and richer in terms of information than text. Text is discrete.
1242880	1249280	Video is high dimensional and continuous. A lot of details in this. So if I take a video of this
1249280	1257840	room, and the video is, you know, a camera panning around, there is no way I can predict
1257840	1261200	everything that's going to be in the room as I pan around. The system cannot predict what's going
1261200	1266880	to be in the room as the camera is panning. Maybe it's going to predict this is, this is a room
1266880	1270400	where there's a light and there is a wall and things like that. It can't predict what the
1270400	1275280	painting on the wall looks like or what the texture of the couch looks like. Certainly not the texture
1275280	1282480	of the carpet. So there's no way I can predict all those details. So the way to handle this is
1283280	1287680	one way to possibly to handle this, which we've been working for a long time, is to have a model that
1287680	1293280	has what's called a latent variable. And the latent variable is fed to a neural net. And it's
1293280	1300240	supposed to represent all the information about the world that you don't perceive yet. And that you
1300240	1308080	need to augment the system for the prediction to do a good job at predicting pixels, including the,
1308080	1314640	you know, fine texture of the carpet and the couch and the painting on the wall.
1317040	1322800	That has been a complete failure, essentially. And we've tried lots of things. We tried just
1322800	1330000	straight neural nets. We tried GANs. We tried, you know, VAEs, all kinds of regularized auto
1330000	1336160	encoders. We tried many things. We also tried those kind of methods to learn
1337600	1344800	good representations of images or video that could then be used as input to, for example,
1344800	1350880	an image classification system. And that also has basically failed, like all the systems that attempt
1350880	1360160	to predict missing parts of an image or video, you know, from a corrupted version of it, basically.
1360160	1365200	So I take an image or a video, corrupt it or transform it in some way, and then try to reconstruct
1365200	1372560	the complete video or image from the corrupted version. And then hope that internally the system
1372560	1377520	will develop good representations of images that you can use for object recognition, segmentation,
1377520	1384320	whatever it is. That has been essentially a complete failure. And it works really well for text.
1384320	1390160	That's the principle that is used for LLMs, right? So where's the failure exactly? Is that it's very
1390160	1398160	difficult to form a good representation of an image, like a good embedding of all the important
1398160	1402640	information in the image? Is it in terms of the consistency of image to image to image to image
1402640	1407840	that forms the video? Like where, what is the, if you do a highlight reel of all the ways you
1407840	1415840	failed? What does that look like? Okay, so the reason this doesn't work is, first of all, I have to
1415840	1420320	tell you exactly what doesn't work because there is something else that does work. So the thing
1420320	1428560	that does not work is training the system to learn representations of images by training it to
1429520	1434800	reconstruct a good image from a corrupted version of it. Okay, that's what doesn't work.
1435520	1441680	And we have a whole slew of techniques for this that are, you know, a variant of denoising auto
1441680	1446800	encoders, something called MAE developed by some of my colleagues at FAIR, Max Dotto Encoder.
1446800	1453120	So it's basically like the, you know, LLMs or things like this where you train the system by
1453120	1457200	corrupting text, except you corrupt images, you remove patches from it, then you train a
1457200	1461920	gigantic neural net to reconstruct. The features you get are not good. And you know, they're not
1461920	1466560	good because if you now train the same architecture, but you train it supervised
1467680	1475600	with label data, with text textual descriptions of images, etc. You do get good representations
1475600	1481520	and the performance on recognition tasks is much better than if you do this self-supervised free
1481520	1485920	training. So the architecture is good? The architecture is good. The architecture of the
1485920	1491760	encoder is good. Okay, but the fact that you train the system to reconstruct images does not
1492480	1498240	lead it to produce to learn good generic features of images. When you train in a self-supervised way?
1498240	1502400	Self-supervised by reconstruction. Yeah, by reconstruction. Okay, so what's the alternative?
1504160	1509520	The alternative is joint embedding. What is joint embedding? What are these
1509520	1513680	architectures that you're so excited about? Okay, so now instead of training a system to
1513680	1519280	encode the image and then training it to reconstruct the full image from a corrupted version,
1519920	1526400	you take the full image, you take the corrupted or transformed version, you run them both through
1526400	1535120	encoders, which in general are identical, but not necessarily. And then you train a predictor on top
1535120	1544640	of those encoders to predict the representation of the full input from the representation of the
1544640	1551680	corrupted one. Okay, so joint embedding because you're taking the full input and the corrupted
1551680	1557200	version or transformed version, run them both through encoders, you get a joint embedding,
1557200	1562240	and then you're saying, can I predict the representation of the full one from the
1562240	1568480	representation of the corrupted one? Okay, and I call this a JEPA, so that means Joint
1568480	1572160	Embedding Predictive Architecture because it's joint embedding and there is this predictor that
1572160	1578640	predicts the representation of the good guy from the bad guy. And the big question is, how do you
1578640	1585360	train something like this? And until five years ago or six years ago, we didn't have particularly
1585360	1591680	good answers for how you train those things, except for one called Contrastive Learning,
1593280	1600080	where, and the idea of Contrastive Learning is you take a pair of images that are, again,
1600640	1605520	an image and a corrupted version or degraded version somehow or transformed version of the
1605520	1611920	original one, and you train the predicted representation to be the same as that. If you
1611920	1616480	only do this, the system collapses. It basically completely ignores the input and produces
1616480	1623680	representations that are constant. So the Contrastive Methods, avoid this, and those things have
1623680	1632160	been around since the early 90s, I had a paper on this in 1993, is you also show pairs of images
1632160	1637680	that you know are different, and then you push away the representations from each other. So you
1637680	1642800	say, not only do representations of things that we know are the same should be the same or should
1642800	1646160	be similar, but representation of things that we know are different should be different.
1647520	1651360	And that prevents the collapse, but it has some limitation. And there's a whole bunch of
1651360	1657760	techniques that have appeared over the last six, seven years that can revive this type of method,
1659040	1662960	some of them from FAIR, some of them from Google and other places.
1663680	1668480	But there are limitations to those Contrastive Methods. What has changed in the last
1670640	1676400	three, four years is now we have methods that are non-contrastive, so they don't require those
1677120	1683760	negative, contrastive samples of images that we know are different. You turn them only with
1683760	1687520	images that are different versions or different views are the same thing.
1688400	1693280	And you rely on some other tricks to prevent the system from collapsing, and we have
1693280	1695040	half a dozen different methods for this now.
1695920	1700160	So what is the fundamental difference between joint embedding architectures
1700880	1709520	and LLMs? So can JAP take us to AGI? But whether we should say that you don't like
1710480	1715440	the term AGI, and we'll probably argue, I think every single time I've talked to you with argued
1715440	1722640	about the G in AGI, I get it. I get it. We'll probably continue to argue about it. It's great.
1726160	1734400	Because you like French, and Amis is, I guess, friend in French, and AMI stands for Advanced
1734400	1741840	Machine Intelligence. But either way, can JAP take us to that, towards that advanced machine
1741840	1747440	intelligence? Well, so it's a first step. So first of all, what's the difference with
1748160	1756720	generative architectures like LLMs? So LLMs or vision systems that are trained by reconstruction
1757600	1765920	generate the inputs. They generate the original input that is non-corrupted, non-transformed.
1766880	1771040	Right? So you have to predict all the pixels. And there is a huge amount of
1771840	1775600	resources spent in the system to actually predict all those pixels, all the details.
1777200	1782240	In a JAPA, you're not trying to predict all the pixels. You're only trying to predict
1782240	1788720	an abstract representation of the inputs. Right? And that's much easier in many ways.
1789440	1794000	So what the JAPA system, when it's being trained, is trying to do is extract as much information
1794000	1800320	as possible from the input, but yet only extract information that is relatively easily predictable.
1801600	1805120	Okay. So there's a lot of things in the world that we cannot predict. For example,
1805120	1812160	if you have a self-driving car driving down the street or road, there may be trees around the
1812160	1818960	road. And it could be a windy day. So the leaves on the tree are kind of moving in kind of semi-chaotic
1818960	1822640	random ways that you can't predict and you don't care. You don't want to predict.
1823600	1827120	So what you want is your encoder to basically eliminate all those details.
1827120	1830640	We'll tell you there's moving leaves, but it's not going to keep the details of exactly what's
1830640	1836640	going on. And so when you do the prediction in representation space, you're not going to have
1836640	1843440	to predict every single pixel of a relief. And that, you know, not only is a lot simpler,
1843440	1850400	but also it allows the system to essentially learn and abstract representation of the world where,
1850400	1857520	you know, what can be modeled and predicted is preserved. And the rest is viewed as noise and
1857520	1862160	eliminated by the encoder. So it kind of lifts the level of abstraction of the representation.
1862160	1865840	If you think about this, this is something we do absolutely all the time. Whenever we
1865840	1870720	describe a phenomenon, we describe it at a particular level of abstraction. And we don't
1870720	1875600	always describe every natural phenomenon in terms of quantum field theory, right? That would be
1876160	1881840	impossible, right? So we have multiple levels of abstraction to describe what happens in the world,
1881840	1886720	you know, starting from quantum field theory to like atomic theory and molecules, you know,
1886720	1892720	and chemistry, materials, and, you know, all the way up to, you know, kind of concrete objects in
1892720	1900320	the real world and things like that. So we can't just only model everything at the lowest level.
1900400	1906800	And that's what the idea of JEPA is really about, learn abstract representation
1907520	1912080	in a self-supervised manner. And, you know, you can do it hierarchically as well.
1912080	1916960	So that, I think, is an essential component of an intelligent system. And in language,
1916960	1921840	we can get away without doing this because language is already, to some level, abstract
1922480	1926320	and already has eliminated a lot of information that is not predictable.
1927040	1932000	And so we can get away without doing the joint embedding, without, you know,
1932000	1935120	lifting the abstraction level and by directly predicting words.
1936240	1942880	So joint embedding, it's still generative, but it's generative in this abstract representation
1942880	1948320	space. And you're saying language, we were lazy with language because we already got the
1948320	1953200	abstract representation for free. And now we have to zoom out, actually think about generally
1953200	1960080	intelligent systems, we have to deal with a full mess of physical reality of reality.
1960080	1964560	And you can't, you do have to do this step of jumping from
1967920	1974720	the full, rich, detailed reality to a abstract representation of that reality
1974720	1977280	based on what you can then reason and all that kind of stuff.
1977280	1982000	Right. And the thing is, those self-supervised algorithms that learned by prediction,
1983200	1991040	even in representation space, they learned more concept if the input data you feed them
1991040	1995520	is more redundant. The more redundancy there is in the data, the more they're able to capture
1995520	2001360	some internal structure of it. And so there there is way more redundancy and structure in
2002160	2009600	perceptual inputs, sensory input, like vision than there is in text, which is not nearly as
2009600	2013920	redundant. This is back to the question you were asking a few minutes ago. Language might
2013920	2017520	represent more information really, because it's already compressed. You're right about that,
2017520	2023600	but that means it's also less redundant. And so self-supervised learning will not work as well.
2023600	2031760	Is it possible to join the self-supervised training on visual data and self-supervised
2031760	2038160	training on language data? There is a huge amount of knowledge, even though you talk down about those
2038160	2046640	10 to the 13 tokens. Those 10 to the 13 tokens represent the entirety, a large fraction of what
2046640	2053200	us humans have figured out, both the shit talk on Reddit and the contents of all the books and
2053200	2060400	the articles and the full spectrum of human intellectual creation. So is it possible to
2060400	2066960	join those two together? Well, eventually, yes. But I think if we do this too early,
2067680	2072000	we run the risk of being tempted to cheat. And in fact, that's what people are doing at the moment
2072000	2078560	with the vision language model. We're basically cheating. We're using language as a crutch to
2078560	2086400	help the deficiencies of our vision systems to learn good representations from images and video.
2087040	2093520	The problem with this is that we might improve our vision language system a bit.
2093520	2100080	I mean, our language models by feeding them images. But we're not going to get to the level of even
2100640	2106000	the intelligence or level of understanding of the world of a cat or a dog, which doesn't have
2106000	2111120	language. They don't have language, and they understand the world much better than any LLM.
2111920	2117120	They can plan really complex actions and imagine the result of a bunch of actions.
2117920	2123280	How do we get machines to learn that before we combine that with language? Obviously,
2123280	2130240	if we combine this with language, this is going to be a winner. But before that, we have to focus
2130240	2135440	on how do we get systems to learn how the world works. So this kind of joint embedding,
2136160	2141280	predictive architecture. For you, that's going to be able to learn something that common sense,
2141280	2149120	something like what a cat uses to predict how to mess with its owner most optimally by knocking over
2149120	2155760	a thing. That's the hope. In fact, the techniques we're using are non-contrastive. So not only is
2155760	2161280	the architecture non-generative, the learning procedures we're using are non-contrastive.
2161360	2166400	We have two sets of techniques. One set is based on distillation, and there's a number of
2167440	2176720	methods that use this principle. One by DeepMind called BYOL, a couple by Fair, one called Vicreg,
2177840	2183440	and another one called IJEPA. Vicreg, I should say, is not a distillation method, actually,
2183440	2190480	but IJEPA and BYOL certainly are. There's another one also called Dino or Dino, also produced from
2190480	2194480	at Fair. And the idea of those things is that you take the full input, let's say an image,
2195600	2202240	you run it through an encoder, produces a representation, and then you corrupt that
2202240	2206320	input or transform it, run it through the essentially what amounts to the same encoder
2206320	2211760	with some other differences, and then train a predictor. Sometimes the predictor is very simple,
2211760	2215360	sometimes it doesn't exist, but train a predictor to predict the representation of
2216320	2224400	the first uncorrupted input from the corrupted input. But you only train the second branch.
2225280	2231520	You only train the part of the network that is fed with the corrupted input. The other network,
2231520	2235840	you don't train, but it seems to share the same weight. When you modify the first one,
2235840	2241280	it also modifies the second one. And with various tricks, you can prevent the system from collapsing
2242240	2246240	with the collapse of the type I was explaining before with the system basically ignores the input.
2248080	2257760	So that works very well. The two techniques we developed at Fair, Dino and IJEPA work really
2257760	2264960	well for that. So what kind of data are we talking about here? So this several scenario, one scenario
2264960	2273760	is you take an image, you corrupt it by changing the cropping, for example, changing the size
2273760	2278720	a little bit, maybe changing the orientation, blurring it, changing the colors, doing all kinds
2278720	2283280	of horrible things to it. But basic horrible things? Basic horrible things that sort of degrade the
2283280	2291200	quality a little bit and change the framing, you know, crop the image. And in some cases,
2291200	2295120	in the case of IJEPA, you don't need to do any of this, you just mask some parts of it,
2295840	2303120	right? You just basically remove some regions like a big block, essentially. And then, you know,
2303120	2308160	run through the encoders and train the entire system, encoder and predictor, to predict the
2308160	2315280	representation of the good one from the representation of the corrupted one. So that's IJEPA. It
2315280	2319440	doesn't need to know that it's an image, for example, because the only thing it needs to know
2319440	2324960	is how to do this masking. Whereas with Dino, you need to know it's an image because you need to do
2324960	2329600	things like, you know, geometry transformation and blurring and things like that that are really
2329600	2335680	image specific. A more recent version of this that we have is called VJEPA. So it's basically the same
2335680	2341360	idea as IJEPA, except it's applied to video. So now you take a whole video and you mask a whole
2341360	2347280	chunk of it. And what we mask is actually kind of a temporal tube. So an all, like a whole segment
2347280	2352720	of each frame in the video over the entire video. And that tube was like statically positioned
2352720	2358720	throughout the frames? Right, throughout the tube. Yeah, typically it's 16 frames or something,
2358720	2364000	and we mask the same region over the entire 16 frames. It's a different one for every video,
2364000	2371280	obviously. And then again, train that system so as to predict the representation of the full video
2371280	2377040	from the partially masked video. That works really well. It's the first system that we have that
2377840	2382800	learns good representations of video so that when you feed those representations to a supervised
2384000	2388640	classifier head, it can tell you what action is taking place in the video with, you know,
2388640	2395840	pretty good accuracy. So that's the first time we get something of that quality.
2395840	2400160	So that's a good test that a good representation is formed. That means there's something to this?
2400160	2406960	Yeah. We also preliminary result that seemed to indicate that the representation allows us,
2406960	2413840	allow our system to tell whether the video is physically possible or completely impossible
2413840	2418800	because some object disappeared or an object, you know, suddenly jumped from one location to another
2419360	2427280	or change shape or something. So it's able to capture some physical, some physics-based constraints
2427280	2432400	about the reality represented in the video? Yeah. About the appearance and the disappearance of
2432400	2442960	objects. Yeah. That's really new. Okay. But can this actually get us to this kind of world model
2442960	2451040	that understands enough about the world to be able to drive a car? Possibly. This is going to take
2451040	2456880	a while before we get to that point. But there are systems already, you know, robotic systems that
2456880	2464800	are based on this idea. And what you need for this is a slightly modified version of this,
2464800	2474240	where imagine that you have a video and a complete video. And what you're doing to this video is that
2474240	2479520	you're either translating it in time towards the future. So you'll only see the beginning of the
2479520	2485040	video, but you don't see the latter part of it that is in the original one. Or you just mask
2485040	2492080	the second half of the video, for example. And then you train a JEPA system of the type I describe
2492080	2498320	to predict the representation of the full video from the shifted one. But you also feed the predictor
2498320	2504320	with an action. For example, you know, the wheel is turned 10 degrees to the right or something.
2505120	2511760	Right. So if it's a dash cam in a car, and you know the angle of the wheel, you should be able
2511760	2518720	to predict to some extent what's going to happen to what you see. You're not going to be able to
2518720	2525360	predict all the details of objects that appear in the view, obviously. But at a abstract representation
2525360	2532400	level, you can probably predict what's going to happen. So now what you have is a internal model
2532400	2538000	that says, here is my idea of state of the world at time t. Here is an action I'm taking. Here is
2538000	2542960	a prediction of the state of the world at time t plus one, t plus delta t, t plus two seconds,
2542960	2549520	whatever it is. If you have a model of this type, you can use it for planning. So now you can do
2549520	2555600	what LLMS cannot do, which is planning what you're going to do so as to arrive at a particular
2556960	2563120	outcome or satisfy a particular objective. Right. So you can have a number of objectives.
2564000	2572960	Right. I can predict that if I have an object like this, right, and I open my hand, it's going to
2572960	2579200	fall, right? And if I push it with a particular force on the table, it's going to move. If I push
2579200	2586000	the table itself, it's probably not going to move with the same force. So we have this internal model
2586000	2592800	of the world in our mind, which allows us to plan sequences of actions to arrive at a particular
2592800	2602000	goal. And so now if you have this world model, we can imagine a sequence of actions, predict what
2602000	2608240	the outcome of the sequence of action is going to be, measure to what extent the final state
2608240	2614720	satisfies a particular objective, like, you know, moving the bottle to the left of the table,
2616560	2621520	and then plan a sequence of actions that will minimize this objective at runtime. We're not
2621600	2625200	talking about learning. We're talking about inference time. Right. So this is planning, really.
2625920	2630560	And in optimal control, this is a very classical thing. It's called model predictive control. You
2630560	2636160	have a model of the system you want to control that, you know, can predict the sequence of states
2636160	2642880	corresponding to a sequence of commands. And you're planning a sequence of commands so that,
2642880	2650560	according to your world model, the end state of the system will satisfy an objective that you
2650560	2657280	fix. This is the way, you know, rocket trajectories have been planned since computers have been
2657280	2664000	around since the early 60s, essentially. So yes, for model predictive control, but you also often
2664000	2668800	talk about hierarchical planning. Yeah. Can hierarchical planning emerge from this somehow?
2668800	2674400	Well, so no, you will have to build a specific architecture to allow for hierarchical planning.
2674400	2679120	So hierarchical planning is absolutely necessary if you want to plan complex actions.
2680560	2684560	If I want to go from, let's say, from New York to Paris, this example I use all the time,
2685280	2691360	and I'm sitting in my office at NYU, my objective that I need to minimize is my distance to Paris
2691920	2698240	at a high level, a very abstract representation of my location. I would have to decompose this
2698240	2703680	into two sub goals. First one is go to the airport. Second one is catch a plane to Paris.
2704480	2711360	Okay, so my sub goal is now going to the airport. My objective function is my distance to the airport.
2712480	2717360	How do I go to the airport? Well, I have to go in the street and hail the taxi,
2718080	2723840	which you can do in New York. Okay, now I have another sub goal, go down on the street.
2724880	2729600	Well, that means going to the elevator, going down the elevator, walk out the street.
2729600	2737280	How do I go to the elevator? I have to stand up for my chair, open the door of my office,
2737920	2744080	go to the elevator, push the button. How do I get up from my chair? You can imagine going down all
2744080	2749200	the way down to basically what amounts to millisecond by millisecond muscle control.
2750320	2757360	Okay, and obviously you're not going to plan your entire trip from New York to Paris in terms of
2757360	2762080	millisecond by millisecond muscle control. First, that would be incredibly expensive,
2762080	2765600	but it will also be completely impossible because you don't know all the conditions
2766320	2770080	of what's going to happen, you know, how long it's going to take to catch a taxi
2771920	2776800	or to go to the airport with traffic, you know. I mean, you would have to know exactly
2776800	2781360	the condition of everything to be able to do this planning and you don't have the information.
2781360	2786080	So you have to do this hierarchical planning so that you can start acting and then sort of
2786080	2794800	replanning as you go. And nobody really knows how to do this in AI. Nobody knows how to train a
2794800	2799920	system to learn the appropriate multiple levels of representation so that hierarchical planning
2800560	2805040	works. There's something like that already emerged. So like, can you use an LLM,
2806560	2814080	state-of-the-art LLM, to get you from New York to Paris by doing exactly the kind of detailed
2814160	2820160	set of questions that you just did? Which is, can you give me a list of 10 steps I need to do
2821040	2826720	to get from New York to Paris? And then for each of those steps, can you give me a list of 10
2826720	2832080	steps, how I make that step happen? And for each of those steps, can you give me a list of 10 steps
2832080	2838240	to make each one of those until you're moving your individual muscles? Maybe not. Whatever you
2838240	2843520	can actually act upon using your mind. Right. So there's a lot of questions that are sort of
2843520	2848640	implied by this, right? So the first thing is LLMs will be able to answer some of those questions
2848640	2855040	down to some level of abstraction under the condition that they've been trained with similar
2855040	2860880	scenarios in the training set. They would be able to answer all those questions, but some of them
2861600	2865840	may be hallucinated, meaning nonfactual. Yeah, true. I mean, they will probably produce some
2865840	2869440	answer, except they're not going to be able to really kind of produce millisecond by millisecond
2869440	2874960	muscle control of how you stand up from your chair, right? So, but down to some level of
2874960	2879360	abstraction where you can describe things by words, they might be able to give you a plan,
2879360	2882480	but only under the condition that they've been trained to produce those kind of plans,
2883680	2888640	right? They're not going to be able to plan for situations where that they never encountered
2888640	2892800	before. They basically are going to have to regurgitate the template that they've been trained on.
2892800	2897360	But where, like just for the example of New York to Paris, is it going to start getting
2898000	2903040	into trouble? Like at which layer of abstraction do you think you'll start? Because like I can
2903040	2907600	imagine almost every single part of that LLM will be able to answer somewhat accurately,
2907600	2910960	especially when you're talking about New York and Paris major cities.
2910960	2915680	So, I mean, certainly LLM would be able to solve that problem if you fine tune it for it,
2916400	2923840	you know, just and so I can't say that LLM cannot do this. It can do this if you train it for it.
2923840	2931120	There's no question. Down to a certain level where things can be formulated in terms of words,
2931120	2935680	but like if you want to go down to like how you climb down the stairs or just stand up from your
2935680	2943280	chair in terms of words, like you can't do it. You need, that's one of the reasons you need
2944160	2948640	experience of the physical world, which is much higher bandwidth than what you can express in
2948640	2954000	words in human language. So everything we've been talking about on the joint embedding space,
2954960	2959200	is it possible that that's what we need for like the interaction with physical reality for
2959200	2965200	on the robotics front? And then just the LLMs are the thing that sits on top of it for the
2965200	2971520	bigger reasoning about like the fact that I need to book a plane ticket and I need to know
2971520	2976480	I know how to go to the websites and so on. Sure. And, you know, a lot of plans that people know
2976480	2982800	about that are relatively high level are actually learned. They're not people, most people don't
2982800	2992560	invent the, you know, plans. They, by themselves, they, you know, we have some ability to do this,
2992560	2998240	of course, obviously, but, but most plans that people use are plans that they've been
2998960	3002560	trained on, like they've seen other people use those plans or they've been told how to do things,
3002560	3009600	right? That you can't invent how you like take a person who's never heard of airplanes and
3009600	3013920	tell them like, how do you go from New York to Paris? And they're probably not going to be able
3013920	3017920	to kind of, you know, deconstruct the whole plan unless they've seen examples of that before.
3018640	3024720	So certainly LLMs are going to be able to do this, but, but then how you link this from the
3024720	3032240	the low level of actions that needs to be done with things like, like JPA that basically
3032240	3036800	lift the abstraction level of the representation without attempting to reconstruct the detail of
3036800	3044240	the situation. That's why we need JPAs for. I would love to sort of linger on your skepticism
3044240	3053440	around auto aggressive LLMs. So one way I would like to test that skepticism is everything you
3053440	3063120	say makes a lot of sense. But if I apply everything you said today and in general to like, I don't
3063120	3068560	know, 10 years ago, maybe a little bit less, no, let's say three years ago, I wouldn't be able to
3068560	3078000	predict the success of LLMs. So does it make sense to you that auto aggressive LLMs are able to be
3078080	3085520	so damn good? Yes. Can you explain your intuition? Because if I were to take
3086320	3092800	your wisdom and intuition at face value, I would say there's no way auto aggressive LLMs
3092800	3097440	one token at a time would be able to do the kind of things they're doing. No, there's one thing that
3097440	3103440	auto aggressive LLMs or that LLMs in general, not just the auto aggressive one, but including the
3103520	3109680	bird style by direction or ones are exploiting and it's self supervised learning and I've been a
3109680	3115200	very, very strong advocate of self supervised learning for many years. So those things are
3116080	3120880	a incredibly impressive demonstration that self supervised learning actually works.
3122000	3128480	The idea that, you know, started, it didn't start with with with bird, but it was really kind of
3128480	3134000	a good demonstration with this. So the the idea that, you know, you take a piece of text, you
3134000	3137600	corrupt it, and then you train some gigantic neural net to reconstruct the parts that are missing.
3139040	3140320	That has been an enormous,
3143360	3148560	produced an enormous amount of benefits. It allowed us to create systems that understand
3149520	3152320	understand language, systems that can translate,
3153200	3158560	hundreds of languages in any direction, systems that are multilingual. So they're not,
3159280	3162080	it's a single system that can be trained to understand hundreds of languages
3163040	3170720	and translate in any direction and produce summaries and then answer questions and produce text.
3171520	3177200	And then there's a special case of it where, you know, you, which is your progressive trick where
3177280	3182960	you constrain the system to not elaborate a representation of the text from looking at
3182960	3188400	the entire text, but only predicting a word from the words that are come before, right?
3188400	3191760	Then you do this by the constraining the architecture of the network. And that's
3191760	3197760	what you can build an autoregressive LLM from. So there was a surprise many years ago with
3198480	3203920	what's called decoder only LLM. So since, you know, systems of this type that are just trying to
3204480	3210080	produce words from the, from the previous one. And the fact that when you scale them up,
3211120	3217280	they tend to really kind of understand more about the, about language when you train them
3217280	3221280	on loss of data, you make them really big. That was kind of a surprise. And that surprise
3221280	3229280	occurred quite a while back, like, you know, with work from, you know, Google meta, open AI,
3229280	3236560	et cetera, you know, going back to, you know, the GPT kind of work general pretrained transformers.
3236560	3242560	You mean like GPT2? Like there's a certain place where you start to realize scaling might actually
3242560	3248160	keep giving us an emergent benefit. Yeah. I mean, there were, there were work from,
3248160	3254400	from various places, but if you want to kind of, you know, place it in the, in the GPT
3255760	3257360	timeline, that would be around GPT2.
3257680	3263760	Well, I just, because you said it, you're so charismatic and you said so many words, but
3263760	3270080	self-supervised learning. Yes. But again, the same intuition you're applying to saying that
3270080	3276960	autoregressive LLMs cannot have a deep understanding of the world. If we just apply that same
3276960	3283280	intuition, does it make sense to you that they're able to form enough of a representation in the
3283280	3290640	world to be damn convincing, essentially passing the original Turing test with flying colors?
3290640	3295520	Well, we're fooled by their fluency, right? We just assume that if a system is fluent
3296320	3300640	in manipulating language, then it has all the characteristics of human intelligence,
3300640	3305520	but that impression is false. We're really fooled by it.
3306400	3310800	What do you think Alan Turing would say? It, without understanding anything, just hanging out
3310800	3316160	with it. Alan Turing would decide that a Turing test is a really bad test. Okay. This is what
3316160	3320480	the AI community has decided many years ago, that the Turing test was a really bad test of
3320480	3326000	intelligence. What would Hans Moravek say about the, about the larger language models?
3326000	3332320	Hans Moravek would say that Moravek paradox still applies. Okay. Okay. Okay. We can pass.
3332320	3335760	You don't think he would be really impressed? No, of course, everybody would be impressed, but
3335840	3341920	it's not a question of being impressed or not. It's the question of knowing what the limit of
3341920	3347520	those systems can do. Again, they are impressive. They can do a lot of useful things. There's a
3347520	3352640	whole industry that is being built around them. They're going to make progress, but there's a
3352640	3357600	lot of things they cannot do, and we have to realize what they cannot do and then figure out
3358560	3366320	how we get there. And I'm not seeing this. I'm seeing this from basically 10 years of research
3368160	3373600	on the idea of self-supervised learning. Actually, that's going back more than 10 years,
3373600	3378080	but the idea of self-supervised learning. So basically capturing the internal structure
3378080	3383440	of a set of inputs without training the system for any particular task,
3383520	3390240	learning representations. The conference I co-founded 14 years ago is called International
3390240	3394640	Conference on Learning Representations. That's the entire issue that deep learning is dealing
3394640	3400880	with, right? And it's been my obsession for almost 40 years now. So learning representation is
3400880	3405520	really the thing. For the longest time, we could only do this with supervised learning.
3405520	3409600	And then we started working on what we used to call unsupervised learning
3409840	3417200	and sort of revive the idea of unsupervised learning in the early 2000s with
3417200	3421520	Yoshua Benjo and Jeff Hinton. Then discovered that supervised learning actually works pretty
3421520	3427280	well if you can collect enough data. And so the whole idea of unsupervised self-supervised
3427280	3433600	learning, can I took a backseat for a bit? And then I kind of tried to revive it
3434480	3444240	in a big way, starting in 2014 basically when we started FAIR and really pushing for finding new
3444240	3449280	methods to do self-supervised learning, both for text and for images and for video and audio.
3449840	3455440	And some of that work has been incredibly successful. The reason why we have multilingual
3455440	3461040	translation system thinks to do content moderation on Meta, for example, on Facebook,
3461600	3465760	that are multilingual to understand whether a piece of text is hate speech or not or something,
3466400	3470960	is due to that progress using self-supervised learning for NLP, combining this with
3470960	3475360	transformer architectures and blah, blah, blah. But that's the big success of self-supervised
3475360	3480080	learning. We had similar success in speech recognition, a system called Wave2Vec,
3480080	3483680	which is also a joint embedding architecture, by the way, trained with contrastive learning.
3483680	3490480	And that system also can produce speech recognition systems that are multilingual
3490480	3496000	with mostly unlabeled data and only need a few minutes of label data to actually do
3496000	3503040	speech recognition. That's amazing. We have systems now based on those combination of ideas that can
3503040	3508000	do real-time translation of hundreds of languages into each other, speech to speech.
3508000	3514240	Speech to speech, even including just fascinating languages that don't have written forms.
3514240	3515440	That's right. Just spoken only.
3515440	3519120	That's right. We don't go through text. It goes directly from speech to speech using an
3519120	3524480	internal representation of speech units that are discrete. But it's called text lesson LP.
3524480	3531280	We used to call it this way. But yeah, incredible success there. And then for 10 years, we tried
3531280	3537280	to apply this idea to learning representations of images by training a system to predict videos,
3537280	3541360	learning intuitive physics by training a system to predict what's going to happen in the video,
3542240	3548000	and tried and tried and failed and failed with generative models, with models that predict pixels.
3550000	3553440	We could not get them to learn good representations of images. We could not
3553440	3558160	get them to learn good representations of videos. We tried many times. We published lots of papers
3558160	3565520	on it. They kind of sort of work, but not really great. They started working. We abandoned this idea
3565520	3570960	of predicting every pixel and basically just doing the joint embedding and predicting in
3570960	3577760	representation space. That works. So there's ample evidence that we're not going to be able to
3578560	3583040	learn good representations of the real world using generative model.
3583040	3587840	So I'm telling people, everybody's talking about generative AI. If you're really interested in
3587840	3595120	human-level AI, abandon the idea of generative AI. Okay, but you really think it's possible to get
3595120	3602000	far with the joint embedding representation. So there's common sense reasoning, and then there's
3602080	3611520	high-level reasoning. I feel like those are two, the kind of reasoning that LLMs are able to do.
3611520	3616000	Okay, let me not use the word reasoning, but the kind of stuff that LLMs are able to do
3616000	3620720	seems fundamentally different than the common sense reasoning we use to navigate the world.
3620720	3625840	It seems like we're going to need both. Would you be able to get with the joint embedding,
3625840	3630160	which is a JAPA type of approach, looking at video? Would you be able to learn,
3632240	3635680	let's see, well, how to get from New York to Paris or
3638080	3645760	how to understand the state of politics in the world today? Right? These are things where various
3645760	3652080	humans generate a lot of language and opinions on in the space of language, but don't visually
3652080	3658480	represent that in any clearly compressible way. Right. Well, there's a lot of situations that
3658480	3667440	might be difficult for a purely language-based system to know. You can probably learn from
3667440	3673040	reading text, the entirety of the publicly available text in the world that I cannot get from New York
3673040	3680080	to Paris by slapping my fingers. That's not going to work. But there's probably sort of more complex
3680880	3687600	scenarios of this type, which an LLM may never have encountered and may not be able to determine
3687600	3695760	whether it's possible or not. So that link from the low level to the high level, the thing is
3695760	3702240	that the high level that language expresses is based on the common experience of the low level,
3703120	3709440	which LLMs currently do not have. When we talk to each other, we know we have a common experience
3710000	3718880	of the world. A lot of it is similar, and LLMs don't have that.
3718880	3723600	But see, it's present. You and I have a common experience of the world in terms of the physics
3723600	3730720	of how gravity works and stuff like this, and that common knowledge of the world,
3731600	3738800	I feel like is there in the language. We don't explicitly express it, but if you have a huge
3738800	3746000	amount of text, you're going to get this stuff that's between the lines. In order to
3747200	3751520	form a consistent world model, you're going to have to understand how gravity works,
3751520	3757280	even if you don't have an explicit explanation of gravity. So even though in the case of gravity,
3757280	3764320	there is explicit explanations of gravity and Wikipedia. But the stuff that we think of as
3764320	3770880	common sense reasoning, I feel like to generate language correctly, you're going to have to figure
3770880	3777120	that out. Now, you could say as you have, there's not enough text, sorry. Okay, so what? You don't
3777120	3780960	think so? No, I agree with what you just said, which is that to be able to do high level
3782960	3786880	common sense, to have high level common sense, you need to have the low level common sense to
3786880	3792400	build on top of. But that's not there. And that's not there in LLMs. LLMs are purely
3792480	3797600	trained from text. So then the other statement you made, I would not agree with the fact that
3798320	3803280	implicit in all languages in the world is the underlying reality. There's a lot of
3803280	3808800	that underlying reality, which is not expressed in language. Is that obvious to you? Yeah, totally.
3810240	3818240	So like all the conversations, okay, there's the dark web, meaning whatever, the private
3818240	3824080	conversations like DMs and stuff like this, which is much, much larger probably than what's
3824080	3828880	available, what LLMs are trained on. You don't need to communicate the stuff that is common.
3829840	3834480	But the humor, all of it, no, you do, like when you, you don't need to, but it comes through,
3834480	3840960	like if I accidentally knock this over, you'll probably make fun of me in the content of the
3840960	3848160	you making fun of me will be an explanation of the fact that cups fall, and then, you know,
3848160	3854080	gravity works in this way. And then you'll have some very vague information about what kind of
3854080	3859280	things explode when they hit the ground. And then maybe you'll make a joke about entropy or something
3859280	3864240	like this, and we'll never be able to reconstruct this again. Like, okay, you'll make a little
3864240	3869040	joke like this, and there'll be trillion of other jokes. And from the jokes, you can piece together
3869040	3873600	the fact that gravity works and mugs can break and all this kind of stuff, you don't need to see
3875280	3884080	it'll be very inefficient. It's easier for like, not to think over. But I feel like it would be
3884080	3890720	there if you have enough of that data. I just think that most of the information of this type that
3890800	3899200	we have accumulated when we were babies is just not present in text, in any description,
3899200	3904080	essentially. And the sensory data as much as a much richer source for getting that kind of
3904080	3910880	understanding. I mean, that's the 16,000 hours of wake time of a four year old, and 10 to the 15
3910880	3916880	bytes, you know, going through vision, just vision, right? There is a similar bandwidth, you know,
3916880	3923360	of touch and a little less through audio. And then text doesn't language doesn't come in until like,
3923360	3929040	you know, a year in life. And by the time you are nine years old, you've learned
3929920	3933680	about gravity, you know, about inertia, you know, about gravity, you know, the stability,
3933680	3938800	you know, you know, about the distinction between animate and animate objects, you know,
3938800	3945040	by 18 months, you know, about like, why people want to do things and you help them if they can't,
3945040	3948800	you know, I mean, there's a lot of things that you learn mostly by observation,
3949600	3954240	really, not even through interaction in the first few months of life, babies don't
3954240	3957040	don't really have any influence on the world, they can only observe, right?
3957920	3963360	And you accumulate like a gigantic amount of knowledge just just from that. So that that's
3963360	3969920	what we're missing from current AI systems. I think in one of your slides, you have this nice plot
3970000	3975280	that is one of the ways you show that LLMs are limited. I wonder if you could talk about
3975280	3983440	hallucinations from your perspectives, the why hallucinations happen from large language models
3983440	3988640	and why and to what degrees that a fundamental flaw of large language models.
3989360	3996960	Right. So because of the autoregressive prediction, every time an LLM produces a token or a word,
3997920	4003840	there is some level of probability for that word to take you out of the set of reasonable answers.
4005520	4010320	And if you assume, which is a very strong assumption that the probability of such error
4013120	4019680	is that those errors are independent across a sequence of tokens being produced. What that
4019680	4025520	means is that every time you produce a token, the probability that you stay within the set of
4025520	4030160	correct answer decreases and it decreases exponentially. So there's a strong assumption
4030160	4035600	there that if there's a non-zero probability of making a mistake, which there appears to be,
4036240	4042160	then there's going to be a kind of drift. Yeah. And that drift is exponential. It's like errors
4042160	4049440	accumulate, right? So the probability that an answer would be nonsensical increases exponentially
4049520	4054000	with the number of tokens. Is that obvious to you, by the way? Well,
4054960	4060480	so mathematically speaking, maybe, but isn't there a kind of gravitational pull towards
4060480	4068960	the truth because on average, hopefully, the truth is well represented in the training set?
4068960	4076480	No, it's basically a struggle against the curse of dimensionality. So the way you can correct
4076480	4082400	for this is that you fine-tune the system by having it produce answers for all kinds of questions
4082400	4088640	that people might come up with. And people are people. So a lot of the questions that they have
4088640	4094560	are very similar to each other. So you can probably cover 80% or whatever of questions
4094560	4103920	that people will ask by collecting data. And then you fine-tune the system to produce
4104000	4108480	good answers for all of those things. And it's probably going to be able to learn that because
4108480	4117680	it's got a lot of capacity to learn. But then there is the enormous set of prompts that you
4117680	4122560	have not covered during training. And that set is enormous. Like within the set of all possible
4122560	4128400	prompts, the proportion of prompts that have been used for training is absolutely tiny.
4129360	4135920	It's a tiny, tiny, tiny subset of all possible prompts. And so the system will behave properly
4135920	4142320	on the prompts that has been either trained, pre-trained, or fine-tuned. But then there is
4142320	4148640	an entire space of things that it cannot possibly have been trained on because the number is
4148640	4157920	gigantic. So whatever training the system has been subject to to produce appropriate answers,
4157920	4164560	you can break it by finding out a prompt that will be outside of the set of prompts that's
4164560	4170240	been trained on or things that are similar. And then it will just spew complete nonsense.
4170240	4175120	When you say prompt, do you mean that exact prompt? Or do you mean a prompt that's like
4175920	4183520	in many parts very different than? Is it that easy to ask a question or to say a thing that
4183520	4189520	hasn't been said before on the internet? I mean, people have come up with things where you put
4190640	4194960	essentially a random sequence of characters in the prompt. And that's enough to kind of throw the
4194960	4200720	system into a mode where it's going to answer something completely different than it would
4201280	4205920	have answered without this. So that's a way to jailbreak the system, basically get it,
4205920	4212160	go outside of its conditioning. So that's a very clear demonstration of it. But of course,
4214240	4220080	you know, that's that goes outside of what is designed to do, right? If you actually stitch
4220080	4225520	together reasonably grammatical sentences, is that is it that easy to break it?
4226400	4232240	Yeah, some people have done things like you you write a sentence in English, right, that has an
4232240	4237680	or you ask a question in English, and it produces a perfectly fine answer. And then you just substitute
4237680	4243760	a few words by the same word in another language. And all of a sudden, the answer is completely
4243760	4250400	nonsense. Yes. So I guess what I'm saying is like, which fraction of prompts that humans are likely
4251120	4256880	to generate are going to break the system. So the problem is that there is a long tail.
4257440	4263600	Yes. This is an issue that a lot of people have realized, you know, in social networks and stuff
4263600	4269360	like that, which is there's a very, very long tail of things that people will ask. And you can
4269360	4276240	fine tune the system for the 80% or whatever of the things that most people will ask. And then
4276240	4281040	this long tail is so large that you're not going to be able to fine tune the system for all the
4281040	4286560	conditions. And in the end, the system has been kind of a giant lookup table, essentially,
4286560	4291120	which is not really what you want. You want systems that can reason, certainly they can plan. So
4291200	4296320	the type of reasoning that takes place in LLM is very, very primitive. And the reason you can tell
4296320	4302800	is primitive is because the amount of computation that is spent per token produced is constant.
4303760	4309520	So if you ask a question, and that question has an answer in a given number of token,
4310080	4315200	the amount of computation devoted to computing that answer can be exactly estimated. It's like,
4316160	4322720	it's the size of the prediction network with its 36 layers or 92 layers or whatever it is,
4323760	4329600	multiplied by number of tokens. That's it. And so essentially, it doesn't matter if the question
4329600	4338320	being asked is simple to answer, complicated to answer, impossible to answer, because it's
4338320	4344400	undecidable or something. The amount of computation the system will be able to devote to the answer
4344400	4349920	is constant or is proportional to the number of token produced in the answer. This is not the
4349920	4358320	way we work. The way we reason is that when we're faced with a complex problem or complex question,
4358320	4363680	we spend more time trying to solve it and answer it because it's more difficult.
4363680	4371280	There's a prediction element. There's an iterative element where you're adjusting your
4371280	4375920	understanding of a thing by going over and over and over. There's a hierarchical element,
4375920	4382080	so on. Does this mean that it's a fundamental flaw of LLM? So does it mean that there's more
4382080	4389360	part to that question? Now you're just behaving like an LLM, immediately answering. No, that
4391120	4398000	it's just a low level world model on top of which we can then build some of these kinds of
4398080	4406400	mechanisms, like you said, persistent long-term memory or reasoning, so on. But we need that world
4406400	4413600	model that comes from language. Maybe it is not so difficult to build this kind of reasoning system
4413600	4420800	on top of a well-constructed world model. Whether it's difficult or not, the near future will say
4420800	4426400	because a lot of people are working on reasoning and planning abilities for dialogue systems.
4427360	4430320	I mean, even if we restrict ourselves to language,
4432480	4438960	just having the ability to plan your answer before you answer in terms that are not necessarily
4438960	4443600	linked with the language you're going to use to produce the answer. So this idea of this mental
4443600	4451600	model that allows you to plan what you're going to say before you say it. That is very important.
4451600	4455840	I think there's going to be a lot of systems over the next few years that are going to have this
4455840	4461840	capability. But the blueprint of those systems would be extremely different from autoregressive
4461840	4471200	LLMs. So it's the same difference as the difference between what psychologists call system one and
4471200	4477200	system two in humans. So system one is the type of task that you can accomplish without deliberately,
4477200	4484080	consciously think about how you do them. You've done them enough that you can just do it
4484080	4489040	subconsciously without thinking about them. If you're an experienced driver, you can drive
4489680	4493520	without really thinking about it and you can talk to someone at the same time or listen to the radio.
4495520	4500880	If you are a very experienced chess player, you can play against a non-experienced chess player
4500880	4506320	without really thinking either. You just recognize the pattern that you play. That's system one.
4508000	4512640	So all the things that you do instinctively without really having to deliberately plan and
4512640	4517280	think about it. And then there is all the tasks where you need to plan. So if you are not too
4517280	4522160	experienced chess player or you are experienced when you play against another experienced chess
4522160	4528720	player, you think about all kinds of options. You think about it for a while and you're much
4528720	4534400	better if you have time to think about it than you are if you play Blitz with limited time.
4534480	4542000	So this type of deliberate planning which uses your internal world model,
4543360	4547920	that's system two. This is what LLMs currently cannot do. So how do we get them to do this?
4548800	4555680	How do we build a system that can do this kind of planning or reasoning that devotes
4556480	4561600	more resources to complex problems than to simple problems? And it's not going to be
4561600	4568720	autoregressive prediction of tokens. It's going to be more something akin to inference of latent
4568720	4576720	variables in what used to be called probabilistic models or graphical models and things of that
4576720	4584320	type. So basically the principle is like this. The prompt is like observed variables.
4584800	4595040	And what the model does is that it's basically a measure of, it can measure to what extent
4595040	4601040	an answer is a good answer for a prompt. So think of it as some gigantic neural net,
4601040	4606960	but it's got only one output and that output is a scalar number which is, let's say zero,
4606960	4612000	if the answer is a good answer for the question and a large number if the answer is not a good
4612000	4617200	answer for the question. Imagine you had this model. If you had such a model, you could use it to
4617200	4623840	produce good answers. The way you would do is produce the prompt and then search through
4623840	4631440	the space of possible answers for one that minimizes that number. That's called an energy-based model.
4631440	4637600	But that energy-based model would need the model constructed by the LLM?
4638400	4644880	Well, so really what you need to do would be to not search over possible strings of text that
4644880	4651600	minimize that energy. But what you would do is do this in abstract representation space. So in
4651600	4658080	sort of the space of abstract thoughts, you would elaborate a thought using this process
4658720	4666320	of minimizing the output of your model, which is just a scalar. It's an optimization process.
4666320	4670560	So now the way the system produces its answer is through optimization,
4672400	4677520	by minimizing an objective function, basically. And this is, we're talking about inference,
4677520	4681680	we're not talking about training. The system has been trained already. So now we have an
4681680	4685920	abstract representation of the thought of the answer, representation of the answer.
4686480	4692960	We feed that to basically an autoreactive decoder, which can be very simple, that turns this into
4693040	4698560	a text that expresses this thought. So that, in my opinion, is the blueprint of future
4699120	4705760	data systems. They will think about their answer, plan their answer by optimization
4705760	4710480	before turning it into text. And that is turning complete.
4711120	4716560	Can you explain exactly what the optimization problem there is? What's the objective function?
4717520	4723600	Just link on it, you kind of briefly described it, but over what space are you optimizing?
4723600	4724960	The space of representations.
4725680	4727040	Goes abstract representation.
4727040	4731760	Abstract representation. So you have an abstract representation inside the system. You have a
4731760	4735760	prompt. The prompt goes through an encoder, produces a representation, perhaps goes through
4735760	4739920	a predictor that predicts a representation of the answer, of the proper answer. But that
4739920	4746080	representation may not be a good answer because there might be some complicated
4746080	4753200	reasoning you need to do. So then you have another process that takes the representation
4753200	4760960	of the answers and modifies it so as to minimize a cost function that measures to what extent
4760960	4769520	the answer is a good answer for the question. Now, we sort of ignore the issue for a moment
4769520	4775760	of how you train that system to measure whether an answer is a good answer for a question.
4775840	4781680	But suppose such a system could be created, but what's the process, this kind of search-like
4781680	4786800	process? It's an optimization process. You can do this if the entire system is differentiable,
4787600	4794320	that scalar output is the result of running through some neural net, running the answer,
4794320	4797920	the representation of the answer through some neural net, then by gradient descent,
4797920	4803360	by backpropagating gradients, you can figure out how to modify the representation of the
4803360	4809680	answer. So that's still a gradient-based inference. So now you have a representation
4809680	4817920	of the answer in abstract space. Now you can turn it into text. And the cool thing about this is that
4818560	4824000	the representation now can be optimized through gradient descent, but also is independent of
4824000	4829280	the language in which you're going to express the answer. So you're operating in the subtract
4829280	4834000	representation. I mean, this goes back to the joint embedding that is better to work in the
4835280	4842720	space of, I don't know, to romanticize the notion like space of concepts versus the space of concrete
4844480	4851120	sensory information. Okay, but can this do something like reasoning, which is what we're
4851120	4855840	talking about? Well, not really. Only in a very simple way. I mean, basically, you can think of
4855840	4861200	those things as doing the kind of optimization I was talking about, except they optimize in
4861200	4866960	the discrete space, which is the space of possible sequences of tokens. And they do this
4866960	4871760	optimization in a horribly inefficient way, which is generate a lot of hypotheses and then select
4871760	4879840	the best ones. And that's incredibly wasteful in terms of computation. Because you basically
4879840	4886640	have to run your LLM for like every possible, you know, generating sequence. And it's incredibly
4886640	4894320	wasteful. So it's much better to do an optimization in continuous space, where you can do gradient
4894320	4898320	descent, as opposed to like generate tons of things and then select the best, you just
4899040	4904000	iteratively refine your answer to go towards the best, right? That's much more efficient.
4904000	4908000	But you can only do this in continuous spaces with differentiable functions.
4908000	4913760	You're talking about the reasoning, like ability to think deeply or to reason deeply.
4915040	4924560	How do you know what is an answer that's better or worse based on deep reasoning?
4924560	4928640	Right. So then we're asking the question of conceptually, how do you train an energy-based
4928640	4933520	model, right? So an energy-based model is a function with a scalar output, just a number.
4933680	4940560	You give it two inputs, x and y, and it tells you whether y is compatible with x or not. x,
4940560	4947920	you observe, let's say it's a prompt, an image, a video, whatever. And y is a proposal for an answer,
4947920	4954160	a continuation of the video, you know, whatever. And it tells you whether y is compatible with x.
4954960	4959120	And the way it tells you that y is compatible with x is that the output of that function would be
4959120	4965600	zero. If y is compatible with x, it would be a positive number, non-zero, if y is not compatible
4965600	4973120	with x. Okay, how do you train a system like this at a completely general level is you show it
4973920	4977760	pairs of x and y that are compatible, a question and a corresponding answer.
4978560	4983440	And you train the parameters of the big neural net inside to produce zero.
4984400	4988880	Okay, now that doesn't completely work because the system might decide, well,
4988880	4993760	I'm just going to say zero for everything. So now you have to have a process to make sure that for
4994480	5000720	a wrong y, the energy would be larger than zero. And there you have two options. One is
5000720	5006880	contrastive methods. So contrastive method is you show an x and a bad y. And you tell the system,
5006880	5011120	well, that's, you know, give a high energy to this, like push up the energy, right? Change the
5011120	5017440	weights in the neural net that confuse the energy so that it goes up. So that's contrastive methods.
5017440	5024480	The problem with this is if the space of y is large, the number of such contrastive samples
5024480	5032160	you're going to have to show is gigantic. But people do this. They do this when you train
5032160	5037440	a system with RLHF, basically what you're training is what's called a reward model,
5037440	5042560	which is basically an objective function that tells you whether an answer is good or bad. And
5042560	5048880	that's basically exactly what this is. So we already do this to some extent. We're just not
5048880	5056000	using it for inference. We're just using it for training. There is another set of methods which
5056000	5061520	are non-contrastive. And I prefer those. And those non-contrastive methods basically say,
5062400	5069440	okay, the energy function needs to have low energy on pairs of x, y's that are compatible,
5069440	5074400	that come from your training set. How do you make sure that the energy is going to be higher
5074400	5083200	everywhere else? And the way you do this is by having a regularizer, a criterion, a term in
5083200	5089520	your cost function that basically minimizes the volume of space that can take low energy.
5090480	5094240	And the precise way to do this is all kinds of different specific ways to do this depending
5094240	5098800	on the architecture. But that's the basic principle. So that if you push down the energy
5098800	5104640	function for particular regions in the x, y space, it will automatically go up in other places
5104640	5111280	because it is only a limited volume of space that can take low energy, okay, by the construction
5111280	5114720	of the system or by the regularizer, regularizing function.
5115360	5122480	We've been talking very generally, but what is a good x and a good y? What is a good representation
5122480	5129520	of x and y? Because we've been talking about language and if you just take language directly,
5130320	5134960	that presumably is not good. So there has to be some kind of abstract representation of ideas.
5136000	5143040	Yeah, so you can do this with language directly by just x is a text and y is a continuation of
5143040	5149280	that text. Yes. Or x is a question, y is the answer. But you're saying that's not going to
5149280	5155360	take it. I mean, that's going to do what LLMs are doing. Well, no, it depends on how the internal
5155360	5159840	structure of the system is built. If the internal structure of the system is built in such a way
5159840	5168560	that inside of the system, there is a latent variable, this could be z, that you can manipulate
5169280	5176560	so as to minimize the output energy, then that z can be viewed as a representation of a good answer
5176560	5179120	that you can translate into a y that is a good answer.
5180960	5183600	So this kind of system could be trained in a very similar way?
5184400	5189280	Very similar way, but you have to have this way of preventing collapse, of ensuring that
5190160	5197600	there is high energy for things you don't train it on. And currently, it's very
5197680	5201440	implicit in LLM. It's done in a way that people don't realize it's being done, but it is being
5201440	5209680	done. It's due to the fact that when you give a high probability to a word, automatically,
5209680	5214800	you give low probability to other words because you only have a finite amount of probability
5214800	5220240	to go around right there, to some to one. So when you minimize the cross entropy or whatever,
5220240	5227040	when you train your LLM to predict the next word, you're increasing the
5227040	5230080	probability your system will give to the correct word, but you're also decreasing the
5230080	5237600	probability it will give to the incorrect words. Now, indirectly, that gives a high probability
5237600	5241280	to sequences of words that are good and low probability to sequences of words that are
5241280	5246800	bad, but it's very indirect. And it's not obvious why this actually works at all, but
5248160	5252720	because you're not doing it on a joint probability of all the symbols in a sequence,
5252720	5259440	you're just doing it kind of factorize that probability in terms of conditional probabilities
5259440	5264960	over successive tokens. So how do you do this for visual data? So we've been doing this with
5264960	5272400	OJEPA architectures basically with IJEPA. So there, the compatibility between two things is,
5272960	5278720	here's an image or a video. Here's a corrupted, shifted or transformed version of that image
5278720	5287760	or video or masked. And then the energy of the system is the prediction error of the
5289280	5294560	representation, the predicted representation of the good thing,
5294560	5300880	versus the actual representation of the good thing. So you run the corrupted image to the system,
5300880	5306400	predict the representation of the good input and corrupted, and then compute the prediction error.
5306400	5311600	That's the energy of the system. So this system will tell you, this is a good,
5313520	5318000	you know, this is a good image and this is a corrupted version. It will give you zero energy
5318000	5323120	if those two things are effectively, one of them is a corrupted version of the other,
5323120	5326480	give you a high energy if the two images are completely different.
5326480	5331440	And hopefully that whole process gives you a really nice compressed representation of
5332400	5337520	reality, of visual reality. And we know it does because then we use those for our presentations
5337520	5341520	as input to a classification system. And then that classification system works really nicely.
5341520	5350320	Okay. Well, so to summarize, you recommend in a spicy way that only Yalakoon can,
5350320	5354880	you recommend that we abandon generative models in favor of joint embedding architectures.
5354880	5360320	Yes. Abandon auto-aggressive generation. Yes. Abandon problem. This feels like
5360320	5365040	court testimony. Abandon probabilistic models in favor of energy-based models,
5365040	5369440	as we talked about, abandon contrastive methods in favor of regularized methods.
5370080	5376240	And let me ask you about this. You've been for a while a critic of reinforcement learning.
5376240	5383520	Yes. So the last recommendation is that we abandon RL in favor of model predictive control,
5383520	5389520	as you were talking about, and only use RL when planning doesn't yield the predicted outcome.
5390320	5396080	And we use RL in that case to adjust the world model or the critic. Yes. So
5397520	5404720	you mentioned RLHF, reinforcement learning with human feedback. Why do you still hate
5404720	5409200	reinforcement learning? I don't hate reinforcement learning. And I think it should not be
5410480	5416400	abandoned completely. But I think its use should be minimized because it's incredibly inefficient
5416480	5423040	in terms of samples. And so the proper way to train a system is to first have it learn
5424800	5429440	good representations of the world and world models from mostly observation,
5429440	5433840	maybe a little bit of interactions. And then steered based on that. If the representation
5433840	5438880	is good, then the adjustments should be minimal. Yeah. Now there's two things. You can use,
5438880	5442720	if you've learned a world model, you can use the world model to plan a sequence of actions to
5442720	5450240	arrive at a particular objective. You don't need RL unless the way you measure whether you succeed
5450240	5458960	might be an exact. Your idea of whether you're going to fall from your bike might be wrong.
5459680	5463760	Or whether the person you're fighting with MMA was going to do something and then do something else.
5465440	5471920	So there's two ways you can be wrong. Either your objective function
5472800	5477920	does not reflect the actual objective function you want to optimize, or your world model is
5477920	5483840	inaccurate. So the prediction you were making about what was going to happen in the world
5483840	5490160	is inaccurate. So if you want to adjust your world model while you are operating the world,
5490800	5497440	or your objective function, that is basically in the realm of RL. This is what RL deals with
5498240	5502320	to some extent. So adjust your world model. And the way to adjust your world model,
5502960	5510000	even in advance, is to explore parts of the space where you know that your world model is inaccurate.
5510560	5518640	That's called curiosity, basically, or play. When you play, you explore parts of the state space
5518640	5527360	that you don't want to do for real because it might be dangerous, but you can adjust your world
5527360	5536000	model without killing yourself, basically. So that's what you want to use RL for. When it comes
5536000	5540800	down to learning a particular task, you already have all the good world presentations, you already
5540800	5546480	have your world model, but you need to adjust it for the situation at hand. That's when you use RL.
5546560	5552480	What do you think RLHF works so well, this enforcement learning with human feedback?
5552480	5558000	What did it have such a transformational effect on large language models it came before?
5558000	5563840	What's had the transformational effect is human feedback. There is many ways to use it, and some
5563840	5567280	of it is just purely supervised, actually. It's not really reinforcement learning.
5567280	5573520	So it's the HF. It's the HF. And then there is various ways to use human feedback, right? So you
5574240	5579760	can ask humans to rate answers, multiple answers that are produced by a world model,
5580640	5587040	and then what you do is you train an objective function to predict that rating,
5588160	5593840	and then you can use that objective function to predict whether an answer is good, and you can
5593840	5599120	back propagate gradient through this to fine tune your system so that it only produces highly-rated
5599120	5608320	answers. So that's one way. In RL, that means training what's called a reward model,
5609200	5613680	so something that basically a small neural net that estimates to what extent an answer is good.
5614880	5620320	It's very similar to the objective I was talking about earlier for planning, except now it's not
5620320	5625360	used for planning. It's used for fine-tuning your system. I think it would be much more efficient
5625440	5632480	to use it for planning, but currently it's used to fine-tune the parameters of the system.
5632480	5639600	Now, there are several ways to do this. Some of them are supervised. You just ask a human person,
5639600	5647200	like, what is a good answer for this? Then you just type the answer. There's lots of ways that
5647200	5654320	those systems are being adjusted. Now, a lot of people have been very critical of the recently
5654320	5664080	released Google's Gemini 1.5 for essentially, in my words, I could say super woke, woke in the
5664080	5670240	negative connotation of that word. There are some almost hilariously absurd things that it does,
5670240	5677840	like it modifies history, like generating images of a Black George Washington, or
5678400	5686800	perhaps more seriously something that you commented on Twitter, which is refusing to comment on or
5686800	5695920	generate images of, or even descriptions of Tiananmen Square or the Tankman, one of the most
5695920	5705360	sort of legendary protest images in history. Of course, these images are highly censored by
5705360	5711120	the Chinese government. Therefore, everybody started asking questions of what is the process of
5713200	5718240	designing these LLMs, what is the role of censorship and all that kind of stuff.
5718960	5727520	So you commented on Twitter saying that open source is the answer, essentially. So can you
5727520	5736160	explain? I actually made that comment on just about every social network I can. I've made that
5736160	5745280	point multiple times in various forums. Here's my point of view on this. People can complain that
5745280	5751920	AI systems are biased, and they generally are biased by the distribution of the training data
5751920	5763040	that they've been trained on that reflects biases in society, and that is potentially offensive to
5763040	5772800	some people, or potentially not. And some techniques to de-bias then become offensive to some people
5773040	5780640	because of historical incorrectness and things like that.
5783040	5787200	And so you can ask the question, you can ask two questions. The first question is,
5787200	5792400	is it possible to produce an AI system that is not biased? And the answer is absolutely not.
5793120	5798000	And it's not because of technological challenges, although there are
5798960	5805200	technological challenges to that. It's because bias is in the eye of the beholder.
5806640	5813600	Different people may have different ideas about what constitutes bias for a lot of things. I mean,
5813600	5820400	there are facts that are indisputable, but there are a lot of opinions or things that can be expressed
5820400	5826080	in different ways. And so you cannot have an unbiased system that's just an impossibility.
5828560	5835760	And so what's the answer to this? And the answer is the same answer that we've found
5836480	5843840	in liberal democracy about the press. The press needs to be free and diverse.
5845280	5854720	We have free speech for a good reason, is because we don't want all of our information to come from
5854720	5863600	a unique source, because that's opposite to the whole idea of democracy and progress of ideas and
5863600	5870400	even science. In science, people have to argue for different opinions and science makes progress
5870400	5875680	when people disagree and they come up with an answer and a consensus forms. And it's true in
5875680	5882560	all democracies around the world. So there is a future, which is already happening,
5883200	5890080	where every single one of our interaction with the digital world will be mediated by AI systems.
5892800	5898000	We're going to have smart glasses. You can already buy them from Meta, the Ray-Ban Meta,
5898000	5903520	where you can talk to them and they are connected with an LLM and you can get answers
5903520	5909680	on any question you have. Or you can be looking at a monument and there is a camera in the
5910560	5915440	system that in the glasses, you can ask it, like, what can you tell me about this building,
5915440	5920080	on this monument? You can be looking at a menu in a foreign language and I think we'll translate
5920080	5926240	it for you or we can do real-time translation if we speak different languages. So a lot of our
5926240	5930560	interactions with the digital world are going to be mediated by those systems in the near future.
5933040	5938080	Increasingly, the search engines that we're going to use are not going to be search engines. They're
5938080	5946400	going to be dialogue systems that we just ask a question and it will answer and then point you to
5946400	5951920	perhaps an appropriate reference for it. But here is the thing. We cannot afford those systems to
5951920	5958560	come from a handful of companies on the west coast of the US because those systems will
5958560	5966080	constitute the repository of all human knowledge and we cannot have that be controlled by a small
5966080	5970720	number of people, right? It has to be diverse. For the same reason, the press has to be diverse.
5972080	5979280	So how do we get a diverse set of AI assistance? It's very expensive and difficult to train a
5979280	5983760	base model, right? A base LLM at the moment, you know, in the future might be something different,
5983760	5990720	but at the moment that's an LLM. So only a few companies can do this properly. And
5991040	5996960	if some of those top systems are open source, anybody can use them. Anybody can fine tune them.
5996960	6008160	If we put in place some systems that allows any group of people, whether they are individual
6008160	6018560	citizens, groups of citizens, government organizations, NGOs, companies, whatever, to take those open
6019520	6026320	source systems, AI systems and fine tune them for their own purpose on their own data,
6027520	6032800	they were going to have a very large diversity of different AI systems that are specialized for
6032800	6038400	all of those things, right? So I tell you, I talked to the French government quite a bit and the
6038400	6045600	French government will not accept that the digital diet of all their citizens be controlled by
6045600	6050080	three companies in the West Coast of the US. That's just not acceptable. It's a danger to
6050080	6058080	democracy, regardless of how well-intentioned those companies are, right? And so, and it's also
6058080	6070640	a danger to local culture, to values, to language, right? I was talking with the founder of Infosys
6070640	6078960	in India. He's funding a project to fine tune Lamatu, the open source model produced by META,
6079760	6086240	so that Lamatu speaks all 22 official languages in India. It's very important for people in India.
6086240	6090560	I was talking to a former colleague of mine, Mustafa Assisi, who used to be a scientist at fair,
6091120	6095360	and then moved back to Africa. I created a research lab for Google in Africa and now
6096000	6100880	is as a new startup called CARA. And what he's trying to do is basically have LLM that speaks
6100880	6106400	the local languages in Senegal so that people can have access to medical information because they
6106400	6113920	don't have access to doctors. It's a very small number of doctors per capita in Senegal. I mean,
6113920	6119040	you can't have any of this unless you have open source platforms. So with open source platforms,
6119040	6124000	you can have AI systems that are not only diverse in terms of political opinions or things of that
6124000	6132720	type, but in terms of language, culture, value systems, political opinions,
6135360	6142080	technical abilities in various domains. And you can have an industry, an ecosystem of companies
6142080	6147200	that fine tune those open source systems for vertical applications in industry, right? You
6147200	6152000	have, I don't know, a publisher has thousands of books, and they want to build a system that
6152000	6156720	allows a customer to just ask a question about any, about the content of any of their books.
6157440	6163120	You need to train on their proprietary data, right? You have a company, we have one within
6163120	6168080	Meta, it's called MetaMate, and it's basically an LLM that can answer any question about internal
6169760	6175680	stuff about the company. Very useful. A lot of companies want this, right? A lot of companies
6175680	6179920	want this not just for their employees, but also for their customers to take care of their customers.
6180640	6185440	So the only way you're going to have an AI industry, the only way you're going to have
6185440	6190880	AI systems that are not uniquely biased is if you have open source platforms on top of which
6192240	6202800	any group can build specialized systems. So the direction of inevitable direction of history
6202800	6208320	is that the vast majority of AI systems will be built on top of open source platforms.
6208320	6216800	So that's a beautiful vision. So meaning like a company like Meta or Google or so on
6217840	6223920	should take only minimal fine-tuning steps after the building the foundation pre-trained model
6224720	6232960	as few steps as possible. Basically. Can Meta afford to do that? No. So I don't know if you
6232960	6240000	know this, but companies are supposed to make money somehow, and open source is giving away,
6240560	6251440	I don't know, Mark made a video, Mark Zuckerberg, very sexy video talking about 350,000 NVIDIA H100s.
6253040	6261280	The math of that is just for the GPUs, that's 100 billion, plus the infrastructure for training
6261280	6268640	everything. So I'm no business guy. But how do you make money on that? So the division you paint
6268640	6275120	is a really powerful one. But how is it possible to make money? Okay, so you have several business
6275120	6286800	models, right? The business model that Meta is built around is your first service. And the
6286800	6293200	financing of that service is either through ads or through business customers. So for example,
6293200	6302080	if you have an LLM that can help a mom and pop pizza place by talking to the customers
6302080	6307920	with WhatsApp, and so the customers can just order a pizza and the system will just, you know,
6307920	6313920	has them like what topping do you want or websites, blah, blah, blah. The business will pay for that.
6314000	6323760	Okay, that's a model. And otherwise, you know, if it's a system that is on the more kind of classical
6323760	6328880	services, it can be ad supported or, you know, the several models. But the point is,
6330400	6336000	if you have a big enough potential customer base, and you need to build that system
6337360	6343200	anyway, for them, it doesn't hurt you to actually distribute it in open source.
6343200	6348000	Again, I'm no business guy. But if you release the open source model,
6348000	6355280	then other people can do the same kind of task and compete on it, basically provide fine tune
6355280	6361360	models for businesses. As the bet that Meta is making, by the way, I'm a huge fan of all this,
6361360	6364960	but as the bet that Meta is making is like, we'll do a better job of it.
6365520	6372720	Well, no, the bet is more we have, we already have a huge user base and customer base.
6373760	6378320	So it's going to be useful to them, whatever we offer them is going to be useful. And there is
6378320	6386320	a way to derive revenue from this. And it doesn't hurt that we provide that system
6387680	6394560	or the base model, the foundation model in open source for others to build applications on top
6394560	6399280	of it too. If those applications turn out to be useful for our customers, we can just buy it from
6399280	6406480	them. It could be that they will improve the platform. In fact, we see this already. I mean,
6406480	6413600	there is literally millions of downloads of Lama 2 and thousands of people who have provided ideas
6413600	6420880	about how to make it better. So this clearly accelerates progress to make the system available to
6422240	6427760	it's sort of a wide community of people. And there is literally thousands of businesses who
6427760	6438160	are building applications with it. So our ability to, Meta's ability to derive revenue
6438160	6446320	from this technology is not impaired by the distribution of it, of base models in open source.
6446320	6450880	The fundamental criticism that Gemini is getting is that, as you pointed out on the west coast,
6450880	6457280	just to clarify, we're currently in the east coast where I would suppose Meta AI headquarters would
6457280	6465600	be. So there are strong words about the west coast, but I guess the issue that happens is
6466720	6473440	I think it's fair to say that most tech people have a political affiliation with the left wing.
6473440	6479520	They lean left. And so the problem that people are criticizing Gemini with is that there's
6479520	6488480	in that debiasing process that you mentioned that their ideological lean becomes obvious.
6489840	6496960	Is this something that could be escaped? You're saying open source is the only way.
6496960	6502560	Have you witnessed this kind of ideological lean that makes engineering difficult?
6502560	6506640	No, I don't think it has to do, I don't think the issue has to do with the political leaning
6506640	6514800	of the people designing those systems. It has to do with the acceptability or political leanings
6514800	6522800	of their customer base or audience. So a big company cannot afford to offend too many people.
6523600	6530080	So they're going to make sure that whatever product they put out is safe, whatever that means.
6530080	6539200	And it's very possible to overdo it. And it's also very possible to, it's impossible to do it
6539200	6544080	properly for everyone. You're not going to satisfy everyone. So that's what I said before. You cannot
6544080	6551600	have a system that is unbiased and is perceived as unbiased by everyone. You push it in one way,
6551600	6555600	one set of people are going to see it as biased, and then you push it the other way,
6555600	6559680	and another set of people is going to see it as biased. And then in addition to this,
6559760	6564160	there's the issue of if you push the system perhaps a little too far in one direction,
6564160	6571280	it's going to be nonfactual, right? You're going to have Black Nazi soldiers in the...
6571280	6578880	Yes, we should mention image generation of Black Nazi soldiers, which is not factually accurate.
6578880	6586880	Right. And can be offensive for some people as well. So it's going to be impossible to
6586880	6591440	kind of produce systems that are unbiased for everyone. So the only solution that I see is
6591440	6596880	diversity. And diversity in full meaning of that word, diversity in every possible way.
6599200	6608240	Mark Andreessen just tweeted today, let me do a TLDR. The conclusion is only startups and open
6608240	6614880	source can avoid the issue that he's highlighting with Big Tech. He's asking, can Big Tech actually
6614880	6621760	field generative AI products? One, ever escalating demands from internal activists, employee mobs,
6621760	6627120	crazed executives, broken boards, pressure groups, extremist regulators, government agencies,
6627120	6635600	the press, in quotes, experts, and everything corrupting the output? Two, constant risk of
6635600	6642480	generating a bad answer or drawing a bad picture or rendering a bad video. Who knows what is going
6642560	6649040	to say or do at any moment. Three, legal exposure, product liability, slander, election law, many
6649040	6656560	other things, and so on. Anything that makes Congress mad. Four, continuous attempts to
6656560	6661040	tighten grip on acceptable output to grade the model, like how good it actually is,
6662400	6666800	in terms of usable and pleasant to use and effective and all that kind of stuff.
6666880	6672720	And five, publicity of bad text, images, video, actual puts those examples into the training
6672720	6679280	data for the next version, and so on. So he just highlights how difficult this is from all kinds
6679280	6686160	of people being unhappy. He said you can't create a system that makes everybody happy. So if you're
6686160	6693200	going to do the fine-tuning yourself and keep a closed source, essentially the problem there is
6693200	6698640	then trying to minimize the number of people who are going to be unhappy. And you're saying
6698640	6704480	that almost impossible to do right, and that's the better ways to do open source.
6705520	6714560	Basically, yeah. Mark is right about a number of things that he lists that indeed scare large
6714560	6724880	companies. Certainly congressional investigations is one of them. Legal liability, making things
6724880	6734160	that get people to hurt themselves or hurt others. Big companies are really careful about not producing
6734160	6741840	things of this type because they don't want to hurt anyone, first of all, and then second,
6741840	6746960	don't want to preserve their business. So it's essentially impossible for systems like this to
6746960	6753120	can inevitably formulate political opinions and opinions about various things that may be
6753120	6763360	political or not, but that people may disagree about moral issues and questions about religion
6763360	6768400	and things like that, or cultural issues that people from different communities would disagree
6768400	6772960	with in the first place. So there's only kind of a relatively small number of things that people
6772960	6780640	will agree on basic principles. But beyond that, if you want those systems to be useful,
6781760	6787920	they will necessarily have to offend a number of people inevitably.
6789040	6793200	And so open source is just better. And then diversity is better, right?
6793200	6797520	And open source enables diversity. That's right. Open source enables diversity.
6798080	6802800	That's going to be fascinating world where if it's true that the open source world,
6802800	6807440	if Meta Lee is the way and the crisis kind of open source foundation model world,
6807440	6814560	there's going to be like governments will have a fine two model. And then potentially,
6816400	6821040	you know, people that vote left and right will have their own model and preference to be able to
6821040	6827600	choose. And it will potentially divide us even more. But that's on us humans, we get to figure
6827600	6835200	out basically the technology enables humans to human more effectively. And all the difficult
6835200	6841680	ethical questions that humans raise will just leave it up to us to figure it out.
6842480	6846000	Yeah. I mean, there are some limits to what, you know, the same way there are limits to free
6846080	6851360	speech, there has to be some limit to the kind of stuff that those systems might be authorized to
6853200	6858560	produce, you know, some guardrails. So I mean, that's one thing I've been interested in, which is
6859680	6865360	in the type of architecture that we were discussing before, where the output of the system
6866080	6871680	is a result of an inference to satisfy an objective that objective can include guardrails.
6872640	6879040	And we can put guardrails in open source systems. I mean, if we eventually have systems that are
6879040	6885040	built with this blueprint, we can put guardrails in those systems that guarantee that there is
6885040	6890960	sort of a minimum set of guardrails that make the system non-dangerous and non-toxic, etc.
6890960	6897280	You know, basic things that everybody would agree on. And then, you know, the fine tuning that
6897360	6901840	people will add or the additional guardrails that people will add will kind of cater to their
6903040	6909040	community, whatever it is. And yeah, the fine tuning will be more about the gray areas of what is
6909040	6912800	hate speech, what is dangerous and all that kind of stuff. I mean, you've- With different value
6912800	6917600	systems. Still value systems. I mean, like, but still even with the objectives of how to build a
6917600	6922080	bio weapon, for example, I think something you've commented on, or at least there's a paper
6923040	6928480	where a collection of researchers is trying to understand the social impacts of these LLMs.
6929280	6937040	And I guess one threshold is nice. It's like, does the LLM make it any easier than a search
6937040	6945360	would, like a Google search would? Right. So the increasing number of studies on this seems to point
6945360	6955680	to the fact that it doesn't help. So having an LLM doesn't help you design or build a bio weapon
6955680	6960800	or a chemical weapon, if you already have access to, you know, a search engine and a library.
6962320	6966480	And so the sort of increased information you get or the ease with which you get it doesn't really
6966480	6972560	help you. That's the first thing. The second thing is, it's one thing to have a list of instructions
6972560	6978160	of how to make a chemical weapon, for example, a bio weapon. It's another thing to actually
6978720	6983680	build it. And it's much harder than you might think. And then LLM will not help you with that.
6985600	6989680	In fact, you know, nobody in the world, not even like, you know, countries use bio weapons because
6990800	6996000	most of the times they have no idea to protect their own populations against it. So it's too
6996000	7003200	dangerous actually to kind of ever use. And it's in fact banned by international treaties.
7004000	7011280	Chemical weapons is different. It's also banned by treaties. But it's the same problem. It's difficult
7011280	7017760	to use in situations that doesn't turn against the perpetrators. But we could ask Elon Musk,
7017760	7023120	like I can give you a very precise list of instructions of how you build a rocket engine.
7024080	7028480	And even if you have a team of 15 engineers sort of re experience building it, you're still
7028480	7033840	going to have to blow up a dozen of them before you get one that works. And, you know, it's the same
7033840	7041280	with, you know, chemical weapons or bio weapons or things like this. It requires expertise,
7041280	7045120	you know, in the real world that LLM is not going to help you with.
7045120	7050960	And it requires even the common sense expertise that we've been talking about, which is how to take
7051360	7058000	language based instructions and materialize them in the physical world requires a lot of
7058000	7064000	knowledge. It's not in the instructions. Yeah, exactly. A lot of biologists have posted on this
7064000	7068560	actually in response to those things saying like, do you realize how hard it is to actually do the
7068560	7075520	lab work? And I can know this is not trivial. Yeah. And that's Hans Mervic comes comes to light
7075520	7081120	once again. Just the linger on llama, you know, Mark announced that llama three is coming out
7081120	7087040	eventually, I don't think there's a release date. But what, what are you most excited about? First
7087040	7091840	of all, llama two that's already out there, and maybe the future llama three, four, five, six,
7091840	7099520	10, just the future of the open source under meta. Well, number of things. So there's going to be
7099520	7107280	like various versions of llama that are, you know, improvements of previous llamas, bigger,
7107280	7113440	better, multimodal, things like that. And then in future generations, systems that are capable of
7113440	7118240	planning that really understand how the world works, maybe are trained from video, so they have
7118240	7122960	some world model, maybe, you know, capable of the type of reasoning and planning I was talking
7122960	7127600	about earlier, like, how long is that going to take? Like, when is the research that is going
7127680	7134000	in that direction going to sort of feed into the product line? If you want a llama, I don't know,
7134000	7137680	I can tell you, and there's, you know, a few breakthroughs that we have to basically
7139120	7146160	go through before we can get there. But you'll be able to monitor our progress because we publish
7146160	7152720	our research, right? So, you know, if last week we published the VJPA work, which is sort of a
7152720	7158880	first step towards training systems from video. And then the next step is going to be world models
7158880	7164960	based on kind of this type of idea, training, training from video. There's similar work at
7164960	7173680	a deep mind also, and taking place people, and also at UC Berkeley on world models from video.
7173680	7177600	A lot of people are working on this. I think a lot of good ideas are coming, are appearing.
7178400	7182320	My bet is that those systems are going to be JEPA-like, they're not going to be
7182880	7191440	generating models. And we'll see what the future will tell. There's really good work at
7194080	7197600	a gentleman called Daniel J. Hafner, who is not a deep mind, who has worked on
7197600	7201360	kind of models of this type that learn representations and then use them for
7201360	7207520	planning or learning tasks by reinforcement learning. And a lot of work at Berkeley by
7208800	7214640	Peter I. Beal, Saagya Levine, a bunch of other people of that type. I'm collaborating with actually
7214640	7222560	in the context of some grants with my NYU hat. And then collaborations also through META because
7223680	7229840	the lab at Berkeley is associated with META in some way, so with fair. So I think it's very
7229840	7236560	exciting. I think I'm super excited about, I haven't been that excited about the direction
7236560	7242800	of machine learning and AI since 10 years ago when fair was started. And before that,
7243920	7251680	30 years ago, we were working on, what's it, 35 on computational nets and the early days of neural
7251680	7258960	nets. So I'm super excited because I see a path towards potentially human level intelligence.
7260240	7268720	With systems that can understand the world, remember, plan, reason. There is some set
7268720	7273200	of ideas to make progress there that might have a chance of working. And I'm really excited about
7273200	7283280	this. What I like is that we get onto a good direction and perhaps succeed before my brain
7283280	7291280	turns to a white sauce or before I need to retire. Yeah. You're also excited by,
7294240	7301200	is it beautiful to you just the amount of GPUs involved, the whole training process on this
7301200	7307920	much compute? Just zooming out, just looking at Earth and humans together have built these
7308000	7315280	computing devices and are able to train this one brain. Then we then open source,
7317520	7324240	like giving birth to this open source brain trained on this gigantic compute system.
7324240	7330000	There's just the details of how to train on that, how to build the infrastructure and the hardware,
7330000	7335120	the cooling, all of this kind of stuff. Are you just still the most of your excitement is in the
7335120	7341360	theory aspect of it? Meaning like the software? Well, I used to be a hardware guy many years ago.
7343040	7350160	Hardware has improved a little bit, changed a little bit. I mean, certainly scale is necessary,
7350720	7355840	but not sufficient. Absolutely. So we certainly need computation. I mean, we're still far in terms
7355840	7361760	of compute power from what we would need to match the compute power of the human brain.
7362720	7368160	This may occur in the next couple of decades, but we're still some ways away. And certainly in
7368160	7375200	terms of power efficiency, we're really far. So there's a lot of progress to make in hardware.
7377440	7382960	Right now, a lot of progress is not, I mean, there's a bit coming from Silicon technology,
7382960	7387440	but a lot of it coming from architectural innovation and quite a bit coming from
7387760	7393520	more efficient ways of implementing the architectures that have become popular,
7393520	7401280	basically combination of transformers and components. So there's still some ways to go
7402160	7410000	until we're going to saturate, we're going to have to come up with new principles,
7410000	7418160	new fabrication technology, new basic components, perhaps based on sort of different
7418160	7425120	principles and those classical digital CMOS. Interesting. So you think in order to build
7426080	7431760	AMI, I mean, we need, we potentially might need some hardware innovation too.
7432720	7437520	Well, if we want to make it ubiquitous, yeah, certainly, because we're going to have to reduce
7437520	7445280	the power consumption. A GPU today is half a kilowatt to a kilowatt.
7446720	7453600	Human brain is about 25 watts. And a GPU is way below the power of human brain. You need
7453600	7459760	something like 100,000 or a million to match it. So we are off by a huge factor here.
7460320	7467520	You often say that AGI is not coming soon, meaning like not this year,
7468400	7475040	not the next few years, potentially farther away. What's your basic intuition behind that?
7475680	7481920	So first of all, it's not going to be an event. The idea somehow, which is popularized by
7481920	7486880	science fiction and Hollywood, that somehow somebody is going to discover the secret,
7486960	7491600	the secret to AGI or human level AI or AMI, whatever you want to call it.
7492240	7497040	And then, you know, turn on a machine and then we have a GI. That's just not going to happen.
7497040	7504480	It's not going to be an event. It's going to be gradual progress. Are we going to have systems
7504480	7510560	that can learn from video, how the world works and learn good world presentations? Yeah. Before
7510560	7514640	we get them to the scale and performance that we observe in humans, it's going to take quite a
7514640	7522880	while. It's not going to happen in one day. Are we going to get systems that can have large
7522880	7527760	amount of associative memory so that they can remember stuff? Yeah, but same. It's not going
7527760	7531680	to happen tomorrow. I mean, there is some basic techniques that need to be developed. We have
7531680	7536480	a lot of them, but like, you know, to get this to work together with a full system is another
7536480	7540400	story. Are we going to have systems that can reason and plan perhaps along the lines of
7541360	7546640	objective-driven AI architectures that I described before? Yeah, but like before we get this to work,
7546640	7550720	you know, properly, it's going to take a while. So, and before we get all those things to work
7550720	7555040	together and then on top of this have systems that can learn like hierarchical planning,
7555040	7559840	hierarchical representations, systems that can be configured for a lot of different
7559840	7566320	situation at hand, the way the human brain can. You know, all of this is going to take,
7566320	7571040	you know, at least a decade and probably much more because there are a lot of problems that
7571040	7576080	we're not seeing right now that we have not encountered. And so we don't know if there is
7576080	7583520	a easy solution within this framework. So, you know, it's not just around the corner. I mean,
7583520	7589360	I've been hearing people for the last 12, 15 years claiming that, you know, AGI is just around
7589360	7594240	the corner and being systematically wrong. And I knew they were wrong when they were saying it.
7594240	7598960	I called their bullshit. Why do you think people have been calling, first of all, I mean, from the
7598960	7605040	beginning, from the birth of the term artificial intelligence, there has been a eternal optimism
7606160	7614000	that's perhaps unlike other technologies. Is it a Morovox paradox? Is the explanation for why
7614000	7619440	people are so optimistic about AGI? I don't think it's just Morovox paradox. Morovox paradox is a
7619520	7624320	consequence of realizing that the world is not as easy as we think. So first of all,
7626480	7631280	intelligence is not a linear thing that you can measure with a scalar, with a single number.
7632720	7640480	You know, can you say that humans are smarter than orangutans? In some ways, yes. But in some
7640480	7645920	ways, orangutans are smarter than humans in a lot of domains that allows them to survive in the forest,
7645920	7650480	for example. So IQ is a very limited measure of intelligence. Do you
7650480	7655360	intelligence is bigger than what IQ, for example, measures? Well, IQ can measure, you know,
7656480	7663840	approximately something for humans. But because humans kind of, you know, come in
7664640	7674320	relatively kind of uniform form, right? But it only measures one type of ability that, you know,
7674320	7681440	maybe relevant for some tasks, but not others. And, but then if you're talking about other
7681440	7687360	intelligent entities for which the, you know, the basic things that are easy to them is very
7687360	7696400	different, then it doesn't mean anything. So intelligence is a collection of skills
7697360	7706080	and an ability to acquire new skills efficiently, right? And the collection of skills that any
7706080	7710960	intelligent, particular intelligent entity possess or is capable of learning quickly,
7710960	7716400	is different from the collection of skills of another one. And because it's a multi-dimensional
7716400	7721040	thing, the set of skills is high dimensional space, you can't measure, you can compare,
7721040	7724720	you cannot compare two things as to whether one is more intelligent than the other.
7725520	7734560	It's multi-dimensional. So you push back against what are called AI doomers a lot.
7735760	7738880	Can you explain their perspective and why you think they're wrong?
7739600	7745920	Okay, so AI doomers imagine all kinds of catastrophy scenarios of how AI could escape
7745920	7753680	or control and basically kill us all. And that relies on a whole bunch of
7753760	7759280	assumptions that are mostly false. So the first assumption is that the emergence of
7759280	7763520	superintelligence is going to be an event that at some point we're going to have,
7763520	7767280	we're going to figure out the secret and we'll turn on a machine that is super intelligent.
7768160	7771920	And because we've never done it before is going to take over the world and kill us all.
7772880	7777040	That is false. It's not going to be an event. We're going to have systems that are like
7778000	7784000	as smart as a cat, have all the characteristics of human level intelligence,
7784640	7789440	but their level of intelligence would be like a cat or a parrot maybe or something.
7791200	7795600	And then we're going to walk our way up to kind of make those things more intelligent. And as we
7795600	7799280	make them more intelligent, we're also going to put some guardrails in them and learn how to kind
7799280	7803600	of put some guardrails so they behave properly. And we're not going to do this with just one,
7803680	7806880	it's not going to be one effort that is going to be lots of different people doing this.
7807440	7810880	And some of them are going to succeed at making intelligent systems that are
7812000	7815840	controllable and safe and have the right guardrails. And if some other goes rogue,
7815840	7822400	then we can use the good ones to go against the rogue ones. So it's going to be my smart
7822400	7828320	AI police against your rogue AI. So it's not going to be like, we're going to be exposed to a single
7828320	7833120	rogue AI that's going to kill us all. That's just not happening. Now there is another fallacy,
7833120	7837680	which is the fact that because the system is intelligent, it necessarily wants to take over.
7840480	7846560	And there is several arguments that make people scared of this, which I think are completely
7846560	7855440	false as well. So one of them is, in nature, it seems to be that the more intelligent species
7855440	7863040	are the one that end up dominating the other. And even, you know, extinguishing the others,
7863680	7871200	sometimes by design, sometimes just by mistake. And so, you know, there is sort of
7872560	7877280	thinking by which you say, well, if AI systems are more intelligent than us,
7877280	7882880	surely they're going to eliminate us if not by design, simply because they don't care about us.
7883600	7888640	And that's just preposterous for a number of reasons. First reason is they're not going to be a
7888640	7893760	species. They're not going to be a species that competes with us. They're not going to have the
7893760	7899360	desire to dominate because the desire to dominate is something that has to be hardwired into an
7899360	7906960	intelligent system. It is hardwired in humans. It is hardwired in baboons, in chimpanzees, in wolves,
7907520	7916880	not in orangutans. The species in which this desire to dominate or submit or attain status in
7916880	7924400	other ways is specific to social species. Non-social species like orangutans don't have it.
7925040	7931280	And they are as smart as we are, almost. And to you, there's not significant incentive for humans
7932000	7939040	to encode that into the AI systems. And to the degree they do, there'll be other AI's that
7940320	7942800	sort of punish them for it. I'll compete them over.
7942800	7947520	Well, there's all kinds of incentive to make AI systems submissive to humans, right? I mean,
7947520	7952560	this is the way we're going to build them, right? And so then people say, oh, but look at LLMs,
7952560	7957760	LLMs are not controllable. And they're right. LLMs are not controllable. But object-driven AI,
7957840	7963600	so systems that derive their answers by optimization of an objective,
7963600	7967360	means they have to optimize its objective. And that objective can include guardrails.
7968080	7976320	One guardrail is obey humans. Another guardrail is don't obey humans if it's hurting other humans.
7977520	7979360	I've heard that before somewhere. I don't remember.
7979360	7981600	Yes, maybe in a book.
7981920	7989040	Yeah. But speaking of that book, could there be unintended consequences also from all of this?
7989040	7994560	No, of course. So this is not a simple problem, right? I mean, designing those guardrails so
7994560	8001680	that the system behaves properly is not going to be a simple issue for which there is a silver
8001680	8006400	bullet, for which you have a mathematical proof that the system can be safe. It's going to be
8006400	8011840	very progressive iterative design system where we put those guardrails in such a way that the
8011840	8016960	system behaves properly. And sometimes they're going to do something that was unexpected,
8016960	8020160	because the guardrail wasn't right, and we're going to correct them so that they do it right.
8020960	8025280	The idea somehow that we can't get it slightly wrong, because if we get it slightly wrong,
8025280	8034640	we all die, is ridiculous. We're just going to go progressively. And the analogy I've used many
8034640	8045280	times is turbojet design. How did we figure out how to make turbojets so unbelievably reliable,
8045920	8052000	right? I mean, those are incredibly complex pieces of hardware that run at really high
8052000	8062240	temperatures for 20 hours at a time sometimes. And we can fly halfway around the world on a two-engine
8064960	8070320	jetliner at near the speed of sound. Like, how incredible is this? It's just unbelievable, right?
8070960	8076960	And did we do this because we invented like a general principle of how to make turbojet safe?
8076960	8082640	No, it took decades to kind of fine tune the design of those systems so that they were safe.
8083200	8090240	Is there a separate group within General Electric or SNACMA or whatever that
8091200	8100000	is specialized in turbojet safety? No, the design is all about safety, because a better turbojet
8100000	8107120	is also a safer turbojet. So a more reliable one. It's the same for AI. Do you need specific
8107120	8111680	provisions to make AI safe? No, you need to make better AI systems, and they will be safe because
8111680	8118880	they are designed to be more useful and more controllable. So let's imagine a system, AI system,
8118880	8127040	that's able to be incredibly convincing and can convince you of anything. I can at least imagine
8127040	8135360	such a system, and I can see such a system be weapon-like because it can control people's minds.
8135360	8140960	We're pretty gullible. We want to believe a thing. You can have an AI system that controls it,
8140960	8147280	and you could see governments using that as a weapon. So do you think if you imagine such a system,
8148240	8158560	there's any parallel to something like nuclear weapons? No. So why is that technology different?
8158560	8164640	So you're saying there's going to be gradual development? Yeah. It might be rapid, but there'll
8164640	8171280	be iterative, and then we'll be able to respond and so on. So that AI system designed by Vladimir
8171280	8182800	Putin or whatever, or his minions is going to be trying to talk to every American to convince
8182800	8197040	them to vote for whoever pleases Putin or whatever, or riled people up against each other as they've
8197040	8202480	been trying to do. They're not going to be talking to you. They're going to be talking to your AI
8202480	8210960	assistant, which is going to be as smart as theirs. That AI, because as I said, in the future,
8210960	8215600	every single one of your interactions with the digital world will be mediated by your AI assistant.
8215600	8219680	So the first thing you're going to ask is, is this a scam? Like is this thing like turning me
8219680	8224880	to the truth? It's not even going to be able to get to you because it's only going to talk to
8224880	8231280	your AI assistant. It's not even going to, it's going to be like a spam filter. You're not even
8231280	8237600	seeing the spam email. It's automatically put in a folder that you never see. It's going to be the
8237600	8242400	same thing. That AI system that tries to convince you of something is going to be talking to your
8242400	8250480	assistant, which is going to be at least as smart as it is going to say, this is spam. It's not even
8250480	8255440	going to bring it to your attention. So to you, it's very difficult for any one AI system to take
8255440	8262320	such a big leap ahead to where you can convince even the other AI systems. So there's always going
8262320	8269200	to be this kind of race where nobody's way ahead. That's the history of the world. History of the
8269200	8277040	world is whenever there is a progress someplace, there is a countermeasure and it's a cat and mouse
8277440	8282240	This is why, mostly yes, but this is why nuclear weapons are so interesting because
8282240	8290000	that was such a powerful weapon that it mattered who got it first. That you could imagine
8291040	8300480	Hitler, Stalin, Mao getting the weapon first and that having a different kind of impact on the world
8301360	8309600	than the United States getting the weapon first. To you, nuclear weapons, you don't imagine a
8310720	8318320	breakthrough discovery and then Manhattan project like for AI. No, as I said, it's not going to be
8318320	8326400	an event. It's going to be continuous progress and whenever one breakthrough occurs, it's going
8326400	8331680	to be widely disseminated really quickly. Probably first within industry. I mean, this is not a
8331680	8338480	domain where government or military organizations are particularly innovative and they're in fact
8338480	8344560	way behind. And so this is going to come from industry and this kind of information disseminates
8344560	8351680	extremely quickly. We've seen this over the last few years where you have a new, even take Alpha
8351680	8357200	Go, this was reproduced within three months, even without particularly detailed information.
8358000	8363840	This is an industry that's not good at secrecy. Even if there is, just the fact that you know
8363840	8370960	that something is possible makes you realize that it's worth investing the time to actually do it.
8370960	8381120	You may be the second person to do it, but you'll do it. And say for all the innovations
8382080	8387360	of self-supervisioning, transformers, decoder-only architectures, LLMs, I mean those things,
8387360	8391200	you don't need to know exactly the details of how they worked. You know that it's possible
8392640	8397920	because it's deployed and then it's getting reproduced. And then people who work for those
8397920	8404960	companies move. They go from one company to another and the information disseminates.
8404960	8411440	What makes the success of the US tech industry and Silicon Valley in particular is exactly
8411440	8416320	that, is because the information circulates really, really quickly and this disseminates
8416320	8423600	very quickly. And so the whole region is ahead because of that circulation of information.
8424880	8431760	Maybe just to linger on the psychology of AI doomers, you give in the classic Yanlacoon way
8431760	8439440	a pretty good example of just when a new technology comes to be. You say engineer says,
8439440	8446320	I invented this new thing. I call it a ball pen. And then the Twitter sphere responds,
8446320	8450960	OMG, people could write horrible things with it like misinformation, propaganda, hate speech,
8450960	8459680	ban it now. Then writing doomers come in akin to the AI doomers. Imagine if everyone can get a
8459680	8464640	ball pen. This could destroy society. There should be a law against using ball pen to write hate
8464720	8471680	speech, regulate ball pens now. And then the pencil industry mogul says, yeah, ball pens are very
8471680	8479040	dangerous. Unlike pencil writing, which is erasable ball pen writing stays forever. Government should
8479040	8487360	require a license for a pen manufacturer. I mean, this does seem to be part of human psychology
8488080	8496640	when it comes up against new technology. What deep insights can you speak to about this?
8497600	8505840	Well, there is a natural fear of new technology and the impact it can have on society. And people
8505840	8515440	have kind of instinctive reaction to the world they know being threatened by major transformations
8516240	8522880	that are either cultural phenomena or technological revolutions. And they fear for
8523920	8529680	their culture, they fear for their job, they fear for their future of their children
8531280	8540960	and the way of life. So any change is feared. And you see this a long history, like any
8541040	8545600	technological revolution or cultural phenomenon was always accompanied by
8548000	8556240	groups or reaction in the media that basically attributed all the problems,
8556240	8562560	the current problems of society to that particular change. Electricity was going to kill everyone
8563440	8569120	at some point. The train was going to be a horrible thing because you can't breathe
8569120	8575280	past 50 kilometers an hour. And so there's a wonderful website called a pessimist archive
8577280	8583040	which has all those newspaper clips of all the horrible things people imagine would arrive because
8583040	8594800	of either technological innovation or a cultural phenomenon. It's just wonderful examples of
8595040	8605600	jazz or comic books being blamed for an employment or young people not wanting to
8605600	8610400	work anymore and things like that. And that has existed for centuries.
8612960	8623360	And it's knee-jerk reactions. The question is, do we embrace change or do we resist it?
8624000	8630160	And what are the real dangers as opposed to the imagined ones?
8631680	8636480	So people worry about, I think one thing they worry about with big tech, something we've been
8636480	8644000	talking about over and over, but I think worth mentioning again, they worry about how powerful
8644000	8650320	AI will be and they worry about it being in the hands of one centralized power or just
8650320	8657200	a handful of central control. And so that's the skepticism with big tech. These companies can
8657200	8665520	make a huge amount of money and control this technology and by so doing, take advantage,
8666560	8671760	abuse the little guy in society. Well, that's exactly why we need open source platforms.
8671760	8679440	Yeah, I just wanted to nail the point home more and more. So let me ask you on your,
8680480	8685760	like I said, you do get a little bit flavorful on the internet.
8686640	8692480	Yoshibak tweeted something that you LOL'd at in reference to Hal 9000.
8693840	8699600	I appreciate your argument and I fully understand your frustration, but whether the pod bay doors
8699600	8706480	should be opened or closed is a complex and nuanced issue. So you're at the head of meta AI.
8709200	8716720	This is something that really worries me that AI or AI overlords will speak down to us with
8716720	8724880	corporate speak of this nature and you sort of resist that with your way of being. Is this
8724880	8732320	something you can just comment on sort of working at a big company? How you can avoid the
8734480	8740800	overfearing, I suppose, through caution, create harm?
8741280	8747200	Yeah. Again, I think the answer to this is open source platforms and then enabling
8748080	8755760	widely diverse set of people to build AI assistance that represent the diversity of
8756320	8761520	cultures, opinions, languages and value systems across the world. So that you're not bound to just
8763120	8769760	be brainwashed by a particular way of thinking because of a single AI entity.
8770640	8776480	So I mean, I think it's really, really important question for society. And the problem I'm seeing is
8777600	8785040	that, which is why I've been so vocal and sometimes a little sardonic about it.
8785040	8789280	Never stop. Never stop, yeah. We love it.
8789280	8797360	It's because I see the danger of this concentration of power through proprietary AI systems as a much
8797360	8806400	bigger danger than everything else. That if we really want diversity of opinion AI systems that
8807760	8814160	in the future where we'll all be interacting through AI systems, we need those to be diverse
8814160	8823520	for the preservation of diversity of ideas and creeds and political opinions and whatever
8825920	8833520	and the preservation of democracy. And what works against this is people who think that
8833520	8839680	for reasons of security, we should keep AI systems under lock and key because it's too
8839680	8846000	dangerous to put it in the hands of everybody because it could be used by terrorists or something.
8848800	8858160	That would lead to potentially a very bad future in which all of our information
8858160	8862880	diet is controlled by a small number of companies through proprietary systems.
8864240	8872560	Do you trust humans with this technology to build systems that are on the whole good for
8872560	8876400	humanity? Isn't that what democracy and free speech is all about?
8876400	8882480	I think so. Do you trust institutions to do the right thing? Do you trust people to do the right
8882480	8886720	thing? And yeah, there's bad people who are going to do bad things, but they're not going to have
8886720	8891920	superior technology to the good people. So then it's going to be my good AI against your bad AI.
8893600	8902160	Examples that we were just talking about of maybe some rogue country will build some AI system
8902160	8910160	that's going to try to convince everybody to go into a civil war or something or elect a favorable
8911040	8915520	ruler, but then they will have to go past our AI systems.
8916320	8920160	An AI system with a strong Russian accent will be trying to convince us.
8920160	8922960	And doesn't put any articles in their sentences.
8925600	8934480	Well, it'll be at the very least absurdly comedic. Since we talked about the physical
8934480	8940480	reality, I'd love to ask your vision of the future with robots in this physical reality.
8940480	8947200	So many of the kinds of intelligence you've been speaking about would empower robots to be more
8947200	8955200	effective collaborators with us humans. So since Tesla's Optimus team has been showing us some
8955200	8961200	progress on human robots, I think it really reinvigorated the whole industry that I think
8961280	8965120	Boston Dynamics has been leading for a very, very long time. So now there's all kinds of
8965120	8973600	companies figure AI, obviously Boston Dynamics, Unitree, but there's like a lot of them. It's
8973600	8982560	great. It's great. I mean, I love it. So do you think there'll be millions of human robots walking
8982560	8987680	around soon? Not soon, but it's going to happen. Like the next decade, I think it's going to be
8988160	8994080	interesting in robots. The emergence of the robotics industry has been
8995040	8999360	in the waiting for 10, 20 years without really emerging other than for
9000400	9007680	kind of pre-programmed behavior and stuff like that. And the main issue is, again,
9007680	9012320	the Moravec paradox. How do we get the systems to understand how the world works and kind of
9012400	9015440	plan actions? And so we can do it for really specialized tasks.
9017680	9022640	And the way Boston Dynamics goes about it is basically with a lot of
9023600	9030880	handcrafted dynamical models and careful planning in advance, which is very classical robotics with
9030880	9037040	a lot of innovation, a little bit of perception. But it's still not like they can't build a domestic
9037040	9045920	robot. And we're still some distance away from completely autonomous level 5 driving.
9048000	9054320	And we're certainly very far away from having level 5 autonomous driving by a system that can
9054320	9066080	train itself by driving 20 hours like any 17-year-old. So until we have, again,
9066800	9070720	world models, systems that can train themselves to understand how the world works,
9072800	9079440	we're not going to have significant progress in robotics. So a lot of the people working on
9079440	9085840	robotic hardware at the moment are betting or banking on the fact that AI is going to
9085840	9090640	make sufficient progress towards that. And they're hoping to discover a product in it too.
9092400	9097360	Before you have a really strong world model, there'll be an almost strong world model.
9099920	9102720	People are trying to find a product in a clumsy robot, I suppose,
9103600	9107440	like not a perfectly efficient robot. So there's the factory setting where
9107520	9112160	human robots can help automate some of the aspects of the factory. I think that's a
9112160	9115920	crazy difficult task because of all the safety required and all this kind of stuff.
9115920	9119120	I think in the home is more interesting. But then you start to think,
9120320	9126560	I think you mentioned loading the dishwasher, right? I suppose that's one of the main problems
9126560	9136800	you're working on. I mean, cleaning up the house, clearing up the table after a meal.
9137920	9143840	Washing the dishes, all those tasks, cooking. All the tasks that in principle could be automated,
9143840	9147360	but are actually incredibly sophisticated, really complicated.
9148080	9151920	But even just basic navigation around an unspaceful of uncertainty.
9151920	9156400	That sort of works. You can sort of do this now. Navigation is fine.
9157120	9162240	Well, navigation in a way that's compelling to us humans is a different thing.
9162800	9167440	Yeah, it's not going to be necessarily. I mean, we have demos, actually, because there is a
9168560	9175040	so-called embodied AI group at fair. And they've been not building their own robots,
9175040	9182160	but using commercial robots. And you can tell the robot dog go to the fridge,
9182160	9186160	and they can actually open the fridge, and they can probably pick up a can in the fridge and stuff
9186160	9193600	like that and bring it to you. So you can navigate, you can grab objects, as long as it's been
9193600	9196880	trying to recognize them, which vision systems work pretty well nowadays.
9198320	9206320	But it's not like a completely general robot that would be sophisticated enough to do things like
9207680	9214160	clearing up the dinner table. Yeah, to me, that's an exciting future of getting human
9214240	9219600	robots, robots in general in the home more and more, because that gets humans to really directly
9219600	9225120	interact with AI systems in the physical space. And so doing it allows us to philosophically,
9225120	9230560	psychologically explore our relationships with robots can be really, really, really interesting.
9230560	9234160	So I hope you make progress on the whole JAPA thing soon.
9234160	9238240	Well, I mean, I hope things kind of work as planned.
9239200	9244160	And I mean, again, we've been kind of working on this idea of self supervised running
9245040	9252000	from video for 10 years, and you know, only made significant progress in the last two or three.
9252000	9255600	And actually, you've mentioned that there's a lot of interesting breakers that can happen
9255600	9260800	without having access to a lot of compute. So if you're interested in doing a PhD and this kind
9260800	9266400	of stuff, there's a lot of possibilities still to do innovative work. So like what advice would
9266400	9271360	you give to a undergrad that's looking to go to grad school and do a PhD?
9272240	9278320	So basically, I've listed them already, this idea of how do you train a world model by observation.
9279680	9283760	And you don't have to train necessarily on gigantic datasets or
9285440	9289040	I mean, you could turn that to be necessary to actually train on large datasets to have
9289040	9293040	emergent properties like we have with LLMs. But I think there's a lot of good ideas that
9293040	9299760	can be done without necessarily scaling up. Then there is how do you do planning with a learn world
9299760	9304560	model. If the world the system evolves in is not the physical world, but it's the world of
9305680	9312800	this at the internet or, you know, some sort of world of where an action consists in doing a search
9312800	9319600	in a search engine or interrogating a database or running a simulation or calling a calculator
9319600	9325200	or solving a differential equation. How do you get a system to actually plan a sequence of actions
9325200	9334160	to give the solution to a problem? And so the question of planning is not just a question of
9334160	9340160	planning physical actions, it could be planning actions to use tools for a dialogue system or
9340160	9347200	for any kind of intelligent system. And there's some work on this, but not a huge amount. Some
9347200	9354240	work at fair, one called tool former, which was a couple years ago and some more recent work on
9354240	9361440	planning. But I don't think we have like a good solution for any of that. Then there is the
9362240	9368160	question of hierarchical planning. So the example I mentioned of, you know, planning a trip from
9368880	9373040	New York to Paris, that's hierarchical, but almost every action that we take
9374000	9380560	involves hierarchical planning in some sense. And we really have absolutely no idea how to do this,
9380560	9392640	like this zero demonstration of hierarchical planning in AI, where the various levels of
9392640	9398800	representations that are necessary have been learned. We can do like two level hierarchical
9398800	9404720	planning when we design the two levels. So for example, you have like a dog-like robot, right?
9404720	9409840	You want it to go from the living room to the kitchen, you can plan a path that avoids the
9409840	9416560	obstacle. And then you can send this to a lower level planner that figures out how to move the
9416560	9422160	legs to kind of follow that trajectory. So that works. But that two level planning is designed
9422160	9430960	by hand, right? We specify what the proper levels of abstraction, the representation at each level
9430960	9436480	of abstraction has to be. How do you learn this? How do you learn that hierarchical representation
9436480	9443680	of action plans, right? With cognize and deep learning, we can train the system to learn
9443680	9449200	hierarchical representations of percepts. What is the equivalent when what you're trying to
9449280	9454480	represent are action plans? For action plans. Yeah. So you want basically a robot dog or
9454480	9459920	humanoid robot that turns on and travels from New York to Paris all by itself.
9460960	9467840	For example. All right. It might have some trouble at the TSA, but yeah. No, but even
9467840	9474400	doing something fairly simple like a household task, like cooking or something. Yeah, there's
9474400	9479040	a lot involved. It's a super complex task. And once again, we take it for granted.
9479440	9487680	What hope do you have for the future of humanity? We're talking about so many exciting technologies,
9487680	9493120	so many exciting possibilities. What gives you hope when you look out over the next 10,
9493120	9499280	20, 50, 100 years? If you look at social media, there's wars going on. There's division.
9500560	9505840	There's hatred, all this kind of stuff. That's also part of humanity. But amidst all that,
9505840	9509760	what gives you hope? I love that question.
9514400	9523840	We can make humanity smarter with AI. AI basically will amplify human intelligence.
9525120	9533200	It's as if every one of us will have a staff of smart AI assistants. They might be smarter than
9533200	9544160	us. They'll do our bidding. Perhaps execute a task in ways that are much better than we could do
9544160	9550960	ourselves because they'll be smarter than us. And so it's like everyone would be the boss of a
9551760	9558960	staff of super smart virtual people. So we shouldn't feel threatened by this any more than we
9558960	9564320	should feel threatened by being the manager of a group of people, some of whom are more intelligent
9564320	9573680	than us. I certainly have a lot of experience with this of having people working with me who are
9573680	9580720	smarter than me. That's actually a wonderful thing. So having machines that are smarter than us that
9580720	9586000	assist us in all of our tasks or daily lives, whether it's professional or personal, I think
9586000	9593200	would be an absolutely wonderful thing. Because intelligence is the commodity that is most in
9593200	9599120	demand. All the mistakes that humanity makes is because of lack of intelligence, really,
9599120	9607200	or lack of knowledge which is related. So making people smarter can only be better.
9607840	9610400	For the same reason that public education is a good thing.
9611360	9618240	And books are a good thing. And the internet is also a good thing intrinsically. And even social
9618240	9623200	networks are a good thing if you run them properly. It's difficult, but you can.
9626000	9634080	Because it helps the communication of information and knowledge and the transmission of knowledge.
9634080	9639360	So AI is going to make humanity smarter. And the analogy I've been using
9640640	9650320	is the fact that perhaps an equivalent event in the history of humanity to what might be provided by
9651440	9657120	generalization of AI assistant is the invention of the printing press. It made everybody smarter.
9657120	9665200	The fact that people could have access to books. Books were a lot cheaper than they
9665200	9671520	were before. And so a lot more people had an incentive to learn to read, which wasn't the case
9671520	9681840	before. And people became smarter. It enabled the enlightenment. There wouldn't be an
9681920	9692080	enlightenment without the printing press. It enabled philosophy, rationalism, escape from
9692080	9705200	religious doctrine, democracy, science. And certainly without this, it wouldn't have been the
9705200	9709600	American revolution or the French revolution. And so it would still be under a feudal
9710560	9720000	regime, perhaps. And so it completely transformed the world because people became smarter and
9720000	9726880	kind of learned about things. Now, it also created 200 years of essentially religious
9726880	9734880	conflicts in Europe. Because the first thing that people read was the Bible. And realized that
9734880	9738400	perhaps there was a different interpretation of the Bible than what the priests were telling them.
9739760	9744880	And so that created the Protestant movement and created the rift. And in fact, the Catholic
9744880	9750640	Church didn't like the idea of the printing press, but they had no choice. And so it had
9750640	9754880	some bad effects and some good effects. I don't think anyone today would say that the invention
9754880	9759680	of the printing press had an overall negative effect, despite the fact that it created
9760320	9768720	200 years of religious conflicts in Europe. Now, compare this. And I thought I was very
9768720	9774000	proud of myself to come up with this analogy, but realize someone else came with the same idea
9774000	9780880	before me. Compare this with what happened in the Ottoman Empire. The Ottoman Empire banned
9780880	9790960	the printing press for 200 years. And it didn't ban it for all languages, only for Arabic.
9791680	9798080	You could actually print books in Latin or Hebrew or whatever in the Ottoman Empire,
9798080	9807520	just not in Arabic. And I thought it was because the rulers just wanted to preserve
9807520	9811920	the control over the population and the dogma, religious dogma and everything.
9812880	9823840	But after talking with the UAE minister of AI, Omar, he told me, no, there was another reason.
9824800	9831360	And the other reason was that it was to preserve the corporation of
9831360	9837920	geographers. There's like an art form, which is writing those beautiful
9840240	9846240	Arabic poems or whatever religious text in this thing. And it was a very powerful
9846240	9853120	corporation of scribes, basically that kind of run a big chunk of the empire. And it couldn't
9853120	9858240	put them out of business. So they banned the printing press in part to protect that business.
9861200	9865920	Now, what's the analogy for AI today? Who are we protecting by banning AI? Who are the people
9865920	9874320	who are asking that AI be regulated to protect their jobs? And of course, it's a real question
9875120	9881760	of what is going to be the effect of technological transformation like AI on the
9882640	9889360	job market and the labor market. And the economies too are much more expert at this than I am. But
9889360	9896240	when I talk to them, they tell us, we're not going to run out of job. This is not going to
9896240	9902400	cause mass unemployment. This is just going to be gradual shift of different professions. The
9902400	9908240	professions are going to be hot 10 or 15 years from now. We have no idea today what they're going to
9908240	9914880	be. The same way if we go back 20 years in the past, like who could have thought 20 years ago
9914880	9921760	that like the hottest job even like five, 10 years ago was mobile app developer, like smartphones
9921760	9928160	weren't invented. Most of the jobs of the future might be in the metaverse. Well, it could be. Yeah.
9928960	9933920	But the point is you can't possibly predict. But you're right. I mean, you made a lot of
9933920	9941680	strong points and I believe that people are fundamentally good. And so if AI, especially
9941680	9948240	open source AI, can make them smarter, it just empowers the goodness in humans.
9948240	9956560	So I share that feeling. Okay. I think people are fine. And in fact, a lot of doomers are doomers
9956560	9963120	because they don't think that people are fundamentally good. And they either don't trust
9963120	9969120	people or they don't trust the institution to do the right thing so that people behave properly.
9970480	9975520	Well, I think both you and I believe in humanity. And I think I speak for a lot of people
9976320	9980800	in saying thank you for pushing the open source movement, pushing to making
9981760	9987600	both research and AI open source, making it available to people and also the models themselves,
9987600	9992480	making it open source. So thank you for that. And thank you for speaking your mind in such
9992480	9996640	colorful and beautiful ways on the internet. I hope you never stop. You're one of the most
9996640	10002240	fun people I know and get to be a fan. Oh, so yeah, thank you for speaking to me once again.
10002240	10007200	And thank you for being you. Thank you. Thanks. Thanks for listening to this conversation with
10007200	10011520	Jan Lacoon. To support this podcast, please check out our sponsors in the description.
10012080	10015280	And now let me leave you with some words from Arthur C. Clarke.
10016880	10020720	The only way to discover the limits of the possible is to go beyond them.
10021680	10037040	And to the impossible. Thank you for listening and hope to see you next time.
