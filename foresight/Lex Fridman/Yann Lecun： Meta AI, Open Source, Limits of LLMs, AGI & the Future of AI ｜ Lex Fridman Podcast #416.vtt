WEBVTT

00:00.000 --> 00:07.360
I see the danger of this concentration of power through proprietary AI systems as a much bigger

00:07.360 --> 00:13.840
danger than everything else. What works against this is people who think that for reasons of

00:13.840 --> 00:20.000
security we should keep AI systems under lock and key because it's too dangerous to put it in the

00:20.000 --> 00:27.280
hands of everybody. That would lead to a very bad future in which all of our information

00:27.280 --> 00:32.240
diet is controlled by a small number of companies through proprietary systems.

00:32.240 --> 00:38.720
I believe that people are fundamentally good and so if AI, especially open source AI, can

00:40.160 --> 00:44.160
make them smarter, it just empowers the goodness in humans.

00:44.160 --> 00:48.080
So I share that feeling. Okay, I think people are fundamentally good.

00:50.080 --> 00:54.960
And in fact a lot of doomers are doomers because they don't think that people are fundamentally

00:55.280 --> 01:02.880
good. The following is a conversation with Yan Lukun, his third time on this podcast.

01:02.880 --> 01:09.920
He is the chief AI scientist at Meta, professor at NYU, touring award winner, and one of the

01:09.920 --> 01:16.240
seminal figures in the history of artificial intelligence. He and Meta AI have been big

01:16.240 --> 01:22.560
proponents of open sourcing AI development and have been walking the walk by open sourcing many

01:22.560 --> 01:30.960
of their biggest models, including Lama 2 and eventually Lama 3. Also, Yan has been an outspoken

01:30.960 --> 01:37.520
critic of those people in the AI community who warned about the looming danger and existential

01:37.520 --> 01:46.240
threat of AGI. He believes the AGI will be created one day, but it will be good. It will not escape

01:46.240 --> 01:54.320
human control, nor will it dominate and kill all humans. At this moment of rapid AI development,

01:54.320 --> 02:01.440
this happens to be somewhat a controversial position. And so it's been fun seeing Yan get

02:01.440 --> 02:07.680
into a lot of intense and fascinating discussions online, as we do in this very conversation.

02:08.560 --> 02:12.640
This is the lecture room and podcast that supported. Please check out our sponsors in the

02:12.640 --> 02:20.240
description. And now, dear friends, here's Yan Likun. You've had some strong statements,

02:21.200 --> 02:25.760
technical statements about the future of artificial intelligence recently throughout

02:25.760 --> 02:32.160
your career, actually, but recently as well. You've said that autoregressive LLMs are

02:34.080 --> 02:39.840
not the way we're going to make progress towards superhuman intelligence. These are the large

02:39.920 --> 02:46.000
language models like GPT-4 like Lama 2 and 3 soon and so on. How do they work and why are they not

02:46.000 --> 02:50.880
going to take us all the way? For a number of reasons. The first is that there is a number of

02:50.880 --> 02:57.920
characteristics of intelligent behavior. For example, the capacity to understand the world,

02:58.720 --> 03:07.920
understand the physical world, the ability to remember and retrieve things, persistent memory,

03:08.800 --> 03:14.400
the ability to reason and the ability to plan. Those are four essential characteristics of

03:14.400 --> 03:24.080
intelligent systems or entities, humans, animals. LLMs can do none of those. Or they can only do

03:24.080 --> 03:29.840
them in a very primitive way. And they don't really understand the physical world. They don't

03:29.840 --> 03:34.640
really have persistent memory. They can't really reason and they certainly can't plan. And so,

03:35.600 --> 03:42.720
if you expect a system to become intelligent just without having the possibility of doing those

03:42.720 --> 03:50.080
things, you're making a mistake. That is not to say that autoregressive LLMs are not useful.

03:50.640 --> 03:58.000
They're certainly useful. That they're not interesting, that we can't build a whole ecosystem

03:58.560 --> 04:05.040
of applications around them. Of course, we can. But as a path towards human level intelligence,

04:05.760 --> 04:11.680
they are missing essential components. And then there is another tidbit or fact that I think is

04:12.320 --> 04:19.120
very interesting. Those LLMs are trained on enormous amounts of text. Basically, the entirety of

04:19.120 --> 04:25.920
all publicly available texts on the internet. That's typically on the order of 10 to the 13

04:25.920 --> 04:31.200
tokens. Each token is typically two bytes. So that's two 10 to the 13 bytes as training data.

04:31.760 --> 04:36.960
It would take you or me 170,000 years to just read through this at eight hours a day.

04:38.560 --> 04:42.560
So it seems like an enormous amount of knowledge that those systems can accumulate.

04:46.000 --> 04:49.360
But then you realize it's really not that much data. If you talk to

04:50.320 --> 04:55.760
a developmental psychologist and they tell you a four-year-old has been awake for 16,000 hours

04:56.400 --> 05:06.720
in his or her life. And the amount of information that has reached the visual cortex of that child

05:06.720 --> 05:14.240
in four years is about 10 to the 15 bytes. And you can compute this by estimating that the

05:15.200 --> 05:21.280
optical nerve carry about 20 megabytes per second, roughly. And so 10 to the 15 bytes

05:21.280 --> 05:28.320
for a four-year-old versus two times 10 to the 13 bytes for 170,000 years worth of reading.

05:29.280 --> 05:36.800
What I tell you is that through sensory input, we see a lot more information than we do through

05:36.800 --> 05:42.880
language. And that despite our intuition, most of what we learn and most of our knowledge

05:43.840 --> 05:48.720
is through our observation and interaction with the real world, not through language.

05:49.360 --> 05:53.840
Everything that we learn in the first few years of life and certainly everything that

05:53.840 --> 05:59.440
animals learn has nothing to do with language. So it would be good to maybe push against some

05:59.440 --> 06:05.840
of the intuition behind what you're saying. So it is true there's several orders of magnitude,

06:05.840 --> 06:12.240
more data coming into the human mind, how much faster and the human mind is able to learn very

06:12.240 --> 06:17.200
quickly from that filter of the data very quickly. Somebody might argue your comparison

06:17.200 --> 06:23.600
between sensory data versus language. That language is already very compressed. It already

06:23.600 --> 06:28.320
contains a lot more information than the bytes it takes to store them if you compare it to visual

06:28.320 --> 06:33.760
data. So there's a lot of wisdom in language. There's words and the way we stitch them together,

06:33.760 --> 06:41.520
it already contains a lot of information. So is it possible that language alone already has

06:42.160 --> 06:49.440
enough wisdom and knowledge in there to be able to from that language construct a

06:50.080 --> 06:54.560
world model and understanding of the world and understanding of the physical world

06:54.560 --> 07:01.040
that you're saying all limbs lack? So it's a big debate among philosophers and also cognitive

07:01.040 --> 07:07.360
scientists like whether intelligence needs to be grounded in reality. I'm clearly in the camp that

07:08.080 --> 07:14.800
yes, intelligence cannot appear without some grounding in some reality. It doesn't need to be

07:15.920 --> 07:21.200
physical reality. It could be simulated, but the environment is just much richer than what you

07:21.200 --> 07:26.880
can express in language. Language is a very approximate representation of our perceives

07:27.520 --> 07:33.120
and our mental models. There's a lot of tasks that we accomplish where we manipulate

07:34.080 --> 07:40.560
a mental model of the situation at hand, and that has nothing to do with language.

07:40.560 --> 07:45.520
Everything that's physical, mechanical, whatever, when we build something, when we

07:45.520 --> 07:52.240
accomplish a task, a model task of grabbing something, etc., we plan for action sequences and

07:52.240 --> 08:00.160
we do this by essentially imagining the outcome of a sequence of actions that we might imagine.

08:01.120 --> 08:06.640
And that requires mental models that don't have much to do with language. And that's,

08:06.640 --> 08:13.680
I would argue, most of our knowledge is derived from that interaction with the physical world.

08:13.680 --> 08:18.400
So a lot of my colleagues who are more interested in things like computer vision

08:19.120 --> 08:26.240
are really on that camp that AI needs to be embodied, essentially. And then other people

08:26.240 --> 08:33.840
coming from the NLP side or maybe some other motivation don't necessarily agree with that.

08:34.800 --> 08:41.760
And philosophers are split as well. And the complexity of the world is hard to

08:44.000 --> 08:52.720
imagine. It's hard to represent all the complexities that we take completely for granted

08:52.720 --> 08:56.720
in the real world that we don't even imagine require intelligence. This is the old Moravec

08:56.720 --> 09:03.120
paradox from the pioneer of robotics, Hans Moravec. We said, how is it that with computers,

09:03.120 --> 09:08.240
it seems to be easy to do high-level complex tasks like playing chess and solving integrals

09:08.240 --> 09:12.960
and doing things like that? Whereas the thing we take for granted that we do every day,

09:14.160 --> 09:19.600
like, I don't know, learning to drive a car or grabbing an object, we can do with computers.

09:23.200 --> 09:29.680
We have LLMs that can pass the bar exam, so they must be smart. But then

09:30.880 --> 09:36.880
they can't learn to drive in 20 hours like any 17-year-old. They can't learn to clear up the

09:36.880 --> 09:41.840
dinner table and fill up the dishwasher like any 10-year-old can learn in one shot.

09:43.440 --> 09:51.200
Why is that? What are we missing? What type of learning or reasoning architecture or whatever

09:51.200 --> 09:59.920
are we missing that basically prevent us from having LLMs in cars and domestic robots?

10:00.800 --> 10:08.160
Can a large language model construct a world model that does know how to drive and does know

10:08.160 --> 10:12.640
how to fill a dishwasher but just doesn't know how to deal with visual data at this time? So

10:13.680 --> 10:16.240
it can operate in a space of concepts.

10:17.040 --> 10:21.600
So yeah, that's what a lot of people are working on. So the answer, the short answer is no.

10:22.320 --> 10:32.560
And the more complex answer is you can use all kinds of tricks to get an LLM to basically digest

10:33.840 --> 10:40.320
visual representations of images or video or audio for that matter.

10:41.120 --> 10:48.240
And a classical way of doing this is you train a vision system in some way.

10:49.200 --> 10:52.720
And we have a number of ways to train vision systems, either supervised, semi-supervised,

10:52.720 --> 11:00.000
self-supervised, all kinds of different ways. That will turn any image into a high-level

11:00.000 --> 11:05.840
representation, basically a list of tokens that are really similar to the kind of tokens that

11:05.920 --> 11:16.240
typical LLM takes as an input. And then you just feed that to the LLM in addition to the text.

11:16.960 --> 11:23.440
And you just expect the LLM to kind of, you know, during training to kind of be able to

11:24.080 --> 11:28.000
use those representations to help make decisions. I mean, it's been working

11:28.000 --> 11:32.720
along those lines for quite a long time. And now you see those systems, right? I mean,

11:32.720 --> 11:38.320
there are LLMs that have some vision extension, but they're basically hacks in the sense that

11:39.440 --> 11:43.600
those things are not like trained end-to-end to handle, to really understand the world.

11:43.600 --> 11:48.160
They're not trained with video, for example. They don't really understand intuitive physics,

11:48.800 --> 11:53.760
at least not at the moment. So you don't think there's something special to you about intuitive

11:53.760 --> 11:58.560
physics, about sort of common sense reasoning about the physical space, about physical reality?

11:58.880 --> 12:02.640
That to you is a giant leap that LLMs are just not able to do.

12:02.640 --> 12:07.600
We're not going to be able to do this with the type of LLMs that we are working with today.

12:07.600 --> 12:10.640
And there's a number of reasons for this. But the main reason is,

12:12.240 --> 12:16.640
the way LLMs are trained is that you take a piece of text, you

12:17.840 --> 12:21.680
remove some of the words in that text, you mask them, you replace them by

12:21.680 --> 12:25.840
blank markers, and you train a genetic neural net to predict the words that are missing.

12:26.720 --> 12:31.520
And if you build this neural net in a particular way, so that it can only look at

12:32.720 --> 12:34.960
words that are to the left of the one it's trying to predict,

12:35.920 --> 12:39.840
then what you have is a system that basically is trying to predict the next word in a text.

12:39.840 --> 12:45.600
So then you can feed it a text, a prompt, and you can ask it to predict the next word.

12:45.600 --> 12:50.240
It can never predict the next word exactly. And so what it's going to do is produce

12:51.120 --> 12:54.800
a probability distribution over all the possible words in your dictionary.

12:54.800 --> 12:58.160
In fact, it doesn't predict words, it predicts tokens that are kind of subword units.

12:58.880 --> 13:04.240
And so it's easy to handle the uncertainty in the prediction there because there's only a finite

13:04.240 --> 13:09.200
number of possible words in the dictionary. And you can just compute a distribution over them.

13:10.880 --> 13:16.720
Then what the system does is that it picks a word from that distribution.

13:16.720 --> 13:20.320
Of course, there's a higher chance of picking words that have a higher probability

13:20.320 --> 13:24.160
within that distribution. So you sample from that distribution to actually produce a word.

13:25.120 --> 13:30.240
And then you shift that word into the input. And so that allows the system not to predict

13:30.240 --> 13:35.600
the second word. And once you do this, you shift it into the input, etc. That's called

13:35.600 --> 13:41.280
autoregressive prediction, which is why those LLMs should be called autoregressive LLMs.

13:43.200 --> 13:50.400
But we just call them LLMs. And there is a difference between this kind of process

13:50.400 --> 13:56.480
and a process by which before producing a word, when you talk, when you and I talk,

13:56.480 --> 14:01.840
you and I are bilingual, we think about what we're going to say, and it's relatively independent

14:01.840 --> 14:06.800
of the language in which we're going to say it. When we talk about, I don't know,

14:06.800 --> 14:10.080
let's say a mathematical concept or something, the kind of thinking that we're doing,

14:10.640 --> 14:17.040
and the answer that we're planning to produce is not linked to whether we're going to say it in

14:17.040 --> 14:22.400
French, Russian, or English. Chomsky just rolled his eyes, but I understand. So you're saying

14:22.400 --> 14:30.080
that there's a bigger abstraction that goes before language, maps onto language.

14:30.080 --> 14:33.760
Right. It's certainly true for a lot of thinking that we do.

14:33.760 --> 14:38.080
Is that obvious that we don't? You're saying your thinking is

14:38.080 --> 14:40.800
same in French as it is in English? Yeah, pretty much.

14:41.760 --> 14:47.600
Pretty much? Or is this like, how flexible are you? Like, if there's a probability distribution?

14:48.880 --> 14:54.000
Well, it depends what kind of thinking, right? If it's just, if it's like producing puns,

14:54.000 --> 14:56.080
I get much better in French than English about that.

14:56.720 --> 15:01.760
No, but is there an abstract representation of puns? Like, is your humor an abstract?

15:01.760 --> 15:05.360
But like when you tweet, and your tweets are sometimes a little bit spicy,

15:05.920 --> 15:11.040
what's, is there an abstract representation in your brain of a tweet before it maps onto English?

15:11.600 --> 15:18.560
There is an abstract representation of imagining the reaction of a reader to that text.

15:18.560 --> 15:21.760
Well, you start with laughter and then figure out how to make that happen.

15:21.760 --> 15:27.120
Or a figure out like a reaction you want to cause and then figure out how to say it,

15:27.120 --> 15:30.640
right? So that it causes that reaction. But that's like really close to language.

15:30.640 --> 15:36.560
But think about like a mathematical concept or, you know, imagining, you know,

15:36.560 --> 15:39.280
something you want to build out of wood or something like that, right?

15:39.840 --> 15:43.280
The kind of thinking you're doing is absolutely nothing to do with language, really.

15:43.280 --> 15:47.520
Like it's not like you have necessarily like an internal monologue in any particular language.

15:47.520 --> 15:53.680
You're, you know, imagining mental models of the thing, right? I mean, if I ask you to,

15:53.680 --> 15:58.320
like imagine what this water bottle will look like if I rotate it 90 degrees,

15:58.720 --> 16:05.600
that has nothing to do with language. And so, so clearly there is, you know,

16:05.600 --> 16:12.160
a more abstract level of representation in which we do most of our thinking and we plan

16:12.960 --> 16:21.440
what we're going to say if the output is, you know, uttered words as opposed to an output being,

16:22.400 --> 16:29.120
you know, muscle actions, right? We plan our answer before we produce it.

16:29.120 --> 16:32.640
And LLMs don't do that. They just produce one word after the other,

16:33.760 --> 16:40.560
instinctively, if you want. It's like, it's a bit like the, you know, subconscious actions where

16:40.560 --> 16:45.440
you don't, like you're distracted, you're doing something, you're completely concentrated and

16:45.440 --> 16:49.520
someone comes to you and, you know, ask you a question and you kind of answer the question.

16:49.520 --> 16:53.120
You don't have time to think about the answer, but the answer is easy, so you don't need to pay

16:53.120 --> 16:58.800
attention. You sort of respond automatically. That's kind of what an LLM does, right? It doesn't

16:58.800 --> 17:04.320
think about its answer really. It retrieves it because it's accumulated a lot of knowledge,

17:04.320 --> 17:10.800
so it can retrieve some, some things, but it's going to just spit out one token after the other

17:10.800 --> 17:17.120
without planning the answer. But you're making it sound just one token after the other,

17:17.120 --> 17:27.280
one token at a time generation is bound to be simplistic. But if the world model is sufficiently

17:27.280 --> 17:34.880
sophisticated, that one token at a time, the, the most likely thing it generates is a sequence of

17:34.880 --> 17:42.640
tokens is going to be a deeply profound thing. Okay, but then that assumes that those systems

17:42.640 --> 17:48.080
actually possess a neutral world model. So it really goes to the, I think the fundamental question

17:48.080 --> 17:56.720
is, can you build a really complete world model, not complete, but a one that has a deep

17:56.720 --> 18:02.800
understanding of the world? Yeah. So can you build this, first of all, by prediction?

18:03.440 --> 18:10.560
Right. And the answer is probably yes. Can you predict, can you build it by predicting words?

18:10.560 --> 18:18.160
And the answer is most probably no, because language is very poor in terms of weak or low

18:18.160 --> 18:23.760
bandwidth, if you want, there's just not enough information there. So building world models means

18:24.960 --> 18:33.840
observing the world and understanding why the world is evolving the way, the way it is. And then

18:34.720 --> 18:41.840
the, the extra component of a world model is something that can predict how the world is

18:41.840 --> 18:46.880
going to evolve as a consequence of an action you might take, right? So what model really is,

18:46.880 --> 18:50.240
here is my idea of the state of the world at time t, here is an action I might take.

18:50.880 --> 18:57.520
What is the predicted state of the world at time t plus one. Now that state of the world doesn't,

18:57.520 --> 19:02.240
that does not need to represent everything about the world. It just needs to represent

19:02.240 --> 19:07.600
enough that's relevant for this planning of the action, but not necessarily all the details.

19:08.240 --> 19:13.760
Now here is the problem. You're not going to be able to do this with generative models.

19:14.720 --> 19:18.960
So a generative model has trained on video, and we've tried to do this for 10 years. You take a

19:18.960 --> 19:25.120
video, show a system a piece of video, and then ask it to predict the reminder of the video.

19:25.680 --> 19:31.520
Basically, predict what's going to happen. One frame at a time, do the same thing as sort of

19:31.520 --> 19:36.240
the auto aggressive LLMs do, but for video. Right. Either one frame at a time or a group of

19:36.240 --> 19:45.840
friends at a time. But yeah, a large video model if you want. The idea of doing this has been floating

19:45.840 --> 19:51.920
around for a long time. And at fair, some of my colleagues and I have been trying to do this for

19:51.920 --> 19:58.720
about 10 years. And you can't really do the same trick as with LLMs because

19:59.680 --> 20:05.760
you know LLMs, as I said, you can't predict exactly which word is going to follow a

20:05.760 --> 20:10.720
sequence of words, but you can predict the distribution over words. Now, if you go to video,

20:11.440 --> 20:15.120
what you would have to do is predict the distribution over all possible frames

20:15.120 --> 20:21.600
in a video. And we don't really know how to do that properly. We do not know how to represent

20:21.600 --> 20:25.520
distributions over high dimensional continuous spaces in ways that are useful.

20:26.080 --> 20:34.480
And that's, there lies the main issue. And the reason we can do this is because the world is

20:34.480 --> 20:41.360
incredibly more complicated and richer in terms of information than text. Text is discrete.

20:42.880 --> 20:49.280
Video is high dimensional and continuous. A lot of details in this. So if I take a video of this

20:49.280 --> 20:57.840
room, and the video is, you know, a camera panning around, there is no way I can predict

20:57.840 --> 21:01.200
everything that's going to be in the room as I pan around. The system cannot predict what's going

21:01.200 --> 21:06.880
to be in the room as the camera is panning. Maybe it's going to predict this is, this is a room

21:06.880 --> 21:10.400
where there's a light and there is a wall and things like that. It can't predict what the

21:10.400 --> 21:15.280
painting on the wall looks like or what the texture of the couch looks like. Certainly not the texture

21:15.280 --> 21:22.480
of the carpet. So there's no way I can predict all those details. So the way to handle this is

21:23.280 --> 21:27.680
one way to possibly to handle this, which we've been working for a long time, is to have a model that

21:27.680 --> 21:33.280
has what's called a latent variable. And the latent variable is fed to a neural net. And it's

21:33.280 --> 21:40.240
supposed to represent all the information about the world that you don't perceive yet. And that you

21:40.240 --> 21:48.080
need to augment the system for the prediction to do a good job at predicting pixels, including the,

21:48.080 --> 21:54.640
you know, fine texture of the carpet and the couch and the painting on the wall.

21:57.040 --> 22:02.800
That has been a complete failure, essentially. And we've tried lots of things. We tried just

22:02.800 --> 22:10.000
straight neural nets. We tried GANs. We tried, you know, VAEs, all kinds of regularized auto

22:10.000 --> 22:16.160
encoders. We tried many things. We also tried those kind of methods to learn

22:17.600 --> 22:24.800
good representations of images or video that could then be used as input to, for example,

22:24.800 --> 22:30.880
an image classification system. And that also has basically failed, like all the systems that attempt

22:30.880 --> 22:40.160
to predict missing parts of an image or video, you know, from a corrupted version of it, basically.

22:40.160 --> 22:45.200
So I take an image or a video, corrupt it or transform it in some way, and then try to reconstruct

22:45.200 --> 22:52.560
the complete video or image from the corrupted version. And then hope that internally the system

22:52.560 --> 22:57.520
will develop good representations of images that you can use for object recognition, segmentation,

22:57.520 --> 23:04.320
whatever it is. That has been essentially a complete failure. And it works really well for text.

23:04.320 --> 23:10.160
That's the principle that is used for LLMs, right? So where's the failure exactly? Is that it's very

23:10.160 --> 23:18.160
difficult to form a good representation of an image, like a good embedding of all the important

23:18.160 --> 23:22.640
information in the image? Is it in terms of the consistency of image to image to image to image

23:22.640 --> 23:27.840
that forms the video? Like where, what is the, if you do a highlight reel of all the ways you

23:27.840 --> 23:35.840
failed? What does that look like? Okay, so the reason this doesn't work is, first of all, I have to

23:35.840 --> 23:40.320
tell you exactly what doesn't work because there is something else that does work. So the thing

23:40.320 --> 23:48.560
that does not work is training the system to learn representations of images by training it to

23:49.520 --> 23:54.800
reconstruct a good image from a corrupted version of it. Okay, that's what doesn't work.

23:55.520 --> 24:01.680
And we have a whole slew of techniques for this that are, you know, a variant of denoising auto

24:01.680 --> 24:06.800
encoders, something called MAE developed by some of my colleagues at FAIR, Max Dotto Encoder.

24:06.800 --> 24:13.120
So it's basically like the, you know, LLMs or things like this where you train the system by

24:13.120 --> 24:17.200
corrupting text, except you corrupt images, you remove patches from it, then you train a

24:17.200 --> 24:21.920
gigantic neural net to reconstruct. The features you get are not good. And you know, they're not

24:21.920 --> 24:26.560
good because if you now train the same architecture, but you train it supervised

24:27.680 --> 24:35.600
with label data, with text textual descriptions of images, etc. You do get good representations

24:35.600 --> 24:41.520
and the performance on recognition tasks is much better than if you do this self-supervised free

24:41.520 --> 24:45.920
training. So the architecture is good? The architecture is good. The architecture of the

24:45.920 --> 24:51.760
encoder is good. Okay, but the fact that you train the system to reconstruct images does not

24:52.480 --> 24:58.240
lead it to produce to learn good generic features of images. When you train in a self-supervised way?

24:58.240 --> 25:02.400
Self-supervised by reconstruction. Yeah, by reconstruction. Okay, so what's the alternative?

25:04.160 --> 25:09.520
The alternative is joint embedding. What is joint embedding? What are these

25:09.520 --> 25:13.680
architectures that you're so excited about? Okay, so now instead of training a system to

25:13.680 --> 25:19.280
encode the image and then training it to reconstruct the full image from a corrupted version,

25:19.920 --> 25:26.400
you take the full image, you take the corrupted or transformed version, you run them both through

25:26.400 --> 25:35.120
encoders, which in general are identical, but not necessarily. And then you train a predictor on top

25:35.120 --> 25:44.640
of those encoders to predict the representation of the full input from the representation of the

25:44.640 --> 25:51.680
corrupted one. Okay, so joint embedding because you're taking the full input and the corrupted

25:51.680 --> 25:57.200
version or transformed version, run them both through encoders, you get a joint embedding,

25:57.200 --> 26:02.240
and then you're saying, can I predict the representation of the full one from the

26:02.240 --> 26:08.480
representation of the corrupted one? Okay, and I call this a JEPA, so that means Joint

26:08.480 --> 26:12.160
Embedding Predictive Architecture because it's joint embedding and there is this predictor that

26:12.160 --> 26:18.640
predicts the representation of the good guy from the bad guy. And the big question is, how do you

26:18.640 --> 26:25.360
train something like this? And until five years ago or six years ago, we didn't have particularly

26:25.360 --> 26:31.680
good answers for how you train those things, except for one called Contrastive Learning,

26:33.280 --> 26:40.080
where, and the idea of Contrastive Learning is you take a pair of images that are, again,

26:40.640 --> 26:45.520
an image and a corrupted version or degraded version somehow or transformed version of the

26:45.520 --> 26:51.920
original one, and you train the predicted representation to be the same as that. If you

26:51.920 --> 26:56.480
only do this, the system collapses. It basically completely ignores the input and produces

26:56.480 --> 27:03.680
representations that are constant. So the Contrastive Methods, avoid this, and those things have

27:03.680 --> 27:12.160
been around since the early 90s, I had a paper on this in 1993, is you also show pairs of images

27:12.160 --> 27:17.680
that you know are different, and then you push away the representations from each other. So you

27:17.680 --> 27:22.800
say, not only do representations of things that we know are the same should be the same or should

27:22.800 --> 27:26.160
be similar, but representation of things that we know are different should be different.

27:27.520 --> 27:31.360
And that prevents the collapse, but it has some limitation. And there's a whole bunch of

27:31.360 --> 27:37.760
techniques that have appeared over the last six, seven years that can revive this type of method,

27:39.040 --> 27:42.960
some of them from FAIR, some of them from Google and other places.

27:43.680 --> 27:48.480
But there are limitations to those Contrastive Methods. What has changed in the last

27:50.640 --> 27:56.400
three, four years is now we have methods that are non-contrastive, so they don't require those

27:57.120 --> 28:03.760
negative, contrastive samples of images that we know are different. You turn them only with

28:03.760 --> 28:07.520
images that are different versions or different views are the same thing.

28:08.400 --> 28:13.280
And you rely on some other tricks to prevent the system from collapsing, and we have

28:13.280 --> 28:15.040
half a dozen different methods for this now.

28:15.920 --> 28:20.160
So what is the fundamental difference between joint embedding architectures

28:20.880 --> 28:29.520
and LLMs? So can JAP take us to AGI? But whether we should say that you don't like

28:30.480 --> 28:35.440
the term AGI, and we'll probably argue, I think every single time I've talked to you with argued

28:35.440 --> 28:42.640
about the G in AGI, I get it. I get it. We'll probably continue to argue about it. It's great.

28:46.160 --> 28:54.400
Because you like French, and Amis is, I guess, friend in French, and AMI stands for Advanced

28:54.400 --> 29:01.840
Machine Intelligence. But either way, can JAP take us to that, towards that advanced machine

29:01.840 --> 29:07.440
intelligence? Well, so it's a first step. So first of all, what's the difference with

29:08.160 --> 29:16.720
generative architectures like LLMs? So LLMs or vision systems that are trained by reconstruction

29:17.600 --> 29:25.920
generate the inputs. They generate the original input that is non-corrupted, non-transformed.

29:26.880 --> 29:31.040
Right? So you have to predict all the pixels. And there is a huge amount of

29:31.840 --> 29:35.600
resources spent in the system to actually predict all those pixels, all the details.

29:37.200 --> 29:42.240
In a JAPA, you're not trying to predict all the pixels. You're only trying to predict

29:42.240 --> 29:48.720
an abstract representation of the inputs. Right? And that's much easier in many ways.

29:49.440 --> 29:54.000
So what the JAPA system, when it's being trained, is trying to do is extract as much information

29:54.000 --> 30:00.320
as possible from the input, but yet only extract information that is relatively easily predictable.

30:01.600 --> 30:05.120
Okay. So there's a lot of things in the world that we cannot predict. For example,

30:05.120 --> 30:12.160
if you have a self-driving car driving down the street or road, there may be trees around the

30:12.160 --> 30:18.960
road. And it could be a windy day. So the leaves on the tree are kind of moving in kind of semi-chaotic

30:18.960 --> 30:22.640
random ways that you can't predict and you don't care. You don't want to predict.

30:23.600 --> 30:27.120
So what you want is your encoder to basically eliminate all those details.

30:27.120 --> 30:30.640
We'll tell you there's moving leaves, but it's not going to keep the details of exactly what's

30:30.640 --> 30:36.640
going on. And so when you do the prediction in representation space, you're not going to have

30:36.640 --> 30:43.440
to predict every single pixel of a relief. And that, you know, not only is a lot simpler,

30:43.440 --> 30:50.400
but also it allows the system to essentially learn and abstract representation of the world where,

30:50.400 --> 30:57.520
you know, what can be modeled and predicted is preserved. And the rest is viewed as noise and

30:57.520 --> 31:02.160
eliminated by the encoder. So it kind of lifts the level of abstraction of the representation.

31:02.160 --> 31:05.840
If you think about this, this is something we do absolutely all the time. Whenever we

31:05.840 --> 31:10.720
describe a phenomenon, we describe it at a particular level of abstraction. And we don't

31:10.720 --> 31:15.600
always describe every natural phenomenon in terms of quantum field theory, right? That would be

31:16.160 --> 31:21.840
impossible, right? So we have multiple levels of abstraction to describe what happens in the world,

31:21.840 --> 31:26.720
you know, starting from quantum field theory to like atomic theory and molecules, you know,

31:26.720 --> 31:32.720
and chemistry, materials, and, you know, all the way up to, you know, kind of concrete objects in

31:32.720 --> 31:40.320
the real world and things like that. So we can't just only model everything at the lowest level.

31:40.400 --> 31:46.800
And that's what the idea of JEPA is really about, learn abstract representation

31:47.520 --> 31:52.080
in a self-supervised manner. And, you know, you can do it hierarchically as well.

31:52.080 --> 31:56.960
So that, I think, is an essential component of an intelligent system. And in language,

31:56.960 --> 32:01.840
we can get away without doing this because language is already, to some level, abstract

32:02.480 --> 32:06.320
and already has eliminated a lot of information that is not predictable.

32:07.040 --> 32:12.000
And so we can get away without doing the joint embedding, without, you know,

32:12.000 --> 32:15.120
lifting the abstraction level and by directly predicting words.

32:16.240 --> 32:22.880
So joint embedding, it's still generative, but it's generative in this abstract representation

32:22.880 --> 32:28.320
space. And you're saying language, we were lazy with language because we already got the

32:28.320 --> 32:33.200
abstract representation for free. And now we have to zoom out, actually think about generally

32:33.200 --> 32:40.080
intelligent systems, we have to deal with a full mess of physical reality of reality.

32:40.080 --> 32:44.560
And you can't, you do have to do this step of jumping from

32:47.920 --> 32:54.720
the full, rich, detailed reality to a abstract representation of that reality

32:54.720 --> 32:57.280
based on what you can then reason and all that kind of stuff.

32:57.280 --> 33:02.000
Right. And the thing is, those self-supervised algorithms that learned by prediction,

33:03.200 --> 33:11.040
even in representation space, they learned more concept if the input data you feed them

33:11.040 --> 33:15.520
is more redundant. The more redundancy there is in the data, the more they're able to capture

33:15.520 --> 33:21.360
some internal structure of it. And so there there is way more redundancy and structure in

33:22.160 --> 33:29.600
perceptual inputs, sensory input, like vision than there is in text, which is not nearly as

33:29.600 --> 33:33.920
redundant. This is back to the question you were asking a few minutes ago. Language might

33:33.920 --> 33:37.520
represent more information really, because it's already compressed. You're right about that,

33:37.520 --> 33:43.600
but that means it's also less redundant. And so self-supervised learning will not work as well.

33:43.600 --> 33:51.760
Is it possible to join the self-supervised training on visual data and self-supervised

33:51.760 --> 33:58.160
training on language data? There is a huge amount of knowledge, even though you talk down about those

33:58.160 --> 34:06.640
10 to the 13 tokens. Those 10 to the 13 tokens represent the entirety, a large fraction of what

34:06.640 --> 34:13.200
us humans have figured out, both the shit talk on Reddit and the contents of all the books and

34:13.200 --> 34:20.400
the articles and the full spectrum of human intellectual creation. So is it possible to

34:20.400 --> 34:26.960
join those two together? Well, eventually, yes. But I think if we do this too early,

34:27.680 --> 34:32.000
we run the risk of being tempted to cheat. And in fact, that's what people are doing at the moment

34:32.000 --> 34:38.560
with the vision language model. We're basically cheating. We're using language as a crutch to

34:38.560 --> 34:46.400
help the deficiencies of our vision systems to learn good representations from images and video.

34:47.040 --> 34:53.520
The problem with this is that we might improve our vision language system a bit.

34:53.520 --> 35:00.080
I mean, our language models by feeding them images. But we're not going to get to the level of even

35:00.640 --> 35:06.000
the intelligence or level of understanding of the world of a cat or a dog, which doesn't have

35:06.000 --> 35:11.120
language. They don't have language, and they understand the world much better than any LLM.

35:11.920 --> 35:17.120
They can plan really complex actions and imagine the result of a bunch of actions.

35:17.920 --> 35:23.280
How do we get machines to learn that before we combine that with language? Obviously,

35:23.280 --> 35:30.240
if we combine this with language, this is going to be a winner. But before that, we have to focus

35:30.240 --> 35:35.440
on how do we get systems to learn how the world works. So this kind of joint embedding,

35:36.160 --> 35:41.280
predictive architecture. For you, that's going to be able to learn something that common sense,

35:41.280 --> 35:49.120
something like what a cat uses to predict how to mess with its owner most optimally by knocking over

35:49.120 --> 35:55.760
a thing. That's the hope. In fact, the techniques we're using are non-contrastive. So not only is

35:55.760 --> 36:01.280
the architecture non-generative, the learning procedures we're using are non-contrastive.

36:01.360 --> 36:06.400
We have two sets of techniques. One set is based on distillation, and there's a number of

36:07.440 --> 36:16.720
methods that use this principle. One by DeepMind called BYOL, a couple by Fair, one called Vicreg,

36:17.840 --> 36:23.440
and another one called IJEPA. Vicreg, I should say, is not a distillation method, actually,

36:23.440 --> 36:30.480
but IJEPA and BYOL certainly are. There's another one also called Dino or Dino, also produced from

36:30.480 --> 36:34.480
at Fair. And the idea of those things is that you take the full input, let's say an image,

36:35.600 --> 36:42.240
you run it through an encoder, produces a representation, and then you corrupt that

36:42.240 --> 36:46.320
input or transform it, run it through the essentially what amounts to the same encoder

36:46.320 --> 36:51.760
with some other differences, and then train a predictor. Sometimes the predictor is very simple,

36:51.760 --> 36:55.360
sometimes it doesn't exist, but train a predictor to predict the representation of

36:56.320 --> 37:04.400
the first uncorrupted input from the corrupted input. But you only train the second branch.

37:05.280 --> 37:11.520
You only train the part of the network that is fed with the corrupted input. The other network,

37:11.520 --> 37:15.840
you don't train, but it seems to share the same weight. When you modify the first one,

37:15.840 --> 37:21.280
it also modifies the second one. And with various tricks, you can prevent the system from collapsing

37:22.240 --> 37:26.240
with the collapse of the type I was explaining before with the system basically ignores the input.

37:28.080 --> 37:37.760
So that works very well. The two techniques we developed at Fair, Dino and IJEPA work really

37:37.760 --> 37:44.960
well for that. So what kind of data are we talking about here? So this several scenario, one scenario

37:44.960 --> 37:53.760
is you take an image, you corrupt it by changing the cropping, for example, changing the size

37:53.760 --> 37:58.720
a little bit, maybe changing the orientation, blurring it, changing the colors, doing all kinds

37:58.720 --> 38:03.280
of horrible things to it. But basic horrible things? Basic horrible things that sort of degrade the

38:03.280 --> 38:11.200
quality a little bit and change the framing, you know, crop the image. And in some cases,

38:11.200 --> 38:15.120
in the case of IJEPA, you don't need to do any of this, you just mask some parts of it,

38:15.840 --> 38:23.120
right? You just basically remove some regions like a big block, essentially. And then, you know,

38:23.120 --> 38:28.160
run through the encoders and train the entire system, encoder and predictor, to predict the

38:28.160 --> 38:35.280
representation of the good one from the representation of the corrupted one. So that's IJEPA. It

38:35.280 --> 38:39.440
doesn't need to know that it's an image, for example, because the only thing it needs to know

38:39.440 --> 38:44.960
is how to do this masking. Whereas with Dino, you need to know it's an image because you need to do

38:44.960 --> 38:49.600
things like, you know, geometry transformation and blurring and things like that that are really

38:49.600 --> 38:55.680
image specific. A more recent version of this that we have is called VJEPA. So it's basically the same

38:55.680 --> 39:01.360
idea as IJEPA, except it's applied to video. So now you take a whole video and you mask a whole

39:01.360 --> 39:07.280
chunk of it. And what we mask is actually kind of a temporal tube. So an all, like a whole segment

39:07.280 --> 39:12.720
of each frame in the video over the entire video. And that tube was like statically positioned

39:12.720 --> 39:18.720
throughout the frames? Right, throughout the tube. Yeah, typically it's 16 frames or something,

39:18.720 --> 39:24.000
and we mask the same region over the entire 16 frames. It's a different one for every video,

39:24.000 --> 39:31.280
obviously. And then again, train that system so as to predict the representation of the full video

39:31.280 --> 39:37.040
from the partially masked video. That works really well. It's the first system that we have that

39:37.840 --> 39:42.800
learns good representations of video so that when you feed those representations to a supervised

39:44.000 --> 39:48.640
classifier head, it can tell you what action is taking place in the video with, you know,

39:48.640 --> 39:55.840
pretty good accuracy. So that's the first time we get something of that quality.

39:55.840 --> 40:00.160
So that's a good test that a good representation is formed. That means there's something to this?

40:00.160 --> 40:06.960
Yeah. We also preliminary result that seemed to indicate that the representation allows us,

40:06.960 --> 40:13.840
allow our system to tell whether the video is physically possible or completely impossible

40:13.840 --> 40:18.800
because some object disappeared or an object, you know, suddenly jumped from one location to another

40:19.360 --> 40:27.280
or change shape or something. So it's able to capture some physical, some physics-based constraints

40:27.280 --> 40:32.400
about the reality represented in the video? Yeah. About the appearance and the disappearance of

40:32.400 --> 40:42.960
objects. Yeah. That's really new. Okay. But can this actually get us to this kind of world model

40:42.960 --> 40:51.040
that understands enough about the world to be able to drive a car? Possibly. This is going to take

40:51.040 --> 40:56.880
a while before we get to that point. But there are systems already, you know, robotic systems that

40:56.880 --> 41:04.800
are based on this idea. And what you need for this is a slightly modified version of this,

41:04.800 --> 41:14.240
where imagine that you have a video and a complete video. And what you're doing to this video is that

41:14.240 --> 41:19.520
you're either translating it in time towards the future. So you'll only see the beginning of the

41:19.520 --> 41:25.040
video, but you don't see the latter part of it that is in the original one. Or you just mask

41:25.040 --> 41:32.080
the second half of the video, for example. And then you train a JEPA system of the type I describe

41:32.080 --> 41:38.320
to predict the representation of the full video from the shifted one. But you also feed the predictor

41:38.320 --> 41:44.320
with an action. For example, you know, the wheel is turned 10 degrees to the right or something.

41:45.120 --> 41:51.760
Right. So if it's a dash cam in a car, and you know the angle of the wheel, you should be able

41:51.760 --> 41:58.720
to predict to some extent what's going to happen to what you see. You're not going to be able to

41:58.720 --> 42:05.360
predict all the details of objects that appear in the view, obviously. But at a abstract representation

42:05.360 --> 42:12.400
level, you can probably predict what's going to happen. So now what you have is a internal model

42:12.400 --> 42:18.000
that says, here is my idea of state of the world at time t. Here is an action I'm taking. Here is

42:18.000 --> 42:22.960
a prediction of the state of the world at time t plus one, t plus delta t, t plus two seconds,

42:22.960 --> 42:29.520
whatever it is. If you have a model of this type, you can use it for planning. So now you can do

42:29.520 --> 42:35.600
what LLMS cannot do, which is planning what you're going to do so as to arrive at a particular

42:36.960 --> 42:43.120
outcome or satisfy a particular objective. Right. So you can have a number of objectives.

42:44.000 --> 42:52.960
Right. I can predict that if I have an object like this, right, and I open my hand, it's going to

42:52.960 --> 42:59.200
fall, right? And if I push it with a particular force on the table, it's going to move. If I push

42:59.200 --> 43:06.000
the table itself, it's probably not going to move with the same force. So we have this internal model

43:06.000 --> 43:12.800
of the world in our mind, which allows us to plan sequences of actions to arrive at a particular

43:12.800 --> 43:22.000
goal. And so now if you have this world model, we can imagine a sequence of actions, predict what

43:22.000 --> 43:28.240
the outcome of the sequence of action is going to be, measure to what extent the final state

43:28.240 --> 43:34.720
satisfies a particular objective, like, you know, moving the bottle to the left of the table,

43:36.560 --> 43:41.520
and then plan a sequence of actions that will minimize this objective at runtime. We're not

43:41.600 --> 43:45.200
talking about learning. We're talking about inference time. Right. So this is planning, really.

43:45.920 --> 43:50.560
And in optimal control, this is a very classical thing. It's called model predictive control. You

43:50.560 --> 43:56.160
have a model of the system you want to control that, you know, can predict the sequence of states

43:56.160 --> 44:02.880
corresponding to a sequence of commands. And you're planning a sequence of commands so that,

44:02.880 --> 44:10.560
according to your world model, the end state of the system will satisfy an objective that you

44:10.560 --> 44:17.280
fix. This is the way, you know, rocket trajectories have been planned since computers have been

44:17.280 --> 44:24.000
around since the early 60s, essentially. So yes, for model predictive control, but you also often

44:24.000 --> 44:28.800
talk about hierarchical planning. Yeah. Can hierarchical planning emerge from this somehow?

44:28.800 --> 44:34.400
Well, so no, you will have to build a specific architecture to allow for hierarchical planning.

44:34.400 --> 44:39.120
So hierarchical planning is absolutely necessary if you want to plan complex actions.

44:40.560 --> 44:44.560
If I want to go from, let's say, from New York to Paris, this example I use all the time,

44:45.280 --> 44:51.360
and I'm sitting in my office at NYU, my objective that I need to minimize is my distance to Paris

44:51.920 --> 44:58.240
at a high level, a very abstract representation of my location. I would have to decompose this

44:58.240 --> 45:03.680
into two sub goals. First one is go to the airport. Second one is catch a plane to Paris.

45:04.480 --> 45:11.360
Okay, so my sub goal is now going to the airport. My objective function is my distance to the airport.

45:12.480 --> 45:17.360
How do I go to the airport? Well, I have to go in the street and hail the taxi,

45:18.080 --> 45:23.840
which you can do in New York. Okay, now I have another sub goal, go down on the street.

45:24.880 --> 45:29.600
Well, that means going to the elevator, going down the elevator, walk out the street.

45:29.600 --> 45:37.280
How do I go to the elevator? I have to stand up for my chair, open the door of my office,

45:37.920 --> 45:44.080
go to the elevator, push the button. How do I get up from my chair? You can imagine going down all

45:44.080 --> 45:49.200
the way down to basically what amounts to millisecond by millisecond muscle control.

45:50.320 --> 45:57.360
Okay, and obviously you're not going to plan your entire trip from New York to Paris in terms of

45:57.360 --> 46:02.080
millisecond by millisecond muscle control. First, that would be incredibly expensive,

46:02.080 --> 46:05.600
but it will also be completely impossible because you don't know all the conditions

46:06.320 --> 46:10.080
of what's going to happen, you know, how long it's going to take to catch a taxi

46:11.920 --> 46:16.800
or to go to the airport with traffic, you know. I mean, you would have to know exactly

46:16.800 --> 46:21.360
the condition of everything to be able to do this planning and you don't have the information.

46:21.360 --> 46:26.080
So you have to do this hierarchical planning so that you can start acting and then sort of

46:26.080 --> 46:34.800
replanning as you go. And nobody really knows how to do this in AI. Nobody knows how to train a

46:34.800 --> 46:39.920
system to learn the appropriate multiple levels of representation so that hierarchical planning

46:40.560 --> 46:45.040
works. There's something like that already emerged. So like, can you use an LLM,

46:46.560 --> 46:54.080
state-of-the-art LLM, to get you from New York to Paris by doing exactly the kind of detailed

46:54.160 --> 47:00.160
set of questions that you just did? Which is, can you give me a list of 10 steps I need to do

47:01.040 --> 47:06.720
to get from New York to Paris? And then for each of those steps, can you give me a list of 10

47:06.720 --> 47:12.080
steps, how I make that step happen? And for each of those steps, can you give me a list of 10 steps

47:12.080 --> 47:18.240
to make each one of those until you're moving your individual muscles? Maybe not. Whatever you

47:18.240 --> 47:23.520
can actually act upon using your mind. Right. So there's a lot of questions that are sort of

47:23.520 --> 47:28.640
implied by this, right? So the first thing is LLMs will be able to answer some of those questions

47:28.640 --> 47:35.040
down to some level of abstraction under the condition that they've been trained with similar

47:35.040 --> 47:40.880
scenarios in the training set. They would be able to answer all those questions, but some of them

47:41.600 --> 47:45.840
may be hallucinated, meaning nonfactual. Yeah, true. I mean, they will probably produce some

47:45.840 --> 47:49.440
answer, except they're not going to be able to really kind of produce millisecond by millisecond

47:49.440 --> 47:54.960
muscle control of how you stand up from your chair, right? So, but down to some level of

47:54.960 --> 47:59.360
abstraction where you can describe things by words, they might be able to give you a plan,

47:59.360 --> 48:02.480
but only under the condition that they've been trained to produce those kind of plans,

48:03.680 --> 48:08.640
right? They're not going to be able to plan for situations where that they never encountered

48:08.640 --> 48:12.800
before. They basically are going to have to regurgitate the template that they've been trained on.

48:12.800 --> 48:17.360
But where, like just for the example of New York to Paris, is it going to start getting

48:18.000 --> 48:23.040
into trouble? Like at which layer of abstraction do you think you'll start? Because like I can

48:23.040 --> 48:27.600
imagine almost every single part of that LLM will be able to answer somewhat accurately,

48:27.600 --> 48:30.960
especially when you're talking about New York and Paris major cities.

48:30.960 --> 48:35.680
So, I mean, certainly LLM would be able to solve that problem if you fine tune it for it,

48:36.400 --> 48:43.840
you know, just and so I can't say that LLM cannot do this. It can do this if you train it for it.

48:43.840 --> 48:51.120
There's no question. Down to a certain level where things can be formulated in terms of words,

48:51.120 --> 48:55.680
but like if you want to go down to like how you climb down the stairs or just stand up from your

48:55.680 --> 49:03.280
chair in terms of words, like you can't do it. You need, that's one of the reasons you need

49:04.160 --> 49:08.640
experience of the physical world, which is much higher bandwidth than what you can express in

49:08.640 --> 49:14.000
words in human language. So everything we've been talking about on the joint embedding space,

49:14.960 --> 49:19.200
is it possible that that's what we need for like the interaction with physical reality for

49:19.200 --> 49:25.200
on the robotics front? And then just the LLMs are the thing that sits on top of it for the

49:25.200 --> 49:31.520
bigger reasoning about like the fact that I need to book a plane ticket and I need to know

49:31.520 --> 49:36.480
I know how to go to the websites and so on. Sure. And, you know, a lot of plans that people know

49:36.480 --> 49:42.800
about that are relatively high level are actually learned. They're not people, most people don't

49:42.800 --> 49:52.560
invent the, you know, plans. They, by themselves, they, you know, we have some ability to do this,

49:52.560 --> 49:58.240
of course, obviously, but, but most plans that people use are plans that they've been

49:58.960 --> 50:02.560
trained on, like they've seen other people use those plans or they've been told how to do things,

50:02.560 --> 50:09.600
right? That you can't invent how you like take a person who's never heard of airplanes and

50:09.600 --> 50:13.920
tell them like, how do you go from New York to Paris? And they're probably not going to be able

50:13.920 --> 50:17.920
to kind of, you know, deconstruct the whole plan unless they've seen examples of that before.

50:18.640 --> 50:24.720
So certainly LLMs are going to be able to do this, but, but then how you link this from the

50:24.720 --> 50:32.240
the low level of actions that needs to be done with things like, like JPA that basically

50:32.240 --> 50:36.800
lift the abstraction level of the representation without attempting to reconstruct the detail of

50:36.800 --> 50:44.240
the situation. That's why we need JPAs for. I would love to sort of linger on your skepticism

50:44.240 --> 50:53.440
around auto aggressive LLMs. So one way I would like to test that skepticism is everything you

50:53.440 --> 51:03.120
say makes a lot of sense. But if I apply everything you said today and in general to like, I don't

51:03.120 --> 51:08.560
know, 10 years ago, maybe a little bit less, no, let's say three years ago, I wouldn't be able to

51:08.560 --> 51:18.000
predict the success of LLMs. So does it make sense to you that auto aggressive LLMs are able to be

51:18.080 --> 51:25.520
so damn good? Yes. Can you explain your intuition? Because if I were to take

51:26.320 --> 51:32.800
your wisdom and intuition at face value, I would say there's no way auto aggressive LLMs

51:32.800 --> 51:37.440
one token at a time would be able to do the kind of things they're doing. No, there's one thing that

51:37.440 --> 51:43.440
auto aggressive LLMs or that LLMs in general, not just the auto aggressive one, but including the

51:43.520 --> 51:49.680
bird style by direction or ones are exploiting and it's self supervised learning and I've been a

51:49.680 --> 51:55.200
very, very strong advocate of self supervised learning for many years. So those things are

51:56.080 --> 52:00.880
a incredibly impressive demonstration that self supervised learning actually works.

52:02.000 --> 52:08.480
The idea that, you know, started, it didn't start with with with bird, but it was really kind of

52:08.480 --> 52:14.000
a good demonstration with this. So the the idea that, you know, you take a piece of text, you

52:14.000 --> 52:17.600
corrupt it, and then you train some gigantic neural net to reconstruct the parts that are missing.

52:19.040 --> 52:20.320
That has been an enormous,

52:23.360 --> 52:28.560
produced an enormous amount of benefits. It allowed us to create systems that understand

52:29.520 --> 52:32.320
understand language, systems that can translate,

52:33.200 --> 52:38.560
hundreds of languages in any direction, systems that are multilingual. So they're not,

52:39.280 --> 52:42.080
it's a single system that can be trained to understand hundreds of languages

52:43.040 --> 52:50.720
and translate in any direction and produce summaries and then answer questions and produce text.

52:51.520 --> 52:57.200
And then there's a special case of it where, you know, you, which is your progressive trick where

52:57.280 --> 53:02.960
you constrain the system to not elaborate a representation of the text from looking at

53:02.960 --> 53:08.400
the entire text, but only predicting a word from the words that are come before, right?

53:08.400 --> 53:11.760
Then you do this by the constraining the architecture of the network. And that's

53:11.760 --> 53:17.760
what you can build an autoregressive LLM from. So there was a surprise many years ago with

53:18.480 --> 53:23.920
what's called decoder only LLM. So since, you know, systems of this type that are just trying to

53:24.480 --> 53:30.080
produce words from the, from the previous one. And the fact that when you scale them up,

53:31.120 --> 53:37.280
they tend to really kind of understand more about the, about language when you train them

53:37.280 --> 53:41.280
on loss of data, you make them really big. That was kind of a surprise. And that surprise

53:41.280 --> 53:49.280
occurred quite a while back, like, you know, with work from, you know, Google meta, open AI,

53:49.280 --> 53:56.560
et cetera, you know, going back to, you know, the GPT kind of work general pretrained transformers.

53:56.560 --> 54:02.560
You mean like GPT2? Like there's a certain place where you start to realize scaling might actually

54:02.560 --> 54:08.160
keep giving us an emergent benefit. Yeah. I mean, there were, there were work from,

54:08.160 --> 54:14.400
from various places, but if you want to kind of, you know, place it in the, in the GPT

54:15.760 --> 54:17.360
timeline, that would be around GPT2.

54:17.680 --> 54:23.760
Well, I just, because you said it, you're so charismatic and you said so many words, but

54:23.760 --> 54:30.080
self-supervised learning. Yes. But again, the same intuition you're applying to saying that

54:30.080 --> 54:36.960
autoregressive LLMs cannot have a deep understanding of the world. If we just apply that same

54:36.960 --> 54:43.280
intuition, does it make sense to you that they're able to form enough of a representation in the

54:43.280 --> 54:50.640
world to be damn convincing, essentially passing the original Turing test with flying colors?

54:50.640 --> 54:55.520
Well, we're fooled by their fluency, right? We just assume that if a system is fluent

54:56.320 --> 55:00.640
in manipulating language, then it has all the characteristics of human intelligence,

55:00.640 --> 55:05.520
but that impression is false. We're really fooled by it.

55:06.400 --> 55:10.800
What do you think Alan Turing would say? It, without understanding anything, just hanging out

55:10.800 --> 55:16.160
with it. Alan Turing would decide that a Turing test is a really bad test. Okay. This is what

55:16.160 --> 55:20.480
the AI community has decided many years ago, that the Turing test was a really bad test of

55:20.480 --> 55:26.000
intelligence. What would Hans Moravek say about the, about the larger language models?

55:26.000 --> 55:32.320
Hans Moravek would say that Moravek paradox still applies. Okay. Okay. Okay. We can pass.

55:32.320 --> 55:35.760
You don't think he would be really impressed? No, of course, everybody would be impressed, but

55:35.840 --> 55:41.920
it's not a question of being impressed or not. It's the question of knowing what the limit of

55:41.920 --> 55:47.520
those systems can do. Again, they are impressive. They can do a lot of useful things. There's a

55:47.520 --> 55:52.640
whole industry that is being built around them. They're going to make progress, but there's a

55:52.640 --> 55:57.600
lot of things they cannot do, and we have to realize what they cannot do and then figure out

55:58.560 --> 56:06.320
how we get there. And I'm not seeing this. I'm seeing this from basically 10 years of research

56:08.160 --> 56:13.600
on the idea of self-supervised learning. Actually, that's going back more than 10 years,

56:13.600 --> 56:18.080
but the idea of self-supervised learning. So basically capturing the internal structure

56:18.080 --> 56:23.440
of a set of inputs without training the system for any particular task,

56:23.520 --> 56:30.240
learning representations. The conference I co-founded 14 years ago is called International

56:30.240 --> 56:34.640
Conference on Learning Representations. That's the entire issue that deep learning is dealing

56:34.640 --> 56:40.880
with, right? And it's been my obsession for almost 40 years now. So learning representation is

56:40.880 --> 56:45.520
really the thing. For the longest time, we could only do this with supervised learning.

56:45.520 --> 56:49.600
And then we started working on what we used to call unsupervised learning

56:49.840 --> 56:57.200
and sort of revive the idea of unsupervised learning in the early 2000s with

56:57.200 --> 57:01.520
Yoshua Benjo and Jeff Hinton. Then discovered that supervised learning actually works pretty

57:01.520 --> 57:07.280
well if you can collect enough data. And so the whole idea of unsupervised self-supervised

57:07.280 --> 57:13.600
learning, can I took a backseat for a bit? And then I kind of tried to revive it

57:14.480 --> 57:24.240
in a big way, starting in 2014 basically when we started FAIR and really pushing for finding new

57:24.240 --> 57:29.280
methods to do self-supervised learning, both for text and for images and for video and audio.

57:29.840 --> 57:35.440
And some of that work has been incredibly successful. The reason why we have multilingual

57:35.440 --> 57:41.040
translation system thinks to do content moderation on Meta, for example, on Facebook,

57:41.600 --> 57:45.760
that are multilingual to understand whether a piece of text is hate speech or not or something,

57:46.400 --> 57:50.960
is due to that progress using self-supervised learning for NLP, combining this with

57:50.960 --> 57:55.360
transformer architectures and blah, blah, blah. But that's the big success of self-supervised

57:55.360 --> 58:00.080
learning. We had similar success in speech recognition, a system called Wave2Vec,

58:00.080 --> 58:03.680
which is also a joint embedding architecture, by the way, trained with contrastive learning.

58:03.680 --> 58:10.480
And that system also can produce speech recognition systems that are multilingual

58:10.480 --> 58:16.000
with mostly unlabeled data and only need a few minutes of label data to actually do

58:16.000 --> 58:23.040
speech recognition. That's amazing. We have systems now based on those combination of ideas that can

58:23.040 --> 58:28.000
do real-time translation of hundreds of languages into each other, speech to speech.

58:28.000 --> 58:34.240
Speech to speech, even including just fascinating languages that don't have written forms.

58:34.240 --> 58:35.440
That's right. Just spoken only.

58:35.440 --> 58:39.120
That's right. We don't go through text. It goes directly from speech to speech using an

58:39.120 --> 58:44.480
internal representation of speech units that are discrete. But it's called text lesson LP.

58:44.480 --> 58:51.280
We used to call it this way. But yeah, incredible success there. And then for 10 years, we tried

58:51.280 --> 58:57.280
to apply this idea to learning representations of images by training a system to predict videos,

58:57.280 --> 59:01.360
learning intuitive physics by training a system to predict what's going to happen in the video,

59:02.240 --> 59:08.000
and tried and tried and failed and failed with generative models, with models that predict pixels.

59:10.000 --> 59:13.440
We could not get them to learn good representations of images. We could not

59:13.440 --> 59:18.160
get them to learn good representations of videos. We tried many times. We published lots of papers

59:18.160 --> 59:25.520
on it. They kind of sort of work, but not really great. They started working. We abandoned this idea

59:25.520 --> 59:30.960
of predicting every pixel and basically just doing the joint embedding and predicting in

59:30.960 --> 59:37.760
representation space. That works. So there's ample evidence that we're not going to be able to

59:38.560 --> 59:43.040
learn good representations of the real world using generative model.

59:43.040 --> 59:47.840
So I'm telling people, everybody's talking about generative AI. If you're really interested in

59:47.840 --> 59:55.120
human-level AI, abandon the idea of generative AI. Okay, but you really think it's possible to get

59:55.120 --> 01:00:02.000
far with the joint embedding representation. So there's common sense reasoning, and then there's

01:00:02.080 --> 01:00:11.520
high-level reasoning. I feel like those are two, the kind of reasoning that LLMs are able to do.

01:00:11.520 --> 01:00:16.000
Okay, let me not use the word reasoning, but the kind of stuff that LLMs are able to do

01:00:16.000 --> 01:00:20.720
seems fundamentally different than the common sense reasoning we use to navigate the world.

01:00:20.720 --> 01:00:25.840
It seems like we're going to need both. Would you be able to get with the joint embedding,

01:00:25.840 --> 01:00:30.160
which is a JAPA type of approach, looking at video? Would you be able to learn,

01:00:32.240 --> 01:00:35.680
let's see, well, how to get from New York to Paris or

01:00:38.080 --> 01:00:45.760
how to understand the state of politics in the world today? Right? These are things where various

01:00:45.760 --> 01:00:52.080
humans generate a lot of language and opinions on in the space of language, but don't visually

01:00:52.080 --> 01:00:58.480
represent that in any clearly compressible way. Right. Well, there's a lot of situations that

01:00:58.480 --> 01:01:07.440
might be difficult for a purely language-based system to know. You can probably learn from

01:01:07.440 --> 01:01:13.040
reading text, the entirety of the publicly available text in the world that I cannot get from New York

01:01:13.040 --> 01:01:20.080
to Paris by slapping my fingers. That's not going to work. But there's probably sort of more complex

01:01:20.880 --> 01:01:27.600
scenarios of this type, which an LLM may never have encountered and may not be able to determine

01:01:27.600 --> 01:01:35.760
whether it's possible or not. So that link from the low level to the high level, the thing is

01:01:35.760 --> 01:01:42.240
that the high level that language expresses is based on the common experience of the low level,

01:01:43.120 --> 01:01:49.440
which LLMs currently do not have. When we talk to each other, we know we have a common experience

01:01:50.000 --> 01:01:58.880
of the world. A lot of it is similar, and LLMs don't have that.

01:01:58.880 --> 01:02:03.600
But see, it's present. You and I have a common experience of the world in terms of the physics

01:02:03.600 --> 01:02:10.720
of how gravity works and stuff like this, and that common knowledge of the world,

01:02:11.600 --> 01:02:18.800
I feel like is there in the language. We don't explicitly express it, but if you have a huge

01:02:18.800 --> 01:02:26.000
amount of text, you're going to get this stuff that's between the lines. In order to

01:02:27.200 --> 01:02:31.520
form a consistent world model, you're going to have to understand how gravity works,

01:02:31.520 --> 01:02:37.280
even if you don't have an explicit explanation of gravity. So even though in the case of gravity,

01:02:37.280 --> 01:02:44.320
there is explicit explanations of gravity and Wikipedia. But the stuff that we think of as

01:02:44.320 --> 01:02:50.880
common sense reasoning, I feel like to generate language correctly, you're going to have to figure

01:02:50.880 --> 01:02:57.120
that out. Now, you could say as you have, there's not enough text, sorry. Okay, so what? You don't

01:02:57.120 --> 01:03:00.960
think so? No, I agree with what you just said, which is that to be able to do high level

01:03:02.960 --> 01:03:06.880
common sense, to have high level common sense, you need to have the low level common sense to

01:03:06.880 --> 01:03:12.400
build on top of. But that's not there. And that's not there in LLMs. LLMs are purely

01:03:12.480 --> 01:03:17.600
trained from text. So then the other statement you made, I would not agree with the fact that

01:03:18.320 --> 01:03:23.280
implicit in all languages in the world is the underlying reality. There's a lot of

01:03:23.280 --> 01:03:28.800
that underlying reality, which is not expressed in language. Is that obvious to you? Yeah, totally.

01:03:30.240 --> 01:03:38.240
So like all the conversations, okay, there's the dark web, meaning whatever, the private

01:03:38.240 --> 01:03:44.080
conversations like DMs and stuff like this, which is much, much larger probably than what's

01:03:44.080 --> 01:03:48.880
available, what LLMs are trained on. You don't need to communicate the stuff that is common.

01:03:49.840 --> 01:03:54.480
But the humor, all of it, no, you do, like when you, you don't need to, but it comes through,

01:03:54.480 --> 01:04:00.960
like if I accidentally knock this over, you'll probably make fun of me in the content of the

01:04:00.960 --> 01:04:08.160
you making fun of me will be an explanation of the fact that cups fall, and then, you know,

01:04:08.160 --> 01:04:14.080
gravity works in this way. And then you'll have some very vague information about what kind of

01:04:14.080 --> 01:04:19.280
things explode when they hit the ground. And then maybe you'll make a joke about entropy or something

01:04:19.280 --> 01:04:24.240
like this, and we'll never be able to reconstruct this again. Like, okay, you'll make a little

01:04:24.240 --> 01:04:29.040
joke like this, and there'll be trillion of other jokes. And from the jokes, you can piece together

01:04:29.040 --> 01:04:33.600
the fact that gravity works and mugs can break and all this kind of stuff, you don't need to see

01:04:35.280 --> 01:04:44.080
it'll be very inefficient. It's easier for like, not to think over. But I feel like it would be

01:04:44.080 --> 01:04:50.720
there if you have enough of that data. I just think that most of the information of this type that

01:04:50.800 --> 01:04:59.200
we have accumulated when we were babies is just not present in text, in any description,

01:04:59.200 --> 01:05:04.080
essentially. And the sensory data as much as a much richer source for getting that kind of

01:05:04.080 --> 01:05:10.880
understanding. I mean, that's the 16,000 hours of wake time of a four year old, and 10 to the 15

01:05:10.880 --> 01:05:16.880
bytes, you know, going through vision, just vision, right? There is a similar bandwidth, you know,

01:05:16.880 --> 01:05:23.360
of touch and a little less through audio. And then text doesn't language doesn't come in until like,

01:05:23.360 --> 01:05:29.040
you know, a year in life. And by the time you are nine years old, you've learned

01:05:29.920 --> 01:05:33.680
about gravity, you know, about inertia, you know, about gravity, you know, the stability,

01:05:33.680 --> 01:05:38.800
you know, you know, about the distinction between animate and animate objects, you know,

01:05:38.800 --> 01:05:45.040
by 18 months, you know, about like, why people want to do things and you help them if they can't,

01:05:45.040 --> 01:05:48.800
you know, I mean, there's a lot of things that you learn mostly by observation,

01:05:49.600 --> 01:05:54.240
really, not even through interaction in the first few months of life, babies don't

01:05:54.240 --> 01:05:57.040
don't really have any influence on the world, they can only observe, right?

01:05:57.920 --> 01:06:03.360
And you accumulate like a gigantic amount of knowledge just just from that. So that that's

01:06:03.360 --> 01:06:09.920
what we're missing from current AI systems. I think in one of your slides, you have this nice plot

01:06:10.000 --> 01:06:15.280
that is one of the ways you show that LLMs are limited. I wonder if you could talk about

01:06:15.280 --> 01:06:23.440
hallucinations from your perspectives, the why hallucinations happen from large language models

01:06:23.440 --> 01:06:28.640
and why and to what degrees that a fundamental flaw of large language models.

01:06:29.360 --> 01:06:36.960
Right. So because of the autoregressive prediction, every time an LLM produces a token or a word,

01:06:37.920 --> 01:06:43.840
there is some level of probability for that word to take you out of the set of reasonable answers.

01:06:45.520 --> 01:06:50.320
And if you assume, which is a very strong assumption that the probability of such error

01:06:53.120 --> 01:06:59.680
is that those errors are independent across a sequence of tokens being produced. What that

01:06:59.680 --> 01:07:05.520
means is that every time you produce a token, the probability that you stay within the set of

01:07:05.520 --> 01:07:10.160
correct answer decreases and it decreases exponentially. So there's a strong assumption

01:07:10.160 --> 01:07:15.600
there that if there's a non-zero probability of making a mistake, which there appears to be,

01:07:16.240 --> 01:07:22.160
then there's going to be a kind of drift. Yeah. And that drift is exponential. It's like errors

01:07:22.160 --> 01:07:29.440
accumulate, right? So the probability that an answer would be nonsensical increases exponentially

01:07:29.520 --> 01:07:34.000
with the number of tokens. Is that obvious to you, by the way? Well,

01:07:34.960 --> 01:07:40.480
so mathematically speaking, maybe, but isn't there a kind of gravitational pull towards

01:07:40.480 --> 01:07:48.960
the truth because on average, hopefully, the truth is well represented in the training set?

01:07:48.960 --> 01:07:56.480
No, it's basically a struggle against the curse of dimensionality. So the way you can correct

01:07:56.480 --> 01:08:02.400
for this is that you fine-tune the system by having it produce answers for all kinds of questions

01:08:02.400 --> 01:08:08.640
that people might come up with. And people are people. So a lot of the questions that they have

01:08:08.640 --> 01:08:14.560
are very similar to each other. So you can probably cover 80% or whatever of questions

01:08:14.560 --> 01:08:23.920
that people will ask by collecting data. And then you fine-tune the system to produce

01:08:24.000 --> 01:08:28.480
good answers for all of those things. And it's probably going to be able to learn that because

01:08:28.480 --> 01:08:37.680
it's got a lot of capacity to learn. But then there is the enormous set of prompts that you

01:08:37.680 --> 01:08:42.560
have not covered during training. And that set is enormous. Like within the set of all possible

01:08:42.560 --> 01:08:48.400
prompts, the proportion of prompts that have been used for training is absolutely tiny.

01:08:49.360 --> 01:08:55.920
It's a tiny, tiny, tiny subset of all possible prompts. And so the system will behave properly

01:08:55.920 --> 01:09:02.320
on the prompts that has been either trained, pre-trained, or fine-tuned. But then there is

01:09:02.320 --> 01:09:08.640
an entire space of things that it cannot possibly have been trained on because the number is

01:09:08.640 --> 01:09:17.920
gigantic. So whatever training the system has been subject to to produce appropriate answers,

01:09:17.920 --> 01:09:24.560
you can break it by finding out a prompt that will be outside of the set of prompts that's

01:09:24.560 --> 01:09:30.240
been trained on or things that are similar. And then it will just spew complete nonsense.

01:09:30.240 --> 01:09:35.120
When you say prompt, do you mean that exact prompt? Or do you mean a prompt that's like

01:09:35.920 --> 01:09:43.520
in many parts very different than? Is it that easy to ask a question or to say a thing that

01:09:43.520 --> 01:09:49.520
hasn't been said before on the internet? I mean, people have come up with things where you put

01:09:50.640 --> 01:09:54.960
essentially a random sequence of characters in the prompt. And that's enough to kind of throw the

01:09:54.960 --> 01:10:00.720
system into a mode where it's going to answer something completely different than it would

01:10:01.280 --> 01:10:05.920
have answered without this. So that's a way to jailbreak the system, basically get it,

01:10:05.920 --> 01:10:12.160
go outside of its conditioning. So that's a very clear demonstration of it. But of course,

01:10:14.240 --> 01:10:20.080
you know, that's that goes outside of what is designed to do, right? If you actually stitch

01:10:20.080 --> 01:10:25.520
together reasonably grammatical sentences, is that is it that easy to break it?

01:10:26.400 --> 01:10:32.240
Yeah, some people have done things like you you write a sentence in English, right, that has an

01:10:32.240 --> 01:10:37.680
or you ask a question in English, and it produces a perfectly fine answer. And then you just substitute

01:10:37.680 --> 01:10:43.760
a few words by the same word in another language. And all of a sudden, the answer is completely

01:10:43.760 --> 01:10:50.400
nonsense. Yes. So I guess what I'm saying is like, which fraction of prompts that humans are likely

01:10:51.120 --> 01:10:56.880
to generate are going to break the system. So the problem is that there is a long tail.

01:10:57.440 --> 01:11:03.600
Yes. This is an issue that a lot of people have realized, you know, in social networks and stuff

01:11:03.600 --> 01:11:09.360
like that, which is there's a very, very long tail of things that people will ask. And you can

01:11:09.360 --> 01:11:16.240
fine tune the system for the 80% or whatever of the things that most people will ask. And then

01:11:16.240 --> 01:11:21.040
this long tail is so large that you're not going to be able to fine tune the system for all the

01:11:21.040 --> 01:11:26.560
conditions. And in the end, the system has been kind of a giant lookup table, essentially,

01:11:26.560 --> 01:11:31.120
which is not really what you want. You want systems that can reason, certainly they can plan. So

01:11:31.200 --> 01:11:36.320
the type of reasoning that takes place in LLM is very, very primitive. And the reason you can tell

01:11:36.320 --> 01:11:42.800
is primitive is because the amount of computation that is spent per token produced is constant.

01:11:43.760 --> 01:11:49.520
So if you ask a question, and that question has an answer in a given number of token,

01:11:50.080 --> 01:11:55.200
the amount of computation devoted to computing that answer can be exactly estimated. It's like,

01:11:56.160 --> 01:12:02.720
it's the size of the prediction network with its 36 layers or 92 layers or whatever it is,

01:12:03.760 --> 01:12:09.600
multiplied by number of tokens. That's it. And so essentially, it doesn't matter if the question

01:12:09.600 --> 01:12:18.320
being asked is simple to answer, complicated to answer, impossible to answer, because it's

01:12:18.320 --> 01:12:24.400
undecidable or something. The amount of computation the system will be able to devote to the answer

01:12:24.400 --> 01:12:29.920
is constant or is proportional to the number of token produced in the answer. This is not the

01:12:29.920 --> 01:12:38.320
way we work. The way we reason is that when we're faced with a complex problem or complex question,

01:12:38.320 --> 01:12:43.680
we spend more time trying to solve it and answer it because it's more difficult.

01:12:43.680 --> 01:12:51.280
There's a prediction element. There's an iterative element where you're adjusting your

01:12:51.280 --> 01:12:55.920
understanding of a thing by going over and over and over. There's a hierarchical element,

01:12:55.920 --> 01:13:02.080
so on. Does this mean that it's a fundamental flaw of LLM? So does it mean that there's more

01:13:02.080 --> 01:13:09.360
part to that question? Now you're just behaving like an LLM, immediately answering. No, that

01:13:11.120 --> 01:13:18.000
it's just a low level world model on top of which we can then build some of these kinds of

01:13:18.080 --> 01:13:26.400
mechanisms, like you said, persistent long-term memory or reasoning, so on. But we need that world

01:13:26.400 --> 01:13:33.600
model that comes from language. Maybe it is not so difficult to build this kind of reasoning system

01:13:33.600 --> 01:13:40.800
on top of a well-constructed world model. Whether it's difficult or not, the near future will say

01:13:40.800 --> 01:13:46.400
because a lot of people are working on reasoning and planning abilities for dialogue systems.

01:13:47.360 --> 01:13:50.320
I mean, even if we restrict ourselves to language,

01:13:52.480 --> 01:13:58.960
just having the ability to plan your answer before you answer in terms that are not necessarily

01:13:58.960 --> 01:14:03.600
linked with the language you're going to use to produce the answer. So this idea of this mental

01:14:03.600 --> 01:14:11.600
model that allows you to plan what you're going to say before you say it. That is very important.

01:14:11.600 --> 01:14:15.840
I think there's going to be a lot of systems over the next few years that are going to have this

01:14:15.840 --> 01:14:21.840
capability. But the blueprint of those systems would be extremely different from autoregressive

01:14:21.840 --> 01:14:31.200
LLMs. So it's the same difference as the difference between what psychologists call system one and

01:14:31.200 --> 01:14:37.200
system two in humans. So system one is the type of task that you can accomplish without deliberately,

01:14:37.200 --> 01:14:44.080
consciously think about how you do them. You've done them enough that you can just do it

01:14:44.080 --> 01:14:49.040
subconsciously without thinking about them. If you're an experienced driver, you can drive

01:14:49.680 --> 01:14:53.520
without really thinking about it and you can talk to someone at the same time or listen to the radio.

01:14:55.520 --> 01:15:00.880
If you are a very experienced chess player, you can play against a non-experienced chess player

01:15:00.880 --> 01:15:06.320
without really thinking either. You just recognize the pattern that you play. That's system one.

01:15:08.000 --> 01:15:12.640
So all the things that you do instinctively without really having to deliberately plan and

01:15:12.640 --> 01:15:17.280
think about it. And then there is all the tasks where you need to plan. So if you are not too

01:15:17.280 --> 01:15:22.160
experienced chess player or you are experienced when you play against another experienced chess

01:15:22.160 --> 01:15:28.720
player, you think about all kinds of options. You think about it for a while and you're much

01:15:28.720 --> 01:15:34.400
better if you have time to think about it than you are if you play Blitz with limited time.

01:15:34.480 --> 01:15:42.000
So this type of deliberate planning which uses your internal world model,

01:15:43.360 --> 01:15:47.920
that's system two. This is what LLMs currently cannot do. So how do we get them to do this?

01:15:48.800 --> 01:15:55.680
How do we build a system that can do this kind of planning or reasoning that devotes

01:15:56.480 --> 01:16:01.600
more resources to complex problems than to simple problems? And it's not going to be

01:16:01.600 --> 01:16:08.720
autoregressive prediction of tokens. It's going to be more something akin to inference of latent

01:16:08.720 --> 01:16:16.720
variables in what used to be called probabilistic models or graphical models and things of that

01:16:16.720 --> 01:16:24.320
type. So basically the principle is like this. The prompt is like observed variables.

01:16:24.800 --> 01:16:35.040
And what the model does is that it's basically a measure of, it can measure to what extent

01:16:35.040 --> 01:16:41.040
an answer is a good answer for a prompt. So think of it as some gigantic neural net,

01:16:41.040 --> 01:16:46.960
but it's got only one output and that output is a scalar number which is, let's say zero,

01:16:46.960 --> 01:16:52.000
if the answer is a good answer for the question and a large number if the answer is not a good

01:16:52.000 --> 01:16:57.200
answer for the question. Imagine you had this model. If you had such a model, you could use it to

01:16:57.200 --> 01:17:03.840
produce good answers. The way you would do is produce the prompt and then search through

01:17:03.840 --> 01:17:11.440
the space of possible answers for one that minimizes that number. That's called an energy-based model.

01:17:11.440 --> 01:17:17.600
But that energy-based model would need the model constructed by the LLM?

01:17:18.400 --> 01:17:24.880
Well, so really what you need to do would be to not search over possible strings of text that

01:17:24.880 --> 01:17:31.600
minimize that energy. But what you would do is do this in abstract representation space. So in

01:17:31.600 --> 01:17:38.080
sort of the space of abstract thoughts, you would elaborate a thought using this process

01:17:38.720 --> 01:17:46.320
of minimizing the output of your model, which is just a scalar. It's an optimization process.

01:17:46.320 --> 01:17:50.560
So now the way the system produces its answer is through optimization,

01:17:52.400 --> 01:17:57.520
by minimizing an objective function, basically. And this is, we're talking about inference,

01:17:57.520 --> 01:18:01.680
we're not talking about training. The system has been trained already. So now we have an

01:18:01.680 --> 01:18:05.920
abstract representation of the thought of the answer, representation of the answer.

01:18:06.480 --> 01:18:12.960
We feed that to basically an autoreactive decoder, which can be very simple, that turns this into

01:18:13.040 --> 01:18:18.560
a text that expresses this thought. So that, in my opinion, is the blueprint of future

01:18:19.120 --> 01:18:25.760
data systems. They will think about their answer, plan their answer by optimization

01:18:25.760 --> 01:18:30.480
before turning it into text. And that is turning complete.

01:18:31.120 --> 01:18:36.560
Can you explain exactly what the optimization problem there is? What's the objective function?

01:18:37.520 --> 01:18:43.600
Just link on it, you kind of briefly described it, but over what space are you optimizing?

01:18:43.600 --> 01:18:44.960
The space of representations.

01:18:45.680 --> 01:18:47.040
Goes abstract representation.

01:18:47.040 --> 01:18:51.760
Abstract representation. So you have an abstract representation inside the system. You have a

01:18:51.760 --> 01:18:55.760
prompt. The prompt goes through an encoder, produces a representation, perhaps goes through

01:18:55.760 --> 01:18:59.920
a predictor that predicts a representation of the answer, of the proper answer. But that

01:18:59.920 --> 01:19:06.080
representation may not be a good answer because there might be some complicated

01:19:06.080 --> 01:19:13.200
reasoning you need to do. So then you have another process that takes the representation

01:19:13.200 --> 01:19:20.960
of the answers and modifies it so as to minimize a cost function that measures to what extent

01:19:20.960 --> 01:19:29.520
the answer is a good answer for the question. Now, we sort of ignore the issue for a moment

01:19:29.520 --> 01:19:35.760
of how you train that system to measure whether an answer is a good answer for a question.

01:19:35.840 --> 01:19:41.680
But suppose such a system could be created, but what's the process, this kind of search-like

01:19:41.680 --> 01:19:46.800
process? It's an optimization process. You can do this if the entire system is differentiable,

01:19:47.600 --> 01:19:54.320
that scalar output is the result of running through some neural net, running the answer,

01:19:54.320 --> 01:19:57.920
the representation of the answer through some neural net, then by gradient descent,

01:19:57.920 --> 01:20:03.360
by backpropagating gradients, you can figure out how to modify the representation of the

01:20:03.360 --> 01:20:09.680
answer. So that's still a gradient-based inference. So now you have a representation

01:20:09.680 --> 01:20:17.920
of the answer in abstract space. Now you can turn it into text. And the cool thing about this is that

01:20:18.560 --> 01:20:24.000
the representation now can be optimized through gradient descent, but also is independent of

01:20:24.000 --> 01:20:29.280
the language in which you're going to express the answer. So you're operating in the subtract

01:20:29.280 --> 01:20:34.000
representation. I mean, this goes back to the joint embedding that is better to work in the

01:20:35.280 --> 01:20:42.720
space of, I don't know, to romanticize the notion like space of concepts versus the space of concrete

01:20:44.480 --> 01:20:51.120
sensory information. Okay, but can this do something like reasoning, which is what we're

01:20:51.120 --> 01:20:55.840
talking about? Well, not really. Only in a very simple way. I mean, basically, you can think of

01:20:55.840 --> 01:21:01.200
those things as doing the kind of optimization I was talking about, except they optimize in

01:21:01.200 --> 01:21:06.960
the discrete space, which is the space of possible sequences of tokens. And they do this

01:21:06.960 --> 01:21:11.760
optimization in a horribly inefficient way, which is generate a lot of hypotheses and then select

01:21:11.760 --> 01:21:19.840
the best ones. And that's incredibly wasteful in terms of computation. Because you basically

01:21:19.840 --> 01:21:26.640
have to run your LLM for like every possible, you know, generating sequence. And it's incredibly

01:21:26.640 --> 01:21:34.320
wasteful. So it's much better to do an optimization in continuous space, where you can do gradient

01:21:34.320 --> 01:21:38.320
descent, as opposed to like generate tons of things and then select the best, you just

01:21:39.040 --> 01:21:44.000
iteratively refine your answer to go towards the best, right? That's much more efficient.

01:21:44.000 --> 01:21:48.000
But you can only do this in continuous spaces with differentiable functions.

01:21:48.000 --> 01:21:53.760
You're talking about the reasoning, like ability to think deeply or to reason deeply.

01:21:55.040 --> 01:22:04.560
How do you know what is an answer that's better or worse based on deep reasoning?

01:22:04.560 --> 01:22:08.640
Right. So then we're asking the question of conceptually, how do you train an energy-based

01:22:08.640 --> 01:22:13.520
model, right? So an energy-based model is a function with a scalar output, just a number.

01:22:13.680 --> 01:22:20.560
You give it two inputs, x and y, and it tells you whether y is compatible with x or not. x,

01:22:20.560 --> 01:22:27.920
you observe, let's say it's a prompt, an image, a video, whatever. And y is a proposal for an answer,

01:22:27.920 --> 01:22:34.160
a continuation of the video, you know, whatever. And it tells you whether y is compatible with x.

01:22:34.960 --> 01:22:39.120
And the way it tells you that y is compatible with x is that the output of that function would be

01:22:39.120 --> 01:22:45.600
zero. If y is compatible with x, it would be a positive number, non-zero, if y is not compatible

01:22:45.600 --> 01:22:53.120
with x. Okay, how do you train a system like this at a completely general level is you show it

01:22:53.920 --> 01:22:57.760
pairs of x and y that are compatible, a question and a corresponding answer.

01:22:58.560 --> 01:23:03.440
And you train the parameters of the big neural net inside to produce zero.

01:23:04.400 --> 01:23:08.880
Okay, now that doesn't completely work because the system might decide, well,

01:23:08.880 --> 01:23:13.760
I'm just going to say zero for everything. So now you have to have a process to make sure that for

01:23:14.480 --> 01:23:20.720
a wrong y, the energy would be larger than zero. And there you have two options. One is

01:23:20.720 --> 01:23:26.880
contrastive methods. So contrastive method is you show an x and a bad y. And you tell the system,

01:23:26.880 --> 01:23:31.120
well, that's, you know, give a high energy to this, like push up the energy, right? Change the

01:23:31.120 --> 01:23:37.440
weights in the neural net that confuse the energy so that it goes up. So that's contrastive methods.

01:23:37.440 --> 01:23:44.480
The problem with this is if the space of y is large, the number of such contrastive samples

01:23:44.480 --> 01:23:52.160
you're going to have to show is gigantic. But people do this. They do this when you train

01:23:52.160 --> 01:23:57.440
a system with RLHF, basically what you're training is what's called a reward model,

01:23:57.440 --> 01:24:02.560
which is basically an objective function that tells you whether an answer is good or bad. And

01:24:02.560 --> 01:24:08.880
that's basically exactly what this is. So we already do this to some extent. We're just not

01:24:08.880 --> 01:24:16.000
using it for inference. We're just using it for training. There is another set of methods which

01:24:16.000 --> 01:24:21.520
are non-contrastive. And I prefer those. And those non-contrastive methods basically say,

01:24:22.400 --> 01:24:29.440
okay, the energy function needs to have low energy on pairs of x, y's that are compatible,

01:24:29.440 --> 01:24:34.400
that come from your training set. How do you make sure that the energy is going to be higher

01:24:34.400 --> 01:24:43.200
everywhere else? And the way you do this is by having a regularizer, a criterion, a term in

01:24:43.200 --> 01:24:49.520
your cost function that basically minimizes the volume of space that can take low energy.

01:24:50.480 --> 01:24:54.240
And the precise way to do this is all kinds of different specific ways to do this depending

01:24:54.240 --> 01:24:58.800
on the architecture. But that's the basic principle. So that if you push down the energy

01:24:58.800 --> 01:25:04.640
function for particular regions in the x, y space, it will automatically go up in other places

01:25:04.640 --> 01:25:11.280
because it is only a limited volume of space that can take low energy, okay, by the construction

01:25:11.280 --> 01:25:14.720
of the system or by the regularizer, regularizing function.

01:25:15.360 --> 01:25:22.480
We've been talking very generally, but what is a good x and a good y? What is a good representation

01:25:22.480 --> 01:25:29.520
of x and y? Because we've been talking about language and if you just take language directly,

01:25:30.320 --> 01:25:34.960
that presumably is not good. So there has to be some kind of abstract representation of ideas.

01:25:36.000 --> 01:25:43.040
Yeah, so you can do this with language directly by just x is a text and y is a continuation of

01:25:43.040 --> 01:25:49.280
that text. Yes. Or x is a question, y is the answer. But you're saying that's not going to

01:25:49.280 --> 01:25:55.360
take it. I mean, that's going to do what LLMs are doing. Well, no, it depends on how the internal

01:25:55.360 --> 01:25:59.840
structure of the system is built. If the internal structure of the system is built in such a way

01:25:59.840 --> 01:26:08.560
that inside of the system, there is a latent variable, this could be z, that you can manipulate

01:26:09.280 --> 01:26:16.560
so as to minimize the output energy, then that z can be viewed as a representation of a good answer

01:26:16.560 --> 01:26:19.120
that you can translate into a y that is a good answer.

01:26:20.960 --> 01:26:23.600
So this kind of system could be trained in a very similar way?

01:26:24.400 --> 01:26:29.280
Very similar way, but you have to have this way of preventing collapse, of ensuring that

01:26:30.160 --> 01:26:37.600
there is high energy for things you don't train it on. And currently, it's very

01:26:37.680 --> 01:26:41.440
implicit in LLM. It's done in a way that people don't realize it's being done, but it is being

01:26:41.440 --> 01:26:49.680
done. It's due to the fact that when you give a high probability to a word, automatically,

01:26:49.680 --> 01:26:54.800
you give low probability to other words because you only have a finite amount of probability

01:26:54.800 --> 01:27:00.240
to go around right there, to some to one. So when you minimize the cross entropy or whatever,

01:27:00.240 --> 01:27:07.040
when you train your LLM to predict the next word, you're increasing the

01:27:07.040 --> 01:27:10.080
probability your system will give to the correct word, but you're also decreasing the

01:27:10.080 --> 01:27:17.600
probability it will give to the incorrect words. Now, indirectly, that gives a high probability

01:27:17.600 --> 01:27:21.280
to sequences of words that are good and low probability to sequences of words that are

01:27:21.280 --> 01:27:26.800
bad, but it's very indirect. And it's not obvious why this actually works at all, but

01:27:28.160 --> 01:27:32.720
because you're not doing it on a joint probability of all the symbols in a sequence,

01:27:32.720 --> 01:27:39.440
you're just doing it kind of factorize that probability in terms of conditional probabilities

01:27:39.440 --> 01:27:44.960
over successive tokens. So how do you do this for visual data? So we've been doing this with

01:27:44.960 --> 01:27:52.400
OJEPA architectures basically with IJEPA. So there, the compatibility between two things is,

01:27:52.960 --> 01:27:58.720
here's an image or a video. Here's a corrupted, shifted or transformed version of that image

01:27:58.720 --> 01:28:07.760
or video or masked. And then the energy of the system is the prediction error of the

01:28:09.280 --> 01:28:14.560
representation, the predicted representation of the good thing,

01:28:14.560 --> 01:28:20.880
versus the actual representation of the good thing. So you run the corrupted image to the system,

01:28:20.880 --> 01:28:26.400
predict the representation of the good input and corrupted, and then compute the prediction error.

01:28:26.400 --> 01:28:31.600
That's the energy of the system. So this system will tell you, this is a good,

01:28:33.520 --> 01:28:38.000
you know, this is a good image and this is a corrupted version. It will give you zero energy

01:28:38.000 --> 01:28:43.120
if those two things are effectively, one of them is a corrupted version of the other,

01:28:43.120 --> 01:28:46.480
give you a high energy if the two images are completely different.

01:28:46.480 --> 01:28:51.440
And hopefully that whole process gives you a really nice compressed representation of

01:28:52.400 --> 01:28:57.520
reality, of visual reality. And we know it does because then we use those for our presentations

01:28:57.520 --> 01:29:01.520
as input to a classification system. And then that classification system works really nicely.

01:29:01.520 --> 01:29:10.320
Okay. Well, so to summarize, you recommend in a spicy way that only Yalakoon can,

01:29:10.320 --> 01:29:14.880
you recommend that we abandon generative models in favor of joint embedding architectures.

01:29:14.880 --> 01:29:20.320
Yes. Abandon auto-aggressive generation. Yes. Abandon problem. This feels like

01:29:20.320 --> 01:29:25.040
court testimony. Abandon probabilistic models in favor of energy-based models,

01:29:25.040 --> 01:29:29.440
as we talked about, abandon contrastive methods in favor of regularized methods.

01:29:30.080 --> 01:29:36.240
And let me ask you about this. You've been for a while a critic of reinforcement learning.

01:29:36.240 --> 01:29:43.520
Yes. So the last recommendation is that we abandon RL in favor of model predictive control,

01:29:43.520 --> 01:29:49.520
as you were talking about, and only use RL when planning doesn't yield the predicted outcome.

01:29:50.320 --> 01:29:56.080
And we use RL in that case to adjust the world model or the critic. Yes. So

01:29:57.520 --> 01:30:04.720
you mentioned RLHF, reinforcement learning with human feedback. Why do you still hate

01:30:04.720 --> 01:30:09.200
reinforcement learning? I don't hate reinforcement learning. And I think it should not be

01:30:10.480 --> 01:30:16.400
abandoned completely. But I think its use should be minimized because it's incredibly inefficient

01:30:16.480 --> 01:30:23.040
in terms of samples. And so the proper way to train a system is to first have it learn

01:30:24.800 --> 01:30:29.440
good representations of the world and world models from mostly observation,

01:30:29.440 --> 01:30:33.840
maybe a little bit of interactions. And then steered based on that. If the representation

01:30:33.840 --> 01:30:38.880
is good, then the adjustments should be minimal. Yeah. Now there's two things. You can use,

01:30:38.880 --> 01:30:42.720
if you've learned a world model, you can use the world model to plan a sequence of actions to

01:30:42.720 --> 01:30:50.240
arrive at a particular objective. You don't need RL unless the way you measure whether you succeed

01:30:50.240 --> 01:30:58.960
might be an exact. Your idea of whether you're going to fall from your bike might be wrong.

01:30:59.680 --> 01:31:03.760
Or whether the person you're fighting with MMA was going to do something and then do something else.

01:31:05.440 --> 01:31:11.920
So there's two ways you can be wrong. Either your objective function

01:31:12.800 --> 01:31:17.920
does not reflect the actual objective function you want to optimize, or your world model is

01:31:17.920 --> 01:31:23.840
inaccurate. So the prediction you were making about what was going to happen in the world

01:31:23.840 --> 01:31:30.160
is inaccurate. So if you want to adjust your world model while you are operating the world,

01:31:30.800 --> 01:31:37.440
or your objective function, that is basically in the realm of RL. This is what RL deals with

01:31:38.240 --> 01:31:42.320
to some extent. So adjust your world model. And the way to adjust your world model,

01:31:42.960 --> 01:31:50.000
even in advance, is to explore parts of the space where you know that your world model is inaccurate.

01:31:50.560 --> 01:31:58.640
That's called curiosity, basically, or play. When you play, you explore parts of the state space

01:31:58.640 --> 01:32:07.360
that you don't want to do for real because it might be dangerous, but you can adjust your world

01:32:07.360 --> 01:32:16.000
model without killing yourself, basically. So that's what you want to use RL for. When it comes

01:32:16.000 --> 01:32:20.800
down to learning a particular task, you already have all the good world presentations, you already

01:32:20.800 --> 01:32:26.480
have your world model, but you need to adjust it for the situation at hand. That's when you use RL.

01:32:26.560 --> 01:32:32.480
What do you think RLHF works so well, this enforcement learning with human feedback?

01:32:32.480 --> 01:32:38.000
What did it have such a transformational effect on large language models it came before?

01:32:38.000 --> 01:32:43.840
What's had the transformational effect is human feedback. There is many ways to use it, and some

01:32:43.840 --> 01:32:47.280
of it is just purely supervised, actually. It's not really reinforcement learning.

01:32:47.280 --> 01:32:53.520
So it's the HF. It's the HF. And then there is various ways to use human feedback, right? So you

01:32:54.240 --> 01:32:59.760
can ask humans to rate answers, multiple answers that are produced by a world model,

01:33:00.640 --> 01:33:07.040
and then what you do is you train an objective function to predict that rating,

01:33:08.160 --> 01:33:13.840
and then you can use that objective function to predict whether an answer is good, and you can

01:33:13.840 --> 01:33:19.120
back propagate gradient through this to fine tune your system so that it only produces highly-rated

01:33:19.120 --> 01:33:28.320
answers. So that's one way. In RL, that means training what's called a reward model,

01:33:29.200 --> 01:33:33.680
so something that basically a small neural net that estimates to what extent an answer is good.

01:33:34.880 --> 01:33:40.320
It's very similar to the objective I was talking about earlier for planning, except now it's not

01:33:40.320 --> 01:33:45.360
used for planning. It's used for fine-tuning your system. I think it would be much more efficient

01:33:45.440 --> 01:33:52.480
to use it for planning, but currently it's used to fine-tune the parameters of the system.

01:33:52.480 --> 01:33:59.600
Now, there are several ways to do this. Some of them are supervised. You just ask a human person,

01:33:59.600 --> 01:34:07.200
like, what is a good answer for this? Then you just type the answer. There's lots of ways that

01:34:07.200 --> 01:34:14.320
those systems are being adjusted. Now, a lot of people have been very critical of the recently

01:34:14.320 --> 01:34:24.080
released Google's Gemini 1.5 for essentially, in my words, I could say super woke, woke in the

01:34:24.080 --> 01:34:30.240
negative connotation of that word. There are some almost hilariously absurd things that it does,

01:34:30.240 --> 01:34:37.840
like it modifies history, like generating images of a Black George Washington, or

01:34:38.400 --> 01:34:46.800
perhaps more seriously something that you commented on Twitter, which is refusing to comment on or

01:34:46.800 --> 01:34:55.920
generate images of, or even descriptions of Tiananmen Square or the Tankman, one of the most

01:34:55.920 --> 01:35:05.360
sort of legendary protest images in history. Of course, these images are highly censored by

01:35:05.360 --> 01:35:11.120
the Chinese government. Therefore, everybody started asking questions of what is the process of

01:35:13.200 --> 01:35:18.240
designing these LLMs, what is the role of censorship and all that kind of stuff.

01:35:18.960 --> 01:35:27.520
So you commented on Twitter saying that open source is the answer, essentially. So can you

01:35:27.520 --> 01:35:36.160
explain? I actually made that comment on just about every social network I can. I've made that

01:35:36.160 --> 01:35:45.280
point multiple times in various forums. Here's my point of view on this. People can complain that

01:35:45.280 --> 01:35:51.920
AI systems are biased, and they generally are biased by the distribution of the training data

01:35:51.920 --> 01:36:03.040
that they've been trained on that reflects biases in society, and that is potentially offensive to

01:36:03.040 --> 01:36:12.800
some people, or potentially not. And some techniques to de-bias then become offensive to some people

01:36:13.040 --> 01:36:20.640
because of historical incorrectness and things like that.

01:36:23.040 --> 01:36:27.200
And so you can ask the question, you can ask two questions. The first question is,

01:36:27.200 --> 01:36:32.400
is it possible to produce an AI system that is not biased? And the answer is absolutely not.

01:36:33.120 --> 01:36:38.000
And it's not because of technological challenges, although there are

01:36:38.960 --> 01:36:45.200
technological challenges to that. It's because bias is in the eye of the beholder.

01:36:46.640 --> 01:36:53.600
Different people may have different ideas about what constitutes bias for a lot of things. I mean,

01:36:53.600 --> 01:37:00.400
there are facts that are indisputable, but there are a lot of opinions or things that can be expressed

01:37:00.400 --> 01:37:06.080
in different ways. And so you cannot have an unbiased system that's just an impossibility.

01:37:08.560 --> 01:37:15.760
And so what's the answer to this? And the answer is the same answer that we've found

01:37:16.480 --> 01:37:23.840
in liberal democracy about the press. The press needs to be free and diverse.

01:37:25.280 --> 01:37:34.720
We have free speech for a good reason, is because we don't want all of our information to come from

01:37:34.720 --> 01:37:43.600
a unique source, because that's opposite to the whole idea of democracy and progress of ideas and

01:37:43.600 --> 01:37:50.400
even science. In science, people have to argue for different opinions and science makes progress

01:37:50.400 --> 01:37:55.680
when people disagree and they come up with an answer and a consensus forms. And it's true in

01:37:55.680 --> 01:38:02.560
all democracies around the world. So there is a future, which is already happening,

01:38:03.200 --> 01:38:10.080
where every single one of our interaction with the digital world will be mediated by AI systems.

01:38:12.800 --> 01:38:18.000
We're going to have smart glasses. You can already buy them from Meta, the Ray-Ban Meta,

01:38:18.000 --> 01:38:23.520
where you can talk to them and they are connected with an LLM and you can get answers

01:38:23.520 --> 01:38:29.680
on any question you have. Or you can be looking at a monument and there is a camera in the

01:38:30.560 --> 01:38:35.440
system that in the glasses, you can ask it, like, what can you tell me about this building,

01:38:35.440 --> 01:38:40.080
on this monument? You can be looking at a menu in a foreign language and I think we'll translate

01:38:40.080 --> 01:38:46.240
it for you or we can do real-time translation if we speak different languages. So a lot of our

01:38:46.240 --> 01:38:50.560
interactions with the digital world are going to be mediated by those systems in the near future.

01:38:53.040 --> 01:38:58.080
Increasingly, the search engines that we're going to use are not going to be search engines. They're

01:38:58.080 --> 01:39:06.400
going to be dialogue systems that we just ask a question and it will answer and then point you to

01:39:06.400 --> 01:39:11.920
perhaps an appropriate reference for it. But here is the thing. We cannot afford those systems to

01:39:11.920 --> 01:39:18.560
come from a handful of companies on the west coast of the US because those systems will

01:39:18.560 --> 01:39:26.080
constitute the repository of all human knowledge and we cannot have that be controlled by a small

01:39:26.080 --> 01:39:30.720
number of people, right? It has to be diverse. For the same reason, the press has to be diverse.

01:39:32.080 --> 01:39:39.280
So how do we get a diverse set of AI assistance? It's very expensive and difficult to train a

01:39:39.280 --> 01:39:43.760
base model, right? A base LLM at the moment, you know, in the future might be something different,

01:39:43.760 --> 01:39:50.720
but at the moment that's an LLM. So only a few companies can do this properly. And

01:39:51.040 --> 01:39:56.960
if some of those top systems are open source, anybody can use them. Anybody can fine tune them.

01:39:56.960 --> 01:40:08.160
If we put in place some systems that allows any group of people, whether they are individual

01:40:08.160 --> 01:40:18.560
citizens, groups of citizens, government organizations, NGOs, companies, whatever, to take those open

01:40:19.520 --> 01:40:26.320
source systems, AI systems and fine tune them for their own purpose on their own data,

01:40:27.520 --> 01:40:32.800
they were going to have a very large diversity of different AI systems that are specialized for

01:40:32.800 --> 01:40:38.400
all of those things, right? So I tell you, I talked to the French government quite a bit and the

01:40:38.400 --> 01:40:45.600
French government will not accept that the digital diet of all their citizens be controlled by

01:40:45.600 --> 01:40:50.080
three companies in the West Coast of the US. That's just not acceptable. It's a danger to

01:40:50.080 --> 01:40:58.080
democracy, regardless of how well-intentioned those companies are, right? And so, and it's also

01:40:58.080 --> 01:41:10.640
a danger to local culture, to values, to language, right? I was talking with the founder of Infosys

01:41:10.640 --> 01:41:18.960
in India. He's funding a project to fine tune Lamatu, the open source model produced by META,

01:41:19.760 --> 01:41:26.240
so that Lamatu speaks all 22 official languages in India. It's very important for people in India.

01:41:26.240 --> 01:41:30.560
I was talking to a former colleague of mine, Mustafa Assisi, who used to be a scientist at fair,

01:41:31.120 --> 01:41:35.360
and then moved back to Africa. I created a research lab for Google in Africa and now

01:41:36.000 --> 01:41:40.880
is as a new startup called CARA. And what he's trying to do is basically have LLM that speaks

01:41:40.880 --> 01:41:46.400
the local languages in Senegal so that people can have access to medical information because they

01:41:46.400 --> 01:41:53.920
don't have access to doctors. It's a very small number of doctors per capita in Senegal. I mean,

01:41:53.920 --> 01:41:59.040
you can't have any of this unless you have open source platforms. So with open source platforms,

01:41:59.040 --> 01:42:04.000
you can have AI systems that are not only diverse in terms of political opinions or things of that

01:42:04.000 --> 01:42:12.720
type, but in terms of language, culture, value systems, political opinions,

01:42:15.360 --> 01:42:22.080
technical abilities in various domains. And you can have an industry, an ecosystem of companies

01:42:22.080 --> 01:42:27.200
that fine tune those open source systems for vertical applications in industry, right? You

01:42:27.200 --> 01:42:32.000
have, I don't know, a publisher has thousands of books, and they want to build a system that

01:42:32.000 --> 01:42:36.720
allows a customer to just ask a question about any, about the content of any of their books.

01:42:37.440 --> 01:42:43.120
You need to train on their proprietary data, right? You have a company, we have one within

01:42:43.120 --> 01:42:48.080
Meta, it's called MetaMate, and it's basically an LLM that can answer any question about internal

01:42:49.760 --> 01:42:55.680
stuff about the company. Very useful. A lot of companies want this, right? A lot of companies

01:42:55.680 --> 01:42:59.920
want this not just for their employees, but also for their customers to take care of their customers.

01:43:00.640 --> 01:43:05.440
So the only way you're going to have an AI industry, the only way you're going to have

01:43:05.440 --> 01:43:10.880
AI systems that are not uniquely biased is if you have open source platforms on top of which

01:43:12.240 --> 01:43:22.800
any group can build specialized systems. So the direction of inevitable direction of history

01:43:22.800 --> 01:43:28.320
is that the vast majority of AI systems will be built on top of open source platforms.

01:43:28.320 --> 01:43:36.800
So that's a beautiful vision. So meaning like a company like Meta or Google or so on

01:43:37.840 --> 01:43:43.920
should take only minimal fine-tuning steps after the building the foundation pre-trained model

01:43:44.720 --> 01:43:52.960
as few steps as possible. Basically. Can Meta afford to do that? No. So I don't know if you

01:43:52.960 --> 01:44:00.000
know this, but companies are supposed to make money somehow, and open source is giving away,

01:44:00.560 --> 01:44:11.440
I don't know, Mark made a video, Mark Zuckerberg, very sexy video talking about 350,000 NVIDIA H100s.

01:44:13.040 --> 01:44:21.280
The math of that is just for the GPUs, that's 100 billion, plus the infrastructure for training

01:44:21.280 --> 01:44:28.640
everything. So I'm no business guy. But how do you make money on that? So the division you paint

01:44:28.640 --> 01:44:35.120
is a really powerful one. But how is it possible to make money? Okay, so you have several business

01:44:35.120 --> 01:44:46.800
models, right? The business model that Meta is built around is your first service. And the

01:44:46.800 --> 01:44:53.200
financing of that service is either through ads or through business customers. So for example,

01:44:53.200 --> 01:45:02.080
if you have an LLM that can help a mom and pop pizza place by talking to the customers

01:45:02.080 --> 01:45:07.920
with WhatsApp, and so the customers can just order a pizza and the system will just, you know,

01:45:07.920 --> 01:45:13.920
has them like what topping do you want or websites, blah, blah, blah. The business will pay for that.

01:45:14.000 --> 01:45:23.760
Okay, that's a model. And otherwise, you know, if it's a system that is on the more kind of classical

01:45:23.760 --> 01:45:28.880
services, it can be ad supported or, you know, the several models. But the point is,

01:45:30.400 --> 01:45:36.000
if you have a big enough potential customer base, and you need to build that system

01:45:37.360 --> 01:45:43.200
anyway, for them, it doesn't hurt you to actually distribute it in open source.

01:45:43.200 --> 01:45:48.000
Again, I'm no business guy. But if you release the open source model,

01:45:48.000 --> 01:45:55.280
then other people can do the same kind of task and compete on it, basically provide fine tune

01:45:55.280 --> 01:46:01.360
models for businesses. As the bet that Meta is making, by the way, I'm a huge fan of all this,

01:46:01.360 --> 01:46:04.960
but as the bet that Meta is making is like, we'll do a better job of it.

01:46:05.520 --> 01:46:12.720
Well, no, the bet is more we have, we already have a huge user base and customer base.

01:46:13.760 --> 01:46:18.320
So it's going to be useful to them, whatever we offer them is going to be useful. And there is

01:46:18.320 --> 01:46:26.320
a way to derive revenue from this. And it doesn't hurt that we provide that system

01:46:27.680 --> 01:46:34.560
or the base model, the foundation model in open source for others to build applications on top

01:46:34.560 --> 01:46:39.280
of it too. If those applications turn out to be useful for our customers, we can just buy it from

01:46:39.280 --> 01:46:46.480
them. It could be that they will improve the platform. In fact, we see this already. I mean,

01:46:46.480 --> 01:46:53.600
there is literally millions of downloads of Lama 2 and thousands of people who have provided ideas

01:46:53.600 --> 01:47:00.880
about how to make it better. So this clearly accelerates progress to make the system available to

01:47:02.240 --> 01:47:07.760
it's sort of a wide community of people. And there is literally thousands of businesses who

01:47:07.760 --> 01:47:18.160
are building applications with it. So our ability to, Meta's ability to derive revenue

01:47:18.160 --> 01:47:26.320
from this technology is not impaired by the distribution of it, of base models in open source.

01:47:26.320 --> 01:47:30.880
The fundamental criticism that Gemini is getting is that, as you pointed out on the west coast,

01:47:30.880 --> 01:47:37.280
just to clarify, we're currently in the east coast where I would suppose Meta AI headquarters would

01:47:37.280 --> 01:47:45.600
be. So there are strong words about the west coast, but I guess the issue that happens is

01:47:46.720 --> 01:47:53.440
I think it's fair to say that most tech people have a political affiliation with the left wing.

01:47:53.440 --> 01:47:59.520
They lean left. And so the problem that people are criticizing Gemini with is that there's

01:47:59.520 --> 01:48:08.480
in that debiasing process that you mentioned that their ideological lean becomes obvious.

01:48:09.840 --> 01:48:16.960
Is this something that could be escaped? You're saying open source is the only way.

01:48:16.960 --> 01:48:22.560
Have you witnessed this kind of ideological lean that makes engineering difficult?

01:48:22.560 --> 01:48:26.640
No, I don't think it has to do, I don't think the issue has to do with the political leaning

01:48:26.640 --> 01:48:34.800
of the people designing those systems. It has to do with the acceptability or political leanings

01:48:34.800 --> 01:48:42.800
of their customer base or audience. So a big company cannot afford to offend too many people.

01:48:43.600 --> 01:48:50.080
So they're going to make sure that whatever product they put out is safe, whatever that means.

01:48:50.080 --> 01:48:59.200
And it's very possible to overdo it. And it's also very possible to, it's impossible to do it

01:48:59.200 --> 01:49:04.080
properly for everyone. You're not going to satisfy everyone. So that's what I said before. You cannot

01:49:04.080 --> 01:49:11.600
have a system that is unbiased and is perceived as unbiased by everyone. You push it in one way,

01:49:11.600 --> 01:49:15.600
one set of people are going to see it as biased, and then you push it the other way,

01:49:15.600 --> 01:49:19.680
and another set of people is going to see it as biased. And then in addition to this,

01:49:19.760 --> 01:49:24.160
there's the issue of if you push the system perhaps a little too far in one direction,

01:49:24.160 --> 01:49:31.280
it's going to be nonfactual, right? You're going to have Black Nazi soldiers in the...

01:49:31.280 --> 01:49:38.880
Yes, we should mention image generation of Black Nazi soldiers, which is not factually accurate.

01:49:38.880 --> 01:49:46.880
Right. And can be offensive for some people as well. So it's going to be impossible to

01:49:46.880 --> 01:49:51.440
kind of produce systems that are unbiased for everyone. So the only solution that I see is

01:49:51.440 --> 01:49:56.880
diversity. And diversity in full meaning of that word, diversity in every possible way.

01:49:59.200 --> 01:50:08.240
Mark Andreessen just tweeted today, let me do a TLDR. The conclusion is only startups and open

01:50:08.240 --> 01:50:14.880
source can avoid the issue that he's highlighting with Big Tech. He's asking, can Big Tech actually

01:50:14.880 --> 01:50:21.760
field generative AI products? One, ever escalating demands from internal activists, employee mobs,

01:50:21.760 --> 01:50:27.120
crazed executives, broken boards, pressure groups, extremist regulators, government agencies,

01:50:27.120 --> 01:50:35.600
the press, in quotes, experts, and everything corrupting the output? Two, constant risk of

01:50:35.600 --> 01:50:42.480
generating a bad answer or drawing a bad picture or rendering a bad video. Who knows what is going

01:50:42.560 --> 01:50:49.040
to say or do at any moment. Three, legal exposure, product liability, slander, election law, many

01:50:49.040 --> 01:50:56.560
other things, and so on. Anything that makes Congress mad. Four, continuous attempts to

01:50:56.560 --> 01:51:01.040
tighten grip on acceptable output to grade the model, like how good it actually is,

01:51:02.400 --> 01:51:06.800
in terms of usable and pleasant to use and effective and all that kind of stuff.

01:51:06.880 --> 01:51:12.720
And five, publicity of bad text, images, video, actual puts those examples into the training

01:51:12.720 --> 01:51:19.280
data for the next version, and so on. So he just highlights how difficult this is from all kinds

01:51:19.280 --> 01:51:26.160
of people being unhappy. He said you can't create a system that makes everybody happy. So if you're

01:51:26.160 --> 01:51:33.200
going to do the fine-tuning yourself and keep a closed source, essentially the problem there is

01:51:33.200 --> 01:51:38.640
then trying to minimize the number of people who are going to be unhappy. And you're saying

01:51:38.640 --> 01:51:44.480
that almost impossible to do right, and that's the better ways to do open source.

01:51:45.520 --> 01:51:54.560
Basically, yeah. Mark is right about a number of things that he lists that indeed scare large

01:51:54.560 --> 01:52:04.880
companies. Certainly congressional investigations is one of them. Legal liability, making things

01:52:04.880 --> 01:52:14.160
that get people to hurt themselves or hurt others. Big companies are really careful about not producing

01:52:14.160 --> 01:52:21.840
things of this type because they don't want to hurt anyone, first of all, and then second,

01:52:21.840 --> 01:52:26.960
don't want to preserve their business. So it's essentially impossible for systems like this to

01:52:26.960 --> 01:52:33.120
can inevitably formulate political opinions and opinions about various things that may be

01:52:33.120 --> 01:52:43.360
political or not, but that people may disagree about moral issues and questions about religion

01:52:43.360 --> 01:52:48.400
and things like that, or cultural issues that people from different communities would disagree

01:52:48.400 --> 01:52:52.960
with in the first place. So there's only kind of a relatively small number of things that people

01:52:52.960 --> 01:53:00.640
will agree on basic principles. But beyond that, if you want those systems to be useful,

01:53:01.760 --> 01:53:07.920
they will necessarily have to offend a number of people inevitably.

01:53:09.040 --> 01:53:13.200
And so open source is just better. And then diversity is better, right?

01:53:13.200 --> 01:53:17.520
And open source enables diversity. That's right. Open source enables diversity.

01:53:18.080 --> 01:53:22.800
That's going to be fascinating world where if it's true that the open source world,

01:53:22.800 --> 01:53:27.440
if Meta Lee is the way and the crisis kind of open source foundation model world,

01:53:27.440 --> 01:53:34.560
there's going to be like governments will have a fine two model. And then potentially,

01:53:36.400 --> 01:53:41.040
you know, people that vote left and right will have their own model and preference to be able to

01:53:41.040 --> 01:53:47.600
choose. And it will potentially divide us even more. But that's on us humans, we get to figure

01:53:47.600 --> 01:53:55.200
out basically the technology enables humans to human more effectively. And all the difficult

01:53:55.200 --> 01:54:01.680
ethical questions that humans raise will just leave it up to us to figure it out.

01:54:02.480 --> 01:54:06.000
Yeah. I mean, there are some limits to what, you know, the same way there are limits to free

01:54:06.080 --> 01:54:11.360
speech, there has to be some limit to the kind of stuff that those systems might be authorized to

01:54:13.200 --> 01:54:18.560
produce, you know, some guardrails. So I mean, that's one thing I've been interested in, which is

01:54:19.680 --> 01:54:25.360
in the type of architecture that we were discussing before, where the output of the system

01:54:26.080 --> 01:54:31.680
is a result of an inference to satisfy an objective that objective can include guardrails.

01:54:32.640 --> 01:54:39.040
And we can put guardrails in open source systems. I mean, if we eventually have systems that are

01:54:39.040 --> 01:54:45.040
built with this blueprint, we can put guardrails in those systems that guarantee that there is

01:54:45.040 --> 01:54:50.960
sort of a minimum set of guardrails that make the system non-dangerous and non-toxic, etc.

01:54:50.960 --> 01:54:57.280
You know, basic things that everybody would agree on. And then, you know, the fine tuning that

01:54:57.360 --> 01:55:01.840
people will add or the additional guardrails that people will add will kind of cater to their

01:55:03.040 --> 01:55:09.040
community, whatever it is. And yeah, the fine tuning will be more about the gray areas of what is

01:55:09.040 --> 01:55:12.800
hate speech, what is dangerous and all that kind of stuff. I mean, you've- With different value

01:55:12.800 --> 01:55:17.600
systems. Still value systems. I mean, like, but still even with the objectives of how to build a

01:55:17.600 --> 01:55:22.080
bio weapon, for example, I think something you've commented on, or at least there's a paper

01:55:23.040 --> 01:55:28.480
where a collection of researchers is trying to understand the social impacts of these LLMs.

01:55:29.280 --> 01:55:37.040
And I guess one threshold is nice. It's like, does the LLM make it any easier than a search

01:55:37.040 --> 01:55:45.360
would, like a Google search would? Right. So the increasing number of studies on this seems to point

01:55:45.360 --> 01:55:55.680
to the fact that it doesn't help. So having an LLM doesn't help you design or build a bio weapon

01:55:55.680 --> 01:56:00.800
or a chemical weapon, if you already have access to, you know, a search engine and a library.

01:56:02.320 --> 01:56:06.480
And so the sort of increased information you get or the ease with which you get it doesn't really

01:56:06.480 --> 01:56:12.560
help you. That's the first thing. The second thing is, it's one thing to have a list of instructions

01:56:12.560 --> 01:56:18.160
of how to make a chemical weapon, for example, a bio weapon. It's another thing to actually

01:56:18.720 --> 01:56:23.680
build it. And it's much harder than you might think. And then LLM will not help you with that.

01:56:25.600 --> 01:56:29.680
In fact, you know, nobody in the world, not even like, you know, countries use bio weapons because

01:56:30.800 --> 01:56:36.000
most of the times they have no idea to protect their own populations against it. So it's too

01:56:36.000 --> 01:56:43.200
dangerous actually to kind of ever use. And it's in fact banned by international treaties.

01:56:44.000 --> 01:56:51.280
Chemical weapons is different. It's also banned by treaties. But it's the same problem. It's difficult

01:56:51.280 --> 01:56:57.760
to use in situations that doesn't turn against the perpetrators. But we could ask Elon Musk,

01:56:57.760 --> 01:57:03.120
like I can give you a very precise list of instructions of how you build a rocket engine.

01:57:04.080 --> 01:57:08.480
And even if you have a team of 15 engineers sort of re experience building it, you're still

01:57:08.480 --> 01:57:13.840
going to have to blow up a dozen of them before you get one that works. And, you know, it's the same

01:57:13.840 --> 01:57:21.280
with, you know, chemical weapons or bio weapons or things like this. It requires expertise,

01:57:21.280 --> 01:57:25.120
you know, in the real world that LLM is not going to help you with.

01:57:25.120 --> 01:57:30.960
And it requires even the common sense expertise that we've been talking about, which is how to take

01:57:31.360 --> 01:57:38.000
language based instructions and materialize them in the physical world requires a lot of

01:57:38.000 --> 01:57:44.000
knowledge. It's not in the instructions. Yeah, exactly. A lot of biologists have posted on this

01:57:44.000 --> 01:57:48.560
actually in response to those things saying like, do you realize how hard it is to actually do the

01:57:48.560 --> 01:57:55.520
lab work? And I can know this is not trivial. Yeah. And that's Hans Mervic comes comes to light

01:57:55.520 --> 01:58:01.120
once again. Just the linger on llama, you know, Mark announced that llama three is coming out

01:58:01.120 --> 01:58:07.040
eventually, I don't think there's a release date. But what, what are you most excited about? First

01:58:07.040 --> 01:58:11.840
of all, llama two that's already out there, and maybe the future llama three, four, five, six,

01:58:11.840 --> 01:58:19.520
10, just the future of the open source under meta. Well, number of things. So there's going to be

01:58:19.520 --> 01:58:27.280
like various versions of llama that are, you know, improvements of previous llamas, bigger,

01:58:27.280 --> 01:58:33.440
better, multimodal, things like that. And then in future generations, systems that are capable of

01:58:33.440 --> 01:58:38.240
planning that really understand how the world works, maybe are trained from video, so they have

01:58:38.240 --> 01:58:42.960
some world model, maybe, you know, capable of the type of reasoning and planning I was talking

01:58:42.960 --> 01:58:47.600
about earlier, like, how long is that going to take? Like, when is the research that is going

01:58:47.680 --> 01:58:54.000
in that direction going to sort of feed into the product line? If you want a llama, I don't know,

01:58:54.000 --> 01:58:57.680
I can tell you, and there's, you know, a few breakthroughs that we have to basically

01:58:59.120 --> 01:59:06.160
go through before we can get there. But you'll be able to monitor our progress because we publish

01:59:06.160 --> 01:59:12.720
our research, right? So, you know, if last week we published the VJPA work, which is sort of a

01:59:12.720 --> 01:59:18.880
first step towards training systems from video. And then the next step is going to be world models

01:59:18.880 --> 01:59:24.960
based on kind of this type of idea, training, training from video. There's similar work at

01:59:24.960 --> 01:59:33.680
a deep mind also, and taking place people, and also at UC Berkeley on world models from video.

01:59:33.680 --> 01:59:37.600
A lot of people are working on this. I think a lot of good ideas are coming, are appearing.

01:59:38.400 --> 01:59:42.320
My bet is that those systems are going to be JEPA-like, they're not going to be

01:59:42.880 --> 01:59:51.440
generating models. And we'll see what the future will tell. There's really good work at

01:59:54.080 --> 01:59:57.600
a gentleman called Daniel J. Hafner, who is not a deep mind, who has worked on

01:59:57.600 --> 02:00:01.360
kind of models of this type that learn representations and then use them for

02:00:01.360 --> 02:00:07.520
planning or learning tasks by reinforcement learning. And a lot of work at Berkeley by

02:00:08.800 --> 02:00:14.640
Peter I. Beal, Saagya Levine, a bunch of other people of that type. I'm collaborating with actually

02:00:14.640 --> 02:00:22.560
in the context of some grants with my NYU hat. And then collaborations also through META because

02:00:23.680 --> 02:00:29.840
the lab at Berkeley is associated with META in some way, so with fair. So I think it's very

02:00:29.840 --> 02:00:36.560
exciting. I think I'm super excited about, I haven't been that excited about the direction

02:00:36.560 --> 02:00:42.800
of machine learning and AI since 10 years ago when fair was started. And before that,

02:00:43.920 --> 02:00:51.680
30 years ago, we were working on, what's it, 35 on computational nets and the early days of neural

02:00:51.680 --> 02:00:58.960
nets. So I'm super excited because I see a path towards potentially human level intelligence.

02:01:00.240 --> 02:01:08.720
With systems that can understand the world, remember, plan, reason. There is some set

02:01:08.720 --> 02:01:13.200
of ideas to make progress there that might have a chance of working. And I'm really excited about

02:01:13.200 --> 02:01:23.280
this. What I like is that we get onto a good direction and perhaps succeed before my brain

02:01:23.280 --> 02:01:31.280
turns to a white sauce or before I need to retire. Yeah. You're also excited by,

02:01:34.240 --> 02:01:41.200
is it beautiful to you just the amount of GPUs involved, the whole training process on this

02:01:41.200 --> 02:01:47.920
much compute? Just zooming out, just looking at Earth and humans together have built these

02:01:48.000 --> 02:01:55.280
computing devices and are able to train this one brain. Then we then open source,

02:01:57.520 --> 02:02:04.240
like giving birth to this open source brain trained on this gigantic compute system.

02:02:04.240 --> 02:02:10.000
There's just the details of how to train on that, how to build the infrastructure and the hardware,

02:02:10.000 --> 02:02:15.120
the cooling, all of this kind of stuff. Are you just still the most of your excitement is in the

02:02:15.120 --> 02:02:21.360
theory aspect of it? Meaning like the software? Well, I used to be a hardware guy many years ago.

02:02:23.040 --> 02:02:30.160
Hardware has improved a little bit, changed a little bit. I mean, certainly scale is necessary,

02:02:30.720 --> 02:02:35.840
but not sufficient. Absolutely. So we certainly need computation. I mean, we're still far in terms

02:02:35.840 --> 02:02:41.760
of compute power from what we would need to match the compute power of the human brain.

02:02:42.720 --> 02:02:48.160
This may occur in the next couple of decades, but we're still some ways away. And certainly in

02:02:48.160 --> 02:02:55.200
terms of power efficiency, we're really far. So there's a lot of progress to make in hardware.

02:02:57.440 --> 02:03:02.960
Right now, a lot of progress is not, I mean, there's a bit coming from Silicon technology,

02:03:02.960 --> 02:03:07.440
but a lot of it coming from architectural innovation and quite a bit coming from

02:03:07.760 --> 02:03:13.520
more efficient ways of implementing the architectures that have become popular,

02:03:13.520 --> 02:03:21.280
basically combination of transformers and components. So there's still some ways to go

02:03:22.160 --> 02:03:30.000
until we're going to saturate, we're going to have to come up with new principles,

02:03:30.000 --> 02:03:38.160
new fabrication technology, new basic components, perhaps based on sort of different

02:03:38.160 --> 02:03:45.120
principles and those classical digital CMOS. Interesting. So you think in order to build

02:03:46.080 --> 02:03:51.760
AMI, I mean, we need, we potentially might need some hardware innovation too.

02:03:52.720 --> 02:03:57.520
Well, if we want to make it ubiquitous, yeah, certainly, because we're going to have to reduce

02:03:57.520 --> 02:04:05.280
the power consumption. A GPU today is half a kilowatt to a kilowatt.

02:04:06.720 --> 02:04:13.600
Human brain is about 25 watts. And a GPU is way below the power of human brain. You need

02:04:13.600 --> 02:04:19.760
something like 100,000 or a million to match it. So we are off by a huge factor here.

02:04:20.320 --> 02:04:27.520
You often say that AGI is not coming soon, meaning like not this year,

02:04:28.400 --> 02:04:35.040
not the next few years, potentially farther away. What's your basic intuition behind that?

02:04:35.680 --> 02:04:41.920
So first of all, it's not going to be an event. The idea somehow, which is popularized by

02:04:41.920 --> 02:04:46.880
science fiction and Hollywood, that somehow somebody is going to discover the secret,

02:04:46.960 --> 02:04:51.600
the secret to AGI or human level AI or AMI, whatever you want to call it.

02:04:52.240 --> 02:04:57.040
And then, you know, turn on a machine and then we have a GI. That's just not going to happen.

02:04:57.040 --> 02:05:04.480
It's not going to be an event. It's going to be gradual progress. Are we going to have systems

02:05:04.480 --> 02:05:10.560
that can learn from video, how the world works and learn good world presentations? Yeah. Before

02:05:10.560 --> 02:05:14.640
we get them to the scale and performance that we observe in humans, it's going to take quite a

02:05:14.640 --> 02:05:22.880
while. It's not going to happen in one day. Are we going to get systems that can have large

02:05:22.880 --> 02:05:27.760
amount of associative memory so that they can remember stuff? Yeah, but same. It's not going

02:05:27.760 --> 02:05:31.680
to happen tomorrow. I mean, there is some basic techniques that need to be developed. We have

02:05:31.680 --> 02:05:36.480
a lot of them, but like, you know, to get this to work together with a full system is another

02:05:36.480 --> 02:05:40.400
story. Are we going to have systems that can reason and plan perhaps along the lines of

02:05:41.360 --> 02:05:46.640
objective-driven AI architectures that I described before? Yeah, but like before we get this to work,

02:05:46.640 --> 02:05:50.720
you know, properly, it's going to take a while. So, and before we get all those things to work

02:05:50.720 --> 02:05:55.040
together and then on top of this have systems that can learn like hierarchical planning,

02:05:55.040 --> 02:05:59.840
hierarchical representations, systems that can be configured for a lot of different

02:05:59.840 --> 02:06:06.320
situation at hand, the way the human brain can. You know, all of this is going to take,

02:06:06.320 --> 02:06:11.040
you know, at least a decade and probably much more because there are a lot of problems that

02:06:11.040 --> 02:06:16.080
we're not seeing right now that we have not encountered. And so we don't know if there is

02:06:16.080 --> 02:06:23.520
a easy solution within this framework. So, you know, it's not just around the corner. I mean,

02:06:23.520 --> 02:06:29.360
I've been hearing people for the last 12, 15 years claiming that, you know, AGI is just around

02:06:29.360 --> 02:06:34.240
the corner and being systematically wrong. And I knew they were wrong when they were saying it.

02:06:34.240 --> 02:06:38.960
I called their bullshit. Why do you think people have been calling, first of all, I mean, from the

02:06:38.960 --> 02:06:45.040
beginning, from the birth of the term artificial intelligence, there has been a eternal optimism

02:06:46.160 --> 02:06:54.000
that's perhaps unlike other technologies. Is it a Morovox paradox? Is the explanation for why

02:06:54.000 --> 02:06:59.440
people are so optimistic about AGI? I don't think it's just Morovox paradox. Morovox paradox is a

02:06:59.520 --> 02:07:04.320
consequence of realizing that the world is not as easy as we think. So first of all,

02:07:06.480 --> 02:07:11.280
intelligence is not a linear thing that you can measure with a scalar, with a single number.

02:07:12.720 --> 02:07:20.480
You know, can you say that humans are smarter than orangutans? In some ways, yes. But in some

02:07:20.480 --> 02:07:25.920
ways, orangutans are smarter than humans in a lot of domains that allows them to survive in the forest,

02:07:25.920 --> 02:07:30.480
for example. So IQ is a very limited measure of intelligence. Do you

02:07:30.480 --> 02:07:35.360
intelligence is bigger than what IQ, for example, measures? Well, IQ can measure, you know,

02:07:36.480 --> 02:07:43.840
approximately something for humans. But because humans kind of, you know, come in

02:07:44.640 --> 02:07:54.320
relatively kind of uniform form, right? But it only measures one type of ability that, you know,

02:07:54.320 --> 02:08:01.440
maybe relevant for some tasks, but not others. And, but then if you're talking about other

02:08:01.440 --> 02:08:07.360
intelligent entities for which the, you know, the basic things that are easy to them is very

02:08:07.360 --> 02:08:16.400
different, then it doesn't mean anything. So intelligence is a collection of skills

02:08:17.360 --> 02:08:26.080
and an ability to acquire new skills efficiently, right? And the collection of skills that any

02:08:26.080 --> 02:08:30.960
intelligent, particular intelligent entity possess or is capable of learning quickly,

02:08:30.960 --> 02:08:36.400
is different from the collection of skills of another one. And because it's a multi-dimensional

02:08:36.400 --> 02:08:41.040
thing, the set of skills is high dimensional space, you can't measure, you can compare,

02:08:41.040 --> 02:08:44.720
you cannot compare two things as to whether one is more intelligent than the other.

02:08:45.520 --> 02:08:54.560
It's multi-dimensional. So you push back against what are called AI doomers a lot.

02:08:55.760 --> 02:08:58.880
Can you explain their perspective and why you think they're wrong?

02:08:59.600 --> 02:09:05.920
Okay, so AI doomers imagine all kinds of catastrophy scenarios of how AI could escape

02:09:05.920 --> 02:09:13.680
or control and basically kill us all. And that relies on a whole bunch of

02:09:13.760 --> 02:09:19.280
assumptions that are mostly false. So the first assumption is that the emergence of

02:09:19.280 --> 02:09:23.520
superintelligence is going to be an event that at some point we're going to have,

02:09:23.520 --> 02:09:27.280
we're going to figure out the secret and we'll turn on a machine that is super intelligent.

02:09:28.160 --> 02:09:31.920
And because we've never done it before is going to take over the world and kill us all.

02:09:32.880 --> 02:09:37.040
That is false. It's not going to be an event. We're going to have systems that are like

02:09:38.000 --> 02:09:44.000
as smart as a cat, have all the characteristics of human level intelligence,

02:09:44.640 --> 02:09:49.440
but their level of intelligence would be like a cat or a parrot maybe or something.

02:09:51.200 --> 02:09:55.600
And then we're going to walk our way up to kind of make those things more intelligent. And as we

02:09:55.600 --> 02:09:59.280
make them more intelligent, we're also going to put some guardrails in them and learn how to kind

02:09:59.280 --> 02:10:03.600
of put some guardrails so they behave properly. And we're not going to do this with just one,

02:10:03.680 --> 02:10:06.880
it's not going to be one effort that is going to be lots of different people doing this.

02:10:07.440 --> 02:10:10.880
And some of them are going to succeed at making intelligent systems that are

02:10:12.000 --> 02:10:15.840
controllable and safe and have the right guardrails. And if some other goes rogue,

02:10:15.840 --> 02:10:22.400
then we can use the good ones to go against the rogue ones. So it's going to be my smart

02:10:22.400 --> 02:10:28.320
AI police against your rogue AI. So it's not going to be like, we're going to be exposed to a single

02:10:28.320 --> 02:10:33.120
rogue AI that's going to kill us all. That's just not happening. Now there is another fallacy,

02:10:33.120 --> 02:10:37.680
which is the fact that because the system is intelligent, it necessarily wants to take over.

02:10:40.480 --> 02:10:46.560
And there is several arguments that make people scared of this, which I think are completely

02:10:46.560 --> 02:10:55.440
false as well. So one of them is, in nature, it seems to be that the more intelligent species

02:10:55.440 --> 02:11:03.040
are the one that end up dominating the other. And even, you know, extinguishing the others,

02:11:03.680 --> 02:11:11.200
sometimes by design, sometimes just by mistake. And so, you know, there is sort of

02:11:12.560 --> 02:11:17.280
thinking by which you say, well, if AI systems are more intelligent than us,

02:11:17.280 --> 02:11:22.880
surely they're going to eliminate us if not by design, simply because they don't care about us.

02:11:23.600 --> 02:11:28.640
And that's just preposterous for a number of reasons. First reason is they're not going to be a

02:11:28.640 --> 02:11:33.760
species. They're not going to be a species that competes with us. They're not going to have the

02:11:33.760 --> 02:11:39.360
desire to dominate because the desire to dominate is something that has to be hardwired into an

02:11:39.360 --> 02:11:46.960
intelligent system. It is hardwired in humans. It is hardwired in baboons, in chimpanzees, in wolves,

02:11:47.520 --> 02:11:56.880
not in orangutans. The species in which this desire to dominate or submit or attain status in

02:11:56.880 --> 02:12:04.400
other ways is specific to social species. Non-social species like orangutans don't have it.

02:12:05.040 --> 02:12:11.280
And they are as smart as we are, almost. And to you, there's not significant incentive for humans

02:12:12.000 --> 02:12:19.040
to encode that into the AI systems. And to the degree they do, there'll be other AI's that

02:12:20.320 --> 02:12:22.800
sort of punish them for it. I'll compete them over.

02:12:22.800 --> 02:12:27.520
Well, there's all kinds of incentive to make AI systems submissive to humans, right? I mean,

02:12:27.520 --> 02:12:32.560
this is the way we're going to build them, right? And so then people say, oh, but look at LLMs,

02:12:32.560 --> 02:12:37.760
LLMs are not controllable. And they're right. LLMs are not controllable. But object-driven AI,

02:12:37.840 --> 02:12:43.600
so systems that derive their answers by optimization of an objective,

02:12:43.600 --> 02:12:47.360
means they have to optimize its objective. And that objective can include guardrails.

02:12:48.080 --> 02:12:56.320
One guardrail is obey humans. Another guardrail is don't obey humans if it's hurting other humans.

02:12:57.520 --> 02:12:59.360
I've heard that before somewhere. I don't remember.

02:12:59.360 --> 02:13:01.600
Yes, maybe in a book.

02:13:01.920 --> 02:13:09.040
Yeah. But speaking of that book, could there be unintended consequences also from all of this?

02:13:09.040 --> 02:13:14.560
No, of course. So this is not a simple problem, right? I mean, designing those guardrails so

02:13:14.560 --> 02:13:21.680
that the system behaves properly is not going to be a simple issue for which there is a silver

02:13:21.680 --> 02:13:26.400
bullet, for which you have a mathematical proof that the system can be safe. It's going to be

02:13:26.400 --> 02:13:31.840
very progressive iterative design system where we put those guardrails in such a way that the

02:13:31.840 --> 02:13:36.960
system behaves properly. And sometimes they're going to do something that was unexpected,

02:13:36.960 --> 02:13:40.160
because the guardrail wasn't right, and we're going to correct them so that they do it right.

02:13:40.960 --> 02:13:45.280
The idea somehow that we can't get it slightly wrong, because if we get it slightly wrong,

02:13:45.280 --> 02:13:54.640
we all die, is ridiculous. We're just going to go progressively. And the analogy I've used many

02:13:54.640 --> 02:14:05.280
times is turbojet design. How did we figure out how to make turbojets so unbelievably reliable,

02:14:05.920 --> 02:14:12.000
right? I mean, those are incredibly complex pieces of hardware that run at really high

02:14:12.000 --> 02:14:22.240
temperatures for 20 hours at a time sometimes. And we can fly halfway around the world on a two-engine

02:14:24.960 --> 02:14:30.320
jetliner at near the speed of sound. Like, how incredible is this? It's just unbelievable, right?

02:14:30.960 --> 02:14:36.960
And did we do this because we invented like a general principle of how to make turbojet safe?

02:14:36.960 --> 02:14:42.640
No, it took decades to kind of fine tune the design of those systems so that they were safe.

02:14:43.200 --> 02:14:50.240
Is there a separate group within General Electric or SNACMA or whatever that

02:14:51.200 --> 02:15:00.000
is specialized in turbojet safety? No, the design is all about safety, because a better turbojet

02:15:00.000 --> 02:15:07.120
is also a safer turbojet. So a more reliable one. It's the same for AI. Do you need specific

02:15:07.120 --> 02:15:11.680
provisions to make AI safe? No, you need to make better AI systems, and they will be safe because

02:15:11.680 --> 02:15:18.880
they are designed to be more useful and more controllable. So let's imagine a system, AI system,

02:15:18.880 --> 02:15:27.040
that's able to be incredibly convincing and can convince you of anything. I can at least imagine

02:15:27.040 --> 02:15:35.360
such a system, and I can see such a system be weapon-like because it can control people's minds.

02:15:35.360 --> 02:15:40.960
We're pretty gullible. We want to believe a thing. You can have an AI system that controls it,

02:15:40.960 --> 02:15:47.280
and you could see governments using that as a weapon. So do you think if you imagine such a system,

02:15:48.240 --> 02:15:58.560
there's any parallel to something like nuclear weapons? No. So why is that technology different?

02:15:58.560 --> 02:16:04.640
So you're saying there's going to be gradual development? Yeah. It might be rapid, but there'll

02:16:04.640 --> 02:16:11.280
be iterative, and then we'll be able to respond and so on. So that AI system designed by Vladimir

02:16:11.280 --> 02:16:22.800
Putin or whatever, or his minions is going to be trying to talk to every American to convince

02:16:22.800 --> 02:16:37.040
them to vote for whoever pleases Putin or whatever, or riled people up against each other as they've

02:16:37.040 --> 02:16:42.480
been trying to do. They're not going to be talking to you. They're going to be talking to your AI

02:16:42.480 --> 02:16:50.960
assistant, which is going to be as smart as theirs. That AI, because as I said, in the future,

02:16:50.960 --> 02:16:55.600
every single one of your interactions with the digital world will be mediated by your AI assistant.

02:16:55.600 --> 02:16:59.680
So the first thing you're going to ask is, is this a scam? Like is this thing like turning me

02:16:59.680 --> 02:17:04.880
to the truth? It's not even going to be able to get to you because it's only going to talk to

02:17:04.880 --> 02:17:11.280
your AI assistant. It's not even going to, it's going to be like a spam filter. You're not even

02:17:11.280 --> 02:17:17.600
seeing the spam email. It's automatically put in a folder that you never see. It's going to be the

02:17:17.600 --> 02:17:22.400
same thing. That AI system that tries to convince you of something is going to be talking to your

02:17:22.400 --> 02:17:30.480
assistant, which is going to be at least as smart as it is going to say, this is spam. It's not even

02:17:30.480 --> 02:17:35.440
going to bring it to your attention. So to you, it's very difficult for any one AI system to take

02:17:35.440 --> 02:17:42.320
such a big leap ahead to where you can convince even the other AI systems. So there's always going

02:17:42.320 --> 02:17:49.200
to be this kind of race where nobody's way ahead. That's the history of the world. History of the

02:17:49.200 --> 02:17:57.040
world is whenever there is a progress someplace, there is a countermeasure and it's a cat and mouse

02:17:57.440 --> 02:18:02.240
This is why, mostly yes, but this is why nuclear weapons are so interesting because

02:18:02.240 --> 02:18:10.000
that was such a powerful weapon that it mattered who got it first. That you could imagine

02:18:11.040 --> 02:18:20.480
Hitler, Stalin, Mao getting the weapon first and that having a different kind of impact on the world

02:18:21.360 --> 02:18:29.600
than the United States getting the weapon first. To you, nuclear weapons, you don't imagine a

02:18:30.720 --> 02:18:38.320
breakthrough discovery and then Manhattan project like for AI. No, as I said, it's not going to be

02:18:38.320 --> 02:18:46.400
an event. It's going to be continuous progress and whenever one breakthrough occurs, it's going

02:18:46.400 --> 02:18:51.680
to be widely disseminated really quickly. Probably first within industry. I mean, this is not a

02:18:51.680 --> 02:18:58.480
domain where government or military organizations are particularly innovative and they're in fact

02:18:58.480 --> 02:19:04.560
way behind. And so this is going to come from industry and this kind of information disseminates

02:19:04.560 --> 02:19:11.680
extremely quickly. We've seen this over the last few years where you have a new, even take Alpha

02:19:11.680 --> 02:19:17.200
Go, this was reproduced within three months, even without particularly detailed information.

02:19:18.000 --> 02:19:23.840
This is an industry that's not good at secrecy. Even if there is, just the fact that you know

02:19:23.840 --> 02:19:30.960
that something is possible makes you realize that it's worth investing the time to actually do it.

02:19:30.960 --> 02:19:41.120
You may be the second person to do it, but you'll do it. And say for all the innovations

02:19:42.080 --> 02:19:47.360
of self-supervisioning, transformers, decoder-only architectures, LLMs, I mean those things,

02:19:47.360 --> 02:19:51.200
you don't need to know exactly the details of how they worked. You know that it's possible

02:19:52.640 --> 02:19:57.920
because it's deployed and then it's getting reproduced. And then people who work for those

02:19:57.920 --> 02:20:04.960
companies move. They go from one company to another and the information disseminates.

02:20:04.960 --> 02:20:11.440
What makes the success of the US tech industry and Silicon Valley in particular is exactly

02:20:11.440 --> 02:20:16.320
that, is because the information circulates really, really quickly and this disseminates

02:20:16.320 --> 02:20:23.600
very quickly. And so the whole region is ahead because of that circulation of information.

02:20:24.880 --> 02:20:31.760
Maybe just to linger on the psychology of AI doomers, you give in the classic Yanlacoon way

02:20:31.760 --> 02:20:39.440
a pretty good example of just when a new technology comes to be. You say engineer says,

02:20:39.440 --> 02:20:46.320
I invented this new thing. I call it a ball pen. And then the Twitter sphere responds,

02:20:46.320 --> 02:20:50.960
OMG, people could write horrible things with it like misinformation, propaganda, hate speech,

02:20:50.960 --> 02:20:59.680
ban it now. Then writing doomers come in akin to the AI doomers. Imagine if everyone can get a

02:20:59.680 --> 02:21:04.640
ball pen. This could destroy society. There should be a law against using ball pen to write hate

02:21:04.720 --> 02:21:11.680
speech, regulate ball pens now. And then the pencil industry mogul says, yeah, ball pens are very

02:21:11.680 --> 02:21:19.040
dangerous. Unlike pencil writing, which is erasable ball pen writing stays forever. Government should

02:21:19.040 --> 02:21:27.360
require a license for a pen manufacturer. I mean, this does seem to be part of human psychology

02:21:28.080 --> 02:21:36.640
when it comes up against new technology. What deep insights can you speak to about this?

02:21:37.600 --> 02:21:45.840
Well, there is a natural fear of new technology and the impact it can have on society. And people

02:21:45.840 --> 02:21:55.440
have kind of instinctive reaction to the world they know being threatened by major transformations

02:21:56.240 --> 02:22:02.880
that are either cultural phenomena or technological revolutions. And they fear for

02:22:03.920 --> 02:22:09.680
their culture, they fear for their job, they fear for their future of their children

02:22:11.280 --> 02:22:20.960
and the way of life. So any change is feared. And you see this a long history, like any

02:22:21.040 --> 02:22:25.600
technological revolution or cultural phenomenon was always accompanied by

02:22:28.000 --> 02:22:36.240
groups or reaction in the media that basically attributed all the problems,

02:22:36.240 --> 02:22:42.560
the current problems of society to that particular change. Electricity was going to kill everyone

02:22:43.440 --> 02:22:49.120
at some point. The train was going to be a horrible thing because you can't breathe

02:22:49.120 --> 02:22:55.280
past 50 kilometers an hour. And so there's a wonderful website called a pessimist archive

02:22:57.280 --> 02:23:03.040
which has all those newspaper clips of all the horrible things people imagine would arrive because

02:23:03.040 --> 02:23:14.800
of either technological innovation or a cultural phenomenon. It's just wonderful examples of

02:23:15.040 --> 02:23:25.600
jazz or comic books being blamed for an employment or young people not wanting to

02:23:25.600 --> 02:23:30.400
work anymore and things like that. And that has existed for centuries.

02:23:32.960 --> 02:23:43.360
And it's knee-jerk reactions. The question is, do we embrace change or do we resist it?

02:23:44.000 --> 02:23:50.160
And what are the real dangers as opposed to the imagined ones?

02:23:51.680 --> 02:23:56.480
So people worry about, I think one thing they worry about with big tech, something we've been

02:23:56.480 --> 02:24:04.000
talking about over and over, but I think worth mentioning again, they worry about how powerful

02:24:04.000 --> 02:24:10.320
AI will be and they worry about it being in the hands of one centralized power or just

02:24:10.320 --> 02:24:17.200
a handful of central control. And so that's the skepticism with big tech. These companies can

02:24:17.200 --> 02:24:25.520
make a huge amount of money and control this technology and by so doing, take advantage,

02:24:26.560 --> 02:24:31.760
abuse the little guy in society. Well, that's exactly why we need open source platforms.

02:24:31.760 --> 02:24:39.440
Yeah, I just wanted to nail the point home more and more. So let me ask you on your,

02:24:40.480 --> 02:24:45.760
like I said, you do get a little bit flavorful on the internet.

02:24:46.640 --> 02:24:52.480
Yoshibak tweeted something that you LOL'd at in reference to Hal 9000.

02:24:53.840 --> 02:24:59.600
I appreciate your argument and I fully understand your frustration, but whether the pod bay doors

02:24:59.600 --> 02:25:06.480
should be opened or closed is a complex and nuanced issue. So you're at the head of meta AI.

02:25:09.200 --> 02:25:16.720
This is something that really worries me that AI or AI overlords will speak down to us with

02:25:16.720 --> 02:25:24.880
corporate speak of this nature and you sort of resist that with your way of being. Is this

02:25:24.880 --> 02:25:32.320
something you can just comment on sort of working at a big company? How you can avoid the

02:25:34.480 --> 02:25:40.800
overfearing, I suppose, through caution, create harm?

02:25:41.280 --> 02:25:47.200
Yeah. Again, I think the answer to this is open source platforms and then enabling

02:25:48.080 --> 02:25:55.760
widely diverse set of people to build AI assistance that represent the diversity of

02:25:56.320 --> 02:26:01.520
cultures, opinions, languages and value systems across the world. So that you're not bound to just

02:26:03.120 --> 02:26:09.760
be brainwashed by a particular way of thinking because of a single AI entity.

02:26:10.640 --> 02:26:16.480
So I mean, I think it's really, really important question for society. And the problem I'm seeing is

02:26:17.600 --> 02:26:25.040
that, which is why I've been so vocal and sometimes a little sardonic about it.

02:26:25.040 --> 02:26:29.280
Never stop. Never stop, yeah. We love it.

02:26:29.280 --> 02:26:37.360
It's because I see the danger of this concentration of power through proprietary AI systems as a much

02:26:37.360 --> 02:26:46.400
bigger danger than everything else. That if we really want diversity of opinion AI systems that

02:26:47.760 --> 02:26:54.160
in the future where we'll all be interacting through AI systems, we need those to be diverse

02:26:54.160 --> 02:27:03.520
for the preservation of diversity of ideas and creeds and political opinions and whatever

02:27:05.920 --> 02:27:13.520
and the preservation of democracy. And what works against this is people who think that

02:27:13.520 --> 02:27:19.680
for reasons of security, we should keep AI systems under lock and key because it's too

02:27:19.680 --> 02:27:26.000
dangerous to put it in the hands of everybody because it could be used by terrorists or something.

02:27:28.800 --> 02:27:38.160
That would lead to potentially a very bad future in which all of our information

02:27:38.160 --> 02:27:42.880
diet is controlled by a small number of companies through proprietary systems.

02:27:44.240 --> 02:27:52.560
Do you trust humans with this technology to build systems that are on the whole good for

02:27:52.560 --> 02:27:56.400
humanity? Isn't that what democracy and free speech is all about?

02:27:56.400 --> 02:28:02.480
I think so. Do you trust institutions to do the right thing? Do you trust people to do the right

02:28:02.480 --> 02:28:06.720
thing? And yeah, there's bad people who are going to do bad things, but they're not going to have

02:28:06.720 --> 02:28:11.920
superior technology to the good people. So then it's going to be my good AI against your bad AI.

02:28:13.600 --> 02:28:22.160
Examples that we were just talking about of maybe some rogue country will build some AI system

02:28:22.160 --> 02:28:30.160
that's going to try to convince everybody to go into a civil war or something or elect a favorable

02:28:31.040 --> 02:28:35.520
ruler, but then they will have to go past our AI systems.

02:28:36.320 --> 02:28:40.160
An AI system with a strong Russian accent will be trying to convince us.

02:28:40.160 --> 02:28:42.960
And doesn't put any articles in their sentences.

02:28:45.600 --> 02:28:54.480
Well, it'll be at the very least absurdly comedic. Since we talked about the physical

02:28:54.480 --> 02:29:00.480
reality, I'd love to ask your vision of the future with robots in this physical reality.

02:29:00.480 --> 02:29:07.200
So many of the kinds of intelligence you've been speaking about would empower robots to be more

02:29:07.200 --> 02:29:15.200
effective collaborators with us humans. So since Tesla's Optimus team has been showing us some

02:29:15.200 --> 02:29:21.200
progress on human robots, I think it really reinvigorated the whole industry that I think

02:29:21.280 --> 02:29:25.120
Boston Dynamics has been leading for a very, very long time. So now there's all kinds of

02:29:25.120 --> 02:29:33.600
companies figure AI, obviously Boston Dynamics, Unitree, but there's like a lot of them. It's

02:29:33.600 --> 02:29:42.560
great. It's great. I mean, I love it. So do you think there'll be millions of human robots walking

02:29:42.560 --> 02:29:47.680
around soon? Not soon, but it's going to happen. Like the next decade, I think it's going to be

02:29:48.160 --> 02:29:54.080
interesting in robots. The emergence of the robotics industry has been

02:29:55.040 --> 02:29:59.360
in the waiting for 10, 20 years without really emerging other than for

02:30:00.400 --> 02:30:07.680
kind of pre-programmed behavior and stuff like that. And the main issue is, again,

02:30:07.680 --> 02:30:12.320
the Moravec paradox. How do we get the systems to understand how the world works and kind of

02:30:12.400 --> 02:30:15.440
plan actions? And so we can do it for really specialized tasks.

02:30:17.680 --> 02:30:22.640
And the way Boston Dynamics goes about it is basically with a lot of

02:30:23.600 --> 02:30:30.880
handcrafted dynamical models and careful planning in advance, which is very classical robotics with

02:30:30.880 --> 02:30:37.040
a lot of innovation, a little bit of perception. But it's still not like they can't build a domestic

02:30:37.040 --> 02:30:45.920
robot. And we're still some distance away from completely autonomous level 5 driving.

02:30:48.000 --> 02:30:54.320
And we're certainly very far away from having level 5 autonomous driving by a system that can

02:30:54.320 --> 02:31:06.080
train itself by driving 20 hours like any 17-year-old. So until we have, again,

02:31:06.800 --> 02:31:10.720
world models, systems that can train themselves to understand how the world works,

02:31:12.800 --> 02:31:19.440
we're not going to have significant progress in robotics. So a lot of the people working on

02:31:19.440 --> 02:31:25.840
robotic hardware at the moment are betting or banking on the fact that AI is going to

02:31:25.840 --> 02:31:30.640
make sufficient progress towards that. And they're hoping to discover a product in it too.

02:31:32.400 --> 02:31:37.360
Before you have a really strong world model, there'll be an almost strong world model.

02:31:39.920 --> 02:31:42.720
People are trying to find a product in a clumsy robot, I suppose,

02:31:43.600 --> 02:31:47.440
like not a perfectly efficient robot. So there's the factory setting where

02:31:47.520 --> 02:31:52.160
human robots can help automate some of the aspects of the factory. I think that's a

02:31:52.160 --> 02:31:55.920
crazy difficult task because of all the safety required and all this kind of stuff.

02:31:55.920 --> 02:31:59.120
I think in the home is more interesting. But then you start to think,

02:32:00.320 --> 02:32:06.560
I think you mentioned loading the dishwasher, right? I suppose that's one of the main problems

02:32:06.560 --> 02:32:16.800
you're working on. I mean, cleaning up the house, clearing up the table after a meal.

02:32:17.920 --> 02:32:23.840
Washing the dishes, all those tasks, cooking. All the tasks that in principle could be automated,

02:32:23.840 --> 02:32:27.360
but are actually incredibly sophisticated, really complicated.

02:32:28.080 --> 02:32:31.920
But even just basic navigation around an unspaceful of uncertainty.

02:32:31.920 --> 02:32:36.400
That sort of works. You can sort of do this now. Navigation is fine.

02:32:37.120 --> 02:32:42.240
Well, navigation in a way that's compelling to us humans is a different thing.

02:32:42.800 --> 02:32:47.440
Yeah, it's not going to be necessarily. I mean, we have demos, actually, because there is a

02:32:48.560 --> 02:32:55.040
so-called embodied AI group at fair. And they've been not building their own robots,

02:32:55.040 --> 02:33:02.160
but using commercial robots. And you can tell the robot dog go to the fridge,

02:33:02.160 --> 02:33:06.160
and they can actually open the fridge, and they can probably pick up a can in the fridge and stuff

02:33:06.160 --> 02:33:13.600
like that and bring it to you. So you can navigate, you can grab objects, as long as it's been

02:33:13.600 --> 02:33:16.880
trying to recognize them, which vision systems work pretty well nowadays.

02:33:18.320 --> 02:33:26.320
But it's not like a completely general robot that would be sophisticated enough to do things like

02:33:27.680 --> 02:33:34.160
clearing up the dinner table. Yeah, to me, that's an exciting future of getting human

02:33:34.240 --> 02:33:39.600
robots, robots in general in the home more and more, because that gets humans to really directly

02:33:39.600 --> 02:33:45.120
interact with AI systems in the physical space. And so doing it allows us to philosophically,

02:33:45.120 --> 02:33:50.560
psychologically explore our relationships with robots can be really, really, really interesting.

02:33:50.560 --> 02:33:54.160
So I hope you make progress on the whole JAPA thing soon.

02:33:54.160 --> 02:33:58.240
Well, I mean, I hope things kind of work as planned.

02:33:59.200 --> 02:34:04.160
And I mean, again, we've been kind of working on this idea of self supervised running

02:34:05.040 --> 02:34:12.000
from video for 10 years, and you know, only made significant progress in the last two or three.

02:34:12.000 --> 02:34:15.600
And actually, you've mentioned that there's a lot of interesting breakers that can happen

02:34:15.600 --> 02:34:20.800
without having access to a lot of compute. So if you're interested in doing a PhD and this kind

02:34:20.800 --> 02:34:26.400
of stuff, there's a lot of possibilities still to do innovative work. So like what advice would

02:34:26.400 --> 02:34:31.360
you give to a undergrad that's looking to go to grad school and do a PhD?

02:34:32.240 --> 02:34:38.320
So basically, I've listed them already, this idea of how do you train a world model by observation.

02:34:39.680 --> 02:34:43.760
And you don't have to train necessarily on gigantic datasets or

02:34:45.440 --> 02:34:49.040
I mean, you could turn that to be necessary to actually train on large datasets to have

02:34:49.040 --> 02:34:53.040
emergent properties like we have with LLMs. But I think there's a lot of good ideas that

02:34:53.040 --> 02:34:59.760
can be done without necessarily scaling up. Then there is how do you do planning with a learn world

02:34:59.760 --> 02:35:04.560
model. If the world the system evolves in is not the physical world, but it's the world of

02:35:05.680 --> 02:35:12.800
this at the internet or, you know, some sort of world of where an action consists in doing a search

02:35:12.800 --> 02:35:19.600
in a search engine or interrogating a database or running a simulation or calling a calculator

02:35:19.600 --> 02:35:25.200
or solving a differential equation. How do you get a system to actually plan a sequence of actions

02:35:25.200 --> 02:35:34.160
to give the solution to a problem? And so the question of planning is not just a question of

02:35:34.160 --> 02:35:40.160
planning physical actions, it could be planning actions to use tools for a dialogue system or

02:35:40.160 --> 02:35:47.200
for any kind of intelligent system. And there's some work on this, but not a huge amount. Some

02:35:47.200 --> 02:35:54.240
work at fair, one called tool former, which was a couple years ago and some more recent work on

02:35:54.240 --> 02:36:01.440
planning. But I don't think we have like a good solution for any of that. Then there is the

02:36:02.240 --> 02:36:08.160
question of hierarchical planning. So the example I mentioned of, you know, planning a trip from

02:36:08.880 --> 02:36:13.040
New York to Paris, that's hierarchical, but almost every action that we take

02:36:14.000 --> 02:36:20.560
involves hierarchical planning in some sense. And we really have absolutely no idea how to do this,

02:36:20.560 --> 02:36:32.640
like this zero demonstration of hierarchical planning in AI, where the various levels of

02:36:32.640 --> 02:36:38.800
representations that are necessary have been learned. We can do like two level hierarchical

02:36:38.800 --> 02:36:44.720
planning when we design the two levels. So for example, you have like a dog-like robot, right?

02:36:44.720 --> 02:36:49.840
You want it to go from the living room to the kitchen, you can plan a path that avoids the

02:36:49.840 --> 02:36:56.560
obstacle. And then you can send this to a lower level planner that figures out how to move the

02:36:56.560 --> 02:37:02.160
legs to kind of follow that trajectory. So that works. But that two level planning is designed

02:37:02.160 --> 02:37:10.960
by hand, right? We specify what the proper levels of abstraction, the representation at each level

02:37:10.960 --> 02:37:16.480
of abstraction has to be. How do you learn this? How do you learn that hierarchical representation

02:37:16.480 --> 02:37:23.680
of action plans, right? With cognize and deep learning, we can train the system to learn

02:37:23.680 --> 02:37:29.200
hierarchical representations of percepts. What is the equivalent when what you're trying to

02:37:29.280 --> 02:37:34.480
represent are action plans? For action plans. Yeah. So you want basically a robot dog or

02:37:34.480 --> 02:37:39.920
humanoid robot that turns on and travels from New York to Paris all by itself.

02:37:40.960 --> 02:37:47.840
For example. All right. It might have some trouble at the TSA, but yeah. No, but even

02:37:47.840 --> 02:37:54.400
doing something fairly simple like a household task, like cooking or something. Yeah, there's

02:37:54.400 --> 02:37:59.040
a lot involved. It's a super complex task. And once again, we take it for granted.

02:37:59.440 --> 02:38:07.680
What hope do you have for the future of humanity? We're talking about so many exciting technologies,

02:38:07.680 --> 02:38:13.120
so many exciting possibilities. What gives you hope when you look out over the next 10,

02:38:13.120 --> 02:38:19.280
20, 50, 100 years? If you look at social media, there's wars going on. There's division.

02:38:20.560 --> 02:38:25.840
There's hatred, all this kind of stuff. That's also part of humanity. But amidst all that,

02:38:25.840 --> 02:38:29.760
what gives you hope? I love that question.

02:38:34.400 --> 02:38:43.840
We can make humanity smarter with AI. AI basically will amplify human intelligence.

02:38:45.120 --> 02:38:53.200
It's as if every one of us will have a staff of smart AI assistants. They might be smarter than

02:38:53.200 --> 02:39:04.160
us. They'll do our bidding. Perhaps execute a task in ways that are much better than we could do

02:39:04.160 --> 02:39:10.960
ourselves because they'll be smarter than us. And so it's like everyone would be the boss of a

02:39:11.760 --> 02:39:18.960
staff of super smart virtual people. So we shouldn't feel threatened by this any more than we

02:39:18.960 --> 02:39:24.320
should feel threatened by being the manager of a group of people, some of whom are more intelligent

02:39:24.320 --> 02:39:33.680
than us. I certainly have a lot of experience with this of having people working with me who are

02:39:33.680 --> 02:39:40.720
smarter than me. That's actually a wonderful thing. So having machines that are smarter than us that

02:39:40.720 --> 02:39:46.000
assist us in all of our tasks or daily lives, whether it's professional or personal, I think

02:39:46.000 --> 02:39:53.200
would be an absolutely wonderful thing. Because intelligence is the commodity that is most in

02:39:53.200 --> 02:39:59.120
demand. All the mistakes that humanity makes is because of lack of intelligence, really,

02:39:59.120 --> 02:40:07.200
or lack of knowledge which is related. So making people smarter can only be better.

02:40:07.840 --> 02:40:10.400
For the same reason that public education is a good thing.

02:40:11.360 --> 02:40:18.240
And books are a good thing. And the internet is also a good thing intrinsically. And even social

02:40:18.240 --> 02:40:23.200
networks are a good thing if you run them properly. It's difficult, but you can.

02:40:26.000 --> 02:40:34.080
Because it helps the communication of information and knowledge and the transmission of knowledge.

02:40:34.080 --> 02:40:39.360
So AI is going to make humanity smarter. And the analogy I've been using

02:40:40.640 --> 02:40:50.320
is the fact that perhaps an equivalent event in the history of humanity to what might be provided by

02:40:51.440 --> 02:40:57.120
generalization of AI assistant is the invention of the printing press. It made everybody smarter.

02:40:57.120 --> 02:41:05.200
The fact that people could have access to books. Books were a lot cheaper than they

02:41:05.200 --> 02:41:11.520
were before. And so a lot more people had an incentive to learn to read, which wasn't the case

02:41:11.520 --> 02:41:21.840
before. And people became smarter. It enabled the enlightenment. There wouldn't be an

02:41:21.920 --> 02:41:32.080
enlightenment without the printing press. It enabled philosophy, rationalism, escape from

02:41:32.080 --> 02:41:45.200
religious doctrine, democracy, science. And certainly without this, it wouldn't have been the

02:41:45.200 --> 02:41:49.600
American revolution or the French revolution. And so it would still be under a feudal

02:41:50.560 --> 02:42:00.000
regime, perhaps. And so it completely transformed the world because people became smarter and

02:42:00.000 --> 02:42:06.880
kind of learned about things. Now, it also created 200 years of essentially religious

02:42:06.880 --> 02:42:14.880
conflicts in Europe. Because the first thing that people read was the Bible. And realized that

02:42:14.880 --> 02:42:18.400
perhaps there was a different interpretation of the Bible than what the priests were telling them.

02:42:19.760 --> 02:42:24.880
And so that created the Protestant movement and created the rift. And in fact, the Catholic

02:42:24.880 --> 02:42:30.640
Church didn't like the idea of the printing press, but they had no choice. And so it had

02:42:30.640 --> 02:42:34.880
some bad effects and some good effects. I don't think anyone today would say that the invention

02:42:34.880 --> 02:42:39.680
of the printing press had an overall negative effect, despite the fact that it created

02:42:40.320 --> 02:42:48.720
200 years of religious conflicts in Europe. Now, compare this. And I thought I was very

02:42:48.720 --> 02:42:54.000
proud of myself to come up with this analogy, but realize someone else came with the same idea

02:42:54.000 --> 02:43:00.880
before me. Compare this with what happened in the Ottoman Empire. The Ottoman Empire banned

02:43:00.880 --> 02:43:10.960
the printing press for 200 years. And it didn't ban it for all languages, only for Arabic.

02:43:11.680 --> 02:43:18.080
You could actually print books in Latin or Hebrew or whatever in the Ottoman Empire,

02:43:18.080 --> 02:43:27.520
just not in Arabic. And I thought it was because the rulers just wanted to preserve

02:43:27.520 --> 02:43:31.920
the control over the population and the dogma, religious dogma and everything.

02:43:32.880 --> 02:43:43.840
But after talking with the UAE minister of AI, Omar, he told me, no, there was another reason.

02:43:44.800 --> 02:43:51.360
And the other reason was that it was to preserve the corporation of

02:43:51.360 --> 02:43:57.920
geographers. There's like an art form, which is writing those beautiful

02:44:00.240 --> 02:44:06.240
Arabic poems or whatever religious text in this thing. And it was a very powerful

02:44:06.240 --> 02:44:13.120
corporation of scribes, basically that kind of run a big chunk of the empire. And it couldn't

02:44:13.120 --> 02:44:18.240
put them out of business. So they banned the printing press in part to protect that business.

02:44:21.200 --> 02:44:25.920
Now, what's the analogy for AI today? Who are we protecting by banning AI? Who are the people

02:44:25.920 --> 02:44:34.320
who are asking that AI be regulated to protect their jobs? And of course, it's a real question

02:44:35.120 --> 02:44:41.760
of what is going to be the effect of technological transformation like AI on the

02:44:42.640 --> 02:44:49.360
job market and the labor market. And the economies too are much more expert at this than I am. But

02:44:49.360 --> 02:44:56.240
when I talk to them, they tell us, we're not going to run out of job. This is not going to

02:44:56.240 --> 02:45:02.400
cause mass unemployment. This is just going to be gradual shift of different professions. The

02:45:02.400 --> 02:45:08.240
professions are going to be hot 10 or 15 years from now. We have no idea today what they're going to

02:45:08.240 --> 02:45:14.880
be. The same way if we go back 20 years in the past, like who could have thought 20 years ago

02:45:14.880 --> 02:45:21.760
that like the hottest job even like five, 10 years ago was mobile app developer, like smartphones

02:45:21.760 --> 02:45:28.160
weren't invented. Most of the jobs of the future might be in the metaverse. Well, it could be. Yeah.

02:45:28.960 --> 02:45:33.920
But the point is you can't possibly predict. But you're right. I mean, you made a lot of

02:45:33.920 --> 02:45:41.680
strong points and I believe that people are fundamentally good. And so if AI, especially

02:45:41.680 --> 02:45:48.240
open source AI, can make them smarter, it just empowers the goodness in humans.

02:45:48.240 --> 02:45:56.560
So I share that feeling. Okay. I think people are fine. And in fact, a lot of doomers are doomers

02:45:56.560 --> 02:46:03.120
because they don't think that people are fundamentally good. And they either don't trust

02:46:03.120 --> 02:46:09.120
people or they don't trust the institution to do the right thing so that people behave properly.

02:46:10.480 --> 02:46:15.520
Well, I think both you and I believe in humanity. And I think I speak for a lot of people

02:46:16.320 --> 02:46:20.800
in saying thank you for pushing the open source movement, pushing to making

02:46:21.760 --> 02:46:27.600
both research and AI open source, making it available to people and also the models themselves,

02:46:27.600 --> 02:46:32.480
making it open source. So thank you for that. And thank you for speaking your mind in such

02:46:32.480 --> 02:46:36.640
colorful and beautiful ways on the internet. I hope you never stop. You're one of the most

02:46:36.640 --> 02:46:42.240
fun people I know and get to be a fan. Oh, so yeah, thank you for speaking to me once again.

02:46:42.240 --> 02:46:47.200
And thank you for being you. Thank you. Thanks. Thanks for listening to this conversation with

02:46:47.200 --> 02:46:51.520
Jan Lacoon. To support this podcast, please check out our sponsors in the description.

02:46:52.080 --> 02:46:55.280
And now let me leave you with some words from Arthur C. Clarke.

02:46:56.880 --> 02:47:00.720
The only way to discover the limits of the possible is to go beyond them.

02:47:01.680 --> 02:47:17.040
And to the impossible. Thank you for listening and hope to see you next time.

