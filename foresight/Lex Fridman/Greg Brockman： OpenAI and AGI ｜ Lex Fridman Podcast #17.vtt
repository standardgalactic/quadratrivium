WEBVTT

00:00.000 --> 00:02.880
The following is a conversation with Greg Brockman.

00:02.880 --> 00:05.360
He's the co-founder and CTO of OpenAI,

00:05.360 --> 00:09.040
a world-class research organization developing ideas in AI

00:09.040 --> 00:12.880
with the goal of eventually creating a safe and friendly

00:12.880 --> 00:15.360
artificial general intelligence.

00:15.360 --> 00:18.800
One that benefits and empowers humanity.

00:18.800 --> 00:24.400
OpenAI is not only a source of publications, algorithms, tools, and data sets.

00:24.400 --> 00:28.080
Their mission is a catalyst for an important public discourse

00:28.080 --> 00:33.200
about our future with both narrow and general intelligence systems.

00:33.920 --> 00:38.800
This conversation is part of the Artificial Intelligence Podcast at MIT and BEYOND.

00:39.440 --> 00:44.400
If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on Twitter

00:44.400 --> 00:48.000
at Lex Friedman, spelled F-R-I-D.

00:48.000 --> 00:51.600
And now here's my conversation with Greg Brockman.

00:52.640 --> 00:56.080
So in high school and right after you wrote a draft of a chemistry textbook,

00:56.640 --> 01:00.720
I saw that that covers everything from basic structure of the atom to quantum mechanics.

01:01.440 --> 01:08.240
So it's clear you have an intuition and a passion for both the physical world with chemistry

01:08.240 --> 01:14.400
and non-robotics to the digital world with AI, deep learning, reinforcement learning,

01:14.400 --> 01:18.480
so on. Do you see the physical world and the digital world as different?

01:18.480 --> 01:19.760
And what do you think is the gap?

01:20.400 --> 01:23.200
A lot of it actually boils down to iteration speed, right?

01:23.200 --> 01:26.480
That I think that a lot of what really motivates me is building things, right?

01:27.680 --> 01:30.800
Think about mathematics, for example, where you think really hard about a problem,

01:30.800 --> 01:33.920
you understand it. You're right down in this very obscure form that we call proof.

01:34.480 --> 01:38.320
But then this is in humanity's library, right? It's there forever.

01:38.320 --> 01:39.760
This is some truth that we've discovered.

01:40.480 --> 01:42.880
And maybe only five people in your field will ever read it,

01:42.880 --> 01:44.720
but somehow you've kind of moved humanity forward.

01:45.520 --> 01:48.640
And so I actually used to really think that I was going to be a mathematician.

01:48.640 --> 01:51.520
And then I actually started writing this chemistry textbook.

01:51.520 --> 01:54.880
One of my friends told me, you'll never publish it because you don't have a PhD.

01:54.880 --> 01:59.840
So instead, I decided to build a website and try to promote my ideas that way.

01:59.840 --> 02:05.200
And then I discovered programming. And in programming, you think hard about a problem,

02:05.200 --> 02:09.040
you understand it, you're right down in a very obscure form that we call a program.

02:09.840 --> 02:12.160
But then once again, it's in humanity's library, right?

02:12.160 --> 02:15.600
And anyone can get the benefit from it and the scalability is massive.

02:15.600 --> 02:19.040
And so I think that the thing that really appeals to me about the digital world

02:19.040 --> 02:21.920
is that you can have this insane leverage, right?

02:21.920 --> 02:26.240
A single individual with an idea is able to affect the entire planet.

02:26.240 --> 02:30.240
And that's something I think is really hard to do if you're moving around physical atoms.

02:30.240 --> 02:36.320
But you said mathematics. So if you look at the wet thing over here, our mind,

02:37.120 --> 02:41.680
you ultimately see it as just math, as just information processing,

02:41.680 --> 02:47.040
or is there some other magic if you've seen through biology and chemistry and so on?

02:47.040 --> 02:50.960
I think it's really interesting to think about humans as just information processing systems,

02:50.960 --> 02:54.960
and that it seems like it's actually a pretty good way of describing a lot of

02:56.240 --> 03:00.560
how the world works, a lot of what we're capable of, to think that, again,

03:00.560 --> 03:04.480
if you just look at technological innovations over time, that in some ways,

03:04.480 --> 03:07.680
the most transformative innovation that we've had has been the computer, right?

03:07.680 --> 03:10.400
In some ways, the internet, what has the internet done?

03:10.400 --> 03:12.640
The internet is not about these physical cables.

03:12.640 --> 03:16.480
It's about the fact that I am suddenly able to instantly communicate with any other human

03:16.480 --> 03:20.720
on the planet. I'm able to retrieve any piece of knowledge that, in some ways,

03:20.720 --> 03:26.000
the human race has ever had, and that those are these insane transformations.

03:26.000 --> 03:31.360
Do you see our society as a whole the collective as another extension of the intelligence of the

03:31.360 --> 03:34.960
human being? So if you look at the human being as an information processing system,

03:34.960 --> 03:39.280
you mentioned the internet, the networking. Do you see us all together as a civilization

03:39.280 --> 03:41.600
as a kind of intelligent system?

03:41.600 --> 03:45.760
Yeah, I think this is actually a really interesting perspective to take and to think about

03:45.760 --> 03:49.360
that you sort of have this collective intelligence of all of society.

03:49.360 --> 03:54.320
The economy itself is this superhuman machine that is optimizing something, right?

03:54.320 --> 03:57.840
And in some ways, a company has a will of its own, right?

03:57.840 --> 04:01.040
That you have all these individuals who are all pursuing their own individual goals and thinking

04:01.040 --> 04:05.280
really hard and thinking about the right things to do, but somehow the company does something

04:05.280 --> 04:10.480
that is this emergent thing and that it's a really useful abstraction.

04:10.480 --> 04:15.040
And so I think that in some ways, we think of ourselves as the most intelligent things on the

04:15.040 --> 04:18.800
planet and the most powerful things on the planet, but there are things that are bigger

04:18.800 --> 04:22.400
than us that are these systems that we all contribute to. And so I think actually,

04:22.400 --> 04:27.040
you know, it's interesting to think about, if you've read Asa Geismov's Foundation,

04:27.040 --> 04:30.880
right, that there's this concept of psycho history in there, which is effectively this,

04:30.880 --> 04:35.040
that if you have trillions or quadrillions of beings, then maybe you could actually

04:35.040 --> 04:40.880
predict what that being, that huge macro being will do and almost independent of what the

04:40.880 --> 04:44.880
individuals want. And I actually have a second angle on this that I think is interesting,

04:44.880 --> 04:50.560
which is thinking about technological determinism. One thing that I actually think a lot about with

04:50.560 --> 04:56.320
OpenAI, right, is that we're kind of coming on onto this insanely transformational technology of

04:56.320 --> 04:59.920
general intelligence, right, that will happen at some point. And there's a question of,

04:59.920 --> 05:04.000
how can you take actions that will actually steer it to go better rather than worse?

05:04.720 --> 05:09.280
And that I think one question you need to ask is, as a scientist, as an event as a creator,

05:09.280 --> 05:13.280
what impact can you have in general, right? You look at things like the telephone invented by

05:13.280 --> 05:17.040
two people on the same day. Like, what does that mean? Like, what does that mean about the shape

05:17.040 --> 05:21.040
of innovation? And I think that what's going on is everyone's building on the shoulders of the same

05:21.040 --> 05:25.600
giants. And so you can kind of, you can't really hope to create something no one else ever would.

05:25.600 --> 05:29.040
You know, if Einstein wasn't born, someone else would have come up with relativity.

05:29.040 --> 05:32.880
You know, you change the timeline a bit, right, that maybe it would have taken another 20 years,

05:32.880 --> 05:37.120
but it wouldn't be that fundamentally humanity would never discover these fundamental truths.

05:37.520 --> 05:44.320
There's some kind of invisible momentum that some people like Einstein or OpenAI is plugging into

05:45.280 --> 05:50.240
that anybody else can also plug into. And ultimately, that wave takes us into a certain

05:50.240 --> 05:53.120
direction. That's what you mean by digitalism. That's right. That's right. And, you know,

05:53.120 --> 05:56.800
this kind of seems to play out in a bunch of different ways, that there's some exponential

05:56.800 --> 06:00.880
that is being written, and that the exponential itself, which one it is, changes, think about

06:00.880 --> 06:06.080
Moore's law, an entire industry set, it's clocked to it for 50 years. Like, how can that be, right?

06:06.080 --> 06:11.760
How is that possible? And yet somehow it happened. And so I think you can't hope to ever invent

06:11.760 --> 06:16.080
something that no one else will. Maybe you can change the timeline a little bit. But if you

06:16.080 --> 06:19.280
really want to make a difference, I think that the thing that you really have to do,

06:19.280 --> 06:23.840
the only real degree of freedom you have is to set the initial conditions under which a technology

06:23.840 --> 06:27.760
is born. And so you think about the internet, right, that there are lots of other competitors

06:27.760 --> 06:33.440
trying to build similar things. And the internet one, and that the initial conditions were that

06:33.440 --> 06:38.480
it was created by this group that really valued people being able to be, you know, anyone being

06:38.480 --> 06:43.040
able to plug in this very academic mindset of being open and connected. And I think that the

06:43.040 --> 06:47.920
internet for the next 40 years really played out that way. You know, maybe today, things are

06:47.920 --> 06:51.440
starting to shift in a different direction. But I think that those initial conditions were really

06:51.440 --> 06:56.560
important to determine the next 40 years worth of progress. That's really beautifully put. So

06:56.560 --> 07:01.680
another example of that I think about, you know, I recently looked at it, I looked at Wikipedia,

07:01.680 --> 07:06.880
the formation of Wikipedia. And I wonder what the internet would be like if Wikipedia had ads.

07:07.600 --> 07:13.200
You know, there's a interesting argument that why they chose not to make it put advertisement

07:13.200 --> 07:18.800
on Wikipedia. I think it's, I think Wikipedia is one of the greatest resources we have on the internet.

07:18.800 --> 07:23.920
It's extremely surprising how well it works and how well it was able to aggregate all this kind of

07:23.920 --> 07:28.640
good information. And essentially the creator of Wikipedia, I don't know, there's probably some

07:28.640 --> 07:33.440
debates there, but set the initial conditions. And now it carried itself forward. That's really

07:33.440 --> 07:37.920
interesting. So you're the way you're thinking about AGI or artificial intelligence is you're

07:37.920 --> 07:43.360
focused on setting the initial conditions for the progress. That's right. That's powerful. Okay, so

07:43.920 --> 07:50.080
look into the future. If you create an AGI system, like one that can ace the Turing test,

07:50.080 --> 07:56.080
natural language, what do you think would be the interactions you would have with it? What do you

07:56.080 --> 07:59.280
think are the questions you would ask? Like what would be the first question you would ask?

08:00.480 --> 08:05.840
It, her, him. That's right. I think that at that point, if you've really built a powerful system

08:05.840 --> 08:10.240
that is capable of shaping the future of humanity, the first question that you really should ask is

08:10.240 --> 08:14.560
how do we make sure that this plays out well? And so that's actually the first question that I would

08:14.560 --> 08:20.640
ask a powerful AGI system is. So you wouldn't ask your colleague, you wouldn't ask like Ilya,

08:20.640 --> 08:24.320
you would ask the AGI system. Oh, we've already had the conversation with Ilya,

08:24.320 --> 08:27.600
right? And everyone here. And so you want as many perspectives and

08:28.400 --> 08:32.400
piece of wisdom as you can for answering this question. So I don't think you necessarily defer

08:32.400 --> 08:37.680
to whatever your powerful system tells you, but you use it as one input to try to figure out what

08:37.680 --> 08:42.960
to do. But I guess fundamentally, what it really comes down to is if you built something really

08:42.960 --> 08:47.600
powerful, and you think about, think about, for example, the creation of, of shortly after the

08:47.600 --> 08:51.280
creation of nuclear weapons, right? The most important question in the world was, what's the

08:51.280 --> 08:55.760
world we're going to be like? How do we set ourselves up in a place where we're going to be

08:55.760 --> 09:00.640
able to survive as a species? With AGI, I think the question is slightly different, right? That

09:00.640 --> 09:04.720
there is a question of how do we make sure that we don't get the negative effects? But there's

09:04.720 --> 09:09.600
also the positive side, right? You imagine that, you know, like, like, what will AGI be like?

09:09.600 --> 09:14.400
Like, what will it be capable of? And I think that one of the core reasons that an AGI can be

09:14.400 --> 09:19.120
powerful and transformative is actually due to technological development, right? If you have

09:19.120 --> 09:25.280
something that's capable, as capable as a human, and that it's much more scalable, that you absolutely

09:25.280 --> 09:28.800
want that thing to go read the whole scientific literature and think about how to create cures

09:28.800 --> 09:33.040
for all the diseases, right? You want it to think about how to go and build technologies to help

09:33.040 --> 09:37.920
us create material abundance and to figure out societal problems that we have trouble with,

09:37.920 --> 09:41.200
like how are we supposed to clean up the environment? And, you know, maybe you want

09:41.200 --> 09:46.480
this to go and invent a bunch of little robots that will go out and be biodegradable and turn

09:46.480 --> 09:54.720
ocean debris into harmless molecules. And I think that that positive side is something that I think

09:54.720 --> 09:59.680
people miss sometimes when thinking about what an AGI will be like. And so I think that if you

09:59.680 --> 10:03.840
have a system that's capable of all of that, you absolutely want its advice about how do I make sure

10:03.840 --> 10:09.040
that we're using your capabilities in a positive way for humanity.

10:09.040 --> 10:14.080
So what do you think about that psychology that looks at all the different possible

10:14.080 --> 10:19.840
trajectories of an AGI system, many of which perhaps the majority of which are positive,

10:19.840 --> 10:24.560
and nevertheless focuses on the negative trajectories? I mean, you get to interact with folks,

10:24.560 --> 10:30.480
you get to think about this, maybe within yourself as well. You look at Sam Harris and so on.

10:30.480 --> 10:34.960
It seems to be, sorry to put it this way, but almost more fun to think about the negative

10:35.600 --> 10:40.720
possibilities. Whatever that's deep in our psychology, what do you think about that?

10:40.800 --> 10:44.400
And how do we deal with it? Because we want AI to help us.

10:44.400 --> 10:47.200
So I think there's kind of two problems

10:48.160 --> 10:53.840
entailed in that question. The first is more of the question of how can you even picture

10:53.840 --> 10:58.320
what a world with a new technology will be like? Now imagine we're in 1950 and I'm trying to

10:58.320 --> 11:08.240
describe Uber to someone. Apps and the internet. Yeah, I mean, that's going to be extremely

11:08.240 --> 11:14.560
complicated, but it's imaginable. It's imaginable, right? And now imagine being in 1950 and predicting

11:14.560 --> 11:19.760
Uber, right? And you need to describe the internet, you need to describe GPS, you need to describe

11:19.760 --> 11:25.680
the fact that everyone's going to have this phone in their pocket. And so I think that just the first

11:25.680 --> 11:30.240
truth is that it is hard to picture how a transformative technology will play out in the world.

11:31.040 --> 11:35.440
We've seen that before with technologies that are far less transformative than AGI will be.

11:35.440 --> 11:41.120
And so I think that one piece is that it's just even hard to imagine and to really put yourself

11:41.120 --> 11:47.760
in a world where you can predict what that positive vision would be like. And I think the

11:47.760 --> 11:54.720
second thing is that I think it is always easier to support the negative side than the positive

11:54.720 --> 12:01.600
side. It's always easier to destroy than create. And less in a physical sense and more just in an

12:01.600 --> 12:06.160
intellectual sense, right? Because I think that with creating something, you need to just get a

12:06.160 --> 12:11.360
bunch of things right and to destroy, you just need to get one thing wrong. And so I think that

12:11.360 --> 12:15.360
what that means is that I think a lot of people's thinking dead ends as soon as they see the negative

12:15.360 --> 12:23.040
story. But that being said, I actually have some hope, right? I think that the positive vision

12:23.040 --> 12:28.640
is something that I think can be, is something that we can talk about. And I think that just

12:28.720 --> 12:32.400
simply saying this fact of, yeah, like there's positive, there's negatives, everyone likes to

12:32.400 --> 12:36.160
dwell on the negative, people actually respond well to that message and say, huh, you're right,

12:36.160 --> 12:40.080
there's a part of this that we're not talking about, not thinking about. And that's actually

12:40.080 --> 12:46.640
something that's I think really been a key part of how we think about AGI at OpenAI, right? You

12:46.640 --> 12:50.880
can kind of look at it as like, okay, like OpenAI talks about the fact that there are risks,

12:50.880 --> 12:55.360
and yet they're trying to build this system. Like, how do you square those two facts?

12:55.920 --> 13:01.440
So do you share the intuition that some people have, I mean, from Sam Harris to even Elon Musk

13:01.440 --> 13:10.400
himself, that it's tricky as you develop AGI to keep it from slipping into the existential threats,

13:10.400 --> 13:18.240
into the negative? What's your intuition about how hard is it to keep AI development on the

13:18.240 --> 13:22.240
positive track? What's your intuition there? To answer that question, you can really look at

13:22.240 --> 13:27.120
how we structure OpenAI. So we really have three main arms. So we have capabilities, which is

13:27.120 --> 13:32.400
actually doing the technical work and pushing forward what these systems can do. There's safety,

13:32.400 --> 13:37.520
which is working on technical mechanisms to ensure that the systems we build are aligned with human

13:37.520 --> 13:42.240
values. And then there's policy, which is making sure that we have governance mechanisms, answering

13:42.240 --> 13:47.760
that question of, well, whose values. And so I think that the technical safety one is the one

13:47.760 --> 13:52.160
that people kind of talk about the most, right? You talk about, like, think about, you know,

13:52.160 --> 13:57.040
all of the dystopic AI movies, a lot of that is about not having good technical safety in place.

13:57.760 --> 14:01.280
And what we've been finding is that, you know, I think that actually a lot of people

14:01.280 --> 14:03.760
look at the technical safety problem and think it's just intractable.

14:05.040 --> 14:09.040
Right, this question of, what do humans want? How am I supposed to write that down?

14:09.040 --> 14:15.280
Can I even write down what I want? No way. And then they stop there. But the thing is,

14:15.280 --> 14:21.280
we've already built systems that are able to learn things that humans can't specify, you know,

14:21.280 --> 14:24.880
even the rules for how to recognize if there's a cat or a dog in an image.

14:24.880 --> 14:27.760
Turns out it's intractable to write that down. And yet we're able to learn it.

14:28.320 --> 14:31.680
And that what we're seeing with systems we build at OpenAI, and they're still

14:31.680 --> 14:36.480
in early proof of concept stage, is that you are able to learn human preferences. You're able

14:36.480 --> 14:40.960
to learn what humans want from data. And so that's kind of the core focus for our technical

14:40.960 --> 14:45.680
safety team. And I think that they're actually, we've had some pretty encouraging updates in

14:45.680 --> 14:50.880
terms of what we've been able to make work. So you have an intuition and a hope that from data,

14:51.600 --> 14:56.160
you know, looking at the value alignment problem, from data, we can build systems that align

14:56.960 --> 15:04.080
with the collective better angels of our nature. So align with the ethics and the morals of human

15:04.080 --> 15:08.240
beings. To even say this in a different way, I mean, think about how do we align humans,

15:08.240 --> 15:13.120
right? Think about like a human baby can grow up to be an evil person or a great person. And a lot

15:13.120 --> 15:17.520
of that is from learning from data, right? That you have some feedback as a child is growing up,

15:17.520 --> 15:23.680
they get to see positive examples. And so I think that just like the only example we have of a

15:23.680 --> 15:30.160
general intelligence that is able to learn from data, to align with human values and to learn

15:30.160 --> 15:36.160
values, I think we shouldn't be surprised that we can do the same sorts of techniques or whether

15:36.160 --> 15:40.320
the same sort of techniques end up being how we, we, we saw value alignment for AGI's.

15:40.960 --> 15:46.800
So let's go even higher. I don't know if you've read the book sapiens, but there's an idea that,

15:46.800 --> 15:51.680
you know, that as a collective as us human beings, we kind of develop together

15:52.480 --> 15:58.640
and ideas that we hold. There's no, in that context, objective truth, we just kind of all

15:58.640 --> 16:02.720
agree to certain ideas and hold them as a collective. Did you have a sense that there is

16:03.360 --> 16:07.440
in the world of good and evil, do you have a sense that to the first approximation,

16:07.440 --> 16:13.520
there are some things that are good and that you could teach systems to behave to be good?

16:14.400 --> 16:19.840
So I think that this actually blends into our third team, right, which is the policy team.

16:19.840 --> 16:24.960
And this is the one, the aspect that I think people really talk about way less than they should,

16:24.960 --> 16:28.960
right? Because imagine that we build super powerful systems that we've managed to figure

16:28.960 --> 16:33.280
out all the mechanisms for these things to do whatever the operator wants. The most important

16:33.280 --> 16:38.400
question becomes, who's the operator? What do they want? And how is that going to affect everyone

16:38.400 --> 16:44.640
else? Right? And, and I think that this question of what is good, what are those values? I mean,

16:44.640 --> 16:48.320
I think you don't even have to go to those, those, those very grand existential places

16:48.320 --> 16:53.280
to start to realize how hard this problem is. You just look at different countries and cultures

16:53.280 --> 16:58.800
across the world and that there's, there's a very different conception of how the world works and

16:58.960 --> 17:04.800
you know, what, what, what kinds of ways that society wants to operate. And so I think that,

17:04.800 --> 17:10.880
that the really core question is, is actually very concrete. And I think it's not a question

17:10.880 --> 17:16.480
that we have ready answers to, right? Is how do you have a world where all the different countries

17:16.480 --> 17:22.720
that we have, United States, China, Russia, and you know, the hundreds of other countries out there

17:22.720 --> 17:29.440
are able to continue to not just operate in the way that they, that they see fit, but in, in a,

17:29.440 --> 17:34.320
that the world that emerges in these, where you have these very powerful systems,

17:36.000 --> 17:40.560
operating alongside humans, ends up being something that empowers humans more, that makes

17:40.560 --> 17:46.480
like, human existence be a more meaningful thing and that people are happier and wealthier and

17:47.040 --> 17:50.960
able to live more fulfilling lives. It's not an obvious thing for how to design that world

17:51.520 --> 17:56.480
once you have that very powerful system. So if we take a little step back and we're having

17:56.480 --> 18:01.920
like a fascinating conversation and Open AI is in many ways a tech leader in the world.

18:01.920 --> 18:06.240
And yet we're thinking about these big existential questions, which is fascinating,

18:06.240 --> 18:10.720
really important. I think you're a leader in that space and that's a really important space

18:10.720 --> 18:15.760
of just thinking how AI affects society in a big picture view. So Oscar Wilde said,

18:16.320 --> 18:20.800
we're all in the gutter, but some of us are looking at the stars. And I think Open AI has a

18:21.360 --> 18:26.240
charter that looks to the stars, I would say, to create intelligence, to create general

18:26.240 --> 18:30.080
intelligence, make it beneficial, safe, and collaborative. So can you tell me

18:32.160 --> 18:37.760
how that came about? How a mission like that and the path to creating a mission like that at Open

18:37.760 --> 18:44.000
AI was founded? Yeah. So I think that in some ways it really boils down to taking a look at the

18:44.000 --> 18:49.840
landscape. So if you think about the history of AI that basically for the past 60 or 70 years,

18:49.840 --> 18:54.960
people have thought about this goal of what could happen if you could automate human intellectual

18:54.960 --> 19:00.560
labor. Imagine you can build a computer system that could do that. What becomes possible? We have

19:00.560 --> 19:04.960
a lot of sci-fi that tells stories of various dystopias and increasingly you have movies like

19:04.960 --> 19:10.000
Her that tell you a little bit about maybe more of a little bit utopic vision. You think about

19:10.560 --> 19:17.040
the impacts that we've seen from being able to have bicycles for our minds in computers,

19:17.760 --> 19:24.320
and that I think that the impact of computers on the internet has just far outstripped what anyone

19:24.320 --> 19:29.280
really could have predicted. And so I think that it's very clear that if you can build an AGI,

19:29.280 --> 19:32.640
it will be the most transformative technology that humans will ever create.

19:34.480 --> 19:39.680
And so what it boils down to then is a question of, well, is there a path? Is there hope? Is there

19:39.680 --> 19:45.040
a way to build such a system? And I think that for 60 or 70 years that people got excited and

19:46.080 --> 19:51.920
that ended up not being able to deliver on the hopes that people had pinned on them. And I think

19:51.920 --> 19:59.520
that then that after two winters of AI development, that people I think kind of almost stopped daring

19:59.520 --> 20:05.360
to dream that really talking about AGI or thinking about AGI became almost this taboo in the community.

20:06.320 --> 20:10.880
But I actually think that people took the wrong lesson from AI history. And if you look back,

20:10.880 --> 20:15.760
starting in 1959 is when the Perceptron was released. And this is basically one of the

20:15.760 --> 20:20.640
earliest neural networks. It was released to what was perceived as this massive over-hype.

20:20.640 --> 20:27.200
So in the New York Times in 1959, you have this article saying that the Perceptron will one day

20:27.200 --> 20:31.360
recognize people, call out their names, instantly translate speech between languages,

20:31.360 --> 20:36.080
and people at the time looked at this and said, this is your system can't do any of that. And

20:36.080 --> 20:40.560
basically spent 10 years trying to discredit the whole Perceptron direction and succeeded.

20:40.560 --> 20:45.840
And all the funding dried up and people kind of went in other directions. And in the 80s,

20:45.840 --> 20:50.160
there was this resurgence. And I'd always heard that the resurgence in the 80s was due to the

20:50.160 --> 20:54.960
invention of back propagation and these algorithms that got people excited. But actually, the causality

20:54.960 --> 20:59.040
was due to people building larger computers. That you can find these articles from the 80s

20:59.040 --> 21:02.800
saying that the democratization of computing power suddenly meant that you could run these

21:02.800 --> 21:06.960
larger neural networks. And then people started to do all these amazing things back propagation

21:06.960 --> 21:10.880
algorithm was invented. And the neural nets people were running were these tiny little

21:10.880 --> 21:16.080
like 20 neuron neural nets. What are you supposed to learn with 20 neurons? And so, of course,

21:16.720 --> 21:21.840
they weren't able to get great results. And it really wasn't until 2012 that this approach

21:21.840 --> 21:26.800
that's almost the most simple, natural approach that people had come up with in the 50s.

21:27.360 --> 21:30.640
Right. And in some ways, even in the 40s, before there were computers with the pits,

21:30.640 --> 21:36.480
McCullin there in Iran, suddenly this became the best way of solving problems.

21:37.120 --> 21:40.640
Right. And I think there are three core properties that deep learning has

21:41.360 --> 21:46.560
that I think are very worth paying attention to. The first is generality. We have a very small

21:46.560 --> 21:52.000
number of deep learning tools, SGD, deep neural net, maybe some, some, you know, RL,

21:52.960 --> 21:57.200
and it solves this huge variety of problems, speech recognition, machine translation,

21:57.200 --> 22:01.920
game playing, all of these problems, small set of tools. So there's the generality.

22:02.800 --> 22:06.160
There's a second piece, which is the competence. You want to solve any of those problems

22:06.880 --> 22:11.360
throughout 40 years worth of normal computer vision research, replace it with a deep neural

22:11.360 --> 22:16.160
net. It's kind of worked better. And there's a third piece, which is the scalability, right?

22:16.160 --> 22:20.400
That one thing that has been shown time and time again is that you, if you have a larger neural

22:20.400 --> 22:26.880
network, throw more compute, more data at it, it will work better. Those three properties together

22:26.880 --> 22:31.920
feel like essential parts of building a general intelligence. Now, it doesn't just mean that

22:31.920 --> 22:36.640
if we scale up what we have, that we will have an AGI, right? There are clearly missing pieces.

22:36.640 --> 22:42.320
There are missing ideas. We need to have answers for reasoning. But I think that the core here

22:42.320 --> 22:47.840
is that for the first time, it feels that we have a paradigm that gives us hope

22:47.840 --> 22:52.080
that general intelligence can be achievable. And so as soon as you believe that,

22:52.080 --> 22:56.880
everything else becomes, comes into focus, right? If you imagine that you may be able to and,

22:56.880 --> 23:01.040
you know, that the timeline, I think, remains uncertain. But I think that, you know,

23:01.040 --> 23:04.880
certainly within our lifetimes and possibly within a much shorter period of time than,

23:04.880 --> 23:09.440
than people would expect, if you can really build the most transformative technology that will

23:09.440 --> 23:13.520
ever exist, you stop thinking about yourself so much, right? And you start thinking about just

23:13.520 --> 23:17.280
like, how do you have a world where this goes well, and that you need to think about the

23:17.280 --> 23:21.600
practicalities of how do you build an organization and get together a bunch of people and resources,

23:22.240 --> 23:29.760
and to make sure that people feel motivated and ready to do it. But I think that then

23:29.760 --> 23:33.520
you start thinking about, well, what if we succeed? And how do we make sure that when we

23:33.520 --> 23:38.640
succeed, that the world is actually the place that we want ourselves to exist in? And, you know,

23:38.640 --> 23:43.840
almost in the Rawlsian Bale sense of the word. And so that's kind of the broader landscape.

23:43.840 --> 23:51.440
And OpenAI was really formed in 2015 with that high level picture of AGI might be possible

23:51.440 --> 23:57.440
sooner than people think, and that we need to try to do our best to make sure it's going to go well.

23:57.440 --> 24:01.040
And then we spent the next couple of years really trying to figure out, what does that mean? How do

24:01.040 --> 24:07.440
we do it? And, you know, I think that typically with a company, you start out very small. So you

24:07.440 --> 24:10.960
want to co-founder and you build a product, you get some users, you get a product market fit,

24:11.280 --> 24:16.560
then at some point you raise some money, you hire people, you scale, and then down the road,

24:16.560 --> 24:21.120
then the big companies realize you exist and try to kill you. And for OpenAI, it was basically

24:21.120 --> 24:27.440
everything in exactly the opposite order. Let me just pause for a second. You said a lot of things.

24:27.440 --> 24:35.040
And let me just admire the jarring aspect of what OpenAI stands for, which is daring to dream.

24:35.040 --> 24:39.200
I mean, you said it's pretty powerful. You caught me off guard because I think that's very true.

24:40.160 --> 24:46.640
The step of just daring to dream about the possibilities of creating intelligence

24:46.640 --> 24:53.520
in a positive and a safe way, but just even creating intelligence is a much needed, refreshing

24:55.600 --> 25:00.320
catalyst for the AI community. So that's the starting point. Okay. So then the formation of

25:00.320 --> 25:06.880
OpenAI. I would just say that, you know, when we're starting OpenAI, that kind of the first

25:06.880 --> 25:11.280
question that we had is, is it too late to start a lab with a bunch of the best people?

25:11.920 --> 25:15.200
Right? Was that even possible? That was an actual question. That was really,

25:15.200 --> 25:20.400
that was the core question of, you know, we had this dinner in July of 2015, and that was

25:20.400 --> 25:25.120
really what we spent the whole time talking about. And, you know, because it's, you think

25:25.120 --> 25:31.440
about kind of where AI was, is that it transitioned from being an academic pursuit to an industrial

25:31.440 --> 25:36.000
pursuit. And so a lot of the best people were in these big research labs, and that we wanted to

25:36.000 --> 25:41.200
start our own one that, you know, no matter how much resources we could accumulate would be,

25:41.200 --> 25:45.280
you know, pale in comparison to the big tech companies. And we knew that. And there's a

25:45.280 --> 25:49.280
question of, are we going to be actually able to get this thing off the ground? You need critical

25:49.280 --> 25:53.440
mass. You can't just do you and a co-founder build a product, right? You really need to have a group

25:53.440 --> 25:59.600
of, you know, five to 10 people. And we kind of concluded it wasn't obviously impossible. So it

25:59.600 --> 26:05.760
seemed worth trying. Well, you're also a dreamer. So who knows, right? That's right. Okay. So

26:06.880 --> 26:14.400
speaking of that, competing with the big players, let's talk about some of the tricky things as you

26:14.400 --> 26:21.440
think through this process of growing, of seeing how you can develop these systems at scale that

26:21.440 --> 26:30.160
competes. So you recently formed OpenAI LP, a new cap profit company that now carries the name

26:30.160 --> 26:36.560
OpenAI. So OpenAI is now this official company. The original nonprofit company still exists and

26:36.560 --> 26:42.480
carries the OpenAI nonprofit name. So can you explain what this company is, what the purpose

26:42.480 --> 26:50.720
of its creation is, and how did you arrive at the decision to create it? OpenAI, the whole entity

26:50.720 --> 26:56.960
and OpenAI LP as a vehicle is trying to accomplish the mission of ensuring that artificial general

26:56.960 --> 27:00.800
intelligence benefits everyone. And the main way that we're trying to do that is by actually

27:00.800 --> 27:04.800
trying to build general intelligence to ourselves and make sure the benefits are distributed to the

27:04.800 --> 27:09.840
world. That's the primary way. We're also fine if someone else does this, right? It doesn't have

27:09.840 --> 27:14.560
to be us. If someone else is going to build an AGI and make sure that the benefits don't get locked

27:14.560 --> 27:20.320
up in one company or, you know, with one set of people, like, we're actually fine with that.

27:21.040 --> 27:28.240
And so those ideas are baked into our charter, which is kind of the foundational document

27:28.240 --> 27:34.320
that describes kind of our values and how we operate. But it's also really baked into the

27:34.320 --> 27:40.640
structure of OpenAI LP. And so the way that we've set up OpenAI LP is that in the case where we

27:40.640 --> 27:46.480
succeed, right, if we actually build what we're trying to build, then investors are able to get

27:46.560 --> 27:52.560
a return. But that return is something that is capped. And so if you think of AGI in terms of the

27:52.560 --> 27:56.480
value that you could really create, you're talking about the most transformative technology ever

27:56.480 --> 28:01.120
created, it's going to create, or does the magnitude more value than any existing company?

28:01.760 --> 28:07.760
And that all of that value will be owned by the world, like legally titled to the nonprofit

28:07.760 --> 28:11.680
to fulfill that mission. And so that's the structure.

28:12.640 --> 28:18.880
So the mission is a powerful one. And it's one that I think most people would agree with.

28:18.880 --> 28:25.600
It's how we would hope AI progresses. And so how do you tie yourself to that mission? How do you

28:25.600 --> 28:33.360
make sure you do not deviate from that mission? That, you know, other incentives that are profit

28:33.360 --> 28:38.960
driven wouldn't don't interfere with the mission. So this was actually a really core question for

28:38.960 --> 28:43.200
us for the past couple of years, because, you know, I'd say that like the way that our history

28:43.200 --> 28:46.960
went was that for the first year, we were getting off the ground, right? We had this high level

28:46.960 --> 28:53.360
picture, but we didn't know exactly how we wanted to accomplish it. And really two years ago is

28:53.360 --> 28:57.840
when we first started realizing in order to build AGI, we're just going to need to raise way more

28:57.840 --> 29:03.120
money than we can as a nonprofit. And you know, we're talking many billions of dollars. And so

29:04.320 --> 29:07.920
the first question is, how are you supposed to do that and stay true to this mission?

29:08.560 --> 29:12.000
And we looked at every legal structure out there and included none of them are quite right for

29:12.000 --> 29:15.200
what we wanted to do. And I guess it shouldn't be too surprising if you're going to do some like

29:15.200 --> 29:18.800
crazy unprecedented technology that you're going to have to come with some crazy unprecedented

29:18.800 --> 29:26.000
structure to do it in. And a lot of our conversation was with people at OpenAI, right,

29:26.000 --> 29:30.400
the people who really joined because they believe so much in this mission, and thinking about how

29:30.400 --> 29:36.160
do we actually raise the resources to do it and also stay true to what we stand for. And the place

29:36.160 --> 29:40.000
you've got to start is to really align on what is it that we stand for, right? What are those

29:40.000 --> 29:44.880
values? What's really important to us? And so I'd say that we spent about a year really compiling

29:44.880 --> 29:49.680
the OpenAI charter and that determines, and if you even look at the first the first line item in

29:49.680 --> 29:53.600
there, it says that look, we expect we're going to have to marshal huge amounts of resources,

29:53.600 --> 29:58.560
but we're going to make sure that we minimize conflict of interest with the mission. And that

29:58.560 --> 30:03.280
kind of aligning on all of those pieces was the most important step towards figuring out

30:04.160 --> 30:09.600
how do we structure a company that can actually raise the resources to do what we need to do.

30:10.240 --> 30:17.040
I imagine OpenAI, the decision to create OpenAI LP was a really difficult one. And there was a

30:17.040 --> 30:23.760
lot of discussions as you mentioned for a year. And there's different ideas, perhaps detractors

30:23.760 --> 30:30.080
within OpenAI, sort of different paths that you could have taken. What were those concerns?

30:30.080 --> 30:33.920
What were the different paths considered? What was that process of making that decision like?

30:34.800 --> 30:40.240
But so if you look actually at the OpenAI charter, that there's almost two paths embedded within it.

30:40.800 --> 30:46.160
There is, we are primarily trying to build AGI ourselves, but we're also okay if someone else

30:46.160 --> 30:51.440
does it. And this is a weird thing for a company. It's really interesting, actually. Yeah. There

30:51.440 --> 30:57.360
is an element of competition that you do want to be the one that does it. But at the same time,

30:57.360 --> 31:00.880
you're okay if somebody else doesn't. We'll talk about that a little bit, that trade-off,

31:00.880 --> 31:04.960
that dance that's really interesting. And I think this was the core tension as we were

31:04.960 --> 31:09.920
designing OpenAI LP and really the OpenAI strategy, is how do you make sure that both,

31:09.920 --> 31:15.120
you have a shot at being a primary actor, which really requires building an organization,

31:15.760 --> 31:20.480
raising massive resources, and really having the will to go and execute on some really,

31:20.480 --> 31:25.120
really hard vision. You need to really sign up for a long period to go and take on a lot of

31:25.120 --> 31:31.600
pain and a lot of risk. And to do that, normally you just import the startup mindset, right?

31:31.600 --> 31:35.360
And that you think about, okay, how do we out-execute everyone? You have this very competitive

31:35.360 --> 31:39.760
angle. But you also have the second angle of saying that, well, the true mission isn't for

31:39.760 --> 31:46.560
OpenAI to build AGI. The true mission is for AGI to go well for humanity. And so how do you take

31:46.560 --> 31:52.000
all of those first actions and make sure you don't close the door on outcomes that would actually

31:52.000 --> 31:56.560
be positive and fulfill the mission? And so I think it's a very delicate balance, right?

31:56.560 --> 32:01.200
And I think that going 100% one direction or the other is clearly not the correct answer.

32:01.200 --> 32:05.280
And so I think that even in terms of just how we talk about OpenAI and think about it,

32:05.280 --> 32:09.600
there's just like one thing that's always in the back of my mind is to make sure

32:09.600 --> 32:14.640
that we're not just saying OpenAI's goal is to build AGI, right? That it's actually much

32:14.640 --> 32:20.160
broader than that, right? That, first of all, it's not just AGI. It's safe AGI that's very important.

32:20.160 --> 32:23.920
But secondly, our goal isn't to be the ones to build it. Our goal is to make sure it goes well

32:23.920 --> 32:28.720
for the world. And so I think that figuring out how do you balance all of those and to get people

32:28.720 --> 32:35.760
to really come to the table and compile the like a single document that encompasses all of that,

32:36.480 --> 32:43.440
wasn't trivial. So part of the challenge here is your mission is, I would say, beautiful,

32:43.440 --> 32:48.160
empowering and a beacon of hope for people in the research community and just people thinking

32:48.160 --> 32:55.760
about AI. So your decisions are scrutinized more than I think a regular profit driven company.

32:55.760 --> 33:00.080
Do you feel the burden of this in the creation of the charter and just in the way you operate?

33:00.080 --> 33:00.320
Yes.

33:02.880 --> 33:10.320
So why do you lean into the burden by creating such a charter? Why not keep it quiet?

33:10.320 --> 33:15.360
I mean, it just boils down to the mission, right? Like, I'm here and everyone else is here because

33:15.360 --> 33:17.840
we think this is the most important mission, right?

33:17.840 --> 33:24.640
Dare to dream. All right. So do you think you can be good for the world or create an AGI system

33:24.640 --> 33:31.520
that's good when you're a for profit company? From my perspective, I don't understand why profit

33:32.880 --> 33:41.280
interferes with positive impact on society. I don't understand why Google that makes most

33:41.280 --> 33:47.360
of its money from ads can't also do good for the world or other companies, Facebook, anything.

33:47.360 --> 33:55.920
I don't understand why those have to interfere. Profit isn't the thing in my view that affects

33:55.920 --> 34:01.280
the impact of a company. What affects the impact of the company is the charter, is the culture,

34:01.280 --> 34:08.640
is the people inside and profit is the thing that just fuels those people. What are your views there?

34:08.640 --> 34:14.080
Yeah. So I think that's a really good question and there's some real long-standing debates

34:14.080 --> 34:18.560
in human society that are wrapped up in it. The way that I think about it is just think about

34:18.560 --> 34:25.840
what are the most impactful nonprofits in the world? What are the most impactful for profits in

34:25.840 --> 34:30.800
the world? Right. It's much easier to list the for profits. That's right. And I think that there's

34:30.800 --> 34:37.120
some real truth here that the system that we set up, the system for kind of how today's world is

34:37.120 --> 34:44.320
organized is one that really allows for huge impact and that kind of part of that is that you

34:44.320 --> 34:51.280
need to be, that for profits are self-sustaining and able to kind of build on their own momentum.

34:51.280 --> 34:55.920
And I think that's a really powerful thing. It's something that when it turns out that we

34:55.920 --> 35:00.000
haven't set the guardrails correctly causes problems. Think about logging companies that go

35:00.000 --> 35:05.840
into the rainforest. That's really bad. We don't want that. And it's actually really interesting

35:05.840 --> 35:11.200
to me that kind of this question of how do you get positive benefits out of a for-profit company?

35:11.200 --> 35:14.560
It's actually very similar to how do you get positive benefits out of an AGI,

35:15.600 --> 35:20.240
that you have this very powerful system. It's more powerful than any human and

35:20.240 --> 35:24.400
it's kind of autonomous in some ways. It's super human in a lot of axes and somehow you have to

35:24.400 --> 35:30.000
set the guardrails to get good things to happen. But when you do, the benefits are massive. And so

35:30.000 --> 35:35.600
I think that when I think about nonprofit versus for-profit, I think it's just not enough

35:35.680 --> 35:39.920
happens in nonprofits. They're very pure, but it's just kind of, you know, it's just hard to do things

35:39.920 --> 35:46.320
there. And for-profits in some ways, like too much happens. But if kind of shaped in the right way,

35:46.320 --> 35:52.320
it can actually be very positive. And so with OpenILP, we're picking a road in between. Now,

35:52.320 --> 35:56.000
the thing that I think is really important to recognize is that the way that we think about

35:56.000 --> 36:01.520
OpenILP is that in the world where AGI actually happens, right? In a world where we are successful,

36:01.520 --> 36:04.960
we build the most transformative technology ever, the amount of value we're going to create will

36:04.960 --> 36:14.160
be astronomical. And so then in that case, the cap that we have will be a small fraction of the

36:14.160 --> 36:18.880
value we create. And the amount of value that goes back to investors and employees looks pretty

36:18.880 --> 36:25.200
similar to what would happen in a pretty successful startup. And that's really the case that we're

36:25.200 --> 36:30.560
optimizing for, right? That we're thinking about in the success case, making sure that the value we

36:30.560 --> 36:35.200
create doesn't get locked up. And I expect that in other, you know, for-profit companies that it's

36:35.200 --> 36:40.320
possible to do something like that, I think it's not obvious how to do it, right? And I think that

36:40.320 --> 36:44.400
as a for-profit company, you have a lot of fiduciary duty to your shareholders and that

36:44.400 --> 36:49.360
there are certain decisions that you just cannot make. In our structure, we've set it up so that

36:50.080 --> 36:55.200
we have a fiduciary duty to the charter that we always get to make the decision that is right

36:55.200 --> 37:00.080
for the charter rather than even if it comes at the expense of our own stakeholders.

37:00.640 --> 37:04.880
And so I think that when I think about what's really important, it's not really about

37:04.880 --> 37:10.320
non-profit versus for-profit. It's really a question of if you build a GI and you kind of,

37:10.320 --> 37:16.240
you know, humanity is now at this new age, who benefits? Whose lives are better? And I think

37:16.240 --> 37:22.080
that what's really important is to have an answer that is everyone. Yeah, which is one of the core

37:22.080 --> 37:27.040
aspects of the charter. So one concern people have, not just with OpenAI, but with Google,

37:27.040 --> 37:36.400
Facebook, Amazon, anybody, really, that's creating impact at scale is how do we avoid,

37:36.400 --> 37:42.480
as your charter says, avoid enabling the use of AI or AGI to unduly concentrate power?

37:43.520 --> 37:48.480
Why would not a company like OpenAI keep all the power of an AGI system to itself?

37:48.480 --> 37:49.360
The charter.

37:49.360 --> 37:51.520
The charter. So, you know, how does the charter

37:51.920 --> 37:56.640
actualize itself in day to day?

37:57.200 --> 38:02.000
So I think that the first to zoom out, right, that the way that we structure the company is so

38:02.000 --> 38:07.280
that the power for sort of, you know, dictating the actions that OpenAI takes ultimately rests

38:07.280 --> 38:12.240
with the board, right? The board of the non-profit and the board is set up in certain ways with

38:12.240 --> 38:16.640
certain restrictions that you can read about in the OpenAI LP blog post. But effectively,

38:16.720 --> 38:24.320
the board is the governing body for OpenAI LP. And the board has a duty to fulfill the mission

38:24.320 --> 38:30.080
of the non-profit. And so that's kind of how we tie, how we thread all these things together.

38:30.800 --> 38:35.520
Now, there's a question of so day to day, how do people, the individuals who in some ways are

38:35.520 --> 38:39.760
the most empowered ones, right? You know, the board sort of gets to call the shots at the high level,

38:39.760 --> 38:43.840
but the people who are actually executing are the employees, right? The people here

38:43.840 --> 38:47.360
on a day to day basis who have the, you know, the keys to the technical whole kingdom.

38:48.880 --> 38:53.840
And there, I think that the answer looks a lot like, well, how does any company's values

38:53.840 --> 38:58.080
get actualized, right? And I think that a lot of that comes down to the unique people who are here

38:58.080 --> 39:03.440
because they really believe in that mission and they believe in the charter and that they

39:03.440 --> 39:08.720
are willing to take actions that maybe are worse for them, but are better for the charter.

39:08.720 --> 39:13.120
And that's something that's really baked into the culture. And honestly, I think it's, you know,

39:13.120 --> 39:17.600
I think that that's one of the things that we really have to work to preserve as time goes on.

39:18.240 --> 39:22.080
And that's a really important part of how we think about hiring people and bringing people

39:22.080 --> 39:27.440
into open AI. So there's people here, there's people here who could speak up and say,

39:28.960 --> 39:34.480
like, hold on a second, this is totally against what we stand for, culture wise.

39:34.480 --> 39:38.000
Yeah, yeah, for sure. I mean, I think that we actually have, I think that's like a pretty

39:38.000 --> 39:44.320
important part of how we operate and how we have, even again, with designing the charter and

39:44.320 --> 39:49.440
designing open ALP in the first place, that there has been a lot of conversation with employees

39:49.440 --> 39:53.200
here and a lot of times where employees said, wait a second, this seems like it's going in

39:53.200 --> 39:57.280
the wrong direction. And let's talk about it. And so I think one thing that's, I think a really,

39:57.280 --> 40:02.080
and you know, here's actually one thing that I think is very unique about us as a small company

40:02.080 --> 40:05.760
is that if you're at a massive tech giant, that's a little bit hard for someone who's

40:05.760 --> 40:10.000
aligned employee to go and talk to the CEO and say, I think that we're doing this wrong.

40:10.720 --> 40:15.600
And, you know, you'll get companies like Google that have had some collective action from employees

40:15.600 --> 40:20.080
to, you know, make ethical changes around things like Maven. And so maybe there are

40:20.080 --> 40:24.320
mechanisms that other companies that work. But here, super easy for anyone to pull me aside,

40:24.320 --> 40:27.040
to pull Sam aside, to pull Eli aside, and people do it all the time.

40:27.680 --> 40:32.080
One of the interesting things in the charter is this idea that it'd be great if you could try to

40:32.080 --> 40:38.720
describe or untangle switching from competition to collaboration and lead stage AGI development.

40:38.720 --> 40:42.400
It's really interesting this dance between competition and collaboration. How do you

40:42.400 --> 40:46.880
think about that? Yeah, assuming that you can actually do the technical side of AGI development,

40:46.880 --> 40:50.320
I think there's going to be two key problems with figuring out how do you actually deploy it,

40:50.320 --> 40:55.280
make it go well. The first one of these is the run up to building the first AGI.

40:56.240 --> 40:59.840
You look at how self-driving cars are being developed, and it's a competitive race.

41:00.800 --> 41:04.080
The thing that always happens in competitive race is that you have huge amounts of pressure

41:04.080 --> 41:09.920
to get rid of safety. And so that's one thing we're very concerned about, right, is that people,

41:09.920 --> 41:16.640
multiple teams figuring out, we can actually get there. But, you know, if we took the slower path

41:16.640 --> 41:22.240
that is more guaranteed to be safe, we will lose. And so we're going to take the fast path.

41:22.240 --> 41:27.360
And so the more that we can, both ourselves, be in a position where we don't generate that

41:27.360 --> 41:32.400
competitive race, where we say, if the race is being run and that someone else is further

41:32.400 --> 41:36.800
ahead than we are, we're not going to try to leapfrog. We're going to actually work with them,

41:36.800 --> 41:41.360
right? We will help them succeed. As long as what they're trying to do is to fulfill our mission,

41:42.000 --> 41:45.760
then we're good. We don't have to build AGI ourselves. And I think that's a really important

41:45.760 --> 41:49.600
commitment from us. But it can't just be unilateral, right? I think that it's really

41:49.600 --> 41:54.320
important that other players who are serious about building AGI make similar commitments,

41:54.320 --> 41:58.240
right? And I think that that, you know, again, to the extent that everyone believes that AGI

41:58.240 --> 42:01.600
should be something to benefit everyone, then it actually really shouldn't matter which company

42:01.600 --> 42:05.920
builds it. And we should all be concerned about the case where we just race so hard to get there

42:05.920 --> 42:12.000
that something goes wrong. So what role do you think government, our favorite entity,

42:12.000 --> 42:18.400
has in setting policy and rules about this domain, from research to the development to

42:19.360 --> 42:22.800
early stage to late stage AI and AGI development?

42:22.800 --> 42:27.120
So I think that, first of all, it's really important that government's in there,

42:27.680 --> 42:30.800
right? In some way, shape or form, you know, at the end of the day, we're talking about

42:30.800 --> 42:37.120
building technology that will shape how the world operates and that there needs to be government

42:37.120 --> 42:43.520
as part of that answer. And so that's why we've done a number of different congressional testimonies.

42:43.520 --> 42:49.200
We interact with a number of different lawmakers and that, you know, right now, a lot of our message

42:49.200 --> 42:56.800
to them is that it's not the time for regulation, it is the time for measurement, right? That our

42:56.800 --> 43:00.560
main policy recommendation is that people, and, you know, the government does this all the time

43:00.560 --> 43:06.480
with bodies like NIST, spend time trying to figure out just where the technology is,

43:06.480 --> 43:12.160
how fast it's moving and can really become literate and up to speed with respect to what to

43:12.160 --> 43:18.080
expect. So I think that today, the answer really is about measurement. And I think that there will

43:18.080 --> 43:24.160
be a time and place where that will change. And I think it's a little bit hard to predict exactly

43:25.440 --> 43:27.040
what exactly that trajectory should look like.

43:27.040 --> 43:33.440
So there will be a point at which regulation, federal and the United States, the government

43:33.440 --> 43:40.720
steps in and helps be the, I don't want to say the adult in the room, to make sure that there is

43:40.800 --> 43:44.480
strict rules, maybe conservative rules that nobody can cross.

43:45.040 --> 43:49.760
Well, I think there's kind of maybe two angles to it. So today with narrow AI applications,

43:49.760 --> 43:53.840
that I think there are already existing bodies that are responsible and should be responsible

43:53.840 --> 43:58.320
for regulation. You think about, for example, with self-driving cars, that you want the, you know,

43:58.320 --> 44:03.920
the national highway, NETSA, exactly to be regulated in that, that makes sense, right?

44:03.920 --> 44:08.160
That basically what we're saying is that we're going to have these technological systems that

44:08.160 --> 44:13.760
are going to be performing applications that humans already do. Great. We already have ways

44:13.760 --> 44:17.920
of thinking about standards and safety for those. So I think actually empowering those regulators

44:17.920 --> 44:24.080
today is also pretty important. And then I think for, for a GI, you know, that there's going to

44:24.080 --> 44:27.520
be a point where we'll have better answers. And I think that maybe a similar approach

44:27.520 --> 44:31.440
of first measurement and, you know, start thinking about what the rules should be.

44:31.440 --> 44:36.240
I think it's really important that we don't prematurely squash, you know, progress. Like,

44:36.240 --> 44:41.120
I think it's very easy to kind of smother the budding field. And I think that's something to

44:41.120 --> 44:46.240
really avoid. But I don't think that the right way of doing it is to say, let's just try to blaze

44:46.240 --> 44:56.240
ahead and not involve all these other stakeholders. So you've recently released a paper on GPT2

44:56.240 --> 45:03.680
language modeling, but did not release the full model because you had concerns about

45:03.680 --> 45:10.640
the possible negative effects of the availability of such model. It's outside of just that decision.

45:10.640 --> 45:16.960
It's super interesting because of the discussion at a societal level, the discourse it creates. So

45:16.960 --> 45:23.440
it's fascinating in that aspect. But if you think it's the specifics here at first, what are some

45:23.440 --> 45:28.480
negative effects that you envisioned? And of course, what are some of the positive effects?

45:28.480 --> 45:34.960
Yeah. So again, I think to zoom out, like the way that we thought about GPT2 is that with language

45:34.960 --> 45:41.440
modeling, we are clearly on a trajectory right now where we scale up our models and we get

45:42.400 --> 45:47.680
qualitatively better performance. Right. GPT2 itself was actually just a scale up of a model

45:47.680 --> 45:52.720
that we've released in the previous June. Right. And we just ran it at, you know, much larger scale

45:52.720 --> 45:57.760
and we got these results where suddenly starting to write coherent pros, which was not something

45:57.760 --> 46:04.480
we'd seen previously. And what are we doing now? Well, we're going to scale up GPT2 by 10x, by 100x,

46:04.480 --> 46:10.560
by 1000x, and we don't know what we're going to get. And so it's very clear that the model that

46:10.560 --> 46:16.720
we released last June, you know, I think it's kind of like, it's a good academic toy. It's not

46:16.720 --> 46:20.720
something that we think is something that can really have negative applications or, you know,

46:20.720 --> 46:25.360
to the extent that it can, that the positive of people being able to play with it is, you know,

46:25.600 --> 46:31.600
far outweighs the possible harms. You fast forward to not GPT2, but GPT220,

46:32.400 --> 46:37.120
and you think about what that's going to be like. And I think that the capabilities are going to be

46:37.120 --> 46:43.360
substantive. And so there needs to be a point in between the two where you say, this is something

46:43.360 --> 46:47.920
where we are drawing the line, and that we need to start thinking about the safety aspects.

46:47.920 --> 46:51.600
And I think for GPT2, we could have gone either way. And in fact, when we had conversations

46:51.600 --> 46:56.640
internally, that we had a bunch of pros and cons, and it wasn't clear which one, which one

46:56.640 --> 47:01.040
outweighed the other. And I think that when we announced that, hey, we decide not to release

47:01.040 --> 47:05.120
this model, then there was a bunch of conversation where various people said, it's so obvious that

47:05.120 --> 47:07.920
you should have just released it. There are other people said, it's so obvious you should not have

47:07.920 --> 47:12.640
released it. And I think that that almost definitionally means that holding it back was the correct

47:12.640 --> 47:17.760
decision, right? If it's not obvious whether something is beneficial or not, you should

47:17.760 --> 47:23.680
probably default to caution. And so I think that the overall landscape for how we think about it

47:23.680 --> 47:27.840
is that this decision could have gone either way. There are great arguments in both directions.

47:27.840 --> 47:32.640
But for future models down the road, and possibly sooner than you'd expect, because

47:32.640 --> 47:37.360
scaling these things up doesn't actually take that long, those ones, you're definitely not

47:37.360 --> 47:42.480
going to want to release into the wild. And so I think that we almost view this as a test case,

47:42.480 --> 47:48.160
and to see, can we even design, how do you have a society, or how do you have a system that goes

47:48.160 --> 47:53.280
from having no concept of responsible disclosure, where the mere idea of not releasing something

47:53.280 --> 47:58.560
for safety reasons is unfamiliar to a world where you say, okay, we have a powerful model,

47:58.560 --> 48:02.080
let's at least think about it, let's go through some process. And you think about the security

48:02.080 --> 48:06.160
community, it took them a long time to design responsible disclosure, right? You think about

48:06.160 --> 48:10.560
this question of, well, I have a security exploit, I send it to the company, the company is like,

48:10.560 --> 48:17.360
tries to prosecute me, or just ignores it. What do I do? And so the alternatives of, oh,

48:17.360 --> 48:21.040
I just always publish your exploits, that doesn't seem good either. And so it really took a long

48:21.040 --> 48:27.120
time, and it was bigger than any individual. It's really about building a whole community that

48:27.120 --> 48:30.880
believe that, okay, we'll have this process where you send it to the company, if they don't act in

48:30.880 --> 48:35.280
a certain time, then you can go public, and you're not a bad person, you've done the right thing.

48:36.080 --> 48:42.560
And I think that in AI, part of the response to GPT-2 just proves that we don't have any concept

48:42.560 --> 48:50.400
of this. So that's the high level picture. And so I think that this was a really important move

48:50.400 --> 48:55.920
to make, and we could have maybe delayed it for GPT-3, but I'm really glad we did it for GPT-2.

48:55.920 --> 48:59.280
And so now you look at GPT-2 itself, and you think about the substance of, okay,

48:59.280 --> 49:03.600
what are potential negative applications? So you have this model that's been trained on the

49:03.600 --> 49:08.240
internet, which is also going to be a bunch of very biased data, a bunch of very offensive

49:08.240 --> 49:14.640
content in there, and you can ask it to generate content for you on basically any topic. You just

49:14.640 --> 49:19.120
give it a prompt, and it'll just start writing, and it writes content like you see on the internet,

49:19.120 --> 49:25.360
even down to saying advertisement in the middle of some of its generations. And you think about

49:25.360 --> 49:30.160
the possibilities for generating fake news or abusive content. And it's interesting seeing

49:30.240 --> 49:35.600
what people have done with... We released a smaller version of GPT-2, and the people have

49:35.600 --> 49:41.280
done things like try to generate, take my own Facebook message history and generate more

49:41.280 --> 49:48.960
Facebook messages like me, and people generating fake politician content or there's a bunch of

49:48.960 --> 49:53.280
things there where you at least have to think, is this going to be good for the world?

49:54.560 --> 49:57.760
There's the flip side, which is I think that there's a lot of awesome applications that

49:57.760 --> 50:04.400
we really want to see, like creative applications in terms of if you have sci-fi authors that can

50:04.400 --> 50:09.600
work with this tool and come with cool ideas, that seems awesome if we can write better sci-fi

50:09.600 --> 50:13.360
through the use of these tools. And we've actually had a bunch of people write into us asking,

50:13.360 --> 50:17.680
hey, can we use it for a variety of different creative applications?

50:18.240 --> 50:27.040
So the positive are actually pretty easy to imagine. The usual NLP applications are

50:27.680 --> 50:33.120
really interesting, but let's go there. It's kind of interesting to think about a world where,

50:34.160 --> 50:41.040
look at Twitter, where not just fake news, but smarter and smarter bots being able to

50:42.480 --> 50:49.440
spread in an interesting complex networking way information that just floods out us regular

50:49.440 --> 50:57.920
human beings with our original thoughts. So what are your views of this world with GPT-20,

50:58.640 --> 51:04.000
right? How do we think about it? Again, it's like one of those things about in the 50s trying to

51:04.000 --> 51:10.320
describe the internet or the smartphone. What do you think about that world, the nature of

51:10.320 --> 51:18.640
information? One possibility is that we'll always try to design systems that identify robot versus

51:18.640 --> 51:24.800
human and we'll do so successfully. And so we'll authenticate that we're still human. And the

51:24.800 --> 51:30.720
other world is that we just accept the fact that we're swimming in a sea of fake news and just

51:30.720 --> 51:41.440
learn to swim there. Have you ever seen the popular meme of a robot with a physical arm and pen

51:41.520 --> 51:48.880
clicking the I'm not a robot button? I think the truth is that really trying to distinguish between

51:49.520 --> 51:54.080
robot and human is a losing battle. Ultimately, you think it's a losing battle? I think it's a

51:54.080 --> 51:58.480
losing battle ultimately, right? I think that in terms of the content, in terms of the actions

51:58.480 --> 52:02.320
that you can take, think about how captures have gone. The captures used to be a very nice,

52:02.320 --> 52:08.320
simple. You just have this image, all of our OCR is terrible. You put a couple of artifacts in it,

52:08.800 --> 52:13.840
humans are going to be able to tell what it is an AI system wouldn't be able to. Today,

52:13.840 --> 52:18.480
I could barely do captures. And I think that this is just kind of where we're going. I think

52:18.480 --> 52:23.600
captures were a moment in time thing. And as AI systems become more powerful, that there being

52:23.600 --> 52:29.280
human capabilities that can be measured in a very easy automated way that the AIs will not be

52:29.280 --> 52:32.960
capable of. I think that's just like, it's just an increasingly hard technical battle.

52:33.920 --> 52:38.800
But it's not that all hope is lost, right? You think about how do we already authenticate

52:39.600 --> 52:44.000
ourselves, right? We have systems, we have social security numbers, if you're in the U.S.

52:44.000 --> 52:51.360
or you have ways of identifying individual people and having real world identity tied to digital

52:51.360 --> 52:56.800
identity seems like a step towards authenticating the source of content rather than the content

52:56.800 --> 53:02.800
itself. Now, there are problems with that. How can you have privacy and anonymity in a world

53:02.880 --> 53:07.120
where the only content you can really trust is, or the only way you can trust content is by looking

53:07.120 --> 53:12.240
at where it comes from? And so I think that building out good reputation networks may be

53:12.240 --> 53:18.000
one possible solution. But yeah, I think that this question is not an obvious one. And I think that

53:19.200 --> 53:24.400
maybe sooner than we think we'll be in a world where today, I often will read a tweet and be like,

53:24.400 --> 53:27.920
do I feel like a real human wrote this? Or do I feel like this is genuine? I feel like I can

53:27.920 --> 53:32.480
kind of judge the content a little bit. And I think in the future, it just won't be the case.

53:32.480 --> 53:38.000
You look at, for example, the FCC comments on net neutrality. It came out later that millions of

53:38.000 --> 53:42.880
those were auto-generated and that the researchers were able to do various statistical techniques

53:42.880 --> 53:47.920
to do that. What do you do in a world where those statistical techniques don't exist? It's just

53:47.920 --> 53:53.840
impossible to tell the difference between humans and AIs. And in fact, the most persuasive arguments

53:53.840 --> 54:00.400
are written by AI. All that stuff, it's not sci-fi anymore. You look at GPT2 making a great argument

54:00.400 --> 54:04.240
for why recycling is bad for the world. You got to read that and be like, huh, you're right.

54:04.240 --> 54:08.240
We are addressing just the symptoms. Yeah, that's, that's quite interesting. I mean,

54:08.240 --> 54:13.520
ultimately, it boils down to the physical world being the last frontier of proving,

54:13.520 --> 54:19.280
so you said like basically networks of people, humans vouching for humans in the physical world

54:19.280 --> 54:26.880
and somehow the authentication ends there. I mean, if I had to ask you, I mean, you're way too

54:26.880 --> 54:32.400
eloquent for a human. So if I had to ask you to authenticate, like prove, how do I know you're

54:32.400 --> 54:40.560
not a robot and how do you know I'm not a robot? I think that's so far we're in this space, this

54:40.560 --> 54:46.960
conversation we just had, the physical movements we did, is the biggest gap between us and AI systems,

54:46.960 --> 54:52.240
is the physical manipulation. So maybe that's the last frontier. Well, here's another question,

54:52.960 --> 54:59.520
why is solving this problem important? Like what aspects are really important to us? I think that

54:59.520 --> 55:04.800
probably where we'll end up is we'll hone in on what do we really want out of knowing if we're

55:04.800 --> 55:10.800
talking to a human. And I think that again, this comes down to identity. And so I think that the

55:10.800 --> 55:14.960
internet of the future, I expect to be one that will have lots of agents out there that will

55:14.960 --> 55:21.920
interact with you. But I think that the question of is this real flesh and blood human or is this

55:22.240 --> 55:28.880
an automated system may actually just be less important. Let's actually go there. It's GPT2

55:29.600 --> 55:40.160
is impressive. And let's look at GPT20. Why is it so bad that all my friends are GPT20? Why is it

55:40.160 --> 55:47.680
so important on the internet? Do you think to interact with only human beings? Why can't we

55:47.760 --> 55:52.560
live in a world where ideas can come from models trained on human data?

55:54.080 --> 55:56.880
I think this is actually a really interesting question. This comes back to the how do you even

55:56.880 --> 56:02.000
picture a world with some new technology? And I think that one thing that I think is important

56:02.000 --> 56:10.960
is, let's say honesty. And I think that if you have almost the Turing test style sense of technology,

56:10.960 --> 56:16.800
you have AIs that are pretending to be humans and deceiving you. I think that feels like a bad

56:16.800 --> 56:21.120
thing. I think that it's really important that we feel like we're in control of our environment,

56:21.120 --> 56:24.880
that we understand who we're interacting with. And if it's an AI or a human,

56:26.560 --> 56:30.480
that's not something that we're being deceived about. But I think that the flip side of can I

56:30.480 --> 56:35.440
have as meaningful of an interaction with an AI as I can with a human? Well, I actually think here

56:35.440 --> 56:41.280
you can turn to sci-fi and her I think is a great example of asking this very question. One thing

56:41.280 --> 56:46.080
I really love about her is it really starts out almost by asking how meaningful our human virtual

56:46.080 --> 56:52.880
relationships and then you have a human who has a relationship with an AI and that you really

56:52.880 --> 56:57.360
start to be drawn into that and that all of your emotional buttons get triggered in the same way

56:57.360 --> 57:02.560
as if there was a real human that was on the other side of that phone. And so I think that this is

57:02.560 --> 57:07.680
one way of thinking about it is that I think that we can have meaningful interactions and that if

57:07.680 --> 57:12.800
there's a funny joke, some sense it doesn't really matter if it was written by a human or an AI,

57:12.800 --> 57:16.320
but what you don't want in a way where I think we should really draw hard lines

57:16.320 --> 57:21.760
is deception. And I think that as long as we're in a world where why do we build AI systems at

57:21.760 --> 57:26.000
all? The reason we want to build them is to enhance human lives, to make humans be able to do more

57:26.000 --> 57:31.040
things, to have humans feel more fulfilled. And if we can build AI systems that do that,

57:32.160 --> 57:39.200
sign me up. So the process of language modeling, how far do you think it take us? Let's look at

57:39.200 --> 57:46.080
Movie Her. Do you think a dialogue, natural language conversation is formulated by the

57:46.080 --> 57:51.040
Turing test, for example? Do you think that process could be achieved through this kind

57:51.040 --> 57:57.760
of unsupervised language modeling? So I think the Turing test in its real form isn't just about

57:57.760 --> 58:02.240
language. It's really about reasoning too. To really pass the Turing test, I should be able

58:02.240 --> 58:07.760
to teach calculus to whoever's on the other side and have it really understand calculus and be able

58:07.760 --> 58:13.360
to, you know, go and solve new calculus problems. And so I think that to really solve the Turing

58:13.360 --> 58:17.600
test, we need more than what we're seeing with language models. We need some way of plugging

58:17.600 --> 58:23.520
in reasoning. Now, how different will that be from what we already do? That's an open question,

58:23.520 --> 58:28.000
right? It might be that we need some sequence of totally radical new ideas, or it might be that we

58:28.000 --> 58:33.680
just need to kind of shape our existing systems in a slightly different way. But I think that

58:33.680 --> 58:38.240
in terms of how far language modeling will go, it's already gone way further than many people

58:38.240 --> 58:42.240
would have expected, right? I think that things like, and I think there's a lot of really interesting

58:42.240 --> 58:48.720
angles to poke in terms of how much does GPT2 understand physical world? Like, you know, you

58:48.720 --> 58:54.000
read a little bit about fire underwater in GPT2. So it's like, okay, maybe it doesn't quite understand

58:54.000 --> 58:59.680
what these things are. But at the same time, I think that you also see various things like smoke

58:59.680 --> 59:04.320
coming from flame and, you know, a bunch of these things that GPT2, it has no body, it has no physical

59:04.320 --> 59:11.520
experience, it's just statically read data. And I think that I think that if the answer is like,

59:11.520 --> 59:16.080
we don't know yet, then these questions, though, we're starting to be able to actually ask them

59:16.080 --> 59:20.480
to physical systems, the real systems that exist. And that's very exciting. Do you think, what's

59:20.480 --> 59:27.600
your intuition? Do you think if you just scale language modeling, like significantly scale,

59:27.600 --> 59:31.200
that reasoning can emerge from the same exact mechanisms?

59:31.200 --> 59:37.840
I think it's unlikely that if we just scale GPT2 that we'll have reasoning in the full fledged

59:37.840 --> 59:41.360
way. And I think that there's like, you know, the type signature is a little bit wrong, right?

59:41.360 --> 59:46.720
That like, there's something we do with, that we call thinking, right? Where we spend a lot of

59:46.720 --> 59:51.120
compute, like a variable amount of compute to get to better answers, right? I think a little bit

59:51.120 --> 59:57.920
harder, I get a better answer. And that that kind of type signature isn't quite encoded in a GPT,

59:58.560 --> 01:00:03.920
right? GPT will kind of like, it's spent a long time in it's like evolutionary history, baking

01:00:03.920 --> 01:00:08.400
and all this information getting very, very good at this predictive process. And then at runtime,

01:00:08.400 --> 01:00:14.320
I just kind of do one forward pass and, and I'm able to generate stuff. And so, you know, there

01:00:14.320 --> 01:00:19.120
might be small tweaks to what we do in order to get the type signature, right? For example, well,

01:00:19.120 --> 01:00:22.480
you know, it's not really one forward pass, right? You know, you generate symbol by symbol.

01:00:22.480 --> 01:00:26.800
And so maybe you generate like a whole sequence of thoughts and you only keep like the last bit

01:00:26.800 --> 01:00:30.960
or something. Right. But I think that at the very least, I would expect you have to make changes

01:00:30.960 --> 01:00:38.000
like that. Yeah, just exactly how we, you said think is the process of generating thought by

01:00:38.000 --> 01:00:42.960
thought in the same kind of way, like you said, keep the last bit, the thing that we converge

01:00:42.960 --> 01:00:47.200
towards. Yep. And I think there's, there's another piece, which is, which is interesting,

01:00:47.200 --> 01:00:52.240
which is this out of distribution generalization, right? That like thinking somehow lets us do that,

01:00:52.240 --> 01:00:56.000
right? That we have an experience of thing. And yet somehow we just kind of keep refining

01:00:56.000 --> 01:01:02.240
our mental model of it. This is again, something that feels tied to whatever reasoning is.

01:01:03.360 --> 01:01:08.000
And maybe it's a small tweak to what we do. Maybe it's many ideas and we'll take as many decades.

01:01:08.000 --> 01:01:14.560
Yeah. So the assumption there, generalization out of distribution is that it's possible to create

01:01:15.440 --> 01:01:20.880
new, new ideas. The pot, you know, it's possible that nobody's ever created any new ideas. And

01:01:20.880 --> 01:01:29.760
then we're scaling GPT-2 to GPT-20, you would, you would essentially generalize to all possible

01:01:29.760 --> 01:01:35.440
thoughts that us humans can have. Just to play devil's advocate. Right. Right. I mean, how many,

01:01:35.440 --> 01:01:39.920
how many new, new story ideas have we come up with since Shakespeare, right? Yeah, exactly.

01:01:40.320 --> 01:01:44.880
It's just all different forms of love and drama and so on. Okay.

01:01:45.680 --> 01:01:49.200
Not sure if you read Biddle Lesson, a recent blog post by Ray Sutton.

01:01:50.800 --> 01:01:55.680
He basically says something that echoes some of the ideas that you've been talking about, which is

01:01:56.720 --> 01:02:01.120
he says the biggest lesson that can be read from 70 years of AI research is that

01:02:01.120 --> 01:02:06.880
general methods that leverage computation are ultimately going to ultimately win out.

01:02:07.760 --> 01:02:14.160
Do you agree with this? So basically open AI in general about the ideas you're exploring,

01:02:14.160 --> 01:02:20.000
about coming up with methods, whether it's GPT-2 modeling, or whether it's open AI-5

01:02:20.000 --> 01:02:29.120
playing Dota, where a general method is better than a more fine-tuned, expert-tuned method.

01:02:29.680 --> 01:02:33.680
Yeah. So I think that, well, one thing that I think was really interesting about the reaction

01:02:33.680 --> 01:02:38.400
to that blog post was that a lot of people have read this as saying that compute is all that

01:02:38.400 --> 01:02:43.360
matters. And that's a very threatening idea, right? And I don't think it's a true idea either,

01:02:43.360 --> 01:02:46.880
right? It's very clear that we have algorithmic ideas that have been very important

01:02:46.880 --> 01:02:50.720
for making progress. And to really build AGI, you want to push as far as you can on the

01:02:50.720 --> 01:02:56.000
computational scale, and you want to push as far as you can on human ingenuity. And so I think

01:02:56.000 --> 01:02:59.280
you need both. But I think the way that you phrase the question is actually very good,

01:02:59.280 --> 01:03:04.640
right? That it's really about what kind of ideas should we be striving for? And absolutely,

01:03:04.640 --> 01:03:09.680
if you can find a scalable idea, you pour more compute into it, you pour more data into it,

01:03:09.680 --> 01:03:16.480
it gets better. Like that's the real Holy Grail. And so I think that the answer to the question,

01:03:16.480 --> 01:03:21.760
I think, is yes. That's really how we think about it, and that part of why we're excited

01:03:21.760 --> 01:03:26.800
about the power of deep learning and the potential for building AGI is because we look at the system

01:03:26.800 --> 01:03:32.720
that exists in the most successful AI systems, and we realize that you scale those up, they're

01:03:32.720 --> 01:03:36.800
going to work better. And I think that that scalability is something that really gives us

01:03:36.800 --> 01:03:41.360
hope for being able to build transformative systems. So I'll tell you, this is partially an

01:03:41.360 --> 01:03:46.880
emotional, you know, a thing that a response that people often have, if compute is so important

01:03:46.880 --> 01:03:51.680
for state of the art performance, you know, individual developers, maybe a 13 year old

01:03:51.680 --> 01:03:55.600
sitting somewhere in Kansas or something like that, you know, they're sitting, they might not

01:03:55.600 --> 01:04:01.600
even have a GPU and or maybe have a single GPU or 1080 or something like that. And there's this

01:04:01.600 --> 01:04:08.560
feeling like, well, how can I possibly compete or contribute to this world of AI if scale is so

01:04:08.560 --> 01:04:14.400
important? So if you can comment on that, and in general, do you think we need to also in the

01:04:14.400 --> 01:04:22.480
future focus on democratizing compute resources more more or as much as we democratize the algorithms?

01:04:22.480 --> 01:04:28.720
Well, so the way that I think about it is that there's this space of possible progress, right?

01:04:28.720 --> 01:04:33.520
There's a space of ideas and sort of systems that will work that will move us forward. And there's a

01:04:33.520 --> 01:04:37.760
portion of that space, and to some extent, an increasingly significant portion of that space

01:04:37.760 --> 01:04:44.080
that does just require massive compute resources. And for that, that I think that the answer is

01:04:44.080 --> 01:04:48.640
kind of clear, and that part of why we have the structure that we do is because we think it's

01:04:48.640 --> 01:04:52.720
really important to be pushing the scale and to be, you know, building these large clusters and

01:04:52.720 --> 01:04:58.000
systems. But there's another portion of the space that isn't about the large scale compute that are

01:04:58.000 --> 01:05:03.200
these ideas that, and again, I think that for the ideas to really be impactful and really shine,

01:05:03.200 --> 01:05:07.280
that they should be ideas that if you scale them up, would work way better than they do at small

01:05:07.280 --> 01:05:13.120
scale. But you can discover them without massive computational resources. And if you look at the

01:05:13.120 --> 01:05:18.080
history of recent developments, you think about things like the GAN or the VAE, that these are

01:05:18.080 --> 01:05:22.560
ones that I think you could come up with them without having, and, you know, in practice,

01:05:22.560 --> 01:05:26.320
people did come up with them without having massive, massive computational resources.

01:05:26.320 --> 01:05:32.560
Right. I just talked to Ian Goodfellow, but the thing is, the initial GAN produced pretty terrible

01:05:32.560 --> 01:05:39.840
results, right? So only because they're smart enough to know that this is quite surprising,

01:05:39.840 --> 01:05:45.840
it can generate anything that they know. Do you see a world, or is that too optimistic and dream

01:05:45.840 --> 01:05:52.320
or like, to imagine that the compute resources are something that's owned by governments and

01:05:52.320 --> 01:05:58.960
provided as utility? Actually, to some extent, this question reminds me of a blog post from

01:05:58.960 --> 01:06:03.600
one of my former professors at Harvard, this guy, Matt Welsh, who was a systems professor.

01:06:03.600 --> 01:06:08.080
I remember sitting in his tenure talk, right? And, you know, that he had literally just gotten

01:06:08.080 --> 01:06:15.600
tenure. He went to Google for the summer and then decided he wasn't going back to academia, right?

01:06:15.600 --> 01:06:20.240
And that kind of in his blog post, he makes this point that, look, as a systems researcher,

01:06:20.880 --> 01:06:25.040
that I come up with these cool system ideas, right? And I kind of build a little proof of concept.

01:06:25.040 --> 01:06:30.880
And the best thing I could hope for is that the people at Google or Yahoo, which was around at

01:06:30.880 --> 01:06:35.840
the time, will implement it and like, actually make it work at scale, right? That's like the

01:06:35.840 --> 01:06:38.480
dream for me, right? I built the little thing and they've turned it into the big thing that's

01:06:38.480 --> 01:06:44.800
actually working. And for him, he said, I'm done with that. I want to be the person who's actually

01:06:44.800 --> 01:06:49.680
doing building and deploying. And I think that there's a similar dichotomy here, right? I think

01:06:49.680 --> 01:06:55.040
that there are people who've really actually find value. And I think it is a valuable thing to do,

01:06:55.040 --> 01:06:59.120
to be the person who produces those ideas, right? Who builds the proof of concept. And yeah, you

01:06:59.120 --> 01:07:05.040
don't get to generate the coolest possible GAN images, but you invented the GAN, right? And so

01:07:05.040 --> 01:07:08.880
that there's a real trade-off there. And I think that that's a very personal choice,

01:07:08.880 --> 01:07:10.480
but I think there's value in both sides.

01:07:10.480 --> 01:07:16.240
So do you think creating AGI, something or some new models,

01:07:17.840 --> 01:07:23.440
we would see echoes of the brilliance even at the prototype level. So you would be able to develop

01:07:23.440 --> 01:07:30.000
those ideas without scale, the initial seeds. So take a look at, I always like to look at

01:07:30.000 --> 01:07:36.240
examples that exist, right? Look at real precedent. And so take a look at the June 2018 model that

01:07:36.240 --> 01:07:41.920
we released that we scaled up to turn to GPT2. And you can see that at small scale, it set some

01:07:41.920 --> 01:07:47.040
records, right? This was the original GPT. We actually had some cool generations that weren't

01:07:47.040 --> 01:07:52.960
nearly as amazing and really stunning as the GPT2 ones, but it was promising. It was interesting.

01:07:52.960 --> 01:07:57.440
And so I think it is the case that with a lot of these ideas that you see promise at small scale.

01:07:58.160 --> 01:08:02.000
But there is an asterisk here, a very big asterisk, which is sometimes we see

01:08:02.480 --> 01:08:07.600
behaviors that emerge that are qualitatively different from anything we saw at small scale.

01:08:07.600 --> 01:08:12.720
And that the original inventor of whatever algorithm looks at and says, I didn't think

01:08:12.720 --> 01:08:18.000
it could do that. This is what we saw in Dota, right? So PPO was created by John Shulman,

01:08:18.000 --> 01:08:24.640
who's a researcher here. And with Dota, we basically just ran PPO at massive, massive scale.

01:08:24.640 --> 01:08:29.520
And there's some tweaks in order to make it work, but fundamentally it's PPO at the core.

01:08:30.320 --> 01:08:36.880
And we were able to get this long-term planning, these behaviors to really play out

01:08:37.840 --> 01:08:42.560
on a time scale that we just thought was not possible. And John looked at that and was like,

01:08:42.560 --> 01:08:45.760
I didn't think it could do that. That's what happens when you're at three orders of magnitude,

01:08:45.760 --> 01:08:50.080
more scale than you tested at. Yeah, but it still has the same flavors of,

01:08:51.120 --> 01:08:57.840
you know, at least echoes of the expected billions. Although I suspect with GPT,

01:08:57.920 --> 01:09:04.160
scaled more and more, you might get surprising things. So yeah, you're right. It's interesting.

01:09:04.800 --> 01:09:10.080
It's difficult to see how far an idea will go when it's scaled. It's an open question.

01:09:10.960 --> 01:09:15.040
Well, so to that point with Dota and PPO, like, I mean, here's a very concrete one, right? It's

01:09:16.000 --> 01:09:19.120
actually one thing that's very surprising about Dota that I think people don't really pay that

01:09:19.120 --> 01:09:24.560
much attention to is the decree of generalization out of distribution that happens, right? That

01:09:24.560 --> 01:09:30.240
you have this AI that's trained against other bots for its entirety, the entirety of its existence.

01:09:30.240 --> 01:09:38.640
Sorry to take a step back. Can you talk through, you know, a story of Dota, a story of leading

01:09:38.640 --> 01:09:44.560
up to opening I-5 and that past, and what was the process of self-play and so on of training?

01:09:45.280 --> 01:09:47.280
Yeah, yeah, yeah. So with Dota. What is Dota?

01:09:48.480 --> 01:09:52.640
Dota is a complex video game. And we started training, we started trying to solve Dota

01:09:52.640 --> 01:09:57.680
because we felt like this was a step towards the real world relative to other games like Chess or Go,

01:09:57.680 --> 01:10:01.760
right? Those very cerebral games where you just kind of have this board of very discrete moves.

01:10:01.760 --> 01:10:05.520
Dota starts to be much more continuous time. So you have this huge variety of different

01:10:05.520 --> 01:10:11.040
actions that you have a 45-minute game with all these different units and it's got a lot of messiness

01:10:11.040 --> 01:10:16.640
to it that really hasn't been captured by previous games. And famously, all of the hard-coded bots

01:10:16.640 --> 01:10:20.400
for Dota were terrible, right? Just impossible to write anything good for it because it's so

01:10:20.400 --> 01:10:25.520
complex. And so this seemed like a really good place to push what's the state of the art in

01:10:25.520 --> 01:10:29.920
reinforcement learning. And so we started by focusing on the one versus one version of the game

01:10:29.920 --> 01:10:35.600
and we're able to solve that. We're able to beat the world champions and the learning,

01:10:35.600 --> 01:10:39.840
you know, the skill curve was this crazy exponential, right? And it was like constantly

01:10:39.840 --> 01:10:43.920
we were just scaling up, that we were fixing bugs and, you know, that you look at the skill

01:10:43.920 --> 01:10:47.760
curve and it was really a very, very smooth one. So it's actually really interesting to see how

01:10:47.760 --> 01:10:52.560
that like human iteration loop yielded very steady exponential progress.

01:10:52.560 --> 01:10:57.360
And to one side note, first of all, it's an exceptionally popular video game. The side

01:10:57.360 --> 01:11:03.120
effect is that there's a lot of incredible human experts at that video game. So the benchmark

01:11:03.120 --> 01:11:07.680
that you're trying to reach is very high. And the other, can you talk about the approach

01:11:07.680 --> 01:11:11.920
that was used initially and throughout training these agents to play this game?

01:11:11.920 --> 01:11:16.480
Yep. And so the person we used is self-play. And so you have two agents that don't know

01:11:16.480 --> 01:11:21.360
anything. They battle each other. They discover something a little bit good. And now they both

01:11:21.360 --> 01:11:24.960
know it. And they just get better and better and better without bound. And that's a really

01:11:24.960 --> 01:11:30.960
powerful idea, right? That we then went from the one versus one version of the game and scaled up

01:11:30.960 --> 01:11:34.720
to five versus five, right? So you think about kind of like with basketball, where you have this like

01:11:34.720 --> 01:11:41.280
team sport and you need to do all this coordination. And we were able to push the same idea, the same

01:11:41.280 --> 01:11:48.320
self-play to really get to the professional level at the full five versus five version of the game.

01:11:49.040 --> 01:11:55.040
And the things I think are really interesting here is that these agents, in some ways, they're

01:11:55.040 --> 01:11:59.600
almost like an insect-like intelligence, right? Where they have a lot in common with how an insect

01:11:59.600 --> 01:12:03.760
is trained, right? An insect kind of lives in this environment for a very long time, or the

01:12:03.760 --> 01:12:07.440
ancestors of this insect have been around for a long time and had a lot of experience that gets

01:12:07.440 --> 01:12:12.960
baked into this agent. And, you know, it's not really smart in the sense of a human, right? It's

01:12:12.960 --> 01:12:17.200
not able to go and learn calculus, but it's able to navigate its environment extremely well. It's

01:12:17.200 --> 01:12:22.480
able to handle unexpected things in the environment that's never seen before pretty well. And we see

01:12:22.480 --> 01:12:26.960
the same sort of thing with our Dota bots, right? That they're able to, within this game, they're

01:12:26.960 --> 01:12:31.520
able to play against humans, which is something that never existed in its evolutionary environment.

01:12:31.520 --> 01:12:36.480
Totally different play styles from humans versus the bots. And yet it's able to handle it extremely

01:12:36.480 --> 01:12:42.960
well. And that's something that I think was very surprising to us, was something that doesn't really

01:12:42.960 --> 01:12:48.080
emerge from what we've seen with PPO at smaller scale, right? And the kind of scale we're running

01:12:48.080 --> 01:12:54.160
this stuff at was, you know, I could take 100,000 CPU cores running with like hundreds of GPUs.

01:12:54.160 --> 01:13:00.320
It's probably about, you know, like, you know, something like hundreds of years of experience

01:13:00.320 --> 01:13:07.200
going into this bot every single real day. And so that scale is massive. And we start to see

01:13:07.200 --> 01:13:10.160
very different kinds of behaviors out of the algorithms that we all know and love.

01:13:10.800 --> 01:13:20.560
Dota, you mentioned beat the world expert 1v1. And then you weren't able to win 5v5 this year

01:13:21.200 --> 01:13:26.960
at the best players in the world. So what's the comeback story? What's, first of all,

01:13:27.040 --> 01:13:32.480
talk through that. That's an exceptionally exciting event. And what's the following months in this

01:13:32.480 --> 01:13:37.200
year look like? Yeah. Yeah. So one thing that's interesting is that, you know, we lose all the

01:13:37.200 --> 01:13:44.240
time because we play, so the Dota team at OpenAI, we play the bot against better players

01:13:44.240 --> 01:13:49.360
than our system all the time. Or at least we used to, right? Like, you know, the first time we lost

01:13:49.360 --> 01:13:54.000
publicly was we went up on stage at the international and we played against some of the best teams in

01:13:54.000 --> 01:13:58.480
the world. And we ended up losing both games, but we gave them a run for their money, right?

01:13:58.480 --> 01:14:02.880
That both games were kind of 30 minutes, 25 minutes, and they went back and forth, back and

01:14:02.880 --> 01:14:08.400
forth, back and forth. And so I think that really shows that we're at the professional level. And

01:14:08.400 --> 01:14:12.240
that kind of looking at those games, we think that the coin could have gone a different direction

01:14:12.240 --> 01:14:16.720
and it could have had some wins. That was actually very encouraging for us. And, you know, it's

01:14:16.720 --> 01:14:21.680
interesting because the international was at a fixed time, right? So we knew exactly what day

01:14:21.680 --> 01:14:24.560
we were going to be playing and we pushed as far as we could, as fast as we could.

01:14:25.520 --> 01:14:30.320
Two weeks later, we had a bot that had an 80% win rate versus the one that played at TI. So the

01:14:30.320 --> 01:14:34.320
march of progress, you know, you should think of as a snapshot rather than as an end state.

01:14:34.960 --> 01:14:40.960
And so in fact, we'll be announcing our finals pretty soon. I actually think that we'll announce

01:14:40.960 --> 01:14:47.360
our final match prior to this podcast being released. So there should be, we'll be playing,

01:14:48.160 --> 01:14:53.200
against the world champions. And, you know, for us, it's really less about like the way that we

01:14:53.200 --> 01:15:00.080
think about what's upcoming is the final milestone, the final competitive milestone for the project,

01:15:00.080 --> 01:15:06.160
right? That our goal in all of this isn't really about beating humans at Dota. Our goal is to push

01:15:06.160 --> 01:15:09.440
the state of the art and reinforcement learning. And we've done that, right? And we've actually

01:15:09.440 --> 01:15:13.840
learned a lot from our system. And that we have, you know, I think a lot of exciting next steps

01:15:13.840 --> 01:15:17.680
that we want to take. And so, you know, kind of as a final showcase of what we built, we're going

01:15:17.680 --> 01:15:23.280
to do this match. But for us, it's not really the successor failure to see, you know, do we have

01:15:23.280 --> 01:15:28.720
the coin flip go in our direction or against? Where do you see the field of deep learning

01:15:28.720 --> 01:15:36.720
heading in the next few years? Where do you see the work and reinforcement learning perhaps heading?

01:15:36.720 --> 01:15:43.520
And more specifically with OpenAI, all the exciting projects that you're working on,

01:15:44.400 --> 01:15:49.680
what does 2019 hold for you? Massive scale. Scale. I will put an actress on that and just say, you

01:15:49.680 --> 01:15:55.200
know, I think that it's about ideas plus scale, you need both. So that's a really good point. So

01:15:55.200 --> 01:16:03.360
the question, in terms of ideas, you have a lot of projects that are exploring different areas of

01:16:03.360 --> 01:16:08.240
intelligence. And the question is, when you, when you think of scale, do you think about

01:16:08.800 --> 01:16:13.760
growing the scale of those individual projects? Or do you think about adding new projects? And

01:16:13.760 --> 01:16:18.880
sorry to do that. And if you're thinking about adding new projects, or if you look at the past,

01:16:18.880 --> 01:16:23.360
what's the process of coming up with new projects and new ideas? So we really have a

01:16:23.360 --> 01:16:28.480
life cycle of project here. So we start with a few people just working on a small scale idea.

01:16:28.480 --> 01:16:32.080
And language is actually a very good example of this, that it was really, you know, one person

01:16:32.080 --> 01:16:37.040
here who was pushing on language for a long time. And then you get signs of life, right? And so this

01:16:37.040 --> 01:16:42.640
is like, let's say, you know, with, with the original GPT, we had something that was interesting.

01:16:42.640 --> 01:16:46.000
And we said, okay, it's time to scale this, right? It's time to put more people on it,

01:16:46.000 --> 01:16:51.040
put more computational resources behind it. And, and then we just kind of keep pushing and keep

01:16:51.040 --> 01:16:54.720
pushing. And the end state is something that looks like Dota or robotics, where you have a

01:16:54.720 --> 01:16:59.520
large team of, you know, 10 or 15 people that are running things at very large scale, and that

01:16:59.520 --> 01:17:04.880
you're able to really have material engineering, and, and, and, and, you know, sort of machine

01:17:04.880 --> 01:17:10.560
learning science coming together to make systems that work and get material results that just

01:17:10.560 --> 01:17:14.480
would have been impossible otherwise. So we do that whole life cycle, we've done it a number of

01:17:14.480 --> 01:17:20.400
times, you know, typically end to end, it's probably two, two years or so to do it, you know,

01:17:20.400 --> 01:17:23.360
the organization's been around for three years. So maybe we'll find that we also have longer

01:17:23.360 --> 01:17:30.800
life cycle projects. But, you know, we will work up to those. We have, so one team that we were

01:17:30.800 --> 01:17:34.640
actually just starting, Illy and I are kicking off a new team called the reasoning team. And

01:17:34.640 --> 01:17:40.080
that this is to really try to tackle, how do you get neural networks to reason? And we think that

01:17:40.080 --> 01:17:43.680
this will be a long term project. It's one that we're very excited about.

01:17:44.640 --> 01:17:51.840
In terms of reasoning, super exciting topic. What do you, what kind of benchmarks? What kind of

01:17:51.920 --> 01:17:56.560
tests of reasoning do you envision? What, what would, if you set back

01:17:57.840 --> 01:18:02.720
whatever drink and you would be impressed that this system is able to do something,

01:18:02.720 --> 01:18:03.600
what would that look like?

01:18:03.600 --> 01:18:04.800
Not theorem proving.

01:18:04.800 --> 01:18:10.480
Theorem proving. So some kind of logic and especially mathematical logic.

01:18:10.480 --> 01:18:13.760
I think so, right? And I think that there's, there's, there's kind of other problems that are

01:18:13.760 --> 01:18:18.640
dual to theorem proving in particular. You know, you think about programming, I think about even

01:18:18.640 --> 01:18:25.280
like security analysis of, of code, that these all kind of capture the same sorts of core reasoning

01:18:25.280 --> 01:18:28.000
and being able to do some out of distribution generalization.

01:18:29.280 --> 01:18:34.720
It would be quite exciting if open AI reasoning team was able to prove that P equals NP, that

01:18:34.720 --> 01:18:35.440
would be very nice.

01:18:36.000 --> 01:18:39.680
It would be very, very, very exciting, especially if it turns out that P equals NP,

01:18:39.680 --> 01:18:40.640
that'll be interesting too.

01:18:41.600 --> 01:18:45.760
It just, it would be ironic and humorous.

01:18:47.760 --> 01:18:54.160
So what problem stands out to you as the most exciting and challenging and impactful to the

01:18:54.160 --> 01:18:59.520
work for us as a community in general and for open AI this year? You mentioned reasoning,

01:18:59.520 --> 01:19:01.280
I think that's, that's a heck of a problem.

01:19:01.280 --> 01:19:04.160
Yeah. So I think reasoning is an important one. I think it's going to be hard to get good results

01:19:04.160 --> 01:19:09.680
in 2019. You know, again, just like we think about the life cycle takes time. I think for 2019,

01:19:09.680 --> 01:19:13.520
language modeling seems to be kind of on that ramp, right? It's at the point that we have a

01:19:13.520 --> 01:19:16.960
technique that works. We want to scale 100x, 1000x see what happens.

01:19:18.000 --> 01:19:20.560
Awesome. Do you think we're living in a simulation?

01:19:21.760 --> 01:19:25.920
I think it's, I think it's hard to have a real opinion about it. You know, it's actually

01:19:25.920 --> 01:19:30.000
interesting. I separate out things that I think can have like, you know, yield

01:19:30.000 --> 01:19:34.080
materially different predictions about the world from ones that are just kind of,

01:19:34.080 --> 01:19:37.840
you know, fun, fun to speculate about. And I kind of view simulation as more like,

01:19:37.840 --> 01:19:40.480
is there a flying teapot between Mars and Jupiter? Like,

01:19:41.760 --> 01:19:45.040
maybe, but it's a little bit hard to know what that would mean for my life.

01:19:45.040 --> 01:19:51.360
So there is something actionable. So some of the best work opening has done is in the field of

01:19:51.360 --> 01:19:57.680
reinforcement learning. And some of the success of reinforcement learning come from being able

01:19:57.680 --> 01:20:03.760
to simulate the problem you're trying to solve. So do you have a hope for reinforcement for the

01:20:03.760 --> 01:20:07.680
future of reinforcement learning and for the future of simulation? Like, whether it's we're

01:20:07.680 --> 01:20:13.440
talking about autonomous vehicles or any kind of system, do you see that scaling? So we'll be able

01:20:13.440 --> 01:20:20.720
to simulate systems and hence be able to create a simulator that echoes our real world and proving

01:20:20.720 --> 01:20:23.840
once and for all, even though you're denying it that we're living in a simulation?

01:20:24.880 --> 01:20:28.240
I feel like I've used that for questions, right? So, you know, kind of at the core there of like,

01:20:28.240 --> 01:20:33.520
can we use simulation for self-driving cars? Take a look at our robotic system, Dackel,

01:20:33.520 --> 01:20:39.200
right? That was trained in simulation using the Dota system, in fact, and it transfers to a physical

01:20:39.200 --> 01:20:43.600
robot. And I think everyone looks at our Dota system, they're like, okay, it's just a game. How

01:20:43.600 --> 01:20:47.040
are you ever going to escape to the real world? And the answer is, well, we did it with the physical

01:20:47.040 --> 01:20:51.120
robot, the no-wink program. And so I think the answer is simulation goes a lot further than you

01:20:51.120 --> 01:20:55.680
think if you apply the right techniques to it. Now, there's a question of, you know, are the

01:20:55.680 --> 01:21:01.360
beings in that simulation going to wake up and have consciousness? I think that one seems a lot

01:21:01.360 --> 01:21:05.280
harder to again reason about. I think that, you know, you really should think about like,

01:21:05.280 --> 01:21:09.680
where exactly does human consciousness come from in our own self-awareness? And, you know,

01:21:09.680 --> 01:21:12.640
is it just that like, once you have like a complicated enough neural net, do you have to

01:21:12.640 --> 01:21:18.640
worry about the agent's feeling pain? And, you know, I think there's like interesting speculation

01:21:18.640 --> 01:21:22.960
to do there. But, you know, again, I think it's a little bit hard to know for sure.

01:21:22.960 --> 01:21:27.600
Well, let me just keep with the speculation. Do you think to create intelligence, general

01:21:27.600 --> 01:21:34.560
intelligence, you need one consciousness and two a body? Do you think any of those elements are

01:21:34.560 --> 01:21:38.400
needed or is intelligence something that's orthogonal to those?

01:21:38.400 --> 01:21:42.800
I'll stick to the kind of like the non-grand answer first, right? So the non-grand answer

01:21:42.800 --> 01:21:46.800
is just to look at, you know, what are we already making work? You look at GPT2,

01:21:46.800 --> 01:21:50.560
a lot of people would have said that to even get these kinds of results, you need real world

01:21:50.560 --> 01:21:54.480
experience. You need a body, you need grounding. How are you supposed to reason about any of these

01:21:54.480 --> 01:21:58.000
things? How are you supposed to like even kind of know about smoke and fire and those things

01:21:58.000 --> 01:22:03.040
if you've never experienced them? And GPT2 shows that you can actually go way further

01:22:03.040 --> 01:22:10.480
than that kind of reasoning would predict. So I think that in terms of do we need consciousness,

01:22:10.480 --> 01:22:13.920
do we need a body? It seems the answer is probably not, right? That we probably just

01:22:13.920 --> 01:22:19.520
continue to push kind of the systems we have. They already feel general. They're not as competent

01:22:19.520 --> 01:22:24.240
or as general or able to learn as quickly as an AGI would. But, you know, they're at least like

01:22:24.560 --> 01:22:31.440
proto-AGI in some way, and they don't need any of those things. Now, let's move to the grand

01:22:31.440 --> 01:22:37.680
answer, which is, you know, if our neural nets conscious already, would we ever know? How can

01:22:37.680 --> 01:22:44.880
we tell, right? Here's where the speculation starts to become, you know, at least interesting or fun

01:22:44.880 --> 01:22:49.680
and maybe a little bit disturbing, depending on where you take it. But it certainly seems that

01:22:49.680 --> 01:22:53.920
when we think about animals, that there's some continuum of consciousness. You know, my cat,

01:22:53.920 --> 01:22:58.960
I think, is conscious in some way, right? You know, not as conscious as a human. And you could

01:22:58.960 --> 01:23:02.160
imagine that you could build a little consciousness meter, right? You pointed a cat, it gives you

01:23:02.160 --> 01:23:06.960
a little reading, pointed a human, it gives you much bigger reading. What would happen if you

01:23:06.960 --> 01:23:12.000
pointed one of those at a Dota neural net? And if you're training in this massive simulation,

01:23:12.000 --> 01:23:19.120
do the neural nets feel pain? You know, it becomes pretty hard to know that the answer is no. And

01:23:19.120 --> 01:23:23.920
it becomes pretty hard to really think about what that would mean if the answer were yes.

01:23:25.360 --> 01:23:30.160
And it's very possible, you know, for example, you could imagine that maybe the reason these humans

01:23:30.160 --> 01:23:35.520
are have consciousness is because it's a convenient computational shortcut, right? If you think about

01:23:35.520 --> 01:23:40.080
it, if you have a being that wants to avoid pain, which seems pretty important to survive in this

01:23:40.080 --> 01:23:45.680
environment and wants to like, you know, eat food, then that maybe the best way of doing it is to

01:23:45.680 --> 01:23:49.520
have a being that's conscious, right? That, you know, in order to succeed in the environment,

01:23:49.520 --> 01:23:53.360
you need to have those properties and how are you supposed to implement them? And maybe this

01:23:53.360 --> 01:23:57.920
consciousness is a way of doing that. If that's true, then actually, maybe we should expect that

01:23:57.920 --> 01:24:02.480
really competent reinforcement learning agents will also have consciousness. But, you know,

01:24:02.480 --> 01:24:05.920
that's a big if and I think there are a lot of other arguments that you can make in other directions.

01:24:06.640 --> 01:24:11.440
I think that's a really interesting idea that even GPT2 has some degree of consciousness,

01:24:11.440 --> 01:24:16.000
that something is actually not as crazy to think about. It's useful to think about

01:24:17.040 --> 01:24:20.880
as we think about what it means to create intelligence of a dog, intelligence of a cat,

01:24:22.560 --> 01:24:31.120
and the intelligence of a human. So last question, do you think we will ever fall in love like in

01:24:31.120 --> 01:24:36.240
the movie, her with an artificial intelligence system, or an artificial intelligence system

01:24:36.320 --> 01:24:43.680
falling out in love with a human? I hope so. If there's any better way to end it is on love.

01:24:43.680 --> 01:24:46.480
So, Greg, thanks so much for talking today. Thank you for having me.

