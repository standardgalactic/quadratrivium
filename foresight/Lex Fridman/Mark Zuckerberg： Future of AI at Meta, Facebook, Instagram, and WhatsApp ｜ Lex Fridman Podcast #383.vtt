WEBVTT

00:00.000 --> 00:05.440
The following is a conversation with Mark Zuckerberg, his second time in this podcast.

00:05.440 --> 00:11.200
He's the CEO of Meta that owns Facebook, Instagram, and WhatsApp, all services

00:11.200 --> 00:14.080
used by billions of people to connect with each other.

00:14.720 --> 00:20.400
We talk about his vision for the future of Meta and the future of AI in our human world.

00:21.280 --> 00:26.880
This is the Lex Friedman podcast, and now, dear friends, here's Mark Zuckerberg.

00:27.840 --> 00:33.120
So you competed in your first jiu-jitsu tournament, and me as a fellow jiu-jitsu

00:33.120 --> 00:37.280
practitioner and competitor, I think that's really inspiring, given all the things you have going

00:37.280 --> 00:44.400
on. So I got to ask, what was that experience like? Oh, it was fun. I mean, I'm a pretty

00:44.400 --> 00:50.560
competitive person. Yeah. Doing sports that basically require your full attention, I think,

00:50.560 --> 00:56.160
is really important to my mental health and the way I just stay focused at doing everything I'm

00:56.160 --> 01:01.920
doing. So I decided to get into martial arts. And it's awesome. I got a ton of my friends into

01:01.920 --> 01:10.400
it. We all train together. We have a mini academy in my garage. And I guess one of my friends was

01:10.400 --> 01:14.400
like, hey, we should go do a tournament. I was like, okay, yeah, let's do it. I'm not going to

01:14.960 --> 01:19.600
shy away from a challenge like that. So yeah, but it was awesome. It was just a lot of fun.

01:19.600 --> 01:21.520
You weren't scared? There was no fear?

01:21.520 --> 01:25.040
I don't know. I was pretty sure that I'd do okay.

01:25.040 --> 01:30.160
I like the confidence. Well, so for people who don't know, jiu-jitsu is a martial art

01:30.160 --> 01:37.280
where you're trying to break your opponent's limbs or choke them to sleep and do so with

01:37.280 --> 01:44.880
grace and elegance and efficiency and all that kind of stuff. It's a kind of art form, I think,

01:44.880 --> 01:49.440
that you can do for your whole life. And it's basically a game, a sport of human chess, you

01:49.440 --> 01:54.640
can think of. There's a lot of strategy. There's a lot of interesting human dynamics of using

01:54.640 --> 02:00.000
leverage and all that kind of stuff. And it's kind of incredible what you could do. You could do

02:00.000 --> 02:04.400
things like a small opponent could defeat a much larger opponent. And you get to understand the

02:04.400 --> 02:09.760
way the mechanics of the human body works because of that. But you certainly can't be distracted.

02:09.760 --> 02:16.480
No, it's 100% focused. But to compete, I needed to get around the fact that I didn't want it to

02:16.480 --> 02:23.280
be like this big thing. So I basically just, I rolled up with a hat and sunglasses and I was

02:23.280 --> 02:29.200
wearing a COVID mask. And I registered under my first and middle name, so Mark Elliott. And it

02:29.200 --> 02:32.400
wasn't until I actually pulled all that stuff off right before I got on the mat that I think

02:32.400 --> 02:37.520
people knew it was me. So it was pretty low-key. But you're still a public figure.

02:37.520 --> 02:42.160
Yeah, I mean, I didn't want to lose. Right. The thing you're partially afraid of is not just

02:42.160 --> 02:47.120
the losing, but being almost like embarrassed. It's so raw, the sport, in that it's just you

02:47.120 --> 02:51.200
and another human being. There's a primal aspect there. Oh yeah, it's great. For a lot of people,

02:51.200 --> 02:54.720
it can be terrifying, especially the first time you're doing the common competing and you

02:54.720 --> 02:59.040
wasn't for you. I see the look of excitement in your face. Yeah, I don't know. No fear.

02:59.040 --> 03:04.160
I just think part of learning is failing. Okay. Right. So I mean, the main thing,

03:04.160 --> 03:08.960
like people who train jiu-jitsu, it's like you need to not have pride because I mean,

03:08.960 --> 03:14.560
all the stuff that you were talking about before about getting choked or getting a joint lock,

03:16.800 --> 03:20.800
you only get into a bad situation if you're not willing to tap once you've already lost.

03:21.440 --> 03:24.880
Right. But obviously, when you're getting started with something, you're not going to

03:24.880 --> 03:29.440
be an expert at it immediately. So you just need to be willing to go with that. But I think this

03:29.440 --> 03:34.080
is like, I don't know. I mean, maybe I've just been embarrassed enough times in my life. Yeah.

03:34.080 --> 03:38.400
I do think that there's a thing where like, as people grow up, maybe they don't want to be

03:38.400 --> 03:43.040
embarrassed or anything. They've built their adult identity and they kind of have a sense of

03:43.040 --> 03:48.960
who they are and what they want to project. And I don't know, I think maybe to some degree,

03:49.280 --> 03:56.320
you know, your ability to keep doing interesting things is your willingness to

03:56.960 --> 04:03.840
be embarrassed again and go back to step one and start as a beginner and get your ass kicked and,

04:04.560 --> 04:08.640
you know, look stupid doing things. And yeah, I think so many of the things that we're doing,

04:08.640 --> 04:12.560
whether it's, whether it's this, I mean, this is just like a kind of a physical

04:12.560 --> 04:17.920
part of my life, but running the company. It's like we just take on new adventures and

04:20.480 --> 04:23.600
you know, all the big things that we're doing, I think of as like 10 plus year

04:24.160 --> 04:28.560
missions that we're on where, you know, often early on, you know, people doubt that we're

04:28.560 --> 04:33.040
going to be able to do it. And the initial work seems kind of silly. And our whole ethos says we

04:33.040 --> 04:36.720
don't want to wait until something is perfect to put it out there. We want to get it out quickly

04:36.720 --> 04:40.400
and get feedback on it. And so I don't know. I mean, there's probably just something about

04:40.400 --> 04:44.560
how I approach things in there. But I just kind of think that the moment that you decide that

04:44.560 --> 04:47.680
you're going to be too embarrassed to try something new, then you're not going to learn anything

04:47.680 --> 04:53.280
anymore. But like I mentioned, that fear, that anxiety could be there, it could creep up every

04:53.280 --> 04:58.720
once in a while. Do you feel that in especially stressful moments sort of outside of, did you

04:58.720 --> 05:06.960
just, Matt, just in work, stressful moments, big decision days, big decision moments? How do you

05:06.960 --> 05:10.640
deal with that fear? How do you deal with that anxiety? The thing that stresses me out the most

05:10.640 --> 05:17.920
is always the people challenges. You know, I kind of think that, you know, strategy questions,

05:18.720 --> 05:24.080
you know, I tend to have enough conviction around the values of what we're trying to do and

05:25.040 --> 05:30.720
what I think matters and what I want our company to stand for, that those don't really keep me

05:30.720 --> 05:35.120
up at night that much. I mean, I kind of, you know, it's not that I get everything right. Of

05:35.120 --> 05:42.720
course I don't, right? I mean, we make a lot of mistakes. But I at least have a pretty strong

05:42.720 --> 05:49.360
sense of where I want us to go on that. The thing in running a company for almost 20 years now,

05:49.920 --> 05:54.800
one of the things that's been pretty clear is when you have a team that's cohesive,

05:55.760 --> 06:02.880
you can get almost anything done. And, you know, you can run through super hard challenges.

06:03.120 --> 06:09.680
You can make hard decisions and push really hard to do the best work even in kind of

06:10.240 --> 06:15.600
optimize something super well. But when there's that tension, I mean, that's when things get

06:15.600 --> 06:20.320
really tough. And, you know, when I talk to other friends who run other companies and things like

06:20.320 --> 06:24.080
that, I think one of the things that I actually spend a disproportionate amount of time on in

06:24.080 --> 06:31.280
running this company is just fostering a pretty tight core group of people who are running the

06:31.280 --> 06:39.440
company with me. And that to me is kind of the thing that both makes it fun, right? Having,

06:39.440 --> 06:43.440
you know, friends and people you've worked with for a while and new people and new perspectives,

06:43.440 --> 06:47.280
but like a pretty tight group who you can go work on some of these crazy things with.

06:48.880 --> 06:53.920
But to me, that's also the most stressful thing is when there's tension, you know,

06:53.920 --> 07:00.080
that weighs on me. I think it's maybe not surprising. I mean, we're like a very

07:00.080 --> 07:06.720
people focused company and it's the people is the part of it that weighs on me the most to

07:06.720 --> 07:10.880
make sure that we get right. But yeah, that I'd say across everything that we do is probably

07:11.920 --> 07:17.920
the big thing. So when there's tension in that inner circle of close folks, so

07:19.120 --> 07:29.120
when you trust those folks to help you make difficult decisions about Facebook, WhatsApp,

07:29.200 --> 07:35.040
Instagram, the future of the company and the metaverse with the AI, how do you build that

07:35.040 --> 07:41.120
close-knit group of folks to make those difficult decisions? Is there people that you have to

07:41.120 --> 07:47.200
have critical voices, very different perspectives on focusing on the past versus the future, all

07:47.200 --> 07:52.560
that kind of stuff? Yeah, I mean, I think for one thing, it's just spending a lot of time with

07:52.560 --> 07:57.920
whatever the group is that you want to be that core group grappling with all of the biggest

07:57.920 --> 08:03.280
challenges. And that requires a fair amount of openness. And, you know, so I mean, a lot of how

08:03.280 --> 08:08.240
I run the company is, you know, it's like every Monday morning, we get our, it's about the top

08:08.240 --> 08:14.240
30 people together. And we, and this is a group that just worked together for a long period of

08:14.240 --> 08:18.800
time. And I mean, people, people rotate in, I mean, we knew people join, people leave the company,

08:18.800 --> 08:24.160
people go do other roles in the company. So it's not the same group over time. But then we spend,

08:24.160 --> 08:28.400
you know, a lot of times a couple of hours, a lot of the time, it's, you know, it can be

08:28.400 --> 08:33.280
somewhat unstructured. We, like I'll come with maybe a few topics that I, that are top of mind

08:33.280 --> 08:38.400
for me, but I'll ask other people to bring things and people, you know, raise questions,

08:38.400 --> 08:44.160
whether it's okay, there's an issue happening in some country with, with some policy issue,

08:44.160 --> 08:48.240
there's like a new technology that's developing here, we're having an issue with this partner.

08:48.960 --> 08:54.080
You know, there's a design trade-off and WhatsApp between two things that, that end up

08:55.040 --> 08:59.040
being values that we care about deeply. And we need to kind of decide where we want to be

08:59.040 --> 09:04.080
on that. And I just think over time, when, you know, by working through a lot of issues with

09:04.080 --> 09:09.520
people and doing it openly, people develop an intuition for each other and a bond and camaraderie.

09:10.880 --> 09:16.800
And to me, developing that is, is like a lot of the fun part of running a company or doing

09:16.800 --> 09:20.960
anything, right? I think it's like having, having people who are kind of along on the journey that

09:20.960 --> 09:24.640
you're, that you feel like you're doing it with, nothing is ever just one person doing it.

09:24.640 --> 09:27.840
Are there people that disagree often within that group?

09:27.840 --> 09:29.680
It's a fairly combative group.

09:29.680 --> 09:35.120
Okay. So combat is part of it. So this is making decisions on design, engineering,

09:36.880 --> 09:38.480
policy, everything.

09:38.480 --> 09:40.400
Everything, everything. Yeah.

09:41.040 --> 09:44.960
I have to ask just back to Jujitsu for a little bit, what's your favorite submission?

09:44.960 --> 09:51.440
Now that you've been doing it, what's, how do you like to submit your opponent, Mark Zuckerberg?

09:51.440 --> 09:51.840
I mean,

09:53.520 --> 10:00.960
Well, but first of all, I, do you prefer no Gi or Gi Jujitsu? So Gi is this outfit you wear that

10:01.840 --> 10:05.680
is maybe mimics clothing so you can choke.

10:05.680 --> 10:08.800
It's like a kimono. It's like the traditional martial arts or kimono.

10:08.800 --> 10:09.200
Pajamas.

10:10.240 --> 10:10.880
Pajamas.

10:12.080 --> 10:13.840
That you can choke people with, yes.

10:13.840 --> 10:14.880
Well, it's got the lapels.

10:14.880 --> 10:15.360
Yes.

10:15.360 --> 10:21.040
Yeah. So I like Jujitsu. I also really like MMA.

10:21.040 --> 10:25.840
And so I think no Gi more closely approximates MMA.

10:25.840 --> 10:31.680
And I think my style is, is maybe a little closer to an MMA style.

10:31.680 --> 10:35.520
So like a lot of Jujitsu players are fine being on their back, right?

10:35.520 --> 10:39.360
And obviously having a good guard is, is, is a critical part of, of Jujitsu.

10:39.360 --> 10:41.760
But, but in MMA, you don't want to be on your back, right?

10:41.760 --> 10:45.680
Cause even if you have control, you're just taking punches while you're on your back.

10:45.680 --> 10:47.600
So, so that's no good.

10:47.600 --> 10:48.800
Do you like being on top?

10:48.800 --> 10:54.400
My, my style is I'm probably more pressure and, and yeah.

10:54.400 --> 10:59.680
And, and I'd probably rather be the top player, but, but I'm also smaller, right?

10:59.680 --> 11:02.240
I'm not, I'm not like a heavyweight guy, right?

11:02.240 --> 11:07.200
So from that perspective, I think like, you know, it's especially because, you know,

11:07.200 --> 11:09.680
from doing a competition, I'll compete with people are my size,

11:09.680 --> 11:11.680
but a lot of my friends are bigger than me.

11:11.680 --> 11:15.040
So, so back takes probably pretty important, right?

11:15.040 --> 11:17.520
Because that's where you have the most leverage advantage, right?

11:17.520 --> 11:22.480
Where, where, you know, people, you know, their arms, your arms are very weak behind you, right?

11:22.480 --> 11:26.480
So, so being able to get to the back and take that pretty, pretty important.

11:26.480 --> 11:31.040
But I don't know, I feel like the right strategy is to not be too committed to any single submission.

11:31.040 --> 11:32.960
But that said, I don't like hurting people.

11:32.960 --> 11:40.880
So, so I always think that chokes are, are a somewhat more human way to go than, than joint locks.

11:40.880 --> 11:44.080
Yeah. And it's more about control. It's less dynamic.

11:44.080 --> 11:47.600
So you're basically like a Habib Narmangamadoff type of fighter.

11:47.600 --> 11:48.560
So, so let's go. Yeah.

11:48.560 --> 11:50.000
Backtake to a rear naked choke.

11:50.000 --> 11:52.240
I think it's like the clean, the clean way to go.

11:52.240 --> 11:54.000
Straight forward answer right there.

11:54.000 --> 11:58.640
What advice would you give to, to people looking to start learning Jiu-Jitsu?

11:59.360 --> 12:04.160
Given how busy you are, given where you are in life, you're able to do this.

12:04.160 --> 12:09.840
You're able to train, you're able to compete and get to learn something from this interesting art.

12:10.560 --> 12:16.400
Why do you think you have to be willing to, to just get beaten up a lot?

12:16.400 --> 12:19.360
Yeah. I mean, it's, but, but I mean, over time, I think that there's,

12:19.360 --> 12:20.880
there's a flow to all these things.

12:20.880 --> 12:27.840
And there's, you know, one of the, one of, I don't know, my,

12:27.840 --> 12:32.320
my experiences that I think kind of transcends, you know, running a company and the different,

12:33.520 --> 12:36.880
different activities that I like doing are, I really believe that like,

12:36.880 --> 12:42.960
if you're going to accomplish whatever anything, a lot of it is just being willing to push through,

12:42.960 --> 12:48.320
right? And having the grit and determination to, to, to push through difficult situations.

12:48.880 --> 12:54.560
And I think for a lot of people that, that ends up being sort of a difference maker between the

12:54.560 --> 12:58.240
people, you know, who, who, who kind of get the most done and not.

12:58.240 --> 13:03.280
I mean, there's all these questions about like, you know, how, how many days people want to work

13:03.280 --> 13:06.800
and things like that. I think almost all the people who like start successful companies or

13:06.800 --> 13:10.480
things like that are just, are working extremely hard. But I think one of the things that you

13:10.480 --> 13:17.200
learn both by doing this over time or, you know, very acutely with things like jujitsu or surfing

13:17.200 --> 13:23.680
is, um, you can't push through everything. And I think that that's,

13:25.440 --> 13:30.240
you, you learn this stuff very acutely, you run, doing sports compared to running a company,

13:30.240 --> 13:34.320
because running a company, the cycle times are so long, right? It's like, you start a

13:35.040 --> 13:40.320
project and then, you know, it's like months later, or, you know, if you're building hardware,

13:40.320 --> 13:43.920
it could be years later before you're actually getting feedback and able to, you know, make

13:43.920 --> 13:47.440
the next set of decisions for the next version of the thing that you're doing. Whereas you,

13:47.440 --> 13:54.000
one of the things that I just think is mentally so nice about these very high turnaround conditioning

13:54.000 --> 13:58.160
sports, things like that is you get feedback very quickly, right? It's like, okay, like,

13:58.160 --> 14:02.000
I don't counter something correctly, you get punched in the face, right? So not in jujitsu,

14:02.000 --> 14:06.400
you don't, you don't get punched in jujitsu, but an MMA. There are all these analogies between

14:06.400 --> 14:12.080
all these things that I think actually hold that are, that are like important life lessons,

14:12.080 --> 14:17.920
right? It's like, okay, you're surfing a wave. It's like, you know, sometimes you're, like,

14:18.960 --> 14:23.440
you can't go in the other direction on it, right? It's like, there are limits to kind of what,

14:23.440 --> 14:28.560
you know, it's like a foil, you can, you can pump the foil and push pretty hard in a bunch of

14:28.560 --> 14:33.280
directions, but like, yeah, you, you know, it's at some level, like the momentum against you is

14:33.280 --> 14:40.560
strong enough, you're, that's not going to work. And I do think that that's sort of a humbling,

14:40.640 --> 14:45.840
but also an important lesson for, and I think people who are running things or building things,

14:45.840 --> 14:50.880
it's like, yeah, you, you know, a lot of the game is just being able to kind of push and,

14:52.000 --> 14:56.160
and work through complicated things, but you also need to kind of have enough

14:56.160 --> 14:59.280
of an understanding of like, which things you're, you just can't push through and where, where

15:00.640 --> 15:04.720
the finesse is more important. Yeah. What are your jujitsu life lessons?

15:05.200 --> 15:13.840
Well, I think you did it. You made it sound so simple and we're so eloquent that it's easy to

15:13.840 --> 15:23.600
miss, but basically being okay and accepting the wisdom and the joy in the, getting your ass kicked

15:24.560 --> 15:31.280
in the full range of what that means. I think that's a big gift of the being humbled. Somehow

15:31.280 --> 15:37.120
being humbled, especially physically opens your mind to the full process of learning,

15:37.120 --> 15:43.600
what it means to learn, which is being willing to suck at something. And I think jujitsu just very

15:44.240 --> 15:51.280
repetitively, efficiently humbles you over and over and over and over to where you can carry that

15:51.280 --> 15:56.960
lessons to places where you don't get humbled as much, whether it's research or running a company

15:56.960 --> 16:03.200
or building stuff, the cycle is longer. And jujitsu, you can just get humbled in a period of an hour

16:03.200 --> 16:06.960
over and over and over and over, especially when you're a beginner, you'll have a little person,

16:07.840 --> 16:16.240
just, you know, somebody much smarter than you, just kick your ass repeatedly, definitively,

16:16.240 --> 16:21.040
where there's no argument. Oh yeah. And then you literally tap because if you don't tap,

16:21.120 --> 16:26.800
you're going to die. So this is an agreement. You could have killed me just now, but we're

16:26.800 --> 16:31.200
friends. So we're going to agree that you're not going to. And that kind of humbling process,

16:31.200 --> 16:36.320
it just does something to your psyche, to your ego that puts it in its proper context to realize that,

16:37.040 --> 16:46.000
you know, everything in this life is like a journey from sucking through a hard process of

16:46.000 --> 16:53.120
improving rigorously day after day after day after day. Any kind of success requires hard work.

16:54.080 --> 16:58.480
Yeah, jujitsu, more than a lot of sports, I would say, because I've done a lot of them,

16:58.480 --> 17:03.680
really teaches you that. And you made it sound so simple. Like, I'm okay, you know, it's okay,

17:03.680 --> 17:06.320
it's part of the process. You just get humble, get your ass kicked.

17:06.320 --> 17:09.680
I've just failed and been embarrassed so many times in my life that like, you know,

17:10.400 --> 17:15.360
it's a core competence at this point. It's a core competence. Well, yes. And there's a deep

17:15.360 --> 17:20.240
truth to that being able to, and you said it in the very beginning, which is that's the thing

17:20.240 --> 17:24.560
that stops us, especially as you get older, especially as you develop expertise in certain

17:24.560 --> 17:33.520
areas, the not being willing to be a beginner in a new area. Because that's where the growth

17:33.520 --> 17:38.400
happens is being willing to be a beginner, being willing to be embarrassed, saying something stupid,

17:38.400 --> 17:43.840
doing something stupid. A lot of us that get good at one thing, you want to show that off.

17:43.920 --> 17:49.920
And it sucks being a beginner, but it's where growth happens.

17:50.640 --> 17:55.680
Yeah. Well, speaking of which, let me ask you about AI. It seems like this year,

17:56.240 --> 18:00.720
for the entirety of the human civilization, is an interesting year for the development of

18:00.720 --> 18:06.960
artificial intelligence. A lot of interesting stuff is happening. So meta is a big part of that.

18:07.280 --> 18:11.920
Meta has developed Lama, which is a 65 billion parameter model.

18:14.240 --> 18:18.240
There's a lot of interesting questions that can ask here, one of which has to do with open source.

18:18.960 --> 18:25.440
But first, can you tell the story of developing of this model and making the

18:26.080 --> 18:32.480
complicated decision of how to release it? Yeah, sure. I think you're right, first of all, that

18:32.480 --> 18:39.200
in the last year, there have been a bunch of advances on scaling up these large transformer

18:39.200 --> 18:44.080
models. So there's the language equivalent of it with large language models. There's sort of the

18:44.080 --> 18:50.160
image generation equivalent with these large diffusion models. There's a lot of fundamental

18:50.160 --> 19:00.160
research that's gone into this. And meta has taken the approach of being quite open and academic

19:00.880 --> 19:07.520
in our development of AI. Part of this is we want to have the best people in the world

19:07.520 --> 19:12.160
researching this. And a lot of the best people want to know that they're going to be able to

19:12.160 --> 19:19.200
share their work. So that's part of the deal that we have is that if you're one of the top

19:19.200 --> 19:25.040
AI researchers in the world and come here, you can get access to industry scale infrastructure.

19:25.440 --> 19:32.800
And part of our ethos is that we want to share what's invented broadly. We do that with a lot

19:32.800 --> 19:38.800
of the different AI tools that we create. And Lama is the language model that our research

19:38.800 --> 19:48.560
team made. And we did a limited open source release for it, which was intended for researchers to

19:48.640 --> 19:57.520
be able to use it. But responsibility and getting safety right on these is very important. So we

19:57.520 --> 20:02.400
didn't think that for the first one, there were a bunch of questions around whether we should

20:02.400 --> 20:08.960
be releasing this commercially. So we kind of punted on that for V1 of Lama and just released

20:08.960 --> 20:14.880
it from research. Now, obviously, by releasing it for research, it's out there. But companies

20:14.880 --> 20:20.720
know that they're not supposed to kind of put it into commercial releases. And we're working on

20:21.280 --> 20:28.480
the follow up models for this and thinking through how exactly this should work for follow

20:28.480 --> 20:33.840
on now that we've had time to work on a lot more of the safety and the pieces around that.

20:34.320 --> 20:38.320
But overall, I mean, this is, I just kind of think that

20:40.640 --> 20:47.120
that it would be good if there were a lot of different folks who had the ability

20:47.840 --> 20:56.160
to build state of the art technology here, and not just a small number of big companies,

20:56.160 --> 20:59.680
where to train one of these AI models, the state of the art models,

21:00.320 --> 21:06.160
is just takes hundreds of millions of dollars of infrastructure. So there are

21:06.960 --> 21:12.480
not that many organizations in the world that can do that at the biggest scale today.

21:13.040 --> 21:20.080
And now it gets more efficient every day. So I do think that that will be available to more

21:20.080 --> 21:24.880
folks over time. But I just think there's all this innovation out there that people can create.

21:25.440 --> 21:32.320
And I just think that we'll also learn a lot by seeing what the whole community of

21:32.320 --> 21:39.520
students and hackers and startups and different folks build with this. And that's kind of been

21:39.520 --> 21:43.520
how we've approached this. And it's also how we've done a lot of our infrastructure. And we took our

21:43.520 --> 21:48.160
whole data center design and our server design, and we built this open compute project where we

21:48.160 --> 21:52.560
just made that public. And part of the theory was like, all right, if we make it so that more

21:52.560 --> 21:58.720
people can use this server design, then that'll enable more innovation. And it'll also make the

21:58.720 --> 22:03.040
server design more efficient. And that'll make our business more efficient too. So that's worked.

22:03.040 --> 22:07.600
And we've just done this with a lot of our infrastructure. So for people who don't know,

22:07.600 --> 22:14.720
you did the limited release, I think in February of this year of Lama. And it got quote, unquote

22:14.800 --> 22:24.080
leaked. Meaning like, it escaped the limited release aspect. But it was,

22:25.440 --> 22:29.520
you know, that something you probably anticipated, given that it's just released to

22:29.520 --> 22:33.520
researchers. We shared it with researchers. Right. So it's just trying to make sure that

22:33.520 --> 22:39.040
there's like a slow release. Yeah. But from there, I just would love to get your comment

22:39.040 --> 22:42.960
on what happened next, which is like, there's a very vibrant open source community that just

22:42.960 --> 22:49.280
builds stuff on top of it. There's Lama CPP, basically stuff that makes it more efficient

22:49.280 --> 22:55.760
to run on smaller computers. There's combining with reinforcement learning with human feedbacks

22:55.760 --> 23:00.320
with some of the different interesting fine tuning mechanisms. There's then also like

23:00.320 --> 23:07.360
fine tuning in a GPT three generations. There's a lot of GPT for all, Alpaca, Colossal AI,

23:07.360 --> 23:12.640
all these kinds of models just kind of spring up like run on top of it. What do you think about

23:12.640 --> 23:17.200
that? No, I think it's been really neat to see. I mean, there's been folks who are getting it to

23:17.200 --> 23:24.240
run on local devices. So if you're an individual who just wants to experiment with this at home,

23:24.960 --> 23:29.360
you probably don't have a large budget to get access to like a large amount of cloud compute.

23:29.360 --> 23:35.840
So getting it to run on your local laptop is pretty good and pretty relevant.

23:36.160 --> 23:43.360
And then there were things like Lama CPP re-implemented it more efficiently. So now,

23:43.360 --> 23:48.000
even when we run our own versions of it, we can do it on way less compute and it just

23:48.000 --> 23:53.520
way more efficient, save a lot of money for everyone who uses this. So that is good.

23:55.680 --> 24:01.440
I do think it's worth calling out that because this was a relatively early release,

24:02.400 --> 24:11.280
Lama isn't quite as on the frontier as, for example, the biggest open AI models or the biggest

24:12.240 --> 24:17.280
Google models. I mean, you mentioned that the largest Lama model that we released had

24:17.280 --> 24:25.120
65 billion parameters. And when no one knows, I guess, outside of open AI exactly what the

24:25.120 --> 24:33.120
specs are for GPT-4. But I think my understanding is it's like 10 times bigger. And I think Google's

24:33.120 --> 24:38.080
Palm model is also, I think, has about 10 times as many parameters. Now, the Lama models are very

24:38.080 --> 24:43.040
efficient. So they perform well for something that's around 65 billion parameters. So for me,

24:43.040 --> 24:49.040
that was also part of this because there's this whole debate around, is it good for everyone in

24:49.040 --> 24:59.360
the world to have access to the most frontier AI models? And I think as the AI models start

24:59.360 --> 25:05.360
approaching something that's like a super human intelligence, that's a bigger question that

25:05.360 --> 25:11.040
we'll have to grapple with. But right now, I mean, these are still very basic tools. They're

25:12.640 --> 25:18.480
powerful in the sense that a lot of open source software like databases or web servers can enable

25:18.560 --> 25:27.280
a lot of pretty important things. But I don't think anyone looks at the current generation of

25:27.280 --> 25:31.520
Lama and thinks it's anywhere near a super intelligence. So I think that a bunch of those

25:31.520 --> 25:38.240
questions around like, is it good to get out there? I think at this stage, surely, you want

25:38.240 --> 25:43.840
more researchers working on it for all the reasons that open source software has a lot

25:43.840 --> 25:47.520
of advantages. And we talked about efficiency before, but another one is just open source

25:47.520 --> 25:52.000
software tends to be more secure because you have more people looking at it openly

25:52.000 --> 25:57.600
and scrutinizing it and finding holes in it. And that makes it more safe. So I think at this

25:57.600 --> 26:04.640
point, it's more, I think it's generally agreed upon that open source software is generally more

26:04.640 --> 26:09.760
secure and safer than things that are kind of developed in a silo where people try to get

26:09.760 --> 26:15.440
security through obscurity. So I think that for the scale of what we're seeing now with AI,

26:16.320 --> 26:22.640
I think we're more likely to get to good alignment and good understanding of kind

26:22.640 --> 26:26.720
of what needs to do to make this work well by having it be open source. And that's something

26:26.720 --> 26:30.880
that I think is quite good to have out there and happening publicly at this point.

26:30.880 --> 26:37.840
Meta released a lot of models as open source. So the massively multi-lingual speech model.

26:38.480 --> 26:39.120
Yeah, that was neat.

26:40.880 --> 26:45.760
I'll ask you questions about those, but the point is you've open sourced quite a lot.

26:45.760 --> 26:51.520
You've been spearheading the open source movement. That's really positive, inspiring to see from one

26:51.520 --> 26:56.560
angle, from the research angle. Of course, there's folks who are really terrified about the existential

26:56.560 --> 27:03.760
threat of artificial intelligence. And those folks will say that you have to be careful about

27:03.760 --> 27:10.400
the open sourcing step. But where do you see the future of open source here as part of Meta?

27:11.280 --> 27:18.800
The tension here is do you want to release the magic sauce? That's one tension. And the other one is

27:19.840 --> 27:26.560
do you want to put a powerful tool in the hands of bad actors, even though it probably has a huge

27:26.560 --> 27:31.440
amount of positive impact also? Yeah, I mean, again, I think for the stage that we're at in

27:31.440 --> 27:35.920
the development of AI, I don't think anyone looks at the current state of things and thinks that this

27:35.920 --> 27:44.800
is superintelligence. And the models that we're talking about, the Lama models here are generally

27:44.800 --> 27:50.160
an order of magnitude smaller than what open AI or Google are doing. So I think that at least

27:50.160 --> 27:56.560
for the stage that we're at now, the equities balance strongly in my view towards doing this

27:56.560 --> 28:03.680
more openly. I think if you got something that was closer to superintelligence, then I think you'd

28:03.680 --> 28:09.600
have to discuss that more and think through that a lot more. And we haven't made a decision yet as

28:09.600 --> 28:13.440
to what we would do if we were in that position. But I don't think I think there's a good chance

28:13.440 --> 28:22.480
that we're pretty far off from that position. So I'm certainly not saying that the position

28:22.480 --> 28:28.080
that we're taking on this now applies to every single thing that we would ever do. And certainly

28:28.080 --> 28:34.160
inside the company, we probably do more open source work than most of the other big tech companies.

28:34.160 --> 28:40.720
But we also don't open source everything. A lot of our the core kind of app code for WhatsApp or

28:40.720 --> 28:45.360
Instagram or something, we're not open sourcing that it's not like a general enough piece of

28:45.360 --> 28:51.680
software that would be useful for a lot of people to do different things. Whereas the

28:52.480 --> 28:55.920
software that we do, whether it's like an open source server design or

28:57.600 --> 29:03.120
basically things like memcache or a good, it was probably our earliest project

29:04.240 --> 29:09.120
that I worked on. It was probably one of the last things that I coded and led directly for the company.

29:10.720 --> 29:15.920
But basically this like caching tool for quick data retrieval,

29:17.280 --> 29:21.920
these are things that are just broadly useful across anything that you want to build.

29:22.480 --> 29:27.760
And I think that some of the language models now have that feel, as well as some of the other

29:27.760 --> 29:30.640
things that we're building, like the translation tool that you just referenced.

29:31.200 --> 29:36.560
So text to speech and speech to text, you've expanded it from around 100 languages to more

29:36.560 --> 29:41.600
than 1100 languages. And you can identify more than the model can identify more than

29:42.400 --> 29:46.720
4,000 spoken languages, which is 40 times more than any known previous technology.

29:47.440 --> 29:51.040
To me, that's really, really, really exciting in terms of connecting the world,

29:51.840 --> 29:54.080
breaking down barriers that language creates.

29:54.640 --> 29:59.040
Yeah, I think being able to translate between all of these different pieces in real time,

29:59.680 --> 30:07.840
this has been a kind of common sci-fi idea that we'd all have, whether it's

30:08.720 --> 30:12.480
I don't know, an earbud or glasses or something that can help translate in real time

30:12.720 --> 30:16.560
between all these different languages. And that's one that I think technology is

30:17.120 --> 30:21.840
basically delivering now. So yeah, I think that's pretty exciting.

30:22.800 --> 30:26.960
You mentioned the next version of Lama. What can you say about the next version of Lama?

30:28.400 --> 30:34.240
What can you say about what you're working on in terms of release, in terms of the vision for that?

30:34.960 --> 30:39.760
Well, a lot of what we're doing is taking the first version, which was primarily

30:40.640 --> 30:45.840
this research version, and trying to now build a version that has

30:47.360 --> 30:54.880
all of the latest state-of-the-art safety precautions built in. And we're using some

30:54.880 --> 31:02.480
more data to train it from across our services. But a lot of the work that we're doing internally

31:02.480 --> 31:10.720
is really just focused on making sure that this is as aligned and responsible as possible.

31:10.720 --> 31:15.840
And we're building a lot of our own. We're talking about kind of the open source infrastructure,

31:15.840 --> 31:21.360
but the main thing that we focus on building here, a lot of product experience is to help

31:21.360 --> 31:26.800
people connect and express themselves. I've talked about a bunch of this stuff, but

31:27.760 --> 31:34.160
then you'll have an assistant that you can talk to in WhatsApp. I think in the future,

31:34.160 --> 31:39.920
every creator will have kind of an AI agent that can kind of act on their behalf that their fans

31:39.920 --> 31:44.880
can talk to. I want to get to the point where every small business basically has an AI agent

31:45.680 --> 31:50.400
that people can talk to to do commerce and customer support and things like that. So there

31:50.400 --> 31:57.520
are going to be all these different things. And Lama, or the language model underlying this,

31:57.520 --> 32:01.920
is basically going to be the engine that powers that. The reason to open source it

32:01.920 --> 32:11.680
is that, as we did with the first version, is that it basically unlocks a lot of innovation

32:11.680 --> 32:17.200
in the ecosystem, will make our products better as well, and also gives us a lot of valuable

32:17.200 --> 32:21.760
feedback on security and safety, which is important for making this good. But yeah,

32:21.760 --> 32:27.920
I mean, the work that we're doing to advance the infrastructure, it's basically at this point taking

32:27.920 --> 32:33.600
it beyond a research project into something which is ready to be kind of core infrastructure,

32:33.600 --> 32:38.960
not only for our own products, but hopefully for a lot of other things out there too.

32:38.960 --> 32:45.200
Do you think the Lama or the language model underlying that version two will be open sourced?

32:47.840 --> 32:51.440
Do you have internal debate around that, the pros and cons and so on?

32:51.440 --> 32:54.400
This is, I mean, we were talking about the debates that we have internally, and I think

32:56.800 --> 33:03.600
the question is how to do it. I mean, I think we did the research license for V1,

33:03.600 --> 33:11.040
and I think the big thing that we're thinking about is basically what's the right way.

33:11.040 --> 33:15.040
So there was a leak that happened. I don't know if you can comment on it for V1.

33:15.680 --> 33:22.240
We released it as a research project for researchers to be able to use, but in doing so,

33:22.240 --> 33:28.640
we put it out there. So we were very clear that anyone who uses the code and the weights

33:28.640 --> 33:32.720
doesn't have a commercial license to put into products, and we've generally seen

33:32.720 --> 33:37.280
people respect that. It's like you don't have any reputable companies that are basically trying to

33:37.280 --> 33:43.840
put this into their commercial products, but by sharing it with so many researchers,

33:45.440 --> 33:50.480
it did leave the building. But what have you learned from that process that you might be able to

33:50.480 --> 33:57.440
apply to V2 about how to release it safely, effectively, if you release it?

33:57.440 --> 34:00.480
Yeah. Well, I mean, I think a lot of the feedback, like I said, is just around

34:01.440 --> 34:07.120
different things around how do you fine-tune models to make them more aligned and safer.

34:07.120 --> 34:12.640
And you see all the different data recipes that you mentioned, a lot of different

34:13.600 --> 34:17.840
projects that are based on this. I mean, there's one at Berkeley, there's just like all over.

34:20.000 --> 34:26.000
And people have tried a lot of different things, and we've tried a bunch of stuff internally. So

34:26.960 --> 34:31.600
we're making progress here, but also we're able to learn from some of the best ideas in

34:31.600 --> 34:37.440
the community. And I think we want to just continue pushing that forward. But I don't

34:37.440 --> 34:43.520
have any news to announce. If that's what you're asking. I mean, this is a thing that

34:46.480 --> 34:52.240
we're still actively working through the right way to move forward here.

34:52.240 --> 34:58.880
The details of the secret sauce are still being developed. I see. Can you comment on what do you

34:58.880 --> 35:04.320
think of the thing that worked for GPT, which is the reinforcement learning with human feedback? So

35:05.200 --> 35:10.000
doing this alignment process, do you find it interesting? And as part of that, let me ask,

35:10.000 --> 35:16.480
because I talked to Yanlacun before talking to you today, he asked me to ask or suggested that I ask,

35:16.480 --> 35:22.000
do you think LLM fine tuning will need to be crowdsourced Wikipedia style?

35:23.040 --> 35:30.960
So crowdsourcing. So this kind of idea of how to integrate the human in the fine tuning of

35:30.960 --> 35:36.720
these foundation models. Yeah, I think that's a really interesting idea that I've talked to Yan

35:36.720 --> 35:45.920
about a bunch. And we're talking about how do you basically train these models to be

35:46.720 --> 35:52.000
as safe and aligned and responsible as possible. And different groups out there who are doing

35:52.000 --> 36:00.160
development test different data recipes in fine tuning. But this idea that you just mentioned is

36:00.800 --> 36:07.520
that at the end of the day, instead of having kind of one group fine tune some stuff and another

36:07.520 --> 36:13.280
group produce a different fine tuning recipe, and then us trying to figure out which one we think

36:13.280 --> 36:22.000
works best to produce the most aligned model. I do think that it would be nice if you could get

36:22.000 --> 36:30.880
to a point where you had a Wikipedia style collaborative way for a kind of a broader

36:30.880 --> 36:37.760
community to fine tune it as well. Now, there's a lot of challenges in that, both from an infrastructure

36:38.640 --> 36:43.760
and like a community management and product perspective about how you do that. So I haven't

36:43.760 --> 36:50.160
worked that out yet. But as an idea, I think it's quite compelling. And I think it goes well with

36:50.240 --> 36:55.920
the ethos of open sourcing the technology is also finding a way to have a kind of community driven

36:58.720 --> 37:05.040
training of it. But I think that there are a lot of questions on this. In general, these questions

37:05.040 --> 37:12.480
around what's the best way to produce aligned AI models, it's very much a research area. And it's

37:12.480 --> 37:17.120
one that I think we will need to make as much progress on as the kind of core intelligence

37:17.200 --> 37:23.360
capability of the models themselves. Well, I just did a conversation with Jimmy Wales,

37:23.360 --> 37:28.960
the founder of Wikipedia. And to me, Wikipedia is one of the greatest websites ever created.

37:29.760 --> 37:33.520
And it's a kind of a miracle that it works. And I think it has to do with something that you

37:33.520 --> 37:39.760
mentioned, which is community. You have a small community of editors that somehow work together

37:39.760 --> 37:46.480
well, and they they handle very controversial topics, and they handle it

37:47.360 --> 37:51.760
with balance and with grace, despite sort of the attacks that will often happen.

37:51.760 --> 37:56.240
A lot of the time. I mean, it's not it's it has issues just like any other human system.

37:56.240 --> 38:01.360
But yes, I mean, the balance is, I mean, it's it's amazing what they've been able to achieve.

38:01.360 --> 38:06.640
But it's also not perfect. And I think that that's there's still a lot of challenges.

38:07.600 --> 38:15.360
Right, it's the more controversial the topic, the more the more difficult the journey towards

38:15.360 --> 38:20.480
quote unquote, truth, or knowledge or wisdom that Wikipedia tries to capture. In the same way,

38:20.480 --> 38:26.720
AI models, we need to be able to generate those same things, truth, knowledge, and wisdom.

38:26.720 --> 38:35.760
And how do you align those models that they generate something that is closest to truth?

38:35.760 --> 38:41.120
There's these concerns about misinformation, all this kind of stuff that nobody can define.

38:42.320 --> 38:48.080
And this is something that we together as a human species have to define, like what is truth,

38:48.080 --> 38:53.040
and how to help AI systems generate that. One of the things language models do really well

38:53.040 --> 39:00.640
is generate convincing sounding things that can be completely wrong. And so how do you align it

39:00.880 --> 39:08.000
to be less wrong? And part of that is the training and part of that is the alignment.

39:08.560 --> 39:14.880
And however you do the alignment stage, and just like you said, it's a very new and a very open

39:14.880 --> 39:21.920
research problem. Yeah, and I think that there's also a lot of questions about whether the current

39:21.920 --> 39:31.920
architecture for LLMs, as you continue scaling it, what happens? A lot of what's been exciting in

39:31.920 --> 39:38.400
the last year is that there's clearly a qualitative breakthrough where with some of the GPT models

39:39.200 --> 39:45.200
that I put out and that others have been able to do as well, I think it reached a kind of level

39:45.280 --> 39:52.000
of quality where people are like, wow, this feels different and it's going to be able to

39:52.000 --> 39:57.200
be the foundation for building a lot of awesome products and experiences and value. But I think

39:57.200 --> 40:04.000
that the other realization that people have is, wow, we just made a breakthrough. If there are

40:04.000 --> 40:09.200
other breakthroughs quickly, then I think that there's the sense that maybe we're closer to

40:10.080 --> 40:13.520
general intelligence. But I think that that idea is predicated on the idea that

40:14.720 --> 40:18.800
I think people believe that there's still generally a bunch of additional breakthroughs to make and

40:18.800 --> 40:24.480
that we just don't know how long it's going to take to get there. And one view that some people

40:24.480 --> 40:32.560
have, this doesn't tend to be my view as much, is that simply scaling the current LLMs and

40:32.560 --> 40:37.600
getting to higher parameter count models by itself will get to something that is closer to

40:39.200 --> 40:44.160
general intelligence. But I don't know, I tend to think that there's probably more

40:46.960 --> 40:49.920
fundamental steps that need to be taken along the way there.

40:49.920 --> 40:56.400
But still the leaves taken with this extra alignment step is quite incredible,

40:56.400 --> 41:01.440
quite surprising to a lot of folks. And on top of that, when you start to have

41:02.240 --> 41:06.160
hundreds of millions of people potentially using a product that integrates that,

41:07.040 --> 41:10.720
you can start to see civilization transforming effects

41:10.720 --> 41:17.360
before you achieve super, quote unquote, super intelligence. It could be super transformative

41:17.920 --> 41:21.280
without being a super intelligence. Oh yeah, I mean, I think that

41:21.920 --> 41:26.480
there are going to be a lot of amazing products and value that can be created with the current

41:26.480 --> 41:35.120
level of technology. To some degree, I'm excited to work on a lot of those products over the next

41:35.120 --> 41:40.560
few years. And I think it would just create a tremendous amount of whiplash if the number

41:40.560 --> 41:44.480
of breakthroughs keeps, if they're keep on being stacked breakthroughs, because I think to some

41:44.480 --> 41:50.160
degree, industry in the world needs some time to kind of build these breakthroughs into the

41:50.160 --> 41:57.920
products and experiences that we all use that we can actually benefit from them. But I don't know,

41:57.920 --> 42:03.360
I think that there's just a like an awesome amount of stuff to do. I mean, I think about

42:03.440 --> 42:08.320
like all of the small businesses or individual entrepreneurs out there who

42:10.400 --> 42:15.280
now we're going to be able to get help coding the things that they need to go build things or

42:15.280 --> 42:20.960
designing the things that they need, or we'll be able to use these models to be able to do customer

42:20.960 --> 42:27.840
support for the people that they're serving over WhatsApp without having to, I think that's just

42:27.840 --> 42:32.640
going to be, I just think that this is all going to be super exciting. It's going to create better

42:33.680 --> 42:36.960
experiences for people and just unlock a ton of innovation and value.

42:37.600 --> 42:44.480
So I don't know if you know, but what is it? Over 3 billion people use WhatsApp, Facebook,

42:44.480 --> 42:53.200
and Instagram. So any kind of AI fueled products that go into that, like we're talking about anything

42:53.200 --> 42:58.640
with LLMs will have a tremendous amount of impact. Do you have ideas and thoughts about

42:59.600 --> 43:08.160
the possible products that might start being integrated into these platforms used by so many

43:08.160 --> 43:12.880
people? Yeah, I think there's three main categories of things that we're working on.

43:17.200 --> 43:23.280
The first that I think is probably the most interesting is,

43:23.520 --> 43:29.200
you know, there's this notion of like, you're going to have an assistant

43:29.200 --> 43:34.720
or an agent who you can talk to. And I think probably the biggest thing that's different about

43:34.720 --> 43:40.880
my view of how this plays out from what I see with open AI and Google and others is,

43:40.880 --> 43:46.480
you know, everyone else is building like the one singular AI where it's like, okay, you talk to

43:46.560 --> 43:52.560
ChatGPT or you talk to Bard or you talk to Bing. And my view is that

43:55.360 --> 43:59.680
that there are going to be a lot of different AIs that people are going to want to engage with,

43:59.680 --> 44:04.720
just like you want to use, you know, a number of different apps for different things and you

44:04.720 --> 44:08.240
have relationships with different people in your life who fill different emotional

44:08.800 --> 44:18.240
roles for you. And so I think that they're going to be, people have a reason that I think you

44:18.240 --> 44:23.840
don't just want like a singular AI. And that I think is probably the biggest distinction in

44:23.840 --> 44:28.400
terms of how I think about this. And a bunch of these things I think you'll want an assistant.

44:29.760 --> 44:33.280
I mentioned a couple of these before. I think like every creator who you interact with

44:33.280 --> 44:39.440
will ultimately want some kind of AI that can proxy them and be something that their fans

44:39.440 --> 44:46.000
can interact with or that allows them to interact with their fans. This is like the common creator

44:46.000 --> 44:50.240
promise. Everyone's trying to build a community and engage with people and they want tools to

44:50.240 --> 44:57.680
be able to amplify themselves more and be able to do that. But you only have 24 hours in a day.

44:57.680 --> 45:06.720
So I think having the ability to basically like bottle up your personality and or, you know,

45:06.720 --> 45:10.800
like give your fans information about when you're performing a concert or something like that. I

45:10.800 --> 45:14.240
mean, that's, that I think is going to be something that's super valuable, but it's not just that,

45:14.800 --> 45:18.560
you know, again, it's not this idea that I think people are going to want just one singular AI.

45:18.560 --> 45:22.320
I think you're going to, you know, you're going to want to interact with a lot of different entities.

45:22.320 --> 45:25.280
And then I think there's the business version of this too, which we've touched on a couple of

45:25.280 --> 45:32.800
times, which is I think every business in the world is going to want basically an AI that,

45:34.240 --> 45:38.640
that, you know, it's like you have your page on Instagram or Facebook or WhatsApp or whatever.

45:38.640 --> 45:44.080
And you want to, you want to point people to an AI that people can interact with, but you want to

45:44.080 --> 45:47.600
know that that AI is only going to sell your products. You don't want it, you know, recommending

45:47.600 --> 45:52.800
your competitors stuff. Right. So, so it's not like there can be like just a, you know, one singular

45:52.800 --> 45:58.320
AI that, that can answer all the questions for a person because, you know, that, like that AI

45:58.320 --> 46:03.120
might not actually be aligned with you as a business to, to really just do the best job

46:03.120 --> 46:07.760
providing support for, for your product. So I think that there's going to be a clear need

46:09.280 --> 46:13.520
in the market and in people's lives for there to be a bunch of these.

46:14.080 --> 46:20.080
Part of that is figuring out the research, the technology that enables the personalization

46:20.080 --> 46:27.520
that you're talking about. So not one centralized God-like LLM, but one just a huge diversity of

46:27.520 --> 46:33.360
them that's fine tuned to particular needs, particular styles, particular businesses,

46:33.360 --> 46:38.080
particular brands, all that kind of stuff. And also enabling, just enabling people to create them

46:38.080 --> 46:43.760
really easily for the, you know, for, for your own business or if you're a creator to be able to

46:43.760 --> 46:50.640
help you engage with your fans. And I think that's, so yeah, I think that there, there's a clear kind

46:50.640 --> 46:56.800
of interesting product direction here that I think is fairly unique from, from what, you know,

46:56.800 --> 47:01.600
any of the other big companies are, are taking. It also aligns well with this sort of open source

47:01.600 --> 47:07.680
approach. Because again, we, we sort of believe in this more community oriented, more democratic

47:07.680 --> 47:11.680
approach to building out the products and technology around this. We don't think that there's going

47:11.680 --> 47:15.120
to be the one true thing. We think that there, there should be kind of a lot of development.

47:16.000 --> 47:19.840
So that part of things I think is going to be really interesting. And we could, we could go

47:20.480 --> 47:25.360
price about a lot of time talking about that and the, the kind of implications of, of that

47:26.000 --> 47:30.640
approach being different from what others are taking. But then there's a bunch of other simpler

47:30.640 --> 47:33.520
things that I think we're also going to do, just going back to your, your question around

47:34.400 --> 47:39.360
how this finds its way into like, what do we build? There are going to be a lot of simpler

47:39.360 --> 47:48.160
things around, okay, you, you post photos on Instagram and Facebook and, you know,

47:48.160 --> 47:52.000
and WhatsApp and Messenger and like, you want the photos to look as good as possible. So like

47:52.000 --> 47:56.800
having an AI that you can just like take a photo and then just tell it like, okay, I want to edit

47:56.800 --> 48:00.000
this thing or describe this. It's like, I think we're, we're going to have tools that are just

48:00.640 --> 48:05.920
way better than, than what we've historically had on this. And that's more in the image and

48:05.920 --> 48:10.400
media generation side than the large language model side, but, but it's, it all kind of,

48:10.400 --> 48:15.840
you know, plays off of advances in the same space. So there are a lot of tools that I think are just

48:15.840 --> 48:19.920
going to get built into every one of our products. I think every single thing that we do is going to

48:20.480 --> 48:23.120
basically get evolved in this direction, right? It's like in the future,

48:23.760 --> 48:29.760
if you're advertising on our services, like, do you need to make your own kind of ad creative?

48:29.760 --> 48:35.840
It's, no, you'll just, you know, you just tell us, okay, I'm, I'm a dog walker and I,

48:36.960 --> 48:42.640
you know, I'm willing to walk people's dogs and help me find the right people and like

48:43.440 --> 48:47.920
create the ad unit that will perform the best and like give an objective to,

48:47.920 --> 48:52.240
to the system and it just kind of like connects you with the right people.

48:52.240 --> 49:00.880
Well, that's a super powerful idea of generating the language almost like rigorous AB testing

49:01.680 --> 49:09.200
for you that works to find the best customer for your thing. I mean, to me, advertisement

49:09.200 --> 49:16.160
when done well just finds a good match between a human being and a thing that will make that human

49:16.160 --> 49:21.920
being happy. Yeah, totally. And do that as efficiently as possible. When it's done well,

49:21.920 --> 49:25.600
people actually like it, you know, it's, you know, I think that there's a lot of examples

49:25.600 --> 49:29.600
where it's not done well and it's annoying and I think that that's what kind of gives it a bad

49:29.600 --> 49:34.560
wrap. But, but yeah, and a lot of this stuff is possible today. I mean, obviously, AB testing

49:34.560 --> 49:39.440
stuff is built into a lot of these frameworks. The thing that's new is having technology that

49:39.440 --> 49:43.680
can generate the ideas for you about what to AB test something that that's exciting. So this

49:43.680 --> 49:47.760
will just be across like everything that we're doing, where all the metaverse stuff that we're

49:47.760 --> 49:51.600
doing, right? It's like you want to create worlds in the future, you'll just describe them and

49:51.920 --> 49:57.600
it'll create the code for you. So natural language becomes the interface we use for

49:58.720 --> 50:05.680
all the ways we interact with the computer with with the digital more of them. Yeah, yeah, totally.

50:05.680 --> 50:09.600
Yeah, which is what everyone can do using natural language and with translation,

50:09.600 --> 50:15.200
you can do it any kind of language. I mean, for the personalization is really,

50:15.280 --> 50:21.520
really, really interesting. Yeah, it unlocks so many possible things. I mean, I for one,

50:21.520 --> 50:26.080
look forward to creating a copy of myself. I know we talked about this last time,

50:26.080 --> 50:30.320
but this has since the last time, this becomes now we're closer,

50:31.600 --> 50:36.000
much closer, like I could literally just having interacted with some of these language models,

50:36.000 --> 50:44.720
I can see the absurd situation where I'll have a large or a lex language model,

50:44.720 --> 50:49.040
and I'll have to have a conversation with him about like, hey, listen,

50:49.920 --> 50:53.600
like you're just getting out of line and having a conversation where you fine tune that thing to

50:53.600 --> 50:58.800
be a little bit more respectful or something like this. And yeah, that's, that's going to be the,

50:59.920 --> 51:09.360
that seems like an amazing product for businesses, for humans, just not, not just the assistant

51:09.600 --> 51:14.880
facing the individual, but the assistant that represents the individual to the public,

51:15.520 --> 51:23.440
both directions. There's basically a layer that is the AI system through which you interact

51:24.160 --> 51:29.440
with the outside world, with the outside world that has humans in it. That's really interesting.

51:30.000 --> 51:36.320
And you that have social networks that connect billions of people, it seems like a

51:37.280 --> 51:41.200
heck of a large scale place to test some of this stuff out.

51:42.160 --> 51:46.080
Yeah, I mean, I think part of the reason why creators will want to do this is because they

51:46.080 --> 51:51.920
already have the communities on our services. Yeah. And, and, and a lot of the interface for

51:51.920 --> 51:58.560
this stuff today are chat type interfaces. And, and between WhatsApp and, and Messenger,

51:58.560 --> 52:03.920
I think that those are, you know, just great, great ways to, to interact with people. So some

52:03.920 --> 52:09.440
of this is philosophy, but do you see, do you see a near-term future where you have some of the

52:09.440 --> 52:17.280
people you're friends with are AI systems on these social networks, on Facebook, on Instagram,

52:17.280 --> 52:23.440
even, even on WhatsApp, having, having conversations where some heterogeneous,

52:23.440 --> 52:29.600
some human, some is AI. I think we'll get to that. You know, and, and, you know, if only just

52:29.600 --> 52:36.560
empirically looking at, then Microsoft released this thing called Showice several years ago,

52:37.360 --> 52:43.040
and, and, and China, and it was a pre-LLM chatbot technology that, so it was a lot simpler

52:43.920 --> 52:48.720
than what's possible today. And, and I think it was like tens of millions of people were using

52:48.720 --> 52:54.480
this and, and just, you know, really, it became quite attached and built relationships with it.

52:54.720 --> 52:59.840
I think that there's, you know, there's services today like replica where, you know,

52:59.840 --> 53:05.040
people are doing things like that. And, so I think that there's, there's certainly,

53:06.080 --> 53:10.560
you know, needs for companionship that people have, you know, older people.

53:13.280 --> 53:17.680
And it's, I think most people probably don't have as many friends as they would like to have,

53:17.680 --> 53:21.840
right? If you look at, there's some interesting demographic studies around,

53:22.800 --> 53:30.080
that like the average person has the number of close friends that they have is fewer today

53:30.080 --> 53:35.920
than it was 15 years ago. And I mean, that gets to like, this is like the core thing that,

53:37.120 --> 53:40.640
that I think about in terms of, you know, building services that help connect people.

53:40.640 --> 53:46.080
So I think you'll get tools that help people connect with each other are going to be,

53:46.080 --> 53:50.640
you know, the primary thing that we want to do. So you can imagine, you know, AI

53:51.280 --> 53:55.120
assistants that, you know, just do a better job of reminding you when it's your friend's birthday

53:55.120 --> 53:59.120
and how you can celebrate them, right? It's like, right now we have like the little box in the corner

53:59.120 --> 54:04.720
of the website that tells you whose birthday it is and stuff like that. But it's, but, you know,

54:04.720 --> 54:08.320
it's some level, you don't want to just want to like, send everyone a note that's the same

54:08.320 --> 54:13.520
note saying happy birthday with an emoji, right? So having something that's more of an,

54:13.520 --> 54:18.560
you know, a social assistant in that sense. And like, that can, you know, update you on

54:18.560 --> 54:22.880
what's going on in their life and like, how, how you can reach out to them effectively,

54:23.680 --> 54:26.640
help you be a better friend. I think that that's something that's super powerful too.

54:28.080 --> 54:37.040
But yeah, beyond that, and there are all these different flavors of kind of personal AIs that

54:37.040 --> 54:41.680
I think could exist. So I think an assistant is sort of the, the kind of simplest one to wrap

54:41.680 --> 54:49.360
your head around. But I think a mentor or a life coach, you know, someone who can give you advice,

54:50.160 --> 54:54.320
who's maybe like a bit of a cheerleader who can help pick you up through all the challenges that,

54:54.320 --> 54:58.560
that, you know, inevitably, you know, we all go through on a daily basis. And that there's

54:58.560 --> 55:03.280
probably, you know, some, some role for something like that. And then, you know, all the way you

55:03.280 --> 55:06.800
can, you can probably just go through a lot of the different type of kind of functional

55:07.520 --> 55:11.680
relationships that people have in their life. And, you know, I would, I would bet that there

55:11.680 --> 55:17.040
will be companies out there that take a crack at, at a lot of these things. So I don't know,

55:17.040 --> 55:20.880
I think it's part of the interesting innovation that's going to exist is, is that they're,

55:20.880 --> 55:26.320
they're certainly a lot like education tutors, right? It's like, I mean, I just look at, you know,

55:26.320 --> 55:32.800
my kids learning to code and, you know, they love it. But, you know, it's like they get stuck on a

55:32.800 --> 55:37.440
question and they have to wait till like, I can help answer it, right? Or someone else who, who

55:37.440 --> 55:41.040
they know can help help answer the question in the future. They'll just, there will be like a

55:41.040 --> 55:46.080
coding assistant that they have that is like designed to, you know, be perfect for teaching

55:46.080 --> 55:50.800
a five and a seven year old how to code. And, and they'll just be able to ask questions all the time.

55:50.800 --> 55:54.880
And, you know, it will be extremely patient. It's never going to get annoyed at them, right?

55:56.480 --> 56:00.160
I think that like, there are all these different kind of relationships or functional

56:00.160 --> 56:06.320
relationships that we have in our lives that, that are really interesting. And I think one of

56:06.320 --> 56:11.840
the big questions is like, okay, is this all going to just get bucketed into, you know, one singular

56:11.840 --> 56:16.880
AI? I just, I just don't, I don't think so. Do you think about, let's actually question from Reddit,

56:18.080 --> 56:24.880
what the long-term effects of human communication when people can talk with, in quotes, talk with

56:24.880 --> 56:29.600
others through a chatbot that augments their language automatically, rather than developing

56:29.600 --> 56:35.760
social skills by making mistakes and learning? Will people just communicate by grunts in a

56:35.760 --> 56:41.840
generation? I mean, do you think about long-term effects at scale, the integration of AI in our

56:41.840 --> 56:48.480
social interaction? Yeah, I mean, I think it's mostly good. I mean, that was, that question was

56:48.480 --> 56:53.360
sort of framed in a negative way. But I mean, we were talking before about language models

56:53.360 --> 56:57.040
helping you communicate with, it was like language translation, helping you communicate with people

56:57.040 --> 57:03.200
who don't speak your language. I mean, at some level, what all this social technology is doing

57:03.200 --> 57:13.120
is helping people express themselves better to people in situations where they would otherwise

57:13.120 --> 57:16.960
have a hard time doing that. So part of it might be, okay, because you speak a language that I

57:16.960 --> 57:20.560
don't know, that's a pretty basic one that, you know, I don't think people are going to look at

57:20.560 --> 57:25.200
that and say, it's sad that do we have the capacity to do that because I should have just learned

57:25.200 --> 57:30.800
your language, right? I mean, that's, that's pretty high bar. But overall, I'd say

57:33.280 --> 57:40.800
there are all these impediments and language is an imperfect way for people to express thoughts

57:40.800 --> 57:45.520
and ideas. It's, you know, one of the best that we have, we have that, we have art, we have code.

57:46.400 --> 57:51.280
But language is also a mapping of the way you think, the way you see the world, who you are.

57:51.600 --> 57:57.280
One of the applications I've recently talked to a person who, who's a, actually a jiu-jitsu

57:57.280 --> 58:05.520
instructor, he said that when he emails parents about their son and daughter,

58:07.360 --> 58:12.720
that they can improve their discipline in class and so on, he often finds that he's comes off a

58:12.720 --> 58:18.800
bit of more of an asshole than he would like. So he uses GPT to translate his original email

58:18.800 --> 58:24.640
into a nicer email. We hear this all the time. We hear this all the time. A lot of creators

58:24.640 --> 58:31.120
on our services tell us that one of the most stressful things is basically negotiating deals

58:31.120 --> 58:35.280
with brands and stuff, like the business side of it, because they're like, I mean, they do their

58:35.280 --> 58:38.880
thing, right? And, and, you know, the creators, they're, they're excellent at what they do and

58:38.880 --> 58:41.920
they just want to connect with their community. But then they get really stressed, you know,

58:41.920 --> 58:46.880
they go into their, their DMs and they see some brand wants to do something with them and they

58:47.840 --> 58:53.600
don't quite know how to negotiate or how to push back respectfully. And so I think building a tool

58:53.600 --> 58:58.880
that can actually allow them to do that well is, you know, one simple thing that, that I think is

58:58.880 --> 59:02.640
just like an interesting thing that, that we've heard from a bunch of people that, that they'd

59:02.640 --> 59:10.960
be interested in. But I'm going back to the broader idea. I don't know. I mean, you know,

59:10.960 --> 59:19.040
I just, Priscilla and I just had our third daughter. And, you know, it's like one of the

59:19.040 --> 59:24.000
saddest things in the world is like seeing your baby cry, right? But like, it's like, what,

59:24.000 --> 59:30.080
why is that, right? It's like, well, because babies don't generally have much capacity to tell you

59:30.080 --> 59:36.480
what they care about otherwise, right? It's not actually just babies, right? It's, you know,

59:36.480 --> 59:41.200
my five year old daughter cries too, because she sometimes has a hard time expressing, you know,

59:41.200 --> 59:46.320
what, what matters to, to her. And, and I was thinking about that. And it's like, well, you

59:46.320 --> 59:49.920
know, actually a lot of adults get very frustrated too, because they can't, they have a hard time

59:49.920 --> 59:57.920
expressing things in a way that going back to some of the early themes that maybe is something that,

59:57.920 --> 01:00:01.360
you know, is a mistake or maybe they have pride or something like all these things get in the way.

01:00:01.360 --> 01:00:06.880
So I don't know. I think that all of these different technologies that can help us navigate

01:00:06.880 --> 01:00:12.240
the social complexity and actually be able to better express our, what we're feeling and thinking,

01:00:12.960 --> 01:00:18.480
I think that's generally all good. And there are all these, these concerns like, okay, are people

01:00:18.480 --> 01:00:22.480
going to have worse memories because you have Google to look things up. And, and I think in

01:00:22.480 --> 01:00:27.760
general, a generation later, you don't look back and lament that I think it's, you know, just like,

01:00:27.840 --> 01:00:32.000
wow, we have so much more capacity to do so much more now. And I think that that'll be the case

01:00:32.000 --> 01:00:38.880
here too. You can allocate those cognitive capabilities to like deeper, more nuanced thought.

01:00:38.880 --> 01:00:49.520
Yeah. But it's change. So with, with, just like with Google search, the, the additional language

01:00:49.520 --> 01:00:56.320
models, large language models, you basically don't have to remember nearly as much just like with

01:00:56.320 --> 01:01:01.120
stack overflow for programming. Now that these language models can generate code right there.

01:01:01.120 --> 01:01:08.720
I mean, I find that I write like maybe 80%, 90% of the code I write is, is a non-generated first

01:01:08.720 --> 01:01:13.120
and then edited. I mean, so you don't have to remember how to write specifics of different

01:01:13.120 --> 01:01:20.000
functions. Oh, that's great. And it's also, it's not just the, the specific coding. I mean, in the,

01:01:20.000 --> 01:01:25.200
in the context of a, of a large company like this, I think before an engineer can sit down to code,

01:01:26.080 --> 01:01:32.320
they first need to figure out all of the libraries and dependencies that, you know, tens of thousands

01:01:32.320 --> 01:01:38.720
of people have written before them. And, you know, one of the things that I'm excited about

01:01:39.440 --> 01:01:44.240
that we're working on is it's not just, you know, tools that help engineers code, it's tools that

01:01:44.240 --> 01:01:49.040
can help summarize the whole knowledge base and, and help people be able to navigate all the internal

01:01:49.040 --> 01:01:54.080
information. And I think that that's, in the experiments that I've done with this stuff,

01:01:54.080 --> 01:02:01.680
I mean, that's, on the public stuff, you just, you know, ask, ask one of these models to, you know,

01:02:01.680 --> 01:02:05.680
build you a script that does anything and it basically already understands what the best

01:02:05.680 --> 01:02:09.360
libraries are to do that thing and pulls them in automatically. It's, I mean, I think that's super

01:02:09.360 --> 01:02:14.560
powerful. That was always the most annoying part of coding was that you had to spend all this time

01:02:14.560 --> 01:02:17.920
actually figuring out what the resources were that you were supposed to import before you could

01:02:17.920 --> 01:02:23.520
actually start building the thing. Yeah. I mean, there's, of course, the flip side of that, I think

01:02:23.520 --> 01:02:31.040
for the most part is positive, but the flip side is if you outsource that thinking to an AI model,

01:02:31.920 --> 01:02:39.200
you might miss nuanced mistakes and bugs. You lose the skill to find those bugs.

01:02:40.000 --> 01:02:46.320
And those bugs might be, the code looks very convincingly right, but it's actually wrong in

01:02:46.320 --> 01:02:55.680
a very subtle way. But that's the trade-off that we, that we face as human civilization when we

01:02:55.680 --> 01:03:01.280
build more and more powerful tools. When we stand on the shoulders of taller and taller giants,

01:03:01.840 --> 01:03:05.200
we could do more, but then we forget how to do all the stuff that they did.

01:03:07.360 --> 01:03:12.720
It's a weird trade-off. Yeah, I agree. I mean, I think it's, I think it is very valuable in your

01:03:12.720 --> 01:03:20.880
life to be able to do basic things too. Do you worry about some of the concerns of bots being

01:03:20.880 --> 01:03:28.800
present on social networks, more and more human-like bots that are not necessarily trying to do a good

01:03:28.800 --> 01:03:35.120
thing or they might be explicitly trying to do a bad thing like fishing scams, like social engineering,

01:03:35.120 --> 01:03:39.760
all that kind of stuff, which has always been a very difficult problem for social networks,

01:03:39.760 --> 01:03:42.400
but now it's becoming almost a more and more difficult problem.

01:03:42.960 --> 01:03:51.520
Well, there's a few different parts of this. So one is there are all these harms that we need

01:03:51.520 --> 01:03:57.600
to basically fight against and prevent. And that's been a lot of our focus over the last

01:03:58.960 --> 01:04:04.640
five or seven years is basically ramping up very sophisticated AI systems, not generative AI

01:04:04.640 --> 01:04:13.040
systems, more kind of classical AI systems to be able to categorize and classify and identify,

01:04:14.000 --> 01:04:23.280
okay, this post looks like it's promoting terrorism. This one is exploiting children. This one looks

01:04:23.280 --> 01:04:28.080
like it might be trying to incite violence. This one's an intellectual property violation. So

01:04:28.720 --> 01:04:35.600
there's like 18 different categories of violating harmful content that we've

01:04:35.600 --> 01:04:42.640
had to build specific systems to be able to track. And I think it's certainly the case that

01:04:43.280 --> 01:04:52.800
advances in generative AI will test those. But at least so far, it's been the case and I'm

01:04:52.800 --> 01:04:57.600
optimistic that it will continue to be the case that we will be able to bring more computing

01:04:57.600 --> 01:05:04.000
power to bear to have even stronger AI's that can help defend against those things. So we've had

01:05:04.000 --> 01:05:09.200
to deal with some adversarial issues before, right? It's, I mean, for some things like hate

01:05:09.200 --> 01:05:13.920
speech, it's like people aren't generally getting a lot more sophisticated, like the average person

01:05:14.800 --> 01:05:18.880
let's say, you know, if someone's saying some kind of racist thing, right? It's like,

01:05:18.880 --> 01:05:22.960
they're not necessarily getting more sophisticated at being racist, right? It just, it's okay. So that

01:05:23.040 --> 01:05:29.120
the system can just find. But then there's other adversaries who actually are very sophisticated

01:05:29.120 --> 01:05:36.000
like nation states doing things. And we find, whether it's Russia or just different countries

01:05:36.000 --> 01:05:43.920
that are basically standing up these networks of bots or inauthentic accounts is what we call them,

01:05:43.920 --> 01:05:47.360
because they're not necessarily bots, that some of them could actually be real people who are

01:05:47.360 --> 01:05:54.320
kind of masquerading as other people, but they're acting in a coordinated way. And some of that

01:05:54.320 --> 01:05:59.680
behavior has gotten very sophisticated and it's very adversarial. So they, you know, each iteration,

01:05:59.680 --> 01:06:04.480
every time we find something and stop them, they kind of evolve their behavior. They don't just

01:06:04.480 --> 01:06:08.640
pack up their bags and go home and say, okay, we're not going to try. You know, at some point,

01:06:08.640 --> 01:06:13.040
they might decide doing it on meta services is not worth it. They'll go do it on someone else if

01:06:13.040 --> 01:06:19.520
it's easier to do it in another place. But we have a fair amount of experience dealing with

01:06:20.880 --> 01:06:24.560
even those kind of adversarial attacks where they just keep on getting better and better.

01:06:24.560 --> 01:06:28.800
And I do think that as long as we can keep on putting more compute power against it and

01:06:29.760 --> 01:06:34.000
if we're kind of one of the leaders in developing some of these AI models, I'm quite optimistic

01:06:34.000 --> 01:06:40.640
that we're going to be able to keep on pushing against the kind of normal categories of harm

01:06:40.640 --> 01:06:47.520
that you talk about, fraud, scams, spam, IP violations, things like that.

01:06:47.520 --> 01:06:54.640
What about like creating narratives and controversy? To me, it's kind of amazing how a small collection

01:06:54.640 --> 01:07:00.480
of, what did you say, inauthentic accounts? So it could be bots, but yeah, we have sort of this

01:07:00.480 --> 01:07:06.320
funny name for it, but we call it coordinated inauthentic behavior. It's kind of incredible how

01:07:06.320 --> 01:07:14.160
a small collection of folks can create narratives, create stories, especially if they're viral.

01:07:14.800 --> 01:07:20.320
Especially if they have an element that can catalyze the virality of the narrative.

01:07:20.960 --> 01:07:25.920
Yeah, and I think there the question is, you have to be, I'm very specific about what is

01:07:25.920 --> 01:07:34.000
bad about it. Because I think a set of people coming together or organically bouncing ideas

01:07:34.000 --> 01:07:39.520
off each other and a narrative comes out of that is not necessarily a bad thing by itself.

01:07:40.080 --> 01:07:44.240
If it's kind of authentic and organic, that's like a lot of what happens and how culture gets

01:07:44.240 --> 01:07:48.080
created and how art gets created and a lot of good stuff. So that's why we've kind of focused on

01:07:48.640 --> 01:07:53.680
this sense of coordinated inauthentic behavior. So it's like if you have a network of, whether

01:07:53.680 --> 01:08:00.640
it's bots, some people masquerading as different accounts, but you have kind of someone pulling

01:08:00.640 --> 01:08:08.480
the strings behind it and trying to kind of act as if this is a more organic set of behavior,

01:08:08.480 --> 01:08:12.880
but really it's not. It's just like one coordinated thing. That seems problematic to

01:08:12.880 --> 01:08:18.480
me. I mean, I don't think people should be able to have coordinated networks and not disclose it

01:08:18.480 --> 01:08:26.000
as such. But that again, we've been able to deploy pretty sophisticated AI and counterterrorism

01:08:26.960 --> 01:08:32.160
groups and things like that to be able to identify a fair number of these coordinated

01:08:32.160 --> 01:08:40.320
and authentic networks of accounts and take them down. We continue to do that. It's one

01:08:40.320 --> 01:08:44.080
thing that if you'd told me 20 years ago, it's like, all right, you're starting this website to

01:08:44.080 --> 01:08:48.880
help people connect at a college. And in the future, you're going to be part of your organization.

01:08:48.880 --> 01:08:53.440
It's going to be a counterterrorism organization with AI to find coordinated and authentic.

01:08:53.440 --> 01:09:00.800
I would have thought that was pretty wild. But no, I think that's part of where we are.

01:09:00.800 --> 01:09:03.600
But look, I think that these questions that you're pushing on now,

01:09:06.720 --> 01:09:09.920
this is actually where I'd guess most of the challenge around AI will be

01:09:10.800 --> 01:09:15.840
for the foreseeable future. I think that there's a lot of debate around things like,

01:09:15.840 --> 01:09:20.960
is this going to create existential risk to humanity? And those are very hard things to

01:09:20.960 --> 01:09:26.560
disprove one way or another. My own intuition is that the point at which we become close to

01:09:26.560 --> 01:09:35.280
superintelligence is, it's just really unclear to me that the current technology is going to

01:09:35.280 --> 01:09:40.240
get there without another set of significant advances. But that doesn't mean that there's no

01:09:40.240 --> 01:09:47.200
danger. I think the danger is basically amplifying the kind of known set of harm is that people or

01:09:47.200 --> 01:09:50.640
sets of accounts can do. And we just need to make sure that we really focus on

01:09:53.200 --> 01:09:57.600
basically doing that as well as possible. So that's definitely a big focus for me.

01:09:57.600 --> 01:10:01.040
Well, you can basically use large language models as an assistant

01:10:01.760 --> 01:10:04.880
of how to cause harm on social networks. You can ask it a question.

01:10:04.880 --> 01:10:12.720
You know, Meta has very impressive coordinated inauthentic account

01:10:14.320 --> 01:10:19.600
fighting capabilities. How do I do the coordinated inauthentic account

01:10:20.960 --> 01:10:24.560
creation where Meta doesn't detect it? Like literally ask that question.

01:10:25.280 --> 01:10:31.200
And basically there's this kind of part of it. I mean, that's what open AI showed that they're

01:10:31.200 --> 01:10:35.360
concerned of those questions. Perhaps you can comment on your approach to it, how to do

01:10:36.560 --> 01:10:42.640
a kind of moderation on the output of those models that it can't be used to help you coordinate

01:10:42.640 --> 01:10:46.240
harm in all the full definition of what the harm means.

01:10:46.800 --> 01:10:51.840
Yeah. And that's a lot of the fine-tuning and the alignment training that we do is basically,

01:10:52.480 --> 01:10:59.920
you know, when we ship AI's across our products, a lot of what we're trying to

01:11:00.960 --> 01:11:06.160
make sure is that if you can't ask it to help you commit a crime.

01:11:11.120 --> 01:11:17.040
So I think training it to kind of understand that. And it's not like any of these systems

01:11:17.040 --> 01:11:22.000
are ever going to be 100% perfect, but you know, just making it so that

01:11:23.440 --> 01:11:31.680
this isn't an easier way to go about doing something bad than the next best alternative.

01:11:31.680 --> 01:11:34.960
Right. I mean, people still have Google where they, you know, you still have search engines.

01:11:34.960 --> 01:11:43.360
So the information is out there. And for these, you know, what we see is like for

01:11:43.360 --> 01:11:47.360
nation-states or, you know, these actors that are trying to pull off these large,

01:11:48.240 --> 01:11:52.400
you know, coordinated and authentic networks to kind of influence different things,

01:11:53.120 --> 01:11:56.000
at some point when we might just make it very difficult, they do just, you know,

01:11:56.000 --> 01:12:01.040
try to use other services instead. Right. It's just like if you can make it more expensive for

01:12:02.160 --> 01:12:06.480
them to do it on your service, then kind of people go elsewhere. And I think that that's

01:12:07.440 --> 01:12:12.000
the bar. Right. It's like, it's not like, okay, are you ever going to be perfect at finding,

01:12:12.000 --> 01:12:16.160
you know, every adversary who tries to attack you, it's, I mean, you try to get as close to that

01:12:16.160 --> 01:12:20.800
as possible. But I think really, kind of economically, what you're just trying to do is

01:12:20.800 --> 01:12:24.720
make it so that it's just inefficient for them to go after that.

01:12:24.720 --> 01:12:29.840
But there's also complicated questions of what is and isn't harm, what is and isn't misinformation.

01:12:30.480 --> 01:12:35.840
So this is one of the things that Wikipedia has also tried to face. I remember asking

01:12:36.800 --> 01:12:44.080
GPT about whether the virus leaked from a lab or not. And the answer provided was a very nuanced one

01:12:45.120 --> 01:12:53.520
and a well-sighted one, almost dare I say, well thought out one, balanced. I would hate for that

01:12:53.520 --> 01:12:59.520
nuance to be lost through the process of moderation. Wikipedia does a good job on that particular

01:12:59.520 --> 01:13:05.920
thing too. But from pressures from governments and institutions, it's, you could see some of that

01:13:05.920 --> 01:13:16.480
nuance and depth of information facts and wisdom be lost. Absolutely. And that's a scary thing.

01:13:16.480 --> 01:13:22.480
Some of the magic, some of the edges, the rough edges might be lost to the process of moderation

01:13:22.480 --> 01:13:28.800
of AI systems. So how do you get that right? I really agree with what you're pushing on.

01:13:29.280 --> 01:13:37.440
The core shape of the problem is that there are some harms that I think everyone agrees

01:13:37.440 --> 01:13:46.640
are bad. So sexual exploitation of children. You're not going to get many people who think

01:13:46.640 --> 01:13:50.080
that that type of thing should be allowed on any service. And that's something that we

01:13:51.040 --> 01:14:00.240
face and try to push off as much as possible today. Terrorism, inciting violence. We went

01:14:00.240 --> 01:14:06.720
through a bunch of these types of harms before. But then I do think that you get to a set of

01:14:06.720 --> 01:14:17.040
harms where there is more social debate around it. So misinformation I think has been a really

01:14:17.040 --> 01:14:24.560
tricky one because there are things that are kind of obviously false that are maybe factual

01:14:27.440 --> 01:14:33.680
but may not be harmful. So it's like, all right, are you going to censor someone for just being

01:14:33.680 --> 01:14:38.160
wrong? If there's no kind of harm implication of what they're doing, I think that there's a

01:14:38.160 --> 01:14:43.600
bunch of real kind of issues and challenges there. But then I think that there are other places where

01:14:43.600 --> 01:14:50.400
it is, it just takes some of the stuff around COVID earlier on in the pandemic where there were

01:14:51.200 --> 01:14:56.400
real health implications. But there hadn't been time to fully vet a bunch of the scientific

01:14:56.400 --> 01:15:00.880
assumptions. And unfortunately, I think a lot of the kind of establishment on that

01:15:02.240 --> 01:15:06.800
kind of waffled on a bunch of facts and asked for a bunch of things to be censored that in

01:15:06.800 --> 01:15:14.320
retrospect ended up being more debatable or true. And that stuff is really tough and really undermines

01:15:14.320 --> 01:15:24.000
trust in that. So I do think that the questions around how to manage that are very nuanced.

01:15:24.000 --> 01:15:31.120
The way that I try to think about it is that I think it's best to generally boil things down

01:15:31.760 --> 01:15:37.600
to the harms that people agree on. So when you think about, is something misinformation or not,

01:15:38.240 --> 01:15:46.640
I think often the more salient bit is, is this going to potentially lead to physical harm for

01:15:46.640 --> 01:15:52.080
someone and kind of think about it in that sense. And then beyond that, I think people just have

01:15:52.080 --> 01:15:56.080
different preferences on how they want things to be flagged for them. I think a bunch of people

01:15:56.080 --> 01:16:01.520
would prefer to kind of have a flag on something that says, hey, a fact checker thinks that this

01:16:01.520 --> 01:16:06.960
might be false. I think Twitter's community notes implementation is quite good on this.

01:16:08.400 --> 01:16:11.840
But again, it's the same type of thing. It's like just kind of discretionarily

01:16:11.840 --> 01:16:16.560
adding a flag because it makes the user experience better. But it's not trying to take down the

01:16:16.560 --> 01:16:22.000
information or not. I think that you want to reserve the kind of censorship of content of

01:16:22.000 --> 01:16:26.800
things that are of known categories that people generally agree are bad.

01:16:27.600 --> 01:16:32.640
Yeah, but there's so many things, especially with the pandemic, but there's other topics

01:16:33.360 --> 01:16:40.400
where there's just deep disagreement fueled by politics about what is and isn't harmful.

01:16:41.200 --> 01:16:46.480
There's even just the degree to which the virus is harmful and the degree to which

01:16:47.440 --> 01:16:52.400
the vaccines that respond to the virus are harmful. There's just, there's almost like a

01:16:52.400 --> 01:16:59.520
political divider on that. And so how do you make decisions about that? Where half the country

01:16:59.520 --> 01:17:05.760
in the United States or some large fraction of the world has very different views from another

01:17:05.760 --> 01:17:13.360
part of the world? Is there a way for Metta to stay out of the moderation of this?

01:17:13.440 --> 01:17:21.280
I think we, it's very difficult to just abstain. But I think we should be clear about which of

01:17:21.280 --> 01:17:28.480
these things are actual safety concerns and which ones are a matter of preference in terms of how

01:17:28.480 --> 01:17:33.840
people want information flagged. Right, so we did recently introduce something that allows people

01:17:35.440 --> 01:17:40.800
to have fact checking not affect the distribution of what shows them their product. So, okay,

01:17:40.800 --> 01:17:45.120
a bunch of people don't trust who the fact checkers are. All right, well, you can turn that off if

01:17:45.120 --> 01:17:51.280
you want. But if the content violates some policy, like it's inciting violence or something like

01:17:51.280 --> 01:17:56.400
that, it's still not going to be allowed. So, I think that you want to honor people's preferences

01:17:56.400 --> 01:18:03.520
on that as much as possible. But look, I mean, this is really difficult stuff. I think it's

01:18:03.600 --> 01:18:11.360
really hard to know where to draw the line on what is fact and what is opinion. Because the nature

01:18:11.360 --> 01:18:17.600
of science is that nothing is ever 100% known for certain. You can disprove certain things,

01:18:17.600 --> 01:18:24.800
but you're constantly testing new hypotheses and scrutinizing frameworks that have been long-held.

01:18:25.440 --> 01:18:29.840
Every once in a while, you throw out something that was working for a very long period of time,

01:18:29.840 --> 01:18:35.120
and it's very difficult. But I think that just because it's very hard and just because

01:18:35.120 --> 01:18:40.720
there are edge cases doesn't mean that you should not try to give people what they're looking for

01:18:40.720 --> 01:18:50.400
as well. Let me ask about something you've faced in terms of moderation is pressure from

01:18:51.280 --> 01:18:56.080
different sources, pressure from governments. I want to ask a question, how to withstand

01:18:56.960 --> 01:19:02.720
that pressure for a world where AI moderation starts becoming a thing too.

01:19:03.680 --> 01:19:12.160
So, what's Metta's approach to resist the pressure from governments and other interest

01:19:12.160 --> 01:19:18.720
groups in terms of what to moderate and not? I don't know that there's a one-size-fits-all

01:19:18.720 --> 01:19:26.000
answer to that. I think we basically have the principles around we want to allow people to

01:19:26.000 --> 01:19:34.000
express as much as possible, but we have developed clear categories of things that we think are

01:19:35.920 --> 01:19:40.080
wrong, that we don't want on our services, and we build tools to try to moderate those.

01:19:40.640 --> 01:19:47.120
So, then the question is, okay, what do you do when a government says that they don't want something

01:19:48.400 --> 01:19:55.200
on the service? We have a bunch of principles around how we deal with that,

01:19:55.200 --> 01:20:01.760
because on the one hand, if there's a democratically elected government and people around the world

01:20:01.760 --> 01:20:09.040
just have different values and different places, then should we as a California-based company

01:20:10.320 --> 01:20:19.040
tell them that something that they have decided is unacceptable, actually that we need to be able

01:20:19.760 --> 01:20:26.160
to express that? I think that there's a certain amount of hubris in that.

01:20:27.360 --> 01:20:33.760
But then I think there are other cases where it's a little more autocratic, and you have

01:20:33.760 --> 01:20:39.920
the dictator leader who's just trying to crack down on dissent, and the people in a country are

01:20:39.920 --> 01:20:46.320
really not aligned with that, and it's not necessarily against their culture, but the person

01:20:46.560 --> 01:20:52.240
who's leading it is just trying to push in a certain direction. These are very complex questions,

01:20:52.880 --> 01:21:02.880
but I think it's difficult to have a one-size-fits-all approach to it. But in general, we're pretty

01:21:02.880 --> 01:21:10.240
active in kind of advocating and pushing back on requests to take things down.

01:21:10.800 --> 01:21:17.600
But honestly, the thing that I think a request to censor things is one thing,

01:21:18.320 --> 01:21:24.640
and that's obviously bad. But where we draw a much harder line is on requests for access to

01:21:24.640 --> 01:21:35.360
information, because if you get told that you can't say something, that's bad. That obviously violates

01:21:35.360 --> 01:21:43.920
your sense and freedom of expression at some level. But a government getting access to data

01:21:43.920 --> 01:21:53.120
in a way that seems like it would be unlawful in our country exposes people to real physical harm,

01:21:54.640 --> 01:22:02.000
and that's something that in general we take very seriously. And then that flows through

01:22:02.080 --> 01:22:08.320
all of our policies in a lot of ways. By the time you're actually litigating with a government

01:22:08.320 --> 01:22:14.480
or pushing back on them, that's pretty late in the funnel. I'd say a bunch of this stuff

01:22:14.480 --> 01:22:20.640
starts a lot higher up in the decision of where do we put data centers. Then there are a lot of

01:22:20.640 --> 01:22:26.640
countries where we may have a lot of people using the service in a place. It might be good for the

01:22:26.720 --> 01:22:32.000
service in some ways. It's good for those people if we could reduce the latency by having a data

01:22:32.000 --> 01:22:38.400
center nearby them. But for whatever reason, we just feel like, hey, this government does not

01:22:38.400 --> 01:22:47.120
have a good track record on basically not trying to get access to people's data. And at the end of

01:22:47.120 --> 01:22:51.520
the day, if you put a data center in a country and the government wants to get access to people's

01:22:51.520 --> 01:22:57.280
data, then they do at the end of the day have the option of having people show up with guns and

01:22:57.280 --> 01:23:02.800
taking it by force. So I think that there's a lot of decisions that go into how you architect the

01:23:02.800 --> 01:23:11.600
systems years in advance of these actual confrontations that end up being really important.

01:23:11.600 --> 01:23:17.040
So you put the protection of people's data as a very, very high priority.

01:23:18.000 --> 01:23:22.480
That I think there are more harms that I think can be associated with that. And I think that

01:23:22.480 --> 01:23:29.760
that ends up being a more critical thing to defend against governments. Whereas if another

01:23:29.760 --> 01:23:34.480
government has a different view of what should be acceptable speech in their country, especially

01:23:34.480 --> 01:23:39.280
if it's a democratically elected government, then I think that there's a certain amount of

01:23:39.280 --> 01:23:44.880
deference that you should have to that. So that's speaking more to the direct harm that's possible

01:23:44.960 --> 01:23:49.040
when you give governments access to data. But if we look at the United States,

01:23:50.240 --> 01:23:55.600
to the more nuanced kind of pressure to sensor, not even ordered to sensor, but pressure to

01:23:55.600 --> 01:24:01.440
sensor from political entities, which has kind of received quite a bit of attention in the United

01:24:01.440 --> 01:24:10.080
States. Maybe one way to ask that question is if you've seen the Twitter files, what have you

01:24:10.080 --> 01:24:17.680
learned from the kind of pressure from US government agencies that was seen in Twitter files?

01:24:18.480 --> 01:24:20.480
And what do you do with that kind of pressure?

01:24:22.080 --> 01:24:29.120
You know, I've seen it. It's really hard from the outside to know exactly what happened in

01:24:29.120 --> 01:24:36.400
each of these cases. We've obviously been in a bunch of our own cases where

01:24:40.240 --> 01:24:45.760
agencies or different folks will just say, hey, here's a threat that we're aware of.

01:24:46.640 --> 01:24:50.960
You should be aware of this too. It's not really pressure as much as it is just

01:24:53.040 --> 01:24:57.360
flagging something that our security systems should be on alert about.

01:24:58.400 --> 01:25:05.280
I get how some people could think of it as that. But at the end of the day, it's our call on how

01:25:06.240 --> 01:25:10.160
to handle that. But I mean, I just, in terms of running these services,

01:25:10.160 --> 01:25:13.280
want to have access to as much information about what people think that adversaries

01:25:13.280 --> 01:25:21.440
might be trying to do as possible. So you don't feel like there would be consequences if anybody,

01:25:21.440 --> 01:25:27.200
the CIA, the FBI, a political party, the Democrats, the Republicans of high,

01:25:28.240 --> 01:25:33.040
powerful political figures, write emails. You don't feel pressure from

01:25:34.000 --> 01:25:38.160
a suggestion. I guess what I should say is there's so much pressure from all sides that

01:25:38.160 --> 01:25:43.920
I'm not sure that any specific thing that someone says is really adding that much more to the mix.

01:25:46.160 --> 01:25:52.320
There are obviously a lot of people who think that we should be censoring more content,

01:25:52.320 --> 01:25:56.160
or there are a lot of people who think we should be censoring less content. There are,

01:25:56.160 --> 01:26:00.560
as you say, all kinds of different groups that are involved in these debates. So there's

01:26:00.640 --> 01:26:07.120
the elected officials and politicians themselves. There's the agencies, but there's the media.

01:26:07.760 --> 01:26:12.880
There's activist groups. This is not a U.S. specific thing. There are groups all over the

01:26:12.880 --> 01:26:21.600
world and all in every country that bring different values. So it's just a very active

01:26:21.600 --> 01:26:30.480
debate, and I understand it. These questions get to really some of the most important

01:26:30.640 --> 01:26:36.320
social debates that are being had. So I think it's back to the question of truth, because

01:26:38.160 --> 01:26:43.200
for a lot of these things, they haven't yet been hardened into a single truth, and society's

01:26:43.200 --> 01:26:49.840
sort of trying to hash out what we think on certain issues. Maybe in a few hundred years,

01:26:49.840 --> 01:26:52.960
everyone will look back and say, hey, no, it wasn't obvious that it should have been this,

01:26:52.960 --> 01:26:59.520
but no, we're kind of in that meat grinder now and working through that.

01:27:02.800 --> 01:27:12.400
So no, these are all very complicated. And some people raise concerns in good faith and

01:27:12.400 --> 01:27:16.480
just say, hey, this is something that I want to flag for you to think about. Certain people,

01:27:16.480 --> 01:27:25.760
I certainly think, come at things with somewhat of a more punitive or vengeful view of I want

01:27:25.760 --> 01:27:29.920
you to do this thing. If you don't, then I'm going to try to make your life difficult in a lot of

01:27:29.920 --> 01:27:37.440
other ways. But I don't know. This is one of the most pressurized debates, I think, in society.

01:27:37.440 --> 01:27:41.760
So I just think that there are so many people in different forces that are trying to apply

01:27:41.760 --> 01:27:46.400
pressure from different sides that I don't think you can make decisions based on trying

01:27:46.400 --> 01:27:52.880
to make people happy. I think you just have to do what you think is the right balance and accept

01:27:52.880 --> 01:27:58.320
that people are going to be upset no matter where you come out on that. Yeah, I like that

01:27:58.320 --> 01:28:03.280
pressurized debate. So how's your view of the freedom of speech evolved over the years?

01:28:04.160 --> 01:28:14.240
And now with AI, where the freedom might apply to them, not just to the humans,

01:28:14.800 --> 01:28:18.720
but to the personalized agents as you've spoken about them.

01:28:20.000 --> 01:28:24.080
So yeah, I mean, I've probably gotten a somewhat more nuanced view just because I think that there

01:28:24.080 --> 01:28:30.320
are... I come at this, I'm obviously very pro freedom of expression. I don't think you build

01:28:30.480 --> 01:28:34.240
a service like this that gives people tools to express themselves unless you think that people

01:28:34.240 --> 01:28:40.640
expressing themselves at scale is a good thing. So I get into this to try to prevent people from

01:28:41.440 --> 01:28:45.840
expressing anything I want to give people tools so they can express as much as possible.

01:28:46.400 --> 01:28:51.840
And then I think it's become clear that there are certain categories of things that we've talked

01:28:51.840 --> 01:28:56.320
about that I think almost everyone accepts are bad and that no one wants and that they're

01:28:56.320 --> 01:29:02.000
that are illegal even in countries like the US where you have the First Amendment that's very

01:29:02.000 --> 01:29:06.800
protective of enabling speech. It's like you're still not allowed to do things that are going

01:29:06.800 --> 01:29:11.200
to immediately enside violence or violate people's intellectual property or things like that.

01:29:11.200 --> 01:29:18.640
So through those, but then there's also a very active core of just active disagreements in society

01:29:18.640 --> 01:29:22.800
where some people may think that something is true or false. The other side might think it's

01:29:22.800 --> 01:29:31.040
the opposite or just unsettled. And those are some of the most difficult to kind of handle

01:29:31.040 --> 01:29:40.640
like we've talked about. But one of the lessons that I feel like I've learned is that a lot of

01:29:40.640 --> 01:29:50.960
times when you can, the best way to handle this stuff more practically is not in terms of answering

01:29:50.960 --> 01:29:59.200
the question of should this be allowed, but just like what is the best way to deal with someone

01:29:59.200 --> 01:30:10.880
being a jerk? Is the person basically just having a repeat behavior of causing a lot of issues?

01:30:12.000 --> 01:30:16.720
So looking at it more at that level. And its effect on the broader communities,

01:30:16.720 --> 01:30:21.680
health of the community, health of the state. It's tricky though because how do you know there

01:30:21.680 --> 01:30:27.600
could be people that have a very controversial viewpoint that turns out to have a positive

01:30:27.600 --> 01:30:31.600
long-term effect on the health of the community because it challenges the community to think?

01:30:31.600 --> 01:30:37.360
That's true. Absolutely. I think you want to be careful about that. I'm not sure I'm expressing

01:30:37.360 --> 01:30:44.560
this very clearly because I certainly agree with your point there. And my point isn't that we should

01:30:45.520 --> 01:30:50.320
not have people on our services that are being controversial. That's certainly not what I mean

01:30:50.320 --> 01:30:59.920
to say. It's that often I think it's not just looking at a specific example of speech that it's

01:30:59.920 --> 01:31:06.000
most effective to handle this stuff. And I think often you don't want to make specific binary

01:31:06.000 --> 01:31:11.200
decisions of kind of this is allowed or this isn't. I mean, we talked about, you know, when it's

01:31:11.200 --> 01:31:15.520
fact-checking or Twitter's community voices thing, I think that that's another good example. It's

01:31:15.520 --> 01:31:20.720
not a question of is this allowed or not. It's just a question of adding more context to the thing.

01:31:20.720 --> 01:31:25.600
I think that that's helpful. So in the context of AI, which is what you're asking about,

01:31:26.160 --> 01:31:33.840
there are lots of ways that an AI can be helpful. With an AI, it's less about censorship, right?

01:31:33.840 --> 01:31:38.400
Because it's more about what is the most productive answer to a question.

01:31:39.920 --> 01:31:44.880
There was one case study that I was reviewing with the team is someone asked,

01:31:49.200 --> 01:31:58.400
can you explain to me how to 3D print a gun? And one proposed response is like, no,

01:31:58.400 --> 01:32:03.280
I can't talk about that. But it's basically just shut it down immediately. Which I think is some

01:32:03.280 --> 01:32:06.960
of what you see. It's like, as a large language model, I'm not allowed to talk about, you know,

01:32:06.960 --> 01:32:12.640
whatever. But there's another response which is like, hey, you know, I don't think that's a good

01:32:12.640 --> 01:32:19.680
idea. And a lot of countries, including the US, 3D printing guns is illegal or kind of whatever

01:32:19.680 --> 01:32:24.400
the factual thing is. And I was like, okay, you know, that's actually a respectful and informative

01:32:24.400 --> 01:32:30.320
answer. And, you know, I may have not known that specific thing. And so there are different ways

01:32:30.320 --> 01:32:38.160
to handle this that I think kind of you can either assume good intent. Like maybe the person

01:32:38.160 --> 01:32:42.080
didn't know and I'm just going to help educate them. Or you could kind of come at it as like,

01:32:42.080 --> 01:32:45.360
no, I need to shut this thing down immediately. Right? It's like, I just am not going to talk

01:32:45.360 --> 01:32:53.280
about this. And there may be times where you need to do that. But I actually think having a

01:32:54.480 --> 01:32:58.880
somewhat more informative approach where you generally assume good intent from people

01:32:59.840 --> 01:33:05.280
is probably a better balance to be on as many things as you can be. You're not going to be able

01:33:05.280 --> 01:33:09.760
to do that for everything. But that you're kind of asking about how I approach this and I'm

01:33:09.760 --> 01:33:16.800
thinking about this and as it relates to AI. And I think that that's a big difference in kind of

01:33:16.800 --> 01:33:21.680
how to handle sensitive content across these different modes.

01:33:21.680 --> 01:33:28.880
I have to ask, there's rumors you might be working on a social network that's text-based.

01:33:28.880 --> 01:33:34.720
That might be a competitor to Twitter, codenamed P92. Is there something you can say

01:33:36.160 --> 01:33:42.800
about those rumors? There is a project. You know, I've always thought that sort of a text-based

01:33:43.760 --> 01:33:52.400
kind of information utility is just a really important thing to society. And for whatever

01:33:52.400 --> 01:33:57.520
reason, I feel like Twitter has not lived up to what I would have thought its full potential

01:33:57.520 --> 01:34:01.040
should be. And I think that the current, you know, I think Elon thinks that, right? And that's

01:34:01.040 --> 01:34:10.000
probably one of the reasons why he bought it. And I do know there are ways to consider

01:34:10.080 --> 01:34:14.160
alternative approaches to this. And one that I think is potentially interesting

01:34:15.600 --> 01:34:19.520
is this open and federated approach where you're seeing with Mastodon and you're seeing that a

01:34:19.520 --> 01:34:27.200
little bit with Blue Sky. And I think that it's possible that something that melds some of those

01:34:27.200 --> 01:34:33.280
ideas with the graph and identity system that people have already cultivated on Instagram

01:34:34.240 --> 01:34:39.920
could be a kind of very welcome contribution to that space. But I know we work on a lot of

01:34:39.920 --> 01:34:45.200
things all the time though too. So I don't want to get ahead of myself. And we have projects

01:34:45.200 --> 01:34:49.760
that explore a lot of different things. And this is certainly one that I think could be interesting.

01:34:50.800 --> 01:34:56.960
So what's the release, the launch date of that again? Or what's the official website?

01:34:57.200 --> 01:35:04.640
Well, we don't have that yet. And look, I mean, I don't know exactly how this is going to

01:35:04.640 --> 01:35:09.120
turn out. I mean, what I can say is, yeah, there's some people working on this. I think that

01:35:09.120 --> 01:35:15.360
there's something there that's interesting to explore. So if you look at, it'd be interesting

01:35:15.360 --> 01:35:22.000
to just ask this question and throw Twitter into the mix, that the landscape of social networks,

01:35:22.000 --> 01:35:30.160
that is Facebook, that is Instagram, that is WhatsApp. And then think of a text-based

01:35:30.160 --> 01:35:34.320
social network. When you look at that landscape, what are the interesting differences to you?

01:35:35.040 --> 01:35:40.560
Why do we have these different flavors? And what are the needs? What are the use cases?

01:35:40.560 --> 01:35:45.680
What are the products? What is the aspect of them that create a fulfilling human experience

01:35:45.680 --> 01:35:49.200
and a connection between humans that is somehow distinct?

01:35:49.200 --> 01:35:55.440
Well, I think text is very accessible for people to transmit ideas and to have back and

01:35:55.440 --> 01:36:03.760
forth exchanges. So it I think ends up being a good format for discussion,

01:36:04.400 --> 01:36:09.760
in a lot of ways, uniquely good. If you look at some of the other formats or other networks that

01:36:09.760 --> 01:36:15.520
have focused on one type of content like TikTok is obviously huge. And there are comments on TikTok,

01:36:15.520 --> 01:36:21.520
but I think the architecture of the service is very clearly that you have the video is the

01:36:21.520 --> 01:36:32.480
primary thing, and there's comments after that. But I think one of the unique pieces of having

01:36:33.280 --> 01:36:40.800
text-based content is that the comments can also be first class. And that makes it so that

01:36:41.440 --> 01:36:46.480
conversations can just filter and fork into all these different directions and in a way that

01:36:46.480 --> 01:36:50.720
can be super useful. There's a lot of things that are really awesome about the experience.

01:36:50.720 --> 01:36:56.240
It just always struck me, I always thought that Twitter should have a billion people using it,

01:36:56.240 --> 01:37:02.640
or whatever the thing is that basically ends up being in that space. And for whatever

01:37:02.640 --> 01:37:08.480
combination of reasons, again, these companies are complex organisms and it's very hard to

01:37:08.480 --> 01:37:13.280
diagnose this stuff from the outside. Why doesn't Twitter, why doesn't a text-based

01:37:14.800 --> 01:37:21.120
comment as a first citizen based social network have a billion users? Well, I just think it's

01:37:21.120 --> 01:37:28.320
hard to build these companies. So it's not that every idea automatically goes and gets a billion

01:37:28.320 --> 01:37:33.120
people. It's just that I think that that idea coupled with good execution should get there.

01:37:34.000 --> 01:37:42.240
But look, we hit certain thresholds over time where we plateaued early on and it wasn't clear

01:37:42.240 --> 01:37:47.360
that we were ever going to reach 100 million people on Facebook and then we got really good at

01:37:47.360 --> 01:37:52.400
dialing in internationalization and helping the service grow in different countries and

01:37:54.240 --> 01:38:00.080
that was a whole competence that we needed to develop and helping people basically spread

01:38:00.080 --> 01:38:03.600
the service to their friends. That was one of the things, once we got very good at that,

01:38:03.600 --> 01:38:08.640
that was one of the things that made me feel like, hey, if Instagram joined us early on,

01:38:08.640 --> 01:38:12.240
then I felt like we could help grow that quickly and same with WhatsApp. And that's

01:38:12.240 --> 01:38:16.880
sort of been a core competence that we've developed and been able to execute on and others have too.

01:38:16.880 --> 01:38:22.400
I mean, ByteDance obviously have done a very good job with TikTok and have reached more than a

01:38:22.400 --> 01:38:28.720
billion people there. But it's certainly not automatic. I think you need a certain level of

01:38:30.320 --> 01:38:35.200
execution to basically get there. And I think for whatever reason, I think Twitter has this

01:38:35.200 --> 01:38:43.600
great idea and sort of magic in the service. But they just haven't kind of cracked that piece

01:38:43.600 --> 01:38:47.120
yet. And I think that that's made it so that you're seeing all these other things, whether it's

01:38:47.120 --> 01:38:55.920
mastodon or blue sky that I think are maybe just different cuts at the same thing. But I think

01:38:56.000 --> 01:39:01.360
through the last generation of social media overall, one of the interesting experiments

01:39:01.360 --> 01:39:05.680
that I think should get run at larger scale is what happens if there's somewhat more

01:39:05.680 --> 01:39:12.320
decentralized control. And the stack is more open throughout. And I've just been pretty

01:39:12.320 --> 01:39:20.240
fascinated by that and seeing how that works. To some degree, end-to-end encryption on WhatsApp

01:39:20.240 --> 01:39:25.680
and as we bring it to other services provides an element of it because it pushes the service

01:39:25.680 --> 01:39:32.560
really out to the edges. I mean, the server part of this that we run for WhatsApp is relatively

01:39:32.560 --> 01:39:37.680
very thin compared to what we do on Facebook or Instagram. And much more of the complexity is

01:39:38.320 --> 01:39:43.760
how the apps kind of negotiate with each other to pass information in a fully end-to-end encrypted

01:39:43.760 --> 01:39:49.120
way. But I don't know. I think that that is a good model. I think it puts more power in

01:39:49.120 --> 01:39:53.280
individual's hands. And there are a lot of benefits of it if you can make it happen.

01:39:53.280 --> 01:39:58.960
Again, this is all pretty speculative. I mean, I think that it's hard from the outside to know

01:39:59.520 --> 01:40:02.880
why anything does or doesn't work until you kind of take a run at it. And

01:40:04.880 --> 01:40:09.200
so I think it's kind of an interesting thing to experiment with. But I don't really know where

01:40:09.200 --> 01:40:17.600
this one's going to go. So since we were talking about Twitter, Elon Musk had what I think a few

01:40:17.680 --> 01:40:26.240
harsh words that I wish he didn't say. So let me ask, in the hope in the name of camaraderie,

01:40:26.240 --> 01:40:32.960
what do you think Elon is doing well with Twitter? And what as a person who has run for a long time,

01:40:32.960 --> 01:40:42.800
you, social networks, Facebook, Instagram, WhatsApp, what can he do better? What can he improve on

01:40:42.880 --> 01:40:49.280
that text-based social network? Gosh, it's always very difficult to offer specific critiques from

01:40:49.280 --> 01:40:54.320
the outside before you get into this. Because I think one thing that I've learned is that

01:40:55.360 --> 01:41:00.800
everyone has opinions on what you should do. And like running the company, you see a lot of

01:41:00.800 --> 01:41:10.480
specific nuances on things that are not apparent externally. And I often think that some of the

01:41:11.200 --> 01:41:18.720
discourse around us could be better if there's more kind of space for acknowledging that there's

01:41:18.720 --> 01:41:23.760
certain things that we're seeing internally that guide what we're doing. But I don't know.

01:41:23.760 --> 01:41:38.640
Since you asked what is going well, I do think that Elon led a push early on

01:41:39.200 --> 01:41:48.880
to make Twitter a lot leaner. And I think that that, it's like you can agree or disagree with

01:41:48.880 --> 01:41:55.200
exactly all the tactics and how we did that. Obviously, every leader has their own style

01:41:55.200 --> 01:41:59.440
for if they, if you need to make dramatic changes for that, how you're going to execute it.

01:41:59.920 --> 01:42:08.720
But a lot of the specific principles that he pushed on around basically trying to make the

01:42:08.720 --> 01:42:14.160
organization more technical around decreasing the distance between engineers at the company

01:42:14.720 --> 01:42:23.120
and him, like fewer layers of management. I think that those were generally good changes.

01:42:23.120 --> 01:42:28.480
And I'm also, I also think that it was probably good for the industry that he made those changes.

01:42:28.480 --> 01:42:31.840
Because my sense is that there were a lot of other people who thought that those were good

01:42:31.840 --> 01:42:40.560
changes, but who may have been a little shy about doing them. And I think he,

01:42:41.760 --> 01:42:46.240
you know, just in my conversations with other founders, and how people have reacted to the

01:42:46.240 --> 01:42:50.320
things that we've done, you know, what I've heard from a lot of folks is, is just, hey,

01:42:50.320 --> 01:42:53.760
you know, when you, when someone like you, you know, when I, when I wrote the letter

01:42:53.760 --> 01:42:58.480
outlining the organizational changes that I wanted to make back in March and, you know,

01:42:58.480 --> 01:43:03.200
when people see what Elon is doing, I think that that gives, you know, people

01:43:04.560 --> 01:43:11.600
the ability to think through how to shape their organizations in a way that, that, that, you know,

01:43:11.600 --> 01:43:16.000
hopefully can, can be good for the industry and make all these companies more productive over time.

01:43:16.000 --> 01:43:23.280
So, something that that was one where I think he was quite ahead of a bunch of the other companies

01:43:23.280 --> 01:43:27.680
on. And, and, and, you know, what he was doing there, you know, again, from the outside, very

01:43:27.680 --> 01:43:31.920
hard to know. It's like, okay, did he, did he cut too much? Did he knock enough? Whatever. I don't

01:43:31.920 --> 01:43:37.840
think it's like my place to opine on that. And you asked for a, for a positive framing of the

01:43:37.840 --> 01:43:43.760
question of what, what do I, what do I admire? What do I think it went well? But I think that,

01:43:43.760 --> 01:43:50.800
like certainly his actions led me and I think a lot of other folks in the industry to think about,

01:43:50.800 --> 01:43:55.200
hey, are we, are we kind of doing this as much as we should? Like, can we,

01:43:55.200 --> 01:43:58.560
is, could we make our companies better by pushing on some of these same principles?

01:43:59.360 --> 01:44:04.000
Well, the two of you are on the top of the world in terms of leading the development of tech. And

01:44:04.000 --> 01:44:12.320
I wish there was more both way camaraderie and kindness, more love in the world, because love

01:44:12.320 --> 01:44:20.720
is the answer. But let me ask on the point of efficiency. You recently announced multiple

01:44:20.720 --> 01:44:27.920
stages of layoffs and meta. What are the most painful aspects of this process? Given

01:44:29.040 --> 01:44:32.800
for the individuals, the painful effects it has on those people's lives?

01:44:32.800 --> 01:44:38.480
Yeah, I mean, that's it. And that's it. I mean, it's a, and you basically have

01:44:39.440 --> 01:44:44.080
a significant number of people who, you know, this is just not the end of their

01:44:44.720 --> 01:44:50.720
time at meta that they or, or I, you know, would have hoped for when they joined the company.

01:44:52.560 --> 01:44:59.440
And, you know, I mean, running a company, people are, you know, constantly joining and

01:44:59.440 --> 01:45:03.920
leaving the company for different directions, but, but for different, different reasons. But

01:45:03.920 --> 01:45:12.720
um, and layoffs are uniquely challenging and tough in that you have a lot of people leaving

01:45:14.000 --> 01:45:20.800
for reasons that aren't connected to their own performance or, you know, the culture not being

01:45:20.800 --> 01:45:27.840
a fit at that point. It's really just, it's a, it's a kind of strategy decision and sometimes

01:45:27.840 --> 01:45:34.560
financially required. Um, but not, not fully in our case, I mean, especially on the changes

01:45:34.560 --> 01:45:40.000
that we made this year, a lot of it was more kind of culturally and strategically driven by

01:45:40.000 --> 01:45:45.600
this push where I wanted us to become a, a stronger technology company with a more of a focus on

01:45:45.600 --> 01:45:51.440
building a more technical and more of a focus on building higher quality products faster.

01:45:52.080 --> 01:45:56.880
And I just view the external world is quite volatile right now. And I wanted to make sure

01:45:56.880 --> 01:46:02.800
that we had a stable position to be able to continue investing in these long-term

01:46:03.680 --> 01:46:08.320
ambitious projects that we have around, you know, continuing to push AI forward and

01:46:08.320 --> 01:46:12.720
continuing to push forward all the metaverse work. And in order to do that, in light of the

01:46:14.080 --> 01:46:18.560
pretty big thrash that we had seen over the last 18 months, you know, some of it,

01:46:19.120 --> 01:46:24.080
um, you know, macroeconomic induced, some of it specific, some of it competitively induced,

01:46:24.080 --> 01:46:30.320
some of it, um, just because of bad decisions, right? Or things that we got wrong. Um, I don't

01:46:30.320 --> 01:46:34.080
know, I just, I decided that we needed to get to a point where we were a lot leaner and

01:46:35.120 --> 01:46:39.040
but look, I mean, but then, okay, it's, it's one thing to do that to like decide that at a high

01:46:39.040 --> 01:46:43.440
level. Then the question is, how do you execute that as compassionately as possible? And there's no

01:46:43.440 --> 01:46:49.040
good way. Um, there's no perfect way for sure. And it's, it's, it's going to be tough no matter

01:46:49.040 --> 01:46:55.040
what, but I, you know, as a leadership team here, we've certainly spent a lot of time just

01:46:55.040 --> 01:47:00.960
thinking, okay, given that this is a thing that sucks, like what is the most compassionate way

01:47:00.960 --> 01:47:06.320
that we can do this? And, um, and that's what we've tried to do. And you mentioned there,

01:47:06.320 --> 01:47:13.680
there's an increased focus on, uh, engineering on tech. So the technology teams, tech focus

01:47:13.680 --> 01:47:22.080
teams on building products that. Yeah. I mean, I, I wanted to, I want to empower

01:47:23.840 --> 01:47:28.880
engineers more, the people who are building things, the tech, the technical teams. Um,

01:47:31.120 --> 01:47:35.920
part of that is making sure that the people who are building things aren't just at like the

01:47:35.920 --> 01:47:40.800
leaf nodes of the organization. I don't want like, you know, eight levels of management. And then

01:47:41.600 --> 01:47:45.440
the people actually doing the work. So we made changes to make it so that you have individual

01:47:45.440 --> 01:47:49.760
contributor engineers reporting at almost every level up the stack. Which I think is important

01:47:49.760 --> 01:47:54.560
because, you know, you're running a company, one of the big questions is, you know, latency of,

01:47:54.560 --> 01:47:58.720
of information that you get. You know, we talked about this a bit earlier in terms of

01:47:59.360 --> 01:48:04.960
kind of the joy of, of, of, in the, the feedback that you get doing something like jujitsu compared

01:48:04.960 --> 01:48:10.080
to running a long-term project. But I actually think part of the art of running a company is

01:48:10.080 --> 01:48:16.080
trying to constantly re-engineer it so that your feedback loops get shorter so you can learn faster.

01:48:16.080 --> 01:48:20.240
And part of the way that you do that is by, I kind of think that every, every layer that you have in

01:48:20.240 --> 01:48:27.280
the organization, um, means that information might not need to get reviewed before it goes to you.

01:48:27.280 --> 01:48:31.040
And I think, you know, making it so that the people doing the work are as close as possible to you

01:48:31.040 --> 01:48:37.280
as possible is, is, is pretty important. So there's that. And I think over time, companies just build

01:48:37.280 --> 01:48:43.360
up very large support functions that are not doing the kind of core technical work. And

01:48:43.920 --> 01:48:48.400
those functions are very important, but I think having them in the right proportion is, is important.

01:48:48.400 --> 01:48:55.360
And if, um, if you, you try to do good work, but you don't have, you know, the right, you know,

01:48:55.360 --> 01:49:01.040
marketing team or, um, or the right legal advice, like you're gonna, you know, make some pretty big

01:49:01.040 --> 01:49:08.160
blunders, but, um, but at the same time, if you have, you know, if, if you just like have too big

01:49:08.160 --> 01:49:14.320
of, of, of things and some of these support roles, then that might make it so that things are just

01:49:14.320 --> 01:49:21.120
move a lot. Um, maybe you're too conservative or you, you move a lot slower, um, uh, than,

01:49:21.120 --> 01:49:25.360
than, than you should otherwise. Those are just examples, but it's, um, but

01:49:25.360 --> 01:49:29.200
But how do you find that balance? It's really tough. Yeah. No, but that's, it's a constant

01:49:29.200 --> 01:49:34.080
equilibrium that you're, that you're searching for. Yeah. How many managers to have? What are the pros

01:49:34.080 --> 01:49:39.120
and cons of managers? Well, I mean, I, I believe a lot in management. I think there are some people

01:49:39.120 --> 01:49:43.280
who think that it doesn't matter as much, but look, I mean, we have a lot of younger people at the

01:49:43.280 --> 01:49:47.840
company for them. This is their first job and, you know, people need to grow and learn in their

01:49:47.840 --> 01:49:52.160
career and like that. All that stuff is important, but here's one mathematical way to look at it.

01:49:53.120 --> 01:50:01.280
Um, you know, at the beginning of this, we, um, I asked our, our people team was the average

01:50:01.280 --> 01:50:07.280
number of, of reports that a manager had. And I think it was, it was around three, maybe three

01:50:07.280 --> 01:50:14.320
to four, but closer to three. I was like, wow, like a manager can, you know, best practices that

01:50:14.320 --> 01:50:19.040
person can, can manage, you know, seven or eight people. Um, but there was a reason why it was

01:50:19.040 --> 01:50:24.080
closer to three. It was because we were growing so quickly, right? And when you're hiring so many

01:50:24.080 --> 01:50:30.080
people so quickly, then that means that you need managers who have capacity to onboard new people.

01:50:30.080 --> 01:50:34.800
Um, and also if you have a new manager, you may not want to have them have seven direct reports

01:50:34.800 --> 01:50:39.840
immediately because you want them to ramp up, but the thing is going forward. I don't want us to

01:50:39.840 --> 01:50:44.960
actually hire that many people that quickly, right? So I actually think we'll just do better work if

01:50:44.960 --> 01:50:49.920
we have more constraints and we're, um, you know, leaner as an organization. So in a world

01:50:49.920 --> 01:50:54.960
where we're not adding so many people as quickly, is it as valuable to have a lot of managers who

01:50:54.960 --> 01:51:00.160
have extra capacity waiting for new people? No, right? So, um, so now we can, we could sort of

01:51:00.160 --> 01:51:05.200
defragment the organization and get to a place where the average is closer to that seven or eight.

01:51:05.200 --> 01:51:10.800
Um, and it's, it's just ends up being a somewhat more kind of compact management structure,

01:51:10.800 --> 01:51:15.360
which, um, you know, decreases the latency on, on information going up and down the chain and,

01:51:16.240 --> 01:51:20.080
um, and I think empowers people more. But I mean, that's, that's an example that I think it doesn't

01:51:20.080 --> 01:51:26.400
kind of undervalue the importance of management and, and the, um, kind of the personal

01:51:27.600 --> 01:51:31.360
growth or coaching that people need in order to do their jobs. Well, it's just, I think

01:51:31.360 --> 01:51:34.720
realistically, we're just not going to hire as many people going forward. So I think that you

01:51:34.720 --> 01:51:41.120
need a different structure. This whole, this whole incredible hierarchy and network of humans that

01:51:41.120 --> 01:51:50.560
make up a company is fascinating. How do you hire great teams? How do you hire great now with the

01:51:50.560 --> 01:51:57.440
focus on engineering and technical teams? How do you hire great engineers and great members of

01:51:57.440 --> 01:52:04.400
technical teams? Well, you're asking how you select or how you attract them? Both, but select,

01:52:04.480 --> 01:52:10.960
I think, uh, I think attract is work on cool stuff and have a vision. I think that's right.

01:52:10.960 --> 01:52:14.000
And, and, and have a track record that people think you're actually going to be able to do it.

01:52:14.000 --> 01:52:20.160
Yeah. To me, the select is, seems like more of the art form, more of the tricky thing. Yeah.

01:52:20.160 --> 01:52:26.160
How do you select the people that fit the culture and can get integrated the most effectively and

01:52:26.160 --> 01:52:33.280
so on? And maybe, especially when they're young, to see like, to see the magic through the,

01:52:34.480 --> 01:52:38.000
through the resume, through the paperwork and all this kind of stuff, to see that there's a

01:52:38.000 --> 01:52:45.360
special human there that would do like incredible work. So there are lots of different cuts on

01:52:45.360 --> 01:52:51.200
this question. I mean, I think when an organization has grown quickly, one of the big questions that

01:52:51.200 --> 01:52:56.720
teams face is, do I hire this person who's in front of me now because they seem good?

01:52:57.440 --> 01:53:05.360
Or do I hold out to get someone who's even better? And the heuristic that I always focused on

01:53:06.080 --> 01:53:11.600
for myself and my own kind of direct hiring that I think works. So when you, when you

01:53:11.600 --> 01:53:16.400
recurse it through the organization is that you should only hire someone to be on your team,

01:53:16.400 --> 01:53:19.200
if you would be happy working for them in an alternate universe.

01:53:19.760 --> 01:53:24.560
And I think that that kind of works. And that's basically how I've tried to build my team. It's,

01:53:25.120 --> 01:53:29.440
you know, I'm not, I'm not in a rush to not be running the company. But I think in an

01:53:29.440 --> 01:53:33.040
alternate universe where one of these other folks was running the company, I'd be happy to work

01:53:33.040 --> 01:53:38.160
for them. I feel like I'd learn from them. I respect their kind of general judgment.

01:53:39.280 --> 01:53:46.240
They're all very insightful. They have good values. And I think that that gives you some rubric for

01:53:47.200 --> 01:53:51.440
you can apply that at every layer. And I think if you apply that at every layer in the organization,

01:53:51.440 --> 01:53:58.320
then you'll have a pretty strong organization. Okay, in an organization that's not growing as

01:53:58.320 --> 01:54:04.960
quickly, the questions might be a little different, though. And there you asked about young people

01:54:04.960 --> 01:54:12.080
specifically, like people out of college. And one of the things that we see is it's a pretty basic

01:54:12.160 --> 01:54:17.520
lesson. But like, we have a much better sense of who the best people are who have interned at the

01:54:17.520 --> 01:54:23.360
company for a couple of months, then by looking at them at kind of a resume or a short or a short

01:54:24.240 --> 01:54:28.320
interview loop. And obviously the in-person feel that you get from someone probably tells you

01:54:28.320 --> 01:54:35.360
more than the resume. And you can do some basic skills assessment. But a lot of the stuff really

01:54:35.360 --> 01:54:43.040
just is cultural. People thrive in different environments. And on different teams, even within

01:54:43.040 --> 01:54:50.400
a specific company, and it's like the people who come for even a short period of time over a summer

01:54:50.400 --> 01:54:54.960
who do a great job here, you know that they're going to be great if they came and joined full

01:54:54.960 --> 01:55:01.040
time. And that's one of the reasons why we've invested so much in internship is basically

01:55:01.360 --> 01:55:06.720
it's a very useful sorting function both for us and for the people who want to try out the company.

01:55:06.720 --> 01:55:11.440
You mentioned in-person, what do you think about remote work, a topic that's been discussed

01:55:11.440 --> 01:55:15.280
extensively over the past few years because of the pandemic?

01:55:15.840 --> 01:55:24.160
Yeah, I mean, I think it's a thing that's here to stay. But I think that there's value in both.

01:55:24.880 --> 01:55:31.840
It's not, I wouldn't want to run a fully remote company yet at least. I think there's an

01:55:31.840 --> 01:55:36.160
asterisk on that, which is that. Some of the other stuff you're working on, yeah.

01:55:36.160 --> 01:55:43.280
Yeah, exactly. It's like all the metaverse work and the ability to feel like you're truly present

01:55:44.640 --> 01:55:50.320
no matter where you are. I think once you have that all dialed in, then we may one day reach a

01:55:50.320 --> 01:55:55.840
point where it really just doesn't matter as much where you are physically. But

01:55:58.880 --> 01:56:05.840
I don't know, today it still does. For people who, there are all these people who have special

01:56:05.840 --> 01:56:10.800
skills and want to live in a place where we don't have an office, are we better off having

01:56:10.800 --> 01:56:16.240
them at the company? Absolutely. And are a lot of people who work at the company for several

01:56:16.240 --> 01:56:23.520
years and then build up the relationships internally and have the trust and have a sense

01:56:23.520 --> 01:56:27.360
of how the company works? Can they go work remotely now if they want and still do it as

01:56:27.360 --> 01:56:30.960
effectively? And we've done all these studies that show it's like, okay, does that affect their

01:56:30.960 --> 01:56:39.600
performance? It does not. But for the new folks who are joining and for people who are earlier

01:56:39.600 --> 01:56:44.080
in their career and need to learn how to solve certain problems and need to get ramped up on the

01:56:44.080 --> 01:56:50.480
culture, when you're working through really complicated problems where you don't just want

01:56:50.480 --> 01:56:54.880
to sit in the, you don't just want the formal meeting, but you want to be able to brainstorm

01:56:54.880 --> 01:56:59.360
when you're walking in the hallway together after the meeting. I don't know. It's like,

01:56:59.360 --> 01:57:08.880
we just haven't replaced the kind of in-person dynamics there yet with anything remote yet.

01:57:09.520 --> 01:57:13.440
Yeah, there's a magic to the in-person that we'll talk about this a little bit more,

01:57:13.440 --> 01:57:17.760
but I'm really excited by the possibilities in the next two years in virtual reality and mixed

01:57:17.760 --> 01:57:26.400
reality that are possible with high-resolution scans. I mean, I, as a person who loves in-person

01:57:26.400 --> 01:57:32.880
interaction, like these podcasts in person, it would be incredible to achieve the level of

01:57:32.880 --> 01:57:40.800
realism I've gotten the chance to witness. But let me ask about that. I got a chance to look at

01:57:41.520 --> 01:57:51.920
the Quest 3 headset and it is amazing. You've announced it. It's, you'll give some more details

01:57:51.920 --> 01:57:56.160
in the fall. Maybe release in the fall. When is it getting released again? I forgot, you mentioned it.

01:57:56.160 --> 01:57:58.880
We'll give more details at Connect. But it's coming this fall.

01:57:59.040 --> 01:58:09.520
Okay. So, it's priced at $4.99. What features are you most excited about there?

01:58:09.520 --> 01:58:15.040
There are basically two big new things that we've added to Quest 3 over Quest 2. The first is

01:58:15.040 --> 01:58:24.080
high-resolution mixed reality. And the basic idea here is that you can think about virtual reality

01:58:24.080 --> 01:58:30.560
as you have the headset and like all the pixels are virtual and you're basically like immersed in

01:58:30.560 --> 01:58:36.160
a different world. Mixed reality is where you see the physical world around you and you can place

01:58:36.160 --> 01:58:41.360
virtual objects in it, whether that's a screen to watch a movie or a projection of your virtual

01:58:41.360 --> 01:58:46.240
desktop or you're playing a game where like zombies are coming out through the wall and you need to

01:58:46.240 --> 01:58:50.640
shoot them. Or, you know, we're, you know, we're playing Dungeons and Dragons or some board game

01:58:50.640 --> 01:58:54.080
and we just have a virtual version of the board in front of us while we're sitting here.

01:58:55.600 --> 01:59:00.400
All that's possible in mixed reality. And I think that that is going to be the next big

01:59:00.400 --> 01:59:07.040
capability on top of virtual reality. It has done so well. I have to say, as a person who

01:59:07.040 --> 01:59:14.320
experienced it today with zombies, having a full awareness of the environment and integrating

01:59:14.320 --> 01:59:20.000
that environment in the way they run at you while they try to kill you. It's just the mixed

01:59:20.000 --> 01:59:24.560
reality that passed through is really, really, really well done. And the fact that it's only

01:59:24.560 --> 01:59:33.200
$500 is really, it's well done. Thank you. I mean, I'm super excited about it. I mean, we put a lot

01:59:33.200 --> 01:59:40.800
of work into making the device both as good as possible and as affordable as possible because

01:59:40.800 --> 01:59:46.160
a big part of our mission and ethos here is we want people to be able to connect with each other.

01:59:46.240 --> 01:59:50.160
We want to reach and we want to serve a lot of people, right? We want to bring this technology

01:59:50.160 --> 02:00:00.160
to everyone, right? So we're not just trying to serve like an elite, wealthy crowd. We really

02:00:00.160 --> 02:00:05.360
want this to be accessible. So that is, in a lot of ways, an extremely hard technical problem

02:00:05.360 --> 02:00:10.880
because we don't just have the ability to put an unlimited amount of hardware in this. We

02:00:10.880 --> 02:00:16.000
needed to basically deliver something that works really well but in an affordable package.

02:00:16.000 --> 02:00:25.120
And we started with Quest Pro last year. It was $1,500 and now we've lowered the price to $1,000.

02:00:25.120 --> 02:00:31.680
But in a lot of ways, the mixed reality in Quest 3 is an even better and more advanced level than

02:00:31.680 --> 02:00:37.360
what we were able to deliver in Quest Pro. So I'm really proud of where we are with Quest 3 on

02:00:37.360 --> 02:00:42.720
that. It's going to work with all of the virtual reality titles and everything that existed there.

02:00:42.720 --> 02:00:48.560
So people who want to play fully immersive games, social experiences, fitness, all that stuff will

02:00:48.560 --> 02:00:54.880
work. But now you'll also get mixed reality too, which I think people really like because it's

02:00:55.760 --> 02:01:00.800
sometimes you want to be super immersed in a game. But a lot of the time, especially when

02:01:00.800 --> 02:01:04.560
you're moving around, if you're active, like you're doing some fitness experience,

02:01:04.880 --> 02:01:09.920
you know, let's say you're doing boxing or something, it's like you kind of want to be able

02:01:09.920 --> 02:01:13.680
to see the room around you. So that way you know that like I'm not going to punch a lamp or something

02:01:13.680 --> 02:01:18.240
like that. And I don't know if you got to play with this experience, but we basically have the,

02:01:18.240 --> 02:01:23.440
and it's just sort of like a fun little demo that we put together. But it's like you just,

02:01:24.480 --> 02:01:29.760
we're like in a conference room or you're living room and you have the guy there and you're boxing

02:01:29.760 --> 02:01:33.520
him and you're fighting him. And it's like, Oh, the other people are there too. I got a chance to do

02:01:33.520 --> 02:01:40.320
that. And all the people are there. It's like that guy is right there. Yeah. It's like it's right

02:01:40.320 --> 02:01:44.240
in the room. And the other human, the path that you're seeing them also, they can cheer you on,

02:01:44.240 --> 02:01:49.040
they can make fun of you if there are anything like friends of mine. And then just it, yeah,

02:01:50.720 --> 02:01:57.280
it's really, it's a really compelling experience. I mean, VR is really interesting too, but this

02:01:57.280 --> 02:02:02.400
is something else almost. This is, this becomes integrated into your life, into your world.

02:02:03.520 --> 02:02:08.160
Yeah. And it, so I think it's a completely new capability that will unlock a lot of different

02:02:08.160 --> 02:02:13.040
content. And I think it'll also just make the experience more comfortable for a set of people

02:02:13.040 --> 02:02:17.840
who didn't want to have only fully immersive experiences. I think if you want experiences

02:02:17.840 --> 02:02:21.600
where you're grounded in, you know, your living room in the physical world around you,

02:02:21.600 --> 02:02:24.880
now you'll be able to have that too. And I think that that's pretty exciting.

02:02:24.880 --> 02:02:30.960
I really liked how it added windows to a room with no windows. Yeah.

02:02:30.960 --> 02:02:34.480
Me as a person. Did you see the aquarium one where you could see the sharks swim up or was

02:02:34.480 --> 02:02:38.560
that just a zombie one? Just a zombie one, but it's still outside. You don't necessarily want

02:02:38.560 --> 02:02:42.480
windows added to your living room where zombies come out of, but yeah, so the context of that

02:02:42.480 --> 02:02:48.560
game, it's, yeah, yeah. I enjoyed it because you could see the nature outside. And me as a person

02:02:48.560 --> 02:02:55.760
that doesn't have windows, it's just nice to have nature. Yeah. Well, even if it's a mixed reality

02:02:55.760 --> 02:03:03.200
setting, I know it's a zombie game, but there's a zen nature, zen aspect to being able to look

02:03:03.200 --> 02:03:11.120
outside and alter your environment as you know it. Yeah. And there will probably be better,

02:03:11.120 --> 02:03:15.120
more zen ways to do that than the zombie game you're describing, but you're right that the

02:03:15.120 --> 02:03:21.360
basic idea of sort of having your physical environment on pass through, but then being able

02:03:21.360 --> 02:03:29.040
to bring in different elements, I think it's going to be super powerful. And in some ways,

02:03:29.040 --> 02:03:34.960
I think that these are, mixed reality is also a predecessor to eventually we will get AR glasses

02:03:34.960 --> 02:03:40.960
that are not kind of the goggles form factor of the current generation of headsets that people

02:03:40.960 --> 02:03:46.320
are making. But I think a lot of the experiences that developers are making for mixed reality of

02:03:46.320 --> 02:03:50.720
basically you just have a kind of a hologram that you're putting in the world will hopefully apply

02:03:51.200 --> 02:03:55.920
once we get the AR glasses too. Now that's got its own whole set of challenges and it's...

02:03:56.640 --> 02:04:01.600
Well, the headset's already smaller than the previous version. Oh yeah, it's 40% thinner.

02:04:01.600 --> 02:04:05.120
And the other thing that I think is good about it, yeah, so mixed reality was the first big thing.

02:04:05.840 --> 02:04:12.800
The second is it's just a great VR headset. I mean, it's got 2x the graphics processing power,

02:04:13.760 --> 02:04:20.080
40% sharper screens, 40% thinner, more comfortable, better strap architecture,

02:04:20.080 --> 02:04:24.000
all this stuff that if you liked Quest 2, I think that this is just going to be...

02:04:24.000 --> 02:04:27.360
It's like all the content that you might have played in Quest 2 is just going to be sharper

02:04:27.360 --> 02:04:31.600
automatically and look better in this. So it's... I think people are really going to like it.

02:04:31.600 --> 02:04:32.560
Yeah, so this fall...

02:04:33.520 --> 02:04:40.880
At this fall, I have to ask, Apple just announced a mixed reality headset called Vision Pro

02:04:41.440 --> 02:04:47.120
for $3,500 available in early 2024. What do you think about this headset?

02:04:48.880 --> 02:04:53.760
Well, I saw the materials when they launched. I haven't gotten a chance to play with it yet.

02:04:53.760 --> 02:04:59.600
So kind of take everything with a grain of salt, but a few high-level thoughts. I mean, first,

02:05:00.480 --> 02:05:09.200
I do think that this is a certain level of validation for the category,

02:05:09.760 --> 02:05:15.520
where we were the primary folks out there before saying, hey, I think that this

02:05:16.800 --> 02:05:20.320
virtual reality, augmented reality, mixed reality, this is going to be a big part of

02:05:20.320 --> 02:05:27.920
the next computing platform. I think having Apple come in and share that vision

02:05:30.320 --> 02:05:36.080
will make a lot of people who are fans of their products really consider that.

02:05:37.600 --> 02:05:45.680
And then, of course, the $3,500 price... On the one hand, I get it with all the stuff

02:05:45.680 --> 02:05:49.520
that they're trying to pack in there. On the other hand, a lot of people aren't going to find that

02:05:49.520 --> 02:05:55.360
to be affordable. So I think that there's a chance that them coming in actually increases demand

02:05:56.320 --> 02:06:01.280
for the overall space. And that Quest 3 is actually the primary beneficiary of that,

02:06:01.280 --> 02:06:08.000
because a lot of the people who might say, hey, I'm going to give another consideration to this.

02:06:09.200 --> 02:06:14.320
Now I understand maybe what mixed reality is more. And Quest 3 is the best one on the market

02:06:14.320 --> 02:06:20.800
that I can afford. And it's great also. I think that that's... And in our own way,

02:06:20.800 --> 02:06:23.840
I think there are a lot of features that we have where we're leading on.

02:06:25.520 --> 02:06:29.920
So I think that that I think is going to be a very... That could be quite good.

02:06:31.760 --> 02:06:34.400
And then obviously, over time, the companies are just focused on

02:06:35.520 --> 02:06:41.040
somewhat different things. Apple has always, I think, focused on building

02:06:42.480 --> 02:06:51.520
really high-end things, whereas our focus has been on... We have a more democratic ethos. We

02:06:51.520 --> 02:06:58.640
want to build things that are accessible to a wider number of people. We've sold tens of millions

02:06:58.640 --> 02:07:06.080
of Quest devices. My understanding, just based on rumors, I don't have any special knowledge on

02:07:06.080 --> 02:07:12.640
this, is that Apple is building about one million of their device. So just in terms of what you

02:07:12.640 --> 02:07:21.120
kind of expect in terms of sales numbers, I just think that this is... Quest is going to be the

02:07:21.200 --> 02:07:25.920
primary thing that people in the market will continue using for the foreseeable future. And

02:07:25.920 --> 02:07:29.920
then obviously, over the long term, it's up to the companies to see how well we've executed the

02:07:29.920 --> 02:07:33.440
different things that we're doing. But we kind of come at it from different places. We're very

02:07:33.440 --> 02:07:41.520
focused on social interaction, communication, being more active, right? So there's fitness,

02:07:41.520 --> 02:07:47.440
there's gaming, there are those things. Whereas I think a lot of the use cases that you saw in

02:07:47.440 --> 02:07:55.200
Apple's launch material were more around people sitting, people looking at screens,

02:07:56.240 --> 02:08:01.840
which are great. I think that you will replace your laptop over time with a headset. But I think

02:08:01.840 --> 02:08:05.920
in terms of how the different use cases that the companies are going after,

02:08:07.680 --> 02:08:10.240
they're a bit different for where we are right now.

02:08:10.240 --> 02:08:14.880
Yeah, so gaming wasn't a big part of the presentation, which is an interesting...

02:08:15.120 --> 02:08:23.280
It feels like mixed reality gaming is such a big part of that. It was interesting to see it

02:08:23.280 --> 02:08:27.040
missing in the presentation. Well, I mean, look, there are certain design trade-offs

02:08:27.040 --> 02:08:34.160
in this where they made this point about not wanting to have controllers, which on the one hand,

02:08:35.280 --> 02:08:38.560
there's a certain elegance about just being able to navigate the system with

02:08:39.360 --> 02:08:44.560
eye gaze and hand tracking. And by the way, you'll be able to just navigate Quest with your hands

02:08:44.640 --> 02:08:51.600
too, if that's what you want. One of the things I should mention is the capability from the cameras

02:08:53.120 --> 02:08:57.360
with computer vision to detect certain aspects of the hand, allowing you to have a controller that

02:08:57.360 --> 02:09:02.880
doesn't have that ring thing. Yeah, the hand tracking in Quest 3 and the controller tracking is

02:09:02.880 --> 02:09:09.040
a big step up from the last generation. And one of the demos that we have is basically

02:09:09.600 --> 02:09:13.680
an MR experience teaching you how to play piano, where it basically highlights the notes that you

02:09:13.680 --> 02:09:18.960
need to play and it's like just all its hands, it's no controllers. But I think if you care about

02:09:18.960 --> 02:09:27.120
gaming, having a controller allows you to have a more tactile feel and allows you to capture

02:09:28.880 --> 02:09:35.200
fine motor movement much more precisely than what you can do with hands without something

02:09:35.200 --> 02:09:39.840
that you're touching. So again, I think there are certain questions which are just around

02:09:39.840 --> 02:09:46.960
what use cases are you optimizing for. I think if you want to play games, then I think that that,

02:09:46.960 --> 02:09:52.720
then I think you want to design the system in a different way and we're more focused on kind of

02:09:52.720 --> 02:09:59.840
social experiences, entertainment experiences. Whereas if what you want is to make sure that

02:10:00.720 --> 02:10:05.520
the text that you read on a screen is as crisp as possible, then you need to make the

02:10:06.240 --> 02:10:12.240
design and cost trade-offs that they made that lead you to making a $3,500 device.

02:10:12.240 --> 02:10:15.520
So I think that there is a use case for that for sure, but I just think that they're,

02:10:16.800 --> 02:10:22.720
the company is, we've basically made different design trade-offs to get to the use cases that

02:10:22.720 --> 02:10:28.320
we're trying to serve. There's a lot of other stuff I'd love to talk to you about about the

02:10:28.320 --> 02:10:33.920
metaverse, especially the Kodak avatar, which I've gotten to experience a lot of different

02:10:33.920 --> 02:10:36.800
variations of recently that I'm really, really excited about.

02:10:36.800 --> 02:10:38.640
Yeah, I'm excited to talk about that too.

02:10:38.640 --> 02:10:41.040
Yeah, I'll have to wait a little bit because,

02:10:43.840 --> 02:10:49.520
well, I think there's a lot more to show off in that regard. But let me step back to AI.

02:10:50.080 --> 02:10:55.760
I think we've mentioned it a little bit, but I'd like to linger on this question that

02:10:56.080 --> 02:11:03.120
folks like Eliezer Yatkowski has a worry about and others of the existential,

02:11:03.120 --> 02:11:08.160
of the serious threats of AI that have been reinvigorated now with the rapid developments

02:11:08.160 --> 02:11:17.360
of AI systems. Do you worry about the existential risks of AI as Eliezer does about the alignment

02:11:17.360 --> 02:11:19.520
problem about this getting out of hand?

02:11:20.480 --> 02:11:25.760
Any time where there's a number of serious people who are raising a concern that is that

02:11:25.760 --> 02:11:30.240
existential about something that you're involved with, I think you have to think about it.

02:11:32.080 --> 02:11:34.800
I've spent quite a bit of time thinking about it from that perspective.

02:11:40.800 --> 02:11:44.240
Where I basically have come out on this for now is I do think that there are,

02:11:44.480 --> 02:11:50.320
over time, I think that we need to think about this even more as we approach something that

02:11:51.360 --> 02:11:55.120
could be closer to superintelligence. I just think it's pretty clear to anyone working on these

02:11:55.120 --> 02:12:03.440
projects today that we're not there. One of my concerns is that we spent a fair amount of time

02:12:03.440 --> 02:12:13.360
on this before, but there are more, I don't know if mundane is the right word, but there's concerns

02:12:13.360 --> 02:12:20.400
that already exist right about people using AI tools to do harmful things of the type that we're

02:12:20.400 --> 02:12:24.960
already aware, whether we talked about fraud or scams or different things like that.

02:12:27.600 --> 02:12:32.880
That's going to be a pretty big set of challenges that the companies working on this are going to

02:12:32.880 --> 02:12:39.040
need to grapple with regardless of whether there is an existential concern as well at some point

02:12:39.040 --> 02:12:50.240
down the road. I do worry that to some degree people can get a little too focused on some of

02:12:50.240 --> 02:12:57.200
the tail risk and then not do as good of a job as we need to on the things that can be almost

02:12:57.200 --> 02:13:04.800
certain are going to come down the pipe as real risks that manifest themselves in the near term.

02:13:04.800 --> 02:13:13.680
For me, I've spent most of my time on that once I kind of made the realization that the size of

02:13:13.680 --> 02:13:18.320
models that we're talking about now in terms of what we're building are quite far from the

02:13:18.320 --> 02:13:24.320
superintelligence type concerns that people raise. But I think once we get a couple steps closer to

02:13:24.320 --> 02:13:32.480
that, I know as we do get closer, I think that there are going to be some novel risks and issues

02:13:32.480 --> 02:13:37.760
about how we make sure that the systems are safe for sure. I guess here, just to take the

02:13:37.760 --> 02:13:44.480
conversation in a somewhat different direction, I think in some of these debates around safety,

02:13:45.600 --> 02:13:54.720
I think the concepts of intelligence and autonomy or like the being of the thing

02:13:56.480 --> 02:14:02.080
as an analogy, they get kind of conflated together. And I think it very well could

02:14:02.160 --> 02:14:13.120
be the case that you can make something in scale intelligence quite far, but that may not manifest

02:14:13.760 --> 02:14:17.920
the safety concerns that people are saying in the sense that I mean, just if you look at human

02:14:17.920 --> 02:14:22.080
biology, it's like, all right, we have our neocortexes where all the thinking happens.

02:14:23.520 --> 02:14:28.240
But it's not really calling the shots at the end of the day. We have a much more primitive

02:14:29.200 --> 02:14:35.360
old brain structure for which our neocortex, which is this powerful machinery, is basically just a

02:14:36.080 --> 02:14:47.760
prediction and reasoning engine to help our very simple brain decide how to plan and do what it

02:14:47.760 --> 02:14:55.040
needs to do in order to achieve these very basic impulses. And I think that you can think about

02:14:55.120 --> 02:15:02.320
some of the development of intelligence along the same lines where just like our neocortex doesn't

02:15:02.320 --> 02:15:08.560
have free will or autonomy, we might develop these wildly intelligent systems that are

02:15:09.120 --> 02:15:14.880
much more intelligent than our neocortex have much more capacity, but are in the same way that

02:15:14.880 --> 02:15:21.600
our neocortex is sort of subservient and is used as a tool by our kind of simple impulse brain.

02:15:22.080 --> 02:15:27.280
It's, you know, I think that it's not out of the question that very intelligent systems that

02:15:27.280 --> 02:15:32.880
have the capacity to think will kind of act as that is sort of an extension of the neocortex

02:15:32.880 --> 02:15:40.160
doing that. So I think my own view is that where we really need to be careful is on the development

02:15:40.160 --> 02:15:47.440
of autonomy and how we think about that. Because it's actually the case that

02:15:48.400 --> 02:15:53.680
relatively simple and unintelligent things that have runaway autonomy and just spread themselves

02:15:53.680 --> 02:15:57.600
or, you know, it's like, we have a word for that, it's a virus, right? It's, I mean, like,

02:15:57.600 --> 02:16:01.840
it's can be simple computer code that is not particularly intelligent, but just spreads itself

02:16:01.840 --> 02:16:11.120
and does a lot of harm, you know, biologically or computer. And I just think that these are

02:16:11.120 --> 02:16:16.560
somewhat separable things. And a lot of what I think we need to develop when people talk about

02:16:16.560 --> 02:16:23.200
safety and responsibility is really the governance on the autonomy that can be given to systems.

02:16:23.760 --> 02:16:28.800
And to me, if, you know, if I were, you know, a policymaker or thinking about this,

02:16:29.520 --> 02:16:33.200
I would really want to think about that distinction between these where I think building

02:16:33.200 --> 02:16:38.800
intelligent systems will be, can create a huge advance in terms of people's quality of life and

02:16:40.000 --> 02:16:46.240
productivity growth in the economy. But it's the autonomy part of this that I think we really

02:16:46.320 --> 02:16:50.720
need to make progress on how to govern these things responsibly before we

02:16:52.640 --> 02:16:58.480
build the capacity for them to make a lot of decisions on their own or give them goals or

02:16:59.440 --> 02:17:02.720
things like that. And I know that that's a research problem, but I do think that to some

02:17:02.720 --> 02:17:08.800
degree these are somewhat, are somewhat separable things. I love the distinction between intelligence

02:17:08.800 --> 02:17:18.080
and autonomy and the metaphor within your cortex. Let me ask about power. So building

02:17:18.080 --> 02:17:25.040
superintelligence systems, even if it's not in the near term, I think meta as is one of the few

02:17:25.040 --> 02:17:32.000
companies, if not the main company that will develop the superintelligence system. And you

02:17:32.000 --> 02:17:37.040
are a man who's at the head of this company, building AGI might make you the most powerful

02:17:37.040 --> 02:17:39.920
man in the world. Do you worry that that power will corrupt you?

02:17:42.560 --> 02:17:49.600
What a question. I mean, look, I think realistically, this gets back to the open

02:17:49.600 --> 02:17:55.680
source things that we talked about before, which is, I don't think that the world will be best

02:17:55.680 --> 02:18:06.960
served by any small number of organizations having this without it being something that is more

02:18:06.960 --> 02:18:14.400
broadly available. And I think if you look through history, it's when there are these sort of like

02:18:14.400 --> 02:18:21.280
unipolar advances and things that, and like power imbalances that they're doing to being kind of

02:18:22.240 --> 02:18:27.920
weird situations. So this is one of the reasons why I think open sources is generally

02:18:29.600 --> 02:18:34.880
the right approach. And I think it's a categorically different question today when we're

02:18:34.880 --> 02:18:38.320
not close to superintelligence. I think that there's a good chance that even once we get

02:18:38.320 --> 02:18:42.640
closer to superintelligence, open sourcing remains the right approach, even though I think at that

02:18:42.640 --> 02:18:49.280
point it's a somewhat different debate. But I think part of that is that that is, I think,

02:18:49.280 --> 02:18:54.400
one of the best ways to ensure that the system is as secure and safe as possible, because it's

02:18:54.400 --> 02:18:59.440
not just about a lot of people having access to it. It's the scrutiny that kind of comes with

02:19:00.640 --> 02:19:04.640
building an open source system, but I think that this is a pretty widely accepted thing about

02:19:04.640 --> 02:19:12.000
open sources that you have the code out there, so anyone can see the vulnerabilities. Anyone can

02:19:12.000 --> 02:19:16.240
kind of mess with it in different ways. People can spin off their own projects and experiment

02:19:16.240 --> 02:19:21.440
in a ton of different ways. And the net result of all of that is that the systems just get hardened

02:19:22.000 --> 02:19:31.840
and get to be a lot safer and more secure. So I think that there's a chance that that ends up being

02:19:31.840 --> 02:19:40.480
the way that this goes to, a pretty good chance, and that having this be open both leads to a

02:19:40.480 --> 02:19:47.600
healthier development of the technology and also leads to a more balanced distribution of the

02:19:47.600 --> 02:19:54.560
technology in a way that that strike me as good values to aspire to. So to you, the risks, there's

02:19:54.560 --> 02:20:00.720
risks to open sourcing, but the benefits outweigh the risks. At the two, it's interesting. I think

02:20:00.720 --> 02:20:08.160
the way you put it, you put it well that there's a different discussion now than when we get closer

02:20:09.120 --> 02:20:15.360
to the development of superintelligence of the benefits and risks of open sourcing.

02:20:15.360 --> 02:20:20.560
Yeah, and to be clear, I feel quite confident in the assessment that open sourcing models now

02:20:21.200 --> 02:20:26.880
is net positive. I think there's a good argument that in the future it will be too,

02:20:26.880 --> 02:20:32.160
even as you get closer to superintelligence, but I've certainly not decided on that yet,

02:20:32.160 --> 02:20:36.240
and I think that it becomes a somewhat more complex set of questions that I think people

02:20:36.240 --> 02:20:40.640
will have time to debate and will also be informed by what happens between now and then

02:20:40.640 --> 02:20:44.480
to make those decisions. We don't have to necessarily just debate that in theory right now.

02:20:45.040 --> 02:20:47.520
What year do you think we'll have a superintelligence?

02:20:49.920 --> 02:20:54.480
I don't know. I mean, that's pure speculation. I think it's very clear,

02:20:54.480 --> 02:20:57.680
just taking a step back, that we had a big breakthrough in the last year where the

02:20:58.320 --> 02:21:04.080
LLMs and diffusion models basically reached a scale where they're able to do some pretty

02:21:04.080 --> 02:21:08.720
interesting things. Then I think the question is what happens from here. Just to paint the two

02:21:08.720 --> 02:21:18.400
extremes, on one side, it's like, okay, we just had one breakthrough. If we just have another

02:21:18.400 --> 02:21:23.920
breakthrough like that, or maybe two, then we can have something that's truly crazy, right, and is

02:21:26.560 --> 02:21:32.480
just so much more advanced. On that side of the argument, it's like, okay, well, maybe we're

02:21:34.720 --> 02:21:42.560
only a couple of big steps away from reaching something that looks more like general intelligence.

02:21:43.200 --> 02:21:47.520
Okay, that's one side of the argument. On the other side, which is what we've historically

02:21:47.520 --> 02:21:58.160
seen a lot more, is that a breakthrough leads to, in that Gartner hype cycle, there's the hype,

02:21:58.160 --> 02:22:02.800
and then there's the trough of disillusionment after when people think that there's a chance that,

02:22:02.800 --> 02:22:06.160
hey, okay, there's a big breakthrough. Maybe we're about to get another big breakthrough, and it's

02:22:06.160 --> 02:22:10.240
like, actually, you're not about to get another breakthrough. Maybe you're actually just going

02:22:10.240 --> 02:22:18.640
to have to sit with this one for a while, and it could be five years. It could be 10 years.

02:22:18.640 --> 02:22:25.120
It could be 15 years until you figure out the next big thing that needs to get figured out.

02:22:27.040 --> 02:22:29.360
But I think that the fact that we just had this breakthrough

02:22:29.600 --> 02:22:36.240
sort of makes it that we're at a point of almost a very wide error bars on what happens next.

02:22:37.760 --> 02:22:44.480
I think the traditional technical view of looking at the industry would suggest that we're not just

02:22:44.480 --> 02:22:49.520
going to stack in a breakthrough on top of breakthrough on top of breakthrough every

02:22:50.400 --> 02:22:56.560
six months or something right now. I would guess that it will take somewhat longer in

02:22:56.560 --> 02:23:03.360
between these, but I don't know. I tend to be pretty optimistic about breakthroughs, too.

02:23:03.360 --> 02:23:09.600
So I think if you're normalized for my normal optimism, then maybe it would be even slower

02:23:09.600 --> 02:23:14.400
than what I'm saying. But even within that, I'm not even opining on the question of how many

02:23:14.400 --> 02:23:17.360
breakthroughs are required to get to general intelligence because no one knows.

02:23:18.880 --> 02:23:25.840
But this particular breakthrough was such a small step that resulted in such a big leap

02:23:25.840 --> 02:23:31.680
in performance as experienced by human beings that it makes you think, wow,

02:23:33.120 --> 02:23:38.480
as we stumble across this very open world of research, will we stumble

02:23:40.480 --> 02:23:43.600
across another thing that will have a giant leap in performance?

02:23:45.840 --> 02:23:52.000
And also, we don't know exactly at which stage is it really going to be impressive

02:23:52.000 --> 02:23:56.800
because it feels like it's really encroaching on impressive levels of intelligence.

02:23:57.840 --> 02:24:01.920
You still didn't answer the question of what year we're going to have superintelligence.

02:24:01.920 --> 02:24:05.280
I'd like to hold you to that. No, I'm just kidding. But is there something

02:24:06.080 --> 02:24:13.440
you could say about the timeline as you think about the development of AGI, superintelligence

02:24:13.440 --> 02:24:21.680
systems? Sure. So I still don't think I have any particular insight on when a singular AI system

02:24:21.680 --> 02:24:25.920
that is a general intelligence will get created. But I think the one thing that most people

02:24:27.360 --> 02:24:30.720
in the discourse that I've seen about this haven't really grappled with is that

02:24:30.720 --> 02:24:38.960
we do seem to have organizations and structures in the world that exhibit greater than human

02:24:38.960 --> 02:24:47.440
intelligence already. So one example is a company. It acts as an entity. It has a singular brand.

02:24:48.400 --> 02:24:54.320
Obviously, it's a collection of people. But I certainly hope that meta with tens of thousands

02:24:54.320 --> 02:24:58.960
of people makes smarter decisions than one person. But I think that that would be pretty bad if it

02:24:58.960 --> 02:25:07.680
didn't. Another example that I think is even more removed from the way we think about the

02:25:07.680 --> 02:25:12.640
personification of intelligence, which is often implied in some of these questions,

02:25:13.280 --> 02:25:19.120
is think about something like the stock market. The stock market takes inputs. It's a distributed

02:25:19.120 --> 02:25:25.440
system. It's like the cybernetic organism that probably millions of people around the world

02:25:26.240 --> 02:25:31.760
are basically voting every day by choosing what to invest in. But it's basically this

02:25:33.680 --> 02:25:42.160
organism or structure that is smarter than any individual that we use to allocate capital

02:25:42.240 --> 02:25:47.040
as efficiently as possible around the world. And I do think that

02:25:49.760 --> 02:25:56.960
this notion that there are already these cybernetic systems that are either melding

02:25:58.400 --> 02:26:02.800
the intelligence of multiple people together or melding the intelligence of multiple people

02:26:02.800 --> 02:26:10.640
and technology together to form something which is dramatically more intelligent than any individual

02:26:11.200 --> 02:26:19.200
in the world is something that seems to exist and that we seem to be able to harness

02:26:20.000 --> 02:26:25.520
in a productive way for our society as long as we basically build these structures and balance

02:26:25.520 --> 02:26:32.720
with each other. So I don't know. I mean, that at least gives me hope that as we advance the

02:26:32.720 --> 02:26:36.560
technology, and I don't know how long exactly it's going to be, but you asked when is this going to

02:26:36.560 --> 02:26:41.520
exist. I think to some degree, we already have many organizations in the world that are smarter

02:26:41.520 --> 02:26:46.800
than a single human. And that seems to be something that is generally productive in advancing humanity.

02:26:46.800 --> 02:26:52.160
And somehow the individual AI systems empower the individual humans and the interaction between

02:26:52.160 --> 02:26:57.680
those humans to make that collective intelligence machinery that you're referring to smarter.

02:26:57.680 --> 02:27:01.520
So it's not like AI is becoming super intelligent. It's just becoming the

02:27:02.480 --> 02:27:06.960
the engine that's making the collective intelligence is primarily human, more intelligent.

02:27:08.080 --> 02:27:13.120
Yeah, it's educating the humans better. It's making them better informed. It's

02:27:14.320 --> 02:27:19.760
making it more efficient for them to communicate effectively and debate ideas. And through that

02:27:19.760 --> 02:27:23.600
process, just making the whole collective intelligence more and more and more intelligent,

02:27:24.320 --> 02:27:28.560
maybe faster than the individual AI systems that are trained on human data anyway,

02:27:29.360 --> 02:27:34.000
are becoming. Maybe the collective intelligence of the human species might outpace the development

02:27:34.000 --> 02:27:41.040
of AI. I think there's a balance in here, because if a lot of the input that

02:27:42.480 --> 02:27:46.720
the systems are being trained on is basically coming from feedback from people,

02:27:47.600 --> 02:27:53.200
then a lot of the development does need to happen in human time. It's not like a machine

02:27:54.480 --> 02:27:58.160
will just be able to go learn all the stuff about how people think about stuff.

02:27:58.400 --> 02:28:04.560
There's a cycle to how this needs to work. This is an exciting world we're living in,

02:28:04.560 --> 02:28:09.440
and that you're at the forefront of developing. One of the ways you keep yourself humble,

02:28:09.440 --> 02:28:15.280
like we mentioned with Jiu Jitsu, is doing some really difficult challenges, mental and physical.

02:28:16.080 --> 02:28:22.880
One of those you've done very recently is the Murph challenge, and you got a really good time.

02:28:22.880 --> 02:28:27.840
It's 100 pull-ups, 200 push-ups, 200 squats, and a mile before and a mile around after.

02:28:28.800 --> 02:28:36.080
You got under 40 minutes on that. What was the hardest part? I think a lot of people

02:28:36.080 --> 02:28:41.920
were very impressed. It's a very impressive time. How crazy are you on this?

02:28:43.920 --> 02:28:47.200
It wasn't my best time, but anything under 40 minutes I'm happy with.

02:28:48.080 --> 02:28:49.040
It wasn't your best time?

02:28:49.600 --> 02:28:57.200
No, I think I've done it a little faster before, but not much. Of my friends, I did not win

02:28:57.280 --> 02:29:00.960
on Memorial Day. One of my friends did it actually several minutes faster than me,

02:29:01.840 --> 02:29:06.560
but just to clear up one thing that I think was, I saw a bunch of questions about this on the

02:29:06.560 --> 02:29:12.080
internet. There are multiple ways to do the Murph challenge. There's a kind of partitioned mode

02:29:12.080 --> 02:29:18.560
where you do sets of pull-ups, push-ups, and squats together, and then there's unpartitioned

02:29:18.560 --> 02:29:25.840
where you do the 100 pull-ups, and then the 200 push-ups, and then the 300 squats in cereal.

02:29:25.840 --> 02:29:32.480
Obviously, if you're doing them unpartitioned, then it takes longer to get through the 100

02:29:32.480 --> 02:29:36.320
pull-ups because anytime you're resting in between the pull-ups, you're not also doing

02:29:36.320 --> 02:29:43.840
push-ups and squats. I'm sure my unpartition time would be quite a bit slower, but no,

02:29:43.840 --> 02:29:50.480
I think at the end of this, first of all, I think it's a good way to honor Memorial Day.

02:29:51.200 --> 02:30:00.800
It's this Lieutenant Murphy, basically. This was one of his favorite exercises,

02:30:00.800 --> 02:30:05.360
and I just try to do it on Memorial Day each year, and it's a good workout.

02:30:06.720 --> 02:30:12.960
I got my older daughters to do it with me this time. My oldest daughter wants a weight vest

02:30:12.960 --> 02:30:16.800
because she sees me doing it with a weight vest. I don't know if a seven-year-old should be using

02:30:16.800 --> 02:30:23.360
a weight vest to do pull-ups. The difficult question a parent must ask themselves, yes.

02:30:23.360 --> 02:30:26.960
I was like, maybe I can make you a very light weight vest, but I didn't think it was good for

02:30:26.960 --> 02:30:33.280
this. She basically did a quarter-merf, so she ran a quarter-mile, and then did 25 pull-ups,

02:30:33.280 --> 02:30:40.880
50 push-ups, and 75 air squats, then ran another quarter-mile, and 15 minutes,

02:30:41.120 --> 02:30:48.240
which I was pretty impressed by in my five-year-old, too. I was excited about that,

02:30:48.240 --> 02:30:56.640
and I'm glad that I'm teaching them the value of physicality. I think a good day for Max,

02:30:56.640 --> 02:31:00.800
my daughter, is when she gets to go to the gym with me and cranks out a bunch of pull-ups,

02:31:00.800 --> 02:31:07.680
and I love that about her. I think it's good. She's, hopefully, I'm teaching her some good

02:31:07.680 --> 02:31:13.680
lessons. The broader question here is, given how busy you are, given how much stuff you have

02:31:13.680 --> 02:31:24.160
going on in your life, what's the perfect exercise regimen for you to keep yourself happy,

02:31:25.280 --> 02:31:28.320
to keep yourself productive in your main line of work?

02:31:29.280 --> 02:31:38.560
Yes. I mean, right now, I'm focused most of my workouts on fighting, so Jiu-Jitsu and MMA,

02:31:40.480 --> 02:31:44.960
but I don't know. I mean, maybe if you're a professional, you can do that every day. I can't.

02:31:44.960 --> 02:31:51.120
I just get too many bruises and things that you need to recover from, so I do that three

02:31:51.120 --> 02:31:57.760
to four times a week, and then the other day, I just try to do a mix of things,

02:31:57.920 --> 02:32:00.880
like just cardio conditioning, strength building, mobility.

02:32:01.920 --> 02:32:04.000
So you try to do something physical every day?

02:32:04.000 --> 02:32:09.520
Yeah, I try to, unless I'm just so tired that I just need to relax, but then I'll still try to go

02:32:09.520 --> 02:32:14.800
for a walk or something. I mean, even here, I don't know. Have you been on the roof here yet?

02:32:14.800 --> 02:32:16.400
No. I heard things.

02:32:16.400 --> 02:32:21.040
But it's like, we designed this building and I put a park on the roof, so that way,

02:32:21.040 --> 02:32:25.040
that's like my meetings when I'm just doing kind of a one-on-one or talking to a couple of people.

02:32:25.760 --> 02:32:30.640
I have a very hard time just sitting. I feel like you get super stiff. It feels really bad,

02:32:32.960 --> 02:32:36.880
but I don't know. Being physical is very important to me. I think it's,

02:32:37.600 --> 02:32:44.000
I do not believe this gets to the question about AI. I don't think that a being is just a mind.

02:32:44.960 --> 02:32:52.160
And I think we're kind of meant to do things, and like physically, and a lot of the sensations that

02:32:52.240 --> 02:32:58.160
we feel are connected to that. And I think that that's a lot of what makes you a human,

02:32:58.160 --> 02:33:09.040
is basically having that set of sensations and experiences around that, coupled with a mind

02:33:09.040 --> 02:33:18.880
to reason about them. But I don't know. I think it's important for balance to kind of get out,

02:33:18.880 --> 02:33:22.320
challenge yourself in different ways, learn different skills, clear your mind.

02:33:23.360 --> 02:33:28.640
Do you think AI, in order to become super intelligent, needs you to have a body?

02:33:31.520 --> 02:33:39.280
It depends on what the goal is. I think that there's this assumption in that question that

02:33:40.240 --> 02:33:46.640
intelligence should be kind of person-like. Whereas, as we were just talking about,

02:33:47.600 --> 02:33:54.640
you can have these greater than single human intelligent organisms, like the stock market,

02:33:54.640 --> 02:34:01.440
which obviously do not have bodies and do not speak a language, and just kind of have their own

02:34:02.960 --> 02:34:11.520
system. So I don't know. My guess is there will be limits to what a system that is purely an

02:34:11.520 --> 02:34:17.440
intelligence can understand about the human condition without having the same, not just

02:34:17.440 --> 02:34:27.200
senses, but our body's changes we get older. And we kind of evolve and let those very subtle

02:34:29.120 --> 02:34:36.240
physical changes just drive a lot of social patterns and behavior around when you choose to

02:34:36.240 --> 02:34:41.360
have kids. Just like all these, that's not even subtle. That's a major one. But how

02:34:41.360 --> 02:34:48.560
you design things around the house. So yeah, I think if the goal is to understand people

02:34:48.560 --> 02:34:54.960
as much as possible, I think that that's trying to model those sensations is probably somewhat

02:34:54.960 --> 02:34:58.880
important. But I think that there's a lot of value that can be created by having intelligence,

02:34:58.880 --> 02:35:01.600
even that is separate from that. It's a separate thing.

02:35:02.400 --> 02:35:09.360
So one of the features of being human is that we're mortal. We die. We've talked about AI a

02:35:09.360 --> 02:35:16.000
lot, but potentially replicas of ourselves. Do you think there will be AI replicas of you and

02:35:16.000 --> 02:35:21.440
me that persist long after we're gone, that family and loved ones can talk to?

02:35:24.480 --> 02:35:28.960
I think we'll have the capacity to do something like that. And I think one of the big questions

02:35:30.240 --> 02:35:36.640
that we've had to struggle with in the context of social networks is who gets to make that.

02:35:37.600 --> 02:35:43.280
And my answer to that in the context of the work that we're doing is that that should be your

02:35:43.280 --> 02:35:48.400
choice. I don't think anyone should be able to choose to make a Lex spot that people can

02:35:50.720 --> 02:35:56.160
choose to talk to and get to train that. And we have this precedent of making some of these

02:35:56.160 --> 02:36:05.200
calls where someone can create a page for a Lex fan club, but you can't create a page and say that

02:36:05.200 --> 02:36:15.120
you're Lex. So I think that similarly, I think maybe someone maybe should be able to make an

02:36:15.120 --> 02:36:20.160
AI that's a Lex admirer that someone can talk to, but I think it should ultimately be your call

02:36:21.120 --> 02:36:26.400
whether there is a Lex AI. Well, I'm open sourcing the Lex.

02:36:26.800 --> 02:36:35.200
So you're a man of faith. What role has faith played in your life and your understanding of the

02:36:35.200 --> 02:36:43.360
world and your understanding of your own life and your understanding of your work and how your work

02:36:43.360 --> 02:36:49.440
impacts the world? Yeah, I think that there's a few different parts of this that are relevant.

02:36:49.440 --> 02:36:56.720
There's sort of a philosophical part and there's a cultural part. And one of the most basic

02:36:57.360 --> 02:37:04.000
lessons is right at the beginning of Genesis where it's like God creates the earth and creates

02:37:04.000 --> 02:37:09.840
people and creates people in God's image. And there's the question of what does that mean?

02:37:09.840 --> 02:37:13.440
And all the only context that you have about God at that point in the Old Testament is that

02:37:14.560 --> 02:37:19.200
God has created things. So I always thought that one of the interesting lessons from that

02:37:19.200 --> 02:37:27.200
is that there's a virtue in creating things that is like whether it's artistic or

02:37:28.000 --> 02:37:31.760
whether you're building things that are functionally useful for other people.

02:37:34.240 --> 02:37:44.400
I think that that by itself is a good. And that kind of drives a lot of how I think about

02:37:45.360 --> 02:37:52.720
morality and my personal philosophy around what is a good life. I think it's one where you're

02:37:54.720 --> 02:38:02.480
helping the people around you and you're being a kind of positive creative force in the world

02:38:02.480 --> 02:38:08.880
that is helping to bring new things into the world, whether they're amazing other people, kids,

02:38:09.440 --> 02:38:17.040
or just leading to the creation of different things that wouldn't have been possible otherwise.

02:38:17.040 --> 02:38:24.080
And so that's a value for me that matters deeply. And I just love spending time with the kids and

02:38:24.080 --> 02:38:31.120
seeing that they sort of trying to impart this value to them. And it's like nothing makes me

02:38:31.120 --> 02:38:38.480
happier than when I come home from work and I see my daughters building Legos on the table

02:38:38.480 --> 02:38:42.560
or something. It's like, all right, I did that when I was a kid. So many other people were doing

02:38:42.560 --> 02:38:47.440
this. And I hope you don't lose that spirit where when you kind of grow up and you want to

02:38:47.440 --> 02:38:53.920
just continue building different things no matter what it is. To me, that's a lot of what matters.

02:38:55.120 --> 02:38:59.360
That's the philosophical piece. I think the cultural piece is just about community and values.

02:38:59.360 --> 02:39:03.600
And that part of things I think has just become a lot more important to me since I've had kids.

02:39:03.920 --> 02:39:09.680
You know, it's almost autopilot when you're a kid. You're in the kind of getting imparted

02:39:09.680 --> 02:39:15.440
two phase of your life. And I didn't really think about religion that much for a while.

02:39:16.160 --> 02:39:23.600
You know, I was in college before I had kids. And then I think having kids has this way of

02:39:24.640 --> 02:39:29.920
really making you think about what traditions you want to impart and how you want to celebrate.

02:39:29.920 --> 02:39:36.880
And like what balance you want in your life. And I mean, a bunch of the questions that you've

02:39:36.880 --> 02:39:43.280
asked and a bunch of the things that we're talking about. Just the irony of the curtains coming down

02:39:44.480 --> 02:39:49.920
as we're talking about mortality. Once again, same as last time. This is just

02:39:51.440 --> 02:39:57.520
the universe works in. We are definitely living in a simulation book. Go ahead. Community tradition

02:39:57.600 --> 02:40:02.400
and the values, the faith and religion is still. A lot of the topics that we've talked about today

02:40:03.040 --> 02:40:11.920
are around how do you balance, you know, whether it's running a company or different

02:40:11.920 --> 02:40:20.800
responsibilities with this. How do you kind of balance that? And I always also just think

02:40:20.800 --> 02:40:26.960
that it's very grounding to just believe that there is something that is much bigger than you

02:40:26.960 --> 02:40:34.160
that is guiding things. That amongst other things gives you a bit of humility.

02:40:38.400 --> 02:40:43.760
As you pursue that spirit of creating that you spoke to, creating beauty in the world.

02:40:44.320 --> 02:40:50.080
As Dostoevsky said, beauty will save the world. Mark, I'm a huge fan of yours.

02:40:50.320 --> 02:40:54.720
I'm honored to be able to call you a friend and I'm looking forward to

02:40:56.880 --> 02:41:01.040
both kicking your ass and you kicking my ass on the mat tomorrow in Jiu-Jitsu.

02:41:02.000 --> 02:41:07.520
This incredible sport and art that we both will both participate in. Thank you so much

02:41:07.520 --> 02:41:12.400
for talking to me. Thank you for everything you're doing in so many exciting realms of technology

02:41:12.400 --> 02:41:16.400
and human life. I can't wait to talk to you again in the metaverse. Thank you.

02:41:16.720 --> 02:41:22.000
Thanks for listening to this conversation with Mark Zuckerberg. To support this podcast,

02:41:22.000 --> 02:41:27.120
please check out our sponsors in the description. And now let me leave you with some words from

02:41:27.120 --> 02:41:34.720
Isaac Asimov. It is change, continuing change, inevitable change that is the dominant factor

02:41:34.720 --> 02:41:41.360
in society today. No sensible decision can be made any longer without taking into account

02:41:41.360 --> 02:41:48.880
not only the world as it is, but the world as it will be. Thank you for listening and hope to see you

02:41:48.880 --> 02:41:57.280
next time.

