1
00:00:00,000 --> 00:00:02,960
The following is a conversation with Kai-Fu Li.

2
00:00:02,960 --> 00:00:06,480
He's the chairman and CEO of Sinovation Ventures

3
00:00:06,480 --> 00:00:10,480
that manages a $2 billion dual currency investment fund

4
00:00:10,480 --> 00:00:14,800
with a focus on developing the next generation of Chinese high-tech companies.

5
00:00:15,440 --> 00:00:17,840
He's the former president of Google China

6
00:00:17,840 --> 00:00:21,680
and the founder of what is now called Microsoft Research Asia,

7
00:00:21,680 --> 00:00:26,560
an institute that trained many of the artificial intelligence leaders in China,

8
00:00:26,560 --> 00:00:33,600
including CTOs or AI execs at Baidu, Tencent, Alibaba, Lenovo, and Huawei.

9
00:00:34,720 --> 00:00:40,000
He was named one of the 100 most influential people in the world by Time Magazine.

10
00:00:40,560 --> 00:00:44,880
He's the author of seven best-selling books in Chinese and most recently

11
00:00:44,880 --> 00:00:48,800
the New York Times bestseller called AI Superpowers,

12
00:00:48,800 --> 00:00:51,760
China, Silicon Valley, and the New World Order.

13
00:00:52,720 --> 00:00:57,600
He has unparalleled experience in working across major tech companies

14
00:00:57,600 --> 00:00:59,920
and governments on applications of AI,

15
00:00:59,920 --> 00:01:04,880
and so he has a unique perspective on global innovation in the future of AI

16
00:01:04,880 --> 00:01:07,920
that I think is important to listen to and think about.

17
00:01:08,880 --> 00:01:11,840
This is the Artificial Intelligence Podcast.

18
00:01:11,840 --> 00:01:15,120
If you enjoy it, subscribe on YouTube and iTunes,

19
00:01:15,120 --> 00:01:20,240
support it on Patreon, or simply connect with me on Twitter at Lex Freedman.

20
00:01:20,880 --> 00:01:24,560
And now here's my conversation with Kaifu Li.

21
00:01:25,920 --> 00:01:29,280
I immigrated from Russia to U.S. when I was 13.

22
00:01:29,280 --> 00:01:31,600
You immigrated to U.S. at about the same age.

23
00:01:32,320 --> 00:01:35,840
The Russian people, the American people, the Chinese people,

24
00:01:35,840 --> 00:01:41,280
each have a certain soul, a spirit that permeates throughout the generations.

25
00:01:42,000 --> 00:01:45,040
So maybe it's a little bit of a poetic question,

26
00:01:45,040 --> 00:01:50,960
but could you describe your sense of what defines the Chinese soul?

27
00:01:51,920 --> 00:01:56,000
I think the Chinese soul of people today, right?

28
00:01:56,000 --> 00:02:01,920
We're talking about people who have had centuries of burden

29
00:02:01,920 --> 00:02:05,120
because of the poverty that the country has gone through

30
00:02:05,120 --> 00:02:10,400
and suddenly shined with hope of prosperity

31
00:02:10,400 --> 00:02:13,280
in the past 40 years as China opened up

32
00:02:13,360 --> 00:02:15,120
and embraced market economy.

33
00:02:16,320 --> 00:02:20,960
And undoubtedly, there are two sets of pressures on the people,

34
00:02:21,600 --> 00:02:27,920
that of the tradition, that of facing difficult situations,

35
00:02:27,920 --> 00:02:33,760
and that of hope of wanting to be the first to become successful and wealthy.

36
00:02:33,760 --> 00:02:38,240
So that's a very strong hunger and a strong desire

37
00:02:38,240 --> 00:02:41,040
and strong work ethic that drives China forward.

38
00:02:41,040 --> 00:02:43,840
And is there roots to not just this generation,

39
00:02:43,840 --> 00:02:49,920
but before that's deeper than just the new economic developments?

40
00:02:49,920 --> 00:02:54,560
Is there something that's unique to China that you could speak to that's in the people?

41
00:02:55,840 --> 00:03:02,000
Well, the Chinese tradition is about excellence, dedication, and results.

42
00:03:02,560 --> 00:03:07,120
And the Chinese exams and study subjects in schools

43
00:03:07,120 --> 00:03:10,720
have traditionally started from memorizing 10,000 characters.

44
00:03:11,040 --> 00:03:13,440
Not an easy task to start with.

45
00:03:13,440 --> 00:03:18,960
And further by memorizing historic philosophers' literature poetry.

46
00:03:18,960 --> 00:03:24,160
So it really is probably the strongest road learning mechanism

47
00:03:24,160 --> 00:03:28,800
created to make sure people had good memory and remembered things extremely well.

48
00:03:30,080 --> 00:03:35,840
That, I think, at the same time suppresses the breakthrough innovation

49
00:03:36,160 --> 00:03:41,840
and also enhances the speed execution, get results.

50
00:03:42,480 --> 00:03:46,720
And that, I think, characterizes the historic basis of China.

51
00:03:47,280 --> 00:03:50,640
That's interesting because there's echoes of that in Russian education as well,

52
00:03:50,640 --> 00:03:51,920
as rote memorization.

53
00:03:51,920 --> 00:03:53,760
So you memorize a lot of poetry.

54
00:03:53,760 --> 00:03:58,400
I mean, there's just an emphasis on perfection in all forms

55
00:03:59,200 --> 00:04:03,520
that's not conducive to perhaps what you're speaking to, which is creativity.

56
00:04:03,520 --> 00:04:08,720
But you think that kind of education holds back the innovative spirit

57
00:04:08,720 --> 00:04:10,320
that you might see in the United States?

58
00:04:10,880 --> 00:04:16,400
Well, it holds back the breakthrough innovative spirits that we see in the United States.

59
00:04:16,400 --> 00:04:21,840
But it does not hold back the valuable execution-oriented,

60
00:04:21,840 --> 00:04:27,200
result-oriented value-creating engines, which we see China being very successful.

61
00:04:27,920 --> 00:04:34,720
So is there a difference between a Chinese AI engineer today and an American AI engineer,

62
00:04:34,720 --> 00:04:36,960
perhaps rooted in the culture that we just talked about,

63
00:04:36,960 --> 00:04:41,040
or the education, or the very soul of the people, or no?

64
00:04:41,040 --> 00:04:44,560
And what would your advice be to each if there's a difference?

65
00:04:45,440 --> 00:04:48,560
Well, there's a lot that's similar because AI is about

66
00:04:49,840 --> 00:04:54,800
mastering sciences, about using known technologies and trying new things.

67
00:04:54,800 --> 00:05:00,320
But it's also about picking from many parts of possible networks to use

68
00:05:00,320 --> 00:05:02,960
and different types of parameters to tune.

69
00:05:02,960 --> 00:05:05,360
And that part is somewhat rote.

70
00:05:05,360 --> 00:05:10,160
And it is also, as anyone who's built AI products can tell you,

71
00:05:10,160 --> 00:05:15,200
a lot about cleansing the data, because AI runs better with more data.

72
00:05:15,200 --> 00:05:22,400
And data is generally unstructured, errorful, and unclean.

73
00:05:22,400 --> 00:05:25,520
And the effort to clean the data is immense.

74
00:05:26,320 --> 00:05:32,720
So I think the better part of the American AI engineering process

75
00:05:33,280 --> 00:05:37,200
is to try new things, to do things people haven't done before,

76
00:05:37,920 --> 00:05:42,320
and to use technology to solve most, if not all, problems.

77
00:05:43,280 --> 00:05:47,120
So to make the algorithm work, despite not so great data,

78
00:05:47,120 --> 00:05:49,840
find error-tolerant ways to deal with the data.

79
00:05:50,560 --> 00:05:56,960
The Chinese way would be to basically enumerate, to the fullest extent,

80
00:05:56,960 --> 00:06:02,240
all the possible ways by a lot of machines, try lots of different ways to get it to work,

81
00:06:02,240 --> 00:06:07,600
and spend a lot of resources and money and time cleaning up data.

82
00:06:07,600 --> 00:06:12,880
That means the AI engineer may be writing data cleansing algorithms,

83
00:06:12,880 --> 00:06:19,040
working with thousands of people who label or correct or do things with the data.

84
00:06:19,040 --> 00:06:23,840
That is the incredible hard work that might lead to better results.

85
00:06:23,840 --> 00:06:28,880
So the Chinese engineer would rely on and ask for more and more and more data,

86
00:06:28,880 --> 00:06:32,400
and find ways to cleanse them and make them work in the system,

87
00:06:32,400 --> 00:06:39,440
and probably less time thinking about new algorithms that can overcome data or other issues.

88
00:06:39,440 --> 00:06:40,720
So where's your intuition?

89
00:06:40,720 --> 00:06:44,080
What do you think the biggest impact in the next 10 years lies?

90
00:06:44,080 --> 00:06:52,320
Is it in some breakthrough algorithms or is it in just this at scale rigor,

91
00:06:53,040 --> 00:06:58,320
a rigorous approach to data, cleaning data, organizing data onto the same algorithms?

92
00:06:58,320 --> 00:07:01,680
What do you think the big impact in the applied world is?

93
00:07:02,480 --> 00:07:06,800
Well, if you're really in the company and you have to deliver results,

94
00:07:06,800 --> 00:07:12,160
using known techniques and enhancing data seems like the more expedient approach.

95
00:07:12,160 --> 00:07:17,120
That's very low risk and likely to generate better and better results.

96
00:07:17,120 --> 00:07:19,840
And that's why the Chinese approach has done quite well.

97
00:07:20,480 --> 00:07:27,440
Now, there are a lot of more challenging startups and problems, such as autonomous vehicles,

98
00:07:28,400 --> 00:07:33,200
medical diagnosis, that existing algorithms may probably won't solve.

99
00:07:34,240 --> 00:07:38,640
And that would put the Chinese approach more challenged

100
00:07:38,640 --> 00:07:45,360
and give them more breakthrough innovation approach, more of an edge on those kinds of problems.

101
00:07:45,360 --> 00:07:46,960
So let me talk to that a little more.

102
00:07:46,960 --> 00:07:52,800
So my intuition, personally, is that data can take us extremely far.

103
00:07:53,600 --> 00:07:56,320
So you brought up autonomous vehicles and medical diagnosis.

104
00:07:56,320 --> 00:08:02,960
So your intuition is that huge amounts of data might not be able to completely help us solve that problem.

105
00:08:03,920 --> 00:08:08,000
Right. So breaking that down further in autonomous vehicle,

106
00:08:08,000 --> 00:08:13,040
I think huge amounts of data probably will solve trucks driving on highways,

107
00:08:13,040 --> 00:08:15,520
which will deliver significant value.

108
00:08:15,520 --> 00:08:17,760
And China will probably lead in that.

109
00:08:19,280 --> 00:08:25,680
And full L5 autonomous is likely to require new technologies we don't yet know.

110
00:08:26,240 --> 00:08:30,320
And that might require academia and great industrial research,

111
00:08:30,320 --> 00:08:32,400
both innovating and working together.

112
00:08:32,400 --> 00:08:34,720
And in that case, US has an advantage.

113
00:08:35,200 --> 00:08:39,280
So the interesting question there is, I don't know if you're familiar on the autonomous vehicle

114
00:08:39,280 --> 00:08:45,520
space and the developments with Tesla and Elon Musk, where they are, in fact,

115
00:08:46,480 --> 00:08:54,240
a full steam ahead into this mysterious complex world of full autonomy, L5, L4, L5,

116
00:08:55,120 --> 00:08:58,720
and they're trying to solve that purely with data.

117
00:08:58,720 --> 00:09:02,000
So the same kind of thing that you're saying is just for highway,

118
00:09:02,000 --> 00:09:04,560
which is what a lot of people share your intuition.

119
00:09:05,360 --> 00:09:07,120
They're trying to solve with data.

120
00:09:07,120 --> 00:09:09,280
It's just to linger on that moment further.

121
00:09:09,280 --> 00:09:12,480
Do you think possible for them to achieve success

122
00:09:13,520 --> 00:09:17,840
with simply just a huge amount of this training on edge cases,

123
00:09:17,840 --> 00:09:21,520
on difficult cases in urban environments, not just highway and so on?

124
00:09:22,720 --> 00:09:24,480
I think they'll be very hard.

125
00:09:24,480 --> 00:09:29,760
One could characterize Tesla's approach as kind of a Chinese strength approach,

126
00:09:29,760 --> 00:09:33,280
gather all the data you can and hope that will overcome the problems.

127
00:09:33,920 --> 00:09:41,440
But in autonomous driving, clearly a lot of the decisions aren't merely solved by aggregating data

128
00:09:41,440 --> 00:09:43,440
and having feedback loop.

129
00:09:43,440 --> 00:09:47,280
There are things that are more akin to human thinking.

130
00:09:47,920 --> 00:09:50,560
And how would those be integrated and built?

131
00:09:51,520 --> 00:09:56,320
There has not yet been a lot of success integrating human intelligence,

132
00:09:56,320 --> 00:10:02,320
or call it expert systems, if you will, even though that's a taboo word with the machine learning.

133
00:10:02,800 --> 00:10:07,120
And the integration of the two types of thinking hasn't yet been demonstrated.

134
00:10:07,760 --> 00:10:11,520
And the question is, how much can you push a purely machine learning approach?

135
00:10:12,320 --> 00:10:17,440
And of course, Tesla also has an additional constraint that they don't have all the sensors.

136
00:10:18,320 --> 00:10:20,960
I know that they think it's foolish to use lidars,

137
00:10:20,960 --> 00:10:27,520
but that's clearly a one less very valuable and reliable source of input

138
00:10:27,520 --> 00:10:30,720
that they're foregoing, which may also have consequences.

139
00:10:32,320 --> 00:10:36,320
I think the advantage, of course, is capturing data that no one has ever seen before.

140
00:10:36,960 --> 00:10:42,080
And in some cases, such as computer vision and speech recognition,

141
00:10:42,080 --> 00:10:47,200
I have seen Chinese companies accumulate data that's not seen anywhere in the Western world,

142
00:10:47,200 --> 00:10:50,080
and they have delivered superior results.

143
00:10:50,080 --> 00:10:57,040
But then speech recognition and object recognition are relatively suitable problems for deep learning

144
00:10:57,040 --> 00:11:04,320
and don't have the potentially need for the human intelligence analytical planning elements.

145
00:11:04,320 --> 00:11:06,320
And the same on the speech recognition side,

146
00:11:06,320 --> 00:11:10,640
your intuition that speech recognition and the machine learning approaches to speech

147
00:11:10,640 --> 00:11:15,840
recognition won't take us to a conversational system that can pass the Turing test,

148
00:11:15,840 --> 00:11:20,000
which is sort of maybe akin to what driving is.

149
00:11:20,000 --> 00:11:22,480
So it needs to have something more than just simply

150
00:11:23,440 --> 00:11:26,800
simple language understanding, simple language generation.

151
00:11:26,800 --> 00:11:32,400
Roughly right, I would say that based on purely machine learning approaches,

152
00:11:32,400 --> 00:11:41,760
it's hard to imagine it could lead to a full conversational experience across arbitrary domains,

153
00:11:41,760 --> 00:11:43,760
which is akin to L5.

154
00:11:43,760 --> 00:11:49,760
I'm a little hesitant to use the word Turing test because the original definition was probably too easy.

155
00:11:50,160 --> 00:11:51,280
We probably do that.

156
00:11:52,240 --> 00:11:55,120
The spirit of the Turing test is what I was referring to.

157
00:11:56,400 --> 00:12:01,520
So you've had major leadership research positions at Apple, Microsoft, Google.

158
00:12:01,520 --> 00:12:07,280
So continuing on the discussion of America, Russia, Chinese soul and culture and so on,

159
00:12:09,120 --> 00:12:16,320
what is the culture of Silicon Valley in contrast to China and maybe U.S. broadly,

160
00:12:16,320 --> 00:12:22,000
and what is the unique culture of each of these three major companies in your view?

161
00:12:22,000 --> 00:12:27,120
I think in aggregate, Silicon Valley companies, and we could probably include Microsoft in that,

162
00:12:27,120 --> 00:12:33,280
even though they're not in the Valley, is really dream big and have visionary goals

163
00:12:33,920 --> 00:12:42,240
and believe that technology will conquer all and also the self-confidence and the self-entitlement

164
00:12:42,240 --> 00:12:46,080
that whatever they produce, the whole world should use and must use.

165
00:12:47,200 --> 00:12:51,920
And those are historically important.

166
00:12:51,920 --> 00:12:58,960
I think Steve Jobs' famous quote that he doesn't do focus groups.

167
00:12:58,960 --> 00:13:03,520
He looks in the mirror and asks the person in the mirror, what do you want?

168
00:13:03,520 --> 00:13:09,760
And that really is an inspirational comment that says the great company shouldn't just

169
00:13:09,760 --> 00:13:16,160
ask users what they want, but develop something that users will know they want when they see it,

170
00:13:16,160 --> 00:13:18,880
but they could never come up with themselves.

171
00:13:18,880 --> 00:13:25,840
I think that is probably the most exhilarating description of what the essence of Silicon Valley

172
00:13:25,840 --> 00:13:32,640
is, that this brilliant idea could cause you to build something that couldn't come out of

173
00:13:32,640 --> 00:13:38,000
focus groups or A.B. tests. And iPhone would be an example of that.

174
00:13:38,000 --> 00:13:42,160
No one in the age of Blackberry would write down they want an iPhone or multi-touch,

175
00:13:42,720 --> 00:13:47,360
a browser might be another example, no one would say they want that in the days of FTP,

176
00:13:47,360 --> 00:13:49,360
but once they see it, they want it.

177
00:13:49,360 --> 00:13:53,200
So I think that is what Silicon Valley is best at.

178
00:13:55,520 --> 00:13:58,800
But it also came with a lot of success.

179
00:13:58,800 --> 00:14:04,400
These products became global platforms and there were basically no competitors anywhere.

180
00:14:05,040 --> 00:14:12,640
And that has also led to a belief that these are the only things that one should do,

181
00:14:13,200 --> 00:14:17,360
that companies should not tread on other company's territory.

182
00:14:18,000 --> 00:14:26,160
So that's a, you know, Groupon and the Yelp and then OpenTable and the Grubhub would each feel,

183
00:14:26,160 --> 00:14:30,960
okay, I'm not going to do the other company's business because that would not be the pride

184
00:14:31,040 --> 00:14:35,600
of innovating what each of these four companies have innovated.

185
00:14:36,880 --> 00:14:42,080
But I think the Chinese approach is do whatever it takes to win.

186
00:14:42,720 --> 00:14:44,960
And it's a winner take all market.

187
00:14:44,960 --> 00:14:50,880
And in fact, in the internet space, the market leader will get predominantly all the value

188
00:14:50,880 --> 00:14:52,800
extracted out of the system.

189
00:14:53,360 --> 00:14:59,520
So and the system isn't just defined as one narrow category,

190
00:14:59,520 --> 00:15:01,360
but gets broader and broader.

191
00:15:01,360 --> 00:15:09,600
So it's amazing ambition for success and domination of increasingly larger

192
00:15:10,240 --> 00:15:17,040
product categories leading to clear market winner status and the opportunity to extract

193
00:15:17,040 --> 00:15:18,160
tremendous value.

194
00:15:19,040 --> 00:15:28,640
And that develops a practical, result oriented, ultra ambitious winner take all,

195
00:15:28,720 --> 00:15:30,720
gladiatorial mentality.

196
00:15:31,520 --> 00:15:38,640
And if what it takes is to build what the competitors built, essentially a copycat,

197
00:15:38,640 --> 00:15:41,840
that can be done without infringing laws.

198
00:15:41,840 --> 00:15:47,680
If what it takes is to satisfy a foreign countries need by forking the code base and

199
00:15:47,680 --> 00:15:50,880
building something that looks really ugly and different, they'll do it.

200
00:15:51,440 --> 00:15:55,680
So it's contrasted very sharply with the Silicon Valley approach.

201
00:15:56,160 --> 00:16:01,840
And I think the flexibility and the speed and execution has helped the Chinese approach.

202
00:16:01,840 --> 00:16:09,600
And I think the Silicon Valley approach is potentially challenged if every Chinese

203
00:16:09,600 --> 00:16:14,560
entrepreneurs learning from the whole world, US and China and the American entrepreneurs

204
00:16:14,560 --> 00:16:18,640
only look internally and write off China as a copycat.

205
00:16:19,520 --> 00:16:23,440
And the second part of your question about the three companies,

206
00:16:23,440 --> 00:16:26,000
the unique elements of the three companies perhaps.

207
00:16:26,000 --> 00:16:37,440
Yeah, I think Apple represents while the user please the user and the essence of design and

208
00:16:37,440 --> 00:16:46,560
brand and it's the one company and perhaps the only tech company that draws people with a

209
00:16:48,160 --> 00:16:53,120
strong serious desire for the product and the need and the willingness to pay a premium.

210
00:16:53,520 --> 00:16:59,440
Because of the halo effect of the brand, which came from the attention to detail

211
00:17:00,000 --> 00:17:01,920
and great respect for user needs.

212
00:17:03,360 --> 00:17:13,680
Microsoft represents a platform approach that builds giant products that become very strong

213
00:17:13,680 --> 00:17:20,880
modes that others can't do because it's well architected at the bottom level.

214
00:17:21,520 --> 00:17:26,720
And the work is efficiently delegated to individuals.

215
00:17:26,720 --> 00:17:33,600
And then the whole product is built by adding small parts that sum together.

216
00:17:33,600 --> 00:17:39,920
So it's probably the most effective high tech assembly line that builds a very difficult

217
00:17:39,920 --> 00:17:49,920
product that and the whole process of doing that is kind of a differentiation and something

218
00:17:49,920 --> 00:17:52,480
competitors can't easily repeat.

219
00:17:52,480 --> 00:17:58,480
Are there elements of the Chinese approach in the way Microsoft went about assembling those

220
00:17:58,480 --> 00:18:04,560
little pieces and dominating essentially dominating the market for a long time or do you see those

221
00:18:04,560 --> 00:18:05,520
as distinct?

222
00:18:05,520 --> 00:18:07,360
I think there are elements that are the same.

223
00:18:08,160 --> 00:18:14,400
I think the three American companies that had or have Chinese characteristics and obviously

224
00:18:14,400 --> 00:18:19,680
as well as American characteristics are Microsoft, Facebook and Amazon.

225
00:18:20,320 --> 00:18:21,200
Yes, that's right.

226
00:18:21,200 --> 00:18:28,880
Because these are companies that will tenaciously go after adjacent markets, build up strong

227
00:18:29,920 --> 00:18:39,120
product offering and find ways to extract greater value from a sphere that's ever increasing.

228
00:18:40,000 --> 00:18:42,720
And they understand the value of the platforms.

229
00:18:43,520 --> 00:18:44,880
So that's the similarity.

230
00:18:45,520 --> 00:18:55,840
And then with Google, I think it's a genuinely value oriented company that does have a heart

231
00:18:55,840 --> 00:19:01,040
and soul and that wants to do great things for the world by connecting information.

232
00:19:01,840 --> 00:19:14,160
And that has also very strong technology genes and wants to use technology and has found

233
00:19:15,920 --> 00:19:22,720
out of the box ways to use technology to deliver incredible value to the end user.

234
00:19:23,520 --> 00:19:26,400
If we can look at Google, for example, you mentioned heart and soul.

235
00:19:26,880 --> 00:19:33,920
There seems to be an element where Google is after making the world better.

236
00:19:33,920 --> 00:19:35,520
There's a more positive view.

237
00:19:35,520 --> 00:19:39,840
I mean, they used to have the slogan, don't be evil and Facebook a little bit more has

238
00:19:39,840 --> 00:19:44,480
a negative tend to it, at least in the perception of privacy and so on.

239
00:19:44,480 --> 00:19:51,440
Do you have a sense of how these different companies can achieve because you've talked

240
00:19:51,440 --> 00:19:55,280
about how much we can make the world better in all these kinds of ways with AI?

241
00:19:55,600 --> 00:20:01,120
What is it about a company that can make, give it a heart and soul, gain the trust

242
00:20:01,120 --> 00:20:05,760
of the public and just actually just not be evil and do good for the world?

243
00:20:06,800 --> 00:20:07,760
It's really hard.

244
00:20:07,760 --> 00:20:10,560
And I think Google has struggled with that.

245
00:20:11,760 --> 00:20:18,160
First, that don't do evil mantra is very dangerous because every employee's definition

246
00:20:18,160 --> 00:20:19,360
of evil is different.

247
00:20:19,360 --> 00:20:23,120
And that has led to some difficult employee situations for them.

248
00:20:23,840 --> 00:20:29,600
So I don't necessarily think that's a good value statement, but just watching the kinds

249
00:20:29,600 --> 00:20:35,920
of things Google or its parent company Alphabet does in new areas like healthcare,

250
00:20:35,920 --> 00:20:40,560
like, you know, eradicating mosquitoes, things that are really not in the business

251
00:20:40,560 --> 00:20:42,720
of a internet tech company.

252
00:20:42,720 --> 00:20:48,720
I think that shows that there is a heart and soul and desire to do good and willingness

253
00:20:49,680 --> 00:20:56,880
to put in the resources to do something when they see it's good, they will pursue it.

254
00:20:58,240 --> 00:21:02,400
That doesn't necessarily mean it has all the trust of the users.

255
00:21:02,400 --> 00:21:09,680
I realize while most people would view Facebook as the primary target of their recent unhappiness

256
00:21:09,680 --> 00:21:14,000
about Silicon Valley companies, many would put Google in that category.

257
00:21:14,000 --> 00:21:19,200
And some have named Google's business practices as predatory also.

258
00:21:19,840 --> 00:21:26,720
So it's kind of difficult to have the two parts of a body, the brain wants to do what

259
00:21:26,720 --> 00:21:30,560
it's supposed to do for a shareholder, maximize profit, and then the heart and soul

260
00:21:30,560 --> 00:21:36,080
wants to do good things that may run against what the brain wants to do.

261
00:21:36,080 --> 00:21:41,680
So in this complex balancing that these companies have to do, you've mentioned that

262
00:21:41,680 --> 00:21:47,280
you're concerned about a future where too few companies like Google, Facebook, Amazon

263
00:21:47,280 --> 00:21:53,280
are controlling our data or controlling too much of our digital lives.

264
00:21:53,280 --> 00:21:57,280
Can you elaborate on this concern and perhaps do you have a better way forward?

265
00:21:58,640 --> 00:22:03,600
I think I'm hardly the most vocal complainer of this.

266
00:22:04,880 --> 00:22:07,280
There are a lot louder complainers out there.

267
00:22:07,280 --> 00:22:16,880
I do observe that having a lot of data does perpetuate their strength and limit competition

268
00:22:16,880 --> 00:22:18,320
in many spaces.

269
00:22:19,360 --> 00:22:24,080
But I also believe AI is much broader than the internet space.

270
00:22:24,080 --> 00:22:32,400
So the entrepreneurial opportunities still exists in using AI to empower financial, retail,

271
00:22:32,400 --> 00:22:35,360
manufacturing, education, and applications.

272
00:22:35,360 --> 00:22:42,720
So I don't think it's quite a case of full monopolistic dominance that totally stifles

273
00:22:42,720 --> 00:22:43,840
innovation.

274
00:22:43,840 --> 00:22:48,480
But I do believe in their areas of strength, it's hard to dislodge them.

275
00:22:49,680 --> 00:22:52,640
I don't know if I have a good solution.

276
00:22:53,280 --> 00:22:58,320
Probably the best solution is let the entrepreneurial VC ecosystem work well

277
00:22:58,960 --> 00:23:04,240
and find all the places that can create the next Google, the next Facebook.

278
00:23:04,240 --> 00:23:07,920
So there will always be increasing number of challengers.

279
00:23:08,480 --> 00:23:11,280
In some sense that has happened a little bit.

280
00:23:11,280 --> 00:23:17,280
You see Uber, Airbnb having emerged despite the strength of the big three.

281
00:23:20,240 --> 00:23:25,200
I think China as an environment may be more interesting for the emergence

282
00:23:25,200 --> 00:23:32,800
because if you look at companies between let's say 50 to 300 billion dollars,

283
00:23:33,520 --> 00:23:40,320
China has emerged more of such companies than the US in the last three to four years because

284
00:23:40,320 --> 00:23:45,440
of the larger marketplace, because of the more fearless nature of the entrepreneurs

285
00:23:46,400 --> 00:23:50,800
that and the Chinese giants are just as powerful as American ones.

286
00:23:50,800 --> 00:23:58,080
Tencent Alibaba are very strong, but Bytes Dance has emerged worth 75 billion and financial.

287
00:23:58,080 --> 00:24:03,040
While it's Alibaba affiliated, it's nevertheless independent and worth 150 billion.

288
00:24:03,840 --> 00:24:11,120
And so I do think if we start to extend to traditional businesses, we will see very

289
00:24:11,120 --> 00:24:12,640
valuable companies.

290
00:24:12,640 --> 00:24:19,600
So it's probably not the case that in five or 10 years we'll still see the whole world

291
00:24:19,600 --> 00:24:21,920
with these five companies having such dominance.

292
00:24:22,560 --> 00:24:28,320
So you mentioned a couple of times this fascinating world of entrepreneurship in China

293
00:24:29,120 --> 00:24:31,040
of the fearless nature of the entrepreneurs.

294
00:24:31,040 --> 00:24:35,360
So can you maybe talk a little bit about what it takes to be an entrepreneur in China?

295
00:24:35,360 --> 00:24:37,600
What are the strategies that are undertaken?

296
00:24:38,320 --> 00:24:41,040
What are the ways that you success?

297
00:24:41,040 --> 00:24:46,400
What is the dynamic of VCF funding of the way the government helps companies and so on?

298
00:24:46,400 --> 00:24:49,360
What are the interesting aspects here that are distinct from,

299
00:24:49,360 --> 00:24:54,000
that are different from the Silicon Valley world of entrepreneurship?

300
00:24:55,120 --> 00:25:02,320
Well, many of the listeners probably still would brand Chinese entrepreneur as copycats.

301
00:25:02,960 --> 00:25:08,000
And no doubt 10 years ago, that would not be an inaccurate description.

302
00:25:09,040 --> 00:25:15,280
Back 10 years ago, an entrepreneur probably could not get funding if he or she could not describe

303
00:25:15,920 --> 00:25:19,200
what product he or she is copying from the US.

304
00:25:20,320 --> 00:25:23,440
The first question is, who has proven this business model,

305
00:25:23,440 --> 00:25:26,400
which is a nice way of asking, who are you copying?

306
00:25:27,120 --> 00:25:34,080
And that reason is understandable because China had a much lower internet penetration

307
00:25:34,800 --> 00:25:42,480
and didn't have enough indigenous experience to build innovative products.

308
00:25:43,200 --> 00:25:47,600
And secondly, internet was emerging.

309
00:25:47,600 --> 00:25:52,880
Link startup was the way to do things, building a first minimally viable product

310
00:25:52,880 --> 00:25:54,640
and then expanding was the right way to go.

311
00:25:55,280 --> 00:26:02,480
And the American successes have given a shortcut that if you built your minimally viable product

312
00:26:02,480 --> 00:26:06,640
based on an American product, it's guaranteed to be a decent starting point.

313
00:26:06,640 --> 00:26:07,840
Then you tweak it afterwards.

314
00:26:08,400 --> 00:26:12,160
So as long as there are no IP infringement, which as far as I know,

315
00:26:12,160 --> 00:26:18,560
there hasn't been in the mobile and AI spaces, that's a much better shortcut.

316
00:26:19,360 --> 00:26:24,400
And I think Silicon Valley would view that as still not very honorable

317
00:26:25,120 --> 00:26:29,120
because that's not your own idea to start with.

318
00:26:29,120 --> 00:26:35,040
But you can't really at the same time believe every idea must be your own

319
00:26:35,120 --> 00:26:38,480
and believe in the link startup methodology because

320
00:26:39,360 --> 00:26:43,520
link startup is intended to try many, many things and then converge when that works.

321
00:26:44,160 --> 00:26:46,720
And it's meant to be iterated and changed.

322
00:26:46,720 --> 00:26:50,000
So finding a decent starting point without legal violations,

323
00:26:51,120 --> 00:26:55,440
there should be nothing morally dishonorable about that.

324
00:26:55,440 --> 00:26:57,040
So just a quick pause on that.

325
00:26:57,040 --> 00:27:01,840
It's fascinating that that's why is that not honorable, right?

326
00:27:01,920 --> 00:27:06,960
Exactly as you formulated, it seems like a perfect start for business

327
00:27:08,000 --> 00:27:14,400
is to take a look at Amazon and say, okay, we'll do exactly what Amazon is doing.

328
00:27:14,400 --> 00:27:19,120
Let's start there in this particular market and then let's out innovate them

329
00:27:19,120 --> 00:27:22,080
from that starting point and come up with new ways.

330
00:27:22,080 --> 00:27:27,040
I mean, is it wrong to be, except the word copycat just sounds bad,

331
00:27:27,040 --> 00:27:28,720
but is it wrong to be a copycat?

332
00:27:28,720 --> 00:27:31,520
It just seems like a smart strategy.

333
00:27:31,520 --> 00:27:38,960
But yes, doesn't have a heroic nature to it that like Steve Jobs,

334
00:27:39,920 --> 00:27:43,760
Elon Musk sort of in something completely coming up with something completely new.

335
00:27:43,760 --> 00:27:45,200
Yeah, I like the way you describe it.

336
00:27:45,200 --> 00:27:52,240
It's a non-heroic acceptable way to start the company and maybe more expedient.

337
00:27:52,880 --> 00:28:00,080
So that's, I think, a baggage for Silicon Valley, that if it doesn't let go,

338
00:28:00,720 --> 00:28:05,040
then it may limit the ultimate ceiling of the company.

339
00:28:05,040 --> 00:28:06,560
Take Snapchat as an example.

340
00:28:07,120 --> 00:28:11,440
I think, you know, Evan's brilliant, he built a great product,

341
00:28:11,440 --> 00:28:16,720
but he's very proud that he wants to build his own features, not copy others.

342
00:28:16,720 --> 00:28:20,960
While Facebook was more willing to copy his features,

343
00:28:20,960 --> 00:28:22,800
and you see what happens in the competition.

344
00:28:23,440 --> 00:28:30,160
So I think putting that handcuff on the company would limit its ability to reach the maximum

345
00:28:30,160 --> 00:28:36,720
potential. So back to the Chinese environment, copying was merely a way to learn from the

346
00:28:36,720 --> 00:28:43,360
American masters. Just like we, if you would, we learned to play piano or painting,

347
00:28:43,360 --> 00:28:44,480
you start by copying.

348
00:28:44,480 --> 00:28:48,160
You don't start by innovating when you don't have the basic skill sets.

349
00:28:48,160 --> 00:28:56,080
So very amazingly, the Chinese entrepreneurs, about six years ago, started to branch off

350
00:28:56,080 --> 00:29:02,160
with these lean startups built on American ideas to build better products than American products.

351
00:29:02,160 --> 00:29:04,240
But they did start from the American idea.

352
00:29:04,960 --> 00:29:08,560
And today, WeChat is better than WhatsApp.

353
00:29:08,560 --> 00:29:09,920
Weibo is better than Twitter.

354
00:29:10,480 --> 00:29:12,880
Zihu is better than Quora and so on.

355
00:29:12,880 --> 00:29:17,840
So that I think is Chinese entrepreneurs going to step two.

356
00:29:18,400 --> 00:29:23,680
And then step three is once these entrepreneurs have done one or two of these companies,

357
00:29:23,680 --> 00:29:27,360
they now look at the Chinese market and the opportunities

358
00:29:27,360 --> 00:29:30,000
and come up with ideas that didn't exist elsewhere.

359
00:29:30,640 --> 00:29:37,520
So products like and financial under which includes AliPay, which is mobile payments.

360
00:29:38,320 --> 00:29:44,320
And also the financial products for loans built on that.

361
00:29:44,320 --> 00:29:47,280
And also in education VIP kid.

362
00:29:48,560 --> 00:29:57,600
And in social video, social network, TikTok, and in social e-commerce, Pinduoduo.

363
00:29:58,640 --> 00:30:01,680
And then in ride sharing, Mobike.

364
00:30:01,680 --> 00:30:08,080
These are all Chinese innovative products that now are being copied elsewhere.

365
00:30:08,720 --> 00:30:15,440
So and an additional interesting observation is some of these products are built on unique

366
00:30:15,440 --> 00:30:21,760
Chinese demographics, which may not work in the US, but may work very well in Southeast Asia,

367
00:30:22,400 --> 00:30:27,760
Africa, and other developing worlds that are a few years behind China.

368
00:30:27,760 --> 00:30:32,160
And a few of these products maybe are universal and are getting traction,

369
00:30:32,160 --> 00:30:34,640
even in the United States, such as TikTok.

370
00:30:35,360 --> 00:30:43,440
So this whole ecosystem is supported by VCs as a virtuous cycle,

371
00:30:43,440 --> 00:30:48,720
because a large market with with the innovative entrepreneurs will draw a lot of money.

372
00:30:49,360 --> 00:30:53,680
And then investing these companies, as the market gets larger and larger,

373
00:30:53,680 --> 00:30:57,440
US mark China market is easily three, four times larger than the US.

374
00:30:58,320 --> 00:31:02,160
They will create greater value and greater returns for the VCs,

375
00:31:02,160 --> 00:31:04,480
thereby raising even more money.

376
00:31:05,360 --> 00:31:09,840
So at Sinovation Ventures, our first fund was 15 million.

377
00:31:09,840 --> 00:31:11,440
Our last fund was 500 million.

378
00:31:12,000 --> 00:31:19,040
So it reflects the valuation of the companies and our us going multi stage and things like that.

379
00:31:19,680 --> 00:31:21,600
It also has government support.

380
00:31:22,880 --> 00:31:26,000
But not in the way most Americans would think of it.

381
00:31:26,000 --> 00:31:31,120
The government actually leaves the entrepreneurial space as a private enterprise,

382
00:31:31,200 --> 00:31:36,640
so the self regulating and the government would build infrastructures that would

383
00:31:37,280 --> 00:31:39,200
around it to make it work better.

384
00:31:39,200 --> 00:31:44,800
For example, the mass entrepreneur mass innovation plan built 8000 incubators.

385
00:31:44,800 --> 00:31:49,520
So the pipeline is very strong to the VCs for autonomous vehicles.

386
00:31:49,520 --> 00:31:54,240
The Chinese government is building smart highways with sensors,

387
00:31:54,240 --> 00:32:00,720
smart cities that separate pedestrians from cars that may allow initially an inferior

388
00:32:01,120 --> 00:32:06,800
autonomous vehicle company to launch a car without increasing with lower casualty,

389
00:32:07,600 --> 00:32:10,800
because the roads or the city is smart.

390
00:32:11,440 --> 00:32:16,560
And the Chinese government at local levels would have these guiding funds acting as

391
00:32:16,560 --> 00:32:19,360
LPs, passive LPs to funds.

392
00:32:19,360 --> 00:32:25,920
And when the fund makes money, part of the money made is given back to the GPs and potentially

393
00:32:25,920 --> 00:32:33,600
other LPs to increase everybody's return at the expense of the government's return.

394
00:32:33,600 --> 00:32:41,520
So that's an interesting incentive that entrusts the task of choosing entrepreneurs to VCs

395
00:32:41,520 --> 00:32:46,560
who are better at it than the government by letting some of the profits move that way.

396
00:32:46,560 --> 00:32:48,640
So this is really fascinating, right?

397
00:32:48,640 --> 00:32:54,240
So I look at the Russian government as a case study where let me put it this way,

398
00:32:54,240 --> 00:33:00,080
there's no such government driven large scale support of entrepreneurship.

399
00:33:00,800 --> 00:33:06,960
And probably the same is true in the United States, but the entrepreneurs themselves kind of find a way.

400
00:33:07,600 --> 00:33:15,520
So maybe in a form of advice or explanation, how did the Chinese government arrive to be this way,

401
00:33:15,520 --> 00:33:21,920
so supportive on entrepreneurship to be in this particular way so forward thinking at such a

402
00:33:21,920 --> 00:33:26,720
large scale, and also perhaps, how can we copy it in other countries?

403
00:33:28,160 --> 00:33:31,440
How can we encourage other governments, like even the United States government,

404
00:33:31,440 --> 00:33:35,520
to support infrastructure for autonomous vehicles in that same kind of way, perhaps?

405
00:33:35,520 --> 00:33:45,920
Yes. So these techniques are the result of several key things, some of which may be learnable,

406
00:33:45,920 --> 00:33:47,280
some of which may be very hard.

407
00:33:48,000 --> 00:33:52,800
One is just trial and error and watching what everyone else is doing.

408
00:33:52,800 --> 00:33:56,800
I think it's important to be humble and not feel like you know all the answers.

409
00:33:56,800 --> 00:34:00,560
The guiding funds idea came from Singapore, which came from Israel,

410
00:34:01,360 --> 00:34:07,760
and China made a few tweaks and turned it into a, because the Chinese cities

411
00:34:07,760 --> 00:34:12,720
and government officials kind of compete with each other because they all want to

412
00:34:12,720 --> 00:34:19,440
make their city more successful, so they can get the next level in their political career,

413
00:34:20,240 --> 00:34:25,120
and it's somewhat competitive. So the central government made it a bit of a competition.

414
00:34:25,120 --> 00:34:31,040
Everybody has a budget, they can put it on AI, or they can put it on bio, or they can put it on

415
00:34:31,040 --> 00:34:36,000
energy, and then whoever gets the results, the city shines, the people are better off,

416
00:34:36,000 --> 00:34:42,240
the mayor gets a promotion. So the tools, this is kind of almost like an entrepreneurial environment

417
00:34:42,800 --> 00:34:46,400
for local governments to see who can do a better job.

418
00:34:47,440 --> 00:34:55,040
And also many of them tried different experiments. Some have given award to

419
00:34:55,760 --> 00:35:00,800
very smart researchers, just give them money and hope they'll start a company.

420
00:35:00,800 --> 00:35:08,000
Some have given money to academic research labs, maybe government research labs,

421
00:35:08,000 --> 00:35:13,040
to see if they can spin off some companies from the science lab or something like that.

422
00:35:14,000 --> 00:35:18,960
Some have tried to recruit overseas Chinese to come back and start companies,

423
00:35:18,960 --> 00:35:23,360
and they've had mixed results. The one that worked the best was the guiding funds.

424
00:35:23,360 --> 00:35:27,920
So it's almost like a lean startup idea where people try different things and

425
00:35:27,920 --> 00:35:32,800
what works sticks and everybody copies. So now every city has a guiding fund.

426
00:35:32,800 --> 00:35:41,120
So that's how that came about. The autonomous vehicle and the massive spending in highways

427
00:35:41,120 --> 00:35:49,440
and smart cities, that's a Chinese way. It's about building infrastructure to facilitate.

428
00:35:49,440 --> 00:35:55,920
It's a clear division of the government's responsibility from the market. The market

429
00:35:55,920 --> 00:36:02,480
should do everything in a private freeway, but there are things the market can't afford to do.

430
00:36:02,880 --> 00:36:09,520
Like infrastructure. So the government always appropriates large amounts of money

431
00:36:09,520 --> 00:36:16,800
for infrastructure building. This happens with not only autonomous vehicle and AI,

432
00:36:16,800 --> 00:36:25,920
but happened with the 3G and 4G. You'll find that the Chinese wireless reception is better than the

433
00:36:25,920 --> 00:36:31,920
US because massive spending that tries to cover the whole country, whereas in the US it may be

434
00:36:31,920 --> 00:36:38,560
a little spotty. It's a government driven because I think they view the coverage of

435
00:36:41,040 --> 00:36:46,160
cell access and 3G, 4G access to be a governmental infrastructure spending

436
00:36:46,960 --> 00:36:54,160
as opposed to capitalistic. So of course their state-owned enterprises are also publicly traded,

437
00:36:54,160 --> 00:37:00,080
but they also carry a government responsibility to deliver infrastructure to all.

438
00:37:00,080 --> 00:37:05,280
So it's a different way of thinking that may be very hard to inject into western countries

439
00:37:05,280 --> 00:37:10,320
to say starting tomorrow, bandwidth infrastructure and highways are going to be

440
00:37:12,080 --> 00:37:15,040
governmental spending with some characteristics.

441
00:37:16,160 --> 00:37:20,160
What's your sense and sorry to interrupt, but because it's such a fascinating point.

442
00:37:21,520 --> 00:37:23,760
Do you think on the autonomous vehicle space

443
00:37:24,400 --> 00:37:31,440
it's possible to solve the problem of full autonomy without significant investment in

444
00:37:31,440 --> 00:37:38,160
infrastructure? Well, that's really hard to speculate. I think it's not a yes-no question,

445
00:37:38,160 --> 00:37:45,360
but how long does it take question? You know, 15 years, 30 years, 45 years. Clearly with

446
00:37:45,360 --> 00:37:52,640
infrastructure augmentation, whether it's road, city or whole city planning, building a new city,

447
00:37:53,280 --> 00:38:00,800
I'm sure that will accelerate the day of the L5. I'm not knowledgeable enough and it's hard to

448
00:38:00,800 --> 00:38:07,520
predict even when we're knowledgeable because a lot of it is speculative. But in the U.S.,

449
00:38:07,520 --> 00:38:12,960
I don't think people would consider building a new city the size of Chicago to make it the AI

450
00:38:12,960 --> 00:38:19,360
slash autonomous city. There are smaller ones being built, I'm aware of that. But is infrastructure

451
00:38:20,320 --> 00:38:26,720
spent really impossible for U.S. or Western countries? I don't think so. The U.S. highway

452
00:38:26,720 --> 00:38:36,400
system was built. Was that during President Eisenhower or Kennedy? So maybe historians can

453
00:38:36,400 --> 00:38:42,640
study how the President Eisenhower get the resources to build this massive infrastructure

454
00:38:42,720 --> 00:38:50,720
that surely gave U.S. a tremendous amount of prosperity over the next decade, if not century.

455
00:38:50,720 --> 00:38:55,920
If I may comment on that then, it takes us to artificial intelligence a little bit because

456
00:38:55,920 --> 00:39:01,600
in order to build infrastructure, it creates a lot of jobs. So I'll be actually interested

457
00:39:02,320 --> 00:39:07,520
if you would say that you're talking your book about all kinds of jobs that could and could not

458
00:39:07,520 --> 00:39:14,240
be automated. I wonder if building infrastructure is one of the jobs that would not be easily

459
00:39:14,240 --> 00:39:18,960
automated. It's something you can think about because I think you've mentioned somewhere in a

460
00:39:18,960 --> 00:39:26,080
talk or that there might be, as jobs are being automated, a role for government to create jobs

461
00:39:26,080 --> 00:39:33,200
that can't be automated. Yes, I think that's a possibility. Back in the last financial crisis,

462
00:39:34,160 --> 00:39:43,280
China put a lot of money to basically give this economy a boost and a lot of it went into

463
00:39:43,280 --> 00:39:50,080
infrastructure building. And I think that's a legitimate way at the government level to

464
00:39:51,840 --> 00:39:57,680
deal with the employment issues as well as build out the infrastructure. As long as the

465
00:39:57,680 --> 00:40:02,320
infrastructures are truly needed and as long as there isn't an employment problem, which

466
00:40:03,040 --> 00:40:11,920
no, we don't know. So maybe taking a little step back, if you've been a leader and a researcher

467
00:40:11,920 --> 00:40:20,880
in AI for several decades, at least 30 years, so how has AI changed in the West and the East

468
00:40:20,880 --> 00:40:24,160
as you've observed as you've been deep in it over the past 30 years?

469
00:40:24,960 --> 00:40:31,520
Well, AI began as the pursuit of understanding human intelligence and the term itself

470
00:40:32,560 --> 00:40:38,800
represents that. But it kind of drifted into the one subarea that worked extremely well,

471
00:40:38,800 --> 00:40:45,200
which is machine intelligence. And that's actually more using pattern recognition techniques to

472
00:40:46,560 --> 00:40:53,840
basically do incredibly well on a limited domain, large amount of data, but relatively

473
00:40:53,920 --> 00:41:01,360
simple kinds of planning tasks and not very creative. So we didn't end up building human

474
00:41:01,360 --> 00:41:08,000
intelligence. We built a different machine that was a lot better than us, some problems,

475
00:41:08,000 --> 00:41:14,960
but nowhere close to us on other problems. So today, I think a lot of people still misunderstand

476
00:41:16,160 --> 00:41:21,840
when we say artificial intelligence and what various products can do, people still think

477
00:41:21,920 --> 00:41:27,760
it's about replicating human intelligence. But the products out there really are closer to

478
00:41:29,120 --> 00:41:34,240
having invented the internet or the spreadsheet or the database and getting broader adoption.

479
00:41:35,280 --> 00:41:41,200
And speaking further to the fears, near-term fears that people have about AI, so you're commenting

480
00:41:41,200 --> 00:41:48,560
on the general intelligence that people in the popular culture from sci-fi movies have a sense

481
00:41:48,560 --> 00:41:54,720
about AI. But there's practical fears about AI, the kind, the narrow AI that you're talking about

482
00:41:54,720 --> 00:42:00,480
of automating particular kinds of jobs. And you talk about them in the book. So what are the kinds

483
00:42:00,480 --> 00:42:07,680
of jobs in your view that you see in the next five, 10 years beginning to be automated by AI systems,

484
00:42:07,680 --> 00:42:15,200
algorithms? Yes, this is also maybe a little bit counterintuitive, because it's the routine jobs

485
00:42:15,200 --> 00:42:24,000
that will be displaced the soonest. And they may not be displaced entirely, maybe 50%, 80% of a job,

486
00:42:24,000 --> 00:42:30,640
but when the workload drops by that much, employment will come down. And also another part of

487
00:42:30,640 --> 00:42:36,560
misunderstanding is most people think of AI replacing routine jobs, then they think of the

488
00:42:36,560 --> 00:42:42,960
assembly line, the workers. Well, that will have some effect, but it's actually the routine white

489
00:42:42,960 --> 00:42:49,680
color workers that's easiest to replace. Because to replace a white color worker, you just need

490
00:42:49,680 --> 00:42:58,400
software. To replace a blue color worker, you need robotics, mechanical excellence, and the ability to

491
00:42:58,400 --> 00:43:07,040
deal with dexterity, and maybe even unknown environments, very, very difficult. So if we were

492
00:43:07,040 --> 00:43:14,960
to categorize the most dangerous white collar jobs, they would be things like back office,

493
00:43:15,520 --> 00:43:24,240
people who copy and paste and deal with simple computer programs and data, and maybe paper and

494
00:43:24,240 --> 00:43:32,960
OCR, and they don't make strategic decisions, they basically facilitate the process, these softwares

495
00:43:32,960 --> 00:43:38,880
and paper systems don't work. So you have people dealing with new employee orientation,

496
00:43:40,640 --> 00:43:47,280
searching for past lawsuits and financial documents, and doing reference check,

497
00:43:47,840 --> 00:43:53,360
the basic searching and management of data, that's the most in danger of being lost. In addition to

498
00:43:53,360 --> 00:44:00,480
the white collar repetitive work, a lot of simple interaction work can also be taken care of, such

499
00:44:00,480 --> 00:44:08,480
as tele-sales, telemarketing, customer service, as well as many physical jobs that are in the same

500
00:44:08,480 --> 00:44:16,720
location and don't require a high degree of dexterity. So fruit picking, dishwashing, assembly line,

501
00:44:16,720 --> 00:44:24,000
inspection are jobs in that category. So altogether, back office is a big part.

502
00:44:24,960 --> 00:44:31,920
And the other, the blue collar may be smaller initially, but over time AI will get better.

503
00:44:31,920 --> 00:44:38,720
And when we start to get to over the next 15, 20 years, the ability to actually have the dexterity

504
00:44:38,720 --> 00:44:44,960
of doing assembly line, that's a huge chunk of jobs. And when autonomous vehicles start to work,

505
00:44:44,960 --> 00:44:50,560
initially starting with truck drivers, but eventually to all drivers, that's another huge group of

506
00:44:50,960 --> 00:44:58,000
workers. So I see modest numbers in the next five years, but increasing rapidly after that.

507
00:44:58,000 --> 00:45:02,720
On the worry of the jobs that are in danger and the gradual loss of jobs.

508
00:45:03,920 --> 00:45:06,000
I'm not sure if you're familiar with Andrew Yang.

509
00:45:06,560 --> 00:45:07,120
Yes, I am.

510
00:45:07,760 --> 00:45:13,760
So there's a candidate for president of the United States whose platform Andrew Yang is based around,

511
00:45:14,320 --> 00:45:21,680
in part, around job loss due to automation. And also in addition, the need perhaps of universal

512
00:45:21,680 --> 00:45:28,560
basic income to support jobs that are folks who lose their job due to automation and so on. And

513
00:45:28,560 --> 00:45:36,000
in general, support people under complex, unstable job market. So what are your thoughts about his

514
00:45:36,000 --> 00:45:42,400
concerns, him as a candidate, his ideas in general? I think his thinking is generally in the right

515
00:45:42,400 --> 00:45:50,560
direction. But his approach as a presidential candidate may be a little bit ahead of the time.

516
00:45:52,160 --> 00:45:58,800
I think the displacements will happen. But will they happen soon enough for people to agree to

517
00:45:58,800 --> 00:46:06,720
vote for him? The unemployment numbers are not very high yet. And I think he and I have the same

518
00:46:06,720 --> 00:46:12,400
challenge. If I want to theoretically convince people this is an issue and he wants to become

519
00:46:12,400 --> 00:46:19,680
the president, people have to see how can this be the case when unemployment numbers are low.

520
00:46:19,680 --> 00:46:26,400
So that is the challenge. And I think we do, I do agree with him on the displacement issue,

521
00:46:27,280 --> 00:46:35,440
on universal basic income. At a very vanilla level, I don't agree with it. Because I think

522
00:46:35,440 --> 00:46:43,920
the main issue is retraining. So people need to be incented, not by just giving a monthly $2,000

523
00:46:43,920 --> 00:46:52,400
check or $1,000 check and do whatever they want, because they don't have the know how to know what

524
00:46:52,400 --> 00:47:00,400
to retrain to go into what type of a job and guidance is needed. And retraining is needed

525
00:47:00,400 --> 00:47:05,040
because historically, when technology revolutions, when routine jobs were displaced,

526
00:47:05,040 --> 00:47:11,760
new routine jobs came up. So there was always room for that. But with AI and automation,

527
00:47:11,760 --> 00:47:16,480
the whole point is replacing all routine jobs eventually. So there will be fewer and fewer

528
00:47:16,480 --> 00:47:23,840
routine jobs. And AI will create jobs, but it won't create routine jobs. Because if it creates

529
00:47:23,840 --> 00:47:30,320
routine jobs, why wouldn't AI just do it? So therefore, the people who are losing the jobs

530
00:47:30,320 --> 00:47:36,720
are losing routine jobs, the jobs that are becoming available are non-routine jobs. So the social

531
00:47:36,720 --> 00:47:43,280
stipend needs to be put in place is for the routine workers who lost their jobs to be retrained,

532
00:47:43,280 --> 00:47:48,960
maybe in six months, maybe in three years, takes a while to retrain on the non-routine job, and then

533
00:47:48,960 --> 00:47:55,680
take on a job that will last for that person's lifetime. Now, having said that, if you look

534
00:47:55,680 --> 00:48:01,840
deeply into Andrew's document, he does cater for that. So I'm not disagreeing with what he's trying

535
00:48:01,840 --> 00:48:08,640
to do. But for simplification, sometimes he just says UBI, but simple UBI wouldn't work.

536
00:48:08,640 --> 00:48:15,680
And I think you've mentioned elsewhere that the goal isn't necessarily to give people enough money

537
00:48:15,760 --> 00:48:22,080
to survive or live or even to prosper. The point is to give them a job that gives them meaning.

538
00:48:22,640 --> 00:48:28,480
That meaning is extremely important. That our employment, at least in the United States,

539
00:48:28,480 --> 00:48:34,480
and perhaps it cares across the world, provides something that's, forgive me for saying,

540
00:48:34,480 --> 00:48:44,640
greater than money. It provides meaning. So now, what kind of jobs do you think can't be automated?

541
00:48:44,720 --> 00:48:49,600
You talk a little bit about creativity and compassion in your book. What aspects do you

542
00:48:49,600 --> 00:48:56,320
think it's difficult to automate for an AI system? Because an AI system is currently

543
00:48:56,320 --> 00:49:02,880
merely optimizing. It's not able to reason, plan, or think creatively or strategically.

544
00:49:03,520 --> 00:49:10,160
It's not able to deal with complex problems. It can't come up with a new problem and solve it.

545
00:49:10,240 --> 00:49:16,240
A human needs to find the problem and pose it as an optimization problem,

546
00:49:16,240 --> 00:49:24,560
then have the AI work at it. So an AI would have a very hard time discovering a new drug or

547
00:49:24,560 --> 00:49:32,880
discovering a new style of painting or dealing with complex tasks such as managing a company

548
00:49:32,880 --> 00:49:38,000
that isn't just about optimizing the bottom line, but also about employee satisfaction,

549
00:49:38,640 --> 00:49:45,360
corporate brand, and many, many other things. So that is one category of things. And because

550
00:49:45,360 --> 00:49:50,560
these things are challenging, creative, complex, doing them creates a higher,

551
00:49:50,560 --> 00:49:56,160
high degree of satisfaction, and therefore appealing to our desire for working, which

552
00:49:56,160 --> 00:50:01,120
isn't just to make the money, make the ends meet, but also that we've accomplished something that

553
00:50:01,120 --> 00:50:07,600
others maybe can't do or can't do as well. Another type of job that is much numerous

554
00:50:07,680 --> 00:50:14,960
would be compassionate jobs, jobs that require compassion, empathy, human touch, human trust.

555
00:50:14,960 --> 00:50:22,400
AI can't do that because AI is cold, calculating, and even if it can fake that to some extent,

556
00:50:23,200 --> 00:50:29,680
it will make errors and that will make it look very silly. And also, I think even if AI did okay,

557
00:50:29,680 --> 00:50:37,200
people would want to interact with another person, whether it's for some kind of a service or teacher

558
00:50:37,200 --> 00:50:45,520
or doctor or concierge or a masseuse or bartender. There are so many jobs where people just don't

559
00:50:45,520 --> 00:50:53,120
want to interact with a cold robot or software. I've had an entrepreneur who built an elderly care

560
00:50:53,120 --> 00:50:59,760
robot, and they found that the elderly really only use it for customer service. But not to

561
00:50:59,760 --> 00:51:05,360
service the product, but they click on customer service and the video of a person comes up,

562
00:51:05,440 --> 00:51:10,320
and then the person says, how come my daughter didn't call me? Let me show you a picture of

563
00:51:10,320 --> 00:51:17,520
her grandkids. So people earn for that, people people interaction. So even if robots improved,

564
00:51:17,520 --> 00:51:23,280
people just don't want it. And those jobs are going to be increasing because AI will create a lot

565
00:51:23,280 --> 00:51:30,960
of value, $16 trillion to the world in next 11 years, according to PWC. And that will give people

566
00:51:31,520 --> 00:51:39,520
money to enjoy services, whether it's eating a gourmet meal, or tourism and traveling,

567
00:51:39,520 --> 00:51:46,080
or having concierge services, the services revolving around every dollar of that $16

568
00:51:46,080 --> 00:51:52,640
trillion will be tremendous. It will create more opportunities to service the people who did well

569
00:51:52,720 --> 00:52:02,320
through AI with things. But even at the same time, the entire society is very much short in need of

570
00:52:02,320 --> 00:52:09,280
many service oriented, compassionate oriented jobs. The best example is probably in healthcare

571
00:52:09,280 --> 00:52:16,800
services. There's going to be 2 million new jobs, not counting replacement, just brand new incremental

572
00:52:16,800 --> 00:52:24,160
jobs in the next six years in healthcare services. That includes nurses, orderly in the hospital,

573
00:52:24,880 --> 00:52:33,600
elderly care, and also at home care. It's particularly lacking. And those jobs are not

574
00:52:33,600 --> 00:52:38,480
likely to be filled. So there's likely to be a shortage. And the reason they're not filled

575
00:52:38,480 --> 00:52:45,200
is simply because they don't pay very well. And that the social status of these jobs are

576
00:52:45,920 --> 00:52:52,240
not very good. So they pay about half as much as a heavy equipment operator,

577
00:52:52,240 --> 00:52:59,120
which will be replaced a lot sooner. And they pay probably comparably to someone on the assembly line.

578
00:52:59,840 --> 00:53:07,200
And if so, if we ignoring all the other issues and just think about satisfaction from one's job,

579
00:53:07,200 --> 00:53:13,120
someone repetitively doing the same manual action and assembly line, that can't create a lot of

580
00:53:13,120 --> 00:53:19,680
job satisfaction, but someone taking care of a sick person and getting a hug and thank you from

581
00:53:19,680 --> 00:53:27,200
that person and the family, I think is quite satisfying. So if only we could fix the pay

582
00:53:27,200 --> 00:53:33,360
for service jobs, there are plenty of jobs that require some training or a lot of training

583
00:53:33,360 --> 00:53:41,680
for the people coming off the routine jobs to take. We can easily imagine someone who was maybe a

584
00:53:41,680 --> 00:53:48,560
cashier at the grocery store, as stores become automated, learns to become a nurse or a at home

585
00:53:48,560 --> 00:53:54,720
care. Also do want to point out the blue collar jobs are going to stay around a bit longer,

586
00:53:54,720 --> 00:54:03,120
some of them quite a bit longer. AI cannot be told go clean an arbitrary home. That's incredibly

587
00:54:03,120 --> 00:54:10,000
hard, arguably is an L five level of difficulty, right? And then AI cannot be a good plumber,

588
00:54:10,000 --> 00:54:15,680
because plumber is almost like a mini detective that has to figure out where the leak came from.

589
00:54:15,680 --> 00:54:24,640
So yet AI probably can be an assembly line and auto mechanic and so on. So one has to study

590
00:54:24,640 --> 00:54:29,760
which blue collar jobs are going away and facilitate retraining for the people to go into

591
00:54:29,760 --> 00:54:34,560
the ones that won't go away or maybe even will increase. I mean, it is fascinating that it's

592
00:54:34,560 --> 00:54:41,680
easier to build a world champion chess player than it is to build a mediocre plumber.

593
00:54:43,040 --> 00:54:48,880
Very true. And that goes counterintuitive to a lot of people's understanding of what artificial

594
00:54:48,880 --> 00:54:54,640
intelligence is. So it sounds, I mean, you're painting a pretty optimistic picture about

595
00:54:54,640 --> 00:54:58,880
retraining about the number of jobs and actually the meaningful nature of those jobs

596
00:54:59,440 --> 00:55:05,200
once we automate repetitive tasks. So overall, are you optimistic about

597
00:55:06,800 --> 00:55:13,120
the future where much of the repetitive tasks are automated, that there is a lot of room for

598
00:55:13,120 --> 00:55:18,880
humans for the compassionate for the creative input that only humans can provide?

599
00:55:19,920 --> 00:55:27,520
I am optimistic if we start to take action. If we have no action in the next five years,

600
00:55:27,520 --> 00:55:33,040
I think it's going to be hard to deal with the devastating losses that will emerge.

601
00:55:34,400 --> 00:55:38,560
So if we start thinking about retraining, maybe with the low hanging fruits,

602
00:55:39,280 --> 00:55:45,360
explaining to vocational schools why they should train more plumbers than auto mechanics,

603
00:55:46,560 --> 00:55:51,440
maybe starting with some government subsidy for corporations to have more training

604
00:55:52,400 --> 00:55:58,960
positions. We start to explain to people why retraining is important. We start to think about

605
00:55:58,960 --> 00:56:05,520
what the future of education, how that needs to be tweaked for the era of AI. If we start to make

606
00:56:05,520 --> 00:56:10,560
incremental progress and a greater number of people understand, then there's no reason to think we

607
00:56:10,560 --> 00:56:17,200
can't deal with this because this technological revolution is arguably similar to what electricity,

608
00:56:17,200 --> 00:56:22,560
industrial revolutions and internet brought about. Do you think there's a role for policy,

609
00:56:22,560 --> 00:56:26,880
for governments to step in to help with policy to create a better world?

610
00:56:28,080 --> 00:56:33,280
Absolutely, and the governments don't have to believe an employment will go up,

611
00:56:33,920 --> 00:56:38,160
and they don't have to believe automation will be this fast to do something.

612
00:56:39,280 --> 00:56:45,520
Revamping vocational school would be one example. Another is if there's a big gap in healthcare

613
00:56:45,520 --> 00:56:52,720
service employment, and we know that a country's population is growing older, more longevity,

614
00:56:52,720 --> 00:56:59,040
living older, because people over 80 require five times as much care as those under 80,

615
00:56:59,600 --> 00:57:06,880
then it is a good time to incent training programs for elderly care to find ways to improve the pay.

616
00:57:07,440 --> 00:57:13,760
Maybe one way would be to offer as part of Medicare or the equivalent program for people

617
00:57:13,760 --> 00:57:22,000
over 80 to be entitled to a few hours of elderly care at home, and then that might be reimbursable,

618
00:57:22,000 --> 00:57:28,720
and that will stimulate the service industry around the policy.

619
00:57:28,720 --> 00:57:35,280
Do you have concerns about large entities, whether it's governments or companies,

620
00:57:35,280 --> 00:57:40,800
controlling the future of AI development in general? So we talked about companies,

621
00:57:40,800 --> 00:57:49,360
do you have a better sense that governments can better represent the interests of the people

622
00:57:49,360 --> 00:57:54,720
than companies, or do you believe companies are better at representing the interests of the people,

623
00:57:54,720 --> 00:57:56,560
or is there no easy answer?

624
00:57:56,560 --> 00:57:59,440
I don't think there's an easy answer, because it's a double-edged sword.

625
00:58:00,000 --> 00:58:05,440
The companies and governments can provide better services with more access to data and more access

626
00:58:05,440 --> 00:58:13,440
to AI, but that also leads to greater power, which can lead to uncontrollable problems,

627
00:58:13,440 --> 00:58:17,040
whether it's monopoly or corruption in the government.

628
00:58:17,680 --> 00:58:24,880
So I think one has to be careful to look at how much data that companies and governments have,

629
00:58:24,880 --> 00:58:29,120
and some kind of checks and balances would be helpful.

630
00:58:29,200 --> 00:58:35,680
So again, I come from Russia. There's something called the Cold War.

631
00:58:36,720 --> 00:58:40,640
So let me ask a difficult question here, looking at conflict.

632
00:58:40,640 --> 00:58:45,280
Steven Pinker wrote a great book that conflict all over the world is decreasing in general,

633
00:58:45,280 --> 00:58:51,760
but do you have a sense that having written the book AI Superpowers,

634
00:58:51,760 --> 00:58:57,760
do you see a major international conflict potentially arising between major nations,

635
00:58:57,760 --> 00:59:02,960
whatever they are, whether it's Russia, China, European nations, United States,

636
00:59:02,960 --> 00:59:10,080
or others in the next 10, 20, 50 years around AI, around the digital space, cyber space?

637
00:59:10,080 --> 00:59:18,320
Do you worry about that? Is that something we need to think about and try to alleviate or prevent?

638
00:59:19,520 --> 00:59:26,000
I believe in greater engagement. A lot of the worries about more powerful AI

639
00:59:26,720 --> 00:59:40,320
are based on a arms race metaphor. And when you extrapolate into military kinds of scenarios,

640
00:59:41,440 --> 00:59:48,080
AI can automate and autonomous weapons that needs to be controlled somehow,

641
00:59:48,800 --> 00:59:56,880
and autonomous decision making can lead to not enough time to fix international crises.

642
00:59:57,600 --> 01:00:04,000
So I actually believe a Cold War mentality would be very dangerous, because should

643
01:00:04,000 --> 01:00:09,920
two countries rely on AI to make certain decisions and they don't talk to each other,

644
01:00:09,920 --> 01:00:14,000
they do their own scenario planning, then something could easily go wrong.

645
01:00:14,960 --> 01:00:23,280
I think engagement, interaction, some protocols to avoid inadvertent disasters

646
01:00:23,280 --> 01:00:29,040
is actually needed. So it's natural for each country to want to be the best,

647
01:00:29,040 --> 01:00:37,840
whether it's in nuclear technologies or AI or bio. But I think it's important to realize if

648
01:00:38,640 --> 01:00:44,880
each country has a black box AI and don't talk to each other, that probably presents greater

649
01:00:46,400 --> 01:00:53,600
challenges to humanity than if they interacted. I think there can still be competition,

650
01:00:53,600 --> 01:01:01,120
but with some degree of protocol for interaction. Just like when there was a nuclear competition,

651
01:01:02,080 --> 01:01:08,400
there were some protocol for deterrence among US, Russia and China. And I think

652
01:01:08,960 --> 01:01:15,280
that engagement is needed. So of course, we're still far from AI presenting that kind of danger.

653
01:01:15,920 --> 01:01:22,880
But what I worry the most about is the level of engagement seems to be coming down.

654
01:01:22,880 --> 01:01:29,680
The level of distrust seems to be going up, especially from the US towards other large

655
01:01:29,680 --> 01:01:35,360
countries such as China and Russia. Is there a way to make that better? So that's beautifully

656
01:01:35,360 --> 01:01:46,960
put, level of engagement and even just basic trust and communication as opposed to making

657
01:01:46,960 --> 01:01:56,320
artificial enemies out of particular countries. Do you have a sense how we can make it better?

658
01:01:57,120 --> 01:02:04,320
actionable items that as a society we can take on? I'm not an expert at geopolitics,

659
01:02:04,880 --> 01:02:13,120
but I would say that we look pretty foolish as humankind when we are faced with the opportunity

660
01:02:13,120 --> 01:02:26,160
to create 16 trillion dollars for humanity. And yet we're not solving fundamental problems with

661
01:02:26,640 --> 01:02:33,040
parts of the world still in poverty. And for the first time, we have the resources to overcome

662
01:02:33,040 --> 01:02:38,640
poverty and hunger. We're not using it on that, but we're fueling competition among superpowers.

663
01:02:38,640 --> 01:02:45,520
And that's a very unfortunate thing. If we become utopian for a moment, imagine a

664
01:02:45,840 --> 01:02:56,400
benevolent world government that has this 16 trillion dollars and maybe some AI to figure

665
01:02:56,400 --> 01:03:02,480
out how to use it to deal with diseases and problems and hate and things like that,

666
01:03:02,480 --> 01:03:08,640
world would be a lot better off. So what is wrong with the current world? I think the people

667
01:03:08,720 --> 01:03:15,680
with more skill than I should think about this. And then the geopolitics issue with

668
01:03:15,680 --> 01:03:21,520
superpower competition is one side of the issue. There's another side which I worry maybe even

669
01:03:23,200 --> 01:03:30,560
more, which is as the 16 trillion dollars all gets made by US and China and a few of the other

670
01:03:30,560 --> 01:03:35,840
developed countries, the poorer country will get nothing because they don't have technology.

671
01:03:36,800 --> 01:03:44,480
And the wealth disparity and inequality will increase. So a poorer country with a large

672
01:03:44,480 --> 01:03:51,280
population will not only benefit from the AI boom or other technology booms, but they will have

673
01:03:51,280 --> 01:03:57,440
their workers who previously had hoped they could do the China model and do outsource manufacturing

674
01:03:57,440 --> 01:04:03,360
or the India model. So they could do the outsource process or call center while all those jobs are

675
01:04:03,360 --> 01:04:11,920
going to be gone in 10 or 15 years. So the individual citizen may be a net liability,

676
01:04:11,920 --> 01:04:19,120
I mean financially speaking to a poorer country and not an asset to claw itself out of poverty.

677
01:04:19,760 --> 01:04:27,280
So in that kind of situation, these large countries with not much tech are going to be

678
01:04:27,280 --> 01:04:34,960
facing a downward spiral and it's unclear what could be done. And then when we look back and say

679
01:04:34,960 --> 01:04:40,560
there's 16 trillion dollars being created and it's all being kept by US, China and other developed

680
01:04:40,560 --> 01:04:48,000
countries, it just doesn't feel right. So I hope people who know about geopolitics can find solutions

681
01:04:48,000 --> 01:04:54,240
that's beyond my expertise. So different countries that we've talked about have different value

682
01:04:54,240 --> 01:05:01,280
systems. If you look at the United States to an almost extreme degree, there is an absolute

683
01:05:01,280 --> 01:05:06,000
desire for freedom of speech. If you look at a country where I was raised, that desire just

684
01:05:06,000 --> 01:05:14,880
amongst the people is not as elevated as it is to basically fundamental level

685
01:05:14,880 --> 01:05:19,040
to the essence of what it means to be America. And the same is true with China,

686
01:05:19,040 --> 01:05:28,400
there's different value systems. There is some censorship of internet content that China and

687
01:05:28,400 --> 01:05:36,000
Russia and many other countries undertake. Do you see that having effects on innovation,

688
01:05:36,640 --> 01:05:41,520
other aspects of some of the tech stuff AI development we talked about, and maybe from

689
01:05:41,520 --> 01:05:47,840
another angle, do you see that changing in different ways over the next 10 years, 20 years,

690
01:05:47,840 --> 01:05:54,480
50 years as China continues to grow as it does now in its tech innovation?

691
01:05:55,600 --> 01:06:02,880
There's a common belief that full freedom of speech and expression is correlated with creativity,

692
01:06:02,880 --> 01:06:11,760
which is correlated with entrepreneurial success. I think empirically, we have seen that is not

693
01:06:11,760 --> 01:06:19,680
true. And China has been successful. That's not to say the fundamental values are not right or

694
01:06:19,680 --> 01:06:27,840
not the best, but it's just that that perfect correlation isn't there. It's hard to read the

695
01:06:27,840 --> 01:06:34,880
tea leaves on an opening up or not in any country. And I've not been very good at that in my past

696
01:06:34,880 --> 01:06:44,880
predictions. But I do believe every country shares some fundamental value, a lot of fundamental

697
01:06:44,880 --> 01:06:57,200
values for the long term. So you know, China is drafting its privacy policy for individual citizens.

698
01:06:57,200 --> 01:07:05,200
And they don't look that different from the American or European ones. So people do want

699
01:07:05,200 --> 01:07:13,360
to protect their privacy and have the opportunity to express. And I think the fundamental values

700
01:07:13,360 --> 01:07:21,040
are there. The question is in the execution and timing, how soon or when will that start to open

701
01:07:21,040 --> 01:07:27,920
up? So as long as each government knows, ultimately, people want that kind of protection,

702
01:07:29,120 --> 01:07:34,960
there should be a plan to move towards that as to when or how, and I'm not an expert.

703
01:07:36,160 --> 01:07:42,480
On the point of privacy to me, it's really interesting. So AI needs data to create a

704
01:07:42,480 --> 01:07:47,360
personalized, awesome experience. I'm just speaking generally in terms of products.

705
01:07:48,080 --> 01:07:53,040
And then we have currently, depending on the age and depending on the demographics of who we're

706
01:07:53,040 --> 01:07:58,960
talking about, some people are more or less concerned about the amount of data they hand over.

707
01:07:58,960 --> 01:08:07,280
So in your view, how do we get this balance right? That we provide an amazing experience to

708
01:08:07,920 --> 01:08:13,440
people that use products? You look at Facebook, you know, the more Facebook knows about you,

709
01:08:13,440 --> 01:08:21,040
yes, it's scary to say, the better it can probably, a better experience it can probably create.

710
01:08:21,040 --> 01:08:23,280
So in your view, how do we get that balance right?

711
01:08:25,120 --> 01:08:34,320
Yes, I think a lot of people have a misunderstanding that it's okay and possible to just rip all the

712
01:08:34,320 --> 01:08:41,120
data out from a provider and give it back to you. So you can deny them access to further data

713
01:08:41,120 --> 01:08:47,440
and still enjoy the services we have. If we take back all the data, all the services will give us

714
01:08:47,440 --> 01:08:53,520
nonsense. We'll no longer be able to use products that function well in terms of, you know,

715
01:08:54,480 --> 01:09:00,880
right ranking, right products, right user experience. So yet I do understand we don't

716
01:09:00,880 --> 01:09:10,240
want to permit misuse of the data. From legal policy standpoint, I think there can be severe

717
01:09:10,240 --> 01:09:19,680
punishment for those who have egregious misuse of the data. That's, I think, a good first step,

718
01:09:19,680 --> 01:09:25,920
actually China in this side, on this aspect has very strong laws about people who sell or give

719
01:09:25,920 --> 01:09:33,600
data to other companies. And that over the past few years, since that law came into effect,

720
01:09:33,600 --> 01:09:43,760
pretty much eradicated the illegal distribution sharing of data. Additionally, I think giving,

721
01:09:45,440 --> 01:09:54,640
I think technology is often a very good way to solve technology misuse. So can we come up with

722
01:09:54,640 --> 01:10:00,080
new technologies that will let us have our cake and eat it too? People are looking into

723
01:10:00,080 --> 01:10:06,240
homomorphic encryption, which is letting you keep the data, have it encrypted and train on

724
01:10:06,240 --> 01:10:11,760
encrypted data. Of course, we haven't solved that one yet, but that kind of direction may be worth

725
01:10:11,760 --> 01:10:18,720
pursuing. Also federated learning, which would allow one hospital to train on its hospitals,

726
01:10:18,720 --> 01:10:24,080
patient data fully, because they have a license for that. And then hospitals will then share

727
01:10:24,080 --> 01:10:30,640
their models, not data but models, to create a supra AI. And that also maybe has some promise.

728
01:10:30,640 --> 01:10:38,720
So I would want to encourage us to be open-minded and think of this as not just the policy binary,

729
01:10:38,720 --> 01:10:44,880
yes, no, but letting the technologists try to find solutions to let us have our cake and eat it too,

730
01:10:44,880 --> 01:10:51,680
or have most of our cake and eat most of it too. Finally, I think giving each end user a choice

731
01:10:51,760 --> 01:10:57,680
is important. And having transparency is important. Also, I think that's universal. But

732
01:10:58,480 --> 01:11:03,840
the choice you give to the user should not be at a granular level that the user cannot understand.

733
01:11:04,720 --> 01:11:11,360
GDPR today causes all these pop-ups of yes, no, will you give this site this right to use this part

734
01:11:11,360 --> 01:11:17,840
of your data? I don't think any user understands what they're saying yes or no to. And I suspect

735
01:11:17,840 --> 01:11:24,400
most are just saying yes because they don't understand it. So while GDPR in its current

736
01:11:24,400 --> 01:11:31,440
implementation has lived up to its promise of transparency and user choice, it implemented

737
01:11:31,440 --> 01:11:40,640
it in such a way that really didn't deliver the spirit of GDPR. They fit the letter but not the

738
01:11:40,640 --> 01:11:48,640
spirit. So again, I think we need to think about is there a way to fit the spirit of GDPR by using

739
01:11:48,640 --> 01:11:56,080
some kind of technology? Can we have a slider that's an AI trying to figure out how much you want to

740
01:11:56,080 --> 01:12:04,160
slide between perfect protection security of your personal data versus a high degree of convenience

741
01:12:04,160 --> 01:12:10,640
with some risks of not having full privacy? Each user should have some preference and that gives

742
01:12:10,640 --> 01:12:16,560
you the user choice. But maybe we should turn the problem on its head and ask, can there be an AI

743
01:12:16,560 --> 01:12:22,080
algorithm that can customize this? Because we can understand the slider but we sure cannot

744
01:12:22,080 --> 01:12:29,040
understand every pop-up question. And I think getting that right requires getting the balance

745
01:12:29,120 --> 01:12:36,080
between what we talked about earlier, which is hard and soul versus profit driven decisions and

746
01:12:36,080 --> 01:12:43,680
strategy. I think from my perspective, the best way to make a lot of money in the long term is to

747
01:12:43,680 --> 01:12:51,840
keep your heart and soul intact. I think getting that slider right in the short term may feel like

748
01:12:51,840 --> 01:12:58,560
you'll be sacrificing profit. But in the long term, you'll be getting user trust and providing a

749
01:12:58,560 --> 01:13:05,680
great experience. Do you share that kind of view in general? Yes, absolutely. I sure would hope

750
01:13:05,680 --> 01:13:12,960
there is a way we can do long term projects that really do the right thing. I think a lot of people

751
01:13:12,960 --> 01:13:19,040
who embrace GDPR, their heart's in the right place. I think they just need to figure out

752
01:13:19,040 --> 01:13:24,640
how to build a solution. I've heard utopians talk about solutions that get me excited but

753
01:13:24,640 --> 01:13:31,200
I'm not sure how in the current funding environment they can get started. People talk about, imagine

754
01:13:31,200 --> 01:13:41,200
this crowdsourced data collection that we all trust and then we have these agents that we

755
01:13:41,920 --> 01:13:50,720
ask them to ask the trusted agent, that agent only, that platform. So a trusted joint platform

756
01:13:51,200 --> 01:14:03,040
that we all believe is trustworthy that can give us all the close loop personal suggestions

757
01:14:03,040 --> 01:14:08,960
by the new social network, new search engine, new e-commerce engine that has access to even

758
01:14:08,960 --> 01:14:16,720
more of our data but not directly but indirectly. So I think that general concept of licensing

759
01:14:16,720 --> 01:14:22,160
to some trusted engine and finding a way to trust that engine seems like a great idea

760
01:14:22,160 --> 01:14:27,600
but if you think how long it's going to take to implement and tweak and develop it right

761
01:14:27,600 --> 01:14:32,560
as well as to collect all the trusts and the data from the people, it's beyond the

762
01:14:32,560 --> 01:14:37,200
current cycle of venture capital. So how do you do that is a big question.

763
01:14:38,000 --> 01:14:42,960
You've recently had a fight with cancer, stage 4 lymphoma

764
01:14:44,640 --> 01:14:51,760
and in a sort of deep personal level, what did it feel like in the darker moments to face your

765
01:14:51,760 --> 01:15:01,360
own mortality? Well I've been the workaholic my whole life and I've basically worked 9.96,

766
01:15:01,360 --> 01:15:08,800
9am to 9pm, 6 days a week roughly and I didn't really pay a lot of attention to my family,

767
01:15:08,800 --> 01:15:14,960
friends and people who loved me and my life revolved around optimizing for work. While my

768
01:15:14,960 --> 01:15:24,560
work was not routine, my optimization really what made my life basically a very mechanical

769
01:15:24,560 --> 01:15:31,840
process but I got a lot of highs out of it because of accomplishments that I thought were

770
01:15:32,800 --> 01:15:39,440
really important and dear and the highest priority to me but when I faced mortality and the possible

771
01:15:39,440 --> 01:15:46,000
death in matter of months, I suddenly realized that this really meant nothing to me, that I

772
01:15:46,000 --> 01:15:51,680
didn't feel like working for another minute, that if I had six months left in my life I would spend

773
01:15:51,920 --> 01:15:59,920
all with my loved ones and thanking them, giving them love back and apologizing to them that I

774
01:15:59,920 --> 01:16:09,520
lived my life the wrong way. So that moment of reckoning caused me to really rethink that why

775
01:16:09,520 --> 01:16:18,800
we exist in this world is something that we might be too much shaped by the society to think that

776
01:16:18,800 --> 01:16:28,160
success and accomplishments is why we live but while that can get you periodic successes and

777
01:16:28,160 --> 01:16:37,440
satisfaction, it's really in them facing death you see what's truly important to you. So as a result

778
01:16:37,440 --> 01:16:44,480
of going through them, the challenges with cancer, I've resolved to live a more balanced

779
01:16:44,480 --> 01:16:52,880
lifestyle. I'm now in remission, knock on wood and I'm spending more time with my family, my wife

780
01:16:52,880 --> 01:17:01,120
travels with me, when my kids need me, I spend more time with them and before I used to prioritize

781
01:17:01,120 --> 01:17:06,400
everything around work when I had a little bit of time I would dole it out to my family. Now

782
01:17:07,040 --> 01:17:11,680
when my family needs something, really needs something, I drop everything at work and go to

783
01:17:11,680 --> 01:17:18,880
them and then in the time remaining, I allocate to work. But one's family is very understanding,

784
01:17:18,880 --> 01:17:27,040
it's not like they will take 50 hours a week from me. So I'm actually able to still work pretty hard,

785
01:17:27,040 --> 01:17:34,560
maybe 10 hours less per week. So I realize the most important thing in my life is really love

786
01:17:34,560 --> 01:17:41,280
and the people I love and I give that the highest priority. It isn't the only thing I do but when

787
01:17:41,280 --> 01:17:49,360
that is needed, I put that at the top priority and I feel much better and I feel much more balanced.

788
01:17:50,000 --> 01:17:58,160
And I think this also gives a hint as to a life of routine work, a life of pursuit of numbers.

789
01:17:58,720 --> 01:18:05,520
While my job was not routine, it wasn't pursuit of numbers, pursuit of can I make more money,

790
01:18:05,520 --> 01:18:12,320
can I fund more great companies, can I raise more money, can I make sure our VCs ranked higher and

791
01:18:12,320 --> 01:18:20,240
higher every year. This competitive nature of driving for bigger numbers and better numbers

792
01:18:20,800 --> 01:18:31,760
became a endless pursuit of that's mechanical. And bigger numbers really didn't make me happier

793
01:18:31,760 --> 01:18:35,600
and faced with death, I realized bigger numbers really meant nothing.

794
01:18:36,320 --> 01:18:43,680
And what was important is that people who have given their heart and their love to me deserve

795
01:18:43,680 --> 01:18:50,960
for me to do the same. So there's deep profound truth in that, that everyone should hear and

796
01:18:50,960 --> 01:18:59,360
internalize and that's really powerful for you to say that. I have to ask sort of a difficult

797
01:19:00,320 --> 01:19:08,800
question here. So I've competed in sports my whole life, looking historically. I'd like to

798
01:19:09,600 --> 01:19:17,440
challenge some aspect of that a little bit on the point of hard work, that it feels that there are

799
01:19:17,440 --> 01:19:24,560
certain aspects that is the greatest, the most beautiful aspects of human nature is the ability

800
01:19:24,560 --> 01:19:32,880
to become obsessed of becoming extremely passionate to the point where yes, flaws are revealed

801
01:19:33,600 --> 01:19:40,960
and just giving yourself fully to a task. That is, in another sense, you mentioned love being

802
01:19:40,960 --> 01:19:46,800
important, but in another sense, this kind of obsession, this pure exhibition of passion and

803
01:19:46,800 --> 01:19:53,120
hard work is truly what it means to be human. What lessons should we take this deeper because

804
01:19:53,120 --> 01:19:59,440
you've accomplished incredible things. You say it chasing numbers, but really, there's some incredible

805
01:19:59,440 --> 01:20:08,480
work there. So how do you think about that? When you look back in your 20s, your 30s, what would

806
01:20:08,480 --> 01:20:14,480
you do differently? Would you really take back some of the incredible hard work?

807
01:20:15,280 --> 01:20:24,800
I would, but it's in percentages. We're both computer scientists. So I think when one balances

808
01:20:24,800 --> 01:20:31,920
one's life, when one is younger, you might give a smaller percentage to family, but you would still

809
01:20:31,920 --> 01:20:36,800
give them high priority. And when you get older, you would give a larger percentage to them and

810
01:20:36,800 --> 01:20:42,800
still the high priority. And when you're near retirement, you give most of it to them and the

811
01:20:42,800 --> 01:20:50,000
highest priority. So I think the key point is not that we would work 20 hours less for the whole

812
01:20:50,000 --> 01:20:58,800
life and just spend it aimlessly with the family, but that when the family has a need, when your

813
01:20:58,800 --> 01:21:05,520
wife is having a baby, when your daughter has a birthday, or when they're depressed, or when

814
01:21:05,520 --> 01:21:10,720
they're celebrating something, or when they have a get together, or when we have family time,

815
01:21:11,360 --> 01:21:20,320
that it's important for us to put down our phone and PC and be 100% with them. And that priority

816
01:21:21,200 --> 01:21:29,440
on the things that really matter isn't going to be so taxing that it would eliminate or even

817
01:21:29,440 --> 01:21:35,360
dramatically reduce our accomplishments. It might have some impact, but it might also have other

818
01:21:35,360 --> 01:21:40,480
impact because if you have a happier family, maybe you fight less. If you fight less, you don't

819
01:21:40,480 --> 01:21:47,760
spend time taking care of all the aftermath of a fight. So it's unclear that it would take more

820
01:21:47,760 --> 01:21:55,280
time. And if it did, I'd be willing to take that reduction. And it's not a dramatic number,

821
01:21:55,280 --> 01:22:01,440
but it's a number that I think would give me a greater degree of happiness and knowing that

822
01:22:01,520 --> 01:22:09,280
I've done the right thing and still have plenty of hours to get the success that I want to get.

823
01:22:09,920 --> 01:22:16,320
So given the many successful companies that you've launched and much success throughout your career,

824
01:22:17,680 --> 01:22:26,720
what advice would you give to young people today looking, or it doesn't have to be young,

825
01:22:26,720 --> 01:22:33,600
but people today looking to launch and to create the next $1 billion tech startup or even AI-based

826
01:22:33,600 --> 01:22:43,680
startup? I would suggest that people understand technology waves move quickly. What worked two

827
01:22:43,680 --> 01:22:50,960
years ago may not work today. And that is very much a case in point for AI. I think two years ago,

828
01:22:51,600 --> 01:22:56,720
or maybe three years ago, you certainly could say, I have a couple of super smart PhDs,

829
01:22:57,360 --> 01:23:03,120
and we're not sure what we're going to do, but here's how we're going to start and get funding

830
01:23:03,120 --> 01:23:10,400
for a very high valuation. Those days are over because AI is going from rocket science towards

831
01:23:10,400 --> 01:23:19,680
mainstream, not yet commodity, but more mainstream. So first, the creation of any company to

832
01:23:19,680 --> 01:23:28,240
venture capitalists has to be creation of business value and monetary value. And when you have a very

833
01:23:28,240 --> 01:23:37,200
scarce commodity, VCs may be willing to accept greater uncertainty. But now the number of people

834
01:23:37,200 --> 01:23:43,760
who have the equivalent of PhD three years ago, because that can be learned more quickly,

835
01:23:43,760 --> 01:23:50,320
platforms are emerging, the cost to become an AI engineer is much lower, and there are many more

836
01:23:50,320 --> 01:23:56,560
AI engineers. So the market is different. So I would suggest someone who wants to build an AI

837
01:23:56,560 --> 01:24:05,120
company be thinking about the normal business questions. What customer cases are you trying to

838
01:24:05,120 --> 01:24:10,800
address? What kind of pain are you trying to address? How does that translate to value?

839
01:24:10,800 --> 01:24:18,400
How will you extract value and get paid through what channel? And how much business value will

840
01:24:18,400 --> 01:24:26,640
get created? That today needs to be thought about much earlier up front than it did three years ago.

841
01:24:26,640 --> 01:24:33,440
The scarcity question of AI talent has changed. The number of AI talent has changed. So now you

842
01:24:33,440 --> 01:24:40,560
need not just AI, but also understanding of business customer and the marketplace.

843
01:24:41,920 --> 01:24:52,160
So I also think you should have a more reasonable valuation expectation and growth expectation.

844
01:24:52,160 --> 01:24:57,760
There's going to be more competition. But the good news, though, is that AI technologies

845
01:24:57,760 --> 01:25:06,080
are now more available in open source. TensorFlow, PyTorch and such tools are much easier to use.

846
01:25:06,640 --> 01:25:14,400
So you should be able to experiment and get results iteratively faster than before. So

847
01:25:15,200 --> 01:25:22,240
take more of a business mindset to this. Think less of this as a laboratory taken

848
01:25:22,240 --> 01:25:28,720
into a company because we've gone beyond that stage. The only exception is if you truly have a

849
01:25:28,720 --> 01:25:35,040
breakthrough in some technology that really no one has, then the old way still works. But I think

850
01:25:35,040 --> 01:25:41,920
that's harder and harder now. So I know you believe, as many do, that we're far from creating an

851
01:25:41,920 --> 01:25:50,720
artificial general intelligence system. But say once we do, and you get to ask her one question,

852
01:25:51,280 --> 01:25:52,720
what would that question be?

853
01:25:57,520 --> 01:25:59,840
What is it that differentiates you and me?

854
01:26:02,080 --> 01:26:05,600
Beautifully put. Kaifu, thank you so much for your time today.

855
01:26:05,600 --> 01:26:06,320
Thank you.

