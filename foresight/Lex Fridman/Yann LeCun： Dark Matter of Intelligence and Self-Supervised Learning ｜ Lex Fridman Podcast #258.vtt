WEBVTT

00:00.000 --> 00:04.400
The following is a conversation with Yann LeCun, his second time in the podcast.

00:04.720 --> 00:11.280
He is the chief AI scientist at Meta, formerly Facebook, professor at NYU,

00:11.680 --> 00:16.120
touring award winner, one of the seminal figures in the history of machine

00:16.120 --> 00:21.640
learning and artificial intelligence, and someone who is brilliant and opinionated

00:22.000 --> 00:23.240
in the best kind of way.

00:23.600 --> 00:25.200
And so it was always fun to talk to him.

00:26.000 --> 00:28.640
This is the Lex Friedman podcast to support it.

00:28.840 --> 00:30.800
Please check out our sponsors in the description.

00:31.240 --> 00:34.880
And now here's my conversation with Yann LeCun.

00:36.120 --> 00:40.840
You co-wrote the article, Self-Supervised Learning, the Dark Matter of Intelligence.

00:40.880 --> 00:43.200
Great title, by the way, with Yann Mizra.

00:43.680 --> 00:49.360
So let me ask, what is self-supervised learning and why is it the dark matter of intelligence?

00:49.880 --> 00:51.560
I'll start by the dark matter part.

00:52.200 --> 01:01.120
There is obviously a kind of learning that humans and animals are doing that we currently

01:01.120 --> 01:04.640
are not reproducing properly with machines or with AI, right?

01:04.640 --> 01:09.640
So the most popular approaches to machine learning today are, or pydimes, I should say,

01:09.640 --> 01:11.680
are supervised learning and reinforcement learning.

01:12.680 --> 01:14.560
And they are extremely inefficient.

01:15.080 --> 01:18.920
Supervised learning requires many samples for learning anything.

01:19.760 --> 01:24.880
And reinforcement learning requires a ridiculously large number of trial and errors

01:24.880 --> 01:27.160
to, for, you know, a system to run anything.

01:29.320 --> 01:30.960
And that's why we don't have self-driving cars.

01:32.960 --> 01:34.560
That's a big leap from one to the other.

01:34.800 --> 01:35.240
Okay.

01:35.320 --> 01:42.560
So that to solve difficult problems, you have to have a lot of human annotation for

01:42.560 --> 01:46.360
supervised learning to work and to solve those difficult problems with reinforcement

01:46.360 --> 01:50.920
learning. You have to have some way to maybe simulate that problem such that you can do

01:50.920 --> 01:54.240
that large scale kind of learning that reinforcement learning requires.

01:54.400 --> 01:54.760
Right.

01:54.760 --> 02:00.720
So how is it that, you know, most teenagers can learn to drive a car in about 20 hours of

02:00.840 --> 02:08.520
practice, whereas even with millions of hours of simulated practice, a self-driving car can't

02:08.640 --> 02:10.600
actually learn to drive itself properly.

02:12.120 --> 02:13.920
And so obviously we're missing something, right?

02:13.920 --> 02:18.480
And it's quite obvious for a lot of people that, you know, the immediate response you get from

02:18.920 --> 02:24.240
many people is, well, you know, humans use their background knowledge to learn faster.

02:24.680 --> 02:25.320
And they're right.

02:25.840 --> 02:28.080
Now, how was that background knowledge acquired?

02:28.280 --> 02:29.360
And that's the big question.

02:30.080 --> 02:36.000
So now you have to ask, you know, how do babies in the first few months of life learn how the

02:36.000 --> 02:40.040
world works, mostly by observation, because they can hardly act in the world.

02:40.920 --> 02:45.960
And they learn an enormous amount of background knowledge about the world that may be the basis

02:45.960 --> 02:47.240
of what we call common sense.

02:48.080 --> 02:51.200
This type of learning is not learning a task.

02:51.240 --> 02:53.640
It's not being reinforced for anything.

02:53.680 --> 02:57.080
It's just observing the world and figuring out how it works.

02:58.320 --> 03:00.560
Building world models, learning world models.

03:01.160 --> 03:01.880
How do we do this?

03:02.040 --> 03:04.480
And how do we reproduce this in machine?

03:04.480 --> 03:11.400
So self-supervisioning is, you know, one instance or one attempt at trying to reproduce

03:11.400 --> 03:12.040
this kind of learning.

03:13.040 --> 03:13.320
Okay.

03:13.320 --> 03:16.320
So you're looking at just observation.

03:16.320 --> 03:18.440
So not even the interacting part of a child.

03:18.640 --> 03:23.360
It's just sitting there watching mom and dad walk around, pick up stuff, all of that.

03:23.400 --> 03:25.280
That's what we mean by background knowledge.

03:25.440 --> 03:29.320
Perhaps not even watching mom and dad, just, you know, watching the world go by.

03:29.920 --> 03:33.920
Just having eyes open or having eyes closed or the very act of opening and closing eyes.

03:34.480 --> 03:37.680
That the world appears and disappears, all of that basic information.

03:39.120 --> 03:44.960
And you're saying in order to learn to drive, like the reason humans are able to learn to

03:44.960 --> 03:48.680
drive quickly, some faster than others, is because of the background knowledge.

03:48.680 --> 03:53.720
They were able to watch cars operate in the world in the many years leading up to it, the

03:53.720 --> 03:55.800
physics of basics, objects, all that kind of stuff.

03:55.800 --> 03:56.120
That's right.

03:56.160 --> 03:59.680
I mean, the basic physics of objects, you don't even know, you don't even need to know, you

03:59.680 --> 04:00.880
know, how a car works, right?

04:00.880 --> 04:02.400
Because that you can learn fairly quickly.

04:02.480 --> 04:07.640
I mean, the example I use very often is you're driving next to a cliff and you know in

04:07.640 --> 04:13.000
advance because of your, you know, understanding of intuitive physics that if you turn the

04:13.000 --> 04:17.240
wheel to the right, the car will veer to the right, we'll run off the cliff, fall off the

04:17.240 --> 04:19.200
cliff and nothing good will come out of this, right?

04:20.400 --> 04:25.520
But if you are a sort of, you know, tabular rice reinforcement learning system that doesn't

04:25.520 --> 04:31.280
have a model of the world, you have to repeat folding off this cliff thousands of times

04:31.280 --> 04:32.800
before you figure out it's a bad idea.

04:32.800 --> 04:36.560
And then a few more thousand times before you figure out how to not do it.

04:37.120 --> 04:40.680
And then a few more million times before you figure out how to not do it in every situation

04:40.680 --> 04:41.520
you ever encounter.

04:42.520 --> 04:49.280
So self-supervised learning still has to have some source of truth being told to it by

04:49.280 --> 04:49.880
somebody.

04:50.120 --> 04:55.480
And so you have to figure out a way without human assistance or without significant

04:55.480 --> 04:58.720
amount of human assistance to get that truth from the world.

04:59.080 --> 05:03.960
So the mystery there is how much signal is there?

05:03.960 --> 05:08.440
How much truth is there that the world gives you, whether it's the human world, like you

05:08.440 --> 05:12.360
watch YouTube or something like that, or it's the more natural world.

05:12.960 --> 05:14.360
So how much signal is there?

05:14.880 --> 05:16.000
So here's the trick.

05:16.280 --> 05:22.480
There is way more signal in sort of a self-supervised setting than there is in either a supervised

05:22.480 --> 05:23.680
or reinforcement setting.

05:24.520 --> 05:28.200
And this is going to my, you know, analogy of the cake.

05:29.520 --> 05:34.560
The, you know, low cake has someone that's called it, where when you try to figure out

05:34.560 --> 05:39.080
how much information you ask the machine to predict and how much feedback you give the

05:39.080 --> 05:40.280
machine at every trial.

05:40.960 --> 05:43.280
In reinforcement learning, you give the machine a single scalar.

05:43.280 --> 05:45.000
You tell the machine you did good, you did bad.

05:45.360 --> 05:48.960
And you only tell this to the machine once in a while.

05:49.560 --> 05:52.320
When I say you, it could be the universe telling the machine, right?

05:54.040 --> 05:55.360
But it's just one scalar.

05:55.840 --> 06:00.120
So as a consequence of this, you cannot possibly learn something very complicated without many,

06:00.120 --> 06:04.200
many, many trials where you get many, many feedbacks of this type.

06:04.720 --> 06:10.040
Supervised learning, you, you give a few bits to the machine at every, every sample.

06:11.240 --> 06:16.320
Let's say your training image system on, you know, recognizing images on the image net.

06:16.320 --> 06:20.720
There is 1000 categories, that's a little less than 10 bits of information per sample.

06:20.960 --> 06:22.560
But self-supervised learning, here is the setting.

06:22.560 --> 06:28.320
You ideally, we don't know how to do this yet, but ideally you would show a machine a segment

06:28.320 --> 06:34.080
of a video and then stop the video and ask, ask the machine to predict what's going to happen next.

06:36.320 --> 06:42.160
So you let the machine predict and then you let time go by and show the machine what actually

06:42.160 --> 06:47.440
happened and hope the machine will, you know, learn to do a better job at predicting next

06:47.440 --> 06:48.040
time around.

06:48.200 --> 06:53.400
There's a huge amount of information you give the machine because it's an entire video clip

06:54.600 --> 07:00.040
of, you know, of the future after the video clip you fed it in the first place.

07:00.040 --> 07:06.760
So both for language and for vision, there's a subtle, seemingly trivial construction,

07:06.760 --> 07:10.920
but maybe that's representative of what is required to create intelligence, which is

07:11.720 --> 07:12.520
filling the gap.

07:13.640 --> 07:17.480
So it sounds dumb, but can you

07:18.520 --> 07:23.000
it's, it is possible that you can solve all of intelligence in this way.

07:23.000 --> 07:30.120
Just for both language, just give a sentence and continue it or give a sentence and there's

07:30.120 --> 07:35.000
a gap in it, some words blanked out and you fill in what words go there.

07:35.640 --> 07:41.080
For vision, you give a sequence of images and predict what's going to happen next or

07:41.080 --> 07:42.680
you fill in what happened in between.

07:43.640 --> 07:46.600
Do you think it's possible that formulation alone

07:48.520 --> 07:53.560
as a signal for self-supervised learning can solve intelligence for vision and language?

07:53.560 --> 07:55.240
I think that's our best shot at the moment.

07:56.280 --> 08:01.400
So whether this will take us all the way to, you know, human level intelligence or

08:01.400 --> 08:07.240
something or just cat level intelligence is not clear, but among all the possible approaches

08:07.240 --> 08:09.480
that people have proposed, I think it's our best shot.

08:09.480 --> 08:16.440
So I think this idea of an intelligent system filling in the blanks, either,

08:17.160 --> 08:21.800
predicting the future, inferring the past, filling in missing information.

08:23.720 --> 08:28.120
I'm currently filling the blank of what is behind your head and what your head looks like

08:28.120 --> 08:33.160
and from the back because I have a basic knowledge about how humans are made.

08:33.720 --> 08:37.160
And I don't know if you're going to say at which point you're going to speak,

08:37.160 --> 08:40.120
whether you're going to move your head this way or that way, which way you're going to look,

08:40.120 --> 08:44.680
but I know you're not going to just dematerialize and reappear three meters down the hall

08:45.640 --> 08:50.760
because I know what's possible and what's impossible according to the physics.

08:50.760 --> 08:54.360
But you have a model of what's possible, what's impossible and then you'd be very surprised

08:54.360 --> 08:57.160
if it happens and then you'll have to reconstruct your model.

08:57.720 --> 09:02.120
Right. So that's the model of the world. It's what tells you, you know, what fills in the blanks.

09:02.120 --> 09:06.760
So given your partial information about the state of the world, given by your perception,

09:07.960 --> 09:12.680
your model of the world fills in the missing information and that includes predicting the

09:12.680 --> 09:18.280
future, rich predicting the past, you know, filling in things you don't immediately perceive.

09:18.280 --> 09:24.200
And that doesn't have to be purely generic vision or visual information or generic language.

09:24.200 --> 09:31.000
You can go to specifics like predicting what control decision you make when you're driving

09:31.000 --> 09:38.280
in a lane. You have a sequence of images from a vehicle and then you could, you have information

09:38.360 --> 09:44.120
if you recorded on video where the car ended up going. So you can go back in time and predict

09:44.120 --> 09:48.520
where the car went based on the visual information. That's very specific, domain specific.

09:49.400 --> 09:52.680
Right. But the question is whether we can come up with sort of a generic

09:54.120 --> 09:59.800
method for, you know, training machines to do this kind of prediction or filling in the blanks.

09:59.800 --> 10:06.200
So right now, this type of approach has been unbelievably successful in the context of

10:06.200 --> 10:10.440
natural language processing. Every modern natural language processing is pre-trained in

10:10.440 --> 10:15.720
self-supervised manner to fill in the blanks. You show it a sequence of words, you remove 10%

10:15.720 --> 10:19.080
of them, and then you train some gigantic neural net to predict the words that are missing.

10:20.280 --> 10:25.800
And once you've pre-trained that network, you can use the internal representation

10:25.800 --> 10:31.160
learned by it as input to, you know, something that you train supervised or whatever.

10:32.040 --> 10:36.680
That's been incredibly successful, not so successful in images, although it's making progress.

10:37.480 --> 10:44.040
And it's based on sort of manual data augmentation. We can go into this later. But

10:44.040 --> 10:48.360
what has not been successful yet is training from video. So getting a machine to learn,

10:48.360 --> 10:53.480
to represent the visual world, for example, by just watching video. Nobody has really

10:53.480 --> 10:58.760
succeeded in doing this. Okay. Well, let's kind of give a high-level overview. What's the difference

10:58.840 --> 11:06.360
in kind and in difficulty between vision and language? So you said people haven't been able to

11:07.720 --> 11:11.880
really kind of crack the problem of vision open in terms of self-supervised learning,

11:11.880 --> 11:16.920
but that may not be necessarily because it's fundamentally more difficult. Maybe like when

11:16.920 --> 11:23.240
we're talking about achieving, like passing the Turing test in the full spirit of the Turing test

11:23.240 --> 11:28.680
in language might be harder than vision. That's not obvious. So in your view, which is harder

11:29.320 --> 11:34.680
or perhaps are they just the same problem? When the farther we get to solving each,

11:34.680 --> 11:38.040
the more we realize it's all the same thing. It's all the same cake. I think

11:38.840 --> 11:43.480
what I'm looking for are methods that make them look essentially like the same cake,

11:43.480 --> 11:49.080
but currently they're not. And the main issue with learning world models or learning predictive

11:49.080 --> 11:58.360
models is that the prediction is never a single thing because the world is not entirely predictable.

11:59.160 --> 12:02.920
It may be deterministic or stochastic. We can get into the philosophical discussion about it,

12:02.920 --> 12:09.480
but even if it's deterministic, it's not entirely predictable. And so if I play

12:10.680 --> 12:14.040
a short video clip and then I ask you to predict what's going to happen next,

12:14.040 --> 12:19.880
there's many, many plausible continuations for that video clip. And the number of continuation

12:19.880 --> 12:25.480
grows with the interval of time that you're asking the system to make a prediction for.

12:26.360 --> 12:32.520
And so one big question with self-provisioning is how you represent this uncertainty, how you

12:32.520 --> 12:38.120
represent multiple discrete outcomes, how you represent a continuum of possible outcomes,

12:39.320 --> 12:45.560
et cetera. And if you are a classical machine learning person, you say, oh,

12:45.560 --> 12:52.440
you just represent a distribution. And that we know how to do when we're predicting words,

12:52.520 --> 12:57.880
missing words in the text, because you can have a neural net give a score for every word in the

12:57.880 --> 13:03.720
dictionary. It's a big list of numbers, maybe 100,000 or so. And you can turn them into a

13:03.720 --> 13:11.480
probability distribution that tells you when I say a sentence, the cat is chasing the blank

13:11.480 --> 13:17.240
in the kitchen. There are only a few words that make sense there. It could be a mouse or it could

13:17.240 --> 13:25.720
be a lizard spot or something like that. And if I say the blank is chasing the blank in the savannah,

13:25.720 --> 13:32.680
you also have a bunch of plausible options for those two words. Because you have kind of a

13:32.680 --> 13:41.400
underlying reality that you can refer to to fill in those blanks. So you cannot say for sure in

13:41.400 --> 13:46.920
the savannah, if it's a lion or a cheetah or whatever, you cannot know if it's a zebra or

13:47.560 --> 13:56.680
a goo or whatever. We're the beast, the same thing. But you can represent the uncertainty

13:56.680 --> 14:02.920
by just a long list of numbers. Now, if I do the same thing with video and I ask you to predict

14:02.920 --> 14:07.960
a video clip, it's not a discrete set of potential frames. You have to have

14:08.760 --> 14:13.400
somewhere representing a sort of infinite number of plausible continuations

14:13.400 --> 14:18.360
of multiple frames in a high-dimensional, continuous space. And we just have no idea

14:18.360 --> 14:25.320
how to do this properly. Finite high-dimensional. It's finite high-dimensional, yes.

14:25.320 --> 14:33.320
Just like the words. They try to get it down to a small finite set of like under a million,

14:33.320 --> 14:38.920
something like that. I mean, it's kind of ridiculous that we're doing a distribution

14:38.920 --> 14:44.280
over every single possible word for language, and it works. It feels like that's a really dumb

14:44.280 --> 14:53.080
way to do it. It seems to be like there should be some more compressed representation of the

14:53.080 --> 14:58.840
distribution of the words. You're right about that. I agree. Do you have any interesting ideas

14:58.840 --> 15:03.320
about how to represent all the reality in a compressed way such that you can form a distribution

15:03.320 --> 15:09.320
over it? That's one of the big questions. How do you do that? I mean, another thing that really is

15:10.600 --> 15:15.560
stupid about, I shouldn't say stupid, but simplistic about current approaches to

15:15.560 --> 15:22.840
self-supervisioning in NLP in text is that not only do you represent a giant distribution over

15:22.840 --> 15:27.560
words, but for multiple words that are missing, those distributions are essentially independent

15:27.560 --> 15:38.200
of each other. You don't pay too much of a price for this. The system, in the sentence that I gave

15:38.200 --> 15:44.120
earlier, if it gives a certain probability for a lion and cheetah, and then a certain

15:44.120 --> 15:54.360
probability for gazelle, wildebeest, and zebra, those two probabilities are independent of each

15:54.360 --> 15:59.960
other. It's not the case that those things are independent lions actually attack bigger animals

15:59.960 --> 16:07.640
than cheetahs. There's a huge independent hypothesis in this process which is not actually true.

16:07.640 --> 16:13.080
The reason for this is that we don't know how to represent properly distributions over

16:13.080 --> 16:19.080
combinatorial sequences of symbols, essentially, because the number grows exponentially with the

16:19.080 --> 16:27.160
length of the symbols. We have to use tricks for this, but those techniques don't even deal with

16:27.160 --> 16:34.840
it. The big question is, would there be some sort of abstract latent representation of text

16:35.480 --> 16:44.440
that would say that when I switch lion for cheetah, I also have to switch zebra for gazelle.

16:45.160 --> 16:51.000
Yeah, so this independence assumption, let me throw some criticism at you that I often hear

16:51.000 --> 16:56.520
and see how you respond. This kind of feeling in the blanks is just statistics. You're not

16:56.520 --> 17:03.720
learning anything, like the deep underlying concepts. You're just mimicking stuff from

17:05.000 --> 17:10.440
the past. You're not learning anything new such that you can use it to generalize about the world.

17:11.400 --> 17:17.160
Okay, let me just say the crude version, which is just statistics. It's not intelligence.

17:18.200 --> 17:22.520
What do you have to say to that? What do you usually say to that if you hear this kind of thing?

17:22.520 --> 17:25.640
I don't get into those discussions because they are kind of pointless.

17:27.240 --> 17:30.760
First of all, it's quite possible that intelligence is just statistics. It's just

17:30.760 --> 17:38.440
statistics of a particular kind. This is the philosophical question. Is it possible that

17:38.440 --> 17:45.800
intelligence is just statistics? Yeah, but what kind of statistics? So if you're asking the question,

17:47.080 --> 17:52.520
are the models of the world that we learn, do they have some notion of causality? Yes.

17:53.320 --> 17:58.680
So if the criticism comes from people who say a current machine non-existent don't care about

17:58.680 --> 18:05.640
causality, which by the way is wrong, I agree with that. Your model of the world should have

18:05.720 --> 18:11.320
your actions as one of the inputs and that will drive you to learn causal models of the world

18:11.320 --> 18:17.400
where you know what intervention in the world will cause, what results, or you can do this by

18:17.400 --> 18:23.400
observation of other agents acting in the world and observing the effect of other humans, for

18:23.400 --> 18:32.120
example. So I think at some level of description, intelligence is just statistics, but that doesn't

18:32.120 --> 18:39.000
mean you won't have models that have deep mechanistic explanation for what goes on.

18:39.880 --> 18:46.200
The question is how do you learn them? That's the question I'm interested in. Because a lot of people

18:46.200 --> 18:51.880
who actually voice their criticism say that those mechanistic models have to come from

18:51.880 --> 18:55.400
someplace else. They have to come from human designers. They have to come from, I don't know

18:55.400 --> 19:02.120
what, and obviously we learn them. Or if we don't learn them as an individual, nature

19:03.080 --> 19:08.040
learned them for us using evolution. So regardless of what you think, those processes have been

19:08.040 --> 19:14.760
learned somehow. So if you look at the human brain, just like when we humans introspect about

19:14.760 --> 19:21.480
how the brain works, it seems like when we think about what is intelligence, we think about the

19:21.480 --> 19:26.280
high level stuff, like the models we've constructed, concepts like cognitive science, like concepts of

19:26.280 --> 19:33.880
memory and reasoning module, almost like these high level modules. Is this serve as a good analogy?

19:35.320 --> 19:44.520
Like, are we ignoring the dark matter, the basic low level mechanisms, just like we ignore the way

19:44.520 --> 19:50.440
the operating system works, we're just using the high level software. We're ignoring that

19:51.160 --> 19:56.920
at the low level, the neural network might be doing something like statistics. Like,

19:56.920 --> 20:01.640
meaning, sorry to use this word, probably incorrectly and crudely, but doing this kind

20:01.640 --> 20:06.280
of fill in the gap kind of learning and just kind of updating the model constantly in order to be

20:06.280 --> 20:11.480
able to support the raw sensory information, to predict it and then adjust to the prediction when

20:11.480 --> 20:17.160
it's wrong. But like when we look at our brain at the high level, it feels like we're doing,

20:17.160 --> 20:22.440
like we're playing chess, like we're, we're like playing with high level concepts and we're

20:22.440 --> 20:27.160
stitching them together and we're putting them into long term memory. But really what's going

20:27.160 --> 20:33.640
underneath is something we're not able to introspect, which is this kind of simple large

20:33.640 --> 20:38.120
neural network that's just filling in the gaps. Right. Well, okay, so there's a lot of questions

20:38.120 --> 20:42.600
that are answers there. Okay, so first of all, there's a whole school of thought in neuroscience,

20:42.600 --> 20:47.720
competition on neuroscience in particular, that likes the idea of predictive coding,

20:47.720 --> 20:52.200
which is really related to the idea I was talking about in self supervised running. So

20:52.200 --> 20:55.640
everything is about prediction. The essence of intelligence is the ability to predict.

20:56.280 --> 21:01.240
And everything the brain does is trying to predict, predict everything from everything

21:01.240 --> 21:05.400
else. Okay, and that's really sort of the underlying principle if you want that

21:06.920 --> 21:11.080
self supervised learning is trying to kind of reproduce this idea of prediction as kind of an

21:11.080 --> 21:18.760
essential mechanism of task independent learning if you want. The next step is what kind of

21:18.760 --> 21:23.400
intelligence are you interested in reproducing? And of course, you know, we all think about,

21:23.400 --> 21:28.280
you know, trying to reproduce sort of, you know, high level cognitive processes in humans.

21:28.280 --> 21:32.520
But like with machines, we're not even at the level of even reproducing the

21:33.240 --> 21:39.000
running processes in a cat brain. You know, the most intelligent or intelligent systems

21:39.320 --> 21:45.640
don't have as much common sense as a house cat. So how is it that cats learn? And, you know,

21:45.640 --> 21:50.280
cats don't do a whole lot of reasoning. They certainly have causal models. They certainly have,

21:51.480 --> 21:55.400
because, you know, many cats can figure out like how they can act on the world to get what they want.

21:56.520 --> 22:04.120
They certainly have a fantastic model of intuitive physics, certainly the dynamics of their own

22:04.120 --> 22:10.520
bodies, but also praise and things like that. Right. So they're pretty smart. They only do

22:10.520 --> 22:17.880
this with about 800 million neurons. We are not anywhere close to reproducing this kind of thing.

22:17.880 --> 22:24.600
So to some extent, I could say, let's not even worry about like the high level cognition

22:26.200 --> 22:29.400
and kind of, you know, long term planning and reasoning that humans can do until we figure

22:29.400 --> 22:34.760
out like, you know, can we even reproduce what cats are doing? Now that said, this ability to

22:35.640 --> 22:42.040
learn world models, I think is the key to the possibility of running machines that can also

22:42.040 --> 22:46.680
reason. So whenever I give a talk, I say there are three challenges in the three main challenges

22:46.680 --> 22:50.760
in machine learning. The first one is, you know, getting machines to learn to represent the world

22:51.720 --> 22:58.280
and proposing self supervised learning. The second is getting machines to reason in ways

22:58.280 --> 23:02.760
that are compatible with essentially gradient based learning because this is what deep learning is

23:02.760 --> 23:08.200
all about really. And the third one is something we have no idea how to solve. At least I have no

23:08.200 --> 23:15.640
idea how to solve is can we get machines to learn hierarchical representations of action plans?

23:16.920 --> 23:20.280
You know, like, you know, we know how to train them to learn hierarchical representations of

23:20.280 --> 23:25.160
perception, you know, with convolutional nets and things like that and transformers. But what

23:25.160 --> 23:29.960
about action plans? Can we get them to spontaneously learn good hierarchical representations of

23:29.960 --> 23:35.800
actions also gradient based? Yeah, all of that, you know, needs to be somewhat differentiable

23:35.800 --> 23:39.880
so that you can apply sort of gradient based learning, which is really what deep learning is

23:39.880 --> 23:50.680
about. So it's background knowledge, ability to reason in a way that's differentiable that

23:51.640 --> 23:56.200
is somehow connected deeply integrated with that background knowledge or builds on top of

23:56.200 --> 24:00.360
that background knowledge. And then giving that background knowledge be able to make hierarchical

24:00.360 --> 24:05.800
plans right in the world. So if you take classical optimal control, there's something

24:05.800 --> 24:11.560
classical optimal control called model predictive control. And it's, you know, it's been around

24:11.560 --> 24:17.560
since the early 60s. NASA uses that to compute trajectories of rockets. And the basic idea is

24:17.640 --> 24:23.800
that you have a predictive model of the rocket, let's say, or whatever system you are, you intend

24:23.800 --> 24:30.840
to control, which given the state of the system at time t and given an action that you're taking

24:30.840 --> 24:36.840
the system. So for a rocket to be thrust and, you know, all the controls you can have, it gives

24:36.840 --> 24:40.840
you the state of the system at time t plus delta t, right? So basically differential equation,

24:40.920 --> 24:48.360
something like that. And if you have this model, and you have this model in the form of some sort

24:48.360 --> 24:53.480
of neural net, or some sort of set of formula that you can back propagate gradient through,

24:53.480 --> 24:58.280
you can do what's called model predictive control, or gradient based model predictive control.

24:58.280 --> 25:09.880
So you have, you can enroll that model in time, you feel it a hypothesized sequence of actions.

25:11.000 --> 25:15.960
And then you have some objective function that measures how well at the end of the trajectory

25:15.960 --> 25:20.920
of the system has succeeded or matched what you wanted to do. You know, is it a robot harm,

25:20.920 --> 25:25.960
as if you grasp the object you want to grasp, if it's a rocket, you know, are you at the right

25:25.960 --> 25:31.080
place near the space station, things like that. And by back propagation through time, and again,

25:31.080 --> 25:37.960
this was invented in the 1960s by optimal control theorists, you can figure out what is the optimal

25:37.960 --> 25:46.920
sequence of actions that will, you know, get my system to the best final state. So that's a form

25:46.920 --> 25:51.560
of reasoning. It's basically planning. And a lot of planning systems in robotics are actually based

25:51.560 --> 25:58.520
on this. And, and you can think of this as a form of reasoning. So, you know, to take the example

25:58.520 --> 26:03.080
of the teenager driving a car again, you have a pretty good dynamical model of the car, it doesn't

26:03.080 --> 26:07.560
need to be very accurate. But you know, again, that if you turn the wheel to the right, and there

26:07.560 --> 26:10.520
is a cliff, you're going to run off the cliff, right, you don't need to have a very accurate

26:10.520 --> 26:15.400
model to predict that. And you can run this in your mind, and decide not to do it for that reason.

26:15.960 --> 26:19.240
Because you can predict in advance that the result is going to be bad. So you can sort of

26:19.240 --> 26:25.160
imagine different scenarios, and, and then, you know, employ, or take the first step in the scenario

26:25.160 --> 26:29.160
that is most favorable, and then repeat the process of planning that's called receding horizon

26:29.160 --> 26:33.640
model predictive control. So even, you know, all those things have names, you know, going back,

26:33.640 --> 26:39.880
you know, decades. And so, if you're not not the, you know, in classical optimal control,

26:39.880 --> 26:44.840
the model of the world is not generally learned. This, you know, sometimes a few parameters you

26:44.840 --> 26:50.120
have to identify that's called systems identification. But, but generally, the model is

26:50.920 --> 26:56.760
mostly deterministic and mostly built by hand. So the big question of AI, I think the big challenge

26:56.760 --> 27:01.800
of AI for the next decade is how do we get machines to learn predictive models of the world

27:01.800 --> 27:06.280
that deal with uncertainty, and deal with the real world in all this complexity. So it's not

27:06.280 --> 27:11.160
just trajectory of a rocket, which you can reduce to first principles, it's not, it's not even just

27:11.160 --> 27:16.120
a trajectory of a robot arm, which again, you can model by, you know, careful mathematics.

27:16.120 --> 27:19.640
But it's everything else, everything we observe in the world, you know, people, behavior,

27:21.640 --> 27:27.720
you know, physical systems that involve collective phenomena, like water or, or, you know,

27:29.000 --> 27:34.760
trees and, you know, branches in a tree or something, or, or like complex things that,

27:34.760 --> 27:39.640
you know, humans have no trouble developing abstract representations and predictive model for,

27:39.640 --> 27:41.560
but we still don't know how to deal with machines.

27:41.560 --> 27:45.720
Where do you put in, in these three, maybe in the, in the planning stages,

27:47.240 --> 27:53.960
the, the game theoretic nature of this world, where your actions not only respond to the dynamic

27:53.960 --> 27:59.720
nature of the world, the environment, but also affect it. So if there's other humans involved,

27:59.720 --> 28:04.440
is this, is this point number four, or is it somehow integrated into the hierarchical

28:04.440 --> 28:09.400
representation of action in your view? I think it's integrated. It's just, it's just that now

28:09.400 --> 28:13.000
your model of the world has to deal with, you know, it just makes it more complicated, right?

28:13.000 --> 28:18.120
The fact that humans are complicated and not easily predictable, that makes your model of

28:18.120 --> 28:20.840
the world much more complicated, that much more complicated.

28:20.840 --> 28:27.240
Well, there's a chess, I mean, I suppose chess is an analogy. So multi-carvel tree search.

28:27.960 --> 28:34.840
I mean, it, there's a, I go, you go, I go, you go, like, under Kapatha recently gave a talk

28:34.840 --> 28:40.840
at MIT about car doors. I think there's some machine learning too, but mostly car doors.

28:40.840 --> 28:45.000
And there's a dynamic nature to the car, like the person opening the door, checking,

28:45.560 --> 28:48.760
I mean, he wasn't talking about that. He was talking about the perception problem of what

28:48.760 --> 28:53.320
the, the ontology of what defines a car door, this big philosophical question. But to me,

28:53.320 --> 28:57.400
it was interesting because like, it's obvious that the person opening the car doors, they're

28:57.400 --> 29:02.680
trying to get out like here in New York, trying to get out of the car, you slowing down is going

29:02.680 --> 29:06.840
to signal something, you speeding up is going to signal something. And that's a dance. It's a

29:07.640 --> 29:17.080
asynchronous chess game. I don't know. So it feels like it's not just, I mean,

29:17.080 --> 29:21.480
I guess you can integrate all of them to one giant model, like the entirety of the,

29:22.680 --> 29:26.440
these little interactions, because it's not as complicated as chess, it's just like a little

29:26.440 --> 29:29.880
dance. We do like a little dance together. And then we figure it out.

29:29.880 --> 29:34.920
Well, in some ways, it's way more complicated than chess because, because it's continuous,

29:34.920 --> 29:39.720
it's uncertain in a continuous manner. It doesn't feel more complicated,

29:39.720 --> 29:43.560
but it doesn't feel more complicated because that's what we're, we've evolved to solve.

29:43.560 --> 29:47.080
This is the kind of problem we've evolved to solve. And so we're good at it because, you know,

29:47.640 --> 29:53.880
nature has made us good at it. Nature has not made us good at chess. We completely suck at chess.

29:54.440 --> 30:01.400
In fact, that's why we designed it as a game is to be challenging. And if there is something that,

30:01.400 --> 30:06.520
you know, recent progress in chess and Go has made us realize is that humans are

30:06.520 --> 30:11.400
really terrible at those things, like really bad. You know, there was a story, right, before Alpha Go

30:11.400 --> 30:17.240
that, you know, the best Go player thought there were maybe two or three stones behind,

30:17.240 --> 30:22.120
you know, an ideal player that they would call God. In fact, no, there are like nine or 10

30:22.680 --> 30:29.000
stones behind. I mean, we're just bad. So we're not good at, and it's because we have limited

30:29.000 --> 30:33.000
working memory. We know we're not very good at like doing this tree exploration that,

30:33.000 --> 30:38.360
you know, computers are much better at doing than we are, but we are much better at learning

30:38.360 --> 30:43.640
differentiable models to the world. I mean, I said differentiable in the kind of, you know,

30:43.640 --> 30:47.320
I should say not differentiable in the sense that, you know, we run back from through it,

30:47.400 --> 30:53.160
but in the sense that our brain has some mechanism for estimating gradients of some kind.

30:53.960 --> 31:00.040
And that's what, you know, makes us efficient. So if you have an agent that consists of

31:01.240 --> 31:05.720
a model of the world, which, you know, in the human brain is basically the entire front half of

31:05.720 --> 31:15.160
your brain, an objective function, which in humans is a combination of two things. There is

31:15.240 --> 31:19.000
your sort of intrinsic motivation module, which is in the basal ganglia, you know,

31:19.000 --> 31:23.080
the base of your brain. That's the thing that measures pain and hunger and things like that,

31:23.080 --> 31:31.080
like immediate feelings and emotions. And then there is, you know, the equivalent of what people

31:31.080 --> 31:36.840
in Reference Metronomy call a critic, which is a sort of module that predicts ahead what the outcome

31:37.800 --> 31:44.920
of a situation will be. And so it's not a cost function, but it's sort of not an objective

31:44.920 --> 31:50.760
function, but it's sort of a, you know, trained predictor of the ultimate objective function.

31:50.760 --> 31:55.640
And that also is differentiable. And so if all of this is differentiable, your cost function,

31:55.640 --> 32:03.320
your critic, your, you know, your world model, then you can use gradient based type methods to do

32:03.320 --> 32:08.200
planning, to do reasoning, to do learning, you know, to do all the things that would like an

32:08.200 --> 32:15.560
intelligent agent to do. And gradient based learning, like what's your intuition, that's

32:15.560 --> 32:23.640
probably at the core of what can solve intelligence. So you don't need like a logic based reasoning

32:24.520 --> 32:27.960
in your view. I don't know how to make logic based reasoning compatible with

32:28.760 --> 32:33.720
efficient learning. And okay, I mean, there is a big question, perhaps a philosophical question.

32:33.720 --> 32:38.680
I mean, it's not that philosophical, but that we can ask is, is that, you know, all the learning

32:39.320 --> 32:45.080
algorithms we know from engineering and computer science, proceed by optimizing some objective

32:45.080 --> 32:53.000
function? Yeah, right. So one question we may ask is, is those learning in the brain minimize

32:53.000 --> 32:58.360
an objective function? I mean, it could be, you know, a composite of multiple objective functions,

32:58.360 --> 33:04.760
but it's still an objective function. Second, if it does optimize an objective function,

33:04.760 --> 33:10.360
does it do, does it do it by some sort of gradient estimation? You know, it doesn't need to be

33:10.360 --> 33:15.560
backdrop, but you know, some way of estimating the gradient in efficient manner, whose complexity

33:15.560 --> 33:20.520
is on the same order of magnitude as, you know, actually running the inference.

33:21.480 --> 33:26.520
Because you can't afford to do things like, you know, perturbing a weight in your brain to

33:26.520 --> 33:31.480
figure out what the effect is, and then sort of, you know, you can do sort of estimating gradient

33:31.480 --> 33:37.240
by perturbation. It's, to me, it seems very implausible that the brain uses some sort of,

33:38.840 --> 33:44.600
you know, zero-thorough black box gradient free optimization, because it's so much less

33:44.600 --> 33:48.120
efficient than gradient optimization. So it has to have a way of estimating gradient.

33:49.080 --> 33:55.320
Is it possible that some kind of logic-based reasoning emerges in pockets as a useful,

33:55.320 --> 33:59.560
like you said, if the brain is an objective function? Maybe it's a mechanism for creating

33:59.560 --> 34:05.640
objective functions. It's a mechanism for creating knowledge bases, for example,

34:06.360 --> 34:11.000
that can then be quarried. Like maybe it's an efficient representation of knowledge that's

34:11.000 --> 34:13.560
learned in a gradient-based way or something like that.

34:13.560 --> 34:17.880
Well, so I think there is a lot of different types of intelligence. So first of all,

34:17.960 --> 34:23.720
I think the type of logical reasoning that we think about that we are, you know, maybe stemming

34:23.720 --> 34:31.400
from, you know, sort of classical AI of the 1970s and 80s, I think humans use that relatively

34:31.400 --> 34:34.520
rarely and are not particularly good at it.

34:34.520 --> 34:41.560
But we judge each other based on our ability to solve those rare problems called IQ tests.

34:41.560 --> 34:44.280
I think so. Like, I'm not very good at chess.

34:45.160 --> 34:49.640
Yes, I'm judging you this whole time because, well, we actually...

34:49.640 --> 34:52.680
With your, you know, heritage, I'm sure you're good at chess.

34:53.400 --> 34:56.440
No, stereotypes. Not all stereotypes are true.

34:57.880 --> 35:04.600
Well, I'm terrible at chess. So, you know, but I think perhaps another type of intelligence

35:04.600 --> 35:09.720
that I have is this, you know, ability of sort of building models to the world from,

35:10.680 --> 35:14.920
you know, reasoning, obviously, but also data.

35:15.800 --> 35:21.480
And those models generally are more kind of analogical, right? So it's reasoning by simulation

35:22.200 --> 35:26.760
and by analogy, where you use one model to apply to a new situation.

35:26.760 --> 35:31.640
Even though you've never seen that situation, you can sort of connect it to a situation you've

35:31.640 --> 35:38.360
encountered before. And your reasoning is more akin to some sort of internal simulation.

35:38.360 --> 35:42.120
So you're kind of simulating what's happening when you're building, I don't know,

35:42.120 --> 35:45.720
a box out of wood or something, right? You're going to imagine in advance,

35:46.280 --> 35:49.480
like, will we be the result of, you know, cutting the wood in this particular way?

35:49.480 --> 35:51.480
Are you going to use, you know, screws on nails or whatever?

35:52.760 --> 35:56.920
When you are interacting with someone, you also have a model of that person and sort of

35:56.920 --> 36:03.560
interact with that person. You know, having this model in mind to kind of tell the person

36:03.720 --> 36:10.200
what you think is useful to them. So I think this ability to construct models to the world

36:10.200 --> 36:16.600
is basically the essence of intelligence. And the ability to use it then to plan

36:17.400 --> 36:25.320
actions that will fulfill a particular criterion, of course, is necessary as well.

36:25.320 --> 36:30.120
So I'm going to ask you a series of impossible questions as we keep asking, as I've been doing.

36:30.120 --> 36:35.160
So if that's the fundamental sort of dark matter of intelligence, this ability to form

36:35.160 --> 36:40.280
a background model, what's your intuition about how much knowledge is required?

36:41.320 --> 36:46.040
You know, I think dark matter, you could put a percentage on it of

36:48.680 --> 36:52.600
the composition of the universe and how much of it is dark matter, how much of it is dark energy,

36:52.600 --> 37:01.080
how much information do you think is required to be a house cat? So you have to be able to,

37:01.080 --> 37:07.000
when you see a box going, when you see a human compute the most evil action, if there's a thing

37:07.000 --> 37:12.680
that's near an edge, you knock it off, all of that, plus the extra stuff you mentioned,

37:12.680 --> 37:17.800
which is a great self-awareness of the physics of your own body and the world.

37:18.600 --> 37:20.840
How much knowledge is required, do you think, to solve it?

37:21.800 --> 37:25.480
I don't even know how to measure an answer to that question.

37:25.480 --> 37:30.760
I'm not sure how to measure it, but whatever it is, it fits in about 800,000 neurons,

37:32.280 --> 37:33.640
800 million neurons.

37:33.640 --> 37:35.000
The representation does.

37:36.280 --> 37:38.120
Everything, all knowledge, everything, right?

37:39.880 --> 37:44.120
You know, it's less than a billion, a dog is 2 billion, but a cat is less than 1 billion.

37:45.320 --> 37:49.240
And so multiply that by a thousand and you get the number of synapses.

37:50.200 --> 37:55.880
And I think almost all of it is learned through this, you know, a sort of self-supervised running,

37:55.880 --> 37:59.800
although, you know, I think a tiny sliver is learned through reinforcement running,

37:59.800 --> 38:03.560
and certainly very little through, you know, classical supervised running, although it's

38:03.560 --> 38:07.880
not even clear how supervised running actually works in the biological world.

38:09.160 --> 38:16.680
So I think almost all of it is self-supervised running, but it's driven by the sort of

38:16.680 --> 38:21.320
ingrained objective functions that a cat or human have at the base of their brain,

38:21.320 --> 38:24.840
which kind of drives their behavior.

38:24.840 --> 38:28.280
So, you know, nature tells us you're hungry.

38:29.320 --> 38:31.800
It doesn't tell us how to feed ourselves.

38:31.800 --> 38:34.520
That's something that the rest of our brain has to figure out, right?

38:35.640 --> 38:39.720
What's interesting, because there might be more like deeper objective functions than

38:39.720 --> 38:40.680
allowing the whole thing.

38:41.240 --> 38:47.160
So hunger may be some kind of, now you go to like neurobiology, it might be just the brain

38:49.960 --> 38:51.560
trying to maintain homeostasis.

38:52.360 --> 38:59.480
So hunger is just one of the human perceivable symptoms of the brain being unhappy with the

38:59.480 --> 39:00.520
way things are currently.

39:01.320 --> 39:04.760
It could be just like one really dumb objective function at the core.

39:04.760 --> 39:07.560
But that's how behavior is driven.

39:08.440 --> 39:11.080
The fact that, you know, the, or basal ganglia

39:12.360 --> 39:17.240
drivers to do things that are different from say an orangutan or certainly a cat

39:18.120 --> 39:22.280
is what makes, you know, human nature versus orangutan nature versus cat nature.

39:23.240 --> 39:31.320
So for example, you know, our basal ganglia drives us to seek the company of other humans.

39:32.120 --> 39:37.080
And that's because nature has figured out that we need to be social animals for our species to

39:37.080 --> 39:43.080
survive. And it's true of many primates. It's not true of orangutans, orangutans are

39:43.080 --> 39:47.880
solitary animals. They don't seek the company of others. In fact, they avoid them.

39:49.240 --> 39:51.960
In fact, they scream at them when they come too close because they're territorial.

39:52.600 --> 39:57.400
Because for their survival, you know, evolution has figured out that's the best thing.

39:58.120 --> 40:00.760
I mean, they're occasionally social, of course, for, you know,

40:02.120 --> 40:05.880
reproduction and stuff like that. But they're mostly solitary.

40:06.840 --> 40:10.360
So all of those behaviors are not part of intelligence. You know, people say, oh,

40:10.360 --> 40:13.800
you're never going to have intelligent machines because, you know, human intelligence is social.

40:13.800 --> 40:17.960
But then you look at orangutans, you look at octopus. Octopus never know their parents.

40:18.680 --> 40:23.800
They barely interact with any other and they get to be really smart in less than a year,

40:23.800 --> 40:28.760
in like half a year. You know, in a year or they're adults, in two years they're dead.

40:28.760 --> 40:35.720
So there are things that we think, as humans, are intimately linked with intelligence,

40:35.720 --> 40:42.280
like social interaction, like language. We think, I think we give way too much

40:42.280 --> 40:45.880
importance to language as a substrate of intelligence as humans,

40:46.680 --> 40:49.800
because we think our reasoning is so linked with language.

40:49.800 --> 40:55.080
So for, to solve the house cat intelligence problem, you think you could do it on a desert

40:55.080 --> 41:03.080
island. You could have a cat sitting there, looking at the waves, at the ocean waves,

41:03.080 --> 41:08.600
and figure a lot of it out. It needs to have sort of, you know, the right set of drives

41:10.040 --> 41:14.440
to kind of, you know, get it to do the thing and learn the appropriate things, right? But

41:16.120 --> 41:19.560
like, for example, you know, baby humans are driven to

41:20.520 --> 41:26.360
learn to stand up and walk. Okay, that's kind of, this desire is hardwired. How to do it

41:26.360 --> 41:31.480
precisely is not, that's learned. But the desire to, to walk, move around and stand up,

41:32.760 --> 41:37.400
that's sort of hardwired. It's very simple to hardwire this kind of stuff.

41:38.840 --> 41:43.960
Oh, like the desire to, well, that's interesting. You're hardwired to want to walk.

41:44.360 --> 41:51.960
That's not a, there's got to be a deeper need for walking. I think it was probably socially

41:51.960 --> 41:57.080
imposed by society that you need to walk all the other bipedal. No, no, like a lot of simple

41:57.080 --> 42:02.840
animals that, you know, would probably walk without ever watching any other members of the

42:02.840 --> 42:09.160
species. It seems like a scary thing to have to do because you suck at bipedal walking at first.

42:09.160 --> 42:14.840
It seems crawling is much safer, much more like, why are you in a hurry?

42:15.560 --> 42:18.840
Well, because, because you have this thing that drives you to do it, you know,

42:20.200 --> 42:24.920
which is sort of part of the sort of human development.

42:24.920 --> 42:26.600
Is that understood actually what?

42:26.600 --> 42:27.640
Not entirely, no.

42:27.640 --> 42:31.800
What is, what's the reason to get on two feet? It's really hard. Like most animals don't get on

42:31.800 --> 42:32.760
two feet. Why not?

42:32.760 --> 42:35.720
Well, they get on four feet. You know, many mammals get on four feet.

42:35.720 --> 42:36.040
Yeah, they do.

42:36.040 --> 42:37.800
Very quickly. Some of them extremely quickly.

42:38.440 --> 42:42.520
But I don't, you know, like from the last time I've interacted with a table,

42:42.520 --> 42:46.360
that's much more stable than a thing on two legs. It's just a really hard problem.

42:46.360 --> 42:48.360
Yeah, I mean birds have figured it out with two feet.

42:49.240 --> 42:54.280
Well, technically, we can go into ontology. They have four, I guess they have two feet.

42:54.280 --> 42:55.000
They have two feet.

42:55.000 --> 42:55.480
Chickens.

42:56.280 --> 42:58.760
You know, dinosaurs have two feet, many of them.

42:58.760 --> 42:59.400
Allegedly.

43:01.400 --> 43:05.320
I'm just now learning that T-Rex was eating grass, not other animals.

43:05.320 --> 43:11.080
T-Rex might have been a friendly pet. What do you think about, I don't know if you looked at

43:12.440 --> 43:16.200
the test for general intelligence that Francois Chalet put together?

43:16.200 --> 43:18.600
I don't know if you got a chance to look at that kind of thing.

43:18.600 --> 43:23.640
Like what's your intuition about how to solve like an IQ type of test?

43:23.640 --> 43:27.320
I don't know. I think it's so outside of my radar screen that it's not really

43:28.120 --> 43:29.880
relevant, I think in the short term.

43:30.600 --> 43:35.720
Well, I guess one way to ask another way, perhaps more closer to what

43:36.600 --> 43:42.600
to your work is like, how do you solve MNIST with very little example data?

43:42.600 --> 43:45.800
That's right. And that's the answer to this probably is self-supervised running.

43:45.800 --> 43:51.240
Just learn to represent images and then learning, you know, to recognize handwritten digits on top

43:51.240 --> 43:57.160
of this will only require a few samples. And we observe this in humans, right? You show a

43:57.160 --> 44:01.080
young child a picture book with a couple of pictures of an elephant and that's it.

44:01.800 --> 44:07.000
The child knows what an elephant is. And we see this today with practical systems that we

44:07.960 --> 44:15.640
train image recognition systems with enormous amounts of images either completely self-supervised

44:15.640 --> 44:21.960
or very weakly supervised. For example, you can train a neural net to predict whatever

44:21.960 --> 44:25.720
hashtag people type on Instagram, right? Then you can do this with billions of images because

44:25.800 --> 44:31.080
it's billions per day that are showing up. So the amount of training data there is essentially

44:31.080 --> 44:36.760
unlimited. And then you take the output representation, you know, a couple layers down from the outputs

44:37.560 --> 44:43.080
of what the system learned and feed this as input to a classifier for any object in the

44:43.080 --> 44:47.880
world that you want and it works pretty well. So that's transfer learning, okay? Or weekly

44:47.880 --> 44:54.280
supervised transfer learning. People are making very, very fast progress using self-supervised

44:54.280 --> 45:02.360
learning with this kind of scenario as well. And my guess is that that's going to be the future.

45:02.360 --> 45:08.280
For self-supervised learning, how much cleaning do you think is needed for filtering

45:10.600 --> 45:15.480
malicious signal or with a better term? But a lot of people use hashtags on Instagram

45:16.680 --> 45:22.200
to get good SEO that doesn't fully represent the contents of the image.

45:22.920 --> 45:28.440
Like they'll put a picture of a cat and hashtag it with like science, awesome, fun, I don't know,

45:28.440 --> 45:32.200
all kind of... Why would you put science? That's not very good SEO.

45:32.200 --> 45:38.280
The way my colleagues who worked on this project at Facebook now, META, META AI,

45:38.840 --> 45:43.640
a few years ago, they helped with this is that they only selected something like 17,000 tags

45:43.640 --> 45:49.640
that correspond to kind of physical things or situations. Like, you know, that has some visual

45:49.640 --> 45:56.440
content. So, you wouldn't have like hash TBT or anything like that.

45:57.000 --> 46:01.160
Also, they keep a very select set of hashtags. Is that what you're saying?

46:01.160 --> 46:02.040
Yeah. Okay.

46:02.040 --> 46:07.800
But it's still on the order of 10 to 20,000. So it's fairly large.

46:07.800 --> 46:13.960
Okay. Can you tell me about data augmentation? What the heck is data augmentation? And how is it

46:13.960 --> 46:20.120
used maybe contrast of learning for video? What are some cool ideas here?

46:20.760 --> 46:24.680
Right. So data augmentation, I mean, first, data augmentation, you know, is the idea of

46:24.680 --> 46:30.280
artificially increasing the size of your training set by distorting the images that you have in ways

46:30.280 --> 46:34.360
that don't change the nature of the image. Right? So you take... You do MNIST, you can do data

46:34.360 --> 46:39.560
augmentation on MNIST, and people have done this since the 1990s, right? You take a MNIST digit

46:39.640 --> 46:46.840
and you shift it a little bit or you change the size or rotate it, skew it, you know, etc.

46:46.840 --> 46:47.560
Add noise.

46:48.200 --> 46:53.400
Add noise, etc. And it works better. If you train a supervised classifier with augmented data,

46:53.400 --> 46:59.960
you're going to get better results. Now, it's become really interesting over the last couple

46:59.960 --> 47:06.440
years because a lot of self-supervised learning techniques to pre-train vision systems are based

47:06.440 --> 47:14.840
on data augmentation. And the basic techniques is originally inspired by techniques that I worked

47:14.840 --> 47:18.600
on in the early 90s and Jeff Hinton worked on also in the early 90s. They were sort of parallel

47:19.400 --> 47:25.000
work. I used to call this Siamese network. So basically, you take two identical copies of

47:25.000 --> 47:31.720
the same network, they share the same weights, and you show two different views of the same object.

47:31.720 --> 47:35.320
Either those two different views may have been obtained by data augmentation,

47:35.320 --> 47:39.240
or maybe it's two different views of the same scene from a camera that you moved

47:39.240 --> 47:43.160
or at different times or something like that, right? Or two pictures of the same person,

47:43.160 --> 47:48.360
things like that. And then you train this neural net, those two identical copies of this neural net,

47:48.360 --> 47:54.440
to produce an output representation, a vector, in such a way that the representation for those two

47:55.400 --> 48:00.840
images are as close to each other as possible, as identical to each other as possible, right?

48:00.840 --> 48:06.040
Because you want the system to basically learn a function that will be invariant,

48:06.040 --> 48:10.120
that will not change, whose output will not change when you transform those inputs

48:10.840 --> 48:17.720
in those particular ways, right? So that's easy to do. What's complicated is how do you make sure

48:17.720 --> 48:22.520
that when you show two images that are different, the system will produce different things. Because

48:22.520 --> 48:27.640
if you don't have a specific provision for this, the system will just ignore the inputs

48:28.360 --> 48:31.880
when you train it, it will end up ignoring the input and just produce a constant vector that

48:31.880 --> 48:36.600
is the same for every input, right? That's called a collapse. Now, how do you avoid collapse?

48:36.600 --> 48:43.000
So there's two ideas. One idea that I proposed in the early 90s with my colleagues at Bell Labs,

48:43.000 --> 48:48.200
Gene Bromley and a couple other people, which we now call contrastive learning,

48:48.200 --> 48:53.000
which is to have negative examples, right? So you have pairs of images that you know are different,

48:53.960 --> 48:59.000
and you show them to the network and those two copies, and then you push the two output vectors

48:59.000 --> 49:03.720
away from each other, and they will eventually guarantee that things that are symmetrically

49:03.720 --> 49:07.400
similar produce similar representations and things that are different produce different

49:07.400 --> 49:13.240
representations. We actually came up with this idea for a project of doing signature

49:13.240 --> 49:19.560
verification. So we would collect signatures from multiple signatures on the same person

49:20.120 --> 49:23.560
and then train a neural net to produce the same representation, and then

49:25.720 --> 49:29.400
force the system to produce different representation for different signatures.

49:30.840 --> 49:36.280
This was actually the problem was proposed by people from what was a subsidiary of AT&T at

49:36.280 --> 49:42.680
the time called NCR, and they were interested in storing representation of the signature on the 80

49:42.680 --> 49:48.520
bytes of the magnetic strip of a credit card. So we came up with this idea of having a neural

49:48.520 --> 49:53.800
net with 80 outputs that we would quantize on bytes so that we could encode the...

49:53.800 --> 49:57.000
And that encoding was then used to compare whether the signature matches or not?

49:57.000 --> 50:01.000
That's right. So then you would sign, it would run through the neural net, and then you would

50:01.000 --> 50:03.400
compare the output vector to whatever is stored on your card.

50:03.400 --> 50:04.040
Did it actually work?

50:04.600 --> 50:10.520
It worked, but they ended up not using it because nobody cares actually. I mean, the

50:11.480 --> 50:17.480
American financial payment system is incredibly lax in that respect compared to Europe.

50:17.560 --> 50:20.520
Over the signatures? What's the purpose of the signatures anyway?

50:20.520 --> 50:22.520
Nobody looks at them, nobody cares.

50:25.960 --> 50:29.400
So that's contrastive learning, right? So you need positive and negative pairs,

50:29.400 --> 50:34.040
and the problem with that is that even though I had the original paper on this,

50:34.680 --> 50:38.600
I'm actually not very positive about it because it doesn't work in high dimension.

50:38.600 --> 50:43.000
If your representation is high dimensional, there's just too many ways for two things to be

50:43.000 --> 50:47.320
different. And so you would need lots and lots and lots of negative pairs.

50:48.120 --> 50:53.480
So there is a particular implementation of this, which is relatively recent, from actually the

50:53.480 --> 50:59.640
Google Toronto group, where Jeff Hinton is the senior member there, and it's called Simclear,

50:59.640 --> 51:05.400
S-I-M-C-L-R. And it's basically a particular way of implementing this idea of

51:05.400 --> 51:07.960
contrastive learning, the particular objective function.

51:08.600 --> 51:14.840
Now, what I'm much more enthusiastic about these days is non-contrastive methods. So

51:14.840 --> 51:23.000
other ways to guarantee that the representations would be different for different inputs.

51:24.280 --> 51:29.880
And it's actually based on an idea that Jeff Hinton proposed in the early 90s with his

51:29.880 --> 51:33.720
student at the time, Sue Becker. And it's based on the idea of maximizing the mutual

51:33.720 --> 51:37.320
information between the outputs of the two systems. You only show positive pairs,

51:37.320 --> 51:42.200
you only show pairs of images that you know are somewhat similar. And you're trying the two

51:42.200 --> 51:50.120
networks to be informative, but also to be as informative of each other as possible. So basically,

51:50.120 --> 51:57.080
one representation has to be predictable from the other, essentially. And he proposed that idea

51:57.080 --> 52:02.920
had a couple of papers in the early 90s, and then nothing was done about it for decades.

52:03.000 --> 52:09.000
And I kind of revived this idea together with my postdocs at FAIR, particularly a postdoc called

52:09.000 --> 52:15.960
Steph Anthony, who's now a junior professor in Finland at University of Alto. We came up with

52:15.960 --> 52:21.560
something called, that we call Balu twins. And it's a particular way of maximizing the

52:21.560 --> 52:30.040
information content of a vector, you know, using some hypothesis. And we have kind of

52:31.000 --> 52:35.640
another version of it that's more recent now called Vicreg, V-I-C-A-R-E-G, that means

52:35.640 --> 52:39.880
variance invariance covariance regularization. And it's the thing I'm the most excited about

52:39.880 --> 52:44.360
in machine learning in the last 15 years. I mean, I'm not, I'm really, really excited about this.

52:44.360 --> 52:49.240
What kind of data augmentation is useful for that non-contrasting learning method?

52:50.120 --> 52:55.160
Are we talking about, does that not matter that much? Or it seems like a very important part of

52:55.160 --> 52:59.480
the step. Yeah. How you generate the images that are similar, but sufficiently different.

52:59.480 --> 53:03.000
Yeah, that's right. It's an important step. And it's also an annoying step because you need to

53:03.000 --> 53:09.320
have that knowledge of what the documentation you can do that do not change the nature of the

53:09.320 --> 53:16.360
object. And so the standard scenario, which a lot of people working in this area are using, is you

53:16.360 --> 53:23.320
use the type of distortion. So basically you do geometric distortion. So one basically just shifts

53:23.320 --> 53:27.640
the image a little bit, it's called cropping. Another one kind of changes the scale a little bit.

53:27.720 --> 53:32.280
Another one kind of rotates it. Another one changes the colors. You know, you can do a shift in

53:32.280 --> 53:37.720
color balance or something like that. Saturation. Another one sort of blurs it. Another one adds

53:37.720 --> 53:42.360
noise. So you have like a catalog of kind of standard things and people try to use the same

53:42.360 --> 53:47.800
ones for different algorithms so that they can compare. But some algorithms, some self-supervised

53:47.800 --> 53:53.160
algorithm actually can deal with much bigger, like more aggressive data augmentation and some

53:53.160 --> 53:57.880
don't. So that kind of makes the whole thing difficult. But that's the kind of distortions

53:57.880 --> 54:07.240
we're talking about. And so you train with those distortions. And then you chop off the last layer

54:07.240 --> 54:13.640
or couple layers of the network. And you use the representation as input to a classifier. You

54:13.640 --> 54:20.760
train the classifier on ImageNet, let's say, or whatever, and measure the performance. And

54:21.640 --> 54:25.960
interestingly enough, the methods that are really good at eliminating the information that is

54:25.960 --> 54:31.320
irrelevant, which is the distortions between those images, do a good job at eliminating it.

54:32.360 --> 54:37.800
And as a consequence, you cannot use the representations in those systems for things

54:37.800 --> 54:43.400
like object detection and localization because that information is gone. So the type of

54:43.400 --> 54:48.600
data augmentation you need to do depends on the tasks you want eventually the system to solve.

54:48.600 --> 54:53.480
And the type of standard data augmentation that we use today are only appropriate for

54:53.480 --> 54:57.640
object recognition or image classification. They're not appropriate for things like...

54:57.640 --> 55:02.040
Can you help me out understand why the localization... So you're saying it's just not good at the

55:02.040 --> 55:07.800
negative, like at classifying the negative. So that's why it can't be used for the localization?

55:07.800 --> 55:13.560
No, it's just that you train the system, you give it an image and then you give it the same image

55:13.560 --> 55:19.000
shifted and scaled, and you tell it that's the same image. So the system basically is trained

55:19.000 --> 55:24.200
to eliminate the information about position and size. So now, and now you want to use that

55:25.000 --> 55:29.320
to figure out where an object is and what size it is. Like a bounding box, like they'd be able to

55:29.320 --> 55:35.880
actually... Okay, it can still find the object in the image. It's just not very good at finding

55:35.880 --> 55:42.280
the exact boundaries of that object. Interesting. Which, you know, that's an interesting sort of

55:42.360 --> 55:48.520
philosophical question. How important is object localization anyway? We're like obsessed by

55:48.520 --> 55:54.120
measuring like image segmentation, obsessed by measuring perfectly knowing the boundaries of

55:54.120 --> 56:03.720
objects when arguably that's not that essential to understanding what are the contents of the scene.

56:03.720 --> 56:08.680
On the other hand, I think evolutionarily, the first vision systems in animals were basically

56:08.680 --> 56:14.360
all about localization, very little about recognition. And in the human brain, you have two

56:14.360 --> 56:22.840
separate pathways for recognizing the nature of a scene or an object and localizing objects. So

56:22.840 --> 56:27.880
you use the first pathway called the ventral pathway for telling what you're looking at.

56:29.000 --> 56:34.040
The other pathway, the dorsal pathway is used for navigation, for grasping, for everything else.

56:34.040 --> 56:39.480
And basically, a lot of the things you need for survival are localization and detection.

56:41.800 --> 56:47.000
Is similarity learning or contrastive learning or these non-contrastive methods the same as

56:47.000 --> 56:52.680
understanding something? Just because you know a distorted cat is the same as a non-distorted cat,

56:52.680 --> 56:55.800
does that mean you understand what it means to be a cat?

56:56.600 --> 56:59.960
To some extent. I mean, it's a superficial understanding, obviously.

56:59.960 --> 57:03.160
But what is the ceiling of this method, do you think? Is this just one

57:03.880 --> 57:09.960
trick on the path to doing self-supervised learning or can we go really, really far?

57:09.960 --> 57:16.600
I think we can go really far. So if we figure out how to use techniques of that type, perhaps

57:16.600 --> 57:23.320
very different, but of the same nature, to train a system from video to do video prediction,

57:23.320 --> 57:32.840
essentially. I think we'll have a path towards, I wouldn't say unlimited, but a path towards

57:32.840 --> 57:44.520
some level of physical common sense in machines. And I also think that that ability to learn how

57:44.600 --> 57:52.360
the world works from a high throughput channel like vision is a necessary step towards

57:54.040 --> 57:58.520
real artificial intelligence. In other words, I believe in grounded intelligence. I don't think

57:58.520 --> 58:04.280
we can train a machine to be intelligent purely from text. Because I think the amount of information

58:04.280 --> 58:09.640
about the world that's contained in text is tiny compared to what we need to know.

58:10.280 --> 58:17.720
So, for example, people have attempted to do this for 30 years, the SAG project and things

58:17.720 --> 58:23.480
like that, of basically writing down all the facts that are known and hoping that some sort

58:23.480 --> 58:28.200
of common sense would emerge. I think it's basically hopeless. But let me take an example.

58:28.200 --> 58:33.400
You take an object. I describe a situation to you. I take an object, I put it on the table,

58:33.400 --> 58:38.520
and I push the table. It's completely obvious to you that the object will be pushed with the table,

58:39.080 --> 58:44.280
because it's sitting on it. There's no text in the world, I believe, that explains this.

58:44.920 --> 58:53.800
And so, if you train a machine as powerful as it could be, your GPT 5000 or whatever it is,

58:53.800 --> 59:00.200
it's never going to learn about this. That information is just not present in any text.

59:00.920 --> 59:04.600
Well, the question with the SAG project, the dream I think is to have like

59:05.560 --> 59:14.680
10 million, say, facts like that, that give you a head start, like a parent guiding you.

59:15.400 --> 59:18.440
Now, we humans don't need a parent to tell us that the table will move,

59:19.080 --> 59:25.800
sorry, the smartphone will move with the table. But we get a lot of guidance in other ways,

59:25.800 --> 59:28.200
so it's possible that we can give it a quick shortcut.

59:28.200 --> 59:30.040
And what about cat? The cat knows that.

59:30.920 --> 59:34.360
No, but they evolved. No, they learned like us.

59:35.960 --> 59:45.400
Sorry, the physics of stuff. Well, yeah, so you're putting a lot of intelligence onto the

59:45.400 --> 59:53.560
nurture side, not the nature. There's a very inefficient, arguably, process of evolution

59:53.560 --> 59:59.800
that got us from bacteria to who we are today. Started at the bottom, now we're here.

59:59.800 --> 01:00:07.640
So the question is how fundamental is that the nature of the whole hardware?

01:00:08.520 --> 01:00:14.200
And then is there any way to shortcut it if it's fundamental? If it's not, if it's most of intelligence,

01:00:14.200 --> 01:00:19.080
most of the cool stuff we've been talking about is mostly nurture, mostly trained. We figured

01:00:19.080 --> 01:00:25.000
out by observing the world. We can form that big, beautiful, sexy background model that you're

01:00:25.000 --> 01:00:32.120
talking about just by sitting there. Then, okay, then you need to then like maybe

01:00:34.760 --> 01:00:38.760
it is all supervised learning all the way down. It's all supervised learning.

01:00:38.760 --> 01:00:44.920
Whatever it is that makes human intelligence different from other animals, which a lot of

01:00:44.920 --> 01:00:49.800
people think is language and logical reasoning and this kind of stuff. It cannot be that complicated

01:00:49.800 --> 01:00:59.160
because it only popped up in the last million years. It only involves less than 1% of a genome,

01:00:59.160 --> 01:01:03.560
right, which is the difference between human genome and chimps or whatever. So

01:01:05.480 --> 01:01:10.760
it can be that complicated. It can be that fundamental. Most of the so complicated stuff

01:01:10.760 --> 01:01:15.560
already exist in cats and dogs and certainly primates, non-human primates.

01:01:15.880 --> 01:01:22.440
Yeah, that little thing with humans might be just something about social interaction and

01:01:22.440 --> 01:01:30.200
ability to maintain ideas across like a collective of people. It sounds very dramatic and very

01:01:30.200 --> 01:01:33.240
impressive, but it probably isn't mechanistically speaking.

01:01:33.240 --> 01:01:40.920
It is, but we're not there yet. We have, I mean, this is number 634 in the list of

01:01:40.920 --> 01:01:47.240
problems we have to solve. So basic physics of the world is number one. What are you,

01:01:48.200 --> 01:01:56.760
just a quick tangent on data augmentation. So a lot of it is hard coded versus learned.

01:01:58.200 --> 01:02:02.840
Do you have any intuition that maybe there could be some weird data augmentation,

01:02:03.560 --> 01:02:08.280
like generative type of data augmentation, like doing something weird to images, which

01:02:08.280 --> 01:02:16.120
then improves the similarity learning process. So not just kind of dumb, simple distortions,

01:02:16.120 --> 01:02:20.760
but by you shaking your head, just saying that even simple distortions are enough.

01:02:20.760 --> 01:02:24.840
I think no, I think data augmentation is a temporary necessary evil.

01:02:26.360 --> 01:02:31.800
So what people are working on now is two things. One is the type of self-supervisioning,

01:02:33.080 --> 01:02:36.920
like trying to translate the type of self-supervisioning people using language,

01:02:37.000 --> 01:02:41.080
translating these two images, which is basically a denosing autoencoder method.

01:02:41.720 --> 01:02:49.320
So you take an image, you block, you mask some parts of it, and then you train some giant neural

01:02:49.320 --> 01:02:57.640
net to reconstruct the parts that are missing. And until very recently, there was no working

01:02:57.640 --> 01:03:02.360
methods for that. All the autoencoder type methods for images weren't producing very

01:03:02.360 --> 01:03:07.240
good representation. But there's a paper now coming out of the Fair Group at Immelo Park

01:03:07.240 --> 01:03:13.080
that actually works very well. So that doesn't require the documentation, that requires only

01:03:13.080 --> 01:03:20.440
masking. Okay. Only masking for images. Okay. Right. So you mask a part of the image and you

01:03:20.440 --> 01:03:28.520
train a system, which in this case is a transformer because the transformer represents the image as

01:03:29.240 --> 01:03:33.160
non-overlapping patches. So it's easy to mask patches and things like that.

01:03:33.160 --> 01:03:38.760
Okay. Then my question transfers to that problem, then masking. Why should the mask be a square

01:03:38.760 --> 01:03:46.520
rectangle? So it doesn't matter. I think we're gonna come up probably in the future with ways

01:03:46.520 --> 01:03:52.840
to mask that are kind of random, essentially. I mean, they are random already, but...

01:03:52.840 --> 01:04:01.480
No, no. But something that's challenging, optimally challenging. So maybe it's a metaphor

01:04:01.480 --> 01:04:08.120
that doesn't apply, but it seems like there's a data augmentation or masking. There's an

01:04:08.120 --> 01:04:13.880
interactive element with it. You're almost playing with an image, and it's the way we

01:04:13.880 --> 01:04:17.960
play with an image in our minds. No, but it's like dropout. It's like Boston Machine Training.

01:04:18.760 --> 01:04:28.920
You know, every time you see a percept, you can perturb it in some way. And then the principle

01:04:28.920 --> 01:04:35.000
of the training procedure is to minimize the difference of the output or the representation

01:04:35.000 --> 01:04:41.400
between the clean version and the corrupted version, essentially. And you can do this in

01:04:41.400 --> 01:04:47.720
real time. So Boston Machine worked like this. You show a percept, you tell the machine that's

01:04:47.720 --> 01:04:56.760
a good combination of activities or your input neurons. And then you either let them go their

01:04:56.760 --> 01:05:02.680
merry way without clamping them to values, or you only do this with a subset. And what you're

01:05:02.680 --> 01:05:08.440
doing is you're training the system so that the stable state of the entire network is the same,

01:05:08.440 --> 01:05:12.120
regardless of whether it sees the entire input or whether it sees only part of it.

01:05:13.080 --> 01:05:17.880
You know, the nosing autoencoder method is basically the same thing, right? You're training

01:05:17.880 --> 01:05:22.920
a system to reproduce the input, the complete input and filling the blanks, regardless of which

01:05:22.920 --> 01:05:26.920
parts are missing. And that's really the underlying principle. And you could imagine,

01:05:26.920 --> 01:05:32.440
sort of even in the brain, some sort of neural principle where, you know, neurons can oscillate,

01:05:32.440 --> 01:05:37.960
right? So they take their activity and then temporarily they kind of shut off to, you know,

01:05:37.960 --> 01:05:43.800
force the rest of the system to basically reconstruct the input without their help,

01:05:43.800 --> 01:05:50.520
you know? And I mean, you can imagine, you know, more or less biologically possible

01:05:50.520 --> 01:05:56.920
processes. Something like that. And I guess with this denoising autoencoder and masking and

01:05:56.920 --> 01:06:02.760
data augmentation, you don't have to worry about being super efficient. You can just do as much

01:06:02.760 --> 01:06:08.680
as you want and get better over time. Because I was thinking like you might want to be clever

01:06:08.680 --> 01:06:15.000
about the way you do all these procedures, you know, but that's only if it's somehow costly

01:06:15.000 --> 01:06:21.400
to do every iteration, but it's not really. Not really. And then there is, you know,

01:06:21.400 --> 01:06:25.480
data augmentation without explicit data augmentation is data augmentation by weighting,

01:06:25.480 --> 01:06:31.400
which is, you know, the sort of video prediction. You're observing a video clip,

01:06:31.480 --> 01:06:38.040
observing the, you know, the continuation of that video clip. You try to learn a representation

01:06:38.040 --> 01:06:42.280
using the joint embedding architectures in such a way that the representation of the

01:06:42.280 --> 01:06:47.160
future clip is easily predictable from the representation of the observed clip.

01:06:48.520 --> 01:06:55.400
Do you think YouTube has enough raw data from which to learn how to be a cat?

01:06:56.280 --> 01:06:56.840
I think so.

01:06:57.720 --> 01:07:01.160
So the amount of data is not the constraint?

01:07:01.160 --> 01:07:03.400
No, it would require some selection, I think.

01:07:04.040 --> 01:07:08.360
Some selection of, you know, maybe the right type of data.

01:07:08.360 --> 01:07:13.800
Don't go down the rabbit hole of just cat videos. You might need to watch some lectures or something.

01:07:15.720 --> 01:07:22.120
How meta would that be if it like watches lectures about intelligence and then learns,

01:07:22.200 --> 01:07:26.040
watches your lectures in NYU and learns from that how to be intelligent?

01:07:26.040 --> 01:07:27.080
I don't think that would be enough.

01:07:30.280 --> 01:07:34.360
What's your, do you find multimodal learning interesting? We've been talking about visual

01:07:34.360 --> 01:07:38.040
language, like combining those together, maybe audio, all those kinds of things.

01:07:38.040 --> 01:07:41.960
There's a lot of things that I find interesting in the short term, but are not

01:07:42.520 --> 01:07:47.160
addressing the important problem that I think are really kind of the big challenges. So I think,

01:07:47.160 --> 01:07:51.240
you know, things like multitask learning, continual learning, you know,

01:07:53.320 --> 01:07:57.640
adversarial issues. I mean, those have, you know, great practical interests in the

01:07:57.640 --> 01:08:02.520
relatively short term, possibly, but I don't think they're fundamental, you know, active learning,

01:08:02.520 --> 01:08:08.120
even to some extent reinforcement learning. I think those things will become either obsolete or

01:08:09.000 --> 01:08:16.120
useless or easy once we figure out how to do self-supervised representation learning or

01:08:17.560 --> 01:08:23.000
predictive world models. And so I think that's what, you know, the entire community should be

01:08:23.000 --> 01:08:27.160
focusing on. At least people are interested in sort of fundamental questions or, you know,

01:08:27.160 --> 01:08:31.720
really kind of pushing the envelope of AI towards the next stage. But of course,

01:08:31.720 --> 01:08:35.160
there is like a huge amount of, you know, very interesting work to do in sort of practical

01:08:35.160 --> 01:08:41.400
questions that have, you know, short term impact. Well, you know, it's difficult to talk about the

01:08:41.400 --> 01:08:46.360
temporal scale because all of human civilization will eventually be destroyed because the

01:08:47.480 --> 01:08:53.240
the sun will die out. And even if Elon Musk is successful, multi-planetary colonization across

01:08:53.240 --> 01:08:59.240
the galaxy, eventually the entirety of it will just become giant black holes. And

01:09:00.280 --> 01:09:05.560
that's gonna take a while though. So, but what I'm saying is then that logic can be used to say

01:09:05.560 --> 01:09:14.280
it's all meaningless. I'm saying all that to say that multitask learning might be your song,

01:09:14.280 --> 01:09:18.920
you're calling it practical or pragmatic or whatever. That might be the thing that achieves

01:09:18.920 --> 01:09:27.080
something very akin to intelligence while we're trying to solve the more general problem of

01:09:27.080 --> 01:09:32.200
self-supervised learning of background knowledge. So the reason I bring that up may be one way to

01:09:32.200 --> 01:09:36.600
ask that question. I've been very impressed by what Tesla autopilot team is doing. I don't know

01:09:36.600 --> 01:09:41.960
if you've gotten a chance to glance at this particular one example of multitask learning

01:09:42.040 --> 01:09:48.840
where they're literally taking the problem like, I don't know, Charles Darwin start studying animals.

01:09:48.840 --> 01:09:54.120
They're studying the problem of driving and asking, okay, what are all the things you have to perceive?

01:09:55.000 --> 01:09:59.960
And the way they're solving it is one, there's an ontology where you're bringing that to the

01:09:59.960 --> 01:10:03.800
table. So you're formulating a bunch of different tasks. It's like over a hundred tasks or something

01:10:03.800 --> 01:10:08.520
like that that they're involved in driving. And then they're deploying it and then getting data

01:10:08.520 --> 01:10:13.320
back from people that run to trouble. And they're trying to figure out, do we add tasks? Do we,

01:10:13.320 --> 01:10:18.840
like we focus on each individual task separately? In fact, half. So the, I would say, I'll classify

01:10:18.840 --> 01:10:23.640
Andre Carpathi's talk in two ways. So one was about doors. And the other one about how much

01:10:23.640 --> 01:10:30.840
ImageNet sucks. He kept going back and forth on those two topics, which ImageNet sucks, meaning

01:10:30.840 --> 01:10:37.320
you can't just use a single benchmark. There's so like, you have to have like a giant suite of

01:10:37.320 --> 01:10:39.880
benchmarks to understand how well your system actually works.

01:10:39.880 --> 01:10:47.240
Oh, I agree with him. I mean, he's a very sensible guy. Now, okay, it's very clear that if you're

01:10:47.240 --> 01:10:51.880
faced with an engineering problem that you need to solve in a relatively short time,

01:10:51.880 --> 01:10:56.520
particularly if you have Elon Musk breathing down your neck, you're going to have to take

01:10:56.520 --> 01:11:03.000
shortcuts, right? You might think about the fact that the right thing to do and the long-term

01:11:03.000 --> 01:11:09.240
solution involves some fancy self-improvement running, but you have Elon Musk breathing down

01:11:09.240 --> 01:11:18.760
your neck. And this involves human lives. And so you have to basically just do the systematic

01:11:18.760 --> 01:11:26.680
engineering and fine-tuning and refinements and try and error and all that stuff. There's nothing

01:11:26.680 --> 01:11:34.840
wrong with that. That's called engineering. That's called putting technology out in the world.

01:11:35.800 --> 01:11:45.960
And you have to kind of ironclad it before you do this so much for grand ideas and principles.

01:11:48.200 --> 01:11:55.640
But I'm placing myself sort of some upstream of this, quite a bit upstream of this.

01:11:55.720 --> 01:11:57.800
You're a play-doh think about platonic forms.

01:11:59.000 --> 01:12:06.120
It's not platonic because eventually, I want the stuff to get used, but it's okay if it takes

01:12:06.120 --> 01:12:10.280
five or 10 years for the community to realize this is the right thing to do. I've done this

01:12:10.280 --> 01:12:16.840
before. It's been the case before that I've made that case. I mean, if you look back in the mid-2000,

01:12:16.840 --> 01:12:21.800
for example, and you ask yourself the question, okay, I want to recognize cars or faces or whatever.

01:12:22.200 --> 01:12:26.920
You know, I can use convolutional nets, so I can use more conventional

01:12:28.200 --> 01:12:32.040
kind of computer vision techniques using interest point detectors or sift

01:12:32.600 --> 01:12:38.040
density features and sticking an SVM on top. At that time, the datasets were so small that

01:12:38.760 --> 01:12:43.960
those methods that use more hand-engineering worked better than comnets. There was just

01:12:43.960 --> 01:12:48.920
not enough data for comnets. And comnets were a little slow with the kind of hardware that

01:12:48.920 --> 01:12:56.120
was available at the time. And there was a sea change when, basically, when datasets became

01:12:56.120 --> 01:13:04.280
bigger and GPUs became available. That's what two of the main factors that basically made people

01:13:04.280 --> 01:13:14.520
change their mind. And you can look at the history of all sub-branches of AI or pattern

01:13:14.520 --> 01:13:21.560
recognition. And there's a similar trajectory followed by techniques where people start by,

01:13:21.560 --> 01:13:29.160
you know, engineering the hell out of it. You know, be it optical character recognition,

01:13:29.160 --> 01:13:34.840
speech recognition, computer vision like image recognition in general, natural language

01:13:34.840 --> 01:13:39.000
understanding like, you know, translation, things like that, right? You start to engineer the hell

01:13:39.000 --> 01:13:44.680
out of it. You start to acquire all the knowledge, the prior knowledge you know about image formation,

01:13:44.680 --> 01:13:49.480
about, you know, the shape of characters, about, you know, morphological operations,

01:13:49.480 --> 01:13:54.200
about like feature extraction, Fourier transforms, you know, vernike moments, you know, whatever,

01:13:54.200 --> 01:13:58.920
right? People have come up with thousands of ways of representing images so that they could be easily

01:13:59.800 --> 01:14:04.920
classified afterwards. Same for speech recognition, right? There is, you know, two decades for people

01:14:04.920 --> 01:14:11.080
to figure out a good front-end to prepossess a speech signal so that, you know, the information

01:14:11.080 --> 01:14:16.360
about what is being said is preserved, but most of the information about the identity of the speaker

01:14:16.360 --> 01:14:23.560
is gone. You know, casserole coefficients or whatever, right? And same for text, right?

01:14:24.440 --> 01:14:33.320
You do name identity recognition and you parse and you do tagging of the parts of speech and,

01:14:33.320 --> 01:14:38.360
you know, you do this sort of tree representation of clauses and all that stuff right before you

01:14:38.360 --> 01:14:47.240
can do anything. So that's how it starts, right? Just engineer the hell out of it. And then you

01:14:47.240 --> 01:14:52.680
start having data and maybe you have more powerful computers, maybe you know something about statistical

01:14:52.680 --> 01:14:56.680
learning. So you start using machine learning and it's usually a small sliver on top of your

01:14:56.680 --> 01:15:01.960
kind of handcrafted system where, you know, you extract features by hand, okay? And now, you know,

01:15:01.960 --> 01:15:05.720
nowadays the standard way of doing this is that you train the entire thing end-to-end with a deep

01:15:05.720 --> 01:15:11.000
learning system and it learns its own features and, you know, speech recognition systems nowadays,

01:15:11.880 --> 01:15:17.160
OCR systems are completely end-to-end. It's, you know, it's some giant neural net that takes

01:15:17.160 --> 01:15:22.760
raw waveforms and produces a sequence of characters coming out. And it's just a huge neural net,

01:15:22.760 --> 01:15:28.120
right? There's no, you know, Markov model, there's no language model that is explicit other than,

01:15:28.120 --> 01:15:31.640
you know, something that's ingrained in the, in the sort of neural language model if you want.

01:15:31.960 --> 01:15:37.800
Same for translation, same for all kinds of stuff. So you see this continuous evolution from,

01:15:39.240 --> 01:15:49.000
you know, less and less hand-crafting and more and more learning. And I think it's true in biology

01:15:49.000 --> 01:15:56.760
as well. So, I mean, we might disagree about this, maybe not in this one little piece at the end,

01:15:56.760 --> 01:16:02.840
you mentioned active learning. It feels like active learning, which is the selection of data,

01:16:02.840 --> 01:16:07.640
and also the interactivity needs to be part of this giant neural network. You cannot just be

01:16:07.640 --> 01:16:13.800
an observer to do self-supervised learning. You have to, well, self-supervised learning is just

01:16:13.800 --> 01:16:19.560
a word, but I would, whatever this giant stack of a neural network that's automatically learning,

01:16:19.560 --> 01:16:28.200
it feels, my intuition is that you have to have a system, whether it's a physical robot

01:16:28.200 --> 01:16:34.680
or a digital robot that's interacting with the world and doing so in a flawed way and

01:16:34.680 --> 01:16:43.000
improving over time in order to form the self-supervised learning. Well, you can't just give it a

01:16:43.080 --> 01:16:48.520
giant sea of data. Okay, I agree and I disagree. I agree in the sense that I think,

01:16:50.120 --> 01:16:56.280
I agree in two ways. The first way I agree is that if you want and you certainly need

01:16:56.280 --> 01:16:59.720
a causal model of the world that allows you to predict the consequences of your actions,

01:17:00.360 --> 01:17:04.920
to train that model, you need to take actions. You need to be able to act in a world and see the

01:17:04.920 --> 01:17:10.360
effect for you to learn causal models of the world. So that's not obvious because you can

01:17:10.440 --> 01:17:15.880
observe others and you can infer that they're similar to you and then you can learn from that.

01:17:15.880 --> 01:17:20.280
Yeah, but then you have to kind of hardware that part and mirror neurons and all that stuff.

01:17:22.120 --> 01:17:29.320
And it's not clear to me how you would do this in a machine. So I think the action part would be

01:17:29.320 --> 01:17:36.920
necessary for having causal models of the world. The second reason it may be necessary or at least

01:17:37.000 --> 01:17:43.960
more efficient is that active learning basically goes for the juggler of what you don't know.

01:17:43.960 --> 01:17:51.800
Right? There's obvious areas of uncertainty about your world and about how the world behaves.

01:17:52.920 --> 01:18:00.200
And you can resolve this uncertainty by systematic exploration of that part that you don't know.

01:18:00.200 --> 01:18:04.120
And if you know that you don't know, then it makes you curious. You kind of look into

01:18:04.120 --> 01:18:12.520
situations that and across the animal world, different species at different levels of curiosity.

01:18:13.720 --> 01:18:19.560
Depending on how they're built. So cats and rats are incredibly curious. Dogs know so much,

01:18:19.560 --> 01:18:23.800
I mean less. So it could be useful to have that kind of curiosity.

01:18:23.800 --> 01:18:28.440
So it'd be useful, but curiosity just makes the process faster. It doesn't make the process exist.

01:18:29.400 --> 01:18:37.640
So what process, what learning process is it that active learning makes more efficient?

01:18:38.600 --> 01:18:44.680
And I'm asking that first question. We haven't answered that question yet,

01:18:44.680 --> 01:18:47.960
so I'll worry about active learning once this question is...

01:18:47.960 --> 01:18:55.240
So it's the more fundamental question to ask. And if active learning or interaction increases

01:18:55.320 --> 01:19:01.560
the efficiency of the learning. See, sometimes it becomes very different if the increase is

01:19:01.560 --> 01:19:08.040
several orders of magnitude. Right? That's true. But fundamentally, it's still the same thing.

01:19:08.040 --> 01:19:12.840
And building up the intuition about how to in a self-supervised way to construct

01:19:12.840 --> 01:19:19.240
background models, efficient or inefficient, is the core problem. What do you think about

01:19:19.240 --> 01:19:24.520
Yosha Ben-Jos talking about consciousness and all of these kinds of concepts?

01:19:24.520 --> 01:19:28.680
Okay. I don't know what consciousness is, but...

01:19:30.120 --> 01:19:31.000
It's a good opener.

01:19:31.880 --> 01:19:36.120
And to some extent, a lot of the things that are said about consciousness remind me of

01:19:37.000 --> 01:19:41.560
the questions people were asking themselves in the 18th century or 17th century when they

01:19:41.560 --> 01:19:48.680
discovered that how the eye works and the fact that the image at the back of the eye was upside

01:19:48.680 --> 01:19:54.440
down because you have a lens. And so on your retina, the image that forms is an image of the

01:19:54.440 --> 01:19:59.720
world, but it's upside down. How is it that you see right side up? And with what we know today

01:19:59.720 --> 01:20:05.880
in science, we realize this question doesn't make any sense or is kind of ridiculous in some way.

01:20:05.880 --> 01:20:09.560
Right? So I think a lot of what is said about consciousness is of that nature. Now that said,

01:20:09.560 --> 01:20:14.520
there's a lot of really smart people for whom I have a lot of respect who are talking about this

01:20:14.520 --> 01:20:21.880
topic, people like David Chalmers, who is a colleague of mine at NYU. I have kind of an

01:20:21.880 --> 01:20:29.960
orthodox folk speculative hypothesis about consciousness. So we're talking about this

01:20:29.960 --> 01:20:38.360
idea of a world model. And I think our entire prefrontal context basically is the engine for

01:20:38.440 --> 01:20:45.320
a world model. But when we are attending at a particular situation, we're focused on that

01:20:45.320 --> 01:20:53.240
situation, we basically cannot attend to anything else. And that seems to suggest that we basically

01:20:53.240 --> 01:21:01.320
have only one world model engine in our prefrontal context. That engine is configurable to the

01:21:01.320 --> 01:21:08.280
situation at hand. So we are building a box out of wood or we are driving down the highway

01:21:08.280 --> 01:21:14.200
playing chess. We basically have a single model of the world that we configure into the situation

01:21:14.200 --> 01:21:20.440
at hand, which is why we can only attend to one task at a time. Now, if there is a task that we

01:21:20.440 --> 01:21:27.320
do repeatedly, it goes from the sort of deliberate reasoning using model of the world and prediction

01:21:27.320 --> 01:21:30.360
and perhaps something like model predictive control, which I was talking about earlier,

01:21:31.240 --> 01:21:35.480
to something that is more subconscious that becomes automatic. So I don't know if you've

01:21:35.480 --> 01:21:45.400
ever played against a chess grandmaster. I get wiped out in 10 plays. And I have to think about

01:21:45.400 --> 01:21:53.080
my move for like 15 minutes. And the person in front of me, the grandmaster, would just like

01:21:53.720 --> 01:21:59.880
react within seconds. He doesn't need to think about it. That's become part of the subconscious

01:21:59.880 --> 01:22:07.560
because it's basically just pattern recognition at this point. The first few hours you drive a car,

01:22:07.560 --> 01:22:11.720
you are really attentive, you can't do anything else. And then after 20, 30 hours of practice,

01:22:11.720 --> 01:22:16.120
50 hours, it's subconscious. You can talk to the person next to you, things like that.

01:22:16.920 --> 01:22:20.040
Unless the situation becomes unpredictable, and then you have to stop talking.

01:22:21.000 --> 01:22:27.240
So that suggests you only have one model in your head. And it might suggest the idea that

01:22:27.240 --> 01:22:32.440
consciousness basically is the module that configures this world model of yours. You need to have

01:22:32.440 --> 01:22:39.160
some sort of executive kind of overseer that configures your world model for the situation

01:22:39.160 --> 01:22:45.400
at hand. And that needs to kind of the really curious concept that consciousness is not a

01:22:45.400 --> 01:22:50.840
consequence of the power of our mind, but of the limitation of our brains. But because we have only

01:22:50.840 --> 01:22:57.720
one world model, we have to be conscious. If we had as many world models as there are situations we

01:22:57.720 --> 01:23:01.960
encounter, then we could do all of them simultaneously. And we wouldn't need this sort of

01:23:01.960 --> 01:23:06.840
executive control that we call consciousness. Yeah, interesting. And somehow maybe that

01:23:06.840 --> 01:23:12.120
executive controller, I mean, the hard problem of consciousness, there's some kind of chemicals

01:23:12.120 --> 01:23:17.480
and biology that's creating a feeling like it feels to experience some of these things.

01:23:18.440 --> 01:23:25.080
That's kind of like the hard question is, what the heck is that? And why is that useful? Maybe the

01:23:25.080 --> 01:23:31.560
more pragmatic question? Why is it useful to feel like this is really you experiencing this versus

01:23:31.560 --> 01:23:40.920
just like information being processed? It could be just a very nice side effect of the way we

01:23:40.920 --> 01:23:49.800
evolved. That's just very useful to feel a sense of ownership to the decisions you make,

01:23:49.800 --> 01:23:54.040
to the perceptions you make, to the model you're trying to maintain, like you own this thing.

01:23:54.760 --> 01:23:59.080
And this is the only one you got. And if you lose it, it's going to really suck. And so you should

01:23:59.080 --> 01:24:07.800
really send the brain some signals about it. What ideas do you believe might be true that most or at

01:24:07.800 --> 01:24:12.840
least many people disagree with you with? Let's say in the space of machine learning.

01:24:13.640 --> 01:24:21.000
Well, depends who you talk about. So certainly there is a bunch of people who are nativists,

01:24:21.000 --> 01:24:25.000
who think that a lot of the basic things about the world are kind of hardwired in our minds.

01:24:26.360 --> 01:24:29.560
Things like the world is three dimensional, for example, is that hardwired.

01:24:30.360 --> 01:24:37.400
Things like object permanence is something that we learn before the age of three months or so,

01:24:37.800 --> 01:24:44.280
or are we born with it? And there are very wide disagreements among the cognitive scientists

01:24:45.560 --> 01:24:48.280
for this. I think those things are actually very simple to learn.

01:24:50.520 --> 01:24:56.360
Is it the case that the oriented edge detectors in V1 are learned or are they hardwired? I think

01:24:56.360 --> 01:25:00.520
they are learned. They might be learned before both because it's really easy to generate signals

01:25:00.520 --> 01:25:06.360
from the retina that actually will train edge detectors. And again, those are things that can

01:25:06.360 --> 01:25:14.120
be learned within minutes of opening your eyes. Since the 1990s, we have algorithms that can

01:25:14.120 --> 01:25:18.760
learn oriented edge detectors completely unsupervised with the equivalent of a few minutes of real

01:25:18.760 --> 01:25:24.840
time. So those things have to be learned. And there's also those MIT experiments where you

01:25:24.840 --> 01:25:31.240
kind of plug the optical nerve on the auditory cortex of a baby ferret and that auditory cortex

01:25:31.240 --> 01:25:37.000
become a visual cortex essentially. So clearly, there's running taking place there.

01:25:37.880 --> 01:25:42.440
So I think a lot of what people think are so basic that they need to be hardwired,

01:25:43.080 --> 01:25:45.320
I think a lot of those things are learned because they're easy to learn.

01:25:46.200 --> 01:25:51.480
So you put a lot of value in the power of learning. What kind of things do you suspect

01:25:51.480 --> 01:25:55.320
might not be learned? Is there something that could not be learned?

01:25:55.960 --> 01:26:03.320
So your intrinsic drives are not learned. There are the things that make humans human

01:26:03.320 --> 01:26:10.200
or make cats different from dogs. It's the basic drives that are kind of hardwired in our

01:26:10.840 --> 01:26:15.240
Bezo Ganglia. I mean, there are people who are working on this kind of stuff that's called

01:26:15.240 --> 01:26:19.480
intrinsic motivation in the context of reinforcement learning. So these are objective

01:26:19.480 --> 01:26:24.440
functions where the reward doesn't come from the external world. It's computed by your own brain.

01:26:24.440 --> 01:26:29.160
Your own brain computes whether you're happy or not. It measures your degree of

01:26:30.840 --> 01:26:37.000
comfort or in comfort. And because it's your brain computing this, presumably it knows also

01:26:37.000 --> 01:26:45.240
how to estimate gradients of this. So it's easier to learn when your objective is intrinsic.

01:26:46.920 --> 01:26:53.240
So that has to be hardwired. The critic that makes long-term prediction of the outcome,

01:26:53.320 --> 01:26:59.560
which is the eventual result of this, that's learned. And perception is learned and your

01:26:59.560 --> 01:27:04.360
model of the world is learned. But let me take an example of why the critic... I mean,

01:27:04.360 --> 01:27:11.480
an example of how the critic may be learned. If I come to you, I reach across the table and

01:27:11.480 --> 01:27:16.040
I pinch your arm, complete surprise for you. You would not have expected this from me.

01:27:16.040 --> 01:27:19.800
I was expecting that the whole time, but yes. Let's say for the sake of the story.

01:27:20.520 --> 01:27:26.520
Okay. Your bezelganglia is going to light up because it's going to hurt, right?

01:27:28.280 --> 01:27:32.840
And now your model of the world includes the fact that I may pinch you if I approach my...

01:27:34.680 --> 01:27:35.800
Don't trust humans.

01:27:36.920 --> 01:27:41.320
My hand to your arm. So if I try again, you're going to recoil. And that's your critic,

01:27:42.280 --> 01:27:51.960
your predictor of your ultimate pain system that predicts that something bad is going to

01:27:51.960 --> 01:27:55.080
happen and you recoil to avoid it. So even that can be learned.

01:27:55.080 --> 01:28:00.760
That is learned, definitely. This is what allows you also to define some goals, right? So

01:28:02.280 --> 01:28:06.760
the fact that you're a school child who wake up in the morning and you go to school and

01:28:07.000 --> 01:28:12.680
it's not because you necessarily like waking up early and going to school,

01:28:12.680 --> 01:28:15.800
but you know that there is a long-term objective you're trying to optimize.

01:28:15.800 --> 01:28:19.800
So Ernest Becker, I'm not sure if you're familiar with the philosopher who wrote the

01:28:19.800 --> 01:28:24.680
book Denial of Death. And his idea is that one of the core motivations of human beings is our

01:28:24.680 --> 01:28:30.440
terror of death, our fear of death. That's what makes us unique from cats. Cats are just surviving.

01:28:30.440 --> 01:28:41.000
They do not have a deep, like cognizance introspection that over the horizon is the end.

01:28:41.720 --> 01:28:45.640
And he says that, I mean, there's a terror management theory that just all these psychological

01:28:45.640 --> 01:28:54.280
experiments that show basically this idea that all of human civilization, everything we create,

01:28:54.280 --> 01:28:59.640
is kind of trying to forget if even for a brief moment that we're going to die.

01:29:00.840 --> 01:29:07.160
When do you think humans understand that they're going to die? Is it learned early on also?

01:29:08.920 --> 01:29:14.040
I don't know at what point. I mean, it's a question like, at what point do you realize

01:29:14.040 --> 01:29:18.760
that what death really is? And I think most people don't actually realize what death is,

01:29:18.760 --> 01:29:21.800
right? I mean, most people believe that you go to heaven or something, right?

01:29:21.880 --> 01:29:28.600
So to push back on that, what Ernest Becker says and Sheldon Solomon, all of those folks,

01:29:29.240 --> 01:29:34.040
and I find those ideas a little bit compelling is that there is moments in life, early in life.

01:29:34.040 --> 01:29:42.120
A lot of this fun happens early in life when you are, when you do deeply experience the terror

01:29:42.120 --> 01:29:47.080
of this realization and all the things you think about about religion, all those kinds of things

01:29:47.080 --> 01:29:51.960
that we kind of think about more like teenage years and later. We're talking about way earlier.

01:29:51.960 --> 01:29:54.120
No, there's like seven or eight years or something like that. Yeah.

01:29:54.120 --> 01:30:02.760
You realize, holy crap, this is like the mystery, the terror, like it's almost like you're a little

01:30:02.760 --> 01:30:06.440
prey, a little baby deer sitting in the darkness of the jungle of the woods,

01:30:07.000 --> 01:30:11.480
looking all around you, there's darkness full of terror. I mean, that's, that realization says,

01:30:11.480 --> 01:30:16.280
okay, I'm going to go back in the comfort of my mind where there is a, where there is a deep

01:30:16.280 --> 01:30:23.800
meaning where there is a, maybe like pretend I'm immortal and however way, however kind of idea I

01:30:23.800 --> 01:30:29.080
can construct to help me understand that I'm immortal. Religion helps with that. You can,

01:30:29.080 --> 01:30:34.120
you can delude yourself in all kinds of ways, like lose yourself in the busyness of each day,

01:30:34.120 --> 01:30:38.120
have little goals in mind, all those kinds of things to think that it's going to go on forever.

01:30:38.120 --> 01:30:43.080
And you kind of know you're going to die. Yeah. And it's going to be sad, but you don't really

01:30:43.160 --> 01:30:47.800
understand that you're going to die. And so that's, that's their idea. And I find that compelling

01:30:48.520 --> 01:30:54.520
because it does seem to be a core unique aspect of human nature that we were able to think that

01:30:54.520 --> 01:31:00.520
we're going, we're able to really understand that this life is finite. That seems important.

01:31:00.520 --> 01:31:03.560
There's, there's a bunch of different things there. So first of all, I don't think there is

01:31:03.560 --> 01:31:08.600
a qualitative difference between, between us and cats in the term. I think the difference is that

01:31:08.600 --> 01:31:15.480
we just have a better long term ability to predict, you know, in the long term. And so

01:31:15.480 --> 01:31:18.680
we have a better understanding of other world works. So we have better understanding of,

01:31:18.680 --> 01:31:23.480
you know, funniness of life and things like that. Do we have a better planning engine than cats?

01:31:23.480 --> 01:31:29.720
Yeah. Okay. But what's the motivation for planning that far? Well, I think it's just a side

01:31:29.720 --> 01:31:34.360
effect to the fact that we have just a better planning engine because it makes us, as I said,

01:31:34.360 --> 01:31:39.240
you know, the essence of intelligence is the ability to predict. And so the, because we're

01:31:39.240 --> 01:31:43.880
smarter, as a side effect, we also have this ability to kind of make predictions about our own

01:31:45.080 --> 01:31:51.080
future existence or lack the rough. You say religion helps with that. I think religion

01:31:51.080 --> 01:31:55.800
hurts actually. It makes people worry about like, you know, what's going to happen after

01:31:55.800 --> 01:32:01.080
their death, etc. If you believe that, you know, you just don't exist after death, like, you know,

01:32:01.080 --> 01:32:04.840
it solves completely the problem at least. You're saying if you don't believe in God,

01:32:04.840 --> 01:32:09.960
you don't worry about what happens after death? Yeah. I don't know. You only worry about the,

01:32:09.960 --> 01:32:13.000
about, you know, this life, because that's the only one you have.

01:32:14.120 --> 01:32:17.640
I think it's, well, I don't, I don't know, if I were to say what Ernest Becker says,

01:32:17.640 --> 01:32:27.080
and I'll say I agree with him more than not is you do deeply worry. If you, if you believe

01:32:27.080 --> 01:32:32.600
there's no God, there's still a deep worry like of the mystery of it all. Like, how does that make

01:32:32.600 --> 01:32:39.880
any sense that it just ends? I don't think we can truly understand that this right. I mean,

01:32:39.880 --> 01:32:46.520
so much of our life, the consciousness, the ego is invested in this, in this being. And then

01:32:47.560 --> 01:32:53.080
science keeps bringing humanity down from its pedestal. And that's, that's just another,

01:32:53.800 --> 01:32:58.520
example of it. That's wonderful. But for us individual humans, we don't like to be

01:32:58.520 --> 01:33:04.040
brought down from a pedestal. You're saying like, but see, you're fine with it because, well,

01:33:04.040 --> 01:33:07.720
so what Ernest Becker would say is you're fine with it because that's just a more peaceful

01:33:07.720 --> 01:33:11.880
existence for you, but you're not really fine. You're hiding from, in fact, some of the people

01:33:11.880 --> 01:33:19.000
that experienced the deepest trauma that earlier in life, they often, before they seek extensive

01:33:19.080 --> 01:33:23.400
therapy will say that I'm fine. It's like, when you talk to people who are truly angry,

01:33:23.400 --> 01:33:27.080
how are you doing? I'm fine. The question is, what's going on?

01:33:27.640 --> 01:33:33.800
Now I had a near death experience. I had a very bad motorbike accident when I was 17. So,

01:33:34.920 --> 01:33:40.360
but that didn't have any impact on my reflection on that topic.

01:33:40.360 --> 01:33:45.000
So I'm basically just playing a bit of devil's advocate and pushing back and wondering,

01:33:45.720 --> 01:33:49.320
is it truly possible to accept death? And the flip side that's more interesting,

01:33:49.320 --> 01:33:57.080
I think, for AI and robotics is how important is it to have this as one of the suite of motivations

01:33:57.080 --> 01:34:09.000
is to not just avoid falling off the roof or something like that, but ponder the end of the ride.

01:34:09.720 --> 01:34:16.760
Well, if you listen to the Stoics, it's a great motivator. It adds a sense of urgency.

01:34:16.760 --> 01:34:22.200
So it might be to truly fear death or be cognizant of it might give

01:34:23.640 --> 01:34:27.960
a deeper meaning and urgency to the moment to live fully.

01:34:30.440 --> 01:34:34.440
Maybe I don't disagree with that. I mean, I think what motivates me here is

01:34:34.680 --> 01:34:41.960
knowing more about human nature. I mean, I think human nature and human intelligence is a big

01:34:41.960 --> 01:34:50.120
mystery. It's a scientific mystery in addition to philosophical and etc. But I'm a true believer

01:34:50.120 --> 01:34:58.840
in science. And I do have kind of a belief that for complex systems like the brain and the mind,

01:34:59.800 --> 01:35:07.320
the way to understand it is to try to reproduce it with artifacts that you built. Because you

01:35:07.320 --> 01:35:12.280
know what's essential to it when you try to build it. The same way, I've used this analogy before

01:35:12.280 --> 01:35:18.760
with you, I believe, the same way we only started to understand aerodynamics when we started building

01:35:18.760 --> 01:35:25.400
airplanes. And that helped us understand how birds fly. So I think there's a similar process here

01:35:25.400 --> 01:35:31.640
where we don't have a theory of a full theory of intelligence. But building intelligent artifacts

01:35:31.640 --> 01:35:38.680
will help us perhaps develop some underlying theory that encompasses not just artificial

01:35:38.680 --> 01:35:43.720
implements, but also human and biological intelligence in general.

01:35:43.720 --> 01:35:48.760
So you're an interesting person to ask this question about sort of all kinds of different other

01:35:49.400 --> 01:35:56.280
intelligent entities or intelligences. What are your thoughts about kind of like the touring or

01:35:56.280 --> 01:36:04.120
the Chinese room question? If we create an AI system that exhibits a lot of properties of

01:36:04.120 --> 01:36:11.160
intelligence and consciousness, how comfortable are you thinking of that entity as intelligent or

01:36:11.160 --> 01:36:16.520
conscious? So you're trying to build now systems that have intelligence and there's metrics about

01:36:16.520 --> 01:36:26.280
their performance. But that metric is external. So are you okay calling a thing intelligent?

01:36:26.280 --> 01:36:32.840
Or are you going to be like most humans and be once again unhappy to be brought down from a

01:36:32.840 --> 01:36:42.200
pedestal of consciousness slash intelligence? No, I'll be very happy to understand more about

01:36:42.200 --> 01:36:47.400
human nature, human mind and human intelligence through the construction of machines that

01:36:48.840 --> 01:36:55.480
have similar abilities. And if a consequence of this is to bring down humanity one notch down from

01:36:56.200 --> 01:37:01.960
its already low pedestal, I'm just fine with it. That's just a reality of life. So I'm fine with

01:37:01.960 --> 01:37:06.920
that. Now you were asking me about things that opinions I have that a lot of people may disagree

01:37:06.920 --> 01:37:15.160
with. I think if we think about the design of an autonomous intelligence system, so assuming that

01:37:15.160 --> 01:37:20.280
we are somewhat successful at some level of getting machines to learn models of the world,

01:37:20.280 --> 01:37:26.440
predictive models of the world, we build intrinsic motivation, objective functions to drive the

01:37:26.440 --> 01:37:31.480
behavior of that system. The system also has perception modules that allows it to estimate

01:37:31.480 --> 01:37:36.200
the state of the world and then have some way of figuring out a sequence of actions that to

01:37:36.280 --> 01:37:42.600
optimize a particular objective. If it has a critic of the type that was describing before,

01:37:42.600 --> 01:37:45.800
the thing that makes recoil your arm the second time I try to pinch you,

01:37:48.440 --> 01:37:54.200
intelligent autonomous machine will have emotions. I think emotions are an integral part of

01:37:54.200 --> 01:38:01.560
autonomous intelligence. If you have an intelligence system that is driven by intrinsic

01:38:01.560 --> 01:38:07.960
motivation, by objectives, if it has a critic that allows it to predict in advance whether the

01:38:07.960 --> 01:38:12.600
outcome of a situation is going to be good or bad, it's going to have emotions. It's going to have

01:38:12.600 --> 01:38:20.120
fear when it predicts that the outcome is going to be bad and something to avoid is going to have

01:38:20.120 --> 01:38:28.840
elation when it predicts it's going to be good. If it has drives to relate with humans in some

01:38:28.920 --> 01:38:36.680
ways, the way humans have, it's going to be social. It's going to have emotions about

01:38:36.680 --> 01:38:46.840
attachment and things of that type. I think the sci-fi thing where you see commander data

01:38:46.840 --> 01:38:51.080
like having an emotion chip that you can turn off, I think that's ridiculous.

01:38:51.480 --> 01:38:59.000
So, here's the difficult philosophical social question. Do you think there will be a time

01:38:59.880 --> 01:39:06.360
like a civil rights movement for robots where, okay, forget the movement, but a discussion

01:39:06.360 --> 01:39:14.520
like the Supreme Court that particular kinds of robots, particular kinds of systems,

01:39:15.480 --> 01:39:21.320
deserve the same rights as humans because they can suffer just as humans can,

01:39:22.840 --> 01:39:30.040
all those kinds of things? Well, perhaps not. Imagine that humans were that you could

01:39:32.200 --> 01:39:39.400
die and be restored. You could be 3D reprinted and your brain could be reconstructed in its

01:39:39.400 --> 01:39:46.760
finest details. Our ideas of rights will change in that case. If there's always a backup,

01:39:46.760 --> 01:39:51.880
you could always restore. Maybe the importance of murder will go down one notch.

01:39:51.880 --> 01:40:00.520
That's right. But also, your desire to do dangerous things like skydiving or

01:40:00.680 --> 01:40:11.160
race car racing or that kind of stuff would probably increase or airplane aerobatics or

01:40:11.160 --> 01:40:16.760
that kind of stuff. It would be fine to do a lot of those things or explore dangerous areas and

01:40:16.760 --> 01:40:21.960
things like that. It would change your relationship. Now, it's very likely that robots would be like

01:40:21.960 --> 01:40:27.880
that because they'll be based on perhaps technology that is somewhat similar to

01:40:28.760 --> 01:40:35.320
this technology and you can always have a backup. So, it's possible. I don't know if you like video

01:40:35.320 --> 01:40:44.520
games, but there's a game called Diablo. My sons are huge fans of this. Yes. In fact,

01:40:44.520 --> 01:40:51.080
they made a game that's inspired by it. Awesome. Like built a game? My three sons have a game design

01:40:51.080 --> 01:40:55.080
studio between them. That's awesome. They came out with a game. They just came out of the game.

01:40:55.080 --> 01:40:58.040
Last year? No, this was last year, about a year ago.

01:40:58.040 --> 01:41:03.320
That's awesome. But in Diablo, there's something called hardcore mode, which if you die,

01:41:03.320 --> 01:41:09.240
there's no, you're gone. That's it. And so, it's possible with AI systems

01:41:10.600 --> 01:41:15.800
for them to be able to operate successfully and for us to treat them in a certain way because

01:41:15.800 --> 01:41:21.960
they have to be integrated in human society, they have to be able to die no copies allowed.

01:41:21.960 --> 01:41:26.920
In fact, copying is illegal. It's possible with humans as well, like cloning will be illegal,

01:41:26.920 --> 01:41:30.040
even when it's possible. But cloning is not copying, right? I mean,

01:41:30.040 --> 01:41:35.400
you don't reproduce the mind of the person and experience. It's just a delayed twin.

01:41:36.280 --> 01:41:41.400
But then, we were talking about with computers that you'll be able to copy. You'll be able to

01:41:41.400 --> 01:41:50.040
perfectly save, pickle the mind state. And it's possible that that would be illegal because

01:41:50.840 --> 01:41:58.840
that will destroy the motivation of the system. Okay, so let's say you have a domestic robot,

01:42:00.200 --> 01:42:07.240
sometime in the future. And the domestic robot comes to you somewhat pre-trained,

01:42:07.240 --> 01:42:11.400
can do a bunch of things. But it has a particular personality that makes it slightly different

01:42:11.400 --> 01:42:16.360
from the other robots because that makes them more interesting. And then because it's lived

01:42:16.360 --> 01:42:22.840
with you for five years, you've grown some attachment to it and vice versa. And it's learned

01:42:22.840 --> 01:42:28.920
a lot about you. Or maybe it's not a household robot, maybe it's a virtual assistant that lives in

01:42:28.920 --> 01:42:34.760
your augmented reality glasses or whatever, right? The HER movie type thing, right?

01:42:36.600 --> 01:42:44.280
And that system to some extent, the intelligence in that system is a bit like your child or maybe

01:42:44.280 --> 01:42:50.520
your PhD student in the sense that there's a lot of you in that machine now, right? And so,

01:42:51.080 --> 01:42:57.240
if it were a living thing, you would do this for free if you want, right? If it's your child,

01:42:57.240 --> 01:43:03.880
your child can then live his or her own life. And the fact that they learn stuff from you

01:43:03.880 --> 01:43:09.240
doesn't mean that you have any ownership of it, right? But if it's a robot that you've trained,

01:43:09.240 --> 01:43:13.800
perhaps you have some intellectual property claim about...

01:43:13.800 --> 01:43:19.080
Going to intellectual property. Oh, I thought you meant like permanent value in the sense that's part

01:43:19.080 --> 01:43:23.720
of you is in... Well, there is permanent value, right? So you would lose a lot if that robot

01:43:23.720 --> 01:43:27.800
were to be destroyed and you had no backup, you would lose a lot. You would lose a lot of investment,

01:43:27.800 --> 01:43:35.640
you know, kind of like a person dying, you know, that a friend of yours dying or a co-worker or

01:43:35.640 --> 01:43:45.000
something like that. But also you have intellectual property rights in the sense that that system

01:43:45.000 --> 01:43:50.600
is fine-tuned to your particular existence. So that's now a very unique instantiation of that

01:43:50.600 --> 01:43:55.400
original background model, whatever it was that arrived. And then there are issues of privacy,

01:43:55.400 --> 01:44:01.480
right? Because now imagine that that robot has its own kind of volition and decides to work

01:44:01.480 --> 01:44:07.640
for someone else or kind of thinks life with you is sort of untenable or whatever.

01:44:09.560 --> 01:44:12.280
Now, all the things that that system learned from you,

01:44:14.520 --> 01:44:18.520
you know, how can you like, you know, delete all the personal information that that system

01:44:18.520 --> 01:44:22.440
knows about you? Yeah. I mean, that would be kind of an ethical question. Like, you know,

01:44:22.440 --> 01:44:30.520
can you erase the mind of an intelligent robot to protect your privacy? You can't do this with

01:44:30.600 --> 01:44:35.560
humans. You can ask them to shut up, but that you don't have complete power over them.

01:44:35.560 --> 01:44:40.040
Can't erase humans. Yeah, it's the problem with the relationships, you know, that you break up,

01:44:40.040 --> 01:44:44.840
you can't you can't erase the other human with robots. I think it will have to be the same thing

01:44:44.840 --> 01:44:54.440
with robots that that risk that there has to be some risk to our interactions to truly experience

01:44:54.440 --> 01:45:00.520
them deeply, it feels like. So you have to be able to lose your robot friend. And that robot

01:45:00.520 --> 01:45:06.120
friend to go tweeting about how much of an asshole you are. But then are you allowed to, you know,

01:45:06.120 --> 01:45:10.680
murder the robot to protect your private information? Yeah, probably not. I have this

01:45:10.680 --> 01:45:17.560
intuition that for robots with with certain, like it's almost like a regulation, if you declare

01:45:17.560 --> 01:45:22.600
your robot to be, let's call it sentient or something like that, like this, this robot is

01:45:22.600 --> 01:45:26.680
designed for human interaction, then you're not allowed to murder these robots, it's the same as

01:45:26.680 --> 01:45:31.400
murdering other humans. Well, but what about you do a backup of the robot, you do preserve on the

01:45:31.400 --> 01:45:36.200
on a high drive or the equivalent in the future, that might be illegal, just like it's a piracy,

01:45:36.200 --> 01:45:41.480
piracy is illegal. But it's your own, it's your own robot, right? But you can't, you don't,

01:45:41.480 --> 01:45:46.840
but then but then you can wipe out his brain. So the this robot doesn't know anything about you

01:45:46.840 --> 01:45:51.560
anymore, but you still have technically is still in existence because you backed it up.

01:45:51.560 --> 01:45:56.600
And then there'll be these great speeches at the Supreme Court by saying, Oh, sure, you can erase

01:45:56.600 --> 01:46:01.080
the mind of the robot, just like you can erase the mind of a human, we both can suffer. There'll

01:46:01.080 --> 01:46:07.080
be some epic like Obama type character with a speech that we we like the robots and the humans

01:46:07.080 --> 01:46:14.200
are the same. We can both suffer, we can both hope, we can both all those all those kinds of

01:46:14.200 --> 01:46:19.720
things, raise families, all that kind of stuff. It's it's interesting for these just like you

01:46:19.720 --> 01:46:26.360
said, emotion seems to be a fascinatingly powerful aspect of human human interaction, human robot

01:46:26.360 --> 01:46:32.280
interaction. And if they're able to exhibit emotions at the end of the day, that's probably

01:46:32.280 --> 01:46:39.080
going to have us deeply consider human rights, like what we value in humans, what we value in

01:46:39.080 --> 01:46:44.840
other animals. That's why robots and AI is great. It makes us ask as the hard questions.

01:46:44.840 --> 01:46:49.240
Yeah. But you, I mean, you asked about you asked about the Chinese room type argument,

01:46:49.240 --> 01:46:53.160
you know, is it real? If it looks real? I think the Chinese room argument is the

01:46:53.160 --> 01:46:59.880
ridiculous one. So, so for people who don't know Chinese room is you can, I don't even know how

01:46:59.880 --> 01:47:06.200
to formulate it well, but basically, you can mimic the behavior of an intelligent system by just

01:47:06.200 --> 01:47:12.760
following a giant algorithm code book that tells you exactly how to respond in exactly each case.

01:47:12.760 --> 01:47:17.720
But is that really intelligent? It's like a giant lookup table. When this person says this,

01:47:17.720 --> 01:47:23.880
you answer this, when this person says this, you answer this. And if you understand how that

01:47:23.880 --> 01:47:28.760
works, you have this giant nearly infinite lookup table. Is that really intelligence? Because

01:47:28.760 --> 01:47:33.960
intelligence seems to be a mechanism that's much more interesting and complex than this lookup

01:47:33.960 --> 01:47:39.400
table. I don't think so. So the, I mean, the real question comes down to, do you think,

01:47:40.280 --> 01:47:46.680
you know, you can, you can mechanize intelligence in some way, even if that involves learning?

01:47:47.480 --> 01:47:52.040
And the answer is, of course, yes, there's no question. There's a second question then,

01:47:52.040 --> 01:47:58.360
which is assuming you can reproduce intelligence in sort of different hardware than biological

01:47:58.360 --> 01:48:06.680
hardware, you know, like computers. Can you, you know, match human intelligence in

01:48:07.160 --> 01:48:14.840
all the domains in which humans are intelligent? Is it possible, right? So this is quite the

01:48:14.840 --> 01:48:21.080
hypothesis of strong AI. The answer to this, in my opinion, is an unqualified yes. This would

01:48:21.080 --> 01:48:25.640
as well happen at some point. There's no question that machines at some point will become more

01:48:25.640 --> 01:48:30.040
intelligent than humans in all domains where humans are intelligent. This is not for tomorrow,

01:48:30.040 --> 01:48:36.360
it's going to take a long time, regardless of what, you know, Elon and others have claimed

01:48:36.440 --> 01:48:41.800
or believed. This is a lot, a lot harder than many of, many of those guys think it is.

01:48:43.240 --> 01:48:47.400
And many of those guys who thought it was simpler than that years, you know, five years ago,

01:48:47.400 --> 01:48:52.360
now think it's hard because it's been five years and they realize it's going to take a lot longer

01:48:53.240 --> 01:48:55.320
than includes a bunch of people at DeepMind, for example. But

01:48:55.960 --> 01:49:00.200
Oh, interesting. I haven't actually touched base with the DeepMind folks, but some of it,

01:49:00.200 --> 01:49:08.840
Elon or Dennis Sousa, I mean, sometimes in your role, you have to kind of create deadlines that

01:49:08.840 --> 01:49:13.880
are nearer than farther away to kind of create an urgency, because, you know, you have to believe

01:49:13.880 --> 01:49:18.120
the impossible is possible in order to accomplish it. And there's, of course, a flip side to that

01:49:18.120 --> 01:49:22.360
coin, but it's a weird, you can't be too cynical if you want to get something done.

01:49:22.360 --> 01:49:28.120
Absolutely. I agree with that. But I mean, you have to inspire people to work on sort of ambitious

01:49:28.120 --> 01:49:36.280
things. So, you know, it's certainly a lot harder than we believe, but there's no question in my

01:49:36.280 --> 01:49:40.360
mind that this will, this will happen. And now, you know, people are kind of worried about what

01:49:40.360 --> 01:49:45.640
does that mean for humans, they are going to be brought down from their pedestal, you know,

01:49:45.640 --> 01:49:51.800
a bunch of notches with that. And, you know, is that going to be good or bad? I mean,

01:49:51.800 --> 01:49:55.560
it's just going to give more power, right? It's an amplifier for human intelligence really.

01:49:56.120 --> 01:49:58.840
So speaking of doing cool, ambitious things,

01:49:59.800 --> 01:50:04.840
FAIR, the Facebook AI Research Group, has recently celebrated its eighth birthday.

01:50:05.400 --> 01:50:12.280
Or maybe you can correct me on that. Looking back, what has been the successes, the failures,

01:50:12.280 --> 01:50:16.440
the lessons learned from the eight years of FAIR? And maybe you can also give context of

01:50:16.440 --> 01:50:22.520
where does the newly minted meta AI fit into how does it relate to FAIR?

01:50:22.520 --> 01:50:24.920
Right. So let me tell you a little bit about the organization of all this.

01:50:26.600 --> 01:50:31.080
Yeah, FAIR was created almost exactly eight years ago. It wasn't called FAIR yet.

01:50:31.080 --> 01:50:38.440
It took that name a few months later. And at the time, I joined Facebook. There was a group

01:50:38.440 --> 01:50:44.680
called the AI Group that had about 12 engineers and a few scientists, like, you know, 10 engineers

01:50:44.680 --> 01:50:49.640
and two scientists or something like that. I ran it for three and a half years as a director,

01:50:50.520 --> 01:50:54.920
you know, hired the first few scientists and kind of set up the culture and organized it,

01:50:54.920 --> 01:51:00.680
you know, explained to the Facebook leadership what fundamental research was about and how it

01:51:00.680 --> 01:51:10.360
can work within industry and how it needs to be open and everything. And I think it's been an

01:51:10.360 --> 01:51:17.880
unqualified success in the sense that FAIR has simultaneously produced, you know,

01:51:17.960 --> 01:51:23.320
top level research and advanced the science and the technology provided tools, open source tools

01:51:23.320 --> 01:51:31.320
like PyTorch and many others. But at the same time as had a direct or mostly indirect impact

01:51:31.960 --> 01:51:40.360
on Facebook at the time, now meta, in the sense that a lot of systems that are that meta is built

01:51:40.360 --> 01:51:49.320
around now are based on research projects that started at FAIR. And so if you were to take out,

01:51:49.320 --> 01:51:56.360
you know, deep running out of Facebook services now and meta more generally, I mean, the company

01:51:56.360 --> 01:52:01.720
would literally crumble. I mean, it's completely built around AI these days. And it's really

01:52:01.720 --> 01:52:08.600
essential to the operations. So what happened after three and a half years is that I changed

01:52:08.600 --> 01:52:15.560
role, I became chief scientist. So I'm not doing day to day management of FAIR anymore. I'm more of a

01:52:15.560 --> 01:52:21.320
kind of, you know, think about strategy and things like that. And I carry my, I conduct my own research,

01:52:21.320 --> 01:52:24.600
I've, you know, my own kind of research group working on self supervised learning and things

01:52:24.600 --> 01:52:31.560
like this, which I didn't have time to do when I was director. So now FAIR is run by Joel Pinot

01:52:31.560 --> 01:52:37.160
and Antoine Baud together, because FAIR is kind of split into now there's something called FAIR

01:52:37.160 --> 01:52:42.920
Labs, which is sort of bottom up census driven research and FAIR Excel, which is slightly more

01:52:42.920 --> 01:52:48.360
organized for bigger projects that require a little more kind of focus and more engineering

01:52:48.360 --> 01:52:52.760
support and things like that. So Joel needs FAIR Lab and Antoine Baud needs FAIR Excel.

01:52:52.760 --> 01:53:00.200
Where are they located? It's delocalized all over. So there's no question that the leadership

01:53:01.160 --> 01:53:07.560
of the company believes that this was a very worthwhile investment. And what that means is that

01:53:10.120 --> 01:53:17.080
it's there for the long run, right? So there is, if you want to talk in these terms, which I don't

01:53:17.080 --> 01:53:22.920
like, there's a business model, if you want, where FAIR, despite being a very fundamental

01:53:22.920 --> 01:53:27.560
research lab brings a lot of value to the company, either mostly indirectly through other groups.

01:53:27.560 --> 01:53:33.800
Now, what happened three and a half years ago when I stepped down was also the creation of

01:53:33.800 --> 01:53:40.600
Facebook AI, which was basically a larger organization that covers FAIR. So FAIR is

01:53:40.600 --> 01:53:48.440
included in it, but also has other organizations that are focused on applied research or advanced

01:53:48.440 --> 01:53:54.520
development of AI technology that is more focused on the products of the company.

01:53:54.520 --> 01:53:56.520
So less emphasis on fundamental research?

01:53:56.600 --> 01:53:59.720
Less fundamental, but it's still a research. I mean, there's a lot of papers coming out of

01:53:59.720 --> 01:54:08.120
those organizations and people are awesome and wonderful to interact with. But it serves as

01:54:10.280 --> 01:54:19.320
a way to scale up, if you want, AI technology, which may be very experimental and lab prototypes

01:54:19.320 --> 01:54:24.760
in two things that are usable. So FAIR is a subset of meta AI. If FAIR becomes like KFC,

01:54:25.080 --> 01:54:28.280
it'll just keep the F. Nobody cares what the F stands for.

01:54:29.400 --> 01:54:35.400
Will knows soon enough by probably by the end of 2021.

01:54:35.400 --> 01:54:38.200
This is not a giant change, FAIR.

01:54:38.200 --> 01:54:43.400
Well, FAIR doesn't sound too good. But the brand people are kind of deciding on this,

01:54:43.400 --> 01:54:47.880
and they've been hesitating for a while now, and they tell us they're going to come up with

01:54:47.880 --> 01:54:51.560
an answer as to whether FAIR is going to change name or whether we're going to change just the

01:54:51.560 --> 01:54:56.040
meaning of the F. Oh, that's a good call. I will keep FAIR and change the meaning of the F.

01:54:56.040 --> 01:54:59.800
That would be my preference. I would turn the F into fundamental.

01:55:00.840 --> 01:55:02.200
Oh, that's good. Fundamental AI research.

01:55:02.200 --> 01:55:03.000
Oh, that's really good. Yeah.

01:55:03.000 --> 01:55:08.200
Within meta AI. So this would be meta FAIR, but people will call it FAIR, right?

01:55:08.200 --> 01:55:09.720
Yeah, exactly. I like it.

01:55:09.720 --> 01:55:20.920
And now meta AI is part of the reality lab. So meta now, the new Facebook

01:55:20.920 --> 01:55:28.360
where it's called meta, and it's kind of divided into Facebook, Instagram, WhatsApp,

01:55:30.360 --> 01:55:39.800
and reality lab. And reality lab is about AR, VR, telepresence, communication technology,

01:55:39.800 --> 01:55:45.000
and stuff like that. It's kind of the, you can think of it as the sort of a combination of

01:55:45.720 --> 01:55:51.880
sort of new products and technology part of meta.

01:55:51.880 --> 01:55:56.040
Is that where the touch sensing for robots? I saw that you were posting about that.

01:55:56.040 --> 01:55:58.840
Touch sensing for robots is part of FAIR, actually. That's a fact.

01:55:58.840 --> 01:55:59.880
Oh, it is. Okay, cool.

01:55:59.880 --> 01:56:05.560
Yeah. There's also the, no, but there is the other way, the haptic glove, right?

01:56:05.560 --> 01:56:07.560
Yes. That's more reality lab.

01:56:07.560 --> 01:56:09.880
That's reality lab research.

01:56:09.880 --> 01:56:13.640
Reality lab research. But by the way, the touch sense is super interesting,

01:56:14.280 --> 01:56:20.040
like integrating that modality into the whole sensing suite is very interesting.

01:56:20.040 --> 01:56:24.600
So what do you think about the metaverse? What do you think about this whole,

01:56:25.960 --> 01:56:30.680
this whole kind of expansion of the view of the role of Facebook and meta in the world?

01:56:30.680 --> 01:56:35.240
Well, metaverse really should be thought of as the next step in the internet, right?

01:56:35.240 --> 01:56:46.520
Sort of trying to kind of make the experience more compelling of being connected either with

01:56:46.520 --> 01:56:55.640
other people or with content. And we are evolved and trained to evolve in 3D environments where

01:56:57.160 --> 01:57:00.920
we can see other people, we can talk to them when you're near them,

01:57:01.000 --> 01:57:05.000
or, you know, and other people are far away, can hear us, you know, things like that, right?

01:57:05.000 --> 01:57:09.960
So it, it, there's a lot of social conventions that exist in the real world that we can try to

01:57:09.960 --> 01:57:16.200
transpose. Now, what is going to be eventually the, the, how compelling is it going to be?

01:57:16.200 --> 01:57:19.800
Like our, you know, is it going to be the case that people are going to be willing to

01:57:20.760 --> 01:57:25.160
do this if they have to wear, you know, a huge pair of goggles all day? Maybe not.

01:57:26.280 --> 01:57:30.200
But then again, if the experience is sufficiently compelling, maybe so.

01:57:30.200 --> 01:57:34.440
Or if the device that you have to wear is just basically a pair of glasses, you know,

01:57:34.440 --> 01:57:41.480
technology makes sufficient progress for that. You know, AR is a much easier concept to grasp

01:57:41.480 --> 01:57:47.000
that you're going to have, you know, augmented reality glasses that basically contain some sort

01:57:47.000 --> 01:57:50.120
of, you know, virtual assistant that can help you in your daily lives.

01:57:50.120 --> 01:57:54.120
But at the same time with the AR, you have to contend with reality. With VR, you can

01:57:54.120 --> 01:57:58.440
completely detach yourself from reality. So it gives you freedom. It might be easier to design

01:57:58.440 --> 01:58:05.960
worlds in VR. Yeah, but you, you can imagine how, you know, the metaverse being a mix, a mix,

01:58:05.960 --> 01:58:09.800
right? Or, or like you can have objects that exist in a metaverse that, you know,

01:58:09.800 --> 01:58:13.720
pop up on top of the real world or only exist in virtual reality.

01:58:14.280 --> 01:58:19.160
Okay, let me ask the hard question. Because all of this was easy. This was easy.

01:58:20.600 --> 01:58:27.400
The Facebook now meta, the social network has been painted by the media as net negative for

01:58:27.400 --> 01:58:33.480
society, even destructive and evil at times. You've pushed back against this defending Facebook.

01:58:34.040 --> 01:58:39.960
Can you explain your defense? Yeah, so the, the description, the company that is being described

01:58:39.960 --> 01:58:48.360
in the, in some media is not the company we know when we work inside. And, you know,

01:58:49.320 --> 01:58:54.520
it could be claimed that a lot of employees are uninformed about what really goes on in the company.

01:58:54.520 --> 01:58:58.680
But, you know, I'm a vice president. I mean, I have a pretty good vision of what goes on. You

01:58:58.680 --> 01:59:02.920
know, I don't know everything, obviously, I'm not involved in, in, in everything, but certainly

01:59:02.920 --> 01:59:07.000
not in decision about like, you know, content moderation or anything like this. But, but I

01:59:07.000 --> 01:59:12.840
have some decent vision of what goes on. And this evil that is being described, I just don't see it.

01:59:13.560 --> 01:59:20.520
And then, you know, I think there is an easy story to buy, which is that, you know, all the bad

01:59:20.520 --> 01:59:24.760
things in the, in the world and, you know, the, the reason your friend believe crazy stuff,

01:59:26.600 --> 01:59:32.680
you know, there's an easy scapegoat, right, in the, in, in, in social media, in general,

01:59:32.680 --> 01:59:38.840
Facebook in particular, we have to look at the data, like, is it the case that Facebook, for

01:59:38.840 --> 01:59:45.880
example, polarizes people politically? Are there academic studies that show this? Is it the case

01:59:45.880 --> 01:59:53.080
that, you know, teenagers think of themselves less if they use Instagram more? Is it the case that,

01:59:54.520 --> 02:00:01.320
you know, people get more riled up against, you know, opposite sides in a, in a debate or

02:00:01.320 --> 02:00:07.400
political opinion, if they, if they are more on Facebook, or if they are less. And study after

02:00:07.400 --> 02:00:12.600
study show that none of this is true. This is independent studies by academic, they're not

02:00:12.600 --> 02:00:18.200
funded by Facebook or Meta, you know, studied by Stanford, by some of my colleagues at NYU,

02:00:18.200 --> 02:00:23.320
actually, with whom I have no connection. You know, there's a study recently, they, they,

02:00:23.320 --> 02:00:30.760
they paid people, I think it was in, in, in, in the former Yugoslavia, I'm not exactly sure in

02:00:30.760 --> 02:00:39.400
what, what part, but they paid people to not use Facebook for a while in the period before the

02:00:39.400 --> 02:00:45.480
anniversary of the cybernature massacres, right? So, you know, people get riled up like, you know,

02:00:45.480 --> 02:00:51.400
should we have a celebration? I mean, a memorial kind of celebration for it or not. So they paid

02:00:51.400 --> 02:00:59.320
a bunch of people to not use Facebook for a few weeks. And it turns out that those people ended

02:00:59.320 --> 02:01:03.720
up being more polarized than they were at the beginning. And the people who were more on Facebook

02:01:03.720 --> 02:01:10.600
were less polarized. There's a study, you know, from Stanford of economics at Stanford that

02:01:11.240 --> 02:01:16.920
tried to identify the causes of increasing polarization in the US. And it's been going

02:01:16.920 --> 02:01:25.080
on for 40 years before, you know, Mark Zuckerberg was born continuously. And, and so if there is

02:01:25.080 --> 02:01:29.480
a cause, it's not Facebook or social media. So you could say if social media just accelerated,

02:01:29.480 --> 02:01:34.920
but no, I mean, it's basically a continuous evolution by some measure of polarization in the

02:01:34.920 --> 02:01:42.040
US. And then you compare this with other countries like the West half of Germany, because you can

02:01:42.040 --> 02:01:49.320
go 40 years in the East side, or Denmark or other countries. And they use Facebook just as much.

02:01:49.320 --> 02:01:53.320
And they're not getting more polarized, they're getting less polarized. So if you want to look for,

02:01:53.400 --> 02:01:59.880
you know, a causal relationship there, you can find a scapegoat, but you can't find a cause. Now,

02:01:59.880 --> 02:02:05.080
if you want to fix the problem, you have to find the right cause. And what rise me up is that people

02:02:05.080 --> 02:02:10.440
now are accusing Facebook of bad deeds that are done by others. And those others are, we're not

02:02:10.440 --> 02:02:15.560
doing anything about them. And by the way, those others include the owner of the Wall Street Journal

02:02:15.560 --> 02:02:20.040
in which all of those papers were published. So I should mention that I'm talking to Shrep,

02:02:20.040 --> 02:02:24.520
Mike Shrep on this podcast, and also Mark Zuckerberg, and probably these are the conversations

02:02:24.520 --> 02:02:29.880
you can have with them. Because it's very interesting to me, even if Facebook has some

02:02:29.880 --> 02:02:34.680
measurable negative effect, you can't just consider that in isolation, you have to consider about

02:02:34.680 --> 02:02:40.600
all the positive ways it connects us. So like every technology, you can't just say like,

02:02:41.720 --> 02:02:46.840
there's an increase in division. Yes, probably Google search engine has created

02:02:46.920 --> 02:02:51.080
increase in division, we have to consider about how much information are brought to the world.

02:02:51.080 --> 02:02:55.160
Like, I'm sure Wikipedia created more division, if you just look at the division,

02:02:55.160 --> 02:02:59.000
we have to look at the full context of the world and it didn't make a better world.

02:02:59.000 --> 02:03:02.040
I mean, the printing press has created more difference, right? Exactly.

02:03:03.000 --> 02:03:09.720
So, you know, when the printing press was invented, the first books that were printed were

02:03:09.720 --> 02:03:13.720
things like the Bible, and that allowed people to read the Bible by themselves,

02:03:13.720 --> 02:03:20.360
not get the message uniquely from priests in Europe, and that created the Protestant movement

02:03:20.360 --> 02:03:25.640
and 200 years of religious persecution and wars. So, that's a bad side effect of the printing

02:03:25.640 --> 02:03:29.800
press. Social networks aren't being nearly as bad as the printing press, but nobody would

02:03:29.800 --> 02:03:36.040
say the printing press was a bad idea. Yeah, a lot of this perception, and there's a lot of

02:03:36.040 --> 02:03:41.560
different incentives operating here. Maybe a quick comment, since you're one of the top

02:03:41.560 --> 02:03:48.440
leaders at Facebook and at Meta, sorry, that's in the tech space, I'm sure Facebook involves

02:03:48.440 --> 02:03:53.640
a lot of incredible technological challenges that need to be solved. A lot of it probably

02:03:53.640 --> 02:03:59.480
is in the computer infrastructure, the hardware, I mean, it's just a huge amount. Maybe can you

02:03:59.480 --> 02:04:06.120
give me context about how much of Shrep's life is AI and how much of it is low-level compute,

02:04:06.120 --> 02:04:11.960
how much of it is flying all around doing business stuff, and the same with Mark Zuckerberg?

02:04:11.960 --> 02:04:21.400
They really focus on AI. I mean, certainly in the run-up of the Creation Affair, and for at

02:04:21.400 --> 02:04:27.400
least a year after that, if not more, Mark was very, very much focused on AI and was spending

02:04:27.400 --> 02:04:31.960
quite a lot of effort on it, and that's his style. When he gets interested in something,

02:04:31.960 --> 02:04:36.200
he reads everything about it. He read some of my papers, for example, before I joined.

02:04:39.560 --> 02:04:42.360
And so he learns a lot about it.

02:04:46.360 --> 02:04:52.200
And Shrep was really to it also. I mean, Shrep is really kind of,

02:04:54.680 --> 02:05:00.040
has something I've tried to preserve also, despite my not so young age,

02:05:00.040 --> 02:05:05.000
which is a sense of wonder about science and technology, and he certainly has that.

02:05:06.120 --> 02:05:11.400
He's also a wonderful person. I mean, in terms of, as a manager, like dealing with people and

02:05:11.400 --> 02:05:19.400
everything, Mark also actually. I mean, they're very human people. In the case of Mark, it's

02:05:19.400 --> 02:05:27.240
shockingly human, given his trajectory. I mean, the personality of him that he's

02:05:27.240 --> 02:05:31.320
spending in the press, it's just completely wrong. Yeah. But you have to know how to play

02:05:31.320 --> 02:05:36.760
the press. So that's, I put some of that responsibility on him, too. You have to,

02:05:38.760 --> 02:05:44.920
it's like, you know, like the director, the conductor of an orchestra, you have to play

02:05:44.920 --> 02:05:49.720
the press and the public in a certain kind of way, where you convey your true self to them,

02:05:49.720 --> 02:05:54.440
if there's a depth of kindness. And it's probably not the best at it. So, yeah.

02:05:55.160 --> 02:06:02.440
You have to learn. And it's sad to see, I'll talk to him about it, but the Shrep is slowly

02:06:02.440 --> 02:06:08.680
stepping down. It's always sad to see folks sort of be there for a long time and slowly,

02:06:09.400 --> 02:06:17.160
I guess time is sad. I think he's done the thing he said had to do and, you know, he's got, you know,

02:06:17.320 --> 02:06:25.960
you know, family priorities and stuff like that. And I understand, you know, after 13 years or

02:06:25.960 --> 02:06:32.760
something. It's been a good run. Which in Silicon Valley is basically a lifetime. Yeah. You know,

02:06:32.760 --> 02:06:37.240
because, you know, it's dog years. So, in Europe, the conference just wrapped up.

02:06:38.520 --> 02:06:43.320
Let me just go back to something else. You posted the paper you co-authored was rejected from

02:06:43.400 --> 02:06:54.520
Europe. As you said, proudly in quotes rejected. Can you describe this paper and like, what was

02:06:54.520 --> 02:07:00.440
the idea in it? And also, maybe this is a good opportunity to ask what are the pros and cons,

02:07:00.440 --> 02:07:04.840
what works and what doesn't about the review process. Yeah. Let me talk about the paper first.

02:07:04.840 --> 02:07:11.720
I'll talk about the review process afterwards. The paper is called Vicreg. So this is, I mentioned

02:07:11.720 --> 02:07:16.520
that before, variance in variance, covariance, regularization. And it's a technique, a non-

02:07:16.520 --> 02:07:22.040
contrastive learning technique for what I call joint embedding architecture. So,

02:07:22.040 --> 02:07:26.200
sami's nets are an example of joint embedding architecture. So joint embedding architecture is

02:07:29.240 --> 02:07:32.200
let me back up a little bit, right? So if you want to do self-supervised learning,

02:07:33.240 --> 02:07:38.680
you can do it by prediction. So let's say you want to train a system to predict video, right?

02:07:38.680 --> 02:07:44.360
You show it a video clip and you train the system to predict the next, the continuation of that

02:07:44.360 --> 02:07:48.920
video clip. Now, because you need to handle uncertainty, because there are many, you know,

02:07:48.920 --> 02:07:53.960
many continuations that are plausible, you need to have, you need to handle this in some way.

02:07:53.960 --> 02:08:01.480
You need to have a way for the system to be able to produce multiple predictions. And the way,

02:08:01.480 --> 02:08:06.360
the only way I know to do this is through what's called a latent variable. So you have some sort of

02:08:07.320 --> 02:08:12.920
hidden vector of a variable that you can vary over a set or draw from a distribution. And as you

02:08:12.920 --> 02:08:17.560
vary this vector over a set, the output, the prediction varies over a set of plausible predictions.

02:08:18.280 --> 02:08:22.840
Okay. So that's called, I call this a generative latent variable model.

02:08:23.960 --> 02:08:30.120
Got it. Okay. Now, there is an alternative to this to handle uncertainty. And instead of directly

02:08:30.200 --> 02:08:39.400
predicting the next frames of the clip, you also run those through another neural net.

02:08:40.920 --> 02:08:47.880
So you now have two neural nets, one that looks at the, you know, the initial segment of the video

02:08:47.880 --> 02:08:54.440
clip. And another one that looks at the continuation during training, right? And what you're trying

02:08:54.440 --> 02:09:01.400
to do is learn a representation of those two video clips that is maximally informative about

02:09:01.400 --> 02:09:07.880
the video clips themselves. But it's such that you can predict the representation of the second

02:09:07.880 --> 02:09:13.320
video clip from the representation of the first one easily. Okay. And you can sort of formalize

02:09:13.320 --> 02:09:16.680
this in terms of maximizing mutual information and some stuff like that, but it doesn't matter.

02:09:17.400 --> 02:09:24.360
What you want is informative, representative, you know, informative representations

02:09:24.360 --> 02:09:29.800
of the two video clips that are mutually predictable. What that means is that there's a

02:09:29.800 --> 02:09:37.800
lot of details in the second video clips that are irrelevant. You know, I, let's say a video clip

02:09:37.800 --> 02:09:43.000
consists in, you know, a camera panning the scene, there's going to be a piece of that

02:09:43.000 --> 02:09:47.320
room that is going to be revealed. And I can somewhat predict what the, what that room is going

02:09:47.320 --> 02:09:52.520
to look like, but I may not be able to predict the details of the texture of the ground and where

02:09:52.520 --> 02:09:57.240
the tiles are ending and stuff like that. Right. So those are irrelevant details that perhaps

02:09:57.240 --> 02:10:03.960
my representation will eliminate. And so what I need is to train this second neural net in such a

02:10:03.960 --> 02:10:11.880
way that whenever the continuation video clip varies over all the plausible continuations,

02:10:13.480 --> 02:10:19.240
the representation doesn't change. Got it. So it's the, yeah, yeah. Got it. Over the space of

02:10:19.240 --> 02:10:24.440
representations, doing the same kind of thing as you do with similarity learning. Right.

02:10:25.640 --> 02:10:30.680
So, so these are two ways to handle multimodality in a prediction, right? In the first way,

02:10:30.680 --> 02:10:35.160
you prioritize the prediction with a latent variable, but you predict pixels essentially,

02:10:35.160 --> 02:10:39.240
right? In the second one, you don't, you don't predict pixels. You predict an abstract

02:10:39.240 --> 02:10:44.040
representation of pixels. And you guarantee that this abstract representation has as much

02:10:44.040 --> 02:10:48.440
information as possible about the input, but sort of, you know, drops all the stuff that you really

02:10:48.440 --> 02:10:54.600
can't predict essentially. I used to be a big fan of the first approach. And in fact, in this paper

02:10:54.600 --> 02:10:59.160
with the Chen Mishra, this blog post, the dark matter intelligence, I was kind of advocating

02:10:59.160 --> 02:11:03.480
for this. And in the last year and a half, I've completely changed my mind. I'm now a big fan

02:11:03.560 --> 02:11:10.760
of the second one. And it's because of a small collection of algorithms that have been proposed

02:11:10.760 --> 02:11:18.520
over the last year and a half or so, two years to do this, including V Craig. It's predecessor

02:11:18.520 --> 02:11:24.200
called Barlow Twins, which I mentioned, a method from our friends at DeepMind could be YOL.

02:11:25.960 --> 02:11:30.360
And, and, and there's a bunch of others now that kind of work similarly. So they're all based on

02:11:30.440 --> 02:11:35.480
this idea of joint embedding. Some of them have an explicit criterion that is an approximation of

02:11:35.480 --> 02:11:39.960
mutual information. Some others are BOL work, but we don't really know why. And there's been like

02:11:39.960 --> 02:11:44.040
lots of theoretical papers about why BOL works. No, it's not that because we take it out and it

02:11:44.040 --> 02:11:49.320
still works. And, you know, blah, blah, blah. I mean, so there's like a big debate. But, but

02:11:49.320 --> 02:11:53.160
the important point is that we now have a collection of non-contrastive joint embedding

02:11:53.160 --> 02:11:58.200
methods, which I think is the best thing since sliced bread. So I'm super excited about this,

02:11:58.280 --> 02:12:05.160
because I think it's our best shot for techniques that would allow us to kind of build predictive

02:12:05.160 --> 02:12:09.800
work models. And at the same time, learn hierarchical representations of the world,

02:12:09.800 --> 02:12:13.400
where what matters about the world is preserved and what is irrelevant is eliminated.

02:12:14.520 --> 02:12:19.800
By the way, the representations that before and after is across in the space in a sequence of

02:12:19.800 --> 02:12:24.600
images, or is it for single images? It would be either for a single image for a sequence. It

02:12:24.600 --> 02:12:28.120
doesn't have to be images. This could be applied to text. This could be applied to just about any

02:12:28.120 --> 02:12:32.840
signal. I'm looking at, you know, I'm looking for methods that are generally applicable,

02:12:32.840 --> 02:12:37.480
that are not specific to, you know, one particular modality, you know, it could be audio or whatever.

02:12:37.480 --> 02:12:42.760
Got it. So what's the story behind this paper? This paper is what is describing one of the one

02:12:42.760 --> 02:12:48.200
such method? This is this Vicreg method. So this is co-authored. The first author is a student

02:12:48.200 --> 02:12:54.840
called Adrien Bard, who is a resident PhD student at Fer Paris. He's co-advised by me and Jean Ponce,

02:12:55.640 --> 02:13:00.440
who's a professor at Economa Supérieure, also a research director at INRIA.

02:13:01.480 --> 02:13:06.120
So this is a wonderful program in France where PhD students can basically do their PhD in

02:13:06.120 --> 02:13:14.360
industry. And that's kind of what's happening here. And this paper is a follow-up on this

02:13:14.360 --> 02:13:21.960
Balotuin paper by my former postdoc, now Stéphane Denis, with Lijing and Yorish Bontar and a bunch

02:13:21.960 --> 02:13:29.000
of other people from Fer. And one of the main criticism from reviewers is that Vicreg is not

02:13:29.000 --> 02:13:36.040
different enough from Balotuin's. But, you know, my impression is that it's, you know,

02:13:36.760 --> 02:13:42.520
Balotuin's with a few bugs fixed, essentially. And in the end, this is what people will use.

02:13:43.080 --> 02:13:48.840
Right. So, but, you know, I'm used to stuff that I submit being rejected forward.

02:13:48.840 --> 02:13:52.120
So it might be rejected and actually exceptionally well cited because people use it.

02:13:52.120 --> 02:13:54.360
Well, it's already cited like a bunch of times.

02:13:54.360 --> 02:14:00.120
So, I mean, the question is then to the deeper question about peer review and conferences.

02:14:00.120 --> 02:14:04.200
I mean, computer science is a field that's kind of unique that the conference is highly prized.

02:14:04.840 --> 02:14:06.040
That's one. Right.

02:14:06.040 --> 02:14:11.080
And it's interesting because the peer review process there is similar, I suppose, to journals,

02:14:11.080 --> 02:14:15.640
but it's accelerated significantly. Well, not significantly, but it goes fast.

02:14:16.360 --> 02:14:21.800
And it's a nice way to get stuff out quickly, to peer review quickly, go to present it quickly

02:14:21.800 --> 02:14:28.200
to the community. So, not quickly, but quicker. But nevertheless, it has many of the same flaws of

02:14:28.200 --> 02:14:32.680
peer review because it's a limited number of people look at it. There's bias and following,

02:14:32.680 --> 02:14:36.600
like that if you want to do new ideas, you're going to get pushed back.

02:14:37.960 --> 02:14:45.240
There's self-interested people that kind of can infer who submitted it and kind of, you know,

02:14:45.240 --> 02:14:49.080
be cranky about it, all that kind of stuff. Yeah. I mean, there's a lot of, you know,

02:14:49.080 --> 02:14:54.120
social phenomena there. There's one social phenomenon, which is that because the field

02:14:54.120 --> 02:14:59.560
has been growing exponentially, the vast majority of people in the field are extremely junior.

02:15:00.520 --> 02:15:05.480
So, as a consequence, and that's just a consequence of the field growing, right? So,

02:15:05.480 --> 02:15:10.360
as the number of, as the size of the field kind of starts saturating, you will have less of that

02:15:10.360 --> 02:15:17.240
problem of reviewers being very inexperienced. A consequence of this is that, you know,

02:15:18.280 --> 02:15:24.200
young reviewers, I mean, there's a phenomenon which is that reviewers try to make their life

02:15:24.200 --> 02:15:28.600
easy. And to make their life easy when reviewing a paper is very simple. You just have to find

02:15:28.600 --> 02:15:34.840
a flaw in the paper, right? So, basically, they see their task as finding flaws in papers. And

02:15:34.840 --> 02:15:43.560
most papers have flaws, even the good ones. So, it's easy to do that. Your job is easier as a

02:15:43.560 --> 02:15:51.000
reviewer if you just focus on this. But what's important is, like, is there a new idea in that

02:15:51.000 --> 02:15:56.120
paper that is likely to influence? It doesn't matter if the experiments are not that great,

02:15:56.120 --> 02:16:04.280
if the protocol is, you know, so things like that. As long as there is a worthy idea in it,

02:16:05.000 --> 02:16:10.600
that will influence the way people think about the problem. Even if they make it better, you know,

02:16:10.600 --> 02:16:17.640
eventually, I think that's really what makes a paper useful. And so, this combination of

02:16:18.200 --> 02:16:25.080
social phenomena creates a disease that has plagued, you know, other fields in the past,

02:16:25.080 --> 02:16:30.600
like speech recognition, where basically, you know, people chase numbers on benchmarks.

02:16:31.560 --> 02:16:37.320
And it's much easier to get a paper accepted if it brings an incremental improvement on a

02:16:38.120 --> 02:16:45.880
sort of mainstream, well-accepted method or problem. And those are, to me, boring papers.

02:16:45.880 --> 02:16:51.560
I mean, they're not useless, right? Because industry, you know, strives on those kind of progress.

02:16:52.200 --> 02:16:55.720
But they're not the ones that I'm interested in, in terms of, like, new concepts and new ideas. So,

02:16:56.360 --> 02:17:02.680
papers that are really trying to strike kind of new advances generally don't make it. Now,

02:17:02.680 --> 02:17:08.040
thankfully, we have archive. Archive, exactly. And then there's open review type of situations

02:17:08.040 --> 02:17:13.400
we use. And then, I mean, Twitter is a kind of open review. I'm a huge believer that review should

02:17:13.400 --> 02:17:19.080
be done by thousands of people, not two people. I agree. And so, archive, like, do you see a

02:17:19.080 --> 02:17:23.880
future where a lot of really strong papers, it's already the present, but a growing future where

02:17:23.880 --> 02:17:32.040
it'll just be archive. And you're presenting an ongoing continuous conference called Twitter

02:17:32.040 --> 02:17:39.560
slash the internet slash archive sanity. Andre just released a new version. So, just not, you know,

02:17:39.560 --> 02:17:45.000
not being so elitist about this particular gating. It's not a question of being elitist or not. It's

02:17:45.000 --> 02:17:53.560
a question of being basically recommendation and zero approvals for people who don't see themselves

02:17:53.560 --> 02:17:58.600
having the ability to do so by themselves, right? And so, it saves time, right? If you rely on other

02:17:58.600 --> 02:18:07.000
people's opinion, and you trust those people or those groups to evaluate a paper for you,

02:18:08.760 --> 02:18:13.240
that saves you time because, you know, you don't have to, like, scrutinize the paper as much,

02:18:13.240 --> 02:18:16.520
you know, it is brought to your attention. I mean, it's the whole idea of sort of, you know,

02:18:16.520 --> 02:18:22.520
collective recommender system. So, I actually thought about this a lot, you know, about 10,

02:18:22.520 --> 02:18:29.480
15 years ago, because there were discussions at NIPPS and, you know, and we're about to create

02:18:29.480 --> 02:18:36.120
iClear with Yosha Benjo. And so, I wrote a document kind of describing a reviewing system,

02:18:36.120 --> 02:18:40.520
which basically was, you know, you post your paper on some repository, let's say archive,

02:18:40.520 --> 02:18:47.320
or now could be open review. And then you can form a reviewing entity, which is equivalent to a

02:18:47.320 --> 02:18:54.200
reviewing board, you know, of a journal or program committee of a conference. You have to

02:18:54.200 --> 02:19:01.720
list the members. And then that group reviewing entity can choose to review a particular paper

02:19:02.440 --> 02:19:07.000
spontaneously or not. There is no exclusive relationship anymore between a paper and a

02:19:07.000 --> 02:19:13.800
venue or reviewing entity. Any reviewing entity can review any paper or may choose not to.

02:19:14.760 --> 02:19:18.200
And then, you know, give an evaluation. It's not published, not published, it's just an

02:19:18.200 --> 02:19:23.800
evaluation and a comment, which would be public, signed by the reviewing entity. And

02:19:24.520 --> 02:19:27.880
if it's signed by a reviewing entity, you know, it's one of the members of reviewing entity. So,

02:19:27.880 --> 02:19:33.880
if the reviewing entity is, you know, Lex Friedman's, you know, preferred papers, right, you know,

02:19:33.880 --> 02:19:38.680
it's Lex Friedman writing a review. Yes. What, so for me, one, that's a beautiful

02:19:39.000 --> 02:19:45.720
system, I think. But what's in addition to that, it feels like there should be a reputation system

02:19:45.720 --> 02:19:50.120
for the reviewers. Absolutely. For the reviewing entities. Not the reviewers individually.

02:19:50.120 --> 02:19:54.120
The reviewing entities, sure. But even within that, the reviewers too. Because

02:19:55.720 --> 02:20:01.000
there's another thing here. It's not just the reputation. It's an incentive for an individual

02:20:01.000 --> 02:20:06.120
person to do great. Right now, in the academic setting, the incentive is kind of

02:20:07.080 --> 02:20:11.160
internal, just wanting to do a good job. But honestly, that's not a strong enough incentive

02:20:11.160 --> 02:20:15.880
to do a really good job at reading a paper and finding the beautiful amidst the mistakes and

02:20:15.880 --> 02:20:20.680
the flaws and all that kind of stuff. Right. Like, if you're the person that first discovered

02:20:20.680 --> 02:20:27.240
a powerful paper, and you get to be proud of that discovery, then that gives a huge incentive to

02:20:27.240 --> 02:20:31.560
you. That's, that's a big part of my proposal. Actually, I described that as, you know, if,

02:20:31.640 --> 02:20:39.000
if your evaluation of papers is predictive of future success, then your reputation should

02:20:39.000 --> 02:20:46.680
go up as a reviewing entity. So yeah, exactly. I mean, I even had a master's student who was a

02:20:47.240 --> 02:20:51.720
master's student in library science and computer science, actually kind of work out exactly

02:20:52.280 --> 02:20:56.680
how that should work with formulas and everything. But so in terms of implementation,

02:20:56.680 --> 02:21:00.120
do you think that's something that's doable? I mean, I've been sort of, you know, talking about

02:21:00.200 --> 02:21:05.880
this to sort of various people like, you know, Andrew McCallum, who started Open Review. And

02:21:05.880 --> 02:21:10.440
the reason why we picked Open Review for iClear initially, even though it was very early for them,

02:21:11.320 --> 02:21:15.720
is because my hope was that iClear, it was eventually going to kind of

02:21:16.600 --> 02:21:23.320
inaugurate this type of system. So iClear kept the idea of Open Reviews. So whether reviews are,

02:21:23.320 --> 02:21:29.160
you know, published with a paper, which I think is very useful. But in many ways, that's kind of

02:21:29.160 --> 02:21:35.560
reverted to kind of more of a conventional type conferences for everything else. And that, I mean,

02:21:36.760 --> 02:21:44.040
I don't run iClear, I'm just the president of the foundation. But, you know, people who run it

02:21:44.040 --> 02:21:47.800
should make decisions about how to run it. And I'm not going to tell them, because they are

02:21:47.800 --> 02:21:53.880
volunteers. And I'm really thankful that they do that. So, but I'm saddened by the fact that we're

02:21:53.880 --> 02:22:01.240
not being innovative enough. Yeah, me too. I hope that changes. Yeah. Because the communication

02:22:01.240 --> 02:22:07.560
science broadly, but communication, computer science ideas is how you make those ideas have

02:22:07.560 --> 02:22:14.040
impact, I think. Yeah. And I think, you know, a lot of this is because people have in their minds

02:22:14.040 --> 02:22:21.640
kind of an objective, which is, you know, fairness for authors, and the ability to count points,

02:22:21.640 --> 02:22:28.040
basically, and give credits accurately. But that comes at the expense of the progress of science.

02:22:28.680 --> 02:22:31.320
So to some extent, we're slowing down the progress of science.

02:22:32.040 --> 02:22:34.280
And are we actually achieving fairness?

02:22:34.280 --> 02:22:38.520
And we're not achieving fairness, you know, we still have biases, you know, we're doing,

02:22:38.520 --> 02:22:45.000
you know, a double blind review, but, you know, the biases are still there, there are different

02:22:45.000 --> 02:22:50.840
kinds of biases. You write that the phenomenon of emergence, collective behavior exhibited by

02:22:50.920 --> 02:22:56.440
large collection of simple elements in interaction is one of the things that got you into neural

02:22:56.440 --> 02:23:02.840
nets in the first place. I love cellular automata. I love simple interacting elements and the things

02:23:02.840 --> 02:23:08.680
that emerge from them. Do you think we understand how complex systems can emerge from such simple

02:23:08.680 --> 02:23:13.800
components that interact simply? No, we don't. It's a big mystery. Also, it's a mystery for

02:23:13.800 --> 02:23:22.200
physicists, it's a mystery for biologists. You know, how is it that the universe around us seems

02:23:22.200 --> 02:23:28.280
to be increasing in complexity and not decreasing? I mean, that is a kind of curious property of

02:23:29.160 --> 02:23:35.880
physics that despite the second law of thermodynamics, we seem to be, you know, evolution and learning

02:23:35.880 --> 02:23:44.040
and et cetera seems to be kind of at least locally to increase complexity and not decrease it. So,

02:23:44.040 --> 02:23:48.360
perhaps the ultimate purpose of the universe is to just get more complex.

02:23:49.000 --> 02:23:57.080
Have these, I mean, small pockets of beautiful complexity. Does that, to sell your time under

02:23:57.080 --> 02:24:04.040
these kinds of emergence and complex systems give you some intuition or guide your understanding

02:24:04.120 --> 02:24:08.760
of machine learning systems and neural networks and so on? Are these for you right now disparate

02:24:08.760 --> 02:24:15.480
concepts? Well, it got me into it. You know, I discovered the existence of the perceptron

02:24:15.480 --> 02:24:21.640
when I was a college student by reading a good book. It was a debate between Chomsky and Piaget

02:24:21.640 --> 02:24:27.160
and Seymour Papert from MIT who was kind of singing the praise of the perceptron in that

02:24:27.160 --> 02:24:31.320
book. And the first time I heard about the running machine, right? So, I started digging the literature

02:24:31.320 --> 02:24:37.800
and I found those books which were basically transcription of, you know, workshops or conferences

02:24:38.600 --> 02:24:44.520
from the 50s and 60s about self-organizing systems. So, there was a series of conferences

02:24:44.520 --> 02:24:50.360
on self-organizing systems and these books on this. Some of them are, you can actually get them at the

02:24:50.360 --> 02:24:57.640
Internet Archive, you know, the digital version. And there are like fascinating articles in there

02:24:57.640 --> 02:25:03.960
by, there's a guy whose name has been largely forgotten, Heinz von Förster. He's a German

02:25:03.960 --> 02:25:11.560
physicist who emigrated to the US and worked on self-organizing systems in the 50s. And in the

02:25:11.560 --> 02:25:16.680
60s, he created at the University of Illinois, Japan, Japan, he created the biological computer

02:25:16.680 --> 02:25:22.520
laboratory, VCL, which was, you know, all about neural nets. Unfortunately, that was kind of

02:25:22.520 --> 02:25:27.640
towards the end of the popularity of neural nets. So, that lab never kind of strived very much.

02:25:27.640 --> 02:25:33.320
But he wrote a bunch of papers about self-organization and about the mystery of self-organization.

02:25:33.320 --> 02:25:38.760
An example he has is, you take, imagine you are in space, there's no gravity. You have a big box

02:25:38.760 --> 02:25:45.400
with magnets in it, okay? You know, kind of rectangular magnets with north pole on one end,

02:25:45.400 --> 02:25:49.640
south pole on the other end. You shake the box gently and the magnets will kind of stick to

02:25:49.640 --> 02:25:55.320
themselves and probably form like complex structure, you know, spontaneously. You know,

02:25:55.320 --> 02:25:58.520
that could be an example of self-organization. But, you know, you have lots of example, neural

02:25:58.520 --> 02:26:04.520
nets are an example of self-organization to, you know, in many respects. And it's a bit of a mystery,

02:26:05.560 --> 02:26:11.080
you know, how, like, what is possible with this? You know, pattern formation in physical systems,

02:26:11.800 --> 02:26:16.440
in chaotic system and things like that, you know, the emergence of life, you know, things

02:26:16.440 --> 02:26:22.440
like that. So, you know, how does that happen? So, it's a big puzzle for physicists as well.

02:26:22.440 --> 02:26:29.880
It feels like understanding this, the mathematics of emergence in some constrained situations might

02:26:29.880 --> 02:26:37.640
help us create intelligence. Like, help us add a little spice to the systems because you seem to

02:26:37.640 --> 02:26:44.840
be able to, in complex systems with emergence, to be able to get a lot from little. And so,

02:26:44.920 --> 02:26:51.960
that seems like a shortcut to get big leaps in performance. But there's a missing

02:26:51.960 --> 02:27:00.600
concept that we don't have. And it's something also I've been fascinated by since my undergrad days.

02:27:00.600 --> 02:27:07.080
And it's how you measure complexity, right? So, we don't actually have good ways of measuring or at

02:27:07.080 --> 02:27:11.960
least we don't have good ways of interpreting the measures that we have at our disposal. Like,

02:27:11.960 --> 02:27:15.640
how do you measure the complexity of something, right? So, there's all those things, you know,

02:27:15.640 --> 02:27:19.960
like, you know, Karmogorov, Chaitin, Solomonov complexity of, you know, the length of the

02:27:19.960 --> 02:27:24.600
shortest program that would generate a bit string can be thought of as the complexity of that bit

02:27:24.600 --> 02:27:32.440
string. I've been fascinated by that concept. The problem with that is that that complexity is

02:27:32.440 --> 02:27:38.760
defined up to a constant, which can be very large. There are similar concepts that are derived from,

02:27:38.840 --> 02:27:46.760
you know, Bayesian probability theory, where, you know, the complexity of something is the

02:27:46.760 --> 02:27:50.680
negative log of its probability essentially, right? And you have a complete equivalence between

02:27:50.680 --> 02:27:55.240
the two things. And there you would think, you know, the probability is something that's well

02:27:55.240 --> 02:27:59.480
defined mathematically, which means complexity is well defined. But it's not true. You need to have

02:27:59.480 --> 02:28:05.160
a model of the distribution. You may need to have a prior, if you're doing Bayesian inference. And

02:28:05.160 --> 02:28:09.320
the prior plays the same role as the choice of the computer with which you measure your Karmogorov

02:28:09.320 --> 02:28:16.600
complexity. And so, every measure of complexity we have has some arbitraryness in it, you know,

02:28:16.600 --> 02:28:23.480
an additive constant, which is, can be arbitrarily large. And so, you know, how can we come up with

02:28:23.480 --> 02:28:26.840
a good theory of how things become more complex if we don't have a good measure of complexity?

02:28:26.840 --> 02:28:32.840
Yeah, which we need for is one way that people study this in the space of biology,

02:28:32.840 --> 02:28:37.160
the people that study the origin of life or try to recreate life in the laboratory.

02:28:37.720 --> 02:28:41.160
And the more interesting one is the alien one is when we go to other planets,

02:28:41.960 --> 02:28:47.480
how do we recognize this life? Because, you know, complexity, we associate complexity,

02:28:47.480 --> 02:28:53.480
maybe some level of mobility with life, you know, we have to be able to, like, have concrete

02:28:54.680 --> 02:29:01.320
algorithms for, like, measuring the level of complexity we see in order to know the

02:29:01.320 --> 02:29:04.840
difference between life and non-life. And the problem is that complexity is in the

02:29:04.840 --> 02:29:13.480
IODB holder. So, let me give you an example. If I give you an image of the MNIST digits,

02:29:13.480 --> 02:29:19.000
right, and I flip through MNIST digits, there is some, obviously some structure to it because

02:29:19.720 --> 02:29:25.160
local structure, you know, neighboring pixels are correlated across the entire dataset.

02:29:25.960 --> 02:29:31.640
Now, imagine that I apply a random permutation to all the pixels,

02:29:32.600 --> 02:29:37.960
a fixed random permutation. Now, I show you those images, they will look, you know,

02:29:37.960 --> 02:29:43.480
really disorganized to you, more complex. In fact, they're not more complex in absolute terms,

02:29:43.480 --> 02:29:47.480
they're exactly the same as originally, right? And if you knew what the permutation was,

02:29:47.480 --> 02:29:53.240
you know, you could undo the permutation. Now, imagine I give you special glasses that undo

02:29:53.240 --> 02:29:58.680
that permutation. Now, all of a sudden, what looked complicated becomes simple. Right. So,

02:29:58.680 --> 02:30:03.880
if you have two, if you have, you know, humans on one end, and then another race of aliens that

02:30:03.880 --> 02:30:07.320
sees the universe with permutation glasses. Yeah, with the permutation glasses.

02:30:08.600 --> 02:30:12.280
What we perceive as simple to them is hardly complicated, it's probably heat.

02:30:12.280 --> 02:30:19.000
Yeah, heat, yeah. Okay. And what they perceive as simple to us is random fluctuation, it's heat.

02:30:19.000 --> 02:30:24.280
Yeah. So, truly in the eye of the beholder, depends what kind of glasses you're wearing.

02:30:24.840 --> 02:30:28.280
Right. Depends what kind of algorithm you're running in your perception system.

02:30:28.280 --> 02:30:33.240
So, I don't think we'll have a theory of intelligence, self-organization, evolution,

02:30:33.240 --> 02:30:39.320
things like that, until we have a good handle on a notion of complexity, which we know is in the

02:30:39.320 --> 02:30:45.080
high, the eye of the beholder. Yeah, it's sad to think that we might not be able to detect or

02:30:45.080 --> 02:30:49.160
interact with alien species because we're wearing different glasses.

02:30:50.200 --> 02:30:52.760
Because their notion of locality might be different from ours. Yeah.

02:30:52.760 --> 02:30:56.920
This actually connects with fascinating questions in physics at the moment, like modern physics,

02:30:58.040 --> 02:31:02.440
quantum physics, like, you know, questions about like, you know, can we recover the information

02:31:02.440 --> 02:31:07.640
that's lost in a black hole and things like this, right? And that relies on notions of complexity,

02:31:09.000 --> 02:31:11.560
which, you know, I find it's fascinating.

02:31:11.560 --> 02:31:19.080
Can you describe your personal quest to build an expressive electronic wind instrument EWI?

02:31:19.720 --> 02:31:23.960
What is it? What does it take to build it?

02:31:23.960 --> 02:31:28.360
Well, I'm a thinker. I like building things. I like building things with combinations of

02:31:28.360 --> 02:31:34.280
electronics and, you know, mechanical stuff. You know, I have a bunch of different hobbies, but

02:31:35.480 --> 02:31:39.560
you know, probably my first one was little was building model airplanes and stuff like

02:31:39.560 --> 02:31:44.040
that. And I still do that to some extent. But also electronics, I taught myself electronics before

02:31:44.040 --> 02:31:50.440
I studied it. And the reason I taught myself electronics is because of music. My cousin

02:31:51.400 --> 02:31:55.560
was an aspiring electronic musician, and then he had an analog synthesizer. And I was, you know,

02:31:55.560 --> 02:32:00.120
basically modifying it for him and building sequencers and stuff like that, right, for him.

02:32:00.120 --> 02:32:01.800
I was in high school when I was doing this.

02:32:02.520 --> 02:32:07.880
How's the interest in like progressive rock like 80s? Like, what's the greatest band of all time,

02:32:07.880 --> 02:32:12.600
according to Yonah Kuhn? There's too many of them. But, you know, it's a combination of

02:32:15.800 --> 02:32:22.760
you know, my vision orchestra, weather report, yes, Genesis, you know,

02:32:22.760 --> 02:32:28.520
yes, Genesis, Peter Gabriel, gentle giant, you know, things like that.

02:32:29.080 --> 02:32:34.200
Great. Okay. So this, this love of electronics and this love of music combined together.

02:32:34.200 --> 02:32:40.280
Right. So I was actually trained to play Baroque and Renaissance music. And I played in

02:32:41.480 --> 02:32:46.840
orchestra when I was in high school and first years of college. And I played the recorder,

02:32:46.840 --> 02:32:52.520
Cromhorn, a little bit of oboe, you know, things like that. So I'm a wind instrument player. But

02:32:52.520 --> 02:32:55.240
I always wanted to play improvised music, even though I don't know anything about it.

02:32:56.280 --> 02:33:01.400
And the only way I figured, you know, short of like learning to play saxophone was to

02:33:02.120 --> 02:33:07.000
play electronic wind instruments. So they behave on the fingering is similar to a saxophone, but,

02:33:07.000 --> 02:33:11.000
you know, you have a wide variety of sound because you control the synthesizer with it.

02:33:11.000 --> 02:33:18.120
So I had a bunch of those, you know, going back to the late 80s from either Yamaha or

02:33:18.120 --> 02:33:23.640
Akai, they're both kind of the main manufacturers of those that they were classically, you know,

02:33:23.640 --> 02:33:28.200
going back several decades. But I've never been completely satisfied with them because of lack

02:33:28.200 --> 02:33:33.480
of expressivity. And, you know, those things, you know, are somewhat expressive. I mean,

02:33:33.480 --> 02:33:37.000
they measure the breath pressure, they measure the lip pressure, and, you know,

02:33:38.360 --> 02:33:44.040
you have various parameters, you can vary with fingers, but they're not really

02:33:44.040 --> 02:33:49.320
as expressive as an acoustic instrument, right? You hear John Coltrane play two notes,

02:33:49.320 --> 02:33:51.960
and you know it's John Coltrane, you know, it's got a unique sound.

02:33:52.920 --> 02:33:59.560
Or Miles Davis, right? You can hear it's Miles Davis playing the trumpet because the sound

02:34:00.440 --> 02:34:06.120
reflects their, you know, physiognomy, basically the shape of the vocal track

02:34:08.040 --> 02:34:13.960
kind of shapes the sound. So how do you do this with an electronic instrument? And I was, many

02:34:13.960 --> 02:34:19.400
years ago, I met a guy called David Wessel. He was a professor at Berkeley and created the

02:34:19.640 --> 02:34:24.680
center for like, you know, music technology there. And he was interested in that question.

02:34:25.960 --> 02:34:30.120
And so I kept kind of thinking about this for many years. And finally, because of COVID, you

02:34:30.120 --> 02:34:35.960
know, I was at home, I was in my workshop, my workshop serves also as my kind of Zoom room

02:34:35.960 --> 02:34:43.240
and home office. And this is in New Jersey? In New Jersey. And I started really being serious about,

02:34:43.240 --> 02:34:48.200
you know, building my own EWI instrument. What else is going on in the New Jersey workshop? Is

02:34:49.160 --> 02:34:55.320
some crazy stuff you built or like left on the workshop floor left behind?

02:34:55.320 --> 02:35:01.000
A lot of crazy stuff is, you know, electronics built with microcontrollers of various kinds.

02:35:01.560 --> 02:35:07.960
And, you know, weird flying contraptions. So you still love flying?

02:35:08.520 --> 02:35:15.000
It's a family disease. My dad got me into it when I was a kid. And he was building

02:35:15.080 --> 02:35:20.760
model airplanes when he was a kid. And he was a mechanical engineer. He taught himself electronics

02:35:20.760 --> 02:35:28.840
also. So he built his early radio control systems in the late 60s, early 70s. And so that's what

02:35:28.840 --> 02:35:32.920
got me into, I mean, he got me into kind of, you know, engineering and science and technology.

02:35:32.920 --> 02:35:37.400
Do you also have an interest in appreciation of flight in other forms, like with drones,

02:35:37.400 --> 02:35:44.040
quadroptors? Or do you, is it model airplane? You know, before drones were,

02:35:45.000 --> 02:35:51.080
you know, kind of consumer products, you know, I built my own, you know, with also building

02:35:51.080 --> 02:35:56.520
a microcontroller with JavaScripts and accelerometers for stabilization, writing the

02:35:56.520 --> 02:35:59.480
firmware for it, you know, and then when it became kind of a standard thing you could buy,

02:35:59.480 --> 02:36:02.120
it was boring, you know, I stopped doing it. It was in front anymore.

02:36:03.400 --> 02:36:09.960
Yeah, you were doing it before it was cool. What advice would you give to a young person today

02:36:09.960 --> 02:36:15.800
in high school and college that dreams of doing something big, like young like Coon,

02:36:15.800 --> 02:36:21.560
like let's talk in the space of intelligence, dreams of having a chance to solve some fundamental

02:36:21.560 --> 02:36:27.560
problem in space of intelligence, both for their career and just in life, being somebody who was

02:36:27.560 --> 02:36:35.320
a part of creating something special. So try to get interested by big questions,

02:36:35.320 --> 02:36:41.560
things like, you know, what is intelligence? What is the universe made of? What's life all about?

02:36:41.560 --> 02:36:50.120
Things like that. Like even like crazy big questions like what's time, like nobody knows what time is.

02:36:53.080 --> 02:37:01.880
And then learn basic things like basic methods, either from math, from physics, or from engineering.

02:37:02.840 --> 02:37:07.640
Things that have a long shelf life. Like if you have a choice between like, you know,

02:37:08.680 --> 02:37:14.280
learning, you know, mobile programming on iPhone, or quantum mechanics, take quantum mechanics.

02:37:17.000 --> 02:37:23.400
Because you're going to learn things that you have no idea exist. You may not, you may never be a

02:37:23.400 --> 02:37:27.880
quantum physicist, but you'll learn about path integrals. And path integrals are used

02:37:28.600 --> 02:37:32.520
everywhere. It's the same formula that you use for, you know, vision integration and stuff like

02:37:32.520 --> 02:37:39.160
that. So the ideas, the little ideas within quantum mechanics within some of these kind

02:37:39.160 --> 02:37:46.280
of more solidified fields will have a longer shelf life, they will use somehow use indirectly in your

02:37:46.280 --> 02:37:52.440
work. Learn classical mechanics, like you learn about Lagrangians, for example, which is like a

02:37:52.520 --> 02:37:57.480
huge, hugely useful concept, you know, for all kinds of different things. Learn

02:37:58.360 --> 02:38:04.120
statistical physics, because all the math that comes out of, you know, for machine learning,

02:38:05.400 --> 02:38:10.200
basically comes out of what we got out by statistical physicists in the, you know, late 19, early 20th

02:38:10.200 --> 02:38:16.040
century. Right. So, and for some of them, actually, more recently, by people like George O'Parisi,

02:38:16.040 --> 02:38:21.640
who just got the Nobel Prize for the replica method, among other things, it's used for a lot

02:38:21.640 --> 02:38:27.400
of different things, you know, variational inference, that math comes from statistical physics.

02:38:28.520 --> 02:38:35.320
So, so a lot of those kind of, you know, basic courses, you know, you'll, if you do it

02:38:35.320 --> 02:38:38.920
like you're engineering, you take signal processing, you'll learn about Fourier transforms.

02:38:39.800 --> 02:38:45.800
Again, something super useful is at the basis of things like graph neural nets, which is an

02:38:45.800 --> 02:38:51.640
entirely new subarea of, you know, AI machine learning, deep learning, which I think is super

02:38:51.640 --> 02:38:56.040
promising for all kinds of applications. Something very promising, if you're more interested in

02:38:56.040 --> 02:39:01.880
applications is the applications of AI machine learning and deep learning to science, or to

02:39:03.240 --> 02:39:08.520
science that can help solve big problems in the world. I have colleagues at Meta, at fair,

02:39:09.080 --> 02:39:14.680
who started this project called Open Catalyst, and it's an open project collaborative. And the

02:39:14.680 --> 02:39:22.360
idea is to use deep learning to help design new chemical compounds or materials that would

02:39:22.360 --> 02:39:28.920
facilitate the separation of hydrogen from oxygen. If you can efficiently separate oxygen from hydrogen

02:39:28.920 --> 02:39:37.240
with electricity, you solve climate change. It's as simple as that, because you cover,

02:39:37.240 --> 02:39:43.320
you know, some random desert with solar panels, and you have them work all day, produce hydrogen,

02:39:43.320 --> 02:39:46.200
and then you see the hydrogen wherever it's needed. You don't need anything else.

02:39:48.440 --> 02:39:59.000
You know, you have controllable power that can be transported anywhere. So if we have a large-scale,

02:39:59.000 --> 02:40:06.920
efficient energy storage technology like producing hydrogen, we solve climate change. Here's another

02:40:06.920 --> 02:40:11.480
way to solve climate change is figuring out how to make fusion work. Now, the problem with fusion

02:40:11.480 --> 02:40:16.120
is that you make a super hot plasma, and the plasma is unstable, and you can control it.

02:40:16.120 --> 02:40:19.720
Maybe with deep learning, you can find controllers that will stabilize plasma and make, you know,

02:40:19.720 --> 02:40:24.440
practical fusion reactors. I mean, that's very speculative, but, you know, it's worth trying,

02:40:24.440 --> 02:40:31.080
because, you know, the payoff is huge. There's a group at Google working on this led by John Platt.

02:40:31.080 --> 02:40:36.760
So control, convert as many problems in science and physics and biology and chemistry

02:40:36.760 --> 02:40:40.920
into a, into a learnable problem and see if a machine can learn it.

02:40:41.480 --> 02:40:46.360
Right. I mean, there's properties of, you know, complex materials that we don't understand from

02:40:46.360 --> 02:40:53.720
first principle, for example. Right. So, you know, if we could design new, you know, new materials,

02:40:54.600 --> 02:40:58.920
we could make more efficient batteries, you know, we could make maybe faster electronics. We could,

02:40:58.920 --> 02:41:04.680
I mean, there's a lot of things we can imagine doing, or, you know, lighter materials for,

02:41:04.680 --> 02:41:08.360
for cars or airplanes or things like that, maybe better fuel cells. I mean, there's all kinds of

02:41:08.360 --> 02:41:13.080
stuff we can imagine. If we had good fuel cells, hydrogen fuel cells, we could use them to power

02:41:13.080 --> 02:41:19.400
airplanes, and, you know, transportation wouldn't be, or cars, and we wouldn't have a emission

02:41:19.400 --> 02:41:25.880
problem, CO2 emission problems for, for air transportation anymore. So there's a lot of

02:41:25.880 --> 02:41:32.040
those things, I think, where AI, you know, can be used. And this is not even talking about all the

02:41:32.040 --> 02:41:37.720
sort of medicine biology and everything like that, right? You know, like protein folding,

02:41:37.720 --> 02:41:41.800
you know, figuring out, like, how could you design your proteins, that it sticks to another protein

02:41:41.800 --> 02:41:47.000
that a particular site, because that's how you design drugs in the end. So, you know, deep learning

02:41:47.000 --> 02:41:51.080
would be useful, all of this. And those are kind of, you know, would be sort of enormous progress

02:41:51.080 --> 02:41:58.200
if we could use it for that. Here's an example. If you take, this is like from recent material physics,

02:41:58.200 --> 02:42:05.560
you take a monoatomic layer of graphene, right? So it's just carbon on an hexagonal mesh, and you

02:42:05.560 --> 02:42:11.640
make this single, single atom thick. You put another one on top, you twist them by some magic

02:42:12.280 --> 02:42:17.960
number of degrees, three degrees or something. It becomes superconductor. Nobody has any idea why.

02:42:21.080 --> 02:42:23.800
I want to know how that was discovered, but that's the kind of thing that machine learning

02:42:23.800 --> 02:42:29.320
can actually discover, these kinds of things. Well, maybe not, but there is a hint, perhaps, that

02:42:29.320 --> 02:42:35.480
with machine learning, we would train a system to basically be a phenomenological model of some

02:42:35.480 --> 02:42:40.120
complex emergent phenomenon, which, you know, superconductivity is one of those,

02:42:42.360 --> 02:42:46.840
where, you know, this collective phenomenon is too difficult to describe from first principles

02:42:46.840 --> 02:42:52.680
with the current, you know, the usual sort of reductionist type method. But we could have

02:42:54.040 --> 02:42:59.480
deep learning systems that predict the properties of a system from a description of it after being

02:42:59.480 --> 02:43:08.760
trained with sufficiently many samples. This guy, Pascal Foua at DPFL, he has a startup company that

02:43:09.640 --> 02:43:15.960
where he basically trained a convolutional net essentially to predict the aerodynamic

02:43:15.960 --> 02:43:20.520
properties of solids. And you can generate as much data as you want by just running

02:43:20.520 --> 02:43:28.600
a computational free dynamics, right? So you give, like, a wing, a foil or something shape

02:43:28.600 --> 02:43:34.440
of some kind, and you run computational free dynamics, you get, as a result, the drag and,

02:43:34.440 --> 02:43:40.600
you know, lift and all that stuff, right? And you can generate lots of data, train a neural

02:43:40.600 --> 02:43:45.240
net to make those predictions. And now what you have is a differentiable model of, let's say,

02:43:45.240 --> 02:43:49.560
drag and lift as a function of the shape of that solid. And so you can do background and

02:43:49.560 --> 02:43:52.680
design, you can optimize the shape, so you get the properties you want.

02:43:54.680 --> 02:44:00.200
Yeah, that's incredible. That's incredible. And on top of all that, probably, you should read a

02:44:00.200 --> 02:44:06.760
little bit of literature and a little bit of history for inspiration and for wisdom, because

02:44:06.760 --> 02:44:11.640
after all, all of these technologies will have to work in a human world. And the human world is

02:44:11.640 --> 02:44:19.240
complicated. Yeah, and this is an amazing conversation. I really honored that you

02:44:19.320 --> 02:44:23.160
talked with me today. Thank you for all the amazing work you're doing at FAIR at Metta.

02:44:23.720 --> 02:44:28.680
And thank you for being so passionate after all these years about everything that's going on.

02:44:28.680 --> 02:44:32.600
You're a beacon of hope for the machine learning community. And thank you so much

02:44:32.600 --> 02:44:35.080
for spending your valuable time with me today. That was awesome.

02:44:35.080 --> 02:44:37.480
Thanks for having me on. That was a pleasure.

02:44:38.680 --> 02:44:42.680
Thanks for listening to this conversation with Yann LeCun. To support this podcast,

02:44:42.680 --> 02:44:48.680
please check out our sponsors in the description. And now let me leave you some words from Isaac

02:44:48.680 --> 02:44:55.880
Asimov. Your assumptions are your windows on the world. Scrub them off every once in a while,

02:44:55.880 --> 02:45:04.600
or the light won't come in. Thank you for listening and hope to see you next time.

