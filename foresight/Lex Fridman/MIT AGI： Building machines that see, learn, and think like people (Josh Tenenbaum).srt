1
00:00:00,000 --> 00:00:06,000
Today we have Josh Tenenbaum. He's a professor here at MIT, leading the Computational Cognitive

2
00:00:06,000 --> 00:00:12,000
Science Group. Among many other topics in cognition and intelligence, he is fascinated

3
00:00:12,000 --> 00:00:18,000
with the question of how human beings learn so much from so little, and how these insights

4
00:00:18,000 --> 00:00:23,480
can lead to build AI systems that are much more efficient learning from data. So please

5
00:00:23,480 --> 00:00:25,480
give Josh a warm welcome.

6
00:00:31,480 --> 00:00:37,480
Alright, thank you very much. Thanks for having me. Excited to be part of what looks like

7
00:00:37,480 --> 00:00:43,480
really quite a very impressive lineup, especially starting after today. And it's, I think, quite a

8
00:00:43,480 --> 00:00:47,480
great opportunity to get to see perspectives on artificial intelligence from many of the

9
00:00:47,480 --> 00:00:54,480
leaders in industry and other entities working on this great quest. So I'm going to talk to you

10
00:00:54,480 --> 00:00:58,480
about some of the work that we do in our group, but also I'm going to try to give a broader

11
00:00:58,480 --> 00:01:02,480
perspective, reflective of a number of MIT faculty, especially those who are affiliated with the

12
00:01:02,480 --> 00:01:07,480
Center for Brains, Minds, and Machines. So you can see up there on my affiliation, academically

13
00:01:07,480 --> 00:01:12,480
I'm part of Brain and Cognitive Science, or Course 9. I'm also part of CSAIL, but I'm also part of

14
00:01:12,480 --> 00:01:17,480
the Center for Brains, Minds, and Machines, which is an NSF-funded center, Science and Technology

15
00:01:17,480 --> 00:01:22,480
Center, which really stands for the bridge between the science and the engineering of intelligence. It

16
00:01:22,480 --> 00:01:26,480
literally straddles Vassar Street in that we have CSAIL and BCS members. We also have partners at

17
00:01:26,480 --> 00:01:31,480
Harvard and other academic institutions. And again, what we stand for, I want to try to convey

18
00:01:31,480 --> 00:01:35,480
some of the specific things we're doing in the center and where we want to go with a vision that

19
00:01:35,480 --> 00:01:41,480
really is about jointly pursuing the science, the basic science of how intelligence arises in the

20
00:01:41,480 --> 00:01:47,480
human mind and brain, and also the engineering enterprise of how to build something increasingly

21
00:01:47,480 --> 00:01:51,480
like human intelligence in machines. And we deeply believe that these two projects have something to do

22
00:01:51,480 --> 00:01:57,480
with each other and are best pursued jointly. Now it's a really exciting time to be doing anything

23
00:01:57,480 --> 00:02:02,480
related to intelligence or certainly to AI for all the reasons that, you know, brought you all here.

24
00:02:02,480 --> 00:02:07,480
I don't have to tell you this. We have all these ways in which AI is kind of finally here. We finally

25
00:02:07,480 --> 00:02:13,480
live in the era of something like real practical AI, or for those who've been around for a while and

26
00:02:13,480 --> 00:02:18,480
have seen some of the rises and falls, you know, AI is back in a big way. But from my perspective,

27
00:02:18,480 --> 00:02:24,480
and I think maybe this reflects, you know, why we distinguish what we might call AGI from AI, we don't

28
00:02:24,480 --> 00:02:29,480
really have any real AI, basically. We have what I like to call AI technologies, which are systems

29
00:02:29,480 --> 00:02:34,480
that do things we used to think that only humans could do. And now we have machines that do them,

30
00:02:34,480 --> 00:02:40,480
often quite well, maybe even better than any human who's ever lived, right? Like a machine that plays go.

31
00:02:40,480 --> 00:02:45,480
But none of these systems, I would say, are truly intelligent. None of them have anything like common sense.

32
00:02:45,480 --> 00:02:51,480
None of them have anything like the flexible general purpose intelligence that each of you might use to learn

33
00:02:51,480 --> 00:02:57,480
every one of these skills or tasks, right? Each of these systems had to be built by large teams of engineers,

34
00:02:57,480 --> 00:03:02,480
working together often for a number of years at often at great cost to somebody who's willing to pay for it.

35
00:03:02,480 --> 00:03:09,480
And each of them just does one thing. So AlphaGo might beat the world's best, but it can't drive to the match,

36
00:03:09,480 --> 00:03:16,480
or even tell you what go is. It can't even tell you that go is a game, because it doesn't even know what a game is, right?

37
00:03:16,480 --> 00:03:23,480
So what's missing? What is it that makes every one of your brains, maybe you can't beat, you know, the world's best in go,

38
00:03:23,480 --> 00:03:29,480
but any one of you can get behind the wheel of a car. I think of this because my daughter is going to turn 16 tomorrow.

39
00:03:29,480 --> 00:03:36,480
If she lived in California, she'd have a driver's license. It's a little bit down the line for us here in Massachusetts.

40
00:03:36,480 --> 00:03:41,480
But, you know, she didn't have to be specially engineered by billion-dollar startups.

41
00:03:41,480 --> 00:03:46,480
And, you know, she got really into chess recently, and now she's taught herself chess by playing just, you know,

42
00:03:46,480 --> 00:03:51,480
and a handful of games, basically. And she can do any one of these activities, and any one of us can.

43
00:03:51,480 --> 00:03:56,480
So what is it? What makes up the difference? Well, there's many things, right?

44
00:03:56,480 --> 00:04:04,480
I'll talk about the focus for us in our research, and a lot of us, again, in CBMM, is summarized here.

45
00:04:04,480 --> 00:04:13,480
What drives the successes right now in AI, especially in industry, okay, and all these AI technologies, is many, many things, many things.

46
00:04:13,480 --> 00:04:19,480
But what's where the progress has been made most recently, and what's getting most of the attention is, of course, deep learning,

47
00:04:19,480 --> 00:04:27,480
but other kinds of machine learning technologies, which essentially represent the maturation of a decades-long effort to solve the problem of pattern recognition.

48
00:04:27,480 --> 00:04:37,480
That means taking data and finding patterns in the data that tell you something you care about, like how to label a class or how to predict some other signal, okay?

49
00:04:37,480 --> 00:04:45,480
And pattern recognition is great. It's an important part of intelligence, and it's reasonable to say that deep learning as a technology

50
00:04:45,480 --> 00:04:52,480
has really made great strides on pattern recognition, and maybe even, you know, has come in close to solving the problems of pattern recognition.

51
00:04:52,480 --> 00:04:59,480
But intelligence is about many other things. Intelligence is about a lot more. In particular, it's about modeling the world,

52
00:04:59,480 --> 00:05:06,480
and think about all the activities that a human does to model the world that go beyond, just say, recognizing patterns in data,

53
00:05:06,480 --> 00:05:10,480
but actually trying to explain and understand what we see, for instance, okay?

54
00:05:10,480 --> 00:05:17,480
Or to be able to imagine things that we've never seen, that never seen, maybe even very different from anything we've ever seen,

55
00:05:17,480 --> 00:05:24,480
but what I want to see, and then to set those as goals, to make plans and solve problems needed to make those things real,

56
00:05:24,480 --> 00:05:29,480
or thinking about learning, again, that, you know, some kinds of learning can be thought of as pattern recognition

57
00:05:29,480 --> 00:05:33,480
if you're learning sufficient statistics or weights in a neural net that are used for those purposes.

58
00:05:33,480 --> 00:05:37,480
But many activities of learning are about building out new models, right?

59
00:05:37,480 --> 00:05:44,480
Either refining, reusing, improving old models, or actually building fundamentally new models as you've experienced more of the world.

60
00:05:44,480 --> 00:05:49,480
And then think about sharing our models, communicating our models to others, modeling their models, learning from them.

61
00:05:49,480 --> 00:05:57,480
All these activities of modeling, these are at the heart of human intelligence, and it requires a much broader set of tools.

62
00:05:58,480 --> 00:06:03,480
So I want to talk about the ways we're studying these activities of modeling the world and something in a pretty non-technical way

63
00:06:03,480 --> 00:06:07,480
about what are the kind of tools that allow us to capture these abilities.

64
00:06:07,480 --> 00:06:12,480
Now, I think it's, I want to be very honest up front and to say this is just the beginning of a story, right?

65
00:06:12,480 --> 00:06:17,480
When you look at deep learning successes, that itself is a story that goes back decades.

66
00:06:17,480 --> 00:06:19,480
I'll say a little bit about that history in a minute.

67
00:06:19,480 --> 00:06:24,480
But where we are now is just looking forward to a future when we might be able to capture these abilities,

68
00:06:25,480 --> 00:06:27,480
at a really mature engineering scale.

69
00:06:27,480 --> 00:06:34,480
And I would say we are far from being able to capture all the ways in which humans richly, flexibly, quickly build models of the world

70
00:06:34,480 --> 00:06:36,480
at the kind of scale that, say, Silicon Valley wants.

71
00:06:36,480 --> 00:06:42,480
Either big tech companies like Google, or Microsoft, or IBM, or Facebook, or small startups, right?

72
00:06:42,480 --> 00:06:44,480
We can get there.

73
00:06:44,480 --> 00:06:48,480
And I think what I want to talk to you about here is one route for trying to get there.

74
00:06:48,480 --> 00:06:51,480
And this is the route that CBMM stands for.

75
00:06:51,480 --> 00:06:55,480
The idea that by reverse engineering how intelligence works in the human mind and brain,

76
00:06:55,480 --> 00:06:59,480
that will give us a route to engineering these abilities in machines.

77
00:06:59,480 --> 00:07:03,480
When we say reverse engineering, we're talking about science, but doing science like engineers.

78
00:07:03,480 --> 00:07:05,480
This is our fundamental principle.

79
00:07:05,480 --> 00:07:08,480
That if we approach cognitive science and neuroscience like an engineer,

80
00:07:08,480 --> 00:07:12,480
where, say, the output of our science isn't just a description of the brain or the mind in words,

81
00:07:12,480 --> 00:07:16,480
but in the same terms that an engineer would use to build an intelligent system,

82
00:07:16,480 --> 00:07:20,480
then that will be both the basis for a much more rigorous and deeply insightful science,

83
00:07:20,480 --> 00:07:25,480
but also direct translation of those insights into engineering applications.

84
00:07:25,480 --> 00:07:30,480
Now, before I talk a little bit about history, what I mean by that is this.

85
00:07:30,480 --> 00:07:33,480
Again, if part of what brought you here is deep learning, and I know,

86
00:07:33,480 --> 00:07:36,480
even if you've never heard of deep learning before, which I'm sure is unlikely,

87
00:07:36,480 --> 00:07:42,480
you saw a good spectrum of that in the overview session last night.

88
00:07:42,480 --> 00:07:48,480
It's really interesting and important to look back on the history of where did techniques for deep learning come from,

89
00:07:48,480 --> 00:07:49,480
or reinforcement learning.

90
00:07:49,480 --> 00:07:54,480
Those are the two tools in the current machine learning arsenal that are getting the most attention.

91
00:07:54,480 --> 00:07:57,480
Things like back propagation or end-to-end stochastic gradient descent

92
00:07:57,480 --> 00:07:59,480
or temporal difference learning or Q-learning.

93
00:07:59,480 --> 00:08:01,480
Here's a few papers from the literature.

94
00:08:01,480 --> 00:08:03,480
Maybe some of you have read these original papers.

95
00:08:03,480 --> 00:08:07,480
Here's the original paper by Rumelhardt, Hinton, and colleagues

96
00:08:07,480 --> 00:08:11,480
in which they introduced the back propagation algorithm for training multi-layer perceptrons,

97
00:08:11,480 --> 00:08:12,480
multi-layer neural networks.

98
00:08:12,480 --> 00:08:14,480
Here's the original perceptron paper by Rosenblatt,

99
00:08:14,480 --> 00:08:17,480
which introduced the one-layer version of that architecture

100
00:08:17,480 --> 00:08:19,480
and the basic perceptron learning algorithm.

101
00:08:19,480 --> 00:08:23,480
Here's the first paper on the temporal difference learning method

102
00:08:23,480 --> 00:08:26,480
for reinforcement learning from Sutton and Bartow.

103
00:08:26,480 --> 00:08:30,480
Here's the original Boltzmann machine paper also by Hinton and colleagues,

104
00:08:30,480 --> 00:08:38,480
those of you who don't know that architecture think of a probabilistic undirected multi-layer perceptron.

105
00:08:38,480 --> 00:08:43,480
For example, before there were LSTMs, if you know about current-recurrent neural network architecture,

106
00:08:43,480 --> 00:08:47,480
earlier much simpler versions of the same idea were proposed by Jeff Elman

107
00:08:47,480 --> 00:08:49,480
and his simple-recurrent networks.

108
00:08:49,480 --> 00:08:51,480
The reason I want to put up the original papers here

109
00:08:51,480 --> 00:08:55,480
is for you to look at both when they were published and where they were published.

110
00:08:55,480 --> 00:09:00,480
If you look at the dates, you'll see papers going back to the 80s,

111
00:09:00,480 --> 00:09:03,480
but even the 60s or even the 1950s.

112
00:09:03,480 --> 00:09:05,480
Look at where they were published.

113
00:09:05,480 --> 00:09:07,480
Most of them were published in psychology journals.

114
00:09:07,480 --> 00:09:09,480
The journal Psychological Review, if you don't know,

115
00:09:09,480 --> 00:09:13,480
it is like the leading journal of theoretical psychology and mathematical psychology,

116
00:09:13,480 --> 00:09:17,480
or Cognitive Science, the journal of the Cognitive Science Society,

117
00:09:17,480 --> 00:09:21,480
or the back prop paper was published in Nature, which is a general interest science journal,

118
00:09:21,480 --> 00:09:25,480
but by people who are mostly affiliated with the Institute for Cognitive Science in San Diego.

119
00:09:25,480 --> 00:09:30,480
What you see here is already a long history of scientists thinking like engineers.

120
00:09:30,480 --> 00:09:33,480
These are people who are in psychology or cognitive science departments

121
00:09:33,480 --> 00:09:35,480
and publishing in those places,

122
00:09:35,480 --> 00:09:40,480
but by formalizing even very basic insights about how humans might learn

123
00:09:40,480 --> 00:09:44,480
or how brains might learn in the right kind of math,

124
00:09:44,480 --> 00:09:47,480
that led to, of course, progress on the science side,

125
00:09:47,480 --> 00:09:49,480
but it led to all the engineering that we see now.

126
00:09:49,480 --> 00:09:51,480
It wasn't sufficient.

127
00:09:51,480 --> 00:09:54,480
We needed, of course, lots of innovations and advances

128
00:09:54,480 --> 00:09:57,480
in computing, hardware, and software systems.

129
00:09:57,480 --> 00:10:00,480
But this is where the basic math came from

130
00:10:00,480 --> 00:10:03,480
and it came from doing science like an engineer.

131
00:10:03,480 --> 00:10:06,480
What we want to talk about in our vision is what does the future of this look like?

132
00:10:06,480 --> 00:10:08,480
If we were to look 50 years into the future,

133
00:10:08,480 --> 00:10:11,480
what would we be looking back on now over this time scale?

134
00:10:11,480 --> 00:10:15,480
Well, here's a long-term research roadmap that reflects some of my ambitions

135
00:10:15,480 --> 00:10:18,480
and some of our center's goals and many others, too.

136
00:10:18,480 --> 00:10:22,480
We'd like to be able to address basic questions, fundamental questions

137
00:10:22,480 --> 00:10:24,480
of what it is to be and to think like a human,

138
00:10:24,480 --> 00:10:29,480
questions, for example, of consciousness or meaning in language or real learning,

139
00:10:29,480 --> 00:10:33,480
questions like, you know, even beyond the individual, like questions of culture

140
00:10:33,480 --> 00:10:35,480
or creativity.

141
00:10:35,480 --> 00:10:37,480
Those are our big ideas up there.

142
00:10:37,480 --> 00:10:39,480
And for each of these, there are basic scientific questions, right?

143
00:10:39,480 --> 00:10:42,480
How do we become aware of the world and ourselves in it?

144
00:10:42,480 --> 00:10:45,480
It starts with perception, but it really turns into awareness,

145
00:10:45,480 --> 00:10:49,480
awareness of yourself and of the world and what we might call consciousness, right?

146
00:10:49,480 --> 00:10:51,480
Or how does a word start to have a meaning?

147
00:10:51,480 --> 00:10:54,480
What really is a meaning and how does a child grasp it?

148
00:10:54,480 --> 00:10:55,480
Or how do children actually learn?

149
00:10:55,480 --> 00:10:57,480
What do babies' brains actually start with?

150
00:10:57,480 --> 00:11:00,480
Are they blank slates or do they start with some kind of cognitive structure?

151
00:11:00,480 --> 00:11:02,480
And then what does real learning look like?

152
00:11:02,480 --> 00:11:05,480
These are just some of the questions that we're interested in working on.

153
00:11:05,480 --> 00:11:07,480
Or when we talk about culture, we mean,

154
00:11:07,480 --> 00:11:11,480
how do you learn all the things you didn't directly experience, right?

155
00:11:11,480 --> 00:11:15,480
But that's somehow you got from the accumulation of knowledge in society over many generations.

156
00:11:15,480 --> 00:11:18,480
Or how do you ever think of new ideas or answers to new questions?

157
00:11:18,480 --> 00:11:20,480
How do you think of the new questions themselves?

158
00:11:20,480 --> 00:11:21,480
How do you decide what to think about?

159
00:11:21,480 --> 00:11:24,480
These are all key activities of human intelligence

160
00:11:24,480 --> 00:11:27,480
when we talk about how we model the world, where our models come from,

161
00:11:27,480 --> 00:11:29,480
what we do with our models, this is what we're talking about.

162
00:11:29,480 --> 00:11:31,480
And if we could get machines that could do these things,

163
00:11:31,480 --> 00:11:36,480
well, again, on the bottom row, think of all the actual real engineering payoffs.

164
00:11:36,480 --> 00:11:40,480
Now, in our center, in both my own activities and a lot of what my group does these days

165
00:11:40,480 --> 00:11:44,480
and what a number of other colleagues in the center for brains, minds, and machines do,

166
00:11:44,480 --> 00:11:47,480
as well as very broadly people in BCS and CSAIL,

167
00:11:47,480 --> 00:11:50,480
one place where we work on the beginnings of these problems in the near term.

168
00:11:50,480 --> 00:11:53,480
This is the long term, like, think 50 years, okay?

169
00:11:53,480 --> 00:11:59,480
Maybe shorter, maybe longer, I don't know, but think well beyond 10 years, okay?

170
00:11:59,480 --> 00:12:03,480
But in the short term, 5 to 10 years, a lot of our focus is around visual intelligence.

171
00:12:03,480 --> 00:12:04,480
And there's many reasons for that.

172
00:12:04,480 --> 00:12:07,480
Again, we can build on the successes of deep networks

173
00:12:07,480 --> 00:12:09,480
and a lot of pattern recognition and machine vision.

174
00:12:09,480 --> 00:12:11,480
It's a good way to put these ideas into practice.

175
00:12:11,480 --> 00:12:15,480
When we look at the actual brain, the visual system in the brain,

176
00:12:15,480 --> 00:12:18,480
in the human and other mammalian brains, for example,

177
00:12:18,480 --> 00:12:21,480
is really very clearly the best understood part of the brain.

178
00:12:21,480 --> 00:12:26,480
And at a circuit level, it's the part of the brain that's most inspired current deep learning

179
00:12:26,480 --> 00:12:27,480
and neural network systems.

180
00:12:27,480 --> 00:12:32,480
But even there, there's things which we still don't really understand like engineers.

181
00:12:32,480 --> 00:12:35,480
So here's an example of a basic problem in visual intelligence

182
00:12:35,480 --> 00:12:39,480
that we and others in the center are trying to solve.

183
00:12:39,480 --> 00:12:44,480
Look around you and you feel like there's a whole world around you.

184
00:12:44,480 --> 00:12:45,480
And there is a whole world around you.

185
00:12:45,480 --> 00:12:47,480
You feel like your brain captures it.

186
00:12:47,480 --> 00:12:50,480
But what the actual sense data that's coming in through your eyes

187
00:12:50,480 --> 00:12:53,480
looks more like this photograph here where you can see there's a crowd scene

188
00:12:53,480 --> 00:12:57,480
but it's mostly blurry except for a small region of high resolution in the center.

189
00:12:57,480 --> 00:13:01,480
So that corresponds biologically to what part of the image is in your fovea.

190
00:13:01,480 --> 00:13:03,480
That's the central region of cells in the retina

191
00:13:03,480 --> 00:13:06,480
where you have really high resolution visual data.

192
00:13:06,480 --> 00:13:10,480
The size of your fovea is roughly like if you hold out your thumb at arm's length.

193
00:13:10,480 --> 00:13:12,480
It's a little bit bigger than that but not much bigger, right?

194
00:13:12,480 --> 00:13:16,480
Most of the image, in terms of the actual information coming in

195
00:13:16,480 --> 00:13:19,480
in a bottom-up sense to your brain is really quite blurry.

196
00:13:19,480 --> 00:13:23,480
But somehow by looking at just one part and then by saccading around

197
00:13:23,480 --> 00:13:26,480
or making a few eye movements, you get a few glimpses,

198
00:13:26,480 --> 00:13:29,480
each not much bigger than the size of your thumb at arm's length.

199
00:13:29,480 --> 00:13:32,480
Somehow you stitch that information together into what feels like

200
00:13:32,480 --> 00:13:35,480
and really is a rich representation of the whole world around you.

201
00:13:35,480 --> 00:13:38,480
And when I say around you, I mean literally around you.

202
00:13:38,480 --> 00:13:40,480
So here's another kind of demonstration.

203
00:13:40,480 --> 00:13:44,480
Without turning around, nobody's allowed to turn around.

204
00:13:44,480 --> 00:13:46,480
Ask yourself what's behind you.

205
00:13:46,480 --> 00:13:48,480
Now the answer is going to be different for different people

206
00:13:48,480 --> 00:13:50,480
depending on where you're sitting, right?

207
00:13:50,480 --> 00:13:53,480
For most of you, you might think, well, I think there's a person

208
00:13:53,480 --> 00:13:55,480
pretty close behind me, right?

209
00:13:55,480 --> 00:13:57,480
You know you're in a crowded auditorium, although you haven't seen that person.

210
00:13:57,480 --> 00:14:00,480
You know that they're there, right?

211
00:14:00,480 --> 00:14:03,480
For people in the very back row, you know there isn't a person behind you

212
00:14:03,480 --> 00:14:05,480
and you're conscious of being in the back row, right?

213
00:14:05,480 --> 00:14:07,480
You might be conscious that there's a wall right behind you.

214
00:14:07,480 --> 00:14:11,480
But now for the people who are in the room, not in the very back,

215
00:14:11,480 --> 00:14:13,480
think about how far behind you is the back,

216
00:14:13,480 --> 00:14:15,480
like where's the nearest wall behind you?

217
00:14:15,480 --> 00:14:18,480
Maybe we can call out, try a little demonstration.

218
00:14:18,480 --> 00:14:20,480
So I don't know. I'm pointing to someone there.

219
00:14:20,480 --> 00:14:23,480
Can you say something if you think I'm pointing at you?

220
00:14:23,480 --> 00:14:25,480
Well, I could have been pointing at you,

221
00:14:25,480 --> 00:14:27,480
but I'm pointing someone behind you, okay?

222
00:14:27,480 --> 00:14:29,480
I'll point to you. Yeah, I'm pointing to you.

223
00:14:29,480 --> 00:14:31,480
All right, so how far is the nearest wall...

224
00:14:31,480 --> 00:14:33,480
No, you can't turn around. You've blown your chance.

225
00:14:33,480 --> 00:14:36,480
Without turning around. Okay, so you were...

226
00:14:36,480 --> 00:14:38,480
Okay, do you see I'm pointing to you there with the tie?

227
00:14:38,480 --> 00:14:42,480
Okay, so without turning around, how far is the nearest wall behind you?

228
00:14:46,480 --> 00:14:48,480
Sorry, how far?

229
00:14:48,480 --> 00:14:51,480
Five meters. Okay, well, I mean, that might be about right.

230
00:14:51,480 --> 00:14:54,480
Other people can turn around.

231
00:14:54,480 --> 00:14:57,480
Now, how about you? How far is the nearest wall behind you?

232
00:14:57,480 --> 00:14:59,480
Ten meters.

233
00:14:59,480 --> 00:15:01,480
Ten meters, okay.

234
00:15:01,480 --> 00:15:03,480
That might be right, yeah.

235
00:15:03,480 --> 00:15:06,480
How about here? What do you think?

236
00:15:06,480 --> 00:15:08,480
20, okay.

237
00:15:08,480 --> 00:15:10,480
So yeah, since I didn't grow up in the metric system, I barely know.

238
00:15:10,480 --> 00:15:12,480
But yeah, I mean...

239
00:15:12,480 --> 00:15:14,480
The point is that like, you're...

240
00:15:14,480 --> 00:15:17,480
Each of you is surely not exactly right,

241
00:15:17,480 --> 00:15:19,480
but you're certainly within an order of magnitude,

242
00:15:19,480 --> 00:15:22,480
and I guess if we actually tried to measure, you know, you're probably...

243
00:15:22,480 --> 00:15:25,480
My guess is you're probably right within, you know, 50% or less,

244
00:15:25,480 --> 00:15:28,480
often, you know, maybe just 20% error.

245
00:15:28,480 --> 00:15:30,480
Okay, so how do you know this?

246
00:15:30,480 --> 00:15:32,480
I mean, even if it's not... What did you say, 20 meters?

247
00:15:32,480 --> 00:15:35,480
Even if it's not 20 meters, it's probably closer to 20 meters

248
00:15:35,480 --> 00:15:38,480
than it is to 5 or 10 meters, and then it is to 50 meters.

249
00:15:38,480 --> 00:15:41,480
So how did you know this? You haven't turned around in a while, right?

250
00:15:41,480 --> 00:15:45,480
So how do you know if your brain is tracking the whole world around you, right?

251
00:15:45,480 --> 00:15:48,480
And how many people are behind you?

252
00:15:48,480 --> 00:15:50,480
Yeah, like a few hundred, right?

253
00:15:50,480 --> 00:15:52,480
I mean, I don't know if it's 200 or 300 or...

254
00:15:52,480 --> 00:15:54,480
But it's not a thousand.

255
00:15:54,480 --> 00:15:56,480
I don't think so.

256
00:15:56,480 --> 00:15:58,480
And it's certainly not 10 or 20 or 50, right?

257
00:15:58,480 --> 00:16:02,480
So you track these things, and you use them to plan your actions.

258
00:16:02,480 --> 00:16:05,480
Okay, so again, think about how instantly, effortlessly,

259
00:16:05,480 --> 00:16:07,480
and very reliably, okay?

260
00:16:07,480 --> 00:16:09,480
Your brain computes all these things.

261
00:16:09,480 --> 00:16:11,480
There's a lot of people and objects around you,

262
00:16:11,480 --> 00:16:13,480
and it's not just approximations.

263
00:16:13,480 --> 00:16:16,480
Certainly, when we're talking about what's behind you in space,

264
00:16:16,480 --> 00:16:18,480
there's a lot of imprecision.

265
00:16:18,480 --> 00:16:20,480
But when it comes to reaching for things right in front of you,

266
00:16:20,480 --> 00:16:23,480
very precise shape and physical property estimates

267
00:16:23,480 --> 00:16:25,480
needed to pick up and manipulate objects.

268
00:16:25,480 --> 00:16:28,480
And then when it comes to people, it's not just the existence of the people,

269
00:16:28,480 --> 00:16:30,480
but something about what's in their head, right?

270
00:16:30,480 --> 00:16:32,480
You track whether someone's paying attention to you

271
00:16:32,480 --> 00:16:34,480
and you're talking to them, what they might want from you,

272
00:16:34,480 --> 00:16:36,480
what they might be thinking about you,

273
00:16:36,480 --> 00:16:38,480
what they might be thinking about other people, okay?

274
00:16:38,480 --> 00:16:40,480
So when we talk about visual intelligence,

275
00:16:40,480 --> 00:16:42,480
this is the whole stuff we're talking about.

276
00:16:42,480 --> 00:16:45,480
And you can start to see how it turns into basic questions, I think,

277
00:16:45,480 --> 00:16:49,480
of what we might call the beginnings of consciousness,

278
00:16:49,480 --> 00:16:52,480
or at least our awareness of our self in the world,

279
00:16:52,480 --> 00:16:55,480
and of ourselves as a self in the world,

280
00:16:55,480 --> 00:16:58,480
but also other aspects of higher level intelligence and cognition

281
00:16:58,480 --> 00:17:00,480
that are not just about perception, like symbols, right?

282
00:17:00,480 --> 00:17:04,480
To describe even to ourselves what's around us and where we are

283
00:17:04,480 --> 00:17:06,480
and what we can do with it.

284
00:17:06,480 --> 00:17:10,480
We can go beyond just what we would normally call the stuff of perception

285
00:17:10,480 --> 00:17:14,480
to say the thoughts in somebody's head and your own thoughts about that, okay?

286
00:17:14,480 --> 00:17:16,480
So what we've been doing in CBMM

287
00:17:16,480 --> 00:17:19,480
is trying to develop an architecture for visual intelligence.

288
00:17:19,480 --> 00:17:22,480
And I'm not going to go into any of the details of how this works,

289
00:17:22,480 --> 00:17:24,480
and this is just notional, this is just a picture,

290
00:17:24,480 --> 00:17:27,480
it's like a sketch from a grand proposal of what we say we want to do.

291
00:17:27,480 --> 00:17:30,480
But it's based on a lot of scientific understanding

292
00:17:30,480 --> 00:17:32,480
of how the brain works.

293
00:17:32,480 --> 00:17:35,480
There are different parts of the brain that correspond to these different modules

294
00:17:35,480 --> 00:17:38,480
of architecture, as well as some kind of emerging engineering way

295
00:17:38,480 --> 00:17:41,480
to try to capture at the software and maybe even hardware levels

296
00:17:41,480 --> 00:17:43,480
how these modules might work.

297
00:17:43,480 --> 00:17:46,480
So we talk about sort of an early module of visual or perceptual stream,

298
00:17:46,480 --> 00:17:50,480
which is like bottom-up visual or other perceptual input.

299
00:17:50,480 --> 00:17:53,480
That's the kind of thing that is pretty close to what we currently have

300
00:17:53,480 --> 00:17:55,480
in say deep convolutional neural networks.

301
00:17:55,480 --> 00:18:00,480
But then we talk about some kind of, the output of that isn't just pattern class labels,

302
00:18:00,480 --> 00:18:03,480
but what we call the cognitive core or core cognition.

303
00:18:03,480 --> 00:18:06,480
And an understanding of space and objects, their physics,

304
00:18:06,480 --> 00:18:09,480
other people, their minds, that's the real stuff of cognition

305
00:18:09,480 --> 00:18:12,480
that has to be the output of perception.

306
00:18:12,480 --> 00:18:16,480
But somehow we have to have, this is what we call the brain OS in this picture,

307
00:18:16,480 --> 00:18:19,480
we have to get there by stitching together the bottom-up inputs

308
00:18:19,480 --> 00:18:22,480
from a glimpse here, a glimpse here, a little bit here and there,

309
00:18:22,480 --> 00:18:25,480
and accessing prior knowledge that comes from our memory systems

310
00:18:25,480 --> 00:18:28,480
to tell us how to stitch these things together

311
00:18:28,480 --> 00:18:32,480
into the really core cognitive representations of what's out there in the world.

312
00:18:32,480 --> 00:18:35,480
And then if we're going to start to talk about it in language

313
00:18:35,480 --> 00:18:40,480
or to build plans on top of what we have seen and understood,

314
00:18:40,480 --> 00:18:44,480
that's where we talk about symbols coming into the picture.

315
00:18:44,480 --> 00:18:48,480
The building blocks of language and plans and so on.

316
00:18:48,480 --> 00:18:51,480
So now we might say, well, okay, this is an architecture

317
00:18:51,480 --> 00:18:54,480
that is brain-inspired and cognitively inspired

318
00:18:54,480 --> 00:18:57,480
and we're planning to turn into real engineering.

319
00:18:57,480 --> 00:18:59,480
And you can say, well, do we need that?

320
00:18:59,480 --> 00:19:02,480
Again, I know this is a question you considered in the first lecture.

321
00:19:02,480 --> 00:19:06,480
Maybe the engineering toolkit that's currently been making a lot of progress

322
00:19:06,480 --> 00:19:08,480
in, let's say, industry, maybe that's good enough.

323
00:19:08,480 --> 00:19:10,480
Maybe, you know, let's take deep learning

324
00:19:10,480 --> 00:19:14,480
but to stand for a broader set of modern pattern recognition-based

325
00:19:14,480 --> 00:19:16,480
and reinforcement learning-based tools

326
00:19:16,480 --> 00:19:20,480
and say, okay, well, maybe that can scale up to this.

327
00:19:20,480 --> 00:19:22,480
And you might, you know, maybe that's possible.

328
00:19:22,480 --> 00:19:24,480
I'm happy in the question period if people want to debate this.

329
00:19:24,480 --> 00:19:26,480
My sense is, no.

330
00:19:26,480 --> 00:19:29,480
I think that it's not, when I say no,

331
00:19:29,480 --> 00:19:32,480
I don't mean like it can't happen or it won't happen.

332
00:19:32,480 --> 00:19:36,480
What I mean is the highest value, the highest expected route right now

333
00:19:36,480 --> 00:19:39,480
is to take this more science-based reverse engineering approach.

334
00:19:39,480 --> 00:19:42,480
And that at least if you follow the current trajectory

335
00:19:42,480 --> 00:19:45,480
that industry incentives especially optimize for,

336
00:19:45,480 --> 00:19:48,480
it's not even really trying to take us to these things.

337
00:19:48,480 --> 00:19:51,480
So think about, for example, a case study of visual intelligence

338
00:19:51,480 --> 00:19:54,480
that is in some ways, as pattern recognition, very much of a success.

339
00:19:54,480 --> 00:19:56,480
It's again been mostly driven by industry.

340
00:19:56,480 --> 00:19:59,480
It's something that if you read in the news

341
00:19:59,480 --> 00:20:02,480
or even play around with in certain publicly available data sets

342
00:20:02,480 --> 00:20:04,480
feels like we've made great progress.

343
00:20:04,480 --> 00:20:06,480
And this is an aspect of visual intelligence

344
00:20:06,480 --> 00:20:09,480
which is sometimes called image captioning.

345
00:20:09,480 --> 00:20:12,480
It's mapping images to text.

346
00:20:12,480 --> 00:20:14,480
You know, basically there's been a bunch of systems.

347
00:20:14,480 --> 00:20:16,480
Here's a couple of press releases.

348
00:20:16,480 --> 00:20:18,480
I guess this one's about Google.

349
00:20:18,480 --> 00:20:21,480
Google's AI can now capture images almost as well as humans.

350
00:20:21,480 --> 00:20:23,480
Here's one's about Microsoft.

351
00:20:23,480 --> 00:20:26,480
A couple of years ago, I think there were something like

352
00:20:26,480 --> 00:20:29,480
eight papers all released on to archive around the same time

353
00:20:29,480 --> 00:20:32,480
from basically all the major industry computer vision groups

354
00:20:32,480 --> 00:20:35,480
as well as a couple of academic partners, okay,

355
00:20:35,480 --> 00:20:38,480
which all driven by basically the same data set

356
00:20:38,480 --> 00:20:41,480
produced by some Microsoft researchers and other collaborators,

357
00:20:41,480 --> 00:20:44,480
trained a combination of deep convolutional neural networks,

358
00:20:44,480 --> 00:20:47,480
you know, state-of-the-art visual pattern recognition

359
00:20:47,480 --> 00:20:50,480
with recurrent neural networks which had recently been developed

360
00:20:50,480 --> 00:20:53,480
for, you know, basically kinds of neural statistical language modeling,

361
00:20:53,480 --> 00:20:55,480
glued them together and produced a system

362
00:20:55,480 --> 00:20:59,480
which made very impressive results in a big training set

363
00:20:59,480 --> 00:21:02,480
and a held out test set where the goal was to take an image

364
00:21:02,480 --> 00:21:05,480
and write a sentence like a short sentence caption

365
00:21:05,480 --> 00:21:09,480
that would seem like the kind of way a human would describe that image.

366
00:21:09,480 --> 00:21:12,480
And these systems, you know, surpassed human level accuracy

367
00:21:12,480 --> 00:21:15,480
on the held out test set from a big training set.

368
00:21:15,480 --> 00:21:17,480
But what you can see when you really dig into these things

369
00:21:17,480 --> 00:21:20,480
is there's often a lot of what I would call data set overfitting.

370
00:21:20,480 --> 00:21:22,480
It's not overfitting to the training set,

371
00:21:22,480 --> 00:21:25,480
but it's overfitting to whatever are the particular characteristics

372
00:21:25,480 --> 00:21:28,480
of this data set, you know, wherever it came from,

373
00:21:28,480 --> 00:21:31,480
certain set of photographs and certain ways of captioning them, okay,

374
00:21:31,480 --> 00:21:34,480
which even a big data set, it's not about quantity,

375
00:21:34,480 --> 00:21:38,480
it's more about the quality, the nature of what people are doing, all right.

376
00:21:38,480 --> 00:21:41,480
So one way to test this system is to apply it

377
00:21:41,480 --> 00:21:44,480
to what seems like basically the same problem

378
00:21:44,480 --> 00:21:48,480
but not within a certain curated or built data set.

379
00:21:48,480 --> 00:21:51,480
And there's a convenient Twitter bot that lets you do this.

380
00:21:51,480 --> 00:21:53,480
So there's something called the PIC desk bot,

381
00:21:53,480 --> 00:21:57,480
which takes one of the state of the art industry AI captioning systems,

382
00:21:57,480 --> 00:21:59,480
a very good one. Again, this is not meant to,

383
00:21:59,480 --> 00:22:02,480
I'm not trying to critique these systems for what they're trying to do,

384
00:22:02,480 --> 00:22:04,480
I'm just trying to point out what they don't really even try to do.

385
00:22:04,480 --> 00:22:07,480
So this takes the Microsoft caption bot

386
00:22:07,480 --> 00:22:10,480
and just every couple of hours takes a random image from the web,

387
00:22:10,480 --> 00:22:13,480
captions it and uploads the results to Twitter.

388
00:22:13,480 --> 00:22:16,480
And a couple of months ago when I prepared the first version of this talk,

389
00:22:16,480 --> 00:22:19,480
I just took a few days in the life of this Twitter bot.

390
00:22:19,480 --> 00:22:22,480
I didn't take every single image, but I took, you know,

391
00:22:22,480 --> 00:22:25,480
most of the images in a way that was meant to be representative of the successes

392
00:22:25,480 --> 00:22:28,480
and the kinds of failures that such a system will make.

393
00:22:28,480 --> 00:22:30,480
So we can go through this and it's a little bit entertaining

394
00:22:30,480 --> 00:22:32,480
and I think quite informative.

395
00:22:32,480 --> 00:22:37,480
So here's just a somewhat random sample of a few days in the life

396
00:22:37,480 --> 00:22:39,480
of one of these caption bots.

397
00:22:39,480 --> 00:22:42,480
So here we have a picture of a person holding,

398
00:22:42,480 --> 00:22:44,480
fortunately my screen is very small here and I can't read up there,

399
00:22:44,480 --> 00:22:46,480
so maybe you'll have to tell me what it is.

400
00:22:46,480 --> 00:22:48,480
But a person holding a cell phone, I guess I'll just read along with you.

401
00:22:48,480 --> 00:22:50,480
So you have a person holding a cell phone.

402
00:22:50,480 --> 00:22:52,480
Well, it's not a person holding a cell phone, but it's kind of close.

403
00:22:52,480 --> 00:22:54,480
It's a person holding some kind of machine.

404
00:22:54,480 --> 00:22:58,480
I don't even know what that is, but it's some kind of musical instrument, right?

405
00:22:58,480 --> 00:23:01,480
So that's a mixed success or failure.

406
00:23:01,480 --> 00:23:05,480
Here's a pretty good one, a group of people on a field playing football.

407
00:23:05,480 --> 00:23:09,480
I would call that an A result, maybe even A plus.

408
00:23:09,480 --> 00:23:12,480
Here's a group of people standing on top of a mountain.

409
00:23:12,480 --> 00:23:15,480
So less good, there's a mountain, but as far as I can tell there's no people.

410
00:23:15,480 --> 00:23:18,480
But these systems like to see people because of both the combination,

411
00:23:18,480 --> 00:23:20,480
because in the data set they were trained on there's a lot of people

412
00:23:20,480 --> 00:23:22,480
and people often talk about people.

413
00:23:22,480 --> 00:23:27,480
And the fact that you can appreciate both what I said and why it's funny,

414
00:23:27,480 --> 00:23:32,480
there you did some of my cognitive activities that this system is not even trying to do.

415
00:23:32,480 --> 00:23:34,480
Here we've got a building with a cake.

416
00:23:34,480 --> 00:23:35,480
I'll go through these fast.

417
00:23:35,480 --> 00:23:38,480
Building with a cake, a large stone building with a clock tower.

418
00:23:38,480 --> 00:23:39,480
I think that's pretty good.

419
00:23:39,480 --> 00:23:40,480
I'd give that like a B plus.

420
00:23:40,480 --> 00:23:42,480
There's no clock, but it's plausibly right.

421
00:23:42,480 --> 00:23:44,480
There might be a clock in there.

422
00:23:44,480 --> 00:23:45,480
There's definitely something like that.

423
00:23:45,480 --> 00:23:47,480
Here's a truck parked on the side of a building.

424
00:23:47,480 --> 00:23:49,480
I don't know, maybe a B minus.

425
00:23:49,480 --> 00:23:52,480
There is a car on the side of a building, but it's not a truck

426
00:23:52,480 --> 00:23:55,480
and it doesn't seem like the main thing in the image.

427
00:23:55,480 --> 00:23:57,480
Here's a necklace made of bananas.

428
00:23:57,480 --> 00:24:01,480
Here's a large ship in the water.

429
00:24:01,480 --> 00:24:02,480
This is pretty good.

430
00:24:02,480 --> 00:24:05,480
I give this like an A minus or B plus because there is a ship in the water,

431
00:24:05,480 --> 00:24:06,480
but it's not very large.

432
00:24:06,480 --> 00:24:08,480
It's really more of like a tugboat or something.

433
00:24:08,480 --> 00:24:10,480
Here's a sign sitting on the grass.

434
00:24:10,480 --> 00:24:11,480
You know, in some sense, that's great.

435
00:24:11,480 --> 00:24:14,480
No, but in another sense, it's really missing what's actually

436
00:24:14,480 --> 00:24:17,480
interesting and important and meaningful to humans.

437
00:24:17,480 --> 00:24:22,480
Here's a garden is in the dirt.

438
00:24:22,480 --> 00:24:24,480
A pizza sitting on top of a building.

439
00:24:24,480 --> 00:24:26,480
A small house with a red brick building.

440
00:24:26,480 --> 00:24:28,480
That's pretty good, although a kind of weird way of saying it.

441
00:24:28,480 --> 00:24:30,480
A vintage photo of a pond.

442
00:24:30,480 --> 00:24:31,480
That's good.

443
00:24:31,480 --> 00:24:32,480
They like vintage photos.

444
00:24:32,480 --> 00:24:34,480
A group of people that are standing in the grass near a bridge.

445
00:24:34,480 --> 00:24:36,480
Again, there's two people and there's some grass and there's a bridge,

446
00:24:36,480 --> 00:24:39,480
but it's really not what's going on.

447
00:24:39,480 --> 00:24:42,480
A person in a yard, kind of.

448
00:24:42,480 --> 00:24:44,480
A group of people standing on top of a boat.

449
00:24:44,480 --> 00:24:45,480
There's a boat.

450
00:24:45,480 --> 00:24:46,480
There's a group of people.

451
00:24:46,480 --> 00:24:47,480
They're standing.

452
00:24:47,480 --> 00:24:51,480
But again, the sentence that you see is more based on a bias of what people

453
00:24:51,480 --> 00:24:55,480
have said in the past about images that are only vaguely like this.

454
00:24:55,480 --> 00:24:56,480
A clock tower is a little bit night.

455
00:24:56,480 --> 00:24:58,480
That's really, I think, pretty impressive.

456
00:24:58,480 --> 00:25:00,480
A large clock mounted to the side of a building.

457
00:25:00,480 --> 00:25:01,480
A little bit less so.

458
00:25:01,480 --> 00:25:03,480
A snow-covered feel, very good.

459
00:25:03,480 --> 00:25:05,480
A building with snow on the ground.

460
00:25:05,480 --> 00:25:06,480
A little bit less good.

461
00:25:06,480 --> 00:25:07,480
There's no snow.

462
00:25:07,480 --> 00:25:08,480
It's white.

463
00:25:08,480 --> 00:25:11,480
Some people who I don't know them, but I bet that's probably right

464
00:25:11,480 --> 00:25:14,480
because identifying faces and recognizing people who are famous

465
00:25:14,480 --> 00:25:16,480
because they won medals in the Olympics,

466
00:25:16,480 --> 00:25:20,480
probably I would trust current pattern recognition systems to get that.

467
00:25:20,480 --> 00:25:22,480
A painting of a vase in front of a mirror.

468
00:25:22,480 --> 00:25:23,480
Less good.

469
00:25:23,480 --> 00:25:26,480
Also a famous person there, but we didn't get him.

470
00:25:26,480 --> 00:25:28,480
A person walking in the rain.

471
00:25:28,480 --> 00:25:31,480
Again, there is sort of a person and there's some puddles.

472
00:25:31,480 --> 00:25:35,480
A group of stuffed animals.

473
00:25:35,480 --> 00:25:37,480
A car parked in a parking lot.

474
00:25:37,480 --> 00:25:38,480
That's good.

475
00:25:38,480 --> 00:25:40,480
A car parked in front of a building.

476
00:25:40,480 --> 00:25:41,480
Less good.

477
00:25:41,480 --> 00:25:43,480
A plate with a fork and knife.

478
00:25:43,480 --> 00:25:44,480
A clear blue sky.

479
00:25:44,480 --> 00:25:45,480
Okay.

480
00:25:45,480 --> 00:25:46,480
So you get the idea.

481
00:25:46,480 --> 00:25:49,480
Again, if you actually go and play with this system,

482
00:25:49,480 --> 00:25:53,480
partly because I think my friends at Microsoft told me they've improved it some.

483
00:25:53,480 --> 00:25:56,480
This is partly for entertainment values.

484
00:25:56,480 --> 00:25:58,480
I chose what also would be the funnier examples.

485
00:25:58,480 --> 00:26:00,480
So I want to be quite honest about this.

486
00:26:00,480 --> 00:26:04,480
I'm not trying to take away what are impressive AI technologies,

487
00:26:04,480 --> 00:26:09,480
but I think it's clear that there's a sense of understanding any one of these images,

488
00:26:09,480 --> 00:26:12,480
that it's important to see that even when it seems to be correct,

489
00:26:12,480 --> 00:26:15,480
if it can make the kind of errors that it makes,

490
00:26:15,480 --> 00:26:17,480
that even when it seems to be correct,

491
00:26:17,480 --> 00:26:19,480
it's probably not doing what you're doing

492
00:26:19,480 --> 00:26:22,480
and it's probably not even trying to scale towards the dimensions of intelligence

493
00:26:22,480 --> 00:26:25,480
that we think about when we're talking about human intelligence.

494
00:26:25,480 --> 00:26:27,480
Another way to put this,

495
00:26:27,480 --> 00:26:30,480
I want to show you a really insightful blog post from one of your other speakers.

496
00:26:30,480 --> 00:26:32,480
So in a couple of days,

497
00:26:32,480 --> 00:26:34,480
I'm not sure you're going to have Andre Carpathi,

498
00:26:34,480 --> 00:26:37,480
who's one of the leading people in deep learning.

499
00:26:37,480 --> 00:26:41,480
This is a really great blog post he wrote a couple of years ago,

500
00:26:41,480 --> 00:26:43,480
when he was, I think, still at Stanford.

501
00:26:43,480 --> 00:26:45,480
He got his PhD from Stanford.

502
00:26:45,480 --> 00:26:50,480
He worked at Google a little bit on some early big neural net AI projects there.

503
00:26:50,480 --> 00:26:52,480
He was at OpenAI.

504
00:26:52,480 --> 00:26:54,480
He was one of the founders of OpenAI,

505
00:26:54,480 --> 00:26:58,480
and recently he joined Tesla as their director of AI research.

506
00:26:58,480 --> 00:27:00,480
But about five years ago,

507
00:27:00,480 --> 00:27:04,480
he was looking at the state of computer vision from a human intelligence point of view

508
00:27:04,480 --> 00:27:06,480
and lamenting how far away we were.

509
00:27:06,480 --> 00:27:08,480
So this is the title of his blog post,

510
00:27:08,480 --> 00:27:10,480
the state of computer vision and AI.

511
00:27:10,480 --> 00:27:12,480
We are really, really far away.

512
00:27:12,480 --> 00:27:16,480
And he took this image, which was a sort of a famous image in its own right.

513
00:27:16,480 --> 00:27:19,480
It was a popular image of Obama back when he was president,

514
00:27:19,480 --> 00:27:22,480
kind of playing around as he liked to do when he was on tour.

515
00:27:22,480 --> 00:27:24,480
If you take a look at this, you can see,

516
00:27:24,480 --> 00:27:27,480
you probably all can recognize the previous president of the United States,

517
00:27:27,480 --> 00:27:30,480
but you can also get the sense of where he is and what's going on,

518
00:27:30,480 --> 00:27:32,480
and you might see people smiling,

519
00:27:32,480 --> 00:27:34,480
and you might get the sense that he's playing a joke on someone.

520
00:27:34,480 --> 00:27:36,480
Can you see that?

521
00:27:36,480 --> 00:27:39,480
So how do you know that he's playing a joke and what that joke is?

522
00:27:39,480 --> 00:27:42,480
Well, as Andre goes on to talk about in his blog post,

523
00:27:42,480 --> 00:27:47,480
if you think about all the things that you have to really deploy in your mind to understand that,

524
00:27:47,480 --> 00:27:49,480
it's a huge list.

525
00:27:49,480 --> 00:27:53,480
Of course, it starts with seeing people and objects and maybe doing some face recognition,

526
00:27:53,480 --> 00:27:55,480
but you have to do things like, for example,

527
00:27:55,480 --> 00:27:59,480
notice his foot on the scale and understand enough about how scales work,

528
00:27:59,480 --> 00:28:01,480
that when a foot presses down, it exerts force,

529
00:28:01,480 --> 00:28:04,480
that the scale is sensitive, doesn't just magically measure people's weight,

530
00:28:04,480 --> 00:28:06,480
but it does that somehow through force.

531
00:28:06,480 --> 00:28:09,480
You have to see who can see that he's doing that and who can't,

532
00:28:09,480 --> 00:28:11,480
who cannot see that he's doing that, right,

533
00:28:11,480 --> 00:28:13,480
in particular the person on the scale,

534
00:28:13,480 --> 00:28:15,480
and why some people can see that he's doing that

535
00:28:15,480 --> 00:28:17,480
and can see that some other people can't see it,

536
00:28:17,480 --> 00:28:19,480
and that makes it funny to them.

537
00:28:19,480 --> 00:28:23,480
And someday, we should have machines that can understand this,

538
00:28:23,480 --> 00:28:29,480
but hopefully you can see why the kind of architecture that I'm talking about

539
00:28:29,480 --> 00:28:34,480
would be the building blocks of the ingredients to be able to get them to do that.

540
00:28:34,480 --> 00:28:37,480
Now, again, I prepared a version of this talk a few months ago

541
00:28:37,480 --> 00:28:40,480
and I wrote to Andre and I said, I was going to use this

542
00:28:40,480 --> 00:28:44,480
and I was curious if he had any reflections on this

543
00:28:44,480 --> 00:28:47,480
and where he thought we were relative to five years ago,

544
00:28:47,480 --> 00:28:49,480
because certainly a lot of progress has been made.

545
00:28:49,480 --> 00:28:53,480
But he said, here's his email, I hope he doesn't mind me sharing it,

546
00:28:53,480 --> 00:28:55,480
but I mean, again, he's a very honest person

547
00:28:55,480 --> 00:28:58,480
and that's one of the many reasons why he's such an important person right now in AI.

548
00:28:58,480 --> 00:29:01,480
He's both very technically strong and honest about what we can do what we can't do,

549
00:29:01,480 --> 00:29:03,480
and as he says, what does he say?

550
00:29:03,480 --> 00:29:06,480
It's nice to hear from you, it's fun you should bring this up.

551
00:29:06,480 --> 00:29:09,480
I was also thinking about writing a return to this

552
00:29:09,480 --> 00:29:12,480
and in short, basically I don't believe we've made very much progress, right?

553
00:29:12,480 --> 00:29:16,480
He points out that in his long list of things that you need to understand the image,

554
00:29:16,480 --> 00:29:18,480
we have made progress on some, the ability to, again,

555
00:29:18,480 --> 00:29:22,480
detect people and do face recognition for well-known individuals, okay?

556
00:29:22,480 --> 00:29:24,480
But that's kind of about it, all right?

557
00:29:24,480 --> 00:29:28,480
And he wasn't particularly optimistic that the current route that's being pursued in industry

558
00:29:28,480 --> 00:29:34,480
is anywhere close to solving or even really trying to solve these larger questions.

559
00:29:34,480 --> 00:29:38,480
If we give this image to that caption bot,

560
00:29:38,480 --> 00:29:41,480
what we see is, again, represents the same point.

561
00:29:41,480 --> 00:29:43,480
Here's the caption bot, it says,

562
00:29:43,480 --> 00:29:46,480
I think it's a group of people standing next to a man in a suit and tie, right?

563
00:29:46,480 --> 00:29:48,480
So that's right, right?

564
00:29:48,480 --> 00:29:51,480
As far as it goes, it just doesn't go far enough,

565
00:29:51,480 --> 00:29:54,480
and the current ideas of build a data set,

566
00:29:54,480 --> 00:29:58,480
train a deep learning algorithm on it, and then repeat,

567
00:29:58,480 --> 00:30:03,480
aren't really even, I would venture, trying to get to what we're talking about.

568
00:30:03,480 --> 00:30:06,480
Or here's another, I'll just give you one other example of a couple of photographs

569
00:30:06,480 --> 00:30:10,480
from my recent vacation in a nice warm tropical locale,

570
00:30:10,480 --> 00:30:14,480
which I think illustrate ways in which, again, the gap where we have machines

571
00:30:14,480 --> 00:30:18,480
that can, say, beat the world's best at go,

572
00:30:18,480 --> 00:30:21,480
but can't even beat a child at tic-tac-toe.

573
00:30:21,480 --> 00:30:23,480
Now, what do I mean by that?

574
00:30:23,480 --> 00:30:26,480
Well, of course, we don't even need reinforcement learning or deep learning

575
00:30:26,480 --> 00:30:30,480
to build a machine that can win or tie, do optimally in tic-tac-toe.

576
00:30:30,480 --> 00:30:33,480
But think about this, this is a real tic-tac-toe game,

577
00:30:33,480 --> 00:30:36,480
which I saw on the grass outside my hotel, right?

578
00:30:36,480 --> 00:30:39,480
What do you have to do to look at this and recognize that it's a tic-tac-toe game?

579
00:30:39,480 --> 00:30:42,480
You have to see the objects, you have to see what's, you know, in some sense,

580
00:30:42,480 --> 00:30:46,480
there's a three-by-three grid, but it's only abstract, right?

581
00:30:46,480 --> 00:30:50,480
It's only delimited by these ropes or strings, okay?

582
00:30:50,480 --> 00:30:54,480
It's not actually a grid in any simple geometric sense, all right?

583
00:30:54,480 --> 00:30:58,480
But yet a child can look at that, and indeed, here's an actual child who was looking at it

584
00:30:58,480 --> 00:31:01,480
and recognize, oh, it's a game of tic-tac-toe, and even know what they need to do to win,

585
00:31:01,480 --> 00:31:04,480
namely put the X and complete it, and now they've got three in a row, right?

586
00:31:04,480 --> 00:31:07,480
That's literally child's play, okay?

587
00:31:07,480 --> 00:31:11,480
You show this sort of thing, though, to one of these, you know, image-understanding caption bots,

588
00:31:11,480 --> 00:31:14,480
and I think it's a close-up of a sign, okay?

589
00:31:14,480 --> 00:31:22,480
Again, saying that this is a close-up of a sign is not the same thing, I would venture,

590
00:31:22,480 --> 00:31:27,480
as a cognitive or computational activity that's going to give us what we need to say,

591
00:31:27,480 --> 00:31:30,480
recognize the objects, recognize it as a game to understand the goal

592
00:31:30,480 --> 00:31:32,480
and how to plan to achieve those goals.

593
00:31:32,480 --> 00:31:36,480
Whereas this kind of architecture is designed to try to do all of these things, ultimately, right?

594
00:31:36,480 --> 00:31:44,480
And I bring in these examples of games or jokes to really show where perception goes to cognition,

595
00:31:44,480 --> 00:31:47,480
you know, all the way up to symbols, right?

596
00:31:47,480 --> 00:31:52,480
So to get objects and forces and mental states, that's the cognitive core,

597
00:31:52,480 --> 00:31:58,480
but to be able to get goals and plans and what do I do or how do I talk about it, that symbols, okay?

598
00:31:58,480 --> 00:32:04,480
Here's another way into this, and it's one that also motivates, I think, a lot of really good work on the engineering side

599
00:32:04,480 --> 00:32:08,480
and a lot of our interest in the science side is think about robotics

600
00:32:08,480 --> 00:32:14,480
and think about what do you have to do to, you know, what does the brain have to be like to control the body?

601
00:32:14,480 --> 00:32:18,480
So again, you're going to hear from, shortly, I think maybe it's next week, from Mark Rayburt,

602
00:32:18,480 --> 00:32:24,480
who's one of the founders of Boston Dynamics, which is one of my favorite companies anywhere.

603
00:32:24,480 --> 00:32:31,480
They're, without doubt, the leading maker of humanoid robots, legged, locomoting robots in industry.

604
00:32:31,480 --> 00:32:37,480
They have all sorts of other really cool robots, robots like dogs, robots that have all, you know,

605
00:32:37,480 --> 00:32:40,480
I think you'll even get to see a live demonstration of one of these robots.

606
00:32:40,480 --> 00:32:43,480
It's really awesome, impressive stuff, okay?

607
00:32:43,480 --> 00:32:46,480
But what about the minds and brains of these robots?

608
00:32:46,480 --> 00:32:51,480
Well, again, if you ask Mark, ask them how much of human-like cognition do they have in their robots,

609
00:32:51,480 --> 00:32:54,480
and I think he would say very little.

610
00:32:54,480 --> 00:32:56,480
In fact, we have asked him that, and he would say very little.

611
00:32:56,480 --> 00:32:58,480
He has said very little.

612
00:32:58,480 --> 00:33:03,480
He's actually one of the advisors of our center, and I think in many ways we're very much on the same page.

613
00:33:03,480 --> 00:33:10,480
We both want to know how do you build the kind of intelligence that can control these bodies like the way a human does?

614
00:33:10,480 --> 00:33:13,480
All right, here's another example of an industry robotics effort.

615
00:33:13,480 --> 00:33:16,480
This is Google's arm farm, where, you know, they've got lots of robot arms,

616
00:33:16,480 --> 00:33:21,480
and they're trying to train them to pick up objects using various kinds of deep learning and reinforcement learning techniques.

617
00:33:21,480 --> 00:33:23,480
And I think it's one approach.

618
00:33:23,480 --> 00:33:29,480
I just think it's very, very different from the way humans learn to, say, control their body and manipulate objects.

619
00:33:29,480 --> 00:33:33,480
And you can see that in terms of things that go back to what you were saying when you were introducing me, right?

620
00:33:33,480 --> 00:33:35,480
Think about how quickly we learn things, right?

621
00:33:35,480 --> 00:33:40,480
Here you have these, the arm farm is trying to generate, you know, effectively, maybe if not infinite,

622
00:33:40,480 --> 00:33:46,480
but hundreds of thousands, millions of examples of reaches and pickups of objects, even with just a single gripper.

623
00:33:46,480 --> 00:33:53,480
And yet a child who in some ways can't control their body nearly as well as robots can be controlled at the low level

624
00:33:53,480 --> 00:33:55,480
is able to do so much more.

625
00:33:55,480 --> 00:34:01,480
So I'll show you two of my favorite videos from YouTube here, which motivate some of the research that we're doing.

626
00:34:01,480 --> 00:34:04,480
The one on the left is a one-and-a-half-year-old and the other one's a one-year-old.

627
00:34:04,480 --> 00:34:09,480
So just watch this one-and-a-half-year-old here doing a popular activity for many kids.

628
00:34:09,480 --> 00:34:11,480
Is it playing?

629
00:34:11,480 --> 00:34:13,480
Hmm.

630
00:34:15,480 --> 00:34:17,480
You see video up there?

631
00:34:18,480 --> 00:34:19,480
Okay, there we go.

632
00:34:19,480 --> 00:34:23,480
Okay, so he's doing this stacking cup activity.

633
00:34:23,480 --> 00:34:27,480
All right, he's stacking up cups to make a tall tower.

634
00:34:27,480 --> 00:34:28,480
He's got a stack of three.

635
00:34:28,480 --> 00:34:33,480
And what you can see for the first part of this video is it looks like he's trying to make a second stack

636
00:34:33,480 --> 00:34:35,480
that he's trying to pick up at once.

637
00:34:35,480 --> 00:34:39,480
Basically, he's trying to make a stack of two that'll go on the stack of three.

638
00:34:39,480 --> 00:34:44,480
And, you know, he's trying to debug his plan because it got a little bit stuck here.

639
00:34:44,480 --> 00:34:49,480
But, and think about, I mean, again, if you know anything about robots manipulating objects,

640
00:34:49,480 --> 00:34:53,480
even just what he just did, no robot can decide to do that and actually do it, right?

641
00:34:53,480 --> 00:34:55,480
At some point, he's almost got it.

642
00:34:55,480 --> 00:34:59,480
It's a little bit tricky, but at some point he's going to get that stack of two.

643
00:34:59,480 --> 00:35:02,480
He realizes he has to move that object out of the way.

644
00:35:02,480 --> 00:35:03,480
Look at what he just did.

645
00:35:03,480 --> 00:35:05,480
Move it out of the way, use two hands to pick it up.

646
00:35:05,480 --> 00:35:09,480
And now he's got a stack of two on a stack of three, and suddenly, you know, sub-goal completed.

647
00:35:09,480 --> 00:35:10,480
He's now got a stack of five.

648
00:35:10,480 --> 00:35:16,480
And he gives himself a hand because he knows he accomplished a key waypoint along the way to his final goal.

649
00:35:16,480 --> 00:35:19,480
That's a kind of early symbolic cognition, right?

650
00:35:19,480 --> 00:35:24,480
To understand that I'm trying to build a tall tower, but a tower is made up of little towers.

651
00:35:24,480 --> 00:35:28,480
And you can take a tower and put it on top of another tower or stack a stack on a stack,

652
00:35:28,480 --> 00:35:30,480
and you have a bigger stack, right?

653
00:35:30,480 --> 00:35:34,480
So think about how he goes from bottom-up perception to the objects that the physics needed

654
00:35:34,480 --> 00:35:39,480
to manipulate the objects to the ability to make even those early kinds of symbolic plans.

655
00:35:39,480 --> 00:35:40,480
At some point, he keeps doing this.

656
00:35:40,480 --> 00:35:43,480
He puts another stack on there.

657
00:35:43,480 --> 00:35:44,480
I'll just jump to the end.

658
00:35:44,480 --> 00:35:45,480
Oops, sorry.

659
00:35:45,480 --> 00:35:46,480
You missed.

660
00:35:46,480 --> 00:35:47,480
Sorry.

661
00:35:47,480 --> 00:35:51,480
He gets really excited and he gives himself another big hand, but falls over.

662
00:35:51,480 --> 00:35:52,480
Okay.

663
00:35:52,480 --> 00:35:57,480
Again, Boston Dynamics now has robots that could pick themselves up after that.

664
00:35:57,480 --> 00:35:59,480
That's really impressive, again.

665
00:35:59,480 --> 00:36:03,480
But all the other stuff to get to that point, we don't really know how to do in a robotic setting.

666
00:36:03,480 --> 00:36:04,480
Or think about this baby here.

667
00:36:04,480 --> 00:36:05,480
This is a younger baby.

668
00:36:05,480 --> 00:36:12,480
This is one of the Internet's very most popular videos because it features a baby and a cat.

669
00:36:12,480 --> 00:36:14,480
But the baby's doing something interesting.

670
00:36:14,480 --> 00:36:19,480
He's got the same cups, but he's decided, again, decided to try a new thing.

671
00:36:19,480 --> 00:36:20,480
So think about creativity.

672
00:36:20,480 --> 00:36:25,480
He's decided that his goal is to stack up cups on the back of a cat, I guess.

673
00:36:25,480 --> 00:36:27,480
He's asking, how many cups can I fit on the back of a cat?

674
00:36:27,480 --> 00:36:28,480
Well, three.

675
00:36:28,480 --> 00:36:31,480
Let's see, can I fit more?

676
00:36:31,480 --> 00:36:33,480
Let's try another one.

677
00:36:33,480 --> 00:36:34,480
Okay.

678
00:36:34,480 --> 00:36:35,480
Well, he can't fit more than three.

679
00:36:35,480 --> 00:36:36,480
It turns out.

680
00:36:36,480 --> 00:36:37,480
And then he, then it's not working.

681
00:36:37,480 --> 00:36:39,480
So he changes his goal.

682
00:36:39,480 --> 00:36:42,480
Now his goal appears to be to get the cups on the other side of the cat.

683
00:36:42,480 --> 00:36:45,480
Now watch that part when he reaches back behind him there.

684
00:36:45,480 --> 00:36:47,480
I'll just pause it there for a moment.

685
00:36:47,480 --> 00:36:51,480
So when he just reached back there, that's a particularly striking moment in the video.

686
00:36:51,480 --> 00:36:56,480
It shows a very strong form of what we call in cognitive science, object permanence.

687
00:36:56,480 --> 00:36:57,480
Okay.

688
00:36:57,480 --> 00:37:01,480
That's the idea that you represent objects as these permanent, enduring entities in the

689
00:37:01,480 --> 00:37:03,480
world, even when you can't see them.

690
00:37:03,480 --> 00:37:07,480
In this case, he hadn't seen or touched that object behind him for like at least a minute,

691
00:37:07,480 --> 00:37:08,480
right?

692
00:37:08,480 --> 00:37:09,480
Maybe much longer, I don't know.

693
00:37:09,480 --> 00:37:12,480
And yet he still knew it was there and he was able to incorporate it in his plan, right?

694
00:37:12,480 --> 00:37:15,480
There's a moment before that when he's about to reach for it, but then he sees this other

695
00:37:15,480 --> 00:37:16,480
one, right?

696
00:37:16,480 --> 00:37:19,480
And it's only when he's now exhausted all the other objects here that he can see.

697
00:37:19,480 --> 00:37:22,480
He's like, okay, now time to get this object and bring it into play, right?

698
00:37:22,480 --> 00:37:27,480
So think about what has to be going on in his brain for him to be able to do that, right?

699
00:37:27,480 --> 00:37:31,480
That's like the analog of you understanding what's behind you, okay?

700
00:37:31,480 --> 00:37:34,480
It's not that these things are impossible to capture machines far from it.

701
00:37:34,480 --> 00:37:37,480
It's just that like training a deep neural network or any kind of pattern recognition

702
00:37:37,480 --> 00:37:39,480
system we don't think is going to do it.

703
00:37:39,480 --> 00:37:43,480
But we think by reverse engineering how it works in the brain, we might be able to do

704
00:37:43,480 --> 00:37:44,480
it.

705
00:37:44,480 --> 00:37:45,480
I think we can do it, okay?

706
00:37:45,480 --> 00:37:47,480
It's not just humans that do this kind of activity.

707
00:37:47,480 --> 00:37:49,480
Here's a couple of, again, rather famous videos.

708
00:37:49,480 --> 00:37:51,480
You can watch all of these on YouTube.

709
00:37:51,480 --> 00:37:58,480
Crows are famous object manipulators and tool users, but also orangutans, other primates,

710
00:37:58,480 --> 00:37:59,480
rodents.

711
00:37:59,480 --> 00:38:02,480
We can watch, if we just, here, let me pause this one for a second.

712
00:38:02,480 --> 00:38:05,480
If we watch this orangutan here, he's got a bunch of big Legos.

713
00:38:05,480 --> 00:38:10,480
And over the course of this video, he's building up a stack of Legos.

714
00:38:10,480 --> 00:38:13,480
It's really quite impressive.

715
00:38:13,480 --> 00:38:16,480
He's just jumping to the end.

716
00:38:16,480 --> 00:38:21,480
There's actually some controversy out there of whether this video is a fake.

717
00:38:21,480 --> 00:38:25,480
But the controversy isn't about, you know, it's not like whether it was, I don't know,

718
00:38:25,480 --> 00:38:27,480
done with computer animation.

719
00:38:27,480 --> 00:38:31,480
Some people think the video was actually filmed backwards, that a human built up the stack

720
00:38:31,480 --> 00:38:34,480
and the orangutan just slowly disassembled it piece by piece.

721
00:38:34,480 --> 00:38:37,480
And it turns out it's remarkably hard to tell whether it's played forward or backwards in

722
00:38:37,480 --> 00:38:38,480
time.

723
00:38:38,480 --> 00:38:40,480
And people have argued over little details because, you know, it would be quite impressive

724
00:38:40,480 --> 00:38:45,480
if an orangutan actually was able to build up this really impressive stack of Legos.

725
00:38:45,480 --> 00:38:48,480
And I would submit that it would be almost as impressive if he disassembled it.

726
00:38:48,480 --> 00:38:50,480
Think about the activity.

727
00:38:50,480 --> 00:38:53,480
I mean, if I wanted to disassemble that, the easiest thing to do would just be to knock it over.

728
00:38:53,480 --> 00:38:55,480
That's really all most robots could do.

729
00:38:55,480 --> 00:38:59,480
But to piece by piece disassemble it, even if it's played backwards like this,

730
00:38:59,480 --> 00:39:03,480
that's still a really impressive act of symbolic planning on physical objects.

731
00:39:03,480 --> 00:39:08,480
Or here you've got this famous mouse, this you can find on the internet under the

732
00:39:08,480 --> 00:39:10,480
Mouse vs. Cracker video.

733
00:39:10,480 --> 00:39:15,480
And what you'll see here over the course of this video is a mouse valiantly and mostly

734
00:39:15,480 --> 00:39:20,480
hopelessly struggling with a cracker that they're hoping to bring back to their nest.

735
00:39:20,480 --> 00:39:23,480
I guess it's a very appealing big meal.

736
00:39:23,480 --> 00:39:28,480
And at some point after just trying to get it over the wall, at some point the mouse

737
00:39:28,480 --> 00:39:30,480
just gives up because it's just never going to happen.

738
00:39:30,480 --> 00:39:31,480
And he just goes away.

739
00:39:31,480 --> 00:39:37,480
Except that because even mouses can dream or mice can dream, at some point he decides,

740
00:39:37,480 --> 00:39:39,480
he's not going to come back for one more try.

741
00:39:39,480 --> 00:39:42,480
And he tries one more time and this time valiantly gets it over.

742
00:39:42,480 --> 00:39:44,480
Isn't that very impressive?

743
00:39:44,480 --> 00:39:45,480
Congratulations.

744
00:39:45,480 --> 00:39:46,480
You don't have to clap.

745
00:39:46,480 --> 00:39:49,480
You can clap for me at the end or clap for whoever later.

746
00:39:49,480 --> 00:39:52,480
But I want to applaud the mouse there every time I see that.

747
00:39:52,480 --> 00:39:56,480
But again, think what had to be going on in his brain to be able to do that.

748
00:39:56,480 --> 00:39:58,480
It's a crazy thing.

749
00:39:58,480 --> 00:40:01,480
And yet he formulated the goal and was able to achieve it.

750
00:40:01,480 --> 00:40:04,480
I'll just show one more video that is really more about science.

751
00:40:04,480 --> 00:40:07,480
These other ones are, you know, some of them actually were from scientific experiments.

752
00:40:07,480 --> 00:40:10,480
But this is one that motivates a lot of the science that I do.

753
00:40:10,480 --> 00:40:15,480
And to me it sets up kind of a grand cognitive science challenge for AI and robotics.

754
00:40:15,480 --> 00:40:20,480
It's from an experiment with humans, again 18-month-olds or one and a half-year-olds.

755
00:40:20,480 --> 00:40:23,480
So the kids in this experiment were the same age as the first baby I showed you,

756
00:40:23,480 --> 00:40:24,480
the one who did the stacking.

757
00:40:24,480 --> 00:40:29,480
And 18 months is really a very, very good age to study if you're interested in intelligence

758
00:40:29,480 --> 00:40:32,480
for reasons we can talk about later if you're interested.

759
00:40:32,480 --> 00:40:35,480
This is from a very famous experiment done by two psychologists,

760
00:40:35,480 --> 00:40:38,480
Felix Wernicke and Michael Tomasello.

761
00:40:38,480 --> 00:40:42,480
And it was studying the spontaneous helping behavior of young children.

762
00:40:42,480 --> 00:40:44,480
It also contrasted humans and chimps.

763
00:40:44,480 --> 00:40:48,480
And the punchline is that chimps sometimes do things that are kind of like what this human did,

764
00:40:48,480 --> 00:40:51,480
but not nearly as reliably or as flexibly.

765
00:40:51,480 --> 00:40:56,480
Okay, so not nearly in his, and I'll show you a particular kind of unusual situation

766
00:40:56,480 --> 00:41:00,480
where human kids had relatively little trouble figuring out kind of what to do

767
00:41:00,480 --> 00:41:02,480
or even whether they should do it.

768
00:41:02,480 --> 00:41:06,480
Whereas basically no chimp did what you're going to see humans sometimes doing here.

769
00:41:06,480 --> 00:41:11,480
So the experimenter in this movie, and I'll turn on the sound here if you can hear it.

770
00:41:11,480 --> 00:41:17,480
The experimenter is the tall guy and the participant is the little kid in the corner.

771
00:41:17,480 --> 00:41:21,480
There's sound but no words, right?

772
00:41:21,480 --> 00:41:25,480
And at some point he stops and then the kid just does whatever they want to do.

773
00:41:25,480 --> 00:41:26,480
So watch what he does.

774
00:41:26,480 --> 00:41:31,480
He goes over, he opens the cabinet, looks inside, then he steps back

775
00:41:31,480 --> 00:41:36,480
and he looks up at Felix and then looks down, okay, and then the action is completed.

776
00:41:36,480 --> 00:41:41,480
Now, I want you to watch it one more time and think about what's got to be going inside the kid's head

777
00:41:41,480 --> 00:41:45,480
to understand this, to understand, like, so it seems like what it looks like to us

778
00:41:45,480 --> 00:41:48,480
is the kid figured out that this guy needed help and helped him.

779
00:41:48,480 --> 00:41:50,480
And the paper is full of many other situations like this.

780
00:41:50,480 --> 00:41:52,480
This is just one, okay?

781
00:41:52,480 --> 00:41:54,480
But the key idea is that the situation is somewhat novel.

782
00:41:54,480 --> 00:41:57,480
People have seen people holding books and opening cabinets,

783
00:41:57,480 --> 00:42:01,480
but probably it's very rare to see this kind of situation exactly, right?

784
00:42:01,480 --> 00:42:04,480
It's different in some important details from what you might have seen before.

785
00:42:04,480 --> 00:42:06,480
And there's other ones in there that are really truly novel

786
00:42:06,480 --> 00:42:09,480
because they just made up a machine right there, okay?

787
00:42:09,480 --> 00:42:14,480
But somehow he has to understand causally from the way the guy is banging the books against the thing.

788
00:42:14,480 --> 00:42:18,480
It's sort of both a symbol, but it's also somehow he's got to understand

789
00:42:18,480 --> 00:42:22,480
what he can do and what he can't do and then what the kid can do to help.

790
00:42:22,480 --> 00:42:25,480
And I'll show this again, but really just watch,

791
00:42:25,480 --> 00:42:32,480
the main part I want you to see is, I'll just sort of skip ahead.

792
00:42:32,480 --> 00:42:35,480
So watch this part here.

793
00:42:35,480 --> 00:42:39,480
Let's say I'll just jump right when he, watch, right now he's about to look up.

794
00:42:39,480 --> 00:42:43,480
He looks up and makes eye contact and then his eyes look down.

795
00:42:43,480 --> 00:42:48,480
So again, he looks up, he looks up and then a saccade,

796
00:42:48,480 --> 00:42:52,480
a sudden rapid eye movement down, down to his hands, up, down, okay?

797
00:42:52,480 --> 00:42:55,480
So that's, again, that's this brain OS in action, right?

798
00:42:55,480 --> 00:43:01,480
He's making one glance, small glance, at the big guy's eyes to make eye contact,

799
00:43:01,480 --> 00:43:05,480
to see, to get a signal, did I understand what you wanted

800
00:43:05,480 --> 00:43:08,480
and did you register that joint attention?

801
00:43:08,480 --> 00:43:11,480
And then he makes a prediction about what the guy's going to do,

802
00:43:11,480 --> 00:43:13,480
so he looks right down, he doesn't just like look around randomly,

803
00:43:13,480 --> 00:43:18,480
he looks right down to the guy's hands to track the action that he expects to see happening

804
00:43:18,480 --> 00:43:22,480
if I did the right thing to help you, then I expect you're going to put the books there, okay?

805
00:43:22,480 --> 00:43:26,480
So you can see these things happening and we want to know what's going on inside the mind

806
00:43:26,480 --> 00:43:28,480
that guides all of that, all right?

807
00:43:28,480 --> 00:43:32,480
So that's this sort of big scientific agenda that we're working on over the next few years

808
00:43:32,480 --> 00:43:38,480
where we think some kind of human, understanding of human intelligence and scientific terms

809
00:43:38,480 --> 00:43:40,480
could lead to all sorts of AI payoffs.

810
00:43:40,480 --> 00:43:43,480
In particular, suppose we could build a robot that could do what this kid

811
00:43:43,480 --> 00:43:47,480
and many other kids in these experiments do to say help you out around the house

812
00:43:47,480 --> 00:43:51,480
without having to be programmed or even really instructed just to kind of get a sense,

813
00:43:51,480 --> 00:43:54,480
oh yeah, you need to hand with that, sure, let me help you out, okay?

814
00:43:54,480 --> 00:43:58,480
Even 18-month-olds will do that, sometimes not very reliably or effectively,

815
00:43:58,480 --> 00:44:01,480
sometimes they'll try to help and really do the opposite, right?

816
00:44:01,480 --> 00:44:07,480
But imagine if you could take the flexible understanding of humans, actions, goals and so on

817
00:44:07,480 --> 00:44:11,480
and make those reliable engineering technology, that would be very useful.

818
00:44:11,480 --> 00:44:15,480
And it would also be related to, say, machines that you could actually start to talk to

819
00:44:15,480 --> 00:44:18,480
and trust in some ways, right, that shared understanding.

820
00:44:18,480 --> 00:44:20,480
So how are we going to do this?

821
00:44:20,480 --> 00:44:23,480
Well, let me spend the rest of the time talking about how we try to do this, right?

822
00:44:23,480 --> 00:44:28,480
Some of the technology that we're building both in our group and more broadly

823
00:44:28,480 --> 00:44:31,480
to try to make these kinds of architectures real.

824
00:44:31,480 --> 00:44:36,480
And I'll talk about two or three technical ideas, again, not in any detail, all right?

825
00:44:36,480 --> 00:44:39,480
One is the idea of a probabilistic program.

826
00:44:39,480 --> 00:44:45,480
So this is a kind of, think of it as a computational abstraction

827
00:44:45,480 --> 00:44:49,480
that we can use to capture the common sense knowledge of this core cognition.

828
00:44:49,480 --> 00:44:53,480
So when I say we have an intuitive understanding of physical objects and people's goals,

829
00:44:53,480 --> 00:44:56,480
how do I build a model of that model you have in the head?

830
00:44:56,480 --> 00:44:59,480
Probabilistic programs, a little bit more technically,

831
00:44:59,480 --> 00:45:03,480
one way to understand them is as a generalization of Bayesian networks

832
00:45:03,480 --> 00:45:07,480
or other kinds of directed graphical models, if you know those, okay?

833
00:45:07,480 --> 00:45:11,480
But where instead of defining a probability model on a graph,

834
00:45:11,480 --> 00:45:13,480
you define it on a program,

835
00:45:13,480 --> 00:45:19,480
and thereby have access to a much more expressive toolkit of knowledge representation.

836
00:45:19,480 --> 00:45:24,480
So data structures, other kinds of algorithmic tools for representing knowledge, okay?

837
00:45:24,480 --> 00:45:27,480
But you still have access to the ability to do probabilistic inference,

838
00:45:27,480 --> 00:45:32,480
like in a graphical model, but also causal inference in a directed graphical model.

839
00:45:32,480 --> 00:45:35,480
So for those of you who know about graphical models, that might make some sense to you.

840
00:45:35,480 --> 00:45:38,480
But just more broadly, what this is, think of this as a toolkit

841
00:45:38,480 --> 00:45:41,480
that allows us to combine several of the best ideas,

842
00:45:41,480 --> 00:45:43,480
not just of the recent deep learning era,

843
00:45:43,480 --> 00:45:47,480
but if you look back over the whole scope of AI and as well as cognitive science,

844
00:45:47,480 --> 00:45:51,480
I think there's three or four ideas and more,

845
00:45:51,480 --> 00:45:53,480
but definitely like three ideas we can really put up there

846
00:45:53,480 --> 00:45:57,480
that have proven their worth and have risen and fallen in terms of,

847
00:45:57,480 --> 00:46:00,480
each of these had ideas when the mainstream of the field thought,

848
00:46:00,480 --> 00:46:04,480
this was totally the way to go and every other idea was obviously a waste of time,

849
00:46:04,480 --> 00:46:08,480
and also had its time when many people thought it was a waste of time, okay?

850
00:46:08,480 --> 00:46:13,480
And these three big ideas, I would say, are first of all the idea of symbolic representation,

851
00:46:13,480 --> 00:46:16,480
or symbolic languages for knowledge representation,

852
00:46:16,480 --> 00:46:20,480
probabilistic inference in generative models to capture uncertainty, ambiguity,

853
00:46:20,480 --> 00:46:25,480
learning from sparse data, and in their hierarchical setting, learning to learn, right?

854
00:46:25,480 --> 00:46:32,480
And then, of course, the recent developments with neural-inspired architectures for pattern recognition, okay?

855
00:46:32,480 --> 00:46:36,480
Each of these things, each of these ideas, symbolic languages, probabilistic inference,

856
00:46:36,480 --> 00:46:41,480
and neural networks have some distinctive strengths that are real weak points of the other approaches, right?

857
00:46:41,480 --> 00:46:44,480
So to take one example that I haven't really talked about here,

858
00:46:44,480 --> 00:46:49,480
people in the, but you mentioned as an outstanding challenge for neural networks,

859
00:46:49,480 --> 00:46:54,480
transfer learning, or learning to take knowledge across a number of previous tasks to transfer to others,

860
00:46:54,480 --> 00:46:58,480
is a real challenge and has always been a challenge in a neural net, okay?

861
00:46:58,480 --> 00:47:03,480
But it's something that's addressed very naturally and very scalably in, for example, a hierarchical Bayesian model.

862
00:47:03,480 --> 00:47:07,480
And if you look at some of the recent attempts, really interesting attempts within the deep learning world

863
00:47:07,480 --> 00:47:11,480
to try to get kinds of transfer learning and learning to learn, they're really cool, okay?

864
00:47:11,480 --> 00:47:16,480
But many of them are in some ways kind of reinventing within a neural network paradigm

865
00:47:16,480 --> 00:47:21,480
ideas that people, you know, maybe just 10 or 15 years ago developed in very sophisticated ways

866
00:47:21,480 --> 00:47:24,480
in, let's say, hierarchical Bayesian models, okay?

867
00:47:24,480 --> 00:47:29,480
And a lot of attempts to get sort of symbolic algorithm-like behavior in neural networks, again,

868
00:47:29,480 --> 00:47:34,480
are really, you know, they're very small steps towards something which is a very mature technology

869
00:47:34,480 --> 00:47:37,480
in computer systems and programming languages.

870
00:47:37,480 --> 00:47:41,480
Probabilistic programs, I'll just sort of advertise mostly,

871
00:47:41,480 --> 00:47:44,480
are a way to combine the strengths of all of these approaches,

872
00:47:44,480 --> 00:47:49,480
to have knowledge representations which are as expressive as anything that anybody ever did in the symbolic paradigm,

873
00:47:49,480 --> 00:47:54,480
that are as flexible at dealing with uncertainty and sparse data as anything in the probabilistic paradigm,

874
00:47:54,480 --> 00:47:59,480
but that also can support pattern recognition tools to be able to, for example,

875
00:47:59,480 --> 00:48:03,480
do very fast efficient inference in very complex scenarios.

876
00:48:03,480 --> 00:48:06,480
And there's a number of, that's the kind of conceptual framework.

877
00:48:06,480 --> 00:48:09,480
There's a number of actually implemented tools.

878
00:48:09,480 --> 00:48:14,480
I point to here on the slide a number of probabilistic programming languages which you can go explore.

879
00:48:14,480 --> 00:48:17,480
For example, there's one that was developed in our group a few years ago,

880
00:48:17,480 --> 00:48:21,480
almost 10 years ago, now called Church, which was the antecedent of some of these other languages

881
00:48:21,480 --> 00:48:23,480
built on a functional programming core.

882
00:48:23,480 --> 00:48:26,480
So Church is a probabilistic programming language built on the Lambda Calculus,

883
00:48:26,480 --> 00:48:28,480
or really in LISP, basically.

884
00:48:28,480 --> 00:48:34,480
But there are many other more modern tools, especially if you are interested in neural networks.

885
00:48:34,480 --> 00:48:38,480
There are tools like, for example, Pyro, or ProbTorch, or BayesFlow,

886
00:48:38,480 --> 00:48:42,480
that try to combine all these ideas in a, or for example, Gen here,

887
00:48:42,480 --> 00:48:46,480
which is a project of Vakash Mansingh's probabilistic computing group.

888
00:48:46,480 --> 00:48:51,480
These are all things which are just in the very beginning stages, very, very alpha.

889
00:48:51,480 --> 00:48:55,480
But you can find out more about them online or by writing to their creators.

890
00:48:55,480 --> 00:49:02,480
And I think this is a very exciting place where the convergence of a number of different AI tools are happening.

891
00:49:02,480 --> 00:49:08,480
And this will be absolutely necessary for making the kind of architecture that I'm talking about work.

892
00:49:08,480 --> 00:49:11,480
Another key idea, which we've been building on in our lab,

893
00:49:11,480 --> 00:49:15,480
and I think, again, many people are using some version of this idea,

894
00:49:15,480 --> 00:49:18,480
but maybe a little bit different from the way we're doing it,

895
00:49:18,480 --> 00:49:23,480
is what the version of this idea that I like to talk about is what I call the game engine in the head.

896
00:49:23,480 --> 00:49:27,480
So this is the idea that it's really what the programs are about.

897
00:49:27,480 --> 00:49:31,480
When I talk about probabilistic programs, I haven't said anything about what kind of programs we're using.

898
00:49:31,480 --> 00:49:35,480
We're just basically, these probabilistic programming languages at their best,

899
00:49:35,480 --> 00:49:39,480
and church, the language that was developed by Noah Goodman and Vakash and others,

900
00:49:39,480 --> 00:49:41,480
and Dan Roy and our group some 10 years ago,

901
00:49:41,480 --> 00:49:45,480
was intended to be a turing complete probabilistic programming language.

902
00:49:45,480 --> 00:49:51,480
So any probability model that was computable, or for whose conditional inferences are computable,

903
00:49:51,480 --> 00:49:53,480
you could represent in these languages.

904
00:49:53,480 --> 00:49:59,480
But that leaves completely open what kind of program I'm going to write to model the world.

905
00:49:59,480 --> 00:50:06,480
And I've been very inspired in the last few years by thinking about the kinds of programs that are in modern video game engines.

906
00:50:06,480 --> 00:50:09,480
So again, probably most of you are familiar with these,

907
00:50:09,480 --> 00:50:12,480
and increasingly they're playing a role in all sorts of ways in AI,

908
00:50:12,480 --> 00:50:18,480
but these are tools that were developed by the video game industry to allow a game designer to make a new game

909
00:50:18,480 --> 00:50:23,480
without having to do most of, in some sense, many, most of the hard technical work from scratch,

910
00:50:23,480 --> 00:50:27,480
but rather to focus on the characters, the world, the story,

911
00:50:27,480 --> 00:50:31,480
the things that are more interesting for designing a novel game.

912
00:50:31,480 --> 00:50:36,480
In particular, if we want a player to explore some new three-dimensional world,

913
00:50:36,480 --> 00:50:39,480
but to have them be able to interact with the world in real time

914
00:50:39,480 --> 00:50:46,480
and to render nice-looking graphics in real time in an interactive way as the player moves around and explores the world,

915
00:50:46,480 --> 00:50:51,480
or if you want to populate the world with non-player characters that will behave in an even vaguely intelligent way.

916
00:50:51,480 --> 00:50:56,480
Game engines give you tools for doing all of this without having to write all of graphics from scratch

917
00:50:56,480 --> 00:51:00,480
or all of physics, the rules of physics from scratch,

918
00:51:00,480 --> 00:51:05,480
so what are called game physics engines, and in some sense are a set of principles,

919
00:51:05,480 --> 00:51:08,480
but also hacks from Newtonian mechanics and other areas of physics

920
00:51:08,480 --> 00:51:13,480
that allow you to simulate plausible-looking physical interactions in very complex world,

921
00:51:13,480 --> 00:51:16,480
very approximately, but very fast.

922
00:51:16,480 --> 00:51:20,480
There's also what's called game AI, which are basically very simple planning models.

923
00:51:20,480 --> 00:51:26,480
So let's say I want to have an AI in the game that is like a guard that guards a base and a player is going to attack the space.

924
00:51:26,480 --> 00:51:29,480
So back in the old Atari days, like when I was a kid,

925
00:51:29,480 --> 00:51:34,480
the guards would just be random things that would fire missiles kind of randomly in random directions at random times,

926
00:51:34,480 --> 00:51:37,480
but let's say you want a guard to be a little intelligent,

927
00:51:37,480 --> 00:51:42,480
so to actually look around and, oh, and I see the player, and then to actually start shooting at you and to even maybe pursue you.

928
00:51:42,480 --> 00:51:45,480
So that requires putting a little AI in the game,

929
00:51:45,480 --> 00:51:49,480
and you do that by having basically simple agent models in the game.

930
00:51:49,480 --> 00:51:54,480
So what we think, and some of you might think this is crazy and some of you might think this is a very natural idea,

931
00:51:54,480 --> 00:51:56,480
I get both kinds of reactions.

932
00:51:56,480 --> 00:52:01,480
What we think is that these tools of fast-approximate renderers, physics engines,

933
00:52:01,480 --> 00:52:06,480
and sort of very simple kinds of AI planning are an interesting first approximation

934
00:52:06,480 --> 00:52:11,480
to the kinds of common-sense knowledge representations that evolution has built into our brains.

935
00:52:11,480 --> 00:52:16,480
So when we talk about the cognitive core or how do babies start,

936
00:52:16,480 --> 00:52:19,480
ways in which a baby's brain isn't a blank slate,

937
00:52:19,480 --> 00:52:24,480
one interesting idea is that it starts with something like these tools

938
00:52:24,480 --> 00:52:27,480
and then wrapped inside a framework for probabilistic inference,

939
00:52:27,480 --> 00:52:29,480
that's what we mean by probabilistic programs,

940
00:52:29,480 --> 00:52:33,480
that can support many activities of common-sense perception and thinking.

941
00:52:33,480 --> 00:52:38,480
So I'll just give you one example of what we call this intuitive physics engine.

942
00:52:38,480 --> 00:52:45,480
So this is work that we did in our groups that Pete Battaglia and Jess Hamrick started this work about five years ago now,

943
00:52:45,480 --> 00:52:48,480
where we showed people, in some sense,

944
00:52:48,480 --> 00:52:52,480
and this is also an illustration of a kind of experiment that you might do,

945
00:52:52,480 --> 00:52:55,480
when I keep talking about science, I'll show you now a couple of experiments.

946
00:52:55,480 --> 00:52:59,480
So we would show people simple physical scenes like these blocks world scenes

947
00:52:59,480 --> 00:53:01,480
and ask them to make a number of judgments.

948
00:53:01,480 --> 00:53:05,480
And the model we built does basically a little bit of probabilistic inference

949
00:53:05,480 --> 00:53:08,480
in a game-style physics engine, it perceives the physical state

950
00:53:08,480 --> 00:53:13,480
and imagines a few different possible ways the world could go over the next one or two seconds

951
00:53:13,480 --> 00:53:16,480
to answer questions like, will the stack of blocks fall?

952
00:53:16,480 --> 00:53:18,480
Or if they fall, how far will they fall?

953
00:53:18,480 --> 00:53:19,480
Or which way will they fall?

954
00:53:19,480 --> 00:53:25,480
Or what would happen if, say, one color of blocks or one material, like the green stuff,

955
00:53:25,480 --> 00:53:27,480
is ten times heavier than the gray stuff?

956
00:53:27,480 --> 00:53:29,480
Or vice versa, how will that change the direction of fall?

957
00:53:29,480 --> 00:53:34,480
Or look at those red and yellow blocks, some of which look like they should be falling but aren't.

958
00:53:34,480 --> 00:53:35,480
So why?

959
00:53:35,480 --> 00:53:41,480
Can you infer from the fact that they're not falling that one color block is much heavier than the other?

960
00:53:41,480 --> 00:53:44,480
Or let me show you a sort of a slightly weird task.

961
00:53:44,480 --> 00:53:46,480
It's like other behavioral experiments.

962
00:53:46,480 --> 00:53:51,480
Sometimes we do weird things so that we can test ways in which you use your knowledge

963
00:53:51,480 --> 00:53:54,480
that you didn't just learn from pattern recognition,

964
00:53:54,480 --> 00:53:57,480
but use it to do new kinds of tasks that you'd never seen before.

965
00:53:57,480 --> 00:54:01,480
So here's a task which many of you have maybe seen me talk about these things.

966
00:54:01,480 --> 00:54:05,480
So you might have seen this task, but probably only if you saw me give a talk around here before.

967
00:54:05,480 --> 00:54:09,480
We call this the red-yellow task, and again, we'll make this one interactive.

968
00:54:09,480 --> 00:54:14,480
So imagine that the blocks on the table are knocked hard enough to bump,

969
00:54:14,480 --> 00:54:17,480
the tables bumped hard enough to knock some of the blocks onto the floor.

970
00:54:17,480 --> 00:54:20,480
So you tell me, is it more likely to be red blocks or yellow blocks?

971
00:54:20,480 --> 00:54:21,480
What do you say?

972
00:54:21,480 --> 00:54:22,480
Red.

973
00:54:22,480 --> 00:54:23,480
Okay, good.

974
00:54:23,480 --> 00:54:25,480
How about here?

975
00:54:25,480 --> 00:54:26,480
Yellow.

976
00:54:26,480 --> 00:54:27,480
Yellow.

977
00:54:27,480 --> 00:54:28,480
Good.

978
00:54:28,480 --> 00:54:29,480
How about here?

979
00:54:29,480 --> 00:54:30,480
Uh-huh.

980
00:54:30,480 --> 00:54:31,480
Here?

981
00:54:31,480 --> 00:54:33,480
Here?

982
00:54:33,480 --> 00:54:35,480
Okay.

983
00:54:35,480 --> 00:54:38,480
Here?

984
00:54:38,480 --> 00:54:39,480
Here?

985
00:54:39,480 --> 00:54:40,480
Here?

986
00:54:40,480 --> 00:54:41,480
Okay.

987
00:54:41,480 --> 00:54:46,480
So you just experience for yourself what it's like to be a subject in one of these experiments.

988
00:54:46,480 --> 00:54:47,480
We just did the experiment here.

989
00:54:47,480 --> 00:54:50,480
The data is all captured on video, sort of, right?

990
00:54:50,480 --> 00:54:51,480
Okay.

991
00:54:51,480 --> 00:54:55,480
You could see that sometimes people were very quick, other times people were slower.

992
00:54:55,480 --> 00:54:59,480
Sometimes there was a lot of consensus, sometimes there was a little bit less consensus, right?

993
00:54:59,480 --> 00:55:01,480
That reflects uncertainty.

994
00:55:01,480 --> 00:55:04,480
So again, there's a long history of studying this scientifically.

995
00:55:05,480 --> 00:55:09,480
That, you know, you could, but you can see some, you can see the probabilistic inference at work.

996
00:55:09,480 --> 00:55:11,480
Probabilistic inference over what?

997
00:55:11,480 --> 00:55:18,480
Well, I would say one way to describe it is over one or a few short low precision simulations of the physics of these scenes.

998
00:55:18,480 --> 00:55:20,480
So here is what I mean by this.

999
00:55:20,480 --> 00:55:26,480
I'm going to show you a video of a game engine reconstruction of one of these scenes that simulates a small bump.

1000
00:55:26,480 --> 00:55:27,480
So here's a small bump.

1001
00:55:27,480 --> 00:55:29,480
Here's the same scene with the big bump.

1002
00:55:29,480 --> 00:55:30,480
Okay.

1003
00:55:30,480 --> 00:55:33,480
Now notice that at the micro level, different things happen.

1004
00:55:33,480 --> 00:55:38,480
But at the cognitive or macro level that matters for common sense reasoning, the same thing happened.

1005
00:55:38,480 --> 00:55:43,480
Namely, all the yellow blocks went over onto one side of the table and few or none of the red blocks did.

1006
00:55:43,480 --> 00:55:45,480
So it didn't matter which of those simulations you ran in your head.

1007
00:55:45,480 --> 00:55:47,480
You'd get the same answer in this case, right?

1008
00:55:47,480 --> 00:55:49,480
This is one that's very easy and high confidence and quick.

1009
00:55:49,480 --> 00:55:52,480
Also, you didn't have to run the simulation for very long.

1010
00:55:52,480 --> 00:55:56,480
You only have to run it for a few time steps like that to see what's going to happen or similarly here.

1011
00:55:56,480 --> 00:55:58,480
You only have to run it for a few time steps, okay?

1012
00:55:58,480 --> 00:56:00,480
And it doesn't have to be even very accurate.

1013
00:56:00,480 --> 00:56:06,480
Even a fair amount of imprecision will give you basically the same answer at the level that matters for common sense.

1014
00:56:06,480 --> 00:56:08,480
So that's the kind of thing our model does.

1015
00:56:08,480 --> 00:56:11,480
It runs a few low precision simulations for a few time steps.

1016
00:56:11,480 --> 00:56:15,480
But if you take the average of what happens there and you compare that with people's judgments,

1017
00:56:15,480 --> 00:56:17,480
you get results like what I show you here.

1018
00:56:17,480 --> 00:56:21,480
This scatter plot shows on the y-axis the average judgments of people.

1019
00:56:21,480 --> 00:56:23,480
On the x-axis, the average judgments of this model.

1020
00:56:23,480 --> 00:56:24,480
And it does a pretty good job.

1021
00:56:24,480 --> 00:56:32,480
It's not perfect, but the model basically captures people's graded sense of what's going on in this scene and many of these others, okay?

1022
00:56:32,480 --> 00:56:35,480
And it doesn't do it with any learning, but I'll come back to that in a second.

1023
00:56:35,480 --> 00:56:39,480
It just does it by probabilistic reasoning over a game physics simulation.

1024
00:56:39,480 --> 00:56:45,480
Now we can use and we have used the same kind of technology to capture in very simple forms.

1025
00:56:45,480 --> 00:56:47,480
Really just proofs of concept at this point.

1026
00:56:47,480 --> 00:56:52,480
The kind of common sense physical scene understanding in a child playing with blocks or other objects.

1027
00:56:52,480 --> 00:56:56,480
Or in what might go on in a young child's understanding of other people's actions.

1028
00:56:56,480 --> 00:56:58,480
What we call the intuitive psychology engine.

1029
00:56:58,480 --> 00:57:04,480
Where now the probabilistic programs are defined over these kind of very simple planning and perception programs.

1030
00:57:04,480 --> 00:57:05,480
And I won't go into any details.

1031
00:57:05,480 --> 00:57:09,480
I'll just point to a couple of papers that my group played a very small role in.

1032
00:57:09,480 --> 00:57:12,480
But we provided some models, which together with some infant researchers,

1033
00:57:12,480 --> 00:57:18,480
people working on both of these are experiments that were done with 10 or 12 month infants.

1034
00:57:18,480 --> 00:57:20,480
So younger than even some of the babies I showed you before.

1035
00:57:20,480 --> 00:57:23,480
But basically like that youngest baby, the one with the cat.

1036
00:57:23,480 --> 00:57:26,480
Here's an example of showing simple physical scenes.

1037
00:57:26,480 --> 00:57:33,480
These are moving objects to 12 month olds where they saw a few objects bouncing around inside a gumball machine.

1038
00:57:33,480 --> 00:57:36,480
And after some point in time, the scene gets occluded.

1039
00:57:36,480 --> 00:57:37,480
You'll see the scene is occluded.

1040
00:57:37,480 --> 00:57:41,480
And then after another period of time, one of the objects will appear at the bottom.

1041
00:57:41,480 --> 00:57:44,480
And the question is, is that the object you expected to see or not?

1042
00:57:44,480 --> 00:57:46,480
Is it expected or surprising?

1043
00:57:46,480 --> 00:57:50,480
The standard way you study what infants know is by what's called looking time methods.

1044
00:57:50,480 --> 00:57:51,480
Just like an adult.

1045
00:57:51,480 --> 00:57:54,480
If I show you something that's surprising, you might look longer.

1046
00:57:54,480 --> 00:57:57,480
If you're bored, you'll look away.

1047
00:57:57,480 --> 00:58:00,480
So you can do that same kind of thing with infants.

1048
00:58:00,480 --> 00:58:02,480
And by measuring how long they look at a scene,

1049
00:58:02,480 --> 00:58:06,480
you can measure whether you've shown them something surprising or not.

1050
00:58:06,480 --> 00:58:09,480
There are literally hundreds of studies, if not more,

1051
00:58:09,480 --> 00:58:12,480
using looking time measures to study what infants know.

1052
00:58:12,480 --> 00:58:16,480
But only with this paper that we published a few years ago,

1053
00:58:16,480 --> 00:58:19,480
did we have a quantitative model where we were able to show a relation

1054
00:58:19,480 --> 00:58:21,480
between inverse probability in this case and surprise.

1055
00:58:21,480 --> 00:58:26,480
So things which were objectively lower probability under one of these probabilistic physics simulations

1056
00:58:26,480 --> 00:58:29,480
across a number of different manipulations of how fast the objects were,

1057
00:58:29,480 --> 00:58:31,480
where they were, when the scene was occluded,

1058
00:58:31,480 --> 00:58:34,480
how long the delay was, various physically relevant variables,

1059
00:58:34,480 --> 00:58:36,480
how many objects there were of one type or another.

1060
00:58:36,480 --> 00:58:39,480
Infants' expectations connected with this model.

1061
00:58:39,480 --> 00:58:42,480
Or another paper that we published, that one was done,

1062
00:58:42,480 --> 00:58:46,480
the experiments there were done by Erno Teglas and Luca Benotti's lab.

1063
00:58:46,480 --> 00:58:51,480
Here is a study that was done just recently by Sherry Liu in Liz Spelke's lab

1064
00:58:51,480 --> 00:58:54,480
there at Harvard, but they're partners with us in CBMM,

1065
00:58:54,480 --> 00:58:56,480
which was about infants' understanding of goals.

1066
00:58:56,480 --> 00:58:59,480
So this is more like, again, understanding of agents and intuitive psychology,

1067
00:58:59,480 --> 00:59:02,480
where, again, in very simple cartoon scenes,

1068
00:59:02,480 --> 00:59:05,480
you show an infant an agent that seems to be doing something,

1069
00:59:05,480 --> 00:59:07,480
like an animated cartoon character.

1070
00:59:07,480 --> 00:59:11,480
But it jumps over a wall, or it rolls up a hill, or it jumps over a gap.

1071
00:59:11,480 --> 00:59:15,480
And the question is, basically, how much does the agent want the goal

1072
00:59:15,480 --> 00:59:17,480
that it seems to be trying to achieve?

1073
00:59:17,480 --> 00:59:21,480
And what this study showed, and the models here were done by Tomer Omen,

1074
00:59:21,480 --> 00:59:26,480
was that infants appeared to be sensitive to the physical work done by the agent.

1075
00:59:26,480 --> 00:59:31,480
The more work the agent did, in the sense of the integral of force applied over a path,

1076
00:59:31,480 --> 00:59:36,480
the more the infants thought the agent wanted the goal.

1077
00:59:36,480 --> 00:59:40,480
We think of this as representing what we've sometimes called the naive utility calculus.

1078
00:59:40,480 --> 00:59:44,480
So the idea that there's a basic calculus of cost and benefit,

1079
00:59:44,480 --> 00:59:48,480
you know, we take actions which are a little bit costly to achieve goal states

1080
00:59:48,480 --> 00:59:51,480
which give us some reward, that's the most basic way,

1081
00:59:51,480 --> 00:59:54,480
the oldest way to think about rational and intentional action.

1082
00:59:54,480 --> 00:59:57,480
And it seems that even 10-month-olds understand some version of that,

1083
00:59:57,480 --> 01:00:00,480
where the cost can be measured in physical terms.

1084
01:00:00,480 --> 01:00:03,480
I see I'm running a little bit behind on time,

1085
01:00:03,480 --> 01:00:05,480
and I wanted to leave some time for discussion.

1086
01:00:05,480 --> 01:00:08,480
So I'll just go very quickly through a couple of other things,

1087
01:00:08,480 --> 01:00:13,480
and happy to stay around at the end for discussion.

1088
01:00:13,480 --> 01:00:17,480
What I showed you here was the science, where does the engineering go?

1089
01:00:17,480 --> 01:00:20,480
So one thing you can do with this is say,

1090
01:00:20,480 --> 01:00:23,480
build a machine system that can look not a little animated cartoon

1091
01:00:23,480 --> 01:00:26,480
like these baby experiments, but a real person doing something,

1092
01:00:26,480 --> 01:00:29,480
and again, combine physical costs and constraints of actions

1093
01:00:29,480 --> 01:00:32,480
with some understanding of the agent's utilities,

1094
01:00:32,480 --> 01:00:36,480
that's the math of planning, to figure out what they wanted.

1095
01:00:36,480 --> 01:00:40,480
So look in this scene here, and see if you can judge which object

1096
01:00:40,480 --> 01:00:42,480
the woman is reaching for.

1097
01:00:42,480 --> 01:00:45,480
So you can see there's a grid of four by four objects,

1098
01:00:45,480 --> 01:00:48,480
there's 16 objects here, and she's going to be reaching for one of them.

1099
01:00:48,480 --> 01:00:50,480
It's going to play in slow motion,

1100
01:00:50,480 --> 01:00:53,480
but raise your hand when you know which one she's reaching for.

1101
01:00:53,480 --> 01:01:00,480
So just watch and raise your hand when you know which one she wants.

1102
01:01:00,480 --> 01:01:02,480
So most of the hands are up by now.

1103
01:01:02,480 --> 01:01:04,480
And notice, I was looking at your hands not here,

1104
01:01:04,480 --> 01:01:06,480
but what happened is most of the hands were up

1105
01:01:06,480 --> 01:01:10,480
at about the time when that gray, or the one that dashed line,

1106
01:01:10,480 --> 01:01:11,480
shot up.

1107
01:01:11,480 --> 01:01:15,480
That's not human data, you provided the data, this is our model.

1108
01:01:15,480 --> 01:01:17,480
So our model is predicting, more or less,

1109
01:01:17,480 --> 01:01:19,480
when you're able to say what her goal was.

1110
01:01:19,480 --> 01:01:21,480
It's well before she actually touched the object.

1111
01:01:21,480 --> 01:01:22,480
How does the model work?

1112
01:01:22,480 --> 01:01:26,480
Again, I'll skip the details, but it does the same kind of thing

1113
01:01:26,480 --> 01:01:28,480
that our models of those infants did.

1114
01:01:28,480 --> 01:01:32,480
Namely, but in this case, it does it with a full body model from robotics.

1115
01:01:32,480 --> 01:01:34,480
We use what's called the Mujoko physics engine,

1116
01:01:34,480 --> 01:01:37,480
which is a standard tool in robotics for planning

1117
01:01:37,480 --> 01:01:40,480
physically efficient reaches of, say, a humanoid robot.

1118
01:01:40,480 --> 01:01:45,480
And we say, we can give this planner program a goal object as input.

1119
01:01:45,480 --> 01:01:47,480
We can give it each of the possible goal objects as input

1120
01:01:47,480 --> 01:01:49,480
and say, plan the most physically efficient action,

1121
01:01:49,480 --> 01:01:52,480
so the one that uses the least energy to get to that object.

1122
01:01:52,480 --> 01:01:54,480
And then we can do a Bayesian inference.

1123
01:01:54,480 --> 01:01:56,480
This is the probabilistic inference part.

1124
01:01:56,480 --> 01:01:59,480
This program is the Mujoko planner.

1125
01:01:59,480 --> 01:02:02,480
But then we can say, I want to do Bayesian inference

1126
01:02:02,480 --> 01:02:04,480
to work backwards from what I observed,

1127
01:02:04,480 --> 01:02:06,480
which was the action, to the input to that program.

1128
01:02:06,480 --> 01:02:08,480
What goal was provided as input to the planner?

1129
01:02:08,480 --> 01:02:12,480
And here you can see the full array of four by four possible inputs

1130
01:02:12,480 --> 01:02:14,480
and those bars that are moving up and down,

1131
01:02:14,480 --> 01:02:16,480
that's the Bayesian posterior probability

1132
01:02:16,480 --> 01:02:19,480
of how likely each of those was to be the goal.

1133
01:02:19,480 --> 01:02:21,480
And what you can see is it converges on the right answer,

1134
01:02:21,480 --> 01:02:24,480
at least, well, it turns out to be the ground truth right answer,

1135
01:02:24,480 --> 01:02:26,480
but also the right answer according to what people think,

1136
01:02:26,480 --> 01:02:29,480
with about the same kind of data that people took.

1137
01:02:29,480 --> 01:02:31,480
Now, you might say, well, okay,

1138
01:02:31,480 --> 01:02:33,480
sure, if I just wanted to build a system

1139
01:02:33,480 --> 01:02:35,480
that could detect what somebody was reaching for,

1140
01:02:35,480 --> 01:02:38,480
I could generate a training data set of this sort of scene

1141
01:02:38,480 --> 01:02:41,480
and train something up to analyze patterns of motion.

1142
01:02:41,480 --> 01:02:43,480
But again, because the engine in your head

1143
01:02:43,480 --> 01:02:45,480
actually does something we think more like this,

1144
01:02:45,480 --> 01:02:48,480
it does what we call inverse planning over a physics model.

1145
01:02:48,480 --> 01:02:50,480
It can apply to much more interesting scenes

1146
01:02:50,480 --> 01:02:52,480
that you haven't really seen much of before.

1147
01:02:52,480 --> 01:02:54,480
So take the scene on the left, right,

1148
01:02:54,480 --> 01:02:56,480
where again, you see somebody reaching for one

1149
01:02:56,480 --> 01:02:58,480
of a four by four array of objects,

1150
01:02:58,480 --> 01:03:00,480
but what you see is a strange kind of reach.

1151
01:03:00,480 --> 01:03:02,480
Can you see why he's doing a strange reach?

1152
01:03:02,480 --> 01:03:04,480
Up there, it's a little small,

1153
01:03:04,480 --> 01:03:07,480
but you can see that he's reaching over something, right?

1154
01:03:07,480 --> 01:03:09,480
It's actually a pane of glass, right?

1155
01:03:09,480 --> 01:03:10,480
You see that?

1156
01:03:10,480 --> 01:03:13,480
And then there's this other guy who's helping him,

1157
01:03:13,480 --> 01:03:16,480
who sees what he wants and hands him the thing he wants.

1158
01:03:16,480 --> 01:03:20,480
So how does the guy on the foreground see the other guy's goal?

1159
01:03:20,480 --> 01:03:23,480
How does he infer his goal and know how to help him?

1160
01:03:23,480 --> 01:03:25,480
And then how do we look at the two of them

1161
01:03:25,480 --> 01:03:27,480
and figure out who's trying to help who?

1162
01:03:27,480 --> 01:03:29,480
Or that in a scene like this one here,

1163
01:03:29,480 --> 01:03:31,480
that it's not somebody trying to help somebody,

1164
01:03:31,480 --> 01:03:33,480
but rather the opposite, okay?

1165
01:03:33,480 --> 01:03:35,480
So here's a model on the left of how that might work, right?

1166
01:03:35,480 --> 01:03:37,480
And we think this is the kind of model

1167
01:03:37,480 --> 01:03:39,480
needed to tackle this sort of challenge here, right?

1168
01:03:39,480 --> 01:03:41,480
Basically, it's a model,

1169
01:03:41,480 --> 01:03:43,480
we take this model of planning,

1170
01:03:43,480 --> 01:03:45,480
sort of maximal expected utility planning,

1171
01:03:45,480 --> 01:03:47,480
which you can run backwards,

1172
01:03:47,480 --> 01:03:49,480
but then we recursively nest these models inside each other.

1173
01:03:49,480 --> 01:03:50,480
Okay?

1174
01:03:50,480 --> 01:03:52,480
An agent is helping another agent.

1175
01:03:52,480 --> 01:03:54,480
If this agent is acting apparently to us,

1176
01:03:54,480 --> 01:03:57,480
seems to be maximizing an expected utility,

1177
01:03:57,480 --> 01:04:00,480
that's a positive function of that agent's expectation

1178
01:04:00,480 --> 01:04:02,480
about another agent's expected utility,

1179
01:04:02,480 --> 01:04:04,480
and that's what it means to be a helper.

1180
01:04:04,480 --> 01:04:06,480
Hindering is sort of the opposite

1181
01:04:06,480 --> 01:04:09,480
if one seems to be trying to lower somebody else's utility, okay?

1182
01:04:09,480 --> 01:04:11,480
And we've used these same kind of models

1183
01:04:11,480 --> 01:04:13,480
to also describe infant's understanding

1184
01:04:13,480 --> 01:04:15,480
of helping and hindering in a range of scenes.

1185
01:04:15,480 --> 01:04:18,480
I'll just say one last word about learning

1186
01:04:18,480 --> 01:04:20,480
that everybody wants to know about learning,

1187
01:04:20,480 --> 01:04:22,480
and the key thing here,

1188
01:04:22,480 --> 01:04:24,480
and it's definitely part of any picture of AGI,

1189
01:04:24,480 --> 01:04:26,480
but the thought I want to leave you on

1190
01:04:26,480 --> 01:04:28,480
is really about what learning is about, okay?

1191
01:04:28,480 --> 01:04:31,480
It'll be just a few more slides, and then I'll stop, I promise.

1192
01:04:31,480 --> 01:04:33,480
None of the models I showed you so far

1193
01:04:33,480 --> 01:04:34,480
really did any learning.

1194
01:04:34,480 --> 01:04:37,480
They certainly didn't do any task-specific learning, okay?

1195
01:04:37,480 --> 01:04:39,480
We set up a probabilistic program, and then we let it do inference.

1196
01:04:39,480 --> 01:04:41,480
Now, that's not to say that we don't think

1197
01:04:41,480 --> 01:04:43,480
people learn to do these things, we do.

1198
01:04:43,480 --> 01:04:46,480
But the real learning goes on when you're much younger, right?

1199
01:04:46,480 --> 01:04:48,480
The other thing I showed you in basic form,

1200
01:04:48,480 --> 01:04:50,480
even a one-year-old baby can do, okay?

1201
01:04:50,480 --> 01:04:53,480
The basic learning goes on to support these kinds of abilities,

1202
01:04:53,480 --> 01:04:55,480
not that there isn't learning beyond one year,

1203
01:04:55,480 --> 01:04:57,480
but the basic way you learn to, say,

1204
01:04:57,480 --> 01:04:59,480
solve these physics problems is what goes on

1205
01:04:59,480 --> 01:05:03,480
in the brain of a child between zero and 12 months.

1206
01:05:03,480 --> 01:05:05,480
So this is just an example of some phenomena

1207
01:05:05,480 --> 01:05:08,480
that come from the literature on infant cognitive development.

1208
01:05:08,480 --> 01:05:10,480
These are very rough timelines.

1209
01:05:10,480 --> 01:05:12,480
You can take pictures of this if you like.

1210
01:05:12,480 --> 01:05:15,480
This is always a popular slide because it really is quite inspiring, I think,

1211
01:05:15,480 --> 01:05:17,480
and I can give you lots of literature pointers,

1212
01:05:17,480 --> 01:05:20,480
but I'm summarizing in very broad strokes with big error bars

1213
01:05:20,480 --> 01:05:24,480
what we've learned in the field of infant cognitive development

1214
01:05:24,480 --> 01:05:27,480
about when and how kids seem to at least come

1215
01:05:27,480 --> 01:05:30,480
to a certain understanding of basic aspects of physics.

1216
01:05:30,480 --> 01:05:34,480
So if you really want to study how people learn to be intelligent,

1217
01:05:34,480 --> 01:05:36,480
a lot of what you have to study are kids at this age.

1218
01:05:36,480 --> 01:05:39,480
You have to study what's already in their brain at zero months

1219
01:05:39,480 --> 01:05:41,480
and what they learn and how they learn

1220
01:05:41,480 --> 01:05:43,480
between four, six, eight, ten, twelve, and so on,

1221
01:05:43,480 --> 01:05:46,480
and all that beyond that, okay?

1222
01:05:46,480 --> 01:05:49,480
Now, effectively what that amounts to, we think,

1223
01:05:49,480 --> 01:05:52,480
is if what you're learning is something like, let's say,

1224
01:05:52,480 --> 01:05:56,480
an intuitive game physics engine to capture these basic abilities,

1225
01:05:56,480 --> 01:05:59,480
then what we need, if we're going to try to reverse engineer that,

1226
01:05:59,480 --> 01:06:01,480
is what we might think of as a program learning program.

1227
01:06:01,480 --> 01:06:03,480
If your knowledge is in the form of a program,

1228
01:06:03,480 --> 01:06:06,480
then you have to have programs that build other programs, right?

1229
01:06:06,480 --> 01:06:08,480
This is what I was talking about at the beginning,

1230
01:06:08,480 --> 01:06:10,480
about learning as building models of the world.

1231
01:06:10,480 --> 01:06:12,480
Or ultimately, if you think what we start off with

1232
01:06:12,480 --> 01:06:15,480
is something like a game engine that can play any game,

1233
01:06:15,480 --> 01:06:17,480
then what you have to learn is the program of the game

1234
01:06:17,480 --> 01:06:18,480
that you're actually playing,

1235
01:06:18,480 --> 01:06:21,480
or the many different games that you might be playing over your life.

1236
01:06:21,480 --> 01:06:24,480
So think of learning as programming the game engine in your head

1237
01:06:24,480 --> 01:06:27,480
to fit with your experience and to fit with the possible actions

1238
01:06:27,480 --> 01:06:29,480
that you seem like you can take.

1239
01:06:29,480 --> 01:06:32,480
Now, this is what you could call the hard problem of learning

1240
01:06:32,480 --> 01:06:34,480
if you come to learning from, say, neural networks

1241
01:06:34,480 --> 01:06:36,480
or other tools in machine learning, right?

1242
01:06:36,480 --> 01:06:39,480
So what makes most of machine learning go right now,

1243
01:06:39,480 --> 01:06:41,480
and certainly what makes neural networks so appealing,

1244
01:06:41,480 --> 01:06:44,480
is that you can set up basically a big function approximator

1245
01:06:44,480 --> 01:06:47,480
that can approximate many of the functions you might want to do

1246
01:06:47,480 --> 01:06:49,480
in a certain application or task,

1247
01:06:49,480 --> 01:06:51,480
but in a way that's end-to-end differentiable

1248
01:06:51,480 --> 01:06:53,480
and with a meaningful cost function.

1249
01:06:53,480 --> 01:06:55,480
So you can have one of these nice optimization landscapes.

1250
01:06:55,480 --> 01:06:58,480
You can compute the gradients and basically just roll downhill

1251
01:06:58,480 --> 01:07:00,480
until you get to an optimal solution.

1252
01:07:00,480 --> 01:07:03,480
But if you're talking about learning as something like search

1253
01:07:03,480 --> 01:07:05,480
in the space of programs,

1254
01:07:05,480 --> 01:07:07,480
we don't know how to do anything like that yet.

1255
01:07:07,480 --> 01:07:10,480
We don't know how to set this up as any kind of a nice optimization problem

1256
01:07:10,480 --> 01:07:13,480
with any notion of smoothness or gradients, okay?

1257
01:07:13,480 --> 01:07:16,480
Rather, what we need is instead of learning as, like,

1258
01:07:16,480 --> 01:07:18,480
rolling downhill effectively, right,

1259
01:07:18,480 --> 01:07:20,480
a process which just, if you're willing to wait long enough,

1260
01:07:20,480 --> 01:07:24,480
you know, some, you know, simple algorithm we'll take care of,

1261
01:07:24,480 --> 01:07:27,480
think of what we call the idea of learning as programming.

1262
01:07:27,480 --> 01:07:30,480
There's a popular metaphor in cognitive development

1263
01:07:30,480 --> 01:07:32,480
called the child as scientist,

1264
01:07:32,480 --> 01:07:35,480
which emphasizes children as active theory builders

1265
01:07:35,480 --> 01:07:39,480
and children's play as a kind of casual experimentation.

1266
01:07:39,480 --> 01:07:41,480
But this is the algorithmic complement to that,

1267
01:07:41,480 --> 01:07:43,480
what we call the child as coder,

1268
01:07:43,480 --> 01:07:45,480
or around MIT we'll say the child as hacker,

1269
01:07:45,480 --> 01:07:47,480
but the rest of the world, if you say child as hacker,

1270
01:07:47,480 --> 01:07:49,480
they think of someone who breaks into your email

1271
01:07:49,480 --> 01:07:51,480
and steals your credit card numbers,

1272
01:07:51,480 --> 01:07:55,480
we all know that hacking is, you know, making your code more awesome, right?

1273
01:07:55,480 --> 01:07:57,480
If your knowledge is some kind of code,

1274
01:07:57,480 --> 01:07:59,480
or, you know, library of programs,

1275
01:07:59,480 --> 01:08:02,480
then learning is all the ways that a child hacks on their code

1276
01:08:02,480 --> 01:08:04,480
to make it more awesome.

1277
01:08:04,480 --> 01:08:06,480
Awesome can mean more accurate,

1278
01:08:06,480 --> 01:08:08,480
but it can also mean faster, more elegant,

1279
01:08:08,480 --> 01:08:11,480
more transportable to other applications or their tasks,

1280
01:08:11,480 --> 01:08:14,480
more explainable to others, maybe just more entertaining, okay?

1281
01:08:14,480 --> 01:08:17,480
Children have all of those goals in learning,

1282
01:08:17,480 --> 01:08:20,480
and the activities by which they make their code more awesome

1283
01:08:20,480 --> 01:08:24,480
also correspond to many of the activities of coding, right?

1284
01:08:24,480 --> 01:08:27,480
So think about all the ways on a day-to-day basis

1285
01:08:27,480 --> 01:08:29,480
you might make your code more awesome, right?

1286
01:08:29,480 --> 01:08:33,480
You might have a big library of existing functions

1287
01:08:33,480 --> 01:08:35,480
with some parameters that you can tune on a data set.

1288
01:08:35,480 --> 01:08:38,480
That's basically what you do with backprop or stochastic gradient descent

1289
01:08:38,480 --> 01:08:40,480
in training a deep learning system.

1290
01:08:40,480 --> 01:08:43,480
But think about all the ways in which you might actually modify the underlying function.

1291
01:08:43,480 --> 01:08:46,480
So write new code, or take old code from some other thing

1292
01:08:46,480 --> 01:08:49,480
and map it over here, or make a whole new library of code,

1293
01:08:49,480 --> 01:08:51,480
or refactor your code to some other, you know,

1294
01:08:51,480 --> 01:08:54,480
some other basis for it that will work more robustly

1295
01:08:54,480 --> 01:08:58,480
and be more extensible, or transpiling, or compiling, right?

1296
01:08:58,480 --> 01:09:00,480
Or even just commenting your code,

1297
01:09:00,480 --> 01:09:02,480
or asking someone else for their code, okay?

1298
01:09:02,480 --> 01:09:05,480
Again, these are all ways that we make our code more awesome

1299
01:09:05,480 --> 01:09:08,480
and children's learning has analogues to all of these

1300
01:09:08,480 --> 01:09:10,480
that we would want to understand as an engineer

1301
01:09:10,480 --> 01:09:12,480
from an algorithmic point of view.

1302
01:09:12,480 --> 01:09:15,480
So in our group, we've been working on various early steps towards this.

1303
01:09:15,480 --> 01:09:19,480
And again, we don't have anything like program writing programs

1304
01:09:19,480 --> 01:09:21,480
at the level of children's learning algorithms.

1305
01:09:21,480 --> 01:09:23,480
But one example of something that we did in our group,

1306
01:09:23,480 --> 01:09:25,480
which you might not have thought of being about this,

1307
01:09:25,480 --> 01:09:27,480
but it's definitely the AI work we did

1308
01:09:27,480 --> 01:09:31,480
that got the most attention in the last couple of years from our group.

1309
01:09:31,480 --> 01:09:33,480
We had this paper that was in science,

1310
01:09:33,480 --> 01:09:35,480
it was actually on the cover of science,

1311
01:09:35,480 --> 01:09:38,480
sort of just hit the market at the right time, if you like,

1312
01:09:38,480 --> 01:09:40,480
and it got about 100 times more publicity

1313
01:09:40,480 --> 01:09:42,480
than anything else I've ever done,

1314
01:09:42,480 --> 01:09:44,480
which is partly a testament to the really great work

1315
01:09:44,480 --> 01:09:47,480
that Brendan Lake, who was the first author, did for his PhD here,

1316
01:09:47,480 --> 01:09:50,480
but much more so just about the hunger for AI systems at the time

1317
01:09:50,480 --> 01:09:52,480
when we published this in 2015.

1318
01:09:52,480 --> 01:09:55,480
And we built a machine system that, the way we described it,

1319
01:09:55,480 --> 01:09:58,480
was doing human-level concept learning for simple concepts,

1320
01:09:58,480 --> 01:10:00,480
very simple visual concepts.

1321
01:10:00,480 --> 01:10:03,480
These handwritten characters in many of the world's alphabets.

1322
01:10:03,480 --> 01:10:05,480
For those of you who know the famous MNIST dataset,

1323
01:10:05,480 --> 01:10:08,480
the dataset of handwritten digits 0 through 9,

1324
01:10:08,480 --> 01:10:11,480
sorry, that drove so much good research in deep learning

1325
01:10:11,480 --> 01:10:13,480
and pattern recognition,

1326
01:10:13,480 --> 01:10:16,480
it did that not because Jan Lacoon, who put that together,

1327
01:10:16,480 --> 01:10:19,480
or Jeff Hinton, who did a lot of work on deep learning with MNIST,

1328
01:10:19,480 --> 01:10:21,480
they were interested fundamentally in character recognition,

1329
01:10:21,480 --> 01:10:24,480
that they saw that as a very simple test bed

1330
01:10:24,480 --> 01:10:26,480
for developing more general ideas.

1331
01:10:26,480 --> 01:10:28,480
And similarly, we did this work on getting machines

1332
01:10:28,480 --> 01:10:32,480
to do a kind of one-shot learning of generative models,

1333
01:10:32,480 --> 01:10:34,480
also to develop more general ideas.

1334
01:10:34,480 --> 01:10:38,480
We saw this as learning very simple, little mini probabilistic programs.

1335
01:10:38,480 --> 01:10:40,480
In this case, what are those programs?

1336
01:10:40,480 --> 01:10:42,480
They're the programs you use to draw a character.

1337
01:10:42,480 --> 01:10:45,480
So ask yourself, how can you look at any one of these characters

1338
01:10:45,480 --> 01:10:47,480
and see, in a sense, how somebody might draw it?

1339
01:10:47,480 --> 01:10:51,480
The way we tested this in our system was this little visual-turing test

1340
01:10:51,480 --> 01:10:54,480
where we showed people one character in a novel alphabet

1341
01:10:54,480 --> 01:10:56,480
and we said, draw another one.

1342
01:10:56,480 --> 01:10:58,480
We paired nine people, like say on the left,

1343
01:10:58,480 --> 01:11:01,480
and nine samples from our machine, say on the right,

1344
01:11:01,480 --> 01:11:03,480
and we asked other people,

1345
01:11:03,480 --> 01:11:05,480
could you tell which was the human drawing another example,

1346
01:11:05,480 --> 01:11:08,480
or imagining another example, and which was the machine?

1347
01:11:08,480 --> 01:11:09,480
And people couldn't tell.

1348
01:11:09,480 --> 01:11:11,480
When I said ones on the left and ones on the right,

1349
01:11:11,480 --> 01:11:12,480
I don't actually remember,

1350
01:11:12,480 --> 01:11:14,480
and on different ones you can see if you can tell.

1351
01:11:14,480 --> 01:11:15,480
It's very hard to tell.

1352
01:11:15,480 --> 01:11:17,480
Can you tell for each one of these characters

1353
01:11:17,480 --> 01:11:21,480
which new set of examples were drawn by a human versus a machine?

1354
01:11:21,480 --> 01:11:24,480
Here's the right answer, and probably you couldn't tell.

1355
01:11:24,480 --> 01:11:28,480
The way we did this was by assembling a simple kind of program learning program.

1356
01:11:28,480 --> 01:11:30,480
So we basically said, when you draw a character,

1357
01:11:30,480 --> 01:11:33,480
you're assembling strokes and substrokes with goals and sub-goals

1358
01:11:33,480 --> 01:11:35,480
that produce ink on the page,

1359
01:11:35,480 --> 01:11:38,480
and when you see a character, you're working backwards to figure out

1360
01:11:38,480 --> 01:11:41,480
what was the program, the most efficient program that did that.

1361
01:11:41,480 --> 01:11:44,480
So you're basically inverting a probabilistic program,

1362
01:11:44,480 --> 01:11:46,480
doing Bayesian inference to the program,

1363
01:11:46,480 --> 01:11:48,480
most likely to have generated what you saw.

1364
01:11:48,480 --> 01:11:50,480
This is one small step, we think,

1365
01:11:50,480 --> 01:11:52,480
towards being able to learn programs,

1366
01:11:52,480 --> 01:11:55,480
to being able to learn something ultimately like a whole game engine program.

1367
01:11:55,480 --> 01:11:59,480
The last thing I'll leave you with is just a pointer to sort of work in action.

1368
01:11:59,480 --> 01:12:02,480
So this is some work being done by a current PhD student

1369
01:12:02,480 --> 01:12:03,480
who works partly with me,

1370
01:12:03,480 --> 01:12:05,480
but also with Armando, Solar Lasama, and CSAIL.

1371
01:12:05,480 --> 01:12:06,480
This is Kevin Ellis.

1372
01:12:06,480 --> 01:12:09,480
It's an example of what's now, I think, again,

1373
01:12:09,480 --> 01:12:11,480
an emerging, exciting area in AI

1374
01:12:11,480 --> 01:12:13,480
well beyond anything that we're doing,

1375
01:12:13,480 --> 01:12:16,480
is combining techniques from where Armando comes from,

1376
01:12:16,480 --> 01:12:18,480
which is the world of programming languages,

1377
01:12:18,480 --> 01:12:20,480
not machine learning or AI,

1378
01:12:20,480 --> 01:12:22,480
but tools from programming languages

1379
01:12:22,480 --> 01:12:25,480
which can be used to automatically synthesize code.

1380
01:12:25,480 --> 01:12:27,480
With the machine learning toolkit,

1381
01:12:27,480 --> 01:12:30,480
in this case a kind of Bayesian minimum description-length idea,

1382
01:12:30,480 --> 01:12:32,480
to be able to make, again,

1383
01:12:32,480 --> 01:12:35,480
what is really one small step towards machines that can learn programs

1384
01:12:35,480 --> 01:12:39,480
by basically trying to efficiently find the shortest, simplest program

1385
01:12:39,480 --> 01:12:41,480
which can capture some data set.

1386
01:12:41,480 --> 01:12:43,480
So we think by combining these kinds of tools,

1387
01:12:43,480 --> 01:12:46,480
in this case, let's say, from Bayesian inference over programs

1388
01:12:46,480 --> 01:12:48,480
with a number of tools that have been developed

1389
01:12:48,480 --> 01:12:50,480
in other areas of computer science

1390
01:12:50,480 --> 01:12:52,480
that don't look anything or haven't been considered

1391
01:12:52,480 --> 01:12:55,480
to be machine learning or AI, like programming languages.

1392
01:12:55,480 --> 01:12:58,480
It's one of the many ways that, going forward,

1393
01:12:58,480 --> 01:13:01,480
we're going to be able to build smarter, more human-like machines.

1394
01:13:01,480 --> 01:13:05,480
So just to end then, what I've tried to tell you here is talk,

1395
01:13:05,480 --> 01:13:08,480
first of all, identify the ways in which human intelligence

1396
01:13:08,480 --> 01:13:10,480
goes beyond pattern recognition

1397
01:13:10,480 --> 01:13:12,480
to really all these activities of modeling the world.

1398
01:13:12,480 --> 01:13:14,480
To give you a sense of some of the domains

1399
01:13:14,480 --> 01:13:16,480
where we can start to study this,

1400
01:13:16,480 --> 01:13:19,480
like common sense scene understanding, for example,

1401
01:13:19,480 --> 01:13:23,480
or something like one-shot learning, for example,

1402
01:13:23,480 --> 01:13:25,480
like what we were just doing there,

1403
01:13:25,480 --> 01:13:28,480
or learning as programming the engine in your head.

1404
01:13:28,480 --> 01:13:31,480
And to give you a sense of some of the technical tools,

1405
01:13:31,480 --> 01:13:34,480
probabilistic programs, program synthesis, game engines,

1406
01:13:34,480 --> 01:13:37,480
for example, as well as a little bit of deep learning,

1407
01:13:37,480 --> 01:13:41,480
that bringing together, we're starting to be able to make these things real.

1408
01:13:41,480 --> 01:13:44,480
Now, that's the science agenda and the reverse engineering agenda,

1409
01:13:44,480 --> 01:13:47,480
but think about, for those of you who are interested in technology,

1410
01:13:47,480 --> 01:13:50,480
what are the many big AI frontiers that this opens up?

1411
01:13:50,480 --> 01:13:53,480
So the one I'm most excited about is this idea

1412
01:13:53,480 --> 01:13:56,480
which I've highlighted here in our big research agenda.

1413
01:13:56,480 --> 01:13:58,480
This is the one I'm most excited about to work on

1414
01:13:58,480 --> 01:14:00,480
for the rest of my career, honestly.

1415
01:14:00,480 --> 01:14:04,480
But it's really what is the oldest and maybe the best dream

1416
01:14:04,480 --> 01:14:08,480
of AI researchers of how to build a human-like intelligence system,

1417
01:14:08,480 --> 01:14:10,480
a real AGI system.

1418
01:14:10,480 --> 01:14:13,480
It's the idea that Turing proposed when he proposed the Turing test,

1419
01:14:13,480 --> 01:14:15,480
and Minsky proposed this at different times in his life.

1420
01:14:15,480 --> 01:14:17,480
Or many people have proposed this, right?

1421
01:14:17,480 --> 01:14:19,480
Which is to build a system that grows into intelligence

1422
01:14:19,480 --> 01:14:22,480
the way a human does, that starts like a baby and learns like a child.

1423
01:14:22,480 --> 01:14:26,480
And I've tried to show you how we're starting to be able to understand those things.

1424
01:14:26,480 --> 01:14:30,480
What a baby's mind starts with, how children actually learn.

1425
01:14:30,480 --> 01:14:32,480
And looking forward, we might imagine

1426
01:14:32,480 --> 01:14:34,480
that someday we'll be able to build machines that can do this.

1427
01:14:34,480 --> 01:14:37,480
I think we can actually start working on this right now.

1428
01:14:37,480 --> 01:14:40,480
And that's something that we're doing in our group.

1429
01:14:40,480 --> 01:14:43,480
So if that kind of thing excites you, then I encourage you to work on it,

1430
01:14:43,480 --> 01:14:44,480
maybe even with us.

1431
01:14:44,480 --> 01:14:47,480
Or if any one of these other activities of human intelligence excite you,

1432
01:14:47,480 --> 01:14:51,480
I think taking the kind of science-based reverse engineering approach

1433
01:14:51,480 --> 01:14:55,480
that we're doing and then trying to put that into engineering practice,

1434
01:14:55,480 --> 01:14:58,480
this is not just a possible route,

1435
01:14:58,480 --> 01:15:01,480
but I think it's quite possibly the most valuable route

1436
01:15:01,480 --> 01:15:04,480
that you could work on right now to try to actually achieve

1437
01:15:04,480 --> 01:15:07,480
at least some kind of artificial general intelligence,

1438
01:15:07,480 --> 01:15:10,480
especially the kind of intelligence, AI system,

1439
01:15:10,480 --> 01:15:13,480
that's going to live in a human world and interact with humans.

1440
01:15:13,480 --> 01:15:16,480
There's many kinds of AI systems that could live in worlds of data

1441
01:15:16,480 --> 01:15:18,480
that none of us can understand or will ever live in ourselves.

1442
01:15:18,480 --> 01:15:21,480
But if you want to build machines that can live in our world

1443
01:15:21,480 --> 01:15:24,480
and interact with us the way we are used to interacting with other people,

1444
01:15:24,480 --> 01:15:27,480
then I think this is a route that you should consider.

1445
01:15:27,480 --> 01:15:28,480
Okay, thank you.

1446
01:15:28,480 --> 01:15:39,480
Hi there.

1447
01:15:39,480 --> 01:15:43,480
So, early in the talk you expressed some skepticism about whether or not

1448
01:15:43,480 --> 01:15:46,480
industry would get us to understanding human level intelligence.

1449
01:15:46,480 --> 01:15:49,480
It seems that there's a couple of trends that favour industry.

1450
01:15:49,480 --> 01:15:52,480
One is that industry is better than that academia at accumulating resources

1451
01:15:52,480 --> 01:15:54,480
and plowing them back into the topic.

1452
01:15:54,480 --> 01:15:57,480
And it seems at the moment we've got a bit of brain drain going on

1453
01:15:57,480 --> 01:16:01,480
from academia into industry, and that seems like an ongoing trend.

1454
01:16:01,480 --> 01:16:05,480
If you look at something like learning to fly or learning to fly into space,

1455
01:16:05,480 --> 01:16:10,480
then it looks like the story is one of industry kind of taking over the field

1456
01:16:10,480 --> 01:16:13,480
and going off on its own a little bit.

1457
01:16:13,480 --> 01:16:17,480
Academics still have a role, but industry kind of dominates,

1458
01:16:17,480 --> 01:16:19,480
so is industry going to overtake the field, do you think?

1459
01:16:19,480 --> 01:16:21,480
Well, that's a really good question,

1460
01:16:21,480 --> 01:16:23,480
and it's got several good questions packed into one there, right?

1461
01:16:23,480 --> 01:16:26,480
I didn't mean to say, this wasn't meant to say,

1462
01:16:26,480 --> 01:16:29,480
go academia, bad industry, right?

1463
01:16:29,480 --> 01:16:34,480
What I tried to say was the approaches that are currently getting

1464
01:16:34,480 --> 01:16:36,480
the most attention in industry, and they're really,

1465
01:16:36,480 --> 01:16:39,480
because they're really the most valuable ones right now for the short term,

1466
01:16:39,480 --> 01:16:42,480
you know, any industry is really focused on what it can do,

1467
01:16:42,480 --> 01:16:45,480
what are the value propositions on basically a two-year timescale at most?

1468
01:16:45,480 --> 01:16:48,480
I mean, if you ask, say, Google researchers to take the most prominent example,

1469
01:16:48,480 --> 01:16:51,480
that's pretty much what they'll all tell you, okay?

1470
01:16:51,480 --> 01:16:56,480
Maybe things that might pay off initially in two years,

1471
01:16:56,480 --> 01:16:58,480
but maybe take five years or more to really develop,

1472
01:16:58,480 --> 01:17:01,480
but if you can't show that it's going to do something practical for us

1473
01:17:01,480 --> 01:17:03,480
in two years in a way that matters for our bottom line,

1474
01:17:03,480 --> 01:17:06,480
then it's not really worth doing, okay?

1475
01:17:06,480 --> 01:17:10,480
So what I'm talking about is the technologies which right now

1476
01:17:10,480 --> 01:17:13,480
industry sees as meeting that specification,

1477
01:17:13,480 --> 01:17:18,480
and what I'm saying is right now, I think that's not where the root is

1478
01:17:18,480 --> 01:17:22,480
to something like human-like, but not the most valuable promising root

1479
01:17:22,480 --> 01:17:24,480
to human-like kinds of AI systems, all right?

1480
01:17:24,480 --> 01:17:27,480
But I hope that like in the case as you said, you know,

1481
01:17:27,480 --> 01:17:30,480
the basic research that we're doing now will be successful enough

1482
01:17:30,480 --> 01:17:34,480
that it will get the attention of industry when the time is right.

1483
01:17:34,480 --> 01:17:37,480
So, you know, I hope at some point, you know,

1484
01:17:37,480 --> 01:17:41,480
it won't, at least the engineering side will have to be done in industry,

1485
01:17:41,480 --> 01:17:46,480
not just in academia, but you're also pointing to issues of like brain drain

1486
01:17:46,480 --> 01:17:49,480
and other things like that, that I think these are real issues confronting our community.

1487
01:17:49,480 --> 01:17:53,480
I think everybody knows this, and I'm sure this will come up multiple times here,

1488
01:17:53,480 --> 01:17:57,480
which is, you know, I think we have to find ways to, even now,

1489
01:17:57,480 --> 01:18:02,480
to combine the best of the ideas, the energy, and the resources of academia and industry

1490
01:18:02,480 --> 01:18:06,480
if we want to keep doing basically something interesting, right?

1491
01:18:06,480 --> 01:18:10,480
If we just want to redefine AI to be, well, whatever people currently call AI

1492
01:18:10,480 --> 01:18:14,480
but scaled up, well, then fine, forget about it.

1493
01:18:14,480 --> 01:18:17,480
Or if we just want to say, let me and people like me do what we're doing

1494
01:18:17,480 --> 01:18:21,480
at what industry we consider a snail's pace on toy problems, okay, fine.

1495
01:18:21,480 --> 01:18:26,480
But if we want to, you know, if I want to take what I'm doing to the level

1496
01:18:26,480 --> 01:18:30,480
that will really be, you know, paying off the level that industry can appreciate

1497
01:18:30,480 --> 01:18:34,480
or just that really has technological impact on a broad scale, right?

1498
01:18:34,480 --> 01:18:38,480
Or I think if industry wants to take what it's doing and really build machines

1499
01:18:38,480 --> 01:18:42,480
that are actually intelligent, right, or machine learning that actually learns like a person,

1500
01:18:42,480 --> 01:18:46,480
then I think we need each other now and not just in some point in the future.

1501
01:18:46,480 --> 01:18:50,480
So this is a general challenge for MIT and for everywhere and for Google.

1502
01:18:50,480 --> 01:18:54,480
I mean, we just spent a few days talking to Google about exactly this issue.

1503
01:18:54,480 --> 01:18:57,480
In fact, this was a talk I prepared partly for that purpose.

1504
01:18:57,480 --> 01:19:01,480
So we wanted to raise those issues and it's just, I mean, really, I don't know what,

1505
01:19:01,480 --> 01:19:05,480
I mean, rather I can think of some solutions to that problem

1506
01:19:05,480 --> 01:19:08,480
of what you could call brain drain from the academic point of view

1507
01:19:08,480 --> 01:19:12,480
or what you could call just narrowing in into certain local minima in the industry point of view.

1508
01:19:12,480 --> 01:19:17,480
But they will require the leadership of both academic institutions like MIT

1509
01:19:17,480 --> 01:19:20,480
and companies like Google being creative about how they might work together

1510
01:19:20,480 --> 01:19:22,480
in ways that are a little bit outside of their comfort zone.

1511
01:19:22,480 --> 01:19:27,480
I hope that will start to happen, including at MIT and at many other universities

1512
01:19:27,480 --> 01:19:29,480
and at companies like Google and many others.

1513
01:19:29,480 --> 01:19:33,480
And I think we need it to happen for the health of all parties concerned.

1514
01:19:33,480 --> 01:19:34,480
Okay, thank you very much.

1515
01:19:34,480 --> 01:19:36,480
Thanks.

1516
01:19:36,480 --> 01:19:42,480
I'm curious about sort of the premise that you gave that one of the big gaps missing

1517
01:19:42,480 --> 01:19:49,480
at determining intelligence is the fact that we need to teach machines how to recognize models.

1518
01:19:49,480 --> 01:19:57,480
And I'm curious as to what you think sort of non-goal-oriented cognitive activity comes into play.

1519
01:19:57,480 --> 01:20:03,480
There are things like feelings and emotions and why you don't think

1520
01:20:03,480 --> 01:20:09,480
that might not necessarily be like the most important question.

1521
01:20:09,480 --> 01:20:14,480
The only reason emotions didn't appear on my slide is because there's a few reasons,

1522
01:20:14,480 --> 01:20:16,480
but the slide is only so big.

1523
01:20:16,480 --> 01:20:19,480
I wanted the font to be big, readable for such an important slide.

1524
01:20:19,480 --> 01:20:24,480
I have versions of my slide in which I do talk about that.

1525
01:20:24,480 --> 01:20:27,480
It's not that I think feelings or emotions aren't important.

1526
01:20:27,480 --> 01:20:28,480
I think they are important.

1527
01:20:28,480 --> 01:20:32,480
And I used to not have many insights about what to do about them,

1528
01:20:32,480 --> 01:20:36,480
but actually partly based on some of my colleagues here at MIT, BCS,

1529
01:20:36,480 --> 01:20:41,480
Laura Shultz and Rebecca Sacks, two of my cognitive colleagues who I work closely with,

1530
01:20:41,480 --> 01:20:45,480
they've been starting to do research on how people understand emotions,

1531
01:20:45,480 --> 01:20:49,480
both their own and others, and we've been starting to work with them on computational models.

1532
01:20:49,480 --> 01:20:52,480
So that's actually something I'm actively interested in and even working on.

1533
01:20:52,480 --> 01:20:55,480
But I would say, and again, for those of you who study emotion or know about this,

1534
01:20:55,480 --> 01:20:57,480
actually you're going to have Lisa coming in, right?

1535
01:20:57,480 --> 01:21:00,480
So she's going to basically say a version of the same thing, I think.

1536
01:21:00,480 --> 01:21:03,480
The deepest way to understand, she's one of the world's experts on this,

1537
01:21:03,480 --> 01:21:07,480
the deepest way to understand emotion is very much based on our mental models

1538
01:21:07,480 --> 01:21:10,480
of ourselves, of the situation we're in and of other people, right?

1539
01:21:10,480 --> 01:21:16,480
Think about, for example, all of the different, I mean, if you think about,

1540
01:21:16,480 --> 01:21:19,480
I mean, again, Lisa will talk all about this, but if you think about emotion,

1541
01:21:19,480 --> 01:21:22,480
it's just a very small set of what are sometimes called basic emotions,

1542
01:21:22,480 --> 01:21:29,480
like being happy or angry or sad or, you know, those are small number of them, right?

1543
01:21:29,480 --> 01:21:32,480
There's usually only a few, right?

1544
01:21:32,480 --> 01:21:36,480
You might not say, you might see that as somehow like very basic things

1545
01:21:36,480 --> 01:21:38,480
that are opposed to some kind of cognitive activity.

1546
01:21:38,480 --> 01:21:42,480
But think about all the different words we have for emotion, right?

1547
01:21:42,480 --> 01:21:47,480
For example, think about a famous cognitive emotion like regret.

1548
01:21:47,480 --> 01:21:50,480
What does it mean to feel regret or frustration, right?

1549
01:21:50,480 --> 01:21:55,480
To know both for yourself when you're not just feeling kind of down or negative,

1550
01:21:55,480 --> 01:21:57,480
but you're feeling regret.

1551
01:21:57,480 --> 01:22:01,480
That means something like I have to feel like there's a situation that came out

1552
01:22:01,480 --> 01:22:06,480
differently from how I hoped and I realized I could have done something differently, right?

1553
01:22:06,480 --> 01:22:09,480
So that means you have to be able to understand, you have to have a model.

1554
01:22:09,480 --> 01:22:12,480
You have to be able to do a kind of counterfactual reasoning and to think,

1555
01:22:12,480 --> 01:22:15,480
oh, if only I had acted a different way, then I can predict that the world would have

1556
01:22:15,480 --> 01:22:17,480
come out differently and that's the situation I wanted,

1557
01:22:17,480 --> 01:22:20,480
but instead it came out this other way, right?

1558
01:22:20,480 --> 01:22:24,480
Or think about frustration, again, that requires something like understanding,

1559
01:22:24,480 --> 01:22:27,480
okay, I've tried a bunch of times, I thought this would work,

1560
01:22:27,480 --> 01:22:30,480
but it doesn't seem to be working, maybe I'm ready to give up.

1561
01:22:30,480 --> 01:22:33,480
Those are all, those are very important human emotions.

1562
01:22:33,480 --> 01:22:36,480
We have to understand ourselves, we need that,

1563
01:22:36,480 --> 01:22:38,480
to understand other people, to understand communication,

1564
01:22:38,480 --> 01:22:42,480
but those are all filtered through the kinds of models of action

1565
01:22:42,480 --> 01:22:44,480
that I was, just the ones I was talking about here

1566
01:22:44,480 --> 01:22:46,480
with these say cost-benefit analyses of action.

1567
01:22:46,480 --> 01:22:49,480
So what I'm, so I'm just trying to say I think this is very basic stuff

1568
01:22:49,480 --> 01:22:52,480
that will be the basis for building, I think,

1569
01:22:52,480 --> 01:22:56,480
better engineering-style models of the full spectrum of human emotion

1570
01:22:56,480 --> 01:22:59,480
beyond just like, well, I'm feeling good or bad or scared, okay?

1571
01:22:59,480 --> 01:23:02,480
And I think when you see Lisa, she will, in her own way,

1572
01:23:02,480 --> 01:23:04,480
say something very similar.

1573
01:23:04,480 --> 01:23:06,480
Interesting, thanks.

1574
01:23:06,480 --> 01:23:07,480
Yeah.

1575
01:23:07,480 --> 01:23:09,480
Thanks, Josh, for your nice talk.

1576
01:23:09,480 --> 01:23:13,480
So all is about human cognition and try to build a model to mimic those cognition,

1577
01:23:13,480 --> 01:23:16,480
but you don't, how much could help you to understand

1578
01:23:16,480 --> 01:23:18,480
how the circuit implement those things?

1579
01:23:18,480 --> 01:23:20,480
I mean, like the circuit's in the brain.

1580
01:23:20,480 --> 01:23:21,480
Yeah, and what's the...

1581
01:23:21,480 --> 01:23:23,480
Is that what you work on by any chance?

1582
01:23:23,480 --> 01:23:24,480
Sorry, what?

1583
01:23:24,480 --> 01:23:25,480
Is that what you work on by any chance?

1584
01:23:25,480 --> 01:23:26,480
Yeah, I know.

1585
01:23:26,480 --> 01:23:28,480
I'm kidding, yeah, yeah.

1586
01:23:28,480 --> 01:23:30,480
So in the center for brains, minds, and machines,

1587
01:23:30,480 --> 01:23:32,480
as well as in brain and cognitive science, yeah,

1588
01:23:32,480 --> 01:23:36,480
I have a number of colleagues who study the actual hardware basis

1589
01:23:36,480 --> 01:23:38,480
of this stuff in the brain, and that includes

1590
01:23:38,480 --> 01:23:40,480
like the large-scale architecture of the brain,

1591
01:23:40,480 --> 01:23:42,480
say like what Nancy Camuscher, Rebecca Sacks,

1592
01:23:42,480 --> 01:23:45,480
study with functional brain imaging, or the more detailed circuitry,

1593
01:23:45,480 --> 01:23:48,480
usually requires recording from, say, non-human brains, right,

1594
01:23:48,480 --> 01:23:51,480
at the level of individual neurons and connections between neurons.

1595
01:23:51,480 --> 01:23:53,480
All right, so I'm very interested in those things,

1596
01:23:53,480 --> 01:23:56,480
although it's not mostly what I work on, right?

1597
01:23:56,480 --> 01:23:59,480
But I would say, you know, again, like in many other areas of science,

1598
01:23:59,480 --> 01:24:02,480
certainly in neuroscience, the kind of work I'm talking about here

1599
01:24:02,480 --> 01:24:04,480
in a sort of classic reductionist program

1600
01:24:04,480 --> 01:24:07,480
sets the target for what we might look for.

1601
01:24:07,480 --> 01:24:09,480
Like if I just want to go...

1602
01:24:09,480 --> 01:24:10,480
I mean, I would...

1603
01:24:10,480 --> 01:24:14,480
What I would assert, right, in my working conjecture

1604
01:24:14,480 --> 01:24:17,480
is that if you do the kind of work that I'm talking about here,

1605
01:24:17,480 --> 01:24:19,480
it gives you the right targets

1606
01:24:19,480 --> 01:24:21,480
or gives you a candidate set of targets to look for,

1607
01:24:21,480 --> 01:24:24,480
what are the neural circuits computing, right?

1608
01:24:24,480 --> 01:24:29,480
Whereas if you just go in and just, say, start poking around in the brain

1609
01:24:29,480 --> 01:24:31,480
or have some idea that what you're going to try to do

1610
01:24:31,480 --> 01:24:33,480
is find the neural circuits which underlie behavior,

1611
01:24:33,480 --> 01:24:38,480
without a sense of the computations needed to produce those behaviors,

1612
01:24:38,480 --> 01:24:42,480
I think it's going to be very difficult to know what to look for

1613
01:24:42,480 --> 01:24:45,480
and to know when you've found even viable answers.

1614
01:24:45,480 --> 01:24:50,480
So I think that's the standard kind of reductionist program.

1615
01:24:50,480 --> 01:24:52,480
But it's not...

1616
01:24:52,480 --> 01:24:57,480
I also think it's not one that is divorced from the study of neural circuits.

1617
01:24:57,480 --> 01:25:00,480
It's also one, if you look at the broad picture of reverse engineering,

1618
01:25:00,480 --> 01:25:05,480
it's one where neural circuits and understanding the circuits in the brain

1619
01:25:05,480 --> 01:25:08,480
play an absolutely critical role, okay?

1620
01:25:08,480 --> 01:25:13,480
I would say when you look at the brain at the hardware level as an engineer,

1621
01:25:13,480 --> 01:25:15,480
I'm mostly looking at the software level, right?

1622
01:25:15,480 --> 01:25:18,480
But when you look at the hardware level, there are some remarkable properties.

1623
01:25:18,480 --> 01:25:21,480
One remarkable property, again, is how much parallelism there is

1624
01:25:21,480 --> 01:25:24,480
and in many ways how fast the computations are, okay?

1625
01:25:24,480 --> 01:25:28,480
Neurons are slow, but the computations of intelligence are very fast.

1626
01:25:28,480 --> 01:25:32,480
So how do we get elements that are, in some sense, quite slow in their time constant

1627
01:25:32,480 --> 01:25:34,480
to produce such intelligent behavior so quickly?

1628
01:25:34,480 --> 01:25:37,480
That's a great mystery and I think if we understood that,

1629
01:25:37,480 --> 01:25:40,480
we would have payoff for building all sorts of, you know,

1630
01:25:40,480 --> 01:25:43,480
basically application embedded circuits, okay?

1631
01:25:43,480 --> 01:25:46,480
But also, maybe most important is the power consumption

1632
01:25:46,480 --> 01:25:49,480
and again, many people have noted this, right?

1633
01:25:49,480 --> 01:25:52,480
If you look at the power consumption, the power that the brain consumes,

1634
01:25:52,480 --> 01:25:54,480
like, what did I eat today, okay?

1635
01:25:54,480 --> 01:25:56,480
Almost nothing.

1636
01:25:56,480 --> 01:25:59,480
My daughter, who's, again, she's doing an internship here,

1637
01:25:59,480 --> 01:26:01,480
literally yesterday, all she ate was a burrito.

1638
01:26:01,480 --> 01:26:05,480
And yet, she wrote 300 lines of code for her internship project

1639
01:26:05,480 --> 01:26:08,480
on a really cool computational linguistics project.

1640
01:26:08,480 --> 01:26:13,480
So somehow she turned a burrito into, you know, a model of child language acquisition, okay?

1641
01:26:13,480 --> 01:26:16,480
But how did she do that or how do any of us do this, right?

1642
01:26:16,480 --> 01:26:18,480
Where if you look at the power that we consume, when we simulate

1643
01:26:18,480 --> 01:26:22,480
even a very, very small chunk of cortex on our conventional hardware,

1644
01:26:22,480 --> 01:26:24,480
or we do any kind of machine learning thing,

1645
01:26:24,480 --> 01:26:28,480
we have systems which are very, very, very, very far from the power

1646
01:26:28,480 --> 01:26:30,480
of the human brain computationally,

1647
01:26:30,480 --> 01:26:33,480
but in terms of physical energy consumed,

1648
01:26:33,480 --> 01:26:36,480
way past what any individual brain is doing.

1649
01:26:36,480 --> 01:26:41,480
So how do we get circuitry of any sort, biological or just any physical circuits

1650
01:26:41,480 --> 01:26:44,480
to be as smart as we are with as little energy as we are?

1651
01:26:44,480 --> 01:26:49,480
This is a huge problem for basically every area of engineering, right?

1652
01:26:49,480 --> 01:26:54,480
If you want to have any kind of robot, the power consumption is a key bottleneck.

1653
01:26:54,480 --> 01:26:56,480
Same for self-driving cars.

1654
01:26:56,480 --> 01:27:01,480
If we want to build AI without contributing to global warming and climate change,

1655
01:27:01,480 --> 01:27:05,480
let alone use AI to solve climate change, we really need to address these issues,

1656
01:27:05,480 --> 01:27:09,480
and the brain is a huge guide there, right?

1657
01:27:09,480 --> 01:27:11,480
I think there are some people who are really starting to think about this.

1658
01:27:11,480 --> 01:27:15,480
How can we say, for example, build somehow brain-inspired computers

1659
01:27:15,480 --> 01:27:18,480
which are very, very low power but maybe only approximate?

1660
01:27:18,480 --> 01:27:20,480
So I'm thinking here of Joe Bates.

1661
01:27:20,480 --> 01:27:22,480
I don't know if you know Joe.

1662
01:27:22,480 --> 01:27:25,480
He's been around MIT and other places for quite a while.

1663
01:27:25,480 --> 01:27:27,480
Can I tell them about your company?

1664
01:27:27,480 --> 01:27:31,480
So Joe has a startup in Kendall Square called Singular Computing,

1665
01:27:31,480 --> 01:27:35,480
and they have some very interesting ideas including some actual implemented technology

1666
01:27:35,480 --> 01:27:39,480
for low power approximate computing in a sort of a brain-like way

1667
01:27:39,480 --> 01:27:44,480
that might lead to possibly even like the ability to build something, this is Joe's dream,

1668
01:27:44,480 --> 01:27:47,480
to build something that's about the size of this table but that has a billion cores,

1669
01:27:47,480 --> 01:27:51,480
a billion cores and runs on a reasonable kind of power consumption.

1670
01:27:51,480 --> 01:27:53,480
I would love to have such a machine.

1671
01:27:53,480 --> 01:27:57,480
If somebody wants to help Joe build it, I think he'd love to talk to you.

1672
01:27:57,480 --> 01:28:00,480
But it's one of a number of ideas.

1673
01:28:00,480 --> 01:28:02,480
I mean Google X, people are working on similar things.

1674
01:28:02,480 --> 01:28:06,480
Probably most of the major chip companies are also inspired by this idea.

1675
01:28:06,480 --> 01:28:09,480
And I think even if you didn't think you were interested in the brain,

1676
01:28:09,480 --> 01:28:12,480
if you want to build the kind of AI we're talking about

1677
01:28:12,480 --> 01:28:14,480
and run it on physical hardware of any sort

1678
01:28:14,480 --> 01:28:18,480
and understanding how the brain's circuits compute what they do,

1679
01:28:18,480 --> 01:28:22,480
what I'm talking about with as little power as they do,

1680
01:28:22,480 --> 01:28:24,480
I don't know any better place to look.

1681
01:28:24,480 --> 01:28:28,480
It seems like a lot of the improvements in AI have been driven by increasing

1682
01:28:28,480 --> 01:28:30,480
like computational power.

1683
01:28:30,480 --> 01:28:32,480
How far would you say...

1684
01:28:32,480 --> 01:28:34,480
I mean like GPUs or CPUs.

1685
01:28:34,480 --> 01:28:40,480
How far would you say we are from hardware that could run a general artificial intelligence?

1686
01:28:40,480 --> 01:28:42,480
Of the kind that I'm talking about.

1687
01:28:42,480 --> 01:28:45,480
Yeah I don't know, I'll start with a billion cores and then we'll see.

1688
01:28:45,480 --> 01:28:48,480
I mean I think we're...

1689
01:28:48,480 --> 01:28:51,480
I think there's no way to answer that question in a way that's software independent.

1690
01:28:51,480 --> 01:28:53,480
I don't know how to do that, right?

1691
01:28:53,480 --> 01:28:57,480
But I think that it's...

1692
01:28:57,480 --> 01:29:00,480
And I don't know, like when you say how far are we,

1693
01:29:00,480 --> 01:29:03,480
you mean how far am I with the resources I have right now?

1694
01:29:03,480 --> 01:29:07,480
How far am I if Google decides to put all of its resources at my disposal,

1695
01:29:07,480 --> 01:29:10,480
like they might if I were working at DeepMind?

1696
01:29:10,480 --> 01:29:12,480
I don't know the answer to that question.

1697
01:29:12,480 --> 01:29:16,480
But I think what we can say is this.

1698
01:29:16,480 --> 01:29:20,480
Individual neurons, I mean again this goes back to another reason to study neural circuits.

1699
01:29:20,480 --> 01:29:24,480
If you look at what we currently call neural networks in the AI side,

1700
01:29:24,480 --> 01:29:28,480
the model of a neuron is this very very simple thing, right?

1701
01:29:28,480 --> 01:29:30,480
Individual neurons are not only much more complex,

1702
01:29:30,480 --> 01:29:32,480
but have a lot more computational power.

1703
01:29:32,480 --> 01:29:35,480
It's not clear how they use it or whether they use it.

1704
01:29:35,480 --> 01:29:39,480
But I think it's just as likely that a neuron is something like a real you, right?

1705
01:29:39,480 --> 01:29:41,480
Is that a neuron is something like a computer.

1706
01:29:41,480 --> 01:29:46,480
Like one neuron in your brain is more like a CPU node, okay?

1707
01:29:46,480 --> 01:29:47,480
Maybe.

1708
01:29:47,480 --> 01:29:50,480
And thus the ten billion or trillion, you know,

1709
01:29:50,480 --> 01:29:53,480
the large number of neurons in your brain,

1710
01:29:53,480 --> 01:29:55,480
I think it's like ten billion cortical,

1711
01:29:55,480 --> 01:29:57,480
pyramidal neurons or something,

1712
01:29:57,480 --> 01:29:59,480
might be like ten billion cores, okay?

1713
01:29:59,480 --> 01:30:02,480
For example, that's at least as plausible I think to me as any other estimate.

1714
01:30:02,480 --> 01:30:08,480
So I think we're definitely on the underside with very big error bars.

1715
01:30:08,480 --> 01:30:10,480
So I completely agree that,

1716
01:30:10,480 --> 01:30:12,480
or if this is what you might be suggesting,

1717
01:30:12,480 --> 01:30:14,480
and going back to my answer to your question,

1718
01:30:14,480 --> 01:30:16,480
I don't think we're going to get to what I'm talking about,

1719
01:30:16,480 --> 01:30:18,480
anything like a real brain scale

1720
01:30:18,480 --> 01:30:21,480
without major innovations on the hardware side.

1721
01:30:21,480 --> 01:30:24,480
And you know, it's interesting that what drove those innovations

1722
01:30:24,480 --> 01:30:27,480
that support current AI was mostly not AI.

1723
01:30:27,480 --> 01:30:29,480
It was the video game industry.

1724
01:30:29,480 --> 01:30:32,480
When I point to the video game engine in your head,

1725
01:30:32,480 --> 01:30:36,480
that's a similar thing that was driven by the video game industry on the software side.

1726
01:30:36,480 --> 01:30:40,480
I think we should all play as many video games as we can

1727
01:30:40,480 --> 01:30:42,480
and contribute to the growth of the video game industry.

1728
01:30:43,480 --> 01:30:46,480
Because, no, because I mean, you can see this in very,

1729
01:30:46,480 --> 01:30:48,480
there are companies out there, for example,

1730
01:30:48,480 --> 01:30:50,480
there's a company called Improbable,

1731
01:30:50,480 --> 01:30:52,480
which is a London company,

1732
01:30:52,480 --> 01:30:55,480
London-based startup, a pretty sizable startup at this point,

1733
01:30:55,480 --> 01:30:58,480
which is building something that they call Spatial OS,

1734
01:30:58,480 --> 01:31:00,480
which it's not a hardware idea,

1735
01:31:00,480 --> 01:31:02,480
but it's a kind of software idea

1736
01:31:02,480 --> 01:31:04,480
for very, very big distributed computing environments

1737
01:31:04,480 --> 01:31:06,480
to run much, much more complex,

1738
01:31:06,480 --> 01:31:08,480
realistic simulations of the world

1739
01:31:08,480 --> 01:31:11,480
for much more interesting, immersive, permanent video games.

1740
01:31:11,480 --> 01:31:13,480
I think that's one thing that might, hopefully,

1741
01:31:13,480 --> 01:31:16,480
that will lead to more fun new kinds of games,

1742
01:31:16,480 --> 01:31:19,480
but that's one example of where we might look to that industry

1743
01:31:19,480 --> 01:31:22,480
to drive some of the, you know, just computer systems,

1744
01:31:22,480 --> 01:31:24,480
really hardware and software systems

1745
01:31:24,480 --> 01:31:28,480
that will take our game to the next level.

1746
01:31:29,480 --> 01:31:33,480
Josh, understanding on the algorithmic level or cognitive level

1747
01:31:33,480 --> 01:31:36,480
is just to understanding the meaning of learning

1748
01:31:36,480 --> 01:31:38,480
would be how to predict,

1749
01:31:38,480 --> 01:31:40,480
but on the circuit level it's different.

1750
01:31:40,480 --> 01:31:42,480
On the circuit level?

1751
01:31:42,480 --> 01:31:44,480
Well, of course it's different, right?

1752
01:31:44,480 --> 01:31:47,480
But already, I think you made a mistake there, honestly.

1753
01:31:47,480 --> 01:31:50,480
Like, you said the cognitive level is learning how to predict,

1754
01:31:50,480 --> 01:31:52,480
but I'm not sure what you mean by that.

1755
01:31:52,480 --> 01:31:54,480
There's many things you could mean,

1756
01:31:54,480 --> 01:31:56,480
and what our cognitive science is about

1757
01:31:56,480 --> 01:31:58,480
is learning which of those versions,

1758
01:31:58,480 --> 01:32:00,480
like, I don't think it's learning how to predict.

1759
01:32:00,480 --> 01:32:03,480
I think it's learning what you need to know to plan actions

1760
01:32:03,480 --> 01:32:05,480
and to, you know, all those things.

1761
01:32:05,480 --> 01:32:07,480
Like, it's not just about predicting.

1762
01:32:07,480 --> 01:32:09,480
Because there are things we can imagine

1763
01:32:09,480 --> 01:32:11,480
that make the world different.

1764
01:32:11,480 --> 01:32:13,480
So generalization, sorry, you're not predicting.

1765
01:32:13,480 --> 01:32:15,480
When your model could generalize.

1766
01:32:15,480 --> 01:32:17,480
But especially in the transfer learning that you are interested in.

1767
01:32:17,480 --> 01:32:19,480
A few hundred of neurons in prefrontal cortex

1768
01:32:19,480 --> 01:32:21,480
they could generalize a lot.

1769
01:32:21,480 --> 01:32:25,480
But not kind of a Bayesian model could do that.

1770
01:32:25,480 --> 01:32:28,480
You said, but a Bayesian model won't do that?

1771
01:32:28,480 --> 01:32:30,480
Or they don't do it the way a Bayesian model does?

1772
01:32:30,480 --> 01:32:33,480
For sure, because that's in the abstract level.

1773
01:32:33,480 --> 01:32:35,480
Well, I mean, how do you really know?

1774
01:32:35,480 --> 01:32:38,480
Like, and what does it mean to say that some neurons do it?

1775
01:32:38,480 --> 01:32:40,480
So maybe another way to put this is to say,

1776
01:32:40,480 --> 01:32:42,480
look, we have a certain math that we use

1777
01:32:42,480 --> 01:32:44,480
to capture these, you could call it abstract

1778
01:32:44,480 --> 01:32:46,480
or I call it software level abstractions.

1779
01:32:46,480 --> 01:32:49,480
I mean, all engineering is based on some kind of abstraction.

1780
01:32:49,480 --> 01:32:51,480
But you might have a circuit level abstraction,

1781
01:32:51,480 --> 01:32:53,480
a certain kind of hardware level

1782
01:32:53,480 --> 01:32:55,480
that you're interested in describing the brain at.

1783
01:32:55,480 --> 01:32:57,480
And I'm mostly working out or starting from

1784
01:32:57,480 --> 01:32:59,480
a more software level of abstraction.

1785
01:32:59,480 --> 01:33:01,480
They're all abstractions. We're not talking about molecules here.

1786
01:33:01,480 --> 01:33:03,480
We're talking about some abstract notion

1787
01:33:03,480 --> 01:33:06,480
of maybe a circuit or of a program.

1788
01:33:06,480 --> 01:33:08,480
Now, it's a really interesting question.

1789
01:33:08,480 --> 01:33:10,480
If I look at some circuits, how do I know

1790
01:33:10,480 --> 01:33:12,480
what program they're implementing?

1791
01:33:12,480 --> 01:33:14,480
If I look at the circuits in this machine,

1792
01:33:14,480 --> 01:33:16,480
could I tell what program they're implementing?

1793
01:33:16,480 --> 01:33:18,480
Well, maybe, but certainly it would be a lot easier

1794
01:33:18,480 --> 01:33:20,480
if I knew something about what programs they might be implementing

1795
01:33:20,480 --> 01:33:22,480
before I start to look at the circuitry.

1796
01:33:22,480 --> 01:33:24,480
If I just looked at the circuitry without

1797
01:33:24,480 --> 01:33:26,480
knowing what a program was or what programs

1798
01:33:26,480 --> 01:33:28,480
the thing might be doing or what kind of

1799
01:33:28,480 --> 01:33:30,480
programming components would be

1800
01:33:30,480 --> 01:33:32,480
mappable to circuits in different ways,

1801
01:33:32,480 --> 01:33:34,480
I don't even know how I'd begin to answer that question.

1802
01:33:34,480 --> 01:33:36,480
So I think we've made some progress

1803
01:33:36,480 --> 01:33:38,480
at understanding what neurons

1804
01:33:38,480 --> 01:33:40,480
are doing in certain low-level parts

1805
01:33:40,480 --> 01:33:42,480
of sensory system and certain parts

1806
01:33:42,480 --> 01:33:44,480
of the motor system, like primary motor cortex,

1807
01:33:44,480 --> 01:33:46,480
like basically the parts of the neurons

1808
01:33:46,480 --> 01:33:48,480
that are closest to the inputs and outputs of the brain,

1809
01:33:48,480 --> 01:33:50,480
where we don't...

1810
01:33:50,480 --> 01:33:52,480
Well, you could say we don't need

1811
01:33:52,480 --> 01:33:54,480
the kind of software abstractions

1812
01:33:54,480 --> 01:33:56,480
that I'm talking about or where

1813
01:33:56,480 --> 01:33:58,480
we sort of agree on what those things already are

1814
01:33:58,480 --> 01:34:00,480
so we can make enough progress

1815
01:34:00,480 --> 01:34:02,480
on knowing what to look for and how to know

1816
01:34:02,480 --> 01:34:04,480
when we found it.

1817
01:34:04,480 --> 01:34:06,480
But if you want to talk about flexible planning,

1818
01:34:06,480 --> 01:34:08,480
things that are more like cognition that

1819
01:34:08,480 --> 01:34:10,480
go on in prefrontal cortex,

1820
01:34:10,480 --> 01:34:12,480
at this point I don't think

1821
01:34:12,480 --> 01:34:14,480
that just by recording from those neurons

1822
01:34:14,480 --> 01:34:16,480
we're going to be able to answer those questions

1823
01:34:16,480 --> 01:34:18,480
in a meaningful engineering way.

1824
01:34:18,480 --> 01:34:20,480
A way that any engineer, software, hardware, whatever,

1825
01:34:20,480 --> 01:34:22,480
could really say, yeah, okay, I get it.

1826
01:34:22,480 --> 01:34:24,480
I get those insights in a way that I can engineer with.

1827
01:34:24,480 --> 01:34:26,480
And that's what my goal is.

1828
01:34:26,480 --> 01:34:28,480
So that's my goal to do at the software level,

1829
01:34:28,480 --> 01:34:30,480
the hardware level, or the entire systems level

1830
01:34:30,480 --> 01:34:32,480
connecting them.

1831
01:34:32,480 --> 01:34:34,480
And I think that we can do that

1832
01:34:34,480 --> 01:34:36,480
by taking what we're doing and bringing into contact

1833
01:34:36,480 --> 01:34:38,480
with people studying neural circuits.

1834
01:34:38,480 --> 01:34:40,480
But I don't think you can leave this level out

1835
01:34:40,480 --> 01:34:42,480
and just go straight to the neural circuits.

1836
01:34:42,480 --> 01:34:44,480
And I think the more progress we make,

1837
01:34:44,480 --> 01:34:46,480
the more we can help people who are

1838
01:34:46,480 --> 01:34:48,480
studying at the neural circuit level.

1839
01:34:48,480 --> 01:34:50,480
And they can help us address these other engineering questions

1840
01:34:50,480 --> 01:34:52,480
that we don't really have access to,

1841
01:34:52,480 --> 01:34:54,480
like the power issue or the speed issue.

1842
01:34:54,480 --> 01:34:56,480
Okay, thanks.

1843
01:34:56,480 --> 01:34:58,480
That was great.

1844
01:34:58,480 --> 01:35:00,480
Thank you.

1845
01:35:00,480 --> 01:35:02,480
Thank you.

1846
01:35:02,480 --> 01:35:04,480
Thank you.

