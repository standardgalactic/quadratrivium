1
00:00:00,000 --> 00:00:08,000
Welcome and good evening everyone. My name is Atman, one of the volunteers of the BiData Jiddah chapter.

2
00:00:08,000 --> 00:00:16,000
I'm here with my team members Yasmin and Hassan Shadad and Abdelillah.

3
00:00:16,000 --> 00:00:22,000
Today's meetup is titled Transformers from the Ground Up by Dr. Sebastian Rashka.

4
00:00:22,000 --> 00:00:28,000
Dr. Sebastian is an assistant professor of statistics at the University of Wisconsin Medicine,

5
00:00:28,000 --> 00:00:32,000
focusing on deep learning and machine learning research.

6
00:00:32,000 --> 00:00:39,000
He's also a contributor to open source software and the author of the best-selling book, Python Machine Learning.

7
00:00:39,000 --> 00:00:47,000
Thank you Dr. Sebastian for joining us and also we want to thank our sponsors, Namfocus and Muzun.

8
00:00:47,000 --> 00:00:57,000
If you want to visit our BiData Jiddah website or follow the Twitter account to be updated about our new upcoming meetups.

9
00:00:57,000 --> 00:01:03,000
And we also post a lot of cool things on there.

10
00:01:03,000 --> 00:01:09,000
And I guess that's all from me. I'll stop sharing so Dr. Sebastian can start. Thank you.

11
00:01:09,000 --> 00:01:15,000
Yes, thank you so much for the kind introduction. I really appreciate it. It's my pleasure to be here. Thanks for the invitation.

12
00:01:15,000 --> 00:01:21,000
So I will start then by sharing my screen and if there's some issue, please let me know.

13
00:01:21,000 --> 00:01:28,000
I can then try to fix that so it should be visible now. Can you see everything?

14
00:01:28,000 --> 00:01:30,000
Yes, it's clear. Yeah.

15
00:01:30,000 --> 00:01:41,000
Oh, perfect. Great. Okay. Yeah, then yeah, I will get started. So today, the talk is about transformers, the natural language processing models in deep learning.

16
00:01:41,000 --> 00:01:47,000
And I'm trying to cover transformers from the ground up, like explaining a little bit how they work.

17
00:01:47,000 --> 00:01:53,000
And then also, I hope we have time for that at the end, showing you an implementation in PyTorch.

18
00:01:53,000 --> 00:02:02,000
So yeah, why this topic? So yeah, I personally got very interested in this topic recently and I was also working on the new edition for my Python machine learning book.

19
00:02:02,000 --> 00:02:10,000
It was just putting together the chapter and had this very long lecture at my end of the semester last, last semester, when I was teaching the deep learning class.

20
00:02:10,000 --> 00:02:21,000
And this is essentially more like a condensed version of that highlighting the main concepts by skipping over the mathematical details, which I think would take too much time to digest.

21
00:02:21,000 --> 00:02:34,000
It's not like a week long effort to dig through all the details, but I hope this will give you like a good idea of how transformers work, and also helping you like navigating the jungle of all the terminology a little bit because there are so many different models

22
00:02:34,000 --> 00:02:43,000
out there like BERT, GPT version one, GPT version two, and yeah, you might wonder how they are different and what you can do with transformers.

23
00:02:43,000 --> 00:02:56,000
Yeah, just to start a few examples where transformers are used nowadays. So one popular example would be language translation. That is also how the original transformer architecture was developed.

24
00:02:56,000 --> 00:03:01,000
So language translation was the main motivation behind developing the original architecture.

25
00:03:01,000 --> 00:03:11,000
And yeah, nowadays, transformers are pretty much everywhere when it comes to natural language processing. And even beyond that, there are many real-world applications of transformers.

26
00:03:11,000 --> 00:03:28,000
For example, recently I also saw this article. So I'm also working in computational biology problems. Recently there was this article on using transformers to kind of elucidate or decipher properties and proteins like organizing proteins by

27
00:03:28,000 --> 00:03:42,000
structure properties, but also by function. And they trained, for instance, transformers on a large scale database of amino acid sequences like protein sequences, and they found all kinds of interesting insights from that.

28
00:03:42,000 --> 00:03:55,000
And yeah, you probably have seen recently the GitHub co-pilot project. I think this was in collaboration with OpenAI and I think also Microsoft, where they trained a transformer to generate code.

29
00:03:55,000 --> 00:04:07,000
What's interesting is that you can essentially enter a query, a comment, a code comment, and this AI system, the model will generate a corresponding code here at the bottom.

30
00:04:07,000 --> 00:04:19,000
So if you go to this website, they have like a nice animation of how that works, and they have a Microsoft Visual Studio Code plug-in where you can download this and actually use that in action, which is really impressive.

31
00:04:19,000 --> 00:04:35,000
So I think right now they have JavaScript support and Python support, so that would be another interesting application of transformers. So transformers mainly are about generating things like generating translations or generating code, generating text,

32
00:04:35,000 --> 00:04:48,000
for instance, also in chatbots, but you can also use them for classification, like classifying, let's say, whether a movie review is positive or negative, and this will be a code example we will be covering at the end of this lecture or talk.

33
00:04:49,000 --> 00:05:07,000
So the topics I have in mind for today are those. So first I wanted to, from the historic perspective, just briefly explain how attention came to be. So there was essentially this attention mechanism that was first added to recurrent neural networks, and then there was this

34
00:05:07,000 --> 00:05:18,000
self-attention mechanism, which is a, I would say, a more sophisticated version of that, there's this so-called scared dog product. Attention, oops, I almost ripped off my microphone here.

35
00:05:18,000 --> 00:05:36,000
Yeah, so and then we will talk about the original transformer architecture, which is the first architecture that was just using attention. So in step one here, this is a recurrent neural network or commented with attention, and the transformer is then an architecture is just using attention

36
00:05:36,000 --> 00:05:47,000
without any recurrent neural network layers. And then I will briefly dive into some of these large scale language models, for example, your GPT and bird, which are pretty popular these days.

37
00:05:47,000 --> 00:06:00,000
And then after that we will pre-train or sorry, we will fine tune a pre-trained bird model in PyTorch. So one thing about these large scale language models is that they are really large scale and will take forever to train them.

38
00:06:00,000 --> 00:06:15,000
In practice, usually they are trained on large data sets, thousands or millions of websites and books and so forth. And so usually people download a pre-trained version, and then fine tune it to a target task.

39
00:06:15,000 --> 00:06:29,000
So we'll also take a look at how it looks like. And then I have a few closing thoughts or words on transformers. And yeah, with that, I think we have plenty of things to talk about in the next, I would say, 40 minutes.

40
00:06:29,000 --> 00:06:45,000
Let's get started with the attention mechanism that was also first small disclaimer here. So like I mentioned in the beginning. So this will be more like a conceptual overview. So if there are some, let's say, notational details in these figures, don't worry about them too much.

41
00:06:45,000 --> 00:06:56,000
To be honest, I think it's most important to kind of understand the big picture and all the mathematical details. That is something that are almost like implementation details. Some of them are almost arbitrary.

42
00:06:56,000 --> 00:07:10,000
So I wouldn't worry about it too much. But I hope this talk kind of piques your interest in terms of transformers. So if you're interested in that you can dive in deeper into the topic afterwards with having this big picture overview in mind.

43
00:07:11,000 --> 00:07:22,000
So starting with the first topic augmenting the recurrent networks with attention. So first, what is the motivation behind using attention why don't we just use recurrent neural networks.

44
00:07:22,000 --> 00:07:32,000
So, if you think of a task sequence to sequence task, which is a task where you have an input sequence and you want to generate an output sequence.

45
00:07:32,000 --> 00:07:45,000
So one example of that would be a language translation, where you have an input sequence, let's say this one here in blue, in one language, let's say, German, and you want to translate this into English.

46
00:07:45,000 --> 00:07:59,000
So here on the output, then you have an output sequence. And what happens is if you use a, or if you design an arcade architecture for that, you usually use this encoder decoder setup in RNNs.

47
00:07:59,000 --> 00:08:14,000
So let's say that the RNN first ingests the whole input sequence. So you have input element one, two and three so these are for instance, individual works, and they are then processed into these hidden states.

48
00:08:14,000 --> 00:08:31,000
So once you have processed the whole input in this encoder so this would be essentially the encoder part, you have all the information in one, one hidden state here, and then you have the decoder part, which yeah generates the output.

49
00:08:31,000 --> 00:08:44,000
So here, why don't you, yeah, why don't you translate this one by one because here one problem would be that this hidden state has to remember all the input in one hidden state and this might be a little bit overwhelming.

50
00:08:44,000 --> 00:09:00,000
Right, so if you have one hidden state that has to capture the whole input, there might be some information loss. And this is essentially one of the challenges of these RNNs they work kind of well if you have short sentences but the longer your sentence becomes the more challenging

51
00:09:00,000 --> 00:09:10,000
the network to generate a good output because it kind of forgets what happened let's say here in the very beginning if you have a very, very long input sequence.

52
00:09:10,000 --> 00:09:17,000
So and also, yeah, why don't we just translate sentence word by word so here's just one example.

53
00:09:17,000 --> 00:09:32,000
Yeah, because I only speak two languages, German and a little bit of English, I have a German to English translation example here. So it's just an attempt on translating this German sentence here at the top into an English sentence.

54
00:09:32,000 --> 00:09:47,000
So what happens if you translate this word by word, so it doesn't really work. So you have a sentence. I'm an English as a second language speaker so I'm usually also not that good at that but I can tell that this one is definitely wrong.

55
00:09:47,000 --> 00:09:58,000
So you can see here, the sentences, can you help me, or can you me help this sentence to translate it doesn't really make sense if you just let's say take a dictionary and translate word by word.

56
00:09:58,000 --> 00:10:16,000
So to have a correct, I hope this is correct if you have a correct English translation, the sentence would be can you help me to translate the sentence, and you can see, there is some, yeah, some longer range dependencies there are some words here for instance those that you can just

57
00:10:16,000 --> 00:10:25,000
translate word by word, but then you have, let's say these two here, where they are kind of flipped so in order to translate this word, the help.

58
00:10:25,000 --> 00:10:32,000
You have to look into the future basically so you have to find an element that is further away in the input sequence.

59
00:10:32,000 --> 00:10:42,000
For example, if you have this word here, it goes here, you can see there's a more longer range dependency and the longer your sentences are, the longer these dependencies become.

60
00:10:42,000 --> 00:10:53,000
So in that way, we can't really have this word by word translation attempt because it would not result in any meaningful grammatically wrong translation.

61
00:10:53,000 --> 00:11:07,000
So going back, this is why we first read the whole input, because yeah we need to have knowledge about the whole input before we can attempt the translation but then again the challenge is okay, we may forget earlier hidden states.

62
00:11:07,000 --> 00:11:19,000
So one idea to deal with this problem that we can capture these long range dependencies is this attention mechanism so this was first proposed in the context of RNNs in 2014.

63
00:11:19,000 --> 00:11:28,000
So I also, I will share my slides so you if you are interested you can look up these papers and read a little bit more details about that.

64
00:11:28,000 --> 00:11:39,000
So this is an attention mechanism that the researchers used to augment the RNN to better process the input. So here's just like a sketch in the lower left corner.

65
00:11:39,000 --> 00:11:49,000
For instance, if we are thinking of the second step here, the hidden state might have access or at this step in general you might have access to the whole sequence.

66
00:11:49,000 --> 00:11:57,000
You can also align with to denote that one input sequence might be more important than another and this is where the attention comes in.

67
00:11:57,000 --> 00:12:11,000
So one step will have access to the whole input, but with a waiting term that says okay maybe at this position this term is more important than let's say this term and so forth so not every word is equally important.

68
00:12:11,000 --> 00:12:18,000
So there's also some some sort of waiting and the architecture looks approximately like that.

69
00:12:18,000 --> 00:12:33,000
So there's a sketch I made after the paper. So it's just a little bit different notation but essentially what we have is we have an input sequence. So this is just individual words, one to two words, and then they have two RNNs kind of connected here.

70
00:12:33,000 --> 00:12:43,000
So there's one RNN. It's a bi-directional RNN. Bi-directional RNN is just a fancy word for an RNN that processes the sequence from left to right and from right to left.

71
00:12:43,000 --> 00:12:51,000
It's just so that it captures, you have information from both ends. It's just increasing the amount of information you have essentially.

72
00:12:51,000 --> 00:13:07,000
So the forward direction would be the regular RNN direction and then you essentially just flip the sequence and process it backwards. So this is what we denoted here so you can denote it from right to left but you can also think of it as just reversing the sentence and processing it in the regular way.

73
00:13:07,000 --> 00:13:20,000
Then you connect these hidden states and then you calculate the so-called attention weights. I will skip over the details of how these attention weights are computed here but I will show you how they are computed in the transformer.

74
00:13:20,000 --> 00:13:33,000
It's a little bit different but not too different. So but the main part here is really that using these attention weights, we obtain the so-called context vector at each step.

75
00:13:33,000 --> 00:13:48,000
So the context vector here is essentially a weighted version of all the inputs. So you can consider all the hidden states here and it's essentially a weighted sum. You have this attention weight, you multiply it by this state and then you just sum them up.

76
00:13:48,000 --> 00:13:54,000
And the larger the attention weight, the more the hidden state will contribute to this context vector.

77
00:13:54,000 --> 00:14:05,000
So this context vector is essentially then kind of capturing all the input information in a certain processed way and this goes into each decoder part.

78
00:14:05,000 --> 00:14:14,000
So if I go back a few slides, we had this decoder part here where we were generating some output and this is what we are having here at the top of the O's.

79
00:14:14,000 --> 00:14:29,000
The O's, they should just denote, let's say, the translated sentence. And each output step, what we have is we have the hidden state that goes into, let's say, the that position, the previous word, because it's also important.

80
00:14:29,000 --> 00:14:40,000
I mean, if you generate a sentence, you kind of look at the previous word you have just written, right? So what you currently translate the current word is also depending on the previous word you have written.

81
00:14:40,000 --> 00:14:53,000
The sentence might not make sense. So you provide the previous word, you provide the hidden state, and you provide this context vector and with that you can capture all the input when you attempt the translation of the next word.

82
00:14:53,000 --> 00:15:04,000
So this is essentially an enhanced version of recurrent neural network for translation, where this party is essentially new, it's this attention part.

83
00:15:04,000 --> 00:15:16,000
Yeah, and from this attention part, people then develop this self attention mechanism. So the self attention mechanism is one major building book of this original transformer architecture that we will cover.

84
00:15:16,000 --> 00:15:21,000
And it's kind of inspired by this RNN that I just showed you.

85
00:15:21,000 --> 00:15:33,000
This is a very simple version of self attention. It's not the version that is actually used in the transformer that comes in the future slide. This is a simplified version just to understand the main concept.

86
00:15:33,000 --> 00:15:40,000
So what you can see here is, so to cover it step by step, there are three steps.

87
00:15:40,000 --> 00:15:59,000
So this one is computing these dot products. So imagine, we are currently processing a given word, Xi, and we compute the similarity of this word. This is how we get a certain score for computing the attention weights, we compute the similarity of this to all the other words, including

88
00:16:00,000 --> 00:16:10,000
and why are so what we have here is a sequence of words one to T and why are the four blocks there so the four blocks, they just know that this is a vector.

89
00:16:10,000 --> 00:16:23,000
So it's for example a word embedding so usually when we do natural language processing, we don't really feed it in the string of the word so we usually have a method that converts the string let's say to a vector of real numbers.

90
00:16:23,000 --> 00:16:33,000
So this should just denote a word embedding here. And then there's this, you know, this dog product between two word embeddings to vectors that we multiply.

91
00:16:33,000 --> 00:16:44,000
And let's say this gives us the attention weight. So it's just a simplified version. Usually we would some use something like a softmax function which is a function that would normalize them so that they sum up to one.

92
00:16:44,000 --> 00:17:00,000
So that we then again compute the weighted sum of these. So we have these attention weights, and then we have our vectors. So vector one to T, and we just wait them by the corresponding attention weights.

93
00:17:00,000 --> 00:17:11,000
Also, we have on a special a specific attention weight for each query. So if, let's say, Xi here's my query.

94
00:17:11,000 --> 00:17:24,000
These attention weights will be different, compared to if x two is my query, because it depends on the on the dot products right. So these would change each word has different embedding so every

95
00:17:24,000 --> 00:17:32,000
query has its unique attention weights and from that we derive this context vector which is kind of similar what I showed you in the RNN example.

96
00:17:32,000 --> 00:17:42,000
So this is a simple form of self attention, where we just introduce some, you know, waiting of all the inputs when we compute some output for current word.

97
00:17:42,000 --> 00:17:55,000
So one problem with that though is, so the main takeaway here is that we compute these attention weights but one problem is that we don't have any mechanism for training a neural network now right because there is no, no weight parameter involved.

98
00:17:55,000 --> 00:18:03,000
So how do we improve this how do we use that say back propagation or gradient descent with that to learn the optimal attention weights for example.

99
00:18:03,000 --> 00:18:17,000
So for that, we in practice actually use a different type of attention it's called on scaled dot product attention it's one form of self attention with learnable weights so this involves weight parameters.

100
00:18:17,000 --> 00:18:30,000
And yeah there are then three matrices, and with these matrices, you compute also again like a vector a query a key and a value using the input query.

101
00:18:30,000 --> 00:18:41,000
So even now these matrices are weight parameters that you can update during back propagation or gradient descent to optimize the model.

102
00:18:41,000 --> 00:18:49,000
So here, what's going on here is we have a current input query that we are currently processing let's say the second word here again.

103
00:18:49,000 --> 00:18:57,000
And from the second word we apply these three matrices to compute three values or three vectors so a query a key and a value.

104
00:18:58,000 --> 00:19:13,000
Using those so so what we do then is we use the query first so this this query to compute these omega values these are kind of like unscaled attention rates with each key.

105
00:19:13,000 --> 00:19:17,000
So the key is computed for each input word again.

106
00:19:17,000 --> 00:19:35,000
And we compute the dot product between query and key, but notice here we always use the query to which is our current word that we are processing so we again obtain these unique values or unique attention weights for each query that we are currently processing.

107
00:19:35,000 --> 00:19:43,000
These are some normalization steps so this is on scale dot product attention. It's a softmax with a scaling factor essentially.

108
00:19:43,000 --> 00:19:58,000
And then again, like before we are summing them up to this context vector zine. So this is essentially the same that I showed you here except that it now involves here these trainable matrices and this is, yeah this is the basic on building block behind these transformer

109
00:19:58,000 --> 00:20:13,000
networks. So, um, yeah it's maybe a little bit fast but to be honest is really not that much behind the self attention it looks maybe a little bit complicated so maybe you need to look at this a little bit longer it took me definitely some time on to understand

110
00:20:13,000 --> 00:20:23,000
what's going on there, to be honest, a good way for me learning about that was really like reading the paper and then doing these or making these figures and drawing that to kind of make sense of it.

111
00:20:23,000 --> 00:20:34,000
So yeah once once you take a look at this it's actually not as complicated as it looks like looks like. I think the complicated part here is more like that these language models are so large.

112
00:20:34,000 --> 00:20:44,000
And if you look at the code itself it's kind of like overwhelming because there's so many things going on there in the code not not in the building block here.

113
00:20:44,000 --> 00:20:51,000
So now to get to the original transformer architecture which is built on this self attention mechanism.

114
00:20:51,000 --> 00:21:02,000
So, this is based on this paper attention is all you need that started all the trends in terms of developing transformers it's a very influential paper that came out in 2017.

115
00:21:02,000 --> 00:21:09,000
It's not too long ago four years ago and since then they have been hundreds if not thousands of different transformer architectures.

116
00:21:10,000 --> 00:21:25,000
The main part is essentially that this is centered around this self attention mechanism, and it does not use any RNN parts so there's no recurrent neural network layer nothing it's just using the self attention concepts and stacking them.

117
00:21:25,000 --> 00:21:32,000
And then also having some fully connected layers like in a multi layer perception just a very basic neural network layers.

118
00:21:32,000 --> 00:21:49,000
So this is a sketch of how the transformer architecture looks like. So yeah, like I mentioned, it looks very complicated at first glance there are a lot of boxes and lots of things connected to each other, but essentially it's not too bad it's relatively straightforward it's

119
00:21:49,000 --> 00:22:06,000
actually, again, our encoder and decoder like in the RNN that I showed you earlier where the job of the encoder is to, yeah, ingest this on input sequence, and the decoder job is to generate the output sequence.

120
00:22:06,000 --> 00:22:16,000
Yeah, what you do first is you have a word embedding for example to embed the words into real valued vectors. There's something like a positional encoding.

121
00:22:16,000 --> 00:22:26,000
Because the problem is, there's its permutation invariant here but we will skip over these details. It's just like a way of encoding the position of the word.

122
00:22:27,000 --> 00:22:38,000
Then we have this so called multi head attention. It's kind of like, I would say an extension of the self attention mechanism I will have a slide on that to briefly explain what that is.

123
00:22:38,000 --> 00:22:42,000
Then there is a layer normalization.

124
00:22:42,000 --> 00:22:52,000
Yeah, like a multi layer perceptron a fully connected network. And then again a layer normalization and so forth. So it's just repeating some of these units and in between.

125
00:22:52,000 --> 00:22:59,000
We also have something like these arrows here connecting things. And these are skip connections.

126
00:22:59,000 --> 00:23:09,000
It's essentially just this is what the ad stands for it's taking this value and adds it back here it's similar to what a residual network does if you afford a residual network.

127
00:23:09,000 --> 00:23:27,000
So the means of preventing, let's say a bad gradients. So if you have something like a vanishing gradient problem here, then this one will be added to this one it's it's kind of like skipping layers if the layers are bad, then there's an opportunity to skip them

128
00:23:27,000 --> 00:23:29,000
via these connections so to prevent.

129
00:23:30,000 --> 00:23:41,000
Bad, bad layers for from ruining the whole network. And then yeah this encoder just captures essentially the input in this self attention weighted way.

130
00:23:41,000 --> 00:23:51,000
It's like an encoding of the input. And this feeds into the decoder which has itself also multi head attention there's a masked version which I will also talk a lot of bit about.

131
00:23:51,000 --> 00:24:05,000
Yeah, and this is essentially it so it goes again through a fully connected network and then there's a fully connected layer to produce the outputs so the fully connected layer is then producing word probabilities with a softmax function like how likely is this word at a given

132
00:24:05,000 --> 00:24:17,000
position and then in practice we usually use the highest probability if we, let's say do a language translation or if we generate text in general, we want to have some randomness, we want our model to generate different texts.

133
00:24:17,000 --> 00:24:26,000
So just on sample from this distribution. But yeah and in the original transformer architecture what they had in mind was language translation.

134
00:24:26,000 --> 00:24:42,000
Yeah, to dive a little bit more into this multi head attention, it sounds like it's a whole new concept, but it's essentially just our self attention mechanism so here this is the scaled dot product attention that we talked about earlier with these different matrices.

135
00:24:42,000 --> 00:24:53,000
And multi head attention is just repeating this multiple times it's basically a stack of these things. It's kind of like similar when you.

136
00:24:53,000 --> 00:25:07,000
If you have worked with convolution networks and you have an input, and you apply multiple kernels to produce multiple feature maps as the output. It's kind of like the same concept that each channel on captures different type of information here.

137
00:25:07,000 --> 00:25:15,000
Anyway, this is a similar concept where you just repeat the self attention multiple times and stack it up to each other.

138
00:25:15,000 --> 00:25:25,000
And this helps you're extracting different features that are useful. And yeah one thing about that is on what's also nice is that here.

139
00:25:25,000 --> 00:25:40,000
So stacking or what the advantages is we can parallelize that because there's no dependency between these individual scale product attention so you can compute this in parallel helps us also to leverage multiple GPS for example.

140
00:25:40,000 --> 00:25:56,000
So we have now these so called hats attention hats, and each attention had has essentially one set of these matrices so if we have each on attention hats we have each sets of these matrices, and then we just repeat the same thing multiple times.

141
00:25:56,000 --> 00:26:11,000
And also I should say here. This is six times. So these blocks are also repeated six times. I think this is what makes the architecture so big and complicated when you look at the code if you look at the, let's say from scratch implementation is just there are so many layers and there's

142
00:26:11,000 --> 00:26:27,000
not so much on code that implements all of that, but the underlying concepts, if you draw them out there not itself themselves that are not that complicated it's just your matrix multiplication, and then softmax and matrix modification.

143
00:26:27,000 --> 00:26:44,000
Let's talk a bit about this mask maybe here. So the mask multi hat attention is masking out words that we haven't generated yet. So we have a target sequence that is our, let's say desired output that we want to generate let's say if we want to translate

144
00:26:44,000 --> 00:26:54,000
it, but at a given time step we don't want to have something that is in the future. So if we generate a sentence let's say on right.

145
00:26:54,000 --> 00:26:57,000
Let's say have a sentence help.

146
00:26:57,000 --> 00:27:01,000
We translate this.

147
00:27:01,000 --> 00:27:14,000
First, we would have then let's say a German version of that but let's say this is our targets version. So in the first step, we would only have access to help, and then this one would be masked out.

148
00:27:14,000 --> 00:27:30,000
And then as a second step, we would have access to these two words and these would be masked out and the mask multi hat attention is essentially ensuring that future words that are not generated yet are masked out.

149
00:27:30,000 --> 00:27:34,000
So actually a slide about that. Yeah and this masking out.

150
00:27:34,000 --> 00:27:49,000
This can also be considered as a unidirectional form of language modeling, because we are masking out everything that is in the future from the current position. So we are masking out from left to right so it's kind of unidirectional.

151
00:27:49,000 --> 00:28:07,000
And we will also talk about a model that has so called bidirectional approach so the Burke model has a bidirectional approach and we will revisit this topic it's kind of in it sounds actually fancier than it is so we will see in the future what this bidirectional

152
00:28:07,000 --> 00:28:09,000
means also.

153
00:28:09,000 --> 00:28:18,000
Okay, so yeah, this was already on all the transformer general original transformer stuff I wanted to talk about.

154
00:28:18,000 --> 00:28:32,000
And these were all the original ideas like from a historic perspective so nowadays, people don't use this original transformer architecture anymore but there are many, many other architectures that have been inspired by this architecture.

155
00:28:32,000 --> 00:28:41,000
So here in the section I want to briefly highlight some of them. So one is this GPT model developed mainly by the open AI team.

156
00:28:41,000 --> 00:28:52,000
And this takes this unidirectional approach which is kind of similar to the decoder of the original transformer. And this is used for tasks that involve generating texts.

157
00:28:53,000 --> 00:29:12,000
And then there is the Burke model and of course there are yet other different types of versions of it for example, Roberta and Alberta and so forth. But in general the Burke model is bidirectional approach, and this is more suited or better suited for classification

158
00:29:12,000 --> 00:29:23,000
for example, in the code example that I prepared where we predict or whether the movie review is positive or negative. So whether someone liked the movie or not.

159
00:29:23,000 --> 00:29:34,000
Yeah, but one, one scheme that is common amongst most transformers is that there are two steps. One is a pre training step, a pre training step on very large unable data sets.

160
00:29:34,000 --> 00:29:48,000
And then there's a fine tuning step where we take a pre trained model, and then we train it on our target data set. So here in step one, this is usually a general data set that is not really related to our target task.

161
00:29:48,000 --> 00:30:03,000
It's for instance just a library of books or websites, but we don't have any labels and yeah this is just used to pre train the model they are very large models so they need a lot of data to be kind of pre trained.

162
00:30:03,000 --> 00:30:12,000
And for this training, there are these two steps that I mentioned the pre training and the fine tuning one and two.

163
00:30:12,000 --> 00:30:24,000
And the fine tuning can also be split into two different approaches, and both are very common. I think fine tuning is a little bit more common performance better but it's also more expensive.

164
00:30:24,000 --> 00:30:40,000
One is this feature based approach and one is this fine tuning based approach. So how do they differ. So assume you have step one where you pre train your transformer. So this is sometimes called in the original literature it's called unsupervised pre training, but the more

165
00:30:40,000 --> 00:30:48,000
modern term for that is self supervised learning. So the same thing essentially, where you train a model on unlabeled data.

166
00:30:48,000 --> 00:30:57,000
And then, so the data is unlabeled but it's still a supervised learning approach. This is why it's called super as a self supervised learning.

167
00:30:57,000 --> 00:31:12,000
So you generate essentially the labels on yourself. So you take let's say a book, and then you train the model to predict you remove some words and then you train the model to predict missing words, or you predict the next word in the sentence and the next word in the

168
00:31:12,000 --> 00:31:23,000
sentence would be your label, but it's not like a label for classification it's just like a label that you can generate yourself from the text you don't need to have a human labeling these texts.

169
00:31:23,000 --> 00:31:36,000
Yeah, so and then let's assume we have now our pre train transformer in this red block. So then, once you have a pre train on a large data set you never have to change it again in terms of

170
00:31:36,000 --> 00:31:47,000
you basically save that somehow on your computer, and then you can use it for all your downstream projects that is what I mean. And one way is to never really change it you take it.

171
00:31:47,000 --> 00:31:57,000
And then you just extract the last layer so you give it a new sentence, and then you extract the output layers, one or more output layers you can concatenate them.

172
00:31:57,000 --> 00:32:10,000
There's output layers to a classifier, for example, logistic regression multi layer perceptron maybe a random forest, whatever you desire, and you only train this classifier. So, you essentially treat this pre train transformer as a feature

173
00:32:10,000 --> 00:32:15,000
extractor. If you have used on something like principle component analysis.

174
00:32:15,000 --> 00:32:24,000
This is also one method that say it's a linear transformation of your input. It's one method to, let's say, extract some features from your original features so it's in that way.

175
00:32:24,000 --> 00:32:33,000
So pre train transformer acts like a feature extractor where you just use it to extract these last layers, and then you just train a classifier on top of that.

176
00:32:33,000 --> 00:32:44,000
Another approach is this fine tuning approach it's a little bit more expensive. This is, you know, for what I prepared this code example. So this fine tuning approach is a little bit different in that you update the whole model.

177
00:32:44,000 --> 00:32:56,000
And then you start with your pre train transformer. Let's say you saved it somewhere on your hard drive, you make a copy of that. And then you feed it the label training data set that you have.

178
00:32:56,000 --> 00:33:11,000
And you add a few layers for classification. Let's say if you have if we have our movie review data set we add one final note in the output layer, have a softmax activation, and then we just train the whole model with back propagation.

179
00:33:12,000 --> 00:33:25,000
And because we have to load all the models, the whole model in memory and do the back propagation it's a little bit more expensive than than this step where we only use the transformer for generating the features.

180
00:33:25,000 --> 00:33:36,000
So that way, it depends. It's of course also slower. So it depends on what type of architecture you have access to to use this approach but in practice it performs a little bit better.

181
00:33:36,000 --> 00:33:43,000
I forgot which paper it was either GPT one or bird. They also directly compared those approaches.

182
00:33:43,000 --> 00:33:58,000
I think my GPT one but I would have to double check. And they found I think if you lose use the last three layers here, you can get approximately the same performance as with fine tuning so in that way it's, it's really, you can get the same performance but it's really a trade off also

183
00:33:58,000 --> 00:34:03,000
of how much computational resources you have.

184
00:34:03,000 --> 00:34:19,000
Yeah, so talking a little bit about the GPT models. So there are three GPT models now. So one was released in 2018 one in 2019 one in 2020 and yeah, if the sequence continues we maybe have a new version by the end of this year.

185
00:34:19,000 --> 00:34:33,000
And yeah, one obvious thing you can see from these GPT models which stands for generative pretrained transformer is that they increased a lot in size so they started with 110 million parameters which was already really huge.

186
00:34:33,000 --> 00:34:42,000
Then the next one had 1.5. And now we have 107 175 billion. So who knows maybe the next one is in the trillion range.

187
00:34:42,000 --> 00:34:59,000
So just like I said, I will share my slides later so you can also read more about that in the original manuscripts, but yet just to give you just brief summary of how these models work, I have a few slides on those.

188
00:34:59,000 --> 00:35:10,000
So GPT one, that's the first model in the series. It's essentially very similar to the transformer we talked about, especially I mean, it's just taking the decoder part.

189
00:35:10,000 --> 00:35:28,000
And it's trained on a sentence in this next word prediction manner. So the pre training is taking a large context or a large corpus of books or websites I think, I think they use the big a mix of words from websites and books.

190
00:35:28,000 --> 00:35:43,000
There are instances where they remove through this masking, the future words in the model's task is to predict the next word, and they just train the model in this manner. So they train it just for this prediction of the next word in the sentence.

191
00:35:43,000 --> 00:35:52,000
So by doing that, you can then fine tune it so you add. So for having this task classifier, you can add additional layers.

192
00:35:52,000 --> 00:36:02,000
After you pre trained the model, and depending on what you're interested in this you can do our classification entailment similarity multiple choice questions and so forth.

193
00:36:02,000 --> 00:36:07,000
And how that works is essentially they have this pre trained transformer here.

194
00:36:07,000 --> 00:36:15,000
You connect the linear layers and then it's like a regular classifier. And yeah, one thing here is.

195
00:36:15,000 --> 00:36:26,000
I won't go into too much detail but it's how you format the input. If you have a classification, you have like a start token a text, the sentences and then some extraction token.

196
00:36:26,000 --> 00:36:45,000
If you have something like similarity here, text similarity, they have on both texts and then in flipped order for example so it just the first a little bit about how you set up the inputs, but yeah the main, the main thing is that you can just adopt this pre trained network to do whatever

197
00:36:45,000 --> 00:37:01,000
you like in the fine tuning. So this is really expensive. This is like a really expensive step that something you don't want to do yourself you want to maybe download this GPT and then you can on your target data set to the fine tuning.

198
00:37:01,000 --> 00:37:16,000
And then really what was really novel about GPT two and GPT three is that they actually removed this fine tuning most language transformers they use the fine tuning stuff that I talked about what GPT two and GPT three are doing though is they tried something new.

199
00:37:16,000 --> 00:37:26,000
They were so ambitious that they kind of removed this fine tuning, and they only have a model now where you provide what you want to do as input.

200
00:37:26,000 --> 00:37:40,000
So for example, if you want to use their model the GPT two model to generate text, what you do is you give it the input that is formatted as follows where you say to the model translate to French.

201
00:37:40,000 --> 00:37:49,000
And then you insert your English text, and then you insert your French text and it will automatically figure out what you want to do basically.

202
00:37:49,000 --> 00:38:06,000
There are different questions like you give it a sentence and you can ask something like this is object. So you can say for example, this is a very nice sky and is the sky blue and it will what can generate like output that says yes or no or something like that.

203
00:38:06,000 --> 00:38:16,000
So it's kind of very flexible in terms of what it can do by only having a certain formatted input without any fine tuning.

204
00:38:16,000 --> 00:38:18,000
So in version three.

205
00:38:18,000 --> 00:38:33,000
I think they kind of went back a little bit because I think. So here, this is called in this context on zero shot learning where they don't have any examples of the task the example is in the scriptures in the task itself they don't have any training examples in that sense.

206
00:38:33,000 --> 00:38:43,000
The task was maybe a little bit too ambitious. So they went back with GPT three made it a little bit bigger, and they switched also to few short learning in this context where in the future learning.

207
00:38:43,000 --> 00:39:01,000
They have at least a few examples of that task. So just to illustrate how that looks like so here is this zero shot where you have again the prompt or task description sorry, and then a prompt, and then it would insert here the French word for cheese.

208
00:39:01,000 --> 00:39:08,000
I honestly don't know any French so I can't tell you what it is, but yeah it would generate this output here.

209
00:39:08,000 --> 00:39:18,000
And for the one shot. It's a little bit more, I would say, easier for the model because it sees at least an example. It's not fine to it's just seeing the example as part of the input.

210
00:39:18,000 --> 00:39:25,000
So you have again the task description here at the top. And then you have one example of what you want to do.

211
00:39:25,000 --> 00:39:36,000
And then few short learning so if you have let's say here on the three shot case where you have one example second example the third example, it becomes easier for the model.

212
00:39:36,000 --> 00:39:39,000
If you showed at least a few examples.

213
00:39:39,000 --> 00:39:41,000
See that's just a note in the chat.

214
00:39:41,000 --> 00:39:50,000
I will maybe answer the questions after the talk and think almost over time with your slides left.

215
00:39:50,000 --> 00:39:54,000
Yeah, so here.

216
00:39:54,000 --> 00:40:08,000
Well, where was I so you're here on the limitation of the future tasks though is that you have a limited size input. I honestly don't remember what the sizes for GPT three I think they have some IP eyes that I've never used.

217
00:40:08,000 --> 00:40:18,000
But let's say for bird models there's a token input limitation of 512 tokens. It's like characters or work and characters but like words or punctuation.

218
00:40:18,000 --> 00:40:26,000
So that way, you're kind of limited by what the model can process as input and how many examples you provide.

219
00:40:26,000 --> 00:40:38,000
So this is actually a fine but I can imagine it's maybe a challenge if you have longer sentences here so that wouldn't probably work with very long sentences because then you fill up all the input with examples.

220
00:40:38,000 --> 00:40:51,000
So this is in contrast to the fine tuning approach with which most other transformers use where you have an example as part of your training set that then you updated with back propagation like the gradient update showed another example and so forth.

221
00:40:51,000 --> 00:41:05,000
So this is more like the traditional way of doing it. And yeah the GPT three models very ambitious and I think I'm not sure if actually the paper is out yet but yeah this is one of the latest state of the art models for generating texts.

222
00:41:05,000 --> 00:41:11,000
And another approach is the bird model, which is bidirectional encoder.

223
00:41:11,000 --> 00:41:26,000
So you can think of GPT more as the decoder which generates some output whereas bird is more like the encoder of the transformer which ingests like the input, and then creates a representation that you can then use to train a class to fire on.

224
00:41:26,000 --> 00:41:32,000
So GPT is better for generating texts bird is better for classification.

225
00:41:32,000 --> 00:41:41,000
So how that works is they have a pre training step or they have actually two different types of pre training tasks. One is on this masked language model.

226
00:41:41,000 --> 00:41:58,000
So what they do is they mask 15% of the words. I like to call it marked, not masked yet, because what they do is they take these 15 words or 15% of the words if you have a sentence, and then they do different things to it so here's an example.

227
00:41:58,000 --> 00:42:16,000
So if you have a sentence here and input sentence, you would randomly pick 15% of the words for example that you would pick, let's say Fox, and then based on these 15% what you do is 80% of the time, you replace this token, the fox with a mask.

228
00:42:16,000 --> 00:42:24,000
And 10% of the time, you replace it with a random word, for example, coffee, and 10% of the time you keep it unchanged.

229
00:42:24,000 --> 00:42:36,000
And so this is how you pre train the model you have all these sentences and 15% of the words are changed in a certain way and the model has to predict what the correct word is at a certain position.

230
00:42:36,000 --> 00:42:44,000
For example, it has to fill in the right word if there's a mask, or it has to detect whether this word is right or wrong.

231
00:42:44,000 --> 00:43:01,000
And then, sometimes it's unchanged so why would you have this unchanged. So this is kind of important for the model in order to perform well during inference when you train the model and you want to use it in the real world, the real world, you usually don't have masked sentences.

232
00:43:01,000 --> 00:43:18,000
But also then basically learns that sometimes it should not do anything so in order to work well or some real text which doesn't have masks, otherwise it would always expect okay there are some random words or masked words so in this way they found that this performs better if they have also 10% of the

233
00:43:18,000 --> 00:43:21,000
or marked words unchanged.

234
00:43:21,000 --> 00:43:38,000
And the second pre training tasks that they do at the same time is next sentence prediction so they have a sentence a and a sentence be separated by a token, and the model has to predict whether sentence be indeed follows sentence a.

235
00:43:38,000 --> 00:43:48,000
So if you have a text and you reverse the order of the sentences the model learns the correct order and this helps the model yet to work with inputs that are more than one sentence.

236
00:43:48,000 --> 00:43:55,000
So it's essentially classification a binary classification is next or is not next essentially.

237
00:43:55,000 --> 00:44:10,000
The classification token is used as a placeholder for generating the output, because the number of inputs also matches the number of outputs, but in bird we are not interested in generating new text, we are essentially interested in classification.

238
00:44:10,000 --> 00:44:16,000
So how that works is just a brief overview of the different types of tasks you can do with bird.

239
00:44:16,000 --> 00:44:20,000
There is a sentence pair classification.

240
00:44:20,000 --> 00:44:38,000
One is just a single sentence classification question answering and single sentence tagging so here tagging is, for example, if you think about something like language, a grammar software you can label a sentence by on what type of like a word or

241
00:44:39,000 --> 00:44:48,000
word or grammar each word corresponds to, but let's focus maybe only here on the, because we have for the interest of time on the single sentence classification task.

242
00:44:48,000 --> 00:45:04,000
So you provided an input sentence, you have some embedding here, and then you provide some output. So you have during training you have all these tokens so it has to predict what the input is, given that something is masked or not.

243
00:45:04,000 --> 00:45:10,000
If you have a prediction task you don't I mean it still generates a sentence but you don't do anything with it.

244
00:45:10,000 --> 00:45:17,000
What you care about is this class table here so this is why you have this classification token. This goes then into the class table that you care about.

245
00:45:17,000 --> 00:45:29,000
And this is something we can leverage for arbitrary classification. So I prepared a code example for fine tuning such a bird model for this single sentence classification in pytorch.

246
00:45:29,000 --> 00:45:34,000
And it says single sentence classification and for the code example.

247
00:45:34,000 --> 00:45:46,000
I have this large movie review data set. I mean, back then it was large but it's just a 50,000 movie reviews, and I think 50 25,000 in the training and 25,000 in test set.

248
00:45:46,000 --> 00:46:03,000
And this is essentially movies movie reviews from the IMDB movie review database, and these are multiple sentences. So we can also use bird for multiple sentences, even though let's say it says single sentence classification, but just concatenating these

249
00:46:03,000 --> 00:46:17,000
sentences. So in that way, the only difference is we don't use this separator token because the separator token is really more for comparing two things like this sentence pair classification so this one can for example say whether these sentences are similar or not like

250
00:46:17,000 --> 00:46:22,000
some sentence similarity and things like that.

251
00:46:22,000 --> 00:46:39,000
Yeah, so I'm not sure how much time we have I could maybe briefly went way over. I can maybe briefly show you the code example. It's actually not too complicated. I have it on GitHub. I have some annotation.

252
00:46:39,000 --> 00:46:52,000
It's based on a library called hugging face. It's a very, very, very popular library for or you probably can't see it now have to reshare my screen. Okay.

253
00:46:52,000 --> 00:46:59,000
Let me see how many slides I have left. So we probably don't have to cover this.

254
00:46:59,000 --> 00:47:06,000
Let me share my browser screen so you can briefly see the code example.

255
00:47:06,000 --> 00:47:10,000
Here I opened it from GitHub.

256
00:47:10,000 --> 00:47:18,000
I've also my Jupyter left version which is maybe better because I can zoom in here.

257
00:47:19,000 --> 00:47:37,000
So here is a notebook for downloading a pre trained bird model and fine tuning it on the movie review classification. There's a lot of boilerplate code here like importing the libraries, some settings for reproducible reproducibility, downloading the data set.

258
00:47:38,000 --> 00:47:58,000
And then processing it. So here's a pen us data frame of how the data set looks like there's like a text a movie review, and then it's whether it's positive or negative, like a binary classification, 50,000 combined, and here I'm just spitting them into training and validation and test sets.

259
00:47:58,000 --> 00:48:18,000
So here's a tokenization step. So tokenization goes from taking the words and then converting them also, sorry, taking the sentence and chopping it into words, and then also taking care of punctuation so punctuation is also, I think, given token, but it really depends on what type of tokenizer you use.

260
00:48:18,000 --> 00:48:30,000
So there's a collection of different tokenizers out there they all behave a little bit differently, but I recommend if you use, let's say, on hugging face. I recommend on finding a tokenizer that matches your model.

261
00:48:30,000 --> 00:48:36,000
I should say we are using here, not birth, but the silver I think I had a note about that at the top.

262
00:48:36,000 --> 00:48:53,000
This is essentially a smaller version of work because if you only have let's say one GPU like 1080 TI or 2080 TI like a graphics card with only 11 gigabyte memory, you or I couldn't at least log the whole work model into memory.

263
00:48:53,000 --> 00:48:57,000
I think I could just barely load it but they're not training.

264
00:48:57,000 --> 00:49:17,000
So there's a version called the silver which is a little bit smaller so in the silver what they did is they trained the bird model, and then they removed some weights, but preserving its performance it has 95% of the performance of work, but it's more lightweight it has 40% fewer parameters.

265
00:49:17,000 --> 00:49:29,000
So, we are using a tokenizer that is made for this, the still bird model, and then encode the data set into tokens.

266
00:49:29,000 --> 00:49:43,000
And then I have my data loader here in pytorch. So this is a custom data loader here there's nothing really special about it. If you would build the data loader for any type of image data set for example, except that we on the way we process essentially on encodings.

267
00:49:43,000 --> 00:49:55,000
So that's like a special on representation that is, I think it's kind of like a dictionary it's our own Python. So yeah a Python object or class, but it kind of behaves like a dictionary you can index into that.

268
00:49:55,000 --> 00:50:01,000
And I will show you how that looks like. So yeah here's an example you can have a.

269
00:50:01,000 --> 00:50:12,000
So this is this sorry this will create a dictionary right, but I think itself it's also some certain object that contains multiple things.

270
00:50:13,000 --> 00:50:26,000
Yeah, so here I'm just creating data loaders from the data set. So this is basically all just set up. And here's the main part where we are loading the still bird model, and they have multiple models on their website if you go to the hugging face

271
00:50:26,000 --> 00:50:37,000
transformer website. And here this would be for example a distilled bird model for sentence or sequence classification you can basically tell based on the class name, what this is made for.

272
00:50:37,000 --> 00:50:48,000
And then I'm loading it the pre trained version and they have also again, I think they have different ones uncased you would mean it would ignore sentence case whether it's upper or lower case.

273
00:50:48,000 --> 00:50:56,000
Yeah, and then I'm putting the model on my device, putting it into training mode and then initializing an optimizer for back propagation.

274
00:50:56,000 --> 00:50:59,000
This is just a warning it's.

275
00:50:59,000 --> 00:51:03,000
I don't exactly know what they want here.

276
00:51:03,000 --> 00:51:18,000
I think it's just some hints. It's nothing really we have to be concerned about. And here I implemented the accuracy function to add so what I'm showing you is like a manual training how basically how you would implement this and let's say your

277
00:51:18,000 --> 00:51:26,000
own pytorch code. And then at the end I will also show you trainer class I mean we are almost done it's just the last little bit here.

278
00:51:26,000 --> 00:51:39,000
So what I have here in this accuracy function how I write my accuracy function is usually that I law the data in batches because I usually have large data sets and you can just put the whole data set into the model as input because you would have

279
00:51:39,000 --> 00:51:50,000
a large matrix multiplication and then you run out of GPU memory. So what I usually do is I bet like similar to training I batch up my data set into chunks into these batches.

280
00:51:50,000 --> 00:52:01,000
It's automatically done by the data loader here, and then I compute the predicted labels and collect them. So I'm just collecting the number of correct predictions.

281
00:52:01,000 --> 00:52:13,000
So I have summed up all the correct predictions. I divide by the number of training examples. So I keep track of the number of examples, and use that to divide the number of correct predictions and that gives me the accuracy.

282
00:52:13,000 --> 00:52:24,000
So it's in that way, not requiring to load all the data all at once. And yeah there are some interesting things here going on so we have the input IDs of the works.

283
00:52:25,000 --> 00:52:36,000
The tension mask. So the tension mask is sort of weird here why do we need a tension mask. So it's essentially for padding so if we have sentences of different lengths.

284
00:52:36,000 --> 00:52:46,000
It will use zeros to do a padding so it's essentially denoting which character is a padding character, and which is an actual word.

285
00:52:46,000 --> 00:52:56,000
So we have our class labels, and we provide as a model input, the input IDs, the words. So the model itself will then do the word embedding, and then the attention mask.

286
00:52:56,000 --> 00:53:04,000
And then we obtain from the outputs the output is kind of I think it's an object but you can index into it might be a dictionary.

287
00:53:04,000 --> 00:53:12,000
So you get the logits, which you can then use. You could use a softmax function to get probabilities but it's not necessary, because the largest largest.

288
00:53:12,000 --> 00:53:18,000
And also the largest probability in softmax. So you get the class table with arc max.

289
00:53:18,000 --> 00:53:28,000
It's your class table and then you know I was just checking whether the predicted label is matching the correct label, the actual label.

290
00:53:28,000 --> 00:53:32,000
Okay, um, this is just how I compute the accuracy.

291
00:53:32,000 --> 00:53:43,000
This is my training group. So I usually have a very simple training group because when I do research I like to tinker with things so I like to have like a more manual approach.

292
00:53:43,000 --> 00:54:03,000
And this works similarly I obtained my input IDs from the data loader, if my attention mask labels, compute the outputs here in addition to, to the logits I also need the loss for optimization so you have to provide also to the class labels as the model input.

293
00:54:03,000 --> 00:54:12,000
So you get both the logits and the loss, and then like in regular pytorch I use backward on the loss and optimize and this is just for logging purposes.

294
00:54:12,000 --> 00:54:24,000
So when I do that on my data set it trained for like an hour, and get gets like a 91% accuracy. One interesting thing is about that briefly I tried an RNN on that and I think I got like 88% accuracy.

295
00:54:24,000 --> 00:54:43,000
I was using this pre trained model, even though it's a small data set the MDB review data set even though it's small, I can use a pre trained transformer to fine tune it on the small data set to get both formats that is actually better than let's say logistic regression which was like 85% and an RNN which was like 88%.

296
00:54:43,000 --> 00:54:53,000
And I didn't do any tuning here by the way I just used it one time because I was short on time I didn't do any learning rate tuning nothing so it worked kind of like out of the box.

297
00:54:53,000 --> 00:55:09,000
And then there's also, if you really want to use transformer seriously I recommend using on the APIs that hugging face on provides so they have a trainer class, where you specify options in the so called trainer arguments.

298
00:55:09,000 --> 00:55:24,000
And then you have this trainer class where you provide this as input and it handles everything automatically. So I also have a run here, and actually it got on even better performance I think they have some certain default parameters for regularization and so forth.

299
00:55:24,000 --> 00:55:33,000
So just first of all faster, and also the performance was a better I think they have better better on default options because my training was very minimal.

300
00:55:33,000 --> 00:55:38,000
So they have some additional options. And actually, I, for comparison.

301
00:55:38,000 --> 00:55:52,000
There's a setting on, I only used one GPU, when I was disabling that one of my computers where I ran this had a for GPS was running in 16 minutes. So, in practice you may be also want to disable that and you get also way faster training.

302
00:55:52,000 --> 00:55:54,000
Okay, so just briefly.

303
00:55:54,000 --> 00:55:59,000
Sorry, Sebastian, we have those minutes to finish please.

304
00:55:59,000 --> 00:56:11,000
Yeah, let's. Sorry, I was going way over time. So yeah, maybe let's go to the yeah we will take some questions. Yes, okay, can start asking the questions for you.

305
00:56:11,000 --> 00:56:17,000
So your last thing is, I will just go to my slides for the questions also.

306
00:56:17,000 --> 00:56:20,000
Okay, that's not the right one.

307
00:56:20,000 --> 00:56:36,000
But I am ready for questions now. So I wanted to say I have also way more detailed transformer lecture on YouTube from my deep learning class and there will be a version of the Python machine learning book coming out for pytorch, which also has a transformer

308
00:56:36,000 --> 00:56:43,000
chapter that will be later this year. So if you want to read more details about this also this might be a useful resource.

309
00:56:43,000 --> 00:56:53,000
So I'm also I wanted to mention this the figures here. They are from that chapter so I want to give also credit to Jitian Joe who helped me a lot with the figures and yeah writing this chapter.

310
00:56:53,000 --> 00:56:57,000
Okay, sorry. So I think we can take questions now.

311
00:56:57,000 --> 00:56:59,000
No worries hasn't you can start.

312
00:56:59,000 --> 00:57:01,000
Yeah, thank you so much.

313
00:57:01,000 --> 00:57:04,000
Thank you so much for your time and for this wonderful talk.

314
00:57:04,000 --> 00:57:11,000
Now it's your time but if you have any questions, and you would like, you'd like to ask it to the first question.

315
00:57:11,000 --> 00:57:21,000
Feel free to write it in the chat, or even you can raise your hand and we'll unmute you so you can ask your question.

316
00:57:21,000 --> 00:57:26,000
So let's start with our first question for today.

317
00:57:26,000 --> 00:57:36,000
This is from Sunny. She's asking, can you be to do classification.

318
00:57:36,000 --> 00:57:53,000
I think so yeah the definitely the first version I think the other versions can do classification to on the hugging way web, sorry hugging face website they have, I think also an implementation of GPT a re implementation, where you can try this out.

319
00:57:53,000 --> 00:57:59,000
I remember I have demonstrated this action spring semester in the class.

320
00:57:59,000 --> 00:58:09,000
So they definitely have capabilities to do classification and GPT two and three, you have to provide it by other context, but in GPT version one.

321
00:58:09,000 --> 00:58:20,000
They also provide classification as an example in the paper I'm not sure if I had a figure on this here, but yeah you can, but people just because the way it's trained in this unit directional way.

322
00:58:20,000 --> 00:58:30,000
Usually, people agree that a bird type of models would be better for classification, although you can use GPT two for classification.

323
00:58:30,000 --> 00:58:33,000
Another question from Marietta Barata.

324
00:58:33,000 --> 00:58:43,000
Is there any use cases right now with transformers based vision models out, out before various models like those based on this net.

325
00:58:43,000 --> 00:58:50,000
And should I use transformers architecture based models now to classify images and to base with net.

326
00:58:50,000 --> 00:59:02,000
Yeah, that's a good question so I, um, yeah, I have seen a lot of papers recently using transformers for computer vision. There's the classic vision transformer and they're also newer models.

327
00:59:02,000 --> 00:59:11,000
And I think they are depends on which paper you read, they are set up in the art on compared to computer vision models.

328
00:59:11,000 --> 00:59:23,000
I think the efficient net version three paper that came out in April on show that they perform better than computer than transformers but this was in April so things might be different now.

329
00:59:23,000 --> 00:59:33,000
But one downside of the of transformers and computer vision is that they require more data for pre training. If you can get the pre trained models it might be for the fine tuning part it might be worthwhile.

330
00:59:33,000 --> 00:59:40,000
Otherwise, otherwise, I think it's more like a research topic now that people are exploring. In practice, it's probably very expensive to train them.

331
00:59:40,000 --> 00:59:42,000
I think there's also this paper.

332
00:59:42,000 --> 00:59:53,000
I'm not sure if they compared directly. So efficient at version three was better than the vision transformer and there's the halo net paper that came out in June, it's like the rest of 50 like architecture with attention.

333
00:59:53,000 --> 01:00:04,000
It's not a vision transform but it has like resonate like backbone with attention. And I think this one ought to form the vision at version three on image net I think they got like 85 point something performance.

334
01:00:04,000 --> 01:00:09,000
So in that way, I think, yeah, it's right now really where transformers are.

335
01:00:09,000 --> 01:00:18,000
It's not a state of the art, but I think it's still worthwhile using CNN for classification for the time being, because they're also cheaper to train.

336
01:00:18,000 --> 01:00:24,000
And then of course we also have architectures like MLP mixer which is neither of them it's just multi layer perceptrons.

337
01:00:24,000 --> 01:00:38,000
So yeah, it's an interesting time I interesting question I, I would say personally I stick with CNN because they are easier to train, but in the future who knows, I think maybe computer vision transformers will take over completely.

338
01:00:38,000 --> 01:00:39,000
Great.

339
01:00:39,000 --> 01:00:48,000
I read about is also asking is transform architecture practical for real world tasks outside of his academia and big tech companies.

340
01:00:48,000 --> 01:00:50,000
And what are the next steps.

341
01:00:50,000 --> 01:00:51,000
Oh, sorry.

342
01:00:51,000 --> 01:01:03,000
What are the next steps to make transform architecture based models more accessible with transformers to be able to adapt or something more automatic will replace it.

343
01:01:03,000 --> 01:01:12,000
Yeah, good question I think that is one of the big frustrations that these transform models are so big that as private personal and academia it's really hard to train them.

344
01:01:12,000 --> 01:01:26,000
I think on some labs in academia have flat resources for example this this paper here on I think the preprint was I saw the preprint I think from this for this paper like in 2019.

345
01:01:26,000 --> 01:01:35,000
Even two years ago they had the resources for training but it must have been really expensive I think they're like 50 GPUs or even 50 GPU TPU pots.

346
01:01:35,000 --> 01:01:41,000
I think they collaborated with big tech companies but yeah, how do you as a normal researcher use these I think.

347
01:01:41,000 --> 01:01:53,000
Yeah, I think it's really infeasible to use these are train pre train these models as a private person I mean you can probably manage but you probably need a whole team of engineers to even set up all the infrastructure

348
01:01:53,000 --> 01:02:08,000
for this work. And in that way I find it more interesting to really fine tune these models like training or using a pre trained model, and then just focusing on fine tuning and like you've seen in this code example I showed you it's like in another one hour you can find

349
01:02:08,000 --> 01:02:27,000
but yeah there are also approaches where people have made transformers way more efficient. I haven't covered them here because of your time constraints but as for example, the nice trim former or my head the sparse transformer, which I think they have usually you have quadratic complexity

350
01:02:27,000 --> 01:02:32,000
they have linear complexity scaling in terms of the input sequence size.

351
01:02:32,000 --> 01:02:39,000
They have made efforts to make transformers more efficient. Well, if I also had a slide even on that here.

352
01:02:39,000 --> 01:02:56,000
There are efforts. So this is unfortunately kept at or clipped 2019 here, but you can see there's this trend that they become bigger and bigger and bigger, but there are also some efforts, for example, the distill bird that I showed you and other methods that try to reduce the number

353
01:02:56,000 --> 01:03:07,000
of operators and allow people to use transformers even though they may not have access to large computer infrastructure. But yeah it's like a concern, many people are concerned about.

354
01:03:07,000 --> 01:03:16,000
There was also this paper calculating the cost. If you have a 1.5 billion parameter model it costs like 80 to 1000 to 1.6 million just to train this model.

355
01:03:16,000 --> 01:03:25,000
So yeah, it's, we will see where things go but yeah this is definitely a concern that this is really unfeasible for many people.

356
01:03:26,000 --> 01:03:35,000
We have another question from Jason. Can GPT to GPT to be fine tuned on some more datasets like few hundreds.

357
01:03:35,000 --> 01:03:50,000
Yeah, I think so. Personally, I have only used GPT to via the hugging face website where they had like derivative of GPT to I don't, I forgot the name I think it's called GPT meal.

358
01:03:50,000 --> 01:04:01,000
So this GPT meal is like an effort by the open source community to train a GPT to model because I don't think, I think maybe has changed but I don't think they.

359
01:04:01,000 --> 01:04:15,000
I have shared the full model I think they only provide access through an API. So that way if you only have access to an API I don't know if you can find unit, but there's this GPT meal project.

360
01:04:16,000 --> 01:04:29,000
I would have to search the website I don't know my head by a thing it's like this, where they have reengineered either GPT version two or three and I think this might be something that you could download and fine tune.

361
01:04:29,000 --> 01:04:38,000
And I think hugging face also has maybe some models I would have to double check. I haven't done this myself, but I think it should be possible.

362
01:04:38,000 --> 01:04:49,000
So for GPT two and three it's kind of not necessary because you have like these contexts so they say with the context it's sufficient but you could maybe fine tune it I, I don't know for sure.

363
01:04:49,000 --> 01:04:52,000
It's a good question.

364
01:04:52,000 --> 01:04:54,000
Great.

365
01:04:54,000 --> 01:05:03,000
I'm wondering if you can use between for specific domains, like, for example, medicine.

366
01:05:03,000 --> 01:05:10,000
Yeah, definitely. So this is, yeah, yeah, so you can definitely adopt the architecture for specific domains.

367
01:05:10,000 --> 01:05:22,000
Anyway, if you have a small medical image data set like texts you could technically fine tune this, but if you have a large data set you can also maybe try to train it from scratch, like the researchers did here.

368
01:05:22,000 --> 01:05:26,000
So here instead of training it on sentences they use the BERT model.

369
01:05:26,000 --> 01:05:29,000
I think this group, there were two groups that did something similar.

370
01:05:29,000 --> 01:05:36,000
They used the BERT model and trained it on amino acid sequences where they had millions of sequences from the protein structure database.

371
01:05:36,000 --> 01:05:46,000
They were like, I mean you can think of an amino acid sequence as a sentence, but each character there are 20 different amino acids, each character would represent a word essentially because it's on.

372
01:05:46,000 --> 01:05:56,000
It's more like a character level type thing, but they did that and it worked well so I can imagine you can also train it on other types of sequences and in the medical domain depends on

373
01:05:56,000 --> 01:06:08,000
that you have a sequence of specific domain specific encoding that say you have a certain device that outputs certain values that don't save a meaning to a human, but that could be passed by a machine.

374
01:06:08,000 --> 01:06:17,000
I can also think of a transformer maybe being trained on that or if you have just texts like annotations of patient records I think that could also be done.

375
01:06:17,000 --> 01:06:24,000
So yeah, you can definitely train this on other things other than general text.

376
01:06:25,000 --> 01:06:43,000
Altie is also asking, when we fine-tune the BERT train, will that update the vocab of any initial vocab will be the same after fine-tuning?

377
01:06:44,000 --> 01:06:56,000
I think a good question so when you fine-tune it, so you have the text embedding, I think it would update also the embedding step.

378
01:06:57,000 --> 01:07:12,000
This is something I can't say 100% sure because when you use this code from HuggingFace, the code from HuggingFace includes the encoding as far as I know because you only provide the input IDs of the words and then coding is part of the model.

379
01:07:12,000 --> 01:07:19,000
And if you update this, I think you should also, it should also update the embeddings.

380
01:07:19,000 --> 01:07:35,000
I'm like 90% sure but not 100% because I haven't really looked at the code line by line, maybe they have a line that freezes the weights for the embeddings so that I don't know for sure but I suspect the embeddings are also updated.

381
01:07:36,000 --> 01:07:49,000
Sorry, I think the question might have been about the vocabulary size. So I think the vocabulary size is fixed so I don't think this can be updated because the vocabulary size depends on the input ID on the tokenizer.

382
01:07:50,000 --> 01:08:09,000
I think the tokenizer would remove words from your input text that is not included in the pre-training. So if you have some arbitrary words that are not in the database where the model was pre-trained on, I think you don't have to worry about it if you use the right tokenizer because it would replace it by an arbitrary token.

383
01:08:09,000 --> 01:08:27,000
And of course also the model wouldn't then be able to perform well on if you have a lot of different tokens because these are tokens the model has never seen before and they are all, I think, converted to arbitrary unknown token or something like that.

384
01:08:27,000 --> 01:08:31,000
Okay, nice. Also, do you want to say anything?

385
01:08:36,000 --> 01:08:39,000
You can unmute yourself and ask any questions.

386
01:08:39,000 --> 01:08:40,000
Can you hear me?

387
01:08:40,000 --> 01:08:41,000
Yes, sure.

388
01:08:41,000 --> 01:08:54,000
Thank you. Sorry for insisting on this question, but I just wanted to return a bit to this point, to the last one, to the vocabulary one.

389
01:08:54,000 --> 01:09:03,000
My idea is that of a fast AI library in which they have the NLP part of it together with the image.

390
01:09:03,000 --> 01:09:20,000
And actually, Jeremy Howard has made this fast AI library in a way that when you fine tune the pre-trained model to a specific domain, then all the words that are not tokens,

391
01:09:20,000 --> 01:09:28,000
but part of the original vocabulary of the pre-trained model will be added to the new vocabulary.

392
01:09:28,000 --> 01:09:37,000
And that will be, they will train by starting from scratch, let's say, no, as you do with the pre-training, but with this.

393
01:09:37,000 --> 01:09:45,000
But I'm not sure that this is the case for Bert, to be honest, because I've been asking the guys in some other talks like this.

394
01:09:45,000 --> 01:09:48,000
And as far as I know, they don't do that.

395
01:09:48,000 --> 01:10:03,000
And that is the original question, like, how well does perform Bert pre-trained, let's say pre-trained Bert, when we fine tune it, I don't know, in medicine, when most of the characters, I don't know, in whatever other domain specific task,

396
01:10:03,000 --> 01:10:09,000
when most of the words or tokens, sorry, tokens might be quite different, and they will be just unknown, no?

397
01:10:09,000 --> 01:10:12,000
It's like technical question, sorry about that.

398
01:10:12,000 --> 01:10:21,000
Yeah, partly you answered, thank you, but just like it's not about the size of the vocabulary, but like exactly in this thing, thank you so much.

399
01:10:21,000 --> 01:10:27,000
Yeah, so yeah, I think that's a good point, I think they are kind of related.

400
01:10:27,000 --> 01:10:40,000
So I think in the Bert model, they basically have a fixed vocabulary where all the words appear that appeared during pre-training plus an unknown token.

401
01:10:40,000 --> 01:10:47,000
And then if you have to in fine-tuning the new words, it will just be mapped to this unknown token so they don't expand the vocabulary, right?

402
01:10:47,000 --> 01:10:54,000
That's what I'm suspecting. And you mentioned that fast AI, they would add it to vocabulary.

403
01:10:54,000 --> 01:11:06,000
Yeah, I haven't really worked with the fast AI version of this, so that sounds like a good approach right now if you then have some really, if you don't want to maybe pre-trained completely from scratch,

404
01:11:06,000 --> 01:11:15,000
it's a very different data set with different tokens, so you kind of, it's kind of more flexible in that way, I would say. Yeah, that makes sense, yeah.

405
01:11:15,000 --> 01:11:18,000
Thank you very much.

406
01:11:18,000 --> 01:11:28,000
Thank you, Altie. Again, if you want to ask any questions, you can raise your hand and we will unmute you so we can ask questions to Dr. Spastien.

407
01:11:28,000 --> 01:11:42,000
In the chat, we have a question from Anna. How to handle the bottleneck caused by the data loader reading each example from memory?

408
01:11:42,000 --> 01:11:57,000
Yeah, the bottleneck, oh, so in terms of speed, yeah, you could increase, so what I do sometimes if it's too slow for a very large data set, what I would do is I would increase, I would make a new data loader for the training set.

409
01:11:57,000 --> 01:12:13,000
If I want to compute the training set accuracy, I would have a second one, which has just a larger batch size than a one I used during training and for the test set, I would just set the batch size as large as it goes until it crashes, because at some point it would just crash.

410
01:12:13,000 --> 01:12:29,000
In a case like this, I would say it's not really a bottleneck, it's, I mean, it takes maybe a minute or maybe a few seconds to process the whole data set in each epoch, and we are only computing in the accuracy function on, like,

411
01:12:29,000 --> 01:12:38,000
the performance in each epoch, so it's not like too often, I think this is reasonable, but maybe the question is also about like how we deal with that bottleneck during training.

412
01:12:38,000 --> 01:12:48,000
That's a good question, I am actually, yeah, that's one limitation, so I could only go up to batch size 16 here in this example, because otherwise I would run out of memory.

413
01:12:48,000 --> 01:13:03,000
So one way to deal with that would be using a different type of transformer, for example, so there are more efficient transformers that have fewer parameters and smaller maybe inputs sizes, smaller number of tokens that might be in possibility.

414
01:13:03,000 --> 01:13:17,000
But there are also different ways for distributed computing, it depends on what type of distributed computing you do some can kind of put the model onto different devices and handle with handle batches like that.

415
01:13:17,000 --> 01:13:27,000
So for instance, if you remove the number of GPU restriction I had in my code, it would run on multiple GPUs and I think it splits up the batch size.

416
01:13:27,000 --> 01:13:46,000
In this case, it I think would multiply the batch size if you have four GPUs before and then each GPU gets 16 but still you're processing 64 training examples at the same time and then you average the gradients from the different GPUs in that way, you can use larger batch sizes if you have multiple GPUs, that's another option.

417
01:13:47,000 --> 01:13:59,000
But yeah, that's fundamentally a limitation of matrix modification in memory, for example, if you have only access to one GPU that could be challenging.

418
01:13:59,000 --> 01:14:00,000
Great.

419
01:14:00,000 --> 01:14:12,000
Well, I studied with Anna, she's asking, how can an individual like graduate student train a transformer for data for which there is no break train transformer available.

420
01:14:12,000 --> 01:14:15,000
Is it even possible to resource wise.

421
01:14:15,000 --> 01:14:19,000
Yeah, good question. I haven't done this myself yet I'm.

422
01:14:19,000 --> 01:14:32,000
I think based on what I've seen from papers where people have done that for example this protein sequence paper training from scratch look very scary in terms of what resources on they had to use in order to make that happen.

423
01:14:32,000 --> 01:14:45,000
And if the only case is like maybe if you have a small data set, maybe that might be feasible but I think then maybe the transformer won't be the best option if you only have a small data set for pre training, then maybe using a classic RNN or

424
01:14:46,000 --> 01:15:01,000
even a bag of words model might be a better approach. But yeah I don't have unfortunately a good answer for that I think that's a concern I have to, if you I personally also don't have access to a large cluster where I could do that and even if I would have access to that.

425
01:15:02,000 --> 01:15:16,000
All the engineering efforts that go into setting this up is also kind of challenging so companies, so far as I know, have really large teams of engineers that only really focus on the coding part just to run the model on these multiple devices and GPUs.

426
01:15:16,000 --> 01:15:30,000
I mean there are API said make that simpler but I think in practice it's still not very easy to do that we need to have like experts doing that but maybe I'm wrong so I don't want to discourage anyone from trying this out in practice but

427
01:15:30,000 --> 01:15:35,000
I think there's a better answer at that moment.

428
01:15:35,000 --> 01:15:39,000
Back to Ritu Paratha.

429
01:15:39,000 --> 01:15:58,000
I have a personally point tuned a pre trained GBT to model, which has 115 million bar meters with about 2600 mid sized text on 70 lines each and talk with me 15 minutes in.

430
01:15:59,000 --> 01:16:01,000
Yeah, that's nice. Okay.

431
01:16:01,000 --> 01:16:16,000
So on a very small 3000 data point. Okay, I didn't expect it to perform that well that is actually cool. So, one at 7 million it will be the size of GPT one approximately that is impressive I, I think that that sounds very promising then.

432
01:16:16,000 --> 01:16:25,000
So, forget what I said in the previous answer. Maybe it is possible to really pretend train them on small data sets so yeah.

433
01:16:26,000 --> 01:16:35,000
A question from Sujana. Why is there a norm used and not bad snow.

434
01:16:35,000 --> 01:16:46,000
That is a good question why is layer norm used and not bedroom. I think it has something to do with the fact that we have a sequence.

435
01:16:46,000 --> 01:16:58,000
We normalize across the sequence and not across across the batches by top of my head, I don't have a good answer I could.

436
01:16:58,000 --> 01:17:11,000
I think I was thinking about that at some point but I forgot I think it's the way we normalize over over the batches in a way.

437
01:17:11,000 --> 01:17:22,000
That's a good question. Another thing is batch not doesn't really perform well. If you have batch sizes smaller than 32 or 16, but I don't think this is the answer I think the answer is mobile.

438
01:17:22,000 --> 01:17:29,000
The way we normalize this I would have to think about this again why they're not not batch norm.

439
01:17:29,000 --> 01:17:32,000
Good question.

440
01:17:32,000 --> 01:17:41,000
Anyway, I think this concludes the Q&A session for this talk. Thank you so much, Dr. Sebastian.

441
01:17:41,000 --> 01:17:43,000
Back to you.

442
01:17:43,000 --> 01:17:46,000
Thank you, thank you Dr. Sebastian.

443
01:17:46,000 --> 01:17:54,000
Sebastian, can you share the slides or the code that you show us on the chat so we can post it on the Twitter. Yep.

444
01:17:54,000 --> 01:17:58,000
I have it right away. I have this GitHub repository here.

445
01:17:58,000 --> 01:18:06,000
And the GitHub repository has a link to the slides. I can also post it separately.

446
01:18:06,000 --> 01:18:07,000
It's right here.

447
01:18:07,000 --> 01:18:09,000
Thank you.

448
01:18:09,000 --> 01:18:16,000
Great so we definitely want to give a huge thank you for you Sebastian. This was super insightful and hopeful.

449
01:18:16,000 --> 01:18:25,000
Thank you everyone for joining. And please follow us on Twitter for more details about upcoming talks and see you soon. Thank you.

450
01:18:25,000 --> 01:18:39,000
Yeah, thank you so much everyone for attending and also very good questions. Sorry for rushing a little bit about through the topics I don't know it's like I always want to talk about so many things and there's only so little time, but I appreciate your

451
01:18:39,000 --> 01:18:44,000
patience and joining me here today that was fun and I really liked your questions that was cool.

452
01:18:44,000 --> 01:18:47,000
Yeah, then have a great rest of the day everyone.

453
01:18:47,000 --> 01:18:48,000
Bye bye.

454
01:18:48,000 --> 01:18:49,000
Bye bye.

