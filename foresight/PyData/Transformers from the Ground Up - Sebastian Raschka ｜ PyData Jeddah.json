{"text": " Welcome and good evening everyone. My name is Atman, one of the volunteers of the BiData Jiddah chapter. I'm here with my team members Yasmin and Hassan Shadad and Abdelillah. Today's meetup is titled Transformers from the Ground Up by Dr. Sebastian Rashka. Dr. Sebastian is an assistant professor of statistics at the University of Wisconsin Medicine, focusing on deep learning and machine learning research. He's also a contributor to open source software and the author of the best-selling book, Python Machine Learning. Thank you Dr. Sebastian for joining us and also we want to thank our sponsors, Namfocus and Muzun. If you want to visit our BiData Jiddah website or follow the Twitter account to be updated about our new upcoming meetups. And we also post a lot of cool things on there. And I guess that's all from me. I'll stop sharing so Dr. Sebastian can start. Thank you. Yes, thank you so much for the kind introduction. I really appreciate it. It's my pleasure to be here. Thanks for the invitation. So I will start then by sharing my screen and if there's some issue, please let me know. I can then try to fix that so it should be visible now. Can you see everything? Yes, it's clear. Yeah. Oh, perfect. Great. Okay. Yeah, then yeah, I will get started. So today, the talk is about transformers, the natural language processing models in deep learning. And I'm trying to cover transformers from the ground up, like explaining a little bit how they work. And then also, I hope we have time for that at the end, showing you an implementation in PyTorch. So yeah, why this topic? So yeah, I personally got very interested in this topic recently and I was also working on the new edition for my Python machine learning book. It was just putting together the chapter and had this very long lecture at my end of the semester last, last semester, when I was teaching the deep learning class. And this is essentially more like a condensed version of that highlighting the main concepts by skipping over the mathematical details, which I think would take too much time to digest. It's not like a week long effort to dig through all the details, but I hope this will give you like a good idea of how transformers work, and also helping you like navigating the jungle of all the terminology a little bit because there are so many different models out there like BERT, GPT version one, GPT version two, and yeah, you might wonder how they are different and what you can do with transformers. Yeah, just to start a few examples where transformers are used nowadays. So one popular example would be language translation. That is also how the original transformer architecture was developed. So language translation was the main motivation behind developing the original architecture. And yeah, nowadays, transformers are pretty much everywhere when it comes to natural language processing. And even beyond that, there are many real-world applications of transformers. For example, recently I also saw this article. So I'm also working in computational biology problems. Recently there was this article on using transformers to kind of elucidate or decipher properties and proteins like organizing proteins by structure properties, but also by function. And they trained, for instance, transformers on a large scale database of amino acid sequences like protein sequences, and they found all kinds of interesting insights from that. And yeah, you probably have seen recently the GitHub co-pilot project. I think this was in collaboration with OpenAI and I think also Microsoft, where they trained a transformer to generate code. What's interesting is that you can essentially enter a query, a comment, a code comment, and this AI system, the model will generate a corresponding code here at the bottom. So if you go to this website, they have like a nice animation of how that works, and they have a Microsoft Visual Studio Code plug-in where you can download this and actually use that in action, which is really impressive. So I think right now they have JavaScript support and Python support, so that would be another interesting application of transformers. So transformers mainly are about generating things like generating translations or generating code, generating text, for instance, also in chatbots, but you can also use them for classification, like classifying, let's say, whether a movie review is positive or negative, and this will be a code example we will be covering at the end of this lecture or talk. So the topics I have in mind for today are those. So first I wanted to, from the historic perspective, just briefly explain how attention came to be. So there was essentially this attention mechanism that was first added to recurrent neural networks, and then there was this self-attention mechanism, which is a, I would say, a more sophisticated version of that, there's this so-called scared dog product. Attention, oops, I almost ripped off my microphone here. Yeah, so and then we will talk about the original transformer architecture, which is the first architecture that was just using attention. So in step one here, this is a recurrent neural network or commented with attention, and the transformer is then an architecture is just using attention without any recurrent neural network layers. And then I will briefly dive into some of these large scale language models, for example, your GPT and bird, which are pretty popular these days. And then after that we will pre-train or sorry, we will fine tune a pre-trained bird model in PyTorch. So one thing about these large scale language models is that they are really large scale and will take forever to train them. In practice, usually they are trained on large data sets, thousands or millions of websites and books and so forth. And so usually people download a pre-trained version, and then fine tune it to a target task. So we'll also take a look at how it looks like. And then I have a few closing thoughts or words on transformers. And yeah, with that, I think we have plenty of things to talk about in the next, I would say, 40 minutes. Let's get started with the attention mechanism that was also first small disclaimer here. So like I mentioned in the beginning. So this will be more like a conceptual overview. So if there are some, let's say, notational details in these figures, don't worry about them too much. To be honest, I think it's most important to kind of understand the big picture and all the mathematical details. That is something that are almost like implementation details. Some of them are almost arbitrary. So I wouldn't worry about it too much. But I hope this talk kind of piques your interest in terms of transformers. So if you're interested in that you can dive in deeper into the topic afterwards with having this big picture overview in mind. So starting with the first topic augmenting the recurrent networks with attention. So first, what is the motivation behind using attention why don't we just use recurrent neural networks. So, if you think of a task sequence to sequence task, which is a task where you have an input sequence and you want to generate an output sequence. So one example of that would be a language translation, where you have an input sequence, let's say this one here in blue, in one language, let's say, German, and you want to translate this into English. So here on the output, then you have an output sequence. And what happens is if you use a, or if you design an arcade architecture for that, you usually use this encoder decoder setup in RNNs. So let's say that the RNN first ingests the whole input sequence. So you have input element one, two and three so these are for instance, individual works, and they are then processed into these hidden states. So once you have processed the whole input in this encoder so this would be essentially the encoder part, you have all the information in one, one hidden state here, and then you have the decoder part, which yeah generates the output. So here, why don't you, yeah, why don't you translate this one by one because here one problem would be that this hidden state has to remember all the input in one hidden state and this might be a little bit overwhelming. Right, so if you have one hidden state that has to capture the whole input, there might be some information loss. And this is essentially one of the challenges of these RNNs they work kind of well if you have short sentences but the longer your sentence becomes the more challenging the network to generate a good output because it kind of forgets what happened let's say here in the very beginning if you have a very, very long input sequence. So and also, yeah, why don't we just translate sentence word by word so here's just one example. Yeah, because I only speak two languages, German and a little bit of English, I have a German to English translation example here. So it's just an attempt on translating this German sentence here at the top into an English sentence. So what happens if you translate this word by word, so it doesn't really work. So you have a sentence. I'm an English as a second language speaker so I'm usually also not that good at that but I can tell that this one is definitely wrong. So you can see here, the sentences, can you help me, or can you me help this sentence to translate it doesn't really make sense if you just let's say take a dictionary and translate word by word. So to have a correct, I hope this is correct if you have a correct English translation, the sentence would be can you help me to translate the sentence, and you can see, there is some, yeah, some longer range dependencies there are some words here for instance those that you can just translate word by word, but then you have, let's say these two here, where they are kind of flipped so in order to translate this word, the help. You have to look into the future basically so you have to find an element that is further away in the input sequence. For example, if you have this word here, it goes here, you can see there's a more longer range dependency and the longer your sentences are, the longer these dependencies become. So in that way, we can't really have this word by word translation attempt because it would not result in any meaningful grammatically wrong translation. So going back, this is why we first read the whole input, because yeah we need to have knowledge about the whole input before we can attempt the translation but then again the challenge is okay, we may forget earlier hidden states. So one idea to deal with this problem that we can capture these long range dependencies is this attention mechanism so this was first proposed in the context of RNNs in 2014. So I also, I will share my slides so you if you are interested you can look up these papers and read a little bit more details about that. So this is an attention mechanism that the researchers used to augment the RNN to better process the input. So here's just like a sketch in the lower left corner. For instance, if we are thinking of the second step here, the hidden state might have access or at this step in general you might have access to the whole sequence. You can also align with to denote that one input sequence might be more important than another and this is where the attention comes in. So one step will have access to the whole input, but with a waiting term that says okay maybe at this position this term is more important than let's say this term and so forth so not every word is equally important. So there's also some some sort of waiting and the architecture looks approximately like that. So there's a sketch I made after the paper. So it's just a little bit different notation but essentially what we have is we have an input sequence. So this is just individual words, one to two words, and then they have two RNNs kind of connected here. So there's one RNN. It's a bi-directional RNN. Bi-directional RNN is just a fancy word for an RNN that processes the sequence from left to right and from right to left. It's just so that it captures, you have information from both ends. It's just increasing the amount of information you have essentially. So the forward direction would be the regular RNN direction and then you essentially just flip the sequence and process it backwards. So this is what we denoted here so you can denote it from right to left but you can also think of it as just reversing the sentence and processing it in the regular way. Then you connect these hidden states and then you calculate the so-called attention weights. I will skip over the details of how these attention weights are computed here but I will show you how they are computed in the transformer. It's a little bit different but not too different. So but the main part here is really that using these attention weights, we obtain the so-called context vector at each step. So the context vector here is essentially a weighted version of all the inputs. So you can consider all the hidden states here and it's essentially a weighted sum. You have this attention weight, you multiply it by this state and then you just sum them up. And the larger the attention weight, the more the hidden state will contribute to this context vector. So this context vector is essentially then kind of capturing all the input information in a certain processed way and this goes into each decoder part. So if I go back a few slides, we had this decoder part here where we were generating some output and this is what we are having here at the top of the O's. The O's, they should just denote, let's say, the translated sentence. And each output step, what we have is we have the hidden state that goes into, let's say, the that position, the previous word, because it's also important. I mean, if you generate a sentence, you kind of look at the previous word you have just written, right? So what you currently translate the current word is also depending on the previous word you have written. The sentence might not make sense. So you provide the previous word, you provide the hidden state, and you provide this context vector and with that you can capture all the input when you attempt the translation of the next word. So this is essentially an enhanced version of recurrent neural network for translation, where this party is essentially new, it's this attention part. Yeah, and from this attention part, people then develop this self attention mechanism. So the self attention mechanism is one major building book of this original transformer architecture that we will cover. And it's kind of inspired by this RNN that I just showed you. This is a very simple version of self attention. It's not the version that is actually used in the transformer that comes in the future slide. This is a simplified version just to understand the main concept. So what you can see here is, so to cover it step by step, there are three steps. So this one is computing these dot products. So imagine, we are currently processing a given word, Xi, and we compute the similarity of this word. This is how we get a certain score for computing the attention weights, we compute the similarity of this to all the other words, including and why are so what we have here is a sequence of words one to T and why are the four blocks there so the four blocks, they just know that this is a vector. So it's for example a word embedding so usually when we do natural language processing, we don't really feed it in the string of the word so we usually have a method that converts the string let's say to a vector of real numbers. So this should just denote a word embedding here. And then there's this, you know, this dog product between two word embeddings to vectors that we multiply. And let's say this gives us the attention weight. So it's just a simplified version. Usually we would some use something like a softmax function which is a function that would normalize them so that they sum up to one. So that we then again compute the weighted sum of these. So we have these attention weights, and then we have our vectors. So vector one to T, and we just wait them by the corresponding attention weights. Also, we have on a special a specific attention weight for each query. So if, let's say, Xi here's my query. These attention weights will be different, compared to if x two is my query, because it depends on the on the dot products right. So these would change each word has different embedding so every query has its unique attention weights and from that we derive this context vector which is kind of similar what I showed you in the RNN example. So this is a simple form of self attention, where we just introduce some, you know, waiting of all the inputs when we compute some output for current word. So one problem with that though is, so the main takeaway here is that we compute these attention weights but one problem is that we don't have any mechanism for training a neural network now right because there is no, no weight parameter involved. So how do we improve this how do we use that say back propagation or gradient descent with that to learn the optimal attention weights for example. So for that, we in practice actually use a different type of attention it's called on scaled dot product attention it's one form of self attention with learnable weights so this involves weight parameters. And yeah there are then three matrices, and with these matrices, you compute also again like a vector a query a key and a value using the input query. So even now these matrices are weight parameters that you can update during back propagation or gradient descent to optimize the model. So here, what's going on here is we have a current input query that we are currently processing let's say the second word here again. And from the second word we apply these three matrices to compute three values or three vectors so a query a key and a value. Using those so so what we do then is we use the query first so this this query to compute these omega values these are kind of like unscaled attention rates with each key. So the key is computed for each input word again. And we compute the dot product between query and key, but notice here we always use the query to which is our current word that we are processing so we again obtain these unique values or unique attention weights for each query that we are currently processing. These are some normalization steps so this is on scale dot product attention. It's a softmax with a scaling factor essentially. And then again, like before we are summing them up to this context vector zine. So this is essentially the same that I showed you here except that it now involves here these trainable matrices and this is, yeah this is the basic on building block behind these transformer networks. So, um, yeah it's maybe a little bit fast but to be honest is really not that much behind the self attention it looks maybe a little bit complicated so maybe you need to look at this a little bit longer it took me definitely some time on to understand what's going on there, to be honest, a good way for me learning about that was really like reading the paper and then doing these or making these figures and drawing that to kind of make sense of it. So yeah once once you take a look at this it's actually not as complicated as it looks like looks like. I think the complicated part here is more like that these language models are so large. And if you look at the code itself it's kind of like overwhelming because there's so many things going on there in the code not not in the building block here. So now to get to the original transformer architecture which is built on this self attention mechanism. So, this is based on this paper attention is all you need that started all the trends in terms of developing transformers it's a very influential paper that came out in 2017. It's not too long ago four years ago and since then they have been hundreds if not thousands of different transformer architectures. The main part is essentially that this is centered around this self attention mechanism, and it does not use any RNN parts so there's no recurrent neural network layer nothing it's just using the self attention concepts and stacking them. And then also having some fully connected layers like in a multi layer perception just a very basic neural network layers. So this is a sketch of how the transformer architecture looks like. So yeah, like I mentioned, it looks very complicated at first glance there are a lot of boxes and lots of things connected to each other, but essentially it's not too bad it's relatively straightforward it's actually, again, our encoder and decoder like in the RNN that I showed you earlier where the job of the encoder is to, yeah, ingest this on input sequence, and the decoder job is to generate the output sequence. Yeah, what you do first is you have a word embedding for example to embed the words into real valued vectors. There's something like a positional encoding. Because the problem is, there's its permutation invariant here but we will skip over these details. It's just like a way of encoding the position of the word. Then we have this so called multi head attention. It's kind of like, I would say an extension of the self attention mechanism I will have a slide on that to briefly explain what that is. Then there is a layer normalization. Yeah, like a multi layer perceptron a fully connected network. And then again a layer normalization and so forth. So it's just repeating some of these units and in between. We also have something like these arrows here connecting things. And these are skip connections. It's essentially just this is what the ad stands for it's taking this value and adds it back here it's similar to what a residual network does if you afford a residual network. So the means of preventing, let's say a bad gradients. So if you have something like a vanishing gradient problem here, then this one will be added to this one it's it's kind of like skipping layers if the layers are bad, then there's an opportunity to skip them via these connections so to prevent. Bad, bad layers for from ruining the whole network. And then yeah this encoder just captures essentially the input in this self attention weighted way. It's like an encoding of the input. And this feeds into the decoder which has itself also multi head attention there's a masked version which I will also talk a lot of bit about. Yeah, and this is essentially it so it goes again through a fully connected network and then there's a fully connected layer to produce the outputs so the fully connected layer is then producing word probabilities with a softmax function like how likely is this word at a given position and then in practice we usually use the highest probability if we, let's say do a language translation or if we generate text in general, we want to have some randomness, we want our model to generate different texts. So just on sample from this distribution. But yeah and in the original transformer architecture what they had in mind was language translation. Yeah, to dive a little bit more into this multi head attention, it sounds like it's a whole new concept, but it's essentially just our self attention mechanism so here this is the scaled dot product attention that we talked about earlier with these different matrices. And multi head attention is just repeating this multiple times it's basically a stack of these things. It's kind of like similar when you. If you have worked with convolution networks and you have an input, and you apply multiple kernels to produce multiple feature maps as the output. It's kind of like the same concept that each channel on captures different type of information here. Anyway, this is a similar concept where you just repeat the self attention multiple times and stack it up to each other. And this helps you're extracting different features that are useful. And yeah one thing about that is on what's also nice is that here. So stacking or what the advantages is we can parallelize that because there's no dependency between these individual scale product attention so you can compute this in parallel helps us also to leverage multiple GPS for example. So we have now these so called hats attention hats, and each attention had has essentially one set of these matrices so if we have each on attention hats we have each sets of these matrices, and then we just repeat the same thing multiple times. And also I should say here. This is six times. So these blocks are also repeated six times. I think this is what makes the architecture so big and complicated when you look at the code if you look at the, let's say from scratch implementation is just there are so many layers and there's not so much on code that implements all of that, but the underlying concepts, if you draw them out there not itself themselves that are not that complicated it's just your matrix multiplication, and then softmax and matrix modification. Let's talk a bit about this mask maybe here. So the mask multi hat attention is masking out words that we haven't generated yet. So we have a target sequence that is our, let's say desired output that we want to generate let's say if we want to translate it, but at a given time step we don't want to have something that is in the future. So if we generate a sentence let's say on right. Let's say have a sentence help. We translate this. First, we would have then let's say a German version of that but let's say this is our targets version. So in the first step, we would only have access to help, and then this one would be masked out. And then as a second step, we would have access to these two words and these would be masked out and the mask multi hat attention is essentially ensuring that future words that are not generated yet are masked out. So actually a slide about that. Yeah and this masking out. This can also be considered as a unidirectional form of language modeling, because we are masking out everything that is in the future from the current position. So we are masking out from left to right so it's kind of unidirectional. And we will also talk about a model that has so called bidirectional approach so the Burke model has a bidirectional approach and we will revisit this topic it's kind of in it sounds actually fancier than it is so we will see in the future what this bidirectional means also. Okay, so yeah, this was already on all the transformer general original transformer stuff I wanted to talk about. And these were all the original ideas like from a historic perspective so nowadays, people don't use this original transformer architecture anymore but there are many, many other architectures that have been inspired by this architecture. So here in the section I want to briefly highlight some of them. So one is this GPT model developed mainly by the open AI team. And this takes this unidirectional approach which is kind of similar to the decoder of the original transformer. And this is used for tasks that involve generating texts. And then there is the Burke model and of course there are yet other different types of versions of it for example, Roberta and Alberta and so forth. But in general the Burke model is bidirectional approach, and this is more suited or better suited for classification for example, in the code example that I prepared where we predict or whether the movie review is positive or negative. So whether someone liked the movie or not. Yeah, but one, one scheme that is common amongst most transformers is that there are two steps. One is a pre training step, a pre training step on very large unable data sets. And then there's a fine tuning step where we take a pre trained model, and then we train it on our target data set. So here in step one, this is usually a general data set that is not really related to our target task. It's for instance just a library of books or websites, but we don't have any labels and yeah this is just used to pre train the model they are very large models so they need a lot of data to be kind of pre trained. And for this training, there are these two steps that I mentioned the pre training and the fine tuning one and two. And the fine tuning can also be split into two different approaches, and both are very common. I think fine tuning is a little bit more common performance better but it's also more expensive. One is this feature based approach and one is this fine tuning based approach. So how do they differ. So assume you have step one where you pre train your transformer. So this is sometimes called in the original literature it's called unsupervised pre training, but the more modern term for that is self supervised learning. So the same thing essentially, where you train a model on unlabeled data. And then, so the data is unlabeled but it's still a supervised learning approach. This is why it's called super as a self supervised learning. So you generate essentially the labels on yourself. So you take let's say a book, and then you train the model to predict you remove some words and then you train the model to predict missing words, or you predict the next word in the sentence and the next word in the sentence would be your label, but it's not like a label for classification it's just like a label that you can generate yourself from the text you don't need to have a human labeling these texts. Yeah, so and then let's assume we have now our pre train transformer in this red block. So then, once you have a pre train on a large data set you never have to change it again in terms of you basically save that somehow on your computer, and then you can use it for all your downstream projects that is what I mean. And one way is to never really change it you take it. And then you just extract the last layer so you give it a new sentence, and then you extract the output layers, one or more output layers you can concatenate them. There's output layers to a classifier, for example, logistic regression multi layer perceptron maybe a random forest, whatever you desire, and you only train this classifier. So, you essentially treat this pre train transformer as a feature extractor. If you have used on something like principle component analysis. This is also one method that say it's a linear transformation of your input. It's one method to, let's say, extract some features from your original features so it's in that way. So pre train transformer acts like a feature extractor where you just use it to extract these last layers, and then you just train a classifier on top of that. Another approach is this fine tuning approach it's a little bit more expensive. This is, you know, for what I prepared this code example. So this fine tuning approach is a little bit different in that you update the whole model. And then you start with your pre train transformer. Let's say you saved it somewhere on your hard drive, you make a copy of that. And then you feed it the label training data set that you have. And you add a few layers for classification. Let's say if you have if we have our movie review data set we add one final note in the output layer, have a softmax activation, and then we just train the whole model with back propagation. And because we have to load all the models, the whole model in memory and do the back propagation it's a little bit more expensive than than this step where we only use the transformer for generating the features. So that way, it depends. It's of course also slower. So it depends on what type of architecture you have access to to use this approach but in practice it performs a little bit better. I forgot which paper it was either GPT one or bird. They also directly compared those approaches. I think my GPT one but I would have to double check. And they found I think if you lose use the last three layers here, you can get approximately the same performance as with fine tuning so in that way it's, it's really, you can get the same performance but it's really a trade off also of how much computational resources you have. Yeah, so talking a little bit about the GPT models. So there are three GPT models now. So one was released in 2018 one in 2019 one in 2020 and yeah, if the sequence continues we maybe have a new version by the end of this year. And yeah, one obvious thing you can see from these GPT models which stands for generative pretrained transformer is that they increased a lot in size so they started with 110 million parameters which was already really huge. Then the next one had 1.5. And now we have 107 175 billion. So who knows maybe the next one is in the trillion range. So just like I said, I will share my slides later so you can also read more about that in the original manuscripts, but yet just to give you just brief summary of how these models work, I have a few slides on those. So GPT one, that's the first model in the series. It's essentially very similar to the transformer we talked about, especially I mean, it's just taking the decoder part. And it's trained on a sentence in this next word prediction manner. So the pre training is taking a large context or a large corpus of books or websites I think, I think they use the big a mix of words from websites and books. There are instances where they remove through this masking, the future words in the model's task is to predict the next word, and they just train the model in this manner. So they train it just for this prediction of the next word in the sentence. So by doing that, you can then fine tune it so you add. So for having this task classifier, you can add additional layers. After you pre trained the model, and depending on what you're interested in this you can do our classification entailment similarity multiple choice questions and so forth. And how that works is essentially they have this pre trained transformer here. You connect the linear layers and then it's like a regular classifier. And yeah, one thing here is. I won't go into too much detail but it's how you format the input. If you have a classification, you have like a start token a text, the sentences and then some extraction token. If you have something like similarity here, text similarity, they have on both texts and then in flipped order for example so it just the first a little bit about how you set up the inputs, but yeah the main, the main thing is that you can just adopt this pre trained network to do whatever you like in the fine tuning. So this is really expensive. This is like a really expensive step that something you don't want to do yourself you want to maybe download this GPT and then you can on your target data set to the fine tuning. And then really what was really novel about GPT two and GPT three is that they actually removed this fine tuning most language transformers they use the fine tuning stuff that I talked about what GPT two and GPT three are doing though is they tried something new. They were so ambitious that they kind of removed this fine tuning, and they only have a model now where you provide what you want to do as input. So for example, if you want to use their model the GPT two model to generate text, what you do is you give it the input that is formatted as follows where you say to the model translate to French. And then you insert your English text, and then you insert your French text and it will automatically figure out what you want to do basically. There are different questions like you give it a sentence and you can ask something like this is object. So you can say for example, this is a very nice sky and is the sky blue and it will what can generate like output that says yes or no or something like that. So it's kind of very flexible in terms of what it can do by only having a certain formatted input without any fine tuning. So in version three. I think they kind of went back a little bit because I think. So here, this is called in this context on zero shot learning where they don't have any examples of the task the example is in the scriptures in the task itself they don't have any training examples in that sense. The task was maybe a little bit too ambitious. So they went back with GPT three made it a little bit bigger, and they switched also to few short learning in this context where in the future learning. They have at least a few examples of that task. So just to illustrate how that looks like so here is this zero shot where you have again the prompt or task description sorry, and then a prompt, and then it would insert here the French word for cheese. I honestly don't know any French so I can't tell you what it is, but yeah it would generate this output here. And for the one shot. It's a little bit more, I would say, easier for the model because it sees at least an example. It's not fine to it's just seeing the example as part of the input. So you have again the task description here at the top. And then you have one example of what you want to do. And then few short learning so if you have let's say here on the three shot case where you have one example second example the third example, it becomes easier for the model. If you showed at least a few examples. See that's just a note in the chat. I will maybe answer the questions after the talk and think almost over time with your slides left. Yeah, so here. Well, where was I so you're here on the limitation of the future tasks though is that you have a limited size input. I honestly don't remember what the sizes for GPT three I think they have some IP eyes that I've never used. But let's say for bird models there's a token input limitation of 512 tokens. It's like characters or work and characters but like words or punctuation. So that way, you're kind of limited by what the model can process as input and how many examples you provide. So this is actually a fine but I can imagine it's maybe a challenge if you have longer sentences here so that wouldn't probably work with very long sentences because then you fill up all the input with examples. So this is in contrast to the fine tuning approach with which most other transformers use where you have an example as part of your training set that then you updated with back propagation like the gradient update showed another example and so forth. So this is more like the traditional way of doing it. And yeah the GPT three models very ambitious and I think I'm not sure if actually the paper is out yet but yeah this is one of the latest state of the art models for generating texts. And another approach is the bird model, which is bidirectional encoder. So you can think of GPT more as the decoder which generates some output whereas bird is more like the encoder of the transformer which ingests like the input, and then creates a representation that you can then use to train a class to fire on. So GPT is better for generating texts bird is better for classification. So how that works is they have a pre training step or they have actually two different types of pre training tasks. One is on this masked language model. So what they do is they mask 15% of the words. I like to call it marked, not masked yet, because what they do is they take these 15 words or 15% of the words if you have a sentence, and then they do different things to it so here's an example. So if you have a sentence here and input sentence, you would randomly pick 15% of the words for example that you would pick, let's say Fox, and then based on these 15% what you do is 80% of the time, you replace this token, the fox with a mask. And 10% of the time, you replace it with a random word, for example, coffee, and 10% of the time you keep it unchanged. And so this is how you pre train the model you have all these sentences and 15% of the words are changed in a certain way and the model has to predict what the correct word is at a certain position. For example, it has to fill in the right word if there's a mask, or it has to detect whether this word is right or wrong. And then, sometimes it's unchanged so why would you have this unchanged. So this is kind of important for the model in order to perform well during inference when you train the model and you want to use it in the real world, the real world, you usually don't have masked sentences. But also then basically learns that sometimes it should not do anything so in order to work well or some real text which doesn't have masks, otherwise it would always expect okay there are some random words or masked words so in this way they found that this performs better if they have also 10% of the or marked words unchanged. And the second pre training tasks that they do at the same time is next sentence prediction so they have a sentence a and a sentence be separated by a token, and the model has to predict whether sentence be indeed follows sentence a. So if you have a text and you reverse the order of the sentences the model learns the correct order and this helps the model yet to work with inputs that are more than one sentence. So it's essentially classification a binary classification is next or is not next essentially. The classification token is used as a placeholder for generating the output, because the number of inputs also matches the number of outputs, but in bird we are not interested in generating new text, we are essentially interested in classification. So how that works is just a brief overview of the different types of tasks you can do with bird. There is a sentence pair classification. One is just a single sentence classification question answering and single sentence tagging so here tagging is, for example, if you think about something like language, a grammar software you can label a sentence by on what type of like a word or word or grammar each word corresponds to, but let's focus maybe only here on the, because we have for the interest of time on the single sentence classification task. So you provided an input sentence, you have some embedding here, and then you provide some output. So you have during training you have all these tokens so it has to predict what the input is, given that something is masked or not. If you have a prediction task you don't I mean it still generates a sentence but you don't do anything with it. What you care about is this class table here so this is why you have this classification token. This goes then into the class table that you care about. And this is something we can leverage for arbitrary classification. So I prepared a code example for fine tuning such a bird model for this single sentence classification in pytorch. And it says single sentence classification and for the code example. I have this large movie review data set. I mean, back then it was large but it's just a 50,000 movie reviews, and I think 50 25,000 in the training and 25,000 in test set. And this is essentially movies movie reviews from the IMDB movie review database, and these are multiple sentences. So we can also use bird for multiple sentences, even though let's say it says single sentence classification, but just concatenating these sentences. So in that way, the only difference is we don't use this separator token because the separator token is really more for comparing two things like this sentence pair classification so this one can for example say whether these sentences are similar or not like some sentence similarity and things like that. Yeah, so I'm not sure how much time we have I could maybe briefly went way over. I can maybe briefly show you the code example. It's actually not too complicated. I have it on GitHub. I have some annotation. It's based on a library called hugging face. It's a very, very, very popular library for or you probably can't see it now have to reshare my screen. Okay. Let me see how many slides I have left. So we probably don't have to cover this. Let me share my browser screen so you can briefly see the code example. Here I opened it from GitHub. I've also my Jupyter left version which is maybe better because I can zoom in here. So here is a notebook for downloading a pre trained bird model and fine tuning it on the movie review classification. There's a lot of boilerplate code here like importing the libraries, some settings for reproducible reproducibility, downloading the data set. And then processing it. So here's a pen us data frame of how the data set looks like there's like a text a movie review, and then it's whether it's positive or negative, like a binary classification, 50,000 combined, and here I'm just spitting them into training and validation and test sets. So here's a tokenization step. So tokenization goes from taking the words and then converting them also, sorry, taking the sentence and chopping it into words, and then also taking care of punctuation so punctuation is also, I think, given token, but it really depends on what type of tokenizer you use. So there's a collection of different tokenizers out there they all behave a little bit differently, but I recommend if you use, let's say, on hugging face. I recommend on finding a tokenizer that matches your model. I should say we are using here, not birth, but the silver I think I had a note about that at the top. This is essentially a smaller version of work because if you only have let's say one GPU like 1080 TI or 2080 TI like a graphics card with only 11 gigabyte memory, you or I couldn't at least log the whole work model into memory. I think I could just barely load it but they're not training. So there's a version called the silver which is a little bit smaller so in the silver what they did is they trained the bird model, and then they removed some weights, but preserving its performance it has 95% of the performance of work, but it's more lightweight it has 40% fewer parameters. So, we are using a tokenizer that is made for this, the still bird model, and then encode the data set into tokens. And then I have my data loader here in pytorch. So this is a custom data loader here there's nothing really special about it. If you would build the data loader for any type of image data set for example, except that we on the way we process essentially on encodings. So that's like a special on representation that is, I think it's kind of like a dictionary it's our own Python. So yeah a Python object or class, but it kind of behaves like a dictionary you can index into that. And I will show you how that looks like. So yeah here's an example you can have a. So this is this sorry this will create a dictionary right, but I think itself it's also some certain object that contains multiple things. Yeah, so here I'm just creating data loaders from the data set. So this is basically all just set up. And here's the main part where we are loading the still bird model, and they have multiple models on their website if you go to the hugging face transformer website. And here this would be for example a distilled bird model for sentence or sequence classification you can basically tell based on the class name, what this is made for. And then I'm loading it the pre trained version and they have also again, I think they have different ones uncased you would mean it would ignore sentence case whether it's upper or lower case. Yeah, and then I'm putting the model on my device, putting it into training mode and then initializing an optimizer for back propagation. This is just a warning it's. I don't exactly know what they want here. I think it's just some hints. It's nothing really we have to be concerned about. And here I implemented the accuracy function to add so what I'm showing you is like a manual training how basically how you would implement this and let's say your own pytorch code. And then at the end I will also show you trainer class I mean we are almost done it's just the last little bit here. So what I have here in this accuracy function how I write my accuracy function is usually that I law the data in batches because I usually have large data sets and you can just put the whole data set into the model as input because you would have a large matrix multiplication and then you run out of GPU memory. So what I usually do is I bet like similar to training I batch up my data set into chunks into these batches. It's automatically done by the data loader here, and then I compute the predicted labels and collect them. So I'm just collecting the number of correct predictions. So I have summed up all the correct predictions. I divide by the number of training examples. So I keep track of the number of examples, and use that to divide the number of correct predictions and that gives me the accuracy. So it's in that way, not requiring to load all the data all at once. And yeah there are some interesting things here going on so we have the input IDs of the works. The tension mask. So the tension mask is sort of weird here why do we need a tension mask. So it's essentially for padding so if we have sentences of different lengths. It will use zeros to do a padding so it's essentially denoting which character is a padding character, and which is an actual word. So we have our class labels, and we provide as a model input, the input IDs, the words. So the model itself will then do the word embedding, and then the attention mask. And then we obtain from the outputs the output is kind of I think it's an object but you can index into it might be a dictionary. So you get the logits, which you can then use. You could use a softmax function to get probabilities but it's not necessary, because the largest largest. And also the largest probability in softmax. So you get the class table with arc max. It's your class table and then you know I was just checking whether the predicted label is matching the correct label, the actual label. Okay, um, this is just how I compute the accuracy. This is my training group. So I usually have a very simple training group because when I do research I like to tinker with things so I like to have like a more manual approach. And this works similarly I obtained my input IDs from the data loader, if my attention mask labels, compute the outputs here in addition to, to the logits I also need the loss for optimization so you have to provide also to the class labels as the model input. So you get both the logits and the loss, and then like in regular pytorch I use backward on the loss and optimize and this is just for logging purposes. So when I do that on my data set it trained for like an hour, and get gets like a 91% accuracy. One interesting thing is about that briefly I tried an RNN on that and I think I got like 88% accuracy. I was using this pre trained model, even though it's a small data set the MDB review data set even though it's small, I can use a pre trained transformer to fine tune it on the small data set to get both formats that is actually better than let's say logistic regression which was like 85% and an RNN which was like 88%. And I didn't do any tuning here by the way I just used it one time because I was short on time I didn't do any learning rate tuning nothing so it worked kind of like out of the box. And then there's also, if you really want to use transformer seriously I recommend using on the APIs that hugging face on provides so they have a trainer class, where you specify options in the so called trainer arguments. And then you have this trainer class where you provide this as input and it handles everything automatically. So I also have a run here, and actually it got on even better performance I think they have some certain default parameters for regularization and so forth. So just first of all faster, and also the performance was a better I think they have better better on default options because my training was very minimal. So they have some additional options. And actually, I, for comparison. There's a setting on, I only used one GPU, when I was disabling that one of my computers where I ran this had a for GPS was running in 16 minutes. So, in practice you may be also want to disable that and you get also way faster training. Okay, so just briefly. Sorry, Sebastian, we have those minutes to finish please. Yeah, let's. Sorry, I was going way over time. So yeah, maybe let's go to the yeah we will take some questions. Yes, okay, can start asking the questions for you. So your last thing is, I will just go to my slides for the questions also. Okay, that's not the right one. But I am ready for questions now. So I wanted to say I have also way more detailed transformer lecture on YouTube from my deep learning class and there will be a version of the Python machine learning book coming out for pytorch, which also has a transformer chapter that will be later this year. So if you want to read more details about this also this might be a useful resource. So I'm also I wanted to mention this the figures here. They are from that chapter so I want to give also credit to Jitian Joe who helped me a lot with the figures and yeah writing this chapter. Okay, sorry. So I think we can take questions now. No worries hasn't you can start. Yeah, thank you so much. Thank you so much for your time and for this wonderful talk. Now it's your time but if you have any questions, and you would like, you'd like to ask it to the first question. Feel free to write it in the chat, or even you can raise your hand and we'll unmute you so you can ask your question. So let's start with our first question for today. This is from Sunny. She's asking, can you be to do classification. I think so yeah the definitely the first version I think the other versions can do classification to on the hugging way web, sorry hugging face website they have, I think also an implementation of GPT a re implementation, where you can try this out. I remember I have demonstrated this action spring semester in the class. So they definitely have capabilities to do classification and GPT two and three, you have to provide it by other context, but in GPT version one. They also provide classification as an example in the paper I'm not sure if I had a figure on this here, but yeah you can, but people just because the way it's trained in this unit directional way. Usually, people agree that a bird type of models would be better for classification, although you can use GPT two for classification. Another question from Marietta Barata. Is there any use cases right now with transformers based vision models out, out before various models like those based on this net. And should I use transformers architecture based models now to classify images and to base with net. Yeah, that's a good question so I, um, yeah, I have seen a lot of papers recently using transformers for computer vision. There's the classic vision transformer and they're also newer models. And I think they are depends on which paper you read, they are set up in the art on compared to computer vision models. I think the efficient net version three paper that came out in April on show that they perform better than computer than transformers but this was in April so things might be different now. But one downside of the of transformers and computer vision is that they require more data for pre training. If you can get the pre trained models it might be for the fine tuning part it might be worthwhile. Otherwise, otherwise, I think it's more like a research topic now that people are exploring. In practice, it's probably very expensive to train them. I think there's also this paper. I'm not sure if they compared directly. So efficient at version three was better than the vision transformer and there's the halo net paper that came out in June, it's like the rest of 50 like architecture with attention. It's not a vision transform but it has like resonate like backbone with attention. And I think this one ought to form the vision at version three on image net I think they got like 85 point something performance. So in that way, I think, yeah, it's right now really where transformers are. It's not a state of the art, but I think it's still worthwhile using CNN for classification for the time being, because they're also cheaper to train. And then of course we also have architectures like MLP mixer which is neither of them it's just multi layer perceptrons. So yeah, it's an interesting time I interesting question I, I would say personally I stick with CNN because they are easier to train, but in the future who knows, I think maybe computer vision transformers will take over completely. Great. I read about is also asking is transform architecture practical for real world tasks outside of his academia and big tech companies. And what are the next steps. Oh, sorry. What are the next steps to make transform architecture based models more accessible with transformers to be able to adapt or something more automatic will replace it. Yeah, good question I think that is one of the big frustrations that these transform models are so big that as private personal and academia it's really hard to train them. I think on some labs in academia have flat resources for example this this paper here on I think the preprint was I saw the preprint I think from this for this paper like in 2019. Even two years ago they had the resources for training but it must have been really expensive I think they're like 50 GPUs or even 50 GPU TPU pots. I think they collaborated with big tech companies but yeah, how do you as a normal researcher use these I think. Yeah, I think it's really infeasible to use these are train pre train these models as a private person I mean you can probably manage but you probably need a whole team of engineers to even set up all the infrastructure for this work. And in that way I find it more interesting to really fine tune these models like training or using a pre trained model, and then just focusing on fine tuning and like you've seen in this code example I showed you it's like in another one hour you can find but yeah there are also approaches where people have made transformers way more efficient. I haven't covered them here because of your time constraints but as for example, the nice trim former or my head the sparse transformer, which I think they have usually you have quadratic complexity they have linear complexity scaling in terms of the input sequence size. They have made efforts to make transformers more efficient. Well, if I also had a slide even on that here. There are efforts. So this is unfortunately kept at or clipped 2019 here, but you can see there's this trend that they become bigger and bigger and bigger, but there are also some efforts, for example, the distill bird that I showed you and other methods that try to reduce the number of operators and allow people to use transformers even though they may not have access to large computer infrastructure. But yeah it's like a concern, many people are concerned about. There was also this paper calculating the cost. If you have a 1.5 billion parameter model it costs like 80 to 1000 to 1.6 million just to train this model. So yeah, it's, we will see where things go but yeah this is definitely a concern that this is really unfeasible for many people. We have another question from Jason. Can GPT to GPT to be fine tuned on some more datasets like few hundreds. Yeah, I think so. Personally, I have only used GPT to via the hugging face website where they had like derivative of GPT to I don't, I forgot the name I think it's called GPT meal. So this GPT meal is like an effort by the open source community to train a GPT to model because I don't think, I think maybe has changed but I don't think they. I have shared the full model I think they only provide access through an API. So that way if you only have access to an API I don't know if you can find unit, but there's this GPT meal project. I would have to search the website I don't know my head by a thing it's like this, where they have reengineered either GPT version two or three and I think this might be something that you could download and fine tune. And I think hugging face also has maybe some models I would have to double check. I haven't done this myself, but I think it should be possible. So for GPT two and three it's kind of not necessary because you have like these contexts so they say with the context it's sufficient but you could maybe fine tune it I, I don't know for sure. It's a good question. Great. I'm wondering if you can use between for specific domains, like, for example, medicine. Yeah, definitely. So this is, yeah, yeah, so you can definitely adopt the architecture for specific domains. Anyway, if you have a small medical image data set like texts you could technically fine tune this, but if you have a large data set you can also maybe try to train it from scratch, like the researchers did here. So here instead of training it on sentences they use the BERT model. I think this group, there were two groups that did something similar. They used the BERT model and trained it on amino acid sequences where they had millions of sequences from the protein structure database. They were like, I mean you can think of an amino acid sequence as a sentence, but each character there are 20 different amino acids, each character would represent a word essentially because it's on. It's more like a character level type thing, but they did that and it worked well so I can imagine you can also train it on other types of sequences and in the medical domain depends on that you have a sequence of specific domain specific encoding that say you have a certain device that outputs certain values that don't save a meaning to a human, but that could be passed by a machine. I can also think of a transformer maybe being trained on that or if you have just texts like annotations of patient records I think that could also be done. So yeah, you can definitely train this on other things other than general text. Altie is also asking, when we fine-tune the BERT train, will that update the vocab of any initial vocab will be the same after fine-tuning? I think a good question so when you fine-tune it, so you have the text embedding, I think it would update also the embedding step. This is something I can't say 100% sure because when you use this code from HuggingFace, the code from HuggingFace includes the encoding as far as I know because you only provide the input IDs of the words and then coding is part of the model. And if you update this, I think you should also, it should also update the embeddings. I'm like 90% sure but not 100% because I haven't really looked at the code line by line, maybe they have a line that freezes the weights for the embeddings so that I don't know for sure but I suspect the embeddings are also updated. Sorry, I think the question might have been about the vocabulary size. So I think the vocabulary size is fixed so I don't think this can be updated because the vocabulary size depends on the input ID on the tokenizer. I think the tokenizer would remove words from your input text that is not included in the pre-training. So if you have some arbitrary words that are not in the database where the model was pre-trained on, I think you don't have to worry about it if you use the right tokenizer because it would replace it by an arbitrary token. And of course also the model wouldn't then be able to perform well on if you have a lot of different tokens because these are tokens the model has never seen before and they are all, I think, converted to arbitrary unknown token or something like that. Okay, nice. Also, do you want to say anything? You can unmute yourself and ask any questions. Can you hear me? Yes, sure. Thank you. Sorry for insisting on this question, but I just wanted to return a bit to this point, to the last one, to the vocabulary one. My idea is that of a fast AI library in which they have the NLP part of it together with the image. And actually, Jeremy Howard has made this fast AI library in a way that when you fine tune the pre-trained model to a specific domain, then all the words that are not tokens, but part of the original vocabulary of the pre-trained model will be added to the new vocabulary. And that will be, they will train by starting from scratch, let's say, no, as you do with the pre-training, but with this. But I'm not sure that this is the case for Bert, to be honest, because I've been asking the guys in some other talks like this. And as far as I know, they don't do that. And that is the original question, like, how well does perform Bert pre-trained, let's say pre-trained Bert, when we fine tune it, I don't know, in medicine, when most of the characters, I don't know, in whatever other domain specific task, when most of the words or tokens, sorry, tokens might be quite different, and they will be just unknown, no? It's like technical question, sorry about that. Yeah, partly you answered, thank you, but just like it's not about the size of the vocabulary, but like exactly in this thing, thank you so much. Yeah, so yeah, I think that's a good point, I think they are kind of related. So I think in the Bert model, they basically have a fixed vocabulary where all the words appear that appeared during pre-training plus an unknown token. And then if you have to in fine-tuning the new words, it will just be mapped to this unknown token so they don't expand the vocabulary, right? That's what I'm suspecting. And you mentioned that fast AI, they would add it to vocabulary. Yeah, I haven't really worked with the fast AI version of this, so that sounds like a good approach right now if you then have some really, if you don't want to maybe pre-trained completely from scratch, it's a very different data set with different tokens, so you kind of, it's kind of more flexible in that way, I would say. Yeah, that makes sense, yeah. Thank you very much. Thank you, Altie. Again, if you want to ask any questions, you can raise your hand and we will unmute you so we can ask questions to Dr. Spastien. In the chat, we have a question from Anna. How to handle the bottleneck caused by the data loader reading each example from memory? Yeah, the bottleneck, oh, so in terms of speed, yeah, you could increase, so what I do sometimes if it's too slow for a very large data set, what I would do is I would increase, I would make a new data loader for the training set. If I want to compute the training set accuracy, I would have a second one, which has just a larger batch size than a one I used during training and for the test set, I would just set the batch size as large as it goes until it crashes, because at some point it would just crash. In a case like this, I would say it's not really a bottleneck, it's, I mean, it takes maybe a minute or maybe a few seconds to process the whole data set in each epoch, and we are only computing in the accuracy function on, like, the performance in each epoch, so it's not like too often, I think this is reasonable, but maybe the question is also about like how we deal with that bottleneck during training. That's a good question, I am actually, yeah, that's one limitation, so I could only go up to batch size 16 here in this example, because otherwise I would run out of memory. So one way to deal with that would be using a different type of transformer, for example, so there are more efficient transformers that have fewer parameters and smaller maybe inputs sizes, smaller number of tokens that might be in possibility. But there are also different ways for distributed computing, it depends on what type of distributed computing you do some can kind of put the model onto different devices and handle with handle batches like that. So for instance, if you remove the number of GPU restriction I had in my code, it would run on multiple GPUs and I think it splits up the batch size. In this case, it I think would multiply the batch size if you have four GPUs before and then each GPU gets 16 but still you're processing 64 training examples at the same time and then you average the gradients from the different GPUs in that way, you can use larger batch sizes if you have multiple GPUs, that's another option. But yeah, that's fundamentally a limitation of matrix modification in memory, for example, if you have only access to one GPU that could be challenging. Great. Well, I studied with Anna, she's asking, how can an individual like graduate student train a transformer for data for which there is no break train transformer available. Is it even possible to resource wise. Yeah, good question. I haven't done this myself yet I'm. I think based on what I've seen from papers where people have done that for example this protein sequence paper training from scratch look very scary in terms of what resources on they had to use in order to make that happen. And if the only case is like maybe if you have a small data set, maybe that might be feasible but I think then maybe the transformer won't be the best option if you only have a small data set for pre training, then maybe using a classic RNN or even a bag of words model might be a better approach. But yeah I don't have unfortunately a good answer for that I think that's a concern I have to, if you I personally also don't have access to a large cluster where I could do that and even if I would have access to that. All the engineering efforts that go into setting this up is also kind of challenging so companies, so far as I know, have really large teams of engineers that only really focus on the coding part just to run the model on these multiple devices and GPUs. I mean there are API said make that simpler but I think in practice it's still not very easy to do that we need to have like experts doing that but maybe I'm wrong so I don't want to discourage anyone from trying this out in practice but I think there's a better answer at that moment. Back to Ritu Paratha. I have a personally point tuned a pre trained GBT to model, which has 115 million bar meters with about 2600 mid sized text on 70 lines each and talk with me 15 minutes in. Yeah, that's nice. Okay. So on a very small 3000 data point. Okay, I didn't expect it to perform that well that is actually cool. So, one at 7 million it will be the size of GPT one approximately that is impressive I, I think that that sounds very promising then. So, forget what I said in the previous answer. Maybe it is possible to really pretend train them on small data sets so yeah. A question from Sujana. Why is there a norm used and not bad snow. That is a good question why is layer norm used and not bedroom. I think it has something to do with the fact that we have a sequence. We normalize across the sequence and not across across the batches by top of my head, I don't have a good answer I could. I think I was thinking about that at some point but I forgot I think it's the way we normalize over over the batches in a way. That's a good question. Another thing is batch not doesn't really perform well. If you have batch sizes smaller than 32 or 16, but I don't think this is the answer I think the answer is mobile. The way we normalize this I would have to think about this again why they're not not batch norm. Good question. Anyway, I think this concludes the Q&A session for this talk. Thank you so much, Dr. Sebastian. Back to you. Thank you, thank you Dr. Sebastian. Sebastian, can you share the slides or the code that you show us on the chat so we can post it on the Twitter. Yep. I have it right away. I have this GitHub repository here. And the GitHub repository has a link to the slides. I can also post it separately. It's right here. Thank you. Great so we definitely want to give a huge thank you for you Sebastian. This was super insightful and hopeful. Thank you everyone for joining. And please follow us on Twitter for more details about upcoming talks and see you soon. Thank you. Yeah, thank you so much everyone for attending and also very good questions. Sorry for rushing a little bit about through the topics I don't know it's like I always want to talk about so many things and there's only so little time, but I appreciate your patience and joining me here today that was fun and I really liked your questions that was cool. Yeah, then have a great rest of the day everyone. Bye bye. Bye bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.0, "text": " Welcome and good evening everyone. My name is Atman, one of the volunteers of the BiData Jiddah chapter.", "tokens": [50364, 4027, 293, 665, 5634, 1518, 13, 1222, 1315, 307, 1711, 1601, 11, 472, 295, 264, 14352, 295, 264, 13007, 35, 3274, 508, 14273, 545, 7187, 13, 50764], "temperature": 0.0, "avg_logprob": -0.28132132645491714, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.15903812646865845}, {"id": 1, "seek": 0, "start": 8.0, "end": 16.0, "text": " I'm here with my team members Yasmin and Hassan Shadad and Abdelillah.", "tokens": [50764, 286, 478, 510, 365, 452, 1469, 2679, 30557, 2367, 293, 32711, 282, 1160, 345, 345, 293, 2847, 18105, 32703, 13, 51164], "temperature": 0.0, "avg_logprob": -0.28132132645491714, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.15903812646865845}, {"id": 2, "seek": 0, "start": 16.0, "end": 22.0, "text": " Today's meetup is titled Transformers from the Ground Up by Dr. Sebastian Rashka.", "tokens": [51164, 2692, 311, 1677, 1010, 307, 19841, 27938, 433, 490, 264, 28371, 5858, 538, 2491, 13, 31102, 46298, 2330, 13, 51464], "temperature": 0.0, "avg_logprob": -0.28132132645491714, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.15903812646865845}, {"id": 3, "seek": 0, "start": 22.0, "end": 28.0, "text": " Dr. Sebastian is an assistant professor of statistics at the University of Wisconsin Medicine,", "tokens": [51464, 2491, 13, 31102, 307, 364, 10994, 8304, 295, 12523, 412, 264, 3535, 295, 17977, 20338, 11, 51764], "temperature": 0.0, "avg_logprob": -0.28132132645491714, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.15903812646865845}, {"id": 4, "seek": 2800, "start": 28.0, "end": 32.0, "text": " focusing on deep learning and machine learning research.", "tokens": [50364, 8416, 322, 2452, 2539, 293, 3479, 2539, 2132, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16986130934495192, "compression_ratio": 1.478021978021978, "no_speech_prob": 0.01436154730618}, {"id": 5, "seek": 2800, "start": 32.0, "end": 39.0, "text": " He's also a contributor to open source software and the author of the best-selling book, Python Machine Learning.", "tokens": [50564, 634, 311, 611, 257, 42859, 281, 1269, 4009, 4722, 293, 264, 3793, 295, 264, 1151, 12, 30427, 1446, 11, 15329, 22155, 15205, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16986130934495192, "compression_ratio": 1.478021978021978, "no_speech_prob": 0.01436154730618}, {"id": 6, "seek": 2800, "start": 39.0, "end": 47.0, "text": " Thank you Dr. Sebastian for joining us and also we want to thank our sponsors, Namfocus and Muzun.", "tokens": [50914, 1044, 291, 2491, 13, 31102, 337, 5549, 505, 293, 611, 321, 528, 281, 1309, 527, 22593, 11, 10684, 69, 15206, 293, 376, 3334, 409, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16986130934495192, "compression_ratio": 1.478021978021978, "no_speech_prob": 0.01436154730618}, {"id": 7, "seek": 4700, "start": 47.0, "end": 57.0, "text": " If you want to visit our BiData Jiddah website or follow the Twitter account to be updated about our new upcoming meetups.", "tokens": [50364, 759, 291, 528, 281, 3441, 527, 13007, 35, 3274, 508, 14273, 545, 3144, 420, 1524, 264, 5794, 2696, 281, 312, 10588, 466, 527, 777, 11500, 1677, 7528, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11434059411707059, "compression_ratio": 1.430939226519337, "no_speech_prob": 0.09439676254987717}, {"id": 8, "seek": 4700, "start": 57.0, "end": 63.0, "text": " And we also post a lot of cool things on there.", "tokens": [50864, 400, 321, 611, 2183, 257, 688, 295, 1627, 721, 322, 456, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11434059411707059, "compression_ratio": 1.430939226519337, "no_speech_prob": 0.09439676254987717}, {"id": 9, "seek": 4700, "start": 63.0, "end": 69.0, "text": " And I guess that's all from me. I'll stop sharing so Dr. Sebastian can start. Thank you.", "tokens": [51164, 400, 286, 2041, 300, 311, 439, 490, 385, 13, 286, 603, 1590, 5414, 370, 2491, 13, 31102, 393, 722, 13, 1044, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11434059411707059, "compression_ratio": 1.430939226519337, "no_speech_prob": 0.09439676254987717}, {"id": 10, "seek": 6900, "start": 69.0, "end": 75.0, "text": " Yes, thank you so much for the kind introduction. I really appreciate it. It's my pleasure to be here. Thanks for the invitation.", "tokens": [50364, 1079, 11, 1309, 291, 370, 709, 337, 264, 733, 9339, 13, 286, 534, 4449, 309, 13, 467, 311, 452, 6834, 281, 312, 510, 13, 2561, 337, 264, 17890, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10883989005253233, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.27531692385673523}, {"id": 11, "seek": 6900, "start": 75.0, "end": 81.0, "text": " So I will start then by sharing my screen and if there's some issue, please let me know.", "tokens": [50664, 407, 286, 486, 722, 550, 538, 5414, 452, 2568, 293, 498, 456, 311, 512, 2734, 11, 1767, 718, 385, 458, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10883989005253233, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.27531692385673523}, {"id": 12, "seek": 6900, "start": 81.0, "end": 88.0, "text": " I can then try to fix that so it should be visible now. Can you see everything?", "tokens": [50964, 286, 393, 550, 853, 281, 3191, 300, 370, 309, 820, 312, 8974, 586, 13, 1664, 291, 536, 1203, 30, 51314], "temperature": 0.0, "avg_logprob": -0.10883989005253233, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.27531692385673523}, {"id": 13, "seek": 6900, "start": 88.0, "end": 90.0, "text": " Yes, it's clear. Yeah.", "tokens": [51314, 1079, 11, 309, 311, 1850, 13, 865, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10883989005253233, "compression_ratio": 1.5070422535211268, "no_speech_prob": 0.27531692385673523}, {"id": 14, "seek": 9000, "start": 90.0, "end": 101.0, "text": " Oh, perfect. Great. Okay. Yeah, then yeah, I will get started. So today, the talk is about transformers, the natural language processing models in deep learning.", "tokens": [50364, 876, 11, 2176, 13, 3769, 13, 1033, 13, 865, 11, 550, 1338, 11, 286, 486, 483, 1409, 13, 407, 965, 11, 264, 751, 307, 466, 4088, 433, 11, 264, 3303, 2856, 9007, 5245, 294, 2452, 2539, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1187203656072202, "compression_ratio": 1.518987341772152, "no_speech_prob": 0.08867046236991882}, {"id": 15, "seek": 9000, "start": 101.0, "end": 107.0, "text": " And I'm trying to cover transformers from the ground up, like explaining a little bit how they work.", "tokens": [50914, 400, 286, 478, 1382, 281, 2060, 4088, 433, 490, 264, 2727, 493, 11, 411, 13468, 257, 707, 857, 577, 436, 589, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1187203656072202, "compression_ratio": 1.518987341772152, "no_speech_prob": 0.08867046236991882}, {"id": 16, "seek": 9000, "start": 107.0, "end": 113.0, "text": " And then also, I hope we have time for that at the end, showing you an implementation in PyTorch.", "tokens": [51214, 400, 550, 611, 11, 286, 1454, 321, 362, 565, 337, 300, 412, 264, 917, 11, 4099, 291, 364, 11420, 294, 9953, 51, 284, 339, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1187203656072202, "compression_ratio": 1.518987341772152, "no_speech_prob": 0.08867046236991882}, {"id": 17, "seek": 11300, "start": 113.0, "end": 122.0, "text": " So yeah, why this topic? So yeah, I personally got very interested in this topic recently and I was also working on the new edition for my Python machine learning book.", "tokens": [50364, 407, 1338, 11, 983, 341, 4829, 30, 407, 1338, 11, 286, 5665, 658, 588, 3102, 294, 341, 4829, 3938, 293, 286, 390, 611, 1364, 322, 264, 777, 11377, 337, 452, 15329, 3479, 2539, 1446, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12149274458578967, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.025532348081469536}, {"id": 18, "seek": 11300, "start": 122.0, "end": 130.0, "text": " It was just putting together the chapter and had this very long lecture at my end of the semester last, last semester, when I was teaching the deep learning class.", "tokens": [50814, 467, 390, 445, 3372, 1214, 264, 7187, 293, 632, 341, 588, 938, 7991, 412, 452, 917, 295, 264, 11894, 1036, 11, 1036, 11894, 11, 562, 286, 390, 4571, 264, 2452, 2539, 1508, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12149274458578967, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.025532348081469536}, {"id": 19, "seek": 11300, "start": 130.0, "end": 141.0, "text": " And this is essentially more like a condensed version of that highlighting the main concepts by skipping over the mathematical details, which I think would take too much time to digest.", "tokens": [51214, 400, 341, 307, 4476, 544, 411, 257, 36398, 3037, 295, 300, 26551, 264, 2135, 10392, 538, 31533, 670, 264, 18894, 4365, 11, 597, 286, 519, 576, 747, 886, 709, 565, 281, 13884, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12149274458578967, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.025532348081469536}, {"id": 20, "seek": 14100, "start": 141.0, "end": 154.0, "text": " It's not like a week long effort to dig through all the details, but I hope this will give you like a good idea of how transformers work, and also helping you like navigating the jungle of all the terminology a little bit because there are so many different models", "tokens": [50364, 467, 311, 406, 411, 257, 1243, 938, 4630, 281, 2528, 807, 439, 264, 4365, 11, 457, 286, 1454, 341, 486, 976, 291, 411, 257, 665, 1558, 295, 577, 4088, 433, 589, 11, 293, 611, 4315, 291, 411, 32054, 264, 18228, 295, 439, 264, 27575, 257, 707, 857, 570, 456, 366, 370, 867, 819, 5245, 51014], "temperature": 0.0, "avg_logprob": -0.15412155954461348, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.04598568379878998}, {"id": 21, "seek": 14100, "start": 154.0, "end": 163.0, "text": " out there like BERT, GPT version one, GPT version two, and yeah, you might wonder how they are different and what you can do with transformers.", "tokens": [51014, 484, 456, 411, 363, 31479, 11, 26039, 51, 3037, 472, 11, 26039, 51, 3037, 732, 11, 293, 1338, 11, 291, 1062, 2441, 577, 436, 366, 819, 293, 437, 291, 393, 360, 365, 4088, 433, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15412155954461348, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.04598568379878998}, {"id": 22, "seek": 16300, "start": 163.0, "end": 176.0, "text": " Yeah, just to start a few examples where transformers are used nowadays. So one popular example would be language translation. That is also how the original transformer architecture was developed.", "tokens": [50364, 865, 11, 445, 281, 722, 257, 1326, 5110, 689, 4088, 433, 366, 1143, 13434, 13, 407, 472, 3743, 1365, 576, 312, 2856, 12853, 13, 663, 307, 611, 577, 264, 3380, 31782, 9482, 390, 4743, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1202622961114954, "compression_ratio": 1.7305389221556886, "no_speech_prob": 0.06944272667169571}, {"id": 23, "seek": 16300, "start": 176.0, "end": 181.0, "text": " So language translation was the main motivation behind developing the original architecture.", "tokens": [51014, 407, 2856, 12853, 390, 264, 2135, 12335, 2261, 6416, 264, 3380, 9482, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1202622961114954, "compression_ratio": 1.7305389221556886, "no_speech_prob": 0.06944272667169571}, {"id": 24, "seek": 18100, "start": 181.0, "end": 191.0, "text": " And yeah, nowadays, transformers are pretty much everywhere when it comes to natural language processing. And even beyond that, there are many real-world applications of transformers.", "tokens": [50364, 400, 1338, 11, 13434, 11, 4088, 433, 366, 1238, 709, 5315, 562, 309, 1487, 281, 3303, 2856, 9007, 13, 400, 754, 4399, 300, 11, 456, 366, 867, 957, 12, 13217, 5821, 295, 4088, 433, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12352303571479264, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.021931419149041176}, {"id": 25, "seek": 18100, "start": 191.0, "end": 208.0, "text": " For example, recently I also saw this article. So I'm also working in computational biology problems. Recently there was this article on using transformers to kind of elucidate or decipher properties and proteins like organizing proteins by", "tokens": [50864, 1171, 1365, 11, 3938, 286, 611, 1866, 341, 7222, 13, 407, 286, 478, 611, 1364, 294, 28270, 14956, 2740, 13, 20072, 456, 390, 341, 7222, 322, 1228, 4088, 433, 281, 733, 295, 806, 1311, 327, 473, 420, 49859, 7221, 293, 15577, 411, 17608, 15577, 538, 51714], "temperature": 0.0, "avg_logprob": -0.12352303571479264, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.021931419149041176}, {"id": 26, "seek": 20800, "start": 208.0, "end": 222.0, "text": " structure properties, but also by function. And they trained, for instance, transformers on a large scale database of amino acid sequences like protein sequences, and they found all kinds of interesting insights from that.", "tokens": [50364, 3877, 7221, 11, 457, 611, 538, 2445, 13, 400, 436, 8895, 11, 337, 5197, 11, 4088, 433, 322, 257, 2416, 4373, 8149, 295, 24674, 8258, 22978, 411, 7944, 22978, 11, 293, 436, 1352, 439, 3685, 295, 1880, 14310, 490, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16258432673311782, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.011681818403303623}, {"id": 27, "seek": 20800, "start": 222.0, "end": 235.0, "text": " And yeah, you probably have seen recently the GitHub co-pilot project. I think this was in collaboration with OpenAI and I think also Microsoft, where they trained a transformer to generate code.", "tokens": [51064, 400, 1338, 11, 291, 1391, 362, 1612, 3938, 264, 23331, 598, 12, 79, 31516, 1716, 13, 286, 519, 341, 390, 294, 9363, 365, 7238, 48698, 293, 286, 519, 611, 8116, 11, 689, 436, 8895, 257, 31782, 281, 8460, 3089, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16258432673311782, "compression_ratio": 1.613899613899614, "no_speech_prob": 0.011681818403303623}, {"id": 28, "seek": 23500, "start": 235.0, "end": 247.0, "text": " What's interesting is that you can essentially enter a query, a comment, a code comment, and this AI system, the model will generate a corresponding code here at the bottom.", "tokens": [50364, 708, 311, 1880, 307, 300, 291, 393, 4476, 3242, 257, 14581, 11, 257, 2871, 11, 257, 3089, 2871, 11, 293, 341, 7318, 1185, 11, 264, 2316, 486, 8460, 257, 11760, 3089, 510, 412, 264, 2767, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11777562773629521, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.025937087833881378}, {"id": 29, "seek": 23500, "start": 247.0, "end": 259.0, "text": " So if you go to this website, they have like a nice animation of how that works, and they have a Microsoft Visual Studio Code plug-in where you can download this and actually use that in action, which is really impressive.", "tokens": [50964, 407, 498, 291, 352, 281, 341, 3144, 11, 436, 362, 411, 257, 1481, 9603, 295, 577, 300, 1985, 11, 293, 436, 362, 257, 8116, 23187, 13500, 15549, 5452, 12, 259, 689, 291, 393, 5484, 341, 293, 767, 764, 300, 294, 3069, 11, 597, 307, 534, 8992, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11777562773629521, "compression_ratio": 1.6229508196721312, "no_speech_prob": 0.025937087833881378}, {"id": 30, "seek": 25900, "start": 259.0, "end": 275.0, "text": " So I think right now they have JavaScript support and Python support, so that would be another interesting application of transformers. So transformers mainly are about generating things like generating translations or generating code, generating text,", "tokens": [50364, 407, 286, 519, 558, 586, 436, 362, 15778, 1406, 293, 15329, 1406, 11, 370, 300, 576, 312, 1071, 1880, 3861, 295, 4088, 433, 13, 407, 4088, 433, 8704, 366, 466, 17746, 721, 411, 17746, 37578, 420, 17746, 3089, 11, 17746, 2487, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10637446283136756, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.1310334950685501}, {"id": 31, "seek": 25900, "start": 275.0, "end": 288.0, "text": " for instance, also in chatbots, but you can also use them for classification, like classifying, let's say, whether a movie review is positive or negative, and this will be a code example we will be covering at the end of this lecture or talk.", "tokens": [51164, 337, 5197, 11, 611, 294, 5081, 65, 1971, 11, 457, 291, 393, 611, 764, 552, 337, 21538, 11, 411, 1508, 5489, 11, 718, 311, 584, 11, 1968, 257, 3169, 3131, 307, 3353, 420, 3671, 11, 293, 341, 486, 312, 257, 3089, 1365, 321, 486, 312, 10322, 412, 264, 917, 295, 341, 7991, 420, 751, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10637446283136756, "compression_ratio": 1.7678571428571428, "no_speech_prob": 0.1310334950685501}, {"id": 32, "seek": 28800, "start": 289.0, "end": 307.0, "text": " So the topics I have in mind for today are those. So first I wanted to, from the historic perspective, just briefly explain how attention came to be. So there was essentially this attention mechanism that was first added to recurrent neural networks, and then there was this", "tokens": [50414, 407, 264, 8378, 286, 362, 294, 1575, 337, 965, 366, 729, 13, 407, 700, 286, 1415, 281, 11, 490, 264, 13236, 4585, 11, 445, 10515, 2903, 577, 3202, 1361, 281, 312, 13, 407, 456, 390, 4476, 341, 3202, 7513, 300, 390, 700, 3869, 281, 18680, 1753, 18161, 9590, 11, 293, 550, 456, 390, 341, 51314], "temperature": 0.0, "avg_logprob": -0.0873554986098717, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.004826361779123545}, {"id": 33, "seek": 30700, "start": 307.0, "end": 318.0, "text": " self-attention mechanism, which is a, I would say, a more sophisticated version of that, there's this so-called scared dog product. Attention, oops, I almost ripped off my microphone here.", "tokens": [50364, 2698, 12, 1591, 1251, 7513, 11, 597, 307, 257, 11, 286, 576, 584, 11, 257, 544, 16950, 3037, 295, 300, 11, 456, 311, 341, 370, 12, 11880, 5338, 3000, 1674, 13, 31858, 11, 34166, 11, 286, 1920, 22780, 766, 452, 10952, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19621547217507965, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.037845928221940994}, {"id": 34, "seek": 30700, "start": 318.0, "end": 336.0, "text": " Yeah, so and then we will talk about the original transformer architecture, which is the first architecture that was just using attention. So in step one here, this is a recurrent neural network or commented with attention, and the transformer is then an architecture is just using attention", "tokens": [50914, 865, 11, 370, 293, 550, 321, 486, 751, 466, 264, 3380, 31782, 9482, 11, 597, 307, 264, 700, 9482, 300, 390, 445, 1228, 3202, 13, 407, 294, 1823, 472, 510, 11, 341, 307, 257, 18680, 1753, 18161, 3209, 420, 26940, 365, 3202, 11, 293, 264, 31782, 307, 550, 364, 9482, 307, 445, 1228, 3202, 51814], "temperature": 0.0, "avg_logprob": -0.19621547217507965, "compression_ratio": 1.8045112781954886, "no_speech_prob": 0.037845928221940994}, {"id": 35, "seek": 33600, "start": 336.0, "end": 347.0, "text": " without any recurrent neural network layers. And then I will briefly dive into some of these large scale language models, for example, your GPT and bird, which are pretty popular these days.", "tokens": [50364, 1553, 604, 18680, 1753, 18161, 3209, 7914, 13, 400, 550, 286, 486, 10515, 9192, 666, 512, 295, 613, 2416, 4373, 2856, 5245, 11, 337, 1365, 11, 428, 26039, 51, 293, 5255, 11, 597, 366, 1238, 3743, 613, 1708, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1114299658573035, "compression_ratio": 1.7458333333333333, "no_speech_prob": 0.005298434756696224}, {"id": 36, "seek": 33600, "start": 347.0, "end": 360.0, "text": " And then after that we will pre-train or sorry, we will fine tune a pre-trained bird model in PyTorch. So one thing about these large scale language models is that they are really large scale and will take forever to train them.", "tokens": [50914, 400, 550, 934, 300, 321, 486, 659, 12, 83, 7146, 420, 2597, 11, 321, 486, 2489, 10864, 257, 659, 12, 17227, 2001, 5255, 2316, 294, 9953, 51, 284, 339, 13, 407, 472, 551, 466, 613, 2416, 4373, 2856, 5245, 307, 300, 436, 366, 534, 2416, 4373, 293, 486, 747, 5680, 281, 3847, 552, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1114299658573035, "compression_ratio": 1.7458333333333333, "no_speech_prob": 0.005298434756696224}, {"id": 37, "seek": 36000, "start": 360.0, "end": 375.0, "text": " In practice, usually they are trained on large data sets, thousands or millions of websites and books and so forth. And so usually people download a pre-trained version, and then fine tune it to a target task.", "tokens": [50364, 682, 3124, 11, 2673, 436, 366, 8895, 322, 2416, 1412, 6352, 11, 5383, 420, 6803, 295, 12891, 293, 3642, 293, 370, 5220, 13, 400, 370, 2673, 561, 5484, 257, 659, 12, 17227, 2001, 3037, 11, 293, 550, 2489, 10864, 309, 281, 257, 3779, 5633, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1213083267211914, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.19403176009655}, {"id": 38, "seek": 36000, "start": 375.0, "end": 389.0, "text": " So we'll also take a look at how it looks like. And then I have a few closing thoughts or words on transformers. And yeah, with that, I think we have plenty of things to talk about in the next, I would say, 40 minutes.", "tokens": [51114, 407, 321, 603, 611, 747, 257, 574, 412, 577, 309, 1542, 411, 13, 400, 550, 286, 362, 257, 1326, 10377, 4598, 420, 2283, 322, 4088, 433, 13, 400, 1338, 11, 365, 300, 11, 286, 519, 321, 362, 7140, 295, 721, 281, 751, 466, 294, 264, 958, 11, 286, 576, 584, 11, 3356, 2077, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1213083267211914, "compression_ratio": 1.6150943396226416, "no_speech_prob": 0.19403176009655}, {"id": 39, "seek": 38900, "start": 389.0, "end": 405.0, "text": " Let's get started with the attention mechanism that was also first small disclaimer here. So like I mentioned in the beginning. So this will be more like a conceptual overview. So if there are some, let's say, notational details in these figures, don't worry about them too much.", "tokens": [50364, 961, 311, 483, 1409, 365, 264, 3202, 7513, 300, 390, 611, 700, 1359, 40896, 510, 13, 407, 411, 286, 2835, 294, 264, 2863, 13, 407, 341, 486, 312, 544, 411, 257, 24106, 12492, 13, 407, 498, 456, 366, 512, 11, 718, 311, 584, 11, 406, 1478, 4365, 294, 613, 9624, 11, 500, 380, 3292, 466, 552, 886, 709, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16678826771085226, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.06637898832559586}, {"id": 40, "seek": 40500, "start": 405.0, "end": 416.0, "text": " To be honest, I think it's most important to kind of understand the big picture and all the mathematical details. That is something that are almost like implementation details. Some of them are almost arbitrary.", "tokens": [50364, 1407, 312, 3245, 11, 286, 519, 309, 311, 881, 1021, 281, 733, 295, 1223, 264, 955, 3036, 293, 439, 264, 18894, 4365, 13, 663, 307, 746, 300, 366, 1920, 411, 11420, 4365, 13, 2188, 295, 552, 366, 1920, 23211, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0851720109277842, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.03562823310494423}, {"id": 41, "seek": 40500, "start": 416.0, "end": 430.0, "text": " So I wouldn't worry about it too much. But I hope this talk kind of piques your interest in terms of transformers. So if you're interested in that you can dive in deeper into the topic afterwards with having this big picture overview in mind.", "tokens": [50914, 407, 286, 2759, 380, 3292, 466, 309, 886, 709, 13, 583, 286, 1454, 341, 751, 733, 295, 280, 4911, 428, 1179, 294, 2115, 295, 4088, 433, 13, 407, 498, 291, 434, 3102, 294, 300, 291, 393, 9192, 294, 7731, 666, 264, 4829, 10543, 365, 1419, 341, 955, 3036, 12492, 294, 1575, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0851720109277842, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.03562823310494423}, {"id": 42, "seek": 43000, "start": 431.0, "end": 442.0, "text": " So starting with the first topic augmenting the recurrent networks with attention. So first, what is the motivation behind using attention why don't we just use recurrent neural networks.", "tokens": [50414, 407, 2891, 365, 264, 700, 4829, 29919, 278, 264, 18680, 1753, 9590, 365, 3202, 13, 407, 700, 11, 437, 307, 264, 12335, 2261, 1228, 3202, 983, 500, 380, 321, 445, 764, 18680, 1753, 18161, 9590, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11279545603571711, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.052526090294122696}, {"id": 43, "seek": 43000, "start": 442.0, "end": 452.0, "text": " So, if you think of a task sequence to sequence task, which is a task where you have an input sequence and you want to generate an output sequence.", "tokens": [50964, 407, 11, 498, 291, 519, 295, 257, 5633, 8310, 281, 8310, 5633, 11, 597, 307, 257, 5633, 689, 291, 362, 364, 4846, 8310, 293, 291, 528, 281, 8460, 364, 5598, 8310, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11279545603571711, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.052526090294122696}, {"id": 44, "seek": 45200, "start": 452.0, "end": 465.0, "text": " So one example of that would be a language translation, where you have an input sequence, let's say this one here in blue, in one language, let's say, German, and you want to translate this into English.", "tokens": [50364, 407, 472, 1365, 295, 300, 576, 312, 257, 2856, 12853, 11, 689, 291, 362, 364, 4846, 8310, 11, 718, 311, 584, 341, 472, 510, 294, 3344, 11, 294, 472, 2856, 11, 718, 311, 584, 11, 6521, 11, 293, 291, 528, 281, 13799, 341, 666, 3669, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14995725787415795, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.01824217289686203}, {"id": 45, "seek": 45200, "start": 465.0, "end": 479.0, "text": " So here on the output, then you have an output sequence. And what happens is if you use a, or if you design an arcade architecture for that, you usually use this encoder decoder setup in RNNs.", "tokens": [51014, 407, 510, 322, 264, 5598, 11, 550, 291, 362, 364, 5598, 8310, 13, 400, 437, 2314, 307, 498, 291, 764, 257, 11, 420, 498, 291, 1715, 364, 25664, 9482, 337, 300, 11, 291, 2673, 764, 341, 2058, 19866, 979, 19866, 8657, 294, 45702, 45, 82, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14995725787415795, "compression_ratio": 1.706896551724138, "no_speech_prob": 0.01824217289686203}, {"id": 46, "seek": 47900, "start": 479.0, "end": 494.0, "text": " So let's say that the RNN first ingests the whole input sequence. So you have input element one, two and three so these are for instance, individual works, and they are then processed into these hidden states.", "tokens": [50364, 407, 718, 311, 584, 300, 264, 45702, 45, 700, 3957, 4409, 264, 1379, 4846, 8310, 13, 407, 291, 362, 4846, 4478, 472, 11, 732, 293, 1045, 370, 613, 366, 337, 5197, 11, 2609, 1985, 11, 293, 436, 366, 550, 18846, 666, 613, 7633, 4368, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19919008138228436, "compression_ratio": 1.4513888888888888, "no_speech_prob": 0.03303655609488487}, {"id": 47, "seek": 49400, "start": 494.0, "end": 511.0, "text": " So once you have processed the whole input in this encoder so this would be essentially the encoder part, you have all the information in one, one hidden state here, and then you have the decoder part, which yeah generates the output.", "tokens": [50364, 407, 1564, 291, 362, 18846, 264, 1379, 4846, 294, 341, 2058, 19866, 370, 341, 576, 312, 4476, 264, 2058, 19866, 644, 11, 291, 362, 439, 264, 1589, 294, 472, 11, 472, 7633, 1785, 510, 11, 293, 550, 291, 362, 264, 979, 19866, 644, 11, 597, 1338, 23815, 264, 5598, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11123046168574581, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.12919031083583832}, {"id": 48, "seek": 51100, "start": 511.0, "end": 524.0, "text": " So here, why don't you, yeah, why don't you translate this one by one because here one problem would be that this hidden state has to remember all the input in one hidden state and this might be a little bit overwhelming.", "tokens": [50364, 407, 510, 11, 983, 500, 380, 291, 11, 1338, 11, 983, 500, 380, 291, 13799, 341, 472, 538, 472, 570, 510, 472, 1154, 576, 312, 300, 341, 7633, 1785, 575, 281, 1604, 439, 264, 4846, 294, 472, 7633, 1785, 293, 341, 1062, 312, 257, 707, 857, 13373, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1185489914634011, "compression_ratio": 1.8597785977859778, "no_speech_prob": 0.1400373876094818}, {"id": 49, "seek": 51100, "start": 524.0, "end": 540.0, "text": " Right, so if you have one hidden state that has to capture the whole input, there might be some information loss. And this is essentially one of the challenges of these RNNs they work kind of well if you have short sentences but the longer your sentence becomes the more challenging", "tokens": [51014, 1779, 11, 370, 498, 291, 362, 472, 7633, 1785, 300, 575, 281, 7983, 264, 1379, 4846, 11, 456, 1062, 312, 512, 1589, 4470, 13, 400, 341, 307, 4476, 472, 295, 264, 4759, 295, 613, 45702, 45, 82, 436, 589, 733, 295, 731, 498, 291, 362, 2099, 16579, 457, 264, 2854, 428, 8174, 3643, 264, 544, 7595, 51814], "temperature": 0.0, "avg_logprob": -0.1185489914634011, "compression_ratio": 1.8597785977859778, "no_speech_prob": 0.1400373876094818}, {"id": 50, "seek": 54000, "start": 540.0, "end": 550.0, "text": " the network to generate a good output because it kind of forgets what happened let's say here in the very beginning if you have a very, very long input sequence.", "tokens": [50364, 264, 3209, 281, 8460, 257, 665, 5598, 570, 309, 733, 295, 2870, 82, 437, 2011, 718, 311, 584, 510, 294, 264, 588, 2863, 498, 291, 362, 257, 588, 11, 588, 938, 4846, 8310, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1704327114044674, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.010784300044178963}, {"id": 51, "seek": 54000, "start": 550.0, "end": 557.0, "text": " So and also, yeah, why don't we just translate sentence word by word so here's just one example.", "tokens": [50864, 407, 293, 611, 11, 1338, 11, 983, 500, 380, 321, 445, 13799, 8174, 1349, 538, 1349, 370, 510, 311, 445, 472, 1365, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1704327114044674, "compression_ratio": 1.4742857142857142, "no_speech_prob": 0.010784300044178963}, {"id": 52, "seek": 55700, "start": 557.0, "end": 572.0, "text": " Yeah, because I only speak two languages, German and a little bit of English, I have a German to English translation example here. So it's just an attempt on translating this German sentence here at the top into an English sentence.", "tokens": [50364, 865, 11, 570, 286, 787, 1710, 732, 8650, 11, 6521, 293, 257, 707, 857, 295, 3669, 11, 286, 362, 257, 6521, 281, 3669, 12853, 1365, 510, 13, 407, 309, 311, 445, 364, 5217, 322, 35030, 341, 6521, 8174, 510, 412, 264, 1192, 666, 364, 3669, 8174, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1396041383930281, "compression_ratio": 1.5364238410596027, "no_speech_prob": 0.3262082636356354}, {"id": 53, "seek": 57200, "start": 572.0, "end": 587.0, "text": " So what happens if you translate this word by word, so it doesn't really work. So you have a sentence. I'm an English as a second language speaker so I'm usually also not that good at that but I can tell that this one is definitely wrong.", "tokens": [50364, 407, 437, 2314, 498, 291, 13799, 341, 1349, 538, 1349, 11, 370, 309, 1177, 380, 534, 589, 13, 407, 291, 362, 257, 8174, 13, 286, 478, 364, 3669, 382, 257, 1150, 2856, 8145, 370, 286, 478, 2673, 611, 406, 300, 665, 412, 300, 457, 286, 393, 980, 300, 341, 472, 307, 2138, 2085, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1669893429197114, "compression_ratio": 1.4691358024691359, "no_speech_prob": 0.5668683648109436}, {"id": 54, "seek": 58700, "start": 587.0, "end": 598.0, "text": " So you can see here, the sentences, can you help me, or can you me help this sentence to translate it doesn't really make sense if you just let's say take a dictionary and translate word by word.", "tokens": [50364, 407, 291, 393, 536, 510, 11, 264, 16579, 11, 393, 291, 854, 385, 11, 420, 393, 291, 385, 854, 341, 8174, 281, 13799, 309, 1177, 380, 534, 652, 2020, 498, 291, 445, 718, 311, 584, 747, 257, 25890, 293, 13799, 1349, 538, 1349, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16402127327175314, "compression_ratio": 2.0, "no_speech_prob": 0.15770377218723297}, {"id": 55, "seek": 58700, "start": 598.0, "end": 616.0, "text": " So to have a correct, I hope this is correct if you have a correct English translation, the sentence would be can you help me to translate the sentence, and you can see, there is some, yeah, some longer range dependencies there are some words here for instance those that you can just", "tokens": [50914, 407, 281, 362, 257, 3006, 11, 286, 1454, 341, 307, 3006, 498, 291, 362, 257, 3006, 3669, 12853, 11, 264, 8174, 576, 312, 393, 291, 854, 385, 281, 13799, 264, 8174, 11, 293, 291, 393, 536, 11, 456, 307, 512, 11, 1338, 11, 512, 2854, 3613, 36606, 456, 366, 512, 2283, 510, 337, 5197, 729, 300, 291, 393, 445, 51814], "temperature": 0.0, "avg_logprob": -0.16402127327175314, "compression_ratio": 2.0, "no_speech_prob": 0.15770377218723297}, {"id": 56, "seek": 61600, "start": 616.0, "end": 625.0, "text": " translate word by word, but then you have, let's say these two here, where they are kind of flipped so in order to translate this word, the help.", "tokens": [50364, 13799, 1349, 538, 1349, 11, 457, 550, 291, 362, 11, 718, 311, 584, 613, 732, 510, 11, 689, 436, 366, 733, 295, 26273, 370, 294, 1668, 281, 13799, 341, 1349, 11, 264, 854, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13649605214595795, "compression_ratio": 1.5654761904761905, "no_speech_prob": 0.03249732032418251}, {"id": 57, "seek": 61600, "start": 625.0, "end": 632.0, "text": " You have to look into the future basically so you have to find an element that is further away in the input sequence.", "tokens": [50814, 509, 362, 281, 574, 666, 264, 2027, 1936, 370, 291, 362, 281, 915, 364, 4478, 300, 307, 3052, 1314, 294, 264, 4846, 8310, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13649605214595795, "compression_ratio": 1.5654761904761905, "no_speech_prob": 0.03249732032418251}, {"id": 58, "seek": 63200, "start": 632.0, "end": 642.0, "text": " For example, if you have this word here, it goes here, you can see there's a more longer range dependency and the longer your sentences are, the longer these dependencies become.", "tokens": [50364, 1171, 1365, 11, 498, 291, 362, 341, 1349, 510, 11, 309, 1709, 510, 11, 291, 393, 536, 456, 311, 257, 544, 2854, 3613, 33621, 293, 264, 2854, 428, 16579, 366, 11, 264, 2854, 613, 36606, 1813, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10520576106177436, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.03158671781420708}, {"id": 59, "seek": 63200, "start": 642.0, "end": 653.0, "text": " So in that way, we can't really have this word by word translation attempt because it would not result in any meaningful grammatically wrong translation.", "tokens": [50864, 407, 294, 300, 636, 11, 321, 393, 380, 534, 362, 341, 1349, 538, 1349, 12853, 5217, 570, 309, 576, 406, 1874, 294, 604, 10995, 17570, 5030, 2085, 12853, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10520576106177436, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.03158671781420708}, {"id": 60, "seek": 65300, "start": 653.0, "end": 667.0, "text": " So going back, this is why we first read the whole input, because yeah we need to have knowledge about the whole input before we can attempt the translation but then again the challenge is okay, we may forget earlier hidden states.", "tokens": [50364, 407, 516, 646, 11, 341, 307, 983, 321, 700, 1401, 264, 1379, 4846, 11, 570, 1338, 321, 643, 281, 362, 3601, 466, 264, 1379, 4846, 949, 321, 393, 5217, 264, 12853, 457, 550, 797, 264, 3430, 307, 1392, 11, 321, 815, 2870, 3071, 7633, 4368, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07666976928710938, "compression_ratio": 1.519736842105263, "no_speech_prob": 0.035103939473629}, {"id": 61, "seek": 66700, "start": 667.0, "end": 679.0, "text": " So one idea to deal with this problem that we can capture these long range dependencies is this attention mechanism so this was first proposed in the context of RNNs in 2014.", "tokens": [50364, 407, 472, 1558, 281, 2028, 365, 341, 1154, 300, 321, 393, 7983, 613, 938, 3613, 36606, 307, 341, 3202, 7513, 370, 341, 390, 700, 10348, 294, 264, 4319, 295, 45702, 45, 82, 294, 8227, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08656709061728583, "compression_ratio": 1.4976076555023923, "no_speech_prob": 0.045316386967897415}, {"id": 62, "seek": 66700, "start": 679.0, "end": 688.0, "text": " So I also, I will share my slides so you if you are interested you can look up these papers and read a little bit more details about that.", "tokens": [50964, 407, 286, 611, 11, 286, 486, 2073, 452, 9788, 370, 291, 498, 291, 366, 3102, 291, 393, 574, 493, 613, 10577, 293, 1401, 257, 707, 857, 544, 4365, 466, 300, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08656709061728583, "compression_ratio": 1.4976076555023923, "no_speech_prob": 0.045316386967897415}, {"id": 63, "seek": 68800, "start": 688.0, "end": 699.0, "text": " So this is an attention mechanism that the researchers used to augment the RNN to better process the input. So here's just like a sketch in the lower left corner.", "tokens": [50364, 407, 341, 307, 364, 3202, 7513, 300, 264, 10309, 1143, 281, 29919, 264, 45702, 45, 281, 1101, 1399, 264, 4846, 13, 407, 510, 311, 445, 411, 257, 12325, 294, 264, 3126, 1411, 4538, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1600220654461835, "compression_ratio": 1.5951219512195123, "no_speech_prob": 0.25025948882102966}, {"id": 64, "seek": 68800, "start": 699.0, "end": 709.0, "text": " For instance, if we are thinking of the second step here, the hidden state might have access or at this step in general you might have access to the whole sequence.", "tokens": [50914, 1171, 5197, 11, 498, 321, 366, 1953, 295, 264, 1150, 1823, 510, 11, 264, 7633, 1785, 1062, 362, 2105, 420, 412, 341, 1823, 294, 2674, 291, 1062, 362, 2105, 281, 264, 1379, 8310, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1600220654461835, "compression_ratio": 1.5951219512195123, "no_speech_prob": 0.25025948882102966}, {"id": 65, "seek": 70900, "start": 709.0, "end": 717.0, "text": " You can also align with to denote that one input sequence might be more important than another and this is where the attention comes in.", "tokens": [50364, 509, 393, 611, 7975, 365, 281, 45708, 300, 472, 4846, 8310, 1062, 312, 544, 1021, 813, 1071, 293, 341, 307, 689, 264, 3202, 1487, 294, 13, 50764], "temperature": 0.0, "avg_logprob": -0.167459351675851, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.1538136601448059}, {"id": 66, "seek": 70900, "start": 717.0, "end": 731.0, "text": " So one step will have access to the whole input, but with a waiting term that says okay maybe at this position this term is more important than let's say this term and so forth so not every word is equally important.", "tokens": [50764, 407, 472, 1823, 486, 362, 2105, 281, 264, 1379, 4846, 11, 457, 365, 257, 3806, 1433, 300, 1619, 1392, 1310, 412, 341, 2535, 341, 1433, 307, 544, 1021, 813, 718, 311, 584, 341, 1433, 293, 370, 5220, 370, 406, 633, 1349, 307, 12309, 1021, 13, 51464], "temperature": 0.0, "avg_logprob": -0.167459351675851, "compression_ratio": 1.6889952153110048, "no_speech_prob": 0.1538136601448059}, {"id": 67, "seek": 73100, "start": 731.0, "end": 738.0, "text": " So there's also some some sort of waiting and the architecture looks approximately like that.", "tokens": [50364, 407, 456, 311, 611, 512, 512, 1333, 295, 3806, 293, 264, 9482, 1542, 10447, 411, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1488345146179199, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.01690901257097721}, {"id": 68, "seek": 73100, "start": 738.0, "end": 753.0, "text": " So there's a sketch I made after the paper. So it's just a little bit different notation but essentially what we have is we have an input sequence. So this is just individual words, one to two words, and then they have two RNNs kind of connected here.", "tokens": [50714, 407, 456, 311, 257, 12325, 286, 1027, 934, 264, 3035, 13, 407, 309, 311, 445, 257, 707, 857, 819, 24657, 457, 4476, 437, 321, 362, 307, 321, 362, 364, 4846, 8310, 13, 407, 341, 307, 445, 2609, 2283, 11, 472, 281, 732, 2283, 11, 293, 550, 436, 362, 732, 45702, 45, 82, 733, 295, 4582, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1488345146179199, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.01690901257097721}, {"id": 69, "seek": 75300, "start": 753.0, "end": 763.0, "text": " So there's one RNN. It's a bi-directional RNN. Bi-directional RNN is just a fancy word for an RNN that processes the sequence from left to right and from right to left.", "tokens": [50364, 407, 456, 311, 472, 45702, 45, 13, 467, 311, 257, 3228, 12, 18267, 41048, 45702, 45, 13, 13007, 12, 18267, 41048, 45702, 45, 307, 445, 257, 10247, 1349, 337, 364, 45702, 45, 300, 7555, 264, 8310, 490, 1411, 281, 558, 293, 490, 558, 281, 1411, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10278159757203693, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005818527191877365}, {"id": 70, "seek": 75300, "start": 763.0, "end": 771.0, "text": " It's just so that it captures, you have information from both ends. It's just increasing the amount of information you have essentially.", "tokens": [50864, 467, 311, 445, 370, 300, 309, 27986, 11, 291, 362, 1589, 490, 1293, 5314, 13, 467, 311, 445, 5662, 264, 2372, 295, 1589, 291, 362, 4476, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10278159757203693, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.005818527191877365}, {"id": 71, "seek": 77100, "start": 771.0, "end": 787.0, "text": " So the forward direction would be the regular RNN direction and then you essentially just flip the sequence and process it backwards. So this is what we denoted here so you can denote it from right to left but you can also think of it as just reversing the sentence and processing it in the regular way.", "tokens": [50364, 407, 264, 2128, 3513, 576, 312, 264, 3890, 45702, 45, 3513, 293, 550, 291, 4476, 445, 7929, 264, 8310, 293, 1399, 309, 12204, 13, 407, 341, 307, 437, 321, 1441, 23325, 510, 370, 291, 393, 45708, 309, 490, 558, 281, 1411, 457, 291, 393, 611, 519, 295, 309, 382, 445, 14582, 278, 264, 8174, 293, 9007, 309, 294, 264, 3890, 636, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10605176289876302, "compression_ratio": 1.6927374301675977, "no_speech_prob": 0.01001134142279625}, {"id": 72, "seek": 78700, "start": 787.0, "end": 800.0, "text": " Then you connect these hidden states and then you calculate the so-called attention weights. I will skip over the details of how these attention weights are computed here but I will show you how they are computed in the transformer.", "tokens": [50364, 1396, 291, 1745, 613, 7633, 4368, 293, 550, 291, 8873, 264, 370, 12, 11880, 3202, 17443, 13, 286, 486, 10023, 670, 264, 4365, 295, 577, 613, 3202, 17443, 366, 40610, 510, 457, 286, 486, 855, 291, 577, 436, 366, 40610, 294, 264, 31782, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10452569764235924, "compression_ratio": 1.8295964125560538, "no_speech_prob": 0.06949703395366669}, {"id": 73, "seek": 78700, "start": 800.0, "end": 813.0, "text": " It's a little bit different but not too different. So but the main part here is really that using these attention weights, we obtain the so-called context vector at each step.", "tokens": [51014, 467, 311, 257, 707, 857, 819, 457, 406, 886, 819, 13, 407, 457, 264, 2135, 644, 510, 307, 534, 300, 1228, 613, 3202, 17443, 11, 321, 12701, 264, 370, 12, 11880, 4319, 8062, 412, 1184, 1823, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10452569764235924, "compression_ratio": 1.8295964125560538, "no_speech_prob": 0.06949703395366669}, {"id": 74, "seek": 81300, "start": 813.0, "end": 828.0, "text": " So the context vector here is essentially a weighted version of all the inputs. So you can consider all the hidden states here and it's essentially a weighted sum. You have this attention weight, you multiply it by this state and then you just sum them up.", "tokens": [50364, 407, 264, 4319, 8062, 510, 307, 4476, 257, 32807, 3037, 295, 439, 264, 15743, 13, 407, 291, 393, 1949, 439, 264, 7633, 4368, 510, 293, 309, 311, 4476, 257, 32807, 2408, 13, 509, 362, 341, 3202, 3364, 11, 291, 12972, 309, 538, 341, 1785, 293, 550, 291, 445, 2408, 552, 493, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08047931225268871, "compression_ratio": 1.8505154639175259, "no_speech_prob": 0.0029341636691242456}, {"id": 75, "seek": 81300, "start": 828.0, "end": 834.0, "text": " And the larger the attention weight, the more the hidden state will contribute to this context vector.", "tokens": [51114, 400, 264, 4833, 264, 3202, 3364, 11, 264, 544, 264, 7633, 1785, 486, 10586, 281, 341, 4319, 8062, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08047931225268871, "compression_ratio": 1.8505154639175259, "no_speech_prob": 0.0029341636691242456}, {"id": 76, "seek": 83400, "start": 834.0, "end": 845.0, "text": " So this context vector is essentially then kind of capturing all the input information in a certain processed way and this goes into each decoder part.", "tokens": [50364, 407, 341, 4319, 8062, 307, 4476, 550, 733, 295, 23384, 439, 264, 4846, 1589, 294, 257, 1629, 18846, 636, 293, 341, 1709, 666, 1184, 979, 19866, 644, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11567356851365831, "compression_ratio": 1.5824742268041236, "no_speech_prob": 0.0010321141453459859}, {"id": 77, "seek": 83400, "start": 845.0, "end": 854.0, "text": " So if I go back a few slides, we had this decoder part here where we were generating some output and this is what we are having here at the top of the O's.", "tokens": [50914, 407, 498, 286, 352, 646, 257, 1326, 9788, 11, 321, 632, 341, 979, 19866, 644, 510, 689, 321, 645, 17746, 512, 5598, 293, 341, 307, 437, 321, 366, 1419, 510, 412, 264, 1192, 295, 264, 422, 311, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11567356851365831, "compression_ratio": 1.5824742268041236, "no_speech_prob": 0.0010321141453459859}, {"id": 78, "seek": 85400, "start": 854.0, "end": 869.0, "text": " The O's, they should just denote, let's say, the translated sentence. And each output step, what we have is we have the hidden state that goes into, let's say, the that position, the previous word, because it's also important.", "tokens": [50364, 440, 422, 311, 11, 436, 820, 445, 45708, 11, 718, 311, 584, 11, 264, 16805, 8174, 13, 400, 1184, 5598, 1823, 11, 437, 321, 362, 307, 321, 362, 264, 7633, 1785, 300, 1709, 666, 11, 718, 311, 584, 11, 264, 300, 2535, 11, 264, 3894, 1349, 11, 570, 309, 311, 611, 1021, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17081788006950827, "compression_ratio": 1.8319327731092436, "no_speech_prob": 0.10363873094320297}, {"id": 79, "seek": 85400, "start": 869.0, "end": 880.0, "text": " I mean, if you generate a sentence, you kind of look at the previous word you have just written, right? So what you currently translate the current word is also depending on the previous word you have written.", "tokens": [51114, 286, 914, 11, 498, 291, 8460, 257, 8174, 11, 291, 733, 295, 574, 412, 264, 3894, 1349, 291, 362, 445, 3720, 11, 558, 30, 407, 437, 291, 4362, 13799, 264, 2190, 1349, 307, 611, 5413, 322, 264, 3894, 1349, 291, 362, 3720, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17081788006950827, "compression_ratio": 1.8319327731092436, "no_speech_prob": 0.10363873094320297}, {"id": 80, "seek": 88000, "start": 880.0, "end": 893.0, "text": " The sentence might not make sense. So you provide the previous word, you provide the hidden state, and you provide this context vector and with that you can capture all the input when you attempt the translation of the next word.", "tokens": [50364, 440, 8174, 1062, 406, 652, 2020, 13, 407, 291, 2893, 264, 3894, 1349, 11, 291, 2893, 264, 7633, 1785, 11, 293, 291, 2893, 341, 4319, 8062, 293, 365, 300, 291, 393, 7983, 439, 264, 4846, 562, 291, 5217, 264, 12853, 295, 264, 958, 1349, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1064279420035226, "compression_ratio": 1.6013986013986015, "no_speech_prob": 0.01301147136837244}, {"id": 81, "seek": 89300, "start": 893.0, "end": 904.0, "text": " So this is essentially an enhanced version of recurrent neural network for translation, where this party is essentially new, it's this attention part.", "tokens": [50364, 407, 341, 307, 4476, 364, 21191, 3037, 295, 18680, 1753, 18161, 3209, 337, 12853, 11, 689, 341, 3595, 307, 4476, 777, 11, 309, 311, 341, 3202, 644, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13113751194693826, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.02000860869884491}, {"id": 82, "seek": 89300, "start": 904.0, "end": 916.0, "text": " Yeah, and from this attention part, people then develop this self attention mechanism. So the self attention mechanism is one major building book of this original transformer architecture that we will cover.", "tokens": [50914, 865, 11, 293, 490, 341, 3202, 644, 11, 561, 550, 1499, 341, 2698, 3202, 7513, 13, 407, 264, 2698, 3202, 7513, 307, 472, 2563, 2390, 1446, 295, 341, 3380, 31782, 9482, 300, 321, 486, 2060, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13113751194693826, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.02000860869884491}, {"id": 83, "seek": 89300, "start": 916.0, "end": 921.0, "text": " And it's kind of inspired by this RNN that I just showed you.", "tokens": [51514, 400, 309, 311, 733, 295, 7547, 538, 341, 45702, 45, 300, 286, 445, 4712, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.13113751194693826, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.02000860869884491}, {"id": 84, "seek": 92100, "start": 921.0, "end": 933.0, "text": " This is a very simple version of self attention. It's not the version that is actually used in the transformer that comes in the future slide. This is a simplified version just to understand the main concept.", "tokens": [50364, 639, 307, 257, 588, 2199, 3037, 295, 2698, 3202, 13, 467, 311, 406, 264, 3037, 300, 307, 767, 1143, 294, 264, 31782, 300, 1487, 294, 264, 2027, 4137, 13, 639, 307, 257, 26335, 3037, 445, 281, 1223, 264, 2135, 3410, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10139492680044736, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.010811949148774147}, {"id": 85, "seek": 92100, "start": 933.0, "end": 940.0, "text": " So what you can see here is, so to cover it step by step, there are three steps.", "tokens": [50964, 407, 437, 291, 393, 536, 510, 307, 11, 370, 281, 2060, 309, 1823, 538, 1823, 11, 456, 366, 1045, 4439, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10139492680044736, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.010811949148774147}, {"id": 86, "seek": 94000, "start": 940.0, "end": 959.0, "text": " So this one is computing these dot products. So imagine, we are currently processing a given word, Xi, and we compute the similarity of this word. This is how we get a certain score for computing the attention weights, we compute the similarity of this to all the other words, including", "tokens": [50364, 407, 341, 472, 307, 15866, 613, 5893, 3383, 13, 407, 3811, 11, 321, 366, 4362, 9007, 257, 2212, 1349, 11, 15712, 11, 293, 321, 14722, 264, 32194, 295, 341, 1349, 13, 639, 307, 577, 321, 483, 257, 1629, 6175, 337, 15866, 264, 3202, 17443, 11, 321, 14722, 264, 32194, 295, 341, 281, 439, 264, 661, 2283, 11, 3009, 51314], "temperature": 0.0, "avg_logprob": -0.17920377177576866, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.10958817601203918}, {"id": 87, "seek": 95900, "start": 960.0, "end": 970.0, "text": " and why are so what we have here is a sequence of words one to T and why are the four blocks there so the four blocks, they just know that this is a vector.", "tokens": [50414, 293, 983, 366, 370, 437, 321, 362, 510, 307, 257, 8310, 295, 2283, 472, 281, 314, 293, 983, 366, 264, 1451, 8474, 456, 370, 264, 1451, 8474, 11, 436, 445, 458, 300, 341, 307, 257, 8062, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1598389123075752, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.011149061843752861}, {"id": 88, "seek": 95900, "start": 970.0, "end": 983.0, "text": " So it's for example a word embedding so usually when we do natural language processing, we don't really feed it in the string of the word so we usually have a method that converts the string let's say to a vector of real numbers.", "tokens": [50914, 407, 309, 311, 337, 1365, 257, 1349, 12240, 3584, 370, 2673, 562, 321, 360, 3303, 2856, 9007, 11, 321, 500, 380, 534, 3154, 309, 294, 264, 6798, 295, 264, 1349, 370, 321, 2673, 362, 257, 3170, 300, 38874, 264, 6798, 718, 311, 584, 281, 257, 8062, 295, 957, 3547, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1598389123075752, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.011149061843752861}, {"id": 89, "seek": 98300, "start": 983.0, "end": 993.0, "text": " So this should just denote a word embedding here. And then there's this, you know, this dog product between two word embeddings to vectors that we multiply.", "tokens": [50364, 407, 341, 820, 445, 45708, 257, 1349, 12240, 3584, 510, 13, 400, 550, 456, 311, 341, 11, 291, 458, 11, 341, 3000, 1674, 1296, 732, 1349, 12240, 29432, 281, 18875, 300, 321, 12972, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1723104783858376, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0030745824333280325}, {"id": 90, "seek": 98300, "start": 993.0, "end": 1004.0, "text": " And let's say this gives us the attention weight. So it's just a simplified version. Usually we would some use something like a softmax function which is a function that would normalize them so that they sum up to one.", "tokens": [50864, 400, 718, 311, 584, 341, 2709, 505, 264, 3202, 3364, 13, 407, 309, 311, 445, 257, 26335, 3037, 13, 11419, 321, 576, 512, 764, 746, 411, 257, 2787, 41167, 2445, 597, 307, 257, 2445, 300, 576, 2710, 1125, 552, 370, 300, 436, 2408, 493, 281, 472, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1723104783858376, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0030745824333280325}, {"id": 91, "seek": 100400, "start": 1004.0, "end": 1020.0, "text": " So that we then again compute the weighted sum of these. So we have these attention weights, and then we have our vectors. So vector one to T, and we just wait them by the corresponding attention weights.", "tokens": [50364, 407, 300, 321, 550, 797, 14722, 264, 32807, 2408, 295, 613, 13, 407, 321, 362, 613, 3202, 17443, 11, 293, 550, 321, 362, 527, 18875, 13, 407, 8062, 472, 281, 314, 11, 293, 321, 445, 1699, 552, 538, 264, 11760, 3202, 17443, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2164584423633332, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.024044962599873543}, {"id": 92, "seek": 102000, "start": 1020.0, "end": 1031.0, "text": " Also, we have on a special a specific attention weight for each query. So if, let's say, Xi here's my query.", "tokens": [50364, 2743, 11, 321, 362, 322, 257, 2121, 257, 2685, 3202, 3364, 337, 1184, 14581, 13, 407, 498, 11, 718, 311, 584, 11, 15712, 510, 311, 452, 14581, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2351818607278066, "compression_ratio": 1.6031746031746033, "no_speech_prob": 0.003171892138198018}, {"id": 93, "seek": 102000, "start": 1031.0, "end": 1044.0, "text": " These attention weights will be different, compared to if x two is my query, because it depends on the on the dot products right. So these would change each word has different embedding so every", "tokens": [50914, 1981, 3202, 17443, 486, 312, 819, 11, 5347, 281, 498, 2031, 732, 307, 452, 14581, 11, 570, 309, 5946, 322, 264, 322, 264, 5893, 3383, 558, 13, 407, 613, 576, 1319, 1184, 1349, 575, 819, 12240, 3584, 370, 633, 51564], "temperature": 0.0, "avg_logprob": -0.2351818607278066, "compression_ratio": 1.6031746031746033, "no_speech_prob": 0.003171892138198018}, {"id": 94, "seek": 104400, "start": 1044.0, "end": 1052.0, "text": " query has its unique attention weights and from that we derive this context vector which is kind of similar what I showed you in the RNN example.", "tokens": [50364, 14581, 575, 1080, 3845, 3202, 17443, 293, 490, 300, 321, 28446, 341, 4319, 8062, 597, 307, 733, 295, 2531, 437, 286, 4712, 291, 294, 264, 45702, 45, 1365, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11441662732292623, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.009122229181230068}, {"id": 95, "seek": 104400, "start": 1052.0, "end": 1062.0, "text": " So this is a simple form of self attention, where we just introduce some, you know, waiting of all the inputs when we compute some output for current word.", "tokens": [50764, 407, 341, 307, 257, 2199, 1254, 295, 2698, 3202, 11, 689, 321, 445, 5366, 512, 11, 291, 458, 11, 3806, 295, 439, 264, 15743, 562, 321, 14722, 512, 5598, 337, 2190, 1349, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11441662732292623, "compression_ratio": 1.52020202020202, "no_speech_prob": 0.009122229181230068}, {"id": 96, "seek": 106200, "start": 1062.0, "end": 1075.0, "text": " So one problem with that though is, so the main takeaway here is that we compute these attention weights but one problem is that we don't have any mechanism for training a neural network now right because there is no, no weight parameter involved.", "tokens": [50364, 407, 472, 1154, 365, 300, 1673, 307, 11, 370, 264, 2135, 30681, 510, 307, 300, 321, 14722, 613, 3202, 17443, 457, 472, 1154, 307, 300, 321, 500, 380, 362, 604, 7513, 337, 3097, 257, 18161, 3209, 586, 558, 570, 456, 307, 572, 11, 572, 3364, 13075, 3288, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11345757507696384, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.007230363320559263}, {"id": 97, "seek": 106200, "start": 1075.0, "end": 1083.0, "text": " So how do we improve this how do we use that say back propagation or gradient descent with that to learn the optimal attention weights for example.", "tokens": [51014, 407, 577, 360, 321, 3470, 341, 577, 360, 321, 764, 300, 584, 646, 38377, 420, 16235, 23475, 365, 300, 281, 1466, 264, 16252, 3202, 17443, 337, 1365, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11345757507696384, "compression_ratio": 1.7400881057268722, "no_speech_prob": 0.007230363320559263}, {"id": 98, "seek": 108300, "start": 1083.0, "end": 1097.0, "text": " So for that, we in practice actually use a different type of attention it's called on scaled dot product attention it's one form of self attention with learnable weights so this involves weight parameters.", "tokens": [50364, 407, 337, 300, 11, 321, 294, 3124, 767, 764, 257, 819, 2010, 295, 3202, 309, 311, 1219, 322, 36039, 5893, 1674, 3202, 309, 311, 472, 1254, 295, 2698, 3202, 365, 1466, 712, 17443, 370, 341, 11626, 3364, 9834, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14039030942049893, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.001926601049490273}, {"id": 99, "seek": 108300, "start": 1097.0, "end": 1110.0, "text": " And yeah there are then three matrices, and with these matrices, you compute also again like a vector a query a key and a value using the input query.", "tokens": [51064, 400, 1338, 456, 366, 550, 1045, 32284, 11, 293, 365, 613, 32284, 11, 291, 14722, 611, 797, 411, 257, 8062, 257, 14581, 257, 2141, 293, 257, 2158, 1228, 264, 4846, 14581, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14039030942049893, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.001926601049490273}, {"id": 100, "seek": 111000, "start": 1110.0, "end": 1121.0, "text": " So even now these matrices are weight parameters that you can update during back propagation or gradient descent to optimize the model.", "tokens": [50364, 407, 754, 586, 613, 32284, 366, 3364, 9834, 300, 291, 393, 5623, 1830, 646, 38377, 420, 16235, 23475, 281, 19719, 264, 2316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16252740593843681, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.0012446003966033459}, {"id": 101, "seek": 111000, "start": 1121.0, "end": 1129.0, "text": " So here, what's going on here is we have a current input query that we are currently processing let's say the second word here again.", "tokens": [50914, 407, 510, 11, 437, 311, 516, 322, 510, 307, 321, 362, 257, 2190, 4846, 14581, 300, 321, 366, 4362, 9007, 718, 311, 584, 264, 1150, 1349, 510, 797, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16252740593843681, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.0012446003966033459}, {"id": 102, "seek": 111000, "start": 1129.0, "end": 1137.0, "text": " And from the second word we apply these three matrices to compute three values or three vectors so a query a key and a value.", "tokens": [51314, 400, 490, 264, 1150, 1349, 321, 3079, 613, 1045, 32284, 281, 14722, 1045, 4190, 420, 1045, 18875, 370, 257, 14581, 257, 2141, 293, 257, 2158, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16252740593843681, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.0012446003966033459}, {"id": 103, "seek": 113700, "start": 1138.0, "end": 1153.0, "text": " Using those so so what we do then is we use the query first so this this query to compute these omega values these are kind of like unscaled attention rates with each key.", "tokens": [50414, 11142, 729, 370, 370, 437, 321, 360, 550, 307, 321, 764, 264, 14581, 700, 370, 341, 341, 14581, 281, 14722, 613, 10498, 4190, 613, 366, 733, 295, 411, 2693, 66, 5573, 3202, 6846, 365, 1184, 2141, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13709691675697885, "compression_ratio": 1.4132231404958677, "no_speech_prob": 0.005553519818931818}, {"id": 104, "seek": 115300, "start": 1153.0, "end": 1157.0, "text": " So the key is computed for each input word again.", "tokens": [50364, 407, 264, 2141, 307, 40610, 337, 1184, 4846, 1349, 797, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13751008293845438, "compression_ratio": 1.6994535519125684, "no_speech_prob": 0.00034597000922076404}, {"id": 105, "seek": 115300, "start": 1157.0, "end": 1175.0, "text": " And we compute the dot product between query and key, but notice here we always use the query to which is our current word that we are processing so we again obtain these unique values or unique attention weights for each query that we are currently processing.", "tokens": [50564, 400, 321, 14722, 264, 5893, 1674, 1296, 14581, 293, 2141, 11, 457, 3449, 510, 321, 1009, 764, 264, 14581, 281, 597, 307, 527, 2190, 1349, 300, 321, 366, 9007, 370, 321, 797, 12701, 613, 3845, 4190, 420, 3845, 3202, 17443, 337, 1184, 14581, 300, 321, 366, 4362, 9007, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13751008293845438, "compression_ratio": 1.6994535519125684, "no_speech_prob": 0.00034597000922076404}, {"id": 106, "seek": 117500, "start": 1175.0, "end": 1183.0, "text": " These are some normalization steps so this is on scale dot product attention. It's a softmax with a scaling factor essentially.", "tokens": [50364, 1981, 366, 512, 2710, 2144, 4439, 370, 341, 307, 322, 4373, 5893, 1674, 3202, 13, 467, 311, 257, 2787, 41167, 365, 257, 21589, 5952, 4476, 13, 50764], "temperature": 0.0, "avg_logprob": -0.21747100216218795, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.020953623577952385}, {"id": 107, "seek": 117500, "start": 1183.0, "end": 1198.0, "text": " And then again, like before we are summing them up to this context vector zine. So this is essentially the same that I showed you here except that it now involves here these trainable matrices and this is, yeah this is the basic on building block behind these transformer", "tokens": [50764, 400, 550, 797, 11, 411, 949, 321, 366, 2408, 2810, 552, 493, 281, 341, 4319, 8062, 710, 533, 13, 407, 341, 307, 4476, 264, 912, 300, 286, 4712, 291, 510, 3993, 300, 309, 586, 11626, 510, 613, 3847, 712, 32284, 293, 341, 307, 11, 1338, 341, 307, 264, 3875, 322, 2390, 3461, 2261, 613, 31782, 51514], "temperature": 0.0, "avg_logprob": -0.21747100216218795, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.020953623577952385}, {"id": 108, "seek": 119800, "start": 1198.0, "end": 1213.0, "text": " networks. So, um, yeah it's maybe a little bit fast but to be honest is really not that much behind the self attention it looks maybe a little bit complicated so maybe you need to look at this a little bit longer it took me definitely some time on to understand", "tokens": [50364, 9590, 13, 407, 11, 1105, 11, 1338, 309, 311, 1310, 257, 707, 857, 2370, 457, 281, 312, 3245, 307, 534, 406, 300, 709, 2261, 264, 2698, 3202, 309, 1542, 1310, 257, 707, 857, 6179, 370, 1310, 291, 643, 281, 574, 412, 341, 257, 707, 857, 2854, 309, 1890, 385, 2138, 512, 565, 322, 281, 1223, 51114], "temperature": 0.0, "avg_logprob": -0.0999063436801617, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.035108134150505066}, {"id": 109, "seek": 119800, "start": 1213.0, "end": 1223.0, "text": " what's going on there, to be honest, a good way for me learning about that was really like reading the paper and then doing these or making these figures and drawing that to kind of make sense of it.", "tokens": [51114, 437, 311, 516, 322, 456, 11, 281, 312, 3245, 11, 257, 665, 636, 337, 385, 2539, 466, 300, 390, 534, 411, 3760, 264, 3035, 293, 550, 884, 613, 420, 1455, 613, 9624, 293, 6316, 300, 281, 733, 295, 652, 2020, 295, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0999063436801617, "compression_ratio": 1.807843137254902, "no_speech_prob": 0.035108134150505066}, {"id": 110, "seek": 122300, "start": 1223.0, "end": 1234.0, "text": " So yeah once once you take a look at this it's actually not as complicated as it looks like looks like. I think the complicated part here is more like that these language models are so large.", "tokens": [50364, 407, 1338, 1564, 1564, 291, 747, 257, 574, 412, 341, 309, 311, 767, 406, 382, 6179, 382, 309, 1542, 411, 1542, 411, 13, 286, 519, 264, 6179, 644, 510, 307, 544, 411, 300, 613, 2856, 5245, 366, 370, 2416, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12705538272857667, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.020318184047937393}, {"id": 111, "seek": 122300, "start": 1234.0, "end": 1244.0, "text": " And if you look at the code itself it's kind of like overwhelming because there's so many things going on there in the code not not in the building block here.", "tokens": [50914, 400, 498, 291, 574, 412, 264, 3089, 2564, 309, 311, 733, 295, 411, 13373, 570, 456, 311, 370, 867, 721, 516, 322, 456, 294, 264, 3089, 406, 406, 294, 264, 2390, 3461, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12705538272857667, "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.020318184047937393}, {"id": 112, "seek": 124400, "start": 1244.0, "end": 1251.0, "text": " So now to get to the original transformer architecture which is built on this self attention mechanism.", "tokens": [50364, 407, 586, 281, 483, 281, 264, 3380, 31782, 9482, 597, 307, 3094, 322, 341, 2698, 3202, 7513, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09144336327739146, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.053350817412137985}, {"id": 113, "seek": 124400, "start": 1251.0, "end": 1262.0, "text": " So, this is based on this paper attention is all you need that started all the trends in terms of developing transformers it's a very influential paper that came out in 2017.", "tokens": [50714, 407, 11, 341, 307, 2361, 322, 341, 3035, 3202, 307, 439, 291, 643, 300, 1409, 439, 264, 13892, 294, 2115, 295, 6416, 4088, 433, 309, 311, 257, 588, 22215, 3035, 300, 1361, 484, 294, 6591, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09144336327739146, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.053350817412137985}, {"id": 114, "seek": 124400, "start": 1262.0, "end": 1269.0, "text": " It's not too long ago four years ago and since then they have been hundreds if not thousands of different transformer architectures.", "tokens": [51264, 467, 311, 406, 886, 938, 2057, 1451, 924, 2057, 293, 1670, 550, 436, 362, 668, 6779, 498, 406, 5383, 295, 819, 31782, 6331, 1303, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09144336327739146, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.053350817412137985}, {"id": 115, "seek": 126900, "start": 1270.0, "end": 1285.0, "text": " The main part is essentially that this is centered around this self attention mechanism, and it does not use any RNN parts so there's no recurrent neural network layer nothing it's just using the self attention concepts and stacking them.", "tokens": [50414, 440, 2135, 644, 307, 4476, 300, 341, 307, 18988, 926, 341, 2698, 3202, 7513, 11, 293, 309, 775, 406, 764, 604, 45702, 45, 3166, 370, 456, 311, 572, 18680, 1753, 18161, 3209, 4583, 1825, 309, 311, 445, 1228, 264, 2698, 3202, 10392, 293, 41376, 552, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10321394172874657, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.009263643063604832}, {"id": 116, "seek": 126900, "start": 1285.0, "end": 1292.0, "text": " And then also having some fully connected layers like in a multi layer perception just a very basic neural network layers.", "tokens": [51164, 400, 550, 611, 1419, 512, 4498, 4582, 7914, 411, 294, 257, 4825, 4583, 12860, 445, 257, 588, 3875, 18161, 3209, 7914, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10321394172874657, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.009263643063604832}, {"id": 117, "seek": 129200, "start": 1292.0, "end": 1309.0, "text": " So this is a sketch of how the transformer architecture looks like. So yeah, like I mentioned, it looks very complicated at first glance there are a lot of boxes and lots of things connected to each other, but essentially it's not too bad it's relatively straightforward it's", "tokens": [50364, 407, 341, 307, 257, 12325, 295, 577, 264, 31782, 9482, 1542, 411, 13, 407, 1338, 11, 411, 286, 2835, 11, 309, 1542, 588, 6179, 412, 700, 21094, 456, 366, 257, 688, 295, 9002, 293, 3195, 295, 721, 4582, 281, 1184, 661, 11, 457, 4476, 309, 311, 406, 886, 1578, 309, 311, 7226, 15325, 309, 311, 51214], "temperature": 0.0, "avg_logprob": -0.15967713372182038, "compression_ratio": 1.4945652173913044, "no_speech_prob": 0.02554616518318653}, {"id": 118, "seek": 130900, "start": 1309.0, "end": 1326.0, "text": " actually, again, our encoder and decoder like in the RNN that I showed you earlier where the job of the encoder is to, yeah, ingest this on input sequence, and the decoder job is to generate the output sequence.", "tokens": [50364, 767, 11, 797, 11, 527, 2058, 19866, 293, 979, 19866, 411, 294, 264, 45702, 45, 300, 286, 4712, 291, 3071, 689, 264, 1691, 295, 264, 2058, 19866, 307, 281, 11, 1338, 11, 3957, 377, 341, 322, 4846, 8310, 11, 293, 264, 979, 19866, 1691, 307, 281, 8460, 264, 5598, 8310, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15448625737970526, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.014055212028324604}, {"id": 119, "seek": 132600, "start": 1326.0, "end": 1336.0, "text": " Yeah, what you do first is you have a word embedding for example to embed the words into real valued vectors. There's something like a positional encoding.", "tokens": [50364, 865, 11, 437, 291, 360, 700, 307, 291, 362, 257, 1349, 12240, 3584, 337, 1365, 281, 12240, 264, 2283, 666, 957, 22608, 18875, 13, 821, 311, 746, 411, 257, 2535, 304, 43430, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10338366998208535, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.005299082025885582}, {"id": 120, "seek": 132600, "start": 1336.0, "end": 1346.0, "text": " Because the problem is, there's its permutation invariant here but we will skip over these details. It's just like a way of encoding the position of the word.", "tokens": [50864, 1436, 264, 1154, 307, 11, 456, 311, 1080, 4784, 11380, 33270, 394, 510, 457, 321, 486, 10023, 670, 613, 4365, 13, 467, 311, 445, 411, 257, 636, 295, 43430, 264, 2535, 295, 264, 1349, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10338366998208535, "compression_ratio": 1.5621890547263682, "no_speech_prob": 0.005299082025885582}, {"id": 121, "seek": 134600, "start": 1347.0, "end": 1358.0, "text": " Then we have this so called multi head attention. It's kind of like, I would say an extension of the self attention mechanism I will have a slide on that to briefly explain what that is.", "tokens": [50414, 1396, 321, 362, 341, 370, 1219, 4825, 1378, 3202, 13, 467, 311, 733, 295, 411, 11, 286, 576, 584, 364, 10320, 295, 264, 2698, 3202, 7513, 286, 486, 362, 257, 4137, 322, 300, 281, 10515, 2903, 437, 300, 307, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1619837725604022, "compression_ratio": 1.4387096774193548, "no_speech_prob": 0.03112863190472126}, {"id": 122, "seek": 134600, "start": 1358.0, "end": 1362.0, "text": " Then there is a layer normalization.", "tokens": [50964, 1396, 456, 307, 257, 4583, 2710, 2144, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1619837725604022, "compression_ratio": 1.4387096774193548, "no_speech_prob": 0.03112863190472126}, {"id": 123, "seek": 136200, "start": 1362.0, "end": 1372.0, "text": " Yeah, like a multi layer perceptron a fully connected network. And then again a layer normalization and so forth. So it's just repeating some of these units and in between.", "tokens": [50364, 865, 11, 411, 257, 4825, 4583, 43276, 2044, 257, 4498, 4582, 3209, 13, 400, 550, 797, 257, 4583, 2710, 2144, 293, 370, 5220, 13, 407, 309, 311, 445, 18617, 512, 295, 613, 6815, 293, 294, 1296, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1459347214361634, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.004263195209205151}, {"id": 124, "seek": 136200, "start": 1372.0, "end": 1379.0, "text": " We also have something like these arrows here connecting things. And these are skip connections.", "tokens": [50864, 492, 611, 362, 746, 411, 613, 19669, 510, 11015, 721, 13, 400, 613, 366, 10023, 9271, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1459347214361634, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.004263195209205151}, {"id": 125, "seek": 136200, "start": 1379.0, "end": 1389.0, "text": " It's essentially just this is what the ad stands for it's taking this value and adds it back here it's similar to what a residual network does if you afford a residual network.", "tokens": [51214, 467, 311, 4476, 445, 341, 307, 437, 264, 614, 7382, 337, 309, 311, 1940, 341, 2158, 293, 10860, 309, 646, 510, 309, 311, 2531, 281, 437, 257, 27980, 3209, 775, 498, 291, 6157, 257, 27980, 3209, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1459347214361634, "compression_ratio": 1.7490196078431373, "no_speech_prob": 0.004263195209205151}, {"id": 126, "seek": 138900, "start": 1389.0, "end": 1407.0, "text": " So the means of preventing, let's say a bad gradients. So if you have something like a vanishing gradient problem here, then this one will be added to this one it's it's kind of like skipping layers if the layers are bad, then there's an opportunity to skip them", "tokens": [50364, 407, 264, 1355, 295, 19965, 11, 718, 311, 584, 257, 1578, 2771, 2448, 13, 407, 498, 291, 362, 746, 411, 257, 3161, 3807, 16235, 1154, 510, 11, 550, 341, 472, 486, 312, 3869, 281, 341, 472, 309, 311, 309, 311, 733, 295, 411, 31533, 7914, 498, 264, 7914, 366, 1578, 11, 550, 456, 311, 364, 2650, 281, 10023, 552, 51264], "temperature": 0.0, "avg_logprob": -0.21610514322916666, "compression_ratio": 1.625, "no_speech_prob": 0.0034283625427633524}, {"id": 127, "seek": 138900, "start": 1407.0, "end": 1409.0, "text": " via these connections so to prevent.", "tokens": [51264, 5766, 613, 9271, 370, 281, 4871, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21610514322916666, "compression_ratio": 1.625, "no_speech_prob": 0.0034283625427633524}, {"id": 128, "seek": 140900, "start": 1410.0, "end": 1421.0, "text": " Bad, bad layers for from ruining the whole network. And then yeah this encoder just captures essentially the input in this self attention weighted way.", "tokens": [50414, 11523, 11, 1578, 7914, 337, 490, 38938, 264, 1379, 3209, 13, 400, 550, 1338, 341, 2058, 19866, 445, 27986, 4476, 264, 4846, 294, 341, 2698, 3202, 32807, 636, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20486341940390096, "compression_ratio": 1.6019417475728155, "no_speech_prob": 0.01743454858660698}, {"id": 129, "seek": 140900, "start": 1421.0, "end": 1431.0, "text": " It's like an encoding of the input. And this feeds into the decoder which has itself also multi head attention there's a masked version which I will also talk a lot of bit about.", "tokens": [50964, 467, 311, 411, 364, 43430, 295, 264, 4846, 13, 400, 341, 23712, 666, 264, 979, 19866, 597, 575, 2564, 611, 4825, 1378, 3202, 456, 311, 257, 45249, 3037, 597, 286, 486, 611, 751, 257, 688, 295, 857, 466, 13, 51464], "temperature": 0.0, "avg_logprob": -0.20486341940390096, "compression_ratio": 1.6019417475728155, "no_speech_prob": 0.01743454858660698}, {"id": 130, "seek": 143100, "start": 1431.0, "end": 1445.0, "text": " Yeah, and this is essentially it so it goes again through a fully connected network and then there's a fully connected layer to produce the outputs so the fully connected layer is then producing word probabilities with a softmax function like how likely is this word at a given", "tokens": [50364, 865, 11, 293, 341, 307, 4476, 309, 370, 309, 1709, 797, 807, 257, 4498, 4582, 3209, 293, 550, 456, 311, 257, 4498, 4582, 4583, 281, 5258, 264, 23930, 370, 264, 4498, 4582, 4583, 307, 550, 10501, 1349, 33783, 365, 257, 2787, 41167, 2445, 411, 577, 3700, 307, 341, 1349, 412, 257, 2212, 51064], "temperature": 0.0, "avg_logprob": -0.12949856427999643, "compression_ratio": 1.8736059479553904, "no_speech_prob": 0.00467835646122694}, {"id": 131, "seek": 143100, "start": 1445.0, "end": 1457.0, "text": " position and then in practice we usually use the highest probability if we, let's say do a language translation or if we generate text in general, we want to have some randomness, we want our model to generate different texts.", "tokens": [51064, 2535, 293, 550, 294, 3124, 321, 2673, 764, 264, 6343, 8482, 498, 321, 11, 718, 311, 584, 360, 257, 2856, 12853, 420, 498, 321, 8460, 2487, 294, 2674, 11, 321, 528, 281, 362, 512, 4974, 1287, 11, 321, 528, 527, 2316, 281, 8460, 819, 15765, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12949856427999643, "compression_ratio": 1.8736059479553904, "no_speech_prob": 0.00467835646122694}, {"id": 132, "seek": 145700, "start": 1457.0, "end": 1466.0, "text": " So just on sample from this distribution. But yeah and in the original transformer architecture what they had in mind was language translation.", "tokens": [50364, 407, 445, 322, 6889, 490, 341, 7316, 13, 583, 1338, 293, 294, 264, 3380, 31782, 9482, 437, 436, 632, 294, 1575, 390, 2856, 12853, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1695202057620129, "compression_ratio": 1.615686274509804, "no_speech_prob": 0.005725491791963577}, {"id": 133, "seek": 145700, "start": 1466.0, "end": 1482.0, "text": " Yeah, to dive a little bit more into this multi head attention, it sounds like it's a whole new concept, but it's essentially just our self attention mechanism so here this is the scaled dot product attention that we talked about earlier with these different matrices.", "tokens": [50814, 865, 11, 281, 9192, 257, 707, 857, 544, 666, 341, 4825, 1378, 3202, 11, 309, 3263, 411, 309, 311, 257, 1379, 777, 3410, 11, 457, 309, 311, 4476, 445, 527, 2698, 3202, 7513, 370, 510, 341, 307, 264, 36039, 5893, 1674, 3202, 300, 321, 2825, 466, 3071, 365, 613, 819, 32284, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1695202057620129, "compression_ratio": 1.615686274509804, "no_speech_prob": 0.005725491791963577}, {"id": 134, "seek": 148200, "start": 1482.0, "end": 1493.0, "text": " And multi head attention is just repeating this multiple times it's basically a stack of these things. It's kind of like similar when you.", "tokens": [50364, 400, 4825, 1378, 3202, 307, 445, 18617, 341, 3866, 1413, 309, 311, 1936, 257, 8630, 295, 613, 721, 13, 467, 311, 733, 295, 411, 2531, 562, 291, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09191695554756824, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.02402660995721817}, {"id": 135, "seek": 148200, "start": 1493.0, "end": 1507.0, "text": " If you have worked with convolution networks and you have an input, and you apply multiple kernels to produce multiple feature maps as the output. It's kind of like the same concept that each channel on captures different type of information here.", "tokens": [50914, 759, 291, 362, 2732, 365, 45216, 9590, 293, 291, 362, 364, 4846, 11, 293, 291, 3079, 3866, 23434, 1625, 281, 5258, 3866, 4111, 11317, 382, 264, 5598, 13, 467, 311, 733, 295, 411, 264, 912, 3410, 300, 1184, 2269, 322, 27986, 819, 2010, 295, 1589, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09191695554756824, "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.02402660995721817}, {"id": 136, "seek": 150700, "start": 1507.0, "end": 1515.0, "text": " Anyway, this is a similar concept where you just repeat the self attention multiple times and stack it up to each other.", "tokens": [50364, 5684, 11, 341, 307, 257, 2531, 3410, 689, 291, 445, 7149, 264, 2698, 3202, 3866, 1413, 293, 8630, 309, 493, 281, 1184, 661, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14148427700174265, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.018818965181708336}, {"id": 137, "seek": 150700, "start": 1515.0, "end": 1525.0, "text": " And this helps you're extracting different features that are useful. And yeah one thing about that is on what's also nice is that here.", "tokens": [50764, 400, 341, 3665, 291, 434, 49844, 819, 4122, 300, 366, 4420, 13, 400, 1338, 472, 551, 466, 300, 307, 322, 437, 311, 611, 1481, 307, 300, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14148427700174265, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.018818965181708336}, {"id": 138, "seek": 152500, "start": 1525.0, "end": 1540.0, "text": " So stacking or what the advantages is we can parallelize that because there's no dependency between these individual scale product attention so you can compute this in parallel helps us also to leverage multiple GPS for example.", "tokens": [50364, 407, 41376, 420, 437, 264, 14906, 307, 321, 393, 8952, 1125, 300, 570, 456, 311, 572, 33621, 1296, 613, 2609, 4373, 1674, 3202, 370, 291, 393, 14722, 341, 294, 8952, 3665, 505, 611, 281, 13982, 3866, 19462, 337, 1365, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3118191415613348, "compression_ratio": 1.4074074074074074, "no_speech_prob": 0.07151364535093307}, {"id": 139, "seek": 154000, "start": 1540.0, "end": 1556.0, "text": " So we have now these so called hats attention hats, and each attention had has essentially one set of these matrices so if we have each on attention hats we have each sets of these matrices, and then we just repeat the same thing multiple times.", "tokens": [50364, 407, 321, 362, 586, 613, 370, 1219, 20549, 3202, 20549, 11, 293, 1184, 3202, 632, 575, 4476, 472, 992, 295, 613, 32284, 370, 498, 321, 362, 1184, 322, 3202, 20549, 321, 362, 1184, 6352, 295, 613, 32284, 11, 293, 550, 321, 445, 7149, 264, 912, 551, 3866, 1413, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2550627330564103, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.10810495167970657}, {"id": 140, "seek": 155600, "start": 1556.0, "end": 1571.0, "text": " And also I should say here. This is six times. So these blocks are also repeated six times. I think this is what makes the architecture so big and complicated when you look at the code if you look at the, let's say from scratch implementation is just there are so many layers and there's", "tokens": [50364, 400, 611, 286, 820, 584, 510, 13, 639, 307, 2309, 1413, 13, 407, 613, 8474, 366, 611, 10477, 2309, 1413, 13, 286, 519, 341, 307, 437, 1669, 264, 9482, 370, 955, 293, 6179, 562, 291, 574, 412, 264, 3089, 498, 291, 574, 412, 264, 11, 718, 311, 584, 490, 8459, 11420, 307, 445, 456, 366, 370, 867, 7914, 293, 456, 311, 51114], "temperature": 0.0, "avg_logprob": -0.12984849489652195, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.007456923834979534}, {"id": 141, "seek": 157100, "start": 1571.0, "end": 1587.0, "text": " not so much on code that implements all of that, but the underlying concepts, if you draw them out there not itself themselves that are not that complicated it's just your matrix multiplication, and then softmax and matrix modification.", "tokens": [50364, 406, 370, 709, 322, 3089, 300, 704, 17988, 439, 295, 300, 11, 457, 264, 14217, 10392, 11, 498, 291, 2642, 552, 484, 456, 406, 2564, 2969, 300, 366, 406, 300, 6179, 309, 311, 445, 428, 8141, 27290, 11, 293, 550, 2787, 41167, 293, 8141, 26747, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19294925689697265, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.024037936702370644}, {"id": 142, "seek": 158700, "start": 1587.0, "end": 1604.0, "text": " Let's talk a bit about this mask maybe here. So the mask multi hat attention is masking out words that we haven't generated yet. So we have a target sequence that is our, let's say desired output that we want to generate let's say if we want to translate", "tokens": [50364, 961, 311, 751, 257, 857, 466, 341, 6094, 1310, 510, 13, 407, 264, 6094, 4825, 2385, 3202, 307, 31226, 484, 2283, 300, 321, 2378, 380, 10833, 1939, 13, 407, 321, 362, 257, 3779, 8310, 300, 307, 527, 11, 718, 311, 584, 14721, 5598, 300, 321, 528, 281, 8460, 718, 311, 584, 498, 321, 528, 281, 13799, 51214], "temperature": 0.0, "avg_logprob": -0.16147391001383463, "compression_ratio": 1.6178343949044587, "no_speech_prob": 0.15994474291801453}, {"id": 143, "seek": 160400, "start": 1604.0, "end": 1614.0, "text": " it, but at a given time step we don't want to have something that is in the future. So if we generate a sentence let's say on right.", "tokens": [50364, 309, 11, 457, 412, 257, 2212, 565, 1823, 321, 500, 380, 528, 281, 362, 746, 300, 307, 294, 264, 2027, 13, 407, 498, 321, 8460, 257, 8174, 718, 311, 584, 322, 558, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18474415113341133, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.011146164499223232}, {"id": 144, "seek": 160400, "start": 1614.0, "end": 1617.0, "text": " Let's say have a sentence help.", "tokens": [50864, 961, 311, 584, 362, 257, 8174, 854, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18474415113341133, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.011146164499223232}, {"id": 145, "seek": 160400, "start": 1617.0, "end": 1621.0, "text": " We translate this.", "tokens": [51014, 492, 13799, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18474415113341133, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.011146164499223232}, {"id": 146, "seek": 162100, "start": 1621.0, "end": 1634.0, "text": " First, we would have then let's say a German version of that but let's say this is our targets version. So in the first step, we would only have access to help, and then this one would be masked out.", "tokens": [50364, 2386, 11, 321, 576, 362, 550, 718, 311, 584, 257, 6521, 3037, 295, 300, 457, 718, 311, 584, 341, 307, 527, 12911, 3037, 13, 407, 294, 264, 700, 1823, 11, 321, 576, 787, 362, 2105, 281, 854, 11, 293, 550, 341, 472, 576, 312, 45249, 484, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11924491681550678, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.060734882950782776}, {"id": 147, "seek": 162100, "start": 1634.0, "end": 1650.0, "text": " And then as a second step, we would have access to these two words and these would be masked out and the mask multi hat attention is essentially ensuring that future words that are not generated yet are masked out.", "tokens": [51014, 400, 550, 382, 257, 1150, 1823, 11, 321, 576, 362, 2105, 281, 613, 732, 2283, 293, 613, 576, 312, 45249, 484, 293, 264, 6094, 4825, 2385, 3202, 307, 4476, 16882, 300, 2027, 2283, 300, 366, 406, 10833, 1939, 366, 45249, 484, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11924491681550678, "compression_ratio": 1.9166666666666667, "no_speech_prob": 0.060734882950782776}, {"id": 148, "seek": 165000, "start": 1650.0, "end": 1654.0, "text": " So actually a slide about that. Yeah and this masking out.", "tokens": [50364, 407, 767, 257, 4137, 466, 300, 13, 865, 293, 341, 31226, 484, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12313812800816128, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.01613275334239006}, {"id": 149, "seek": 165000, "start": 1654.0, "end": 1669.0, "text": " This can also be considered as a unidirectional form of language modeling, because we are masking out everything that is in the future from the current position. So we are masking out from left to right so it's kind of unidirectional.", "tokens": [50564, 639, 393, 611, 312, 4888, 382, 257, 517, 327, 621, 41048, 1254, 295, 2856, 15983, 11, 570, 321, 366, 31226, 484, 1203, 300, 307, 294, 264, 2027, 490, 264, 2190, 2535, 13, 407, 321, 366, 31226, 484, 490, 1411, 281, 558, 370, 309, 311, 733, 295, 517, 327, 621, 41048, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12313812800816128, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.01613275334239006}, {"id": 150, "seek": 166900, "start": 1669.0, "end": 1687.0, "text": " And we will also talk about a model that has so called bidirectional approach so the Burke model has a bidirectional approach and we will revisit this topic it's kind of in it sounds actually fancier than it is so we will see in the future what this bidirectional", "tokens": [50364, 400, 321, 486, 611, 751, 466, 257, 2316, 300, 575, 370, 1219, 12957, 621, 41048, 3109, 370, 264, 37396, 2316, 575, 257, 12957, 621, 41048, 3109, 293, 321, 486, 32676, 341, 4829, 309, 311, 733, 295, 294, 309, 3263, 767, 3429, 27674, 813, 309, 307, 370, 321, 486, 536, 294, 264, 2027, 437, 341, 12957, 621, 41048, 51264], "temperature": 0.0, "avg_logprob": -0.17302005941217596, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.02756953239440918}, {"id": 151, "seek": 166900, "start": 1687.0, "end": 1689.0, "text": " means also.", "tokens": [51264, 1355, 611, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17302005941217596, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.02756953239440918}, {"id": 152, "seek": 168900, "start": 1689.0, "end": 1698.0, "text": " Okay, so yeah, this was already on all the transformer general original transformer stuff I wanted to talk about.", "tokens": [50364, 1033, 11, 370, 1338, 11, 341, 390, 1217, 322, 439, 264, 31782, 2674, 3380, 31782, 1507, 286, 1415, 281, 751, 466, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1046046381411345, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.004262262023985386}, {"id": 153, "seek": 168900, "start": 1698.0, "end": 1712.0, "text": " And these were all the original ideas like from a historic perspective so nowadays, people don't use this original transformer architecture anymore but there are many, many other architectures that have been inspired by this architecture.", "tokens": [50814, 400, 613, 645, 439, 264, 3380, 3487, 411, 490, 257, 13236, 4585, 370, 13434, 11, 561, 500, 380, 764, 341, 3380, 31782, 9482, 3602, 457, 456, 366, 867, 11, 867, 661, 6331, 1303, 300, 362, 668, 7547, 538, 341, 9482, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1046046381411345, "compression_ratio": 1.7087378640776698, "no_speech_prob": 0.004262262023985386}, {"id": 154, "seek": 171200, "start": 1712.0, "end": 1721.0, "text": " So here in the section I want to briefly highlight some of them. So one is this GPT model developed mainly by the open AI team.", "tokens": [50364, 407, 510, 294, 264, 3541, 286, 528, 281, 10515, 5078, 512, 295, 552, 13, 407, 472, 307, 341, 26039, 51, 2316, 4743, 8704, 538, 264, 1269, 7318, 1469, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10768989203632742, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.0320778414607048}, {"id": 155, "seek": 171200, "start": 1721.0, "end": 1732.0, "text": " And this takes this unidirectional approach which is kind of similar to the decoder of the original transformer. And this is used for tasks that involve generating texts.", "tokens": [50814, 400, 341, 2516, 341, 517, 327, 621, 41048, 3109, 597, 307, 733, 295, 2531, 281, 264, 979, 19866, 295, 264, 3380, 31782, 13, 400, 341, 307, 1143, 337, 9608, 300, 9494, 17746, 15765, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10768989203632742, "compression_ratio": 1.505050505050505, "no_speech_prob": 0.0320778414607048}, {"id": 156, "seek": 173200, "start": 1733.0, "end": 1752.0, "text": " And then there is the Burke model and of course there are yet other different types of versions of it for example, Roberta and Alberta and so forth. But in general the Burke model is bidirectional approach, and this is more suited or better suited for classification", "tokens": [50414, 400, 550, 456, 307, 264, 37396, 2316, 293, 295, 1164, 456, 366, 1939, 661, 819, 3467, 295, 9606, 295, 309, 337, 1365, 11, 15800, 1328, 293, 43279, 293, 370, 5220, 13, 583, 294, 2674, 264, 37396, 2316, 307, 12957, 621, 41048, 3109, 11, 293, 341, 307, 544, 24736, 420, 1101, 24736, 337, 21538, 51364], "temperature": 0.0, "avg_logprob": -0.2500844420048228, "compression_ratio": 1.592814371257485, "no_speech_prob": 0.4952651560306549}, {"id": 157, "seek": 175200, "start": 1752.0, "end": 1763.0, "text": " for example, in the code example that I prepared where we predict or whether the movie review is positive or negative. So whether someone liked the movie or not.", "tokens": [50364, 337, 1365, 11, 294, 264, 3089, 1365, 300, 286, 4927, 689, 321, 6069, 420, 1968, 264, 3169, 3131, 307, 3353, 420, 3671, 13, 407, 1968, 1580, 4501, 264, 3169, 420, 406, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15218217032296316, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.03257128223776817}, {"id": 158, "seek": 175200, "start": 1763.0, "end": 1774.0, "text": " Yeah, but one, one scheme that is common amongst most transformers is that there are two steps. One is a pre training step, a pre training step on very large unable data sets.", "tokens": [50914, 865, 11, 457, 472, 11, 472, 12232, 300, 307, 2689, 12918, 881, 4088, 433, 307, 300, 456, 366, 732, 4439, 13, 1485, 307, 257, 659, 3097, 1823, 11, 257, 659, 3097, 1823, 322, 588, 2416, 11299, 1412, 6352, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15218217032296316, "compression_ratio": 1.6683168316831682, "no_speech_prob": 0.03257128223776817}, {"id": 159, "seek": 177400, "start": 1774.0, "end": 1788.0, "text": " And then there's a fine tuning step where we take a pre trained model, and then we train it on our target data set. So here in step one, this is usually a general data set that is not really related to our target task.", "tokens": [50364, 400, 550, 456, 311, 257, 2489, 15164, 1823, 689, 321, 747, 257, 659, 8895, 2316, 11, 293, 550, 321, 3847, 309, 322, 527, 3779, 1412, 992, 13, 407, 510, 294, 1823, 472, 11, 341, 307, 2673, 257, 2674, 1412, 992, 300, 307, 406, 534, 4077, 281, 527, 3779, 5633, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0919837406703404, "compression_ratio": 1.7601626016260163, "no_speech_prob": 0.0012446229811757803}, {"id": 160, "seek": 177400, "start": 1788.0, "end": 1803.0, "text": " It's for instance just a library of books or websites, but we don't have any labels and yeah this is just used to pre train the model they are very large models so they need a lot of data to be kind of pre trained.", "tokens": [51064, 467, 311, 337, 5197, 445, 257, 6405, 295, 3642, 420, 12891, 11, 457, 321, 500, 380, 362, 604, 16949, 293, 1338, 341, 307, 445, 1143, 281, 659, 3847, 264, 2316, 436, 366, 588, 2416, 5245, 370, 436, 643, 257, 688, 295, 1412, 281, 312, 733, 295, 659, 8895, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0919837406703404, "compression_ratio": 1.7601626016260163, "no_speech_prob": 0.0012446229811757803}, {"id": 161, "seek": 180300, "start": 1803.0, "end": 1812.0, "text": " And for this training, there are these two steps that I mentioned the pre training and the fine tuning one and two.", "tokens": [50364, 400, 337, 341, 3097, 11, 456, 366, 613, 732, 4439, 300, 286, 2835, 264, 659, 3097, 293, 264, 2489, 15164, 472, 293, 732, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10658611269558177, "compression_ratio": 1.641711229946524, "no_speech_prob": 0.014491056092083454}, {"id": 162, "seek": 180300, "start": 1812.0, "end": 1824.0, "text": " And the fine tuning can also be split into two different approaches, and both are very common. I think fine tuning is a little bit more common performance better but it's also more expensive.", "tokens": [50814, 400, 264, 2489, 15164, 393, 611, 312, 7472, 666, 732, 819, 11587, 11, 293, 1293, 366, 588, 2689, 13, 286, 519, 2489, 15164, 307, 257, 707, 857, 544, 2689, 3389, 1101, 457, 309, 311, 611, 544, 5124, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10658611269558177, "compression_ratio": 1.641711229946524, "no_speech_prob": 0.014491056092083454}, {"id": 163, "seek": 182400, "start": 1824.0, "end": 1840.0, "text": " One is this feature based approach and one is this fine tuning based approach. So how do they differ. So assume you have step one where you pre train your transformer. So this is sometimes called in the original literature it's called unsupervised pre training, but the more", "tokens": [50364, 1485, 307, 341, 4111, 2361, 3109, 293, 472, 307, 341, 2489, 15164, 2361, 3109, 13, 407, 577, 360, 436, 743, 13, 407, 6552, 291, 362, 1823, 472, 689, 291, 659, 3847, 428, 31782, 13, 407, 341, 307, 2171, 1219, 294, 264, 3380, 10394, 309, 311, 1219, 2693, 12879, 24420, 659, 3097, 11, 457, 264, 544, 51164], "temperature": 0.0, "avg_logprob": -0.1493818153769283, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.04269741475582123}, {"id": 164, "seek": 184000, "start": 1840.0, "end": 1848.0, "text": " modern term for that is self supervised learning. So the same thing essentially, where you train a model on unlabeled data.", "tokens": [50364, 4363, 1433, 337, 300, 307, 2698, 46533, 2539, 13, 407, 264, 912, 551, 4476, 11, 689, 291, 3847, 257, 2316, 322, 32118, 18657, 292, 1412, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18133677542209625, "compression_ratio": 1.6319018404907975, "no_speech_prob": 0.02798675000667572}, {"id": 165, "seek": 184000, "start": 1848.0, "end": 1857.0, "text": " And then, so the data is unlabeled but it's still a supervised learning approach. This is why it's called super as a self supervised learning.", "tokens": [50764, 400, 550, 11, 370, 264, 1412, 307, 32118, 18657, 292, 457, 309, 311, 920, 257, 46533, 2539, 3109, 13, 639, 307, 983, 309, 311, 1219, 1687, 382, 257, 2698, 46533, 2539, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18133677542209625, "compression_ratio": 1.6319018404907975, "no_speech_prob": 0.02798675000667572}, {"id": 166, "seek": 185700, "start": 1857.0, "end": 1872.0, "text": " So you generate essentially the labels on yourself. So you take let's say a book, and then you train the model to predict you remove some words and then you train the model to predict missing words, or you predict the next word in the sentence and the next word in the", "tokens": [50364, 407, 291, 8460, 4476, 264, 16949, 322, 1803, 13, 407, 291, 747, 718, 311, 584, 257, 1446, 11, 293, 550, 291, 3847, 264, 2316, 281, 6069, 291, 4159, 512, 2283, 293, 550, 291, 3847, 264, 2316, 281, 6069, 5361, 2283, 11, 420, 291, 6069, 264, 958, 1349, 294, 264, 8174, 293, 264, 958, 1349, 294, 264, 51114], "temperature": 0.0, "avg_logprob": -0.06182927992737409, "compression_ratio": 2.0440528634361232, "no_speech_prob": 0.03406902775168419}, {"id": 167, "seek": 185700, "start": 1872.0, "end": 1883.0, "text": " sentence would be your label, but it's not like a label for classification it's just like a label that you can generate yourself from the text you don't need to have a human labeling these texts.", "tokens": [51114, 8174, 576, 312, 428, 7645, 11, 457, 309, 311, 406, 411, 257, 7645, 337, 21538, 309, 311, 445, 411, 257, 7645, 300, 291, 393, 8460, 1803, 490, 264, 2487, 291, 500, 380, 643, 281, 362, 257, 1952, 40244, 613, 15765, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06182927992737409, "compression_ratio": 2.0440528634361232, "no_speech_prob": 0.03406902775168419}, {"id": 168, "seek": 188300, "start": 1883.0, "end": 1896.0, "text": " Yeah, so and then let's assume we have now our pre train transformer in this red block. So then, once you have a pre train on a large data set you never have to change it again in terms of", "tokens": [50364, 865, 11, 370, 293, 550, 718, 311, 6552, 321, 362, 586, 527, 659, 3847, 31782, 294, 341, 2182, 3461, 13, 407, 550, 11, 1564, 291, 362, 257, 659, 3847, 322, 257, 2416, 1412, 992, 291, 1128, 362, 281, 1319, 309, 797, 294, 2115, 295, 51014], "temperature": 0.0, "avg_logprob": -0.14439319570859274, "compression_ratio": 1.4351145038167938, "no_speech_prob": 0.010479571297764778}, {"id": 169, "seek": 189600, "start": 1896.0, "end": 1907.0, "text": " you basically save that somehow on your computer, and then you can use it for all your downstream projects that is what I mean. And one way is to never really change it you take it.", "tokens": [50364, 291, 1936, 3155, 300, 6063, 322, 428, 3820, 11, 293, 550, 291, 393, 764, 309, 337, 439, 428, 30621, 4455, 300, 307, 437, 286, 914, 13, 400, 472, 636, 307, 281, 1128, 534, 1319, 309, 291, 747, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1178145585236726, "compression_ratio": 1.751269035532995, "no_speech_prob": 0.18212026357650757}, {"id": 170, "seek": 189600, "start": 1907.0, "end": 1917.0, "text": " And then you just extract the last layer so you give it a new sentence, and then you extract the output layers, one or more output layers you can concatenate them.", "tokens": [50914, 400, 550, 291, 445, 8947, 264, 1036, 4583, 370, 291, 976, 309, 257, 777, 8174, 11, 293, 550, 291, 8947, 264, 5598, 7914, 11, 472, 420, 544, 5598, 7914, 291, 393, 1588, 7186, 473, 552, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1178145585236726, "compression_ratio": 1.751269035532995, "no_speech_prob": 0.18212026357650757}, {"id": 171, "seek": 191700, "start": 1917.0, "end": 1930.0, "text": " There's output layers to a classifier, for example, logistic regression multi layer perceptron maybe a random forest, whatever you desire, and you only train this classifier. So, you essentially treat this pre train transformer as a feature", "tokens": [50364, 821, 311, 5598, 7914, 281, 257, 1508, 9902, 11, 337, 1365, 11, 3565, 3142, 24590, 4825, 4583, 43276, 2044, 1310, 257, 4974, 6719, 11, 2035, 291, 7516, 11, 293, 291, 787, 3847, 341, 1508, 9902, 13, 407, 11, 291, 4476, 2387, 341, 659, 3847, 31782, 382, 257, 4111, 51014], "temperature": 0.0, "avg_logprob": -0.1760909167203036, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.12237932533025742}, {"id": 172, "seek": 191700, "start": 1930.0, "end": 1935.0, "text": " extractor. If you have used on something like principle component analysis.", "tokens": [51014, 8947, 284, 13, 759, 291, 362, 1143, 322, 746, 411, 8665, 6542, 5215, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1760909167203036, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.12237932533025742}, {"id": 173, "seek": 191700, "start": 1935.0, "end": 1944.0, "text": " This is also one method that say it's a linear transformation of your input. It's one method to, let's say, extract some features from your original features so it's in that way.", "tokens": [51264, 639, 307, 611, 472, 3170, 300, 584, 309, 311, 257, 8213, 9887, 295, 428, 4846, 13, 467, 311, 472, 3170, 281, 11, 718, 311, 584, 11, 8947, 512, 4122, 490, 428, 3380, 4122, 370, 309, 311, 294, 300, 636, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1760909167203036, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.12237932533025742}, {"id": 174, "seek": 194400, "start": 1944.0, "end": 1953.0, "text": " So pre train transformer acts like a feature extractor where you just use it to extract these last layers, and then you just train a classifier on top of that.", "tokens": [50364, 407, 659, 3847, 31782, 10672, 411, 257, 4111, 8947, 284, 689, 291, 445, 764, 309, 281, 8947, 613, 1036, 7914, 11, 293, 550, 291, 445, 3847, 257, 1508, 9902, 322, 1192, 295, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09222829752954943, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0021823388524353504}, {"id": 175, "seek": 194400, "start": 1953.0, "end": 1964.0, "text": " Another approach is this fine tuning approach it's a little bit more expensive. This is, you know, for what I prepared this code example. So this fine tuning approach is a little bit different in that you update the whole model.", "tokens": [50814, 3996, 3109, 307, 341, 2489, 15164, 3109, 309, 311, 257, 707, 857, 544, 5124, 13, 639, 307, 11, 291, 458, 11, 337, 437, 286, 4927, 341, 3089, 1365, 13, 407, 341, 2489, 15164, 3109, 307, 257, 707, 857, 819, 294, 300, 291, 5623, 264, 1379, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09222829752954943, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0021823388524353504}, {"id": 176, "seek": 196400, "start": 1964.0, "end": 1976.0, "text": " And then you start with your pre train transformer. Let's say you saved it somewhere on your hard drive, you make a copy of that. And then you feed it the label training data set that you have.", "tokens": [50364, 400, 550, 291, 722, 365, 428, 659, 3847, 31782, 13, 961, 311, 584, 291, 6624, 309, 4079, 322, 428, 1152, 3332, 11, 291, 652, 257, 5055, 295, 300, 13, 400, 550, 291, 3154, 309, 264, 7645, 3097, 1412, 992, 300, 291, 362, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1413349151611328, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.11427908390760422}, {"id": 177, "seek": 196400, "start": 1976.0, "end": 1991.0, "text": " And you add a few layers for classification. Let's say if you have if we have our movie review data set we add one final note in the output layer, have a softmax activation, and then we just train the whole model with back propagation.", "tokens": [50964, 400, 291, 909, 257, 1326, 7914, 337, 21538, 13, 961, 311, 584, 498, 291, 362, 498, 321, 362, 527, 3169, 3131, 1412, 992, 321, 909, 472, 2572, 3637, 294, 264, 5598, 4583, 11, 362, 257, 2787, 41167, 24433, 11, 293, 550, 321, 445, 3847, 264, 1379, 2316, 365, 646, 38377, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1413349151611328, "compression_ratio": 1.7727272727272727, "no_speech_prob": 0.11427908390760422}, {"id": 178, "seek": 199100, "start": 1992.0, "end": 2005.0, "text": " And because we have to load all the models, the whole model in memory and do the back propagation it's a little bit more expensive than than this step where we only use the transformer for generating the features.", "tokens": [50414, 400, 570, 321, 362, 281, 3677, 439, 264, 5245, 11, 264, 1379, 2316, 294, 4675, 293, 360, 264, 646, 38377, 309, 311, 257, 707, 857, 544, 5124, 813, 813, 341, 1823, 689, 321, 787, 764, 264, 31782, 337, 17746, 264, 4122, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12567314234646884, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.04079815000295639}, {"id": 179, "seek": 199100, "start": 2005.0, "end": 2016.0, "text": " So that way, it depends. It's of course also slower. So it depends on what type of architecture you have access to to use this approach but in practice it performs a little bit better.", "tokens": [51064, 407, 300, 636, 11, 309, 5946, 13, 467, 311, 295, 1164, 611, 14009, 13, 407, 309, 5946, 322, 437, 2010, 295, 9482, 291, 362, 2105, 281, 281, 764, 341, 3109, 457, 294, 3124, 309, 26213, 257, 707, 857, 1101, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12567314234646884, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.04079815000295639}, {"id": 180, "seek": 201600, "start": 2016.0, "end": 2023.0, "text": " I forgot which paper it was either GPT one or bird. They also directly compared those approaches.", "tokens": [50364, 286, 5298, 597, 3035, 309, 390, 2139, 26039, 51, 472, 420, 5255, 13, 814, 611, 3838, 5347, 729, 11587, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1513315238574944, "compression_ratio": 1.6796875, "no_speech_prob": 0.009408553130924702}, {"id": 181, "seek": 201600, "start": 2023.0, "end": 2038.0, "text": " I think my GPT one but I would have to double check. And they found I think if you lose use the last three layers here, you can get approximately the same performance as with fine tuning so in that way it's, it's really, you can get the same performance but it's really a trade off also", "tokens": [50714, 286, 519, 452, 26039, 51, 472, 457, 286, 576, 362, 281, 3834, 1520, 13, 400, 436, 1352, 286, 519, 498, 291, 3624, 764, 264, 1036, 1045, 7914, 510, 11, 291, 393, 483, 10447, 264, 912, 3389, 382, 365, 2489, 15164, 370, 294, 300, 636, 309, 311, 11, 309, 311, 534, 11, 291, 393, 483, 264, 912, 3389, 457, 309, 311, 534, 257, 4923, 766, 611, 51464], "temperature": 0.0, "avg_logprob": -0.1513315238574944, "compression_ratio": 1.6796875, "no_speech_prob": 0.009408553130924702}, {"id": 182, "seek": 201600, "start": 2038.0, "end": 2043.0, "text": " of how much computational resources you have.", "tokens": [51464, 295, 577, 709, 28270, 3593, 291, 362, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1513315238574944, "compression_ratio": 1.6796875, "no_speech_prob": 0.009408553130924702}, {"id": 183, "seek": 204300, "start": 2043.0, "end": 2059.0, "text": " Yeah, so talking a little bit about the GPT models. So there are three GPT models now. So one was released in 2018 one in 2019 one in 2020 and yeah, if the sequence continues we maybe have a new version by the end of this year.", "tokens": [50364, 865, 11, 370, 1417, 257, 707, 857, 466, 264, 26039, 51, 5245, 13, 407, 456, 366, 1045, 26039, 51, 5245, 586, 13, 407, 472, 390, 4736, 294, 6096, 472, 294, 6071, 472, 294, 4808, 293, 1338, 11, 498, 264, 8310, 6515, 321, 1310, 362, 257, 777, 3037, 538, 264, 917, 295, 341, 1064, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13851129597630993, "compression_ratio": 1.4458598726114649, "no_speech_prob": 0.0016738101840019226}, {"id": 184, "seek": 205900, "start": 2059.0, "end": 2073.0, "text": " And yeah, one obvious thing you can see from these GPT models which stands for generative pretrained transformer is that they increased a lot in size so they started with 110 million parameters which was already really huge.", "tokens": [50364, 400, 1338, 11, 472, 6322, 551, 291, 393, 536, 490, 613, 26039, 51, 5245, 597, 7382, 337, 1337, 1166, 1162, 31774, 31782, 307, 300, 436, 6505, 257, 688, 294, 2744, 370, 436, 1409, 365, 20154, 2459, 9834, 597, 390, 1217, 534, 2603, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13475461006164552, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.01613902859389782}, {"id": 185, "seek": 205900, "start": 2073.0, "end": 2082.0, "text": " Then the next one had 1.5. And now we have 107 175 billion. So who knows maybe the next one is in the trillion range.", "tokens": [51064, 1396, 264, 958, 472, 632, 502, 13, 20, 13, 400, 586, 321, 362, 1266, 22, 41165, 5218, 13, 407, 567, 3255, 1310, 264, 958, 472, 307, 294, 264, 18723, 3613, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13475461006164552, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.01613902859389782}, {"id": 186, "seek": 208200, "start": 2082.0, "end": 2099.0, "text": " So just like I said, I will share my slides later so you can also read more about that in the original manuscripts, but yet just to give you just brief summary of how these models work, I have a few slides on those.", "tokens": [50364, 407, 445, 411, 286, 848, 11, 286, 486, 2073, 452, 9788, 1780, 370, 291, 393, 611, 1401, 544, 466, 300, 294, 264, 3380, 42849, 11, 457, 1939, 445, 281, 976, 291, 445, 5353, 12691, 295, 577, 613, 5245, 589, 11, 286, 362, 257, 1326, 9788, 322, 729, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17411514428945687, "compression_ratio": 1.423841059602649, "no_speech_prob": 0.18452765047550201}, {"id": 187, "seek": 209900, "start": 2099.0, "end": 2110.0, "text": " So GPT one, that's the first model in the series. It's essentially very similar to the transformer we talked about, especially I mean, it's just taking the decoder part.", "tokens": [50364, 407, 26039, 51, 472, 11, 300, 311, 264, 700, 2316, 294, 264, 2638, 13, 467, 311, 4476, 588, 2531, 281, 264, 31782, 321, 2825, 466, 11, 2318, 286, 914, 11, 309, 311, 445, 1940, 264, 979, 19866, 644, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14558313269364206, "compression_ratio": 1.6431535269709543, "no_speech_prob": 0.4518570303916931}, {"id": 188, "seek": 209900, "start": 2110.0, "end": 2128.0, "text": " And it's trained on a sentence in this next word prediction manner. So the pre training is taking a large context or a large corpus of books or websites I think, I think they use the big a mix of words from websites and books.", "tokens": [50914, 400, 309, 311, 8895, 322, 257, 8174, 294, 341, 958, 1349, 17630, 9060, 13, 407, 264, 659, 3097, 307, 1940, 257, 2416, 4319, 420, 257, 2416, 1181, 31624, 295, 3642, 420, 12891, 286, 519, 11, 286, 519, 436, 764, 264, 955, 257, 2890, 295, 2283, 490, 12891, 293, 3642, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14558313269364206, "compression_ratio": 1.6431535269709543, "no_speech_prob": 0.4518570303916931}, {"id": 189, "seek": 212800, "start": 2128.0, "end": 2143.0, "text": " There are instances where they remove through this masking, the future words in the model's task is to predict the next word, and they just train the model in this manner. So they train it just for this prediction of the next word in the sentence.", "tokens": [50364, 821, 366, 14519, 689, 436, 4159, 807, 341, 31226, 11, 264, 2027, 2283, 294, 264, 2316, 311, 5633, 307, 281, 6069, 264, 958, 1349, 11, 293, 436, 445, 3847, 264, 2316, 294, 341, 9060, 13, 407, 436, 3847, 309, 445, 337, 341, 17630, 295, 264, 958, 1349, 294, 264, 8174, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18779508417302912, "compression_ratio": 1.6689189189189189, "no_speech_prob": 0.08986937999725342}, {"id": 190, "seek": 214300, "start": 2143.0, "end": 2152.0, "text": " So by doing that, you can then fine tune it so you add. So for having this task classifier, you can add additional layers.", "tokens": [50364, 407, 538, 884, 300, 11, 291, 393, 550, 2489, 10864, 309, 370, 291, 909, 13, 407, 337, 1419, 341, 5633, 1508, 9902, 11, 291, 393, 909, 4497, 7914, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15651779864207807, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.2938336431980133}, {"id": 191, "seek": 214300, "start": 2152.0, "end": 2162.0, "text": " After you pre trained the model, and depending on what you're interested in this you can do our classification entailment similarity multiple choice questions and so forth.", "tokens": [50814, 2381, 291, 659, 8895, 264, 2316, 11, 293, 5413, 322, 437, 291, 434, 3102, 294, 341, 291, 393, 360, 527, 21538, 948, 864, 518, 32194, 3866, 3922, 1651, 293, 370, 5220, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15651779864207807, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.2938336431980133}, {"id": 192, "seek": 214300, "start": 2162.0, "end": 2167.0, "text": " And how that works is essentially they have this pre trained transformer here.", "tokens": [51314, 400, 577, 300, 1985, 307, 4476, 436, 362, 341, 659, 8895, 31782, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15651779864207807, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.2938336431980133}, {"id": 193, "seek": 216700, "start": 2167.0, "end": 2175.0, "text": " You connect the linear layers and then it's like a regular classifier. And yeah, one thing here is.", "tokens": [50364, 509, 1745, 264, 8213, 7914, 293, 550, 309, 311, 411, 257, 3890, 1508, 9902, 13, 400, 1338, 11, 472, 551, 510, 307, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11602445961772531, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.06845828145742416}, {"id": 194, "seek": 216700, "start": 2175.0, "end": 2186.0, "text": " I won't go into too much detail but it's how you format the input. If you have a classification, you have like a start token a text, the sentences and then some extraction token.", "tokens": [50764, 286, 1582, 380, 352, 666, 886, 709, 2607, 457, 309, 311, 577, 291, 7877, 264, 4846, 13, 759, 291, 362, 257, 21538, 11, 291, 362, 411, 257, 722, 14862, 257, 2487, 11, 264, 16579, 293, 550, 512, 30197, 14862, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11602445961772531, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.06845828145742416}, {"id": 195, "seek": 218600, "start": 2186.0, "end": 2205.0, "text": " If you have something like similarity here, text similarity, they have on both texts and then in flipped order for example so it just the first a little bit about how you set up the inputs, but yeah the main, the main thing is that you can just adopt this pre trained network to do whatever", "tokens": [50364, 759, 291, 362, 746, 411, 32194, 510, 11, 2487, 32194, 11, 436, 362, 322, 1293, 15765, 293, 550, 294, 26273, 1668, 337, 1365, 370, 309, 445, 264, 700, 257, 707, 857, 466, 577, 291, 992, 493, 264, 15743, 11, 457, 1338, 264, 2135, 11, 264, 2135, 551, 307, 300, 291, 393, 445, 6878, 341, 659, 8895, 3209, 281, 360, 2035, 51314], "temperature": 0.0, "avg_logprob": -0.18875154852867126, "compression_ratio": 1.6201117318435754, "no_speech_prob": 0.04601385071873665}, {"id": 196, "seek": 220500, "start": 2205.0, "end": 2221.0, "text": " you like in the fine tuning. So this is really expensive. This is like a really expensive step that something you don't want to do yourself you want to maybe download this GPT and then you can on your target data set to the fine tuning.", "tokens": [50364, 291, 411, 294, 264, 2489, 15164, 13, 407, 341, 307, 534, 5124, 13, 639, 307, 411, 257, 534, 5124, 1823, 300, 746, 291, 500, 380, 528, 281, 360, 1803, 291, 528, 281, 1310, 5484, 341, 26039, 51, 293, 550, 291, 393, 322, 428, 3779, 1412, 992, 281, 264, 2489, 15164, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1436387755654075, "compression_ratio": 1.6275862068965516, "no_speech_prob": 0.04810013622045517}, {"id": 197, "seek": 222100, "start": 2221.0, "end": 2236.0, "text": " And then really what was really novel about GPT two and GPT three is that they actually removed this fine tuning most language transformers they use the fine tuning stuff that I talked about what GPT two and GPT three are doing though is they tried something new.", "tokens": [50364, 400, 550, 534, 437, 390, 534, 7613, 466, 26039, 51, 732, 293, 26039, 51, 1045, 307, 300, 436, 767, 7261, 341, 2489, 15164, 881, 2856, 4088, 433, 436, 764, 264, 2489, 15164, 1507, 300, 286, 2825, 466, 437, 26039, 51, 732, 293, 26039, 51, 1045, 366, 884, 1673, 307, 436, 3031, 746, 777, 13, 51114], "temperature": 0.0, "avg_logprob": -0.133887448153653, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.016647573560476303}, {"id": 198, "seek": 222100, "start": 2236.0, "end": 2246.0, "text": " They were so ambitious that they kind of removed this fine tuning, and they only have a model now where you provide what you want to do as input.", "tokens": [51114, 814, 645, 370, 20239, 300, 436, 733, 295, 7261, 341, 2489, 15164, 11, 293, 436, 787, 362, 257, 2316, 586, 689, 291, 2893, 437, 291, 528, 281, 360, 382, 4846, 13, 51614], "temperature": 0.0, "avg_logprob": -0.133887448153653, "compression_ratio": 1.801762114537445, "no_speech_prob": 0.016647573560476303}, {"id": 199, "seek": 224600, "start": 2246.0, "end": 2260.0, "text": " So for example, if you want to use their model the GPT two model to generate text, what you do is you give it the input that is formatted as follows where you say to the model translate to French.", "tokens": [50364, 407, 337, 1365, 11, 498, 291, 528, 281, 764, 641, 2316, 264, 26039, 51, 732, 2316, 281, 8460, 2487, 11, 437, 291, 360, 307, 291, 976, 309, 264, 4846, 300, 307, 1254, 32509, 382, 10002, 689, 291, 584, 281, 264, 2316, 13799, 281, 5522, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1369406905355333, "compression_ratio": 1.7085427135678393, "no_speech_prob": 0.09525166451931}, {"id": 200, "seek": 224600, "start": 2260.0, "end": 2269.0, "text": " And then you insert your English text, and then you insert your French text and it will automatically figure out what you want to do basically.", "tokens": [51064, 400, 550, 291, 8969, 428, 3669, 2487, 11, 293, 550, 291, 8969, 428, 5522, 2487, 293, 309, 486, 6772, 2573, 484, 437, 291, 528, 281, 360, 1936, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1369406905355333, "compression_ratio": 1.7085427135678393, "no_speech_prob": 0.09525166451931}, {"id": 201, "seek": 226900, "start": 2269.0, "end": 2286.0, "text": " There are different questions like you give it a sentence and you can ask something like this is object. So you can say for example, this is a very nice sky and is the sky blue and it will what can generate like output that says yes or no or something like that.", "tokens": [50364, 821, 366, 819, 1651, 411, 291, 976, 309, 257, 8174, 293, 291, 393, 1029, 746, 411, 341, 307, 2657, 13, 407, 291, 393, 584, 337, 1365, 11, 341, 307, 257, 588, 1481, 5443, 293, 307, 264, 5443, 3344, 293, 309, 486, 437, 393, 8460, 411, 5598, 300, 1619, 2086, 420, 572, 420, 746, 411, 300, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16904892010635206, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.19651779532432556}, {"id": 202, "seek": 226900, "start": 2286.0, "end": 2296.0, "text": " So it's kind of very flexible in terms of what it can do by only having a certain formatted input without any fine tuning.", "tokens": [51214, 407, 309, 311, 733, 295, 588, 11358, 294, 2115, 295, 437, 309, 393, 360, 538, 787, 1419, 257, 1629, 1254, 32509, 4846, 1553, 604, 2489, 15164, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16904892010635206, "compression_ratio": 1.696035242290749, "no_speech_prob": 0.19651779532432556}, {"id": 203, "seek": 229600, "start": 2296.0, "end": 2298.0, "text": " So in version three.", "tokens": [50364, 407, 294, 3037, 1045, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15914589425791864, "compression_ratio": 1.7151162790697674, "no_speech_prob": 0.1962142288684845}, {"id": 204, "seek": 229600, "start": 2298.0, "end": 2313.0, "text": " I think they kind of went back a little bit because I think. So here, this is called in this context on zero shot learning where they don't have any examples of the task the example is in the scriptures in the task itself they don't have any training examples in that sense.", "tokens": [50464, 286, 519, 436, 733, 295, 1437, 646, 257, 707, 857, 570, 286, 519, 13, 407, 510, 11, 341, 307, 1219, 294, 341, 4319, 322, 4018, 3347, 2539, 689, 436, 500, 380, 362, 604, 5110, 295, 264, 5633, 264, 1365, 307, 294, 264, 29969, 294, 264, 5633, 2564, 436, 500, 380, 362, 604, 3097, 5110, 294, 300, 2020, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15914589425791864, "compression_ratio": 1.7151162790697674, "no_speech_prob": 0.1962142288684845}, {"id": 205, "seek": 231300, "start": 2313.0, "end": 2323.0, "text": " The task was maybe a little bit too ambitious. So they went back with GPT three made it a little bit bigger, and they switched also to few short learning in this context where in the future learning.", "tokens": [50364, 440, 5633, 390, 1310, 257, 707, 857, 886, 20239, 13, 407, 436, 1437, 646, 365, 26039, 51, 1045, 1027, 309, 257, 707, 857, 3801, 11, 293, 436, 16858, 611, 281, 1326, 2099, 2539, 294, 341, 4319, 689, 294, 264, 2027, 2539, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15376396934584816, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.12918621301651}, {"id": 206, "seek": 231300, "start": 2323.0, "end": 2341.0, "text": " They have at least a few examples of that task. So just to illustrate how that looks like so here is this zero shot where you have again the prompt or task description sorry, and then a prompt, and then it would insert here the French word for cheese.", "tokens": [50864, 814, 362, 412, 1935, 257, 1326, 5110, 295, 300, 5633, 13, 407, 445, 281, 23221, 577, 300, 1542, 411, 370, 510, 307, 341, 4018, 3347, 689, 291, 362, 797, 264, 12391, 420, 5633, 3855, 2597, 11, 293, 550, 257, 12391, 11, 293, 550, 309, 576, 8969, 510, 264, 5522, 1349, 337, 5399, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15376396934584816, "compression_ratio": 1.7279693486590038, "no_speech_prob": 0.12918621301651}, {"id": 207, "seek": 234100, "start": 2341.0, "end": 2348.0, "text": " I honestly don't know any French so I can't tell you what it is, but yeah it would generate this output here.", "tokens": [50364, 286, 6095, 500, 380, 458, 604, 5522, 370, 286, 393, 380, 980, 291, 437, 309, 307, 11, 457, 1338, 309, 576, 8460, 341, 5598, 510, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11689793722970145, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.0251531470566988}, {"id": 208, "seek": 234100, "start": 2348.0, "end": 2358.0, "text": " And for the one shot. It's a little bit more, I would say, easier for the model because it sees at least an example. It's not fine to it's just seeing the example as part of the input.", "tokens": [50714, 400, 337, 264, 472, 3347, 13, 467, 311, 257, 707, 857, 544, 11, 286, 576, 584, 11, 3571, 337, 264, 2316, 570, 309, 8194, 412, 1935, 364, 1365, 13, 467, 311, 406, 2489, 281, 309, 311, 445, 2577, 264, 1365, 382, 644, 295, 264, 4846, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11689793722970145, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.0251531470566988}, {"id": 209, "seek": 234100, "start": 2358.0, "end": 2365.0, "text": " So you have again the task description here at the top. And then you have one example of what you want to do.", "tokens": [51214, 407, 291, 362, 797, 264, 5633, 3855, 510, 412, 264, 1192, 13, 400, 550, 291, 362, 472, 1365, 295, 437, 291, 528, 281, 360, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11689793722970145, "compression_ratio": 1.662551440329218, "no_speech_prob": 0.0251531470566988}, {"id": 210, "seek": 236500, "start": 2365.0, "end": 2376.0, "text": " And then few short learning so if you have let's say here on the three shot case where you have one example second example the third example, it becomes easier for the model.", "tokens": [50364, 400, 550, 1326, 2099, 2539, 370, 498, 291, 362, 718, 311, 584, 510, 322, 264, 1045, 3347, 1389, 689, 291, 362, 472, 1365, 1150, 1365, 264, 2636, 1365, 11, 309, 3643, 3571, 337, 264, 2316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22838992164248512, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00940894614905119}, {"id": 211, "seek": 236500, "start": 2376.0, "end": 2379.0, "text": " If you showed at least a few examples.", "tokens": [50914, 759, 291, 4712, 412, 1935, 257, 1326, 5110, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22838992164248512, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00940894614905119}, {"id": 212, "seek": 236500, "start": 2379.0, "end": 2381.0, "text": " See that's just a note in the chat.", "tokens": [51064, 3008, 300, 311, 445, 257, 3637, 294, 264, 5081, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22838992164248512, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00940894614905119}, {"id": 213, "seek": 236500, "start": 2381.0, "end": 2390.0, "text": " I will maybe answer the questions after the talk and think almost over time with your slides left.", "tokens": [51164, 286, 486, 1310, 1867, 264, 1651, 934, 264, 751, 293, 519, 1920, 670, 565, 365, 428, 9788, 1411, 13, 51614], "temperature": 0.0, "avg_logprob": -0.22838992164248512, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00940894614905119}, {"id": 214, "seek": 239000, "start": 2390.0, "end": 2394.0, "text": " Yeah, so here.", "tokens": [50364, 865, 11, 370, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.19622230529785156, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.01796632446348667}, {"id": 215, "seek": 239000, "start": 2394.0, "end": 2408.0, "text": " Well, where was I so you're here on the limitation of the future tasks though is that you have a limited size input. I honestly don't remember what the sizes for GPT three I think they have some IP eyes that I've never used.", "tokens": [50564, 1042, 11, 689, 390, 286, 370, 291, 434, 510, 322, 264, 27432, 295, 264, 2027, 9608, 1673, 307, 300, 291, 362, 257, 5567, 2744, 4846, 13, 286, 6095, 500, 380, 1604, 437, 264, 11602, 337, 26039, 51, 1045, 286, 519, 436, 362, 512, 8671, 2575, 300, 286, 600, 1128, 1143, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19622230529785156, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.01796632446348667}, {"id": 216, "seek": 239000, "start": 2408.0, "end": 2418.0, "text": " But let's say for bird models there's a token input limitation of 512 tokens. It's like characters or work and characters but like words or punctuation.", "tokens": [51264, 583, 718, 311, 584, 337, 5255, 5245, 456, 311, 257, 14862, 4846, 27432, 295, 1025, 4762, 22667, 13, 467, 311, 411, 4342, 420, 589, 293, 4342, 457, 411, 2283, 420, 27006, 16073, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19622230529785156, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.01796632446348667}, {"id": 217, "seek": 241800, "start": 2418.0, "end": 2426.0, "text": " So that way, you're kind of limited by what the model can process as input and how many examples you provide.", "tokens": [50364, 407, 300, 636, 11, 291, 434, 733, 295, 5567, 538, 437, 264, 2316, 393, 1399, 382, 4846, 293, 577, 867, 5110, 291, 2893, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06784304645326403, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.012046156451106071}, {"id": 218, "seek": 241800, "start": 2426.0, "end": 2438.0, "text": " So this is actually a fine but I can imagine it's maybe a challenge if you have longer sentences here so that wouldn't probably work with very long sentences because then you fill up all the input with examples.", "tokens": [50764, 407, 341, 307, 767, 257, 2489, 457, 286, 393, 3811, 309, 311, 1310, 257, 3430, 498, 291, 362, 2854, 16579, 510, 370, 300, 2759, 380, 1391, 589, 365, 588, 938, 16579, 570, 550, 291, 2836, 493, 439, 264, 4846, 365, 5110, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06784304645326403, "compression_ratio": 1.5891089108910892, "no_speech_prob": 0.012046156451106071}, {"id": 219, "seek": 243800, "start": 2438.0, "end": 2451.0, "text": " So this is in contrast to the fine tuning approach with which most other transformers use where you have an example as part of your training set that then you updated with back propagation like the gradient update showed another example and so forth.", "tokens": [50364, 407, 341, 307, 294, 8712, 281, 264, 2489, 15164, 3109, 365, 597, 881, 661, 4088, 433, 764, 689, 291, 362, 364, 1365, 382, 644, 295, 428, 3097, 992, 300, 550, 291, 10588, 365, 646, 38377, 411, 264, 16235, 5623, 4712, 1071, 1365, 293, 370, 5220, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1256713683788593, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.04081221669912338}, {"id": 220, "seek": 243800, "start": 2451.0, "end": 2465.0, "text": " So this is more like the traditional way of doing it. And yeah the GPT three models very ambitious and I think I'm not sure if actually the paper is out yet but yeah this is one of the latest state of the art models for generating texts.", "tokens": [51014, 407, 341, 307, 544, 411, 264, 5164, 636, 295, 884, 309, 13, 400, 1338, 264, 26039, 51, 1045, 5245, 588, 20239, 293, 286, 519, 286, 478, 406, 988, 498, 767, 264, 3035, 307, 484, 1939, 457, 1338, 341, 307, 472, 295, 264, 6792, 1785, 295, 264, 1523, 5245, 337, 17746, 15765, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1256713683788593, "compression_ratio": 1.7062937062937062, "no_speech_prob": 0.04081221669912338}, {"id": 221, "seek": 246500, "start": 2465.0, "end": 2471.0, "text": " And another approach is the bird model, which is bidirectional encoder.", "tokens": [50364, 400, 1071, 3109, 307, 264, 5255, 2316, 11, 597, 307, 12957, 621, 41048, 2058, 19866, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10466539594862197, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0046783797442913055}, {"id": 222, "seek": 246500, "start": 2471.0, "end": 2486.0, "text": " So you can think of GPT more as the decoder which generates some output whereas bird is more like the encoder of the transformer which ingests like the input, and then creates a representation that you can then use to train a class to fire on.", "tokens": [50664, 407, 291, 393, 519, 295, 26039, 51, 544, 382, 264, 979, 19866, 597, 23815, 512, 5598, 9735, 5255, 307, 544, 411, 264, 2058, 19866, 295, 264, 31782, 597, 3957, 4409, 411, 264, 4846, 11, 293, 550, 7829, 257, 10290, 300, 291, 393, 550, 764, 281, 3847, 257, 1508, 281, 2610, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10466539594862197, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0046783797442913055}, {"id": 223, "seek": 246500, "start": 2486.0, "end": 2492.0, "text": " So GPT is better for generating texts bird is better for classification.", "tokens": [51414, 407, 26039, 51, 307, 1101, 337, 17746, 15765, 5255, 307, 1101, 337, 21538, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10466539594862197, "compression_ratio": 1.7716894977168949, "no_speech_prob": 0.0046783797442913055}, {"id": 224, "seek": 249200, "start": 2492.0, "end": 2501.0, "text": " So how that works is they have a pre training step or they have actually two different types of pre training tasks. One is on this masked language model.", "tokens": [50364, 407, 577, 300, 1985, 307, 436, 362, 257, 659, 3097, 1823, 420, 436, 362, 767, 732, 819, 3467, 295, 659, 3097, 9608, 13, 1485, 307, 322, 341, 45249, 2856, 2316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12585039237110884, "compression_ratio": 1.7644444444444445, "no_speech_prob": 0.026737868785858154}, {"id": 225, "seek": 249200, "start": 2501.0, "end": 2518.0, "text": " So what they do is they mask 15% of the words. I like to call it marked, not masked yet, because what they do is they take these 15 words or 15% of the words if you have a sentence, and then they do different things to it so here's an example.", "tokens": [50814, 407, 437, 436, 360, 307, 436, 6094, 2119, 4, 295, 264, 2283, 13, 286, 411, 281, 818, 309, 12658, 11, 406, 45249, 1939, 11, 570, 437, 436, 360, 307, 436, 747, 613, 2119, 2283, 420, 2119, 4, 295, 264, 2283, 498, 291, 362, 257, 8174, 11, 293, 550, 436, 360, 819, 721, 281, 309, 370, 510, 311, 364, 1365, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12585039237110884, "compression_ratio": 1.7644444444444445, "no_speech_prob": 0.026737868785858154}, {"id": 226, "seek": 251800, "start": 2518.0, "end": 2536.0, "text": " So if you have a sentence here and input sentence, you would randomly pick 15% of the words for example that you would pick, let's say Fox, and then based on these 15% what you do is 80% of the time, you replace this token, the fox with a mask.", "tokens": [50364, 407, 498, 291, 362, 257, 8174, 510, 293, 4846, 8174, 11, 291, 576, 16979, 1888, 2119, 4, 295, 264, 2283, 337, 1365, 300, 291, 576, 1888, 11, 718, 311, 584, 11388, 11, 293, 550, 2361, 322, 613, 2119, 4, 437, 291, 360, 307, 4688, 4, 295, 264, 565, 11, 291, 7406, 341, 14862, 11, 264, 21026, 365, 257, 6094, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19720755517482758, "compression_ratio": 1.4969325153374233, "no_speech_prob": 0.06552726775407791}, {"id": 227, "seek": 253600, "start": 2536.0, "end": 2544.0, "text": " And 10% of the time, you replace it with a random word, for example, coffee, and 10% of the time you keep it unchanged.", "tokens": [50364, 400, 1266, 4, 295, 264, 565, 11, 291, 7406, 309, 365, 257, 4974, 1349, 11, 337, 1365, 11, 4982, 11, 293, 1266, 4, 295, 264, 565, 291, 1066, 309, 44553, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1391799807548523, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.18209141492843628}, {"id": 228, "seek": 253600, "start": 2544.0, "end": 2556.0, "text": " And so this is how you pre train the model you have all these sentences and 15% of the words are changed in a certain way and the model has to predict what the correct word is at a certain position.", "tokens": [50764, 400, 370, 341, 307, 577, 291, 659, 3847, 264, 2316, 291, 362, 439, 613, 16579, 293, 2119, 4, 295, 264, 2283, 366, 3105, 294, 257, 1629, 636, 293, 264, 2316, 575, 281, 6069, 437, 264, 3006, 1349, 307, 412, 257, 1629, 2535, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1391799807548523, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.18209141492843628}, {"id": 229, "seek": 255600, "start": 2556.0, "end": 2564.0, "text": " For example, it has to fill in the right word if there's a mask, or it has to detect whether this word is right or wrong.", "tokens": [50364, 1171, 1365, 11, 309, 575, 281, 2836, 294, 264, 558, 1349, 498, 456, 311, 257, 6094, 11, 420, 309, 575, 281, 5531, 1968, 341, 1349, 307, 558, 420, 2085, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12285009685315584, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.15178482234477997}, {"id": 230, "seek": 255600, "start": 2564.0, "end": 2581.0, "text": " And then, sometimes it's unchanged so why would you have this unchanged. So this is kind of important for the model in order to perform well during inference when you train the model and you want to use it in the real world, the real world, you usually don't have masked sentences.", "tokens": [50764, 400, 550, 11, 2171, 309, 311, 44553, 370, 983, 576, 291, 362, 341, 44553, 13, 407, 341, 307, 733, 295, 1021, 337, 264, 2316, 294, 1668, 281, 2042, 731, 1830, 38253, 562, 291, 3847, 264, 2316, 293, 291, 528, 281, 764, 309, 294, 264, 957, 1002, 11, 264, 957, 1002, 11, 291, 2673, 500, 380, 362, 45249, 16579, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12285009685315584, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.15178482234477997}, {"id": 231, "seek": 258100, "start": 2581.0, "end": 2598.0, "text": " But also then basically learns that sometimes it should not do anything so in order to work well or some real text which doesn't have masks, otherwise it would always expect okay there are some random words or masked words so in this way they found that this performs better if they have also 10% of the", "tokens": [50364, 583, 611, 550, 1936, 27152, 300, 2171, 309, 820, 406, 360, 1340, 370, 294, 1668, 281, 589, 731, 420, 512, 957, 2487, 597, 1177, 380, 362, 11830, 11, 5911, 309, 576, 1009, 2066, 1392, 456, 366, 512, 4974, 2283, 420, 45249, 2283, 370, 294, 341, 636, 436, 1352, 300, 341, 26213, 1101, 498, 436, 362, 611, 1266, 4, 295, 264, 51214], "temperature": 0.0, "avg_logprob": -0.15210512280464172, "compression_ratio": 1.578125, "no_speech_prob": 0.028832288458943367}, {"id": 232, "seek": 259800, "start": 2598.0, "end": 2601.0, "text": " or marked words unchanged.", "tokens": [50364, 420, 12658, 2283, 44553, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2173776291964347, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.016650306060910225}, {"id": 233, "seek": 259800, "start": 2601.0, "end": 2618.0, "text": " And the second pre training tasks that they do at the same time is next sentence prediction so they have a sentence a and a sentence be separated by a token, and the model has to predict whether sentence be indeed follows sentence a.", "tokens": [50514, 400, 264, 1150, 659, 3097, 9608, 300, 436, 360, 412, 264, 912, 565, 307, 958, 8174, 17630, 370, 436, 362, 257, 8174, 257, 293, 257, 8174, 312, 12005, 538, 257, 14862, 11, 293, 264, 2316, 575, 281, 6069, 1968, 8174, 312, 6451, 10002, 8174, 257, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2173776291964347, "compression_ratio": 1.6560509554140128, "no_speech_prob": 0.016650306060910225}, {"id": 234, "seek": 261800, "start": 2618.0, "end": 2628.0, "text": " So if you have a text and you reverse the order of the sentences the model learns the correct order and this helps the model yet to work with inputs that are more than one sentence.", "tokens": [50364, 407, 498, 291, 362, 257, 2487, 293, 291, 9943, 264, 1668, 295, 264, 16579, 264, 2316, 27152, 264, 3006, 1668, 293, 341, 3665, 264, 2316, 1939, 281, 589, 365, 15743, 300, 366, 544, 813, 472, 8174, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11425361794940496, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.006095225922763348}, {"id": 235, "seek": 261800, "start": 2628.0, "end": 2635.0, "text": " So it's essentially classification a binary classification is next or is not next essentially.", "tokens": [50864, 407, 309, 311, 4476, 21538, 257, 17434, 21538, 307, 958, 420, 307, 406, 958, 4476, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11425361794940496, "compression_ratio": 1.7468354430379747, "no_speech_prob": 0.006095225922763348}, {"id": 236, "seek": 263500, "start": 2635.0, "end": 2650.0, "text": " The classification token is used as a placeholder for generating the output, because the number of inputs also matches the number of outputs, but in bird we are not interested in generating new text, we are essentially interested in classification.", "tokens": [50364, 440, 21538, 14862, 307, 1143, 382, 257, 1081, 20480, 337, 17746, 264, 5598, 11, 570, 264, 1230, 295, 15743, 611, 10676, 264, 1230, 295, 23930, 11, 457, 294, 5255, 321, 366, 406, 3102, 294, 17746, 777, 2487, 11, 321, 366, 4476, 3102, 294, 21538, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11114664872487386, "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.265350878238678}, {"id": 237, "seek": 263500, "start": 2650.0, "end": 2656.0, "text": " So how that works is just a brief overview of the different types of tasks you can do with bird.", "tokens": [51114, 407, 577, 300, 1985, 307, 445, 257, 5353, 12492, 295, 264, 819, 3467, 295, 9608, 291, 393, 360, 365, 5255, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11114664872487386, "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.265350878238678}, {"id": 238, "seek": 265600, "start": 2656.0, "end": 2660.0, "text": " There is a sentence pair classification.", "tokens": [50364, 821, 307, 257, 8174, 6119, 21538, 13, 50564], "temperature": 0.0, "avg_logprob": -0.19652549556044283, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.1190737634897232}, {"id": 239, "seek": 265600, "start": 2660.0, "end": 2678.0, "text": " One is just a single sentence classification question answering and single sentence tagging so here tagging is, for example, if you think about something like language, a grammar software you can label a sentence by on what type of like a word or", "tokens": [50564, 1485, 307, 445, 257, 2167, 8174, 21538, 1168, 13430, 293, 2167, 8174, 6162, 3249, 370, 510, 6162, 3249, 307, 11, 337, 1365, 11, 498, 291, 519, 466, 746, 411, 2856, 11, 257, 22317, 4722, 291, 393, 7645, 257, 8174, 538, 322, 437, 2010, 295, 411, 257, 1349, 420, 51464], "temperature": 0.0, "avg_logprob": -0.19652549556044283, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.1190737634897232}, {"id": 240, "seek": 267800, "start": 2679.0, "end": 2688.0, "text": " word or grammar each word corresponds to, but let's focus maybe only here on the, because we have for the interest of time on the single sentence classification task.", "tokens": [50414, 1349, 420, 22317, 1184, 1349, 23249, 281, 11, 457, 718, 311, 1879, 1310, 787, 510, 322, 264, 11, 570, 321, 362, 337, 264, 1179, 295, 565, 322, 264, 2167, 8174, 21538, 5633, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14539003372192383, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.042680852115154266}, {"id": 241, "seek": 267800, "start": 2688.0, "end": 2704.0, "text": " So you provided an input sentence, you have some embedding here, and then you provide some output. So you have during training you have all these tokens so it has to predict what the input is, given that something is masked or not.", "tokens": [50864, 407, 291, 5649, 364, 4846, 8174, 11, 291, 362, 512, 12240, 3584, 510, 11, 293, 550, 291, 2893, 512, 5598, 13, 407, 291, 362, 1830, 3097, 291, 362, 439, 613, 22667, 370, 309, 575, 281, 6069, 437, 264, 4846, 307, 11, 2212, 300, 746, 307, 45249, 420, 406, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14539003372192383, "compression_ratio": 1.7155172413793103, "no_speech_prob": 0.042680852115154266}, {"id": 242, "seek": 270400, "start": 2704.0, "end": 2710.0, "text": " If you have a prediction task you don't I mean it still generates a sentence but you don't do anything with it.", "tokens": [50364, 759, 291, 362, 257, 17630, 5633, 291, 500, 380, 286, 914, 309, 920, 23815, 257, 8174, 457, 291, 500, 380, 360, 1340, 365, 309, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0881959954086615, "compression_ratio": 1.8471074380165289, "no_speech_prob": 0.0014774218434467912}, {"id": 243, "seek": 270400, "start": 2710.0, "end": 2717.0, "text": " What you care about is this class table here so this is why you have this classification token. This goes then into the class table that you care about.", "tokens": [50664, 708, 291, 1127, 466, 307, 341, 1508, 3199, 510, 370, 341, 307, 983, 291, 362, 341, 21538, 14862, 13, 639, 1709, 550, 666, 264, 1508, 3199, 300, 291, 1127, 466, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0881959954086615, "compression_ratio": 1.8471074380165289, "no_speech_prob": 0.0014774218434467912}, {"id": 244, "seek": 270400, "start": 2717.0, "end": 2729.0, "text": " And this is something we can leverage for arbitrary classification. So I prepared a code example for fine tuning such a bird model for this single sentence classification in pytorch.", "tokens": [51014, 400, 341, 307, 746, 321, 393, 13982, 337, 23211, 21538, 13, 407, 286, 4927, 257, 3089, 1365, 337, 2489, 15164, 1270, 257, 5255, 2316, 337, 341, 2167, 8174, 21538, 294, 25878, 284, 339, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0881959954086615, "compression_ratio": 1.8471074380165289, "no_speech_prob": 0.0014774218434467912}, {"id": 245, "seek": 272900, "start": 2729.0, "end": 2734.0, "text": " And it says single sentence classification and for the code example.", "tokens": [50364, 400, 309, 1619, 2167, 8174, 21538, 293, 337, 264, 3089, 1365, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17624862492084503, "compression_ratio": 1.4906832298136645, "no_speech_prob": 0.0033234667498618364}, {"id": 246, "seek": 272900, "start": 2734.0, "end": 2746.0, "text": " I have this large movie review data set. I mean, back then it was large but it's just a 50,000 movie reviews, and I think 50 25,000 in the training and 25,000 in test set.", "tokens": [50614, 286, 362, 341, 2416, 3169, 3131, 1412, 992, 13, 286, 914, 11, 646, 550, 309, 390, 2416, 457, 309, 311, 445, 257, 2625, 11, 1360, 3169, 10229, 11, 293, 286, 519, 2625, 3552, 11, 1360, 294, 264, 3097, 293, 3552, 11, 1360, 294, 1500, 992, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17624862492084503, "compression_ratio": 1.4906832298136645, "no_speech_prob": 0.0033234667498618364}, {"id": 247, "seek": 274600, "start": 2746.0, "end": 2763.0, "text": " And this is essentially movies movie reviews from the IMDB movie review database, and these are multiple sentences. So we can also use bird for multiple sentences, even though let's say it says single sentence classification, but just concatenating these", "tokens": [50364, 400, 341, 307, 4476, 6233, 3169, 10229, 490, 264, 21463, 27735, 3169, 3131, 8149, 11, 293, 613, 366, 3866, 16579, 13, 407, 321, 393, 611, 764, 5255, 337, 3866, 16579, 11, 754, 1673, 718, 311, 584, 309, 1619, 2167, 8174, 21538, 11, 457, 445, 1588, 7186, 990, 613, 51214], "temperature": 0.0, "avg_logprob": -0.1850143946134127, "compression_ratio": 1.5393939393939393, "no_speech_prob": 0.007457092870026827}, {"id": 248, "seek": 276300, "start": 2763.0, "end": 2777.0, "text": " sentences. So in that way, the only difference is we don't use this separator token because the separator token is really more for comparing two things like this sentence pair classification so this one can for example say whether these sentences are similar or not like", "tokens": [50364, 16579, 13, 407, 294, 300, 636, 11, 264, 787, 2649, 307, 321, 500, 380, 764, 341, 3128, 1639, 14862, 570, 264, 3128, 1639, 14862, 307, 534, 544, 337, 15763, 732, 721, 411, 341, 8174, 6119, 21538, 370, 341, 472, 393, 337, 1365, 584, 1968, 613, 16579, 366, 2531, 420, 406, 411, 51064], "temperature": 0.0, "avg_logprob": -0.17588213407076322, "compression_ratio": 1.7808988764044944, "no_speech_prob": 0.04737860709428787}, {"id": 249, "seek": 276300, "start": 2777.0, "end": 2782.0, "text": " some sentence similarity and things like that.", "tokens": [51064, 512, 8174, 32194, 293, 721, 411, 300, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17588213407076322, "compression_ratio": 1.7808988764044944, "no_speech_prob": 0.04737860709428787}, {"id": 250, "seek": 278200, "start": 2782.0, "end": 2799.0, "text": " Yeah, so I'm not sure how much time we have I could maybe briefly went way over. I can maybe briefly show you the code example. It's actually not too complicated. I have it on GitHub. I have some annotation.", "tokens": [50364, 865, 11, 370, 286, 478, 406, 988, 577, 709, 565, 321, 362, 286, 727, 1310, 10515, 1437, 636, 670, 13, 286, 393, 1310, 10515, 855, 291, 264, 3089, 1365, 13, 467, 311, 767, 406, 886, 6179, 13, 286, 362, 309, 322, 23331, 13, 286, 362, 512, 48654, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10706654878763053, "compression_ratio": 1.4375, "no_speech_prob": 0.02840885892510414}, {"id": 251, "seek": 279900, "start": 2799.0, "end": 2812.0, "text": " It's based on a library called hugging face. It's a very, very, very popular library for or you probably can't see it now have to reshare my screen. Okay.", "tokens": [50364, 467, 311, 2361, 322, 257, 6405, 1219, 41706, 1851, 13, 467, 311, 257, 588, 11, 588, 11, 588, 3743, 6405, 337, 420, 291, 1391, 393, 380, 536, 309, 586, 362, 281, 725, 31932, 452, 2568, 13, 1033, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14375434687108168, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.006386306136846542}, {"id": 252, "seek": 279900, "start": 2812.0, "end": 2819.0, "text": " Let me see how many slides I have left. So we probably don't have to cover this.", "tokens": [51014, 961, 385, 536, 577, 867, 9788, 286, 362, 1411, 13, 407, 321, 1391, 500, 380, 362, 281, 2060, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14375434687108168, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.006386306136846542}, {"id": 253, "seek": 279900, "start": 2819.0, "end": 2826.0, "text": " Let me share my browser screen so you can briefly see the code example.", "tokens": [51364, 961, 385, 2073, 452, 11185, 2568, 370, 291, 393, 10515, 536, 264, 3089, 1365, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14375434687108168, "compression_ratio": 1.5906735751295338, "no_speech_prob": 0.006386306136846542}, {"id": 254, "seek": 282600, "start": 2826.0, "end": 2830.0, "text": " Here I opened it from GitHub.", "tokens": [50364, 1692, 286, 5625, 309, 490, 23331, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3432268662886186, "compression_ratio": 1.1414141414141414, "no_speech_prob": 0.008840500377118587}, {"id": 255, "seek": 282600, "start": 2830.0, "end": 2838.0, "text": " I've also my Jupyter left version which is maybe better because I can zoom in here.", "tokens": [50564, 286, 600, 611, 452, 22125, 88, 391, 1411, 3037, 597, 307, 1310, 1101, 570, 286, 393, 8863, 294, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3432268662886186, "compression_ratio": 1.1414141414141414, "no_speech_prob": 0.008840500377118587}, {"id": 256, "seek": 283800, "start": 2839.0, "end": 2857.0, "text": " So here is a notebook for downloading a pre trained bird model and fine tuning it on the movie review classification. There's a lot of boilerplate code here like importing the libraries, some settings for reproducible reproducibility, downloading the data set.", "tokens": [50414, 407, 510, 307, 257, 21060, 337, 32529, 257, 659, 8895, 5255, 2316, 293, 2489, 15164, 309, 322, 264, 3169, 3131, 21538, 13, 821, 311, 257, 688, 295, 39228, 37008, 3089, 510, 411, 43866, 264, 15148, 11, 512, 6257, 337, 11408, 32128, 11408, 537, 39802, 11, 32529, 264, 1412, 992, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17015455387256764, "compression_ratio": 1.5568862275449102, "no_speech_prob": 0.15585875511169434}, {"id": 257, "seek": 285700, "start": 2858.0, "end": 2878.0, "text": " And then processing it. So here's a pen us data frame of how the data set looks like there's like a text a movie review, and then it's whether it's positive or negative, like a binary classification, 50,000 combined, and here I'm just spitting them into training and validation and test sets.", "tokens": [50414, 400, 550, 9007, 309, 13, 407, 510, 311, 257, 3435, 505, 1412, 3920, 295, 577, 264, 1412, 992, 1542, 411, 456, 311, 411, 257, 2487, 257, 3169, 3131, 11, 293, 550, 309, 311, 1968, 309, 311, 3353, 420, 3671, 11, 411, 257, 17434, 21538, 11, 2625, 11, 1360, 9354, 11, 293, 510, 286, 478, 445, 637, 2414, 552, 666, 3097, 293, 24071, 293, 1500, 6352, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22843284606933595, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.11575976759195328}, {"id": 258, "seek": 287800, "start": 2878.0, "end": 2898.0, "text": " So here's a tokenization step. So tokenization goes from taking the words and then converting them also, sorry, taking the sentence and chopping it into words, and then also taking care of punctuation so punctuation is also, I think, given token, but it really depends on what type of tokenizer you use.", "tokens": [50364, 407, 510, 311, 257, 14862, 2144, 1823, 13, 407, 14862, 2144, 1709, 490, 1940, 264, 2283, 293, 550, 29942, 552, 611, 11, 2597, 11, 1940, 264, 8174, 293, 35205, 309, 666, 2283, 11, 293, 550, 611, 1940, 1127, 295, 27006, 16073, 370, 27006, 16073, 307, 611, 11, 286, 519, 11, 2212, 14862, 11, 457, 309, 534, 5946, 322, 437, 2010, 295, 14862, 6545, 291, 764, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21547107696533202, "compression_ratio": 1.7314285714285715, "no_speech_prob": 0.25943467020988464}, {"id": 259, "seek": 289800, "start": 2898.0, "end": 2910.0, "text": " So there's a collection of different tokenizers out there they all behave a little bit differently, but I recommend if you use, let's say, on hugging face. I recommend on finding a tokenizer that matches your model.", "tokens": [50364, 407, 456, 311, 257, 5765, 295, 819, 14862, 22525, 484, 456, 436, 439, 15158, 257, 707, 857, 7614, 11, 457, 286, 2748, 498, 291, 764, 11, 718, 311, 584, 11, 322, 41706, 1851, 13, 286, 2748, 322, 5006, 257, 14862, 6545, 300, 10676, 428, 2316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2660352022219927, "compression_ratio": 1.6091370558375635, "no_speech_prob": 0.3694775402545929}, {"id": 260, "seek": 289800, "start": 2910.0, "end": 2916.0, "text": " I should say we are using here, not birth, but the silver I think I had a note about that at the top.", "tokens": [50964, 286, 820, 584, 321, 366, 1228, 510, 11, 406, 3965, 11, 457, 264, 8753, 286, 519, 286, 632, 257, 3637, 466, 300, 412, 264, 1192, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2660352022219927, "compression_ratio": 1.6091370558375635, "no_speech_prob": 0.3694775402545929}, {"id": 261, "seek": 291600, "start": 2916.0, "end": 2933.0, "text": " This is essentially a smaller version of work because if you only have let's say one GPU like 1080 TI or 2080 TI like a graphics card with only 11 gigabyte memory, you or I couldn't at least log the whole work model into memory.", "tokens": [50364, 639, 307, 4476, 257, 4356, 3037, 295, 589, 570, 498, 291, 787, 362, 718, 311, 584, 472, 18407, 411, 1266, 4702, 28819, 420, 945, 4702, 28819, 411, 257, 11837, 2920, 365, 787, 2975, 8741, 34529, 4675, 11, 291, 420, 286, 2809, 380, 412, 1935, 3565, 264, 1379, 589, 2316, 666, 4675, 13, 51214], "temperature": 0.0, "avg_logprob": -0.30677444594247, "compression_ratio": 1.3902439024390243, "no_speech_prob": 0.07915452122688293}, {"id": 262, "seek": 293300, "start": 2933.0, "end": 2937.0, "text": " I think I could just barely load it but they're not training.", "tokens": [50364, 286, 519, 286, 727, 445, 10268, 3677, 309, 457, 436, 434, 406, 3097, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18863121079809872, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.03356963396072388}, {"id": 263, "seek": 293300, "start": 2937.0, "end": 2957.0, "text": " So there's a version called the silver which is a little bit smaller so in the silver what they did is they trained the bird model, and then they removed some weights, but preserving its performance it has 95% of the performance of work, but it's more lightweight it has 40% fewer parameters.", "tokens": [50564, 407, 456, 311, 257, 3037, 1219, 264, 8753, 597, 307, 257, 707, 857, 4356, 370, 294, 264, 8753, 437, 436, 630, 307, 436, 8895, 264, 5255, 2316, 11, 293, 550, 436, 7261, 512, 17443, 11, 457, 33173, 1080, 3389, 309, 575, 13420, 4, 295, 264, 3389, 295, 589, 11, 457, 309, 311, 544, 22052, 309, 575, 3356, 4, 13366, 9834, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18863121079809872, "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.03356963396072388}, {"id": 264, "seek": 295700, "start": 2957.0, "end": 2969.0, "text": " So, we are using a tokenizer that is made for this, the still bird model, and then encode the data set into tokens.", "tokens": [50364, 407, 11, 321, 366, 1228, 257, 14862, 6545, 300, 307, 1027, 337, 341, 11, 264, 920, 5255, 2316, 11, 293, 550, 2058, 1429, 264, 1412, 992, 666, 22667, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14555014396200375, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.024777697399258614}, {"id": 265, "seek": 295700, "start": 2969.0, "end": 2983.0, "text": " And then I have my data loader here in pytorch. So this is a custom data loader here there's nothing really special about it. If you would build the data loader for any type of image data set for example, except that we on the way we process essentially on encodings.", "tokens": [50964, 400, 550, 286, 362, 452, 1412, 3677, 260, 510, 294, 25878, 284, 339, 13, 407, 341, 307, 257, 2375, 1412, 3677, 260, 510, 456, 311, 1825, 534, 2121, 466, 309, 13, 759, 291, 576, 1322, 264, 1412, 3677, 260, 337, 604, 2010, 295, 3256, 1412, 992, 337, 1365, 11, 3993, 300, 321, 322, 264, 636, 321, 1399, 4476, 322, 2058, 378, 1109, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14555014396200375, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.024777697399258614}, {"id": 266, "seek": 298300, "start": 2983.0, "end": 2995.0, "text": " So that's like a special on representation that is, I think it's kind of like a dictionary it's our own Python. So yeah a Python object or class, but it kind of behaves like a dictionary you can index into that.", "tokens": [50364, 407, 300, 311, 411, 257, 2121, 322, 10290, 300, 307, 11, 286, 519, 309, 311, 733, 295, 411, 257, 25890, 309, 311, 527, 1065, 15329, 13, 407, 1338, 257, 15329, 2657, 420, 1508, 11, 457, 309, 733, 295, 36896, 411, 257, 25890, 291, 393, 8186, 666, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15421480398911697, "compression_ratio": 1.8270042194092826, "no_speech_prob": 0.023656634613871574}, {"id": 267, "seek": 298300, "start": 2995.0, "end": 3001.0, "text": " And I will show you how that looks like. So yeah here's an example you can have a.", "tokens": [50964, 400, 286, 486, 855, 291, 577, 300, 1542, 411, 13, 407, 1338, 510, 311, 364, 1365, 291, 393, 362, 257, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15421480398911697, "compression_ratio": 1.8270042194092826, "no_speech_prob": 0.023656634613871574}, {"id": 268, "seek": 298300, "start": 3001.0, "end": 3012.0, "text": " So this is this sorry this will create a dictionary right, but I think itself it's also some certain object that contains multiple things.", "tokens": [51264, 407, 341, 307, 341, 2597, 341, 486, 1884, 257, 25890, 558, 11, 457, 286, 519, 2564, 309, 311, 611, 512, 1629, 2657, 300, 8306, 3866, 721, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15421480398911697, "compression_ratio": 1.8270042194092826, "no_speech_prob": 0.023656634613871574}, {"id": 269, "seek": 301200, "start": 3013.0, "end": 3026.0, "text": " Yeah, so here I'm just creating data loaders from the data set. So this is basically all just set up. And here's the main part where we are loading the still bird model, and they have multiple models on their website if you go to the hugging face", "tokens": [50414, 865, 11, 370, 510, 286, 478, 445, 4084, 1412, 3677, 433, 490, 264, 1412, 992, 13, 407, 341, 307, 1936, 439, 445, 992, 493, 13, 400, 510, 311, 264, 2135, 644, 689, 321, 366, 15114, 264, 920, 5255, 2316, 11, 293, 436, 362, 3866, 5245, 322, 641, 3144, 498, 291, 352, 281, 264, 41706, 1851, 51064], "temperature": 0.0, "avg_logprob": -0.1277698044924392, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.004329047631472349}, {"id": 270, "seek": 301200, "start": 3026.0, "end": 3037.0, "text": " transformer website. And here this would be for example a distilled bird model for sentence or sequence classification you can basically tell based on the class name, what this is made for.", "tokens": [51064, 31782, 3144, 13, 400, 510, 341, 576, 312, 337, 1365, 257, 1483, 6261, 5255, 2316, 337, 8174, 420, 8310, 21538, 291, 393, 1936, 980, 2361, 322, 264, 1508, 1315, 11, 437, 341, 307, 1027, 337, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1277698044924392, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.004329047631472349}, {"id": 271, "seek": 303700, "start": 3037.0, "end": 3048.0, "text": " And then I'm loading it the pre trained version and they have also again, I think they have different ones uncased you would mean it would ignore sentence case whether it's upper or lower case.", "tokens": [50364, 400, 550, 286, 478, 15114, 309, 264, 659, 8895, 3037, 293, 436, 362, 611, 797, 11, 286, 519, 436, 362, 819, 2306, 6219, 1937, 291, 576, 914, 309, 576, 11200, 8174, 1389, 1968, 309, 311, 6597, 420, 3126, 1389, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1706932500465629, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.002216201275587082}, {"id": 272, "seek": 303700, "start": 3048.0, "end": 3056.0, "text": " Yeah, and then I'm putting the model on my device, putting it into training mode and then initializing an optimizer for back propagation.", "tokens": [50914, 865, 11, 293, 550, 286, 478, 3372, 264, 2316, 322, 452, 4302, 11, 3372, 309, 666, 3097, 4391, 293, 550, 5883, 3319, 364, 5028, 6545, 337, 646, 38377, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1706932500465629, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.002216201275587082}, {"id": 273, "seek": 303700, "start": 3056.0, "end": 3059.0, "text": " This is just a warning it's.", "tokens": [51314, 639, 307, 445, 257, 9164, 309, 311, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1706932500465629, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.002216201275587082}, {"id": 274, "seek": 303700, "start": 3059.0, "end": 3063.0, "text": " I don't exactly know what they want here.", "tokens": [51464, 286, 500, 380, 2293, 458, 437, 436, 528, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1706932500465629, "compression_ratio": 1.6408163265306122, "no_speech_prob": 0.002216201275587082}, {"id": 275, "seek": 306300, "start": 3063.0, "end": 3078.0, "text": " I think it's just some hints. It's nothing really we have to be concerned about. And here I implemented the accuracy function to add so what I'm showing you is like a manual training how basically how you would implement this and let's say your", "tokens": [50364, 286, 519, 309, 311, 445, 512, 27271, 13, 467, 311, 1825, 534, 321, 362, 281, 312, 5922, 466, 13, 400, 510, 286, 12270, 264, 14170, 2445, 281, 909, 370, 437, 286, 478, 4099, 291, 307, 411, 257, 9688, 3097, 577, 1936, 577, 291, 576, 4445, 341, 293, 718, 311, 584, 428, 51114], "temperature": 0.0, "avg_logprob": -0.16397340562608506, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0035915682092309}, {"id": 276, "seek": 306300, "start": 3078.0, "end": 3086.0, "text": " own pytorch code. And then at the end I will also show you trainer class I mean we are almost done it's just the last little bit here.", "tokens": [51114, 1065, 25878, 284, 339, 3089, 13, 400, 550, 412, 264, 917, 286, 486, 611, 855, 291, 21110, 1508, 286, 914, 321, 366, 1920, 1096, 309, 311, 445, 264, 1036, 707, 857, 510, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16397340562608506, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.0035915682092309}, {"id": 277, "seek": 308600, "start": 3086.0, "end": 3099.0, "text": " So what I have here in this accuracy function how I write my accuracy function is usually that I law the data in batches because I usually have large data sets and you can just put the whole data set into the model as input because you would have", "tokens": [50364, 407, 437, 286, 362, 510, 294, 341, 14170, 2445, 577, 286, 2464, 452, 14170, 2445, 307, 2673, 300, 286, 2101, 264, 1412, 294, 15245, 279, 570, 286, 2673, 362, 2416, 1412, 6352, 293, 291, 393, 445, 829, 264, 1379, 1412, 992, 666, 264, 2316, 382, 4846, 570, 291, 576, 362, 51014], "temperature": 0.0, "avg_logprob": -0.12331876931367097, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.03730256110429764}, {"id": 278, "seek": 309900, "start": 3099.0, "end": 3110.0, "text": " a large matrix multiplication and then you run out of GPU memory. So what I usually do is I bet like similar to training I batch up my data set into chunks into these batches.", "tokens": [50364, 257, 2416, 8141, 27290, 293, 550, 291, 1190, 484, 295, 18407, 4675, 13, 407, 437, 286, 2673, 360, 307, 286, 778, 411, 2531, 281, 3097, 286, 15245, 493, 452, 1412, 992, 666, 24004, 666, 613, 15245, 279, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1126841012533609, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.1401277780532837}, {"id": 279, "seek": 309900, "start": 3110.0, "end": 3121.0, "text": " It's automatically done by the data loader here, and then I compute the predicted labels and collect them. So I'm just collecting the number of correct predictions.", "tokens": [50914, 467, 311, 6772, 1096, 538, 264, 1412, 3677, 260, 510, 11, 293, 550, 286, 14722, 264, 19147, 16949, 293, 2500, 552, 13, 407, 286, 478, 445, 12510, 264, 1230, 295, 3006, 21264, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1126841012533609, "compression_ratio": 1.566820276497696, "no_speech_prob": 0.1401277780532837}, {"id": 280, "seek": 312100, "start": 3121.0, "end": 3133.0, "text": " So I have summed up all the correct predictions. I divide by the number of training examples. So I keep track of the number of examples, and use that to divide the number of correct predictions and that gives me the accuracy.", "tokens": [50364, 407, 286, 362, 2408, 1912, 493, 439, 264, 3006, 21264, 13, 286, 9845, 538, 264, 1230, 295, 3097, 5110, 13, 407, 286, 1066, 2837, 295, 264, 1230, 295, 5110, 11, 293, 764, 300, 281, 9845, 264, 1230, 295, 3006, 21264, 293, 300, 2709, 385, 264, 14170, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10969253162761311, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.03512635454535484}, {"id": 281, "seek": 312100, "start": 3133.0, "end": 3144.0, "text": " So it's in that way, not requiring to load all the data all at once. And yeah there are some interesting things here going on so we have the input IDs of the works.", "tokens": [50964, 407, 309, 311, 294, 300, 636, 11, 406, 24165, 281, 3677, 439, 264, 1412, 439, 412, 1564, 13, 400, 1338, 456, 366, 512, 1880, 721, 510, 516, 322, 370, 321, 362, 264, 4846, 48212, 295, 264, 1985, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10969253162761311, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.03512635454535484}, {"id": 282, "seek": 314400, "start": 3145.0, "end": 3156.0, "text": " The tension mask. So the tension mask is sort of weird here why do we need a tension mask. So it's essentially for padding so if we have sentences of different lengths.", "tokens": [50414, 440, 8980, 6094, 13, 407, 264, 8980, 6094, 307, 1333, 295, 3657, 510, 983, 360, 321, 643, 257, 8980, 6094, 13, 407, 309, 311, 4476, 337, 39562, 370, 498, 321, 362, 16579, 295, 819, 26329, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18695720945085798, "compression_ratio": 1.7751479289940828, "no_speech_prob": 0.022616827860474586}, {"id": 283, "seek": 314400, "start": 3156.0, "end": 3166.0, "text": " It will use zeros to do a padding so it's essentially denoting which character is a padding character, and which is an actual word.", "tokens": [50964, 467, 486, 764, 35193, 281, 360, 257, 39562, 370, 309, 311, 4476, 1441, 17001, 597, 2517, 307, 257, 39562, 2517, 11, 293, 597, 307, 364, 3539, 1349, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18695720945085798, "compression_ratio": 1.7751479289940828, "no_speech_prob": 0.022616827860474586}, {"id": 284, "seek": 316600, "start": 3166.0, "end": 3176.0, "text": " So we have our class labels, and we provide as a model input, the input IDs, the words. So the model itself will then do the word embedding, and then the attention mask.", "tokens": [50364, 407, 321, 362, 527, 1508, 16949, 11, 293, 321, 2893, 382, 257, 2316, 4846, 11, 264, 4846, 48212, 11, 264, 2283, 13, 407, 264, 2316, 2564, 486, 550, 360, 264, 1349, 12240, 3584, 11, 293, 550, 264, 3202, 6094, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1836450078465917, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.02296346053481102}, {"id": 285, "seek": 316600, "start": 3176.0, "end": 3184.0, "text": " And then we obtain from the outputs the output is kind of I think it's an object but you can index into it might be a dictionary.", "tokens": [50864, 400, 550, 321, 12701, 490, 264, 23930, 264, 5598, 307, 733, 295, 286, 519, 309, 311, 364, 2657, 457, 291, 393, 8186, 666, 309, 1062, 312, 257, 25890, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1836450078465917, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.02296346053481102}, {"id": 286, "seek": 316600, "start": 3184.0, "end": 3192.0, "text": " So you get the logits, which you can then use. You could use a softmax function to get probabilities but it's not necessary, because the largest largest.", "tokens": [51264, 407, 291, 483, 264, 3565, 1208, 11, 597, 291, 393, 550, 764, 13, 509, 727, 764, 257, 2787, 41167, 2445, 281, 483, 33783, 457, 309, 311, 406, 4818, 11, 570, 264, 6443, 6443, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1836450078465917, "compression_ratio": 1.709433962264151, "no_speech_prob": 0.02296346053481102}, {"id": 287, "seek": 319200, "start": 3192.0, "end": 3198.0, "text": " And also the largest probability in softmax. So you get the class table with arc max.", "tokens": [50364, 400, 611, 264, 6443, 8482, 294, 2787, 41167, 13, 407, 291, 483, 264, 1508, 3199, 365, 10346, 11469, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2433325354732684, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.0010479846969246864}, {"id": 288, "seek": 319200, "start": 3198.0, "end": 3208.0, "text": " It's your class table and then you know I was just checking whether the predicted label is matching the correct label, the actual label.", "tokens": [50664, 467, 311, 428, 1508, 3199, 293, 550, 291, 458, 286, 390, 445, 8568, 1968, 264, 19147, 7645, 307, 14324, 264, 3006, 7645, 11, 264, 3539, 7645, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2433325354732684, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.0010479846969246864}, {"id": 289, "seek": 319200, "start": 3208.0, "end": 3212.0, "text": " Okay, um, this is just how I compute the accuracy.", "tokens": [51164, 1033, 11, 1105, 11, 341, 307, 445, 577, 286, 14722, 264, 14170, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2433325354732684, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.0010479846969246864}, {"id": 290, "seek": 321200, "start": 3212.0, "end": 3223.0, "text": " This is my training group. So I usually have a very simple training group because when I do research I like to tinker with things so I like to have like a more manual approach.", "tokens": [50364, 639, 307, 452, 3097, 1594, 13, 407, 286, 2673, 362, 257, 588, 2199, 3097, 1594, 570, 562, 286, 360, 2132, 286, 411, 281, 256, 40467, 365, 721, 370, 286, 411, 281, 362, 411, 257, 544, 9688, 3109, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18759316489810035, "compression_ratio": 1.4193548387096775, "no_speech_prob": 0.03112468682229519}, {"id": 291, "seek": 322300, "start": 3223.0, "end": 3243.0, "text": " And this works similarly I obtained my input IDs from the data loader, if my attention mask labels, compute the outputs here in addition to, to the logits I also need the loss for optimization so you have to provide also to the class labels as the model input.", "tokens": [50364, 400, 341, 1985, 14138, 286, 14879, 452, 4846, 48212, 490, 264, 1412, 3677, 260, 11, 498, 452, 3202, 6094, 16949, 11, 14722, 264, 23930, 510, 294, 4500, 281, 11, 281, 264, 3565, 1208, 286, 611, 643, 264, 4470, 337, 19618, 370, 291, 362, 281, 2893, 611, 281, 264, 1508, 16949, 382, 264, 2316, 4846, 13, 51364], "temperature": 0.0, "avg_logprob": -0.25120811139122917, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.016397574916481972}, {"id": 292, "seek": 324300, "start": 3243.0, "end": 3252.0, "text": " So you get both the logits and the loss, and then like in regular pytorch I use backward on the loss and optimize and this is just for logging purposes.", "tokens": [50364, 407, 291, 483, 1293, 264, 3565, 1208, 293, 264, 4470, 11, 293, 550, 411, 294, 3890, 25878, 284, 339, 286, 764, 23897, 322, 264, 4470, 293, 19719, 293, 341, 307, 445, 337, 27991, 9932, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18224658966064453, "compression_ratio": 1.6, "no_speech_prob": 0.0618470273911953}, {"id": 293, "seek": 324300, "start": 3252.0, "end": 3264.0, "text": " So when I do that on my data set it trained for like an hour, and get gets like a 91% accuracy. One interesting thing is about that briefly I tried an RNN on that and I think I got like 88% accuracy.", "tokens": [50814, 407, 562, 286, 360, 300, 322, 452, 1412, 992, 309, 8895, 337, 411, 364, 1773, 11, 293, 483, 2170, 411, 257, 31064, 4, 14170, 13, 1485, 1880, 551, 307, 466, 300, 10515, 286, 3031, 364, 45702, 45, 322, 300, 293, 286, 519, 286, 658, 411, 24587, 4, 14170, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18224658966064453, "compression_ratio": 1.6, "no_speech_prob": 0.0618470273911953}, {"id": 294, "seek": 326400, "start": 3264.0, "end": 3283.0, "text": " I was using this pre trained model, even though it's a small data set the MDB review data set even though it's small, I can use a pre trained transformer to fine tune it on the small data set to get both formats that is actually better than let's say logistic regression which was like 85% and an RNN which was like 88%.", "tokens": [50364, 286, 390, 1228, 341, 659, 8895, 2316, 11, 754, 1673, 309, 311, 257, 1359, 1412, 992, 264, 22521, 33, 3131, 1412, 992, 754, 1673, 309, 311, 1359, 11, 286, 393, 764, 257, 659, 8895, 31782, 281, 2489, 10864, 309, 322, 264, 1359, 1412, 992, 281, 483, 1293, 25879, 300, 307, 767, 1101, 813, 718, 311, 584, 3565, 3142, 24590, 597, 390, 411, 14695, 4, 293, 364, 45702, 45, 597, 390, 411, 24587, 6856, 51314], "temperature": 0.0, "avg_logprob": -0.15683131921486776, "compression_ratio": 1.7864768683274022, "no_speech_prob": 0.04811541736125946}, {"id": 295, "seek": 326400, "start": 3283.0, "end": 3293.0, "text": " And I didn't do any tuning here by the way I just used it one time because I was short on time I didn't do any learning rate tuning nothing so it worked kind of like out of the box.", "tokens": [51314, 400, 286, 994, 380, 360, 604, 15164, 510, 538, 264, 636, 286, 445, 1143, 309, 472, 565, 570, 286, 390, 2099, 322, 565, 286, 994, 380, 360, 604, 2539, 3314, 15164, 1825, 370, 309, 2732, 733, 295, 411, 484, 295, 264, 2424, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15683131921486776, "compression_ratio": 1.7864768683274022, "no_speech_prob": 0.04811541736125946}, {"id": 296, "seek": 329300, "start": 3293.0, "end": 3309.0, "text": " And then there's also, if you really want to use transformer seriously I recommend using on the APIs that hugging face on provides so they have a trainer class, where you specify options in the so called trainer arguments.", "tokens": [50364, 400, 550, 456, 311, 611, 11, 498, 291, 534, 528, 281, 764, 31782, 6638, 286, 2748, 1228, 322, 264, 21445, 300, 41706, 1851, 322, 6417, 370, 436, 362, 257, 21110, 1508, 11, 689, 291, 16500, 3956, 294, 264, 370, 1219, 21110, 12869, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1918649267643056, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.010978448204696178}, {"id": 297, "seek": 330900, "start": 3309.0, "end": 3324.0, "text": " And then you have this trainer class where you provide this as input and it handles everything automatically. So I also have a run here, and actually it got on even better performance I think they have some certain default parameters for regularization and so forth.", "tokens": [50364, 400, 550, 291, 362, 341, 21110, 1508, 689, 291, 2893, 341, 382, 4846, 293, 309, 18722, 1203, 6772, 13, 407, 286, 611, 362, 257, 1190, 510, 11, 293, 767, 309, 658, 322, 754, 1101, 3389, 286, 519, 436, 362, 512, 1629, 7576, 9834, 337, 3890, 2144, 293, 370, 5220, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12110108799404568, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.017978625372052193}, {"id": 298, "seek": 332400, "start": 3324.0, "end": 3333.0, "text": " So just first of all faster, and also the performance was a better I think they have better better on default options because my training was very minimal.", "tokens": [50364, 407, 445, 700, 295, 439, 4663, 11, 293, 611, 264, 3389, 390, 257, 1101, 286, 519, 436, 362, 1101, 1101, 322, 7576, 3956, 570, 452, 3097, 390, 588, 13206, 13, 50814], "temperature": 0.0, "avg_logprob": -0.20403046564224664, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.3496999442577362}, {"id": 299, "seek": 332400, "start": 3333.0, "end": 3338.0, "text": " So they have some additional options. And actually, I, for comparison.", "tokens": [50814, 407, 436, 362, 512, 4497, 3956, 13, 400, 767, 11, 286, 11, 337, 9660, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20403046564224664, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.3496999442577362}, {"id": 300, "seek": 332400, "start": 3338.0, "end": 3352.0, "text": " There's a setting on, I only used one GPU, when I was disabling that one of my computers where I ran this had a for GPS was running in 16 minutes. So, in practice you may be also want to disable that and you get also way faster training.", "tokens": [51064, 821, 311, 257, 3287, 322, 11, 286, 787, 1143, 472, 18407, 11, 562, 286, 390, 717, 20112, 300, 472, 295, 452, 10807, 689, 286, 5872, 341, 632, 257, 337, 19462, 390, 2614, 294, 3165, 2077, 13, 407, 11, 294, 3124, 291, 815, 312, 611, 528, 281, 28362, 300, 293, 291, 483, 611, 636, 4663, 3097, 13, 51764], "temperature": 0.0, "avg_logprob": -0.20403046564224664, "compression_ratio": 1.6630824372759856, "no_speech_prob": 0.3496999442577362}, {"id": 301, "seek": 335200, "start": 3352.0, "end": 3354.0, "text": " Okay, so just briefly.", "tokens": [50364, 1033, 11, 370, 445, 10515, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2708503116260875, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.038398947566747665}, {"id": 302, "seek": 335200, "start": 3354.0, "end": 3359.0, "text": " Sorry, Sebastian, we have those minutes to finish please.", "tokens": [50464, 4919, 11, 31102, 11, 321, 362, 729, 2077, 281, 2413, 1767, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2708503116260875, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.038398947566747665}, {"id": 303, "seek": 335200, "start": 3359.0, "end": 3371.0, "text": " Yeah, let's. Sorry, I was going way over time. So yeah, maybe let's go to the yeah we will take some questions. Yes, okay, can start asking the questions for you.", "tokens": [50714, 865, 11, 718, 311, 13, 4919, 11, 286, 390, 516, 636, 670, 565, 13, 407, 1338, 11, 1310, 718, 311, 352, 281, 264, 1338, 321, 486, 747, 512, 1651, 13, 1079, 11, 1392, 11, 393, 722, 3365, 264, 1651, 337, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2708503116260875, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.038398947566747665}, {"id": 304, "seek": 335200, "start": 3371.0, "end": 3377.0, "text": " So your last thing is, I will just go to my slides for the questions also.", "tokens": [51314, 407, 428, 1036, 551, 307, 11, 286, 486, 445, 352, 281, 452, 9788, 337, 264, 1651, 611, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2708503116260875, "compression_ratio": 1.614213197969543, "no_speech_prob": 0.038398947566747665}, {"id": 305, "seek": 337700, "start": 3377.0, "end": 3380.0, "text": " Okay, that's not the right one.", "tokens": [50364, 1033, 11, 300, 311, 406, 264, 558, 472, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14139517733925266, "compression_ratio": 1.6586345381526104, "no_speech_prob": 0.009856264106929302}, {"id": 306, "seek": 337700, "start": 3380.0, "end": 3396.0, "text": " But I am ready for questions now. So I wanted to say I have also way more detailed transformer lecture on YouTube from my deep learning class and there will be a version of the Python machine learning book coming out for pytorch, which also has a transformer", "tokens": [50514, 583, 286, 669, 1919, 337, 1651, 586, 13, 407, 286, 1415, 281, 584, 286, 362, 611, 636, 544, 9942, 31782, 7991, 322, 3088, 490, 452, 2452, 2539, 1508, 293, 456, 486, 312, 257, 3037, 295, 264, 15329, 3479, 2539, 1446, 1348, 484, 337, 25878, 284, 339, 11, 597, 611, 575, 257, 31782, 51314], "temperature": 0.0, "avg_logprob": -0.14139517733925266, "compression_ratio": 1.6586345381526104, "no_speech_prob": 0.009856264106929302}, {"id": 307, "seek": 337700, "start": 3396.0, "end": 3403.0, "text": " chapter that will be later this year. So if you want to read more details about this also this might be a useful resource.", "tokens": [51314, 7187, 300, 486, 312, 1780, 341, 1064, 13, 407, 498, 291, 528, 281, 1401, 544, 4365, 466, 341, 611, 341, 1062, 312, 257, 4420, 7684, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14139517733925266, "compression_ratio": 1.6586345381526104, "no_speech_prob": 0.009856264106929302}, {"id": 308, "seek": 340300, "start": 3403.0, "end": 3413.0, "text": " So I'm also I wanted to mention this the figures here. They are from that chapter so I want to give also credit to Jitian Joe who helped me a lot with the figures and yeah writing this chapter.", "tokens": [50364, 407, 286, 478, 611, 286, 1415, 281, 2152, 341, 264, 9624, 510, 13, 814, 366, 490, 300, 7187, 370, 286, 528, 281, 976, 611, 5397, 281, 508, 270, 952, 6807, 567, 4254, 385, 257, 688, 365, 264, 9624, 293, 1338, 3579, 341, 7187, 13, 50864], "temperature": 0.0, "avg_logprob": -0.22504559958853373, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.031764060258865356}, {"id": 309, "seek": 340300, "start": 3413.0, "end": 3417.0, "text": " Okay, sorry. So I think we can take questions now.", "tokens": [50864, 1033, 11, 2597, 13, 407, 286, 519, 321, 393, 747, 1651, 586, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22504559958853373, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.031764060258865356}, {"id": 310, "seek": 340300, "start": 3417.0, "end": 3419.0, "text": " No worries hasn't you can start.", "tokens": [51064, 883, 16340, 6132, 380, 291, 393, 722, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22504559958853373, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.031764060258865356}, {"id": 311, "seek": 340300, "start": 3419.0, "end": 3421.0, "text": " Yeah, thank you so much.", "tokens": [51164, 865, 11, 1309, 291, 370, 709, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22504559958853373, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.031764060258865356}, {"id": 312, "seek": 342100, "start": 3421.0, "end": 3424.0, "text": " Thank you so much for your time and for this wonderful talk.", "tokens": [50364, 1044, 291, 370, 709, 337, 428, 565, 293, 337, 341, 3715, 751, 13, 50514], "temperature": 0.0, "avg_logprob": -0.28663703070746527, "compression_ratio": 1.7185929648241205, "no_speech_prob": 0.11801426112651825}, {"id": 313, "seek": 342100, "start": 3424.0, "end": 3431.0, "text": " Now it's your time but if you have any questions, and you would like, you'd like to ask it to the first question.", "tokens": [50514, 823, 309, 311, 428, 565, 457, 498, 291, 362, 604, 1651, 11, 293, 291, 576, 411, 11, 291, 1116, 411, 281, 1029, 309, 281, 264, 700, 1168, 13, 50864], "temperature": 0.0, "avg_logprob": -0.28663703070746527, "compression_ratio": 1.7185929648241205, "no_speech_prob": 0.11801426112651825}, {"id": 314, "seek": 342100, "start": 3431.0, "end": 3441.0, "text": " Feel free to write it in the chat, or even you can raise your hand and we'll unmute you so you can ask your question.", "tokens": [50864, 14113, 1737, 281, 2464, 309, 294, 264, 5081, 11, 420, 754, 291, 393, 5300, 428, 1011, 293, 321, 603, 41445, 291, 370, 291, 393, 1029, 428, 1168, 13, 51364], "temperature": 0.0, "avg_logprob": -0.28663703070746527, "compression_ratio": 1.7185929648241205, "no_speech_prob": 0.11801426112651825}, {"id": 315, "seek": 342100, "start": 3441.0, "end": 3446.0, "text": " So let's start with our first question for today.", "tokens": [51364, 407, 718, 311, 722, 365, 527, 700, 1168, 337, 965, 13, 51614], "temperature": 0.0, "avg_logprob": -0.28663703070746527, "compression_ratio": 1.7185929648241205, "no_speech_prob": 0.11801426112651825}, {"id": 316, "seek": 344600, "start": 3446.0, "end": 3456.0, "text": " This is from Sunny. She's asking, can you be to do classification.", "tokens": [50364, 639, 307, 490, 34665, 13, 1240, 311, 3365, 11, 393, 291, 312, 281, 360, 21538, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2849705219268799, "compression_ratio": 1.6544502617801047, "no_speech_prob": 0.03720096871256828}, {"id": 317, "seek": 344600, "start": 3456.0, "end": 3473.0, "text": " I think so yeah the definitely the first version I think the other versions can do classification to on the hugging way web, sorry hugging face website they have, I think also an implementation of GPT a re implementation, where you can try this out.", "tokens": [50864, 286, 519, 370, 1338, 264, 2138, 264, 700, 3037, 286, 519, 264, 661, 9606, 393, 360, 21538, 281, 322, 264, 41706, 636, 3670, 11, 2597, 41706, 1851, 3144, 436, 362, 11, 286, 519, 611, 364, 11420, 295, 26039, 51, 257, 319, 11420, 11, 689, 291, 393, 853, 341, 484, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2849705219268799, "compression_ratio": 1.6544502617801047, "no_speech_prob": 0.03720096871256828}, {"id": 318, "seek": 347300, "start": 3473.0, "end": 3479.0, "text": " I remember I have demonstrated this action spring semester in the class.", "tokens": [50364, 286, 1604, 286, 362, 18772, 341, 3069, 5587, 11894, 294, 264, 1508, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16661369323730468, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0510098822414875}, {"id": 319, "seek": 347300, "start": 3479.0, "end": 3489.0, "text": " So they definitely have capabilities to do classification and GPT two and three, you have to provide it by other context, but in GPT version one.", "tokens": [50664, 407, 436, 2138, 362, 10862, 281, 360, 21538, 293, 26039, 51, 732, 293, 1045, 11, 291, 362, 281, 2893, 309, 538, 661, 4319, 11, 457, 294, 26039, 51, 3037, 472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16661369323730468, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.0510098822414875}, {"id": 320, "seek": 348900, "start": 3489.0, "end": 3500.0, "text": " They also provide classification as an example in the paper I'm not sure if I had a figure on this here, but yeah you can, but people just because the way it's trained in this unit directional way.", "tokens": [50364, 814, 611, 2893, 21538, 382, 364, 1365, 294, 264, 3035, 286, 478, 406, 988, 498, 286, 632, 257, 2573, 322, 341, 510, 11, 457, 1338, 291, 393, 11, 457, 561, 445, 570, 264, 636, 309, 311, 8895, 294, 341, 4985, 42242, 636, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16192710876464844, "compression_ratio": 1.5837320574162679, "no_speech_prob": 0.02550267055630684}, {"id": 321, "seek": 348900, "start": 3500.0, "end": 3510.0, "text": " Usually, people agree that a bird type of models would be better for classification, although you can use GPT two for classification.", "tokens": [50914, 11419, 11, 561, 3986, 300, 257, 5255, 2010, 295, 5245, 576, 312, 1101, 337, 21538, 11, 4878, 291, 393, 764, 26039, 51, 732, 337, 21538, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16192710876464844, "compression_ratio": 1.5837320574162679, "no_speech_prob": 0.02550267055630684}, {"id": 322, "seek": 351000, "start": 3510.0, "end": 3513.0, "text": " Another question from Marietta Barata.", "tokens": [50364, 3996, 1168, 490, 2039, 1684, 1328, 4156, 3274, 13, 50514], "temperature": 0.0, "avg_logprob": -0.31168980752268144, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.026148540899157524}, {"id": 323, "seek": 351000, "start": 3513.0, "end": 3523.0, "text": " Is there any use cases right now with transformers based vision models out, out before various models like those based on this net.", "tokens": [50514, 1119, 456, 604, 764, 3331, 558, 586, 365, 4088, 433, 2361, 5201, 5245, 484, 11, 484, 949, 3683, 5245, 411, 729, 2361, 322, 341, 2533, 13, 51014], "temperature": 0.0, "avg_logprob": -0.31168980752268144, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.026148540899157524}, {"id": 324, "seek": 351000, "start": 3523.0, "end": 3530.0, "text": " And should I use transformers architecture based models now to classify images and to base with net.", "tokens": [51014, 400, 820, 286, 764, 4088, 433, 9482, 2361, 5245, 586, 281, 33872, 5267, 293, 281, 3096, 365, 2533, 13, 51364], "temperature": 0.0, "avg_logprob": -0.31168980752268144, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.026148540899157524}, {"id": 325, "seek": 353000, "start": 3530.0, "end": 3542.0, "text": " Yeah, that's a good question so I, um, yeah, I have seen a lot of papers recently using transformers for computer vision. There's the classic vision transformer and they're also newer models.", "tokens": [50364, 865, 11, 300, 311, 257, 665, 1168, 370, 286, 11, 1105, 11, 1338, 11, 286, 362, 1612, 257, 688, 295, 10577, 3938, 1228, 4088, 433, 337, 3820, 5201, 13, 821, 311, 264, 7230, 5201, 31782, 293, 436, 434, 611, 17628, 5245, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18674160983111407, "compression_ratio": 1.61139896373057, "no_speech_prob": 0.012234441936016083}, {"id": 326, "seek": 353000, "start": 3542.0, "end": 3551.0, "text": " And I think they are depends on which paper you read, they are set up in the art on compared to computer vision models.", "tokens": [50964, 400, 286, 519, 436, 366, 5946, 322, 597, 3035, 291, 1401, 11, 436, 366, 992, 493, 294, 264, 1523, 322, 5347, 281, 3820, 5201, 5245, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18674160983111407, "compression_ratio": 1.61139896373057, "no_speech_prob": 0.012234441936016083}, {"id": 327, "seek": 355100, "start": 3551.0, "end": 3563.0, "text": " I think the efficient net version three paper that came out in April on show that they perform better than computer than transformers but this was in April so things might be different now.", "tokens": [50364, 286, 519, 264, 7148, 2533, 3037, 1045, 3035, 300, 1361, 484, 294, 6929, 322, 855, 300, 436, 2042, 1101, 813, 3820, 813, 4088, 433, 457, 341, 390, 294, 6929, 370, 721, 1062, 312, 819, 586, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1413739976428804, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.015411501750349998}, {"id": 328, "seek": 355100, "start": 3563.0, "end": 3573.0, "text": " But one downside of the of transformers and computer vision is that they require more data for pre training. If you can get the pre trained models it might be for the fine tuning part it might be worthwhile.", "tokens": [50964, 583, 472, 25060, 295, 264, 295, 4088, 433, 293, 3820, 5201, 307, 300, 436, 3651, 544, 1412, 337, 659, 3097, 13, 759, 291, 393, 483, 264, 659, 8895, 5245, 309, 1062, 312, 337, 264, 2489, 15164, 644, 309, 1062, 312, 28159, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1413739976428804, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.015411501750349998}, {"id": 329, "seek": 357300, "start": 3573.0, "end": 3580.0, "text": " Otherwise, otherwise, I think it's more like a research topic now that people are exploring. In practice, it's probably very expensive to train them.", "tokens": [50364, 10328, 11, 5911, 11, 286, 519, 309, 311, 544, 411, 257, 2132, 4829, 586, 300, 561, 366, 12736, 13, 682, 3124, 11, 309, 311, 1391, 588, 5124, 281, 3847, 552, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14976986594822095, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.2220541387796402}, {"id": 330, "seek": 357300, "start": 3580.0, "end": 3582.0, "text": " I think there's also this paper.", "tokens": [50714, 286, 519, 456, 311, 611, 341, 3035, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14976986594822095, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.2220541387796402}, {"id": 331, "seek": 357300, "start": 3582.0, "end": 3593.0, "text": " I'm not sure if they compared directly. So efficient at version three was better than the vision transformer and there's the halo net paper that came out in June, it's like the rest of 50 like architecture with attention.", "tokens": [50814, 286, 478, 406, 988, 498, 436, 5347, 3838, 13, 407, 7148, 412, 3037, 1045, 390, 1101, 813, 264, 5201, 31782, 293, 456, 311, 264, 46268, 2533, 3035, 300, 1361, 484, 294, 6928, 11, 309, 311, 411, 264, 1472, 295, 2625, 411, 9482, 365, 3202, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14976986594822095, "compression_ratio": 1.6224899598393574, "no_speech_prob": 0.2220541387796402}, {"id": 332, "seek": 359300, "start": 3593.0, "end": 3604.0, "text": " It's not a vision transform but it has like resonate like backbone with attention. And I think this one ought to form the vision at version three on image net I think they got like 85 point something performance.", "tokens": [50364, 467, 311, 406, 257, 5201, 4088, 457, 309, 575, 411, 34285, 411, 34889, 365, 3202, 13, 400, 286, 519, 341, 472, 13416, 281, 1254, 264, 5201, 412, 3037, 1045, 322, 3256, 2533, 286, 519, 436, 658, 411, 14695, 935, 746, 3389, 13, 50914], "temperature": 0.0, "avg_logprob": -0.186784029006958, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.05996314436197281}, {"id": 333, "seek": 359300, "start": 3604.0, "end": 3609.0, "text": " So in that way, I think, yeah, it's right now really where transformers are.", "tokens": [50914, 407, 294, 300, 636, 11, 286, 519, 11, 1338, 11, 309, 311, 558, 586, 534, 689, 4088, 433, 366, 13, 51164], "temperature": 0.0, "avg_logprob": -0.186784029006958, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.05996314436197281}, {"id": 334, "seek": 360900, "start": 3609.0, "end": 3618.0, "text": " It's not a state of the art, but I think it's still worthwhile using CNN for classification for the time being, because they're also cheaper to train.", "tokens": [50364, 467, 311, 406, 257, 1785, 295, 264, 1523, 11, 457, 286, 519, 309, 311, 920, 28159, 1228, 24859, 337, 21538, 337, 264, 565, 885, 11, 570, 436, 434, 611, 12284, 281, 3847, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18497378366035327, "compression_ratio": 1.6633663366336633, "no_speech_prob": 0.2533877491950989}, {"id": 335, "seek": 360900, "start": 3618.0, "end": 3624.0, "text": " And then of course we also have architectures like MLP mixer which is neither of them it's just multi layer perceptrons.", "tokens": [50814, 400, 550, 295, 1164, 321, 611, 362, 6331, 1303, 411, 21601, 47, 24063, 597, 307, 9662, 295, 552, 309, 311, 445, 4825, 4583, 43276, 13270, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18497378366035327, "compression_ratio": 1.6633663366336633, "no_speech_prob": 0.2533877491950989}, {"id": 336, "seek": 360900, "start": 3624.0, "end": 3638.0, "text": " So yeah, it's an interesting time I interesting question I, I would say personally I stick with CNN because they are easier to train, but in the future who knows, I think maybe computer vision transformers will take over completely.", "tokens": [51114, 407, 1338, 11, 309, 311, 364, 1880, 565, 286, 1880, 1168, 286, 11, 286, 576, 584, 5665, 286, 2897, 365, 24859, 570, 436, 366, 3571, 281, 3847, 11, 457, 294, 264, 2027, 567, 3255, 11, 286, 519, 1310, 3820, 5201, 4088, 433, 486, 747, 670, 2584, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18497378366035327, "compression_ratio": 1.6633663366336633, "no_speech_prob": 0.2533877491950989}, {"id": 337, "seek": 363800, "start": 3638.0, "end": 3639.0, "text": " Great.", "tokens": [50364, 3769, 13, 50414], "temperature": 0.0, "avg_logprob": -0.28710531259511973, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.006885009817779064}, {"id": 338, "seek": 363800, "start": 3639.0, "end": 3648.0, "text": " I read about is also asking is transform architecture practical for real world tasks outside of his academia and big tech companies.", "tokens": [50414, 286, 1401, 466, 307, 611, 3365, 307, 4088, 9482, 8496, 337, 957, 1002, 9608, 2380, 295, 702, 28937, 293, 955, 7553, 3431, 13, 50864], "temperature": 0.0, "avg_logprob": -0.28710531259511973, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.006885009817779064}, {"id": 339, "seek": 363800, "start": 3648.0, "end": 3650.0, "text": " And what are the next steps.", "tokens": [50864, 400, 437, 366, 264, 958, 4439, 13, 50964], "temperature": 0.0, "avg_logprob": -0.28710531259511973, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.006885009817779064}, {"id": 340, "seek": 363800, "start": 3650.0, "end": 3651.0, "text": " Oh, sorry.", "tokens": [50964, 876, 11, 2597, 13, 51014], "temperature": 0.0, "avg_logprob": -0.28710531259511973, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.006885009817779064}, {"id": 341, "seek": 363800, "start": 3651.0, "end": 3663.0, "text": " What are the next steps to make transform architecture based models more accessible with transformers to be able to adapt or something more automatic will replace it.", "tokens": [51014, 708, 366, 264, 958, 4439, 281, 652, 4088, 9482, 2361, 5245, 544, 9515, 365, 4088, 433, 281, 312, 1075, 281, 6231, 420, 746, 544, 12509, 486, 7406, 309, 13, 51614], "temperature": 0.0, "avg_logprob": -0.28710531259511973, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.006885009817779064}, {"id": 342, "seek": 366300, "start": 3663.0, "end": 3672.0, "text": " Yeah, good question I think that is one of the big frustrations that these transform models are so big that as private personal and academia it's really hard to train them.", "tokens": [50364, 865, 11, 665, 1168, 286, 519, 300, 307, 472, 295, 264, 955, 7454, 12154, 300, 613, 4088, 5245, 366, 370, 955, 300, 382, 4551, 2973, 293, 28937, 309, 311, 534, 1152, 281, 3847, 552, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17485967874526978, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.007458446081727743}, {"id": 343, "seek": 366300, "start": 3672.0, "end": 3686.0, "text": " I think on some labs in academia have flat resources for example this this paper here on I think the preprint was I saw the preprint I think from this for this paper like in 2019.", "tokens": [50814, 286, 519, 322, 512, 20339, 294, 28937, 362, 4962, 3593, 337, 1365, 341, 341, 3035, 510, 322, 286, 519, 264, 659, 14030, 390, 286, 1866, 264, 659, 14030, 286, 519, 490, 341, 337, 341, 3035, 411, 294, 6071, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17485967874526978, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.007458446081727743}, {"id": 344, "seek": 368600, "start": 3686.0, "end": 3695.0, "text": " Even two years ago they had the resources for training but it must have been really expensive I think they're like 50 GPUs or even 50 GPU TPU pots.", "tokens": [50364, 2754, 732, 924, 2057, 436, 632, 264, 3593, 337, 3097, 457, 309, 1633, 362, 668, 534, 5124, 286, 519, 436, 434, 411, 2625, 18407, 82, 420, 754, 2625, 18407, 314, 8115, 22022, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16931956898082387, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.19899384677410126}, {"id": 345, "seek": 368600, "start": 3695.0, "end": 3701.0, "text": " I think they collaborated with big tech companies but yeah, how do you as a normal researcher use these I think.", "tokens": [50814, 286, 519, 436, 42463, 365, 955, 7553, 3431, 457, 1338, 11, 577, 360, 291, 382, 257, 2710, 21751, 764, 613, 286, 519, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16931956898082387, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.19899384677410126}, {"id": 346, "seek": 368600, "start": 3701.0, "end": 3713.0, "text": " Yeah, I think it's really infeasible to use these are train pre train these models as a private person I mean you can probably manage but you probably need a whole team of engineers to even set up all the infrastructure", "tokens": [51114, 865, 11, 286, 519, 309, 311, 534, 1536, 68, 296, 964, 281, 764, 613, 366, 3847, 659, 3847, 613, 5245, 382, 257, 4551, 954, 286, 914, 291, 393, 1391, 3067, 457, 291, 1391, 643, 257, 1379, 1469, 295, 11955, 281, 754, 992, 493, 439, 264, 6896, 51714], "temperature": 0.0, "avg_logprob": -0.16931956898082387, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.19899384677410126}, {"id": 347, "seek": 371300, "start": 3713.0, "end": 3728.0, "text": " for this work. And in that way I find it more interesting to really fine tune these models like training or using a pre trained model, and then just focusing on fine tuning and like you've seen in this code example I showed you it's like in another one hour you can find", "tokens": [50364, 337, 341, 589, 13, 400, 294, 300, 636, 286, 915, 309, 544, 1880, 281, 534, 2489, 10864, 613, 5245, 411, 3097, 420, 1228, 257, 659, 8895, 2316, 11, 293, 550, 445, 8416, 322, 2489, 15164, 293, 411, 291, 600, 1612, 294, 341, 3089, 1365, 286, 4712, 291, 309, 311, 411, 294, 1071, 472, 1773, 291, 393, 915, 51114], "temperature": 0.0, "avg_logprob": -0.1833947760159852, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0367264486849308}, {"id": 348, "seek": 372800, "start": 3728.0, "end": 3747.0, "text": " but yeah there are also approaches where people have made transformers way more efficient. I haven't covered them here because of your time constraints but as for example, the nice trim former or my head the sparse transformer, which I think they have usually you have quadratic complexity", "tokens": [50364, 457, 1338, 456, 366, 611, 11587, 689, 561, 362, 1027, 4088, 433, 636, 544, 7148, 13, 286, 2378, 380, 5343, 552, 510, 570, 295, 428, 565, 18491, 457, 382, 337, 1365, 11, 264, 1481, 10445, 5819, 420, 452, 1378, 264, 637, 11668, 31782, 11, 597, 286, 519, 436, 362, 2673, 291, 362, 37262, 14024, 51314], "temperature": 0.0, "avg_logprob": -0.27670821098432147, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.03355315327644348}, {"id": 349, "seek": 372800, "start": 3747.0, "end": 3752.0, "text": " they have linear complexity scaling in terms of the input sequence size.", "tokens": [51314, 436, 362, 8213, 14024, 21589, 294, 2115, 295, 264, 4846, 8310, 2744, 13, 51564], "temperature": 0.0, "avg_logprob": -0.27670821098432147, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.03355315327644348}, {"id": 350, "seek": 375200, "start": 3752.0, "end": 3759.0, "text": " They have made efforts to make transformers more efficient. Well, if I also had a slide even on that here.", "tokens": [50364, 814, 362, 1027, 6484, 281, 652, 4088, 433, 544, 7148, 13, 1042, 11, 498, 286, 611, 632, 257, 4137, 754, 322, 300, 510, 13, 50714], "temperature": 0.0, "avg_logprob": -0.20501044626986042, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.07576432079076767}, {"id": 351, "seek": 375200, "start": 3759.0, "end": 3776.0, "text": " There are efforts. So this is unfortunately kept at or clipped 2019 here, but you can see there's this trend that they become bigger and bigger and bigger, but there are also some efforts, for example, the distill bird that I showed you and other methods that try to reduce the number", "tokens": [50714, 821, 366, 6484, 13, 407, 341, 307, 7015, 4305, 412, 420, 596, 5529, 6071, 510, 11, 457, 291, 393, 536, 456, 311, 341, 6028, 300, 436, 1813, 3801, 293, 3801, 293, 3801, 11, 457, 456, 366, 611, 512, 6484, 11, 337, 1365, 11, 264, 42923, 5255, 300, 286, 4712, 291, 293, 661, 7150, 300, 853, 281, 5407, 264, 1230, 51564], "temperature": 0.0, "avg_logprob": -0.20501044626986042, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.07576432079076767}, {"id": 352, "seek": 377600, "start": 3776.0, "end": 3787.0, "text": " of operators and allow people to use transformers even though they may not have access to large computer infrastructure. But yeah it's like a concern, many people are concerned about.", "tokens": [50364, 295, 19077, 293, 2089, 561, 281, 764, 4088, 433, 754, 1673, 436, 815, 406, 362, 2105, 281, 2416, 3820, 6896, 13, 583, 1338, 309, 311, 411, 257, 3136, 11, 867, 561, 366, 5922, 466, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14236862009221857, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.1308634877204895}, {"id": 353, "seek": 377600, "start": 3787.0, "end": 3796.0, "text": " There was also this paper calculating the cost. If you have a 1.5 billion parameter model it costs like 80 to 1000 to 1.6 million just to train this model.", "tokens": [50914, 821, 390, 611, 341, 3035, 28258, 264, 2063, 13, 759, 291, 362, 257, 502, 13, 20, 5218, 13075, 2316, 309, 5497, 411, 4688, 281, 9714, 281, 502, 13, 21, 2459, 445, 281, 3847, 341, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14236862009221857, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.1308634877204895}, {"id": 354, "seek": 377600, "start": 3796.0, "end": 3805.0, "text": " So yeah, it's, we will see where things go but yeah this is definitely a concern that this is really unfeasible for many people.", "tokens": [51364, 407, 1338, 11, 309, 311, 11, 321, 486, 536, 689, 721, 352, 457, 1338, 341, 307, 2138, 257, 3136, 300, 341, 307, 534, 517, 2106, 296, 964, 337, 867, 561, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14236862009221857, "compression_ratio": 1.6595744680851063, "no_speech_prob": 0.1308634877204895}, {"id": 355, "seek": 380500, "start": 3806.0, "end": 3815.0, "text": " We have another question from Jason. Can GPT to GPT to be fine tuned on some more datasets like few hundreds.", "tokens": [50414, 492, 362, 1071, 1168, 490, 11181, 13, 1664, 26039, 51, 281, 26039, 51, 281, 312, 2489, 10870, 322, 512, 544, 42856, 411, 1326, 6779, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3167216313349736, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.005627456121146679}, {"id": 356, "seek": 380500, "start": 3815.0, "end": 3830.0, "text": " Yeah, I think so. Personally, I have only used GPT to via the hugging face website where they had like derivative of GPT to I don't, I forgot the name I think it's called GPT meal.", "tokens": [50864, 865, 11, 286, 519, 370, 13, 21079, 11, 286, 362, 787, 1143, 26039, 51, 281, 5766, 264, 41706, 1851, 3144, 689, 436, 632, 411, 13760, 295, 26039, 51, 281, 286, 500, 380, 11, 286, 5298, 264, 1315, 286, 519, 309, 311, 1219, 26039, 51, 6791, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3167216313349736, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.005627456121146679}, {"id": 357, "seek": 383000, "start": 3830.0, "end": 3841.0, "text": " So this GPT meal is like an effort by the open source community to train a GPT to model because I don't think, I think maybe has changed but I don't think they.", "tokens": [50364, 407, 341, 26039, 51, 6791, 307, 411, 364, 4630, 538, 264, 1269, 4009, 1768, 281, 3847, 257, 26039, 51, 281, 2316, 570, 286, 500, 380, 519, 11, 286, 519, 1310, 575, 3105, 457, 286, 500, 380, 519, 436, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1683188742333716, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.004068376030772924}, {"id": 358, "seek": 383000, "start": 3841.0, "end": 3855.0, "text": " I have shared the full model I think they only provide access through an API. So that way if you only have access to an API I don't know if you can find unit, but there's this GPT meal project.", "tokens": [50914, 286, 362, 5507, 264, 1577, 2316, 286, 519, 436, 787, 2893, 2105, 807, 364, 9362, 13, 407, 300, 636, 498, 291, 787, 362, 2105, 281, 364, 9362, 286, 500, 380, 458, 498, 291, 393, 915, 4985, 11, 457, 456, 311, 341, 26039, 51, 6791, 1716, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1683188742333716, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.004068376030772924}, {"id": 359, "seek": 385500, "start": 3856.0, "end": 3869.0, "text": " I would have to search the website I don't know my head by a thing it's like this, where they have reengineered either GPT version two or three and I think this might be something that you could download and fine tune.", "tokens": [50414, 286, 576, 362, 281, 3164, 264, 3144, 286, 500, 380, 458, 452, 1378, 538, 257, 551, 309, 311, 411, 341, 11, 689, 436, 362, 319, 25609, 4073, 2139, 26039, 51, 3037, 732, 420, 1045, 293, 286, 519, 341, 1062, 312, 746, 300, 291, 727, 5484, 293, 2489, 10864, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17700407970910784, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.04269108176231384}, {"id": 360, "seek": 385500, "start": 3869.0, "end": 3878.0, "text": " And I think hugging face also has maybe some models I would have to double check. I haven't done this myself, but I think it should be possible.", "tokens": [51064, 400, 286, 519, 41706, 1851, 611, 575, 1310, 512, 5245, 286, 576, 362, 281, 3834, 1520, 13, 286, 2378, 380, 1096, 341, 2059, 11, 457, 286, 519, 309, 820, 312, 1944, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17700407970910784, "compression_ratio": 1.6061946902654867, "no_speech_prob": 0.04269108176231384}, {"id": 361, "seek": 387800, "start": 3878.0, "end": 3889.0, "text": " So for GPT two and three it's kind of not necessary because you have like these contexts so they say with the context it's sufficient but you could maybe fine tune it I, I don't know for sure.", "tokens": [50364, 407, 337, 26039, 51, 732, 293, 1045, 309, 311, 733, 295, 406, 4818, 570, 291, 362, 411, 613, 30628, 370, 436, 584, 365, 264, 4319, 309, 311, 11563, 457, 291, 727, 1310, 2489, 10864, 309, 286, 11, 286, 500, 380, 458, 337, 988, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19578415552775066, "compression_ratio": 1.3987341772151898, "no_speech_prob": 0.006789797451347113}, {"id": 362, "seek": 387800, "start": 3889.0, "end": 3892.0, "text": " It's a good question.", "tokens": [50914, 467, 311, 257, 665, 1168, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19578415552775066, "compression_ratio": 1.3987341772151898, "no_speech_prob": 0.006789797451347113}, {"id": 363, "seek": 387800, "start": 3892.0, "end": 3894.0, "text": " Great.", "tokens": [51064, 3769, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19578415552775066, "compression_ratio": 1.3987341772151898, "no_speech_prob": 0.006789797451347113}, {"id": 364, "seek": 389400, "start": 3894.0, "end": 3903.0, "text": " I'm wondering if you can use between for specific domains, like, for example, medicine.", "tokens": [50364, 286, 478, 6359, 498, 291, 393, 764, 1296, 337, 2685, 25514, 11, 411, 11, 337, 1365, 11, 7195, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2761787970860799, "compression_ratio": 1.4961832061068703, "no_speech_prob": 0.013601548969745636}, {"id": 365, "seek": 389400, "start": 3903.0, "end": 3910.0, "text": " Yeah, definitely. So this is, yeah, yeah, so you can definitely adopt the architecture for specific domains.", "tokens": [50814, 865, 11, 2138, 13, 407, 341, 307, 11, 1338, 11, 1338, 11, 370, 291, 393, 2138, 6878, 264, 9482, 337, 2685, 25514, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2761787970860799, "compression_ratio": 1.4961832061068703, "no_speech_prob": 0.013601548969745636}, {"id": 366, "seek": 391000, "start": 3910.0, "end": 3922.0, "text": " Anyway, if you have a small medical image data set like texts you could technically fine tune this, but if you have a large data set you can also maybe try to train it from scratch, like the researchers did here.", "tokens": [50364, 5684, 11, 498, 291, 362, 257, 1359, 4625, 3256, 1412, 992, 411, 15765, 291, 727, 12120, 2489, 10864, 341, 11, 457, 498, 291, 362, 257, 2416, 1412, 992, 291, 393, 611, 1310, 853, 281, 3847, 309, 490, 8459, 11, 411, 264, 10309, 630, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1434572615274569, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.013217618688941002}, {"id": 367, "seek": 391000, "start": 3922.0, "end": 3926.0, "text": " So here instead of training it on sentences they use the BERT model.", "tokens": [50964, 407, 510, 2602, 295, 3097, 309, 322, 16579, 436, 764, 264, 363, 31479, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1434572615274569, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.013217618688941002}, {"id": 368, "seek": 391000, "start": 3926.0, "end": 3929.0, "text": " I think this group, there were two groups that did something similar.", "tokens": [51164, 286, 519, 341, 1594, 11, 456, 645, 732, 3935, 300, 630, 746, 2531, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1434572615274569, "compression_ratio": 1.6175115207373272, "no_speech_prob": 0.013217618688941002}, {"id": 369, "seek": 392900, "start": 3929.0, "end": 3936.0, "text": " They used the BERT model and trained it on amino acid sequences where they had millions of sequences from the protein structure database.", "tokens": [50364, 814, 1143, 264, 363, 31479, 2316, 293, 8895, 309, 322, 24674, 8258, 22978, 689, 436, 632, 6803, 295, 22978, 490, 264, 7944, 3877, 8149, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1256245800427028, "compression_ratio": 1.7910958904109588, "no_speech_prob": 0.2655321955680847}, {"id": 370, "seek": 392900, "start": 3936.0, "end": 3946.0, "text": " They were like, I mean you can think of an amino acid sequence as a sentence, but each character there are 20 different amino acids, each character would represent a word essentially because it's on.", "tokens": [50714, 814, 645, 411, 11, 286, 914, 291, 393, 519, 295, 364, 24674, 8258, 8310, 382, 257, 8174, 11, 457, 1184, 2517, 456, 366, 945, 819, 24674, 21667, 11, 1184, 2517, 576, 2906, 257, 1349, 4476, 570, 309, 311, 322, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1256245800427028, "compression_ratio": 1.7910958904109588, "no_speech_prob": 0.2655321955680847}, {"id": 371, "seek": 392900, "start": 3946.0, "end": 3956.0, "text": " It's more like a character level type thing, but they did that and it worked well so I can imagine you can also train it on other types of sequences and in the medical domain depends on", "tokens": [51214, 467, 311, 544, 411, 257, 2517, 1496, 2010, 551, 11, 457, 436, 630, 300, 293, 309, 2732, 731, 370, 286, 393, 3811, 291, 393, 611, 3847, 309, 322, 661, 3467, 295, 22978, 293, 294, 264, 4625, 9274, 5946, 322, 51714], "temperature": 0.0, "avg_logprob": -0.1256245800427028, "compression_ratio": 1.7910958904109588, "no_speech_prob": 0.2655321955680847}, {"id": 372, "seek": 395600, "start": 3956.0, "end": 3968.0, "text": " that you have a sequence of specific domain specific encoding that say you have a certain device that outputs certain values that don't save a meaning to a human, but that could be passed by a machine.", "tokens": [50364, 300, 291, 362, 257, 8310, 295, 2685, 9274, 2685, 43430, 300, 584, 291, 362, 257, 1629, 4302, 300, 23930, 1629, 4190, 300, 500, 380, 3155, 257, 3620, 281, 257, 1952, 11, 457, 300, 727, 312, 4678, 538, 257, 3479, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14402519663174948, "compression_ratio": 1.7950819672131149, "no_speech_prob": 0.021900994703173637}, {"id": 373, "seek": 395600, "start": 3968.0, "end": 3977.0, "text": " I can also think of a transformer maybe being trained on that or if you have just texts like annotations of patient records I think that could also be done.", "tokens": [50964, 286, 393, 611, 519, 295, 257, 31782, 1310, 885, 8895, 322, 300, 420, 498, 291, 362, 445, 15765, 411, 25339, 763, 295, 4537, 7724, 286, 519, 300, 727, 611, 312, 1096, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14402519663174948, "compression_ratio": 1.7950819672131149, "no_speech_prob": 0.021900994703173637}, {"id": 374, "seek": 395600, "start": 3977.0, "end": 3984.0, "text": " So yeah, you can definitely train this on other things other than general text.", "tokens": [51414, 407, 1338, 11, 291, 393, 2138, 3847, 341, 322, 661, 721, 661, 813, 2674, 2487, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14402519663174948, "compression_ratio": 1.7950819672131149, "no_speech_prob": 0.021900994703173637}, {"id": 375, "seek": 398400, "start": 3985.0, "end": 4003.0, "text": " Altie is also asking, when we fine-tune the BERT train, will that update the vocab of any initial vocab will be the same after fine-tuning?", "tokens": [50414, 15992, 414, 307, 611, 3365, 11, 562, 321, 2489, 12, 83, 2613, 264, 363, 31479, 3847, 11, 486, 300, 5623, 264, 2329, 455, 295, 604, 5883, 2329, 455, 486, 312, 264, 912, 934, 2489, 12, 83, 37726, 30, 51314], "temperature": 0.0, "avg_logprob": -0.3402803057716006, "compression_ratio": 1.287037037037037, "no_speech_prob": 0.013756519183516502}, {"id": 376, "seek": 400300, "start": 4004.0, "end": 4016.0, "text": " I think a good question so when you fine-tune it, so you have the text embedding, I think it would update also the embedding step.", "tokens": [50414, 286, 519, 257, 665, 1168, 370, 562, 291, 2489, 12, 83, 2613, 309, 11, 370, 291, 362, 264, 2487, 12240, 3584, 11, 286, 519, 309, 576, 5623, 611, 264, 12240, 3584, 1823, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19286008783288905, "compression_ratio": 1.2745098039215685, "no_speech_prob": 0.009855927899479866}, {"id": 377, "seek": 401600, "start": 4017.0, "end": 4032.0, "text": " This is something I can't say 100% sure because when you use this code from HuggingFace, the code from HuggingFace includes the encoding as far as I know because you only provide the input IDs of the words and then coding is part of the model.", "tokens": [50414, 639, 307, 746, 286, 393, 380, 584, 2319, 4, 988, 570, 562, 291, 764, 341, 3089, 490, 46892, 3249, 37, 617, 11, 264, 3089, 490, 46892, 3249, 37, 617, 5974, 264, 43430, 382, 1400, 382, 286, 458, 570, 291, 787, 2893, 264, 4846, 48212, 295, 264, 2283, 293, 550, 17720, 307, 644, 295, 264, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1702036698659261, "compression_ratio": 1.5379746835443038, "no_speech_prob": 0.04396224021911621}, {"id": 378, "seek": 403200, "start": 4032.0, "end": 4039.0, "text": " And if you update this, I think you should also, it should also update the embeddings.", "tokens": [50364, 400, 498, 291, 5623, 341, 11, 286, 519, 291, 820, 611, 11, 309, 820, 611, 5623, 264, 12240, 29432, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12068888581829307, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013837949372828007}, {"id": 379, "seek": 403200, "start": 4039.0, "end": 4055.0, "text": " I'm like 90% sure but not 100% because I haven't really looked at the code line by line, maybe they have a line that freezes the weights for the embeddings so that I don't know for sure but I suspect the embeddings are also updated.", "tokens": [50714, 286, 478, 411, 4289, 4, 988, 457, 406, 2319, 4, 570, 286, 2378, 380, 534, 2956, 412, 264, 3089, 1622, 538, 1622, 11, 1310, 436, 362, 257, 1622, 300, 1737, 12214, 264, 17443, 337, 264, 12240, 29432, 370, 300, 286, 500, 380, 458, 337, 988, 457, 286, 9091, 264, 12240, 29432, 366, 611, 10588, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12068888581829307, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.013837949372828007}, {"id": 380, "seek": 405500, "start": 4056.0, "end": 4069.0, "text": " Sorry, I think the question might have been about the vocabulary size. So I think the vocabulary size is fixed so I don't think this can be updated because the vocabulary size depends on the input ID on the tokenizer.", "tokens": [50414, 4919, 11, 286, 519, 264, 1168, 1062, 362, 668, 466, 264, 19864, 2744, 13, 407, 286, 519, 264, 19864, 2744, 307, 6806, 370, 286, 500, 380, 519, 341, 393, 312, 10588, 570, 264, 19864, 2744, 5946, 322, 264, 4846, 7348, 322, 264, 14862, 6545, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13506188684580278, "compression_ratio": 1.5724637681159421, "no_speech_prob": 0.007934252731502056}, {"id": 381, "seek": 406900, "start": 4070.0, "end": 4089.0, "text": " I think the tokenizer would remove words from your input text that is not included in the pre-training. So if you have some arbitrary words that are not in the database where the model was pre-trained on, I think you don't have to worry about it if you use the right tokenizer because it would replace it by an arbitrary token.", "tokens": [50414, 286, 519, 264, 14862, 6545, 576, 4159, 2283, 490, 428, 4846, 2487, 300, 307, 406, 5556, 294, 264, 659, 12, 17227, 1760, 13, 407, 498, 291, 362, 512, 23211, 2283, 300, 366, 406, 294, 264, 8149, 689, 264, 2316, 390, 659, 12, 17227, 2001, 322, 11, 286, 519, 291, 500, 380, 362, 281, 3292, 466, 309, 498, 291, 764, 264, 558, 14862, 6545, 570, 309, 576, 7406, 309, 538, 364, 23211, 14862, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09997279303414482, "compression_ratio": 1.694300518134715, "no_speech_prob": 0.0952383503317833}, {"id": 382, "seek": 408900, "start": 4089.0, "end": 4107.0, "text": " And of course also the model wouldn't then be able to perform well on if you have a lot of different tokens because these are tokens the model has never seen before and they are all, I think, converted to arbitrary unknown token or something like that.", "tokens": [50364, 400, 295, 1164, 611, 264, 2316, 2759, 380, 550, 312, 1075, 281, 2042, 731, 322, 498, 291, 362, 257, 688, 295, 819, 22667, 570, 613, 366, 22667, 264, 2316, 575, 1128, 1612, 949, 293, 436, 366, 439, 11, 286, 519, 11, 16424, 281, 23211, 9841, 14862, 420, 746, 411, 300, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12982650236649948, "compression_ratio": 1.5180722891566265, "no_speech_prob": 0.0035256277769804}, {"id": 383, "seek": 410700, "start": 4107.0, "end": 4111.0, "text": " Okay, nice. Also, do you want to say anything?", "tokens": [50364, 1033, 11, 1481, 13, 2743, 11, 360, 291, 528, 281, 584, 1340, 30, 50564], "temperature": 0.0, "avg_logprob": -0.18833110207005552, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.07096612453460693}, {"id": 384, "seek": 410700, "start": 4116.0, "end": 4119.0, "text": " You can unmute yourself and ask any questions.", "tokens": [50814, 509, 393, 41445, 1803, 293, 1029, 604, 1651, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18833110207005552, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.07096612453460693}, {"id": 385, "seek": 410700, "start": 4119.0, "end": 4120.0, "text": " Can you hear me?", "tokens": [50964, 1664, 291, 1568, 385, 30, 51014], "temperature": 0.0, "avg_logprob": -0.18833110207005552, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.07096612453460693}, {"id": 386, "seek": 410700, "start": 4120.0, "end": 4121.0, "text": " Yes, sure.", "tokens": [51014, 1079, 11, 988, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18833110207005552, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.07096612453460693}, {"id": 387, "seek": 410700, "start": 4121.0, "end": 4134.0, "text": " Thank you. Sorry for insisting on this question, but I just wanted to return a bit to this point, to the last one, to the vocabulary one.", "tokens": [51064, 1044, 291, 13, 4919, 337, 13466, 278, 322, 341, 1168, 11, 457, 286, 445, 1415, 281, 2736, 257, 857, 281, 341, 935, 11, 281, 264, 1036, 472, 11, 281, 264, 19864, 472, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18833110207005552, "compression_ratio": 1.4550561797752808, "no_speech_prob": 0.07096612453460693}, {"id": 388, "seek": 413400, "start": 4134.0, "end": 4143.0, "text": " My idea is that of a fast AI library in which they have the NLP part of it together with the image.", "tokens": [50364, 1222, 1558, 307, 300, 295, 257, 2370, 7318, 6405, 294, 597, 436, 362, 264, 426, 45196, 644, 295, 309, 1214, 365, 264, 3256, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16401711063108582, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.06490413099527359}, {"id": 389, "seek": 413400, "start": 4143.0, "end": 4160.0, "text": " And actually, Jeremy Howard has made this fast AI library in a way that when you fine tune the pre-trained model to a specific domain, then all the words that are not tokens,", "tokens": [50814, 400, 767, 11, 17809, 17626, 575, 1027, 341, 2370, 7318, 6405, 294, 257, 636, 300, 562, 291, 2489, 10864, 264, 659, 12, 17227, 2001, 2316, 281, 257, 2685, 9274, 11, 550, 439, 264, 2283, 300, 366, 406, 22667, 11, 51664], "temperature": 0.0, "avg_logprob": -0.16401711063108582, "compression_ratio": 1.5222222222222221, "no_speech_prob": 0.06490413099527359}, {"id": 390, "seek": 416000, "start": 4160.0, "end": 4168.0, "text": " but part of the original vocabulary of the pre-trained model will be added to the new vocabulary.", "tokens": [50364, 457, 644, 295, 264, 3380, 19864, 295, 264, 659, 12, 17227, 2001, 2316, 486, 312, 3869, 281, 264, 777, 19864, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1575086028487594, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.3479907214641571}, {"id": 391, "seek": 416000, "start": 4168.0, "end": 4177.0, "text": " And that will be, they will train by starting from scratch, let's say, no, as you do with the pre-training, but with this.", "tokens": [50764, 400, 300, 486, 312, 11, 436, 486, 3847, 538, 2891, 490, 8459, 11, 718, 311, 584, 11, 572, 11, 382, 291, 360, 365, 264, 659, 12, 17227, 1760, 11, 457, 365, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1575086028487594, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.3479907214641571}, {"id": 392, "seek": 416000, "start": 4177.0, "end": 4185.0, "text": " But I'm not sure that this is the case for Bert, to be honest, because I've been asking the guys in some other talks like this.", "tokens": [51214, 583, 286, 478, 406, 988, 300, 341, 307, 264, 1389, 337, 29594, 11, 281, 312, 3245, 11, 570, 286, 600, 668, 3365, 264, 1074, 294, 512, 661, 6686, 411, 341, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1575086028487594, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.3479907214641571}, {"id": 393, "seek": 416000, "start": 4185.0, "end": 4188.0, "text": " And as far as I know, they don't do that.", "tokens": [51614, 400, 382, 1400, 382, 286, 458, 11, 436, 500, 380, 360, 300, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1575086028487594, "compression_ratio": 1.6738197424892705, "no_speech_prob": 0.3479907214641571}, {"id": 394, "seek": 418800, "start": 4188.0, "end": 4203.0, "text": " And that is the original question, like, how well does perform Bert pre-trained, let's say pre-trained Bert, when we fine tune it, I don't know, in medicine, when most of the characters, I don't know, in whatever other domain specific task,", "tokens": [50364, 400, 300, 307, 264, 3380, 1168, 11, 411, 11, 577, 731, 775, 2042, 29594, 659, 12, 17227, 2001, 11, 718, 311, 584, 659, 12, 17227, 2001, 29594, 11, 562, 321, 2489, 10864, 309, 11, 286, 500, 380, 458, 11, 294, 7195, 11, 562, 881, 295, 264, 4342, 11, 286, 500, 380, 458, 11, 294, 2035, 661, 9274, 2685, 5633, 11, 51114], "temperature": 0.0, "avg_logprob": -0.16562004459714427, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.030503304675221443}, {"id": 395, "seek": 418800, "start": 4203.0, "end": 4209.0, "text": " when most of the words or tokens, sorry, tokens might be quite different, and they will be just unknown, no?", "tokens": [51114, 562, 881, 295, 264, 2283, 420, 22667, 11, 2597, 11, 22667, 1062, 312, 1596, 819, 11, 293, 436, 486, 312, 445, 9841, 11, 572, 30, 51414], "temperature": 0.0, "avg_logprob": -0.16562004459714427, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.030503304675221443}, {"id": 396, "seek": 418800, "start": 4209.0, "end": 4212.0, "text": " It's like technical question, sorry about that.", "tokens": [51414, 467, 311, 411, 6191, 1168, 11, 2597, 466, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16562004459714427, "compression_ratio": 1.6893617021276597, "no_speech_prob": 0.030503304675221443}, {"id": 397, "seek": 421200, "start": 4212.0, "end": 4221.0, "text": " Yeah, partly you answered, thank you, but just like it's not about the size of the vocabulary, but like exactly in this thing, thank you so much.", "tokens": [50364, 865, 11, 17031, 291, 10103, 11, 1309, 291, 11, 457, 445, 411, 309, 311, 406, 466, 264, 2744, 295, 264, 19864, 11, 457, 411, 2293, 294, 341, 551, 11, 1309, 291, 370, 709, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15387677131815158, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.00879610050469637}, {"id": 398, "seek": 421200, "start": 4221.0, "end": 4227.0, "text": " Yeah, so yeah, I think that's a good point, I think they are kind of related.", "tokens": [50814, 865, 11, 370, 1338, 11, 286, 519, 300, 311, 257, 665, 935, 11, 286, 519, 436, 366, 733, 295, 4077, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15387677131815158, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.00879610050469637}, {"id": 399, "seek": 421200, "start": 4227.0, "end": 4240.0, "text": " So I think in the Bert model, they basically have a fixed vocabulary where all the words appear that appeared during pre-training plus an unknown token.", "tokens": [51114, 407, 286, 519, 294, 264, 29594, 2316, 11, 436, 1936, 362, 257, 6806, 19864, 689, 439, 264, 2283, 4204, 300, 8516, 1830, 659, 12, 17227, 1760, 1804, 364, 9841, 14862, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15387677131815158, "compression_ratio": 1.6563876651982379, "no_speech_prob": 0.00879610050469637}, {"id": 400, "seek": 424000, "start": 4240.0, "end": 4247.0, "text": " And then if you have to in fine-tuning the new words, it will just be mapped to this unknown token so they don't expand the vocabulary, right?", "tokens": [50364, 400, 550, 498, 291, 362, 281, 294, 2489, 12, 83, 37726, 264, 777, 2283, 11, 309, 486, 445, 312, 33318, 281, 341, 9841, 14862, 370, 436, 500, 380, 5268, 264, 19864, 11, 558, 30, 50714], "temperature": 0.0, "avg_logprob": -0.15154672102494673, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.003374603809788823}, {"id": 401, "seek": 424000, "start": 4247.0, "end": 4254.0, "text": " That's what I'm suspecting. And you mentioned that fast AI, they would add it to vocabulary.", "tokens": [50714, 663, 311, 437, 286, 478, 9091, 278, 13, 400, 291, 2835, 300, 2370, 7318, 11, 436, 576, 909, 309, 281, 19864, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15154672102494673, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.003374603809788823}, {"id": 402, "seek": 424000, "start": 4254.0, "end": 4266.0, "text": " Yeah, I haven't really worked with the fast AI version of this, so that sounds like a good approach right now if you then have some really, if you don't want to maybe pre-trained completely from scratch,", "tokens": [51064, 865, 11, 286, 2378, 380, 534, 2732, 365, 264, 2370, 7318, 3037, 295, 341, 11, 370, 300, 3263, 411, 257, 665, 3109, 558, 586, 498, 291, 550, 362, 512, 534, 11, 498, 291, 500, 380, 528, 281, 1310, 659, 12, 17227, 2001, 2584, 490, 8459, 11, 51664], "temperature": 0.0, "avg_logprob": -0.15154672102494673, "compression_ratio": 1.6199261992619927, "no_speech_prob": 0.003374603809788823}, {"id": 403, "seek": 426600, "start": 4266.0, "end": 4275.0, "text": " it's a very different data set with different tokens, so you kind of, it's kind of more flexible in that way, I would say. Yeah, that makes sense, yeah.", "tokens": [50364, 309, 311, 257, 588, 819, 1412, 992, 365, 819, 22667, 11, 370, 291, 733, 295, 11, 309, 311, 733, 295, 544, 11358, 294, 300, 636, 11, 286, 576, 584, 13, 865, 11, 300, 1669, 2020, 11, 1338, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2224611003747147, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.015032071620225906}, {"id": 404, "seek": 426600, "start": 4275.0, "end": 4278.0, "text": " Thank you very much.", "tokens": [50814, 1044, 291, 588, 709, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2224611003747147, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.015032071620225906}, {"id": 405, "seek": 426600, "start": 4278.0, "end": 4288.0, "text": " Thank you, Altie. Again, if you want to ask any questions, you can raise your hand and we will unmute you so we can ask questions to Dr. Spastien.", "tokens": [50964, 1044, 291, 11, 15992, 414, 13, 3764, 11, 498, 291, 528, 281, 1029, 604, 1651, 11, 291, 393, 5300, 428, 1011, 293, 321, 486, 41445, 291, 370, 321, 393, 1029, 1651, 281, 2491, 13, 1738, 525, 1053, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2224611003747147, "compression_ratio": 1.5841584158415842, "no_speech_prob": 0.015032071620225906}, {"id": 406, "seek": 428800, "start": 4288.0, "end": 4302.0, "text": " In the chat, we have a question from Anna. How to handle the bottleneck caused by the data loader reading each example from memory?", "tokens": [50364, 682, 264, 5081, 11, 321, 362, 257, 1168, 490, 12899, 13, 1012, 281, 4813, 264, 44641, 547, 7008, 538, 264, 1412, 3677, 260, 3760, 1184, 1365, 490, 4675, 30, 51064], "temperature": 0.0, "avg_logprob": -0.1900938318130818, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.011472510173916817}, {"id": 407, "seek": 428800, "start": 4302.0, "end": 4317.0, "text": " Yeah, the bottleneck, oh, so in terms of speed, yeah, you could increase, so what I do sometimes if it's too slow for a very large data set, what I would do is I would increase, I would make a new data loader for the training set.", "tokens": [51064, 865, 11, 264, 44641, 547, 11, 1954, 11, 370, 294, 2115, 295, 3073, 11, 1338, 11, 291, 727, 3488, 11, 370, 437, 286, 360, 2171, 498, 309, 311, 886, 2964, 337, 257, 588, 2416, 1412, 992, 11, 437, 286, 576, 360, 307, 286, 576, 3488, 11, 286, 576, 652, 257, 777, 1412, 3677, 260, 337, 264, 3097, 992, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1900938318130818, "compression_ratio": 1.6605504587155964, "no_speech_prob": 0.011472510173916817}, {"id": 408, "seek": 431700, "start": 4317.0, "end": 4333.0, "text": " If I want to compute the training set accuracy, I would have a second one, which has just a larger batch size than a one I used during training and for the test set, I would just set the batch size as large as it goes until it crashes, because at some point it would just crash.", "tokens": [50364, 759, 286, 528, 281, 14722, 264, 3097, 992, 14170, 11, 286, 576, 362, 257, 1150, 472, 11, 597, 575, 445, 257, 4833, 15245, 2744, 813, 257, 472, 286, 1143, 1830, 3097, 293, 337, 264, 1500, 992, 11, 286, 576, 445, 992, 264, 15245, 2744, 382, 2416, 382, 309, 1709, 1826, 309, 28642, 11, 570, 412, 512, 935, 309, 576, 445, 8252, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18418716661857837, "compression_ratio": 1.6449704142011834, "no_speech_prob": 0.017160847783088684}, {"id": 409, "seek": 433300, "start": 4333.0, "end": 4349.0, "text": " In a case like this, I would say it's not really a bottleneck, it's, I mean, it takes maybe a minute or maybe a few seconds to process the whole data set in each epoch, and we are only computing in the accuracy function on, like,", "tokens": [50364, 682, 257, 1389, 411, 341, 11, 286, 576, 584, 309, 311, 406, 534, 257, 44641, 547, 11, 309, 311, 11, 286, 914, 11, 309, 2516, 1310, 257, 3456, 420, 1310, 257, 1326, 3949, 281, 1399, 264, 1379, 1412, 992, 294, 1184, 30992, 339, 11, 293, 321, 366, 787, 15866, 294, 264, 14170, 2445, 322, 11, 411, 11, 51164], "temperature": 0.0, "avg_logprob": -0.11808515062519148, "compression_ratio": 1.7, "no_speech_prob": 0.0802798941731453}, {"id": 410, "seek": 433300, "start": 4349.0, "end": 4358.0, "text": " the performance in each epoch, so it's not like too often, I think this is reasonable, but maybe the question is also about like how we deal with that bottleneck during training.", "tokens": [51164, 264, 3389, 294, 1184, 30992, 339, 11, 370, 309, 311, 406, 411, 886, 2049, 11, 286, 519, 341, 307, 10585, 11, 457, 1310, 264, 1168, 307, 611, 466, 411, 577, 321, 2028, 365, 300, 44641, 547, 1830, 3097, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11808515062519148, "compression_ratio": 1.7, "no_speech_prob": 0.0802798941731453}, {"id": 411, "seek": 435800, "start": 4358.0, "end": 4368.0, "text": " That's a good question, I am actually, yeah, that's one limitation, so I could only go up to batch size 16 here in this example, because otherwise I would run out of memory.", "tokens": [50364, 663, 311, 257, 665, 1168, 11, 286, 669, 767, 11, 1338, 11, 300, 311, 472, 27432, 11, 370, 286, 727, 787, 352, 493, 281, 15245, 2744, 3165, 510, 294, 341, 1365, 11, 570, 5911, 286, 576, 1190, 484, 295, 4675, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11782944997151693, "compression_ratio": 1.300751879699248, "no_speech_prob": 0.003592632245272398}, {"id": 412, "seek": 436800, "start": 4368.0, "end": 4383.0, "text": " So one way to deal with that would be using a different type of transformer, for example, so there are more efficient transformers that have fewer parameters and smaller maybe inputs sizes, smaller number of tokens that might be in possibility.", "tokens": [50364, 407, 472, 636, 281, 2028, 365, 300, 576, 312, 1228, 257, 819, 2010, 295, 31782, 11, 337, 1365, 11, 370, 456, 366, 544, 7148, 4088, 433, 300, 362, 13366, 9834, 293, 4356, 1310, 15743, 11602, 11, 4356, 1230, 295, 22667, 300, 1062, 312, 294, 7959, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17498708724975587, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.016910294070839882}, {"id": 413, "seek": 438300, "start": 4383.0, "end": 4397.0, "text": " But there are also different ways for distributed computing, it depends on what type of distributed computing you do some can kind of put the model onto different devices and handle with handle batches like that.", "tokens": [50364, 583, 456, 366, 611, 819, 2098, 337, 12631, 15866, 11, 309, 5946, 322, 437, 2010, 295, 12631, 15866, 291, 360, 512, 393, 733, 295, 829, 264, 2316, 3911, 819, 5759, 293, 4813, 365, 4813, 15245, 279, 411, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1269282296646473, "compression_ratio": 1.536231884057971, "no_speech_prob": 0.00884330552071333}, {"id": 414, "seek": 439700, "start": 4397.0, "end": 4407.0, "text": " So for instance, if you remove the number of GPU restriction I had in my code, it would run on multiple GPUs and I think it splits up the batch size.", "tokens": [50364, 407, 337, 5197, 11, 498, 291, 4159, 264, 1230, 295, 18407, 29529, 286, 632, 294, 452, 3089, 11, 309, 576, 1190, 322, 3866, 18407, 82, 293, 286, 519, 309, 37741, 493, 264, 15245, 2744, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13047632149287633, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.039014458656311035}, {"id": 415, "seek": 439700, "start": 4407.0, "end": 4426.0, "text": " In this case, it I think would multiply the batch size if you have four GPUs before and then each GPU gets 16 but still you're processing 64 training examples at the same time and then you average the gradients from the different GPUs in that way, you can use larger batch sizes if you have multiple GPUs, that's another option.", "tokens": [50864, 682, 341, 1389, 11, 309, 286, 519, 576, 12972, 264, 15245, 2744, 498, 291, 362, 1451, 18407, 82, 949, 293, 550, 1184, 18407, 2170, 3165, 457, 920, 291, 434, 9007, 12145, 3097, 5110, 412, 264, 912, 565, 293, 550, 291, 4274, 264, 2771, 2448, 490, 264, 819, 18407, 82, 294, 300, 636, 11, 291, 393, 764, 4833, 15245, 11602, 498, 291, 362, 3866, 18407, 82, 11, 300, 311, 1071, 3614, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13047632149287633, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.039014458656311035}, {"id": 416, "seek": 442600, "start": 4427.0, "end": 4439.0, "text": " But yeah, that's fundamentally a limitation of matrix modification in memory, for example, if you have only access to one GPU that could be challenging.", "tokens": [50414, 583, 1338, 11, 300, 311, 17879, 257, 27432, 295, 8141, 26747, 294, 4675, 11, 337, 1365, 11, 498, 291, 362, 787, 2105, 281, 472, 18407, 300, 727, 312, 7595, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2261228676301887, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.0031636671628803015}, {"id": 417, "seek": 442600, "start": 4439.0, "end": 4440.0, "text": " Great.", "tokens": [51014, 3769, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2261228676301887, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.0031636671628803015}, {"id": 418, "seek": 442600, "start": 4440.0, "end": 4452.0, "text": " Well, I studied with Anna, she's asking, how can an individual like graduate student train a transformer for data for which there is no break train transformer available.", "tokens": [51064, 1042, 11, 286, 9454, 365, 12899, 11, 750, 311, 3365, 11, 577, 393, 364, 2609, 411, 8080, 3107, 3847, 257, 31782, 337, 1412, 337, 597, 456, 307, 572, 1821, 3847, 31782, 2435, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2261228676301887, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.0031636671628803015}, {"id": 419, "seek": 442600, "start": 4452.0, "end": 4455.0, "text": " Is it even possible to resource wise.", "tokens": [51664, 1119, 309, 754, 1944, 281, 7684, 10829, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2261228676301887, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.0031636671628803015}, {"id": 420, "seek": 445500, "start": 4455.0, "end": 4459.0, "text": " Yeah, good question. I haven't done this myself yet I'm.", "tokens": [50364, 865, 11, 665, 1168, 13, 286, 2378, 380, 1096, 341, 2059, 1939, 286, 478, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12828090374286358, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.0015002802247181535}, {"id": 421, "seek": 445500, "start": 4459.0, "end": 4472.0, "text": " I think based on what I've seen from papers where people have done that for example this protein sequence paper training from scratch look very scary in terms of what resources on they had to use in order to make that happen.", "tokens": [50564, 286, 519, 2361, 322, 437, 286, 600, 1612, 490, 10577, 689, 561, 362, 1096, 300, 337, 1365, 341, 7944, 8310, 3035, 3097, 490, 8459, 574, 588, 6958, 294, 2115, 295, 437, 3593, 322, 436, 632, 281, 764, 294, 1668, 281, 652, 300, 1051, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12828090374286358, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.0015002802247181535}, {"id": 422, "seek": 447200, "start": 4472.0, "end": 4485.0, "text": " And if the only case is like maybe if you have a small data set, maybe that might be feasible but I think then maybe the transformer won't be the best option if you only have a small data set for pre training, then maybe using a classic RNN or", "tokens": [50364, 400, 498, 264, 787, 1389, 307, 411, 1310, 498, 291, 362, 257, 1359, 1412, 992, 11, 1310, 300, 1062, 312, 26648, 457, 286, 519, 550, 1310, 264, 31782, 1582, 380, 312, 264, 1151, 3614, 498, 291, 787, 362, 257, 1359, 1412, 992, 337, 659, 3097, 11, 550, 1310, 1228, 257, 7230, 45702, 45, 420, 51014], "temperature": 0.0, "avg_logprob": -0.15643966609034046, "compression_ratio": 1.577922077922078, "no_speech_prob": 0.0012253000168129802}, {"id": 423, "seek": 448500, "start": 4486.0, "end": 4501.0, "text": " even a bag of words model might be a better approach. But yeah I don't have unfortunately a good answer for that I think that's a concern I have to, if you I personally also don't have access to a large cluster where I could do that and even if I would have access to that.", "tokens": [50414, 754, 257, 3411, 295, 2283, 2316, 1062, 312, 257, 1101, 3109, 13, 583, 1338, 286, 500, 380, 362, 7015, 257, 665, 1867, 337, 300, 286, 519, 300, 311, 257, 3136, 286, 362, 281, 11, 498, 291, 286, 5665, 611, 500, 380, 362, 2105, 281, 257, 2416, 13630, 689, 286, 727, 360, 300, 293, 754, 498, 286, 576, 362, 2105, 281, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18510564168294272, "compression_ratio": 1.56, "no_speech_prob": 0.05575927346944809}, {"id": 424, "seek": 450100, "start": 4502.0, "end": 4516.0, "text": " All the engineering efforts that go into setting this up is also kind of challenging so companies, so far as I know, have really large teams of engineers that only really focus on the coding part just to run the model on these multiple devices and GPUs.", "tokens": [50414, 1057, 264, 7043, 6484, 300, 352, 666, 3287, 341, 493, 307, 611, 733, 295, 7595, 370, 3431, 11, 370, 1400, 382, 286, 458, 11, 362, 534, 2416, 5491, 295, 11955, 300, 787, 534, 1879, 322, 264, 17720, 644, 445, 281, 1190, 264, 2316, 322, 613, 3866, 5759, 293, 18407, 82, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11698029894347585, "compression_ratio": 1.664406779661017, "no_speech_prob": 0.19879229366779327}, {"id": 425, "seek": 450100, "start": 4516.0, "end": 4530.0, "text": " I mean there are API said make that simpler but I think in practice it's still not very easy to do that we need to have like experts doing that but maybe I'm wrong so I don't want to discourage anyone from trying this out in practice but", "tokens": [51114, 286, 914, 456, 366, 9362, 848, 652, 300, 18587, 457, 286, 519, 294, 3124, 309, 311, 920, 406, 588, 1858, 281, 360, 300, 321, 643, 281, 362, 411, 8572, 884, 300, 457, 1310, 286, 478, 2085, 370, 286, 500, 380, 528, 281, 21497, 609, 2878, 490, 1382, 341, 484, 294, 3124, 457, 51814], "temperature": 0.0, "avg_logprob": -0.11698029894347585, "compression_ratio": 1.664406779661017, "no_speech_prob": 0.19879229366779327}, {"id": 426, "seek": 453000, "start": 4530.0, "end": 4535.0, "text": " I think there's a better answer at that moment.", "tokens": [50364, 286, 519, 456, 311, 257, 1101, 1867, 412, 300, 1623, 13, 50614], "temperature": 0.0, "avg_logprob": -0.43348946204552286, "compression_ratio": 1.3672316384180792, "no_speech_prob": 0.04267924651503563}, {"id": 427, "seek": 453000, "start": 4535.0, "end": 4539.0, "text": " Back to Ritu Paratha.", "tokens": [50614, 5833, 281, 497, 6380, 3457, 29441, 13, 50814], "temperature": 0.0, "avg_logprob": -0.43348946204552286, "compression_ratio": 1.3672316384180792, "no_speech_prob": 0.04267924651503563}, {"id": 428, "seek": 453000, "start": 4539.0, "end": 4558.0, "text": " I have a personally point tuned a pre trained GBT to model, which has 115 million bar meters with about 2600 mid sized text on 70 lines each and talk with me 15 minutes in.", "tokens": [50814, 286, 362, 257, 5665, 935, 10870, 257, 659, 8895, 26809, 51, 281, 2316, 11, 597, 575, 39436, 2459, 2159, 8146, 365, 466, 7551, 628, 2062, 20004, 2487, 322, 5285, 3876, 1184, 293, 751, 365, 385, 2119, 2077, 294, 13, 51764], "temperature": 0.0, "avg_logprob": -0.43348946204552286, "compression_ratio": 1.3672316384180792, "no_speech_prob": 0.04267924651503563}, {"id": 429, "seek": 455800, "start": 4559.0, "end": 4561.0, "text": " Yeah, that's nice. Okay.", "tokens": [50414, 865, 11, 300, 311, 1481, 13, 1033, 13, 50514], "temperature": 0.0, "avg_logprob": -0.22436069955631177, "compression_ratio": 1.552, "no_speech_prob": 0.0033700992353260517}, {"id": 430, "seek": 455800, "start": 4561.0, "end": 4576.0, "text": " So on a very small 3000 data point. Okay, I didn't expect it to perform that well that is actually cool. So, one at 7 million it will be the size of GPT one approximately that is impressive I, I think that that sounds very promising then.", "tokens": [50514, 407, 322, 257, 588, 1359, 20984, 1412, 935, 13, 1033, 11, 286, 994, 380, 2066, 309, 281, 2042, 300, 731, 300, 307, 767, 1627, 13, 407, 11, 472, 412, 1614, 2459, 309, 486, 312, 264, 2744, 295, 26039, 51, 472, 10447, 300, 307, 8992, 286, 11, 286, 519, 300, 300, 3263, 588, 20257, 550, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22436069955631177, "compression_ratio": 1.552, "no_speech_prob": 0.0033700992353260517}, {"id": 431, "seek": 455800, "start": 4576.0, "end": 4585.0, "text": " So, forget what I said in the previous answer. Maybe it is possible to really pretend train them on small data sets so yeah.", "tokens": [51264, 407, 11, 2870, 437, 286, 848, 294, 264, 3894, 1867, 13, 2704, 309, 307, 1944, 281, 534, 11865, 3847, 552, 322, 1359, 1412, 6352, 370, 1338, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22436069955631177, "compression_ratio": 1.552, "no_speech_prob": 0.0033700992353260517}, {"id": 432, "seek": 458500, "start": 4586.0, "end": 4595.0, "text": " A question from Sujana. Why is there a norm used and not bad snow.", "tokens": [50414, 316, 1168, 490, 2746, 73, 2095, 13, 1545, 307, 456, 257, 2026, 1143, 293, 406, 1578, 5756, 13, 50864], "temperature": 0.0, "avg_logprob": -0.544243032282049, "compression_ratio": 0.9295774647887324, "no_speech_prob": 0.012413015589118004}, {"id": 433, "seek": 459500, "start": 4595.0, "end": 4606.0, "text": " That is a good question why is layer norm used and not bedroom. I think it has something to do with the fact that we have a sequence.", "tokens": [50364, 663, 307, 257, 665, 1168, 983, 307, 4583, 2026, 1143, 293, 406, 11211, 13, 286, 519, 309, 575, 746, 281, 360, 365, 264, 1186, 300, 321, 362, 257, 8310, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18047593621646657, "compression_ratio": 1.2429906542056075, "no_speech_prob": 0.010008161887526512}, {"id": 434, "seek": 460600, "start": 4606.0, "end": 4618.0, "text": " We normalize across the sequence and not across across the batches by top of my head, I don't have a good answer I could.", "tokens": [50364, 492, 2710, 1125, 2108, 264, 8310, 293, 406, 2108, 2108, 264, 15245, 279, 538, 1192, 295, 452, 1378, 11, 286, 500, 380, 362, 257, 665, 1867, 286, 727, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2105742512327252, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.057456351816654205}, {"id": 435, "seek": 460600, "start": 4618.0, "end": 4631.0, "text": " I think I was thinking about that at some point but I forgot I think it's the way we normalize over over the batches in a way.", "tokens": [50964, 286, 519, 286, 390, 1953, 466, 300, 412, 512, 935, 457, 286, 5298, 286, 519, 309, 311, 264, 636, 321, 2710, 1125, 670, 670, 264, 15245, 279, 294, 257, 636, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2105742512327252, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.057456351816654205}, {"id": 436, "seek": 463100, "start": 4631.0, "end": 4642.0, "text": " That's a good question. Another thing is batch not doesn't really perform well. If you have batch sizes smaller than 32 or 16, but I don't think this is the answer I think the answer is mobile.", "tokens": [50364, 663, 311, 257, 665, 1168, 13, 3996, 551, 307, 15245, 406, 1177, 380, 534, 2042, 731, 13, 759, 291, 362, 15245, 11602, 4356, 813, 8858, 420, 3165, 11, 457, 286, 500, 380, 519, 341, 307, 264, 1867, 286, 519, 264, 1867, 307, 6013, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22521635773894075, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.00921730138361454}, {"id": 437, "seek": 463100, "start": 4642.0, "end": 4649.0, "text": " The way we normalize this I would have to think about this again why they're not not batch norm.", "tokens": [50914, 440, 636, 321, 2710, 1125, 341, 286, 576, 362, 281, 519, 466, 341, 797, 983, 436, 434, 406, 406, 15245, 2026, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22521635773894075, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.00921730138361454}, {"id": 438, "seek": 463100, "start": 4649.0, "end": 4652.0, "text": " Good question.", "tokens": [51264, 2205, 1168, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22521635773894075, "compression_ratio": 1.5482233502538072, "no_speech_prob": 0.00921730138361454}, {"id": 439, "seek": 465200, "start": 4652.0, "end": 4661.0, "text": " Anyway, I think this concludes the Q&A session for this talk. Thank you so much, Dr. Sebastian.", "tokens": [50364, 5684, 11, 286, 519, 341, 24643, 264, 1249, 5, 32, 5481, 337, 341, 751, 13, 1044, 291, 370, 709, 11, 2491, 13, 31102, 13, 50814], "temperature": 0.0, "avg_logprob": -0.22001360575358073, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.06285157799720764}, {"id": 440, "seek": 465200, "start": 4661.0, "end": 4663.0, "text": " Back to you.", "tokens": [50814, 5833, 281, 291, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22001360575358073, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.06285157799720764}, {"id": 441, "seek": 465200, "start": 4663.0, "end": 4666.0, "text": " Thank you, thank you Dr. Sebastian.", "tokens": [50914, 1044, 291, 11, 1309, 291, 2491, 13, 31102, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22001360575358073, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.06285157799720764}, {"id": 442, "seek": 465200, "start": 4666.0, "end": 4674.0, "text": " Sebastian, can you share the slides or the code that you show us on the chat so we can post it on the Twitter. Yep.", "tokens": [51064, 31102, 11, 393, 291, 2073, 264, 9788, 420, 264, 3089, 300, 291, 855, 505, 322, 264, 5081, 370, 321, 393, 2183, 309, 322, 264, 5794, 13, 7010, 13, 51464], "temperature": 0.0, "avg_logprob": -0.22001360575358073, "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.06285157799720764}, {"id": 443, "seek": 467400, "start": 4674.0, "end": 4678.0, "text": " I have it right away. I have this GitHub repository here.", "tokens": [50364, 286, 362, 309, 558, 1314, 13, 286, 362, 341, 23331, 25841, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2120456042355054, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.06721504777669907}, {"id": 444, "seek": 467400, "start": 4678.0, "end": 4686.0, "text": " And the GitHub repository has a link to the slides. I can also post it separately.", "tokens": [50564, 400, 264, 23331, 25841, 575, 257, 2113, 281, 264, 9788, 13, 286, 393, 611, 2183, 309, 14759, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2120456042355054, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.06721504777669907}, {"id": 445, "seek": 467400, "start": 4686.0, "end": 4687.0, "text": " It's right here.", "tokens": [50964, 467, 311, 558, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2120456042355054, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.06721504777669907}, {"id": 446, "seek": 467400, "start": 4687.0, "end": 4689.0, "text": " Thank you.", "tokens": [51014, 1044, 291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2120456042355054, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.06721504777669907}, {"id": 447, "seek": 467400, "start": 4689.0, "end": 4696.0, "text": " Great so we definitely want to give a huge thank you for you Sebastian. This was super insightful and hopeful.", "tokens": [51114, 3769, 370, 321, 2138, 528, 281, 976, 257, 2603, 1309, 291, 337, 291, 31102, 13, 639, 390, 1687, 46401, 293, 20531, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2120456042355054, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.06721504777669907}, {"id": 448, "seek": 469600, "start": 4696.0, "end": 4705.0, "text": " Thank you everyone for joining. And please follow us on Twitter for more details about upcoming talks and see you soon. Thank you.", "tokens": [50364, 1044, 291, 1518, 337, 5549, 13, 400, 1767, 1524, 505, 322, 5794, 337, 544, 4365, 466, 11500, 6686, 293, 536, 291, 2321, 13, 1044, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11584477154713757, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.07763908803462982}, {"id": 449, "seek": 469600, "start": 4705.0, "end": 4719.0, "text": " Yeah, thank you so much everyone for attending and also very good questions. Sorry for rushing a little bit about through the topics I don't know it's like I always want to talk about so many things and there's only so little time, but I appreciate your", "tokens": [50814, 865, 11, 1309, 291, 370, 709, 1518, 337, 15862, 293, 611, 588, 665, 1651, 13, 4919, 337, 25876, 257, 707, 857, 466, 807, 264, 8378, 286, 500, 380, 458, 309, 311, 411, 286, 1009, 528, 281, 751, 466, 370, 867, 721, 293, 456, 311, 787, 370, 707, 565, 11, 457, 286, 4449, 428, 51514], "temperature": 0.0, "avg_logprob": -0.11584477154713757, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.07763908803462982}, {"id": 450, "seek": 469600, "start": 4719.0, "end": 4724.0, "text": " patience and joining me here today that was fun and I really liked your questions that was cool.", "tokens": [51514, 14826, 293, 5549, 385, 510, 965, 300, 390, 1019, 293, 286, 534, 4501, 428, 1651, 300, 390, 1627, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11584477154713757, "compression_ratio": 1.736462093862816, "no_speech_prob": 0.07763908803462982}, {"id": 451, "seek": 472400, "start": 4724.0, "end": 4727.0, "text": " Yeah, then have a great rest of the day everyone.", "tokens": [50364, 865, 11, 550, 362, 257, 869, 1472, 295, 264, 786, 1518, 13, 50514], "temperature": 0.0, "avg_logprob": -0.33848068237304685, "compression_ratio": 1.0634920634920635, "no_speech_prob": 0.26023924350738525}, {"id": 452, "seek": 472400, "start": 4727.0, "end": 4728.0, "text": " Bye bye.", "tokens": [50514, 4621, 6543, 13, 50564], "temperature": 0.0, "avg_logprob": -0.33848068237304685, "compression_ratio": 1.0634920634920635, "no_speech_prob": 0.26023924350738525}, {"id": 453, "seek": 472400, "start": 4728.0, "end": 4729.0, "text": " Bye bye.", "tokens": [50564, 4621, 6543, 13, 50614], "temperature": 0.0, "avg_logprob": -0.33848068237304685, "compression_ratio": 1.0634920634920635, "no_speech_prob": 0.26023924350738525}], "language": "en"}